8
1
0
2
 
g
u
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
2
v
2
3
7
4
0
.
4
0
8
1
:
v
i
X
r
a

Multimodal Unsupervised
Image-to-Image Translation

Xun Huang1, Ming-Yu Liu2, Serge Belongie1, Jan Kautz2

Cornell University1

NVIDIA2

Abstract. Unsupervised image-to-image translation is an important and
challenging problem in computer vision. Given an image in the source
domain, the goal is to learn the conditional distribution of correspond-
ing images in the target domain, without seeing any examples of corre-
sponding image pairs. While this conditional distribution is inherently
multimodal, existing approaches make an overly simpliﬁed assumption,
modeling it as a deterministic one-to-one mapping. As a result, they fail
to generate diverse outputs from a given source domain image. To address
this limitation, we propose a Multimodal Unsupervised Image-to-image
Translation (MUNIT) framework. We assume that the image represen-
tation can be decomposed into a content code that is domain-invariant,
and a style code that captures domain-speciﬁc properties. To translate
an image to another domain, we recombine its content code with a ran-
dom style code sampled from the style space of the target domain. We
analyze the proposed framework and establish several theoretical results.
Extensive experiments with comparisons to state-of-the-art approaches
further demonstrate the advantage of the proposed framework. Moreover,
our framework allows users to control the style of translation outputs by
providing an example style image. Code and pretrained models are avail-
able at https://github.com/nvlabs/MUNIT.

Keywords: GANs, image-to-image translation, style transfer

1 Introduction

Many problems in computer vision aim at translating images from one domain to
another, including super-resolution [1], colorization [2], inpainting [3], attribute
transfer [4], and style transfer [5]. This cross-domain image-to-image transla-
tion setting has therefore received signiﬁcant attention [6–25]. When the dataset
contains paired examples, this problem can be approached by a conditional gen-
erative model [6] or a simple regression model [13]. In this work, we focus on the
much more challenging setting when such supervision is unavailable.

In many scenarios, the cross-domain mapping of interest is multimodal. For
example, a winter scene could have many possible appearances during summer
due to weather, timing, lighting, etc. Unfortunately, existing techniques usually
assume a deterministic [8–10] or unimodal [15] mapping. As a result, they fail
to capture the full distribution of possible outputs. Even if the model is made
stochastic by injecting noise, the network usually learns to ignore it [6, 26].

2

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

Fig. 1. An illustration of our method. (a) Images in each domain Xi are encoded to a
shared content space C and a domain-speciﬁc style space Si. Each encoder has an inverse
decoder omitted from this ﬁgure. (b) To translate an image in X1 (e.g., a leopard) to
X2 (e.g., domestic cats), we recombine the content code of the input with a random
style code in the target style space. Diﬀerent style codes lead to diﬀerent outputs.

In this paper, we propose a principled framework for the Multimodal UNsu-
pervised Image-to-image Translation (MUNIT) problem. As shown in Fig. 1 (a),
our framework makes several assumptions. We ﬁrst assume that the latent space
of images can be decomposed into a content space and a style space. We further
assume that images in diﬀerent domains share a common content space but not
the style space. To translate an image to the target domain, we recombine its
content code with a random style code in the target style space (Fig. 1 (b)). The
content code encodes the information that should be preserved during transla-
tion, while the style code represents remaining variations that are not contained
in the input image. By sampling diﬀerent style codes, our model is able to pro-
duce diverse and multimodal outputs. Extensive experiments demonstrate the
eﬀectiveness of our method in modeling multimodal output distributions and
its superior image quality compared with state-of-the-art approaches. Moreover,
the decomposition of content and style spaces allows our framework to perform
example-guided image translation, in which the style of the translation outputs
are controlled by a user-provided example image in the target domain.

2 Related Works

Generative adversarial networks (GANs). The GAN framework [27] has
achieved impressive results in image generation. In GAN training, a generator is
trained to fool a discriminator which in turn tries to distinguish between gener-
ated samples and real samples. Various improvements to GANs have been pro-
posed, such as multi-stage generation [28–33], better training objectives [34–39],
and combination with auto-encoders [40–44]. In this work, we employ GANs to
align the distribution of translated images with real images in the target domain.
Image-to-image translation. Isola et al. [6] propose the ﬁrst uniﬁed frame-
work for image-to-image translation based on conditional GANs, which has been
extended to generating high-resolution images by Wang et al. [20]. Recent stud-
ies have also attempted to learn image translation without supervision. This

Multimodal Unsupervised Image-to-Image Translation

3

problem is inherently ill-posed and requires additional constraints. Some works
enforce the translation to preserve certain properties of the source domain data,
such as pixel values [21], pixel gradients [22], semantic features [10], class labels
[22], or pairwise sample distances [16]. Another popular constraint is the cycle
consistency loss [7–9]. It enforces that if we translate an image to the target do-
main and back, we should obtain the original image. In addition, Liu et al. [15]
propose the UNIT framework, which assumes a shared latent space such that
corresponding images in two domains are mapped to the same latent code.

A signiﬁcant limitation of most existing image-to-image translation meth-
ods is the lack of diversity in the translated outputs. To tackle this problem,
some works propose to simultaneously generate multiple outputs given the same
input and encourage them to be diﬀerent [13, 45, 46]. Still, these methods can
only generate a discrete number of outputs. Zhu et al. [11] propose a Bicycle-
GAN that can model continuous and multimodal distributions. However, all the
aforementioned methods require pair supervision, while our method does not. A
couple of concurrent works also recognize this limitation and propose extensions
of CycleGAN/UNIT for multimodal mapping [47]/[48].

Our problem has some connections with multi-domain image-to-image trans-
lation [19, 49, 50]. Speciﬁcally, when we know how many modes each domain has
and the mode each sample belongs to, it is possible to treat each mode as a sep-
arate domain and use multi-domain image-to-image translation techniques to
learn a mapping between each pair of modes, thus achieving multimodal trans-
lation. However, in general we do not assume such information is available. Also,
our stochastic model can represent continuous output distributions, while [19,
49, 50] still use a deterministic model for each pair of domains.

Style transfer. Style transfer aims at modifying the style of an image while
preserving its content, which is closely related to image-to-image translation.
Here, we make a distinction between example-guided style transfer, in which the
target style comes from a single example, and collection style transfer, in which
the target style is deﬁned by a collection of images. Classical style transfer ap-
proaches [5, 51–56] typically tackle the former problem, whereas image-to-image
translation methods have been demonstrated to perform well in the latter [8].
We will show that our model is able to address both problems, thanks to its
disentangled representation of content and style.

Learning disentangled representations. Our work draws inspiration from
recent works on disentangled representation learning. For example, InfoGAN [57]
and β-VAE [58] have been proposed to learn disentangled representations with-
out supervision. Some other works [59–66] focus on disentangling content from
style. Although it is diﬃcult to deﬁne content/style and diﬀerent works use dif-
ferent deﬁnitions, we refer to “content” as the underling spatial structure and
“style” as the rendering of the structure. In our setting, we have two domains
that share the same content distribution but have diﬀerent style distributions.

4

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

3 Multimodal Unsupervised Image-to-image Translation

3.1 Assumptions

Let x1 ∈ X1 and x2 ∈ X2 be images from two diﬀerent image domains. In the
unsupervised image-to-image translation setting, we are given samples drawn
from two marginal distributions p(x1) and p(x2), without access to the joint
distribution p(x1, x2). Our goal is to estimate the two conditionals p(x2|
x1)
x1) and
x2) with learned image-to-image translation models p(x1
and p(x1|
2|
X2 (similar
2 is a sample produced by translating x1 to
p(x2
→
for x2
x2) are complex and multimodal distri-
butions, in which case a deterministic translation model does not work well.

x2), where x1
1|
1). In general, p(x2|
→

x1) and p(x1|

→

→

∈ C

∈ S

∈ X

To tackle this problem, we make a partially shared latent space assumption.
i is generated from a content
Speciﬁcally, we assume that each image xi
that is shared by both domains, and a style latent code
latent code c
si
i that is speciﬁc to the individual domain. In other words, a pair of
corresponding images (x1, x2) from the joint distribution is generated by x1 =
G∗1(c, s1) and x2 = G∗2(c, s2), where c, s1, s2 are from some prior distributions
and G∗1, G∗2 are the underlying generators. We further assume that G∗1 and G∗2 are
1 and E∗2 =
deterministic functions and have their inverse encoders E∗1 = (G∗1)−
1. Our goal is to learn the underlying generator and encoder functions with
(G∗2)−
neural networks. Note that although the encoders and decoders are deterministic,
p(x2|
Our assumption is closely related to the shared latent space assumption pro-
posed in UNIT [15]. While UNIT assumes a fully shared latent space, we postu-
late that only part of the latent space (the content) can be shared across domains
whereas the other part (the style) is domain speciﬁc, which is a more reasonable
assumption when the cross-domain mapping is many-to-many.

x1) is a continuous distribution due to the dependency of s2.

3.2 Model

X

i (xi), Es

Fig. 2 shows an overview of our model and its learning process. Similar to Liu
et al. [15], our translation model consists of an encoder Ei and a decoder Gi for
each domain
i (i = 1, 2). As shown in Fig. 2 (a), the latent code of each auto-
encoder is factorized into a content code ci and a style code si, where (ci, si) =
(Ec
i (xi)) = Ei(xi). Image-to-image translation is performed by swapping
encoder-decoder pairs, as illustrated in Fig. 2 (b). For example, to translate an
X2, we ﬁrst extract its content latent code c1 = Ec
1(x1) and
image x1 ∈ X1 to
(0, I).
randomly draw a style latent code s2 from the prior distribution q(s2)
We then use G2 to produce the ﬁnal output image x1
2 = G2(c1, s2). We note
that although the prior distribution is unimodal, the output image distribution
can be multimodal thanks to the nonlinearity of the decoder.

∼ N

→

Our loss function comprises a bidirectional reconstruction loss that ensures
the encoders and decoders are inverses, and an adversarial loss that matches the
distribution of translated images to the image distribution in the target domain.

Multimodal Unsupervised Image-to-Image Translation

5

Fig. 2. Model overview. Our image-to-image translation model consists of two auto-
encoders (denoted by red and blue arrows respectively), one for each domain. The latent
code of each auto-encoder is composed of a content code c and a style code s. We train
the model with adversarial objectives (dotted lines) that ensure the translated images
to be indistinguishable from real images in the target domain, as well as bidirectional
reconstruction objectives (dashed lines) that reconstruct both images and latent codes.

Bidirectional reconstruction loss. To learn pairs of encoder and decoder that
are inverses of each other, we use objective functions that encourage reconstruc-
latent directions:
image and latent
tion in both image

image

latent

→

→

→

→

– Image reconstruction. Given an image sampled from the data distribution,

we should be able to reconstruct it after encoding and decoding.

(1)

(2)

(3)

recon = Ex1
x1

G1(Ec
p(x1)[
||

∼

1(x1), Es

1(x1))

x1||1]

−

L

– Latent reconstruction. Given a latent code (style and content) sampled
from the latent distribution at translation time, we should be able to recon-
struct it after decoding and encoding.

recon = Ec1
c1
L
recon = Ec1
s2
L

∼

∼

p(c1),s2

Ec
q(s2)[
||
Es
q(s2)[
||
∼
(0, I), p(c1) is given by c1 = Ec

2(G2(c1, s2))
2(G2(c1, s2))

p(c1),s2

∼

−

c1||1]
s2||1]

where q(s2) is the prior

N

−
1(x1) and x1 ∼
s1
recon are deﬁned in a similar

p(x1).

We note the other loss terms
manner. We use

L

x2
recon,

c2
recon, and

L

L

L1 reconstruction loss as it encourages sharp output images.

The style reconstruction loss

si
recon is reminiscent of the latent reconstruction
loss used in the prior works [11, 31, 44, 57]. It has the eﬀect on encouraging diverse
ci
outputs given diﬀerent style codes. The content reconstruction loss
recon en-
courages the translated image to preserve semantic content of the input image.
Adversarial loss. We employ GANs to match the distribution of translated
images to the target data distribution. In other words, images generated by our

L

L

6

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

model should be indistinguishable from real images in the target domain.

x2
GAN = Ec1

L

p(c1),s2

∼

∼

−

q(s2)[log(1

D2(G2(c1, s2)))] + Ex2

p(x2)[log D2(x2)] (4)

∼

where D2 is a discriminator that tries to distinguish between translated images
x1
GAN are deﬁned similarly.
and real images in
Total loss. We jointly train the encoders, decoders, and discriminators to opti-
mize the ﬁnal objective, which is a weighted sum of the adversarial loss and the
bidirectional reconstruction loss terms.

X2. The discriminator D1 and loss

L

min
E1,E2,G1,G2

x1
recon +

λx(

L

L

max
D1,D2 L
x2
recon) + λc(

c1
recon +

L

L

(E1, E2, G1, G2, D1, D2) =

L
c2
recon) + λs(

x1
GAN +

L
s1
recon +

L

x2
GAN +

s2
recon)

L

(5)

where λx, λc, λs are weights that control the importance of reconstruction terms.

4 Theoretical Analysis

We now establish some theoretical properties of our framework. Speciﬁcally, we
show that minimizing the proposed loss function leads to 1) matching of latent
distributions during encoding and generation, 2) matching of two joint image
distributions induced by our framework, and 3) enforcing a weak form of cycle
consistency constraint. All the proofs are given in Appendix A.

First, we note that the total loss in Eq. (5) is minimized when the translated
distribution matches the data distribution and the encoder-decoder are inverses.

1
Proposition 1. Suppose there exists E∗1 , E∗2 , G∗1, G∗2 such that: 1) E∗1 = (G∗1)−
1) = p(x1). Then E∗1 , E∗2 ,
and E∗2 = (G∗2)−
(E1, E2, G1, G2, D1, D2) (Eq. (5)).
G∗1, G∗2 minimizes

→
(E1, E2, G1, G2) = max

2) = p(x2) and p(x2

1, and 2) p(x1

→

L

D1,D2 L

Latent Distribution Matching For image generation, existing works on com-
bining auto-encoders and GANs need to match the encoded latent distribution
with the latent distribution the decoder receives at generation time, using ei-
ther KLD loss [15, 40] or adversarial loss [17, 42] in the latent space. The auto-
encoder training would not help GAN training if the decoder received a very
diﬀerent latent distribution during generation. Although our loss function does
not contain terms that explicitly encourage the match of latent distributions,
it has the eﬀect of matching them implicitly.

Proposition 2. When optimality is reached, we have:

p(c1) = p(c2), p(s1) = q(s1), p(s2) = q(s2)

The above proposition shows that at optimality, the encoded style distributions
match their Gaussian priors. Also, the encoded content distribution matches the
distribution at generation time, which is just the encoded distribution from the
other domain. This suggests that the content space becomes domain-invariant.

Multimodal Unsupervised Image-to-Image Translation

7

→

→

1|

2|

x1) and p(x2

Joint Distribution Matching Our model learns two conditional distributions
x2), which, together with the data distributions, deﬁne
p(x1
two joint distributions p(x1, x1
1, x2). Since both of them are de-
signed to approximate the same underlying joint distribution p(x1, x2), it is de-
sirable that they are consistent with each other, i.e., p(x1, x1
1, x2).
Joint distribution matching provides an important constraint for unsuper-
vised image-to-image translation and is behind the success of many recent meth-
ods. Here, we show our model matches the joint distributions at optimality.

2) and p(x2

2) = p(x2

→

→

→

→

Proposition 3. When optimality is reached, we have p(x1, x1

2) = p(x2

1, x2).

→

→

Style-augmented Cycle Consistency Joint distribution matching can be
realized via cycle consistency constraint [8], assuming deterministic transla-
tion models and matched marginals [43, 67, 68]. However, we note that this
constraint is too strong for multimodal image translation. In fact, we prove
in Appendix A that the translation model will degenerate to a deterministic
function if cycle consistency is enforced. In the following proposition, we show
that our framework admits a weaker form of cycle consistency, termed as style-
augmented cycle consistency, between the image–style joint spaces, which is more
suited for multimodal image translation.

∈ H2. h1, h2 are
Proposition 4. Denote h1 = (x1, s2)
points in the joint spaces of image and style. Our model deﬁnes a deterministic
2(x1, s2) (cid:44)
mapping F1
H2 (and vice versa) by F1
2(h1) = F1
(G2(Ec
1.
1(x1)). When optimality is achieved, we have F1
2 = F −
2
→

∈ H1 and h2 = (x2, s1)

2 from
→
1(x1), s2), Es

H1 to

→

→

→

1

Intuitively, style-augmented cycle consistency implies that if we translate an
image to the target domain and translate it back using the original style, we
should obtain the original image. Style-augmented cycle consistency is implied
by the proposed bidirectional reconstruction loss, but explicitly enforcing it could
be useful for some datasets:

cc = Ex1
x1

p(x1),s2

∼

G1(Ec
q(s2)[
||

∼

L

2(G2(Ec

1(x1), s2)), Es

1(x1))

x1||1]

−

(6)

5 Experiments

5.1

Implementation Details

Fig. 3 shows the architecture of our auto-encoder. It consists of a content encoder,
a style encoder, and a joint decoder. More detailed information and hyperparam-
eters are given in Appendix B. We also provide an open-source implementation
in PyTorch [69] at https://github.com/nvlabs/MUNIT.

Content encoder. Our content encoder consists of several strided convolutional
layers to downsample the input and several residual blocks [70] to further process
it. All the convolutional layers are followed by Instance Normalization (IN) [71].

8

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

Fig. 3. Our auto-encoder architecture. The content encoder consists of several strided
convolutional layers followed by residual blocks. The style encoder contains several
strided convolutional layers followed by a global average pooling layer and a fully
connected layer. The decoder uses a MLP to produce a set of AdaIN [54] parameters
from the style code. The content code is then processed by residual blocks with AdaIN
layers, and ﬁnally decoded to the image space by upsampling and convolutional layers.

Style encoder. The style encoder includes several strided convolutional layers,
followed by a global average pooling layer and a fully connected (FC) layer. We
do not use IN layers in the style encoder, since IN removes the original feature
mean and variance that represent important style information [54].

Decoder. Our decoder reconstructs the input image from its content and style
code. It processes the content code by a set of residual blocks and ﬁnally pro-
duces the reconstructed image by several upsampling and convolutional layers.
Inspired by recent works that use aﬃne transformation parameters in normal-
ization layers to represent styles [54, 72–74], we equip the residual blocks with
Adaptive Instance Normalization (AdaIN) [54] layers whose parameters are dy-
namically generated by a multilayer perceptron (MLP) from the style code.

AdaIN(z, γ, β) = γ

(cid:18) z

(cid:19)

µ(z)

−
σ(z)

+ β

(7)

where z is the activation of the previous convolutional layer, µ and σ are channel-
wise mean and standard deviation, γ and β are parameters generated by the
MLP. Note that the aﬃne parameters are produced by a learned network, instead
of computed from statistics of a pretrained network as in Huang et al. [54].

Discriminator. We use the LSGAN objective proposed by Mao et al. [38]. We
employ multi-scale discriminators proposed by Wang et al. [20] to guide the
generators to produce both realistic details and correct global structure.

Domain-invariant perceptual loss. The perceptual loss, often computed as
a distance in the VGG [75] feature space between the output and the reference
image, has been shown to beneﬁt image-to-image translation when paired su-
pervision is available [13, 20]. In the unsupervised setting, however, we do not
have a reference image in the target domain. We propose a modiﬁed version
of perceptual loss that is more domain-invariant, so that we can use the input

Multimodal Unsupervised Image-to-Image Translation

9

image as the reference. Speciﬁcally, before computing the distance, we perform
Instance Normalization [71] (without aﬃne transformations) on the VGG fea-
tures in order to remove the original feature mean and variance, which con-
tains much domain-speciﬁc information [54, 76]. In Appendix C, we quantita-
tively show that Instance Normalization can indeed make the VGG features
more domain-invariant. We ﬁnd the domain-invariant perceptual loss acceler-
ates training on high-resolution (
512) datasets and thus employ it on
those datasets.

512

×

≥

5.2 Evaluation Metrics

Human Preference. To compare the realism and faithfulness of translation
outputs generated by diﬀerent methods, we perform human perceptual study
on Amazon Mechanical Turk (AMT). Similar to Wang et al. [20], the workers
are given an input image and two translation outputs from diﬀerent methods.
They are then given unlimited time to select which translation output looks
more accurate. For each comparison, we randomly generate 500 questions and
each question is answered by 5 diﬀerent workers.
LPIPS Distance. To measure translation diversity, we compute the average
LPIPS distance [77] between pairs of randomly-sampled translation outputs from
the same input as in Zhu et al. [11]. LPIPS is given by a weighted
L2 distance
between deep features of images. It has been demonstrated to correlate well with
human perceptual similarity [77]. Following Zhu et al. [11], we use 100 input im-
ages and sample 19 output pairs per input, which amounts to 1900 pairs in total.
We use the ImageNet-pretrained AlexNet [78] as the deep feature extractor.
(Conditional) Inception Score. The Inception Score (IS) [34] is a popu-
lar metric for image generation tasks. We propose a modiﬁed version called
Conditional Inception Score (CIS), which is more suited for evaluating multi-
modal image translation. When we know the number of modes in
X2 as well
as the ground truth mode each sample belongs to, we can train a classiﬁer
p(y2|
x2) to classify an image x2 into its mode y2. Conditioned on a single
2 should be mode-covering (thus
input image x1, the translation samples x1
2 should have high entropy) and each
p(y2|
2) should
individual sample should belong to a speciﬁc mode (thus p(y2|
have low entropy). Combing these two requirements we get:

x1) = (cid:82) p(y

x1) dx1

2)p(x1

x1
|

x1

2|

→

→

→

→

→

CIS = Ex1

p(x1)[Ex1→2

∼

p(x2→1

∼

|

x1

p(y2|
x1)[KL(p(y2|
||
x1) is replaced with the unconditional

x1))]]

(8)

2)

→

To compute the (unconditional) IS, p(y2|
class probability p(y2) = (cid:82)(cid:82) p(y
2)p(x1

x1
|
p(x1)[Ex1→2
∼

∼

→

p(x2→1

→

2|
x1)[KL(p(y2|

|

→
p(y2))]]

x1

2)

→

||

x1)p(x1) dx1 dx1

2.

IS = Ex1

(9)

To obtain a high CIS/IS score, a model needs to generate samples that are both
high-quality and diverse. While IS measures diversity of all output images, CIS
measures diversity of outputs conditioned on a single input image. A model that

10

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

deterministically generates a single output given an input image will receive
a zero CIS score, though it might still get a high score under IS. We use the
Inception-v3 [79] ﬁne-tuned on our speciﬁc datasets as the classiﬁer and estimate
Eq. (8) and Eq. (9) using 100 input images and 100 samples per input.

5.3 Baselines

UNIT [15]. The UNIT model consists of two VAE-GANs with a fully shared
latent space. The stochasticity of the translation comes from the Gaussian en-
coders as well as the dropout layers in the VAEs.
CycleGAN [8]. CycleGAN consists of two residual translation networks trained
with adversarial loss and cycle reconstruction loss. We use Dropout during both
training and testing to encourage diversity, as suggested in Isola et al. [6].
CycleGAN* [8] with noise. To test whether we can generate multimodal
outputs within the CycleGAN framework, we additionally inject noise vectors
to both translation networks. We use the U-net architecture [11] with noise added
to input, since we ﬁnd the noise vectors are ignored by the residual architecture
in CycleGAN [8]. Dropout is also utilized during both training and testing.
BicycleGAN [11]. BicycleGAN is the only existing image-to-image translation
model we are aware of that can generate continuous and multimodal output
distributions. However, it requires paired training data. We compare our model
with BicycleGAN when the dataset contains pair information.

5.4 Datasets

↔

↔

Edges
shoes/handbags. We use the datasets provided by Isola et al. [6],
Yu et al. [80], and Zhu et al. [81], which contain images of shoes and handbags
with edge maps generated by HED [82]. We train one model for edges
shoes
and another for edges
handbags without using paired information.
Animal image translation. We collect images from 3 categories/domains,
including house cats, big cats, and dogs. Each domain contains 4 modes which
are ﬁne-grained categories belonging to the same parent category. Note that the
modes of the images are not known during learning the translation model. We
learn a separate model for each pair of domains.
Street scene images. We experiment with two street scene translation tasks:
real. We perform translation between synthetic images in the
SYNTHIA dataset [83] and real-world images in the Cityscape dataset [84].
For the SYNTHIA dataset, we use the SYNTHIA-Seqs subset which contains
images in diﬀerent seasons, weather, and illumination conditions.

– Synthetic

↔

↔

– Summer

winter. We use the dataset from Liu et al. [15], which contains
summer and winter street images extracted from real-world driving videos.

↔

winter (HD). We collect a new high-resolution dataset
Yosemite summer
containing 3253 summer photos and 2385 winter photos of Yosemite. The images
are downsampled such that the shortest side of each image is 1024 pixels.

↔

Multimodal Unsupervised Image-to-Image Translation

11

Input

& GT

UNIT

CycleGAN

CycleGAN*

MUNIT

MUNIT

MUNIT

MUNIT

Bicycle-

with noise

w/o Lx

recon

w/o Lc

recon

w/o Ls

recon

(ours)

GAN

Fig. 4. Qualitative comparison on edges → shoes. The ﬁrst column shows the input and
ground truth output. Each following column shows 3 random outputs from a method.

Table 1. Quantitative evaluation on edges → shoes/handbags. The diversity score is
the average LPIPS distance [77]. The quality score is the human preference score, the
percentage a method is preferred over MUNIT. For both metrics, the higher the better.

edges → shoes

edges → handbags

Quality

Diversity

Quality

Diversity

0.011

0.010

0.016

0.213

0.172

0.070

0.109

0.104

0.293

37.3%

40.8%

45.1%

29.0%

9.3%

24.6%

50.0%

51.2%

N/A

0.023

0.012

0.011

0.191

0.185

0.139

0.175

0.140

0.371

UNIT [15]

CycleGAN [8]

CycleGAN* [8] with noise

MUNIT w/o Lx
MUNIT w/o Lc
MUNIT w/o Ls

recon

recon

recon

MUNIT
BicycleGAN [11]†

Real data

37.4%

36.0%

29.5%

6.0%

20.7%

28.6%

50.0%

56.7%

N/A

† Trained with paired supervision.

5.5 Results

x
recon,
L

First, we qualitatively compare MUNIT with the four baselines above, and three
s
variants of MUNIT that ablate
recon respectively. Fig. 4 shows
example results on edges
shoes. Both UNIT and CycleGAN (with or without
noise) fail to generate diverse outputs, despite the injected randomness. Without
s
x
recon,
recon or
L
the model suﬀers from partial mode collapse, with many outputs being almost
identical (e.g., the ﬁrst two rows). Our full model produces images that are both
diverse and realistic, similar to BicycleGAN but does not need supervision.

c
recon, the image quality of MUNIT is unsatisfactory. Without

c
recon,
L

→

L

L

L

The qualitative observations above are conﬁrmed by quantitative evaluations.
We use human preference to measure quality and LPIPS distance to evaluate

12

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

Input

GT

Sample translations

Input

GT

Sample translations

(a) edges ↔ shoes

(b) edges ↔ handbags

Fig. 5. Example results of (a) edges ↔ shoes and (b) edges ↔ handbags.

Input

Sample translations

Input

Sample translations

(a) house cats → big cats

(b) big cats → house cats

(c) house cats → dogs

(d) dogs → house cats

(e) big cats → dogs

(f) dogs → big cats

Fig. 6. Example results of animal image translation.

→

x
recon or
L

diversity, as described in Sec. 5.2. We conduct this experiment on the task of
shoes/handbags. As shown in Table 1, UNIT and CycleGAN produce
edges
c
very little diversity according to LPIPS distance. Removing
recon from
L
s
recon, both quality and
MUNIT leads to signiﬁcantly worse quality. Without
diversity deteriorate. The full model obtains quality and diversity comparable to
the fully supervised BicycleGAN, and signiﬁcantly better than all unsupervised
shoes/handbags.
baselines. In Fig. 5, we show more example results on edges
We proceed to perform experiments on the animal image translation dataset.
As shown in Fig. 6, our model successfully translate one kind of animal to an-
other. Given an input image, the translation outputs cover multiple modes, i.e.,
multiple ﬁne-grained animal categories in the target domain. The shape of an
animal has undergone signiﬁcant transformations, but the pose is overall pre-
served. As shown in Table 2, our model obtains the highest scores according to
both CIS and IS. In particular, the baselines all obtain a very low CIS, indicating

↔

L

Multimodal Unsupervised Image-to-Image Translation

13

Input

Sample translations

(a) Cityscape → SYNTHIA

(b) SYNTHIA → Cityscape

(c) summer → winter

(d) winter → summer

Fig. 7. Example results on street scene translations.

Input

Sample translations

(a) Yosemite summer → winter

(b) Yosemite winter → summer

Fig. 8. Example results on Yosemite summer ↔ winter (HD resolution).

their failure to generate multimodal outputs from a given input. As the IS has
been shown to correlate well to image quality [34], the higher IS of our method
suggests that it also generates images of high quality than baseline approaches.
Fig. 7 shows results on street scene datasets. Our model is able to generate
SYNTHIA images with diverse renderings (e.g., rainy, snowy, sunset) from a
given Cityscape image, and generate Cityscape images with diﬀerent lighting,

14

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

(a) edges → shoes

(b) big cats → house cats

Fig. 9. image translation. Each row has the same content while each column has the
same style. The color of the generated shoes and the appearance of the generated cats
can be speciﬁed by providing example style images.

Table 2. Quantitative evaluation on animal image translation. This dataset contains 3
domains. We perform bidirectional translation for each domain pair, resulting in 6
translation tasks. We use CIS and IS to measure the performance on each task. To
obtain a high CIS/IS score, a model needs to generate samples that are both high-
quality and diverse. While IS measures diversity of all output images, CIS measures
diversity of outputs conditioned on a single input image.

CycleGAN

CycleGAN*
with noise

UNIT

MUNIT

CIS

0.078
0.109
0.044
0.121
0.058
0.047

IS

0.795
0.887
0.895
0.921
0.762
0.620

CIS

0.034
0.124
0.070
0.137
0.019
0.022

IS

0.701
0.848
0.901
0.978
0.589
0.558

CIS

0.096
0.164
0.045
0.193
0.094
0.096

IS

0.666
0.817
0.827
0.982
0.910
0.754

CIS

0.911
0.956
1.231
1.035
1.205
0.897

IS

0.923
0.954
1.255
1.034
1.233
0.901

house cats → big cats
big cats → house cats
house cats → dogs
dogs → house cats
big cats → dogs
dogs → big cats

Average

0.076

0.813

0.068

0.762

0.115

0.826

1.039

1.050

shadow, and road textures from a given SYNTHIA image. Similarly, it gener-
ates winter images with diﬀerent amount of snow from a given summer image,
and summer images with diﬀerent amount of leafs from a given winter image.
Fig. 8 shows example results of summer
winter transfer on the high-resolution
↔
Yosemite dataset. Our algorithm generates output images with diﬀerent lighting.

Example-guided Image Translation. Instead of sampling the style code from
the prior, it is also possible to extract the style code from a reference image.

Multimodal Unsupervised Image-to-Image Translation

15

Input

Style

Ours

Gatys et al. Chen et al.

AdaIN

WCT

Fig. 10. Comparison with existing style transfer methods.

→

→
2 = G2(Ec

Speciﬁcally, given a content image x1 ∈ X1 and a style image x2 ∈ X2, our
2 that recombines the content of the former and
model produces an image x1
the style latter by x1
2(x2)). Examples are shown in Fig. 9.
Note that this is similar to classical style transfer algorithms [5, 51–56] that
transfer the style of one image to another. In Fig. 10, we compare out method
with classical style transfer algorithms including Gatys et al. [5], Chen et al. [85],
AdaIN [54], and WCT [55]. Our method produces results that are signiﬁcantly
more faithful and realistic, since our method learns the distribution of target
domain images using GANs.

1(x1), Es

We presented a framework for multimodal unsupervised image-to-image transla-
tion. Our model achieves quality and diversity superior to existing unsupervised
methods and comparable to state-of-the-art supervised approach. Future work
includes extending this framework to other domains, such as videos and text.

6 Conclusions

References

1. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for

image super-resolution. In: ECCV. (2014)

2. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV. (2016)

16

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

3. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-

coders: Feature learning by inpainting. In: CVPR. (2016)

4. Laﬀont, P.Y., Ren, Z., Tao, X., Qian, C., Hays, J.: Transient attributes for high-

level understanding and editing of outdoor scenes. TOG (2014)

5. Gatys, L.A., Ecker, A.S., Bethge, M.:
neural networks. In: CVPR. (2016)

Image style transfer using convolutional

6. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with con-

ditional adversarial networks. In: CVPR. (2017)

7. Yi, Z., Zhang, H., Tan, P., Gong, M.: Dualgan: Unsupervised dual learning for

image-to-image translation. In: ICCV. (2017)

8. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation

using cycle-consistent adversarial networks. In: ICCV. (2017)

9. Kim, T., Cha, M., Kim, H., Lee, J., Kim, J.: Learning to discover cross-domain

relations with generative adversarial networks. In: ICML. (2017)

10. Taigman, Y., Polyak, A., Wolf, L.: Unsupervised cross-domain image generation.

In: ICLR. (2017)

11. Zhu, J.Y., Zhang, R., Pathak, D., Darrell, T., Efros, A.A., Wang, O., Shechtman,

E.: Toward multimodal image-to-image translation. In: NIPS. (2017)

12. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: NIPS. (2016)
13. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded reﬁnement

networks. In: ICCV. (2017)

14. Liang, X., Zhang, H., Xing, E.P.: Generative semantic manipulation with contrast-

ing gan. arXiv preprint arXiv:1708.00315 (2017)

15. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation net-

works. In: NIPS. (2017)

16. Benaim, S., Wolf, L.: One-sided unsupervised domain mapping. In: NIPS. (2017)
17. Royer, A., Bousmalis, K., Gouws, S., Bertsch, F., Moressi, I., Cole, F., Murphy,
K.: Xgan: Unsupervised image-to-image translation for many-to-many mappings.
arXiv preprint arXiv:1711.05139 (2017)

18. Gan, Z., Chen, L., Wang, W., Pu, Y., Zhang, Y., Liu, H., Li, C., Carin, L.: Triangle

generative adversarial networks. In: NIPS. (2017) 5253–5262

19. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image translation. In: CVPR.
(2018)

20. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-
resolution image synthesis and semantic manipulation with conditional gans. In:
CVPR. (2018)

21. Shrivastava, A., Pﬁster, T., Tuzel, O., Susskind, J., Wang, W., Webb, R.: Learning
from simulated and unsupervised images through adversarial training. In: CVPR.
(2017)

22. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsupervised
In: CVPR.

pixel-level domain adaptation with generative adversarial networks.
(2017)

23. Wolf, L., Taigman, Y., Polyak, A.: Unsupervised creation of parameterized avatars.

In: ICCV. (2017)

24. TAU, T.G., Wolf, L., TAU, S.B.: The role of minimal complexity functions in

unsupervised learning of semantic mappings. In: ICLR. (2018)

25. Hoshen, Y., Wolf, L.: Identifying analogies across domains. In: ICLR. (2018)
26. Mathieu, M., Couprie, C., LeCun, Y.: Deep multi-scale video prediction beyond

mean square error. In: ICLR. (2016)

Multimodal Unsupervised Image-to-Image Translation

17

27. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)

28. Denton, E.L., Chintala, S., Fergus, R.: Deep generative image models using a

laplacian pyramid of adversarial networks. In: NIPS. (2015)

29. Wang, X., Gupta, A.: Generative image modeling using style and structure adver-

sarial networks. In: ECCV. (2016)

30. Yang, J., Kannan, A., Batra, D., Parikh, D.: Lr-gan: Layered recursive generative

adversarial networks for image generation. In: ICLR. (2017)

31. Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., Belongie, S.: Stacked generative

adversarial networks. In: CVPR. (2017)

32. Zhang, H., Xu, T., Li, H., Zhang, S., Huang, X., Wang, X., Metaxas, D.: Stack-
gan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. In: ICCV. (2017)

33. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for im-

proved quality, stability, and variation. In: ICLR. (2018)

34. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:

Improved techniques for training gans. In: NIPS. (2016)

35. Zhao, J., Mathieu, M., LeCun, Y.: Energy-based generative adversarial network.

In: ICLR. (2017)

In: ICML. (2017)

36. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks.

37. Berthelot, D., Schumm, T., Metz, L.: Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717 (2017)

38. Mao, X., Li, Q., Xie, H., Lau, Y.R., Wang, Z., Smolley, S.P.: Least squares gener-

ative adversarial networks. In: ICCV. (2017)

39. Tolstikhin, I., Bousquet, O., Gelly, S., Schoelkopf, B.: Wasserstein auto-encoders.

In: ICLR. (2018)

40. Larsen, A.B.L., Sønderby, S.K., Larochelle, H., Winther, O.: Autoencoding beyond

pixels using a learned similarity metric. In: ICML. (2016)

41. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics

based on deep networks. In: NIPS. (2016)

42. Rosca, M., Lakshminarayanan, B., Warde-Farley, D., Mohamed, S.: Variational
arXiv preprint

approaches for auto-encoding generative adversarial networks.
arXiv:1706.04987 (2017)

43. Li, C., Liu, H., Chen, C., Pu, Y., Chen, L., Henao, R., Carin, L.: Alice: Towards un-
derstanding adversarial learning for joint distribution matching. In: NIPS. (2017)
44. Srivastava, A., Valkoz, L., Russell, C., Gutmann, M.U., Sutton, C.: Veegan: Re-
ducing mode collapse in gans using implicit variational learning. In: NIPS. (2017)
45. Ghosh, A., Kulharia, V., Namboodiri, V., Torr, P.H., Dokania, P.K.: Multi-agent

diverse generative adversarial networks. arXiv preprint arXiv:1704.02906 (2017)

46. Bansal, A., Sheikh, Y., Ramanan, D.: Pixelnn: Example-based image synthesis.

In: ICLR. (2018)

47. Almahairi, A., Rajeswar, S., Sordoni, A., Bachman, P., Courville, A.: Augmented
cyclegan: Learning many-to-many mappings from unpaired data. arXiv preprint
arXiv:1802.10151 (2018)

48. Lee, H.Y., Tseng, H.Y., Huang, J.B., Singh, M.K., Yang, M.H.: Diverse image-to-

image translation via disentangled representation. In: ECCV. (2018)

49. Anoosheh, A., Agustsson, E., Timofte, R., Van Gool, L.: Combogan: Unrestrained
scalability for image domain translation. arXiv preprint arXiv:1712.06909 (2017)

18

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

50. Hui, L., Li, X., Chen, J., He, H., Yang, J., et al.: Unsupervised multi-
domain image translation with domain-speciﬁc encoders/decoders. arXiv preprint
arXiv:1712.02050 (2017)

51. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-

gies. In: SIGGRAPH. (2001)

52. Li, C., Wand, M.: Combining markov random ﬁelds and convolutional neural

networks for image synthesis. In: CVPR. (2016)

53. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer

and super-resolution. In: ECCV. (2016)

54. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance

normalization. In: ICCV. (2017)

55. Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer

via feature transforms. In: NIPS. (2017) 385–395

56. Li, Y., Liu, M.Y., Li, X., Yang, M.H., Kautz, J.: A closed-form solution to photo-

realistic image stylization. In: ECCV. (2018)

57. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Info-
gan: Interpretable representation learning by information maximizing generative
adversarial nets. In: NIPS. (2016)

58. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed,
S., Lerchner, A.: beta-vae: Learning basic visual concepts with a constrained vari-
ational framework. In: ICLR. (2017)

59. Tenenbaum, J.B., Freeman, W.T.: Separating style and content. In: NIPS. (1997)
60. Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., Erhan, D.: Domain

separation networks. In: NIPS. (2016)

61. Villegas, R., Yang, J., Hong, S., Lin, X., Lee, H.: Decomposing motion and content

for natural video sequence prediction. In: ICLR. (2017)

62. Mathieu, M.F., Zhao, J.J., Zhao, J., Ramesh, A., Sprechmann, P., LeCun, Y.:
Disentangling factors of variation in deep representation using adversarial training.
In: NIPS. (2016)

63. Denton, E.L., et al.: Unsupervised learning of disentangled representations from

video. In: NIPS. (2017)

64. Tulyakov, S., Liu, M.Y., Yang, X., Kautz, J.: Mocogan: Decomposing motion and

content for video generation. In: CVPR. (2018)

65. Donahue, C., Balsubramani, A., McAuley, J., Lipton, Z.C.: Semantically decom-
posing the latent spaces of generative adversarial networks. In: ICLR. (2018)
66. Shen, T., Lei, T., Barzilay, R., Jaakkola, T.: Style transfer from non-parallel text by
cross-alignment. In: Advances in Neural Information Processing Systems. (2017)
6833–6844

67. Donahue, J., Kr¨ahenb¨uhl, P., Darrell, T.: Adversarial feature learning. In: ICLR.

(2017)

68. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,

Courville, A.: Adversarially learned inference. In: ICLR. (2017)

69. Automatic diﬀerentiation in PyTorch. In: NIPS Autodiﬀ Workshop. (2017)
70. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR. (2016)

71. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: Maximizing
quality and diversity in feed-forward stylization and texture synthesis. In: CVPR.
(2017)

72. Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style.

In: ICLR. (2017)

Multimodal Unsupervised Image-to-Image Translation

19

73. Wang, H., Liang, X., Zhang, H., Yeung, D.Y., Xing, E.P.: Zm-net: Real-time zero-

shot image manipulation network. arXiv preprint arXiv:1703.07255 (2017)

74. Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V., Shlens, J.: Exploring the structure
of a real-time, arbitrary neural artistic stylization network. In: BMVC. (2017)
75. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR. (2015)

76. Li, Y., Wang, N., Shi, J., Liu, J., Hou, X.: Revisiting batch normalization for

practical domain adaptation. arXiv preprint arXiv:1603.04779 (2016)

77. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable

eﬀectiveness of deep features as a perceptual metric. In: CVPR. (2018)

78. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in neural information processing systems.
(2012)

79. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-

tion architecture for computer vision. In: CVPR. (2016)

80. Yu, A., Grauman, K.: Fine-grained visual comparisons with local learning.

In:

CVPR. (2014)

81. Zhu, J.Y., Kr¨ahenb¨uhl, P., Shechtman, E., Efros, A.A.: Generative visual manip-

ulation on the natural image manifold. In: ECCV. (2016)

82. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV. (2015)
83. Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia
dataset: A large collection of synthetic images for semantic segmentation of urban
scenes. In: CVPR. (2016)

84. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: CVPR. (2016)

85. Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv

preprint arXiv:1612.04337 (2016)

86. Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., Frey, B.: Adversarial autoen-

coders. arXiv preprint arXiv:1511.05644 (2015)

87. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR.

(2015)

A Proofs

Proof.

L

Proposition 1. Suppose there exists E∗1 , E∗2 , G∗1, G∗2 such that: 1) E∗1 = (G∗1)−
1) = p(x1). Then E∗1 , E∗2 ,
and E∗2 = (G∗2)−
(E1, E2, G1, G2, D1, D2) (Eq. (5)).
G∗1, G∗2 minimizes

→
(E1, E2, G1, G2) = max

2) = p(x2) and p(x2

1, and 2) p(x1

→

1

L

D1,D2 L

(E1, E2, G1, G2) = max

(E1, E2, G1, G2, D1, D2) = max

D1,D2 L
x1
recon +

L

L

+λx(

x2
recon) + λc(

c1
recon +

L

D1 L
c2
recon) + λs(

x1
GAN + max

x2
GAN

D2 L
s2
recon)

s1
recon +

L
JSD(p(x2)

L
p(x1
|

L
log 4

As shown in Goodfellow et al. [27], max

x2
GAN = 2
which has a global minimum when p(x2) = p(x1
2). Also, the bidirectional
reconstruction loss terms are minimized when Ei inverts Gi. Thus the total loss

D2 L

2))

−

→

→

·

20

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

is minimized under the two stated conditions. Below, we assume the networks
have suﬃcient capacity and the optimality is reachable as in prior works [27, 43].
That is E1 →
Proposition 2. When optimality is reached, we have:

G∗1, and G2 →

E∗2 , G1 →

E∗1 , E2 →

G∗2.

p(c1) = p(c2), p(s1) = q(s1), p(s2) = q(s2)

Proof. Let z1 denote the latent code, which is the concatenation of c1 and s1.
We denote the encoded latent distribution by pE(z1), which is deﬁned by z1 =
E1(x1) and x1 sampled from the data distribution p(x1). We denote the latent
distribution at generation time by p(z1), which is obtained by s1 ∼
q(s1) and
1) is deﬁned by
p(c2). The generated image distribution pG(x1) = p(x2
c1 ∼
x1 = G1(z1) and z1 sampled from p(z1). According to the change of variable
formula for probability density functions:

→

pG(x1) =

pE(z1) =

1

∂G−

1 (x1)
∂x1
1
1 (z1)
∂E−
∂z1

|

|

|

1

1 (x1))

p(G−
|

1

p(E−

1 (z1))

According to Proposition 1, we have pG(x1) = p(x1) and E1 = G−
optimality is reached. Thus:

1

1 when

pE(z1) =

1

1 (z1))

p(E−
|

1
pG(E−
|

1 (z1))

1

∂G−

1

1 (E−
1 (z1))
1
∂E−
1 (z1)
1
1 (G1(z1))
1
1 (z1)
∂E−

|

|

∂E−

∂E−

∂E−

1
1 (z1)
∂z1
1
1 (z1)
∂z1
1
1 (z1)
∂z1
1
1 (z1)
∂z1
1
1 (z1)
∂z1

∂E−

|

|

|

|

|

||

||

||

=

=

=

=

= p(z1)

∂E−

∂G−

∂z1
1
1 (z1) |
∂E−

1
p(G−

1 (G1(z1)))

1
p(G−

1
1 (E−

1 (z1)))

1

p(G−

1 (G1(z1)))

Similarly we have pE(z2) = p(z2), which together prove the original proposition.
x1
From another perspective, we note that
GAN coincide with the
objective of a WAE [39] or AAE [86] in the latent space, which pushes the
encoded latent distribution towards the latent distribution at generation time.

c2
recon,

s1
recon,

L

L

L

Proposition 3. When optimality is reached, we have p(x1, x1

2) = p(x2

1, x2).

→

→

Proof. For the ease of notation we denote the joint distribution p(x1, x1
by p1

2)
1(x1, x2). Both densities are zero when

2(x1, x2) and p(x2

1, x2) by p2

→

→

→

→

Multimodal Unsupervised Image-to-Image Translation

21

Ec

1(x1)

= Ec

2(x2). When Ec

1(x1) = Ec

2(x2), we also have:

p1

→

1(x1))p(x1)

Ec
2(x1, x2) = pG(x2|
∂Es
2(x2)
=
x2
|
Ec
= p(x2|
1(x1, x2)
= p2

q(Es
|

→

1(x1))pG(x1)

2(x2))p(x1)

Proposition 4. Denote h1 = (x1, s2)
∈ H2. h1, h2 are
points in the joint spaces of image and style. Our model deﬁnes a deterministic
2(x1, s2) (cid:44)
mapping F1
2(h1) = F1
H2 (and vice versa) by F1
1
(G2(Ec
1.
2 = F −
1(x1)). When optimality is achieved, we have F1
2
→

∈ H1 and h2 = (x2, s1)

2 from
→
1(x1), s2), Es

H1 to

→

→

→

Proof.

F2

1(F1

→

→

2(x1, s2)) (cid:44) F2

1(G2(Ec

1(x1), s2), Es

1(x1))

2(G2(Ec

1(x1), s2)), Es

1(x1)), Es

2(G2(Ec

1(x1), s2)))

→
(cid:44) (G1(Ec

= (G1(Ec
= (G1(Ec
= (x1, s2)

2(G2(Ec
1(x1), Es

1(x1), s2)), Es
1(x1)), s2)

1(x1)), s2)

(10)

(11)

(12)

(13)

(14)

→

2(F2

And we can prove F1
speciﬁc, (3) is implied by the style reconstruction loss
content reconstruction loss
loss
by the proposed bidirectional reconstruction loss.

1(x2, s1)) = (x2, s1) in a similar manner. To be more
s
recon, (4) is implied by the
L
c
recon, and (5) is implied by the image reconstruction
L
x
recon. As a result, style-augmented cycle consistency is implicitly implied
L

→

Proposition 5. (Cycle consistency implies deterministic translations).
Let p(x1) and p(x2) denote the data distributions. pG(x1|
x1) are
two conditionals deﬁned by generators. Given 1) matched marginals: p(x1) =
(cid:82) pG(x1|
x1)p(x1) dx1, and 2) cycle consistency:
Ex2
x∗2) for
x1)]= δ(x2 −
2 )[pG(x2|
x1) collapse to deterministic
every x∗1 ∈ X1, x∗2 ∈ X2, then pG(x1|
delta functions.

x2)p(x2) dx2, p(x1) = (cid:82) pG(x2|
x∗1), Ex1
x2) and pG(x2|

x2) and pG(x2|

x2)] = δ(x1 −

1 )[pG(x1|

pG(x2

pG(x1

x∗

x∗

∼

∼

|

|

Proof. Let x∗1 be a sample from p(x1). x(cid:48)2, x(cid:48)(cid:48)2 are two samples from pG(x2|
x∗1).
x(cid:48)2) = pG(x1|
Due to cycle consistency in
x(cid:48)(cid:48)2 ) =
X1 → X2 → X1, we have pG(x1|
δ(x1 −
x∗1). Also, x(cid:48)2 ∈ X2 and x(cid:48)(cid:48)2 ∈ X2 because of matched marginals. Due
x∗1) = δ(x2 −
to cycle consistency in
x(cid:48)2) =
X2 → X1 → X2, we have pG(x2|
x2).
x1) collapses to a delta function, similar for pG(x1|
x(cid:48)(cid:48)2 ). Thus pG(x2|
δ(x2 −
This proposition shows that cycle consistency [8] is a too strong constraint for
multimodal image translation.

22

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

B Training Details

B.1 Hyperparameters

We use the Adam optimizer [87] with β1 = 0.5, β2 = 0.999, and an initial
learning rate of 0.0001. The learning rate is decreased by half every 100, 000
iterations. In all experiments, we use a batch size of 1 and set the loss weights
to λx = 10, λc = 1, λs = 1. We use the domain-invariant perceptual loss with
weight 1 in the street scene and Yosemite datasets. We choose the dimension of
the style code to be 8 across all datasets. Random mirroring is applied during
training.

B.2 Network Architectures

×

×

7 convolutional block with k ﬁlters and stride 1. dk
4 convolutional block with k ﬁlters and stride 2. Rk denotes a

Let c7s1-k denote a 7
denotes a 4
residual block that contains two 3
×
nearest-neighbor upsampling layer followed by a 5
5 convolutional block with
k ﬁlters and stride 1. GAP denotes a global average pooling layer. fck denotes a
fully connected layer with k ﬁlters. We apply Instance Normalization (IN) [71]
to the content encoder and Adaptive Instance Normalization (AdaIN) [54] to
the decoder. We use ReLU activations in the generator and Leaky ReLU with
slope 0.2 in the discriminator. We use multi-scale discriminators with 3 scales.

3 convolutional blocks. uk denotes a 2

×

×

– Generator architecture

Content encoder: c7s1-64, d128, d256, R256, R256, R256, R256
Style encoder: c7s1-64, d128, d256, d256, d256, GAP, fc8
Decoder: R256, R256, R256, R256, u128, u64, c7s1-3

– Discriminator architecture: d64, d128, d256, d512

•
•
•

C Domain-invariant Perceptual Loss

↔

We conduct an experiment to verify if applying IN before computing the feature
distance can indeed make the distance more domain-invariant. We experiment
dataset used by Isola et al. [6] and originally proposed by Laﬀont et
on the day
al. [4]. We randomly sample two sets of image pairs: 1) images from the same
domain (both day or both night) but diﬀerent scenes, 2) images from the same
scene but diﬀerent domains. Fig. 11 shows examples from the two sets of image
pairs. We then compute the VGG feature (relu4 3) distance between each image
pair, with IN either applied or not before computing the distance. In Fig. 12,
we show histograms of the distance computed either with or without IN, and
from image pairs either of the same domain or the same scene. Without applying
IN before computing the distance, the distribution of feature distance is similar
for both sets of image pairs. With IN enabled, however, image pairs from the
same scene have clearly smaller distance, even they come from diﬀerent domains.
The results suggest that applying IN before computing the distance makes the
feature distance much more domain-invariant.

Multimodal Unsupervised Image-to-Image Translation

23

(a) Image pairs from the same scene.

(b) Image pairs from the same domain.

Fig. 11. Example image pairs for domain-invariant perceptual loss experiments.

Fig. 12. Histograms of the VGG feature distance. Left: distance computed without
using IN. Right: distance computed after IN. Blue: distance between image pairs from
the same domain (but diﬀerent scenes). Green: distance between image pairs from the
same scene (but diﬀerent domains).

8
1
0
2
 
g
u
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
2
v
2
3
7
4
0
.
4
0
8
1
:
v
i
X
r
a

Multimodal Unsupervised
Image-to-Image Translation

Xun Huang1, Ming-Yu Liu2, Serge Belongie1, Jan Kautz2

Cornell University1

NVIDIA2

Abstract. Unsupervised image-to-image translation is an important and
challenging problem in computer vision. Given an image in the source
domain, the goal is to learn the conditional distribution of correspond-
ing images in the target domain, without seeing any examples of corre-
sponding image pairs. While this conditional distribution is inherently
multimodal, existing approaches make an overly simpliﬁed assumption,
modeling it as a deterministic one-to-one mapping. As a result, they fail
to generate diverse outputs from a given source domain image. To address
this limitation, we propose a Multimodal Unsupervised Image-to-image
Translation (MUNIT) framework. We assume that the image represen-
tation can be decomposed into a content code that is domain-invariant,
and a style code that captures domain-speciﬁc properties. To translate
an image to another domain, we recombine its content code with a ran-
dom style code sampled from the style space of the target domain. We
analyze the proposed framework and establish several theoretical results.
Extensive experiments with comparisons to state-of-the-art approaches
further demonstrate the advantage of the proposed framework. Moreover,
our framework allows users to control the style of translation outputs by
providing an example style image. Code and pretrained models are avail-
able at https://github.com/nvlabs/MUNIT.

Keywords: GANs, image-to-image translation, style transfer

1 Introduction

Many problems in computer vision aim at translating images from one domain to
another, including super-resolution [1], colorization [2], inpainting [3], attribute
transfer [4], and style transfer [5]. This cross-domain image-to-image transla-
tion setting has therefore received signiﬁcant attention [6–25]. When the dataset
contains paired examples, this problem can be approached by a conditional gen-
erative model [6] or a simple regression model [13]. In this work, we focus on the
much more challenging setting when such supervision is unavailable.

In many scenarios, the cross-domain mapping of interest is multimodal. For
example, a winter scene could have many possible appearances during summer
due to weather, timing, lighting, etc. Unfortunately, existing techniques usually
assume a deterministic [8–10] or unimodal [15] mapping. As a result, they fail
to capture the full distribution of possible outputs. Even if the model is made
stochastic by injecting noise, the network usually learns to ignore it [6, 26].

2

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

Fig. 1. An illustration of our method. (a) Images in each domain Xi are encoded to a
shared content space C and a domain-speciﬁc style space Si. Each encoder has an inverse
decoder omitted from this ﬁgure. (b) To translate an image in X1 (e.g., a leopard) to
X2 (e.g., domestic cats), we recombine the content code of the input with a random
style code in the target style space. Diﬀerent style codes lead to diﬀerent outputs.

In this paper, we propose a principled framework for the Multimodal UNsu-
pervised Image-to-image Translation (MUNIT) problem. As shown in Fig. 1 (a),
our framework makes several assumptions. We ﬁrst assume that the latent space
of images can be decomposed into a content space and a style space. We further
assume that images in diﬀerent domains share a common content space but not
the style space. To translate an image to the target domain, we recombine its
content code with a random style code in the target style space (Fig. 1 (b)). The
content code encodes the information that should be preserved during transla-
tion, while the style code represents remaining variations that are not contained
in the input image. By sampling diﬀerent style codes, our model is able to pro-
duce diverse and multimodal outputs. Extensive experiments demonstrate the
eﬀectiveness of our method in modeling multimodal output distributions and
its superior image quality compared with state-of-the-art approaches. Moreover,
the decomposition of content and style spaces allows our framework to perform
example-guided image translation, in which the style of the translation outputs
are controlled by a user-provided example image in the target domain.

2 Related Works

Generative adversarial networks (GANs). The GAN framework [27] has
achieved impressive results in image generation. In GAN training, a generator is
trained to fool a discriminator which in turn tries to distinguish between gener-
ated samples and real samples. Various improvements to GANs have been pro-
posed, such as multi-stage generation [28–33], better training objectives [34–39],
and combination with auto-encoders [40–44]. In this work, we employ GANs to
align the distribution of translated images with real images in the target domain.
Image-to-image translation. Isola et al. [6] propose the ﬁrst uniﬁed frame-
work for image-to-image translation based on conditional GANs, which has been
extended to generating high-resolution images by Wang et al. [20]. Recent stud-
ies have also attempted to learn image translation without supervision. This

Multimodal Unsupervised Image-to-Image Translation

3

problem is inherently ill-posed and requires additional constraints. Some works
enforce the translation to preserve certain properties of the source domain data,
such as pixel values [21], pixel gradients [22], semantic features [10], class labels
[22], or pairwise sample distances [16]. Another popular constraint is the cycle
consistency loss [7–9]. It enforces that if we translate an image to the target do-
main and back, we should obtain the original image. In addition, Liu et al. [15]
propose the UNIT framework, which assumes a shared latent space such that
corresponding images in two domains are mapped to the same latent code.

A signiﬁcant limitation of most existing image-to-image translation meth-
ods is the lack of diversity in the translated outputs. To tackle this problem,
some works propose to simultaneously generate multiple outputs given the same
input and encourage them to be diﬀerent [13, 45, 46]. Still, these methods can
only generate a discrete number of outputs. Zhu et al. [11] propose a Bicycle-
GAN that can model continuous and multimodal distributions. However, all the
aforementioned methods require pair supervision, while our method does not. A
couple of concurrent works also recognize this limitation and propose extensions
of CycleGAN/UNIT for multimodal mapping [47]/[48].

Our problem has some connections with multi-domain image-to-image trans-
lation [19, 49, 50]. Speciﬁcally, when we know how many modes each domain has
and the mode each sample belongs to, it is possible to treat each mode as a sep-
arate domain and use multi-domain image-to-image translation techniques to
learn a mapping between each pair of modes, thus achieving multimodal trans-
lation. However, in general we do not assume such information is available. Also,
our stochastic model can represent continuous output distributions, while [19,
49, 50] still use a deterministic model for each pair of domains.

Style transfer. Style transfer aims at modifying the style of an image while
preserving its content, which is closely related to image-to-image translation.
Here, we make a distinction between example-guided style transfer, in which the
target style comes from a single example, and collection style transfer, in which
the target style is deﬁned by a collection of images. Classical style transfer ap-
proaches [5, 51–56] typically tackle the former problem, whereas image-to-image
translation methods have been demonstrated to perform well in the latter [8].
We will show that our model is able to address both problems, thanks to its
disentangled representation of content and style.

Learning disentangled representations. Our work draws inspiration from
recent works on disentangled representation learning. For example, InfoGAN [57]
and β-VAE [58] have been proposed to learn disentangled representations with-
out supervision. Some other works [59–66] focus on disentangling content from
style. Although it is diﬃcult to deﬁne content/style and diﬀerent works use dif-
ferent deﬁnitions, we refer to “content” as the underling spatial structure and
“style” as the rendering of the structure. In our setting, we have two domains
that share the same content distribution but have diﬀerent style distributions.

4

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

3 Multimodal Unsupervised Image-to-image Translation

3.1 Assumptions

Let x1 ∈ X1 and x2 ∈ X2 be images from two diﬀerent image domains. In the
unsupervised image-to-image translation setting, we are given samples drawn
from two marginal distributions p(x1) and p(x2), without access to the joint
distribution p(x1, x2). Our goal is to estimate the two conditionals p(x2|
x1)
x1) and
x2) with learned image-to-image translation models p(x1
and p(x1|
2|
X2 (similar
2 is a sample produced by translating x1 to
p(x2
→
for x2
x2) are complex and multimodal distri-
butions, in which case a deterministic translation model does not work well.

x2), where x1
1|
1). In general, p(x2|
→

x1) and p(x1|

→

→

∈ C

∈ S

∈ X

To tackle this problem, we make a partially shared latent space assumption.
i is generated from a content
Speciﬁcally, we assume that each image xi
that is shared by both domains, and a style latent code
latent code c
si
i that is speciﬁc to the individual domain. In other words, a pair of
corresponding images (x1, x2) from the joint distribution is generated by x1 =
G∗1(c, s1) and x2 = G∗2(c, s2), where c, s1, s2 are from some prior distributions
and G∗1, G∗2 are the underlying generators. We further assume that G∗1 and G∗2 are
1 and E∗2 =
deterministic functions and have their inverse encoders E∗1 = (G∗1)−
1. Our goal is to learn the underlying generator and encoder functions with
(G∗2)−
neural networks. Note that although the encoders and decoders are deterministic,
p(x2|
Our assumption is closely related to the shared latent space assumption pro-
posed in UNIT [15]. While UNIT assumes a fully shared latent space, we postu-
late that only part of the latent space (the content) can be shared across domains
whereas the other part (the style) is domain speciﬁc, which is a more reasonable
assumption when the cross-domain mapping is many-to-many.

x1) is a continuous distribution due to the dependency of s2.

3.2 Model

X

i (xi), Es

Fig. 2 shows an overview of our model and its learning process. Similar to Liu
et al. [15], our translation model consists of an encoder Ei and a decoder Gi for
each domain
i (i = 1, 2). As shown in Fig. 2 (a), the latent code of each auto-
encoder is factorized into a content code ci and a style code si, where (ci, si) =
(Ec
i (xi)) = Ei(xi). Image-to-image translation is performed by swapping
encoder-decoder pairs, as illustrated in Fig. 2 (b). For example, to translate an
X2, we ﬁrst extract its content latent code c1 = Ec
1(x1) and
image x1 ∈ X1 to
(0, I).
randomly draw a style latent code s2 from the prior distribution q(s2)
We then use G2 to produce the ﬁnal output image x1
2 = G2(c1, s2). We note
that although the prior distribution is unimodal, the output image distribution
can be multimodal thanks to the nonlinearity of the decoder.

∼ N

→

Our loss function comprises a bidirectional reconstruction loss that ensures
the encoders and decoders are inverses, and an adversarial loss that matches the
distribution of translated images to the image distribution in the target domain.

Multimodal Unsupervised Image-to-Image Translation

5

Fig. 2. Model overview. Our image-to-image translation model consists of two auto-
encoders (denoted by red and blue arrows respectively), one for each domain. The latent
code of each auto-encoder is composed of a content code c and a style code s. We train
the model with adversarial objectives (dotted lines) that ensure the translated images
to be indistinguishable from real images in the target domain, as well as bidirectional
reconstruction objectives (dashed lines) that reconstruct both images and latent codes.

Bidirectional reconstruction loss. To learn pairs of encoder and decoder that
are inverses of each other, we use objective functions that encourage reconstruc-
latent directions:
image and latent
tion in both image

image

latent

→

→

→

→

– Image reconstruction. Given an image sampled from the data distribution,

we should be able to reconstruct it after encoding and decoding.

(1)

(2)

(3)

recon = Ex1
x1

G1(Ec
p(x1)[
||

∼

1(x1), Es

1(x1))

x1||1]

−

L

– Latent reconstruction. Given a latent code (style and content) sampled
from the latent distribution at translation time, we should be able to recon-
struct it after decoding and encoding.

recon = Ec1
c1
L
recon = Ec1
s2
L

∼

∼

p(c1),s2

Ec
q(s2)[
||
Es
q(s2)[
||
∼
(0, I), p(c1) is given by c1 = Ec

2(G2(c1, s2))
2(G2(c1, s2))

p(c1),s2

∼

−

c1||1]
s2||1]

where q(s2) is the prior

N

−
1(x1) and x1 ∼
s1
recon are deﬁned in a similar

p(x1).

We note the other loss terms
manner. We use

L

x2
recon,

c2
recon, and

L

L

L1 reconstruction loss as it encourages sharp output images.

The style reconstruction loss

si
recon is reminiscent of the latent reconstruction
loss used in the prior works [11, 31, 44, 57]. It has the eﬀect on encouraging diverse
ci
outputs given diﬀerent style codes. The content reconstruction loss
recon en-
courages the translated image to preserve semantic content of the input image.
Adversarial loss. We employ GANs to match the distribution of translated
images to the target data distribution. In other words, images generated by our

L

L

6

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

model should be indistinguishable from real images in the target domain.

x2
GAN = Ec1

L

p(c1),s2

∼

∼

−

q(s2)[log(1

D2(G2(c1, s2)))] + Ex2

p(x2)[log D2(x2)] (4)

∼

where D2 is a discriminator that tries to distinguish between translated images
x1
GAN are deﬁned similarly.
and real images in
Total loss. We jointly train the encoders, decoders, and discriminators to opti-
mize the ﬁnal objective, which is a weighted sum of the adversarial loss and the
bidirectional reconstruction loss terms.

X2. The discriminator D1 and loss

L

min
E1,E2,G1,G2

x1
recon +

λx(

L

L

max
D1,D2 L
x2
recon) + λc(

c1
recon +

L

L

(E1, E2, G1, G2, D1, D2) =

L
c2
recon) + λs(

x1
GAN +

L
s1
recon +

L

x2
GAN +

s2
recon)

L

(5)

where λx, λc, λs are weights that control the importance of reconstruction terms.

4 Theoretical Analysis

We now establish some theoretical properties of our framework. Speciﬁcally, we
show that minimizing the proposed loss function leads to 1) matching of latent
distributions during encoding and generation, 2) matching of two joint image
distributions induced by our framework, and 3) enforcing a weak form of cycle
consistency constraint. All the proofs are given in Appendix A.

First, we note that the total loss in Eq. (5) is minimized when the translated
distribution matches the data distribution and the encoder-decoder are inverses.

1
Proposition 1. Suppose there exists E∗1 , E∗2 , G∗1, G∗2 such that: 1) E∗1 = (G∗1)−
1) = p(x1). Then E∗1 , E∗2 ,
and E∗2 = (G∗2)−
(E1, E2, G1, G2, D1, D2) (Eq. (5)).
G∗1, G∗2 minimizes

→
(E1, E2, G1, G2) = max

2) = p(x2) and p(x2

1, and 2) p(x1

→

L

D1,D2 L

Latent Distribution Matching For image generation, existing works on com-
bining auto-encoders and GANs need to match the encoded latent distribution
with the latent distribution the decoder receives at generation time, using ei-
ther KLD loss [15, 40] or adversarial loss [17, 42] in the latent space. The auto-
encoder training would not help GAN training if the decoder received a very
diﬀerent latent distribution during generation. Although our loss function does
not contain terms that explicitly encourage the match of latent distributions,
it has the eﬀect of matching them implicitly.

Proposition 2. When optimality is reached, we have:

p(c1) = p(c2), p(s1) = q(s1), p(s2) = q(s2)

The above proposition shows that at optimality, the encoded style distributions
match their Gaussian priors. Also, the encoded content distribution matches the
distribution at generation time, which is just the encoded distribution from the
other domain. This suggests that the content space becomes domain-invariant.

Multimodal Unsupervised Image-to-Image Translation

7

→

→

1|

2|

x1) and p(x2

Joint Distribution Matching Our model learns two conditional distributions
x2), which, together with the data distributions, deﬁne
p(x1
two joint distributions p(x1, x1
1, x2). Since both of them are de-
signed to approximate the same underlying joint distribution p(x1, x2), it is de-
sirable that they are consistent with each other, i.e., p(x1, x1
1, x2).
Joint distribution matching provides an important constraint for unsuper-
vised image-to-image translation and is behind the success of many recent meth-
ods. Here, we show our model matches the joint distributions at optimality.

2) and p(x2

2) = p(x2

→

→

→

→

Proposition 3. When optimality is reached, we have p(x1, x1

2) = p(x2

1, x2).

→

→

Style-augmented Cycle Consistency Joint distribution matching can be
realized via cycle consistency constraint [8], assuming deterministic transla-
tion models and matched marginals [43, 67, 68]. However, we note that this
constraint is too strong for multimodal image translation. In fact, we prove
in Appendix A that the translation model will degenerate to a deterministic
function if cycle consistency is enforced. In the following proposition, we show
that our framework admits a weaker form of cycle consistency, termed as style-
augmented cycle consistency, between the image–style joint spaces, which is more
suited for multimodal image translation.

∈ H2. h1, h2 are
Proposition 4. Denote h1 = (x1, s2)
points in the joint spaces of image and style. Our model deﬁnes a deterministic
2(x1, s2) (cid:44)
mapping F1
H2 (and vice versa) by F1
2(h1) = F1
(G2(Ec
1.
1(x1)). When optimality is achieved, we have F1
2 = F −
2
→

∈ H1 and h2 = (x2, s1)

2 from
→
1(x1), s2), Es

H1 to

→

→

→

1

Intuitively, style-augmented cycle consistency implies that if we translate an
image to the target domain and translate it back using the original style, we
should obtain the original image. Style-augmented cycle consistency is implied
by the proposed bidirectional reconstruction loss, but explicitly enforcing it could
be useful for some datasets:

cc = Ex1
x1

p(x1),s2

∼

G1(Ec
q(s2)[
||

∼

L

2(G2(Ec

1(x1), s2)), Es

1(x1))

x1||1]

−

(6)

5 Experiments

5.1

Implementation Details

Fig. 3 shows the architecture of our auto-encoder. It consists of a content encoder,
a style encoder, and a joint decoder. More detailed information and hyperparam-
eters are given in Appendix B. We also provide an open-source implementation
in PyTorch [69] at https://github.com/nvlabs/MUNIT.

Content encoder. Our content encoder consists of several strided convolutional
layers to downsample the input and several residual blocks [70] to further process
it. All the convolutional layers are followed by Instance Normalization (IN) [71].

8

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

Fig. 3. Our auto-encoder architecture. The content encoder consists of several strided
convolutional layers followed by residual blocks. The style encoder contains several
strided convolutional layers followed by a global average pooling layer and a fully
connected layer. The decoder uses a MLP to produce a set of AdaIN [54] parameters
from the style code. The content code is then processed by residual blocks with AdaIN
layers, and ﬁnally decoded to the image space by upsampling and convolutional layers.

Style encoder. The style encoder includes several strided convolutional layers,
followed by a global average pooling layer and a fully connected (FC) layer. We
do not use IN layers in the style encoder, since IN removes the original feature
mean and variance that represent important style information [54].

Decoder. Our decoder reconstructs the input image from its content and style
code. It processes the content code by a set of residual blocks and ﬁnally pro-
duces the reconstructed image by several upsampling and convolutional layers.
Inspired by recent works that use aﬃne transformation parameters in normal-
ization layers to represent styles [54, 72–74], we equip the residual blocks with
Adaptive Instance Normalization (AdaIN) [54] layers whose parameters are dy-
namically generated by a multilayer perceptron (MLP) from the style code.

AdaIN(z, γ, β) = γ

(cid:18) z

(cid:19)

µ(z)

−
σ(z)

+ β

(7)

where z is the activation of the previous convolutional layer, µ and σ are channel-
wise mean and standard deviation, γ and β are parameters generated by the
MLP. Note that the aﬃne parameters are produced by a learned network, instead
of computed from statistics of a pretrained network as in Huang et al. [54].

Discriminator. We use the LSGAN objective proposed by Mao et al. [38]. We
employ multi-scale discriminators proposed by Wang et al. [20] to guide the
generators to produce both realistic details and correct global structure.

Domain-invariant perceptual loss. The perceptual loss, often computed as
a distance in the VGG [75] feature space between the output and the reference
image, has been shown to beneﬁt image-to-image translation when paired su-
pervision is available [13, 20]. In the unsupervised setting, however, we do not
have a reference image in the target domain. We propose a modiﬁed version
of perceptual loss that is more domain-invariant, so that we can use the input

Multimodal Unsupervised Image-to-Image Translation

9

image as the reference. Speciﬁcally, before computing the distance, we perform
Instance Normalization [71] (without aﬃne transformations) on the VGG fea-
tures in order to remove the original feature mean and variance, which con-
tains much domain-speciﬁc information [54, 76]. In Appendix C, we quantita-
tively show that Instance Normalization can indeed make the VGG features
more domain-invariant. We ﬁnd the domain-invariant perceptual loss acceler-
ates training on high-resolution (
512) datasets and thus employ it on
those datasets.

512

≥

×

5.2 Evaluation Metrics

Human Preference. To compare the realism and faithfulness of translation
outputs generated by diﬀerent methods, we perform human perceptual study
on Amazon Mechanical Turk (AMT). Similar to Wang et al. [20], the workers
are given an input image and two translation outputs from diﬀerent methods.
They are then given unlimited time to select which translation output looks
more accurate. For each comparison, we randomly generate 500 questions and
each question is answered by 5 diﬀerent workers.
LPIPS Distance. To measure translation diversity, we compute the average
LPIPS distance [77] between pairs of randomly-sampled translation outputs from
the same input as in Zhu et al. [11]. LPIPS is given by a weighted
L2 distance
between deep features of images. It has been demonstrated to correlate well with
human perceptual similarity [77]. Following Zhu et al. [11], we use 100 input im-
ages and sample 19 output pairs per input, which amounts to 1900 pairs in total.
We use the ImageNet-pretrained AlexNet [78] as the deep feature extractor.
(Conditional) Inception Score. The Inception Score (IS) [34] is a popu-
lar metric for image generation tasks. We propose a modiﬁed version called
Conditional Inception Score (CIS), which is more suited for evaluating multi-
modal image translation. When we know the number of modes in
X2 as well
as the ground truth mode each sample belongs to, we can train a classiﬁer
p(y2|
x2) to classify an image x2 into its mode y2. Conditioned on a single
2 should be mode-covering (thus
input image x1, the translation samples x1
2 should have high entropy) and each
p(y2|
2) should
individual sample should belong to a speciﬁc mode (thus p(y2|
have low entropy). Combing these two requirements we get:

x1) = (cid:82) p(y

x1) dx1

2)p(x1

x1
|

x1

2|

→

→

→

→

→

CIS = Ex1

p(x1)[Ex1→2

∼

p(x2→1

∼

|

x1

p(y2|
x1)[KL(p(y2|
||
x1) is replaced with the unconditional

x1))]]

(8)

2)

→

To compute the (unconditional) IS, p(y2|
class probability p(y2) = (cid:82)(cid:82) p(y
2)p(x1

x1
|
p(x1)[Ex1→2
∼

∼

→

p(x2→1

→

2|
x1)[KL(p(y2|

|

→
p(y2))]]

x1

2)

→

||

x1)p(x1) dx1 dx1

2.

IS = Ex1

(9)

To obtain a high CIS/IS score, a model needs to generate samples that are both
high-quality and diverse. While IS measures diversity of all output images, CIS
measures diversity of outputs conditioned on a single input image. A model that

10

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

deterministically generates a single output given an input image will receive
a zero CIS score, though it might still get a high score under IS. We use the
Inception-v3 [79] ﬁne-tuned on our speciﬁc datasets as the classiﬁer and estimate
Eq. (8) and Eq. (9) using 100 input images and 100 samples per input.

5.3 Baselines

UNIT [15]. The UNIT model consists of two VAE-GANs with a fully shared
latent space. The stochasticity of the translation comes from the Gaussian en-
coders as well as the dropout layers in the VAEs.
CycleGAN [8]. CycleGAN consists of two residual translation networks trained
with adversarial loss and cycle reconstruction loss. We use Dropout during both
training and testing to encourage diversity, as suggested in Isola et al. [6].
CycleGAN* [8] with noise. To test whether we can generate multimodal
outputs within the CycleGAN framework, we additionally inject noise vectors
to both translation networks. We use the U-net architecture [11] with noise added
to input, since we ﬁnd the noise vectors are ignored by the residual architecture
in CycleGAN [8]. Dropout is also utilized during both training and testing.
BicycleGAN [11]. BicycleGAN is the only existing image-to-image translation
model we are aware of that can generate continuous and multimodal output
distributions. However, it requires paired training data. We compare our model
with BicycleGAN when the dataset contains pair information.

5.4 Datasets

↔

↔

Edges
shoes/handbags. We use the datasets provided by Isola et al. [6],
Yu et al. [80], and Zhu et al. [81], which contain images of shoes and handbags
with edge maps generated by HED [82]. We train one model for edges
shoes
and another for edges
handbags without using paired information.
Animal image translation. We collect images from 3 categories/domains,
including house cats, big cats, and dogs. Each domain contains 4 modes which
are ﬁne-grained categories belonging to the same parent category. Note that the
modes of the images are not known during learning the translation model. We
learn a separate model for each pair of domains.
Street scene images. We experiment with two street scene translation tasks:
real. We perform translation between synthetic images in the
SYNTHIA dataset [83] and real-world images in the Cityscape dataset [84].
For the SYNTHIA dataset, we use the SYNTHIA-Seqs subset which contains
images in diﬀerent seasons, weather, and illumination conditions.

– Synthetic

↔

↔

– Summer

winter. We use the dataset from Liu et al. [15], which contains
summer and winter street images extracted from real-world driving videos.

↔

winter (HD). We collect a new high-resolution dataset
Yosemite summer
containing 3253 summer photos and 2385 winter photos of Yosemite. The images
are downsampled such that the shortest side of each image is 1024 pixels.

↔

Multimodal Unsupervised Image-to-Image Translation

11

Input

& GT

UNIT

CycleGAN

CycleGAN*

MUNIT

MUNIT

MUNIT

MUNIT

Bicycle-

with noise

w/o Lx

recon

w/o Lc

recon

w/o Ls

recon

(ours)

GAN

Fig. 4. Qualitative comparison on edges → shoes. The ﬁrst column shows the input and
ground truth output. Each following column shows 3 random outputs from a method.

Table 1. Quantitative evaluation on edges → shoes/handbags. The diversity score is
the average LPIPS distance [77]. The quality score is the human preference score, the
percentage a method is preferred over MUNIT. For both metrics, the higher the better.

edges → shoes

edges → handbags

Quality

Diversity

Quality

Diversity

0.011

0.010

0.016

0.213

0.172

0.070

0.109

0.104

0.293

37.3%

40.8%

45.1%

29.0%

9.3%

24.6%

50.0%

51.2%

N/A

0.023

0.012

0.011

0.191

0.185

0.139

0.175

0.140

0.371

UNIT [15]

CycleGAN [8]

CycleGAN* [8] with noise

MUNIT w/o Lx
MUNIT w/o Lc
MUNIT w/o Ls

recon

recon

recon

MUNIT
BicycleGAN [11]†

Real data

37.4%

36.0%

29.5%

6.0%

20.7%

28.6%

50.0%

56.7%

N/A

† Trained with paired supervision.

5.5 Results

x
recon,
L

First, we qualitatively compare MUNIT with the four baselines above, and three
s
variants of MUNIT that ablate
recon respectively. Fig. 4 shows
example results on edges
shoes. Both UNIT and CycleGAN (with or without
noise) fail to generate diverse outputs, despite the injected randomness. Without
s
x
recon,
recon or
L
the model suﬀers from partial mode collapse, with many outputs being almost
identical (e.g., the ﬁrst two rows). Our full model produces images that are both
diverse and realistic, similar to BicycleGAN but does not need supervision.

c
recon, the image quality of MUNIT is unsatisfactory. Without

c
recon,
L

→

L

L

L

The qualitative observations above are conﬁrmed by quantitative evaluations.
We use human preference to measure quality and LPIPS distance to evaluate

12

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

Input

GT

Sample translations

Input

GT

Sample translations

(a) edges ↔ shoes

(b) edges ↔ handbags

Fig. 5. Example results of (a) edges ↔ shoes and (b) edges ↔ handbags.

Input

Sample translations

Input

Sample translations

(a) house cats → big cats

(b) big cats → house cats

(c) house cats → dogs

(d) dogs → house cats

(e) big cats → dogs

(f) dogs → big cats

Fig. 6. Example results of animal image translation.

→

x
recon or
L

diversity, as described in Sec. 5.2. We conduct this experiment on the task of
shoes/handbags. As shown in Table 1, UNIT and CycleGAN produce
edges
c
very little diversity according to LPIPS distance. Removing
recon from
L
s
recon, both quality and
MUNIT leads to signiﬁcantly worse quality. Without
diversity deteriorate. The full model obtains quality and diversity comparable to
the fully supervised BicycleGAN, and signiﬁcantly better than all unsupervised
shoes/handbags.
baselines. In Fig. 5, we show more example results on edges
We proceed to perform experiments on the animal image translation dataset.
As shown in Fig. 6, our model successfully translate one kind of animal to an-
other. Given an input image, the translation outputs cover multiple modes, i.e.,
multiple ﬁne-grained animal categories in the target domain. The shape of an
animal has undergone signiﬁcant transformations, but the pose is overall pre-
served. As shown in Table 2, our model obtains the highest scores according to
both CIS and IS. In particular, the baselines all obtain a very low CIS, indicating

↔

L

Multimodal Unsupervised Image-to-Image Translation

13

Input

Sample translations

(a) Cityscape → SYNTHIA

(b) SYNTHIA → Cityscape

(c) summer → winter

(d) winter → summer

Fig. 7. Example results on street scene translations.

Input

Sample translations

(a) Yosemite summer → winter

(b) Yosemite winter → summer

Fig. 8. Example results on Yosemite summer ↔ winter (HD resolution).

their failure to generate multimodal outputs from a given input. As the IS has
been shown to correlate well to image quality [34], the higher IS of our method
suggests that it also generates images of high quality than baseline approaches.
Fig. 7 shows results on street scene datasets. Our model is able to generate
SYNTHIA images with diverse renderings (e.g., rainy, snowy, sunset) from a
given Cityscape image, and generate Cityscape images with diﬀerent lighting,

14

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

(a) edges → shoes

(b) big cats → house cats

Fig. 9. image translation. Each row has the same content while each column has the
same style. The color of the generated shoes and the appearance of the generated cats
can be speciﬁed by providing example style images.

Table 2. Quantitative evaluation on animal image translation. This dataset contains 3
domains. We perform bidirectional translation for each domain pair, resulting in 6
translation tasks. We use CIS and IS to measure the performance on each task. To
obtain a high CIS/IS score, a model needs to generate samples that are both high-
quality and diverse. While IS measures diversity of all output images, CIS measures
diversity of outputs conditioned on a single input image.

CycleGAN

CycleGAN*
with noise

UNIT

MUNIT

CIS

0.078
0.109
0.044
0.121
0.058
0.047

IS

0.795
0.887
0.895
0.921
0.762
0.620

CIS

0.034
0.124
0.070
0.137
0.019
0.022

IS

0.701
0.848
0.901
0.978
0.589
0.558

CIS

0.096
0.164
0.045
0.193
0.094
0.096

IS

0.666
0.817
0.827
0.982
0.910
0.754

CIS

0.911
0.956
1.231
1.035
1.205
0.897

IS

0.923
0.954
1.255
1.034
1.233
0.901

house cats → big cats
big cats → house cats
house cats → dogs
dogs → house cats
big cats → dogs
dogs → big cats

Average

0.076

0.813

0.068

0.762

0.115

0.826

1.039

1.050

shadow, and road textures from a given SYNTHIA image. Similarly, it gener-
ates winter images with diﬀerent amount of snow from a given summer image,
and summer images with diﬀerent amount of leafs from a given winter image.
Fig. 8 shows example results of summer
winter transfer on the high-resolution
↔
Yosemite dataset. Our algorithm generates output images with diﬀerent lighting.

Example-guided Image Translation. Instead of sampling the style code from
the prior, it is also possible to extract the style code from a reference image.

Multimodal Unsupervised Image-to-Image Translation

15

Input

Style

Ours

Gatys et al. Chen et al.

AdaIN

WCT

Fig. 10. Comparison with existing style transfer methods.

→

→
2 = G2(Ec

Speciﬁcally, given a content image x1 ∈ X1 and a style image x2 ∈ X2, our
2 that recombines the content of the former and
model produces an image x1
the style latter by x1
2(x2)). Examples are shown in Fig. 9.
Note that this is similar to classical style transfer algorithms [5, 51–56] that
transfer the style of one image to another. In Fig. 10, we compare out method
with classical style transfer algorithms including Gatys et al. [5], Chen et al. [85],
AdaIN [54], and WCT [55]. Our method produces results that are signiﬁcantly
more faithful and realistic, since our method learns the distribution of target
domain images using GANs.

1(x1), Es

We presented a framework for multimodal unsupervised image-to-image transla-
tion. Our model achieves quality and diversity superior to existing unsupervised
methods and comparable to state-of-the-art supervised approach. Future work
includes extending this framework to other domains, such as videos and text.

6 Conclusions

References

1. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for

image super-resolution. In: ECCV. (2014)

2. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV. (2016)

16

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

3. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-

coders: Feature learning by inpainting. In: CVPR. (2016)

4. Laﬀont, P.Y., Ren, Z., Tao, X., Qian, C., Hays, J.: Transient attributes for high-

level understanding and editing of outdoor scenes. TOG (2014)

5. Gatys, L.A., Ecker, A.S., Bethge, M.:
neural networks. In: CVPR. (2016)

Image style transfer using convolutional

6. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with con-

ditional adversarial networks. In: CVPR. (2017)

7. Yi, Z., Zhang, H., Tan, P., Gong, M.: Dualgan: Unsupervised dual learning for

image-to-image translation. In: ICCV. (2017)

8. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation

using cycle-consistent adversarial networks. In: ICCV. (2017)

9. Kim, T., Cha, M., Kim, H., Lee, J., Kim, J.: Learning to discover cross-domain

relations with generative adversarial networks. In: ICML. (2017)

10. Taigman, Y., Polyak, A., Wolf, L.: Unsupervised cross-domain image generation.

In: ICLR. (2017)

11. Zhu, J.Y., Zhang, R., Pathak, D., Darrell, T., Efros, A.A., Wang, O., Shechtman,

E.: Toward multimodal image-to-image translation. In: NIPS. (2017)

12. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: NIPS. (2016)
13. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded reﬁnement

networks. In: ICCV. (2017)

14. Liang, X., Zhang, H., Xing, E.P.: Generative semantic manipulation with contrast-

ing gan. arXiv preprint arXiv:1708.00315 (2017)

15. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation net-

works. In: NIPS. (2017)

16. Benaim, S., Wolf, L.: One-sided unsupervised domain mapping. In: NIPS. (2017)
17. Royer, A., Bousmalis, K., Gouws, S., Bertsch, F., Moressi, I., Cole, F., Murphy,
K.: Xgan: Unsupervised image-to-image translation for many-to-many mappings.
arXiv preprint arXiv:1711.05139 (2017)

18. Gan, Z., Chen, L., Wang, W., Pu, Y., Zhang, Y., Liu, H., Li, C., Carin, L.: Triangle

generative adversarial networks. In: NIPS. (2017) 5253–5262

19. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image translation. In: CVPR.
(2018)

20. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-
resolution image synthesis and semantic manipulation with conditional gans. In:
CVPR. (2018)

21. Shrivastava, A., Pﬁster, T., Tuzel, O., Susskind, J., Wang, W., Webb, R.: Learning
from simulated and unsupervised images through adversarial training. In: CVPR.
(2017)

22. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsupervised
In: CVPR.

pixel-level domain adaptation with generative adversarial networks.
(2017)

23. Wolf, L., Taigman, Y., Polyak, A.: Unsupervised creation of parameterized avatars.

In: ICCV. (2017)

24. TAU, T.G., Wolf, L., TAU, S.B.: The role of minimal complexity functions in

unsupervised learning of semantic mappings. In: ICLR. (2018)

25. Hoshen, Y., Wolf, L.: Identifying analogies across domains. In: ICLR. (2018)
26. Mathieu, M., Couprie, C., LeCun, Y.: Deep multi-scale video prediction beyond

mean square error. In: ICLR. (2016)

Multimodal Unsupervised Image-to-Image Translation

17

27. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)

28. Denton, E.L., Chintala, S., Fergus, R.: Deep generative image models using a

laplacian pyramid of adversarial networks. In: NIPS. (2015)

29. Wang, X., Gupta, A.: Generative image modeling using style and structure adver-

sarial networks. In: ECCV. (2016)

30. Yang, J., Kannan, A., Batra, D., Parikh, D.: Lr-gan: Layered recursive generative

adversarial networks for image generation. In: ICLR. (2017)

31. Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., Belongie, S.: Stacked generative

adversarial networks. In: CVPR. (2017)

32. Zhang, H., Xu, T., Li, H., Zhang, S., Huang, X., Wang, X., Metaxas, D.: Stack-
gan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. In: ICCV. (2017)

33. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for im-

proved quality, stability, and variation. In: ICLR. (2018)

34. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:

Improved techniques for training gans. In: NIPS. (2016)

35. Zhao, J., Mathieu, M., LeCun, Y.: Energy-based generative adversarial network.

In: ICLR. (2017)

In: ICML. (2017)

36. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks.

37. Berthelot, D., Schumm, T., Metz, L.: Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717 (2017)

38. Mao, X., Li, Q., Xie, H., Lau, Y.R., Wang, Z., Smolley, S.P.: Least squares gener-

ative adversarial networks. In: ICCV. (2017)

39. Tolstikhin, I., Bousquet, O., Gelly, S., Schoelkopf, B.: Wasserstein auto-encoders.

In: ICLR. (2018)

40. Larsen, A.B.L., Sønderby, S.K., Larochelle, H., Winther, O.: Autoencoding beyond

pixels using a learned similarity metric. In: ICML. (2016)

41. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics

based on deep networks. In: NIPS. (2016)

42. Rosca, M., Lakshminarayanan, B., Warde-Farley, D., Mohamed, S.: Variational
arXiv preprint

approaches for auto-encoding generative adversarial networks.
arXiv:1706.04987 (2017)

43. Li, C., Liu, H., Chen, C., Pu, Y., Chen, L., Henao, R., Carin, L.: Alice: Towards un-
derstanding adversarial learning for joint distribution matching. In: NIPS. (2017)
44. Srivastava, A., Valkoz, L., Russell, C., Gutmann, M.U., Sutton, C.: Veegan: Re-
ducing mode collapse in gans using implicit variational learning. In: NIPS. (2017)
45. Ghosh, A., Kulharia, V., Namboodiri, V., Torr, P.H., Dokania, P.K.: Multi-agent

diverse generative adversarial networks. arXiv preprint arXiv:1704.02906 (2017)

46. Bansal, A., Sheikh, Y., Ramanan, D.: Pixelnn: Example-based image synthesis.

In: ICLR. (2018)

47. Almahairi, A., Rajeswar, S., Sordoni, A., Bachman, P., Courville, A.: Augmented
cyclegan: Learning many-to-many mappings from unpaired data. arXiv preprint
arXiv:1802.10151 (2018)

48. Lee, H.Y., Tseng, H.Y., Huang, J.B., Singh, M.K., Yang, M.H.: Diverse image-to-

image translation via disentangled representation. In: ECCV. (2018)

49. Anoosheh, A., Agustsson, E., Timofte, R., Van Gool, L.: Combogan: Unrestrained
scalability for image domain translation. arXiv preprint arXiv:1712.06909 (2017)

18

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

50. Hui, L., Li, X., Chen, J., He, H., Yang, J., et al.: Unsupervised multi-
domain image translation with domain-speciﬁc encoders/decoders. arXiv preprint
arXiv:1712.02050 (2017)

51. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-

gies. In: SIGGRAPH. (2001)

52. Li, C., Wand, M.: Combining markov random ﬁelds and convolutional neural

networks for image synthesis. In: CVPR. (2016)

53. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer

and super-resolution. In: ECCV. (2016)

54. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance

normalization. In: ICCV. (2017)

55. Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Universal style transfer

via feature transforms. In: NIPS. (2017) 385–395

56. Li, Y., Liu, M.Y., Li, X., Yang, M.H., Kautz, J.: A closed-form solution to photo-

realistic image stylization. In: ECCV. (2018)

57. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Info-
gan: Interpretable representation learning by information maximizing generative
adversarial nets. In: NIPS. (2016)

58. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed,
S., Lerchner, A.: beta-vae: Learning basic visual concepts with a constrained vari-
ational framework. In: ICLR. (2017)

59. Tenenbaum, J.B., Freeman, W.T.: Separating style and content. In: NIPS. (1997)
60. Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., Erhan, D.: Domain

separation networks. In: NIPS. (2016)

61. Villegas, R., Yang, J., Hong, S., Lin, X., Lee, H.: Decomposing motion and content

for natural video sequence prediction. In: ICLR. (2017)

62. Mathieu, M.F., Zhao, J.J., Zhao, J., Ramesh, A., Sprechmann, P., LeCun, Y.:
Disentangling factors of variation in deep representation using adversarial training.
In: NIPS. (2016)

63. Denton, E.L., et al.: Unsupervised learning of disentangled representations from

video. In: NIPS. (2017)

64. Tulyakov, S., Liu, M.Y., Yang, X., Kautz, J.: Mocogan: Decomposing motion and

content for video generation. In: CVPR. (2018)

65. Donahue, C., Balsubramani, A., McAuley, J., Lipton, Z.C.: Semantically decom-
posing the latent spaces of generative adversarial networks. In: ICLR. (2018)
66. Shen, T., Lei, T., Barzilay, R., Jaakkola, T.: Style transfer from non-parallel text by
cross-alignment. In: Advances in Neural Information Processing Systems. (2017)
6833–6844

67. Donahue, J., Kr¨ahenb¨uhl, P., Darrell, T.: Adversarial feature learning. In: ICLR.

(2017)

68. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,

Courville, A.: Adversarially learned inference. In: ICLR. (2017)

69. Automatic diﬀerentiation in PyTorch. In: NIPS Autodiﬀ Workshop. (2017)
70. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR. (2016)

71. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: Maximizing
quality and diversity in feed-forward stylization and texture synthesis. In: CVPR.
(2017)

72. Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style.

In: ICLR. (2017)

Multimodal Unsupervised Image-to-Image Translation

19

73. Wang, H., Liang, X., Zhang, H., Yeung, D.Y., Xing, E.P.: Zm-net: Real-time zero-

shot image manipulation network. arXiv preprint arXiv:1703.07255 (2017)

74. Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V., Shlens, J.: Exploring the structure
of a real-time, arbitrary neural artistic stylization network. In: BMVC. (2017)
75. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR. (2015)

76. Li, Y., Wang, N., Shi, J., Liu, J., Hou, X.: Revisiting batch normalization for

practical domain adaptation. arXiv preprint arXiv:1603.04779 (2016)

77. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable

eﬀectiveness of deep features as a perceptual metric. In: CVPR. (2018)

78. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in neural information processing systems.
(2012)

79. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-

tion architecture for computer vision. In: CVPR. (2016)

80. Yu, A., Grauman, K.: Fine-grained visual comparisons with local learning.

In:

CVPR. (2014)

81. Zhu, J.Y., Kr¨ahenb¨uhl, P., Shechtman, E., Efros, A.A.: Generative visual manip-

ulation on the natural image manifold. In: ECCV. (2016)

82. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV. (2015)
83. Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia
dataset: A large collection of synthetic images for semantic segmentation of urban
scenes. In: CVPR. (2016)

84. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: CVPR. (2016)

85. Chen, T.Q., Schmidt, M.: Fast patch-based style transfer of arbitrary style. arXiv

preprint arXiv:1612.04337 (2016)

86. Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., Frey, B.: Adversarial autoen-

coders. arXiv preprint arXiv:1511.05644 (2015)

87. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR.

(2015)

A Proofs

Proof.

L

Proposition 1. Suppose there exists E∗1 , E∗2 , G∗1, G∗2 such that: 1) E∗1 = (G∗1)−
1) = p(x1). Then E∗1 , E∗2 ,
and E∗2 = (G∗2)−
(E1, E2, G1, G2, D1, D2) (Eq. (5)).
G∗1, G∗2 minimizes

→
(E1, E2, G1, G2) = max

2) = p(x2) and p(x2

1, and 2) p(x1

→

1

L

D1,D2 L

(E1, E2, G1, G2) = max

(E1, E2, G1, G2, D1, D2) = max

D1,D2 L
x1
recon +

L

L

+λx(

x2
recon) + λc(

c1
recon +

L

D1 L
c2
recon) + λs(

x1
GAN + max

x2
GAN

D2 L
s2
recon)

s1
recon +

L
JSD(p(x2)

L
p(x1
|

L
log 4

As shown in Goodfellow et al. [27], max

x2
GAN = 2
which has a global minimum when p(x2) = p(x1
2). Also, the bidirectional
reconstruction loss terms are minimized when Ei inverts Gi. Thus the total loss

D2 L

2))

−

→

→

·

20

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

is minimized under the two stated conditions. Below, we assume the networks
have suﬃcient capacity and the optimality is reachable as in prior works [27, 43].
That is E1 →
Proposition 2. When optimality is reached, we have:

G∗1, and G2 →

E∗2 , G1 →

E∗1 , E2 →

G∗2.

p(c1) = p(c2), p(s1) = q(s1), p(s2) = q(s2)

Proof. Let z1 denote the latent code, which is the concatenation of c1 and s1.
We denote the encoded latent distribution by pE(z1), which is deﬁned by z1 =
E1(x1) and x1 sampled from the data distribution p(x1). We denote the latent
distribution at generation time by p(z1), which is obtained by s1 ∼
q(s1) and
1) is deﬁned by
p(c2). The generated image distribution pG(x1) = p(x2
c1 ∼
x1 = G1(z1) and z1 sampled from p(z1). According to the change of variable
formula for probability density functions:

→

pG(x1) =

pE(z1) =

1

∂G−

1 (x1)
∂x1
1
1 (z1)
∂E−
∂z1

|

|

|

1

1 (x1))

p(G−
|

1

p(E−

1 (z1))

According to Proposition 1, we have pG(x1) = p(x1) and E1 = G−
optimality is reached. Thus:

1

1 when

pE(z1) =

1

1 (z1))

p(E−
|

1
pG(E−
|

1 (z1))

1

∂G−

1

1 (E−
1 (z1))
1
∂E−
1 (z1)
1
1 (G1(z1))
1
1 (z1)
∂E−

|

|

∂E−

∂E−

∂E−

1
1 (z1)
∂z1
1
1 (z1)
∂z1
1
1 (z1)
∂z1
1
1 (z1)
∂z1
1
1 (z1)
∂z1

∂E−

|

|

|

|

|

||

||

||

=

=

=

=

= p(z1)

∂E−

∂G−

∂z1
1
1 (z1) |
∂E−

1
p(G−

1 (G1(z1)))

1
p(G−

1
1 (E−

1 (z1)))

1

p(G−

1 (G1(z1)))

Similarly we have pE(z2) = p(z2), which together prove the original proposition.
x1
From another perspective, we note that
GAN coincide with the
objective of a WAE [39] or AAE [86] in the latent space, which pushes the
encoded latent distribution towards the latent distribution at generation time.

c2
recon,

s1
recon,

L

L

L

Proposition 3. When optimality is reached, we have p(x1, x1

2) = p(x2

1, x2).

→

→

Proof. For the ease of notation we denote the joint distribution p(x1, x1
by p1

2)
1(x1, x2). Both densities are zero when

2(x1, x2) and p(x2

1, x2) by p2

→

→

→

→

Multimodal Unsupervised Image-to-Image Translation

21

Ec

1(x1)

= Ec

2(x2). When Ec

1(x1) = Ec

2(x2), we also have:

p1

→

1(x1))p(x1)

Ec
2(x1, x2) = pG(x2|
∂Es
2(x2)
=
x2
|
Ec
= p(x2|
1(x1, x2)
= p2

q(Es
|

→

1(x1))pG(x1)

2(x2))p(x1)

Proposition 4. Denote h1 = (x1, s2)
∈ H2. h1, h2 are
points in the joint spaces of image and style. Our model deﬁnes a deterministic
2(x1, s2) (cid:44)
mapping F1
2(h1) = F1
H2 (and vice versa) by F1
1
(G2(Ec
1.
2 = F −
1(x1)). When optimality is achieved, we have F1
2
→

∈ H1 and h2 = (x2, s1)

2 from
→
1(x1), s2), Es

H1 to

→

→

→

Proof.

F2

1(F1

→

→

2(x1, s2)) (cid:44) F2

1(G2(Ec

1(x1), s2), Es

1(x1))

2(G2(Ec

1(x1), s2)), Es

1(x1)), Es

2(G2(Ec

1(x1), s2)))

→
(cid:44) (G1(Ec

= (G1(Ec
= (G1(Ec
= (x1, s2)

2(G2(Ec
1(x1), Es

1(x1), s2)), Es
1(x1)), s2)

1(x1)), s2)

(10)

(11)

(12)

(13)

(14)

→

2(F2

And we can prove F1
speciﬁc, (3) is implied by the style reconstruction loss
content reconstruction loss
loss
by the proposed bidirectional reconstruction loss.

1(x2, s1)) = (x2, s1) in a similar manner. To be more
s
recon, (4) is implied by the
L
c
recon, and (5) is implied by the image reconstruction
L
x
recon. As a result, style-augmented cycle consistency is implicitly implied
L

→

Proposition 5. (Cycle consistency implies deterministic translations).
Let p(x1) and p(x2) denote the data distributions. pG(x1|
x1) are
two conditionals deﬁned by generators. Given 1) matched marginals: p(x1) =
(cid:82) pG(x1|
x1)p(x1) dx1, and 2) cycle consistency:
Ex2
x∗2) for
x1)]= δ(x2 −
2 )[pG(x2|
x1) collapse to deterministic
every x∗1 ∈ X1, x∗2 ∈ X2, then pG(x1|
delta functions.

x2)p(x2) dx2, p(x1) = (cid:82) pG(x2|
x∗1), Ex1
x2) and pG(x2|

x2) and pG(x2|

x2)] = δ(x1 −

1 )[pG(x1|

pG(x2

pG(x1

x∗

x∗

∼

∼

|

|

Proof. Let x∗1 be a sample from p(x1). x(cid:48)2, x(cid:48)(cid:48)2 are two samples from pG(x2|
x∗1).
x(cid:48)2) = pG(x1|
Due to cycle consistency in
x(cid:48)(cid:48)2 ) =
X1 → X2 → X1, we have pG(x1|
δ(x1 −
x∗1). Also, x(cid:48)2 ∈ X2 and x(cid:48)(cid:48)2 ∈ X2 because of matched marginals. Due
x∗1) = δ(x2 −
to cycle consistency in
x(cid:48)2) =
X2 → X1 → X2, we have pG(x2|
x2).
x1) collapses to a delta function, similar for pG(x1|
x(cid:48)(cid:48)2 ). Thus pG(x2|
δ(x2 −
This proposition shows that cycle consistency [8] is a too strong constraint for
multimodal image translation.

22

Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz

B Training Details

B.1 Hyperparameters

We use the Adam optimizer [87] with β1 = 0.5, β2 = 0.999, and an initial
learning rate of 0.0001. The learning rate is decreased by half every 100, 000
iterations. In all experiments, we use a batch size of 1 and set the loss weights
to λx = 10, λc = 1, λs = 1. We use the domain-invariant perceptual loss with
weight 1 in the street scene and Yosemite datasets. We choose the dimension of
the style code to be 8 across all datasets. Random mirroring is applied during
training.

B.2 Network Architectures

×

×

7 convolutional block with k ﬁlters and stride 1. dk
4 convolutional block with k ﬁlters and stride 2. Rk denotes a

Let c7s1-k denote a 7
denotes a 4
residual block that contains two 3
×
nearest-neighbor upsampling layer followed by a 5
5 convolutional block with
k ﬁlters and stride 1. GAP denotes a global average pooling layer. fck denotes a
fully connected layer with k ﬁlters. We apply Instance Normalization (IN) [71]
to the content encoder and Adaptive Instance Normalization (AdaIN) [54] to
the decoder. We use ReLU activations in the generator and Leaky ReLU with
slope 0.2 in the discriminator. We use multi-scale discriminators with 3 scales.

3 convolutional blocks. uk denotes a 2

×

×

– Generator architecture

Content encoder: c7s1-64, d128, d256, R256, R256, R256, R256
Style encoder: c7s1-64, d128, d256, d256, d256, GAP, fc8
Decoder: R256, R256, R256, R256, u128, u64, c7s1-3

– Discriminator architecture: d64, d128, d256, d512

•
•
•

C Domain-invariant Perceptual Loss

↔

We conduct an experiment to verify if applying IN before computing the feature
distance can indeed make the distance more domain-invariant. We experiment
dataset used by Isola et al. [6] and originally proposed by Laﬀont et
on the day
al. [4]. We randomly sample two sets of image pairs: 1) images from the same
domain (both day or both night) but diﬀerent scenes, 2) images from the same
scene but diﬀerent domains. Fig. 11 shows examples from the two sets of image
pairs. We then compute the VGG feature (relu4 3) distance between each image
pair, with IN either applied or not before computing the distance. In Fig. 12,
we show histograms of the distance computed either with or without IN, and
from image pairs either of the same domain or the same scene. Without applying
IN before computing the distance, the distribution of feature distance is similar
for both sets of image pairs. With IN enabled, however, image pairs from the
same scene have clearly smaller distance, even they come from diﬀerent domains.
The results suggest that applying IN before computing the distance makes the
feature distance much more domain-invariant.

Multimodal Unsupervised Image-to-Image Translation

23

(a) Image pairs from the same scene.

(b) Image pairs from the same domain.

Fig. 11. Example image pairs for domain-invariant perceptual loss experiments.

Fig. 12. Histograms of the VGG feature distance. Left: distance computed without
using IN. Right: distance computed after IN. Blue: distance between image pairs from
the same domain (but diﬀerent scenes). Green: distance between image pairs from the
same scene (but diﬀerent domains).

