Overview of the NLPCC 2017 Shared Task: Chinese News Headline
Categorization

Xipeng Qiu, Jingjing Gong, Xuanjing Huang
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{xpqiu, jjgong15, xjhuang}@fudan.edu.cn

Abstract

Category

Train Dev Test

7
1
0
2
 
n
u
J
 
9
 
 
]
L
C
.
s
c
[
 
 
1
v
3
8
8
2
0
.
6
0
7
1
:
v
i
X
r
a

In this paper, we give an overview for the
shared task at the CCF Conference on Nat-
ural Language Processing & Chinese Com-
puting (NLPCC 2017): Chinese News Head-
The dataset of this
line Categorization.
shared task consists 18 classes, 12,000 short
texts along with corresponded labels for each
class. The dataset and example code can be
accessed at https://github.com/FudanNLP/
nlpcc2017_news_headline_categorization.

1 Task Deﬁnition
This task aims to evaluate the automatic classiﬁcation
techniques for very short texts, i.e., Chinese news head-
lines. Each news headline (i.e., news title) is required
to be classiﬁed into one or more predeﬁned categories.
With the rise of Internet and social media, the text data
on the web is growing exponentially. Make a human
being to analysis all those data is impractical, while ma-
chine learning techniques suits perfectly for this kind of
tasks. after all, human brain capacity is too limited and
precious for tedious and non-obvious phenomenons.

Formally, the task is deﬁned as follows: given a news
headline x = (x1, x2, ..., xn), where xj represents jth
word in x, the object is to ﬁnd its possible category or
label c ∈ C. More speciﬁcally, we need to ﬁnd a function
to predict in which category does x belong to.

c∗ = arg max

f (x; θc),

c∈C

(1)

where θ is the parameter for the function.

2 Data
We collected news headlines (titles) from several Chinese
news websites, such as toutiao, sina, and so on.

There are 18 categories in total. The detailed infor-
mation of each category is shown in Table 1. All the
sentences are segmented by using the python Chinese
segmentation tool jieba.

Some samples from training dataset are shown in Ta-

ble 2.

entertainment
sports
car
society
tech
world
ﬁnance
game
travel
military
history
baby
fashion
food
discovery
story
regimen
essay

10000
10000
10000
10000
10000
10000
10000
10000
10000
10000
10000
10000
10000
10000
4000
4000
4000
4000

2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000

2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000

Table 1: The information of categories.

Length Figure 1 shows that most of title sentence
character number is less than 40, with a mean of 21.05.
Title sentence word length is even shorter, most of which
is less than 20 with a mean of 12.07.
on

released
https:
//github.com/FudanNLP/nlpcc2017_news_headline_
categorization along with code that implement three
basic models.

dataset

github

The

is

3 Evaluation
We use the macro-averaged precision, recall and F1 to
evaulate the performance.

The Macro Avg. is deﬁned as follow:

And Micro Avg. is deﬁned as:

M acro avg =

1
m

m
(cid:88)

i=1

ρi

M icro avg =

wiρi

1
N

m
(cid:88)

i=1

Category

Title Sentence

The results generated from baseline models are shown

in Table 4.

world
society
ﬁnance
travel
ﬁnance
sports
entertainment

..
...
9000

28 ...

Table 2: Samples from dataset. The ﬁrst column is Cat-
egory and the second column is news headline.

Model Macro P Macro R Macro F Accuracy

0.760
LSTM
CNN
0.769
NBoW 0.791

0.747
0.763
0.783

0.7497
0.764
0.784

0.747
0.763
0.783

Table 4: Results of the baseline models.

5 Participants Submitted Results

Participant Macro P Macro R Macro F Accu.

P1
P2
P3
P4
P5
P6
P7
P8
P9
P10
P11
P12
P13
P14
P15
P16
P17
P18
P19
P20
P21
P22
P23
P24
P25
P26
P27
P28
P29
P30
P31
P32

0.831
0.828
0.818
0.816
0.812
0.811
0.809
0.806
0.803
0.805
0.799
0.797
0.793
0.791
0.792
0.786
0.778
0.785
0.785
0.766
0.768
0.768
0.744
0.729
0.745
0.734
0.698
0.640
0.645
0.437
0.474
0.053

0.829
0.825
0.814
0.809
0.809
0.807
0.804
0.802
0.800
0.800
0.798
0.795
0.789
0.789
0.787
0.783
0.775
0.775
0.775
0.765
0.759
0.748
0.729
0.726
0.700
0.688
0.685
0.633
0.629
0.430
0.399
0.056

0.830
0.826
0.816
0.813
0.810
0.809
0.806
0.804
0.802
0.802
0.798
0.796
0.791
0.790
0.789
0.785
0.777
0.780
0.780
0.765
0.764
0.758
0.736
0.728
0.722
0.710
0.691
0.637
0.637
0.433
0.433
0.054

0.829
0.825
0.814
0.809
0.809
0.807
0.804
0.802
0.800
0.800
0.798
0.795
0.789
0.789
0.786
0.783
0.775
0.775
0.775
0.765
0.759
0.748
0.729
0.726
0.700
0.688
0.685
0.633
0.629
0.430
0.399
0.056

Table 5: Results submitted by participants.

There are 32 participants actively participate and sub-
mit they predictions on the test set. The predictions are
evaluated and the results are shown in table 5.

6 Conclusion
Since large amount of data is required for Machine
Learning techniques like Deep Learning, we have col-
lected considerable amount of News headline data and
contributed to the research community.

Figure 1: The blue line is character length statistic, and
blue line is word length.

Category Size Avg. Chars Avg. Words

train
dev.
test

156000 22.06
22.05
36000
22.05
36000

13.08
13.09
13.08

Table 3: Statistical information of the dataset.

Where m denotes the number of class, in the case of this
dataset is 18. ρi is the accuracy of ith category, wi rep-
resents how many test examples reside in ith category,
N is total number of examples in the test set.

4 Baseline Implementations

As a branch of machine learning, Deep Learning (DL)
has gained much attention in recent years due to its
prominent achievement in several domains such as Com-
puter vision and Natural Language processing.

We have implemented some basic DL models such as
neural bag-of-words (NBoW), convolutional neural net-
works (CNN) [Kim, 2014] and Long short-term memory
network (LSTM) [Hochreiter and Schmidhuber, 1997].

Empirically, 2 Gigabytes of GPU Memory should be
suﬃcient for most models, set batch to a smaller number
if not.

References
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter
and J¨urgen Schmidhuber. Long short-term memory.
Neural computation, 9(8):1735–1780, 1997.

[Kim, 2014] Yoon Kim.

works for sentence classiﬁcation.
arXiv:1408.5882, 2014.

Convolutional neural net-
arXiv preprint

