7
1
0
2
 
v
o
N
 
1
2
 
 
]

G
L
.
s
c
[
 
 
2
v
9
7
0
6
0
.
9
0
7
1
:
v
i
X
r
a

Orthogonal Weight Normalization: Solution to Optimization over
Multiple Dependent Stiefel Manifolds in Deep Neural Networks

Lei Huang, Xianglong Liu, Bo Lang
Beihang University
{huanglei,xlliu,langbo}@nlsde.buaa.edu.cn

Adams Wei Yu
CMU
weiyu@cs.cmu.edu

Yongliang Wang
JD.COM
wangyongliang1@jd.com

Bo Li
UC Berkeley
crystalboli@berkeley.edu

Abstract

Orthogonal matrix has shown advantages in training Recurrent Neural Networks (RNNs), but such
matrix is limited to be square for the hidden-to-hidden transformation in RNNs. In this paper, we generalize
such square orthogonal matrix to orthogonal rectangular matrix and formulating this problem in feed-
forward Neural Networks (FNNs) as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM).
We show that the rectangular orthogonal matrix can stabilize the distribution of network activations and
regularize FNNs. We also propose a novel orthogonal weight normalization method to solve OMDSM.
Particularly, it constructs orthogonal transformation over proxy parameters to ensure the weight matrix
is orthogonal and back-propagates gradient information through the transformation during training. To
guarantee stability, we minimize the distortions between proxy parameters and canonical weights over
all tractable orthogonal transformations. In addition, we design an orthogonal linear module (OLM) to
learn orthogonal ﬁlter banks in practice, which can be used as an alternative to standard linear module.
Extensive experiments demonstrate that by simply substituting OLM for standard linear module without
revising any experimental protocols, our method largely improves the performance of the state-of-the-art
networks, including Inception and residual networks on CIFAR and ImageNet datasets. In particular, we
have reduced the test error of wide residual network on CIFAR-100 from 20.04% to 18.61% with such
simple substitution. Our code is available online for result reproduction.

1 Introduction

Standard deep neural networks (DNNs) can be viewed as a composition of multiple simple nonlinear functions,
each of which usually consists of one linear transformation with learnable weights or parameters followed
by an element-wise nonlinearity. Such hierarchy and deep architectures equip DNNs with large capacity to
represent complicated relationships between inputs and outputs. However, they also introduce potential risk
of overﬁtting. Many methods have been proposed to address this issue, e.g. weight decay [19] and Dropout
[30] are commonly applied by perturbing objectives or adding random noise directly. These techniques can
improve generalization of networks, but hurt optimization efﬁciency, which means one needs to train more
epochs to achieve better performance. This naturally rises one question: is there any technique that can
regularize DNNs to guarantee generalization while still guarantee efﬁcient convergence?

To achieve this goal, we focus on the orthogonality constraint, which is imposed in linear transformation
between layers of DNNs. This technique performs optimization over low embedded submanifolds, where

1

weights are orthogonal, and thus regularizes networks. Besides, the orthogonality implies energy preservation,
which is extensively explored for ﬁlter banks in signal processing and guarantees that energy of activations
will not be ampliﬁed [39] . Therefore, it can stabilize the distribution of activations over layers within DNNs
[6, 25] and make optimization more efﬁcient.

Orthogonal matrix has been actively explored in Recurrent Neural Networks (RNNs) [7, 3, 35, 33, 21].
It helps to avoiding gradient vanishing and explosion problem in RNNs due to its energy preservation
property [7]. However, the orthogonal matrix here is limited to be square for the hidden-to-hidden trans-
formation in RNNs. More general setting of learning orthogonal rectangular matrix is barely studied in
DNNs [10], especially in deep Convolutional Neural Networks (CNNs) [22]. We formulate such a problem
as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM), due to the fact that the weight matrix
with orthogonality constraint in each layer is an embedded Stiefel Manifold [1] and the weight matrix in
certain layer is affected by those in preceding layers in DNNs.

To solve OMDSM problem, one straightforward idea is to use Riemannian optimization method that
is extensively used for single manifold or multiple independent manifolds problem, either in optimization
communities [1, 32, 2, 34, 5] or in applications to the hidden-to-hidden transformation of RNNs [35, 33].
However, Riemannian optimization methods suffer instability in convergence or inferior performance in deep
feed-forward neural networks based on our comprehensive experiments. Therefore, a stable method is highly
required for OMDSM problem.

Inspired by the orthogonality-for-vectors problem [9] and the fact that eigenvalue decomposition is
differentiable [14], we propose a novel proxy parameters based solution referred to as orthogonal weight
normalization. Speciﬁcally, we devise explicitly a transformation that maps the proxy parameters to canonical
weights such that the canonical weights are orthogonal. Updating is performed on the proxy parameters when
gradient signal is ensured to back-propagate through the transformation. To guarantee stability, we minimize
the distortions between proxy parameters and canonical weights over all tractable orthogonal transformations.
Based on orthogonal weight normalization, we design orthogonal linear module for practical purpose.
This module is a linear transformation with orthogonality, and can be used as an alternative of standard linear
modules for DNNs. At the same time, this module is capable of stabilizing the distribution of activation in
each layer, and therefore facilitates optimization process. Our method can also cooperate well with other
practical techniques in deep learning community, e.g., batch normalization [13], Adam optimization [16]
and Dropout [30], and moreover improve their original performance.

Comprehensive experiments are conducted over Multilayer Perceptrons (MLPs) and CNNs. By simply
substituting the orthogonal linear modules for standard ones without revising any experimental protocols, our
method improves the performance of various state-of-the-art CNN architectures, including BN-Inception
[13] and residual networks [11] over CIFAR [17] and ImageNet [26] datasets. For example, on wide residual
network, we improve the performance on CIFAR-10 and CIFAR-100 with test error as 3.73% and 18.61%,
respectively, compared to the best reported results 4.17% and 20.04% in [38].

In summarization, our main contributions are as follows.

• To the best of our knowledge, this is the ﬁrst work to formulate the problem of learning orthogonal
ﬁlters in DNNs as optimization over multiple dependent Stiefel manifolds problem (OMDSM). We
further analyze two remarkable properties of orthogonal ﬁlters for DNNs: stabilizing the distributions
of activation and regularizing the networks.

• We conduct comprehensive experiments to show that several extensively used Riemannian optimization
methods for single Stiefel manifold suffer severe instability in solving OMDSM, We thus propose a
novel orthogonal weight normalization method to solve OMDSM and show that the solution is stable
and efﬁcient in convergence.

2

• We devise an orthogonal linear module to perform as an alternative to standard linear module for

practical purpose.

• We apply the proposed method to various architectures including BN-Inception and residual networks,
and achieve signiﬁcant performance improvement over large scale datasets, including ImageNet.

2 Optimization over Multiple Dependent Stiefel Manifolds

Let X ⊆ Rd be the feature space, with d the number of features. Suppose the training set {(xi, yi)}M
i=1 is
comprised of feature vector xi ∈ X generated according to some unknown distribution xi ∼ D, with yi the
corresponding labels. A standard feed-forward neural network with L-layers can be viewed as a function
f (x; θ) parameterized by θ, which is expected to ﬁt the given training data and generalize well for unseen
data points. Here f (x; θ) is a composition of multiple simple nonlinear functions. Each of them usually
consists of a linear transformation sl = Wlhl−1 + bl with learnable weights Wl ∈ Rnl×dl and biases
bl ∈ Rnl, followed by an element-wise nonlinearity: hl = ϕ(sl). Here l ∈ {1, 2, ..., L} indexes the layers.
Under this notation, the learnable parameters are θ = {Wl, bl|l = 1, 2, . . . , L}. Training neural networks is
to minimize the discrepancy between the desired output y and the predicted output f (x; θ). This discrepancy
is usually described by a loss function L(y, f (x; θ)), and thus the objective is to optimize θ by minimizing
the loss function: θ∗ = arg minθ E(x,y)∈D[L(y, f (x; θ))].

2.1 Formulation

This paper targets to train deep neural networks (DNNs) with orthogonal rectangular weight matrix Wl ∈
Rnl×dl in each layer. Particularly, we expect to learn orthogonal ﬁlters of each layer (the rows of W ). We
thus formulate it as a constrained optimization problem:

θ∗ = arg minθ E(x,y)∈D [L (y, f (x; θ))]
, l = 1, 2, ..., L

s.t. Wl ∈ Onl×dl

l

(1)

l

where the matrix family Onl×dl
= {Wl ∈ Rnl×dl : Wl(Wl)T = I}1 is real Stiefel manifold [1, 5], which is
an embedded sub-manifold of Rnl×dl. The formulated problem has following characteristics: (1) the optimiza-
, ..., OnL×dL
tion space is over multiple embedded submanifolds; (2) the embedded submanifolds {On1×d1
}
L
is dependent due to the fact that the optimization of weight matrix Wl is affected by those in preceding layers
{Wi, i < l}; (3) moreover, the dependencies amplify as the network becomes deeper. We thus call such a
problem as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM). To our best knowledge, we
are the ﬁrst to learn orthogonal ﬁlters for deep feed-forward neural networks and formulate such a problem as
OMDSM. Indeed, the previous works [35, 33] that learning orthogonal hidden-to-hidden transformation in
RNNs is over single manifold due to weight sharing of hidden-to-hidden transformation.

1

2.2 Properties of Orthogonal Weight Matrix

Before solving OMDSM, we ﬁrst introduce two remarkable properties of orthogonal weight matrix for DNNs.

1 We ﬁrst assume nl ≤ dl and will discuss how to handle the case nl > dl in subsequent sections.

3

2.2.1 Stabilize the Distribution of Activations

Orthogonal weight matrix can stabilize the distributions of activations in DNNs as illustrated in the following
theorem.

Theorem 1. Let s = Wx, where WWT = I and W ∈ Rn×d. (1) Assume the mean of x is Ex[x] = 0, and
covariance matrix of x is cov(x) = σ2I. Then Es[s] = 0, cov(s) = σ2I. (2) If n = d, we have (cid:107)s(cid:107) = (cid:107)x(cid:107).
(3) Given the back-propagated gradient ∂L

∂s , we have (cid:107) ∂L

∂x (cid:107) = (cid:107) ∂L

∂s (cid:107).

The proof of Theorem 1 is shown in Appendix A.1. The ﬁrst point of Theorem 1 shows that in each
layer of DNNs the weight matrix with orthonormality can maintain the activation s to be normalized and
even de-correlated if the input is whitened. The normalized and de-correlated activation is well known for
improving the conditioning of the Fisher information matrix and accelerating the training of deep neural
networks [20, 6, 37]. Besides, orthogonal ﬁlters can well keep the norm of the activation and back-propagated
gradient information in DNNs as shown by the second and third point of Theorem 1.

2.2.2 Regularize Neural Networks

Orthogonal weight matrix can also ensure each ﬁlter to be orthonomal: i.e. wT
i wj = 0, i (cid:54)= j and (cid:107)wi(cid:107)2 = 1,
where wi ∈ Rd indicates the weight vector of the i-th neuron and (cid:107)wi(cid:107)2 denotes the Euclidean norm
of wi. This provides n(n + 1)/2 constraints. Therefore, orthogonal weight matrix regularizes the neural
networks as the embedded Stiefel manifold On×d with degree of freedom nd − n(n + 1)/2 [1]. Note that
this regularization may harm the representation capacity if neural networks is not enough deep. We can
relax the constraint of orthonormal to orthogonal, which means we don’t need (cid:107)wi(cid:107)2 = 1. A practical
method is to introduce a learnable scalar parameter g to ﬁne tune the norm of w [27]. This trick can recover
the representation capacity of orthogonal weight layer to some extent, that is practical in shallow neural
networks but for deep CNNs, it is unnecessary based on our observation. We also discuss how to trade off the
regularization and optimization efﬁciency of orthogonal weight matrix in subsequent sections.

3 Orthogonal Weight Normalization

To solve OMDSM problem, one straightforward idea is to use Riemannian optimization methods that are used
for the hidden-to-hidden transform in RNNs [35, 33]. However, we ﬁnd that the Riemannian optimization
methods to solve OMDSM suffered instability in convergence or inferior performance as shown in the
experiment section.

Here we propose a novel algorithm to solve OMDSM problem via re-parameterization [27]. For each layer
l, we represent the weight matrix Wl in terms of the proxy parameter matrix Vl ∈ Rnl×dl as Wl = φ(Vl),
and parameter update is performed with respect to Vl. By devising a transformation φ : Rnl×dl → Rnl×dl
such that φ(Vl) ∗ φ(Vl)T = I, we can ensure the weight matrix Wl is orthogonal. Besides, we require the
gradient information back-propagates through the transformation φ. An illustrative example is shown in
Figure 1. Without loss of generality, we drop the layer indexes of Wl and Vl for clarity.

3.0.1 Devising Transformation

Inspired by the classic problem of orthogonality-for-vectors [9], we represent φ(V) as linear transformation
φ(V) = PV. In general, vectors in this problem are usually assumed to be zero-centered. We therefore

4

Figure 1: An illustrative example of orthogonal weight normalization in certain layer of neural networks (for
brevity, we leave out the bias nodes).

ﬁrst center V by: VC = V − c1T
transformation is performed over VC.

d where c = 1

d V1d and 1d is d-dimension vector with all ones. The

There can be inﬁnite P satisfying W = PVC and WWT = I. For example, if ˆP is the solution,
Q ˆP is also the solution where Q is an arbitrary orthogonal matrix Q ∈ Rn×n, since we have WWT =
Q ˆPVCVT
C

ˆPT QT = QQT = I. The question is which P should be chosen?

In order to achieve a stable solution, we expect the singular values of Jacobians ∂W/∂V close to 1 [28].
However, this constraint is difﬁcult to be formulated. We thus look for a relaxation and tractable constraint as
minimizing the distortion between W and VC in a least square way:
(W − VC) (W − VC)T (cid:17)
s.t. W = PVC and WWT = I,

minP tr

(2)

(cid:16)

where tr(·) indicates the trace of matrix. We omit the derivation of solving this optimization to Appendix
A.2. The solution is P∗ = DΛ−1/2DT , where Λ = diag(σ1, . . . , σn) and D represent the eigenvalues and
d )T . Based on this solution, we use the
eigenvectors of the covariance matrix Σ = (V − c1T
transformation as follows:

d )(V − c1T

W = φ(V) = DΛ−1/2DT (V − c1T

d ).

(3)

We also consider another transformation Pvar = Λ−1/2DT without minimizing such distortions, and observe
that Pvar suffers the instability problem and fails convergence in subsequent experiments. Therefore, we
hypothesize that minimizing distortions formulated by Eqn. 2 is essential to ensure the stability of solving
OMDSM.

3.0.2 Back-Propagation

We target to update proxy parameters V, and therefore it is necessary to back-propagate the gradient
information through the transformation φ(V). To achieve this, we use the result from matrix differential
calculus [14], which combines the derivatives of eigenvalues and eigenvectors based on chain rule: given
∂D ∈ Rn×n and ∂L
∂L
∂Σ =
D((KT (cid:12) (DT ∂L
[i (cid:54)= j],

∂Λ ∈ Rn×n, where L is the loss function, the back-propagate derivatives are ∂L
∂Λ )diag)DT , where K ∈ Rn×n is 0-diagonal and structured as Kij = 1

∂D )) + ( ∂L

σi−σj

5

∂Λ )diag sets all off-diagonal elements of ∂L

and ( ∂L
multiplication. Based on the chain rule, the back-propagated formulations for calculating ∂L
below.

∂Λ to zero. The (cid:12) operator represents element-wise matrix
∂V are shown as

∂L
∂Λ = −

1
2

DT ∂L
∂W

WT DΛ−1

T

∂L
∂W

1

∂L

2 DT W

∂L
∂D = DΛ
∂Σ = D((KT (cid:12) (DT ∂L
∂D
∂L
∂W
2 DT ∂L
∂W

∂V = DΛ− 1

∂c = −1T

DΛ− 1

∂L

∂L

T

d

DΛ− 1

2 +

)) + (

∂L
∂Λ

WT D

∂L
∂W
)diag)DT

2 DT − 2 · 1T

d (V − c1T

d )T (

+ 2(

)s(V − c1T

d ) +

∂L
∂Σ
T

)s

1T
d

1
d

∂L
∂c

∂L
∂Σ

T

∂Σ )s means symmetrizing ∂L

where ( ∂L
decent or other tractable optimization methods to update V. Note that symmetrizing ∂L
on the perturbation theory, since wiggling c or V will make Σ wiggle symmetrically.

∂Σ ). Given ∂L

∂Σ by ( ∂L

∂V , we can apply regular gradient
∂Σ is necessary based

∂Σ )s = 1

2 ( ∂L

+ ∂L

∂Σ

Algorithm 1 Forward pass of OLM.
1: Input: mini-batch input H ∈ Rd×m and parameters: b ∈ Rn×1, V ∈ Rn×d.
2: Output: S ∈ Rn×m and W ∈ Rn×d.
3: Calculate: Σ = (V − 1
d )(V − 1
4: Eigenvalue decomposition: Σ = DΛDT .
5: Calculate W based on Eqn. 3.
6: Calculate S as standard linear module does.

d V1d1T

d V1d1T

d )T .

Algorithm 2 Backward pass of OLM.
1: Input: activation derivative ∂L
2: Output: { ∂L
3: Calculate: ∂L
4: Calculate ∂L
5: Update V and b.

∂H ∈ Rd×m}, V ∈ Rn×d and b ∈ Rn×1.
∂W , ∂L
∂V base on Eqn. 4

∂b and ∂L

∂H as standard linear module does.

∂S ∈ Rn×m and variables from respective forward pass.

3.1 Orthogonal Linear Module

Based on our orthogonal weight normalization method for solving OMDSM, we build up the Orthogonal
Linear Module (OLM) from practical perspective. Algorithm 1 and 2 summarize the forward and backward
pass of OLM, respectively. This module can be an alternative of standard linear module. Based on this, we
can train DNNs with orthogonality constraints by simply substituting it for standard linear module without
any extra efforts. After training, we calculate the weight matrix W based on Eqn. 3. Then W will be saved
and used for inference as the standard module does.

6

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 2: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 4-layer MLP.
We show the training loss curves for different learning rate of ‘EI+QR’, ‘CI+QR’ and ‘CayT’ compared to the baseline
‘plain’ in (a), (b) and (c) respectively. We compare our methods to baselines and report the best performance among all
learning rates based on the training loss for each method in (d).

3.1.1 Convolutional Layer

With regards to the convolutional layer parameterized by weights WC ∈ Rn×d×Fh×Fw where Fh and Fw
are the height and width of the ﬁlter, it takes feature maps X ∈ Rd×h×r as input, where h and r are the
height and width of the feature maps, respectively. We denote ∆ the set of spatial locations and Ω the set of
spatial offsets. For each output feature map k and its spatial location δ ∈ ∆, the convolutional layer computes
the activation {sk,δ} as: sk,δ = (cid:80)d
τ ∈Ω wk,i,τ hi,δ+τ =< wk, hδ >. Here wk eventually can be viewed
as unrolled ﬁlter produced by WC. We thus reshape WC as W ∈ Rn×p where p = d · Fh · Fw, and the
orthogonalization is executed over the unrolled weight matrix W ∈ Rn×(d·Fh·Fw).

(cid:80)

i=1

3.1.2 Group Based Orthogonalization

In previous sections, we assume n <= d, and obtain the solution of OMDSM such that the rows of W is
orthogonal. To handle the case with n > d, we propose the group based orthogonalization method. That is,
we divide the weights {wi}n
i=1 into groups with size NG <= d and the orthogonalization is performed over
each group, such that the weights in each group is orthogonal.

One appealing property of group based orthogonalization is that we can use group size NG to control to
what extent we regularize the networks. Assume NG can be divided by n, the free dimension of embedded
manifold is nd − n(NG + 1)/2 by using orthogonal group method. If we use NG = 1, this method reduces
to Weight Normalization [27] without learnable scalar parameters.

Besides, group based orthogonalization is a practical strategy in real application, especially reducing the
computational burden. Actually, the cost of eigen decomposition with high dimension in GPU is expensive.
When using group with small size (e.g., 64), the eigen decomposition is not the bottleneck of computation,
compared to convolution operation. This make our orthogonal linear module possible to be applied in very
deep and high dimensional CNNs.

3.1.3 Computational Complexity

We show our method is scalable from complexity analysis here and provide empirical results later for
large CNNs. Given a convolutional layer with ﬁlters W ∈ Rn×d×Fh×Fw , and m mini-batch data {xi ∈
Rd×h×w}m
G +
nmdhwFhFw) per iteration, and if we control a small group size NG (cid:28) mhw, it will be close to the standard
convolutional layer as O(nmdhwFhFw).

i=1. The computational complexity of our method with group size NG is O(nNGdFhFw + nN 2

7

4 Experiments

In this section, we ﬁrst conduct comprehensive experiments to explore different methods to solve the OMDSM
problem, and show the advantages of our proposed orthogonal weight normalization solution in terms of the
stability and efﬁciency in optimization. We then evaluate the effectiveness of the proposed method that learns
orthogonal weight matrix in DNNs, by simply replacing our OLM with standard ones on MLPs and CNNs.
Codes to reproduce our results are available from: https://github.com/huangleiBuaa/OthogonalWN.

4.1 Comparing Methods for Solving OMDSM

In this section, we use 3 widely used Riemannian optimization methods for solving OMDSM and compared
two other baselines. For completeness, we provide a brief review for Riemannian optimization shown in
Appendix A.3 and for more details please refer to [1] and references therein.

We design comprehensive experiments on MNIST dataset to compare methods for solving OMDSM.
The compared methods including: (1) ‘EI+QR’: using Riemannian gradient with Euclidean inner product
and QR-retraction [10]; (2) ‘CI+QR’: using Riemannian gradient with canonical inner product and QR-
retraction; (3) ‘CayT’: using the Cayley transformation [35, 33]; (4) ‘QR’: a conventional method that runs
∂W and projects the solution back to the manifold M by
the ordinary gradient descent based on gradient ∂F
QR decomposition; (5) ‘OLMvar’: using orthogonal transformation: Pvar = Λ−1/2DT ; (6) ‘OLM’: our
proposed orthogonal transformation by minimizing distortions: P∗ = DΛ−1/2DT . The baseline is the
standard network without any orthogonal constraints referred to as ‘plain’.

We use MLP architecture with 4 hidden layers. The number of neurons in each hidden layer is 100. We
train the model with stochastic gradient descent and mini-batch size of 1024. We tried a broadly learning rate
in ranges of {0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5}.

We ﬁrstly explored the performance of Riemannian optimization methods for solving OMDSM problem.
Figure 2 (a), (b) and (c) show the training loss curves for different learning rate of ‘EI+QR’, ‘CI+QR’ and
‘CayT’ respectively, compared to the baseline ‘plain’. From Figure 2, we can ﬁnd that under larger learning
rate (e.g., larger than 0.05) these Riemannian optimization methods suffer severe instability and divergence,
even though they show good performance in the initial iterations. They can also obtain stable optimization
behaviours under small learning rate but are signiﬁcantly slower in convergence than the baseline ‘plain’ and
suffer worse performance.

We then compared our proposed method with the baseline ‘plain’ and the conventional method ‘QR’, and
report the best performance among all learning rates based on the training loss for each method in Figure
2 (d). We can ﬁnd that the conventional method ‘QR’ performs stably. However, it also suffers inferior
performance of ﬁnal training loss compared to ‘plain’. The proposed ‘OLM’ works stably and converges
the fastest. Besides, we ﬁnd that ‘OLMvar’ suffered instability, which means that minimizing distortions
formulated by Eqn. 2 is essential to ensure the stability of solving OMDSM.

We also explore 6-layer and 8-layer MLPs and further with mini-batch size of 512 and 256. We observe
the similar phenomena shown in Appendix A.4. Especially with the number of layer increasing, ‘OLM’
shows more advantages compared to other methods. These comprehensive experiments strongly support our
empirical conclusions that: (1) Riemannian optimization methods probably do not work for the OMDSM
problem, and if work, they must be under ﬁne designed algorithms or tuned hyper-parameters; (2) deep
feed-forward neural networks (e.g., MLP in this experiment) equipped with orthogonal weight matrix is
easier for optimization by our ‘OLM’ solution.

8

(a) training error

(b) test error

Figure 3: Performance comparisons in MLP architecture on PIE dataset. We compare the effect of different group size
of OLM.

(a) batch normalization

(b) Adam optimization

Figure 4: Performance comparisons in MLP architecture on PIE dataset by combining (a) batch normalization; (b)
Adam optimization. We evaluate the training error (solid lines) and test error (dash lines marked with triangle).

4.2 MLP Architecture

Now we investigate the performance of OLM in MLP architecture. On PIE face recognition dataset with
11,554 images from 68 classes, we sample 1,340 images as the test set and others as training set. Here,
we employ standard networks (referred as plain) and networks with Weight Normalization (WN) [27] as
baselines for comparisons. WN is one of the most related study that normalizes the weights as unit norm via
re-parameterization as OLM does, but it does not introduce the orthogonality for the weights matrix. For all
methods, we train a 6-layers MLP with the number of neurons in each hidden layer as 128,128,128,128,128,
and Relu as nonlinearity. The mini-batch size is set to 256. We evaluate the training error and test error as a
function with respect to epochs.

4.2.1 Using Different Group Sizes

We explore the effects of group size NG on the performance when applying OLM. In this setup, we employ
stochastic gradient descent (SGD) optimization and the learning rates are selected based on the validation
set (10% samples of the training set) from {0.05, 0.1, 0.2, 0.5, 1}. Figure 3 shows the performance of OLM

9

(a) CIFAR-10

(b) CIFAR-100

Figure 5: Experimental results on VGG-style architectures over CIFAR datasets. We evaluate the training error (solid
lines) and test error (dash lines marked with triangle) with respect to epochs, and all results are averaged over 5 runs.

using different NG (‘OLM-NG’), compared with plain and WN methods. We can ﬁnd that OLM achieves
signiﬁcantly better performance in all cases, which means introducing orthogonality to weight matrix can
largely improve the network performance. Another observation is that though increasing group size would
help improve orthogonalization, too large group size will reduce the performance. This is mainly because a
large NG = 128 provides overmuch regularization. Fortunately, when we add extra learnable scale (indicated
by ‘OLM-scale-128’) to recover the model capacity as described in previous section, it can help to achieve
the best performance.

4.2.2 Combining with Batch Normalization

Batch normalization [13] has been shown to be helpful for training the deep architectures [13, 12]. Here,
we show that OLM enjoys good compatibility to incorporate well with batch normalization, which still
outperforms others in this case. Figure 4 (a) shows the results of training/test error with respect to epochs.
We can see that WN with batch normalization (‘WN+batch’) has no advantages compared with the standard
network with batch normalization (‘batch’), while ‘OLM+batch’ consistently achieves the best performance.

4.2.3 Applying Adam Optimization

We also try different optimization technique such as Adam [16] optimization. The hyper-parameters are
selected from learning rates in {0.001, 0.002, 0.005, 0.01}. We show error rates based on Adam optimization
in Figure 4 (b). From the ﬁgure, we can see OLM also obtains the best performance.

4.3 CNN Architectures

In this section, We evaluate our method on a VGG-style CNN [29], BN-Inception [31, 13], and Wide
Residual Networks [38] for image classiﬁcation, respectively on CIFAR-10 and CIFAR-100 [17]. For each
dataset, We use the ofﬁcial training set of 50k images and the standard test set of 10k images. The data
preprocessing and data argumentation follow the commonly used mean&std normalization and ﬂip translation
as described in [11]. For OLM method, we replace all convolution layers with our OLM modules by default
on CNNs, if we do not specify it. Among all experiments, the group size NG of OLM is set as 64.

10

(a) CIFAR-10

(b) CIFAR-100

Figure 6: Experimental results on BN-Inception over CIFAR datasets. We evaluate the training error (solid lines) and
test error (dash lines marked with triangle) with respect to epochs, and all results are averaged over 5 runs.

4.3.1 VGG-style network

We adopt the 3 × 3 convolutional layer as the following speciﬁcation: → conv(64) → conv(128) →
maxP ool(2, 2, 2, 2) → conv(256) → conv(256) → maxP ool(2, 2, 2, 2) → conv(512) → conv(512)
→ AveP ool(8, 8, 1, 1) → f c(512 × ClassN um). SGD is used as our optimization method with mini-batch
size of 256. The best initial learning rate is chosen from {0.01, 0.05, 0.1} over the validation set of 5k
examples from the training set, and exponentially decayed to 1% in the last (100th) epoch. We set the
momentum to 0.9 and weight decay to 5 × 10−4. Table 1 reports the test error, from which we can ﬁnd OLM
achieves the best performance consistently on both datasets. Figure 5 (a) and (b) show the training and test
errors with respect to epochs on CIFAR-10 and CIFAR-100, respectively. On CIFAR-100, to achieve the
ﬁnal test error of plain as 36.02 %, OLM takes only 17 epochs. Similarly, on CIFAR-10, OLM only takes 21
epochs to achieve the ﬁnal test error of plain as 10.39 %. While on both datasets, ’plain’ takes about 100
epochs. Results demonstrate that OLM converges signiﬁcantly faster in terms of training epochs and achieves
better error rate compared to baselines.

We also study the effect of OLM on different layers. We optionally replace the ﬁrst 2 and 4 convolution
layers with OLM modules (referred as OLM-L2 and OLM-L4 respectively). From Figure 5 and Table 1, we
can ﬁnd that with the numbers of used OLM increasing, the VGG-style network achieves better performance
both in optimization efﬁciency and generalization.

4.3.2 BN-Inception

For BN-inception network, batch normalization [13] is inserted after each linear layer based on original
Inception architecture [31]. Again, we train the network using SGD, with the momentum 0.9, weight decay
5 × 10−4 and the batch size 64. The initial learning rate is set to 0.1 and decays exponentially every two
epochs until the end of 100 epoches with 0.001. Table 2 reports the test error after training and Figure 5 (c)
and (d) show the training/test error with respect to epochs on CIFAR-10 and CIFAR-100, respectively. We
can ﬁnd that OLM converges faster in terms of training epochs and achieve better optimum, compared to
baselines, which indicate consistent conclusions for VGG-style network above.

11

Table 1: Test error (%) on VGG-style over CIFAR datasets. We report the ‘mean ±std’ computed over 5
independent runs.

plain
WN
OLM-L2
OLM-L4
OLM

CIFAR-10
10.39 ± 0.14
10.29± 0.39
10.06 ± 0.23
9.61 ± 0.23
8.61 ± 0.18

CIFAR-100
36.02 ± 0.40
34.66 ± 0.75
35.42 ± 0.32
33.66 ± 0.11
32.58 ± 0.10

CIFAR-10
5.38± 0.18
plain
5.87± 0.35
WN
OLM 4.74± 0.16

CIFAR-100
24.87 ± 0.15
23.85 ± 0.28
22.02 ± 0.13

Table 2: Test error (%) on BN-Inception over CIFAR datasets. We report the ‘mean ±std’ computed over 5
independent runs.

4.3.3 Wide Residual Netwok

Wide Residual Network (WRN) has been reported to achieve state-of-the-art results on CIFARs [38]. We
adopt WRN architecture with depth 28 and width 10 and the same experimental setting as in [38]. Instead
of ZCA whitening, we preprocess the data using per-pixel mean subtract and standard variance divided as
described in [11]. We implement two setups of OLM: (1) replace all the convolutional layers by WRN
(WRN-OLM); (2) only replace the ﬁrst convolutional layer in WRN (WRN-OLM-L1). Table 3 reports the
test errors. We can see that OLM can further improve the state-of-the-art results achieved by WRN. For
example, on CIFAR-100, our method WRN-OLM achieves the best 18.61 test error, compared to 20.04 of
WRN reported in [38]. Another interesting observation is that WRN-OLM-L1 obtains the best performance
on CIFAR-10 with test error as 3.73%, compare to 4.17% of WRN, which means that we can improve
residual networks by only constraining the ﬁrst convolution layer orthogonal and the extra computation cost
is negligible.

4.3.4 Computation Cost

We also evaluate computational cost per iteration in our current Torch-based implementation, where the
convolution relies on the fastest cudnn package. In the small VGG-style architecture with batch size of 256,
OLM costs 0.46s, while plain and WN cost 0.26s and 0.38s, respectively. On large WRN network, OLM costs
3.12s compared to 1.1s of plain. Note that, our current implementation of OLM can be further optimized.

4.4 Large Scale Classiﬁcation on ImageNet Challenge

To further validate the effectiveness of OLM on large-scale dataset, we employ ImageNet 2012 consisting of
more than 1.2M images from 1,000 classes [26]. We use the given 1.28M labeled images for training and
the validation set with 50k images for testing. We evaluate the classiﬁcation performance based on top-5

12

Table 3: Test errors (%) of different methods on CIFAR-10 and CIFAR-100. For OLM, we report the ‘mean
±std’ computed over 5 independent runs. ‘WRN-28-10*’ indicates the new results given by authors on their
Github.

pre-Resnet-1001
WRN-28-10
WRN-28-10*
WRN-28-10-OLM (ours)
WRN-28-10-OLM-L1 (ours)

CIFAR-10
4.62
4.17
3.89
3.73 ± 0.12
3.82 ± 0.19

CIFAR-100
22.71
20.04
18.85
18.76 ± 0.40
18.61 ± 0.14

Table 4: Top-5 test error (%, single model and single-crop) on ImageNet dataset.

AlexNet BN-Inception ResNet
12.5
9.83

9.84
9.68

plain
20.91
OLM 20.43

Pre-ResNet
9.79
9.45

error. We apply the well-known AlexNet [18] with batch normalization inserted after the convolution layers,
BN-Inception2, ResNet [11] with 34 layers and its advanced version Pre-ResNet [12]3. In AlexNet and
BN-Inception, we replace all the convolution layers with OLM modules for our method, and in ResNet and
Pre-ResNet, we only replace the ﬁrst convolution layer with OLM module, which is shown effective with
negligible computation cost based on previous experiment.

We run our experiments on one GPU. To guarantee a fair comparison between our method with the
baseline, we keep all the experiments settings the same as the publicly available Torch implementation (e.g.,
we apply stochastic gradient descent with momentum of 0.9, weight decay of 0.0001, and set the initial
learning rate to 0.1). The exception is that we use mini-batch size of 64 and 50 training epochs considering
the GPU memory limitations and training time costs. Regarding learning rate annealing, we use exponential
decay to 0.001, which has slightly better performance than the method of lowering by a factor of 10 after
epoch 20 and epoch 40 for each method. The ﬁnal test errors are shown in Table 4. We can ﬁnd that our
proposed OLM method obtains better performance compared to the baselines over AlexNet, BN-Inception,
ResNet and Pre-ResNet architectures.

5 Related Work and Discussion

In optimization community, there exist methods to solve the optimization problem over matrix manifolds with
orthogonal constraints [32, 2, 34, 1, 5]. They are usually limited for one manifold (one linear mapping) [5],
and are mainly based on full batch gradient. When applying to DNNs, it suffered instability in convergence
or inferior performance, as observed by either [10, 33] or our experiments.

In deep learning community, there exist researches using orthogonal matrix [3, 35, 7, 33] or normalization
techniques [13, 27, 37] to avoid the gradient vanish and explosion problem. Arjovsky et al. [3] introduced the
orthogonal matrix for the hidden to hidden transformation in RNN. They constructed an expressive unitary

2We use the public Torch implementation of AlexNet and BN-Inception available on: https://github.com/soumith/imagenet-

multiGPU.torch

3We use the public Torch implementation available on: https://github.com/facebook/fb.resnet.torch

13

weight matrix by composing several structured matrices that act as building blocks with parameters to be
learned. Wisdom et al. [35] pointed out that Arjovsky’s method [3] has restricted representational capacity
and proposed a method of optimizing a full-capacity unitary matrix by using Reimannian gradient in canonical
inner product with Cayley transformation [35, 33]. Dorobantu et al. [7] also proposed a simple method of
updating orthogonal linear transformations in RNN in a way that maintains orthogonality. Vorontsov [33]
advocate soft constraints on orthogonality when they ﬁnd that hard constraints on orthogonality can negatively
affect the speed of convergence and model performance. However, these methods are limited for the hidden
to hidden transformation in RNN because both methods require the weight matrix to be square matrix. Our
methods are more general and can adapt to situations where the weight matrix are not square, especially for
deep Convolutional Neural Networks (CNNs).

Recently, Harandi and Fernando [10] have proposed Stiefel layer to guarantee fully connected layer
to be orthogonal by using Reimannian gradient in Euclidean inner product with QR-retraction. However,
this method only shows the experimental results of replacing the last one or two layers of neural networks
with Stiefel layers. Besides, this method is not stable as shown in their experiments: when they constrain the
last two fully connected layer to be orthogonal in VGG networks, the accuracy performance is low. We also
observe similar phenomena as shown in Section 4.1 that ‘EI+QR’ shows instability in convergence. Ozay and
Okatani [22] also used Riemannian optimization to guarantee convolutional kernels within a channel are
orthogonal, while our ensure ﬁlters (among channels) orthogonal. Some methods [36, 25] used orthogonal
regularizer as a penalty posed on the objective function. These methods can not learn orthogonal ﬁlters,
because they relax the constraints and only penalize their violations. The solution of these methods is on
infeasible set of the optimization problem with orthogonality constraint while our on feasible set.

6 Conclusions and Further Work

We formulate learning orthogonal linear transformation in DNNs as Optimization over Multiple Dependent
Stiefel Manifolds (OMDSM) and propose the Orthogonal Weight Normalization method to solve it, which is
stable and can be applied to large and deep networks. Base on this solution, we design Orthogonal Linear
Module (OLM) which can be applied as an alternative to standard linear module. We show that neural
networks equipped with OLM can improve optimization efﬁciency and generalization ability. In addition,
new deep architectures that address domain-speciﬁc representation can also beneﬁt from the proposed method
by simply replacing standard linear module with OLM.

Various shallow dimensional reduction methods have been uniﬁed under the optimization framework
with orthogonality constraints [5]. Our method has potentials to improve the performance of corresponding
unsupervised [23] and semi-supervised methods [24] in DNNs. Besides, our method has great potential to
be used in improving the robust of the networks to adversarial examples [4].

References

Press, Princeton, NJ, 2008.

Optimization, 22(1):135–158, 2012.

[1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University

[2] Pierre-Antoine Absil and Jerome Malick. Projection-like retractions on matrix manifolds. SIAM Journal on

[3] Martín Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In ICML, 2016.

[4] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks:

Improving robustness to adversarial examples. In ICML, 2017.

14

[5] John P. Cunningham and Zoubin Ghahramani. Linear dimensionality reduction: Survey, insights, and generaliza-

tions. J. Mach. Learn. Res., 16(1):2859–2900, January 2015.

[6] Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and Koray Kavukcuoglu. Natural neural networks. In

NIPS, 2015.

[7] Victor Dorobantu, Per Andre Stromhaug, and Jess Renteria. Dizzyrnn: Reparameterizing recurrent neural networks

for norm-preserving backpropagation. CoRR, abs/1612.04035, 2016.

[8] Y. C. Eldar and A. V. Oppenheim. Mmse whitening and subspace whitening. IEEE Trans. Inf. Theor., 49(7):1846–

1851, September 2006.

[9] Paul H. Garthwaite, Frank Critchley, Karim Anaya-Izquierdo, and Emmanuel Mubwandarikwa. Orthogonalization

of vectors with minimal adjustment. Biometrika, 99(4):787 – 798, 2012.

[10] Mehrtash Harandi and Basura Fernando. Generalized backpropagation, etude de cas: Orthogonality. CoRR,

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In

abs/1611.05927, 2016.

CVPR, 2016.

ECCV, 2016.

[13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal

[14] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Training deep networks with structured layers by

covariate shift. In ICML, 2015.

matrix backpropagation. In ICCV, 2015.

[15] Tetsuya Kaneko, Simone G. O. Fiori, and Toshihisa Tanaka. Empirical arithmetic averaging over the compact

stiefel manifold. IEEE Trans. Signal Processing, 61(4):883–894, 2013.

[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.

[17] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional neural

[19] Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. In NIPS, 1992.

[20] Yann LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. Efﬁicient backprop. In Neural Networks:

[21] Zakaria Mhammedi, Andrew D. Hellicar, Ashfaqur Rahman, and James Bailey. Efﬁcient orthogonal parametrisa-

tion of recurrent neural networks using householder reﬂections. In ICML, 2017.

[22] Mete Ozay and Takayuki Okatani. Optimization on submanifolds of convolution kernels in cnns. CoRR,

networks. In NIPS. 2012.

Tricks of the Trade, 1998.

abs/1610.07008, 2016.

IJCAI, 2017.

[23] Feiping Nie Yuan Yuan Qi Wang, Zequn Qin. Convolutional 2d lda for nonlinear dimensionality reduction. In

[24] Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised learning
with ladder networks. In Proceedings of the 28th International Conference on Neural Information Processing
Systems, NIPS’15, pages 3546–3554, 2015.

[25] Pau Rodríguez, Jordi Gonzàlez, Guillem Cucurull, Josep M. Gonfaus, and F. Xavier Roca. Regularizing cnns with

locally constrained decorrelations. In ICLR, 2017.

15

[26] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.

[27] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training

of deep neural networks. In NIPS, 2016.

[28] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning

in deep linear neural networks. CoRR, abs/1312.6120, 2013.

[29] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In

ICLR, 2015.

[30] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A
simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res., 15(1):1929–1958, January 2014.

[31] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.

Going deeper with convolutions. In CVPR, 2015.

[32] Hemant D. Tagare. Notes on optimization on stiefel manifolds. Technical report, Yale University, 2011.

[33] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent

networks with long term dependencies. In ICML, 2017.

[34] Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints. Math. Program.,

142(1-2):397–434, 2013.

[35] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent

neural networks. In NIPS, pages 4880–4888. 2016.

[36] Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training
extremely deep convolutional neural networks with orthonormality and modulation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.

[37] Adams Wei Yu, Lei Huang, Qihang Lin, Ruslan Salakhutdinov, and Jaime G. Carbonell. Block-normalized

gradient method: An empirical study for training deep neural network. CoRR, abs/1707.04822, 2017.

[38] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.

[39] Jianping Zhou, Minh N. Do, and Jelena Kovacevic. Special paraunitary matrices, cayley transform, and multidi-

mensional orthogonal ﬁlter banks. IEEE Trans. Image Processing, 15(2):511–519, 2006.

A Appendix

A.1 Proof of Theorem 1
Theorem 1. Let s = Wx, where WWT = I and W ∈ Rn×d. (1) Assume the mean of x is Ex[x] = 0, and
covariance matrix of x is cov(x) = σ2I. Then Es[s] = 0, cov(s) = σ2I. (2) If n = d, we have (cid:107)s(cid:107) = (cid:107)x(cid:107). (3) Given
the back-propagated gradient ∂L

∂s , we have (cid:107) ∂L

∂x (cid:107) = (cid:107) ∂L

∂s (cid:107).

Proof. (1) It’s easy to calculate:

Es[s] = wT Ex[x] = wT µ1 = 0

(4)

16

The covariance of s is given by

cov(s) = Es[s − Es[s]]2

= Ex[W(x − Ex[x])]2
= Ex[W(x − Ex[x])] · Ex[W(x − Ex[x])]T
= WEx[(x − Ex[x])] · Ex[(x − Ex[x])]T WT
= Wcov(x)WT
= Wσ2IWT
= σ2WWT = σ2

(2) If n = d, W is orthogonal matrix, therefore WT W = WWT = I. (cid:107)s(cid:107) = sT s = xT WT Wx = xT x = (cid:107)x(cid:107).
(3) As similar as the proof of (2), (cid:107) ∂L

∂s W(cid:107) = ∂L

∂s WWT ∂L

∂x (cid:107) = (cid:107) ∂L

= (cid:107) ∂L
∂s (cid:107)

= ∂L
∂s

∂L
∂s

∂s

T

T

A.2 Derivation of Minimizing Orthogonal Vectors Transformation Problem

Given a matrix V ∈ Rn×d where n ≤ d and Rank(V) = n, we expect to transform it by W = PV such that
WWT = I where W ∈ Rn×d and P ∈ Rn×n. Besides, we minimize the distortion between the transformed matrix
W and the original matrix V in a least square way. This can be formulated as:

minP tr((W − V)(W − V)T )
s.t. W = PV and WWT = I

where tr(A) denotes the trace of the matrix A. The derivation of this problem is based on paper [8] where a similar
problem is considered in the context of data whitening. To solve this problem, we ﬁrst calculate the covariance matrix
Σ = VVT . Since Rank(V) = n, we have Σ is real positive deﬁnite and thus invertible with positive eigenvalues.
By performing eigenvalue decomposition on Σ, we have Σ = DΛDT where Λ = diag(σ1, . . . , σn) and D are the
eigenvalues and eigenvectors respectively. Given the constraint WWT = I, we have PVVT PT = I. Therefor, we
have PΣPT = I.

By construction, we have

where M is an n × n orthonomal matrix with MMT = MT M = I. For convenience, we use Σ− 1
Therefore, we have:

2 = DΛ−1/2DT ,

P = MΣ− 1

2

tr((W − V)(W − V)T )

= tr(WWT − VWT − WVT + VVT )
= tr(I) + tr(Σ) − tr(VWT ) − tr((VWT )T )
= tr(I) + tr(Σ) − 2tr(VWT ))
= d + tr(Σ) − 2tr(ΣPT ))
= d + tr(Σ) − 2tr(DΛDT DΛ−1/2DT MT )
= d + tr(Σ) − 2tr(DΛ1/2DT MT )

Minimizing 8 with respect to P is equivalent to maximizing tr(DΛ1/2DT MT ) with respect to M. Note that
tr(DΛ1/2DT MT ) = tr(Λ1/2DT MT D) = tr(Λ1/2Q) with Q = DT MT D is orthogonal matrix. One important
point fact is that Qij ≤ 1, otherwise the norm of the i-th row or the j-th column of Q will larger than 1, which is
contradictory to that Q is real orthogonal matrix. Therefore, maximizing tr(Λ1/2Q = (cid:80)n
ii Qii) guarantees
Q = I. We thus have M = DIDT = I. Therefore, we get the solution P = Σ− 1

2 = DΛ−1/2DT .

i=1 Λ1/2

17

(5)

(6)

(7)

(8)

A.3 Riemannian Optimization over Stiefel Manifold

For comparison purpose, here we review the Riemannian optimization over Stiefel manifold brieﬂy and for more
details please refer to [1] and references therein. The objective is arg minW∈M f (W), where f is a real-value smooth
function over M = {W ∈ Rn×d : WT W = I, n (cid:62) d}. Note that in this section, we follow the common description
for Riemannian optimization over Stiefel manifold with the columns of W being d orthonormal vectors in Rn, and
therefore with the constraints WT W = I. It is different to the description of our formulation in the paper with
constraints WWT = I and n (cid:54) d.

Conventional optimization techniques are based gradient descent method over manifold by iteratively seeking for
updated points Wt ∈ M. In each iteration t, the keys are: (1) ﬁnding the Riemannian gradient GM f (Wt) ∈ TWt
where TWt is the tangent space of M at current point Wt; and (2) ﬁnding the descent direction and ensuring that the
new points is on the manifold M.

For obtaining the Riemannian gradient GM f (W), the inner dot should be deﬁned in TW. There are two extensively
1 X2)
2 WWT )X2) where X1, X2 ∈ TW and tr(·) denote the

used inner products for tangent space of Stiefel manifold [34]: (1) Euclidean inner product: < X1, X2 >e= tr(XT
1 (I − 1
and (2) canonical inner product: < X1, X2 >c= tr(XT
trace of the matrix. Based on these two inner products, the respective Riemannian gradient can be obtained as [34]:

(9)

(10)

(11)

(12)

and

GM

e f (W) =

∂f
∂W

− W

T

∂f
∂W

W

GM

c f (W) =

∂f
∂W

−

1
2

(WWT ∂f
∂W

+ W

W)

T

∂f
∂W

where ∂f

∂W is the ordinary gradient.

Given the Riemannian gradient, the next step is to ﬁnd the descent direction and guarantee that the new point is on
the manifold M, which can be supported by the so called operation retraction. One well recommended retractionis the
QR-Decomposition-Type retraction [15, 10] that maps a tangent vector of TW onto M by: PW(Z) = qf (W + Z),
where qf (·) denotes the Q factor of the QR decomposition with Q ∈ M, and the R-factor is an upper-trangular matrix
with strictly positive elements on its main diagonal such that the decomposition is unique [15]. Given the Riemannian
gradient GM f (W) and the learning rate η, the new point is:

Another well known technique to jointly move in the descent direction and make sure the new solution on the
manifold M is Cayley transformation [34, 35, 33]. It produces the feasible solution Wk+1 with the current solution
Wk by:

Wt+1 = qf (Wt − η GM f (W))

Wt+1 = (I +

At)−1(I −

At)W

η
2

η
2

where η is the learning rate and At = ∂F
∂Wt
the tangent space.

T

Wt − WT
t

∂F
∂Wt

that is induced by the deﬁned canonical inner product in

A.4 More Experimental Results

Here, we show more experimental results for Solving OMDSM on MNIST dataset under MLP architectures. The
experimental setups are described in the paper. Figure 7 and 8 show the results under the respective 6-layer and 8-layer
MLPs with mini-batch size of 1024. We also train the model with mini-batch size of 512 and the results under the
4-layer, 6-layer and 8-layer MLPs are shown in Figure 9, 10 and 11 respectively. We further train the model with
mini-batch size of 256 and the results under the 4-layer, 6-layer and 8-layer MLPs are shown in Figure 12, 13 and 14.
These comprehensive experiments strongly support our empirical conclusions that: (1) Riemannian optimization
methods probably do not work for the OMDSM problem, and if work, they must be under ﬁne designed algorithms or
tuned hyper-parameters; (2) deep feed-forward neural networks (e.g., MLP in this experiment) equipped with orthogonal
weight matrix is easier for optimization by our ‘OLM’ solution.

18

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 7: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 6-layer MLP.
We train the model with batch size of 1024 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 8: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 8-layer MLP.
We train the model with batch size of 1024 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 9: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 4-layer MLP.
We train the model with batch size of 512 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 10: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 6-layer MLP.
We train the model with batch size of 512 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

19

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 11: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 8-layer MLP.
We train the model with batch size of 512 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 12: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 4-layer MLP.
We train the model with batch size of 256 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 13: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 6-layer MLP.
We train the model with batch size of 256 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 14: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 8-layer MLP.
We train the model with batch size of 256 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

20

7
1
0
2
 
v
o
N
 
1
2
 
 
]

G
L
.
s
c
[
 
 
2
v
9
7
0
6
0
.
9
0
7
1
:
v
i
X
r
a

Orthogonal Weight Normalization: Solution to Optimization over
Multiple Dependent Stiefel Manifolds in Deep Neural Networks

Lei Huang, Xianglong Liu, Bo Lang
Beihang University
{huanglei,xlliu,langbo}@nlsde.buaa.edu.cn

Adams Wei Yu
CMU
weiyu@cs.cmu.edu

Yongliang Wang
JD.COM
wangyongliang1@jd.com

Bo Li
UC Berkeley
crystalboli@berkeley.edu

Abstract

Orthogonal matrix has shown advantages in training Recurrent Neural Networks (RNNs), but such
matrix is limited to be square for the hidden-to-hidden transformation in RNNs. In this paper, we generalize
such square orthogonal matrix to orthogonal rectangular matrix and formulating this problem in feed-
forward Neural Networks (FNNs) as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM).
We show that the rectangular orthogonal matrix can stabilize the distribution of network activations and
regularize FNNs. We also propose a novel orthogonal weight normalization method to solve OMDSM.
Particularly, it constructs orthogonal transformation over proxy parameters to ensure the weight matrix
is orthogonal and back-propagates gradient information through the transformation during training. To
guarantee stability, we minimize the distortions between proxy parameters and canonical weights over
all tractable orthogonal transformations. In addition, we design an orthogonal linear module (OLM) to
learn orthogonal ﬁlter banks in practice, which can be used as an alternative to standard linear module.
Extensive experiments demonstrate that by simply substituting OLM for standard linear module without
revising any experimental protocols, our method largely improves the performance of the state-of-the-art
networks, including Inception and residual networks on CIFAR and ImageNet datasets. In particular, we
have reduced the test error of wide residual network on CIFAR-100 from 20.04% to 18.61% with such
simple substitution. Our code is available online for result reproduction.

1 Introduction

Standard deep neural networks (DNNs) can be viewed as a composition of multiple simple nonlinear functions,
each of which usually consists of one linear transformation with learnable weights or parameters followed
by an element-wise nonlinearity. Such hierarchy and deep architectures equip DNNs with large capacity to
represent complicated relationships between inputs and outputs. However, they also introduce potential risk
of overﬁtting. Many methods have been proposed to address this issue, e.g. weight decay [19] and Dropout
[30] are commonly applied by perturbing objectives or adding random noise directly. These techniques can
improve generalization of networks, but hurt optimization efﬁciency, which means one needs to train more
epochs to achieve better performance. This naturally rises one question: is there any technique that can
regularize DNNs to guarantee generalization while still guarantee efﬁcient convergence?

To achieve this goal, we focus on the orthogonality constraint, which is imposed in linear transformation
between layers of DNNs. This technique performs optimization over low embedded submanifolds, where

1

weights are orthogonal, and thus regularizes networks. Besides, the orthogonality implies energy preservation,
which is extensively explored for ﬁlter banks in signal processing and guarantees that energy of activations
will not be ampliﬁed [39] . Therefore, it can stabilize the distribution of activations over layers within DNNs
[6, 25] and make optimization more efﬁcient.

Orthogonal matrix has been actively explored in Recurrent Neural Networks (RNNs) [7, 3, 35, 33, 21].
It helps to avoiding gradient vanishing and explosion problem in RNNs due to its energy preservation
property [7]. However, the orthogonal matrix here is limited to be square for the hidden-to-hidden trans-
formation in RNNs. More general setting of learning orthogonal rectangular matrix is barely studied in
DNNs [10], especially in deep Convolutional Neural Networks (CNNs) [22]. We formulate such a problem
as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM), due to the fact that the weight matrix
with orthogonality constraint in each layer is an embedded Stiefel Manifold [1] and the weight matrix in
certain layer is affected by those in preceding layers in DNNs.

To solve OMDSM problem, one straightforward idea is to use Riemannian optimization method that
is extensively used for single manifold or multiple independent manifolds problem, either in optimization
communities [1, 32, 2, 34, 5] or in applications to the hidden-to-hidden transformation of RNNs [35, 33].
However, Riemannian optimization methods suffer instability in convergence or inferior performance in deep
feed-forward neural networks based on our comprehensive experiments. Therefore, a stable method is highly
required for OMDSM problem.

Inspired by the orthogonality-for-vectors problem [9] and the fact that eigenvalue decomposition is
differentiable [14], we propose a novel proxy parameters based solution referred to as orthogonal weight
normalization. Speciﬁcally, we devise explicitly a transformation that maps the proxy parameters to canonical
weights such that the canonical weights are orthogonal. Updating is performed on the proxy parameters when
gradient signal is ensured to back-propagate through the transformation. To guarantee stability, we minimize
the distortions between proxy parameters and canonical weights over all tractable orthogonal transformations.
Based on orthogonal weight normalization, we design orthogonal linear module for practical purpose.
This module is a linear transformation with orthogonality, and can be used as an alternative of standard linear
modules for DNNs. At the same time, this module is capable of stabilizing the distribution of activation in
each layer, and therefore facilitates optimization process. Our method can also cooperate well with other
practical techniques in deep learning community, e.g., batch normalization [13], Adam optimization [16]
and Dropout [30], and moreover improve their original performance.

Comprehensive experiments are conducted over Multilayer Perceptrons (MLPs) and CNNs. By simply
substituting the orthogonal linear modules for standard ones without revising any experimental protocols, our
method improves the performance of various state-of-the-art CNN architectures, including BN-Inception
[13] and residual networks [11] over CIFAR [17] and ImageNet [26] datasets. For example, on wide residual
network, we improve the performance on CIFAR-10 and CIFAR-100 with test error as 3.73% and 18.61%,
respectively, compared to the best reported results 4.17% and 20.04% in [38].

In summarization, our main contributions are as follows.

• To the best of our knowledge, this is the ﬁrst work to formulate the problem of learning orthogonal
ﬁlters in DNNs as optimization over multiple dependent Stiefel manifolds problem (OMDSM). We
further analyze two remarkable properties of orthogonal ﬁlters for DNNs: stabilizing the distributions
of activation and regularizing the networks.

• We conduct comprehensive experiments to show that several extensively used Riemannian optimization
methods for single Stiefel manifold suffer severe instability in solving OMDSM, We thus propose a
novel orthogonal weight normalization method to solve OMDSM and show that the solution is stable
and efﬁcient in convergence.

2

• We devise an orthogonal linear module to perform as an alternative to standard linear module for

practical purpose.

• We apply the proposed method to various architectures including BN-Inception and residual networks,
and achieve signiﬁcant performance improvement over large scale datasets, including ImageNet.

2 Optimization over Multiple Dependent Stiefel Manifolds

Let X ⊆ Rd be the feature space, with d the number of features. Suppose the training set {(xi, yi)}M
i=1 is
comprised of feature vector xi ∈ X generated according to some unknown distribution xi ∼ D, with yi the
corresponding labels. A standard feed-forward neural network with L-layers can be viewed as a function
f (x; θ) parameterized by θ, which is expected to ﬁt the given training data and generalize well for unseen
data points. Here f (x; θ) is a composition of multiple simple nonlinear functions. Each of them usually
consists of a linear transformation sl = Wlhl−1 + bl with learnable weights Wl ∈ Rnl×dl and biases
bl ∈ Rnl, followed by an element-wise nonlinearity: hl = ϕ(sl). Here l ∈ {1, 2, ..., L} indexes the layers.
Under this notation, the learnable parameters are θ = {Wl, bl|l = 1, 2, . . . , L}. Training neural networks is
to minimize the discrepancy between the desired output y and the predicted output f (x; θ). This discrepancy
is usually described by a loss function L(y, f (x; θ)), and thus the objective is to optimize θ by minimizing
the loss function: θ∗ = arg minθ E(x,y)∈D[L(y, f (x; θ))].

2.1 Formulation

This paper targets to train deep neural networks (DNNs) with orthogonal rectangular weight matrix Wl ∈
Rnl×dl in each layer. Particularly, we expect to learn orthogonal ﬁlters of each layer (the rows of W ). We
thus formulate it as a constrained optimization problem:

θ∗ = arg minθ E(x,y)∈D [L (y, f (x; θ))]
, l = 1, 2, ..., L

s.t. Wl ∈ Onl×dl

l

(1)

l

where the matrix family Onl×dl
= {Wl ∈ Rnl×dl : Wl(Wl)T = I}1 is real Stiefel manifold [1, 5], which is
an embedded sub-manifold of Rnl×dl. The formulated problem has following characteristics: (1) the optimiza-
, ..., OnL×dL
tion space is over multiple embedded submanifolds; (2) the embedded submanifolds {On1×d1
}
L
is dependent due to the fact that the optimization of weight matrix Wl is affected by those in preceding layers
{Wi, i < l}; (3) moreover, the dependencies amplify as the network becomes deeper. We thus call such a
problem as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM). To our best knowledge, we
are the ﬁrst to learn orthogonal ﬁlters for deep feed-forward neural networks and formulate such a problem as
OMDSM. Indeed, the previous works [35, 33] that learning orthogonal hidden-to-hidden transformation in
RNNs is over single manifold due to weight sharing of hidden-to-hidden transformation.

1

2.2 Properties of Orthogonal Weight Matrix

Before solving OMDSM, we ﬁrst introduce two remarkable properties of orthogonal weight matrix for DNNs.

1 We ﬁrst assume nl ≤ dl and will discuss how to handle the case nl > dl in subsequent sections.

3

2.2.1 Stabilize the Distribution of Activations

Orthogonal weight matrix can stabilize the distributions of activations in DNNs as illustrated in the following
theorem.

Theorem 1. Let s = Wx, where WWT = I and W ∈ Rn×d. (1) Assume the mean of x is Ex[x] = 0, and
covariance matrix of x is cov(x) = σ2I. Then Es[s] = 0, cov(s) = σ2I. (2) If n = d, we have (cid:107)s(cid:107) = (cid:107)x(cid:107).
(3) Given the back-propagated gradient ∂L

∂s , we have (cid:107) ∂L

∂x (cid:107) = (cid:107) ∂L

∂s (cid:107).

The proof of Theorem 1 is shown in Appendix A.1. The ﬁrst point of Theorem 1 shows that in each
layer of DNNs the weight matrix with orthonormality can maintain the activation s to be normalized and
even de-correlated if the input is whitened. The normalized and de-correlated activation is well known for
improving the conditioning of the Fisher information matrix and accelerating the training of deep neural
networks [20, 6, 37]. Besides, orthogonal ﬁlters can well keep the norm of the activation and back-propagated
gradient information in DNNs as shown by the second and third point of Theorem 1.

2.2.2 Regularize Neural Networks

Orthogonal weight matrix can also ensure each ﬁlter to be orthonomal: i.e. wT
i wj = 0, i (cid:54)= j and (cid:107)wi(cid:107)2 = 1,
where wi ∈ Rd indicates the weight vector of the i-th neuron and (cid:107)wi(cid:107)2 denotes the Euclidean norm
of wi. This provides n(n + 1)/2 constraints. Therefore, orthogonal weight matrix regularizes the neural
networks as the embedded Stiefel manifold On×d with degree of freedom nd − n(n + 1)/2 [1]. Note that
this regularization may harm the representation capacity if neural networks is not enough deep. We can
relax the constraint of orthonormal to orthogonal, which means we don’t need (cid:107)wi(cid:107)2 = 1. A practical
method is to introduce a learnable scalar parameter g to ﬁne tune the norm of w [27]. This trick can recover
the representation capacity of orthogonal weight layer to some extent, that is practical in shallow neural
networks but for deep CNNs, it is unnecessary based on our observation. We also discuss how to trade off the
regularization and optimization efﬁciency of orthogonal weight matrix in subsequent sections.

3 Orthogonal Weight Normalization

To solve OMDSM problem, one straightforward idea is to use Riemannian optimization methods that are used
for the hidden-to-hidden transform in RNNs [35, 33]. However, we ﬁnd that the Riemannian optimization
methods to solve OMDSM suffered instability in convergence or inferior performance as shown in the
experiment section.

Here we propose a novel algorithm to solve OMDSM problem via re-parameterization [27]. For each layer
l, we represent the weight matrix Wl in terms of the proxy parameter matrix Vl ∈ Rnl×dl as Wl = φ(Vl),
and parameter update is performed with respect to Vl. By devising a transformation φ : Rnl×dl → Rnl×dl
such that φ(Vl) ∗ φ(Vl)T = I, we can ensure the weight matrix Wl is orthogonal. Besides, we require the
gradient information back-propagates through the transformation φ. An illustrative example is shown in
Figure 1. Without loss of generality, we drop the layer indexes of Wl and Vl for clarity.

3.0.1 Devising Transformation

Inspired by the classic problem of orthogonality-for-vectors [9], we represent φ(V) as linear transformation
φ(V) = PV. In general, vectors in this problem are usually assumed to be zero-centered. We therefore

4

Figure 1: An illustrative example of orthogonal weight normalization in certain layer of neural networks (for
brevity, we leave out the bias nodes).

ﬁrst center V by: VC = V − c1T
transformation is performed over VC.

d where c = 1

d V1d and 1d is d-dimension vector with all ones. The

There can be inﬁnite P satisfying W = PVC and WWT = I. For example, if ˆP is the solution,
Q ˆP is also the solution where Q is an arbitrary orthogonal matrix Q ∈ Rn×n, since we have WWT =
Q ˆPVCVT
C

ˆPT QT = QQT = I. The question is which P should be chosen?

In order to achieve a stable solution, we expect the singular values of Jacobians ∂W/∂V close to 1 [28].
However, this constraint is difﬁcult to be formulated. We thus look for a relaxation and tractable constraint as
minimizing the distortion between W and VC in a least square way:
(W − VC) (W − VC)T (cid:17)
s.t. W = PVC and WWT = I,

minP tr

(2)

(cid:16)

where tr(·) indicates the trace of matrix. We omit the derivation of solving this optimization to Appendix
A.2. The solution is P∗ = DΛ−1/2DT , where Λ = diag(σ1, . . . , σn) and D represent the eigenvalues and
d )T . Based on this solution, we use the
eigenvectors of the covariance matrix Σ = (V − c1T
transformation as follows:

d )(V − c1T

W = φ(V) = DΛ−1/2DT (V − c1T

d ).

(3)

We also consider another transformation Pvar = Λ−1/2DT without minimizing such distortions, and observe
that Pvar suffers the instability problem and fails convergence in subsequent experiments. Therefore, we
hypothesize that minimizing distortions formulated by Eqn. 2 is essential to ensure the stability of solving
OMDSM.

3.0.2 Back-Propagation

We target to update proxy parameters V, and therefore it is necessary to back-propagate the gradient
information through the transformation φ(V). To achieve this, we use the result from matrix differential
calculus [14], which combines the derivatives of eigenvalues and eigenvectors based on chain rule: given
∂D ∈ Rn×n and ∂L
∂L
∂Σ =
D((KT (cid:12) (DT ∂L
[i (cid:54)= j],

∂Λ ∈ Rn×n, where L is the loss function, the back-propagate derivatives are ∂L
∂Λ )diag)DT , where K ∈ Rn×n is 0-diagonal and structured as Kij = 1

∂D )) + ( ∂L

σi−σj

5

∂Λ )diag sets all off-diagonal elements of ∂L

and ( ∂L
multiplication. Based on the chain rule, the back-propagated formulations for calculating ∂L
below.

∂Λ to zero. The (cid:12) operator represents element-wise matrix
∂V are shown as

∂L
∂Λ = −

1
2

DT ∂L
∂W

WT DΛ−1

T

∂L
∂W

1

∂L

2 DT W

∂L
∂D = DΛ
∂Σ = D((KT (cid:12) (DT ∂L
∂D
∂L
∂W
2 DT ∂L
∂W

∂V = DΛ− 1

∂c = −1T

DΛ− 1

∂L

∂L

T

d

DΛ− 1

2 +

)) + (

∂L
∂Λ

WT D

∂L
∂W
)diag)DT

2 DT − 2 · 1T

d (V − c1T

d )T (

+ 2(

)s(V − c1T

d ) +

∂L
∂Σ
T

)s

1T
d

1
d

∂L
∂c

∂L
∂Σ

T

∂Σ )s means symmetrizing ∂L

where ( ∂L
decent or other tractable optimization methods to update V. Note that symmetrizing ∂L
on the perturbation theory, since wiggling c or V will make Σ wiggle symmetrically.

∂Σ ). Given ∂L

∂Σ by ( ∂L

∂V , we can apply regular gradient
∂Σ is necessary based

∂Σ )s = 1

2 ( ∂L

+ ∂L

∂Σ

Algorithm 1 Forward pass of OLM.
1: Input: mini-batch input H ∈ Rd×m and parameters: b ∈ Rn×1, V ∈ Rn×d.
2: Output: S ∈ Rn×m and W ∈ Rn×d.
3: Calculate: Σ = (V − 1
d )(V − 1
4: Eigenvalue decomposition: Σ = DΛDT .
5: Calculate W based on Eqn. 3.
6: Calculate S as standard linear module does.

d V1d1T

d V1d1T

d )T .

Algorithm 2 Backward pass of OLM.
1: Input: activation derivative ∂L
2: Output: { ∂L
3: Calculate: ∂L
4: Calculate ∂L
5: Update V and b.

∂H ∈ Rd×m}, V ∈ Rn×d and b ∈ Rn×1.
∂W , ∂L
∂V base on Eqn. 4

∂b and ∂L

∂H as standard linear module does.

∂S ∈ Rn×m and variables from respective forward pass.

3.1 Orthogonal Linear Module

Based on our orthogonal weight normalization method for solving OMDSM, we build up the Orthogonal
Linear Module (OLM) from practical perspective. Algorithm 1 and 2 summarize the forward and backward
pass of OLM, respectively. This module can be an alternative of standard linear module. Based on this, we
can train DNNs with orthogonality constraints by simply substituting it for standard linear module without
any extra efforts. After training, we calculate the weight matrix W based on Eqn. 3. Then W will be saved
and used for inference as the standard module does.

6

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 2: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 4-layer MLP.
We show the training loss curves for different learning rate of ‘EI+QR’, ‘CI+QR’ and ‘CayT’ compared to the baseline
‘plain’ in (a), (b) and (c) respectively. We compare our methods to baselines and report the best performance among all
learning rates based on the training loss for each method in (d).

3.1.1 Convolutional Layer

With regards to the convolutional layer parameterized by weights WC ∈ Rn×d×Fh×Fw where Fh and Fw
are the height and width of the ﬁlter, it takes feature maps X ∈ Rd×h×r as input, where h and r are the
height and width of the feature maps, respectively. We denote ∆ the set of spatial locations and Ω the set of
spatial offsets. For each output feature map k and its spatial location δ ∈ ∆, the convolutional layer computes
the activation {sk,δ} as: sk,δ = (cid:80)d
τ ∈Ω wk,i,τ hi,δ+τ =< wk, hδ >. Here wk eventually can be viewed
as unrolled ﬁlter produced by WC. We thus reshape WC as W ∈ Rn×p where p = d · Fh · Fw, and the
orthogonalization is executed over the unrolled weight matrix W ∈ Rn×(d·Fh·Fw).

(cid:80)

i=1

3.1.2 Group Based Orthogonalization

In previous sections, we assume n <= d, and obtain the solution of OMDSM such that the rows of W is
orthogonal. To handle the case with n > d, we propose the group based orthogonalization method. That is,
we divide the weights {wi}n
i=1 into groups with size NG <= d and the orthogonalization is performed over
each group, such that the weights in each group is orthogonal.

One appealing property of group based orthogonalization is that we can use group size NG to control to
what extent we regularize the networks. Assume NG can be divided by n, the free dimension of embedded
manifold is nd − n(NG + 1)/2 by using orthogonal group method. If we use NG = 1, this method reduces
to Weight Normalization [27] without learnable scalar parameters.

Besides, group based orthogonalization is a practical strategy in real application, especially reducing the
computational burden. Actually, the cost of eigen decomposition with high dimension in GPU is expensive.
When using group with small size (e.g., 64), the eigen decomposition is not the bottleneck of computation,
compared to convolution operation. This make our orthogonal linear module possible to be applied in very
deep and high dimensional CNNs.

3.1.3 Computational Complexity

We show our method is scalable from complexity analysis here and provide empirical results later for
large CNNs. Given a convolutional layer with ﬁlters W ∈ Rn×d×Fh×Fw , and m mini-batch data {xi ∈
Rd×h×w}m
G +
nmdhwFhFw) per iteration, and if we control a small group size NG (cid:28) mhw, it will be close to the standard
convolutional layer as O(nmdhwFhFw).

i=1. The computational complexity of our method with group size NG is O(nNGdFhFw + nN 2

7

4 Experiments

In this section, we ﬁrst conduct comprehensive experiments to explore different methods to solve the OMDSM
problem, and show the advantages of our proposed orthogonal weight normalization solution in terms of the
stability and efﬁciency in optimization. We then evaluate the effectiveness of the proposed method that learns
orthogonal weight matrix in DNNs, by simply replacing our OLM with standard ones on MLPs and CNNs.
Codes to reproduce our results are available from: https://github.com/huangleiBuaa/OthogonalWN.

4.1 Comparing Methods for Solving OMDSM

In this section, we use 3 widely used Riemannian optimization methods for solving OMDSM and compared
two other baselines. For completeness, we provide a brief review for Riemannian optimization shown in
Appendix A.3 and for more details please refer to [1] and references therein.

We design comprehensive experiments on MNIST dataset to compare methods for solving OMDSM.
The compared methods including: (1) ‘EI+QR’: using Riemannian gradient with Euclidean inner product
and QR-retraction [10]; (2) ‘CI+QR’: using Riemannian gradient with canonical inner product and QR-
retraction; (3) ‘CayT’: using the Cayley transformation [35, 33]; (4) ‘QR’: a conventional method that runs
∂W and projects the solution back to the manifold M by
the ordinary gradient descent based on gradient ∂F
QR decomposition; (5) ‘OLMvar’: using orthogonal transformation: Pvar = Λ−1/2DT ; (6) ‘OLM’: our
proposed orthogonal transformation by minimizing distortions: P∗ = DΛ−1/2DT . The baseline is the
standard network without any orthogonal constraints referred to as ‘plain’.

We use MLP architecture with 4 hidden layers. The number of neurons in each hidden layer is 100. We
train the model with stochastic gradient descent and mini-batch size of 1024. We tried a broadly learning rate
in ranges of {0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5}.

We ﬁrstly explored the performance of Riemannian optimization methods for solving OMDSM problem.
Figure 2 (a), (b) and (c) show the training loss curves for different learning rate of ‘EI+QR’, ‘CI+QR’ and
‘CayT’ respectively, compared to the baseline ‘plain’. From Figure 2, we can ﬁnd that under larger learning
rate (e.g., larger than 0.05) these Riemannian optimization methods suffer severe instability and divergence,
even though they show good performance in the initial iterations. They can also obtain stable optimization
behaviours under small learning rate but are signiﬁcantly slower in convergence than the baseline ‘plain’ and
suffer worse performance.

We then compared our proposed method with the baseline ‘plain’ and the conventional method ‘QR’, and
report the best performance among all learning rates based on the training loss for each method in Figure
2 (d). We can ﬁnd that the conventional method ‘QR’ performs stably. However, it also suffers inferior
performance of ﬁnal training loss compared to ‘plain’. The proposed ‘OLM’ works stably and converges
the fastest. Besides, we ﬁnd that ‘OLMvar’ suffered instability, which means that minimizing distortions
formulated by Eqn. 2 is essential to ensure the stability of solving OMDSM.

We also explore 6-layer and 8-layer MLPs and further with mini-batch size of 512 and 256. We observe
the similar phenomena shown in Appendix A.4. Especially with the number of layer increasing, ‘OLM’
shows more advantages compared to other methods. These comprehensive experiments strongly support our
empirical conclusions that: (1) Riemannian optimization methods probably do not work for the OMDSM
problem, and if work, they must be under ﬁne designed algorithms or tuned hyper-parameters; (2) deep
feed-forward neural networks (e.g., MLP in this experiment) equipped with orthogonal weight matrix is
easier for optimization by our ‘OLM’ solution.

8

(a) training error

(b) test error

Figure 3: Performance comparisons in MLP architecture on PIE dataset. We compare the effect of different group size
of OLM.

(a) batch normalization

(b) Adam optimization

Figure 4: Performance comparisons in MLP architecture on PIE dataset by combining (a) batch normalization; (b)
Adam optimization. We evaluate the training error (solid lines) and test error (dash lines marked with triangle).

4.2 MLP Architecture

Now we investigate the performance of OLM in MLP architecture. On PIE face recognition dataset with
11,554 images from 68 classes, we sample 1,340 images as the test set and others as training set. Here,
we employ standard networks (referred as plain) and networks with Weight Normalization (WN) [27] as
baselines for comparisons. WN is one of the most related study that normalizes the weights as unit norm via
re-parameterization as OLM does, but it does not introduce the orthogonality for the weights matrix. For all
methods, we train a 6-layers MLP with the number of neurons in each hidden layer as 128,128,128,128,128,
and Relu as nonlinearity. The mini-batch size is set to 256. We evaluate the training error and test error as a
function with respect to epochs.

4.2.1 Using Different Group Sizes

We explore the effects of group size NG on the performance when applying OLM. In this setup, we employ
stochastic gradient descent (SGD) optimization and the learning rates are selected based on the validation
set (10% samples of the training set) from {0.05, 0.1, 0.2, 0.5, 1}. Figure 3 shows the performance of OLM

9

(a) CIFAR-10

(b) CIFAR-100

Figure 5: Experimental results on VGG-style architectures over CIFAR datasets. We evaluate the training error (solid
lines) and test error (dash lines marked with triangle) with respect to epochs, and all results are averaged over 5 runs.

using different NG (‘OLM-NG’), compared with plain and WN methods. We can ﬁnd that OLM achieves
signiﬁcantly better performance in all cases, which means introducing orthogonality to weight matrix can
largely improve the network performance. Another observation is that though increasing group size would
help improve orthogonalization, too large group size will reduce the performance. This is mainly because a
large NG = 128 provides overmuch regularization. Fortunately, when we add extra learnable scale (indicated
by ‘OLM-scale-128’) to recover the model capacity as described in previous section, it can help to achieve
the best performance.

4.2.2 Combining with Batch Normalization

Batch normalization [13] has been shown to be helpful for training the deep architectures [13, 12]. Here,
we show that OLM enjoys good compatibility to incorporate well with batch normalization, which still
outperforms others in this case. Figure 4 (a) shows the results of training/test error with respect to epochs.
We can see that WN with batch normalization (‘WN+batch’) has no advantages compared with the standard
network with batch normalization (‘batch’), while ‘OLM+batch’ consistently achieves the best performance.

4.2.3 Applying Adam Optimization

We also try different optimization technique such as Adam [16] optimization. The hyper-parameters are
selected from learning rates in {0.001, 0.002, 0.005, 0.01}. We show error rates based on Adam optimization
in Figure 4 (b). From the ﬁgure, we can see OLM also obtains the best performance.

4.3 CNN Architectures

In this section, We evaluate our method on a VGG-style CNN [29], BN-Inception [31, 13], and Wide
Residual Networks [38] for image classiﬁcation, respectively on CIFAR-10 and CIFAR-100 [17]. For each
dataset, We use the ofﬁcial training set of 50k images and the standard test set of 10k images. The data
preprocessing and data argumentation follow the commonly used mean&std normalization and ﬂip translation
as described in [11]. For OLM method, we replace all convolution layers with our OLM modules by default
on CNNs, if we do not specify it. Among all experiments, the group size NG of OLM is set as 64.

10

(a) CIFAR-10

(b) CIFAR-100

Figure 6: Experimental results on BN-Inception over CIFAR datasets. We evaluate the training error (solid lines) and
test error (dash lines marked with triangle) with respect to epochs, and all results are averaged over 5 runs.

4.3.1 VGG-style network

We adopt the 3 × 3 convolutional layer as the following speciﬁcation: → conv(64) → conv(128) →
maxP ool(2, 2, 2, 2) → conv(256) → conv(256) → maxP ool(2, 2, 2, 2) → conv(512) → conv(512)
→ AveP ool(8, 8, 1, 1) → f c(512 × ClassN um). SGD is used as our optimization method with mini-batch
size of 256. The best initial learning rate is chosen from {0.01, 0.05, 0.1} over the validation set of 5k
examples from the training set, and exponentially decayed to 1% in the last (100th) epoch. We set the
momentum to 0.9 and weight decay to 5 × 10−4. Table 1 reports the test error, from which we can ﬁnd OLM
achieves the best performance consistently on both datasets. Figure 5 (a) and (b) show the training and test
errors with respect to epochs on CIFAR-10 and CIFAR-100, respectively. On CIFAR-100, to achieve the
ﬁnal test error of plain as 36.02 %, OLM takes only 17 epochs. Similarly, on CIFAR-10, OLM only takes 21
epochs to achieve the ﬁnal test error of plain as 10.39 %. While on both datasets, ’plain’ takes about 100
epochs. Results demonstrate that OLM converges signiﬁcantly faster in terms of training epochs and achieves
better error rate compared to baselines.

We also study the effect of OLM on different layers. We optionally replace the ﬁrst 2 and 4 convolution
layers with OLM modules (referred as OLM-L2 and OLM-L4 respectively). From Figure 5 and Table 1, we
can ﬁnd that with the numbers of used OLM increasing, the VGG-style network achieves better performance
both in optimization efﬁciency and generalization.

4.3.2 BN-Inception

For BN-inception network, batch normalization [13] is inserted after each linear layer based on original
Inception architecture [31]. Again, we train the network using SGD, with the momentum 0.9, weight decay
5 × 10−4 and the batch size 64. The initial learning rate is set to 0.1 and decays exponentially every two
epochs until the end of 100 epoches with 0.001. Table 2 reports the test error after training and Figure 5 (c)
and (d) show the training/test error with respect to epochs on CIFAR-10 and CIFAR-100, respectively. We
can ﬁnd that OLM converges faster in terms of training epochs and achieve better optimum, compared to
baselines, which indicate consistent conclusions for VGG-style network above.

11

Table 1: Test error (%) on VGG-style over CIFAR datasets. We report the ‘mean ±std’ computed over 5
independent runs.

plain
WN
OLM-L2
OLM-L4
OLM

CIFAR-10
10.39 ± 0.14
10.29± 0.39
10.06 ± 0.23
9.61 ± 0.23
8.61 ± 0.18

CIFAR-100
36.02 ± 0.40
34.66 ± 0.75
35.42 ± 0.32
33.66 ± 0.11
32.58 ± 0.10

CIFAR-10
5.38± 0.18
plain
5.87± 0.35
WN
OLM 4.74± 0.16

CIFAR-100
24.87 ± 0.15
23.85 ± 0.28
22.02 ± 0.13

Table 2: Test error (%) on BN-Inception over CIFAR datasets. We report the ‘mean ±std’ computed over 5
independent runs.

4.3.3 Wide Residual Netwok

Wide Residual Network (WRN) has been reported to achieve state-of-the-art results on CIFARs [38]. We
adopt WRN architecture with depth 28 and width 10 and the same experimental setting as in [38]. Instead
of ZCA whitening, we preprocess the data using per-pixel mean subtract and standard variance divided as
described in [11]. We implement two setups of OLM: (1) replace all the convolutional layers by WRN
(WRN-OLM); (2) only replace the ﬁrst convolutional layer in WRN (WRN-OLM-L1). Table 3 reports the
test errors. We can see that OLM can further improve the state-of-the-art results achieved by WRN. For
example, on CIFAR-100, our method WRN-OLM achieves the best 18.61 test error, compared to 20.04 of
WRN reported in [38]. Another interesting observation is that WRN-OLM-L1 obtains the best performance
on CIFAR-10 with test error as 3.73%, compare to 4.17% of WRN, which means that we can improve
residual networks by only constraining the ﬁrst convolution layer orthogonal and the extra computation cost
is negligible.

4.3.4 Computation Cost

We also evaluate computational cost per iteration in our current Torch-based implementation, where the
convolution relies on the fastest cudnn package. In the small VGG-style architecture with batch size of 256,
OLM costs 0.46s, while plain and WN cost 0.26s and 0.38s, respectively. On large WRN network, OLM costs
3.12s compared to 1.1s of plain. Note that, our current implementation of OLM can be further optimized.

4.4 Large Scale Classiﬁcation on ImageNet Challenge

To further validate the effectiveness of OLM on large-scale dataset, we employ ImageNet 2012 consisting of
more than 1.2M images from 1,000 classes [26]. We use the given 1.28M labeled images for training and
the validation set with 50k images for testing. We evaluate the classiﬁcation performance based on top-5

12

Table 3: Test errors (%) of different methods on CIFAR-10 and CIFAR-100. For OLM, we report the ‘mean
±std’ computed over 5 independent runs. ‘WRN-28-10*’ indicates the new results given by authors on their
Github.

pre-Resnet-1001
WRN-28-10
WRN-28-10*
WRN-28-10-OLM (ours)
WRN-28-10-OLM-L1 (ours)

CIFAR-10
4.62
4.17
3.89
3.73 ± 0.12
3.82 ± 0.19

CIFAR-100
22.71
20.04
18.85
18.76 ± 0.40
18.61 ± 0.14

Table 4: Top-5 test error (%, single model and single-crop) on ImageNet dataset.

AlexNet BN-Inception ResNet
12.5
9.83

9.84
9.68

plain
20.91
OLM 20.43

Pre-ResNet
9.79
9.45

error. We apply the well-known AlexNet [18] with batch normalization inserted after the convolution layers,
BN-Inception2, ResNet [11] with 34 layers and its advanced version Pre-ResNet [12]3. In AlexNet and
BN-Inception, we replace all the convolution layers with OLM modules for our method, and in ResNet and
Pre-ResNet, we only replace the ﬁrst convolution layer with OLM module, which is shown effective with
negligible computation cost based on previous experiment.

We run our experiments on one GPU. To guarantee a fair comparison between our method with the
baseline, we keep all the experiments settings the same as the publicly available Torch implementation (e.g.,
we apply stochastic gradient descent with momentum of 0.9, weight decay of 0.0001, and set the initial
learning rate to 0.1). The exception is that we use mini-batch size of 64 and 50 training epochs considering
the GPU memory limitations and training time costs. Regarding learning rate annealing, we use exponential
decay to 0.001, which has slightly better performance than the method of lowering by a factor of 10 after
epoch 20 and epoch 40 for each method. The ﬁnal test errors are shown in Table 4. We can ﬁnd that our
proposed OLM method obtains better performance compared to the baselines over AlexNet, BN-Inception,
ResNet and Pre-ResNet architectures.

5 Related Work and Discussion

In optimization community, there exist methods to solve the optimization problem over matrix manifolds with
orthogonal constraints [32, 2, 34, 1, 5]. They are usually limited for one manifold (one linear mapping) [5],
and are mainly based on full batch gradient. When applying to DNNs, it suffered instability in convergence
or inferior performance, as observed by either [10, 33] or our experiments.

In deep learning community, there exist researches using orthogonal matrix [3, 35, 7, 33] or normalization
techniques [13, 27, 37] to avoid the gradient vanish and explosion problem. Arjovsky et al. [3] introduced the
orthogonal matrix for the hidden to hidden transformation in RNN. They constructed an expressive unitary

2We use the public Torch implementation of AlexNet and BN-Inception available on: https://github.com/soumith/imagenet-

multiGPU.torch

3We use the public Torch implementation available on: https://github.com/facebook/fb.resnet.torch

13

weight matrix by composing several structured matrices that act as building blocks with parameters to be
learned. Wisdom et al. [35] pointed out that Arjovsky’s method [3] has restricted representational capacity
and proposed a method of optimizing a full-capacity unitary matrix by using Reimannian gradient in canonical
inner product with Cayley transformation [35, 33]. Dorobantu et al. [7] also proposed a simple method of
updating orthogonal linear transformations in RNN in a way that maintains orthogonality. Vorontsov [33]
advocate soft constraints on orthogonality when they ﬁnd that hard constraints on orthogonality can negatively
affect the speed of convergence and model performance. However, these methods are limited for the hidden
to hidden transformation in RNN because both methods require the weight matrix to be square matrix. Our
methods are more general and can adapt to situations where the weight matrix are not square, especially for
deep Convolutional Neural Networks (CNNs).

Recently, Harandi and Fernando [10] have proposed Stiefel layer to guarantee fully connected layer
to be orthogonal by using Reimannian gradient in Euclidean inner product with QR-retraction. However,
this method only shows the experimental results of replacing the last one or two layers of neural networks
with Stiefel layers. Besides, this method is not stable as shown in their experiments: when they constrain the
last two fully connected layer to be orthogonal in VGG networks, the accuracy performance is low. We also
observe similar phenomena as shown in Section 4.1 that ‘EI+QR’ shows instability in convergence. Ozay and
Okatani [22] also used Riemannian optimization to guarantee convolutional kernels within a channel are
orthogonal, while our ensure ﬁlters (among channels) orthogonal. Some methods [36, 25] used orthogonal
regularizer as a penalty posed on the objective function. These methods can not learn orthogonal ﬁlters,
because they relax the constraints and only penalize their violations. The solution of these methods is on
infeasible set of the optimization problem with orthogonality constraint while our on feasible set.

6 Conclusions and Further Work

We formulate learning orthogonal linear transformation in DNNs as Optimization over Multiple Dependent
Stiefel Manifolds (OMDSM) and propose the Orthogonal Weight Normalization method to solve it, which is
stable and can be applied to large and deep networks. Base on this solution, we design Orthogonal Linear
Module (OLM) which can be applied as an alternative to standard linear module. We show that neural
networks equipped with OLM can improve optimization efﬁciency and generalization ability. In addition,
new deep architectures that address domain-speciﬁc representation can also beneﬁt from the proposed method
by simply replacing standard linear module with OLM.

Various shallow dimensional reduction methods have been uniﬁed under the optimization framework
with orthogonality constraints [5]. Our method has potentials to improve the performance of corresponding
unsupervised [23] and semi-supervised methods [24] in DNNs. Besides, our method has great potential to
be used in improving the robust of the networks to adversarial examples [4].

References

Press, Princeton, NJ, 2008.

Optimization, 22(1):135–158, 2012.

[1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University

[2] Pierre-Antoine Absil and Jerome Malick. Projection-like retractions on matrix manifolds. SIAM Journal on

[3] Martín Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In ICML, 2016.

[4] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks:

Improving robustness to adversarial examples. In ICML, 2017.

14

[5] John P. Cunningham and Zoubin Ghahramani. Linear dimensionality reduction: Survey, insights, and generaliza-

tions. J. Mach. Learn. Res., 16(1):2859–2900, January 2015.

[6] Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and Koray Kavukcuoglu. Natural neural networks. In

NIPS, 2015.

[7] Victor Dorobantu, Per Andre Stromhaug, and Jess Renteria. Dizzyrnn: Reparameterizing recurrent neural networks

for norm-preserving backpropagation. CoRR, abs/1612.04035, 2016.

[8] Y. C. Eldar and A. V. Oppenheim. Mmse whitening and subspace whitening. IEEE Trans. Inf. Theor., 49(7):1846–

1851, September 2006.

[9] Paul H. Garthwaite, Frank Critchley, Karim Anaya-Izquierdo, and Emmanuel Mubwandarikwa. Orthogonalization

of vectors with minimal adjustment. Biometrika, 99(4):787 – 798, 2012.

[10] Mehrtash Harandi and Basura Fernando. Generalized backpropagation, etude de cas: Orthogonality. CoRR,

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In

abs/1611.05927, 2016.

CVPR, 2016.

ECCV, 2016.

[13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal

[14] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Training deep networks with structured layers by

covariate shift. In ICML, 2015.

matrix backpropagation. In ICCV, 2015.

[15] Tetsuya Kaneko, Simone G. O. Fiori, and Toshihisa Tanaka. Empirical arithmetic averaging over the compact

stiefel manifold. IEEE Trans. Signal Processing, 61(4):883–894, 2013.

[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.

[17] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional neural

[19] Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. In NIPS, 1992.

[20] Yann LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. Efﬁicient backprop. In Neural Networks:

[21] Zakaria Mhammedi, Andrew D. Hellicar, Ashfaqur Rahman, and James Bailey. Efﬁcient orthogonal parametrisa-

tion of recurrent neural networks using householder reﬂections. In ICML, 2017.

[22] Mete Ozay and Takayuki Okatani. Optimization on submanifolds of convolution kernels in cnns. CoRR,

networks. In NIPS. 2012.

Tricks of the Trade, 1998.

abs/1610.07008, 2016.

IJCAI, 2017.

[23] Feiping Nie Yuan Yuan Qi Wang, Zequn Qin. Convolutional 2d lda for nonlinear dimensionality reduction. In

[24] Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised learning
with ladder networks. In Proceedings of the 28th International Conference on Neural Information Processing
Systems, NIPS’15, pages 3546–3554, 2015.

[25] Pau Rodríguez, Jordi Gonzàlez, Guillem Cucurull, Josep M. Gonfaus, and F. Xavier Roca. Regularizing cnns with

locally constrained decorrelations. In ICLR, 2017.

15

[26] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.

[27] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training

of deep neural networks. In NIPS, 2016.

[28] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning

in deep linear neural networks. CoRR, abs/1312.6120, 2013.

[29] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In

ICLR, 2015.

[30] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A
simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res., 15(1):1929–1958, January 2014.

[31] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.

Going deeper with convolutions. In CVPR, 2015.

[32] Hemant D. Tagare. Notes on optimization on stiefel manifolds. Technical report, Yale University, 2011.

[33] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent

networks with long term dependencies. In ICML, 2017.

[34] Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints. Math. Program.,

142(1-2):397–434, 2013.

[35] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent

neural networks. In NIPS, pages 4880–4888. 2016.

[36] Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training
extremely deep convolutional neural networks with orthonormality and modulation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.

[37] Adams Wei Yu, Lei Huang, Qihang Lin, Ruslan Salakhutdinov, and Jaime G. Carbonell. Block-normalized

gradient method: An empirical study for training deep neural network. CoRR, abs/1707.04822, 2017.

[38] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.

[39] Jianping Zhou, Minh N. Do, and Jelena Kovacevic. Special paraunitary matrices, cayley transform, and multidi-

mensional orthogonal ﬁlter banks. IEEE Trans. Image Processing, 15(2):511–519, 2006.

A Appendix

A.1 Proof of Theorem 1
Theorem 1. Let s = Wx, where WWT = I and W ∈ Rn×d. (1) Assume the mean of x is Ex[x] = 0, and
covariance matrix of x is cov(x) = σ2I. Then Es[s] = 0, cov(s) = σ2I. (2) If n = d, we have (cid:107)s(cid:107) = (cid:107)x(cid:107). (3) Given
the back-propagated gradient ∂L

∂s , we have (cid:107) ∂L

∂x (cid:107) = (cid:107) ∂L

∂s (cid:107).

Proof. (1) It’s easy to calculate:

Es[s] = wT Ex[x] = wT µ1 = 0

(4)

16

The covariance of s is given by

cov(s) = Es[s − Es[s]]2

= Ex[W(x − Ex[x])]2
= Ex[W(x − Ex[x])] · Ex[W(x − Ex[x])]T
= WEx[(x − Ex[x])] · Ex[(x − Ex[x])]T WT
= Wcov(x)WT
= Wσ2IWT
= σ2WWT = σ2

(2) If n = d, W is orthogonal matrix, therefore WT W = WWT = I. (cid:107)s(cid:107) = sT s = xT WT Wx = xT x = (cid:107)x(cid:107).
(3) As similar as the proof of (2), (cid:107) ∂L

∂s W(cid:107) = ∂L

∂s WWT ∂L

∂x (cid:107) = (cid:107) ∂L

= (cid:107) ∂L
∂s (cid:107)

= ∂L
∂s

∂L
∂s

∂s

T

T

A.2 Derivation of Minimizing Orthogonal Vectors Transformation Problem

Given a matrix V ∈ Rn×d where n ≤ d and Rank(V) = n, we expect to transform it by W = PV such that
WWT = I where W ∈ Rn×d and P ∈ Rn×n. Besides, we minimize the distortion between the transformed matrix
W and the original matrix V in a least square way. This can be formulated as:

minP tr((W − V)(W − V)T )
s.t. W = PV and WWT = I

where tr(A) denotes the trace of the matrix A. The derivation of this problem is based on paper [8] where a similar
problem is considered in the context of data whitening. To solve this problem, we ﬁrst calculate the covariance matrix
Σ = VVT . Since Rank(V) = n, we have Σ is real positive deﬁnite and thus invertible with positive eigenvalues.
By performing eigenvalue decomposition on Σ, we have Σ = DΛDT where Λ = diag(σ1, . . . , σn) and D are the
eigenvalues and eigenvectors respectively. Given the constraint WWT = I, we have PVVT PT = I. Therefor, we
have PΣPT = I.

By construction, we have

where M is an n × n orthonomal matrix with MMT = MT M = I. For convenience, we use Σ− 1
Therefore, we have:

2 = DΛ−1/2DT ,

P = MΣ− 1

2

tr((W − V)(W − V)T )

= tr(WWT − VWT − WVT + VVT )
= tr(I) + tr(Σ) − tr(VWT ) − tr((VWT )T )
= tr(I) + tr(Σ) − 2tr(VWT ))
= d + tr(Σ) − 2tr(ΣPT ))
= d + tr(Σ) − 2tr(DΛDT DΛ−1/2DT MT )
= d + tr(Σ) − 2tr(DΛ1/2DT MT )

Minimizing 8 with respect to P is equivalent to maximizing tr(DΛ1/2DT MT ) with respect to M. Note that
tr(DΛ1/2DT MT ) = tr(Λ1/2DT MT D) = tr(Λ1/2Q) with Q = DT MT D is orthogonal matrix. One important
point fact is that Qij ≤ 1, otherwise the norm of the i-th row or the j-th column of Q will larger than 1, which is
contradictory to that Q is real orthogonal matrix. Therefore, maximizing tr(Λ1/2Q = (cid:80)n
ii Qii) guarantees
Q = I. We thus have M = DIDT = I. Therefore, we get the solution P = Σ− 1

2 = DΛ−1/2DT .

i=1 Λ1/2

17

(5)

(6)

(7)

(8)

A.3 Riemannian Optimization over Stiefel Manifold

For comparison purpose, here we review the Riemannian optimization over Stiefel manifold brieﬂy and for more
details please refer to [1] and references therein. The objective is arg minW∈M f (W), where f is a real-value smooth
function over M = {W ∈ Rn×d : WT W = I, n (cid:62) d}. Note that in this section, we follow the common description
for Riemannian optimization over Stiefel manifold with the columns of W being d orthonormal vectors in Rn, and
therefore with the constraints WT W = I. It is different to the description of our formulation in the paper with
constraints WWT = I and n (cid:54) d.

Conventional optimization techniques are based gradient descent method over manifold by iteratively seeking for
updated points Wt ∈ M. In each iteration t, the keys are: (1) ﬁnding the Riemannian gradient GM f (Wt) ∈ TWt
where TWt is the tangent space of M at current point Wt; and (2) ﬁnding the descent direction and ensuring that the
new points is on the manifold M.

For obtaining the Riemannian gradient GM f (W), the inner dot should be deﬁned in TW. There are two extensively
1 X2)
2 WWT )X2) where X1, X2 ∈ TW and tr(·) denote the

used inner products for tangent space of Stiefel manifold [34]: (1) Euclidean inner product: < X1, X2 >e= tr(XT
1 (I − 1
and (2) canonical inner product: < X1, X2 >c= tr(XT
trace of the matrix. Based on these two inner products, the respective Riemannian gradient can be obtained as [34]:

(9)

(10)

(11)

(12)

and

GM

e f (W) =

∂f
∂W

− W

T

∂f
∂W

W

GM

c f (W) =

∂f
∂W

−

1
2

(WWT ∂f
∂W

+ W

W)

T

∂f
∂W

where ∂f

∂W is the ordinary gradient.

Given the Riemannian gradient, the next step is to ﬁnd the descent direction and guarantee that the new point is on
the manifold M, which can be supported by the so called operation retraction. One well recommended retractionis the
QR-Decomposition-Type retraction [15, 10] that maps a tangent vector of TW onto M by: PW(Z) = qf (W + Z),
where qf (·) denotes the Q factor of the QR decomposition with Q ∈ M, and the R-factor is an upper-trangular matrix
with strictly positive elements on its main diagonal such that the decomposition is unique [15]. Given the Riemannian
gradient GM f (W) and the learning rate η, the new point is:

Another well known technique to jointly move in the descent direction and make sure the new solution on the
manifold M is Cayley transformation [34, 35, 33]. It produces the feasible solution Wk+1 with the current solution
Wk by:

Wt+1 = qf (Wt − η GM f (W))

Wt+1 = (I +

At)−1(I −

At)W

η
2

η
2

where η is the learning rate and At = ∂F
∂Wt
the tangent space.

T

Wt − WT
t

∂F
∂Wt

that is induced by the deﬁned canonical inner product in

A.4 More Experimental Results

Here, we show more experimental results for Solving OMDSM on MNIST dataset under MLP architectures. The
experimental setups are described in the paper. Figure 7 and 8 show the results under the respective 6-layer and 8-layer
MLPs with mini-batch size of 1024. We also train the model with mini-batch size of 512 and the results under the
4-layer, 6-layer and 8-layer MLPs are shown in Figure 9, 10 and 11 respectively. We further train the model with
mini-batch size of 256 and the results under the 4-layer, 6-layer and 8-layer MLPs are shown in Figure 12, 13 and 14.
These comprehensive experiments strongly support our empirical conclusions that: (1) Riemannian optimization
methods probably do not work for the OMDSM problem, and if work, they must be under ﬁne designed algorithms or
tuned hyper-parameters; (2) deep feed-forward neural networks (e.g., MLP in this experiment) equipped with orthogonal
weight matrix is easier for optimization by our ‘OLM’ solution.

18

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 7: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 6-layer MLP.
We train the model with batch size of 1024 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 8: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 8-layer MLP.
We train the model with batch size of 1024 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 9: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 4-layer MLP.
We train the model with batch size of 512 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 10: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 6-layer MLP.
We train the model with batch size of 512 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

19

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 11: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 8-layer MLP.
We train the model with batch size of 512 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 12: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 4-layer MLP.
We train the model with batch size of 256 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 13: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 6-layer MLP.
We train the model with batch size of 256 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

(a) EI+QR

(b) CI+QR

(c) CayT

(d) Our OLM

Figure 14: Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 8-layer MLP.
We train the model with batch size of 256 and show the training loss curves for different learning rate of ‘EI+QR’,
‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to
baselines and report the best performance among all learning rates based on the training loss for each method in (d).

20

