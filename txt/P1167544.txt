7
1
0
2
 
v
o
N
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
8
5
8
0
.
5
0
7
1
:
v
i
X
r
a

MMD GAN: Towards Deeper Understanding of
Moment Matching Network

Chun-Liang Li1,∗ Wei-Cheng Chang1,∗ Yu Cheng2 Yiming Yang1 Barnabás Póczos1

1 Carnegie Mellon University,

2AI Foundations, IBM Research

{chunlial,wchang2,yiming,bapoczos}@cs.cmu.edu
(∗ denotes equal contribution)

chengyu@us.ibm.com

Abstract

Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the discriminator
in GAN with a two-sample test based on kernel maximum mean discrepancy
(MMD). Although some theoretical guarantees of MMD have been studied, the
empirical performance of GMMN is still not as competitive as that of GAN on
challenging and large benchmark datasets. The computational efﬁciency of GMMN
is also less desirable in comparison with GAN, partially due to its requirement for
a rather large batch size during the training. In this paper, we propose to improve
both the model expressiveness of GMMN and its computational efﬁciency by
introducing adversarial kernel learning techniques, as the replacement of a ﬁxed
Gaussian kernel in the original GMMN. The new approach combines the key ideas
in both GMMN and GAN, hence we name it MMD GAN. The new distance measure
in MMD GAN is a meaningful loss that enjoys the advantage of weak∗ topology
and can be optimized via gradient descent with relatively small batch sizes. In our
evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA
and LSUN, the performance of MMD GAN signiﬁcantly outperforms GMMN, and
is competitive with other representative GAN works.

1

Introduction

The essence of unsupervised learning models the underlying distribution PX of the data X . Deep
generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with
promising results. However, modeling arbitrary density is a statistically challenging task [3]. In many
applications, such as caption generation [4], accurate density estimation is not even necessary since
we are only interested in sampling from the approximated distribution.
Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a
base distribution PZ over Z, such as Gaussian distribution, then trains a transformation network gθ
such that Pθ ≈ PX , where Pθ is the underlying distribution of gθ(z) and z ∼ PZ . During the training,
GAN-based algorithms require an auxiliary network fφ to estimate the distance between PX and Pθ.
Different probabilistic (pseudo) metrics have been studied [5–8] under GAN framework.
Instead of training an auxiliary network fφ for measuring the distance between PX and Pθ, Generative
moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11],
which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.
During the training, gθ is trained to pass the hypothesis test (minimize MMD distance). [11] shows
even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the
empirical performance of GMMN does not meet its theoretical properties. There is no promising
empirical results comparable with GAN on challenging benchmarks [12, 13]. Computationally,

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

it also requires larger batch size than GAN needs for training, which is considered to be less
efﬁcient [9, 10, 14, 8]

In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels
instead of ﬁxed Gaussian kernels to have better hypothesis testing power. The main contributions of
this work are:

• In Section 2, we prove that training gθ via MMD with learned kernels is continuous and differen-
tiable, which guarantees the model can be trained by gradient descent. Second, we prove a new
distance measure via kernel learning, which is a sensitive loss function to the distance between
PX and Pθ (weak∗ topology). Empirically, the loss decreases when two distributions get closer.
• In Section 3, we propose a practical realization called MMD GAN that learns generator gθ with
the adversarially trained kernel. We further propose a feasible set reduction to speed up and
stabilize the training of MMD GAN.

• In Section 5, we show that MMD GAN is computationally more efﬁcient than GMMN, which can
be trained with much smaller batch size. We also demonstrate that MMD GAN has promising
results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To
our best knowledge, we are the ﬁrst MMD based work to achieve comparable results with other
GAN works on these datasets.

Finally, we also study the connection to existing works in Section 4. Interestingly, we show Wasser-
stein GAN [8] is the special case of the proposed MMD GAN under certain conditions. The uniﬁed
view shows more connections between moment matching and GAN, which can potentially inspire
new algorithms based on well-developed tools in statistics [15]. Our experiment code is available at
https://github.com/OctoberChang/MMD-GAN.

2 GAN, Two-Sample Test and GMMN

i=1, where xi ∈ X and xi ∼ PX . If we are interested in sampling
Assume we are given data {xi}n
from PX , it is not necessary to estimate the density of PX . Instead, Generative Adversarial Network
(GAN) [5] trains a generator gθ parameterized by θ to transform samples z ∼ PZ , where z ∈ Z,
into gθ(z) ∼ Pθ such that Pθ ≈ PX . To measure the similarity between PX and Pθ via their
samples {x}n
j=1 during the training, [5] trains the discriminator fφ parameterized
by φ for help. The learning is done by playing a two-player game, where fφ tries to distinguish xi
and gθ(zj) while gθ aims to confuse fφ by generating gθ(zj) similar to xi.

i=1 and {gθ(zj)}n

On the other hand, distinguishing two distributions by ﬁnite samples is known as Two-Sample Test in
statistics. One way to conduct two-sample test is via kernel maximum mean discrepancy (MMD) [11].
Given two distributions P and Q, and a kernel k, the square of MMD distance is deﬁned as

Mk(P, Q) = (cid:107)µP − µQ(cid:107)2

H = EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

Theorem 1. [11] Given a kernel k, if k is a characteristic kernel, then Mk(P, Q) = 0 iff P = Q.

GMMN: One example of characteristic kernel is Gaussian kernel k(x, x(cid:48)) = exp((cid:107)x − x(cid:48)(cid:107)2). Based
on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains gθ by

Mk(PX , Pθ),

min
θ

(1)

with a ﬁxed Gaussian kernel k rather than training an additional discriminator f as GAN.

2.1 MMD with Kernel Learning

In practice we use ﬁnite samples from distributions to estimate MMD distance. Given X =
{x1, · · · , xn} ∼ P and Y = {y1, · · · , yn} ∼ Q, one estimator of Mk(P, Q) is

ˆMk(X, Y ) =

k(xi, x(cid:48)

i) −

k(xi, yj) +

k(yj, y(cid:48)

j).

1
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=i(cid:48)

2
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=j

1
(cid:1)
(cid:0)n
2

(cid:88)

j(cid:54)=j(cid:48)

Because of the sampling variance, ˆM (X, Y ) may not be zero even when P = Q. We then conduct
hypothesis test with null hypothesis H0 : P = Q. For a given allowable probability of false rejection α,

2

we can only reject H0, which imply P (cid:54)= Q, if ˆM (X, Y ) > cα for some chose threshold cα > 0.
Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to [11] for
more details.
Intuitively, if kernel k cannot result in high MMD distance Mk(P, Q) when P (cid:54)= Q, ˆMk(P, Q) has
more chance to be smaller than cα. Then we are unlikely to reject the null hypothesis H0 with ﬁnite
samples, which implies Q is not distinguishable from P. Therefore, instead of training gθ via (1) with
a pre-speciﬁed kernel k as GMMN, we consider training gθ via

min
θ

max
k∈K

Mk(PX , Pθ),

(2)

(3)

which takes different possible characteristic kernels k ∈ K into account. On the other hand, we
could also view (2) as replacing the ﬁxed kernel k in (1) with the adversarially learned kernel
arg maxk∈K Mk(PX , Pθ) to have stronger signal where P (cid:54)= Pθ to train gθ. We refer interested
readers to [16] for more rigorous discussions about testing power and increasing MMD distances.

However, it is difﬁcult to optimize over all characteristic kernels when we solve (2). By [11, 17] if f
is a injective function and k is characteristic, then the resulted kernel ˜k = k ◦ f , where ˜k(x, x(cid:48)) =
k(f (x), f (x(cid:48))) is still characteristic. If we have a family of injective functions parameterized by φ,
which is denoted as fφ, we are able to change the objective to be

min
θ

max
φ

Mk◦fφ(PX , Pθ),

In this paper, we consider the case that combining Gaussian kernels with injective functions fφ, where
˜k(x, x(cid:48)) = exp(−(cid:107)fφ(x) − fφ(x)(cid:48)(cid:107)2). One example function class of f is {fφ|fφ(x) = φx, φ > 0},
which is equivalent to the kernel bandwidth tuning. A more complicated realization will be discussed
in Section 3. Next, we abuse the notation Mfφ(P, Q) to be MMD distance given the composition
kernel of Gaussian kernel and fφ in the following. Note that [18] considers the linear combination
of characteristic kernels, which can also be incorporated into the discussed composition kernels. A
more general kernel is studied in [19].

2.2 Properties of MMD with Kernel Learning

[8] discuss different distances between distributions adopted by existing deep learning algorithms, and
show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7],
except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training.
From (3), we train gθ via minimizing maxφ Mfφ(PX , Pθ). Next, we show maxφ Mfφ(PX , Pθ) also
enjoys the advantage of being a continuous and differentiable objective in θ under mild assumptions.
Assumption 2. g : Z × Rm → X is locally Lipschitz, where Z ⊆ Rd. We will denote gθ(z) the
evaluation on (z, θ) for convenience. Given fφ and a probability distribution Pz over Z, g satisﬁes
Assumption 2 if there are local Lipschitz constants L(θ, z) for fφ ◦ g, which is independent of φ, such
that Ez∼Pz [L(θ, z)] < +∞.
Theorem 3. The generator function gθ parameterized by θ is under Assumption 2. Let PX be a ﬁxed
distribution over X and Z be a random variable over the space Z. We denote Pθ the distribution
of gθ(Z), then maxφ Mfφ(PX , Pθ) is continuous everywhere and differentiable almost everywhere
in θ.

If gθ is parameterized by a feed-forward neural network, it satisﬁes Assumption 2 and can be trained
via gradient descent as well as propagation, since the objective is continuous and differentiable
followed by Theorem 3. More technical discussions are shown in Appendix B.
Theorem 4. (weak∗ topology) Let {Pn} be a sequence of distributions. Considering n → ∞,
D−→ PX , where D−→ means converging in
under mild Assumption, maxφ Mfφ(PX , Pn) → 0 ⇐⇒ Pn
distribution [3].

Theorem 4 shows that maxφ Mfφ(PX , Pn) is a sensible cost function to the distance between PX
and Pn. The distance is decreasing when Pn is getting closer to PX , which beneﬁts the supervision
of the improvement during the training. All proofs are omitted to Appendix A. In the next section,
we introduce a practical realization of training gθ via optimizing minθ maxφ Mfφ(PX , Pθ).

3

3 MMD GAN

To approximate (3), we use neural networks to parameterized gθ and fφ with expressive power.
For gθ, the assumption is locally Lipschitz, where commonly used feed-forward neural networks
satisfy this constraint. Also, the gradient (cid:53)θ (maxφ fφ ◦ gθ) has to be bounded, which can be
done by clipping φ [8] or gradient penalty [20]. The non-trivial part is fφ has to be injective.
For an injective function f , there exists an function f −1 such that f −1(f (x)) = x, ∀x ∈ X and
f −1(f (g(z))) = g(z), ∀z ∈ Z 1, which can be approximated by an autoencoder. In the following,
we denote φ = {φe, φd} to be the parameter of discriminator networks, which consists of an encoder
fφe , and train the corresponding decoder fφd ≈ f −1 to regularize f . The objective (3) is relaxed to
be

min
θ

max
φ

Mfφe

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe(y))(cid:107)2.

(4)

Note that we ignore the autoencoder objective when we train θ, but we use (4) for a concise
presentation. We note that the empirical study suggests autoencoder objective is not necessary to
lead the successful GAN training as we will show in Section 5, even though the injective property is
required in Theorem 1.

The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks gθ and fφ
in a minmax formulation, while the meaning of the objective is different. In [5], fφe is a discriminator
(binary) classiﬁer to distinguish two distributions. In the proposed algorithm, distinguishing two
distribution is still done by two-sample test via MMD, but with an adversarially learned kernel
parametrized by fφe . gθ is then trained to pass the hypothesis test. More connection and difference
with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed
algorithm MMD GAN. We present an implementation with the weight clipping in Algorithm 1, but
one can easily extend to other Lipschitz approximations, such as gradient penalty [20].

Algorithm 1: MMD GAN, our proposed algorithm.
input

:α the learning rate, c the clipping parameter, B the batch size, nc the number of iterations of
discriminator per generator update.

initialize generator parameter θ and discriminator parameter φ;
while θ has not converged do
for t = 1, . . . , nc do

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe (y))(cid:107)2

Sample a minibatches {xi}B
gφ ← ∇φMfφe
φ ← φ + α · RMSProp(φ, gφ)
φ ← clip(φ, −c, c)
Sample a minibatches {xi}B
gθ ← ∇θMfφe
θ ← θ − α · RMSProp(θ, gθ)

(P(X ), P(gθ(Z)))

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

Encoding Perspective of MMD GAN: Besides from using kernel selection to explain MMD GAN,
the other way to see the proposed MMD GAN is viewing fφe as a feature transformation function,
and the kernel two-sample test is performed on this transformed feature space (i.e., the code space of
the autoencoder). The optimization is ﬁnding a manifold with stronger signals for MMD two-sample
test. From this perspective, [9] is the special case of MMD GAN if fφe is the identity mapping
function. In such circumstance, the kernel two-sample test is conducted in the original data space.

3.1 Feasible Set Reduction

Theorem 5. For any fφ, there exists f (cid:48)
Ez[fφ(cid:48)(gθ(z))].

φ such that Mfφ(Pr, Pθ) = Mf (cid:48)

φ

(Pr, Pθ) and Ex[fφ(x)] (cid:23)

With Theorem 5, we could reduce the feasible set of φ during the optimization by solving

minθ maxφ Mfφ(Pr, Pθ)

s.t. E[fφ(x)] (cid:23) E[fφ(gθ(z))]

1Note that injective is not necessary invertible.

4

which the optimal solution is still equivalent to solving (2).

However, it is hard to solve the constrained optimization problem with backpropagation. We relax
the constraint by ordinal regression [21] to be

min
θ

max
φ

Mfφ(Pr, Pθ) + λ min (cid:0)E[fφ(x)] − E[fφ(gθ(z))], 0(cid:1),

which only penalizes the objective when the constraint is violated. In practice, we observe that
reducing the feasible set makes the training faster and stabler.

4 Related Works

There has been a recent surge on improving GAN [5]. We review some related works here.

If we composite fφ with linear kernel instead of Gaussian kernel, and

Connection with WGAN:
restricting the output dimension h to be 1, we then have the objective
(cid:107)E[fφ(x)] − E[fφ(gθ(z))](cid:107)2.

(5)

min
θ

max
φ
Parameterizing fφ and gθ with neural networks and assuming ∃φ(cid:48) ∈ Φ such f (cid:48)
φ = −fφ, ∀Φ, recovers
Wasserstein GAN (WGAN) [8] 2. If we treat fφ(x) as the data transform function, WGAN can
be interpreted as ﬁrst-order moment matching (linear kernel) while MMD GAN aims to match
inﬁnite order of moments with Gaussian kernel form Taylor expansion [9]. Theoretically, Wasserstein
distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, [22] show neural
networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we
demonstrate matching high-order moments beneﬁts the results. [23] also propose McGAN that
matches second order moment from the primal-dual norm perspective. However, the proposed
algorithm requires matrix (tensor) decompositions because of exact moment matching [24], which
is hard to scale to higher order moment matching. On the other hand, by giving up exact moment
matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions
are in Appendix B.3.

Difference from Other Works with Autoencoders: Energy-based GANs [7, 25] also utilizes
the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes
the reconstruction error of real samples x while maximize the reconstruction error of generated
samples gθ(z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing
the reconstruction errors of both real samples x and generated samples gθ(z). Also, [8] show EBGAN
approximates total variation, with the drawback of discontinuity, while MMD GAN optimizes MMD
distance. The other line of works [2, 26, 9] aims to match the AE codespace f (x), and utilize the
decoder fdec(·). [2, 26] match the distribution of f (x) and z via different distribution distances and
generate data (e.g. image) by fdec(z). [9] use MMD to match f (x) and g(z), and generate data via
fdec(g(z)). The proposed MMD GAN matches the f (x) and f (g(z)), and generates data via g(z)
directly as GAN. [27] is similar to MMD GAN but it considers KL-divergence without showing
continuity and weak∗ topology guarantee as we prove in Section 2.

Other GAN Works:
In addition to the discussed works, there are several extended works of GAN.
[28] proposes using the linear kernel to match ﬁrst moment of its discriminator’s latent features. [14]
considers the variance of empirical MMD score during the training. Also, [14] only improves the
latent feature matching in [28] by using kernel MMD, instead of proposing an adversarial training
framework as we studied in Section 2. [29] uses Wasserstein distance to match the distribution of
autoencoder loss instead of data. One can consider to extend [29] to higher order matching based on
the proposed MMD GAN. A parallel work [30] use energy distance, which can be treated as MMD
GAN with different kernel. However, there are some potential problems of its critic. More discussion
can be referred to [31].

5 Experiment

We train MMD GAN for image generation on the MNIST [32], CIFAR-10 [33], CelebA [13],
and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M

2Theoretically, they are not equivalent but the practical neural network approximation results in the same

algorithm.

5

respectively. All the samples images are generated from a ﬁxed noise random vectors and are not
cherry-picked.

Network architecture: In our experiments, we follow the architecture of DCGAN [34] to design gθ
by its generator and fφ by its discriminator except for expanding the output layer of fφ to be h
dimensions.

Kernel designs: The loss function of MMD GAN is implicitly associated with a family of character-
istic kernels. Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF
kernels k(x, x(cid:48)) = (cid:80)K
q=1 kσq (x, x(cid:48)) where kσq is a Gaussian kernel with bandwidth parameter σq.
Tuning kernel bandwidth σq optimally still remains an open problem. In this works, we ﬁxed K = 5
and σq to be {1, 2, 4, 8, 16} and left the fφ to learn the kernel (feature representation) under these σq.

Hyper-parameters: We use RMSProp [35] with learning rate of 0.00005 for a fair comparison with
WGAN as suggested in its original paper [8]. We ensure the boundedness of model parameters
of discriminator by clipping the weights point-wisely to the range [−0.01, 0.01] as required by
Assumption 2. The dimensionality h of the latent space is manually set according to the complexity
of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and
LSUN bedrooms. The batch size is set to be B = 64 for all datasets.

5.1 Qualitative Analysis

(a) GMMN-D MNIST

(b) GMMN-C MNIST

(c) MMD GAN MNIST

(d) GMMN-D CIFAR-10

(e) GMMN-C CIFAR-10

(f) MMD GAN CIFAR-10

Figure 1: Generated samples from GMMN-D (Dataspace), GMMN-C (Codespace) and our MMD
GAN with batch size B = 64.

We start with comparing MMD GAN with GMMN on two standard benchmarks, MNIST and CIFAR-
10. We consider two variants for GMMN. The ﬁrst one is original GMMN, which trains the generator
by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare
with MMD GAN, we also pretrain an autoencoder for projecting data to a manifold, then ﬁx the
autoencoder as a feature transformation, and train the generator by minimizing the MMD distance in
the code space. We call it as GMMN-C.

The results are pictured in Figure 1. Both GMMN-D and GMMN-C are able to generate meaningful
digits on MNIST because of the simple data structure. By a closer look, nonetheless, the boundary
and shape of the digits in Figure 1a and 1b are often irregular and non-smooth. In contrast, the sample

6

(a) WGAN MNIST

(b) WGAN CelebA

(c) WGAN LSUN

(d) MMD GAN MNIST

(e) MMD GAN CelebA

(f) MMD GAN LSUN

Figure 2: Generated samples from WGAN and MMD GAN on MNIST, CelebA, and LSUN bedroom
datasets.

digits in Figure 1c are more natural with smooth outline and sharper strike. For CIFAR-10 dataset,
both GMMN variants fail to generate meaningful images, but resulting some low level visual features.
We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms,
thus results are omitted. On the other hand, the proposed MMD GAN successfully outputs natural
images with sharp boundary and high diversity. The results in Figure 1 conﬁrm the success of the
proposed adversarial learned kernels to enrich statistical testing power, which is the key difference
between GMMN and MMD GAN.

If we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still
not competitive to MMD GAN with B = 64. The images are put in Appendix C. This demonstrates
that the proposed MMD GAN can be trained more efﬁciently than GMMN with smaller batch size.

Comparisons with GANs: There are several representative extensions of GANs. We consider
recent state-of-art WGAN [8] based on DCGAN structure [34], because of the connection with MMD
GAN discussed in Section 4. The results are shown in Figure 2. For MNIST, the digits generated
from WGAN in Figure 2a are more unnatural with peculiar strikes. In Contrary, the digits from
MMD GAN in Figure 2d enjoy smoother contour. Furthermore, both WGAN and MMD GAN
generate diversiﬁed digits, avoiding the mode collapse problems appeared in the literature of training
GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD GAN.
Speciﬁcally, we observe varied poses, expressions, genders, skin colors and light exposure in Figure
2b and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN
have higher chances to be blurry and twisted while faces from MMD GAN are more spontaneous with
sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient differences
between the samples generated from MMD GAN and WGAN.

5.2 Quantitative Analysis

To quantitatively measure the quality and diversity of generated samples, we compute the inception
score [28] on CIFAR-10 images. The inception score is used for GANs to measure samples quality
and diversity on the pretrained inception model [28]. Models that generate collapsed samples have
a relatively low score. Table 1 lists the results for 50K samples generated by various unsupervised

7

generative models trained on CIFAR-10 dataset. The inception scores of [36, 37, 28] are directly
derived from the corresponding references.

Although both WGAN and MMD GAN can generate sharp images as we show in Section 5.1, our
score is better than other GAN techniques except for DFM [36]. This seems to conﬁrm empirically
that higher order of moment matching between the real data and fake sample distribution beneﬁts
generating more diversiﬁed sample images. Also note DFM appears compatible with our method and
combing training techniques in DFM is a possible avenue for future work.

Method

Real data

Scores ± std.

11.95 ± .20

DFM [36]
ALI [37]
Improved GANs [28]

7.72
5.34
4.36

MMD GAN
WGAN
GMMN-C
GMMN-D
Table 1: Inception scores

6.17 ± .07
5.88 ± .07
3.94 ± .04
3.47 ± .03

5.3 Stability of MMD GAN

Figure 3: Computation time

We further illustrate how the MMD distance correlates well with the quality of the generated samples.
Figure 4 plots the evolution of the MMD GAN estimate the MMD distance during training for
MNIST, CelebA and LSUN datasets. We report the average of the ˆMfφ(PX , Pθ) with moving
average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We
observe during the whole training process, samples generated from the same noise vector across
iterations, remain similar in nature. (e.g., face identity and bedroom style are alike while details and
backgrounds will evolve.) This qualitative observation indicates valuable stability of the training
process. The decreasing curve with the improving quality of images supports the weak∗ topology
shown in Theorem 4. Also, We can see from the plot that the model converges very quickly. In Figure
4b, for example, it converges shortly after tens of thousands of generator iterations on CelebA dataset.

(a) MNIST

(b) CelebA

(c) LSUN Bedrooms

Figure 4: Training curves and generative samples at different stages of training. We can see a clear
correlation between lower distance and better sample quality.

5.4 Computation Issue

We conduct time complexity analysis with respect to the batch size B. The time complexity of each
iteration is O(B) for WGAN and O(KB2) for our proposed MMD GAN with a mixture of K RBF
kernels. The quadratic complexity O(B2) of MMD GAN is introduced by computing kernel matrix,
which is sometimes criticized for being inapplicable with large batch size in practice. However,
we point that there are several recent works, such as EBGAN [7], also matching pairwise relation
between samples of batch size, leading to O(B2) complexity as well.

8

Empirically, we ﬁnd that under GPU environment, the highly parallelized matrix operation tremen-
dously alleviated the quadratic time to almost linear time with modest B. Figure 3 compares the
computational time per generator iterations versus different B on Titan X. When B = 64, which
is adapted for training MMD GAN in our experiments setting, the time per iteration of WGAN
and MMD GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for
training GMMN in its references [9], the time per iteration becomes 4.431 and 8.565 seconds, respec-
tively. This result coheres our argument that the empirical computational time for MMD GAN is not
quadratically expensive compared to WGAN with powerful GPU parallel computation.

5.5 Better Lipschitz Approximation and Necessity of Auto-Encoder

We used weight-clipping for Lipschitz constraint in Assumption 2. Another approach for obtaining a
discriminator with the similar constraints that approximates a Wasserstein distance is [20], where the
gradient of the discriminator is constrained to be 1 between the generated and data points. Inspired
by [20], an alternative approach is to apply the gradient constraint as a regularizer to the witness
function for a different IPM, such as the MMD. This idea was ﬁrst proposed in [30] for the Energy
Distance (in parallel to our submission), which was shown in [31] to correspond to a gradient penalty
on the witness function for any RKHS-based MMD. Here we undertake a preliminary investigation
of this approach, where we also drop the requirement in Algorithm 1 for fφ to be injective, which we
observe that it is not necessary in practice. We show some preliminary results of training MMD GAN
with gradient penalty and without the auto-encoder in Figure 5. The preliminary study indicates that
MMD GAN can generate satisfactory results with other Lipschitz constraint approximation. One
potential future work is conducting more thorough empirical comparison studies between different
approximations.

(a) Cifar10, Giter = 300K

(b) CelebA, Giter = 300K

Figure 5: MMD GAN results using gradient penalty [20] and without auto-encoder reconstruction
loss during training.

6 Discussion

We introduce a new deep generative model trained via MMD with adversarially learned kernels. We
further study its theoretical properties and propose a practical realization MMD GAN, which can be
trained with much smaller batch size than GMMN and has competitive performances with state-of-the-
art GANs. We can view MMD GAN as the ﬁrst practical step forward connecting moment matching
network and GAN. One important direction is applying developed tools in moment matching [15] on
general GAN works based the connections shown by MMD GAN. Also, in Section 4, we connect
WGAN and MMD GAN by ﬁrst-order and inﬁnite-order moment matching. [24] shows ﬁnite-order
moment matching (∼ 5) achieves the best performance on domain adaption. One could extend MMD
GAN to this by using polynomial kernels. Last, in theory, an injective mapping fφ is necessary for
the theoretical guarantees. However, we observe that it is not mandatory in practice as we show
in Section 5.5. One conjecture is it usually learns the injective mapping with high probability by
parameterizing with neural networks, which worth more study as a future work.

9

We thank the reviewers for their helpful comments. We also thank Dougal Sutherland and Arthur
Gretton for their valuable feedbacks and pointing out an error in the previous draft. This work is
supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-
1563887.

Acknowledgments

References

[1] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In AISTATS, 2009.

[2] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.

[3] Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &

Business Media, 2013.

[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In ICML, 2015.

[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[6] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural

samplers using variational divergence minimization. In NIPS, 2016.

[7] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based Generative Adversarial Network. In ICLR,

[8] Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. In ICML, 2017.

[9] Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,

2017.

2015.

[10] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative

neural networks via maximum mean discrepancy optimization. In UAI, 2015.

[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander

Smola. A kernel two-sample test. JMLR, 2012.

[12] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

[13] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In CVPR, 2015.

[14] Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas,
Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized
maximum mean discrepancy. In ICLR, 2017.

[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel
mean embedding of distributions: A review and beyonds. arXiv preprint arXiv:1605.09522,
2016.

[16] Kenji Fukumizu, Arthur Gretton, Gert R Lanckriet, Bernhard Schölkopf, and Bharath K Sripe-
rumbudur. Kernel choice and classiﬁability for rkhs embeddings of probability distributions. In
NIPS, 2009.

[17] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and

K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.

[18] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano
Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale
two-sample tests. In NIPS, 2012.

10

[19] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel

learning. In AISTATS, 2016.

[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.

Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.

[21] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal

regression. 1999.

[22] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.

[23] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature

matching gan. arxiv pre-print 1702.08398, 2017.

[24] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learn-
ing. arXiv preprint arXiv:1702.08811, 2017.

[25] Shuangfei Zhai, Yu Cheng, Rogério Schmidt Feris, and Zhongfei Zhang. Generative adversarial
networks as variational training of energy based models. CoRR, abs/1611.01799, 2016.

[26] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

[27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Adversarial generator-encoder net-

works. arXiv preprint arXiv:1704.02304, 2017.

[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.

Improved techniques for training gans. In NIPS, 2016.

[29] David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717, 2017.

[30] Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Rémi Munos. The cramer distance as a solution to biased wasserstein
gradients. arXiv preprint arXiv:1705.10743, 2017.

[31] Arthur Gretton. Notes on the cramer gan. https://medium.com/towards-data-science/

notes-on-the-cramer-gan-752abd505c00, 2017. Accessed: 2017-11-2.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 1998.

[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[34] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with

deep convolutional generative adversarial networks. In ICLR, 2016.

[35] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.

[36] D Warde-Farley and Y Bengio. Improving generative adversarial networks with denoising

feature matching. In ICLR, 2017.

[37] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mas-

tropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017.

[38] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R.G.
Lanckriet. Hilbert space embeddings and metrics on probability measures. JMLR, 2010.

11

A Technical Proof

A.1 Proof of Theorem 3

Proof. Since MMD is a probabilistic metric [11], we have the triangular inequality for every Mf .
Therefore,
| max
φ

|Mfφ(PX , Pθ) − Mfφ(PX , Pθ(cid:48))|

Mfφ(PX , Pθ) − max

Mfφ(PX , Pθ(cid:48))| ≤

max
φ

(6)

φ

=

≤

|Mfφ∗ (PX , Pθ) − Mfφ∗ (PX , Pθ(cid:48))|
Mfφ∗ (Pθ, Pθ(cid:48)),

(7)

(8)

where φ∗ is the solution of (6).
By deﬁnition, given any φ ∈ Φ, deﬁne hθ = fφ ◦ gθ, the MMD distance Mfφ(Pθ, Pθ(cid:48))
= Ez,z(cid:48) [k(hθ(z), hθ(z(cid:48))) − 2k(hθ(z), hθ(cid:48)(z(cid:48))) + k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48)))]
≤ Ez,z(cid:48) [|k(hθ(z), hθ(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|] + Ez,z(cid:48) [|k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|]

In this, we consider Gaussian kernel k, therefore
|k(hθ(z), hθ(z(cid:48)))−k(hθ(z), hθ(cid:48)(z(cid:48)))| = | exp(−(cid:107)hθ(z)−hθ(z(cid:48))(cid:107)2)−exp(−(cid:107)hθ(z)−hθ(cid:48)(z(cid:48))(cid:107)2)| ≤ 1,
for all (θ, θ(cid:48)) and (z, z(cid:48)). Similarly, |k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))| ≤ 1. Combining the
above claim with (7) and bounded convergence theorem, we have

Mfφ(PX , Pθ) − max

| max
φ
which proves the continuity of maxf M M Df (PX , Pθ).
By Mean Value Theorem, |e−x2
with (8) and triangular inequality, we have

− e−y2

| ≤ maxz |2ze−z2

φ

Mfφ(PX , Pθ(cid:48))| θ→θ(cid:48)

−−−→ 0,

Mfφ(Pθ, Pθ(cid:48)) ≤ Ez,z(cid:48)[(cid:107)hθ(z(cid:48)) − hθ(cid:48)(z(cid:48))(cid:107) + (cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

= 2Ez[(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

| × |x − y| ≤ |x − y|. Incorporating it

Now let h be locally Lipschitz. For a given pair (θ, z) there is a constant L(θ, z) and an open set Uθ
such that for every (θ(cid:48), z) ∈ Uθ we have

(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107) ≤ L(θ, z)((cid:107)θ − θ(cid:48)(cid:107))

Under Assumption 2, we then achieve

|Mfφ(PX , Pθ) − Mfφ(PX , P(cid:48)

(9)
Combining (7) and (9) implies maxφ Mfφ(PX , Pθ) is locally Lipschitz and continuous everywhere.
Last, applying Radamacher’s theorem proves maxφ Mfφ(PX , Pθ) is differentiable almost everywhere,
which completes the proof.

θ)| ≤ Mfφ(Pθ, Pθ(cid:48)) ≤ 2Ez[L(θ, z)](cid:107)θ − θ(cid:48)(cid:107).

A.2 Proof of Theorem 4

Proof. The proof utilizes parts of results from [38].

If maxφ Mfφ(Pn, P) → 0, there exists φ ∈ Φ such that Mfφ(P, Pn) → 0 since Mfφ(P, Pn)

(⇒)
is non-negative. By [38], for any characteristic kernel k,

Therefore, if maxφ Mfφ(Pn, P) → 0, Pn

Mk(Pn, P) → 0 ⇐⇒ Pn
D−→ P.

D−→ P.

(⇐) By [38], given a characteristic kernel k, if supx,x(cid:48) k(x, x(cid:48)) ≤ 1, (cid:112)Mk(P, Q) ≤ W (P, Q),
where W (P, Q) is Wasserstein distance.
In this paper, we consider the kernel k(x, x(cid:48)) =
exp(−(cid:107)fφ(x) − fφ(x(cid:48))(cid:107)2) ≤ 1. By above, (cid:112)Mfφ(P, Q) ≤ W (P, Q), ∀φ. By [8], if Pn
D−→ P,
W (P, Pn) → 0. Combining all of them, we get

Pn

D−→ P =⇒ W (P, Pn) → 0 =⇒ max

Mfφ(P, Pn) → 0.

φ

12

A.3 Proof of Theorem 5

Proof. The proof assumes fφ(x) is scalar, but the vector case can be proved with the same sketch.
First, if E[fφ(x)] > E[fφ(gθ(z))], then φ = φ(cid:48). If E[fφ(x)] < E[fφ(gθ(z))], we let f = −fφ, then
E[f (x)] > E[f (gθ(z))] and ﬂipping sign does not change the MMD distance. If we parameterized
fφ by a neural network, which has a linear output layer, φ(cid:48) can realized by ﬂipping the sign of the
weights of the last layer.

B Property of MMD with Fixed and Learned Kernels

B.1 Continuity and Differentiability

One can simplify Theorem 3 and its proof for standard MMD distance to show MMD is also
continuous and differentiable almost everywhere. In [8], they propose a counterexample to show the
discontinuity of MMD by assuming H = L2. However, it is known that L2 is not in RKHS, so the
discussed counterexample is not appropriate.

B.2 IPM Framework

From integral probability metrics (IPM), the probabilistic distance can be deﬁned as

∆(P, Q) = sup
f ∈F

Ex∼P[f (x)] − Ey∼Q[f (y)].

(10)

By changing the function class F, we can recover several distances, such as total variation, Wasser-
stein distance and MMD distance. From [8], the discriminator fφ in different existing works of GAN
can be explained to be used to solve different probabilistic metrics based on (10). For MMD, the
function class F is {(cid:107)f (cid:107)Hk ≤ 1}, where H is RKHS associated with kernel k. Different form many
distances, such as total variation and Wasserstein distance, there is an analytical representation [11]
as we show in Section 2, which is

∆(P, Q) = M M D(P, Q) = (cid:107)µP − µQ(cid:107)H =

EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

(cid:113)

Because of the analytical representation of (10), GMMN does not need an additional network fφ for
estimating the distance.

Here we also provide an explanation of the proposed MMD with adversarially learned kernel under
IPM framework. The MMD distance with adversarially learned kernel is represented as

M M D(P, Q),

max
k∈K

The corresponding IPM formulation is
∆(P, Q) = max
k∈K

M M D(P, Q) =

sup
f ∈Hk1 ∪···∪Hkn

Ex∼P[f (x)] − Ey∼Q[f (y)],

where ki ∈ K, ∀i. From this perspective, the proposed MMD distance with adversarially learned
kernel is still deﬁned by IPM but with a larger function class.

B.3 MMD is an Efﬁcient Moment Matching

We consider the example of matching ﬁrst and second moments of P and Q. The (cid:96)1 objective in
McGAN [23] is

(cid:107)µP − µQ(cid:107)1 + (cid:107)ΣP − ΣQ(cid:107)∗,

where (cid:107) · (cid:107)∗ is the trace norm. The objective can be changed to be general (cid:96)q norm.

In MMD, with polynomial kernel k(x, y) = 1 + x(cid:62)y, the MMD distance is

2(cid:107)µP − µQ(cid:107)2 + (cid:107)ΣP − ΣQ + µPµ(cid:62)

P − µQµ(cid:62)

Q (cid:107)2
F ,

which is inexact moment matching because the second term contains the quadratic of the ﬁrst moment.
It is difﬁcult to match high-order moments, because we have to deal with high order tensors directly.
On the other hand, MMD can easily match high-order moments (even inﬁnite order moments by
using Gaussian kernel) with kernel tricks, and enjoys strong theoretical guarantee.

13

C Performance of GMMN

(a) GMMN-D on MNIST

(b) GMMN-C on MNIST

(c) GMMN-D on CIFAR-10

(d) GMMN-C CIFAR-10

Figure 6: Generative samples from GMMN-D and GMMN-C with training batch size B = 1024.

14

7
1
0
2
 
v
o
N
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
8
5
8
0
.
5
0
7
1
:
v
i
X
r
a

MMD GAN: Towards Deeper Understanding of
Moment Matching Network

Chun-Liang Li1,∗ Wei-Cheng Chang1,∗ Yu Cheng2 Yiming Yang1 Barnabás Póczos1

1 Carnegie Mellon University,

2AI Foundations, IBM Research

{chunlial,wchang2,yiming,bapoczos}@cs.cmu.edu
(∗ denotes equal contribution)

chengyu@us.ibm.com

Abstract

Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the discriminator
in GAN with a two-sample test based on kernel maximum mean discrepancy
(MMD). Although some theoretical guarantees of MMD have been studied, the
empirical performance of GMMN is still not as competitive as that of GAN on
challenging and large benchmark datasets. The computational efﬁciency of GMMN
is also less desirable in comparison with GAN, partially due to its requirement for
a rather large batch size during the training. In this paper, we propose to improve
both the model expressiveness of GMMN and its computational efﬁciency by
introducing adversarial kernel learning techniques, as the replacement of a ﬁxed
Gaussian kernel in the original GMMN. The new approach combines the key ideas
in both GMMN and GAN, hence we name it MMD GAN. The new distance measure
in MMD GAN is a meaningful loss that enjoys the advantage of weak∗ topology
and can be optimized via gradient descent with relatively small batch sizes. In our
evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA
and LSUN, the performance of MMD GAN signiﬁcantly outperforms GMMN, and
is competitive with other representative GAN works.

1

Introduction

The essence of unsupervised learning models the underlying distribution PX of the data X . Deep
generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with
promising results. However, modeling arbitrary density is a statistically challenging task [3]. In many
applications, such as caption generation [4], accurate density estimation is not even necessary since
we are only interested in sampling from the approximated distribution.
Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a
base distribution PZ over Z, such as Gaussian distribution, then trains a transformation network gθ
such that Pθ ≈ PX , where Pθ is the underlying distribution of gθ(z) and z ∼ PZ . During the training,
GAN-based algorithms require an auxiliary network fφ to estimate the distance between PX and Pθ.
Different probabilistic (pseudo) metrics have been studied [5–8] under GAN framework.
Instead of training an auxiliary network fφ for measuring the distance between PX and Pθ, Generative
moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11],
which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.
During the training, gθ is trained to pass the hypothesis test (minimize MMD distance). [11] shows
even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the
empirical performance of GMMN does not meet its theoretical properties. There is no promising
empirical results comparable with GAN on challenging benchmarks [12, 13]. Computationally,

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

it also requires larger batch size than GAN needs for training, which is considered to be less
efﬁcient [9, 10, 14, 8]

In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels
instead of ﬁxed Gaussian kernels to have better hypothesis testing power. The main contributions of
this work are:

• In Section 2, we prove that training gθ via MMD with learned kernels is continuous and differen-
tiable, which guarantees the model can be trained by gradient descent. Second, we prove a new
distance measure via kernel learning, which is a sensitive loss function to the distance between
PX and Pθ (weak∗ topology). Empirically, the loss decreases when two distributions get closer.
• In Section 3, we propose a practical realization called MMD GAN that learns generator gθ with
the adversarially trained kernel. We further propose a feasible set reduction to speed up and
stabilize the training of MMD GAN.

• In Section 5, we show that MMD GAN is computationally more efﬁcient than GMMN, which can
be trained with much smaller batch size. We also demonstrate that MMD GAN has promising
results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To
our best knowledge, we are the ﬁrst MMD based work to achieve comparable results with other
GAN works on these datasets.

Finally, we also study the connection to existing works in Section 4. Interestingly, we show Wasser-
stein GAN [8] is the special case of the proposed MMD GAN under certain conditions. The uniﬁed
view shows more connections between moment matching and GAN, which can potentially inspire
new algorithms based on well-developed tools in statistics [15]. Our experiment code is available at
https://github.com/OctoberChang/MMD-GAN.

2 GAN, Two-Sample Test and GMMN

i=1, where xi ∈ X and xi ∼ PX . If we are interested in sampling
Assume we are given data {xi}n
from PX , it is not necessary to estimate the density of PX . Instead, Generative Adversarial Network
(GAN) [5] trains a generator gθ parameterized by θ to transform samples z ∼ PZ , where z ∈ Z,
into gθ(z) ∼ Pθ such that Pθ ≈ PX . To measure the similarity between PX and Pθ via their
samples {x}n
j=1 during the training, [5] trains the discriminator fφ parameterized
by φ for help. The learning is done by playing a two-player game, where fφ tries to distinguish xi
and gθ(zj) while gθ aims to confuse fφ by generating gθ(zj) similar to xi.

i=1 and {gθ(zj)}n

On the other hand, distinguishing two distributions by ﬁnite samples is known as Two-Sample Test in
statistics. One way to conduct two-sample test is via kernel maximum mean discrepancy (MMD) [11].
Given two distributions P and Q, and a kernel k, the square of MMD distance is deﬁned as

Mk(P, Q) = (cid:107)µP − µQ(cid:107)2

H = EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

Theorem 1. [11] Given a kernel k, if k is a characteristic kernel, then Mk(P, Q) = 0 iff P = Q.

GMMN: One example of characteristic kernel is Gaussian kernel k(x, x(cid:48)) = exp((cid:107)x − x(cid:48)(cid:107)2). Based
on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains gθ by

Mk(PX , Pθ),

min
θ

(1)

with a ﬁxed Gaussian kernel k rather than training an additional discriminator f as GAN.

2.1 MMD with Kernel Learning

In practice we use ﬁnite samples from distributions to estimate MMD distance. Given X =
{x1, · · · , xn} ∼ P and Y = {y1, · · · , yn} ∼ Q, one estimator of Mk(P, Q) is

ˆMk(X, Y ) =

k(xi, x(cid:48)

i) −

k(xi, yj) +

k(yj, y(cid:48)

j).

1
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=i(cid:48)

2
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=j

1
(cid:1)
(cid:0)n
2

(cid:88)

j(cid:54)=j(cid:48)

Because of the sampling variance, ˆM (X, Y ) may not be zero even when P = Q. We then conduct
hypothesis test with null hypothesis H0 : P = Q. For a given allowable probability of false rejection α,

2

we can only reject H0, which imply P (cid:54)= Q, if ˆM (X, Y ) > cα for some chose threshold cα > 0.
Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to [11] for
more details.
Intuitively, if kernel k cannot result in high MMD distance Mk(P, Q) when P (cid:54)= Q, ˆMk(P, Q) has
more chance to be smaller than cα. Then we are unlikely to reject the null hypothesis H0 with ﬁnite
samples, which implies Q is not distinguishable from P. Therefore, instead of training gθ via (1) with
a pre-speciﬁed kernel k as GMMN, we consider training gθ via

min
θ

max
k∈K

Mk(PX , Pθ),

(2)

(3)

which takes different possible characteristic kernels k ∈ K into account. On the other hand, we
could also view (2) as replacing the ﬁxed kernel k in (1) with the adversarially learned kernel
arg maxk∈K Mk(PX , Pθ) to have stronger signal where P (cid:54)= Pθ to train gθ. We refer interested
readers to [16] for more rigorous discussions about testing power and increasing MMD distances.

However, it is difﬁcult to optimize over all characteristic kernels when we solve (2). By [11, 17] if f
is a injective function and k is characteristic, then the resulted kernel ˜k = k ◦ f , where ˜k(x, x(cid:48)) =
k(f (x), f (x(cid:48))) is still characteristic. If we have a family of injective functions parameterized by φ,
which is denoted as fφ, we are able to change the objective to be

min
θ

max
φ

Mk◦fφ(PX , Pθ),

In this paper, we consider the case that combining Gaussian kernels with injective functions fφ, where
˜k(x, x(cid:48)) = exp(−(cid:107)fφ(x) − fφ(x)(cid:48)(cid:107)2). One example function class of f is {fφ|fφ(x) = φx, φ > 0},
which is equivalent to the kernel bandwidth tuning. A more complicated realization will be discussed
in Section 3. Next, we abuse the notation Mfφ(P, Q) to be MMD distance given the composition
kernel of Gaussian kernel and fφ in the following. Note that [18] considers the linear combination
of characteristic kernels, which can also be incorporated into the discussed composition kernels. A
more general kernel is studied in [19].

2.2 Properties of MMD with Kernel Learning

[8] discuss different distances between distributions adopted by existing deep learning algorithms, and
show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7],
except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training.
From (3), we train gθ via minimizing maxφ Mfφ(PX , Pθ). Next, we show maxφ Mfφ(PX , Pθ) also
enjoys the advantage of being a continuous and differentiable objective in θ under mild assumptions.
Assumption 2. g : Z × Rm → X is locally Lipschitz, where Z ⊆ Rd. We will denote gθ(z) the
evaluation on (z, θ) for convenience. Given fφ and a probability distribution Pz over Z, g satisﬁes
Assumption 2 if there are local Lipschitz constants L(θ, z) for fφ ◦ g, which is independent of φ, such
that Ez∼Pz [L(θ, z)] < +∞.
Theorem 3. The generator function gθ parameterized by θ is under Assumption 2. Let PX be a ﬁxed
distribution over X and Z be a random variable over the space Z. We denote Pθ the distribution
of gθ(Z), then maxφ Mfφ(PX , Pθ) is continuous everywhere and differentiable almost everywhere
in θ.

If gθ is parameterized by a feed-forward neural network, it satisﬁes Assumption 2 and can be trained
via gradient descent as well as propagation, since the objective is continuous and differentiable
followed by Theorem 3. More technical discussions are shown in Appendix B.
Theorem 4. (weak∗ topology) Let {Pn} be a sequence of distributions. Considering n → ∞,
D−→ PX , where D−→ means converging in
under mild Assumption, maxφ Mfφ(PX , Pn) → 0 ⇐⇒ Pn
distribution [3].

Theorem 4 shows that maxφ Mfφ(PX , Pn) is a sensible cost function to the distance between PX
and Pn. The distance is decreasing when Pn is getting closer to PX , which beneﬁts the supervision
of the improvement during the training. All proofs are omitted to Appendix A. In the next section,
we introduce a practical realization of training gθ via optimizing minθ maxφ Mfφ(PX , Pθ).

3

3 MMD GAN

To approximate (3), we use neural networks to parameterized gθ and fφ with expressive power.
For gθ, the assumption is locally Lipschitz, where commonly used feed-forward neural networks
satisfy this constraint. Also, the gradient (cid:53)θ (maxφ fφ ◦ gθ) has to be bounded, which can be
done by clipping φ [8] or gradient penalty [20]. The non-trivial part is fφ has to be injective.
For an injective function f , there exists an function f −1 such that f −1(f (x)) = x, ∀x ∈ X and
f −1(f (g(z))) = g(z), ∀z ∈ Z 1, which can be approximated by an autoencoder. In the following,
we denote φ = {φe, φd} to be the parameter of discriminator networks, which consists of an encoder
fφe , and train the corresponding decoder fφd ≈ f −1 to regularize f . The objective (3) is relaxed to
be

min
θ

max
φ

Mfφe

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe(y))(cid:107)2.

(4)

Note that we ignore the autoencoder objective when we train θ, but we use (4) for a concise
presentation. We note that the empirical study suggests autoencoder objective is not necessary to
lead the successful GAN training as we will show in Section 5, even though the injective property is
required in Theorem 1.

The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks gθ and fφ
in a minmax formulation, while the meaning of the objective is different. In [5], fφe is a discriminator
(binary) classiﬁer to distinguish two distributions. In the proposed algorithm, distinguishing two
distribution is still done by two-sample test via MMD, but with an adversarially learned kernel
parametrized by fφe . gθ is then trained to pass the hypothesis test. More connection and difference
with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed
algorithm MMD GAN. We present an implementation with the weight clipping in Algorithm 1, but
one can easily extend to other Lipschitz approximations, such as gradient penalty [20].

Algorithm 1: MMD GAN, our proposed algorithm.
input

:α the learning rate, c the clipping parameter, B the batch size, nc the number of iterations of
discriminator per generator update.

initialize generator parameter θ and discriminator parameter φ;
while θ has not converged do
for t = 1, . . . , nc do

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe (y))(cid:107)2

Sample a minibatches {xi}B
gφ ← ∇φMfφe
φ ← φ + α · RMSProp(φ, gφ)
φ ← clip(φ, −c, c)
Sample a minibatches {xi}B
gθ ← ∇θMfφe
θ ← θ − α · RMSProp(θ, gθ)

(P(X ), P(gθ(Z)))

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

Encoding Perspective of MMD GAN: Besides from using kernel selection to explain MMD GAN,
the other way to see the proposed MMD GAN is viewing fφe as a feature transformation function,
and the kernel two-sample test is performed on this transformed feature space (i.e., the code space of
the autoencoder). The optimization is ﬁnding a manifold with stronger signals for MMD two-sample
test. From this perspective, [9] is the special case of MMD GAN if fφe is the identity mapping
function. In such circumstance, the kernel two-sample test is conducted in the original data space.

3.1 Feasible Set Reduction

Theorem 5. For any fφ, there exists f (cid:48)
Ez[fφ(cid:48)(gθ(z))].

φ such that Mfφ(Pr, Pθ) = Mf (cid:48)

φ

(Pr, Pθ) and Ex[fφ(x)] (cid:23)

With Theorem 5, we could reduce the feasible set of φ during the optimization by solving

minθ maxφ Mfφ(Pr, Pθ)

s.t. E[fφ(x)] (cid:23) E[fφ(gθ(z))]

1Note that injective is not necessary invertible.

4

which the optimal solution is still equivalent to solving (2).

However, it is hard to solve the constrained optimization problem with backpropagation. We relax
the constraint by ordinal regression [21] to be

min
θ

max
φ

Mfφ(Pr, Pθ) + λ min (cid:0)E[fφ(x)] − E[fφ(gθ(z))], 0(cid:1),

which only penalizes the objective when the constraint is violated. In practice, we observe that
reducing the feasible set makes the training faster and stabler.

4 Related Works

There has been a recent surge on improving GAN [5]. We review some related works here.

If we composite fφ with linear kernel instead of Gaussian kernel, and

Connection with WGAN:
restricting the output dimension h to be 1, we then have the objective
(cid:107)E[fφ(x)] − E[fφ(gθ(z))](cid:107)2.

(5)

min
θ

max
φ
Parameterizing fφ and gθ with neural networks and assuming ∃φ(cid:48) ∈ Φ such f (cid:48)
φ = −fφ, ∀Φ, recovers
Wasserstein GAN (WGAN) [8] 2. If we treat fφ(x) as the data transform function, WGAN can
be interpreted as ﬁrst-order moment matching (linear kernel) while MMD GAN aims to match
inﬁnite order of moments with Gaussian kernel form Taylor expansion [9]. Theoretically, Wasserstein
distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, [22] show neural
networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we
demonstrate matching high-order moments beneﬁts the results. [23] also propose McGAN that
matches second order moment from the primal-dual norm perspective. However, the proposed
algorithm requires matrix (tensor) decompositions because of exact moment matching [24], which
is hard to scale to higher order moment matching. On the other hand, by giving up exact moment
matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions
are in Appendix B.3.

Difference from Other Works with Autoencoders: Energy-based GANs [7, 25] also utilizes
the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes
the reconstruction error of real samples x while maximize the reconstruction error of generated
samples gθ(z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing
the reconstruction errors of both real samples x and generated samples gθ(z). Also, [8] show EBGAN
approximates total variation, with the drawback of discontinuity, while MMD GAN optimizes MMD
distance. The other line of works [2, 26, 9] aims to match the AE codespace f (x), and utilize the
decoder fdec(·). [2, 26] match the distribution of f (x) and z via different distribution distances and
generate data (e.g. image) by fdec(z). [9] use MMD to match f (x) and g(z), and generate data via
fdec(g(z)). The proposed MMD GAN matches the f (x) and f (g(z)), and generates data via g(z)
directly as GAN. [27] is similar to MMD GAN but it considers KL-divergence without showing
continuity and weak∗ topology guarantee as we prove in Section 2.

Other GAN Works:
In addition to the discussed works, there are several extended works of GAN.
[28] proposes using the linear kernel to match ﬁrst moment of its discriminator’s latent features. [14]
considers the variance of empirical MMD score during the training. Also, [14] only improves the
latent feature matching in [28] by using kernel MMD, instead of proposing an adversarial training
framework as we studied in Section 2. [29] uses Wasserstein distance to match the distribution of
autoencoder loss instead of data. One can consider to extend [29] to higher order matching based on
the proposed MMD GAN. A parallel work [30] use energy distance, which can be treated as MMD
GAN with different kernel. However, there are some potential problems of its critic. More discussion
can be referred to [31].

5 Experiment

We train MMD GAN for image generation on the MNIST [32], CIFAR-10 [33], CelebA [13],
and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M

2Theoretically, they are not equivalent but the practical neural network approximation results in the same

algorithm.

5

respectively. All the samples images are generated from a ﬁxed noise random vectors and are not
cherry-picked.

Network architecture: In our experiments, we follow the architecture of DCGAN [34] to design gθ
by its generator and fφ by its discriminator except for expanding the output layer of fφ to be h
dimensions.

Kernel designs: The loss function of MMD GAN is implicitly associated with a family of character-
istic kernels. Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF
kernels k(x, x(cid:48)) = (cid:80)K
q=1 kσq (x, x(cid:48)) where kσq is a Gaussian kernel with bandwidth parameter σq.
Tuning kernel bandwidth σq optimally still remains an open problem. In this works, we ﬁxed K = 5
and σq to be {1, 2, 4, 8, 16} and left the fφ to learn the kernel (feature representation) under these σq.

Hyper-parameters: We use RMSProp [35] with learning rate of 0.00005 for a fair comparison with
WGAN as suggested in its original paper [8]. We ensure the boundedness of model parameters
of discriminator by clipping the weights point-wisely to the range [−0.01, 0.01] as required by
Assumption 2. The dimensionality h of the latent space is manually set according to the complexity
of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and
LSUN bedrooms. The batch size is set to be B = 64 for all datasets.

5.1 Qualitative Analysis

(a) GMMN-D MNIST

(b) GMMN-C MNIST

(c) MMD GAN MNIST

(d) GMMN-D CIFAR-10

(e) GMMN-C CIFAR-10

(f) MMD GAN CIFAR-10

Figure 1: Generated samples from GMMN-D (Dataspace), GMMN-C (Codespace) and our MMD
GAN with batch size B = 64.

We start with comparing MMD GAN with GMMN on two standard benchmarks, MNIST and CIFAR-
10. We consider two variants for GMMN. The ﬁrst one is original GMMN, which trains the generator
by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare
with MMD GAN, we also pretrain an autoencoder for projecting data to a manifold, then ﬁx the
autoencoder as a feature transformation, and train the generator by minimizing the MMD distance in
the code space. We call it as GMMN-C.

The results are pictured in Figure 1. Both GMMN-D and GMMN-C are able to generate meaningful
digits on MNIST because of the simple data structure. By a closer look, nonetheless, the boundary
and shape of the digits in Figure 1a and 1b are often irregular and non-smooth. In contrast, the sample

6

(a) WGAN MNIST

(b) WGAN CelebA

(c) WGAN LSUN

(d) MMD GAN MNIST

(e) MMD GAN CelebA

(f) MMD GAN LSUN

Figure 2: Generated samples from WGAN and MMD GAN on MNIST, CelebA, and LSUN bedroom
datasets.

digits in Figure 1c are more natural with smooth outline and sharper strike. For CIFAR-10 dataset,
both GMMN variants fail to generate meaningful images, but resulting some low level visual features.
We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms,
thus results are omitted. On the other hand, the proposed MMD GAN successfully outputs natural
images with sharp boundary and high diversity. The results in Figure 1 conﬁrm the success of the
proposed adversarial learned kernels to enrich statistical testing power, which is the key difference
between GMMN and MMD GAN.

If we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still
not competitive to MMD GAN with B = 64. The images are put in Appendix C. This demonstrates
that the proposed MMD GAN can be trained more efﬁciently than GMMN with smaller batch size.

Comparisons with GANs: There are several representative extensions of GANs. We consider
recent state-of-art WGAN [8] based on DCGAN structure [34], because of the connection with MMD
GAN discussed in Section 4. The results are shown in Figure 2. For MNIST, the digits generated
from WGAN in Figure 2a are more unnatural with peculiar strikes. In Contrary, the digits from
MMD GAN in Figure 2d enjoy smoother contour. Furthermore, both WGAN and MMD GAN
generate diversiﬁed digits, avoiding the mode collapse problems appeared in the literature of training
GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD GAN.
Speciﬁcally, we observe varied poses, expressions, genders, skin colors and light exposure in Figure
2b and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN
have higher chances to be blurry and twisted while faces from MMD GAN are more spontaneous with
sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient differences
between the samples generated from MMD GAN and WGAN.

5.2 Quantitative Analysis

To quantitatively measure the quality and diversity of generated samples, we compute the inception
score [28] on CIFAR-10 images. The inception score is used for GANs to measure samples quality
and diversity on the pretrained inception model [28]. Models that generate collapsed samples have
a relatively low score. Table 1 lists the results for 50K samples generated by various unsupervised

7

generative models trained on CIFAR-10 dataset. The inception scores of [36, 37, 28] are directly
derived from the corresponding references.

Although both WGAN and MMD GAN can generate sharp images as we show in Section 5.1, our
score is better than other GAN techniques except for DFM [36]. This seems to conﬁrm empirically
that higher order of moment matching between the real data and fake sample distribution beneﬁts
generating more diversiﬁed sample images. Also note DFM appears compatible with our method and
combing training techniques in DFM is a possible avenue for future work.

Method

Real data

Scores ± std.

11.95 ± .20

DFM [36]
ALI [37]
Improved GANs [28]

7.72
5.34
4.36

MMD GAN
WGAN
GMMN-C
GMMN-D
Table 1: Inception scores

6.17 ± .07
5.88 ± .07
3.94 ± .04
3.47 ± .03

5.3 Stability of MMD GAN

Figure 3: Computation time

We further illustrate how the MMD distance correlates well with the quality of the generated samples.
Figure 4 plots the evolution of the MMD GAN estimate the MMD distance during training for
MNIST, CelebA and LSUN datasets. We report the average of the ˆMfφ(PX , Pθ) with moving
average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We
observe during the whole training process, samples generated from the same noise vector across
iterations, remain similar in nature. (e.g., face identity and bedroom style are alike while details and
backgrounds will evolve.) This qualitative observation indicates valuable stability of the training
process. The decreasing curve with the improving quality of images supports the weak∗ topology
shown in Theorem 4. Also, We can see from the plot that the model converges very quickly. In Figure
4b, for example, it converges shortly after tens of thousands of generator iterations on CelebA dataset.

(a) MNIST

(b) CelebA

(c) LSUN Bedrooms

Figure 4: Training curves and generative samples at different stages of training. We can see a clear
correlation between lower distance and better sample quality.

5.4 Computation Issue

We conduct time complexity analysis with respect to the batch size B. The time complexity of each
iteration is O(B) for WGAN and O(KB2) for our proposed MMD GAN with a mixture of K RBF
kernels. The quadratic complexity O(B2) of MMD GAN is introduced by computing kernel matrix,
which is sometimes criticized for being inapplicable with large batch size in practice. However,
we point that there are several recent works, such as EBGAN [7], also matching pairwise relation
between samples of batch size, leading to O(B2) complexity as well.

8

Empirically, we ﬁnd that under GPU environment, the highly parallelized matrix operation tremen-
dously alleviated the quadratic time to almost linear time with modest B. Figure 3 compares the
computational time per generator iterations versus different B on Titan X. When B = 64, which
is adapted for training MMD GAN in our experiments setting, the time per iteration of WGAN
and MMD GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for
training GMMN in its references [9], the time per iteration becomes 4.431 and 8.565 seconds, respec-
tively. This result coheres our argument that the empirical computational time for MMD GAN is not
quadratically expensive compared to WGAN with powerful GPU parallel computation.

5.5 Better Lipschitz Approximation and Necessity of Auto-Encoder

We used weight-clipping for Lipschitz constraint in Assumption 2. Another approach for obtaining a
discriminator with the similar constraints that approximates a Wasserstein distance is [20], where the
gradient of the discriminator is constrained to be 1 between the generated and data points. Inspired
by [20], an alternative approach is to apply the gradient constraint as a regularizer to the witness
function for a different IPM, such as the MMD. This idea was ﬁrst proposed in [30] for the Energy
Distance (in parallel to our submission), which was shown in [31] to correspond to a gradient penalty
on the witness function for any RKHS-based MMD. Here we undertake a preliminary investigation
of this approach, where we also drop the requirement in Algorithm 1 for fφ to be injective, which we
observe that it is not necessary in practice. We show some preliminary results of training MMD GAN
with gradient penalty and without the auto-encoder in Figure 5. The preliminary study indicates that
MMD GAN can generate satisfactory results with other Lipschitz constraint approximation. One
potential future work is conducting more thorough empirical comparison studies between different
approximations.

(a) Cifar10, Giter = 300K

(b) CelebA, Giter = 300K

Figure 5: MMD GAN results using gradient penalty [20] and without auto-encoder reconstruction
loss during training.

6 Discussion

We introduce a new deep generative model trained via MMD with adversarially learned kernels. We
further study its theoretical properties and propose a practical realization MMD GAN, which can be
trained with much smaller batch size than GMMN and has competitive performances with state-of-the-
art GANs. We can view MMD GAN as the ﬁrst practical step forward connecting moment matching
network and GAN. One important direction is applying developed tools in moment matching [15] on
general GAN works based the connections shown by MMD GAN. Also, in Section 4, we connect
WGAN and MMD GAN by ﬁrst-order and inﬁnite-order moment matching. [24] shows ﬁnite-order
moment matching (∼ 5) achieves the best performance on domain adaption. One could extend MMD
GAN to this by using polynomial kernels. Last, in theory, an injective mapping fφ is necessary for
the theoretical guarantees. However, we observe that it is not mandatory in practice as we show
in Section 5.5. One conjecture is it usually learns the injective mapping with high probability by
parameterizing with neural networks, which worth more study as a future work.

9

We thank the reviewers for their helpful comments. We also thank Dougal Sutherland and Arthur
Gretton for their valuable feedbacks and pointing out an error in the previous draft. This work is
supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-
1563887.

Acknowledgments

References

[1] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In AISTATS, 2009.

[2] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.

[3] Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &

Business Media, 2013.

[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In ICML, 2015.

[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[6] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural

samplers using variational divergence minimization. In NIPS, 2016.

[7] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based Generative Adversarial Network. In ICLR,

[8] Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. In ICML, 2017.

[9] Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,

2017.

2015.

[10] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative

neural networks via maximum mean discrepancy optimization. In UAI, 2015.

[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander

Smola. A kernel two-sample test. JMLR, 2012.

[12] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

[13] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In CVPR, 2015.

[14] Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas,
Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized
maximum mean discrepancy. In ICLR, 2017.

[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel
mean embedding of distributions: A review and beyonds. arXiv preprint arXiv:1605.09522,
2016.

[16] Kenji Fukumizu, Arthur Gretton, Gert R Lanckriet, Bernhard Schölkopf, and Bharath K Sripe-
rumbudur. Kernel choice and classiﬁability for rkhs embeddings of probability distributions. In
NIPS, 2009.

[17] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and

K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.

[18] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano
Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale
two-sample tests. In NIPS, 2012.

10

[19] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel

learning. In AISTATS, 2016.

[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.

Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.

[21] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal

regression. 1999.

[22] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.

[23] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature

matching gan. arxiv pre-print 1702.08398, 2017.

[24] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learn-
ing. arXiv preprint arXiv:1702.08811, 2017.

[25] Shuangfei Zhai, Yu Cheng, Rogério Schmidt Feris, and Zhongfei Zhang. Generative adversarial
networks as variational training of energy based models. CoRR, abs/1611.01799, 2016.

[26] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

[27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Adversarial generator-encoder net-

works. arXiv preprint arXiv:1704.02304, 2017.

[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.

Improved techniques for training gans. In NIPS, 2016.

[29] David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717, 2017.

[30] Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Rémi Munos. The cramer distance as a solution to biased wasserstein
gradients. arXiv preprint arXiv:1705.10743, 2017.

[31] Arthur Gretton. Notes on the cramer gan. https://medium.com/towards-data-science/

notes-on-the-cramer-gan-752abd505c00, 2017. Accessed: 2017-11-2.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 1998.

[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[34] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with

deep convolutional generative adversarial networks. In ICLR, 2016.

[35] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.

[36] D Warde-Farley and Y Bengio. Improving generative adversarial networks with denoising

feature matching. In ICLR, 2017.

[37] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mas-

tropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017.

[38] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R.G.
Lanckriet. Hilbert space embeddings and metrics on probability measures. JMLR, 2010.

11

A Technical Proof

A.1 Proof of Theorem 3

Proof. Since MMD is a probabilistic metric [11], we have the triangular inequality for every Mf .
Therefore,
| max
φ

|Mfφ(PX , Pθ) − Mfφ(PX , Pθ(cid:48))|

Mfφ(PX , Pθ) − max

Mfφ(PX , Pθ(cid:48))| ≤

max
φ

(6)

φ

=

≤

|Mfφ∗ (PX , Pθ) − Mfφ∗ (PX , Pθ(cid:48))|
Mfφ∗ (Pθ, Pθ(cid:48)),

(7)

(8)

where φ∗ is the solution of (6).
By deﬁnition, given any φ ∈ Φ, deﬁne hθ = fφ ◦ gθ, the MMD distance Mfφ(Pθ, Pθ(cid:48))
= Ez,z(cid:48) [k(hθ(z), hθ(z(cid:48))) − 2k(hθ(z), hθ(cid:48)(z(cid:48))) + k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48)))]
≤ Ez,z(cid:48) [|k(hθ(z), hθ(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|] + Ez,z(cid:48) [|k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|]

In this, we consider Gaussian kernel k, therefore
|k(hθ(z), hθ(z(cid:48)))−k(hθ(z), hθ(cid:48)(z(cid:48)))| = | exp(−(cid:107)hθ(z)−hθ(z(cid:48))(cid:107)2)−exp(−(cid:107)hθ(z)−hθ(cid:48)(z(cid:48))(cid:107)2)| ≤ 1,
for all (θ, θ(cid:48)) and (z, z(cid:48)). Similarly, |k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))| ≤ 1. Combining the
above claim with (7) and bounded convergence theorem, we have

Mfφ(PX , Pθ) − max

| max
φ
which proves the continuity of maxf M M Df (PX , Pθ).
By Mean Value Theorem, |e−x2
with (8) and triangular inequality, we have

− e−y2

| ≤ maxz |2ze−z2

φ

Mfφ(PX , Pθ(cid:48))| θ→θ(cid:48)

−−−→ 0,

Mfφ(Pθ, Pθ(cid:48)) ≤ Ez,z(cid:48)[(cid:107)hθ(z(cid:48)) − hθ(cid:48)(z(cid:48))(cid:107) + (cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

= 2Ez[(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

| × |x − y| ≤ |x − y|. Incorporating it

Now let h be locally Lipschitz. For a given pair (θ, z) there is a constant L(θ, z) and an open set Uθ
such that for every (θ(cid:48), z) ∈ Uθ we have

(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107) ≤ L(θ, z)((cid:107)θ − θ(cid:48)(cid:107))

Under Assumption 2, we then achieve

|Mfφ(PX , Pθ) − Mfφ(PX , P(cid:48)

(9)
Combining (7) and (9) implies maxφ Mfφ(PX , Pθ) is locally Lipschitz and continuous everywhere.
Last, applying Radamacher’s theorem proves maxφ Mfφ(PX , Pθ) is differentiable almost everywhere,
which completes the proof.

θ)| ≤ Mfφ(Pθ, Pθ(cid:48)) ≤ 2Ez[L(θ, z)](cid:107)θ − θ(cid:48)(cid:107).

A.2 Proof of Theorem 4

Proof. The proof utilizes parts of results from [38].

If maxφ Mfφ(Pn, P) → 0, there exists φ ∈ Φ such that Mfφ(P, Pn) → 0 since Mfφ(P, Pn)

(⇒)
is non-negative. By [38], for any characteristic kernel k,

Therefore, if maxφ Mfφ(Pn, P) → 0, Pn

Mk(Pn, P) → 0 ⇐⇒ Pn
D−→ P.

D−→ P.

(⇐) By [38], given a characteristic kernel k, if supx,x(cid:48) k(x, x(cid:48)) ≤ 1, (cid:112)Mk(P, Q) ≤ W (P, Q),
where W (P, Q) is Wasserstein distance.
In this paper, we consider the kernel k(x, x(cid:48)) =
exp(−(cid:107)fφ(x) − fφ(x(cid:48))(cid:107)2) ≤ 1. By above, (cid:112)Mfφ(P, Q) ≤ W (P, Q), ∀φ. By [8], if Pn
D−→ P,
W (P, Pn) → 0. Combining all of them, we get

Pn

D−→ P =⇒ W (P, Pn) → 0 =⇒ max

Mfφ(P, Pn) → 0.

φ

12

A.3 Proof of Theorem 5

Proof. The proof assumes fφ(x) is scalar, but the vector case can be proved with the same sketch.
First, if E[fφ(x)] > E[fφ(gθ(z))], then φ = φ(cid:48). If E[fφ(x)] < E[fφ(gθ(z))], we let f = −fφ, then
E[f (x)] > E[f (gθ(z))] and ﬂipping sign does not change the MMD distance. If we parameterized
fφ by a neural network, which has a linear output layer, φ(cid:48) can realized by ﬂipping the sign of the
weights of the last layer.

B Property of MMD with Fixed and Learned Kernels

B.1 Continuity and Differentiability

One can simplify Theorem 3 and its proof for standard MMD distance to show MMD is also
continuous and differentiable almost everywhere. In [8], they propose a counterexample to show the
discontinuity of MMD by assuming H = L2. However, it is known that L2 is not in RKHS, so the
discussed counterexample is not appropriate.

B.2 IPM Framework

From integral probability metrics (IPM), the probabilistic distance can be deﬁned as

∆(P, Q) = sup
f ∈F

Ex∼P[f (x)] − Ey∼Q[f (y)].

(10)

By changing the function class F, we can recover several distances, such as total variation, Wasser-
stein distance and MMD distance. From [8], the discriminator fφ in different existing works of GAN
can be explained to be used to solve different probabilistic metrics based on (10). For MMD, the
function class F is {(cid:107)f (cid:107)Hk ≤ 1}, where H is RKHS associated with kernel k. Different form many
distances, such as total variation and Wasserstein distance, there is an analytical representation [11]
as we show in Section 2, which is

∆(P, Q) = M M D(P, Q) = (cid:107)µP − µQ(cid:107)H =

EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

(cid:113)

Because of the analytical representation of (10), GMMN does not need an additional network fφ for
estimating the distance.

Here we also provide an explanation of the proposed MMD with adversarially learned kernel under
IPM framework. The MMD distance with adversarially learned kernel is represented as

M M D(P, Q),

max
k∈K

The corresponding IPM formulation is
∆(P, Q) = max
k∈K

M M D(P, Q) =

sup
f ∈Hk1 ∪···∪Hkn

Ex∼P[f (x)] − Ey∼Q[f (y)],

where ki ∈ K, ∀i. From this perspective, the proposed MMD distance with adversarially learned
kernel is still deﬁned by IPM but with a larger function class.

B.3 MMD is an Efﬁcient Moment Matching

We consider the example of matching ﬁrst and second moments of P and Q. The (cid:96)1 objective in
McGAN [23] is

(cid:107)µP − µQ(cid:107)1 + (cid:107)ΣP − ΣQ(cid:107)∗,

where (cid:107) · (cid:107)∗ is the trace norm. The objective can be changed to be general (cid:96)q norm.

In MMD, with polynomial kernel k(x, y) = 1 + x(cid:62)y, the MMD distance is

2(cid:107)µP − µQ(cid:107)2 + (cid:107)ΣP − ΣQ + µPµ(cid:62)

P − µQµ(cid:62)

Q (cid:107)2
F ,

which is inexact moment matching because the second term contains the quadratic of the ﬁrst moment.
It is difﬁcult to match high-order moments, because we have to deal with high order tensors directly.
On the other hand, MMD can easily match high-order moments (even inﬁnite order moments by
using Gaussian kernel) with kernel tricks, and enjoys strong theoretical guarantee.

13

C Performance of GMMN

(a) GMMN-D on MNIST

(b) GMMN-C on MNIST

(c) GMMN-D on CIFAR-10

(d) GMMN-C CIFAR-10

Figure 6: Generative samples from GMMN-D and GMMN-C with training batch size B = 1024.

14

7
1
0
2
 
v
o
N
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
8
5
8
0
.
5
0
7
1
:
v
i
X
r
a

MMD GAN: Towards Deeper Understanding of
Moment Matching Network

Chun-Liang Li1,∗ Wei-Cheng Chang1,∗ Yu Cheng2 Yiming Yang1 Barnabás Póczos1

1 Carnegie Mellon University,

2AI Foundations, IBM Research

{chunlial,wchang2,yiming,bapoczos}@cs.cmu.edu
(∗ denotes equal contribution)

chengyu@us.ibm.com

Abstract

Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the discriminator
in GAN with a two-sample test based on kernel maximum mean discrepancy
(MMD). Although some theoretical guarantees of MMD have been studied, the
empirical performance of GMMN is still not as competitive as that of GAN on
challenging and large benchmark datasets. The computational efﬁciency of GMMN
is also less desirable in comparison with GAN, partially due to its requirement for
a rather large batch size during the training. In this paper, we propose to improve
both the model expressiveness of GMMN and its computational efﬁciency by
introducing adversarial kernel learning techniques, as the replacement of a ﬁxed
Gaussian kernel in the original GMMN. The new approach combines the key ideas
in both GMMN and GAN, hence we name it MMD GAN. The new distance measure
in MMD GAN is a meaningful loss that enjoys the advantage of weak∗ topology
and can be optimized via gradient descent with relatively small batch sizes. In our
evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA
and LSUN, the performance of MMD GAN signiﬁcantly outperforms GMMN, and
is competitive with other representative GAN works.

1

Introduction

The essence of unsupervised learning models the underlying distribution PX of the data X . Deep
generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with
promising results. However, modeling arbitrary density is a statistically challenging task [3]. In many
applications, such as caption generation [4], accurate density estimation is not even necessary since
we are only interested in sampling from the approximated distribution.
Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a
base distribution PZ over Z, such as Gaussian distribution, then trains a transformation network gθ
such that Pθ ≈ PX , where Pθ is the underlying distribution of gθ(z) and z ∼ PZ . During the training,
GAN-based algorithms require an auxiliary network fφ to estimate the distance between PX and Pθ.
Different probabilistic (pseudo) metrics have been studied [5–8] under GAN framework.
Instead of training an auxiliary network fφ for measuring the distance between PX and Pθ, Generative
moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11],
which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.
During the training, gθ is trained to pass the hypothesis test (minimize MMD distance). [11] shows
even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the
empirical performance of GMMN does not meet its theoretical properties. There is no promising
empirical results comparable with GAN on challenging benchmarks [12, 13]. Computationally,

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

it also requires larger batch size than GAN needs for training, which is considered to be less
efﬁcient [9, 10, 14, 8]

In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels
instead of ﬁxed Gaussian kernels to have better hypothesis testing power. The main contributions of
this work are:

• In Section 2, we prove that training gθ via MMD with learned kernels is continuous and differen-
tiable, which guarantees the model can be trained by gradient descent. Second, we prove a new
distance measure via kernel learning, which is a sensitive loss function to the distance between
PX and Pθ (weak∗ topology). Empirically, the loss decreases when two distributions get closer.
• In Section 3, we propose a practical realization called MMD GAN that learns generator gθ with
the adversarially trained kernel. We further propose a feasible set reduction to speed up and
stabilize the training of MMD GAN.

• In Section 5, we show that MMD GAN is computationally more efﬁcient than GMMN, which can
be trained with much smaller batch size. We also demonstrate that MMD GAN has promising
results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To
our best knowledge, we are the ﬁrst MMD based work to achieve comparable results with other
GAN works on these datasets.

Finally, we also study the connection to existing works in Section 4. Interestingly, we show Wasser-
stein GAN [8] is the special case of the proposed MMD GAN under certain conditions. The uniﬁed
view shows more connections between moment matching and GAN, which can potentially inspire
new algorithms based on well-developed tools in statistics [15]. Our experiment code is available at
https://github.com/OctoberChang/MMD-GAN.

2 GAN, Two-Sample Test and GMMN

i=1, where xi ∈ X and xi ∼ PX . If we are interested in sampling
Assume we are given data {xi}n
from PX , it is not necessary to estimate the density of PX . Instead, Generative Adversarial Network
(GAN) [5] trains a generator gθ parameterized by θ to transform samples z ∼ PZ , where z ∈ Z,
into gθ(z) ∼ Pθ such that Pθ ≈ PX . To measure the similarity between PX and Pθ via their
samples {x}n
j=1 during the training, [5] trains the discriminator fφ parameterized
by φ for help. The learning is done by playing a two-player game, where fφ tries to distinguish xi
and gθ(zj) while gθ aims to confuse fφ by generating gθ(zj) similar to xi.

i=1 and {gθ(zj)}n

On the other hand, distinguishing two distributions by ﬁnite samples is known as Two-Sample Test in
statistics. One way to conduct two-sample test is via kernel maximum mean discrepancy (MMD) [11].
Given two distributions P and Q, and a kernel k, the square of MMD distance is deﬁned as

Mk(P, Q) = (cid:107)µP − µQ(cid:107)2

H = EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

Theorem 1. [11] Given a kernel k, if k is a characteristic kernel, then Mk(P, Q) = 0 iff P = Q.

GMMN: One example of characteristic kernel is Gaussian kernel k(x, x(cid:48)) = exp((cid:107)x − x(cid:48)(cid:107)2). Based
on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains gθ by

Mk(PX , Pθ),

min
θ

(1)

with a ﬁxed Gaussian kernel k rather than training an additional discriminator f as GAN.

2.1 MMD with Kernel Learning

In practice we use ﬁnite samples from distributions to estimate MMD distance. Given X =
{x1, · · · , xn} ∼ P and Y = {y1, · · · , yn} ∼ Q, one estimator of Mk(P, Q) is

ˆMk(X, Y ) =

k(xi, x(cid:48)

i) −

k(xi, yj) +

k(yj, y(cid:48)

j).

1
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=i(cid:48)

2
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=j

1
(cid:1)
(cid:0)n
2

(cid:88)

j(cid:54)=j(cid:48)

Because of the sampling variance, ˆM (X, Y ) may not be zero even when P = Q. We then conduct
hypothesis test with null hypothesis H0 : P = Q. For a given allowable probability of false rejection α,

2

we can only reject H0, which imply P (cid:54)= Q, if ˆM (X, Y ) > cα for some chose threshold cα > 0.
Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to [11] for
more details.
Intuitively, if kernel k cannot result in high MMD distance Mk(P, Q) when P (cid:54)= Q, ˆMk(P, Q) has
more chance to be smaller than cα. Then we are unlikely to reject the null hypothesis H0 with ﬁnite
samples, which implies Q is not distinguishable from P. Therefore, instead of training gθ via (1) with
a pre-speciﬁed kernel k as GMMN, we consider training gθ via

min
θ

max
k∈K

Mk(PX , Pθ),

(2)

(3)

which takes different possible characteristic kernels k ∈ K into account. On the other hand, we
could also view (2) as replacing the ﬁxed kernel k in (1) with the adversarially learned kernel
arg maxk∈K Mk(PX , Pθ) to have stronger signal where P (cid:54)= Pθ to train gθ. We refer interested
readers to [16] for more rigorous discussions about testing power and increasing MMD distances.

However, it is difﬁcult to optimize over all characteristic kernels when we solve (2). By [11, 17] if f
is a injective function and k is characteristic, then the resulted kernel ˜k = k ◦ f , where ˜k(x, x(cid:48)) =
k(f (x), f (x(cid:48))) is still characteristic. If we have a family of injective functions parameterized by φ,
which is denoted as fφ, we are able to change the objective to be

min
θ

max
φ

Mk◦fφ(PX , Pθ),

In this paper, we consider the case that combining Gaussian kernels with injective functions fφ, where
˜k(x, x(cid:48)) = exp(−(cid:107)fφ(x) − fφ(x)(cid:48)(cid:107)2). One example function class of f is {fφ|fφ(x) = φx, φ > 0},
which is equivalent to the kernel bandwidth tuning. A more complicated realization will be discussed
in Section 3. Next, we abuse the notation Mfφ(P, Q) to be MMD distance given the composition
kernel of Gaussian kernel and fφ in the following. Note that [18] considers the linear combination
of characteristic kernels, which can also be incorporated into the discussed composition kernels. A
more general kernel is studied in [19].

2.2 Properties of MMD with Kernel Learning

[8] discuss different distances between distributions adopted by existing deep learning algorithms, and
show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7],
except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training.
From (3), we train gθ via minimizing maxφ Mfφ(PX , Pθ). Next, we show maxφ Mfφ(PX , Pθ) also
enjoys the advantage of being a continuous and differentiable objective in θ under mild assumptions.
Assumption 2. g : Z × Rm → X is locally Lipschitz, where Z ⊆ Rd. We will denote gθ(z) the
evaluation on (z, θ) for convenience. Given fφ and a probability distribution Pz over Z, g satisﬁes
Assumption 2 if there are local Lipschitz constants L(θ, z) for fφ ◦ g, which is independent of φ, such
that Ez∼Pz [L(θ, z)] < +∞.
Theorem 3. The generator function gθ parameterized by θ is under Assumption 2. Let PX be a ﬁxed
distribution over X and Z be a random variable over the space Z. We denote Pθ the distribution
of gθ(Z), then maxφ Mfφ(PX , Pθ) is continuous everywhere and differentiable almost everywhere
in θ.

If gθ is parameterized by a feed-forward neural network, it satisﬁes Assumption 2 and can be trained
via gradient descent as well as propagation, since the objective is continuous and differentiable
followed by Theorem 3. More technical discussions are shown in Appendix B.
Theorem 4. (weak∗ topology) Let {Pn} be a sequence of distributions. Considering n → ∞,
D−→ PX , where D−→ means converging in
under mild Assumption, maxφ Mfφ(PX , Pn) → 0 ⇐⇒ Pn
distribution [3].

Theorem 4 shows that maxφ Mfφ(PX , Pn) is a sensible cost function to the distance between PX
and Pn. The distance is decreasing when Pn is getting closer to PX , which beneﬁts the supervision
of the improvement during the training. All proofs are omitted to Appendix A. In the next section,
we introduce a practical realization of training gθ via optimizing minθ maxφ Mfφ(PX , Pθ).

3

3 MMD GAN

To approximate (3), we use neural networks to parameterized gθ and fφ with expressive power.
For gθ, the assumption is locally Lipschitz, where commonly used feed-forward neural networks
satisfy this constraint. Also, the gradient (cid:53)θ (maxφ fφ ◦ gθ) has to be bounded, which can be
done by clipping φ [8] or gradient penalty [20]. The non-trivial part is fφ has to be injective.
For an injective function f , there exists an function f −1 such that f −1(f (x)) = x, ∀x ∈ X and
f −1(f (g(z))) = g(z), ∀z ∈ Z 1, which can be approximated by an autoencoder. In the following,
we denote φ = {φe, φd} to be the parameter of discriminator networks, which consists of an encoder
fφe , and train the corresponding decoder fφd ≈ f −1 to regularize f . The objective (3) is relaxed to
be

min
θ

max
φ

Mfφe

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe(y))(cid:107)2.

(4)

Note that we ignore the autoencoder objective when we train θ, but we use (4) for a concise
presentation. We note that the empirical study suggests autoencoder objective is not necessary to
lead the successful GAN training as we will show in Section 5, even though the injective property is
required in Theorem 1.

The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks gθ and fφ
in a minmax formulation, while the meaning of the objective is different. In [5], fφe is a discriminator
(binary) classiﬁer to distinguish two distributions. In the proposed algorithm, distinguishing two
distribution is still done by two-sample test via MMD, but with an adversarially learned kernel
parametrized by fφe . gθ is then trained to pass the hypothesis test. More connection and difference
with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed
algorithm MMD GAN. We present an implementation with the weight clipping in Algorithm 1, but
one can easily extend to other Lipschitz approximations, such as gradient penalty [20].

Algorithm 1: MMD GAN, our proposed algorithm.
input

:α the learning rate, c the clipping parameter, B the batch size, nc the number of iterations of
discriminator per generator update.

initialize generator parameter θ and discriminator parameter φ;
while θ has not converged do
for t = 1, . . . , nc do

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe (y))(cid:107)2

Sample a minibatches {xi}B
gφ ← ∇φMfφe
φ ← φ + α · RMSProp(φ, gφ)
φ ← clip(φ, −c, c)
Sample a minibatches {xi}B
gθ ← ∇θMfφe
θ ← θ − α · RMSProp(θ, gθ)

(P(X ), P(gθ(Z)))

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

Encoding Perspective of MMD GAN: Besides from using kernel selection to explain MMD GAN,
the other way to see the proposed MMD GAN is viewing fφe as a feature transformation function,
and the kernel two-sample test is performed on this transformed feature space (i.e., the code space of
the autoencoder). The optimization is ﬁnding a manifold with stronger signals for MMD two-sample
test. From this perspective, [9] is the special case of MMD GAN if fφe is the identity mapping
function. In such circumstance, the kernel two-sample test is conducted in the original data space.

3.1 Feasible Set Reduction

Theorem 5. For any fφ, there exists f (cid:48)
Ez[fφ(cid:48)(gθ(z))].

φ such that Mfφ(Pr, Pθ) = Mf (cid:48)

φ

(Pr, Pθ) and Ex[fφ(x)] (cid:23)

With Theorem 5, we could reduce the feasible set of φ during the optimization by solving

minθ maxφ Mfφ(Pr, Pθ)

s.t. E[fφ(x)] (cid:23) E[fφ(gθ(z))]

1Note that injective is not necessary invertible.

4

which the optimal solution is still equivalent to solving (2).

However, it is hard to solve the constrained optimization problem with backpropagation. We relax
the constraint by ordinal regression [21] to be

min
θ

max
φ

Mfφ(Pr, Pθ) + λ min (cid:0)E[fφ(x)] − E[fφ(gθ(z))], 0(cid:1),

which only penalizes the objective when the constraint is violated. In practice, we observe that
reducing the feasible set makes the training faster and stabler.

4 Related Works

There has been a recent surge on improving GAN [5]. We review some related works here.

If we composite fφ with linear kernel instead of Gaussian kernel, and

Connection with WGAN:
restricting the output dimension h to be 1, we then have the objective
(cid:107)E[fφ(x)] − E[fφ(gθ(z))](cid:107)2.

(5)

min
θ

max
φ
Parameterizing fφ and gθ with neural networks and assuming ∃φ(cid:48) ∈ Φ such f (cid:48)
φ = −fφ, ∀Φ, recovers
Wasserstein GAN (WGAN) [8] 2. If we treat fφ(x) as the data transform function, WGAN can
be interpreted as ﬁrst-order moment matching (linear kernel) while MMD GAN aims to match
inﬁnite order of moments with Gaussian kernel form Taylor expansion [9]. Theoretically, Wasserstein
distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, [22] show neural
networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we
demonstrate matching high-order moments beneﬁts the results. [23] also propose McGAN that
matches second order moment from the primal-dual norm perspective. However, the proposed
algorithm requires matrix (tensor) decompositions because of exact moment matching [24], which
is hard to scale to higher order moment matching. On the other hand, by giving up exact moment
matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions
are in Appendix B.3.

Difference from Other Works with Autoencoders: Energy-based GANs [7, 25] also utilizes
the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes
the reconstruction error of real samples x while maximize the reconstruction error of generated
samples gθ(z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing
the reconstruction errors of both real samples x and generated samples gθ(z). Also, [8] show EBGAN
approximates total variation, with the drawback of discontinuity, while MMD GAN optimizes MMD
distance. The other line of works [2, 26, 9] aims to match the AE codespace f (x), and utilize the
decoder fdec(·). [2, 26] match the distribution of f (x) and z via different distribution distances and
generate data (e.g. image) by fdec(z). [9] use MMD to match f (x) and g(z), and generate data via
fdec(g(z)). The proposed MMD GAN matches the f (x) and f (g(z)), and generates data via g(z)
directly as GAN. [27] is similar to MMD GAN but it considers KL-divergence without showing
continuity and weak∗ topology guarantee as we prove in Section 2.

Other GAN Works:
In addition to the discussed works, there are several extended works of GAN.
[28] proposes using the linear kernel to match ﬁrst moment of its discriminator’s latent features. [14]
considers the variance of empirical MMD score during the training. Also, [14] only improves the
latent feature matching in [28] by using kernel MMD, instead of proposing an adversarial training
framework as we studied in Section 2. [29] uses Wasserstein distance to match the distribution of
autoencoder loss instead of data. One can consider to extend [29] to higher order matching based on
the proposed MMD GAN. A parallel work [30] use energy distance, which can be treated as MMD
GAN with different kernel. However, there are some potential problems of its critic. More discussion
can be referred to [31].

5 Experiment

We train MMD GAN for image generation on the MNIST [32], CIFAR-10 [33], CelebA [13],
and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M

2Theoretically, they are not equivalent but the practical neural network approximation results in the same

algorithm.

5

respectively. All the samples images are generated from a ﬁxed noise random vectors and are not
cherry-picked.

Network architecture: In our experiments, we follow the architecture of DCGAN [34] to design gθ
by its generator and fφ by its discriminator except for expanding the output layer of fφ to be h
dimensions.

Kernel designs: The loss function of MMD GAN is implicitly associated with a family of character-
istic kernels. Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF
kernels k(x, x(cid:48)) = (cid:80)K
q=1 kσq (x, x(cid:48)) where kσq is a Gaussian kernel with bandwidth parameter σq.
Tuning kernel bandwidth σq optimally still remains an open problem. In this works, we ﬁxed K = 5
and σq to be {1, 2, 4, 8, 16} and left the fφ to learn the kernel (feature representation) under these σq.

Hyper-parameters: We use RMSProp [35] with learning rate of 0.00005 for a fair comparison with
WGAN as suggested in its original paper [8]. We ensure the boundedness of model parameters
of discriminator by clipping the weights point-wisely to the range [−0.01, 0.01] as required by
Assumption 2. The dimensionality h of the latent space is manually set according to the complexity
of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and
LSUN bedrooms. The batch size is set to be B = 64 for all datasets.

5.1 Qualitative Analysis

(a) GMMN-D MNIST

(b) GMMN-C MNIST

(c) MMD GAN MNIST

(d) GMMN-D CIFAR-10

(e) GMMN-C CIFAR-10

(f) MMD GAN CIFAR-10

Figure 1: Generated samples from GMMN-D (Dataspace), GMMN-C (Codespace) and our MMD
GAN with batch size B = 64.

We start with comparing MMD GAN with GMMN on two standard benchmarks, MNIST and CIFAR-
10. We consider two variants for GMMN. The ﬁrst one is original GMMN, which trains the generator
by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare
with MMD GAN, we also pretrain an autoencoder for projecting data to a manifold, then ﬁx the
autoencoder as a feature transformation, and train the generator by minimizing the MMD distance in
the code space. We call it as GMMN-C.

The results are pictured in Figure 1. Both GMMN-D and GMMN-C are able to generate meaningful
digits on MNIST because of the simple data structure. By a closer look, nonetheless, the boundary
and shape of the digits in Figure 1a and 1b are often irregular and non-smooth. In contrast, the sample

6

(a) WGAN MNIST

(b) WGAN CelebA

(c) WGAN LSUN

(d) MMD GAN MNIST

(e) MMD GAN CelebA

(f) MMD GAN LSUN

Figure 2: Generated samples from WGAN and MMD GAN on MNIST, CelebA, and LSUN bedroom
datasets.

digits in Figure 1c are more natural with smooth outline and sharper strike. For CIFAR-10 dataset,
both GMMN variants fail to generate meaningful images, but resulting some low level visual features.
We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms,
thus results are omitted. On the other hand, the proposed MMD GAN successfully outputs natural
images with sharp boundary and high diversity. The results in Figure 1 conﬁrm the success of the
proposed adversarial learned kernels to enrich statistical testing power, which is the key difference
between GMMN and MMD GAN.

If we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still
not competitive to MMD GAN with B = 64. The images are put in Appendix C. This demonstrates
that the proposed MMD GAN can be trained more efﬁciently than GMMN with smaller batch size.

Comparisons with GANs: There are several representative extensions of GANs. We consider
recent state-of-art WGAN [8] based on DCGAN structure [34], because of the connection with MMD
GAN discussed in Section 4. The results are shown in Figure 2. For MNIST, the digits generated
from WGAN in Figure 2a are more unnatural with peculiar strikes. In Contrary, the digits from
MMD GAN in Figure 2d enjoy smoother contour. Furthermore, both WGAN and MMD GAN
generate diversiﬁed digits, avoiding the mode collapse problems appeared in the literature of training
GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD GAN.
Speciﬁcally, we observe varied poses, expressions, genders, skin colors and light exposure in Figure
2b and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN
have higher chances to be blurry and twisted while faces from MMD GAN are more spontaneous with
sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient differences
between the samples generated from MMD GAN and WGAN.

5.2 Quantitative Analysis

To quantitatively measure the quality and diversity of generated samples, we compute the inception
score [28] on CIFAR-10 images. The inception score is used for GANs to measure samples quality
and diversity on the pretrained inception model [28]. Models that generate collapsed samples have
a relatively low score. Table 1 lists the results for 50K samples generated by various unsupervised

7

generative models trained on CIFAR-10 dataset. The inception scores of [36, 37, 28] are directly
derived from the corresponding references.

Although both WGAN and MMD GAN can generate sharp images as we show in Section 5.1, our
score is better than other GAN techniques except for DFM [36]. This seems to conﬁrm empirically
that higher order of moment matching between the real data and fake sample distribution beneﬁts
generating more diversiﬁed sample images. Also note DFM appears compatible with our method and
combing training techniques in DFM is a possible avenue for future work.

Method

Real data

Scores ± std.

11.95 ± .20

DFM [36]
ALI [37]
Improved GANs [28]

7.72
5.34
4.36

MMD GAN
WGAN
GMMN-C
GMMN-D
Table 1: Inception scores

6.17 ± .07
5.88 ± .07
3.94 ± .04
3.47 ± .03

5.3 Stability of MMD GAN

Figure 3: Computation time

We further illustrate how the MMD distance correlates well with the quality of the generated samples.
Figure 4 plots the evolution of the MMD GAN estimate the MMD distance during training for
MNIST, CelebA and LSUN datasets. We report the average of the ˆMfφ(PX , Pθ) with moving
average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We
observe during the whole training process, samples generated from the same noise vector across
iterations, remain similar in nature. (e.g., face identity and bedroom style are alike while details and
backgrounds will evolve.) This qualitative observation indicates valuable stability of the training
process. The decreasing curve with the improving quality of images supports the weak∗ topology
shown in Theorem 4. Also, We can see from the plot that the model converges very quickly. In Figure
4b, for example, it converges shortly after tens of thousands of generator iterations on CelebA dataset.

(a) MNIST

(b) CelebA

(c) LSUN Bedrooms

Figure 4: Training curves and generative samples at different stages of training. We can see a clear
correlation between lower distance and better sample quality.

5.4 Computation Issue

We conduct time complexity analysis with respect to the batch size B. The time complexity of each
iteration is O(B) for WGAN and O(KB2) for our proposed MMD GAN with a mixture of K RBF
kernels. The quadratic complexity O(B2) of MMD GAN is introduced by computing kernel matrix,
which is sometimes criticized for being inapplicable with large batch size in practice. However,
we point that there are several recent works, such as EBGAN [7], also matching pairwise relation
between samples of batch size, leading to O(B2) complexity as well.

8

Empirically, we ﬁnd that under GPU environment, the highly parallelized matrix operation tremen-
dously alleviated the quadratic time to almost linear time with modest B. Figure 3 compares the
computational time per generator iterations versus different B on Titan X. When B = 64, which
is adapted for training MMD GAN in our experiments setting, the time per iteration of WGAN
and MMD GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for
training GMMN in its references [9], the time per iteration becomes 4.431 and 8.565 seconds, respec-
tively. This result coheres our argument that the empirical computational time for MMD GAN is not
quadratically expensive compared to WGAN with powerful GPU parallel computation.

5.5 Better Lipschitz Approximation and Necessity of Auto-Encoder

We used weight-clipping for Lipschitz constraint in Assumption 2. Another approach for obtaining a
discriminator with the similar constraints that approximates a Wasserstein distance is [20], where the
gradient of the discriminator is constrained to be 1 between the generated and data points. Inspired
by [20], an alternative approach is to apply the gradient constraint as a regularizer to the witness
function for a different IPM, such as the MMD. This idea was ﬁrst proposed in [30] for the Energy
Distance (in parallel to our submission), which was shown in [31] to correspond to a gradient penalty
on the witness function for any RKHS-based MMD. Here we undertake a preliminary investigation
of this approach, where we also drop the requirement in Algorithm 1 for fφ to be injective, which we
observe that it is not necessary in practice. We show some preliminary results of training MMD GAN
with gradient penalty and without the auto-encoder in Figure 5. The preliminary study indicates that
MMD GAN can generate satisfactory results with other Lipschitz constraint approximation. One
potential future work is conducting more thorough empirical comparison studies between different
approximations.

(a) Cifar10, Giter = 300K

(b) CelebA, Giter = 300K

Figure 5: MMD GAN results using gradient penalty [20] and without auto-encoder reconstruction
loss during training.

6 Discussion

We introduce a new deep generative model trained via MMD with adversarially learned kernels. We
further study its theoretical properties and propose a practical realization MMD GAN, which can be
trained with much smaller batch size than GMMN and has competitive performances with state-of-the-
art GANs. We can view MMD GAN as the ﬁrst practical step forward connecting moment matching
network and GAN. One important direction is applying developed tools in moment matching [15] on
general GAN works based the connections shown by MMD GAN. Also, in Section 4, we connect
WGAN and MMD GAN by ﬁrst-order and inﬁnite-order moment matching. [24] shows ﬁnite-order
moment matching (∼ 5) achieves the best performance on domain adaption. One could extend MMD
GAN to this by using polynomial kernels. Last, in theory, an injective mapping fφ is necessary for
the theoretical guarantees. However, we observe that it is not mandatory in practice as we show
in Section 5.5. One conjecture is it usually learns the injective mapping with high probability by
parameterizing with neural networks, which worth more study as a future work.

9

We thank the reviewers for their helpful comments. We also thank Dougal Sutherland and Arthur
Gretton for their valuable feedbacks and pointing out an error in the previous draft. This work is
supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-
1563887.

Acknowledgments

References

[1] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In AISTATS, 2009.

[2] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.

[3] Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &

Business Media, 2013.

[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In ICML, 2015.

[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[6] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural

samplers using variational divergence minimization. In NIPS, 2016.

[7] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based Generative Adversarial Network. In ICLR,

[8] Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. In ICML, 2017.

[9] Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,

2017.

2015.

[10] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative

neural networks via maximum mean discrepancy optimization. In UAI, 2015.

[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander

Smola. A kernel two-sample test. JMLR, 2012.

[12] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

[13] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In CVPR, 2015.

[14] Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas,
Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized
maximum mean discrepancy. In ICLR, 2017.

[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel
mean embedding of distributions: A review and beyonds. arXiv preprint arXiv:1605.09522,
2016.

[16] Kenji Fukumizu, Arthur Gretton, Gert R Lanckriet, Bernhard Schölkopf, and Bharath K Sripe-
rumbudur. Kernel choice and classiﬁability for rkhs embeddings of probability distributions. In
NIPS, 2009.

[17] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and

K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.

[18] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano
Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale
two-sample tests. In NIPS, 2012.

10

[19] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel

learning. In AISTATS, 2016.

[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.

Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.

[21] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal

regression. 1999.

[22] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.

[23] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature

matching gan. arxiv pre-print 1702.08398, 2017.

[24] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learn-
ing. arXiv preprint arXiv:1702.08811, 2017.

[25] Shuangfei Zhai, Yu Cheng, Rogério Schmidt Feris, and Zhongfei Zhang. Generative adversarial
networks as variational training of energy based models. CoRR, abs/1611.01799, 2016.

[26] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

[27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Adversarial generator-encoder net-

works. arXiv preprint arXiv:1704.02304, 2017.

[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.

Improved techniques for training gans. In NIPS, 2016.

[29] David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717, 2017.

[30] Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Rémi Munos. The cramer distance as a solution to biased wasserstein
gradients. arXiv preprint arXiv:1705.10743, 2017.

[31] Arthur Gretton. Notes on the cramer gan. https://medium.com/towards-data-science/

notes-on-the-cramer-gan-752abd505c00, 2017. Accessed: 2017-11-2.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 1998.

[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[34] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with

deep convolutional generative adversarial networks. In ICLR, 2016.

[35] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.

[36] D Warde-Farley and Y Bengio. Improving generative adversarial networks with denoising

feature matching. In ICLR, 2017.

[37] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mas-

tropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017.

[38] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R.G.
Lanckriet. Hilbert space embeddings and metrics on probability measures. JMLR, 2010.

11

A Technical Proof

A.1 Proof of Theorem 3

Proof. Since MMD is a probabilistic metric [11], we have the triangular inequality for every Mf .
Therefore,
| max
φ

|Mfφ(PX , Pθ) − Mfφ(PX , Pθ(cid:48))|

Mfφ(PX , Pθ) − max

Mfφ(PX , Pθ(cid:48))| ≤

max
φ

(6)

φ

=

≤

|Mfφ∗ (PX , Pθ) − Mfφ∗ (PX , Pθ(cid:48))|
Mfφ∗ (Pθ, Pθ(cid:48)),

(7)

(8)

where φ∗ is the solution of (6).
By deﬁnition, given any φ ∈ Φ, deﬁne hθ = fφ ◦ gθ, the MMD distance Mfφ(Pθ, Pθ(cid:48))
= Ez,z(cid:48) [k(hθ(z), hθ(z(cid:48))) − 2k(hθ(z), hθ(cid:48)(z(cid:48))) + k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48)))]
≤ Ez,z(cid:48) [|k(hθ(z), hθ(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|] + Ez,z(cid:48) [|k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|]

In this, we consider Gaussian kernel k, therefore
|k(hθ(z), hθ(z(cid:48)))−k(hθ(z), hθ(cid:48)(z(cid:48)))| = | exp(−(cid:107)hθ(z)−hθ(z(cid:48))(cid:107)2)−exp(−(cid:107)hθ(z)−hθ(cid:48)(z(cid:48))(cid:107)2)| ≤ 1,
for all (θ, θ(cid:48)) and (z, z(cid:48)). Similarly, |k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))| ≤ 1. Combining the
above claim with (7) and bounded convergence theorem, we have

Mfφ(PX , Pθ) − max

| max
φ
which proves the continuity of maxf M M Df (PX , Pθ).
By Mean Value Theorem, |e−x2
with (8) and triangular inequality, we have

− e−y2

| ≤ maxz |2ze−z2

φ

Mfφ(PX , Pθ(cid:48))| θ→θ(cid:48)

−−−→ 0,

Mfφ(Pθ, Pθ(cid:48)) ≤ Ez,z(cid:48)[(cid:107)hθ(z(cid:48)) − hθ(cid:48)(z(cid:48))(cid:107) + (cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

= 2Ez[(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

| × |x − y| ≤ |x − y|. Incorporating it

Now let h be locally Lipschitz. For a given pair (θ, z) there is a constant L(θ, z) and an open set Uθ
such that for every (θ(cid:48), z) ∈ Uθ we have

(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107) ≤ L(θ, z)((cid:107)θ − θ(cid:48)(cid:107))

Under Assumption 2, we then achieve

|Mfφ(PX , Pθ) − Mfφ(PX , P(cid:48)

(9)
Combining (7) and (9) implies maxφ Mfφ(PX , Pθ) is locally Lipschitz and continuous everywhere.
Last, applying Radamacher’s theorem proves maxφ Mfφ(PX , Pθ) is differentiable almost everywhere,
which completes the proof.

θ)| ≤ Mfφ(Pθ, Pθ(cid:48)) ≤ 2Ez[L(θ, z)](cid:107)θ − θ(cid:48)(cid:107).

A.2 Proof of Theorem 4

Proof. The proof utilizes parts of results from [38].

If maxφ Mfφ(Pn, P) → 0, there exists φ ∈ Φ such that Mfφ(P, Pn) → 0 since Mfφ(P, Pn)

(⇒)
is non-negative. By [38], for any characteristic kernel k,

Therefore, if maxφ Mfφ(Pn, P) → 0, Pn

Mk(Pn, P) → 0 ⇐⇒ Pn
D−→ P.

D−→ P.

(⇐) By [38], given a characteristic kernel k, if supx,x(cid:48) k(x, x(cid:48)) ≤ 1, (cid:112)Mk(P, Q) ≤ W (P, Q),
where W (P, Q) is Wasserstein distance.
In this paper, we consider the kernel k(x, x(cid:48)) =
exp(−(cid:107)fφ(x) − fφ(x(cid:48))(cid:107)2) ≤ 1. By above, (cid:112)Mfφ(P, Q) ≤ W (P, Q), ∀φ. By [8], if Pn
D−→ P,
W (P, Pn) → 0. Combining all of them, we get

Pn

D−→ P =⇒ W (P, Pn) → 0 =⇒ max

Mfφ(P, Pn) → 0.

φ

12

A.3 Proof of Theorem 5

Proof. The proof assumes fφ(x) is scalar, but the vector case can be proved with the same sketch.
First, if E[fφ(x)] > E[fφ(gθ(z))], then φ = φ(cid:48). If E[fφ(x)] < E[fφ(gθ(z))], we let f = −fφ, then
E[f (x)] > E[f (gθ(z))] and ﬂipping sign does not change the MMD distance. If we parameterized
fφ by a neural network, which has a linear output layer, φ(cid:48) can realized by ﬂipping the sign of the
weights of the last layer.

B Property of MMD with Fixed and Learned Kernels

B.1 Continuity and Differentiability

One can simplify Theorem 3 and its proof for standard MMD distance to show MMD is also
continuous and differentiable almost everywhere. In [8], they propose a counterexample to show the
discontinuity of MMD by assuming H = L2. However, it is known that L2 is not in RKHS, so the
discussed counterexample is not appropriate.

B.2 IPM Framework

From integral probability metrics (IPM), the probabilistic distance can be deﬁned as

∆(P, Q) = sup
f ∈F

Ex∼P[f (x)] − Ey∼Q[f (y)].

(10)

By changing the function class F, we can recover several distances, such as total variation, Wasser-
stein distance and MMD distance. From [8], the discriminator fφ in different existing works of GAN
can be explained to be used to solve different probabilistic metrics based on (10). For MMD, the
function class F is {(cid:107)f (cid:107)Hk ≤ 1}, where H is RKHS associated with kernel k. Different form many
distances, such as total variation and Wasserstein distance, there is an analytical representation [11]
as we show in Section 2, which is

∆(P, Q) = M M D(P, Q) = (cid:107)µP − µQ(cid:107)H =

EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

(cid:113)

Because of the analytical representation of (10), GMMN does not need an additional network fφ for
estimating the distance.

Here we also provide an explanation of the proposed MMD with adversarially learned kernel under
IPM framework. The MMD distance with adversarially learned kernel is represented as

M M D(P, Q),

max
k∈K

The corresponding IPM formulation is
∆(P, Q) = max
k∈K

M M D(P, Q) =

sup
f ∈Hk1 ∪···∪Hkn

Ex∼P[f (x)] − Ey∼Q[f (y)],

where ki ∈ K, ∀i. From this perspective, the proposed MMD distance with adversarially learned
kernel is still deﬁned by IPM but with a larger function class.

B.3 MMD is an Efﬁcient Moment Matching

We consider the example of matching ﬁrst and second moments of P and Q. The (cid:96)1 objective in
McGAN [23] is

(cid:107)µP − µQ(cid:107)1 + (cid:107)ΣP − ΣQ(cid:107)∗,

where (cid:107) · (cid:107)∗ is the trace norm. The objective can be changed to be general (cid:96)q norm.

In MMD, with polynomial kernel k(x, y) = 1 + x(cid:62)y, the MMD distance is

2(cid:107)µP − µQ(cid:107)2 + (cid:107)ΣP − ΣQ + µPµ(cid:62)

P − µQµ(cid:62)

Q (cid:107)2
F ,

which is inexact moment matching because the second term contains the quadratic of the ﬁrst moment.
It is difﬁcult to match high-order moments, because we have to deal with high order tensors directly.
On the other hand, MMD can easily match high-order moments (even inﬁnite order moments by
using Gaussian kernel) with kernel tricks, and enjoys strong theoretical guarantee.

13

C Performance of GMMN

(a) GMMN-D on MNIST

(b) GMMN-C on MNIST

(c) GMMN-D on CIFAR-10

(d) GMMN-C CIFAR-10

Figure 6: Generative samples from GMMN-D and GMMN-C with training batch size B = 1024.

14

7
1
0
2
 
v
o
N
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
8
5
8
0
.
5
0
7
1
:
v
i
X
r
a

MMD GAN: Towards Deeper Understanding of
Moment Matching Network

Chun-Liang Li1,∗ Wei-Cheng Chang1,∗ Yu Cheng2 Yiming Yang1 Barnabás Póczos1

1 Carnegie Mellon University,

2AI Foundations, IBM Research

{chunlial,wchang2,yiming,bapoczos}@cs.cmu.edu
(∗ denotes equal contribution)

chengyu@us.ibm.com

Abstract

Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the discriminator
in GAN with a two-sample test based on kernel maximum mean discrepancy
(MMD). Although some theoretical guarantees of MMD have been studied, the
empirical performance of GMMN is still not as competitive as that of GAN on
challenging and large benchmark datasets. The computational efﬁciency of GMMN
is also less desirable in comparison with GAN, partially due to its requirement for
a rather large batch size during the training. In this paper, we propose to improve
both the model expressiveness of GMMN and its computational efﬁciency by
introducing adversarial kernel learning techniques, as the replacement of a ﬁxed
Gaussian kernel in the original GMMN. The new approach combines the key ideas
in both GMMN and GAN, hence we name it MMD GAN. The new distance measure
in MMD GAN is a meaningful loss that enjoys the advantage of weak∗ topology
and can be optimized via gradient descent with relatively small batch sizes. In our
evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA
and LSUN, the performance of MMD GAN signiﬁcantly outperforms GMMN, and
is competitive with other representative GAN works.

1

Introduction

The essence of unsupervised learning models the underlying distribution PX of the data X . Deep
generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with
promising results. However, modeling arbitrary density is a statistically challenging task [3]. In many
applications, such as caption generation [4], accurate density estimation is not even necessary since
we are only interested in sampling from the approximated distribution.
Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a
base distribution PZ over Z, such as Gaussian distribution, then trains a transformation network gθ
such that Pθ ≈ PX , where Pθ is the underlying distribution of gθ(z) and z ∼ PZ . During the training,
GAN-based algorithms require an auxiliary network fφ to estimate the distance between PX and Pθ.
Different probabilistic (pseudo) metrics have been studied [5–8] under GAN framework.
Instead of training an auxiliary network fφ for measuring the distance between PX and Pθ, Generative
moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11],
which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.
During the training, gθ is trained to pass the hypothesis test (minimize MMD distance). [11] shows
even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the
empirical performance of GMMN does not meet its theoretical properties. There is no promising
empirical results comparable with GAN on challenging benchmarks [12, 13]. Computationally,

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

it also requires larger batch size than GAN needs for training, which is considered to be less
efﬁcient [9, 10, 14, 8]

In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels
instead of ﬁxed Gaussian kernels to have better hypothesis testing power. The main contributions of
this work are:

• In Section 2, we prove that training gθ via MMD with learned kernels is continuous and differen-
tiable, which guarantees the model can be trained by gradient descent. Second, we prove a new
distance measure via kernel learning, which is a sensitive loss function to the distance between
PX and Pθ (weak∗ topology). Empirically, the loss decreases when two distributions get closer.
• In Section 3, we propose a practical realization called MMD GAN that learns generator gθ with
the adversarially trained kernel. We further propose a feasible set reduction to speed up and
stabilize the training of MMD GAN.

• In Section 5, we show that MMD GAN is computationally more efﬁcient than GMMN, which can
be trained with much smaller batch size. We also demonstrate that MMD GAN has promising
results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To
our best knowledge, we are the ﬁrst MMD based work to achieve comparable results with other
GAN works on these datasets.

Finally, we also study the connection to existing works in Section 4. Interestingly, we show Wasser-
stein GAN [8] is the special case of the proposed MMD GAN under certain conditions. The uniﬁed
view shows more connections between moment matching and GAN, which can potentially inspire
new algorithms based on well-developed tools in statistics [15]. Our experiment code is available at
https://github.com/OctoberChang/MMD-GAN.

2 GAN, Two-Sample Test and GMMN

i=1, where xi ∈ X and xi ∼ PX . If we are interested in sampling
Assume we are given data {xi}n
from PX , it is not necessary to estimate the density of PX . Instead, Generative Adversarial Network
(GAN) [5] trains a generator gθ parameterized by θ to transform samples z ∼ PZ , where z ∈ Z,
into gθ(z) ∼ Pθ such that Pθ ≈ PX . To measure the similarity between PX and Pθ via their
samples {x}n
j=1 during the training, [5] trains the discriminator fφ parameterized
by φ for help. The learning is done by playing a two-player game, where fφ tries to distinguish xi
and gθ(zj) while gθ aims to confuse fφ by generating gθ(zj) similar to xi.

i=1 and {gθ(zj)}n

On the other hand, distinguishing two distributions by ﬁnite samples is known as Two-Sample Test in
statistics. One way to conduct two-sample test is via kernel maximum mean discrepancy (MMD) [11].
Given two distributions P and Q, and a kernel k, the square of MMD distance is deﬁned as

Mk(P, Q) = (cid:107)µP − µQ(cid:107)2

H = EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

Theorem 1. [11] Given a kernel k, if k is a characteristic kernel, then Mk(P, Q) = 0 iff P = Q.

GMMN: One example of characteristic kernel is Gaussian kernel k(x, x(cid:48)) = exp((cid:107)x − x(cid:48)(cid:107)2). Based
on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains gθ by

Mk(PX , Pθ),

min
θ

(1)

with a ﬁxed Gaussian kernel k rather than training an additional discriminator f as GAN.

2.1 MMD with Kernel Learning

In practice we use ﬁnite samples from distributions to estimate MMD distance. Given X =
{x1, · · · , xn} ∼ P and Y = {y1, · · · , yn} ∼ Q, one estimator of Mk(P, Q) is

ˆMk(X, Y ) =

k(xi, x(cid:48)

i) −

k(xi, yj) +

k(yj, y(cid:48)

j).

1
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=i(cid:48)

2
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=j

1
(cid:1)
(cid:0)n
2

(cid:88)

j(cid:54)=j(cid:48)

Because of the sampling variance, ˆM (X, Y ) may not be zero even when P = Q. We then conduct
hypothesis test with null hypothesis H0 : P = Q. For a given allowable probability of false rejection α,

2

we can only reject H0, which imply P (cid:54)= Q, if ˆM (X, Y ) > cα for some chose threshold cα > 0.
Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to [11] for
more details.
Intuitively, if kernel k cannot result in high MMD distance Mk(P, Q) when P (cid:54)= Q, ˆMk(P, Q) has
more chance to be smaller than cα. Then we are unlikely to reject the null hypothesis H0 with ﬁnite
samples, which implies Q is not distinguishable from P. Therefore, instead of training gθ via (1) with
a pre-speciﬁed kernel k as GMMN, we consider training gθ via

min
θ

max
k∈K

Mk(PX , Pθ),

(2)

(3)

which takes different possible characteristic kernels k ∈ K into account. On the other hand, we
could also view (2) as replacing the ﬁxed kernel k in (1) with the adversarially learned kernel
arg maxk∈K Mk(PX , Pθ) to have stronger signal where P (cid:54)= Pθ to train gθ. We refer interested
readers to [16] for more rigorous discussions about testing power and increasing MMD distances.

However, it is difﬁcult to optimize over all characteristic kernels when we solve (2). By [11, 17] if f
is a injective function and k is characteristic, then the resulted kernel ˜k = k ◦ f , where ˜k(x, x(cid:48)) =
k(f (x), f (x(cid:48))) is still characteristic. If we have a family of injective functions parameterized by φ,
which is denoted as fφ, we are able to change the objective to be

min
θ

max
φ

Mk◦fφ(PX , Pθ),

In this paper, we consider the case that combining Gaussian kernels with injective functions fφ, where
˜k(x, x(cid:48)) = exp(−(cid:107)fφ(x) − fφ(x)(cid:48)(cid:107)2). One example function class of f is {fφ|fφ(x) = φx, φ > 0},
which is equivalent to the kernel bandwidth tuning. A more complicated realization will be discussed
in Section 3. Next, we abuse the notation Mfφ(P, Q) to be MMD distance given the composition
kernel of Gaussian kernel and fφ in the following. Note that [18] considers the linear combination
of characteristic kernels, which can also be incorporated into the discussed composition kernels. A
more general kernel is studied in [19].

2.2 Properties of MMD with Kernel Learning

[8] discuss different distances between distributions adopted by existing deep learning algorithms, and
show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7],
except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training.
From (3), we train gθ via minimizing maxφ Mfφ(PX , Pθ). Next, we show maxφ Mfφ(PX , Pθ) also
enjoys the advantage of being a continuous and differentiable objective in θ under mild assumptions.
Assumption 2. g : Z × Rm → X is locally Lipschitz, where Z ⊆ Rd. We will denote gθ(z) the
evaluation on (z, θ) for convenience. Given fφ and a probability distribution Pz over Z, g satisﬁes
Assumption 2 if there are local Lipschitz constants L(θ, z) for fφ ◦ g, which is independent of φ, such
that Ez∼Pz [L(θ, z)] < +∞.
Theorem 3. The generator function gθ parameterized by θ is under Assumption 2. Let PX be a ﬁxed
distribution over X and Z be a random variable over the space Z. We denote Pθ the distribution
of gθ(Z), then maxφ Mfφ(PX , Pθ) is continuous everywhere and differentiable almost everywhere
in θ.

If gθ is parameterized by a feed-forward neural network, it satisﬁes Assumption 2 and can be trained
via gradient descent as well as propagation, since the objective is continuous and differentiable
followed by Theorem 3. More technical discussions are shown in Appendix B.
Theorem 4. (weak∗ topology) Let {Pn} be a sequence of distributions. Considering n → ∞,
D−→ PX , where D−→ means converging in
under mild Assumption, maxφ Mfφ(PX , Pn) → 0 ⇐⇒ Pn
distribution [3].

Theorem 4 shows that maxφ Mfφ(PX , Pn) is a sensible cost function to the distance between PX
and Pn. The distance is decreasing when Pn is getting closer to PX , which beneﬁts the supervision
of the improvement during the training. All proofs are omitted to Appendix A. In the next section,
we introduce a practical realization of training gθ via optimizing minθ maxφ Mfφ(PX , Pθ).

3

3 MMD GAN

To approximate (3), we use neural networks to parameterized gθ and fφ with expressive power.
For gθ, the assumption is locally Lipschitz, where commonly used feed-forward neural networks
satisfy this constraint. Also, the gradient (cid:53)θ (maxφ fφ ◦ gθ) has to be bounded, which can be
done by clipping φ [8] or gradient penalty [20]. The non-trivial part is fφ has to be injective.
For an injective function f , there exists an function f −1 such that f −1(f (x)) = x, ∀x ∈ X and
f −1(f (g(z))) = g(z), ∀z ∈ Z 1, which can be approximated by an autoencoder. In the following,
we denote φ = {φe, φd} to be the parameter of discriminator networks, which consists of an encoder
fφe , and train the corresponding decoder fφd ≈ f −1 to regularize f . The objective (3) is relaxed to
be

min
θ

max
φ

Mfφe

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe(y))(cid:107)2.

(4)

Note that we ignore the autoencoder objective when we train θ, but we use (4) for a concise
presentation. We note that the empirical study suggests autoencoder objective is not necessary to
lead the successful GAN training as we will show in Section 5, even though the injective property is
required in Theorem 1.

The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks gθ and fφ
in a minmax formulation, while the meaning of the objective is different. In [5], fφe is a discriminator
(binary) classiﬁer to distinguish two distributions. In the proposed algorithm, distinguishing two
distribution is still done by two-sample test via MMD, but with an adversarially learned kernel
parametrized by fφe . gθ is then trained to pass the hypothesis test. More connection and difference
with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed
algorithm MMD GAN. We present an implementation with the weight clipping in Algorithm 1, but
one can easily extend to other Lipschitz approximations, such as gradient penalty [20].

Algorithm 1: MMD GAN, our proposed algorithm.
input

:α the learning rate, c the clipping parameter, B the batch size, nc the number of iterations of
discriminator per generator update.

initialize generator parameter θ and discriminator parameter φ;
while θ has not converged do
for t = 1, . . . , nc do

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe (y))(cid:107)2

Sample a minibatches {xi}B
gφ ← ∇φMfφe
φ ← φ + α · RMSProp(φ, gφ)
φ ← clip(φ, −c, c)
Sample a minibatches {xi}B
gθ ← ∇θMfφe
θ ← θ − α · RMSProp(θ, gθ)

(P(X ), P(gθ(Z)))

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

Encoding Perspective of MMD GAN: Besides from using kernel selection to explain MMD GAN,
the other way to see the proposed MMD GAN is viewing fφe as a feature transformation function,
and the kernel two-sample test is performed on this transformed feature space (i.e., the code space of
the autoencoder). The optimization is ﬁnding a manifold with stronger signals for MMD two-sample
test. From this perspective, [9] is the special case of MMD GAN if fφe is the identity mapping
function. In such circumstance, the kernel two-sample test is conducted in the original data space.

3.1 Feasible Set Reduction

Theorem 5. For any fφ, there exists f (cid:48)
Ez[fφ(cid:48)(gθ(z))].

φ such that Mfφ(Pr, Pθ) = Mf (cid:48)

φ

(Pr, Pθ) and Ex[fφ(x)] (cid:23)

With Theorem 5, we could reduce the feasible set of φ during the optimization by solving

minθ maxφ Mfφ(Pr, Pθ)

s.t. E[fφ(x)] (cid:23) E[fφ(gθ(z))]

1Note that injective is not necessary invertible.

4

which the optimal solution is still equivalent to solving (2).

However, it is hard to solve the constrained optimization problem with backpropagation. We relax
the constraint by ordinal regression [21] to be

min
θ

max
φ

Mfφ(Pr, Pθ) + λ min (cid:0)E[fφ(x)] − E[fφ(gθ(z))], 0(cid:1),

which only penalizes the objective when the constraint is violated. In practice, we observe that
reducing the feasible set makes the training faster and stabler.

4 Related Works

There has been a recent surge on improving GAN [5]. We review some related works here.

If we composite fφ with linear kernel instead of Gaussian kernel, and

Connection with WGAN:
restricting the output dimension h to be 1, we then have the objective
(cid:107)E[fφ(x)] − E[fφ(gθ(z))](cid:107)2.

(5)

min
θ

max
φ
Parameterizing fφ and gθ with neural networks and assuming ∃φ(cid:48) ∈ Φ such f (cid:48)
φ = −fφ, ∀Φ, recovers
Wasserstein GAN (WGAN) [8] 2. If we treat fφ(x) as the data transform function, WGAN can
be interpreted as ﬁrst-order moment matching (linear kernel) while MMD GAN aims to match
inﬁnite order of moments with Gaussian kernel form Taylor expansion [9]. Theoretically, Wasserstein
distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, [22] show neural
networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we
demonstrate matching high-order moments beneﬁts the results. [23] also propose McGAN that
matches second order moment from the primal-dual norm perspective. However, the proposed
algorithm requires matrix (tensor) decompositions because of exact moment matching [24], which
is hard to scale to higher order moment matching. On the other hand, by giving up exact moment
matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions
are in Appendix B.3.

Difference from Other Works with Autoencoders: Energy-based GANs [7, 25] also utilizes
the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes
the reconstruction error of real samples x while maximize the reconstruction error of generated
samples gθ(z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing
the reconstruction errors of both real samples x and generated samples gθ(z). Also, [8] show EBGAN
approximates total variation, with the drawback of discontinuity, while MMD GAN optimizes MMD
distance. The other line of works [2, 26, 9] aims to match the AE codespace f (x), and utilize the
decoder fdec(·). [2, 26] match the distribution of f (x) and z via different distribution distances and
generate data (e.g. image) by fdec(z). [9] use MMD to match f (x) and g(z), and generate data via
fdec(g(z)). The proposed MMD GAN matches the f (x) and f (g(z)), and generates data via g(z)
directly as GAN. [27] is similar to MMD GAN but it considers KL-divergence without showing
continuity and weak∗ topology guarantee as we prove in Section 2.

Other GAN Works:
In addition to the discussed works, there are several extended works of GAN.
[28] proposes using the linear kernel to match ﬁrst moment of its discriminator’s latent features. [14]
considers the variance of empirical MMD score during the training. Also, [14] only improves the
latent feature matching in [28] by using kernel MMD, instead of proposing an adversarial training
framework as we studied in Section 2. [29] uses Wasserstein distance to match the distribution of
autoencoder loss instead of data. One can consider to extend [29] to higher order matching based on
the proposed MMD GAN. A parallel work [30] use energy distance, which can be treated as MMD
GAN with different kernel. However, there are some potential problems of its critic. More discussion
can be referred to [31].

5 Experiment

We train MMD GAN for image generation on the MNIST [32], CIFAR-10 [33], CelebA [13],
and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M

2Theoretically, they are not equivalent but the practical neural network approximation results in the same

algorithm.

5

respectively. All the samples images are generated from a ﬁxed noise random vectors and are not
cherry-picked.

Network architecture: In our experiments, we follow the architecture of DCGAN [34] to design gθ
by its generator and fφ by its discriminator except for expanding the output layer of fφ to be h
dimensions.

Kernel designs: The loss function of MMD GAN is implicitly associated with a family of character-
istic kernels. Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF
kernels k(x, x(cid:48)) = (cid:80)K
q=1 kσq (x, x(cid:48)) where kσq is a Gaussian kernel with bandwidth parameter σq.
Tuning kernel bandwidth σq optimally still remains an open problem. In this works, we ﬁxed K = 5
and σq to be {1, 2, 4, 8, 16} and left the fφ to learn the kernel (feature representation) under these σq.

Hyper-parameters: We use RMSProp [35] with learning rate of 0.00005 for a fair comparison with
WGAN as suggested in its original paper [8]. We ensure the boundedness of model parameters
of discriminator by clipping the weights point-wisely to the range [−0.01, 0.01] as required by
Assumption 2. The dimensionality h of the latent space is manually set according to the complexity
of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and
LSUN bedrooms. The batch size is set to be B = 64 for all datasets.

5.1 Qualitative Analysis

(a) GMMN-D MNIST

(b) GMMN-C MNIST

(c) MMD GAN MNIST

(d) GMMN-D CIFAR-10

(e) GMMN-C CIFAR-10

(f) MMD GAN CIFAR-10

Figure 1: Generated samples from GMMN-D (Dataspace), GMMN-C (Codespace) and our MMD
GAN with batch size B = 64.

We start with comparing MMD GAN with GMMN on two standard benchmarks, MNIST and CIFAR-
10. We consider two variants for GMMN. The ﬁrst one is original GMMN, which trains the generator
by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare
with MMD GAN, we also pretrain an autoencoder for projecting data to a manifold, then ﬁx the
autoencoder as a feature transformation, and train the generator by minimizing the MMD distance in
the code space. We call it as GMMN-C.

The results are pictured in Figure 1. Both GMMN-D and GMMN-C are able to generate meaningful
digits on MNIST because of the simple data structure. By a closer look, nonetheless, the boundary
and shape of the digits in Figure 1a and 1b are often irregular and non-smooth. In contrast, the sample

6

(a) WGAN MNIST

(b) WGAN CelebA

(c) WGAN LSUN

(d) MMD GAN MNIST

(e) MMD GAN CelebA

(f) MMD GAN LSUN

Figure 2: Generated samples from WGAN and MMD GAN on MNIST, CelebA, and LSUN bedroom
datasets.

digits in Figure 1c are more natural with smooth outline and sharper strike. For CIFAR-10 dataset,
both GMMN variants fail to generate meaningful images, but resulting some low level visual features.
We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms,
thus results are omitted. On the other hand, the proposed MMD GAN successfully outputs natural
images with sharp boundary and high diversity. The results in Figure 1 conﬁrm the success of the
proposed adversarial learned kernels to enrich statistical testing power, which is the key difference
between GMMN and MMD GAN.

If we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still
not competitive to MMD GAN with B = 64. The images are put in Appendix C. This demonstrates
that the proposed MMD GAN can be trained more efﬁciently than GMMN with smaller batch size.

Comparisons with GANs: There are several representative extensions of GANs. We consider
recent state-of-art WGAN [8] based on DCGAN structure [34], because of the connection with MMD
GAN discussed in Section 4. The results are shown in Figure 2. For MNIST, the digits generated
from WGAN in Figure 2a are more unnatural with peculiar strikes. In Contrary, the digits from
MMD GAN in Figure 2d enjoy smoother contour. Furthermore, both WGAN and MMD GAN
generate diversiﬁed digits, avoiding the mode collapse problems appeared in the literature of training
GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD GAN.
Speciﬁcally, we observe varied poses, expressions, genders, skin colors and light exposure in Figure
2b and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN
have higher chances to be blurry and twisted while faces from MMD GAN are more spontaneous with
sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient differences
between the samples generated from MMD GAN and WGAN.

5.2 Quantitative Analysis

To quantitatively measure the quality and diversity of generated samples, we compute the inception
score [28] on CIFAR-10 images. The inception score is used for GANs to measure samples quality
and diversity on the pretrained inception model [28]. Models that generate collapsed samples have
a relatively low score. Table 1 lists the results for 50K samples generated by various unsupervised

7

generative models trained on CIFAR-10 dataset. The inception scores of [36, 37, 28] are directly
derived from the corresponding references.

Although both WGAN and MMD GAN can generate sharp images as we show in Section 5.1, our
score is better than other GAN techniques except for DFM [36]. This seems to conﬁrm empirically
that higher order of moment matching between the real data and fake sample distribution beneﬁts
generating more diversiﬁed sample images. Also note DFM appears compatible with our method and
combing training techniques in DFM is a possible avenue for future work.

Method

Real data

Scores ± std.

11.95 ± .20

DFM [36]
ALI [37]
Improved GANs [28]

7.72
5.34
4.36

MMD GAN
WGAN
GMMN-C
GMMN-D
Table 1: Inception scores

6.17 ± .07
5.88 ± .07
3.94 ± .04
3.47 ± .03

5.3 Stability of MMD GAN

Figure 3: Computation time

We further illustrate how the MMD distance correlates well with the quality of the generated samples.
Figure 4 plots the evolution of the MMD GAN estimate the MMD distance during training for
MNIST, CelebA and LSUN datasets. We report the average of the ˆMfφ(PX , Pθ) with moving
average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We
observe during the whole training process, samples generated from the same noise vector across
iterations, remain similar in nature. (e.g., face identity and bedroom style are alike while details and
backgrounds will evolve.) This qualitative observation indicates valuable stability of the training
process. The decreasing curve with the improving quality of images supports the weak∗ topology
shown in Theorem 4. Also, We can see from the plot that the model converges very quickly. In Figure
4b, for example, it converges shortly after tens of thousands of generator iterations on CelebA dataset.

(a) MNIST

(b) CelebA

(c) LSUN Bedrooms

Figure 4: Training curves and generative samples at different stages of training. We can see a clear
correlation between lower distance and better sample quality.

5.4 Computation Issue

We conduct time complexity analysis with respect to the batch size B. The time complexity of each
iteration is O(B) for WGAN and O(KB2) for our proposed MMD GAN with a mixture of K RBF
kernels. The quadratic complexity O(B2) of MMD GAN is introduced by computing kernel matrix,
which is sometimes criticized for being inapplicable with large batch size in practice. However,
we point that there are several recent works, such as EBGAN [7], also matching pairwise relation
between samples of batch size, leading to O(B2) complexity as well.

8

Empirically, we ﬁnd that under GPU environment, the highly parallelized matrix operation tremen-
dously alleviated the quadratic time to almost linear time with modest B. Figure 3 compares the
computational time per generator iterations versus different B on Titan X. When B = 64, which
is adapted for training MMD GAN in our experiments setting, the time per iteration of WGAN
and MMD GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for
training GMMN in its references [9], the time per iteration becomes 4.431 and 8.565 seconds, respec-
tively. This result coheres our argument that the empirical computational time for MMD GAN is not
quadratically expensive compared to WGAN with powerful GPU parallel computation.

5.5 Better Lipschitz Approximation and Necessity of Auto-Encoder

We used weight-clipping for Lipschitz constraint in Assumption 2. Another approach for obtaining a
discriminator with the similar constraints that approximates a Wasserstein distance is [20], where the
gradient of the discriminator is constrained to be 1 between the generated and data points. Inspired
by [20], an alternative approach is to apply the gradient constraint as a regularizer to the witness
function for a different IPM, such as the MMD. This idea was ﬁrst proposed in [30] for the Energy
Distance (in parallel to our submission), which was shown in [31] to correspond to a gradient penalty
on the witness function for any RKHS-based MMD. Here we undertake a preliminary investigation
of this approach, where we also drop the requirement in Algorithm 1 for fφ to be injective, which we
observe that it is not necessary in practice. We show some preliminary results of training MMD GAN
with gradient penalty and without the auto-encoder in Figure 5. The preliminary study indicates that
MMD GAN can generate satisfactory results with other Lipschitz constraint approximation. One
potential future work is conducting more thorough empirical comparison studies between different
approximations.

(a) Cifar10, Giter = 300K

(b) CelebA, Giter = 300K

Figure 5: MMD GAN results using gradient penalty [20] and without auto-encoder reconstruction
loss during training.

6 Discussion

We introduce a new deep generative model trained via MMD with adversarially learned kernels. We
further study its theoretical properties and propose a practical realization MMD GAN, which can be
trained with much smaller batch size than GMMN and has competitive performances with state-of-the-
art GANs. We can view MMD GAN as the ﬁrst practical step forward connecting moment matching
network and GAN. One important direction is applying developed tools in moment matching [15] on
general GAN works based the connections shown by MMD GAN. Also, in Section 4, we connect
WGAN and MMD GAN by ﬁrst-order and inﬁnite-order moment matching. [24] shows ﬁnite-order
moment matching (∼ 5) achieves the best performance on domain adaption. One could extend MMD
GAN to this by using polynomial kernels. Last, in theory, an injective mapping fφ is necessary for
the theoretical guarantees. However, we observe that it is not mandatory in practice as we show
in Section 5.5. One conjecture is it usually learns the injective mapping with high probability by
parameterizing with neural networks, which worth more study as a future work.

9

We thank the reviewers for their helpful comments. We also thank Dougal Sutherland and Arthur
Gretton for their valuable feedbacks and pointing out an error in the previous draft. This work is
supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-
1563887.

Acknowledgments

References

[1] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In AISTATS, 2009.

[2] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.

[3] Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &

Business Media, 2013.

[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In ICML, 2015.

[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[6] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural

samplers using variational divergence minimization. In NIPS, 2016.

[7] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based Generative Adversarial Network. In ICLR,

[8] Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. In ICML, 2017.

[9] Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,

2017.

2015.

[10] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative

neural networks via maximum mean discrepancy optimization. In UAI, 2015.

[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander

Smola. A kernel two-sample test. JMLR, 2012.

[12] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

[13] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In CVPR, 2015.

[14] Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas,
Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized
maximum mean discrepancy. In ICLR, 2017.

[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel
mean embedding of distributions: A review and beyonds. arXiv preprint arXiv:1605.09522,
2016.

[16] Kenji Fukumizu, Arthur Gretton, Gert R Lanckriet, Bernhard Schölkopf, and Bharath K Sripe-
rumbudur. Kernel choice and classiﬁability for rkhs embeddings of probability distributions. In
NIPS, 2009.

[17] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and

K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.

[18] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano
Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale
two-sample tests. In NIPS, 2012.

10

[19] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel

learning. In AISTATS, 2016.

[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.

Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.

[21] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal

regression. 1999.

[22] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.

[23] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature

matching gan. arxiv pre-print 1702.08398, 2017.

[24] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learn-
ing. arXiv preprint arXiv:1702.08811, 2017.

[25] Shuangfei Zhai, Yu Cheng, Rogério Schmidt Feris, and Zhongfei Zhang. Generative adversarial
networks as variational training of energy based models. CoRR, abs/1611.01799, 2016.

[26] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

[27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Adversarial generator-encoder net-

works. arXiv preprint arXiv:1704.02304, 2017.

[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.

Improved techniques for training gans. In NIPS, 2016.

[29] David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717, 2017.

[30] Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Rémi Munos. The cramer distance as a solution to biased wasserstein
gradients. arXiv preprint arXiv:1705.10743, 2017.

[31] Arthur Gretton. Notes on the cramer gan. https://medium.com/towards-data-science/

notes-on-the-cramer-gan-752abd505c00, 2017. Accessed: 2017-11-2.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 1998.

[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[34] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with

deep convolutional generative adversarial networks. In ICLR, 2016.

[35] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.

[36] D Warde-Farley and Y Bengio. Improving generative adversarial networks with denoising

feature matching. In ICLR, 2017.

[37] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mas-

tropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017.

[38] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R.G.
Lanckriet. Hilbert space embeddings and metrics on probability measures. JMLR, 2010.

11

A Technical Proof

A.1 Proof of Theorem 3

Proof. Since MMD is a probabilistic metric [11], we have the triangular inequality for every Mf .
Therefore,
| max
φ

|Mfφ(PX , Pθ) − Mfφ(PX , Pθ(cid:48))|

Mfφ(PX , Pθ) − max

Mfφ(PX , Pθ(cid:48))| ≤

max
φ

(6)

φ

=

≤

|Mfφ∗ (PX , Pθ) − Mfφ∗ (PX , Pθ(cid:48))|
Mfφ∗ (Pθ, Pθ(cid:48)),

(7)

(8)

where φ∗ is the solution of (6).
By deﬁnition, given any φ ∈ Φ, deﬁne hθ = fφ ◦ gθ, the MMD distance Mfφ(Pθ, Pθ(cid:48))
= Ez,z(cid:48) [k(hθ(z), hθ(z(cid:48))) − 2k(hθ(z), hθ(cid:48)(z(cid:48))) + k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48)))]
≤ Ez,z(cid:48) [|k(hθ(z), hθ(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|] + Ez,z(cid:48) [|k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|]

In this, we consider Gaussian kernel k, therefore
|k(hθ(z), hθ(z(cid:48)))−k(hθ(z), hθ(cid:48)(z(cid:48)))| = | exp(−(cid:107)hθ(z)−hθ(z(cid:48))(cid:107)2)−exp(−(cid:107)hθ(z)−hθ(cid:48)(z(cid:48))(cid:107)2)| ≤ 1,
for all (θ, θ(cid:48)) and (z, z(cid:48)). Similarly, |k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))| ≤ 1. Combining the
above claim with (7) and bounded convergence theorem, we have

Mfφ(PX , Pθ) − max

| max
φ
which proves the continuity of maxf M M Df (PX , Pθ).
By Mean Value Theorem, |e−x2
with (8) and triangular inequality, we have

− e−y2

| ≤ maxz |2ze−z2

φ

Mfφ(PX , Pθ(cid:48))| θ→θ(cid:48)

−−−→ 0,

Mfφ(Pθ, Pθ(cid:48)) ≤ Ez,z(cid:48)[(cid:107)hθ(z(cid:48)) − hθ(cid:48)(z(cid:48))(cid:107) + (cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

= 2Ez[(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

| × |x − y| ≤ |x − y|. Incorporating it

Now let h be locally Lipschitz. For a given pair (θ, z) there is a constant L(θ, z) and an open set Uθ
such that for every (θ(cid:48), z) ∈ Uθ we have

(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107) ≤ L(θ, z)((cid:107)θ − θ(cid:48)(cid:107))

Under Assumption 2, we then achieve

|Mfφ(PX , Pθ) − Mfφ(PX , P(cid:48)

(9)
Combining (7) and (9) implies maxφ Mfφ(PX , Pθ) is locally Lipschitz and continuous everywhere.
Last, applying Radamacher’s theorem proves maxφ Mfφ(PX , Pθ) is differentiable almost everywhere,
which completes the proof.

θ)| ≤ Mfφ(Pθ, Pθ(cid:48)) ≤ 2Ez[L(θ, z)](cid:107)θ − θ(cid:48)(cid:107).

A.2 Proof of Theorem 4

Proof. The proof utilizes parts of results from [38].

If maxφ Mfφ(Pn, P) → 0, there exists φ ∈ Φ such that Mfφ(P, Pn) → 0 since Mfφ(P, Pn)

(⇒)
is non-negative. By [38], for any characteristic kernel k,

Therefore, if maxφ Mfφ(Pn, P) → 0, Pn

Mk(Pn, P) → 0 ⇐⇒ Pn
D−→ P.

D−→ P.

(⇐) By [38], given a characteristic kernel k, if supx,x(cid:48) k(x, x(cid:48)) ≤ 1, (cid:112)Mk(P, Q) ≤ W (P, Q),
where W (P, Q) is Wasserstein distance.
In this paper, we consider the kernel k(x, x(cid:48)) =
exp(−(cid:107)fφ(x) − fφ(x(cid:48))(cid:107)2) ≤ 1. By above, (cid:112)Mfφ(P, Q) ≤ W (P, Q), ∀φ. By [8], if Pn
D−→ P,
W (P, Pn) → 0. Combining all of them, we get

Pn

D−→ P =⇒ W (P, Pn) → 0 =⇒ max

Mfφ(P, Pn) → 0.

φ

12

A.3 Proof of Theorem 5

Proof. The proof assumes fφ(x) is scalar, but the vector case can be proved with the same sketch.
First, if E[fφ(x)] > E[fφ(gθ(z))], then φ = φ(cid:48). If E[fφ(x)] < E[fφ(gθ(z))], we let f = −fφ, then
E[f (x)] > E[f (gθ(z))] and ﬂipping sign does not change the MMD distance. If we parameterized
fφ by a neural network, which has a linear output layer, φ(cid:48) can realized by ﬂipping the sign of the
weights of the last layer.

B Property of MMD with Fixed and Learned Kernels

B.1 Continuity and Differentiability

One can simplify Theorem 3 and its proof for standard MMD distance to show MMD is also
continuous and differentiable almost everywhere. In [8], they propose a counterexample to show the
discontinuity of MMD by assuming H = L2. However, it is known that L2 is not in RKHS, so the
discussed counterexample is not appropriate.

B.2 IPM Framework

From integral probability metrics (IPM), the probabilistic distance can be deﬁned as

∆(P, Q) = sup
f ∈F

Ex∼P[f (x)] − Ey∼Q[f (y)].

(10)

By changing the function class F, we can recover several distances, such as total variation, Wasser-
stein distance and MMD distance. From [8], the discriminator fφ in different existing works of GAN
can be explained to be used to solve different probabilistic metrics based on (10). For MMD, the
function class F is {(cid:107)f (cid:107)Hk ≤ 1}, where H is RKHS associated with kernel k. Different form many
distances, such as total variation and Wasserstein distance, there is an analytical representation [11]
as we show in Section 2, which is

∆(P, Q) = M M D(P, Q) = (cid:107)µP − µQ(cid:107)H =

EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

(cid:113)

Because of the analytical representation of (10), GMMN does not need an additional network fφ for
estimating the distance.

Here we also provide an explanation of the proposed MMD with adversarially learned kernel under
IPM framework. The MMD distance with adversarially learned kernel is represented as

M M D(P, Q),

max
k∈K

The corresponding IPM formulation is
∆(P, Q) = max
k∈K

M M D(P, Q) =

sup
f ∈Hk1 ∪···∪Hkn

Ex∼P[f (x)] − Ey∼Q[f (y)],

where ki ∈ K, ∀i. From this perspective, the proposed MMD distance with adversarially learned
kernel is still deﬁned by IPM but with a larger function class.

B.3 MMD is an Efﬁcient Moment Matching

We consider the example of matching ﬁrst and second moments of P and Q. The (cid:96)1 objective in
McGAN [23] is

(cid:107)µP − µQ(cid:107)1 + (cid:107)ΣP − ΣQ(cid:107)∗,

where (cid:107) · (cid:107)∗ is the trace norm. The objective can be changed to be general (cid:96)q norm.

In MMD, with polynomial kernel k(x, y) = 1 + x(cid:62)y, the MMD distance is

2(cid:107)µP − µQ(cid:107)2 + (cid:107)ΣP − ΣQ + µPµ(cid:62)

P − µQµ(cid:62)

Q (cid:107)2
F ,

which is inexact moment matching because the second term contains the quadratic of the ﬁrst moment.
It is difﬁcult to match high-order moments, because we have to deal with high order tensors directly.
On the other hand, MMD can easily match high-order moments (even inﬁnite order moments by
using Gaussian kernel) with kernel tricks, and enjoys strong theoretical guarantee.

13

C Performance of GMMN

(a) GMMN-D on MNIST

(b) GMMN-C on MNIST

(c) GMMN-D on CIFAR-10

(d) GMMN-C CIFAR-10

Figure 6: Generative samples from GMMN-D and GMMN-C with training batch size B = 1024.

14

7
1
0
2
 
v
o
N
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
8
5
8
0
.
5
0
7
1
:
v
i
X
r
a

MMD GAN: Towards Deeper Understanding of
Moment Matching Network

Chun-Liang Li1,∗ Wei-Cheng Chang1,∗ Yu Cheng2 Yiming Yang1 Barnabás Póczos1

1 Carnegie Mellon University,

2AI Foundations, IBM Research

{chunlial,wchang2,yiming,bapoczos}@cs.cmu.edu
(∗ denotes equal contribution)

chengyu@us.ibm.com

Abstract

Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the discriminator
in GAN with a two-sample test based on kernel maximum mean discrepancy
(MMD). Although some theoretical guarantees of MMD have been studied, the
empirical performance of GMMN is still not as competitive as that of GAN on
challenging and large benchmark datasets. The computational efﬁciency of GMMN
is also less desirable in comparison with GAN, partially due to its requirement for
a rather large batch size during the training. In this paper, we propose to improve
both the model expressiveness of GMMN and its computational efﬁciency by
introducing adversarial kernel learning techniques, as the replacement of a ﬁxed
Gaussian kernel in the original GMMN. The new approach combines the key ideas
in both GMMN and GAN, hence we name it MMD GAN. The new distance measure
in MMD GAN is a meaningful loss that enjoys the advantage of weak∗ topology
and can be optimized via gradient descent with relatively small batch sizes. In our
evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA
and LSUN, the performance of MMD GAN signiﬁcantly outperforms GMMN, and
is competitive with other representative GAN works.

1

Introduction

The essence of unsupervised learning models the underlying distribution PX of the data X . Deep
generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with
promising results. However, modeling arbitrary density is a statistically challenging task [3]. In many
applications, such as caption generation [4], accurate density estimation is not even necessary since
we are only interested in sampling from the approximated distribution.
Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a
base distribution PZ over Z, such as Gaussian distribution, then trains a transformation network gθ
such that Pθ ≈ PX , where Pθ is the underlying distribution of gθ(z) and z ∼ PZ . During the training,
GAN-based algorithms require an auxiliary network fφ to estimate the distance between PX and Pθ.
Different probabilistic (pseudo) metrics have been studied [5–8] under GAN framework.
Instead of training an auxiliary network fφ for measuring the distance between PX and Pθ, Generative
moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11],
which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.
During the training, gθ is trained to pass the hypothesis test (minimize MMD distance). [11] shows
even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the
empirical performance of GMMN does not meet its theoretical properties. There is no promising
empirical results comparable with GAN on challenging benchmarks [12, 13]. Computationally,

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

it also requires larger batch size than GAN needs for training, which is considered to be less
efﬁcient [9, 10, 14, 8]

In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels
instead of ﬁxed Gaussian kernels to have better hypothesis testing power. The main contributions of
this work are:

• In Section 2, we prove that training gθ via MMD with learned kernels is continuous and differen-
tiable, which guarantees the model can be trained by gradient descent. Second, we prove a new
distance measure via kernel learning, which is a sensitive loss function to the distance between
PX and Pθ (weak∗ topology). Empirically, the loss decreases when two distributions get closer.
• In Section 3, we propose a practical realization called MMD GAN that learns generator gθ with
the adversarially trained kernel. We further propose a feasible set reduction to speed up and
stabilize the training of MMD GAN.

• In Section 5, we show that MMD GAN is computationally more efﬁcient than GMMN, which can
be trained with much smaller batch size. We also demonstrate that MMD GAN has promising
results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To
our best knowledge, we are the ﬁrst MMD based work to achieve comparable results with other
GAN works on these datasets.

Finally, we also study the connection to existing works in Section 4. Interestingly, we show Wasser-
stein GAN [8] is the special case of the proposed MMD GAN under certain conditions. The uniﬁed
view shows more connections between moment matching and GAN, which can potentially inspire
new algorithms based on well-developed tools in statistics [15]. Our experiment code is available at
https://github.com/OctoberChang/MMD-GAN.

2 GAN, Two-Sample Test and GMMN

i=1, where xi ∈ X and xi ∼ PX . If we are interested in sampling
Assume we are given data {xi}n
from PX , it is not necessary to estimate the density of PX . Instead, Generative Adversarial Network
(GAN) [5] trains a generator gθ parameterized by θ to transform samples z ∼ PZ , where z ∈ Z,
into gθ(z) ∼ Pθ such that Pθ ≈ PX . To measure the similarity between PX and Pθ via their
samples {x}n
j=1 during the training, [5] trains the discriminator fφ parameterized
by φ for help. The learning is done by playing a two-player game, where fφ tries to distinguish xi
and gθ(zj) while gθ aims to confuse fφ by generating gθ(zj) similar to xi.

i=1 and {gθ(zj)}n

On the other hand, distinguishing two distributions by ﬁnite samples is known as Two-Sample Test in
statistics. One way to conduct two-sample test is via kernel maximum mean discrepancy (MMD) [11].
Given two distributions P and Q, and a kernel k, the square of MMD distance is deﬁned as

Mk(P, Q) = (cid:107)µP − µQ(cid:107)2

H = EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

Theorem 1. [11] Given a kernel k, if k is a characteristic kernel, then Mk(P, Q) = 0 iff P = Q.

GMMN: One example of characteristic kernel is Gaussian kernel k(x, x(cid:48)) = exp((cid:107)x − x(cid:48)(cid:107)2). Based
on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains gθ by

Mk(PX , Pθ),

min
θ

(1)

with a ﬁxed Gaussian kernel k rather than training an additional discriminator f as GAN.

2.1 MMD with Kernel Learning

In practice we use ﬁnite samples from distributions to estimate MMD distance. Given X =
{x1, · · · , xn} ∼ P and Y = {y1, · · · , yn} ∼ Q, one estimator of Mk(P, Q) is

ˆMk(X, Y ) =

k(xi, x(cid:48)

i) −

k(xi, yj) +

k(yj, y(cid:48)

j).

1
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=i(cid:48)

2
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=j

1
(cid:1)
(cid:0)n
2

(cid:88)

j(cid:54)=j(cid:48)

Because of the sampling variance, ˆM (X, Y ) may not be zero even when P = Q. We then conduct
hypothesis test with null hypothesis H0 : P = Q. For a given allowable probability of false rejection α,

2

we can only reject H0, which imply P (cid:54)= Q, if ˆM (X, Y ) > cα for some chose threshold cα > 0.
Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to [11] for
more details.
Intuitively, if kernel k cannot result in high MMD distance Mk(P, Q) when P (cid:54)= Q, ˆMk(P, Q) has
more chance to be smaller than cα. Then we are unlikely to reject the null hypothesis H0 with ﬁnite
samples, which implies Q is not distinguishable from P. Therefore, instead of training gθ via (1) with
a pre-speciﬁed kernel k as GMMN, we consider training gθ via

min
θ

max
k∈K

Mk(PX , Pθ),

(2)

(3)

which takes different possible characteristic kernels k ∈ K into account. On the other hand, we
could also view (2) as replacing the ﬁxed kernel k in (1) with the adversarially learned kernel
arg maxk∈K Mk(PX , Pθ) to have stronger signal where P (cid:54)= Pθ to train gθ. We refer interested
readers to [16] for more rigorous discussions about testing power and increasing MMD distances.

However, it is difﬁcult to optimize over all characteristic kernels when we solve (2). By [11, 17] if f
is a injective function and k is characteristic, then the resulted kernel ˜k = k ◦ f , where ˜k(x, x(cid:48)) =
k(f (x), f (x(cid:48))) is still characteristic. If we have a family of injective functions parameterized by φ,
which is denoted as fφ, we are able to change the objective to be

min
θ

max
φ

Mk◦fφ(PX , Pθ),

In this paper, we consider the case that combining Gaussian kernels with injective functions fφ, where
˜k(x, x(cid:48)) = exp(−(cid:107)fφ(x) − fφ(x)(cid:48)(cid:107)2). One example function class of f is {fφ|fφ(x) = φx, φ > 0},
which is equivalent to the kernel bandwidth tuning. A more complicated realization will be discussed
in Section 3. Next, we abuse the notation Mfφ(P, Q) to be MMD distance given the composition
kernel of Gaussian kernel and fφ in the following. Note that [18] considers the linear combination
of characteristic kernels, which can also be incorporated into the discussed composition kernels. A
more general kernel is studied in [19].

2.2 Properties of MMD with Kernel Learning

[8] discuss different distances between distributions adopted by existing deep learning algorithms, and
show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7],
except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training.
From (3), we train gθ via minimizing maxφ Mfφ(PX , Pθ). Next, we show maxφ Mfφ(PX , Pθ) also
enjoys the advantage of being a continuous and differentiable objective in θ under mild assumptions.
Assumption 2. g : Z × Rm → X is locally Lipschitz, where Z ⊆ Rd. We will denote gθ(z) the
evaluation on (z, θ) for convenience. Given fφ and a probability distribution Pz over Z, g satisﬁes
Assumption 2 if there are local Lipschitz constants L(θ, z) for fφ ◦ g, which is independent of φ, such
that Ez∼Pz [L(θ, z)] < +∞.
Theorem 3. The generator function gθ parameterized by θ is under Assumption 2. Let PX be a ﬁxed
distribution over X and Z be a random variable over the space Z. We denote Pθ the distribution
of gθ(Z), then maxφ Mfφ(PX , Pθ) is continuous everywhere and differentiable almost everywhere
in θ.

If gθ is parameterized by a feed-forward neural network, it satisﬁes Assumption 2 and can be trained
via gradient descent as well as propagation, since the objective is continuous and differentiable
followed by Theorem 3. More technical discussions are shown in Appendix B.
Theorem 4. (weak∗ topology) Let {Pn} be a sequence of distributions. Considering n → ∞,
D−→ PX , where D−→ means converging in
under mild Assumption, maxφ Mfφ(PX , Pn) → 0 ⇐⇒ Pn
distribution [3].

Theorem 4 shows that maxφ Mfφ(PX , Pn) is a sensible cost function to the distance between PX
and Pn. The distance is decreasing when Pn is getting closer to PX , which beneﬁts the supervision
of the improvement during the training. All proofs are omitted to Appendix A. In the next section,
we introduce a practical realization of training gθ via optimizing minθ maxφ Mfφ(PX , Pθ).

3

3 MMD GAN

To approximate (3), we use neural networks to parameterized gθ and fφ with expressive power.
For gθ, the assumption is locally Lipschitz, where commonly used feed-forward neural networks
satisfy this constraint. Also, the gradient (cid:53)θ (maxφ fφ ◦ gθ) has to be bounded, which can be
done by clipping φ [8] or gradient penalty [20]. The non-trivial part is fφ has to be injective.
For an injective function f , there exists an function f −1 such that f −1(f (x)) = x, ∀x ∈ X and
f −1(f (g(z))) = g(z), ∀z ∈ Z 1, which can be approximated by an autoencoder. In the following,
we denote φ = {φe, φd} to be the parameter of discriminator networks, which consists of an encoder
fφe , and train the corresponding decoder fφd ≈ f −1 to regularize f . The objective (3) is relaxed to
be

min
θ

max
φ

Mfφe

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe(y))(cid:107)2.

(4)

Note that we ignore the autoencoder objective when we train θ, but we use (4) for a concise
presentation. We note that the empirical study suggests autoencoder objective is not necessary to
lead the successful GAN training as we will show in Section 5, even though the injective property is
required in Theorem 1.

The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks gθ and fφ
in a minmax formulation, while the meaning of the objective is different. In [5], fφe is a discriminator
(binary) classiﬁer to distinguish two distributions. In the proposed algorithm, distinguishing two
distribution is still done by two-sample test via MMD, but with an adversarially learned kernel
parametrized by fφe . gθ is then trained to pass the hypothesis test. More connection and difference
with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed
algorithm MMD GAN. We present an implementation with the weight clipping in Algorithm 1, but
one can easily extend to other Lipschitz approximations, such as gradient penalty [20].

Algorithm 1: MMD GAN, our proposed algorithm.
input

:α the learning rate, c the clipping parameter, B the batch size, nc the number of iterations of
discriminator per generator update.

initialize generator parameter θ and discriminator parameter φ;
while θ has not converged do
for t = 1, . . . , nc do

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe (y))(cid:107)2

Sample a minibatches {xi}B
gφ ← ∇φMfφe
φ ← φ + α · RMSProp(φ, gφ)
φ ← clip(φ, −c, c)
Sample a minibatches {xi}B
gθ ← ∇θMfφe
θ ← θ − α · RMSProp(θ, gθ)

(P(X ), P(gθ(Z)))

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

Encoding Perspective of MMD GAN: Besides from using kernel selection to explain MMD GAN,
the other way to see the proposed MMD GAN is viewing fφe as a feature transformation function,
and the kernel two-sample test is performed on this transformed feature space (i.e., the code space of
the autoencoder). The optimization is ﬁnding a manifold with stronger signals for MMD two-sample
test. From this perspective, [9] is the special case of MMD GAN if fφe is the identity mapping
function. In such circumstance, the kernel two-sample test is conducted in the original data space.

3.1 Feasible Set Reduction

Theorem 5. For any fφ, there exists f (cid:48)
Ez[fφ(cid:48)(gθ(z))].

φ such that Mfφ(Pr, Pθ) = Mf (cid:48)

φ

(Pr, Pθ) and Ex[fφ(x)] (cid:23)

With Theorem 5, we could reduce the feasible set of φ during the optimization by solving

minθ maxφ Mfφ(Pr, Pθ)

s.t. E[fφ(x)] (cid:23) E[fφ(gθ(z))]

1Note that injective is not necessary invertible.

4

which the optimal solution is still equivalent to solving (2).

However, it is hard to solve the constrained optimization problem with backpropagation. We relax
the constraint by ordinal regression [21] to be

min
θ

max
φ

Mfφ(Pr, Pθ) + λ min (cid:0)E[fφ(x)] − E[fφ(gθ(z))], 0(cid:1),

which only penalizes the objective when the constraint is violated. In practice, we observe that
reducing the feasible set makes the training faster and stabler.

4 Related Works

There has been a recent surge on improving GAN [5]. We review some related works here.

If we composite fφ with linear kernel instead of Gaussian kernel, and

Connection with WGAN:
restricting the output dimension h to be 1, we then have the objective
(cid:107)E[fφ(x)] − E[fφ(gθ(z))](cid:107)2.

(5)

min
θ

max
φ
Parameterizing fφ and gθ with neural networks and assuming ∃φ(cid:48) ∈ Φ such f (cid:48)
φ = −fφ, ∀Φ, recovers
Wasserstein GAN (WGAN) [8] 2. If we treat fφ(x) as the data transform function, WGAN can
be interpreted as ﬁrst-order moment matching (linear kernel) while MMD GAN aims to match
inﬁnite order of moments with Gaussian kernel form Taylor expansion [9]. Theoretically, Wasserstein
distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, [22] show neural
networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we
demonstrate matching high-order moments beneﬁts the results. [23] also propose McGAN that
matches second order moment from the primal-dual norm perspective. However, the proposed
algorithm requires matrix (tensor) decompositions because of exact moment matching [24], which
is hard to scale to higher order moment matching. On the other hand, by giving up exact moment
matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions
are in Appendix B.3.

Difference from Other Works with Autoencoders: Energy-based GANs [7, 25] also utilizes
the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes
the reconstruction error of real samples x while maximize the reconstruction error of generated
samples gθ(z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing
the reconstruction errors of both real samples x and generated samples gθ(z). Also, [8] show EBGAN
approximates total variation, with the drawback of discontinuity, while MMD GAN optimizes MMD
distance. The other line of works [2, 26, 9] aims to match the AE codespace f (x), and utilize the
decoder fdec(·). [2, 26] match the distribution of f (x) and z via different distribution distances and
generate data (e.g. image) by fdec(z). [9] use MMD to match f (x) and g(z), and generate data via
fdec(g(z)). The proposed MMD GAN matches the f (x) and f (g(z)), and generates data via g(z)
directly as GAN. [27] is similar to MMD GAN but it considers KL-divergence without showing
continuity and weak∗ topology guarantee as we prove in Section 2.

Other GAN Works:
In addition to the discussed works, there are several extended works of GAN.
[28] proposes using the linear kernel to match ﬁrst moment of its discriminator’s latent features. [14]
considers the variance of empirical MMD score during the training. Also, [14] only improves the
latent feature matching in [28] by using kernel MMD, instead of proposing an adversarial training
framework as we studied in Section 2. [29] uses Wasserstein distance to match the distribution of
autoencoder loss instead of data. One can consider to extend [29] to higher order matching based on
the proposed MMD GAN. A parallel work [30] use energy distance, which can be treated as MMD
GAN with different kernel. However, there are some potential problems of its critic. More discussion
can be referred to [31].

5 Experiment

We train MMD GAN for image generation on the MNIST [32], CIFAR-10 [33], CelebA [13],
and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M

2Theoretically, they are not equivalent but the practical neural network approximation results in the same

algorithm.

5

respectively. All the samples images are generated from a ﬁxed noise random vectors and are not
cherry-picked.

Network architecture: In our experiments, we follow the architecture of DCGAN [34] to design gθ
by its generator and fφ by its discriminator except for expanding the output layer of fφ to be h
dimensions.

Kernel designs: The loss function of MMD GAN is implicitly associated with a family of character-
istic kernels. Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF
kernels k(x, x(cid:48)) = (cid:80)K
q=1 kσq (x, x(cid:48)) where kσq is a Gaussian kernel with bandwidth parameter σq.
Tuning kernel bandwidth σq optimally still remains an open problem. In this works, we ﬁxed K = 5
and σq to be {1, 2, 4, 8, 16} and left the fφ to learn the kernel (feature representation) under these σq.

Hyper-parameters: We use RMSProp [35] with learning rate of 0.00005 for a fair comparison with
WGAN as suggested in its original paper [8]. We ensure the boundedness of model parameters
of discriminator by clipping the weights point-wisely to the range [−0.01, 0.01] as required by
Assumption 2. The dimensionality h of the latent space is manually set according to the complexity
of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and
LSUN bedrooms. The batch size is set to be B = 64 for all datasets.

5.1 Qualitative Analysis

(a) GMMN-D MNIST

(b) GMMN-C MNIST

(c) MMD GAN MNIST

(d) GMMN-D CIFAR-10

(e) GMMN-C CIFAR-10

(f) MMD GAN CIFAR-10

Figure 1: Generated samples from GMMN-D (Dataspace), GMMN-C (Codespace) and our MMD
GAN with batch size B = 64.

We start with comparing MMD GAN with GMMN on two standard benchmarks, MNIST and CIFAR-
10. We consider two variants for GMMN. The ﬁrst one is original GMMN, which trains the generator
by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare
with MMD GAN, we also pretrain an autoencoder for projecting data to a manifold, then ﬁx the
autoencoder as a feature transformation, and train the generator by minimizing the MMD distance in
the code space. We call it as GMMN-C.

The results are pictured in Figure 1. Both GMMN-D and GMMN-C are able to generate meaningful
digits on MNIST because of the simple data structure. By a closer look, nonetheless, the boundary
and shape of the digits in Figure 1a and 1b are often irregular and non-smooth. In contrast, the sample

6

(a) WGAN MNIST

(b) WGAN CelebA

(c) WGAN LSUN

(d) MMD GAN MNIST

(e) MMD GAN CelebA

(f) MMD GAN LSUN

Figure 2: Generated samples from WGAN and MMD GAN on MNIST, CelebA, and LSUN bedroom
datasets.

digits in Figure 1c are more natural with smooth outline and sharper strike. For CIFAR-10 dataset,
both GMMN variants fail to generate meaningful images, but resulting some low level visual features.
We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms,
thus results are omitted. On the other hand, the proposed MMD GAN successfully outputs natural
images with sharp boundary and high diversity. The results in Figure 1 conﬁrm the success of the
proposed adversarial learned kernels to enrich statistical testing power, which is the key difference
between GMMN and MMD GAN.

If we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still
not competitive to MMD GAN with B = 64. The images are put in Appendix C. This demonstrates
that the proposed MMD GAN can be trained more efﬁciently than GMMN with smaller batch size.

Comparisons with GANs: There are several representative extensions of GANs. We consider
recent state-of-art WGAN [8] based on DCGAN structure [34], because of the connection with MMD
GAN discussed in Section 4. The results are shown in Figure 2. For MNIST, the digits generated
from WGAN in Figure 2a are more unnatural with peculiar strikes. In Contrary, the digits from
MMD GAN in Figure 2d enjoy smoother contour. Furthermore, both WGAN and MMD GAN
generate diversiﬁed digits, avoiding the mode collapse problems appeared in the literature of training
GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD GAN.
Speciﬁcally, we observe varied poses, expressions, genders, skin colors and light exposure in Figure
2b and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN
have higher chances to be blurry and twisted while faces from MMD GAN are more spontaneous with
sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient differences
between the samples generated from MMD GAN and WGAN.

5.2 Quantitative Analysis

To quantitatively measure the quality and diversity of generated samples, we compute the inception
score [28] on CIFAR-10 images. The inception score is used for GANs to measure samples quality
and diversity on the pretrained inception model [28]. Models that generate collapsed samples have
a relatively low score. Table 1 lists the results for 50K samples generated by various unsupervised

7

generative models trained on CIFAR-10 dataset. The inception scores of [36, 37, 28] are directly
derived from the corresponding references.

Although both WGAN and MMD GAN can generate sharp images as we show in Section 5.1, our
score is better than other GAN techniques except for DFM [36]. This seems to conﬁrm empirically
that higher order of moment matching between the real data and fake sample distribution beneﬁts
generating more diversiﬁed sample images. Also note DFM appears compatible with our method and
combing training techniques in DFM is a possible avenue for future work.

Method

Real data

Scores ± std.

11.95 ± .20

DFM [36]
ALI [37]
Improved GANs [28]

7.72
5.34
4.36

MMD GAN
WGAN
GMMN-C
GMMN-D
Table 1: Inception scores

6.17 ± .07
5.88 ± .07
3.94 ± .04
3.47 ± .03

5.3 Stability of MMD GAN

Figure 3: Computation time

We further illustrate how the MMD distance correlates well with the quality of the generated samples.
Figure 4 plots the evolution of the MMD GAN estimate the MMD distance during training for
MNIST, CelebA and LSUN datasets. We report the average of the ˆMfφ(PX , Pθ) with moving
average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We
observe during the whole training process, samples generated from the same noise vector across
iterations, remain similar in nature. (e.g., face identity and bedroom style are alike while details and
backgrounds will evolve.) This qualitative observation indicates valuable stability of the training
process. The decreasing curve with the improving quality of images supports the weak∗ topology
shown in Theorem 4. Also, We can see from the plot that the model converges very quickly. In Figure
4b, for example, it converges shortly after tens of thousands of generator iterations on CelebA dataset.

(a) MNIST

(b) CelebA

(c) LSUN Bedrooms

Figure 4: Training curves and generative samples at different stages of training. We can see a clear
correlation between lower distance and better sample quality.

5.4 Computation Issue

We conduct time complexity analysis with respect to the batch size B. The time complexity of each
iteration is O(B) for WGAN and O(KB2) for our proposed MMD GAN with a mixture of K RBF
kernels. The quadratic complexity O(B2) of MMD GAN is introduced by computing kernel matrix,
which is sometimes criticized for being inapplicable with large batch size in practice. However,
we point that there are several recent works, such as EBGAN [7], also matching pairwise relation
between samples of batch size, leading to O(B2) complexity as well.

8

Empirically, we ﬁnd that under GPU environment, the highly parallelized matrix operation tremen-
dously alleviated the quadratic time to almost linear time with modest B. Figure 3 compares the
computational time per generator iterations versus different B on Titan X. When B = 64, which
is adapted for training MMD GAN in our experiments setting, the time per iteration of WGAN
and MMD GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for
training GMMN in its references [9], the time per iteration becomes 4.431 and 8.565 seconds, respec-
tively. This result coheres our argument that the empirical computational time for MMD GAN is not
quadratically expensive compared to WGAN with powerful GPU parallel computation.

5.5 Better Lipschitz Approximation and Necessity of Auto-Encoder

We used weight-clipping for Lipschitz constraint in Assumption 2. Another approach for obtaining a
discriminator with the similar constraints that approximates a Wasserstein distance is [20], where the
gradient of the discriminator is constrained to be 1 between the generated and data points. Inspired
by [20], an alternative approach is to apply the gradient constraint as a regularizer to the witness
function for a different IPM, such as the MMD. This idea was ﬁrst proposed in [30] for the Energy
Distance (in parallel to our submission), which was shown in [31] to correspond to a gradient penalty
on the witness function for any RKHS-based MMD. Here we undertake a preliminary investigation
of this approach, where we also drop the requirement in Algorithm 1 for fφ to be injective, which we
observe that it is not necessary in practice. We show some preliminary results of training MMD GAN
with gradient penalty and without the auto-encoder in Figure 5. The preliminary study indicates that
MMD GAN can generate satisfactory results with other Lipschitz constraint approximation. One
potential future work is conducting more thorough empirical comparison studies between different
approximations.

(a) Cifar10, Giter = 300K

(b) CelebA, Giter = 300K

Figure 5: MMD GAN results using gradient penalty [20] and without auto-encoder reconstruction
loss during training.

6 Discussion

We introduce a new deep generative model trained via MMD with adversarially learned kernels. We
further study its theoretical properties and propose a practical realization MMD GAN, which can be
trained with much smaller batch size than GMMN and has competitive performances with state-of-the-
art GANs. We can view MMD GAN as the ﬁrst practical step forward connecting moment matching
network and GAN. One important direction is applying developed tools in moment matching [15] on
general GAN works based the connections shown by MMD GAN. Also, in Section 4, we connect
WGAN and MMD GAN by ﬁrst-order and inﬁnite-order moment matching. [24] shows ﬁnite-order
moment matching (∼ 5) achieves the best performance on domain adaption. One could extend MMD
GAN to this by using polynomial kernels. Last, in theory, an injective mapping fφ is necessary for
the theoretical guarantees. However, we observe that it is not mandatory in practice as we show
in Section 5.5. One conjecture is it usually learns the injective mapping with high probability by
parameterizing with neural networks, which worth more study as a future work.

9

We thank the reviewers for their helpful comments. We also thank Dougal Sutherland and Arthur
Gretton for their valuable feedbacks and pointing out an error in the previous draft. This work is
supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-
1563887.

Acknowledgments

References

[1] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In AISTATS, 2009.

[2] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.

[3] Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &

Business Media, 2013.

[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In ICML, 2015.

[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[6] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural

samplers using variational divergence minimization. In NIPS, 2016.

[7] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based Generative Adversarial Network. In ICLR,

[8] Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. In ICML, 2017.

[9] Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,

2017.

2015.

[10] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative

neural networks via maximum mean discrepancy optimization. In UAI, 2015.

[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander

Smola. A kernel two-sample test. JMLR, 2012.

[12] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

[13] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In CVPR, 2015.

[14] Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas,
Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized
maximum mean discrepancy. In ICLR, 2017.

[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel
mean embedding of distributions: A review and beyonds. arXiv preprint arXiv:1605.09522,
2016.

[16] Kenji Fukumizu, Arthur Gretton, Gert R Lanckriet, Bernhard Schölkopf, and Bharath K Sripe-
rumbudur. Kernel choice and classiﬁability for rkhs embeddings of probability distributions. In
NIPS, 2009.

[17] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and

K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.

[18] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano
Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale
two-sample tests. In NIPS, 2012.

10

[19] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel

learning. In AISTATS, 2016.

[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.

Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.

[21] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal

regression. 1999.

[22] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.

[23] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature

matching gan. arxiv pre-print 1702.08398, 2017.

[24] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learn-
ing. arXiv preprint arXiv:1702.08811, 2017.

[25] Shuangfei Zhai, Yu Cheng, Rogério Schmidt Feris, and Zhongfei Zhang. Generative adversarial
networks as variational training of energy based models. CoRR, abs/1611.01799, 2016.

[26] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

[27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Adversarial generator-encoder net-

works. arXiv preprint arXiv:1704.02304, 2017.

[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.

Improved techniques for training gans. In NIPS, 2016.

[29] David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717, 2017.

[30] Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Rémi Munos. The cramer distance as a solution to biased wasserstein
gradients. arXiv preprint arXiv:1705.10743, 2017.

[31] Arthur Gretton. Notes on the cramer gan. https://medium.com/towards-data-science/

notes-on-the-cramer-gan-752abd505c00, 2017. Accessed: 2017-11-2.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 1998.

[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[34] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with

deep convolutional generative adversarial networks. In ICLR, 2016.

[35] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.

[36] D Warde-Farley and Y Bengio. Improving generative adversarial networks with denoising

feature matching. In ICLR, 2017.

[37] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mas-

tropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017.

[38] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R.G.
Lanckriet. Hilbert space embeddings and metrics on probability measures. JMLR, 2010.

11

A Technical Proof

A.1 Proof of Theorem 3

Proof. Since MMD is a probabilistic metric [11], we have the triangular inequality for every Mf .
Therefore,
| max
φ

|Mfφ(PX , Pθ) − Mfφ(PX , Pθ(cid:48))|

Mfφ(PX , Pθ) − max

Mfφ(PX , Pθ(cid:48))| ≤

max
φ

(6)

φ

=

≤

|Mfφ∗ (PX , Pθ) − Mfφ∗ (PX , Pθ(cid:48))|
Mfφ∗ (Pθ, Pθ(cid:48)),

(7)

(8)

where φ∗ is the solution of (6).
By deﬁnition, given any φ ∈ Φ, deﬁne hθ = fφ ◦ gθ, the MMD distance Mfφ(Pθ, Pθ(cid:48))
= Ez,z(cid:48) [k(hθ(z), hθ(z(cid:48))) − 2k(hθ(z), hθ(cid:48)(z(cid:48))) + k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48)))]
≤ Ez,z(cid:48) [|k(hθ(z), hθ(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|] + Ez,z(cid:48) [|k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|]

In this, we consider Gaussian kernel k, therefore
|k(hθ(z), hθ(z(cid:48)))−k(hθ(z), hθ(cid:48)(z(cid:48)))| = | exp(−(cid:107)hθ(z)−hθ(z(cid:48))(cid:107)2)−exp(−(cid:107)hθ(z)−hθ(cid:48)(z(cid:48))(cid:107)2)| ≤ 1,
for all (θ, θ(cid:48)) and (z, z(cid:48)). Similarly, |k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))| ≤ 1. Combining the
above claim with (7) and bounded convergence theorem, we have

Mfφ(PX , Pθ) − max

| max
φ
which proves the continuity of maxf M M Df (PX , Pθ).
By Mean Value Theorem, |e−x2
with (8) and triangular inequality, we have

− e−y2

| ≤ maxz |2ze−z2

φ

Mfφ(PX , Pθ(cid:48))| θ→θ(cid:48)

−−−→ 0,

Mfφ(Pθ, Pθ(cid:48)) ≤ Ez,z(cid:48)[(cid:107)hθ(z(cid:48)) − hθ(cid:48)(z(cid:48))(cid:107) + (cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

= 2Ez[(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

| × |x − y| ≤ |x − y|. Incorporating it

Now let h be locally Lipschitz. For a given pair (θ, z) there is a constant L(θ, z) and an open set Uθ
such that for every (θ(cid:48), z) ∈ Uθ we have

(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107) ≤ L(θ, z)((cid:107)θ − θ(cid:48)(cid:107))

Under Assumption 2, we then achieve

|Mfφ(PX , Pθ) − Mfφ(PX , P(cid:48)

(9)
Combining (7) and (9) implies maxφ Mfφ(PX , Pθ) is locally Lipschitz and continuous everywhere.
Last, applying Radamacher’s theorem proves maxφ Mfφ(PX , Pθ) is differentiable almost everywhere,
which completes the proof.

θ)| ≤ Mfφ(Pθ, Pθ(cid:48)) ≤ 2Ez[L(θ, z)](cid:107)θ − θ(cid:48)(cid:107).

A.2 Proof of Theorem 4

Proof. The proof utilizes parts of results from [38].

If maxφ Mfφ(Pn, P) → 0, there exists φ ∈ Φ such that Mfφ(P, Pn) → 0 since Mfφ(P, Pn)

(⇒)
is non-negative. By [38], for any characteristic kernel k,

Therefore, if maxφ Mfφ(Pn, P) → 0, Pn

Mk(Pn, P) → 0 ⇐⇒ Pn
D−→ P.

D−→ P.

(⇐) By [38], given a characteristic kernel k, if supx,x(cid:48) k(x, x(cid:48)) ≤ 1, (cid:112)Mk(P, Q) ≤ W (P, Q),
where W (P, Q) is Wasserstein distance.
In this paper, we consider the kernel k(x, x(cid:48)) =
exp(−(cid:107)fφ(x) − fφ(x(cid:48))(cid:107)2) ≤ 1. By above, (cid:112)Mfφ(P, Q) ≤ W (P, Q), ∀φ. By [8], if Pn
D−→ P,
W (P, Pn) → 0. Combining all of them, we get

Pn

D−→ P =⇒ W (P, Pn) → 0 =⇒ max

Mfφ(P, Pn) → 0.

φ

12

A.3 Proof of Theorem 5

Proof. The proof assumes fφ(x) is scalar, but the vector case can be proved with the same sketch.
First, if E[fφ(x)] > E[fφ(gθ(z))], then φ = φ(cid:48). If E[fφ(x)] < E[fφ(gθ(z))], we let f = −fφ, then
E[f (x)] > E[f (gθ(z))] and ﬂipping sign does not change the MMD distance. If we parameterized
fφ by a neural network, which has a linear output layer, φ(cid:48) can realized by ﬂipping the sign of the
weights of the last layer.

B Property of MMD with Fixed and Learned Kernels

B.1 Continuity and Differentiability

One can simplify Theorem 3 and its proof for standard MMD distance to show MMD is also
continuous and differentiable almost everywhere. In [8], they propose a counterexample to show the
discontinuity of MMD by assuming H = L2. However, it is known that L2 is not in RKHS, so the
discussed counterexample is not appropriate.

B.2 IPM Framework

From integral probability metrics (IPM), the probabilistic distance can be deﬁned as

∆(P, Q) = sup
f ∈F

Ex∼P[f (x)] − Ey∼Q[f (y)].

(10)

By changing the function class F, we can recover several distances, such as total variation, Wasser-
stein distance and MMD distance. From [8], the discriminator fφ in different existing works of GAN
can be explained to be used to solve different probabilistic metrics based on (10). For MMD, the
function class F is {(cid:107)f (cid:107)Hk ≤ 1}, where H is RKHS associated with kernel k. Different form many
distances, such as total variation and Wasserstein distance, there is an analytical representation [11]
as we show in Section 2, which is

∆(P, Q) = M M D(P, Q) = (cid:107)µP − µQ(cid:107)H =

EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

(cid:113)

Because of the analytical representation of (10), GMMN does not need an additional network fφ for
estimating the distance.

Here we also provide an explanation of the proposed MMD with adversarially learned kernel under
IPM framework. The MMD distance with adversarially learned kernel is represented as

M M D(P, Q),

max
k∈K

The corresponding IPM formulation is
∆(P, Q) = max
k∈K

M M D(P, Q) =

sup
f ∈Hk1 ∪···∪Hkn

Ex∼P[f (x)] − Ey∼Q[f (y)],

where ki ∈ K, ∀i. From this perspective, the proposed MMD distance with adversarially learned
kernel is still deﬁned by IPM but with a larger function class.

B.3 MMD is an Efﬁcient Moment Matching

We consider the example of matching ﬁrst and second moments of P and Q. The (cid:96)1 objective in
McGAN [23] is

(cid:107)µP − µQ(cid:107)1 + (cid:107)ΣP − ΣQ(cid:107)∗,

where (cid:107) · (cid:107)∗ is the trace norm. The objective can be changed to be general (cid:96)q norm.

In MMD, with polynomial kernel k(x, y) = 1 + x(cid:62)y, the MMD distance is

2(cid:107)µP − µQ(cid:107)2 + (cid:107)ΣP − ΣQ + µPµ(cid:62)

P − µQµ(cid:62)

Q (cid:107)2
F ,

which is inexact moment matching because the second term contains the quadratic of the ﬁrst moment.
It is difﬁcult to match high-order moments, because we have to deal with high order tensors directly.
On the other hand, MMD can easily match high-order moments (even inﬁnite order moments by
using Gaussian kernel) with kernel tricks, and enjoys strong theoretical guarantee.

13

C Performance of GMMN

(a) GMMN-D on MNIST

(b) GMMN-C on MNIST

(c) GMMN-D on CIFAR-10

(d) GMMN-C CIFAR-10

Figure 6: Generative samples from GMMN-D and GMMN-C with training batch size B = 1024.

14

7
1
0
2
 
v
o
N
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
8
5
8
0
.
5
0
7
1
:
v
i
X
r
a

MMD GAN: Towards Deeper Understanding of
Moment Matching Network

Chun-Liang Li1,∗ Wei-Cheng Chang1,∗ Yu Cheng2 Yiming Yang1 Barnabás Póczos1

1 Carnegie Mellon University,

2AI Foundations, IBM Research

{chunlial,wchang2,yiming,bapoczos}@cs.cmu.edu
(∗ denotes equal contribution)

chengyu@us.ibm.com

Abstract

Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the discriminator
in GAN with a two-sample test based on kernel maximum mean discrepancy
(MMD). Although some theoretical guarantees of MMD have been studied, the
empirical performance of GMMN is still not as competitive as that of GAN on
challenging and large benchmark datasets. The computational efﬁciency of GMMN
is also less desirable in comparison with GAN, partially due to its requirement for
a rather large batch size during the training. In this paper, we propose to improve
both the model expressiveness of GMMN and its computational efﬁciency by
introducing adversarial kernel learning techniques, as the replacement of a ﬁxed
Gaussian kernel in the original GMMN. The new approach combines the key ideas
in both GMMN and GAN, hence we name it MMD GAN. The new distance measure
in MMD GAN is a meaningful loss that enjoys the advantage of weak∗ topology
and can be optimized via gradient descent with relatively small batch sizes. In our
evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA
and LSUN, the performance of MMD GAN signiﬁcantly outperforms GMMN, and
is competitive with other representative GAN works.

1

Introduction

The essence of unsupervised learning models the underlying distribution PX of the data X . Deep
generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with
promising results. However, modeling arbitrary density is a statistically challenging task [3]. In many
applications, such as caption generation [4], accurate density estimation is not even necessary since
we are only interested in sampling from the approximated distribution.
Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a
base distribution PZ over Z, such as Gaussian distribution, then trains a transformation network gθ
such that Pθ ≈ PX , where Pθ is the underlying distribution of gθ(z) and z ∼ PZ . During the training,
GAN-based algorithms require an auxiliary network fφ to estimate the distance between PX and Pθ.
Different probabilistic (pseudo) metrics have been studied [5–8] under GAN framework.
Instead of training an auxiliary network fφ for measuring the distance between PX and Pθ, Generative
moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11],
which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.
During the training, gθ is trained to pass the hypothesis test (minimize MMD distance). [11] shows
even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the
empirical performance of GMMN does not meet its theoretical properties. There is no promising
empirical results comparable with GAN on challenging benchmarks [12, 13]. Computationally,

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

it also requires larger batch size than GAN needs for training, which is considered to be less
efﬁcient [9, 10, 14, 8]

In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels
instead of ﬁxed Gaussian kernels to have better hypothesis testing power. The main contributions of
this work are:

• In Section 2, we prove that training gθ via MMD with learned kernels is continuous and differen-
tiable, which guarantees the model can be trained by gradient descent. Second, we prove a new
distance measure via kernel learning, which is a sensitive loss function to the distance between
PX and Pθ (weak∗ topology). Empirically, the loss decreases when two distributions get closer.
• In Section 3, we propose a practical realization called MMD GAN that learns generator gθ with
the adversarially trained kernel. We further propose a feasible set reduction to speed up and
stabilize the training of MMD GAN.

• In Section 5, we show that MMD GAN is computationally more efﬁcient than GMMN, which can
be trained with much smaller batch size. We also demonstrate that MMD GAN has promising
results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To
our best knowledge, we are the ﬁrst MMD based work to achieve comparable results with other
GAN works on these datasets.

Finally, we also study the connection to existing works in Section 4. Interestingly, we show Wasser-
stein GAN [8] is the special case of the proposed MMD GAN under certain conditions. The uniﬁed
view shows more connections between moment matching and GAN, which can potentially inspire
new algorithms based on well-developed tools in statistics [15]. Our experiment code is available at
https://github.com/OctoberChang/MMD-GAN.

2 GAN, Two-Sample Test and GMMN

i=1, where xi ∈ X and xi ∼ PX . If we are interested in sampling
Assume we are given data {xi}n
from PX , it is not necessary to estimate the density of PX . Instead, Generative Adversarial Network
(GAN) [5] trains a generator gθ parameterized by θ to transform samples z ∼ PZ , where z ∈ Z,
into gθ(z) ∼ Pθ such that Pθ ≈ PX . To measure the similarity between PX and Pθ via their
samples {x}n
j=1 during the training, [5] trains the discriminator fφ parameterized
by φ for help. The learning is done by playing a two-player game, where fφ tries to distinguish xi
and gθ(zj) while gθ aims to confuse fφ by generating gθ(zj) similar to xi.

i=1 and {gθ(zj)}n

On the other hand, distinguishing two distributions by ﬁnite samples is known as Two-Sample Test in
statistics. One way to conduct two-sample test is via kernel maximum mean discrepancy (MMD) [11].
Given two distributions P and Q, and a kernel k, the square of MMD distance is deﬁned as

Mk(P, Q) = (cid:107)µP − µQ(cid:107)2

H = EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

Theorem 1. [11] Given a kernel k, if k is a characteristic kernel, then Mk(P, Q) = 0 iff P = Q.

GMMN: One example of characteristic kernel is Gaussian kernel k(x, x(cid:48)) = exp((cid:107)x − x(cid:48)(cid:107)2). Based
on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains gθ by

Mk(PX , Pθ),

min
θ

(1)

with a ﬁxed Gaussian kernel k rather than training an additional discriminator f as GAN.

2.1 MMD with Kernel Learning

In practice we use ﬁnite samples from distributions to estimate MMD distance. Given X =
{x1, · · · , xn} ∼ P and Y = {y1, · · · , yn} ∼ Q, one estimator of Mk(P, Q) is

ˆMk(X, Y ) =

k(xi, x(cid:48)

i) −

k(xi, yj) +

k(yj, y(cid:48)

j).

1
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=i(cid:48)

2
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=j

1
(cid:1)
(cid:0)n
2

(cid:88)

j(cid:54)=j(cid:48)

Because of the sampling variance, ˆM (X, Y ) may not be zero even when P = Q. We then conduct
hypothesis test with null hypothesis H0 : P = Q. For a given allowable probability of false rejection α,

2

we can only reject H0, which imply P (cid:54)= Q, if ˆM (X, Y ) > cα for some chose threshold cα > 0.
Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to [11] for
more details.
Intuitively, if kernel k cannot result in high MMD distance Mk(P, Q) when P (cid:54)= Q, ˆMk(P, Q) has
more chance to be smaller than cα. Then we are unlikely to reject the null hypothesis H0 with ﬁnite
samples, which implies Q is not distinguishable from P. Therefore, instead of training gθ via (1) with
a pre-speciﬁed kernel k as GMMN, we consider training gθ via

min
θ

max
k∈K

Mk(PX , Pθ),

(2)

(3)

which takes different possible characteristic kernels k ∈ K into account. On the other hand, we
could also view (2) as replacing the ﬁxed kernel k in (1) with the adversarially learned kernel
arg maxk∈K Mk(PX , Pθ) to have stronger signal where P (cid:54)= Pθ to train gθ. We refer interested
readers to [16] for more rigorous discussions about testing power and increasing MMD distances.

However, it is difﬁcult to optimize over all characteristic kernels when we solve (2). By [11, 17] if f
is a injective function and k is characteristic, then the resulted kernel ˜k = k ◦ f , where ˜k(x, x(cid:48)) =
k(f (x), f (x(cid:48))) is still characteristic. If we have a family of injective functions parameterized by φ,
which is denoted as fφ, we are able to change the objective to be

min
θ

max
φ

Mk◦fφ(PX , Pθ),

In this paper, we consider the case that combining Gaussian kernels with injective functions fφ, where
˜k(x, x(cid:48)) = exp(−(cid:107)fφ(x) − fφ(x)(cid:48)(cid:107)2). One example function class of f is {fφ|fφ(x) = φx, φ > 0},
which is equivalent to the kernel bandwidth tuning. A more complicated realization will be discussed
in Section 3. Next, we abuse the notation Mfφ(P, Q) to be MMD distance given the composition
kernel of Gaussian kernel and fφ in the following. Note that [18] considers the linear combination
of characteristic kernels, which can also be incorporated into the discussed composition kernels. A
more general kernel is studied in [19].

2.2 Properties of MMD with Kernel Learning

[8] discuss different distances between distributions adopted by existing deep learning algorithms, and
show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7],
except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training.
From (3), we train gθ via minimizing maxφ Mfφ(PX , Pθ). Next, we show maxφ Mfφ(PX , Pθ) also
enjoys the advantage of being a continuous and differentiable objective in θ under mild assumptions.
Assumption 2. g : Z × Rm → X is locally Lipschitz, where Z ⊆ Rd. We will denote gθ(z) the
evaluation on (z, θ) for convenience. Given fφ and a probability distribution Pz over Z, g satisﬁes
Assumption 2 if there are local Lipschitz constants L(θ, z) for fφ ◦ g, which is independent of φ, such
that Ez∼Pz [L(θ, z)] < +∞.
Theorem 3. The generator function gθ parameterized by θ is under Assumption 2. Let PX be a ﬁxed
distribution over X and Z be a random variable over the space Z. We denote Pθ the distribution
of gθ(Z), then maxφ Mfφ(PX , Pθ) is continuous everywhere and differentiable almost everywhere
in θ.

If gθ is parameterized by a feed-forward neural network, it satisﬁes Assumption 2 and can be trained
via gradient descent as well as propagation, since the objective is continuous and differentiable
followed by Theorem 3. More technical discussions are shown in Appendix B.
Theorem 4. (weak∗ topology) Let {Pn} be a sequence of distributions. Considering n → ∞,
D−→ PX , where D−→ means converging in
under mild Assumption, maxφ Mfφ(PX , Pn) → 0 ⇐⇒ Pn
distribution [3].

Theorem 4 shows that maxφ Mfφ(PX , Pn) is a sensible cost function to the distance between PX
and Pn. The distance is decreasing when Pn is getting closer to PX , which beneﬁts the supervision
of the improvement during the training. All proofs are omitted to Appendix A. In the next section,
we introduce a practical realization of training gθ via optimizing minθ maxφ Mfφ(PX , Pθ).

3

3 MMD GAN

To approximate (3), we use neural networks to parameterized gθ and fφ with expressive power.
For gθ, the assumption is locally Lipschitz, where commonly used feed-forward neural networks
satisfy this constraint. Also, the gradient (cid:53)θ (maxφ fφ ◦ gθ) has to be bounded, which can be
done by clipping φ [8] or gradient penalty [20]. The non-trivial part is fφ has to be injective.
For an injective function f , there exists an function f −1 such that f −1(f (x)) = x, ∀x ∈ X and
f −1(f (g(z))) = g(z), ∀z ∈ Z 1, which can be approximated by an autoencoder. In the following,
we denote φ = {φe, φd} to be the parameter of discriminator networks, which consists of an encoder
fφe , and train the corresponding decoder fφd ≈ f −1 to regularize f . The objective (3) is relaxed to
be

min
θ

max
φ

Mfφe

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe(y))(cid:107)2.

(4)

Note that we ignore the autoencoder objective when we train θ, but we use (4) for a concise
presentation. We note that the empirical study suggests autoencoder objective is not necessary to
lead the successful GAN training as we will show in Section 5, even though the injective property is
required in Theorem 1.

The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks gθ and fφ
in a minmax formulation, while the meaning of the objective is different. In [5], fφe is a discriminator
(binary) classiﬁer to distinguish two distributions. In the proposed algorithm, distinguishing two
distribution is still done by two-sample test via MMD, but with an adversarially learned kernel
parametrized by fφe . gθ is then trained to pass the hypothesis test. More connection and difference
with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed
algorithm MMD GAN. We present an implementation with the weight clipping in Algorithm 1, but
one can easily extend to other Lipschitz approximations, such as gradient penalty [20].

Algorithm 1: MMD GAN, our proposed algorithm.
input

:α the learning rate, c the clipping parameter, B the batch size, nc the number of iterations of
discriminator per generator update.

initialize generator parameter θ and discriminator parameter φ;
while θ has not converged do
for t = 1, . . . , nc do

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe (y))(cid:107)2

Sample a minibatches {xi}B
gφ ← ∇φMfφe
φ ← φ + α · RMSProp(φ, gφ)
φ ← clip(φ, −c, c)
Sample a minibatches {xi}B
gθ ← ∇θMfφe
θ ← θ − α · RMSProp(θ, gθ)

(P(X ), P(gθ(Z)))

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

Encoding Perspective of MMD GAN: Besides from using kernel selection to explain MMD GAN,
the other way to see the proposed MMD GAN is viewing fφe as a feature transformation function,
and the kernel two-sample test is performed on this transformed feature space (i.e., the code space of
the autoencoder). The optimization is ﬁnding a manifold with stronger signals for MMD two-sample
test. From this perspective, [9] is the special case of MMD GAN if fφe is the identity mapping
function. In such circumstance, the kernel two-sample test is conducted in the original data space.

3.1 Feasible Set Reduction

Theorem 5. For any fφ, there exists f (cid:48)
Ez[fφ(cid:48)(gθ(z))].

φ such that Mfφ(Pr, Pθ) = Mf (cid:48)

φ

(Pr, Pθ) and Ex[fφ(x)] (cid:23)

With Theorem 5, we could reduce the feasible set of φ during the optimization by solving

minθ maxφ Mfφ(Pr, Pθ)

s.t. E[fφ(x)] (cid:23) E[fφ(gθ(z))]

1Note that injective is not necessary invertible.

4

which the optimal solution is still equivalent to solving (2).

However, it is hard to solve the constrained optimization problem with backpropagation. We relax
the constraint by ordinal regression [21] to be

min
θ

max
φ

Mfφ(Pr, Pθ) + λ min (cid:0)E[fφ(x)] − E[fφ(gθ(z))], 0(cid:1),

which only penalizes the objective when the constraint is violated. In practice, we observe that
reducing the feasible set makes the training faster and stabler.

4 Related Works

There has been a recent surge on improving GAN [5]. We review some related works here.

If we composite fφ with linear kernel instead of Gaussian kernel, and

Connection with WGAN:
restricting the output dimension h to be 1, we then have the objective
(cid:107)E[fφ(x)] − E[fφ(gθ(z))](cid:107)2.

(5)

min
θ

max
φ
Parameterizing fφ and gθ with neural networks and assuming ∃φ(cid:48) ∈ Φ such f (cid:48)
φ = −fφ, ∀Φ, recovers
Wasserstein GAN (WGAN) [8] 2. If we treat fφ(x) as the data transform function, WGAN can
be interpreted as ﬁrst-order moment matching (linear kernel) while MMD GAN aims to match
inﬁnite order of moments with Gaussian kernel form Taylor expansion [9]. Theoretically, Wasserstein
distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, [22] show neural
networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we
demonstrate matching high-order moments beneﬁts the results. [23] also propose McGAN that
matches second order moment from the primal-dual norm perspective. However, the proposed
algorithm requires matrix (tensor) decompositions because of exact moment matching [24], which
is hard to scale to higher order moment matching. On the other hand, by giving up exact moment
matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions
are in Appendix B.3.

Difference from Other Works with Autoencoders: Energy-based GANs [7, 25] also utilizes
the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes
the reconstruction error of real samples x while maximize the reconstruction error of generated
samples gθ(z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing
the reconstruction errors of both real samples x and generated samples gθ(z). Also, [8] show EBGAN
approximates total variation, with the drawback of discontinuity, while MMD GAN optimizes MMD
distance. The other line of works [2, 26, 9] aims to match the AE codespace f (x), and utilize the
decoder fdec(·). [2, 26] match the distribution of f (x) and z via different distribution distances and
generate data (e.g. image) by fdec(z). [9] use MMD to match f (x) and g(z), and generate data via
fdec(g(z)). The proposed MMD GAN matches the f (x) and f (g(z)), and generates data via g(z)
directly as GAN. [27] is similar to MMD GAN but it considers KL-divergence without showing
continuity and weak∗ topology guarantee as we prove in Section 2.

Other GAN Works:
In addition to the discussed works, there are several extended works of GAN.
[28] proposes using the linear kernel to match ﬁrst moment of its discriminator’s latent features. [14]
considers the variance of empirical MMD score during the training. Also, [14] only improves the
latent feature matching in [28] by using kernel MMD, instead of proposing an adversarial training
framework as we studied in Section 2. [29] uses Wasserstein distance to match the distribution of
autoencoder loss instead of data. One can consider to extend [29] to higher order matching based on
the proposed MMD GAN. A parallel work [30] use energy distance, which can be treated as MMD
GAN with different kernel. However, there are some potential problems of its critic. More discussion
can be referred to [31].

5 Experiment

We train MMD GAN for image generation on the MNIST [32], CIFAR-10 [33], CelebA [13],
and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M

2Theoretically, they are not equivalent but the practical neural network approximation results in the same

algorithm.

5

respectively. All the samples images are generated from a ﬁxed noise random vectors and are not
cherry-picked.

Network architecture: In our experiments, we follow the architecture of DCGAN [34] to design gθ
by its generator and fφ by its discriminator except for expanding the output layer of fφ to be h
dimensions.

Kernel designs: The loss function of MMD GAN is implicitly associated with a family of character-
istic kernels. Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF
kernels k(x, x(cid:48)) = (cid:80)K
q=1 kσq (x, x(cid:48)) where kσq is a Gaussian kernel with bandwidth parameter σq.
Tuning kernel bandwidth σq optimally still remains an open problem. In this works, we ﬁxed K = 5
and σq to be {1, 2, 4, 8, 16} and left the fφ to learn the kernel (feature representation) under these σq.

Hyper-parameters: We use RMSProp [35] with learning rate of 0.00005 for a fair comparison with
WGAN as suggested in its original paper [8]. We ensure the boundedness of model parameters
of discriminator by clipping the weights point-wisely to the range [−0.01, 0.01] as required by
Assumption 2. The dimensionality h of the latent space is manually set according to the complexity
of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and
LSUN bedrooms. The batch size is set to be B = 64 for all datasets.

5.1 Qualitative Analysis

(a) GMMN-D MNIST

(b) GMMN-C MNIST

(c) MMD GAN MNIST

(d) GMMN-D CIFAR-10

(e) GMMN-C CIFAR-10

(f) MMD GAN CIFAR-10

Figure 1: Generated samples from GMMN-D (Dataspace), GMMN-C (Codespace) and our MMD
GAN with batch size B = 64.

We start with comparing MMD GAN with GMMN on two standard benchmarks, MNIST and CIFAR-
10. We consider two variants for GMMN. The ﬁrst one is original GMMN, which trains the generator
by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare
with MMD GAN, we also pretrain an autoencoder for projecting data to a manifold, then ﬁx the
autoencoder as a feature transformation, and train the generator by minimizing the MMD distance in
the code space. We call it as GMMN-C.

The results are pictured in Figure 1. Both GMMN-D and GMMN-C are able to generate meaningful
digits on MNIST because of the simple data structure. By a closer look, nonetheless, the boundary
and shape of the digits in Figure 1a and 1b are often irregular and non-smooth. In contrast, the sample

6

(a) WGAN MNIST

(b) WGAN CelebA

(c) WGAN LSUN

(d) MMD GAN MNIST

(e) MMD GAN CelebA

(f) MMD GAN LSUN

Figure 2: Generated samples from WGAN and MMD GAN on MNIST, CelebA, and LSUN bedroom
datasets.

digits in Figure 1c are more natural with smooth outline and sharper strike. For CIFAR-10 dataset,
both GMMN variants fail to generate meaningful images, but resulting some low level visual features.
We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms,
thus results are omitted. On the other hand, the proposed MMD GAN successfully outputs natural
images with sharp boundary and high diversity. The results in Figure 1 conﬁrm the success of the
proposed adversarial learned kernels to enrich statistical testing power, which is the key difference
between GMMN and MMD GAN.

If we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still
not competitive to MMD GAN with B = 64. The images are put in Appendix C. This demonstrates
that the proposed MMD GAN can be trained more efﬁciently than GMMN with smaller batch size.

Comparisons with GANs: There are several representative extensions of GANs. We consider
recent state-of-art WGAN [8] based on DCGAN structure [34], because of the connection with MMD
GAN discussed in Section 4. The results are shown in Figure 2. For MNIST, the digits generated
from WGAN in Figure 2a are more unnatural with peculiar strikes. In Contrary, the digits from
MMD GAN in Figure 2d enjoy smoother contour. Furthermore, both WGAN and MMD GAN
generate diversiﬁed digits, avoiding the mode collapse problems appeared in the literature of training
GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD GAN.
Speciﬁcally, we observe varied poses, expressions, genders, skin colors and light exposure in Figure
2b and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN
have higher chances to be blurry and twisted while faces from MMD GAN are more spontaneous with
sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient differences
between the samples generated from MMD GAN and WGAN.

5.2 Quantitative Analysis

To quantitatively measure the quality and diversity of generated samples, we compute the inception
score [28] on CIFAR-10 images. The inception score is used for GANs to measure samples quality
and diversity on the pretrained inception model [28]. Models that generate collapsed samples have
a relatively low score. Table 1 lists the results for 50K samples generated by various unsupervised

7

generative models trained on CIFAR-10 dataset. The inception scores of [36, 37, 28] are directly
derived from the corresponding references.

Although both WGAN and MMD GAN can generate sharp images as we show in Section 5.1, our
score is better than other GAN techniques except for DFM [36]. This seems to conﬁrm empirically
that higher order of moment matching between the real data and fake sample distribution beneﬁts
generating more diversiﬁed sample images. Also note DFM appears compatible with our method and
combing training techniques in DFM is a possible avenue for future work.

Method

Real data

Scores ± std.

11.95 ± .20

DFM [36]
ALI [37]
Improved GANs [28]

7.72
5.34
4.36

MMD GAN
WGAN
GMMN-C
GMMN-D
Table 1: Inception scores

6.17 ± .07
5.88 ± .07
3.94 ± .04
3.47 ± .03

5.3 Stability of MMD GAN

Figure 3: Computation time

We further illustrate how the MMD distance correlates well with the quality of the generated samples.
Figure 4 plots the evolution of the MMD GAN estimate the MMD distance during training for
MNIST, CelebA and LSUN datasets. We report the average of the ˆMfφ(PX , Pθ) with moving
average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We
observe during the whole training process, samples generated from the same noise vector across
iterations, remain similar in nature. (e.g., face identity and bedroom style are alike while details and
backgrounds will evolve.) This qualitative observation indicates valuable stability of the training
process. The decreasing curve with the improving quality of images supports the weak∗ topology
shown in Theorem 4. Also, We can see from the plot that the model converges very quickly. In Figure
4b, for example, it converges shortly after tens of thousands of generator iterations on CelebA dataset.

(a) MNIST

(b) CelebA

(c) LSUN Bedrooms

Figure 4: Training curves and generative samples at different stages of training. We can see a clear
correlation between lower distance and better sample quality.

5.4 Computation Issue

We conduct time complexity analysis with respect to the batch size B. The time complexity of each
iteration is O(B) for WGAN and O(KB2) for our proposed MMD GAN with a mixture of K RBF
kernels. The quadratic complexity O(B2) of MMD GAN is introduced by computing kernel matrix,
which is sometimes criticized for being inapplicable with large batch size in practice. However,
we point that there are several recent works, such as EBGAN [7], also matching pairwise relation
between samples of batch size, leading to O(B2) complexity as well.

8

Empirically, we ﬁnd that under GPU environment, the highly parallelized matrix operation tremen-
dously alleviated the quadratic time to almost linear time with modest B. Figure 3 compares the
computational time per generator iterations versus different B on Titan X. When B = 64, which
is adapted for training MMD GAN in our experiments setting, the time per iteration of WGAN
and MMD GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for
training GMMN in its references [9], the time per iteration becomes 4.431 and 8.565 seconds, respec-
tively. This result coheres our argument that the empirical computational time for MMD GAN is not
quadratically expensive compared to WGAN with powerful GPU parallel computation.

5.5 Better Lipschitz Approximation and Necessity of Auto-Encoder

We used weight-clipping for Lipschitz constraint in Assumption 2. Another approach for obtaining a
discriminator with the similar constraints that approximates a Wasserstein distance is [20], where the
gradient of the discriminator is constrained to be 1 between the generated and data points. Inspired
by [20], an alternative approach is to apply the gradient constraint as a regularizer to the witness
function for a different IPM, such as the MMD. This idea was ﬁrst proposed in [30] for the Energy
Distance (in parallel to our submission), which was shown in [31] to correspond to a gradient penalty
on the witness function for any RKHS-based MMD. Here we undertake a preliminary investigation
of this approach, where we also drop the requirement in Algorithm 1 for fφ to be injective, which we
observe that it is not necessary in practice. We show some preliminary results of training MMD GAN
with gradient penalty and without the auto-encoder in Figure 5. The preliminary study indicates that
MMD GAN can generate satisfactory results with other Lipschitz constraint approximation. One
potential future work is conducting more thorough empirical comparison studies between different
approximations.

(a) Cifar10, Giter = 300K

(b) CelebA, Giter = 300K

Figure 5: MMD GAN results using gradient penalty [20] and without auto-encoder reconstruction
loss during training.

6 Discussion

We introduce a new deep generative model trained via MMD with adversarially learned kernels. We
further study its theoretical properties and propose a practical realization MMD GAN, which can be
trained with much smaller batch size than GMMN and has competitive performances with state-of-the-
art GANs. We can view MMD GAN as the ﬁrst practical step forward connecting moment matching
network and GAN. One important direction is applying developed tools in moment matching [15] on
general GAN works based the connections shown by MMD GAN. Also, in Section 4, we connect
WGAN and MMD GAN by ﬁrst-order and inﬁnite-order moment matching. [24] shows ﬁnite-order
moment matching (∼ 5) achieves the best performance on domain adaption. One could extend MMD
GAN to this by using polynomial kernels. Last, in theory, an injective mapping fφ is necessary for
the theoretical guarantees. However, we observe that it is not mandatory in practice as we show
in Section 5.5. One conjecture is it usually learns the injective mapping with high probability by
parameterizing with neural networks, which worth more study as a future work.

9

We thank the reviewers for their helpful comments. We also thank Dougal Sutherland and Arthur
Gretton for their valuable feedbacks and pointing out an error in the previous draft. This work is
supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-
1563887.

Acknowledgments

References

[1] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In AISTATS, 2009.

[2] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.

[3] Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &

Business Media, 2013.

[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In ICML, 2015.

[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[6] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural

samplers using variational divergence minimization. In NIPS, 2016.

[7] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based Generative Adversarial Network. In ICLR,

[8] Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. In ICML, 2017.

[9] Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,

2017.

2015.

[10] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative

neural networks via maximum mean discrepancy optimization. In UAI, 2015.

[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander

Smola. A kernel two-sample test. JMLR, 2012.

[12] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

[13] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In CVPR, 2015.

[14] Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas,
Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized
maximum mean discrepancy. In ICLR, 2017.

[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel
mean embedding of distributions: A review and beyonds. arXiv preprint arXiv:1605.09522,
2016.

[16] Kenji Fukumizu, Arthur Gretton, Gert R Lanckriet, Bernhard Schölkopf, and Bharath K Sripe-
rumbudur. Kernel choice and classiﬁability for rkhs embeddings of probability distributions. In
NIPS, 2009.

[17] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and

K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.

[18] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano
Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale
two-sample tests. In NIPS, 2012.

10

[19] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel

learning. In AISTATS, 2016.

[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.

Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.

[21] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal

regression. 1999.

[22] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.

[23] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature

matching gan. arxiv pre-print 1702.08398, 2017.

[24] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learn-
ing. arXiv preprint arXiv:1702.08811, 2017.

[25] Shuangfei Zhai, Yu Cheng, Rogério Schmidt Feris, and Zhongfei Zhang. Generative adversarial
networks as variational training of energy based models. CoRR, abs/1611.01799, 2016.

[26] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

[27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Adversarial generator-encoder net-

works. arXiv preprint arXiv:1704.02304, 2017.

[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.

Improved techniques for training gans. In NIPS, 2016.

[29] David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717, 2017.

[30] Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Rémi Munos. The cramer distance as a solution to biased wasserstein
gradients. arXiv preprint arXiv:1705.10743, 2017.

[31] Arthur Gretton. Notes on the cramer gan. https://medium.com/towards-data-science/

notes-on-the-cramer-gan-752abd505c00, 2017. Accessed: 2017-11-2.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 1998.

[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[34] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with

deep convolutional generative adversarial networks. In ICLR, 2016.

[35] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.

[36] D Warde-Farley and Y Bengio. Improving generative adversarial networks with denoising

feature matching. In ICLR, 2017.

[37] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mas-

tropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017.

[38] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R.G.
Lanckriet. Hilbert space embeddings and metrics on probability measures. JMLR, 2010.

11

A Technical Proof

A.1 Proof of Theorem 3

Proof. Since MMD is a probabilistic metric [11], we have the triangular inequality for every Mf .
Therefore,
| max
φ

|Mfφ(PX , Pθ) − Mfφ(PX , Pθ(cid:48))|

Mfφ(PX , Pθ) − max

Mfφ(PX , Pθ(cid:48))| ≤

max
φ

(6)

φ

=

≤

|Mfφ∗ (PX , Pθ) − Mfφ∗ (PX , Pθ(cid:48))|
Mfφ∗ (Pθ, Pθ(cid:48)),

(7)

(8)

where φ∗ is the solution of (6).
By deﬁnition, given any φ ∈ Φ, deﬁne hθ = fφ ◦ gθ, the MMD distance Mfφ(Pθ, Pθ(cid:48))
= Ez,z(cid:48) [k(hθ(z), hθ(z(cid:48))) − 2k(hθ(z), hθ(cid:48)(z(cid:48))) + k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48)))]
≤ Ez,z(cid:48) [|k(hθ(z), hθ(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|] + Ez,z(cid:48) [|k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|]

In this, we consider Gaussian kernel k, therefore
|k(hθ(z), hθ(z(cid:48)))−k(hθ(z), hθ(cid:48)(z(cid:48)))| = | exp(−(cid:107)hθ(z)−hθ(z(cid:48))(cid:107)2)−exp(−(cid:107)hθ(z)−hθ(cid:48)(z(cid:48))(cid:107)2)| ≤ 1,
for all (θ, θ(cid:48)) and (z, z(cid:48)). Similarly, |k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))| ≤ 1. Combining the
above claim with (7) and bounded convergence theorem, we have

Mfφ(PX , Pθ) − max

| max
φ
which proves the continuity of maxf M M Df (PX , Pθ).
By Mean Value Theorem, |e−x2
with (8) and triangular inequality, we have

− e−y2

| ≤ maxz |2ze−z2

φ

Mfφ(PX , Pθ(cid:48))| θ→θ(cid:48)

−−−→ 0,

Mfφ(Pθ, Pθ(cid:48)) ≤ Ez,z(cid:48)[(cid:107)hθ(z(cid:48)) − hθ(cid:48)(z(cid:48))(cid:107) + (cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

= 2Ez[(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

| × |x − y| ≤ |x − y|. Incorporating it

Now let h be locally Lipschitz. For a given pair (θ, z) there is a constant L(θ, z) and an open set Uθ
such that for every (θ(cid:48), z) ∈ Uθ we have

(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107) ≤ L(θ, z)((cid:107)θ − θ(cid:48)(cid:107))

Under Assumption 2, we then achieve

|Mfφ(PX , Pθ) − Mfφ(PX , P(cid:48)

(9)
Combining (7) and (9) implies maxφ Mfφ(PX , Pθ) is locally Lipschitz and continuous everywhere.
Last, applying Radamacher’s theorem proves maxφ Mfφ(PX , Pθ) is differentiable almost everywhere,
which completes the proof.

θ)| ≤ Mfφ(Pθ, Pθ(cid:48)) ≤ 2Ez[L(θ, z)](cid:107)θ − θ(cid:48)(cid:107).

A.2 Proof of Theorem 4

Proof. The proof utilizes parts of results from [38].

If maxφ Mfφ(Pn, P) → 0, there exists φ ∈ Φ such that Mfφ(P, Pn) → 0 since Mfφ(P, Pn)

(⇒)
is non-negative. By [38], for any characteristic kernel k,

Therefore, if maxφ Mfφ(Pn, P) → 0, Pn

Mk(Pn, P) → 0 ⇐⇒ Pn
D−→ P.

D−→ P.

(⇐) By [38], given a characteristic kernel k, if supx,x(cid:48) k(x, x(cid:48)) ≤ 1, (cid:112)Mk(P, Q) ≤ W (P, Q),
where W (P, Q) is Wasserstein distance.
In this paper, we consider the kernel k(x, x(cid:48)) =
exp(−(cid:107)fφ(x) − fφ(x(cid:48))(cid:107)2) ≤ 1. By above, (cid:112)Mfφ(P, Q) ≤ W (P, Q), ∀φ. By [8], if Pn
D−→ P,
W (P, Pn) → 0. Combining all of them, we get

Pn

D−→ P =⇒ W (P, Pn) → 0 =⇒ max

Mfφ(P, Pn) → 0.

φ

12

A.3 Proof of Theorem 5

Proof. The proof assumes fφ(x) is scalar, but the vector case can be proved with the same sketch.
First, if E[fφ(x)] > E[fφ(gθ(z))], then φ = φ(cid:48). If E[fφ(x)] < E[fφ(gθ(z))], we let f = −fφ, then
E[f (x)] > E[f (gθ(z))] and ﬂipping sign does not change the MMD distance. If we parameterized
fφ by a neural network, which has a linear output layer, φ(cid:48) can realized by ﬂipping the sign of the
weights of the last layer.

B Property of MMD with Fixed and Learned Kernels

B.1 Continuity and Differentiability

One can simplify Theorem 3 and its proof for standard MMD distance to show MMD is also
continuous and differentiable almost everywhere. In [8], they propose a counterexample to show the
discontinuity of MMD by assuming H = L2. However, it is known that L2 is not in RKHS, so the
discussed counterexample is not appropriate.

B.2 IPM Framework

From integral probability metrics (IPM), the probabilistic distance can be deﬁned as

∆(P, Q) = sup
f ∈F

Ex∼P[f (x)] − Ey∼Q[f (y)].

(10)

By changing the function class F, we can recover several distances, such as total variation, Wasser-
stein distance and MMD distance. From [8], the discriminator fφ in different existing works of GAN
can be explained to be used to solve different probabilistic metrics based on (10). For MMD, the
function class F is {(cid:107)f (cid:107)Hk ≤ 1}, where H is RKHS associated with kernel k. Different form many
distances, such as total variation and Wasserstein distance, there is an analytical representation [11]
as we show in Section 2, which is

∆(P, Q) = M M D(P, Q) = (cid:107)µP − µQ(cid:107)H =

EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

(cid:113)

Because of the analytical representation of (10), GMMN does not need an additional network fφ for
estimating the distance.

Here we also provide an explanation of the proposed MMD with adversarially learned kernel under
IPM framework. The MMD distance with adversarially learned kernel is represented as

M M D(P, Q),

max
k∈K

The corresponding IPM formulation is
∆(P, Q) = max
k∈K

M M D(P, Q) =

sup
f ∈Hk1 ∪···∪Hkn

Ex∼P[f (x)] − Ey∼Q[f (y)],

where ki ∈ K, ∀i. From this perspective, the proposed MMD distance with adversarially learned
kernel is still deﬁned by IPM but with a larger function class.

B.3 MMD is an Efﬁcient Moment Matching

We consider the example of matching ﬁrst and second moments of P and Q. The (cid:96)1 objective in
McGAN [23] is

(cid:107)µP − µQ(cid:107)1 + (cid:107)ΣP − ΣQ(cid:107)∗,

where (cid:107) · (cid:107)∗ is the trace norm. The objective can be changed to be general (cid:96)q norm.

In MMD, with polynomial kernel k(x, y) = 1 + x(cid:62)y, the MMD distance is

2(cid:107)µP − µQ(cid:107)2 + (cid:107)ΣP − ΣQ + µPµ(cid:62)

P − µQµ(cid:62)

Q (cid:107)2
F ,

which is inexact moment matching because the second term contains the quadratic of the ﬁrst moment.
It is difﬁcult to match high-order moments, because we have to deal with high order tensors directly.
On the other hand, MMD can easily match high-order moments (even inﬁnite order moments by
using Gaussian kernel) with kernel tricks, and enjoys strong theoretical guarantee.

13

C Performance of GMMN

(a) GMMN-D on MNIST

(b) GMMN-C on MNIST

(c) GMMN-D on CIFAR-10

(d) GMMN-C CIFAR-10

Figure 6: Generative samples from GMMN-D and GMMN-C with training batch size B = 1024.

14

7
1
0
2
 
v
o
N
 
7
2
 
 
]

G
L
.
s
c
[
 
 
3
v
4
8
5
8
0
.
5
0
7
1
:
v
i
X
r
a

MMD GAN: Towards Deeper Understanding of
Moment Matching Network

Chun-Liang Li1,∗ Wei-Cheng Chang1,∗ Yu Cheng2 Yiming Yang1 Barnabás Póczos1

1 Carnegie Mellon University,

2AI Foundations, IBM Research

{chunlial,wchang2,yiming,bapoczos}@cs.cmu.edu
(∗ denotes equal contribution)

chengyu@us.ibm.com

Abstract

Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the discriminator
in GAN with a two-sample test based on kernel maximum mean discrepancy
(MMD). Although some theoretical guarantees of MMD have been studied, the
empirical performance of GMMN is still not as competitive as that of GAN on
challenging and large benchmark datasets. The computational efﬁciency of GMMN
is also less desirable in comparison with GAN, partially due to its requirement for
a rather large batch size during the training. In this paper, we propose to improve
both the model expressiveness of GMMN and its computational efﬁciency by
introducing adversarial kernel learning techniques, as the replacement of a ﬁxed
Gaussian kernel in the original GMMN. The new approach combines the key ideas
in both GMMN and GAN, hence we name it MMD GAN. The new distance measure
in MMD GAN is a meaningful loss that enjoys the advantage of weak∗ topology
and can be optimized via gradient descent with relatively small batch sizes. In our
evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA
and LSUN, the performance of MMD GAN signiﬁcantly outperforms GMMN, and
is competitive with other representative GAN works.

1

Introduction

The essence of unsupervised learning models the underlying distribution PX of the data X . Deep
generative model [1, 2] uses deep learning to approximate the distribution of complex datasets with
promising results. However, modeling arbitrary density is a statistically challenging task [3]. In many
applications, such as caption generation [4], accurate density estimation is not even necessary since
we are only interested in sampling from the approximated distribution.
Rather than estimating the density of PX , Generative Adversarial Network (GAN) [5] starts from a
base distribution PZ over Z, such as Gaussian distribution, then trains a transformation network gθ
such that Pθ ≈ PX , where Pθ is the underlying distribution of gθ(z) and z ∼ PZ . During the training,
GAN-based algorithms require an auxiliary network fφ to estimate the distance between PX and Pθ.
Different probabilistic (pseudo) metrics have been studied [5–8] under GAN framework.
Instead of training an auxiliary network fφ for measuring the distance between PX and Pθ, Generative
moment matching network (GMMN) [9, 10] uses kernel maximum mean discrepancy (MMD) [11],
which is the centerpiece of nonparametric two-sample test, to determine the distribution distances.
During the training, gθ is trained to pass the hypothesis test (minimize MMD distance). [11] shows
even the simple Gaussian kernel enjoys the strong theoretical guarantees (Theorem 1). However, the
empirical performance of GMMN does not meet its theoretical properties. There is no promising
empirical results comparable with GAN on challenging benchmarks [12, 13]. Computationally,

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

it also requires larger batch size than GAN needs for training, which is considered to be less
efﬁcient [9, 10, 14, 8]

In this work, we try to improve GMMN and consider using MMD with adversarially learned kernels
instead of ﬁxed Gaussian kernels to have better hypothesis testing power. The main contributions of
this work are:

• In Section 2, we prove that training gθ via MMD with learned kernels is continuous and differen-
tiable, which guarantees the model can be trained by gradient descent. Second, we prove a new
distance measure via kernel learning, which is a sensitive loss function to the distance between
PX and Pθ (weak∗ topology). Empirically, the loss decreases when two distributions get closer.
• In Section 3, we propose a practical realization called MMD GAN that learns generator gθ with
the adversarially trained kernel. We further propose a feasible set reduction to speed up and
stabilize the training of MMD GAN.

• In Section 5, we show that MMD GAN is computationally more efﬁcient than GMMN, which can
be trained with much smaller batch size. We also demonstrate that MMD GAN has promising
results on challenging datasets, including CIFAR-10, CelebA and LSUN, where GMMN fails. To
our best knowledge, we are the ﬁrst MMD based work to achieve comparable results with other
GAN works on these datasets.

Finally, we also study the connection to existing works in Section 4. Interestingly, we show Wasser-
stein GAN [8] is the special case of the proposed MMD GAN under certain conditions. The uniﬁed
view shows more connections between moment matching and GAN, which can potentially inspire
new algorithms based on well-developed tools in statistics [15]. Our experiment code is available at
https://github.com/OctoberChang/MMD-GAN.

2 GAN, Two-Sample Test and GMMN

i=1, where xi ∈ X and xi ∼ PX . If we are interested in sampling
Assume we are given data {xi}n
from PX , it is not necessary to estimate the density of PX . Instead, Generative Adversarial Network
(GAN) [5] trains a generator gθ parameterized by θ to transform samples z ∼ PZ , where z ∈ Z,
into gθ(z) ∼ Pθ such that Pθ ≈ PX . To measure the similarity between PX and Pθ via their
samples {x}n
j=1 during the training, [5] trains the discriminator fφ parameterized
by φ for help. The learning is done by playing a two-player game, where fφ tries to distinguish xi
and gθ(zj) while gθ aims to confuse fφ by generating gθ(zj) similar to xi.

i=1 and {gθ(zj)}n

On the other hand, distinguishing two distributions by ﬁnite samples is known as Two-Sample Test in
statistics. One way to conduct two-sample test is via kernel maximum mean discrepancy (MMD) [11].
Given two distributions P and Q, and a kernel k, the square of MMD distance is deﬁned as

Mk(P, Q) = (cid:107)µP − µQ(cid:107)2

H = EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

Theorem 1. [11] Given a kernel k, if k is a characteristic kernel, then Mk(P, Q) = 0 iff P = Q.

GMMN: One example of characteristic kernel is Gaussian kernel k(x, x(cid:48)) = exp((cid:107)x − x(cid:48)(cid:107)2). Based
on Theorem 1, [9, 10] propose generative moment-matching network (GMMN), which trains gθ by

Mk(PX , Pθ),

min
θ

(1)

with a ﬁxed Gaussian kernel k rather than training an additional discriminator f as GAN.

2.1 MMD with Kernel Learning

In practice we use ﬁnite samples from distributions to estimate MMD distance. Given X =
{x1, · · · , xn} ∼ P and Y = {y1, · · · , yn} ∼ Q, one estimator of Mk(P, Q) is

ˆMk(X, Y ) =

k(xi, x(cid:48)

i) −

k(xi, yj) +

k(yj, y(cid:48)

j).

1
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=i(cid:48)

2
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=j

1
(cid:1)
(cid:0)n
2

(cid:88)

j(cid:54)=j(cid:48)

Because of the sampling variance, ˆM (X, Y ) may not be zero even when P = Q. We then conduct
hypothesis test with null hypothesis H0 : P = Q. For a given allowable probability of false rejection α,

2

we can only reject H0, which imply P (cid:54)= Q, if ˆM (X, Y ) > cα for some chose threshold cα > 0.
Otherwise, Q passes the test and Q is indistinguishable from P under this test. Please refer to [11] for
more details.
Intuitively, if kernel k cannot result in high MMD distance Mk(P, Q) when P (cid:54)= Q, ˆMk(P, Q) has
more chance to be smaller than cα. Then we are unlikely to reject the null hypothesis H0 with ﬁnite
samples, which implies Q is not distinguishable from P. Therefore, instead of training gθ via (1) with
a pre-speciﬁed kernel k as GMMN, we consider training gθ via

min
θ

max
k∈K

Mk(PX , Pθ),

(2)

(3)

which takes different possible characteristic kernels k ∈ K into account. On the other hand, we
could also view (2) as replacing the ﬁxed kernel k in (1) with the adversarially learned kernel
arg maxk∈K Mk(PX , Pθ) to have stronger signal where P (cid:54)= Pθ to train gθ. We refer interested
readers to [16] for more rigorous discussions about testing power and increasing MMD distances.

However, it is difﬁcult to optimize over all characteristic kernels when we solve (2). By [11, 17] if f
is a injective function and k is characteristic, then the resulted kernel ˜k = k ◦ f , where ˜k(x, x(cid:48)) =
k(f (x), f (x(cid:48))) is still characteristic. If we have a family of injective functions parameterized by φ,
which is denoted as fφ, we are able to change the objective to be

min
θ

max
φ

Mk◦fφ(PX , Pθ),

In this paper, we consider the case that combining Gaussian kernels with injective functions fφ, where
˜k(x, x(cid:48)) = exp(−(cid:107)fφ(x) − fφ(x)(cid:48)(cid:107)2). One example function class of f is {fφ|fφ(x) = φx, φ > 0},
which is equivalent to the kernel bandwidth tuning. A more complicated realization will be discussed
in Section 3. Next, we abuse the notation Mfφ(P, Q) to be MMD distance given the composition
kernel of Gaussian kernel and fφ in the following. Note that [18] considers the linear combination
of characteristic kernels, which can also be incorporated into the discussed composition kernels. A
more general kernel is studied in [19].

2.2 Properties of MMD with Kernel Learning

[8] discuss different distances between distributions adopted by existing deep learning algorithms, and
show many of them are discontinuous, such as Jensen-Shannon divergence [5] and Total variation [7],
except for Wasserstein distance. The discontinuity makes the gradient descent infeasible for training.
From (3), we train gθ via minimizing maxφ Mfφ(PX , Pθ). Next, we show maxφ Mfφ(PX , Pθ) also
enjoys the advantage of being a continuous and differentiable objective in θ under mild assumptions.
Assumption 2. g : Z × Rm → X is locally Lipschitz, where Z ⊆ Rd. We will denote gθ(z) the
evaluation on (z, θ) for convenience. Given fφ and a probability distribution Pz over Z, g satisﬁes
Assumption 2 if there are local Lipschitz constants L(θ, z) for fφ ◦ g, which is independent of φ, such
that Ez∼Pz [L(θ, z)] < +∞.
Theorem 3. The generator function gθ parameterized by θ is under Assumption 2. Let PX be a ﬁxed
distribution over X and Z be a random variable over the space Z. We denote Pθ the distribution
of gθ(Z), then maxφ Mfφ(PX , Pθ) is continuous everywhere and differentiable almost everywhere
in θ.

If gθ is parameterized by a feed-forward neural network, it satisﬁes Assumption 2 and can be trained
via gradient descent as well as propagation, since the objective is continuous and differentiable
followed by Theorem 3. More technical discussions are shown in Appendix B.
Theorem 4. (weak∗ topology) Let {Pn} be a sequence of distributions. Considering n → ∞,
D−→ PX , where D−→ means converging in
under mild Assumption, maxφ Mfφ(PX , Pn) → 0 ⇐⇒ Pn
distribution [3].

Theorem 4 shows that maxφ Mfφ(PX , Pn) is a sensible cost function to the distance between PX
and Pn. The distance is decreasing when Pn is getting closer to PX , which beneﬁts the supervision
of the improvement during the training. All proofs are omitted to Appendix A. In the next section,
we introduce a practical realization of training gθ via optimizing minθ maxφ Mfφ(PX , Pθ).

3

3 MMD GAN

To approximate (3), we use neural networks to parameterized gθ and fφ with expressive power.
For gθ, the assumption is locally Lipschitz, where commonly used feed-forward neural networks
satisfy this constraint. Also, the gradient (cid:53)θ (maxφ fφ ◦ gθ) has to be bounded, which can be
done by clipping φ [8] or gradient penalty [20]. The non-trivial part is fφ has to be injective.
For an injective function f , there exists an function f −1 such that f −1(f (x)) = x, ∀x ∈ X and
f −1(f (g(z))) = g(z), ∀z ∈ Z 1, which can be approximated by an autoencoder. In the following,
we denote φ = {φe, φd} to be the parameter of discriminator networks, which consists of an encoder
fφe , and train the corresponding decoder fφd ≈ f −1 to regularize f . The objective (3) is relaxed to
be

min
θ

max
φ

Mfφe

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe(y))(cid:107)2.

(4)

Note that we ignore the autoencoder objective when we train θ, but we use (4) for a concise
presentation. We note that the empirical study suggests autoencoder objective is not necessary to
lead the successful GAN training as we will show in Section 5, even though the injective property is
required in Theorem 1.

The proposed algorithm is similar to GAN [5], which aims to optimize two neural networks gθ and fφ
in a minmax formulation, while the meaning of the objective is different. In [5], fφe is a discriminator
(binary) classiﬁer to distinguish two distributions. In the proposed algorithm, distinguishing two
distribution is still done by two-sample test via MMD, but with an adversarially learned kernel
parametrized by fφe . gθ is then trained to pass the hypothesis test. More connection and difference
with related works is discussed in Section 4. Because of the similarity of GAN, we call the proposed
algorithm MMD GAN. We present an implementation with the weight clipping in Algorithm 1, but
one can easily extend to other Lipschitz approximations, such as gradient penalty [20].

Algorithm 1: MMD GAN, our proposed algorithm.
input

:α the learning rate, c the clipping parameter, B the batch size, nc the number of iterations of
discriminator per generator update.

initialize generator parameter θ and discriminator parameter φ;
while θ has not converged do
for t = 1, . . . , nc do

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

(P(X ), P(gθ(Z))) − λEy∈X ∪g(Z)(cid:107)y − fφd (fφe (y))(cid:107)2

Sample a minibatches {xi}B
gφ ← ∇φMfφe
φ ← φ + α · RMSProp(φ, gφ)
φ ← clip(φ, −c, c)
Sample a minibatches {xi}B
gθ ← ∇θMfφe
θ ← θ − α · RMSProp(θ, gθ)

(P(X ), P(gθ(Z)))

i=1 ∼ P(X ) and {zj}B

j=1 ∼ P(Z)

Encoding Perspective of MMD GAN: Besides from using kernel selection to explain MMD GAN,
the other way to see the proposed MMD GAN is viewing fφe as a feature transformation function,
and the kernel two-sample test is performed on this transformed feature space (i.e., the code space of
the autoencoder). The optimization is ﬁnding a manifold with stronger signals for MMD two-sample
test. From this perspective, [9] is the special case of MMD GAN if fφe is the identity mapping
function. In such circumstance, the kernel two-sample test is conducted in the original data space.

3.1 Feasible Set Reduction

Theorem 5. For any fφ, there exists f (cid:48)
Ez[fφ(cid:48)(gθ(z))].

φ such that Mfφ(Pr, Pθ) = Mf (cid:48)

φ

(Pr, Pθ) and Ex[fφ(x)] (cid:23)

With Theorem 5, we could reduce the feasible set of φ during the optimization by solving

minθ maxφ Mfφ(Pr, Pθ)

s.t. E[fφ(x)] (cid:23) E[fφ(gθ(z))]

1Note that injective is not necessary invertible.

4

which the optimal solution is still equivalent to solving (2).

However, it is hard to solve the constrained optimization problem with backpropagation. We relax
the constraint by ordinal regression [21] to be

min
θ

max
φ

Mfφ(Pr, Pθ) + λ min (cid:0)E[fφ(x)] − E[fφ(gθ(z))], 0(cid:1),

which only penalizes the objective when the constraint is violated. In practice, we observe that
reducing the feasible set makes the training faster and stabler.

4 Related Works

There has been a recent surge on improving GAN [5]. We review some related works here.

If we composite fφ with linear kernel instead of Gaussian kernel, and

Connection with WGAN:
restricting the output dimension h to be 1, we then have the objective
(cid:107)E[fφ(x)] − E[fφ(gθ(z))](cid:107)2.

(5)

min
θ

max
φ
Parameterizing fφ and gθ with neural networks and assuming ∃φ(cid:48) ∈ Φ such f (cid:48)
φ = −fφ, ∀Φ, recovers
Wasserstein GAN (WGAN) [8] 2. If we treat fφ(x) as the data transform function, WGAN can
be interpreted as ﬁrst-order moment matching (linear kernel) while MMD GAN aims to match
inﬁnite order of moments with Gaussian kernel form Taylor expansion [9]. Theoretically, Wasserstein
distance has similar theoretically guarantee as Theorem 1, 3 and 4. In practice, [22] show neural
networks does not have enough capacity to approximate Wasserstein distance. In Section 5, we
demonstrate matching high-order moments beneﬁts the results. [23] also propose McGAN that
matches second order moment from the primal-dual norm perspective. However, the proposed
algorithm requires matrix (tensor) decompositions because of exact moment matching [24], which
is hard to scale to higher order moment matching. On the other hand, by giving up exact moment
matching, MMD GAN can match high-order moments with kernel tricks. More detailed discussions
are in Appendix B.3.

Difference from Other Works with Autoencoders: Energy-based GANs [7, 25] also utilizes
the autoencoder (AE) in its discriminator from the energy model perspective, which minimizes
the reconstruction error of real samples x while maximize the reconstruction error of generated
samples gθ(z). In contrast, MMD GAN uses AE to approximate invertible functions by minimizing
the reconstruction errors of both real samples x and generated samples gθ(z). Also, [8] show EBGAN
approximates total variation, with the drawback of discontinuity, while MMD GAN optimizes MMD
distance. The other line of works [2, 26, 9] aims to match the AE codespace f (x), and utilize the
decoder fdec(·). [2, 26] match the distribution of f (x) and z via different distribution distances and
generate data (e.g. image) by fdec(z). [9] use MMD to match f (x) and g(z), and generate data via
fdec(g(z)). The proposed MMD GAN matches the f (x) and f (g(z)), and generates data via g(z)
directly as GAN. [27] is similar to MMD GAN but it considers KL-divergence without showing
continuity and weak∗ topology guarantee as we prove in Section 2.

Other GAN Works:
In addition to the discussed works, there are several extended works of GAN.
[28] proposes using the linear kernel to match ﬁrst moment of its discriminator’s latent features. [14]
considers the variance of empirical MMD score during the training. Also, [14] only improves the
latent feature matching in [28] by using kernel MMD, instead of proposing an adversarial training
framework as we studied in Section 2. [29] uses Wasserstein distance to match the distribution of
autoencoder loss instead of data. One can consider to extend [29] to higher order matching based on
the proposed MMD GAN. A parallel work [30] use energy distance, which can be treated as MMD
GAN with different kernel. However, there are some potential problems of its critic. More discussion
can be referred to [31].

5 Experiment

We train MMD GAN for image generation on the MNIST [32], CIFAR-10 [33], CelebA [13],
and LSUN bedrooms [12] datasets, where the size of training instances are 50K, 50K, 160K, 3M

2Theoretically, they are not equivalent but the practical neural network approximation results in the same

algorithm.

5

respectively. All the samples images are generated from a ﬁxed noise random vectors and are not
cherry-picked.

Network architecture: In our experiments, we follow the architecture of DCGAN [34] to design gθ
by its generator and fφ by its discriminator except for expanding the output layer of fφ to be h
dimensions.

Kernel designs: The loss function of MMD GAN is implicitly associated with a family of character-
istic kernels. Similar to the prior MMD seminal papers [10, 9, 14], we consider a mixture of K RBF
kernels k(x, x(cid:48)) = (cid:80)K
q=1 kσq (x, x(cid:48)) where kσq is a Gaussian kernel with bandwidth parameter σq.
Tuning kernel bandwidth σq optimally still remains an open problem. In this works, we ﬁxed K = 5
and σq to be {1, 2, 4, 8, 16} and left the fφ to learn the kernel (feature representation) under these σq.

Hyper-parameters: We use RMSProp [35] with learning rate of 0.00005 for a fair comparison with
WGAN as suggested in its original paper [8]. We ensure the boundedness of model parameters
of discriminator by clipping the weights point-wisely to the range [−0.01, 0.01] as required by
Assumption 2. The dimensionality h of the latent space is manually set according to the complexity
of the dataset. We thus use h = 16 for MNIST, h = 64 for CelebA, and h = 128 for CIFAR-10 and
LSUN bedrooms. The batch size is set to be B = 64 for all datasets.

5.1 Qualitative Analysis

(a) GMMN-D MNIST

(b) GMMN-C MNIST

(c) MMD GAN MNIST

(d) GMMN-D CIFAR-10

(e) GMMN-C CIFAR-10

(f) MMD GAN CIFAR-10

Figure 1: Generated samples from GMMN-D (Dataspace), GMMN-C (Codespace) and our MMD
GAN with batch size B = 64.

We start with comparing MMD GAN with GMMN on two standard benchmarks, MNIST and CIFAR-
10. We consider two variants for GMMN. The ﬁrst one is original GMMN, which trains the generator
by minimizing the MMD distance on the original data space. We call it as GMMN-D. To compare
with MMD GAN, we also pretrain an autoencoder for projecting data to a manifold, then ﬁx the
autoencoder as a feature transformation, and train the generator by minimizing the MMD distance in
the code space. We call it as GMMN-C.

The results are pictured in Figure 1. Both GMMN-D and GMMN-C are able to generate meaningful
digits on MNIST because of the simple data structure. By a closer look, nonetheless, the boundary
and shape of the digits in Figure 1a and 1b are often irregular and non-smooth. In contrast, the sample

6

(a) WGAN MNIST

(b) WGAN CelebA

(c) WGAN LSUN

(d) MMD GAN MNIST

(e) MMD GAN CelebA

(f) MMD GAN LSUN

Figure 2: Generated samples from WGAN and MMD GAN on MNIST, CelebA, and LSUN bedroom
datasets.

digits in Figure 1c are more natural with smooth outline and sharper strike. For CIFAR-10 dataset,
both GMMN variants fail to generate meaningful images, but resulting some low level visual features.
We observe similar cases in other complex large-scale datasets such as CelebA and LSUN bedrooms,
thus results are omitted. On the other hand, the proposed MMD GAN successfully outputs natural
images with sharp boundary and high diversity. The results in Figure 1 conﬁrm the success of the
proposed adversarial learned kernels to enrich statistical testing power, which is the key difference
between GMMN and MMD GAN.

If we increase the batch size of GMMN to 1024, the image quality is improved, however, it is still
not competitive to MMD GAN with B = 64. The images are put in Appendix C. This demonstrates
that the proposed MMD GAN can be trained more efﬁciently than GMMN with smaller batch size.

Comparisons with GANs: There are several representative extensions of GANs. We consider
recent state-of-art WGAN [8] based on DCGAN structure [34], because of the connection with MMD
GAN discussed in Section 4. The results are shown in Figure 2. For MNIST, the digits generated
from WGAN in Figure 2a are more unnatural with peculiar strikes. In Contrary, the digits from
MMD GAN in Figure 2d enjoy smoother contour. Furthermore, both WGAN and MMD GAN
generate diversiﬁed digits, avoiding the mode collapse problems appeared in the literature of training
GANs. For CelebA, we can see the difference of generated samples from WGAN and MMD GAN.
Speciﬁcally, we observe varied poses, expressions, genders, skin colors and light exposure in Figure
2b and 2e. By a closer look (view on-screen with zooming in), we observe that faces from WGAN
have higher chances to be blurry and twisted while faces from MMD GAN are more spontaneous with
sharp and acute outline of faces. As for LSUN dataset, we could not distinguish salient differences
between the samples generated from MMD GAN and WGAN.

5.2 Quantitative Analysis

To quantitatively measure the quality and diversity of generated samples, we compute the inception
score [28] on CIFAR-10 images. The inception score is used for GANs to measure samples quality
and diversity on the pretrained inception model [28]. Models that generate collapsed samples have
a relatively low score. Table 1 lists the results for 50K samples generated by various unsupervised

7

generative models trained on CIFAR-10 dataset. The inception scores of [36, 37, 28] are directly
derived from the corresponding references.

Although both WGAN and MMD GAN can generate sharp images as we show in Section 5.1, our
score is better than other GAN techniques except for DFM [36]. This seems to conﬁrm empirically
that higher order of moment matching between the real data and fake sample distribution beneﬁts
generating more diversiﬁed sample images. Also note DFM appears compatible with our method and
combing training techniques in DFM is a possible avenue for future work.

Method

Real data

Scores ± std.

11.95 ± .20

DFM [36]
ALI [37]
Improved GANs [28]

7.72
5.34
4.36

MMD GAN
WGAN
GMMN-C
GMMN-D
Table 1: Inception scores

6.17 ± .07
5.88 ± .07
3.94 ± .04
3.47 ± .03

5.3 Stability of MMD GAN

Figure 3: Computation time

We further illustrate how the MMD distance correlates well with the quality of the generated samples.
Figure 4 plots the evolution of the MMD GAN estimate the MMD distance during training for
MNIST, CelebA and LSUN datasets. We report the average of the ˆMfφ(PX , Pθ) with moving
average to smooth the graph to reduce the variance caused by mini-batch stochastic training. We
observe during the whole training process, samples generated from the same noise vector across
iterations, remain similar in nature. (e.g., face identity and bedroom style are alike while details and
backgrounds will evolve.) This qualitative observation indicates valuable stability of the training
process. The decreasing curve with the improving quality of images supports the weak∗ topology
shown in Theorem 4. Also, We can see from the plot that the model converges very quickly. In Figure
4b, for example, it converges shortly after tens of thousands of generator iterations on CelebA dataset.

(a) MNIST

(b) CelebA

(c) LSUN Bedrooms

Figure 4: Training curves and generative samples at different stages of training. We can see a clear
correlation between lower distance and better sample quality.

5.4 Computation Issue

We conduct time complexity analysis with respect to the batch size B. The time complexity of each
iteration is O(B) for WGAN and O(KB2) for our proposed MMD GAN with a mixture of K RBF
kernels. The quadratic complexity O(B2) of MMD GAN is introduced by computing kernel matrix,
which is sometimes criticized for being inapplicable with large batch size in practice. However,
we point that there are several recent works, such as EBGAN [7], also matching pairwise relation
between samples of batch size, leading to O(B2) complexity as well.

8

Empirically, we ﬁnd that under GPU environment, the highly parallelized matrix operation tremen-
dously alleviated the quadratic time to almost linear time with modest B. Figure 3 compares the
computational time per generator iterations versus different B on Titan X. When B = 64, which
is adapted for training MMD GAN in our experiments setting, the time per iteration of WGAN
and MMD GAN is 0.268 and 0.676 seconds, respectively. When B = 1024, which is used for
training GMMN in its references [9], the time per iteration becomes 4.431 and 8.565 seconds, respec-
tively. This result coheres our argument that the empirical computational time for MMD GAN is not
quadratically expensive compared to WGAN with powerful GPU parallel computation.

5.5 Better Lipschitz Approximation and Necessity of Auto-Encoder

We used weight-clipping for Lipschitz constraint in Assumption 2. Another approach for obtaining a
discriminator with the similar constraints that approximates a Wasserstein distance is [20], where the
gradient of the discriminator is constrained to be 1 between the generated and data points. Inspired
by [20], an alternative approach is to apply the gradient constraint as a regularizer to the witness
function for a different IPM, such as the MMD. This idea was ﬁrst proposed in [30] for the Energy
Distance (in parallel to our submission), which was shown in [31] to correspond to a gradient penalty
on the witness function for any RKHS-based MMD. Here we undertake a preliminary investigation
of this approach, where we also drop the requirement in Algorithm 1 for fφ to be injective, which we
observe that it is not necessary in practice. We show some preliminary results of training MMD GAN
with gradient penalty and without the auto-encoder in Figure 5. The preliminary study indicates that
MMD GAN can generate satisfactory results with other Lipschitz constraint approximation. One
potential future work is conducting more thorough empirical comparison studies between different
approximations.

(a) Cifar10, Giter = 300K

(b) CelebA, Giter = 300K

Figure 5: MMD GAN results using gradient penalty [20] and without auto-encoder reconstruction
loss during training.

6 Discussion

We introduce a new deep generative model trained via MMD with adversarially learned kernels. We
further study its theoretical properties and propose a practical realization MMD GAN, which can be
trained with much smaller batch size than GMMN and has competitive performances with state-of-the-
art GANs. We can view MMD GAN as the ﬁrst practical step forward connecting moment matching
network and GAN. One important direction is applying developed tools in moment matching [15] on
general GAN works based the connections shown by MMD GAN. Also, in Section 4, we connect
WGAN and MMD GAN by ﬁrst-order and inﬁnite-order moment matching. [24] shows ﬁnite-order
moment matching (∼ 5) achieves the best performance on domain adaption. One could extend MMD
GAN to this by using polynomial kernels. Last, in theory, an injective mapping fφ is necessary for
the theoretical guarantees. However, we observe that it is not mandatory in practice as we show
in Section 5.5. One conjecture is it usually learns the injective mapping with high probability by
parameterizing with neural networks, which worth more study as a future work.

9

We thank the reviewers for their helpful comments. We also thank Dougal Sutherland and Arthur
Gretton for their valuable feedbacks and pointing out an error in the previous draft. This work is
supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-
1563887.

Acknowledgments

References

[1] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In AISTATS, 2009.

[2] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.

[3] Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &

Business Media, 2013.

[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In ICML, 2015.

[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[6] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural

samplers using variational divergence minimization. In NIPS, 2016.

[7] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based Generative Adversarial Network. In ICLR,

[8] Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. In ICML, 2017.

[9] Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,

2017.

2015.

[10] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative

neural networks via maximum mean discrepancy optimization. In UAI, 2015.

[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander

Smola. A kernel two-sample test. JMLR, 2012.

[12] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.

[13] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In CVPR, 2015.

[14] Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas,
Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized
maximum mean discrepancy. In ICLR, 2017.

[15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel
mean embedding of distributions: A review and beyonds. arXiv preprint arXiv:1605.09522,
2016.

[16] Kenji Fukumizu, Arthur Gretton, Gert R Lanckriet, Bernhard Schölkopf, and Bharath K Sripe-
rumbudur. Kernel choice and classiﬁability for rkhs embeddings of probability distributions. In
NIPS, 2009.

[17] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and

K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.

[18] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano
Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale
two-sample tests. In NIPS, 2012.

10

[19] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel

learning. In AISTATS, 2016.

[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.

Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.

[21] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. Support vector learning for ordinal

regression. 1999.

[22] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and
equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.

[23] Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature

matching gan. arxiv pre-print 1702.08398, 2017.

[24] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learn-
ing. arXiv preprint arXiv:1702.08811, 2017.

[25] Shuangfei Zhai, Yu Cheng, Rogério Schmidt Feris, and Zhongfei Zhang. Generative adversarial
networks as variational training of energy based models. CoRR, abs/1611.01799, 2016.

[26] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

[27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Adversarial generator-encoder net-

works. arXiv preprint arXiv:1704.02304, 2017.

[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.

Improved techniques for training gans. In NIPS, 2016.

[29] David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717, 2017.

[30] Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Rémi Munos. The cramer distance as a solution to biased wasserstein
gradients. arXiv preprint arXiv:1705.10743, 2017.

[31] Arthur Gretton. Notes on the cramer gan. https://medium.com/towards-data-science/

notes-on-the-cramer-gan-752abd505c00, 2017. Accessed: 2017-11-2.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 1998.

[33] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[34] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with

deep convolutional generative adversarial networks. In ICLR, 2016.

[35] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.

[36] D Warde-Farley and Y Bengio. Improving generative adversarial networks with denoising

feature matching. In ICLR, 2017.

[37] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mas-

tropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017.

[38] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert R.G.
Lanckriet. Hilbert space embeddings and metrics on probability measures. JMLR, 2010.

11

A Technical Proof

A.1 Proof of Theorem 3

Proof. Since MMD is a probabilistic metric [11], we have the triangular inequality for every Mf .
Therefore,
| max
φ

|Mfφ(PX , Pθ) − Mfφ(PX , Pθ(cid:48))|

Mfφ(PX , Pθ) − max

Mfφ(PX , Pθ(cid:48))| ≤

max
φ

(6)

φ

=

≤

|Mfφ∗ (PX , Pθ) − Mfφ∗ (PX , Pθ(cid:48))|
Mfφ∗ (Pθ, Pθ(cid:48)),

(7)

(8)

where φ∗ is the solution of (6).
By deﬁnition, given any φ ∈ Φ, deﬁne hθ = fφ ◦ gθ, the MMD distance Mfφ(Pθ, Pθ(cid:48))
= Ez,z(cid:48) [k(hθ(z), hθ(z(cid:48))) − 2k(hθ(z), hθ(cid:48)(z(cid:48))) + k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48)))]
≤ Ez,z(cid:48) [|k(hθ(z), hθ(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|] + Ez,z(cid:48) [|k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))|]

In this, we consider Gaussian kernel k, therefore
|k(hθ(z), hθ(z(cid:48)))−k(hθ(z), hθ(cid:48)(z(cid:48)))| = | exp(−(cid:107)hθ(z)−hθ(z(cid:48))(cid:107)2)−exp(−(cid:107)hθ(z)−hθ(cid:48)(z(cid:48))(cid:107)2)| ≤ 1,
for all (θ, θ(cid:48)) and (z, z(cid:48)). Similarly, |k(hθ(cid:48)(z), hθ(cid:48)(z(cid:48))) − k(hθ(z), hθ(cid:48)(z(cid:48)))| ≤ 1. Combining the
above claim with (7) and bounded convergence theorem, we have

Mfφ(PX , Pθ) − max

| max
φ
which proves the continuity of maxf M M Df (PX , Pθ).
By Mean Value Theorem, |e−x2
with (8) and triangular inequality, we have

− e−y2

| ≤ maxz |2ze−z2

φ

Mfφ(PX , Pθ(cid:48))| θ→θ(cid:48)

−−−→ 0,

Mfφ(Pθ, Pθ(cid:48)) ≤ Ez,z(cid:48)[(cid:107)hθ(z(cid:48)) − hθ(cid:48)(z(cid:48))(cid:107) + (cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

= 2Ez[(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107)]

| × |x − y| ≤ |x − y|. Incorporating it

Now let h be locally Lipschitz. For a given pair (θ, z) there is a constant L(θ, z) and an open set Uθ
such that for every (θ(cid:48), z) ∈ Uθ we have

(cid:107)hθ(z) − hθ(cid:48)(z)(cid:107) ≤ L(θ, z)((cid:107)θ − θ(cid:48)(cid:107))

Under Assumption 2, we then achieve

|Mfφ(PX , Pθ) − Mfφ(PX , P(cid:48)

(9)
Combining (7) and (9) implies maxφ Mfφ(PX , Pθ) is locally Lipschitz and continuous everywhere.
Last, applying Radamacher’s theorem proves maxφ Mfφ(PX , Pθ) is differentiable almost everywhere,
which completes the proof.

θ)| ≤ Mfφ(Pθ, Pθ(cid:48)) ≤ 2Ez[L(θ, z)](cid:107)θ − θ(cid:48)(cid:107).

A.2 Proof of Theorem 4

Proof. The proof utilizes parts of results from [38].

If maxφ Mfφ(Pn, P) → 0, there exists φ ∈ Φ such that Mfφ(P, Pn) → 0 since Mfφ(P, Pn)

(⇒)
is non-negative. By [38], for any characteristic kernel k,

Therefore, if maxφ Mfφ(Pn, P) → 0, Pn

Mk(Pn, P) → 0 ⇐⇒ Pn
D−→ P.

D−→ P.

(⇐) By [38], given a characteristic kernel k, if supx,x(cid:48) k(x, x(cid:48)) ≤ 1, (cid:112)Mk(P, Q) ≤ W (P, Q),
where W (P, Q) is Wasserstein distance.
In this paper, we consider the kernel k(x, x(cid:48)) =
exp(−(cid:107)fφ(x) − fφ(x(cid:48))(cid:107)2) ≤ 1. By above, (cid:112)Mfφ(P, Q) ≤ W (P, Q), ∀φ. By [8], if Pn
D−→ P,
W (P, Pn) → 0. Combining all of them, we get

Pn

D−→ P =⇒ W (P, Pn) → 0 =⇒ max

Mfφ(P, Pn) → 0.

φ

12

A.3 Proof of Theorem 5

Proof. The proof assumes fφ(x) is scalar, but the vector case can be proved with the same sketch.
First, if E[fφ(x)] > E[fφ(gθ(z))], then φ = φ(cid:48). If E[fφ(x)] < E[fφ(gθ(z))], we let f = −fφ, then
E[f (x)] > E[f (gθ(z))] and ﬂipping sign does not change the MMD distance. If we parameterized
fφ by a neural network, which has a linear output layer, φ(cid:48) can realized by ﬂipping the sign of the
weights of the last layer.

B Property of MMD with Fixed and Learned Kernels

B.1 Continuity and Differentiability

One can simplify Theorem 3 and its proof for standard MMD distance to show MMD is also
continuous and differentiable almost everywhere. In [8], they propose a counterexample to show the
discontinuity of MMD by assuming H = L2. However, it is known that L2 is not in RKHS, so the
discussed counterexample is not appropriate.

B.2 IPM Framework

From integral probability metrics (IPM), the probabilistic distance can be deﬁned as

∆(P, Q) = sup
f ∈F

Ex∼P[f (x)] − Ey∼Q[f (y)].

(10)

By changing the function class F, we can recover several distances, such as total variation, Wasser-
stein distance and MMD distance. From [8], the discriminator fφ in different existing works of GAN
can be explained to be used to solve different probabilistic metrics based on (10). For MMD, the
function class F is {(cid:107)f (cid:107)Hk ≤ 1}, where H is RKHS associated with kernel k. Different form many
distances, such as total variation and Wasserstein distance, there is an analytical representation [11]
as we show in Section 2, which is

∆(P, Q) = M M D(P, Q) = (cid:107)µP − µQ(cid:107)H =

EP[k(x, x(cid:48))] − 2EP,Q[k(x, y)] + EQ[k(y, y(cid:48))].

(cid:113)

Because of the analytical representation of (10), GMMN does not need an additional network fφ for
estimating the distance.

Here we also provide an explanation of the proposed MMD with adversarially learned kernel under
IPM framework. The MMD distance with adversarially learned kernel is represented as

M M D(P, Q),

max
k∈K

The corresponding IPM formulation is
∆(P, Q) = max
k∈K

M M D(P, Q) =

sup
f ∈Hk1 ∪···∪Hkn

Ex∼P[f (x)] − Ey∼Q[f (y)],

where ki ∈ K, ∀i. From this perspective, the proposed MMD distance with adversarially learned
kernel is still deﬁned by IPM but with a larger function class.

B.3 MMD is an Efﬁcient Moment Matching

We consider the example of matching ﬁrst and second moments of P and Q. The (cid:96)1 objective in
McGAN [23] is

(cid:107)µP − µQ(cid:107)1 + (cid:107)ΣP − ΣQ(cid:107)∗,

where (cid:107) · (cid:107)∗ is the trace norm. The objective can be changed to be general (cid:96)q norm.

In MMD, with polynomial kernel k(x, y) = 1 + x(cid:62)y, the MMD distance is

2(cid:107)µP − µQ(cid:107)2 + (cid:107)ΣP − ΣQ + µPµ(cid:62)

P − µQµ(cid:62)

Q (cid:107)2
F ,

which is inexact moment matching because the second term contains the quadratic of the ﬁrst moment.
It is difﬁcult to match high-order moments, because we have to deal with high order tensors directly.
On the other hand, MMD can easily match high-order moments (even inﬁnite order moments by
using Gaussian kernel) with kernel tricks, and enjoys strong theoretical guarantee.

13

C Performance of GMMN

(a) GMMN-D on MNIST

(b) GMMN-C on MNIST

(c) GMMN-D on CIFAR-10

(d) GMMN-C CIFAR-10

Figure 6: Generative samples from GMMN-D and GMMN-C with training batch size B = 1024.

14

