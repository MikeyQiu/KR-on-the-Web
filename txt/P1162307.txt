Multiscale sequence modeling with a learned dictionary

Bart van Merri¨enboer
MILA

Amartya Sanyal
IIT Kanpur

Hugo Larochelle
Google Brain

Yoshua Bengio
MILA, CIFAR Fellow

7
1
0
2
 
l
u
J
 

5

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
6
7
0
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

We propose a generalization of neural network
sequence models.
Instead of predicting one
symbol at a time, our multi-scale model makes
predictions over multiple, potentially overlap-
ping multi-symbol tokens. A variation of the
byte-pair encoding (BPE) compression algo-
rithm is used to learn the dictionary of to-
kens that the model is trained with. When
applied to language modelling, our model has
the ﬂexibility of character-level models while
maintaining many of the performance beneﬁts
of word-level models. Our experiments show
that this model performs better than a regular
LSTM on language modeling tasks, especially
for smaller models.

1 Introduction

Sequence modeling is the task of learning a probability
distribution over a set of ﬁnite sequences. We consider
sequences of elements drawn from a ﬁnite set of sym-
bols, st ∈ Σ. In the context of language modeling this
is the probability mass function of a set of strings given
some alphabet of characters.

p (s1 . . . sn) ,

st ∈ Σ

(1)

in

approaches

language modeling

fol-
Most
low (Shannon, 1948) and model
sequences as
acyclic Markov chains, exploiting the fact that natural
languages have strong temporal dependencies (see
ﬁgure 1).

n

Y
t=1

p (s1 . . . sn) ≈

p (st | s1 . . . st−1) ,

st ∈ Σ (2)

Recurrent neural networks (RNNs) can be used to
efﬁciently model these Markov chains (Mikolov, 2012;
Mikolov et al., 2010). The hidden state of the net-
work can encode the subsequence that is conditioned
on (s1 . . . st−1) using constant memory. Let xt be an
embedding of symbol st, then an RNN model is of the
form

ht = f (xt, ht−1)
yt = g(ht)

(3)
(4)

Typically the function f is a long-short term memory
(LSTM) unit or gated recurrent unit (GRU), and g is a
linear transformation followed by a softmax activation.

1.1 Tokenization

Natural language is naturally represented as a sequence
of characters (Sutskever et al., 2011; Mikolov, 2012;
Graves, 2013). However, in practice text is usually ‘to-
kenized’ and modeled as a sequence of words instead
of characters (see ﬁgure 2). Word-level models display
superior performance to character-level models, which
we argue can be explained by several factors.

1.1.1 Training difﬁculties
Tokenization reduces the length of the sequences to
model. Learning long-term dependencies with RNNs
can be difﬁcult (Pascanu et al., 2013; Bengio et al.,
1994; Hochreiter, 1991), and in natural language de-
pendencies such as agreement in number or case can
span tens or even hundreds of characters.

Furthermore, the softmax generally used in neural
networks can never assign a probability of exactly 1 to
the correct token. The product of many probabilities
less than 1 in equation 2 causes the probability of a se-
quence to decay quickly as it grows longer. To counter-
act this behaviour a network will quickly learn to fully
saturate the softmax. However, this slows down learn-
ing (LeCun et al., 1998).

1.1.2 Compositionality
In the context of natural language it can be argued that
character-level and word-level language modeling are
fundamentally different.

Word-level models rely on the principle of com-
positionality (Szab, 2013) and can learn to rep-
resent
the meaning of a sentence by combin-
ing the semantic representations of its constituent
words (Mitchell and Lapata, 2008). This mapping is
arguably ‘smooth’: If words have similar semantics,
the sentences they form are likely to have a similar
meaning and representation as well.

A character-level model performs a second, differ-
ent task: Mapping a sequence of characters to the se-
mantic representation of a morpheme. The principle
of compositionality does not apply in this case. It is
a lookup operation which is entirely non-linear: ‘the’,

G

H

G

H

ε

k

l

m

e

f

Hek

Helk

k

l

He

Hel

Hell

Hello

Hello·

·

Hf

Hem

Hellp

Figure 1: Diagrammatic representation of a character-level language model as a Markov chain. The probability
of the string “Hello” is the probability of reaching the absorbing state “Hello·” starting from the empty string (ε),
where · is a special end-of-string (EOS) token. Each state transition is analogous to concatenating a token from the
dictionary Σ to the state.

o

p

world

·

there

Hello
world

Hello·

Hello
there

ε

Hallo

Hello

Hi

Hallo

Hello

Hi

Figure 2: A word-level language model, requiring fewer transitions in order to reach the state “Hello·”. The state
space is signiﬁcantly reduced, which means that many strings cannot be modeled.

‘then’ and ‘they’ are entirely unrelated. It is possible
that character-level RNN models perform worse than
word-level models because RNNs are ill-suited to per-
form this lookup operation. It is feasible that other ap-
plication domains have a similar hierarchical structure
in their sequential data e.g. sequence motifs in genet-
ics.

2 Multi-scale sequence modeling

In typical sequence models, we model the likelihood
of the next symbol individually. A single symbol (e.g.
a character) is selected from a dictionary of mutually
exclusive options.
In this paper we propose a more
general setting in which at each step we make predic-
tions over multi-symbol tokens, potentially multiple of
which are correct if they share a preﬁx (see ﬁgure 3).

Formally, given a set of symbols Σ, consider a dic-
tionary of multi-symbol tokens T , where Σ ⊂ T . (This
condition guarantees that the space of sequences we
can model is the same as for typical symbol-level mod-
els.) Let |ti| denote the number of symbols in token
ti. The Markov chain (see ﬁgure 3) for a sequence

s1 . . . sn, st ∈ Σ, can be modeled using an RNN as
follows:

ht =

f (xi, ht−|ti|),

1
|Tt| X
Tt

Tt = {ti : ti ∈ T, ti = st−|ti|+1 . . . st}

yt = g(ht)

(5)

(6)
(7)

where xi is an embedding of token ti. Note that a typi-
cal RNN model (e.g. a character-level language model)
is a special case of this model where T = Σ.

The likelihood of this model is tractable and can be
easily calculated using dynamic programming. We can
optimize this likelihood directly using gradient descent.
This is similar to the forward-backward algorithm used
in hidden Markov models and connectionist temporal
classiﬁcation (CTC) (Graves et al., 2006).

p (s1 . . . st) =

p

ti|s1 . . . st−|ti|(cid:1)
(cid:0)

p

X
Tt

s1 . . . st−|ti|(cid:1)
(cid:0)
(8)

Hi

H

i

e

Hi

He

Hell

Help

lo

p

l

Hellos

s

·

o·

H

ε

l

o

Hel

Hell

Hello

Hello·

Figure 3: The multi-scale model allows multiple outgoing transitions, maintaining the ﬂexibility of a character-
level model while incorporating many of the beneﬁts of word-level models. Any path through the Markov chain
from ε to Hello· is a segmentation of the string Hello using the tokens in the dictionary. The probability of the state
Hello· is the sum of the likelihood of each segmentation. When modeled using an RNN, each state corresponds to
a hidden state ht, and each arrow corresponds to the application of the transition function f which takes inputs ht
and token embedding xi.

This approach can be used in general for the mod-
eling of Markov chains without cycles in the case of a
ﬁnite set of transitions (even if the state space is inﬁ-
nite). The recurrent neural network predicts the transi-
tion probabilities over this ﬁnite set of transitions for
each state using a representation of the state and a
learned representation of each transition. We believe
this is a novel approach to modelling acyclical Markov
chains using RNNs.

In this work we consider a multiscale generalization
of LSTM networks. For transition functions, f , with
multiple operations we can perform the averaging at
any point. We choose to average the cell states and out-
put gates. Note that performing the averaging earlier
on reduces the amount of computation.

fi
ii
oi
gi













= Whht−|ti| + Wxxi + b

ct =

1
N

N

X
i=1

σ(fi) ⊙ ct−|ti| + σ(ii) ⊙ tanh(gi)
(cid:1)
(cid:0)

ht = σ(

oi) ⊙ tanh(ct)

1
N

N

X
i=1

2.1 Model characteristics

The computational complexity of a regular RNN model
grows as a function of the sequence length, O(T ). The
multiscale model’s complexity instead grows as a func-
tion of the number of arcs. The number of arcs in a
sequence is theoretically bounded by T (T +1)
, but in
practice it grows sublinearly with the size of the dictio-
nary. For example, for a dictionary with 16384 tokens
we ﬁnd an average of 2.7 arcs per time step for the text8
dataset.

2

It should be noted that the computation of arcs can be
entirely parallelized, so on a parallel computer (e.g. a

GPU) the span (depth) of the computation is equivalent
to that of a normal RNN, O(T ).

During training time the memory usage of an RNN
model grows as O(T ) because of the need to store
the hidden states for the backward propagation. The
multiscale model’s memory usage grows the same and
does not depend on the number of arcs, since the av-
erages (see formula 5) can be calculated by accumu-
lating values in-place. The need to keep token embed-
dings in memory means that the memory usage grows
as O(T + D) where D is the dictionary size.

In conclusion, the multiscale model is both compu-
tationally and memory efﬁcient. On a parallel architec-
ture it has a the same computational complexity as a
regular RNN and only requires a small amount of extra
memory in order to store the token embeddings.

2.2 Dictionary learning

The formulation of our multi-scale model requires the
construction of a dictionary of multi-symbol tokens.
Heuristically speaking, we would simplify our model-
ing problem if we construct a dictionary which allows
each sequence to be segmented into a short sequence
of tokens, minimizing the shortest path length through
the graph (see ﬁgure 3).

In natural language processing, word-level models
usually construct a dictionary by splitting strings on
whitespace and punctuation. The dictionary then con-
sists of the N most frequent tokens, with the rest of
the words replaced with a special out-of-vocabulary
(OOV) token. Note that many other application do-
mains (e.g. modeling DNA sequences) don’t have any
straightforward heuristics to tokenize the data.

Even in language modeling this type of tokeniza-
tion is problematic for a variety of reasons. The num-
ber of words in natural language is effectively inﬁ-
nite for synthetic languages, which means there will
always be OOV tokens. Furthermore, it is arguably
arbitrary from a linguistic perspective. Whereas En-
glish is a rather isolating language, with ∼1.67 mor-

phemes per word (Greenberg, 1960) on average, syn-
thetic languages such as Turkish or Eskimo have ∼2.33
and ∼3.70 morphemes per word respectively. For
example, the Dutch word meervoudigepersoonlijkhei-
dsstoornis (multiple personality disorder) contains 10
morphemes. For these types of languages, we might
want to consider a tokenization that contains subword
units. On the other hand, for highly isolating languages
we might want to model several words as a single token
e.g. chng ti, Vietnamese for ‘we’.

2.3 Dictionary coders

Instead of arbitrarily splitting on whitespace, a more
principled approach is to to ‘learn’ the tokens to be
modeled. Here we propose an approach which is
grounded in text compression and inspired by the
byte-pair encoding (BPE) algorithm. BPE has been
used in the domain of neural machine translation to
learn subword units, reducing the number of OOV to-
kens (Sennrich et al., 2015).

Dictionary coder algorithms like BPE learn dic-
tionaries of tokens with the purpose of representing
strings with as few tokens as possible, increasing the
level of compression. This reduces the effective depth
of the unrolled RNN network (i.e.
the shortest path
through the graph in ﬁgure 3), which is a reasonable
learning objective for our dictionary.

Algorithm 1 Adapted byte-pair encoding algorithm
Require: Tmax, T = {s1, . . . , sm}, s = si1 , . . . , sin

{Initial dictionary T = Σ, string s}
while true do

j, k ←[ arg maxj,k paircount(sj, sk)
snew ←[ [sj|sk],
if |T | = Tmax then

T ←[ T

{snew}

S

break

end if
Substitute each occurrence of sj, sk in s with snew
for all l ∈ {l : count(sl) < count(snew)} do

T ←[ T \ {sl}
Substitute each occurrence of sl in s with so, sp
s.t. [so|sp] = sl

end for
end while
return T

# Token
and·
1
a·
2
the·
3
s·
7
of·the·
8
in·the·
11
ed·
12
ing·
17
ation·
65
people·
239
245 man
296
525
540
565
608
1468
2727

ed·by·the·
external·links·
at·the·end·of·the·
at·the·university·of·
united·states·
in·the·united·states·
one·of·the·most·

Table 1: A sample from the tokens in the dictionary
of size 8192 constructed using the text8 dataset by our
adapted BPE algorithm. Spaces are visualized with the
· character. The most common tokens are similar to
the ones traditionally found in word-level models e.g.
‘and ’, ‘the ’, and ‘a ’. The dictionary also contains
common sufﬁxes such as ‘s ’ (for plural nouns and third
person singular verbs), ‘ing ’ (for gerunds and verbal
actions), and ‘ed ’ (for adjectives, past tenses and past
participles), as well as multi-word tokens e.g. ‘of the’,
‘and the’, ‘in the’, etc. and longer phrases.

Our implementation of this algorithm uses a bit array
of the size of the input data, where each element sig-
niﬁes whether the corresponding character is merged
with the subsequent character. We maintain two d-ary
heaps of tokens and token pairs sorted by their fre-
quency. The algorithm proceeds by repeatedly popping
the most common pair from the heap and searching the
text and bit array for occurences. If an occurence is
found, the bit array is updated to represent the merge,
and the d-ary heaps are updated to reﬂect the new to-
ken and pair counts. This requires a minimum of D
passes over the data to construct a dictionary of size D
but uses a relatively small amount of memory.

3 Experiments

3.1 Implementation

Regular BPE starts with a dictionary of characters
and consecutively replaces the most frequent pairs of
tokens with a single new token, until a given dictio-
nary size Tmax is reached. We extend the algorithm
by reversing the merger of two tokens whenever a to-
ken becomes too rare. As a motivating example con-
sider the string abcabcabc. . . . In this case a and b are
merged into ab, followed by a merger between ab and
c into abc. Our extension makes sure that the token ab,
which now occurs zero times, is removed from the dic-
tionary. This removal prevents us from wasting space
in the dictionary on rare tokens.

The irregular, data-dependent access pattern makes the
multiscale model difﬁcult to implement in a performant
manner using existing GPU-accelerated deep learning
frameworks such as Theano and Torch. Hence, experi-
ments were performed with a hand-written CUDA im-
plementation of both the model (including layer nor-
malization) and the dynamic programming forward and
backward sweeps. Our implementation was able to ex-
ploit the parallelism inherent in the model, fully uti-
lizing the K20 and K80 NVidia GPUs that the models
were trained on.

Training curves

Regular LSTM
Multiscale LSTM

r
e
t
c
a
r
a
h
c

r
e
p

s
t
i

B

2.5

3

2

1.5

1

0

1,000

2,000

3,000

4,000

5,000

Updates

Figure 4: Training curves of the regular and multiscale
LSTM. Note how the multiscale LSTM training loss
starts lower because of the learned dictionary, which
shows that the use of compression algorithms for dic-
tionary construction is effective.

3.2 Penn Treebank

We evaluate the multiscale model on the widely used
Penn Treebank dataset using the training, validation
and test split proposed by Mikolov (Mikolov et al.,
2012). Our baseline is an LSTM with 1024 hidden
units and embeddings of dimension 512 trained with
truncated backpropagation-through-time (TBPTT) on
sequences of length 400. These optimal values were
found using a grid search. We train using the
Adam (Kingma and Ba, 2014) optimizer (learning rate
of 0.001) and to increase convergence speed we use
layer normalization (Ba et al., 2016).

Our baseline model achieves a score of 1.43 bits per
character. The multiscale model is trained using the ex-
act same conﬁguration but using a dictionary of 2048
tokens. It achieves a test score of 1.42 bits per char-
acter. Note that the multiscale models improvements
are orthogonal to what can be achieved by straightfor-
wardly increasing the capacity of the network. The
regular LSTM networks with more than 1024 units
showed decreased performance in our experiments due
to overﬁtting.

Moreover, our network is able to achieve better per-
formance with far fewer parameters. The multiscale
model with 512 hidden units, embeddings of size 256,
and 2048 tokens has 51% fewer parameters compared
to our baseline, but achieves a score of 1.41 bpc, com-
pared to 1.48 bpc for a regular LSTM with the same
embedding and hidden state size.

3.3 Text8

Text8 (Mahoney, 2009) is a text dataset of 100 million
characters built from the English Wikipedia. The char-
acters are limited to the 26-letter alphabet and spaces.
We use the traditional split of 90, 5 and 5 million for
the training, validation and test set respectively.

We compare the performance of our multi-scale
language
model with a single-layer character-level
model with 2048 units. The same training procedure
as described in the previous subsection is used. The
baseline achieves a score of 1.45 bits per character. The
multiscale model improves on this performance using
a dictionary of 16,384 tokens, achieving a test score of
1.41 bits per character.

4 Related work

In language modeling a variety of approaches have at-
tempted to bridge the gap between character and word-
level models. The approach in (Kim et al., 2015) is to
apply a convolutional neural network (CNN) with tem-
poral pooling over the constituent characters of a word.
The CNN ﬁlters of this network can be interpreted as
character n-gram detectors. The output of this network
is used as the input to an LSTM network which models
the word-level dynamics. Note that the resulting model
still requires information about word-boundaries.

Other approaches use multi-scale RNN architec-
tures. The model in (Bojanowski et al., 2015) uses both
a word-level and character-level RNN, the latter being
conditioned on the former. This model too still re-
quires knowledge of word boundaries. The approach
in (Chung et al., 2016) does not require word bound-
aries, and instead uses the straight-through estimator
to learn the latent hierarchical structure directly. Their
model does not learn separate embeddings for the seg-
ments however, and can only output a single character
at a time.

The latent sequence decomposition (LSD) model in-
troduced in (Chan et al., 2016) is related to our multi-
scale model, and was shown to improve performance
on a speech recognition task.
Instead of using com-
pression algorithms the LSD model uses a dictionary
of all possible n-grams. Since the number of n-grams
grows exponentially, this limits the the dictionary to
very short tokens only. The LSD model uses a reg-
ular RNN which is trained on a set of sampled seg-
mentations instead of averaging the hidden states using
dynamic programming. This complicates training and
makes the likelihood of the model intractable. The re-
cent Gram-CTC model (Liu et al., 2017) is also related
and does use dynamic programming but still uses a dic-
tionary of character n-grams.
Although our model

is competitive with recent
methods such as MI-LSTM (Wu et al., 2016) and td-
LSTM (Zhang et al., 2016), which achieve 1.44 and
1.63 bits per character on the text8 dataset respectively,
other recent models such as HM-LSTM (Chung et al.,
2016) have achieved lower scores (1.29 bpc). Since
many of the LSTM variations in the literature can be
extended to the multiscale model, we believe it is pos-
sible to improve the performance of multiscale models
further in the future. Similarly, deeper multi-layer ex-
tensions to our model are feasible.

Samples

the ·independ·ence ·in the ·third quarter ·the ·chief ·ex·port ·stock ·prices ·for the ·year· ·and ·into the ·disa·ster·l·and
gains ·so ·on the ·economy ·because ·in addition ·to a ·compl·ex ·closed ·higher ·comm·ut·e ·pres·sure ·of ·his ·company
meeting ·in ·the ·trust ·is ·expected to ·be ·an·ticip·ated ·$· ·ofﬁc·es ·during the ·past
actu·ally ·have ·spok·es·man ·with ·hous·ing ·their ·junk ·bond ·due ·$ N billion from ·most ·important ·next ·day ·at N

Table 2: Samples from the multiscale model trained on Penn Treebank. Token boundaries are marked with the
· symbol. The samples show the model’s ability to model a sentence by predicting entire words or phrases (‘in
addition’, ‘into the’) at a time, while also being able to exploit subword structure (‘comm·ut·e’) and maintaing the
ﬂexibility of character language models to output unseen words (‘disa·ster·l·and’).

5 Discussion

References

Through arithmetic encoding it can be shown that mod-
eling data is equivalent to compressing it (Mahoney,
1999).
Using neural networks to improve upon
text compression algorithms is a common tech-
nique (Mahoney, 2000), but as far as we are aware
the reverse has not been researched. One can see our
model as a mix between non-parametric and parametric
approaches: As discussed in section 1.1.2, character-
level models learn a parametric mapping from con-
stituent characters to semantic representations of mor-
phemes. Word-level models avoid learning this highly
non-linear function by constructing a dictionary and
learning a representation for each word, which is non-
parametric. Our multiscale model generalizes this ap-
proach, combining non-parametric dictionary coders
and parametric RNN models. The size of the dictio-
nary allows us to choose the balance between the two
approaches.

A rough parallel can be drawn between our multi-
scale approach for sequences and superpixels in the
computer vision domain (Ren and Malik, 2003), where
pixels are clustered in order to improve computational
and representational efﬁciency

The multiscale model can also be related to work
on text segmentation. The hierarchical Pitman-Yor lan-
guage model in (Mochihashi et al., 2009) learns how to
segment a string of characters into words, while simul-
taneously learning a word-level n-gram model. Each
path through the graph of the multiscale model (ﬁgure
3) can be considered a single segmentation of the text,
with the likelihood of the string being the marginaliza-
tion over all possible segmentations.

A large number of combinations of data compres-
sion and neural network sequence modeling are still
open to investigation. Besides BPE, there are many
other dictionary coder algorithms out there. Another
consideration would be to learn the dictionary and the
sequence model jointly. Subsequently, a variety of neu-
ral network models can conceivably be adapted to work
with the multi-scale representation of text used in this
paper e.g. bag-of-words (BOW) models could be re-
placed with bag-of-token models instead, similar to the
approach in (Bojanowski et al., 2016) which uses char-
acter n-grams.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
arXiv preprint

ton. 2016. Layer normalization.
arXiv:1607.06450 .

Y. Bengio, P. Simard, and P. Frasconi. 1994. Learn-
ing long-term dependencies with gradient descent is
difﬁcult. IEEE Transactions on Neural Nets pages
157–166.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
arXiv preprint
tors with subword information.
arXiv:1607.04606 .

Piotr Bojanowski, Armand Joulin, and Tomas Mikolov.
2015. Alternative structures for character-level rnns.
arXiv preprint arXiv:1511.06303 .

William Chan, Yu Zhang, Quoc Le, and Navdeep
Jaitly. 2016. Latent sequence decompositions. arXiv
preprint arXiv:1610.03035 .

Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
2016. Hierarchical multiscale recurrent neural net-
works. arXiv preprint arXiv:1609.01704 .

A. Graves, S. Fernndez, F. Gomez, and J. Schmid-
huber. 2006. Connectionist temporal classiﬁcation:
Labelling unsegmented sequence data with recurrent
In ICML’2006. Pittsburgh, USA,
neural networks.
pages 369–376.

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
report,
Technical

Joseph H Greenberg. 1960. A quantitative approach
Inter-
to the morphological typology of language.
national journal of American linguistics 26(3):178–
194.

S.

Hochreiter.

1991.

Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis,
http://www7.informatik.tu-muenchen.de/ Ehochreit.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2015. Character-aware neural language
models. arXiv preprint arXiv:1508.06615 .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Ilya Sutskever, James Martens, and Geoffrey E. Hin-
ton. 2011. Generating text with recurrent neural net-
works. In ICML’2011. pages 1017–1024.

Zoltn Gendler Szab. 2013. Compositionality. In Ed-
ward N. Zalta, editor, The Stanford Encyclopedia of
Philosophy. Fall 2013 edition.

Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua
Bengio, and Ruslan R Salakhutdinov. 2016. On
multiplicative integration with recurrent neural net-
works. In Advances In Neural Information Process-
ing Systems. pages 2856–2864.

the 2016 Conference of

Xingxing Zhang, Liang Lu, and Mirella Lapata. 2016.
Top-down tree long short-term memory networks.
the
In Proceedings of
the Association for
North American Chapter of
Computational Linguistics:
Human Language
Technologies. Association for Computational Lin-
guistics, San Diego, California, pages 310–320.
http://www.aclweb.org/anthology/N16-1035.

Yann LeCun, L´eon Bottou, Genevieve B. Orr, and
Klaus-Robert M¨uller. 1998. Efﬁcient backprop. In
Neural Networks, Tricks of the Trade, Springer Ver-
lag, Lecture Notes in Computer Science LNCS 1524.

Hairong Liu, Zhenyao Zhu, Xiangang Li, and Sanjeev
Satheesh. 2017. Gram-ctc: Automatic unit selec-
tion and target decomposition for sequence labelling.
arXiv preprint arXiv:1703.00096 .

Matt Mahoney. 2009. Large text compression bench-
mark. URL: http://www. mattmahoney. net/text/text.
html .

Matthew V Mahoney. 1999. Text compression as a test
for artiﬁcial intelligence. In AAAI/IAAI. page 970.

Matthew V Mahoney. 2000. Fast text compression
with neural networks. In FLAIRS Conference. pages
230–234.

Tomas Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.

Tomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent
In INTER-
neural network based language model.
SPEECH. volume 2, page 3.

Tom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
Subword language modeling with neu-
2012.
(http://www. ﬁt. vutbr.
ral networks.
cz/imikolov/rnnlm/char. pdf) .

preprint

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
In ACL. pages

models of semantic composition.
236–244.

Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmenta-
tion with nested pitman-yor language modeling. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1. Association for
Computational Linguistics, pages 100–108.

Tomas
Bengio.

Pascanu,
Yoshua

Razvan
and
On the difﬁculty of training recurrent neural networks.
In Proceedings of the 30th International Confer-
ence on Machine Learning (ICML’13). ACM.
http://icml.cc/2013/.

Mikolov,
2013.

Xiaofeng Ren and Jitendra Malik. 2003. Learning a
classiﬁcation model for segmentation. In ICCV. vol-
ume 1, pages 10–17.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909 .

Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal
27(3):379–423.

