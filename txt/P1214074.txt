9
1
0
2
 
y
a
M
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
5
1
5
7
0
.
5
0
9
1
:
v
i
X
r
a

Learning Perspective Undistortion of Portraits

Yajie Zhao1,*, Zeng Huang1,2,*, Tianye Li1,2, Weikai Chen1, Chloe LeGendre1,2, Xinglei Ren1, Jun
Xing1, Ari Shapiro1, and Hao Li1,2,3

1USC Institute for Creative Technologies
2University of Southern California
3Pinscreen

Figure 1: We propose a learning-based method to remove perspective distortion from portraits. For a subject with two different facial
expressions, we show input photos (b) (e), our undistortion results (c) (f), and reference images (d) (g) captured simultaneously using a
beam splitter rig (a). Our approach handles even extreme perspective distortions.

Abstract

1. Introduction

Near-range portrait photographs often contain perspec-
tive distortion artifacts that bias human perception and
challenge both facial recognition and reconstruction tech-
niques. We present the ﬁrst deep learning based approach
to remove such artifacts from unconstrained portraits. In
contrast to the previous state-of-the-art approach [25], our
method handles even portraits with extreme perspective dis-
tortion, as we avoid the inaccurate and error-prone step of
ﬁrst ﬁtting a 3D face model. Instead, we predict a distor-
tion correction ﬂow map that encodes a per-pixel displace-
ment that removes distortion artifacts when applied to the
input image. Our method also automatically infers missing
facial features, i.e. occluded ears caused by strong perspec-
tive distortion, with coherent details. We demonstrate that
our approach signiﬁcantly outperforms the previous state-
of-the-art [25] both qualitatively and quantitatively, partic-
ularly for portraits with extreme perspective distortion or
facial expressions. We further show that our technique ben-
eﬁts a number of fundamental tasks, signiﬁcantly improving
the accuracy of both face recognition and 3D reconstruc-
tion and enables a novel camera calibration technique from
a single portrait. Moreover, we also build the ﬁrst perspec-
tive portrait database with a large diversity in identities,
expression and poses.

the

distance

Perspective distortion artifacts are often observed in
portrait photographs, in part due to the popularity of the
“selﬁe” image captured at a near-range distance. The
inset images, where a person is photographed from dis-
tances of 160cm and 25cm, demonstrate these artifacts.
object-to-
When
camera
is
comparable to the size of
a human head, as in the
25cm distance example,
there is a large propor-
tional difference between
the camera-to-nose dis-
tance and camera-to-ear
distance. This difference
creates a face with unusual proportions, with the nose and
eyes appearing larger and the ears vanishing all together
[61].

Perspective distortion in portraits not only inﬂuences the
way humans perceive one another [10], but also greatly im-
pairs a number of computer vision-based tasks, such as face
veriﬁcation and landmark detection. Prior research [41, 42]
has demonstrated that face recognition is strongly compro-
mised by the perspective distortion of facial features. Ad-
ditionally, 3D face reconstruction from such portraits is

1

highly inaccurate, as geometry ﬁtting starts from biased fa-
cial landmarks and distorted textures.

Correcting perspective distortion in portrait photography
is a largely unstudied topic. Recently, Fried et al. [25] in-
vestigated a related problem, aiming to manipulate the rel-
ative pose and distance between a camera and a subject in a
given portrait. Towards this goal, they ﬁt a full perspective
camera and parametric 3D head model to the input image
and performed 2D warping according to the desired change
in 3D. However, the technique relied on a potentially inac-
curate 3D reconstruction from facial landmarks biased by
perspective distortion. Furthermore, if the 3D face model
ﬁtting step failed, as it could for extreme perspective distor-
tion or dramatic facial expressions, so would their broader
pose manipulation method. In contrast, our approach does
not rely on model ﬁtting from distorted inputs and thus can
handle even these challenging inputs. Our GAN-based syn-
thesis approach also enables high-ﬁdelity inference of any
occluded features, not considered by Fried et al. [25].

In our approach, we propose a cascaded network that
maps a near-range portrait with perspective distortion to its
distortion-free counterpart at a canonical distance of 1.6m
(although any distance between 1.4m ∼ 2m could be used
as the target distance for good portraits). Our cascaded net-
work includes a distortion correction ﬂow network and a
completion network. Our distortion correction ﬂow method
encodes a per-pixel displacement, maintaining the origi-
nal image’s resolution and its high frequency details in the
output. However, as near-range portraits often suffer from
signiﬁcant perspective occlusions, ﬂowing individual pixels
often does not yield a complete ﬁnal image. Thus, the com-
pletion network inpaints any missing features. A ﬁnal tex-
ture blending step combines the face from the completion
network and the warped output from the distortion correc-
tion network. As the possible range of per-pixel ﬂow values
vary by camera distance, we ﬁrst train a camera distance
prediction network, and feed this prediction along with the
input portrait to the distortion correction network.

Training our proposed networks requires a large corpus
of paired portraits with and without perspective distortion.
However, to the best of our knowledge, no previously ex-
isting dataset is suited for this task. As such, we construct
the ﬁrst portrait dataset rendered from 3D head models with
large variations in camera distance, head pose, subject iden-
tity, and illumination. To visually and numerically evalu-
ate the effectiveness of our approach on real portrait pho-
tographs, we also design a beam-splitter photography sys-
tem (see Teaser) to capture portrait pairs of real subjects
simultaneously on the same optical axis, eliminating differ-
ences in poses, expressions and lighting conditions.

Experimental results demonstrate that our approach re-
moves a wide range of perspective distortion artifacts (e.g.,
increased nose size, squeezed face, etc), and even restores

missing facial features like ears or the rim of the face. We
show that our approach signiﬁcantly outperforms Fried et
al. [25] both qualitatively and quantitatively for a synthetic
dataset, constrained portraits, and unconstrained portraits
from the Internet. We also show that our proposed face
undistortion technique, when applied as a pre-processing
step, improves a wide range of fundamental tasks in com-
puter vision and computer graphics, including face recogni-
tion/veriﬁcation, landmark detection on near-range portraits
(such as head mounted cameras in visual effects), and 3D
face model reconstruction, which can help 3D avatar cre-
ation and the generation of 3D photos (Section 6.3). Ad-
ditionally, our novel camera distance prediction provides
accurate camera calibration from a single portrait.

Our main contributions can be summarized as follows:

• The ﬁrst deep learning based method to automatically
remove perspective distortion from an unconstrained
near-range portrait, beneﬁting a wide range of funda-
mental tasks in computer vision and graphics.

• A novel and accurate camera calibration approach that
only requires a single near-range portrait as input.

• A new perspective portrait database for face undistor-
tion with a wide range of subject identities, head poses,
camera distances, and lighting conditions.

2. Related Work

Face Modeling. We refer the reader to [46] for a com-
prehensive overview and introduction to the modeling of
digital faces. With advances in 3D scanning and sens-
ing technologies, sophisticated laboratory capture systems
[6, 7, 9, 23, 26, 40, 44, 62] have been developed for high-
quality face reconstruction. However, 3D face geometry
reconstruction from a single unconstrained image remains
challenging. The seminal work of Blanz and Vetter [8] pro-
posed a PCA-based morphable model, which laid the foun-
dation for modern image-based 3D face modeling and in-
spired numerous extensions including face modeling from
internet pictures [35], multi-view stereo [3], and reconstruc-
tion based on shading cues [36]. To better capture a vari-
ety of identities and facial expressions, the multi-linear face
models [60] and the FACS-based blendshapes [12] were
later proposed. When reconstructing a 3D face from im-
ages, sparse 2D facial landmarks [20, 21, 52, 65] are widely
used for a robust initialization. Shape regressions have
been exploited in the state-of-the-art landmark detection ap-
proaches [13, 34, 48] to achieve impressive accuracy.

Due to the low dimensionality and effectiveness of mor-
phable models in representing facial geometry, there have
been signiﬁcant recent advances in single-view face recon-
struction [56, 49, 38, 55, 30]. However, for near-range por-
trait photos, the perspective distortion of facial features may

lead to erroneous reconstructions even when using the state-
of-the-art techniques. Therefore, portrait perspective undis-
tortion must be considered as a part of a pipeline for accu-
rately modeling facial geometry.

using vanishing points from a single scene photos with hori-
zontal lines. To the best of our knowledge, our method is the
ﬁrst to estimate camera parameters from a single portrait.

Face Normalization. Unconstrained photographs often
include occlusions, non-frontal views, perspective distor-
tion, and even extreme poses, which introduce a myriad of
challenges for face recognition and reconstruction. How-
ever, many prior works [27, 67, 50, 31] only focused on
normalizing head pose. Hassner et al. [27] “frontalized” a
face from an input image by estimating the intrinsic camera
matrix given a ﬁxed 3D template model. Cole et al. [17] in-
troduced a neural network that mapped an unconstrained fa-
cial image to a front-facing image with a neutral expression.
Huang et al. [31] used a generative model to synthesize an
identity-preserving frontal view from a proﬁle. Bas et al. [5]
proposed an approach for ﬁtting a 3D morphable model to
2D landmarks or contours under either orthographic or per-
spective projection.

Psychological research suggests a direct connection be-
tween camera distance and human portrait perception.
Bryan et al. [10] showed that there is an “optimal dis-
tance” at which portraits should be taken. Cooper et al. [19]
showed that the 50mm lens is most suitable for photograph-
ing an undistorted facial image. Valente et al. [58] proposed
to model perspective distortion as a one-parameter family
of warping functions with known focal length. Most related
to our work, Fried et al. [25] investigated the problem of
editing the facial appearance by manipulating the distance
between a virtual camera and a reconstructed head model.
Though this technique corrected some mild perspective dis-
tortion, it was not designed to handle extreme distortions, as
it relied on a 3D face ﬁtting step. In contrast, our technique
requires no shape prior and therefore can generate undis-
torted facial images even from highly distorted inputs.

Image-based Camera Calibration. Camera calibration
is an essential prerequisite for extracting precise and re-
liable 3D metric information from images. We refer the
reader to [47, 51, 45] for a survey of such techniques. The
state-of-the-art calibration methods mainly require a phys-
ical target such as checkerboard pattern [57, 70] or circu-
lar control points [15, 18, 28, 33, 22], used for locating
point correspondences. Flores et al. [24] proposed the ﬁrst
method to infer camera-to-subject distance from a single
image with a calibrated camera. Burgos-Artizzu et al. [11]
built the Caltech Multi-Distance Portraits Dataset (CMDP)
of portraits of a variety of subjects captured from seven dis-
tances. Many recent works directly estimate camera param-
eters using deep neural networks. PoseNet [37] proposed
an end-to-end solution for 6-DOF camera pose localization.
Others [63, 64, 29] proposed to extract camera parameters

3. Overview

The overview of our system is shown in Fig. 2. We
pre-process the input portraits with background segmenta-
tion, scaling, and spatial alignment (see appendix), and then
feed them to a camera distance prediction network to esti-
mates camera-to-subject distance. The estimated distance
and the portrait are fed into our cascaded network including
FlowNet, which predicts a distortion correction ﬂow map,
and CompletionNet, which inpaints any missing facial fea-
tures caused by perspective distortion. Perspective undis-
tortion is not a typical image-to-image translation problem,
because the input and output pixels are not spatially corre-
sponded. Thus, we factor this challenging problem into two
sub tasks: ﬁrst ﬁnding a per-pixel undistortion ﬂow map,
and then image completion via inpainting. In particular, the
vectorized ﬂow representation undistorts an input image at
its original resolution, preserving its high frequency details,
which would be challenging if using only generative image
synthesis techniques. In our cascaded architecture (Fig. 3),
CompletionNet is fed the warping result of FlowNet. We
provide details of FlowNet and CompletionNet in Sec. 4.
Finally, we combined the results of the two cascaded net-
works using the Laplacian blending [1] to synthesize high-
resolution details while maintaining plausible blending with
existing skin texture.

4. Portrait Undistortion

4.1. Camera Distance Prediction Network

Rather than regress directly from the input image to
the camera distance D, which is known to be challeng-
ing to train, we use a distance classiﬁer. We check if the
camera-to-subject distance of the input is larger than an
query distance d (cid:15) (17.4cm, 130cm)1. Our strategy learns
a continuous mapping from input images to the target dis-
tances. Given any query distance D and input image pair
(input, D), the output is a ﬂoating point number in the
range of 0 ∼ 1, which indicates the probability that the dis-
tance of the input image is greater than the query distance
D. As shown in Fig. 4, the vertical axis indicates the out-
put of our distance prediction network while the horizontal
axis is the query distance. To predict the distance, we lo-
cate the query distance with a network output of 0.5. With
our network denoted as φ, our network holds the transitivity
property that if d1 > d2, φ(input, d1) > φ(input, d2).

To train the camera distance network, we append the
value of log2 d as an additional channel of the input im-
117.4cm and 130cm camera-to-subject distances correspond to 14mm

and 105mm in 35mm equivalent focal length

Figure 2: The pipeline workﬂow and applications of our approach. The input portrait is ﬁrst segmented and scaled in the
preprocessing stage and then fed to a network consisting of three cascaded components. The FlowNet rectiﬁes the distorted
artifacts in the visible regions of input by predicting a distortion correction ﬂow map. The CompletionNet inpaints the missing
facial features due to the strong perspective distortions and obtains the completed image. The outcomes of two networks are
then scaled back to the original resolution and blended with high-ﬁdelity mean texture to restore ﬁne details.

Figure 3: Cascade Network Structure.

Figure 4: Illustration of Camera Distance Prediction Clas-
siﬁer. Green Curve and Red Curve are the response curves
of input a and b; d1 and d2 are the predicted distances of
input a and input b.

age and extract features from the input images using the
VGG-11 network [54], followed by a classic classiﬁer con-
sisting of fully connected layers. As training data, for
each of the training image with ground truth distance d, we
sample a set of log2 d using normal distribution log2 d ∼
N (log2 D, 0.52).

4.2. FlowNet

The FlowNet operates on the normalized input image
(512 × 512) A and estimates a correction forward ﬂow F
that rectiﬁes the facial distortions. However, due to the im-
mense range of possible perspective distortions, the correc-
tion displacement for portraits taken at different distances
will exhibit different distributions. Directly predicting such
high-dimensional per-pixel displacement is highly under-
constrained and often leads to inferior results (Figure 11).
To ensure more efﬁcient learning, we propose to attach the

estimated distance to the input of FlowNet in the similar
way as in Section 4.1.
Instead of directly attaching the
predicted number, we propose to classify these distances
into eight intervals2 and use the class label as the input to
FlowNet. The use of label will decrease the risk of accumu-
lation error from camera distance prediction network, be-
cause the accuracy of predicting a label is higher than ﬂoat-
ing number.

FlowNet takes A and distance label L as input, and it will
predict a forward ﬂow map FAB, which can be used to ob-
tain undistorted output B when applied to A. For each pixel
(x, y) of A, FAB encodes the translation vector (∆x, ∆y).
Denote the correspondence of (x, y) on B as (x(cid:48), y(cid:48)), then
(x(cid:48), y(cid:48)) = (x, y) + (∆x, ∆y). In FlowNet, we denote gener-
ator and discriminator as G and D separately. Then L1 loss
of ﬂow is as below:

LG = (cid:107)y − FAB(cid:107)1

(1)

In which y is the ground truth ﬂow. For the discriminator
loss, as forward ﬂow FAB is per-pixel correspondence to A
but not B, thus B will have holes, seams and discontinuities
which is hard to used in discriminator. To make the problem
more tractable, instead of applying discriminator on B, we
use the FAB to map B to A(cid:48) and use A and A(cid:48) as pairs for
discriminator on the condition of L.

LD = min
G
Ez∼pz(z)

Ex∼pdata(x)
max
D
(cid:2)log (1 − D(A(cid:48), L))(cid:3).

(cid:2)log D(A, L)(cid:3)+

(2)

where pdata(x) and pz(z) represent the distributions of
real data x(input image A domain) and noise variables z in

2The eight distance intervals are [23, 26), [26, 30), [30, 35), [35, 43),
[43, 62), [62, 105), [105, 168) and [168, +∞), which are measured in
centimeters.

the domain of A respectively. The discriminator will penal-
ize the joint conﬁguration in the space of A, which leads to
shaper results.

4.3. CompletionNet

The distortion-free result will then be fed into the Com-
pletionNet, which focuses on inpainting missing features
and ﬁlling the holes. Note that as trained on a vast number
of paired examples with large variations of camera distance,
CompletionNet has learned an adaptive mapping regarding
to varying distortion magnitude inputs.

4.4. Network Architecture

We employ a U-net structure with skip connections simi-
lar to [32] to both FlowNet and CompletionNet. There is not
direct correspondence between each pixel in the input and
those in the output. In the FlowNet, the L1 and GAN dis-
criminator loss are only computed within the segmentation
mask, leading the network to focus on correcting details that
only will be used in the ﬁnal output. In the CompletionNet,
as the output tends to cover more pixels than the input, dur-
ing training, we compute a new mask that denotes the novel
region compared to input and assign higher L1 loss weight
to this region. In implementation, we set the weight ratio of
losses computed inside and outside the mask as 5 : 1 for the
CompletionNet. We found that this modiﬁcations leads to
better inference performance while producing results with
more details.

Implementation Details. We train FlowNet and Com-
pletionNet separately using the Adam optimizer [39] with
learning rate 0.0002. All training was performed on an
NVIDIA GTX 1080 Ti graphics card. For both networks,
we set the weights of L1 and GAN loss as 10.0 and 1.0
respectively. As shown in Fig. 3, the generator in both net-
works uses the mirrored structure, in which both encoder
and decoder use 8-layer convolutional network. ReLU acti-
vation and batch normalization are used in all layers except
the input and output layers. The discriminator consists of
four convolutional layers and one fully connected layer.

5. Data Preparation

Training Data Acquisition and Rendering. As there is
no database of paired portraits with only perspective chang-
ing. We therefore generate a novel training dataset where
we can control the subject-to-camera distance, head pose,
illumination and ensure that the image differences are only
caused only by perspective distortion. Our synthetic train-
ing corpus is rendered from 3D head models acquired by
two scanning system. The ﬁrst is a light-stage[26, 44] scan-
ning system which produces pore-level 3D head models for
photo-real rendering. Limited by the post-processing time,

Figure 5: Training dataset. The left side triplet are the syn-
thetic images generated from BU-4DFE dataset[66]. From
the left to the right, the camera-to-subject distances are:
24cm, 30cm, 52cm, 160cm. The right side triplet are the
synthetic images rendered from high-resolution 3D model.
From left to the right, the camera-to-subject distances are:
22cm, 30cm, 53cm, 160cm.

cost and number of individuals that can be rapidly captured,
we also employ a second capture system engineered for
rapid throughput.
In total, we captured 15 subjects with
well deﬁned expressions in the high-ﬁdelity system, gener-
ating 307 individual meshes, and 200 additional subjects in
the rapid-throughput system.

We rendered synthetic portraits using a variety of camera
distances, head poses, and incident illumination conditions.
We sample distances distributed from 23cm to 160cm with
corresponding 35mm equivalent focal length from 18mm
to 128mm to ensure the same framing. We also randomly
sample candidate head poses in the range of -45◦ to +45◦
in pitch, yaw, and roll. We used 107 different image-based
environment for global illumination combined with point
lights. With the random combination of camera distances,
head poses, and illuminations, we totally generate 35,654
pairs of distroted/ undistored portraits along with forward
ﬂow.

17,000 additional portrait pairs warped from U-4DFE
dataset [66](38 females and 25 males subjects for training
while 20 females and 17 males for testing) are used to ex-
pand diversity of identities.

Test Data Acquisition. To demonstrate that our system
scales well to real-world portraits, we also devised a two-
camera beam splitter capture system that would enable si-
multaneous photography of a subject at two different dis-
tances. As shown in Teaser left, we setup a beam-splitter
(50% reﬂection, 50% transmission) at 45◦ along a metal
rail, such that a ﬁrst DSLR camera was set at the canoni-
cal ﬁxed distance of 1.6m from the subject, with a 180mm
prime lens to image the subject directly, while a second
DSLR camera was set at a variable distance of 23cm -
1.37m,
to image the subject’s reﬂection off the beam-
splitter with a 28mm zoom lens. With carefully geome-
try and color calibration, the two hardware-synced cameras
were able to capture nearly ground truth portraits pairs of
real subjects both with and without perspective distortions.
(More details can be found in the appendix).

Figure 6: Undistortion results of beam splitter system com-
pared to Fried et al. [25]. From left to the right : inputs,
results of Fried et al. [25], error maps of Fried et al. [25]
compared to references, ours results, error map of our re-
sults compared to references, reference.

6. Results and Experiments

6.1. Evaluations

Figure 7: Undistortion results of synthetic data generated
from BU-4DFE dataset compared to Fried et al. [25]. From
left to the right : inputs, results of Fried et al. [25], ours
results, ground truth, error map of Fried et al. [25] compared
to ground truth, error map of our results compared to ground
truth.

Face Image Undistortion.
In Fig. 6, Fig. 7 and Fig. 8 we
show the undistortion results of ours compared to Fried et
al.[25]. To better visualize the reconstruction error, we also
show the error map compared to groundtruth or references
in Fig. 6 and Fig. 7. We perform histogram equalization
before computing the error maps. Our results are visually
more accurate than Fried et al.[25] especially on the face

Figure 8: Evaluation and comparisons with Fried et al. [25]
on a variety of datasets with in the wild database. (a). in-
puts; (b). Fried et al. [25]; (c). Ours; (d). The Mixture of
(a) and (b) for better visualization of undistortion; (e). The
Mixture of (a) and (c); Shaded portraits indicate the failure
of Fried et al. [25].

boundaries. To numerically evaluate our undistortion ac-
curacy, we compare with Fried et al.[25] the average error
over 1000 synthetic pair from BU-4DFE dataset. With an

Figure 9: Distance prediction probability curve of three dif-
ferent input portraits with query distance sampled densely
along the whole range.

average intensity error of 0.39 we signiﬁcantly outperform
Fried et al.[25] which has an average intensity error of 1.28.
In Fig. 8, as we do not have references or ground truth, to
better visualize the motion of before-after undistortion, we
replace the g channel of input with g channel of result image
to amplify the difference.

Camera Parameter Estimation. Under the assumption
of same framing(keeping the head size in the same scale for
all the photos), the distance is equivalent to focal length by
multiplying a scalar. The scalar of converting distances to
focal length is s = 0.8, which means when taking photo
at 160cm camera-to-subject distance, to achieve the desired
framing in this paper, a 128mm focal length should be used.
Thus, as long as we can predict accurate distances of the in-
put photo, we can directly get the 35mm equivalent focal
length of that photo. We numerically evaluate the accuracy
of our Camera Distance Prediction network by testing with
1000 synthetic distorted portraits generated from BU-4DFE
dataset. The mean error of distance prediction is 8.2% with
a standard deviation of 0.083. We also evaluate the accu-
racy of labeling. As the intervals mentioned in Section 4.2
are successive, some of the images may lie on the fence of
two neighboring intervals. So we regard label prediction as
correct within its one step neighbor. Under this assumption,
the accuracy of labeling is 96.9% which insures input reli-
ability for the cascade networks. Fig. 9 shows the distance
prediction probability curve of three images. For each of
them we densely sampled query distances along the whole
distance range and and the classiﬁer results are monotoni-
cally changing. We tested on 1000 images and found that
on average the transitivity holds 98.83%.

6.2. Ablation Study

In Fig. 10, we compare the single network and proposed
cascade network. The results show that with a FlowNet
as prior, the recovered texture will be sharper especially

Figure 10: Ablation analysis on cascade network. In each
triplet, from left to the right: inputs, results of single image-
to-image network similar to [32], results of using cascade
network including FlowNet and CompletionNet.

Figure 11: Ablation analysis on attach label as a input to
FlowNet. In each triplet, from left to the right: inputs, re-
sults from the network without label channel, results of our
proposed network.

(a)

(b)

Figure 12: (a). Receiver operating characteristic (ROC)
curve for face veriﬁcation performance on raw input and
undistorted input using our method. Raw input (A,B) com-
pares to undistorted input (N (A),B); (b). Cumulative er-
ror curve for facial landmark detection performance given
unnormalized image and image normalized by our method.
Metric is measured in normalized mean error (NME).

on boundaries. Large holes and missing textures are more
likely to be inpainted properly. Fig. 11 demonstrates the ef-
fectiveness of our label channel introduced in FlowNet. The
results without label channel are more distorted compared
to the ones with label as inputs, especially the proportions
of noses, mouth regions and the face rims.

Figure 13: Comparing 3D face reconstruction from por-
traits, without and with our undistortion technique. (a) and
(c) are heavily distorted portraits and the 3D mesh ﬁtted
using the landmarks of the portraits. (b) and (d) are undis-
torted results of (a) and (c) with 3D reconstruction based on
them. Gray meshes show reconstructed facial geometry and
color-coded meshes show reconstruction error.

Input
Raw input (A, B)
Undistorted input (N (A), B)

mean
0.9137
0.9473

std
0.0090
0.0067

Table 1: Comparison of face veriﬁcation accuracy for im-
ages with and without our undistortion as pre-processing.
Accuracy is reported on random 10-folds of test data with
mean and standard deviation.

6.3. Applications

Face Veriﬁcation. Our facial undistortion technique can
improve the performance of face veriﬁcation, which we test
using the common face recognition system OpenFace [4].
We synthesized 6,976 positive (same identity) and 6,980
negative (different identity) pairs from BU-4DFE dataset
[66] as test data. We rendered one image A in a pair of
images (A,B) as a near-distance portrait with perspective
distortion; while we rendered B at the canonical distance
of 1.6m to minimize the distortion. This is the setting of
most face veriﬁcation security system, which retrieves the
database for the nearest neighbor. We evaluated face veriﬁ-
cation performance on raw data (A, B) and data (N (A), B)
and (N (A), N (B)) in which perspective distortion was re-
moved using our method. Veriﬁcation accuracy and receiver
operating characteristic (ROC) comparisons are shown in
Table 1 and Fig. 12a.

Landmark Detection Enhancement. We use the state-
of-the-art facial landmark tracker OpenPose [14] on 6,539
renderings from the BU-4DFE dataset [66] as previously
described, where each input image is rendered at a short
camera-to-object distance with signiﬁcant perspective dis-
tortion. We either directly apply the landmark detection to
the raw image, or the undistorted image using our network
and then locate the landmark on the raw input using the
ﬂow from our network. Landmark detection gives a 100%

performance based on our pre-processed images, on which
domain alignments are applied while fails on 9.0% origi-
nal perspective-distorted portraits. For quantitative compar-
isons, we evaluate the landmark accuracy using a standard
metric, Normalized Mean Error (NME) [68]. Given the
ground truth 3D facial geometry, we can ﬁnd the ground
truth 2D landmark locations of the inputs. For images
with successful detection for both the raw and undistorted
portraits, our method produces lower landmark error, with
mean NME = 4.4% (undistorted images), compared to 4.9%
(raw images). Fig. 12b shows the cumulative error curves,
showing an obvious improvement for facial landmark de-
tection for portraits undistorted using our approach.

Face Reconstruction. One difﬁculty of reconstructing
highly distorted faces is that the boundaries can be severely
self-occluded (e.g., disappearing ears or occlusion by the
hair), which is a common problem in 3D face reconstruc-
tion methods regardless if the method is based on 2D land-
marks or texture. Fig. 13 shows that processing a near-
range portrait input using our method in advance can sig-
niﬁcantly improves 3D face reconstruction. The 3D facial
geometry is obtained by ﬁtting a morphable model (Face-
Warehouse [12]) to 2D facial landmarks. Using the origi-
nal perspective-distorted image as input, the reconstructed
geometry appears distortion, while applying our technique
as a pre-processing step retains both identity and geomet-
ric details. We show error map of 3D geometry compared
to ground truth, demonstrating that our method applied as a
pre-processing step improves reconstruction accuracy, com-
pared with the baseline approach without any perspective-
distortion correction.

7. Conclusion

We have presented the ﬁrst automatic approach that cor-
rects the perspective distortion of unconstrained near-range
portraits. Our approach even handles extreme distortions.
We proposed a novel cascade network including camera pa-
rameter prediction network, forward ﬂow prediction net-
work and feature inpainting network. We also built the
ﬁrst database of perspective portraits pairs with a large vari-
ations on identities, expressions, illuminations and head
poses. Furthermore, we designed a novel duo-camera sys-
tem to capture testing images pairs of real human. Our
approach signiﬁcantly outperforms the state-of-the-art ap-
proach [25] on the task of perspective undistortion with an
accurate camera parameter prediction. Our approach also
boosts the performance of fundamental tasks like face veri-
ﬁcation, landmark detection and 3D face reconstruction.

Limitations and Future Work. One limitation of our
work is that the proposed approach does not generalize to

lens distortions, as our synthetic training dataset rendered
with an ideal perspective camera does not include this ar-
tifact. Another limitation is the time-spatial consistency.
Similarly, our current method is not explicitly trained to
handle portraits with large occlusions and head poses. We
plan to resolve the limitations in future work by augment-
ing training examples with lens distortions, dynamic mo-
tion sequences, large facial occlusions and head poses. An-
other future avenue is to investigate end-to-end training of
the cascaded network, which could further boost the perfor-
mance of our approach.

References

[1] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and
J. M. Ogden. Pyramid methods in image processing. RCA
engineer, 29(6):33–41, 1984. 3

[2] O. Alexander, G. Fyffe, J. Busch, X. Yu, R. Ichikari,
A. Jones, P. Debevec, J. Jimenez, E. Danvoye, B. Antionazzi,
et al. Digital ira: Creating a real-time photoreal digital actor.
In ACM SIGGRAPH 2013 Posters, page 1. ACM, 2013. 11
[3] B. Amberg, A. Blake, A. Fitzgibbon, S. Romdhani, and
T. Vetter. Reconstructing high quality face-surfaces using
model based stereo. In Computer Vision, 2007. ICCV 2007.
IEEE 11th International Conference on, pages 1–8. IEEE,
2007. 2

[4] B. Amos, B. Ludwiczuk, and M. Satyanarayanan. Openface:
A general-purpose face recognition library with mobile ap-
plications. CMU School of Computer Science, 2016. 8
[5] A. Bas and W. A. Smith. What does 2d geometric infor-
mation really tell us about 3d face shape? arXiv preprint
arXiv:1708.06703, 2017. 3

[6] T. Beeler, B. Bickel, P. Beardsley, B. Sumner, and M. Gross.
High-quality single-shot capture of facial geometry. In ACM
Transactions on Graphics (ToG), volume 29, page 40. ACM,
2010. 2

[7] T. Beeler, F. Hahn, D. Bradley, B. Bickel, P. Beardsley,
C. Gotsman, R. W. Sumner, and M. Gross. High-quality
passive facial performance capture using anchor frames. In
ACM Transactions on Graphics (TOG), volume 30, page 75.
ACM, 2011. 2

[8] V. Blanz and T. Vetter. A morphable model for the synthesis
of 3d faces. In Proceedings of the 26th annual conference on
Computer graphics and interactive techniques, pages 187–
194. ACM Press/Addison-Wesley Publishing Co., 1999. 2
[9] D. Bradley, W. Heidrich, T. Popa, and A. Sheffer. High res-
olution passive facial performance capture. In ACM transac-
tions on graphics (TOG), volume 29, page 41. ACM, 2010.
2

[10] R. Bryan, P. Perona, and R. Adolphs. Perspective distortion
from interpersonal distance is an implicit visual cue for so-
cial judgments of faces. PloS one, 7(9):e45301, 2012. 1,
3

[11] X. P. Burgos-Artizzu, M. R. Ronchi, and P. Perona. Dis-
tance estimation of an unknown person from a portrait. In
European Conference on Computer Vision, pages 313–327.
Springer, 2014. 3

[12] C. Cao, Y. Weng, S. Zhou, Y. Tong, and K. Zhou. Faceware-
house: A 3d facial expression database for visual computing.
IEEE Transactions on Visualization and Computer Graphics,
20(3):413–425, 2014. 2, 8

[13] X. Cao, Y. Wei, F. Wen, and J. Sun. Face alignment by ex-
plicit shape regression. International Journal of Computer
Vision, 107(2):177–190, 2014. 2

[14] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh.
OpenPose: realtime multi-person 2D pose estimation using
In arXiv preprint arXiv:1812.08008,
Part Afﬁnity Fields.
2018. 8

[15] Q. Chen, H. Wu, and T. Wada. Camera calibration with two
arbitrary coplanar circles. In European Conference on Com-
puter Vision, pages 521–532. Springer, 2004. 3

[16] M. Chiang and G. Fyffe. Realistic real-time rendering of

eyes and teeth. 2010. 11

[17] F. Cole, D. Belanger, D. Krishnan, A. Sarna, I. Mosseri, and
W. T. Freeman. Synthesizing normalized faces from facial
identity features. In Computer Vision and Pattern Recogni-
tion (CVPR), 2017 IEEE Conference on, pages 3386–3395.
IEEE, 2017. 3

[18] C. Colombo, D. Comanducci, and A. Del Bimbo. Camera
calibration with two arbitrary coaxial circles. In European
Conference on Computer Vision, pages 265–276. Springer,
2006. 3

[19] E. A. Cooper, E. A. Piazza, and M. S. Banks. The perceptual
basis of common photographic practice. Journal of vision,
12(5):8–8, 2012. 3

[20] T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active appear-
IEEE Transactions on pattern analysis and

ance models.
machine intelligence, 23(6):681–685, 2001. 2

[21] D. Cristinacce and T. Cootes. Automatic feature localisa-
tion with constrained local models. Pattern Recognition,
41(10):3054–3067, 2008. 2

[22] A. Datta, J.-S. Kim, and T. Kanade. Accurate camera cali-
bration using iterative reﬁnement of control points. In Com-
puter Vision Workshops (ICCV Workshops), 2009 IEEE 12th
International Conference on, pages 1201–1208. IEEE, 2009.
3

[23] P. Debevec, T. Hawkins, C. Tchou, H.-P. Duiker, W. Sarokin,
and M. Sagar. Acquiring the reﬂectance ﬁeld of a human
face. In Proceedings of the 27th annual conference on Com-
puter graphics and interactive techniques, pages 145–156.
ACM Press/Addison-Wesley Publishing Co., 2000. 2
[24] A. Flores, E. Christiansen, D. Kriegman, and S. Belongie.
Camera distance from face images. In International Sympo-
sium on Visual Computing, pages 513–522. Springer, 2013.
3

[25] O. Fried, E. Shechtman, D. B. Goldman, and A. Finkelstein.
Perspective-aware manipulation of portrait photos. ACM
Transactions on Graphics (TOG), 35(4):128, 2016. 1, 2, 3,
6, 7, 8

[26] A. Ghosh, G. Fyffe, B. Tunwattanapong, J. Busch, X. Yu,
and P. Debevec. Multiview face capture using polarized
In ACM Transactions on
spherical gradient illumination.
Graphics (TOG), volume 30, page 129. ACM, 2011. 2, 5

[27] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face
frontalization in unconstrained images. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4295–4304, 2015. 3

[28] J. Heikkila. Geometric camera calibration using circular con-
trol points. IEEE Transactions on pattern analysis and ma-
chine intelligence, 22(10):1066–1077, 2000. 3

[29] Y. Hold-Geoffroy, K. Sunkavalli, J. Eisenmann, M. Fisher,
E. Gambaretto, S. Hadap, and J.-F. Lalonde. A perceptual
measure for deep single image camera calibration. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2354–2363, 2018. 3

[30] L. Hu, S. Saito, L. Wei, K. Nagano, J. Seo, J. Fursund,
I. Sadeghi, C. Sun, Y.-C. Chen, and H. Li. Avatar digiti-
zation from a single image for real-time rendering. ACM
Transactions on Graphics (TOG), 36(6):195, 2017. 2
[31] R. Huang, S. Zhang, T. Li, R. He, et al. Beyond face rota-
tion: Global and local perception gan for photorealistic and
identity preserving frontal view synthesis. arXiv preprint
arXiv:1704.04086, 2017. 3

[32] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.

Image-
to-image translation with conditional adversarial networks.
arXiv preprint, 2017. 5, 7

[33] G. Jiang and L. Quan. Detection of concentric circles for
camera calibration. In Computer Vision, 2005. ICCV 2005.
Tenth IEEE International Conference on, volume 1, pages
333–340. IEEE, 2005. 3

[34] V. Kazemi and S. Josephine. One millisecond face alignment
with an ensemble of regression trees. In 27th IEEE Confer-
ence on Computer Vision and Pattern Recognition, CVPR
2014, Columbus, United States, 23 June 2014 through 28
June 2014, pages 1867–1874. IEEE Computer Society, 2014.
2

[35] I. Kemelmacher-Shlizerman.

Internet based morphable
model. In Computer Vision (ICCV), 2013 IEEE International
Conference on, pages 3256–3263. IEEE, 2013. 2

[36] I. Kemelmacher-Shlizerman and R. Basri. 3d face recon-
struction from a single image using a single reference face
shape. IEEE transactions on pattern analysis and machine
intelligence, 33(2):394–405, 2011. 2

[37] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu-
tional network for real-time 6-dof camera relocalization. In
Proceedings of the IEEE international conference on com-
puter vision, pages 2938–2946, 2015. 3

[38] H. Kim, M. Zollh¨ofer, A. Tewari, J. Thies, C. Richardt,
Inversefacenet: Deep single-shot in-
and C. Theobalt.
verse face rendering from a single image. arXiv preprint
arXiv:1703.10956, 2017. 2

[39] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014. 5
[40] H. Li, B. Adams, L. J. Guibas, and M. Pauly. Robust single-
view geometry and motion reconstruction. In ACM Transac-
tions on Graphics (TOG), volume 28, page 175. ACM, 2009.
2

[41] C. H. Liu and A. Chaudhuri. Face recognition with perspec-
tive transformation. Vision Research, 43(23):2393–2402,
2003. 1

[42] C. H. Liu and J. Ward. Face recognition in pictures is af-
fected by perspective transformation but not by the centre of
projection. Perception, 35(12):1637–1650, 2006. 1

[43] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 3431–3440, 2015. 11

[44] W.-C. Ma, T. Hawkins, P. Peers, C.-F. Chabert, M. Weiss,
and P. Debevec. Rapid acquisition of specular and diffuse
normal maps from polarized spherical gradient illumination.
In Proceedings of the 18th Eurographics conference on Ren-
dering Techniques, pages 183–194. Eurographics Associa-
tion, 2007. 2, 5

[45] G. Medioni and S. B. Kang. Emerging topics in computer

vision. Prentice Hall PTR, 2004. 3

[46] F. I. Parke and K. Waters. Computer facial animation. CRC

Press, 2008. 2

[47] F. Remondino and C. Fraser. Digital camera calibration
International
methods: considerations and comparisons.
Archives of Photogrammetry, Remote Sensing and Spatial
Information Sciences, 36(5):266–272, 2006. 3

[48] S. Ren, X. Cao, Y. Wei, and J. Sun. Face alignment at 3000
fps via regressing local binary features. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1685–1692, 2014. 2

[49] E. Richardson, M. Sela, R. Or-El, and R. Kimmel. Learning
In 2017
detailed face reconstruction from a single image.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 5553–5562. IEEE, 2017. 2

[50] C. Sagonas, Y. Panagakis, S. Zafeiriou, and M. Pantic. Ro-
In Proceedings of the
bust statistical face frontalization.
IEEE International Conference on Computer Vision, pages
3871–3879, 2015. 3

[51] J. Salvi, X. Armangu´e, and J. Batlle. A comparative review
of camera calibrating methods with accuracy evaluation. Pat-
tern recognition, 35(7):1617–1635, 2002. 3

[52] J. M. Saragih, S. Lucey, and J. F. Cohn. Deformable model
International

ﬁtting by regularized landmark mean-shift.
Journal of Computer Vision, 91(2):200–215, 2011. 2
[53] X. Shen, A. Hertzmann, J. Jia, S. Paris, B. Price, E. Shecht-
man, and I. Sachs. Automatic portrait segmentation for im-
age stylization. In Computer Graphics Forum, volume 35,
pages 93–102. Wiley Online Library, 2016. 11

[54] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 4

[55] A. Tewari, M. Zollh¨ofer, H. Kim, P. Garrido, F. Bernard,
P. Perez, and C. Theobalt. Mofa: Model-based deep convo-
lutional face autoencoder for unsupervised monocular recon-
In The IEEE International Conference on Com-
struction.
puter Vision (ICCV), volume 2, 2017. 2

[56] J. Thies, M. Zollh¨ofer, M. Stamminger, C. Theobalt, and
M. Nießner. Face2face: Real-time face capture and reenact-
ment of rgb videos. In Computer Vision and Pattern Recogni-
tion (CVPR), 2016 IEEE Conference on, pages 2387–2395.
IEEE, 2016. 2

[57] R. Tsai. A versatile camera calibration technique for high-
accuracy 3d machine vision metrology using off-the-shelf tv
cameras and lenses. IEEE Journal on Robotics and Automa-
tion, 3(4):323–344, 1987. 3

[58] J. Valente and S. Soatto. Perspective distortion modeling,
learning and compensation. In Computer Vision and Pattern
Recognition Workshops (CVPRW), 2015 IEEE Conference
on, pages 9–16. IEEE, 2015. 3

[59] P. Viola and M. J. Jones. Robust real-time face detection.
International journal of computer vision, 57(2):137–154,
2004. 11

[60] D. Vlasic, M. Brand, H. Pﬁster, and J. Popovi´c. Face trans-
fer with multilinear models. ACM transactions on graphics
(TOG), 24(3):426–433, 2005. 2

[61] B. Ward, M. Ward, O. Fried, and B. Paskhover. Nasal distor-
tion in short-distance photographs: The selﬁe effect. JAMA
facial plastic surgery, 2018. 1

[62] T. Weise, H. Li, L. Van Gool, and M. Pauly. Face/off:
Live facial puppetry. In Proceedings of the 2009 ACM SIG-
GRAPH/Eurographics Symposium on Computer animation,
pages 7–16. ACM, 2009. 2

[63] S. Workman, C. Greenwell, M. Zhai, R. Baltenberger, and
N. Jacobs. Deepfocal: a method for direct focal length es-
timation. In Image Processing (ICIP), 2015 IEEE Interna-
tional Conference on, pages 1369–1373. IEEE, 2015. 3
[64] S. Workman, M. Zhai, and N. Jacobs. Horizon lines in the
wild. In British Machine Vision Conference (BMVC), 2016.
3

[65] X. Xiong and F. De la Torre. Supervised descent method
and its applications to face alignment. In Computer Vision
and Pattern Recognition (CVPR), 2013 IEEE Conference on,
pages 532–539. IEEE, 2013. 2

[66] L. Yin, X. Chen, Y. Sun, T. Worm, and M. Reale. A high-
resolution 3d dynamic facial expression database. In Auto-
matic Face & Gesture Recognition, 2008. FG’08. 8th IEEE
International Conference on, pages 1–6. IEEE, 2008. 5, 8

[67] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker. Towards
In Proc. ICCV,

large-pose face frontalization in the wild.
pages 1–10, 2017. 3

[68] S. Zafeiriou, G. Trigeorgis, G. Chrysos, J. Deng, and J. Shen.
The menpo facial landmark localisation challenge: A step to-
wards the solution. In The IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) Workshops, volume 1,
page 2, 2017. 8

[69] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale,
A. Horowitz, and P. Liu. A high-resolution spontaneous 3d
dynamic facial expression database. In 2013 10th IEEE In-
ternational Conference and Workshops on Automatic Face
and Gesture Recognition (FG), pages 1–6. IEEE, 2013. 11

[70] Z. Zhang. A ﬂexible new technique for camera calibration.
IEEE Transactions on pattern analysis and machine intelli-
gence, 22(11):1330–1334, 2000. 3

Appendix

A. Test Data Pre-Process

Before feeding an arbitrary portrait to our cascaded net-
work, we ﬁrst detect the face bounding box using the
method of Viola and Jones [59] and then segment out the
head region with a segmentation network based on FCN-
8s [43]. The network is trained using modiﬁed portrait data
from Shen et al. [53]. In order to handle images with arbi-
trary resolution, we pre-process the segmented images to a
uniform size of 512 × 512. The input image is ﬁrst scaled
so that its detected inter-pupillary distance matches a target
length, computed by averaging that of ground truth images.
The image is then aligned in the same manner as the train-
ing data, by matching the inner corner of each subject’s right
eye to a ﬁxed position. We then crop and pad the image to
512 × 512, maintaining the right eye inner alignment.

B. Training Data Preparation

We rendered synthetic portraits using a variety of camera
distances, head poses, and incident illumination conditions.
In particular, we rendered 10 different views for each sub-
ject, randomly sampling candidate head poses in the range
of -45 to +45 in pitch, yaw, and roll. For each view, we
rendered the subject using global illumination and image-
based lighting, randomly sampling the illumination from a
light probe image gallery of 107 unique environments. For
each head pose and lighting condition, we rendered the sub-
ject at twenty different distances, including the canonical
distance of 1.6m, observed as free from perspective distor-
tion. Empirically, we observed that 23cm is the minimal
distance that captures a full face without obvious lens dis-
tortion (note that we do not consider special types of lenses,
e.g. ﬁsheye or wide-angle); thus we sample the distances
by considering 10 distance bins uniformly distributed from
23cm to 1.6m. We adjusted the focal length of the ren-
dering camera to ensure consistent framing, which yields a
focal length range of 18mm − 128mm. To ensure efﬁcient
learning, we aligned the renderings by matching the inner
corner of each subject’s right eye to a ﬁxed position. We
use the techniques of Alexander et al.
[2] and Chiang and
Fyffe [16] to render photo-realistic portraits in real-time
using OpenGL shaders, which include separable subsurface
scattering in screen-space and photo-real eye rendering.

To supplement this data with additional identities, we
also rendered portraits using the 3D facials scans of the BU-
4DFE dataset [69], after randomly sampling candidate head
poses in the range of -15 to +15 in pitch and yaw and sam-
pling the same range of camera distances. These were ren-
dered simply using perspective projection and rasterization,
without changing illumination or considering complex re-
ﬂectance properties. Out of 58 female subjects and 43 male
subjects total, we trained using 38 female and 25 male sub-

jects, keeping the rest set aside as test data, for a total of
17,000 additional training pairs.

C. Beam-splitter System and Calibration

of CAM A. We photograph a color chart with each camera
and compute a color correction matrix to match the color
rendition of the image pairs, which may be different ow-
ing to different light transport phenomena of the different
lenses of CAM A and CAM B. We also captured a checker
board image for each camera immediately after each sub-
ject photograph, which we used for per-pixel alignment of
the image pairs, achieved via a similarity transformation.

D. Distance Estimation in log-space

Figure 14: Beam-splitter capturing system.

Our beam-splitter capture system enables simultaneous
photography of a subject at two different distances. As
shown in Figure 14, our system includes one ﬁxed camera
CAM B at the canonical distance of 1.6m, one sliding cam-
era CAM A at variable distances from 23cm to 137cm and
a plate beam-splitter at 45◦ along a metal rail. Figure 15
shows the details of our system.

Figure 15: Illustration of Beam-splitter system.

Hardware Speciﬁcations.
In Table 2, we list the speciﬁ-
cations of the two cameras. The beam-splitter is a 254mm ×
356mm Edmund Optics plate beam-splitter, with 50% re-
ﬂection and 50% transmission of incident light.

Calibration.
In order to capture near “ground truth” im-
ages pairs where only distances vary, we carefully calibrate
the two cameras geometrically and radiometrically, thereby
eliminating potential differences in pose and color rendi-
tion. We start with a leveled surface, aligning CAM A and
CAM B horizontally. We then use a laser pointer to align
the optical axes of the two cameras. In particular, we point
the laser at C as shown in Figure 15, which shines a red
dot at the lens center of CAM B. We then adjust the angle
of the beam-splitter so that when looking through beam-
splitter from D, the red dot passes through the lens center

Figure 16: Illustration of camera perspective projection.

In Figure 16, p(x, y, z) is one point of a 3D object, and
(u, v) is the corresponding pixel location of p projected on
the image plane. The equation of perspective projection of
u is as below (v is similar):

u = x · f /z

When only the camera-to-subject distance of the point
p is varied, this is equivalent to a change only in z. The
derivative of u with respect to z is as below:

du
dz

1
z2

= (−x · f )

= C1 ·

1
z2

In which C1 = (−x · f ) is a constant. The pixel loca-
tion change caused by varying camera-to-subject distance is
therefore non-linear. However, to use uniform sampling in
our distance prediction network, a linear space is preferable.
Thus we consider a log-space mapping instead. Equation 3
now becomes:

log(u) = log(x · f /z)

= log(x · f ) − log(z)
= C2 − log(z)

In Equation 8, C2 is a constant and log(u) is a linear
mapping of log(z), which means that in log-space, the per-
spective projection is a linear mapping of camera-to-subject

(3)

(4)

(5)

(6)

(7)

(8)

Specs
Model
Lens
F-stop
Sync
Data Streaming

CAM A
Canon EOS-1DX
AF-s NIKKOR 17 − 35mm
F-2.8
Hardware sync
FTP Server

CAM B
Canon EOS-1DX Mark II
Nikon 135mm
F-2.8
Hardware sync
FTP Server

Table 2: Camera Speciﬁcations.

distances. Thus in our distance estimation network, we
adopt log-space instead of directly using of the actual dis-
tances.

E. 35mm Equivalent Focal Length

The term “35mm equivalent focal length” of a digital or
ﬁlm camera with an accompanying lens is the focal length
that would be required for a standard 35mm ﬁlm camera to
achieve the same ﬁeld of view. This standard ﬁlm camera
was the most widely used type before the digital camera rev-
olution. Now that a variety of imaging sensor sizes are com-
monly available in digital cameras, the focal length alone
does not determine the ﬁeld-of-view. Hence, this canoni-
cal reference helps photographers standardize their ﬁeld-of-
view understanding for different lenses and sensor combi-
nations. In our case, our canonical camera (at a distance of
1.6m)’s 35mm equivalent to a focal length is 128.4mm.

F. Conversion between Focal Length and Dis-
tances

To ensure that all portraits have the same framing (or,
stated differently, that the size of the subject’s head re-
mains the same as measured by the inter-pupillary distance),
when the camera-to-subject distance changes, then the focal
length of the camera should change proportionally. In our
case, the equation to convert a change in camera-to-subject
distance to a change in 35mm equivalent focal length is as
below:

f = D × 128.4mm/160cm

(9)

In which D is the camera-to-subject distance, and f is

the corresponding 35mm equivalent focal length.

