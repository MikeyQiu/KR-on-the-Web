LINE: Large-scale Information Network Embedding

Jian Tang1, Meng Qu2∗, Mingzhe Wang2, Ming Zhang2, Jun Yan1, Qiaozhu Mei3
1Microsoft Research Asia, {jiatang, junyan}@microsoft.com
2School of EECS, Peking University, {mnqu, wangmingzhe, mzhang_cs}@pku.edu.cn
3School of Information, University of Michigan, qmei@umich.edu

5
1
0
2
 
r
a

M
 
2
1
 
 
]

G
L
.
s
c
[
 
 
1
v
8
7
5
3
0
.
3
0
5
1
:
v
i
X
r
a

ABSTRACT
This paper studies the problem of embedding very large
information networks into low-dimensional vector spaces,
which is useful in many tasks such as visualization, node
classiﬁcation, and link prediction. Most existing graph em-
bedding methods do not scale for real world information
In this
networks which usually contain millions of nodes.
paper, we propose a novel network embedding method called
the “LINE,” which is suitable for arbitrary types of informa-
tion networks: undirected, directed, and/or weighted. The
method optimizes a carefully designed objective function
that preserves both the local and global network structures.
An edge-sampling algorithm is proposed that addresses the
limitation of the classical stochastic gradient descent and
improves both the eﬀectiveness and the eﬃciency of the in-
ference. Empirical experiments prove the eﬀectiveness of
the LINE on a variety of real-world information networks,
including language networks, social networks, and citation
networks. The algorithm is very eﬃcient, which is able to
learn the embedding of a network with millions of vertices
and billions of edges in a few hours on a typical single ma-
chine. The source code of the LINE is available online.1

Categories and Subject Descriptors
I.2.6 [Artiﬁcial Intelligence]: Learning

General Terms
Algorithms, Experimentation

Keywords
information network embedding; scalability; feature learn-
ing; dimension reduction

∗This work was done when the second author was an intern
at Microsoft Research Asia.
1https://github.com/tangjianpku/LINE

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the
author’s site if the Material is used in electronic media.
WWW 2015, May 18–22, 2015, Florence, Italy.
ACM 978-1-4503-3469-3/15/05.
http://dx.doi.org/10.1145/2736277.2741093.

1.

INTRODUCTION

Information networks are ubiquitous in the real world with
examples such as airline networks, publication networks, so-
cial and communication networks, and the World Wide Web.
The size of these information networks ranges from hundreds
of nodes to millions and billions of nodes. Analyzing large
information networks has been attracting increasing atten-
tion in both academia and industry. This paper studies
the problem of embedding information networks into low-
dimensional spaces, in which every vertex is represented as
a low-dimensional vector. Such a low-dimensional embed-
ding is very useful in a variety of applications such as vi-
sualization [21], node classiﬁcation [3], link prediction [10],
and recommendation [23].

Various methods of graph embedding have been proposed
in the machine learning literature (e.g., [4, 20, 2]). They
generally perform well on smaller networks. The problem
becomes much more challenging when a real world informa-
tion network is concerned, which typically contains millions
of nodes and billions of edges. For example, the Twitter
followee-follower network contains 175 million active users
and around twenty billion edges in 2012 [14]. Most exist-
ing graph embedding algorithms do not scale for networks
of this size. For example, the time complexity of classical
graph embedding algorithms such as MDS [4], IsoMap [20],
Laplacian eigenmap [2] are at least quadratic to the number
of vertices, which is too expensive for networks with mil-
lions of nodes. Although a few very recent studies approach
the embedding of large-scale networks, these methods either
use an indirect approach that is not designed for networks
(e.g., [1]) or lack a clear objective function tailored for net-
work embedding (e.g., [16]). We anticipate that a new model
with a carefully designed objective function that preserves
properties of the graph and an eﬃcient optimization tech-
nique should eﬀectively ﬁnd the embedding of millions of
nodes.

In this paper, we propose such a network embedding model
called the “LINE,” which is able to scale to very large, arbi-
trary types of networks: undirected, directed and/or weighted.
The model optimizes an objective which preserves both the
local and global network structures. Naturally, the local
structures are represented by the observed links in the net-
works, which capture the ﬁrst-order proximity between the
vertices. Most existing graph embedding algorithms are de-
signed to preserve this ﬁrst-order proximity, e.g., IsoMap [20]
and Laplacian eigenmap [2], even if they do not scale. We
observe that in a real-world network many (if not the major-
ity of) legitimate links are actually not observed. In other

pling process, the objective function remains the same and
the weights of the edges no longer aﬀect the gradients.

The LINE is very general, which works well for directed
or undirected, weighted or unweighted graphs. We evalu-
ate the performance of the LINE with various real-world
information networks, including language networks, social
networks, and citation networks. The eﬀectiveness of the
learned embeddings is evaluated within multiple data min-
ing tasks, including word analogy, text classiﬁcation, and
node classiﬁcation. The results suggest that the LINE model
outperforms other competitive baselines in terms of both ef-
fectiveness and eﬃciency. It is able to learn the embedding
of a network with millions of nodes and billions of edges in
a few hours on a single machine.

To summarize, we make the following contributions:

• We propose a novel network embedding model called
the “LINE,” which suits arbitrary types of information
networks and easily scales to millions of nodes. It has
a carefully designed objective function that preserves
both the ﬁrst-order and second-order proximities.

• We propose an edge-sampling algorithm for optimizing
the objective. The algorithm tackles the limitation of
the classical stochastic gradient decent and improves
the eﬀectiveness and eﬃciency of the inference.

• We conduct extensive experiments on real-world infor-
mation networks. Experimental results prove the ef-
fectiveness and eﬃciency of the proposed LINE model.

Organization. The rest of this paper is organized as
follows. Section 2 summarizes the related work. Section 3
formally deﬁnes the problem of large-scale information net-
work embedding. Section 4 introduces the LINE model in
details. Section 5 presents the experimental results. Finally
we conclude in Section 6.

2. RELATED WORK

Our work is related to classical methods of graph em-
bedding or dimension reduction in general, such as multi-
dimensional scaling (MDS) [4], IsoMap [20], LLE [18] and
Laplacian Eigenmap [2]. These approaches typically ﬁrst
construct the aﬃnity graph using the feature vectors of the
data points, e.g., the K-nearest neighbor graph of data, and
then embed the aﬃnity graph [22] into a low dimensional
space. However, these algorithms usually rely on solving the
leading eigenvectors of the aﬃnity matrices, the complexity
of which is at least quadratic to the number of nodes, making
them ineﬃcient to handle large-scale networks.

Among the most recent literature is a technique called
graph factorization [1]. It ﬁnds the low-dimensional embed-
ding of a large graph through matrix factorization, which is
optimized using stochastic gradient descent. This is possi-
ble because a graph can be represented as an aﬃnity ma-
trix. However, the objective of matrix factorization is not
designed for networks, therefore does not necessarily pre-
serve the global network structure. Intuitively, graph fac-
torization expects nodes with higher ﬁrst-order proximity
are represented closely.
Instead, the LINE model uses an
objective that is particularly designed for networks, which
preserves both the ﬁrst-order and the second-order prox-
imities. Practically, the graph factorization method only
applies to undirected graphs while the proposed model is
applicable for both undirected and directed graphs.

Figure 1: A toy example of information network. Edges can
be undirected, directed, and/or weighted. Vertex 6 and 7
should be placed closely in the low-dimensional space as they
are connected through a strong tie. Vertex 5 and 6 should
also be placed closely as they share similar neighbors.

words, the observed ﬁrst-order proximity in the real world
data is not suﬃcient for preserving the global network struc-
tures. As a complement, we explore the second-order prox-
imity between the vertices, which is not determined through
the observed tie strength but through the shared neighbor-
hood structures of the vertices. The general notion of the
second-order proximity can be interpreted as nodes with
shared neighbors being likely to be similar. Such an intu-
ition can be found in the theories of sociology and linguistics.
For example, “the degree of overlap of two people’s friend-
ship networks correlates with the strength of ties between
them,” in a social network [6]; and “You shall know a word
by the company it keeps” (Firth, J. R. 1957:11) in text cor-
pora [5]. Indeed, people who share many common friends
are likely to share the same interest and become friends, and
words that are used together with many similar words are
likely to have similar meanings.

Fig. 1 presents an illustrative example. As the weight of
the edge between vertex 6 and 7 is large, i.e., 6 and 7 have a
high ﬁrst-order proximity, they should be represented closely
to each other in the embedded space. On the other hand,
though there is no link between vertex 5 and 6, they share
many common neighbors, i.e., they have a high second-order
proximity and therefore should also be represented closely to
each other. We expect that the consideration of the second-
order proximity eﬀectively complements the sparsity of the
ﬁrst-order proximity and better preserves the global struc-
ture of the network.
In this paper, we will present care-
fully designed objectives that preserve the ﬁrst-order and
the second-order proximities.

Even if a sound objective is found, optimizing it for a very
large network is challenging. One approach that attracts
attention in recent years is using the stochastic gradient de-
scent for the optimization. However, we show that directly
deploying the stochastic gradient descent is problematic for
real world information networks. This is because in many
networks, edges are weighted and the weights usually present
a high variance. Consider a word co-occurrence network, in
which the weights (co-occurrences) of word pairs may range
from one to hundreds of thousands. These weights of the
edges will be multiplied into the gradients, resulting in the
explosion of the gradients and thus compromise the perfor-
mance. To address this, we propose a novel edge-sampling
method, which improves both the eﬀectiveness and eﬃciency
of the inference. We sample the edges with the probabilities
proportional to their weights, and then treat the sampled
edges as binary edges for model updating. With this sam-

The most recent work related with ours is DeepWalk [16],
which deploys a truncated random walk for social network
embedding. Although empirically eﬀective, the DeepWalk
does not provide a clear objective that articulates what net-
work properties are preserved.
Intuitively, DeepWalk ex-
pects nodes with higher second-order proximity yield similar
low-dimensional representations, while the LINE preserves
both ﬁrst-order and second-order proximities. DeepWalk
uses random walks to expand the neighborhood of a vertex,
which is analogical to a depth-ﬁrst search. We use a breadth-
ﬁrst search strategy, which is a more reasonable approach to
the second-order proximity. Practically, DeepWalk only ap-
plies to unweighted networks, while our model is applicable
for networks with both weighted and unweighted edges.

In Section 5, we empirically compare the proposed model

with these methods using various real world networks.

3. PROBLEM DEFINITION

We formally deﬁne the problem of large-scale information
network embedding using ﬁrst-order and second-order prox-
imities. We ﬁrst deﬁne an information network as follows:

Definition 1. (Information Network) An informa-
tion network is deﬁned as G = (V, E), where V is the set
of vertices, each representing a data object and E is the
set of edges between the vertices, each representing a re-
lationship between two data objects. Each edge e ∈ E is
an ordered pair e = (u, v) and is associated with a weight
wuv > 0, which indicates the strength of the relation. If G
is undirected, we have (u, v) ≡ (v, u) and wuv ≡ wvu; if G
is directed, we have (u, v) (cid:54)≡ (v, u) and wuv (cid:54)≡ wvu.

In practice, information networks can be either directed
(e.g., citation networks) or undirected (e.g., social network
of users in Facebook). The weights of the edges can be either
binary or take any real value. Note that while negative edge
weights are possible, in this study we only consider non-
negative weights. For example, in citation networks and
social networks, wuv takes binary values; in co-occurrence
networks between diﬀerent objects, wuv can take any non-
negative value. The weights of the edges in some networks
may diverge as some objects co-occur many times while oth-
ers may just co-occur a few times.

Embedding an information network into a low-dimensional
space is useful in a variety of applications. To conduct the
embedding, the network structures must be preserved. The
ﬁrst intuition is that the local network structure, i.e., the
local pairwise proximity between the vertices, must be pre-
served. We deﬁne the local network structures as the ﬁrst-
order proximity between the vertices:

Definition 2. (First-order Proximity) The ﬁrst-order

proximity in a network is the local pairwise proximity be-
tween two vertices. For each pair of vertices linked by an
edge (u, v), the weight on that edge, wuv, indicates the ﬁrst-
If no edge is observed
order proximity between u and v.
between u and v, their ﬁrst-order proximity is 0.

The ﬁrst-order proximity usually implies the similarity of
two nodes in a real-world network. For example, people who
are friends with each other in a social network tend to share
similar interests; pages linking to each other in World Wide
Web tend to talk about similar topics. Because of this im-
portance, many existing graph embedding algorithms such

as IsoMap, LLE, Laplacian eigenmap, and graph factoriza-
tion have the objective to preserve the ﬁrst-order proximity.
However, in a real world information network, the links
observed are only a small proportion, with many others
missing [10]. A pair of nodes on a missing link has a zero
ﬁrst-order proximity, even though they are intrinsically very
similar to each other. Therefore, ﬁrst-order proximity alone
is not suﬃcient for preserving the network structures, and
it is important to seek an alternative notion of proximity
that addresses the problem of sparsity. A natural intuition
is that vertices that share similar neighbors tend to be sim-
ilar to each other. For example, in social networks, people
who share similar friends tend to have similar interests and
thus become friends; in word co-occurrence networks, words
that always co-occur with the same set of words tend to
have similar meanings. We therefore deﬁne the second-order
proximity, which complements the ﬁrst-order proximity and
preserves the network structure.

Definition 3. (Second-order Proximity) The second-
order proximity between a pair of vertices (u, v) in a net-
work is the similarity between their neighborhood network
structures. Mathematically, let pu = (wu,1, . . . , wu,|V |) de-
note the ﬁrst-order proximity of u with all the other vertices,
then the second-order proximity between u and v is deter-
mined by the similarity between pu and pv. If no vertex is
linked from/to both u and v, the second-order proximity
between u and v is 0.

We investigate both ﬁrst-order and second-order proxim-

ity for network embedding, which is deﬁned as follows.

Definition 4. (Large-scale Information Network Em-

bedding) Given a large network G = (V, E), the problem
of Large-scale Information Network Embedding aims
to represent each vertex v ∈ V into a low-dimensional space
Rd, i.e., learning a function fG : V → Rd, where d (cid:28) |V |.
In the space Rd, both the ﬁrst-order proximity and the
second-order proximity between the vertices are preserved.

Next, we introduce a large-scale network embedding model

that preserves both ﬁrst- and second-order proximities.

4. LINE: LARGE-SCALE INFORMATION

NETWORK EMBEDDING

A desirable embedding model for real world information
networks must satisfy several requirements: ﬁrst, it must
be able to preserve both the ﬁrst-order proximity and the
second-order proximity between the vertices; second, it must
scale for very large networks, say millions of vertices and bil-
lions of edges; third, it can deal with networks with arbitrary
types of edges: directed, undirected and/or weighted. In this
section, we present a novel network embedding model called
the “LINE,” which satisﬁes all the three requirements.

4.1 Model Description

We describe the LINE model to preserve the ﬁrst-order
proximity and second-order proximity separately, and then
introduce a simple way to combine the two proximity.

4.1.1 LINE with First-order Proximity

The ﬁrst-order proximity refers to the local pairwise prox-
imity between the vertices in the network. To model the

O1 = −

wij log p1(vi, vj),

(3)

(cid:88)

(i,j)∈E

imities

ﬁrst-order proximity, for each undirected edge (i, j), we de-
ﬁne the joint probability between vertex vi and vj as follows:

p1(vi, vj) =

,

(1)

1 + exp(−(cid:126)uT

i · (cid:126)uj)

1

where (cid:126)ui ∈ Rd is the low-dimensional vector representation
of vertex vi. Eqn. (1) deﬁnes a distribution p(·, ·) over the
space V × V , and its empirical probability can be deﬁned
as ˆp1(i, j) = wij
(i,j)∈E wij. To preserve the
ﬁrst-order proximity, a straightforward way is to minimize
the following objective function:

W , where W = (cid:80)

O1 = d(ˆp1(·, ·), p1(·, ·)),

(2)

where d(·, ·) is the distance between two distributions. We
choose to minimize the KL-divergence of two probability dis-
tributions. Replacing d(·, ·) with KL-divergence and omit-
ting some constants, we have:

Note that the ﬁrst-order proximity is only applicable for
undirected graphs, not for directed graphs. By ﬁnding the
{(cid:126)ui}i=1..|V | that minimize the objective in Eqn. (3), we can
represent every vertex in the d-dimensional space.

4.1.2 LINE with Second-order Proximity

The second-order proximity is applicable for both directed
and undirected graphs. Given a network, without loss of
generality, we assume it is directed (an undirected edge can
be considered as two directed edges with opposite directions
and equal weights). The second-order proximity assumes
that vertices sharing many connections to other vertices are
similar to each other. In this case, each vertex is also treated
as a speciﬁc “context” and vertices with similar distributions
over the “contexts” are assumed to be similar. Therefore,
each vertex plays two roles: the vertex itself and a speciﬁc
“context” of other vertices. We introduce two vectors (cid:126)ui and
(cid:126)u(cid:48)
i, where (cid:126)ui is the representation of vi when it is treated
as a vertex while (cid:126)u(cid:48)
i is the representation of vi when it is
treated as a speciﬁc “context”. For each directed edge (i, j),
we ﬁrst deﬁne the probability of “context” vj generated by
vertex vi as:

p2(vj|vi) =

exp((cid:126)u(cid:48)T
j
k=1 exp((cid:126)u(cid:48)T

· (cid:126)ui)
k · (cid:126)ui)

,

(cid:80)|V |

(4)

where |V | is the number of vertices or “contexts.” For each
vertex vi, Eqn. (4) actually deﬁnes a conditional distribution
p2(·|vi) over the contexts, i.e., the entire set of vertices in the
network. As mentioned above, the second-order proximity
assumes that vertices with similar distributions over the con-
texts are similar to each other. To preserve the second-order
proximity, we should make the conditional distribution of
the contexts p2(·|vi) speciﬁed by the low-dimensional rep-
resentation be close to the empirical distribution ˆp2(·|vi).
Therefore, we minimize the following objective function:

O2 =

λid(ˆp2(·|vi), p2(·|vi)),

(5)

(cid:88)

i∈V

where d(·, ·) is the distance between two distributions. As
the importance of the vertices in the network may be diﬀer-
ent, we introduce λi in the objective function to represent

the prestige of vertex i in the network, which can be mea-
sured by the degree or estimated through algorithms such as
PageRank [15]. The empirical distribution ˆp2(·|vi) is deﬁned
as ˆp2(vj|vi) = wij
, where wij is the weight of the edge (i, j)
di
and di is the out-degree of vertex i, i.e. di = (cid:80)
k∈N (i) wik,
where N (i) is the set of out-neighbors of vi. In this paper,
for simplicity we set λi as the degree of vertex i, i.e., λi = di,
and here we also adopt KL-divergence as the distance func-
tion. Replacing d(·, ·) with KL-divergence, setting λi = di
and omitting some constants, we have:

O2 = −

wij log p2(vj|vi).

(6)

(cid:88)

(i,j)∈E

By learning {(cid:126)ui}i=1..|V | and {(cid:126)u(cid:48)

i}i=1..|V | that minimize
this objective, we are able to represent every vertex vi with
a d-dimensional vector (cid:126)ui.

4.1.3 Combining ﬁrst-order and second-order prox-

To embed the networks by preserving both the ﬁrst-order
and second-order proximity, a simple and eﬀective way we
ﬁnd in practice is to train the LINE model which preserves
the ﬁrst-order proximity and second-order proximity sepa-
rately and then concatenate the embeddings trained by the
two methods for each vertex. A more principled way to
combine the two proximity is to jointly train the objective
function (3) and (6), which we leave as future work.

4.2 Model Optimization

Optimizing objective (6) is computationally expensive,
which requires the summation over the entire set of ver-
tices when calculating the conditional probability p2(·|vi).
To address this problem, we adopt the approach of negative
sampling proposed in [13], which samples multiple negative
edges according to some noisy distribution for each edge
(i, j). More speciﬁcally, it speciﬁes the following objective
function for each edge (i, j):

log σ((cid:126)u(cid:48)
j

T · (cid:126)ui) +

Evn∼Pn(v)[log σ(−(cid:126)u(cid:48)

n

T · (cid:126)ui)],

(7)

K
(cid:88)

i=1

where σ(x) = 1/(1 + exp(−x)) is the sigmoid function. The
ﬁrst term models the observed edges, the second term mod-
els the negative edges drawn from the noise distribution and
3/4
K is the number of negative edges. We set Pn(v) ∝ dv
as proposed in [13], where dv is the out-degree of vertex v.
For the objective function (3), there exists a trivial solu-
tion: uik = ∞, for i=1, . . . , |V | and k = 1, . . . , d. To avoid
the trivial solution, we can still utilize the negative sampling
approach (7) by just changing (cid:126)u(cid:48)T
j

to (cid:126)uT
j .

We adopt the asynchronous stochastic gradient algorithm
(ASGD) [17] for optimizing Eqn. (7).
In each step, the
ASGD algorithm samples a mini-batch of edges and then
updates the model parameters. If an edge (i, j) is sampled,
the gradient w.r.t. the embedding vector (cid:126)ui of vertex i will
be calculated as:

∂O2
∂(cid:126)ui

= wij ·

∂ log p2(vj|vi)
∂(cid:126)ui

(8)

Note that the gradient will be multiplied by the weight of
the edge. This will become problematic when the weights

of edges have a high variance. For example, in a word co-
occurrence network, some words co-occur many times (e.g.,
tens of thousands) while some words co-occur only a few
times. In such networks, the scales of the gradients diverge
and it is very hard to ﬁnd a good learning rate. If we select a
large learning rate according to the edges with small weights,
the gradients on edges with large weights will explode while
the gradients will become too small if we select the learning
rate according to the edges with large weights.

4.2.1 Optimization via Edge Sampling

The intuition in solving the above problem is that if the
weights of all the edges are equal (e.g., network with binary
edges), then there will be no problem of choosing an appro-
priate learning rate. A simple treatment is thus to unfold
a weighted edge into multiple binary edges, e.g., an edge
with weight w is unfolded into w binary edges. This will
solve the problem but will signiﬁcantly increase the memory
requirement, especially when the weights of the edges are
very large. To resolve this, one can sample from the origi-
nal edges and treat the sampled edges as binary edges, with
the sampling probabilities proportional to the original edge
weights. With this edge-sampling treatment, the overall ob-
jective function remains the same. The problem boils down
to how to sample the edges according to their weights.

Let W = (w1, w2, . . . , w|E|) denote the sequence of the
weights of the edges. One can simply calculate the sum of
the weights wsum = (cid:80)|E|
i=1 wi ﬁrst, and then to sample a
random value within the range of [0, wsum] to see which in-
terval [(cid:80)i−1
j=0 wj) the random value falls into. This
approach takes O(|E|) time to draw a sample, which is costly
when the number of edges |E| is large. We use the alias table
method [9] to draw a sample according to the weights of the
edges, which takes only O(1) time when repeatedly drawing
samples from the same discrete distribution.

j=0 wj, (cid:80)i

Sampling an edge from the alias table takes constant time,
O(1), and optimization with negative sampling takes O(d(K+
1)) time, where K is the number of negative samples. There-
fore, overall each step takes O(dK) time.
In practice, we
ﬁnd that the number of steps used for optimization is usu-
ally proportional to the number of edges O(|E|). Therefore,
the overall time complexity of the LINE is O(dK|E|), which
is linear to the number of edges |E|, and does not depend
on the number of vertices |V |. The edge sampling treatment
improves the eﬀectiveness of the stochastic gradient descent
without compromising the eﬃciency.

4.3 Discussion

We discuss several practical issues of the LINE model.
Low degree vertices. One practical issue is how to ac-
curately embed vertices with small degrees. As the number
of neighbors of such a node is very small, it is very hard
to accurately infer its representation, especially with the
second-order proximity based methods which heavily rely
on the number of “contexts.” An intuitive solution to this is
expanding the neighbors of those vertices by adding higher
order neighbors, such as neighbors of neighbors. In this pa-
per, we only consider adding second-order neighbors, i.e.,
neighbors of neighbors, to each vertex. The weight between
vertex i and its second-order neighbor j is measured as

In practice, one can only add a subset of vertices {j} which
have the largest proximity wij with the low degree vertex i.
New vertices. Another practical issue is how to ﬁnd the
representation of newly arrived vertices. For a new vertex i,
if its connections to the existing vertices are known, we can
obtain the empirical distribution ˆp1(·, vi) and ˆp2(·|vi) over
existing vertices. To obtain the embedding of the new ver-
tex, according to the objective function Eqn. (3) or Eqn. (6),
a straightforward way is to minimize either one of the fol-
lowing objective functions

(cid:88)

−

j∈N (i)

(cid:88)

j∈N (i)

wji log p1(vj, vi), or −

wji log p2(vj|vi), (10)

by updating the embedding of the new vertex and keeping
the embeddings of existing vertices. If no connections be-
tween the new vertex and existing vertices are observed, we
must resort to other information, such as the textual infor-
mation of the vertices, and we leave it as our future work.

5. EXPERIMENTS

We empirically evaluated the eﬀectiveness and eﬃciency
of the LINE. We applied the method to several large-scale
real-world networks of diﬀerent types, including a language
network, two social networks, and two citation networks.

5.1 Experiment Setup

Data Sets.

(1) Language network. We constructed a word co-
occurrence network from the entire set of English Wikipedia
pages. Words within every 5-word sliding window are con-
sidered to be co-occurring with each other. Words with
frequency smaller than 5 are ﬁltered out. (2) Social net-
works. We use two social networks: Flickr and Youtube2.
The Flickr network is denser than the Youtube network
(the same network as used in DeepWalk [16]). (3) Citation
Networks. Two types of citation networks are used: an au-
thor citation network and a paper citation network. We use
the DBLP data set [19]3 to construct the citation networks
between authors and between papers. The author citation
network records the number of papers written by one author
and cited by another author. The detailed statistics of these
networks are summarized into Table 1. They represent a
variety of information networks: directed and undirected,
binary and weighted. Each network contains at least half a
million nodes and millions of edges, with the largest network
containing around two million nodes and a billion edges.

Compared Algorithms.

We compare the LINE model with several existing graph
embedding methods that are able to scale up to very large
networks. We do not compare with some classical graph
embedding algorithms such as MDS, IsoMap, and Laplacian
eigenmap, as they cannot handle networks of this scale.

• Graph factorization (GF) [1]. We compare with the
matrix factorization techniques for graph factorization.
An information network can be represented as an aﬃn-
ity matrix, and is able to represent each vertex with a

wij =

(cid:88)

k∈N (i)

wik

wkj
dk

.

(9)

2Available
data-imc2007.html
3Available at http://arnetminer.org/citation

http://socialnetworks.mpi-sws.org/

at

Table 1: Statistics of the real-world information networks.

Name
Type
|V|
|E|
Avg. degree
#Labels
#train

Language Network
Wikipedia
undirected,weighted
1,985,098
1,000,924,086
504.22
7
70,000

Social Network

Flickr
undirected,binary
1,715,256
22,613,981
26.37
5
75,958

Youtube
undirected,binary
1,138,499
2,990,443
5.25
47
31,703

Citation Network
DBLP(AuthorCitation) DBLP(PaperCitation)

dircted,weighted
524,061
20,580,238
78.54
7
20,684

directed,binary
781,109
4,191,677
10.73
7
10,398

low-dimensional vector through matrix factorization.
Graph factorization is optimized through stochastic
gradient descent and is able to handle large networks.
It only applies to undirected networks.

• DeepWalk [16]. DeepWalk is an approach recently
proposed for social network embedding, which is only
applicable for networks with binary edges. For each
vertex, truncated random walks starting from the ver-
tex are used to obtain the contextual information, and
therefore only second-order proximity is utilized.

• LINE-SGD. This is the LINE model introduced in Sec-
tion 4.1 that optimizes the objective Eqn. (3) or Eqn. (6)
directly with stochastic gradient descent. With this
approach, the weights of the edges are directly multi-
plied into the gradients when the edges are sampled
for model updating. There are two variants of this ap-
proach: LINE-SGD(1st) and LINE-SGD(2nd), which
use ﬁrst- and second-order proximity respectively.

• LINE. This is the LINE model optimized through the
edge-sampling treatment introduced in Section 4.2. In
each stochastic gradient step, an edge is sampled with
the probability proportional to its weight and then
treated as binary for model updating. There are also
two variants: LINE(1st) and LINE(2nd). Like the
graph factorization, both LINE(1st) and LINE-SGD(1st)
only apply to undirected graphs. LINE(2nd) and LINE-
SGD(2nd) apply to both undirected and directed graphs.

• LINE (1st+2nd): To utilize both ﬁrst-order and second-
order proximity, a simple and eﬀective way is to con-
catenate the vector representations learned by LINE(1st)
and LINE(2nd) into a longer vector. After concate-
nation, the dimensions should be re-weighted to bal-
ance the two representations. In a supervised learning
task, the weighting of dimensions can be automatically
found based on the training data. In an unsupervised
task, however, it is more diﬃcult to set the weights.
Therefore we only apply LINE (1st+2nd) to the sce-
nario of supervised tasks.

Parameter Settings.

The mini-batch size of the stochastic gradient descent is
set as 1 for all the methods. Similar to [13], the learning rate
is set with the starting value ρ0 = 0.025 and ρt = ρ0(1−t/T ),
where T is the total number of mini-batches or edge samples.
For fair comparisons, the dimensionality of the embeddings
of the language network is set to 200, as used in word em-
bedding [13]. For other networks, the dimension is set as
128 by default, as used in [16]. Other default settings in-
clude: the number of negative samples K = 5 for LINE and

LINE-SGD; the total number of samples T = 10 billion for
LINE(1st) and LINE(2nd), T = 20 billion for GF; window
size win = 10, walk length t = 40, walks per vertex γ = 40
for DeepWalk. All the embedding vectors are ﬁnally nor-
malized by setting || (cid:126)w||2 = 1.

5.2 Quantitative Results

5.2.1 Language Network

We start with the results on the language network, which
contains two million nodes and a billion edges. Two appli-
cations are used to evaluate the eﬀectiveness of the learned
embeddings: word analogy [12] and document classiﬁcation.

Table 2: Results of word analogy on Wikipedia data.
Syntactic (%) Overall (%) Running time
Algorithm
51.93
GF
43.65
DeepWalk
63.02
SkipGram
8.50
LINE-SGD(1st)
14.49
LINE-SGD(2nd)
53.35
LINE(1st)
66.10
LINE(2nd)

Semantic (%)
61.38
50.79
69.14
9.72
20.42
58.08
73.79

2.96h
16.64h
2.82h
3.83h
3.94h
2.44h
2.55h

44.08
37.70
57.94
7.48
9.56
49.42
59.72

Word Analogy. This task is introduced by Mikolov et
al. [12]. Given a word pair (a, b) and a word c, the task aims
to ﬁnd a word d, such that the relation between c and d is
similar to the relation between a and b, or denoted as: a :
b → c :?. For instance, given a word pair (“China”,“Beijing”)
and a word “France,” the right answer should be “Paris”
because “Beijing” is the capital of “China” just as “Paris” is
the capital of “France.” Given the word embeddings, this
task is solved by ﬁnding the word d∗ whose embedding is
closest to the vector (cid:126)ub −(cid:126)ua +(cid:126)uc in terms of cosine proximity,
i.e., d∗ = argmaxd cos(((cid:126)ub − (cid:126)ua + (cid:126)uc), (cid:126)ud). Two categories of
word analogy are used in this task: semantic and syntactic.
Table 2 reports the results of word analogy using the em-
beddings of words learned on the Wikipedia corpora (Skip-
Gram) or the Wikipedia word network (all other meth-
ods). For graph factorization, the weight between each pair
of words is deﬁned as the logarithm of the number of co-
occurrences, which leads to better performance than the
original value of co-occurrences. For DeepWalk, diﬀerent
cutoﬀ thresholds are tried to convert the language network
into a binary network, and the best performance is achieved
when all the edges are kept in the network. We also com-
pare with the state-of-the-art word embedding model Skip-
Gram [12], which learns the word embeddings directly from
the original Wikipedia pages and is also implicitly a matrix
factorization approach [8]. The window size is set as 5, the
same as used for constructing the language network.

We can see that LINE(2nd) outperforms all other meth-
ods, including the graph embedding methods and the Skip-
Gram. This indicates that the second-order proximity bet-

Metric

Micro-F1

Macro-F1

Table 3: Results of Wikipedia page classiﬁcation on Wikipedia data set.
80%
81.71
81.35
82.05
78.44
79.40
81.69
82.11
83.63**
81.61
81.24
81.98
78.29
79.29
81.60
82.00
83.55**

Algorithm
GF
DeepWalk
SkipGram
LINE-SGD(1st)
LINE-SGD(2nd)
LINE(1st)
LINE(2nd)
LINE(1st+2nd)
GF
DeepWalk
SkipGram
LINE-SGD(1st)
LINE-SGD(2nd)
LINE(1st)
LINE(2nd)
LINE(1st+2nd)

20%
80.51
79.92
80.82
77.05
76.53
80.55
80.90
82.08**
80.39
79.78
80.71
76.90
76.45
80.44
80.81
81.99**

70%
81.63
81.21
81.98
78.39
79.19
81.61
82.00
83.52**
81.52
81.11
81.88
78.24
79.08
81.51
81.92
83.42**

10%
79.63
78.89
79.84
76.03
74.68
79.67
79.93
81.04**
79.49
78.78
79.74
75.85
74.70
79.54
79.82
80.94**

30%
80.94
80.41
81.28
77.57
77.54
80.94
81.31
82.58**
80.82
80.30
81.15
77.40
77.43
80.82
81.22
82.49**

40%
81.18
80.69
81.57
77.85
78.18
81.24
81.63
82.93**
81.08
80.56
81.46
77.71
78.09
81.13
81.52
82.83**

50%
81.38
80.92
81.71
78.08
78.63
81.40
81.80
83.16**
81.26
80.82
81.63
77.94
78.53
81.29
81.71
83.07**

60%
81.54
81.08
81.87
78.25
78.96
81.52
81.91
83.37**
81.40
80.97
81.78
78.12
78.83
81.43
81.82
83.29**

Signiﬁcantly outperforms GF at the: ** 0.01 and * 0.05 level, paired t-test.

90%
81.78
81.42
82.09
78.49
79.57
81.67
82.17
83.74**
81.68
81.32
82.01
78.36
79.46
81.59
82.07
83.66**

ter captures the word semantics compared to the ﬁrst-order
proximity. This is not surprising, as a high second-order
proximity implies that two words can be replaced in the
same context, which is a stronger indicator of similar se-
mantics than ﬁrst-order co-occurrences. It is intriguing that
the LINE(2nd) outperforms the state-of-the-art word em-
bedding model trained on the original corpus. The reason
may be that a language network better captures the global
structure of word co-occurrences than the original word se-
quences. Among other methods, both graph factorization
and LINE(1st) signiﬁcantly outperform DeepWalk even if
DeepWalk explores second-order proximity. This is because
DeepWalk has to ignore the weights (i.e., co-occurrences) of
the edges, which is very important in a language network.
The performance by the LINE models directly optimized
with SGD is much worse, because the weights of the edges
in the language network diverge, which range from a single
digit to tens of thousands, making the learning process suf-
fer. The LINE optimized by the edge-sampling treatment
eﬀectively addresses this problem, and performs very well
using either ﬁrst-order or second-order proximity.

All the models are run on a single machine with 1T mem-
ory, 40 CPU cores at 2.0GHZ using 16 threads. Both the
LINE(1st) and LINE(2nd) are quite eﬃcient, which take
less than 3 hours to process such a network with 2 million
nodes and a billion edges. Both are at least 10% faster than
graph factorization, and much more eﬃcient than DeepWalk
(ﬁve times slower). The reason that LINE-SGDs are slightly
slower is that a threshold-cutting technique has to be applied
to prevent the gradients from exploding.

Document Classiﬁcation. Another way to evaluate the
quality of the word embeddings is to use the word vectors to
compute document representation, which can be evaluated
with document classiﬁcation tasks. To obtain document vec-
tors, we choose a very simple approach, taking the average
of the word vector representations in that document. This is
because we aim to compare the word embeddings with diﬀer-
ent approaches instead of ﬁnding the best method for docu-
ment embeddings. The readers can ﬁnd advanced document
embedding approaches in [7]. We download the abstracts
of Wikipedia pages from http://downloads.dbpedia.org/
3.9/en/long_abstracts_en.nq.bz2 and the categories of
these pages from http://downloads.dbpedia.org/3.9/en/
article_categories_en.nq.bz2. We choose 7 diverse cate-
gories for classiﬁcation including “Arts,”“History,”“Human,”
“Mathematics,” “Nature,” “Technology,” and “Sports.” For

each category, we randomly select 10,000 articles, and ar-
ticles belonging to multiple categories are discarded. We
randomly sample diﬀerent percentages of the labeled doc-
uments for training and use the rest for evaluation. All
document vectors are used to train a one-vs-rest logistic re-
gression classiﬁer using the LibLinear package4. We report
the classiﬁcation metrics Micro-F1 and Macro-F1 [11]. The
results are averaged over 10 diﬀerent runs by sampling dif-
ferent training data.

Table 3 reports the results of Wikipedia page classiﬁca-
tion. Similar conclusion can be made as in the word anal-
ogy task. The graph factorization outperforms DeepWalk
as DeepWalk ignores the weights of the edges. The LINE-
SGDs perform worse due to the divergence of the weights of
the edges. The LINE optimized by the edge-sampling treat-
ment performs much better than directly deploying SGD.
The LINE(2nd) outperforms LINE(1st) and is slightly bet-
ter than the graph factorization. Note that with the su-
pervised task, it is feasible to concatenate the embeddings
learned with LINE(1st) and LINE(2nd). As a result, the
LINE(1st+2nd) method performs signiﬁcantly better than
all other methods. This indicates that the ﬁrst-order and
second-order proximities are complementary to each other.
To provide the readers more insight about the ﬁrst-order
and second-order proximities, Table 4 compares the most
similar words to a given word using ﬁrst-order and second-
order proximity. We can see that by using the contextual
proximity, the most similar words returned by the second-
order proximity are all semantically related words. The
most similar words returned by the ﬁrst-order proximity are
a mixture of syntactically and semantically related words.

Table 4: Comparison of most similar words using 1st-order
and 2nd-order proximity.

information

Word

good

graph

learn

Similarity
1st
2nd
1st
2nd
1st
2nd
1st
2nd

Top similar words
luck bad faith assume nice
decent bad excellent lousy reasonable
provide provides detailed facts veriﬁable
infomation informaiton informations nonspammy animecons
graphs algebraic ﬁnite symmetric topology
graphs subgraph matroid hypergraph undirected
teach learned inform educate how
learned teach relearn learnt understand

5.2.2

Social Network

Compared with the language networks, the social net-
works are much sparser, especially the Youtube network.
We evaluate the vertex embeddings through a multi-label
4http://www.csie.ntu.edu.tw/~cjlin/liblinear/

Table 5: Results of multi-label classiﬁcation on the Flickr network.

Metric

Micro-F1

Macro-F1

Algorithm
GF
DeepWalk
DeepWalk(256dim)
LINE(1st)
LINE(2nd)
LINE(1st+2nd)
GF
DeepWalk
DeepWalk(256dim)
LINE(1st)
LINE(2nd)
LINE(1st+2nd)

10%
53.23
60.38
60.41
63.27
62.83
63.20**
48.66
58.60
59.00
62.14
61.46
62.23**

20%
53.68
60.77
61.09
63.69
63.24
63.97**
48.73
58.93
59.59
62.53
61.82
62.95**

30%
53.98
60.90
61.35
63.82
63.34
64.25**
48.84
59.04
59.80
62.64
61.92
63.20**

40%
54.14
61.05
61.52
63.92
63.44
64.39**
48.91
59.18
59.94
62.74
62.02
63.35**

50%
54.32
61.13
61.69
63.96
63.55
64.53**
49.03
59.26
60.09
62.78
62.13
63.48**

60%
54.38
61.18
61.76
64.03
63.55
64.55**
49.03
59.29
60.17
62.82
62.12
63.48**

70%
54.43
61.19
61.80
64.06
63.59
64.61**
49.07
59.28
60.18
62.86
62.17
63.55**

80%
54.50
61.29
61.91
64.17
63.66
64.75**
49.08
59.39
60.27
62.96
62.23
63.69**

90%
54.48
61.22
61.83
64.10
63.69
64.74**
49.02
59.30
60.18
62.89
62.25
63.68**

Signiﬁcantly outperforms DeepWalk at the: ** 0.01 and * 0.05 level, paired t-test.

classiﬁcation task that assigns every node into one or more
communities. Diﬀerent percentages of the vertices are ran-
domly sampled for training and the rest are used for evalu-
ation. The results are averaged over 10 diﬀerent runs.
Flickr Network. Let us ﬁrst take a look at the results on
the Flickr network. We choose the most popular 5 commu-
nities as the categories of the vertices for multi-label classiﬁ-
cation. Table 5 reports the results. Again, LINE(1st+2nd)
signiﬁcantly outperforms all other methods. LINE(1st) is
slightly better than LINE(2nd), which is opposite to the re-
sults on the language network. The reasons are two fold: (1)
ﬁrst-order proximity is still more important than second-
order proximity in social network, which indicates strong
ties; (2) when the network is too sparse and the average
number of neighbors of a node is too small, the second-order
proximity may become inaccurate. We will further investi-
gate this issue in Section 5.4. LINE(1st) outperforms graph
factorization, indicating a better capability of modeling the
ﬁrst-order proximity. LINE(2nd) outperforms DeepWalk,
indicating a better capability of modeling the second-order
proximity. By concatenating the representations learned
by LINE(1st) and LINE(2nd), the performance further im-
proves, conﬁrming that the two proximities are complemen-
tary to each other.
Youtube Network. Table 6 reports the results on Youtube
network, which is extremely sparse and the average degree
is as low as 5.
In most cases with diﬀerent percentages
of training data, LINE(1st) outperforms LINE(2nd), con-
sistent with the results on the Flickr network. Due to the
extreme sparsity, the performance of LINE(2nd) is even infe-
rior to DeepWalk. By combining the representations learned
by the LINE with both the ﬁrst- and second-order proxim-
ity, the performance of LINE outperforms DeepWalk with
either 128 or 256 dimension, showing that the two proxim-
ities are complementary to each other and able to address
the problem of network sparsity.

It is interesting to observe how DeepWalk tackles the net-
work sparsity through truncated random walks, which en-
rich the neighbors or contexts of each vertex. The random
walk approach acts like a depth-ﬁrst search. Such an ap-
proach may quickly alleviate the sparsity of the neighbor-
hood of nodes by bringing in indirect neighbors, but it may
also introduce nodes that are long range away. A more rea-
sonable way is to expand the neighborhood of each vertex
using a breadth-ﬁrst search strategy, i.e., recursively adding
neighbors of neighbors. To verify this, we expand the neigh-
borhood of the vertices whose degree are less than 1,000
by adding the neighbors of neighbors until the size of the
extended neighborhood reaches 1,000 nodes. We ﬁnd that

adding more than 1,000 vertices does not further increase
the performance.

The results in the brackets in Table 6 are obtained on this
reconstructed network. The performance of GF, LINE(1st)
and LINE(2nd) all improves, especially LINE(2nd). In the
reconstructed network, the LINE(2nd) outperforms Deep-
Walk in most cases. We can also see that the performance
of LINE(1st+2nd) on the reconstructed network does not
improve too much compared with those on the original net-
work. This implies that the combination of ﬁrst-order and
second-order proximity on the original network has already
captured most information and LINE(1st+2nd) approach is
a quite eﬀective and eﬃcient way for network embedding,
suitable for both dense and sparse networks.

5.2.3 Citation Network

We present the results on two citation networks, both of
which are directed networks. Both the GF and LINE meth-
ods, which use ﬁrst-order proximity, are not applicable for
directed networks, and hence we only compare DeepWalk
and LINE(2nd). We also evaluate the vertex embeddings
through a multi-label classiﬁcation task. We choose 7 popu-
lar conferences including AAAI, CIKM, ICML, KDD, NIPS,
SIGIR, and WWW as the classiﬁcation categories. Authors
publishing in the conferences or papers published in the con-
ferences are assumed to belong to the categories correspond-
ing to the conferences.
DBLP(AuthorCitation) Network. Table 7 reports the
results on the DBLP(AuthorCitation) network. As this
network is also very sparse, DeepWalk outperforms LINE(2nd).
However, by reconstructing the network through recursively
adding neighbors of neighbors for vertices with small degrees
(smaller than 500), the performance of LINE(2nd) signif-
icantly increases and outperforms DeepWalk. The LINE
model directly optimized by stochastic gradient descent,
LINE(2nd), does not perform well as expected.
DBLP(PaperCitation) Network. Table 8 reports the re-
sults on the DBLP(PaperCitation) network. The LINE(2nd)
signiﬁcantly outperforms DeepWalk. This is because the
random walk on the paper citation network can only reach
papers along the citing path (i.e., older papers) and cannot
reach other references. Instead, the LINE(2nd) represents
each paper with its references, which is obviously more rea-
sonable. The performance of LINE(2nd) is further improved
when the network is reconstructed by enriching the neigh-
bors of vertices with small degrees (smaller than 200).

5.3 Network Layouts

An important application of network embedding is to cre-
ate meaningful visualizations that layout a network on a

Table 6: Results of multi-label classiﬁcation on the Youtube network. The results in the brackets are on the reconstructed
network, which adds second-order neighbors (i.e., neighbors of neighbors) as neighbors for vertices with a low degree.

Metric

Algorithm

Micro-F1

Macro-F1

GF

DeepWalk
DeepWalk(256dim)

LINE(1st)

LINE(2nd)

LINE(1st+2nd)

GF

DeepWalk
DeepWalk (256dim)

LINE(1st)

LINE(2nd)

LINE(1st+2nd)

1%
25.43
(24.97)
39.68
39.94
35.43
(36.47)
32.98
(36.78)
39.01*
(40.20)
7.38
(11.01)
28.39
28.95
28.74
(29.40)
17.06
(22.18)
29.85
(29.24)

2%
26.16
(26.48)
41.78
42.17
38.08
(38.87)
36.70
(40.37)
41.89
(42.70)
8.44
(13.55)
30.96
31.79
31.24
(31.75)
21.73
(27.25)
31.93
(33.16**)

3%
26.60
(27.25)
42.78
43.19
39.33
(40.01)
38.93
(42.10)
43.14
(43.94**)
9.35
(14.93)
32.28
33.16
32.26
(32.74)
25.28
(29.87)
33.96
(35.08**)

4%
26.91
(27.87)
43.55
44.05
40.21
(40.85)
40.26
(43.25)
44.04
(44.71**)
9.80
(15.90)
33.43
34.42
33.05
(33.41)
27.36
(31.88)
35.46**
(36.45**)

5%
27.32
(28.31)
43.96
44.47
40.77
(41.33)
41.08
(43.90)
44.62
(45.19**)
10.38
(16.45)
33.92
34.93
33.30
(33.70)
28.50
(32.86)
36.25**
(37.14**)

6%
27.61
(28.68)
44.31
44.84
41.24
(41.73)
41.79
(44.44)
45.06
(45.55**)
10.79
(16.93)
34.32
35.44
33.60
(33.99)
29.59
(33.73)
36.90**
(37.69**)

7%
27.88
(29.01)
44.61
45.17
41.53
(42.05)
42.28
(44.83)
45.34
(45.87**)
11.21
(17.38)
34.83
35.99
33.86
(34.26)
30.43
(34.50)
37.48**
(38.30**)

8%
28.13
(29.21)
44.89
45.43
41.89
(42.34)
42.70
(45.18)
45.69**
(46.15**)
11.55
(17.64)
35.27
36.41
34.18
(34.52)
31.14
(35.15)
38.10**
(38.80**)

9%
28.30
(29.36)
45.06
45.65
42.07
(42.57)
43.04
(45.50)
45.91**
(46.33**)
11.81
(17.80)
35.54
36.78
34.33
(34.77)
31.81
(35.76)
38.46**
(39.15**)

10%
28.51
(29.63)
45.23
45.81
42.21
(42.73)
43.34
(45.67)
46.08**
(46.43**)
12.08
(18.09)
35.86
37.11
34.44
(34.92)
32.32
(36.19)
38.82**
(39.40**)

Signiﬁcantly outperforms DeepWalk at the: ** 0.01 and * 0.05 level, paired t-test.
Table 7: Results of multi-label classiﬁcation on DBLP(AuthorCitation) network.
40%
Algorithm
64.81
DeepWalk
60.20
LINE-SGD(2nd)
63.77
LINE(2nd)
(66.04**)
63.90
58.82
62.87
(65.05**)

70%
64.99
60.58
63.96
(66.30**)
64.09
59.28
63.07
(65.29**)

60%
64.99
60.61
63.94
(66.25**)
64.06
59.27
63.05
(65.26**)

30%
64.75
59.89
63.63
(65.85**)
63.84
58.56
62.73
(64.84**)

50%
64.92
60.44
63.84
(66.19**)
63.98
59.11
62.93
(65.19**)

20%
64.51
58.95
63.30
(65.47**)
63.60
57.63
62.38
(64.42**)

80%
65.00
60.73
64.00
(66.12**)
64.11
59.46
63.13
(65.14**)

10%
63.98
56.64
62.49
(64.69*)
63.02
55.24
61.43
(63.49*)

DeepWalk
LINE-SGD(2nd)
LINE(2nd)

90%
64.90
60.59
63.77
(66.05**)
64.05
59.37
62.95
(65.14**)

Metric

Micro-F1

Macro-F1

Metric

Micro-F1

Macro-F1

Signiﬁcantly outperforms DeepWalk at the: ** 0.01 and * 0.05 level, paired t-test.
Table 8: Results of multi-label classiﬁcation on DBLP(PaperCitation) network.
40%
Algorithm
54.75
DeepWalk
60.78
LINE(2nd)
(61.73**)
45.85
51.31
(52.20**)
Signiﬁcantly outperforms DeepWalk at the: ** 0.01 and * 0.05 level, paired t-test.

30%
54.24
60.29
(61.46**)
45.34
50.84
(51.92**)

50%
55.07
60.94
(61.85**)
46.20
51.61
(52.40**)

70%
55.48
61.39
(62.21**)
46.51
51.94
(52.78**)

80%
55.42
61.39
(62.25**)
46.36
51.89
(52.70**)

10%
52.83
58.42
(60.10**)
43.74
48.74
(50.22**)

20%
53.80
59.58
(61.06**)
44.85
50.10
(51.41**)

60%
55.13
61.20
(62.10**)
46.25
51.77
(52.59**)

DeepWalk
LINE(2nd)

90%
55.90
61.79
(62.80**)
46.73
52.16
(53.02**)

(a) GF

(b) DeepWalk
Figure 2: Visualization of the co-author network. The authors are mapped to the 2-D space using the t-SNE package with
learned embeddings as input. Color of a node indicates the community of the author. Red: “data Mining,” blue: “machine
learning,” green: “computer vision.”

(c) LINE(2nd)

two dimensional space. We visualize a co-author network
extracted from the DBLP data. We select some conferences
from three diﬀerent research ﬁelds: WWW, KDD from “data
mining,” NIPS, ICML from “machine learning,” and CVPR,
ICCV from “computer vision.” The co-author network is
built from the papers published in these conferences. Au-
thors with degree less than 3 are ﬁltered out, and ﬁnally the
network contains 18,561 authors and 207,074 edges. Laying
out this co-author network is very challenging as the three
research ﬁelds are very close to each other. We ﬁrst map the
co-author network into a low-dimensional space with diﬀer-
ent embedding approaches and then further map the low-

dimensional vectors of the vertices to a 2-D space with the
t-SNE package [21]. Fig. 2 compares the visualization results
with diﬀerent embedding approaches. The visualization us-
ing graph factorization is not very meaningful, in which the
authors belonging to the same communities are not clustered
together. The result of DeepWalk is much better. However,
many authors belonging to diﬀerent communities are clus-
tered tightly into the center area, most of which are high
degree vertices. This is because DeepWalk uses a random
walk based approach to enrich the neighbors of the vertices,
which brings in a lot of noise due to the randomness, es-
pecially for vertices with higher degrees. The LINE(2nd)

performs quite well and generates meaningful layout of the
network (nodes with same colors are distributed closer).

5.4 Performance w.r.t. Network Sparsity

(a) Sparsity.

(b) Degree of vertex.

Figure 3: Performance w.r.t. network sparsity.

In this subsection, we formally analyze the performance
of the above models w.r.t.
the sparsity of networks. We
use the social networks as examples. We ﬁrst investigate
how the sparsity of the networks aﬀects the LINE(1st) and
LINE(2nd). Fig. 3(a) shows the results w.r.t. the percentage
of links on the Flickr network. We choose Flickr network
as it is much denser than the Youtube network. We ran-
domly select diﬀerent percentages of links from the original
network to construct networks with diﬀerent levels of spar-
sity. We can see that in the beginning, when the network is
very sparse, the LINE(1st) outperforms LINE(2nd). As we
gradually increase the percentage of links, the LINE(2nd)
begins to outperform the LINE(1st). This shows that the
second-order proximity suﬀers when the network is extremely
sparse, and it outperforms ﬁrst-order proximity when there
are suﬃcient nodes in the neighborhood of a node.

Fig. 3(b) shows the performance w.r.t. the degrees of the
vertices on both the original and reconstructed Youtube
networks. We categorize the vertices into diﬀerent groups
according to their degrees including (0, 1], [2, 3], [4, 6], [7, 12],
[13, 30], [31, +∞), and then evaluate the performance of ver-
tices in diﬀerent groups. Overall, the performance of dif-
ferent models increases when the degrees of the vertices in-
crease. In the original network, the LINE(2nd) outperforms
LINE(1st) except for the ﬁrst group, which conﬁrms that the
second-order proximity does not work well for nodes with a
low degree. In the reconstructed dense network, the perfor-
mance of the LINE(1st) or LINE(2nd) improves, especially
the LINE(2nd) that preserves the second-order proximity.
We can also see that the LINE(2nd) model on the recon-
structed network outperforms DeepWalk in all the groups.

5.5 Parameter Sensitivity

Next, we investigate the performance w.r.t. the parame-
ter dimension d and the converging performance of diﬀerent
models w.r.t the number of samples on the reconstructed
Youtube network. Fig. 4(a) reports the performance of the
LINE model w.r.t. the dimension d. We can see that the
performance of the LINE(1st) or LINE(2nd) drops when the
dimension becomes too large. Fig. 4(b) shows the results
of the LINE and DeepWalk w.r.t. the number of samples
during the optimization. The LINE(2nd) consistently out-
performs LINE(1st) and DeepWalk, and both the LINE(1st)
and LINE(2nd) converge much faster than DeepWalk.

(a) #Dimension.

(b) #Samples.

Figure 4: Sensitivity w.r.t. dimension and samples.

5.6 Scalability

(a) Speed up v.s. #threads. (b) Micro-F1 v.s. #threads.
Figure 5: Performance w.r.t. # threads.

Finally, we investigate the scalability of the LINE model
optimized by the edge-sampling treatment and asynchronous
stochastic gradient descent, which deploys multiple threads
for optimization. Fig. 5(a) shows the speed up w.r.t. the
number of threads on the Youtube data set. The speed up
is quite close to linear. Fig. 5(b) shows that the classiﬁcation
performance remains stable when using multiple threads for
model updating. The two ﬁgures together show that the
inference algorithm of the LINE model is quite scalable.

6. CONCLUSION

This paper presented a novel network embedding model
called the “LINE,” which can easily scale up to networks with
millions of vertices and billions of edges. It has carefully de-
signed objective functions that preserve both the ﬁrst-order
and second-order proximities, which are complementary to
each other. An eﬃcient and eﬀective edge-sampling method
is proposed for model inference, which solved the limitation
of stochastic gradient descent on weighted edges without
compromising the eﬃciency. Experimental results on vari-
ous real-world networks prove the eﬃciency and eﬀectiveness
of LINE. In the future, we plan to investigate higher-order
proximity beyond the ﬁrst-order and second-order proxim-
ities in the network. Besides, we also plan to investigate
the embedding of heterogeneous information networks, e.g.,
vertices with multiple types.

Acknowledgments
The authors thank the three anonymous reviewers for the
helpful comments. The co-author Ming Zhang is supported
by the National Natural Science Foundation of China (NSFC
Grant No. 61472006); Qiaozhu Mei is supported by the Na-
tional Science Foundation under grant numbers IIS-1054199
and CCF-1048168.

7. REFERENCES
[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy,

V. Josifovski, and A. J. Smola. Distributed large-scale
natural graph factorization. In Proceedings of the 22nd
international conference on World Wide Web, pages
37–48. International World Wide Web Conferences
Steering Committee, 2013.

[2] M. Belkin and P. Niyogi. Laplacian eigenmaps and

spectral techniques for embedding and clustering. In
NIPS, volume 14, pages 585–591, 2001.

[3] S. Bhagat, G. Cormode, and S. Muthukrishnan. Node
classiﬁcation in social networks. In Social Network
Data Analytics, pages 115–148. Springer, 2011.
[4] T. F. Cox and M. A. Cox. Multidimensional scaling.

CRC Press, 2000.

[5] J. R. Firth. A synopsis of linguistic theory, 1930–1955.
In J. R. Firth (Ed.), Studies in linguistic analysis,
pages 1–32.

[6] M. S. Granovetter. The strength of weak ties.

American journal of sociology, pages 1360–1380, 1973.

[7] Q. Le and T. Mikolov. Distributed representations of
sentences and documents. In Proceedings of The 31st
International Conference on Machine Learning, pages
1188–1196, 2014.

[8] O. Levy and Y. Goldberg. Neural word embedding as
implicit matrix factorization. In Advances in Neural
Information Processing Systems, pages 2177–2185,
2014.

[9] A. Q. Li, A. Ahmed, S. Ravi, and A. J. Smola.

Reducing the sampling complexity of topic models. In
Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 891–900. ACM, 2014.

[10] D. Liben-Nowell and J. Kleinberg. The link-prediction
problem for social networks. Journal of the American
society for information science and technology,
58(7):1019–1031, 2007.

[11] C. D. Manning, P. Raghavan, and H. Sch¨utze.

Introduction to information retrieval, volume 1.
Cambridge university press Cambridge, 2008.

[12] T. Mikolov, K. Chen, G. Corrado, and J. Dean.

Eﬃcient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781, 2013.

[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and
phrases and their compositionality. In Advances in
Neural Information Processing Systems, pages
3111–3119, 2013.

[14] S. A. Myers, A. Sharma, P. Gupta, and J. Lin.

Information network or social network?: the structure
of the twitter follow graph. In Proceedings of the
companion publication of the 23rd international
conference on World wide web companion, pages
493–498. International World Wide Web Conferences
Steering Committee, 2014.

[15] L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
1999.

[16] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk:
Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 701–710. ACM, 2014.

[17] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A

lock-free approach to parallelizing stochastic gradient
descent. In Advances in Neural Information Processing
Systems, pages 693–701, 2011.

[18] S. T. Roweis and L. K. Saul. Nonlinear dimensionality

reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000.

[19] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
Arnetminer: extraction and mining of academic social
networks. In Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 990–998. ACM, 2008.

[20] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A

global geometric framework for nonlinear
dimensionality reduction. Science,
290(5500):2319–2323, 2000.

[21] L. Van der Maaten and G. Hinton. Visualizing data
using t-sne. Journal of Machine Learning Research,
9(2579-2605):85, 2008.

[22] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and

S. Lin. Graph embedding and extensions: a general
framework for dimensionality reduction. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 29(1):40–51, 2007.

[23] X. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt,

U. Khandelwal, B. Norick, and J. Han. Personalized
entity recommendation: A heterogeneous information
network approach. In Proceedings of the 7th ACM
international conference on Web search and data
mining, pages 283–292. ACM, 2014.

