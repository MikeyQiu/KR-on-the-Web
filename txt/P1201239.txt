7
1
0
2
 
b
e
F
 
6
2
 
 
]

G
L
.
s
c
[
 
 
4
v
5
4
7
5
0
.
8
0
6
1
:
v
i
X
r
a

RETAIN: An Interpretable Predictive Model for
Healthcare using Reverse Time Attention Mechanism

Edward Choi∗, Mohammad Taha Bahadori∗, Joshua A. Kulas∗,
Andy Schuetz†, Walter F. Stewart†, Jimeng Sun∗
† Sutter Health
∗ Georgia Institute of Technology

{mp2893,bahadori,jkulas3}@gatech.edu,
{schueta1,stewarwf}@sutterhealth.org, jsun@cc.gatech.edu

Abstract
Accuracy and interpretability are two dominant features of successful predictive
models. Typically, a choice must be made in favor of complex black box models
such as recurrent neural networks (RNN) for accuracy versus less accurate but
more interpretable traditional models such as logistic regression. This tradeoff
poses challenges in medicine where both accuracy and interpretability are impor-
tant. We addressed this challenge by developing the REverse Time AttentIoN
model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN
achieves high accuracy while remaining clinically interpretable and is based on
a two-level neural attention model that detects inﬂuential past visits and signiﬁ-
cant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics
physician practice by attending the EHR data in a reverse time order so that recent
clinical visits are likely to receive higher attention. RETAIN was tested on a large
health system EHR dataset with 14 million visits completed by 263K patients over
an 8 year period and demonstrated predictive accuracy and computational scalabil-
ity comparable to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.

1

Introduction

The broad adoption of Electronic Health Record (EHR) systems has opened the possibility of
applying clinical predictive models to improve the quality of clinical care. Several systematic reviews
have underlined the care quality improvement using predictive analysis [7, 22, 5, 18]. EHR data
can be represented as temporal sequences of high-dimensional clinical variables (e.g., diagnoses,
medications and procedures), where the sequence ensemble represents the documented content of
medical visits from a single patient. Traditional machine learning tools summarize this ensemble into
aggregate features, ignoring the temporal and sequence relationships among the feature elements.
The opportunity to improve both predictive accuracy and interpretability is likely to derive from
effectively modeling temporality and high-dimensionality of these event sequences.

Accuracy and interpretability are two dominant features of successful predictive models. There is a
common belief that one has to trade accuracy for interpretability using one of three types of traditional
models [6]: 1) identifying a set of rules (e.g. via decision trees [24]), 2) case-based reasoning by
ﬁnding similar patients (e.g. via k-nearest neighbors [16] and distance metric learning [33]), and 3)
identifying a list of risk factors (e.g. via LASSO coefﬁcients [15]). While interpretable, all of these
models rely on aggregated features, ignoring the temporal relation among features inherent to EHR
data. As a consequence, model accuracy is sub-optimal. Latent-variable time-series models, such as
[31, 32], account for temporality, but often have limited interpretation due to abstract state variables.

Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential
EHR data to predict diagnoses [27] and model encounter sequences [11, 14]. But, the gain in accuracy

29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

(a) Standard attention model

(b) RETAIN model

Figure 1: Common attention models vs. RETAIN, using folded diagrams of RNNs. (a) Standard
attention mechanism: the recurrence on the hidden state vector vi hinders interpretation of the model.
(b) Attention mechanism in RETAIN: The recurrence is on the attention generation components (hi or
gi) while the hidden state vi is generated by a simpler more interpretable output.

from use of RNNs is at the cost of model output that is notoriously difﬁcult to interpret. While
there have been several attempts at directly interpreting RNNs [17, 23, 8], these methods are not
sufﬁciently developed for application in clinical care.

We have addressed this limitation using a modeling strategy known as RETAIN, a two-level neural
attention model for sequential data that provides detailed interpretation of the prediction results while
retaining the prediction accuracy comparable to RNN. To this end, RETAIN relies on an attention
mechanism modeled to represent the behavior of physicians during an encounter. A distinguishing
feature of RETAIN (see Figure 1) is to leverage sequence information using an attention generation
mechanism, while learning an interpretable representation. And emulating physician behaviors,
RETAIN examines a patient’s past visits in reverse time order, facilitating a more stable attention
generation. As a result, RETAIN identiﬁes the most meaningful visits and quantiﬁes visit speciﬁc
features that contribute to the prediction.

RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K
patients over an 8 year period. We compared predictive accuracy of RETAIN to traditional machine
learning methods and to RNN variants using a case-control dataset to predict a future diagnosis of
heart failure. The comparative analysis demonstrates that RETAIN achieves comparable performance
to RNN in both accuracy and speed and signiﬁcantly outperforms traditional models. Moreover,
using a concrete case study and visualization method, we demonstrate how RETAIN offers an intuitive
interpretation.

2 Methodology

We ﬁrst describe the structure of sequential EHR data and our notation, then follow with a general
framework for predictive analysis in healthcare using EHR, followed by details of the RETAIN method.

EHR Structure and our Notation. The EHR data of each patient can be represented as a time-
labeled sequence of multivariate observations. Assuming we use r different variables, the n-th patient
of N total patients can be represented by a sequence of T (n) tuples (t(n)
) ∈ R × Rr, i =
1, . . . , T (n). The timestamps t(n)
denotes the time of the i-th visit of the n-th patient and T (n) the
number of visits of the n-th patient. To minimize clutter, we describe the algorithms for a single patient
and have dropped the superscript (n) whenever it is unambiguous. The goal of predictive modeling
is to predict the label at each time step yi ∈ {0, 1}s or at the end of the sequence y ∈ {0, 1}s. The
number of labels s can be more than one.

, x(n)
i

i

i

For example, in encounter sequence modeling (ESM) [11], each visit (e.g. encounter) of a patient’s
visit sequence is represented by a set of varying number of medical codes {c1, c2, . . . , cn}. cj is
the j-th code from the vocabulary C. Therefore, in ESM, the number of variables r = |C| and input

2

xi ∈ {0, 1}|C| is a binary vector where the value one in the j-th coordinate indicates that cj was
documented in i-th visit. Given a sequence of visits x1, . . . , xT , the goal of ESM is, for each time
step i, to predict the codes occurring at the next visit x2, . . . , xT +1, with the number of labels s = |C|.

In case of learning to diagnose (L2D) [27], the input vector xi consists of continuous clinical measures.
If there are r different measurements, then xi ∈ Rr. The goal of L2D is, given an input sequence
x1, . . . , xT , to predict the occurrence of a speciﬁc disease (s = 1) or multiple diseases (s > 1).
Without loss of generality, we will describe the algorithm for ESM, as L2D can be seen as a special
case of ESM where we make a single prediction at the end of the visit sequence.

In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural
network variants that can cope with the vanishing gradient problem [3], such as LSTM [21], GRU
[9], and IRNN [26], with any depth (number of hidden layers).

2.1 Preliminaries on Neural Attention Models

Attention based neural network models are being successfully applied to image processing [1, 29, 19,
34], natural language processing [2, 20, 30] and speech recognition [12]. The utility of the attention
mechanism can be seen in the language translation task [2] where it is inefﬁcient to represent an
entire sentence with one ﬁxed-size vector because neural translation machines ﬁnds it difﬁcult to
translate the given sentence represented by a single vector.

Intuitively, the attention mechanism for language translation works as follows: given a sentence of
length S in the original language, we generate h1, . . . , hS, to represent the words in the sentence. To
ﬁnd the j-th word in the target language, we generate attentions αj
i for i = 1, . . . , S for each word in
the original sentence. Then, we compute the context vector cj = (cid:80)
i hi and use it to predict the
j-th word in the target language. In general, the attention mechanism allows the model to focus on a
speciﬁc word (or words) in the given sentence when generating each word in the target language.

i αj

We rely on a conceptually similar temporal attention mechanism to generate interpretable prediction
models using EHR data. Our model framework is motivated by and mimics how doctors attend to a
patient’s needs and explore the patient record, where there is a focus on speciﬁc clinical information
(e.g., key risk factors) working from the present to the past.

2.2 Reverse Time Attention Model RETAIN

Figure 2 shows the high-level overview of our model, where a central feature is to delegate a
considerable portion of the prediction responsibility to the process for generating attention weights.
This is intended to address, in part, the difﬁculty with interpreting RNNs where the recurrent weights
feed past information to the hidden layer. Therefore, to consider both the visit-level and the variable-
level (individual coordinates of xi) inﬂuence, we use a linear embedding of the input vector xi. That
is, we deﬁne

vi = Wembxi,
(Step 1)
where vi ∈ Rm denotes the embedding of the input vector xi ∈ Rr, m the size of the embedding di-
mension, Wemb ∈ Rm×r the embedding matrix to learn. We can alternatively use more sophisticated
yet interpretable representations such as those derived from multilayer perceptron (MLP) [13, 25].
MLP has been used for representation learning in EHR data [10].

We use two sets of weights, one for the visit-level attention and the other for variable-level attention,
respectively. The scalars α1, . . . , αi are the visit-level attention weights that govern the inﬂuence of
each visit embedding v1, . . . , vi. The vectors β1, . . . , βi are the variable-level attention weights that
focus on each coordinate of the visit embedding v1,1, v1,2, . . . , v1,m, . . . , vi,1, vi,2, . . . , vi,m.

We use two RNNs, RNNα and RNNβ, to separately generate α’s and β’s as follows,

gi, gi−1, . . . , g1 = RNNα(vi, vi−1, . . . , v1),

ej = w(cid:62)

α gj + bα,

for

j = 1, . . . , i

α1, α2, . . . , αi = Softmax(e1, e2, . . . , ei)
hi, hi−1, . . . , h1 = RNNβ(vi, vi−1, . . . , v1)
for

βj = tanh (cid:0)Wβhj + bβ

(cid:1)

(Step 2)

j = 1, . . . , i,

(Step 3)

3

Figure 2: Unfolded view of RETAIN’s architecture: Given input sequence x1, . . . , xi, we predict the
label yi. Step 1: Embedding, Step 2: generating α values using RNNα, Step 3: generating β values
using RNNβ, Step 4: Generating the context vector using attention and representation vectors, and
Step 5: Making prediction. Note that in Steps 2 and 3 we use RNN in the reversed time.

where gi ∈ Rp is the hidden layer of RNNα at time step i, hi ∈ Rq the hidden layer of RNNβ
at time step i and wα ∈ Rp, bα ∈ R, Wβ ∈ Rm×q and bβ ∈ Rm are the parameters to learn.
The hyperparameters p and q determine the hidden layer size of RNNα and RNNβ, respectively.
Note that for prediction at each timestamp, we generate a new set of attention vectors α and β. For
simplicity of notation, we do not include the index for predicting at different time steps. In Step 2,
we can use Sparsemax [28] instead of Softmax for sparser attention weights.

As noted, RETAIN generates the attention vectors by running the RNNs backward in time; i.e., RNNα
and RNNβ both take the visit embeddings in a reverse order vi, vi−1, . . . , v1. Running the RNN
in reversed time order also offers computational advantages since the reverse time order allows us
to generate e’s and β’s that dynamically change their values when making predictions at different
time steps i = 1, 2, . . . , T . This ensures that the attention vectors are modiﬁed at each time step,
increasing the computational stability of the attention generation process.1

Using the generated attentions, we obtain the context vector ci for a patient up to the i-th visit as
follows,

ci =

αjβj (cid:12) vj,

i
(cid:88)

j=1

(Step 4)

where (cid:12) denotes element-wise multiplication. We use the context vector ci ∈ Rm to predict the true
label yi ∈ {0, 1}s as follows,

(cid:98)yi = Softmax(Wci + b),
(Step 5)
where W ∈ Rs×m and b ∈ Rs are parameters to learn. We use the cross-entropy to calculate the
classiﬁcation loss as follows,

L(x1, . . . , xT ) = −

1
N

N
(cid:88)

n=1

1
T (n)

T (n)
(cid:88)

(cid:16)

i=1

y(cid:62)
i

(cid:17)
log((cid:98)yi) + (1 − yi)(cid:62) log(1 − (cid:98)yi)

(1)

where we sum the cross entropy errors from all dimensions of (cid:98)yi. In case of real-valued output
yi ∈ Rs, we can change the cross-entropy in Eq. (1) to, for example, mean squared error.
Overall, our attention mechanism can be viewed as the inverted architecture of the standard attention
mechanism for NLP [2] where the words are encoded by RNN and the attention weights are generated
by MLP. In contrast, our method uses MLP to embed the visit information to preserve interpretability
and uses RNN to generate two sets of attention weights, recovering the sequential information as
well as mimicking the behavior of physicians. Note that we did not use the timestamp of each visit
in our formulation. Using timestamps, however, provides a small improvement in the prediction
performance. We propose a method to use timestamps in Appendix A.

1For example, feeding visit embeddings in the original order to RNNα and RNNβ will generate the same e1
and β1 for every time step i = 1, 2, . . . , T . Moreover, in many cases, a patient’s recent visit records deserve
more attention than the old records. Then we need to have ej+1 > ej which makes the process computationally
unstable for long sequences.

4

3

Interpreting RETAIN

Finding the visits that contribute to prediction are derived using the largest αi, which is straightforward.
However, ﬁnding inﬂuential variables is slightly more involved as a visit is represented by an ensemble
of medical variables, each of which can vary in its predictive contribution. The contribution of each
variable is determined by v, β and α, and interpretation of α alone informs which visit is inﬂuential
in prediction but not why.

We propose a method to interpret the end-to-end behavior of RETAIN. By keeping α and β values
ﬁxed as the attention of doctors, we analyze changes in the probability of each label yi,1, . . . , yi,s
in relation to changes in the original input x1,1, . . . , x1,r, . . . , xi,1, . . . , xi,r. The xj,k that yields the
largest change in yi,d will be the input variable with highest contribution. More formally, given the
sequence x1, . . . , xi, we are trying to predict the probability of the output vector yi ∈ {0, 1}s, which
can be expressed as follows

p(yi|x1, . . . , xi) = p(yi|ci) = Softmax (Wci + b)
(2)
where ci ∈ Rm denotes the context vector. According to Step 4, ci is the sum of the visit embeddings
v1, . . . , vi weighted by the attentions α’s and β’s. Therefore Eq (2) can be rewritten as follows,

p(yi|x1, . . . , xi) = p(yi|ci) = Softmax

W

αjβj (cid:12) vj

+ b

(3)

(cid:18)

(cid:16) i
(cid:88)

j=1

(cid:17)

(cid:19)

Using the fact that the visit embedding vi is the sum of the columns of Wemb weighted by each
element of xi, Eq (3) can be rewritten as follows,

p(yi|x1, . . . , xi) = Softmax

W

αjβj (cid:12)

xj,kWemb[:, k]

+ b

(cid:18)

(cid:16) i
(cid:88)

j=1

(cid:18) i

(cid:88)

r
(cid:88)

j=1

k=1

r
(cid:88)

k=1

(cid:16)

(cid:17)

(cid:17)

(cid:19)

(cid:19)

= Softmax

xj,k αjW

βj (cid:12) Wemb[:, k]

+ b

(4)

where xj,k is the k-th element of the input vector xj. Eq (4) can be completely deconstructed to the
variables at each input x1, . . . , xi, which allows for calculating the contribution ω of the k-th variable
of the input xj at time step j ≤ i, for predicting yi as follows,

ω(yi, xj,k) = αjW(βj (cid:12) Wemb[:, k])
(cid:125)

(cid:123)(cid:122)
Contribution coefﬁcient

(cid:124)

xj,k
(cid:124)(cid:123)(cid:122)(cid:125)
Input value

,

(5)

where the index i of yi is omitted in the αj and βj. As we have described in Section 2.2, we are
generating α’s and β’s at time step i in the visit sequence x1, . . . , xT . Therefore the index i is always
assumed for α’s and β’s. Additionally, Eq (5) shows that when we are using a binary input value, the
coefﬁcient itself is the contribution. However, when we are using a non-binary input value, we need
to multiply the coefﬁcient and the input value xj,k to correctly calculate the contribution.

4 Experiments

We compared performance of RETAIN to RNNs and traditional machine learning methods. Given
space constraints, we only report the results on the learning to diagnose (L2D) task and summarize the
encounter sequence modeling (ESM) in Appendix C. The RETAIN source code is publicly available
at https://github.com/mp2893/retain.

4.1 Experimental setting
Source of data: The dataset consists of electronic health records from Sutter Health. The patients
are 50 to 80 years old adults chosen for a heart failure prediction model study. From the encounter
records, medication orders, procedure orders and problem lists, we extracted visit records consisting
of diagnosis, medication and procedure codes. To reduce the dimensionality while preserving the
clinical information, we used existing medical groupers to aggregate the codes into input variables.
The details of the medical groupers are given in the Appendix B. A proﬁle of the dataset is summarized
in Table 1.

5

Table 1: Statistics of EHR dataset. (D:Diagnosis, R:Medication, P:Procedure)

# of patients
# of visits
Avg. # of visits per patient
# of medical code groups

263,683
14,366,030
54.48

Avg. # of codes in a visit
Max # of codes in a visit
Avg. # of Dx codes in a visit

615 (D:283, R:94, P:238) Max # of Dx in a visit

3.03
62
1.83
42

Implementation details: We implemented RETAIN with Theano 0.8 [4]. For training the model, we
used Adadelta [35] with the mini-batch of 100 patients. The training was done in a machine equipped
with Intel Xeon E5-2630, 256GB RAM, two Nvidia Tesla K80’s and CUDA 7.5.

Baselines: For comparison, we completed the following models.

• Logistic regression (LR): We compute the counts of medical codes for each patient based on all
her visits as input variables and normalize the vector to zero mean and unit variance. We use the
resulting vector to train the logistic regression.

• MLP: We use the same feature construction as LR, but put a hidden layer of size 256 between

the input and output.

• RNN: RNN with two hidden layers of size 256 implemented by the GRU. Input sequences
x1, . . . , xi are used. Logistic regression is applied to the top hidden layer. We use two layers of
RNN of to match the model complexity of RETAIN.

• RNN+αM : One layer single directional RNN (hidden layer size 256) along time to generate the
input embeddings v1, . . . , vi. We use the MLP with a single hidden layer of size 256 to generate
the visit-level attentions α1, . . . , αi. We use the input embeddings v1, . . . , vi as the input to the
MLP. This baseline corresponds to Figure 1a.

• RNN+αR: This is similar to RNN+αM but uses the reverse-order RNN (hidden layer size 256)
to generate the visit-level attentions α1, . . . , αi. We use this baseline to conﬁrm the effectiveness
of generating the attentions using reverse time order.

The comparative visualization of the baselines are provided in Appendix D. We use the same
implementation and training method for the baselines as described above. The details on the hyper-
parameters, regularization and drop-out strategies for the baselines are described in Appendix B.

Evaluation measures: Model accuracy was measured by:

• Negative log-likelihood that measures the model loss on the test set. The loss can be calculated

by Eq (1).

• Area Under the ROC Curve (AUC) of comparing (cid:98)yi with the true label yi. AUC is more
robust to imbalanced positive/negative prediction labels, making it appropriate for evaluation of
classiﬁcation accuracy in the heart failure prediction task.

We also report the bootstrap (10,000 runs) estimate of the standard deviation of the evaluation
measures.

4.2 Heart Failure Prediction

Objective: Given a visit sequence x1, . . . , xT , we predicted if a primary care patient will be
diagnosed with heart failure (HF). This is a special case of ESM with a single disease outcome at
the end of the sequence. Since this is a binary prediction task, we use the logistic sigmoid function
instead of the Softmax in Step 5.

Cohort construction: From the source dataset, 3,884 cases are selected and approximately 10
controls are selected for each case (28,903 controls). The case/control selection criteria are fully
described in the supplementary section. Cases have index dates to denote the date they are diagnosed
with HF. Controls have the same index dates as their corresponding cases. We extract diagnosis codes,
medication codes and procedure codes in the 18-months window before the index date.

Training details: The patient cohort was divided into the training, validation and test sets in a
0.75:0.1:0.15 ratio. The validation set was used to determine the values of the hyper-parameters. See
Appendix B for details of hyper-parameter tuning.

6

Table 2: Heart failure prediction performance of RETAIN and the baselines

Train Time / epoch Test Time

Model
LR
MLP
RNN
RNN+αM
RNN+αR
RETAIN

Test Neg Log Likelihood
0.3269 ± 0.0105
0.2959 ± 0.0083
0.2577 ± 0.0082
0.2691 ± 0.0082
0.2605 ± 0.0088
0.2562 ± 0.0083

AUC
0.7900 ± 0.0111
0.8256 ± 0.0096
0.8706 ± 0.0080
0.8624 ± 0.0079
0.8717 ± 0.0080
0.8705 ± 0.0081

0.15s
0.25s
10.3s
6.7s
10.4s
10.8s

0.11s
0.11s
0.57s
0.48s
0.62s
0.63s

Results: Logistic regression and MLP underperformed compared to the four temporal learning
algorithms (Table 2). RETAIN is comparable to the other RNN variants in terms of prediction
performance while offering the interpretation beneﬁt.

Note that RNN+αR model are a degenerated version of RETAIN with only scalar attention, which is
still a competitive model as shown in table 2. This conﬁrms the efﬁciency of generating attention
weights using the RNN. However, RNN+αR model only provides scalar visit-level attention, which
is not sufﬁcient for healthcare applications. Patients often receives several medical codes at a single
visit, and it will be important to distinguish their relative importance to the target. We show such a
case study in section 4.3.

Table 2 also shows the scalability of RETAIN, as its training time (the number of seconds to train
the model over the entire training set once) is comparable to RNN. The test time is the number
of seconds to generate the prediction output for the entire test set. We use the mini-batch of 100
patients when assessing both training and test times. RNN takes longer than RNN+αM because of its
two-layer structure, whereas RNN+αM uses a single layer RNN. The models that use two RNNs
(RNN, RNN+αR, RETAIN)2 take similar time to train for one epoch. However, each model required
a different number of epochs to converge. RNN typically takes approximately 10 epochs, RNN+αM
and RNN+αR 15 epochs and RETAIN 30 epochs. Lastly, training the attention models (RNN+αM ,
RNN+αR and RETAIN) for ESM would take considerably longer than L2D, because ESM modeling
generates context vectors at each time step. RNN, on the other hand, does not require additional
computation other than embedding the visit to its hidden layer to predict target labels at each time
step. Therefore, in ESM, the training time of the attention models will increase linearly in relation to
the length of the input sequence.

4.3 Model Interpretation for Heart Failure Prediction
We evaluated the interpretability of RETAIN in the HF prediction task by choosing a HF patient from
the test set and calculating the contribution of the variables (medical codes in this case) to diagnostic
prediction. The patient suffered from skin problems, skin disorder (SD), benign neoplasm (BN),
excision of skin lesion (ESL), for some time before showing symptoms of HF, cardiac dysrhythmia
(CD), heart valve disease (HVD) and coronary atherosclerosis (CA), and then a diagnosis of HF
(Figure 3). We can see that skin-related codes from the earlier visits made little contribution to HF
prediction as expected. RETAIN properly puts much attention to the HF-related codes that occurred in
recent visits.

To conﬁrm RETAIN’s ability to exploit the sequence information of the EHR data, we reverse the visit
sequence of Figure 3a and feed it to RETAIN. Figure 3b shows the contribution of the medical codes
of the reversed visit record. HF-related codes in the past are still making positive contributions, but
not as much as they did in Figure 3a. Figure 3b also emphasizes RETAIN’s superiority to interpretable,
but stationary models such as logistic regression. Stationary models often aggregate past information
and remove the temporality from the input data, which can mistakenly lead to the same risk prediction
for Figure 3a and 3b. RETAIN, however, can correctly digest the sequence information and calculates
the HF risk score of 9.0%, which is signiﬁcantly lower than that of Figure 3a.

Figure 3c shows how the contributions of codes change when selected medication data are used in
the model. We added two medications from day 219: antiarrhythmics (AA) and anticoagulants (AC),
both of which are used to treat cardiac dysrhythmia (CD). The two medications make a negative
contributions, especially towards the end of the record. The medications decreased the positive
contributions of heart valve disease and cardiac dysrhythmia in the last visit. Indeed, the HF risk

2The RNN baseline uses two layers of RNN, RNN+αR uses one for visit embedding and one for generating

α, RETAIN uses each for generating α and β

7

Figure 3: (a) Temporal visualization of a patient’s visit records where the contribution of variables for
diagnosis of heart failure (HF) is summarized along the x-axis (i.e. time) with the y-axis indicating
the magnitude of visit and code speciﬁc contributions to HF diagnosis. (b) We reverse the order of
the visit sequence to see if RETAIN can properly take into account the modiﬁed sequence information.
(c) Medication codes are added to the visit record to see how it changes the behavior of RETAIN.

prediction (0.2165) of Figure 3c is lower than that of Figure 3a (0.2474). This suggests that taking
proper medications can help the patient in reducing their HF risk.

5 Conclusion

Our approach to modeling event sequences as predictors of HF diagnosis suggest that complex
models can offer both superior predictive accuracy and more precise interpretability. Given the power
of RNNs for analyzing sequential data, we proposed RETAIN, which preserves RNN’s predictive
power while allowing a higher degree of interpretation. The key idea of RETAIN is to improve
the prediction accuracy through a sophisticated attention generation process, while keeping the
representation learning part simple for interpretation, making the entire algorithm accurate and
interpretable. RETAIN trains two RNN in a reverse time order to efﬁciently generate the appropriate
attention variables. For future work, we plan to develop an interactive visualization system for
RETAIN and evaluating RETAIN in other healthcare applications.

References

In ICLR, 2015.

[1] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. In ICLR, 2015.
[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.

[3] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.

Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.

[4] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of SciPy, 2010.
[5] A. D. Black, J. Car, C. Pagliari, C. Anandan, K. Cresswell, T. Bokun, B. McKinstry, R. Procter, A. Majeed,
and A. Sheikh. The impact of ehealth on the quality and safety of health care: a systematic overview. PLoS
Med, 8(1):e1000387, 2011.

[6] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad. Intelligible models for healthcare:

Predicting pneumonia risk and hospital 30-day readmission. In KDD, 2015.

[7] B. Chaudhry, J. Wang, S. Wu, M. Maglione, W. Mojica, E. Roth, S. C. Morton, and P. G. Shekelle.
Systematic review: impact of health information technology on quality, efﬁciency, and costs of medical
care. Annals of internal medicine, 144(10):742–752, 2006.

8

[8] Z. Che, S. Purushotham, R. Khemani, and Y. Liu. Distilling knowledge from deep networks with

applications to healthcare domain. arXiv preprint arXiv:1512.03542, 2015.

[9] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning
phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014.

[10] E. Choi, M. T. Bahadori, E. Searles, C. Coffey, and J. Sun. Multi-layer representation learning for medical

[11] E. Choi, M. T. Bahadori, and J. Sun. Doctor ai: Predicting clinical events via recurrent neural networks.

concepts. In KDD, 2016.

arXiv preprint arXiv:1511.05942, 2015.

recognition. In NIPS, pages 577–585, 2015.

University of Montreal, 2009.

[12] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. Attention-based models for speech

[13] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.

[14] C. Esteban, O. Staeck, Y. Yang, and V. Tresp. Predicting clinical events by combining static and dynamic

information using recurrent neural networks. arXiv preprint arXiv:1602.02685, 2016.

[15] A. S. Fleisher, B. B. Sowell, C. Taylor, A. C. Gamst, R. C. Petersen, L. J. Thal, and f. t. A. D. C. Study.
Clinical predictors of progression to Alzheimer disease in amnestic mild cognitive impairment. Neurology,
68(19):1588–1595, 2007.

[16] B. Gallego, S. R. Walter, R. O. Day, A. G. Dunn, V. Sivaraman, N. Shah, C. A. Longhurst, and E. Coiera.
Bringing cohort studies to the bedside: framework for a ’green button’ to support clinical decision-making.
Journal of Comparative Effectiveness Research, pages 1–7, 2015.

[17] J. Ghosh and V. Karamcheti. Sequence learning with recurrent networks: analysis of internal representations.

In Aerospace Sensing, pages 449–460, 1992.

[18] C. L. Goldzweig, A. Towﬁgh, M. Maglione, and P. G. Shekelle. Costs and beneﬁts of health information

technology: new trends from the literature. Health affairs, 28(2):w282–w293, 2009.

[19] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image

generation. arXiv preprint arXiv:1502.04623, 2015.

[20] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching

machines to read and comprehend. In NIPS, pages 1684–1692, 2015.

[21] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

[22] A. K. Jha, C. M. DesRoches, E. G. Campbell, K. Donelan, S. R. Rao, T. G. Ferris, A. Shields, S. Rosenbaum,

and D. Blumenthal. Use of electronic health records in us hospitals. N Engl J Med, 2009.

[23] A. Karpathy, J. Johnson, and F.-F. Li. Visualizing and understanding recurrent networks. arXiv preprint

arXiv:1506.02078, 2015.

[24] A. N. Kho et al. Use of diverse electronic medical record systems to identify genetic risk for type 2 diabetes

within a genome-wide association study. JAMIA, 19(2):212–218, 2012.

[25] Q. V. Le. Building high-level features using large scale unsupervised learning. In ICASSP, 2013.

[26] Q. V. Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent networks of rectiﬁed linear units.

arXiv preprint arXiv:1504.00941, 2015.

[27] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell. Learning to Diagnose with LSTM Recurrent Neural

Networks. In ICLR, 2016.

classiﬁcation. In ICML, 2016.

[28] A. F. Martins and R. F. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label

[29] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of visual attention. In NIPS, 2014.

[30] A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence summarization.

In EMNLP, 2015.

[31] S. Saria, D. Koller, and A. Penn. Learning individual and population level traits from clinical temporal

data. In NIPS, Predictive Models in Personalized Medicine workshop, 2010.

[32] P. Schulam and S. Saria. A probabilistic graphical model for individualizing prognosis in chronic, complex

diseases. In AMIA, volume 2015, page 143, 2015.

[33] J. Sun, F. Wang, J. Hu, and S. Edabollahi. Supervised patient similarity measure of heterogeneous patient

records. ACM SIGKDD Explorations Newsletter, 14(1):16–24, 2012.

[34] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell:

Neural image caption generation with visual attention. In ICML, 2015.

[35] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.

9

A A method to use the timestamps

i

As before, we use t(n)
to represent the timestamp of the i-th visit of the n-th patient. In the following,
we suppress the superscript (n) to avoid cluttered notation. Note that the timestamp ti can be anything
that provides the temporal information of the i-th visit: the number of days from the ﬁrst visit, the
number of days between two consecutive visits, or the number of days until the index date of some
event such as heart failure diagnosis.

In order to use the timestamps, we modify Step 2 and Step 3 in Section 2.2 as follows:

gi, gi−1, . . . , g1 = RNNα(v

(cid:48)

(cid:48)

i, v
α gj + bα,

i−1, . . . , v
for

(cid:48)

1),

j = 1, . . . , i

ej = w(cid:62)

α1, α2, . . . , αi = Softmax(e1, e2, . . . , ei)

hi, hi−1, . . . , h1 = RNNβ(v

(cid:48)

(cid:48)

i, v

i−1, . . . , v

βj = tanh (cid:0)Wβhj + bβ
i = [vi, ti]

(cid:48)

(cid:1)

where v

(cid:48)

1)
for

j = 1, . . . , i,

(cid:48)

where we use v
i, the concatenation of the visit embedding vi and the timestamp ti, to generate the
(cid:48)
attentions α and β. However, when obtaining the context vector ci as per Step 4, we use vi, not v
i
to match the dimensionality. The entire process could be understood such that we use the temporal
information not to embed each visit, but to calculate the attentions for the entire visit sequence. This
is consistent with our modeling approach where we lose the sequential information in embedding the
visit with MLP, then recover the sequential information by generating the attentions using the RNN.
By using the temporal information, speciﬁcally the log of the number of days from the ﬁrst visit, we
were able to improve the heart failure prediction AUC by 0.003 without any hyper-parameter tuning.

B Details of the experiment settings

B.1 Hyper-parameter Tuning

We used the validation set to tune the hyper-parameters: visit embedding size m, RNNα’s hidden
layer size p, RNNβ’s hidden layer size q, L2 regularization coefﬁcient, and drop-out rates.

L2 regularization was applied to all weights except the ones in RNNα and RNNβ. Two separate
drop-outs were used on the visit embedding vi and the context vector ci. We performed the random
search with predeﬁned ranges m, p, q ∈ {32, 64, 128, 200, 256}, L2 ∈ {0.1, 0.01, 0.001, 0.0001},
dropoutvi, dropoutci ∈ {0.0, 0.2, 0.4, 0.6, 0.8}. We also performed the random search with m, p
and q ﬁxed to 256.

The ﬁnal value we used to train RETAIN for heart failure prediction is m, p, q = 128, dropoutvi = 0.6,
dropoutci = 0.6 and 0.0001 for the L2 regularization coefﬁcient.

B.2 Code Grouper

Diagnosis codes, medication codes and procedure codes in the dataset are respectively using Interna-
tional Classiﬁcation of Diseases (ICD-9), Generic Product Identiﬁer (GPI) and Current Procedural
Terminology (CPT).
Diagnosis codes are grouped by Clinical Classiﬁcations Software for ICD-93 which reduces the
number of diagnosis code from approximately 14,000 to 283. Medication codes are grouped by
Generic Product Identiﬁer Drug Group4 which reduces the dimension to from approximately 151,000
to 96. Procedure codes are grouped by Clinical Classiﬁcations Software for CPT5, which reduces the
number of CPT codes from approximately 9,000 to 238.

3https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp
4http://www.wolterskluwercdi.com/drug-data/medi-span-electronic-drug-ﬁle/
5https://www.hcup-us.ahrq.gov/toolssoftware/ccs_svcsproc/ccssvcproc.jsp

10

Table 3: Qualifying ICD-9 codes for heart failure

B.3 Training Speciﬁcs of the Basline Models

• LR: We use 0.01 L2 regularization coefﬁcient for the logistic regression weight.
• MLP: We use drop-out rate 0.6 on the output of the hidden layer. We use 0.0001 L2 regularization

coefﬁcient for the hidden layer weight and the logistic regression weight.

• RNN: We use drop-out rate 0.6 on the outputs of both hidden layers. We use 0.0001 L2
regularization coefﬁcient for the logistic regression weight. The dimension size of both hidden
layers is 256.

• RNN+αM : We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of
the context vector (cid:80)
i αivi. We use 0.0001 L2 regularization coefﬁcient for the hidden layer
weight of the MLP that generates α’s and the logistic regression weight. The dimension size of
the hidden layers in both RNN and MLP is 256.

• RNN+αR: We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of
the context vector (cid:80)
i αivi. We use 0.0001 L2 regularization coefﬁcient for the hidden layer
weight of the RNN that generates α’s and the logistic regression weight. The dimension size of
the hidden layers in both RNNs is 256.

B.4 Heart Failure Case/Control Selection Criteria

Case patients were 40 to 85 years of age at the time of HF diagnosis. HF diagnosis (HFDx) is
deﬁned as: 1) Qualifying ICD-9 codes for HF appeared in the encounter records or medication orders.
Qualifying ICD-9 codes are displayed in Table 3. 2) a minimum of three clinical encounters with
qualifying ICD-9 codes had to occur within 12 months of each other, where the date of diagnosis was
assigned to the earliest of the three dates. If the time span between the ﬁrst and second appearances of
the HF diagnostic code was greater than 12 months, the date of the second encounter was used as the

11

ﬁrst qualifying encounter. The date at which HF diagnosis was given to the case is denoted as HFDx.
Up to ten eligible controls (in terms of sex, age, location) were selected for each case, yielding an
overall ratio of 9 controls per case. Each control was also assigned an index date, which is the HFDx
of the matched case. Controls are selected such that they did not meet the operational criteria for
HF diagnosis prior to the HFDx plus 182 days of their corresponding case. Control subjects were
required to have their ﬁrst ofﬁce encounter within one year of the matching HF case patient’s ﬁrst
ofﬁce visit, and have at least one ofﬁce encounter 30 days before or any time after the case’s HF
diagnosis date to ensure similar duration of observations among cases and controls.

C Results on encounter sequence modeling

Objective: Given a sequence of visits x1, . . . , xT , the goal of encounter sequence modeling is, for
each time step i, to predict the codes occurring at the next visit x2, . . . , xT +1. In this experiment,
we focus on predicting the diagnosis codes in the encounter sequence, so we create a separate set of
labels y1, . . . , yT that do not contain non-diagnosis codes such as medication codes or procedure
codes. Therefore yi will contain diagnosis codes from the next visit xi+1.

Dataset: We divide the entire dataset described in Table 1 into 0.75:0.10:0.15 ratio, respectively for
training set, validation set, and test set.

Baseline: We use the same baseline models we used for HF prediction. However, since we are
predicting 283 binary labels now, we replace the logistic regression function with the Softmax
function. The drop-out and L2 regularization policies remain the same.
For LR and MLP, at each step i, we aggregate maximum ten past input vectors6 xi−9, . . . , xi to create
a pseudo-context vector (cid:98)ci. LR applies the Softmax function on top of (cid:98)ci. MLP places a hidden layer
on top of (cid:98)ci then applies the Softmax function.
Evaluation metric: We use the negative log likelihood Eq (1) on the test set to evaluate the model
performance. We also use Recall@k as an additional metric to measure the prediction accuracy.

• Recall@k: Given a sequence of visits x1, . . . , xT , we evaluate the model performance based
on how accurately it can predict the diagnosis codes y1, . . . , yT . We use the average Recall@k,
which is expressed as below,

1
N

N
(cid:88)

n=1

1
T (n)

T (n)
(cid:88)

i=1

Recall@k((cid:98)yi), where Recall@k((cid:98)yi) =

|argsort((cid:98)yi)[: k] ∩ nonzero(yi)|
|nonzero(yi)|

where argsort returns a list of indices that will decrementally sort a given vector and nonzero
returns a list of indices of the coordinates with non-zero values. We use Recall@k because of its
similar nature to the way a human physician performs the differential diagnostic procedure, which
is to generate a list of most likely diseases for an undiagnosed patient, then perform medical
practice until the true disease, or diseases are determined.

Prediction accuracy: Table 4 displays the prediction performance of RETAIN and the baselines.
We use k = 5, 10 for Recall@k to allow a reasonable number of prediction trials, as well as cover
complex patients who often receive multiple diagnosis codes at a single visit.

RNN shows the best prediction accuracy for encounter diagnosis prediction. However, considering
the purpose of encounter diagnosis prediction, which is to assist doctors to provide quality care for
the patient, black-box behavior of RNN makes it unattractive as a clinical tool. On the other hand,
RETAIN performs as well as other attention models, only slightly inferior to RNN, provides full
interpretation of its prediction behavior, making it a feasible solution for clinical applications.

The interesting ﬁnding in Table 4 is that MLP is able to perform as accurately as RNN+αM in terms
of Recall@10. Considering the fact that MLP uses aggregated information of past ten visits, we can
assume that encounter diagnosis prediction depends more on the frequency of disease occurrences
rather than the order in which they occurred. This is quite different from the HF prediction task,
where stationary models (LF, MLP) performed signiﬁcantly worse than sequential models.

6We also tried aggregating all past input vectors x1, . . . , xi, but the performance was slightly worse than

using just ten.

12

Table 4: Encounter diagnosis prediction performance of RETAIN and the baselines

Model

LR
MLP
RNN
RNN+αM
RNN+αR
RETAIN

Negative
Likelihood
0.0288
0.0267
0.0258
0.0262
0.0259
0.0259

Recall@5 Recall@10

43.15
50.72
55.18
52.15
53.89
54.25

55.84
65.02
69.09
65.81
67.45
67.74

(a)

(b)

(c)

(d)

(e)

Figure 4: Graphical illustration of the baselines: (a) Logistic regression (LR), (b) Multilayer Percep-
tron (MLP), (c) Recurrent neural network (RNN), (d) RNN with attention vectors generated via an
MLP (RNN+αM ), (e) RNN with attention vectors generated via an RNN (RNN+αR). RETAIN is
given in Figure 1b.

D Illustration and comparison of the baselines

Figure 4 illustrates the baselines used in the experiments and shows the relationship among them.

13

7
1
0
2
 
b
e
F
 
6
2
 
 
]

G
L
.
s
c
[
 
 
4
v
5
4
7
5
0
.
8
0
6
1
:
v
i
X
r
a

RETAIN: An Interpretable Predictive Model for
Healthcare using Reverse Time Attention Mechanism

Edward Choi∗, Mohammad Taha Bahadori∗, Joshua A. Kulas∗,
Andy Schuetz†, Walter F. Stewart†, Jimeng Sun∗
† Sutter Health
∗ Georgia Institute of Technology

{mp2893,bahadori,jkulas3}@gatech.edu,
{schueta1,stewarwf}@sutterhealth.org, jsun@cc.gatech.edu

Abstract
Accuracy and interpretability are two dominant features of successful predictive
models. Typically, a choice must be made in favor of complex black box models
such as recurrent neural networks (RNN) for accuracy versus less accurate but
more interpretable traditional models such as logistic regression. This tradeoff
poses challenges in medicine where both accuracy and interpretability are impor-
tant. We addressed this challenge by developing the REverse Time AttentIoN
model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN
achieves high accuracy while remaining clinically interpretable and is based on
a two-level neural attention model that detects inﬂuential past visits and signiﬁ-
cant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics
physician practice by attending the EHR data in a reverse time order so that recent
clinical visits are likely to receive higher attention. RETAIN was tested on a large
health system EHR dataset with 14 million visits completed by 263K patients over
an 8 year period and demonstrated predictive accuracy and computational scalabil-
ity comparable to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.

1

Introduction

The broad adoption of Electronic Health Record (EHR) systems has opened the possibility of
applying clinical predictive models to improve the quality of clinical care. Several systematic reviews
have underlined the care quality improvement using predictive analysis [7, 22, 5, 18]. EHR data
can be represented as temporal sequences of high-dimensional clinical variables (e.g., diagnoses,
medications and procedures), where the sequence ensemble represents the documented content of
medical visits from a single patient. Traditional machine learning tools summarize this ensemble into
aggregate features, ignoring the temporal and sequence relationships among the feature elements.
The opportunity to improve both predictive accuracy and interpretability is likely to derive from
effectively modeling temporality and high-dimensionality of these event sequences.

Accuracy and interpretability are two dominant features of successful predictive models. There is a
common belief that one has to trade accuracy for interpretability using one of three types of traditional
models [6]: 1) identifying a set of rules (e.g. via decision trees [24]), 2) case-based reasoning by
ﬁnding similar patients (e.g. via k-nearest neighbors [16] and distance metric learning [33]), and 3)
identifying a list of risk factors (e.g. via LASSO coefﬁcients [15]). While interpretable, all of these
models rely on aggregated features, ignoring the temporal relation among features inherent to EHR
data. As a consequence, model accuracy is sub-optimal. Latent-variable time-series models, such as
[31, 32], account for temporality, but often have limited interpretation due to abstract state variables.

Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential
EHR data to predict diagnoses [27] and model encounter sequences [11, 14]. But, the gain in accuracy

29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

(a) Standard attention model

(b) RETAIN model

Figure 1: Common attention models vs. RETAIN, using folded diagrams of RNNs. (a) Standard
attention mechanism: the recurrence on the hidden state vector vi hinders interpretation of the model.
(b) Attention mechanism in RETAIN: The recurrence is on the attention generation components (hi or
gi) while the hidden state vi is generated by a simpler more interpretable output.

from use of RNNs is at the cost of model output that is notoriously difﬁcult to interpret. While
there have been several attempts at directly interpreting RNNs [17, 23, 8], these methods are not
sufﬁciently developed for application in clinical care.

We have addressed this limitation using a modeling strategy known as RETAIN, a two-level neural
attention model for sequential data that provides detailed interpretation of the prediction results while
retaining the prediction accuracy comparable to RNN. To this end, RETAIN relies on an attention
mechanism modeled to represent the behavior of physicians during an encounter. A distinguishing
feature of RETAIN (see Figure 1) is to leverage sequence information using an attention generation
mechanism, while learning an interpretable representation. And emulating physician behaviors,
RETAIN examines a patient’s past visits in reverse time order, facilitating a more stable attention
generation. As a result, RETAIN identiﬁes the most meaningful visits and quantiﬁes visit speciﬁc
features that contribute to the prediction.

RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K
patients over an 8 year period. We compared predictive accuracy of RETAIN to traditional machine
learning methods and to RNN variants using a case-control dataset to predict a future diagnosis of
heart failure. The comparative analysis demonstrates that RETAIN achieves comparable performance
to RNN in both accuracy and speed and signiﬁcantly outperforms traditional models. Moreover,
using a concrete case study and visualization method, we demonstrate how RETAIN offers an intuitive
interpretation.

2 Methodology

We ﬁrst describe the structure of sequential EHR data and our notation, then follow with a general
framework for predictive analysis in healthcare using EHR, followed by details of the RETAIN method.

EHR Structure and our Notation. The EHR data of each patient can be represented as a time-
labeled sequence of multivariate observations. Assuming we use r different variables, the n-th patient
of N total patients can be represented by a sequence of T (n) tuples (t(n)
) ∈ R × Rr, i =
1, . . . , T (n). The timestamps t(n)
denotes the time of the i-th visit of the n-th patient and T (n) the
number of visits of the n-th patient. To minimize clutter, we describe the algorithms for a single patient
and have dropped the superscript (n) whenever it is unambiguous. The goal of predictive modeling
is to predict the label at each time step yi ∈ {0, 1}s or at the end of the sequence y ∈ {0, 1}s. The
number of labels s can be more than one.

, x(n)
i

i

i

For example, in encounter sequence modeling (ESM) [11], each visit (e.g. encounter) of a patient’s
visit sequence is represented by a set of varying number of medical codes {c1, c2, . . . , cn}. cj is
the j-th code from the vocabulary C. Therefore, in ESM, the number of variables r = |C| and input

2

xi ∈ {0, 1}|C| is a binary vector where the value one in the j-th coordinate indicates that cj was
documented in i-th visit. Given a sequence of visits x1, . . . , xT , the goal of ESM is, for each time
step i, to predict the codes occurring at the next visit x2, . . . , xT +1, with the number of labels s = |C|.

In case of learning to diagnose (L2D) [27], the input vector xi consists of continuous clinical measures.
If there are r different measurements, then xi ∈ Rr. The goal of L2D is, given an input sequence
x1, . . . , xT , to predict the occurrence of a speciﬁc disease (s = 1) or multiple diseases (s > 1).
Without loss of generality, we will describe the algorithm for ESM, as L2D can be seen as a special
case of ESM where we make a single prediction at the end of the visit sequence.

In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural
network variants that can cope with the vanishing gradient problem [3], such as LSTM [21], GRU
[9], and IRNN [26], with any depth (number of hidden layers).

2.1 Preliminaries on Neural Attention Models

Attention based neural network models are being successfully applied to image processing [1, 29, 19,
34], natural language processing [2, 20, 30] and speech recognition [12]. The utility of the attention
mechanism can be seen in the language translation task [2] where it is inefﬁcient to represent an
entire sentence with one ﬁxed-size vector because neural translation machines ﬁnds it difﬁcult to
translate the given sentence represented by a single vector.

Intuitively, the attention mechanism for language translation works as follows: given a sentence of
length S in the original language, we generate h1, . . . , hS, to represent the words in the sentence. To
ﬁnd the j-th word in the target language, we generate attentions αj
i for i = 1, . . . , S for each word in
the original sentence. Then, we compute the context vector cj = (cid:80)
i hi and use it to predict the
j-th word in the target language. In general, the attention mechanism allows the model to focus on a
speciﬁc word (or words) in the given sentence when generating each word in the target language.

i αj

We rely on a conceptually similar temporal attention mechanism to generate interpretable prediction
models using EHR data. Our model framework is motivated by and mimics how doctors attend to a
patient’s needs and explore the patient record, where there is a focus on speciﬁc clinical information
(e.g., key risk factors) working from the present to the past.

2.2 Reverse Time Attention Model RETAIN

Figure 2 shows the high-level overview of our model, where a central feature is to delegate a
considerable portion of the prediction responsibility to the process for generating attention weights.
This is intended to address, in part, the difﬁculty with interpreting RNNs where the recurrent weights
feed past information to the hidden layer. Therefore, to consider both the visit-level and the variable-
level (individual coordinates of xi) inﬂuence, we use a linear embedding of the input vector xi. That
is, we deﬁne

vi = Wembxi,
(Step 1)
where vi ∈ Rm denotes the embedding of the input vector xi ∈ Rr, m the size of the embedding di-
mension, Wemb ∈ Rm×r the embedding matrix to learn. We can alternatively use more sophisticated
yet interpretable representations such as those derived from multilayer perceptron (MLP) [13, 25].
MLP has been used for representation learning in EHR data [10].

We use two sets of weights, one for the visit-level attention and the other for variable-level attention,
respectively. The scalars α1, . . . , αi are the visit-level attention weights that govern the inﬂuence of
each visit embedding v1, . . . , vi. The vectors β1, . . . , βi are the variable-level attention weights that
focus on each coordinate of the visit embedding v1,1, v1,2, . . . , v1,m, . . . , vi,1, vi,2, . . . , vi,m.

We use two RNNs, RNNα and RNNβ, to separately generate α’s and β’s as follows,

gi, gi−1, . . . , g1 = RNNα(vi, vi−1, . . . , v1),

ej = w(cid:62)

α gj + bα,

for

j = 1, . . . , i

α1, α2, . . . , αi = Softmax(e1, e2, . . . , ei)
hi, hi−1, . . . , h1 = RNNβ(vi, vi−1, . . . , v1)
for

βj = tanh (cid:0)Wβhj + bβ

(cid:1)

(Step 2)

j = 1, . . . , i,

(Step 3)

3

Figure 2: Unfolded view of RETAIN’s architecture: Given input sequence x1, . . . , xi, we predict the
label yi. Step 1: Embedding, Step 2: generating α values using RNNα, Step 3: generating β values
using RNNβ, Step 4: Generating the context vector using attention and representation vectors, and
Step 5: Making prediction. Note that in Steps 2 and 3 we use RNN in the reversed time.

where gi ∈ Rp is the hidden layer of RNNα at time step i, hi ∈ Rq the hidden layer of RNNβ
at time step i and wα ∈ Rp, bα ∈ R, Wβ ∈ Rm×q and bβ ∈ Rm are the parameters to learn.
The hyperparameters p and q determine the hidden layer size of RNNα and RNNβ, respectively.
Note that for prediction at each timestamp, we generate a new set of attention vectors α and β. For
simplicity of notation, we do not include the index for predicting at different time steps. In Step 2,
we can use Sparsemax [28] instead of Softmax for sparser attention weights.

As noted, RETAIN generates the attention vectors by running the RNNs backward in time; i.e., RNNα
and RNNβ both take the visit embeddings in a reverse order vi, vi−1, . . . , v1. Running the RNN
in reversed time order also offers computational advantages since the reverse time order allows us
to generate e’s and β’s that dynamically change their values when making predictions at different
time steps i = 1, 2, . . . , T . This ensures that the attention vectors are modiﬁed at each time step,
increasing the computational stability of the attention generation process.1

Using the generated attentions, we obtain the context vector ci for a patient up to the i-th visit as
follows,

ci =

αjβj (cid:12) vj,

i
(cid:88)

j=1

(Step 4)

where (cid:12) denotes element-wise multiplication. We use the context vector ci ∈ Rm to predict the true
label yi ∈ {0, 1}s as follows,

(cid:98)yi = Softmax(Wci + b),
(Step 5)
where W ∈ Rs×m and b ∈ Rs are parameters to learn. We use the cross-entropy to calculate the
classiﬁcation loss as follows,

L(x1, . . . , xT ) = −

1
N

N
(cid:88)

n=1

1
T (n)

T (n)
(cid:88)

(cid:16)

i=1

y(cid:62)
i

(cid:17)
log((cid:98)yi) + (1 − yi)(cid:62) log(1 − (cid:98)yi)

(1)

where we sum the cross entropy errors from all dimensions of (cid:98)yi. In case of real-valued output
yi ∈ Rs, we can change the cross-entropy in Eq. (1) to, for example, mean squared error.
Overall, our attention mechanism can be viewed as the inverted architecture of the standard attention
mechanism for NLP [2] where the words are encoded by RNN and the attention weights are generated
by MLP. In contrast, our method uses MLP to embed the visit information to preserve interpretability
and uses RNN to generate two sets of attention weights, recovering the sequential information as
well as mimicking the behavior of physicians. Note that we did not use the timestamp of each visit
in our formulation. Using timestamps, however, provides a small improvement in the prediction
performance. We propose a method to use timestamps in Appendix A.

1For example, feeding visit embeddings in the original order to RNNα and RNNβ will generate the same e1
and β1 for every time step i = 1, 2, . . . , T . Moreover, in many cases, a patient’s recent visit records deserve
more attention than the old records. Then we need to have ej+1 > ej which makes the process computationally
unstable for long sequences.

4

3

Interpreting RETAIN

Finding the visits that contribute to prediction are derived using the largest αi, which is straightforward.
However, ﬁnding inﬂuential variables is slightly more involved as a visit is represented by an ensemble
of medical variables, each of which can vary in its predictive contribution. The contribution of each
variable is determined by v, β and α, and interpretation of α alone informs which visit is inﬂuential
in prediction but not why.

We propose a method to interpret the end-to-end behavior of RETAIN. By keeping α and β values
ﬁxed as the attention of doctors, we analyze changes in the probability of each label yi,1, . . . , yi,s
in relation to changes in the original input x1,1, . . . , x1,r, . . . , xi,1, . . . , xi,r. The xj,k that yields the
largest change in yi,d will be the input variable with highest contribution. More formally, given the
sequence x1, . . . , xi, we are trying to predict the probability of the output vector yi ∈ {0, 1}s, which
can be expressed as follows

p(yi|x1, . . . , xi) = p(yi|ci) = Softmax (Wci + b)
(2)
where ci ∈ Rm denotes the context vector. According to Step 4, ci is the sum of the visit embeddings
v1, . . . , vi weighted by the attentions α’s and β’s. Therefore Eq (2) can be rewritten as follows,

p(yi|x1, . . . , xi) = p(yi|ci) = Softmax

W

αjβj (cid:12) vj

+ b

(3)

(cid:18)

(cid:16) i
(cid:88)

j=1

(cid:17)

(cid:19)

Using the fact that the visit embedding vi is the sum of the columns of Wemb weighted by each
element of xi, Eq (3) can be rewritten as follows,

p(yi|x1, . . . , xi) = Softmax

W

αjβj (cid:12)

xj,kWemb[:, k]

+ b

(cid:18)

(cid:16) i
(cid:88)

j=1

(cid:18) i

(cid:88)

r
(cid:88)

j=1

k=1

r
(cid:88)

k=1

(cid:16)

(cid:17)

(cid:17)

(cid:19)

(cid:19)

= Softmax

xj,k αjW

βj (cid:12) Wemb[:, k]

+ b

(4)

where xj,k is the k-th element of the input vector xj. Eq (4) can be completely deconstructed to the
variables at each input x1, . . . , xi, which allows for calculating the contribution ω of the k-th variable
of the input xj at time step j ≤ i, for predicting yi as follows,

ω(yi, xj,k) = αjW(βj (cid:12) Wemb[:, k])
(cid:125)

(cid:123)(cid:122)
Contribution coefﬁcient

(cid:124)

xj,k
(cid:124)(cid:123)(cid:122)(cid:125)
Input value

,

(5)

where the index i of yi is omitted in the αj and βj. As we have described in Section 2.2, we are
generating α’s and β’s at time step i in the visit sequence x1, . . . , xT . Therefore the index i is always
assumed for α’s and β’s. Additionally, Eq (5) shows that when we are using a binary input value, the
coefﬁcient itself is the contribution. However, when we are using a non-binary input value, we need
to multiply the coefﬁcient and the input value xj,k to correctly calculate the contribution.

4 Experiments

We compared performance of RETAIN to RNNs and traditional machine learning methods. Given
space constraints, we only report the results on the learning to diagnose (L2D) task and summarize the
encounter sequence modeling (ESM) in Appendix C. The RETAIN source code is publicly available
at https://github.com/mp2893/retain.

4.1 Experimental setting
Source of data: The dataset consists of electronic health records from Sutter Health. The patients
are 50 to 80 years old adults chosen for a heart failure prediction model study. From the encounter
records, medication orders, procedure orders and problem lists, we extracted visit records consisting
of diagnosis, medication and procedure codes. To reduce the dimensionality while preserving the
clinical information, we used existing medical groupers to aggregate the codes into input variables.
The details of the medical groupers are given in the Appendix B. A proﬁle of the dataset is summarized
in Table 1.

5

Table 1: Statistics of EHR dataset. (D:Diagnosis, R:Medication, P:Procedure)

# of patients
# of visits
Avg. # of visits per patient
# of medical code groups

263,683
14,366,030
54.48

Avg. # of codes in a visit
Max # of codes in a visit
Avg. # of Dx codes in a visit

615 (D:283, R:94, P:238) Max # of Dx in a visit

3.03
62
1.83
42

Implementation details: We implemented RETAIN with Theano 0.8 [4]. For training the model, we
used Adadelta [35] with the mini-batch of 100 patients. The training was done in a machine equipped
with Intel Xeon E5-2630, 256GB RAM, two Nvidia Tesla K80’s and CUDA 7.5.

Baselines: For comparison, we completed the following models.

• Logistic regression (LR): We compute the counts of medical codes for each patient based on all
her visits as input variables and normalize the vector to zero mean and unit variance. We use the
resulting vector to train the logistic regression.

• MLP: We use the same feature construction as LR, but put a hidden layer of size 256 between

the input and output.

• RNN: RNN with two hidden layers of size 256 implemented by the GRU. Input sequences
x1, . . . , xi are used. Logistic regression is applied to the top hidden layer. We use two layers of
RNN of to match the model complexity of RETAIN.

• RNN+αM : One layer single directional RNN (hidden layer size 256) along time to generate the
input embeddings v1, . . . , vi. We use the MLP with a single hidden layer of size 256 to generate
the visit-level attentions α1, . . . , αi. We use the input embeddings v1, . . . , vi as the input to the
MLP. This baseline corresponds to Figure 1a.

• RNN+αR: This is similar to RNN+αM but uses the reverse-order RNN (hidden layer size 256)
to generate the visit-level attentions α1, . . . , αi. We use this baseline to conﬁrm the effectiveness
of generating the attentions using reverse time order.

The comparative visualization of the baselines are provided in Appendix D. We use the same
implementation and training method for the baselines as described above. The details on the hyper-
parameters, regularization and drop-out strategies for the baselines are described in Appendix B.

Evaluation measures: Model accuracy was measured by:

• Negative log-likelihood that measures the model loss on the test set. The loss can be calculated

by Eq (1).

• Area Under the ROC Curve (AUC) of comparing (cid:98)yi with the true label yi. AUC is more
robust to imbalanced positive/negative prediction labels, making it appropriate for evaluation of
classiﬁcation accuracy in the heart failure prediction task.

We also report the bootstrap (10,000 runs) estimate of the standard deviation of the evaluation
measures.

4.2 Heart Failure Prediction

Objective: Given a visit sequence x1, . . . , xT , we predicted if a primary care patient will be
diagnosed with heart failure (HF). This is a special case of ESM with a single disease outcome at
the end of the sequence. Since this is a binary prediction task, we use the logistic sigmoid function
instead of the Softmax in Step 5.

Cohort construction: From the source dataset, 3,884 cases are selected and approximately 10
controls are selected for each case (28,903 controls). The case/control selection criteria are fully
described in the supplementary section. Cases have index dates to denote the date they are diagnosed
with HF. Controls have the same index dates as their corresponding cases. We extract diagnosis codes,
medication codes and procedure codes in the 18-months window before the index date.

Training details: The patient cohort was divided into the training, validation and test sets in a
0.75:0.1:0.15 ratio. The validation set was used to determine the values of the hyper-parameters. See
Appendix B for details of hyper-parameter tuning.

6

Table 2: Heart failure prediction performance of RETAIN and the baselines

Train Time / epoch Test Time

Model
LR
MLP
RNN
RNN+αM
RNN+αR
RETAIN

Test Neg Log Likelihood
0.3269 ± 0.0105
0.2959 ± 0.0083
0.2577 ± 0.0082
0.2691 ± 0.0082
0.2605 ± 0.0088
0.2562 ± 0.0083

AUC
0.7900 ± 0.0111
0.8256 ± 0.0096
0.8706 ± 0.0080
0.8624 ± 0.0079
0.8717 ± 0.0080
0.8705 ± 0.0081

0.15s
0.25s
10.3s
6.7s
10.4s
10.8s

0.11s
0.11s
0.57s
0.48s
0.62s
0.63s

Results: Logistic regression and MLP underperformed compared to the four temporal learning
algorithms (Table 2). RETAIN is comparable to the other RNN variants in terms of prediction
performance while offering the interpretation beneﬁt.

Note that RNN+αR model are a degenerated version of RETAIN with only scalar attention, which is
still a competitive model as shown in table 2. This conﬁrms the efﬁciency of generating attention
weights using the RNN. However, RNN+αR model only provides scalar visit-level attention, which
is not sufﬁcient for healthcare applications. Patients often receives several medical codes at a single
visit, and it will be important to distinguish their relative importance to the target. We show such a
case study in section 4.3.

Table 2 also shows the scalability of RETAIN, as its training time (the number of seconds to train
the model over the entire training set once) is comparable to RNN. The test time is the number
of seconds to generate the prediction output for the entire test set. We use the mini-batch of 100
patients when assessing both training and test times. RNN takes longer than RNN+αM because of its
two-layer structure, whereas RNN+αM uses a single layer RNN. The models that use two RNNs
(RNN, RNN+αR, RETAIN)2 take similar time to train for one epoch. However, each model required
a different number of epochs to converge. RNN typically takes approximately 10 epochs, RNN+αM
and RNN+αR 15 epochs and RETAIN 30 epochs. Lastly, training the attention models (RNN+αM ,
RNN+αR and RETAIN) for ESM would take considerably longer than L2D, because ESM modeling
generates context vectors at each time step. RNN, on the other hand, does not require additional
computation other than embedding the visit to its hidden layer to predict target labels at each time
step. Therefore, in ESM, the training time of the attention models will increase linearly in relation to
the length of the input sequence.

4.3 Model Interpretation for Heart Failure Prediction
We evaluated the interpretability of RETAIN in the HF prediction task by choosing a HF patient from
the test set and calculating the contribution of the variables (medical codes in this case) to diagnostic
prediction. The patient suffered from skin problems, skin disorder (SD), benign neoplasm (BN),
excision of skin lesion (ESL), for some time before showing symptoms of HF, cardiac dysrhythmia
(CD), heart valve disease (HVD) and coronary atherosclerosis (CA), and then a diagnosis of HF
(Figure 3). We can see that skin-related codes from the earlier visits made little contribution to HF
prediction as expected. RETAIN properly puts much attention to the HF-related codes that occurred in
recent visits.

To conﬁrm RETAIN’s ability to exploit the sequence information of the EHR data, we reverse the visit
sequence of Figure 3a and feed it to RETAIN. Figure 3b shows the contribution of the medical codes
of the reversed visit record. HF-related codes in the past are still making positive contributions, but
not as much as they did in Figure 3a. Figure 3b also emphasizes RETAIN’s superiority to interpretable,
but stationary models such as logistic regression. Stationary models often aggregate past information
and remove the temporality from the input data, which can mistakenly lead to the same risk prediction
for Figure 3a and 3b. RETAIN, however, can correctly digest the sequence information and calculates
the HF risk score of 9.0%, which is signiﬁcantly lower than that of Figure 3a.

Figure 3c shows how the contributions of codes change when selected medication data are used in
the model. We added two medications from day 219: antiarrhythmics (AA) and anticoagulants (AC),
both of which are used to treat cardiac dysrhythmia (CD). The two medications make a negative
contributions, especially towards the end of the record. The medications decreased the positive
contributions of heart valve disease and cardiac dysrhythmia in the last visit. Indeed, the HF risk

2The RNN baseline uses two layers of RNN, RNN+αR uses one for visit embedding and one for generating

α, RETAIN uses each for generating α and β

7

Figure 3: (a) Temporal visualization of a patient’s visit records where the contribution of variables for
diagnosis of heart failure (HF) is summarized along the x-axis (i.e. time) with the y-axis indicating
the magnitude of visit and code speciﬁc contributions to HF diagnosis. (b) We reverse the order of
the visit sequence to see if RETAIN can properly take into account the modiﬁed sequence information.
(c) Medication codes are added to the visit record to see how it changes the behavior of RETAIN.

prediction (0.2165) of Figure 3c is lower than that of Figure 3a (0.2474). This suggests that taking
proper medications can help the patient in reducing their HF risk.

5 Conclusion

Our approach to modeling event sequences as predictors of HF diagnosis suggest that complex
models can offer both superior predictive accuracy and more precise interpretability. Given the power
of RNNs for analyzing sequential data, we proposed RETAIN, which preserves RNN’s predictive
power while allowing a higher degree of interpretation. The key idea of RETAIN is to improve
the prediction accuracy through a sophisticated attention generation process, while keeping the
representation learning part simple for interpretation, making the entire algorithm accurate and
interpretable. RETAIN trains two RNN in a reverse time order to efﬁciently generate the appropriate
attention variables. For future work, we plan to develop an interactive visualization system for
RETAIN and evaluating RETAIN in other healthcare applications.

References

In ICLR, 2015.

[1] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. In ICLR, 2015.
[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.

[3] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.

Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.

[4] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of SciPy, 2010.
[5] A. D. Black, J. Car, C. Pagliari, C. Anandan, K. Cresswell, T. Bokun, B. McKinstry, R. Procter, A. Majeed,
and A. Sheikh. The impact of ehealth on the quality and safety of health care: a systematic overview. PLoS
Med, 8(1):e1000387, 2011.

[6] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad. Intelligible models for healthcare:

Predicting pneumonia risk and hospital 30-day readmission. In KDD, 2015.

[7] B. Chaudhry, J. Wang, S. Wu, M. Maglione, W. Mojica, E. Roth, S. C. Morton, and P. G. Shekelle.
Systematic review: impact of health information technology on quality, efﬁciency, and costs of medical
care. Annals of internal medicine, 144(10):742–752, 2006.

8

[8] Z. Che, S. Purushotham, R. Khemani, and Y. Liu. Distilling knowledge from deep networks with

applications to healthcare domain. arXiv preprint arXiv:1512.03542, 2015.

[9] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning
phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014.

[10] E. Choi, M. T. Bahadori, E. Searles, C. Coffey, and J. Sun. Multi-layer representation learning for medical

[11] E. Choi, M. T. Bahadori, and J. Sun. Doctor ai: Predicting clinical events via recurrent neural networks.

concepts. In KDD, 2016.

arXiv preprint arXiv:1511.05942, 2015.

recognition. In NIPS, pages 577–585, 2015.

University of Montreal, 2009.

[12] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. Attention-based models for speech

[13] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.

[14] C. Esteban, O. Staeck, Y. Yang, and V. Tresp. Predicting clinical events by combining static and dynamic

information using recurrent neural networks. arXiv preprint arXiv:1602.02685, 2016.

[15] A. S. Fleisher, B. B. Sowell, C. Taylor, A. C. Gamst, R. C. Petersen, L. J. Thal, and f. t. A. D. C. Study.
Clinical predictors of progression to Alzheimer disease in amnestic mild cognitive impairment. Neurology,
68(19):1588–1595, 2007.

[16] B. Gallego, S. R. Walter, R. O. Day, A. G. Dunn, V. Sivaraman, N. Shah, C. A. Longhurst, and E. Coiera.
Bringing cohort studies to the bedside: framework for a ’green button’ to support clinical decision-making.
Journal of Comparative Effectiveness Research, pages 1–7, 2015.

[17] J. Ghosh and V. Karamcheti. Sequence learning with recurrent networks: analysis of internal representations.

In Aerospace Sensing, pages 449–460, 1992.

[18] C. L. Goldzweig, A. Towﬁgh, M. Maglione, and P. G. Shekelle. Costs and beneﬁts of health information

technology: new trends from the literature. Health affairs, 28(2):w282–w293, 2009.

[19] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image

generation. arXiv preprint arXiv:1502.04623, 2015.

[20] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching

machines to read and comprehend. In NIPS, pages 1684–1692, 2015.

[21] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

[22] A. K. Jha, C. M. DesRoches, E. G. Campbell, K. Donelan, S. R. Rao, T. G. Ferris, A. Shields, S. Rosenbaum,

and D. Blumenthal. Use of electronic health records in us hospitals. N Engl J Med, 2009.

[23] A. Karpathy, J. Johnson, and F.-F. Li. Visualizing and understanding recurrent networks. arXiv preprint

arXiv:1506.02078, 2015.

[24] A. N. Kho et al. Use of diverse electronic medical record systems to identify genetic risk for type 2 diabetes

within a genome-wide association study. JAMIA, 19(2):212–218, 2012.

[25] Q. V. Le. Building high-level features using large scale unsupervised learning. In ICASSP, 2013.

[26] Q. V. Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent networks of rectiﬁed linear units.

arXiv preprint arXiv:1504.00941, 2015.

[27] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell. Learning to Diagnose with LSTM Recurrent Neural

Networks. In ICLR, 2016.

classiﬁcation. In ICML, 2016.

[28] A. F. Martins and R. F. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label

[29] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of visual attention. In NIPS, 2014.

[30] A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence summarization.

In EMNLP, 2015.

[31] S. Saria, D. Koller, and A. Penn. Learning individual and population level traits from clinical temporal

data. In NIPS, Predictive Models in Personalized Medicine workshop, 2010.

[32] P. Schulam and S. Saria. A probabilistic graphical model for individualizing prognosis in chronic, complex

diseases. In AMIA, volume 2015, page 143, 2015.

[33] J. Sun, F. Wang, J. Hu, and S. Edabollahi. Supervised patient similarity measure of heterogeneous patient

records. ACM SIGKDD Explorations Newsletter, 14(1):16–24, 2012.

[34] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell:

Neural image caption generation with visual attention. In ICML, 2015.

[35] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.

9

A A method to use the timestamps

i

As before, we use t(n)
to represent the timestamp of the i-th visit of the n-th patient. In the following,
we suppress the superscript (n) to avoid cluttered notation. Note that the timestamp ti can be anything
that provides the temporal information of the i-th visit: the number of days from the ﬁrst visit, the
number of days between two consecutive visits, or the number of days until the index date of some
event such as heart failure diagnosis.

In order to use the timestamps, we modify Step 2 and Step 3 in Section 2.2 as follows:

gi, gi−1, . . . , g1 = RNNα(v

(cid:48)

(cid:48)

i, v
α gj + bα,

i−1, . . . , v
for

(cid:48)

1),

j = 1, . . . , i

ej = w(cid:62)

α1, α2, . . . , αi = Softmax(e1, e2, . . . , ei)

hi, hi−1, . . . , h1 = RNNβ(v

(cid:48)

(cid:48)

i, v

i−1, . . . , v

βj = tanh (cid:0)Wβhj + bβ
i = [vi, ti]

(cid:48)

(cid:1)

where v

(cid:48)

1)
for

j = 1, . . . , i,

(cid:48)

where we use v
i, the concatenation of the visit embedding vi and the timestamp ti, to generate the
(cid:48)
attentions α and β. However, when obtaining the context vector ci as per Step 4, we use vi, not v
i
to match the dimensionality. The entire process could be understood such that we use the temporal
information not to embed each visit, but to calculate the attentions for the entire visit sequence. This
is consistent with our modeling approach where we lose the sequential information in embedding the
visit with MLP, then recover the sequential information by generating the attentions using the RNN.
By using the temporal information, speciﬁcally the log of the number of days from the ﬁrst visit, we
were able to improve the heart failure prediction AUC by 0.003 without any hyper-parameter tuning.

B Details of the experiment settings

B.1 Hyper-parameter Tuning

We used the validation set to tune the hyper-parameters: visit embedding size m, RNNα’s hidden
layer size p, RNNβ’s hidden layer size q, L2 regularization coefﬁcient, and drop-out rates.

L2 regularization was applied to all weights except the ones in RNNα and RNNβ. Two separate
drop-outs were used on the visit embedding vi and the context vector ci. We performed the random
search with predeﬁned ranges m, p, q ∈ {32, 64, 128, 200, 256}, L2 ∈ {0.1, 0.01, 0.001, 0.0001},
dropoutvi, dropoutci ∈ {0.0, 0.2, 0.4, 0.6, 0.8}. We also performed the random search with m, p
and q ﬁxed to 256.

The ﬁnal value we used to train RETAIN for heart failure prediction is m, p, q = 128, dropoutvi = 0.6,
dropoutci = 0.6 and 0.0001 for the L2 regularization coefﬁcient.

B.2 Code Grouper

Diagnosis codes, medication codes and procedure codes in the dataset are respectively using Interna-
tional Classiﬁcation of Diseases (ICD-9), Generic Product Identiﬁer (GPI) and Current Procedural
Terminology (CPT).
Diagnosis codes are grouped by Clinical Classiﬁcations Software for ICD-93 which reduces the
number of diagnosis code from approximately 14,000 to 283. Medication codes are grouped by
Generic Product Identiﬁer Drug Group4 which reduces the dimension to from approximately 151,000
to 96. Procedure codes are grouped by Clinical Classiﬁcations Software for CPT5, which reduces the
number of CPT codes from approximately 9,000 to 238.

3https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp
4http://www.wolterskluwercdi.com/drug-data/medi-span-electronic-drug-ﬁle/
5https://www.hcup-us.ahrq.gov/toolssoftware/ccs_svcsproc/ccssvcproc.jsp

10

Table 3: Qualifying ICD-9 codes for heart failure

B.3 Training Speciﬁcs of the Basline Models

• LR: We use 0.01 L2 regularization coefﬁcient for the logistic regression weight.
• MLP: We use drop-out rate 0.6 on the output of the hidden layer. We use 0.0001 L2 regularization

coefﬁcient for the hidden layer weight and the logistic regression weight.

• RNN: We use drop-out rate 0.6 on the outputs of both hidden layers. We use 0.0001 L2
regularization coefﬁcient for the logistic regression weight. The dimension size of both hidden
layers is 256.

• RNN+αM : We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of
the context vector (cid:80)
i αivi. We use 0.0001 L2 regularization coefﬁcient for the hidden layer
weight of the MLP that generates α’s and the logistic regression weight. The dimension size of
the hidden layers in both RNN and MLP is 256.

• RNN+αR: We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of
the context vector (cid:80)
i αivi. We use 0.0001 L2 regularization coefﬁcient for the hidden layer
weight of the RNN that generates α’s and the logistic regression weight. The dimension size of
the hidden layers in both RNNs is 256.

B.4 Heart Failure Case/Control Selection Criteria

Case patients were 40 to 85 years of age at the time of HF diagnosis. HF diagnosis (HFDx) is
deﬁned as: 1) Qualifying ICD-9 codes for HF appeared in the encounter records or medication orders.
Qualifying ICD-9 codes are displayed in Table 3. 2) a minimum of three clinical encounters with
qualifying ICD-9 codes had to occur within 12 months of each other, where the date of diagnosis was
assigned to the earliest of the three dates. If the time span between the ﬁrst and second appearances of
the HF diagnostic code was greater than 12 months, the date of the second encounter was used as the

11

ﬁrst qualifying encounter. The date at which HF diagnosis was given to the case is denoted as HFDx.
Up to ten eligible controls (in terms of sex, age, location) were selected for each case, yielding an
overall ratio of 9 controls per case. Each control was also assigned an index date, which is the HFDx
of the matched case. Controls are selected such that they did not meet the operational criteria for
HF diagnosis prior to the HFDx plus 182 days of their corresponding case. Control subjects were
required to have their ﬁrst ofﬁce encounter within one year of the matching HF case patient’s ﬁrst
ofﬁce visit, and have at least one ofﬁce encounter 30 days before or any time after the case’s HF
diagnosis date to ensure similar duration of observations among cases and controls.

C Results on encounter sequence modeling

Objective: Given a sequence of visits x1, . . . , xT , the goal of encounter sequence modeling is, for
each time step i, to predict the codes occurring at the next visit x2, . . . , xT +1. In this experiment,
we focus on predicting the diagnosis codes in the encounter sequence, so we create a separate set of
labels y1, . . . , yT that do not contain non-diagnosis codes such as medication codes or procedure
codes. Therefore yi will contain diagnosis codes from the next visit xi+1.

Dataset: We divide the entire dataset described in Table 1 into 0.75:0.10:0.15 ratio, respectively for
training set, validation set, and test set.

Baseline: We use the same baseline models we used for HF prediction. However, since we are
predicting 283 binary labels now, we replace the logistic regression function with the Softmax
function. The drop-out and L2 regularization policies remain the same.
For LR and MLP, at each step i, we aggregate maximum ten past input vectors6 xi−9, . . . , xi to create
a pseudo-context vector (cid:98)ci. LR applies the Softmax function on top of (cid:98)ci. MLP places a hidden layer
on top of (cid:98)ci then applies the Softmax function.
Evaluation metric: We use the negative log likelihood Eq (1) on the test set to evaluate the model
performance. We also use Recall@k as an additional metric to measure the prediction accuracy.

• Recall@k: Given a sequence of visits x1, . . . , xT , we evaluate the model performance based
on how accurately it can predict the diagnosis codes y1, . . . , yT . We use the average Recall@k,
which is expressed as below,

1
N

N
(cid:88)

n=1

1
T (n)

T (n)
(cid:88)

i=1

Recall@k((cid:98)yi), where Recall@k((cid:98)yi) =

|argsort((cid:98)yi)[: k] ∩ nonzero(yi)|
|nonzero(yi)|

where argsort returns a list of indices that will decrementally sort a given vector and nonzero
returns a list of indices of the coordinates with non-zero values. We use Recall@k because of its
similar nature to the way a human physician performs the differential diagnostic procedure, which
is to generate a list of most likely diseases for an undiagnosed patient, then perform medical
practice until the true disease, or diseases are determined.

Prediction accuracy: Table 4 displays the prediction performance of RETAIN and the baselines.
We use k = 5, 10 for Recall@k to allow a reasonable number of prediction trials, as well as cover
complex patients who often receive multiple diagnosis codes at a single visit.

RNN shows the best prediction accuracy for encounter diagnosis prediction. However, considering
the purpose of encounter diagnosis prediction, which is to assist doctors to provide quality care for
the patient, black-box behavior of RNN makes it unattractive as a clinical tool. On the other hand,
RETAIN performs as well as other attention models, only slightly inferior to RNN, provides full
interpretation of its prediction behavior, making it a feasible solution for clinical applications.

The interesting ﬁnding in Table 4 is that MLP is able to perform as accurately as RNN+αM in terms
of Recall@10. Considering the fact that MLP uses aggregated information of past ten visits, we can
assume that encounter diagnosis prediction depends more on the frequency of disease occurrences
rather than the order in which they occurred. This is quite different from the HF prediction task,
where stationary models (LF, MLP) performed signiﬁcantly worse than sequential models.

6We also tried aggregating all past input vectors x1, . . . , xi, but the performance was slightly worse than

using just ten.

12

Table 4: Encounter diagnosis prediction performance of RETAIN and the baselines

Model

LR
MLP
RNN
RNN+αM
RNN+αR
RETAIN

Negative
Likelihood
0.0288
0.0267
0.0258
0.0262
0.0259
0.0259

Recall@5 Recall@10

43.15
50.72
55.18
52.15
53.89
54.25

55.84
65.02
69.09
65.81
67.45
67.74

(a)

(b)

(c)

(d)

(e)

Figure 4: Graphical illustration of the baselines: (a) Logistic regression (LR), (b) Multilayer Percep-
tron (MLP), (c) Recurrent neural network (RNN), (d) RNN with attention vectors generated via an
MLP (RNN+αM ), (e) RNN with attention vectors generated via an RNN (RNN+αR). RETAIN is
given in Figure 1b.

D Illustration and comparison of the baselines

Figure 4 illustrates the baselines used in the experiments and shows the relationship among them.

13

7
1
0
2
 
b
e
F
 
6
2
 
 
]

G
L
.
s
c
[
 
 
4
v
5
4
7
5
0
.
8
0
6
1
:
v
i
X
r
a

RETAIN: An Interpretable Predictive Model for
Healthcare using Reverse Time Attention Mechanism

Edward Choi∗, Mohammad Taha Bahadori∗, Joshua A. Kulas∗,
Andy Schuetz†, Walter F. Stewart†, Jimeng Sun∗
† Sutter Health
∗ Georgia Institute of Technology

{mp2893,bahadori,jkulas3}@gatech.edu,
{schueta1,stewarwf}@sutterhealth.org, jsun@cc.gatech.edu

Abstract
Accuracy and interpretability are two dominant features of successful predictive
models. Typically, a choice must be made in favor of complex black box models
such as recurrent neural networks (RNN) for accuracy versus less accurate but
more interpretable traditional models such as logistic regression. This tradeoff
poses challenges in medicine where both accuracy and interpretability are impor-
tant. We addressed this challenge by developing the REverse Time AttentIoN
model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN
achieves high accuracy while remaining clinically interpretable and is based on
a two-level neural attention model that detects inﬂuential past visits and signiﬁ-
cant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics
physician practice by attending the EHR data in a reverse time order so that recent
clinical visits are likely to receive higher attention. RETAIN was tested on a large
health system EHR dataset with 14 million visits completed by 263K patients over
an 8 year period and demonstrated predictive accuracy and computational scalabil-
ity comparable to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.

1

Introduction

The broad adoption of Electronic Health Record (EHR) systems has opened the possibility of
applying clinical predictive models to improve the quality of clinical care. Several systematic reviews
have underlined the care quality improvement using predictive analysis [7, 22, 5, 18]. EHR data
can be represented as temporal sequences of high-dimensional clinical variables (e.g., diagnoses,
medications and procedures), where the sequence ensemble represents the documented content of
medical visits from a single patient. Traditional machine learning tools summarize this ensemble into
aggregate features, ignoring the temporal and sequence relationships among the feature elements.
The opportunity to improve both predictive accuracy and interpretability is likely to derive from
effectively modeling temporality and high-dimensionality of these event sequences.

Accuracy and interpretability are two dominant features of successful predictive models. There is a
common belief that one has to trade accuracy for interpretability using one of three types of traditional
models [6]: 1) identifying a set of rules (e.g. via decision trees [24]), 2) case-based reasoning by
ﬁnding similar patients (e.g. via k-nearest neighbors [16] and distance metric learning [33]), and 3)
identifying a list of risk factors (e.g. via LASSO coefﬁcients [15]). While interpretable, all of these
models rely on aggregated features, ignoring the temporal relation among features inherent to EHR
data. As a consequence, model accuracy is sub-optimal. Latent-variable time-series models, such as
[31, 32], account for temporality, but often have limited interpretation due to abstract state variables.

Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential
EHR data to predict diagnoses [27] and model encounter sequences [11, 14]. But, the gain in accuracy

29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

(a) Standard attention model

(b) RETAIN model

Figure 1: Common attention models vs. RETAIN, using folded diagrams of RNNs. (a) Standard
attention mechanism: the recurrence on the hidden state vector vi hinders interpretation of the model.
(b) Attention mechanism in RETAIN: The recurrence is on the attention generation components (hi or
gi) while the hidden state vi is generated by a simpler more interpretable output.

from use of RNNs is at the cost of model output that is notoriously difﬁcult to interpret. While
there have been several attempts at directly interpreting RNNs [17, 23, 8], these methods are not
sufﬁciently developed for application in clinical care.

We have addressed this limitation using a modeling strategy known as RETAIN, a two-level neural
attention model for sequential data that provides detailed interpretation of the prediction results while
retaining the prediction accuracy comparable to RNN. To this end, RETAIN relies on an attention
mechanism modeled to represent the behavior of physicians during an encounter. A distinguishing
feature of RETAIN (see Figure 1) is to leverage sequence information using an attention generation
mechanism, while learning an interpretable representation. And emulating physician behaviors,
RETAIN examines a patient’s past visits in reverse time order, facilitating a more stable attention
generation. As a result, RETAIN identiﬁes the most meaningful visits and quantiﬁes visit speciﬁc
features that contribute to the prediction.

RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K
patients over an 8 year period. We compared predictive accuracy of RETAIN to traditional machine
learning methods and to RNN variants using a case-control dataset to predict a future diagnosis of
heart failure. The comparative analysis demonstrates that RETAIN achieves comparable performance
to RNN in both accuracy and speed and signiﬁcantly outperforms traditional models. Moreover,
using a concrete case study and visualization method, we demonstrate how RETAIN offers an intuitive
interpretation.

2 Methodology

We ﬁrst describe the structure of sequential EHR data and our notation, then follow with a general
framework for predictive analysis in healthcare using EHR, followed by details of the RETAIN method.

EHR Structure and our Notation. The EHR data of each patient can be represented as a time-
labeled sequence of multivariate observations. Assuming we use r different variables, the n-th patient
of N total patients can be represented by a sequence of T (n) tuples (t(n)
) ∈ R × Rr, i =
1, . . . , T (n). The timestamps t(n)
denotes the time of the i-th visit of the n-th patient and T (n) the
number of visits of the n-th patient. To minimize clutter, we describe the algorithms for a single patient
and have dropped the superscript (n) whenever it is unambiguous. The goal of predictive modeling
is to predict the label at each time step yi ∈ {0, 1}s or at the end of the sequence y ∈ {0, 1}s. The
number of labels s can be more than one.

, x(n)
i

i

i

For example, in encounter sequence modeling (ESM) [11], each visit (e.g. encounter) of a patient’s
visit sequence is represented by a set of varying number of medical codes {c1, c2, . . . , cn}. cj is
the j-th code from the vocabulary C. Therefore, in ESM, the number of variables r = |C| and input

2

xi ∈ {0, 1}|C| is a binary vector where the value one in the j-th coordinate indicates that cj was
documented in i-th visit. Given a sequence of visits x1, . . . , xT , the goal of ESM is, for each time
step i, to predict the codes occurring at the next visit x2, . . . , xT +1, with the number of labels s = |C|.

In case of learning to diagnose (L2D) [27], the input vector xi consists of continuous clinical measures.
If there are r different measurements, then xi ∈ Rr. The goal of L2D is, given an input sequence
x1, . . . , xT , to predict the occurrence of a speciﬁc disease (s = 1) or multiple diseases (s > 1).
Without loss of generality, we will describe the algorithm for ESM, as L2D can be seen as a special
case of ESM where we make a single prediction at the end of the visit sequence.

In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural
network variants that can cope with the vanishing gradient problem [3], such as LSTM [21], GRU
[9], and IRNN [26], with any depth (number of hidden layers).

2.1 Preliminaries on Neural Attention Models

Attention based neural network models are being successfully applied to image processing [1, 29, 19,
34], natural language processing [2, 20, 30] and speech recognition [12]. The utility of the attention
mechanism can be seen in the language translation task [2] where it is inefﬁcient to represent an
entire sentence with one ﬁxed-size vector because neural translation machines ﬁnds it difﬁcult to
translate the given sentence represented by a single vector.

Intuitively, the attention mechanism for language translation works as follows: given a sentence of
length S in the original language, we generate h1, . . . , hS, to represent the words in the sentence. To
ﬁnd the j-th word in the target language, we generate attentions αj
i for i = 1, . . . , S for each word in
the original sentence. Then, we compute the context vector cj = (cid:80)
i hi and use it to predict the
j-th word in the target language. In general, the attention mechanism allows the model to focus on a
speciﬁc word (or words) in the given sentence when generating each word in the target language.

i αj

We rely on a conceptually similar temporal attention mechanism to generate interpretable prediction
models using EHR data. Our model framework is motivated by and mimics how doctors attend to a
patient’s needs and explore the patient record, where there is a focus on speciﬁc clinical information
(e.g., key risk factors) working from the present to the past.

2.2 Reverse Time Attention Model RETAIN

Figure 2 shows the high-level overview of our model, where a central feature is to delegate a
considerable portion of the prediction responsibility to the process for generating attention weights.
This is intended to address, in part, the difﬁculty with interpreting RNNs where the recurrent weights
feed past information to the hidden layer. Therefore, to consider both the visit-level and the variable-
level (individual coordinates of xi) inﬂuence, we use a linear embedding of the input vector xi. That
is, we deﬁne

vi = Wembxi,
(Step 1)
where vi ∈ Rm denotes the embedding of the input vector xi ∈ Rr, m the size of the embedding di-
mension, Wemb ∈ Rm×r the embedding matrix to learn. We can alternatively use more sophisticated
yet interpretable representations such as those derived from multilayer perceptron (MLP) [13, 25].
MLP has been used for representation learning in EHR data [10].

We use two sets of weights, one for the visit-level attention and the other for variable-level attention,
respectively. The scalars α1, . . . , αi are the visit-level attention weights that govern the inﬂuence of
each visit embedding v1, . . . , vi. The vectors β1, . . . , βi are the variable-level attention weights that
focus on each coordinate of the visit embedding v1,1, v1,2, . . . , v1,m, . . . , vi,1, vi,2, . . . , vi,m.

We use two RNNs, RNNα and RNNβ, to separately generate α’s and β’s as follows,

gi, gi−1, . . . , g1 = RNNα(vi, vi−1, . . . , v1),

ej = w(cid:62)

α gj + bα,

for

j = 1, . . . , i

α1, α2, . . . , αi = Softmax(e1, e2, . . . , ei)
hi, hi−1, . . . , h1 = RNNβ(vi, vi−1, . . . , v1)
for

βj = tanh (cid:0)Wβhj + bβ

(cid:1)

(Step 2)

j = 1, . . . , i,

(Step 3)

3

Figure 2: Unfolded view of RETAIN’s architecture: Given input sequence x1, . . . , xi, we predict the
label yi. Step 1: Embedding, Step 2: generating α values using RNNα, Step 3: generating β values
using RNNβ, Step 4: Generating the context vector using attention and representation vectors, and
Step 5: Making prediction. Note that in Steps 2 and 3 we use RNN in the reversed time.

where gi ∈ Rp is the hidden layer of RNNα at time step i, hi ∈ Rq the hidden layer of RNNβ
at time step i and wα ∈ Rp, bα ∈ R, Wβ ∈ Rm×q and bβ ∈ Rm are the parameters to learn.
The hyperparameters p and q determine the hidden layer size of RNNα and RNNβ, respectively.
Note that for prediction at each timestamp, we generate a new set of attention vectors α and β. For
simplicity of notation, we do not include the index for predicting at different time steps. In Step 2,
we can use Sparsemax [28] instead of Softmax for sparser attention weights.

As noted, RETAIN generates the attention vectors by running the RNNs backward in time; i.e., RNNα
and RNNβ both take the visit embeddings in a reverse order vi, vi−1, . . . , v1. Running the RNN
in reversed time order also offers computational advantages since the reverse time order allows us
to generate e’s and β’s that dynamically change their values when making predictions at different
time steps i = 1, 2, . . . , T . This ensures that the attention vectors are modiﬁed at each time step,
increasing the computational stability of the attention generation process.1

Using the generated attentions, we obtain the context vector ci for a patient up to the i-th visit as
follows,

ci =

αjβj (cid:12) vj,

i
(cid:88)

j=1

(Step 4)

where (cid:12) denotes element-wise multiplication. We use the context vector ci ∈ Rm to predict the true
label yi ∈ {0, 1}s as follows,

(cid:98)yi = Softmax(Wci + b),
(Step 5)
where W ∈ Rs×m and b ∈ Rs are parameters to learn. We use the cross-entropy to calculate the
classiﬁcation loss as follows,

L(x1, . . . , xT ) = −

1
N

N
(cid:88)

n=1

1
T (n)

T (n)
(cid:88)

(cid:16)

i=1

y(cid:62)
i

(cid:17)
log((cid:98)yi) + (1 − yi)(cid:62) log(1 − (cid:98)yi)

(1)

where we sum the cross entropy errors from all dimensions of (cid:98)yi. In case of real-valued output
yi ∈ Rs, we can change the cross-entropy in Eq. (1) to, for example, mean squared error.
Overall, our attention mechanism can be viewed as the inverted architecture of the standard attention
mechanism for NLP [2] where the words are encoded by RNN and the attention weights are generated
by MLP. In contrast, our method uses MLP to embed the visit information to preserve interpretability
and uses RNN to generate two sets of attention weights, recovering the sequential information as
well as mimicking the behavior of physicians. Note that we did not use the timestamp of each visit
in our formulation. Using timestamps, however, provides a small improvement in the prediction
performance. We propose a method to use timestamps in Appendix A.

1For example, feeding visit embeddings in the original order to RNNα and RNNβ will generate the same e1
and β1 for every time step i = 1, 2, . . . , T . Moreover, in many cases, a patient’s recent visit records deserve
more attention than the old records. Then we need to have ej+1 > ej which makes the process computationally
unstable for long sequences.

4

3

Interpreting RETAIN

Finding the visits that contribute to prediction are derived using the largest αi, which is straightforward.
However, ﬁnding inﬂuential variables is slightly more involved as a visit is represented by an ensemble
of medical variables, each of which can vary in its predictive contribution. The contribution of each
variable is determined by v, β and α, and interpretation of α alone informs which visit is inﬂuential
in prediction but not why.

We propose a method to interpret the end-to-end behavior of RETAIN. By keeping α and β values
ﬁxed as the attention of doctors, we analyze changes in the probability of each label yi,1, . . . , yi,s
in relation to changes in the original input x1,1, . . . , x1,r, . . . , xi,1, . . . , xi,r. The xj,k that yields the
largest change in yi,d will be the input variable with highest contribution. More formally, given the
sequence x1, . . . , xi, we are trying to predict the probability of the output vector yi ∈ {0, 1}s, which
can be expressed as follows

p(yi|x1, . . . , xi) = p(yi|ci) = Softmax (Wci + b)
(2)
where ci ∈ Rm denotes the context vector. According to Step 4, ci is the sum of the visit embeddings
v1, . . . , vi weighted by the attentions α’s and β’s. Therefore Eq (2) can be rewritten as follows,

p(yi|x1, . . . , xi) = p(yi|ci) = Softmax

W

αjβj (cid:12) vj

+ b

(3)

(cid:18)

(cid:16) i
(cid:88)

j=1

(cid:17)

(cid:19)

Using the fact that the visit embedding vi is the sum of the columns of Wemb weighted by each
element of xi, Eq (3) can be rewritten as follows,

p(yi|x1, . . . , xi) = Softmax

W

αjβj (cid:12)

xj,kWemb[:, k]

+ b

(cid:18)

(cid:16) i
(cid:88)

j=1

(cid:18) i

(cid:88)

r
(cid:88)

j=1

k=1

r
(cid:88)

k=1

(cid:16)

(cid:17)

(cid:17)

(cid:19)

(cid:19)

= Softmax

xj,k αjW

βj (cid:12) Wemb[:, k]

+ b

(4)

where xj,k is the k-th element of the input vector xj. Eq (4) can be completely deconstructed to the
variables at each input x1, . . . , xi, which allows for calculating the contribution ω of the k-th variable
of the input xj at time step j ≤ i, for predicting yi as follows,

ω(yi, xj,k) = αjW(βj (cid:12) Wemb[:, k])
(cid:125)

(cid:123)(cid:122)
Contribution coefﬁcient

(cid:124)

xj,k
(cid:124)(cid:123)(cid:122)(cid:125)
Input value

,

(5)

where the index i of yi is omitted in the αj and βj. As we have described in Section 2.2, we are
generating α’s and β’s at time step i in the visit sequence x1, . . . , xT . Therefore the index i is always
assumed for α’s and β’s. Additionally, Eq (5) shows that when we are using a binary input value, the
coefﬁcient itself is the contribution. However, when we are using a non-binary input value, we need
to multiply the coefﬁcient and the input value xj,k to correctly calculate the contribution.

4 Experiments

We compared performance of RETAIN to RNNs and traditional machine learning methods. Given
space constraints, we only report the results on the learning to diagnose (L2D) task and summarize the
encounter sequence modeling (ESM) in Appendix C. The RETAIN source code is publicly available
at https://github.com/mp2893/retain.

4.1 Experimental setting
Source of data: The dataset consists of electronic health records from Sutter Health. The patients
are 50 to 80 years old adults chosen for a heart failure prediction model study. From the encounter
records, medication orders, procedure orders and problem lists, we extracted visit records consisting
of diagnosis, medication and procedure codes. To reduce the dimensionality while preserving the
clinical information, we used existing medical groupers to aggregate the codes into input variables.
The details of the medical groupers are given in the Appendix B. A proﬁle of the dataset is summarized
in Table 1.

5

Table 1: Statistics of EHR dataset. (D:Diagnosis, R:Medication, P:Procedure)

# of patients
# of visits
Avg. # of visits per patient
# of medical code groups

263,683
14,366,030
54.48

Avg. # of codes in a visit
Max # of codes in a visit
Avg. # of Dx codes in a visit

615 (D:283, R:94, P:238) Max # of Dx in a visit

3.03
62
1.83
42

Implementation details: We implemented RETAIN with Theano 0.8 [4]. For training the model, we
used Adadelta [35] with the mini-batch of 100 patients. The training was done in a machine equipped
with Intel Xeon E5-2630, 256GB RAM, two Nvidia Tesla K80’s and CUDA 7.5.

Baselines: For comparison, we completed the following models.

• Logistic regression (LR): We compute the counts of medical codes for each patient based on all
her visits as input variables and normalize the vector to zero mean and unit variance. We use the
resulting vector to train the logistic regression.

• MLP: We use the same feature construction as LR, but put a hidden layer of size 256 between

the input and output.

• RNN: RNN with two hidden layers of size 256 implemented by the GRU. Input sequences
x1, . . . , xi are used. Logistic regression is applied to the top hidden layer. We use two layers of
RNN of to match the model complexity of RETAIN.

• RNN+αM : One layer single directional RNN (hidden layer size 256) along time to generate the
input embeddings v1, . . . , vi. We use the MLP with a single hidden layer of size 256 to generate
the visit-level attentions α1, . . . , αi. We use the input embeddings v1, . . . , vi as the input to the
MLP. This baseline corresponds to Figure 1a.

• RNN+αR: This is similar to RNN+αM but uses the reverse-order RNN (hidden layer size 256)
to generate the visit-level attentions α1, . . . , αi. We use this baseline to conﬁrm the effectiveness
of generating the attentions using reverse time order.

The comparative visualization of the baselines are provided in Appendix D. We use the same
implementation and training method for the baselines as described above. The details on the hyper-
parameters, regularization and drop-out strategies for the baselines are described in Appendix B.

Evaluation measures: Model accuracy was measured by:

• Negative log-likelihood that measures the model loss on the test set. The loss can be calculated

by Eq (1).

• Area Under the ROC Curve (AUC) of comparing (cid:98)yi with the true label yi. AUC is more
robust to imbalanced positive/negative prediction labels, making it appropriate for evaluation of
classiﬁcation accuracy in the heart failure prediction task.

We also report the bootstrap (10,000 runs) estimate of the standard deviation of the evaluation
measures.

4.2 Heart Failure Prediction

Objective: Given a visit sequence x1, . . . , xT , we predicted if a primary care patient will be
diagnosed with heart failure (HF). This is a special case of ESM with a single disease outcome at
the end of the sequence. Since this is a binary prediction task, we use the logistic sigmoid function
instead of the Softmax in Step 5.

Cohort construction: From the source dataset, 3,884 cases are selected and approximately 10
controls are selected for each case (28,903 controls). The case/control selection criteria are fully
described in the supplementary section. Cases have index dates to denote the date they are diagnosed
with HF. Controls have the same index dates as their corresponding cases. We extract diagnosis codes,
medication codes and procedure codes in the 18-months window before the index date.

Training details: The patient cohort was divided into the training, validation and test sets in a
0.75:0.1:0.15 ratio. The validation set was used to determine the values of the hyper-parameters. See
Appendix B for details of hyper-parameter tuning.

6

Table 2: Heart failure prediction performance of RETAIN and the baselines

Train Time / epoch Test Time

Model
LR
MLP
RNN
RNN+αM
RNN+αR
RETAIN

Test Neg Log Likelihood
0.3269 ± 0.0105
0.2959 ± 0.0083
0.2577 ± 0.0082
0.2691 ± 0.0082
0.2605 ± 0.0088
0.2562 ± 0.0083

AUC
0.7900 ± 0.0111
0.8256 ± 0.0096
0.8706 ± 0.0080
0.8624 ± 0.0079
0.8717 ± 0.0080
0.8705 ± 0.0081

0.15s
0.25s
10.3s
6.7s
10.4s
10.8s

0.11s
0.11s
0.57s
0.48s
0.62s
0.63s

Results: Logistic regression and MLP underperformed compared to the four temporal learning
algorithms (Table 2). RETAIN is comparable to the other RNN variants in terms of prediction
performance while offering the interpretation beneﬁt.

Note that RNN+αR model are a degenerated version of RETAIN with only scalar attention, which is
still a competitive model as shown in table 2. This conﬁrms the efﬁciency of generating attention
weights using the RNN. However, RNN+αR model only provides scalar visit-level attention, which
is not sufﬁcient for healthcare applications. Patients often receives several medical codes at a single
visit, and it will be important to distinguish their relative importance to the target. We show such a
case study in section 4.3.

Table 2 also shows the scalability of RETAIN, as its training time (the number of seconds to train
the model over the entire training set once) is comparable to RNN. The test time is the number
of seconds to generate the prediction output for the entire test set. We use the mini-batch of 100
patients when assessing both training and test times. RNN takes longer than RNN+αM because of its
two-layer structure, whereas RNN+αM uses a single layer RNN. The models that use two RNNs
(RNN, RNN+αR, RETAIN)2 take similar time to train for one epoch. However, each model required
a different number of epochs to converge. RNN typically takes approximately 10 epochs, RNN+αM
and RNN+αR 15 epochs and RETAIN 30 epochs. Lastly, training the attention models (RNN+αM ,
RNN+αR and RETAIN) for ESM would take considerably longer than L2D, because ESM modeling
generates context vectors at each time step. RNN, on the other hand, does not require additional
computation other than embedding the visit to its hidden layer to predict target labels at each time
step. Therefore, in ESM, the training time of the attention models will increase linearly in relation to
the length of the input sequence.

4.3 Model Interpretation for Heart Failure Prediction
We evaluated the interpretability of RETAIN in the HF prediction task by choosing a HF patient from
the test set and calculating the contribution of the variables (medical codes in this case) to diagnostic
prediction. The patient suffered from skin problems, skin disorder (SD), benign neoplasm (BN),
excision of skin lesion (ESL), for some time before showing symptoms of HF, cardiac dysrhythmia
(CD), heart valve disease (HVD) and coronary atherosclerosis (CA), and then a diagnosis of HF
(Figure 3). We can see that skin-related codes from the earlier visits made little contribution to HF
prediction as expected. RETAIN properly puts much attention to the HF-related codes that occurred in
recent visits.

To conﬁrm RETAIN’s ability to exploit the sequence information of the EHR data, we reverse the visit
sequence of Figure 3a and feed it to RETAIN. Figure 3b shows the contribution of the medical codes
of the reversed visit record. HF-related codes in the past are still making positive contributions, but
not as much as they did in Figure 3a. Figure 3b also emphasizes RETAIN’s superiority to interpretable,
but stationary models such as logistic regression. Stationary models often aggregate past information
and remove the temporality from the input data, which can mistakenly lead to the same risk prediction
for Figure 3a and 3b. RETAIN, however, can correctly digest the sequence information and calculates
the HF risk score of 9.0%, which is signiﬁcantly lower than that of Figure 3a.

Figure 3c shows how the contributions of codes change when selected medication data are used in
the model. We added two medications from day 219: antiarrhythmics (AA) and anticoagulants (AC),
both of which are used to treat cardiac dysrhythmia (CD). The two medications make a negative
contributions, especially towards the end of the record. The medications decreased the positive
contributions of heart valve disease and cardiac dysrhythmia in the last visit. Indeed, the HF risk

2The RNN baseline uses two layers of RNN, RNN+αR uses one for visit embedding and one for generating

α, RETAIN uses each for generating α and β

7

Figure 3: (a) Temporal visualization of a patient’s visit records where the contribution of variables for
diagnosis of heart failure (HF) is summarized along the x-axis (i.e. time) with the y-axis indicating
the magnitude of visit and code speciﬁc contributions to HF diagnosis. (b) We reverse the order of
the visit sequence to see if RETAIN can properly take into account the modiﬁed sequence information.
(c) Medication codes are added to the visit record to see how it changes the behavior of RETAIN.

prediction (0.2165) of Figure 3c is lower than that of Figure 3a (0.2474). This suggests that taking
proper medications can help the patient in reducing their HF risk.

5 Conclusion

Our approach to modeling event sequences as predictors of HF diagnosis suggest that complex
models can offer both superior predictive accuracy and more precise interpretability. Given the power
of RNNs for analyzing sequential data, we proposed RETAIN, which preserves RNN’s predictive
power while allowing a higher degree of interpretation. The key idea of RETAIN is to improve
the prediction accuracy through a sophisticated attention generation process, while keeping the
representation learning part simple for interpretation, making the entire algorithm accurate and
interpretable. RETAIN trains two RNN in a reverse time order to efﬁciently generate the appropriate
attention variables. For future work, we plan to develop an interactive visualization system for
RETAIN and evaluating RETAIN in other healthcare applications.

References

In ICLR, 2015.

[1] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. In ICLR, 2015.
[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.

[3] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.

Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.

[4] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of SciPy, 2010.
[5] A. D. Black, J. Car, C. Pagliari, C. Anandan, K. Cresswell, T. Bokun, B. McKinstry, R. Procter, A. Majeed,
and A. Sheikh. The impact of ehealth on the quality and safety of health care: a systematic overview. PLoS
Med, 8(1):e1000387, 2011.

[6] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad. Intelligible models for healthcare:

Predicting pneumonia risk and hospital 30-day readmission. In KDD, 2015.

[7] B. Chaudhry, J. Wang, S. Wu, M. Maglione, W. Mojica, E. Roth, S. C. Morton, and P. G. Shekelle.
Systematic review: impact of health information technology on quality, efﬁciency, and costs of medical
care. Annals of internal medicine, 144(10):742–752, 2006.

8

[8] Z. Che, S. Purushotham, R. Khemani, and Y. Liu. Distilling knowledge from deep networks with

applications to healthcare domain. arXiv preprint arXiv:1512.03542, 2015.

[9] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning
phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014.

[10] E. Choi, M. T. Bahadori, E. Searles, C. Coffey, and J. Sun. Multi-layer representation learning for medical

[11] E. Choi, M. T. Bahadori, and J. Sun. Doctor ai: Predicting clinical events via recurrent neural networks.

concepts. In KDD, 2016.

arXiv preprint arXiv:1511.05942, 2015.

recognition. In NIPS, pages 577–585, 2015.

University of Montreal, 2009.

[12] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. Attention-based models for speech

[13] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.

[14] C. Esteban, O. Staeck, Y. Yang, and V. Tresp. Predicting clinical events by combining static and dynamic

information using recurrent neural networks. arXiv preprint arXiv:1602.02685, 2016.

[15] A. S. Fleisher, B. B. Sowell, C. Taylor, A. C. Gamst, R. C. Petersen, L. J. Thal, and f. t. A. D. C. Study.
Clinical predictors of progression to Alzheimer disease in amnestic mild cognitive impairment. Neurology,
68(19):1588–1595, 2007.

[16] B. Gallego, S. R. Walter, R. O. Day, A. G. Dunn, V. Sivaraman, N. Shah, C. A. Longhurst, and E. Coiera.
Bringing cohort studies to the bedside: framework for a ’green button’ to support clinical decision-making.
Journal of Comparative Effectiveness Research, pages 1–7, 2015.

[17] J. Ghosh and V. Karamcheti. Sequence learning with recurrent networks: analysis of internal representations.

In Aerospace Sensing, pages 449–460, 1992.

[18] C. L. Goldzweig, A. Towﬁgh, M. Maglione, and P. G. Shekelle. Costs and beneﬁts of health information

technology: new trends from the literature. Health affairs, 28(2):w282–w293, 2009.

[19] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image

generation. arXiv preprint arXiv:1502.04623, 2015.

[20] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching

machines to read and comprehend. In NIPS, pages 1684–1692, 2015.

[21] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

[22] A. K. Jha, C. M. DesRoches, E. G. Campbell, K. Donelan, S. R. Rao, T. G. Ferris, A. Shields, S. Rosenbaum,

and D. Blumenthal. Use of electronic health records in us hospitals. N Engl J Med, 2009.

[23] A. Karpathy, J. Johnson, and F.-F. Li. Visualizing and understanding recurrent networks. arXiv preprint

arXiv:1506.02078, 2015.

[24] A. N. Kho et al. Use of diverse electronic medical record systems to identify genetic risk for type 2 diabetes

within a genome-wide association study. JAMIA, 19(2):212–218, 2012.

[25] Q. V. Le. Building high-level features using large scale unsupervised learning. In ICASSP, 2013.

[26] Q. V. Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent networks of rectiﬁed linear units.

arXiv preprint arXiv:1504.00941, 2015.

[27] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell. Learning to Diagnose with LSTM Recurrent Neural

Networks. In ICLR, 2016.

classiﬁcation. In ICML, 2016.

[28] A. F. Martins and R. F. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label

[29] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of visual attention. In NIPS, 2014.

[30] A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence summarization.

In EMNLP, 2015.

[31] S. Saria, D. Koller, and A. Penn. Learning individual and population level traits from clinical temporal

data. In NIPS, Predictive Models in Personalized Medicine workshop, 2010.

[32] P. Schulam and S. Saria. A probabilistic graphical model for individualizing prognosis in chronic, complex

diseases. In AMIA, volume 2015, page 143, 2015.

[33] J. Sun, F. Wang, J. Hu, and S. Edabollahi. Supervised patient similarity measure of heterogeneous patient

records. ACM SIGKDD Explorations Newsletter, 14(1):16–24, 2012.

[34] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell:

Neural image caption generation with visual attention. In ICML, 2015.

[35] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.

9

A A method to use the timestamps

i

As before, we use t(n)
to represent the timestamp of the i-th visit of the n-th patient. In the following,
we suppress the superscript (n) to avoid cluttered notation. Note that the timestamp ti can be anything
that provides the temporal information of the i-th visit: the number of days from the ﬁrst visit, the
number of days between two consecutive visits, or the number of days until the index date of some
event such as heart failure diagnosis.

In order to use the timestamps, we modify Step 2 and Step 3 in Section 2.2 as follows:

gi, gi−1, . . . , g1 = RNNα(v

(cid:48)

(cid:48)

i, v
α gj + bα,

i−1, . . . , v
for

(cid:48)

1),

j = 1, . . . , i

ej = w(cid:62)

α1, α2, . . . , αi = Softmax(e1, e2, . . . , ei)

hi, hi−1, . . . , h1 = RNNβ(v

(cid:48)

(cid:48)

i, v

i−1, . . . , v

βj = tanh (cid:0)Wβhj + bβ
i = [vi, ti]

(cid:48)

(cid:1)

where v

(cid:48)

1)
for

j = 1, . . . , i,

(cid:48)

where we use v
i, the concatenation of the visit embedding vi and the timestamp ti, to generate the
(cid:48)
attentions α and β. However, when obtaining the context vector ci as per Step 4, we use vi, not v
i
to match the dimensionality. The entire process could be understood such that we use the temporal
information not to embed each visit, but to calculate the attentions for the entire visit sequence. This
is consistent with our modeling approach where we lose the sequential information in embedding the
visit with MLP, then recover the sequential information by generating the attentions using the RNN.
By using the temporal information, speciﬁcally the log of the number of days from the ﬁrst visit, we
were able to improve the heart failure prediction AUC by 0.003 without any hyper-parameter tuning.

B Details of the experiment settings

B.1 Hyper-parameter Tuning

We used the validation set to tune the hyper-parameters: visit embedding size m, RNNα’s hidden
layer size p, RNNβ’s hidden layer size q, L2 regularization coefﬁcient, and drop-out rates.

L2 regularization was applied to all weights except the ones in RNNα and RNNβ. Two separate
drop-outs were used on the visit embedding vi and the context vector ci. We performed the random
search with predeﬁned ranges m, p, q ∈ {32, 64, 128, 200, 256}, L2 ∈ {0.1, 0.01, 0.001, 0.0001},
dropoutvi, dropoutci ∈ {0.0, 0.2, 0.4, 0.6, 0.8}. We also performed the random search with m, p
and q ﬁxed to 256.

The ﬁnal value we used to train RETAIN for heart failure prediction is m, p, q = 128, dropoutvi = 0.6,
dropoutci = 0.6 and 0.0001 for the L2 regularization coefﬁcient.

B.2 Code Grouper

Diagnosis codes, medication codes and procedure codes in the dataset are respectively using Interna-
tional Classiﬁcation of Diseases (ICD-9), Generic Product Identiﬁer (GPI) and Current Procedural
Terminology (CPT).
Diagnosis codes are grouped by Clinical Classiﬁcations Software for ICD-93 which reduces the
number of diagnosis code from approximately 14,000 to 283. Medication codes are grouped by
Generic Product Identiﬁer Drug Group4 which reduces the dimension to from approximately 151,000
to 96. Procedure codes are grouped by Clinical Classiﬁcations Software for CPT5, which reduces the
number of CPT codes from approximately 9,000 to 238.

3https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp
4http://www.wolterskluwercdi.com/drug-data/medi-span-electronic-drug-ﬁle/
5https://www.hcup-us.ahrq.gov/toolssoftware/ccs_svcsproc/ccssvcproc.jsp

10

Table 3: Qualifying ICD-9 codes for heart failure

B.3 Training Speciﬁcs of the Basline Models

• LR: We use 0.01 L2 regularization coefﬁcient for the logistic regression weight.
• MLP: We use drop-out rate 0.6 on the output of the hidden layer. We use 0.0001 L2 regularization

coefﬁcient for the hidden layer weight and the logistic regression weight.

• RNN: We use drop-out rate 0.6 on the outputs of both hidden layers. We use 0.0001 L2
regularization coefﬁcient for the logistic regression weight. The dimension size of both hidden
layers is 256.

• RNN+αM : We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of
the context vector (cid:80)
i αivi. We use 0.0001 L2 regularization coefﬁcient for the hidden layer
weight of the MLP that generates α’s and the logistic regression weight. The dimension size of
the hidden layers in both RNN and MLP is 256.

• RNN+αR: We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of
the context vector (cid:80)
i αivi. We use 0.0001 L2 regularization coefﬁcient for the hidden layer
weight of the RNN that generates α’s and the logistic regression weight. The dimension size of
the hidden layers in both RNNs is 256.

B.4 Heart Failure Case/Control Selection Criteria

Case patients were 40 to 85 years of age at the time of HF diagnosis. HF diagnosis (HFDx) is
deﬁned as: 1) Qualifying ICD-9 codes for HF appeared in the encounter records or medication orders.
Qualifying ICD-9 codes are displayed in Table 3. 2) a minimum of three clinical encounters with
qualifying ICD-9 codes had to occur within 12 months of each other, where the date of diagnosis was
assigned to the earliest of the three dates. If the time span between the ﬁrst and second appearances of
the HF diagnostic code was greater than 12 months, the date of the second encounter was used as the

11

ﬁrst qualifying encounter. The date at which HF diagnosis was given to the case is denoted as HFDx.
Up to ten eligible controls (in terms of sex, age, location) were selected for each case, yielding an
overall ratio of 9 controls per case. Each control was also assigned an index date, which is the HFDx
of the matched case. Controls are selected such that they did not meet the operational criteria for
HF diagnosis prior to the HFDx plus 182 days of their corresponding case. Control subjects were
required to have their ﬁrst ofﬁce encounter within one year of the matching HF case patient’s ﬁrst
ofﬁce visit, and have at least one ofﬁce encounter 30 days before or any time after the case’s HF
diagnosis date to ensure similar duration of observations among cases and controls.

C Results on encounter sequence modeling

Objective: Given a sequence of visits x1, . . . , xT , the goal of encounter sequence modeling is, for
each time step i, to predict the codes occurring at the next visit x2, . . . , xT +1. In this experiment,
we focus on predicting the diagnosis codes in the encounter sequence, so we create a separate set of
labels y1, . . . , yT that do not contain non-diagnosis codes such as medication codes or procedure
codes. Therefore yi will contain diagnosis codes from the next visit xi+1.

Dataset: We divide the entire dataset described in Table 1 into 0.75:0.10:0.15 ratio, respectively for
training set, validation set, and test set.

Baseline: We use the same baseline models we used for HF prediction. However, since we are
predicting 283 binary labels now, we replace the logistic regression function with the Softmax
function. The drop-out and L2 regularization policies remain the same.
For LR and MLP, at each step i, we aggregate maximum ten past input vectors6 xi−9, . . . , xi to create
a pseudo-context vector (cid:98)ci. LR applies the Softmax function on top of (cid:98)ci. MLP places a hidden layer
on top of (cid:98)ci then applies the Softmax function.
Evaluation metric: We use the negative log likelihood Eq (1) on the test set to evaluate the model
performance. We also use Recall@k as an additional metric to measure the prediction accuracy.

• Recall@k: Given a sequence of visits x1, . . . , xT , we evaluate the model performance based
on how accurately it can predict the diagnosis codes y1, . . . , yT . We use the average Recall@k,
which is expressed as below,

1
N

N
(cid:88)

n=1

1
T (n)

T (n)
(cid:88)

i=1

Recall@k((cid:98)yi), where Recall@k((cid:98)yi) =

|argsort((cid:98)yi)[: k] ∩ nonzero(yi)|
|nonzero(yi)|

where argsort returns a list of indices that will decrementally sort a given vector and nonzero
returns a list of indices of the coordinates with non-zero values. We use Recall@k because of its
similar nature to the way a human physician performs the differential diagnostic procedure, which
is to generate a list of most likely diseases for an undiagnosed patient, then perform medical
practice until the true disease, or diseases are determined.

Prediction accuracy: Table 4 displays the prediction performance of RETAIN and the baselines.
We use k = 5, 10 for Recall@k to allow a reasonable number of prediction trials, as well as cover
complex patients who often receive multiple diagnosis codes at a single visit.

RNN shows the best prediction accuracy for encounter diagnosis prediction. However, considering
the purpose of encounter diagnosis prediction, which is to assist doctors to provide quality care for
the patient, black-box behavior of RNN makes it unattractive as a clinical tool. On the other hand,
RETAIN performs as well as other attention models, only slightly inferior to RNN, provides full
interpretation of its prediction behavior, making it a feasible solution for clinical applications.

The interesting ﬁnding in Table 4 is that MLP is able to perform as accurately as RNN+αM in terms
of Recall@10. Considering the fact that MLP uses aggregated information of past ten visits, we can
assume that encounter diagnosis prediction depends more on the frequency of disease occurrences
rather than the order in which they occurred. This is quite different from the HF prediction task,
where stationary models (LF, MLP) performed signiﬁcantly worse than sequential models.

6We also tried aggregating all past input vectors x1, . . . , xi, but the performance was slightly worse than

using just ten.

12

Table 4: Encounter diagnosis prediction performance of RETAIN and the baselines

Model

LR
MLP
RNN
RNN+αM
RNN+αR
RETAIN

Negative
Likelihood
0.0288
0.0267
0.0258
0.0262
0.0259
0.0259

Recall@5 Recall@10

43.15
50.72
55.18
52.15
53.89
54.25

55.84
65.02
69.09
65.81
67.45
67.74

(a)

(b)

(c)

(d)

(e)

Figure 4: Graphical illustration of the baselines: (a) Logistic regression (LR), (b) Multilayer Percep-
tron (MLP), (c) Recurrent neural network (RNN), (d) RNN with attention vectors generated via an
MLP (RNN+αM ), (e) RNN with attention vectors generated via an RNN (RNN+αR). RETAIN is
given in Figure 1b.

D Illustration and comparison of the baselines

Figure 4 illustrates the baselines used in the experiments and shows the relationship among them.

13

7
1
0
2
 
b
e
F
 
6
2
 
 
]

G
L
.
s
c
[
 
 
4
v
5
4
7
5
0
.
8
0
6
1
:
v
i
X
r
a

RETAIN: An Interpretable Predictive Model for
Healthcare using Reverse Time Attention Mechanism

Edward Choi∗, Mohammad Taha Bahadori∗, Joshua A. Kulas∗,
Andy Schuetz†, Walter F. Stewart†, Jimeng Sun∗
† Sutter Health
∗ Georgia Institute of Technology

{mp2893,bahadori,jkulas3}@gatech.edu,
{schueta1,stewarwf}@sutterhealth.org, jsun@cc.gatech.edu

Abstract
Accuracy and interpretability are two dominant features of successful predictive
models. Typically, a choice must be made in favor of complex black box models
such as recurrent neural networks (RNN) for accuracy versus less accurate but
more interpretable traditional models such as logistic regression. This tradeoff
poses challenges in medicine where both accuracy and interpretability are impor-
tant. We addressed this challenge by developing the REverse Time AttentIoN
model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN
achieves high accuracy while remaining clinically interpretable and is based on
a two-level neural attention model that detects inﬂuential past visits and signiﬁ-
cant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics
physician practice by attending the EHR data in a reverse time order so that recent
clinical visits are likely to receive higher attention. RETAIN was tested on a large
health system EHR dataset with 14 million visits completed by 263K patients over
an 8 year period and demonstrated predictive accuracy and computational scalabil-
ity comparable to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.

1

Introduction

The broad adoption of Electronic Health Record (EHR) systems has opened the possibility of
applying clinical predictive models to improve the quality of clinical care. Several systematic reviews
have underlined the care quality improvement using predictive analysis [7, 22, 5, 18]. EHR data
can be represented as temporal sequences of high-dimensional clinical variables (e.g., diagnoses,
medications and procedures), where the sequence ensemble represents the documented content of
medical visits from a single patient. Traditional machine learning tools summarize this ensemble into
aggregate features, ignoring the temporal and sequence relationships among the feature elements.
The opportunity to improve both predictive accuracy and interpretability is likely to derive from
effectively modeling temporality and high-dimensionality of these event sequences.

Accuracy and interpretability are two dominant features of successful predictive models. There is a
common belief that one has to trade accuracy for interpretability using one of three types of traditional
models [6]: 1) identifying a set of rules (e.g. via decision trees [24]), 2) case-based reasoning by
ﬁnding similar patients (e.g. via k-nearest neighbors [16] and distance metric learning [33]), and 3)
identifying a list of risk factors (e.g. via LASSO coefﬁcients [15]). While interpretable, all of these
models rely on aggregated features, ignoring the temporal relation among features inherent to EHR
data. As a consequence, model accuracy is sub-optimal. Latent-variable time-series models, such as
[31, 32], account for temporality, but often have limited interpretation due to abstract state variables.

Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential
EHR data to predict diagnoses [27] and model encounter sequences [11, 14]. But, the gain in accuracy

29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

(a) Standard attention model

(b) RETAIN model

Figure 1: Common attention models vs. RETAIN, using folded diagrams of RNNs. (a) Standard
attention mechanism: the recurrence on the hidden state vector vi hinders interpretation of the model.
(b) Attention mechanism in RETAIN: The recurrence is on the attention generation components (hi or
gi) while the hidden state vi is generated by a simpler more interpretable output.

from use of RNNs is at the cost of model output that is notoriously difﬁcult to interpret. While
there have been several attempts at directly interpreting RNNs [17, 23, 8], these methods are not
sufﬁciently developed for application in clinical care.

We have addressed this limitation using a modeling strategy known as RETAIN, a two-level neural
attention model for sequential data that provides detailed interpretation of the prediction results while
retaining the prediction accuracy comparable to RNN. To this end, RETAIN relies on an attention
mechanism modeled to represent the behavior of physicians during an encounter. A distinguishing
feature of RETAIN (see Figure 1) is to leverage sequence information using an attention generation
mechanism, while learning an interpretable representation. And emulating physician behaviors,
RETAIN examines a patient’s past visits in reverse time order, facilitating a more stable attention
generation. As a result, RETAIN identiﬁes the most meaningful visits and quantiﬁes visit speciﬁc
features that contribute to the prediction.

RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K
patients over an 8 year period. We compared predictive accuracy of RETAIN to traditional machine
learning methods and to RNN variants using a case-control dataset to predict a future diagnosis of
heart failure. The comparative analysis demonstrates that RETAIN achieves comparable performance
to RNN in both accuracy and speed and signiﬁcantly outperforms traditional models. Moreover,
using a concrete case study and visualization method, we demonstrate how RETAIN offers an intuitive
interpretation.

2 Methodology

We ﬁrst describe the structure of sequential EHR data and our notation, then follow with a general
framework for predictive analysis in healthcare using EHR, followed by details of the RETAIN method.

EHR Structure and our Notation. The EHR data of each patient can be represented as a time-
labeled sequence of multivariate observations. Assuming we use r different variables, the n-th patient
of N total patients can be represented by a sequence of T (n) tuples (t(n)
) ∈ R × Rr, i =
1, . . . , T (n). The timestamps t(n)
denotes the time of the i-th visit of the n-th patient and T (n) the
number of visits of the n-th patient. To minimize clutter, we describe the algorithms for a single patient
and have dropped the superscript (n) whenever it is unambiguous. The goal of predictive modeling
is to predict the label at each time step yi ∈ {0, 1}s or at the end of the sequence y ∈ {0, 1}s. The
number of labels s can be more than one.

, x(n)
i

i

i

For example, in encounter sequence modeling (ESM) [11], each visit (e.g. encounter) of a patient’s
visit sequence is represented by a set of varying number of medical codes {c1, c2, . . . , cn}. cj is
the j-th code from the vocabulary C. Therefore, in ESM, the number of variables r = |C| and input

2

xi ∈ {0, 1}|C| is a binary vector where the value one in the j-th coordinate indicates that cj was
documented in i-th visit. Given a sequence of visits x1, . . . , xT , the goal of ESM is, for each time
step i, to predict the codes occurring at the next visit x2, . . . , xT +1, with the number of labels s = |C|.

In case of learning to diagnose (L2D) [27], the input vector xi consists of continuous clinical measures.
If there are r different measurements, then xi ∈ Rr. The goal of L2D is, given an input sequence
x1, . . . , xT , to predict the occurrence of a speciﬁc disease (s = 1) or multiple diseases (s > 1).
Without loss of generality, we will describe the algorithm for ESM, as L2D can be seen as a special
case of ESM where we make a single prediction at the end of the visit sequence.

In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural
network variants that can cope with the vanishing gradient problem [3], such as LSTM [21], GRU
[9], and IRNN [26], with any depth (number of hidden layers).

2.1 Preliminaries on Neural Attention Models

Attention based neural network models are being successfully applied to image processing [1, 29, 19,
34], natural language processing [2, 20, 30] and speech recognition [12]. The utility of the attention
mechanism can be seen in the language translation task [2] where it is inefﬁcient to represent an
entire sentence with one ﬁxed-size vector because neural translation machines ﬁnds it difﬁcult to
translate the given sentence represented by a single vector.

Intuitively, the attention mechanism for language translation works as follows: given a sentence of
length S in the original language, we generate h1, . . . , hS, to represent the words in the sentence. To
ﬁnd the j-th word in the target language, we generate attentions αj
i for i = 1, . . . , S for each word in
the original sentence. Then, we compute the context vector cj = (cid:80)
i hi and use it to predict the
j-th word in the target language. In general, the attention mechanism allows the model to focus on a
speciﬁc word (or words) in the given sentence when generating each word in the target language.

i αj

We rely on a conceptually similar temporal attention mechanism to generate interpretable prediction
models using EHR data. Our model framework is motivated by and mimics how doctors attend to a
patient’s needs and explore the patient record, where there is a focus on speciﬁc clinical information
(e.g., key risk factors) working from the present to the past.

2.2 Reverse Time Attention Model RETAIN

Figure 2 shows the high-level overview of our model, where a central feature is to delegate a
considerable portion of the prediction responsibility to the process for generating attention weights.
This is intended to address, in part, the difﬁculty with interpreting RNNs where the recurrent weights
feed past information to the hidden layer. Therefore, to consider both the visit-level and the variable-
level (individual coordinates of xi) inﬂuence, we use a linear embedding of the input vector xi. That
is, we deﬁne

vi = Wembxi,
(Step 1)
where vi ∈ Rm denotes the embedding of the input vector xi ∈ Rr, m the size of the embedding di-
mension, Wemb ∈ Rm×r the embedding matrix to learn. We can alternatively use more sophisticated
yet interpretable representations such as those derived from multilayer perceptron (MLP) [13, 25].
MLP has been used for representation learning in EHR data [10].

We use two sets of weights, one for the visit-level attention and the other for variable-level attention,
respectively. The scalars α1, . . . , αi are the visit-level attention weights that govern the inﬂuence of
each visit embedding v1, . . . , vi. The vectors β1, . . . , βi are the variable-level attention weights that
focus on each coordinate of the visit embedding v1,1, v1,2, . . . , v1,m, . . . , vi,1, vi,2, . . . , vi,m.

We use two RNNs, RNNα and RNNβ, to separately generate α’s and β’s as follows,

gi, gi−1, . . . , g1 = RNNα(vi, vi−1, . . . , v1),

ej = w(cid:62)

α gj + bα,

for

j = 1, . . . , i

α1, α2, . . . , αi = Softmax(e1, e2, . . . , ei)
hi, hi−1, . . . , h1 = RNNβ(vi, vi−1, . . . , v1)
for

βj = tanh (cid:0)Wβhj + bβ

(cid:1)

(Step 2)

j = 1, . . . , i,

(Step 3)

3

Figure 2: Unfolded view of RETAIN’s architecture: Given input sequence x1, . . . , xi, we predict the
label yi. Step 1: Embedding, Step 2: generating α values using RNNα, Step 3: generating β values
using RNNβ, Step 4: Generating the context vector using attention and representation vectors, and
Step 5: Making prediction. Note that in Steps 2 and 3 we use RNN in the reversed time.

where gi ∈ Rp is the hidden layer of RNNα at time step i, hi ∈ Rq the hidden layer of RNNβ
at time step i and wα ∈ Rp, bα ∈ R, Wβ ∈ Rm×q and bβ ∈ Rm are the parameters to learn.
The hyperparameters p and q determine the hidden layer size of RNNα and RNNβ, respectively.
Note that for prediction at each timestamp, we generate a new set of attention vectors α and β. For
simplicity of notation, we do not include the index for predicting at different time steps. In Step 2,
we can use Sparsemax [28] instead of Softmax for sparser attention weights.

As noted, RETAIN generates the attention vectors by running the RNNs backward in time; i.e., RNNα
and RNNβ both take the visit embeddings in a reverse order vi, vi−1, . . . , v1. Running the RNN
in reversed time order also offers computational advantages since the reverse time order allows us
to generate e’s and β’s that dynamically change their values when making predictions at different
time steps i = 1, 2, . . . , T . This ensures that the attention vectors are modiﬁed at each time step,
increasing the computational stability of the attention generation process.1

Using the generated attentions, we obtain the context vector ci for a patient up to the i-th visit as
follows,

ci =

αjβj (cid:12) vj,

i
(cid:88)

j=1

(Step 4)

where (cid:12) denotes element-wise multiplication. We use the context vector ci ∈ Rm to predict the true
label yi ∈ {0, 1}s as follows,

(cid:98)yi = Softmax(Wci + b),
(Step 5)
where W ∈ Rs×m and b ∈ Rs are parameters to learn. We use the cross-entropy to calculate the
classiﬁcation loss as follows,

L(x1, . . . , xT ) = −

1
N

N
(cid:88)

n=1

1
T (n)

T (n)
(cid:88)

(cid:16)

i=1

y(cid:62)
i

(cid:17)
log((cid:98)yi) + (1 − yi)(cid:62) log(1 − (cid:98)yi)

(1)

where we sum the cross entropy errors from all dimensions of (cid:98)yi. In case of real-valued output
yi ∈ Rs, we can change the cross-entropy in Eq. (1) to, for example, mean squared error.
Overall, our attention mechanism can be viewed as the inverted architecture of the standard attention
mechanism for NLP [2] where the words are encoded by RNN and the attention weights are generated
by MLP. In contrast, our method uses MLP to embed the visit information to preserve interpretability
and uses RNN to generate two sets of attention weights, recovering the sequential information as
well as mimicking the behavior of physicians. Note that we did not use the timestamp of each visit
in our formulation. Using timestamps, however, provides a small improvement in the prediction
performance. We propose a method to use timestamps in Appendix A.

1For example, feeding visit embeddings in the original order to RNNα and RNNβ will generate the same e1
and β1 for every time step i = 1, 2, . . . , T . Moreover, in many cases, a patient’s recent visit records deserve
more attention than the old records. Then we need to have ej+1 > ej which makes the process computationally
unstable for long sequences.

4

3

Interpreting RETAIN

Finding the visits that contribute to prediction are derived using the largest αi, which is straightforward.
However, ﬁnding inﬂuential variables is slightly more involved as a visit is represented by an ensemble
of medical variables, each of which can vary in its predictive contribution. The contribution of each
variable is determined by v, β and α, and interpretation of α alone informs which visit is inﬂuential
in prediction but not why.

We propose a method to interpret the end-to-end behavior of RETAIN. By keeping α and β values
ﬁxed as the attention of doctors, we analyze changes in the probability of each label yi,1, . . . , yi,s
in relation to changes in the original input x1,1, . . . , x1,r, . . . , xi,1, . . . , xi,r. The xj,k that yields the
largest change in yi,d will be the input variable with highest contribution. More formally, given the
sequence x1, . . . , xi, we are trying to predict the probability of the output vector yi ∈ {0, 1}s, which
can be expressed as follows

p(yi|x1, . . . , xi) = p(yi|ci) = Softmax (Wci + b)
(2)
where ci ∈ Rm denotes the context vector. According to Step 4, ci is the sum of the visit embeddings
v1, . . . , vi weighted by the attentions α’s and β’s. Therefore Eq (2) can be rewritten as follows,

p(yi|x1, . . . , xi) = p(yi|ci) = Softmax

W

αjβj (cid:12) vj

+ b

(3)

(cid:18)

(cid:16) i
(cid:88)

j=1

(cid:17)

(cid:19)

Using the fact that the visit embedding vi is the sum of the columns of Wemb weighted by each
element of xi, Eq (3) can be rewritten as follows,

p(yi|x1, . . . , xi) = Softmax

W

αjβj (cid:12)

xj,kWemb[:, k]

+ b

(cid:18)

(cid:16) i
(cid:88)

j=1

(cid:18) i

(cid:88)

r
(cid:88)

j=1

k=1

r
(cid:88)

k=1

(cid:16)

(cid:17)

(cid:17)

(cid:19)

(cid:19)

= Softmax

xj,k αjW

βj (cid:12) Wemb[:, k]

+ b

(4)

where xj,k is the k-th element of the input vector xj. Eq (4) can be completely deconstructed to the
variables at each input x1, . . . , xi, which allows for calculating the contribution ω of the k-th variable
of the input xj at time step j ≤ i, for predicting yi as follows,

ω(yi, xj,k) = αjW(βj (cid:12) Wemb[:, k])
(cid:125)

(cid:123)(cid:122)
Contribution coefﬁcient

(cid:124)

xj,k
(cid:124)(cid:123)(cid:122)(cid:125)
Input value

,

(5)

where the index i of yi is omitted in the αj and βj. As we have described in Section 2.2, we are
generating α’s and β’s at time step i in the visit sequence x1, . . . , xT . Therefore the index i is always
assumed for α’s and β’s. Additionally, Eq (5) shows that when we are using a binary input value, the
coefﬁcient itself is the contribution. However, when we are using a non-binary input value, we need
to multiply the coefﬁcient and the input value xj,k to correctly calculate the contribution.

4 Experiments

We compared performance of RETAIN to RNNs and traditional machine learning methods. Given
space constraints, we only report the results on the learning to diagnose (L2D) task and summarize the
encounter sequence modeling (ESM) in Appendix C. The RETAIN source code is publicly available
at https://github.com/mp2893/retain.

4.1 Experimental setting
Source of data: The dataset consists of electronic health records from Sutter Health. The patients
are 50 to 80 years old adults chosen for a heart failure prediction model study. From the encounter
records, medication orders, procedure orders and problem lists, we extracted visit records consisting
of diagnosis, medication and procedure codes. To reduce the dimensionality while preserving the
clinical information, we used existing medical groupers to aggregate the codes into input variables.
The details of the medical groupers are given in the Appendix B. A proﬁle of the dataset is summarized
in Table 1.

5

Table 1: Statistics of EHR dataset. (D:Diagnosis, R:Medication, P:Procedure)

# of patients
# of visits
Avg. # of visits per patient
# of medical code groups

263,683
14,366,030
54.48

Avg. # of codes in a visit
Max # of codes in a visit
Avg. # of Dx codes in a visit

615 (D:283, R:94, P:238) Max # of Dx in a visit

3.03
62
1.83
42

Implementation details: We implemented RETAIN with Theano 0.8 [4]. For training the model, we
used Adadelta [35] with the mini-batch of 100 patients. The training was done in a machine equipped
with Intel Xeon E5-2630, 256GB RAM, two Nvidia Tesla K80’s and CUDA 7.5.

Baselines: For comparison, we completed the following models.

• Logistic regression (LR): We compute the counts of medical codes for each patient based on all
her visits as input variables and normalize the vector to zero mean and unit variance. We use the
resulting vector to train the logistic regression.

• MLP: We use the same feature construction as LR, but put a hidden layer of size 256 between

the input and output.

• RNN: RNN with two hidden layers of size 256 implemented by the GRU. Input sequences
x1, . . . , xi are used. Logistic regression is applied to the top hidden layer. We use two layers of
RNN of to match the model complexity of RETAIN.

• RNN+αM : One layer single directional RNN (hidden layer size 256) along time to generate the
input embeddings v1, . . . , vi. We use the MLP with a single hidden layer of size 256 to generate
the visit-level attentions α1, . . . , αi. We use the input embeddings v1, . . . , vi as the input to the
MLP. This baseline corresponds to Figure 1a.

• RNN+αR: This is similar to RNN+αM but uses the reverse-order RNN (hidden layer size 256)
to generate the visit-level attentions α1, . . . , αi. We use this baseline to conﬁrm the effectiveness
of generating the attentions using reverse time order.

The comparative visualization of the baselines are provided in Appendix D. We use the same
implementation and training method for the baselines as described above. The details on the hyper-
parameters, regularization and drop-out strategies for the baselines are described in Appendix B.

Evaluation measures: Model accuracy was measured by:

• Negative log-likelihood that measures the model loss on the test set. The loss can be calculated

by Eq (1).

• Area Under the ROC Curve (AUC) of comparing (cid:98)yi with the true label yi. AUC is more
robust to imbalanced positive/negative prediction labels, making it appropriate for evaluation of
classiﬁcation accuracy in the heart failure prediction task.

We also report the bootstrap (10,000 runs) estimate of the standard deviation of the evaluation
measures.

4.2 Heart Failure Prediction

Objective: Given a visit sequence x1, . . . , xT , we predicted if a primary care patient will be
diagnosed with heart failure (HF). This is a special case of ESM with a single disease outcome at
the end of the sequence. Since this is a binary prediction task, we use the logistic sigmoid function
instead of the Softmax in Step 5.

Cohort construction: From the source dataset, 3,884 cases are selected and approximately 10
controls are selected for each case (28,903 controls). The case/control selection criteria are fully
described in the supplementary section. Cases have index dates to denote the date they are diagnosed
with HF. Controls have the same index dates as their corresponding cases. We extract diagnosis codes,
medication codes and procedure codes in the 18-months window before the index date.

Training details: The patient cohort was divided into the training, validation and test sets in a
0.75:0.1:0.15 ratio. The validation set was used to determine the values of the hyper-parameters. See
Appendix B for details of hyper-parameter tuning.

6

Table 2: Heart failure prediction performance of RETAIN and the baselines

Train Time / epoch Test Time

Model
LR
MLP
RNN
RNN+αM
RNN+αR
RETAIN

Test Neg Log Likelihood
0.3269 ± 0.0105
0.2959 ± 0.0083
0.2577 ± 0.0082
0.2691 ± 0.0082
0.2605 ± 0.0088
0.2562 ± 0.0083

AUC
0.7900 ± 0.0111
0.8256 ± 0.0096
0.8706 ± 0.0080
0.8624 ± 0.0079
0.8717 ± 0.0080
0.8705 ± 0.0081

0.15s
0.25s
10.3s
6.7s
10.4s
10.8s

0.11s
0.11s
0.57s
0.48s
0.62s
0.63s

Results: Logistic regression and MLP underperformed compared to the four temporal learning
algorithms (Table 2). RETAIN is comparable to the other RNN variants in terms of prediction
performance while offering the interpretation beneﬁt.

Note that RNN+αR model are a degenerated version of RETAIN with only scalar attention, which is
still a competitive model as shown in table 2. This conﬁrms the efﬁciency of generating attention
weights using the RNN. However, RNN+αR model only provides scalar visit-level attention, which
is not sufﬁcient for healthcare applications. Patients often receives several medical codes at a single
visit, and it will be important to distinguish their relative importance to the target. We show such a
case study in section 4.3.

Table 2 also shows the scalability of RETAIN, as its training time (the number of seconds to train
the model over the entire training set once) is comparable to RNN. The test time is the number
of seconds to generate the prediction output for the entire test set. We use the mini-batch of 100
patients when assessing both training and test times. RNN takes longer than RNN+αM because of its
two-layer structure, whereas RNN+αM uses a single layer RNN. The models that use two RNNs
(RNN, RNN+αR, RETAIN)2 take similar time to train for one epoch. However, each model required
a different number of epochs to converge. RNN typically takes approximately 10 epochs, RNN+αM
and RNN+αR 15 epochs and RETAIN 30 epochs. Lastly, training the attention models (RNN+αM ,
RNN+αR and RETAIN) for ESM would take considerably longer than L2D, because ESM modeling
generates context vectors at each time step. RNN, on the other hand, does not require additional
computation other than embedding the visit to its hidden layer to predict target labels at each time
step. Therefore, in ESM, the training time of the attention models will increase linearly in relation to
the length of the input sequence.

4.3 Model Interpretation for Heart Failure Prediction
We evaluated the interpretability of RETAIN in the HF prediction task by choosing a HF patient from
the test set and calculating the contribution of the variables (medical codes in this case) to diagnostic
prediction. The patient suffered from skin problems, skin disorder (SD), benign neoplasm (BN),
excision of skin lesion (ESL), for some time before showing symptoms of HF, cardiac dysrhythmia
(CD), heart valve disease (HVD) and coronary atherosclerosis (CA), and then a diagnosis of HF
(Figure 3). We can see that skin-related codes from the earlier visits made little contribution to HF
prediction as expected. RETAIN properly puts much attention to the HF-related codes that occurred in
recent visits.

To conﬁrm RETAIN’s ability to exploit the sequence information of the EHR data, we reverse the visit
sequence of Figure 3a and feed it to RETAIN. Figure 3b shows the contribution of the medical codes
of the reversed visit record. HF-related codes in the past are still making positive contributions, but
not as much as they did in Figure 3a. Figure 3b also emphasizes RETAIN’s superiority to interpretable,
but stationary models such as logistic regression. Stationary models often aggregate past information
and remove the temporality from the input data, which can mistakenly lead to the same risk prediction
for Figure 3a and 3b. RETAIN, however, can correctly digest the sequence information and calculates
the HF risk score of 9.0%, which is signiﬁcantly lower than that of Figure 3a.

Figure 3c shows how the contributions of codes change when selected medication data are used in
the model. We added two medications from day 219: antiarrhythmics (AA) and anticoagulants (AC),
both of which are used to treat cardiac dysrhythmia (CD). The two medications make a negative
contributions, especially towards the end of the record. The medications decreased the positive
contributions of heart valve disease and cardiac dysrhythmia in the last visit. Indeed, the HF risk

2The RNN baseline uses two layers of RNN, RNN+αR uses one for visit embedding and one for generating

α, RETAIN uses each for generating α and β

7

Figure 3: (a) Temporal visualization of a patient’s visit records where the contribution of variables for
diagnosis of heart failure (HF) is summarized along the x-axis (i.e. time) with the y-axis indicating
the magnitude of visit and code speciﬁc contributions to HF diagnosis. (b) We reverse the order of
the visit sequence to see if RETAIN can properly take into account the modiﬁed sequence information.
(c) Medication codes are added to the visit record to see how it changes the behavior of RETAIN.

prediction (0.2165) of Figure 3c is lower than that of Figure 3a (0.2474). This suggests that taking
proper medications can help the patient in reducing their HF risk.

5 Conclusion

Our approach to modeling event sequences as predictors of HF diagnosis suggest that complex
models can offer both superior predictive accuracy and more precise interpretability. Given the power
of RNNs for analyzing sequential data, we proposed RETAIN, which preserves RNN’s predictive
power while allowing a higher degree of interpretation. The key idea of RETAIN is to improve
the prediction accuracy through a sophisticated attention generation process, while keeping the
representation learning part simple for interpretation, making the entire algorithm accurate and
interpretable. RETAIN trains two RNN in a reverse time order to efﬁciently generate the appropriate
attention variables. For future work, we plan to develop an interactive visualization system for
RETAIN and evaluating RETAIN in other healthcare applications.

References

In ICLR, 2015.

[1] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. In ICLR, 2015.
[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.

[3] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.

Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.

[4] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of SciPy, 2010.
[5] A. D. Black, J. Car, C. Pagliari, C. Anandan, K. Cresswell, T. Bokun, B. McKinstry, R. Procter, A. Majeed,
and A. Sheikh. The impact of ehealth on the quality and safety of health care: a systematic overview. PLoS
Med, 8(1):e1000387, 2011.

[6] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad. Intelligible models for healthcare:

Predicting pneumonia risk and hospital 30-day readmission. In KDD, 2015.

[7] B. Chaudhry, J. Wang, S. Wu, M. Maglione, W. Mojica, E. Roth, S. C. Morton, and P. G. Shekelle.
Systematic review: impact of health information technology on quality, efﬁciency, and costs of medical
care. Annals of internal medicine, 144(10):742–752, 2006.

8

[8] Z. Che, S. Purushotham, R. Khemani, and Y. Liu. Distilling knowledge from deep networks with

applications to healthcare domain. arXiv preprint arXiv:1512.03542, 2015.

[9] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning
phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, 2014.

[10] E. Choi, M. T. Bahadori, E. Searles, C. Coffey, and J. Sun. Multi-layer representation learning for medical

[11] E. Choi, M. T. Bahadori, and J. Sun. Doctor ai: Predicting clinical events via recurrent neural networks.

concepts. In KDD, 2016.

arXiv preprint arXiv:1511.05942, 2015.

recognition. In NIPS, pages 577–585, 2015.

University of Montreal, 2009.

[12] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. Attention-based models for speech

[13] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.

[14] C. Esteban, O. Staeck, Y. Yang, and V. Tresp. Predicting clinical events by combining static and dynamic

information using recurrent neural networks. arXiv preprint arXiv:1602.02685, 2016.

[15] A. S. Fleisher, B. B. Sowell, C. Taylor, A. C. Gamst, R. C. Petersen, L. J. Thal, and f. t. A. D. C. Study.
Clinical predictors of progression to Alzheimer disease in amnestic mild cognitive impairment. Neurology,
68(19):1588–1595, 2007.

[16] B. Gallego, S. R. Walter, R. O. Day, A. G. Dunn, V. Sivaraman, N. Shah, C. A. Longhurst, and E. Coiera.
Bringing cohort studies to the bedside: framework for a ’green button’ to support clinical decision-making.
Journal of Comparative Effectiveness Research, pages 1–7, 2015.

[17] J. Ghosh and V. Karamcheti. Sequence learning with recurrent networks: analysis of internal representations.

In Aerospace Sensing, pages 449–460, 1992.

[18] C. L. Goldzweig, A. Towﬁgh, M. Maglione, and P. G. Shekelle. Costs and beneﬁts of health information

technology: new trends from the literature. Health affairs, 28(2):w282–w293, 2009.

[19] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image

generation. arXiv preprint arXiv:1502.04623, 2015.

[20] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching

machines to read and comprehend. In NIPS, pages 1684–1692, 2015.

[21] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

[22] A. K. Jha, C. M. DesRoches, E. G. Campbell, K. Donelan, S. R. Rao, T. G. Ferris, A. Shields, S. Rosenbaum,

and D. Blumenthal. Use of electronic health records in us hospitals. N Engl J Med, 2009.

[23] A. Karpathy, J. Johnson, and F.-F. Li. Visualizing and understanding recurrent networks. arXiv preprint

arXiv:1506.02078, 2015.

[24] A. N. Kho et al. Use of diverse electronic medical record systems to identify genetic risk for type 2 diabetes

within a genome-wide association study. JAMIA, 19(2):212–218, 2012.

[25] Q. V. Le. Building high-level features using large scale unsupervised learning. In ICASSP, 2013.

[26] Q. V. Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent networks of rectiﬁed linear units.

arXiv preprint arXiv:1504.00941, 2015.

[27] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell. Learning to Diagnose with LSTM Recurrent Neural

Networks. In ICLR, 2016.

classiﬁcation. In ICML, 2016.

[28] A. F. Martins and R. F. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label

[29] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of visual attention. In NIPS, 2014.

[30] A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence summarization.

In EMNLP, 2015.

[31] S. Saria, D. Koller, and A. Penn. Learning individual and population level traits from clinical temporal

data. In NIPS, Predictive Models in Personalized Medicine workshop, 2010.

[32] P. Schulam and S. Saria. A probabilistic graphical model for individualizing prognosis in chronic, complex

diseases. In AMIA, volume 2015, page 143, 2015.

[33] J. Sun, F. Wang, J. Hu, and S. Edabollahi. Supervised patient similarity measure of heterogeneous patient

records. ACM SIGKDD Explorations Newsletter, 14(1):16–24, 2012.

[34] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell:

Neural image caption generation with visual attention. In ICML, 2015.

[35] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.

9

A A method to use the timestamps

i

As before, we use t(n)
to represent the timestamp of the i-th visit of the n-th patient. In the following,
we suppress the superscript (n) to avoid cluttered notation. Note that the timestamp ti can be anything
that provides the temporal information of the i-th visit: the number of days from the ﬁrst visit, the
number of days between two consecutive visits, or the number of days until the index date of some
event such as heart failure diagnosis.

In order to use the timestamps, we modify Step 2 and Step 3 in Section 2.2 as follows:

gi, gi−1, . . . , g1 = RNNα(v

(cid:48)

(cid:48)

i, v
α gj + bα,

i−1, . . . , v
for

(cid:48)

1),

j = 1, . . . , i

ej = w(cid:62)

α1, α2, . . . , αi = Softmax(e1, e2, . . . , ei)

hi, hi−1, . . . , h1 = RNNβ(v

(cid:48)

(cid:48)

i, v

i−1, . . . , v

βj = tanh (cid:0)Wβhj + bβ
i = [vi, ti]

(cid:48)

(cid:1)

where v

(cid:48)

1)
for

j = 1, . . . , i,

(cid:48)

where we use v
i, the concatenation of the visit embedding vi and the timestamp ti, to generate the
(cid:48)
attentions α and β. However, when obtaining the context vector ci as per Step 4, we use vi, not v
i
to match the dimensionality. The entire process could be understood such that we use the temporal
information not to embed each visit, but to calculate the attentions for the entire visit sequence. This
is consistent with our modeling approach where we lose the sequential information in embedding the
visit with MLP, then recover the sequential information by generating the attentions using the RNN.
By using the temporal information, speciﬁcally the log of the number of days from the ﬁrst visit, we
were able to improve the heart failure prediction AUC by 0.003 without any hyper-parameter tuning.

B Details of the experiment settings

B.1 Hyper-parameter Tuning

We used the validation set to tune the hyper-parameters: visit embedding size m, RNNα’s hidden
layer size p, RNNβ’s hidden layer size q, L2 regularization coefﬁcient, and drop-out rates.

L2 regularization was applied to all weights except the ones in RNNα and RNNβ. Two separate
drop-outs were used on the visit embedding vi and the context vector ci. We performed the random
search with predeﬁned ranges m, p, q ∈ {32, 64, 128, 200, 256}, L2 ∈ {0.1, 0.01, 0.001, 0.0001},
dropoutvi, dropoutci ∈ {0.0, 0.2, 0.4, 0.6, 0.8}. We also performed the random search with m, p
and q ﬁxed to 256.

The ﬁnal value we used to train RETAIN for heart failure prediction is m, p, q = 128, dropoutvi = 0.6,
dropoutci = 0.6 and 0.0001 for the L2 regularization coefﬁcient.

B.2 Code Grouper

Diagnosis codes, medication codes and procedure codes in the dataset are respectively using Interna-
tional Classiﬁcation of Diseases (ICD-9), Generic Product Identiﬁer (GPI) and Current Procedural
Terminology (CPT).
Diagnosis codes are grouped by Clinical Classiﬁcations Software for ICD-93 which reduces the
number of diagnosis code from approximately 14,000 to 283. Medication codes are grouped by
Generic Product Identiﬁer Drug Group4 which reduces the dimension to from approximately 151,000
to 96. Procedure codes are grouped by Clinical Classiﬁcations Software for CPT5, which reduces the
number of CPT codes from approximately 9,000 to 238.

3https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp
4http://www.wolterskluwercdi.com/drug-data/medi-span-electronic-drug-ﬁle/
5https://www.hcup-us.ahrq.gov/toolssoftware/ccs_svcsproc/ccssvcproc.jsp

10

Table 3: Qualifying ICD-9 codes for heart failure

B.3 Training Speciﬁcs of the Basline Models

• LR: We use 0.01 L2 regularization coefﬁcient for the logistic regression weight.
• MLP: We use drop-out rate 0.6 on the output of the hidden layer. We use 0.0001 L2 regularization

coefﬁcient for the hidden layer weight and the logistic regression weight.

• RNN: We use drop-out rate 0.6 on the outputs of both hidden layers. We use 0.0001 L2
regularization coefﬁcient for the logistic regression weight. The dimension size of both hidden
layers is 256.

• RNN+αM : We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of
the context vector (cid:80)
i αivi. We use 0.0001 L2 regularization coefﬁcient for the hidden layer
weight of the MLP that generates α’s and the logistic regression weight. The dimension size of
the hidden layers in both RNN and MLP is 256.

• RNN+αR: We use drop-out rate 0.4 on the output of the hidden layer and 0.6 on the output of
the context vector (cid:80)
i αivi. We use 0.0001 L2 regularization coefﬁcient for the hidden layer
weight of the RNN that generates α’s and the logistic regression weight. The dimension size of
the hidden layers in both RNNs is 256.

B.4 Heart Failure Case/Control Selection Criteria

Case patients were 40 to 85 years of age at the time of HF diagnosis. HF diagnosis (HFDx) is
deﬁned as: 1) Qualifying ICD-9 codes for HF appeared in the encounter records or medication orders.
Qualifying ICD-9 codes are displayed in Table 3. 2) a minimum of three clinical encounters with
qualifying ICD-9 codes had to occur within 12 months of each other, where the date of diagnosis was
assigned to the earliest of the three dates. If the time span between the ﬁrst and second appearances of
the HF diagnostic code was greater than 12 months, the date of the second encounter was used as the

11

ﬁrst qualifying encounter. The date at which HF diagnosis was given to the case is denoted as HFDx.
Up to ten eligible controls (in terms of sex, age, location) were selected for each case, yielding an
overall ratio of 9 controls per case. Each control was also assigned an index date, which is the HFDx
of the matched case. Controls are selected such that they did not meet the operational criteria for
HF diagnosis prior to the HFDx plus 182 days of their corresponding case. Control subjects were
required to have their ﬁrst ofﬁce encounter within one year of the matching HF case patient’s ﬁrst
ofﬁce visit, and have at least one ofﬁce encounter 30 days before or any time after the case’s HF
diagnosis date to ensure similar duration of observations among cases and controls.

C Results on encounter sequence modeling

Objective: Given a sequence of visits x1, . . . , xT , the goal of encounter sequence modeling is, for
each time step i, to predict the codes occurring at the next visit x2, . . . , xT +1. In this experiment,
we focus on predicting the diagnosis codes in the encounter sequence, so we create a separate set of
labels y1, . . . , yT that do not contain non-diagnosis codes such as medication codes or procedure
codes. Therefore yi will contain diagnosis codes from the next visit xi+1.

Dataset: We divide the entire dataset described in Table 1 into 0.75:0.10:0.15 ratio, respectively for
training set, validation set, and test set.

Baseline: We use the same baseline models we used for HF prediction. However, since we are
predicting 283 binary labels now, we replace the logistic regression function with the Softmax
function. The drop-out and L2 regularization policies remain the same.
For LR and MLP, at each step i, we aggregate maximum ten past input vectors6 xi−9, . . . , xi to create
a pseudo-context vector (cid:98)ci. LR applies the Softmax function on top of (cid:98)ci. MLP places a hidden layer
on top of (cid:98)ci then applies the Softmax function.
Evaluation metric: We use the negative log likelihood Eq (1) on the test set to evaluate the model
performance. We also use Recall@k as an additional metric to measure the prediction accuracy.

• Recall@k: Given a sequence of visits x1, . . . , xT , we evaluate the model performance based
on how accurately it can predict the diagnosis codes y1, . . . , yT . We use the average Recall@k,
which is expressed as below,

1
N

N
(cid:88)

n=1

1
T (n)

T (n)
(cid:88)

i=1

Recall@k((cid:98)yi), where Recall@k((cid:98)yi) =

|argsort((cid:98)yi)[: k] ∩ nonzero(yi)|
|nonzero(yi)|

where argsort returns a list of indices that will decrementally sort a given vector and nonzero
returns a list of indices of the coordinates with non-zero values. We use Recall@k because of its
similar nature to the way a human physician performs the differential diagnostic procedure, which
is to generate a list of most likely diseases for an undiagnosed patient, then perform medical
practice until the true disease, or diseases are determined.

Prediction accuracy: Table 4 displays the prediction performance of RETAIN and the baselines.
We use k = 5, 10 for Recall@k to allow a reasonable number of prediction trials, as well as cover
complex patients who often receive multiple diagnosis codes at a single visit.

RNN shows the best prediction accuracy for encounter diagnosis prediction. However, considering
the purpose of encounter diagnosis prediction, which is to assist doctors to provide quality care for
the patient, black-box behavior of RNN makes it unattractive as a clinical tool. On the other hand,
RETAIN performs as well as other attention models, only slightly inferior to RNN, provides full
interpretation of its prediction behavior, making it a feasible solution for clinical applications.

The interesting ﬁnding in Table 4 is that MLP is able to perform as accurately as RNN+αM in terms
of Recall@10. Considering the fact that MLP uses aggregated information of past ten visits, we can
assume that encounter diagnosis prediction depends more on the frequency of disease occurrences
rather than the order in which they occurred. This is quite different from the HF prediction task,
where stationary models (LF, MLP) performed signiﬁcantly worse than sequential models.

6We also tried aggregating all past input vectors x1, . . . , xi, but the performance was slightly worse than

using just ten.

12

Table 4: Encounter diagnosis prediction performance of RETAIN and the baselines

Model

LR
MLP
RNN
RNN+αM
RNN+αR
RETAIN

Negative
Likelihood
0.0288
0.0267
0.0258
0.0262
0.0259
0.0259

Recall@5 Recall@10

43.15
50.72
55.18
52.15
53.89
54.25

55.84
65.02
69.09
65.81
67.45
67.74

(a)

(b)

(c)

(d)

(e)

Figure 4: Graphical illustration of the baselines: (a) Logistic regression (LR), (b) Multilayer Percep-
tron (MLP), (c) Recurrent neural network (RNN), (d) RNN with attention vectors generated via an
MLP (RNN+αM ), (e) RNN with attention vectors generated via an RNN (RNN+αR). RETAIN is
given in Figure 1b.

D Illustration and comparison of the baselines

Figure 4 illustrates the baselines used in the experiments and shows the relationship among them.

13

