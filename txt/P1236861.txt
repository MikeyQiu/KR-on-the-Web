Randomized Block Frank-Wolfe for Convergent
Large-Scale Learning

Liang Zhang, Student Member, IEEE, Gang Wang, Student Member, IEEE,
Daniel Romero, Member, IEEE, and Georgios B. Giannakis, Fellow, IEEE

1

7
1
0
2
 
p
e
S
2
2
 
 
]

 

C
O
.
h
t
a
m

[
 
 
2
v
1
6
4
8
0
.
2
1
6
1
:
v
i
X
r
a

Abstract—Owing to their low-complexity iterations, Frank-
Wolfe (FW) solvers are well suited for various large-scale learning
tasks. When block-separable constraints are present, randomized
block FW (RB-FW) has been shown to further reduce complexity
by updating only a fraction of coordinate blocks per iteration. To
circumvent the limitations of existing methods, the present work
develops step sizes for RB-FW that enable a ﬂexible selection of
the number of blocks to update per iteration while ensuring con-
vergence and feasibility of the iterates. To this end, convergence
rates of RB-FW are established through computational bounds
on a primal sub-optimality measure and on the duality gap. The
novel bounds extend the existing convergence analysis, which only
applies to a step-size sequence that does not generally lead to fea-
sible iterates. Furthermore, two classes of step-size sequences that
guarantee feasibility of the iterates are also proposed to enhance
ﬂexibility in choosing decay rates. The novel convergence results
are markedly broadened to encompass also nonconvex objectives,
and further assert that RB-FW with exact line-search reaches a
stationary point at rate O(1/
t). Performance of RB-FW with
different step sizes and number of blocks is demonstrated in two
applications, namely charging of electrical vehicles and structural
support vector machines. Extensive simulated tests demonstrate
the performance improvement of RB-FW relative to existing
randomized single-block FW methods.

√

Index Terms—Conditional gradient descent, nonconvex opti-

mization, block coordinate, parallel optimization.

I. INTRODUCTION

The Frank-Wolfe (FW) algorithm [1], also known as condi-
tional gradient descent [2], has well-documented merits as a
ﬁrst-order solver especially for smooth constrained optimiza-
tion tasks over convex compact sets. FW has recently received
revived interest due to its simplicity and versatility in handling
structured constraint sets in various signal processing and
machine learning applications [3]. This growing popularity is
due to its per-iteration simplicity that only entails minimizing
a linear function over the feasible set, whereas competing ﬁrst-
order alternatives, such as projected gradient descent [4] and
their accelerated versions [5], involve minimizing a quadratic
function over the feasible set per iteration. Typically, solv-
ing a constrained linear optimization is considerably easier
than ﬁnding the aforementioned projections per iteration. The

L. Zhang, G. Wang, and G. B. Giannakis are with the Digital Technology
Center and the Department of Electrical and Computer Engineering at
the University of Minnesota, Minneapolis, MN 55455, USA. D. Romero
is with the Department of Information and Communication Technology,
University of Agder, Grimstad 4879, Norway. G. Wang is also with the State
Key Laboratory of Intelligent Control and Decision of Complex Systems,
Beijing Institute of Technology, Beijing 100081, P. R. China. E-mails:
{zhan3523, gangwang, georgios}@umn.edu, daniel.romero@uia.no.

resulting savings beneﬁt diverse large-scale learning tasks,
including matrix completion [6], multi-class classiﬁcation [7],
image reconstruction [7], structural support vector machines
(SVMs) [8], particle ﬁltering [9], sparse phase retrieval [10],
[11], and scheduling electric vehicle (EV) charging [12].

Despite its simplicity, FW can become prohibitively ex-
pensive when dealing with high-dimensional data. For this
reason, randomized single-block FW has been advocated for
solving large-scale convex constrained programs [8], where
only a randomly selected block of variables is updated per
iteration. At the price of obtaining the duality gap, convergence
of randomized single-block FW has been improved in [13].
Furthermore, randomized multiple-block FW was devised to
reduce convergence time by updating multiple blocks per itera-
tion in parallel [14]. Unfortunately, feasibility of the resulting
iterates is in general not guaranteed by the original parallel
randomized block (RB)-FW [14]. Moreover, all results on
randomized FW focus on convex objectives, and convergence
of RB-FW for nonconvex programs remained hitherto an open
problem.

The present paper is the ﬁrst to introduce a broad class of
step sizes for RB-FW that offer: (i) guaranteed convergence
and feasibility of the iterates along with (ii) ﬂexibility to select
a step-size sequence whose decay rate is attuned to the prob-
lem at hand. RB-FW with this rich class of step sizes subsumes
the classical FW as well as the randomized single-block FW
solvers as special cases. We further broaden the scope of RB-
FW by allowing for nonconvex smooth objective functions.
Speciﬁcally, we establish that RB-FW with typical step sizes
attains a stationary point at rate O(1/ log t), whereas line-
t).
search-based step sizes enjoys an improved rate of O(1/
Remarkably, the latter coincides with the rate afforded by
classical FW for nonconvex problems [15]. Finally, simulated
tests on optimal coordination of EV charging and structural
SVMs corroborate the merits of RB-FW with our novel step
sizes relative to single-block FW.

√

The remainder of this paper is organized as follows. Sec-
tion II outlines the FW and RB-FW algorithms. Section III
describes two novel families of step sizes for RB-FW, and
establishes their feasibility and convergence. Section IV de-
rives the RB-FW convergence rates for non-convex programs,
whereas Section V highlights the implications of the results in
Section III for classical FW. Section VI shows the merits of
RB-FW in two application settings, whereas Section VII tests
the RB-FW performance numerically. Finally, Section VIII
concludes the paper.

Regarding common notation, lower- (upper-) case boldface

2

Algorithm 1 Frank-Wolfe [1]
1: Initialize t = 0, x0 ∈ X
2: while stopping criterion not met do
3:
4:
5:
6: end while

Compute st = arg mins∈X s(cid:62)∇f (xt)
Update xt+1 = (1 − γt)xt + γtst
t ← t + 1

letters represent column vectors (matrices). Sets are denoted
by calligraphic letters, |B| stands for the cardinality of set
B, and N \ B := {x ∈ N : x /∈ B} denotes set difference.
Symbol (cid:62) is reserved for transposition of vectors and matrices,
whereas 0 and 1 denote the all-zero and all-one vectors
of suitable dimensions, respectively. Operator (cid:100)x(cid:101) gives the
smallest integer greater than or equal to x, and log(x) returns
the natural logarithm of x.

II. PRELIMINARIES

The classical FW algorithm [1] aims at solving the follow-

ing generic constrained optimization problem

minimize
x∈Rd
subject to

f (x)

x ∈ X

(1)

(2)

(3)

(4)

(5)

(6)

where f (x) is convex and differentiable, while the feasible
set X is convex and compact. A number of problems in
signal processing and machine learning, e.g., ridge regression
or basis pursuit [16], can be expressed in this form. Listed as
Algorithm 1, FW is initialized with a feasible x0. Given iterate
xt, it then solves the following so-termed “linear oracle”

and uses a convex combination of st with xt to obtain

where the step size γt ∈ (0, 1] is typically selected as [3]

st := arg min
s∈X

s(cid:62)∇f (xt)

xt+1 = (1 − γt)xt + γtst

γt =

2
t + 2

.

Alternatively, γt can be chosen via line search, which picks
xt+1 as the best point on the line segment between xt and st:

γt = arg min
0≤γ≤1

f (cid:0)(1 − γ)xt + γst(cid:1) .

In either case, Algorithm 1 converges at rate O(1/t) [3].

When d is large, updating all d entries of x at each t is
computationally challenging. Randomized FW alleviates this
difﬁculty by updating only a subset of the d entries [8], [14].
Splitting x into Nb blocks {xn}Nb
n=1 with respective feasible
sets {Xn}Nb

n=1 assumed convex and compact, (1) becomes

minimize
x∈Rd

f (x)

subject to x1 ∈ X1, . . . , xNb ∈ XNb

where x(cid:62) := [x(cid:62)
1 , x(cid:62)
(6) boils down to (1).

2 , · · · , x(cid:62)
Nb

]. Note that if Nb = 1, then

The decomposition X = X1 × . . . × XNb entails no loss of
generality since any X can be expressed in this form by setting

Algorithm 2 Randomized Block Frank-Wolfe

1: Initialize t = 0, x0 ∈ X
2: while stopping criterion not met do
3:
4:

Randomly pick Bt ⊆ Nb such that |Bt| = B
Compute st

n ∇xn f (xt), ∀n ∈ Bt
s(cid:62)

n = arg min
sn∈Xn

5:

Update

xt+1

n =

(cid:26) (1 − γt)xt
xt
n,

n + γtst
n,

∀n ∈ Bt
∀n ∈ Nb \ Bt

t ← t + 1

6:
7: end while

Fig. 1. Parallel implementation for Algorithm 2 at iteration t ≥ 0. Left: The
control center sends gradient ∇xn f (xt) to processor n ∈ Bt. Right: The
updated {st+1

n }n∈Bt are sent to the control center.

Nb = 1. It also emerges naturally in a number of applications,
including the dual problem of structural SVMs [8], trace-norm
regularized tensor completion [17], EV charging [12], the
dual problem of group fused Lasso [18], and structured sub-
modular minimization [19]. Thanks to the separable structure
of X , the linear oracle in (2) decouples across Nb blocks as

st
n = arg min
sn∈Xn

(cid:104)sn, ∇xn f (xt)(cid:105), n = 1, 2, . . . , Nb

(7)

where ∇xn f (xt) comprises the partial derivatives of f (x) with
respect to the entries of xn.

Instead of solving the Nb problems in (7), RB-FW reduces
complexity by solving just B of them, where B ∈ {1, . . . , Nb}
is a pre-selected constant. Let Nb := {1, . . . , Nb} be the
index set of all blocks, and let Bt be chosen at iteration t
uniformly at random among all subsets of Nb with B elements.
The RB-FW solver of (6) is summarized as Algorithm 2. To
save computation time, step 4 of Algorithm 2 can be run in
parallel [14] as illustrated in Fig. 1. In this case, B can be
selected according to the number of physical processor cores
in the control center.

The only step-size sequence for RB-FW available in the

literature is [14]

γt =

2α
α2t + 2/Nb

,

t = 0, 1, . . .

(8)

where α := B/Nb is the fraction of updated blocks. For α = 1
and Nb (cid:54)= 1, note that (8) is different from (4); hence, FW
is not generally a special case of the parallel RB-FW in [14].
Interestingly, Sec. III will introduce a family of step sizes for
RB-FW that subsumes the one in (4) as a special case.

3

Regarding convergence of FW solvers, two quantities play
instrumental roles. The ﬁrst one is the curvature constant,
which for a differentiable f (x) over X is deﬁned as [20], [3]

Cf :=

sup
γ∈[0,1]
x,s∈X
y:=(1−γ)x+γs

2
γ2 [f (y) − f (x) − (cid:104)y − x, ∇f (x)(cid:105)] .

(9)
Cf is the least upper bound of a scaled difference between
f (y) and its linear approximation around x. Throughout, Cf
is assumed bounded. This property is closely related to the
L-Lipschitz continuity of ∇f (x) over X , which is deﬁned as

∃L > 0 : (cid:107)∇f (x)−∇f (s)(cid:107) ≤ L(cid:107)x−s(cid:107), ∀x, s ∈ X . (10)

If (10) holds, it is easy to check that [3, Appendix D]

Cf ≤ LD2
X

(11)

where DX := supx,s∈X (cid:107)x − s(cid:107) is the diameter of X , which
is ﬁnite for X compact. Equation (11) evidences that Cf is
bounded whenever ∇f (x) is L-Lipschitz continuous over X .
When it comes to RB-FW, the set curvature for an index
set B ⊆ Nb is commonly used instead of the constant Cf [14]

C B

f :=

sup
γ∈[0,1]
x∈X
{sn ∈Xn}n∈B

(cid:16)

2
γ2

where

f (y)−f (x)−

(cid:104)yn−xn, ∇xn f (x)(cid:105)

(cid:88)

n∈B

(cid:17)

(12)

(cid:26) (1 − γ)xn + γsn, n ∈ B

yn :=

xn,

n ∈ Nb \ B

1 , . . . , y(cid:62)
Nb

and y(cid:62) := [y(cid:62)
]. The expected set curvature for a
B selected uniformly at random with |B| = B can thus be
expressed as

¯C B

f := EB

(cid:2)C B

f

(cid:3) =

(cid:19)−1

(cid:18)Nb
B

(cid:88)

C B
f

(13)

{B: B⊆Nb,|B|=B}

B

f ≤ Cf by observing from (9) and (12) that C B

where (cid:0)Nb
¯C B
∀B ⊆ Nb. Note however that ¯C B

(cid:1) := Nb!/(cid:0)B!(Nb − B)!(cid:1). It is easy to verify that
f ≤ Cf ,
f = Cf , when B = Nb.
The second quantity of interest is the so-termed duality gap

f = C B

g(x) := sup
s∈X

(x − s)(cid:62)∇f (x), x ∈ X

(14)

whose name stems from Fenchel duality; see [8, Appendix
D], [3, Section 2]. Clearly, for the constrained problem (1),
x is a stationary point if and only if g(x) = 0. In addition,
it holds that g(x) ≥ 0, ∀x ∈ X , since (x − s)(cid:62)∇f (x) = 0
for s = x. Thus, g(x) quantiﬁes the distance of x from a
stationary point of f (x) [15].

III. FEASIBILITY-ENSURING STEP SIZES FOR RB-FW

To motivate the need for novel step sizes, this section starts
by showing that γt in (8) does not guarantee feasibility of the
iterates {xt}. It then introduces two families of feasibility-
ensuring step size sequences, and proves that the iterates they
generate are convergent for convex objectives. Moreover, these
families are shown to offer a gamut of decay rates, thereby

allowing for a ﬂexible selection of the most suitable step size
for a given problem.

With e.g., Nb = 103 and B = 2, the step size in (8) will
be γt > 1, ∀t < 500. As a result, step 5 of Algorithm 2 can
generate infeasible iterates xt, which render RB-FW unstable
since the gradient of the objective at the resulting xt may not
even be deﬁned. For example, consider applying Algorithm 2
with Nb = 100, B = 10 and step size as in (8) to solve the
smooth and convex program

minimize
{xn}100
n=1

subject to

100
(cid:88)

(xn)2 − log xn

n=1
2 ≤ xn ≤ 3, n = 1, . . . , 100.

(15)

Initializing with {x1
n = 2}n∈B1 and {x2

n = 3}n∈Nb , it is easy to verify that
{s1
n = −11/3}n∈B1, implying that f (x2)
and ∇f (x2) do not exist. Thus, the parallel RB-FW algorithm
in [14], whose step size is given by (8), cannot solve (15).

In a nutshell, existing step sizes do not guarantee feasibility
of RB-FW iterates. Besides, the decay rates of existing step
sizes can not be ﬂexibly adjusted to optimize convergence in
a given problem; see Remark 2. To ﬁll this gap, convergence
analysis of RB-FW will be pursued ﬁrst for a rich class of
step sizes.

A. Convergence of RB-FW for convex programs

For randomized FW, convergence analysis typically focuses
on f (xt) and g(xt) in (14) [8], [14]. Let x∗ denote one
globally optimal solution of (6), and deﬁne the primal sub-
optimality of xt as h(xt) := f (xt) − f (x∗). The next lemma,
which quantiﬁes the improvement of h(xt) per iteration, will
prove handy in the ensuing convergence analysis.

Lemma 1. If {xt}t=0,1,... is generated by Algorithm 2 with an
arbitrary predeﬁned step-size sequence {γt}t=0,1,... satisfying
γt ∈ [0, 1] ∀t, then it holds that

E (cid:2)h(xt+1)(cid:3) ≤ E (cid:2)h(xt)(cid:3) − αγtE (cid:2)g(xt)(cid:3) + γ2
for t ≥ 0, where the expectation is taken over {Bτ }t

¯C B

t

τ =0.

f /2 (16)

A detailed proof can be found in [8], [14]; see also part A
of the Appendix for an outline. Note that Lemma 1 can be
applied regardless of whether f (x) is convex or not.

Aiming to upper bound E [h(xt)], we will consider that

{γt}t=0,1,... satisfy

0 < γt ≤ 1, ∀t ≥ 0
1
γ2
t

, ∀t ≥ 0.

1 − αγt+1
γ2
t+1

≤

(17a)

(17b)

It can be easily seen that (17b) is equivalent to
(cid:19)

(cid:18)(cid:113)

α2γ2

t + 4 − αγt

γt+1 ≥

γt
2

which implies that (17b) limits how rapidly {γt}t=0,1,... can
decrease. Condition (17) is very general and subsumes existing
step sizes as special cases. For example, if B = Nb, Algo-
rithm 2 boils down to Algorithm 1, for which (4) is typically
adopted [3]. For γt as in (4), it is clear that (17a) is satisﬁed,

4

whereas (17b) follows from (t + 1)(t + 3) ≤ (t + 2)2. Another
example arises if B = 1, in which case Algorithm 2 reduces
to Algorithm 4 in [8]. The sequence
2Nb
t + 2Nb

γt =

(18)

which was proposed in [8] for Algorithm 2, clearly satisﬁes
(17a), and also (17b) since it holds that (t+1+2Nb)2−2(t+1+
2Nb) ≤ (t + 2Nb). The step size (8) also satisﬁes (17b) since
(α2t+α2 +2/Nb)2 −2α2(α2t+α2 +2/Nb) ≤ (α2t+2/Nb)2,
but fails to satisfy (17a), which ensures feasible iterates.
However, upon observing that γt in (8) satisﬁes γt ≤ 1 for
t ≥ ˜t := (2BNb − 2Nb)/B2, one deduces that the shifted
sequence

˜γt := γt+˜t =

2
αt + 2

does satisfy (17a), and therefore constitutes a feasible al-
ternative to (8). Furthermore, it also satisﬁes (17b) because
(αt + 2 + α)(αt + 2 − α) ≤ (αt + 2)2.

To proceed with convergence rate analysis for a broad class
of step sizes, an upper bound on E [h(xt)] for step sizes
satisfying (17) will be developed.

Theorem 1 (Primal convergence). If f (x) is convex and
{γt}t=0,1,... satisﬁes (17), the iterates of Algorithm 2 satisfy

E (cid:2)h(xt)(cid:3) ≤

1 − αγ0
γ2
0

t−1h(x0) +
γ2

γ2
t−1,

t ≥ 1. (20)

t ¯C B
f
2

Proof. Since f (x) is differentiable, convexity of f (x) implies
that

f (xt) − f (x∗) ≤ (xt − x∗)(cid:62)∇f (xt)

(21)

where x∗ denotes any solution to (6). Combining (14) and (21)
yields

g(xt) ≥ f (xt) − f (x∗) = h(xt) ≥ 0.

(22)

Thus, E[g(xt)] ≥ E[h(xt)] and (16) can be rewritten as

E (cid:2)h(xt+1)(cid:3) ≤ (1 − αγt)E (cid:2)h(xt)(cid:3) + γ2

¯C B

f /2.

t

(23)

Dividing both sides of (23) by γ2

t gives rise to

1
γ2
t

E (cid:2)h(xt+1)(cid:3) ≤

E (cid:2)h(xt)(cid:3) +

(24)

1 − αγt
γ2
t

¯C B
f
2

.

1
γ2
t

E[h(xt+1)] ≤

Utilizing successively (17b) and (24) yields
1
γ2
t−1
1 − αγt−1
γ2
t−1

E[h(xt)] +

E[h(xt−1)] +

¯C B
f

1
2

≤

¯C B

f +

¯C B
f

1
2

1
2
t + 1
2

1 − αγ0
γ2
0
where the last inequality uses E[h(x0)] = h(x0). Therefore,

h(x0) +

≤ . . . ≤

¯C B
f

(25)

E (cid:2)h(xt+1)(cid:3) ≤

1 − αγ0
γ2
0

t h(x0) +
γ2

t + 1
2

γ2
t

¯C B
f

sheds light on step size design for arbitrary B by providing
computational guarantees for Algorithm 2 with any step-size
sequence satisfying (17).

Another quantity of interest to characterize the convergence
of Algorithm 2 is g(xt), which can be used to assess how
close is xt from being a solution [8], [14] since g(xt) ≥
h(xt); cf. (22). However, since ﬁnding upper bounds on g(xt)
is difﬁcult [8], [15], [14], bounds on the minimal expected
duality gap until iteration t, deﬁned as [8], [14]

gt := min

k∈{0,1,...t}

E (cid:2)g(xk)(cid:3)

(26)

are pursued next.

(19)

Theorem 2 (Primal-dual convergence). Let {γt}t=0,1,... sat-
isfy (17) and γt+1 ≤ γt, ∀t ≥ 0. If f (x) is convex
and {xt}t=1,2,... is generated by Algorithm 2, then for all
K ∈ {1, . . . , t} it holds that

gt ≤

E (cid:2)h(xK)(cid:3)
α(t − K + 1)γt

+

¯C B
f γ2
K
2αγt

.

(27)

Proof. Lemma 1 asserts that
αγkE[g(xk)] ≤ E (cid:2)h(xk)(cid:3) − E (cid:2)h(xk+1)(cid:3) + γ2

¯C B

f /2.

k

(28)

From gt ≤ E[g(xk)] and (28), it follows that

αgt

γk ≤α

γkE[g(xk)]

t
(cid:88)

k=K

t
(cid:88)

k=K
t
(cid:88)

k=K

≤

(cid:0)E (cid:2)h(xk)(cid:3) − E (cid:2)h(xk+1)(cid:3)(cid:1) +

=E (cid:2)h(xK)(cid:3) − E (cid:2)h(xt+1)(cid:3) +

¯C B
f
2

t
(cid:88)

k=K

γ2
k

¯C B
f
2

t
(cid:88)

k=K

γ2
k

≤E (cid:2)h(xK)(cid:3) +

¯C B
f
2
where the last inequality follows from E[h(xt+1)] ≥ 0 and
γt+1 ≤ γt. But since γk ≥ γt, ∀k ≤ t, one arrives at

(t − K + 1)γ2
K

(29)

t
(cid:88)

k=K

γk ≥ (t − K + 1)γt.

(30)

Finally, (27) follows after combining (29) with (30), and divid-
ing both sides of the resulting inequality by α(t−K+1)γt.

Theorem 2 characterizes the primal-dual convergence of
RB-FW for any non-increasing step size satisfying (17). Plug-
ging (20) into (27) and ﬁxing the step-size sequence yields an
upper bound on gt that can be minimized with respect to K
to obtain the convergence rate of gt. This approach will be
pursued in Section III-B.

which establishes (20).

B. Proposed step sizes

Theorem 1 generalizes existing results on the convergence
of Algorithm 2, which apply only for speciﬁc step sizes
either assume B = 1 [8] or B = Nb [3]. Thus, Theorem 1

This section develops two classes of step sizes obeying (17)
for arbitrary values of B. Theorems 1 and 2 will then be
invoked to derive the resulting convergence rates. To start with,

5

and

and

consider the following general family of diminishing step-size
sequences for ﬁxed q ∈ (0, α] and decay rate ρ ∈ (0.5, 1]:

γt =

2
qtρ + 2

,

∀t ≥ 0.

(31)

As will be seen, this family includes, as special cases, the step
sizes in (4), (18), and (19).

Lemma 2. If {γt}t=0,1,... is given by (31), it satisﬁes (17b).

Proof. See part B of the Appendix.

Upon noticing that 0 < γt ≤ 1 and γt+1 ≤ γt for
{γt}t=0,1,... in (31), the convergence rate of RB-FW can be
derived by appealing to Theorems 1 and 2 as follows.

Corollary 1. For convex f (x), the iterates {xt}t=1,2,... of
Algorithm 2 with step size (31) satisfy

E (cid:2)h(xt)(cid:3) ≤

4 (1 − α) h(x0)
[q(t − 1)ρ + 2]2 +

2t ¯C B
f
[q(t − 1)ρ + 2]2

gt ≤

(2ρ + 1)2ρ+1(qtρ + 2)
αq2(2ρ)2ρ

·

(t + 1) ¯C B

f + 2(1 − α)h(x0)

t2ρ+1

Notably, the bound in (36) is tighter than the one reported
in [8, Theorem 2], while the bound on gt in (37) is of the
same order as that in [8, Theorem 2].

Finally, note that Corollary 1 also characterizes convergence
for the step size ˜γt in (19), since ˜γt is recovered from (31)
upon setting q = α and ρ = 1.

The decreasing rates of the bounds in Theorems 1 and 2
are determined by the decay rates of the step size sequence.
The faster γt diminishes, the more rapidly the upper bound in
Theorem 1 vanishes. However, the sequence in (31) decreases
at most as fast as 2/(αt + 2). To improve the bound in
Theorem 1, a more rapidly vanishing sequence is proposed
next. Speciﬁcally, consider the sequence

γ0 = 1, and γt+1 =

, ∀t ≥ 0.

(38)

(cid:112)α2γ4

t − αγ2
t

t + 4γ2
2

It is then possible to establish the following.

Lemma 3 (Recursive step size). If {γt}t=0,1,... is chosen as
in (38), it then holds that

1
αt + 1

≤ γt ≤

, ∀t ≥ 0

2
αt + 2

γt+1 ≤ γt, ∀t ≥ 0.

(39a)

(39b)

Proof. See part C of the Appendix.

Proof. See part D of the Appendix.

Corollary 1 subsumes existing convergence results as special
cases. Indeed, when B = Nb, one has that Bt = Nb ∀t,
which implies that ¯C B
f = Cf , and Algorithm 2 reduces to
the traditional FW solver. By selecting q = 1 and ρ = 1, the
classical step size in (4) is retrieved. From Corollary 1, the
resulting computational bounds are

The upper bound in (39a) conﬁrms that the step size in (38)
vanishes at least as fast as 2/(αt + 2). To check whether (38)
meets (17), note that (17a) follows from (39a), whereas (38)
implies that (17b) holds with equality. Because (38) satisﬁes
(17) and (39b), the following computational bounds for (38)
can be derived by plugging (39a) into Theorems 1 and 2.

h(xt) ≤

2tCf
(t + 1)2 ≤

2Cf
t + 2

gt ≤

27Cf
4

·

(t + 1)(t + 2)
t3

.

The resulting convergence rate of h(xt) coincides with the
one in [3, Theorem 1]. As for gt, the bound in (35) is of the
same order as that in [3, Theorem 2].

In addition, with B = 1, q = 1/Nb, and ρ = 1, the step
size (18) proposed in [8] is recovered. From Corollary 1, the
primal computational bound is

E[h(xt)] ≤

4(N 2
b − Nb)h(x0)
(t − 1 + 2Nb)2 +
b − Nb)h(x0)
4(N 2
(t − 1 + 2Nb)2 +
where the last inequality follows from

≤

2tN 2

b C 1
f

(t − 1 + 2Nb)2
b C 1
f
t + 4Nb − 2

2N 2

(36)

t
t + 2Nb − 1
Meanwhile, gt is bounded by

≤

t + 2Nb − 1
t + 4Nb − 2

.

gt ≤

27Nb(t + 2Nb)
4t3

(cid:2)(t + 1)NbC 1

f + 2(Nb − 1)h(x0)(cid:3) .

Corollary 2. For convex f (x), the iterates {xt}t=1,2,... of
Algorithm 2 with step size as in (38), satisfy

E (cid:2)h(xt)(cid:3) ≤

4(1 − α)h(x0)
(αt + 2 − α)2 +

2t ¯C B
f
(αt + 2 − α)2

(40)

and

gt ≤

27(αt + 1)
2α3t3

(cid:2)(t + 1) ¯C B

f + 2(1 − α)h(x0)(cid:3) .

(41)

Proof. See part E of the Appendix.

To recap, this section put forth two families of step sizes
for Algorithm 2 with arbitrary B, namely (31) and (38).
Corollaries 1 and 2 establish convergence of Algorithm 2 for
these step sizes, which also guarantee feasibility of the iterates
since they satisfy (17a). When {γt}t=0,1,... is given by (31)
with q = α and ρ = 1 or when it is deﬁned as in (38), the
convergence rates of Algorithm 2 are in the order of O (1/t),
thus matching those of the traditional FW algorithm, yet the
computational cost of the former is potentially much lower
than that of the latter.

Remark 1. The step size of RB-FW can also be chosen through
line search, which prescribes

(37)

γt = arg min
0≤γ≤1

f (cid:0)(1 − γ)xt + γˆst(cid:1)

(42)

(32)

.

(33)

(34)

(35)

6

with ˆst(cid:62) := [ˆst
1

(cid:62)] and

(cid:62), . . . , ˆst
Nb
(cid:26) st
n, n ∈ Bt
xt
n, n ∈ Nb \ Bt.

ˆst
n :=

Let {ˇxt}t=0,1,... be the iterates generated by Algorithm 2 with
γt given by (42). By (16) and (42), it holds that
E (cid:2)h(ˇxt+1)(cid:3) ≤ E (cid:2)h(ˇxt)(cid:3) − αγtE (cid:2)g(ˇxt)(cid:3) + γ2

¯C B

(43)

f /2.

t

for any predeﬁned step-size sequence {γt ∈ [0, 1]} [8], [14].
Particularly, (43) holds for {γt := 2/(αt + 2)}t=0,1,.... It can
then be shown that {ˇxt}t=0,1,... satisfy for t ≥ 1
2t ¯C B
f
(αt + 2 − α)2

4(1 − α)h(x0)
(αt + 2 − α)2 +

E (cid:2)h(ˇxt)(cid:3) ≤

and

ˇgt ≤

27(αt + 2)
4α3

·

(t + 1) ¯C B

f + 2(1 − α)h(x0)

t3

where ˇgt := mink∈{0,1,...t} E (cid:2)g(ˇxk)(cid:3). The proof follows the
steps of the one for Corollary 1. The convergence rate of line-
search-based Algorithm 2 therefore remains in the order of
O(1/t). Note however that extra computational cost is incurred
for ﬁnding γt via (42).

Remark 2. At this point, it is worth discussing the choice of the
step size leading to the fastest convergence in a given problem.
Even though the bounds in this section suggest that the more
rapid the decrease of the step sizes, the quicker the decrease of
h(xt), this is not always the case in practice. This is because
step sizes with large decay rates become small after the ﬁrst
few iterations, and small step sizes lead to slow changes in
h(xt). Conversely, small decay rates tend to yield rapidly
decreasing h(xt) in the ﬁrst few iterations since the step
sizes remain relatively large. Hence, it is difﬁcult to provide
universal guidelines since rapidly or slowly diminishing step
sizes may be preferred depending on the speciﬁc optimization
problem at hand. For example, if optimal solutions lie in
the interior of the feasible set, rapidly diminishing step sizes
can help reduce oscillations around optimal solutions, thus
improving the overall convergence rates. On the other hand,
if f (x) is monotone on X , the solution lies on the boundary,
which means that no oscillatory behavior is produced and,
hence, slowly diminishing step sizes will be preferable.

IV. RB-FW FOR NONCONVEX PROGRAMS

The objective function of (6) is nonconvex in certain ap-
plications, such as constrained multilinear decomposition [21]
and power system state estimation [22], [23]. Yet, convergence
of RB-FW has never been investigated for this case. The rest of
this section ﬁlls this gap by analyzing the convergence rate of
RB-FW in problems involving a nonconvex objective. Similar
to Sec. III, computational bounds are ﬁrst derived for a wide
class of step sizes, and are subsequently tailored for γt as
in (31) as well as for exact line search.

Recall that Section II introduced g(x) as a non-stationarity
measure of point x with respect to f (x). In the sequel, RB-FW
with be analyzed in terms of upper bounds on gt [cf. (26)].

Theorem 3. If {γt}t=0,1,... satisfy 0 ≤ γt ≤ 1 ∀t, it holds for
the iterates {xt}t=0,1,... of Algorithm 2 that

gt ≤

h(x0)

α (cid:80)t

k=0 γk

+

(cid:80)t
¯C B
f
2α (cid:80)t

k=0 γ2
k
k=0 γk

,

t ≥ 0.

(44)

Proof. Using 0 ≤ gt ≤ E[g(xk)] and (28), we deduce that

αgt

γk ≤α

γkE[g(xk)]

t
(cid:88)

k=0

t
(cid:88)

k=0
t
(cid:88)

(cid:16)

k=0

≤

E(cid:2)h(xk)(cid:3) − E(cid:2)h(xk+1)(cid:3)(cid:17)

+ ( ¯C B

f /2)

t
(cid:88)

k=0

γ2
k

t
(cid:88)

k=0

γ2
k

=E (cid:2)h(x0)(cid:3) − E (cid:2)h(xt+1)(cid:3) + ( ¯C B

f /2)

≤h(x0) + ( ¯C B

f /2)

t
(cid:88)

k=0

γ2
k

where the last
Dividing both sides by α (cid:80)t

inequality follows from E (cid:2)h(xt+1)(cid:3) ≥ 0.
k=0 γk leads to (44).

Clearly, Theorem 3 afﬁrms that limt→∞ gt = 0 if the step-

size sequence {γt}t=0,1,... satisﬁes

lim
t→∞

t
(cid:88)

k=0

γk = ∞, and

lim
t→∞

t
(cid:88)

k=0

γ2
k = S

for some ﬁnite S > 0. In other words, if {γt}t=0,1,... is not
summable and {γ2
t }t=0,1,... is summable, then either xt is a
stationary point for some t, or, a subsequence of {xt}t=0,1,...
converges to a stationary point.

For any given step size, the convergence rates of RB-FW can
be derived through (44). To start with, consider {γt}t=0,1,...
in (31) with q = α, ρ = 1; that is, γt = 2/(αt + 2), and
note that

t
(cid:88)

k=0
t
(cid:88)

k=0

(cid:90) t

2
αk + 2

≥

2
αx + 2

dx =

log

2
α

(cid:17)

(cid:16) αt + 2
2

4
(αk + 2)2 ≤

4

(αx + 2)2 dx =

4
α

(cid:16) 1

−

2 − α

x=−1

x=0

(cid:90) t

(45a)

(cid:17)

.

1
αt + 2
(45b)

By substituting (45) into Theorem 3, it follows that Algo-
rithm 2 attains a stationary point of a nonconvex program at
rate O(1/ log t).

This rather slow rate can be substantially improved upon

adopting exact line search for RB-FW.

Theorem 4. If {γt}t=0,1,... is chosen as in (42), it holds for
the iterates {xt}t=0,1,... of Algorithm 2 that

(cid:110)

max

(cid:111)

2h(x0), ¯C B
f
√

α

t + 1

gt ≤

,

t ≥ 0.

(46)

Proof. The right-hand side of (16) is minimized for

ˆγk = arg min
γ∈[0,1]

E (cid:2)h(xk)(cid:3) − αγE (cid:2)g(xk)(cid:3) + γ2 ¯C B

f /2

= min (cid:8)1, αE (cid:2)g(xk)(cid:3)/ ¯C B

(cid:9).

f

(47)

Thus, if E (cid:2)g(xk)(cid:3) ≥ ¯C B

f /α, then ˆγk = 1 and (16) becomes

E (cid:2)h(xk+1)(cid:3) ≤ E (cid:2)h(xk)(cid:3) − αE (cid:2)g(xk)(cid:3) + ¯C B

f /2

≤ E (cid:2)h(xk)(cid:3) − αE (cid:2)g(xk)(cid:3) /2

where the second inequality follows from ¯C B
Similarly, if E (cid:2)g(xk)(cid:3) < ¯C B
and (16) becomes

(48)
f ≤ αE (cid:2)g(xk)(cid:3).
f /α, then ˆγk = αE (cid:2)g(xk)(cid:3)/ ¯C B

f

E (cid:2)h(xk+1)(cid:3) ≤ E (cid:2)h(xk)(cid:3) − α2E (cid:2)g(xk)(cid:3)2

/2 ¯C B
f .

(49)

Combining both cases, (48) and (49) establish that

E (cid:2)h(xk+1)(cid:3) ≤ E (cid:2)h(xk)(cid:3)−min

(cid:40)

αE (cid:2)g(xk)(cid:3)
2

,

α2E2 (cid:2)g(xk)(cid:3)
2 ¯C B
f

and

(cid:41)

.

(50)
When γk is given by (42) with t = k, h(xk+1) is not greater
than when γk = ˆγk. Therefore, (50) still holds in the former
case. Thus, for {γk}k=0,1,... as in (42), it follows that
α2E (cid:2)g(xk)(cid:3)2
2 ¯C B
f

αE (cid:2)g(xk)(cid:3)
2

≤ E (cid:2)h(xk)(cid:3)−E (cid:2)h(xk+1)(cid:3) .

min

(cid:41)

(cid:40)

,

(51)

and

Summing (51) from k = 0 to t yields

(t + 1) min

(cid:41)

(cid:40)

αgt
2

,

α2g2
t
2 ¯C B
f

Therefore,

≤ h(x0) − E (cid:2)h(xt+1)(cid:3) .

(52)

gt ≤ max






2h(x0)
α(t + 1)

,

(cid:113)

2 ¯C B
√

f h(x0)
t + 1

α






.

(53)

√

t + 1 and

(cid:113)

2 ¯C B

f h(x0) ≤ max{2h(x0), ¯C B

f },

Since t+1 ≥
(46) holds.

Theorem 4 generalizes the recent result in [15], which only
applies to the classical FW method. The improved bound
in (46) is attained at the price of performing exact line search,
which requires the solution to a potentially nonconvex univari-
ate optimization subproblem (42). It is worth mentioning that
an optimal solution to this subproblem can be readily found
in a number of cases. For example, if f ((1 − γ)xt + γst) is
quadratic in γ, then γt can be readily found by evaluating this
function at three points.

All in all, the main contribution here is a convergence
rate analysis of RB-FW for minimizing (6) with nonconvex
f (x). Interestingly, when RB-FW relies on step sizes ob-
tained through line search, a stationary point is reached with
rate O(1/

t).

√

V. GENERALIZED STEP SIZES FOR FW

The availability of satisfactory step sizes for FW is rather
limited. Indeed, besides line search, convergence rate of FW
has only been established for γt = 2
t+1 [24].
This limits the user’s control on convergence of FW iterates;
cf. Remark 2. To alleviate this limitation, this section examines
the usage of step sizes in (31) and (38) in the classical FW
solver, namely Algorithm 1. Since the latter can be viewed
as a special case of Algorithm 2 with B = Nb, Corollaries 1

t+2 [3], and γt = 1

and 2 can be leveraged to derive the convergence rates of FW
for convex programs with the novel step sizes. Speciﬁcally,
the ensuing computational bounds hold.

Corollary 3. If f (x) is convex and the step size sequence
is chosen as in (31) with α = 1, q ∈ (0, 1]
{γt}t=0,1,...
and ρ ∈ (0.5, 1], then the successive iterates {xt}t=1,2,... of
Algorithm 1 satisfy for t ≥ 1

h(xt) ≤

2tCf
[q(t − 1)ρ + 2]2

gt ≤

(2ρ + 1)2ρ+1(qtρ + 2)
q2(2ρ)2ρ

(t + 1)Cf
t2ρ+1

.

Proof. This is a special case of Corollary 1 for α = 1.

Corollary 4. If f (x) is convex and the step-size sequence
then the
{γt}t=0,1,...
successive iterates {xt}t=1,2,... of Algorithm 1 satisfy for t ≥ 1

is chosen as in (38) with α = 1,

h(xt) ≤

2Cf
t + 2

gt ≤

27Cf
2

(cid:18) 1
t

+

2
t2 +

1
t3

(cid:19)

.

7

(54)

(55)

(56)

(57)

Proof. Corollary 4 follows directly from Corollary 2.

Corollaries 3 and 4 establish convergence rates in terms of
both h(xt) and gt for the classical FW method with step sizes
of different decay rates. For a given problem, the most suitable
step size can be selected following the guidelines in Remark 2.
Interestingly, comparing Corollaries 3 and 4 with Corollaries 1
and 2 reveals that the initial optimality gap h(x0) no longer
affects the bounds for FW.

VI. APPLICATIONS

Two applications where RB-FW exhibits signiﬁcant compu-
tational advantages over existing alternatives will be delineated
in this section.

A. Coordination of EV charging

The convex setup of optimal schedules for EV charging
in [25] is brieﬂy reviewed next. Suppose that a load aggregator
coordinates the charging of N EVs over the T consecutive
time slots T := {1, . . . , T } of length ∆τ . Let Tn ⊆ T denote
the time slots in which vehicle n is connected to the power
grid, and let pn(τ ) be the charging rate of EV n at time τ
to be scheduled by the load aggregator. If ¯pn is the charging
rate limitation imposed by the battery of vehicle n, then pn(τ )
should lie in the interval [0, ¯pn(τ )] with

¯pn(τ ) :=

(cid:26) ¯pn,
0,

τ ∈ Tn,
otherwise.

The charging proﬁle for vehicle n, denoted by p(cid:62)
:=
n
[pn(1), · · · , pn(T )], should therefore belong to the convex and
compact set
Pn := (cid:8)pn : ∆τ p(cid:62)

n 1 = Rn, 0 ≤ pn(τ ) ≤ ¯pn(τ ), ∀τ ∈ T (cid:9)

8

where Rn represents the total energy needed by EV n.

n=1, {¯pn}N

Given {Rn}N

the problem
solved by the aggregator is to ﬁnd the charging proﬁles
minimizing its electricity cost [25]; that is,

n=1, and {Tn}N

n=1,

p∗ ∈ arg min
p

f (p)

subject to pn ∈ Pn, ∀ n ∈ N

(58)

where p(cid:62) := [p(cid:62)
{D(τ )}T
f (p) is

N ] and N := {1, . . . , N }. With
τ =1 denoting additional known loads, the total cost

1 , · · · , p(cid:62)

n=1, {Tn}N

n} and t = 0

n=1, {¯pn}N

Algorithm 3 EV charging coordination solver
Input: {Rn}N
n=1, and B
1: Initialize {p0
2: while stopping criterion not met do
3:
4:
5:
6:

Randomly pick Bt ⊆ N such that |Bt| = B
Evaluate ct via (61) and broadcast ct entry order
Calculate {st
n}n∈Bt via (62) and (63)
Update {pt+1

n }n∈N via

pt+1

n =

(cid:26) (1 − γt)pt
pt
n,

n + γtst
n,

∀n ∈ Bt
∀n ∈ N \ Bt

f (p) =

D(τ ) +

T
(cid:88)

(cid:16)

τ =1

(cid:17)2

.

pn(τ )

N
(cid:88)

n=1

(59)

t ← t + 1

7:
8: end while

Note that f (p) is convex but not strongly convex in p. The fea-
sible set for (58) is the Cartesian product P := P1 × . . . × PN ,
which is convex and compact. Thus, problem (58) is convex
and of the form (6).

Assuming that the aggregator can only afford updating the
charging proﬁles of B out of the N vehicles in parallel
due to a limited number of processors, the ensuing B linear
subproblems arise when solving (58) via Algorithm 2:

st
n ∈ arg min
sn∈Pn

(cid:104)sn, ct(cid:105), n ∈ Bt

(60)

where |Bt| = B and ct := ∇pn f (pt). The latter does not
depend on n since the gradient ∇pn f (pt) is identical across
the N vehicles. Its τ -th entry is given by

ct(τ ) := 2(cid:0)D(τ ) +

n(τ )(cid:1).
pt

(61)

The subproblem (60) can be solved in closed form [26]. To
ﬁnd a solution, sort the entries of ct in non-decreasing order
2) ≤ . . . ≤ ct(τ t
by ﬁnding {τ t
1) ≤ ct(τ t
T ).
Subsequently, one needs to ﬁnd the index ¯τ t
n ≥ 1 for which

i=1 such that ct(τ t

i }T

¯τ t
n−1
(cid:88)

i=1

¯pn(τ t

i ) ≤ Rn and

¯pn(τ t

i ) > Rn.

(62)

Finally, the entries of the minimizer st

n(τ t
st

i ) =

n

j=1 ¯pn(τ t

j ),






¯pn(τ t
i ),
Rn − (cid:80)¯τ t
0,

n are found as
i = 1, . . . , ¯τ t
i = ¯τ t
n
i = ¯τ t
n + 1, . . . , T.

n − 1

(63)
The computational advantage of RB-FW for solving (58)
stems from the fact that the solution to the subproblems (60)
can be obtained efﬁciently via (63) upon receiving the ct
entry order, whereas competing alternatives require projections
onto {Pn}n∈Bt per iteration [12]. Our RB-FW-based charging
scheme is summarized in Algorithm 3.

N
(cid:88)

n=1

¯τ t
n(cid:88)

i=1

B. Structural SVMs

The term structured prediction comprises a family of ma-
chine learning problems, where the output to the predictors
have variable sizes [27]. An example is the optical character
recognition (OCR) task, where one is given a vector z ∈ RP
containing the P -pixel image of an M -letter word. The goal

is to produce a vector y ∈ {1, . . . , 26}M , whose m-th entry
indicates which of the 26 letters of the alphabet corresponds to
the m-th character in that word. This problem is challenging
because the output y may take 26M values, and also the same
predictor must work for different values of M .

Structural SVMs have been widely adopted to carry out the
aforementioned structured prediction tasks [28], [29]. Upon
deﬁning the application-dependent feature map φ [29] that
information for the input-output pair
encodes the relevant
(z, y) in the d-dimensional vector φ(z, y), a vector w is
learned so that (cid:104)w, φ(z, y)(cid:105) when seen as a function of y
is maximized at the correct y for a given input z. Given N
training pairs {(zn, yn)}N

n=1, w is learned by solving
N
(cid:88)

(cid:107)w(cid:107)2 +

ξn

1
N

n=1

λ
2

(64a)

minimize
w,ξ

subject to (cid:104)w, ψn(˜y)(cid:105) ≥ Ln(˜y) − ξn

(64b)

∀˜y ∈ Yn, ∀n ∈ N

where N := {1, . . . , N }, ψn(˜y) := φ(zn, yn) − φ(zn, ˜y),
Ln(˜y) is the incurred loss by predicting ˜y instead of the given
label yn, {ξn}N
n=1 are slack variables, λ is a nonnegative con-
stant, and Yn is the set of all possible outputs for input zn.
In the OCR example, Yn = {1, . . . , 26}Mn , where Mn is the
number of characters of the n-th word.

Problem (64) is difﬁcult since the number of constraints
explodes with |Yn|. If βn(˜y) is the Lagrange dual variable
associated with (64b), vector βn is formed with entries
{βn(˜y)}˜y∈Yn , and vector β has entries {βn}n∈Nb , the dual
of (64) is [8]

minimize
β∈Rm
β≥0

subject to

f (β) :=

(cid:107)Aβ(cid:107)2 − b(cid:62)β

(65)

λ
2

1(cid:62)βn = 1,

∀n ∈ N

where m := (cid:80)
n |Yn|, A ∈ Rd×m is formed with columns
λN ψn(˜y) ∈ Rd| ˜y ∈ Yn, n ∈ N }, and vector b ∈ Rm has
{ 1
entries { 1

N Ln(˜y)}˜y∈Yn,n∈N .

A randomized single-block FW is adopted by [8], to solve
(65). Extending this approach to B > 1 yields Algorithm 4.
To avoid storing the high-dimensional vector βt, auxiliary
variables ˜wt := Aβt,
t = 0, 1, . . . are introduced. It can
be shown that iterates { ˜wt}t=0,1... converge to the global
minimizer of (64); see [8] for additional details.

9

Fig. 2. Convergence performance of Algorithm 3 with B = 1.

n=1, {Yn}N

Algorithm 4 Structural SVMs solver
Input: {(zn, yn)}N
1: Initialize β0, ˆ(cid:96)0 = (cid:96)0
2: Calculate ˜w0 = ˜w0
3: while stopping criterion not met do
4:

1 = . . . = (cid:96)0
Nb
1 = . . . = ˜w0
Nb

n=1, and B

Randomly pick Bt ⊆ N such that |Bt| = B
for n ∈ Bt do
Compute

= 0, and t = 0
= Aβ0

y∗

n := arg max
y∈Yn

Ln(y) − (cid:104) ˜wt, ψn(y)(cid:105)

n + γt

n = (1 − γt) ˜wt
n = (1 − γt)(cid:96)t

Update ˜wt+1
Update (cid:96)t+1
Update ˜wt+1 = ˜wt + ˜wt+1
Update (cid:96)t+1 = (cid:96)t + (cid:96)t+1

n + γt
n − ˜wt
n
n − (cid:96)t
n

λN ψn(y∗
n)
N Ln(y∗
n)

5:
6:

7:
8:

9:
10:
11:
12:
13: end while

end for
t ← t + 1

VII. SIMULATED TESTS
This section demonstrates the efﬁcacy of the novel step
sizes, and our parallel RB-FW solvers, in the context of the
large-scale applications of Sec. VI.

A. Coordination of EV charging

In the ﬁrst experiment, 63 EVs with maximum charging
power ¯pn = 3.45 kW ∀n, were scheduled. The simulation
comprises T = 96 time slots ranging from 12:00 pm to
12:00 pm of the next day. The values of {Tn}N
n=1 and
{Rn}N
n=1 were set according to real travel data of the National
Household Travel Survey [30], [12]. The base load {Dτ }T
τ =1
were obtained by averaging the 2014 residential load data from
Southern California Edison [31].

Convergence is assessed in terms of the relative error
(cid:15)(pt) := (f (pt) − f (p∗)) /f (p∗), where p∗ is obtained using
the off-the-shelf solver SeDuMi.

The following step-size sequences were compared.

(S1) :

γt :=

2
αt + 2
(cid:113)
α2γ4

(S2) :

γt :=

(S3) :

γt :=

(S4) :

γt :=

(S5) :

γt :=

2

2
0.5αt + 2

2
0.5αt0.9 + 2
2
0.5αt0.8 + 2

.

Fig. 3. Convergence performance of Algorithm 3 with B = 10.

(66)

where the index ¯τ 0

n ≥ 1 was found as

t−1 + 4γ2

t−1 − αγ2

t−1

, γ0 = 1

¯τ 0
n−1
(cid:88)

τ =1

¯τ 0
n(cid:88)

τ =1

¯pn(τ ) ≤ Rn and

¯pn(τ 0) > Rn.

S2 is the sequence in (38), whereas S1 and S3-S5 are special
cases of (31). Sequences S1-S5 cover a wide range of decay
rates. S2 vanishes faster than S1 [cf. (39a)], whereas the decay
rates of S3-S5 are smaller than that of S1. Note that S1 boils
down to the step size in (18) when setting B = 1. For all
n = 1, . . . , N , p0

n was initialized as
¯pn(τ ),
Rn − (cid:80)¯τ 0
0,

j=1 ¯pn(j),

n






p0
n(τ ) =

n − 1

τ = 1, . . . , ¯τ 0
τ = ¯τ 0
n
τ = ¯τ 0
n + 1, . . . , T

The ﬁrst experiment assumed that only one vehicle was
randomly selected to update its charging proﬁle per iteration.
Algorithm 3 with B = 1 was run with the step sizes S1-S5 for
1,000 iterations. Fig. 2 depicts the evolution of (cid:15)(pt) across the
iteration index t for Algorithm 3 with step sizes S1-S5 when
B = 1. It is observed that the algorithm converges towards a
global minimum for all the tested step sizes. In this scenario,
the more slowly the step size diminishes, the faster the relative
error decreases. Since B = 1 and Algorithm 3 is a special
instance of Algorithm 2, Fig. 2 therefore highlights how
randomized single-block FW can beneﬁt from the proposed
step sizes. Speciﬁcally, the proposed step sizes S3-S5 lead to
a much faster convergence than S1, which coincides with the
step size in (18) since B = 1.

10

Fig. 4. Number of iterations to achieve (cid:15)(pt) ≤ 10−5.

Fig. 6. Progress of g(βt) for Algorithm 4 with B = 1.

Fig. 5. Empirical success rate for S1-S5 with different values of B.

Fig. 7. Progress of g(βt) for Algorithm 4 with B = 2.

The second experiment tested Algorithm 3 with B = 10.
Fig. 3 further conﬁrms that slowly diminishing step sizes lead
to fast convergence in the ﬁrst few iterations. However, as
the iterates approach a minimum, the slowly diminishing step
sizes yield larger oscillations; see e.g. S5 in Fig. 3. This
phenomenon has already been described in Remark 2. Com-
paring Figs. 2 and 3 reveals that considerably less iterations
are required to achieve a target accuracy for larger B. For
example, about one ﬁfth of iterations are now required for
Algorithm 3 with S5 to reach (cid:15)(p) ≤ 10−5. Thus, if the ten
blocks can be processed in parallel, setting B = 10 roughly
reduces the computation time by a factor of ﬁve, which further
corroborates the merits of parallel RB-FW.

The next experiment highlights the impact of B on the
convergence of Algorithm 3. Five copies of Algorithm 3, each
one with a different step size S1-S5, are executed for 100
independent trials. The minimum value of t such that at least
one of these copies satisﬁes (cid:15)(pt) ≤ 10−5 is recorded. Fig. 4
represents the sample mean and standard derivation of this
minimum t averaged over the 100 trials for different values

of B. It is observed that both mean and standard derivation
decrease for increasing B. If each iteration of Algorithm 3
is run in B cores in parallel, then the number of iterations
constitutes a proxy for runtime. Fig. 4 adopts this proxy to
showcase the beneﬁt of adopting B > 1. Nonetheless, observe
that the inﬂuence of B on the number of iterations decreases
for large B. Fig. 5 depicts the fraction of trials that each copy
of Algorithm 3 is the ﬁrst among the ﬁve copies in achieving
(cid:15)(pt) ≤ 10−5. This ﬁgure reveals that slowly diminishing step
sizes are preferable for small values of B.

B. Structural SVMs

The structural SVMs experiment was conducted on a subset
of the OCR dataset [28], [32]. The feature mapping φ(z, y),
loss function Ln(˜y), and solution to the subproblems in
step 5 of Algorithm 4 were evaluated using the open source
code [33] released by the authors in [8]. The dimension of
φ(z, y) is d = 4, 028, and the number of training examples is
N = 6, 251. To initialize each β0
n, one of its entries chosen

uniformly at random was set to one, whereas all the remaining
entries were set to zero. Algorithm 4 with λ = 0.1 and step
sizes S1-S5 was run for six passes through all the training
examples. The duality gap g(βt) in (14) is depicted in Figs. 6
and 7 for B = 1 and B = 2, respectively. In both cases,
Algorithm 4 with S5 outperforms all other variants in the ﬁrst
few iterations. Furthermore, it can be seen that the required
number of iterations to achieve a target accuracy almost halves
when increasing B from one to two.

B. Proof of Lemma 2

Plugging (31) into the left-hand side of (17b) yields

1 − αγt+1
γ2
t+1

[q(t + 1)ρ + 2 − α]2 − α2
4

=

≤

≤

[q(t + 1)ρ + 2 − α]2
4
[q(t + 1)ρ − q + 2]2
4

.

11

(68)

VIII. CONCLUDING SUMMARY

The RB-FW algorithm is especially suited for solving high-
dimensional constrained learning problems whose feasible set
is block separable. For convex programs, the present contribu-
tion developed a rich family of feasibility-ensuring step sizes
that enable parallel updates of provably convergent RB-FW
iterates. The novel step sizes admit various decay rates, leading
to ﬂexible convergence rates of RB-FW. Convergence of RB-
FW is further established for constrained nonconvex problems
too. Numerical tests using real-world datasets corroborated the
speed-up advantage of parallel RB-FW with the proposed step
sizes over randomized single-block FW. In addition, single-
block FW with the developed slowly diminishing step sizes
converges markedly faster than that with existing step sizes.

where the last inequality follows from q ≤ α ≤ 1. Consider
the auxiliary function ϕ(x) := (x + c)ρ − xρ − c,
x ≥ 0 for
some constant c ≥ 1, and its ﬁrst-order derivative

ϕ(cid:48)(x) = ρ(x + 1)ρ−1 − ρxρ−1,

x ≥ 0.

Since ρ ≤ 1, it holds that ϕ(cid:48)(x) ≤ 0, and thus,

ϕ(x) ≤ ϕ(0) = cρ − c ≤ 0,

∀x ≥ 0

or,

(x + c)ρ − c ≤ xρ,

∀x ≥ 0.

(69)

Multiplying both sides of (69) by q, and setting c = 1 and
x = t gives rise to

0 ≤ q(t + 1)ρ − q ≤ qtρ,

∀t ≥ 0.

(70)

Combining (68) and (70) yields

1 − αγt+1
γ2
t+1

≤

[qtρ + 2]2
4

=

1
γ2
t

which concludes the proof.

Expression (32) follows directly by substituting (31) into

(20). To show (33), apply Theorem 2 to verify that

E (cid:2)h(xK)(cid:3)
γ2
0 (t − K + 1)γt
(1 − αγ0)γ2

K−1h(x0)

+

¯C B
f γ2
K
2γt

≤

≤

≤

γ2
0 (t − K + 1)γt
K−1h(x0)

(1 − α) γ2

(t − K + 1)γt
γ2
K−1
t − K + 1

·

(t + 1) ¯C B

+

K−1

K ¯C B

f γ2
2(t − K + 1)γt
¯C B
γ2
f (t + 1)
K−1
2γt(t − K + 1)
f + 2(1 − α)h(x0)

+

2γt

+

¯C B
f γ2
K
2γt

(71)

for all K ∈ {1, . . . , t}, where the second inequality stems
from (20) and the third one follows from γK ≤ γK−1 and
γ0 = 1. The next step is to bound the ﬁrst quotient in the
right-hand side of (71). To this end, set c = 2/q and x = K −1
in (69) to deduce that

γK−1 =

2
q(K − 1)ρ + 2

≤

2
q(K − 1 + 2/q)ρ .

(72)

Now set K = (cid:100)µ(t + 2/q)(cid:101), where µ is an arbitrary constant.
Since K ∈ {1, . . . , t}, µ needs to satisfy

0 < µ ≤

t
t + 2/q

.

(73)

APPENDIX

A. Proof of Lemma 1

ﬁnd

Using (12) together with steps 4 and 5 of Algorithm 2, we

C. Proof of Corollary 1

f (xt+1) ≤ f (xt) +

(cid:104)xt+1

n − xt

n, ∇xn f (xt)(cid:105) + γ2

t C Bt

f /2

= f (xt) +

γt(cid:104)st

n − xt

n, ∇xn f (xt)(cid:105) + γ2

t C Bt

f /2.

αgt ≤

(cid:88)

n∈Bt
(cid:88)

n∈Bt

(cid:88)

n∈Bt

Subtracting f (x∗) from both sides yields

h(xt+1) ≤ h(xt) +

γt(cid:104)st

n − xt

n, ∇xn f (xt)(cid:105) + γ2

t C Bt

f /2.

Taking conditional expectation with respect to Bt, we arrive
for a given xt at

(cid:2)h(xt+1)|xt(cid:3)
(cid:88)

EBt
≤ h(xt) + α

n∈Nb

γt(cid:104)st

n − xt

n, ∇xn f (xt)(cid:105) + γ2
t

¯C B

f /2

= h(xt) + αγt(cid:104)st − xt, ∇f (xt)(cid:105) + γ2
t
= h(xt) − αγtg(xt) + γ2
t

f /2

¯C B

¯C B

f /2

(67)

where the last equality follows from (14) and step 4 of
Algorithm 2. Since xt
taking
expectations in (67) with respect to {Bτ }t−1

is determined by {Bτ }t−1
τ =0,

τ =0 yields (16).

12

Since

(79)

(80)

(81)

(cid:100)µ(t+2/q)(cid:101)−1+2/q ≥ µ(t+2/q)−2+2/q ≥ µ(t+2/q) > 0

it follows from (72) that

γK−1 ≤

2
qµρ(t + 2/q)ρ .

Therefore,

γ2
K−1
t − K + 1

4
q2(t − K + 1)µ2ρ(t + 2/q)2ρ

≤

≤

4
q2[t − µ(t + 2/q)]µ2ρ(t + 2/q)2ρ .

(74)

α2

(αt + 1)2 + 4 ≥

Minimizing the right-hand side with respect
interval (73) yields

to µ in the

This inequality implies that

(cid:18) 2

(cid:19)

αt + 2

ˆϕ

4α2

(αt + 2)2 + 4 −

2α
(αt + 2)2

(cid:115)

1
αt + 2
2
αt + 2 + α

.

=

≤

Combining (79) with the second inequality in (77) proves the
second inequality in (76).

On the other hand, since (αt + 1 + α)2 ≥ α(αt + 1 + α) +

(αt + 1)2, it holds that

4α
αt + 1 + α

+

4(αt + 1)2
(αt + 1 + α)2

α2
(αt + 1)2 +
(cid:18) α

=

αt + 1

+

2(αt + 1)
αt + 1 + α

(cid:19)2

.

γ2
K−1
t − K + 1

≤

4(2ρ + 1)2ρ+1
q2(2ρ)2ρt2ρ+1

for µ = 2ρ
2ρ+1

t
t+2/q .

From (71), gt can be upper bounded as

gt ≤

·

(t + 1) ¯C B

γ2
K−1
t − K + 1
(2ρ + 1)2ρ+1(qtρ + 2)
αq2(2ρ)2ρ

·

≤

f + 2(1 − α)h(x0)

2αγt
(t + 1) ¯C B

f + 2(1 − α)h(x0)

.

t2ρ+1

where the second inequality follows from (75) and (31).

Thus,

(75)

(cid:18) 1

(cid:19)

αt + 1

ˆϕ

(cid:115)

=

≥

1
2(αt + 1)
1
αt + 1 + α

.

α2

(αt + 1)2 + 4 −

α
2(αt + 1)2

Combining (81) with the ﬁrst inequality in (77) proves the ﬁrst
inequality in (76), thus concluding the proof of (39a).

To prove (39b), one can also proceed by induction. First,
≤ 1. Assuming γt−1 ≤ γt, it follows

γ1 ≤ γ0 since
that γt ≤ γt+1 since ˆϕ(x) is nondecreasing.

α2+4−α
2

√

D. Proof of Lemma 3

E. Proof of Corollary 2

To prove (39a) by induction, it clearly holds for t = 0, and
assume that it holds also for a ﬁxed t ≥ 0. Then, one needs
to show that

1
αt + 1 + α

≤ γt+1 ≤

2
αt + 2 + α

.

(76)

To this end, deﬁne the auxiliary function
√

ˆϕ(x) :=

α2x4 + 4x2 − αx2
2

,

x ≥ 0

which is monotonically increasing since

ˆϕ(cid:48)(x) =

(α2x2 + 2) − αx
√

α2x2 + 4

√

α2x2 + 4

> 0.

Thus, by the induction hypothesis we have

(cid:18) 1

(cid:19)

αt + 1

ˆϕ

≤ γt+1 = ˆϕ(γt) ≤ ˆϕ

(77)

(cid:18) 2

αt + 2

(cid:19)

.

1 −

2α
αt + 2 + α

(cid:18)

≤

1 −

α
αt + 2 + α

(cid:19)2

(cid:18) αt + 2

(cid:19)2

=

αt + 2 + α

Note that

or, equivalently,

(cid:18) αt + 2

(cid:19)2

αt + 2 + α

1 ≤

=

2α
αt + 2 + α
(cid:18) α

+

αt + 2

+

αt + 2
αt + 2 + α

(cid:19)2

−

α2
(αt + 2)2 .

(78)

Inequality (40) readily follows from (20) and (39a). To
prove (41), note that (71) holds because of γ0 = 1 and (39).
Meanwhile, by the second inequality in (39a), the step size
in (38) satisﬁes (75) for q = α and ρ = 1, that is

γ2
K−1
t − K + 1

≤

27
α2t3 .

(82)

Plugging (39a) together with (82) into (71), yields (41).

REFERENCES

[1] M. Frank and P. Wolfe, “An algorithm for quadratic programming,”
Naval Research Logistics Quarterly, vol. 3, no. 1–2, pp. 95–110, 1956.
[2] V. F. Deminaov and A. M. Rubinov, Approximate Methods in Optimiza-

tion Problems. Amsterdam, Netherlands: Elsevier, 1970.

[3] M. Jaggi, “Revisiting Frank-Wolfe: Projection-free sparse convex opti-
mization,” in Proc. Intl. Conf. on Machine Learning, Atlanta, GA, Jun.
2013.

[4] D. P. Bertsekas, Nonlinear Programming, 2nd ed.

Belmont, MA:

[5] Y. Nesterov, Introductory Lectures on Convex Optimization. Boston,

Athena Scientiﬁc, 1999.

MA: Kluwer, 2004.

[6] M. Jaggi and M. Sulovsk, “A simple algorithm for nuclear norm
regularized problems,” in Proc. Intl. Conf. on Machine Learning, Haifa,
Israel, Jun. 2010.

[7] Z. Harchaoui, A. Juditsky, and A. Nemirovski, “Conditional gradient
algorithms for norm-regularized smooth convex optimization,” Math.
Program., vol. 152, no. 1-2, pp. 75–112, 2015.

[8] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher, “Block-
coordinate Frank-Wolfe optimization for structural SVMs,” in Proc. Intl.
Conf. on Machine Learning, Atlanta, GA, Jun. 2013.

[9] S. Lacoste-Julien, F. Lindsten, and F. R. Bach, “Sequential kernel
herding: Frank-Wolfe optimization for particle ﬁltering.” in Intl. Conf.
Artiﬁcial Intelligence and Statistics, San Diego, CA, May 2015.

13

[10] G. Wang, L. Zhang, G. B. Giannakis, J. Chen, and M. Akcakaya, “Sparse
phase retrieval via truncated amplitude ﬂow,” arXiv:1611.07641, 2016.
[11] G. Wang, G. B. Giannakis, and Y. C. Eldar, “Solving systems of random
quadratic equations via truncated amplitude ﬂow,” IEEE Trans. Inf.
Theory, 2017 (to appear); see also arXiv:1605.08285, 2016.

[12] L. Zhang, V. Kekatos, and G. B. Giannakis, “Scalable electric vehicle
charging protocols,” IEEE Trans. Power Syst., vol. 32, no. 2, pp. 1451–
1462, Mar. 2017.

[13] A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. K. Dokania, and S. Lacoste-
Julien, “Minding the gaps for block Frank-Wolfe optimization of struc-
tured SVMs,” in Proc. Intl. Conf. on Machine Learning, New York City,
NY, Jun. 2016.

[14] Y. Wang, V. Sadhanala, W. Dai, W. Neiswanger, S. Sra, and E. P. Xing,
“Parallel and distributed block-coordinate Frank-Wolfe algorithms,” in
Proc. Intl. Conf. on Machine Learning, New York City, NY, Jun. 2016.
[15] S. Lacoste-Julien, “Convergence rate of Frank-Wolfe for non-convex
objectives,” 2016. [Online]. Available: https://arxiv.org/abs/1607.00345
[16] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition
by basis pursuit,” SIAM Review, vol. 43, no. 1, pp. 129–159, Feb. 2001.
[17] J. Liu, P. Musialski, P. Wonka, and J. Ye, “Tensor completion for
estimating missing values in visual data,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. 35, no. 1, pp. 208–220, Jan. 2013.

[18] C. M. Ala´ız, ´A. Barbero, and J. R. Dorronsoro, “Group fused lasso,”
in Proc. Intl. Conf. on Artiﬁcial Neural Networks, Soﬁa, Bulgaria, Mar.
2013, pp. 66–73.

[19] S. Jegelka, F. Bach, and S. Sra, “Reﬂection methods for user-friendly

submodular optimization,” Stateline, NV, Dec. 2013, pp. 1313–1321.

[20] K. L. Clarkson, “Coresets, sparse greedy approximation, and the Frank-
Wolfe algorithm,” ACM Trans. Algorithms, vol. 6, no. 4, p. 63, July
2010.

[21] E. E. Papalexakis, N. D. Sidiropoulos, and R. Bro, “From k-means to
higher-way co-clustering: Multilinear decomposition with sparse latent
factors,” IEEE Trans. Signal Process., vol. 61, no. 2, pp. 493–506, Dec.
2013.

[22] G. B. Giannakis, V. Kekatos, N. Gatsis, S.-J. Kim, H. Zhu, and
B. Wollenberg, “Monitoring and optimization for power grids: A signal
processing perspective,” IEEE Signal Process. Mag., vol. 30, no. 5, pp.
107–128, Sep. 2013.

[23] G. Wang, A. S. Zamzam, G. B. Giannakis, and N. D. Sidiropoulos,
“Power system state estimation via feasible point pursuit: Algorithms
and Crm´er-Rao bound,” arXiv:1705.04031, 2017.

[24] R. M. Freund and P. Grigas, “New analysis and results for the Frank–
Wolfe method,” Math. Program., vol. 155, no. 1-2, pp. 199–230, Jan.
2016.

[25] L. Zhang, V. Kekatos, and G. B. Giannakis, “A generalized Frank-Wolfe
approach to decentralized control of vehicle charging,” in Proc. IEEE
Conf. on Decision and Control, Las Vegas, NV, Dec. 2016.

[26] S. Boyd and L. Vandenberghe, Convex Optimization. New York, NY:

Cambridge University Press, 2004.

[27] G. Bakır, T. Hofmann, B. Sch¨olkopf, A. J. Smola, B. Taskar, and S. V.
Cambridge, MA: MIT

Vishwanathan, Predicting Structured Data.
press, 2007.

[28] B. Taskar, C. Guestrin, and D. Koller, “Max-margin Markov networks,”

Vancouver, Canada, Dec. 2003.

[29] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, “Large margin
methods for structured and interdependent output variables,” vol. 6, pp.
1453–1484, Sep. 2005.

[30] Federal Highway Administration. US Department of Transportation.

[Online]. Available: http://nhts.ornl.gov/2009/pub/stt.pdf

[31] Southern California Edison dynamic load proﬁles. Southern California
[Online]. Available: https://www.sce.com/wps/portal/home/

[32] OCR dataset. Stanford University. [Online]. Available: http://ai.stanford.

Edison.
regulatory/load-proﬁles/

edu/∼btaskar/ocr/

[33] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-
coordinate Frank-Wolfe solver for structural SVMs. [Online]. Available:
https://github.com/ppletscher/BCFWstruct

