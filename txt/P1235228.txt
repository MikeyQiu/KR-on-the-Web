8
1
0
2
 
t
c
O
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
6
2
0
0
1
.
2
0
8
1
:
v
i
X
r
a

Loss Surfaces, Mode Connectivity,
and Fast Ensembling of DNNs

Timur Garipov∗1,2 Pavel Izmailov∗3 Dmitrii Podoprikhin∗4
Dmitry Vetrov5 Andrew Gordon Wilson3

1Samsung AI Center in Moscow, 2Skolkovo Institute of Science and Technology,
3Cornell University,
4Samsung-HSE Laboratory, National Research University Higher School of Economics,
5National Research University Higher School of Economics

Abstract

The loss functions of deep neural networks are complex and their geometric
properties are not well understood. We show that the optima of these complex
loss functions are in fact connected by simple curves over which training and test
accuracy are nearly constant. We introduce a training procedure to discover these
high-accuracy pathways between modes. Inspired by this new geometric insight,
we also propose a new ensembling method entitled Fast Geometric Ensembling
(FGE). Using FGE we can train high-performing ensembles in the time required to
train a single model. We achieve improved performance compared to the recent
state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.

1

Introduction

The loss surfaces of deep neural networks (DNNs) are highly non-convex and can depend on millions
of parameters. The geometric properties of these loss surfaces are not well understood. Even for
simple networks, the number of local optima and saddle points is large and can grow exponentially in
the number of parameters [2, 3, 4]. Moreover, the loss is high along a line segment connecting two
optima [e.g., 8, 17]. These two observations suggest that the local optima are isolated.

In this paper, we provide a new training procedure which can in fact ﬁnd paths of near-constant
accuracy between the modes of large deep neural networks. Furthermore, we show that for a wide
range of architectures we can ﬁnd these paths in the form of a simple polygonal chain of two line
segments. Consider, for example, Figure 1, which illustrates the ResNet-164 (cid:96)2-regularized cross-
entropy train loss on CIFAR-100, through three different planes. We form each two dimensional
plane by all afﬁne combinations of three weight vectors.1

The left panel shows a plane deﬁned by three independently trained networks. In this plane, all
optima are isolated, which corresponds to the standard intuition. However, the middle and right
panels show two different paths of near-constant loss between the modes in weight space, discovered
by our proposed training procedure. The endpoints of these paths are the two independently trained
DNNs corresponding to the two lower modes on the left panel.

∗ Equal contribution.
1Suppose we have three weight vectors w1, w2, w3. We set u = (w2 − w1), v = (w3 − w1) −
(cid:104)w3 − w1, w2 − w1(cid:105)/(cid:107)w2 − w1(cid:107)2 · (w2 − w1). Then the normalized vectors ˆu = u/(cid:107)u(cid:107), ˆv = v/(cid:107)v(cid:107)
form an orthonormal basis in the plane containing w1, w2, w3. To visualize the loss in this plane, we deﬁne a
Cartesian grid in the basis ˆu, ˆv and evaluate the networks corresponding to each of the points in the grid. A point
P with coordinates (x, y) in the plane would then be given by P = w1 + x · ˆu + y · ˆv.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: The (cid:96)2-regularized cross-entropy train loss surface of a ResNet-164 on CIFAR-100, as a
function of network weights in a two-dimensional subspace. In each panel, the horizontal axis is
ﬁxed and is attached to the optima of two independently trained networks. The vertical axis changes
between panels as we change planes (deﬁned in the main text). Left: Three optima for independently
trained networks. Middle and Right: A quadratic Bezier curve, and a polygonal chain with one bend,
connecting the lower two optima on the left panel along a path of near-constant loss. Notice that in
each panel a direct linear path between each mode would incur high loss.

We believe that this geometric discovery has major implications for research into multilayer networks,
including (1) improving the efﬁciency, reliability, and accuracy of training, (2) creating better
ensembles, and (3) deriving more effective posterior approximation families in Bayesian deep
learning. Indeed, in this paper we are inspired by this geometric insight to propose a new ensembling
procedure that can efﬁciently discover multiple high-performing but diverse deep neural networks.

In particular, our contributions include:

The discovery that the local optima for modern deep neural networks are connected by very
simple curves, such as a polygonal chain with only one bend.
A new method that ﬁnds such paths between two local optima, such that the train loss and
test error remain low along these paths.
Using the proposed method we demonstrate that such mode connectivity holds for a wide
range of modern deep neural networks, on key benchmarks such as CIFAR-100. We show
that these paths correspond to meaningfully different representations that can be efﬁciently
ensembled for increased accuracy.
Inspired by these observations, we propose Fast Geometric Ensembling (FGE), which
outperforms the recent state-of-the-art Snapshot Ensembles [13], on CIFAR-10 and CIFAR-
100, using powerful deep neural networks such as VGG-16, Wide ResNet-28-10, and
ResNet-164. On ImageNet we achieve 0.56% top-1 error-rate improvement for a pretrained
ResNet-50 model by running FGE for only 5 epochs.
We release the code for reproducing the results in this paper at
https://github.com/timgaripov/dnn-mode-connectivity

•

•

•

•

•

The rest of the paper is organized as follows. Section 2 discusses existing literature on DNN loss
geometry and ensembling techniques. Section 3 introduces the proposed method to ﬁnd the curves
with low train loss and test error between local optima, which we investigate empirically in Section 4.
Section 5 then introduces our proposed ensembling technique, FGE, which we empirically compare
to the alternatives in Section 6. Finally, in Section 7 we discuss connections to other ﬁelds and
directions for future work.

Note that we interleave two sections where we make methodological proposals (Sections 3, 5), with
two sections where we perform experiments (Sections 4, 6). Our key methodological proposal for
ensembling, FGE, is in Section 5.

2 Related Work

Despite the success of deep learning across many application domains, the loss surfaces of deep
neural networks are not well understood. These loss surfaces are an active area of research, which
falls into two distinct categories.

The ﬁrst category explores the local structure of minima found by SGD and its modiﬁcations.
Researchers typically distinguish sharp and wide local minima, which are respectively found by using

2

large and small mini-batch sizes during training. Hochreiter and Schmidhuber [12] and Keskar et al.
[17], for example, claim that ﬂat minima lead to strong generalization, while sharp minima deliver
poor results on the test dataset. However, recently Dinh et al. [5] argue that most existing notions
of ﬂatness cannot directly explain generalization. To better understand the local structure of DNN
loss minima, Li et al. [20] proposed a new visualization method for the loss surface near the minima
found by SGD. Applying the method for a variety of different architectures, they showed that the loss
surfaces of modern residual networks are seemingly smoother than those of VGG-like models.

The other major category of research considers global loss structure. One of the main questions
in this area is how neural networks are able to overcome poor local optima. Choromanska et al.
[3] investigated the link between the loss function of a simple fully-connected network and the
Hamiltonian of the spherical spin-glass model. Under strong simplifying assumptions they showed
that the values of the investigated loss function at local optima are within a well-deﬁned bound. In
other research, Lee et al. [18] showed that under mild conditions gradient descent almost surely
converges to a local minimizer and not a saddle point, starting from a random initialization.

In recent work Freeman and Bruna [7] theoretically show that local minima of a neural network
with one hidden layer and ReLU activations can be connected with a curve along which the loss
is upper-bounded by a constant that depends on the number of parameters of the network and the
“smoothness of the data”. Their theoretical results do not readily generalize to multilayer networks.
Using a dynamic programming approach they empirically construct a polygonal chain for a CNN
on MNIST and an RNN on PTB next word prediction. However, in more difﬁcult settings such as
AlexNet on CIFAR-10 their approach struggles to achieve even the modest test accuracy of 80%.
Moreover, they do not consider ensembling.

By contrast, we propose a much simpler training procedure that can ﬁnd near-constant accuracy
polygonal chains with only one bend between optima, even on a range of modern state-of-the-art
architectures. Inspired by properties of the loss function discovered by our procedure, we also propose
a new state-of-the-art ensembling method that can be trained in the time required to train a single
DNN, with compelling performance on many key benchmarks (e.g., 96.4% accuracy on CIFAR-10).

Xie et al. [26] proposed a related ensembling approach that gathers outputs of neural networks from
different epochs at the end of training to stabilize ﬁnal predictions. More recently, Huang et al. [13]
proposed snapshot ensembles, which use a cosine cyclical learning rate [21] to save “snapshots” of the
model during training at times when the learning rate achieves its minimum. In our experiments, we
compare our geometrically inspired approach to Huang et al. [13], showing improved performance.

Draxler et al. [6] simultaneously and independently discovered the existence of curves connecting
local optima in DNN loss landscapes. To ﬁnd these curves they used a different approach inspired by
the Nudged Elastic Band method [16] from quantum chemistry.

3 Finding Paths between Modes

We describe a new method to minimize the training error along a path that connects two points in the
space of DNN weights. Section 3.1 introduces this general procedure for arbitrary parametric curves,
and Section 3.2 describes polygonal chains and Bezier curves as two example parametrizations of
such curves. In the supplementary material, we discuss the computational complexity of the proposed
approach and how to apply batch normalization at test time to points on these curves. We note
that after curve ﬁnding experiments in Section 4, we make our key methodological proposal for
ensembling in Section 5.

3.1 Connection Procedure

Let ˆw1 and ˆw2 in R|net| be two sets of weights corresponding to two neural networks independently
trained by minimizing any user-speciﬁed loss
is the
R|net| be a continuous piecewise smooth
number of weights of the DNN. Moreover, let φθ : [0, 1]
parametric curve, with parameters θ, such that φθ(0) = ˆw1, φθ(1) = ˆw2.

(w), such as the cross-entropy loss. Here,

net

→

L

|

|

3

To ﬁnd a path of high accuracy between ˆw1 and ˆw2, we propose to ﬁnd the parameters θ that minimize
the expectation over a uniform distribution on the curve, ˆ(cid:96)(θ):

(cid:82)

ˆ(cid:96)(θ) =

(φθ)dφθ
L
(cid:82) dφθ

=

1
(cid:82)
0 L

(φθ(t))

φ(cid:48)
dt
θ(t)
(cid:107)
(cid:107)

1
(cid:82)
0 (cid:107)

φ(cid:48)
dt
θ(t)
(cid:107)

=

1
(cid:90)

0

L

(φθ(t))qθ(t)dt = Et∼qθ(t)

(cid:105)
(φθ(t))

,

(1)

(cid:104)
L

where the distribution qθ(t) on t

[0, 1] is deﬁned as: qθ(t) =

∈

φ(cid:48)
dt
θ(t)
numerator of (1) is the line integral of the loss
0 (cid:107)
(cid:107)
L
is the normalizing constant of the uniform distribution on the curve deﬁned by φθ(
). Stochastic
·
gradients of ˆ(cid:96)(θ) in Eq. (1) are generally intractable since qθ(t) depends on θ. Therefore we also
propose a more computationally tractable loss

φ(cid:48)
θ(t)
(cid:107)
on the curve, and the denominator (cid:82) 1

φ(cid:48)
θ(t)
(cid:107)

(cid:107) ·

dt

(cid:18) 1
(cid:82)
0 (cid:107)

(cid:19)−1

. The

(cid:90) 1

(cid:96)(θ) =

(φθ(t))dt = Et∼U (0,1)L

0 L

(φθ(t)),

(2)

L

(φθ(t)) with respect to a uniform distribution on t

where U (0, 1) is the uniform distribution on [0, 1]. The difference between (1) and (2) is that the
[0, 1], while
latter is an expectation of the loss
(1) is an expectation with respect to a uniform distribution on the curve. The two losses coincide,
for example, when φθ(
) deﬁnes a polygonal chain with two line segments of equal length and the
·
parametrization of each of the two segments is linear in t.
To minimize (2), at each iteration we sample ˜t from the uniform distribution U (0, 1) and make a
(φθ(˜t)). This way we obtain unbiased estimates of the
gradient step for θ with respect to the loss
gradients of (cid:96)(θ), as

L

∈

(φθ(˜t))

θ

Et∼U (0,1)∇

θ

(φθ(t)) =

θEt∼U (0,1)L

∇

(φθ(t)) =

θ(cid:96)(θ).

∇

L
We repeat these updates until convergence.

∇

(cid:39)

L

3.2 Example Parametrizations

Polygonal chain The simplest parametric curve we consider is the polygonal chain (see Figure 1,
right). The trained networks ˆw1 and ˆw2 serve as the endpoints of the chain and the bends of the chain
are the parameters θ of the curve parametrization. Consider the simplest case of a chain with one
bend θ. Then

φθ(t) =

(cid:26) 2 (tθ + (0.5

t) ˆw1) ,

0

−
0.5) ˆw2 + (1

≤
t)θ) , 0.5

−

t

0.5
1.

≤
t

≤

≤

2 ((t

−

Bezier curve A Bezier curve (see Figure 1, middle) provides a convenient parametrization of
smooth paths with given endpoints. A quadratic Bezier curve φθ(t) with endpoints ˆw1 and ˆw2 is
given by

φθ(t) = (1

t)2 ˆw1 + 2t(1

t)θ + t2 ˆw2, 0

t

1.

−
These formulas naturally generalize for n bends θ =

−

≤
w1, w2, . . . , wn
{

}

≤
(see supplement).

4 Curve Finding Experiments

We show that the proposed training procedure in Section 3 does indeed ﬁnd high accuracy paths
connecting different modes, across a range of architectures and datasets. Moreover, we further
investigate the properties of these curves, showing that they correspond to meaningfully different
representations that can be ensembled for improved accuracy. We use these insights to propose an
improved ensembling procedure in Section 5, which we empirically validate in Section 6.

In particular, we test VGG-16 [24], a 28-layer Wide ResNet with widening factor 10 [27] and a
158-layer ResNet [11] on CIFAR-10, and VGG-16, 164-layer ResNet-bottleneck [11] on CIFAR-100.
For CIFAR-10 and CIFAR-100 we use the same standard data augmentation as Huang et al. [13]. We

4

Figure 2: The (cid:96)2-regularized cross-entropy train loss (left) and test error (middle) as a function of
the point on the curves φθ(t) found by the proposed method (ResNet-164 on CIFAR-100). Right:
Error of the two-network ensemble consisting of the endpoint φθ(0) of the curve and the point φθ(t)
on the curve (CIFAR-100, ResNet-164). “Segment” is a line segment connecting two modes found
by SGD. “Polychain” is a polygonal chain connecting the same endpoints.

provide additional results, including detailed experiments for fully connected and recurrent networks,
in the supplement.

For each model and dataset we train two networks with different random initializations to ﬁnd two
modes. Then we use the proposed algorithm of Section 3 to ﬁnd a path connecting these two modes
in the weight space with a quadratic Bezier curve and a polygonal chain with one bend. We also
connect the two modes with a line segment for comparison. In all experiments we optimize the loss
(2), as for Bezier curves the gradient of loss (1) is intractable, and for polygonal chains we found loss
(2) to be more stable.

Figures 1 and 2 show the results of the proposed mode connecting procedure for ResNet-164 on
CIFAR-100. Here loss refers to (cid:96)2-regularized cross-entropy loss. For both the Bezier curve and
polygonal chain, train loss (Figure 2, left) and test error (Figure 2, middle) are indeed nearly constant.
In addition, we provide plots of train error and test loss in the supplementary material. In the
supplement, we also include a comprehensive table summarizing all path ﬁnding experiments on
CIFAR-10 and CIFAR-100 for VGGs, ResNets and Wide ResNets, as well as fully connected
networks and recurrent neural networks, which follow the same general trends. In the supplementary
material we also show that the connecting curves can be found consistently as we vary the number of
parameters in the network, although the ratio of the arclength for the curves to the length of the line
segment connecting the same endpoints decreases with increasing parametrization. In the supplement,
we also measure the losses (1) and (2) for all the curves we constructed, and ﬁnd that the values of
the two losses are very close, suggesting that the loss (2) is a good practical approximation to the loss
(1).

The constant-error curves connecting two given networks discovered by the proposed method are not
unique. We trained two different polygonal chains with the same endpoints and different random
seeds using VGG-16 on CIFAR-10. We then measured the Euclidean distance between the turning
points of these curves. For VGG-16 on CIFAR-10 this distance is equal to 29.6 and the distance
between the endpoints is 50, showing that the curves are not unique. In this instance, we expect the
distance between turning points to be less than the distance between endpoints, since the locations of
the turning points were initialized to the same value (the center of the line segment connecting the
endpoints).

Although high accuracy connecting curves can often be very simple, such as a polygonal chain with
only one bend, we note that line segments directly connecting two modes generally incur high error.
For VGG-16 on CIFAR-10 the test error goes up to 90% in the center of the segment. For ResNet-158
and Wide ResNet-28-10 the worst errors along direct line segments are still high, but relatively less,
at 80% and 66%, respectively. This ﬁnding suggests that the loss surfaces of state-of-the-art residual
networks are indeed more regular than those of classical models like VGG, in accordance with the
observations in Li et al. [20].

In this paper we focus on connecting pairs of networks trained using the same hyper-parameters,
but from different random initializations. Building upon our work, Gotmare et al. [9] have recently
shown that our mode connectivity approach applies to pairs of networks trained with different batch
sizes, optimizers, data augmentation strategies, weight decays and learning rate schemes.

To motivate the ensembling procedure proposed in the next section, we now examine how far we
need to move along a connecting curve to ﬁnd a point that produces substantially different, but still

5

useful, predictions. Let ˆw1 and ˆw2 be two distinct sets of weights corresponding to optima obtained
by independently training a DNN two times. We have shown that there exists a path connecting
ˆw1 and ˆw2 with high test accuracy. Let φθ(t), t
[0, 1] parametrize this path with φθ(0) = ˆw1,
∈
φθ(1) = ˆw2. We investigate the performance of an ensemble of two networks: the endpoint φθ(0)
of the curve and a point φθ(t) on the curve corresponding to t
[0, 1]. Figure 2 (right) shows the
test error of this ensemble as a function of t, for a ResNet-164 on CIFAR-100. The test error starts
decreasing at t
0.4 the error of an ensemble is already as low as the error of
an ensemble of the two independently trained networks used as the endpoints of the curve. Thus
even by moving away from the endpoint by a relatively small distance along the curve we can ﬁnd
a network that produces meaningfully different predictions from the network at the endpoint. This
result also demonstrates that these curves do not exist only due to degenerate parametrizations of the
network (such as rescaling on either side of a ReLU); instead, points along the curve correspond to
meaningfully different representations of the data that can be ensembled for improved performance.
In the supplementary material we show how to create trivially connecting curves that do not have this
property.

0.1 and for t

≈

≥

∈

5 Fast Geometric Ensembling

In this section, we introduce a practical ensembling procedure, Fast Geometric Ensembling (FGE),
motivated by our observations about mode connectivity.

Figure 3: Left: Plot of the learning rate (Top), test error (Middle) and distance from the initial value
ˆw (Bottom) as a function of iteration for FGE with Preactivation-ResNet-164 on CIFAR-100. Circles
indicate the times when we save models for ensembling. Right: Ensemble performance of FGE
and SSE (Snapshot Ensembles) as a function of training time, using ResNet-164 on CIFAR-100
(B = 150 epochs). Crosses represent the performance of separate “snapshot” models, and diamonds
show the performance of the ensembles constructed of all models available by the given time.

In the previous section, we considered ensembling along mode connecting curves. Suppose now we
instead only have one set of weights ˆw corresponding to a mode of the loss. We cannot explicitly
) as before, but we know that multiple paths passing through ˆw exist, and it is
construct a path φθ(
·
thus possible to move away from ˆw in the weight space without increasing the loss. Further, we know
that we can ﬁnd diverse networks providing meaningfully different predictions by making relatively
small steps in the weight space (see Figure 2, right).

Inspired by these observations, we propose the Fast Geometric Ensembling (FGE) method that aims
to ﬁnd diverse networks with relatively small steps in the weight space, without leaving a region that
corresponds to low test error.

While inspired by mode connectivity, FGE does not rely on explicitly ﬁnding a connecting curve,
and thus does not require pre-trained endpoints, and so can be trained in the time required to train a
single network.

Let us describe Fast Geometric Ensembling. First, we initialize a copy of the network with weights w
set equal to the weights of the trained network ˆw. Now, to force w to move away from ˆw without
) (see
substantially decreasing the prediction accuracy we adopt a cyclical learning rate schedule α(
·
Figure 3, left), with the learning rate at iteration i = 1, 2, . . . deﬁned as

α(i) =

(cid:26) (1
(2

−
−

2t(i))α1 + 2t(i)α2
2t(i))α2 + (2t(i)

0 < t(i)
1
2 < t(i)

1
2
1

≤
≤

,

1)α1

−

6

−

c (mod(i

where t(i) = 1
1, c) + 1), the learning rates are α1 > α2, and the number of iterations
in one cycle is given by even number c. Here by iteration we mean processing one mini-batch of
data. We can train the network w using the standard (cid:96)2-regularized cross-entropy loss function (or
any other loss that can be used for DNN training) with the proposed learning rate schedule for n
iterations. In the middle of each learning rate cycle when the learning rate reaches its minimum value
α(i) = α2 (which corresponds to mod(i
2 ) we collect the checkpoints
of weights w. When the training is ﬁnished we ensemble the collected models. An outline of the
algorithm is provided in the supplement.

1, c) + 1 = c/2, t(i) = 1

−

Figure 3 (left) illustrates the adopted learning rate schedule. During the periods when the learning
rate is large (close to α1), w is exploring the weight space doing larger steps but sacriﬁcing the test
error. When the learning rate is small (close to α2), w is in the exploitation phase in which the steps
become smaller and the test error goes down. The cycle length is usually about 2 to 4 epochs, so that
the method efﬁciently balances exploration and exploitation with relatively-small steps in the weight
space that are still sufﬁcient to gather diverse and meaningful networks for the ensemble.

To ﬁnd a good initialization ˆw for the proposed procedure, we ﬁrst train the network with the standard
learning rate schedule (the schedule used to train single DNN models) for about 80% of the time
required to train a single model. After this pre-training is ﬁnished we initialize FGE with ˆw and run
the proposed fast ensembling algorithm for the remaining computational budget. In order to get more
diverse samples, one can run the algorithm described above several times for a smaller number of
iterations initializing from different checkpoints saved during training of ˆw, and then ensemble all of
the models gathered across these runs.

Cyclical learning rates have also recently been considered in Smith and Topin [25] and Huang et al.
[13]. Our proposed method is perhaps most closely related to Snapshot Ensembles [13], but has
several distinctive features, inspired by our geometric insights. In particular, Snapshot Ensembles
adopt cyclical learning rates with cycle length on the scale of 20 to 40 epochs from the beginning
of the training as they are trying to do large steps in the weight space. However, according to our
analysis of the curves it is sufﬁcient to do relatively small steps in the weight space to get diverse
networks, so we only employ cyclical learning rates with a small cycle length on the scale of 2 to 4
epochs in the last stage of the training. As illustrated in Figure 3 (left), the step sizes made by FGE
between saving two models (that is the euclidean distance between sets of weights of corresponding
models in the weight space) are on the scale of 7 for Preactivation-ResNet-164 on CIFAR-100. For
Snapshot Ensembles for the same model the distance between two snapshots is on the scale of 40. We
also use a piecewise linear cyclical learning rate schedule following Smith and Topin [25] as opposed
to the cosine schedule in Snapshot Ensembles.

6 Fast Geometric Ensembling Experiments

Table 1: Error rates (%) on CIFAR-100 and CIFAR-10 datasets for different ensembling techniques
and training budgets. The best results for each dataset, architecture, and budget are bolded.

CIFAR-100

CIFAR-10

DNN (Budget)

method

1B

2B

3B

1B

VGG-16 (200)

ResNet-164 (150)

WRN-28-10 (200)

Ind
SSE
FGE

Ind
SSE
FGE

Ind
SSE
FGE

27.4
26.4
25.7

21.5
20.9
20.2

19.2
17.9
17.7

0.1
0.1
0.1

0.4
0.2
0.1

0.2
0.2
0.2

±
±
±

±
±
±

±
±
±

25.28
25.16
24.11

19.04
19.28
18.67

17.48
17.3
16.95

24.45
24.69
23.54

18.59
18.91
18.21

17.01
16.97
16.88

6.75
6.57
6.48

4.72
4.66
4.54

3.82
3.73
3.65

0.16
0.12
0.09

0.1
0.02
0.05

0.1
0.04
0.1

±
±
±

±
±
±

±
±
±

2B

5.89
6.19
5.82

4.1
4.37
4.21

3.4
3.54
3.38

3B

5.9
5.95
5.66

3.77
4.3
3.98

3.31
3.55
3.52

In this section we compare the proposed Fast Geometric Ensembling (FGE) technique against
ensembles of independently trained networks (Ind), and SnapShot Ensembles (SSE) [13], a recent
state-of-the-art fast ensembling approach.

7

For the ensembling experiments we use a 164-layer Preactivation-ResNet in addition to the VGG-16
and Wide ResNet-28-10 models. Links for implementations to these models can be found in the
supplement.

We compare the accuracy of each method as a function of computational budget. For each network
architecture and dataset we denote the number of epochs required to train a single model as B. For
a kB budget, we run each of Ind, FGE and SSE k times from random initializations and ensemble
the models gathered from the k runs. In our experiments we set B = 200 for VGG-16 and Wide
ResNet-28-10 (WRN-28-10) models, and B = 150 for ResNet-164, since 150 epochs is typically
sufﬁcient to train this model. We note the runtime per epoch for FGE, SSE, and Ind is the same, and
so the total computation associated with kB budgets is the same for all ensembling approaches.

For Ind, we use an initial learning rate of 0.1 for ResNet and Wide ResNet, and 0.05 for VGG. For
FGE, with VGG we use cycle length c = 2 epochs, and a total of 22 models in the ﬁnal ensemble.
With ResNet and Wide ResNet we use c = 4 epochs, and the total number of models in the ﬁnal
ensemble is 12 for Wide ResNets and 6 for ResNets. For VGG we set the learning rates to α1 = 10−2,
10−4. . For SSE,
α2 = 5
we followed Huang et al. [13] and varied the initial learning rate α0 and number of snapshots per
run M . We report the best results we achieved, which corresponded to α0 = 0.1, M = 4 for ResNet,
α0 = 0.1, M = 5 for Wide ResNet, and α0 = 0.05, M = 5 for VGG. The total number of models in
the FGE ensemble is constrained by network choice and computational budget. Further experimental
details are in the supplement.

10−4; for ResNet and Wide ResNet models we set α1 = 5

10−2, α2 = 5

·

·

·

Table 1 summarizes the results of the experiments. In all conducted experiments FGE outperforms
SSE, particularly as we increase the computational budget. The performance improvement against
Ind is most noticeable for CIFAR-100. With a large number of classes, any two models are less likely
to make the same predictions. Moreover, there will be greater uncertainty over which representation
one should use on CIFAR-100, since the number of classes is increased tenfold from CIFAR-10, but
the number of training examples is held constant. Thus smart ensembling strategies will be especially
important on this dataset. Indeed in all experiments on CIFAR-100, FGE outperformed all other
methods. On CIFAR-10, FGE consistently improved upon SSE for all budgets and architectures.
FGE also improved against Ind for all training budgets with VGG, but is more similar in performance
to Ind on CIFAR-10 when using ResNets.

Figure 3 (right) illustrates the results for Preactivation-ResNet-164 on CIFAR-100 for one and two
training budgets. The training budget B is 150 epochs. Snapshot Ensembles use a cyclical learning
rate from the beginning of the training and they gather the models for the ensemble throughout
training. To ﬁnd a good initialization we run standard independent training for the ﬁrst 125 epochs
before applying FGE. In this case, the whole ensemble is gathered over the following 22 epochs
(126-147) to ﬁt in the budget of each of the two runs. During these 22 epochs FGE is able to gather
diverse enough networks to outperform Snapshot Ensembles both for 1B and 2B budgets.

Diversity of predictions of the individual networks is crucial for the ensembling performance [e.g.,
19]. We note that the diversity of the networks averaged by FGE is lower than that of completely
independently trained networks. Speciﬁcally, two independently trained ResNet-164 on CIFAR-100
make different predictions on 19.97% of test objects, while two networks from the same FGE run
make different predictions on 14.57% of test objects. Further, performance of individual networks
averaged by FGE is slightly lower than that of fully trained networks (e.g. 78.0% against 78.5% on
CIFAR100 for ResNet-164). However, for a given computational budget FGE can propose many
more high-performing networks than independent training, leading to better ensembling performance
(see Table 1).

6.1

ImageNet

ImageNet ILSVRC-2012 [23] is a large-scale dataset containing 1.2 million training images and
50000 validation images divided into 1000 classes.

CIFAR-100 is the primary focus of our ensemble experiments. However, we also include ImageNet
results for the proposed FGE procedure, using a ResNet-50 architecture. We used a pretrained model
with top-1 test error of 23.87 to initialize the FGE procedure. We then ran FGE for 5 epochs with a
cycle length of 2 epochs and with learning rates α1 = 10−3, α2 = 10−5. The top-1 test error-rate of
the ﬁnal ensemble was 23.31. Thus, in just 5 epochs we could improve the accuracy of the model by

8

0.56 using FGE. The ﬁnal ensemble contains 4 models (including the pretrained one). Despite the
harder setting of only 5 epochs to construct an ensemble, FGE performs comparably to the best result
reported by Huang et al. [13] on ImageNet, 23.33 error, which was also achieved using a ResNet-50.

7 Discussion and Future Work

We have shown that the optima of deep neural networks are connected by simple pathways, such
as a polygonal chain with a single bend, with near constant accuracy. We introduced a training
procedure to ﬁnd these pathways, with a user-speciﬁc curve of choice. We were inspired by these
insights to propose a practical new ensembling approach, Fast Geometric Ensembling, which achieves
state-of-the-art results on CIFAR-10, CIFAR-100, and ImageNet.

There are so many exciting future directions for this research. At a high level we have shown that
even though the loss surfaces of deep neural networks are very complex, there is relatively simple
structure connecting different optima. Indeed, we can now move towards thinking about valleys of
low loss, rather than isolated modes.

These valleys could inspire new directions for approximate Bayesian inference, such as stochastic
MCMC approaches which could now jump along these bridges between modes, rather than getting
stuck exploring a single mode. One could similarly derive new proposal distributions for variational
inference, exploiting the ﬂatness of these pathways. These geometric insights could also be used to
accelerate the convergence, stability and accuracy of optimization procedures like SGD, by helping
us understand the trajectories along which the optimizer moves, and making it possible to develop
procedures which can now search in more structured spaces of high accuracy. One could also use
these paths to construct methods which are more robust to adversarial attacks, by using an arbitrary
collection of diverse models described by a high accuracy curve, returning the predictions of a
different model for each query from an adversary. We can also use this new property to create better
visualizations of DNN loss surfaces. Indeed, using the proposed training procedure, we were able to
produce new types of visualizations showing the connectivity of modes, which are normally depicted
as isolated. We also could continue to build on the new training procedure we proposed here, to ﬁnd
curves with particularly desirable properties, such as diversity of networks. Indeed, we could start to
use entirely new loss functions, such as line and surface integrals of cross-entropy across structured
regions of weight space.

Acknowledgements. Timur Garipov was supported by Ministry of Education and Science of the
Russian Federation (grant 14.756.31.0001). Timur Garipov and Dmitrii Podoprikhin were supported
by Samsung Research, Samsung Electronics. Andrew Gordon Wilson and Pavel Izmailov were
supported by Facebook Research and NSF IIS-1563887.

References

[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale
machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467,
2016.

[2] Peter Auer, Mark Herbster, and Manfred K Warmuth. Exponentially many local minima for
single neurons. In Advances in Neural Information Processing Systems, pages 316–322, 1996.

[3] Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics, pages 192–204,
2015.

[4] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In Advances in Neural Information Processing Systems, pages 2933–2941,
2014.

[5] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International

9

Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pages 1019–1028, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
URL http://proceedings.mlr.press/v70/dinh17b.html.

[6] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no
barriers in neural network energy landscape. In Jennifer Dy and Andreas Krause, editors, Pro-
ceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 1309–1318, Stockholmsmässan, Stockholm Sweden,
10–15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/draxler18a.html.

[7] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectiﬁed network optimiza-

tion. International Conference on Learning Representations, 2017.

[8] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural
network optimization problems. International Conference on Learning Representations, 2015.

[9] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Using mode

connectivity for loss landscape analysis. arXiv preprint arXiv:1806.06977, 2018.

[10] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pages 1321–1330, 2017.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[12] Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

[13] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.
Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representa-
tions, 2017.

[14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning, pages
448–456, 2015.

[15] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018.

[16] Hannes Jonsson, Greg Mills, and Karsten W Jacobsen. Nudged elastic band method for ﬁnding
minimum energy paths of transitions. Classical and quantum dynamics in condensed phase
simulations, 1998.

[17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.
International Conference on Learning Representations, 2017.

[18] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on Learning Theory, pages 1246–1257, 2016.

[19] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael Cogswell, Viresh Ranjan, David
Crandall, and Dhruv Batra. Stochastic multiple choice learning for training diverse deep
ensembles. In Advances in Neural Information Processing Systems, pages 2119–2127, 2016.

[20] Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural

nets. arXiv preprint arXiv:1712.09913, 2017.

[21] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts.

International Conference on Learning Representations, 2017.

[22] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.

10

[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2012.

[24] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556, 2014.

[25] Leslie N Smith and Nicholay Topin. Exploring loss function topology with cyclical learning

rates. arXiv preprint arXiv:1702.04283, 2017.

[26] Jingjing Xie, Bing Xu, and Zhang Chuang. Horizontal and vertical ensemble with deep

representation for classiﬁcation. arXiv preprint arXiv:1306.2759, 2013.

[27] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.

A Supplementary Material

We organize the supplementary material as follows. Section A.1 discusses the computational com-
plexity of the proposed curve ﬁnding method. Section A.2 describes how to apply batch normalization
at test time to points on curves connecting pairs of local optima. Section A.3 provides formulas
for a polygonal chain and Bezier curve with n bends. Section A.4 provides details and results of
experiments on curve ﬁnding and contains a table summarizing all path ﬁnding experiments. Section
A.5 provides additional visualizations of the train loss and test accuracy surfaces. Section A.6 contains
details on curve ensembling experiments. Section A.7 describes experiments on relation between
mode connectivity and the number of parameters in the networks. Section A.8 discusses a trivial
construction of curves connecting two modes, where points on the curve represent reparameterization
of the endpoints, unlike the curves in the main text. Section A.9 provides details of experiments on
FGE. Finally, Section A.10 describes pathways traversed by FGE.

A.1 Computational complexity of curve ﬁnding

The forward pass of the proposed method consists of two steps: computing the point φθ(t) and then
passing a mini-batch of data through the DNN corresponding to this point. Similarly, the backward
pass consists of ﬁrst computing the gradient of the loss with respect to φθ(t), and then multiplying the
result by the Jacobian ∂φθ
∂θ . The second step of the forward pass and the ﬁrst step of the backward pass
are exactly the same as the forward and backward pass in the training of a single DNN model. The
additional computational complexity of the procedure compared to single model training comes from
the ﬁrst step of the forward pass and the second step of the backward pass and in general depends on
) of the curve.
the parametrization φθ(
·

In our experiments we use curve parametrizations of a speciﬁc form. The general formula for a curve
with one bend is given by

φθ(t) = ˆw1

c1(t) + θ

c(t) + ˆw2

c2(t).

·

·
R|net| and coefﬁcients c1, c2, c : [0, 1]

·

Here the parameters of the curve are given by θ

∈

For this family of curves the computational complexity of the ﬁrst step of the method is
we only need to compute a weighted sum of ˆw1, ˆw2 and θ

O
R|net|. The Jacobian matrix

R.

→
(
|

net

), as

|

), as we only need
thus the additional computational complexity of the backward pass is also
|
to multiply the gradient with respect to φθ(t) by a scalar. Thus, the total additional computational
). In practice we observe that the gap in time-complexity between one epoch
complexity is
|
of training a single model and one epoch of the proposed method with the same network architecture
is usually below 50%.

net
|

net

(
|

O

O

(

∈

∂φθ(t)
∂θ

= c(t)

I,

·

11

A.2 Batch Normalization

Batch normalization (Ioffe and Szegedy [14]) is essential to modern deep learning architectures.
Batch normalization re-parametrizes the output of each layer as

ˆx = γ

µ(x)
x
σ(x) + (cid:15)

−

+ β,

where µ(x) and σ(x) are the mean and standard deviation of the output x, (cid:15) > 0 is a constant for
numerical stability and γ and β are free parameters. During training, µ(x) and σ(x) are computed
separately for each mini-batch and at test time statistics aggregated during training are used.

When connecting two DNNs that use batch normalization, along a curve φ(t), we compute µ(x) and
σ(x) for any given t over mini-batches during training, as usual. In order to apply batch-normalization
to a network on the curve at the test stage we compute these statistics with one additional pass over
the data, as running averages for these networks are not collected during training.

A.3 Formulas for curves with n bends

For n bends θ =
{
w0, wn+1 is given by

w1, w2, . . . , wn

, the parametrization of a polygonal chain connecting points

φθ(t) = (n + 1)

i
n + 1

t

−

(cid:19)

·

wi+1 +

(cid:18) i + 1

(cid:19)

t

(cid:19)

wi

,

n + 1 −

·

}

(cid:18)(cid:18)

·

n.

t

for

i
n+1 ≤
For n bends θ =
{
wn+1 is given by

≤

i+1
n+1 and 0

i

≤

≤
w1, w2, . . . , wn

, the parametrization of a Bezier curve connecting points w0 and
}

φθ(t) =

wiC i

n+1ti(1

t)n+1−i

−

n+1
(cid:88)

i=0

A.4 Curve Finding Experiments

Figure 4: The (cid:96)2-regularized cross-entropy train loss (Top) and test error (Bottom) surfaces of a
deep residual network (ResNet-164) on CIFAR-100. Left: Three optima for independently trained
networks. Middle and Right: A quadratic Bezier curve, and a polygonal chain with one bend,
connecting the lower two optima on the left panel along a path of near-constant loss. Notice that in
each panel, a direct linear path between each mode would incur high loss.

All experiments on curve ﬁnding were conducted with TensorFlow (Abadi et al. [1]) and as baseline
models we used the following implementations:

ResNet-bottleneck-164 and Wide ResNet-28-10 (https://github.com/tensorflow/
models/tree/master/research/resnet);

•

12

Table 2: The properties of loss and error values along the found curves for different architectures and tasks

Model

Length

Train Loss

Train Error (%)

Test Error (%)

DNN

Curve

Ratio

Min

Int

Mean Max Min

Int

Max Min

Int

Max

FC
FC
FC
FC

3conv3fc
3conv3fc
3conv3fc
3conv3fc

VGG-16
VGG-16
VGG-16
VGG-16

ResNet-158
ResNet-158
ResNet-158
ResNet-158

Single
Segment
Bezier
Polychain

Single
Segment
Bezier
Polychain

Single
Segment
Bezier
Polychain

Single
Segment
Bezier
Polychain

Single
Segment

WRN-10-28
WRN-10-28
WRN-10-28 Bezier
WRN-10-28

Polychain

VGG-16
VGG-16
VGG-16
VGG-16

ResNet-164
ResNet-164
ResNet-164
ResNet-164

Single
Segment
Bezier
Polychain

Single
Segment
Bezier
Polychain

—
1
1.58
1.73

—
1
1.30
1.67

—
1
1.55
1.83

—
1
2.13
3.48

—
1
1.83
1.95

—
1
1.52
1.64

—
1
1.87
2.56

0.018 —
0.018
0.016
0.013

0.252
0.02
0.022

0.05 —
0.05
0.034
0.04

1.124
0.038
0.044

0.04 —
0.039
0.028
0.025

1.759
0.03
0.031

0.015 —
0.013
0.013
0.013

0.551
0.017
0.017

0.033 —
0.033
0.03
0.026

0.412
0.033
0.029

0.14 —
0.137
0.095
0.118

3.606
0.107
0.139

0.079 —
0.076
0.074
0.067

1.844
0.083
0.078

MNIST

CIFAR-10

—
0.252
0.02
0.022

—
1.124
0.037
0.044

—
1.759
0.03
0.031

—
0.551
0.018
0.017

—
0.412
0.038
0.029

—
3.606
0.105
0.139

—
1.844
0.084
0.078

0.018
0.657
0.024
0.029

0.05
2.416
0.05
0.05

0.04
2.569
0.04
0.045

0.015
2.613
0.022
0.047

0.035
2.203
0.038
0.037

0.141
4.941
0.141
0.2

0.08
5.53
0.098
0.109

0.01 —
0.01
0.01
0

0.53
0.02
0.03

0.06 —
0.06
0.05
0.06

35.69
0.1
0.15

0
0
0
0

0
0
0
0

0.02 —
0
0
0

16.37
0.02
0.05

—
61.43
0.01
0.01

—
5.44
0.01
0

0.05 —
0.04
0.03
0.04

73.25
0.08
0.19

0.06 —
0.06
0.05
0.06

38.03
0.28
0.28

0.01
2.13
0.04
0.07

0.06
88.24
0.2
0.31

0.01
90
0.02
0.04

0.02
81.41
0.07
0.139

0
65.62
0.04
0

0.06
99
0.18
0.39

0.09
98.65
0.96
0.85

1.46 —
1.45
1.46
1.46

1.96
1.52
1.51

12.3 —
12.28
12.06
12.17

43.3
12.7
12.68

6.87 —
6.87
6.59
6.54

63.75
6.77
6.89

5.56 —
5.57
5.48
5.48

20.79
5.82
5.88

4.49 —
4.49
4.4
4.38

10.55
4.62
6.93

29.44 —
29.44
29.28
29.33

80.59
30.49
30.13

24.41 —
24.4
24.15
23.98

53.69
24.99
24.92

1.5
3.18
1.56
1.58

12.36
88.27
13.66
13.31

7.01
90
7.01
7.28

5.74
80.00
6.24
7.35

4.56
66.6
4.83
10.38

29.94
99.01
31.23
30.92

24.4
98.83
26.1
26.12

CIFAR-100

Table 3: The value of perplexity along the found curves for PTB dataset

Model

Train

Validation

Test

DNN Curve

Min Max Min Max Min Max

RNN Single
RNN Segment
RNN Bezier

37.5
37.5
29.8

39.2
596.3
39.2

82.7
82.7
82.7

83.1
682.1
88.7

78.7
78.7
78.7

78.9
615.7
84.0

ResNet-158 (https://github.com/tensorflow/models/tree/master/official/
resnet);

A reimplementation of VGG-16 without batch-normalization from (https://github.
com/pytorch/vision/blob/master/torchvision/models/vgg.py);

•

•

PTB

13

Figure 5: Same as Fig. 4 for VGG-16 on CIFAR-10.

Table 2 summarizes the results of the curve ﬁnding experiments with all datasets and architectures.
For each of the models we report the properties of loss and the error on the train and test datasets.
For each of these metrics we report 3 values: “Max” is the maximum values of the metric along
the curve, “Int” is a numerical approximation of the integral (cid:82) <metric>(φθ)dφθ/(cid:82) dφθ, where
<metric> represents the train loss or the error on the train or test dataset and “Min” is the minimum
value of the error on the curve. “Int” represents a mean over a uniform distribution on the curve, and
for the train loss it coincides with the loss (1) in the paper. We use an equally-spaced grid with 121
points on [0, 1] to estimate the values of “Min”, “Max”, “Int”. For “Int” we use the trapezoidal rule to
estimate the integral. For each dataset and architecture we report the performance of single models
used as the endpoints of the curve as “Single”, the performance of a line segment connecting the
two single networks as “Segment”, the performance of a quadratic Bezier curve as “Bezier” and the
performance of a polygonal chain with one bend as “Polychain”. Finally, for each curve we report
the ratio of its length to the length of a line segment connecting the two modes.
We also examined the quantity “Mean” deﬁned as (cid:82) < metric > (φθ(t))dt, which coincides with
the loss (2) from the paper, but in all our experiments it is nearly equal to “Int”.

Besides convolutional and fully-connected architectures we also apply our approach to RNN archi-
tecture on next word prediction task, PTB dataset (Marcus et al. [22]). As a base model we used
the implementation available at https://www.tensorflow.org/tutorials/recurrent. As the
main loss we consider perplexity. The results are presented in Table 3.

A.5 Train loss and test accuracy surfaces

In this section we provide additional visualizations. Fig. 4 and Fig. 5 show visualizations of the train
loss and test accuracy for ResNet-164 on CIFAR-100 and VGG-16 on CIFAR-10.

A.6 Curve Ensembling

Here we explore ensembles constructed from points sampled from these high accuracy curves. In
particular, we train a polygonal chain with one bend connecting two independently trained ResNet-
164 networks on CIFAR-100 and construct an ensemble of networks corresponding to 50 points
placed on an equally-spaced grid on the curve. The resulting ensemble had 21.03% error-rate on
the test dataset. The error-rate of the ensemble constructed from the endpoints of the curve was
22.0%. An ensemble of three independently trained networks has an error rate of 21.01%. Thus, the
ensemble of the networks on the curve outperformed an ensemble of its endpoints implying that the
curves found by the proposed method are actually passing through diverse networks that produce
predictions different from those produced by the endpoints of the curve. Moreover, the ensemble
based on the polygonal chain has the same number of parameters as three independent networks, and
comparable performance.

Furthermore, we can improve the ensemble on the chain without adding additional parameters or
computational expense, by accounting for the pattern of increased training and test loss towards the

14

Figure 6: Error as a function of the point on the curves φθ(t) found by the proposed method, using a
ResNet-164 on CIFAR-100. Top left: train error. Bottom left: test error; dashed lines correspond to
quality of ensemble constructed from curve points before and after logits rescaling. Top right: train
loss ((cid:96)2 regularized cross-entropy). Bottom right: cross-entropy before and after logits rescaling for
the polygonal chain.

centres of the linear paths shown in Figure 6. While the training and test accuracy are relatively
constant, the pattern of loss, shared across train and test sets, indicates overconﬁdence away from
the three points deﬁning the curve: in this region, networks tend to output probabilities closer to 1,
sometimes with the wrong answers. This overconﬁdence decreases the performance of ensembles
constructed from the networks sampled on the curves. In order to correct for this overconﬁdence and
improve the ensembling performance we use temperature scaling [10], which is inversely proportional
to the loss. Figure 6, bottom right, illustrates the test loss of ResNet-164 on CIFAR-100 before and
after temperature scaling. After rescaling the predictions of the networks, the test loss along the curve
decreases and ﬂattens. Further, the test error-rate of the ensemble constructed from the points on the
curve went down from 21.03% to 20.7% after applying the temperature scaling, outperforming 3
independently trained networks.

However, directly ensembling on the curves requires manual intervention for temperature scaling,
and an additional pass over the training data for each of the networks (50 in this case) at test time to
perform batch normalization as described in section A.2. Moreover, we also need to train at least two
networks for the endpoints of the curve.

A.7 The Effects of Increasing Parametrization

Figure 7: The worst train loss along the curve, maximum of the losses of the endpoints, and the ratio
of the length of the curve and the line segment connecting the two modes, as a function of the scaling
factor K of the sizes of fully-connected layers.

One possible factor that inﬂuences the connectedness of a local minima set is the overparameterization
of neural networks. In this section, we investigate the relation between the observed connectedness
of the local optima and the number of parameters (weights) in the neural network. We start with a
network that has three convolutional layers followed by three fully-connected layers, where each
layer has 1000K neurons. We vary K
, and for each value of K we train two
0.3, 0.5, 0.8, 1
}
networks that we connect with a Bezier curve using the proposed procedure.

∈ {

15

Algorithm 1 Fast Geometric Ensembling

Require:

weights ˆw, LR bounds α1, α2,
cycle length c (even), number of iterations n

Ensure: ensemble

←

ˆw {Initialize weight with ˆw}

w
←
ensemble
for i
α
α
w
if mod(i, c) = c/2 then

[ ]
1, 2, . . . , n do
α(i) {Calculate LR for the iteration}
w

∇L

−

←
←
←
ensemble

i(w) {Stochastic gradient update}

ensemble + [w] {Collect weights}

end if
end for

←

For each value of K, Figure 7 shows the worst training loss along the curve, maximum of losses
of the endpoints, and the ratio of the length of the curve and the line segment connecting the two
modes. Increasing the number of parameters we are able to reduce the difference between the worst
value of the loss along the curve and the loss of single models used as the endpoints. The ratio of
the length of the found curve and the length of the line segment connecting the two modes also
decreases monotonically with K. This result is intuitive, since a greater parametrization allows for
more ﬂexibility in how we can navigate the loss surfaces.

A.8 Trivial connecting curves

For convolutional networks with ReLU activations and without batch normalization we can construct
a path connecting two points in weight space such that the accuracy of each point on the curve
(excluding the origin of the weight space) is at least as good as the minimum of the accuracies of
the endpoints. Unlike the paths found by our procedure, these paths are trivial and merely exploit
redundancies in the parametrization. Also, the training loss goes up substantially along these curves.
Below we give a construction of such paths.

Let ˆw1 and ˆw2 be two sets of weights. This path of interest consists of two parts. The ﬁrst part
connects the point ˆw1 with 0 and the second one connects the point ˆw2 with 0. We describe only the
ﬁrst part φ(t) of the path, such that φ(0) = 0, φ(1) = ˆw1, as the second part is completely analogous.
Wi, bi
Let the weights of the network ˆw1 be
1≤i≤n where Wi, bi are the weights and biases of the
{
i-th layer, and n is the total number of layers. Throughout the derivation we consider the inputs of
the network ﬁxed. The output of the i-th layer oi = WiReLU(oi−1) + bi, 1
n, where i = 0
corresponds to the ﬁrst layer and i = n corresponds to logits (the outputs of the last layer). We
1≤i≤n in the following way. We set Wi(t) = Wit and bi(t) = biti.
construct φ(t) =
It is easy to see that logits of the network with weights φ(t) are equal to on(t) = tnon for all t > 0.
Note that the predicted labels corresponding to the logits on(t) and on are the same, so the accuracy
of all networks corresponding to t > 0 is the same.

Wi(t), bi(t)
}
{

≤

≤

}

i

A.9 Fast geometric ensembling experiments

Alg. 1 provides an outline of the algorithm. As baseline models we used the following implementa-
tions:

VGG-16
models/vgg.py);

(https://github.com/pytorch/vision/blob/master/torchvision/

Preactivation-ResNet-164
pytorch-classification/blob/master/models/cifar/preresnet.py);

(https://github.com/bearpaw/

ImageNet
ResNet-50
torchvision/models/resnet.py);

(https://github.com/pytorch/vision/blob/master/

Wide ResNet-28-10
blob/master/networks/wide_resnet.py);

(https://github.com/meliketoy/wide-resnet.pytorch/

•

•

•

•

16

For the FGE (Fast Geometric Ensembling) strategy on ResNet we run the FGE routine summarized
in Alg. 1 after epoch 125 of the usual (same as Ind) training for 22 epochs. The total training
time is thus 125 + 22 = 147 epochs. For VGG and Wide ResNet models we run the pre-training
procedure for 156 epochs to initialize FGE. Then we run FGE for 22 epochs starting from checkpoints
corresponding to epochs 120 and 156 and ensemble all the gathered models. The total training time
is thus 156 + 22 + 22 = 200 epochs. For VGG we use cycle length c = 2 epochs, which means
that the total number of models in the ﬁnal ensemble is 22. For ResNet and Wide ResNet we use
c = 4 epochs, and the total number of models in the ﬁnal ensemble is 12 for Wide ResNets and 6 for
ResNets.

A.10 Polygonal chain connecting FGE proposals

In order to better understand the trajectories followed by FGE we construct a polygonal chain
connecting the points that FGE ensembles. Suppose we run FGE for n learning rate cycles obtaining
n points w1, w2, . . . , wn in the weight space that correspond to the lowest values of the learning
rate. We then consider the polygonal chain consisting of the line segments connecting wi to wi+1 for
i = 1, . . . , n
1. We plot test accuracy and train error along this polygonal chain in Figure 8. We
observe that along this curve both train loss and test error remain low, agreeing with our intuition that
FGE follows the paths of low loss and error. Surprisingly, we ﬁnd that the points on the line segments
connecting the weights wi, wi+1 have lower train loss and test error than wi and wi+1. See Izmailov
et al. [15] for a detailed discussion of this phenomenon.

−

Figure 8: Train loss and test error along the polygonal chain connecting the sequence of points
ensembled in FGE. The plot is generated using PreResNet-164 on CIFAR 100. Circles indicate the
bends on the polygonal chain, i.e. the networks ensembled in FGE.

17

8
1
0
2
 
t
c
O
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
6
2
0
0
1
.
2
0
8
1
:
v
i
X
r
a

Loss Surfaces, Mode Connectivity,
and Fast Ensembling of DNNs

Timur Garipov∗1,2 Pavel Izmailov∗3 Dmitrii Podoprikhin∗4
Dmitry Vetrov5 Andrew Gordon Wilson3

1Samsung AI Center in Moscow, 2Skolkovo Institute of Science and Technology,
3Cornell University,
4Samsung-HSE Laboratory, National Research University Higher School of Economics,
5National Research University Higher School of Economics

Abstract

The loss functions of deep neural networks are complex and their geometric
properties are not well understood. We show that the optima of these complex
loss functions are in fact connected by simple curves over which training and test
accuracy are nearly constant. We introduce a training procedure to discover these
high-accuracy pathways between modes. Inspired by this new geometric insight,
we also propose a new ensembling method entitled Fast Geometric Ensembling
(FGE). Using FGE we can train high-performing ensembles in the time required to
train a single model. We achieve improved performance compared to the recent
state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.

1

Introduction

The loss surfaces of deep neural networks (DNNs) are highly non-convex and can depend on millions
of parameters. The geometric properties of these loss surfaces are not well understood. Even for
simple networks, the number of local optima and saddle points is large and can grow exponentially in
the number of parameters [2, 3, 4]. Moreover, the loss is high along a line segment connecting two
optima [e.g., 8, 17]. These two observations suggest that the local optima are isolated.

In this paper, we provide a new training procedure which can in fact ﬁnd paths of near-constant
accuracy between the modes of large deep neural networks. Furthermore, we show that for a wide
range of architectures we can ﬁnd these paths in the form of a simple polygonal chain of two line
segments. Consider, for example, Figure 1, which illustrates the ResNet-164 (cid:96)2-regularized cross-
entropy train loss on CIFAR-100, through three different planes. We form each two dimensional
plane by all afﬁne combinations of three weight vectors.1

The left panel shows a plane deﬁned by three independently trained networks. In this plane, all
optima are isolated, which corresponds to the standard intuition. However, the middle and right
panels show two different paths of near-constant loss between the modes in weight space, discovered
by our proposed training procedure. The endpoints of these paths are the two independently trained
DNNs corresponding to the two lower modes on the left panel.

∗ Equal contribution.
1Suppose we have three weight vectors w1, w2, w3. We set u = (w2 − w1), v = (w3 − w1) −
(cid:104)w3 − w1, w2 − w1(cid:105)/(cid:107)w2 − w1(cid:107)2 · (w2 − w1). Then the normalized vectors ˆu = u/(cid:107)u(cid:107), ˆv = v/(cid:107)v(cid:107)
form an orthonormal basis in the plane containing w1, w2, w3. To visualize the loss in this plane, we deﬁne a
Cartesian grid in the basis ˆu, ˆv and evaluate the networks corresponding to each of the points in the grid. A point
P with coordinates (x, y) in the plane would then be given by P = w1 + x · ˆu + y · ˆv.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: The (cid:96)2-regularized cross-entropy train loss surface of a ResNet-164 on CIFAR-100, as a
function of network weights in a two-dimensional subspace. In each panel, the horizontal axis is
ﬁxed and is attached to the optima of two independently trained networks. The vertical axis changes
between panels as we change planes (deﬁned in the main text). Left: Three optima for independently
trained networks. Middle and Right: A quadratic Bezier curve, and a polygonal chain with one bend,
connecting the lower two optima on the left panel along a path of near-constant loss. Notice that in
each panel a direct linear path between each mode would incur high loss.

We believe that this geometric discovery has major implications for research into multilayer networks,
including (1) improving the efﬁciency, reliability, and accuracy of training, (2) creating better
ensembles, and (3) deriving more effective posterior approximation families in Bayesian deep
learning. Indeed, in this paper we are inspired by this geometric insight to propose a new ensembling
procedure that can efﬁciently discover multiple high-performing but diverse deep neural networks.

In particular, our contributions include:

The discovery that the local optima for modern deep neural networks are connected by very
simple curves, such as a polygonal chain with only one bend.
A new method that ﬁnds such paths between two local optima, such that the train loss and
test error remain low along these paths.
Using the proposed method we demonstrate that such mode connectivity holds for a wide
range of modern deep neural networks, on key benchmarks such as CIFAR-100. We show
that these paths correspond to meaningfully different representations that can be efﬁciently
ensembled for increased accuracy.
Inspired by these observations, we propose Fast Geometric Ensembling (FGE), which
outperforms the recent state-of-the-art Snapshot Ensembles [13], on CIFAR-10 and CIFAR-
100, using powerful deep neural networks such as VGG-16, Wide ResNet-28-10, and
ResNet-164. On ImageNet we achieve 0.56% top-1 error-rate improvement for a pretrained
ResNet-50 model by running FGE for only 5 epochs.
We release the code for reproducing the results in this paper at
https://github.com/timgaripov/dnn-mode-connectivity

•

•

•

•

•

The rest of the paper is organized as follows. Section 2 discusses existing literature on DNN loss
geometry and ensembling techniques. Section 3 introduces the proposed method to ﬁnd the curves
with low train loss and test error between local optima, which we investigate empirically in Section 4.
Section 5 then introduces our proposed ensembling technique, FGE, which we empirically compare
to the alternatives in Section 6. Finally, in Section 7 we discuss connections to other ﬁelds and
directions for future work.

Note that we interleave two sections where we make methodological proposals (Sections 3, 5), with
two sections where we perform experiments (Sections 4, 6). Our key methodological proposal for
ensembling, FGE, is in Section 5.

2 Related Work

Despite the success of deep learning across many application domains, the loss surfaces of deep
neural networks are not well understood. These loss surfaces are an active area of research, which
falls into two distinct categories.

The ﬁrst category explores the local structure of minima found by SGD and its modiﬁcations.
Researchers typically distinguish sharp and wide local minima, which are respectively found by using

2

large and small mini-batch sizes during training. Hochreiter and Schmidhuber [12] and Keskar et al.
[17], for example, claim that ﬂat minima lead to strong generalization, while sharp minima deliver
poor results on the test dataset. However, recently Dinh et al. [5] argue that most existing notions
of ﬂatness cannot directly explain generalization. To better understand the local structure of DNN
loss minima, Li et al. [20] proposed a new visualization method for the loss surface near the minima
found by SGD. Applying the method for a variety of different architectures, they showed that the loss
surfaces of modern residual networks are seemingly smoother than those of VGG-like models.

The other major category of research considers global loss structure. One of the main questions
in this area is how neural networks are able to overcome poor local optima. Choromanska et al.
[3] investigated the link between the loss function of a simple fully-connected network and the
Hamiltonian of the spherical spin-glass model. Under strong simplifying assumptions they showed
that the values of the investigated loss function at local optima are within a well-deﬁned bound. In
other research, Lee et al. [18] showed that under mild conditions gradient descent almost surely
converges to a local minimizer and not a saddle point, starting from a random initialization.

In recent work Freeman and Bruna [7] theoretically show that local minima of a neural network
with one hidden layer and ReLU activations can be connected with a curve along which the loss
is upper-bounded by a constant that depends on the number of parameters of the network and the
“smoothness of the data”. Their theoretical results do not readily generalize to multilayer networks.
Using a dynamic programming approach they empirically construct a polygonal chain for a CNN
on MNIST and an RNN on PTB next word prediction. However, in more difﬁcult settings such as
AlexNet on CIFAR-10 their approach struggles to achieve even the modest test accuracy of 80%.
Moreover, they do not consider ensembling.

By contrast, we propose a much simpler training procedure that can ﬁnd near-constant accuracy
polygonal chains with only one bend between optima, even on a range of modern state-of-the-art
architectures. Inspired by properties of the loss function discovered by our procedure, we also propose
a new state-of-the-art ensembling method that can be trained in the time required to train a single
DNN, with compelling performance on many key benchmarks (e.g., 96.4% accuracy on CIFAR-10).

Xie et al. [26] proposed a related ensembling approach that gathers outputs of neural networks from
different epochs at the end of training to stabilize ﬁnal predictions. More recently, Huang et al. [13]
proposed snapshot ensembles, which use a cosine cyclical learning rate [21] to save “snapshots” of the
model during training at times when the learning rate achieves its minimum. In our experiments, we
compare our geometrically inspired approach to Huang et al. [13], showing improved performance.

Draxler et al. [6] simultaneously and independently discovered the existence of curves connecting
local optima in DNN loss landscapes. To ﬁnd these curves they used a different approach inspired by
the Nudged Elastic Band method [16] from quantum chemistry.

3 Finding Paths between Modes

We describe a new method to minimize the training error along a path that connects two points in the
space of DNN weights. Section 3.1 introduces this general procedure for arbitrary parametric curves,
and Section 3.2 describes polygonal chains and Bezier curves as two example parametrizations of
such curves. In the supplementary material, we discuss the computational complexity of the proposed
approach and how to apply batch normalization at test time to points on these curves. We note
that after curve ﬁnding experiments in Section 4, we make our key methodological proposal for
ensembling in Section 5.

3.1 Connection Procedure

Let ˆw1 and ˆw2 in R|net| be two sets of weights corresponding to two neural networks independently
trained by minimizing any user-speciﬁed loss
is the
R|net| be a continuous piecewise smooth
number of weights of the DNN. Moreover, let φθ : [0, 1]
parametric curve, with parameters θ, such that φθ(0) = ˆw1, φθ(1) = ˆw2.

(w), such as the cross-entropy loss. Here,

net

→

L

|

|

3

To ﬁnd a path of high accuracy between ˆw1 and ˆw2, we propose to ﬁnd the parameters θ that minimize
the expectation over a uniform distribution on the curve, ˆ(cid:96)(θ):

(cid:82)

ˆ(cid:96)(θ) =

(φθ)dφθ
L
(cid:82) dφθ

=

1
(cid:82)
0 L

(φθ(t))

φ(cid:48)
dt
θ(t)
(cid:107)
(cid:107)

1
(cid:82)
0 (cid:107)

φ(cid:48)
dt
θ(t)
(cid:107)

=

1
(cid:90)

0

L

(φθ(t))qθ(t)dt = Et∼qθ(t)

(cid:105)
(φθ(t))

,

(1)

(cid:104)
L

where the distribution qθ(t) on t

[0, 1] is deﬁned as: qθ(t) =

∈

φ(cid:48)
dt
θ(t)
numerator of (1) is the line integral of the loss
0 (cid:107)
(cid:107)
L
is the normalizing constant of the uniform distribution on the curve deﬁned by φθ(
). Stochastic
·
gradients of ˆ(cid:96)(θ) in Eq. (1) are generally intractable since qθ(t) depends on θ. Therefore we also
propose a more computationally tractable loss

φ(cid:48)
θ(t)
(cid:107)
on the curve, and the denominator (cid:82) 1

φ(cid:48)
θ(t)
(cid:107)

(cid:107) ·

dt

(cid:18) 1
(cid:82)
0 (cid:107)

(cid:19)−1

. The

(cid:90) 1

(cid:96)(θ) =

(φθ(t))dt = Et∼U (0,1)L

0 L

(φθ(t)),

(2)

L

(φθ(t)) with respect to a uniform distribution on t

where U (0, 1) is the uniform distribution on [0, 1]. The difference between (1) and (2) is that the
[0, 1], while
latter is an expectation of the loss
(1) is an expectation with respect to a uniform distribution on the curve. The two losses coincide,
for example, when φθ(
) deﬁnes a polygonal chain with two line segments of equal length and the
·
parametrization of each of the two segments is linear in t.
To minimize (2), at each iteration we sample ˜t from the uniform distribution U (0, 1) and make a
(φθ(˜t)). This way we obtain unbiased estimates of the
gradient step for θ with respect to the loss
gradients of (cid:96)(θ), as

L

∈

(φθ(˜t))

θ

Et∼U (0,1)∇

θ

(φθ(t)) =

θEt∼U (0,1)L

∇

(φθ(t)) =

θ(cid:96)(θ).

∇

L
We repeat these updates until convergence.

∇

(cid:39)

L

3.2 Example Parametrizations

Polygonal chain The simplest parametric curve we consider is the polygonal chain (see Figure 1,
right). The trained networks ˆw1 and ˆw2 serve as the endpoints of the chain and the bends of the chain
are the parameters θ of the curve parametrization. Consider the simplest case of a chain with one
bend θ. Then

φθ(t) =

(cid:26) 2 (tθ + (0.5

t) ˆw1) ,

0

−
0.5) ˆw2 + (1

≤
t)θ) , 0.5

−

t

0.5
1.

≤
t

≤

≤

2 ((t

−

Bezier curve A Bezier curve (see Figure 1, middle) provides a convenient parametrization of
smooth paths with given endpoints. A quadratic Bezier curve φθ(t) with endpoints ˆw1 and ˆw2 is
given by

φθ(t) = (1

t)2 ˆw1 + 2t(1

t)θ + t2 ˆw2, 0

t

1.

−
These formulas naturally generalize for n bends θ =

−

≤
w1, w2, . . . , wn
{

}

≤
(see supplement).

4 Curve Finding Experiments

We show that the proposed training procedure in Section 3 does indeed ﬁnd high accuracy paths
connecting different modes, across a range of architectures and datasets. Moreover, we further
investigate the properties of these curves, showing that they correspond to meaningfully different
representations that can be ensembled for improved accuracy. We use these insights to propose an
improved ensembling procedure in Section 5, which we empirically validate in Section 6.

In particular, we test VGG-16 [24], a 28-layer Wide ResNet with widening factor 10 [27] and a
158-layer ResNet [11] on CIFAR-10, and VGG-16, 164-layer ResNet-bottleneck [11] on CIFAR-100.
For CIFAR-10 and CIFAR-100 we use the same standard data augmentation as Huang et al. [13]. We

4

Figure 2: The (cid:96)2-regularized cross-entropy train loss (left) and test error (middle) as a function of
the point on the curves φθ(t) found by the proposed method (ResNet-164 on CIFAR-100). Right:
Error of the two-network ensemble consisting of the endpoint φθ(0) of the curve and the point φθ(t)
on the curve (CIFAR-100, ResNet-164). “Segment” is a line segment connecting two modes found
by SGD. “Polychain” is a polygonal chain connecting the same endpoints.

provide additional results, including detailed experiments for fully connected and recurrent networks,
in the supplement.

For each model and dataset we train two networks with different random initializations to ﬁnd two
modes. Then we use the proposed algorithm of Section 3 to ﬁnd a path connecting these two modes
in the weight space with a quadratic Bezier curve and a polygonal chain with one bend. We also
connect the two modes with a line segment for comparison. In all experiments we optimize the loss
(2), as for Bezier curves the gradient of loss (1) is intractable, and for polygonal chains we found loss
(2) to be more stable.

Figures 1 and 2 show the results of the proposed mode connecting procedure for ResNet-164 on
CIFAR-100. Here loss refers to (cid:96)2-regularized cross-entropy loss. For both the Bezier curve and
polygonal chain, train loss (Figure 2, left) and test error (Figure 2, middle) are indeed nearly constant.
In addition, we provide plots of train error and test loss in the supplementary material. In the
supplement, we also include a comprehensive table summarizing all path ﬁnding experiments on
CIFAR-10 and CIFAR-100 for VGGs, ResNets and Wide ResNets, as well as fully connected
networks and recurrent neural networks, which follow the same general trends. In the supplementary
material we also show that the connecting curves can be found consistently as we vary the number of
parameters in the network, although the ratio of the arclength for the curves to the length of the line
segment connecting the same endpoints decreases with increasing parametrization. In the supplement,
we also measure the losses (1) and (2) for all the curves we constructed, and ﬁnd that the values of
the two losses are very close, suggesting that the loss (2) is a good practical approximation to the loss
(1).

The constant-error curves connecting two given networks discovered by the proposed method are not
unique. We trained two different polygonal chains with the same endpoints and different random
seeds using VGG-16 on CIFAR-10. We then measured the Euclidean distance between the turning
points of these curves. For VGG-16 on CIFAR-10 this distance is equal to 29.6 and the distance
between the endpoints is 50, showing that the curves are not unique. In this instance, we expect the
distance between turning points to be less than the distance between endpoints, since the locations of
the turning points were initialized to the same value (the center of the line segment connecting the
endpoints).

Although high accuracy connecting curves can often be very simple, such as a polygonal chain with
only one bend, we note that line segments directly connecting two modes generally incur high error.
For VGG-16 on CIFAR-10 the test error goes up to 90% in the center of the segment. For ResNet-158
and Wide ResNet-28-10 the worst errors along direct line segments are still high, but relatively less,
at 80% and 66%, respectively. This ﬁnding suggests that the loss surfaces of state-of-the-art residual
networks are indeed more regular than those of classical models like VGG, in accordance with the
observations in Li et al. [20].

In this paper we focus on connecting pairs of networks trained using the same hyper-parameters,
but from different random initializations. Building upon our work, Gotmare et al. [9] have recently
shown that our mode connectivity approach applies to pairs of networks trained with different batch
sizes, optimizers, data augmentation strategies, weight decays and learning rate schemes.

To motivate the ensembling procedure proposed in the next section, we now examine how far we
need to move along a connecting curve to ﬁnd a point that produces substantially different, but still

5

useful, predictions. Let ˆw1 and ˆw2 be two distinct sets of weights corresponding to optima obtained
by independently training a DNN two times. We have shown that there exists a path connecting
ˆw1 and ˆw2 with high test accuracy. Let φθ(t), t
[0, 1] parametrize this path with φθ(0) = ˆw1,
∈
φθ(1) = ˆw2. We investigate the performance of an ensemble of two networks: the endpoint φθ(0)
of the curve and a point φθ(t) on the curve corresponding to t
[0, 1]. Figure 2 (right) shows the
test error of this ensemble as a function of t, for a ResNet-164 on CIFAR-100. The test error starts
decreasing at t
0.4 the error of an ensemble is already as low as the error of
an ensemble of the two independently trained networks used as the endpoints of the curve. Thus
even by moving away from the endpoint by a relatively small distance along the curve we can ﬁnd
a network that produces meaningfully different predictions from the network at the endpoint. This
result also demonstrates that these curves do not exist only due to degenerate parametrizations of the
network (such as rescaling on either side of a ReLU); instead, points along the curve correspond to
meaningfully different representations of the data that can be ensembled for improved performance.
In the supplementary material we show how to create trivially connecting curves that do not have this
property.

0.1 and for t

≈

≥

∈

5 Fast Geometric Ensembling

In this section, we introduce a practical ensembling procedure, Fast Geometric Ensembling (FGE),
motivated by our observations about mode connectivity.

Figure 3: Left: Plot of the learning rate (Top), test error (Middle) and distance from the initial value
ˆw (Bottom) as a function of iteration for FGE with Preactivation-ResNet-164 on CIFAR-100. Circles
indicate the times when we save models for ensembling. Right: Ensemble performance of FGE
and SSE (Snapshot Ensembles) as a function of training time, using ResNet-164 on CIFAR-100
(B = 150 epochs). Crosses represent the performance of separate “snapshot” models, and diamonds
show the performance of the ensembles constructed of all models available by the given time.

In the previous section, we considered ensembling along mode connecting curves. Suppose now we
instead only have one set of weights ˆw corresponding to a mode of the loss. We cannot explicitly
) as before, but we know that multiple paths passing through ˆw exist, and it is
construct a path φθ(
·
thus possible to move away from ˆw in the weight space without increasing the loss. Further, we know
that we can ﬁnd diverse networks providing meaningfully different predictions by making relatively
small steps in the weight space (see Figure 2, right).

Inspired by these observations, we propose the Fast Geometric Ensembling (FGE) method that aims
to ﬁnd diverse networks with relatively small steps in the weight space, without leaving a region that
corresponds to low test error.

While inspired by mode connectivity, FGE does not rely on explicitly ﬁnding a connecting curve,
and thus does not require pre-trained endpoints, and so can be trained in the time required to train a
single network.

Let us describe Fast Geometric Ensembling. First, we initialize a copy of the network with weights w
set equal to the weights of the trained network ˆw. Now, to force w to move away from ˆw without
) (see
substantially decreasing the prediction accuracy we adopt a cyclical learning rate schedule α(
·
Figure 3, left), with the learning rate at iteration i = 1, 2, . . . deﬁned as

α(i) =

(cid:26) (1
(2

−
−

2t(i))α1 + 2t(i)α2
2t(i))α2 + (2t(i)

0 < t(i)
1
2 < t(i)

1
2
1

≤
≤

,

1)α1

−

6

−

c (mod(i

where t(i) = 1
1, c) + 1), the learning rates are α1 > α2, and the number of iterations
in one cycle is given by even number c. Here by iteration we mean processing one mini-batch of
data. We can train the network w using the standard (cid:96)2-regularized cross-entropy loss function (or
any other loss that can be used for DNN training) with the proposed learning rate schedule for n
iterations. In the middle of each learning rate cycle when the learning rate reaches its minimum value
α(i) = α2 (which corresponds to mod(i
2 ) we collect the checkpoints
of weights w. When the training is ﬁnished we ensemble the collected models. An outline of the
algorithm is provided in the supplement.

1, c) + 1 = c/2, t(i) = 1

−

Figure 3 (left) illustrates the adopted learning rate schedule. During the periods when the learning
rate is large (close to α1), w is exploring the weight space doing larger steps but sacriﬁcing the test
error. When the learning rate is small (close to α2), w is in the exploitation phase in which the steps
become smaller and the test error goes down. The cycle length is usually about 2 to 4 epochs, so that
the method efﬁciently balances exploration and exploitation with relatively-small steps in the weight
space that are still sufﬁcient to gather diverse and meaningful networks for the ensemble.

To ﬁnd a good initialization ˆw for the proposed procedure, we ﬁrst train the network with the standard
learning rate schedule (the schedule used to train single DNN models) for about 80% of the time
required to train a single model. After this pre-training is ﬁnished we initialize FGE with ˆw and run
the proposed fast ensembling algorithm for the remaining computational budget. In order to get more
diverse samples, one can run the algorithm described above several times for a smaller number of
iterations initializing from different checkpoints saved during training of ˆw, and then ensemble all of
the models gathered across these runs.

Cyclical learning rates have also recently been considered in Smith and Topin [25] and Huang et al.
[13]. Our proposed method is perhaps most closely related to Snapshot Ensembles [13], but has
several distinctive features, inspired by our geometric insights. In particular, Snapshot Ensembles
adopt cyclical learning rates with cycle length on the scale of 20 to 40 epochs from the beginning
of the training as they are trying to do large steps in the weight space. However, according to our
analysis of the curves it is sufﬁcient to do relatively small steps in the weight space to get diverse
networks, so we only employ cyclical learning rates with a small cycle length on the scale of 2 to 4
epochs in the last stage of the training. As illustrated in Figure 3 (left), the step sizes made by FGE
between saving two models (that is the euclidean distance between sets of weights of corresponding
models in the weight space) are on the scale of 7 for Preactivation-ResNet-164 on CIFAR-100. For
Snapshot Ensembles for the same model the distance between two snapshots is on the scale of 40. We
also use a piecewise linear cyclical learning rate schedule following Smith and Topin [25] as opposed
to the cosine schedule in Snapshot Ensembles.

6 Fast Geometric Ensembling Experiments

Table 1: Error rates (%) on CIFAR-100 and CIFAR-10 datasets for different ensembling techniques
and training budgets. The best results for each dataset, architecture, and budget are bolded.

CIFAR-100

CIFAR-10

DNN (Budget)

method

1B

2B

3B

1B

VGG-16 (200)

ResNet-164 (150)

WRN-28-10 (200)

Ind
SSE
FGE

Ind
SSE
FGE

Ind
SSE
FGE

27.4
26.4
25.7

21.5
20.9
20.2

19.2
17.9
17.7

0.1
0.1
0.1

0.4
0.2
0.1

0.2
0.2
0.2

±
±
±

±
±
±

±
±
±

25.28
25.16
24.11

19.04
19.28
18.67

17.48
17.3
16.95

24.45
24.69
23.54

18.59
18.91
18.21

17.01
16.97
16.88

6.75
6.57
6.48

4.72
4.66
4.54

3.82
3.73
3.65

0.16
0.12
0.09

0.1
0.02
0.05

0.1
0.04
0.1

±
±
±

±
±
±

±
±
±

2B

5.89
6.19
5.82

4.1
4.37
4.21

3.4
3.54
3.38

3B

5.9
5.95
5.66

3.77
4.3
3.98

3.31
3.55
3.52

In this section we compare the proposed Fast Geometric Ensembling (FGE) technique against
ensembles of independently trained networks (Ind), and SnapShot Ensembles (SSE) [13], a recent
state-of-the-art fast ensembling approach.

7

For the ensembling experiments we use a 164-layer Preactivation-ResNet in addition to the VGG-16
and Wide ResNet-28-10 models. Links for implementations to these models can be found in the
supplement.

We compare the accuracy of each method as a function of computational budget. For each network
architecture and dataset we denote the number of epochs required to train a single model as B. For
a kB budget, we run each of Ind, FGE and SSE k times from random initializations and ensemble
the models gathered from the k runs. In our experiments we set B = 200 for VGG-16 and Wide
ResNet-28-10 (WRN-28-10) models, and B = 150 for ResNet-164, since 150 epochs is typically
sufﬁcient to train this model. We note the runtime per epoch for FGE, SSE, and Ind is the same, and
so the total computation associated with kB budgets is the same for all ensembling approaches.

For Ind, we use an initial learning rate of 0.1 for ResNet and Wide ResNet, and 0.05 for VGG. For
FGE, with VGG we use cycle length c = 2 epochs, and a total of 22 models in the ﬁnal ensemble.
With ResNet and Wide ResNet we use c = 4 epochs, and the total number of models in the ﬁnal
ensemble is 12 for Wide ResNets and 6 for ResNets. For VGG we set the learning rates to α1 = 10−2,
10−4. . For SSE,
α2 = 5
we followed Huang et al. [13] and varied the initial learning rate α0 and number of snapshots per
run M . We report the best results we achieved, which corresponded to α0 = 0.1, M = 4 for ResNet,
α0 = 0.1, M = 5 for Wide ResNet, and α0 = 0.05, M = 5 for VGG. The total number of models in
the FGE ensemble is constrained by network choice and computational budget. Further experimental
details are in the supplement.

10−4; for ResNet and Wide ResNet models we set α1 = 5

10−2, α2 = 5

·

·

·

Table 1 summarizes the results of the experiments. In all conducted experiments FGE outperforms
SSE, particularly as we increase the computational budget. The performance improvement against
Ind is most noticeable for CIFAR-100. With a large number of classes, any two models are less likely
to make the same predictions. Moreover, there will be greater uncertainty over which representation
one should use on CIFAR-100, since the number of classes is increased tenfold from CIFAR-10, but
the number of training examples is held constant. Thus smart ensembling strategies will be especially
important on this dataset. Indeed in all experiments on CIFAR-100, FGE outperformed all other
methods. On CIFAR-10, FGE consistently improved upon SSE for all budgets and architectures.
FGE also improved against Ind for all training budgets with VGG, but is more similar in performance
to Ind on CIFAR-10 when using ResNets.

Figure 3 (right) illustrates the results for Preactivation-ResNet-164 on CIFAR-100 for one and two
training budgets. The training budget B is 150 epochs. Snapshot Ensembles use a cyclical learning
rate from the beginning of the training and they gather the models for the ensemble throughout
training. To ﬁnd a good initialization we run standard independent training for the ﬁrst 125 epochs
before applying FGE. In this case, the whole ensemble is gathered over the following 22 epochs
(126-147) to ﬁt in the budget of each of the two runs. During these 22 epochs FGE is able to gather
diverse enough networks to outperform Snapshot Ensembles both for 1B and 2B budgets.

Diversity of predictions of the individual networks is crucial for the ensembling performance [e.g.,
19]. We note that the diversity of the networks averaged by FGE is lower than that of completely
independently trained networks. Speciﬁcally, two independently trained ResNet-164 on CIFAR-100
make different predictions on 19.97% of test objects, while two networks from the same FGE run
make different predictions on 14.57% of test objects. Further, performance of individual networks
averaged by FGE is slightly lower than that of fully trained networks (e.g. 78.0% against 78.5% on
CIFAR100 for ResNet-164). However, for a given computational budget FGE can propose many
more high-performing networks than independent training, leading to better ensembling performance
(see Table 1).

6.1

ImageNet

ImageNet ILSVRC-2012 [23] is a large-scale dataset containing 1.2 million training images and
50000 validation images divided into 1000 classes.

CIFAR-100 is the primary focus of our ensemble experiments. However, we also include ImageNet
results for the proposed FGE procedure, using a ResNet-50 architecture. We used a pretrained model
with top-1 test error of 23.87 to initialize the FGE procedure. We then ran FGE for 5 epochs with a
cycle length of 2 epochs and with learning rates α1 = 10−3, α2 = 10−5. The top-1 test error-rate of
the ﬁnal ensemble was 23.31. Thus, in just 5 epochs we could improve the accuracy of the model by

8

0.56 using FGE. The ﬁnal ensemble contains 4 models (including the pretrained one). Despite the
harder setting of only 5 epochs to construct an ensemble, FGE performs comparably to the best result
reported by Huang et al. [13] on ImageNet, 23.33 error, which was also achieved using a ResNet-50.

7 Discussion and Future Work

We have shown that the optima of deep neural networks are connected by simple pathways, such
as a polygonal chain with a single bend, with near constant accuracy. We introduced a training
procedure to ﬁnd these pathways, with a user-speciﬁc curve of choice. We were inspired by these
insights to propose a practical new ensembling approach, Fast Geometric Ensembling, which achieves
state-of-the-art results on CIFAR-10, CIFAR-100, and ImageNet.

There are so many exciting future directions for this research. At a high level we have shown that
even though the loss surfaces of deep neural networks are very complex, there is relatively simple
structure connecting different optima. Indeed, we can now move towards thinking about valleys of
low loss, rather than isolated modes.

These valleys could inspire new directions for approximate Bayesian inference, such as stochastic
MCMC approaches which could now jump along these bridges between modes, rather than getting
stuck exploring a single mode. One could similarly derive new proposal distributions for variational
inference, exploiting the ﬂatness of these pathways. These geometric insights could also be used to
accelerate the convergence, stability and accuracy of optimization procedures like SGD, by helping
us understand the trajectories along which the optimizer moves, and making it possible to develop
procedures which can now search in more structured spaces of high accuracy. One could also use
these paths to construct methods which are more robust to adversarial attacks, by using an arbitrary
collection of diverse models described by a high accuracy curve, returning the predictions of a
different model for each query from an adversary. We can also use this new property to create better
visualizations of DNN loss surfaces. Indeed, using the proposed training procedure, we were able to
produce new types of visualizations showing the connectivity of modes, which are normally depicted
as isolated. We also could continue to build on the new training procedure we proposed here, to ﬁnd
curves with particularly desirable properties, such as diversity of networks. Indeed, we could start to
use entirely new loss functions, such as line and surface integrals of cross-entropy across structured
regions of weight space.

Acknowledgements. Timur Garipov was supported by Ministry of Education and Science of the
Russian Federation (grant 14.756.31.0001). Timur Garipov and Dmitrii Podoprikhin were supported
by Samsung Research, Samsung Electronics. Andrew Gordon Wilson and Pavel Izmailov were
supported by Facebook Research and NSF IIS-1563887.

References

[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale
machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467,
2016.

[2] Peter Auer, Mark Herbster, and Manfred K Warmuth. Exponentially many local minima for
single neurons. In Advances in Neural Information Processing Systems, pages 316–322, 1996.

[3] Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics, pages 192–204,
2015.

[4] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In Advances in Neural Information Processing Systems, pages 2933–2941,
2014.

[5] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International

9

Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pages 1019–1028, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
URL http://proceedings.mlr.press/v70/dinh17b.html.

[6] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no
barriers in neural network energy landscape. In Jennifer Dy and Andreas Krause, editors, Pro-
ceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 1309–1318, Stockholmsmässan, Stockholm Sweden,
10–15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/draxler18a.html.

[7] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectiﬁed network optimiza-

tion. International Conference on Learning Representations, 2017.

[8] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural
network optimization problems. International Conference on Learning Representations, 2015.

[9] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Using mode

connectivity for loss landscape analysis. arXiv preprint arXiv:1806.06977, 2018.

[10] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pages 1321–1330, 2017.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[12] Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

[13] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.
Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representa-
tions, 2017.

[14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning, pages
448–456, 2015.

[15] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018.

[16] Hannes Jonsson, Greg Mills, and Karsten W Jacobsen. Nudged elastic band method for ﬁnding
minimum energy paths of transitions. Classical and quantum dynamics in condensed phase
simulations, 1998.

[17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.
International Conference on Learning Representations, 2017.

[18] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on Learning Theory, pages 1246–1257, 2016.

[19] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael Cogswell, Viresh Ranjan, David
Crandall, and Dhruv Batra. Stochastic multiple choice learning for training diverse deep
ensembles. In Advances in Neural Information Processing Systems, pages 2119–2127, 2016.

[20] Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural

nets. arXiv preprint arXiv:1712.09913, 2017.

[21] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts.

International Conference on Learning Representations, 2017.

[22] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.

10

[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2012.

[24] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556, 2014.

[25] Leslie N Smith and Nicholay Topin. Exploring loss function topology with cyclical learning

rates. arXiv preprint arXiv:1702.04283, 2017.

[26] Jingjing Xie, Bing Xu, and Zhang Chuang. Horizontal and vertical ensemble with deep

representation for classiﬁcation. arXiv preprint arXiv:1306.2759, 2013.

[27] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.

A Supplementary Material

We organize the supplementary material as follows. Section A.1 discusses the computational com-
plexity of the proposed curve ﬁnding method. Section A.2 describes how to apply batch normalization
at test time to points on curves connecting pairs of local optima. Section A.3 provides formulas
for a polygonal chain and Bezier curve with n bends. Section A.4 provides details and results of
experiments on curve ﬁnding and contains a table summarizing all path ﬁnding experiments. Section
A.5 provides additional visualizations of the train loss and test accuracy surfaces. Section A.6 contains
details on curve ensembling experiments. Section A.7 describes experiments on relation between
mode connectivity and the number of parameters in the networks. Section A.8 discusses a trivial
construction of curves connecting two modes, where points on the curve represent reparameterization
of the endpoints, unlike the curves in the main text. Section A.9 provides details of experiments on
FGE. Finally, Section A.10 describes pathways traversed by FGE.

A.1 Computational complexity of curve ﬁnding

The forward pass of the proposed method consists of two steps: computing the point φθ(t) and then
passing a mini-batch of data through the DNN corresponding to this point. Similarly, the backward
pass consists of ﬁrst computing the gradient of the loss with respect to φθ(t), and then multiplying the
result by the Jacobian ∂φθ
∂θ . The second step of the forward pass and the ﬁrst step of the backward pass
are exactly the same as the forward and backward pass in the training of a single DNN model. The
additional computational complexity of the procedure compared to single model training comes from
the ﬁrst step of the forward pass and the second step of the backward pass and in general depends on
) of the curve.
the parametrization φθ(
·

In our experiments we use curve parametrizations of a speciﬁc form. The general formula for a curve
with one bend is given by

φθ(t) = ˆw1

c1(t) + θ

c(t) + ˆw2

c2(t).

·

·
R|net| and coefﬁcients c1, c2, c : [0, 1]

·

Here the parameters of the curve are given by θ

∈

For this family of curves the computational complexity of the ﬁrst step of the method is
we only need to compute a weighted sum of ˆw1, ˆw2 and θ

O
R|net|. The Jacobian matrix

R.

→
(
|

net

), as

|

), as we only need
thus the additional computational complexity of the backward pass is also
|
to multiply the gradient with respect to φθ(t) by a scalar. Thus, the total additional computational
). In practice we observe that the gap in time-complexity between one epoch
complexity is
|
of training a single model and one epoch of the proposed method with the same network architecture
is usually below 50%.

net
|

net

(
|

O

O

(

∈

∂φθ(t)
∂θ

= c(t)

I,

·

11

A.2 Batch Normalization

Batch normalization (Ioffe and Szegedy [14]) is essential to modern deep learning architectures.
Batch normalization re-parametrizes the output of each layer as

ˆx = γ

µ(x)
x
σ(x) + (cid:15)

−

+ β,

where µ(x) and σ(x) are the mean and standard deviation of the output x, (cid:15) > 0 is a constant for
numerical stability and γ and β are free parameters. During training, µ(x) and σ(x) are computed
separately for each mini-batch and at test time statistics aggregated during training are used.

When connecting two DNNs that use batch normalization, along a curve φ(t), we compute µ(x) and
σ(x) for any given t over mini-batches during training, as usual. In order to apply batch-normalization
to a network on the curve at the test stage we compute these statistics with one additional pass over
the data, as running averages for these networks are not collected during training.

A.3 Formulas for curves with n bends

For n bends θ =
{
w0, wn+1 is given by

w1, w2, . . . , wn

, the parametrization of a polygonal chain connecting points

φθ(t) = (n + 1)

i
n + 1

t

−

(cid:19)

·

wi+1 +

(cid:18) i + 1

(cid:19)

t

(cid:19)

wi

,

n + 1 −

·

}

(cid:18)(cid:18)

·

n.

t

for

i
n+1 ≤
For n bends θ =
{
wn+1 is given by

≤

i+1
n+1 and 0

i

≤

≤
w1, w2, . . . , wn

, the parametrization of a Bezier curve connecting points w0 and
}

φθ(t) =

wiC i

n+1ti(1

t)n+1−i

−

n+1
(cid:88)

i=0

A.4 Curve Finding Experiments

Figure 4: The (cid:96)2-regularized cross-entropy train loss (Top) and test error (Bottom) surfaces of a
deep residual network (ResNet-164) on CIFAR-100. Left: Three optima for independently trained
networks. Middle and Right: A quadratic Bezier curve, and a polygonal chain with one bend,
connecting the lower two optima on the left panel along a path of near-constant loss. Notice that in
each panel, a direct linear path between each mode would incur high loss.

All experiments on curve ﬁnding were conducted with TensorFlow (Abadi et al. [1]) and as baseline
models we used the following implementations:

ResNet-bottleneck-164 and Wide ResNet-28-10 (https://github.com/tensorflow/
models/tree/master/research/resnet);

•

12

Table 2: The properties of loss and error values along the found curves for different architectures and tasks

Model

Length

Train Loss

Train Error (%)

Test Error (%)

DNN

Curve

Ratio

Min

Int

Mean Max Min

Int

Max Min

Int

Max

FC
FC
FC
FC

3conv3fc
3conv3fc
3conv3fc
3conv3fc

VGG-16
VGG-16
VGG-16
VGG-16

ResNet-158
ResNet-158
ResNet-158
ResNet-158

Single
Segment
Bezier
Polychain

Single
Segment
Bezier
Polychain

Single
Segment
Bezier
Polychain

Single
Segment
Bezier
Polychain

Single
Segment

WRN-10-28
WRN-10-28
WRN-10-28 Bezier
WRN-10-28

Polychain

VGG-16
VGG-16
VGG-16
VGG-16

ResNet-164
ResNet-164
ResNet-164
ResNet-164

Single
Segment
Bezier
Polychain

Single
Segment
Bezier
Polychain

—
1
1.58
1.73

—
1
1.30
1.67

—
1
1.55
1.83

—
1
2.13
3.48

—
1
1.83
1.95

—
1
1.52
1.64

—
1
1.87
2.56

0.018 —
0.018
0.016
0.013

0.252
0.02
0.022

0.05 —
0.05
0.034
0.04

1.124
0.038
0.044

0.04 —
0.039
0.028
0.025

1.759
0.03
0.031

0.015 —
0.013
0.013
0.013

0.551
0.017
0.017

0.033 —
0.033
0.03
0.026

0.412
0.033
0.029

0.14 —
0.137
0.095
0.118

3.606
0.107
0.139

0.079 —
0.076
0.074
0.067

1.844
0.083
0.078

MNIST

CIFAR-10

—
0.252
0.02
0.022

—
1.124
0.037
0.044

—
1.759
0.03
0.031

—
0.551
0.018
0.017

—
0.412
0.038
0.029

—
3.606
0.105
0.139

—
1.844
0.084
0.078

0.018
0.657
0.024
0.029

0.05
2.416
0.05
0.05

0.04
2.569
0.04
0.045

0.015
2.613
0.022
0.047

0.035
2.203
0.038
0.037

0.141
4.941
0.141
0.2

0.08
5.53
0.098
0.109

0.01 —
0.01
0.01
0

0.53
0.02
0.03

0.06 —
0.06
0.05
0.06

35.69
0.1
0.15

0
0
0
0

0
0
0
0

0.02 —
0
0
0

16.37
0.02
0.05

—
61.43
0.01
0.01

—
5.44
0.01
0

0.05 —
0.04
0.03
0.04

73.25
0.08
0.19

0.06 —
0.06
0.05
0.06

38.03
0.28
0.28

0.01
2.13
0.04
0.07

0.06
88.24
0.2
0.31

0.01
90
0.02
0.04

0.02
81.41
0.07
0.139

0
65.62
0.04
0

0.06
99
0.18
0.39

0.09
98.65
0.96
0.85

1.46 —
1.45
1.46
1.46

1.96
1.52
1.51

12.3 —
12.28
12.06
12.17

43.3
12.7
12.68

6.87 —
6.87
6.59
6.54

63.75
6.77
6.89

5.56 —
5.57
5.48
5.48

20.79
5.82
5.88

4.49 —
4.49
4.4
4.38

10.55
4.62
6.93

29.44 —
29.44
29.28
29.33

80.59
30.49
30.13

24.41 —
24.4
24.15
23.98

53.69
24.99
24.92

1.5
3.18
1.56
1.58

12.36
88.27
13.66
13.31

7.01
90
7.01
7.28

5.74
80.00
6.24
7.35

4.56
66.6
4.83
10.38

29.94
99.01
31.23
30.92

24.4
98.83
26.1
26.12

CIFAR-100

Table 3: The value of perplexity along the found curves for PTB dataset

Model

Train

Validation

Test

DNN Curve

Min Max Min Max Min Max

RNN Single
RNN Segment
RNN Bezier

37.5
37.5
29.8

39.2
596.3
39.2

82.7
82.7
82.7

83.1
682.1
88.7

78.7
78.7
78.7

78.9
615.7
84.0

ResNet-158 (https://github.com/tensorflow/models/tree/master/official/
resnet);

A reimplementation of VGG-16 without batch-normalization from (https://github.
com/pytorch/vision/blob/master/torchvision/models/vgg.py);

•

•

PTB

13

Figure 5: Same as Fig. 4 for VGG-16 on CIFAR-10.

Table 2 summarizes the results of the curve ﬁnding experiments with all datasets and architectures.
For each of the models we report the properties of loss and the error on the train and test datasets.
For each of these metrics we report 3 values: “Max” is the maximum values of the metric along
the curve, “Int” is a numerical approximation of the integral (cid:82) <metric>(φθ)dφθ/(cid:82) dφθ, where
<metric> represents the train loss or the error on the train or test dataset and “Min” is the minimum
value of the error on the curve. “Int” represents a mean over a uniform distribution on the curve, and
for the train loss it coincides with the loss (1) in the paper. We use an equally-spaced grid with 121
points on [0, 1] to estimate the values of “Min”, “Max”, “Int”. For “Int” we use the trapezoidal rule to
estimate the integral. For each dataset and architecture we report the performance of single models
used as the endpoints of the curve as “Single”, the performance of a line segment connecting the
two single networks as “Segment”, the performance of a quadratic Bezier curve as “Bezier” and the
performance of a polygonal chain with one bend as “Polychain”. Finally, for each curve we report
the ratio of its length to the length of a line segment connecting the two modes.
We also examined the quantity “Mean” deﬁned as (cid:82) < metric > (φθ(t))dt, which coincides with
the loss (2) from the paper, but in all our experiments it is nearly equal to “Int”.

Besides convolutional and fully-connected architectures we also apply our approach to RNN archi-
tecture on next word prediction task, PTB dataset (Marcus et al. [22]). As a base model we used
the implementation available at https://www.tensorflow.org/tutorials/recurrent. As the
main loss we consider perplexity. The results are presented in Table 3.

A.5 Train loss and test accuracy surfaces

In this section we provide additional visualizations. Fig. 4 and Fig. 5 show visualizations of the train
loss and test accuracy for ResNet-164 on CIFAR-100 and VGG-16 on CIFAR-10.

A.6 Curve Ensembling

Here we explore ensembles constructed from points sampled from these high accuracy curves. In
particular, we train a polygonal chain with one bend connecting two independently trained ResNet-
164 networks on CIFAR-100 and construct an ensemble of networks corresponding to 50 points
placed on an equally-spaced grid on the curve. The resulting ensemble had 21.03% error-rate on
the test dataset. The error-rate of the ensemble constructed from the endpoints of the curve was
22.0%. An ensemble of three independently trained networks has an error rate of 21.01%. Thus, the
ensemble of the networks on the curve outperformed an ensemble of its endpoints implying that the
curves found by the proposed method are actually passing through diverse networks that produce
predictions different from those produced by the endpoints of the curve. Moreover, the ensemble
based on the polygonal chain has the same number of parameters as three independent networks, and
comparable performance.

Furthermore, we can improve the ensemble on the chain without adding additional parameters or
computational expense, by accounting for the pattern of increased training and test loss towards the

14

Figure 6: Error as a function of the point on the curves φθ(t) found by the proposed method, using a
ResNet-164 on CIFAR-100. Top left: train error. Bottom left: test error; dashed lines correspond to
quality of ensemble constructed from curve points before and after logits rescaling. Top right: train
loss ((cid:96)2 regularized cross-entropy). Bottom right: cross-entropy before and after logits rescaling for
the polygonal chain.

centres of the linear paths shown in Figure 6. While the training and test accuracy are relatively
constant, the pattern of loss, shared across train and test sets, indicates overconﬁdence away from
the three points deﬁning the curve: in this region, networks tend to output probabilities closer to 1,
sometimes with the wrong answers. This overconﬁdence decreases the performance of ensembles
constructed from the networks sampled on the curves. In order to correct for this overconﬁdence and
improve the ensembling performance we use temperature scaling [10], which is inversely proportional
to the loss. Figure 6, bottom right, illustrates the test loss of ResNet-164 on CIFAR-100 before and
after temperature scaling. After rescaling the predictions of the networks, the test loss along the curve
decreases and ﬂattens. Further, the test error-rate of the ensemble constructed from the points on the
curve went down from 21.03% to 20.7% after applying the temperature scaling, outperforming 3
independently trained networks.

However, directly ensembling on the curves requires manual intervention for temperature scaling,
and an additional pass over the training data for each of the networks (50 in this case) at test time to
perform batch normalization as described in section A.2. Moreover, we also need to train at least two
networks for the endpoints of the curve.

A.7 The Effects of Increasing Parametrization

Figure 7: The worst train loss along the curve, maximum of the losses of the endpoints, and the ratio
of the length of the curve and the line segment connecting the two modes, as a function of the scaling
factor K of the sizes of fully-connected layers.

One possible factor that inﬂuences the connectedness of a local minima set is the overparameterization
of neural networks. In this section, we investigate the relation between the observed connectedness
of the local optima and the number of parameters (weights) in the neural network. We start with a
network that has three convolutional layers followed by three fully-connected layers, where each
layer has 1000K neurons. We vary K
, and for each value of K we train two
0.3, 0.5, 0.8, 1
}
networks that we connect with a Bezier curve using the proposed procedure.

∈ {

15

Algorithm 1 Fast Geometric Ensembling

Require:

weights ˆw, LR bounds α1, α2,
cycle length c (even), number of iterations n

Ensure: ensemble

←

ˆw {Initialize weight with ˆw}

w
←
ensemble
for i
α
α
w
if mod(i, c) = c/2 then

[ ]
1, 2, . . . , n do
α(i) {Calculate LR for the iteration}
w

∇L

←
←
←
ensemble

−

i(w) {Stochastic gradient update}

ensemble + [w] {Collect weights}

end if
end for

←

For each value of K, Figure 7 shows the worst training loss along the curve, maximum of losses
of the endpoints, and the ratio of the length of the curve and the line segment connecting the two
modes. Increasing the number of parameters we are able to reduce the difference between the worst
value of the loss along the curve and the loss of single models used as the endpoints. The ratio of
the length of the found curve and the length of the line segment connecting the two modes also
decreases monotonically with K. This result is intuitive, since a greater parametrization allows for
more ﬂexibility in how we can navigate the loss surfaces.

A.8 Trivial connecting curves

For convolutional networks with ReLU activations and without batch normalization we can construct
a path connecting two points in weight space such that the accuracy of each point on the curve
(excluding the origin of the weight space) is at least as good as the minimum of the accuracies of
the endpoints. Unlike the paths found by our procedure, these paths are trivial and merely exploit
redundancies in the parametrization. Also, the training loss goes up substantially along these curves.
Below we give a construction of such paths.

Let ˆw1 and ˆw2 be two sets of weights. This path of interest consists of two parts. The ﬁrst part
connects the point ˆw1 with 0 and the second one connects the point ˆw2 with 0. We describe only the
ﬁrst part φ(t) of the path, such that φ(0) = 0, φ(1) = ˆw1, as the second part is completely analogous.
Wi, bi
Let the weights of the network ˆw1 be
1≤i≤n where Wi, bi are the weights and biases of the
{
i-th layer, and n is the total number of layers. Throughout the derivation we consider the inputs of
the network ﬁxed. The output of the i-th layer oi = WiReLU(oi−1) + bi, 1
n, where i = 0
corresponds to the ﬁrst layer and i = n corresponds to logits (the outputs of the last layer). We
1≤i≤n in the following way. We set Wi(t) = Wit and bi(t) = biti.
construct φ(t) =
It is easy to see that logits of the network with weights φ(t) are equal to on(t) = tnon for all t > 0.
Note that the predicted labels corresponding to the logits on(t) and on are the same, so the accuracy
of all networks corresponding to t > 0 is the same.

Wi(t), bi(t)
}
{

≤

≤

}

i

A.9 Fast geometric ensembling experiments

Alg. 1 provides an outline of the algorithm. As baseline models we used the following implementa-
tions:

VGG-16
models/vgg.py);

(https://github.com/pytorch/vision/blob/master/torchvision/

Preactivation-ResNet-164
pytorch-classification/blob/master/models/cifar/preresnet.py);

(https://github.com/bearpaw/

ImageNet
ResNet-50
torchvision/models/resnet.py);

(https://github.com/pytorch/vision/blob/master/

Wide ResNet-28-10
blob/master/networks/wide_resnet.py);

(https://github.com/meliketoy/wide-resnet.pytorch/

•

•

•

•

16

For the FGE (Fast Geometric Ensembling) strategy on ResNet we run the FGE routine summarized
in Alg. 1 after epoch 125 of the usual (same as Ind) training for 22 epochs. The total training
time is thus 125 + 22 = 147 epochs. For VGG and Wide ResNet models we run the pre-training
procedure for 156 epochs to initialize FGE. Then we run FGE for 22 epochs starting from checkpoints
corresponding to epochs 120 and 156 and ensemble all the gathered models. The total training time
is thus 156 + 22 + 22 = 200 epochs. For VGG we use cycle length c = 2 epochs, which means
that the total number of models in the ﬁnal ensemble is 22. For ResNet and Wide ResNet we use
c = 4 epochs, and the total number of models in the ﬁnal ensemble is 12 for Wide ResNets and 6 for
ResNets.

A.10 Polygonal chain connecting FGE proposals

In order to better understand the trajectories followed by FGE we construct a polygonal chain
connecting the points that FGE ensembles. Suppose we run FGE for n learning rate cycles obtaining
n points w1, w2, . . . , wn in the weight space that correspond to the lowest values of the learning
rate. We then consider the polygonal chain consisting of the line segments connecting wi to wi+1 for
i = 1, . . . , n
1. We plot test accuracy and train error along this polygonal chain in Figure 8. We
observe that along this curve both train loss and test error remain low, agreeing with our intuition that
FGE follows the paths of low loss and error. Surprisingly, we ﬁnd that the points on the line segments
connecting the weights wi, wi+1 have lower train loss and test error than wi and wi+1. See Izmailov
et al. [15] for a detailed discussion of this phenomenon.

−

Figure 8: Train loss and test error along the polygonal chain connecting the sequence of points
ensembled in FGE. The plot is generated using PreResNet-164 on CIFAR 100. Circles indicate the
bends on the polygonal chain, i.e. the networks ensembled in FGE.

17

