SHUFFLESEG: REAL-TIME SEMANTIC SEGMENTATION NETWORK

Mostafa Gamal, Mennatullah Siam, Mo’men Abdel-Razek

mostafa.gamal95@eng-st.cu.edu.eg, mennatul@ualberta.ca,
moemen.abdelrazek96@eng-st.cu.edu.eg
Cairo University, University of Alberta

8
1
0
2
 
r
a

M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
6
1
8
3
0
.
3
0
8
1
:
v
i
X
r
a

ABSTRACT

Real-time semantic segmentation is of signiﬁcant importance
for mobile and robotics related applications. We propose a
computationally efﬁcient segmentation network which we
term as ShufﬂeSeg. The proposed architecture is based on
grouped convolution and channel shufﬂing in its encoder for
improving the performance. An ablation study of different
decoding methods is compared including Skip architecture,
Interesting insights on the
UNet, and Dilation Frontend.
speed and accuracy tradeoff is discussed.
It is shown that
skip architecture in the decoding method provides the best
compromise for the goal of real-time performance, while
it provides adequate accuracy by utilizing higher resolution
feature maps for a more accurate segmentation. ShufﬂeSeg
is evaluated on CityScapes and compared against the state
of the art real-time segmentation networks.
It achieves 2x
GFLOPs reduction, while it provides on par mean intersec-
tion over union of 58.3% on CityScapes test set. ShufﬂeSeg
runs at 15.7 frames per second on NVIDIA Jetson TX2,
which makes it of great potential for real-time applications.

Index Terms— realtime - semantic segmentation

1. INTRODUCTION

Building computationally efﬁcient convolutional neural net-
works (CNNs) is still an open research problem. There has
been two main mechanisms for improving their efﬁciency.
The ﬁrst mechanism is focused on designing efﬁcient models,
such as the work in GoogleNet [1], Xception [2], MobileNet
[3], and the recent ShufﬂeNet [4]. The other mechanism is di-
rected towards model acceleration, by pruning network con-
nections [5][6] or channels [7] or network quantization[8][9].
Previous work in improving computational efﬁciency mostly
focused on the end task of image classiﬁcation and object de-
tection. However, few works were targeted towards real-time
semantic segmentation networks although semantic segmen-
tation has numerous beneﬁts in robotics related applications
[10][11][12]. That highlights the need for computationally
efﬁcient semantic segmentation.

In this paper, we propose a real-time semantic segmen-
tation network based on the ShufﬂeNet unit introduced in

[4]. We refer to this network as ShufﬂeSeg throughout the
paper. ShufﬂeSeg incorporates skip connections in its de-
coder for improved segmentation results. Our proposed net-
work requires 2.03 GFLOPs which outperforms the state of
the art in computationally efﬁcient segmentation networks
that required 3.83 GFLOPs[13]. Nonetheless, ShufﬂeSeg
achieves comparable mean intersection over union of 58.2%
on CityScapes test set benchmark. Thus, our network pro-
vides good balance between speed and accuracy. This pro-
vides potential beneﬁts for further deployment on embedded
devices.

Real-time semantic segmentation started to draw attention
recently. Paszke et. al. [13] introduced ENet as an efﬁcient
lightweight segmentation network with a bottleneck module.
Chaurasia et. al. [14] proposed LinkNet architecture that uses
ResNet18 as its encoder. LinkNet achieves better mean inter-
section over union than ENet. However, ENet outperforms it
in terms of computational efﬁciency. Other networks not fo-
cused on computational efﬁciency, but are widely used in seg-
mentation literature, are SegNet and FCN8s. Badrinarayanan
et. al. [15] proposed SegNet as an early attempt for end-to-
end semantic segmentation in encoder-decoder architecture.
Long et. al. [16] proposed the ﬁrst attempt for fully convo-
lutional segmentation network (FCN) that was trained end to
end. He also proposed the skip-net method to utilize higher
resolution feature maps in the segmentation in FCN16s and
FCN8s architectures.

To the best of our knowledge, no previous work on real-
time semantic segmentation utilized group convolution and
In this work, we propose ShufﬂeSeg as
channel shufﬂing.
a computationally efﬁcient segmentation network. The net-
work is inspired from ShufﬂeNet [4] which provided an ef-
ﬁcient classiﬁcation and detection networks. The ShufﬂeNet
unit uses grouped convolution instead of 1x1 convolution to
boost the performance. Grouped convolution alone can hurt
the network accuracy, therefore channel shufﬂing is used to
maintain good accuracies. This is coupled with skip archi-
tecture to improve our segmentation results, by using higher
resolution feature maps. The code for ShufﬂeSeg will be pub-
licly available at 1.

1https://github.com/MSiam/TFSegmentation

2. METHOD

In this section, a detailed description of the network archi-
tecture used for semantic segmentation is presented. The ar-
chitecture is explained as two main modules; the encoding
module is the one responsible for extracting features, while
the decoding module is responsible for with-in the network
up-sampling to compute the ﬁnal class probability maps.

2.1. Encoder Architecture

The encoder used in our proposed architecture is based on
ShufﬂeNet [4]. We primarily inspire from their grouped con-
volution and channel shufﬂing. It was shown in [4][2][3] that
depthwise separable convolution or grouped convolution re-
duce the computational cost, while maintaining good repre-
sentation capability. Stacking grouped convolution can lead
to one major bottleneck. The output channels are going to be
derived from a limited fraction of input channels. To solve
such an issue channel shufﬂing was introduced in [4] and is
used as well inside ShufﬂeSeg architecture.

Figure 1 shows the network architecture of ShufﬂeSeg for
both encoding and decoding parts. An initial 3x3 convolu-
tional layer is used with stride 2 for downsampling, followed
by 2x2 maxpooling. Then, three stages are used, each stage
is composed of multiple ShufﬂeNet units. Stage 2 and 4 are
composed of 3 ShufﬂeNet units while stage 3 has 7 units. This
leads to a down-sampling factor of resolution 32.

ShufﬂeNet units act as residual bottleneck modules. On
the primary branch, it has average pooling with stride 2. How-
ever, on the residual branch it has 1x1 grouped convolution
then depth-wise convolution with stride 2 then another 1x1
grouped convolution. The purpose of the ﬁrst grouped convo-
lution is to act as a bottleneck to reduce the number of chan-
nels for a computationally efﬁcient solution. This is followed
by shufﬂing channels to ensure that all the input are connected
to the output without restrictions from assigned groups. The
second grouped convolution recovers the channels back to the
number of input channels for a better representation capabil-
ity. Figure 1 shows the detailed structure of this ShufﬂeNet
unit in the upper right.

2.2. Decoder Architecture

Transposed convolutions are performed in the decoding part
of the segmentation network in order to upsample to the in-
put resolution. Different decoding methods are used which
inspire from the work in UNet [17], FCN8s [16] and Dilation
Frontend[18]. Four different decoding methods are compared
which are: (1) UNet. (2) SkipNet. (3) Dilation Frontend 8s.
(4) Dilation 4s. Insights regarding how these methods affect
the network efﬁciency and accuracy are provided in the ab-
lation study section 3.3. To the best of our knowledge, this
is the ﬁrst study to provide such insights. This comparison

Fig. 1: ShufﬂeSeg Network Architecture.

would beneﬁt the research community and boost the direction
toward real-time segmentation further on.

SkipNet: The main idea in skip connections[16] is to ben-
eﬁt from higher resolution feature maps to improve accuracy.
It is worth noting that transposed convolutions in this decod-
ing method are applied on the ﬁnal heat maps with channels
equivalent to the required classes. This ensures the compu-
tational efﬁciency of the network, as the number of channels
have direct impact on it. The upsampling factor required is of
resolution 32. In our case, the output from stage 4 is passed
through 1x1 convolution, denoted as score layer, to convert
the channels to the number of classes. Stage 2 and stage 3
are used as input intermediate layers to improve the heat map
resolution. The output from score layer is upsampled by 2x
and then elementwise addition between these and heatmaps
from stage 3 is performed. An equivelant operation is per-
formed for stage 2. Finally, a transposed convolution of stride
8 provides the ﬁnal probability maps that match the input size.
Transposed convolutions are initialized with bilinear upsam-
pling.

UNet: UNet decoding method creates an up-sampling
stage corresponding to each downsampling in the original net-
work. Feature fusion is then performed using elementwise
addition with the corresponding stage. Hence, feature con-
catenation could be used for the fusion method, but elemen-
twise addition provides a computationally efﬁcient solution
that better serves the realtime perspective. It is worth noting
that transposed convolutions in this method are performed on
feature maps instead of heat maps as in SkipNet. This leads

to an increase in the number of channels that hurts the com-
putational efﬁciency in comparison to SkipNet.

Dilation Frontend 8s and 4s: In this decoding method
similar to [18], the ShufﬂeNet downsampling factor is changed
from 32 to 8. This is done by changing the ﬁnal units with
stride 2 to 1. Dilation convolution with dilation rate of 2
then 4 is used, to maintain the higher receptive ﬁeld. Then, a
transposed convolution of stride 8 is performed to provide the
ﬁnal probability map, this denotes Dilation 8s. As for Dila-
tion 4s, the same architecture is used while replacing the last
transposed convolution with stride 4 and downsampling the
target labels during training by factor of 2. Downsampling the
labels with a small factor turned to provide a more efﬁcient
method while keeping adequate accuracy in comparison to
the original resolution.

3. EXPERIMENTS

3.1. Dataset

Evaluation is conducted on CityScapes dataset[10]. It con-
tains 5000 images with ﬁne annotation, with 20 classes in-
cluding the ignored class. The dataset is split into 2975 im-
ages for training, 500 for validation and 1525 for testing.
It also contains another 20,000 images annotated but with
coarse annotation. Coarse annotations are used for pretrain-
ing our ﬁnal model that resulted in an improved accuracy as
shown in the ablation study.

3.2. Experimental Setup

1

Experiments are conducted on images with size 512x1024,
with 20 classes including the last class for the ignored class.
A weighted cross entropy loss is used from [13], to overcome
the imbalance in the data between different classes. The class
ln(c+pclass) , where c is a
weight is computed as wclass =
constant hyper-parameter with value 1.02. L2 regularization
is used to avoid over-ﬁtting with weight decay rate of 5e−4.
Adam optimizer[19] is used with learning rate 1e−4. Batch
normalization[20] is used after all convolutional or transposed
convolution layers, to ensure faster convergence. The feature
extractor part of the network is initialized with pre-trained Im-
ageNet weights. Throughout all the experiments, we use 3 as
the number of groups in grouped convolution. The code is
built on TensorFlow and will be made public to the research
community to beneﬁt from it.

3.3. Ablation Study

A detailed ablation study of different decoding methods is
provided in this section. Four different decoding methods
are compared, which are: (1) UNet. (2) SkipNet. (3) Dila-
tion8s. (4) Dilation4s. Table 1 and 2 shows the results for
GFLOPs on images size 1024x512, mean IoU, perclass IoU,
It
and percategory IoU on the validation set in cityscapes.

clearly demonstrates that SkipNet architecture provides the
most efﬁcient method while providing relatively good accu-
racy. Although UNet provides a more accurate solution, yet
is not efﬁcient for real-time solutions. Dilation 8s and 4s pro-
vide an inefﬁcient solution that does not serve the real-time
goal. They also suffer with classes that have fewer represen-
tation in the dataset, while outperform on more represented
classes. This is shown in Table 2 as they outperform in mean
IoU and in categories with higher representation such as Flat
and Sky.

The above observations motivate the usage of SkipNet de-
coding method for further experiments on ShufﬂeSeg. In or-
der to improve the network accuracy on classes with smaller
representation than others in the dataset such as fence and
pole, pretraining of the network is performed. Pretraining is
conducted on the coarse annotation section with more labeled
images. Afterwards, the network is trained on the ﬁne anno-
tations sections. This alone led to an improvement of 4% in
overall mean IoU. This is termed as Coarse in Table 1 and
provides mean IoU of 59.3%.

3.4. State of the Art Comparison

Comparison to the state of the art real-time segmentation is
provided in Table 3. Experiments in this section are con-
ducted on the test set. ShufﬂeSeg, which is the SkipNet
version with coarse pretraining, outperforms ENet[13] in
computational efﬁciency. It lead to 2x reduction in GFLOPs,
while it is on par with it in accuracy.
In comparison to
SegNet[15], ShufﬂeSeg outperforms it in accuracy while hav-
ing 141x GFLOPs reduction. This clearly shows the beneﬁt
of grouped convolution with channel shufﬂe on the reduction
of operations required. ShufﬂeSeg runs at 15.7 frames per
second on NVIDIA Jetson TX2, so it offers real-time perfor-
mance on embedded devices. Figure 2 shows the qualitative
results of ShufﬂeSeg on CityScapes. UNet provide a more
detailed and accurate segmentation than Dilation 8s. The
SkipNet architecture pretrained with coarse annotated data
provide the most accurate segmentation in comparison to
UNet and Dilation 8s. It is shown that ShufﬂeSeg can provide
good accuracy with less number of operations.

4. CONCLUSION

In this paper, an architecture based on grouped convolution
and channel shufﬂing in its encoder is used. An ablation
study is performed to compare different decoding methods.
Interesting insights on the speed and accuracy trade-off is
discussed. It is shown that skip architecture in the decoding
method provides the best compromise between computational
efﬁciency and accuracy. ShufﬂeSeg achieves 2x GFLOPs re-
duction in comparison to ENet and 141x in comparison to
SegNet. It still provides on par mean intersection over union
of 58.3% on CityScapes test set.

(a)

(c)

(b)

(d)

Fig. 2: ShufﬂeSeg Qualitative Images on CityScapes. (a) Original Image. (b) SkipNet pretrained with Coarse Annotations. (b)
UNet. (c) Dilation 8s.

Table 1: Comparison of different decoding methods in accuracy and computational efﬁciency on class level.

Model
UNet
SkipNet
Dilation8s
Dilation4s
Coarse

GFLOPs mIoU Building
57.0
17.7
4.52
55.5
53.9
17.7
53.4
16
59.3
4.52

83.7
83.9
84.1
82.7
85.5

Sky
89.0
88.6
90.3
89.4
90.8

Car
87.8
86.5
86.6
85.1
87.5

Sign Road
95.1
54.3
94.8
50.5
95.2
57.3
94.1
53.8
94.6
54.9

Person
61.7
60.8
62.9
57.9
60.2

Fence
34.6
35.2
31.4
32.0
41.7

Pole
40.3
37.9
37.2
35.5
40.8

Sidewalk Bicycle
69.5
68.6
68.5
66.4
70.5

59.9
58.8
60.2
59.4
58.8

Table 2: Comparison of different decoding methods in accuracy on category level.

Model
UNet
SkipNet
Dilation8s
Dilation4s
Coarse

mIoU Flat Nature Object
79.1
78.2
79.3
77.4
79.4

47.8
44.5
45.4
43.8
48.1

87.6
87.0
87.3
86.4
87.8

95.9
95.9
96.6
95.2
94.8

Sky
89.0
88.6
90.3
89.4
90.8

Construction Human Vehicle
64.0
83.7
63.3
83.8
66.4
84.4
61.2
82.7
85.3
63.4

85.5
84.2
84.7
83.0
85.5

Table 3: Comparison of ShufﬂeSeg to the state of the art real-time segmentation networks.

Model
SegNet
ENet
ShufﬂeSeg

GFLOPs Class IoU Class iIoU Category IoU Category iIoU
286.03
3.83
2.03

56.1
58.3
58.3

66.4
64.0
62.2

34.2
24.4
32.4

79.8
80.4
80.2

5. REFERENCES

[1] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,

Vincent Vanhoucke, and Andrew Rabinovich, “Going
deeper with convolutions,” in Proceedings of the IEEE
conference on computer vision and pattern recognition,

[12] Jay M Wong, Syler Wagner, Connor Lawson, Vincent
Kee, Mitchell Hebert, Justin Rooney, Gian-Luca Mar-
iottini, Rebecca Russell, Abraham Schneider, Rahul
Chipalkatty, et al.,
“Segicp-dsr: Dense semantic
scene reconstruction and registration,” arXiv preprint
arXiv:1711.02216, 2017.

[13] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and
Eugenio Culurciello, “Enet: A deep neural network ar-
chitecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.

[14] Abhishek Chaurasia and Eugenio Culurciello, “Linknet:
Exploiting encoder representations for efﬁcient seman-
arXiv preprint arXiv:1707.03718,
tic segmentation,”
2017.

[15] Vijay Badrinarayanan, Alex Kendall, and Roberto
“Segnet: A deep convolutional encoder-
Cipolla,
decoder architecture for image segmentation,” arXiv
preprint arXiv:1511.00561, 2015.

[16] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
“Fully convolutional networks for semantic segmenta-
tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
3440.

[17] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” CoRR, vol. abs/1505.04597, 2015.

[18] Fisher Yu and Vladlen Koltun,

“Multi-scale context
aggregation by dilated convolutions,” arXiv preprint
arXiv:1511.07122, 2015.

[19] Diederik Kingma and Jimmy Ba,
method for stochastic optimization,”
arXiv:1412.6980, 2014.

“Adam: A
arXiv preprint

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in International Conference on
Machine Learning, 2015, pp. 448–456.

2015, pp. 1–9.

[2] Franc¸ois Chollet,

“Xception: Deep learning with
arXiv preprint

depthwise separable convolutions,”
arXiv:1610.02357, 2016.

[3] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations,” arXiv preprint arXiv:1704.04861, 2017.

[4] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun, “Shufﬂenet: An extremely efﬁcient convolutional
arXiv preprint
neural network for mobile devices,”
arXiv:1707.01083, 2017.

[5] Song Han, Huizi Mao, and William J Dally, “Deep com-
pression: Compressing deep neural networks with prun-
ing, trained quantization and huffman coding,” arXiv
preprint arXiv:1510.00149, 2015.

[6] Song Han, Jeff Pool, John Tran, and William Dally,
“Learning both weights and connections for efﬁcient
in Advances in Neural Information
neural network,”
Processing Systems, 2015, pp. 1135–1143.

[7] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen,
and Hai Li, “Learning structured sparsity in deep neural
networks,” in Advances in Neural Information Process-
ing Systems, 2016, pp. 2074–2082.

[8] Mohammad Rastegari, Vicente Ordonez, Joseph Red-
mon, and Ali Farhadi, “Xnor-net: Imagenet classiﬁca-
tion using binary convolutional neural networks,” in Eu-
ropean Conference on Computer Vision. Springer, 2016,
pp. 525–542.

[9] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu,
and Jian Cheng, “Quantized convolutional neural net-
works for mobile devices,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, 2016, pp. 4820–4828.

[10] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 3213–
3223.

[11] Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell,
“End-to-end learning of driving models from large-scale
video datasets,” arXiv preprint, 2017.

