8
1
0
2
 
p
e
S
 
9
1
 
 
]

G
L
.
s
c
[
 
 
2
v
7
4
2
5
0
.
9
0
8
1
:
v
i
X
r
a

Revisiting Random Binning Features: Fast Convergence
and Strong Parallelizability

∗

Lingfei Wu
College of William and Mary
Williamsburg, VA 23185
lfwu@cs.wm.edu
Jie Chen
IBM Research
Yorktown Heights, NY 10598
chenjie@us.ibm.com

Ian E.H. Yen *
Unversity of Texas at Austin
Austin, TX 78712
ianyen@cs.utexas.edu

Rui Yan
Baidu Inc.
Beijing 100085, China
yanrui02@baidu.com

ABSTRACT
Kernel method has been developed as one of the standard approaches
for nonlinear learning, which however, does not scale to large data
set due to its quadratic complexity in the number of samples. A
number of kernel approximation methods have thus been proposed
in the recent years, among which the random features method gains
much popularity due to its simplicity and direct reduction of non-
linear problem to a linear one. Different random feature functions
have since been proposed to approximate a variety of kernel func-
tions. Among them the Random Binning (RB) feature, proposed in
the ﬁrst random-feature paper [21], has drawn much less attention
than the Random Fourier (RF) feature proposed also in [21]. In
this work, we observe that the RB features, with right choice of op-
timization solver, could be orders-of-magnitude more efﬁcient than
other random features and kernel approximation methods under the
same requirement of accuracy. We thus propose the ﬁrst analysis
of RB from the perspective of optimization, which by interpret-
ing RB as a Randomized Block Coordinate Descent in the inﬁnite-
dimensional space, gives a faster convergence rate compared to that
of other random features.
In particular, we show that by draw-
ing R random grids with at least κ number of non-empty bins
per grid in expectation, RB method achieves a convergence rate
R) rate from
of O(1/(κR)), which not only sharpens its O(1/
Monte Carlo analysis, but also shows a κ times speedup over other
random features under the same analysis framework. In addition,
we demonstrate another advantage of RB in the L1-regularized set-
ting, where unlike other random features, a RB-based Coordinate
Descent solver can be parallelized with guaranteed speedup pro-
portional to κ. Our extensive experiments demonstrate the superior
performance of the RB features over other random features and
kernel approximation methods. Our code and data is available at
https://github.com/teddylfwu/RB_GEN.

√

∗Both authors contributed equally to this manuscript

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’16, August 13 - 17, 2016, San Francisco, CA, USA
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4232-2/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2939672.2939794

Keywords
Kernel approximation, Random Binning Features, large-scale ma-
chine learning, faster convergence, strong parallelizability

1.

INTRODUCTION

Kernel methods have great promise for learning non-linear model
from simple data input representations and have been demonstrated
successful for solving various learning problems such as regres-
sion, classiﬁcation, feature extraction, clustering and dimensional-
ity reduction [26, 30]. However, they are typically not ﬁrst choice
for large-scale nonlinear learning problems, since large number of
samples (N ) presents signiﬁcant challenges in terms of computa-
tion and memory consumptions to Kernel methods for computing
the dense kernel matrix K ∈ RN ×N which requires at least a
O(N 2) complexity. To scale up the kernel methods, there have
been great efforts addressing this challenge from various perspec-
tives such as numerical linear algebra, sampling approximation, op-
timization and functional analysis [4, 5, 7, 21, 27, 33].

√

√

R + 1/

A line of research [7, 27, 28, 31] approximates the kernel ma-
trix K using low-rank factorizations, K ≈ Z T Z, where Z ∈
RN ×R matrix with R (cid:28) N . Among them, Nyström method
[5, 8, 11, 27, 31] is probably one of the most popular approaches,
which reduces the total computational costs to O(N Rd + N R2 +
R3) or O(N Rd + N Rm) depending whether the algorithm per-
forms on K explicitly or implicitly through Z, where d and m are
the input data dimension and the number of iterations of an iterative
solver respectively. However, the convergence of the low-rank ap-
N ) [5, 33], which
proximation is proportional to O(1/
implies that the rank R may need to be near-linear to the number
of data points in order to achieve comparable generalization error
compared to the vanilla kernel method. For large-scale problems,
the low-rank approximation could become almost as expensive as
the exact kernel method to maintain competitive performance [29].
Another popular approach for scaling up kernel method is ran-
dom features approximation [21, 22]. Unlike previous approach
that approximates kernel matrix, Random Features approximate the
kernel function directly via sampling from an explicit feature map.
Random Fourier (RF) is one of the feature maps that attracted con-
siderable interests due to its easy implementation and fast execution
time [4, 12, 20, 22, 32], which has total computational cost and stor-
age requirement as O(N Rd + N Rm) and O(N R) respectively,
for computing feature matrix Z and operating the subsequent algo-
rithms on Z. A Fastfood approach and its extension [12, 32] was

proposed to reduce the time of computing Fourier features from
O(Rd) to O(R log d) by leveraging Hadamard basis functions,
which improves the efﬁciency for prediction but not necessarily for
training if d (cid:28) m. Although RF has been successfully applied to
speech recognition and vision classiﬁcations on very large datasets
[3, 10, 17], a drawback is that a signiﬁcant large number of random
features are needed to achieve a comparable performance to exact
kernel method. This is not surprising since the convergence of ap-
N ) [4, 22], which
proximation error is in the order O(1/
is the same as that of low-rank kernel approximations.

R+1/

√

√

Mercer’s theorem [19] guarantees that any positive-deﬁnite ker-
nel permits a feature-map decomposition. However, the decompo-
sition is not unique. One may ﬁnd different feature maps to con-
struct the same kernel function [21, 33]. Therefore, we ask follow-
ing question: do some of the feature maps lead to faster conver-
gence than the others in terms of approximation? In this paper, we
address this question by reconsidering the Random Binning (RB)
feature map, which was proposed in the ﬁrst Random-Feature pa-
per [21] but has drawn much less attentions since then compared to
the RF feature. Our main contributions are fourfold.

First, we propose the ﬁrst analysis of RB from the perspective
of optimization. By interpreting RB as a Randomized Block Coor-
dinate Descent (RBCD) in the inﬁnite-dimensional space induced
from the kernel, we prove that RB enjoys faster convergence than
other random features. Speciﬁcally, by drawing R grids with ex-
pected number of non-empty bins per grid lower bounded by κ,
RB can achieve a solution comparable to exact kernel method with
O(1/(κR)) precision in terms of the objective function, which is
R) rate from Monte Carlo
not only better than the existing O(1/
analysis [21], but also shows a κ times speedup over the rate of
other random features under the same analysis framework [33].

√

Second, we exploit the sparse structure of the feature matrix Z,
which is the key to rapidly transform the data features into a very
high-dimension feature space that is linearly separately by any re-
gressors and classiﬁers. In addition, we discuss how to efﬁciently
perform the computation for a large, sparse matrix by using state-
of-the-art iterative solvers and advanced matrix storage techniques.
As a result, the computational complexity and storage requirements
in training are still O(N Rd + N Rm) and O(N R), respectively.

Third, we show that Random Binning features is particularly
suitable for Parallel Coordinate Descent solver. Unlike other ran-
dom features, RB guarantees a speedup proportional to κ due to
a sparse feature matrix. This is particularly useful in the Sparse
Random Feature setting [33], where L1 regularization is used to
induce a compact nonlinear predictor and Coordinate Descent is
presumably the state-of-the-art solver in such setting.

Finally, we provide extensive experiments to demonstrate the
faster convergence and better parallelizability of RB in practice.
Compared to other popular low-rank approximations, RB shows
superior performance on both regression and classiﬁcation tasks
under the same computational budgets, and achieves same perfor-
mance with one to three orders of magnitude reduction in time and
memory consumptions. When combined with Coordinate Descent
to solve an L1-regularized objective, RB shows an almost linear
speedup, in contrast to RF that has almost no speedup.

2. RANDOM BINNING FEATURE AS KER-

NEL APPROXIMATION

In this work, we consider the problem of ﬁtting a nonlinear pre-
diction function f : X → Y in Reproducing Kernel Hilbert Space
H from training data pairs {(xn, yn)}N
n=1 via regularized Empiri-

cal Risk Minimization (ERM)

f ∗ = argmin

(cid:107)f (cid:107)2

H +

f ∈H

λ
2

1
N

N
(cid:88)

n=1

L(f (xn), yn),

(1)

where L(z, y) is a convex loss function with Lipschitz-continuous
derivative satisfying |L(cid:48)(z1, y) − L(cid:48)(z2, y)| ≤ β|z1 − z2|, which
includes several standard loss functions such as the square-loss
2 (z − y)2, square-hinge loss L(z, y) = max(1 −
L(z, y) = 1
zy, 0)2 and logistic loss L(z, y) = log(1 + exp(−yz)).

2.1 Learning with RKHS

The RKHS H can be deﬁned via a positive-deﬁnite (PD) kernel

function k(x1, x2) that measures similarity between samples as

H =

f (·) =

αik(xi, ·) | αi ∈ R, xi ∈ X

.

(2)

(cid:41)

(cid:40)

K
(cid:88)

i=1

One can also deﬁne the RKHS via a possibly inﬁnite-dimensional
feature map { ¯φh(x)}h∈H with each h ∈ H deﬁning a feature
function ¯φh(x) : X → R. The space can be expressed as

(cid:26)

(cid:90)

H =

f (·) =

h∈H

w(h) ¯φh(·)dh = (cid:104)w, ¯φ(·)(cid:105)H | (cid:107)f (cid:107)2

H < ∞

,

(cid:27)

(3)
where w(h) speciﬁes weights over the set of features {φh(x)}h∈H.
The Mercer’s theorem [19] connects the above two formulations of
RKHS by stating that every PD kernel k(., .) can be expressed as
an integration over some basis functions {φh(.)}h∈H

(cid:90)

h∈H

k(x1, x2) =

p(h)φh(x1)φh(x2)dh = (cid:104) ¯φ(x1), ¯φ(x2)(cid:105)H,

(4)
However, the decomposition (4) is not unique, so one can ﬁnd dif-
ferent feature maps { ¯φh(.)}h∈H satisfying (4) for the same kernel
k(., .). In particular, as an example used extensively in this work,
the Laplacian Kernel

k(x1, x2) = exp

−

(cid:18)

(cid:107)x1 − x2(cid:107)1
σ

(cid:19)

,

(5)

allows decomposition based on (i) Fourier basis map [21], (ii) RB
map [21], and also (iii) map based on inﬁnite number of decision
trees [15] to name a few. On the other hand, different kernels can
be constructed using the same set of basis function {φh(.)} with
different distribution p(h). For example, the RB feature map can
be used to construct any shift-invariant kernel of the form [21]

K(x1, x2) = K(x1 − x2) =

kj(x1j − x2j),

(6)

d
(cid:89)

j=1

by sampling the "width" of bins δj for each feature j from a distri-
bution proportional to δk(cid:48)(cid:48)
j (δ), where k(cid:48)(cid:48)
j (δ) is the second derivative
of kj(δ), assuming the kernel has a non-negative second derivative.

2.2 Random Binning Features

In this section, we describe the Random Binning (RB) feature

map, which has decomposition of the form

K(x1, x2) =

p(δ)φBδ

(x1)T φBδ

(x2) dδ

(7)

(cid:90)

δ

where Bδ is a grid parameterized by δ = (δ1, u1, ..., δd, ud) that
speciﬁes the width and bias of the grid w.r.t. the d dimensions, and
φBδ

(x) is a vector which has

φb(x) = 1, if b = ((cid:98)

(cid:99), ..., (cid:98)

x1 − u1
δ1

xd − ud
δd

(cid:99)),

Figure 1: Generating process of RB features.

Algorithm 1 Random Binning Features

Given a kernel function k(x1, x2) = (cid:81)d
pj(δ) ∝ δk(cid:48)(cid:48)
j (δ) be a distribution over δ.
for r = 1...R do

j=1 kj(|x1j −x2j|). Let

1. Draw δrj ∼ pj(δ), ∀j ∈ [d]. urj ∈ [0, δrj], ∀j ∈ [d].
2. Compute feature zr(xn) as the the indicator vector of bin
index ((cid:98) xn1−u1

(cid:99)), for ∀n ∈ [N ].

(cid:99), ..., (cid:98) xnd−ud

δ1

δd

[z1(xn); ...; zD(xn)] ∀n ∈ [N ] as the

end for.
Return z(xn) = 1√
D
data with RB Features.

and φb(x) = 0 otherwise for any b ∈ Bδ. Note for each grid
Bδ, the number of bins |Bδ| is countably inﬁnite, so φBδ
(x) has
inﬁnite dimension but only 1 non-zero entry (at the bin x lies in).
Figure 1 illustrates an example when the raw dimension d = 2.
The kernel K(x1, x2) is thus interpreted as the collision probabil-
ity that two data points x1, x2 fall in the same bin, when the grid is
generated from distribution p(δ). In [21], it is pointed out for any
kernel of form (6) with nonnegative second derivative k(cid:48)(cid:48)
j (δ), one
can derive distribution p(δ) = (cid:81)d
j=1 pj(δj)U (uj; 0, δj), where
pj(δj) ∝ δk(cid:48)(cid:48)
j (δj) and U (·, a, b) is uniform distribution in the
range [a, b].

To obtain a kernel approximation scheme from the feature map
(7), a simple Monte Carlo method can be used to approximate (7)
by averaging over R grids {Bδr }R
r=1 with each grid’s parameter
δr drawn from p(δ). The procedure for generating R RB features
from raw data {xn}N
n=1 is given in Algorithm 1.
Using a Monte-Carlo analysis, one can show the approximation
R). From the
to (7) yields approximation error of order O(1/
Representer theorem, one can further bound error of the learned
predictor

√

(cid:12)
(cid:12)wT
(cid:12)

RF z(x) − f ∗(x)

n z(xn)T z(x) −
αRF

α∗

nk(xn, x)

N
(cid:88)

n=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

n=1

as shown in [21] (appendix C). Unfortunately, the rate of conver-
gence suggests that to achieve small approximation error (cid:15), one
needs signiﬁcant amount of random features proportional to Ω(1/(cid:15)2),
and furthermore, the Monte-Carlo analysis does not explain why
empirically RB feature achieves faster convergence than other ran-
dom feature map like Fourier basis by orders of magnitude.

3. FASTER CONVERGENCE OF RANDOM

BINNING

In this section, we ﬁrst illustrate the sparse structure of the fea-
ture matrix Z of RB and discuss how to make efﬁcient computa-

Figure 2: Example of the sparse feature matrix ZN ×D gen-
erated by RB. In this special case, Z has the number of rows
N = 200 and columns D = 395, respectively. The number of
grids R = 10 and the nnz(Z) = 2000. Note that for ith row of
Z, nnz(Z(i, :)) = R and R ≤ D ≤ N R.

tion and storage format of Z. Then by interpreting RB features as
Randomized Block Coordinate Descent in the inﬁnite-dimensional
space, we prove that RB has a faster convergence rate than other
random features. We illustrate them accordingly in the following
sections.

3.1 Sparse Feature Matrix & Iterative Solvers
A special characteristic of RB compared to other low-rank ap-
proximations is the fact that the feature matrix generated by RB is
typically a large, sparse binary matrix Z ∈ RN ×D, where the value
of D is determined by both number of grids R and the kernel width
parameter (ex. σ in the case of Laplacian Kernel). Different from
other random features, D, rather than R, is the actual number of
columns of Z. A direct connection between D and R is that the
matrix has each row i satisfying nnz(Z(i, :)) = R and therefore
R ≤ D ≤ N R.
Intuitively speaking, RB has more expressive
power than RF since it generates a large yet sparse feature matrix
to rapidly transform the data space to a very high dimension space,
where data could become almost linearly separable by the classi-
ﬁers. Fig. 2 gives an example to illustrate the sparse structure of
Z.

In the case of Kernel Ridge Regression (L2-regularization with
square loss), if using RB feature to approximate the RKHS, one
can solve (1) directly in its primal form. The weighting vector is
simply the solution of the linear system:

(Z T Z + λI)wRB = Z T y.

(8)

Note since Z is a large sparse matrix, there is no need to explicitly
compute the covariance matrix Z T Z, which is much denser than Z
itself. One can apply state-of-the-art sparse iterative solvers such
as Conjugate Gradient (CG) and GMRES to directly operate on Z
[25]. The main computation in CG or GMRES is the sparse matrix-
vector products. Let m be the number of iterations, then the total
computational complexity of iterative solver is O(m nnz(Z)) =
O(mN R). In addition, since most elements in Z are zeros, the
Compressed Sparse Row type matrix storage format should be em-
ployed for economically storing Z [9], which gives computational
cost and memory requirement as O(mN R) and O(N R) respec-
tively, a similar cost to that of other low-rank approximations de-
spite its much higher dimension. In testing phase, each point pro-
duces a sparse feature vector z(x) ∈ RD based on the grids stored
during training, yielding a sparse vector z(x) with nnz(z(x))) =

R and computing the decision function z(x)T wRB only requires
O(dR + R).

When the ERM is smooth but not quadratic, a Newton-CG method
that solves smooth problem via a series of local quadratic approx-
imation gives the same complexity per CG iteration [14], and note
that most of state-of-the-art linear classiﬁcation algorithms have
complexity linear to nnz(Z), the number of nonzeros of feature
matrix [6]. In section 4, we further discuss cases of L1-regularized
problem, where a Coordinate Descent algorithm of cost O(nnz(Z))
per iteration is discussed.

3.2 Random Binning Features as Block Coor-

dinate Descent

In [33], a new approach of analysis was proposed, which inter-
preted Random Features as Randomized Coordinate Descent in the
inﬁnite dimensional space, and gives a better O(1/R) rate in the
convergence of objective function. In this section, we extend the
approach of [33] to show that, RB Feature can be interpreted as
RBCD in the inﬁnite-dimensional space, which by drawing a block
of features at a time, produces a number of features D signiﬁcantly
more than the number of blocks R, resulting a provably faster con-
vergence rate than other RF. While at the same time, by exploit-
ing state-of-the-art iterative solvers introduced in section 3.1, the
computational complexity of RB does not increase with number of
features D but only with the number of blocks R. Consequently, to
achieve the same accuracy, RB requires signiﬁcantly less training
and prediction time compared to other RF.

A key quantity to our analysis is an upper bound on the collision
probability νδ which speciﬁes how unlikely data points will fall
into the same bin, and its inverse κδ := 1/νδ which lower bounds
the number of bins containing at least one data point. We deﬁne
them as follows.

Deﬁnition 1. Deﬁne collision probability of data D on bin b ∈ Bδ
as

νb :=

|{n ∈ [N ] | φb(xn) = 1}|
N

.

(9)

Let νδ := maxb∈Bδ νb be an upper bound on (9), and κδ := 1/νδ
be a lower bound on the number of nonempty bins of grid δ.

κ := Eδ[κδ] = Eδ[1/νδ]

(10)

is denoted as the lower bound on the expected number of (used)
bins w.r.t the distribution p(δ).

In the RB matrix, the empirical collision probability is simply the
average number of non-zeros per column, divided by N , a number
much smaller than 1 as in the example of Fig. 2. Our analysis
assumes a smooth loss function satisfying the following criteria.

Assumption 1. The loss function L(z, y) is smooth w.r.t. response
z so difference between function difference and its linear approxi-
mation can be bounded as

where Loss( ¯w; φ) = 1
N

(cid:80)N

n=1 L((cid:104) ¯w, φ(xn)(cid:105), yn) and

¯φ :=

(.))δ∈H

√
p ◦ φ = ((cid:112)p(δ)φBδ
with "◦" denoting the component-wise product. The goal is to show
that, by performing R steps of FC-RBCD on (11), one can obtain a
¯wR with comparable regularized loss to that from optimal solution
of (1). Note one advantage of analysis from this optimization per-
spective is: it does not rely on Representer theorem, and thus R(w)
2 (cid:107)w(cid:107)2 or L1 regularizer λ(cid:107)w(cid:107)1, where the
can be L2 regularizer λ
latter has advantage of giving sparse predictor of faster prediction
[33]. The FC-RBCD algorithm maintains an active set of blocks
A(r) which is expanded for R iterations. At each iteration r, the
FC-RBCD does the following:

1. Draw δ from p(δ) ( derived from the kernel k(., .) ).

2. Expand active set A(r+1) := A(r) ∪ Bδ.

3. Minimize (11) subject to a limited support supp( ¯w) ⊆ A(r+1).

Note this algorithm is only used for analysis. In practice, one
can draw R blocks of features at a time, and solve (11) by any
optimization algorithm such as those mentioned in section 3.1 or
the CD method we introduce in section 4.

Due to space limit, here we prove the case when R(.) is the
non-smooth L1 regularizer λ(cid:107) ¯w(cid:107)1. The smooth case for R(w) =
2 (cid:107) ¯w(cid:107)2 can be shown in a similar way. Note the objective function
λ
(11) can be written as
¯F (w) := F (

p ◦ w) + Loss(w, ¯φ).

p ◦ w) = R(

(12)

√

√

√

by a scaling of variable ¯w =

p ◦ w.
The below theorem states that, running FC-RBCD for R itera-
tions, it generates a solution ¯wR close to any reference solution w∗
in terms of objective (12) with their difference bounded by O( 1
κR ).

Theorem 1. Let R be the number of blocks (grids) generated by
FC-RBCD, and w∗ be any reference solution, we have

E[ ¯F (w(R))] − ¯F (w∗) ≤

(13)

β(cid:107)w∗(cid:107)2
κR(cid:48)

for R(cid:48) := R − c > 0, where c = (cid:100) 2κ( ¯F (0)− ¯F (w∗))

(cid:101).

β(cid:107)w∗(cid:107)2

Proof. Firstly, we obtain an expression for the progress made by
each iteration of FC-RBCD. Let B := Bδ(r) be the block drawn at
step 1 of FC-RBCD, and ¯w(r+1) be the minimizer of (11) subject
to support supp( ¯w) ⊆ A(r+1) given by the step 3. Since B ⊆
A(r+1), we have

F ( ¯w(r+1)) − F ( ¯w(r)) ≤ F ( ¯w(r) + ηB) − F ( ¯w(r))

(14)

for any ηB : supp(η) ⊆ B. Then denote bi as the bin xi falling
i = ∇L( ¯w(r)T φ(xi), yi), by smoothness of the loss (As-
in and L(cid:48)
sumption 1), we have

L(z2, .) − L(z1, .) ≤ ∇L(z1, .)(z2 − z1) +

(z2 − z2)2.

Loss( ¯w(r) + ηB) − Loss( ¯w(r)) ≤

L(cid:48)

iφbi η +

(ηbi φbi )2

β
2

1
N

N
(cid:88)

i=1

β
2

for some constant 0 ≤ β ≤ ∞.

This assumption is satisﬁed for a wide range of loss such as
square loss (β = 1), logistic loss (β = 1/4) and L2-hinge loss
(β = 1).

We interpret RB as a Fully Corrective Randomized Block Coor-

dinate Descent (FC-RBCD) on the objective function

min
¯w

F ( ¯w) := R( ¯w) + Loss( ¯w; φ)

(11)

≤ (cid:104)gB, ηB(cid:105) +

βνδ(r)
2

(cid:107)ηB(cid:107)2

where the second inequality uses the fact φbi = 1 and

gB := ∇BLoss( ¯w(r), φ).
Now consider the regularization term, note since block B is drawn
from an iniﬁnite-dimensional space, the probability that B is in

(15)

active set is 0. Therefore, we have B ∩ A(r) = ∅, ¯w(r)
RB( ¯w(r)

B ) = 0. As a result,

B = 0 and

F ( ¯w(r) + ηB) − F ( ¯w(r))

≤ RB(ηB) + (cid:104)gB, ηB(cid:105) +

βνδ(r)
2

(cid:107)ηB(cid:107)2

(16)

Let ηB be the minimizer of RHS of (16). It satisﬁes ρB + gB +
βvδ(r) ηB = 0 for some ρB ∈ ∂R(ηB), and thus,

F ( ¯w(r) + ηB) − F ( ¯w(r))

≤ (cid:104)ρB, ηB(cid:105) + (cid:104)gB, ηB(cid:105) +

= −

1
2βνδ(r)

(cid:107)ρB + gB(cid:107)2

βνδ(r)
2

(cid:107)ηB(cid:107)2

(17)

Now taking expectation w.r.t. p(δ) on both sides of (17), we have

E[F ( ¯w(r) + ηB)] − F ( ¯w(r)) ≤ −

(cid:21)

(cid:107)ρB + gB(cid:107)2
(cid:21)

E (cid:2)(cid:107)ρB + gB(cid:107)2(cid:3)

E

(cid:20) 1
νδ(r)
(cid:20) 1
νδ(r)
(cid:107)¯ρB + ¯gB(cid:107)2

E

1
2β

1
2β
κ
2β

≤ −

≤ −

√

√

p ◦ ρ, ¯g :=

(18)
where ¯ρ :=
p ◦ g, and the second inequality
uses the fact that the number of used bins κδ(r) = 1/νδ(r) has
non-negative correlation with the discriminative power of block
B measured by the magnitude of gradient with soft-thresholding
(cid:107)¯ρB + ¯gB(cid:107) (i.e.
fewer collisions on grid B implies B to be a
better block of features ).

The result of (18) expresses descent amount in terms of the prox-
imal gradient of the reparameterized objective (12). Note for B :
B ∩A(r) = ∅, we have w(r)
B = 0, and RB(¯η)−RB(0) = (cid:104)¯ρ, ¯η(cid:105);
on the other hand, for B ⊆ A(r), we have

0 ∈ arg min
¯ηB

√

√

RB(

pBwB + ¯ηB) + (cid:104)¯gB,

pBwB + ¯ηB(cid:105)

since they are solved to optimality in the previous iteration. Then

E[F ( ¯w(r) + η)] − F ( ¯w(r))

where the second and fourth inequalities are from convexity of
¯F (.). The α minimizing (20) is α∗ := min( κ( ¯F (w(r))− ¯F (w∗))
, 1),
which leads to

β(cid:107)w∗(cid:107)2

E[ ¯F (w(r) + ¯η)] − ¯F (w(r)) ≤ −

κ( ¯F (w(r)) − ¯F (w∗))2
2β(cid:107)w∗(cid:107)2

(21)
κ (cid:107)w∗(cid:107)2; otherwise, we have E[ ¯F (w(r) +
if ¯F (w(r))− ¯F (w∗) ≤ β
¯η)] − ¯F (w(r)) ≤ − β
2κ (cid:107)w∗(cid:107)2. Note the latter case cannot happen
more than c = (cid:100) 2κ( ¯F (0)− ¯F (w∗))
(cid:101) times since FC-RBCD is a de-
scent method. Therefore, for r(cid:48) := r −c > 0, solving the recursion
(21) leads to the conclusion.

β(cid:107)w∗(cid:107)2

√

√

Note we have (cid:107)

p ◦ w∗(cid:107)1 ≤ (cid:107)

p(cid:107)(cid:107)w∗(cid:107) = (cid:107)w∗(cid:107) in the L1-
regularized case, and thus the FC-RBCD guarantees convergence
of the L1-norm objective to the (non-square) L2-norm objective.
The convergence result of Theorem 1 is of the same form to the
rate proved in [33] for other random features, however, with an
additional multiplicative factor κ ≥ 1 that speeds up the rate by
κ times. Recall that κ is the lower bound on the expected number
of bins being used by data samples for each block of features Bδ,
which in practice is a factor much larger than 1, as shown in the
Figure 2 and also in our experiments. In particular, in case each
grid Bδ has similar number of bins being used, we have D ≈ κR,
and thus obtain a rate of the form

E[ ¯F (w(R))] − ¯F (w∗) (cid:46) β(cid:107)w∗(cid:107)2

.

D

(22)

Note for a ﬁxed R, the total number of features D is increasing with
kernel parameter 1/σ in the case of Laplacian Kernel, which means
the less smooth the kernel, the faster convergence of RB. A simple
extreme case is when σ → 0, where one achieves 0 training loss,
and the RB, by putting each sample in a separate bin, converges
to 0 loss with R = 1, D = N . On the other hand, other random
features, such as Fourier, still require large R for convergence to
0 loss. In practice, there are many data that require a small kernel
bandwidth σ to avoid underﬁtting, for which RB has dramatically
faster convergence than other RF.

β
2κ

κ
2β
√

≤ −

(cid:107)¯ρB + ¯gB(cid:107)2 = (cid:104)¯ρ, ¯η(cid:105) + (cid:104)¯g, ¯η(cid:105) +

(cid:107)¯η ¯A(r) (cid:107)2

4. STRONG PARALLELIZABILITY OF RAN-

√

= R(

p ◦ w(r)) + (cid:104)¯g, ¯η(cid:105) +

p ◦ (w(r) + ¯η)) − R(

(cid:107)¯η ¯A(r) (cid:107)2
(19)
where ¯η ¯A(r) := (¯ηB)B:B∩A(r)=∅ and ¯η :=
p◦η. Thus the ﬁnal
step is to show the descent amount given by RHS of (19) decreases
the suboptimality ¯F (w(r)) − ¯F (w∗) signiﬁcantly. This can be
achieved by considering ¯η of the form α(w∗ − w(r)) for some
α ∈ [0, 1] as follows:

√

β
2κ

DOM BINNING FEATURES

In this section, we study another strength of RB Features in the
context of Sparse Random Feature [33], where one aims to train a
sparse nonlinear predictor that has faster prediction and more com-
pact representation through an L1-regularized objective.
In this
case, the CD method is known as state-of-the-art solver [23, 34],
and we aim to show that the structure of RB allows CD to be paral-
lelized with much more speedup than that of other random features.

E[ ¯F (w(r) + ¯η)] − ¯F (w(r))

≤ min

R(

p ◦ (w(r) + ¯η)) − R(

p ◦ w(r)) + (cid:104)¯g, ¯η(cid:105) +

√

√

≤ min

¯F (w(r) + ¯η) − ¯F (w(r)) +

β
2κ

(cid:107)¯η ¯A(r) (cid:107)2

¯F ((1 − α)w(r) + αw∗) − ¯F (w(r)) +

(cid:107)w∗(cid:107)2

βα2
2κ

−α( ¯F (w(r)) − ¯F (w∗)) +

(cid:107)w∗(cid:107)2,

βα2
2κ

¯η

¯η

≤ min
α∈[0,1]

≤ min
α∈[0,1]

4.1 Coordinate Descent Method

β
2κ

(cid:107)¯η ¯A(r) (cid:107)2

a RCD Method solves

Given the N × D data matrix produced by the RB Algorithm 1,

min
w∈RD

λ(cid:107)w(cid:107)1 +

1
N

N
(cid:88)

n=1

L(wT zi, yi)

(23)

by minimizing (23) w.r.t. a single coordinate j

(20)

min
dj

λ|wj + dj| + gjdj +

Mj
2

d2
j

(24)

Algorithm 2 Sparse Random Binning Features via Parallel RCD

0. Generate RB feature matrix Z by Algorithm 1
1. z1 = 0, w1 = 0.
for t=1......T (with τ threads in parallel) do
2. Draw j from [D] uniformly at random.
3. Compute d∗
4. wt+1 := wt + d∗
5. Maintain ˆyi, ∀i ∈ [N ] to satisfy (27).

j by (26).
j ej.

end for

at a time, where

gj :=

(∇jL(wT zi, yi))zij

(25)

1
N

N
(cid:88)

n=1

(cid:80)N

i=1 z2

is the gradient of loss term in (23) w.r.t. the j-th coordinate, and
Mj := β 1
ij is an upper bound on ∇jjL(.). Note, by
N
focusing on single coordinate, (24) has a tighter quadratic upper
bound than other algorithms such as Proximal Gradient Method,
and allows simple closed-form solution

d∗
j := proxR/Mj

(wj −

gj
Mj

) − wj

(26)

where

proxR(vj) :=






0,
vj − λ,
vj + λ,

|vj| ≤ λ
vj > λ
vj < λ

.

To have efﬁcient evaluation of the gradient (25), a practical imple-
mentation maintain the responses

ˆyi := wT zi

(27)

j ej, so the cost for each coordinate-

after each update wt+1 := wt+d∗
wise minimization takes O(nnz(zj)) time for both gradient evalu-
ation and maintenance of (27), where zj := (zij)i∈[N ]. The algo-
rithm is summarized in Alg. 2, which just like the iterative solver
introduced in section 3.1, has cost O(nnz(Z)) for one pass of all
variables j ∈ [D].

4.2 Parallel Randomized Coordinate Descend

on Random Binning Features

The RCD, however, is hard to parallelize [23]. It is known that
simultaneous updates of two coordinates j1, j2 could lead to di-
vergence, and although one can enforce convergence by shortening
the step size 1
, the convergence rate will not be improved
Mp
with parallelization without additional assumption [1, 24].

(cid:28) 1
Mj

On the other hand, in [24], it is shown that a function with par-

tially separable smooth term plus a separable non-smooth term

min
w∈RD

F (w) := Ω(w) +

fi(w)

(28)

N
(cid:88)

i=1

can be parallelized with guaranteed speedup in terms of overall
complexity, where Ω(w) is a non-smooth separable function and
each function fi(w) is a smooth depends only on at most ω num-
ber of variables. The form (28), fortunately, ﬁts our objective (23)
with features zi generated by RB. In particular, the generating pro-
cess of RB guarantees that, for each block of feature Bδ, the i-th
sample can fall in exactly one bin b = ((cid:98) xn1−u1
(cid:99)),
therefore each sample inolves at most R features out of D. Specif-
ically, let Ω(w) := λ(cid:107)w(cid:107)1 and

(cid:99), ..., (cid:98) xnd−ud

δd

δ1

fi(w) :=

L(wT zi, yi),

1
N

we have ω = R. Then by Theorem 19 of [24], a parallel RCD of
τ threads that selects coordinate j uniformly at random achieves a
speed-up (i.e. time-of-sequential/time-of-parallel) of

When D, R (cid:29) 1, and τ = a¯κ+1 where ¯κ := D/R, (29) becomes

speedup-ratio =

τ
1 + (R−1)(τ −1)

D−1

.

speedup-ratio =

a¯κ + 1
1 + a

,

(29)

(30)

which equals (¯κ + 1)/2 when a = 1 and approaches ¯κ when a →
∞. Therefore, it is guaranteed in theory that parallelization can
speedup RCD signiﬁcantly as long as ¯κ = D/R (cid:29) 1. We give
our sparse RB Features algorithm based on parallel RCD in Alg.
2. Note for other Random Features, there is no speedup guaranteed
and our experiment shows that Parallel RCD performed on Random
Fourier features could even have no speedup.

Note that the speedup achieved in this section is orthogonal to
the faster convergence rate achieved in section 3, so by increas-
ing κ, the advantage of RB over other Random Features is super-
linearly increasing if a parallel RCD is used. Note also that the
results (29), (30) also apply to algorithms that utilize Coordinate
Descent as subproblem solvers such as Proximal (Quasi) Newton
Method [13, 35]. Those methods are typically employed for com-
putationally expensive loss functions.

5. EXPERIMENTS

In this section, we present extensive sets of experiments to demon-
strate the efﬁciency and effectiveness of RB. The datasets are cho-
sen to overlap with those in other papers in the literature, where the
details are shown in the table 1. All sets except census are avail-
able at LIBSVM data set [2]. All computations are carried out on a
DELL dual socket with Intel Xeon processors at 2.93GHz for a total
of 16 cores and 250 GB of memory running the SUSE Linux op-
erating system. We implemented all methods in C++ and all dense
matrix operations are performed by using the optimized BLAS and
LAPACK routines provided in the OpenBLAS library. Due to the
limited space, we only choose subsets of our results to present in
each subsection. However, these results are objective and unbiased.

Name
cadata
census
ijcnn1
cod_rna
covtype
SUSY
mnist
acoustic
letter

Table 1: Properties of the datasets.
C: Classes
1
1
2
2
2
2
10
3
26

d: Features N : Train M : Test
16,512
18,186
35,000
49,437
464,809
4,000,000
60,000
78,823
10,500

4,128
2,273
91,701
271,617
116,203
1,000,000
10,000
19,705
5,000

8
119
22
8
54
18
780
50
16

5.1 Effects of σ and R on Random Binning

We perform experiments to investigate the characteristics of RB
by varying the kernel parameter λ and the rank R, respectively.
We use a regularization λ = 0.01 to make sure the reasonable
performance of RB and other low-rank kernels, although we found
that RB is not sensitive to this parameter. We increase the σ in the
large interval from 1e−2 to 1e2 so that the optimal σ locates within
the interval. We apply CG iterative solver to operate on Z directly.

In order to make fair runtime comparison in each run, we set the
tol = 1e − 15 to force similar CG iterations with different σ.

We evaluate the training and testing performance of regression
and classiﬁcation, when varying σ with ﬁxed R. In [21], it does
not consider the effect of σ in their analysis, which however has a
large impact on the performance since D depends on the number
of bins which is controlled by σ. Fig. 3 shows that the training
and testing performance coincidentally decrease (increase) before
they diverge when D grows by increasing σ. This conﬁrms with
our analysis in Theorem 1 that the larger κ, the faster convergence
of RB Feature (recall that the convergence rate is O(1/(κR))).

Second, one should not be surprised that the empirical training
time increases with D. The operations involving the weighting vec-
tor wRB could become as expensive as a sparse matrix-vector oper-
ation in an iterative solver. However, the total computational costs
are still bounded by O(N R) but the constant factor may vary with
different datasets. Fortunately, in most of cases, the training time
corresponding to the peak performance is just slightly higher than
the smallest one. In practice, there are several ways to improve the
computation costs by exploiting more advanced sparse matrix tech-
niques such as preconditioning and efﬁcient storage scheme, which
is out scope of this paper and left for future study.

Finally, we evaluate the training and testing performance when
varying R with ﬁxed σ. Fig.4 shows that the training and testing
performance converge almost linearly with D, which again con-
ﬁrms our analysis in Theorem 1. In addition, we observe that RB
has strong overﬁt ability which turns out to be a strong attribute,
especially when the hypothesis space has not yet saturated.

5.2 Performance Comparisons of All Methods
We present a large sets of experiments to compare RB with other
most popular low-rank kernel approximations, including RF [21],
Nyström [31], and recently proposed independent block approxi-
mation [29]. We also compare all methods with the exact kernel as
a benchmark [26]. We do not report the results of the vanilla ker-
nel on covtype and SUSY since the programs run out of memory.
To make a fair comparison, we also apply CG on RB and Nys-
tröm directly on Z to admit similar computational costs. Since the
independent block kernel approximation approximates the kernel
matrix directly, we employ direct solver of dense matrix for this
method. In practice, the CG iterative solver has no need to solve
in high precision [3], which has also been observed in our experi-
ments. Thus, we set the tolerance to 1e − 3.

Fig.5 clearly demonstrates the superiority of RB compared to
other low-rank kernels. For example, in the ﬁrst column, RB sig-
niﬁcantly outperforms other methods in testing performance on all
of these datasets, especially when R is relatively small. This is
because RB enjoys much faster convergence rate to the optimal
function than other methods. The advantage generally diminishes
when R increases to reasonably large. However, for some large
datasets such as covtype and SUSY, increasing number of random
features or R boosts the performance extremely slow. This is con-
sistent with our analysis that RB enjoys its fast convergence rate
of O(1/(κR)) while other methods has slow convergence rates
R). The third and fourth columns further promote the in-
O(1/
sights about how many number of random features or how large
rank R that is needed for achieving similar performance of RB. In
particular, RB is often between one and three orders of magnitude
faster and less memory consumptions than other methods.

√

In the second column, we also observe that the training time of
all low-rank kernels are linear with R, which is expected since all
these methods has computational complexity of O(kN R). The
difference in training time between these low-rank kernels is only

within some constant factors. However, we point out that the com-
putations of RF, Nyström and independent block approximation are
mainly carried out by the high-optimized BLAS library since they
are dense matrices. In contrast, the computations of RB are most
involved in sparse matrix operations, which are self-implemented
and not yet optimized. In addition, more advanced sparse matrix
techniques such as preconditioning can be explored to signiﬁcantly
accelerate the computation, which we leave it as future work.

5.3 Parallel Performance of Random Binning

and Random Fourier

We perform experiments to compare RB with RF when using
RCD to solve L1-regularized Lasso and kernel SVM for both re-
gression and binary classiﬁcation problems. Since the goal is to
demonstrate the strong parallel performance of RB, we implement
the basic parallel implementation of RCD based on simple shared
memory parallel programming model with OpenMP. We leave the
high-performance distributed RCD implementation as one of the
future works. We deﬁne the speedup of RCD on multicore imple-
mentation as follows:

speedup =

runtime of RCD using single core
runtime using P cores

As shown in Fig.6, when the sparsity level of the feature matrix Z
is high, the near-linear speedup can be achieved [16, 18]. This is
because the minimization problem can almost be separated along
the coordinate axes, then higher degrees of parallelism are possi-
ble. In contrast, if Z is lack of sparsity, then the penalty for data
correlations slows the speedup to none. This is conﬁrmed by no
gain of parallel speedup of RF since Z is always fully dense. Ob-
viously, in order to empower strong parallel performance of RB, a
very large D is expected, which interestingly coincides with power
of its faster convergence. Therefore, one can enjoy the double ben-
eﬁts of fast convergence and strong parallelizability of RB, which
is especially useful for very large-scale problems.

6. CONCLUSIONS

In this paper, we revisit RB features, an overlooked yet very
powerful random features, which we observe often to be orders
of magnitude faster than other random features and kernel approx-
imation methods to achieve the same accuracy. Motivated by these
impressive empirical results, we propose the ﬁrst analysis of RB
from the perspective of optimization, to make a solid attempt to
quantify its faster convergence, which is not captured by tradi-
tional Monte-Carlo analysis. By interpreting RB as a RBCD in
the inﬁnite-dimensional space, we show that by drawing R grids
with at least κ expected number of non-empty bins per grid, RB
achieves a convergence rate of O(1/(κR)). In addition, in the L1-
regularized setting, we demonstrate the sparse structure of RB fea-
tures allows RCD solver to be parallelized with guaranteed speedup
proportional to κ. Our extensive experiments demonstrate the su-
perior performance of the RB features over other random feature
and kernel approximation methods.

7. ACKNOWLEDGEMENT

This work was done while L. Wu was a research intern at IBM
Research. J. Chen is supported in part by the XDATA program
of the Advanced Research Projects Agency (DARPA), adminis-
tered through Air Force Research Laboratory contract FA8750-12-
C-0323.

(a) cadata

(b) cadata

(c) census

(d) census

(e) ijcnn1

(f) ijcnn1

(g) covtype

(h) covtype

(i) SUSY

(j) SUSY

(k) mnist

(l) mnist

(m) acoustic

(n) acoustic

(o) letter

(p) letter

Figure 3: Train and test performance, and train time when varying σ with ﬁxed R. The black line and square box represent the best
test performance of the exact kernel and RB respectively.

(a) cadata

(b) ijcnn1

(c) acoustic

(d) letter

Figure 4: Train and test performance when varying R with ﬁxed σ.

References
[1] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel
coordinate descent for l1-regularized loss minimization. CoRR,
abs/1105.5379, 2011.

[2] C. Chang and C. Lin. Libsvm: a library for support vector machines.

ACM Transactions on Intelligent Systems and Technology,
2:27:1–27:27, 2011.

[3] J. Chen, L. Wu, K. Audhkhasi, B. Kingsbury, and B. Ramabhadran.
Efﬁcient one-vs-one kernel ridge regression for speech recognition.

[24] P. Richtárik and M. Takáˇc. Parallel coordinate descent methods for

big data optimization. Mathematical Programming, pages 1–52,
2015.

[25] Y. Saad. Iterative Methods for Sparse Linear Systems. Society for
Industrial and Applied Mathematics, Philadelphia, PA, USA, 2nd
edition, 2003.

[26] B. Scholkopf and A. J. Smola. Learning with Kernels: Support

Vector Machines, Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA, 2001.

[27] S. Si, C. Hsieh, and I. S. Dhillon. Memory efﬁcient kernel

approximation. In ICML, 2014.

[28] A. J. Smola and B. Schökopf. Sparse greedy matrix approximation

for machine learning. In ICML, ICML ’00, San Francisco, CA, USA,
2000. Morgan Kaufmann Publishers Inc.

[29] M. L. Stein. Limitations on low rank approximations for covariance

matrices of spatial data. Spatial Statistics, 8:1 – 19, 2014. Spatial
Statistics Miami.

[30] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks.

In NIPS. MIT Press, 2004.

[31] C. K. I. Williams and M. Seeger. Using the nyström method to speed

up kernel machines. In NIPS. MIT Press, 2001.

[32] Z. Yang, A. G. Wilson, A. J. Smola, and L. Song. A la carte -

learning fast kernels. In AISTATS, 2015.

[33] I. E.-H. Yen, T.-W. Lin, S.-D. Lin, P. K. Ravikumar, and I. S. Dhillon.
Sparse random feature algorithm as coordinate descent in hilbert
space. In JMLR, 2014.

[34] G.-X. Yuan, K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. A comparison
of optimization methods and software for large-scale l1-regularized
linear classiﬁcation. JMLR, 11:3183–3234, 2010.

[35] K. Zhong, I. E.-H. Yen, I. S. Dhillon, and P. K. Ravikumar. Proximal

quasi-newton for computationally intensive l1-regularized
m-estimators. In Advances in Neural Information Processing
Systems, pages 2375–2383, 2014.

In ICASSP, 2016.

[4] B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. F. Balcan, and

L. Song. Scalable kernel methods via doubly stochastic gradients. In
NIPS. Curran Associates, Inc., 2014.

[5] P. Drineas and M. W. Mahoney. On the nyström method for

approximating a gram matrix for improved kernel-based learning.
JMLR, 6:2153–2175, Dec. 2005.

[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.

Liblinear: A library for large linear classiﬁcation. JMLR,
9:1871–1874, 2008.

[7] S. Fine and K. Scheinberg. Efﬁcient svm training using low-rank

kernel representations. JMLR, 2:243–264, Mar. 2002.

[8] A. Gittens and M. W. Mahoney. Revisiting the nyström method for

improved large-scale machine learning. CoRR, abs/1303.1849, 2013.

[9] G. H. Golub and C. F. Van Loan. Matrix Computations. Johns

Hopkins University Press, Baltimore, MD, USA, 1996.

[10] P.-S. Huang, H. Avron, T. Sainath, V. Sindhwani, and

B. Ramabhadran. Kernel methods match deep neural networks on
timit. In ICASSP, 2014.

[11] S. Kumar, M. Mohri, and A. Talwalkar. Sampling methods for the

nyström method. JMLR, 13:981–1006, Apr. 2012.

[12] Q. V. Le, T. Sarlós, and A. J. Smola. Fastfood: Approximate kernel

expansions in loglinear time. ICML, 2013.

[13] J. Lee, Y. Sun, and M. Saunders. Proximal newton-type methods for
convex optimization. In Advances in Neural Information Processing
Systems, pages 836–844, 2012.

[14] C.-J. Lin, R. C. Weng, and S. S. Keerthi. Trust region newton method

for logistic regression. JMLR, 9:627–650, 2008.

[15] H.-T. Lin and L. Li. Support vector machinery for inﬁnite ensemble

learning. JMLR, 9:285–312, 2008.

[16] J. Liu, S. J. Wright, C. Ré, and V. Bittorf. An asynchronous parallel
stochastic coordinate descent algorithm. JMLR, 32(1):469–477,
2014.

[17] Z. Lu, A. May, K. Liu, A. B. Garakani, D. Guo, A. Bellet, L. Fan,

M. Collins, B. Kingsbury, M. Picheny, and F. Sha. How to scale up
kernel methods to be as good as deep neural nets. CoRR,
abs/1411.4000, 2014.

[18] J. Mareˇcek, P. Richtárik, and M. Takáˇc. Distributed block coordinate
descent for minimizing partially separable functions. Numerical
Analysis and Optimization, 134:261–288, 2014.

[19] J. Mercer. Functions of positive and negative type, and their

connection with the theory of integral equations. Royal Society
London, A 209:415–446, 1909.

[20] M. Raginsky and S. Lazebnik. Locality-sensitive binary codes from
shift-invariant kernels. In Y. Bengio, D. Schuurmans, J. D. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances in Neural
Information Processing Systems 22, pages 1509–1517. Curran
Associates, Inc., 2009.

[21] A. Rahimi and B. Recht. Random features for large-scale kernel

machines. In NIPS. Curran Associates, Inc., 2007.

[22] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks:
Replacing minimization with randomization in learning. In NIPS.
Curran Associates, Inc., 2008.

[23] P. Richtárik and M. Takáˇc. Iteration complexity of randomized
block-coordinate descent methods for minimizing a composite
function. Mathematical Programming, 144(1-2):1–38, 2014.

(a) cadata

(b) cadata

(c) cadata

(d) cadata

(e) ijcnn1

(f) ijcnn1

(g) ijcnn1

(h) ijcnn1

(i) covtype

(j) covtype

(k) covtype

(l) covtype

(m) SUSY

(n) SUSY

(o) SUSY

(p) SUSY

(q) mnist

(r) mnist

(s) mnist

(t) mnist

(u) acoustic

(v) acoustic

(w) acoustic

(x) acoustic

Figure 5: Comparisons among RB, RF, Nyström and Independent Block approximation. The ﬁrst and second columns plot test
performance and train time when increasing R. The third and fourth columns plot the train time and memory consumptions when
achieving the desired test performance.

(a) ijcnn1

(b) cod_rna

(c) covtype

(d) SUSY

Figure 6: Comparisons of parallel performance between RB and RF using RCD when increasing the number of threads.

8
1
0
2
 
p
e
S
 
9
1
 
 
]

G
L
.
s
c
[
 
 
2
v
7
4
2
5
0
.
9
0
8
1
:
v
i
X
r
a

Revisiting Random Binning Features: Fast Convergence
and Strong Parallelizability

∗

Lingfei Wu
College of William and Mary
Williamsburg, VA 23185
lfwu@cs.wm.edu
Jie Chen
IBM Research
Yorktown Heights, NY 10598
chenjie@us.ibm.com

Ian E.H. Yen *
Unversity of Texas at Austin
Austin, TX 78712
ianyen@cs.utexas.edu

Rui Yan
Baidu Inc.
Beijing 100085, China
yanrui02@baidu.com

ABSTRACT
Kernel method has been developed as one of the standard approaches
for nonlinear learning, which however, does not scale to large data
set due to its quadratic complexity in the number of samples. A
number of kernel approximation methods have thus been proposed
in the recent years, among which the random features method gains
much popularity due to its simplicity and direct reduction of non-
linear problem to a linear one. Different random feature functions
have since been proposed to approximate a variety of kernel func-
tions. Among them the Random Binning (RB) feature, proposed in
the ﬁrst random-feature paper [21], has drawn much less attention
than the Random Fourier (RF) feature proposed also in [21]. In
this work, we observe that the RB features, with right choice of op-
timization solver, could be orders-of-magnitude more efﬁcient than
other random features and kernel approximation methods under the
same requirement of accuracy. We thus propose the ﬁrst analysis
of RB from the perspective of optimization, which by interpret-
ing RB as a Randomized Block Coordinate Descent in the inﬁnite-
dimensional space, gives a faster convergence rate compared to that
of other random features.
In particular, we show that by draw-
ing R random grids with at least κ number of non-empty bins
per grid in expectation, RB method achieves a convergence rate
R) rate from
of O(1/(κR)), which not only sharpens its O(1/
Monte Carlo analysis, but also shows a κ times speedup over other
random features under the same analysis framework. In addition,
we demonstrate another advantage of RB in the L1-regularized set-
ting, where unlike other random features, a RB-based Coordinate
Descent solver can be parallelized with guaranteed speedup pro-
portional to κ. Our extensive experiments demonstrate the superior
performance of the RB features over other random features and
kernel approximation methods. Our code and data is available at
https://github.com/teddylfwu/RB_GEN.

√

∗Both authors contributed equally to this manuscript

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’16, August 13 - 17, 2016, San Francisco, CA, USA
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4232-2/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2939672.2939794

Keywords
Kernel approximation, Random Binning Features, large-scale ma-
chine learning, faster convergence, strong parallelizability

1.

INTRODUCTION

Kernel methods have great promise for learning non-linear model
from simple data input representations and have been demonstrated
successful for solving various learning problems such as regres-
sion, classiﬁcation, feature extraction, clustering and dimensional-
ity reduction [26, 30]. However, they are typically not ﬁrst choice
for large-scale nonlinear learning problems, since large number of
samples (N ) presents signiﬁcant challenges in terms of computa-
tion and memory consumptions to Kernel methods for computing
the dense kernel matrix K ∈ RN ×N which requires at least a
O(N 2) complexity. To scale up the kernel methods, there have
been great efforts addressing this challenge from various perspec-
tives such as numerical linear algebra, sampling approximation, op-
timization and functional analysis [4, 5, 7, 21, 27, 33].

√

√

R + 1/

A line of research [7, 27, 28, 31] approximates the kernel ma-
trix K using low-rank factorizations, K ≈ Z T Z, where Z ∈
RN ×R matrix with R (cid:28) N . Among them, Nyström method
[5, 8, 11, 27, 31] is probably one of the most popular approaches,
which reduces the total computational costs to O(N Rd + N R2 +
R3) or O(N Rd + N Rm) depending whether the algorithm per-
forms on K explicitly or implicitly through Z, where d and m are
the input data dimension and the number of iterations of an iterative
solver respectively. However, the convergence of the low-rank ap-
N ) [5, 33], which
proximation is proportional to O(1/
implies that the rank R may need to be near-linear to the number
of data points in order to achieve comparable generalization error
compared to the vanilla kernel method. For large-scale problems,
the low-rank approximation could become almost as expensive as
the exact kernel method to maintain competitive performance [29].
Another popular approach for scaling up kernel method is ran-
dom features approximation [21, 22]. Unlike previous approach
that approximates kernel matrix, Random Features approximate the
kernel function directly via sampling from an explicit feature map.
Random Fourier (RF) is one of the feature maps that attracted con-
siderable interests due to its easy implementation and fast execution
time [4, 12, 20, 22, 32], which has total computational cost and stor-
age requirement as O(N Rd + N Rm) and O(N R) respectively,
for computing feature matrix Z and operating the subsequent algo-
rithms on Z. A Fastfood approach and its extension [12, 32] was

proposed to reduce the time of computing Fourier features from
O(Rd) to O(R log d) by leveraging Hadamard basis functions,
which improves the efﬁciency for prediction but not necessarily for
training if d (cid:28) m. Although RF has been successfully applied to
speech recognition and vision classiﬁcations on very large datasets
[3, 10, 17], a drawback is that a signiﬁcant large number of random
features are needed to achieve a comparable performance to exact
kernel method. This is not surprising since the convergence of ap-
N ) [4, 22], which
proximation error is in the order O(1/
is the same as that of low-rank kernel approximations.

R+1/

√

√

Mercer’s theorem [19] guarantees that any positive-deﬁnite ker-
nel permits a feature-map decomposition. However, the decompo-
sition is not unique. One may ﬁnd different feature maps to con-
struct the same kernel function [21, 33]. Therefore, we ask follow-
ing question: do some of the feature maps lead to faster conver-
gence than the others in terms of approximation? In this paper, we
address this question by reconsidering the Random Binning (RB)
feature map, which was proposed in the ﬁrst Random-Feature pa-
per [21] but has drawn much less attentions since then compared to
the RF feature. Our main contributions are fourfold.

First, we propose the ﬁrst analysis of RB from the perspective
of optimization. By interpreting RB as a Randomized Block Coor-
dinate Descent (RBCD) in the inﬁnite-dimensional space induced
from the kernel, we prove that RB enjoys faster convergence than
other random features. Speciﬁcally, by drawing R grids with ex-
pected number of non-empty bins per grid lower bounded by κ,
RB can achieve a solution comparable to exact kernel method with
O(1/(κR)) precision in terms of the objective function, which is
R) rate from Monte Carlo
not only better than the existing O(1/
analysis [21], but also shows a κ times speedup over the rate of
other random features under the same analysis framework [33].

√

Second, we exploit the sparse structure of the feature matrix Z,
which is the key to rapidly transform the data features into a very
high-dimension feature space that is linearly separately by any re-
gressors and classiﬁers. In addition, we discuss how to efﬁciently
perform the computation for a large, sparse matrix by using state-
of-the-art iterative solvers and advanced matrix storage techniques.
As a result, the computational complexity and storage requirements
in training are still O(N Rd + N Rm) and O(N R), respectively.

Third, we show that Random Binning features is particularly
suitable for Parallel Coordinate Descent solver. Unlike other ran-
dom features, RB guarantees a speedup proportional to κ due to
a sparse feature matrix. This is particularly useful in the Sparse
Random Feature setting [33], where L1 regularization is used to
induce a compact nonlinear predictor and Coordinate Descent is
presumably the state-of-the-art solver in such setting.

Finally, we provide extensive experiments to demonstrate the
faster convergence and better parallelizability of RB in practice.
Compared to other popular low-rank approximations, RB shows
superior performance on both regression and classiﬁcation tasks
under the same computational budgets, and achieves same perfor-
mance with one to three orders of magnitude reduction in time and
memory consumptions. When combined with Coordinate Descent
to solve an L1-regularized objective, RB shows an almost linear
speedup, in contrast to RF that has almost no speedup.

2. RANDOM BINNING FEATURE AS KER-

NEL APPROXIMATION

In this work, we consider the problem of ﬁtting a nonlinear pre-
diction function f : X → Y in Reproducing Kernel Hilbert Space
H from training data pairs {(xn, yn)}N
n=1 via regularized Empiri-

cal Risk Minimization (ERM)

f ∗ = argmin

(cid:107)f (cid:107)2

H +

f ∈H

λ
2

1
N

N
(cid:88)

n=1

L(f (xn), yn),

(1)

where L(z, y) is a convex loss function with Lipschitz-continuous
derivative satisfying |L(cid:48)(z1, y) − L(cid:48)(z2, y)| ≤ β|z1 − z2|, which
includes several standard loss functions such as the square-loss
2 (z − y)2, square-hinge loss L(z, y) = max(1 −
L(z, y) = 1
zy, 0)2 and logistic loss L(z, y) = log(1 + exp(−yz)).

2.1 Learning with RKHS

The RKHS H can be deﬁned via a positive-deﬁnite (PD) kernel

function k(x1, x2) that measures similarity between samples as

H =

f (·) =

αik(xi, ·) | αi ∈ R, xi ∈ X

.

(2)

(cid:41)

(cid:40)

K
(cid:88)

i=1

One can also deﬁne the RKHS via a possibly inﬁnite-dimensional
feature map { ¯φh(x)}h∈H with each h ∈ H deﬁning a feature
function ¯φh(x) : X → R. The space can be expressed as

(cid:26)

(cid:90)

H =

f (·) =

h∈H

w(h) ¯φh(·)dh = (cid:104)w, ¯φ(·)(cid:105)H | (cid:107)f (cid:107)2

H < ∞

,

(cid:27)

(3)
where w(h) speciﬁes weights over the set of features {φh(x)}h∈H.
The Mercer’s theorem [19] connects the above two formulations of
RKHS by stating that every PD kernel k(., .) can be expressed as
an integration over some basis functions {φh(.)}h∈H

(cid:90)

h∈H

k(x1, x2) =

p(h)φh(x1)φh(x2)dh = (cid:104) ¯φ(x1), ¯φ(x2)(cid:105)H,

(4)
However, the decomposition (4) is not unique, so one can ﬁnd dif-
ferent feature maps { ¯φh(.)}h∈H satisfying (4) for the same kernel
k(., .). In particular, as an example used extensively in this work,
the Laplacian Kernel

k(x1, x2) = exp

−

(cid:18)

(cid:107)x1 − x2(cid:107)1
σ

(cid:19)

,

(5)

allows decomposition based on (i) Fourier basis map [21], (ii) RB
map [21], and also (iii) map based on inﬁnite number of decision
trees [15] to name a few. On the other hand, different kernels can
be constructed using the same set of basis function {φh(.)} with
different distribution p(h). For example, the RB feature map can
be used to construct any shift-invariant kernel of the form [21]

K(x1, x2) = K(x1 − x2) =

kj(x1j − x2j),

(6)

d
(cid:89)

j=1

by sampling the "width" of bins δj for each feature j from a distri-
bution proportional to δk(cid:48)(cid:48)
j (δ), where k(cid:48)(cid:48)
j (δ) is the second derivative
of kj(δ), assuming the kernel has a non-negative second derivative.

2.2 Random Binning Features

In this section, we describe the Random Binning (RB) feature

map, which has decomposition of the form

K(x1, x2) =

p(δ)φBδ

(x1)T φBδ

(x2) dδ

(7)

(cid:90)

δ

where Bδ is a grid parameterized by δ = (δ1, u1, ..., δd, ud) that
speciﬁes the width and bias of the grid w.r.t. the d dimensions, and
φBδ

(x) is a vector which has

φb(x) = 1, if b = ((cid:98)

(cid:99), ..., (cid:98)

x1 − u1
δ1

xd − ud
δd

(cid:99)),

Figure 1: Generating process of RB features.

Algorithm 1 Random Binning Features

Given a kernel function k(x1, x2) = (cid:81)d
pj(δ) ∝ δk(cid:48)(cid:48)
j (δ) be a distribution over δ.
for r = 1...R do

j=1 kj(|x1j −x2j|). Let

1. Draw δrj ∼ pj(δ), ∀j ∈ [d]. urj ∈ [0, δrj], ∀j ∈ [d].
2. Compute feature zr(xn) as the the indicator vector of bin
index ((cid:98) xn1−u1

(cid:99)), for ∀n ∈ [N ].

(cid:99), ..., (cid:98) xnd−ud

δ1

δd

[z1(xn); ...; zD(xn)] ∀n ∈ [N ] as the

end for.
Return z(xn) = 1√
D
data with RB Features.

and φb(x) = 0 otherwise for any b ∈ Bδ. Note for each grid
Bδ, the number of bins |Bδ| is countably inﬁnite, so φBδ
(x) has
inﬁnite dimension but only 1 non-zero entry (at the bin x lies in).
Figure 1 illustrates an example when the raw dimension d = 2.
The kernel K(x1, x2) is thus interpreted as the collision probabil-
ity that two data points x1, x2 fall in the same bin, when the grid is
generated from distribution p(δ). In [21], it is pointed out for any
kernel of form (6) with nonnegative second derivative k(cid:48)(cid:48)
j (δ), one
can derive distribution p(δ) = (cid:81)d
j=1 pj(δj)U (uj; 0, δj), where
pj(δj) ∝ δk(cid:48)(cid:48)
j (δj) and U (·, a, b) is uniform distribution in the
range [a, b].

To obtain a kernel approximation scheme from the feature map
(7), a simple Monte Carlo method can be used to approximate (7)
by averaging over R grids {Bδr }R
r=1 with each grid’s parameter
δr drawn from p(δ). The procedure for generating R RB features
from raw data {xn}N
n=1 is given in Algorithm 1.
Using a Monte-Carlo analysis, one can show the approximation
R). From the
to (7) yields approximation error of order O(1/
Representer theorem, one can further bound error of the learned
predictor

√

(cid:12)
(cid:12)wT
(cid:12)

RF z(x) − f ∗(x)

n z(xn)T z(x) −
αRF

α∗

nk(xn, x)

N
(cid:88)

n=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

n=1

as shown in [21] (appendix C). Unfortunately, the rate of conver-
gence suggests that to achieve small approximation error (cid:15), one
needs signiﬁcant amount of random features proportional to Ω(1/(cid:15)2),
and furthermore, the Monte-Carlo analysis does not explain why
empirically RB feature achieves faster convergence than other ran-
dom feature map like Fourier basis by orders of magnitude.

3. FASTER CONVERGENCE OF RANDOM

BINNING

In this section, we ﬁrst illustrate the sparse structure of the fea-
ture matrix Z of RB and discuss how to make efﬁcient computa-

Figure 2: Example of the sparse feature matrix ZN ×D gen-
erated by RB. In this special case, Z has the number of rows
N = 200 and columns D = 395, respectively. The number of
grids R = 10 and the nnz(Z) = 2000. Note that for ith row of
Z, nnz(Z(i, :)) = R and R ≤ D ≤ N R.

tion and storage format of Z. Then by interpreting RB features as
Randomized Block Coordinate Descent in the inﬁnite-dimensional
space, we prove that RB has a faster convergence rate than other
random features. We illustrate them accordingly in the following
sections.

3.1 Sparse Feature Matrix & Iterative Solvers
A special characteristic of RB compared to other low-rank ap-
proximations is the fact that the feature matrix generated by RB is
typically a large, sparse binary matrix Z ∈ RN ×D, where the value
of D is determined by both number of grids R and the kernel width
parameter (ex. σ in the case of Laplacian Kernel). Different from
other random features, D, rather than R, is the actual number of
columns of Z. A direct connection between D and R is that the
matrix has each row i satisfying nnz(Z(i, :)) = R and therefore
R ≤ D ≤ N R.
Intuitively speaking, RB has more expressive
power than RF since it generates a large yet sparse feature matrix
to rapidly transform the data space to a very high dimension space,
where data could become almost linearly separable by the classi-
ﬁers. Fig. 2 gives an example to illustrate the sparse structure of
Z.

In the case of Kernel Ridge Regression (L2-regularization with
square loss), if using RB feature to approximate the RKHS, one
can solve (1) directly in its primal form. The weighting vector is
simply the solution of the linear system:

(Z T Z + λI)wRB = Z T y.

(8)

Note since Z is a large sparse matrix, there is no need to explicitly
compute the covariance matrix Z T Z, which is much denser than Z
itself. One can apply state-of-the-art sparse iterative solvers such
as Conjugate Gradient (CG) and GMRES to directly operate on Z
[25]. The main computation in CG or GMRES is the sparse matrix-
vector products. Let m be the number of iterations, then the total
computational complexity of iterative solver is O(m nnz(Z)) =
O(mN R). In addition, since most elements in Z are zeros, the
Compressed Sparse Row type matrix storage format should be em-
ployed for economically storing Z [9], which gives computational
cost and memory requirement as O(mN R) and O(N R) respec-
tively, a similar cost to that of other low-rank approximations de-
spite its much higher dimension. In testing phase, each point pro-
duces a sparse feature vector z(x) ∈ RD based on the grids stored
during training, yielding a sparse vector z(x) with nnz(z(x))) =

R and computing the decision function z(x)T wRB only requires
O(dR + R).

When the ERM is smooth but not quadratic, a Newton-CG method
that solves smooth problem via a series of local quadratic approx-
imation gives the same complexity per CG iteration [14], and note
that most of state-of-the-art linear classiﬁcation algorithms have
complexity linear to nnz(Z), the number of nonzeros of feature
matrix [6]. In section 4, we further discuss cases of L1-regularized
problem, where a Coordinate Descent algorithm of cost O(nnz(Z))
per iteration is discussed.

3.2 Random Binning Features as Block Coor-

dinate Descent

In [33], a new approach of analysis was proposed, which inter-
preted Random Features as Randomized Coordinate Descent in the
inﬁnite dimensional space, and gives a better O(1/R) rate in the
convergence of objective function. In this section, we extend the
approach of [33] to show that, RB Feature can be interpreted as
RBCD in the inﬁnite-dimensional space, which by drawing a block
of features at a time, produces a number of features D signiﬁcantly
more than the number of blocks R, resulting a provably faster con-
vergence rate than other RF. While at the same time, by exploit-
ing state-of-the-art iterative solvers introduced in section 3.1, the
computational complexity of RB does not increase with number of
features D but only with the number of blocks R. Consequently, to
achieve the same accuracy, RB requires signiﬁcantly less training
and prediction time compared to other RF.

A key quantity to our analysis is an upper bound on the collision
probability νδ which speciﬁes how unlikely data points will fall
into the same bin, and its inverse κδ := 1/νδ which lower bounds
the number of bins containing at least one data point. We deﬁne
them as follows.

Deﬁnition 1. Deﬁne collision probability of data D on bin b ∈ Bδ
as

νb :=

|{n ∈ [N ] | φb(xn) = 1}|
N

.

(9)

Let νδ := maxb∈Bδ νb be an upper bound on (9), and κδ := 1/νδ
be a lower bound on the number of nonempty bins of grid δ.

κ := Eδ[κδ] = Eδ[1/νδ]

(10)

is denoted as the lower bound on the expected number of (used)
bins w.r.t the distribution p(δ).

In the RB matrix, the empirical collision probability is simply the
average number of non-zeros per column, divided by N , a number
much smaller than 1 as in the example of Fig. 2. Our analysis
assumes a smooth loss function satisfying the following criteria.

Assumption 1. The loss function L(z, y) is smooth w.r.t. response
z so difference between function difference and its linear approxi-
mation can be bounded as

where Loss( ¯w; φ) = 1
N

(cid:80)N

n=1 L((cid:104) ¯w, φ(xn)(cid:105), yn) and

¯φ :=

(.))δ∈H

√
p ◦ φ = ((cid:112)p(δ)φBδ
with "◦" denoting the component-wise product. The goal is to show
that, by performing R steps of FC-RBCD on (11), one can obtain a
¯wR with comparable regularized loss to that from optimal solution
of (1). Note one advantage of analysis from this optimization per-
spective is: it does not rely on Representer theorem, and thus R(w)
2 (cid:107)w(cid:107)2 or L1 regularizer λ(cid:107)w(cid:107)1, where the
can be L2 regularizer λ
latter has advantage of giving sparse predictor of faster prediction
[33]. The FC-RBCD algorithm maintains an active set of blocks
A(r) which is expanded for R iterations. At each iteration r, the
FC-RBCD does the following:

1. Draw δ from p(δ) ( derived from the kernel k(., .) ).

2. Expand active set A(r+1) := A(r) ∪ Bδ.

3. Minimize (11) subject to a limited support supp( ¯w) ⊆ A(r+1).

Note this algorithm is only used for analysis. In practice, one
can draw R blocks of features at a time, and solve (11) by any
optimization algorithm such as those mentioned in section 3.1 or
the CD method we introduce in section 4.

Due to space limit, here we prove the case when R(.) is the
non-smooth L1 regularizer λ(cid:107) ¯w(cid:107)1. The smooth case for R(w) =
2 (cid:107) ¯w(cid:107)2 can be shown in a similar way. Note the objective function
λ
(11) can be written as
¯F (w) := F (

p ◦ w) + Loss(w, ¯φ).

p ◦ w) = R(

(12)

√

√

√

by a scaling of variable ¯w =

p ◦ w.
The below theorem states that, running FC-RBCD for R itera-
tions, it generates a solution ¯wR close to any reference solution w∗
in terms of objective (12) with their difference bounded by O( 1
κR ).

Theorem 1. Let R be the number of blocks (grids) generated by
FC-RBCD, and w∗ be any reference solution, we have

E[ ¯F (w(R))] − ¯F (w∗) ≤

(13)

β(cid:107)w∗(cid:107)2
κR(cid:48)

for R(cid:48) := R − c > 0, where c = (cid:100) 2κ( ¯F (0)− ¯F (w∗))

(cid:101).

β(cid:107)w∗(cid:107)2

Proof. Firstly, we obtain an expression for the progress made by
each iteration of FC-RBCD. Let B := Bδ(r) be the block drawn at
step 1 of FC-RBCD, and ¯w(r+1) be the minimizer of (11) subject
to support supp( ¯w) ⊆ A(r+1) given by the step 3. Since B ⊆
A(r+1), we have

F ( ¯w(r+1)) − F ( ¯w(r)) ≤ F ( ¯w(r) + ηB) − F ( ¯w(r))

(14)

for any ηB : supp(η) ⊆ B. Then denote bi as the bin xi falling
i = ∇L( ¯w(r)T φ(xi), yi), by smoothness of the loss (As-
in and L(cid:48)
sumption 1), we have

L(z2, .) − L(z1, .) ≤ ∇L(z1, .)(z2 − z1) +

(z2 − z2)2.

Loss( ¯w(r) + ηB) − Loss( ¯w(r)) ≤

L(cid:48)

iφbi η +

(ηbi φbi )2

β
2

1
N

N
(cid:88)

i=1

β
2

for some constant 0 ≤ β ≤ ∞.

This assumption is satisﬁed for a wide range of loss such as
square loss (β = 1), logistic loss (β = 1/4) and L2-hinge loss
(β = 1).

We interpret RB as a Fully Corrective Randomized Block Coor-

dinate Descent (FC-RBCD) on the objective function

min
¯w

F ( ¯w) := R( ¯w) + Loss( ¯w; φ)

(11)

≤ (cid:104)gB, ηB(cid:105) +

βνδ(r)
2

(cid:107)ηB(cid:107)2

where the second inequality uses the fact φbi = 1 and

gB := ∇BLoss( ¯w(r), φ).
Now consider the regularization term, note since block B is drawn
from an iniﬁnite-dimensional space, the probability that B is in

(15)

active set is 0. Therefore, we have B ∩ A(r) = ∅, ¯w(r)
RB( ¯w(r)

B ) = 0. As a result,

B = 0 and

F ( ¯w(r) + ηB) − F ( ¯w(r))

≤ RB(ηB) + (cid:104)gB, ηB(cid:105) +

βνδ(r)
2

(cid:107)ηB(cid:107)2

(16)

Let ηB be the minimizer of RHS of (16). It satisﬁes ρB + gB +
βvδ(r) ηB = 0 for some ρB ∈ ∂R(ηB), and thus,

F ( ¯w(r) + ηB) − F ( ¯w(r))

≤ (cid:104)ρB, ηB(cid:105) + (cid:104)gB, ηB(cid:105) +

= −

1
2βνδ(r)

(cid:107)ρB + gB(cid:107)2

βνδ(r)
2

(cid:107)ηB(cid:107)2

(17)

Now taking expectation w.r.t. p(δ) on both sides of (17), we have

E[F ( ¯w(r) + ηB)] − F ( ¯w(r)) ≤ −

(cid:21)

(cid:107)ρB + gB(cid:107)2
(cid:21)

E (cid:2)(cid:107)ρB + gB(cid:107)2(cid:3)

E

(cid:20) 1
νδ(r)
(cid:20) 1
νδ(r)
(cid:107)¯ρB + ¯gB(cid:107)2

E

1
2β

1
2β
κ
2β

≤ −

≤ −

√

√

p ◦ ρ, ¯g :=

(18)
where ¯ρ :=
p ◦ g, and the second inequality
uses the fact that the number of used bins κδ(r) = 1/νδ(r) has
non-negative correlation with the discriminative power of block
B measured by the magnitude of gradient with soft-thresholding
(cid:107)¯ρB + ¯gB(cid:107) (i.e.
fewer collisions on grid B implies B to be a
better block of features ).

The result of (18) expresses descent amount in terms of the prox-
imal gradient of the reparameterized objective (12). Note for B :
B ∩A(r) = ∅, we have w(r)
B = 0, and RB(¯η)−RB(0) = (cid:104)¯ρ, ¯η(cid:105);
on the other hand, for B ⊆ A(r), we have

0 ∈ arg min
¯ηB

√

√

RB(

pBwB + ¯ηB) + (cid:104)¯gB,

pBwB + ¯ηB(cid:105)

since they are solved to optimality in the previous iteration. Then

E[F ( ¯w(r) + η)] − F ( ¯w(r))

where the second and fourth inequalities are from convexity of
¯F (.). The α minimizing (20) is α∗ := min( κ( ¯F (w(r))− ¯F (w∗))
, 1),
which leads to

β(cid:107)w∗(cid:107)2

E[ ¯F (w(r) + ¯η)] − ¯F (w(r)) ≤ −

κ( ¯F (w(r)) − ¯F (w∗))2
2β(cid:107)w∗(cid:107)2

(21)
κ (cid:107)w∗(cid:107)2; otherwise, we have E[ ¯F (w(r) +
if ¯F (w(r))− ¯F (w∗) ≤ β
¯η)] − ¯F (w(r)) ≤ − β
2κ (cid:107)w∗(cid:107)2. Note the latter case cannot happen
more than c = (cid:100) 2κ( ¯F (0)− ¯F (w∗))
(cid:101) times since FC-RBCD is a de-
scent method. Therefore, for r(cid:48) := r −c > 0, solving the recursion
(21) leads to the conclusion.

β(cid:107)w∗(cid:107)2

√

√

Note we have (cid:107)

p ◦ w∗(cid:107)1 ≤ (cid:107)

p(cid:107)(cid:107)w∗(cid:107) = (cid:107)w∗(cid:107) in the L1-
regularized case, and thus the FC-RBCD guarantees convergence
of the L1-norm objective to the (non-square) L2-norm objective.
The convergence result of Theorem 1 is of the same form to the
rate proved in [33] for other random features, however, with an
additional multiplicative factor κ ≥ 1 that speeds up the rate by
κ times. Recall that κ is the lower bound on the expected number
of bins being used by data samples for each block of features Bδ,
which in practice is a factor much larger than 1, as shown in the
Figure 2 and also in our experiments. In particular, in case each
grid Bδ has similar number of bins being used, we have D ≈ κR,
and thus obtain a rate of the form

E[ ¯F (w(R))] − ¯F (w∗) (cid:46) β(cid:107)w∗(cid:107)2

.

D

(22)

Note for a ﬁxed R, the total number of features D is increasing with
kernel parameter 1/σ in the case of Laplacian Kernel, which means
the less smooth the kernel, the faster convergence of RB. A simple
extreme case is when σ → 0, where one achieves 0 training loss,
and the RB, by putting each sample in a separate bin, converges
to 0 loss with R = 1, D = N . On the other hand, other random
features, such as Fourier, still require large R for convergence to
0 loss. In practice, there are many data that require a small kernel
bandwidth σ to avoid underﬁtting, for which RB has dramatically
faster convergence than other RF.

β
2κ

κ
2β
√

≤ −

(cid:107)¯ρB + ¯gB(cid:107)2 = (cid:104)¯ρ, ¯η(cid:105) + (cid:104)¯g, ¯η(cid:105) +

(cid:107)¯η ¯A(r) (cid:107)2

4. STRONG PARALLELIZABILITY OF RAN-

√

= R(

p ◦ w(r)) + (cid:104)¯g, ¯η(cid:105) +

p ◦ (w(r) + ¯η)) − R(

(cid:107)¯η ¯A(r) (cid:107)2
(19)
where ¯η ¯A(r) := (¯ηB)B:B∩A(r)=∅ and ¯η :=
p◦η. Thus the ﬁnal
step is to show the descent amount given by RHS of (19) decreases
the suboptimality ¯F (w(r)) − ¯F (w∗) signiﬁcantly. This can be
achieved by considering ¯η of the form α(w∗ − w(r)) for some
α ∈ [0, 1] as follows:

√

β
2κ

DOM BINNING FEATURES

In this section, we study another strength of RB Features in the
context of Sparse Random Feature [33], where one aims to train a
sparse nonlinear predictor that has faster prediction and more com-
pact representation through an L1-regularized objective.
In this
case, the CD method is known as state-of-the-art solver [23, 34],
and we aim to show that the structure of RB allows CD to be paral-
lelized with much more speedup than that of other random features.

E[ ¯F (w(r) + ¯η)] − ¯F (w(r))

≤ min

R(

p ◦ (w(r) + ¯η)) − R(

p ◦ w(r)) + (cid:104)¯g, ¯η(cid:105) +

√

√

≤ min

¯F (w(r) + ¯η) − ¯F (w(r)) +

β
2κ

(cid:107)¯η ¯A(r) (cid:107)2

¯F ((1 − α)w(r) + αw∗) − ¯F (w(r)) +

(cid:107)w∗(cid:107)2

βα2
2κ

−α( ¯F (w(r)) − ¯F (w∗)) +

(cid:107)w∗(cid:107)2,

βα2
2κ

¯η

¯η

≤ min
α∈[0,1]

≤ min
α∈[0,1]

4.1 Coordinate Descent Method

β
2κ

(cid:107)¯η ¯A(r) (cid:107)2

a RCD Method solves

Given the N × D data matrix produced by the RB Algorithm 1,

min
w∈RD

λ(cid:107)w(cid:107)1 +

1
N

N
(cid:88)

n=1

L(wT zi, yi)

(23)

by minimizing (23) w.r.t. a single coordinate j

(20)

min
dj

λ|wj + dj| + gjdj +

Mj
2

d2
j

(24)

Algorithm 2 Sparse Random Binning Features via Parallel RCD

0. Generate RB feature matrix Z by Algorithm 1
1. z1 = 0, w1 = 0.
for t=1......T (with τ threads in parallel) do
2. Draw j from [D] uniformly at random.
3. Compute d∗
4. wt+1 := wt + d∗
5. Maintain ˆyi, ∀i ∈ [N ] to satisfy (27).

j by (26).
j ej.

end for

at a time, where

gj :=

(∇jL(wT zi, yi))zij

(25)

1
N

N
(cid:88)

n=1

(cid:80)N

i=1 z2

is the gradient of loss term in (23) w.r.t. the j-th coordinate, and
Mj := β 1
ij is an upper bound on ∇jjL(.). Note, by
N
focusing on single coordinate, (24) has a tighter quadratic upper
bound than other algorithms such as Proximal Gradient Method,
and allows simple closed-form solution

d∗
j := proxR/Mj

(wj −

gj
Mj

) − wj

(26)

where

proxR(vj) :=






0,
vj − λ,
vj + λ,

|vj| ≤ λ
vj > λ
vj < λ

.

To have efﬁcient evaluation of the gradient (25), a practical imple-
mentation maintain the responses

ˆyi := wT zi

(27)

j ej, so the cost for each coordinate-

after each update wt+1 := wt+d∗
wise minimization takes O(nnz(zj)) time for both gradient evalu-
ation and maintenance of (27), where zj := (zij)i∈[N ]. The algo-
rithm is summarized in Alg. 2, which just like the iterative solver
introduced in section 3.1, has cost O(nnz(Z)) for one pass of all
variables j ∈ [D].

4.2 Parallel Randomized Coordinate Descend

on Random Binning Features

The RCD, however, is hard to parallelize [23]. It is known that
simultaneous updates of two coordinates j1, j2 could lead to di-
vergence, and although one can enforce convergence by shortening
the step size 1
, the convergence rate will not be improved
Mp
with parallelization without additional assumption [1, 24].

(cid:28) 1
Mj

On the other hand, in [24], it is shown that a function with par-

tially separable smooth term plus a separable non-smooth term

min
w∈RD

F (w) := Ω(w) +

fi(w)

(28)

N
(cid:88)

i=1

can be parallelized with guaranteed speedup in terms of overall
complexity, where Ω(w) is a non-smooth separable function and
each function fi(w) is a smooth depends only on at most ω num-
ber of variables. The form (28), fortunately, ﬁts our objective (23)
with features zi generated by RB. In particular, the generating pro-
cess of RB guarantees that, for each block of feature Bδ, the i-th
sample can fall in exactly one bin b = ((cid:98) xn1−u1
(cid:99)),
therefore each sample inolves at most R features out of D. Specif-
ically, let Ω(w) := λ(cid:107)w(cid:107)1 and

(cid:99), ..., (cid:98) xnd−ud

δd

δ1

fi(w) :=

L(wT zi, yi),

1
N

we have ω = R. Then by Theorem 19 of [24], a parallel RCD of
τ threads that selects coordinate j uniformly at random achieves a
speed-up (i.e. time-of-sequential/time-of-parallel) of

When D, R (cid:29) 1, and τ = a¯κ+1 where ¯κ := D/R, (29) becomes

speedup-ratio =

τ
1 + (R−1)(τ −1)

D−1

.

speedup-ratio =

a¯κ + 1
1 + a

,

(29)

(30)

which equals (¯κ + 1)/2 when a = 1 and approaches ¯κ when a →
∞. Therefore, it is guaranteed in theory that parallelization can
speedup RCD signiﬁcantly as long as ¯κ = D/R (cid:29) 1. We give
our sparse RB Features algorithm based on parallel RCD in Alg.
2. Note for other Random Features, there is no speedup guaranteed
and our experiment shows that Parallel RCD performed on Random
Fourier features could even have no speedup.

Note that the speedup achieved in this section is orthogonal to
the faster convergence rate achieved in section 3, so by increas-
ing κ, the advantage of RB over other Random Features is super-
linearly increasing if a parallel RCD is used. Note also that the
results (29), (30) also apply to algorithms that utilize Coordinate
Descent as subproblem solvers such as Proximal (Quasi) Newton
Method [13, 35]. Those methods are typically employed for com-
putationally expensive loss functions.

5. EXPERIMENTS

In this section, we present extensive sets of experiments to demon-
strate the efﬁciency and effectiveness of RB. The datasets are cho-
sen to overlap with those in other papers in the literature, where the
details are shown in the table 1. All sets except census are avail-
able at LIBSVM data set [2]. All computations are carried out on a
DELL dual socket with Intel Xeon processors at 2.93GHz for a total
of 16 cores and 250 GB of memory running the SUSE Linux op-
erating system. We implemented all methods in C++ and all dense
matrix operations are performed by using the optimized BLAS and
LAPACK routines provided in the OpenBLAS library. Due to the
limited space, we only choose subsets of our results to present in
each subsection. However, these results are objective and unbiased.

Name
cadata
census
ijcnn1
cod_rna
covtype
SUSY
mnist
acoustic
letter

Table 1: Properties of the datasets.
C: Classes
1
1
2
2
2
2
10
3
26

d: Features N : Train M : Test
16,512
18,186
35,000
49,437
464,809
4,000,000
60,000
78,823
10,500

4,128
2,273
91,701
271,617
116,203
1,000,000
10,000
19,705
5,000

8
119
22
8
54
18
780
50
16

5.1 Effects of σ and R on Random Binning

We perform experiments to investigate the characteristics of RB
by varying the kernel parameter λ and the rank R, respectively.
We use a regularization λ = 0.01 to make sure the reasonable
performance of RB and other low-rank kernels, although we found
that RB is not sensitive to this parameter. We increase the σ in the
large interval from 1e−2 to 1e2 so that the optimal σ locates within
the interval. We apply CG iterative solver to operate on Z directly.

In order to make fair runtime comparison in each run, we set the
tol = 1e − 15 to force similar CG iterations with different σ.

We evaluate the training and testing performance of regression
and classiﬁcation, when varying σ with ﬁxed R. In [21], it does
not consider the effect of σ in their analysis, which however has a
large impact on the performance since D depends on the number
of bins which is controlled by σ. Fig. 3 shows that the training
and testing performance coincidentally decrease (increase) before
they diverge when D grows by increasing σ. This conﬁrms with
our analysis in Theorem 1 that the larger κ, the faster convergence
of RB Feature (recall that the convergence rate is O(1/(κR))).

Second, one should not be surprised that the empirical training
time increases with D. The operations involving the weighting vec-
tor wRB could become as expensive as a sparse matrix-vector oper-
ation in an iterative solver. However, the total computational costs
are still bounded by O(N R) but the constant factor may vary with
different datasets. Fortunately, in most of cases, the training time
corresponding to the peak performance is just slightly higher than
the smallest one. In practice, there are several ways to improve the
computation costs by exploiting more advanced sparse matrix tech-
niques such as preconditioning and efﬁcient storage scheme, which
is out scope of this paper and left for future study.

Finally, we evaluate the training and testing performance when
varying R with ﬁxed σ. Fig.4 shows that the training and testing
performance converge almost linearly with D, which again con-
ﬁrms our analysis in Theorem 1. In addition, we observe that RB
has strong overﬁt ability which turns out to be a strong attribute,
especially when the hypothesis space has not yet saturated.

5.2 Performance Comparisons of All Methods
We present a large sets of experiments to compare RB with other
most popular low-rank kernel approximations, including RF [21],
Nyström [31], and recently proposed independent block approxi-
mation [29]. We also compare all methods with the exact kernel as
a benchmark [26]. We do not report the results of the vanilla ker-
nel on covtype and SUSY since the programs run out of memory.
To make a fair comparison, we also apply CG on RB and Nys-
tröm directly on Z to admit similar computational costs. Since the
independent block kernel approximation approximates the kernel
matrix directly, we employ direct solver of dense matrix for this
method. In practice, the CG iterative solver has no need to solve
in high precision [3], which has also been observed in our experi-
ments. Thus, we set the tolerance to 1e − 3.

Fig.5 clearly demonstrates the superiority of RB compared to
other low-rank kernels. For example, in the ﬁrst column, RB sig-
niﬁcantly outperforms other methods in testing performance on all
of these datasets, especially when R is relatively small. This is
because RB enjoys much faster convergence rate to the optimal
function than other methods. The advantage generally diminishes
when R increases to reasonably large. However, for some large
datasets such as covtype and SUSY, increasing number of random
features or R boosts the performance extremely slow. This is con-
sistent with our analysis that RB enjoys its fast convergence rate
of O(1/(κR)) while other methods has slow convergence rates
R). The third and fourth columns further promote the in-
O(1/
sights about how many number of random features or how large
rank R that is needed for achieving similar performance of RB. In
particular, RB is often between one and three orders of magnitude
faster and less memory consumptions than other methods.

√

In the second column, we also observe that the training time of
all low-rank kernels are linear with R, which is expected since all
these methods has computational complexity of O(kN R). The
difference in training time between these low-rank kernels is only

within some constant factors. However, we point out that the com-
putations of RF, Nyström and independent block approximation are
mainly carried out by the high-optimized BLAS library since they
are dense matrices. In contrast, the computations of RB are most
involved in sparse matrix operations, which are self-implemented
and not yet optimized. In addition, more advanced sparse matrix
techniques such as preconditioning can be explored to signiﬁcantly
accelerate the computation, which we leave it as future work.

5.3 Parallel Performance of Random Binning

and Random Fourier

We perform experiments to compare RB with RF when using
RCD to solve L1-regularized Lasso and kernel SVM for both re-
gression and binary classiﬁcation problems. Since the goal is to
demonstrate the strong parallel performance of RB, we implement
the basic parallel implementation of RCD based on simple shared
memory parallel programming model with OpenMP. We leave the
high-performance distributed RCD implementation as one of the
future works. We deﬁne the speedup of RCD on multicore imple-
mentation as follows:

speedup =

runtime of RCD using single core
runtime using P cores

As shown in Fig.6, when the sparsity level of the feature matrix Z
is high, the near-linear speedup can be achieved [16, 18]. This is
because the minimization problem can almost be separated along
the coordinate axes, then higher degrees of parallelism are possi-
ble. In contrast, if Z is lack of sparsity, then the penalty for data
correlations slows the speedup to none. This is conﬁrmed by no
gain of parallel speedup of RF since Z is always fully dense. Ob-
viously, in order to empower strong parallel performance of RB, a
very large D is expected, which interestingly coincides with power
of its faster convergence. Therefore, one can enjoy the double ben-
eﬁts of fast convergence and strong parallelizability of RB, which
is especially useful for very large-scale problems.

6. CONCLUSIONS

In this paper, we revisit RB features, an overlooked yet very
powerful random features, which we observe often to be orders
of magnitude faster than other random features and kernel approx-
imation methods to achieve the same accuracy. Motivated by these
impressive empirical results, we propose the ﬁrst analysis of RB
from the perspective of optimization, to make a solid attempt to
quantify its faster convergence, which is not captured by tradi-
tional Monte-Carlo analysis. By interpreting RB as a RBCD in
the inﬁnite-dimensional space, we show that by drawing R grids
with at least κ expected number of non-empty bins per grid, RB
achieves a convergence rate of O(1/(κR)). In addition, in the L1-
regularized setting, we demonstrate the sparse structure of RB fea-
tures allows RCD solver to be parallelized with guaranteed speedup
proportional to κ. Our extensive experiments demonstrate the su-
perior performance of the RB features over other random feature
and kernel approximation methods.

7. ACKNOWLEDGEMENT

This work was done while L. Wu was a research intern at IBM
Research. J. Chen is supported in part by the XDATA program
of the Advanced Research Projects Agency (DARPA), adminis-
tered through Air Force Research Laboratory contract FA8750-12-
C-0323.

(a) cadata

(b) cadata

(c) census

(d) census

(e) ijcnn1

(f) ijcnn1

(g) covtype

(h) covtype

(i) SUSY

(j) SUSY

(k) mnist

(l) mnist

(m) acoustic

(n) acoustic

(o) letter

(p) letter

Figure 3: Train and test performance, and train time when varying σ with ﬁxed R. The black line and square box represent the best
test performance of the exact kernel and RB respectively.

(a) cadata

(b) ijcnn1

(c) acoustic

(d) letter

Figure 4: Train and test performance when varying R with ﬁxed σ.

References
[1] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel
coordinate descent for l1-regularized loss minimization. CoRR,
abs/1105.5379, 2011.

[2] C. Chang and C. Lin. Libsvm: a library for support vector machines.

ACM Transactions on Intelligent Systems and Technology,
2:27:1–27:27, 2011.

[3] J. Chen, L. Wu, K. Audhkhasi, B. Kingsbury, and B. Ramabhadran.
Efﬁcient one-vs-one kernel ridge regression for speech recognition.

[24] P. Richtárik and M. Takáˇc. Parallel coordinate descent methods for

big data optimization. Mathematical Programming, pages 1–52,
2015.

[25] Y. Saad. Iterative Methods for Sparse Linear Systems. Society for
Industrial and Applied Mathematics, Philadelphia, PA, USA, 2nd
edition, 2003.

[26] B. Scholkopf and A. J. Smola. Learning with Kernels: Support

Vector Machines, Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA, 2001.

[27] S. Si, C. Hsieh, and I. S. Dhillon. Memory efﬁcient kernel

approximation. In ICML, 2014.

[28] A. J. Smola and B. Schökopf. Sparse greedy matrix approximation

for machine learning. In ICML, ICML ’00, San Francisco, CA, USA,
2000. Morgan Kaufmann Publishers Inc.

[29] M. L. Stein. Limitations on low rank approximations for covariance

matrices of spatial data. Spatial Statistics, 8:1 – 19, 2014. Spatial
Statistics Miami.

[30] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks.

In NIPS. MIT Press, 2004.

[31] C. K. I. Williams and M. Seeger. Using the nyström method to speed

up kernel machines. In NIPS. MIT Press, 2001.

[32] Z. Yang, A. G. Wilson, A. J. Smola, and L. Song. A la carte -

learning fast kernels. In AISTATS, 2015.

[33] I. E.-H. Yen, T.-W. Lin, S.-D. Lin, P. K. Ravikumar, and I. S. Dhillon.
Sparse random feature algorithm as coordinate descent in hilbert
space. In JMLR, 2014.

[34] G.-X. Yuan, K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. A comparison
of optimization methods and software for large-scale l1-regularized
linear classiﬁcation. JMLR, 11:3183–3234, 2010.

[35] K. Zhong, I. E.-H. Yen, I. S. Dhillon, and P. K. Ravikumar. Proximal

quasi-newton for computationally intensive l1-regularized
m-estimators. In Advances in Neural Information Processing
Systems, pages 2375–2383, 2014.

In ICASSP, 2016.

[4] B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. F. Balcan, and

L. Song. Scalable kernel methods via doubly stochastic gradients. In
NIPS. Curran Associates, Inc., 2014.

[5] P. Drineas and M. W. Mahoney. On the nyström method for

approximating a gram matrix for improved kernel-based learning.
JMLR, 6:2153–2175, Dec. 2005.

[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.

Liblinear: A library for large linear classiﬁcation. JMLR,
9:1871–1874, 2008.

[7] S. Fine and K. Scheinberg. Efﬁcient svm training using low-rank

kernel representations. JMLR, 2:243–264, Mar. 2002.

[8] A. Gittens and M. W. Mahoney. Revisiting the nyström method for

improved large-scale machine learning. CoRR, abs/1303.1849, 2013.

[9] G. H. Golub and C. F. Van Loan. Matrix Computations. Johns

Hopkins University Press, Baltimore, MD, USA, 1996.

[10] P.-S. Huang, H. Avron, T. Sainath, V. Sindhwani, and

B. Ramabhadran. Kernel methods match deep neural networks on
timit. In ICASSP, 2014.

[11] S. Kumar, M. Mohri, and A. Talwalkar. Sampling methods for the

nyström method. JMLR, 13:981–1006, Apr. 2012.

[12] Q. V. Le, T. Sarlós, and A. J. Smola. Fastfood: Approximate kernel

expansions in loglinear time. ICML, 2013.

[13] J. Lee, Y. Sun, and M. Saunders. Proximal newton-type methods for
convex optimization. In Advances in Neural Information Processing
Systems, pages 836–844, 2012.

[14] C.-J. Lin, R. C. Weng, and S. S. Keerthi. Trust region newton method

for logistic regression. JMLR, 9:627–650, 2008.

[15] H.-T. Lin and L. Li. Support vector machinery for inﬁnite ensemble

learning. JMLR, 9:285–312, 2008.

[16] J. Liu, S. J. Wright, C. Ré, and V. Bittorf. An asynchronous parallel
stochastic coordinate descent algorithm. JMLR, 32(1):469–477,
2014.

[17] Z. Lu, A. May, K. Liu, A. B. Garakani, D. Guo, A. Bellet, L. Fan,

M. Collins, B. Kingsbury, M. Picheny, and F. Sha. How to scale up
kernel methods to be as good as deep neural nets. CoRR,
abs/1411.4000, 2014.

[18] J. Mareˇcek, P. Richtárik, and M. Takáˇc. Distributed block coordinate
descent for minimizing partially separable functions. Numerical
Analysis and Optimization, 134:261–288, 2014.

[19] J. Mercer. Functions of positive and negative type, and their

connection with the theory of integral equations. Royal Society
London, A 209:415–446, 1909.

[20] M. Raginsky and S. Lazebnik. Locality-sensitive binary codes from
shift-invariant kernels. In Y. Bengio, D. Schuurmans, J. D. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances in Neural
Information Processing Systems 22, pages 1509–1517. Curran
Associates, Inc., 2009.

[21] A. Rahimi and B. Recht. Random features for large-scale kernel

machines. In NIPS. Curran Associates, Inc., 2007.

[22] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks:
Replacing minimization with randomization in learning. In NIPS.
Curran Associates, Inc., 2008.

[23] P. Richtárik and M. Takáˇc. Iteration complexity of randomized
block-coordinate descent methods for minimizing a composite
function. Mathematical Programming, 144(1-2):1–38, 2014.

(a) cadata

(b) cadata

(c) cadata

(d) cadata

(e) ijcnn1

(f) ijcnn1

(g) ijcnn1

(h) ijcnn1

(i) covtype

(j) covtype

(k) covtype

(l) covtype

(m) SUSY

(n) SUSY

(o) SUSY

(p) SUSY

(q) mnist

(r) mnist

(s) mnist

(t) mnist

(u) acoustic

(v) acoustic

(w) acoustic

(x) acoustic

Figure 5: Comparisons among RB, RF, Nyström and Independent Block approximation. The ﬁrst and second columns plot test
performance and train time when increasing R. The third and fourth columns plot the train time and memory consumptions when
achieving the desired test performance.

(a) ijcnn1

(b) cod_rna

(c) covtype

(d) SUSY

Figure 6: Comparisons of parallel performance between RB and RF using RCD when increasing the number of threads.

8
1
0
2
 
p
e
S
 
9
1
 
 
]

G
L
.
s
c
[
 
 
2
v
7
4
2
5
0
.
9
0
8
1
:
v
i
X
r
a

Revisiting Random Binning Features: Fast Convergence
and Strong Parallelizability

∗

Lingfei Wu
College of William and Mary
Williamsburg, VA 23185
lfwu@cs.wm.edu
Jie Chen
IBM Research
Yorktown Heights, NY 10598
chenjie@us.ibm.com

Ian E.H. Yen *
Unversity of Texas at Austin
Austin, TX 78712
ianyen@cs.utexas.edu

Rui Yan
Baidu Inc.
Beijing 100085, China
yanrui02@baidu.com

ABSTRACT
Kernel method has been developed as one of the standard approaches
for nonlinear learning, which however, does not scale to large data
set due to its quadratic complexity in the number of samples. A
number of kernel approximation methods have thus been proposed
in the recent years, among which the random features method gains
much popularity due to its simplicity and direct reduction of non-
linear problem to a linear one. Different random feature functions
have since been proposed to approximate a variety of kernel func-
tions. Among them the Random Binning (RB) feature, proposed in
the ﬁrst random-feature paper [21], has drawn much less attention
than the Random Fourier (RF) feature proposed also in [21]. In
this work, we observe that the RB features, with right choice of op-
timization solver, could be orders-of-magnitude more efﬁcient than
other random features and kernel approximation methods under the
same requirement of accuracy. We thus propose the ﬁrst analysis
of RB from the perspective of optimization, which by interpret-
ing RB as a Randomized Block Coordinate Descent in the inﬁnite-
dimensional space, gives a faster convergence rate compared to that
of other random features.
In particular, we show that by draw-
ing R random grids with at least κ number of non-empty bins
per grid in expectation, RB method achieves a convergence rate
R) rate from
of O(1/(κR)), which not only sharpens its O(1/
Monte Carlo analysis, but also shows a κ times speedup over other
random features under the same analysis framework. In addition,
we demonstrate another advantage of RB in the L1-regularized set-
ting, where unlike other random features, a RB-based Coordinate
Descent solver can be parallelized with guaranteed speedup pro-
portional to κ. Our extensive experiments demonstrate the superior
performance of the RB features over other random features and
kernel approximation methods. Our code and data is available at
https://github.com/teddylfwu/RB_GEN.

√

∗Both authors contributed equally to this manuscript

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’16, August 13 - 17, 2016, San Francisco, CA, USA
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4232-2/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2939672.2939794

Keywords
Kernel approximation, Random Binning Features, large-scale ma-
chine learning, faster convergence, strong parallelizability

1.

INTRODUCTION

Kernel methods have great promise for learning non-linear model
from simple data input representations and have been demonstrated
successful for solving various learning problems such as regres-
sion, classiﬁcation, feature extraction, clustering and dimensional-
ity reduction [26, 30]. However, they are typically not ﬁrst choice
for large-scale nonlinear learning problems, since large number of
samples (N ) presents signiﬁcant challenges in terms of computa-
tion and memory consumptions to Kernel methods for computing
the dense kernel matrix K ∈ RN ×N which requires at least a
O(N 2) complexity. To scale up the kernel methods, there have
been great efforts addressing this challenge from various perspec-
tives such as numerical linear algebra, sampling approximation, op-
timization and functional analysis [4, 5, 7, 21, 27, 33].

√

√

R + 1/

A line of research [7, 27, 28, 31] approximates the kernel ma-
trix K using low-rank factorizations, K ≈ Z T Z, where Z ∈
RN ×R matrix with R (cid:28) N . Among them, Nyström method
[5, 8, 11, 27, 31] is probably one of the most popular approaches,
which reduces the total computational costs to O(N Rd + N R2 +
R3) or O(N Rd + N Rm) depending whether the algorithm per-
forms on K explicitly or implicitly through Z, where d and m are
the input data dimension and the number of iterations of an iterative
solver respectively. However, the convergence of the low-rank ap-
N ) [5, 33], which
proximation is proportional to O(1/
implies that the rank R may need to be near-linear to the number
of data points in order to achieve comparable generalization error
compared to the vanilla kernel method. For large-scale problems,
the low-rank approximation could become almost as expensive as
the exact kernel method to maintain competitive performance [29].
Another popular approach for scaling up kernel method is ran-
dom features approximation [21, 22]. Unlike previous approach
that approximates kernel matrix, Random Features approximate the
kernel function directly via sampling from an explicit feature map.
Random Fourier (RF) is one of the feature maps that attracted con-
siderable interests due to its easy implementation and fast execution
time [4, 12, 20, 22, 32], which has total computational cost and stor-
age requirement as O(N Rd + N Rm) and O(N R) respectively,
for computing feature matrix Z and operating the subsequent algo-
rithms on Z. A Fastfood approach and its extension [12, 32] was

proposed to reduce the time of computing Fourier features from
O(Rd) to O(R log d) by leveraging Hadamard basis functions,
which improves the efﬁciency for prediction but not necessarily for
training if d (cid:28) m. Although RF has been successfully applied to
speech recognition and vision classiﬁcations on very large datasets
[3, 10, 17], a drawback is that a signiﬁcant large number of random
features are needed to achieve a comparable performance to exact
kernel method. This is not surprising since the convergence of ap-
N ) [4, 22], which
proximation error is in the order O(1/
is the same as that of low-rank kernel approximations.

R+1/

√

√

Mercer’s theorem [19] guarantees that any positive-deﬁnite ker-
nel permits a feature-map decomposition. However, the decompo-
sition is not unique. One may ﬁnd different feature maps to con-
struct the same kernel function [21, 33]. Therefore, we ask follow-
ing question: do some of the feature maps lead to faster conver-
gence than the others in terms of approximation? In this paper, we
address this question by reconsidering the Random Binning (RB)
feature map, which was proposed in the ﬁrst Random-Feature pa-
per [21] but has drawn much less attentions since then compared to
the RF feature. Our main contributions are fourfold.

First, we propose the ﬁrst analysis of RB from the perspective
of optimization. By interpreting RB as a Randomized Block Coor-
dinate Descent (RBCD) in the inﬁnite-dimensional space induced
from the kernel, we prove that RB enjoys faster convergence than
other random features. Speciﬁcally, by drawing R grids with ex-
pected number of non-empty bins per grid lower bounded by κ,
RB can achieve a solution comparable to exact kernel method with
O(1/(κR)) precision in terms of the objective function, which is
R) rate from Monte Carlo
not only better than the existing O(1/
analysis [21], but also shows a κ times speedup over the rate of
other random features under the same analysis framework [33].

√

Second, we exploit the sparse structure of the feature matrix Z,
which is the key to rapidly transform the data features into a very
high-dimension feature space that is linearly separately by any re-
gressors and classiﬁers. In addition, we discuss how to efﬁciently
perform the computation for a large, sparse matrix by using state-
of-the-art iterative solvers and advanced matrix storage techniques.
As a result, the computational complexity and storage requirements
in training are still O(N Rd + N Rm) and O(N R), respectively.

Third, we show that Random Binning features is particularly
suitable for Parallel Coordinate Descent solver. Unlike other ran-
dom features, RB guarantees a speedup proportional to κ due to
a sparse feature matrix. This is particularly useful in the Sparse
Random Feature setting [33], where L1 regularization is used to
induce a compact nonlinear predictor and Coordinate Descent is
presumably the state-of-the-art solver in such setting.

Finally, we provide extensive experiments to demonstrate the
faster convergence and better parallelizability of RB in practice.
Compared to other popular low-rank approximations, RB shows
superior performance on both regression and classiﬁcation tasks
under the same computational budgets, and achieves same perfor-
mance with one to three orders of magnitude reduction in time and
memory consumptions. When combined with Coordinate Descent
to solve an L1-regularized objective, RB shows an almost linear
speedup, in contrast to RF that has almost no speedup.

2. RANDOM BINNING FEATURE AS KER-

NEL APPROXIMATION

In this work, we consider the problem of ﬁtting a nonlinear pre-
diction function f : X → Y in Reproducing Kernel Hilbert Space
H from training data pairs {(xn, yn)}N
n=1 via regularized Empiri-

cal Risk Minimization (ERM)

f ∗ = argmin

(cid:107)f (cid:107)2

H +

f ∈H

λ
2

1
N

N
(cid:88)

n=1

L(f (xn), yn),

(1)

where L(z, y) is a convex loss function with Lipschitz-continuous
derivative satisfying |L(cid:48)(z1, y) − L(cid:48)(z2, y)| ≤ β|z1 − z2|, which
includes several standard loss functions such as the square-loss
2 (z − y)2, square-hinge loss L(z, y) = max(1 −
L(z, y) = 1
zy, 0)2 and logistic loss L(z, y) = log(1 + exp(−yz)).

2.1 Learning with RKHS

The RKHS H can be deﬁned via a positive-deﬁnite (PD) kernel

function k(x1, x2) that measures similarity between samples as

H =

f (·) =

αik(xi, ·) | αi ∈ R, xi ∈ X

.

(2)

(cid:41)

(cid:40)

K
(cid:88)

i=1

One can also deﬁne the RKHS via a possibly inﬁnite-dimensional
feature map { ¯φh(x)}h∈H with each h ∈ H deﬁning a feature
function ¯φh(x) : X → R. The space can be expressed as

(cid:26)

(cid:90)

H =

f (·) =

h∈H

w(h) ¯φh(·)dh = (cid:104)w, ¯φ(·)(cid:105)H | (cid:107)f (cid:107)2

H < ∞

,

(cid:27)

(3)
where w(h) speciﬁes weights over the set of features {φh(x)}h∈H.
The Mercer’s theorem [19] connects the above two formulations of
RKHS by stating that every PD kernel k(., .) can be expressed as
an integration over some basis functions {φh(.)}h∈H

(cid:90)

h∈H

k(x1, x2) =

p(h)φh(x1)φh(x2)dh = (cid:104) ¯φ(x1), ¯φ(x2)(cid:105)H,

(4)
However, the decomposition (4) is not unique, so one can ﬁnd dif-
ferent feature maps { ¯φh(.)}h∈H satisfying (4) for the same kernel
k(., .). In particular, as an example used extensively in this work,
the Laplacian Kernel

k(x1, x2) = exp

−

(cid:18)

(cid:107)x1 − x2(cid:107)1
σ

(cid:19)

,

(5)

allows decomposition based on (i) Fourier basis map [21], (ii) RB
map [21], and also (iii) map based on inﬁnite number of decision
trees [15] to name a few. On the other hand, different kernels can
be constructed using the same set of basis function {φh(.)} with
different distribution p(h). For example, the RB feature map can
be used to construct any shift-invariant kernel of the form [21]

K(x1, x2) = K(x1 − x2) =

kj(x1j − x2j),

(6)

d
(cid:89)

j=1

by sampling the "width" of bins δj for each feature j from a distri-
bution proportional to δk(cid:48)(cid:48)
j (δ), where k(cid:48)(cid:48)
j (δ) is the second derivative
of kj(δ), assuming the kernel has a non-negative second derivative.

2.2 Random Binning Features

In this section, we describe the Random Binning (RB) feature

map, which has decomposition of the form

K(x1, x2) =

p(δ)φBδ

(x1)T φBδ

(x2) dδ

(7)

(cid:90)

δ

where Bδ is a grid parameterized by δ = (δ1, u1, ..., δd, ud) that
speciﬁes the width and bias of the grid w.r.t. the d dimensions, and
φBδ

(x) is a vector which has

φb(x) = 1, if b = ((cid:98)

(cid:99), ..., (cid:98)

x1 − u1
δ1

xd − ud
δd

(cid:99)),

Figure 1: Generating process of RB features.

Algorithm 1 Random Binning Features

Given a kernel function k(x1, x2) = (cid:81)d
pj(δ) ∝ δk(cid:48)(cid:48)
j (δ) be a distribution over δ.
for r = 1...R do

j=1 kj(|x1j −x2j|). Let

1. Draw δrj ∼ pj(δ), ∀j ∈ [d]. urj ∈ [0, δrj], ∀j ∈ [d].
2. Compute feature zr(xn) as the the indicator vector of bin
index ((cid:98) xn1−u1

(cid:99)), for ∀n ∈ [N ].

(cid:99), ..., (cid:98) xnd−ud

δ1

δd

[z1(xn); ...; zD(xn)] ∀n ∈ [N ] as the

end for.
Return z(xn) = 1√
D
data with RB Features.

and φb(x) = 0 otherwise for any b ∈ Bδ. Note for each grid
Bδ, the number of bins |Bδ| is countably inﬁnite, so φBδ
(x) has
inﬁnite dimension but only 1 non-zero entry (at the bin x lies in).
Figure 1 illustrates an example when the raw dimension d = 2.
The kernel K(x1, x2) is thus interpreted as the collision probabil-
ity that two data points x1, x2 fall in the same bin, when the grid is
generated from distribution p(δ). In [21], it is pointed out for any
kernel of form (6) with nonnegative second derivative k(cid:48)(cid:48)
j (δ), one
can derive distribution p(δ) = (cid:81)d
j=1 pj(δj)U (uj; 0, δj), where
pj(δj) ∝ δk(cid:48)(cid:48)
j (δj) and U (·, a, b) is uniform distribution in the
range [a, b].

To obtain a kernel approximation scheme from the feature map
(7), a simple Monte Carlo method can be used to approximate (7)
by averaging over R grids {Bδr }R
r=1 with each grid’s parameter
δr drawn from p(δ). The procedure for generating R RB features
from raw data {xn}N
n=1 is given in Algorithm 1.
Using a Monte-Carlo analysis, one can show the approximation
R). From the
to (7) yields approximation error of order O(1/
Representer theorem, one can further bound error of the learned
predictor

√

(cid:12)
(cid:12)wT
(cid:12)

RF z(x) − f ∗(x)

n z(xn)T z(x) −
αRF

α∗

nk(xn, x)

N
(cid:88)

n=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

N
(cid:88)

n=1

as shown in [21] (appendix C). Unfortunately, the rate of conver-
gence suggests that to achieve small approximation error (cid:15), one
needs signiﬁcant amount of random features proportional to Ω(1/(cid:15)2),
and furthermore, the Monte-Carlo analysis does not explain why
empirically RB feature achieves faster convergence than other ran-
dom feature map like Fourier basis by orders of magnitude.

3. FASTER CONVERGENCE OF RANDOM

BINNING

In this section, we ﬁrst illustrate the sparse structure of the fea-
ture matrix Z of RB and discuss how to make efﬁcient computa-

Figure 2: Example of the sparse feature matrix ZN ×D gen-
erated by RB. In this special case, Z has the number of rows
N = 200 and columns D = 395, respectively. The number of
grids R = 10 and the nnz(Z) = 2000. Note that for ith row of
Z, nnz(Z(i, :)) = R and R ≤ D ≤ N R.

tion and storage format of Z. Then by interpreting RB features as
Randomized Block Coordinate Descent in the inﬁnite-dimensional
space, we prove that RB has a faster convergence rate than other
random features. We illustrate them accordingly in the following
sections.

3.1 Sparse Feature Matrix & Iterative Solvers
A special characteristic of RB compared to other low-rank ap-
proximations is the fact that the feature matrix generated by RB is
typically a large, sparse binary matrix Z ∈ RN ×D, where the value
of D is determined by both number of grids R and the kernel width
parameter (ex. σ in the case of Laplacian Kernel). Different from
other random features, D, rather than R, is the actual number of
columns of Z. A direct connection between D and R is that the
matrix has each row i satisfying nnz(Z(i, :)) = R and therefore
R ≤ D ≤ N R.
Intuitively speaking, RB has more expressive
power than RF since it generates a large yet sparse feature matrix
to rapidly transform the data space to a very high dimension space,
where data could become almost linearly separable by the classi-
ﬁers. Fig. 2 gives an example to illustrate the sparse structure of
Z.

In the case of Kernel Ridge Regression (L2-regularization with
square loss), if using RB feature to approximate the RKHS, one
can solve (1) directly in its primal form. The weighting vector is
simply the solution of the linear system:

(Z T Z + λI)wRB = Z T y.

(8)

Note since Z is a large sparse matrix, there is no need to explicitly
compute the covariance matrix Z T Z, which is much denser than Z
itself. One can apply state-of-the-art sparse iterative solvers such
as Conjugate Gradient (CG) and GMRES to directly operate on Z
[25]. The main computation in CG or GMRES is the sparse matrix-
vector products. Let m be the number of iterations, then the total
computational complexity of iterative solver is O(m nnz(Z)) =
O(mN R). In addition, since most elements in Z are zeros, the
Compressed Sparse Row type matrix storage format should be em-
ployed for economically storing Z [9], which gives computational
cost and memory requirement as O(mN R) and O(N R) respec-
tively, a similar cost to that of other low-rank approximations de-
spite its much higher dimension. In testing phase, each point pro-
duces a sparse feature vector z(x) ∈ RD based on the grids stored
during training, yielding a sparse vector z(x) with nnz(z(x))) =

R and computing the decision function z(x)T wRB only requires
O(dR + R).

When the ERM is smooth but not quadratic, a Newton-CG method
that solves smooth problem via a series of local quadratic approx-
imation gives the same complexity per CG iteration [14], and note
that most of state-of-the-art linear classiﬁcation algorithms have
complexity linear to nnz(Z), the number of nonzeros of feature
matrix [6]. In section 4, we further discuss cases of L1-regularized
problem, where a Coordinate Descent algorithm of cost O(nnz(Z))
per iteration is discussed.

3.2 Random Binning Features as Block Coor-

dinate Descent

In [33], a new approach of analysis was proposed, which inter-
preted Random Features as Randomized Coordinate Descent in the
inﬁnite dimensional space, and gives a better O(1/R) rate in the
convergence of objective function. In this section, we extend the
approach of [33] to show that, RB Feature can be interpreted as
RBCD in the inﬁnite-dimensional space, which by drawing a block
of features at a time, produces a number of features D signiﬁcantly
more than the number of blocks R, resulting a provably faster con-
vergence rate than other RF. While at the same time, by exploit-
ing state-of-the-art iterative solvers introduced in section 3.1, the
computational complexity of RB does not increase with number of
features D but only with the number of blocks R. Consequently, to
achieve the same accuracy, RB requires signiﬁcantly less training
and prediction time compared to other RF.

A key quantity to our analysis is an upper bound on the collision
probability νδ which speciﬁes how unlikely data points will fall
into the same bin, and its inverse κδ := 1/νδ which lower bounds
the number of bins containing at least one data point. We deﬁne
them as follows.

Deﬁnition 1. Deﬁne collision probability of data D on bin b ∈ Bδ
as

νb :=

|{n ∈ [N ] | φb(xn) = 1}|
N

.

(9)

Let νδ := maxb∈Bδ νb be an upper bound on (9), and κδ := 1/νδ
be a lower bound on the number of nonempty bins of grid δ.

κ := Eδ[κδ] = Eδ[1/νδ]

(10)

is denoted as the lower bound on the expected number of (used)
bins w.r.t the distribution p(δ).

In the RB matrix, the empirical collision probability is simply the
average number of non-zeros per column, divided by N , a number
much smaller than 1 as in the example of Fig. 2. Our analysis
assumes a smooth loss function satisfying the following criteria.

Assumption 1. The loss function L(z, y) is smooth w.r.t. response
z so difference between function difference and its linear approxi-
mation can be bounded as

where Loss( ¯w; φ) = 1
N

(cid:80)N

n=1 L((cid:104) ¯w, φ(xn)(cid:105), yn) and

¯φ :=

(.))δ∈H

√
p ◦ φ = ((cid:112)p(δ)φBδ
with "◦" denoting the component-wise product. The goal is to show
that, by performing R steps of FC-RBCD on (11), one can obtain a
¯wR with comparable regularized loss to that from optimal solution
of (1). Note one advantage of analysis from this optimization per-
spective is: it does not rely on Representer theorem, and thus R(w)
2 (cid:107)w(cid:107)2 or L1 regularizer λ(cid:107)w(cid:107)1, where the
can be L2 regularizer λ
latter has advantage of giving sparse predictor of faster prediction
[33]. The FC-RBCD algorithm maintains an active set of blocks
A(r) which is expanded for R iterations. At each iteration r, the
FC-RBCD does the following:

1. Draw δ from p(δ) ( derived from the kernel k(., .) ).

2. Expand active set A(r+1) := A(r) ∪ Bδ.

3. Minimize (11) subject to a limited support supp( ¯w) ⊆ A(r+1).

Note this algorithm is only used for analysis. In practice, one
can draw R blocks of features at a time, and solve (11) by any
optimization algorithm such as those mentioned in section 3.1 or
the CD method we introduce in section 4.

Due to space limit, here we prove the case when R(.) is the
non-smooth L1 regularizer λ(cid:107) ¯w(cid:107)1. The smooth case for R(w) =
2 (cid:107) ¯w(cid:107)2 can be shown in a similar way. Note the objective function
λ
(11) can be written as
¯F (w) := F (

p ◦ w) + Loss(w, ¯φ).

p ◦ w) = R(

(12)

√

√

√

by a scaling of variable ¯w =

p ◦ w.
The below theorem states that, running FC-RBCD for R itera-
tions, it generates a solution ¯wR close to any reference solution w∗
in terms of objective (12) with their difference bounded by O( 1
κR ).

Theorem 1. Let R be the number of blocks (grids) generated by
FC-RBCD, and w∗ be any reference solution, we have

E[ ¯F (w(R))] − ¯F (w∗) ≤

(13)

β(cid:107)w∗(cid:107)2
κR(cid:48)

for R(cid:48) := R − c > 0, where c = (cid:100) 2κ( ¯F (0)− ¯F (w∗))

(cid:101).

β(cid:107)w∗(cid:107)2

Proof. Firstly, we obtain an expression for the progress made by
each iteration of FC-RBCD. Let B := Bδ(r) be the block drawn at
step 1 of FC-RBCD, and ¯w(r+1) be the minimizer of (11) subject
to support supp( ¯w) ⊆ A(r+1) given by the step 3. Since B ⊆
A(r+1), we have

F ( ¯w(r+1)) − F ( ¯w(r)) ≤ F ( ¯w(r) + ηB) − F ( ¯w(r))

(14)

for any ηB : supp(η) ⊆ B. Then denote bi as the bin xi falling
i = ∇L( ¯w(r)T φ(xi), yi), by smoothness of the loss (As-
in and L(cid:48)
sumption 1), we have

L(z2, .) − L(z1, .) ≤ ∇L(z1, .)(z2 − z1) +

(z2 − z2)2.

Loss( ¯w(r) + ηB) − Loss( ¯w(r)) ≤

L(cid:48)

iφbi η +

(ηbi φbi )2

β
2

1
N

N
(cid:88)

i=1

β
2

for some constant 0 ≤ β ≤ ∞.

This assumption is satisﬁed for a wide range of loss such as
square loss (β = 1), logistic loss (β = 1/4) and L2-hinge loss
(β = 1).

We interpret RB as a Fully Corrective Randomized Block Coor-

dinate Descent (FC-RBCD) on the objective function

min
¯w

F ( ¯w) := R( ¯w) + Loss( ¯w; φ)

(11)

≤ (cid:104)gB, ηB(cid:105) +

βνδ(r)
2

(cid:107)ηB(cid:107)2

where the second inequality uses the fact φbi = 1 and

gB := ∇BLoss( ¯w(r), φ).
Now consider the regularization term, note since block B is drawn
from an iniﬁnite-dimensional space, the probability that B is in

(15)

active set is 0. Therefore, we have B ∩ A(r) = ∅, ¯w(r)
RB( ¯w(r)

B ) = 0. As a result,

B = 0 and

F ( ¯w(r) + ηB) − F ( ¯w(r))

≤ RB(ηB) + (cid:104)gB, ηB(cid:105) +

βνδ(r)
2

(cid:107)ηB(cid:107)2

(16)

Let ηB be the minimizer of RHS of (16). It satisﬁes ρB + gB +
βvδ(r) ηB = 0 for some ρB ∈ ∂R(ηB), and thus,

F ( ¯w(r) + ηB) − F ( ¯w(r))

≤ (cid:104)ρB, ηB(cid:105) + (cid:104)gB, ηB(cid:105) +

= −

1
2βνδ(r)

(cid:107)ρB + gB(cid:107)2

βνδ(r)
2

(cid:107)ηB(cid:107)2

(17)

Now taking expectation w.r.t. p(δ) on both sides of (17), we have

E[F ( ¯w(r) + ηB)] − F ( ¯w(r)) ≤ −

(cid:21)

(cid:107)ρB + gB(cid:107)2
(cid:21)

E (cid:2)(cid:107)ρB + gB(cid:107)2(cid:3)

E

(cid:20) 1
νδ(r)
(cid:20) 1
νδ(r)
(cid:107)¯ρB + ¯gB(cid:107)2

E

1
2β

1
2β
κ
2β

≤ −

≤ −

√

√

p ◦ ρ, ¯g :=

(18)
where ¯ρ :=
p ◦ g, and the second inequality
uses the fact that the number of used bins κδ(r) = 1/νδ(r) has
non-negative correlation with the discriminative power of block
B measured by the magnitude of gradient with soft-thresholding
(cid:107)¯ρB + ¯gB(cid:107) (i.e.
fewer collisions on grid B implies B to be a
better block of features ).

The result of (18) expresses descent amount in terms of the prox-
imal gradient of the reparameterized objective (12). Note for B :
B ∩A(r) = ∅, we have w(r)
B = 0, and RB(¯η)−RB(0) = (cid:104)¯ρ, ¯η(cid:105);
on the other hand, for B ⊆ A(r), we have

0 ∈ arg min
¯ηB

√

√

RB(

pBwB + ¯ηB) + (cid:104)¯gB,

pBwB + ¯ηB(cid:105)

since they are solved to optimality in the previous iteration. Then

E[F ( ¯w(r) + η)] − F ( ¯w(r))

where the second and fourth inequalities are from convexity of
¯F (.). The α minimizing (20) is α∗ := min( κ( ¯F (w(r))− ¯F (w∗))
, 1),
which leads to

β(cid:107)w∗(cid:107)2

E[ ¯F (w(r) + ¯η)] − ¯F (w(r)) ≤ −

κ( ¯F (w(r)) − ¯F (w∗))2
2β(cid:107)w∗(cid:107)2

(21)
κ (cid:107)w∗(cid:107)2; otherwise, we have E[ ¯F (w(r) +
if ¯F (w(r))− ¯F (w∗) ≤ β
¯η)] − ¯F (w(r)) ≤ − β
2κ (cid:107)w∗(cid:107)2. Note the latter case cannot happen
more than c = (cid:100) 2κ( ¯F (0)− ¯F (w∗))
(cid:101) times since FC-RBCD is a de-
scent method. Therefore, for r(cid:48) := r −c > 0, solving the recursion
(21) leads to the conclusion.

β(cid:107)w∗(cid:107)2

√

√

Note we have (cid:107)

p ◦ w∗(cid:107)1 ≤ (cid:107)

p(cid:107)(cid:107)w∗(cid:107) = (cid:107)w∗(cid:107) in the L1-
regularized case, and thus the FC-RBCD guarantees convergence
of the L1-norm objective to the (non-square) L2-norm objective.
The convergence result of Theorem 1 is of the same form to the
rate proved in [33] for other random features, however, with an
additional multiplicative factor κ ≥ 1 that speeds up the rate by
κ times. Recall that κ is the lower bound on the expected number
of bins being used by data samples for each block of features Bδ,
which in practice is a factor much larger than 1, as shown in the
Figure 2 and also in our experiments. In particular, in case each
grid Bδ has similar number of bins being used, we have D ≈ κR,
and thus obtain a rate of the form

E[ ¯F (w(R))] − ¯F (w∗) (cid:46) β(cid:107)w∗(cid:107)2

.

D

(22)

Note for a ﬁxed R, the total number of features D is increasing with
kernel parameter 1/σ in the case of Laplacian Kernel, which means
the less smooth the kernel, the faster convergence of RB. A simple
extreme case is when σ → 0, where one achieves 0 training loss,
and the RB, by putting each sample in a separate bin, converges
to 0 loss with R = 1, D = N . On the other hand, other random
features, such as Fourier, still require large R for convergence to
0 loss. In practice, there are many data that require a small kernel
bandwidth σ to avoid underﬁtting, for which RB has dramatically
faster convergence than other RF.

β
2κ

κ
2β
√

≤ −

(cid:107)¯ρB + ¯gB(cid:107)2 = (cid:104)¯ρ, ¯η(cid:105) + (cid:104)¯g, ¯η(cid:105) +

(cid:107)¯η ¯A(r) (cid:107)2

4. STRONG PARALLELIZABILITY OF RAN-

√

= R(

p ◦ w(r)) + (cid:104)¯g, ¯η(cid:105) +

p ◦ (w(r) + ¯η)) − R(

(cid:107)¯η ¯A(r) (cid:107)2
(19)
where ¯η ¯A(r) := (¯ηB)B:B∩A(r)=∅ and ¯η :=
p◦η. Thus the ﬁnal
step is to show the descent amount given by RHS of (19) decreases
the suboptimality ¯F (w(r)) − ¯F (w∗) signiﬁcantly. This can be
achieved by considering ¯η of the form α(w∗ − w(r)) for some
α ∈ [0, 1] as follows:

√

β
2κ

DOM BINNING FEATURES

In this section, we study another strength of RB Features in the
context of Sparse Random Feature [33], where one aims to train a
sparse nonlinear predictor that has faster prediction and more com-
pact representation through an L1-regularized objective.
In this
case, the CD method is known as state-of-the-art solver [23, 34],
and we aim to show that the structure of RB allows CD to be paral-
lelized with much more speedup than that of other random features.

E[ ¯F (w(r) + ¯η)] − ¯F (w(r))

≤ min

R(

p ◦ (w(r) + ¯η)) − R(

p ◦ w(r)) + (cid:104)¯g, ¯η(cid:105) +

√

√

≤ min

¯F (w(r) + ¯η) − ¯F (w(r)) +

β
2κ

(cid:107)¯η ¯A(r) (cid:107)2

¯F ((1 − α)w(r) + αw∗) − ¯F (w(r)) +

(cid:107)w∗(cid:107)2

βα2
2κ

−α( ¯F (w(r)) − ¯F (w∗)) +

(cid:107)w∗(cid:107)2,

βα2
2κ

¯η

¯η

≤ min
α∈[0,1]

≤ min
α∈[0,1]

4.1 Coordinate Descent Method

β
2κ

(cid:107)¯η ¯A(r) (cid:107)2

a RCD Method solves

Given the N × D data matrix produced by the RB Algorithm 1,

min
w∈RD

λ(cid:107)w(cid:107)1 +

1
N

N
(cid:88)

n=1

L(wT zi, yi)

(23)

by minimizing (23) w.r.t. a single coordinate j

(20)

min
dj

λ|wj + dj| + gjdj +

Mj
2

d2
j

(24)

Algorithm 2 Sparse Random Binning Features via Parallel RCD

0. Generate RB feature matrix Z by Algorithm 1
1. z1 = 0, w1 = 0.
for t=1......T (with τ threads in parallel) do
2. Draw j from [D] uniformly at random.
3. Compute d∗
4. wt+1 := wt + d∗
5. Maintain ˆyi, ∀i ∈ [N ] to satisfy (27).

j by (26).
j ej.

end for

at a time, where

gj :=

(∇jL(wT zi, yi))zij

(25)

1
N

N
(cid:88)

n=1

(cid:80)N

i=1 z2

is the gradient of loss term in (23) w.r.t. the j-th coordinate, and
Mj := β 1
ij is an upper bound on ∇jjL(.). Note, by
N
focusing on single coordinate, (24) has a tighter quadratic upper
bound than other algorithms such as Proximal Gradient Method,
and allows simple closed-form solution

d∗
j := proxR/Mj

(wj −

gj
Mj

) − wj

(26)

where

proxR(vj) :=






0,
vj − λ,
vj + λ,

|vj| ≤ λ
vj > λ
vj < λ

.

To have efﬁcient evaluation of the gradient (25), a practical imple-
mentation maintain the responses

ˆyi := wT zi

(27)

j ej, so the cost for each coordinate-

after each update wt+1 := wt+d∗
wise minimization takes O(nnz(zj)) time for both gradient evalu-
ation and maintenance of (27), where zj := (zij)i∈[N ]. The algo-
rithm is summarized in Alg. 2, which just like the iterative solver
introduced in section 3.1, has cost O(nnz(Z)) for one pass of all
variables j ∈ [D].

4.2 Parallel Randomized Coordinate Descend

on Random Binning Features

The RCD, however, is hard to parallelize [23]. It is known that
simultaneous updates of two coordinates j1, j2 could lead to di-
vergence, and although one can enforce convergence by shortening
the step size 1
, the convergence rate will not be improved
Mp
with parallelization without additional assumption [1, 24].

(cid:28) 1
Mj

On the other hand, in [24], it is shown that a function with par-

tially separable smooth term plus a separable non-smooth term

min
w∈RD

F (w) := Ω(w) +

fi(w)

(28)

N
(cid:88)

i=1

can be parallelized with guaranteed speedup in terms of overall
complexity, where Ω(w) is a non-smooth separable function and
each function fi(w) is a smooth depends only on at most ω num-
ber of variables. The form (28), fortunately, ﬁts our objective (23)
with features zi generated by RB. In particular, the generating pro-
cess of RB guarantees that, for each block of feature Bδ, the i-th
sample can fall in exactly one bin b = ((cid:98) xn1−u1
(cid:99)),
therefore each sample inolves at most R features out of D. Specif-
ically, let Ω(w) := λ(cid:107)w(cid:107)1 and

(cid:99), ..., (cid:98) xnd−ud

δd

δ1

fi(w) :=

L(wT zi, yi),

1
N

we have ω = R. Then by Theorem 19 of [24], a parallel RCD of
τ threads that selects coordinate j uniformly at random achieves a
speed-up (i.e. time-of-sequential/time-of-parallel) of

When D, R (cid:29) 1, and τ = a¯κ+1 where ¯κ := D/R, (29) becomes

speedup-ratio =

τ
1 + (R−1)(τ −1)

D−1

.

speedup-ratio =

a¯κ + 1
1 + a

,

(29)

(30)

which equals (¯κ + 1)/2 when a = 1 and approaches ¯κ when a →
∞. Therefore, it is guaranteed in theory that parallelization can
speedup RCD signiﬁcantly as long as ¯κ = D/R (cid:29) 1. We give
our sparse RB Features algorithm based on parallel RCD in Alg.
2. Note for other Random Features, there is no speedup guaranteed
and our experiment shows that Parallel RCD performed on Random
Fourier features could even have no speedup.

Note that the speedup achieved in this section is orthogonal to
the faster convergence rate achieved in section 3, so by increas-
ing κ, the advantage of RB over other Random Features is super-
linearly increasing if a parallel RCD is used. Note also that the
results (29), (30) also apply to algorithms that utilize Coordinate
Descent as subproblem solvers such as Proximal (Quasi) Newton
Method [13, 35]. Those methods are typically employed for com-
putationally expensive loss functions.

5. EXPERIMENTS

In this section, we present extensive sets of experiments to demon-
strate the efﬁciency and effectiveness of RB. The datasets are cho-
sen to overlap with those in other papers in the literature, where the
details are shown in the table 1. All sets except census are avail-
able at LIBSVM data set [2]. All computations are carried out on a
DELL dual socket with Intel Xeon processors at 2.93GHz for a total
of 16 cores and 250 GB of memory running the SUSE Linux op-
erating system. We implemented all methods in C++ and all dense
matrix operations are performed by using the optimized BLAS and
LAPACK routines provided in the OpenBLAS library. Due to the
limited space, we only choose subsets of our results to present in
each subsection. However, these results are objective and unbiased.

Name
cadata
census
ijcnn1
cod_rna
covtype
SUSY
mnist
acoustic
letter

Table 1: Properties of the datasets.
C: Classes
1
1
2
2
2
2
10
3
26

d: Features N : Train M : Test
16,512
18,186
35,000
49,437
464,809
4,000,000
60,000
78,823
10,500

4,128
2,273
91,701
271,617
116,203
1,000,000
10,000
19,705
5,000

8
119
22
8
54
18
780
50
16

5.1 Effects of σ and R on Random Binning

We perform experiments to investigate the characteristics of RB
by varying the kernel parameter λ and the rank R, respectively.
We use a regularization λ = 0.01 to make sure the reasonable
performance of RB and other low-rank kernels, although we found
that RB is not sensitive to this parameter. We increase the σ in the
large interval from 1e−2 to 1e2 so that the optimal σ locates within
the interval. We apply CG iterative solver to operate on Z directly.

In order to make fair runtime comparison in each run, we set the
tol = 1e − 15 to force similar CG iterations with different σ.

We evaluate the training and testing performance of regression
and classiﬁcation, when varying σ with ﬁxed R. In [21], it does
not consider the effect of σ in their analysis, which however has a
large impact on the performance since D depends on the number
of bins which is controlled by σ. Fig. 3 shows that the training
and testing performance coincidentally decrease (increase) before
they diverge when D grows by increasing σ. This conﬁrms with
our analysis in Theorem 1 that the larger κ, the faster convergence
of RB Feature (recall that the convergence rate is O(1/(κR))).

Second, one should not be surprised that the empirical training
time increases with D. The operations involving the weighting vec-
tor wRB could become as expensive as a sparse matrix-vector oper-
ation in an iterative solver. However, the total computational costs
are still bounded by O(N R) but the constant factor may vary with
different datasets. Fortunately, in most of cases, the training time
corresponding to the peak performance is just slightly higher than
the smallest one. In practice, there are several ways to improve the
computation costs by exploiting more advanced sparse matrix tech-
niques such as preconditioning and efﬁcient storage scheme, which
is out scope of this paper and left for future study.

Finally, we evaluate the training and testing performance when
varying R with ﬁxed σ. Fig.4 shows that the training and testing
performance converge almost linearly with D, which again con-
ﬁrms our analysis in Theorem 1. In addition, we observe that RB
has strong overﬁt ability which turns out to be a strong attribute,
especially when the hypothesis space has not yet saturated.

5.2 Performance Comparisons of All Methods
We present a large sets of experiments to compare RB with other
most popular low-rank kernel approximations, including RF [21],
Nyström [31], and recently proposed independent block approxi-
mation [29]. We also compare all methods with the exact kernel as
a benchmark [26]. We do not report the results of the vanilla ker-
nel on covtype and SUSY since the programs run out of memory.
To make a fair comparison, we also apply CG on RB and Nys-
tröm directly on Z to admit similar computational costs. Since the
independent block kernel approximation approximates the kernel
matrix directly, we employ direct solver of dense matrix for this
method. In practice, the CG iterative solver has no need to solve
in high precision [3], which has also been observed in our experi-
ments. Thus, we set the tolerance to 1e − 3.

Fig.5 clearly demonstrates the superiority of RB compared to
other low-rank kernels. For example, in the ﬁrst column, RB sig-
niﬁcantly outperforms other methods in testing performance on all
of these datasets, especially when R is relatively small. This is
because RB enjoys much faster convergence rate to the optimal
function than other methods. The advantage generally diminishes
when R increases to reasonably large. However, for some large
datasets such as covtype and SUSY, increasing number of random
features or R boosts the performance extremely slow. This is con-
sistent with our analysis that RB enjoys its fast convergence rate
of O(1/(κR)) while other methods has slow convergence rates
R). The third and fourth columns further promote the in-
O(1/
sights about how many number of random features or how large
rank R that is needed for achieving similar performance of RB. In
particular, RB is often between one and three orders of magnitude
faster and less memory consumptions than other methods.

√

In the second column, we also observe that the training time of
all low-rank kernels are linear with R, which is expected since all
these methods has computational complexity of O(kN R). The
difference in training time between these low-rank kernels is only

within some constant factors. However, we point out that the com-
putations of RF, Nyström and independent block approximation are
mainly carried out by the high-optimized BLAS library since they
are dense matrices. In contrast, the computations of RB are most
involved in sparse matrix operations, which are self-implemented
and not yet optimized. In addition, more advanced sparse matrix
techniques such as preconditioning can be explored to signiﬁcantly
accelerate the computation, which we leave it as future work.

5.3 Parallel Performance of Random Binning

and Random Fourier

We perform experiments to compare RB with RF when using
RCD to solve L1-regularized Lasso and kernel SVM for both re-
gression and binary classiﬁcation problems. Since the goal is to
demonstrate the strong parallel performance of RB, we implement
the basic parallel implementation of RCD based on simple shared
memory parallel programming model with OpenMP. We leave the
high-performance distributed RCD implementation as one of the
future works. We deﬁne the speedup of RCD on multicore imple-
mentation as follows:

speedup =

runtime of RCD using single core
runtime using P cores

As shown in Fig.6, when the sparsity level of the feature matrix Z
is high, the near-linear speedup can be achieved [16, 18]. This is
because the minimization problem can almost be separated along
the coordinate axes, then higher degrees of parallelism are possi-
ble. In contrast, if Z is lack of sparsity, then the penalty for data
correlations slows the speedup to none. This is conﬁrmed by no
gain of parallel speedup of RF since Z is always fully dense. Ob-
viously, in order to empower strong parallel performance of RB, a
very large D is expected, which interestingly coincides with power
of its faster convergence. Therefore, one can enjoy the double ben-
eﬁts of fast convergence and strong parallelizability of RB, which
is especially useful for very large-scale problems.

6. CONCLUSIONS

In this paper, we revisit RB features, an overlooked yet very
powerful random features, which we observe often to be orders
of magnitude faster than other random features and kernel approx-
imation methods to achieve the same accuracy. Motivated by these
impressive empirical results, we propose the ﬁrst analysis of RB
from the perspective of optimization, to make a solid attempt to
quantify its faster convergence, which is not captured by tradi-
tional Monte-Carlo analysis. By interpreting RB as a RBCD in
the inﬁnite-dimensional space, we show that by drawing R grids
with at least κ expected number of non-empty bins per grid, RB
achieves a convergence rate of O(1/(κR)). In addition, in the L1-
regularized setting, we demonstrate the sparse structure of RB fea-
tures allows RCD solver to be parallelized with guaranteed speedup
proportional to κ. Our extensive experiments demonstrate the su-
perior performance of the RB features over other random feature
and kernel approximation methods.

7. ACKNOWLEDGEMENT

This work was done while L. Wu was a research intern at IBM
Research. J. Chen is supported in part by the XDATA program
of the Advanced Research Projects Agency (DARPA), adminis-
tered through Air Force Research Laboratory contract FA8750-12-
C-0323.

(a) cadata

(b) cadata

(c) census

(d) census

(e) ijcnn1

(f) ijcnn1

(g) covtype

(h) covtype

(i) SUSY

(j) SUSY

(k) mnist

(l) mnist

(m) acoustic

(n) acoustic

(o) letter

(p) letter

Figure 3: Train and test performance, and train time when varying σ with ﬁxed R. The black line and square box represent the best
test performance of the exact kernel and RB respectively.

(a) cadata

(b) ijcnn1

(c) acoustic

(d) letter

Figure 4: Train and test performance when varying R with ﬁxed σ.

References
[1] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel
coordinate descent for l1-regularized loss minimization. CoRR,
abs/1105.5379, 2011.

[2] C. Chang and C. Lin. Libsvm: a library for support vector machines.

ACM Transactions on Intelligent Systems and Technology,
2:27:1–27:27, 2011.

[3] J. Chen, L. Wu, K. Audhkhasi, B. Kingsbury, and B. Ramabhadran.
Efﬁcient one-vs-one kernel ridge regression for speech recognition.

[24] P. Richtárik and M. Takáˇc. Parallel coordinate descent methods for

big data optimization. Mathematical Programming, pages 1–52,
2015.

[25] Y. Saad. Iterative Methods for Sparse Linear Systems. Society for
Industrial and Applied Mathematics, Philadelphia, PA, USA, 2nd
edition, 2003.

[26] B. Scholkopf and A. J. Smola. Learning with Kernels: Support

Vector Machines, Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA, 2001.

[27] S. Si, C. Hsieh, and I. S. Dhillon. Memory efﬁcient kernel

approximation. In ICML, 2014.

[28] A. J. Smola and B. Schökopf. Sparse greedy matrix approximation

for machine learning. In ICML, ICML ’00, San Francisco, CA, USA,
2000. Morgan Kaufmann Publishers Inc.

[29] M. L. Stein. Limitations on low rank approximations for covariance

matrices of spatial data. Spatial Statistics, 8:1 – 19, 2014. Spatial
Statistics Miami.

[30] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks.

In NIPS. MIT Press, 2004.

[31] C. K. I. Williams and M. Seeger. Using the nyström method to speed

up kernel machines. In NIPS. MIT Press, 2001.

[32] Z. Yang, A. G. Wilson, A. J. Smola, and L. Song. A la carte -

learning fast kernels. In AISTATS, 2015.

[33] I. E.-H. Yen, T.-W. Lin, S.-D. Lin, P. K. Ravikumar, and I. S. Dhillon.
Sparse random feature algorithm as coordinate descent in hilbert
space. In JMLR, 2014.

[34] G.-X. Yuan, K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. A comparison
of optimization methods and software for large-scale l1-regularized
linear classiﬁcation. JMLR, 11:3183–3234, 2010.

[35] K. Zhong, I. E.-H. Yen, I. S. Dhillon, and P. K. Ravikumar. Proximal

quasi-newton for computationally intensive l1-regularized
m-estimators. In Advances in Neural Information Processing
Systems, pages 2375–2383, 2014.

In ICASSP, 2016.

[4] B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. F. Balcan, and

L. Song. Scalable kernel methods via doubly stochastic gradients. In
NIPS. Curran Associates, Inc., 2014.

[5] P. Drineas and M. W. Mahoney. On the nyström method for

approximating a gram matrix for improved kernel-based learning.
JMLR, 6:2153–2175, Dec. 2005.

[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.

Liblinear: A library for large linear classiﬁcation. JMLR,
9:1871–1874, 2008.

[7] S. Fine and K. Scheinberg. Efﬁcient svm training using low-rank

kernel representations. JMLR, 2:243–264, Mar. 2002.

[8] A. Gittens and M. W. Mahoney. Revisiting the nyström method for

improved large-scale machine learning. CoRR, abs/1303.1849, 2013.

[9] G. H. Golub and C. F. Van Loan. Matrix Computations. Johns

Hopkins University Press, Baltimore, MD, USA, 1996.

[10] P.-S. Huang, H. Avron, T. Sainath, V. Sindhwani, and

B. Ramabhadran. Kernel methods match deep neural networks on
timit. In ICASSP, 2014.

[11] S. Kumar, M. Mohri, and A. Talwalkar. Sampling methods for the

nyström method. JMLR, 13:981–1006, Apr. 2012.

[12] Q. V. Le, T. Sarlós, and A. J. Smola. Fastfood: Approximate kernel

expansions in loglinear time. ICML, 2013.

[13] J. Lee, Y. Sun, and M. Saunders. Proximal newton-type methods for
convex optimization. In Advances in Neural Information Processing
Systems, pages 836–844, 2012.

[14] C.-J. Lin, R. C. Weng, and S. S. Keerthi. Trust region newton method

for logistic regression. JMLR, 9:627–650, 2008.

[15] H.-T. Lin and L. Li. Support vector machinery for inﬁnite ensemble

learning. JMLR, 9:285–312, 2008.

[16] J. Liu, S. J. Wright, C. Ré, and V. Bittorf. An asynchronous parallel
stochastic coordinate descent algorithm. JMLR, 32(1):469–477,
2014.

[17] Z. Lu, A. May, K. Liu, A. B. Garakani, D. Guo, A. Bellet, L. Fan,

M. Collins, B. Kingsbury, M. Picheny, and F. Sha. How to scale up
kernel methods to be as good as deep neural nets. CoRR,
abs/1411.4000, 2014.

[18] J. Mareˇcek, P. Richtárik, and M. Takáˇc. Distributed block coordinate
descent for minimizing partially separable functions. Numerical
Analysis and Optimization, 134:261–288, 2014.

[19] J. Mercer. Functions of positive and negative type, and their

connection with the theory of integral equations. Royal Society
London, A 209:415–446, 1909.

[20] M. Raginsky and S. Lazebnik. Locality-sensitive binary codes from
shift-invariant kernels. In Y. Bengio, D. Schuurmans, J. D. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances in Neural
Information Processing Systems 22, pages 1509–1517. Curran
Associates, Inc., 2009.

[21] A. Rahimi and B. Recht. Random features for large-scale kernel

machines. In NIPS. Curran Associates, Inc., 2007.

[22] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks:
Replacing minimization with randomization in learning. In NIPS.
Curran Associates, Inc., 2008.

[23] P. Richtárik and M. Takáˇc. Iteration complexity of randomized
block-coordinate descent methods for minimizing a composite
function. Mathematical Programming, 144(1-2):1–38, 2014.

(a) cadata

(b) cadata

(c) cadata

(d) cadata

(e) ijcnn1

(f) ijcnn1

(g) ijcnn1

(h) ijcnn1

(i) covtype

(j) covtype

(k) covtype

(l) covtype

(m) SUSY

(n) SUSY

(o) SUSY

(p) SUSY

(q) mnist

(r) mnist

(s) mnist

(t) mnist

(u) acoustic

(v) acoustic

(w) acoustic

(x) acoustic

Figure 5: Comparisons among RB, RF, Nyström and Independent Block approximation. The ﬁrst and second columns plot test
performance and train time when increasing R. The third and fourth columns plot the train time and memory consumptions when
achieving the desired test performance.

(a) ijcnn1

(b) cod_rna

(c) covtype

(d) SUSY

Figure 6: Comparisons of parallel performance between RB and RF using RCD when increasing the number of threads.

