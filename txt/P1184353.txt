Journal of Machine Learning Research 18 (2017) 1-37

Submitted 10/16; Revised 6/17; Published 8/17

Learning Scalable Deep Kernels with Recurrent Structure

Maruan Al-Shedivat
Carnegie Mellon University

Andrew Gordon Wilson
Cornell University

Yunus Saatchi

Zhiting Hu
Carnegie Mellon University

Eric P. Xing
Carnegie Mellon University

Editor: Neil Lawrence

alshedivat@cs.cmu.edu

andrew@cornell.edu

saatchi@cantab.net

zhitingh@cs.cmu.edu

epxing@cs.cmu.edu

Abstract
Many applications in speech, robotics, ﬁnance, and biology deal with sequential data, where
ordering matters and recurrent structures are common. However, this structure cannot
be easily captured by standard kernel functions. To model such structure, we propose
expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-
LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent
networks, while retaining the non-parametric probabilistic advantages of Gaussian processes.
We learn the properties of the proposed kernels by optimizing the Gaussian process marginal
likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit
the structure of these kernels for scalable training and prediction. This approach provides a
practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance
on several benchmarks, and thoroughly investigate a consequential autonomous driving
application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.

1. Introduction

There exists a vast array of machine learning applications where the underlying datasets
are sequential. Applications range from the entirety of robotics, to speech, audio and video
processing. While neural network based approaches have dealt with the issue of representation
learning for sequential data, the important question of modeling and propagating uncertainty
across time has rarely been addressed by these models. For a robotics application such as a
self-driving car, however, it is not just desirable, but essential to have complete predictive
densities for variables of interest. When trying to stay in lane and keep a safe following
distance from the vehicle front, knowing the uncertainty associated with lanes and lead
vehicles is as important as the point estimates.

c(cid:13)2017 Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, Eric P. Xing.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/16-498.html.

7
1
0
2
 
t
c
O
 
5
 
 
]

G
L
.
s
c
[
 
 
3
v
6
3
9
8
0
.
0
1
6
1
:
v
i
X
r
a

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Recurrent models with long short-term memory (LSTM) (Hochreiter and Schmidhuber,
1997) have recently emerged as the leading approach to modeling sequential structure. The
LSTM is an eﬃcient gradient-based method for training recurrent networks. LSTMs use
a memory cell inside each hidden unit and a special gating mechanism that stabilizes the
ﬂow of the back-propagated errors, improving the learning process of the model. While
the LSTM provides state-of-the-art results on speech and text data (Graves et al., 2013;
Sutskever et al., 2014), quantifying uncertainty or extracting full predictive distributions
from deep models is still an area of active research (Gal and Ghahramani, 2016a).

In this paper, we quantify the predictive uncertainty of deep models by following a
Bayesian nonparametric approach. In particular, we propose kernel functions which fully
encapsulate the structural properties of LSTMs, for use with Gaussian processes. The
resulting model enables Gaussian processes to achieve state-of-the-art performance on se-
quential regression tasks, while also allowing for a principled representation of uncertainty
and non-parametric ﬂexibility. Further, we develop a provably convergent semi-stochastic op-
timization algorithm that allows mini-batch updates of the recurrent kernels. We empirically
demonstrate that this semi-stochastic approach signiﬁcantly improves upon the standard
non-stochastic ﬁrst-order methods in runtime and in the quality of the converged solution.
For additional scalability, we exploit the algebraic structure of these kernels, decomposing
the relevant covariance matrices into Kronecker products of circulant matrices, for
(n)
training time and
(1) test predictions (Wilson et al., 2015; Wilson and Nickisch, 2015).
Our model not only can be interpreted as a Gaussian process with a recurrent kernel, but
also as a deep recurrent network with probabilistic outputs, inﬁnitely many hidden units,
and a utility function robust to overﬁtting.

O

O

Throughout this paper, we assume basic familiarity with Gaussian processes (GPs).
We provide a brief introduction to GPs in the background section; for a comprehensive
reference, see, e.g., Rasmussen and Williams (2006). In the following sections, we formalize
the problem of learning from sequential data, provide background on recurrent networks
and the LSTM, and present an extensive empirical evaluation of our model. Speciﬁcally, we
apply our model to a number of tasks, including system identiﬁcation, energy forecasting,
and self-driving car applications. Quantitatively, the model is assessed on the data ranging
in size from hundreds of points to almost a million with various signal-to-noise ratios
demonstrating state-of-the-art performance and linear scaling of our approach. Qualitatively,
the model is tested on consequential self-driving applications:
lane estimation and lead
vehicle position prediction. Indeed, the main focus of this paper is on achieving state-
of-the-art performance on consequential applications involving sequential data, following
straightforward and scalable approaches to building highly ﬂexible Gaussian process.

We release our code as a library at: http://github.com/alshedivat/keras-gp. This
library implements the ideas in this paper as well as deep kernel learning (Wilson et al.,
2016a) via a Gaussian process layer that can be added to arbitrary deep architectures
and deep learning frameworks, following the Keras API speciﬁcation. More tutorials and
resources can be found at https://people.orie.cornell.edu/andrew/code.

2

Learning Scalable Deep Kernels with Recurrent Structure

2. Background

L

∈

X

X

· · ·

yi
{

xi
{

i , x2
i ,

i ∈ X

n
i=1, yi
}

i=1 be a collection of sequences, xi = [x1
n
}

We consider the problem of learning a regression function that maps sequences to real-valued
, xli],
target vectors. Formally, let X =
each with corresponding length, li, where xj
is an arbitrary domain. Let
, and
Rd, be a collection of the corresponding real-valued target vectors.
y =
Assuming that only the most recent L steps of a sequence are predictive of the targets, the
Rd, from some family,
, based on the available data.
goal is to learn a function, f :
As a working example, consider the problem of estimating position of the lead vehicle at
the next time step from LIDAR, GPS, and gyroscopic measurements of a self-driving car
available for a number of previous steps. This task is a classical instance of the sequence-
to-reals regression, where a temporal sequence of measurements is regressed to the future
position estimates. In our notation, the sequences of inputs are vectors of measurements,
xn], are indexed by time and would be of
x1 = [x1], x2 = [x1, x2], . . . , xn = [x1, x2,
· · ·
growing lengths. Typically, input sequences are considered up to a ﬁnite-time horizon, L,
that is assumed to be predictive for the future targets of interest. The targets, y1, y2, . . . , yn,
are two-dimensional vectors that encode positions of the lead vehicle in the ego-centric
coordinate system of the self-driving car.

(cid:55)→

F

Note that the problem of learning a mapping, f :

Rd, is challenging. While
considering whole sequences of observations as input features is necessary for capturing
long-term temporal correlations, it virtually blows up the dimensionality of the problem. If
Rp, and consider L previous
we assume that each measurement is p-dimensional, i.e.,
X ⊆
steps as distinct features, the regression problem will become (L
p)-dimensional. Therefore,
to avoid overﬁtting and be able to extract meaningful signal from a ﬁnite amount of data, it
is crucial to exploit the sequential nature of observations.

(cid:55)→

×

X

L

Recurrent models. One of the most successful ways to exploit sequential structure of
the data is by using a class of recurrent models. In the sequence-to-reals regression scenario,
Rd in the following general recurrent form:
such a model expresses the mapping f :

L

(cid:55)→
y = ψ(hL) + (cid:15)t, ht = φ(ht−1, xt) + δt, t = 1, . . . , L,

X

(1)

where xt is an input observation at time t, ht is a corresponding latent representation,
) specify model transitions and emissions,
) and ψ(
and y is a target vector. Functions φ(
·
·
respectively, and δt and (cid:15)t are additive noises. While φ(
) can be arbitrary, they
) and ψ(
·
·
are typically time-invariant. This strong but realistic assumption incorporated into the
structure of the recurrent mapping signiﬁcantly reduces the complexity of the family of
functions,

, regularizes the problem, and helps to avoid severe overﬁtting.

Recurrent models can account for various patterns in sequences by memorizing internal
representations of their dynamics via adjusting φ and ψ. Recurrent neural networks (RNNs)
model recurrent processes by using linear parametric maps followed by nonlinear activations:

F

y = ψ(W (cid:62)

hyht−1), ht = φ(W (cid:62)

hhht−1, W (cid:62)

xhxt−1), t = 1, . . . , L,

(2)

3

Al-Shedivat, Wilson, Saatchi, Hu, Xing

where Why, Whh, Wxh are weight matrices to be learned1 and φ(
) here are some
·
·
ﬁxed element-wise functions. Importantly and contrary to the standard hidden Markov
models (HMMs), the state of an RNN at any time t is distributed and eﬀectively represented
by an entire hidden sequence, [h1,
, ht−1, ht]. A major disadvantage of the vanilla RNNs
is that their training is nontrivial due to the so-called vanishing gradient problem (Bengio
et al., 1994): the error back-propagated through t time steps diminishes exponentially which
makes learning long-term relationships nearly impossible.

) and ψ(

· · ·

LSTM. To overcome vanishing gradients, Hochreiter and Schmidhuber (1997) proposed
a long short-term memory (LSTM) mechanism that places a memory cell into each hidden
unit and uses diﬀerentiable gating variables. The update rules for the hidden representation
at time t have the following form (here σ (
) are element-wise sigmoid and
·
hyperbolic tangent functions, respectively):

) and tanh (
·

(cid:16)

it = tanh

W (cid:62)

xcxt + W (cid:62)

hcht−1 + bc

(cid:17)

,

hiht−1 + W (cid:62)

ci ct−1 + bi

(cid:16)

W (cid:62)

gt
i = σ
ct = gt
c ct−1 + gt
(cid:16)
W (cid:62)

xixt + W (cid:62)
i it,
xf xt + W (cid:62)

(cid:17)

,

(cid:17)

(3)

,

xoxt + W (cid:62)

cf ct−1 + bf

hf ht−1 + W (cid:62)

gt
c = σ
ot = tanh (cid:0)ct(cid:1) ,
(cid:16)
gt
o = σ
ht = gt
As illustrated above, gt
o correspond to the input, forget, and output gates,
respectively. These variables take their values in [0, 1] and when combined with the internal
states, ct, and inputs, xt, in a multiplicative fashion, they play the role of soft gating. The
gating mechanism not only improves the ﬂow of errors through time, but also, allows the
the network to decide whether to keep, erase, or overwrite certain memorized information
based on the forward ﬂow of inputs and the backward ﬂow of errors. This mechanism adds
stability to the network’s memory.

W (cid:62)
o ot.

hoht−1 + W (cid:62)

c, and gt

coct + bo

i, gt

(cid:17)

,

Gaussian processes. The Gaussian process (GP) is a Bayesian nonparametric model
that generalizes the Gaussian distributions to functions. We say that a random function
f is drawn from a GP with a mean function µ and a covariance kernel k, f
(µ, k),
if for any vector of inputs, [x1, x2, . . . , xn], the corresponding vector of function values is
Gaussian:

∼ GP

[f (x1), f (x2), . . . , f (xn)]

(µ, KX,X ) ,

∼ N

with mean µ, such that µi = µ(xi), and covariance matrix KX,X that satisﬁes (KX,X )ij =
k(xi, xj). GPs can be seen as distributions over the reproducing kernel Hilbert space (RKHS)
of functions which is uniquely deﬁned by the kernel function, k (Schölkopf and Smola, 2002).

1. The bias terms are omitted for clarity of presentation.

4

Learning Scalable Deep Kernels with Recurrent Structure

GPs with RBF kernels are known to be universal approximators with prior support to within
an arbitrarily small epsilon band of any continuous function (Micchelli et al., 2006).
x

(cid:0)f (x), σ2(cid:1), and a GP prior on f (x), given
∼ N
training inputs x and training targets y, the predictive distribution of the GP evaluated at
an arbitrary test point x∗ is:

Assuming additive Gaussian noise, y

|

where

f∗

|

x∗, x, y, σ2

(E[f∗], Cov[f∗]) ,

∼ N

E[f∗] = µX∗ + KX∗,X [KX,X + σ2I]−1y,

Cov[f∗] = KX∗,X∗ −

KX∗,X [KX,X + σ2I]−1KX,X∗.

(4)

(5)

Here, KX∗,X , KX,X∗, KX,X , and KX∗,X∗ are matrices that consist of the covariance function,
k, evaluated at the corresponding points, x
X∗, and µX∗ is the mean function
∈
X∗. GPs are ﬁt to the data by optimizing the evidence—the marginal
evaluated at x∗
probability of the data given the model—with respect to kernel hyperparameters. The
evidence has the form:

X and x∗

∈

∈

log P (y

x) =

|

−

(cid:105)
(cid:104)
y(cid:62)(K + σ2I)−1y + log det(K + σ2I)

+ const,

(6)

where we use a shorthand K for KX,X , and K implicitly depends on the kernel hyperpa-
rameters. This objective function consists of a model ﬁt and a complexity penalty term that
results in an automatic Occam’s razor for realizable functions (Rasmussen and Ghahramani,
2001). By optimizing the evidence with respect to the kernel hyperparameters, we eﬀectively
learn the the structure of the space of functional relationships between the inputs and the
targets. For further details on Gaussian processes and relevant literature we refer interested
readers to the classical book by Rasmussen and Williams (2006).

Turning back to the problem of learning from sequential data, it seems natural to apply
the powerful GP machinery to modeling complicated relationships. However, GPs are
limited to learning only pairwise correlations between the inputs and are unable to account
for long-term dependencies, often dismissing complex temporal structures. Combining GPs
with recurrent models has potential to addresses this issue.

3. Related work

The problem of learning from sequential data, especially from temporal sequences, is well
known in the control and dynamical systems literature. Stochastic temporal processes
are usually described either with generative autoregressive models (AM) or with state-
space models (SSM) (Van Overschee and De Moor, 2012). The former approach includes
nonlinear auto-regressive models with exogenous inputs (NARX) that are constructed by
using, e.g., neural networks (Lin et al., 1996) or Gaussian processes (Kocijan et al., 2005).
The latter approach additionally introduces unobservable variables, the state, and constructs

5

Al-Shedivat, Wilson, Saatchi, Hu, Xing

autoregressive dynamics in the latent space. This construction allows to represent and
propagate uncertainty through time by explicitly modeling the signal (via the state evolution)
and the noise. Generative SSMs can be also used in conjunction with discriminative models
via the Fisher kernel (Jaakkola and Haussler, 1999).

Modeling time series with GPs is equivalent to using linear-Gaussian autoregressive or
SSM models (Box et al., 1994). Learning and inference are eﬃcient in such models, but they
are not designed to capture long-term dependencies or correlations beyond pairwise. Wang
et al. (2005) introduced GP-based state-space models (GP-SSM) that use GPs for transition
and/or observation functions. These models appear to be more general and ﬂexible as they
account for uncertainty in the state dynamics, though require complicated approximate
training and inference, which are hard to scale (Turner et al., 2010; Frigola et al., 2014).

Perhaps the most recent relevant work to our approach is recurrent Gaussian processes
(RGP) (Mattos et al., 2015). RGP extends the GP-SSM framework to regression on sequences
by using a recurrent architecture with GP-based activation functions. The structure of
the RGP model mimics the standard RNN, where every parametric layer is substituted
with a Gaussian process. This procedure allows one to propagate uncertainty throughout
the network for an additional cost. Inference is intractable in RGP, and eﬃcient training
requires a sophisticated approximation procedure, the so-called recurrent variational Bayes.
In addition, the authors have to turn to RNN-based approximation of the variational mean
functions to battle the growth of the number of variational parameters with the size of
data. While technically promising, RGP seems problematic from the application perspective,
especially in its implementation and scalability aspects.

Our model has several distinctions with prior work aiming to regress sequences to
reals. Firstly, one of our goals is to keep the model as simple as possible while being
able to represent and quantify predictive uncertainty. We maintain an analytical objective
function and refrain from complicated and diﬃcult-to-diagnose inference schemes. This
simplicity is achieved by giving up the idea of propagating signal through a chain GPs
connected in a recurrent fashion. Instead, we propose to directly learn kernels with recurrent
structure via joint optimization of a simple functional composition of a standard GP with
a recurrent model (e.g., LSTM), as described in detail in the following section. Similar
approaches have recently been explored and proved to be fruitful for non-recurrent deep
networks (Wilson et al., 2016a,b; Calandra et al., 2016). We remark that combinations of
GPs with nonlinear functions have also been considered in the past in a slightly diﬀerent
setting of warped regression targets (Snelson et al., 2003; Wilson and Ghahramani, 2010;
Lázaro-Gredilla, 2012). Additionally, uncertainty over the recurrent parts of our model is
represented via dropout, which is computationally cheap and turns out to be equivalent
to approximate Bayesian inference in a deep Gaussian process (Damianou and Lawrence,
2013) with particular intermediate kernels (Gal and Ghahramani, 2016a,b). Finally, one
can also view our model as a standalone ﬂexible Gaussian process, which leverages learning
techniques that scale to massive datasets (Wilson and Nickisch, 2015; Wilson et al., 2015).

6

Learning Scalable Deep Kernels with Recurrent Structure

y5

g3

h3
3

x5

y4

g2

h3
2

x4

x2

h2
3

x4

h1
3

x3

y3

h3

x3

h1

x1

h2

x2

(a)

y3

g1

h3
1

x3

h2
1

x2

h1
1

x1

h2
2

x3

h1
2

x2

x1

x1

(b)

y

g

(c)

φ

x

Figure 1: (a) Graphical representation of a recurrent model (RNN/LSTM) that maps an input
sequence to a target value in one-step-ahead prediction manner. Shaded variables are observable,
diamond variables denote deterministic dependence on the inputs. (b) Graphical model for GP-
LSTM with a time lag, L = 3, two training time points, t = 3 and t = 4, and a testing time point,
t = 5. Latent representations are mapped to the outputs through a Gaussian ﬁeld (denoted in red)
that globally correlates predictions. Dashed variables represent data instances unused at the given
time step. (c) Graphical representation of a GP with a kernel structured with a parametric map, φ.

4. Learning recurrent kernels

Gaussian processes with diﬀerent kernel functions correspond to diﬀerent structured proba-
bilistic models. For example, some special cases of the Matérn class of kernels induce models
with Markovian structure (Stein, 1999). To construct deep kernels with recurrent structure
we transform the original input space with an LSTM network and build a kernel directly in
the transformed space, as shown in Figure 1b.

In particular, let L φ :

L

. The decomposition of the kernel and the transformation, ˜k = k

be an arbitrary deterministic transformation of the
R be a real-valued kernel
φ, is deﬁned

. Next, let k :

(cid:55)→

H

H

2

X

(cid:55)→ H

input sequences into some latent space,
deﬁned on
as

H

˜k(x, x(cid:48)) = k(φ(x), φ(x(cid:48))), where x, x(cid:48)

L, and ˜k : (cid:0)

L(cid:1)2

X

(cid:55)→

∈ X

(7)

◦

R.

It is trivial to show that ˜k(x, x(cid:48)) is a valid kernel deﬁned on
L (MacKay, 1998, Ch. 5.4.3).
In addition, if φ(
) is represented by a neural network, the resulting model can be viewed as
·
the same network, but with an additional GP-layer and the negative log marginal likelihood
(NLML) objective function used instead of the standard mean squared error (MSE).

X

7

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Input embedding is well-known in the Gaussian process literature (e.g. MacKay, 1998;
Hinton and Salakhutdinov, 2008). Recently, Wilson et al. (2016a,b) have successfully scaled
the approach and demonstrated strong results in regression and classiﬁcation tasks for
kernels based on feedforward and convolutional architectures. In this paper, we apply the
same technique to learn kernels with recurrent structure by transforming input sequences
with a recurrent neural network that acts as φ(
). In particular, a (multi-layer) LSTM
·
architecture is used to embed L steps of the input time series into a single vector in the
hidden space,
. For the embedding, as common, we use the last hidden vector produced by
the recurrent network. Note that however, any variations to the embedding (e.g., using other
types of recurrent units, adding 1-dimensional pooling layers, or attention mechanisms) are
all fairly straightforward2. More generally, the recurrent transformation can be random itself
(Figure 1), which would enable direct modeling of uncertainty within the recurrent dynamics,
but would also require inference for φ (e.g., as in Mattos et al., 2015). In this study, we
limit our consideration of random recurrent maps to only those induced by dropout.

H

Unfortunately, once the the MSE objective is substituted with NLML, it no longer factor-
izes over the data. This prevents us from using the well-established stochastic optimization
techniques for training our recurrent model. In the case of feedforward and convolutional
networks, Wilson et al. (2016a) proposed to pre-train the input transformations and then
ﬁne-tune them by jointly optimizing the GP marginal likelihood with respect to hyperpa-
rameters and the network weights using full-batch algorithms. When the transformation
is recurrent, stochastic updates play a key role. Therefore, we propose a semi-stochastic
block-gradient optimization procedure which allows mini-batching weight updates and fully
joint training of the model from scratch.

4.1 Optimization

The negative log marginal likelihood of the Gaussian process has the following form:

(K) = y(cid:62)(Ky + σ2I)−1y + log det(Ky + σ2I) + const,

(8)

L

where Ky + σ2I (∆= K) is the Gram kernel matrix, Ky, is computed on
N
i=1 and
implicitly depends on the base kernel hyperparameters, θ, and the parameters of the
recurrent neural transformation, φ(
), denoted W and further referred as the transformation
·
hyperparameters. Our goal is to optimize

with respect to both θ and W .

φ(xi)
}
{

The derivative of the NLML objective with respect to θ is standard and takes the

L

following form (Rasmussen and Williams, 2006):

(cid:18)(cid:104)

∂
L
∂θ

1
2

=

tr

K−1yy(cid:62)K−1

K−1(cid:105) ∂K
∂θ

−

(cid:19)

,

(9)

2. Details on the particular architectures used in our empirical study are discussed in the next section.

8

Learning Scalable Deep Kernels with Recurrent Structure

where ∂K/∂θ is depends on the kernel function, k(
,
·
The derivative with respect to the l-th transformation hyperparameter, Wl, is as follows:3
(cid:41)

), and usually has an analytic form.
·

K−1(cid:17)

(cid:40)(cid:18) ∂k(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

ij

−

+

(cid:18) ∂k(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

,

K−1yy(cid:62)K−1

∂
L
∂Wl

=

1
2

(cid:88)

(cid:16)

i,j

(10)
where hi = φ(xi) corresponds to the latent representation of the the i-th data instance. Once
the derivatives are computed, the model can be trained with any ﬁrst-order or quasi-Newton
optimization routine. However, application of the stochastic gradient method—the de facto
standard optimization routine for deep recurrent networks—is not straightforward: neither
the objective, nor its derivatives factorize over the data4 due to the kernel matrix inverses,
and hence convergence is not guaranteed.

Semi-stochastic alternating gradient descent. Observe that once the kernel matrix,
K−1(cid:1), can be precomputed on the full data
K, is ﬁxed, the expression, (cid:0)K−1yy(cid:62)K−1
−
and ﬁxed. Subsequently, Eq. (10) turns into a weighted sum of independent functions of
each data point. This observation suggests that, given a ﬁxed kernel matrix, one could
compute a stochastic update for W on a mini-batch of training points by only using the
corresponding sub-matrix of K. Hence, we propose to optimize GPs with recurrent kernels
in a semi-stochastic fashion, alternating between updating the kernel hyperparameters, θ,
on the full data ﬁrst, and then updating the weights of the recurrent network, W , using
stochastic steps. The procedure is given in Algorithm 1.

Semi-stochastic alternating gradient descent is a special case of block-stochastic gradient
iteration (Xu and Yin, 2015). While the latter splits the variables into arbitrary blocks
and applies Gauss–Seidel type stochastic gradient updates to each of them, our procedure
alternates between applying deterministic updates to θ and stochastic updates to W of the
form θ(t+1)
5. The corresponding Algorithm 1
is provably convergent for convex and non-convex problems under certain conditions. The
following theorem adapts results of Xu and Yin (2015) to our optimization scheme.

θ and W (t+1)

W (t) + λ(t)

θ(t) + λ(t)

θ g(t)

W g(t)

←

←

W

Theorem 1 (informal) Semi-stochastic alternating gradient descent converges to a ﬁxed
point when the learning rate, λt, decays as Θ(1/t

1+δ
2 ) for any δ

(0, 1].

∈

Applying alternating gradient to our case has a catch: the kernel matrix (and its inverse)
has to be updated each time W and θ are changed, i.e., on every mini-batch iteration
(marked red in Algorithm 1). Computationally, this updating strategy defeats the purpose
of stochastic gradients because we have to use the entire data on each step. To deal with
the issue of computational eﬃciency, we use ideas from asynchronous optimization.

3. Step-by-step derivations are given in Appendix B.
4. Cannot be represented as sums of independent functions of each data point.
5. In principle, stochastic updates of θ are also possible. As we will see next, we choose in practice to keep
the kernel matrix ﬁxed while performing stochastic updates. Due to sensitivity of the kernel to even
small changes in θ, convergence of the fully stochastic scheme is fragile.

9

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Algorithm 1 Semi-stochastic alternating
gradient descent.
input Data – (X, y), kernel – kθ(
,
·
recurrent transformation – φw(

1: Initialize θ and w; compute initial K.
2: repeat
3:

for all mini-batches Xb in X do

),
·
).
·

4:

5:

θ + updateθ(X, θ, K). and
θ
w + updatew(Xb, w, K).
w
Update the kernel matrix, K.

←
←

end for

6:
7: until Convergence
output Optimal θ∗ and w∗

Algorithm 2 Semi-stochastic asynchronous
gradient descent.
input Data – (X, y), kernel – kθ(
,
·
recurrent transformation – φw(
·

1: Initialize θ and w; compute initial K.
2: repeat
θ
θ + updateθ(X, w, K).
3:
for all mini-batches Xb in X do

),
·
).

←

4:

5:

6:

w + updatew(Xb, w, Kstale).

w
←
end for
Update the kernel matrix, K.

7:
8: until Convergence
output Optimal θ∗ and w∗

Asynchronous techniques. One of the recent trends in parallel and distributed
optimization is applying updates in an asynchronous fashion (Agarwal and Duchi, 2011).
Such strategies naturally require some tolerance to delays in parameter updates (Langford
et al., 2009). In our case, we modify Algorithm 1 to allow delayed kernel matrix updates.

The key observation is very intuitive: when the stochastic updates of W are small
enough, K does not change much between mini-batches, and hence we can perform multiple
stochastic steps for W before re-computing the kernel matrix, K, and still converge. For
example, K may be updated once at the end of each pass through the entire data (see
Algorithm 2). To ensure convergence of the algorithm, it is important to strike the balance
between (a) the learning rate for W and (b) the frequency of the kernel matrix updates.
The following theorem provides convergence results under certain conditions.

Theorem 2 (informal) Semi-stochastic gradient descent with τ -delayed kernel updates
(0, 1].
converges to a ﬁxed point when the learning rate, λt, decays as Θ(1/τ t

1+δ
2 ) for any δ

∈

Formal statements, conditions, and proofs for Theorems 1 and 2 are given in Appendix C.2.
Why stochastic optimization? GPs with recurrent kernels can be also trained with
full-batch gradient descent, as proposed by Wilson et al. (2016a). However, stochastic
gradient methods have been proved to attain better generalization (Hardt et al., 2016)
and often demonstrate superior performance in deep and recurrent architectures (Wilson
and Martinez, 2003). Moreover, stochastic methods are ‘online’, i.e., they update model
parameters based on subsets of an incoming data stream, and hence can scale to very large
datasets. In our experiments, we demonstrate that GPs with recurrent kernels trained with
Algorithm 2 converge faster (i.e., require fewer passes through the data) and attain better
performance than if trained with full-batch techniques.

10

Learning Scalable Deep Kernels with Recurrent Structure

Stochastic variational inference. Stochastic variational inference (SVI) in Gaussian
processes (Hensman et al., 2013) is another viable approach to enabling stochastic opti-
mization for GPs with recurrent kernels. Such method would optimize a variational lower
bound on the original objective that factorizes over the data by construction. Recently,
Wilson et al. (2016b) developed such a stochastic variational approach in the context of deep
kernel learning. Note that unlike all previous existing work, our proposed approach does
not require a variational approximation to the marginal likelihood to perform mini-batch
training of Gaussian processes.

4.2 Scalability

×

(n3) computations for n training data points, and

Learning and inference with Gaussian processes requires solving a linear system involving
n kernel matrix, K−1y, and computing a log determinant over K. These operations
an n
(n2) storage. In our
typically require
approach, scalability is achieved through semi-stochastic training and structure-exploiting
inference. In particular, asynchronous semi-stochastic gradient descent reduces both the
total number of passes through the data required for the model to converge and the number
of calls to the linear system solver; exploiting the structure of the kernels signiﬁcantly
reduces the time and memory complexities of the linear algebraic operations.

O

O

More precisely, we replace all instances of the covariance matrix Ky with W KU,U W (cid:62),
where W is a sparse interpolation matrix, and KU,U is the covariance matrix evaluated
over m latent inducing points, which decomposes into a Kronecker product of circulant
matrices (Wilson and Nickisch, 2015; Wilson et al., 2015). This construction makes inference
and learning scale as
(1), while preserving model structure.
For the sake of completeness, we provide an overview of the underlying algebraic machinery
in Appendix A.

(n) and test predictions be

O

O

At a high level, because W is sparse and KU,U is structured it is possible to take extremely
fast matrix vector multiplications (MVMs) with the approximate covariance matrix KX,X .
One can then use methods such as linear conjugate gradients, which only use MVMs, to
eﬃciently solve linear systems. MVM or scaled eigenvalue approaches (Wilson and Nickisch,
2015; Wilson et al., 2015) can also be used to eﬃciently compute the log determinant and
its derivatives. Kernel interpolation (Wilson et al., 2015) also enables fast predictions, as we
describe further in the Appendix.

5. Experiments

We compare the proposed Gaussian processes with recurrent kernels based on RNN and
LSTM architectures (GP-RNN/LSTM) with a number of baselines on datasets of various
complexity and ranging in size from hundreds to almost a million of time points. For the
datasets with more than a few thousand points, we use a massively scalable version of GPs
(see Section 4.2) and demonstrate its scalability during inference and learning. We carry

11

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Table 1: Statistics for the data used in experiments. SNR was determined by assuming a certain
degree of smoothness of the signal, ﬁtting kernel ridge regression with RBF kernel to predict the
targets from the input time series, and regarding the residuals as the noise. Tasks with low average
correlation between inputs and targets and lower SNR are harder prediction problems.

Dataset Task

# time steps # dim # outputs Abs. corr.

SNR

Drives
Actuator

GEF

Car

system ident.

power load
wind power

speed
gyro yaw
lanes
lead vehicle

500
1,024

38,064
130,963

932,939

1
1

11
16

6
6
26
9

1
1

1
1

1
1
16
2

0.7994
0.0938

0.5147
0.1731

0.1196
0.0764
0.0816
0.1099

25.72
12.47

89.93
4.06

159.33
3.19
—
—

out a number of experiments that help to gain empirical insights about the convergence
properties of the proposed optimization procedure with delayed kernel updates. Additionally,
we analyze the regularization properties of GP-RNN/LSTM and compare them with other
techniques, such as dropout. Finally, we apply the model to the problem of lane estimation
and lead vehicle position prediction, both critical in autonomous driving applications.

5.1 Data and the setup

Below, we describe each dataset we used in our experiments and the associated prediction
tasks. The essential statistics for the datasets are summarized in Table 1.

System identiﬁcation. In the ﬁrst set of experiments, we used publicly available non-
linear system identiﬁcation datasets: Actuator 6 (Sjöberg et al., 1995) and Drives 7 (Wigren,
2010). Both datasets had one dimensional input and output time series. Actuator had the
size of the valve opening as the input and the resulting change in oil pressure as the output.
Drives was from a system with motors that drive a pulley using a ﬂexible belt; the input
was the sum of voltages applied to the motors and the output was the speed of the belt.

Smart grid data8. We considered the problem of forecasting for the smart grid that
consisted of two tasks (Figure 2). The ﬁrst task was to predict power load from the historical
temperature data. The data had 11 input time series coming from hourly measurements of
temperature on 11 zones and an output time series that represented the cumulative hourly
power load on a U.S. utility. The second task was to predict power generated by wind
farms from the wind forecasts. The data consisted of 4 diﬀerent hourly forecasts of the
wind and hourly values of the generated power by a wind farm. Each wind forecast was a

6. http://www.iau.dtu.dk/nnbook/systems.html
7. http://www.it.uu.se/research/publications/reports/2010-020/NonlinearData.zip.
8. The smart grid data were taken from Global Energy Forecasting Kaggle competitions organized in 2012.

12

Learning Scalable Deep Kernels with Recurrent Structure

Figure 2: Left: Visualization of the GEF-power time series for two zones and the cumulative load
with the time resolution of 1 day. Cumulative power load is generally negatively correlated with
the temperature measurements on all the zones. Right: Visualization of the GEF-wind time series
with the time resolution of 1 day.

4-element vector that corresponded to zonal component, meridional component, wind speed
and wind angle. In our experiments, we concatenated the 4 diﬀerent 4-element forecasts,
which resulted in a 16-dimensional input time series.

Self-driving car dataset9. One of the main target applications of the proposed model
is prediction for autonomous driving. We considered a large dataset coming from sensors
of a self-driving car that was recorded on two car trips with discretization of 10 ms. The
data featured two sets of GPS ECEF locations, ECEF velocities, measurements from a
ﬁber-optic gyro compass, LIDAR, and a few more time series from a variety of IMU sensors.
Additionally, locations of the left and right lanes were extracted from a video stream for
each time step as well as the position of the lead vehicle from the LIDAR measurements. We
considered the data from the ﬁrst trip for training and from the second trip for validation
and testing. A visualization of the car routes with 25 second discretization in the ENU
coordinates are given in Figure 3. We consider four tasks, the ﬁrst two of which are more of
proof-of-concept type variety, while the ﬁnal two are fundamental to good performance for
a self-driving car:

1. Speed prediction from noisy GPS velocity estimates and gyroscopic inputs.

2. Prediction of the angular acceleration of the car from the estimates of its speed and

steering angle.

3. Point-wise prediction of the lanes from the estimates at the previous time steps, and

estimates of speed, gyroscopic and compass measurements.

4. Prediction of the lead vehicle location from its location at the previous time steps,

and estimates of speed, gyroscopic and compass measurements.

We provide more speciﬁc details on the smart grid data and self-driving data in Appendix D.

9. The dataset is proprietary.

It was released in part for public use under the Creative Commons
Attribution 3.0 license: http://archive.org/details/comma-dataset. More about the self-driving car:
http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/.

13

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Train

Test

Time

42.8 min

46.5 min

Speed of the self-driving car

Min

Max

0.0 mph

0.0 mph

80.3 mph

70.1 mph

Average

42.4 mph

38.4 mph

Median

58.9 mph

44.8 mph

Distance to the lead vehicle

Min

Max

Average

Median

23.7 m

29.7 m

178.7 m

184.7 m

85.4 m

54.9 m

72.6 m

53.0 m

Figure 3 & Table 2: Left: Train and test routes of the self-driving car in the ENU coordinates
with the origin at the starting location. Arrows point in the direction of motion; color encodes the
speed. Insets zoom selected regions of the routes. Best viewed in color. Right: Summary of the
data collected on the train and test routes.

Models and metrics. We used a number of classical baselines: NARX (Lin et al.,
1996), GP-NARX models (Kocijan et al., 2005), and classical RNN and LSTM architectures.
The kernels of our models, GP-NARX/RNN/LSTM, used the ARD base kernel function and
were structured by the corresponding baselines.10 As the primary metric, we used root mean
squared error (RMSE) on a held out set and additionally negative log marginal likelihood
(NLML) on the training set for the GP-based models.

We train all the models to perform one-step-ahead prediction in an autoregressive setting,
where targets at the future time steps are predicted from the input and target values at a
ﬁxed number of past time steps. For the system identiﬁcation task, we additionally consider
the non-autoregressive scenario (i.e., mapping only input sequences to the future targets),
where we are performing prediction in the free simulation mode, and included recurrent
Gaussian processes (Mattos et al., 2015) in our comparison. In this case, none of the future
targets are available and the models have to re-use their own past predictions to produce
future forecasts).

A note on implementation. Recurrent parts of each model were implemented using
Keras 11 library. We extended Keras with the Gaussian process layer and developed a backed
engine based on the GPML library12. Our approach allows us to take full advantage of
the functionality available in Keras and GPML, e.g., use automatic diﬀerentiation for the
recurrent part of the model. Our code is available at http://github.com/alshedivat/kgp/.

10. GP-NARX is a special instance of our more general framework and we trained it using the proposed

semi-stochastic algorithm.

11. http://www.keras.io
12. http://www.gaussianprocess.org/gpml/code/matlab/doc/

14

Learning Scalable Deep Kernels with Recurrent Structure

Figure 4: Two charts on the left: Convergence of the optimization in terms of RMSE on test and
NLML on train. The inset zooms the region of the plot right beneath it using log scale for the
vertical axis. full and mini denote full-batch and mini-batch optimization procedures, respectively,
while 16 and 64 refer to models with the respective number of units per hidden layer. Two charts on
the right: Test RMSE for a given architecture trained with a speciﬁed method and/or learning rate.

5.2 Analysis

This section discusses quantitative and qualitative experimental results. We only brieﬂy
introduce the model architectures and the training schemes used in each of the experiments.
We provide a comprehensive summary of these details in Appendix E.

5.2.1 Convergence of the optimization

To address the question of whether stochastic optimization of recurrent kernels is necessary
and to assess the behavior of the proposed optimization scheme with delayed kernel updates,
we conducted a number of experiments on the Actuator dataset (Figure 4).

First, we constructed two GP-LSTM models with 1 recurrent hidden layer and 16 or 64
hidden units and trained them with (non-stochastic) full-batch iterative procedure (similar
to the proposal of Wilson et al. (2016a)) and with our semi-stochastic optimizer with delayed
kernel updates (Algorithm 2). The convergence results are given on the ﬁrst two charts.
Both in terms of the error on a held out set and the NLML on the training set, the models
trained with mini-batches converged faster and demonstrated better ﬁnal performance.

Next, we compared the two optimization schemes on the same GP-LSTM architecture
with diﬀerent sizes of the hidden layer ranging from 2 to 32. It is clear from the third chart
that, even though full-batch approach seemed to ﬁnd a better optimum when the number of
hidden units was small, the stochastic approach was clearly superior for larger hidden layers.
Finally, we compared the behavior of Algorithm 2 with diﬀerent number of mini-batches
used for each epoch (equivalently, the number of steps between the kernel matrix updates)
and diﬀerent learning rates. The results are give on the last chart. As expected, there is a
ﬁne balance between the number of mini-batches and the learning rate: if the number of
mini-batches is large (i.e., the delay between the kernel updates becomes too long) while the
learning rate is high enough, optimization does not converge; at the same time, an appropriate
combination of the learning rate and the mini-batch size leads better generalization than
the default batch approach of Wilson et al. (2016a).

15

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Table 3: Average performance of the models in terms of RMSE on the system identiﬁcation tasks.
The averages were computed over 5 runs; the standard deviation is given in the parenthesis. Results
for the RGP model are as reported by Mattos et al. (2015), available only for the free simulation.

regression

auto-regression

free simulation

regression

free simulation

Drives

0.19 (0.03)
0.17 (0.04)
0.14 (0.02)

0.16 (0.04)
0.16 (0.03)
0.13 (0.02)

0.38 (0.03)
0.56 (0.03)
0.40 (0.02)

0.28 (0.02)
0.45 (0.03)
0.32 (0.03)

0.49 (0.05)
0.56 (0.03)
0.40 (0.03)

0.46 (0.03)
0.49 (0.02)
0.36 (0.01)

Actuator
auto-regression

0.18 (0.01)
0.17 (0.01)
0.19 (0.01)

0.14 (0.01)
0.15 (0.01)
0.14 (0.01)

0.57 (0.04)
0.68 (0.05)
0.44 (0.03)

0.63 (0.04)
0.55 (0.04)
0.43 (0.03)

NARX
RNN
LSTM

0.33 (0.02)
0.53 (0.02)
0.29 (0.02)

GP-NARX 0.28 (0.02)
GP-RNN
0.37 (0.04)
GP-LSTM 0.25 (0.02)

RGP

—

—

0.249

—

—

0.368

5.2.2 Regression, Auto-regression, and Free Simulation

In this set of experiments, our main goal is to provide a comparison between three diﬀerent
modes of one-step-ahead prediction, referred to as (i) regression, (ii) autoregression, and
(iii) free simulation, and compare performance of our models with RGP—a classical RNN
with every parametric layer substituted with a Gaussian process (Mattos et al., 2015)—on
the Actuator and Drives datasets. The diﬀerence between the prediction modes consists in
whether and how the information about the past targets is used. In the regression scenario,
inputs and targets are separate time series and the model learns to map input values at
a number of past time points to a target value at a future point in time. Autoregression,
additionally, uses the true past target values as inputs; in the free simulation mode, the
model learns to map past inputs and its own past predictions to a future target.

In the experiments in autoregression and free simulation modes, we used short time lags,
L = 10, as suggested by Mattos et al. (2015). In the regression mode, since the model does
not build the recurrent relationships based on the information about the targets (or their
estimates), it generally requires larger time lags that can capture the state of the dynamics.
Hence we increased the time lag to 32 in the regression mode. More details are given in
Appendix E.

We present the results in Table 3. We note that GP-based architectures consistently
yielded improved predictive performance compared to their vanilla deep learning counterparts
on both of the datasets, in each mode. Given the small size of the datasets, we attribute
such behavior to better regularization properties of the negative log marginal likelihood loss
function. We also found out that when GP-based models were initialized with weights of
pre-trained neural networks, they tended to overﬁt and give overly conﬁdent predictions
on these tasks. The best performance was achieved when the models were trained from a
random initialization (contrary to the ﬁndings of Wilson et al., 2016a). In free simulation
mode RGP performs best of the compared models. This result is expected—RGP was
particularly designed to represent and propagate uncertainty through a recurrent process.

16

Learning Scalable Deep Kernels with Recurrent Structure

Table 4: Average performance of the best models in terms of RMSE on the GEF and Car tasks.
The averages were computed over 5 runs; the standard deviation is given in the parenthesis.

GEF
power load wind power

speed

gyro yaw

lanes

lead vehicle

Car

NARX
RNN
LSTM

0.54 (0.02)
0.61 (0.02)
0.45 (0.01)

GP-NARX 0.78 (0.03)
GP-RNN
0.24 (0.02)
GP-LSTM 0.17 (0.02)

0.84 (0.01)
0.81 (0.01)
0.77 (0.01)

0.83 (0.02)
0.79 (0.01)
0.76 (0.01)

0.114 (0.010)
0.152 (0.012)
0.027 (0.008)

0.19 (0.01)
0.22 (0.01)
0.13 (0.01)

0.13 (0.01)
0.33 (0.02)
0.08 (0.01)

0.41 (0.02)
0.44 (0.03)
0.40 (0.01)

0.125 (0.015)
0.089 (0.013)
0.019 (0.006)

0.23 (0.02)
0.24 (0.01)
0.08 (0.01)

0.10 (0.01)
0.46 (0.08)
0.06 (0.01)

0.34 (0.02)
0.41 (0.02)
0.32 (0.02)

Our framework focuses on using recurrence to build expressive kernels for regression on
sequences.

The suitability of each prediction mode depends on the task at hand. In many applications
where the future targets become readily available as the time passes (e.g., power estimation
or stock market prediction), the autoregression mode is preferable. We particularly consider
autoregressive prediction in the further experiments.

5.2.3 Prediction for smart grid and self-driving car applications

For both smart grid prediction tasks we used LSTM and GP-LSTM models with 48 hour
time lags and were predicting the target values one hour ahead. LSTM and GP-LSTM
were trained with one or two layers and 32 to 256 hidden units. The best models were
selected on 25% of the training data used for validation. For autonomous driving prediction
tasks, we used the same architectures but with 128 time steps of lag (1.28 s). All models
were regularized with dropout (Srivastava et al., 2014; Gal and Ghahramani, 2016b). On
both GEF and self-driving car datasets, we used the scalable version of Gaussian process
(MSGP) (Wilson et al., 2015). Given the scale of the data and the challenge of nonlinear
optimization of the recurrent models, we initialized the recurrent parts of GP-RNN and
GP-LSTM with pre-trained weights of the corresponding neural networks. Fine-tuning of the
models was performed with Algorithm 2. The quantitative results are provided in Table 4
and demonstrate that GPs with recurrent kernels attain the state-of-the-art performance.
Additionally, we investigated convergence and regularization properties of LSTM and
GP-LSTM models on the GEF-power dataset. The ﬁrst two charts of Figure 6 demonstrate
that GP-based models are less prone to overﬁtting, even when the data is not enough. The
third panel shows that architectures with a particular number of hidden units per layer
attain the best performance on the power prediction task. An additional advantage of
the GP-layers over the standard recurrent networks is that the best architecture could be
identiﬁed based on the negative log likelihood of the model as shown on the last chart.

17

Al-Shedivat, Wilson, Saatchi, Hu, Xing

(a) Point-wise predictions of the lanes made by LSTM (upper) and by GP-LSTM (lower). Dashed
lines correspond to the ground truth extracted from video sequences and used for training.

(b) LSTM (upper) and by GP-LSTM (lower) position predictions of the lead vehicle. Black markers
and dashed lines are the ground truth; blue and red markers with solid lines correspond to predictions.

Figure 5: Qualitative comparison of the LSTM and GP-LSTM predictions on self-driving tasks.
Predictive uncertainty of the GP-LSTM model is showed by contour plots and error-bars; the latter
denote one standard deviation of the predictive distributions.

Finally, Figure 5 qualitatively demonstrates the diﬀerence between the predictions given
by LSTM vs. GP-LSTM on point-wise lane estimation (Figure 5a) and the front vehicle
tracking (Figure 5b) tasks. We note that GP-LSTM not only provides a more robust ﬁt,
but also estimates the uncertainty of its predictions. Such information can be further used
in downstream prediction-based decision making, e.g., such as whether a self-driving car
should slow down and switch to a more cautious driving style when the uncertainty is high.

18

Learning Scalable Deep Kernels with Recurrent Structure

Figure 6: Left to right: RMSE vs. the number of training points; RMSE vs. the number model
parameters per layer; NLML vs. the number model parameters per layer for GP-based models. All
metrics are averages over 5 runs with diﬀerent random initializations, computed on a held-out set.

Figure 7: The charts demonstrate scalability of learning and inference of MSGP with an LSTM-based
recurrent kernel. Legends with points denote the number of inducing points used. Legends with
percentages denote the percentage of the training dataset used learning the model.

5.2.4 Scalability of the model

Following Wilson et al. (2015), we performed a generic scalability analysis of the MSGP-
LSTM model on the car sensors data. The LSTM architecture was the same as described
in the previous section: it was transforming multi-dimensional sequences of inputs to a
two-dimensional representation. We trained the model for 10 epochs on 10%, 20%, 40%,
and 80% of the training set with 100, 200, and 400 inducing points per dimension and
measured the average training time per epoch and the average prediction time per testing
point. The measured time was the total time spent on both LSTM optimization and MSGP
computations. The results are presented in Figure 7.

The training time per epoch (one full pass through the entire training data) grows linearly
with the number of training examples and depends linearly on the number of inducing points
(Figure 7, two left charts). Thus, given a ﬁxed number of inducing points per dimension, the
time complexity of MSGP-LSTM learning and inference procedures is linear in the number
of training examples. The prediction time per testing data point is virtually constant and
does not depend on neither on the number of training points, nor on the number of inducing
points (Figure 7, two right charts).

19

Al-Shedivat, Wilson, Saatchi, Hu, Xing

6. Discussion

We proposed a method for learning kernels with recurrent long short-term memory structure
on sequences. Gaussian processes with such kernels, termed the GP-LSTM, have the structure
and learning biases of LSTMs, while retaining a probabilistic Bayesian nonparametric
representation. The GP-LSTM outperforms a range of alternatives on several sequence-to-
reals regression tasks. The GP-LSTM also works on data with low and high signal-to-noise
ratios, and can be scaled to very large datasets, all with a straightforward, practical, and
generally applicable model speciﬁcation. Moreover, the semi-stochastic scheme proposed
in our paper is provably convergent and eﬃcient in practical settings, in conjunction with
structure exploiting algebra. In short, the GP-LSTM provides a natural mechanism for
Bayesian LSTMs, quantifying predictive uncertainty while harmonizing with the standard
deep learning toolbox. Predictive uncertainty is of high value in robotics applications, such
as autonomous driving, and could also be applied to other areas such as ﬁnancial modeling
and computational biology.

There are several exciting directions for future research. The GP-LSTM quantiﬁes
predictive uncertainty but does not model the propagation of uncertainty in the inputs
through a recurrent structure. Treating free simulation as a structured prediction problem
and using online corrective algorithms, e.g., DAGGER (Ross et al., 2011), are likely to
improve performance of GP-LSTM in the free prediction mode. This approach would not
require explicitly modeling and propagating uncertainty through the recurrence and would
maintain the high computational eﬃciency of our method.

Alternatively, it would be exciting to have a probabilistic treatment of all parameters of
the GP-LSTM kernel, including all LSTM weights. Such an extension could be combined with
stochastic variational inference, to enable both classiﬁcation and non-Gaussian likelihoods
as in Wilson et al. (2016b), but also open the doors to stochastic gradient Hamiltonian
Monte Carlo (Chen et al., 2014) (SG-HMC) for eﬃcient inference over kernel parameters.
Indeed, SG-HMC has recently been used for eﬃcient inference over network parameters in
the Bayesian GAN (Saatchi and Wilson, 2017). A Bayesian approach to marginalizing the
weights of the GP-LSTM kernel would also provide a principled probabilistic mechanism for
learning model hyperparameters.

One could relax several additional assumptions. We modeled each output dimension
with independent GPs that shared a recurrent transformation. To capture the correlations
between output dimensions, it would be promising to move to a multi-task formulation. In
the future, one could also learn the time horizon in the recurrent transformation, which
could lead to major additional performance gains.

Finally, the semi-stochastic learning procedure naturally complements research in asyn-
chronous optimization (e.g., Deisenroth and Ng, 2015). In combination with stochastic
variational inference, the semi-stochastic approach could be used for parallel kernel learning,
side-stepping the independence assumptions in prior work. We envision that such eﬀorts for
Gaussian processes will harmonize with current progress in Bayesian deep learning.

20

Learning Scalable Deep Kernels with Recurrent Structure

7. Acknowledgements

The authors thank Yifei Ma for helpful discussions and the anonymous reviewers for the
valuable comments that helped to improve the paper. This work was supported in part by
NIH R01GM114311, AFRL/DARPA FA87501220324, and NSF IIS-1563887.

21

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Appendix A. Massively scalable Gaussian processes

Massively scalable Gaussian processes (MSGP) (Wilson et al., 2015) is a signiﬁcant extension
of the kernel interpolation framework originally proposed by Wilson and Nickisch (2015). The
core idea of the framework is to improve scalability of the inducing point methods (Quinonero-
Candela and Rasmussen, 2005) by (1) placing the virtual points on a regular grid, (2)
exploiting the resulting Kronecker and Toeplitz structures of the relevant covariance matrices,
and (3) do local cubic interpolation to go back to the kernel evaluated at the original points.
This combination of techniques brings the complexity down to
(1)
for each test prediction. Below, we overview the methodology. We remark that a major
diﬀerence in philosophy between MSGP and many classical inducing point methods is that
the points are selected and ﬁxed rather than optimized over. This allows to use signiﬁcantly
more virtual points which typically results in a better approximation of the true kernel.

(n) for training and

O

O

A.1 Structured kernel interpolation

Given a set of m inducing points, the n
m cross-covariance matrix, KX,U , between the
training inputs, X, and the inducing points, U, can be approximated as ˜KX,U = WX KU,U
m matrix of interpolation weights, WX . This allows to
using a (potentially sparse) n
approximate KX,Z for an arbitrary set of inputs Z as KX,Z
Z . For any given
kernel function, K, and a set of inducing points, U, structured kernel interpolation (SKI)
procedure (Wilson and Nickisch, 2015) gives rise to the following approximate kernel:

˜KX,U W (cid:62)

≈

×

×

KSKI(x, z) = WX KU,U W (cid:62)
z ,

(A.1)

≈

WX KU,U W (cid:62)
X . Wilson and Nickisch (2015) note that
which allows to approximate KX,X
standard inducing point approaches, such as subset of regression (SoR) or fully independent
training conditional (FITC), can be reinterpreted from the SKI perspective. Importantly,
the eﬃciency of SKI-based MSGP methods comes from, ﬁrst, a clever choice of a set of
inducing points that allows to exploit algebraic structure of KU,U , and second, from using
very sparse local interpolation matrices. In practice, local cubic interpolation is used (Keys,
1981).

A.2 Kernel approximations

If inducing points, U , form a regularly spaced P -dimensional grid, and we use a stationary
product kernel (e.g., the RBF kernel), then KU,U decomposes as a Kronecker product of
Toeplitz matrices:

KU,U = T1

T2

TP .

⊗

⊗ · · · ⊗

(A.2)

The Kronecker structure allows to compute the eigendecomposition of KU,U by separately
decomposing T1, . . . , TP , each of which is much smaller than KU,U . Further, to eﬃciently

22

Learning Scalable Deep Kernels with Recurrent Structure

eigendecompose a Toeplitz matrix, it can be approximated by a circulant matrix13 which
eigendecomposes by simply applying discrete Fourier transform (DFT) to its ﬁrst column.
Therefore, an approximate eigendecomposition of each T1, . . . , TP is computed via the fast
Fourier transform (FFT) and requires only

(m log m) time.

O

A.3 Structure exploiting inference

To perform inference, we need to solve (KSKI + σ2I)−1y; kernel learning requires evaluating
log det(KSKI + σ2I). The ﬁrst task can be accomplished by using an iterative scheme—linear
conjugate gradients—which depends only on matrix vector multiplications with (KSKI + σ2I).
The second is done by exploiting the Kronecker and Toeplitz structure of KU,U for computing
an approximate eigendecomposition, as described above.

A.4 Fast Test Predictions

To achieve constant time prediction, we approximate the latent mean and variance of f∗ by
applying the same SKI technique. In particular, for a set of n∗ testing points, X∗, we have

E[f∗] = µX∗ + KX∗,X
µX∗ + ˜KX∗,X

(cid:2)KX,X + σ2I(cid:3)−1
(cid:105)−1
(cid:104) ˜KX,X + σ2I

y

y,

≈

(A.3)

where ˜KX,X = W KU,U W (cid:62) and ˜KX∗,X = W∗KU,U W (cid:62), and W and W∗ are n
m
sparse interpolation matrices, respectively. Since KU,U W (cid:62)[ ˜KX,X + σ2I]−1y is precomputed
at training time, at test time, we only multiply the latter with W∗ matrix which results which
(1) operations per test point. Similarly, approximate
(n∗) operations leading to
costs
(1) operations (Wilson et al., 2015).
predictive variance can be also estimated in

m and n∗

O

O

×

×

Note that the fast prediction methodology can be readily applied to any trained Gaussian

O

process model as it is agnostic to the way inference and learning were performed.

Appendix B. Gradients for GPs with recurrent kernels

GPs with deep recurrent kernels are trained by minimizing the negative log marginal
likelihood objective function. Below we derive the update rules.

By applying the chain rule, we get the following ﬁrst order derivatives:

∂
L
∂γ

=

∂
L
∂K ·

∂K
∂γ

,

∂
L
∂W

=

∂
L
∂K ·

∂K
∂φ ·

∂φ
∂W

.

(B.4)

The derivative of the log marginal likelihood w.r.t. to the kernel hyperparameters, θ, and
the parameters of the recurrent map, W , are generic and take the following form (Rasmussen

13. Wilson et al. (2015) explored 5 diﬀerent approximation methods known in the numerical analysis

literature.

23

Al-Shedivat, Wilson, Saatchi, Hu, Xing

and Williams, 2006, Ch. 5, Eq. 5.9):

∂
L
∂θ
∂
L
∂W

= 1

= 1

2 tr (cid:0)(cid:2)K−1yy(cid:62)K−1
2 tr (cid:0)(cid:2)K−1yy(cid:62)K−1

K−1(cid:3) ∂K
∂θ

(cid:1) ,

K−1(cid:3) ∂K
∂W

(cid:1) .

−

−

The derivative ∂K/∂θ is also standard and depends on the form of a particular chosen
kernel function, k(
/∂W is a bit subtle, and hence
,
·
we elaborate these derivations below.

). However, computing each part of ∂
·

L

Consider the ij-th entry of the kernel matrix, Kij. We can think of K as a matrix-valued
function of all the data vectors in d-dimensional transformed space which we denote by
RN ×d. Then Kij is a scalar-valued function of H and its derivative w.r.t. the l-th
H
parameter of the recurrent map, Wl, can be written as follows:

∈

∂Kij
∂Wl

= tr

(cid:32)(cid:18) ∂Kij
∂H

(cid:19)(cid:62) ∂H
∂Wl

(cid:33)

.

Notice that ∂Kij/∂H is a derivative of a scalar w.r.t. to a matrix and hence is a matrix;
∂H/∂Wl is a derivative of a matrix w.r.t. to a scalar which is taken element-wise and also
gives a matrix. Also notice that Kij is a function of H, but it only depends the i-th and
j-th elements for which the kernel is being computed. This means that ∂Kij/∂H will have
only non-zero i-th row and j-th column and allows us to re-write (B.7) as follows:

(B.5)

(B.6)

(B.7)

(B.8)

∂Kij
∂Wl

=

=

(cid:18) ∂Kij
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

+

(cid:18) ∂Kij
∂hj

(cid:18) ∂K(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

+

(cid:19)(cid:62) ∂hj
∂Wl
(cid:18) ∂K(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

.

Since the kernel function has two arguments, the derivatives must be taken with respect of
each of them and evaluated at the corresponding points in the hidden space, hi = φ(xi)
and hj = φ(xj). When we plug this into (B.6), we arrive at the following expression:

∂
L
∂Wl

=

1
2

(cid:88)

(cid:16)

i,j

K−1yy(cid:62)K−1

K−1(cid:17)

(cid:40)(cid:18) ∂K(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

ij

−

+

(cid:18) ∂K(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

(cid:41)

.

(B.9)
The same expression can be written in a more compact form using the Einstein notation:

∂
L
∂Wl

=

(cid:16)

1
2

K−1yy(cid:62)K−1

K−1(cid:17)j

(cid:32)(cid:20) ∂K
∂h

i

(cid:21)jd

i

+

(cid:20) ∂K
∂h

(cid:21)id

(cid:33) (cid:20) ∂h
∂W

(cid:21)dl

i

j

−

(B.10)

where d indexes the dimensions of the h and l indexes the dimensions of W .

24

Learning Scalable Deep Kernels with Recurrent Structure

In practice, deriving a computationally eﬃcient analytical form of ∂K/∂h might be too
complicated for some kernels (e.g., the spectral mixture kernels (Wilson and Adams, 2013)),
especially if the grid-based approximations of the kernel are enabled. In such cases, we can
simply use a ﬁnite diﬀerence approximation of this derivative. As we remark in the following
section, numerical errors that result from this approximation do not aﬀect convergence of
the algorithm.

Appendix C. Convergence results

Convergence results for the semi-stochastic alternating gradient schemes with and without
delayed kernel matrix updates are based on (Xu and Yin, 2015). There are a few notable
diﬀerences between the original setting and the one considered in this paper:

1. Xu and Yin (2015) consider a stochastic program that minimizes the expectation of

the objective w.r.t. some distribution underlying the data:

f (x) := EξF (x; ξ),

min
x∈X

(C.11)

where every iteration a new ξ is sampled from the underlying distribution. In our
case, the goal is to minimize the negative log marginal likelihood on a particular given
dataset. This is equivalent to the original formulation (C.11), but with the expectation
taken w.r.t. the empirical distribution that corresponds to the given dataset.

2. The optimization procedure of Xu and Yin (2015) has access to only a single random
point generated from the data distribution at each step. Our algorithm requires having
access to the entire training data each time the kernel matrix is computed.

3. For a given sample, Xu and Yin (2015) propose to loop over a number of coordinate
blocks and apply Gauss–Seidel type gradient updates to each block. Our semi-stochastic
scheme has only two parameter blocks, θ and W , where θ is updated deterministically
on the entire dataset while W is updated with stochastic gradient on samples from
the empirical distribution.

Noting these diﬀerences, we ﬁrst adapt convergence results for the smooth non-convex
case (Xu and Yin, 2015, Theorem 2.10) to our scenario, and then consider the variant with
delaying kernel matrix updates.

C.1 Semi-stochastic alternating gradient

As shown in Algorithm 1, we alternate between updating θ and W . At step t, we get a
¯xi
mini-batch of size Nt, xt
i∈It, which is just a selection of points from the full set, x.
Deﬁne the gradient errors for θ and W at step t as follows:

≡ {

}

θ := ˜gt
δt

gt
θ,

W := ˜gt
δt

gt
W ,

W −

θ −

(C.12)

25

Al-Shedivat, Wilson, Saatchi, Hu, Xing

where gT
and then W , and hence the expressions for the gradients take the following form:

W are the true gradients and ˜gT

W are estimates14. We ﬁrst update θ

θ and gT

θ and ˜gT

˜gt
θ ≡

gt
θ =

(θt, W t) =

(cid:88)

(cid:16)

1
2N

K−1

t yy(cid:62)K−1

t −

(cid:17)

K−1
t

(cid:19)

(cid:18) ∂Kt
∂θ

ij

1
N ∇

θ

L

1
N ∇

W

L

i,j
1
2N

(cid:88)

(cid:16)

i,j

gt
W =

˜gt
W =

(θt+1, W t) =

K−1

t+1yy(cid:62)K−1

1
2Nt

(cid:88)

(cid:16)

i,j∈It

K−1

t+1yy(cid:62)K−1

t+1 −

(cid:17)

K−1
t+1

(cid:18) ∂Kt+1
∂W

ij

t+1 −
(cid:19)

ij

ij
(cid:18) ∂Kt+1
∂W

(cid:19)

(cid:17)

K−1
t+1

ij

(C.13)

(C.14)
ij

(C.15)

Note that as we have shown in Appendix B, when the kernel matrix is ﬁxed, gW and ˜gW
factorize over x and xt, respectively. Further, we denote all the mini-batches sampled before
t as x[t−1].

Lemma 1 For any step t, E[δt

x[t−1]] = E[δt

x[t−1]] = 0.

W |

θ |

Proof First, δt
have E[δt

0, and hence E[δt

x[t−1]] = 0 is trivial. Next, by deﬁnition of δt

W |

θ ≡

θ |
x[t−1]]. Consider the following:

x[t−1]] = E[˜gt

gt
W −
W |
Consider E[gt
x[t−1]]: gt
being updated deterministically using gt−1
Therefore, it is independent of xt, which means that E[gt

W is a deterministic function of θt+1 and W t. θt+1 is
, and hence only depends on W t and θt.
x[t−1]]

W |

W , we

θ

gt
W .

W |

≡

Now, consider E[˜gt
distribution and Kt+1 does not depend on the current mini-batch, we can write:

x[t−1]]: Noting that the expectation is taken w.r.t. the empirical

W |

•

•

E[˜gt

W |

x[t−1]] = E

K−1

t+1yy(cid:62)K−1

K−1
t+1

t+1 −

(cid:19)





ij

(cid:17)

ij

(cid:18) ∂Kt+1
∂W
(cid:18) ∂Kt+1
∂W

(cid:19)

ij

(C.16)

K−1

t+1yy(cid:62)K−1

t+1 −

(cid:17)

K−1
t+1

ij





1
2Nt

(cid:88)

(cid:16)

i,j∈It
(cid:16)

(cid:88)

i,j

Nt
2N Nt

gt
W .

=

≡

Finally, E[δt

x[t−1]] = gt

W |

gt
W = 0.

W −

In other words, semi-stochastic gradient descent that alternates between updating θ and

W computes unbiased estimates of the gradients on each step.

14. Here, we consider the gradients and their estimates scaled by the number of full data points, N , and the
mini-batch size, Nt, respectively. These constant scaling is introduced for sake of having cleaner proofs.

26

Learning Scalable Deep Kernels with Recurrent Structure

Remark 1 Note that in case if (∂Kt+1/∂W ) is computed approximately, Lemma 1 still
holds since both gt

x[t−1]] will contain exactly the same numerical errors.

W and E[˜gt

W |

Assumption 1 For any step t, E
δt
θ(cid:107)
(cid:107)

2

t and E
δt
σ2
2
W (cid:107)
(cid:107)

≤

≤

σ2
t .

Lemma 1 and Assumption 1 result into a stronger condition than the original assumption
given by Xu and Yin (2015). This is due to the semi-stochastic nature of the algorithm, it
simpliﬁes the analysis, though it is not critical. Assumptions 2 and 3 are straightforwardly
adapted from the original paper.

Assumption 2 The objective function,
θ and W are uniformly Lipschitz with constant L > 0:

L

, is lower bounded and its partial derivatives w.r.t.

(θ, W )

θ
(cid:107)∇

L

θ

− ∇

L

(˜θ, W )

θ

L
(cid:107)

−

˜θ

,
(cid:107)

(cid:107) ≤

(θ, W )

W

(cid:107)∇

L

W

− ∇

L

(θ, ˜W )

W
L
(cid:107)

(cid:107) ≤

˜W
.
(cid:107)
−
(C.17)

Assumption 3 There exists a constant ρ such that

θt

2 + E
W t
(cid:107)

2
(cid:107)

(cid:107)

(cid:107)

≤

ρ2 for all t.

Lemma 2 Under Assumptions 2 and 3,

θt

2

(cid:107)

≤

2
gt
ρ2, E
W (cid:107)
(cid:107)

M 2

ρ , E
(cid:107)

2
gt
θ(cid:107)

≤

M 2
ρ

t,

∀

W t
E
(cid:107)

2
(cid:107)
ρ = 4L2ρ2 + 2 max

ρ2, E
(cid:107)

≤

where M 2

{∇
Proof The inequalities for E
θ
(cid:107)
E
(cid:107)

gt
W (cid:107)

L

(θ0, W 0),

θ

W

∇

2 and E
(cid:107)

(cid:107)

W

2 are merely corollaries from Assumption 2.

≤
(θ0, W 0)

.
}

L
2 are trivial, and the ones for E
gt
θ(cid:107)
(cid:107)

(cid:107)

2 and

Negative log marginal likelihood of a Gaussian process with a structured kernel is a
nonconvex function of its arguments. Therefore, we can only show that the algorithm
converges to a stationary point, i.e., a point at which the gradient of the objective is zero.

Theorem 1 Let
for θ and W being λt
scalars, such that

{

(θt, W t)
}
θαt and λt
θ = ct

be a sequence generated from Algorithm 1 with learning rates
W , αt are positive

W αt, respectively, where ct

W = ct

θ, ct

0 < inf
t

ct
{θ,W } ≤

sup
t

ct
{θ,W } <

∞

, αt

αt+1,

≥

αt = +

,
∞

∞
(cid:88)

t=1

∞
(cid:88)

t=1

α2

t < +

,
∞

Then, under Assumptions 1 through 3 and if σ = supt σt <

, we have

∞

t.

∀
(C.18)

(C.19)

E

lim
t→∞

(cid:107)∇L

(θt, W t)

= 0.

(cid:107)

27

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Proof The proof is an adaptation of the one given by Xu and Yin (2015) with the following
three adjustments: (1) we have only two blocks of coordinates and (2) updates for θ are
deterministic which zeros out their variance terms, (3) the stochastic gradients are unbiased.

From the Lipschitz continuity of

(Assumption 2), we have:

W

∇

L

and

θ

∇

L

•

(θt+1, W t+1)

(θt+1, W t)

L
W , W t+1
gt

≤ (cid:104)

=

=

=

λt
W (cid:104)
(cid:18)
λt
W −

−

−

−
W , ˜gt
gt
W (cid:105)
L
(λt
2
L
2
L(λt
L
2

λt
W −

W −
λt
W −
(cid:0)λt

λt
θ

(cid:18)

−

−

(cid:0)λt
(cid:18)

≤ −

+

L
2

W t+1

W t

2

(cid:107)

−
2

− L
W t

(cid:105)

+

+

L
2 (cid:107)
L
W )2
(λt
2
(cid:19)
W )2

gt
W (cid:107)
(cid:107)

(cid:19)

(cid:107)
2 +

˜gt
W (cid:107)
L
2
L
2

W

(λt
W )2
W )2(cid:1) (cid:0)
(cid:104)
(cid:19)
W )2
(λt

2 +

gt
W (cid:107)
(cid:107)
gt
W − ∇
2 +
gt
W (cid:107)
(cid:107)
W )2(cid:1) (

δt
W (cid:107)
(cid:107)

W + L(λt

(λt

W )2

2
δt
W (cid:107)
(cid:107)
(θt, W t), δt

L
L
(λt
2

W )2

W (cid:105)
2
δt
W (cid:107)

(cid:107)

−

2 +

2),

gt
θ(cid:107)
(cid:107)

(λt

W )2

2
δt
W (cid:107)

(cid:107)

−

(cid:0)λt

W −

L(λt

W )2(cid:1)

W , δt
gt
(cid:104)

W (cid:105)

W

L

+

(cid:104)∇
(cid:0)λt

W −

(cid:1)

W (cid:105)

(θt, W t), δt
W )2(cid:1)

L(λt

W

(cid:104)∇

L

(θt, W t), δt

W (cid:105)

where the last inequality comes from the following (note that gt

W :=

W

∇

L

(θt+1, W t)):

W

(cid:0)λt

L(λt

W −

(θt+1, W t)

−
(cid:12)
(cid:12)λt
W −
(cid:12)
(cid:12)λt
Lλt
W −
θ
L
(cid:0)λt
W + L(λt
2

W )2(cid:1)
L(λt
L
(cid:104)∇
W )2(cid:12)
δt
(θt+1, W t)
(cid:12)
W
W (cid:107)(cid:107)∇
L
(cid:107)
W )2(cid:12)
gt
δt
L(λt
(cid:12)
W (cid:107)(cid:107)
θ(cid:107)
(cid:107)
W )2(cid:1) (
2 +
δt
W (cid:107)
(cid:107)

gt
θ(cid:107)

λt
θ

(cid:107)

2).

≤

≤

≤

W

− ∇

(θt, W t), δt
W (cid:105)
(θt, W t)
(cid:107)

L

L
W

− ∇

Analogously, we derive a bound on

(θt+1, W t)

(θt, W t):

L

− L

L

(θt+1, W t)
(cid:18)
L
2

λt
θ −

− L
θ)2
(λt

(θt, W t)
(cid:19)

2 +

gt
θ(cid:107)

(cid:107)

L
2

≤ −

(λt

θ)2

2
δt
θ(cid:107)

(cid:107)

−

(cid:0)λt

θ −

L(λt

θ)2(cid:1)

θ
(cid:104)∇

L

(θt, W t), δt
θ(cid:105)

Since (θt, W t) is independent from the current mini-batch, xt, using Lemma 1, we
have that E

(θt, W t), δt

= 0 and E

= 0.

•

θ
(cid:104)∇

L

(θt, W t), δt
θ(cid:105)

W

(cid:104)∇

L

W (cid:105)

28

Learning Scalable Deep Kernels with Recurrent Structure

Summing the two obtained inequalities and applying Assumption 1, we get:

≤ −

≤ −

E[

−

−

L
(cid:18)

(cid:18)

(cid:18)

(cid:18)

W )2
(cid:19)

(λt

(λt

λt
θ −

(θt+1, W t+1)
L
λt
W −
2
L
2
L
2
L
2

θ)2

cαt

cαt

−

−

C2α2
t

C2α2
t

(cid:19)

(cid:19)

(θt, W t)]

− L
(cid:19)
E
(cid:107)

2 +

gt
W (cid:107)

L
2

E
(cid:107)

gt
θ(cid:107)

2 +

L
2

(λt

θ)2σ

gt
E
θ(cid:107)
(cid:107)

2 +

L
2

C2α2

t σ,

(λt

W )2σ +

λt
θ

(cid:0)λt

W + L(λt

W )2(cid:1) (σ + E
gt
θ(cid:107)
(cid:107)

2)

L
2

gt
E
W (cid:107)
(cid:107)

2 + LC2σαt +

C2α2

t (1 + LCαt) (σ + E
(cid:107)

gt
θ(cid:107)

2)

L
2

inf t ct
W , inf t ct
where we denoted c = min
θ}
{
from Lemma 2, we also have E
ρ and E
M 2
2
gt
θ(cid:107)
≤
(cid:107)
(cid:107)
the right hand side of the ﬁnal inequality over t and using (C.18), we have:

supt ct
{
M 2
2
gt
W (cid:107)

W , supt ct
. Note that
ρ . Therefore, summing

, C = max

θ}

≤

lim
t→∞

L

E

(θt+1, W t+1)

E

(θ0, W 0)

−

L

αt

(cid:0)E
(cid:107)

gt
θ(cid:107)

gt
2 + E
W (cid:107)
(cid:107)

2(cid:1) .

(C.20)

c
≤ −

∞
(cid:88)

t=1

Since the objective function is lower bounded, this eﬀectively means:

∞
(cid:88)

t=1

αtE
(cid:107)

gt
θ(cid:107)

2 <

,
∞

gt
αtE
W (cid:107)
(cid:107)

2 <

.
∞

∞
(cid:88)

t=1

Finally, using Lemma 2, our assumptions and Jensen’s inequality, it follows that

•

(cid:12)
gt+1
2
(cid:12)E
W (cid:107)
(cid:107)

gt
E
W (cid:107)
(cid:107)

−

2(cid:12)
(cid:12)

≤

(cid:113)

2LMρCαt

2(M 2

ρ + σ2).

According to Proposition 1.2.4 of (Bertsekas, 1999), we have E
gt
θ(cid:107)
(cid:107)
as t

, and hence

2

0 and E
(cid:107)

gt
W (cid:107)

2

→

→

0

→ ∞

E

(θt, W t)

(cid:107)∇L

(cid:107) ≤

E

(cid:107)∇
2LC

W
(cid:113)

L
2(M 2

(θt, W t)

gt
gt
+ E
θ(cid:107)
θ(cid:107)
−
(cid:107)
(cid:107)∇
L
gt
gt
+ E
ρ + σ2)α + E
W (cid:107) →
θ(cid:107)
(cid:107)
(cid:107)

+ E

W

≤

0 as t

,
→ ∞

(θt, W t)

gt
θ(cid:107)

+ E
(cid:107)

gt
W (cid:107)

−

where the ﬁrst term of the last inequality follows from Lemma 2 and Jensen’s inequality.

29

Al-Shedivat, Wilson, Saatchi, Hu, Xing

C.2 Semi-stochastic gradient with delayed kernel matrix updates

We show that given a bounded delay on the kernel matrix updates, the algorithm is still
convergent. Our analysis is based on computing the change in δt
W and applying the
same argument as in Theorem 1. The only diﬀerence is that we need to take into account
the perturbations of the kernel matrix due to the introduced delays, and hence we have to
impose certain assumptions on its spectrum.

θ and δt

Assumption 4 Recurrent transformations, φW (¯x), is L-Lipschitz w.r.t. W for all ¯x

L:

∈ X

φ ˜W (¯x)

(cid:107)

−

φW (¯x)

(cid:107) ≤

˜W

L
(cid:107)

W

.

(cid:107)

−

Assumption 5 The kernel function, k(
derivatives are uniformly J-Lipschitz:

,
·

·

), is uniformly G-Lipschitz and its ﬁrst partial

(cid:13)
(cid:13)k(˜h1, h2)
(cid:13)

k(h1, h2)

−
(cid:13)
∂1k(˜h1, h2)
(cid:13)
(cid:13) ≤

−

(cid:13)
(cid:13)
(cid:13) ≤
J

(cid:107)

G

˜h1
(cid:107)

˜h1

−

,

(cid:13)
(cid:13)k(h1, ˜h2)
(cid:13)
h1
(cid:107)
(cid:13)
(cid:13)
(cid:13)∂2k(h1, h2)

−

k(h1, h2)

(cid:13)
(cid:13)
(cid:13) ≤
−
(cid:13)
∂2k(h1, ˜h2)
(cid:13)
(cid:13) ≤

G

J

˜h2
(cid:107)
˜h2
(cid:107)

−
h1

,

(cid:107)

h2

h2

,

.

(cid:107)

(cid:107)

−

−

(cid:13)
(cid:13)
(cid:13)∂1k(h1, h2)

Assumption 6 For any collection of data representations,
of the corresponding kernel matrix, K, is lower bounded by a positive constant γ > 0.

N
i=1, the smallest eigenvalue
}

hi

{

Note that not only the assumptions are relevant to practice, Assumptions 5 and 6 can be
also controlled by choosing the class of kernel functions used in the model. For example, the
smallest eigenvalue of the kernel matrix, K, can be controlled by the smoothing properties
of the kernel (Rasmussen and Williams, 2006).

Consider a particular stochastic step of Algorithm 2 at time t for a given mini-batch, xt,
assuming that the kernel was last updated τ steps ago. The stochastic gradient will take
the following form:

ˆgt
W =

1
Nt ∇

W

L

(θt+1, W t, Kt−τ ) =

(cid:88)

(cid:16)

1
2Nt

K−1

t−τ yy(cid:62)K−1

K−1
t−τ

t−τ −

(cid:17)

ij

(cid:18) ∂Kt−τ
∂W

(cid:19)

.

ij
(C.21)
We can deﬁne ˆδt
in order to enable the same
W −
argument as in Theorem 1. To do that, we simply need to understand eﬀect of perturbation
of the kernel matrix on ˜gt

gt
W and uniformly bound

ˆδt
W −

W = ˆgt

δt
W (cid:107)

i,j∈It

(cid:107)

W .

Lemma 3 Under the given assumptions, the following bound holds for all i, j = 1, . . . , N :

(cid:12)
(cid:16)
(cid:12)
(cid:12)
(cid:12)

K−1

t−τ yy(cid:62)K−1

t−τ −

(cid:17)

K−1
t−τ

(cid:16)

K−1

t yy(cid:62)K−1

K−1
t

t −

ij −

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤

ij

D2

y

(cid:107)

(cid:107)

2 + D,

(C.22)

where D = γ−1 +

2GLτ λσ√N

(cid:17)−1

.

(cid:16)

γ

−

30

Learning Scalable Deep Kernels with Recurrent Structure

Proof The diﬀerence between K−1
t
computed for W t−τ and the latter for W t. To prove the bound, we need multiple steps.

is simply due to that the former has been

t−τ and K−1

•

First, we need to bound the element-wise diﬀerence between Kt−τ and Kt. This is
done by using Assumptions 4 and 5 and the triangular inequality:

(Kt)ij
|

−

(Kt−τ )ij

φW t−τ (¯xi)

+

φW t(¯xj)

(cid:107)

(cid:107)

φW t−τ (¯xj)
(cid:107)

)

−

| ≤

≤

G (

φW t(¯xi)
(cid:107)
W t
2GL
τ
(cid:88)

(cid:107)

−
W t−τ

−
λt−τ +s
W

(cid:107)
ˆgt−τ +s
W

(cid:107)

= 2GL

(cid:107)

s=1
2GLτ λσ

≤

•

Next, since each element of the perturbed matrix is bounded by 2GLτ λσ, we can
bound its spectral norm as follows:

Kt

(cid:107)

−

Kt−τ

Kt

Kt−τ

(cid:107) ≤ (cid:107)

−

F
(cid:107)

≤

2GLτ λσ√N ,

which means that the minimal singular value of the perturbed matrix is at least
σ1 = γ

2GLτ λσ√N due to Assumption 6.

−

•

The spectral norm of the expression of interest can be bounded (quite pessimistically!)
by summing up together the largest eigenvalues of the matrix inverses:

t−τ yy(cid:62)K−1

K−1
t−τ

t−τ −
(cid:1) yy(cid:62) (cid:0)K−1

K−1
t

(cid:16)

K−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:0)K−1
(cid:13)
(cid:13)
(cid:18)

t−τ −
(cid:16)

t−τ −

K−1
t
(cid:17)−1(cid:19)2

γ−1 +

γ

2GLτ λσ√N

−

y

(cid:107)

(cid:107)

≤

≤

(cid:17)

(cid:16)

ij −

K−1

t yy(cid:62)K−1

t −

(cid:17)

K−1
t

(cid:1)

(cid:0)K−1

t−τ −

−

K−1
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ij
(cid:1)(cid:13)
(cid:13)
(cid:13)

−

2 + γ−1 +

(cid:16)

γ

2GLτ λσ√N

(cid:17)−1

.

Each element of a matrix is bounded by the largest eigenvalue.

Using Lemma 3, it is straightforward to extend Theorem 1 to Algorithm 2.

Theorem 2 Let
for θ and W being λt
scalars, such that

{

(θt, W t)
}
θαt and λt
θ = ct

be a sequence generated from Algorithm 2 with learning rates
W , αt are positive

W αt, respectively, where ct

W = ct

θ, ct

0 < inf
t

ct
{θ,W } ≤

sup
t

ct
{θ,W } <

∞

, αt

αt+1,

≥

αt = +

,
∞

∞
(cid:88)

t=1

∞
(cid:88)

t=1

α2

t < +

,
∞

Then, under Assumptions 1 through 6 and if σ = supt σt <

, we have

t.

∀
(C.23)

(C.24)

E

lim
t→∞

(cid:107)∇L

(θt, W t)

= 0.

(cid:107)

∞

31

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Proof The proof is identical to the proof of Theorem 1. The only diﬀerence is in the
following upper bound on the expected gradient error: E
2 + 1)Jρ,
(cid:107)
(cid:107)
where D is as given in Lemma 3.

δt
W (cid:107) ≤

σ + 2N D(D

(cid:107)

y

Remark 1 Even though the provided bounds are crude due to pessimistic estimates of the
perturbed kernel matrix spectrum15, we still see a ﬁne balance between the delay, τ , and the
learning rate, λ, as given in the expression for the D constant.

Appendix D. Details on the datasets

The datasets varied in the number of time steps (from hundreds to a million), input and
output dimensionality, and the nature of the estimation tasks.

D.1 Self-driving car

The following is description of the input and target time series used in each of the autonomous
driving tasks (dimensionality is given in parenthesis).

Car speed estimation:

– Features: GPS velocity (3), ﬁber gyroscope (3).

– Targets: speed measurements from the car speedometer.

Car yaw estimation:

– Features: acceleration (3), compass measurements (3).

– Targets: yaw in the car-centric frame.

•

•

•

Lane sequence prediction: Each lane was represented by 8 cubic polynomial
coeﬃcients [4 coeﬃcients for x (front) and 4 coeﬃcients for y (left) axes in the car-
centric frame]. Instead of predicting the coeﬃcients (which turned out to lead to
overall less stable results), we discretized the lane curves using 7 points (initial, ﬁnal
and 5 equidistant intermediate points).

– Features: lanes at a previous time point (16), GPS velocity (3), ﬁber gyroscope

(3), compass (3), steering angle (1).

– Targets: coordinates of the lane discretization points (7 points per lane resulting

in 28 total output dimensions).

15. Tighter bounds can be derived by inspecting the eﬀects of perturbations on speciﬁc kernels as well as

using more speciﬁc assumptions about the data distribution.

32

Learning Scalable Deep Kernels with Recurrent Structure

Table 5: Summary of the feedforward and recurrent neural architectures and the corresponding
hyperparameters used in the experiments. GP-based models used the same architectures as their
non-GP counterparts. Activations are given for the hidden units; vanilla neural nets used linear
output activations.

Name Data

Time lag Layers Units∗ Type Regularizer∗∗

Optimizer

NARX

RNN

LSTM

Actuator
Drives
GEF-power
GEF-wind
Car

Actuator
Drives
GEF-power
GEF-wind
Car

Actuator
Drives
GEF-power
GEF-wind
Car

32
16
48
48
128

32
16
48
48
128

32
16
48
48
128

1
1
1
1
1

1
1
1
1
1

1
1
2
1
2

256
128
256
16
128

64
64
16
32
128

256
128
256
64
64

ReLU

dropout(0.5)

Adam(0.01)

tanh

dropout(0.25),
rec_dropout(0.05)

Adam(0.01)

tanh

dropout(0.25),
rec_dropout(0.05)

Adam(0.01)

∗Each layer consisted of the same number of units given in the table.
∗∗rec_dropout denotes the dropout rate of the recurrent weights (Gal and Ghahramani, 2016b).

•

Estimation of the nearest front vehicle position: (x, y) coordinates in the
car-centric frame.

– Features: x and y at a previous time point (2), GPS velocity (3), ﬁber gyroscope

(3), compass (3), steering angle (1).

– Targets: x and y coordinates.

Appendix E. Neural architectures

Details on the best neural architectures used for each of the datasets are given in Table 5.

33

Al-Shedivat, Wilson, Saatchi, Hu, Xing

References

Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances

in Neural Information Processing Systems, pages 873–881, 2011.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with
gradient descent is diﬃcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.

Dimitri P Bertsekas. Nonlinear programming. Athena scientiﬁc Belmont, 1999.

George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series

analysis: forecasting and control. John Wiley & Sons, 1994.

Roberto Calandra, Jan Peters, Carl Edward Rasmussen, and Marc Peter Deisenroth. Mani-
fold gaussian processes for regression. In Neural Networks (IJCNN), 2016 International
Joint Conference on, pages 3338–3345. IEEE, 2016.

Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo.

In International Conference on Machine Learning, pages 1683–1691, 2014.

Andreas C Damianou and Neil D Lawrence. Deep gaussian processes. In AISTATS, pages

207–215, 2013.

Marc Peter Deisenroth and Jun Wei Ng. Distributed gaussian processes. In International

Conference on Machine Learning (ICML), volume 2, page 5, 2015.

Roger Frigola, Yutian Chen, and Carl Rasmussen. Variational gaussian process state-space
models. In Advances in Neural Information Processing Systems, pages 3680–3688, 2014.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing
model uncertainty in deep learning. In Proceedings of the 33rd International Conference
on Machine Learning, pages 1050–1059, 2016a.

Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in
recurrent neural networks. In Advances in Neural Information Processing Systems, pages
1019–1027, 2016b.

Alan Graves, Abdel-rahman Mohamed, and Geoﬀrey Hinton. Speech recognition with deep
recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013
IEEE International Conference on, pages 6645–6649. IEEE, 2013.

Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability
of stochastic gradient descent. In Proceedings of The 33rd International Conference on
Machine Learning, pages 1225–1234, 2016.

34

Learning Scalable Deep Kernels with Recurrent Structure

James Hensman, Nicolò Fusi, and Neil D Lawrence. Gaussian processes for big data. In
Proceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence,
pages 282–290. AUAI Press, 2013.

Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Using deep belief nets to learn covariance
kernels for gaussian processes. In Advances in neural information processing systems,
pages 1249–1256, 2008.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9

(8):1735–1780, 1997.

Tommi S Jaakkola and David Haussler. Exploiting generative models in discriminative
classiﬁers. Advances in neural information processing systems, pages 487–493, 1999.

Robert Keys. Cubic convolution interpolation for digital image processing. IEEE transactions

on acoustics, speech, and signal processing, 29(6):1153–1160, 1981.

Juš Kocijan, Agathe Girard, Blaž Banko, and Roderick Murray-Smith. Dynamic systems
identiﬁcation with gaussian processes. Mathematical and Computer Modelling of Dynamical
Systems, 11(4):411–424, 2005.

John Langford, Alex J Smola, and Martin Zinkevich. Slow learners are fast. Advances in

Neural Information Processing Systems, 22:2331–2339, 2009.

Miguel Lázaro-Gredilla. Bayesian warped gaussian processes.

In Advances in Neural

Information Processing Systems, pages 1619–1627, 2012.

Tsungnam Lin, Bil G Horne, Peter Tiňo, and C Lee Giles. Learning long-term dependencies in
narx recurrent neural networks. Neural Networks, IEEE Transactions on, 7(6):1329–1338,
1996.

David JC MacKay. Introduction to gaussian processes. NATO ASI Series F Computer and

Systems Sciences, 168:133–166, 1998.

César Lincoln C Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth, Guilherme A Bar-
reto, and Neil D Lawrence. Recurrent gaussian processes. arXiv preprint arXiv:1511.06644,
2015.

Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. The Journal of

Machine Learning Research, 7:2651–2667, 2006.

Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse
approximate gaussian process regression. The Journal of Machine Learning Research, 6:
1939–1959, 2005.

35

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Carl Edward Rasmussen and Zoubin Ghahramani. Occam’s razor. Advances in neural

information processing systems, pages 294–300, 2001.

Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine

learning. The MIT Press, 2006.

Stéphane Ross, Geoﬀrey J Gordon, and Drew Bagnell. A reduction of imitation learning
and structured prediction to no-regret online learning. In AISTATS, volume 1, page 6,
2011.

Yunus Saatchi and Andrew Gordon Wilson. Bayesian gan. arXiv preprint arXiv:1705.09558,

2017.

Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines,

regularization, optimization, and beyond. MIT press, 2002.

Jonas Sjöberg, Qinghua Zhang, Lennart Ljung, Albert Benveniste, Bernard Delyon, Pierre-
Yves Glorennec, Håkan Hjalmarsson, and Anatoli Juditsky. Nonlinear black-box modeling
in system identiﬁcation: a uniﬁed overview. Automatica, 31(12):1691–1724, 1995.

Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped gaussian

processes. In NIPS, pages 337–344, 2003.

Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958, 2014.

ML Stein. Interpolation of spatial data, 1999.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural
networks. In Advances in neural information processing systems, pages 3104–3112, 2014.

Ryan D Turner, Marc P Deisenroth, and Carl E Rasmussen. State-space inference and
learning with gaussian processes. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 868–875, 2010.

Peter Van Overschee and Bart De Moor. Subspace identiﬁcation for linear systems: Theory

— Implementation — Applications. Springer Science & Business Media, 2012.

Jack Wang, Aaron Hertzmann, and David M Blei. Gaussian process dynamical models. In

Advances in neural information processing systems, pages 1441–1448, 2005.

Torbjörn Wigren. Input-output data sets for development and benchmarking in nonlinear
identiﬁcation. Technical Reports from the department of Information Technology, 20:
2010–020, 2010.

36

Learning Scalable Deep Kernels with Recurrent Structure

Andrew Wilson and Zoubin Ghahramani. Copula processes. In Advances in Neural Infor-

mation Processing Systems, pages 2460–2468, 2010.

Andrew Gordon Wilson and Ryan Prescott Adams. Gaussian process kernels for pattern
discovery and extrapolation. In Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pages 1067–1075, 2013.

Andrew Gordon Wilson and Hannes Nickisch. Kernel interpolation for scalable structured
gaussian processes (kiss-gp). In Proceedings of The 32nd International Conference on
Machine Learning, pages 1775–1784, 2015.

Andrew Gordon Wilson, Christoph Dann, and Hannes Nickisch. Thoughts on massively

scalable gaussian processes. arXiv preprint arXiv:1511.01870, 2015.

Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel
learning. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence
and Statistics, 2016a.

Andrew Gordon Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic
variational deep kernel learning. In Advances in Neural Information Processing Systems,
pages 2586–2594, 2016b.

D Randall Wilson and Tony R Martinez. The general ineﬃciency of batch training for

gradient descent learning. Neural Networks, 16(10):1429–1451, 2003.

Yangyang Xu and Wotao Yin. Block stochastic gradient iteration for convex and nonconvex

optimization. SIAM Journal on Optimization, 25(3):1686–1716, 2015.

37

Journal of Machine Learning Research 18 (2017) 1-37

Submitted 10/16; Revised 6/17; Published 8/17

Learning Scalable Deep Kernels with Recurrent Structure

Maruan Al-Shedivat
Carnegie Mellon University

Andrew Gordon Wilson
Cornell University

Yunus Saatchi

Zhiting Hu
Carnegie Mellon University

Eric P. Xing
Carnegie Mellon University

Editor: Neil Lawrence

alshedivat@cs.cmu.edu

andrew@cornell.edu

saatchi@cantab.net

zhitingh@cs.cmu.edu

epxing@cs.cmu.edu

Abstract
Many applications in speech, robotics, ﬁnance, and biology deal with sequential data, where
ordering matters and recurrent structures are common. However, this structure cannot
be easily captured by standard kernel functions. To model such structure, we propose
expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-
LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent
networks, while retaining the non-parametric probabilistic advantages of Gaussian processes.
We learn the properties of the proposed kernels by optimizing the Gaussian process marginal
likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit
the structure of these kernels for scalable training and prediction. This approach provides a
practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance
on several benchmarks, and thoroughly investigate a consequential autonomous driving
application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.

1. Introduction

There exists a vast array of machine learning applications where the underlying datasets
are sequential. Applications range from the entirety of robotics, to speech, audio and video
processing. While neural network based approaches have dealt with the issue of representation
learning for sequential data, the important question of modeling and propagating uncertainty
across time has rarely been addressed by these models. For a robotics application such as a
self-driving car, however, it is not just desirable, but essential to have complete predictive
densities for variables of interest. When trying to stay in lane and keep a safe following
distance from the vehicle front, knowing the uncertainty associated with lanes and lead
vehicles is as important as the point estimates.

c(cid:13)2017 Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, Eric P. Xing.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/16-498.html.

7
1
0
2
 
t
c
O
 
5
 
 
]

G
L
.
s
c
[
 
 
3
v
6
3
9
8
0
.
0
1
6
1
:
v
i
X
r
a

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Recurrent models with long short-term memory (LSTM) (Hochreiter and Schmidhuber,
1997) have recently emerged as the leading approach to modeling sequential structure. The
LSTM is an eﬃcient gradient-based method for training recurrent networks. LSTMs use
a memory cell inside each hidden unit and a special gating mechanism that stabilizes the
ﬂow of the back-propagated errors, improving the learning process of the model. While
the LSTM provides state-of-the-art results on speech and text data (Graves et al., 2013;
Sutskever et al., 2014), quantifying uncertainty or extracting full predictive distributions
from deep models is still an area of active research (Gal and Ghahramani, 2016a).

In this paper, we quantify the predictive uncertainty of deep models by following a
Bayesian nonparametric approach. In particular, we propose kernel functions which fully
encapsulate the structural properties of LSTMs, for use with Gaussian processes. The
resulting model enables Gaussian processes to achieve state-of-the-art performance on se-
quential regression tasks, while also allowing for a principled representation of uncertainty
and non-parametric ﬂexibility. Further, we develop a provably convergent semi-stochastic op-
timization algorithm that allows mini-batch updates of the recurrent kernels. We empirically
demonstrate that this semi-stochastic approach signiﬁcantly improves upon the standard
non-stochastic ﬁrst-order methods in runtime and in the quality of the converged solution.
For additional scalability, we exploit the algebraic structure of these kernels, decomposing
the relevant covariance matrices into Kronecker products of circulant matrices, for
(n)
training time and
(1) test predictions (Wilson et al., 2015; Wilson and Nickisch, 2015).
Our model not only can be interpreted as a Gaussian process with a recurrent kernel, but
also as a deep recurrent network with probabilistic outputs, inﬁnitely many hidden units,
and a utility function robust to overﬁtting.

O

O

Throughout this paper, we assume basic familiarity with Gaussian processes (GPs).
We provide a brief introduction to GPs in the background section; for a comprehensive
reference, see, e.g., Rasmussen and Williams (2006). In the following sections, we formalize
the problem of learning from sequential data, provide background on recurrent networks
and the LSTM, and present an extensive empirical evaluation of our model. Speciﬁcally, we
apply our model to a number of tasks, including system identiﬁcation, energy forecasting,
and self-driving car applications. Quantitatively, the model is assessed on the data ranging
in size from hundreds of points to almost a million with various signal-to-noise ratios
demonstrating state-of-the-art performance and linear scaling of our approach. Qualitatively,
the model is tested on consequential self-driving applications:
lane estimation and lead
vehicle position prediction. Indeed, the main focus of this paper is on achieving state-
of-the-art performance on consequential applications involving sequential data, following
straightforward and scalable approaches to building highly ﬂexible Gaussian process.

We release our code as a library at: http://github.com/alshedivat/keras-gp. This
library implements the ideas in this paper as well as deep kernel learning (Wilson et al.,
2016a) via a Gaussian process layer that can be added to arbitrary deep architectures
and deep learning frameworks, following the Keras API speciﬁcation. More tutorials and
resources can be found at https://people.orie.cornell.edu/andrew/code.

2

Learning Scalable Deep Kernels with Recurrent Structure

2. Background

L

∈

X

X

· · ·

yi
{

xi
{

i , x2
i ,

i ∈ X

n
i=1, yi
}

i=1 be a collection of sequences, xi = [x1
n
}

We consider the problem of learning a regression function that maps sequences to real-valued
, xli],
target vectors. Formally, let X =
each with corresponding length, li, where xj
is an arbitrary domain. Let
, and
Rd, be a collection of the corresponding real-valued target vectors.
y =
Assuming that only the most recent L steps of a sequence are predictive of the targets, the
Rd, from some family,
, based on the available data.
goal is to learn a function, f :
As a working example, consider the problem of estimating position of the lead vehicle at
the next time step from LIDAR, GPS, and gyroscopic measurements of a self-driving car
available for a number of previous steps. This task is a classical instance of the sequence-
to-reals regression, where a temporal sequence of measurements is regressed to the future
position estimates. In our notation, the sequences of inputs are vectors of measurements,
xn], are indexed by time and would be of
x1 = [x1], x2 = [x1, x2], . . . , xn = [x1, x2,
· · ·
growing lengths. Typically, input sequences are considered up to a ﬁnite-time horizon, L,
that is assumed to be predictive for the future targets of interest. The targets, y1, y2, . . . , yn,
are two-dimensional vectors that encode positions of the lead vehicle in the ego-centric
coordinate system of the self-driving car.

(cid:55)→

F

Note that the problem of learning a mapping, f :

Rd, is challenging. While
considering whole sequences of observations as input features is necessary for capturing
long-term temporal correlations, it virtually blows up the dimensionality of the problem. If
Rp, and consider L previous
we assume that each measurement is p-dimensional, i.e.,
X ⊆
steps as distinct features, the regression problem will become (L
p)-dimensional. Therefore,
to avoid overﬁtting and be able to extract meaningful signal from a ﬁnite amount of data, it
is crucial to exploit the sequential nature of observations.

(cid:55)→

×

X

L

Recurrent models. One of the most successful ways to exploit sequential structure of
the data is by using a class of recurrent models. In the sequence-to-reals regression scenario,
Rd in the following general recurrent form:
such a model expresses the mapping f :

L

(cid:55)→
y = ψ(hL) + (cid:15)t, ht = φ(ht−1, xt) + δt, t = 1, . . . , L,

X

(1)

where xt is an input observation at time t, ht is a corresponding latent representation,
) specify model transitions and emissions,
) and ψ(
and y is a target vector. Functions φ(
·
·
respectively, and δt and (cid:15)t are additive noises. While φ(
) can be arbitrary, they
) and ψ(
·
·
are typically time-invariant. This strong but realistic assumption incorporated into the
structure of the recurrent mapping signiﬁcantly reduces the complexity of the family of
functions,

, regularizes the problem, and helps to avoid severe overﬁtting.

Recurrent models can account for various patterns in sequences by memorizing internal
representations of their dynamics via adjusting φ and ψ. Recurrent neural networks (RNNs)
model recurrent processes by using linear parametric maps followed by nonlinear activations:

F

y = ψ(W (cid:62)

hyht−1), ht = φ(W (cid:62)

hhht−1, W (cid:62)

xhxt−1), t = 1, . . . , L,

(2)

3

Al-Shedivat, Wilson, Saatchi, Hu, Xing

where Why, Whh, Wxh are weight matrices to be learned1 and φ(
) here are some
·
·
ﬁxed element-wise functions. Importantly and contrary to the standard hidden Markov
models (HMMs), the state of an RNN at any time t is distributed and eﬀectively represented
by an entire hidden sequence, [h1,
, ht−1, ht]. A major disadvantage of the vanilla RNNs
is that their training is nontrivial due to the so-called vanishing gradient problem (Bengio
et al., 1994): the error back-propagated through t time steps diminishes exponentially which
makes learning long-term relationships nearly impossible.

) and ψ(

· · ·

LSTM. To overcome vanishing gradients, Hochreiter and Schmidhuber (1997) proposed
a long short-term memory (LSTM) mechanism that places a memory cell into each hidden
unit and uses diﬀerentiable gating variables. The update rules for the hidden representation
at time t have the following form (here σ (
) are element-wise sigmoid and
·
hyperbolic tangent functions, respectively):

) and tanh (
·

(cid:16)

it = tanh

W (cid:62)

xcxt + W (cid:62)

hcht−1 + bc

(cid:17)

,

hiht−1 + W (cid:62)

ci ct−1 + bi

(cid:16)

W (cid:62)

gt
i = σ
ct = gt
c ct−1 + gt
(cid:16)
W (cid:62)

xixt + W (cid:62)
i it,
xf xt + W (cid:62)

(cid:17)

,

(cid:17)

(3)

,

xoxt + W (cid:62)

cf ct−1 + bf

hf ht−1 + W (cid:62)

gt
c = σ
ot = tanh (cid:0)ct(cid:1) ,
(cid:16)
gt
o = σ
ht = gt
As illustrated above, gt
o correspond to the input, forget, and output gates,
respectively. These variables take their values in [0, 1] and when combined with the internal
states, ct, and inputs, xt, in a multiplicative fashion, they play the role of soft gating. The
gating mechanism not only improves the ﬂow of errors through time, but also, allows the
the network to decide whether to keep, erase, or overwrite certain memorized information
based on the forward ﬂow of inputs and the backward ﬂow of errors. This mechanism adds
stability to the network’s memory.

W (cid:62)
o ot.

hoht−1 + W (cid:62)

c, and gt

coct + bo

i, gt

(cid:17)

,

Gaussian processes. The Gaussian process (GP) is a Bayesian nonparametric model
that generalizes the Gaussian distributions to functions. We say that a random function
f is drawn from a GP with a mean function µ and a covariance kernel k, f
(µ, k),
if for any vector of inputs, [x1, x2, . . . , xn], the corresponding vector of function values is
Gaussian:

∼ GP

[f (x1), f (x2), . . . , f (xn)]

(µ, KX,X ) ,

∼ N

with mean µ, such that µi = µ(xi), and covariance matrix KX,X that satisﬁes (KX,X )ij =
k(xi, xj). GPs can be seen as distributions over the reproducing kernel Hilbert space (RKHS)
of functions which is uniquely deﬁned by the kernel function, k (Schölkopf and Smola, 2002).

1. The bias terms are omitted for clarity of presentation.

4

Learning Scalable Deep Kernels with Recurrent Structure

GPs with RBF kernels are known to be universal approximators with prior support to within
an arbitrarily small epsilon band of any continuous function (Micchelli et al., 2006).
x

(cid:0)f (x), σ2(cid:1), and a GP prior on f (x), given
∼ N
training inputs x and training targets y, the predictive distribution of the GP evaluated at
an arbitrary test point x∗ is:

Assuming additive Gaussian noise, y

|

where

f∗

|

x∗, x, y, σ2

(E[f∗], Cov[f∗]) ,

∼ N

E[f∗] = µX∗ + KX∗,X [KX,X + σ2I]−1y,

Cov[f∗] = KX∗,X∗ −

KX∗,X [KX,X + σ2I]−1KX,X∗.

(4)

(5)

Here, KX∗,X , KX,X∗, KX,X , and KX∗,X∗ are matrices that consist of the covariance function,
k, evaluated at the corresponding points, x
X∗, and µX∗ is the mean function
∈
X∗. GPs are ﬁt to the data by optimizing the evidence—the marginal
evaluated at x∗
probability of the data given the model—with respect to kernel hyperparameters. The
evidence has the form:

X and x∗

∈

∈

log P (y

x) =

|

−

(cid:105)
(cid:104)
y(cid:62)(K + σ2I)−1y + log det(K + σ2I)

+ const,

(6)

where we use a shorthand K for KX,X , and K implicitly depends on the kernel hyperpa-
rameters. This objective function consists of a model ﬁt and a complexity penalty term that
results in an automatic Occam’s razor for realizable functions (Rasmussen and Ghahramani,
2001). By optimizing the evidence with respect to the kernel hyperparameters, we eﬀectively
learn the the structure of the space of functional relationships between the inputs and the
targets. For further details on Gaussian processes and relevant literature we refer interested
readers to the classical book by Rasmussen and Williams (2006).

Turning back to the problem of learning from sequential data, it seems natural to apply
the powerful GP machinery to modeling complicated relationships. However, GPs are
limited to learning only pairwise correlations between the inputs and are unable to account
for long-term dependencies, often dismissing complex temporal structures. Combining GPs
with recurrent models has potential to addresses this issue.

3. Related work

The problem of learning from sequential data, especially from temporal sequences, is well
known in the control and dynamical systems literature. Stochastic temporal processes
are usually described either with generative autoregressive models (AM) or with state-
space models (SSM) (Van Overschee and De Moor, 2012). The former approach includes
nonlinear auto-regressive models with exogenous inputs (NARX) that are constructed by
using, e.g., neural networks (Lin et al., 1996) or Gaussian processes (Kocijan et al., 2005).
The latter approach additionally introduces unobservable variables, the state, and constructs

5

Al-Shedivat, Wilson, Saatchi, Hu, Xing

autoregressive dynamics in the latent space. This construction allows to represent and
propagate uncertainty through time by explicitly modeling the signal (via the state evolution)
and the noise. Generative SSMs can be also used in conjunction with discriminative models
via the Fisher kernel (Jaakkola and Haussler, 1999).

Modeling time series with GPs is equivalent to using linear-Gaussian autoregressive or
SSM models (Box et al., 1994). Learning and inference are eﬃcient in such models, but they
are not designed to capture long-term dependencies or correlations beyond pairwise. Wang
et al. (2005) introduced GP-based state-space models (GP-SSM) that use GPs for transition
and/or observation functions. These models appear to be more general and ﬂexible as they
account for uncertainty in the state dynamics, though require complicated approximate
training and inference, which are hard to scale (Turner et al., 2010; Frigola et al., 2014).

Perhaps the most recent relevant work to our approach is recurrent Gaussian processes
(RGP) (Mattos et al., 2015). RGP extends the GP-SSM framework to regression on sequences
by using a recurrent architecture with GP-based activation functions. The structure of
the RGP model mimics the standard RNN, where every parametric layer is substituted
with a Gaussian process. This procedure allows one to propagate uncertainty throughout
the network for an additional cost. Inference is intractable in RGP, and eﬃcient training
requires a sophisticated approximation procedure, the so-called recurrent variational Bayes.
In addition, the authors have to turn to RNN-based approximation of the variational mean
functions to battle the growth of the number of variational parameters with the size of
data. While technically promising, RGP seems problematic from the application perspective,
especially in its implementation and scalability aspects.

Our model has several distinctions with prior work aiming to regress sequences to
reals. Firstly, one of our goals is to keep the model as simple as possible while being
able to represent and quantify predictive uncertainty. We maintain an analytical objective
function and refrain from complicated and diﬃcult-to-diagnose inference schemes. This
simplicity is achieved by giving up the idea of propagating signal through a chain GPs
connected in a recurrent fashion. Instead, we propose to directly learn kernels with recurrent
structure via joint optimization of a simple functional composition of a standard GP with
a recurrent model (e.g., LSTM), as described in detail in the following section. Similar
approaches have recently been explored and proved to be fruitful for non-recurrent deep
networks (Wilson et al., 2016a,b; Calandra et al., 2016). We remark that combinations of
GPs with nonlinear functions have also been considered in the past in a slightly diﬀerent
setting of warped regression targets (Snelson et al., 2003; Wilson and Ghahramani, 2010;
Lázaro-Gredilla, 2012). Additionally, uncertainty over the recurrent parts of our model is
represented via dropout, which is computationally cheap and turns out to be equivalent
to approximate Bayesian inference in a deep Gaussian process (Damianou and Lawrence,
2013) with particular intermediate kernels (Gal and Ghahramani, 2016a,b). Finally, one
can also view our model as a standalone ﬂexible Gaussian process, which leverages learning
techniques that scale to massive datasets (Wilson and Nickisch, 2015; Wilson et al., 2015).

6

Learning Scalable Deep Kernels with Recurrent Structure

y5

g3

h3
3

x5

y4

g2

h3
2

x4

x2

h2
3

x4

h1
3

x3

y3

h3

x3

h1

x1

h2

x2

(a)

y3

g1

h3
1

x3

h2
1

x2

h1
1

x1

h2
2

x3

h1
2

x2

x1

x1

(b)

y

g

(c)

φ

x

Figure 1: (a) Graphical representation of a recurrent model (RNN/LSTM) that maps an input
sequence to a target value in one-step-ahead prediction manner. Shaded variables are observable,
diamond variables denote deterministic dependence on the inputs. (b) Graphical model for GP-
LSTM with a time lag, L = 3, two training time points, t = 3 and t = 4, and a testing time point,
t = 5. Latent representations are mapped to the outputs through a Gaussian ﬁeld (denoted in red)
that globally correlates predictions. Dashed variables represent data instances unused at the given
time step. (c) Graphical representation of a GP with a kernel structured with a parametric map, φ.

4. Learning recurrent kernels

Gaussian processes with diﬀerent kernel functions correspond to diﬀerent structured proba-
bilistic models. For example, some special cases of the Matérn class of kernels induce models
with Markovian structure (Stein, 1999). To construct deep kernels with recurrent structure
we transform the original input space with an LSTM network and build a kernel directly in
the transformed space, as shown in Figure 1b.

In particular, let L φ :

L

. The decomposition of the kernel and the transformation, ˜k = k

be an arbitrary deterministic transformation of the
R be a real-valued kernel
φ, is deﬁned

. Next, let k :

(cid:55)→

H

H

2

X

(cid:55)→ H

input sequences into some latent space,
deﬁned on
as

H

˜k(x, x(cid:48)) = k(φ(x), φ(x(cid:48))), where x, x(cid:48)

L, and ˜k : (cid:0)

L(cid:1)2

X

(cid:55)→

∈ X

(7)

◦

R.

It is trivial to show that ˜k(x, x(cid:48)) is a valid kernel deﬁned on
L (MacKay, 1998, Ch. 5.4.3).
In addition, if φ(
) is represented by a neural network, the resulting model can be viewed as
·
the same network, but with an additional GP-layer and the negative log marginal likelihood
(NLML) objective function used instead of the standard mean squared error (MSE).

X

7

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Input embedding is well-known in the Gaussian process literature (e.g. MacKay, 1998;
Hinton and Salakhutdinov, 2008). Recently, Wilson et al. (2016a,b) have successfully scaled
the approach and demonstrated strong results in regression and classiﬁcation tasks for
kernels based on feedforward and convolutional architectures. In this paper, we apply the
same technique to learn kernels with recurrent structure by transforming input sequences
with a recurrent neural network that acts as φ(
). In particular, a (multi-layer) LSTM
·
architecture is used to embed L steps of the input time series into a single vector in the
hidden space,
. For the embedding, as common, we use the last hidden vector produced by
the recurrent network. Note that however, any variations to the embedding (e.g., using other
types of recurrent units, adding 1-dimensional pooling layers, or attention mechanisms) are
all fairly straightforward2. More generally, the recurrent transformation can be random itself
(Figure 1), which would enable direct modeling of uncertainty within the recurrent dynamics,
but would also require inference for φ (e.g., as in Mattos et al., 2015). In this study, we
limit our consideration of random recurrent maps to only those induced by dropout.

H

Unfortunately, once the the MSE objective is substituted with NLML, it no longer factor-
izes over the data. This prevents us from using the well-established stochastic optimization
techniques for training our recurrent model. In the case of feedforward and convolutional
networks, Wilson et al. (2016a) proposed to pre-train the input transformations and then
ﬁne-tune them by jointly optimizing the GP marginal likelihood with respect to hyperpa-
rameters and the network weights using full-batch algorithms. When the transformation
is recurrent, stochastic updates play a key role. Therefore, we propose a semi-stochastic
block-gradient optimization procedure which allows mini-batching weight updates and fully
joint training of the model from scratch.

4.1 Optimization

The negative log marginal likelihood of the Gaussian process has the following form:

(K) = y(cid:62)(Ky + σ2I)−1y + log det(Ky + σ2I) + const,

(8)

L

where Ky + σ2I (∆= K) is the Gram kernel matrix, Ky, is computed on
N
i=1 and
implicitly depends on the base kernel hyperparameters, θ, and the parameters of the
recurrent neural transformation, φ(
), denoted W and further referred as the transformation
·
hyperparameters. Our goal is to optimize

with respect to both θ and W .

φ(xi)
}
{

The derivative of the NLML objective with respect to θ is standard and takes the

L

following form (Rasmussen and Williams, 2006):

(cid:18)(cid:104)

∂
L
∂θ

1
2

=

tr

K−1yy(cid:62)K−1

K−1(cid:105) ∂K
∂θ

−

(cid:19)

,

(9)

2. Details on the particular architectures used in our empirical study are discussed in the next section.

8

Learning Scalable Deep Kernels with Recurrent Structure

where ∂K/∂θ is depends on the kernel function, k(
,
·
The derivative with respect to the l-th transformation hyperparameter, Wl, is as follows:3
(cid:41)

), and usually has an analytic form.
·

K−1(cid:17)

(cid:40)(cid:18) ∂k(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

ij

−

+

(cid:18) ∂k(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

,

K−1yy(cid:62)K−1

∂
L
∂Wl

=

1
2

(cid:88)

(cid:16)

i,j

(10)
where hi = φ(xi) corresponds to the latent representation of the the i-th data instance. Once
the derivatives are computed, the model can be trained with any ﬁrst-order or quasi-Newton
optimization routine. However, application of the stochastic gradient method—the de facto
standard optimization routine for deep recurrent networks—is not straightforward: neither
the objective, nor its derivatives factorize over the data4 due to the kernel matrix inverses,
and hence convergence is not guaranteed.

Semi-stochastic alternating gradient descent. Observe that once the kernel matrix,
K−1(cid:1), can be precomputed on the full data
K, is ﬁxed, the expression, (cid:0)K−1yy(cid:62)K−1
−
and ﬁxed. Subsequently, Eq. (10) turns into a weighted sum of independent functions of
each data point. This observation suggests that, given a ﬁxed kernel matrix, one could
compute a stochastic update for W on a mini-batch of training points by only using the
corresponding sub-matrix of K. Hence, we propose to optimize GPs with recurrent kernels
in a semi-stochastic fashion, alternating between updating the kernel hyperparameters, θ,
on the full data ﬁrst, and then updating the weights of the recurrent network, W , using
stochastic steps. The procedure is given in Algorithm 1.

Semi-stochastic alternating gradient descent is a special case of block-stochastic gradient
iteration (Xu and Yin, 2015). While the latter splits the variables into arbitrary blocks
and applies Gauss–Seidel type stochastic gradient updates to each of them, our procedure
alternates between applying deterministic updates to θ and stochastic updates to W of the
form θ(t+1)
5. The corresponding Algorithm 1
is provably convergent for convex and non-convex problems under certain conditions. The
following theorem adapts results of Xu and Yin (2015) to our optimization scheme.

θ and W (t+1)

W (t) + λ(t)

θ(t) + λ(t)

θ g(t)

W g(t)

←

←

W

Theorem 1 (informal) Semi-stochastic alternating gradient descent converges to a ﬁxed
point when the learning rate, λt, decays as Θ(1/t

1+δ
2 ) for any δ

(0, 1].

∈

Applying alternating gradient to our case has a catch: the kernel matrix (and its inverse)
has to be updated each time W and θ are changed, i.e., on every mini-batch iteration
(marked red in Algorithm 1). Computationally, this updating strategy defeats the purpose
of stochastic gradients because we have to use the entire data on each step. To deal with
the issue of computational eﬃciency, we use ideas from asynchronous optimization.

3. Step-by-step derivations are given in Appendix B.
4. Cannot be represented as sums of independent functions of each data point.
5. In principle, stochastic updates of θ are also possible. As we will see next, we choose in practice to keep
the kernel matrix ﬁxed while performing stochastic updates. Due to sensitivity of the kernel to even
small changes in θ, convergence of the fully stochastic scheme is fragile.

9

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Algorithm 1 Semi-stochastic alternating
gradient descent.
input Data – (X, y), kernel – kθ(
,
·
recurrent transformation – φw(

1: Initialize θ and w; compute initial K.
2: repeat
3:

for all mini-batches Xb in X do

),
·
).
·

4:

5:

θ + updateθ(X, θ, K). and
θ
w + updatew(Xb, w, K).
w
Update the kernel matrix, K.

←
←

end for

6:
7: until Convergence
output Optimal θ∗ and w∗

Algorithm 2 Semi-stochastic asynchronous
gradient descent.
input Data – (X, y), kernel – kθ(
,
·
recurrent transformation – φw(
·

1: Initialize θ and w; compute initial K.
2: repeat
θ
θ + updateθ(X, w, K).
3:
for all mini-batches Xb in X do

),
·
).

←

4:

5:

6:

w + updatew(Xb, w, Kstale).

w
←
end for
Update the kernel matrix, K.

7:
8: until Convergence
output Optimal θ∗ and w∗

Asynchronous techniques. One of the recent trends in parallel and distributed
optimization is applying updates in an asynchronous fashion (Agarwal and Duchi, 2011).
Such strategies naturally require some tolerance to delays in parameter updates (Langford
et al., 2009). In our case, we modify Algorithm 1 to allow delayed kernel matrix updates.

The key observation is very intuitive: when the stochastic updates of W are small
enough, K does not change much between mini-batches, and hence we can perform multiple
stochastic steps for W before re-computing the kernel matrix, K, and still converge. For
example, K may be updated once at the end of each pass through the entire data (see
Algorithm 2). To ensure convergence of the algorithm, it is important to strike the balance
between (a) the learning rate for W and (b) the frequency of the kernel matrix updates.
The following theorem provides convergence results under certain conditions.

Theorem 2 (informal) Semi-stochastic gradient descent with τ -delayed kernel updates
(0, 1].
converges to a ﬁxed point when the learning rate, λt, decays as Θ(1/τ t

1+δ
2 ) for any δ

∈

Formal statements, conditions, and proofs for Theorems 1 and 2 are given in Appendix C.2.
Why stochastic optimization? GPs with recurrent kernels can be also trained with
full-batch gradient descent, as proposed by Wilson et al. (2016a). However, stochastic
gradient methods have been proved to attain better generalization (Hardt et al., 2016)
and often demonstrate superior performance in deep and recurrent architectures (Wilson
and Martinez, 2003). Moreover, stochastic methods are ‘online’, i.e., they update model
parameters based on subsets of an incoming data stream, and hence can scale to very large
datasets. In our experiments, we demonstrate that GPs with recurrent kernels trained with
Algorithm 2 converge faster (i.e., require fewer passes through the data) and attain better
performance than if trained with full-batch techniques.

10

Learning Scalable Deep Kernels with Recurrent Structure

Stochastic variational inference. Stochastic variational inference (SVI) in Gaussian
processes (Hensman et al., 2013) is another viable approach to enabling stochastic opti-
mization for GPs with recurrent kernels. Such method would optimize a variational lower
bound on the original objective that factorizes over the data by construction. Recently,
Wilson et al. (2016b) developed such a stochastic variational approach in the context of deep
kernel learning. Note that unlike all previous existing work, our proposed approach does
not require a variational approximation to the marginal likelihood to perform mini-batch
training of Gaussian processes.

4.2 Scalability

×

(n3) computations for n training data points, and

Learning and inference with Gaussian processes requires solving a linear system involving
n kernel matrix, K−1y, and computing a log determinant over K. These operations
an n
(n2) storage. In our
typically require
approach, scalability is achieved through semi-stochastic training and structure-exploiting
inference. In particular, asynchronous semi-stochastic gradient descent reduces both the
total number of passes through the data required for the model to converge and the number
of calls to the linear system solver; exploiting the structure of the kernels signiﬁcantly
reduces the time and memory complexities of the linear algebraic operations.

O

O

More precisely, we replace all instances of the covariance matrix Ky with W KU,U W (cid:62),
where W is a sparse interpolation matrix, and KU,U is the covariance matrix evaluated
over m latent inducing points, which decomposes into a Kronecker product of circulant
matrices (Wilson and Nickisch, 2015; Wilson et al., 2015). This construction makes inference
and learning scale as
(1), while preserving model structure.
For the sake of completeness, we provide an overview of the underlying algebraic machinery
in Appendix A.

(n) and test predictions be

O

O

At a high level, because W is sparse and KU,U is structured it is possible to take extremely
fast matrix vector multiplications (MVMs) with the approximate covariance matrix KX,X .
One can then use methods such as linear conjugate gradients, which only use MVMs, to
eﬃciently solve linear systems. MVM or scaled eigenvalue approaches (Wilson and Nickisch,
2015; Wilson et al., 2015) can also be used to eﬃciently compute the log determinant and
its derivatives. Kernel interpolation (Wilson et al., 2015) also enables fast predictions, as we
describe further in the Appendix.

5. Experiments

We compare the proposed Gaussian processes with recurrent kernels based on RNN and
LSTM architectures (GP-RNN/LSTM) with a number of baselines on datasets of various
complexity and ranging in size from hundreds to almost a million of time points. For the
datasets with more than a few thousand points, we use a massively scalable version of GPs
(see Section 4.2) and demonstrate its scalability during inference and learning. We carry

11

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Table 1: Statistics for the data used in experiments. SNR was determined by assuming a certain
degree of smoothness of the signal, ﬁtting kernel ridge regression with RBF kernel to predict the
targets from the input time series, and regarding the residuals as the noise. Tasks with low average
correlation between inputs and targets and lower SNR are harder prediction problems.

Dataset Task

# time steps # dim # outputs Abs. corr.

SNR

Drives
Actuator

GEF

Car

system ident.

power load
wind power

speed
gyro yaw
lanes
lead vehicle

500
1,024

38,064
130,963

932,939

1
1

11
16

6
6
26
9

1
1

1
1

1
1
16
2

0.7994
0.0938

0.5147
0.1731

0.1196
0.0764
0.0816
0.1099

25.72
12.47

89.93
4.06

159.33
3.19
—
—

out a number of experiments that help to gain empirical insights about the convergence
properties of the proposed optimization procedure with delayed kernel updates. Additionally,
we analyze the regularization properties of GP-RNN/LSTM and compare them with other
techniques, such as dropout. Finally, we apply the model to the problem of lane estimation
and lead vehicle position prediction, both critical in autonomous driving applications.

5.1 Data and the setup

Below, we describe each dataset we used in our experiments and the associated prediction
tasks. The essential statistics for the datasets are summarized in Table 1.

System identiﬁcation. In the ﬁrst set of experiments, we used publicly available non-
linear system identiﬁcation datasets: Actuator 6 (Sjöberg et al., 1995) and Drives 7 (Wigren,
2010). Both datasets had one dimensional input and output time series. Actuator had the
size of the valve opening as the input and the resulting change in oil pressure as the output.
Drives was from a system with motors that drive a pulley using a ﬂexible belt; the input
was the sum of voltages applied to the motors and the output was the speed of the belt.

Smart grid data8. We considered the problem of forecasting for the smart grid that
consisted of two tasks (Figure 2). The ﬁrst task was to predict power load from the historical
temperature data. The data had 11 input time series coming from hourly measurements of
temperature on 11 zones and an output time series that represented the cumulative hourly
power load on a U.S. utility. The second task was to predict power generated by wind
farms from the wind forecasts. The data consisted of 4 diﬀerent hourly forecasts of the
wind and hourly values of the generated power by a wind farm. Each wind forecast was a

6. http://www.iau.dtu.dk/nnbook/systems.html
7. http://www.it.uu.se/research/publications/reports/2010-020/NonlinearData.zip.
8. The smart grid data were taken from Global Energy Forecasting Kaggle competitions organized in 2012.

12

Learning Scalable Deep Kernels with Recurrent Structure

Figure 2: Left: Visualization of the GEF-power time series for two zones and the cumulative load
with the time resolution of 1 day. Cumulative power load is generally negatively correlated with
the temperature measurements on all the zones. Right: Visualization of the GEF-wind time series
with the time resolution of 1 day.

4-element vector that corresponded to zonal component, meridional component, wind speed
and wind angle. In our experiments, we concatenated the 4 diﬀerent 4-element forecasts,
which resulted in a 16-dimensional input time series.

Self-driving car dataset9. One of the main target applications of the proposed model
is prediction for autonomous driving. We considered a large dataset coming from sensors
of a self-driving car that was recorded on two car trips with discretization of 10 ms. The
data featured two sets of GPS ECEF locations, ECEF velocities, measurements from a
ﬁber-optic gyro compass, LIDAR, and a few more time series from a variety of IMU sensors.
Additionally, locations of the left and right lanes were extracted from a video stream for
each time step as well as the position of the lead vehicle from the LIDAR measurements. We
considered the data from the ﬁrst trip for training and from the second trip for validation
and testing. A visualization of the car routes with 25 second discretization in the ENU
coordinates are given in Figure 3. We consider four tasks, the ﬁrst two of which are more of
proof-of-concept type variety, while the ﬁnal two are fundamental to good performance for
a self-driving car:

1. Speed prediction from noisy GPS velocity estimates and gyroscopic inputs.

2. Prediction of the angular acceleration of the car from the estimates of its speed and

steering angle.

3. Point-wise prediction of the lanes from the estimates at the previous time steps, and

estimates of speed, gyroscopic and compass measurements.

4. Prediction of the lead vehicle location from its location at the previous time steps,

and estimates of speed, gyroscopic and compass measurements.

We provide more speciﬁc details on the smart grid data and self-driving data in Appendix D.

9. The dataset is proprietary.

It was released in part for public use under the Creative Commons
Attribution 3.0 license: http://archive.org/details/comma-dataset. More about the self-driving car:
http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/.

13

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Train

Test

Time

42.8 min

46.5 min

Speed of the self-driving car

Min

Max

0.0 mph

0.0 mph

80.3 mph

70.1 mph

Average

42.4 mph

38.4 mph

Median

58.9 mph

44.8 mph

Distance to the lead vehicle

Min

Max

Average

Median

23.7 m

29.7 m

178.7 m

184.7 m

85.4 m

54.9 m

72.6 m

53.0 m

Figure 3 & Table 2: Left: Train and test routes of the self-driving car in the ENU coordinates
with the origin at the starting location. Arrows point in the direction of motion; color encodes the
speed. Insets zoom selected regions of the routes. Best viewed in color. Right: Summary of the
data collected on the train and test routes.

Models and metrics. We used a number of classical baselines: NARX (Lin et al.,
1996), GP-NARX models (Kocijan et al., 2005), and classical RNN and LSTM architectures.
The kernels of our models, GP-NARX/RNN/LSTM, used the ARD base kernel function and
were structured by the corresponding baselines.10 As the primary metric, we used root mean
squared error (RMSE) on a held out set and additionally negative log marginal likelihood
(NLML) on the training set for the GP-based models.

We train all the models to perform one-step-ahead prediction in an autoregressive setting,
where targets at the future time steps are predicted from the input and target values at a
ﬁxed number of past time steps. For the system identiﬁcation task, we additionally consider
the non-autoregressive scenario (i.e., mapping only input sequences to the future targets),
where we are performing prediction in the free simulation mode, and included recurrent
Gaussian processes (Mattos et al., 2015) in our comparison. In this case, none of the future
targets are available and the models have to re-use their own past predictions to produce
future forecasts).

A note on implementation. Recurrent parts of each model were implemented using
Keras 11 library. We extended Keras with the Gaussian process layer and developed a backed
engine based on the GPML library12. Our approach allows us to take full advantage of
the functionality available in Keras and GPML, e.g., use automatic diﬀerentiation for the
recurrent part of the model. Our code is available at http://github.com/alshedivat/kgp/.

10. GP-NARX is a special instance of our more general framework and we trained it using the proposed

semi-stochastic algorithm.

11. http://www.keras.io
12. http://www.gaussianprocess.org/gpml/code/matlab/doc/

14

Learning Scalable Deep Kernels with Recurrent Structure

Figure 4: Two charts on the left: Convergence of the optimization in terms of RMSE on test and
NLML on train. The inset zooms the region of the plot right beneath it using log scale for the
vertical axis. full and mini denote full-batch and mini-batch optimization procedures, respectively,
while 16 and 64 refer to models with the respective number of units per hidden layer. Two charts on
the right: Test RMSE for a given architecture trained with a speciﬁed method and/or learning rate.

5.2 Analysis

This section discusses quantitative and qualitative experimental results. We only brieﬂy
introduce the model architectures and the training schemes used in each of the experiments.
We provide a comprehensive summary of these details in Appendix E.

5.2.1 Convergence of the optimization

To address the question of whether stochastic optimization of recurrent kernels is necessary
and to assess the behavior of the proposed optimization scheme with delayed kernel updates,
we conducted a number of experiments on the Actuator dataset (Figure 4).

First, we constructed two GP-LSTM models with 1 recurrent hidden layer and 16 or 64
hidden units and trained them with (non-stochastic) full-batch iterative procedure (similar
to the proposal of Wilson et al. (2016a)) and with our semi-stochastic optimizer with delayed
kernel updates (Algorithm 2). The convergence results are given on the ﬁrst two charts.
Both in terms of the error on a held out set and the NLML on the training set, the models
trained with mini-batches converged faster and demonstrated better ﬁnal performance.

Next, we compared the two optimization schemes on the same GP-LSTM architecture
with diﬀerent sizes of the hidden layer ranging from 2 to 32. It is clear from the third chart
that, even though full-batch approach seemed to ﬁnd a better optimum when the number of
hidden units was small, the stochastic approach was clearly superior for larger hidden layers.
Finally, we compared the behavior of Algorithm 2 with diﬀerent number of mini-batches
used for each epoch (equivalently, the number of steps between the kernel matrix updates)
and diﬀerent learning rates. The results are give on the last chart. As expected, there is a
ﬁne balance between the number of mini-batches and the learning rate: if the number of
mini-batches is large (i.e., the delay between the kernel updates becomes too long) while the
learning rate is high enough, optimization does not converge; at the same time, an appropriate
combination of the learning rate and the mini-batch size leads better generalization than
the default batch approach of Wilson et al. (2016a).

15

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Table 3: Average performance of the models in terms of RMSE on the system identiﬁcation tasks.
The averages were computed over 5 runs; the standard deviation is given in the parenthesis. Results
for the RGP model are as reported by Mattos et al. (2015), available only for the free simulation.

regression

auto-regression

free simulation

regression

free simulation

Drives

0.19 (0.03)
0.17 (0.04)
0.14 (0.02)

0.16 (0.04)
0.16 (0.03)
0.13 (0.02)

0.38 (0.03)
0.56 (0.03)
0.40 (0.02)

0.28 (0.02)
0.45 (0.03)
0.32 (0.03)

0.49 (0.05)
0.56 (0.03)
0.40 (0.03)

0.46 (0.03)
0.49 (0.02)
0.36 (0.01)

Actuator
auto-regression

0.18 (0.01)
0.17 (0.01)
0.19 (0.01)

0.14 (0.01)
0.15 (0.01)
0.14 (0.01)

0.57 (0.04)
0.68 (0.05)
0.44 (0.03)

0.63 (0.04)
0.55 (0.04)
0.43 (0.03)

NARX
RNN
LSTM

0.33 (0.02)
0.53 (0.02)
0.29 (0.02)

GP-NARX 0.28 (0.02)
GP-RNN
0.37 (0.04)
GP-LSTM 0.25 (0.02)

RGP

—

—

0.249

—

—

0.368

5.2.2 Regression, Auto-regression, and Free Simulation

In this set of experiments, our main goal is to provide a comparison between three diﬀerent
modes of one-step-ahead prediction, referred to as (i) regression, (ii) autoregression, and
(iii) free simulation, and compare performance of our models with RGP—a classical RNN
with every parametric layer substituted with a Gaussian process (Mattos et al., 2015)—on
the Actuator and Drives datasets. The diﬀerence between the prediction modes consists in
whether and how the information about the past targets is used. In the regression scenario,
inputs and targets are separate time series and the model learns to map input values at
a number of past time points to a target value at a future point in time. Autoregression,
additionally, uses the true past target values as inputs; in the free simulation mode, the
model learns to map past inputs and its own past predictions to a future target.

In the experiments in autoregression and free simulation modes, we used short time lags,
L = 10, as suggested by Mattos et al. (2015). In the regression mode, since the model does
not build the recurrent relationships based on the information about the targets (or their
estimates), it generally requires larger time lags that can capture the state of the dynamics.
Hence we increased the time lag to 32 in the regression mode. More details are given in
Appendix E.

We present the results in Table 3. We note that GP-based architectures consistently
yielded improved predictive performance compared to their vanilla deep learning counterparts
on both of the datasets, in each mode. Given the small size of the datasets, we attribute
such behavior to better regularization properties of the negative log marginal likelihood loss
function. We also found out that when GP-based models were initialized with weights of
pre-trained neural networks, they tended to overﬁt and give overly conﬁdent predictions
on these tasks. The best performance was achieved when the models were trained from a
random initialization (contrary to the ﬁndings of Wilson et al., 2016a). In free simulation
mode RGP performs best of the compared models. This result is expected—RGP was
particularly designed to represent and propagate uncertainty through a recurrent process.

16

Learning Scalable Deep Kernels with Recurrent Structure

Table 4: Average performance of the best models in terms of RMSE on the GEF and Car tasks.
The averages were computed over 5 runs; the standard deviation is given in the parenthesis.

GEF
power load wind power

speed

gyro yaw

lanes

lead vehicle

Car

NARX
RNN
LSTM

0.54 (0.02)
0.61 (0.02)
0.45 (0.01)

GP-NARX 0.78 (0.03)
GP-RNN
0.24 (0.02)
GP-LSTM 0.17 (0.02)

0.84 (0.01)
0.81 (0.01)
0.77 (0.01)

0.83 (0.02)
0.79 (0.01)
0.76 (0.01)

0.114 (0.010)
0.152 (0.012)
0.027 (0.008)

0.19 (0.01)
0.22 (0.01)
0.13 (0.01)

0.13 (0.01)
0.33 (0.02)
0.08 (0.01)

0.41 (0.02)
0.44 (0.03)
0.40 (0.01)

0.125 (0.015)
0.089 (0.013)
0.019 (0.006)

0.23 (0.02)
0.24 (0.01)
0.08 (0.01)

0.10 (0.01)
0.46 (0.08)
0.06 (0.01)

0.34 (0.02)
0.41 (0.02)
0.32 (0.02)

Our framework focuses on using recurrence to build expressive kernels for regression on
sequences.

The suitability of each prediction mode depends on the task at hand. In many applications
where the future targets become readily available as the time passes (e.g., power estimation
or stock market prediction), the autoregression mode is preferable. We particularly consider
autoregressive prediction in the further experiments.

5.2.3 Prediction for smart grid and self-driving car applications

For both smart grid prediction tasks we used LSTM and GP-LSTM models with 48 hour
time lags and were predicting the target values one hour ahead. LSTM and GP-LSTM
were trained with one or two layers and 32 to 256 hidden units. The best models were
selected on 25% of the training data used for validation. For autonomous driving prediction
tasks, we used the same architectures but with 128 time steps of lag (1.28 s). All models
were regularized with dropout (Srivastava et al., 2014; Gal and Ghahramani, 2016b). On
both GEF and self-driving car datasets, we used the scalable version of Gaussian process
(MSGP) (Wilson et al., 2015). Given the scale of the data and the challenge of nonlinear
optimization of the recurrent models, we initialized the recurrent parts of GP-RNN and
GP-LSTM with pre-trained weights of the corresponding neural networks. Fine-tuning of the
models was performed with Algorithm 2. The quantitative results are provided in Table 4
and demonstrate that GPs with recurrent kernels attain the state-of-the-art performance.
Additionally, we investigated convergence and regularization properties of LSTM and
GP-LSTM models on the GEF-power dataset. The ﬁrst two charts of Figure 6 demonstrate
that GP-based models are less prone to overﬁtting, even when the data is not enough. The
third panel shows that architectures with a particular number of hidden units per layer
attain the best performance on the power prediction task. An additional advantage of
the GP-layers over the standard recurrent networks is that the best architecture could be
identiﬁed based on the negative log likelihood of the model as shown on the last chart.

17

Al-Shedivat, Wilson, Saatchi, Hu, Xing

(a) Point-wise predictions of the lanes made by LSTM (upper) and by GP-LSTM (lower). Dashed
lines correspond to the ground truth extracted from video sequences and used for training.

(b) LSTM (upper) and by GP-LSTM (lower) position predictions of the lead vehicle. Black markers
and dashed lines are the ground truth; blue and red markers with solid lines correspond to predictions.

Figure 5: Qualitative comparison of the LSTM and GP-LSTM predictions on self-driving tasks.
Predictive uncertainty of the GP-LSTM model is showed by contour plots and error-bars; the latter
denote one standard deviation of the predictive distributions.

Finally, Figure 5 qualitatively demonstrates the diﬀerence between the predictions given
by LSTM vs. GP-LSTM on point-wise lane estimation (Figure 5a) and the front vehicle
tracking (Figure 5b) tasks. We note that GP-LSTM not only provides a more robust ﬁt,
but also estimates the uncertainty of its predictions. Such information can be further used
in downstream prediction-based decision making, e.g., such as whether a self-driving car
should slow down and switch to a more cautious driving style when the uncertainty is high.

18

Learning Scalable Deep Kernels with Recurrent Structure

Figure 6: Left to right: RMSE vs. the number of training points; RMSE vs. the number model
parameters per layer; NLML vs. the number model parameters per layer for GP-based models. All
metrics are averages over 5 runs with diﬀerent random initializations, computed on a held-out set.

Figure 7: The charts demonstrate scalability of learning and inference of MSGP with an LSTM-based
recurrent kernel. Legends with points denote the number of inducing points used. Legends with
percentages denote the percentage of the training dataset used learning the model.

5.2.4 Scalability of the model

Following Wilson et al. (2015), we performed a generic scalability analysis of the MSGP-
LSTM model on the car sensors data. The LSTM architecture was the same as described
in the previous section: it was transforming multi-dimensional sequences of inputs to a
two-dimensional representation. We trained the model for 10 epochs on 10%, 20%, 40%,
and 80% of the training set with 100, 200, and 400 inducing points per dimension and
measured the average training time per epoch and the average prediction time per testing
point. The measured time was the total time spent on both LSTM optimization and MSGP
computations. The results are presented in Figure 7.

The training time per epoch (one full pass through the entire training data) grows linearly
with the number of training examples and depends linearly on the number of inducing points
(Figure 7, two left charts). Thus, given a ﬁxed number of inducing points per dimension, the
time complexity of MSGP-LSTM learning and inference procedures is linear in the number
of training examples. The prediction time per testing data point is virtually constant and
does not depend on neither on the number of training points, nor on the number of inducing
points (Figure 7, two right charts).

19

Al-Shedivat, Wilson, Saatchi, Hu, Xing

6. Discussion

We proposed a method for learning kernels with recurrent long short-term memory structure
on sequences. Gaussian processes with such kernels, termed the GP-LSTM, have the structure
and learning biases of LSTMs, while retaining a probabilistic Bayesian nonparametric
representation. The GP-LSTM outperforms a range of alternatives on several sequence-to-
reals regression tasks. The GP-LSTM also works on data with low and high signal-to-noise
ratios, and can be scaled to very large datasets, all with a straightforward, practical, and
generally applicable model speciﬁcation. Moreover, the semi-stochastic scheme proposed
in our paper is provably convergent and eﬃcient in practical settings, in conjunction with
structure exploiting algebra. In short, the GP-LSTM provides a natural mechanism for
Bayesian LSTMs, quantifying predictive uncertainty while harmonizing with the standard
deep learning toolbox. Predictive uncertainty is of high value in robotics applications, such
as autonomous driving, and could also be applied to other areas such as ﬁnancial modeling
and computational biology.

There are several exciting directions for future research. The GP-LSTM quantiﬁes
predictive uncertainty but does not model the propagation of uncertainty in the inputs
through a recurrent structure. Treating free simulation as a structured prediction problem
and using online corrective algorithms, e.g., DAGGER (Ross et al., 2011), are likely to
improve performance of GP-LSTM in the free prediction mode. This approach would not
require explicitly modeling and propagating uncertainty through the recurrence and would
maintain the high computational eﬃciency of our method.

Alternatively, it would be exciting to have a probabilistic treatment of all parameters of
the GP-LSTM kernel, including all LSTM weights. Such an extension could be combined with
stochastic variational inference, to enable both classiﬁcation and non-Gaussian likelihoods
as in Wilson et al. (2016b), but also open the doors to stochastic gradient Hamiltonian
Monte Carlo (Chen et al., 2014) (SG-HMC) for eﬃcient inference over kernel parameters.
Indeed, SG-HMC has recently been used for eﬃcient inference over network parameters in
the Bayesian GAN (Saatchi and Wilson, 2017). A Bayesian approach to marginalizing the
weights of the GP-LSTM kernel would also provide a principled probabilistic mechanism for
learning model hyperparameters.

One could relax several additional assumptions. We modeled each output dimension
with independent GPs that shared a recurrent transformation. To capture the correlations
between output dimensions, it would be promising to move to a multi-task formulation. In
the future, one could also learn the time horizon in the recurrent transformation, which
could lead to major additional performance gains.

Finally, the semi-stochastic learning procedure naturally complements research in asyn-
chronous optimization (e.g., Deisenroth and Ng, 2015). In combination with stochastic
variational inference, the semi-stochastic approach could be used for parallel kernel learning,
side-stepping the independence assumptions in prior work. We envision that such eﬀorts for
Gaussian processes will harmonize with current progress in Bayesian deep learning.

20

Learning Scalable Deep Kernels with Recurrent Structure

7. Acknowledgements

The authors thank Yifei Ma for helpful discussions and the anonymous reviewers for the
valuable comments that helped to improve the paper. This work was supported in part by
NIH R01GM114311, AFRL/DARPA FA87501220324, and NSF IIS-1563887.

21

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Appendix A. Massively scalable Gaussian processes

Massively scalable Gaussian processes (MSGP) (Wilson et al., 2015) is a signiﬁcant extension
of the kernel interpolation framework originally proposed by Wilson and Nickisch (2015). The
core idea of the framework is to improve scalability of the inducing point methods (Quinonero-
Candela and Rasmussen, 2005) by (1) placing the virtual points on a regular grid, (2)
exploiting the resulting Kronecker and Toeplitz structures of the relevant covariance matrices,
and (3) do local cubic interpolation to go back to the kernel evaluated at the original points.
This combination of techniques brings the complexity down to
(1)
for each test prediction. Below, we overview the methodology. We remark that a major
diﬀerence in philosophy between MSGP and many classical inducing point methods is that
the points are selected and ﬁxed rather than optimized over. This allows to use signiﬁcantly
more virtual points which typically results in a better approximation of the true kernel.

(n) for training and

O

O

A.1 Structured kernel interpolation

Given a set of m inducing points, the n
m cross-covariance matrix, KX,U , between the
training inputs, X, and the inducing points, U, can be approximated as ˜KX,U = WX KU,U
m matrix of interpolation weights, WX . This allows to
using a (potentially sparse) n
approximate KX,Z for an arbitrary set of inputs Z as KX,Z
Z . For any given
kernel function, K, and a set of inducing points, U, structured kernel interpolation (SKI)
procedure (Wilson and Nickisch, 2015) gives rise to the following approximate kernel:

˜KX,U W (cid:62)

≈

×

×

KSKI(x, z) = WX KU,U W (cid:62)
z ,

(A.1)

≈

WX KU,U W (cid:62)
X . Wilson and Nickisch (2015) note that
which allows to approximate KX,X
standard inducing point approaches, such as subset of regression (SoR) or fully independent
training conditional (FITC), can be reinterpreted from the SKI perspective. Importantly,
the eﬃciency of SKI-based MSGP methods comes from, ﬁrst, a clever choice of a set of
inducing points that allows to exploit algebraic structure of KU,U , and second, from using
very sparse local interpolation matrices. In practice, local cubic interpolation is used (Keys,
1981).

A.2 Kernel approximations

If inducing points, U , form a regularly spaced P -dimensional grid, and we use a stationary
product kernel (e.g., the RBF kernel), then KU,U decomposes as a Kronecker product of
Toeplitz matrices:

KU,U = T1

T2

TP .

⊗

⊗ · · · ⊗

(A.2)

The Kronecker structure allows to compute the eigendecomposition of KU,U by separately
decomposing T1, . . . , TP , each of which is much smaller than KU,U . Further, to eﬃciently

22

Learning Scalable Deep Kernels with Recurrent Structure

eigendecompose a Toeplitz matrix, it can be approximated by a circulant matrix13 which
eigendecomposes by simply applying discrete Fourier transform (DFT) to its ﬁrst column.
Therefore, an approximate eigendecomposition of each T1, . . . , TP is computed via the fast
Fourier transform (FFT) and requires only

(m log m) time.

O

A.3 Structure exploiting inference

To perform inference, we need to solve (KSKI + σ2I)−1y; kernel learning requires evaluating
log det(KSKI + σ2I). The ﬁrst task can be accomplished by using an iterative scheme—linear
conjugate gradients—which depends only on matrix vector multiplications with (KSKI + σ2I).
The second is done by exploiting the Kronecker and Toeplitz structure of KU,U for computing
an approximate eigendecomposition, as described above.

A.4 Fast Test Predictions

To achieve constant time prediction, we approximate the latent mean and variance of f∗ by
applying the same SKI technique. In particular, for a set of n∗ testing points, X∗, we have

E[f∗] = µX∗ + KX∗,X
µX∗ + ˜KX∗,X

(cid:2)KX,X + σ2I(cid:3)−1
(cid:105)−1
(cid:104) ˜KX,X + σ2I

y

y,

≈

(A.3)

where ˜KX,X = W KU,U W (cid:62) and ˜KX∗,X = W∗KU,U W (cid:62), and W and W∗ are n
m
sparse interpolation matrices, respectively. Since KU,U W (cid:62)[ ˜KX,X + σ2I]−1y is precomputed
at training time, at test time, we only multiply the latter with W∗ matrix which results which
(1) operations per test point. Similarly, approximate
(n∗) operations leading to
costs
(1) operations (Wilson et al., 2015).
predictive variance can be also estimated in

m and n∗

O

O

×

×

Note that the fast prediction methodology can be readily applied to any trained Gaussian

O

process model as it is agnostic to the way inference and learning were performed.

Appendix B. Gradients for GPs with recurrent kernels

GPs with deep recurrent kernels are trained by minimizing the negative log marginal
likelihood objective function. Below we derive the update rules.

By applying the chain rule, we get the following ﬁrst order derivatives:

∂
L
∂γ

=

∂
L
∂K ·

∂K
∂γ

,

∂
L
∂W

=

∂
L
∂K ·

∂K
∂φ ·

∂φ
∂W

.

(B.4)

The derivative of the log marginal likelihood w.r.t. to the kernel hyperparameters, θ, and
the parameters of the recurrent map, W , are generic and take the following form (Rasmussen

13. Wilson et al. (2015) explored 5 diﬀerent approximation methods known in the numerical analysis

literature.

23

Al-Shedivat, Wilson, Saatchi, Hu, Xing

and Williams, 2006, Ch. 5, Eq. 5.9):

∂
L
∂θ
∂
L
∂W

= 1

= 1

2 tr (cid:0)(cid:2)K−1yy(cid:62)K−1
2 tr (cid:0)(cid:2)K−1yy(cid:62)K−1

K−1(cid:3) ∂K
∂θ

(cid:1) ,

K−1(cid:3) ∂K
∂W

(cid:1) .

−

−

The derivative ∂K/∂θ is also standard and depends on the form of a particular chosen
kernel function, k(
/∂W is a bit subtle, and hence
,
·
we elaborate these derivations below.

). However, computing each part of ∂
·

L

Consider the ij-th entry of the kernel matrix, Kij. We can think of K as a matrix-valued
function of all the data vectors in d-dimensional transformed space which we denote by
RN ×d. Then Kij is a scalar-valued function of H and its derivative w.r.t. the l-th
H
parameter of the recurrent map, Wl, can be written as follows:

∈

∂Kij
∂Wl

= tr

(cid:32)(cid:18) ∂Kij
∂H

(cid:19)(cid:62) ∂H
∂Wl

(cid:33)

.

Notice that ∂Kij/∂H is a derivative of a scalar w.r.t. to a matrix and hence is a matrix;
∂H/∂Wl is a derivative of a matrix w.r.t. to a scalar which is taken element-wise and also
gives a matrix. Also notice that Kij is a function of H, but it only depends the i-th and
j-th elements for which the kernel is being computed. This means that ∂Kij/∂H will have
only non-zero i-th row and j-th column and allows us to re-write (B.7) as follows:

(B.5)

(B.6)

(B.7)

(B.8)

∂Kij
∂Wl

=

=

(cid:18) ∂Kij
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

+

(cid:18) ∂Kij
∂hj

(cid:18) ∂K(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

+

(cid:19)(cid:62) ∂hj
∂Wl
(cid:18) ∂K(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

.

Since the kernel function has two arguments, the derivatives must be taken with respect of
each of them and evaluated at the corresponding points in the hidden space, hi = φ(xi)
and hj = φ(xj). When we plug this into (B.6), we arrive at the following expression:

∂
L
∂Wl

=

1
2

(cid:88)

(cid:16)

i,j

K−1yy(cid:62)K−1

K−1(cid:17)

(cid:40)(cid:18) ∂K(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

ij

−

+

(cid:18) ∂K(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

(cid:41)

.

(B.9)
The same expression can be written in a more compact form using the Einstein notation:

∂
L
∂Wl

=

(cid:16)

1
2

K−1yy(cid:62)K−1

K−1(cid:17)j

(cid:32)(cid:20) ∂K
∂h

i

(cid:21)jd

i

+

(cid:20) ∂K
∂h

(cid:21)id

(cid:33) (cid:20) ∂h
∂W

(cid:21)dl

i

j

−

(B.10)

where d indexes the dimensions of the h and l indexes the dimensions of W .

24

Learning Scalable Deep Kernels with Recurrent Structure

In practice, deriving a computationally eﬃcient analytical form of ∂K/∂h might be too
complicated for some kernels (e.g., the spectral mixture kernels (Wilson and Adams, 2013)),
especially if the grid-based approximations of the kernel are enabled. In such cases, we can
simply use a ﬁnite diﬀerence approximation of this derivative. As we remark in the following
section, numerical errors that result from this approximation do not aﬀect convergence of
the algorithm.

Appendix C. Convergence results

Convergence results for the semi-stochastic alternating gradient schemes with and without
delayed kernel matrix updates are based on (Xu and Yin, 2015). There are a few notable
diﬀerences between the original setting and the one considered in this paper:

1. Xu and Yin (2015) consider a stochastic program that minimizes the expectation of

the objective w.r.t. some distribution underlying the data:

f (x) := EξF (x; ξ),

min
x∈X

(C.11)

where every iteration a new ξ is sampled from the underlying distribution. In our
case, the goal is to minimize the negative log marginal likelihood on a particular given
dataset. This is equivalent to the original formulation (C.11), but with the expectation
taken w.r.t. the empirical distribution that corresponds to the given dataset.

2. The optimization procedure of Xu and Yin (2015) has access to only a single random
point generated from the data distribution at each step. Our algorithm requires having
access to the entire training data each time the kernel matrix is computed.

3. For a given sample, Xu and Yin (2015) propose to loop over a number of coordinate
blocks and apply Gauss–Seidel type gradient updates to each block. Our semi-stochastic
scheme has only two parameter blocks, θ and W , where θ is updated deterministically
on the entire dataset while W is updated with stochastic gradient on samples from
the empirical distribution.

Noting these diﬀerences, we ﬁrst adapt convergence results for the smooth non-convex
case (Xu and Yin, 2015, Theorem 2.10) to our scenario, and then consider the variant with
delaying kernel matrix updates.

C.1 Semi-stochastic alternating gradient

As shown in Algorithm 1, we alternate between updating θ and W . At step t, we get a
¯xi
mini-batch of size Nt, xt
i∈It, which is just a selection of points from the full set, x.
Deﬁne the gradient errors for θ and W at step t as follows:

≡ {

}

θ := ˜gt
δt

gt
θ,

W := ˜gt
δt

gt
W ,

W −

θ −

(C.12)

25

Al-Shedivat, Wilson, Saatchi, Hu, Xing

where gT
and then W , and hence the expressions for the gradients take the following form:

W are the true gradients and ˜gT

W are estimates14. We ﬁrst update θ

θ and gT

θ and ˜gT

˜gt
θ ≡

gt
θ =

(θt, W t) =

(cid:88)

(cid:16)

1
2N

K−1

t yy(cid:62)K−1

t −

(cid:17)

K−1
t

(cid:19)

(cid:18) ∂Kt
∂θ

ij

1
N ∇

θ

L

1
N ∇

W

L

i,j
1
2N

(cid:88)

(cid:16)

i,j

gt
W =

˜gt
W =

(θt+1, W t) =

K−1

t+1yy(cid:62)K−1

1
2Nt

(cid:88)

(cid:16)

i,j∈It

K−1

t+1yy(cid:62)K−1

t+1 −

(cid:17)

K−1
t+1

(cid:18) ∂Kt+1
∂W

ij

t+1 −
(cid:19)

ij

ij
(cid:18) ∂Kt+1
∂W

(cid:19)

(cid:17)

K−1
t+1

ij

(C.13)

(C.14)
ij

(C.15)

Note that as we have shown in Appendix B, when the kernel matrix is ﬁxed, gW and ˜gW
factorize over x and xt, respectively. Further, we denote all the mini-batches sampled before
t as x[t−1].

Lemma 1 For any step t, E[δt

x[t−1]] = E[δt

x[t−1]] = 0.

W |

θ |

Proof First, δt
have E[δt

0, and hence E[δt

x[t−1]] = 0 is trivial. Next, by deﬁnition of δt

W |

θ ≡

θ |
x[t−1]]. Consider the following:

x[t−1]] = E[˜gt

gt
W −
W |
Consider E[gt
x[t−1]]: gt
being updated deterministically using gt−1
Therefore, it is independent of xt, which means that E[gt

W is a deterministic function of θt+1 and W t. θt+1 is
, and hence only depends on W t and θt.
x[t−1]]

W |

W , we

θ

gt
W .

W |

≡

Now, consider E[˜gt
distribution and Kt+1 does not depend on the current mini-batch, we can write:

x[t−1]]: Noting that the expectation is taken w.r.t. the empirical

W |

•

•

E[˜gt

W |

x[t−1]] = E

K−1

t+1yy(cid:62)K−1

K−1
t+1

t+1 −

(cid:19)





ij

(cid:17)

ij

(cid:18) ∂Kt+1
∂W
(cid:18) ∂Kt+1
∂W

(cid:19)

ij

(C.16)

K−1

t+1yy(cid:62)K−1

t+1 −

(cid:17)

K−1
t+1

ij





1
2Nt

(cid:88)

(cid:16)

i,j∈It
(cid:16)

(cid:88)

i,j

Nt
2N Nt

gt
W .

=

≡

Finally, E[δt

x[t−1]] = gt

W |

gt
W = 0.

W −

In other words, semi-stochastic gradient descent that alternates between updating θ and

W computes unbiased estimates of the gradients on each step.

14. Here, we consider the gradients and their estimates scaled by the number of full data points, N , and the
mini-batch size, Nt, respectively. These constant scaling is introduced for sake of having cleaner proofs.

26

Learning Scalable Deep Kernels with Recurrent Structure

Remark 1 Note that in case if (∂Kt+1/∂W ) is computed approximately, Lemma 1 still
holds since both gt

x[t−1]] will contain exactly the same numerical errors.

W and E[˜gt

W |

Assumption 1 For any step t, E
δt
θ(cid:107)
(cid:107)

2

t and E
δt
σ2
2
W (cid:107)
(cid:107)

≤

≤

σ2
t .

Lemma 1 and Assumption 1 result into a stronger condition than the original assumption
given by Xu and Yin (2015). This is due to the semi-stochastic nature of the algorithm, it
simpliﬁes the analysis, though it is not critical. Assumptions 2 and 3 are straightforwardly
adapted from the original paper.

Assumption 2 The objective function,
θ and W are uniformly Lipschitz with constant L > 0:

L

, is lower bounded and its partial derivatives w.r.t.

(θ, W )

θ
(cid:107)∇

L

θ

− ∇

L

(˜θ, W )

θ

L
(cid:107)

−

˜θ

,
(cid:107)

(cid:107) ≤

(θ, W )

W

(cid:107)∇

L

W

− ∇

L

(θ, ˜W )

W
L
(cid:107)

(cid:107) ≤

˜W
.
(cid:107)
−
(C.17)

Assumption 3 There exists a constant ρ such that

θt

2 + E
W t
(cid:107)

2
(cid:107)

(cid:107)

(cid:107)

≤

ρ2 for all t.

Lemma 2 Under Assumptions 2 and 3,

θt

2

(cid:107)

≤

2
gt
ρ2, E
W (cid:107)
(cid:107)

M 2

ρ , E
(cid:107)

2
gt
θ(cid:107)

≤

M 2
ρ

t,

∀

W t
E
(cid:107)

2
(cid:107)
ρ = 4L2ρ2 + 2 max

ρ2, E
(cid:107)

≤

where M 2

{∇
Proof The inequalities for E
θ
(cid:107)
E
(cid:107)

gt
W (cid:107)

L

(θ0, W 0),

θ

W

∇

2 and E
(cid:107)

(cid:107)

W

2 are merely corollaries from Assumption 2.

≤
(θ0, W 0)

.
}

L
2 are trivial, and the ones for E
gt
θ(cid:107)
(cid:107)

(cid:107)

2 and

Negative log marginal likelihood of a Gaussian process with a structured kernel is a
nonconvex function of its arguments. Therefore, we can only show that the algorithm
converges to a stationary point, i.e., a point at which the gradient of the objective is zero.

Theorem 1 Let
for θ and W being λt
scalars, such that

{

(θt, W t)
}
θαt and λt
θ = ct

be a sequence generated from Algorithm 1 with learning rates
W , αt are positive

W αt, respectively, where ct

W = ct

θ, ct

0 < inf
t

ct
{θ,W } ≤

sup
t

ct
{θ,W } <

∞

, αt

αt+1,

≥

αt = +

,
∞

∞
(cid:88)

t=1

∞
(cid:88)

t=1

α2

t < +

,
∞

Then, under Assumptions 1 through 3 and if σ = supt σt <

, we have

∞

t.

∀
(C.18)

(C.19)

E

lim
t→∞

(cid:107)∇L

(θt, W t)

= 0.

(cid:107)

27

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Proof The proof is an adaptation of the one given by Xu and Yin (2015) with the following
three adjustments: (1) we have only two blocks of coordinates and (2) updates for θ are
deterministic which zeros out their variance terms, (3) the stochastic gradients are unbiased.

From the Lipschitz continuity of

(Assumption 2), we have:

W

∇

L

and

θ

∇

L

•

(θt+1, W t+1)

(θt+1, W t)

L
W , W t+1
gt

≤ (cid:104)

=

=

=

λt
W (cid:104)
(cid:18)
λt
W −

−

−

−
W , ˜gt
gt
W (cid:105)
L
(λt
2
L
2
L(λt
L
2

λt
W −

W −
λt
W −
(cid:0)λt

λt
θ

(cid:18)

−

−

(cid:0)λt
(cid:18)

≤ −

+

L
2

W t+1

W t

2

(cid:107)

−
2

− L
W t

(cid:105)

+

+

L
2 (cid:107)
L
W )2
(λt
2
(cid:19)
W )2

gt
W (cid:107)
(cid:107)

(cid:19)

(cid:107)
2 +

˜gt
W (cid:107)
L
2
L
2

W

(λt
W )2
W )2(cid:1) (cid:0)
(cid:104)
(cid:19)
W )2
(λt

2 +

gt
W (cid:107)
(cid:107)
gt
W − ∇
2 +
gt
W (cid:107)
(cid:107)
W )2(cid:1) (

δt
W (cid:107)
(cid:107)

W + L(λt

(λt

W )2

2
δt
W (cid:107)
(cid:107)
(θt, W t), δt

L
L
(λt
2

W )2

W (cid:105)
2
δt
W (cid:107)

(cid:107)

−

2 +

2),

gt
θ(cid:107)
(cid:107)

(λt

W )2

2
δt
W (cid:107)

(cid:107)

−

(cid:0)λt

W −

L(λt

W )2(cid:1)

W , δt
gt
(cid:104)

W (cid:105)

W

L

+

(cid:104)∇
(cid:0)λt

W −

(cid:1)

W (cid:105)

(θt, W t), δt
W )2(cid:1)

L(λt

W

(cid:104)∇

L

(θt, W t), δt

W (cid:105)

where the last inequality comes from the following (note that gt

W :=

W

∇

L

(θt+1, W t)):

W

(cid:0)λt

L(λt

W −

(θt+1, W t)

−
(cid:12)
(cid:12)λt
W −
(cid:12)
(cid:12)λt
Lλt
W −
θ
L
(cid:0)λt
W + L(λt
2

W )2(cid:1)
L(λt
L
(cid:104)∇
W )2(cid:12)
δt
(θt+1, W t)
(cid:12)
W
W (cid:107)(cid:107)∇
L
(cid:107)
W )2(cid:12)
gt
δt
L(λt
(cid:12)
W (cid:107)(cid:107)
θ(cid:107)
(cid:107)
W )2(cid:1) (
2 +
δt
W (cid:107)
(cid:107)

gt
θ(cid:107)

λt
θ

(cid:107)

2).

≤

≤

≤

W

− ∇

(θt, W t), δt
W (cid:105)
(θt, W t)
(cid:107)

L

L
W

− ∇

Analogously, we derive a bound on

(θt+1, W t)

(θt, W t):

L

− L

L

(θt+1, W t)
(cid:18)
L
2

λt
θ −

− L
θ)2
(λt

(θt, W t)
(cid:19)

2 +

gt
θ(cid:107)

(cid:107)

L
2

≤ −

(λt

θ)2

2
δt
θ(cid:107)

(cid:107)

−

(cid:0)λt

θ −

L(λt

θ)2(cid:1)

θ
(cid:104)∇

L

(θt, W t), δt
θ(cid:105)

Since (θt, W t) is independent from the current mini-batch, xt, using Lemma 1, we
have that E

(θt, W t), δt

= 0 and E

= 0.

•

θ
(cid:104)∇

L

(θt, W t), δt
θ(cid:105)

W

(cid:104)∇

L

W (cid:105)

28

Learning Scalable Deep Kernels with Recurrent Structure

Summing the two obtained inequalities and applying Assumption 1, we get:

≤ −

≤ −

E[

−

−

L
(cid:18)

(cid:18)

(cid:18)

(cid:18)

W )2
(cid:19)

(λt

(λt

λt
θ −

(θt+1, W t+1)
L
λt
W −
2
L
2
L
2
L
2

θ)2

cαt

cαt

−

−

C2α2
t

C2α2
t

(cid:19)

(cid:19)

(θt, W t)]

− L
(cid:19)
E
(cid:107)

2 +

gt
W (cid:107)

L
2

E
(cid:107)

gt
θ(cid:107)

2 +

L
2

(λt

θ)2σ

gt
E
θ(cid:107)
(cid:107)

2 +

L
2

C2α2

t σ,

(λt

W )2σ +

λt
θ

(cid:0)λt

W + L(λt

W )2(cid:1) (σ + E
gt
θ(cid:107)
(cid:107)

2)

L
2

gt
E
W (cid:107)
(cid:107)

2 + LC2σαt +

C2α2

t (1 + LCαt) (σ + E
(cid:107)

gt
θ(cid:107)

2)

L
2

inf t ct
W , inf t ct
where we denoted c = min
θ}
{
from Lemma 2, we also have E
ρ and E
M 2
2
gt
θ(cid:107)
≤
(cid:107)
(cid:107)
the right hand side of the ﬁnal inequality over t and using (C.18), we have:

supt ct
{
M 2
2
gt
W (cid:107)

W , supt ct
. Note that
ρ . Therefore, summing

, C = max

θ}

≤

lim
t→∞

L

E

(θt+1, W t+1)

E

(θ0, W 0)

−

L

αt

(cid:0)E
(cid:107)

gt
θ(cid:107)

gt
2 + E
W (cid:107)
(cid:107)

2(cid:1) .

(C.20)

c
≤ −

∞
(cid:88)

t=1

Since the objective function is lower bounded, this eﬀectively means:

∞
(cid:88)

t=1

αtE
(cid:107)

gt
θ(cid:107)

2 <

,
∞

gt
αtE
W (cid:107)
(cid:107)

2 <

.
∞

∞
(cid:88)

t=1

Finally, using Lemma 2, our assumptions and Jensen’s inequality, it follows that

•

(cid:12)
gt+1
2
(cid:12)E
W (cid:107)
(cid:107)

gt
E
W (cid:107)
(cid:107)

−

2(cid:12)
(cid:12)

≤

(cid:113)

2LMρCαt

2(M 2

ρ + σ2).

According to Proposition 1.2.4 of (Bertsekas, 1999), we have E
gt
θ(cid:107)
(cid:107)
as t

, and hence

2

0 and E
(cid:107)

gt
W (cid:107)

2

→

→

0

→ ∞

E

(θt, W t)

(cid:107)∇L

(cid:107) ≤

E

(cid:107)∇
2LC

W
(cid:113)

L
2(M 2

(θt, W t)

gt
gt
+ E
θ(cid:107)
θ(cid:107)
−
(cid:107)
(cid:107)∇
L
gt
gt
+ E
ρ + σ2)α + E
W (cid:107) →
θ(cid:107)
(cid:107)
(cid:107)

+ E

W

≤

0 as t

,
→ ∞

(θt, W t)

gt
θ(cid:107)

+ E
(cid:107)

gt
W (cid:107)

−

where the ﬁrst term of the last inequality follows from Lemma 2 and Jensen’s inequality.

29

Al-Shedivat, Wilson, Saatchi, Hu, Xing

C.2 Semi-stochastic gradient with delayed kernel matrix updates

We show that given a bounded delay on the kernel matrix updates, the algorithm is still
convergent. Our analysis is based on computing the change in δt
W and applying the
same argument as in Theorem 1. The only diﬀerence is that we need to take into account
the perturbations of the kernel matrix due to the introduced delays, and hence we have to
impose certain assumptions on its spectrum.

θ and δt

Assumption 4 Recurrent transformations, φW (¯x), is L-Lipschitz w.r.t. W for all ¯x

L:

∈ X

φ ˜W (¯x)

(cid:107)

−

φW (¯x)

(cid:107) ≤

˜W

L
(cid:107)

W

.

(cid:107)

−

Assumption 5 The kernel function, k(
derivatives are uniformly J-Lipschitz:

,
·

·

), is uniformly G-Lipschitz and its ﬁrst partial

(cid:13)
(cid:13)k(˜h1, h2)
(cid:13)

k(h1, h2)

−
(cid:13)
∂1k(˜h1, h2)
(cid:13)
(cid:13) ≤

−

(cid:13)
(cid:13)
(cid:13) ≤
J

(cid:107)

G

˜h1
(cid:107)

˜h1

−

,

(cid:13)
(cid:13)k(h1, ˜h2)
(cid:13)
h1
(cid:107)
(cid:13)
(cid:13)
(cid:13)∂2k(h1, h2)

−

k(h1, h2)

(cid:13)
(cid:13)
(cid:13) ≤
−
(cid:13)
∂2k(h1, ˜h2)
(cid:13)
(cid:13) ≤

G

J

˜h2
(cid:107)
˜h2
(cid:107)

−
h1

,

(cid:107)

h2

h2

,

.

(cid:107)

(cid:107)

−

−

(cid:13)
(cid:13)
(cid:13)∂1k(h1, h2)

Assumption 6 For any collection of data representations,
of the corresponding kernel matrix, K, is lower bounded by a positive constant γ > 0.

N
i=1, the smallest eigenvalue
}

hi

{

Note that not only the assumptions are relevant to practice, Assumptions 5 and 6 can be
also controlled by choosing the class of kernel functions used in the model. For example, the
smallest eigenvalue of the kernel matrix, K, can be controlled by the smoothing properties
of the kernel (Rasmussen and Williams, 2006).

Consider a particular stochastic step of Algorithm 2 at time t for a given mini-batch, xt,
assuming that the kernel was last updated τ steps ago. The stochastic gradient will take
the following form:

ˆgt
W =

1
Nt ∇

W

L

(θt+1, W t, Kt−τ ) =

(cid:88)

(cid:16)

1
2Nt

K−1

t−τ yy(cid:62)K−1

K−1
t−τ

t−τ −

(cid:17)

ij

(cid:18) ∂Kt−τ
∂W

(cid:19)

.

ij
(C.21)
We can deﬁne ˆδt
in order to enable the same
W −
argument as in Theorem 1. To do that, we simply need to understand eﬀect of perturbation
of the kernel matrix on ˜gt

gt
W and uniformly bound

ˆδt
W −

W = ˆgt

δt
W (cid:107)

i,j∈It

(cid:107)

W .

Lemma 3 Under the given assumptions, the following bound holds for all i, j = 1, . . . , N :

(cid:12)
(cid:16)
(cid:12)
(cid:12)
(cid:12)

K−1

t−τ yy(cid:62)K−1

t−τ −

(cid:17)

K−1
t−τ

(cid:16)

K−1

t yy(cid:62)K−1

K−1
t

t −

ij −

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤

ij

D2

y

(cid:107)

(cid:107)

2 + D,

(C.22)

where D = γ−1 +

2GLτ λσ√N

(cid:17)−1

.

(cid:16)

γ

−

30

Learning Scalable Deep Kernels with Recurrent Structure

Proof The diﬀerence between K−1
t
computed for W t−τ and the latter for W t. To prove the bound, we need multiple steps.

is simply due to that the former has been

t−τ and K−1

•

First, we need to bound the element-wise diﬀerence between Kt−τ and Kt. This is
done by using Assumptions 4 and 5 and the triangular inequality:

(Kt)ij
|

−

(Kt−τ )ij

φW t−τ (¯xi)

+

φW t(¯xj)

(cid:107)

(cid:107)

φW t−τ (¯xj)
(cid:107)

)

−

| ≤

≤

G (

φW t(¯xi)
(cid:107)
W t
2GL
τ
(cid:88)

(cid:107)

−
W t−τ

−
λt−τ +s
W

(cid:107)
ˆgt−τ +s
W

(cid:107)

= 2GL

(cid:107)

s=1
2GLτ λσ

≤

•

Next, since each element of the perturbed matrix is bounded by 2GLτ λσ, we can
bound its spectral norm as follows:

Kt

(cid:107)

−

Kt−τ

Kt

Kt−τ

(cid:107) ≤ (cid:107)

−

F
(cid:107)

≤

2GLτ λσ√N ,

which means that the minimal singular value of the perturbed matrix is at least
σ1 = γ

2GLτ λσ√N due to Assumption 6.

−

•

The spectral norm of the expression of interest can be bounded (quite pessimistically!)
by summing up together the largest eigenvalues of the matrix inverses:

t−τ yy(cid:62)K−1

K−1
t−τ

t−τ −
(cid:1) yy(cid:62) (cid:0)K−1

K−1
t

(cid:16)

K−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:0)K−1
(cid:13)
(cid:13)
(cid:18)

t−τ −
(cid:16)

t−τ −

K−1
t
(cid:17)−1(cid:19)2

γ−1 +

γ

2GLτ λσ√N

−

y

(cid:107)

(cid:107)

≤

≤

(cid:17)

(cid:16)

ij −

K−1

t yy(cid:62)K−1

t −

(cid:17)

K−1
t

(cid:1)

(cid:0)K−1

t−τ −

−

K−1
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ij
(cid:1)(cid:13)
(cid:13)
(cid:13)

−

2 + γ−1 +

(cid:16)

γ

2GLτ λσ√N

(cid:17)−1

.

Each element of a matrix is bounded by the largest eigenvalue.

Using Lemma 3, it is straightforward to extend Theorem 1 to Algorithm 2.

Theorem 2 Let
for θ and W being λt
scalars, such that

{

(θt, W t)
}
θαt and λt
θ = ct

be a sequence generated from Algorithm 2 with learning rates
W , αt are positive

W αt, respectively, where ct

W = ct

θ, ct

0 < inf
t

ct
{θ,W } ≤

sup
t

ct
{θ,W } <

∞

, αt

αt+1,

≥

αt = +

,
∞

∞
(cid:88)

t=1

∞
(cid:88)

t=1

α2

t < +

,
∞

Then, under Assumptions 1 through 6 and if σ = supt σt <

, we have

t.

∀
(C.23)

(C.24)

E

lim
t→∞

(cid:107)∇L

(θt, W t)

= 0.

(cid:107)

∞

31

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Proof The proof is identical to the proof of Theorem 1. The only diﬀerence is in the
following upper bound on the expected gradient error: E
2 + 1)Jρ,
(cid:107)
(cid:107)
where D is as given in Lemma 3.

δt
W (cid:107) ≤

σ + 2N D(D

(cid:107)

y

Remark 1 Even though the provided bounds are crude due to pessimistic estimates of the
perturbed kernel matrix spectrum15, we still see a ﬁne balance between the delay, τ , and the
learning rate, λ, as given in the expression for the D constant.

Appendix D. Details on the datasets

The datasets varied in the number of time steps (from hundreds to a million), input and
output dimensionality, and the nature of the estimation tasks.

D.1 Self-driving car

The following is description of the input and target time series used in each of the autonomous
driving tasks (dimensionality is given in parenthesis).

Car speed estimation:

– Features: GPS velocity (3), ﬁber gyroscope (3).

– Targets: speed measurements from the car speedometer.

Car yaw estimation:

– Features: acceleration (3), compass measurements (3).

– Targets: yaw in the car-centric frame.

•

•

•

Lane sequence prediction: Each lane was represented by 8 cubic polynomial
coeﬃcients [4 coeﬃcients for x (front) and 4 coeﬃcients for y (left) axes in the car-
centric frame]. Instead of predicting the coeﬃcients (which turned out to lead to
overall less stable results), we discretized the lane curves using 7 points (initial, ﬁnal
and 5 equidistant intermediate points).

– Features: lanes at a previous time point (16), GPS velocity (3), ﬁber gyroscope

(3), compass (3), steering angle (1).

– Targets: coordinates of the lane discretization points (7 points per lane resulting

in 28 total output dimensions).

15. Tighter bounds can be derived by inspecting the eﬀects of perturbations on speciﬁc kernels as well as

using more speciﬁc assumptions about the data distribution.

32

Learning Scalable Deep Kernels with Recurrent Structure

Table 5: Summary of the feedforward and recurrent neural architectures and the corresponding
hyperparameters used in the experiments. GP-based models used the same architectures as their
non-GP counterparts. Activations are given for the hidden units; vanilla neural nets used linear
output activations.

Name Data

Time lag Layers Units∗ Type Regularizer∗∗

Optimizer

NARX

RNN

LSTM

Actuator
Drives
GEF-power
GEF-wind
Car

Actuator
Drives
GEF-power
GEF-wind
Car

Actuator
Drives
GEF-power
GEF-wind
Car

32
16
48
48
128

32
16
48
48
128

32
16
48
48
128

1
1
1
1
1

1
1
1
1
1

1
1
2
1
2

256
128
256
16
128

64
64
16
32
128

256
128
256
64
64

ReLU

dropout(0.5)

Adam(0.01)

tanh

dropout(0.25),
rec_dropout(0.05)

Adam(0.01)

tanh

dropout(0.25),
rec_dropout(0.05)

Adam(0.01)

∗Each layer consisted of the same number of units given in the table.
∗∗rec_dropout denotes the dropout rate of the recurrent weights (Gal and Ghahramani, 2016b).

•

Estimation of the nearest front vehicle position: (x, y) coordinates in the
car-centric frame.

– Features: x and y at a previous time point (2), GPS velocity (3), ﬁber gyroscope

(3), compass (3), steering angle (1).

– Targets: x and y coordinates.

Appendix E. Neural architectures

Details on the best neural architectures used for each of the datasets are given in Table 5.

33

Al-Shedivat, Wilson, Saatchi, Hu, Xing

References

Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances

in Neural Information Processing Systems, pages 873–881, 2011.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with
gradient descent is diﬃcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.

Dimitri P Bertsekas. Nonlinear programming. Athena scientiﬁc Belmont, 1999.

George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series

analysis: forecasting and control. John Wiley & Sons, 1994.

Roberto Calandra, Jan Peters, Carl Edward Rasmussen, and Marc Peter Deisenroth. Mani-
fold gaussian processes for regression. In Neural Networks (IJCNN), 2016 International
Joint Conference on, pages 3338–3345. IEEE, 2016.

Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo.

In International Conference on Machine Learning, pages 1683–1691, 2014.

Andreas C Damianou and Neil D Lawrence. Deep gaussian processes. In AISTATS, pages

207–215, 2013.

Marc Peter Deisenroth and Jun Wei Ng. Distributed gaussian processes. In International

Conference on Machine Learning (ICML), volume 2, page 5, 2015.

Roger Frigola, Yutian Chen, and Carl Rasmussen. Variational gaussian process state-space
models. In Advances in Neural Information Processing Systems, pages 3680–3688, 2014.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing
model uncertainty in deep learning. In Proceedings of the 33rd International Conference
on Machine Learning, pages 1050–1059, 2016a.

Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in
recurrent neural networks. In Advances in Neural Information Processing Systems, pages
1019–1027, 2016b.

Alan Graves, Abdel-rahman Mohamed, and Geoﬀrey Hinton. Speech recognition with deep
recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013
IEEE International Conference on, pages 6645–6649. IEEE, 2013.

Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability
of stochastic gradient descent. In Proceedings of The 33rd International Conference on
Machine Learning, pages 1225–1234, 2016.

34

Learning Scalable Deep Kernels with Recurrent Structure

James Hensman, Nicolò Fusi, and Neil D Lawrence. Gaussian processes for big data. In
Proceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence,
pages 282–290. AUAI Press, 2013.

Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Using deep belief nets to learn covariance
kernels for gaussian processes. In Advances in neural information processing systems,
pages 1249–1256, 2008.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9

(8):1735–1780, 1997.

Tommi S Jaakkola and David Haussler. Exploiting generative models in discriminative
classiﬁers. Advances in neural information processing systems, pages 487–493, 1999.

Robert Keys. Cubic convolution interpolation for digital image processing. IEEE transactions

on acoustics, speech, and signal processing, 29(6):1153–1160, 1981.

Juš Kocijan, Agathe Girard, Blaž Banko, and Roderick Murray-Smith. Dynamic systems
identiﬁcation with gaussian processes. Mathematical and Computer Modelling of Dynamical
Systems, 11(4):411–424, 2005.

John Langford, Alex J Smola, and Martin Zinkevich. Slow learners are fast. Advances in

Neural Information Processing Systems, 22:2331–2339, 2009.

Miguel Lázaro-Gredilla. Bayesian warped gaussian processes.

In Advances in Neural

Information Processing Systems, pages 1619–1627, 2012.

Tsungnam Lin, Bil G Horne, Peter Tiňo, and C Lee Giles. Learning long-term dependencies in
narx recurrent neural networks. Neural Networks, IEEE Transactions on, 7(6):1329–1338,
1996.

David JC MacKay. Introduction to gaussian processes. NATO ASI Series F Computer and

Systems Sciences, 168:133–166, 1998.

César Lincoln C Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth, Guilherme A Bar-
reto, and Neil D Lawrence. Recurrent gaussian processes. arXiv preprint arXiv:1511.06644,
2015.

Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. The Journal of

Machine Learning Research, 7:2651–2667, 2006.

Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse
approximate gaussian process regression. The Journal of Machine Learning Research, 6:
1939–1959, 2005.

35

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Carl Edward Rasmussen and Zoubin Ghahramani. Occam’s razor. Advances in neural

information processing systems, pages 294–300, 2001.

Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine

learning. The MIT Press, 2006.

Stéphane Ross, Geoﬀrey J Gordon, and Drew Bagnell. A reduction of imitation learning
and structured prediction to no-regret online learning. In AISTATS, volume 1, page 6,
2011.

Yunus Saatchi and Andrew Gordon Wilson. Bayesian gan. arXiv preprint arXiv:1705.09558,

2017.

Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines,

regularization, optimization, and beyond. MIT press, 2002.

Jonas Sjöberg, Qinghua Zhang, Lennart Ljung, Albert Benveniste, Bernard Delyon, Pierre-
Yves Glorennec, Håkan Hjalmarsson, and Anatoli Juditsky. Nonlinear black-box modeling
in system identiﬁcation: a uniﬁed overview. Automatica, 31(12):1691–1724, 1995.

Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped gaussian

processes. In NIPS, pages 337–344, 2003.

Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958, 2014.

ML Stein. Interpolation of spatial data, 1999.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural
networks. In Advances in neural information processing systems, pages 3104–3112, 2014.

Ryan D Turner, Marc P Deisenroth, and Carl E Rasmussen. State-space inference and
learning with gaussian processes. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 868–875, 2010.

Peter Van Overschee and Bart De Moor. Subspace identiﬁcation for linear systems: Theory

— Implementation — Applications. Springer Science & Business Media, 2012.

Jack Wang, Aaron Hertzmann, and David M Blei. Gaussian process dynamical models. In

Advances in neural information processing systems, pages 1441–1448, 2005.

Torbjörn Wigren. Input-output data sets for development and benchmarking in nonlinear
identiﬁcation. Technical Reports from the department of Information Technology, 20:
2010–020, 2010.

36

Learning Scalable Deep Kernels with Recurrent Structure

Andrew Wilson and Zoubin Ghahramani. Copula processes. In Advances in Neural Infor-

mation Processing Systems, pages 2460–2468, 2010.

Andrew Gordon Wilson and Ryan Prescott Adams. Gaussian process kernels for pattern
discovery and extrapolation. In Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pages 1067–1075, 2013.

Andrew Gordon Wilson and Hannes Nickisch. Kernel interpolation for scalable structured
gaussian processes (kiss-gp). In Proceedings of The 32nd International Conference on
Machine Learning, pages 1775–1784, 2015.

Andrew Gordon Wilson, Christoph Dann, and Hannes Nickisch. Thoughts on massively

scalable gaussian processes. arXiv preprint arXiv:1511.01870, 2015.

Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel
learning. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence
and Statistics, 2016a.

Andrew Gordon Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic
variational deep kernel learning. In Advances in Neural Information Processing Systems,
pages 2586–2594, 2016b.

D Randall Wilson and Tony R Martinez. The general ineﬃciency of batch training for

gradient descent learning. Neural Networks, 16(10):1429–1451, 2003.

Yangyang Xu and Wotao Yin. Block stochastic gradient iteration for convex and nonconvex

optimization. SIAM Journal on Optimization, 25(3):1686–1716, 2015.

37

Journal of Machine Learning Research 18 (2017) 1-37

Submitted 10/16; Revised 6/17; Published 8/17

Learning Scalable Deep Kernels with Recurrent Structure

Maruan Al-Shedivat
Carnegie Mellon University

Andrew Gordon Wilson
Cornell University

Yunus Saatchi

Zhiting Hu
Carnegie Mellon University

Eric P. Xing
Carnegie Mellon University

Editor: Neil Lawrence

alshedivat@cs.cmu.edu

andrew@cornell.edu

saatchi@cantab.net

zhitingh@cs.cmu.edu

epxing@cs.cmu.edu

Abstract
Many applications in speech, robotics, ﬁnance, and biology deal with sequential data, where
ordering matters and recurrent structures are common. However, this structure cannot
be easily captured by standard kernel functions. To model such structure, we propose
expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-
LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent
networks, while retaining the non-parametric probabilistic advantages of Gaussian processes.
We learn the properties of the proposed kernels by optimizing the Gaussian process marginal
likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit
the structure of these kernels for scalable training and prediction. This approach provides a
practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance
on several benchmarks, and thoroughly investigate a consequential autonomous driving
application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.

1. Introduction

There exists a vast array of machine learning applications where the underlying datasets
are sequential. Applications range from the entirety of robotics, to speech, audio and video
processing. While neural network based approaches have dealt with the issue of representation
learning for sequential data, the important question of modeling and propagating uncertainty
across time has rarely been addressed by these models. For a robotics application such as a
self-driving car, however, it is not just desirable, but essential to have complete predictive
densities for variables of interest. When trying to stay in lane and keep a safe following
distance from the vehicle front, knowing the uncertainty associated with lanes and lead
vehicles is as important as the point estimates.

c(cid:13)2017 Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, Eric P. Xing.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v18/16-498.html.

7
1
0
2
 
t
c
O
 
5
 
 
]

G
L
.
s
c
[
 
 
3
v
6
3
9
8
0
.
0
1
6
1
:
v
i
X
r
a

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Recurrent models with long short-term memory (LSTM) (Hochreiter and Schmidhuber,
1997) have recently emerged as the leading approach to modeling sequential structure. The
LSTM is an eﬃcient gradient-based method for training recurrent networks. LSTMs use
a memory cell inside each hidden unit and a special gating mechanism that stabilizes the
ﬂow of the back-propagated errors, improving the learning process of the model. While
the LSTM provides state-of-the-art results on speech and text data (Graves et al., 2013;
Sutskever et al., 2014), quantifying uncertainty or extracting full predictive distributions
from deep models is still an area of active research (Gal and Ghahramani, 2016a).

In this paper, we quantify the predictive uncertainty of deep models by following a
Bayesian nonparametric approach. In particular, we propose kernel functions which fully
encapsulate the structural properties of LSTMs, for use with Gaussian processes. The
resulting model enables Gaussian processes to achieve state-of-the-art performance on se-
quential regression tasks, while also allowing for a principled representation of uncertainty
and non-parametric ﬂexibility. Further, we develop a provably convergent semi-stochastic op-
timization algorithm that allows mini-batch updates of the recurrent kernels. We empirically
demonstrate that this semi-stochastic approach signiﬁcantly improves upon the standard
non-stochastic ﬁrst-order methods in runtime and in the quality of the converged solution.
For additional scalability, we exploit the algebraic structure of these kernels, decomposing
the relevant covariance matrices into Kronecker products of circulant matrices, for
(n)
training time and
(1) test predictions (Wilson et al., 2015; Wilson and Nickisch, 2015).
Our model not only can be interpreted as a Gaussian process with a recurrent kernel, but
also as a deep recurrent network with probabilistic outputs, inﬁnitely many hidden units,
and a utility function robust to overﬁtting.

O

O

Throughout this paper, we assume basic familiarity with Gaussian processes (GPs).
We provide a brief introduction to GPs in the background section; for a comprehensive
reference, see, e.g., Rasmussen and Williams (2006). In the following sections, we formalize
the problem of learning from sequential data, provide background on recurrent networks
and the LSTM, and present an extensive empirical evaluation of our model. Speciﬁcally, we
apply our model to a number of tasks, including system identiﬁcation, energy forecasting,
and self-driving car applications. Quantitatively, the model is assessed on the data ranging
in size from hundreds of points to almost a million with various signal-to-noise ratios
demonstrating state-of-the-art performance and linear scaling of our approach. Qualitatively,
the model is tested on consequential self-driving applications:
lane estimation and lead
vehicle position prediction. Indeed, the main focus of this paper is on achieving state-
of-the-art performance on consequential applications involving sequential data, following
straightforward and scalable approaches to building highly ﬂexible Gaussian process.

We release our code as a library at: http://github.com/alshedivat/keras-gp. This
library implements the ideas in this paper as well as deep kernel learning (Wilson et al.,
2016a) via a Gaussian process layer that can be added to arbitrary deep architectures
and deep learning frameworks, following the Keras API speciﬁcation. More tutorials and
resources can be found at https://people.orie.cornell.edu/andrew/code.

2

Learning Scalable Deep Kernels with Recurrent Structure

2. Background

L

∈

X

X

· · ·

yi
{

xi
{

i , x2
i ,

i ∈ X

n
i=1, yi
}

i=1 be a collection of sequences, xi = [x1
n
}

We consider the problem of learning a regression function that maps sequences to real-valued
, xli],
target vectors. Formally, let X =
each with corresponding length, li, where xj
is an arbitrary domain. Let
, and
Rd, be a collection of the corresponding real-valued target vectors.
y =
Assuming that only the most recent L steps of a sequence are predictive of the targets, the
Rd, from some family,
, based on the available data.
goal is to learn a function, f :
As a working example, consider the problem of estimating position of the lead vehicle at
the next time step from LIDAR, GPS, and gyroscopic measurements of a self-driving car
available for a number of previous steps. This task is a classical instance of the sequence-
to-reals regression, where a temporal sequence of measurements is regressed to the future
position estimates. In our notation, the sequences of inputs are vectors of measurements,
xn], are indexed by time and would be of
x1 = [x1], x2 = [x1, x2], . . . , xn = [x1, x2,
· · ·
growing lengths. Typically, input sequences are considered up to a ﬁnite-time horizon, L,
that is assumed to be predictive for the future targets of interest. The targets, y1, y2, . . . , yn,
are two-dimensional vectors that encode positions of the lead vehicle in the ego-centric
coordinate system of the self-driving car.

(cid:55)→

F

Note that the problem of learning a mapping, f :

Rd, is challenging. While
considering whole sequences of observations as input features is necessary for capturing
long-term temporal correlations, it virtually blows up the dimensionality of the problem. If
Rp, and consider L previous
we assume that each measurement is p-dimensional, i.e.,
X ⊆
steps as distinct features, the regression problem will become (L
p)-dimensional. Therefore,
to avoid overﬁtting and be able to extract meaningful signal from a ﬁnite amount of data, it
is crucial to exploit the sequential nature of observations.

(cid:55)→

×

X

L

Recurrent models. One of the most successful ways to exploit sequential structure of
the data is by using a class of recurrent models. In the sequence-to-reals regression scenario,
Rd in the following general recurrent form:
such a model expresses the mapping f :

L

(cid:55)→
y = ψ(hL) + (cid:15)t, ht = φ(ht−1, xt) + δt, t = 1, . . . , L,

X

(1)

where xt is an input observation at time t, ht is a corresponding latent representation,
) specify model transitions and emissions,
) and ψ(
and y is a target vector. Functions φ(
·
·
respectively, and δt and (cid:15)t are additive noises. While φ(
) can be arbitrary, they
) and ψ(
·
·
are typically time-invariant. This strong but realistic assumption incorporated into the
structure of the recurrent mapping signiﬁcantly reduces the complexity of the family of
functions,

, regularizes the problem, and helps to avoid severe overﬁtting.

Recurrent models can account for various patterns in sequences by memorizing internal
representations of their dynamics via adjusting φ and ψ. Recurrent neural networks (RNNs)
model recurrent processes by using linear parametric maps followed by nonlinear activations:

F

y = ψ(W (cid:62)

hyht−1), ht = φ(W (cid:62)

hhht−1, W (cid:62)

xhxt−1), t = 1, . . . , L,

(2)

3

Al-Shedivat, Wilson, Saatchi, Hu, Xing

where Why, Whh, Wxh are weight matrices to be learned1 and φ(
) here are some
·
·
ﬁxed element-wise functions. Importantly and contrary to the standard hidden Markov
models (HMMs), the state of an RNN at any time t is distributed and eﬀectively represented
by an entire hidden sequence, [h1,
, ht−1, ht]. A major disadvantage of the vanilla RNNs
is that their training is nontrivial due to the so-called vanishing gradient problem (Bengio
et al., 1994): the error back-propagated through t time steps diminishes exponentially which
makes learning long-term relationships nearly impossible.

) and ψ(

· · ·

LSTM. To overcome vanishing gradients, Hochreiter and Schmidhuber (1997) proposed
a long short-term memory (LSTM) mechanism that places a memory cell into each hidden
unit and uses diﬀerentiable gating variables. The update rules for the hidden representation
at time t have the following form (here σ (
) are element-wise sigmoid and
·
hyperbolic tangent functions, respectively):

) and tanh (
·

(cid:16)

it = tanh

W (cid:62)

xcxt + W (cid:62)

hcht−1 + bc

(cid:17)

,

hiht−1 + W (cid:62)

ci ct−1 + bi

(cid:16)

W (cid:62)

gt
i = σ
ct = gt
c ct−1 + gt
(cid:16)
W (cid:62)

xixt + W (cid:62)
i it,
xf xt + W (cid:62)

(cid:17)

,

(cid:17)

(3)

,

xoxt + W (cid:62)

cf ct−1 + bf

hf ht−1 + W (cid:62)

gt
c = σ
ot = tanh (cid:0)ct(cid:1) ,
(cid:16)
gt
o = σ
ht = gt
As illustrated above, gt
o correspond to the input, forget, and output gates,
respectively. These variables take their values in [0, 1] and when combined with the internal
states, ct, and inputs, xt, in a multiplicative fashion, they play the role of soft gating. The
gating mechanism not only improves the ﬂow of errors through time, but also, allows the
the network to decide whether to keep, erase, or overwrite certain memorized information
based on the forward ﬂow of inputs and the backward ﬂow of errors. This mechanism adds
stability to the network’s memory.

W (cid:62)
o ot.

hoht−1 + W (cid:62)

c, and gt

coct + bo

i, gt

(cid:17)

,

Gaussian processes. The Gaussian process (GP) is a Bayesian nonparametric model
that generalizes the Gaussian distributions to functions. We say that a random function
f is drawn from a GP with a mean function µ and a covariance kernel k, f
(µ, k),
if for any vector of inputs, [x1, x2, . . . , xn], the corresponding vector of function values is
Gaussian:

∼ GP

[f (x1), f (x2), . . . , f (xn)]

(µ, KX,X ) ,

∼ N

with mean µ, such that µi = µ(xi), and covariance matrix KX,X that satisﬁes (KX,X )ij =
k(xi, xj). GPs can be seen as distributions over the reproducing kernel Hilbert space (RKHS)
of functions which is uniquely deﬁned by the kernel function, k (Schölkopf and Smola, 2002).

1. The bias terms are omitted for clarity of presentation.

4

Learning Scalable Deep Kernels with Recurrent Structure

GPs with RBF kernels are known to be universal approximators with prior support to within
an arbitrarily small epsilon band of any continuous function (Micchelli et al., 2006).
x

(cid:0)f (x), σ2(cid:1), and a GP prior on f (x), given
∼ N
training inputs x and training targets y, the predictive distribution of the GP evaluated at
an arbitrary test point x∗ is:

Assuming additive Gaussian noise, y

|

where

f∗

|

x∗, x, y, σ2

(E[f∗], Cov[f∗]) ,

∼ N

E[f∗] = µX∗ + KX∗,X [KX,X + σ2I]−1y,

Cov[f∗] = KX∗,X∗ −

KX∗,X [KX,X + σ2I]−1KX,X∗.

(4)

(5)

Here, KX∗,X , KX,X∗, KX,X , and KX∗,X∗ are matrices that consist of the covariance function,
k, evaluated at the corresponding points, x
X∗, and µX∗ is the mean function
∈
X∗. GPs are ﬁt to the data by optimizing the evidence—the marginal
evaluated at x∗
probability of the data given the model—with respect to kernel hyperparameters. The
evidence has the form:

X and x∗

∈

∈

log P (y

x) =

|

−

(cid:105)
(cid:104)
y(cid:62)(K + σ2I)−1y + log det(K + σ2I)

+ const,

(6)

where we use a shorthand K for KX,X , and K implicitly depends on the kernel hyperpa-
rameters. This objective function consists of a model ﬁt and a complexity penalty term that
results in an automatic Occam’s razor for realizable functions (Rasmussen and Ghahramani,
2001). By optimizing the evidence with respect to the kernel hyperparameters, we eﬀectively
learn the the structure of the space of functional relationships between the inputs and the
targets. For further details on Gaussian processes and relevant literature we refer interested
readers to the classical book by Rasmussen and Williams (2006).

Turning back to the problem of learning from sequential data, it seems natural to apply
the powerful GP machinery to modeling complicated relationships. However, GPs are
limited to learning only pairwise correlations between the inputs and are unable to account
for long-term dependencies, often dismissing complex temporal structures. Combining GPs
with recurrent models has potential to addresses this issue.

3. Related work

The problem of learning from sequential data, especially from temporal sequences, is well
known in the control and dynamical systems literature. Stochastic temporal processes
are usually described either with generative autoregressive models (AM) or with state-
space models (SSM) (Van Overschee and De Moor, 2012). The former approach includes
nonlinear auto-regressive models with exogenous inputs (NARX) that are constructed by
using, e.g., neural networks (Lin et al., 1996) or Gaussian processes (Kocijan et al., 2005).
The latter approach additionally introduces unobservable variables, the state, and constructs

5

Al-Shedivat, Wilson, Saatchi, Hu, Xing

autoregressive dynamics in the latent space. This construction allows to represent and
propagate uncertainty through time by explicitly modeling the signal (via the state evolution)
and the noise. Generative SSMs can be also used in conjunction with discriminative models
via the Fisher kernel (Jaakkola and Haussler, 1999).

Modeling time series with GPs is equivalent to using linear-Gaussian autoregressive or
SSM models (Box et al., 1994). Learning and inference are eﬃcient in such models, but they
are not designed to capture long-term dependencies or correlations beyond pairwise. Wang
et al. (2005) introduced GP-based state-space models (GP-SSM) that use GPs for transition
and/or observation functions. These models appear to be more general and ﬂexible as they
account for uncertainty in the state dynamics, though require complicated approximate
training and inference, which are hard to scale (Turner et al., 2010; Frigola et al., 2014).

Perhaps the most recent relevant work to our approach is recurrent Gaussian processes
(RGP) (Mattos et al., 2015). RGP extends the GP-SSM framework to regression on sequences
by using a recurrent architecture with GP-based activation functions. The structure of
the RGP model mimics the standard RNN, where every parametric layer is substituted
with a Gaussian process. This procedure allows one to propagate uncertainty throughout
the network for an additional cost. Inference is intractable in RGP, and eﬃcient training
requires a sophisticated approximation procedure, the so-called recurrent variational Bayes.
In addition, the authors have to turn to RNN-based approximation of the variational mean
functions to battle the growth of the number of variational parameters with the size of
data. While technically promising, RGP seems problematic from the application perspective,
especially in its implementation and scalability aspects.

Our model has several distinctions with prior work aiming to regress sequences to
reals. Firstly, one of our goals is to keep the model as simple as possible while being
able to represent and quantify predictive uncertainty. We maintain an analytical objective
function and refrain from complicated and diﬃcult-to-diagnose inference schemes. This
simplicity is achieved by giving up the idea of propagating signal through a chain GPs
connected in a recurrent fashion. Instead, we propose to directly learn kernels with recurrent
structure via joint optimization of a simple functional composition of a standard GP with
a recurrent model (e.g., LSTM), as described in detail in the following section. Similar
approaches have recently been explored and proved to be fruitful for non-recurrent deep
networks (Wilson et al., 2016a,b; Calandra et al., 2016). We remark that combinations of
GPs with nonlinear functions have also been considered in the past in a slightly diﬀerent
setting of warped regression targets (Snelson et al., 2003; Wilson and Ghahramani, 2010;
Lázaro-Gredilla, 2012). Additionally, uncertainty over the recurrent parts of our model is
represented via dropout, which is computationally cheap and turns out to be equivalent
to approximate Bayesian inference in a deep Gaussian process (Damianou and Lawrence,
2013) with particular intermediate kernels (Gal and Ghahramani, 2016a,b). Finally, one
can also view our model as a standalone ﬂexible Gaussian process, which leverages learning
techniques that scale to massive datasets (Wilson and Nickisch, 2015; Wilson et al., 2015).

6

Learning Scalable Deep Kernels with Recurrent Structure

y5

g3

h3
3

x5

y4

g2

h3
2

x4

x2

h2
3

x4

h1
3

x3

y3

h3

x3

h1

x1

h2

x2

(a)

y3

g1

h3
1

x3

h2
1

x2

h1
1

x1

h2
2

x3

h1
2

x2

x1

x1

(b)

y

g

(c)

φ

x

Figure 1: (a) Graphical representation of a recurrent model (RNN/LSTM) that maps an input
sequence to a target value in one-step-ahead prediction manner. Shaded variables are observable,
diamond variables denote deterministic dependence on the inputs. (b) Graphical model for GP-
LSTM with a time lag, L = 3, two training time points, t = 3 and t = 4, and a testing time point,
t = 5. Latent representations are mapped to the outputs through a Gaussian ﬁeld (denoted in red)
that globally correlates predictions. Dashed variables represent data instances unused at the given
time step. (c) Graphical representation of a GP with a kernel structured with a parametric map, φ.

4. Learning recurrent kernels

Gaussian processes with diﬀerent kernel functions correspond to diﬀerent structured proba-
bilistic models. For example, some special cases of the Matérn class of kernels induce models
with Markovian structure (Stein, 1999). To construct deep kernels with recurrent structure
we transform the original input space with an LSTM network and build a kernel directly in
the transformed space, as shown in Figure 1b.

In particular, let L φ :

L

. The decomposition of the kernel and the transformation, ˜k = k

be an arbitrary deterministic transformation of the
R be a real-valued kernel
φ, is deﬁned

. Next, let k :

(cid:55)→

H

H

2

X

(cid:55)→ H

input sequences into some latent space,
deﬁned on
as

H

˜k(x, x(cid:48)) = k(φ(x), φ(x(cid:48))), where x, x(cid:48)

L, and ˜k : (cid:0)

L(cid:1)2

X

(cid:55)→

∈ X

(7)

◦

R.

It is trivial to show that ˜k(x, x(cid:48)) is a valid kernel deﬁned on
L (MacKay, 1998, Ch. 5.4.3).
In addition, if φ(
) is represented by a neural network, the resulting model can be viewed as
·
the same network, but with an additional GP-layer and the negative log marginal likelihood
(NLML) objective function used instead of the standard mean squared error (MSE).

X

7

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Input embedding is well-known in the Gaussian process literature (e.g. MacKay, 1998;
Hinton and Salakhutdinov, 2008). Recently, Wilson et al. (2016a,b) have successfully scaled
the approach and demonstrated strong results in regression and classiﬁcation tasks for
kernels based on feedforward and convolutional architectures. In this paper, we apply the
same technique to learn kernels with recurrent structure by transforming input sequences
with a recurrent neural network that acts as φ(
). In particular, a (multi-layer) LSTM
·
architecture is used to embed L steps of the input time series into a single vector in the
hidden space,
. For the embedding, as common, we use the last hidden vector produced by
the recurrent network. Note that however, any variations to the embedding (e.g., using other
types of recurrent units, adding 1-dimensional pooling layers, or attention mechanisms) are
all fairly straightforward2. More generally, the recurrent transformation can be random itself
(Figure 1), which would enable direct modeling of uncertainty within the recurrent dynamics,
but would also require inference for φ (e.g., as in Mattos et al., 2015). In this study, we
limit our consideration of random recurrent maps to only those induced by dropout.

H

Unfortunately, once the the MSE objective is substituted with NLML, it no longer factor-
izes over the data. This prevents us from using the well-established stochastic optimization
techniques for training our recurrent model. In the case of feedforward and convolutional
networks, Wilson et al. (2016a) proposed to pre-train the input transformations and then
ﬁne-tune them by jointly optimizing the GP marginal likelihood with respect to hyperpa-
rameters and the network weights using full-batch algorithms. When the transformation
is recurrent, stochastic updates play a key role. Therefore, we propose a semi-stochastic
block-gradient optimization procedure which allows mini-batching weight updates and fully
joint training of the model from scratch.

4.1 Optimization

The negative log marginal likelihood of the Gaussian process has the following form:

(K) = y(cid:62)(Ky + σ2I)−1y + log det(Ky + σ2I) + const,

(8)

L

where Ky + σ2I (∆= K) is the Gram kernel matrix, Ky, is computed on
N
i=1 and
implicitly depends on the base kernel hyperparameters, θ, and the parameters of the
recurrent neural transformation, φ(
), denoted W and further referred as the transformation
·
hyperparameters. Our goal is to optimize

with respect to both θ and W .

φ(xi)
}
{

The derivative of the NLML objective with respect to θ is standard and takes the

L

following form (Rasmussen and Williams, 2006):

(cid:18)(cid:104)

∂
L
∂θ

1
2

=

tr

K−1yy(cid:62)K−1

K−1(cid:105) ∂K
∂θ

−

(cid:19)

,

(9)

2. Details on the particular architectures used in our empirical study are discussed in the next section.

8

Learning Scalable Deep Kernels with Recurrent Structure

where ∂K/∂θ is depends on the kernel function, k(
,
·
The derivative with respect to the l-th transformation hyperparameter, Wl, is as follows:3
(cid:41)

), and usually has an analytic form.
·

K−1(cid:17)

(cid:40)(cid:18) ∂k(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

ij

−

+

(cid:18) ∂k(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

,

K−1yy(cid:62)K−1

∂
L
∂Wl

=

1
2

(cid:88)

(cid:16)

i,j

(10)
where hi = φ(xi) corresponds to the latent representation of the the i-th data instance. Once
the derivatives are computed, the model can be trained with any ﬁrst-order or quasi-Newton
optimization routine. However, application of the stochastic gradient method—the de facto
standard optimization routine for deep recurrent networks—is not straightforward: neither
the objective, nor its derivatives factorize over the data4 due to the kernel matrix inverses,
and hence convergence is not guaranteed.

Semi-stochastic alternating gradient descent. Observe that once the kernel matrix,
K−1(cid:1), can be precomputed on the full data
K, is ﬁxed, the expression, (cid:0)K−1yy(cid:62)K−1
−
and ﬁxed. Subsequently, Eq. (10) turns into a weighted sum of independent functions of
each data point. This observation suggests that, given a ﬁxed kernel matrix, one could
compute a stochastic update for W on a mini-batch of training points by only using the
corresponding sub-matrix of K. Hence, we propose to optimize GPs with recurrent kernels
in a semi-stochastic fashion, alternating between updating the kernel hyperparameters, θ,
on the full data ﬁrst, and then updating the weights of the recurrent network, W , using
stochastic steps. The procedure is given in Algorithm 1.

Semi-stochastic alternating gradient descent is a special case of block-stochastic gradient
iteration (Xu and Yin, 2015). While the latter splits the variables into arbitrary blocks
and applies Gauss–Seidel type stochastic gradient updates to each of them, our procedure
alternates between applying deterministic updates to θ and stochastic updates to W of the
form θ(t+1)
5. The corresponding Algorithm 1
is provably convergent for convex and non-convex problems under certain conditions. The
following theorem adapts results of Xu and Yin (2015) to our optimization scheme.

θ and W (t+1)

W (t) + λ(t)

θ(t) + λ(t)

θ g(t)

W g(t)

←

←

W

Theorem 1 (informal) Semi-stochastic alternating gradient descent converges to a ﬁxed
point when the learning rate, λt, decays as Θ(1/t

1+δ
2 ) for any δ

(0, 1].

∈

Applying alternating gradient to our case has a catch: the kernel matrix (and its inverse)
has to be updated each time W and θ are changed, i.e., on every mini-batch iteration
(marked red in Algorithm 1). Computationally, this updating strategy defeats the purpose
of stochastic gradients because we have to use the entire data on each step. To deal with
the issue of computational eﬃciency, we use ideas from asynchronous optimization.

3. Step-by-step derivations are given in Appendix B.
4. Cannot be represented as sums of independent functions of each data point.
5. In principle, stochastic updates of θ are also possible. As we will see next, we choose in practice to keep
the kernel matrix ﬁxed while performing stochastic updates. Due to sensitivity of the kernel to even
small changes in θ, convergence of the fully stochastic scheme is fragile.

9

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Algorithm 1 Semi-stochastic alternating
gradient descent.
input Data – (X, y), kernel – kθ(
,
·
recurrent transformation – φw(

1: Initialize θ and w; compute initial K.
2: repeat
3:

for all mini-batches Xb in X do

),
·
).
·

4:

5:

θ + updateθ(X, θ, K). and
θ
w + updatew(Xb, w, K).
w
Update the kernel matrix, K.

←
←

end for

6:
7: until Convergence
output Optimal θ∗ and w∗

Algorithm 2 Semi-stochastic asynchronous
gradient descent.
input Data – (X, y), kernel – kθ(
,
·
recurrent transformation – φw(
·

1: Initialize θ and w; compute initial K.
2: repeat
θ
θ + updateθ(X, w, K).
3:
for all mini-batches Xb in X do

),
·
).

←

4:

5:

6:

w + updatew(Xb, w, Kstale).

w
←
end for
Update the kernel matrix, K.

7:
8: until Convergence
output Optimal θ∗ and w∗

Asynchronous techniques. One of the recent trends in parallel and distributed
optimization is applying updates in an asynchronous fashion (Agarwal and Duchi, 2011).
Such strategies naturally require some tolerance to delays in parameter updates (Langford
et al., 2009). In our case, we modify Algorithm 1 to allow delayed kernel matrix updates.

The key observation is very intuitive: when the stochastic updates of W are small
enough, K does not change much between mini-batches, and hence we can perform multiple
stochastic steps for W before re-computing the kernel matrix, K, and still converge. For
example, K may be updated once at the end of each pass through the entire data (see
Algorithm 2). To ensure convergence of the algorithm, it is important to strike the balance
between (a) the learning rate for W and (b) the frequency of the kernel matrix updates.
The following theorem provides convergence results under certain conditions.

Theorem 2 (informal) Semi-stochastic gradient descent with τ -delayed kernel updates
(0, 1].
converges to a ﬁxed point when the learning rate, λt, decays as Θ(1/τ t

1+δ
2 ) for any δ

∈

Formal statements, conditions, and proofs for Theorems 1 and 2 are given in Appendix C.2.
Why stochastic optimization? GPs with recurrent kernels can be also trained with
full-batch gradient descent, as proposed by Wilson et al. (2016a). However, stochastic
gradient methods have been proved to attain better generalization (Hardt et al., 2016)
and often demonstrate superior performance in deep and recurrent architectures (Wilson
and Martinez, 2003). Moreover, stochastic methods are ‘online’, i.e., they update model
parameters based on subsets of an incoming data stream, and hence can scale to very large
datasets. In our experiments, we demonstrate that GPs with recurrent kernels trained with
Algorithm 2 converge faster (i.e., require fewer passes through the data) and attain better
performance than if trained with full-batch techniques.

10

Learning Scalable Deep Kernels with Recurrent Structure

Stochastic variational inference. Stochastic variational inference (SVI) in Gaussian
processes (Hensman et al., 2013) is another viable approach to enabling stochastic opti-
mization for GPs with recurrent kernels. Such method would optimize a variational lower
bound on the original objective that factorizes over the data by construction. Recently,
Wilson et al. (2016b) developed such a stochastic variational approach in the context of deep
kernel learning. Note that unlike all previous existing work, our proposed approach does
not require a variational approximation to the marginal likelihood to perform mini-batch
training of Gaussian processes.

4.2 Scalability

×

(n3) computations for n training data points, and

Learning and inference with Gaussian processes requires solving a linear system involving
n kernel matrix, K−1y, and computing a log determinant over K. These operations
an n
(n2) storage. In our
typically require
approach, scalability is achieved through semi-stochastic training and structure-exploiting
inference. In particular, asynchronous semi-stochastic gradient descent reduces both the
total number of passes through the data required for the model to converge and the number
of calls to the linear system solver; exploiting the structure of the kernels signiﬁcantly
reduces the time and memory complexities of the linear algebraic operations.

O

O

More precisely, we replace all instances of the covariance matrix Ky with W KU,U W (cid:62),
where W is a sparse interpolation matrix, and KU,U is the covariance matrix evaluated
over m latent inducing points, which decomposes into a Kronecker product of circulant
matrices (Wilson and Nickisch, 2015; Wilson et al., 2015). This construction makes inference
and learning scale as
(1), while preserving model structure.
For the sake of completeness, we provide an overview of the underlying algebraic machinery
in Appendix A.

(n) and test predictions be

O

O

At a high level, because W is sparse and KU,U is structured it is possible to take extremely
fast matrix vector multiplications (MVMs) with the approximate covariance matrix KX,X .
One can then use methods such as linear conjugate gradients, which only use MVMs, to
eﬃciently solve linear systems. MVM or scaled eigenvalue approaches (Wilson and Nickisch,
2015; Wilson et al., 2015) can also be used to eﬃciently compute the log determinant and
its derivatives. Kernel interpolation (Wilson et al., 2015) also enables fast predictions, as we
describe further in the Appendix.

5. Experiments

We compare the proposed Gaussian processes with recurrent kernels based on RNN and
LSTM architectures (GP-RNN/LSTM) with a number of baselines on datasets of various
complexity and ranging in size from hundreds to almost a million of time points. For the
datasets with more than a few thousand points, we use a massively scalable version of GPs
(see Section 4.2) and demonstrate its scalability during inference and learning. We carry

11

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Table 1: Statistics for the data used in experiments. SNR was determined by assuming a certain
degree of smoothness of the signal, ﬁtting kernel ridge regression with RBF kernel to predict the
targets from the input time series, and regarding the residuals as the noise. Tasks with low average
correlation between inputs and targets and lower SNR are harder prediction problems.

Dataset Task

# time steps # dim # outputs Abs. corr.

SNR

Drives
Actuator

GEF

Car

system ident.

power load
wind power

speed
gyro yaw
lanes
lead vehicle

500
1,024

38,064
130,963

932,939

1
1

11
16

6
6
26
9

1
1

1
1

1
1
16
2

0.7994
0.0938

0.5147
0.1731

0.1196
0.0764
0.0816
0.1099

25.72
12.47

89.93
4.06

159.33
3.19
—
—

out a number of experiments that help to gain empirical insights about the convergence
properties of the proposed optimization procedure with delayed kernel updates. Additionally,
we analyze the regularization properties of GP-RNN/LSTM and compare them with other
techniques, such as dropout. Finally, we apply the model to the problem of lane estimation
and lead vehicle position prediction, both critical in autonomous driving applications.

5.1 Data and the setup

Below, we describe each dataset we used in our experiments and the associated prediction
tasks. The essential statistics for the datasets are summarized in Table 1.

System identiﬁcation. In the ﬁrst set of experiments, we used publicly available non-
linear system identiﬁcation datasets: Actuator 6 (Sjöberg et al., 1995) and Drives 7 (Wigren,
2010). Both datasets had one dimensional input and output time series. Actuator had the
size of the valve opening as the input and the resulting change in oil pressure as the output.
Drives was from a system with motors that drive a pulley using a ﬂexible belt; the input
was the sum of voltages applied to the motors and the output was the speed of the belt.

Smart grid data8. We considered the problem of forecasting for the smart grid that
consisted of two tasks (Figure 2). The ﬁrst task was to predict power load from the historical
temperature data. The data had 11 input time series coming from hourly measurements of
temperature on 11 zones and an output time series that represented the cumulative hourly
power load on a U.S. utility. The second task was to predict power generated by wind
farms from the wind forecasts. The data consisted of 4 diﬀerent hourly forecasts of the
wind and hourly values of the generated power by a wind farm. Each wind forecast was a

6. http://www.iau.dtu.dk/nnbook/systems.html
7. http://www.it.uu.se/research/publications/reports/2010-020/NonlinearData.zip.
8. The smart grid data were taken from Global Energy Forecasting Kaggle competitions organized in 2012.

12

Learning Scalable Deep Kernels with Recurrent Structure

Figure 2: Left: Visualization of the GEF-power time series for two zones and the cumulative load
with the time resolution of 1 day. Cumulative power load is generally negatively correlated with
the temperature measurements on all the zones. Right: Visualization of the GEF-wind time series
with the time resolution of 1 day.

4-element vector that corresponded to zonal component, meridional component, wind speed
and wind angle. In our experiments, we concatenated the 4 diﬀerent 4-element forecasts,
which resulted in a 16-dimensional input time series.

Self-driving car dataset9. One of the main target applications of the proposed model
is prediction for autonomous driving. We considered a large dataset coming from sensors
of a self-driving car that was recorded on two car trips with discretization of 10 ms. The
data featured two sets of GPS ECEF locations, ECEF velocities, measurements from a
ﬁber-optic gyro compass, LIDAR, and a few more time series from a variety of IMU sensors.
Additionally, locations of the left and right lanes were extracted from a video stream for
each time step as well as the position of the lead vehicle from the LIDAR measurements. We
considered the data from the ﬁrst trip for training and from the second trip for validation
and testing. A visualization of the car routes with 25 second discretization in the ENU
coordinates are given in Figure 3. We consider four tasks, the ﬁrst two of which are more of
proof-of-concept type variety, while the ﬁnal two are fundamental to good performance for
a self-driving car:

1. Speed prediction from noisy GPS velocity estimates and gyroscopic inputs.

2. Prediction of the angular acceleration of the car from the estimates of its speed and

steering angle.

3. Point-wise prediction of the lanes from the estimates at the previous time steps, and

estimates of speed, gyroscopic and compass measurements.

4. Prediction of the lead vehicle location from its location at the previous time steps,

and estimates of speed, gyroscopic and compass measurements.

We provide more speciﬁc details on the smart grid data and self-driving data in Appendix D.

9. The dataset is proprietary.

It was released in part for public use under the Creative Commons
Attribution 3.0 license: http://archive.org/details/comma-dataset. More about the self-driving car:
http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/.

13

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Train

Test

Time

42.8 min

46.5 min

Speed of the self-driving car

Min

Max

0.0 mph

0.0 mph

80.3 mph

70.1 mph

Average

42.4 mph

38.4 mph

Median

58.9 mph

44.8 mph

Distance to the lead vehicle

Min

Max

Average

Median

23.7 m

29.7 m

178.7 m

184.7 m

85.4 m

54.9 m

72.6 m

53.0 m

Figure 3 & Table 2: Left: Train and test routes of the self-driving car in the ENU coordinates
with the origin at the starting location. Arrows point in the direction of motion; color encodes the
speed. Insets zoom selected regions of the routes. Best viewed in color. Right: Summary of the
data collected on the train and test routes.

Models and metrics. We used a number of classical baselines: NARX (Lin et al.,
1996), GP-NARX models (Kocijan et al., 2005), and classical RNN and LSTM architectures.
The kernels of our models, GP-NARX/RNN/LSTM, used the ARD base kernel function and
were structured by the corresponding baselines.10 As the primary metric, we used root mean
squared error (RMSE) on a held out set and additionally negative log marginal likelihood
(NLML) on the training set for the GP-based models.

We train all the models to perform one-step-ahead prediction in an autoregressive setting,
where targets at the future time steps are predicted from the input and target values at a
ﬁxed number of past time steps. For the system identiﬁcation task, we additionally consider
the non-autoregressive scenario (i.e., mapping only input sequences to the future targets),
where we are performing prediction in the free simulation mode, and included recurrent
Gaussian processes (Mattos et al., 2015) in our comparison. In this case, none of the future
targets are available and the models have to re-use their own past predictions to produce
future forecasts).

A note on implementation. Recurrent parts of each model were implemented using
Keras 11 library. We extended Keras with the Gaussian process layer and developed a backed
engine based on the GPML library12. Our approach allows us to take full advantage of
the functionality available in Keras and GPML, e.g., use automatic diﬀerentiation for the
recurrent part of the model. Our code is available at http://github.com/alshedivat/kgp/.

10. GP-NARX is a special instance of our more general framework and we trained it using the proposed

semi-stochastic algorithm.

11. http://www.keras.io
12. http://www.gaussianprocess.org/gpml/code/matlab/doc/

14

Learning Scalable Deep Kernels with Recurrent Structure

Figure 4: Two charts on the left: Convergence of the optimization in terms of RMSE on test and
NLML on train. The inset zooms the region of the plot right beneath it using log scale for the
vertical axis. full and mini denote full-batch and mini-batch optimization procedures, respectively,
while 16 and 64 refer to models with the respective number of units per hidden layer. Two charts on
the right: Test RMSE for a given architecture trained with a speciﬁed method and/or learning rate.

5.2 Analysis

This section discusses quantitative and qualitative experimental results. We only brieﬂy
introduce the model architectures and the training schemes used in each of the experiments.
We provide a comprehensive summary of these details in Appendix E.

5.2.1 Convergence of the optimization

To address the question of whether stochastic optimization of recurrent kernels is necessary
and to assess the behavior of the proposed optimization scheme with delayed kernel updates,
we conducted a number of experiments on the Actuator dataset (Figure 4).

First, we constructed two GP-LSTM models with 1 recurrent hidden layer and 16 or 64
hidden units and trained them with (non-stochastic) full-batch iterative procedure (similar
to the proposal of Wilson et al. (2016a)) and with our semi-stochastic optimizer with delayed
kernel updates (Algorithm 2). The convergence results are given on the ﬁrst two charts.
Both in terms of the error on a held out set and the NLML on the training set, the models
trained with mini-batches converged faster and demonstrated better ﬁnal performance.

Next, we compared the two optimization schemes on the same GP-LSTM architecture
with diﬀerent sizes of the hidden layer ranging from 2 to 32. It is clear from the third chart
that, even though full-batch approach seemed to ﬁnd a better optimum when the number of
hidden units was small, the stochastic approach was clearly superior for larger hidden layers.
Finally, we compared the behavior of Algorithm 2 with diﬀerent number of mini-batches
used for each epoch (equivalently, the number of steps between the kernel matrix updates)
and diﬀerent learning rates. The results are give on the last chart. As expected, there is a
ﬁne balance between the number of mini-batches and the learning rate: if the number of
mini-batches is large (i.e., the delay between the kernel updates becomes too long) while the
learning rate is high enough, optimization does not converge; at the same time, an appropriate
combination of the learning rate and the mini-batch size leads better generalization than
the default batch approach of Wilson et al. (2016a).

15

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Table 3: Average performance of the models in terms of RMSE on the system identiﬁcation tasks.
The averages were computed over 5 runs; the standard deviation is given in the parenthesis. Results
for the RGP model are as reported by Mattos et al. (2015), available only for the free simulation.

regression

auto-regression

free simulation

regression

free simulation

Drives

0.19 (0.03)
0.17 (0.04)
0.14 (0.02)

0.16 (0.04)
0.16 (0.03)
0.13 (0.02)

0.38 (0.03)
0.56 (0.03)
0.40 (0.02)

0.28 (0.02)
0.45 (0.03)
0.32 (0.03)

0.49 (0.05)
0.56 (0.03)
0.40 (0.03)

0.46 (0.03)
0.49 (0.02)
0.36 (0.01)

Actuator
auto-regression

0.18 (0.01)
0.17 (0.01)
0.19 (0.01)

0.14 (0.01)
0.15 (0.01)
0.14 (0.01)

0.57 (0.04)
0.68 (0.05)
0.44 (0.03)

0.63 (0.04)
0.55 (0.04)
0.43 (0.03)

NARX
RNN
LSTM

0.33 (0.02)
0.53 (0.02)
0.29 (0.02)

GP-NARX 0.28 (0.02)
GP-RNN
0.37 (0.04)
GP-LSTM 0.25 (0.02)

RGP

—

—

0.249

—

—

0.368

5.2.2 Regression, Auto-regression, and Free Simulation

In this set of experiments, our main goal is to provide a comparison between three diﬀerent
modes of one-step-ahead prediction, referred to as (i) regression, (ii) autoregression, and
(iii) free simulation, and compare performance of our models with RGP—a classical RNN
with every parametric layer substituted with a Gaussian process (Mattos et al., 2015)—on
the Actuator and Drives datasets. The diﬀerence between the prediction modes consists in
whether and how the information about the past targets is used. In the regression scenario,
inputs and targets are separate time series and the model learns to map input values at
a number of past time points to a target value at a future point in time. Autoregression,
additionally, uses the true past target values as inputs; in the free simulation mode, the
model learns to map past inputs and its own past predictions to a future target.

In the experiments in autoregression and free simulation modes, we used short time lags,
L = 10, as suggested by Mattos et al. (2015). In the regression mode, since the model does
not build the recurrent relationships based on the information about the targets (or their
estimates), it generally requires larger time lags that can capture the state of the dynamics.
Hence we increased the time lag to 32 in the regression mode. More details are given in
Appendix E.

We present the results in Table 3. We note that GP-based architectures consistently
yielded improved predictive performance compared to their vanilla deep learning counterparts
on both of the datasets, in each mode. Given the small size of the datasets, we attribute
such behavior to better regularization properties of the negative log marginal likelihood loss
function. We also found out that when GP-based models were initialized with weights of
pre-trained neural networks, they tended to overﬁt and give overly conﬁdent predictions
on these tasks. The best performance was achieved when the models were trained from a
random initialization (contrary to the ﬁndings of Wilson et al., 2016a). In free simulation
mode RGP performs best of the compared models. This result is expected—RGP was
particularly designed to represent and propagate uncertainty through a recurrent process.

16

Learning Scalable Deep Kernels with Recurrent Structure

Table 4: Average performance of the best models in terms of RMSE on the GEF and Car tasks.
The averages were computed over 5 runs; the standard deviation is given in the parenthesis.

GEF
power load wind power

speed

gyro yaw

lanes

lead vehicle

Car

NARX
RNN
LSTM

0.54 (0.02)
0.61 (0.02)
0.45 (0.01)

GP-NARX 0.78 (0.03)
GP-RNN
0.24 (0.02)
GP-LSTM 0.17 (0.02)

0.84 (0.01)
0.81 (0.01)
0.77 (0.01)

0.83 (0.02)
0.79 (0.01)
0.76 (0.01)

0.114 (0.010)
0.152 (0.012)
0.027 (0.008)

0.19 (0.01)
0.22 (0.01)
0.13 (0.01)

0.13 (0.01)
0.33 (0.02)
0.08 (0.01)

0.41 (0.02)
0.44 (0.03)
0.40 (0.01)

0.125 (0.015)
0.089 (0.013)
0.019 (0.006)

0.23 (0.02)
0.24 (0.01)
0.08 (0.01)

0.10 (0.01)
0.46 (0.08)
0.06 (0.01)

0.34 (0.02)
0.41 (0.02)
0.32 (0.02)

Our framework focuses on using recurrence to build expressive kernels for regression on
sequences.

The suitability of each prediction mode depends on the task at hand. In many applications
where the future targets become readily available as the time passes (e.g., power estimation
or stock market prediction), the autoregression mode is preferable. We particularly consider
autoregressive prediction in the further experiments.

5.2.3 Prediction for smart grid and self-driving car applications

For both smart grid prediction tasks we used LSTM and GP-LSTM models with 48 hour
time lags and were predicting the target values one hour ahead. LSTM and GP-LSTM
were trained with one or two layers and 32 to 256 hidden units. The best models were
selected on 25% of the training data used for validation. For autonomous driving prediction
tasks, we used the same architectures but with 128 time steps of lag (1.28 s). All models
were regularized with dropout (Srivastava et al., 2014; Gal and Ghahramani, 2016b). On
both GEF and self-driving car datasets, we used the scalable version of Gaussian process
(MSGP) (Wilson et al., 2015). Given the scale of the data and the challenge of nonlinear
optimization of the recurrent models, we initialized the recurrent parts of GP-RNN and
GP-LSTM with pre-trained weights of the corresponding neural networks. Fine-tuning of the
models was performed with Algorithm 2. The quantitative results are provided in Table 4
and demonstrate that GPs with recurrent kernels attain the state-of-the-art performance.
Additionally, we investigated convergence and regularization properties of LSTM and
GP-LSTM models on the GEF-power dataset. The ﬁrst two charts of Figure 6 demonstrate
that GP-based models are less prone to overﬁtting, even when the data is not enough. The
third panel shows that architectures with a particular number of hidden units per layer
attain the best performance on the power prediction task. An additional advantage of
the GP-layers over the standard recurrent networks is that the best architecture could be
identiﬁed based on the negative log likelihood of the model as shown on the last chart.

17

Al-Shedivat, Wilson, Saatchi, Hu, Xing

(a) Point-wise predictions of the lanes made by LSTM (upper) and by GP-LSTM (lower). Dashed
lines correspond to the ground truth extracted from video sequences and used for training.

(b) LSTM (upper) and by GP-LSTM (lower) position predictions of the lead vehicle. Black markers
and dashed lines are the ground truth; blue and red markers with solid lines correspond to predictions.

Figure 5: Qualitative comparison of the LSTM and GP-LSTM predictions on self-driving tasks.
Predictive uncertainty of the GP-LSTM model is showed by contour plots and error-bars; the latter
denote one standard deviation of the predictive distributions.

Finally, Figure 5 qualitatively demonstrates the diﬀerence between the predictions given
by LSTM vs. GP-LSTM on point-wise lane estimation (Figure 5a) and the front vehicle
tracking (Figure 5b) tasks. We note that GP-LSTM not only provides a more robust ﬁt,
but also estimates the uncertainty of its predictions. Such information can be further used
in downstream prediction-based decision making, e.g., such as whether a self-driving car
should slow down and switch to a more cautious driving style when the uncertainty is high.

18

Learning Scalable Deep Kernels with Recurrent Structure

Figure 6: Left to right: RMSE vs. the number of training points; RMSE vs. the number model
parameters per layer; NLML vs. the number model parameters per layer for GP-based models. All
metrics are averages over 5 runs with diﬀerent random initializations, computed on a held-out set.

Figure 7: The charts demonstrate scalability of learning and inference of MSGP with an LSTM-based
recurrent kernel. Legends with points denote the number of inducing points used. Legends with
percentages denote the percentage of the training dataset used learning the model.

5.2.4 Scalability of the model

Following Wilson et al. (2015), we performed a generic scalability analysis of the MSGP-
LSTM model on the car sensors data. The LSTM architecture was the same as described
in the previous section: it was transforming multi-dimensional sequences of inputs to a
two-dimensional representation. We trained the model for 10 epochs on 10%, 20%, 40%,
and 80% of the training set with 100, 200, and 400 inducing points per dimension and
measured the average training time per epoch and the average prediction time per testing
point. The measured time was the total time spent on both LSTM optimization and MSGP
computations. The results are presented in Figure 7.

The training time per epoch (one full pass through the entire training data) grows linearly
with the number of training examples and depends linearly on the number of inducing points
(Figure 7, two left charts). Thus, given a ﬁxed number of inducing points per dimension, the
time complexity of MSGP-LSTM learning and inference procedures is linear in the number
of training examples. The prediction time per testing data point is virtually constant and
does not depend on neither on the number of training points, nor on the number of inducing
points (Figure 7, two right charts).

19

Al-Shedivat, Wilson, Saatchi, Hu, Xing

6. Discussion

We proposed a method for learning kernels with recurrent long short-term memory structure
on sequences. Gaussian processes with such kernels, termed the GP-LSTM, have the structure
and learning biases of LSTMs, while retaining a probabilistic Bayesian nonparametric
representation. The GP-LSTM outperforms a range of alternatives on several sequence-to-
reals regression tasks. The GP-LSTM also works on data with low and high signal-to-noise
ratios, and can be scaled to very large datasets, all with a straightforward, practical, and
generally applicable model speciﬁcation. Moreover, the semi-stochastic scheme proposed
in our paper is provably convergent and eﬃcient in practical settings, in conjunction with
structure exploiting algebra. In short, the GP-LSTM provides a natural mechanism for
Bayesian LSTMs, quantifying predictive uncertainty while harmonizing with the standard
deep learning toolbox. Predictive uncertainty is of high value in robotics applications, such
as autonomous driving, and could also be applied to other areas such as ﬁnancial modeling
and computational biology.

There are several exciting directions for future research. The GP-LSTM quantiﬁes
predictive uncertainty but does not model the propagation of uncertainty in the inputs
through a recurrent structure. Treating free simulation as a structured prediction problem
and using online corrective algorithms, e.g., DAGGER (Ross et al., 2011), are likely to
improve performance of GP-LSTM in the free prediction mode. This approach would not
require explicitly modeling and propagating uncertainty through the recurrence and would
maintain the high computational eﬃciency of our method.

Alternatively, it would be exciting to have a probabilistic treatment of all parameters of
the GP-LSTM kernel, including all LSTM weights. Such an extension could be combined with
stochastic variational inference, to enable both classiﬁcation and non-Gaussian likelihoods
as in Wilson et al. (2016b), but also open the doors to stochastic gradient Hamiltonian
Monte Carlo (Chen et al., 2014) (SG-HMC) for eﬃcient inference over kernel parameters.
Indeed, SG-HMC has recently been used for eﬃcient inference over network parameters in
the Bayesian GAN (Saatchi and Wilson, 2017). A Bayesian approach to marginalizing the
weights of the GP-LSTM kernel would also provide a principled probabilistic mechanism for
learning model hyperparameters.

One could relax several additional assumptions. We modeled each output dimension
with independent GPs that shared a recurrent transformation. To capture the correlations
between output dimensions, it would be promising to move to a multi-task formulation. In
the future, one could also learn the time horizon in the recurrent transformation, which
could lead to major additional performance gains.

Finally, the semi-stochastic learning procedure naturally complements research in asyn-
chronous optimization (e.g., Deisenroth and Ng, 2015). In combination with stochastic
variational inference, the semi-stochastic approach could be used for parallel kernel learning,
side-stepping the independence assumptions in prior work. We envision that such eﬀorts for
Gaussian processes will harmonize with current progress in Bayesian deep learning.

20

Learning Scalable Deep Kernels with Recurrent Structure

7. Acknowledgements

The authors thank Yifei Ma for helpful discussions and the anonymous reviewers for the
valuable comments that helped to improve the paper. This work was supported in part by
NIH R01GM114311, AFRL/DARPA FA87501220324, and NSF IIS-1563887.

21

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Appendix A. Massively scalable Gaussian processes

Massively scalable Gaussian processes (MSGP) (Wilson et al., 2015) is a signiﬁcant extension
of the kernel interpolation framework originally proposed by Wilson and Nickisch (2015). The
core idea of the framework is to improve scalability of the inducing point methods (Quinonero-
Candela and Rasmussen, 2005) by (1) placing the virtual points on a regular grid, (2)
exploiting the resulting Kronecker and Toeplitz structures of the relevant covariance matrices,
and (3) do local cubic interpolation to go back to the kernel evaluated at the original points.
This combination of techniques brings the complexity down to
(1)
for each test prediction. Below, we overview the methodology. We remark that a major
diﬀerence in philosophy between MSGP and many classical inducing point methods is that
the points are selected and ﬁxed rather than optimized over. This allows to use signiﬁcantly
more virtual points which typically results in a better approximation of the true kernel.

(n) for training and

O

O

A.1 Structured kernel interpolation

Given a set of m inducing points, the n
m cross-covariance matrix, KX,U , between the
training inputs, X, and the inducing points, U, can be approximated as ˜KX,U = WX KU,U
m matrix of interpolation weights, WX . This allows to
using a (potentially sparse) n
approximate KX,Z for an arbitrary set of inputs Z as KX,Z
Z . For any given
kernel function, K, and a set of inducing points, U, structured kernel interpolation (SKI)
procedure (Wilson and Nickisch, 2015) gives rise to the following approximate kernel:

˜KX,U W (cid:62)

≈

×

×

KSKI(x, z) = WX KU,U W (cid:62)
z ,

(A.1)

≈

WX KU,U W (cid:62)
X . Wilson and Nickisch (2015) note that
which allows to approximate KX,X
standard inducing point approaches, such as subset of regression (SoR) or fully independent
training conditional (FITC), can be reinterpreted from the SKI perspective. Importantly,
the eﬃciency of SKI-based MSGP methods comes from, ﬁrst, a clever choice of a set of
inducing points that allows to exploit algebraic structure of KU,U , and second, from using
very sparse local interpolation matrices. In practice, local cubic interpolation is used (Keys,
1981).

A.2 Kernel approximations

If inducing points, U , form a regularly spaced P -dimensional grid, and we use a stationary
product kernel (e.g., the RBF kernel), then KU,U decomposes as a Kronecker product of
Toeplitz matrices:

KU,U = T1

T2

TP .

⊗

⊗ · · · ⊗

(A.2)

The Kronecker structure allows to compute the eigendecomposition of KU,U by separately
decomposing T1, . . . , TP , each of which is much smaller than KU,U . Further, to eﬃciently

22

Learning Scalable Deep Kernels with Recurrent Structure

eigendecompose a Toeplitz matrix, it can be approximated by a circulant matrix13 which
eigendecomposes by simply applying discrete Fourier transform (DFT) to its ﬁrst column.
Therefore, an approximate eigendecomposition of each T1, . . . , TP is computed via the fast
Fourier transform (FFT) and requires only

(m log m) time.

O

A.3 Structure exploiting inference

To perform inference, we need to solve (KSKI + σ2I)−1y; kernel learning requires evaluating
log det(KSKI + σ2I). The ﬁrst task can be accomplished by using an iterative scheme—linear
conjugate gradients—which depends only on matrix vector multiplications with (KSKI + σ2I).
The second is done by exploiting the Kronecker and Toeplitz structure of KU,U for computing
an approximate eigendecomposition, as described above.

A.4 Fast Test Predictions

To achieve constant time prediction, we approximate the latent mean and variance of f∗ by
applying the same SKI technique. In particular, for a set of n∗ testing points, X∗, we have

E[f∗] = µX∗ + KX∗,X
µX∗ + ˜KX∗,X

(cid:2)KX,X + σ2I(cid:3)−1
(cid:105)−1
(cid:104) ˜KX,X + σ2I

y

y,

≈

(A.3)

where ˜KX,X = W KU,U W (cid:62) and ˜KX∗,X = W∗KU,U W (cid:62), and W and W∗ are n
m
sparse interpolation matrices, respectively. Since KU,U W (cid:62)[ ˜KX,X + σ2I]−1y is precomputed
at training time, at test time, we only multiply the latter with W∗ matrix which results which
(1) operations per test point. Similarly, approximate
(n∗) operations leading to
costs
(1) operations (Wilson et al., 2015).
predictive variance can be also estimated in

m and n∗

O

O

×

×

Note that the fast prediction methodology can be readily applied to any trained Gaussian

O

process model as it is agnostic to the way inference and learning were performed.

Appendix B. Gradients for GPs with recurrent kernels

GPs with deep recurrent kernels are trained by minimizing the negative log marginal
likelihood objective function. Below we derive the update rules.

By applying the chain rule, we get the following ﬁrst order derivatives:

∂
L
∂γ

=

∂
L
∂K ·

∂K
∂γ

,

∂
L
∂W

=

∂
L
∂K ·

∂K
∂φ ·

∂φ
∂W

.

(B.4)

The derivative of the log marginal likelihood w.r.t. to the kernel hyperparameters, θ, and
the parameters of the recurrent map, W , are generic and take the following form (Rasmussen

13. Wilson et al. (2015) explored 5 diﬀerent approximation methods known in the numerical analysis

literature.

23

Al-Shedivat, Wilson, Saatchi, Hu, Xing

and Williams, 2006, Ch. 5, Eq. 5.9):

∂
L
∂θ
∂
L
∂W

= 1

= 1

2 tr (cid:0)(cid:2)K−1yy(cid:62)K−1
2 tr (cid:0)(cid:2)K−1yy(cid:62)K−1

K−1(cid:3) ∂K
∂θ

(cid:1) ,

K−1(cid:3) ∂K
∂W

(cid:1) .

−

−

The derivative ∂K/∂θ is also standard and depends on the form of a particular chosen
kernel function, k(
/∂W is a bit subtle, and hence
,
·
we elaborate these derivations below.

). However, computing each part of ∂
·

L

Consider the ij-th entry of the kernel matrix, Kij. We can think of K as a matrix-valued
function of all the data vectors in d-dimensional transformed space which we denote by
RN ×d. Then Kij is a scalar-valued function of H and its derivative w.r.t. the l-th
H
parameter of the recurrent map, Wl, can be written as follows:

∈

∂Kij
∂Wl

= tr

(cid:32)(cid:18) ∂Kij
∂H

(cid:19)(cid:62) ∂H
∂Wl

(cid:33)

.

Notice that ∂Kij/∂H is a derivative of a scalar w.r.t. to a matrix and hence is a matrix;
∂H/∂Wl is a derivative of a matrix w.r.t. to a scalar which is taken element-wise and also
gives a matrix. Also notice that Kij is a function of H, but it only depends the i-th and
j-th elements for which the kernel is being computed. This means that ∂Kij/∂H will have
only non-zero i-th row and j-th column and allows us to re-write (B.7) as follows:

(B.5)

(B.6)

(B.7)

(B.8)

∂Kij
∂Wl

=

=

(cid:18) ∂Kij
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

+

(cid:18) ∂Kij
∂hj

(cid:18) ∂K(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

+

(cid:19)(cid:62) ∂hj
∂Wl
(cid:18) ∂K(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

.

Since the kernel function has two arguments, the derivatives must be taken with respect of
each of them and evaluated at the corresponding points in the hidden space, hi = φ(xi)
and hj = φ(xj). When we plug this into (B.6), we arrive at the following expression:

∂
L
∂Wl

=

1
2

(cid:88)

(cid:16)

i,j

K−1yy(cid:62)K−1

K−1(cid:17)

(cid:40)(cid:18) ∂K(hi, hj)
∂hi

(cid:19)(cid:62) ∂hi
∂Wl

ij

−

+

(cid:18) ∂K(hi, hj)
∂hj

(cid:19)(cid:62) ∂hj
∂Wl

(cid:41)

.

(B.9)
The same expression can be written in a more compact form using the Einstein notation:

∂
L
∂Wl

=

(cid:16)

1
2

K−1yy(cid:62)K−1

K−1(cid:17)j

(cid:32)(cid:20) ∂K
∂h

i

(cid:21)jd

i

+

(cid:20) ∂K
∂h

(cid:21)id

(cid:33) (cid:20) ∂h
∂W

(cid:21)dl

i

j

−

(B.10)

where d indexes the dimensions of the h and l indexes the dimensions of W .

24

Learning Scalable Deep Kernels with Recurrent Structure

In practice, deriving a computationally eﬃcient analytical form of ∂K/∂h might be too
complicated for some kernels (e.g., the spectral mixture kernels (Wilson and Adams, 2013)),
especially if the grid-based approximations of the kernel are enabled. In such cases, we can
simply use a ﬁnite diﬀerence approximation of this derivative. As we remark in the following
section, numerical errors that result from this approximation do not aﬀect convergence of
the algorithm.

Appendix C. Convergence results

Convergence results for the semi-stochastic alternating gradient schemes with and without
delayed kernel matrix updates are based on (Xu and Yin, 2015). There are a few notable
diﬀerences between the original setting and the one considered in this paper:

1. Xu and Yin (2015) consider a stochastic program that minimizes the expectation of

the objective w.r.t. some distribution underlying the data:

f (x) := EξF (x; ξ),

min
x∈X

(C.11)

where every iteration a new ξ is sampled from the underlying distribution. In our
case, the goal is to minimize the negative log marginal likelihood on a particular given
dataset. This is equivalent to the original formulation (C.11), but with the expectation
taken w.r.t. the empirical distribution that corresponds to the given dataset.

2. The optimization procedure of Xu and Yin (2015) has access to only a single random
point generated from the data distribution at each step. Our algorithm requires having
access to the entire training data each time the kernel matrix is computed.

3. For a given sample, Xu and Yin (2015) propose to loop over a number of coordinate
blocks and apply Gauss–Seidel type gradient updates to each block. Our semi-stochastic
scheme has only two parameter blocks, θ and W , where θ is updated deterministically
on the entire dataset while W is updated with stochastic gradient on samples from
the empirical distribution.

Noting these diﬀerences, we ﬁrst adapt convergence results for the smooth non-convex
case (Xu and Yin, 2015, Theorem 2.10) to our scenario, and then consider the variant with
delaying kernel matrix updates.

C.1 Semi-stochastic alternating gradient

As shown in Algorithm 1, we alternate between updating θ and W . At step t, we get a
¯xi
mini-batch of size Nt, xt
i∈It, which is just a selection of points from the full set, x.
Deﬁne the gradient errors for θ and W at step t as follows:

≡ {

}

θ := ˜gt
δt

gt
θ,

W := ˜gt
δt

gt
W ,

W −

θ −

(C.12)

25

Al-Shedivat, Wilson, Saatchi, Hu, Xing

where gT
and then W , and hence the expressions for the gradients take the following form:

W are the true gradients and ˜gT

W are estimates14. We ﬁrst update θ

θ and ˜gT

θ and gT

˜gt
θ ≡

gt
θ =

(θt, W t) =

(cid:88)

(cid:16)

1
2N

K−1

t yy(cid:62)K−1

t −

(cid:17)

K−1
t

(cid:19)

(cid:18) ∂Kt
∂θ

ij

1
N ∇

θ

L

1
N ∇

W

L

i,j
1
2N

(cid:88)

(cid:16)

i,j

gt
W =

˜gt
W =

(θt+1, W t) =

K−1

t+1yy(cid:62)K−1

1
2Nt

(cid:88)

(cid:16)

i,j∈It

K−1

t+1yy(cid:62)K−1

t+1 −

(cid:17)

K−1
t+1

(cid:18) ∂Kt+1
∂W

ij

t+1 −
(cid:19)

ij

ij
(cid:18) ∂Kt+1
∂W

(cid:19)

(cid:17)

K−1
t+1

ij

(C.13)

(C.14)
ij

(C.15)

Note that as we have shown in Appendix B, when the kernel matrix is ﬁxed, gW and ˜gW
factorize over x and xt, respectively. Further, we denote all the mini-batches sampled before
t as x[t−1].

Lemma 1 For any step t, E[δt

x[t−1]] = E[δt

x[t−1]] = 0.

W |

θ |

Proof First, δt
have E[δt

0, and hence E[δt

x[t−1]] = 0 is trivial. Next, by deﬁnition of δt

W |

θ ≡

θ |
x[t−1]]. Consider the following:

x[t−1]] = E[˜gt

gt
W −
W |
Consider E[gt
x[t−1]]: gt
being updated deterministically using gt−1
Therefore, it is independent of xt, which means that E[gt

W is a deterministic function of θt+1 and W t. θt+1 is
, and hence only depends on W t and θt.
x[t−1]]

W |

W , we

θ

gt
W .

W |

≡

Now, consider E[˜gt
distribution and Kt+1 does not depend on the current mini-batch, we can write:

x[t−1]]: Noting that the expectation is taken w.r.t. the empirical

W |

•

•

E[˜gt

W |

x[t−1]] = E

K−1

t+1yy(cid:62)K−1

K−1
t+1

t+1 −

(cid:19)





ij

(cid:17)

ij

(cid:18) ∂Kt+1
∂W
(cid:18) ∂Kt+1
∂W

(cid:19)

ij

(C.16)

K−1

t+1yy(cid:62)K−1

t+1 −

(cid:17)

K−1
t+1

ij





1
2Nt

(cid:88)

(cid:16)

i,j∈It
(cid:16)

(cid:88)

i,j

Nt
2N Nt

gt
W .

=

≡

Finally, E[δt

x[t−1]] = gt

W |

gt
W = 0.

W −

In other words, semi-stochastic gradient descent that alternates between updating θ and

W computes unbiased estimates of the gradients on each step.

14. Here, we consider the gradients and their estimates scaled by the number of full data points, N , and the
mini-batch size, Nt, respectively. These constant scaling is introduced for sake of having cleaner proofs.

26

Learning Scalable Deep Kernels with Recurrent Structure

Remark 1 Note that in case if (∂Kt+1/∂W ) is computed approximately, Lemma 1 still
holds since both gt

x[t−1]] will contain exactly the same numerical errors.

W and E[˜gt

W |

Assumption 1 For any step t, E
δt
θ(cid:107)
(cid:107)

2

t and E
δt
σ2
2
W (cid:107)
(cid:107)

≤

≤

σ2
t .

Lemma 1 and Assumption 1 result into a stronger condition than the original assumption
given by Xu and Yin (2015). This is due to the semi-stochastic nature of the algorithm, it
simpliﬁes the analysis, though it is not critical. Assumptions 2 and 3 are straightforwardly
adapted from the original paper.

Assumption 2 The objective function,
θ and W are uniformly Lipschitz with constant L > 0:

L

, is lower bounded and its partial derivatives w.r.t.

(θ, W )

θ
(cid:107)∇

L

θ

− ∇

L

(˜θ, W )

θ

L
(cid:107)

−

˜θ

,
(cid:107)

(cid:107) ≤

(θ, W )

W

(cid:107)∇

L

W

− ∇

L

(θ, ˜W )

W
L
(cid:107)

(cid:107) ≤

˜W
.
(cid:107)
−
(C.17)

Assumption 3 There exists a constant ρ such that

θt

2 + E
W t
(cid:107)

2
(cid:107)

(cid:107)

(cid:107)

≤

ρ2 for all t.

Lemma 2 Under Assumptions 2 and 3,

θt

2

(cid:107)

≤

2
gt
ρ2, E
W (cid:107)
(cid:107)

M 2

ρ , E
(cid:107)

2
gt
θ(cid:107)

≤

M 2
ρ

t,

∀

W t
E
(cid:107)

2
(cid:107)
ρ = 4L2ρ2 + 2 max

ρ2, E
(cid:107)

≤

where M 2

{∇
Proof The inequalities for E
θ
(cid:107)
E
(cid:107)

gt
W (cid:107)

L

(θ0, W 0),

θ

W

∇

2 and E
(cid:107)

(cid:107)

W

2 are merely corollaries from Assumption 2.

≤
(θ0, W 0)

.
}

L
2 are trivial, and the ones for E
gt
θ(cid:107)
(cid:107)

(cid:107)

2 and

Negative log marginal likelihood of a Gaussian process with a structured kernel is a
nonconvex function of its arguments. Therefore, we can only show that the algorithm
converges to a stationary point, i.e., a point at which the gradient of the objective is zero.

Theorem 1 Let
for θ and W being λt
scalars, such that

{

(θt, W t)
}
θαt and λt
θ = ct

be a sequence generated from Algorithm 1 with learning rates
W , αt are positive

W αt, respectively, where ct

W = ct

θ, ct

0 < inf
t

ct
{θ,W } ≤

sup
t

ct
{θ,W } <

∞

, αt

αt+1,

≥

αt = +

,
∞

∞
(cid:88)

t=1

∞
(cid:88)

t=1

α2

t < +

,
∞

Then, under Assumptions 1 through 3 and if σ = supt σt <

, we have

∞

t.

∀
(C.18)

(C.19)

E

lim
t→∞

(cid:107)∇L

(θt, W t)

= 0.

(cid:107)

27

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Proof The proof is an adaptation of the one given by Xu and Yin (2015) with the following
three adjustments: (1) we have only two blocks of coordinates and (2) updates for θ are
deterministic which zeros out their variance terms, (3) the stochastic gradients are unbiased.

From the Lipschitz continuity of

(Assumption 2), we have:

W

∇

L

and

θ

∇

L

•

(θt+1, W t+1)

(θt+1, W t)

L
W , W t+1
gt

≤ (cid:104)

=

=

=

λt
W (cid:104)
(cid:18)
λt
W −

−

−

−
W , ˜gt
gt
W (cid:105)
L
(λt
2
L
2
L(λt
L
2

λt
W −

W −
λt
W −
(cid:0)λt

λt
θ

(cid:18)

−

−

(cid:0)λt
(cid:18)

≤ −

+

L
2

W t+1

W t

2

(cid:107)

−
2

− L
W t

(cid:105)

+

+

L
2 (cid:107)
L
W )2
(λt
2
(cid:19)
W )2

gt
W (cid:107)
(cid:107)

(cid:19)

(cid:107)
2 +

˜gt
W (cid:107)
L
2
L
2

W

(λt
W )2
W )2(cid:1) (cid:0)
(cid:104)
(cid:19)
W )2
(λt

2 +

gt
W (cid:107)
(cid:107)
gt
W − ∇
2 +
gt
W (cid:107)
(cid:107)
W )2(cid:1) (

δt
W (cid:107)
(cid:107)

W + L(λt

(λt

W )2

2
δt
W (cid:107)
(cid:107)
(θt, W t), δt

L
L
(λt
2

W )2

W (cid:105)
2
δt
W (cid:107)

(cid:107)

−

2 +

2),

gt
θ(cid:107)
(cid:107)

(λt

W )2

2
δt
W (cid:107)

(cid:107)

−

(cid:0)λt

W −

L(λt

W )2(cid:1)

W , δt
gt
(cid:104)

W (cid:105)

W

L

+

(cid:104)∇
(cid:0)λt

W −

(cid:1)

W (cid:105)

(θt, W t), δt
W )2(cid:1)

L(λt

W

(cid:104)∇

L

(θt, W t), δt

W (cid:105)

where the last inequality comes from the following (note that gt

W :=

W

∇

L

(θt+1, W t)):

W

(cid:0)λt

L(λt

W −

(θt+1, W t)

−
(cid:12)
(cid:12)λt
W −
(cid:12)
(cid:12)λt
Lλt
W −
θ
L
(cid:0)λt
W + L(λt
2

W )2(cid:1)
L(λt
L
(cid:104)∇
W )2(cid:12)
δt
(θt+1, W t)
(cid:12)
W
W (cid:107)(cid:107)∇
L
(cid:107)
W )2(cid:12)
gt
δt
L(λt
(cid:12)
W (cid:107)(cid:107)
θ(cid:107)
(cid:107)
W )2(cid:1) (
2 +
δt
W (cid:107)
(cid:107)

gt
θ(cid:107)

λt
θ

(cid:107)

2).

≤

≤

≤

W

− ∇

(θt, W t), δt
W (cid:105)
(θt, W t)
(cid:107)

L

L
W

− ∇

Analogously, we derive a bound on

(θt+1, W t)

(θt, W t):

L

− L

L

(θt+1, W t)
(cid:18)
L
2

λt
θ −

− L
θ)2
(λt

(θt, W t)
(cid:19)

2 +

gt
θ(cid:107)

(cid:107)

L
2

≤ −

(λt

θ)2

2
δt
θ(cid:107)

(cid:107)

−

(cid:0)λt

θ −

L(λt

θ)2(cid:1)

θ
(cid:104)∇

L

(θt, W t), δt
θ(cid:105)

Since (θt, W t) is independent from the current mini-batch, xt, using Lemma 1, we
have that E

(θt, W t), δt

= 0 and E

= 0.

•

θ
(cid:104)∇

L

(θt, W t), δt
θ(cid:105)

W

(cid:104)∇

L

W (cid:105)

28

Learning Scalable Deep Kernels with Recurrent Structure

Summing the two obtained inequalities and applying Assumption 1, we get:

≤ −

≤ −

E[

−

−

L
(cid:18)

(cid:18)

(cid:18)

(cid:18)

W )2
(cid:19)

(λt

(λt

λt
θ −

(θt+1, W t+1)
L
λt
W −
2
L
2
L
2
L
2

θ)2

cαt

cαt

−

−

C2α2
t

C2α2
t

(cid:19)

(cid:19)

(θt, W t)]

− L
(cid:19)
E
(cid:107)

2 +

gt
W (cid:107)

L
2

E
(cid:107)

gt
θ(cid:107)

2 +

L
2

(λt

θ)2σ

gt
E
θ(cid:107)
(cid:107)

2 +

L
2

C2α2

t σ,

(λt

W )2σ +

λt
θ

(cid:0)λt

W + L(λt

W )2(cid:1) (σ + E
gt
θ(cid:107)
(cid:107)

2)

L
2

gt
E
W (cid:107)
(cid:107)

2 + LC2σαt +

C2α2

t (1 + LCαt) (σ + E
(cid:107)

gt
θ(cid:107)

2)

L
2

inf t ct
W , inf t ct
where we denoted c = min
θ}
{
from Lemma 2, we also have E
ρ and E
M 2
2
gt
θ(cid:107)
≤
(cid:107)
(cid:107)
the right hand side of the ﬁnal inequality over t and using (C.18), we have:

supt ct
{
M 2
2
gt
W (cid:107)

W , supt ct
. Note that
ρ . Therefore, summing

, C = max

θ}

≤

lim
t→∞

L

E

(θt+1, W t+1)

E

(θ0, W 0)

−

L

αt

(cid:0)E
(cid:107)

gt
θ(cid:107)

gt
2 + E
W (cid:107)
(cid:107)

2(cid:1) .

(C.20)

c
≤ −

∞
(cid:88)

t=1

Since the objective function is lower bounded, this eﬀectively means:

∞
(cid:88)

t=1

αtE
(cid:107)

gt
θ(cid:107)

2 <

,
∞

gt
αtE
W (cid:107)
(cid:107)

2 <

.
∞

∞
(cid:88)

t=1

Finally, using Lemma 2, our assumptions and Jensen’s inequality, it follows that

•

(cid:12)
gt+1
2
(cid:12)E
W (cid:107)
(cid:107)

gt
E
W (cid:107)
(cid:107)

−

2(cid:12)
(cid:12)

≤

(cid:113)

2LMρCαt

2(M 2

ρ + σ2).

According to Proposition 1.2.4 of (Bertsekas, 1999), we have E
gt
θ(cid:107)
(cid:107)
as t

, and hence

2

0 and E
(cid:107)

gt
W (cid:107)

2

→

→

0

→ ∞

E

(θt, W t)

(cid:107)∇L

(cid:107) ≤

E

(cid:107)∇
2LC

W
(cid:113)

L
2(M 2

(θt, W t)

gt
gt
+ E
θ(cid:107)
θ(cid:107)
−
(cid:107)
(cid:107)∇
L
gt
gt
+ E
ρ + σ2)α + E
W (cid:107) →
θ(cid:107)
(cid:107)
(cid:107)

+ E

W

≤

0 as t

,
→ ∞

(θt, W t)

gt
θ(cid:107)

+ E
(cid:107)

gt
W (cid:107)

−

where the ﬁrst term of the last inequality follows from Lemma 2 and Jensen’s inequality.

29

Al-Shedivat, Wilson, Saatchi, Hu, Xing

C.2 Semi-stochastic gradient with delayed kernel matrix updates

We show that given a bounded delay on the kernel matrix updates, the algorithm is still
convergent. Our analysis is based on computing the change in δt
W and applying the
same argument as in Theorem 1. The only diﬀerence is that we need to take into account
the perturbations of the kernel matrix due to the introduced delays, and hence we have to
impose certain assumptions on its spectrum.

θ and δt

Assumption 4 Recurrent transformations, φW (¯x), is L-Lipschitz w.r.t. W for all ¯x

L:

∈ X

φ ˜W (¯x)

(cid:107)

−

φW (¯x)

(cid:107) ≤

˜W

L
(cid:107)

W

.

(cid:107)

−

Assumption 5 The kernel function, k(
derivatives are uniformly J-Lipschitz:

,
·

·

), is uniformly G-Lipschitz and its ﬁrst partial

(cid:13)
(cid:13)k(˜h1, h2)
(cid:13)

k(h1, h2)

−
(cid:13)
∂1k(˜h1, h2)
(cid:13)
(cid:13) ≤

−

(cid:13)
(cid:13)
(cid:13) ≤
J

(cid:107)

G

˜h1
(cid:107)

˜h1

−

,

(cid:13)
(cid:13)k(h1, ˜h2)
(cid:13)
h1
(cid:107)
(cid:13)
(cid:13)
(cid:13)∂2k(h1, h2)

−

k(h1, h2)

(cid:13)
(cid:13)
(cid:13) ≤
−
(cid:13)
∂2k(h1, ˜h2)
(cid:13)
(cid:13) ≤

G

J

˜h2
(cid:107)
˜h2
(cid:107)

−
h1

,

(cid:107)

h2

h2

,

.

(cid:107)

(cid:107)

−

−

(cid:13)
(cid:13)
(cid:13)∂1k(h1, h2)

Assumption 6 For any collection of data representations,
of the corresponding kernel matrix, K, is lower bounded by a positive constant γ > 0.

N
i=1, the smallest eigenvalue
}

hi

{

Note that not only the assumptions are relevant to practice, Assumptions 5 and 6 can be
also controlled by choosing the class of kernel functions used in the model. For example, the
smallest eigenvalue of the kernel matrix, K, can be controlled by the smoothing properties
of the kernel (Rasmussen and Williams, 2006).

Consider a particular stochastic step of Algorithm 2 at time t for a given mini-batch, xt,
assuming that the kernel was last updated τ steps ago. The stochastic gradient will take
the following form:

ˆgt
W =

1
Nt ∇

W

L

(θt+1, W t, Kt−τ ) =

(cid:88)

(cid:16)

1
2Nt

K−1

t−τ yy(cid:62)K−1

K−1
t−τ

t−τ −

(cid:17)

ij

(cid:18) ∂Kt−τ
∂W

(cid:19)

.

ij
(C.21)
We can deﬁne ˆδt
in order to enable the same
W −
argument as in Theorem 1. To do that, we simply need to understand eﬀect of perturbation
of the kernel matrix on ˜gt

gt
W and uniformly bound

ˆδt
W −

W = ˆgt

δt
W (cid:107)

i,j∈It

(cid:107)

W .

Lemma 3 Under the given assumptions, the following bound holds for all i, j = 1, . . . , N :

(cid:12)
(cid:16)
(cid:12)
(cid:12)
(cid:12)

K−1

t−τ yy(cid:62)K−1

t−τ −

(cid:17)

K−1
t−τ

(cid:16)

K−1

t yy(cid:62)K−1

K−1
t

t −

ij −

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤

ij

D2

y

(cid:107)

(cid:107)

2 + D,

(C.22)

where D = γ−1 +

2GLτ λσ√N

(cid:17)−1

.

(cid:16)

γ

−

30

Learning Scalable Deep Kernels with Recurrent Structure

Proof The diﬀerence between K−1
t
computed for W t−τ and the latter for W t. To prove the bound, we need multiple steps.

is simply due to that the former has been

t−τ and K−1

•

First, we need to bound the element-wise diﬀerence between Kt−τ and Kt. This is
done by using Assumptions 4 and 5 and the triangular inequality:

(Kt)ij
|

−

(Kt−τ )ij

φW t−τ (¯xi)

+

φW t(¯xj)

(cid:107)

(cid:107)

φW t−τ (¯xj)
(cid:107)

)

−

| ≤

≤

G (

φW t(¯xi)
(cid:107)
W t
2GL
τ
(cid:88)

(cid:107)

−
W t−τ

−
λt−τ +s
W

(cid:107)
ˆgt−τ +s
W

(cid:107)

= 2GL

(cid:107)

s=1
2GLτ λσ

≤

•

Next, since each element of the perturbed matrix is bounded by 2GLτ λσ, we can
bound its spectral norm as follows:

Kt

(cid:107)

−

Kt−τ

Kt

Kt−τ

(cid:107) ≤ (cid:107)

−

F
(cid:107)

≤

2GLτ λσ√N ,

which means that the minimal singular value of the perturbed matrix is at least
σ1 = γ

2GLτ λσ√N due to Assumption 6.

−

•

The spectral norm of the expression of interest can be bounded (quite pessimistically!)
by summing up together the largest eigenvalues of the matrix inverses:

t−τ yy(cid:62)K−1

K−1
t−τ

t−τ −
(cid:1) yy(cid:62) (cid:0)K−1

K−1
t

(cid:16)

K−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:0)K−1
(cid:13)
(cid:13)
(cid:18)

t−τ −
(cid:16)

t−τ −

K−1
t
(cid:17)−1(cid:19)2

γ−1 +

γ

2GLτ λσ√N

−

y

(cid:107)

(cid:107)

≤

≤

(cid:17)

(cid:16)

ij −

K−1

t yy(cid:62)K−1

t −

(cid:17)

K−1
t

(cid:1)

(cid:0)K−1

t−τ −

−

K−1
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ij
(cid:1)(cid:13)
(cid:13)
(cid:13)

−

2 + γ−1 +

(cid:16)

γ

2GLτ λσ√N

(cid:17)−1

.

Each element of a matrix is bounded by the largest eigenvalue.

Using Lemma 3, it is straightforward to extend Theorem 1 to Algorithm 2.

Theorem 2 Let
for θ and W being λt
scalars, such that

{

(θt, W t)
}
θαt and λt
θ = ct

be a sequence generated from Algorithm 2 with learning rates
W , αt are positive

W αt, respectively, where ct

W = ct

θ, ct

0 < inf
t

ct
{θ,W } ≤

sup
t

ct
{θ,W } <

∞

, αt

αt+1,

≥

αt = +

,
∞

∞
(cid:88)

t=1

∞
(cid:88)

t=1

α2

t < +

,
∞

Then, under Assumptions 1 through 6 and if σ = supt σt <

, we have

t.

∀
(C.23)

(C.24)

E

lim
t→∞

(cid:107)∇L

(θt, W t)

= 0.

(cid:107)

∞

31

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Proof The proof is identical to the proof of Theorem 1. The only diﬀerence is in the
following upper bound on the expected gradient error: E
2 + 1)Jρ,
(cid:107)
(cid:107)
where D is as given in Lemma 3.

δt
W (cid:107) ≤

σ + 2N D(D

(cid:107)

y

Remark 1 Even though the provided bounds are crude due to pessimistic estimates of the
perturbed kernel matrix spectrum15, we still see a ﬁne balance between the delay, τ , and the
learning rate, λ, as given in the expression for the D constant.

Appendix D. Details on the datasets

The datasets varied in the number of time steps (from hundreds to a million), input and
output dimensionality, and the nature of the estimation tasks.

D.1 Self-driving car

The following is description of the input and target time series used in each of the autonomous
driving tasks (dimensionality is given in parenthesis).

Car speed estimation:

– Features: GPS velocity (3), ﬁber gyroscope (3).

– Targets: speed measurements from the car speedometer.

Car yaw estimation:

– Features: acceleration (3), compass measurements (3).

– Targets: yaw in the car-centric frame.

•

•

•

Lane sequence prediction: Each lane was represented by 8 cubic polynomial
coeﬃcients [4 coeﬃcients for x (front) and 4 coeﬃcients for y (left) axes in the car-
centric frame]. Instead of predicting the coeﬃcients (which turned out to lead to
overall less stable results), we discretized the lane curves using 7 points (initial, ﬁnal
and 5 equidistant intermediate points).

– Features: lanes at a previous time point (16), GPS velocity (3), ﬁber gyroscope

(3), compass (3), steering angle (1).

– Targets: coordinates of the lane discretization points (7 points per lane resulting

in 28 total output dimensions).

15. Tighter bounds can be derived by inspecting the eﬀects of perturbations on speciﬁc kernels as well as

using more speciﬁc assumptions about the data distribution.

32

Learning Scalable Deep Kernels with Recurrent Structure

Table 5: Summary of the feedforward and recurrent neural architectures and the corresponding
hyperparameters used in the experiments. GP-based models used the same architectures as their
non-GP counterparts. Activations are given for the hidden units; vanilla neural nets used linear
output activations.

Name Data

Time lag Layers Units∗ Type Regularizer∗∗

Optimizer

NARX

RNN

LSTM

Actuator
Drives
GEF-power
GEF-wind
Car

Actuator
Drives
GEF-power
GEF-wind
Car

Actuator
Drives
GEF-power
GEF-wind
Car

32
16
48
48
128

32
16
48
48
128

32
16
48
48
128

1
1
1
1
1

1
1
1
1
1

1
1
2
1
2

256
128
256
16
128

64
64
16
32
128

256
128
256
64
64

ReLU

dropout(0.5)

Adam(0.01)

tanh

dropout(0.25),
rec_dropout(0.05)

Adam(0.01)

tanh

dropout(0.25),
rec_dropout(0.05)

Adam(0.01)

∗Each layer consisted of the same number of units given in the table.
∗∗rec_dropout denotes the dropout rate of the recurrent weights (Gal and Ghahramani, 2016b).

•

Estimation of the nearest front vehicle position: (x, y) coordinates in the
car-centric frame.

– Features: x and y at a previous time point (2), GPS velocity (3), ﬁber gyroscope

(3), compass (3), steering angle (1).

– Targets: x and y coordinates.

Appendix E. Neural architectures

Details on the best neural architectures used for each of the datasets are given in Table 5.

33

Al-Shedivat, Wilson, Saatchi, Hu, Xing

References

Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances

in Neural Information Processing Systems, pages 873–881, 2011.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with
gradient descent is diﬃcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.

Dimitri P Bertsekas. Nonlinear programming. Athena scientiﬁc Belmont, 1999.

George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series

analysis: forecasting and control. John Wiley & Sons, 1994.

Roberto Calandra, Jan Peters, Carl Edward Rasmussen, and Marc Peter Deisenroth. Mani-
fold gaussian processes for regression. In Neural Networks (IJCNN), 2016 International
Joint Conference on, pages 3338–3345. IEEE, 2016.

Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo.

In International Conference on Machine Learning, pages 1683–1691, 2014.

Andreas C Damianou and Neil D Lawrence. Deep gaussian processes. In AISTATS, pages

207–215, 2013.

Marc Peter Deisenroth and Jun Wei Ng. Distributed gaussian processes. In International

Conference on Machine Learning (ICML), volume 2, page 5, 2015.

Roger Frigola, Yutian Chen, and Carl Rasmussen. Variational gaussian process state-space
models. In Advances in Neural Information Processing Systems, pages 3680–3688, 2014.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing
model uncertainty in deep learning. In Proceedings of the 33rd International Conference
on Machine Learning, pages 1050–1059, 2016a.

Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in
recurrent neural networks. In Advances in Neural Information Processing Systems, pages
1019–1027, 2016b.

Alan Graves, Abdel-rahman Mohamed, and Geoﬀrey Hinton. Speech recognition with deep
recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013
IEEE International Conference on, pages 6645–6649. IEEE, 2013.

Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability
of stochastic gradient descent. In Proceedings of The 33rd International Conference on
Machine Learning, pages 1225–1234, 2016.

34

Learning Scalable Deep Kernels with Recurrent Structure

James Hensman, Nicolò Fusi, and Neil D Lawrence. Gaussian processes for big data. In
Proceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence,
pages 282–290. AUAI Press, 2013.

Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Using deep belief nets to learn covariance
kernels for gaussian processes. In Advances in neural information processing systems,
pages 1249–1256, 2008.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9

(8):1735–1780, 1997.

Tommi S Jaakkola and David Haussler. Exploiting generative models in discriminative
classiﬁers. Advances in neural information processing systems, pages 487–493, 1999.

Robert Keys. Cubic convolution interpolation for digital image processing. IEEE transactions

on acoustics, speech, and signal processing, 29(6):1153–1160, 1981.

Juš Kocijan, Agathe Girard, Blaž Banko, and Roderick Murray-Smith. Dynamic systems
identiﬁcation with gaussian processes. Mathematical and Computer Modelling of Dynamical
Systems, 11(4):411–424, 2005.

John Langford, Alex J Smola, and Martin Zinkevich. Slow learners are fast. Advances in

Neural Information Processing Systems, 22:2331–2339, 2009.

Miguel Lázaro-Gredilla. Bayesian warped gaussian processes.

In Advances in Neural

Information Processing Systems, pages 1619–1627, 2012.

Tsungnam Lin, Bil G Horne, Peter Tiňo, and C Lee Giles. Learning long-term dependencies in
narx recurrent neural networks. Neural Networks, IEEE Transactions on, 7(6):1329–1338,
1996.

David JC MacKay. Introduction to gaussian processes. NATO ASI Series F Computer and

Systems Sciences, 168:133–166, 1998.

César Lincoln C Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth, Guilherme A Bar-
reto, and Neil D Lawrence. Recurrent gaussian processes. arXiv preprint arXiv:1511.06644,
2015.

Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. The Journal of

Machine Learning Research, 7:2651–2667, 2006.

Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse
approximate gaussian process regression. The Journal of Machine Learning Research, 6:
1939–1959, 2005.

35

Al-Shedivat, Wilson, Saatchi, Hu, Xing

Carl Edward Rasmussen and Zoubin Ghahramani. Occam’s razor. Advances in neural

information processing systems, pages 294–300, 2001.

Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine

learning. The MIT Press, 2006.

Stéphane Ross, Geoﬀrey J Gordon, and Drew Bagnell. A reduction of imitation learning
and structured prediction to no-regret online learning. In AISTATS, volume 1, page 6,
2011.

Yunus Saatchi and Andrew Gordon Wilson. Bayesian gan. arXiv preprint arXiv:1705.09558,

2017.

Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines,

regularization, optimization, and beyond. MIT press, 2002.

Jonas Sjöberg, Qinghua Zhang, Lennart Ljung, Albert Benveniste, Bernard Delyon, Pierre-
Yves Glorennec, Håkan Hjalmarsson, and Anatoli Juditsky. Nonlinear black-box modeling
in system identiﬁcation: a uniﬁed overview. Automatica, 31(12):1691–1724, 1995.

Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped gaussian

processes. In NIPS, pages 337–344, 2003.

Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958, 2014.

ML Stein. Interpolation of spatial data, 1999.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural
networks. In Advances in neural information processing systems, pages 3104–3112, 2014.

Ryan D Turner, Marc P Deisenroth, and Carl E Rasmussen. State-space inference and
learning with gaussian processes. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 868–875, 2010.

Peter Van Overschee and Bart De Moor. Subspace identiﬁcation for linear systems: Theory

— Implementation — Applications. Springer Science & Business Media, 2012.

Jack Wang, Aaron Hertzmann, and David M Blei. Gaussian process dynamical models. In

Advances in neural information processing systems, pages 1441–1448, 2005.

Torbjörn Wigren. Input-output data sets for development and benchmarking in nonlinear
identiﬁcation. Technical Reports from the department of Information Technology, 20:
2010–020, 2010.

36

Learning Scalable Deep Kernels with Recurrent Structure

Andrew Wilson and Zoubin Ghahramani. Copula processes. In Advances in Neural Infor-

mation Processing Systems, pages 2460–2468, 2010.

Andrew Gordon Wilson and Ryan Prescott Adams. Gaussian process kernels for pattern
discovery and extrapolation. In Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pages 1067–1075, 2013.

Andrew Gordon Wilson and Hannes Nickisch. Kernel interpolation for scalable structured
gaussian processes (kiss-gp). In Proceedings of The 32nd International Conference on
Machine Learning, pages 1775–1784, 2015.

Andrew Gordon Wilson, Christoph Dann, and Hannes Nickisch. Thoughts on massively

scalable gaussian processes. arXiv preprint arXiv:1511.01870, 2015.

Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel
learning. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence
and Statistics, 2016a.

Andrew Gordon Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic
variational deep kernel learning. In Advances in Neural Information Processing Systems,
pages 2586–2594, 2016b.

D Randall Wilson and Tony R Martinez. The general ineﬃciency of batch training for

gradient descent learning. Neural Networks, 16(10):1429–1451, 2003.

Yangyang Xu and Wotao Yin. Block stochastic gradient iteration for convex and nonconvex

optimization. SIAM Journal on Optimization, 25(3):1686–1716, 2015.

37

