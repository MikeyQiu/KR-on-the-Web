LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich
Languages with BRNNs

Daniel Kondratyuk† and Tom´aˇs Gavenˇciak‡ and Milan Straka† and Jan Hajiˇc†
Charles University
Faculty of Mathematics and Physics
†Institute of Formal and Applied Linguistics, ‡Department of Applied Mathematics
dankondratyuk@gmail.com, gavento@kam.mff.cuni.cz, {straka,hajic}@ufal.mff.cuni.cz

Abstract

We present LemmaTag, a featureless neu-
ral network architecture that jointly generates
part-of-speech tags and lemmas for sentences
by using bidirectional RNNs with character-
level and word-level embeddings. We demon-
strate that both tasks beneﬁt from sharing the
encoding part of the network, predicting tag
subcategories, and using the tagger output as
an input to the lemmatizer. We evaluate our
model across several languages with complex
morphology, which surpasses state-of-the-art
accuracy in both part-of-speech tagging and
lemmatization in Czech, German, and Arabic.

1

Introduction

Morphologically rich languages are often difﬁcult
to process in many NLP tasks (Tsarfaty et al.,
2010). As opposed to analytical languages like
English, morphologically rich languages encode
diverse sets of grammatical information within
each word using inﬂections, which convey char-
acteristics such as case, gender, and tense. The
inﬂectional variants across
addition of several
many words dramatically increases the vocabu-
lary size, which results in data sparsity and out-
of-vocabulary (OOV) issues.

Due to these issues, morphological part-of-
speech (POS) tagging and lemmatization are heav-
ily used in NLP tasks such as machine transla-
tion (Fraser et al., 2012) and sentiment analysis
(Abdul-Mageed et al., 2014). In morphologically
rich languages, the POS tags typically consist of
multiple morpho-syntactic subcategories provid-
ing additional information (see Figure 1). Closely
related to POS tagging is lemmatization, which in-
volves transforming each word to its root or dic-
tionary form. Both tasks require context-sensitive
awareness to disambiguate words with the same
form but different syntactic or semantic features
and behavior. Furthermore, lemmatization of a

word form can beneﬁt substantially from the in-
formation present in morphological tags, as gram-
matical attributes often disambiguate word forms
using context (M¨uller et al., 2015).

We address context-sensitive POS tagging and
lemmatization using a neural network model that
jointly performs both tasks on each input word in
a given sentence.1 We train the model in a super-
vised fashion, requiring training data containing
word forms, lemmas, and POS tags. In addition,
we incorporate the ideas from Inoue et al. (2017)
to optionally allow the network to predict the sub-
categories of each tag to improve accuracy. Our
model is related to the work of M¨uller et al. (2015),
which use conditional random ﬁelds (CRF) to
jointly tag and lemmatize words for morphologi-
cally rich languages. The idea of jointly predict-
ing several dimensions of categories has been ex-
plored prior to this work, for example, joint mor-
phological and syntactic analysis (Bohnet et al.,
2013) or joint parsing and semantic role labeling
(Gesmundo et al., 2009).

Our model consists of three parts:

(1) The
shared encoder, which creates an internal repre-
sentation for every word based on its character se-

1The code for this project is available at https://

github.com/hyperparticle/LemmaTag

Figure 1: The tag components of the PDT Czech tree-
bank with the numbers of valid values. Around 1500
different tags are in use in the PDT.

quence and the sentence context. We adopt the
encoder architecture of Chakrabarty et al. (2017),
utilizing character-level (Heigold et al., 2017) and
word-level embeddings (Mikolov et al., 2013b;
Santos and Zadrozny, 2014) processed through
several
layers of bidirectional recurrent neural
networks (BRNN/BiRNN) (Schuster and Paliwal,
1997; Chakrabarty et al., 2017). (2) The tagger
decoder, which applies a fully-connected layer to
the outputs of the shared encoder to predict the
POS tags. (3) The lemmatizer decoder, which
applies an RNN sequence decoder to the combined
outputs of the shared encoder and tagger decoder,
producing a sequence of characters that predict
each lemma (similar to Bergmanis and Goldwater
(2018)).

The main advantages over other proposed mod-
els are: (i) The model is featureless, requiring
little to no text preprocessing or morphological
(ii) The model shares
analysis postprocessing.
the word embeddings, character embeddings, and
RNN encoder weights in the tagger and lemma-
tizer, improving both tagging and lemmatization
accuracy while reducing the number of parameters
required for both tasks. (iii) The model predicts
tag subcategories and provides the output of the
tagger as features for the input of the lemmatizer,
further improving accuracy.

We evaluate the accuracy of our model in POS
tagging and lemmatization across several
lan-
guages: Czech, Arabic, German, and English. For
each language, we also compare the performance
of a fully separate tagger and lemmatizer to the
proposed joint model. Our results show that our
joint model is able to improve the accuracy for
both tasks, and achieves state-of-the-art perfor-
mance in both POS tagging and lemmatization in
Czech, German, and Arabic, while closely match-
ing state-of-the-art performance for English.

2 The Joint LemmaTag Model

Given a sequence of words
in a sentence
w1, . . . , wk, the task of the model is to produce a
sequence of associated tags t1, . . . , tk and lemmas
(cid:96)1, . . . , (cid:96)k. For a word wi at position i, we denote
ci,1, ci,2 . . . ci,mi to be the sequence of characters
that make up wi, where mi indicates the length of
the word string at position i. Analogously, we de-
ﬁne li,1, . . . li,λi to be the sequence of characters
that make up the lemma (cid:96)i.

Our proposed model (shown in Figures 2 and 3)

is split into three parts:
the shared encoder, the
tagger, and the lemmatizer. The initial layers
of the model are shared between the tagger and
lemmatizer, encoding the words, characters, and
context in a given sentence. The encoder then
passes its outputs to two networks, which perform
a classiﬁcation task to predict tags by the tagger
and a sequence prediction task to output lemmas
(character-by-character) in the lemmatizer.

2.1 Shared Encoder

1, . . . , ec

In the encoder shown in Figure 2, each charac-
ter ci,1, ci,2 . . . ci,mi of a word wi is indexed into
an embedding layer to produce ﬁxed-length em-
bedded vectors representing each character. These
vectors are further passed into a layer of BRNNs
composed of gated recurrent units (GRU) (Cho
et al., 2014) producing outputs ec
m, and
whose ﬁnal states are concatenated to produce the
character-level embedding sc
i of the word. Sim-
ilarly, we index wi into a word-level embedding
layer to compute vector eb
i . Then we sum these re-
sults to produce the ﬁnal word embedding ew
i =
i + eb
sc
i .
We repeat this process independently for all
the words in the sentence and feed the resulting
sequence ew
k into another two BRNN lay-
ers composed of long short-term memory units
(LSTM) with residual connections. This pro-
duces word-level outputs ow
k that encode
sentence-level context for each word (we ignore
the ﬁnal hidden states).

1 , . . . ow

1 . . . ew

2.2 Tagger

The task of the tagger is to predict a tag ti ∈ T
given a word wi and its context, where T is a set
of possible tags. As explained the introduction,
morphologically rich languages typically subdi-
vide tags further into several subcategories ti =
(ti,1, . . . , ti,τ ), where ti,j ∈ Tj, the j-th subcate-
gory. See Figure 1 for an illustration taken from
the Czech PDT tagset where τ = 15.

Having the encoded words of a sentence avail-
able, the tagger consists of a fully-connected layer
with |T | neurons whose input is the output of the
word feature RNN ow
(see ﬁgure 2). This layer
i
produces the logits ti of the tag values and the pre-
dictions ti as the maximum-likelihood value (i.e.,
softmax).

To obtain the information about categorical na-
ture of each tag, we also predict every category
ti,j of the tag independently (if they exist in the

— — — — — — — — — — —

Figure 2: Bottom: Word-level encoder. The characters
of every input word are embedded with a look-up table
(c-embed) and encoded with a BRNN. The outputs ec
i,j
are used in decoder attention, and the ﬁnal states are
summed with the word-level embedding (w-embed) to
produce ew
Top: Sentence-level encoder and tag classiﬁer. Two
BRNN layers with residual connections act on the em-
bedded words ew
i of a sentence, providing context. The
output of the tag classiﬁcation are the logits for both
the whole tags ti and their components ti,j.
Both: Thick slanted lines denote training dropout.

i . WD denotes word dropout.

dataset) with τ dense layers similar to Inoue et al.
(2017). The j-th layer has |Tj| neurons and out-
puts the logits ti,j for the category values. While
these values are trained for,
their value is not
used in tag prediction. All tag values Ti =
(ti, ti,1 . . . , ti,τ ) are concatenated into a ﬂat vec-
tor and fed into the lemmatizer as an additional set
of potentially useful features.

2.3 Lemmatizer

The task of the lemmatizer is to produce a se-
quence of characters li,1, . . . , li,λi and the lemma
length λi for each lemma (cid:96)i. We use a recurrent se-
quence decoder, a setup typical of many sequence-
to-sequence (seq2seq) tasks such as in neural ma-

Figure 3: Lemma decoder, consisting of a standard
seq2seq autoregressive decoder with Luong attention
on character encodings, and with additional inputs
of processed tagger features Ti, embeddings ew
i and
sentence-level outputs ow
i . Gradients from the lemma-
tizer are stopped from ﬂowing into the tagger (denoted
GradStop).

chine translation (Sutskever et al., 2014).

The lemmatizer consists of a recurrent LSTM
layer whose initial state is taken from word-level
output ow
i and whose inputs consist of three parts.
The ﬁrst part is the embedding of the previous out-
put character (initially a beginning-of-word char-
acter BOW).

i,1, . . . , ec

The second part is a character-level attention
mechanism (Bahdanau et al., 2014) on the out-
puts of the character-level BRNN ec
i,mi.
We employ the multiplicative attention mecha-
nism described in Luong et al. (2015), which al-
lows the LSTM cell to compute an attention vec-
tor that selectively weights character-level infor-
mation in ec
i,j at each time step j based on the input
state of the LSTM cell.

The third and ﬁnal part of the RNN input allows
the network to receive the information about the
embedding of the word, the surrounding context
of the sentence, and the output of the tagger. This
output is the same for all time steps of a lemma
and is a concatenation of the following: the out-
put of the encoder ow
i and
processed tag features Tf
i . The tag features are ob-
tained by projecting the concatenated outputs of
the tagger Ti through a fully connected layer with
ReLU activation. During training, we do not pass
the gradients back through Ti to prevent the dis-
tortion of the tagger output.

i , the embedded word ew

The decoder performs greedy decoding to pre-
dict the character outputs. It runs until it produces

the end-of-word character EOW or reaches a char-
acter limit of mi + 10.

2.4 Loss Function

lemmas including the senses and the accuracies in
Table 1 take that into account. Ignoring the sense
ambiguity, the lemmatization accuracy of the joint
LemmaTag model is 98.94% for Czech-PDT.

We deﬁne the ﬁnal loss function as the weighted
sum of the losses of the tagger and the lemmatizer:

3.2 Hyperparameters

L(ˆy, y) = αL(ˆyt, yt) + βL(ˆy(cid:96), y(cid:96))

where y are the predicted outputs, ˆy the expected
outputs, yt, the tag components and y(cid:96) are the
lemma characters. The tagger and lemmatizer
losses are separately computed as the softmax
cross entropy of the output logits. The weight hy-
perparameters α, β scale the training losses so that
the subtag and lemmatizer losses do not overpower
the unfactored tag predictor gradients. The vector
α contains τ + 1 weights: one for the whole tag
and one for every component.2

3 Experiments

In this section, we show the outcomes of evalu-
ation when running our joint tagger and lemma-
tizer and compare with the current state of the art
in Czech, German, Arabic, and English datasets.
Additionally, we evaluate the lemmatizer and tag-
ger separately to compare the relative increase in
tagging and lemmatization accuracy.

3.1 Datasets

Our datasets consist of the Czech Prague De-
pendency Treebank (PDT) (Hajiˇc et al., 2006,
2018), the German TIGER corpus (Brants et al.,
2004), the Universal Dependencies Prague Arabic
Dependency Treebank (UD-PADT) (Hajic et al.,
2004), the Universal Dependencies English Web
Treebank (UD-EWT) (Silveira et al., 2014), and
the WSJ portion of the English Penn Treebank
(tags only) (Marcus et al., 1993). In all datasets,
we use the tags speciﬁc to their respective lan-
guage. Of these datasets, only Czech and Ara-
bic provide subcategorical tags, and we use unfac-
tored tags for the rest. See Table 1 for tagger and
lemmatizer accuracies.

Note that the PDT dataset disambiguates lem-
mas with the same textual representation by ap-
pending a number as lemma sense indicator. For
example, the dataset contains disambiguated lem-
mas moc-1 (as power) and moc-2 (as too much).
About 17.5% of the PDT tokens have such sense-
disambiguated lemmas. LemmaTag predicts the

2If no components are available, τ = 0.

We use loss weights α0 = 1.0 for the whole tags,
α1,...,τ = 0.1 for the tag component losses and
β = 0.5 for the lemmatizer loss.3 The RNNs
and word embedding tables have dimensionality
768 except for character-level embeddings and the
character-level RNN, which are of dimension 384.
The fully-connected layer whose inputs are Ti is
of dimension 256.

We train the models for 40 epochs with random
permutations of training sentences and batches of
16 sentences. The starting learning rate is η =
0.001 and we scale this by 0.25 at epochs 20
and 30 to increase accuracy. We train the net-
work using the lazy variant of the Adam optimizer
(Kingma and Ba, 2014), which only updates ac-
cumulators for variables that appear in the cur-
rent batch (TensorFlow, 2018), with parameters
β1 = 0.9 and β2 = 0.99. We clip the global gra-
dient norm to 3.0 to reduce the risk of exploding
gradients.

To prevent the tagger from overﬁtting, we de-
vise several strategies for regularization. We apply
dropouts with rate 0.5 as indicated in Figures 2
and 3. The word dropout (WD) replaces 25% of
words by the unknown token <unk> to force the
network to rely more on context, combatting data
sparsity issues. Lastly, we employ label smooth-
ing (Pereyra et al., 2017) which is a way to pre-
vent the network from being too conﬁdent in any
one class. The label smoothing parameter is set to
0.1 for the tagger logits (both whole tags and the
tag components).

Note that we did not perform any complex hy-
perparameter search. For additional information
on real-world performance and additional tech-
niques which have not improved evaluation accu-
racy, see Appendix A.

4 Conclusion

The evaluation results show that performing
lemmatization and tagging jointly by sharing en-
coder parameters and utilizing tag features is

3These are reasonable values to prevent gradients from
overpowering one another. The lemmatizer tends to inﬂuence
the tagger heavily.

Approach

Czech-PDT∗ German-TIGER Arabic-PADT∗
tag
LemmaTag (sep)
96.83
LemmaTag (joint) 96.90
95.89a
SoTA results

tag
95.03
95.21
+ 91.68d

tag
98.96
98.97
+ 98.04c

lem
98.84
99.05
+ 98.24c

lem
98.02
98.37
+ 97.86b

lem
96.07
96.08

Eng-EWT
lem
97.03
97.53 N/A

tag
95.50
95.37

Eng-WSJ
tag
97.59

+ 92.60e 93.90e 96.90e 97.78f

+

Table 1: Final accuracies on the test sets comparing the LemmaTag architecture as described (joint), LemmaTag
neither sharing the encoder nor providing tagger features (sep), and the state-of-the-art results (SoTA). The state-
of-the-art results are taken from the following papers: (a) Hajiˇc et al. (2009), (b) Strakov´a et al. (2014), (c) Eger
et al. (2016), (d) Inoue et al. (2017), (e) Straka et al. (2016), (f) Ling et al. (2015). The results marked with a
plus+ use additional resources apart from the dataset, and datasets marked with a star∗ indicate the availability of
subcategorical tags.

mutually beneﬁcial in morphologically rich lan-
guages. We have shown that incorporating these
ideas results in excellent performance, surpass-
ing state-of-the-art in Czech, German, and Ara-
bic POS tagging and lemmatization by a substan-
tial margin, while closely matching state-of-the-
art English POS tagging accuracy.

However,

in languages with weak morphol-
ogy such as English (and German to a lesser ex-
tent), sharing the encoder parameters may even
hurt the performance of the tagger. We believe
this is a consequence of tags correlating less with
word-level morphology, and more with sentence-
level syntax in morphologically poor languages.
Lemma prediction could beneﬁt from the syntac-
tic information in the tags, but the tag predictions
rely more on syntactic structure (i.e., word or-
der) rather than on root forms of individual words
which could be ambiguous.

There are some possible performance improve-
ments and additional metrics which we leave for
future work. For simplicity, one improvement
we intentionally left out is the use of additional
data. We can incorporate word2vec (Mikolov
et al., 2013a) or ELMo (Peters et al., 2018) word
representations, which have shown to reduce out-
of-domain issues and provide semantic informa-
tion (Eger et al., 2016). A second improvement
is to integrate information from a morphological
dictionary to resolve certain ambiguities (Hajiˇc
et al., 2009; Inoue et al., 2017). A third im-
provement can be to replace the seq2seq lemma-
tizer decoder with a classiﬁer that chooses a cor-
responding edit tree to modify (reduce) the word
form to its lemma (Chakrabarty et al., 2017). A
fourth possible improvement would be to experi-
ment with the Transformer model (Vaswani et al.,
2017), which utilizes non-recurrent multi-headed

self-attention and has been shown to achieve state-
of-the-art performance in several related sequence
tasks (Dehghani et al., 2018). Lastly, we would
like to evaluate LemmaTag on a wider range of
languages, e.g., on the Universal Dependencies
(Nivre et al., 2016) languages and treebanks which
employ lemmatization, and to analyze the use of
different types of POS tags in the model.

The code we used for LemmaTag is available at
https://github.com/hyperparticle/
LemmaTag.

Acknowledgments

The work described herein has been supported by
the City of Prague under the “OP PPR” program,
project No. CZ.07.1.02/0.0/0.0/16 023/0000108
and it has been using language resources devel-
oped by the LINDAT/CLARIN project of the Min-
istry of Education, Youth and Sports of the Czech
Republic (project LM2015071).

Tom´aˇs Gavenˇciak has been supported by Czech
Science Foundation (GACR) project 17-10090Y
“Network optimization”. Daniel Kondratyuk has
been supported by the Erasmus Mundus pro-
gram in Language & Communication Technolo-
gies (LCT).

References

Muhammad Abdul-Mageed, Mona Diab, and Sandra
K¨ubler. 2014. Samar: Subjectivity and sentiment
analysis for arabic social media. Computer Speech
& Language, 28(1):20–37.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Toms Bergmanis and Sharon Goldwater. 2018. Con-
text sensitive neural lemmatization with lematus. In

Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), volume 1, pages 1391–
1400.

Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richrd
Farkas, Filip Ginter, and Ja n Haji. 2013. Joint mor-
phological and syntactic analysis for richly inﬂected
languages. Transactions of the Association for Com-
putational Linguistics, 1:415–428.

Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen-Schirra, Esther K¨onig, Wolfgang Lezius,
Christian Rohrer, George Smith, and Hans Uszkor-
eit. 2004. Tiger: Linguistic interpretation of a ger-
man corpus. Research on language and computa-
tion, 2(4):597–620.

Abhisek Chakrabarty, Onkar Arun Pandit, and Utpal
Garain. 2017. Context sensitive lemmatization us-
ing two successive bidirectional gated recurrent net-
In Proceedings of the 55th Annual Meet-
works.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1481–
1491.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Learning
Schwenk, and Yoshua Bengio. 2014.
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Łukasz Kaiser. 2018. Univer-
sal transformers. arXiv preprint arXiv:1807.03819.

Steffen Eger, R¨udiger Gleim, and Alexander Mehler.
2016. Lemmatization and morphological tagging in
german and latin: A comparison and a survey of the
state-of-the-art. In LREC.

Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inﬂection and word-
formation in smt. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 664–674. As-
sociation for Computational Linguistics.

Andrea Gesmundo, James Henderson, Paola Merlo,
and Ivan Titov. 2009. A latent variable model of
synchronous syntactic-semantic parsing for multi-
In Proceedings of the Thirteenth
ple languages.
Conference on Computational Natural Language
Learning: Shared Task, CoNLL ’09, pages 37–42,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Jan Hajiˇc, Eduard Bejˇcek, Alevtina B´emov´a, Eva
Bur´aˇnov´a, Eva Hajiˇcov´a, Jiˇr´ı Havelka, Petr Ho-
Jiˇr´ı K´arn´ık, V´aclava Kettnerov´a, Na-
mola,
talia Klyueva, Veronika Kol´aˇrov´a, Lucie Kuˇcov´a,
Mark´eta Lopatkov´a, Marie Mikulov´a, Jiˇr´ı M´ırovsk´y,
Anna Nedoluzhko, Petr Pajas, Jarmila Panevov´a,

Lucie Pol´akov´a, Magdal´ena Rysov´a, Petr Sgall, Jo-
hanka Spoustov´a, Pavel Straˇn´ak, Pavl´ına Synkov´a,
Magda ˇSevˇc´ıkov´a, Jan ˇStˇep´anek, Zdeˇnka Ureˇsov´a,
ˇS´arka
Barbora Vidov´a Hladk´a, Daniel Zeman,
Zik´anov´a, and Zdenˇek ˇZabokrtsk´y. 2018. Prague
dependency treebank 3.5. LINDAT/CLARIN digital
library at the Institute of Formal and Applied Lin-
guistics ( ´UFAL, http://hdl.handle.net/
11234/1-2621), Faculty of Mathematics and
Physics, Charles University.

Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie
Mikulov´a, Zdenˇek ˇZabokrtsk´y, Magda ˇSevˇc´ıkov´a-
Raz´ımov´a, and Zdeˇnka Ureˇsov´a. 2006.
Prague
Dependency Treebank 2.0 (PDT 2.0).
LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics ( ´UFAL), Faculty of
Mathematics and Physics, Charles University.

Jan Hajiˇc, Jan Raab, Miroslav Spousta, et al. 2009.
Semi-supervised training for the averaged percep-
In Proceedings of the 12th Con-
tron pos tagger.
ference of the European Chapter of the Association
for Computational Linguistics, pages 763–771. As-
sociation for Computational Linguistics.

Jan Hajic, Otakar Smrz, Petr Zem´anek, Jan ˇSnaidauf,
and Emanuel Beˇska. 2004. Prague arabic depen-
dency treebank: Development in data and tools. In
Proc. of the NEMLAR Intern. Conf. on Arabic Lan-
guage Resources and Tools, pages 110–117.

Georg Heigold, Guenter Neumann, and Josef van Gen-
abith. 2017. An extensive empirical evaluation of
character-based morphological tagging for 14 lan-
In Proceedings of the 15th Conference of
guages.
the European Chapter of the Association for Com-
putational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 505–513.

Go Inoue, Hiroyuki Shindo, and Yuji Matsumoto.
Joint prediction of morphosyntactic cate-
2017.
gories for ﬁne-grained arabic part-of-speech tagging
In Proceed-
exploiting tag dictionary information.
ings of the 21st Conference on Computational Natu-
ral Language Learning (CoNLL 2017), pages 421–
431.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Wang Ling, Tiago Lu´ıs, Lu´ıs Marujo, Ram´on Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015. Finding function
in form: Compositional character models for open
arXiv preprint
vocabulary word representation.
arXiv:1508.02096.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efﬁcient estimation of word
arXiv preprint
representations in vector space.
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

Thomas M¨uller, Ryan Cotterell, Alexander Fraser, and
Hinrich Sch¨utze. 2015.
Joint lemmatization and
morphological tagging with lemming. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 2268–2274.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016), Paris, France. European
Language Resources Association (ELRA).

Gabriel Pereyra, George Tucker,

Jan Chorowski,
Łukasz Kaiser, and Geoffrey Hinton. 2017. Regular-
izing neural networks by penalizing conﬁdent output
distributions. arXiv preprint arXiv:1701.06548.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Cicero D Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech
In Proceedings of the 31st International
tagging.
Conference on Machine Learning (ICML-14), pages
1818–1826.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Natalia Silveira, Timothy Dozat, Marie-Catherine
de Marneffe, Samuel Bowman, Miriam Connor,
John Bauer, and Christopher D. Manning. 2014. A
gold standard dependency corpus for English.
In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC-
2014).

Milan Straka, Jan Hajic, and Jana Strakov. 2016. Ud-
pipe: Trainable pipeline for processing conll-u ﬁles
performing tokenization, morphological analysis,
pos tagging and parsing. In Proceedings of the Tenth
International Conference on Language Resources
and Evaluation (LREC 2016), Paris, France. Euro-
pean Language Resources Association (ELRA).

Jana Strakov´a, Milan Straka, and Jan Hajiˇc. 2014.
Open-Source Tools for Morphology, Lemmatiza-
tion, POS Tagging and Named Entity Recognition.
In Proceedings of 52nd Annual Meeting of the As-
sociation for Computational Linguistics: System
Demonstrations, pages 13–18, Baltimore, Mary-
land. Association for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

TensorFlow. 2018.

tf.contrib.opt.lazyadamoptimizer:
Class lazyadamoptimizer. TensorFlow documenta-
tion from tensorﬂow.org.

Reut Tsarfaty, Djam´e Seddah, Yoav Goldberg, San-
dra K¨ubler, Marie Candito, Jennifer Foster, Yannick
Versley, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical parsing of morphologically rich languages
(spmrl): what, how and whither. In Proceedings of
the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
1–12. Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

A Appendix

A.1 GPU Performance

We ran all the tests on an NVIDIA GTX 1080 Ti
GPU. The joint LemmaTag training takes about
3 hours for Arabic PADT, 4.5 hours for English
EWT, 12 hours for German TIGER, and 22 hours
for Czech PDT. The separate models take about
50% more time. After training, the lemma and
tag predictions of 219,000 test tokens of the Czech
PDT take about 100 seconds.

A.2 Other Techniques

We brieﬂy summarize some of the additional tech-
niques we have tried but which do not improve the
results. While some of those techniques do help on
smaller models or earlier in the training, the effect
on the fully trained network seems to be marginal
or even detrimental.

Separate sense prediction.

Instead of predict-
ing the sense disambiguation with the lemmatizer
(Czech only), we tried to predict sense as an addi-
tional classiﬁcation problem with one dense layer
based on ow
i and Ti, but it seems to perform
slightly worse (0.2%).

Beam search decoder. We have implemented a
beam search decoder for the lemmatizer instead of

the standard greedy one, but the improvement was
marginal (around 0.01%).

Variational dropout. While the dropouts in
the LemmaTag are completely random, variational
dropout erases the same channels across the time
steps of the RNN. While this generally improves
training in convolutional networks and RNNs, we
saw no signiﬁcant difference.

Layer normalization. Layer normalization ap-
plied to the encoding RNNs did not bring signiﬁ-
cant gain and also slowed down the training.

LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich
Languages with BRNNs

Daniel Kondratyuk† and Tom´aˇs Gavenˇciak‡ and Milan Straka† and Jan Hajiˇc†
Charles University
Faculty of Mathematics and Physics
†Institute of Formal and Applied Linguistics, ‡Department of Applied Mathematics
dankondratyuk@gmail.com, gavento@kam.mff.cuni.cz, {straka,hajic}@ufal.mff.cuni.cz

Abstract

We present LemmaTag, a featureless neu-
ral network architecture that jointly generates
part-of-speech tags and lemmas for sentences
by using bidirectional RNNs with character-
level and word-level embeddings. We demon-
strate that both tasks beneﬁt from sharing the
encoding part of the network, predicting tag
subcategories, and using the tagger output as
an input to the lemmatizer. We evaluate our
model across several languages with complex
morphology, which surpasses state-of-the-art
accuracy in both part-of-speech tagging and
lemmatization in Czech, German, and Arabic.

1

Introduction

Morphologically rich languages are often difﬁcult
to process in many NLP tasks (Tsarfaty et al.,
2010). As opposed to analytical languages like
English, morphologically rich languages encode
diverse sets of grammatical information within
each word using inﬂections, which convey char-
acteristics such as case, gender, and tense. The
inﬂectional variants across
addition of several
many words dramatically increases the vocabu-
lary size, which results in data sparsity and out-
of-vocabulary (OOV) issues.

Due to these issues, morphological part-of-
speech (POS) tagging and lemmatization are heav-
ily used in NLP tasks such as machine transla-
tion (Fraser et al., 2012) and sentiment analysis
(Abdul-Mageed et al., 2014). In morphologically
rich languages, the POS tags typically consist of
multiple morpho-syntactic subcategories provid-
ing additional information (see Figure 1). Closely
related to POS tagging is lemmatization, which in-
volves transforming each word to its root or dic-
tionary form. Both tasks require context-sensitive
awareness to disambiguate words with the same
form but different syntactic or semantic features
and behavior. Furthermore, lemmatization of a

word form can beneﬁt substantially from the in-
formation present in morphological tags, as gram-
matical attributes often disambiguate word forms
using context (M¨uller et al., 2015).

We address context-sensitive POS tagging and
lemmatization using a neural network model that
jointly performs both tasks on each input word in
a given sentence.1 We train the model in a super-
vised fashion, requiring training data containing
word forms, lemmas, and POS tags. In addition,
we incorporate the ideas from Inoue et al. (2017)
to optionally allow the network to predict the sub-
categories of each tag to improve accuracy. Our
model is related to the work of M¨uller et al. (2015),
which use conditional random ﬁelds (CRF) to
jointly tag and lemmatize words for morphologi-
cally rich languages. The idea of jointly predict-
ing several dimensions of categories has been ex-
plored prior to this work, for example, joint mor-
phological and syntactic analysis (Bohnet et al.,
2013) or joint parsing and semantic role labeling
(Gesmundo et al., 2009).

Our model consists of three parts:

(1) The
shared encoder, which creates an internal repre-
sentation for every word based on its character se-

1The code for this project is available at https://

github.com/hyperparticle/LemmaTag

Figure 1: The tag components of the PDT Czech tree-
bank with the numbers of valid values. Around 1500
different tags are in use in the PDT.

quence and the sentence context. We adopt the
encoder architecture of Chakrabarty et al. (2017),
utilizing character-level (Heigold et al., 2017) and
word-level embeddings (Mikolov et al., 2013b;
Santos and Zadrozny, 2014) processed through
several
layers of bidirectional recurrent neural
networks (BRNN/BiRNN) (Schuster and Paliwal,
1997; Chakrabarty et al., 2017). (2) The tagger
decoder, which applies a fully-connected layer to
the outputs of the shared encoder to predict the
POS tags. (3) The lemmatizer decoder, which
applies an RNN sequence decoder to the combined
outputs of the shared encoder and tagger decoder,
producing a sequence of characters that predict
each lemma (similar to Bergmanis and Goldwater
(2018)).

The main advantages over other proposed mod-
els are: (i) The model is featureless, requiring
little to no text preprocessing or morphological
(ii) The model shares
analysis postprocessing.
the word embeddings, character embeddings, and
RNN encoder weights in the tagger and lemma-
tizer, improving both tagging and lemmatization
accuracy while reducing the number of parameters
required for both tasks. (iii) The model predicts
tag subcategories and provides the output of the
tagger as features for the input of the lemmatizer,
further improving accuracy.

We evaluate the accuracy of our model in POS
tagging and lemmatization across several
lan-
guages: Czech, Arabic, German, and English. For
each language, we also compare the performance
of a fully separate tagger and lemmatizer to the
proposed joint model. Our results show that our
joint model is able to improve the accuracy for
both tasks, and achieves state-of-the-art perfor-
mance in both POS tagging and lemmatization in
Czech, German, and Arabic, while closely match-
ing state-of-the-art performance for English.

2 The Joint LemmaTag Model

Given a sequence of words
in a sentence
w1, . . . , wk, the task of the model is to produce a
sequence of associated tags t1, . . . , tk and lemmas
(cid:96)1, . . . , (cid:96)k. For a word wi at position i, we denote
ci,1, ci,2 . . . ci,mi to be the sequence of characters
that make up wi, where mi indicates the length of
the word string at position i. Analogously, we de-
ﬁne li,1, . . . li,λi to be the sequence of characters
that make up the lemma (cid:96)i.

Our proposed model (shown in Figures 2 and 3)

is split into three parts:
the shared encoder, the
tagger, and the lemmatizer. The initial layers
of the model are shared between the tagger and
lemmatizer, encoding the words, characters, and
context in a given sentence. The encoder then
passes its outputs to two networks, which perform
a classiﬁcation task to predict tags by the tagger
and a sequence prediction task to output lemmas
(character-by-character) in the lemmatizer.

2.1 Shared Encoder

1, . . . , ec

In the encoder shown in Figure 2, each charac-
ter ci,1, ci,2 . . . ci,mi of a word wi is indexed into
an embedding layer to produce ﬁxed-length em-
bedded vectors representing each character. These
vectors are further passed into a layer of BRNNs
composed of gated recurrent units (GRU) (Cho
et al., 2014) producing outputs ec
m, and
whose ﬁnal states are concatenated to produce the
character-level embedding sc
i of the word. Sim-
ilarly, we index wi into a word-level embedding
layer to compute vector eb
i . Then we sum these re-
sults to produce the ﬁnal word embedding ew
i =
i + eb
sc
i .
We repeat this process independently for all
the words in the sentence and feed the resulting
sequence ew
k into another two BRNN lay-
ers composed of long short-term memory units
(LSTM) with residual connections. This pro-
duces word-level outputs ow
k that encode
sentence-level context for each word (we ignore
the ﬁnal hidden states).

1 , . . . ow

1 . . . ew

2.2 Tagger

The task of the tagger is to predict a tag ti ∈ T
given a word wi and its context, where T is a set
of possible tags. As explained the introduction,
morphologically rich languages typically subdi-
vide tags further into several subcategories ti =
(ti,1, . . . , ti,τ ), where ti,j ∈ Tj, the j-th subcate-
gory. See Figure 1 for an illustration taken from
the Czech PDT tagset where τ = 15.

Having the encoded words of a sentence avail-
able, the tagger consists of a fully-connected layer
with |T | neurons whose input is the output of the
word feature RNN ow
(see ﬁgure 2). This layer
i
produces the logits ti of the tag values and the pre-
dictions ti as the maximum-likelihood value (i.e.,
softmax).

To obtain the information about categorical na-
ture of each tag, we also predict every category
ti,j of the tag independently (if they exist in the

— — — — — — — — — — —

Figure 2: Bottom: Word-level encoder. The characters
of every input word are embedded with a look-up table
(c-embed) and encoded with a BRNN. The outputs ec
i,j
are used in decoder attention, and the ﬁnal states are
summed with the word-level embedding (w-embed) to
produce ew
Top: Sentence-level encoder and tag classiﬁer. Two
BRNN layers with residual connections act on the em-
bedded words ew
i of a sentence, providing context. The
output of the tag classiﬁcation are the logits for both
the whole tags ti and their components ti,j.
Both: Thick slanted lines denote training dropout.

i . WD denotes word dropout.

dataset) with τ dense layers similar to Inoue et al.
(2017). The j-th layer has |Tj| neurons and out-
puts the logits ti,j for the category values. While
these values are trained for,
their value is not
used in tag prediction. All tag values Ti =
(ti, ti,1 . . . , ti,τ ) are concatenated into a ﬂat vec-
tor and fed into the lemmatizer as an additional set
of potentially useful features.

2.3 Lemmatizer

The task of the lemmatizer is to produce a se-
quence of characters li,1, . . . , li,λi and the lemma
length λi for each lemma (cid:96)i. We use a recurrent se-
quence decoder, a setup typical of many sequence-
to-sequence (seq2seq) tasks such as in neural ma-

Figure 3: Lemma decoder, consisting of a standard
seq2seq autoregressive decoder with Luong attention
on character encodings, and with additional inputs
of processed tagger features Ti, embeddings ew
i and
sentence-level outputs ow
i . Gradients from the lemma-
tizer are stopped from ﬂowing into the tagger (denoted
GradStop).

chine translation (Sutskever et al., 2014).

The lemmatizer consists of a recurrent LSTM
layer whose initial state is taken from word-level
output ow
i and whose inputs consist of three parts.
The ﬁrst part is the embedding of the previous out-
put character (initially a beginning-of-word char-
acter BOW).

i,1, . . . , ec

The second part is a character-level attention
mechanism (Bahdanau et al., 2014) on the out-
puts of the character-level BRNN ec
i,mi.
We employ the multiplicative attention mecha-
nism described in Luong et al. (2015), which al-
lows the LSTM cell to compute an attention vec-
tor that selectively weights character-level infor-
mation in ec
i,j at each time step j based on the input
state of the LSTM cell.

The third and ﬁnal part of the RNN input allows
the network to receive the information about the
embedding of the word, the surrounding context
of the sentence, and the output of the tagger. This
output is the same for all time steps of a lemma
and is a concatenation of the following: the out-
put of the encoder ow
i and
processed tag features Tf
i . The tag features are ob-
tained by projecting the concatenated outputs of
the tagger Ti through a fully connected layer with
ReLU activation. During training, we do not pass
the gradients back through Ti to prevent the dis-
tortion of the tagger output.

i , the embedded word ew

The decoder performs greedy decoding to pre-
dict the character outputs. It runs until it produces

the end-of-word character EOW or reaches a char-
acter limit of mi + 10.

2.4 Loss Function

lemmas including the senses and the accuracies in
Table 1 take that into account. Ignoring the sense
ambiguity, the lemmatization accuracy of the joint
LemmaTag model is 98.94% for Czech-PDT.

We deﬁne the ﬁnal loss function as the weighted
sum of the losses of the tagger and the lemmatizer:

3.2 Hyperparameters

L(ˆy, y) = αL(ˆyt, yt) + βL(ˆy(cid:96), y(cid:96))

where y are the predicted outputs, ˆy the expected
outputs, yt, the tag components and y(cid:96) are the
lemma characters. The tagger and lemmatizer
losses are separately computed as the softmax
cross entropy of the output logits. The weight hy-
perparameters α, β scale the training losses so that
the subtag and lemmatizer losses do not overpower
the unfactored tag predictor gradients. The vector
α contains τ + 1 weights: one for the whole tag
and one for every component.2

3 Experiments

In this section, we show the outcomes of evalu-
ation when running our joint tagger and lemma-
tizer and compare with the current state of the art
in Czech, German, Arabic, and English datasets.
Additionally, we evaluate the lemmatizer and tag-
ger separately to compare the relative increase in
tagging and lemmatization accuracy.

3.1 Datasets

Our datasets consist of the Czech Prague De-
pendency Treebank (PDT) (Hajiˇc et al., 2006,
2018), the German TIGER corpus (Brants et al.,
2004), the Universal Dependencies Prague Arabic
Dependency Treebank (UD-PADT) (Hajic et al.,
2004), the Universal Dependencies English Web
Treebank (UD-EWT) (Silveira et al., 2014), and
the WSJ portion of the English Penn Treebank
(tags only) (Marcus et al., 1993). In all datasets,
we use the tags speciﬁc to their respective lan-
guage. Of these datasets, only Czech and Ara-
bic provide subcategorical tags, and we use unfac-
tored tags for the rest. See Table 1 for tagger and
lemmatizer accuracies.

Note that the PDT dataset disambiguates lem-
mas with the same textual representation by ap-
pending a number as lemma sense indicator. For
example, the dataset contains disambiguated lem-
mas moc-1 (as power) and moc-2 (as too much).
About 17.5% of the PDT tokens have such sense-
disambiguated lemmas. LemmaTag predicts the

2If no components are available, τ = 0.

We use loss weights α0 = 1.0 for the whole tags,
α1,...,τ = 0.1 for the tag component losses and
β = 0.5 for the lemmatizer loss.3 The RNNs
and word embedding tables have dimensionality
768 except for character-level embeddings and the
character-level RNN, which are of dimension 384.
The fully-connected layer whose inputs are Ti is
of dimension 256.

We train the models for 40 epochs with random
permutations of training sentences and batches of
16 sentences. The starting learning rate is η =
0.001 and we scale this by 0.25 at epochs 20
and 30 to increase accuracy. We train the net-
work using the lazy variant of the Adam optimizer
(Kingma and Ba, 2014), which only updates ac-
cumulators for variables that appear in the cur-
rent batch (TensorFlow, 2018), with parameters
β1 = 0.9 and β2 = 0.99. We clip the global gra-
dient norm to 3.0 to reduce the risk of exploding
gradients.

To prevent the tagger from overﬁtting, we de-
vise several strategies for regularization. We apply
dropouts with rate 0.5 as indicated in Figures 2
and 3. The word dropout (WD) replaces 25% of
words by the unknown token <unk> to force the
network to rely more on context, combatting data
sparsity issues. Lastly, we employ label smooth-
ing (Pereyra et al., 2017) which is a way to pre-
vent the network from being too conﬁdent in any
one class. The label smoothing parameter is set to
0.1 for the tagger logits (both whole tags and the
tag components).

Note that we did not perform any complex hy-
perparameter search. For additional information
on real-world performance and additional tech-
niques which have not improved evaluation accu-
racy, see Appendix A.

4 Conclusion

The evaluation results show that performing
lemmatization and tagging jointly by sharing en-
coder parameters and utilizing tag features is

3These are reasonable values to prevent gradients from
overpowering one another. The lemmatizer tends to inﬂuence
the tagger heavily.

Approach

Czech-PDT∗ German-TIGER Arabic-PADT∗
tag
LemmaTag (sep)
96.83
LemmaTag (joint) 96.90
95.89a
SoTA results

tag
95.03
95.21
+ 91.68d

tag
98.96
98.97
+ 98.04c

lem
98.84
99.05
+ 98.24c

lem
98.02
98.37
+ 97.86b

lem
96.07
96.08

Eng-EWT
lem
97.03
97.53 N/A

tag
95.50
95.37

Eng-WSJ
tag
97.59

+ 92.60e 93.90e 96.90e 97.78f

+

Table 1: Final accuracies on the test sets comparing the LemmaTag architecture as described (joint), LemmaTag
neither sharing the encoder nor providing tagger features (sep), and the state-of-the-art results (SoTA). The state-
of-the-art results are taken from the following papers: (a) Hajiˇc et al. (2009), (b) Strakov´a et al. (2014), (c) Eger
et al. (2016), (d) Inoue et al. (2017), (e) Straka et al. (2016), (f) Ling et al. (2015). The results marked with a
plus+ use additional resources apart from the dataset, and datasets marked with a star∗ indicate the availability of
subcategorical tags.

mutually beneﬁcial in morphologically rich lan-
guages. We have shown that incorporating these
ideas results in excellent performance, surpass-
ing state-of-the-art in Czech, German, and Ara-
bic POS tagging and lemmatization by a substan-
tial margin, while closely matching state-of-the-
art English POS tagging accuracy.

However,

in languages with weak morphol-
ogy such as English (and German to a lesser ex-
tent), sharing the encoder parameters may even
hurt the performance of the tagger. We believe
this is a consequence of tags correlating less with
word-level morphology, and more with sentence-
level syntax in morphologically poor languages.
Lemma prediction could beneﬁt from the syntac-
tic information in the tags, but the tag predictions
rely more on syntactic structure (i.e., word or-
der) rather than on root forms of individual words
which could be ambiguous.

There are some possible performance improve-
ments and additional metrics which we leave for
future work. For simplicity, one improvement
we intentionally left out is the use of additional
data. We can incorporate word2vec (Mikolov
et al., 2013a) or ELMo (Peters et al., 2018) word
representations, which have shown to reduce out-
of-domain issues and provide semantic informa-
tion (Eger et al., 2016). A second improvement
is to integrate information from a morphological
dictionary to resolve certain ambiguities (Hajiˇc
et al., 2009; Inoue et al., 2017). A third im-
provement can be to replace the seq2seq lemma-
tizer decoder with a classiﬁer that chooses a cor-
responding edit tree to modify (reduce) the word
form to its lemma (Chakrabarty et al., 2017). A
fourth possible improvement would be to experi-
ment with the Transformer model (Vaswani et al.,
2017), which utilizes non-recurrent multi-headed

self-attention and has been shown to achieve state-
of-the-art performance in several related sequence
tasks (Dehghani et al., 2018). Lastly, we would
like to evaluate LemmaTag on a wider range of
languages, e.g., on the Universal Dependencies
(Nivre et al., 2016) languages and treebanks which
employ lemmatization, and to analyze the use of
different types of POS tags in the model.

The code we used for LemmaTag is available at
https://github.com/hyperparticle/
LemmaTag.

Acknowledgments

The work described herein has been supported by
the City of Prague under the “OP PPR” program,
project No. CZ.07.1.02/0.0/0.0/16 023/0000108
and it has been using language resources devel-
oped by the LINDAT/CLARIN project of the Min-
istry of Education, Youth and Sports of the Czech
Republic (project LM2015071).

Tom´aˇs Gavenˇciak has been supported by Czech
Science Foundation (GACR) project 17-10090Y
“Network optimization”. Daniel Kondratyuk has
been supported by the Erasmus Mundus pro-
gram in Language & Communication Technolo-
gies (LCT).

References

Muhammad Abdul-Mageed, Mona Diab, and Sandra
K¨ubler. 2014. Samar: Subjectivity and sentiment
analysis for arabic social media. Computer Speech
& Language, 28(1):20–37.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Toms Bergmanis and Sharon Goldwater. 2018. Con-
text sensitive neural lemmatization with lematus. In

Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), volume 1, pages 1391–
1400.

Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richrd
Farkas, Filip Ginter, and Ja n Haji. 2013. Joint mor-
phological and syntactic analysis for richly inﬂected
languages. Transactions of the Association for Com-
putational Linguistics, 1:415–428.

Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen-Schirra, Esther K¨onig, Wolfgang Lezius,
Christian Rohrer, George Smith, and Hans Uszkor-
eit. 2004. Tiger: Linguistic interpretation of a ger-
man corpus. Research on language and computa-
tion, 2(4):597–620.

Abhisek Chakrabarty, Onkar Arun Pandit, and Utpal
Garain. 2017. Context sensitive lemmatization us-
ing two successive bidirectional gated recurrent net-
In Proceedings of the 55th Annual Meet-
works.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1481–
1491.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Learning
Schwenk, and Yoshua Bengio. 2014.
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Łukasz Kaiser. 2018. Univer-
sal transformers. arXiv preprint arXiv:1807.03819.

Steffen Eger, R¨udiger Gleim, and Alexander Mehler.
2016. Lemmatization and morphological tagging in
german and latin: A comparison and a survey of the
state-of-the-art. In LREC.

Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inﬂection and word-
formation in smt. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 664–674. As-
sociation for Computational Linguistics.

Andrea Gesmundo, James Henderson, Paola Merlo,
and Ivan Titov. 2009. A latent variable model of
synchronous syntactic-semantic parsing for multi-
In Proceedings of the Thirteenth
ple languages.
Conference on Computational Natural Language
Learning: Shared Task, CoNLL ’09, pages 37–42,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Jan Hajiˇc, Eduard Bejˇcek, Alevtina B´emov´a, Eva
Bur´aˇnov´a, Eva Hajiˇcov´a, Jiˇr´ı Havelka, Petr Ho-
Jiˇr´ı K´arn´ık, V´aclava Kettnerov´a, Na-
mola,
talia Klyueva, Veronika Kol´aˇrov´a, Lucie Kuˇcov´a,
Mark´eta Lopatkov´a, Marie Mikulov´a, Jiˇr´ı M´ırovsk´y,
Anna Nedoluzhko, Petr Pajas, Jarmila Panevov´a,

Lucie Pol´akov´a, Magdal´ena Rysov´a, Petr Sgall, Jo-
hanka Spoustov´a, Pavel Straˇn´ak, Pavl´ına Synkov´a,
Magda ˇSevˇc´ıkov´a, Jan ˇStˇep´anek, Zdeˇnka Ureˇsov´a,
ˇS´arka
Barbora Vidov´a Hladk´a, Daniel Zeman,
Zik´anov´a, and Zdenˇek ˇZabokrtsk´y. 2018. Prague
dependency treebank 3.5. LINDAT/CLARIN digital
library at the Institute of Formal and Applied Lin-
guistics ( ´UFAL, http://hdl.handle.net/
11234/1-2621), Faculty of Mathematics and
Physics, Charles University.

Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie
Mikulov´a, Zdenˇek ˇZabokrtsk´y, Magda ˇSevˇc´ıkov´a-
Raz´ımov´a, and Zdeˇnka Ureˇsov´a. 2006.
Prague
Dependency Treebank 2.0 (PDT 2.0).
LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics ( ´UFAL), Faculty of
Mathematics and Physics, Charles University.

Jan Hajiˇc, Jan Raab, Miroslav Spousta, et al. 2009.
Semi-supervised training for the averaged percep-
In Proceedings of the 12th Con-
tron pos tagger.
ference of the European Chapter of the Association
for Computational Linguistics, pages 763–771. As-
sociation for Computational Linguistics.

Jan Hajic, Otakar Smrz, Petr Zem´anek, Jan ˇSnaidauf,
and Emanuel Beˇska. 2004. Prague arabic depen-
dency treebank: Development in data and tools. In
Proc. of the NEMLAR Intern. Conf. on Arabic Lan-
guage Resources and Tools, pages 110–117.

Georg Heigold, Guenter Neumann, and Josef van Gen-
abith. 2017. An extensive empirical evaluation of
character-based morphological tagging for 14 lan-
In Proceedings of the 15th Conference of
guages.
the European Chapter of the Association for Com-
putational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 505–513.

Go Inoue, Hiroyuki Shindo, and Yuji Matsumoto.
Joint prediction of morphosyntactic cate-
2017.
gories for ﬁne-grained arabic part-of-speech tagging
In Proceed-
exploiting tag dictionary information.
ings of the 21st Conference on Computational Natu-
ral Language Learning (CoNLL 2017), pages 421–
431.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Wang Ling, Tiago Lu´ıs, Lu´ıs Marujo, Ram´on Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015. Finding function
in form: Compositional character models for open
arXiv preprint
vocabulary word representation.
arXiv:1508.02096.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efﬁcient estimation of word
arXiv preprint
representations in vector space.
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

Thomas M¨uller, Ryan Cotterell, Alexander Fraser, and
Hinrich Sch¨utze. 2015.
Joint lemmatization and
morphological tagging with lemming. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 2268–2274.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016), Paris, France. European
Language Resources Association (ELRA).

Gabriel Pereyra, George Tucker,

Jan Chorowski,
Łukasz Kaiser, and Geoffrey Hinton. 2017. Regular-
izing neural networks by penalizing conﬁdent output
distributions. arXiv preprint arXiv:1701.06548.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Cicero D Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech
In Proceedings of the 31st International
tagging.
Conference on Machine Learning (ICML-14), pages
1818–1826.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Natalia Silveira, Timothy Dozat, Marie-Catherine
de Marneffe, Samuel Bowman, Miriam Connor,
John Bauer, and Christopher D. Manning. 2014. A
gold standard dependency corpus for English.
In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC-
2014).

Milan Straka, Jan Hajic, and Jana Strakov. 2016. Ud-
pipe: Trainable pipeline for processing conll-u ﬁles
performing tokenization, morphological analysis,
pos tagging and parsing. In Proceedings of the Tenth
International Conference on Language Resources
and Evaluation (LREC 2016), Paris, France. Euro-
pean Language Resources Association (ELRA).

Jana Strakov´a, Milan Straka, and Jan Hajiˇc. 2014.
Open-Source Tools for Morphology, Lemmatiza-
tion, POS Tagging and Named Entity Recognition.
In Proceedings of 52nd Annual Meeting of the As-
sociation for Computational Linguistics: System
Demonstrations, pages 13–18, Baltimore, Mary-
land. Association for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

TensorFlow. 2018.

tf.contrib.opt.lazyadamoptimizer:
Class lazyadamoptimizer. TensorFlow documenta-
tion from tensorﬂow.org.

Reut Tsarfaty, Djam´e Seddah, Yoav Goldberg, San-
dra K¨ubler, Marie Candito, Jennifer Foster, Yannick
Versley, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical parsing of morphologically rich languages
(spmrl): what, how and whither. In Proceedings of
the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
1–12. Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

A Appendix

A.1 GPU Performance

We ran all the tests on an NVIDIA GTX 1080 Ti
GPU. The joint LemmaTag training takes about
3 hours for Arabic PADT, 4.5 hours for English
EWT, 12 hours for German TIGER, and 22 hours
for Czech PDT. The separate models take about
50% more time. After training, the lemma and
tag predictions of 219,000 test tokens of the Czech
PDT take about 100 seconds.

A.2 Other Techniques

We brieﬂy summarize some of the additional tech-
niques we have tried but which do not improve the
results. While some of those techniques do help on
smaller models or earlier in the training, the effect
on the fully trained network seems to be marginal
or even detrimental.

Separate sense prediction.

Instead of predict-
ing the sense disambiguation with the lemmatizer
(Czech only), we tried to predict sense as an addi-
tional classiﬁcation problem with one dense layer
based on ow
i and Ti, but it seems to perform
slightly worse (0.2%).

Beam search decoder. We have implemented a
beam search decoder for the lemmatizer instead of

the standard greedy one, but the improvement was
marginal (around 0.01%).

Variational dropout. While the dropouts in
the LemmaTag are completely random, variational
dropout erases the same channels across the time
steps of the RNN. While this generally improves
training in convolutional networks and RNNs, we
saw no signiﬁcant difference.

Layer normalization. Layer normalization ap-
plied to the encoding RNNs did not bring signiﬁ-
cant gain and also slowed down the training.

LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich
Languages with BRNNs

Daniel Kondratyuk† and Tom´aˇs Gavenˇciak‡ and Milan Straka† and Jan Hajiˇc†
Charles University
Faculty of Mathematics and Physics
†Institute of Formal and Applied Linguistics, ‡Department of Applied Mathematics
dankondratyuk@gmail.com, gavento@kam.mff.cuni.cz, {straka,hajic}@ufal.mff.cuni.cz

Abstract

We present LemmaTag, a featureless neu-
ral network architecture that jointly generates
part-of-speech tags and lemmas for sentences
by using bidirectional RNNs with character-
level and word-level embeddings. We demon-
strate that both tasks beneﬁt from sharing the
encoding part of the network, predicting tag
subcategories, and using the tagger output as
an input to the lemmatizer. We evaluate our
model across several languages with complex
morphology, which surpasses state-of-the-art
accuracy in both part-of-speech tagging and
lemmatization in Czech, German, and Arabic.

1

Introduction

Morphologically rich languages are often difﬁcult
to process in many NLP tasks (Tsarfaty et al.,
2010). As opposed to analytical languages like
English, morphologically rich languages encode
diverse sets of grammatical information within
each word using inﬂections, which convey char-
acteristics such as case, gender, and tense. The
inﬂectional variants across
addition of several
many words dramatically increases the vocabu-
lary size, which results in data sparsity and out-
of-vocabulary (OOV) issues.

Due to these issues, morphological part-of-
speech (POS) tagging and lemmatization are heav-
ily used in NLP tasks such as machine transla-
tion (Fraser et al., 2012) and sentiment analysis
(Abdul-Mageed et al., 2014). In morphologically
rich languages, the POS tags typically consist of
multiple morpho-syntactic subcategories provid-
ing additional information (see Figure 1). Closely
related to POS tagging is lemmatization, which in-
volves transforming each word to its root or dic-
tionary form. Both tasks require context-sensitive
awareness to disambiguate words with the same
form but different syntactic or semantic features
and behavior. Furthermore, lemmatization of a

word form can beneﬁt substantially from the in-
formation present in morphological tags, as gram-
matical attributes often disambiguate word forms
using context (M¨uller et al., 2015).

We address context-sensitive POS tagging and
lemmatization using a neural network model that
jointly performs both tasks on each input word in
a given sentence.1 We train the model in a super-
vised fashion, requiring training data containing
word forms, lemmas, and POS tags. In addition,
we incorporate the ideas from Inoue et al. (2017)
to optionally allow the network to predict the sub-
categories of each tag to improve accuracy. Our
model is related to the work of M¨uller et al. (2015),
which use conditional random ﬁelds (CRF) to
jointly tag and lemmatize words for morphologi-
cally rich languages. The idea of jointly predict-
ing several dimensions of categories has been ex-
plored prior to this work, for example, joint mor-
phological and syntactic analysis (Bohnet et al.,
2013) or joint parsing and semantic role labeling
(Gesmundo et al., 2009).

Our model consists of three parts:

(1) The
shared encoder, which creates an internal repre-
sentation for every word based on its character se-

1The code for this project is available at https://

github.com/hyperparticle/LemmaTag

Figure 1: The tag components of the PDT Czech tree-
bank with the numbers of valid values. Around 1500
different tags are in use in the PDT.

quence and the sentence context. We adopt the
encoder architecture of Chakrabarty et al. (2017),
utilizing character-level (Heigold et al., 2017) and
word-level embeddings (Mikolov et al., 2013b;
Santos and Zadrozny, 2014) processed through
several
layers of bidirectional recurrent neural
networks (BRNN/BiRNN) (Schuster and Paliwal,
1997; Chakrabarty et al., 2017). (2) The tagger
decoder, which applies a fully-connected layer to
the outputs of the shared encoder to predict the
POS tags. (3) The lemmatizer decoder, which
applies an RNN sequence decoder to the combined
outputs of the shared encoder and tagger decoder,
producing a sequence of characters that predict
each lemma (similar to Bergmanis and Goldwater
(2018)).

The main advantages over other proposed mod-
els are: (i) The model is featureless, requiring
little to no text preprocessing or morphological
(ii) The model shares
analysis postprocessing.
the word embeddings, character embeddings, and
RNN encoder weights in the tagger and lemma-
tizer, improving both tagging and lemmatization
accuracy while reducing the number of parameters
required for both tasks. (iii) The model predicts
tag subcategories and provides the output of the
tagger as features for the input of the lemmatizer,
further improving accuracy.

We evaluate the accuracy of our model in POS
tagging and lemmatization across several
lan-
guages: Czech, Arabic, German, and English. For
each language, we also compare the performance
of a fully separate tagger and lemmatizer to the
proposed joint model. Our results show that our
joint model is able to improve the accuracy for
both tasks, and achieves state-of-the-art perfor-
mance in both POS tagging and lemmatization in
Czech, German, and Arabic, while closely match-
ing state-of-the-art performance for English.

2 The Joint LemmaTag Model

Given a sequence of words
in a sentence
w1, . . . , wk, the task of the model is to produce a
sequence of associated tags t1, . . . , tk and lemmas
(cid:96)1, . . . , (cid:96)k. For a word wi at position i, we denote
ci,1, ci,2 . . . ci,mi to be the sequence of characters
that make up wi, where mi indicates the length of
the word string at position i. Analogously, we de-
ﬁne li,1, . . . li,λi to be the sequence of characters
that make up the lemma (cid:96)i.

Our proposed model (shown in Figures 2 and 3)

is split into three parts:
the shared encoder, the
tagger, and the lemmatizer. The initial layers
of the model are shared between the tagger and
lemmatizer, encoding the words, characters, and
context in a given sentence. The encoder then
passes its outputs to two networks, which perform
a classiﬁcation task to predict tags by the tagger
and a sequence prediction task to output lemmas
(character-by-character) in the lemmatizer.

2.1 Shared Encoder

1, . . . , ec

In the encoder shown in Figure 2, each charac-
ter ci,1, ci,2 . . . ci,mi of a word wi is indexed into
an embedding layer to produce ﬁxed-length em-
bedded vectors representing each character. These
vectors are further passed into a layer of BRNNs
composed of gated recurrent units (GRU) (Cho
et al., 2014) producing outputs ec
m, and
whose ﬁnal states are concatenated to produce the
character-level embedding sc
i of the word. Sim-
ilarly, we index wi into a word-level embedding
layer to compute vector eb
i . Then we sum these re-
sults to produce the ﬁnal word embedding ew
i =
i + eb
sc
i .
We repeat this process independently for all
the words in the sentence and feed the resulting
sequence ew
k into another two BRNN lay-
ers composed of long short-term memory units
(LSTM) with residual connections. This pro-
duces word-level outputs ow
k that encode
sentence-level context for each word (we ignore
the ﬁnal hidden states).

1 , . . . ow

1 . . . ew

2.2 Tagger

The task of the tagger is to predict a tag ti ∈ T
given a word wi and its context, where T is a set
of possible tags. As explained the introduction,
morphologically rich languages typically subdi-
vide tags further into several subcategories ti =
(ti,1, . . . , ti,τ ), where ti,j ∈ Tj, the j-th subcate-
gory. See Figure 1 for an illustration taken from
the Czech PDT tagset where τ = 15.

Having the encoded words of a sentence avail-
able, the tagger consists of a fully-connected layer
with |T | neurons whose input is the output of the
word feature RNN ow
(see ﬁgure 2). This layer
i
produces the logits ti of the tag values and the pre-
dictions ti as the maximum-likelihood value (i.e.,
softmax).

To obtain the information about categorical na-
ture of each tag, we also predict every category
ti,j of the tag independently (if they exist in the

— — — — — — — — — — —

Figure 2: Bottom: Word-level encoder. The characters
of every input word are embedded with a look-up table
(c-embed) and encoded with a BRNN. The outputs ec
i,j
are used in decoder attention, and the ﬁnal states are
summed with the word-level embedding (w-embed) to
produce ew
Top: Sentence-level encoder and tag classiﬁer. Two
BRNN layers with residual connections act on the em-
bedded words ew
i of a sentence, providing context. The
output of the tag classiﬁcation are the logits for both
the whole tags ti and their components ti,j.
Both: Thick slanted lines denote training dropout.

i . WD denotes word dropout.

dataset) with τ dense layers similar to Inoue et al.
(2017). The j-th layer has |Tj| neurons and out-
puts the logits ti,j for the category values. While
these values are trained for,
their value is not
used in tag prediction. All tag values Ti =
(ti, ti,1 . . . , ti,τ ) are concatenated into a ﬂat vec-
tor and fed into the lemmatizer as an additional set
of potentially useful features.

2.3 Lemmatizer

The task of the lemmatizer is to produce a se-
quence of characters li,1, . . . , li,λi and the lemma
length λi for each lemma (cid:96)i. We use a recurrent se-
quence decoder, a setup typical of many sequence-
to-sequence (seq2seq) tasks such as in neural ma-

Figure 3: Lemma decoder, consisting of a standard
seq2seq autoregressive decoder with Luong attention
on character encodings, and with additional inputs
of processed tagger features Ti, embeddings ew
i and
sentence-level outputs ow
i . Gradients from the lemma-
tizer are stopped from ﬂowing into the tagger (denoted
GradStop).

chine translation (Sutskever et al., 2014).

The lemmatizer consists of a recurrent LSTM
layer whose initial state is taken from word-level
output ow
i and whose inputs consist of three parts.
The ﬁrst part is the embedding of the previous out-
put character (initially a beginning-of-word char-
acter BOW).

i,1, . . . , ec

The second part is a character-level attention
mechanism (Bahdanau et al., 2014) on the out-
puts of the character-level BRNN ec
i,mi.
We employ the multiplicative attention mecha-
nism described in Luong et al. (2015), which al-
lows the LSTM cell to compute an attention vec-
tor that selectively weights character-level infor-
mation in ec
i,j at each time step j based on the input
state of the LSTM cell.

The third and ﬁnal part of the RNN input allows
the network to receive the information about the
embedding of the word, the surrounding context
of the sentence, and the output of the tagger. This
output is the same for all time steps of a lemma
and is a concatenation of the following: the out-
put of the encoder ow
i and
processed tag features Tf
i . The tag features are ob-
tained by projecting the concatenated outputs of
the tagger Ti through a fully connected layer with
ReLU activation. During training, we do not pass
the gradients back through Ti to prevent the dis-
tortion of the tagger output.

i , the embedded word ew

The decoder performs greedy decoding to pre-
dict the character outputs. It runs until it produces

the end-of-word character EOW or reaches a char-
acter limit of mi + 10.

2.4 Loss Function

lemmas including the senses and the accuracies in
Table 1 take that into account. Ignoring the sense
ambiguity, the lemmatization accuracy of the joint
LemmaTag model is 98.94% for Czech-PDT.

We deﬁne the ﬁnal loss function as the weighted
sum of the losses of the tagger and the lemmatizer:

3.2 Hyperparameters

L(ˆy, y) = αL(ˆyt, yt) + βL(ˆy(cid:96), y(cid:96))

where y are the predicted outputs, ˆy the expected
outputs, yt, the tag components and y(cid:96) are the
lemma characters. The tagger and lemmatizer
losses are separately computed as the softmax
cross entropy of the output logits. The weight hy-
perparameters α, β scale the training losses so that
the subtag and lemmatizer losses do not overpower
the unfactored tag predictor gradients. The vector
α contains τ + 1 weights: one for the whole tag
and one for every component.2

3 Experiments

In this section, we show the outcomes of evalu-
ation when running our joint tagger and lemma-
tizer and compare with the current state of the art
in Czech, German, Arabic, and English datasets.
Additionally, we evaluate the lemmatizer and tag-
ger separately to compare the relative increase in
tagging and lemmatization accuracy.

3.1 Datasets

Our datasets consist of the Czech Prague De-
pendency Treebank (PDT) (Hajiˇc et al., 2006,
2018), the German TIGER corpus (Brants et al.,
2004), the Universal Dependencies Prague Arabic
Dependency Treebank (UD-PADT) (Hajic et al.,
2004), the Universal Dependencies English Web
Treebank (UD-EWT) (Silveira et al., 2014), and
the WSJ portion of the English Penn Treebank
(tags only) (Marcus et al., 1993). In all datasets,
we use the tags speciﬁc to their respective lan-
guage. Of these datasets, only Czech and Ara-
bic provide subcategorical tags, and we use unfac-
tored tags for the rest. See Table 1 for tagger and
lemmatizer accuracies.

Note that the PDT dataset disambiguates lem-
mas with the same textual representation by ap-
pending a number as lemma sense indicator. For
example, the dataset contains disambiguated lem-
mas moc-1 (as power) and moc-2 (as too much).
About 17.5% of the PDT tokens have such sense-
disambiguated lemmas. LemmaTag predicts the

2If no components are available, τ = 0.

We use loss weights α0 = 1.0 for the whole tags,
α1,...,τ = 0.1 for the tag component losses and
β = 0.5 for the lemmatizer loss.3 The RNNs
and word embedding tables have dimensionality
768 except for character-level embeddings and the
character-level RNN, which are of dimension 384.
The fully-connected layer whose inputs are Ti is
of dimension 256.

We train the models for 40 epochs with random
permutations of training sentences and batches of
16 sentences. The starting learning rate is η =
0.001 and we scale this by 0.25 at epochs 20
and 30 to increase accuracy. We train the net-
work using the lazy variant of the Adam optimizer
(Kingma and Ba, 2014), which only updates ac-
cumulators for variables that appear in the cur-
rent batch (TensorFlow, 2018), with parameters
β1 = 0.9 and β2 = 0.99. We clip the global gra-
dient norm to 3.0 to reduce the risk of exploding
gradients.

To prevent the tagger from overﬁtting, we de-
vise several strategies for regularization. We apply
dropouts with rate 0.5 as indicated in Figures 2
and 3. The word dropout (WD) replaces 25% of
words by the unknown token <unk> to force the
network to rely more on context, combatting data
sparsity issues. Lastly, we employ label smooth-
ing (Pereyra et al., 2017) which is a way to pre-
vent the network from being too conﬁdent in any
one class. The label smoothing parameter is set to
0.1 for the tagger logits (both whole tags and the
tag components).

Note that we did not perform any complex hy-
perparameter search. For additional information
on real-world performance and additional tech-
niques which have not improved evaluation accu-
racy, see Appendix A.

4 Conclusion

The evaluation results show that performing
lemmatization and tagging jointly by sharing en-
coder parameters and utilizing tag features is

3These are reasonable values to prevent gradients from
overpowering one another. The lemmatizer tends to inﬂuence
the tagger heavily.

Approach

Czech-PDT∗ German-TIGER Arabic-PADT∗
tag
LemmaTag (sep)
96.83
LemmaTag (joint) 96.90
95.89a
SoTA results

tag
95.03
95.21
+ 91.68d

tag
98.96
98.97
+ 98.04c

lem
98.84
99.05
+ 98.24c

lem
98.02
98.37
+ 97.86b

lem
96.07
96.08

Eng-EWT
lem
97.03
97.53 N/A

tag
95.50
95.37

Eng-WSJ
tag
97.59

+ 92.60e 93.90e 96.90e 97.78f

+

Table 1: Final accuracies on the test sets comparing the LemmaTag architecture as described (joint), LemmaTag
neither sharing the encoder nor providing tagger features (sep), and the state-of-the-art results (SoTA). The state-
of-the-art results are taken from the following papers: (a) Hajiˇc et al. (2009), (b) Strakov´a et al. (2014), (c) Eger
et al. (2016), (d) Inoue et al. (2017), (e) Straka et al. (2016), (f) Ling et al. (2015). The results marked with a
plus+ use additional resources apart from the dataset, and datasets marked with a star∗ indicate the availability of
subcategorical tags.

mutually beneﬁcial in morphologically rich lan-
guages. We have shown that incorporating these
ideas results in excellent performance, surpass-
ing state-of-the-art in Czech, German, and Ara-
bic POS tagging and lemmatization by a substan-
tial margin, while closely matching state-of-the-
art English POS tagging accuracy.

However,

in languages with weak morphol-
ogy such as English (and German to a lesser ex-
tent), sharing the encoder parameters may even
hurt the performance of the tagger. We believe
this is a consequence of tags correlating less with
word-level morphology, and more with sentence-
level syntax in morphologically poor languages.
Lemma prediction could beneﬁt from the syntac-
tic information in the tags, but the tag predictions
rely more on syntactic structure (i.e., word or-
der) rather than on root forms of individual words
which could be ambiguous.

There are some possible performance improve-
ments and additional metrics which we leave for
future work. For simplicity, one improvement
we intentionally left out is the use of additional
data. We can incorporate word2vec (Mikolov
et al., 2013a) or ELMo (Peters et al., 2018) word
representations, which have shown to reduce out-
of-domain issues and provide semantic informa-
tion (Eger et al., 2016). A second improvement
is to integrate information from a morphological
dictionary to resolve certain ambiguities (Hajiˇc
et al., 2009; Inoue et al., 2017). A third im-
provement can be to replace the seq2seq lemma-
tizer decoder with a classiﬁer that chooses a cor-
responding edit tree to modify (reduce) the word
form to its lemma (Chakrabarty et al., 2017). A
fourth possible improvement would be to experi-
ment with the Transformer model (Vaswani et al.,
2017), which utilizes non-recurrent multi-headed

self-attention and has been shown to achieve state-
of-the-art performance in several related sequence
tasks (Dehghani et al., 2018). Lastly, we would
like to evaluate LemmaTag on a wider range of
languages, e.g., on the Universal Dependencies
(Nivre et al., 2016) languages and treebanks which
employ lemmatization, and to analyze the use of
different types of POS tags in the model.

The code we used for LemmaTag is available at
https://github.com/hyperparticle/
LemmaTag.

Acknowledgments

The work described herein has been supported by
the City of Prague under the “OP PPR” program,
project No. CZ.07.1.02/0.0/0.0/16 023/0000108
and it has been using language resources devel-
oped by the LINDAT/CLARIN project of the Min-
istry of Education, Youth and Sports of the Czech
Republic (project LM2015071).

Tom´aˇs Gavenˇciak has been supported by Czech
Science Foundation (GACR) project 17-10090Y
“Network optimization”. Daniel Kondratyuk has
been supported by the Erasmus Mundus pro-
gram in Language & Communication Technolo-
gies (LCT).

References

Muhammad Abdul-Mageed, Mona Diab, and Sandra
K¨ubler. 2014. Samar: Subjectivity and sentiment
analysis for arabic social media. Computer Speech
& Language, 28(1):20–37.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Toms Bergmanis and Sharon Goldwater. 2018. Con-
text sensitive neural lemmatization with lematus. In

Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), volume 1, pages 1391–
1400.

Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richrd
Farkas, Filip Ginter, and Ja n Haji. 2013. Joint mor-
phological and syntactic analysis for richly inﬂected
languages. Transactions of the Association for Com-
putational Linguistics, 1:415–428.

Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen-Schirra, Esther K¨onig, Wolfgang Lezius,
Christian Rohrer, George Smith, and Hans Uszkor-
eit. 2004. Tiger: Linguistic interpretation of a ger-
man corpus. Research on language and computa-
tion, 2(4):597–620.

Abhisek Chakrabarty, Onkar Arun Pandit, and Utpal
Garain. 2017. Context sensitive lemmatization us-
ing two successive bidirectional gated recurrent net-
In Proceedings of the 55th Annual Meet-
works.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1481–
1491.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Learning
Schwenk, and Yoshua Bengio. 2014.
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Łukasz Kaiser. 2018. Univer-
sal transformers. arXiv preprint arXiv:1807.03819.

Steffen Eger, R¨udiger Gleim, and Alexander Mehler.
2016. Lemmatization and morphological tagging in
german and latin: A comparison and a survey of the
state-of-the-art. In LREC.

Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inﬂection and word-
formation in smt. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 664–674. As-
sociation for Computational Linguistics.

Andrea Gesmundo, James Henderson, Paola Merlo,
and Ivan Titov. 2009. A latent variable model of
synchronous syntactic-semantic parsing for multi-
In Proceedings of the Thirteenth
ple languages.
Conference on Computational Natural Language
Learning: Shared Task, CoNLL ’09, pages 37–42,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Jan Hajiˇc, Eduard Bejˇcek, Alevtina B´emov´a, Eva
Bur´aˇnov´a, Eva Hajiˇcov´a, Jiˇr´ı Havelka, Petr Ho-
Jiˇr´ı K´arn´ık, V´aclava Kettnerov´a, Na-
mola,
talia Klyueva, Veronika Kol´aˇrov´a, Lucie Kuˇcov´a,
Mark´eta Lopatkov´a, Marie Mikulov´a, Jiˇr´ı M´ırovsk´y,
Anna Nedoluzhko, Petr Pajas, Jarmila Panevov´a,

Lucie Pol´akov´a, Magdal´ena Rysov´a, Petr Sgall, Jo-
hanka Spoustov´a, Pavel Straˇn´ak, Pavl´ına Synkov´a,
Magda ˇSevˇc´ıkov´a, Jan ˇStˇep´anek, Zdeˇnka Ureˇsov´a,
ˇS´arka
Barbora Vidov´a Hladk´a, Daniel Zeman,
Zik´anov´a, and Zdenˇek ˇZabokrtsk´y. 2018. Prague
dependency treebank 3.5. LINDAT/CLARIN digital
library at the Institute of Formal and Applied Lin-
guistics ( ´UFAL, http://hdl.handle.net/
11234/1-2621), Faculty of Mathematics and
Physics, Charles University.

Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie
Mikulov´a, Zdenˇek ˇZabokrtsk´y, Magda ˇSevˇc´ıkov´a-
Raz´ımov´a, and Zdeˇnka Ureˇsov´a. 2006.
Prague
Dependency Treebank 2.0 (PDT 2.0).
LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics ( ´UFAL), Faculty of
Mathematics and Physics, Charles University.

Jan Hajiˇc, Jan Raab, Miroslav Spousta, et al. 2009.
Semi-supervised training for the averaged percep-
In Proceedings of the 12th Con-
tron pos tagger.
ference of the European Chapter of the Association
for Computational Linguistics, pages 763–771. As-
sociation for Computational Linguistics.

Jan Hajic, Otakar Smrz, Petr Zem´anek, Jan ˇSnaidauf,
and Emanuel Beˇska. 2004. Prague arabic depen-
dency treebank: Development in data and tools. In
Proc. of the NEMLAR Intern. Conf. on Arabic Lan-
guage Resources and Tools, pages 110–117.

Georg Heigold, Guenter Neumann, and Josef van Gen-
abith. 2017. An extensive empirical evaluation of
character-based morphological tagging for 14 lan-
In Proceedings of the 15th Conference of
guages.
the European Chapter of the Association for Com-
putational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 505–513.

Go Inoue, Hiroyuki Shindo, and Yuji Matsumoto.
Joint prediction of morphosyntactic cate-
2017.
gories for ﬁne-grained arabic part-of-speech tagging
In Proceed-
exploiting tag dictionary information.
ings of the 21st Conference on Computational Natu-
ral Language Learning (CoNLL 2017), pages 421–
431.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Wang Ling, Tiago Lu´ıs, Lu´ıs Marujo, Ram´on Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015. Finding function
in form: Compositional character models for open
arXiv preprint
vocabulary word representation.
arXiv:1508.02096.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efﬁcient estimation of word
arXiv preprint
representations in vector space.
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

Thomas M¨uller, Ryan Cotterell, Alexander Fraser, and
Hinrich Sch¨utze. 2015.
Joint lemmatization and
morphological tagging with lemming. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 2268–2274.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016), Paris, France. European
Language Resources Association (ELRA).

Gabriel Pereyra, George Tucker,

Jan Chorowski,
Łukasz Kaiser, and Geoffrey Hinton. 2017. Regular-
izing neural networks by penalizing conﬁdent output
distributions. arXiv preprint arXiv:1701.06548.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Cicero D Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech
In Proceedings of the 31st International
tagging.
Conference on Machine Learning (ICML-14), pages
1818–1826.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Natalia Silveira, Timothy Dozat, Marie-Catherine
de Marneffe, Samuel Bowman, Miriam Connor,
John Bauer, and Christopher D. Manning. 2014. A
gold standard dependency corpus for English.
In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC-
2014).

Milan Straka, Jan Hajic, and Jana Strakov. 2016. Ud-
pipe: Trainable pipeline for processing conll-u ﬁles
performing tokenization, morphological analysis,
pos tagging and parsing. In Proceedings of the Tenth
International Conference on Language Resources
and Evaluation (LREC 2016), Paris, France. Euro-
pean Language Resources Association (ELRA).

Jana Strakov´a, Milan Straka, and Jan Hajiˇc. 2014.
Open-Source Tools for Morphology, Lemmatiza-
tion, POS Tagging and Named Entity Recognition.
In Proceedings of 52nd Annual Meeting of the As-
sociation for Computational Linguistics: System
Demonstrations, pages 13–18, Baltimore, Mary-
land. Association for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

TensorFlow. 2018.

tf.contrib.opt.lazyadamoptimizer:
Class lazyadamoptimizer. TensorFlow documenta-
tion from tensorﬂow.org.

Reut Tsarfaty, Djam´e Seddah, Yoav Goldberg, San-
dra K¨ubler, Marie Candito, Jennifer Foster, Yannick
Versley, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical parsing of morphologically rich languages
(spmrl): what, how and whither. In Proceedings of
the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
1–12. Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

A Appendix

A.1 GPU Performance

We ran all the tests on an NVIDIA GTX 1080 Ti
GPU. The joint LemmaTag training takes about
3 hours for Arabic PADT, 4.5 hours for English
EWT, 12 hours for German TIGER, and 22 hours
for Czech PDT. The separate models take about
50% more time. After training, the lemma and
tag predictions of 219,000 test tokens of the Czech
PDT take about 100 seconds.

A.2 Other Techniques

We brieﬂy summarize some of the additional tech-
niques we have tried but which do not improve the
results. While some of those techniques do help on
smaller models or earlier in the training, the effect
on the fully trained network seems to be marginal
or even detrimental.

Separate sense prediction.

Instead of predict-
ing the sense disambiguation with the lemmatizer
(Czech only), we tried to predict sense as an addi-
tional classiﬁcation problem with one dense layer
based on ow
i and Ti, but it seems to perform
slightly worse (0.2%).

Beam search decoder. We have implemented a
beam search decoder for the lemmatizer instead of

the standard greedy one, but the improvement was
marginal (around 0.01%).

Variational dropout. While the dropouts in
the LemmaTag are completely random, variational
dropout erases the same channels across the time
steps of the RNN. While this generally improves
training in convolutional networks and RNNs, we
saw no signiﬁcant difference.

Layer normalization. Layer normalization ap-
plied to the encoding RNNs did not bring signiﬁ-
cant gain and also slowed down the training.

LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich
Languages with BRNNs

Daniel Kondratyuk† and Tom´aˇs Gavenˇciak‡ and Milan Straka† and Jan Hajiˇc†
Charles University
Faculty of Mathematics and Physics
†Institute of Formal and Applied Linguistics, ‡Department of Applied Mathematics
dankondratyuk@gmail.com, gavento@kam.mff.cuni.cz, {straka,hajic}@ufal.mff.cuni.cz

Abstract

We present LemmaTag, a featureless neu-
ral network architecture that jointly generates
part-of-speech tags and lemmas for sentences
by using bidirectional RNNs with character-
level and word-level embeddings. We demon-
strate that both tasks beneﬁt from sharing the
encoding part of the network, predicting tag
subcategories, and using the tagger output as
an input to the lemmatizer. We evaluate our
model across several languages with complex
morphology, which surpasses state-of-the-art
accuracy in both part-of-speech tagging and
lemmatization in Czech, German, and Arabic.

1

Introduction

Morphologically rich languages are often difﬁcult
to process in many NLP tasks (Tsarfaty et al.,
2010). As opposed to analytical languages like
English, morphologically rich languages encode
diverse sets of grammatical information within
each word using inﬂections, which convey char-
acteristics such as case, gender, and tense. The
inﬂectional variants across
addition of several
many words dramatically increases the vocabu-
lary size, which results in data sparsity and out-
of-vocabulary (OOV) issues.

Due to these issues, morphological part-of-
speech (POS) tagging and lemmatization are heav-
ily used in NLP tasks such as machine transla-
tion (Fraser et al., 2012) and sentiment analysis
(Abdul-Mageed et al., 2014). In morphologically
rich languages, the POS tags typically consist of
multiple morpho-syntactic subcategories provid-
ing additional information (see Figure 1). Closely
related to POS tagging is lemmatization, which in-
volves transforming each word to its root or dic-
tionary form. Both tasks require context-sensitive
awareness to disambiguate words with the same
form but different syntactic or semantic features
and behavior. Furthermore, lemmatization of a

word form can beneﬁt substantially from the in-
formation present in morphological tags, as gram-
matical attributes often disambiguate word forms
using context (M¨uller et al., 2015).

We address context-sensitive POS tagging and
lemmatization using a neural network model that
jointly performs both tasks on each input word in
a given sentence.1 We train the model in a super-
vised fashion, requiring training data containing
word forms, lemmas, and POS tags. In addition,
we incorporate the ideas from Inoue et al. (2017)
to optionally allow the network to predict the sub-
categories of each tag to improve accuracy. Our
model is related to the work of M¨uller et al. (2015),
which use conditional random ﬁelds (CRF) to
jointly tag and lemmatize words for morphologi-
cally rich languages. The idea of jointly predict-
ing several dimensions of categories has been ex-
plored prior to this work, for example, joint mor-
phological and syntactic analysis (Bohnet et al.,
2013) or joint parsing and semantic role labeling
(Gesmundo et al., 2009).

Our model consists of three parts:

(1) The
shared encoder, which creates an internal repre-
sentation for every word based on its character se-

1The code for this project is available at https://

github.com/hyperparticle/LemmaTag

Figure 1: The tag components of the PDT Czech tree-
bank with the numbers of valid values. Around 1500
different tags are in use in the PDT.

quence and the sentence context. We adopt the
encoder architecture of Chakrabarty et al. (2017),
utilizing character-level (Heigold et al., 2017) and
word-level embeddings (Mikolov et al., 2013b;
Santos and Zadrozny, 2014) processed through
several
layers of bidirectional recurrent neural
networks (BRNN/BiRNN) (Schuster and Paliwal,
1997; Chakrabarty et al., 2017). (2) The tagger
decoder, which applies a fully-connected layer to
the outputs of the shared encoder to predict the
POS tags. (3) The lemmatizer decoder, which
applies an RNN sequence decoder to the combined
outputs of the shared encoder and tagger decoder,
producing a sequence of characters that predict
each lemma (similar to Bergmanis and Goldwater
(2018)).

The main advantages over other proposed mod-
els are: (i) The model is featureless, requiring
little to no text preprocessing or morphological
(ii) The model shares
analysis postprocessing.
the word embeddings, character embeddings, and
RNN encoder weights in the tagger and lemma-
tizer, improving both tagging and lemmatization
accuracy while reducing the number of parameters
required for both tasks. (iii) The model predicts
tag subcategories and provides the output of the
tagger as features for the input of the lemmatizer,
further improving accuracy.

We evaluate the accuracy of our model in POS
tagging and lemmatization across several
lan-
guages: Czech, Arabic, German, and English. For
each language, we also compare the performance
of a fully separate tagger and lemmatizer to the
proposed joint model. Our results show that our
joint model is able to improve the accuracy for
both tasks, and achieves state-of-the-art perfor-
mance in both POS tagging and lemmatization in
Czech, German, and Arabic, while closely match-
ing state-of-the-art performance for English.

2 The Joint LemmaTag Model

Given a sequence of words
in a sentence
w1, . . . , wk, the task of the model is to produce a
sequence of associated tags t1, . . . , tk and lemmas
(cid:96)1, . . . , (cid:96)k. For a word wi at position i, we denote
ci,1, ci,2 . . . ci,mi to be the sequence of characters
that make up wi, where mi indicates the length of
the word string at position i. Analogously, we de-
ﬁne li,1, . . . li,λi to be the sequence of characters
that make up the lemma (cid:96)i.

Our proposed model (shown in Figures 2 and 3)

is split into three parts:
the shared encoder, the
tagger, and the lemmatizer. The initial layers
of the model are shared between the tagger and
lemmatizer, encoding the words, characters, and
context in a given sentence. The encoder then
passes its outputs to two networks, which perform
a classiﬁcation task to predict tags by the tagger
and a sequence prediction task to output lemmas
(character-by-character) in the lemmatizer.

2.1 Shared Encoder

1, . . . , ec

In the encoder shown in Figure 2, each charac-
ter ci,1, ci,2 . . . ci,mi of a word wi is indexed into
an embedding layer to produce ﬁxed-length em-
bedded vectors representing each character. These
vectors are further passed into a layer of BRNNs
composed of gated recurrent units (GRU) (Cho
et al., 2014) producing outputs ec
m, and
whose ﬁnal states are concatenated to produce the
character-level embedding sc
i of the word. Sim-
ilarly, we index wi into a word-level embedding
layer to compute vector eb
i . Then we sum these re-
sults to produce the ﬁnal word embedding ew
i =
i + eb
sc
i .
We repeat this process independently for all
the words in the sentence and feed the resulting
sequence ew
k into another two BRNN lay-
ers composed of long short-term memory units
(LSTM) with residual connections. This pro-
duces word-level outputs ow
k that encode
sentence-level context for each word (we ignore
the ﬁnal hidden states).

1 , . . . ow

1 . . . ew

2.2 Tagger

The task of the tagger is to predict a tag ti ∈ T
given a word wi and its context, where T is a set
of possible tags. As explained the introduction,
morphologically rich languages typically subdi-
vide tags further into several subcategories ti =
(ti,1, . . . , ti,τ ), where ti,j ∈ Tj, the j-th subcate-
gory. See Figure 1 for an illustration taken from
the Czech PDT tagset where τ = 15.

Having the encoded words of a sentence avail-
able, the tagger consists of a fully-connected layer
with |T | neurons whose input is the output of the
word feature RNN ow
(see ﬁgure 2). This layer
i
produces the logits ti of the tag values and the pre-
dictions ti as the maximum-likelihood value (i.e.,
softmax).

To obtain the information about categorical na-
ture of each tag, we also predict every category
ti,j of the tag independently (if they exist in the

— — — — — — — — — — —

Figure 2: Bottom: Word-level encoder. The characters
of every input word are embedded with a look-up table
(c-embed) and encoded with a BRNN. The outputs ec
i,j
are used in decoder attention, and the ﬁnal states are
summed with the word-level embedding (w-embed) to
produce ew
Top: Sentence-level encoder and tag classiﬁer. Two
BRNN layers with residual connections act on the em-
bedded words ew
i of a sentence, providing context. The
output of the tag classiﬁcation are the logits for both
the whole tags ti and their components ti,j.
Both: Thick slanted lines denote training dropout.

i . WD denotes word dropout.

dataset) with τ dense layers similar to Inoue et al.
(2017). The j-th layer has |Tj| neurons and out-
puts the logits ti,j for the category values. While
these values are trained for,
their value is not
used in tag prediction. All tag values Ti =
(ti, ti,1 . . . , ti,τ ) are concatenated into a ﬂat vec-
tor and fed into the lemmatizer as an additional set
of potentially useful features.

2.3 Lemmatizer

The task of the lemmatizer is to produce a se-
quence of characters li,1, . . . , li,λi and the lemma
length λi for each lemma (cid:96)i. We use a recurrent se-
quence decoder, a setup typical of many sequence-
to-sequence (seq2seq) tasks such as in neural ma-

Figure 3: Lemma decoder, consisting of a standard
seq2seq autoregressive decoder with Luong attention
on character encodings, and with additional inputs
of processed tagger features Ti, embeddings ew
i and
sentence-level outputs ow
i . Gradients from the lemma-
tizer are stopped from ﬂowing into the tagger (denoted
GradStop).

chine translation (Sutskever et al., 2014).

The lemmatizer consists of a recurrent LSTM
layer whose initial state is taken from word-level
output ow
i and whose inputs consist of three parts.
The ﬁrst part is the embedding of the previous out-
put character (initially a beginning-of-word char-
acter BOW).

i,1, . . . , ec

The second part is a character-level attention
mechanism (Bahdanau et al., 2014) on the out-
puts of the character-level BRNN ec
i,mi.
We employ the multiplicative attention mecha-
nism described in Luong et al. (2015), which al-
lows the LSTM cell to compute an attention vec-
tor that selectively weights character-level infor-
mation in ec
i,j at each time step j based on the input
state of the LSTM cell.

The third and ﬁnal part of the RNN input allows
the network to receive the information about the
embedding of the word, the surrounding context
of the sentence, and the output of the tagger. This
output is the same for all time steps of a lemma
and is a concatenation of the following: the out-
put of the encoder ow
i and
processed tag features Tf
i . The tag features are ob-
tained by projecting the concatenated outputs of
the tagger Ti through a fully connected layer with
ReLU activation. During training, we do not pass
the gradients back through Ti to prevent the dis-
tortion of the tagger output.

i , the embedded word ew

The decoder performs greedy decoding to pre-
dict the character outputs. It runs until it produces

the end-of-word character EOW or reaches a char-
acter limit of mi + 10.

2.4 Loss Function

lemmas including the senses and the accuracies in
Table 1 take that into account. Ignoring the sense
ambiguity, the lemmatization accuracy of the joint
LemmaTag model is 98.94% for Czech-PDT.

We deﬁne the ﬁnal loss function as the weighted
sum of the losses of the tagger and the lemmatizer:

3.2 Hyperparameters

L(ˆy, y) = αL(ˆyt, yt) + βL(ˆy(cid:96), y(cid:96))

where y are the predicted outputs, ˆy the expected
outputs, yt, the tag components and y(cid:96) are the
lemma characters. The tagger and lemmatizer
losses are separately computed as the softmax
cross entropy of the output logits. The weight hy-
perparameters α, β scale the training losses so that
the subtag and lemmatizer losses do not overpower
the unfactored tag predictor gradients. The vector
α contains τ + 1 weights: one for the whole tag
and one for every component.2

3 Experiments

In this section, we show the outcomes of evalu-
ation when running our joint tagger and lemma-
tizer and compare with the current state of the art
in Czech, German, Arabic, and English datasets.
Additionally, we evaluate the lemmatizer and tag-
ger separately to compare the relative increase in
tagging and lemmatization accuracy.

3.1 Datasets

Our datasets consist of the Czech Prague De-
pendency Treebank (PDT) (Hajiˇc et al., 2006,
2018), the German TIGER corpus (Brants et al.,
2004), the Universal Dependencies Prague Arabic
Dependency Treebank (UD-PADT) (Hajic et al.,
2004), the Universal Dependencies English Web
Treebank (UD-EWT) (Silveira et al., 2014), and
the WSJ portion of the English Penn Treebank
(tags only) (Marcus et al., 1993). In all datasets,
we use the tags speciﬁc to their respective lan-
guage. Of these datasets, only Czech and Ara-
bic provide subcategorical tags, and we use unfac-
tored tags for the rest. See Table 1 for tagger and
lemmatizer accuracies.

Note that the PDT dataset disambiguates lem-
mas with the same textual representation by ap-
pending a number as lemma sense indicator. For
example, the dataset contains disambiguated lem-
mas moc-1 (as power) and moc-2 (as too much).
About 17.5% of the PDT tokens have such sense-
disambiguated lemmas. LemmaTag predicts the

2If no components are available, τ = 0.

We use loss weights α0 = 1.0 for the whole tags,
α1,...,τ = 0.1 for the tag component losses and
β = 0.5 for the lemmatizer loss.3 The RNNs
and word embedding tables have dimensionality
768 except for character-level embeddings and the
character-level RNN, which are of dimension 384.
The fully-connected layer whose inputs are Ti is
of dimension 256.

We train the models for 40 epochs with random
permutations of training sentences and batches of
16 sentences. The starting learning rate is η =
0.001 and we scale this by 0.25 at epochs 20
and 30 to increase accuracy. We train the net-
work using the lazy variant of the Adam optimizer
(Kingma and Ba, 2014), which only updates ac-
cumulators for variables that appear in the cur-
rent batch (TensorFlow, 2018), with parameters
β1 = 0.9 and β2 = 0.99. We clip the global gra-
dient norm to 3.0 to reduce the risk of exploding
gradients.

To prevent the tagger from overﬁtting, we de-
vise several strategies for regularization. We apply
dropouts with rate 0.5 as indicated in Figures 2
and 3. The word dropout (WD) replaces 25% of
words by the unknown token <unk> to force the
network to rely more on context, combatting data
sparsity issues. Lastly, we employ label smooth-
ing (Pereyra et al., 2017) which is a way to pre-
vent the network from being too conﬁdent in any
one class. The label smoothing parameter is set to
0.1 for the tagger logits (both whole tags and the
tag components).

Note that we did not perform any complex hy-
perparameter search. For additional information
on real-world performance and additional tech-
niques which have not improved evaluation accu-
racy, see Appendix A.

4 Conclusion

The evaluation results show that performing
lemmatization and tagging jointly by sharing en-
coder parameters and utilizing tag features is

3These are reasonable values to prevent gradients from
overpowering one another. The lemmatizer tends to inﬂuence
the tagger heavily.

Approach

Czech-PDT∗ German-TIGER Arabic-PADT∗
tag
LemmaTag (sep)
96.83
LemmaTag (joint) 96.90
95.89a
SoTA results

tag
95.03
95.21
+ 91.68d

tag
98.96
98.97
+ 98.04c

lem
98.84
99.05
+ 98.24c

lem
98.02
98.37
+ 97.86b

lem
96.07
96.08

Eng-EWT
lem
97.03
97.53 N/A

tag
95.50
95.37

Eng-WSJ
tag
97.59

+ 92.60e 93.90e 96.90e 97.78f

+

Table 1: Final accuracies on the test sets comparing the LemmaTag architecture as described (joint), LemmaTag
neither sharing the encoder nor providing tagger features (sep), and the state-of-the-art results (SoTA). The state-
of-the-art results are taken from the following papers: (a) Hajiˇc et al. (2009), (b) Strakov´a et al. (2014), (c) Eger
et al. (2016), (d) Inoue et al. (2017), (e) Straka et al. (2016), (f) Ling et al. (2015). The results marked with a
plus+ use additional resources apart from the dataset, and datasets marked with a star∗ indicate the availability of
subcategorical tags.

mutually beneﬁcial in morphologically rich lan-
guages. We have shown that incorporating these
ideas results in excellent performance, surpass-
ing state-of-the-art in Czech, German, and Ara-
bic POS tagging and lemmatization by a substan-
tial margin, while closely matching state-of-the-
art English POS tagging accuracy.

However,

in languages with weak morphol-
ogy such as English (and German to a lesser ex-
tent), sharing the encoder parameters may even
hurt the performance of the tagger. We believe
this is a consequence of tags correlating less with
word-level morphology, and more with sentence-
level syntax in morphologically poor languages.
Lemma prediction could beneﬁt from the syntac-
tic information in the tags, but the tag predictions
rely more on syntactic structure (i.e., word or-
der) rather than on root forms of individual words
which could be ambiguous.

There are some possible performance improve-
ments and additional metrics which we leave for
future work. For simplicity, one improvement
we intentionally left out is the use of additional
data. We can incorporate word2vec (Mikolov
et al., 2013a) or ELMo (Peters et al., 2018) word
representations, which have shown to reduce out-
of-domain issues and provide semantic informa-
tion (Eger et al., 2016). A second improvement
is to integrate information from a morphological
dictionary to resolve certain ambiguities (Hajiˇc
et al., 2009; Inoue et al., 2017). A third im-
provement can be to replace the seq2seq lemma-
tizer decoder with a classiﬁer that chooses a cor-
responding edit tree to modify (reduce) the word
form to its lemma (Chakrabarty et al., 2017). A
fourth possible improvement would be to experi-
ment with the Transformer model (Vaswani et al.,
2017), which utilizes non-recurrent multi-headed

self-attention and has been shown to achieve state-
of-the-art performance in several related sequence
tasks (Dehghani et al., 2018). Lastly, we would
like to evaluate LemmaTag on a wider range of
languages, e.g., on the Universal Dependencies
(Nivre et al., 2016) languages and treebanks which
employ lemmatization, and to analyze the use of
different types of POS tags in the model.

The code we used for LemmaTag is available at
https://github.com/hyperparticle/
LemmaTag.

Acknowledgments

The work described herein has been supported by
the City of Prague under the “OP PPR” program,
project No. CZ.07.1.02/0.0/0.0/16 023/0000108
and it has been using language resources devel-
oped by the LINDAT/CLARIN project of the Min-
istry of Education, Youth and Sports of the Czech
Republic (project LM2015071).

Tom´aˇs Gavenˇciak has been supported by Czech
Science Foundation (GACR) project 17-10090Y
“Network optimization”. Daniel Kondratyuk has
been supported by the Erasmus Mundus pro-
gram in Language & Communication Technolo-
gies (LCT).

References

Muhammad Abdul-Mageed, Mona Diab, and Sandra
K¨ubler. 2014. Samar: Subjectivity and sentiment
analysis for arabic social media. Computer Speech
& Language, 28(1):20–37.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Toms Bergmanis and Sharon Goldwater. 2018. Con-
text sensitive neural lemmatization with lematus. In

Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), volume 1, pages 1391–
1400.

Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richrd
Farkas, Filip Ginter, and Ja n Haji. 2013. Joint mor-
phological and syntactic analysis for richly inﬂected
languages. Transactions of the Association for Com-
putational Linguistics, 1:415–428.

Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen-Schirra, Esther K¨onig, Wolfgang Lezius,
Christian Rohrer, George Smith, and Hans Uszkor-
eit. 2004. Tiger: Linguistic interpretation of a ger-
man corpus. Research on language and computa-
tion, 2(4):597–620.

Abhisek Chakrabarty, Onkar Arun Pandit, and Utpal
Garain. 2017. Context sensitive lemmatization us-
ing two successive bidirectional gated recurrent net-
In Proceedings of the 55th Annual Meet-
works.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1481–
1491.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Learning
Schwenk, and Yoshua Bengio. 2014.
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Łukasz Kaiser. 2018. Univer-
sal transformers. arXiv preprint arXiv:1807.03819.

Steffen Eger, R¨udiger Gleim, and Alexander Mehler.
2016. Lemmatization and morphological tagging in
german and latin: A comparison and a survey of the
state-of-the-art. In LREC.

Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inﬂection and word-
formation in smt. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 664–674. As-
sociation for Computational Linguistics.

Andrea Gesmundo, James Henderson, Paola Merlo,
and Ivan Titov. 2009. A latent variable model of
synchronous syntactic-semantic parsing for multi-
In Proceedings of the Thirteenth
ple languages.
Conference on Computational Natural Language
Learning: Shared Task, CoNLL ’09, pages 37–42,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Jan Hajiˇc, Eduard Bejˇcek, Alevtina B´emov´a, Eva
Bur´aˇnov´a, Eva Hajiˇcov´a, Jiˇr´ı Havelka, Petr Ho-
Jiˇr´ı K´arn´ık, V´aclava Kettnerov´a, Na-
mola,
talia Klyueva, Veronika Kol´aˇrov´a, Lucie Kuˇcov´a,
Mark´eta Lopatkov´a, Marie Mikulov´a, Jiˇr´ı M´ırovsk´y,
Anna Nedoluzhko, Petr Pajas, Jarmila Panevov´a,

Lucie Pol´akov´a, Magdal´ena Rysov´a, Petr Sgall, Jo-
hanka Spoustov´a, Pavel Straˇn´ak, Pavl´ına Synkov´a,
Magda ˇSevˇc´ıkov´a, Jan ˇStˇep´anek, Zdeˇnka Ureˇsov´a,
ˇS´arka
Barbora Vidov´a Hladk´a, Daniel Zeman,
Zik´anov´a, and Zdenˇek ˇZabokrtsk´y. 2018. Prague
dependency treebank 3.5. LINDAT/CLARIN digital
library at the Institute of Formal and Applied Lin-
guistics ( ´UFAL, http://hdl.handle.net/
11234/1-2621), Faculty of Mathematics and
Physics, Charles University.

Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie
Mikulov´a, Zdenˇek ˇZabokrtsk´y, Magda ˇSevˇc´ıkov´a-
Raz´ımov´a, and Zdeˇnka Ureˇsov´a. 2006.
Prague
Dependency Treebank 2.0 (PDT 2.0).
LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics ( ´UFAL), Faculty of
Mathematics and Physics, Charles University.

Jan Hajiˇc, Jan Raab, Miroslav Spousta, et al. 2009.
Semi-supervised training for the averaged percep-
In Proceedings of the 12th Con-
tron pos tagger.
ference of the European Chapter of the Association
for Computational Linguistics, pages 763–771. As-
sociation for Computational Linguistics.

Jan Hajic, Otakar Smrz, Petr Zem´anek, Jan ˇSnaidauf,
and Emanuel Beˇska. 2004. Prague arabic depen-
dency treebank: Development in data and tools. In
Proc. of the NEMLAR Intern. Conf. on Arabic Lan-
guage Resources and Tools, pages 110–117.

Georg Heigold, Guenter Neumann, and Josef van Gen-
abith. 2017. An extensive empirical evaluation of
character-based morphological tagging for 14 lan-
In Proceedings of the 15th Conference of
guages.
the European Chapter of the Association for Com-
putational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 505–513.

Go Inoue, Hiroyuki Shindo, and Yuji Matsumoto.
Joint prediction of morphosyntactic cate-
2017.
gories for ﬁne-grained arabic part-of-speech tagging
In Proceed-
exploiting tag dictionary information.
ings of the 21st Conference on Computational Natu-
ral Language Learning (CoNLL 2017), pages 421–
431.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Wang Ling, Tiago Lu´ıs, Lu´ıs Marujo, Ram´on Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015. Finding function
in form: Compositional character models for open
arXiv preprint
vocabulary word representation.
arXiv:1508.02096.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efﬁcient estimation of word
arXiv preprint
representations in vector space.
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

Thomas M¨uller, Ryan Cotterell, Alexander Fraser, and
Hinrich Sch¨utze. 2015.
Joint lemmatization and
morphological tagging with lemming. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 2268–2274.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016), Paris, France. European
Language Resources Association (ELRA).

Gabriel Pereyra, George Tucker,

Jan Chorowski,
Łukasz Kaiser, and Geoffrey Hinton. 2017. Regular-
izing neural networks by penalizing conﬁdent output
distributions. arXiv preprint arXiv:1701.06548.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Cicero D Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech
In Proceedings of the 31st International
tagging.
Conference on Machine Learning (ICML-14), pages
1818–1826.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Natalia Silveira, Timothy Dozat, Marie-Catherine
de Marneffe, Samuel Bowman, Miriam Connor,
John Bauer, and Christopher D. Manning. 2014. A
gold standard dependency corpus for English.
In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC-
2014).

Milan Straka, Jan Hajic, and Jana Strakov. 2016. Ud-
pipe: Trainable pipeline for processing conll-u ﬁles
performing tokenization, morphological analysis,
pos tagging and parsing. In Proceedings of the Tenth
International Conference on Language Resources
and Evaluation (LREC 2016), Paris, France. Euro-
pean Language Resources Association (ELRA).

Jana Strakov´a, Milan Straka, and Jan Hajiˇc. 2014.
Open-Source Tools for Morphology, Lemmatiza-
tion, POS Tagging and Named Entity Recognition.
In Proceedings of 52nd Annual Meeting of the As-
sociation for Computational Linguistics: System
Demonstrations, pages 13–18, Baltimore, Mary-
land. Association for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

TensorFlow. 2018.

tf.contrib.opt.lazyadamoptimizer:
Class lazyadamoptimizer. TensorFlow documenta-
tion from tensorﬂow.org.

Reut Tsarfaty, Djam´e Seddah, Yoav Goldberg, San-
dra K¨ubler, Marie Candito, Jennifer Foster, Yannick
Versley, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical parsing of morphologically rich languages
(spmrl): what, how and whither. In Proceedings of
the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
1–12. Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

A Appendix

A.1 GPU Performance

We ran all the tests on an NVIDIA GTX 1080 Ti
GPU. The joint LemmaTag training takes about
3 hours for Arabic PADT, 4.5 hours for English
EWT, 12 hours for German TIGER, and 22 hours
for Czech PDT. The separate models take about
50% more time. After training, the lemma and
tag predictions of 219,000 test tokens of the Czech
PDT take about 100 seconds.

A.2 Other Techniques

We brieﬂy summarize some of the additional tech-
niques we have tried but which do not improve the
results. While some of those techniques do help on
smaller models or earlier in the training, the effect
on the fully trained network seems to be marginal
or even detrimental.

Separate sense prediction.

Instead of predict-
ing the sense disambiguation with the lemmatizer
(Czech only), we tried to predict sense as an addi-
tional classiﬁcation problem with one dense layer
based on ow
i and Ti, but it seems to perform
slightly worse (0.2%).

Beam search decoder. We have implemented a
beam search decoder for the lemmatizer instead of

the standard greedy one, but the improvement was
marginal (around 0.01%).

Variational dropout. While the dropouts in
the LemmaTag are completely random, variational
dropout erases the same channels across the time
steps of the RNN. While this generally improves
training in convolutional networks and RNNs, we
saw no signiﬁcant difference.

Layer normalization. Layer normalization ap-
plied to the encoding RNNs did not bring signiﬁ-
cant gain and also slowed down the training.

