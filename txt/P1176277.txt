6
1
0
2
 
y
a
M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
3
7
4
6
0
.
2
1
5
1
:
v
i
X
r
a

Quantized Convolutional Neural Networks for Mobile Devices

Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng
National Laboratory of Patter Recognition
Institute of Automation, Chinese Academy of Sciences
{jiaxiang.wu, cong.leng, yuhang.wang, qinghao.hu, jcheng}@nlpr.ia.ac.cn

Abstract

Original

Q-CNN

Time Consumption (s)

Storage Consumption (MB)

Recently, convolutional neural networks (CNN) have
demonstrated impressive performance in various computer
vision tasks. However, high performance hardware is typ-
ically indispensable for the application of CNN models
due to the high computation complexity, which prohibits
their further extensions. In this paper, we propose an efﬁ-
cient framework, namely Quantized CNN, to simultaneously
speed-up the computation and reduce the storage and mem-
ory overhead of CNN models. Both ﬁlter kernels in con-
volutional layers and weighting matrices in fully-connected
layers are quantized, aiming at minimizing the estimation
error of each layer’s response. Extensive experiments on
the ILSVRC-12 benchmark demonstrate 4 ∼ 6× speed-up
and 15 ∼ 20× compression with merely one percentage
loss of classiﬁcation accuracy. With our quantized CNN
model, even mobile devices can accurately classify images
within one second.

15

10

5

0

500

400

300

200

100

0

AlexNet

CNN-S

AlexNet

CNN-S

Memory Consumption (MB)

Top-5 Error Rate (%)

AlexNet

CNN-S

AlexNet

CNN-S

Figure 1. Comparison on the efﬁciency and classiﬁcation accuracy
between the original and quantized AlexNet [16] and CNN-S [1]
on a Huawei R(cid:13) Mate 7 smartphone.

400

300

200

100

0

25

20

15

10

5

0

1. Introduction

In recent years, we have witnessed the great success
of convolutional neural networks (CNN) [19] in a wide
range of visual applications, including image classiﬁcation
[16, 27], object detection [10, 9], age estimation [24, 23],
etc. This success mainly comes from deeper network ar-
chitectures as well as the tremendous training data. How-
ever, as the network grows deeper, the model complexity is
also increasing exponentially in both the training and testing
stages, which leads to the very high demand in the computa-
tion ability. For instance, the 8-layer AlexNet [16] involves
60M parameters and requires over 729M FLOPs1to classify
a single image. Although the training stage can be ofﬂine
carried out on high performance clusters with GPU acceler-
ation, the testing computation cost may be unaffordable for
common personal computers and mobile devices. Due to
the limited computation ability and memory space, mobile
devices are almost intractable to run deep convolutional net-
works. Therefore, it is crucial to accelerate the computation

and compress the memory consumption for CNN models.

For most CNNs, convolutional layers are the most time-
consuming part, while fully-connected layers involve mas-
sive network parameters. Due to the intrinsical differ-
ence between them, existing works usually focus on im-
proving the efﬁciency for either convolutional layers or
fully-connected layers.
In [7, 13, 32, 31, 18, 17], low-
rank approximation or tensor decomposition is adopted to
speed-up convolutional layers. On the other hand, param-
eter compression in fully-connected layers is explored in
[3, 7, 11, 30, 2, 12, 28]. Overall, the above-mentioned al-
gorithms are able to achieve faster speed or less storage.
However, few of them can achieve signiﬁcant acceleration
and compression simultaneously for the whole network.

In this paper, we propose a uniﬁed framework for con-
volutional networks, namely Quantized CNN (Q-CNN), to
simultaneously accelerate and compress CNN models with

1FLOPs: number of FLoating-point OPerations required to classify one

image with the convolutional network.

1

only minor performance degradation. With network pa-
rameters quantized, the response of both convolutional and
fully-connected layers can be efﬁciently estimated via the
approximate inner product computation. We minimize the
estimation error of each layer’s response during parameter
quantization, which can better preserve the model perfor-
mance. In order to suppress the accumulative error while
quantizing multiple layers, an effective training scheme is
introduced to take previous estimation error into consider-
ation. Our Q-CNN model enables fast test-phase compu-
tation, and the storage and memory consumption are also
signiﬁcantly reduced.

We evaluate our Q-CNN framework for image classi-
ﬁcation on two benchmarks, MNIST [20] and ILSVRC-
12 [26]. For MNIST, our Q-CNN approach achieves over
12× compression for two neural networks (no convolu-
tion), with lower accuracy loss than several baseline meth-
ods. For ILSVRC-12, we attempt to improve the test-phase
efﬁciency of four convolutional networks: AlexNet [16],
CaffeNet [15], CNN-S [1], and VGG-16 [27]. Generally,
Q-CNN achieves 4× acceleration and 15× compression
(sometimes higher) for each network, with less than 1%
drop in the top-5 classiﬁcation accuracy. Moreover, we im-
plement the quantized CNN model on mobile devices, and
dramatically improve the test-phase efﬁciency, as depicted
in Figure 1. The main contributions of this paper can be
summarized as follows:

• We propose a uniﬁed Q-CNN framework to acceler-
ate and compress convolutional networks. We demon-
strate that better quantization can be learned by mini-
mizing the estimation error of each layer’s response.
• We propose an effective training scheme to suppress
the accumulative error while quantizing the whole con-
volutional network.

• Our Q-CNN framework achieves 4 ∼ 6× speed-up
and 15 ∼ 20× compression, while the classiﬁcation
accuracy loss is within one percentage. Moreover, the
quantized CNN model can be implemented on mobile
devices and classify an image within one second.

2. Preliminary

During the test phase of convolutional networks, the
computation overhead is dominated by convolutional lay-
ers; meanwhile, the majority of network parameters are
stored in fully-connected layers. Therefore, for better test-
phase efﬁciency, it is critical to speed-up the convolution
computation and compress parameters in fully-connected
layers.

Our observation is that the forward-passing process of
both convolutional and fully-connected layers is dominated
by the computation of inner products. More formally, we
consider a convolutional layer with input feature maps S ∈

Rds×ds×Cs and response feature maps T ∈ Rdt×dt×Ct,
where ds, dt are the spatial sizes and Cs, Ct are the number
of feature map channels. The response at the 2-D spatial
position pt in the ct-th response feature map is computed
as:

Tpt (ct) = X(pk,ps)

hWct,pk , Spsi

(1)

where Wct ∈ Rdk×dk×Cs is the ct-th convolutional kernel
and dk is the kernel size. We use ps and pk to denote the
2-D spatial positions in the input feature maps and convolu-
tional kernels, and both Wct,pk and Sps are Cs-dimensional
vectors. The layer response is the sum of inner products at
all positions within the dk × dk receptive ﬁeld in the input
feature maps.

Similarly, for a fully-connected layer, we have:

T (ct) = hWct , Si

(2)

where S ∈ RCs and T ∈ RCt are the layer input and layer
response, respectively, and Wct ∈ RCs is the weighting
vector for the ct-th neuron of this layer.

Product quantization [14] is widely used in approximate
nearest neighbor search, demonstrating better performance
than hashing-based methods [21, 22]. The idea is to de-
compose the feature space as the Cartesian product of mul-
tiple subspaces, and then learn sub-codebooks for each sub-
space. A vector is represented by the concatenation of sub-
codewords for efﬁcient distance computation and storage.

In this paper, we leverage product quantization to imple-
ment the efﬁcient inner product computation. Let us con-
sider the inner product computation between x, y ∈ RD. At
ﬁrst, both x and y are split into M sub-vectors, denoted as
x(m) and y(m). Afterwards, each x(m) is quantized with a
sub-codeword from the m-th sub-codebook, then we have

hy, xi = Xm

hy(m), x(m)i ≈ Xm

hy(m), c(m)
km i

(3)

which transforms the O(D) inner product computation to
M addition operations (M ≤ D), if the inner products be-
tween each sub-vector y(m) and all the sub-codewords in
the m-th sub-codebook have been computed in advance.

Quantization-based approaches have been explored in
several works [11, 2, 12]. These approaches mostly fo-
cus on compressing parameters in fully-connected layers
[11, 2], and none of them can provide acceleration for the
test-phase computation. Furthermore, [11, 12] require the
network parameters to be re-constructed during the test-
phase, which limit the compression to disk storage instead
of memory consumption. On the contrary, our approach
offers simultaneous acceleration and compression for both
convolutional and fully-connected layers, and can reduce
the run-time memory consumption dramatically.

3. Quantized CNN

In this section, we present our approach for accelerating
and compressing convolutional networks. Firstly, we intro-
duce an efﬁcient test-phase computation process with the
network parameters quantized. Secondly, we demonstrate
that better quantization can be learned by directly minimiz-
ing the estimation error of each layer’s response. Finally,
we analyze the computation complexity of our quantized
CNN model.

3.1. Quantizing the Fully-connected Layer

For a fully-connected layer, we denote its weighting ma-
trix as W ∈ RCs×Ct, where Cs and Ct are the dimensions
of the layer input and response, respectively. The weighting
vector Wct is the ct-th column vector in W .

We evenly split the Cs-dimensional space (where Wct
lies in) into M subspaces, each of C′
s = Cs/M dimen-
sions. Each Wct is then decomposed into M sub-vectors,
denoted as W (m)
. A sub-codebook can be learned for each
subspace after gathering all the sub-vectors within this sub-
space. Formally, for the m-th subspace, we optimize:

ct

min
D(m),B(m)

2
D(m)B(m) − W (m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
s×K, B(m) ∈ {0, 1}K×Ct

s.t. D(m) ∈ RC ′

(4)

where W (m) ∈ RC ′
s×Ct consists of the m-th sub-vectors
of all weighting vectors. The sub-codebook D(m) contains
K sub-codewords, and each column in B(m) is an indica-
tor vector (only one non-zero entry), specifying which sub-
codeword is used to quantize the corresponding sub-vector.
The optimization can be solved via k-means clustering.
The layer response is approximately computed as:
, S(m)i ≈ Xm
= Xm

T (ct) = Xm

km(ct), S(m)i

hD(m)B(m)

hW (m)
ct

, S(m)i

hD(m)

(5)

ct

ct

ct

where B(m)
is the ct-th column vector in B(m), and S(m) is
the m-th sub-vector of the layer input. km(ct) is the index
of the sub-codeword used to quantize the sub-vector W (m)
.
In Figure 2, we depict the parameter quantization and
test-phase computation process of the fully-connected layer.
By decomposing the weighting matrix into M sub-matrices,
M sub-codebooks can be learned, one per subspace. During
the test-phase, the layer input is split into M sub-vectors,
denoted as S(m). For each subspace, we compute the inner
products between S(m) and every sub-codeword in D(m),
and store the results in a look-up table. Afterwards, only M
addition operations are required to compute each response.
As a result, the overall time complexity can be reduced from
O(CsCt) to O(CsK + CtM ). On the other hand, only
sub-codebooks and quantization indices need to be stored,
which can dramatically reduce the storage consumption.

Figure 2. The parameter quantization and test-phase computation
process of the fully-connected layer.

3.2. Quantizing the Convolutional Layer

Unlike the 1-D weighting vector in the fully-connected
layer, each convolutional kernel is a 3-dimensional tensor:
Wct ∈ Rdk×dk×Cs. Before quantization, we need to deter-
mine how to split it into sub-vectors, i.e. apply subspace
splitting to which dimension. During the test phase, the in-
put feature maps are traversed by each convolutional kernel
with a sliding window in the spatial domain. Since these
sliding windows are partially overlapped, we split each con-
volutional kernel along the dimension of feature map chan-
nels, so that the pre-computed inner products can be re-
used at multiple spatial locations. Speciﬁcally, we learn the
quantization in each subspace by:

min
Xpk
D(m),{B(m)
pk }
s.t. D(m) ∈ RC ′

D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

s×K, B(m)

2
pk − W (m)
pk (cid:13)
(cid:13)
F
(cid:13)
pk ∈ {0, 1}K×Ct

(6)

pk ∈ RC ′

where W (m)
s×Ct contains the m-th sub-vectors of
all convolutional kernels at position pk. The optimization
can also be solved by k-means clustering in each subspace.
With the convolutional kernels quantized, we approxi-

mately compute the response feature maps by:

Tpt(ct) = X(pk,ps) Xm
≈ X(pk,ps) Xm
= X(pk,ps) Xm

hW (m)

ct,pk , S(m)
ps i

hD(m)B(m)

ct,pk , S(m)
ps i

(7)

hD(m)

km(ct,pk), S(m)
ps i

where S(m)
is the m-th sub-vector at position ps in the in-
ps
put feature maps, and km(ct, pk) is the index of the sub-
codeword to quantize the m-th sub-vector at position pk in
the ct-th convolutional kernel.

Similar to the fully-connected layer, we pre-compute the
look-up tables of inner products with the input feature maps.
Then, the response feature maps are approximately com-
puted with (7), and both the time and storage complexity
can be greatly reduced.

and the above optimization can be solved by alternatively
updating the sub-codebook and sub-codeword assignment.
Update D(m). We ﬁx the sub-codeword assignment
B(m), and deﬁne Lk = {ct|B(m)(k, ct) = 1}. The opti-
mization in (10) can be re-formulated as:

3.3. Quantization with Error Correction

So far, we have presented an intuitive approach to quan-
tize parameters and improve the test-phase efﬁciency of
convolutional networks. However, there are still two crit-
ical drawbacks. First, minimizing the quantization error
of model parameters does not necessarily give the optimal
quantized network for the classiﬁcation accuracy. In con-
trast, minimizing the estimation error of each layer’s re-
sponse is more closely related to the network’s classiﬁca-
tion performance. Second, the quantization of one layer is
independent of others, which may lead to the accumulation
of error when quantizing multiple layers. The estimation
error of the network’s ﬁnal response is very likely to be
quickly accumulated, since the error introduced by the pre-
vious quantized layers will also affect the following layers.
To overcome these two limitations, we introduce the idea
of error correction into the quantization of network param-
eters. This improved quantization approach directly min-
imizes the estimation error of the response at each layer,
and can compensate the error introduced by previous lay-
ers. With the error correction scheme, we can quantize the
network with much less performance degradation than the
original quantization method.

3.3.1 Error Correction for the Fully-connected Layer

Suppose we have N images to learn the quantization of a
fully-connected layer, and the layer input and response of
image In are denoted as Sn and Tn. In order to minimize
the estimation error of the layer response, we optimize:

Xn

min
{D(m)},{B(m)}

Tn − Xm
(cid:13)
(cid:13)
(cid:13)

2
(D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)
(8)
where the ﬁrst term in the Frobenius norm is the desired
layer response, and the second term is the approximated
layer response computed via the quantized parameters.

A block coordinate descent approach can be applied to
minimize this objective function. For the m-th subspace, its
residual error is deﬁned as:

R(m)

n = Tn − Xm′6=m

(D(m′

)B(m′

))T S(m′

n

)

(9)

and then we attempt to minimize the residual error of this
subspace, which is:

min

D(m),B(m) Xn

2
n − (D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)

R(m)
(cid:13)
(cid:13)
(cid:13)

(10)

min
{D(m)
k

}

Xn,k Xct∈Lk

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(11)

which implies that the optimization over one sub-codeword
does not affect other sub-codewords. Hence, for each sub-
codeword, we construct a least square problem from (11) to
update it.

Update B(m). With the sub-codebook D(m) ﬁxed, it
is easy to discover that the optimization of each column in
B(m) is mutually independent. For the ct-th column, its
optimal sub-codeword assignment is given by:

k∗
m(ct) = arg min

k Xn

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(12)

3.3.2 Error Correction for the Convolutional Layer

We adopt the similar idea to minimize the estimation error
of the convolutional layer’s response feature maps, that is:

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

(D(m)B(m)

min
{D(m)},{B(m)
pk }

Tn,pt − X
(pk ,ps)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(13)
The optimization also can be solved by block coordinate
descent. More details on solving this optimization can be
found in the supplementary material.

pk )T S(m)
n,ps

X
m

3.3.3 Error Correction for Multiple Layers

The above quantization method can be sequentially applied
to each layer in the CNN model. One concern is that the
estimation error of layer response caused by the previous
layers will be accumulated and affect the quantization of
the following layers. Here, we propose an effective training
scheme to address this issue.

We consider the quantization of a speciﬁc layer, assum-
ing its previous layers have already been quantized. The
optimization of parameter quantization is based on the layer
input and response of a group of training images. To quan-
tize this layer, we take the layer input in the quantized net-
work as {Sn}, and the layer response in the original net-
work (not quantized) as {Tn} in Eq. (8) and (13). In this
way, the optimization is guided by the actual input in the
quantized network and the desired response in the original
network. The accumulative error introduced by the previ-
ous layers is explicitly taken into consideration during op-
timization. In consequence, this training scheme can effec-
tively suppress the accumulative error for the quantization
of multiple layers.

Another possible solution is to adopt back-propagation
to jointly update the sub-codebooks and sub-codeword as-
signments in all quantized layers. However, since the sub-
codeword assignments are discrete, the gradient-based op-
timization can be quite difﬁcult, if not entirely impossible.
Therefore, back-propagation is not adopted here, but could
be a promising extension for future work.

3.4. Computation Complexity

Now we analyze the test-phase computation complex-
ity of convolutional and fully-connected layers, with or
without parameter quantization. For our proposed Q-CNN
model, the forward-passing through each layer mainly con-
sists of two procedures: pre-computation of inner products,
and approximate computation of layer response. Both sub-
codebooks and sub-codeword assignments are stored for the
test-phase computation. We report the detailed comparison
on the computation and storage overhead in Table 1.

Table 1. Comparison on the computation and storage overhead of
convolutional and fully-connected layers.

FLOPs

Bytes

Conv.

FCnt.

Conv.

FCnt.

kM

kCs
t Ctd2

t Ctd2
d2
CNN
sCsK + d2
d2
Q-CNN
CNN
CsCt
Q-CNN
CsK + CtM
4d2
CNN
kCsCt
Q-CNN 4CsK + 1
8 d2
kM Ct log2 K
CNN
4CsCt
Q-CNN
8 M Ct log2 K

4CsK + 1

As we can see from Table 1, the reduction in the compu-
tation and storage overhead largely depends on two hyper-
parameters, M (number of subspaces) and K (number of
sub-codewords in each subspace). Large values of M and
K lead to more ﬁne-grained quantization, but is less efﬁ-
cient in the computation and storage consumption. In prac-
tice, we can vary these two parameters to balance the trade-
off between the test-phase efﬁciency and accuracy loss of
the quantized CNN model.

4. Related Work

There have been a few attempts in accelerating the test-
phase computation of convolutional networks, and many are
inspired from the low-rank decomposition. Denton et al.
[7] presented a series of low-rank decomposition designs
for convolutional kernels. Similarly, CP-decomposition was
adopted in [17] to transform a convolutional layer into mul-
tiple layers with lower complexity. Zhang et al. [32, 31]
considered the subsequent nonlinear units while learning
the low-rank decomposition. [18] applied group-wise prun-
ing to the convolutional tensor to decompose it into the mul-
tiplications of thinned dense matrices. Recently, ﬁxed-point
based approaches are explored in [5, 25]. By representing

the connection weights (or even network activations) with
ﬁxed-point numbers, the computation can greatly beneﬁt
from hardware acceleration.

Another parallel research trend is to compress parame-
ters in fully-connected layers. Ciresan et al. [3] randomly
remove connection to reduce network parameters. Matrix
factorization was adopted in [6, 7] to decompose the weight-
ing matrix into two low-rank matrices, which demonstrated
that signiﬁcant redundancy did exist in network parameters.
Hinton et al. [8] proposed to use dark knowledge (the re-
sponse of a well-trained network) to guide the training of
a much smaller network, which was superior than directly
training. By exploring the similarity among neurons, Srini-
vas et al. [28] proposed a systematic way to remove redun-
dant neurons instead of network connections. In [30], mul-
tiple fully-connected layers were replaced by a single “Fast-
food” layer, which can be trained in an end-to-end style with
[2] randomly grouped
convolutional layers. Chen et al.
connection weights into hash buckets, and then ﬁne-tuned
the network with back-propagation. [12] combined prun-
ing, quantization, and Huffman coding to achieve higher
compression rate. Gong et al. [11] adopted vector quanti-
zation to compress the weighing matrix, which was actually
a special case of our approach (apply Q-CNN without error
correction to fully-connected layers only).

5. Experiments

In this section, we evaluate our quantized CNN frame-
work on two image classiﬁcation benchmarks, MNIST [20]
and ILSVRC-12 [26]. For the acceleration of convolutional
layers, we compare with:

• CPD [17]: CP-Decomposition;
• GBD [18]: Group-wise Brain Damage;
• LANR [31]: Low-rank Approximation of Non-linear

Responses.

and for the compression of fully-connected layers, we com-
pare with the following approaches:

• RER [3]: Random Edge Removal;
• LRD [6]: Low-Rank Decomposition;
• DK [8]: Dark Knowledge;
• HashNet [2]: Hashed Neural Nets;
• DPP [28]: Data-free Parameter Pruning;
• SVD [7]: Singular Value Decomposition;
• DFC [30]: Deep Fried Convnets.

For all above baselines, we use their reported results under
the same setting for fair comparison. We report the theo-
retical speed-up for more consistent results, since the real-
istic speed-up may be affected by various factors, e.g. CPU,
cache, and RAM. We compare the theoretical and realistic
speed-up in Section 5.4, and discuss the effect of adopting
the BLAS library for acceleration.

Our approaches are denoted as “Q-CNN” and “Q-CNN
(EC)”, where the latter one adopts error correction while the
former one does not. We implement the optimization pro-
cess of parameter quantization in MATLAB, and ﬁne-tune
the resulting network with Caffe [15]. Additional results of
our approach can be found in the supplementary material.

5.1. Results on MNIST

The MNIST dataset contains 70k images of hand-written
digits, 60k used for training and 10k for testing. To evalu-
ate the compression performance, we pre-train two neural
networks, one is 3-layer and another one is 5-layer, where
each hidden layer contains 1000 units. Different compres-
sion techniques are then adopted to compress these two net-
work, and the results are as depicted in Table 2.

Table 2. Comparison on the compression rates and classiﬁcation
error on MNIST, based on a 3-layer network (784-1000-10) and a
5-layer network (784-1000-1000-1000-10).

Method

Original
RER [3]
LRD [6]
DK [8]
HashNets [2]
Q-CNN
Q-CNN (EC)

3-layer

5-layer

Error
1.35%
2.19%
1.89%
1.71%
1.43%

Compr.
-
8×
8×
8×
8×

Compr.
-
8×
8×
8×
8×

Error
1.12%
1.24%
1.77%
1.26%
1.22%
12.1× 1.42% 13.4× 1.34%
12.1× 1.39% 13.4× 1.19%

s is determined once C′

In our Q-CNN framework, the trade-off between accu-
racy and efﬁciency is controlled by M (number of sub-
spaces) and K (number of sub-codewrods in each sub-
space). Since M = Cs/C′
s is given,
we tune (C′
s, K) to adjust the quantization precision. In Ta-
ble 2, we set the hyper-parameters as C′
s = 4 and K = 32.
From Table 2, we observe that our Q-CNN (EC) ap-
proach offers higher compression rates with less perfor-
mance degradation than all baselines for both networks.
The error correction scheme is effective in reducing the ac-
curacy loss, especially for deeper networks (5-layer). Also,
we ﬁnd the performance of both Q-CNN and Q-CNN (EC)
quite stable, as the standard deviation of ﬁve random runs is
merely 0.05%. Therefore, we report the single-run perfor-
mance in the remaining experiments.

5.2. Results on ILSVRC-12

The ILSVRC-12 benchmark consists of over one million
training images drawn from 1000 categories, and a disjoint
validation set of 50k images. We report both the top-1 and
top-5 classiﬁcation error rates on the validation set, using
single-view testing (central patch only).

We demonstrate our approach on four convolutional net-
works: AlexNet [16], CaffeNet [15], CNN-S [1], and VGG-

16 [27]. The ﬁrst two models have been adopted in several
related works, and therefore are included for comparison.
CNN-S and VGG-16 use a either wider or deeper structure
for better classiﬁcation accuracy, and are included here to
prove the scalability of our approach. We compare all these
networks’ computation and storage overhead in Table 3, to-
gether with their classiﬁcation error rates on ILSVRC-12.

Table 3. Comparison on the test-phase computation overhead
(FLOPs), storage consumption (Bytes), and classiﬁcation error
rates (Top-1/5 Err.) of AlexNet, CaffeNet, CNN-S, and VGG-16.
Bytes
2.44e+8
2.44e+8
4.12e+8
5.53e+8

Top-5 Err.
19.74%
19.59%
15.82%
10.05%

Top-1 Err.
42.78%
42.53%
37.31%
28.89%

FLOPs
7.29e+8
7.27e+8
2.94e+9
1.55e+10

Model
AlexNet
CaffeNet
CNN-S
VGG-16

5.2.1 Quantizing the Convolutional Layer

To begin with, we quantize the second convolutional layer
of AlexNet, which is the most time-consuming layer during
the test-phase. In Table 4, we report the performance un-
der several (C′
s, K) settings, comparing with two baseline
methods, CPD [17] and GBD [18].

Table 4. Comparison on the speed-up rates and the increase of top-
1/5 error rates for accelerating the second convolutional layer in
AlexNet, with or without ﬁne-tuning (FT). The hyper-parameters
of Q-CNN, C ′

Method

CPD

GBD

Q-CNN

Q-CNN
(EC)

Para.

Speed-up

s and K, are as speciﬁed in the “Para.” column.
Top-5 Err. ↑
Top-1 Err. ↑
FT
FT
0.44%
-
1.22%
-
18.63%
-
-
0.11%
-
0.43%
-
1.13%
1.37%
1.63%
2.27%
2.90%
1.28%
1.57%
2.66%
2.91%
0.17%
0.20%
0.40%
0.39%
0.21%
0.11%
0.31%
0.33%

No FT
0.94%
3.20%
69.06%
-
-
-
8.97%
14.71%
9.10%
18.05%
0.27%
0.50%
0.34%
0.50%

No FT
-
-
-
12.43%
21.93%
48.33%
10.55%
15.93%
10.62%
18.84%
0.35%
0.64%
0.27%
0.55%

3.19×
4.52×
6.51×
3.33×
5.00×
10.00×
3.70×
5.36×
4.84×
6.06×
3.70×
5.36×
4.84×
6.06×

-
-
-
-
-
-
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

From Table 4, we discover that with a large speed-up
rate (over 4×), the performance loss of both CPD and GBD
become severe, especially before ﬁne-tuning. The naive
parameter quantization method also suffers from the sim-
ilar problem. By incorporating the idea of error correction,
our Q-CNN model achieves up to 6× speed-up with merely
0.6% drop in accuracy, even without ﬁne-tuning. The ac-
curacy loss can be further reduced after ﬁne-tuning the sub-
sequent layers. Hence, it is more effective to minimize the
estimation error of each layer’s response than minimize the
quantization error of network parameters.

Next, we take one step further and attempt to speed-up
all the convolutional layers in AlexNet with Q-CNN (EC).

Table 5. Comparison on the speed-up/compression rates and the increase of top-1/5 error rates for accelerating all the convolutional layers
in AlexNet and VGG-16.

Model

Method

Para.

Speed-up Compression

AlexNet

Q-CNN
(EC)

VGG-16

LANR [31]
Q-CNN (EC)

4/64
6/64
6/128
8/128
-
6/128

3.32×
4.32×
3.71×
4.27×
4.00×
4.06×

10.58×
14.32×
10.27×
12.08×
2.73×
14.40×

Top-1 Err. ↑
FT
-
-

Top-5 Err. ↑
FT
-
-

No FT
0.94%
1.90%

No FT
1.33%
2.32%
1.44% 0.13% 1.16% 0.36%
2.25% 0.99% 1.64% 0.60%
0.95% 0.35%
3.04% 1.06% 1.83% 0.45%

-

-

We ﬁx the quantization hyper-parameters (C′
s, K) across all
layers. From Table 5, we observe that the loss in accuracy
grows mildly than the single-layer case. The speed-up rates
reported here are consistently smaller than those in Table 4,
since the acceleration effect is less signiﬁcant for some lay-
ers (i.e. “conv 4” and “conv 5”). For AlexNet, our Q-CNN
model (C′
s = 8, K = 128) can accelerate the computation
of all the convolutional layers by a factor of 4.27×, while
the increase in the top-1 and top-5 error rates are no more
than 2.5%. After ﬁne-tuning the remaining fully-connected
layers, the performance loss can be further reduced to less
than 1%.

In Table 5, we also report the comparison against LANR
[31] on VGG-16. For the similar speed-up rate (4×), their
approach outperforms ours in the top-5 classiﬁcation error
(an increase of 0.95% against 1.83%). After ﬁne-tuning, the
performance gap is narrowed down to 0.35% against 0.45%.
At the same time, our approach offers over 14× compres-
sion of parameters in convolutional layers, much larger than
theirs 2.7× compression2. Therefore, our approach is effec-
tive in accelerating and compressing networks with many
convolutional layers, with only minor performance loss.

5.2.2 Quantizing the Fully-connected Layer

For demonstration, we ﬁrst compress parameters in a single
fully-connected layer. In CaffeNet, the ﬁrst fully-connected
layer possesses over 37 million parameters (9216 × 4096),
more than 60% of whole network parameters. Our Q-CNN
approach is adopted to quantize this layer and the results are
as reported in Table 6. The performance loss of our Q-CNN
model is negligible (within 0.4%), which is much smaller
than baseline methods (DPP and SVD). Furthermore, error
correction is effective in preserving the classiﬁcation accu-
racy, especially under a higher compression rate.

Now we evaluate our approach’s performance for com-
pressing all the fully-connected layers in CaffeNet in Ta-
ble 7. The third layer is actually the combination of 1000
classiﬁers, and is more critical to the classiﬁcation accuracy.
Hence, we adopt a much more ﬁne-grained hyper-parameter

2The compression effect of their approach was not explicitly discussed
in the paper; we estimate the compression rate based on their description.

DPP

Method

Table 6. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing the ﬁrst fully-connected layer
in CaffeNet, without ﬁne-tuning.
Compression
Para.
1.19×
-
1.47×
-
1.91×
-
2.75×
-
1.38×
-
2.77×
-
-
5.54×
11.08×
-
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32

Top-5 Err. ↑
-
-
-
-
-0.03%
0.07%
0.19%
0.86%
0.19%
0.28%
0.12%
0.16%
0.07%
0.03%
0.11%
0.12%

Top-1 Err. ↑
0.16%
1.76%
4.08%
9.68%
0.03%
0.07%
0.36%
1.23%
0.19%
0.35%
0.18%
0.28%
0.10%
0.18%
0.14%
0.16%

Q-CNN
(EC)

Q-CNN

SVD

setting (C′
s = 1, K = 16) for this layer. Although the
speed-up effect no longer exists, we can still achieve around
8× compression for the last layer.

SVD

Method

Table 7. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CaffeNet. Both SVD and DFC are ﬁne-tuned, while Q-CNN
and Q-CNN (EC) are not ﬁne-tuned.
Para.
-
-
-
-
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
1.26×
2.52×
1.79×
3.58×
13.96×
19.14×
15.25×
18.71×
13.96×
19.14×
15.25×
18.71×

Top-5 Err. ↑
-
-
-
-
0.29%
0.47%
0.34%
0.59%
0.30%
0.47%
0.27%
0.39%

Top-1 Err. ↑
0.14%
1.22%
-0.66%
0.31%
0.28%
0.70%
0.44%
0.75%
0.31%
0.59%
0.31%
0.57%

Q-CNN
(EC)

Q-CNN

DFC

From Table 7, we discover that with less than 1% drop in
accuracy, Q-CNN achieves high compression rates (12 ∼
20×), much larger than that of SVD3and DFC (< 4×).
Again, Q-CNN with error correction consistently outper-
forms the naive Q-CNN approach as adopted in [11].

3In Table 6, SVD means replacing the weighting matrix with the multi-
plication of two low-rank matrices; in Table 7, SVD means ﬁne-tuning the
network after the low-rank matrix decomposition.

5.2.3 Quantizing the Whole Network

So far, we have evaluated the performance of CNN models
with either convolutional or fully-connected layers quan-
tized. Now we demonstrate the quantization of the whole
network with a three-stage strategy. Firstly, we quantize all
the convolutional layers with error correction, while fully-
connected layers remain untouched. Secondly, we ﬁne-tune
fully-connected layers in the quantized network with the
ILSVRC-12 training set to restore the classiﬁcation accu-
racy. Finally, fully-connected layers in the ﬁne-tuned net-
work are quantized with error correction. We report the
performance of our Q-CNN models in Table 8.

Table 8. The speed-up/compression rates and the increase of top-
1/5 error rates for the whole CNN model. Particularly, for the
quantization of the third fully-connected layer in each network,
we let C ′
Model

s = 1 and K = 16.
Para.

Compression

Top-1/5 Err. ↑

Speed-up

AlexNet

CaffeNet

CNN-S

VGG-16

Conv.
8/128
8/128
8/128
8/128
8/128
8/128
6/128
6/128

FCnt.
3/32
4/32
3/32
4/32
3/32
4/32
3/32
4/32

4.05×
4.15×
4.04×
4.14×
5.69×
5.78×
4.05×
4.06×

15.40×
18.76×
15.40×
18.76×
16.32×
20.16×
16.55×
20.34×

1.38% / 0.84%
1.46% / 0.97%
1.43% / 0.99%
1.54% / 1.12%
1.48% / 0.81%
1.64% / 0.85%
1.22% / 0.53%
1.35% / 0.58%

For convolutional layers, we let C′

s = 8 and K = 128
for AlexNet, CaffeNet, and CNN-S, and let C′
s = 6 and
K = 128 for VGG-16, to ensure roughly 4 ∼ 6× speed-
up for each network. Then we vary the hyper-parameter
settings in fully-connected layers for different compression
levels. For the former two networks, we achieve 18× com-
pression with about 1% loss in the top-5 classiﬁcation accu-
racy. For CNN-S, we achieve 5.78× speed-up and 20.16×
compression, while the top-5 classiﬁcation accuracy drop is
merely 0.85%. The result on VGG-16 is even more encour-
aging: with 4.06× speed-up and 20.34×, the increase of
top-5 error rate is only 0.58%. Hence, our proposed Q-CNN
framework can improve the efﬁciency of convolutional net-
works with minor performance loss, which is acceptable in
many applications.

5.3. Results on Mobile Devices

We have developed an Android application to fulﬁll
CNN-based image classiﬁcation on mobile devices, based
on our Q-CNN framework. The experiments are carried
out on a Huawei R(cid:13) Mate 7 smartphone, equipped with an
1.8GHz Kirin 925 CPU. The test-phase computation is car-
ried out on a single CPU core, without GPU acceleration.

In Table 9, we compare the computation efﬁciency and
classiﬁcation accuracy of the original and quantized CNN
models. Our Q-CNN framework achieves 3× speed-up for
AlexNet, and 4× speed-up for CNN-S. What’s more, we
compress the storage consumption by 20 ×, and the re-

Table 9. Comparison on the time, storage, memory consumption,
and top-5 classiﬁcation error rates of the original and quantized
AlexNet and CNN-S.
Model

AlexNet

CNN-S

CNN
Q-CNN
CNN
Q-CNN

Time
2.93s
0.95s
10.58s
2.61s

Storage
232.56MB
12.60MB
392.57MB
20.13MB

Memory
264.74MB
74.65MB
468.90MB
129.49MB

Top-5 Err.
19.74%
20.70%
15.82%
16.68%

quired run-time memory is only one quarter of the original
model. At the same time, the loss in the top-5 classiﬁcation
accuracy is no more than 1%. Therefore, our proposed ap-
proach improves the run-time efﬁciency in multiple aspects,
making the deployment of CNN models become tractable
on mobile platforms.

5.4. Theoretical vs. Realistic Speed-up

In Table 10, we compare the theoretical and realistic
speed-up on AlexNet. The BLAS [29] library is used in
Caffe [15] to accelerate the matrix multiplication in con-
volutional and fully-connected layers. However, it may not
always be an option for mobile devices. Therefore, we mea-
sure the run-time speed under two settings, i.e. with BLAS
enabled or disabled. The realistic speed-up is slightly lower
with BLAS on, indicating that Q-CNN does not beneﬁt as
much from BLAS as that of CNN. Other optimization tech-
niques, e.g. SIMD, SSE, and AVX [4], may further improve
our realistic speed-up, and shall be explored in the future.

Table 10. Comparison on the theoretical and realistic speed-up on
AlexNet (CPU only, single-threaded). Here we use the ATLAS
library, which is the default BLAS choice in Caffe [15].

BLAS

Off
On

FLOPs

CNN

Q-CNN

7.29e+8

1.75e+8

Time (ms)

Speed-up

CNN
321.10
167.794

Q-CNN
75.62
55.35

Theo.

4.15×

Real.
4.25×
3.03×

6. Conclusion

In this paper, we propose a uniﬁed framework to si-
multaneously accelerate and compress convolutional neural
networks. We quantize network parameters to enable ef-
ﬁcient test-phase computation. Extensive experiments are
conducted on MNIST and ILSVRC-12, and our approach
achieves outstanding speed-up and compression rates, with
only negligible loss in the classiﬁcation accuracy.

7. Acknowledgement

This work was supported in part by National Natural Sci-
ence Foundation of China (Grant No. 61332016), and 863
program (Grant No. 2014AA015105).

4This is Caffe’s run-time speed. The code for the other three settings is

on https://github.com/jiaxiang-wu/quantized-cnn.

[22] C. Leng, J. Wu, J. Cheng, X. Zhang, and H. Lu. Hashing for dis-
In International Conference on Machine Learning

tributed data.
(ICML), pages 1642–1650, 2015. 2

[23] G. Levi and T. Hassncer. Age and gender classiﬁcation using convo-
lutional neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), pages 34–42, 2015.
1

[24] C. Li, Q. Liu, J. Liu, and H. Lu. Learning ordinal discriminative
features for age estimation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2570–2577, 2012. 1
[25] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks.
CoRR, abs/1603.05279, 2016. 5

[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei. Imagenet large scale visual recognition challenge. Inter-
national Journal of Computer Vision (IJCV), pages 1–42, 2015. 2,
5

[27] K. Simonyan and A. Zisserman. Very deep convolutional networks
In International Conference on

for large-scale image recognition.
Learning Representations (ICLR), 2015. 1, 2, 6

[28] S. Srinivas and R. V. Babu. Data-free parameter pruning for deep
In British Machine Vision Conference (BMVC),

neural networks.
pages 31.1–31.12, 2015. 1, 5

[29] R. C. Whaley and A. Petitet. Minimizing development and mainte-
nance costs in supporting persistently optimized BLAS. Software:
Practice and Experience, 35(2):101–121, Feb 2005. 8

[30] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. CoRR, abs/1412.7149,
2014. 1, 5

[31] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep
convolutional networks for classiﬁcation and detection. CoRR,
abs/1505.06798, 2015. 1, 5, 7

[32] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient and accurate
approximations of nonlinear convolutional networks. In IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), pages
1984–1992, 2015. 1, 5

References

[1] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return
of the devil in the details: Delving deep into convolutional nets. In
British Machine Vision Conference (BMVC), 2014. 1, 2, 6

[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In International
Conference on Machine Learning (ICML), pages 2285–2294, 2015.
1, 2, 5, 6

[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmid-
huber. High-performance neural networks for visual object classiﬁ-
cation. CoRR, abs/1102.0183, 2011. 1, 5, 6

[4] I. Corporation. Intel architecture instruction set extensions program-
ming reference. Technical report, Intel Corporation, Feb 2016. 8
[5] M. Courbariaux, Y. Bengio, and J. David. Training deep neural net-
In International Confer-

works with low precision multiplications.
ence on Learning Representations (ICLR), 2015. 5

[6] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas.
Predicting parameters in deep learning. In Advances in Neural In-
formation Processing Systems (NIPS), pages 2148–2156, 2013. 5,
6

[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Ex-
ploiting linear structure within convolutional networks for efﬁcient
evaluation. In Advances in Neural Information Processing Systems
(NIPS), pages 1269–1277, 2014. 1, 5

[8] J. D. Geoffrey Hinton, Oriol Vinyals. Distilling the knowledge in a

neural network. CoRR, abs/1503.02531, 2015. 5, 6

[9] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 1
[10] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 580–587, 2014. 1

[11] Y. Gong, L. Liu, M. Yang, and L. D. Bourdev. Compressing
deep convolutional networks using vector quantization. CoRR,
abs/1412.6115, 2014. 1, 2, 5, 7

[12] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding. CoRR, abs/1510.00149, 2015. 1, 2, 5

[13] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolu-
tional neural networks with low rank expansions. In British Machine
Vision Conference (BMVC), 2014. 1

[14] H. Jegou, M. Douze, and C. Schmid. Product quantization for near-
est neighbor search. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 33(1):117–128, Jan 2011. 2

[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. CoRR, abs/1408.5093, 2014. 2, 6, 8
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 1106–1114, 2012. 1,
2, 6

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lem-
pitsky. Speeding-up convolutional neural networks using ﬁne-tuned
cp-decomposition. In International Conference on Learning Repre-
sentations (ICLR), 2015. 1, 5, 6

[18] V. Lebedev and V. S. Lempitsky. Fast convnets using group-wise

brain damage. CoRR, abs/1506.02515, 2015. 1, 5, 6

[19] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. E. Hubbard, and L. D. Jackel. Backpropagation applied to hand-
written zip code recognition. Neural Computation, 1(4):541–551,
1989. 1

[20] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998. 2, 5

[21] C. Leng, J. Wu, J. Cheng, X. Bai, and H. Lu. Online sketching hash-
ing. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2503–2511, 2015. 2

Appendix A: Additional Results

In the submission, we report the performance after quan-
tizing all the convolutional layers in AlexNet, and quan-
tizing all the full-connected layers in CaffeNet. Here, we
present experimental results for some other settings.

Quantizing Convolutional Layers in CaffeNet

We quantize all the convolutional layers in CaffeNet, and
the results are as demonstrated in Table 11. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.15% and 0.75%, compared to the original
CaffeNet.

Table 11. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CaffeNet, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.32×
4.32×
3.71×
4.27×
3.32×
4.32×
3.71×
4.27×

Top-1 Err. ↑
18.69%
32.84%
20.08%
35.48%
1.22%
2.44%
1.57%
2.30%

Top-5 Err. ↑
16.73%
33.55%
18.31%
37.82%
0.97%
1.83%
1.12%
1.71%

Quantizing Convolutional Layers in CNN-S

We quantize all the convolutional layers in CNN-S, and
the results are as demonstrated in Table 12. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.24% and 0.63%, compared to the original
CNN-S.

Table 12. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CNN-S, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.69×
5.17×
4.78×
5.92×
3.69×
5.17×
4.78×
5.92×

Top-1 Err. ↑
19.87%
45.74%
27.86%
46.18%
1.60%
3.49%
2.07%
3.42%

Top-5 Err. ↑
16.77%
48.67%
25.09%
50.26%
0.92%
2.32%
1.32%
2.17%

Quantizing Fully-connected Layers in AlexNet

We quantize all the fully-connected layers in AlexNet,

and the results are as demonstrated in Table 13.

Quantizing Fully-connected Layers in CNN-S

We quantize all the fully-connected layers in CNN-S,

Method

Table 13. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in AlexNet, without ﬁne-tuning.
Compression
Para.
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32

Top-5 Err. ↑
0.27%
0.64%
0.33%
0.69%
0.20%
0.22%
0.21%
0.38%

Top-1 Err. ↑
0.25%
0.77%
0.54%
0.71%
0.14%
0.40%
0.40%
0.46%

Q-CNN
(EC)

Q-CNN

Method

Table 14. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CNN-S, without ﬁne-tuning.
Para.
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
14.37×
20.15×
15.79×
19.66×
14.37×
20.15×
15.79×
19.66×

Top-5 Err. ↑
0.07%
0.22%
0.11%
0.27%
0.14%
0.24%
0.11%
0.27%

Top-1 Err. ↑
0.22%
0.45%
0.21%
0.35%
0.36%
0.43%
0.29%
0.56%

Q-CNN
(EC)

Q-CNN

Appendix B: Optimization in Section 3.3.2

Assume we have N images to learn the quantization of a
convolutional layer. For image In, we denote its input fea-
ture maps as Sn ∈ Rds×ds×Cs and response feature maps
as Tn ∈ Rdt×dt×Ct, where ds, dt are the spatial sizes and
Cs, Ct are the number of feature map channels. We use
ps and pt to denote the spatial location in the input and re-
sponse feature maps. The spatial location in the convolu-
tional kernels is denoted as pk.

To learn quantization with error correction for the con-

volutional layer, we attempt to optimize:

2

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
m

X
n,pt

X
(pk,ps)

(D(m)B(m)

pk )T S(m)

min
{D(m)},{B(m)
pk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(14)
where Dm is the m-th sub-codebook, and B(m)
is the cor-
pk
responding sub-codeword assignment indicator for the con-
volutional kernels at spatial location pk.

n,ps − Tn,pt

Similar to the fully-connected layer, we adopt a block co-
ordinate descent approach to solve this optimization prob-
lem. For the m-th subspace, we ﬁrstly deﬁne its residual
feature map as:

R(m)

n,pt = Tn,pt − X
(pk,ps)

X
m′6=m

(D(m′

)B(m′
pk

)

)T S(m′
)
n,ps

(15)

and the results are as demonstrated in Table 14.

and then the optimization in the m-th subspace can be re-

formulated as:

min
D(m),{B(m)
pk }

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(16)
Update D(m). With the assignment indicator {B(m)
pk }

n,ps − R(m)
n,pt

pk )T S(m)

(D(m)B(m)

X
(pk,ps)

X
n,pt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ﬁxed, we let:

Lk,pk = {ct|B(m)

pk (k, ct) = 1}

(17)

We greedily update each sub-codeword in the m-th sub-
codebook D(m) in a sequential style. For the k-th sub-
codeword, we compute the corresponding residual feature
map as:

Q(m)

n,pt,k(ct) = R(m)

n,pt (ct) − X
(pk ,ps)

X
k′6=k

X
ct∈Lk′,pk

D(m)T
k′

S(m)
n,ps

(18)

and then we can alternatively optimize:

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

min
D(m)
k

D(m)T

X
(pk,ps)

2
(cid:13)
(cid:13)
n,pt,k(ct)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(19)
which can be transformed into a least square problem. By
solving it, we can update the k-th sub-codeword.

n,ps − Q(m)
S(m)

X
ct∈Lk,pk

Update {B(m)

pk }. We greedily update the sub-codeword
assignment at each spatial location in the convolutional ker-
nels in a sequential style. For the spatial location pk, we
compute the corresponding residual feature map as:

P (m)
n,pt,pk = R(m)

(D(m)B(m)

p′
k

)T S(m)
n,p′
s

(20)

n,pt − X
(p′
k,p′
s)
pk6=pk

and then the optimization can be re-written as:

min
B(m)
pk

X
n,pt

(D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

pk )T S(m)

n,ps − P (m)

2
n,pt,pk (cid:13)
(cid:13)
F
(cid:13)

(21)

Since B(m)
pk ∈ {0, 1}K is an indicator vector (only one non-
zero entry), we can exhaustively try all sub-codewords and
select the optimal one that minimize the objective function.

6
1
0
2
 
y
a
M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
3
7
4
6
0
.
2
1
5
1
:
v
i
X
r
a

Quantized Convolutional Neural Networks for Mobile Devices

Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng
National Laboratory of Patter Recognition
Institute of Automation, Chinese Academy of Sciences
{jiaxiang.wu, cong.leng, yuhang.wang, qinghao.hu, jcheng}@nlpr.ia.ac.cn

Abstract

Original

Q-CNN

Time Consumption (s)

Storage Consumption (MB)

Recently, convolutional neural networks (CNN) have
demonstrated impressive performance in various computer
vision tasks. However, high performance hardware is typ-
ically indispensable for the application of CNN models
due to the high computation complexity, which prohibits
their further extensions. In this paper, we propose an efﬁ-
cient framework, namely Quantized CNN, to simultaneously
speed-up the computation and reduce the storage and mem-
ory overhead of CNN models. Both ﬁlter kernels in con-
volutional layers and weighting matrices in fully-connected
layers are quantized, aiming at minimizing the estimation
error of each layer’s response. Extensive experiments on
the ILSVRC-12 benchmark demonstrate 4 ∼ 6× speed-up
and 15 ∼ 20× compression with merely one percentage
loss of classiﬁcation accuracy. With our quantized CNN
model, even mobile devices can accurately classify images
within one second.

15

10

5

0

500

400

300

200

100

0

AlexNet

CNN-S

AlexNet

CNN-S

Memory Consumption (MB)

Top-5 Error Rate (%)

AlexNet

CNN-S

AlexNet

CNN-S

Figure 1. Comparison on the efﬁciency and classiﬁcation accuracy
between the original and quantized AlexNet [16] and CNN-S [1]
on a Huawei R(cid:13) Mate 7 smartphone.

400

300

200

100

0

25

20

15

10

5

0

1. Introduction

In recent years, we have witnessed the great success
of convolutional neural networks (CNN) [19] in a wide
range of visual applications, including image classiﬁcation
[16, 27], object detection [10, 9], age estimation [24, 23],
etc. This success mainly comes from deeper network ar-
chitectures as well as the tremendous training data. How-
ever, as the network grows deeper, the model complexity is
also increasing exponentially in both the training and testing
stages, which leads to the very high demand in the computa-
tion ability. For instance, the 8-layer AlexNet [16] involves
60M parameters and requires over 729M FLOPs1to classify
a single image. Although the training stage can be ofﬂine
carried out on high performance clusters with GPU acceler-
ation, the testing computation cost may be unaffordable for
common personal computers and mobile devices. Due to
the limited computation ability and memory space, mobile
devices are almost intractable to run deep convolutional net-
works. Therefore, it is crucial to accelerate the computation

and compress the memory consumption for CNN models.

For most CNNs, convolutional layers are the most time-
consuming part, while fully-connected layers involve mas-
sive network parameters. Due to the intrinsical differ-
ence between them, existing works usually focus on im-
proving the efﬁciency for either convolutional layers or
fully-connected layers.
In [7, 13, 32, 31, 18, 17], low-
rank approximation or tensor decomposition is adopted to
speed-up convolutional layers. On the other hand, param-
eter compression in fully-connected layers is explored in
[3, 7, 11, 30, 2, 12, 28]. Overall, the above-mentioned al-
gorithms are able to achieve faster speed or less storage.
However, few of them can achieve signiﬁcant acceleration
and compression simultaneously for the whole network.

In this paper, we propose a uniﬁed framework for con-
volutional networks, namely Quantized CNN (Q-CNN), to
simultaneously accelerate and compress CNN models with

1FLOPs: number of FLoating-point OPerations required to classify one

image with the convolutional network.

1

only minor performance degradation. With network pa-
rameters quantized, the response of both convolutional and
fully-connected layers can be efﬁciently estimated via the
approximate inner product computation. We minimize the
estimation error of each layer’s response during parameter
quantization, which can better preserve the model perfor-
mance. In order to suppress the accumulative error while
quantizing multiple layers, an effective training scheme is
introduced to take previous estimation error into consider-
ation. Our Q-CNN model enables fast test-phase compu-
tation, and the storage and memory consumption are also
signiﬁcantly reduced.

We evaluate our Q-CNN framework for image classi-
ﬁcation on two benchmarks, MNIST [20] and ILSVRC-
12 [26]. For MNIST, our Q-CNN approach achieves over
12× compression for two neural networks (no convolu-
tion), with lower accuracy loss than several baseline meth-
ods. For ILSVRC-12, we attempt to improve the test-phase
efﬁciency of four convolutional networks: AlexNet [16],
CaffeNet [15], CNN-S [1], and VGG-16 [27]. Generally,
Q-CNN achieves 4× acceleration and 15× compression
(sometimes higher) for each network, with less than 1%
drop in the top-5 classiﬁcation accuracy. Moreover, we im-
plement the quantized CNN model on mobile devices, and
dramatically improve the test-phase efﬁciency, as depicted
in Figure 1. The main contributions of this paper can be
summarized as follows:

• We propose a uniﬁed Q-CNN framework to acceler-
ate and compress convolutional networks. We demon-
strate that better quantization can be learned by mini-
mizing the estimation error of each layer’s response.
• We propose an effective training scheme to suppress
the accumulative error while quantizing the whole con-
volutional network.

• Our Q-CNN framework achieves 4 ∼ 6× speed-up
and 15 ∼ 20× compression, while the classiﬁcation
accuracy loss is within one percentage. Moreover, the
quantized CNN model can be implemented on mobile
devices and classify an image within one second.

2. Preliminary

During the test phase of convolutional networks, the
computation overhead is dominated by convolutional lay-
ers; meanwhile, the majority of network parameters are
stored in fully-connected layers. Therefore, for better test-
phase efﬁciency, it is critical to speed-up the convolution
computation and compress parameters in fully-connected
layers.

Our observation is that the forward-passing process of
both convolutional and fully-connected layers is dominated
by the computation of inner products. More formally, we
consider a convolutional layer with input feature maps S ∈

Rds×ds×Cs and response feature maps T ∈ Rdt×dt×Ct,
where ds, dt are the spatial sizes and Cs, Ct are the number
of feature map channels. The response at the 2-D spatial
position pt in the ct-th response feature map is computed
as:

Tpt (ct) = X(pk,ps)

hWct,pk , Spsi

(1)

where Wct ∈ Rdk×dk×Cs is the ct-th convolutional kernel
and dk is the kernel size. We use ps and pk to denote the
2-D spatial positions in the input feature maps and convolu-
tional kernels, and both Wct,pk and Sps are Cs-dimensional
vectors. The layer response is the sum of inner products at
all positions within the dk × dk receptive ﬁeld in the input
feature maps.

Similarly, for a fully-connected layer, we have:

T (ct) = hWct , Si

(2)

where S ∈ RCs and T ∈ RCt are the layer input and layer
response, respectively, and Wct ∈ RCs is the weighting
vector for the ct-th neuron of this layer.

Product quantization [14] is widely used in approximate
nearest neighbor search, demonstrating better performance
than hashing-based methods [21, 22]. The idea is to de-
compose the feature space as the Cartesian product of mul-
tiple subspaces, and then learn sub-codebooks for each sub-
space. A vector is represented by the concatenation of sub-
codewords for efﬁcient distance computation and storage.

In this paper, we leverage product quantization to imple-
ment the efﬁcient inner product computation. Let us con-
sider the inner product computation between x, y ∈ RD. At
ﬁrst, both x and y are split into M sub-vectors, denoted as
x(m) and y(m). Afterwards, each x(m) is quantized with a
sub-codeword from the m-th sub-codebook, then we have

hy, xi = Xm

hy(m), x(m)i ≈ Xm

hy(m), c(m)
km i

(3)

which transforms the O(D) inner product computation to
M addition operations (M ≤ D), if the inner products be-
tween each sub-vector y(m) and all the sub-codewords in
the m-th sub-codebook have been computed in advance.

Quantization-based approaches have been explored in
several works [11, 2, 12]. These approaches mostly fo-
cus on compressing parameters in fully-connected layers
[11, 2], and none of them can provide acceleration for the
test-phase computation. Furthermore, [11, 12] require the
network parameters to be re-constructed during the test-
phase, which limit the compression to disk storage instead
of memory consumption. On the contrary, our approach
offers simultaneous acceleration and compression for both
convolutional and fully-connected layers, and can reduce
the run-time memory consumption dramatically.

3. Quantized CNN

In this section, we present our approach for accelerating
and compressing convolutional networks. Firstly, we intro-
duce an efﬁcient test-phase computation process with the
network parameters quantized. Secondly, we demonstrate
that better quantization can be learned by directly minimiz-
ing the estimation error of each layer’s response. Finally,
we analyze the computation complexity of our quantized
CNN model.

3.1. Quantizing the Fully-connected Layer

For a fully-connected layer, we denote its weighting ma-
trix as W ∈ RCs×Ct, where Cs and Ct are the dimensions
of the layer input and response, respectively. The weighting
vector Wct is the ct-th column vector in W .

We evenly split the Cs-dimensional space (where Wct
lies in) into M subspaces, each of C′
s = Cs/M dimen-
sions. Each Wct is then decomposed into M sub-vectors,
denoted as W (m)
. A sub-codebook can be learned for each
subspace after gathering all the sub-vectors within this sub-
space. Formally, for the m-th subspace, we optimize:

ct

min
D(m),B(m)

2
D(m)B(m) − W (m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
s×K, B(m) ∈ {0, 1}K×Ct

s.t. D(m) ∈ RC ′

(4)

where W (m) ∈ RC ′
s×Ct consists of the m-th sub-vectors
of all weighting vectors. The sub-codebook D(m) contains
K sub-codewords, and each column in B(m) is an indica-
tor vector (only one non-zero entry), specifying which sub-
codeword is used to quantize the corresponding sub-vector.
The optimization can be solved via k-means clustering.
The layer response is approximately computed as:
, S(m)i ≈ Xm
= Xm

T (ct) = Xm

km(ct), S(m)i

hD(m)B(m)

hW (m)
ct

, S(m)i

hD(m)

(5)

ct

ct

ct

where B(m)
is the ct-th column vector in B(m), and S(m) is
the m-th sub-vector of the layer input. km(ct) is the index
of the sub-codeword used to quantize the sub-vector W (m)
.
In Figure 2, we depict the parameter quantization and
test-phase computation process of the fully-connected layer.
By decomposing the weighting matrix into M sub-matrices,
M sub-codebooks can be learned, one per subspace. During
the test-phase, the layer input is split into M sub-vectors,
denoted as S(m). For each subspace, we compute the inner
products between S(m) and every sub-codeword in D(m),
and store the results in a look-up table. Afterwards, only M
addition operations are required to compute each response.
As a result, the overall time complexity can be reduced from
O(CsCt) to O(CsK + CtM ). On the other hand, only
sub-codebooks and quantization indices need to be stored,
which can dramatically reduce the storage consumption.

Figure 2. The parameter quantization and test-phase computation
process of the fully-connected layer.

3.2. Quantizing the Convolutional Layer

Unlike the 1-D weighting vector in the fully-connected
layer, each convolutional kernel is a 3-dimensional tensor:
Wct ∈ Rdk×dk×Cs. Before quantization, we need to deter-
mine how to split it into sub-vectors, i.e. apply subspace
splitting to which dimension. During the test phase, the in-
put feature maps are traversed by each convolutional kernel
with a sliding window in the spatial domain. Since these
sliding windows are partially overlapped, we split each con-
volutional kernel along the dimension of feature map chan-
nels, so that the pre-computed inner products can be re-
used at multiple spatial locations. Speciﬁcally, we learn the
quantization in each subspace by:

min
Xpk
D(m),{B(m)
pk }
s.t. D(m) ∈ RC ′

D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

s×K, B(m)

2
pk − W (m)
pk (cid:13)
(cid:13)
F
(cid:13)
pk ∈ {0, 1}K×Ct

(6)

pk ∈ RC ′

where W (m)
s×Ct contains the m-th sub-vectors of
all convolutional kernels at position pk. The optimization
can also be solved by k-means clustering in each subspace.
With the convolutional kernels quantized, we approxi-

mately compute the response feature maps by:

Tpt(ct) = X(pk,ps) Xm
≈ X(pk,ps) Xm
= X(pk,ps) Xm

hW (m)

ct,pk , S(m)
ps i

hD(m)B(m)

ct,pk , S(m)
ps i

(7)

hD(m)

km(ct,pk), S(m)
ps i

where S(m)
is the m-th sub-vector at position ps in the in-
ps
put feature maps, and km(ct, pk) is the index of the sub-
codeword to quantize the m-th sub-vector at position pk in
the ct-th convolutional kernel.

Similar to the fully-connected layer, we pre-compute the
look-up tables of inner products with the input feature maps.
Then, the response feature maps are approximately com-
puted with (7), and both the time and storage complexity
can be greatly reduced.

and the above optimization can be solved by alternatively
updating the sub-codebook and sub-codeword assignment.
Update D(m). We ﬁx the sub-codeword assignment
B(m), and deﬁne Lk = {ct|B(m)(k, ct) = 1}. The opti-
mization in (10) can be re-formulated as:

3.3. Quantization with Error Correction

So far, we have presented an intuitive approach to quan-
tize parameters and improve the test-phase efﬁciency of
convolutional networks. However, there are still two crit-
ical drawbacks. First, minimizing the quantization error
of model parameters does not necessarily give the optimal
quantized network for the classiﬁcation accuracy. In con-
trast, minimizing the estimation error of each layer’s re-
sponse is more closely related to the network’s classiﬁca-
tion performance. Second, the quantization of one layer is
independent of others, which may lead to the accumulation
of error when quantizing multiple layers. The estimation
error of the network’s ﬁnal response is very likely to be
quickly accumulated, since the error introduced by the pre-
vious quantized layers will also affect the following layers.
To overcome these two limitations, we introduce the idea
of error correction into the quantization of network param-
eters. This improved quantization approach directly min-
imizes the estimation error of the response at each layer,
and can compensate the error introduced by previous lay-
ers. With the error correction scheme, we can quantize the
network with much less performance degradation than the
original quantization method.

3.3.1 Error Correction for the Fully-connected Layer

Suppose we have N images to learn the quantization of a
fully-connected layer, and the layer input and response of
image In are denoted as Sn and Tn. In order to minimize
the estimation error of the layer response, we optimize:

Xn

min
{D(m)},{B(m)}

Tn − Xm
(cid:13)
(cid:13)
(cid:13)

2
(D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)
(8)
where the ﬁrst term in the Frobenius norm is the desired
layer response, and the second term is the approximated
layer response computed via the quantized parameters.

A block coordinate descent approach can be applied to
minimize this objective function. For the m-th subspace, its
residual error is deﬁned as:

R(m)

n = Tn − Xm′6=m

(D(m′

)B(m′

))T S(m′

n

)

(9)

and then we attempt to minimize the residual error of this
subspace, which is:

min

D(m),B(m) Xn

2
n − (D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)

R(m)
(cid:13)
(cid:13)
(cid:13)

(10)

min
{D(m)
k

}

Xn,k Xct∈Lk

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(11)

which implies that the optimization over one sub-codeword
does not affect other sub-codewords. Hence, for each sub-
codeword, we construct a least square problem from (11) to
update it.

Update B(m). With the sub-codebook D(m) ﬁxed, it
is easy to discover that the optimization of each column in
B(m) is mutually independent. For the ct-th column, its
optimal sub-codeword assignment is given by:

k∗
m(ct) = arg min

k Xn

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(12)

3.3.2 Error Correction for the Convolutional Layer

We adopt the similar idea to minimize the estimation error
of the convolutional layer’s response feature maps, that is:

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

(D(m)B(m)

min
{D(m)},{B(m)
pk }

Tn,pt − X
(pk ,ps)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(13)
The optimization also can be solved by block coordinate
descent. More details on solving this optimization can be
found in the supplementary material.

pk )T S(m)
n,ps

X
m

3.3.3 Error Correction for Multiple Layers

The above quantization method can be sequentially applied
to each layer in the CNN model. One concern is that the
estimation error of layer response caused by the previous
layers will be accumulated and affect the quantization of
the following layers. Here, we propose an effective training
scheme to address this issue.

We consider the quantization of a speciﬁc layer, assum-
ing its previous layers have already been quantized. The
optimization of parameter quantization is based on the layer
input and response of a group of training images. To quan-
tize this layer, we take the layer input in the quantized net-
work as {Sn}, and the layer response in the original net-
work (not quantized) as {Tn} in Eq. (8) and (13). In this
way, the optimization is guided by the actual input in the
quantized network and the desired response in the original
network. The accumulative error introduced by the previ-
ous layers is explicitly taken into consideration during op-
timization. In consequence, this training scheme can effec-
tively suppress the accumulative error for the quantization
of multiple layers.

Another possible solution is to adopt back-propagation
to jointly update the sub-codebooks and sub-codeword as-
signments in all quantized layers. However, since the sub-
codeword assignments are discrete, the gradient-based op-
timization can be quite difﬁcult, if not entirely impossible.
Therefore, back-propagation is not adopted here, but could
be a promising extension for future work.

3.4. Computation Complexity

Now we analyze the test-phase computation complex-
ity of convolutional and fully-connected layers, with or
without parameter quantization. For our proposed Q-CNN
model, the forward-passing through each layer mainly con-
sists of two procedures: pre-computation of inner products,
and approximate computation of layer response. Both sub-
codebooks and sub-codeword assignments are stored for the
test-phase computation. We report the detailed comparison
on the computation and storage overhead in Table 1.

Table 1. Comparison on the computation and storage overhead of
convolutional and fully-connected layers.

FLOPs

Bytes

Conv.

FCnt.

Conv.

FCnt.

kM

kCs
t Ctd2

t Ctd2
d2
CNN
sCsK + d2
d2
Q-CNN
CNN
CsCt
Q-CNN
CsK + CtM
4d2
CNN
kCsCt
Q-CNN 4CsK + 1
8 d2
kM Ct log2 K
CNN
4CsCt
Q-CNN
8 M Ct log2 K

4CsK + 1

As we can see from Table 1, the reduction in the compu-
tation and storage overhead largely depends on two hyper-
parameters, M (number of subspaces) and K (number of
sub-codewords in each subspace). Large values of M and
K lead to more ﬁne-grained quantization, but is less efﬁ-
cient in the computation and storage consumption. In prac-
tice, we can vary these two parameters to balance the trade-
off between the test-phase efﬁciency and accuracy loss of
the quantized CNN model.

4. Related Work

There have been a few attempts in accelerating the test-
phase computation of convolutional networks, and many are
inspired from the low-rank decomposition. Denton et al.
[7] presented a series of low-rank decomposition designs
for convolutional kernels. Similarly, CP-decomposition was
adopted in [17] to transform a convolutional layer into mul-
tiple layers with lower complexity. Zhang et al. [32, 31]
considered the subsequent nonlinear units while learning
the low-rank decomposition. [18] applied group-wise prun-
ing to the convolutional tensor to decompose it into the mul-
tiplications of thinned dense matrices. Recently, ﬁxed-point
based approaches are explored in [5, 25]. By representing

the connection weights (or even network activations) with
ﬁxed-point numbers, the computation can greatly beneﬁt
from hardware acceleration.

Another parallel research trend is to compress parame-
ters in fully-connected layers. Ciresan et al. [3] randomly
remove connection to reduce network parameters. Matrix
factorization was adopted in [6, 7] to decompose the weight-
ing matrix into two low-rank matrices, which demonstrated
that signiﬁcant redundancy did exist in network parameters.
Hinton et al. [8] proposed to use dark knowledge (the re-
sponse of a well-trained network) to guide the training of
a much smaller network, which was superior than directly
training. By exploring the similarity among neurons, Srini-
vas et al. [28] proposed a systematic way to remove redun-
dant neurons instead of network connections. In [30], mul-
tiple fully-connected layers were replaced by a single “Fast-
food” layer, which can be trained in an end-to-end style with
[2] randomly grouped
convolutional layers. Chen et al.
connection weights into hash buckets, and then ﬁne-tuned
the network with back-propagation. [12] combined prun-
ing, quantization, and Huffman coding to achieve higher
compression rate. Gong et al. [11] adopted vector quanti-
zation to compress the weighing matrix, which was actually
a special case of our approach (apply Q-CNN without error
correction to fully-connected layers only).

5. Experiments

In this section, we evaluate our quantized CNN frame-
work on two image classiﬁcation benchmarks, MNIST [20]
and ILSVRC-12 [26]. For the acceleration of convolutional
layers, we compare with:

• CPD [17]: CP-Decomposition;
• GBD [18]: Group-wise Brain Damage;
• LANR [31]: Low-rank Approximation of Non-linear

Responses.

and for the compression of fully-connected layers, we com-
pare with the following approaches:

• RER [3]: Random Edge Removal;
• LRD [6]: Low-Rank Decomposition;
• DK [8]: Dark Knowledge;
• HashNet [2]: Hashed Neural Nets;
• DPP [28]: Data-free Parameter Pruning;
• SVD [7]: Singular Value Decomposition;
• DFC [30]: Deep Fried Convnets.

For all above baselines, we use their reported results under
the same setting for fair comparison. We report the theo-
retical speed-up for more consistent results, since the real-
istic speed-up may be affected by various factors, e.g. CPU,
cache, and RAM. We compare the theoretical and realistic
speed-up in Section 5.4, and discuss the effect of adopting
the BLAS library for acceleration.

Our approaches are denoted as “Q-CNN” and “Q-CNN
(EC)”, where the latter one adopts error correction while the
former one does not. We implement the optimization pro-
cess of parameter quantization in MATLAB, and ﬁne-tune
the resulting network with Caffe [15]. Additional results of
our approach can be found in the supplementary material.

5.1. Results on MNIST

The MNIST dataset contains 70k images of hand-written
digits, 60k used for training and 10k for testing. To evalu-
ate the compression performance, we pre-train two neural
networks, one is 3-layer and another one is 5-layer, where
each hidden layer contains 1000 units. Different compres-
sion techniques are then adopted to compress these two net-
work, and the results are as depicted in Table 2.

Table 2. Comparison on the compression rates and classiﬁcation
error on MNIST, based on a 3-layer network (784-1000-10) and a
5-layer network (784-1000-1000-1000-10).

Method

Original
RER [3]
LRD [6]
DK [8]
HashNets [2]
Q-CNN
Q-CNN (EC)

3-layer

5-layer

Error
1.35%
2.19%
1.89%
1.71%
1.43%

Compr.
-
8×
8×
8×
8×

Compr.
-
8×
8×
8×
8×

Error
1.12%
1.24%
1.77%
1.26%
1.22%
12.1× 1.42% 13.4× 1.34%
12.1× 1.39% 13.4× 1.19%

s is determined once C′

In our Q-CNN framework, the trade-off between accu-
racy and efﬁciency is controlled by M (number of sub-
spaces) and K (number of sub-codewrods in each sub-
space). Since M = Cs/C′
s is given,
we tune (C′
s, K) to adjust the quantization precision. In Ta-
ble 2, we set the hyper-parameters as C′
s = 4 and K = 32.
From Table 2, we observe that our Q-CNN (EC) ap-
proach offers higher compression rates with less perfor-
mance degradation than all baselines for both networks.
The error correction scheme is effective in reducing the ac-
curacy loss, especially for deeper networks (5-layer). Also,
we ﬁnd the performance of both Q-CNN and Q-CNN (EC)
quite stable, as the standard deviation of ﬁve random runs is
merely 0.05%. Therefore, we report the single-run perfor-
mance in the remaining experiments.

5.2. Results on ILSVRC-12

The ILSVRC-12 benchmark consists of over one million
training images drawn from 1000 categories, and a disjoint
validation set of 50k images. We report both the top-1 and
top-5 classiﬁcation error rates on the validation set, using
single-view testing (central patch only).

We demonstrate our approach on four convolutional net-
works: AlexNet [16], CaffeNet [15], CNN-S [1], and VGG-

16 [27]. The ﬁrst two models have been adopted in several
related works, and therefore are included for comparison.
CNN-S and VGG-16 use a either wider or deeper structure
for better classiﬁcation accuracy, and are included here to
prove the scalability of our approach. We compare all these
networks’ computation and storage overhead in Table 3, to-
gether with their classiﬁcation error rates on ILSVRC-12.

Table 3. Comparison on the test-phase computation overhead
(FLOPs), storage consumption (Bytes), and classiﬁcation error
rates (Top-1/5 Err.) of AlexNet, CaffeNet, CNN-S, and VGG-16.
Bytes
2.44e+8
2.44e+8
4.12e+8
5.53e+8

Top-5 Err.
19.74%
19.59%
15.82%
10.05%

Top-1 Err.
42.78%
42.53%
37.31%
28.89%

FLOPs
7.29e+8
7.27e+8
2.94e+9
1.55e+10

Model
AlexNet
CaffeNet
CNN-S
VGG-16

5.2.1 Quantizing the Convolutional Layer

To begin with, we quantize the second convolutional layer
of AlexNet, which is the most time-consuming layer during
the test-phase. In Table 4, we report the performance un-
der several (C′
s, K) settings, comparing with two baseline
methods, CPD [17] and GBD [18].

Table 4. Comparison on the speed-up rates and the increase of top-
1/5 error rates for accelerating the second convolutional layer in
AlexNet, with or without ﬁne-tuning (FT). The hyper-parameters
of Q-CNN, C ′

Method

CPD

GBD

Q-CNN

Q-CNN
(EC)

Para.

Speed-up

s and K, are as speciﬁed in the “Para.” column.
Top-5 Err. ↑
Top-1 Err. ↑
FT
FT
0.44%
-
1.22%
-
18.63%
-
-
0.11%
-
0.43%
-
1.13%
1.37%
1.63%
2.27%
2.90%
1.28%
1.57%
2.66%
2.91%
0.17%
0.20%
0.40%
0.39%
0.21%
0.11%
0.31%
0.33%

No FT
0.94%
3.20%
69.06%
-
-
-
8.97%
14.71%
9.10%
18.05%
0.27%
0.50%
0.34%
0.50%

No FT
-
-
-
12.43%
21.93%
48.33%
10.55%
15.93%
10.62%
18.84%
0.35%
0.64%
0.27%
0.55%

3.19×
4.52×
6.51×
3.33×
5.00×
10.00×
3.70×
5.36×
4.84×
6.06×
3.70×
5.36×
4.84×
6.06×

-
-
-
-
-
-
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

From Table 4, we discover that with a large speed-up
rate (over 4×), the performance loss of both CPD and GBD
become severe, especially before ﬁne-tuning. The naive
parameter quantization method also suffers from the sim-
ilar problem. By incorporating the idea of error correction,
our Q-CNN model achieves up to 6× speed-up with merely
0.6% drop in accuracy, even without ﬁne-tuning. The ac-
curacy loss can be further reduced after ﬁne-tuning the sub-
sequent layers. Hence, it is more effective to minimize the
estimation error of each layer’s response than minimize the
quantization error of network parameters.

Next, we take one step further and attempt to speed-up
all the convolutional layers in AlexNet with Q-CNN (EC).

Table 5. Comparison on the speed-up/compression rates and the increase of top-1/5 error rates for accelerating all the convolutional layers
in AlexNet and VGG-16.

Model

Method

Para.

Speed-up Compression

AlexNet

Q-CNN
(EC)

VGG-16

LANR [31]
Q-CNN (EC)

4/64
6/64
6/128
8/128
-
6/128

3.32×
4.32×
3.71×
4.27×
4.00×
4.06×

10.58×
14.32×
10.27×
12.08×
2.73×
14.40×

Top-1 Err. ↑
FT
-
-

Top-5 Err. ↑
FT
-
-

No FT
0.94%
1.90%

No FT
1.33%
2.32%
1.44% 0.13% 1.16% 0.36%
2.25% 0.99% 1.64% 0.60%
0.95% 0.35%
3.04% 1.06% 1.83% 0.45%

-

-

We ﬁx the quantization hyper-parameters (C′
s, K) across all
layers. From Table 5, we observe that the loss in accuracy
grows mildly than the single-layer case. The speed-up rates
reported here are consistently smaller than those in Table 4,
since the acceleration effect is less signiﬁcant for some lay-
ers (i.e. “conv 4” and “conv 5”). For AlexNet, our Q-CNN
model (C′
s = 8, K = 128) can accelerate the computation
of all the convolutional layers by a factor of 4.27×, while
the increase in the top-1 and top-5 error rates are no more
than 2.5%. After ﬁne-tuning the remaining fully-connected
layers, the performance loss can be further reduced to less
than 1%.

In Table 5, we also report the comparison against LANR
[31] on VGG-16. For the similar speed-up rate (4×), their
approach outperforms ours in the top-5 classiﬁcation error
(an increase of 0.95% against 1.83%). After ﬁne-tuning, the
performance gap is narrowed down to 0.35% against 0.45%.
At the same time, our approach offers over 14× compres-
sion of parameters in convolutional layers, much larger than
theirs 2.7× compression2. Therefore, our approach is effec-
tive in accelerating and compressing networks with many
convolutional layers, with only minor performance loss.

5.2.2 Quantizing the Fully-connected Layer

For demonstration, we ﬁrst compress parameters in a single
fully-connected layer. In CaffeNet, the ﬁrst fully-connected
layer possesses over 37 million parameters (9216 × 4096),
more than 60% of whole network parameters. Our Q-CNN
approach is adopted to quantize this layer and the results are
as reported in Table 6. The performance loss of our Q-CNN
model is negligible (within 0.4%), which is much smaller
than baseline methods (DPP and SVD). Furthermore, error
correction is effective in preserving the classiﬁcation accu-
racy, especially under a higher compression rate.

Now we evaluate our approach’s performance for com-
pressing all the fully-connected layers in CaffeNet in Ta-
ble 7. The third layer is actually the combination of 1000
classiﬁers, and is more critical to the classiﬁcation accuracy.
Hence, we adopt a much more ﬁne-grained hyper-parameter

2The compression effect of their approach was not explicitly discussed
in the paper; we estimate the compression rate based on their description.

DPP

Method

Table 6. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing the ﬁrst fully-connected layer
in CaffeNet, without ﬁne-tuning.
Compression
Para.
1.19×
-
1.47×
-
1.91×
-
2.75×
-
1.38×
-
2.77×
-
-
5.54×
11.08×
-
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32

Top-5 Err. ↑
-
-
-
-
-0.03%
0.07%
0.19%
0.86%
0.19%
0.28%
0.12%
0.16%
0.07%
0.03%
0.11%
0.12%

Top-1 Err. ↑
0.16%
1.76%
4.08%
9.68%
0.03%
0.07%
0.36%
1.23%
0.19%
0.35%
0.18%
0.28%
0.10%
0.18%
0.14%
0.16%

Q-CNN
(EC)

Q-CNN

SVD

setting (C′
s = 1, K = 16) for this layer. Although the
speed-up effect no longer exists, we can still achieve around
8× compression for the last layer.

SVD

Method

Table 7. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CaffeNet. Both SVD and DFC are ﬁne-tuned, while Q-CNN
and Q-CNN (EC) are not ﬁne-tuned.
Para.
-
-
-
-
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
1.26×
2.52×
1.79×
3.58×
13.96×
19.14×
15.25×
18.71×
13.96×
19.14×
15.25×
18.71×

Top-5 Err. ↑
-
-
-
-
0.29%
0.47%
0.34%
0.59%
0.30%
0.47%
0.27%
0.39%

Top-1 Err. ↑
0.14%
1.22%
-0.66%
0.31%
0.28%
0.70%
0.44%
0.75%
0.31%
0.59%
0.31%
0.57%

Q-CNN
(EC)

Q-CNN

DFC

From Table 7, we discover that with less than 1% drop in
accuracy, Q-CNN achieves high compression rates (12 ∼
20×), much larger than that of SVD3and DFC (< 4×).
Again, Q-CNN with error correction consistently outper-
forms the naive Q-CNN approach as adopted in [11].

3In Table 6, SVD means replacing the weighting matrix with the multi-
plication of two low-rank matrices; in Table 7, SVD means ﬁne-tuning the
network after the low-rank matrix decomposition.

5.2.3 Quantizing the Whole Network

So far, we have evaluated the performance of CNN models
with either convolutional or fully-connected layers quan-
tized. Now we demonstrate the quantization of the whole
network with a three-stage strategy. Firstly, we quantize all
the convolutional layers with error correction, while fully-
connected layers remain untouched. Secondly, we ﬁne-tune
fully-connected layers in the quantized network with the
ILSVRC-12 training set to restore the classiﬁcation accu-
racy. Finally, fully-connected layers in the ﬁne-tuned net-
work are quantized with error correction. We report the
performance of our Q-CNN models in Table 8.

Table 8. The speed-up/compression rates and the increase of top-
1/5 error rates for the whole CNN model. Particularly, for the
quantization of the third fully-connected layer in each network,
we let C ′
Model

s = 1 and K = 16.
Para.

Compression

Top-1/5 Err. ↑

Speed-up

AlexNet

CaffeNet

CNN-S

VGG-16

Conv.
8/128
8/128
8/128
8/128
8/128
8/128
6/128
6/128

FCnt.
3/32
4/32
3/32
4/32
3/32
4/32
3/32
4/32

4.05×
4.15×
4.04×
4.14×
5.69×
5.78×
4.05×
4.06×

15.40×
18.76×
15.40×
18.76×
16.32×
20.16×
16.55×
20.34×

1.38% / 0.84%
1.46% / 0.97%
1.43% / 0.99%
1.54% / 1.12%
1.48% / 0.81%
1.64% / 0.85%
1.22% / 0.53%
1.35% / 0.58%

For convolutional layers, we let C′

s = 8 and K = 128
for AlexNet, CaffeNet, and CNN-S, and let C′
s = 6 and
K = 128 for VGG-16, to ensure roughly 4 ∼ 6× speed-
up for each network. Then we vary the hyper-parameter
settings in fully-connected layers for different compression
levels. For the former two networks, we achieve 18× com-
pression with about 1% loss in the top-5 classiﬁcation accu-
racy. For CNN-S, we achieve 5.78× speed-up and 20.16×
compression, while the top-5 classiﬁcation accuracy drop is
merely 0.85%. The result on VGG-16 is even more encour-
aging: with 4.06× speed-up and 20.34×, the increase of
top-5 error rate is only 0.58%. Hence, our proposed Q-CNN
framework can improve the efﬁciency of convolutional net-
works with minor performance loss, which is acceptable in
many applications.

5.3. Results on Mobile Devices

We have developed an Android application to fulﬁll
CNN-based image classiﬁcation on mobile devices, based
on our Q-CNN framework. The experiments are carried
out on a Huawei R(cid:13) Mate 7 smartphone, equipped with an
1.8GHz Kirin 925 CPU. The test-phase computation is car-
ried out on a single CPU core, without GPU acceleration.

In Table 9, we compare the computation efﬁciency and
classiﬁcation accuracy of the original and quantized CNN
models. Our Q-CNN framework achieves 3× speed-up for
AlexNet, and 4× speed-up for CNN-S. What’s more, we
compress the storage consumption by 20 ×, and the re-

Table 9. Comparison on the time, storage, memory consumption,
and top-5 classiﬁcation error rates of the original and quantized
AlexNet and CNN-S.
Model

AlexNet

CNN-S

CNN
Q-CNN
CNN
Q-CNN

Time
2.93s
0.95s
10.58s
2.61s

Storage
232.56MB
12.60MB
392.57MB
20.13MB

Memory
264.74MB
74.65MB
468.90MB
129.49MB

Top-5 Err.
19.74%
20.70%
15.82%
16.68%

quired run-time memory is only one quarter of the original
model. At the same time, the loss in the top-5 classiﬁcation
accuracy is no more than 1%. Therefore, our proposed ap-
proach improves the run-time efﬁciency in multiple aspects,
making the deployment of CNN models become tractable
on mobile platforms.

5.4. Theoretical vs. Realistic Speed-up

In Table 10, we compare the theoretical and realistic
speed-up on AlexNet. The BLAS [29] library is used in
Caffe [15] to accelerate the matrix multiplication in con-
volutional and fully-connected layers. However, it may not
always be an option for mobile devices. Therefore, we mea-
sure the run-time speed under two settings, i.e. with BLAS
enabled or disabled. The realistic speed-up is slightly lower
with BLAS on, indicating that Q-CNN does not beneﬁt as
much from BLAS as that of CNN. Other optimization tech-
niques, e.g. SIMD, SSE, and AVX [4], may further improve
our realistic speed-up, and shall be explored in the future.

Table 10. Comparison on the theoretical and realistic speed-up on
AlexNet (CPU only, single-threaded). Here we use the ATLAS
library, which is the default BLAS choice in Caffe [15].

BLAS

Off
On

FLOPs

CNN

Q-CNN

7.29e+8

1.75e+8

Time (ms)

Speed-up

CNN
321.10
167.794

Q-CNN
75.62
55.35

Theo.

4.15×

Real.
4.25×
3.03×

6. Conclusion

In this paper, we propose a uniﬁed framework to si-
multaneously accelerate and compress convolutional neural
networks. We quantize network parameters to enable ef-
ﬁcient test-phase computation. Extensive experiments are
conducted on MNIST and ILSVRC-12, and our approach
achieves outstanding speed-up and compression rates, with
only negligible loss in the classiﬁcation accuracy.

7. Acknowledgement

This work was supported in part by National Natural Sci-
ence Foundation of China (Grant No. 61332016), and 863
program (Grant No. 2014AA015105).

4This is Caffe’s run-time speed. The code for the other three settings is

on https://github.com/jiaxiang-wu/quantized-cnn.

[22] C. Leng, J. Wu, J. Cheng, X. Zhang, and H. Lu. Hashing for dis-
In International Conference on Machine Learning

tributed data.
(ICML), pages 1642–1650, 2015. 2

[23] G. Levi and T. Hassncer. Age and gender classiﬁcation using convo-
lutional neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), pages 34–42, 2015.
1

[24] C. Li, Q. Liu, J. Liu, and H. Lu. Learning ordinal discriminative
features for age estimation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2570–2577, 2012. 1
[25] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks.
CoRR, abs/1603.05279, 2016. 5

[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei. Imagenet large scale visual recognition challenge. Inter-
national Journal of Computer Vision (IJCV), pages 1–42, 2015. 2,
5

[27] K. Simonyan and A. Zisserman. Very deep convolutional networks
In International Conference on

for large-scale image recognition.
Learning Representations (ICLR), 2015. 1, 2, 6

[28] S. Srinivas and R. V. Babu. Data-free parameter pruning for deep
In British Machine Vision Conference (BMVC),

neural networks.
pages 31.1–31.12, 2015. 1, 5

[29] R. C. Whaley and A. Petitet. Minimizing development and mainte-
nance costs in supporting persistently optimized BLAS. Software:
Practice and Experience, 35(2):101–121, Feb 2005. 8

[30] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. CoRR, abs/1412.7149,
2014. 1, 5

[31] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep
convolutional networks for classiﬁcation and detection. CoRR,
abs/1505.06798, 2015. 1, 5, 7

[32] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient and accurate
approximations of nonlinear convolutional networks. In IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), pages
1984–1992, 2015. 1, 5

References

[1] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return
of the devil in the details: Delving deep into convolutional nets. In
British Machine Vision Conference (BMVC), 2014. 1, 2, 6

[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In International
Conference on Machine Learning (ICML), pages 2285–2294, 2015.
1, 2, 5, 6

[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmid-
huber. High-performance neural networks for visual object classiﬁ-
cation. CoRR, abs/1102.0183, 2011. 1, 5, 6

[4] I. Corporation. Intel architecture instruction set extensions program-
ming reference. Technical report, Intel Corporation, Feb 2016. 8
[5] M. Courbariaux, Y. Bengio, and J. David. Training deep neural net-
In International Confer-

works with low precision multiplications.
ence on Learning Representations (ICLR), 2015. 5

[6] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas.
Predicting parameters in deep learning. In Advances in Neural In-
formation Processing Systems (NIPS), pages 2148–2156, 2013. 5,
6

[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Ex-
ploiting linear structure within convolutional networks for efﬁcient
evaluation. In Advances in Neural Information Processing Systems
(NIPS), pages 1269–1277, 2014. 1, 5

[8] J. D. Geoffrey Hinton, Oriol Vinyals. Distilling the knowledge in a

neural network. CoRR, abs/1503.02531, 2015. 5, 6

[9] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 1
[10] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 580–587, 2014. 1

[11] Y. Gong, L. Liu, M. Yang, and L. D. Bourdev. Compressing
deep convolutional networks using vector quantization. CoRR,
abs/1412.6115, 2014. 1, 2, 5, 7

[12] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding. CoRR, abs/1510.00149, 2015. 1, 2, 5

[13] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolu-
tional neural networks with low rank expansions. In British Machine
Vision Conference (BMVC), 2014. 1

[14] H. Jegou, M. Douze, and C. Schmid. Product quantization for near-
est neighbor search. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 33(1):117–128, Jan 2011. 2

[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. CoRR, abs/1408.5093, 2014. 2, 6, 8
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 1106–1114, 2012. 1,
2, 6

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lem-
pitsky. Speeding-up convolutional neural networks using ﬁne-tuned
cp-decomposition. In International Conference on Learning Repre-
sentations (ICLR), 2015. 1, 5, 6

[18] V. Lebedev and V. S. Lempitsky. Fast convnets using group-wise

brain damage. CoRR, abs/1506.02515, 2015. 1, 5, 6

[19] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. E. Hubbard, and L. D. Jackel. Backpropagation applied to hand-
written zip code recognition. Neural Computation, 1(4):541–551,
1989. 1

[20] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998. 2, 5

[21] C. Leng, J. Wu, J. Cheng, X. Bai, and H. Lu. Online sketching hash-
ing. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2503–2511, 2015. 2

Appendix A: Additional Results

In the submission, we report the performance after quan-
tizing all the convolutional layers in AlexNet, and quan-
tizing all the full-connected layers in CaffeNet. Here, we
present experimental results for some other settings.

Quantizing Convolutional Layers in CaffeNet

We quantize all the convolutional layers in CaffeNet, and
the results are as demonstrated in Table 11. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.15% and 0.75%, compared to the original
CaffeNet.

Table 11. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CaffeNet, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.32×
4.32×
3.71×
4.27×
3.32×
4.32×
3.71×
4.27×

Top-1 Err. ↑
18.69%
32.84%
20.08%
35.48%
1.22%
2.44%
1.57%
2.30%

Top-5 Err. ↑
16.73%
33.55%
18.31%
37.82%
0.97%
1.83%
1.12%
1.71%

Quantizing Convolutional Layers in CNN-S

We quantize all the convolutional layers in CNN-S, and
the results are as demonstrated in Table 12. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.24% and 0.63%, compared to the original
CNN-S.

Table 12. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CNN-S, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.69×
5.17×
4.78×
5.92×
3.69×
5.17×
4.78×
5.92×

Top-1 Err. ↑
19.87%
45.74%
27.86%
46.18%
1.60%
3.49%
2.07%
3.42%

Top-5 Err. ↑
16.77%
48.67%
25.09%
50.26%
0.92%
2.32%
1.32%
2.17%

Quantizing Fully-connected Layers in AlexNet

We quantize all the fully-connected layers in AlexNet,

and the results are as demonstrated in Table 13.

Quantizing Fully-connected Layers in CNN-S

We quantize all the fully-connected layers in CNN-S,

Method

Table 13. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in AlexNet, without ﬁne-tuning.
Compression
Para.
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32

Top-5 Err. ↑
0.27%
0.64%
0.33%
0.69%
0.20%
0.22%
0.21%
0.38%

Top-1 Err. ↑
0.25%
0.77%
0.54%
0.71%
0.14%
0.40%
0.40%
0.46%

Q-CNN
(EC)

Q-CNN

Method

Table 14. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CNN-S, without ﬁne-tuning.
Para.
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
14.37×
20.15×
15.79×
19.66×
14.37×
20.15×
15.79×
19.66×

Top-5 Err. ↑
0.07%
0.22%
0.11%
0.27%
0.14%
0.24%
0.11%
0.27%

Top-1 Err. ↑
0.22%
0.45%
0.21%
0.35%
0.36%
0.43%
0.29%
0.56%

Q-CNN
(EC)

Q-CNN

Appendix B: Optimization in Section 3.3.2

Assume we have N images to learn the quantization of a
convolutional layer. For image In, we denote its input fea-
ture maps as Sn ∈ Rds×ds×Cs and response feature maps
as Tn ∈ Rdt×dt×Ct, where ds, dt are the spatial sizes and
Cs, Ct are the number of feature map channels. We use
ps and pt to denote the spatial location in the input and re-
sponse feature maps. The spatial location in the convolu-
tional kernels is denoted as pk.

To learn quantization with error correction for the con-

volutional layer, we attempt to optimize:

2

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
m

X
n,pt

X
(pk,ps)

(D(m)B(m)

pk )T S(m)

min
{D(m)},{B(m)
pk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(14)
where Dm is the m-th sub-codebook, and B(m)
is the cor-
pk
responding sub-codeword assignment indicator for the con-
volutional kernels at spatial location pk.

n,ps − Tn,pt

Similar to the fully-connected layer, we adopt a block co-
ordinate descent approach to solve this optimization prob-
lem. For the m-th subspace, we ﬁrstly deﬁne its residual
feature map as:

R(m)

n,pt = Tn,pt − X
(pk,ps)

X
m′6=m

(D(m′

)B(m′
pk

)

)T S(m′
)
n,ps

(15)

and the results are as demonstrated in Table 14.

and then the optimization in the m-th subspace can be re-

formulated as:

min
D(m),{B(m)
pk }

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(16)
Update D(m). With the assignment indicator {B(m)
pk }

n,ps − R(m)
n,pt

pk )T S(m)

(D(m)B(m)

X
(pk,ps)

X
n,pt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ﬁxed, we let:

Lk,pk = {ct|B(m)

pk (k, ct) = 1}

(17)

We greedily update each sub-codeword in the m-th sub-
codebook D(m) in a sequential style. For the k-th sub-
codeword, we compute the corresponding residual feature
map as:

Q(m)

n,pt,k(ct) = R(m)

n,pt (ct) − X
(pk ,ps)

X
k′6=k

X
ct∈Lk′,pk

D(m)T
k′

S(m)
n,ps

(18)

and then we can alternatively optimize:

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

min
D(m)
k

D(m)T

X
(pk,ps)

2
(cid:13)
(cid:13)
n,pt,k(ct)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(19)
which can be transformed into a least square problem. By
solving it, we can update the k-th sub-codeword.

n,ps − Q(m)
S(m)

X
ct∈Lk,pk

Update {B(m)

pk }. We greedily update the sub-codeword
assignment at each spatial location in the convolutional ker-
nels in a sequential style. For the spatial location pk, we
compute the corresponding residual feature map as:

P (m)
n,pt,pk = R(m)

(D(m)B(m)

p′
k

)T S(m)
n,p′
s

(20)

n,pt − X
(p′
k,p′
s)
pk6=pk

and then the optimization can be re-written as:

min
B(m)
pk

X
n,pt

(D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

pk )T S(m)

n,ps − P (m)

2
n,pt,pk (cid:13)
(cid:13)
F
(cid:13)

(21)

Since B(m)
pk ∈ {0, 1}K is an indicator vector (only one non-
zero entry), we can exhaustively try all sub-codewords and
select the optimal one that minimize the objective function.

6
1
0
2
 
y
a
M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
3
7
4
6
0
.
2
1
5
1
:
v
i
X
r
a

Quantized Convolutional Neural Networks for Mobile Devices

Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng
National Laboratory of Patter Recognition
Institute of Automation, Chinese Academy of Sciences
{jiaxiang.wu, cong.leng, yuhang.wang, qinghao.hu, jcheng}@nlpr.ia.ac.cn

Abstract

Original

Q-CNN

Time Consumption (s)

Storage Consumption (MB)

Recently, convolutional neural networks (CNN) have
demonstrated impressive performance in various computer
vision tasks. However, high performance hardware is typ-
ically indispensable for the application of CNN models
due to the high computation complexity, which prohibits
their further extensions. In this paper, we propose an efﬁ-
cient framework, namely Quantized CNN, to simultaneously
speed-up the computation and reduce the storage and mem-
ory overhead of CNN models. Both ﬁlter kernels in con-
volutional layers and weighting matrices in fully-connected
layers are quantized, aiming at minimizing the estimation
error of each layer’s response. Extensive experiments on
the ILSVRC-12 benchmark demonstrate 4 ∼ 6× speed-up
and 15 ∼ 20× compression with merely one percentage
loss of classiﬁcation accuracy. With our quantized CNN
model, even mobile devices can accurately classify images
within one second.

15

10

5

0

500

400

300

200

100

0

AlexNet

CNN-S

AlexNet

CNN-S

Memory Consumption (MB)

Top-5 Error Rate (%)

AlexNet

CNN-S

AlexNet

CNN-S

Figure 1. Comparison on the efﬁciency and classiﬁcation accuracy
between the original and quantized AlexNet [16] and CNN-S [1]
on a Huawei R(cid:13) Mate 7 smartphone.

400

300

200

100

0

25

20

15

10

5

0

1. Introduction

In recent years, we have witnessed the great success
of convolutional neural networks (CNN) [19] in a wide
range of visual applications, including image classiﬁcation
[16, 27], object detection [10, 9], age estimation [24, 23],
etc. This success mainly comes from deeper network ar-
chitectures as well as the tremendous training data. How-
ever, as the network grows deeper, the model complexity is
also increasing exponentially in both the training and testing
stages, which leads to the very high demand in the computa-
tion ability. For instance, the 8-layer AlexNet [16] involves
60M parameters and requires over 729M FLOPs1to classify
a single image. Although the training stage can be ofﬂine
carried out on high performance clusters with GPU acceler-
ation, the testing computation cost may be unaffordable for
common personal computers and mobile devices. Due to
the limited computation ability and memory space, mobile
devices are almost intractable to run deep convolutional net-
works. Therefore, it is crucial to accelerate the computation

and compress the memory consumption for CNN models.

For most CNNs, convolutional layers are the most time-
consuming part, while fully-connected layers involve mas-
sive network parameters. Due to the intrinsical differ-
ence between them, existing works usually focus on im-
proving the efﬁciency for either convolutional layers or
fully-connected layers.
In [7, 13, 32, 31, 18, 17], low-
rank approximation or tensor decomposition is adopted to
speed-up convolutional layers. On the other hand, param-
eter compression in fully-connected layers is explored in
[3, 7, 11, 30, 2, 12, 28]. Overall, the above-mentioned al-
gorithms are able to achieve faster speed or less storage.
However, few of them can achieve signiﬁcant acceleration
and compression simultaneously for the whole network.

In this paper, we propose a uniﬁed framework for con-
volutional networks, namely Quantized CNN (Q-CNN), to
simultaneously accelerate and compress CNN models with

1FLOPs: number of FLoating-point OPerations required to classify one

image with the convolutional network.

1

only minor performance degradation. With network pa-
rameters quantized, the response of both convolutional and
fully-connected layers can be efﬁciently estimated via the
approximate inner product computation. We minimize the
estimation error of each layer’s response during parameter
quantization, which can better preserve the model perfor-
mance. In order to suppress the accumulative error while
quantizing multiple layers, an effective training scheme is
introduced to take previous estimation error into consider-
ation. Our Q-CNN model enables fast test-phase compu-
tation, and the storage and memory consumption are also
signiﬁcantly reduced.

We evaluate our Q-CNN framework for image classi-
ﬁcation on two benchmarks, MNIST [20] and ILSVRC-
12 [26]. For MNIST, our Q-CNN approach achieves over
12× compression for two neural networks (no convolu-
tion), with lower accuracy loss than several baseline meth-
ods. For ILSVRC-12, we attempt to improve the test-phase
efﬁciency of four convolutional networks: AlexNet [16],
CaffeNet [15], CNN-S [1], and VGG-16 [27]. Generally,
Q-CNN achieves 4× acceleration and 15× compression
(sometimes higher) for each network, with less than 1%
drop in the top-5 classiﬁcation accuracy. Moreover, we im-
plement the quantized CNN model on mobile devices, and
dramatically improve the test-phase efﬁciency, as depicted
in Figure 1. The main contributions of this paper can be
summarized as follows:

• We propose a uniﬁed Q-CNN framework to acceler-
ate and compress convolutional networks. We demon-
strate that better quantization can be learned by mini-
mizing the estimation error of each layer’s response.
• We propose an effective training scheme to suppress
the accumulative error while quantizing the whole con-
volutional network.

• Our Q-CNN framework achieves 4 ∼ 6× speed-up
and 15 ∼ 20× compression, while the classiﬁcation
accuracy loss is within one percentage. Moreover, the
quantized CNN model can be implemented on mobile
devices and classify an image within one second.

2. Preliminary

During the test phase of convolutional networks, the
computation overhead is dominated by convolutional lay-
ers; meanwhile, the majority of network parameters are
stored in fully-connected layers. Therefore, for better test-
phase efﬁciency, it is critical to speed-up the convolution
computation and compress parameters in fully-connected
layers.

Our observation is that the forward-passing process of
both convolutional and fully-connected layers is dominated
by the computation of inner products. More formally, we
consider a convolutional layer with input feature maps S ∈

Rds×ds×Cs and response feature maps T ∈ Rdt×dt×Ct,
where ds, dt are the spatial sizes and Cs, Ct are the number
of feature map channels. The response at the 2-D spatial
position pt in the ct-th response feature map is computed
as:

Tpt (ct) = X(pk,ps)

hWct,pk , Spsi

(1)

where Wct ∈ Rdk×dk×Cs is the ct-th convolutional kernel
and dk is the kernel size. We use ps and pk to denote the
2-D spatial positions in the input feature maps and convolu-
tional kernels, and both Wct,pk and Sps are Cs-dimensional
vectors. The layer response is the sum of inner products at
all positions within the dk × dk receptive ﬁeld in the input
feature maps.

Similarly, for a fully-connected layer, we have:

T (ct) = hWct , Si

(2)

where S ∈ RCs and T ∈ RCt are the layer input and layer
response, respectively, and Wct ∈ RCs is the weighting
vector for the ct-th neuron of this layer.

Product quantization [14] is widely used in approximate
nearest neighbor search, demonstrating better performance
than hashing-based methods [21, 22]. The idea is to de-
compose the feature space as the Cartesian product of mul-
tiple subspaces, and then learn sub-codebooks for each sub-
space. A vector is represented by the concatenation of sub-
codewords for efﬁcient distance computation and storage.

In this paper, we leverage product quantization to imple-
ment the efﬁcient inner product computation. Let us con-
sider the inner product computation between x, y ∈ RD. At
ﬁrst, both x and y are split into M sub-vectors, denoted as
x(m) and y(m). Afterwards, each x(m) is quantized with a
sub-codeword from the m-th sub-codebook, then we have

hy, xi = Xm

hy(m), x(m)i ≈ Xm

hy(m), c(m)
km i

(3)

which transforms the O(D) inner product computation to
M addition operations (M ≤ D), if the inner products be-
tween each sub-vector y(m) and all the sub-codewords in
the m-th sub-codebook have been computed in advance.

Quantization-based approaches have been explored in
several works [11, 2, 12]. These approaches mostly fo-
cus on compressing parameters in fully-connected layers
[11, 2], and none of them can provide acceleration for the
test-phase computation. Furthermore, [11, 12] require the
network parameters to be re-constructed during the test-
phase, which limit the compression to disk storage instead
of memory consumption. On the contrary, our approach
offers simultaneous acceleration and compression for both
convolutional and fully-connected layers, and can reduce
the run-time memory consumption dramatically.

3. Quantized CNN

In this section, we present our approach for accelerating
and compressing convolutional networks. Firstly, we intro-
duce an efﬁcient test-phase computation process with the
network parameters quantized. Secondly, we demonstrate
that better quantization can be learned by directly minimiz-
ing the estimation error of each layer’s response. Finally,
we analyze the computation complexity of our quantized
CNN model.

3.1. Quantizing the Fully-connected Layer

For a fully-connected layer, we denote its weighting ma-
trix as W ∈ RCs×Ct, where Cs and Ct are the dimensions
of the layer input and response, respectively. The weighting
vector Wct is the ct-th column vector in W .

We evenly split the Cs-dimensional space (where Wct
lies in) into M subspaces, each of C′
s = Cs/M dimen-
sions. Each Wct is then decomposed into M sub-vectors,
denoted as W (m)
. A sub-codebook can be learned for each
subspace after gathering all the sub-vectors within this sub-
space. Formally, for the m-th subspace, we optimize:

ct

min
D(m),B(m)

2
D(m)B(m) − W (m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
s×K, B(m) ∈ {0, 1}K×Ct

s.t. D(m) ∈ RC ′

(4)

where W (m) ∈ RC ′
s×Ct consists of the m-th sub-vectors
of all weighting vectors. The sub-codebook D(m) contains
K sub-codewords, and each column in B(m) is an indica-
tor vector (only one non-zero entry), specifying which sub-
codeword is used to quantize the corresponding sub-vector.
The optimization can be solved via k-means clustering.
The layer response is approximately computed as:
, S(m)i ≈ Xm
= Xm

T (ct) = Xm

km(ct), S(m)i

hD(m)B(m)

hW (m)
ct

, S(m)i

hD(m)

(5)

ct

ct

ct

where B(m)
is the ct-th column vector in B(m), and S(m) is
the m-th sub-vector of the layer input. km(ct) is the index
of the sub-codeword used to quantize the sub-vector W (m)
.
In Figure 2, we depict the parameter quantization and
test-phase computation process of the fully-connected layer.
By decomposing the weighting matrix into M sub-matrices,
M sub-codebooks can be learned, one per subspace. During
the test-phase, the layer input is split into M sub-vectors,
denoted as S(m). For each subspace, we compute the inner
products between S(m) and every sub-codeword in D(m),
and store the results in a look-up table. Afterwards, only M
addition operations are required to compute each response.
As a result, the overall time complexity can be reduced from
O(CsCt) to O(CsK + CtM ). On the other hand, only
sub-codebooks and quantization indices need to be stored,
which can dramatically reduce the storage consumption.

Figure 2. The parameter quantization and test-phase computation
process of the fully-connected layer.

3.2. Quantizing the Convolutional Layer

Unlike the 1-D weighting vector in the fully-connected
layer, each convolutional kernel is a 3-dimensional tensor:
Wct ∈ Rdk×dk×Cs. Before quantization, we need to deter-
mine how to split it into sub-vectors, i.e. apply subspace
splitting to which dimension. During the test phase, the in-
put feature maps are traversed by each convolutional kernel
with a sliding window in the spatial domain. Since these
sliding windows are partially overlapped, we split each con-
volutional kernel along the dimension of feature map chan-
nels, so that the pre-computed inner products can be re-
used at multiple spatial locations. Speciﬁcally, we learn the
quantization in each subspace by:

min
Xpk
D(m),{B(m)
pk }
s.t. D(m) ∈ RC ′

D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

s×K, B(m)

2
pk − W (m)
pk (cid:13)
(cid:13)
F
(cid:13)
pk ∈ {0, 1}K×Ct

(6)

pk ∈ RC ′

where W (m)
s×Ct contains the m-th sub-vectors of
all convolutional kernels at position pk. The optimization
can also be solved by k-means clustering in each subspace.
With the convolutional kernels quantized, we approxi-

mately compute the response feature maps by:

Tpt(ct) = X(pk,ps) Xm
≈ X(pk,ps) Xm
= X(pk,ps) Xm

hW (m)

ct,pk , S(m)
ps i

hD(m)B(m)

ct,pk , S(m)
ps i

(7)

hD(m)

km(ct,pk), S(m)
ps i

where S(m)
is the m-th sub-vector at position ps in the in-
ps
put feature maps, and km(ct, pk) is the index of the sub-
codeword to quantize the m-th sub-vector at position pk in
the ct-th convolutional kernel.

Similar to the fully-connected layer, we pre-compute the
look-up tables of inner products with the input feature maps.
Then, the response feature maps are approximately com-
puted with (7), and both the time and storage complexity
can be greatly reduced.

and the above optimization can be solved by alternatively
updating the sub-codebook and sub-codeword assignment.
Update D(m). We ﬁx the sub-codeword assignment
B(m), and deﬁne Lk = {ct|B(m)(k, ct) = 1}. The opti-
mization in (10) can be re-formulated as:

3.3. Quantization with Error Correction

So far, we have presented an intuitive approach to quan-
tize parameters and improve the test-phase efﬁciency of
convolutional networks. However, there are still two crit-
ical drawbacks. First, minimizing the quantization error
of model parameters does not necessarily give the optimal
quantized network for the classiﬁcation accuracy. In con-
trast, minimizing the estimation error of each layer’s re-
sponse is more closely related to the network’s classiﬁca-
tion performance. Second, the quantization of one layer is
independent of others, which may lead to the accumulation
of error when quantizing multiple layers. The estimation
error of the network’s ﬁnal response is very likely to be
quickly accumulated, since the error introduced by the pre-
vious quantized layers will also affect the following layers.
To overcome these two limitations, we introduce the idea
of error correction into the quantization of network param-
eters. This improved quantization approach directly min-
imizes the estimation error of the response at each layer,
and can compensate the error introduced by previous lay-
ers. With the error correction scheme, we can quantize the
network with much less performance degradation than the
original quantization method.

3.3.1 Error Correction for the Fully-connected Layer

Suppose we have N images to learn the quantization of a
fully-connected layer, and the layer input and response of
image In are denoted as Sn and Tn. In order to minimize
the estimation error of the layer response, we optimize:

Xn

min
{D(m)},{B(m)}

Tn − Xm
(cid:13)
(cid:13)
(cid:13)

2
(D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)
(8)
where the ﬁrst term in the Frobenius norm is the desired
layer response, and the second term is the approximated
layer response computed via the quantized parameters.

A block coordinate descent approach can be applied to
minimize this objective function. For the m-th subspace, its
residual error is deﬁned as:

R(m)

n = Tn − Xm′6=m

(D(m′

)B(m′

))T S(m′

n

)

(9)

and then we attempt to minimize the residual error of this
subspace, which is:

min

D(m),B(m) Xn

2
n − (D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)

R(m)
(cid:13)
(cid:13)
(cid:13)

(10)

min
{D(m)
k

}

Xn,k Xct∈Lk

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(11)

which implies that the optimization over one sub-codeword
does not affect other sub-codewords. Hence, for each sub-
codeword, we construct a least square problem from (11) to
update it.

Update B(m). With the sub-codebook D(m) ﬁxed, it
is easy to discover that the optimization of each column in
B(m) is mutually independent. For the ct-th column, its
optimal sub-codeword assignment is given by:

k∗
m(ct) = arg min

k Xn

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(12)

3.3.2 Error Correction for the Convolutional Layer

We adopt the similar idea to minimize the estimation error
of the convolutional layer’s response feature maps, that is:

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

(D(m)B(m)

min
{D(m)},{B(m)
pk }

Tn,pt − X
(pk ,ps)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(13)
The optimization also can be solved by block coordinate
descent. More details on solving this optimization can be
found in the supplementary material.

pk )T S(m)
n,ps

X
m

3.3.3 Error Correction for Multiple Layers

The above quantization method can be sequentially applied
to each layer in the CNN model. One concern is that the
estimation error of layer response caused by the previous
layers will be accumulated and affect the quantization of
the following layers. Here, we propose an effective training
scheme to address this issue.

We consider the quantization of a speciﬁc layer, assum-
ing its previous layers have already been quantized. The
optimization of parameter quantization is based on the layer
input and response of a group of training images. To quan-
tize this layer, we take the layer input in the quantized net-
work as {Sn}, and the layer response in the original net-
work (not quantized) as {Tn} in Eq. (8) and (13). In this
way, the optimization is guided by the actual input in the
quantized network and the desired response in the original
network. The accumulative error introduced by the previ-
ous layers is explicitly taken into consideration during op-
timization. In consequence, this training scheme can effec-
tively suppress the accumulative error for the quantization
of multiple layers.

Another possible solution is to adopt back-propagation
to jointly update the sub-codebooks and sub-codeword as-
signments in all quantized layers. However, since the sub-
codeword assignments are discrete, the gradient-based op-
timization can be quite difﬁcult, if not entirely impossible.
Therefore, back-propagation is not adopted here, but could
be a promising extension for future work.

3.4. Computation Complexity

Now we analyze the test-phase computation complex-
ity of convolutional and fully-connected layers, with or
without parameter quantization. For our proposed Q-CNN
model, the forward-passing through each layer mainly con-
sists of two procedures: pre-computation of inner products,
and approximate computation of layer response. Both sub-
codebooks and sub-codeword assignments are stored for the
test-phase computation. We report the detailed comparison
on the computation and storage overhead in Table 1.

Table 1. Comparison on the computation and storage overhead of
convolutional and fully-connected layers.

FLOPs

Bytes

Conv.

FCnt.

Conv.

FCnt.

kM

kCs
t Ctd2

t Ctd2
d2
CNN
sCsK + d2
d2
Q-CNN
CNN
CsCt
Q-CNN
CsK + CtM
4d2
CNN
kCsCt
Q-CNN 4CsK + 1
8 d2
kM Ct log2 K
CNN
4CsCt
Q-CNN
8 M Ct log2 K

4CsK + 1

As we can see from Table 1, the reduction in the compu-
tation and storage overhead largely depends on two hyper-
parameters, M (number of subspaces) and K (number of
sub-codewords in each subspace). Large values of M and
K lead to more ﬁne-grained quantization, but is less efﬁ-
cient in the computation and storage consumption. In prac-
tice, we can vary these two parameters to balance the trade-
off between the test-phase efﬁciency and accuracy loss of
the quantized CNN model.

4. Related Work

There have been a few attempts in accelerating the test-
phase computation of convolutional networks, and many are
inspired from the low-rank decomposition. Denton et al.
[7] presented a series of low-rank decomposition designs
for convolutional kernels. Similarly, CP-decomposition was
adopted in [17] to transform a convolutional layer into mul-
tiple layers with lower complexity. Zhang et al. [32, 31]
considered the subsequent nonlinear units while learning
the low-rank decomposition. [18] applied group-wise prun-
ing to the convolutional tensor to decompose it into the mul-
tiplications of thinned dense matrices. Recently, ﬁxed-point
based approaches are explored in [5, 25]. By representing

the connection weights (or even network activations) with
ﬁxed-point numbers, the computation can greatly beneﬁt
from hardware acceleration.

Another parallel research trend is to compress parame-
ters in fully-connected layers. Ciresan et al. [3] randomly
remove connection to reduce network parameters. Matrix
factorization was adopted in [6, 7] to decompose the weight-
ing matrix into two low-rank matrices, which demonstrated
that signiﬁcant redundancy did exist in network parameters.
Hinton et al. [8] proposed to use dark knowledge (the re-
sponse of a well-trained network) to guide the training of
a much smaller network, which was superior than directly
training. By exploring the similarity among neurons, Srini-
vas et al. [28] proposed a systematic way to remove redun-
dant neurons instead of network connections. In [30], mul-
tiple fully-connected layers were replaced by a single “Fast-
food” layer, which can be trained in an end-to-end style with
[2] randomly grouped
convolutional layers. Chen et al.
connection weights into hash buckets, and then ﬁne-tuned
the network with back-propagation. [12] combined prun-
ing, quantization, and Huffman coding to achieve higher
compression rate. Gong et al. [11] adopted vector quanti-
zation to compress the weighing matrix, which was actually
a special case of our approach (apply Q-CNN without error
correction to fully-connected layers only).

5. Experiments

In this section, we evaluate our quantized CNN frame-
work on two image classiﬁcation benchmarks, MNIST [20]
and ILSVRC-12 [26]. For the acceleration of convolutional
layers, we compare with:

• CPD [17]: CP-Decomposition;
• GBD [18]: Group-wise Brain Damage;
• LANR [31]: Low-rank Approximation of Non-linear

Responses.

and for the compression of fully-connected layers, we com-
pare with the following approaches:

• RER [3]: Random Edge Removal;
• LRD [6]: Low-Rank Decomposition;
• DK [8]: Dark Knowledge;
• HashNet [2]: Hashed Neural Nets;
• DPP [28]: Data-free Parameter Pruning;
• SVD [7]: Singular Value Decomposition;
• DFC [30]: Deep Fried Convnets.

For all above baselines, we use their reported results under
the same setting for fair comparison. We report the theo-
retical speed-up for more consistent results, since the real-
istic speed-up may be affected by various factors, e.g. CPU,
cache, and RAM. We compare the theoretical and realistic
speed-up in Section 5.4, and discuss the effect of adopting
the BLAS library for acceleration.

Our approaches are denoted as “Q-CNN” and “Q-CNN
(EC)”, where the latter one adopts error correction while the
former one does not. We implement the optimization pro-
cess of parameter quantization in MATLAB, and ﬁne-tune
the resulting network with Caffe [15]. Additional results of
our approach can be found in the supplementary material.

5.1. Results on MNIST

The MNIST dataset contains 70k images of hand-written
digits, 60k used for training and 10k for testing. To evalu-
ate the compression performance, we pre-train two neural
networks, one is 3-layer and another one is 5-layer, where
each hidden layer contains 1000 units. Different compres-
sion techniques are then adopted to compress these two net-
work, and the results are as depicted in Table 2.

Table 2. Comparison on the compression rates and classiﬁcation
error on MNIST, based on a 3-layer network (784-1000-10) and a
5-layer network (784-1000-1000-1000-10).

Method

Original
RER [3]
LRD [6]
DK [8]
HashNets [2]
Q-CNN
Q-CNN (EC)

3-layer

5-layer

Error
1.35%
2.19%
1.89%
1.71%
1.43%

Compr.
-
8×
8×
8×
8×

Compr.
-
8×
8×
8×
8×

Error
1.12%
1.24%
1.77%
1.26%
1.22%
12.1× 1.42% 13.4× 1.34%
12.1× 1.39% 13.4× 1.19%

s is determined once C′

In our Q-CNN framework, the trade-off between accu-
racy and efﬁciency is controlled by M (number of sub-
spaces) and K (number of sub-codewrods in each sub-
space). Since M = Cs/C′
s is given,
we tune (C′
s, K) to adjust the quantization precision. In Ta-
ble 2, we set the hyper-parameters as C′
s = 4 and K = 32.
From Table 2, we observe that our Q-CNN (EC) ap-
proach offers higher compression rates with less perfor-
mance degradation than all baselines for both networks.
The error correction scheme is effective in reducing the ac-
curacy loss, especially for deeper networks (5-layer). Also,
we ﬁnd the performance of both Q-CNN and Q-CNN (EC)
quite stable, as the standard deviation of ﬁve random runs is
merely 0.05%. Therefore, we report the single-run perfor-
mance in the remaining experiments.

5.2. Results on ILSVRC-12

The ILSVRC-12 benchmark consists of over one million
training images drawn from 1000 categories, and a disjoint
validation set of 50k images. We report both the top-1 and
top-5 classiﬁcation error rates on the validation set, using
single-view testing (central patch only).

We demonstrate our approach on four convolutional net-
works: AlexNet [16], CaffeNet [15], CNN-S [1], and VGG-

16 [27]. The ﬁrst two models have been adopted in several
related works, and therefore are included for comparison.
CNN-S and VGG-16 use a either wider or deeper structure
for better classiﬁcation accuracy, and are included here to
prove the scalability of our approach. We compare all these
networks’ computation and storage overhead in Table 3, to-
gether with their classiﬁcation error rates on ILSVRC-12.

Table 3. Comparison on the test-phase computation overhead
(FLOPs), storage consumption (Bytes), and classiﬁcation error
rates (Top-1/5 Err.) of AlexNet, CaffeNet, CNN-S, and VGG-16.
Bytes
2.44e+8
2.44e+8
4.12e+8
5.53e+8

Top-5 Err.
19.74%
19.59%
15.82%
10.05%

Top-1 Err.
42.78%
42.53%
37.31%
28.89%

FLOPs
7.29e+8
7.27e+8
2.94e+9
1.55e+10

Model
AlexNet
CaffeNet
CNN-S
VGG-16

5.2.1 Quantizing the Convolutional Layer

To begin with, we quantize the second convolutional layer
of AlexNet, which is the most time-consuming layer during
the test-phase. In Table 4, we report the performance un-
der several (C′
s, K) settings, comparing with two baseline
methods, CPD [17] and GBD [18].

Table 4. Comparison on the speed-up rates and the increase of top-
1/5 error rates for accelerating the second convolutional layer in
AlexNet, with or without ﬁne-tuning (FT). The hyper-parameters
of Q-CNN, C ′

Method

CPD

GBD

Q-CNN

Q-CNN
(EC)

Para.

Speed-up

s and K, are as speciﬁed in the “Para.” column.
Top-5 Err. ↑
Top-1 Err. ↑
FT
FT
0.44%
-
1.22%
-
18.63%
-
-
0.11%
-
0.43%
-
1.13%
1.37%
1.63%
2.27%
2.90%
1.28%
1.57%
2.66%
2.91%
0.17%
0.20%
0.40%
0.39%
0.21%
0.11%
0.31%
0.33%

No FT
0.94%
3.20%
69.06%
-
-
-
8.97%
14.71%
9.10%
18.05%
0.27%
0.50%
0.34%
0.50%

No FT
-
-
-
12.43%
21.93%
48.33%
10.55%
15.93%
10.62%
18.84%
0.35%
0.64%
0.27%
0.55%

3.19×
4.52×
6.51×
3.33×
5.00×
10.00×
3.70×
5.36×
4.84×
6.06×
3.70×
5.36×
4.84×
6.06×

-
-
-
-
-
-
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

From Table 4, we discover that with a large speed-up
rate (over 4×), the performance loss of both CPD and GBD
become severe, especially before ﬁne-tuning. The naive
parameter quantization method also suffers from the sim-
ilar problem. By incorporating the idea of error correction,
our Q-CNN model achieves up to 6× speed-up with merely
0.6% drop in accuracy, even without ﬁne-tuning. The ac-
curacy loss can be further reduced after ﬁne-tuning the sub-
sequent layers. Hence, it is more effective to minimize the
estimation error of each layer’s response than minimize the
quantization error of network parameters.

Next, we take one step further and attempt to speed-up
all the convolutional layers in AlexNet with Q-CNN (EC).

Table 5. Comparison on the speed-up/compression rates and the increase of top-1/5 error rates for accelerating all the convolutional layers
in AlexNet and VGG-16.

Model

Method

Para.

Speed-up Compression

AlexNet

Q-CNN
(EC)

VGG-16

LANR [31]
Q-CNN (EC)

4/64
6/64
6/128
8/128
-
6/128

3.32×
4.32×
3.71×
4.27×
4.00×
4.06×

10.58×
14.32×
10.27×
12.08×
2.73×
14.40×

Top-1 Err. ↑
FT
-
-

Top-5 Err. ↑
FT
-
-

No FT
0.94%
1.90%

No FT
1.33%
2.32%
1.44% 0.13% 1.16% 0.36%
2.25% 0.99% 1.64% 0.60%
0.95% 0.35%
3.04% 1.06% 1.83% 0.45%

-

-

We ﬁx the quantization hyper-parameters (C′
s, K) across all
layers. From Table 5, we observe that the loss in accuracy
grows mildly than the single-layer case. The speed-up rates
reported here are consistently smaller than those in Table 4,
since the acceleration effect is less signiﬁcant for some lay-
ers (i.e. “conv 4” and “conv 5”). For AlexNet, our Q-CNN
model (C′
s = 8, K = 128) can accelerate the computation
of all the convolutional layers by a factor of 4.27×, while
the increase in the top-1 and top-5 error rates are no more
than 2.5%. After ﬁne-tuning the remaining fully-connected
layers, the performance loss can be further reduced to less
than 1%.

In Table 5, we also report the comparison against LANR
[31] on VGG-16. For the similar speed-up rate (4×), their
approach outperforms ours in the top-5 classiﬁcation error
(an increase of 0.95% against 1.83%). After ﬁne-tuning, the
performance gap is narrowed down to 0.35% against 0.45%.
At the same time, our approach offers over 14× compres-
sion of parameters in convolutional layers, much larger than
theirs 2.7× compression2. Therefore, our approach is effec-
tive in accelerating and compressing networks with many
convolutional layers, with only minor performance loss.

5.2.2 Quantizing the Fully-connected Layer

For demonstration, we ﬁrst compress parameters in a single
fully-connected layer. In CaffeNet, the ﬁrst fully-connected
layer possesses over 37 million parameters (9216 × 4096),
more than 60% of whole network parameters. Our Q-CNN
approach is adopted to quantize this layer and the results are
as reported in Table 6. The performance loss of our Q-CNN
model is negligible (within 0.4%), which is much smaller
than baseline methods (DPP and SVD). Furthermore, error
correction is effective in preserving the classiﬁcation accu-
racy, especially under a higher compression rate.

Now we evaluate our approach’s performance for com-
pressing all the fully-connected layers in CaffeNet in Ta-
ble 7. The third layer is actually the combination of 1000
classiﬁers, and is more critical to the classiﬁcation accuracy.
Hence, we adopt a much more ﬁne-grained hyper-parameter

2The compression effect of their approach was not explicitly discussed
in the paper; we estimate the compression rate based on their description.

DPP

Method

Table 6. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing the ﬁrst fully-connected layer
in CaffeNet, without ﬁne-tuning.
Compression
Para.
1.19×
-
1.47×
-
1.91×
-
2.75×
-
1.38×
-
2.77×
-
-
5.54×
11.08×
-
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32

Top-5 Err. ↑
-
-
-
-
-0.03%
0.07%
0.19%
0.86%
0.19%
0.28%
0.12%
0.16%
0.07%
0.03%
0.11%
0.12%

Top-1 Err. ↑
0.16%
1.76%
4.08%
9.68%
0.03%
0.07%
0.36%
1.23%
0.19%
0.35%
0.18%
0.28%
0.10%
0.18%
0.14%
0.16%

Q-CNN
(EC)

Q-CNN

SVD

setting (C′
s = 1, K = 16) for this layer. Although the
speed-up effect no longer exists, we can still achieve around
8× compression for the last layer.

SVD

Method

Table 7. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CaffeNet. Both SVD and DFC are ﬁne-tuned, while Q-CNN
and Q-CNN (EC) are not ﬁne-tuned.
Para.
-
-
-
-
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
1.26×
2.52×
1.79×
3.58×
13.96×
19.14×
15.25×
18.71×
13.96×
19.14×
15.25×
18.71×

Top-5 Err. ↑
-
-
-
-
0.29%
0.47%
0.34%
0.59%
0.30%
0.47%
0.27%
0.39%

Top-1 Err. ↑
0.14%
1.22%
-0.66%
0.31%
0.28%
0.70%
0.44%
0.75%
0.31%
0.59%
0.31%
0.57%

Q-CNN
(EC)

Q-CNN

DFC

From Table 7, we discover that with less than 1% drop in
accuracy, Q-CNN achieves high compression rates (12 ∼
20×), much larger than that of SVD3and DFC (< 4×).
Again, Q-CNN with error correction consistently outper-
forms the naive Q-CNN approach as adopted in [11].

3In Table 6, SVD means replacing the weighting matrix with the multi-
plication of two low-rank matrices; in Table 7, SVD means ﬁne-tuning the
network after the low-rank matrix decomposition.

5.2.3 Quantizing the Whole Network

So far, we have evaluated the performance of CNN models
with either convolutional or fully-connected layers quan-
tized. Now we demonstrate the quantization of the whole
network with a three-stage strategy. Firstly, we quantize all
the convolutional layers with error correction, while fully-
connected layers remain untouched. Secondly, we ﬁne-tune
fully-connected layers in the quantized network with the
ILSVRC-12 training set to restore the classiﬁcation accu-
racy. Finally, fully-connected layers in the ﬁne-tuned net-
work are quantized with error correction. We report the
performance of our Q-CNN models in Table 8.

Table 8. The speed-up/compression rates and the increase of top-
1/5 error rates for the whole CNN model. Particularly, for the
quantization of the third fully-connected layer in each network,
we let C ′
Model

s = 1 and K = 16.
Para.

Compression

Top-1/5 Err. ↑

Speed-up

AlexNet

CaffeNet

CNN-S

VGG-16

Conv.
8/128
8/128
8/128
8/128
8/128
8/128
6/128
6/128

FCnt.
3/32
4/32
3/32
4/32
3/32
4/32
3/32
4/32

4.05×
4.15×
4.04×
4.14×
5.69×
5.78×
4.05×
4.06×

15.40×
18.76×
15.40×
18.76×
16.32×
20.16×
16.55×
20.34×

1.38% / 0.84%
1.46% / 0.97%
1.43% / 0.99%
1.54% / 1.12%
1.48% / 0.81%
1.64% / 0.85%
1.22% / 0.53%
1.35% / 0.58%

For convolutional layers, we let C′

s = 8 and K = 128
for AlexNet, CaffeNet, and CNN-S, and let C′
s = 6 and
K = 128 for VGG-16, to ensure roughly 4 ∼ 6× speed-
up for each network. Then we vary the hyper-parameter
settings in fully-connected layers for different compression
levels. For the former two networks, we achieve 18× com-
pression with about 1% loss in the top-5 classiﬁcation accu-
racy. For CNN-S, we achieve 5.78× speed-up and 20.16×
compression, while the top-5 classiﬁcation accuracy drop is
merely 0.85%. The result on VGG-16 is even more encour-
aging: with 4.06× speed-up and 20.34×, the increase of
top-5 error rate is only 0.58%. Hence, our proposed Q-CNN
framework can improve the efﬁciency of convolutional net-
works with minor performance loss, which is acceptable in
many applications.

5.3. Results on Mobile Devices

We have developed an Android application to fulﬁll
CNN-based image classiﬁcation on mobile devices, based
on our Q-CNN framework. The experiments are carried
out on a Huawei R(cid:13) Mate 7 smartphone, equipped with an
1.8GHz Kirin 925 CPU. The test-phase computation is car-
ried out on a single CPU core, without GPU acceleration.

In Table 9, we compare the computation efﬁciency and
classiﬁcation accuracy of the original and quantized CNN
models. Our Q-CNN framework achieves 3× speed-up for
AlexNet, and 4× speed-up for CNN-S. What’s more, we
compress the storage consumption by 20 ×, and the re-

Table 9. Comparison on the time, storage, memory consumption,
and top-5 classiﬁcation error rates of the original and quantized
AlexNet and CNN-S.
Model

AlexNet

CNN-S

CNN
Q-CNN
CNN
Q-CNN

Time
2.93s
0.95s
10.58s
2.61s

Storage
232.56MB
12.60MB
392.57MB
20.13MB

Memory
264.74MB
74.65MB
468.90MB
129.49MB

Top-5 Err.
19.74%
20.70%
15.82%
16.68%

quired run-time memory is only one quarter of the original
model. At the same time, the loss in the top-5 classiﬁcation
accuracy is no more than 1%. Therefore, our proposed ap-
proach improves the run-time efﬁciency in multiple aspects,
making the deployment of CNN models become tractable
on mobile platforms.

5.4. Theoretical vs. Realistic Speed-up

In Table 10, we compare the theoretical and realistic
speed-up on AlexNet. The BLAS [29] library is used in
Caffe [15] to accelerate the matrix multiplication in con-
volutional and fully-connected layers. However, it may not
always be an option for mobile devices. Therefore, we mea-
sure the run-time speed under two settings, i.e. with BLAS
enabled or disabled. The realistic speed-up is slightly lower
with BLAS on, indicating that Q-CNN does not beneﬁt as
much from BLAS as that of CNN. Other optimization tech-
niques, e.g. SIMD, SSE, and AVX [4], may further improve
our realistic speed-up, and shall be explored in the future.

Table 10. Comparison on the theoretical and realistic speed-up on
AlexNet (CPU only, single-threaded). Here we use the ATLAS
library, which is the default BLAS choice in Caffe [15].

BLAS

Off
On

FLOPs

CNN

Q-CNN

7.29e+8

1.75e+8

Time (ms)

Speed-up

CNN
321.10
167.794

Q-CNN
75.62
55.35

Theo.

4.15×

Real.
4.25×
3.03×

6. Conclusion

In this paper, we propose a uniﬁed framework to si-
multaneously accelerate and compress convolutional neural
networks. We quantize network parameters to enable ef-
ﬁcient test-phase computation. Extensive experiments are
conducted on MNIST and ILSVRC-12, and our approach
achieves outstanding speed-up and compression rates, with
only negligible loss in the classiﬁcation accuracy.

7. Acknowledgement

This work was supported in part by National Natural Sci-
ence Foundation of China (Grant No. 61332016), and 863
program (Grant No. 2014AA015105).

4This is Caffe’s run-time speed. The code for the other three settings is

on https://github.com/jiaxiang-wu/quantized-cnn.

[22] C. Leng, J. Wu, J. Cheng, X. Zhang, and H. Lu. Hashing for dis-
In International Conference on Machine Learning

tributed data.
(ICML), pages 1642–1650, 2015. 2

[23] G. Levi and T. Hassncer. Age and gender classiﬁcation using convo-
lutional neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), pages 34–42, 2015.
1

[24] C. Li, Q. Liu, J. Liu, and H. Lu. Learning ordinal discriminative
features for age estimation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2570–2577, 2012. 1
[25] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks.
CoRR, abs/1603.05279, 2016. 5

[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei. Imagenet large scale visual recognition challenge. Inter-
national Journal of Computer Vision (IJCV), pages 1–42, 2015. 2,
5

[27] K. Simonyan and A. Zisserman. Very deep convolutional networks
In International Conference on

for large-scale image recognition.
Learning Representations (ICLR), 2015. 1, 2, 6

[28] S. Srinivas and R. V. Babu. Data-free parameter pruning for deep
In British Machine Vision Conference (BMVC),

neural networks.
pages 31.1–31.12, 2015. 1, 5

[29] R. C. Whaley and A. Petitet. Minimizing development and mainte-
nance costs in supporting persistently optimized BLAS. Software:
Practice and Experience, 35(2):101–121, Feb 2005. 8

[30] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. CoRR, abs/1412.7149,
2014. 1, 5

[31] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep
convolutional networks for classiﬁcation and detection. CoRR,
abs/1505.06798, 2015. 1, 5, 7

[32] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient and accurate
approximations of nonlinear convolutional networks. In IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), pages
1984–1992, 2015. 1, 5

References

[1] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return
of the devil in the details: Delving deep into convolutional nets. In
British Machine Vision Conference (BMVC), 2014. 1, 2, 6

[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In International
Conference on Machine Learning (ICML), pages 2285–2294, 2015.
1, 2, 5, 6

[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmid-
huber. High-performance neural networks for visual object classiﬁ-
cation. CoRR, abs/1102.0183, 2011. 1, 5, 6

[4] I. Corporation. Intel architecture instruction set extensions program-
ming reference. Technical report, Intel Corporation, Feb 2016. 8
[5] M. Courbariaux, Y. Bengio, and J. David. Training deep neural net-
In International Confer-

works with low precision multiplications.
ence on Learning Representations (ICLR), 2015. 5

[6] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas.
Predicting parameters in deep learning. In Advances in Neural In-
formation Processing Systems (NIPS), pages 2148–2156, 2013. 5,
6

[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Ex-
ploiting linear structure within convolutional networks for efﬁcient
evaluation. In Advances in Neural Information Processing Systems
(NIPS), pages 1269–1277, 2014. 1, 5

[8] J. D. Geoffrey Hinton, Oriol Vinyals. Distilling the knowledge in a

neural network. CoRR, abs/1503.02531, 2015. 5, 6

[9] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 1
[10] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 580–587, 2014. 1

[11] Y. Gong, L. Liu, M. Yang, and L. D. Bourdev. Compressing
deep convolutional networks using vector quantization. CoRR,
abs/1412.6115, 2014. 1, 2, 5, 7

[12] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding. CoRR, abs/1510.00149, 2015. 1, 2, 5

[13] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolu-
tional neural networks with low rank expansions. In British Machine
Vision Conference (BMVC), 2014. 1

[14] H. Jegou, M. Douze, and C. Schmid. Product quantization for near-
est neighbor search. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 33(1):117–128, Jan 2011. 2

[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. CoRR, abs/1408.5093, 2014. 2, 6, 8
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 1106–1114, 2012. 1,
2, 6

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lem-
pitsky. Speeding-up convolutional neural networks using ﬁne-tuned
cp-decomposition. In International Conference on Learning Repre-
sentations (ICLR), 2015. 1, 5, 6

[18] V. Lebedev and V. S. Lempitsky. Fast convnets using group-wise

brain damage. CoRR, abs/1506.02515, 2015. 1, 5, 6

[19] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. E. Hubbard, and L. D. Jackel. Backpropagation applied to hand-
written zip code recognition. Neural Computation, 1(4):541–551,
1989. 1

[20] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998. 2, 5

[21] C. Leng, J. Wu, J. Cheng, X. Bai, and H. Lu. Online sketching hash-
ing. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2503–2511, 2015. 2

Appendix A: Additional Results

In the submission, we report the performance after quan-
tizing all the convolutional layers in AlexNet, and quan-
tizing all the full-connected layers in CaffeNet. Here, we
present experimental results for some other settings.

Quantizing Convolutional Layers in CaffeNet

We quantize all the convolutional layers in CaffeNet, and
the results are as demonstrated in Table 11. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.15% and 0.75%, compared to the original
CaffeNet.

Table 11. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CaffeNet, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.32×
4.32×
3.71×
4.27×
3.32×
4.32×
3.71×
4.27×

Top-1 Err. ↑
18.69%
32.84%
20.08%
35.48%
1.22%
2.44%
1.57%
2.30%

Top-5 Err. ↑
16.73%
33.55%
18.31%
37.82%
0.97%
1.83%
1.12%
1.71%

Quantizing Convolutional Layers in CNN-S

We quantize all the convolutional layers in CNN-S, and
the results are as demonstrated in Table 12. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.24% and 0.63%, compared to the original
CNN-S.

Table 12. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CNN-S, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.69×
5.17×
4.78×
5.92×
3.69×
5.17×
4.78×
5.92×

Top-1 Err. ↑
19.87%
45.74%
27.86%
46.18%
1.60%
3.49%
2.07%
3.42%

Top-5 Err. ↑
16.77%
48.67%
25.09%
50.26%
0.92%
2.32%
1.32%
2.17%

Quantizing Fully-connected Layers in AlexNet

We quantize all the fully-connected layers in AlexNet,

and the results are as demonstrated in Table 13.

Quantizing Fully-connected Layers in CNN-S

We quantize all the fully-connected layers in CNN-S,

Method

Table 13. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in AlexNet, without ﬁne-tuning.
Compression
Para.
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32

Top-5 Err. ↑
0.27%
0.64%
0.33%
0.69%
0.20%
0.22%
0.21%
0.38%

Top-1 Err. ↑
0.25%
0.77%
0.54%
0.71%
0.14%
0.40%
0.40%
0.46%

Q-CNN
(EC)

Q-CNN

Method

Table 14. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CNN-S, without ﬁne-tuning.
Para.
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
14.37×
20.15×
15.79×
19.66×
14.37×
20.15×
15.79×
19.66×

Top-5 Err. ↑
0.07%
0.22%
0.11%
0.27%
0.14%
0.24%
0.11%
0.27%

Top-1 Err. ↑
0.22%
0.45%
0.21%
0.35%
0.36%
0.43%
0.29%
0.56%

Q-CNN
(EC)

Q-CNN

Appendix B: Optimization in Section 3.3.2

Assume we have N images to learn the quantization of a
convolutional layer. For image In, we denote its input fea-
ture maps as Sn ∈ Rds×ds×Cs and response feature maps
as Tn ∈ Rdt×dt×Ct, where ds, dt are the spatial sizes and
Cs, Ct are the number of feature map channels. We use
ps and pt to denote the spatial location in the input and re-
sponse feature maps. The spatial location in the convolu-
tional kernels is denoted as pk.

To learn quantization with error correction for the con-

volutional layer, we attempt to optimize:

2

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
m

X
n,pt

X
(pk,ps)

(D(m)B(m)

pk )T S(m)

min
{D(m)},{B(m)
pk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(14)
where Dm is the m-th sub-codebook, and B(m)
is the cor-
pk
responding sub-codeword assignment indicator for the con-
volutional kernels at spatial location pk.

n,ps − Tn,pt

Similar to the fully-connected layer, we adopt a block co-
ordinate descent approach to solve this optimization prob-
lem. For the m-th subspace, we ﬁrstly deﬁne its residual
feature map as:

R(m)

n,pt = Tn,pt − X
(pk,ps)

X
m′6=m

(D(m′

)B(m′
pk

)

)T S(m′
)
n,ps

(15)

and the results are as demonstrated in Table 14.

and then the optimization in the m-th subspace can be re-

formulated as:

min
D(m),{B(m)
pk }

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(16)
Update D(m). With the assignment indicator {B(m)
pk }

n,ps − R(m)
n,pt

pk )T S(m)

(D(m)B(m)

X
(pk,ps)

X
n,pt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ﬁxed, we let:

Lk,pk = {ct|B(m)

pk (k, ct) = 1}

(17)

We greedily update each sub-codeword in the m-th sub-
codebook D(m) in a sequential style. For the k-th sub-
codeword, we compute the corresponding residual feature
map as:

Q(m)

n,pt,k(ct) = R(m)

n,pt (ct) − X
(pk ,ps)

X
k′6=k

X
ct∈Lk′,pk

D(m)T
k′

S(m)
n,ps

(18)

and then we can alternatively optimize:

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

min
D(m)
k

D(m)T

X
(pk,ps)

2
(cid:13)
(cid:13)
n,pt,k(ct)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(19)
which can be transformed into a least square problem. By
solving it, we can update the k-th sub-codeword.

n,ps − Q(m)
S(m)

X
ct∈Lk,pk

Update {B(m)

pk }. We greedily update the sub-codeword
assignment at each spatial location in the convolutional ker-
nels in a sequential style. For the spatial location pk, we
compute the corresponding residual feature map as:

P (m)
n,pt,pk = R(m)

(D(m)B(m)

p′
k

)T S(m)
n,p′
s

(20)

n,pt − X
(p′
k,p′
s)
pk6=pk

and then the optimization can be re-written as:

min
B(m)
pk

X
n,pt

(D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

pk )T S(m)

n,ps − P (m)

2
n,pt,pk (cid:13)
(cid:13)
F
(cid:13)

(21)

Since B(m)
pk ∈ {0, 1}K is an indicator vector (only one non-
zero entry), we can exhaustively try all sub-codewords and
select the optimal one that minimize the objective function.

6
1
0
2
 
y
a
M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
3
7
4
6
0
.
2
1
5
1
:
v
i
X
r
a

Quantized Convolutional Neural Networks for Mobile Devices

Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng
National Laboratory of Patter Recognition
Institute of Automation, Chinese Academy of Sciences
{jiaxiang.wu, cong.leng, yuhang.wang, qinghao.hu, jcheng}@nlpr.ia.ac.cn

Abstract

Original

Q-CNN

Time Consumption (s)

Storage Consumption (MB)

Recently, convolutional neural networks (CNN) have
demonstrated impressive performance in various computer
vision tasks. However, high performance hardware is typ-
ically indispensable for the application of CNN models
due to the high computation complexity, which prohibits
their further extensions. In this paper, we propose an efﬁ-
cient framework, namely Quantized CNN, to simultaneously
speed-up the computation and reduce the storage and mem-
ory overhead of CNN models. Both ﬁlter kernels in con-
volutional layers and weighting matrices in fully-connected
layers are quantized, aiming at minimizing the estimation
error of each layer’s response. Extensive experiments on
the ILSVRC-12 benchmark demonstrate 4 ∼ 6× speed-up
and 15 ∼ 20× compression with merely one percentage
loss of classiﬁcation accuracy. With our quantized CNN
model, even mobile devices can accurately classify images
within one second.

15

10

5

0

500

400

300

200

100

0

AlexNet

CNN-S

AlexNet

CNN-S

Memory Consumption (MB)

Top-5 Error Rate (%)

AlexNet

CNN-S

AlexNet

CNN-S

Figure 1. Comparison on the efﬁciency and classiﬁcation accuracy
between the original and quantized AlexNet [16] and CNN-S [1]
on a Huawei R(cid:13) Mate 7 smartphone.

400

300

200

100

0

25

20

15

10

5

0

1. Introduction

In recent years, we have witnessed the great success
of convolutional neural networks (CNN) [19] in a wide
range of visual applications, including image classiﬁcation
[16, 27], object detection [10, 9], age estimation [24, 23],
etc. This success mainly comes from deeper network ar-
chitectures as well as the tremendous training data. How-
ever, as the network grows deeper, the model complexity is
also increasing exponentially in both the training and testing
stages, which leads to the very high demand in the computa-
tion ability. For instance, the 8-layer AlexNet [16] involves
60M parameters and requires over 729M FLOPs1to classify
a single image. Although the training stage can be ofﬂine
carried out on high performance clusters with GPU acceler-
ation, the testing computation cost may be unaffordable for
common personal computers and mobile devices. Due to
the limited computation ability and memory space, mobile
devices are almost intractable to run deep convolutional net-
works. Therefore, it is crucial to accelerate the computation

and compress the memory consumption for CNN models.

For most CNNs, convolutional layers are the most time-
consuming part, while fully-connected layers involve mas-
sive network parameters. Due to the intrinsical differ-
ence between them, existing works usually focus on im-
proving the efﬁciency for either convolutional layers or
fully-connected layers.
In [7, 13, 32, 31, 18, 17], low-
rank approximation or tensor decomposition is adopted to
speed-up convolutional layers. On the other hand, param-
eter compression in fully-connected layers is explored in
[3, 7, 11, 30, 2, 12, 28]. Overall, the above-mentioned al-
gorithms are able to achieve faster speed or less storage.
However, few of them can achieve signiﬁcant acceleration
and compression simultaneously for the whole network.

In this paper, we propose a uniﬁed framework for con-
volutional networks, namely Quantized CNN (Q-CNN), to
simultaneously accelerate and compress CNN models with

1FLOPs: number of FLoating-point OPerations required to classify one

image with the convolutional network.

1

only minor performance degradation. With network pa-
rameters quantized, the response of both convolutional and
fully-connected layers can be efﬁciently estimated via the
approximate inner product computation. We minimize the
estimation error of each layer’s response during parameter
quantization, which can better preserve the model perfor-
mance. In order to suppress the accumulative error while
quantizing multiple layers, an effective training scheme is
introduced to take previous estimation error into consider-
ation. Our Q-CNN model enables fast test-phase compu-
tation, and the storage and memory consumption are also
signiﬁcantly reduced.

We evaluate our Q-CNN framework for image classi-
ﬁcation on two benchmarks, MNIST [20] and ILSVRC-
12 [26]. For MNIST, our Q-CNN approach achieves over
12× compression for two neural networks (no convolu-
tion), with lower accuracy loss than several baseline meth-
ods. For ILSVRC-12, we attempt to improve the test-phase
efﬁciency of four convolutional networks: AlexNet [16],
CaffeNet [15], CNN-S [1], and VGG-16 [27]. Generally,
Q-CNN achieves 4× acceleration and 15× compression
(sometimes higher) for each network, with less than 1%
drop in the top-5 classiﬁcation accuracy. Moreover, we im-
plement the quantized CNN model on mobile devices, and
dramatically improve the test-phase efﬁciency, as depicted
in Figure 1. The main contributions of this paper can be
summarized as follows:

• We propose a uniﬁed Q-CNN framework to acceler-
ate and compress convolutional networks. We demon-
strate that better quantization can be learned by mini-
mizing the estimation error of each layer’s response.
• We propose an effective training scheme to suppress
the accumulative error while quantizing the whole con-
volutional network.

• Our Q-CNN framework achieves 4 ∼ 6× speed-up
and 15 ∼ 20× compression, while the classiﬁcation
accuracy loss is within one percentage. Moreover, the
quantized CNN model can be implemented on mobile
devices and classify an image within one second.

2. Preliminary

During the test phase of convolutional networks, the
computation overhead is dominated by convolutional lay-
ers; meanwhile, the majority of network parameters are
stored in fully-connected layers. Therefore, for better test-
phase efﬁciency, it is critical to speed-up the convolution
computation and compress parameters in fully-connected
layers.

Our observation is that the forward-passing process of
both convolutional and fully-connected layers is dominated
by the computation of inner products. More formally, we
consider a convolutional layer with input feature maps S ∈

Rds×ds×Cs and response feature maps T ∈ Rdt×dt×Ct,
where ds, dt are the spatial sizes and Cs, Ct are the number
of feature map channels. The response at the 2-D spatial
position pt in the ct-th response feature map is computed
as:

Tpt (ct) = X(pk,ps)

hWct,pk , Spsi

(1)

where Wct ∈ Rdk×dk×Cs is the ct-th convolutional kernel
and dk is the kernel size. We use ps and pk to denote the
2-D spatial positions in the input feature maps and convolu-
tional kernels, and both Wct,pk and Sps are Cs-dimensional
vectors. The layer response is the sum of inner products at
all positions within the dk × dk receptive ﬁeld in the input
feature maps.

Similarly, for a fully-connected layer, we have:

T (ct) = hWct , Si

(2)

where S ∈ RCs and T ∈ RCt are the layer input and layer
response, respectively, and Wct ∈ RCs is the weighting
vector for the ct-th neuron of this layer.

Product quantization [14] is widely used in approximate
nearest neighbor search, demonstrating better performance
than hashing-based methods [21, 22]. The idea is to de-
compose the feature space as the Cartesian product of mul-
tiple subspaces, and then learn sub-codebooks for each sub-
space. A vector is represented by the concatenation of sub-
codewords for efﬁcient distance computation and storage.

In this paper, we leverage product quantization to imple-
ment the efﬁcient inner product computation. Let us con-
sider the inner product computation between x, y ∈ RD. At
ﬁrst, both x and y are split into M sub-vectors, denoted as
x(m) and y(m). Afterwards, each x(m) is quantized with a
sub-codeword from the m-th sub-codebook, then we have

hy, xi = Xm

hy(m), x(m)i ≈ Xm

hy(m), c(m)
km i

(3)

which transforms the O(D) inner product computation to
M addition operations (M ≤ D), if the inner products be-
tween each sub-vector y(m) and all the sub-codewords in
the m-th sub-codebook have been computed in advance.

Quantization-based approaches have been explored in
several works [11, 2, 12]. These approaches mostly fo-
cus on compressing parameters in fully-connected layers
[11, 2], and none of them can provide acceleration for the
test-phase computation. Furthermore, [11, 12] require the
network parameters to be re-constructed during the test-
phase, which limit the compression to disk storage instead
of memory consumption. On the contrary, our approach
offers simultaneous acceleration and compression for both
convolutional and fully-connected layers, and can reduce
the run-time memory consumption dramatically.

3. Quantized CNN

In this section, we present our approach for accelerating
and compressing convolutional networks. Firstly, we intro-
duce an efﬁcient test-phase computation process with the
network parameters quantized. Secondly, we demonstrate
that better quantization can be learned by directly minimiz-
ing the estimation error of each layer’s response. Finally,
we analyze the computation complexity of our quantized
CNN model.

3.1. Quantizing the Fully-connected Layer

For a fully-connected layer, we denote its weighting ma-
trix as W ∈ RCs×Ct, where Cs and Ct are the dimensions
of the layer input and response, respectively. The weighting
vector Wct is the ct-th column vector in W .

We evenly split the Cs-dimensional space (where Wct
lies in) into M subspaces, each of C′
s = Cs/M dimen-
sions. Each Wct is then decomposed into M sub-vectors,
denoted as W (m)
. A sub-codebook can be learned for each
subspace after gathering all the sub-vectors within this sub-
space. Formally, for the m-th subspace, we optimize:

ct

min
D(m),B(m)

2
D(m)B(m) − W (m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
s×K, B(m) ∈ {0, 1}K×Ct

s.t. D(m) ∈ RC ′

(4)

where W (m) ∈ RC ′
s×Ct consists of the m-th sub-vectors
of all weighting vectors. The sub-codebook D(m) contains
K sub-codewords, and each column in B(m) is an indica-
tor vector (only one non-zero entry), specifying which sub-
codeword is used to quantize the corresponding sub-vector.
The optimization can be solved via k-means clustering.
The layer response is approximately computed as:
, S(m)i ≈ Xm
= Xm

T (ct) = Xm

km(ct), S(m)i

hD(m)B(m)

hW (m)
ct

, S(m)i

hD(m)

(5)

ct

ct

ct

where B(m)
is the ct-th column vector in B(m), and S(m) is
the m-th sub-vector of the layer input. km(ct) is the index
of the sub-codeword used to quantize the sub-vector W (m)
.
In Figure 2, we depict the parameter quantization and
test-phase computation process of the fully-connected layer.
By decomposing the weighting matrix into M sub-matrices,
M sub-codebooks can be learned, one per subspace. During
the test-phase, the layer input is split into M sub-vectors,
denoted as S(m). For each subspace, we compute the inner
products between S(m) and every sub-codeword in D(m),
and store the results in a look-up table. Afterwards, only M
addition operations are required to compute each response.
As a result, the overall time complexity can be reduced from
O(CsCt) to O(CsK + CtM ). On the other hand, only
sub-codebooks and quantization indices need to be stored,
which can dramatically reduce the storage consumption.

Figure 2. The parameter quantization and test-phase computation
process of the fully-connected layer.

3.2. Quantizing the Convolutional Layer

Unlike the 1-D weighting vector in the fully-connected
layer, each convolutional kernel is a 3-dimensional tensor:
Wct ∈ Rdk×dk×Cs. Before quantization, we need to deter-
mine how to split it into sub-vectors, i.e. apply subspace
splitting to which dimension. During the test phase, the in-
put feature maps are traversed by each convolutional kernel
with a sliding window in the spatial domain. Since these
sliding windows are partially overlapped, we split each con-
volutional kernel along the dimension of feature map chan-
nels, so that the pre-computed inner products can be re-
used at multiple spatial locations. Speciﬁcally, we learn the
quantization in each subspace by:

min
Xpk
D(m),{B(m)
pk }
s.t. D(m) ∈ RC ′

D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

s×K, B(m)

2
pk − W (m)
pk (cid:13)
(cid:13)
F
(cid:13)
pk ∈ {0, 1}K×Ct

(6)

pk ∈ RC ′

where W (m)
s×Ct contains the m-th sub-vectors of
all convolutional kernels at position pk. The optimization
can also be solved by k-means clustering in each subspace.
With the convolutional kernels quantized, we approxi-

mately compute the response feature maps by:

Tpt(ct) = X(pk,ps) Xm
≈ X(pk,ps) Xm
= X(pk,ps) Xm

hW (m)

ct,pk , S(m)
ps i

hD(m)B(m)

ct,pk , S(m)
ps i

(7)

hD(m)

km(ct,pk), S(m)
ps i

where S(m)
is the m-th sub-vector at position ps in the in-
ps
put feature maps, and km(ct, pk) is the index of the sub-
codeword to quantize the m-th sub-vector at position pk in
the ct-th convolutional kernel.

Similar to the fully-connected layer, we pre-compute the
look-up tables of inner products with the input feature maps.
Then, the response feature maps are approximately com-
puted with (7), and both the time and storage complexity
can be greatly reduced.

and the above optimization can be solved by alternatively
updating the sub-codebook and sub-codeword assignment.
Update D(m). We ﬁx the sub-codeword assignment
B(m), and deﬁne Lk = {ct|B(m)(k, ct) = 1}. The opti-
mization in (10) can be re-formulated as:

3.3. Quantization with Error Correction

So far, we have presented an intuitive approach to quan-
tize parameters and improve the test-phase efﬁciency of
convolutional networks. However, there are still two crit-
ical drawbacks. First, minimizing the quantization error
of model parameters does not necessarily give the optimal
quantized network for the classiﬁcation accuracy. In con-
trast, minimizing the estimation error of each layer’s re-
sponse is more closely related to the network’s classiﬁca-
tion performance. Second, the quantization of one layer is
independent of others, which may lead to the accumulation
of error when quantizing multiple layers. The estimation
error of the network’s ﬁnal response is very likely to be
quickly accumulated, since the error introduced by the pre-
vious quantized layers will also affect the following layers.
To overcome these two limitations, we introduce the idea
of error correction into the quantization of network param-
eters. This improved quantization approach directly min-
imizes the estimation error of the response at each layer,
and can compensate the error introduced by previous lay-
ers. With the error correction scheme, we can quantize the
network with much less performance degradation than the
original quantization method.

3.3.1 Error Correction for the Fully-connected Layer

Suppose we have N images to learn the quantization of a
fully-connected layer, and the layer input and response of
image In are denoted as Sn and Tn. In order to minimize
the estimation error of the layer response, we optimize:

Xn

min
{D(m)},{B(m)}

Tn − Xm
(cid:13)
(cid:13)
(cid:13)

2
(D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)
(8)
where the ﬁrst term in the Frobenius norm is the desired
layer response, and the second term is the approximated
layer response computed via the quantized parameters.

A block coordinate descent approach can be applied to
minimize this objective function. For the m-th subspace, its
residual error is deﬁned as:

R(m)

n = Tn − Xm′6=m

(D(m′

)B(m′

))T S(m′

n

)

(9)

and then we attempt to minimize the residual error of this
subspace, which is:

min

D(m),B(m) Xn

2
n − (D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)

R(m)
(cid:13)
(cid:13)
(cid:13)

(10)

min
{D(m)
k

}

Xn,k Xct∈Lk

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(11)

which implies that the optimization over one sub-codeword
does not affect other sub-codewords. Hence, for each sub-
codeword, we construct a least square problem from (11) to
update it.

Update B(m). With the sub-codebook D(m) ﬁxed, it
is easy to discover that the optimization of each column in
B(m) is mutually independent. For the ct-th column, its
optimal sub-codeword assignment is given by:

k∗
m(ct) = arg min

k Xn

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(12)

3.3.2 Error Correction for the Convolutional Layer

We adopt the similar idea to minimize the estimation error
of the convolutional layer’s response feature maps, that is:

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

(D(m)B(m)

min
{D(m)},{B(m)
pk }

Tn,pt − X
(pk ,ps)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(13)
The optimization also can be solved by block coordinate
descent. More details on solving this optimization can be
found in the supplementary material.

pk )T S(m)
n,ps

X
m

3.3.3 Error Correction for Multiple Layers

The above quantization method can be sequentially applied
to each layer in the CNN model. One concern is that the
estimation error of layer response caused by the previous
layers will be accumulated and affect the quantization of
the following layers. Here, we propose an effective training
scheme to address this issue.

We consider the quantization of a speciﬁc layer, assum-
ing its previous layers have already been quantized. The
optimization of parameter quantization is based on the layer
input and response of a group of training images. To quan-
tize this layer, we take the layer input in the quantized net-
work as {Sn}, and the layer response in the original net-
work (not quantized) as {Tn} in Eq. (8) and (13). In this
way, the optimization is guided by the actual input in the
quantized network and the desired response in the original
network. The accumulative error introduced by the previ-
ous layers is explicitly taken into consideration during op-
timization. In consequence, this training scheme can effec-
tively suppress the accumulative error for the quantization
of multiple layers.

Another possible solution is to adopt back-propagation
to jointly update the sub-codebooks and sub-codeword as-
signments in all quantized layers. However, since the sub-
codeword assignments are discrete, the gradient-based op-
timization can be quite difﬁcult, if not entirely impossible.
Therefore, back-propagation is not adopted here, but could
be a promising extension for future work.

3.4. Computation Complexity

Now we analyze the test-phase computation complex-
ity of convolutional and fully-connected layers, with or
without parameter quantization. For our proposed Q-CNN
model, the forward-passing through each layer mainly con-
sists of two procedures: pre-computation of inner products,
and approximate computation of layer response. Both sub-
codebooks and sub-codeword assignments are stored for the
test-phase computation. We report the detailed comparison
on the computation and storage overhead in Table 1.

Table 1. Comparison on the computation and storage overhead of
convolutional and fully-connected layers.

FLOPs

Bytes

Conv.

FCnt.

Conv.

FCnt.

kM

kCs
t Ctd2

t Ctd2
d2
CNN
sCsK + d2
d2
Q-CNN
CNN
CsCt
Q-CNN
CsK + CtM
4d2
CNN
kCsCt
Q-CNN 4CsK + 1
8 d2
kM Ct log2 K
CNN
4CsCt
Q-CNN
8 M Ct log2 K

4CsK + 1

As we can see from Table 1, the reduction in the compu-
tation and storage overhead largely depends on two hyper-
parameters, M (number of subspaces) and K (number of
sub-codewords in each subspace). Large values of M and
K lead to more ﬁne-grained quantization, but is less efﬁ-
cient in the computation and storage consumption. In prac-
tice, we can vary these two parameters to balance the trade-
off between the test-phase efﬁciency and accuracy loss of
the quantized CNN model.

4. Related Work

There have been a few attempts in accelerating the test-
phase computation of convolutional networks, and many are
inspired from the low-rank decomposition. Denton et al.
[7] presented a series of low-rank decomposition designs
for convolutional kernels. Similarly, CP-decomposition was
adopted in [17] to transform a convolutional layer into mul-
tiple layers with lower complexity. Zhang et al. [32, 31]
considered the subsequent nonlinear units while learning
the low-rank decomposition. [18] applied group-wise prun-
ing to the convolutional tensor to decompose it into the mul-
tiplications of thinned dense matrices. Recently, ﬁxed-point
based approaches are explored in [5, 25]. By representing

the connection weights (or even network activations) with
ﬁxed-point numbers, the computation can greatly beneﬁt
from hardware acceleration.

Another parallel research trend is to compress parame-
ters in fully-connected layers. Ciresan et al. [3] randomly
remove connection to reduce network parameters. Matrix
factorization was adopted in [6, 7] to decompose the weight-
ing matrix into two low-rank matrices, which demonstrated
that signiﬁcant redundancy did exist in network parameters.
Hinton et al. [8] proposed to use dark knowledge (the re-
sponse of a well-trained network) to guide the training of
a much smaller network, which was superior than directly
training. By exploring the similarity among neurons, Srini-
vas et al. [28] proposed a systematic way to remove redun-
dant neurons instead of network connections. In [30], mul-
tiple fully-connected layers were replaced by a single “Fast-
food” layer, which can be trained in an end-to-end style with
[2] randomly grouped
convolutional layers. Chen et al.
connection weights into hash buckets, and then ﬁne-tuned
the network with back-propagation. [12] combined prun-
ing, quantization, and Huffman coding to achieve higher
compression rate. Gong et al. [11] adopted vector quanti-
zation to compress the weighing matrix, which was actually
a special case of our approach (apply Q-CNN without error
correction to fully-connected layers only).

5. Experiments

In this section, we evaluate our quantized CNN frame-
work on two image classiﬁcation benchmarks, MNIST [20]
and ILSVRC-12 [26]. For the acceleration of convolutional
layers, we compare with:

• CPD [17]: CP-Decomposition;
• GBD [18]: Group-wise Brain Damage;
• LANR [31]: Low-rank Approximation of Non-linear

Responses.

and for the compression of fully-connected layers, we com-
pare with the following approaches:

• RER [3]: Random Edge Removal;
• LRD [6]: Low-Rank Decomposition;
• DK [8]: Dark Knowledge;
• HashNet [2]: Hashed Neural Nets;
• DPP [28]: Data-free Parameter Pruning;
• SVD [7]: Singular Value Decomposition;
• DFC [30]: Deep Fried Convnets.

For all above baselines, we use their reported results under
the same setting for fair comparison. We report the theo-
retical speed-up for more consistent results, since the real-
istic speed-up may be affected by various factors, e.g. CPU,
cache, and RAM. We compare the theoretical and realistic
speed-up in Section 5.4, and discuss the effect of adopting
the BLAS library for acceleration.

Our approaches are denoted as “Q-CNN” and “Q-CNN
(EC)”, where the latter one adopts error correction while the
former one does not. We implement the optimization pro-
cess of parameter quantization in MATLAB, and ﬁne-tune
the resulting network with Caffe [15]. Additional results of
our approach can be found in the supplementary material.

5.1. Results on MNIST

The MNIST dataset contains 70k images of hand-written
digits, 60k used for training and 10k for testing. To evalu-
ate the compression performance, we pre-train two neural
networks, one is 3-layer and another one is 5-layer, where
each hidden layer contains 1000 units. Different compres-
sion techniques are then adopted to compress these two net-
work, and the results are as depicted in Table 2.

Table 2. Comparison on the compression rates and classiﬁcation
error on MNIST, based on a 3-layer network (784-1000-10) and a
5-layer network (784-1000-1000-1000-10).

Method

Original
RER [3]
LRD [6]
DK [8]
HashNets [2]
Q-CNN
Q-CNN (EC)

3-layer

5-layer

Error
1.35%
2.19%
1.89%
1.71%
1.43%

Compr.
-
8×
8×
8×
8×

Compr.
-
8×
8×
8×
8×

Error
1.12%
1.24%
1.77%
1.26%
1.22%
12.1× 1.42% 13.4× 1.34%
12.1× 1.39% 13.4× 1.19%

s is determined once C′

In our Q-CNN framework, the trade-off between accu-
racy and efﬁciency is controlled by M (number of sub-
spaces) and K (number of sub-codewrods in each sub-
space). Since M = Cs/C′
s is given,
we tune (C′
s, K) to adjust the quantization precision. In Ta-
ble 2, we set the hyper-parameters as C′
s = 4 and K = 32.
From Table 2, we observe that our Q-CNN (EC) ap-
proach offers higher compression rates with less perfor-
mance degradation than all baselines for both networks.
The error correction scheme is effective in reducing the ac-
curacy loss, especially for deeper networks (5-layer). Also,
we ﬁnd the performance of both Q-CNN and Q-CNN (EC)
quite stable, as the standard deviation of ﬁve random runs is
merely 0.05%. Therefore, we report the single-run perfor-
mance in the remaining experiments.

5.2. Results on ILSVRC-12

The ILSVRC-12 benchmark consists of over one million
training images drawn from 1000 categories, and a disjoint
validation set of 50k images. We report both the top-1 and
top-5 classiﬁcation error rates on the validation set, using
single-view testing (central patch only).

We demonstrate our approach on four convolutional net-
works: AlexNet [16], CaffeNet [15], CNN-S [1], and VGG-

16 [27]. The ﬁrst two models have been adopted in several
related works, and therefore are included for comparison.
CNN-S and VGG-16 use a either wider or deeper structure
for better classiﬁcation accuracy, and are included here to
prove the scalability of our approach. We compare all these
networks’ computation and storage overhead in Table 3, to-
gether with their classiﬁcation error rates on ILSVRC-12.

Table 3. Comparison on the test-phase computation overhead
(FLOPs), storage consumption (Bytes), and classiﬁcation error
rates (Top-1/5 Err.) of AlexNet, CaffeNet, CNN-S, and VGG-16.
Bytes
2.44e+8
2.44e+8
4.12e+8
5.53e+8

Top-5 Err.
19.74%
19.59%
15.82%
10.05%

Top-1 Err.
42.78%
42.53%
37.31%
28.89%

FLOPs
7.29e+8
7.27e+8
2.94e+9
1.55e+10

Model
AlexNet
CaffeNet
CNN-S
VGG-16

5.2.1 Quantizing the Convolutional Layer

To begin with, we quantize the second convolutional layer
of AlexNet, which is the most time-consuming layer during
the test-phase. In Table 4, we report the performance un-
der several (C′
s, K) settings, comparing with two baseline
methods, CPD [17] and GBD [18].

Table 4. Comparison on the speed-up rates and the increase of top-
1/5 error rates for accelerating the second convolutional layer in
AlexNet, with or without ﬁne-tuning (FT). The hyper-parameters
of Q-CNN, C ′

Method

CPD

GBD

Q-CNN

Q-CNN
(EC)

Para.

Speed-up

s and K, are as speciﬁed in the “Para.” column.
Top-5 Err. ↑
Top-1 Err. ↑
FT
FT
0.44%
-
1.22%
-
18.63%
-
-
0.11%
-
0.43%
-
1.13%
1.37%
1.63%
2.27%
2.90%
1.28%
1.57%
2.66%
2.91%
0.17%
0.20%
0.40%
0.39%
0.21%
0.11%
0.31%
0.33%

No FT
0.94%
3.20%
69.06%
-
-
-
8.97%
14.71%
9.10%
18.05%
0.27%
0.50%
0.34%
0.50%

No FT
-
-
-
12.43%
21.93%
48.33%
10.55%
15.93%
10.62%
18.84%
0.35%
0.64%
0.27%
0.55%

3.19×
4.52×
6.51×
3.33×
5.00×
10.00×
3.70×
5.36×
4.84×
6.06×
3.70×
5.36×
4.84×
6.06×

-
-
-
-
-
-
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

From Table 4, we discover that with a large speed-up
rate (over 4×), the performance loss of both CPD and GBD
become severe, especially before ﬁne-tuning. The naive
parameter quantization method also suffers from the sim-
ilar problem. By incorporating the idea of error correction,
our Q-CNN model achieves up to 6× speed-up with merely
0.6% drop in accuracy, even without ﬁne-tuning. The ac-
curacy loss can be further reduced after ﬁne-tuning the sub-
sequent layers. Hence, it is more effective to minimize the
estimation error of each layer’s response than minimize the
quantization error of network parameters.

Next, we take one step further and attempt to speed-up
all the convolutional layers in AlexNet with Q-CNN (EC).

Table 5. Comparison on the speed-up/compression rates and the increase of top-1/5 error rates for accelerating all the convolutional layers
in AlexNet and VGG-16.

Model

Method

Para.

Speed-up Compression

AlexNet

Q-CNN
(EC)

VGG-16

LANR [31]
Q-CNN (EC)

4/64
6/64
6/128
8/128
-
6/128

3.32×
4.32×
3.71×
4.27×
4.00×
4.06×

10.58×
14.32×
10.27×
12.08×
2.73×
14.40×

Top-1 Err. ↑
FT
-
-

Top-5 Err. ↑
FT
-
-

No FT
0.94%
1.90%

No FT
1.33%
2.32%
1.44% 0.13% 1.16% 0.36%
2.25% 0.99% 1.64% 0.60%
0.95% 0.35%
3.04% 1.06% 1.83% 0.45%

-

-

We ﬁx the quantization hyper-parameters (C′
s, K) across all
layers. From Table 5, we observe that the loss in accuracy
grows mildly than the single-layer case. The speed-up rates
reported here are consistently smaller than those in Table 4,
since the acceleration effect is less signiﬁcant for some lay-
ers (i.e. “conv 4” and “conv 5”). For AlexNet, our Q-CNN
model (C′
s = 8, K = 128) can accelerate the computation
of all the convolutional layers by a factor of 4.27×, while
the increase in the top-1 and top-5 error rates are no more
than 2.5%. After ﬁne-tuning the remaining fully-connected
layers, the performance loss can be further reduced to less
than 1%.

In Table 5, we also report the comparison against LANR
[31] on VGG-16. For the similar speed-up rate (4×), their
approach outperforms ours in the top-5 classiﬁcation error
(an increase of 0.95% against 1.83%). After ﬁne-tuning, the
performance gap is narrowed down to 0.35% against 0.45%.
At the same time, our approach offers over 14× compres-
sion of parameters in convolutional layers, much larger than
theirs 2.7× compression2. Therefore, our approach is effec-
tive in accelerating and compressing networks with many
convolutional layers, with only minor performance loss.

5.2.2 Quantizing the Fully-connected Layer

For demonstration, we ﬁrst compress parameters in a single
fully-connected layer. In CaffeNet, the ﬁrst fully-connected
layer possesses over 37 million parameters (9216 × 4096),
more than 60% of whole network parameters. Our Q-CNN
approach is adopted to quantize this layer and the results are
as reported in Table 6. The performance loss of our Q-CNN
model is negligible (within 0.4%), which is much smaller
than baseline methods (DPP and SVD). Furthermore, error
correction is effective in preserving the classiﬁcation accu-
racy, especially under a higher compression rate.

Now we evaluate our approach’s performance for com-
pressing all the fully-connected layers in CaffeNet in Ta-
ble 7. The third layer is actually the combination of 1000
classiﬁers, and is more critical to the classiﬁcation accuracy.
Hence, we adopt a much more ﬁne-grained hyper-parameter

2The compression effect of their approach was not explicitly discussed
in the paper; we estimate the compression rate based on their description.

DPP

Method

Table 6. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing the ﬁrst fully-connected layer
in CaffeNet, without ﬁne-tuning.
Compression
Para.
1.19×
-
1.47×
-
1.91×
-
2.75×
-
1.38×
-
2.77×
-
-
5.54×
11.08×
-
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32

Top-5 Err. ↑
-
-
-
-
-0.03%
0.07%
0.19%
0.86%
0.19%
0.28%
0.12%
0.16%
0.07%
0.03%
0.11%
0.12%

Top-1 Err. ↑
0.16%
1.76%
4.08%
9.68%
0.03%
0.07%
0.36%
1.23%
0.19%
0.35%
0.18%
0.28%
0.10%
0.18%
0.14%
0.16%

Q-CNN
(EC)

Q-CNN

SVD

setting (C′
s = 1, K = 16) for this layer. Although the
speed-up effect no longer exists, we can still achieve around
8× compression for the last layer.

SVD

Method

Table 7. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CaffeNet. Both SVD and DFC are ﬁne-tuned, while Q-CNN
and Q-CNN (EC) are not ﬁne-tuned.
Para.
-
-
-
-
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
1.26×
2.52×
1.79×
3.58×
13.96×
19.14×
15.25×
18.71×
13.96×
19.14×
15.25×
18.71×

Top-5 Err. ↑
-
-
-
-
0.29%
0.47%
0.34%
0.59%
0.30%
0.47%
0.27%
0.39%

Top-1 Err. ↑
0.14%
1.22%
-0.66%
0.31%
0.28%
0.70%
0.44%
0.75%
0.31%
0.59%
0.31%
0.57%

Q-CNN
(EC)

Q-CNN

DFC

From Table 7, we discover that with less than 1% drop in
accuracy, Q-CNN achieves high compression rates (12 ∼
20×), much larger than that of SVD3and DFC (< 4×).
Again, Q-CNN with error correction consistently outper-
forms the naive Q-CNN approach as adopted in [11].

3In Table 6, SVD means replacing the weighting matrix with the multi-
plication of two low-rank matrices; in Table 7, SVD means ﬁne-tuning the
network after the low-rank matrix decomposition.

5.2.3 Quantizing the Whole Network

So far, we have evaluated the performance of CNN models
with either convolutional or fully-connected layers quan-
tized. Now we demonstrate the quantization of the whole
network with a three-stage strategy. Firstly, we quantize all
the convolutional layers with error correction, while fully-
connected layers remain untouched. Secondly, we ﬁne-tune
fully-connected layers in the quantized network with the
ILSVRC-12 training set to restore the classiﬁcation accu-
racy. Finally, fully-connected layers in the ﬁne-tuned net-
work are quantized with error correction. We report the
performance of our Q-CNN models in Table 8.

Table 8. The speed-up/compression rates and the increase of top-
1/5 error rates for the whole CNN model. Particularly, for the
quantization of the third fully-connected layer in each network,
we let C ′
Model

s = 1 and K = 16.
Para.

Compression

Top-1/5 Err. ↑

Speed-up

AlexNet

CaffeNet

CNN-S

VGG-16

Conv.
8/128
8/128
8/128
8/128
8/128
8/128
6/128
6/128

FCnt.
3/32
4/32
3/32
4/32
3/32
4/32
3/32
4/32

4.05×
4.15×
4.04×
4.14×
5.69×
5.78×
4.05×
4.06×

15.40×
18.76×
15.40×
18.76×
16.32×
20.16×
16.55×
20.34×

1.38% / 0.84%
1.46% / 0.97%
1.43% / 0.99%
1.54% / 1.12%
1.48% / 0.81%
1.64% / 0.85%
1.22% / 0.53%
1.35% / 0.58%

For convolutional layers, we let C′

s = 8 and K = 128
for AlexNet, CaffeNet, and CNN-S, and let C′
s = 6 and
K = 128 for VGG-16, to ensure roughly 4 ∼ 6× speed-
up for each network. Then we vary the hyper-parameter
settings in fully-connected layers for different compression
levels. For the former two networks, we achieve 18× com-
pression with about 1% loss in the top-5 classiﬁcation accu-
racy. For CNN-S, we achieve 5.78× speed-up and 20.16×
compression, while the top-5 classiﬁcation accuracy drop is
merely 0.85%. The result on VGG-16 is even more encour-
aging: with 4.06× speed-up and 20.34×, the increase of
top-5 error rate is only 0.58%. Hence, our proposed Q-CNN
framework can improve the efﬁciency of convolutional net-
works with minor performance loss, which is acceptable in
many applications.

5.3. Results on Mobile Devices

We have developed an Android application to fulﬁll
CNN-based image classiﬁcation on mobile devices, based
on our Q-CNN framework. The experiments are carried
out on a Huawei R(cid:13) Mate 7 smartphone, equipped with an
1.8GHz Kirin 925 CPU. The test-phase computation is car-
ried out on a single CPU core, without GPU acceleration.

In Table 9, we compare the computation efﬁciency and
classiﬁcation accuracy of the original and quantized CNN
models. Our Q-CNN framework achieves 3× speed-up for
AlexNet, and 4× speed-up for CNN-S. What’s more, we
compress the storage consumption by 20 ×, and the re-

Table 9. Comparison on the time, storage, memory consumption,
and top-5 classiﬁcation error rates of the original and quantized
AlexNet and CNN-S.
Model

AlexNet

CNN-S

CNN
Q-CNN
CNN
Q-CNN

Time
2.93s
0.95s
10.58s
2.61s

Storage
232.56MB
12.60MB
392.57MB
20.13MB

Memory
264.74MB
74.65MB
468.90MB
129.49MB

Top-5 Err.
19.74%
20.70%
15.82%
16.68%

quired run-time memory is only one quarter of the original
model. At the same time, the loss in the top-5 classiﬁcation
accuracy is no more than 1%. Therefore, our proposed ap-
proach improves the run-time efﬁciency in multiple aspects,
making the deployment of CNN models become tractable
on mobile platforms.

5.4. Theoretical vs. Realistic Speed-up

In Table 10, we compare the theoretical and realistic
speed-up on AlexNet. The BLAS [29] library is used in
Caffe [15] to accelerate the matrix multiplication in con-
volutional and fully-connected layers. However, it may not
always be an option for mobile devices. Therefore, we mea-
sure the run-time speed under two settings, i.e. with BLAS
enabled or disabled. The realistic speed-up is slightly lower
with BLAS on, indicating that Q-CNN does not beneﬁt as
much from BLAS as that of CNN. Other optimization tech-
niques, e.g. SIMD, SSE, and AVX [4], may further improve
our realistic speed-up, and shall be explored in the future.

Table 10. Comparison on the theoretical and realistic speed-up on
AlexNet (CPU only, single-threaded). Here we use the ATLAS
library, which is the default BLAS choice in Caffe [15].

BLAS

Off
On

FLOPs

CNN

Q-CNN

7.29e+8

1.75e+8

Time (ms)

Speed-up

CNN
321.10
167.794

Q-CNN
75.62
55.35

Theo.

4.15×

Real.
4.25×
3.03×

6. Conclusion

In this paper, we propose a uniﬁed framework to si-
multaneously accelerate and compress convolutional neural
networks. We quantize network parameters to enable ef-
ﬁcient test-phase computation. Extensive experiments are
conducted on MNIST and ILSVRC-12, and our approach
achieves outstanding speed-up and compression rates, with
only negligible loss in the classiﬁcation accuracy.

7. Acknowledgement

This work was supported in part by National Natural Sci-
ence Foundation of China (Grant No. 61332016), and 863
program (Grant No. 2014AA015105).

4This is Caffe’s run-time speed. The code for the other three settings is

on https://github.com/jiaxiang-wu/quantized-cnn.

[22] C. Leng, J. Wu, J. Cheng, X. Zhang, and H. Lu. Hashing for dis-
In International Conference on Machine Learning

tributed data.
(ICML), pages 1642–1650, 2015. 2

[23] G. Levi and T. Hassncer. Age and gender classiﬁcation using convo-
lutional neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), pages 34–42, 2015.
1

[24] C. Li, Q. Liu, J. Liu, and H. Lu. Learning ordinal discriminative
features for age estimation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2570–2577, 2012. 1
[25] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks.
CoRR, abs/1603.05279, 2016. 5

[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei. Imagenet large scale visual recognition challenge. Inter-
national Journal of Computer Vision (IJCV), pages 1–42, 2015. 2,
5

[27] K. Simonyan and A. Zisserman. Very deep convolutional networks
In International Conference on

for large-scale image recognition.
Learning Representations (ICLR), 2015. 1, 2, 6

[28] S. Srinivas and R. V. Babu. Data-free parameter pruning for deep
In British Machine Vision Conference (BMVC),

neural networks.
pages 31.1–31.12, 2015. 1, 5

[29] R. C. Whaley and A. Petitet. Minimizing development and mainte-
nance costs in supporting persistently optimized BLAS. Software:
Practice and Experience, 35(2):101–121, Feb 2005. 8

[30] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. CoRR, abs/1412.7149,
2014. 1, 5

[31] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep
convolutional networks for classiﬁcation and detection. CoRR,
abs/1505.06798, 2015. 1, 5, 7

[32] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient and accurate
approximations of nonlinear convolutional networks. In IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), pages
1984–1992, 2015. 1, 5

References

[1] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return
of the devil in the details: Delving deep into convolutional nets. In
British Machine Vision Conference (BMVC), 2014. 1, 2, 6

[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In International
Conference on Machine Learning (ICML), pages 2285–2294, 2015.
1, 2, 5, 6

[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmid-
huber. High-performance neural networks for visual object classiﬁ-
cation. CoRR, abs/1102.0183, 2011. 1, 5, 6

[4] I. Corporation. Intel architecture instruction set extensions program-
ming reference. Technical report, Intel Corporation, Feb 2016. 8
[5] M. Courbariaux, Y. Bengio, and J. David. Training deep neural net-
In International Confer-

works with low precision multiplications.
ence on Learning Representations (ICLR), 2015. 5

[6] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas.
Predicting parameters in deep learning. In Advances in Neural In-
formation Processing Systems (NIPS), pages 2148–2156, 2013. 5,
6

[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Ex-
ploiting linear structure within convolutional networks for efﬁcient
evaluation. In Advances in Neural Information Processing Systems
(NIPS), pages 1269–1277, 2014. 1, 5

[8] J. D. Geoffrey Hinton, Oriol Vinyals. Distilling the knowledge in a

neural network. CoRR, abs/1503.02531, 2015. 5, 6

[9] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 1
[10] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 580–587, 2014. 1

[11] Y. Gong, L. Liu, M. Yang, and L. D. Bourdev. Compressing
deep convolutional networks using vector quantization. CoRR,
abs/1412.6115, 2014. 1, 2, 5, 7

[12] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding. CoRR, abs/1510.00149, 2015. 1, 2, 5

[13] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolu-
tional neural networks with low rank expansions. In British Machine
Vision Conference (BMVC), 2014. 1

[14] H. Jegou, M. Douze, and C. Schmid. Product quantization for near-
est neighbor search. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 33(1):117–128, Jan 2011. 2

[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. CoRR, abs/1408.5093, 2014. 2, 6, 8
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 1106–1114, 2012. 1,
2, 6

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lem-
pitsky. Speeding-up convolutional neural networks using ﬁne-tuned
cp-decomposition. In International Conference on Learning Repre-
sentations (ICLR), 2015. 1, 5, 6

[18] V. Lebedev and V. S. Lempitsky. Fast convnets using group-wise

brain damage. CoRR, abs/1506.02515, 2015. 1, 5, 6

[19] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. E. Hubbard, and L. D. Jackel. Backpropagation applied to hand-
written zip code recognition. Neural Computation, 1(4):541–551,
1989. 1

[20] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998. 2, 5

[21] C. Leng, J. Wu, J. Cheng, X. Bai, and H. Lu. Online sketching hash-
ing. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2503–2511, 2015. 2

Appendix A: Additional Results

In the submission, we report the performance after quan-
tizing all the convolutional layers in AlexNet, and quan-
tizing all the full-connected layers in CaffeNet. Here, we
present experimental results for some other settings.

Quantizing Convolutional Layers in CaffeNet

We quantize all the convolutional layers in CaffeNet, and
the results are as demonstrated in Table 11. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.15% and 0.75%, compared to the original
CaffeNet.

Table 11. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CaffeNet, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.32×
4.32×
3.71×
4.27×
3.32×
4.32×
3.71×
4.27×

Top-1 Err. ↑
18.69%
32.84%
20.08%
35.48%
1.22%
2.44%
1.57%
2.30%

Top-5 Err. ↑
16.73%
33.55%
18.31%
37.82%
0.97%
1.83%
1.12%
1.71%

Quantizing Convolutional Layers in CNN-S

We quantize all the convolutional layers in CNN-S, and
the results are as demonstrated in Table 12. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.24% and 0.63%, compared to the original
CNN-S.

Table 12. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CNN-S, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.69×
5.17×
4.78×
5.92×
3.69×
5.17×
4.78×
5.92×

Top-1 Err. ↑
19.87%
45.74%
27.86%
46.18%
1.60%
3.49%
2.07%
3.42%

Top-5 Err. ↑
16.77%
48.67%
25.09%
50.26%
0.92%
2.32%
1.32%
2.17%

Quantizing Fully-connected Layers in AlexNet

We quantize all the fully-connected layers in AlexNet,

and the results are as demonstrated in Table 13.

Quantizing Fully-connected Layers in CNN-S

We quantize all the fully-connected layers in CNN-S,

Method

Table 13. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in AlexNet, without ﬁne-tuning.
Compression
Para.
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32

Top-5 Err. ↑
0.27%
0.64%
0.33%
0.69%
0.20%
0.22%
0.21%
0.38%

Top-1 Err. ↑
0.25%
0.77%
0.54%
0.71%
0.14%
0.40%
0.40%
0.46%

Q-CNN
(EC)

Q-CNN

Method

Table 14. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CNN-S, without ﬁne-tuning.
Para.
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
14.37×
20.15×
15.79×
19.66×
14.37×
20.15×
15.79×
19.66×

Top-5 Err. ↑
0.07%
0.22%
0.11%
0.27%
0.14%
0.24%
0.11%
0.27%

Top-1 Err. ↑
0.22%
0.45%
0.21%
0.35%
0.36%
0.43%
0.29%
0.56%

Q-CNN
(EC)

Q-CNN

Appendix B: Optimization in Section 3.3.2

Assume we have N images to learn the quantization of a
convolutional layer. For image In, we denote its input fea-
ture maps as Sn ∈ Rds×ds×Cs and response feature maps
as Tn ∈ Rdt×dt×Ct, where ds, dt are the spatial sizes and
Cs, Ct are the number of feature map channels. We use
ps and pt to denote the spatial location in the input and re-
sponse feature maps. The spatial location in the convolu-
tional kernels is denoted as pk.

To learn quantization with error correction for the con-

volutional layer, we attempt to optimize:

2

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
m

X
n,pt

X
(pk,ps)

(D(m)B(m)

pk )T S(m)

min
{D(m)},{B(m)
pk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(14)
where Dm is the m-th sub-codebook, and B(m)
is the cor-
pk
responding sub-codeword assignment indicator for the con-
volutional kernels at spatial location pk.

n,ps − Tn,pt

Similar to the fully-connected layer, we adopt a block co-
ordinate descent approach to solve this optimization prob-
lem. For the m-th subspace, we ﬁrstly deﬁne its residual
feature map as:

R(m)

n,pt = Tn,pt − X
(pk,ps)

X
m′6=m

(D(m′

)B(m′
pk

)

)T S(m′
)
n,ps

(15)

and the results are as demonstrated in Table 14.

and then the optimization in the m-th subspace can be re-

formulated as:

min
D(m),{B(m)
pk }

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(16)
Update D(m). With the assignment indicator {B(m)
pk }

n,ps − R(m)
n,pt

pk )T S(m)

(D(m)B(m)

X
(pk,ps)

X
n,pt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ﬁxed, we let:

Lk,pk = {ct|B(m)

pk (k, ct) = 1}

(17)

We greedily update each sub-codeword in the m-th sub-
codebook D(m) in a sequential style. For the k-th sub-
codeword, we compute the corresponding residual feature
map as:

Q(m)

n,pt,k(ct) = R(m)

n,pt (ct) − X
(pk ,ps)

X
k′6=k

X
ct∈Lk′,pk

D(m)T
k′

S(m)
n,ps

(18)

and then we can alternatively optimize:

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

min
D(m)
k

D(m)T

X
(pk,ps)

2
(cid:13)
(cid:13)
n,pt,k(ct)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(19)
which can be transformed into a least square problem. By
solving it, we can update the k-th sub-codeword.

n,ps − Q(m)
S(m)

X
ct∈Lk,pk

Update {B(m)

pk }. We greedily update the sub-codeword
assignment at each spatial location in the convolutional ker-
nels in a sequential style. For the spatial location pk, we
compute the corresponding residual feature map as:

P (m)
n,pt,pk = R(m)

(D(m)B(m)

p′
k

)T S(m)
n,p′
s

(20)

n,pt − X
(p′
k,p′
s)
pk6=pk

and then the optimization can be re-written as:

min
B(m)
pk

X
n,pt

(D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

pk )T S(m)

n,ps − P (m)

2
n,pt,pk (cid:13)
(cid:13)
F
(cid:13)

(21)

Since B(m)
pk ∈ {0, 1}K is an indicator vector (only one non-
zero entry), we can exhaustively try all sub-codewords and
select the optimal one that minimize the objective function.

6
1
0
2
 
y
a
M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
3
7
4
6
0
.
2
1
5
1
:
v
i
X
r
a

Quantized Convolutional Neural Networks for Mobile Devices

Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng
National Laboratory of Patter Recognition
Institute of Automation, Chinese Academy of Sciences
{jiaxiang.wu, cong.leng, yuhang.wang, qinghao.hu, jcheng}@nlpr.ia.ac.cn

Abstract

Original

Q-CNN

Time Consumption (s)

Storage Consumption (MB)

Recently, convolutional neural networks (CNN) have
demonstrated impressive performance in various computer
vision tasks. However, high performance hardware is typ-
ically indispensable for the application of CNN models
due to the high computation complexity, which prohibits
their further extensions. In this paper, we propose an efﬁ-
cient framework, namely Quantized CNN, to simultaneously
speed-up the computation and reduce the storage and mem-
ory overhead of CNN models. Both ﬁlter kernels in con-
volutional layers and weighting matrices in fully-connected
layers are quantized, aiming at minimizing the estimation
error of each layer’s response. Extensive experiments on
the ILSVRC-12 benchmark demonstrate 4 ∼ 6× speed-up
and 15 ∼ 20× compression with merely one percentage
loss of classiﬁcation accuracy. With our quantized CNN
model, even mobile devices can accurately classify images
within one second.

15

10

5

0

500

400

300

200

100

0

AlexNet

CNN-S

AlexNet

CNN-S

Memory Consumption (MB)

Top-5 Error Rate (%)

AlexNet

CNN-S

AlexNet

CNN-S

Figure 1. Comparison on the efﬁciency and classiﬁcation accuracy
between the original and quantized AlexNet [16] and CNN-S [1]
on a Huawei R(cid:13) Mate 7 smartphone.

400

300

200

100

0

25

20

15

10

5

0

1. Introduction

In recent years, we have witnessed the great success
of convolutional neural networks (CNN) [19] in a wide
range of visual applications, including image classiﬁcation
[16, 27], object detection [10, 9], age estimation [24, 23],
etc. This success mainly comes from deeper network ar-
chitectures as well as the tremendous training data. How-
ever, as the network grows deeper, the model complexity is
also increasing exponentially in both the training and testing
stages, which leads to the very high demand in the computa-
tion ability. For instance, the 8-layer AlexNet [16] involves
60M parameters and requires over 729M FLOPs1to classify
a single image. Although the training stage can be ofﬂine
carried out on high performance clusters with GPU acceler-
ation, the testing computation cost may be unaffordable for
common personal computers and mobile devices. Due to
the limited computation ability and memory space, mobile
devices are almost intractable to run deep convolutional net-
works. Therefore, it is crucial to accelerate the computation

and compress the memory consumption for CNN models.

For most CNNs, convolutional layers are the most time-
consuming part, while fully-connected layers involve mas-
sive network parameters. Due to the intrinsical differ-
ence between them, existing works usually focus on im-
proving the efﬁciency for either convolutional layers or
fully-connected layers.
In [7, 13, 32, 31, 18, 17], low-
rank approximation or tensor decomposition is adopted to
speed-up convolutional layers. On the other hand, param-
eter compression in fully-connected layers is explored in
[3, 7, 11, 30, 2, 12, 28]. Overall, the above-mentioned al-
gorithms are able to achieve faster speed or less storage.
However, few of them can achieve signiﬁcant acceleration
and compression simultaneously for the whole network.

In this paper, we propose a uniﬁed framework for con-
volutional networks, namely Quantized CNN (Q-CNN), to
simultaneously accelerate and compress CNN models with

1FLOPs: number of FLoating-point OPerations required to classify one

image with the convolutional network.

1

only minor performance degradation. With network pa-
rameters quantized, the response of both convolutional and
fully-connected layers can be efﬁciently estimated via the
approximate inner product computation. We minimize the
estimation error of each layer’s response during parameter
quantization, which can better preserve the model perfor-
mance. In order to suppress the accumulative error while
quantizing multiple layers, an effective training scheme is
introduced to take previous estimation error into consider-
ation. Our Q-CNN model enables fast test-phase compu-
tation, and the storage and memory consumption are also
signiﬁcantly reduced.

We evaluate our Q-CNN framework for image classi-
ﬁcation on two benchmarks, MNIST [20] and ILSVRC-
12 [26]. For MNIST, our Q-CNN approach achieves over
12× compression for two neural networks (no convolu-
tion), with lower accuracy loss than several baseline meth-
ods. For ILSVRC-12, we attempt to improve the test-phase
efﬁciency of four convolutional networks: AlexNet [16],
CaffeNet [15], CNN-S [1], and VGG-16 [27]. Generally,
Q-CNN achieves 4× acceleration and 15× compression
(sometimes higher) for each network, with less than 1%
drop in the top-5 classiﬁcation accuracy. Moreover, we im-
plement the quantized CNN model on mobile devices, and
dramatically improve the test-phase efﬁciency, as depicted
in Figure 1. The main contributions of this paper can be
summarized as follows:

• We propose a uniﬁed Q-CNN framework to acceler-
ate and compress convolutional networks. We demon-
strate that better quantization can be learned by mini-
mizing the estimation error of each layer’s response.
• We propose an effective training scheme to suppress
the accumulative error while quantizing the whole con-
volutional network.

• Our Q-CNN framework achieves 4 ∼ 6× speed-up
and 15 ∼ 20× compression, while the classiﬁcation
accuracy loss is within one percentage. Moreover, the
quantized CNN model can be implemented on mobile
devices and classify an image within one second.

2. Preliminary

During the test phase of convolutional networks, the
computation overhead is dominated by convolutional lay-
ers; meanwhile, the majority of network parameters are
stored in fully-connected layers. Therefore, for better test-
phase efﬁciency, it is critical to speed-up the convolution
computation and compress parameters in fully-connected
layers.

Our observation is that the forward-passing process of
both convolutional and fully-connected layers is dominated
by the computation of inner products. More formally, we
consider a convolutional layer with input feature maps S ∈

Rds×ds×Cs and response feature maps T ∈ Rdt×dt×Ct,
where ds, dt are the spatial sizes and Cs, Ct are the number
of feature map channels. The response at the 2-D spatial
position pt in the ct-th response feature map is computed
as:

Tpt (ct) = X(pk,ps)

hWct,pk , Spsi

(1)

where Wct ∈ Rdk×dk×Cs is the ct-th convolutional kernel
and dk is the kernel size. We use ps and pk to denote the
2-D spatial positions in the input feature maps and convolu-
tional kernels, and both Wct,pk and Sps are Cs-dimensional
vectors. The layer response is the sum of inner products at
all positions within the dk × dk receptive ﬁeld in the input
feature maps.

Similarly, for a fully-connected layer, we have:

T (ct) = hWct , Si

(2)

where S ∈ RCs and T ∈ RCt are the layer input and layer
response, respectively, and Wct ∈ RCs is the weighting
vector for the ct-th neuron of this layer.

Product quantization [14] is widely used in approximate
nearest neighbor search, demonstrating better performance
than hashing-based methods [21, 22]. The idea is to de-
compose the feature space as the Cartesian product of mul-
tiple subspaces, and then learn sub-codebooks for each sub-
space. A vector is represented by the concatenation of sub-
codewords for efﬁcient distance computation and storage.

In this paper, we leverage product quantization to imple-
ment the efﬁcient inner product computation. Let us con-
sider the inner product computation between x, y ∈ RD. At
ﬁrst, both x and y are split into M sub-vectors, denoted as
x(m) and y(m). Afterwards, each x(m) is quantized with a
sub-codeword from the m-th sub-codebook, then we have

hy, xi = Xm

hy(m), x(m)i ≈ Xm

hy(m), c(m)
km i

(3)

which transforms the O(D) inner product computation to
M addition operations (M ≤ D), if the inner products be-
tween each sub-vector y(m) and all the sub-codewords in
the m-th sub-codebook have been computed in advance.

Quantization-based approaches have been explored in
several works [11, 2, 12]. These approaches mostly fo-
cus on compressing parameters in fully-connected layers
[11, 2], and none of them can provide acceleration for the
test-phase computation. Furthermore, [11, 12] require the
network parameters to be re-constructed during the test-
phase, which limit the compression to disk storage instead
of memory consumption. On the contrary, our approach
offers simultaneous acceleration and compression for both
convolutional and fully-connected layers, and can reduce
the run-time memory consumption dramatically.

3. Quantized CNN

In this section, we present our approach for accelerating
and compressing convolutional networks. Firstly, we intro-
duce an efﬁcient test-phase computation process with the
network parameters quantized. Secondly, we demonstrate
that better quantization can be learned by directly minimiz-
ing the estimation error of each layer’s response. Finally,
we analyze the computation complexity of our quantized
CNN model.

3.1. Quantizing the Fully-connected Layer

For a fully-connected layer, we denote its weighting ma-
trix as W ∈ RCs×Ct, where Cs and Ct are the dimensions
of the layer input and response, respectively. The weighting
vector Wct is the ct-th column vector in W .

We evenly split the Cs-dimensional space (where Wct
lies in) into M subspaces, each of C′
s = Cs/M dimen-
sions. Each Wct is then decomposed into M sub-vectors,
denoted as W (m)
. A sub-codebook can be learned for each
subspace after gathering all the sub-vectors within this sub-
space. Formally, for the m-th subspace, we optimize:

ct

min
D(m),B(m)

2
D(m)B(m) − W (m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
s×K, B(m) ∈ {0, 1}K×Ct

s.t. D(m) ∈ RC ′

(4)

where W (m) ∈ RC ′
s×Ct consists of the m-th sub-vectors
of all weighting vectors. The sub-codebook D(m) contains
K sub-codewords, and each column in B(m) is an indica-
tor vector (only one non-zero entry), specifying which sub-
codeword is used to quantize the corresponding sub-vector.
The optimization can be solved via k-means clustering.
The layer response is approximately computed as:
, S(m)i ≈ Xm
= Xm

T (ct) = Xm

km(ct), S(m)i

hD(m)B(m)

hW (m)
ct

, S(m)i

hD(m)

(5)

ct

ct

ct

where B(m)
is the ct-th column vector in B(m), and S(m) is
the m-th sub-vector of the layer input. km(ct) is the index
of the sub-codeword used to quantize the sub-vector W (m)
.
In Figure 2, we depict the parameter quantization and
test-phase computation process of the fully-connected layer.
By decomposing the weighting matrix into M sub-matrices,
M sub-codebooks can be learned, one per subspace. During
the test-phase, the layer input is split into M sub-vectors,
denoted as S(m). For each subspace, we compute the inner
products between S(m) and every sub-codeword in D(m),
and store the results in a look-up table. Afterwards, only M
addition operations are required to compute each response.
As a result, the overall time complexity can be reduced from
O(CsCt) to O(CsK + CtM ). On the other hand, only
sub-codebooks and quantization indices need to be stored,
which can dramatically reduce the storage consumption.

Figure 2. The parameter quantization and test-phase computation
process of the fully-connected layer.

3.2. Quantizing the Convolutional Layer

Unlike the 1-D weighting vector in the fully-connected
layer, each convolutional kernel is a 3-dimensional tensor:
Wct ∈ Rdk×dk×Cs. Before quantization, we need to deter-
mine how to split it into sub-vectors, i.e. apply subspace
splitting to which dimension. During the test phase, the in-
put feature maps are traversed by each convolutional kernel
with a sliding window in the spatial domain. Since these
sliding windows are partially overlapped, we split each con-
volutional kernel along the dimension of feature map chan-
nels, so that the pre-computed inner products can be re-
used at multiple spatial locations. Speciﬁcally, we learn the
quantization in each subspace by:

min
Xpk
D(m),{B(m)
pk }
s.t. D(m) ∈ RC ′

D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

s×K, B(m)

2
pk − W (m)
pk (cid:13)
(cid:13)
F
(cid:13)
pk ∈ {0, 1}K×Ct

(6)

pk ∈ RC ′

where W (m)
s×Ct contains the m-th sub-vectors of
all convolutional kernels at position pk. The optimization
can also be solved by k-means clustering in each subspace.
With the convolutional kernels quantized, we approxi-

mately compute the response feature maps by:

Tpt(ct) = X(pk,ps) Xm
≈ X(pk,ps) Xm
= X(pk,ps) Xm

hW (m)

ct,pk , S(m)
ps i

hD(m)B(m)

ct,pk , S(m)
ps i

(7)

hD(m)

km(ct,pk), S(m)
ps i

where S(m)
is the m-th sub-vector at position ps in the in-
ps
put feature maps, and km(ct, pk) is the index of the sub-
codeword to quantize the m-th sub-vector at position pk in
the ct-th convolutional kernel.

Similar to the fully-connected layer, we pre-compute the
look-up tables of inner products with the input feature maps.
Then, the response feature maps are approximately com-
puted with (7), and both the time and storage complexity
can be greatly reduced.

and the above optimization can be solved by alternatively
updating the sub-codebook and sub-codeword assignment.
Update D(m). We ﬁx the sub-codeword assignment
B(m), and deﬁne Lk = {ct|B(m)(k, ct) = 1}. The opti-
mization in (10) can be re-formulated as:

3.3. Quantization with Error Correction

So far, we have presented an intuitive approach to quan-
tize parameters and improve the test-phase efﬁciency of
convolutional networks. However, there are still two crit-
ical drawbacks. First, minimizing the quantization error
of model parameters does not necessarily give the optimal
quantized network for the classiﬁcation accuracy. In con-
trast, minimizing the estimation error of each layer’s re-
sponse is more closely related to the network’s classiﬁca-
tion performance. Second, the quantization of one layer is
independent of others, which may lead to the accumulation
of error when quantizing multiple layers. The estimation
error of the network’s ﬁnal response is very likely to be
quickly accumulated, since the error introduced by the pre-
vious quantized layers will also affect the following layers.
To overcome these two limitations, we introduce the idea
of error correction into the quantization of network param-
eters. This improved quantization approach directly min-
imizes the estimation error of the response at each layer,
and can compensate the error introduced by previous lay-
ers. With the error correction scheme, we can quantize the
network with much less performance degradation than the
original quantization method.

3.3.1 Error Correction for the Fully-connected Layer

Suppose we have N images to learn the quantization of a
fully-connected layer, and the layer input and response of
image In are denoted as Sn and Tn. In order to minimize
the estimation error of the layer response, we optimize:

Xn

min
{D(m)},{B(m)}

Tn − Xm
(cid:13)
(cid:13)
(cid:13)

2
(D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)
(8)
where the ﬁrst term in the Frobenius norm is the desired
layer response, and the second term is the approximated
layer response computed via the quantized parameters.

A block coordinate descent approach can be applied to
minimize this objective function. For the m-th subspace, its
residual error is deﬁned as:

R(m)

n = Tn − Xm′6=m

(D(m′

)B(m′

))T S(m′

n

)

(9)

and then we attempt to minimize the residual error of this
subspace, which is:

min

D(m),B(m) Xn

2
n − (D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)

R(m)
(cid:13)
(cid:13)
(cid:13)

(10)

min
{D(m)
k

}

Xn,k Xct∈Lk

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(11)

which implies that the optimization over one sub-codeword
does not affect other sub-codewords. Hence, for each sub-
codeword, we construct a least square problem from (11) to
update it.

Update B(m). With the sub-codebook D(m) ﬁxed, it
is easy to discover that the optimization of each column in
B(m) is mutually independent. For the ct-th column, its
optimal sub-codeword assignment is given by:

k∗
m(ct) = arg min

k Xn

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(12)

3.3.2 Error Correction for the Convolutional Layer

We adopt the similar idea to minimize the estimation error
of the convolutional layer’s response feature maps, that is:

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

(D(m)B(m)

min
{D(m)},{B(m)
pk }

Tn,pt − X
(pk ,ps)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(13)
The optimization also can be solved by block coordinate
descent. More details on solving this optimization can be
found in the supplementary material.

pk )T S(m)
n,ps

X
m

3.3.3 Error Correction for Multiple Layers

The above quantization method can be sequentially applied
to each layer in the CNN model. One concern is that the
estimation error of layer response caused by the previous
layers will be accumulated and affect the quantization of
the following layers. Here, we propose an effective training
scheme to address this issue.

We consider the quantization of a speciﬁc layer, assum-
ing its previous layers have already been quantized. The
optimization of parameter quantization is based on the layer
input and response of a group of training images. To quan-
tize this layer, we take the layer input in the quantized net-
work as {Sn}, and the layer response in the original net-
work (not quantized) as {Tn} in Eq. (8) and (13). In this
way, the optimization is guided by the actual input in the
quantized network and the desired response in the original
network. The accumulative error introduced by the previ-
ous layers is explicitly taken into consideration during op-
timization. In consequence, this training scheme can effec-
tively suppress the accumulative error for the quantization
of multiple layers.

Another possible solution is to adopt back-propagation
to jointly update the sub-codebooks and sub-codeword as-
signments in all quantized layers. However, since the sub-
codeword assignments are discrete, the gradient-based op-
timization can be quite difﬁcult, if not entirely impossible.
Therefore, back-propagation is not adopted here, but could
be a promising extension for future work.

3.4. Computation Complexity

Now we analyze the test-phase computation complex-
ity of convolutional and fully-connected layers, with or
without parameter quantization. For our proposed Q-CNN
model, the forward-passing through each layer mainly con-
sists of two procedures: pre-computation of inner products,
and approximate computation of layer response. Both sub-
codebooks and sub-codeword assignments are stored for the
test-phase computation. We report the detailed comparison
on the computation and storage overhead in Table 1.

Table 1. Comparison on the computation and storage overhead of
convolutional and fully-connected layers.

FLOPs

Bytes

Conv.

FCnt.

Conv.

FCnt.

kM

kCs
t Ctd2

t Ctd2
d2
CNN
sCsK + d2
d2
Q-CNN
CNN
CsCt
Q-CNN
CsK + CtM
4d2
CNN
kCsCt
Q-CNN 4CsK + 1
8 d2
kM Ct log2 K
CNN
4CsCt
Q-CNN
8 M Ct log2 K

4CsK + 1

As we can see from Table 1, the reduction in the compu-
tation and storage overhead largely depends on two hyper-
parameters, M (number of subspaces) and K (number of
sub-codewords in each subspace). Large values of M and
K lead to more ﬁne-grained quantization, but is less efﬁ-
cient in the computation and storage consumption. In prac-
tice, we can vary these two parameters to balance the trade-
off between the test-phase efﬁciency and accuracy loss of
the quantized CNN model.

4. Related Work

There have been a few attempts in accelerating the test-
phase computation of convolutional networks, and many are
inspired from the low-rank decomposition. Denton et al.
[7] presented a series of low-rank decomposition designs
for convolutional kernels. Similarly, CP-decomposition was
adopted in [17] to transform a convolutional layer into mul-
tiple layers with lower complexity. Zhang et al. [32, 31]
considered the subsequent nonlinear units while learning
the low-rank decomposition. [18] applied group-wise prun-
ing to the convolutional tensor to decompose it into the mul-
tiplications of thinned dense matrices. Recently, ﬁxed-point
based approaches are explored in [5, 25]. By representing

the connection weights (or even network activations) with
ﬁxed-point numbers, the computation can greatly beneﬁt
from hardware acceleration.

Another parallel research trend is to compress parame-
ters in fully-connected layers. Ciresan et al. [3] randomly
remove connection to reduce network parameters. Matrix
factorization was adopted in [6, 7] to decompose the weight-
ing matrix into two low-rank matrices, which demonstrated
that signiﬁcant redundancy did exist in network parameters.
Hinton et al. [8] proposed to use dark knowledge (the re-
sponse of a well-trained network) to guide the training of
a much smaller network, which was superior than directly
training. By exploring the similarity among neurons, Srini-
vas et al. [28] proposed a systematic way to remove redun-
dant neurons instead of network connections. In [30], mul-
tiple fully-connected layers were replaced by a single “Fast-
food” layer, which can be trained in an end-to-end style with
[2] randomly grouped
convolutional layers. Chen et al.
connection weights into hash buckets, and then ﬁne-tuned
the network with back-propagation. [12] combined prun-
ing, quantization, and Huffman coding to achieve higher
compression rate. Gong et al. [11] adopted vector quanti-
zation to compress the weighing matrix, which was actually
a special case of our approach (apply Q-CNN without error
correction to fully-connected layers only).

5. Experiments

In this section, we evaluate our quantized CNN frame-
work on two image classiﬁcation benchmarks, MNIST [20]
and ILSVRC-12 [26]. For the acceleration of convolutional
layers, we compare with:

• CPD [17]: CP-Decomposition;
• GBD [18]: Group-wise Brain Damage;
• LANR [31]: Low-rank Approximation of Non-linear

Responses.

and for the compression of fully-connected layers, we com-
pare with the following approaches:

• RER [3]: Random Edge Removal;
• LRD [6]: Low-Rank Decomposition;
• DK [8]: Dark Knowledge;
• HashNet [2]: Hashed Neural Nets;
• DPP [28]: Data-free Parameter Pruning;
• SVD [7]: Singular Value Decomposition;
• DFC [30]: Deep Fried Convnets.

For all above baselines, we use their reported results under
the same setting for fair comparison. We report the theo-
retical speed-up for more consistent results, since the real-
istic speed-up may be affected by various factors, e.g. CPU,
cache, and RAM. We compare the theoretical and realistic
speed-up in Section 5.4, and discuss the effect of adopting
the BLAS library for acceleration.

Our approaches are denoted as “Q-CNN” and “Q-CNN
(EC)”, where the latter one adopts error correction while the
former one does not. We implement the optimization pro-
cess of parameter quantization in MATLAB, and ﬁne-tune
the resulting network with Caffe [15]. Additional results of
our approach can be found in the supplementary material.

5.1. Results on MNIST

The MNIST dataset contains 70k images of hand-written
digits, 60k used for training and 10k for testing. To evalu-
ate the compression performance, we pre-train two neural
networks, one is 3-layer and another one is 5-layer, where
each hidden layer contains 1000 units. Different compres-
sion techniques are then adopted to compress these two net-
work, and the results are as depicted in Table 2.

Table 2. Comparison on the compression rates and classiﬁcation
error on MNIST, based on a 3-layer network (784-1000-10) and a
5-layer network (784-1000-1000-1000-10).

Method

Original
RER [3]
LRD [6]
DK [8]
HashNets [2]
Q-CNN
Q-CNN (EC)

3-layer

5-layer

Error
1.35%
2.19%
1.89%
1.71%
1.43%

Compr.
-
8×
8×
8×
8×

Compr.
-
8×
8×
8×
8×

Error
1.12%
1.24%
1.77%
1.26%
1.22%
12.1× 1.42% 13.4× 1.34%
12.1× 1.39% 13.4× 1.19%

s is determined once C′

In our Q-CNN framework, the trade-off between accu-
racy and efﬁciency is controlled by M (number of sub-
spaces) and K (number of sub-codewrods in each sub-
space). Since M = Cs/C′
s is given,
we tune (C′
s, K) to adjust the quantization precision. In Ta-
ble 2, we set the hyper-parameters as C′
s = 4 and K = 32.
From Table 2, we observe that our Q-CNN (EC) ap-
proach offers higher compression rates with less perfor-
mance degradation than all baselines for both networks.
The error correction scheme is effective in reducing the ac-
curacy loss, especially for deeper networks (5-layer). Also,
we ﬁnd the performance of both Q-CNN and Q-CNN (EC)
quite stable, as the standard deviation of ﬁve random runs is
merely 0.05%. Therefore, we report the single-run perfor-
mance in the remaining experiments.

5.2. Results on ILSVRC-12

The ILSVRC-12 benchmark consists of over one million
training images drawn from 1000 categories, and a disjoint
validation set of 50k images. We report both the top-1 and
top-5 classiﬁcation error rates on the validation set, using
single-view testing (central patch only).

We demonstrate our approach on four convolutional net-
works: AlexNet [16], CaffeNet [15], CNN-S [1], and VGG-

16 [27]. The ﬁrst two models have been adopted in several
related works, and therefore are included for comparison.
CNN-S and VGG-16 use a either wider or deeper structure
for better classiﬁcation accuracy, and are included here to
prove the scalability of our approach. We compare all these
networks’ computation and storage overhead in Table 3, to-
gether with their classiﬁcation error rates on ILSVRC-12.

Table 3. Comparison on the test-phase computation overhead
(FLOPs), storage consumption (Bytes), and classiﬁcation error
rates (Top-1/5 Err.) of AlexNet, CaffeNet, CNN-S, and VGG-16.
Bytes
2.44e+8
2.44e+8
4.12e+8
5.53e+8

Top-1 Err.
42.78%
42.53%
37.31%
28.89%

Top-5 Err.
19.74%
19.59%
15.82%
10.05%

FLOPs
7.29e+8
7.27e+8
2.94e+9
1.55e+10

Model
AlexNet
CaffeNet
CNN-S
VGG-16

5.2.1 Quantizing the Convolutional Layer

To begin with, we quantize the second convolutional layer
of AlexNet, which is the most time-consuming layer during
the test-phase. In Table 4, we report the performance un-
der several (C′
s, K) settings, comparing with two baseline
methods, CPD [17] and GBD [18].

Table 4. Comparison on the speed-up rates and the increase of top-
1/5 error rates for accelerating the second convolutional layer in
AlexNet, with or without ﬁne-tuning (FT). The hyper-parameters
of Q-CNN, C ′

Method

CPD

GBD

Q-CNN

Q-CNN
(EC)

Para.

Speed-up

s and K, are as speciﬁed in the “Para.” column.
Top-5 Err. ↑
Top-1 Err. ↑
FT
FT
0.44%
-
1.22%
-
18.63%
-
-
0.11%
-
0.43%
-
1.13%
1.37%
1.63%
2.27%
2.90%
1.28%
1.57%
2.66%
2.91%
0.17%
0.20%
0.40%
0.39%
0.21%
0.11%
0.31%
0.33%

No FT
0.94%
3.20%
69.06%
-
-
-
8.97%
14.71%
9.10%
18.05%
0.27%
0.50%
0.34%
0.50%

No FT
-
-
-
12.43%
21.93%
48.33%
10.55%
15.93%
10.62%
18.84%
0.35%
0.64%
0.27%
0.55%

3.19×
4.52×
6.51×
3.33×
5.00×
10.00×
3.70×
5.36×
4.84×
6.06×
3.70×
5.36×
4.84×
6.06×

-
-
-
-
-
-
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

From Table 4, we discover that with a large speed-up
rate (over 4×), the performance loss of both CPD and GBD
become severe, especially before ﬁne-tuning. The naive
parameter quantization method also suffers from the sim-
ilar problem. By incorporating the idea of error correction,
our Q-CNN model achieves up to 6× speed-up with merely
0.6% drop in accuracy, even without ﬁne-tuning. The ac-
curacy loss can be further reduced after ﬁne-tuning the sub-
sequent layers. Hence, it is more effective to minimize the
estimation error of each layer’s response than minimize the
quantization error of network parameters.

Next, we take one step further and attempt to speed-up
all the convolutional layers in AlexNet with Q-CNN (EC).

Table 5. Comparison on the speed-up/compression rates and the increase of top-1/5 error rates for accelerating all the convolutional layers
in AlexNet and VGG-16.

Model

Method

Para.

Speed-up Compression

AlexNet

Q-CNN
(EC)

VGG-16

LANR [31]
Q-CNN (EC)

4/64
6/64
6/128
8/128
-
6/128

3.32×
4.32×
3.71×
4.27×
4.00×
4.06×

10.58×
14.32×
10.27×
12.08×
2.73×
14.40×

Top-1 Err. ↑
FT
-
-

Top-5 Err. ↑
FT
-
-

No FT
0.94%
1.90%

No FT
1.33%
2.32%
1.44% 0.13% 1.16% 0.36%
2.25% 0.99% 1.64% 0.60%
0.95% 0.35%
3.04% 1.06% 1.83% 0.45%

-

-

We ﬁx the quantization hyper-parameters (C′
s, K) across all
layers. From Table 5, we observe that the loss in accuracy
grows mildly than the single-layer case. The speed-up rates
reported here are consistently smaller than those in Table 4,
since the acceleration effect is less signiﬁcant for some lay-
ers (i.e. “conv 4” and “conv 5”). For AlexNet, our Q-CNN
model (C′
s = 8, K = 128) can accelerate the computation
of all the convolutional layers by a factor of 4.27×, while
the increase in the top-1 and top-5 error rates are no more
than 2.5%. After ﬁne-tuning the remaining fully-connected
layers, the performance loss can be further reduced to less
than 1%.

In Table 5, we also report the comparison against LANR
[31] on VGG-16. For the similar speed-up rate (4×), their
approach outperforms ours in the top-5 classiﬁcation error
(an increase of 0.95% against 1.83%). After ﬁne-tuning, the
performance gap is narrowed down to 0.35% against 0.45%.
At the same time, our approach offers over 14× compres-
sion of parameters in convolutional layers, much larger than
theirs 2.7× compression2. Therefore, our approach is effec-
tive in accelerating and compressing networks with many
convolutional layers, with only minor performance loss.

5.2.2 Quantizing the Fully-connected Layer

For demonstration, we ﬁrst compress parameters in a single
fully-connected layer. In CaffeNet, the ﬁrst fully-connected
layer possesses over 37 million parameters (9216 × 4096),
more than 60% of whole network parameters. Our Q-CNN
approach is adopted to quantize this layer and the results are
as reported in Table 6. The performance loss of our Q-CNN
model is negligible (within 0.4%), which is much smaller
than baseline methods (DPP and SVD). Furthermore, error
correction is effective in preserving the classiﬁcation accu-
racy, especially under a higher compression rate.

Now we evaluate our approach’s performance for com-
pressing all the fully-connected layers in CaffeNet in Ta-
ble 7. The third layer is actually the combination of 1000
classiﬁers, and is more critical to the classiﬁcation accuracy.
Hence, we adopt a much more ﬁne-grained hyper-parameter

2The compression effect of their approach was not explicitly discussed
in the paper; we estimate the compression rate based on their description.

DPP

Method

Table 6. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing the ﬁrst fully-connected layer
in CaffeNet, without ﬁne-tuning.
Compression
Para.
1.19×
-
1.47×
-
1.91×
-
2.75×
-
1.38×
-
2.77×
-
-
5.54×
11.08×
-
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32

Top-5 Err. ↑
-
-
-
-
-0.03%
0.07%
0.19%
0.86%
0.19%
0.28%
0.12%
0.16%
0.07%
0.03%
0.11%
0.12%

Top-1 Err. ↑
0.16%
1.76%
4.08%
9.68%
0.03%
0.07%
0.36%
1.23%
0.19%
0.35%
0.18%
0.28%
0.10%
0.18%
0.14%
0.16%

Q-CNN
(EC)

Q-CNN

SVD

setting (C′
s = 1, K = 16) for this layer. Although the
speed-up effect no longer exists, we can still achieve around
8× compression for the last layer.

SVD

Method

Table 7. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CaffeNet. Both SVD and DFC are ﬁne-tuned, while Q-CNN
and Q-CNN (EC) are not ﬁne-tuned.
Para.
-
-
-
-
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
1.26×
2.52×
1.79×
3.58×
13.96×
19.14×
15.25×
18.71×
13.96×
19.14×
15.25×
18.71×

Top-5 Err. ↑
-
-
-
-
0.29%
0.47%
0.34%
0.59%
0.30%
0.47%
0.27%
0.39%

Top-1 Err. ↑
0.14%
1.22%
-0.66%
0.31%
0.28%
0.70%
0.44%
0.75%
0.31%
0.59%
0.31%
0.57%

Q-CNN
(EC)

Q-CNN

DFC

From Table 7, we discover that with less than 1% drop in
accuracy, Q-CNN achieves high compression rates (12 ∼
20×), much larger than that of SVD3and DFC (< 4×).
Again, Q-CNN with error correction consistently outper-
forms the naive Q-CNN approach as adopted in [11].

3In Table 6, SVD means replacing the weighting matrix with the multi-
plication of two low-rank matrices; in Table 7, SVD means ﬁne-tuning the
network after the low-rank matrix decomposition.

5.2.3 Quantizing the Whole Network

So far, we have evaluated the performance of CNN models
with either convolutional or fully-connected layers quan-
tized. Now we demonstrate the quantization of the whole
network with a three-stage strategy. Firstly, we quantize all
the convolutional layers with error correction, while fully-
connected layers remain untouched. Secondly, we ﬁne-tune
fully-connected layers in the quantized network with the
ILSVRC-12 training set to restore the classiﬁcation accu-
racy. Finally, fully-connected layers in the ﬁne-tuned net-
work are quantized with error correction. We report the
performance of our Q-CNN models in Table 8.

Table 8. The speed-up/compression rates and the increase of top-
1/5 error rates for the whole CNN model. Particularly, for the
quantization of the third fully-connected layer in each network,
we let C ′
Model

s = 1 and K = 16.
Para.

Compression

Top-1/5 Err. ↑

Speed-up

AlexNet

CaffeNet

CNN-S

VGG-16

Conv.
8/128
8/128
8/128
8/128
8/128
8/128
6/128
6/128

FCnt.
3/32
4/32
3/32
4/32
3/32
4/32
3/32
4/32

4.05×
4.15×
4.04×
4.14×
5.69×
5.78×
4.05×
4.06×

15.40×
18.76×
15.40×
18.76×
16.32×
20.16×
16.55×
20.34×

1.38% / 0.84%
1.46% / 0.97%
1.43% / 0.99%
1.54% / 1.12%
1.48% / 0.81%
1.64% / 0.85%
1.22% / 0.53%
1.35% / 0.58%

For convolutional layers, we let C′

s = 8 and K = 128
for AlexNet, CaffeNet, and CNN-S, and let C′
s = 6 and
K = 128 for VGG-16, to ensure roughly 4 ∼ 6× speed-
up for each network. Then we vary the hyper-parameter
settings in fully-connected layers for different compression
levels. For the former two networks, we achieve 18× com-
pression with about 1% loss in the top-5 classiﬁcation accu-
racy. For CNN-S, we achieve 5.78× speed-up and 20.16×
compression, while the top-5 classiﬁcation accuracy drop is
merely 0.85%. The result on VGG-16 is even more encour-
aging: with 4.06× speed-up and 20.34×, the increase of
top-5 error rate is only 0.58%. Hence, our proposed Q-CNN
framework can improve the efﬁciency of convolutional net-
works with minor performance loss, which is acceptable in
many applications.

5.3. Results on Mobile Devices

We have developed an Android application to fulﬁll
CNN-based image classiﬁcation on mobile devices, based
on our Q-CNN framework. The experiments are carried
out on a Huawei R(cid:13) Mate 7 smartphone, equipped with an
1.8GHz Kirin 925 CPU. The test-phase computation is car-
ried out on a single CPU core, without GPU acceleration.

In Table 9, we compare the computation efﬁciency and
classiﬁcation accuracy of the original and quantized CNN
models. Our Q-CNN framework achieves 3× speed-up for
AlexNet, and 4× speed-up for CNN-S. What’s more, we
compress the storage consumption by 20 ×, and the re-

Table 9. Comparison on the time, storage, memory consumption,
and top-5 classiﬁcation error rates of the original and quantized
AlexNet and CNN-S.
Model

AlexNet

CNN-S

CNN
Q-CNN
CNN
Q-CNN

Time
2.93s
0.95s
10.58s
2.61s

Storage
232.56MB
12.60MB
392.57MB
20.13MB

Memory
264.74MB
74.65MB
468.90MB
129.49MB

Top-5 Err.
19.74%
20.70%
15.82%
16.68%

quired run-time memory is only one quarter of the original
model. At the same time, the loss in the top-5 classiﬁcation
accuracy is no more than 1%. Therefore, our proposed ap-
proach improves the run-time efﬁciency in multiple aspects,
making the deployment of CNN models become tractable
on mobile platforms.

5.4. Theoretical vs. Realistic Speed-up

In Table 10, we compare the theoretical and realistic
speed-up on AlexNet. The BLAS [29] library is used in
Caffe [15] to accelerate the matrix multiplication in con-
volutional and fully-connected layers. However, it may not
always be an option for mobile devices. Therefore, we mea-
sure the run-time speed under two settings, i.e. with BLAS
enabled or disabled. The realistic speed-up is slightly lower
with BLAS on, indicating that Q-CNN does not beneﬁt as
much from BLAS as that of CNN. Other optimization tech-
niques, e.g. SIMD, SSE, and AVX [4], may further improve
our realistic speed-up, and shall be explored in the future.

Table 10. Comparison on the theoretical and realistic speed-up on
AlexNet (CPU only, single-threaded). Here we use the ATLAS
library, which is the default BLAS choice in Caffe [15].

BLAS

Off
On

FLOPs

CNN

Q-CNN

7.29e+8

1.75e+8

Time (ms)

Speed-up

CNN
321.10
167.794

Q-CNN
75.62
55.35

Theo.

4.15×

Real.
4.25×
3.03×

6. Conclusion

In this paper, we propose a uniﬁed framework to si-
multaneously accelerate and compress convolutional neural
networks. We quantize network parameters to enable ef-
ﬁcient test-phase computation. Extensive experiments are
conducted on MNIST and ILSVRC-12, and our approach
achieves outstanding speed-up and compression rates, with
only negligible loss in the classiﬁcation accuracy.

7. Acknowledgement

This work was supported in part by National Natural Sci-
ence Foundation of China (Grant No. 61332016), and 863
program (Grant No. 2014AA015105).

4This is Caffe’s run-time speed. The code for the other three settings is

on https://github.com/jiaxiang-wu/quantized-cnn.

[22] C. Leng, J. Wu, J. Cheng, X. Zhang, and H. Lu. Hashing for dis-
In International Conference on Machine Learning

tributed data.
(ICML), pages 1642–1650, 2015. 2

[23] G. Levi and T. Hassncer. Age and gender classiﬁcation using convo-
lutional neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), pages 34–42, 2015.
1

[24] C. Li, Q. Liu, J. Liu, and H. Lu. Learning ordinal discriminative
features for age estimation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2570–2577, 2012. 1
[25] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks.
CoRR, abs/1603.05279, 2016. 5

[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei. Imagenet large scale visual recognition challenge. Inter-
national Journal of Computer Vision (IJCV), pages 1–42, 2015. 2,
5

[27] K. Simonyan and A. Zisserman. Very deep convolutional networks
In International Conference on

for large-scale image recognition.
Learning Representations (ICLR), 2015. 1, 2, 6

[28] S. Srinivas and R. V. Babu. Data-free parameter pruning for deep
In British Machine Vision Conference (BMVC),

neural networks.
pages 31.1–31.12, 2015. 1, 5

[29] R. C. Whaley and A. Petitet. Minimizing development and mainte-
nance costs in supporting persistently optimized BLAS. Software:
Practice and Experience, 35(2):101–121, Feb 2005. 8

[30] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. CoRR, abs/1412.7149,
2014. 1, 5

[31] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep
convolutional networks for classiﬁcation and detection. CoRR,
abs/1505.06798, 2015. 1, 5, 7

[32] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient and accurate
approximations of nonlinear convolutional networks. In IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), pages
1984–1992, 2015. 1, 5

References

[1] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return
of the devil in the details: Delving deep into convolutional nets. In
British Machine Vision Conference (BMVC), 2014. 1, 2, 6

[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In International
Conference on Machine Learning (ICML), pages 2285–2294, 2015.
1, 2, 5, 6

[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmid-
huber. High-performance neural networks for visual object classiﬁ-
cation. CoRR, abs/1102.0183, 2011. 1, 5, 6

[4] I. Corporation. Intel architecture instruction set extensions program-
ming reference. Technical report, Intel Corporation, Feb 2016. 8
[5] M. Courbariaux, Y. Bengio, and J. David. Training deep neural net-
In International Confer-

works with low precision multiplications.
ence on Learning Representations (ICLR), 2015. 5

[6] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas.
Predicting parameters in deep learning. In Advances in Neural In-
formation Processing Systems (NIPS), pages 2148–2156, 2013. 5,
6

[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Ex-
ploiting linear structure within convolutional networks for efﬁcient
evaluation. In Advances in Neural Information Processing Systems
(NIPS), pages 1269–1277, 2014. 1, 5

[8] J. D. Geoffrey Hinton, Oriol Vinyals. Distilling the knowledge in a

neural network. CoRR, abs/1503.02531, 2015. 5, 6

[9] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 1
[10] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 580–587, 2014. 1

[11] Y. Gong, L. Liu, M. Yang, and L. D. Bourdev. Compressing
deep convolutional networks using vector quantization. CoRR,
abs/1412.6115, 2014. 1, 2, 5, 7

[12] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding. CoRR, abs/1510.00149, 2015. 1, 2, 5

[13] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolu-
tional neural networks with low rank expansions. In British Machine
Vision Conference (BMVC), 2014. 1

[14] H. Jegou, M. Douze, and C. Schmid. Product quantization for near-
est neighbor search. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 33(1):117–128, Jan 2011. 2

[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. CoRR, abs/1408.5093, 2014. 2, 6, 8
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 1106–1114, 2012. 1,
2, 6

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lem-
pitsky. Speeding-up convolutional neural networks using ﬁne-tuned
cp-decomposition. In International Conference on Learning Repre-
sentations (ICLR), 2015. 1, 5, 6

[18] V. Lebedev and V. S. Lempitsky. Fast convnets using group-wise

brain damage. CoRR, abs/1506.02515, 2015. 1, 5, 6

[19] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. E. Hubbard, and L. D. Jackel. Backpropagation applied to hand-
written zip code recognition. Neural Computation, 1(4):541–551,
1989. 1

[20] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998. 2, 5

[21] C. Leng, J. Wu, J. Cheng, X. Bai, and H. Lu. Online sketching hash-
ing. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2503–2511, 2015. 2

Appendix A: Additional Results

In the submission, we report the performance after quan-
tizing all the convolutional layers in AlexNet, and quan-
tizing all the full-connected layers in CaffeNet. Here, we
present experimental results for some other settings.

Quantizing Convolutional Layers in CaffeNet

We quantize all the convolutional layers in CaffeNet, and
the results are as demonstrated in Table 11. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.15% and 0.75%, compared to the original
CaffeNet.

Table 11. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CaffeNet, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.32×
4.32×
3.71×
4.27×
3.32×
4.32×
3.71×
4.27×

Top-1 Err. ↑
18.69%
32.84%
20.08%
35.48%
1.22%
2.44%
1.57%
2.30%

Top-5 Err. ↑
16.73%
33.55%
18.31%
37.82%
0.97%
1.83%
1.12%
1.71%

Quantizing Convolutional Layers in CNN-S

We quantize all the convolutional layers in CNN-S, and
the results are as demonstrated in Table 12. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.24% and 0.63%, compared to the original
CNN-S.

Table 12. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CNN-S, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.69×
5.17×
4.78×
5.92×
3.69×
5.17×
4.78×
5.92×

Top-1 Err. ↑
19.87%
45.74%
27.86%
46.18%
1.60%
3.49%
2.07%
3.42%

Top-5 Err. ↑
16.77%
48.67%
25.09%
50.26%
0.92%
2.32%
1.32%
2.17%

Quantizing Fully-connected Layers in AlexNet

We quantize all the fully-connected layers in AlexNet,

and the results are as demonstrated in Table 13.

Quantizing Fully-connected Layers in CNN-S

We quantize all the fully-connected layers in CNN-S,

Method

Table 13. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in AlexNet, without ﬁne-tuning.
Compression
Para.
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32

Top-5 Err. ↑
0.27%
0.64%
0.33%
0.69%
0.20%
0.22%
0.21%
0.38%

Top-1 Err. ↑
0.25%
0.77%
0.54%
0.71%
0.14%
0.40%
0.40%
0.46%

Q-CNN
(EC)

Q-CNN

Method

Table 14. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CNN-S, without ﬁne-tuning.
Para.
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
14.37×
20.15×
15.79×
19.66×
14.37×
20.15×
15.79×
19.66×

Top-5 Err. ↑
0.07%
0.22%
0.11%
0.27%
0.14%
0.24%
0.11%
0.27%

Top-1 Err. ↑
0.22%
0.45%
0.21%
0.35%
0.36%
0.43%
0.29%
0.56%

Q-CNN
(EC)

Q-CNN

Appendix B: Optimization in Section 3.3.2

Assume we have N images to learn the quantization of a
convolutional layer. For image In, we denote its input fea-
ture maps as Sn ∈ Rds×ds×Cs and response feature maps
as Tn ∈ Rdt×dt×Ct, where ds, dt are the spatial sizes and
Cs, Ct are the number of feature map channels. We use
ps and pt to denote the spatial location in the input and re-
sponse feature maps. The spatial location in the convolu-
tional kernels is denoted as pk.

To learn quantization with error correction for the con-

volutional layer, we attempt to optimize:

2

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
m

X
n,pt

X
(pk,ps)

(D(m)B(m)

pk )T S(m)

min
{D(m)},{B(m)
pk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(14)
where Dm is the m-th sub-codebook, and B(m)
is the cor-
pk
responding sub-codeword assignment indicator for the con-
volutional kernels at spatial location pk.

n,ps − Tn,pt

Similar to the fully-connected layer, we adopt a block co-
ordinate descent approach to solve this optimization prob-
lem. For the m-th subspace, we ﬁrstly deﬁne its residual
feature map as:

R(m)

n,pt = Tn,pt − X
(pk,ps)

X
m′6=m

(D(m′

)B(m′
pk

)

)T S(m′
)
n,ps

(15)

and the results are as demonstrated in Table 14.

and then the optimization in the m-th subspace can be re-

formulated as:

min
D(m),{B(m)
pk }

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(16)
Update D(m). With the assignment indicator {B(m)
pk }

n,ps − R(m)
n,pt

pk )T S(m)

(D(m)B(m)

X
(pk,ps)

X
n,pt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ﬁxed, we let:

Lk,pk = {ct|B(m)

pk (k, ct) = 1}

(17)

We greedily update each sub-codeword in the m-th sub-
codebook D(m) in a sequential style. For the k-th sub-
codeword, we compute the corresponding residual feature
map as:

Q(m)

n,pt,k(ct) = R(m)

n,pt (ct) − X
(pk ,ps)

X
k′6=k

X
ct∈Lk′,pk

D(m)T
k′

S(m)
n,ps

(18)

and then we can alternatively optimize:

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

min
D(m)
k

D(m)T

X
(pk,ps)

2
(cid:13)
(cid:13)
n,pt,k(ct)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(19)
which can be transformed into a least square problem. By
solving it, we can update the k-th sub-codeword.

n,ps − Q(m)
S(m)

X
ct∈Lk,pk

Update {B(m)

pk }. We greedily update the sub-codeword
assignment at each spatial location in the convolutional ker-
nels in a sequential style. For the spatial location pk, we
compute the corresponding residual feature map as:

P (m)
n,pt,pk = R(m)

(D(m)B(m)

p′
k

)T S(m)
n,p′
s

(20)

n,pt − X
(p′
k,p′
s)
pk6=pk

and then the optimization can be re-written as:

min
B(m)
pk

X
n,pt

(D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

pk )T S(m)

n,ps − P (m)

2
n,pt,pk (cid:13)
(cid:13)
F
(cid:13)

(21)

Since B(m)
pk ∈ {0, 1}K is an indicator vector (only one non-
zero entry), we can exhaustively try all sub-codewords and
select the optimal one that minimize the objective function.

6
1
0
2
 
y
a
M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
3
7
4
6
0
.
2
1
5
1
:
v
i
X
r
a

Quantized Convolutional Neural Networks for Mobile Devices

Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng
National Laboratory of Patter Recognition
Institute of Automation, Chinese Academy of Sciences
{jiaxiang.wu, cong.leng, yuhang.wang, qinghao.hu, jcheng}@nlpr.ia.ac.cn

Abstract

Original

Q-CNN

Time Consumption (s)

Storage Consumption (MB)

Recently, convolutional neural networks (CNN) have
demonstrated impressive performance in various computer
vision tasks. However, high performance hardware is typ-
ically indispensable for the application of CNN models
due to the high computation complexity, which prohibits
their further extensions. In this paper, we propose an efﬁ-
cient framework, namely Quantized CNN, to simultaneously
speed-up the computation and reduce the storage and mem-
ory overhead of CNN models. Both ﬁlter kernels in con-
volutional layers and weighting matrices in fully-connected
layers are quantized, aiming at minimizing the estimation
error of each layer’s response. Extensive experiments on
the ILSVRC-12 benchmark demonstrate 4 ∼ 6× speed-up
and 15 ∼ 20× compression with merely one percentage
loss of classiﬁcation accuracy. With our quantized CNN
model, even mobile devices can accurately classify images
within one second.

15

10

5

0

500

400

300

200

100

0

AlexNet

CNN-S

AlexNet

CNN-S

Memory Consumption (MB)

Top-5 Error Rate (%)

AlexNet

CNN-S

AlexNet

CNN-S

Figure 1. Comparison on the efﬁciency and classiﬁcation accuracy
between the original and quantized AlexNet [16] and CNN-S [1]
on a Huawei R(cid:13) Mate 7 smartphone.

400

300

200

100

0

25

20

15

10

5

0

1. Introduction

In recent years, we have witnessed the great success
of convolutional neural networks (CNN) [19] in a wide
range of visual applications, including image classiﬁcation
[16, 27], object detection [10, 9], age estimation [24, 23],
etc. This success mainly comes from deeper network ar-
chitectures as well as the tremendous training data. How-
ever, as the network grows deeper, the model complexity is
also increasing exponentially in both the training and testing
stages, which leads to the very high demand in the computa-
tion ability. For instance, the 8-layer AlexNet [16] involves
60M parameters and requires over 729M FLOPs1to classify
a single image. Although the training stage can be ofﬂine
carried out on high performance clusters with GPU acceler-
ation, the testing computation cost may be unaffordable for
common personal computers and mobile devices. Due to
the limited computation ability and memory space, mobile
devices are almost intractable to run deep convolutional net-
works. Therefore, it is crucial to accelerate the computation

and compress the memory consumption for CNN models.

For most CNNs, convolutional layers are the most time-
consuming part, while fully-connected layers involve mas-
sive network parameters. Due to the intrinsical differ-
ence between them, existing works usually focus on im-
proving the efﬁciency for either convolutional layers or
fully-connected layers.
In [7, 13, 32, 31, 18, 17], low-
rank approximation or tensor decomposition is adopted to
speed-up convolutional layers. On the other hand, param-
eter compression in fully-connected layers is explored in
[3, 7, 11, 30, 2, 12, 28]. Overall, the above-mentioned al-
gorithms are able to achieve faster speed or less storage.
However, few of them can achieve signiﬁcant acceleration
and compression simultaneously for the whole network.

In this paper, we propose a uniﬁed framework for con-
volutional networks, namely Quantized CNN (Q-CNN), to
simultaneously accelerate and compress CNN models with

1FLOPs: number of FLoating-point OPerations required to classify one

image with the convolutional network.

1

only minor performance degradation. With network pa-
rameters quantized, the response of both convolutional and
fully-connected layers can be efﬁciently estimated via the
approximate inner product computation. We minimize the
estimation error of each layer’s response during parameter
quantization, which can better preserve the model perfor-
mance. In order to suppress the accumulative error while
quantizing multiple layers, an effective training scheme is
introduced to take previous estimation error into consider-
ation. Our Q-CNN model enables fast test-phase compu-
tation, and the storage and memory consumption are also
signiﬁcantly reduced.

We evaluate our Q-CNN framework for image classi-
ﬁcation on two benchmarks, MNIST [20] and ILSVRC-
12 [26]. For MNIST, our Q-CNN approach achieves over
12× compression for two neural networks (no convolu-
tion), with lower accuracy loss than several baseline meth-
ods. For ILSVRC-12, we attempt to improve the test-phase
efﬁciency of four convolutional networks: AlexNet [16],
CaffeNet [15], CNN-S [1], and VGG-16 [27]. Generally,
Q-CNN achieves 4× acceleration and 15× compression
(sometimes higher) for each network, with less than 1%
drop in the top-5 classiﬁcation accuracy. Moreover, we im-
plement the quantized CNN model on mobile devices, and
dramatically improve the test-phase efﬁciency, as depicted
in Figure 1. The main contributions of this paper can be
summarized as follows:

• We propose a uniﬁed Q-CNN framework to acceler-
ate and compress convolutional networks. We demon-
strate that better quantization can be learned by mini-
mizing the estimation error of each layer’s response.
• We propose an effective training scheme to suppress
the accumulative error while quantizing the whole con-
volutional network.

• Our Q-CNN framework achieves 4 ∼ 6× speed-up
and 15 ∼ 20× compression, while the classiﬁcation
accuracy loss is within one percentage. Moreover, the
quantized CNN model can be implemented on mobile
devices and classify an image within one second.

2. Preliminary

During the test phase of convolutional networks, the
computation overhead is dominated by convolutional lay-
ers; meanwhile, the majority of network parameters are
stored in fully-connected layers. Therefore, for better test-
phase efﬁciency, it is critical to speed-up the convolution
computation and compress parameters in fully-connected
layers.

Our observation is that the forward-passing process of
both convolutional and fully-connected layers is dominated
by the computation of inner products. More formally, we
consider a convolutional layer with input feature maps S ∈

Rds×ds×Cs and response feature maps T ∈ Rdt×dt×Ct,
where ds, dt are the spatial sizes and Cs, Ct are the number
of feature map channels. The response at the 2-D spatial
position pt in the ct-th response feature map is computed
as:

Tpt (ct) = X(pk,ps)

hWct,pk , Spsi

(1)

where Wct ∈ Rdk×dk×Cs is the ct-th convolutional kernel
and dk is the kernel size. We use ps and pk to denote the
2-D spatial positions in the input feature maps and convolu-
tional kernels, and both Wct,pk and Sps are Cs-dimensional
vectors. The layer response is the sum of inner products at
all positions within the dk × dk receptive ﬁeld in the input
feature maps.

Similarly, for a fully-connected layer, we have:

T (ct) = hWct , Si

(2)

where S ∈ RCs and T ∈ RCt are the layer input and layer
response, respectively, and Wct ∈ RCs is the weighting
vector for the ct-th neuron of this layer.

Product quantization [14] is widely used in approximate
nearest neighbor search, demonstrating better performance
than hashing-based methods [21, 22]. The idea is to de-
compose the feature space as the Cartesian product of mul-
tiple subspaces, and then learn sub-codebooks for each sub-
space. A vector is represented by the concatenation of sub-
codewords for efﬁcient distance computation and storage.

In this paper, we leverage product quantization to imple-
ment the efﬁcient inner product computation. Let us con-
sider the inner product computation between x, y ∈ RD. At
ﬁrst, both x and y are split into M sub-vectors, denoted as
x(m) and y(m). Afterwards, each x(m) is quantized with a
sub-codeword from the m-th sub-codebook, then we have

hy, xi = Xm

hy(m), x(m)i ≈ Xm

hy(m), c(m)
km i

(3)

which transforms the O(D) inner product computation to
M addition operations (M ≤ D), if the inner products be-
tween each sub-vector y(m) and all the sub-codewords in
the m-th sub-codebook have been computed in advance.

Quantization-based approaches have been explored in
several works [11, 2, 12]. These approaches mostly fo-
cus on compressing parameters in fully-connected layers
[11, 2], and none of them can provide acceleration for the
test-phase computation. Furthermore, [11, 12] require the
network parameters to be re-constructed during the test-
phase, which limit the compression to disk storage instead
of memory consumption. On the contrary, our approach
offers simultaneous acceleration and compression for both
convolutional and fully-connected layers, and can reduce
the run-time memory consumption dramatically.

3. Quantized CNN

In this section, we present our approach for accelerating
and compressing convolutional networks. Firstly, we intro-
duce an efﬁcient test-phase computation process with the
network parameters quantized. Secondly, we demonstrate
that better quantization can be learned by directly minimiz-
ing the estimation error of each layer’s response. Finally,
we analyze the computation complexity of our quantized
CNN model.

3.1. Quantizing the Fully-connected Layer

For a fully-connected layer, we denote its weighting ma-
trix as W ∈ RCs×Ct, where Cs and Ct are the dimensions
of the layer input and response, respectively. The weighting
vector Wct is the ct-th column vector in W .

We evenly split the Cs-dimensional space (where Wct
lies in) into M subspaces, each of C′
s = Cs/M dimen-
sions. Each Wct is then decomposed into M sub-vectors,
denoted as W (m)
. A sub-codebook can be learned for each
subspace after gathering all the sub-vectors within this sub-
space. Formally, for the m-th subspace, we optimize:

ct

min
D(m),B(m)

2
D(m)B(m) − W (m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
s×K, B(m) ∈ {0, 1}K×Ct

s.t. D(m) ∈ RC ′

(4)

where W (m) ∈ RC ′
s×Ct consists of the m-th sub-vectors
of all weighting vectors. The sub-codebook D(m) contains
K sub-codewords, and each column in B(m) is an indica-
tor vector (only one non-zero entry), specifying which sub-
codeword is used to quantize the corresponding sub-vector.
The optimization can be solved via k-means clustering.
The layer response is approximately computed as:
, S(m)i ≈ Xm
= Xm

T (ct) = Xm

km(ct), S(m)i

hD(m)B(m)

hW (m)
ct

, S(m)i

hD(m)

(5)

ct

ct

ct

where B(m)
is the ct-th column vector in B(m), and S(m) is
the m-th sub-vector of the layer input. km(ct) is the index
of the sub-codeword used to quantize the sub-vector W (m)
.
In Figure 2, we depict the parameter quantization and
test-phase computation process of the fully-connected layer.
By decomposing the weighting matrix into M sub-matrices,
M sub-codebooks can be learned, one per subspace. During
the test-phase, the layer input is split into M sub-vectors,
denoted as S(m). For each subspace, we compute the inner
products between S(m) and every sub-codeword in D(m),
and store the results in a look-up table. Afterwards, only M
addition operations are required to compute each response.
As a result, the overall time complexity can be reduced from
O(CsCt) to O(CsK + CtM ). On the other hand, only
sub-codebooks and quantization indices need to be stored,
which can dramatically reduce the storage consumption.

Figure 2. The parameter quantization and test-phase computation
process of the fully-connected layer.

3.2. Quantizing the Convolutional Layer

Unlike the 1-D weighting vector in the fully-connected
layer, each convolutional kernel is a 3-dimensional tensor:
Wct ∈ Rdk×dk×Cs. Before quantization, we need to deter-
mine how to split it into sub-vectors, i.e. apply subspace
splitting to which dimension. During the test phase, the in-
put feature maps are traversed by each convolutional kernel
with a sliding window in the spatial domain. Since these
sliding windows are partially overlapped, we split each con-
volutional kernel along the dimension of feature map chan-
nels, so that the pre-computed inner products can be re-
used at multiple spatial locations. Speciﬁcally, we learn the
quantization in each subspace by:

min
Xpk
D(m),{B(m)
pk }
s.t. D(m) ∈ RC ′

D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

s×K, B(m)

2
pk − W (m)
pk (cid:13)
(cid:13)
F
(cid:13)
pk ∈ {0, 1}K×Ct

(6)

pk ∈ RC ′

where W (m)
s×Ct contains the m-th sub-vectors of
all convolutional kernels at position pk. The optimization
can also be solved by k-means clustering in each subspace.
With the convolutional kernels quantized, we approxi-

mately compute the response feature maps by:

Tpt(ct) = X(pk,ps) Xm
≈ X(pk,ps) Xm
= X(pk,ps) Xm

hW (m)

ct,pk , S(m)
ps i

hD(m)B(m)

ct,pk , S(m)
ps i

(7)

hD(m)

km(ct,pk), S(m)
ps i

where S(m)
is the m-th sub-vector at position ps in the in-
ps
put feature maps, and km(ct, pk) is the index of the sub-
codeword to quantize the m-th sub-vector at position pk in
the ct-th convolutional kernel.

Similar to the fully-connected layer, we pre-compute the
look-up tables of inner products with the input feature maps.
Then, the response feature maps are approximately com-
puted with (7), and both the time and storage complexity
can be greatly reduced.

and the above optimization can be solved by alternatively
updating the sub-codebook and sub-codeword assignment.
Update D(m). We ﬁx the sub-codeword assignment
B(m), and deﬁne Lk = {ct|B(m)(k, ct) = 1}. The opti-
mization in (10) can be re-formulated as:

3.3. Quantization with Error Correction

So far, we have presented an intuitive approach to quan-
tize parameters and improve the test-phase efﬁciency of
convolutional networks. However, there are still two crit-
ical drawbacks. First, minimizing the quantization error
of model parameters does not necessarily give the optimal
quantized network for the classiﬁcation accuracy. In con-
trast, minimizing the estimation error of each layer’s re-
sponse is more closely related to the network’s classiﬁca-
tion performance. Second, the quantization of one layer is
independent of others, which may lead to the accumulation
of error when quantizing multiple layers. The estimation
error of the network’s ﬁnal response is very likely to be
quickly accumulated, since the error introduced by the pre-
vious quantized layers will also affect the following layers.
To overcome these two limitations, we introduce the idea
of error correction into the quantization of network param-
eters. This improved quantization approach directly min-
imizes the estimation error of the response at each layer,
and can compensate the error introduced by previous lay-
ers. With the error correction scheme, we can quantize the
network with much less performance degradation than the
original quantization method.

3.3.1 Error Correction for the Fully-connected Layer

Suppose we have N images to learn the quantization of a
fully-connected layer, and the layer input and response of
image In are denoted as Sn and Tn. In order to minimize
the estimation error of the layer response, we optimize:

Xn

min
{D(m)},{B(m)}

Tn − Xm
(cid:13)
(cid:13)
(cid:13)

2
(D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)
(8)
where the ﬁrst term in the Frobenius norm is the desired
layer response, and the second term is the approximated
layer response computed via the quantized parameters.

A block coordinate descent approach can be applied to
minimize this objective function. For the m-th subspace, its
residual error is deﬁned as:

R(m)

n = Tn − Xm′6=m

(D(m′

)B(m′

))T S(m′

n

)

(9)

and then we attempt to minimize the residual error of this
subspace, which is:

min

D(m),B(m) Xn

2
n − (D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)

R(m)
(cid:13)
(cid:13)
(cid:13)

(10)

min
{D(m)
k

}

Xn,k Xct∈Lk

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(11)

which implies that the optimization over one sub-codeword
does not affect other sub-codewords. Hence, for each sub-
codeword, we construct a least square problem from (11) to
update it.

Update B(m). With the sub-codebook D(m) ﬁxed, it
is easy to discover that the optimization of each column in
B(m) is mutually independent. For the ct-th column, its
optimal sub-codeword assignment is given by:

k∗
m(ct) = arg min

k Xn

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(12)

3.3.2 Error Correction for the Convolutional Layer

We adopt the similar idea to minimize the estimation error
of the convolutional layer’s response feature maps, that is:

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

(D(m)B(m)

min
{D(m)},{B(m)
pk }

Tn,pt − X
(pk ,ps)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(13)
The optimization also can be solved by block coordinate
descent. More details on solving this optimization can be
found in the supplementary material.

pk )T S(m)
n,ps

X
m

3.3.3 Error Correction for Multiple Layers

The above quantization method can be sequentially applied
to each layer in the CNN model. One concern is that the
estimation error of layer response caused by the previous
layers will be accumulated and affect the quantization of
the following layers. Here, we propose an effective training
scheme to address this issue.

We consider the quantization of a speciﬁc layer, assum-
ing its previous layers have already been quantized. The
optimization of parameter quantization is based on the layer
input and response of a group of training images. To quan-
tize this layer, we take the layer input in the quantized net-
work as {Sn}, and the layer response in the original net-
work (not quantized) as {Tn} in Eq. (8) and (13). In this
way, the optimization is guided by the actual input in the
quantized network and the desired response in the original
network. The accumulative error introduced by the previ-
ous layers is explicitly taken into consideration during op-
timization. In consequence, this training scheme can effec-
tively suppress the accumulative error for the quantization
of multiple layers.

Another possible solution is to adopt back-propagation
to jointly update the sub-codebooks and sub-codeword as-
signments in all quantized layers. However, since the sub-
codeword assignments are discrete, the gradient-based op-
timization can be quite difﬁcult, if not entirely impossible.
Therefore, back-propagation is not adopted here, but could
be a promising extension for future work.

3.4. Computation Complexity

Now we analyze the test-phase computation complex-
ity of convolutional and fully-connected layers, with or
without parameter quantization. For our proposed Q-CNN
model, the forward-passing through each layer mainly con-
sists of two procedures: pre-computation of inner products,
and approximate computation of layer response. Both sub-
codebooks and sub-codeword assignments are stored for the
test-phase computation. We report the detailed comparison
on the computation and storage overhead in Table 1.

Table 1. Comparison on the computation and storage overhead of
convolutional and fully-connected layers.

FLOPs

Bytes

Conv.

FCnt.

Conv.

FCnt.

kM

kCs
t Ctd2

t Ctd2
d2
CNN
sCsK + d2
d2
Q-CNN
CNN
CsCt
Q-CNN
CsK + CtM
4d2
CNN
kCsCt
Q-CNN 4CsK + 1
8 d2
kM Ct log2 K
CNN
4CsCt
Q-CNN
8 M Ct log2 K

4CsK + 1

As we can see from Table 1, the reduction in the compu-
tation and storage overhead largely depends on two hyper-
parameters, M (number of subspaces) and K (number of
sub-codewords in each subspace). Large values of M and
K lead to more ﬁne-grained quantization, but is less efﬁ-
cient in the computation and storage consumption. In prac-
tice, we can vary these two parameters to balance the trade-
off between the test-phase efﬁciency and accuracy loss of
the quantized CNN model.

4. Related Work

There have been a few attempts in accelerating the test-
phase computation of convolutional networks, and many are
inspired from the low-rank decomposition. Denton et al.
[7] presented a series of low-rank decomposition designs
for convolutional kernels. Similarly, CP-decomposition was
adopted in [17] to transform a convolutional layer into mul-
tiple layers with lower complexity. Zhang et al. [32, 31]
considered the subsequent nonlinear units while learning
the low-rank decomposition. [18] applied group-wise prun-
ing to the convolutional tensor to decompose it into the mul-
tiplications of thinned dense matrices. Recently, ﬁxed-point
based approaches are explored in [5, 25]. By representing

the connection weights (or even network activations) with
ﬁxed-point numbers, the computation can greatly beneﬁt
from hardware acceleration.

Another parallel research trend is to compress parame-
ters in fully-connected layers. Ciresan et al. [3] randomly
remove connection to reduce network parameters. Matrix
factorization was adopted in [6, 7] to decompose the weight-
ing matrix into two low-rank matrices, which demonstrated
that signiﬁcant redundancy did exist in network parameters.
Hinton et al. [8] proposed to use dark knowledge (the re-
sponse of a well-trained network) to guide the training of
a much smaller network, which was superior than directly
training. By exploring the similarity among neurons, Srini-
vas et al. [28] proposed a systematic way to remove redun-
dant neurons instead of network connections. In [30], mul-
tiple fully-connected layers were replaced by a single “Fast-
food” layer, which can be trained in an end-to-end style with
[2] randomly grouped
convolutional layers. Chen et al.
connection weights into hash buckets, and then ﬁne-tuned
the network with back-propagation. [12] combined prun-
ing, quantization, and Huffman coding to achieve higher
compression rate. Gong et al. [11] adopted vector quanti-
zation to compress the weighing matrix, which was actually
a special case of our approach (apply Q-CNN without error
correction to fully-connected layers only).

5. Experiments

In this section, we evaluate our quantized CNN frame-
work on two image classiﬁcation benchmarks, MNIST [20]
and ILSVRC-12 [26]. For the acceleration of convolutional
layers, we compare with:

• CPD [17]: CP-Decomposition;
• GBD [18]: Group-wise Brain Damage;
• LANR [31]: Low-rank Approximation of Non-linear

Responses.

and for the compression of fully-connected layers, we com-
pare with the following approaches:

• RER [3]: Random Edge Removal;
• LRD [6]: Low-Rank Decomposition;
• DK [8]: Dark Knowledge;
• HashNet [2]: Hashed Neural Nets;
• DPP [28]: Data-free Parameter Pruning;
• SVD [7]: Singular Value Decomposition;
• DFC [30]: Deep Fried Convnets.

For all above baselines, we use their reported results under
the same setting for fair comparison. We report the theo-
retical speed-up for more consistent results, since the real-
istic speed-up may be affected by various factors, e.g. CPU,
cache, and RAM. We compare the theoretical and realistic
speed-up in Section 5.4, and discuss the effect of adopting
the BLAS library for acceleration.

Our approaches are denoted as “Q-CNN” and “Q-CNN
(EC)”, where the latter one adopts error correction while the
former one does not. We implement the optimization pro-
cess of parameter quantization in MATLAB, and ﬁne-tune
the resulting network with Caffe [15]. Additional results of
our approach can be found in the supplementary material.

5.1. Results on MNIST

The MNIST dataset contains 70k images of hand-written
digits, 60k used for training and 10k for testing. To evalu-
ate the compression performance, we pre-train two neural
networks, one is 3-layer and another one is 5-layer, where
each hidden layer contains 1000 units. Different compres-
sion techniques are then adopted to compress these two net-
work, and the results are as depicted in Table 2.

Table 2. Comparison on the compression rates and classiﬁcation
error on MNIST, based on a 3-layer network (784-1000-10) and a
5-layer network (784-1000-1000-1000-10).

Method

Original
RER [3]
LRD [6]
DK [8]
HashNets [2]
Q-CNN
Q-CNN (EC)

3-layer

5-layer

Error
1.35%
2.19%
1.89%
1.71%
1.43%

Compr.
-
8×
8×
8×
8×

Compr.
-
8×
8×
8×
8×

Error
1.12%
1.24%
1.77%
1.26%
1.22%
12.1× 1.42% 13.4× 1.34%
12.1× 1.39% 13.4× 1.19%

s is determined once C′

In our Q-CNN framework, the trade-off between accu-
racy and efﬁciency is controlled by M (number of sub-
spaces) and K (number of sub-codewrods in each sub-
space). Since M = Cs/C′
s is given,
we tune (C′
s, K) to adjust the quantization precision. In Ta-
ble 2, we set the hyper-parameters as C′
s = 4 and K = 32.
From Table 2, we observe that our Q-CNN (EC) ap-
proach offers higher compression rates with less perfor-
mance degradation than all baselines for both networks.
The error correction scheme is effective in reducing the ac-
curacy loss, especially for deeper networks (5-layer). Also,
we ﬁnd the performance of both Q-CNN and Q-CNN (EC)
quite stable, as the standard deviation of ﬁve random runs is
merely 0.05%. Therefore, we report the single-run perfor-
mance in the remaining experiments.

5.2. Results on ILSVRC-12

The ILSVRC-12 benchmark consists of over one million
training images drawn from 1000 categories, and a disjoint
validation set of 50k images. We report both the top-1 and
top-5 classiﬁcation error rates on the validation set, using
single-view testing (central patch only).

We demonstrate our approach on four convolutional net-
works: AlexNet [16], CaffeNet [15], CNN-S [1], and VGG-

16 [27]. The ﬁrst two models have been adopted in several
related works, and therefore are included for comparison.
CNN-S and VGG-16 use a either wider or deeper structure
for better classiﬁcation accuracy, and are included here to
prove the scalability of our approach. We compare all these
networks’ computation and storage overhead in Table 3, to-
gether with their classiﬁcation error rates on ILSVRC-12.

Table 3. Comparison on the test-phase computation overhead
(FLOPs), storage consumption (Bytes), and classiﬁcation error
rates (Top-1/5 Err.) of AlexNet, CaffeNet, CNN-S, and VGG-16.
Bytes
2.44e+8
2.44e+8
4.12e+8
5.53e+8

Top-5 Err.
19.74%
19.59%
15.82%
10.05%

Top-1 Err.
42.78%
42.53%
37.31%
28.89%

FLOPs
7.29e+8
7.27e+8
2.94e+9
1.55e+10

Model
AlexNet
CaffeNet
CNN-S
VGG-16

5.2.1 Quantizing the Convolutional Layer

To begin with, we quantize the second convolutional layer
of AlexNet, which is the most time-consuming layer during
the test-phase. In Table 4, we report the performance un-
der several (C′
s, K) settings, comparing with two baseline
methods, CPD [17] and GBD [18].

Table 4. Comparison on the speed-up rates and the increase of top-
1/5 error rates for accelerating the second convolutional layer in
AlexNet, with or without ﬁne-tuning (FT). The hyper-parameters
of Q-CNN, C ′

Method

CPD

GBD

Q-CNN

Q-CNN
(EC)

Para.

Speed-up

s and K, are as speciﬁed in the “Para.” column.
Top-5 Err. ↑
Top-1 Err. ↑
FT
FT
0.44%
-
1.22%
-
18.63%
-
-
0.11%
-
0.43%
-
1.13%
1.37%
1.63%
2.27%
2.90%
1.28%
1.57%
2.66%
2.91%
0.17%
0.20%
0.40%
0.39%
0.21%
0.11%
0.31%
0.33%

No FT
0.94%
3.20%
69.06%
-
-
-
8.97%
14.71%
9.10%
18.05%
0.27%
0.50%
0.34%
0.50%

No FT
-
-
-
12.43%
21.93%
48.33%
10.55%
15.93%
10.62%
18.84%
0.35%
0.64%
0.27%
0.55%

3.19×
4.52×
6.51×
3.33×
5.00×
10.00×
3.70×
5.36×
4.84×
6.06×
3.70×
5.36×
4.84×
6.06×

-
-
-
-
-
-
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

From Table 4, we discover that with a large speed-up
rate (over 4×), the performance loss of both CPD and GBD
become severe, especially before ﬁne-tuning. The naive
parameter quantization method also suffers from the sim-
ilar problem. By incorporating the idea of error correction,
our Q-CNN model achieves up to 6× speed-up with merely
0.6% drop in accuracy, even without ﬁne-tuning. The ac-
curacy loss can be further reduced after ﬁne-tuning the sub-
sequent layers. Hence, it is more effective to minimize the
estimation error of each layer’s response than minimize the
quantization error of network parameters.

Next, we take one step further and attempt to speed-up
all the convolutional layers in AlexNet with Q-CNN (EC).

Table 5. Comparison on the speed-up/compression rates and the increase of top-1/5 error rates for accelerating all the convolutional layers
in AlexNet and VGG-16.

Model

Method

Para.

Speed-up Compression

AlexNet

Q-CNN
(EC)

VGG-16

LANR [31]
Q-CNN (EC)

4/64
6/64
6/128
8/128
-
6/128

3.32×
4.32×
3.71×
4.27×
4.00×
4.06×

10.58×
14.32×
10.27×
12.08×
2.73×
14.40×

Top-1 Err. ↑
FT
-
-

Top-5 Err. ↑
FT
-
-

No FT
0.94%
1.90%

No FT
1.33%
2.32%
1.44% 0.13% 1.16% 0.36%
2.25% 0.99% 1.64% 0.60%
0.95% 0.35%
3.04% 1.06% 1.83% 0.45%

-

-

We ﬁx the quantization hyper-parameters (C′
s, K) across all
layers. From Table 5, we observe that the loss in accuracy
grows mildly than the single-layer case. The speed-up rates
reported here are consistently smaller than those in Table 4,
since the acceleration effect is less signiﬁcant for some lay-
ers (i.e. “conv 4” and “conv 5”). For AlexNet, our Q-CNN
model (C′
s = 8, K = 128) can accelerate the computation
of all the convolutional layers by a factor of 4.27×, while
the increase in the top-1 and top-5 error rates are no more
than 2.5%. After ﬁne-tuning the remaining fully-connected
layers, the performance loss can be further reduced to less
than 1%.

In Table 5, we also report the comparison against LANR
[31] on VGG-16. For the similar speed-up rate (4×), their
approach outperforms ours in the top-5 classiﬁcation error
(an increase of 0.95% against 1.83%). After ﬁne-tuning, the
performance gap is narrowed down to 0.35% against 0.45%.
At the same time, our approach offers over 14× compres-
sion of parameters in convolutional layers, much larger than
theirs 2.7× compression2. Therefore, our approach is effec-
tive in accelerating and compressing networks with many
convolutional layers, with only minor performance loss.

5.2.2 Quantizing the Fully-connected Layer

For demonstration, we ﬁrst compress parameters in a single
fully-connected layer. In CaffeNet, the ﬁrst fully-connected
layer possesses over 37 million parameters (9216 × 4096),
more than 60% of whole network parameters. Our Q-CNN
approach is adopted to quantize this layer and the results are
as reported in Table 6. The performance loss of our Q-CNN
model is negligible (within 0.4%), which is much smaller
than baseline methods (DPP and SVD). Furthermore, error
correction is effective in preserving the classiﬁcation accu-
racy, especially under a higher compression rate.

Now we evaluate our approach’s performance for com-
pressing all the fully-connected layers in CaffeNet in Ta-
ble 7. The third layer is actually the combination of 1000
classiﬁers, and is more critical to the classiﬁcation accuracy.
Hence, we adopt a much more ﬁne-grained hyper-parameter

2The compression effect of their approach was not explicitly discussed
in the paper; we estimate the compression rate based on their description.

DPP

Method

Table 6. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing the ﬁrst fully-connected layer
in CaffeNet, without ﬁne-tuning.
Compression
Para.
1.19×
-
1.47×
-
1.91×
-
2.75×
-
1.38×
-
2.77×
-
-
5.54×
11.08×
-
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32

Top-5 Err. ↑
-
-
-
-
-0.03%
0.07%
0.19%
0.86%
0.19%
0.28%
0.12%
0.16%
0.07%
0.03%
0.11%
0.12%

Top-1 Err. ↑
0.16%
1.76%
4.08%
9.68%
0.03%
0.07%
0.36%
1.23%
0.19%
0.35%
0.18%
0.28%
0.10%
0.18%
0.14%
0.16%

Q-CNN
(EC)

Q-CNN

SVD

setting (C′
s = 1, K = 16) for this layer. Although the
speed-up effect no longer exists, we can still achieve around
8× compression for the last layer.

SVD

Method

Table 7. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CaffeNet. Both SVD and DFC are ﬁne-tuned, while Q-CNN
and Q-CNN (EC) are not ﬁne-tuned.
Para.
-
-
-
-
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
1.26×
2.52×
1.79×
3.58×
13.96×
19.14×
15.25×
18.71×
13.96×
19.14×
15.25×
18.71×

Top-5 Err. ↑
-
-
-
-
0.29%
0.47%
0.34%
0.59%
0.30%
0.47%
0.27%
0.39%

Top-1 Err. ↑
0.14%
1.22%
-0.66%
0.31%
0.28%
0.70%
0.44%
0.75%
0.31%
0.59%
0.31%
0.57%

Q-CNN
(EC)

Q-CNN

DFC

From Table 7, we discover that with less than 1% drop in
accuracy, Q-CNN achieves high compression rates (12 ∼
20×), much larger than that of SVD3and DFC (< 4×).
Again, Q-CNN with error correction consistently outper-
forms the naive Q-CNN approach as adopted in [11].

3In Table 6, SVD means replacing the weighting matrix with the multi-
plication of two low-rank matrices; in Table 7, SVD means ﬁne-tuning the
network after the low-rank matrix decomposition.

5.2.3 Quantizing the Whole Network

So far, we have evaluated the performance of CNN models
with either convolutional or fully-connected layers quan-
tized. Now we demonstrate the quantization of the whole
network with a three-stage strategy. Firstly, we quantize all
the convolutional layers with error correction, while fully-
connected layers remain untouched. Secondly, we ﬁne-tune
fully-connected layers in the quantized network with the
ILSVRC-12 training set to restore the classiﬁcation accu-
racy. Finally, fully-connected layers in the ﬁne-tuned net-
work are quantized with error correction. We report the
performance of our Q-CNN models in Table 8.

Table 8. The speed-up/compression rates and the increase of top-
1/5 error rates for the whole CNN model. Particularly, for the
quantization of the third fully-connected layer in each network,
we let C ′
Model

s = 1 and K = 16.
Para.

Compression

Top-1/5 Err. ↑

Speed-up

AlexNet

CaffeNet

CNN-S

VGG-16

Conv.
8/128
8/128
8/128
8/128
8/128
8/128
6/128
6/128

FCnt.
3/32
4/32
3/32
4/32
3/32
4/32
3/32
4/32

4.05×
4.15×
4.04×
4.14×
5.69×
5.78×
4.05×
4.06×

15.40×
18.76×
15.40×
18.76×
16.32×
20.16×
16.55×
20.34×

1.38% / 0.84%
1.46% / 0.97%
1.43% / 0.99%
1.54% / 1.12%
1.48% / 0.81%
1.64% / 0.85%
1.22% / 0.53%
1.35% / 0.58%

For convolutional layers, we let C′

s = 8 and K = 128
for AlexNet, CaffeNet, and CNN-S, and let C′
s = 6 and
K = 128 for VGG-16, to ensure roughly 4 ∼ 6× speed-
up for each network. Then we vary the hyper-parameter
settings in fully-connected layers for different compression
levels. For the former two networks, we achieve 18× com-
pression with about 1% loss in the top-5 classiﬁcation accu-
racy. For CNN-S, we achieve 5.78× speed-up and 20.16×
compression, while the top-5 classiﬁcation accuracy drop is
merely 0.85%. The result on VGG-16 is even more encour-
aging: with 4.06× speed-up and 20.34×, the increase of
top-5 error rate is only 0.58%. Hence, our proposed Q-CNN
framework can improve the efﬁciency of convolutional net-
works with minor performance loss, which is acceptable in
many applications.

5.3. Results on Mobile Devices

We have developed an Android application to fulﬁll
CNN-based image classiﬁcation on mobile devices, based
on our Q-CNN framework. The experiments are carried
out on a Huawei R(cid:13) Mate 7 smartphone, equipped with an
1.8GHz Kirin 925 CPU. The test-phase computation is car-
ried out on a single CPU core, without GPU acceleration.

In Table 9, we compare the computation efﬁciency and
classiﬁcation accuracy of the original and quantized CNN
models. Our Q-CNN framework achieves 3× speed-up for
AlexNet, and 4× speed-up for CNN-S. What’s more, we
compress the storage consumption by 20 ×, and the re-

Table 9. Comparison on the time, storage, memory consumption,
and top-5 classiﬁcation error rates of the original and quantized
AlexNet and CNN-S.
Model

AlexNet

CNN-S

CNN
Q-CNN
CNN
Q-CNN

Time
2.93s
0.95s
10.58s
2.61s

Storage
232.56MB
12.60MB
392.57MB
20.13MB

Memory
264.74MB
74.65MB
468.90MB
129.49MB

Top-5 Err.
19.74%
20.70%
15.82%
16.68%

quired run-time memory is only one quarter of the original
model. At the same time, the loss in the top-5 classiﬁcation
accuracy is no more than 1%. Therefore, our proposed ap-
proach improves the run-time efﬁciency in multiple aspects,
making the deployment of CNN models become tractable
on mobile platforms.

5.4. Theoretical vs. Realistic Speed-up

In Table 10, we compare the theoretical and realistic
speed-up on AlexNet. The BLAS [29] library is used in
Caffe [15] to accelerate the matrix multiplication in con-
volutional and fully-connected layers. However, it may not
always be an option for mobile devices. Therefore, we mea-
sure the run-time speed under two settings, i.e. with BLAS
enabled or disabled. The realistic speed-up is slightly lower
with BLAS on, indicating that Q-CNN does not beneﬁt as
much from BLAS as that of CNN. Other optimization tech-
niques, e.g. SIMD, SSE, and AVX [4], may further improve
our realistic speed-up, and shall be explored in the future.

Table 10. Comparison on the theoretical and realistic speed-up on
AlexNet (CPU only, single-threaded). Here we use the ATLAS
library, which is the default BLAS choice in Caffe [15].

BLAS

Off
On

FLOPs

CNN

Q-CNN

7.29e+8

1.75e+8

Time (ms)

Speed-up

CNN
321.10
167.794

Q-CNN
75.62
55.35

Theo.

4.15×

Real.
4.25×
3.03×

6. Conclusion

In this paper, we propose a uniﬁed framework to si-
multaneously accelerate and compress convolutional neural
networks. We quantize network parameters to enable ef-
ﬁcient test-phase computation. Extensive experiments are
conducted on MNIST and ILSVRC-12, and our approach
achieves outstanding speed-up and compression rates, with
only negligible loss in the classiﬁcation accuracy.

7. Acknowledgement

This work was supported in part by National Natural Sci-
ence Foundation of China (Grant No. 61332016), and 863
program (Grant No. 2014AA015105).

4This is Caffe’s run-time speed. The code for the other three settings is

on https://github.com/jiaxiang-wu/quantized-cnn.

[22] C. Leng, J. Wu, J. Cheng, X. Zhang, and H. Lu. Hashing for dis-
In International Conference on Machine Learning

tributed data.
(ICML), pages 1642–1650, 2015. 2

[23] G. Levi and T. Hassncer. Age and gender classiﬁcation using convo-
lutional neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), pages 34–42, 2015.
1

[24] C. Li, Q. Liu, J. Liu, and H. Lu. Learning ordinal discriminative
features for age estimation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2570–2577, 2012. 1
[25] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks.
CoRR, abs/1603.05279, 2016. 5

[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei. Imagenet large scale visual recognition challenge. Inter-
national Journal of Computer Vision (IJCV), pages 1–42, 2015. 2,
5

[27] K. Simonyan and A. Zisserman. Very deep convolutional networks
In International Conference on

for large-scale image recognition.
Learning Representations (ICLR), 2015. 1, 2, 6

[28] S. Srinivas and R. V. Babu. Data-free parameter pruning for deep
In British Machine Vision Conference (BMVC),

neural networks.
pages 31.1–31.12, 2015. 1, 5

[29] R. C. Whaley and A. Petitet. Minimizing development and mainte-
nance costs in supporting persistently optimized BLAS. Software:
Practice and Experience, 35(2):101–121, Feb 2005. 8

[30] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. CoRR, abs/1412.7149,
2014. 1, 5

[31] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep
convolutional networks for classiﬁcation and detection. CoRR,
abs/1505.06798, 2015. 1, 5, 7

[32] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient and accurate
approximations of nonlinear convolutional networks. In IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), pages
1984–1992, 2015. 1, 5

References

[1] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return
of the devil in the details: Delving deep into convolutional nets. In
British Machine Vision Conference (BMVC), 2014. 1, 2, 6

[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In International
Conference on Machine Learning (ICML), pages 2285–2294, 2015.
1, 2, 5, 6

[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmid-
huber. High-performance neural networks for visual object classiﬁ-
cation. CoRR, abs/1102.0183, 2011. 1, 5, 6

[4] I. Corporation. Intel architecture instruction set extensions program-
ming reference. Technical report, Intel Corporation, Feb 2016. 8
[5] M. Courbariaux, Y. Bengio, and J. David. Training deep neural net-
In International Confer-

works with low precision multiplications.
ence on Learning Representations (ICLR), 2015. 5

[6] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas.
Predicting parameters in deep learning. In Advances in Neural In-
formation Processing Systems (NIPS), pages 2148–2156, 2013. 5,
6

[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Ex-
ploiting linear structure within convolutional networks for efﬁcient
evaluation. In Advances in Neural Information Processing Systems
(NIPS), pages 1269–1277, 2014. 1, 5

[8] J. D. Geoffrey Hinton, Oriol Vinyals. Distilling the knowledge in a

neural network. CoRR, abs/1503.02531, 2015. 5, 6

[9] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 1
[10] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 580–587, 2014. 1

[11] Y. Gong, L. Liu, M. Yang, and L. D. Bourdev. Compressing
deep convolutional networks using vector quantization. CoRR,
abs/1412.6115, 2014. 1, 2, 5, 7

[12] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding. CoRR, abs/1510.00149, 2015. 1, 2, 5

[13] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolu-
tional neural networks with low rank expansions. In British Machine
Vision Conference (BMVC), 2014. 1

[14] H. Jegou, M. Douze, and C. Schmid. Product quantization for near-
est neighbor search. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 33(1):117–128, Jan 2011. 2

[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. CoRR, abs/1408.5093, 2014. 2, 6, 8
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 1106–1114, 2012. 1,
2, 6

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lem-
pitsky. Speeding-up convolutional neural networks using ﬁne-tuned
cp-decomposition. In International Conference on Learning Repre-
sentations (ICLR), 2015. 1, 5, 6

[18] V. Lebedev and V. S. Lempitsky. Fast convnets using group-wise

brain damage. CoRR, abs/1506.02515, 2015. 1, 5, 6

[19] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. E. Hubbard, and L. D. Jackel. Backpropagation applied to hand-
written zip code recognition. Neural Computation, 1(4):541–551,
1989. 1

[20] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998. 2, 5

[21] C. Leng, J. Wu, J. Cheng, X. Bai, and H. Lu. Online sketching hash-
ing. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2503–2511, 2015. 2

Appendix A: Additional Results

In the submission, we report the performance after quan-
tizing all the convolutional layers in AlexNet, and quan-
tizing all the full-connected layers in CaffeNet. Here, we
present experimental results for some other settings.

Quantizing Convolutional Layers in CaffeNet

We quantize all the convolutional layers in CaffeNet, and
the results are as demonstrated in Table 11. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.15% and 0.75%, compared to the original
CaffeNet.

Table 11. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CaffeNet, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.32×
4.32×
3.71×
4.27×
3.32×
4.32×
3.71×
4.27×

Top-1 Err. ↑
18.69%
32.84%
20.08%
35.48%
1.22%
2.44%
1.57%
2.30%

Top-5 Err. ↑
16.73%
33.55%
18.31%
37.82%
0.97%
1.83%
1.12%
1.71%

Quantizing Convolutional Layers in CNN-S

We quantize all the convolutional layers in CNN-S, and
the results are as demonstrated in Table 12. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.24% and 0.63%, compared to the original
CNN-S.

Table 12. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CNN-S, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.69×
5.17×
4.78×
5.92×
3.69×
5.17×
4.78×
5.92×

Top-1 Err. ↑
19.87%
45.74%
27.86%
46.18%
1.60%
3.49%
2.07%
3.42%

Top-5 Err. ↑
16.77%
48.67%
25.09%
50.26%
0.92%
2.32%
1.32%
2.17%

Quantizing Fully-connected Layers in AlexNet

We quantize all the fully-connected layers in AlexNet,

and the results are as demonstrated in Table 13.

Quantizing Fully-connected Layers in CNN-S

We quantize all the fully-connected layers in CNN-S,

Method

Table 13. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in AlexNet, without ﬁne-tuning.
Compression
Para.
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32

Top-5 Err. ↑
0.27%
0.64%
0.33%
0.69%
0.20%
0.22%
0.21%
0.38%

Top-1 Err. ↑
0.25%
0.77%
0.54%
0.71%
0.14%
0.40%
0.40%
0.46%

Q-CNN
(EC)

Q-CNN

Method

Table 14. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CNN-S, without ﬁne-tuning.
Para.
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
14.37×
20.15×
15.79×
19.66×
14.37×
20.15×
15.79×
19.66×

Top-5 Err. ↑
0.07%
0.22%
0.11%
0.27%
0.14%
0.24%
0.11%
0.27%

Top-1 Err. ↑
0.22%
0.45%
0.21%
0.35%
0.36%
0.43%
0.29%
0.56%

Q-CNN
(EC)

Q-CNN

Appendix B: Optimization in Section 3.3.2

Assume we have N images to learn the quantization of a
convolutional layer. For image In, we denote its input fea-
ture maps as Sn ∈ Rds×ds×Cs and response feature maps
as Tn ∈ Rdt×dt×Ct, where ds, dt are the spatial sizes and
Cs, Ct are the number of feature map channels. We use
ps and pt to denote the spatial location in the input and re-
sponse feature maps. The spatial location in the convolu-
tional kernels is denoted as pk.

To learn quantization with error correction for the con-

volutional layer, we attempt to optimize:

2

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
m

X
n,pt

X
(pk,ps)

(D(m)B(m)

pk )T S(m)

min
{D(m)},{B(m)
pk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(14)
where Dm is the m-th sub-codebook, and B(m)
is the cor-
pk
responding sub-codeword assignment indicator for the con-
volutional kernels at spatial location pk.

n,ps − Tn,pt

Similar to the fully-connected layer, we adopt a block co-
ordinate descent approach to solve this optimization prob-
lem. For the m-th subspace, we ﬁrstly deﬁne its residual
feature map as:

R(m)

n,pt = Tn,pt − X
(pk,ps)

X
m′6=m

(D(m′

)B(m′
pk

)

)T S(m′
)
n,ps

(15)

and the results are as demonstrated in Table 14.

and then the optimization in the m-th subspace can be re-

formulated as:

min
D(m),{B(m)
pk }

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(16)
Update D(m). With the assignment indicator {B(m)
pk }

n,ps − R(m)
n,pt

pk )T S(m)

(D(m)B(m)

X
(pk,ps)

X
n,pt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ﬁxed, we let:

Lk,pk = {ct|B(m)

pk (k, ct) = 1}

(17)

We greedily update each sub-codeword in the m-th sub-
codebook D(m) in a sequential style. For the k-th sub-
codeword, we compute the corresponding residual feature
map as:

Q(m)

n,pt,k(ct) = R(m)

n,pt (ct) − X
(pk ,ps)

X
k′6=k

X
ct∈Lk′,pk

D(m)T
k′

S(m)
n,ps

(18)

and then we can alternatively optimize:

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

min
D(m)
k

D(m)T

X
(pk,ps)

2
(cid:13)
(cid:13)
n,pt,k(ct)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(19)
which can be transformed into a least square problem. By
solving it, we can update the k-th sub-codeword.

n,ps − Q(m)
S(m)

X
ct∈Lk,pk

Update {B(m)

pk }. We greedily update the sub-codeword
assignment at each spatial location in the convolutional ker-
nels in a sequential style. For the spatial location pk, we
compute the corresponding residual feature map as:

P (m)
n,pt,pk = R(m)

(D(m)B(m)

p′
k

)T S(m)
n,p′
s

(20)

n,pt − X
(p′
k,p′
s)
pk6=pk

and then the optimization can be re-written as:

min
B(m)
pk

X
n,pt

(D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

pk )T S(m)

n,ps − P (m)

2
n,pt,pk (cid:13)
(cid:13)
F
(cid:13)

(21)

Since B(m)
pk ∈ {0, 1}K is an indicator vector (only one non-
zero entry), we can exhaustively try all sub-codewords and
select the optimal one that minimize the objective function.

6
1
0
2
 
y
a
M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
3
7
4
6
0
.
2
1
5
1
:
v
i
X
r
a

Quantized Convolutional Neural Networks for Mobile Devices

Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng
National Laboratory of Patter Recognition
Institute of Automation, Chinese Academy of Sciences
{jiaxiang.wu, cong.leng, yuhang.wang, qinghao.hu, jcheng}@nlpr.ia.ac.cn

Abstract

Original

Q-CNN

Time Consumption (s)

Storage Consumption (MB)

Recently, convolutional neural networks (CNN) have
demonstrated impressive performance in various computer
vision tasks. However, high performance hardware is typ-
ically indispensable for the application of CNN models
due to the high computation complexity, which prohibits
their further extensions. In this paper, we propose an efﬁ-
cient framework, namely Quantized CNN, to simultaneously
speed-up the computation and reduce the storage and mem-
ory overhead of CNN models. Both ﬁlter kernels in con-
volutional layers and weighting matrices in fully-connected
layers are quantized, aiming at minimizing the estimation
error of each layer’s response. Extensive experiments on
the ILSVRC-12 benchmark demonstrate 4 ∼ 6× speed-up
and 15 ∼ 20× compression with merely one percentage
loss of classiﬁcation accuracy. With our quantized CNN
model, even mobile devices can accurately classify images
within one second.

15

10

5

0

500

400

300

200

100

0

AlexNet

CNN-S

AlexNet

CNN-S

Memory Consumption (MB)

Top-5 Error Rate (%)

AlexNet

CNN-S

AlexNet

CNN-S

Figure 1. Comparison on the efﬁciency and classiﬁcation accuracy
between the original and quantized AlexNet [16] and CNN-S [1]
on a Huawei R(cid:13) Mate 7 smartphone.

400

300

200

100

0

25

20

15

10

5

0

1. Introduction

In recent years, we have witnessed the great success
of convolutional neural networks (CNN) [19] in a wide
range of visual applications, including image classiﬁcation
[16, 27], object detection [10, 9], age estimation [24, 23],
etc. This success mainly comes from deeper network ar-
chitectures as well as the tremendous training data. How-
ever, as the network grows deeper, the model complexity is
also increasing exponentially in both the training and testing
stages, which leads to the very high demand in the computa-
tion ability. For instance, the 8-layer AlexNet [16] involves
60M parameters and requires over 729M FLOPs1to classify
a single image. Although the training stage can be ofﬂine
carried out on high performance clusters with GPU acceler-
ation, the testing computation cost may be unaffordable for
common personal computers and mobile devices. Due to
the limited computation ability and memory space, mobile
devices are almost intractable to run deep convolutional net-
works. Therefore, it is crucial to accelerate the computation

and compress the memory consumption for CNN models.

For most CNNs, convolutional layers are the most time-
consuming part, while fully-connected layers involve mas-
sive network parameters. Due to the intrinsical differ-
ence between them, existing works usually focus on im-
proving the efﬁciency for either convolutional layers or
fully-connected layers.
In [7, 13, 32, 31, 18, 17], low-
rank approximation or tensor decomposition is adopted to
speed-up convolutional layers. On the other hand, param-
eter compression in fully-connected layers is explored in
[3, 7, 11, 30, 2, 12, 28]. Overall, the above-mentioned al-
gorithms are able to achieve faster speed or less storage.
However, few of them can achieve signiﬁcant acceleration
and compression simultaneously for the whole network.

In this paper, we propose a uniﬁed framework for con-
volutional networks, namely Quantized CNN (Q-CNN), to
simultaneously accelerate and compress CNN models with

1FLOPs: number of FLoating-point OPerations required to classify one

image with the convolutional network.

1

only minor performance degradation. With network pa-
rameters quantized, the response of both convolutional and
fully-connected layers can be efﬁciently estimated via the
approximate inner product computation. We minimize the
estimation error of each layer’s response during parameter
quantization, which can better preserve the model perfor-
mance. In order to suppress the accumulative error while
quantizing multiple layers, an effective training scheme is
introduced to take previous estimation error into consider-
ation. Our Q-CNN model enables fast test-phase compu-
tation, and the storage and memory consumption are also
signiﬁcantly reduced.

We evaluate our Q-CNN framework for image classi-
ﬁcation on two benchmarks, MNIST [20] and ILSVRC-
12 [26]. For MNIST, our Q-CNN approach achieves over
12× compression for two neural networks (no convolu-
tion), with lower accuracy loss than several baseline meth-
ods. For ILSVRC-12, we attempt to improve the test-phase
efﬁciency of four convolutional networks: AlexNet [16],
CaffeNet [15], CNN-S [1], and VGG-16 [27]. Generally,
Q-CNN achieves 4× acceleration and 15× compression
(sometimes higher) for each network, with less than 1%
drop in the top-5 classiﬁcation accuracy. Moreover, we im-
plement the quantized CNN model on mobile devices, and
dramatically improve the test-phase efﬁciency, as depicted
in Figure 1. The main contributions of this paper can be
summarized as follows:

• We propose a uniﬁed Q-CNN framework to acceler-
ate and compress convolutional networks. We demon-
strate that better quantization can be learned by mini-
mizing the estimation error of each layer’s response.
• We propose an effective training scheme to suppress
the accumulative error while quantizing the whole con-
volutional network.

• Our Q-CNN framework achieves 4 ∼ 6× speed-up
and 15 ∼ 20× compression, while the classiﬁcation
accuracy loss is within one percentage. Moreover, the
quantized CNN model can be implemented on mobile
devices and classify an image within one second.

2. Preliminary

During the test phase of convolutional networks, the
computation overhead is dominated by convolutional lay-
ers; meanwhile, the majority of network parameters are
stored in fully-connected layers. Therefore, for better test-
phase efﬁciency, it is critical to speed-up the convolution
computation and compress parameters in fully-connected
layers.

Our observation is that the forward-passing process of
both convolutional and fully-connected layers is dominated
by the computation of inner products. More formally, we
consider a convolutional layer with input feature maps S ∈

Rds×ds×Cs and response feature maps T ∈ Rdt×dt×Ct,
where ds, dt are the spatial sizes and Cs, Ct are the number
of feature map channels. The response at the 2-D spatial
position pt in the ct-th response feature map is computed
as:

Tpt (ct) = X(pk,ps)

hWct,pk , Spsi

(1)

where Wct ∈ Rdk×dk×Cs is the ct-th convolutional kernel
and dk is the kernel size. We use ps and pk to denote the
2-D spatial positions in the input feature maps and convolu-
tional kernels, and both Wct,pk and Sps are Cs-dimensional
vectors. The layer response is the sum of inner products at
all positions within the dk × dk receptive ﬁeld in the input
feature maps.

Similarly, for a fully-connected layer, we have:

T (ct) = hWct , Si

(2)

where S ∈ RCs and T ∈ RCt are the layer input and layer
response, respectively, and Wct ∈ RCs is the weighting
vector for the ct-th neuron of this layer.

Product quantization [14] is widely used in approximate
nearest neighbor search, demonstrating better performance
than hashing-based methods [21, 22]. The idea is to de-
compose the feature space as the Cartesian product of mul-
tiple subspaces, and then learn sub-codebooks for each sub-
space. A vector is represented by the concatenation of sub-
codewords for efﬁcient distance computation and storage.

In this paper, we leverage product quantization to imple-
ment the efﬁcient inner product computation. Let us con-
sider the inner product computation between x, y ∈ RD. At
ﬁrst, both x and y are split into M sub-vectors, denoted as
x(m) and y(m). Afterwards, each x(m) is quantized with a
sub-codeword from the m-th sub-codebook, then we have

hy, xi = Xm

hy(m), x(m)i ≈ Xm

hy(m), c(m)
km i

(3)

which transforms the O(D) inner product computation to
M addition operations (M ≤ D), if the inner products be-
tween each sub-vector y(m) and all the sub-codewords in
the m-th sub-codebook have been computed in advance.

Quantization-based approaches have been explored in
several works [11, 2, 12]. These approaches mostly fo-
cus on compressing parameters in fully-connected layers
[11, 2], and none of them can provide acceleration for the
test-phase computation. Furthermore, [11, 12] require the
network parameters to be re-constructed during the test-
phase, which limit the compression to disk storage instead
of memory consumption. On the contrary, our approach
offers simultaneous acceleration and compression for both
convolutional and fully-connected layers, and can reduce
the run-time memory consumption dramatically.

3. Quantized CNN

In this section, we present our approach for accelerating
and compressing convolutional networks. Firstly, we intro-
duce an efﬁcient test-phase computation process with the
network parameters quantized. Secondly, we demonstrate
that better quantization can be learned by directly minimiz-
ing the estimation error of each layer’s response. Finally,
we analyze the computation complexity of our quantized
CNN model.

3.1. Quantizing the Fully-connected Layer

For a fully-connected layer, we denote its weighting ma-
trix as W ∈ RCs×Ct, where Cs and Ct are the dimensions
of the layer input and response, respectively. The weighting
vector Wct is the ct-th column vector in W .

We evenly split the Cs-dimensional space (where Wct
lies in) into M subspaces, each of C′
s = Cs/M dimen-
sions. Each Wct is then decomposed into M sub-vectors,
denoted as W (m)
. A sub-codebook can be learned for each
subspace after gathering all the sub-vectors within this sub-
space. Formally, for the m-th subspace, we optimize:

ct

min
D(m),B(m)

2
D(m)B(m) − W (m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(cid:13)
s×K, B(m) ∈ {0, 1}K×Ct

s.t. D(m) ∈ RC ′

(4)

where W (m) ∈ RC ′
s×Ct consists of the m-th sub-vectors
of all weighting vectors. The sub-codebook D(m) contains
K sub-codewords, and each column in B(m) is an indica-
tor vector (only one non-zero entry), specifying which sub-
codeword is used to quantize the corresponding sub-vector.
The optimization can be solved via k-means clustering.
The layer response is approximately computed as:
, S(m)i ≈ Xm
= Xm

T (ct) = Xm

km(ct), S(m)i

hD(m)B(m)

hW (m)
ct

, S(m)i

hD(m)

(5)

ct

ct

ct

where B(m)
is the ct-th column vector in B(m), and S(m) is
the m-th sub-vector of the layer input. km(ct) is the index
of the sub-codeword used to quantize the sub-vector W (m)
.
In Figure 2, we depict the parameter quantization and
test-phase computation process of the fully-connected layer.
By decomposing the weighting matrix into M sub-matrices,
M sub-codebooks can be learned, one per subspace. During
the test-phase, the layer input is split into M sub-vectors,
denoted as S(m). For each subspace, we compute the inner
products between S(m) and every sub-codeword in D(m),
and store the results in a look-up table. Afterwards, only M
addition operations are required to compute each response.
As a result, the overall time complexity can be reduced from
O(CsCt) to O(CsK + CtM ). On the other hand, only
sub-codebooks and quantization indices need to be stored,
which can dramatically reduce the storage consumption.

Figure 2. The parameter quantization and test-phase computation
process of the fully-connected layer.

3.2. Quantizing the Convolutional Layer

Unlike the 1-D weighting vector in the fully-connected
layer, each convolutional kernel is a 3-dimensional tensor:
Wct ∈ Rdk×dk×Cs. Before quantization, we need to deter-
mine how to split it into sub-vectors, i.e. apply subspace
splitting to which dimension. During the test phase, the in-
put feature maps are traversed by each convolutional kernel
with a sliding window in the spatial domain. Since these
sliding windows are partially overlapped, we split each con-
volutional kernel along the dimension of feature map chan-
nels, so that the pre-computed inner products can be re-
used at multiple spatial locations. Speciﬁcally, we learn the
quantization in each subspace by:

min
Xpk
D(m),{B(m)
pk }
s.t. D(m) ∈ RC ′

D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

s×K, B(m)

2
pk − W (m)
pk (cid:13)
(cid:13)
F
(cid:13)
pk ∈ {0, 1}K×Ct

(6)

pk ∈ RC ′

where W (m)
s×Ct contains the m-th sub-vectors of
all convolutional kernels at position pk. The optimization
can also be solved by k-means clustering in each subspace.
With the convolutional kernels quantized, we approxi-

mately compute the response feature maps by:

Tpt(ct) = X(pk,ps) Xm
≈ X(pk,ps) Xm
= X(pk,ps) Xm

hW (m)

ct,pk , S(m)
ps i

hD(m)B(m)

ct,pk , S(m)
ps i

(7)

hD(m)

km(ct,pk), S(m)
ps i

where S(m)
is the m-th sub-vector at position ps in the in-
ps
put feature maps, and km(ct, pk) is the index of the sub-
codeword to quantize the m-th sub-vector at position pk in
the ct-th convolutional kernel.

Similar to the fully-connected layer, we pre-compute the
look-up tables of inner products with the input feature maps.
Then, the response feature maps are approximately com-
puted with (7), and both the time and storage complexity
can be greatly reduced.

and the above optimization can be solved by alternatively
updating the sub-codebook and sub-codeword assignment.
Update D(m). We ﬁx the sub-codeword assignment
B(m), and deﬁne Lk = {ct|B(m)(k, ct) = 1}. The opti-
mization in (10) can be re-formulated as:

3.3. Quantization with Error Correction

So far, we have presented an intuitive approach to quan-
tize parameters and improve the test-phase efﬁciency of
convolutional networks. However, there are still two crit-
ical drawbacks. First, minimizing the quantization error
of model parameters does not necessarily give the optimal
quantized network for the classiﬁcation accuracy. In con-
trast, minimizing the estimation error of each layer’s re-
sponse is more closely related to the network’s classiﬁca-
tion performance. Second, the quantization of one layer is
independent of others, which may lead to the accumulation
of error when quantizing multiple layers. The estimation
error of the network’s ﬁnal response is very likely to be
quickly accumulated, since the error introduced by the pre-
vious quantized layers will also affect the following layers.
To overcome these two limitations, we introduce the idea
of error correction into the quantization of network param-
eters. This improved quantization approach directly min-
imizes the estimation error of the response at each layer,
and can compensate the error introduced by previous lay-
ers. With the error correction scheme, we can quantize the
network with much less performance degradation than the
original quantization method.

3.3.1 Error Correction for the Fully-connected Layer

Suppose we have N images to learn the quantization of a
fully-connected layer, and the layer input and response of
image In are denoted as Sn and Tn. In order to minimize
the estimation error of the layer response, we optimize:

Xn

min
{D(m)},{B(m)}

Tn − Xm
(cid:13)
(cid:13)
(cid:13)

2
(D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)
(8)
where the ﬁrst term in the Frobenius norm is the desired
layer response, and the second term is the approximated
layer response computed via the quantized parameters.

A block coordinate descent approach can be applied to
minimize this objective function. For the m-th subspace, its
residual error is deﬁned as:

R(m)

n = Tn − Xm′6=m

(D(m′

)B(m′

))T S(m′

n

)

(9)

and then we attempt to minimize the residual error of this
subspace, which is:

min

D(m),B(m) Xn

2
n − (D(m)B(m))T S(m)
n (cid:13)
(cid:13)
F
(cid:13)

R(m)
(cid:13)
(cid:13)
(cid:13)

(10)

min
{D(m)
k

}

Xn,k Xct∈Lk

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(11)

which implies that the optimization over one sub-codeword
does not affect other sub-codewords. Hence, for each sub-
codeword, we construct a least square problem from (11) to
update it.

Update B(m). With the sub-codebook D(m) ﬁxed, it
is easy to discover that the optimization of each column in
B(m) is mutually independent. For the ct-th column, its
optimal sub-codeword assignment is given by:

k∗
m(ct) = arg min

k Xn

[R(m)

n (ct) − D(m)T

k

S(m)
n ]2

(12)

3.3.2 Error Correction for the Convolutional Layer

We adopt the similar idea to minimize the estimation error
of the convolutional layer’s response feature maps, that is:

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

(D(m)B(m)

min
{D(m)},{B(m)
pk }

Tn,pt − X
(pk ,ps)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(13)
The optimization also can be solved by block coordinate
descent. More details on solving this optimization can be
found in the supplementary material.

pk )T S(m)
n,ps

X
m

3.3.3 Error Correction for Multiple Layers

The above quantization method can be sequentially applied
to each layer in the CNN model. One concern is that the
estimation error of layer response caused by the previous
layers will be accumulated and affect the quantization of
the following layers. Here, we propose an effective training
scheme to address this issue.

We consider the quantization of a speciﬁc layer, assum-
ing its previous layers have already been quantized. The
optimization of parameter quantization is based on the layer
input and response of a group of training images. To quan-
tize this layer, we take the layer input in the quantized net-
work as {Sn}, and the layer response in the original net-
work (not quantized) as {Tn} in Eq. (8) and (13). In this
way, the optimization is guided by the actual input in the
quantized network and the desired response in the original
network. The accumulative error introduced by the previ-
ous layers is explicitly taken into consideration during op-
timization. In consequence, this training scheme can effec-
tively suppress the accumulative error for the quantization
of multiple layers.

Another possible solution is to adopt back-propagation
to jointly update the sub-codebooks and sub-codeword as-
signments in all quantized layers. However, since the sub-
codeword assignments are discrete, the gradient-based op-
timization can be quite difﬁcult, if not entirely impossible.
Therefore, back-propagation is not adopted here, but could
be a promising extension for future work.

3.4. Computation Complexity

Now we analyze the test-phase computation complex-
ity of convolutional and fully-connected layers, with or
without parameter quantization. For our proposed Q-CNN
model, the forward-passing through each layer mainly con-
sists of two procedures: pre-computation of inner products,
and approximate computation of layer response. Both sub-
codebooks and sub-codeword assignments are stored for the
test-phase computation. We report the detailed comparison
on the computation and storage overhead in Table 1.

Table 1. Comparison on the computation and storage overhead of
convolutional and fully-connected layers.

FLOPs

Bytes

Conv.

FCnt.

Conv.

FCnt.

kM

kCs
t Ctd2

t Ctd2
d2
CNN
sCsK + d2
d2
Q-CNN
CNN
CsCt
Q-CNN
CsK + CtM
4d2
CNN
kCsCt
Q-CNN 4CsK + 1
8 d2
kM Ct log2 K
CNN
4CsCt
Q-CNN
8 M Ct log2 K

4CsK + 1

As we can see from Table 1, the reduction in the compu-
tation and storage overhead largely depends on two hyper-
parameters, M (number of subspaces) and K (number of
sub-codewords in each subspace). Large values of M and
K lead to more ﬁne-grained quantization, but is less efﬁ-
cient in the computation and storage consumption. In prac-
tice, we can vary these two parameters to balance the trade-
off between the test-phase efﬁciency and accuracy loss of
the quantized CNN model.

4. Related Work

There have been a few attempts in accelerating the test-
phase computation of convolutional networks, and many are
inspired from the low-rank decomposition. Denton et al.
[7] presented a series of low-rank decomposition designs
for convolutional kernels. Similarly, CP-decomposition was
adopted in [17] to transform a convolutional layer into mul-
tiple layers with lower complexity. Zhang et al. [32, 31]
considered the subsequent nonlinear units while learning
the low-rank decomposition. [18] applied group-wise prun-
ing to the convolutional tensor to decompose it into the mul-
tiplications of thinned dense matrices. Recently, ﬁxed-point
based approaches are explored in [5, 25]. By representing

the connection weights (or even network activations) with
ﬁxed-point numbers, the computation can greatly beneﬁt
from hardware acceleration.

Another parallel research trend is to compress parame-
ters in fully-connected layers. Ciresan et al. [3] randomly
remove connection to reduce network parameters. Matrix
factorization was adopted in [6, 7] to decompose the weight-
ing matrix into two low-rank matrices, which demonstrated
that signiﬁcant redundancy did exist in network parameters.
Hinton et al. [8] proposed to use dark knowledge (the re-
sponse of a well-trained network) to guide the training of
a much smaller network, which was superior than directly
training. By exploring the similarity among neurons, Srini-
vas et al. [28] proposed a systematic way to remove redun-
dant neurons instead of network connections. In [30], mul-
tiple fully-connected layers were replaced by a single “Fast-
food” layer, which can be trained in an end-to-end style with
[2] randomly grouped
convolutional layers. Chen et al.
connection weights into hash buckets, and then ﬁne-tuned
the network with back-propagation. [12] combined prun-
ing, quantization, and Huffman coding to achieve higher
compression rate. Gong et al. [11] adopted vector quanti-
zation to compress the weighing matrix, which was actually
a special case of our approach (apply Q-CNN without error
correction to fully-connected layers only).

5. Experiments

In this section, we evaluate our quantized CNN frame-
work on two image classiﬁcation benchmarks, MNIST [20]
and ILSVRC-12 [26]. For the acceleration of convolutional
layers, we compare with:

• CPD [17]: CP-Decomposition;
• GBD [18]: Group-wise Brain Damage;
• LANR [31]: Low-rank Approximation of Non-linear

Responses.

and for the compression of fully-connected layers, we com-
pare with the following approaches:

• RER [3]: Random Edge Removal;
• LRD [6]: Low-Rank Decomposition;
• DK [8]: Dark Knowledge;
• HashNet [2]: Hashed Neural Nets;
• DPP [28]: Data-free Parameter Pruning;
• SVD [7]: Singular Value Decomposition;
• DFC [30]: Deep Fried Convnets.

For all above baselines, we use their reported results under
the same setting for fair comparison. We report the theo-
retical speed-up for more consistent results, since the real-
istic speed-up may be affected by various factors, e.g. CPU,
cache, and RAM. We compare the theoretical and realistic
speed-up in Section 5.4, and discuss the effect of adopting
the BLAS library for acceleration.

Our approaches are denoted as “Q-CNN” and “Q-CNN
(EC)”, where the latter one adopts error correction while the
former one does not. We implement the optimization pro-
cess of parameter quantization in MATLAB, and ﬁne-tune
the resulting network with Caffe [15]. Additional results of
our approach can be found in the supplementary material.

5.1. Results on MNIST

The MNIST dataset contains 70k images of hand-written
digits, 60k used for training and 10k for testing. To evalu-
ate the compression performance, we pre-train two neural
networks, one is 3-layer and another one is 5-layer, where
each hidden layer contains 1000 units. Different compres-
sion techniques are then adopted to compress these two net-
work, and the results are as depicted in Table 2.

Table 2. Comparison on the compression rates and classiﬁcation
error on MNIST, based on a 3-layer network (784-1000-10) and a
5-layer network (784-1000-1000-1000-10).

Method

Original
RER [3]
LRD [6]
DK [8]
HashNets [2]
Q-CNN
Q-CNN (EC)

3-layer

5-layer

Error
1.35%
2.19%
1.89%
1.71%
1.43%

Compr.
-
8×
8×
8×
8×

Compr.
-
8×
8×
8×
8×

Error
1.12%
1.24%
1.77%
1.26%
1.22%
12.1× 1.42% 13.4× 1.34%
12.1× 1.39% 13.4× 1.19%

s is determined once C′

In our Q-CNN framework, the trade-off between accu-
racy and efﬁciency is controlled by M (number of sub-
spaces) and K (number of sub-codewrods in each sub-
space). Since M = Cs/C′
s is given,
we tune (C′
s, K) to adjust the quantization precision. In Ta-
ble 2, we set the hyper-parameters as C′
s = 4 and K = 32.
From Table 2, we observe that our Q-CNN (EC) ap-
proach offers higher compression rates with less perfor-
mance degradation than all baselines for both networks.
The error correction scheme is effective in reducing the ac-
curacy loss, especially for deeper networks (5-layer). Also,
we ﬁnd the performance of both Q-CNN and Q-CNN (EC)
quite stable, as the standard deviation of ﬁve random runs is
merely 0.05%. Therefore, we report the single-run perfor-
mance in the remaining experiments.

5.2. Results on ILSVRC-12

The ILSVRC-12 benchmark consists of over one million
training images drawn from 1000 categories, and a disjoint
validation set of 50k images. We report both the top-1 and
top-5 classiﬁcation error rates on the validation set, using
single-view testing (central patch only).

We demonstrate our approach on four convolutional net-
works: AlexNet [16], CaffeNet [15], CNN-S [1], and VGG-

16 [27]. The ﬁrst two models have been adopted in several
related works, and therefore are included for comparison.
CNN-S and VGG-16 use a either wider or deeper structure
for better classiﬁcation accuracy, and are included here to
prove the scalability of our approach. We compare all these
networks’ computation and storage overhead in Table 3, to-
gether with their classiﬁcation error rates on ILSVRC-12.

Table 3. Comparison on the test-phase computation overhead
(FLOPs), storage consumption (Bytes), and classiﬁcation error
rates (Top-1/5 Err.) of AlexNet, CaffeNet, CNN-S, and VGG-16.
Bytes
2.44e+8
2.44e+8
4.12e+8
5.53e+8

Top-5 Err.
19.74%
19.59%
15.82%
10.05%

Top-1 Err.
42.78%
42.53%
37.31%
28.89%

FLOPs
7.29e+8
7.27e+8
2.94e+9
1.55e+10

Model
AlexNet
CaffeNet
CNN-S
VGG-16

5.2.1 Quantizing the Convolutional Layer

To begin with, we quantize the second convolutional layer
of AlexNet, which is the most time-consuming layer during
the test-phase. In Table 4, we report the performance un-
der several (C′
s, K) settings, comparing with two baseline
methods, CPD [17] and GBD [18].

Table 4. Comparison on the speed-up rates and the increase of top-
1/5 error rates for accelerating the second convolutional layer in
AlexNet, with or without ﬁne-tuning (FT). The hyper-parameters
of Q-CNN, C ′

Method

CPD

GBD

Q-CNN

Q-CNN
(EC)

Para.

Speed-up

s and K, are as speciﬁed in the “Para.” column.
Top-5 Err. ↑
Top-1 Err. ↑
FT
FT
0.44%
-
1.22%
-
18.63%
-
-
0.11%
-
0.43%
-
1.13%
1.37%
1.63%
2.27%
2.90%
1.28%
1.57%
2.66%
2.91%
0.17%
0.20%
0.40%
0.39%
0.21%
0.11%
0.31%
0.33%

No FT
0.94%
3.20%
69.06%
-
-
-
8.97%
14.71%
9.10%
18.05%
0.27%
0.50%
0.34%
0.50%

No FT
-
-
-
12.43%
21.93%
48.33%
10.55%
15.93%
10.62%
18.84%
0.35%
0.64%
0.27%
0.55%

3.19×
4.52×
6.51×
3.33×
5.00×
10.00×
3.70×
5.36×
4.84×
6.06×
3.70×
5.36×
4.84×
6.06×

-
-
-
-
-
-
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

From Table 4, we discover that with a large speed-up
rate (over 4×), the performance loss of both CPD and GBD
become severe, especially before ﬁne-tuning. The naive
parameter quantization method also suffers from the sim-
ilar problem. By incorporating the idea of error correction,
our Q-CNN model achieves up to 6× speed-up with merely
0.6% drop in accuracy, even without ﬁne-tuning. The ac-
curacy loss can be further reduced after ﬁne-tuning the sub-
sequent layers. Hence, it is more effective to minimize the
estimation error of each layer’s response than minimize the
quantization error of network parameters.

Next, we take one step further and attempt to speed-up
all the convolutional layers in AlexNet with Q-CNN (EC).

Table 5. Comparison on the speed-up/compression rates and the increase of top-1/5 error rates for accelerating all the convolutional layers
in AlexNet and VGG-16.

Model

Method

Para.

Speed-up Compression

AlexNet

Q-CNN
(EC)

VGG-16

LANR [31]
Q-CNN (EC)

4/64
6/64
6/128
8/128
-
6/128

3.32×
4.32×
3.71×
4.27×
4.00×
4.06×

10.58×
14.32×
10.27×
12.08×
2.73×
14.40×

Top-1 Err. ↑
FT
-
-

Top-5 Err. ↑
FT
-
-

No FT
0.94%
1.90%

No FT
1.33%
2.32%
1.44% 0.13% 1.16% 0.36%
2.25% 0.99% 1.64% 0.60%
0.95% 0.35%
3.04% 1.06% 1.83% 0.45%

-

-

We ﬁx the quantization hyper-parameters (C′
s, K) across all
layers. From Table 5, we observe that the loss in accuracy
grows mildly than the single-layer case. The speed-up rates
reported here are consistently smaller than those in Table 4,
since the acceleration effect is less signiﬁcant for some lay-
ers (i.e. “conv 4” and “conv 5”). For AlexNet, our Q-CNN
model (C′
s = 8, K = 128) can accelerate the computation
of all the convolutional layers by a factor of 4.27×, while
the increase in the top-1 and top-5 error rates are no more
than 2.5%. After ﬁne-tuning the remaining fully-connected
layers, the performance loss can be further reduced to less
than 1%.

In Table 5, we also report the comparison against LANR
[31] on VGG-16. For the similar speed-up rate (4×), their
approach outperforms ours in the top-5 classiﬁcation error
(an increase of 0.95% against 1.83%). After ﬁne-tuning, the
performance gap is narrowed down to 0.35% against 0.45%.
At the same time, our approach offers over 14× compres-
sion of parameters in convolutional layers, much larger than
theirs 2.7× compression2. Therefore, our approach is effec-
tive in accelerating and compressing networks with many
convolutional layers, with only minor performance loss.

5.2.2 Quantizing the Fully-connected Layer

For demonstration, we ﬁrst compress parameters in a single
fully-connected layer. In CaffeNet, the ﬁrst fully-connected
layer possesses over 37 million parameters (9216 × 4096),
more than 60% of whole network parameters. Our Q-CNN
approach is adopted to quantize this layer and the results are
as reported in Table 6. The performance loss of our Q-CNN
model is negligible (within 0.4%), which is much smaller
than baseline methods (DPP and SVD). Furthermore, error
correction is effective in preserving the classiﬁcation accu-
racy, especially under a higher compression rate.

Now we evaluate our approach’s performance for com-
pressing all the fully-connected layers in CaffeNet in Ta-
ble 7. The third layer is actually the combination of 1000
classiﬁers, and is more critical to the classiﬁcation accuracy.
Hence, we adopt a much more ﬁne-grained hyper-parameter

2The compression effect of their approach was not explicitly discussed
in the paper; we estimate the compression rate based on their description.

DPP

Method

Table 6. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing the ﬁrst fully-connected layer
in CaffeNet, without ﬁne-tuning.
Compression
Para.
1.19×
-
1.47×
-
1.91×
-
2.75×
-
1.38×
-
2.77×
-
-
5.54×
11.08×
-
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32
15.06×
2/16
21.94×
3/16
16.70×
3/32
21.33×
4/32

Top-5 Err. ↑
-
-
-
-
-0.03%
0.07%
0.19%
0.86%
0.19%
0.28%
0.12%
0.16%
0.07%
0.03%
0.11%
0.12%

Top-1 Err. ↑
0.16%
1.76%
4.08%
9.68%
0.03%
0.07%
0.36%
1.23%
0.19%
0.35%
0.18%
0.28%
0.10%
0.18%
0.14%
0.16%

Q-CNN
(EC)

Q-CNN

SVD

setting (C′
s = 1, K = 16) for this layer. Although the
speed-up effect no longer exists, we can still achieve around
8× compression for the last layer.

SVD

Method

Table 7. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CaffeNet. Both SVD and DFC are ﬁne-tuned, while Q-CNN
and Q-CNN (EC) are not ﬁne-tuned.
Para.
-
-
-
-
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
1.26×
2.52×
1.79×
3.58×
13.96×
19.14×
15.25×
18.71×
13.96×
19.14×
15.25×
18.71×

Top-5 Err. ↑
-
-
-
-
0.29%
0.47%
0.34%
0.59%
0.30%
0.47%
0.27%
0.39%

Top-1 Err. ↑
0.14%
1.22%
-0.66%
0.31%
0.28%
0.70%
0.44%
0.75%
0.31%
0.59%
0.31%
0.57%

Q-CNN
(EC)

Q-CNN

DFC

From Table 7, we discover that with less than 1% drop in
accuracy, Q-CNN achieves high compression rates (12 ∼
20×), much larger than that of SVD3and DFC (< 4×).
Again, Q-CNN with error correction consistently outper-
forms the naive Q-CNN approach as adopted in [11].

3In Table 6, SVD means replacing the weighting matrix with the multi-
plication of two low-rank matrices; in Table 7, SVD means ﬁne-tuning the
network after the low-rank matrix decomposition.

5.2.3 Quantizing the Whole Network

So far, we have evaluated the performance of CNN models
with either convolutional or fully-connected layers quan-
tized. Now we demonstrate the quantization of the whole
network with a three-stage strategy. Firstly, we quantize all
the convolutional layers with error correction, while fully-
connected layers remain untouched. Secondly, we ﬁne-tune
fully-connected layers in the quantized network with the
ILSVRC-12 training set to restore the classiﬁcation accu-
racy. Finally, fully-connected layers in the ﬁne-tuned net-
work are quantized with error correction. We report the
performance of our Q-CNN models in Table 8.

Table 8. The speed-up/compression rates and the increase of top-
1/5 error rates for the whole CNN model. Particularly, for the
quantization of the third fully-connected layer in each network,
we let C ′
Model

s = 1 and K = 16.
Para.

Compression

Top-1/5 Err. ↑

Speed-up

AlexNet

CaffeNet

CNN-S

VGG-16

Conv.
8/128
8/128
8/128
8/128
8/128
8/128
6/128
6/128

FCnt.
3/32
4/32
3/32
4/32
3/32
4/32
3/32
4/32

4.05×
4.15×
4.04×
4.14×
5.69×
5.78×
4.05×
4.06×

15.40×
18.76×
15.40×
18.76×
16.32×
20.16×
16.55×
20.34×

1.38% / 0.84%
1.46% / 0.97%
1.43% / 0.99%
1.54% / 1.12%
1.48% / 0.81%
1.64% / 0.85%
1.22% / 0.53%
1.35% / 0.58%

For convolutional layers, we let C′

s = 8 and K = 128
for AlexNet, CaffeNet, and CNN-S, and let C′
s = 6 and
K = 128 for VGG-16, to ensure roughly 4 ∼ 6× speed-
up for each network. Then we vary the hyper-parameter
settings in fully-connected layers for different compression
levels. For the former two networks, we achieve 18× com-
pression with about 1% loss in the top-5 classiﬁcation accu-
racy. For CNN-S, we achieve 5.78× speed-up and 20.16×
compression, while the top-5 classiﬁcation accuracy drop is
merely 0.85%. The result on VGG-16 is even more encour-
aging: with 4.06× speed-up and 20.34×, the increase of
top-5 error rate is only 0.58%. Hence, our proposed Q-CNN
framework can improve the efﬁciency of convolutional net-
works with minor performance loss, which is acceptable in
many applications.

5.3. Results on Mobile Devices

We have developed an Android application to fulﬁll
CNN-based image classiﬁcation on mobile devices, based
on our Q-CNN framework. The experiments are carried
out on a Huawei R(cid:13) Mate 7 smartphone, equipped with an
1.8GHz Kirin 925 CPU. The test-phase computation is car-
ried out on a single CPU core, without GPU acceleration.

In Table 9, we compare the computation efﬁciency and
classiﬁcation accuracy of the original and quantized CNN
models. Our Q-CNN framework achieves 3× speed-up for
AlexNet, and 4× speed-up for CNN-S. What’s more, we
compress the storage consumption by 20 ×, and the re-

Table 9. Comparison on the time, storage, memory consumption,
and top-5 classiﬁcation error rates of the original and quantized
AlexNet and CNN-S.
Model

AlexNet

CNN-S

CNN
Q-CNN
CNN
Q-CNN

Time
2.93s
0.95s
10.58s
2.61s

Storage
232.56MB
12.60MB
392.57MB
20.13MB

Memory
264.74MB
74.65MB
468.90MB
129.49MB

Top-5 Err.
19.74%
20.70%
15.82%
16.68%

quired run-time memory is only one quarter of the original
model. At the same time, the loss in the top-5 classiﬁcation
accuracy is no more than 1%. Therefore, our proposed ap-
proach improves the run-time efﬁciency in multiple aspects,
making the deployment of CNN models become tractable
on mobile platforms.

5.4. Theoretical vs. Realistic Speed-up

In Table 10, we compare the theoretical and realistic
speed-up on AlexNet. The BLAS [29] library is used in
Caffe [15] to accelerate the matrix multiplication in con-
volutional and fully-connected layers. However, it may not
always be an option for mobile devices. Therefore, we mea-
sure the run-time speed under two settings, i.e. with BLAS
enabled or disabled. The realistic speed-up is slightly lower
with BLAS on, indicating that Q-CNN does not beneﬁt as
much from BLAS as that of CNN. Other optimization tech-
niques, e.g. SIMD, SSE, and AVX [4], may further improve
our realistic speed-up, and shall be explored in the future.

Table 10. Comparison on the theoretical and realistic speed-up on
AlexNet (CPU only, single-threaded). Here we use the ATLAS
library, which is the default BLAS choice in Caffe [15].

BLAS

Off
On

FLOPs

CNN

Q-CNN

7.29e+8

1.75e+8

Time (ms)

Speed-up

CNN
321.10
167.794

Q-CNN
75.62
55.35

Theo.

4.15×

Real.
4.25×
3.03×

6. Conclusion

In this paper, we propose a uniﬁed framework to si-
multaneously accelerate and compress convolutional neural
networks. We quantize network parameters to enable ef-
ﬁcient test-phase computation. Extensive experiments are
conducted on MNIST and ILSVRC-12, and our approach
achieves outstanding speed-up and compression rates, with
only negligible loss in the classiﬁcation accuracy.

7. Acknowledgement

This work was supported in part by National Natural Sci-
ence Foundation of China (Grant No. 61332016), and 863
program (Grant No. 2014AA015105).

4This is Caffe’s run-time speed. The code for the other three settings is

on https://github.com/jiaxiang-wu/quantized-cnn.

[22] C. Leng, J. Wu, J. Cheng, X. Zhang, and H. Lu. Hashing for dis-
In International Conference on Machine Learning

tributed data.
(ICML), pages 1642–1650, 2015. 2

[23] G. Levi and T. Hassncer. Age and gender classiﬁcation using convo-
lutional neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW), pages 34–42, 2015.
1

[24] C. Li, Q. Liu, J. Liu, and H. Lu. Learning ordinal discriminative
features for age estimation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2570–2577, 2012. 1
[25] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks.
CoRR, abs/1603.05279, 2016. 5

[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei. Imagenet large scale visual recognition challenge. Inter-
national Journal of Computer Vision (IJCV), pages 1–42, 2015. 2,
5

[27] K. Simonyan and A. Zisserman. Very deep convolutional networks
In International Conference on

for large-scale image recognition.
Learning Representations (ICLR), 2015. 1, 2, 6

[28] S. Srinivas and R. V. Babu. Data-free parameter pruning for deep
In British Machine Vision Conference (BMVC),

neural networks.
pages 31.1–31.12, 2015. 1, 5

[29] R. C. Whaley and A. Petitet. Minimizing development and mainte-
nance costs in supporting persistently optimized BLAS. Software:
Practice and Experience, 35(2):101–121, Feb 2005. 8

[30] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. CoRR, abs/1412.7149,
2014. 1, 5

[31] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep
convolutional networks for classiﬁcation and detection. CoRR,
abs/1505.06798, 2015. 1, 5, 7

[32] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient and accurate
approximations of nonlinear convolutional networks. In IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), pages
1984–1992, 2015. 1, 5

References

[1] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return
of the devil in the details: Delving deep into convolutional nets. In
British Machine Vision Conference (BMVC), 2014. 1, 2, 6

[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In International
Conference on Machine Learning (ICML), pages 2285–2294, 2015.
1, 2, 5, 6

[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmid-
huber. High-performance neural networks for visual object classiﬁ-
cation. CoRR, abs/1102.0183, 2011. 1, 5, 6

[4] I. Corporation. Intel architecture instruction set extensions program-
ming reference. Technical report, Intel Corporation, Feb 2016. 8
[5] M. Courbariaux, Y. Bengio, and J. David. Training deep neural net-
In International Confer-

works with low precision multiplications.
ence on Learning Representations (ICLR), 2015. 5

[6] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas.
Predicting parameters in deep learning. In Advances in Neural In-
formation Processing Systems (NIPS), pages 2148–2156, 2013. 5,
6

[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Ex-
ploiting linear structure within convolutional networks for efﬁcient
evaluation. In Advances in Neural Information Processing Systems
(NIPS), pages 1269–1277, 2014. 1, 5

[8] J. D. Geoffrey Hinton, Oriol Vinyals. Distilling the knowledge in a

neural network. CoRR, abs/1503.02531, 2015. 5, 6

[9] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 1
[10] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 580–587, 2014. 1

[11] Y. Gong, L. Liu, M. Yang, and L. D. Bourdev. Compressing
deep convolutional networks using vector quantization. CoRR,
abs/1412.6115, 2014. 1, 2, 5, 7

[12] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding. CoRR, abs/1510.00149, 2015. 1, 2, 5

[13] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolu-
tional neural networks with low rank expansions. In British Machine
Vision Conference (BMVC), 2014. 1

[14] H. Jegou, M. Douze, and C. Schmid. Product quantization for near-
est neighbor search. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 33(1):117–128, Jan 2011. 2

[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. CoRR, abs/1408.5093, 2014. 2, 6, 8
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-
tion with deep convolutional neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 1106–1114, 2012. 1,
2, 6

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lem-
pitsky. Speeding-up convolutional neural networks using ﬁne-tuned
cp-decomposition. In International Conference on Learning Repre-
sentations (ICLR), 2015. 1, 5, 6

[18] V. Lebedev and V. S. Lempitsky. Fast convnets using group-wise

brain damage. CoRR, abs/1506.02515, 2015. 1, 5, 6

[19] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. E. Hubbard, and L. D. Jackel. Backpropagation applied to hand-
written zip code recognition. Neural Computation, 1(4):541–551,
1989. 1

[20] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998. 2, 5

[21] C. Leng, J. Wu, J. Cheng, X. Bai, and H. Lu. Online sketching hash-
ing. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2503–2511, 2015. 2

Appendix A: Additional Results

In the submission, we report the performance after quan-
tizing all the convolutional layers in AlexNet, and quan-
tizing all the full-connected layers in CaffeNet. Here, we
present experimental results for some other settings.

Quantizing Convolutional Layers in CaffeNet

We quantize all the convolutional layers in CaffeNet, and
the results are as demonstrated in Table 11. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.15% and 0.75%, compared to the original
CaffeNet.

Table 11. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CaffeNet, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.32×
4.32×
3.71×
4.27×
3.32×
4.32×
3.71×
4.27×

Top-1 Err. ↑
18.69%
32.84%
20.08%
35.48%
1.22%
2.44%
1.57%
2.30%

Top-5 Err. ↑
16.73%
33.55%
18.31%
37.82%
0.97%
1.83%
1.12%
1.71%

Quantizing Convolutional Layers in CNN-S

We quantize all the convolutional layers in CNN-S, and
the results are as demonstrated in Table 12. Furthermore,
we ﬁne-tune the quantized CNN model learned with error
correction (C′
s = 8, K = 128), and the increase of top-1/5
error rates are 1.24% and 0.63%, compared to the original
CNN-S.

Table 12. Comparison on the speed-up rates and the increase of
top-1/5 error rates for accelerating all the convolutional layers in
CNN-S, without ﬁne-tuning.

Method

Q-CNN

Q-CNN
(EC)

Para.
4/64
6/64
6/128
8/128
4/64
6/64
6/128
8/128

Speed-up
3.69×
5.17×
4.78×
5.92×
3.69×
5.17×
4.78×
5.92×

Top-1 Err. ↑
19.87%
45.74%
27.86%
46.18%
1.60%
3.49%
2.07%
3.42%

Top-5 Err. ↑
16.77%
48.67%
25.09%
50.26%
0.92%
2.32%
1.32%
2.17%

Quantizing Fully-connected Layers in AlexNet

We quantize all the fully-connected layers in AlexNet,

and the results are as demonstrated in Table 13.

Quantizing Fully-connected Layers in CNN-S

We quantize all the fully-connected layers in CNN-S,

Method

Table 13. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in AlexNet, without ﬁne-tuning.
Compression
Para.
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32
13.96×
2/16
19.14×
3/16
15.25×
3/32
18.71×
4/32

Top-5 Err. ↑
0.27%
0.64%
0.33%
0.69%
0.20%
0.22%
0.21%
0.38%

Top-1 Err. ↑
0.25%
0.77%
0.54%
0.71%
0.14%
0.40%
0.40%
0.46%

Q-CNN
(EC)

Q-CNN

Method

Table 14. Comparison on the compression rates and the increase of
top-1/5 error rates for compressing all the fully-connected layers
in CNN-S, without ﬁne-tuning.
Para.
2/16
3/16
3/32
4/32
2/16
3/16
3/32
4/32

Compression
14.37×
20.15×
15.79×
19.66×
14.37×
20.15×
15.79×
19.66×

Top-5 Err. ↑
0.07%
0.22%
0.11%
0.27%
0.14%
0.24%
0.11%
0.27%

Top-1 Err. ↑
0.22%
0.45%
0.21%
0.35%
0.36%
0.43%
0.29%
0.56%

Q-CNN
(EC)

Q-CNN

Appendix B: Optimization in Section 3.3.2

Assume we have N images to learn the quantization of a
convolutional layer. For image In, we denote its input fea-
ture maps as Sn ∈ Rds×ds×Cs and response feature maps
as Tn ∈ Rdt×dt×Ct, where ds, dt are the spatial sizes and
Cs, Ct are the number of feature map channels. We use
ps and pt to denote the spatial location in the input and re-
sponse feature maps. The spatial location in the convolu-
tional kernels is denoted as pk.

To learn quantization with error correction for the con-

volutional layer, we attempt to optimize:

2

}

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
m

X
n,pt

X
(pk,ps)

(D(m)B(m)

pk )T S(m)

min
{D(m)},{B(m)
pk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(14)
where Dm is the m-th sub-codebook, and B(m)
is the cor-
pk
responding sub-codeword assignment indicator for the con-
volutional kernels at spatial location pk.

n,ps − Tn,pt

Similar to the fully-connected layer, we adopt a block co-
ordinate descent approach to solve this optimization prob-
lem. For the m-th subspace, we ﬁrstly deﬁne its residual
feature map as:

R(m)

n,pt = Tn,pt − X
(pk,ps)

X
m′6=m

(D(m′

)B(m′
pk

)

)T S(m′
)
n,ps

(15)

and the results are as demonstrated in Table 14.

and then the optimization in the m-th subspace can be re-

formulated as:

min
D(m),{B(m)
pk }

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(16)
Update D(m). With the assignment indicator {B(m)
pk }

n,ps − R(m)
n,pt

pk )T S(m)

(D(m)B(m)

X
(pk,ps)

X
n,pt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ﬁxed, we let:

Lk,pk = {ct|B(m)

pk (k, ct) = 1}

(17)

We greedily update each sub-codeword in the m-th sub-
codebook D(m) in a sequential style. For the k-th sub-
codeword, we compute the corresponding residual feature
map as:

Q(m)

n,pt,k(ct) = R(m)

n,pt (ct) − X
(pk ,ps)

X
k′6=k

X
ct∈Lk′,pk

D(m)T
k′

S(m)
n,ps

(18)

and then we can alternatively optimize:

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
n,pt

min
D(m)
k

D(m)T

X
(pk,ps)

2
(cid:13)
(cid:13)
n,pt,k(ct)
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
(19)
which can be transformed into a least square problem. By
solving it, we can update the k-th sub-codeword.

n,ps − Q(m)
S(m)

X
ct∈Lk,pk

Update {B(m)

pk }. We greedily update the sub-codeword
assignment at each spatial location in the convolutional ker-
nels in a sequential style. For the spatial location pk, we
compute the corresponding residual feature map as:

P (m)
n,pt,pk = R(m)

(D(m)B(m)

p′
k

)T S(m)
n,p′
s

(20)

n,pt − X
(p′
k,p′
s)
pk6=pk

and then the optimization can be re-written as:

min
B(m)
pk

X
n,pt

(D(m)B(m)
(cid:13)
(cid:13)
(cid:13)

pk )T S(m)

n,ps − P (m)

2
n,pt,pk (cid:13)
(cid:13)
F
(cid:13)

(21)

Since B(m)
pk ∈ {0, 1}K is an indicator vector (only one non-
zero entry), we can exhaustively try all sub-codewords and
select the optimal one that minimize the objective function.

