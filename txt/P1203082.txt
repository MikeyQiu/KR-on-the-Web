0
2
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
7
2
5
0
.
2
1
9
1
:
v
i
X
r
a

MineGAN: effective knowledge transfer from GANs to target domains with few
images

Yaxing Wang1, Abel Gonzalez-Garcia1, David Berga1
Luis Herranz1, Fahad Shahbaz Khan2,3, Joost van de Weijer1
1 Computer Vision Center, Universitat Aut`onoma de Barcelona, Spain
2 Inception Institute of Artiﬁcial Intelligence, UAE 3 CVL, Link¨oping University, Sweden
{yaxing,agonzalez,dberga,lherranz,joost}@cvc.uab.es, fahad.khan@liu.se

Abstract

One of the attractive characteristics of deep neural net-
works is their ability to transfer knowledge obtained in one
domain to other related domains. As a result, high-quality
networks can be trained in domains with relatively little
training data. This property has been extensively studied
for discriminative networks but has received signiﬁcantly
less attention for generative models. Given the often enor-
mous effort required to train GANs, both computationally
as well as in the dataset collection, the re-use of pretrained
GANs is a desirable objective. We propose a novel knowl-
edge transfer method for generative models based on min-
ing the knowledge that is most beneﬁcial to a speciﬁc target
domain, either from a single or multiple pretrained GANs.
This is done using a miner network that identiﬁes which
part of the generative distribution of each pretrained GAN
outputs samples closest to the target domain. Mining ef-
fectively steers GAN sampling towards suitable regions of
the latent space, which facilitates the posterior ﬁnetuning
and avoids pathologies of other methods such as mode col-
lapse and lack of ﬂexibility. We perform experiments on
several complex datasets using various GAN architectures
(BigGAN, Progressive GAN) and show that the proposed
method, called MineGAN, effectively transfers knowledge
to domains with few target images, outperforming existing
methods. In addition, MineGAN can successfully transfer
knowledge from multiple pretrained GANs. Our code is
available at: https:// github.com/ yaxingwang/ MineGAN.

1. Introduction

quality GANs require a signiﬁcant amount of training data
and time. For example, Progressive GANs [14] are trained
on 30K images and are reported to require a month of train-
ing on one NVIDIA Tesla V100. Being able to exploit
these high-quality pretrained models, not just to generate
the distribution on which they are trained, but also to com-
bine them with other models and adjust them to a target
distribution is a desirable objective. For instance, it might
be desirable to only generate women using a GAN trained
to generate men and women alike. Alternatively, one may
want to generate smiling people from two pretrained gener-
ative models, one for men and one for women. The focus
of this paper is on performing these operations using only a
small target set of images, and without access to the large
datasets used to pretrain the models.

Transferring knowledge to domains with limited data has
been extensively studied for discriminative models [7, 27,
28, 34], enabling the re-use of high-quality networks. How-
ever, knowledge transfer for generative models has received
signiﬁcantly less attention, possibly due to its great difﬁ-
culty, especially when transferring to target domains with
few images. Only recently,Wang et al. [35] studied ﬁnetun-
ing from a single pretrained generative model and showed
that it is beneﬁcial for domains with scarce data. How-
ever, Noguchi and Harada [26] observed that this technique
leads to mode collapse. Instead, they proposed to reduce
the number of trainable parameters, and only ﬁnetune the
learnable parameters for the batch normalization (scale and
shift) of the generator. Despite being less prone to over-
ﬁtting, their approach severely limits the ﬂexibility of the
knowledge transfer.

Generative adversarial networks (GANs) can learn the
complex underlying distribution of image collections [10].
They have been shown to generate high-quality realistic im-
ages [14, 15, 4] and are used in many applications includ-
ing image manipulation [13, 41], style transfer [9], com-
pression [33], and colorization [38]. It is known that high-

In this paper, we address knowledge transfer by adapt-
ing a trained generative model for targeted image genera-
tion given a small sample of the target distribution. We in-
troduce the process of mining of GANs. This is performed
by a miner network that transforms a multivariate normal
distribution into a distribution on the input space of the pre-

trained GAN in such a way that the generated images re-
semble those of the target domain. The miner network has
considerably fewer parameters than the pretrained GAN and
is therefore less prone to overﬁtting. The mining step pre-
disposes the pretrained GAN to sample from a narrower re-
gion of the latent distribution that is closer to the target do-
main, which in turn eases the subsequent ﬁnetuning step by
providing a cleaner training signal with lower variance (in
contrast to sampling from the whole source latent space as
in [35]). Consequently, our method preserves the adaptation
capabilities of ﬁnetuning while preventing overﬁtting.

Importantly, our mining approach enables transferring
from multiple pretrained GANs, which allows us to aggre-
gate information from multiple sources simultaneously to
generate samples akin to the target domain. We show that
these networks can be trained by a selective backpropaga-
tion procedure. Our main contributions are:

• We introduce a novel miner network to steer the sam-
pling of the latent distribution of a pretrained GAN to
a target distribution determined by few images.

• We propose the ﬁrst method to transfer knowledge
from multiple GANs to a single generative model.
• We outperform existing competitors on a variety of set-
tings, including transferring knowledge from uncondi-
tional, conditional, and multiple GANs.

2. Related work

Generative adversarial networks. GANs consists of two
modules: generator and discriminator [10]. The generator
aims to generate images to fool the discriminator, while the
discriminator aims to distinguish generated from real im-
ages. Training GANs was initially difﬁcult, due to mode
collapse and training instability. Several methods focus on
addressing these problems [11, 31, 21, 3, 22], while an-
other major line of research aims to improve the architec-
tures to generate higher quality images [29, 6, 14, 16, 4].
For example, Progressive GAN [14] generates better im-
ages by synthesizing them progressively from low to high-
resolution. Finally, BigGAN [4] successfully performs con-
ditional high-realistic generation from ImageNet [5].

Transfer learning for GANs. While knowledge trans-
fer has been widely studied for discriminative models in
computer vision [7, 28, 27, 34], only a few works have ex-
plored transferring knowledge for generative models [26,
35]. Wang et al. [35] investigated ﬁnetuning of pretrained
GANs, leading to improved performance for target domains
with limited samples. This method, however, suffers from
mode collapse and overﬁtting, as it updates all parame-
ters of the generator to adapt to the target domain. Re-
cently, Noguchi and Harada [26] proposed to only update
the batch normalization parameters. Although less suscep-
tible to mode collapse, this approach signiﬁcantly reduces

the adaptation ﬂexibility of the model since changing only
the parameters of the batch normalization permits for style
changes but is not expected to function when shape needs
to be changed. They also replaced the GAN loss with a
mean square error loss. As a result, their model only learns
the relationship between latent vectors and sparse training
samples, requiring the input noise distribution to be trun-
cated during inference to generate realistic samples. The
proposed MineGAN does not suffer from this drawback, as
it learns how to automatically adapt the input distribution.
In addition, we are the ﬁrst to consider transferring knowl-
edge from multiple GANs to a single target domain.
Iterative image generation. Nguyen et al. [25] have in-
vestigated training networks to generate images that maxi-
mize the activation of neurons in a pretrained classiﬁcation
network.
In a follow-up approach [24] that improves the
diversity of the generated images, they use this technique
to generate images of a particular class from a pretrained
classiﬁer network. In principle, these works do not aim at
transferring knowledge to a new domain, and can instead
only be applied to generate a distribution that is exactly de-
scribed by one of the class labels of the pretrained classi-
ﬁer network. Another major difference is that the genera-
tion at inference time of each image is an iterative process
of successive backpropagation updates until convergence,
whereas our method is feedforward during inference.

3. Mining operations on GANs

Assume we have access to one or more pretrained GANs
and wish to use their knowledge to train a new GAN for a
target domain with few images. For clarity’s sake, we ﬁrst
introduce mining from a single GAN in Section 3.2, but
our method is general for an arbitrary number of pretrained
GANs, as explained in Section 3.3. Then, we show how the
miners can be used to train new GANs (Section 3.4).

3.1. GAN formulation

Let pdata(x) be a probability distribution over real data
x determined by a set of real images D, and let pz(z) be a
prior distribution over an input noise variable z. The gener-
ator G is trained to synthesize images given z ∼ pz(z) as
input, inducing a generative distribution pg(x) that should
approximate the real data distribution pdata(x). This is
achieved through an adversarial game [10], in which a dis-
criminator D aims to distinguish between real images and
images generated by G, while the generator tries to gener-
ate images that fool D. In this paper, we follow WGAN-
GP [11], which provides better convergence properties by
using the Wasserstein loss [3] and a gradient penalty term
(omitted from our formulation for simplicity). The discrim-
inator (or critic) and generator losses are deﬁned as follows:

LD = Ez∼pz(z)[D(G(z))] − Ex∼pdata(x)[D(x)],

(1)

Figure 1: (a) Intuition behind our approach for a simple case. Mining shifts the prior input distribution towards the most promising regions
with respect to given target data DT . In practice, the input distribution is much more complex. (b) Architecture implementing the proposed
mining operation on a single GAN. Miner M identiﬁes the relevant regions of the prior distribution so that generated samples are close to
the target data DT . Note that during the ﬁrst stage, when training the miner, the generator remains ﬁxed. In a second stage, we ﬁnetune
the miner, generator and discriminator together. (c) Training setup for multiple generators. Miners M1,...,MN identify subregions of the
pretrained generators while selector S learns the sampling frequencies of the various generators.

(2)

LG = −Ez∼pz(z)[D(G(z))].
We also consider families of pretrained generators {Gi}.
Each Gi has the ability to synthesize images given input
noise z ∼ pi
z(z). For simplicity and without loss of gener-
ality, we assume the prior distributions are Gaussian, i.e.
pi
z(z) = N (z|µi, Σi). Each generator Gi(z) induces a
learned generative distribution pi
g(x), which approximates
the corresponding real data distribution pi
data(x) over real
data x given by the set of source domain images Di.

3.2. Mining from a single GAN

We want to approximate a target real data distribution
pT
data(x) induced by a set of real images DT , given a critic
D and a generator G, which have been trained to approxi-
mate a source data distribution pdata(x) via the generative
distribution pg(x). The mining operation learns a new gen-
erative distribution pT
g (x) by ﬁnding those regions in pg(x)
that better approximate the target data distribution pT
data(x)
while keeping G ﬁxed. In order to ﬁnd such regions, min-
ing actually ﬁnds a new prior distribution pT
z (z) such that
samples G(z) with z ∼ pT
z (z) are similar to samples from
pT
data(x) (see Fig. 1a). For this purpose, we propose a
new GAN component called miner, implemented by a small
multilayer perceptron M . Its goal is to transform the orig-
inal input noise variable u ∼ pz(u) to follow a new, more
suitable prior that identiﬁes the regions in pg(x) that most
closely align with the target distribution.

Our full method acts in two stages. The ﬁrst stage steers
the latent space of the ﬁxed generator G to suitable areas for
the target distribution. We refer to the ﬁrst stage as Mine-
GAN (w/o FT) and present the proposed mining architec-
ture in Fig. 1b. The second stage updates the weights of the
generator via ﬁnetuning (G is no longer ﬁxed). MineGAN
refers to our full method including ﬁnetuning.

Miner M acts as an interface between the input noise
variable and the generator, which remains ﬁxed during
training. To generate an image, we ﬁrst sample u ∼ pz(u),

transform it with M and then input the transformed vari-
able to the generator, i.e. G(M (u)). We train the model
adversarially: the critic D aims to distinguish between fake
images output by the generator G(M (u)) and real images
x from the target data distribution pT
data(x). We implement
this with the following modiﬁcation on the WGAN-GP loss:

D = Eu∼pz(u)[D(G(M (u)))]−E
LM

x∼pT

data(x)[D(x)], (3)

G = −Eu∼pz(u)[D(G(M (u)))].
LM

(4)

The parameters of G are kept unchanged but the gradients
are backgropagated all the way to M to learn its parameters.
This training strategy will gear the miner towards the most
promising regions of the input space, i.e. those that generate
images close to DT . Therefore, M is effectively mining
the relevant input regions of prior pz(u) and giving rise to
a targeted prior pT
z (z), which will focus on these regions
while ignoring other ones that lead to samples far off the
target distribution pT

data(x).

We distinguish two types of targeted generation: on-
manifold and off-manifold. In the on-manifold case, there
is a signiﬁcant overlap between the original distribution
pdata(x) and the target distribution pT
data(x). For exam-
ple, pdata(x) could be the distribution of human faces (both
male and female) while pT
data(x) includes female faces
only. On the other hand, in off-manifold generation, the
overlap between the two distributions is negligible, e.g.
pT
data(x) contains cat faces. The off-manifold task is ev-
idently more challenging as the miner needs to ﬁnd sam-
ples out of the original distribution (see Fig. 4). Speciﬁ-
cally, we can consider the images in D to lie on a high-
dimensional image manifold that contains the support of the
real data distribution pdata(x) [2]. For a target distribution
farther away from pdata(x), its support will be more disjoint
from the original distribution’s support, and thus its samples
might be off the manifold that contains D.

3.3. Mining from multiple GANs

In the general case, the mining operation is applied on
multiple pretrained generators. Given target data DT , the
task consists in mining relevant regions from the induced
generative distributions learned by a family of N genera-
tors {Gi}. In this task, we do not have access to the original
data used to train {Gi} and can only use target data DT .
Fig. 1c presents the architecture of our model, which ex-
tends the mining architecture for a single pretrained GAN
by including multiple miners and an additional component
called selector. In the following, we present this component
and describe the training process in detail.

Supersample.
In traditional GAN training, a fake mini-
batch is composed of fake images G(z) generated with dif-
ferent samples z ∼ pz(z). To construct fake minibatches
for training a set of miners, we introduce the concept of
supersample. A supersample S is a set of samples com-
posed of exactly one sample per generator of the family,
i.e. S = {Gi(z)|z ∼ pi
z(z); i = 1, ..., N }. Each mini-
batch contains K supersamples, which amounts to a total of
K × N fake images per minibatch.

Selector. The selector’s task is choosing which pretrained
model to use for generating samples during inference. For
instance, imagine that D1 is a set of ‘kitchen’ images and
D2 are ‘bedroom’ images, and let DT be ‘white kitchens’.
The selector should prioritize sampling from G1, as the
learned generative distribution p1
g(x) will contain kitchen
images and thus will naturally be closer to pT
data(x), the
target distribution of white kitchens. Should DT comprise
both white kitchens and dark bedrooms, sampling should be
proportional to the distribution in the data.

We model the selector as a random variable s following
a categorical distribution parametrized by p1, ..., pN with
pi > 0, (cid:80) pi = 1. We estimate the parameters of this
distribution as follows. The quality of each sample Gi(z)
is evaluated by a single critic D based on its critic value
D(Gi(z)). Higher critic values indicate that the generated
sample from Gi is closer to the real distribution. For each
supersample S in the minibatch, we record which generator
obtains the maximum critic value, i.e. arg maxi D(Gi(z)).
By accumulating over all K supersamples and normaliz-
ing, we obtain an empirical probability value ˆpi that reﬂects
how often generator Gi obtained the maximum critic value
among all generators for the current minibatch. We estimate
each parameter pi as the empirical average ˆpi in the last
1000 minibatches. Note that pi are learned during training
and ﬁxed during inference, where we apply a multinomial
sampling function to sample the index.

Critic and miner training. We now deﬁne the training
behavior of the remaining learnable components, namely
the critic D and miners {Mi}, when minibatches are com-
posed of supersamples. The critic aims to distinguish real

Figure 2: Application of mining in conditional setting (on Big-
GAN [4]). We apply an additional miner network to estimate the
class embedding. DT : target data, E: class embedding, l: label.

images from fake images. This is done by looking for ar-
tifacts in the fake images which distinguish them from the
real ones. Another less discussed but equally important task
of the critic is to observe the frequency of occurrence of im-
ages: if some (potentially high-quality) image occurs more
often among fake images than real ones, the critic will lower
its score, and thereby motivate the generator to lower the
frequency of occurrence of this image. Training the critic
by backpropagating from all images in the supersample pre-
vents it from assessing the frequency of occurrence of the
generated images and leading to unsatisfactory results em-
pirically. Therefore, the loss for multiple GAN mining is:

D = E{ui∼pi
LM

z(u)}[max

i

{D(Gi(Mi(ui)))}]

G = −E{ui∼pi
LM

z(u)}[max

i

−E

x∼pT

data(x)[D(x)]
{D(Gi(Mi(ui)))}].

(5)

(6)

As a result of the max operator we only backpropagate from
the generated image that obtained the highest critic score.
This allows the critic to assess the frequency of occurrence
correctly. Using this strategy, the critic can perform both
its tasks: boosting the quality of images as well as driving
the miner to closely follow the distribution of the target set.
Note that we initialize the single critic D with the pretrained
weights from one of the pretrained critics1.
Conditional GANs. So far, we have only used uncondi-
tional GANs. However, conditional GANs (cGANs), which
introduce an additional input variable to condition the gen-
eration to the class label, are used by the most successful
approaches [4, 37]. Here we extend our proposed Mine-
GAN to cGANs that condition on the batch normalization
layer [8, 4]2, more concretely, BigGAN [4] (Fig. 2 (left)).
First, a label l is mapped to an embedding vector by means
of a class embedding E, and then this vector is mapped to
layer-speciﬁc batch normalization parameters. The discrim-
inator is further conditioned via label projection [23]. Fig. 2
(right) shows how to mine BigGANs. Alongside the stan-
dard miner M z, we introduce a second miner network M c,
which maps from u to the embedding space, resulting in a
generator G(M c(u), M z(u)). The training is equal to that
of a single GAN and follows Eqs. 3 and 4.

1We empirically found that starting from any pretrained critic leads to

similar results (see Suppl. Mat. (Sec. F))

2See Suppl. Mat. (Sec. D) for results on another type of conditioning.

3.4. Knowledge transfer with MineGAN

The underlying idea of mining is to predispose the pre-
trained model to the target distribution by reducing the
divergence between source and target distributions. The
miner network contains relatively few parameters and is
therefore less prone to overﬁtting, which is known to oc-
cur when directly ﬁnetuning the generator G [26]. We ﬁ-
nalize the knowledge transfer to the new domain by ﬁne-
tuning both the miner M and generator G (by releasing
its weights). The risk of overﬁtting is now diminished as
the generative distribution is closer to the target, requiring
thus a lower degree of parameter adaptation. Moreover, the
training is substantially more efﬁcient than directly ﬁnetun-
ing the pretrained GAN [35], where synthesized images are
not necessarily similar to the target samples. A mined pre-
trained model makes the sampling more effective, leading
to less noisy gradients and a cleaner training signal.

4. Experiments

We ﬁrst introduce the used evaluation measures and ar-
chitectures. Then, we evaluate our method for knowledge
transfer from unconditional GANs, considering both single
and multiple pretrained generators. Finally, we assess trans-
fer learning from conditional GANs. Experiments focus on
transferring knowledge to target domains with few images.
Evaluation measures.
We employ the widely used
Fr´echet Inception Distance (FID) [12] for evaluation. FID
measures the similarity between two sets in the embed-
ding space given by the features of a convolutional neu-
ral network. More speciﬁcally, it computes the differences
between the estimated means and covariances assuming a
multivariate normal distribution on the features. FID mea-
sures both the quality and diversity of the generated images
and has been shown to correlate well with human percep-
tion [12]. However, it suffers from instability on small
datasets. For this reason, we also employ Kernel Maxi-
mum Mean Discrepancy (KMMD) with a Gaussian kernel
and Mean Variance (MV) for some experiments [26]. Low
KMMD values indicate high quality images, while high val-
ues of MV indicate more image diversity.
Baselines. We compare our method with the following
baselines. TransferGAN [35] directly updates both the gen-
erator and the discriminator for the target domain. VAE [18]
is a variational autoencoder trained following [26], i.e. fully
supervised by pairs of latent vectors and training images.
BSA [26] updates only the batch normalization parame-
ters of the generator instead of all the parameters. DGN-
AM [25] generates images that maximize the activation of
neurons in a pretrained classiﬁcation network. PPGN [24]
improves the diversity of DGN-AM by of adding a prior to
the latent code via denoising autoencoder. Note that both of
DGN-AM and PPGN require the target domain label, and

Figure 3: Results for off-manifold generation of MineGAN(w/o
FT). We generate 20 samples of digits ‘5’, ‘8’ or ‘9’.

thus we only include them in the conditional setting.

Architectures. We apply mining to Progressive GAN [14],
SNGAN [22], and BigGAN [4]. The miner has two fully
connected layers for MNIST and four layers for all other
experiments. More training details in Suppl. Mat. (Sec. A).

4.1. Knowledge transfer from unconditional GANs
MNIST dataset. We show results on MNIST to illustrate
the functioning of the miner 3. We use 1000 images of size
28 × 28 as target data. We test mining for off-manifold
targeted image generation. In off-manifold targeted genera-
tion, G is pre-trained to synthesize all MNIST digits except
for the target one, e.g. G generates 0-8 but not 9. The re-
sults in Fig. 3 are after training only the miner, without an
additional ﬁnetuning step. Interestingly, the miner manages
to steer the generator to output samples that resemble the
target digits, by merging patterns from other digits in the
source set. For example, digit ‘9’ frequently resembles a
modiﬁed 4 while ‘8’ heavily borrows from 0s and 3s. Some
digits can be more challenging to generate, for example, ‘5’
is generally more distinct from other digits and thus in more
cases the resulting sample is confused with other digits such
as ‘3’. In conclusion, even though target classes are not in
the training set of the pretrained GAN, still similar exam-
ples might be found on the manifold of the generator.

Single pretrained model. We start by transferring knowl-
edge from a Progressive GAN trained on CelebA [20]. We
evaluate the performance on target datasets of varying size,
using 1024×1024 images. We consider two target domains:
on-manifold, FFHQ women [16] and off-manifold, FFHQ
children face [16]. We refer as MineGAN to our full method
including ﬁnetuning, whereas MineGAN (w/o FT) refers to
only applying mining (ﬁxed generator). We use training
from Scratch, and the TransferGAN method of [35] as base-
lines. Figure 5 shows the performance in terms of FID and
KMMD as a function of the number of images in the target
domain. MineGAN outperforms all baselines. For the on-
manifold experiment, MineGAN (w/o FT) outperforms the
baselines, and results are further improved with additional
ﬁnetuning. Interestingly, for the off-manifold experiment,
MineGAN (w/o FT) obtains only slightly worse results than
TransferGAN, showing that the miner alone already man-
ages to generate images close to the target domain. Fig. 4
shows images generated when the target data contains 100
training images. Training the model from scratch results in

3We add quantitative results on MNIST in Suppl. Mat. (Sec. D)

Figure 4: Results: (Left) On-manifold (CelebA→FFHQ women), (Right) Off-manifold (CelebA→FFHQ children). Based on pretrained
Progressive GAN. The images in red boxes suffer from overﬁtting. See Suppl. Mat. (Figs. 11-14 and Sec. E) for more examples.

Figure 5: KMMD and FID on CelebA→FFHQ women (left) and CelebA→FFHQ children (right).

overﬁtting, a pathology also occasionally suffered by Trans-
ferGAN. MineGAN, in contrast, generates high-quality im-
ages without overﬁtting and images are sharper, more di-
verse, and have more realistic ﬁne details.

We also compare here with Batch Statistics Adaptation
(BSA) [26] using the same settings and architecture, namely
SNGAN [22]. They performed knowledge transfer from a
pretrained SNGAN on ImageNet [19] to FFHQ [16] and
to Anime Face [1]. Target domains have only 25 images
of size 128×128. We added our results to those reported
in [26] in Fig. 6 (bottom). Compared to BSA, Mine-
GAN (w/o FT) obtains similar KMMD scores, showing
that generated images obtain comparable quality. Mine-
GAN outperforms BSA both in KMMD score and Mean
Variance. The qualitative results (shown in Fig. 6 (top))
clearly show that MineGAN outperforms the baselines.
BSA presents blur artifacts, which are probably caused by
the mean square error used to optimize their model.

Multiple pretrained models. We now evaluate the gen-
eral case for MineGAN, where there is more than one pre-
trained model to mine from. We start with two pretrained
Progressive GANs: one on Cars and one on Buses, both
from the LSUN dataset [36]. These pretrained networks

Method

From scratch
TransferGAN [35]
VAE [18]
BSA [26]
MineGAN (w/o FT)
MineGAN

FFHQ

Anime Face

KMMD
0.890
0.346
0.744
0.345
0.349
0.337

MV
-
0.506
-
0.785
0.774
0.812

KMMD
0.753
0.347
0.790
0.342
0.347
0.334

MV
-
0.785
-
0.908
0.891
0.934

Figure 6: Results for various knowledge transfer methods. (Top)
Generated images. (Bottom) KMMD and MV.

generate cars and buses of a variety of different colors. We
collect a target dataset of 200 images (of 256 × 256 res-
olution) of red vehicles, which contains both red cars and
red buses. We consider three target sets with different car-
bus ratios (0.3:0.7, 0.5:0.5, and 0.7:0.3) which allows us to
evaluate the estimated probabilities pi of the selector. To

Figure 7: Results: {car, bus} → red vehicles (left) and {Living room, Bridge, Church, Kitchen} → Tower (right). Based on pretrained
Progressive GAN. For TransferGAN we show the pretrained model between parentheses. More examples in Suppl, Mat (Sec. F).

Method

→ Red vehicle → Tower → Bedroom

Scratch
TransferGAN (car)
TransferGAN (bus)
TransferGAN (livingroom)
TransferGAN (church)
MineGAN (w/o FT)
MineGAN

190 / 185 / 196
76.9 / 72.4 / 75.6
72.8 / 71.3 / 73.5
-
-
67.3 / 65.9 / 65.8
61.2 / 59.4 / 61.5

Estimated pi

Car
Bus
Living room
Kitchen
Bridge
Church

0.34 / 0.48 / 0.64
0.66 / 0.52 / 0.36
-
-
-
-

176
-
-
78.9
73.8
69.2
62.4

-
-
0.07
0.06
0.42
0.45

181
-
-
65.4
71.5
58.9
54.7

-
-
0.45
0.40
0.08
0.07

Table 1: Results for {Car, Bus} → Red vehicles with three differ-
ent target data distributions (ratios cars:buses are 0.3:0.7, 0.5:0.5
and 0.7:0.3) and {Living room, Bridge, Church, Kitchen} →
Tower/Bedroom.
(Top) FID scores between real and generated
samples. (Bottom) Estimated probabilities pi for each model.

successfully generate all types of red vehicle, knowledge
needs to be transferred from both pre-trained models.

Fig. 7 shows the synthesized images. As expected, the
limited amount of data makes training from scratch result in
overﬁtting. TransferGAN [35] produces only high-quality
output samples for one of the two classes (the class that
coincides with the pretrained model) and it cannot extract
knowledge from both pretrained GANs. On the other hand,
MineGAN generates high-quality images by successfully
transferring the knowledge from both source domains si-
multaneously. Table 1 (top rows) quantitatively validates

Method

Off-manifold

On-manifold

Label

FID/KMMD Label

FID/KMMD

Time (ms)

Scratch
TransferGAN
DGN-AM
PPGN
MineGAN (w/o FT)
MineGAN

No
No
Yes
Yes
No
No

190 / 0.96
89.2 / 0.53
214 / 0.98
139 / 0.56
82.3 / 0.47
78.4 / 0.41

No
Yes
Yes
Yes
No
No

187 / 0.93
58.4 / 0.39
180 / 0.95
127 / 0.47
61.8 / 0.32
52.3 / 0.25

5.1
5.1
3020
3830
5.2
5.2

Table 2: Distance between real data and generated samples as
measured by FID score and KMMD value. The off-manifold re-
sults correspond to ImageNet → Places365, and the on-manifold
results correspond to ImageNet → ImageNet. We also indicate
whether the method requires the target label. Finally, we show the
inference time for the various methods in milliseconds.

that our method outperfroms TransferGAN with a signiﬁ-
cantly lower FID score. Furthermore, the probability distri-
bution predicted by the selector, reported in Table 1 (bottom
rows), matches the class distribution of the target data.

To demonstrate the scalability of MineGAN with multi-
ple pretrained models, we conduct experiments using four
different generators, each trained on a different LSUN cat-
egory including Livingroom, Kitchen, Church, and Bridge.
We consider two different off-manifold target datasets, one
with Bedroom images and one with Tower images, both
containing 200 images. Table 1 (top) shows that our method
obtains signiﬁcantly better FID scores even when we choose
the most relevant pretrained GAN to initialize training for
TransferGAN. Table 1 (bottom) shows that the miner identi-
ﬁes the relevant pretrained models, e.g. transferring knowl-
edge from Bridge and Church for the target domain Tower.
Finally, Fig. 7 (right) provides visual examples.

Figure 8: Results for conditional GAN. (Left) Off-manifold (ImageNet→Places365). (Right) On-manifold (ImageNet→ImageNet).

4.2. Knowledge transfer from conditional GANs

Here we transfer knowledge from a pretrained con-
ditional GAN (see Section 3.3). We use BigGAN [4],
which is trained using ImageNet [30], and evaluate on
two target datasets: on-manifold (ImageNet: cock, tape
player, broccoli, ﬁre engine, harvester) and off-manifold
(Places365 [39]: alley, arch, art gallery, auditorium, ball-
room). We use 500 images per category. We compare Mine-
GAN with training from scratch, TransferGAN [35], and
two iterative methods: DGN-AM [25] and PPGN [24] 4. It
should be noted that both DGN-AM [25] and PPGN [24] are
based on a less complex GAN (equivalent to DCGAN [29]).
Therefore, we expect these methods to exhibit results of in-
ferior quality, and so the comparison here should be inter-
preted in the context of GAN quality progress. However,
we would like to stress that both DGN-AM and PPGN do
not aim to transfer knowledge to new domains. They can
only generate samples of a particular class of a pretrained
classiﬁer network, and they have no explicit loss ensuring
that the generated images follow a target distribution.

Fig. 8 shows qualitative results for the different methods.
As in the unconditional case, MineGAN produces very re-
alistic results, even for the challenging off-manifold case.
Table 2 presents quantitative results in terms of FID and
KMMD. We also indicate whether each method uses the
label of the target domain class. Our method obtains the
best scores for both metrics, despite not using target label
information. PPGN performs signiﬁcantly worse than our
method. TransferGAN has a large performance drop for the
off-manifold case, for which it cannot use the target label as
it is not in the pretrained GAN (see [35] for details).

4We were unable to obtain satisfactory results with BSA [26] in this

setting (images suffered from blur artifacts) and have excluded it here.

Another important point regarding DGN-AM and PPGN
is that each image generation during inference is an iter-
ative process of successive backpropagation updates until
convergence, whereas our method is feedforward. For this
reason, we include in Table 2 the inference running time of
each method, using the default 200 iterations for DGN-AM
and PPGN. All timings have been computed with a CPU In-
tel Xeon E5-1620 v3 @ 3.50GHz and GPU NVIDIA RTX
2080 Ti. We can clearly observe that the feedforward meth-
ods (TransferGAN and ours) are three orders of magnitude
faster despite being applied on a more complex GAN [4].

5. Conclusions

We presented a model for knowledge transfer for genera-
tive models. It is based on a mining operation that identiﬁes
the regions on the learned GAN manifold that are closer to
a given target domain. Mining leads to more effective and
efﬁcient ﬁne tuning, even with few target domain images.
Our method can be applied to single and multiple pretrained
GANs. Experiments with various GAN architectures (Big-
GAN, Progressive GAN, and SNGAN) on multiple datasets
demonstrated its effectiveness. Finally, we demonstrated
that MineGAN can be used to transfer knowledge from mul-
tiple domains.

Acknowledgements. We acknowledge the support from
Huawei Kirin Solution,
the Spanish projects TIN2016-
the CERCA Pro-
79717-R and RTI2018-102285-A-I0,
gram of the Generalitat de Catalunya, and the EU Marie
Sklodowska-Curie grant agreement No.6655919.

References

[1] Anonymous, Danbooru community, Gwern Branwen, and
Aaron Gokaslan. Danbooru2018: A large-scale crowd-
sourced and tagged anime illustration dataset. https://www.
gwern.net/Danbooru2018, 2019.

[2] Mart´ın Arjovsky and L´eon Bottou. Towards principled meth-
ICLR,

ods for training generative adversarial networks.
2017.

[3] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.

Wasserstein gan. ICLR, 2017.

[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high ﬁdelity natural image synthesis.
In ICLR, 2019.

[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, pages 248–255. Ieee, 2009.

[6] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep
generative image models using a laplacian pyramid of adver-
sarial networks. In NeurIPS, pages 1486–1494, 2015.
[7] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,
Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep
convolutional activation feature for generic visual recogni-
tion. In ICML, pages 647–655, 2014.

[8] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
A learned representation for artistic style. In ICLR, 2017.
[9] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-
In
age style transfer using convolutional neural networks.
CVPR, pages 2414–2423, 2016.

[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
In NeurIPS,
Yoshua Bengio. Generative adversarial nets.
pages 2672–2680, 2014.

[11] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Improved training of

Dumoulin, and Aaron C Courville.
wasserstein gans. In NeurIPS, pages 5767–5777, 2017.
[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NeurIPS, pages 6626–6637, 2017.

[13] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with conditional adver-

Efros.
sarial networks. In CVPR, pages 1125–1134, 2017.

[14] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. ICLR, 2017.

[15] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
CVPR, 2019.

[16] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR, pages 4401–4410, 2019.

[17] Diederik Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. ICLR, 2014.

[18] Diederik P Kingma and Max Welling. Auto-encoding varia-

tional bayes. ICLR, 2014.

[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NeurIPS, pages 1097–1105, 2012.

[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
In ICCV, pages

Deep learning face attributes in the wild.
3730–3738, 2015.

[21] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares generative
adversarial networks. In ICCV, pages 2794–2802, 2017.
[22] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. In ICLR, 2018.

[23] Takeru Miyato and Masanori Koyama. cgans with projection

discriminator. ICLR, 2018.

[24] Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovit-
skiy, and Jason Yosinski. Plug & play generative networks:
Conditional iterative generation of images in latent space. In
CVPR, pages 4467–4477, 2017.

[25] Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas
Brox, and Jeff Clune. Synthesizing the preferred inputs for
neurons in neural networks via deep generator networks. In
NeurIPS, pages 3387–3395, 2016.
[26] Atsuhiro Noguchi and Tatsuya Harada.

Image generation
ICCV,

from small datasets via batch statistics adaptation.
2019.

[27] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic.
Learning and transferring mid-level image representations
using convolutional neural networks. In CVPR, pages 1717–
1724. IEEE, 2014.

[28] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-

ing. TKDE, 22(10):1345–1359, 2010.

[29] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gener-
ative adversarial networks. In ICLR, 2016.

[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Imagenet large
Aditya Khosla, Michael Bernstein, et al.
IJCV, 115(3):211–252,
scale visual recognition challenge.
2015.

[31] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In NeurIPS, pages 2234–2242, 2016.
[32] Konstantin Shmelkov, Cordelia Schmid, and Karteek Ala-
hari. How good is my gan? In ECCV, pages 213–229, 2018.
[33] Michael Tschannen, Eirikur Agustsson, and Mario Lucic.
Deep generative models for distribution-preserving lossy
compression. In NeurIPS, pages 5929–5940, 2018.

[34] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko.
Simultaneous deep transfer across domains and tasks.
In
CVPR, pages 4068–4076, 2015.

[35] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de
Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu.
Transferring gans: generating images from limited data. In
ECCV, pages 218–234, 2018.

[36] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser, and Jianxiong Xiao. Lsun: Construction of a
large-scale image dataset using deep learning with humans
in the loop. arXiv preprint arXiv:1506.03365, 2015.

[37] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-
tus Odena. Self-attention generative adversarial networks.
ICML, 2018.

[38] Richard Zhang, Phillip Isola, and Alexei A Efros. Color-
ful image colorization. In ECCV, pages 649–666. Springer,
2016.

[39] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Object detectors emerge in deep scene
cnns. ICLR, 2014.

[40] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-
ralba, and Aude Oliva. Learning deep features for scene
recognition using places database. In NeurIPS, pages 487–
495, 2014.

[41] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, pages 2223–2232,
2017.

A. Architecture and training details

MNIST dataset. Our model contains a miner, a gener-
ator and a discriminator. For both unconditional and con-
ditional GANs, we use the same framework [11] to design
the generator and discriminator. The miner is composed of
two fully connected layers with the same dimensionality as
the latent space |z|. The visual results are computed with
|z| = 16; we found that the quantitative results improved
for larger |z| and choose |z| = 128.

We randomly initialize the weights of each miner follow-
ing a Gaussian distribution centered at 0 with 0.01 standard
deviation, and optimize the model using Adam [17] with
batch size of 64. The learning rate of our model is 0.0004,
with exponential decay rates of (β1, β2) = (0.5, 0.999).

In the conditional MNIST case, label c is a one-hot vec-
tor. This differs from the conditioning used for BigGAN [4]
explained in Section 3.3.

Here we extend MineGAN to this type of conditional
models by considering each possible conditioning as an in-
dependently pretrained generator and using the selector to
predict the conditioning label. Given a conditional gener-
ator G(c, z), we consider G(i, z) as Gi and apply the pre-
sented MineGAN approach for multiple pretrained gener-
ators on the family {G(i, z)| i = 1, ..., N }. The resulting
selector now chooses among the N classes of the model
rather than among N pretrained models, but the rest of the
MineGAN training remains the same, including the training
of N independent miners.

CelebA Women, FFHQ Children and LSUN (Tower
and Bedroom) datasets. We design the generator and dis-
criminator based on Progressive GANs [14]. Both networks
use a multi-scale technique to generate high-resolution im-
ages. Note we use a simple miner for tasks with dense
and narrow source domains. The miner comprises out of
four fully connected layers (8-64-128-256-512), each of
which is followed by a ReLU activation and pixel normal-

ization [14] except for last layer. We use a Gaussian distri-
bution centered at 0 with 0.01 standard deviation to initial-
ize the miner, and optimize the model using Adam [17] with
batch size of 4. The learning rate of our model is 0.0015,
with exponential decay rates of (β1, β2) = (0, 0.99).

FFHQ Face and Anime Face datasets. We use the
same network as [22], namely SNGAN. The miner consists
of three fully connected layers (8-32-64-128). We randomly
initialize the weights following a Gaussian distribution cen-
tered at 0 with 0.01 standard deviation. For this additional
set of experiments, we use Adam [17] with a batch size of
8, following a hyper parameter learning rate of 0.0002 and
exponential decay rate of (β1, β2) = (0, 0.9).

Imagenet and Places365 datasets. We use the pre-
trained BigGAN [4]. We ignore the projection loss in the
discriminator, since we do not have access to the label of
the target data. We employ a more powerful miner in or-
der to allocate more capacity to discover the regions re-
lated to target domain. The miner consists of two sub-
networks: miner M z and miner M c. Both M z and M c
are composed of four fully connected layers of sizes (128,
128)-(128, 128)-(128, 128)-(128, 128)-(128, 120) and (128,
128)-(128, 128)-(128, 128)-(128, 128)-(128, 128), respec-
tively. We use Adam [17] with a batch size of 256, and
learning rates of 0.0001 for the miner and the generator
and 0.0004 for the discriminator. The exponential decay
rates are (β1, β2) = (0, 0.999). We randomly initialize the
weights following a Gaussian distribution centered at 0 with
0.01 standard deviation.

The input of BigGAN [4] is a random latent vector and a
class label that is mapped to an embedding space. We there-
fore have two miner networks, the original one that maps to
the input latent space (M z) and a new one that maps to the
latent class embedding (M c). Note that since we have no
class label, the miner (M c) needs to learn what distribution
over the embeddings best represents the target data.

B. Evaluation metric details

Similarly to [26], we compute FID between 10,000 ran-
domly generated images and 10,000 real images, if possi-
ble. When the number of target image exceeds 10,000, we
randomly select a subset containing only 10,000. On the
other hand, if the target set contains fewer images, we cap
the amount of randomly generated images to this number to
compute the FID. Note we also consider KMMD to evalu-
ate the distance between generated images and real images
since FID suffers from instability on small datasets.

C. Ablation study

In this section, we evaluate the effect of each indepen-

dent contribution to MineGAN and their combinations.
Selection strategies. We ablate the use of the max oper-
ation in Eqs. (5) and (6), and replace it with a mean opera-

Figure 9: FID values for different number of fully con-
nected layers in the miner. Results are based on the pre-
trained BigGAN with target class Arch.

Method

MineGAN (mean)

MineGAN (max)

Car
Bus

0.51
0.49

0.34
0.66

Table 3: Estimated probabilities pi for {Car, Bus} → Red vehi-
cles for MineGAN with mean or max in Eqs. (5) and (6). The
actual data distribution is 0.3:0.7 (ratio cars:buses).

tion. We call this setting MineGAN (mean). In this case the
backpropagation is not only performed for the image with
the highest critic score but for all images. We hypothesized
that the max operation is necessary to correctly estimate the
probabilities used by the selector. We report the results in
Table 3, where we refer to our original model as MineGAN
(max). The probability distribution predicted by the selec-
tor indicates that MineGAN (mean) equally chooses both
pretrained models on Car and Bus, while MineGAN suc-
cessfully estimates the class distribution of the target data.

Miner Architecture. We performed an ablation study for
different variants of the miner based on pretrained BigGAN.
The miner always contains only fully connected layers, but
we experiment with varying the number of layers. In Fig. 9,
we show the results on off-manifold target class Arch from
Places365 [40]. Using more layers increases the perfor-
mance of the method. Besides, we ﬁnd that the results of
both 3 and 4 layers are similar, indicating that adding addi-
tional layers would only result in slight improvements for
our model. Therefore, in this paper, we use miners with 4
fully connected layers.

D. MNIST experiment

We expand the MNIST experiments presented in Sec-
tion 5.1 by providing a quantitative evaluation and includ-
ing results on conditional GANs. As evaluation measures,
we use FID (Section 5) and classiﬁer error [32]. To com-
pute classiﬁer error, we ﬁrst train a CNN classiﬁer on real

Figure 10: Results for unconditional off-manifold genera-
tion of digits ‘6’, ‘4’, ‘3’, ‘2’, ‘1’, ‘0’.

d

0
1
2
3
4
5
6
7
8
9

On-manifold

Off-manifold

Unconditional Conditional Unconditional Conditional

13.4 / 2.5
13.1 / 1.7
14.6 / 6.3
14.1 / 10.1
14.7 / 6.4
13.1 / 9.3
13.4 / 2.8
12.9 / 3.2
14.2 / 7.5
11.3 / 6.8

12.6 / 0.7
12.6 / 1.9
12.8 / 2.7
13.3 / 1.6
13.4 / 1.2
11.7 / 2.1
14.3 / 1.8
14.2 / 1.8
14.7 / 5.5
11.2 / 2.9

21.3 / 2.8
15.9 / 2.5
23.1 / 5.2
22.8 / 7.3
23.4 / 6.3
21.9 / 10.9
24 / 3.1
24.8 / 4.9
25.7 / 9.8
12.5 / 7.4

15.6 / 1.1
14.8 / 2.1
18.2 / 3.6
14.2 / 1.5
15.3 / 4.2
17.2 / 5.7
15.8 / 1.6
16.3 / 2.6
18.7 / 5.6
16.3 / 3.5

Average

13.5 / 5.7

13.1 / 2.2

21.5 / 6.0

16.2 / 3.2

Table 4: Quantitative results of mining on MNIST, expressed as
FID / classiﬁer error.

training data to distinguish between multiple classes (e.g.
digit classiﬁer). Then, we classify the generated images that
should belong to a particular class and measure the error as
the percentage of misclassiﬁed images. This gives us an es-
timation of how realistic and accurate the generated images
are in the context of targeted generation.

Table 4 presents the results for both unconditional and
conditional models, using a noise length of |z| = 128. The
relatively low error values indicate that the miner manages
to identify the correct regions for generating the target dig-
its. The conditional model offers better results than the un-
conditional one by selecting the target class more often. We
can also observe that the off-manifold task is more difﬁcult
than the on-manifold task, as indicated by the higher eval-
uation scores. However, the off-manifold scores are still
reasonably low, indicating that the miner manages to ﬁnd
suitable regions from other digits by mining local patterns
shared with the target. Overall, these results indicate the ef-
fectiveness of mining on MNIST for both types of targeted
In addition, in Fig. 10 we have added
image generation.
a visualization for the off-manifold MNIST classes which
were not already shown in Fig. 2.

E. Further results on CelebA

We provide additional results for the on-manifold ex-
periment CelebA→FFHQ women in Fig. 11, and the off-
manifold CelebA→FFHQ children in Fig. 12. In addition,
we have also performed an on-manifold experiment with
CelebA→CelebA women, whose results are provided in
Fig. 13.

F. Further results for LSUN

We provide additional results for the experiment ({Bus,
Car}) → Red vehicles in Fig. 16 and for the experiment
{Bedroom, Bridge, Church, Kitchen} → Tower/Bedroom
in Fig. 17. When applying MineGAN to multiple pretrained
GANs, we use one of the domains to initialize the weights
of the critic. In Fig. 17 we used Church to initialize the critic
in case of the target set Tower, and Kitchen to initialize the
critic for the target set Bedroom. We found this choice to be
of little inﬂuence on the ﬁnal results. When using Kitchen to
initialize the critic for target set Tower results change from
62.4 to 61.7. When using Church to initialize the critic for
target set Bedroom results change from 54.7 to 54.3.

Figure 11: (CelebA→FFHQ women). Based on pretrained Progressive GAN.

Figure 12: (CelebA→ FFHQ children). Based on pretrained Progressive GAN.

Figure 13: (CelebA→CelebA women). Based on pretrained Progressive GAN.

Figure 14: (Top) 100 women faces from HHFQ dataset. (Bottom) training of model from scratch: the images start with low
quality and iteratively overﬁt to a particular training image. Red boxes identify images which are remembered by the model
trained from scratch or from TransferGAN (see Fig. 4). Based on pretrained Progressive GAN.

Figure 15: 100 children faces from HHFQ dataset. Red boxes identify images which are remembered by the model trained
from scratch (see Fig. 4). Based on pretrained Progressive GAN.

Figure 16: ({bus, car}) →red vehicles. Based on pretrained Progressive GAN.

Figure 17: Results for unconditional GAN. (Top) (Livingroom, kitchen, bridge, church )→Tower. (Bottom) (Livingroom, kitchen, bridge,
church )→Bedroom. Based on pretrained Progressive GAN.

