Neural Message Passing for Quantum Chemistry

Justin Gilmer 1 Samuel S. Schoenholz 1 Patrick F. Riley 2 Oriol Vinyals 3 George E. Dahl 1

7
1
0
2
 
n
u
J
 
2
1
 
 
]

G
L
.
s
c
[
 
 
2
v
2
1
2
1
0
.
4
0
7
1
:
v
i
X
r
a

Abstract
Supervised learning on molecules has incredi-
ble potential to be useful in chemistry, drug dis-
covery, and materials science. Luckily, sev-
eral promising and closely related neural network
models invariant to molecular symmetries have
already been described in the literature. These
models learn a message passing algorithm and
aggregation procedure to compute a function of
their entire input graph. At this point, the next
step is to ﬁnd a particularly effective variant of
this general approach and apply it to chemical
prediction benchmarks until we either solve them
or reach the limits of the approach. In this pa-
per, we reformulate existing models into a sin-
gle common framework we call Message Pass-
ing Neural Networks (MPNNs) and explore ad-
ditional novel variations within this framework.
Using MPNNs we demonstrate state of the art re-
sults on an important molecular property predic-
tion benchmark; these results are strong enough
that we believe future work should focus on
datasets with larger molecules or more accurate
ground truth labels.

1. Introduction

The past decade has seen remarkable success in the use
of deep neural networks to understand and translate nat-
ural language (Wu et al., 2016), generate and decode com-
plex audio signals (Hinton et al., 2012), and infer fea-
tures from real-world images and videos (Krizhevsky et al.,
2012). Although chemists have applied machine learn-
ing to many problems over the years, predicting the prop-
erties of molecules and materials using machine learning
(and especially deep learning) is still in its infancy. To
date, most research applying machine learning to chemistry
tasks (Hansen et al., 2015; Huang & von Lilienfeld, 2016;

1Google Brain 2Google 3Google DeepMind. Correspon-
dence to: Justin Gilmer <gilmer@google.com>, George E. Dahl
<gdahl@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. A Message Passing Neural Network predicts quantum
properties of an organic molecule by modeling a computationally
expensive DFT calculation.

Rupp et al., 2012; Rogers & Hahn, 2010; Montavon et al.,
2012; Behler & Parrinello, 2007; Schoenholz et al., 2016)
has revolved around feature engineering. While neural net-
works have been applied in a variety of situations (Merk-
wirth & Lengauer, 2005; Micheli, 2009; Lusci et al., 2013;
Duvenaud et al., 2015), they have yet to become widely
adopted. This situation is reminiscent of the state of image
models before the broad adoption of convolutional neural
networks and is due, in part, to a dearth of empirical evi-
dence that neural architectures with the appropriate induc-
tive bias can be successful in this domain.

Recently, large scale quantum chemistry calculation and
molecular dynamics simulations coupled with advances in
high throughput experiments have begun to generate data
at an unprecedented rate. Most classical techniques do
not make effective use of the larger amounts of data that
are now available. The time is ripe to apply more power-
ful and ﬂexible machine learning methods to these prob-
lems, assuming we can ﬁnd models with suitable inductive
biases. The symmetries of atomic systems suggest neu-
ral networks that operate on graph structured data and are
invariant to graph isomorphism might also be appropriate
for molecules. Sufﬁciently successful models could some-
day help automate challenging chemical search problems
in drug discovery or materials science.

In this paper, our goal is to demonstrate effective ma-
chine learning models for chemical prediction problems

Neural Message Passing for Quantum Chemistry

that are capable of learning their own features from molec-
ular graphs directly and are invariant to graph isomorphism.
To that end, we describe a general framework for super-
vised learning on graphs called Message Passing Neural
Networks (MPNNs) that simply abstracts the commonali-
ties between several of the most promising existing neural
models for graph structured data, in order to make it easier
to understand the relationships between them and come up
with novel variations. Given how many researchers have
published models that ﬁt into the MPNN framework, we
believe that the community should push this general ap-
proach as far as possible on practically important graph
problems and only suggest new variations that are well
motivated by applications, such as the application we con-
sider here: predicting the quantum mechanical properties
of small organic molecules (see task schematic in ﬁgure 1).

In general, the search for practically effective machine
learning (ML) models in a given domain proceeds through
a sequence of increasingly realistic and interesting bench-
marks. Here we focus on the QM9 dataset as such a bench-
mark (Ramakrishnan et al., 2014). QM9 consists of 130k
molecules with 13 properties for each molecule which are
approximated by an expensive1 quantum mechanical simu-
lation method (DFT), to yield 13 corresponding regression
tasks. These tasks are plausibly representative of many im-
portant chemical prediction problems and are (currently)
difﬁcult for many existing methods. Additionally, QM9
also includes complete spatial information for the single
low energy conformation of the atoms in the molecule that
was used in calculating the chemical properties. QM9
therefore lets us consider both the setting where the com-
plete molecular geometry is known (atomic distances, bond
angles, etc.) and the setting where we need to compute
properties that might still be deﬁned in terms of the spa-
tial positions of atoms, but where only the atom and bond
information (i.e. graph) is available as input. In the lat-
ter case, the model must implicitly ﬁt something about the
computation used to determine a low energy 3D conforma-
tion and hopefully would still work on problems where it is
not clear how to compute a reasonable 3D conformation.

When measuring the performance of our models on QM9,
there are two important benchmark error levels. The ﬁrst
is the estimated average error of the DFT approximation
to nature, which we refer to as “DFT error.” The sec-
ond, known as “chemical accuracy,” is a target error that
has been established by the chemistry community. Esti-
mates of DFT error and chemical accuracy are provided
for each of the 13 targets in Faber et al. (2017). One im-
portant goal of this line of research is to produce a model
which can achieve chemical accuracy with respect to the

1By comparison, the inference time of the neural networks dis-

cussed in this work is 300k times faster.

true targets as measured by an extremely precise experi-
ment. The dataset containing the true targets on all 134k
molecules does not currently exist. However, the ability
to ﬁt the DFT approximation to within chemical accuracy
would be an encouraging step in this direction. For all 13
targets, achieving chemical accuracy is at least as hard as
achieving DFT error. In the rest of this paper when we talk
about chemical accuracy we generally mean with respect to
our available ground truth labels.

In this paper, by exploring novel variations of models in the
MPNN family, we are able to both achieve a new state of
the art on the QM9 dataset and to predict the DFT calcula-
tion to within chemical accuracy on all but two targets. In
particular, we provide the following key contributions:

•

•

•

We develop an MPNN which achieves state of the art
results on all 13 targets and predicts DFT to within
chemical accuracy on 11 out of 13 targets.

We develop several different MPNNs which predict
DFT to within chemical accuracy on 5 out of 13 tar-
gets while operating on the topology of the molecule
alone (with no spatial information as input).

We develop a general method to train MPNNs with
larger node representations without a corresponding
increase in computation time or memory, yielding a
substantial savings over previous MPNNs for high di-
mensional node representations.

We believe our work is an important step towards making
well-designed MPNNs the default for supervised learning
on modestly sized molecules.
In order for this to hap-
pen, researchers need to perform careful empirical stud-
ies to ﬁnd the proper way to use these types of models
and to make any necessary improvements to them, it is
not sufﬁcient for these models to have been described in
the literature if there is only limited accompanying empir-
Indeed convolutional
ical work in the chemical domain.
neural networks existed for decades before careful empiri-
cal work applying them to image classiﬁcation (Krizhevsky
et al., 2012) helped them displace SVMs on top of hand-
engineered features for a host of computer vision problems.

2. Message Passing Neural Networks

There are at least eight notable examples of models from
the literature that we can describe using our Message Pass-
ing Neural Networks (MPNN) framework. For simplicity
we describe MPNNs which operate on undirected graphs
G with node features xv and edge features evw. It is triv-
ial to extend the formalism to directed multigraphs. The
forward pass has two phases, a message passing phase and
a readout phase. The message passing phase runs for T

Neural Message Passing for Quantum Chemistry

time steps and is deﬁned in terms of message functions Mt
and vertex update functions Ut. During the message pass-
ing phase, hidden states ht
v at each node in the graph are
updated based on messages mt+1

according to

v

mt+1

v =

(cid:88)

Mt(ht

v, ht

w, evw)

w
N (v)
v = Ut(ht
ht+1

∈

v, mt+1
v

)

(1)

(2)

where in the sum, N (v) denotes the neighbors of v in graph
G. The readout phase computes a feature vector for the
whole graph using some readout function R according to

v

∈

(3)

).
G
}

ˆy = R(
{

hT
v |
The message functions Mt, vertex update functions Ut, and
readout function R are all learned differentiable functions.
R operates on the set of node states and must be invariant to
permutations of the node states in order for the MPNN to be
invariant to graph isomorphism. In what follows, we deﬁne
previous models in the literature by specifying the message
function Mt, vertex update function Ut, and readout func-
tion R used. Note one could also learn edge features in
an MPNN by introducing hidden states for all edges in the
graph ht
evw and updating them analogously to equations 1
and 2. Of the existing MPNNs, only Kearnes et al. (2016)
has used this idea.

Convolutional Networks for Learning Molecular Fin-
gerprints, Duvenaud et al. (2015)

The message function used is M (hv, hw, evw) =
(hw, evw) where (., .) denotes concatenation. The vertex
update function used is Ut(ht
),
where σ is the sigmoid function, deg(v) is the degree of
vertex v and H N
is a learned matrix for each time step t
t
and vertex degree N . R has skip connections to all previous
(cid:33)
,

) = σ(H deg(v)

hidden states ht

v and is equal to f

softmax(Wtht
v)

v, mt+1
v

mt+1
v

(cid:32)

t

(cid:80)
v,t

where f is a neural network and Wt are learned readout
matrices, one for each time step t. This message pass-
ing scheme may be problematic since the resulting mes-
w, (cid:80) evw) , which separately
sage vector is mt+1
sums over connected nodes and connected edges. It fol-
lows that the message passing implemented in Duvenaud
et al. (2015) is unable to identify correlations between edge
states and node states.

v = ((cid:80) ht

Gated Graph Neural Networks (GG-NN), Li et al.
(2016)

The message function used is Mt(ht
w, evw) = Aevw ht
w,
where Aevw is a learned matrix, one for each edge label e
(the model assumes discrete edge types). The update func-
tion is Ut = GRU(ht
), where GRU is the Gated

v, ht

v, mt+1
v

Recurrent Unit introduced in Cho et al. (2014). This work
used weight tying, so the same update function is used at
each time step t. Finally,

(cid:88)

(cid:16)

σ

R =

i(h(T )
v

, h0
v)

(cid:17)

(cid:16)

j(h(T )
v

(cid:17)
)

(4)

(cid:12)

v

V

∈

where i and j are neural networks, and
wise multiplication.

(cid:12)

denotes element-

Interaction Networks, Battaglia et al. (2016)

This work considered both the case where there is a tar-
get at each node in the graph, and where there is a graph
level target.
It also considered the case where there are
node level effects applied at each time step, in such a
case the update function takes as input the concatenation
(hv, xv, mv) where xv is an external vector representing
some outside inﬂuence on the vertex v. The message func-
tion M (hv, hw, evw) is a neural network which takes the
concatenation (hv, hw, evw). The vertex update function
U (hv, xv, mv) is a neural network which takes as input
the concatenation (hv, xv, mv). Finally, in the case where
there is a graph level output, R = f ( (cid:80)
hT
v ) where f is

a neural network which takes the sum of the ﬁnal hidden
states hT
v . Note the original work only deﬁned the model
for T = 1.

v

G

∈

Molecular Graph Convolutions, Kearnes et al. (2016)

edge

during

introduces

the message

it
updated

vw) = et
w, et
v, mt+1
v

representations et
passing

This work deviates slightly from other MPNNs in
vw which
that
phase.
are
The message function used for node messages
is
v, ht
M (ht
vw. The vertex update function
) = α(W1(α(W0ht
is Ut(ht
)) where
(., .) denotes concatenation, α is the ReLU activation
The edge
and W1, W0 are learned weight matrices.
v, ht
vw, ht
vw = U (cid:48)t(et
state update is deﬁned by et+1
w) =
v, ht
vw), α(W3(ht
α(W4(α(W2, et
w)))) where the Wi are
also learned weight matrices.

v), mt+1
v

Deep Tensor Neural Networks, Sch ¨utt et al. (2017)

The message from w to v is computed by

Mt = tanh (cid:0)W f c((W cf ht

w + b1)

(W df evw + b2))(cid:1)

(cid:12)

where W f c, W cf , W df are matrices and b1, b2 are bias
vectors. The update function used is Ut(ht
) =
ht
v + mt+1
. The readout function passes each node inde-
pendently through a single hidden layer neural network and
sums the outputs, in particular

v, mt+1
v

v

R =

NN(hT

v ).

(cid:88)

v

Laplacian Based Methods, Bruna et al. (2013); Deffer-
rard et al. (2016); Kipf & Welling (2016)

Neural Message Passing for Quantum Chemistry

These methods generalize the notion of the convolution op-
eration typically applied to image datasets to an operation
that operates on an arbitrary graph G with a real valued ad-
jacency matrix A. The operations deﬁned in Bruna et al.
(2013); Defferrard et al. (2016) result in message functions
of the form Mt(ht
w, where the matrices
C t
vw are parameterized by the eigenvectors of the graph
laplacian L, and the learned parameters of the model. The
vertex update function used is Ut(ht
) = σ(mt+1
)
where σ is some pointwise non-linearity (such as ReLU).

w) = C t

v, mt+1
v

vwht

v, ht

v

v, ht

w) = cvwht

The Kipf & Welling (2016) model results in a mes-
sage function Mt(ht
w where cvw =
1/2 Avw. The vertex update function is
(deg(v)deg(w))−
) = ReLU(W tmt+1
U t
v, mt+1
). For the exact expres-
v
sions for the C t
vw and the derivation of the reformulation of
these models as MPNNs, see the supplementary material.

v(ht

v

2.1. Moving Forward

Given how many instances of MPNNs have appeared in the
literature, we should focus on pushing this general fam-
ily as far as possible in a speciﬁc application of substan-
tial practical importance. This way we can determine the
most crucial implementation details and potentially reach
the limits of these models to guide us towards future mod-
eling improvements.

One downside of all of these approaches is computation
time. Recent work has adapted the GG-NN architecture to
larger graphs by passing messages on only subsets of the
graph at each time step (Marino et al., 2016). In this work
we also present a MPNN modiﬁcation that can improve the
computational costs.

3. Related Work

Although in principle quantum mechanics lets us compute
the properties of molecules, the laws of physics lead to
equations that are far too difﬁcult to solve exactly. There-
fore scientists have developed a hierarchy of approxima-
tions to quantum mechanics with varying tradeoffs of speed
and accuracy, such as Density Functional Theory (DFT)
with a variety of functionals (Becke, 1993; Hohenberg &
Kohn, 1964), the GW approximation (Hedin, 1965), and
Quantum Monte-Carlo (Ceperley & Alder, 1986). Despite
being widely used, DFT is simultaneously still too slow to
be applied to large systems (scaling as
e ) where Ne is
the number of electrons) and exhibits systematic as well as
random errors relative to exact solutions to Schr¨odinger’s
equation. For example, to run the DFT calculation on a sin-
gle 9 heavy atom molecule in QM9 takes around an hour
on a single core of a Xeon E5-2660 (2.2 GHz) using a ver-
sion of Gaussian G09 (ES64L-G09RevD.01) (Bing et al.,
2017). For a 17 heavy atom molecule, computation time is

(N 3

O

up to 8 hours. Empirical potentials have been developed,
such as the Stillinger-Weber potential (Stillinger & Weber,
1985), that are fast and accurate but must be created from
scratch, from ﬁrst principles, for every new composition of
atoms.

Hu et al. (2003) used neural networks to approximate a par-
ticularly troublesome term in DFT called the exchange cor-
relation potential to improve the accuracy of DFT. How-
ever, their method fails to improve upon the efﬁciency of
DFT and relies on a large set of ad hoc atomic descriptors.

Two more recent approaches by Behler & Parrinello (2007)
and Rupp et al. (2012) attempt to approximate solutions
to quantum mechanics directly without appealing to DFT.
In the ﬁrst case single-hidden-layer neural networks were
used to approximate the energy and forces for conﬁgura-
tions of a Silicon melt with the goal of speeding up molec-
ular dynamics simulations. The second paper used Ker-
nel Ridge Regression (KRR) to infer atomization energies
over a wide range of molecules.
In both cases hand en-
gineered features were used (symmetry functions and the
Coulomb matrix, respectively) that built physical symme-
tries into the input representation. Subsequent papers have
replaced KRR by a neural network.

Both of these lines of research used hand engineered fea-
tures that have intrinsic limitations. The work of Behler &
Parrinello (2007) used a representation that was manifestly
invariant to graph isomorphism, but has difﬁculty when ap-
plied to systems with more than three species of atoms and
fails to generalize to novel compositions. The represen-
tation used in Rupp et al. (2012) is not invariant to graph
isomorphism. Instead, this invariance must be learned by
the downstream model through dataset augmentation.

In addition to the eight MPNNs discussed in Section 2 there
have been a number of other approaches to machine learn-
ing on graphical data which take advantage of the symme-
tries in a number of ways. One such family of approaches
deﬁne a preprocessing step which constructs a canonical
graph representation which can then be fed into into a stan-
dard classiﬁer. Examples in this family include Niepert
et al. (2016) and Rupp et al. (2012). Finally Scarselli et al.
(2009) deﬁne a message passing process on graphs which
is run until convergence, instead of for a ﬁnite number of
time steps as in MPNNs.

4. QM9 Dataset

To investigate the success of MPNNs on predicting chem-
ical properties, we use the publicly available QM9 dataset
(Ramakrishnan et al., 2014). Molecules in the dataset con-
sist of Hydrogen (H), Carbon (C), Oxygen (O), Nitrogen
(N), and Flourine (F) atoms and contain up to 9 heavy (non
Hydrogen) atoms. In all, this results in about 134k drug-

Neural Message Passing for Quantum Chemistry

like organic molecules that span a wide range of chemistry.
For each molecule DFT is used to ﬁnd a reasonable low
energy structure and hence atom “positions” are available.
Additionally a wide range of interesting and fundamental
chemical properties are computed. Given how fundamen-
tal some of the QM9 properties are, it is hard to believe
success on more challenging chemical tasks will be possi-
ble if we can’t make accurate statistical predictions for the
properties computed in QM9.

We can group the different properties we try to predict into
four broad categories. First, we have four properties re-
lated to how tightly bound together the atoms in a molecule
are. These measure the energy required to break up the
molecule at different temperatures and pressures. These
include the atomization energy at 0K, U0 (eV), atomiza-
tion energy at room temperature, U (eV), enthalpy of at-
omization at room temperature, H (eV), and free energy of
atomization, G (eV).

Next there are properties related to fundamental vibrations
of the molecule, including the highest fundamental vibra-
1) and the zero point vibrational
tional frequency ω1 (cm−
energy (ZPVE) (eV).

Additionally, there are a number of properties that concern
the states of the electrons in the molecule. They include
the energy of the electron in the highest occupied molecu-
lar orbital (HOMO) εHOMO (eV), the energy of the lowest
unoccupied molecular orbital (LUMO) εLUMO (eV), and the
electron energy gap (∆ε (eV)). The electron energy gap is
simply the difference εHOMO

−
Finally, there are several measures of the spatial distribu-
tion of electrons in the molecule. These include the elec-
(Bohr2), the norm of the dipole
tronic spatial extent
moment µ (Debye), and the norm of static polarizability α
(Bohr3). For a more detailed description of these proper-
ties, see the supplementary material.

εLUMO.

R2
(cid:104)

(cid:105)

5. MPNN Variants

We began our exploration of MPNNs around the GG-NN
model which we believe to be a strong baseline. We fo-
cused on trying different message functions, output func-
tions, ﬁnding the appropriate input representation, and
properly tuning hyperparameters.

For the rest of the paper we use d to denote the dimension
of the internal hidden representation of each node in the
graph, and n to denote the number of nodes in the graph.
Our implementation of MPNNs in general operates on di-
rected graphs with a separate message channel for incom-
ing and outgoing edges, in which case the incoming mes-
sage mv is the concatenation of min
v , this was also
used in Li et al. (2016). When we apply this to undirected

v and mout

chemical graphs we treat the graph as directed, where each
original edge becomes both an incoming and outgoing edge
with the same label. Note there is nothing special about the
direction of the edge, it is only relevant for parameter tying.
Treating undirected graphs as directed means that the size
of the message channel is 2d instead of d.

The input to our MPNN model is a set of feature vectors
for the nodes of the graph, xv, and an adjacency matrix A
with vector valued entries to indicate different bonds in the
molecule as well as pairwise spatial distance between two
atoms. We experimented as well with the message func-
tion used in the GG-NN family, which assumes discrete
edge labels, in which case the matrix A has entries in a dis-
crete alphabet of size k. The initial hidden states h0
v are set
to be the atom input feature vectors xv and are padded up
to some larger dimension d. All of our experiments used
weight tying at each time step t, and a GRU (Cho et al.,
2014) for the update function as in the GG-NN family.

5.1. Message Functions

Matrix Multiplication: We started with the message func-
tion used in GG-NN which is deﬁned by the equation
M (hv, hw, evw) = Aevw hw.

Edge Network: To allow vector valued edge features
we propose the message function M (hv, hw, evw) =
A(evw)hw where A(evw) is a neural network which maps
the edge vector evw to a d

d matrix.

×

Pair Message: One property that the matrix multiplication
rule has is that the message from node w to node v is a
function only of the hidden state hw and the edge evw. In
particular, it does not depend on the hidden state ht
v. In
theory, a network may be able to use the message channel
more efﬁciently if the node messages are allowed to de-
pend on both the source and destination node. Thus we
also tried using a variant on the message function as de-
scribed in (Battaglia et al., 2016). Here the message from
w to v along edge e is mwv = f (ht
v, evw) where f is
a neural network.

w, ht

When we apply the above message functions to directed
graphs, there are two separate functions used, M in and an
M out. Which function is applied to a particular edge evw
depends on the direction of that edge.

5.2. Virtual Graph Elements

We explored two different ways to change how the mes-
sages are passed throughout the model. The simplest mod-
iﬁcation involves adding a separate “virtual” edge type for
pairs of nodes that are not connected. This can be imple-
mented as a data preprocessing step and allows information
to travel long distances during the propagation phase.

Neural Message Passing for Quantum Chemistry

We also experimented with using a latent “master” node,
which is connected to every input node in the graph with
a special edge type. The master node serves as a global
scratch space that each node both reads from and writes to
in every step of message passing. We allow the master node
to have a separate node dimension dmaster, as well as sep-
arate weights for the internal update function (in our case
a GRU). This allows information to travel long distances
during the propagation phase. It also, in theory, allows ad-
ditional model capacity (e.g. large values of dmaster) with-
out a substantial hit in performance, as the complexity of
the master node model is O(
E
|

d2 + nd2
|

master).

5.3. Readout Functions

We experimented with two readout functions. First is the
readout function used in GG-NN, which is deﬁned by equa-
tion 4. Second is a set2set model from Vinyals et al. (2015).
The set2set model is speciﬁcally designed to operate on sets
and should have more expressive power than simply sum-
ming the ﬁnal node states. This model ﬁrst applies a linear
projection to each tuple (hT
v , xv) and then takes as input
. Then, after M
the set of projected tuples T =
steps of computation, the set2set model produces a graph
level embedding q∗t which is invariant to the order of the of
the tuples T . We feed this embedding q∗t through a neural
network to produce the output.

v , xv)
}

(hT
{

5.4. Multiple Towers

One issue with MPNNs is scalability. In particular, a sin-
gle step of the message passing phase for a dense graph
requires O(n2d2) ﬂoating point multiplications. As n or d
get large this can be computationally expensive. To address
this issue we break the d dimensional node embeddings ht
v
into k different d/k dimensional embeddings ht,k
and run
v
a propagation step on each of the k copies separately to get
˜ht+1,k
temporary embeddings
, using separate
G
v
}
message and update functions for each copy. The k tem-
porary embeddings of each node are then mixed together
according to the equation

, v

∈

{

(cid:0)ht,1

v , ht,2

v , . . . , ht,k
v

(cid:1) = g

(cid:16)˜ht,1

v , ˜ht,2

v , . . . , ˜ht,k
v

(cid:17)

(5)

where g denotes a neural network and (x, y, . . .) denotes
concatenation, with g shared across all nodes in the graph.
This mixing preserves the invariance to permutations of
the nodes, while allowing the different copies of the graph
to communicate with each other during the propagation
phase. This can be advantageous in that it allows larger
hidden states for the same number of parameters, which
yields a computational speedup in practice. For exam-
ple, when the message function is matrix multiplication
(as in GG-NN) a propagation step of a single copy takes
O (cid:0)n2(d/k)2(cid:1) time, and there are k copies, therefore the

Table 1. Atom Features

Feature

Description

Atom type
Atomic number
Acceptor
Donor
Aromatic
Hybridization
Number of Hydrogens

H, C, N, O, F (one-hot)
Number of protons (integer)
Accepts electrons (binary)
Donates electrons (binary)
In an aromatic system (binary)
sp, sp2, sp3 (one-hot or null)
(integer)

overall time complexity is O (cid:0)n2d2/k(cid:1), with some addi-
tional overhead due to the mixing network. For k = 8,
n = 9 and d = 200 we see a factor of 2 speedup in infer-
ence time over a k = 1, n = 9, and d = 200 architecture.
This variation would be most useful for larger molecules,
for instance molecules from GDB-17 (Ruddigkeit et al.,
2012).

6. Input Representation

There are a number of features available for each atom in
a molecule which capture both properties of the electrons
in the atom as well as the bonds that the atom participates
in. For a list of all of the features see table 1. We experi-
mented with making the hydrogen atoms explicit nodes in
the graph (as opposed to simply including the count as a
node feature), in which case graphs have up to 29 nodes.
Note that having larger graphs signiﬁcantly slows training
time, in this case by a factor of roughly 10. For the adja-
cency matrix there are three edge representations used de-
pending on the model.

Chemical Graph: In the abscence of distance information,
adjacency matrix entries are discrete bond types: single,
double, triple, or aromatic.

Distance bins: The matrix multiply message function as-
sumes discrete edge types, so to include distance informa-
tion we bin bond distances into 10 bins, the bins are ob-
tained by uniformly partitioning the interval [2, 6] into 8
bins, followed by adding a bin [0, 2] and [6,
]. These
bins were hand chosen by looking at a histogram of all dis-
tances. The adjacency matrix then has entries in an alpha-
bet of size 14, indicating bond type for bonded atoms and
distance bin for atoms that are not bonded. We found the
distance for bonded atoms to be almost completely deter-
mined by bond type.

∞

Raw distance feature: When using a message function
which operates on vector valued edges, the entries of the
adjacency matrix are then 5 dimensional, where the ﬁrst di-
mension indicates the euclidean distance between the pair
of atoms, and the remaining four are a one-hot encoding of

Neural Message Passing for Quantum Chemistry

the bond type.

7. Training

T

≤

≥

Each model and target combination was trained using a uni-
form random hyper parameter search with 50 trials. T was
constrained to be in the range 3
8 (in practice, any
≤
3 works). The number of set2set computations M
T
was chosen from the range 1
12. All models were
M
≤
trained using SGD with the ADAM optimizer (Kingma &
Ba (2014)), with batch size 20 for 3 million steps ( 540
epochs). The initial learning rate was chosen uniformly be-
4. We used a linear learning rate decay
tween 1e−
that began between 10% and 90% of the way through train-
ing and the initial learning rate l decayed to a ﬁnal learning
rate l

F , using a decay factor F in the range [.01, 1].

5 and 5e−

≤

∗

The QM-9 dataset has 130462 molecules in it. We ran-
domly chose 10000 samples for validation, 10000 samples
for testing, and used the rest for training. We use the vali-
dation set to do early stopping and model selection and we
report scores on the test set. All targets were normalized
to have mean 0 and variance 1. We minimize the mean
squared error between the model output and the target, al-
though we evaluate mean absolute error.

8. Results

In all of our tables we report the ratio of the mean ab-
solute error (MAE) of our models with the provided esti-
mate of chemical accuracy for that target. Thus any model
with error ratio less than 1 has achieved chemical accu-
racy for that target. In the supplementary material we list
the chemical accuracy estimates for each target, these are
the same estimates that were given in Faber et al. (2017).
In this way, the MAE of our models can be calculated as
(Error Ratio)
(Chemical Accuracy). Note, unless other-
wise indicated, all tables display result of models trained
individually on each target (as opposed to training one
model to predict all 13).

×

We performed numerous experiments in order to ﬁnd the
best possible MPNN on this dataset as well as the proper
input representation. In our experiments, we found that in-
cluding the complete edge feature vector (bond type, spatial
distance) and treating hydrogen atoms as explicit nodes in
the graph to be very important for a number of targets. We
also found that training one model per target consistently
In some
outperformed jointly training on all 13 targets.
cases the improvement was up to 40%. Our best MPNN
variant used the edge network message function, set2set
output, and operated on graphs with explicit hydrogens. We
were able to further improve performance on the test set by
ensembling the predictions of the ﬁve models with lowest
validation error.

In table 2 we compare the performance of our best MPNN
variant (denoted with enn-s2s) and the corresponding en-
semble (denoted with enn-s2s-ens5) with the previous state
of the art on this dataset as reported in Faber et al. (2017).
For clarity the error ratios of the best non-ensemble mod-
els are shown in bold. This previous work performed
a comparison study of several existing ML models for
QM9 and we have taken care to use the same train, val-
idation, and test split. These baselines include 5 differ-
ent hand engineered molecular representations, which then
get fed through a standard, off-the-shelf classiﬁer. These
input representations include the Coulomb Matrix (CM,
Rupp et al. (2012)), Bag of Bonds (BoB, Hansen et al.
(2015)), Bonds Angles, Machine Learning (BAML, Huang
& von Lilienfeld (2016)), Extended Connectivity Finger-
prints (ECPF4, Rogers & Hahn (2010)), and “Projected
Histograms” (HDAD, Faber et al. (2017)) representations.
In addition to these hand engineered features we include
two existing baseline MPNNs, the Molecular Graph Con-
volutions model (GC) from Kearnes et al. (2016), and the
original GG-NN model Li et al. (2016) trained with dis-
tance bins. Overall, our new MPNN achieves chemical ac-
curacy on 11 out of 13 targets and state of the art on all 13
targets.

Training Without Spatial Information: We also experi-
mented in the setting where spatial information is not in-
cluded in the input. In general, we ﬁnd that augmenting the
MPNN with some means of capturing long range interac-
tions between nodes in the graph greatly improves perfor-
mance in this setting. To demonstrate this we performed 4
experiments, one where we train the GG-NN model on the
sparse graph, one where we add virtual edges, one where
we add a master node, and one where we change the graph
level output to a set2set output. The error ratios averaged
across the 13 targets are shown in table 3. Overall, these
three modiﬁcations help on all 13 targets, and the Set2Set
output achieves chemical accuracy on 5 out of 13 targets.
For more details, consult the supplementary material. The
experiments shown tables 3 and 4 were run with a partial
charge feature as a node input. This feature is an output of
the DFT calculation and thus could not be used in an ap-
plied setting. The state of art numbers we report in table 2
do not use this feature.

Towers: Our original intent in developing the towers vari-
ant was to improve training time, as well as to allow the
model to be trained on larger graphs. However, we also
found some evidence that the multi-tower structure im-
proves generalization performance.
In table 4 we com-
pare GG-NN + towers + set2set output vs a baseline GG-
NN + set2set output when distance bins are used. We do
this comparison in both the joint training regime and when
training one model per target. The towers model outper-
forms the baseline model on 12 out of 13 targets in both

Neural Message Passing for Quantum Chemistry

Table 2. Comparison of Previous Approaches (left) with MPNN baselines (middle) and our methods (right)

Target

BAML BOB CM ECFP4 HDAD GC

GG-NN DTNN enn-s2s

enn-s2s-ens5

4.34
mu
alpha
3.01
HOMO 2.20
2.76
LUMO
3.28
gap
3.25
R2
3.31
ZPVE
1.21
U0
1.22
U
1.22
H
1.20
G
1.64
Cv
0.27
Omega
2.17
Average

4.23
2.98
2.20
2.74
3.41
0.80
3.40
1.43
1.44
1.44
1.42
1.83
0.35
2.08

4.49
4.33
3.09
4.26
5.32
2.83
4.80
2.98
2.99
2.99
2.97
2.36
1.32
3.37

4.82
34.54
2.89
3.10
3.86
90.68
241.58
85.01
85.59
86.21
78.36
30.29
1.47
53.97

3.34
1.75
1.54
1.96
2.49
1.35
1.91
0.58
0.59
0.59
0.59
0.88
0.34
1.35

0.70
2.27
1.18
1.10
1.78
4.73
9.75
3.02
3.16
3.19
2.95
1.45
0.32
2.59

1.22
1.55
1.17
1.08
1.70
3.99
2.52
0.83
0.86
0.81
0.78
1.19
0.53
1.36

-
-
-
-
-
-
-
-
-
-
.842
-
-
-

0.30
0.92
0.99
0.87
1.60
0.15
1.27
0.45
0.45
0.39
0.44
0.80
0.19
0.68

0.20
0.68
0.74
0.65
1.23
0.14
1.10
0.33
0.34
0.30
0.34
0.62
0.15
0.52

Table 3. Models Trained Without Spatial Information
Model
GG-NN
GG-NN + Virtual Edge
GG-NN + Master Node
GG-NN + set2set

Average Error Ratio
3.47
2.90
2.62
2.57

Table 4. Towers vs Vanilla GG-NN (no explicit hydrogen)

Model
GG-NN + joint training
towers8 + joint training
GG-NN + individual training
towers8 + individual training

Average Error Ratio
1.92
1.75
1.53
1.37

individual and joint target training. We believe the beneﬁt
of towers is that it resembles training an ensemble of mod-
els. Unfortunately, our attempts so far at combining the
towers and edge network message function have failed to
further improve performance, possibly because the combi-
nation makes training more difﬁcult. Further training de-
tails, and error ratios on all targets can be found in the sup-
plementary material.

Additional Experiments: In preliminary experiments, we
tried disabling weight tying across different time steps.
However, we found that the most effective way to increase
performance was to tie the weights and use a larger hidden
dimension d. We also early on found the pair message func-
tion to perform worse than the edge network function. This
included a toy pathﬁnding problem which was originally

2As reported in Sch¨utt et al. (2017). The model was trained
on a different train/test split with 100k training samples vs 110k
used in our experiments.

designed to beneﬁt from using pair messages. Also, when
trained jointly on the 13 targets the edge network function
outperforms pair message on 11 out of 13 targets, and has
an average error ratio of 1.53 compared to 3.98 for pair
message. Given the difﬁculties with training this function
we did not pursue it further. For performance on smaller
sized training sets, consult the supplementary material.

9. Conclusions and Future Work

Our results show that MPNNs with the appropriate mes-
sage, update, and output functions have a useful induc-
tive bias for predicting molecular properties, outperforming
several strong baselines and eliminating the need for com-
plicated feature engineering. Moreover, our results also re-
veal the importance of allowing long range interactions be-
tween nodes in the graph with either the master node or the
set2set output. The towers variation makes these models
more scalable, but additional improvements will be needed
to scale to much larger graphs.

An important future direction is to design MPNNs that can
generalize effectively to larger graphs than those appear-
ing in the training set or at least work with benchmarks
designed to expose issues with generalization across graph
sizes. Generalizing to larger molecule sizes seems partic-
ularly challenging when using spatial information. First of
all, the pairwise distance distribution depends heavily on
the number of atoms. Second, our most successful ways
of using spatial information create a fully connected graph
where the number of incoming messages also depends on
the number of nodes. To address the second issue, we be-
lieve that adding an attention mechanism over the incom-
ing message vectors could be an interesting direction to ex-
plore.

Neural Message Passing for Quantum Chemistry

Acknowledgements

We would like to thank Lukasz Kaiser, Geoffrey Irving,
Alex Graves, and Yujia Li for helpful discussions. Thank
you to Adrian Roitberg for pointing out an issue with the
use of partial charges in an earlier version of this paper.

References

Battaglia, Peter, Pascanu, Razvan, Lai, Matthew, Rezende,
Danilo Jimenez, and Kavukcuoglu, Koray.
Interac-
tion networks for learning about objects, relations and
physics. In Advances in Neural Information Processing
Systems, pp. 4502–4510, 2016.

Becke, Axel D. Density-functional thermochemistry. iii.
the role of exact exchange. The Journal of Chemi-
cal Physics, 98(7):5648–5652, 1993. doi: 10.1063/1.
464913. URL http://dx.doi.org/10.1063/1.
464913.

Behler,

J¨org and Parrinello, Michele.

General-
ized neural-network representation of high-dimensional
Phys. Rev. Lett., 98:
potential-energy surfaces.
146401, Apr 2007.
doi: 10.1103/PhysRevLett.98.
146401. URL http://link.aps.org/doi/10.
1103/PhysRevLett.98.146401.

Bing, Huang, von Lillenfeld, O. Anatole, and Bakowies,

Dirk. personal communication, 2017.

Bruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and Le-
Cun, Yann. Spectral networks and locally connected net-
works on graphs. arXiv preprint arXiv:1312.6203, 2013.

Ceperley, David and Alder, B. Quantum monte carlo. Sci-

ence, 231, 1986.

Cho, Kyunghyun, Van Merri¨enboer, Bart, Bahdanau,
Dzmitry, and Bengio, Yoshua. On the properties of neu-
ral machine translation: Encoder-decoder approaches.
arXiv preprint arXiv:1409.1259, 2014.

Defferrard, Micha¨el, Bresson, Xavier, and Vandergheynst,
Pierre. Convolutional neural networks on graphs with
fast localized spectral ﬁltering. In Advances in Neural
Information Processing Systems, pp. 3837–3845, 2016.

Duvenaud, David K, Maclaurin, Dougal,

Iparraguirre,
Jorge, Bombarell, Rafael, Hirzel, Timothy, Aspuru-
Guzik, Al´an, and Adams, Ryan P. Convolutional net-
works on graphs for learning molecular ﬁngerprints. In
Advances in neural information processing systems, pp.
2224–2232, 2015.

Lilienfeld, O. Anatole. Fast machine learning mod-
els of electronic and energetic properties consistently
reach approximation errors better than dft accuracy.
https://arxiv.org/abs/1702.05532, 2017.

Hansen, Katja, Biegler, Franziska, Ramakrishnan, Raghu-
nathan, Pronobis, Wiktor, von Lilienfeld, O. Anatole,
Mller, Klaus-Robert, and Tkatchenko, Alexandre. Ma-
chine learning predictions of molecular properties: Ac-
curate many-body potentials and nonlocality in chem-
The journal of physical chemistry let-
ical space.
ters, 6(12):2326–2331, 2015. doi: 10.1021/acs.jpclett.
5b00831. URL http://dx.doi.org/10.1021/
acs.jpclett.5b00831.

Hedin, Lars. New method for calculating the one-particle
green’s function with application to the electron-gas
problem. Phys. Rev., 139:A796–A823, Aug 1965. doi:
10.1103/PhysRev.139.A796. URL http://link.
aps.org/doi/10.1103/PhysRev.139.A796.

Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E.,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, An-
drew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic mod-
eling in speech recognition: The shared views of four
research groups. IEEE Signal Processing Magazine, 29
(6):82–97, 2012.

Hohenberg, P. and Kohn, W. Inhomogeneous electron gas.
Phys. Rev., 136:B864–B871, Nov 1964. doi: 10.1103/
PhysRev.136.B864. URL http://link.aps.org/
doi/10.1103/PhysRev.136.B864.

Hu, LiHong, Wang, XiuJun, Wong, LaiHo, and Chen,
GuanHua. Combined ﬁrst-principles calculation and
neural-network correction approach for heat of forma-
tion. The Journal of Chemical Physics, 119(22):11501–
11507, 2003.

Huang, Bing and von Lilienfeld, O. Anatole. Commu-
nication: Understanding molecular representations in
machine learning: The role of uniqueness and target
similarity. The Journal of Chemical Physics, 145(16):
161102, 2016. doi: 10.1063/1.4964627. URL http:
//dx.doi.org/10.1063/1.4964627.

Kearnes, Steven, McCloskey, Kevin, Berndl, Marc, Pande,
Vijay, and Riley, Patrick. Molecular graph convolutions:
Moving beyond ﬁngerprints. Journal of Computer-Aided
Molecular Design, 30(8):595–608, 2016.

Faber, Felix, Hutchison, Luke, Huang, Bing, Gilmer,
Justin, Schoenholz, Samuel S., Dahl, George E., Vinyals,
Oriol, Kearnes, Steven, Riley, Patrick F., and von

Kingma, Diederik and Ba,

Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam:

A
arXiv preprint

Neural Message Passing for Quantum Chemistry

Kipf, T. N. and Welling, M. Semi-Supervised Classiﬁ-
cation with Graph Convolutional Networks. ArXiv e-
prints, September 2016.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pp. 1097–1105, 2012.

Li, Yujia, Tarlow, Daniel, Brockschmidt, Marc, and Zemel,
Richard. Gated graph sequence neural networks. ICLR,
2016.

Lusci, Alessandro, Pollastri, Gianluca, and Baldi, Pierre.
Deep architectures and deep learning in chemoinformat-
ics:
the prediction of aqueous solubility for drug-like
molecules. Journal of chemical information and mod-
eling, 53(7):1563–1575, 2013.

Marino, Kenneth, Salakhutdinov, Ruslan, and Gupta, Ab-
hinav. The more you know: Using knowledge graphs for
image classiﬁcation. arXiv preprint arXiv:1612.04844,
2016.

Merkwirth, Christian and Lengauer, Thomas. Automatic
generation of complementary descriptors with molecular
graph networks. Journal of chemical information and
modeling, 45(5):1159–1168, 2005.

Micheli, Alessio. Neural network for graphs: A contex-
tual constructive approach. IEEE Transactions on Neu-
ral Networks, 20(3):498–511, 2009.

Montavon, Gr´egoire, Hansen, Katja, Fazli, Siamac,
Rupp, Matthias, Biegler, Franziska, Ziehe, Andreas,
Tkatchenko, Alexandre, von Lilienfeld, O. Anatole, and
M¨uller, Klaus-Robert. Learning invariant representa-
tions of molecules for atomization energy prediction. In
Advances in Neural Information Processing Systems, pp.
440–448, 2012.

Niepert, Mathias, Ahmed, Mohamed, and Kutzkov, Kon-
stantin. Learning convolutional neural networks for
graphs. In Proceedings of the 33rd annual international
conference on machine learning. ACM, 2016.

Ramakrishnan, Raghunathan, Dral, Pavlo O, Rupp,
Matthias, and Von Lilienfeld, O Anatole.
Quan-
tum chemistry structures and properties of 134 kilo
molecules. Scientiﬁc data, 1, 2014.

Rogers, David and Hahn, Mathew. Extended-connectivity
ﬁngerprints. Journal of chemical information and mod-
eling, 50(5):742–754, 2010.

Ruddigkeit, Lars, Van Deursen, Ruud, Blum, Lorenz C,
and Reymond, Jean-Louis. Enumeration of 166 bil-
lion organic small molecules in the chemical universe

database gdb-17. Journal of chemical information and
modeling, 52(11):2864–2875, 2012.

Rupp, Matthias, Tkatchenko, Alexandre haand M¨uller,
Klaus-Robert, and von Lilienfeld, O. Anatole.
Fast
and accurate modeling of molecular atomization ener-
gies with machine learning. Physical review letters, 108
(5):058301, Jan 2012. URL http://dx.doi.org/
10.1103/PhysRevLett.108.058301.

Scarselli, Franco, Gori, Marco, Tsoi, Ah Chung, Hagen-
buchner, Markus, and Monfardini, Gabriele. The graph
IEEE Transactions on Neural
neural network model.
Networks, 20(1):61–80, 2009.

Schoenholz, Samuel S., Cubuk, Ekin D., Sussman,
Daniel M, Kaxiras, Efthimios, and Liu, Andrea J. A
structural approach to relaxation in glassy liquids. Na-
ture Physics, 2016.

Sch¨utt, Kristof T, Arbabzadah, Farhad, Chmiela, Stefan,
M¨uller, Klaus R, and Tkatchenko, Alexandre. Quantum-
chemical insights from deep tensor neural networks. Na-
ture Communications, 8, 2017.

Stillinger, Frank H. and Weber, Thomas A. Computer sim-
ulation of local order in condensed phases of silicon.
Phys. Rev. B, 31:5262–5271, Apr 1985. doi: 10.1103/
PhysRevB.31.5262. URL http://link.aps.org/
doi/10.1103/PhysRevB.31.5262.

Vinyals, Oriol, Bengio, Samy, and Kudlur, Manjunath.
Order matters: Sequence to sequence for sets. arXiv
preprint arXiv:1511.06391, 2015.

Wu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc V.,
Norouzi, Mohammad, Macherey, Wolfgang, Krikun,
Maxim, Cao, Yuan, Gao, Qin, Macherey, Klaus, et al.
Google’s neural machine translation system: Bridging
the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

10. Appendix

MPNNs

10.1. Interpretation of Laplacian based models as

Another family of models deﬁned in Defferrard et al.
(2016), Bruna et al. (2013), Kipf & Welling (2016) can be
interpreted as MPNNs. These models generalize the notion
of convolutions a general graph G with N nodes. In the
language of MPNNs, these models tend to have very simple
message functions and are typically applied to much larger
graphs such as social network data. We closely follow the
notation deﬁned in Bruna et al. (2013) equation (3.2). The
model discussed in Defferrard et al. (2016) (equation 5)

Neural Message Passing for Quantum Chemistry

∈
−

RN
D−

×
1/2W D−

and Kipf & Welling (2016) can be viewed as special cases.
N we deﬁne the
Given an adjacency matrix W
1/2 where
graph Laplacian to be L = In
D is the diagonal degree matrix with Dii = deg(vi). Let
V denote the eigenvectors of L, ordered by eigenvalue. Let
σ be a real valued nonlinearity (such as ReLU). We now
deﬁne an operation which transforms an input vector x of
size N
d2 (the full model
can deﬁned as stacking these operations).

d1 to a vector y of size N

×

×

yj = σ

V Fi,jV T xi

(j = 1 . . . d2)

(6)

(cid:33)

(cid:32) d1(cid:88)

i=1

×

×

×

d1 vector x and N

Here yj and xi are all N dimensional vectors correspond-
ing to a scalar feature at each node. The matrices Fi,j are all
diagonal N
N matrices and contain all of the learned pa-
rameters in the layer. We now expand equation 6 in terms
d2 vector y, using
of the full N
v and w to index nodes in the graph G and i, j to index
the dimensions of the node states.
In this way xw,i de-
notes the i’th dimension of node w, and yv,j denotes the
j’th dimension of node v, furthermore we use xw to de-
note the d1 dimensional vector for node state w, and yv to
denote the d2 dimensional vector for node v. Deﬁne the
rank 4 tensor ˜L of dimension N
d2 where
˜Lv,w,i,j = (V Fi,jV T )v,w. We will use ˜Li,j to denote the
N dimensional matrix where ( ˜Li,j)v,w = ˜Lv,w,i,j
N
and ˜Lv,w to denote the d1 ×
d2 dimensional matrix where
( ˜Lv,w)i,j = ˜Lv,w,i,j. Writing equation 6 in this notation
we have

d1 ×

N

×

×

×

yv,j = σ

˜Lv,w,i,jxw,i

yj = σ

yv = σ

(cid:33)

˜Li,jxi

(cid:32) d1(cid:88)

i=1





d1,N
(cid:88)

i=1,w=1

(cid:33)

˜Lv,wxw

.

(cid:32) N
(cid:88)

w=1





Relabelling yv as ht+1
and xw as ht
w this last line can be in-
terpreted as the message passing update in an MPNN where
M (ht

) = σ(mt+1

w) = ˜Lv,wht

w and U (ht

v, mt+1
v

v, ht

).

v

v

10.1.1. THE SPECIAL CASE OF KIPF AND WELLING

(2016)

Motivated as a ﬁrst order approximation of the graph lapla-
cian methods, Kipf & Welling (2016) propose the follow-
ing layer-wise propagation rule:

H l+1 = σ

˜D−

1/2 ˜A ˜D−

(cid:16)

1/2H lW l(cid:17)

(7)

Here ˜A = A + IN where A is the real valued adjacency
matrix for an undirected graph G. Adding the identity ma-
trix IN corresponds to adding self loops to the graph. Also
˜Dii = (cid:80)
˜Aij denotes the degree matrix for the graph with
D is a layer-speciﬁc trainable weight
self loops, W l
∈
) denotes a real valued nonlinearity. Each
matrix, and σ(
·
H l is a RN
D dimensional matrix indicating the D dimen-
sional node states for the N nodes in the graph.

RD

×

×

j

In what follows, given a matrix M we use M(v) to denote
the row in M indexed by v (v will always correspond to a
node in the graph G). Let L = ˜D−
1/2. To get the
updated node state for node v we have
(v) = σ (cid:0)L(v)H lW l(cid:1)
H l+1
(cid:32)
(cid:88)

1/2 ˜A ˜D−

(cid:33)

= σ

LvwH l

(w)W l

w

Relabelling the row vector H l+1
umn vector ht+1

v

the above equation is equivalent to

(v) as an N dimensional col-

ht+1
v = σ

(cid:32)
(W l)T (cid:88)

Lvwht
w

(cid:33)

w

(8)

which is equivalent to a message function

Mt(ht

v, ht

w) = Lvwht

w = ˜Avw(deg(v)deg(w))−

1/2ht

w,

and update function

Ut(ht

v, mt+1
v

) = σ((W t)T mt+1).

Note that the Lvw are all scalar valued, so this model corre-
sponds to taking a certain weighted average of neighboring
nodes at each time step.

10.2. A More Detailed Description of the Quantum

Properties

First there the four atomization energies.

•

•

•

Atomization energy at 0K U0 (eV): This is the en-
ergy required to break up the molecule into all of its
constituent atoms if the molecule is at absolute zero.
This calculation assumes that the molecules are held
at ﬁxed volume.

Atomization energy at room temperature U (eV):
Like U0, this is the energy required to break up the
molecule if it is at room temperature.

Enthalpy of atomization at room temperature H (eV):
The enthalpy of atomization is similar in spirit to the
energy of atomization, U . However, unlike the energy
this calculation assumes that the constituent molecules
are held at ﬁxed pressure.

Neural Message Passing for Quantum Chemistry

•

Free energy of atomization G (eV): Once again this
is similar to U and H, but assumes that the system
is held at ﬁxed temperature and pressure during the
dissociation.

Next there are properties related to fundamental vibrations
of the molecule:

fundamental vibrational

Highest
frequency ω1
1): Every molecule has fundamental vibrational
(cm−
modes that it can naturally oscillate at. ω1 is the mode
that requires the most energy.

Zero Point Vibrational Energy (ZPVE) (eV): Even
at zero temperature quantum mechanical uncertainty
implies that atoms vibrate. This is known as the
zero point vibrational energy and can be calculated
once the allowed vibrational modes of a molecule are
known.

Additionally, there are a number of properties that concern
the states of the electrons in the molecule:

Highest Occupied Molecular Orbital (HOMO) εHOMO
(eV): Quantum mechanics dictates that the allowed
states that electrons can occupy in a molecule are dis-
crete. The Pauli exclusion principle states that no two
electrons may occupy the same state. At zero temper-
ature, therefore, electrons stack in states from lowest
energy to highest energy. HOMO is the energy of the
highest occupied electronic state.

Lowest Unoccupied Molecular Orbital
(LUMO)
εLUMO (eV): Like HOMO, LUMO is the lowest en-
ergy electronic state that is unoccupied.

Electron energy gap ∆ε (eV): This is the difference in
energy between LUMO and HOMO. It is the lowest
energy transition that can occur when an electron is
excited from an occupied state to an unoccupied state.
∆ε also dictates the longest wavelength of light that
the molecule can absorb.

Table 5. Chemical Accuracy For Each Target

DFT Error Chemical Accuracy
Target
.1
mu
alpha
.4
HOMO 2.0
LUMO 2.6
1.2
gap
-
R2
.0097
ZPVE
.1
U0
.1
U
.1
H
.1
G
.34
Cv
28
Omega

.1
.1
.043
.043
.043
1.2
.0012
.043
.043
.043
.043
.050
10.0

the charge is distributed (and hence the strength of the
ﬁeld far from the molecule). This degree of anisotropy
is in turn related to a number of material properties
(for example hydrogen bonding in water causes the
dipole moment to be large which has a large effect on
dynamics and surface tension).

•

Norm of the static polarizability α (Bohr3): α mea-
sures the extent to which a molecule can sponta-
neously incur a dipole moment in response to an ex-
ternal ﬁeld. This is in turn related to the degree to
which i.e. Van der Waals interactions play a role in
the dynamics of the medium.

10.3. Chemical Accuracy and DFT Error

In Table 5 we list the mean absolute error numbers for
chemical accuracy. These are the numbers used to com-
pute the error ratios of all models in the tables. The mean
absolute errors of our models can thus be calculated as
(Error Ratio)
(Chemical Accuracy). We also include
some estimates of the mean absolute error for the DFT cal-
culation to the ground truth. Both the estimates of chem-
ical accruacy and DFT error were provided in Faber et al.
(2017).

×

Finally, there are several measures of the spatial distribu-
tion of electrons in the molecule:

10.4. Additional Results

(Bohr2): The elec-
R2
Electronic Spatial Extent
(cid:104)
tronic spatial extent is the second moment of the
charge distribution, ρ(r), or in other words
=
(cid:82) drr2ρ(r).

R2
(cid:104)

(cid:105)

(cid:105)

Norm of the dipole moment µ (Debye): The dipole
moment, p(r) = (cid:82) dr(cid:48)p(r(cid:48))(r
r(cid:48)), approximates
the electric ﬁeld far from a molecule. The norm of
the dipole moment is related to how anisotropically

−

In Table 6 we compare the performance of the best archi-
tecture (edge network + set2set output) on different sized
training sets. It is surprising how data efﬁcient this model
is on some targets. For example, on both R2 and Omega
our models are equal or better with 11k samples than the
best baseline is with 110k samples.

In Table 7 we compare the performance of several mod-
els trained without spatial information. The left 4 columns
show the results of 4 experiments, one where we train the

•

•

•

•

•

•

•

Table 6. Results from training the edge network + set2set model on different sized training sets (N denotes the number of training
samples)

Neural Message Passing for Quantum Chemistry

N=11k N=35k N=58k N=82k N=110k
Target
1.28
mu
2.76
alpha
HOMO 2.33
LUMO 2.18
3.53
gap
0.28
R2
2.52
ZPVE
1.24
U0
1.05
U
1.14
H
1.23
G
1.99
Cv
0.28
Omega

0.32
1.09
1.19
1.10
1.84
0.21
1.68
0.62
0.52
0.53
0.49
0.87
0.15

0.44
1.26
1.34
1.19
2.07
0.21
1.69
0.58
0.60
0.65
0.64
0.93
0.24

0.30
0.92
0.99
0.87
1.60
0.15
1.27
0.45
0.45
0.39
0.44
0.80
0.19

0.55
1.59
1.50
1.47
2.34
0.22
1.78
0.69
0.69
0.64
0.62
1.24
0.25

GG-NN model on the sparse graph, one where we add vir-
tual edges (ve), one where we add a master node (mn), and
one where we change the graph level output to a set2set
output (s2s). In general, we ﬁnd that it’s important to al-
low the model to capture long range interactions in these
graphs.

In Table 8 we compare GG-NN + towers + set2set output
(tow8) vs a baseline GG-NN + set2set output (GG-NN)
when distance bins are used. We do this comparison in both
the joint training regime (j) and when training one model
per target (i). For joint training of the baseline we used
100 trials with d = 200 as well as 200 trials where d was
chosen randomly in the set
, we report
43, 73, 113, 153
}
{
the minimum test error across all 300 trials. For individual
training of the baseline we used 100 trials where d was cho-
sen uniformly in the range [43, 200]. The towers model was
always trained with d = 200 and k = 8, with 100 tuning
trials for joint training and 50 trials for individual training.
The towers model outperforms the baseline model on 12
out of 13 targets in both individual and joint target training.

In Table 9 right 2 columns compare the edge network (enn)
with the pair message network (pm) in the joint training
regime (j). The edge network consistently outperforms the
pair message function across most targets.

In Table 10 we compare our MPNNs with different input
featurizations. All models use the Set2Set output and GRU
update functions. The no distance model uses the matrix
multiply message function, the distance models use the
edge neural network message function.

Table 7. Comparison of models when distance information is ex-
cluded

GG-NN ve
Target
3.94
mu
alpha
2.43
HOMO 1.80
1.73
LUMO
2.48
gap
14.74
R2
5.93
ZPVE
1.98
U0
2.48
U
2.19
H
2.13
G
1.96
Cv
1.28
Omega
3.47
Average

3.76
2.07
1.60
1.48
2.33
17.11
3.21
0.89
0.93
0.86
0.85
1.61
1.05
2.90

mn
4.02
2.01
1.67
1.48
2.23
13.16
3.26
0.90
0.99
0.95
1.02
1.63
0.78
2.62

s2s
3.81
2.04
1.71
1.41
2.26
13.67
3.02
0.72
0.79
0.80
0.74
1.71
0.78
2.57

Table 8. Towers vs Vanilla Model (no explicit hydrogen)
GG-NN-j
tow8-i
Target
2.23
2.73
mu
1.34
1.66
alpha
1.20
HOMO 1.33
1.11
1.28
LUMO
1.68
1.73
gap
4.11
6.07
R2
2.54
4.07
ZPVE
0.55
1.00
U0
0.52
U
1.01
0.50
1.01
H
0.50
G
0.99
1.09
1.40
Cv
0.50
0.66
Omega
1.37
1.92
Average

tow8-j GG-NN-i
2.47
1.50
1.19
1.12
1.55
6.16
3.43
0.86
0.88
0.88
0.85
1.27
0.62
1.75

2.16
1.47
1.27
1.24
1.78
4.78
2.70
0.71
0.65
0.68
0.66
1.27
0.57
1.53

Neural Message Passing for Quantum Chemistry

Table 9. Pair Message vs Edge Network in joint training
pm-j
Target
2.10
mu
2.30
alpha
HOMO 1.20
1.46
LUMO
1.80
gap
10.87
R2
16.53
ZPVE
3.16
U0
3.18
U
3.20
H
2.97
G
2.17
Cv
0.83
Omega
3.98
Average

enn-j
2.24
1.48
1.30
1.20
1.75
2.41
3.85
0.92
0.93
0.93
0.92
1.28
0.74
1.53

Table 10. Performance With Different Input Information

no distance
Target
3.81
mu
alpha
2.04
HOMO 1.71
1.41
LUMO
2.26
gap
13.67
R2
3.02
ZPVE
0.72
U0
0.79
U
0.80
H
0.74
G
1.71
Cv
0.78
Omega
2.57
Average

distance
0.95
1.18
1.10
1.06
1.74
0.57
2.57
0.55
0.55
0.59
0.55
0.99
0.41
0.98

dist + exp hydrogen
0.30
0.92
0.99
0.87
1.60
0.15
1.27
0.45
0.45
0.39
0.44
0.80
0.19
0.68

