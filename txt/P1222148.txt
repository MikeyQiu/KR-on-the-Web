Centered Weight Normalization
in Accelerating Training of Deep Neural Networks

Lei Huang† Xianglong Liu†∗ Yang Liu† Bo Lang† Dacheng Tao‡
†State Key Laboratory of Software Development Environment, Beihang University, P.R.China
‡UBTECH Sydney AI Centre, School of IT, FEIT, The University of Sydney, Australia
{huanglei, xlliu, blonster, langbo}@nlsde.buaa.edu.cn, dacheng.tao@sydney.edu.au

Abstract

Training deep neural networks is difﬁcult for the patho-
logical curvature problem. Re-parameterization is an effec-
tive way to relieve the problem by learning the curvature
approximately or constraining the solutions of weights with
good properties for optimization. This paper proposes to re-
parameterize the input weight of each neuron in deep neural
networks by normalizing it with zero-mean and unit-norm,
followed by a learnable scalar parameter to adjust the norm
of the weight. This technique effectively stabilizes the dis-
tribution implicitly. Besides, it improves the conditioning
of the optimization problem and thus accelerates the train-
ing of deep neural networks. It can be wrapped as a linear
module in practice and plugged in any architecture to re-
place the standard linear module. We highlight the beneﬁts
of our method on both multi-layer perceptrons and convolu-
tional neural networks, and demonstrate its scalability and
efﬁciency on SVHN, CIFAR-10, CIFAR-100 and ImageNet
datasets.

1. Introduction

Recently, deep neural networks have achieved grand
success across a broad range of applications, e.g.,
im-
age classiﬁcation, speech recognition and object detection
[33, 35, 14, 36, 7, 25]. Neural networks typically are com-
posed of stacked layers, and the transformation between
layers consists of linear mapping with learnable parameters,
in which each neuron computes a weighted sum over its in-
puts and adds a bias term, and followed by an element-wise
nonlinear activation. The stacked structure endows a neu-
ral network learning feature hierarchies with features from
higher levels formed by the composition of lower level fea-
tures. Further, deep architectures provide neural network-
s powerful representation capacity of learning complicated
functions that can represent high-level abstractions.

∗Corresponding author

While the deep and complex structure enjoys appeal-
ing advantages, it also makes learning difﬁcult.
Indeed,
many explanations for the difﬁculty of deep learning have
been explored, such as the problem of vanishing and ex-
ploding gradients [2] , internal covariate shift [16], and
the pathological curvature [20]. To address these prob-
lems, various studies such as ﬁnely weight initialization
[19, 9, 13, 29, 21], normalization of internal activation
[16, 4], and sophistic optimization methods have been pro-
posed accordingly [20, 12, 11].

Our work is dedicated to the problem of pathological
curvature [20], i.e. the condition number of the Hessian ma-
trix of the objective function is low at the optimum region-
s [28], which makes learning extremely difﬁcult via ﬁrst
order stochastic gradient descent. Several studies [12, 11]
recently have attempted to use the pre-conditioning tech-
niques to improve the conditioning of the cost curvature.
However, these methods introduce too much overhead and
are not convenient to be applied.

An alternative track to facilitate the optimization
progress is the transformation of parameterization space of
a model [1], which is called re-parameterization. The mo-
tivation of re-parameterization is that there may be various
equivalent ways to parameterize the same model, some of
which are much easier to optimize than others [28]. There-
fore, exploring good ways of parameterizing neural net-
works [6, 28] are essentially important in training deep neu-
ral networks.

Inspired by the practical trick that weights are sampled
from a distribution with zero mean and a standard deviation
for initialization [19, 9, 13], in this paper we propose to con-
strain the input weight of each neuron with zero mean and
unit norm by re-parameterization during the course of train-
ing, followed by a learnable scalar parameter to adjust the
norm of the input weight. We use proxy parameters and per-
form gradient updating on these proxy parameters by back-
propagating the gradient information through the normal-
ization process (Figure 1). By introducing this process, the
summed input of each neuron is more likely to possess the

12803

properties of zero mean and stable variance. Besides, this
technique can effectively improve the conditioning of the
optimization problem and thus can accelerate the training
of deep neural networks.

We wrap the proposed re-parameterization method into
a linear module in practice, which can be plugged in any
architecture as a substitute for the standard linear module.
The technique we present is generic and can be applied
to a broad range of models. Our method is also orthogo-
nal and complementary to recent advances in deep learn-
ing, such as Adam optimization [17] and batch normaliza-
tion [16]. We conduct comprehensive experiments on Yale-
B, SVHN, CIFAR-10, CIFAR-100 and ImageNet datasets
over multi-layer perceptron and convolutional neural net-
work architectures. The results show that centered weight
normalization draws its strength in improving the perfor-
mance of deep neural networks. Our code is available at
https://github.com/huangleiBuaa/CenteredWN.

2. Related work

Training deep neural network via ﬁrst order stochastic
gradient descent is difﬁcult in practice, mainly due to the
pathological curvature problem. Several works have tried
the preconditioning techniques to accelerate the training.
Martens and Sutskever [20] developed a second-order opti-
mization method based on the Hessian-free approach. Oth-
er studies [12, 11] explicitly pre-multiply the cost gradient
by an approximate inverse of the Fisher information ma-
trix, thereby expecting to obtain an approximate natural
gradient. The approximate inverse can be obtained by us-
ing Cholesky factorization [12] or Kronecker-factored ap-
proximation [11]. These methods usually introduce much
overhead. Besides, their optimization procedures are usu-
ally coupled with the preconditioning, and thus can not be
easily applied.

Some works addressed the beneﬁts of centered activa-
tions [36] and gradients [26]. They show that these transfor-
mations make the Fisher information matrix approximate
block diagonal, and improve the optimization performance.
Batch normalization [16] further standardizes the activa-
tion with centering and scaling based on mini-batch, and
includes normalization as a part of the model architecture.
Ba et al. [4] computed the layer normalization statics over
all the hidden units in the same layers, targeting at the sce-
nario that the size of mini-batch is limited. These methods
focus on normalizing the activation of the neurons explicit-
ly while our method works by re-parameterizing the model,
and is expected to achieve better conditioning and stabilize
the activations implicitly.

Re-parameterization is an effective technique to facili-
tate the optimization progress [6, 28]. Guillaume et al. [6]
tried to estimate the expected projection matrix, and implic-
itly whitening the activations. However, the operation of

(cid:3039)
(cid:2032)(cid:4666)(cid:2204)(cid:3036)

(cid:4667)

(cid:3039)
(cid:2204)(cid:3036)

(cid:3039)
(cid:2205)(cid:3036)

. . .

. . .

. . .

(cid:1486)(cid:4666)

(cid:2034)(cid:1838)
(cid:3039)(cid:4667)
(cid:2034)(cid:2205)(cid:3036)

.
 
.
 
.

=

(cid:3039)
(cid:1878)(cid:3036)

(cid:3039)
(cid:2205)(cid:3036)

(cid:2190)

(cid:4666)(cid:3039)(cid:2879)(cid:2869)(cid:4667)

.
 
.
 
.

. . .

. . .

. . .

.
 
.
 
.

.
 
.
 
.

(cid:4666)(cid:3039)(cid:2879)(cid:2869)(cid:4667)

(cid:3039)
(cid:2190)

(cid:3013)
(cid:2190)

(cid:2207)

(cid:2190)

(cid:2206)

Figure 1. An illustrative example of layered neural networks with
re-parameterization (for brevity, we leave out the bias nodes). The
proposed re-parameterization method ﬁnely constructs a transfor-
mation ψ over the proxy parameter v to ensure that the trans-
formed weight w has certain beneﬁcial properties for the training
of neural network. Gradient updating is executed on the proxy pa-
rameter v by back-propagating the gradient information through
the normalization process.

updating the projection matrix is still coupled with the opti-
mization. Salimans and Kingma [28] designed weight nor-
malization [28] as a part of the model architecture. The pro-
posed weight normalization addresses the normalization of
the input weight for each neuron and decouples the length of
those weight vectors from their directions. Our work further
powers weight normalization by centering the input weight,
and we argue that it can ulteriorly improve the conditioning
and speed up the training for deep neural networks.

There exist studies constructing orthogonal matrix in
recurrent neural networks (RNN) [2, 37, 8] by using re-
parameterization to avoid the gradient vanish and explosion
problem. However, these methods are limited for the hid-
den to hidden transformation in RNN, because they require
the weight matrix to be square matrix. Other alternative
methods [5, 38] focus on reducing the storage and compu-
tation costs by re-parameterization. Different from them,
our work aims to design a general re-parameterized linear
module to accelerate training and improve the performance
of deep neural networks, and make it an alternative for the
standard linear module.

3. Centered weight normalization

We follow the matrix notation that the vector is in col-
umn form, except that the derivative is a row vector. Given
training data D = {(xi, yi), i = 1, 2, ..., M } where x de-
notes the input and y the target. A neural network is a func-
tion f (x; θ) parameterized by θ that is expected to ﬁt well
the training data and have good generalization for the test
data. The function f (x; θ) adopted by neural network usu-
ally consists of stacked layers. The transformation between
layers consists of a linear mapping zl = (Wl)T hl−1 + bl
with learnable parameters Wl ∈ Rd×n and bl ∈ Rn, and
followed by an element-wise nonlinearity: hl = ϕ(sl),
where l ∈ {1, 2, ..., L} indexes the layer and L denotes the

2804

total number of layers. By convention, h0 corresponds to
the input x and hL corresponds to the output of the net-
work f (x; θ). For clariﬁcation, we refer to z and h as pre-
activation and activation respectively. Under the denotation,
the learnable parameters are θ = {Wl, bl|l = 1, 2, . . . , L}.
Training the neural networks can be viewed as tuning the
parameters to minimize the discrepancy between the desired
output y and the predicted output f (x; θ). This discrepancy
is usually described by a loss function L(y, f (x; θ)), and
thus the objective is to optimize the parameters θ by mini-
mizing the loss as follows:

θ∗ = arg min

E

(x,y)∈D[L(y, f (x; θ))].

(1)

θ

Stochastic gradient descent (SGD) has proved to be an
effective way to train deep networks, in which the gradien-
t of the loss function with respect to the parameters ∂L
∂θ is
approximated by the mini-batch x1...m of size m at each
iteration, by computing ∂L
∂θ = 1

∂L(yi,f (xi;θ))
∂θ

m Σm
i=1

.

3.1. Methodology

Despite the fact that SGD can guarantee a local optimum,
it is also well known the practical success of SGD is highly
dependent on the curvature of the objective to be optimized.
Deep neural network usually exhibits pathological curva-
ture problem, which makes the learning difﬁcult. An alter-
native track to relieve the pathological curvature issue and
thus facilitate the optimization progress is the transforma-
tion of parameterization space of a network model, which
is called re-parameterization.

In the literatures, there exist practical tricks for weight
initialization where weights are sampled from a distribu-
tion with zero-mean and a standard deviation [19, 9, 13].
These initialization techniques effectively can avoid expo-
nentially reducing/magnifying the magnitudes of input sig-
nals or back-propagation signals in the initial phases, by
constructing stable and proper variances of the activations
among different layers. Motivated by this observation, we
propose to constrain the input weight of each neuron with
zero mean and unit norm during the course of training by
re-parameterization, which enjoys stable and fast training
of deep neural networks.

For simplicity, we start by considering one certain neu-
ron i in layer l, whose pre-activation zl
i)T h(l−1)+bl
i.
We denote wl
i as the input weight of the neuron, as shown
in Figure 1. We have left out the superscript l and subscript
i for brevity in the following discusses.

i = (wl

Standardize weight We ﬁrst re-parameterize the input
weight w of each neuron and make sure that it has the fol-
lowing properties: (1) zero-mean, i.e. wT 1 = 0 where 1 is
a column vector of all ones; (2) unit-norm, i.e. (cid:3)w(cid:3) = 1
where (cid:3)w(cid:3) denotes the Euclidean norm of w. To achieve

Algorithm 1 Forward pass of linear mapping with centered
weight normalization.

1: Input:

the mini-batch input data X ∈ Rd×m and
parameters to be learned: g ∈ Rn×1, b ∈ Rn×1,
V ∈ Rd×n.

2: Output: pre-activation Z ∈ Rn×m.
3: compute centered weight: ˆV = V − 1
4: for i = 1 to n do
5:

d 1d(1T

d V).

calculate normalized weight with respect to the i-th
neuron: wi = ˆvi
(cid:4)ˆvi(cid:4)

6: end for
7: calculate: ˆZ = WT X.
8: calculate pre-activation: Z = (g1T

m) ⊙ ˆZ + b1T
m.

the goal, we express the input weight w in terms of the prox-
y parameter v (Figure 1) using

w =

v − 1
(cid:3)v − 1

d 1(1T v)
d 1(1T v)(cid:3)

(2)

where d is the dimension of the input weight, and the s-
tochastic gradient descent or other alternative techniques
such as Adam [17] can be directly applied to the optimiza-
tion with respect to the proxy parameter v. We refer to the
proposed re-parametrization as centered weight normaliza-
tion, that is, we center and scale the proxy parameter v to
ensure that the input weight w has the desired zero-mean
and unit-norm properties as discussed before.

In our centered weight normalization, the parameter up-
dating is completed based on the proxy parameters, and
thus the gradient signal should back-propagate through the
normalization process. Speciﬁcally, given the derivative of
∂L/∂w, we can get the derivative of loss with respect to the
proxy parameter v as follows:

∂L
∂v

=

1
(cid:3)ˆv(cid:3)

[

∂L
∂w

− (

∂L
∂w

w)wT −

1)1T ]

(3)

1
d

(

∂L
∂w

where ˆv = v − 1
ter.

d 1(1T v) is the centered auxiliary parame-

Adjustable weight scale Our method can be viewed ef-
fectively as a solution to the constrained optimization prob-
lem over neural networks:

θ∗ = arg minθ E

(x,y)∈D[L(y, f (x; θ))]

s.t. wT 1 = 0 and (cid:3)w(cid:3) = 1

(4)

where w indicates the input weight for each neuron in each
layer. Therefore, we regularize the network with 2n con-
straints in each layer where n is the number of ﬁlters in cer-
tain layer and the optimization is over the embedded sub-
manifold of the original weight space. While these con-
straints provide regularization, they also may reduce the

2805

Algorithm 2 Back-propagation pass of linear mapping with
centered weight normalization.
1: Input: pre-activation derivative { ∂L

∂Z ∈ Rn×m}. Oth-
er auxiliary variables from respective forward pass: ˆV,
W, ˆZ, X, g.

2: Output: the gradients with respect to the inputs { ∂L
∂g ∈ R1×n, ∂L

∂X ∈
∂b ∈

4:

5:

3:

∂Z ⊙ ˆZ)T

Rd×m} and learnable parameters: ∂L
∂V ∈ Rd×n.
R1×n, ∂L
m( ∂L
∂L
∂g = 1T
T
∂L
∂L
∂b = 1T
m
∂Z
= ∂L
∂L
∂Z ⊙ (g1T
m)
∂ ˆZ
∂X = W ∂L
∂L
∂ ˆZ
∂L
∂W = X ∂L
7:
∂ ˆZ
8: for i = 1 to n do
∂L
∂vi = 1
9:
10: end for

∂wi − ( ∂L

∂wi wi)wT

(cid:4)ˆvi(cid:4) ( ∂L

6:

T

i − 1

d ( ∂L

∂wi 1d)1T
d )

representation capacity of the networks. To address it, we
simply introduce a learnable scalar parameter g to ﬁne tune
the norm of w. Similar idea has been introduced in [28],
and proved useful in practice. We initialize it to 1, and ex-
pect the network learning process can ﬁnd a proper scalar
under the supervision signals. To summarize, we rewrite
the pre-activation z of each neuron in the following way

z = g(

v − 1
(cid:3)v − 1

d 1(1T v)
d 1(1T v)(cid:3)

)T h + b.

(5)

Wrapped module with re-parameterization We can
wrap our proposed method as a common module and plug-
in it in the neural networks as a substitute for the ordinary
linear module. To achieve this goal, the key is to implemen-
t the forward and back-propagation passes. Based on Eqn.
(5), it is readily to extend for multiple output neurons of size
n. We describe the details of the forward pass in Algorithm
1, and the back-propagation pass in Algorithm 2. In the al-
gorithms, the ‘⊙’ operator represents element-wise matrix
multiplication, 1d indicates a column vector of all ones of
size d, and X ∈ Rd×m is the mini-batch input data feeded
into this module, where d is the dimension of the input and
m is the size of the mini-batch. vi is the i-th column of V,
and W = (w1, w2, ..., wn).

layer For the convolutional

Convolutional
the
weight of each feature map is wc ∈ Rd×Fh×Fw where Fh
and Fw indicate the height and width of the ﬁlter, and d is
the number of the input feature maps. We unroll wc as a
FhFwd dimension vector w, then the same normalization
can be directly executed over the unrolled w.

layer,

3.2. Analysis and discussions

Next, we will show our centered weight normalization
enjoys certain appealing properties. It can stabilize the dis-
tribution of pre-activation z with respect to each neuron. To
illustrate this point, we introduce the following proposition
where we omit the bias term b to simplify the discussion.

Proposition 1. Let z = wT h, where wT 1 = 0 and (cid:3)w(cid:3) =
1. Assume h has Gaussian distribution with the mean:
Eh[h] = μ1, and covariance matrix: cov(h) = σ2I, where
μ ∈ R and σ2 ∈ R. We have Ez[z] = 0, var(z) = σ2.

The proof of this proposition is provided in the supple-
mentary materials. Such a proposition tells us that for each
neuron the pre-activation z has zero-mean and the same
variance as the activations fed in, when the assumption is
satisﬁed. This property does not strictly hold in practice due
to the issue of nonlinearities in the hidden layers, however,
we still empirically ﬁnd that our proposed centered weight
normalization approximately holds.

Note that our method is similar to batch normalization
[16]. However, batch normalization focuses on normalizing
the pre-activation compulsively such that the pre-activation
of current mini-batch data is zero-mean and unit-variance,
which is a data dependent normalization. Our method nor-
malizes the weights and is expected to have the effect of
zero-mean and stable variance implicitly, which is a data
independent normalization. Actually, our method can work
well by combining batch normalization as described in sub-
sequent experiments.

Besides the good property guaranteed by Proposition
1, another remarkable observation is that the proposed re-
parameterization method can make the optimization prob-
lem easier, which is supported by the following proposition.

Proposition 2. Regarding to the proxy parameter v, cen-
tered weight normalization makes that the gradient ∂L
∂v has
following properties: (1) zero-mean, i.e. ∂L
∂v · 1 = 0; (2)
orthogonal to the parameters w, i.e. ∂L

∂v · w = 0.

The derivative of Proposition 2 is also given in the sup-
plementary materials. The zero-mean property of ∂L/∂v
usually has an effect that the leading eigenvalue of the Hes-
sian is smaller, and therefore the optimization problem is
likely to become better-conditioned [30]. This promises the
network to learn at a higher rate, and hence converge much
faster. Meanwhile, the gradient is orthogonal to the param-
eters w, and therefore is orthogonal to ˆv. Following the
analysis in [28], the gradient ∂L/∂v can self-stabilize it-
s norm, which thus makes optimization of neural networks
robust to the value of the learning rate.

Computation complexity Given mini-batch input data
{X ∈ Rd×m}, regarding n neurons, both the forward pass
(Algorithm 1) and back-propagation pass (Algorithm 2) of

2806

0.5

0

n
a
e
m

−0.5

plain−L1
plain−L2
CWN−L1
CWN−L2

plain−L1
plain−L2
CWN−L1
CWN−L2

4

3

2

1

e
c
n
a
i
r
a
v

1080

1060

1040

1020

M
I
F
 
f
o
 
r
e
b
m
u
n
 
n
o
i
t
i
d
n
o
c

plain
NNN
WN
CWN

4

3

2

1

s
s
o
l
 
g
n
i
n
i
a
r
t

plain
NNN
WN
CWN

−1
0

200
400
updates (x10)

600

0
0

200
400
updates (x10)

600

100

0

20
40
updates (x100)

60

0
0

2000

4000

6000

updates

(a)

(b)

(c)

(d)

Figure 2. A case study on Yale-B dataset (‘-Ll’ indicates the l-th layer). With respect to the training updates, we analyze (a) the mean
over the mini-batch data and all neurons; (b) the variance calculated over the mini-batch data and all neurons; (c) the condition number
(log-scale) of relative FIM in the second last layer; and (d) the training loss.

the centered weight normalization have the computational
complexity of O(mnd + nd). This means that our centered
weight normalization has the same computational complex-
ity as the standard linear module, since the extra cost O(nd)
of centered weight normalization is negligible to O(mnd).
For a convolution layer with ﬁlters W ∈ Rn×d×Fh×Fw ,
given m mini-batch data {xi ∈ Rd×h×w, i = 1, 2, ..., m},
where h and w represent the height and width of the feature
map, the computational complexity of the standard convo-
lution layer is O(nmdhwFhFw), and while our proposed
method is O(mndhwFhFw + ndFhFw). The extra cost
O(ndFhFw) of the proposed normalization is also negligi-
ble compared to the convolution operation.

4. Experiments

In this section, we begin with a set of ablation study to
support the preliminary analysis of the proposed method.
Then in the following three subsections, we show the ef-
fectiveness of our proposed method on the general network
architectures, including (1) Multi-Layer Perceptron (MLP)
architecture for face and digit recognition tasks; and (2)
Convolutional Neural Network (CNN) models for large s-
cale image classiﬁcation.

Experimental protocols For all experiments, we adop-
t ReLUs [22] as the nonlinear activation and the negative
log-likelihood as the loss function, where (x, y) represents
the (input image, target class) pair. We choose the random
weight initialization by default as described in [19] if we
do not specify weight initialization methods. For the exper-
iments on MLP architecture, the input images are resized
and transformed to 1,024 dimensional vectors in gray scale,
with mean subtracted and variance divided.

4.1. Case study

As discussed in Section 3.2,

the proposed Centered
Weight Normalization (CWN) method can stabilize the
distribution of the pre-activations during training under
certain assumptions. A network with the proposed re-
parameterization technique is likely to become better-
conditioned. In this part, we conduct experiments to vali-
date the conclusions empirically.

We conduct the experiments on Yale-B dataset1 for face
recognition task. Yale-B dataset has 2,414 images with 38
classes. We randomly sample 380 images (i.e., 10 images
per class) as the test set and the others as training set, where
all images are resized as 32 × 32 pixels. We train a 5-layer
MLP with {128, 64, 48, 48} neurons in the hidden layers.
We compare our CWN with the plain network without re-
parameterization (named ‘plain’ for short). In the training,
we use stochastic gradient descent with batch size of 32.
For each method, the best results by tuning different learn-
ing rates in {0.1, 0.2, 0.5, 1} are reported.

Stabilize activation We investigate the mean and vari-
ance of pre-activations in each layer during the course of
training. Figure 2 (a) and (b) respectively show the mean
and variance from the ﬁrst and second layers, with respect
to different rounds of training updates. The mean and vari-
ance are calculated over the mini-batch data and all neuron-
s2 in the corresponding layers. We ﬁnd that the distribution
of pre-activation in plain network varies remarkably. For in-
stance, the mean decreases and the variance increases grad-
ually, even though the network nearly converges to the mini-
mal loss. The unstable distribution of pre-activation may re-
sult in an unstable learning, as shown in Figure 2 (d), where
the loss of plain network ﬂuctuates sharply. By comparing
the loss curve of CWN and the others, we ﬁnd that the pre-
activation of the network with CWN re-parameterization
has a stable mean and variance as shown in Figure 2 (a) and
(b). Especially, in the ﬁrst layer the pre-activation is nearly
zero-mean, which empirically supports the fact in Proposi-
tion 1. These beneﬁcial properties make the network with
CWN re-parameterization converge to an optimum regions
in a stable and fast way.

Information Matrix (FIM)

analysis The

Conditioning
the Fisher
dicator
to
lem is easy to solve.
Ex∼π{Ey∼P (y|x,θ)[( ∂ log P (y|x,θ)

show whether

∂θ

condition

of
number
is a good in-
optimization
prob-
FIM is deﬁned as Fθ =

the

)( ∂ log P (y|x,θ)
∂θ

)T ]},

1http://vision.ucsd.edu/ leekc/ExtYaleDatabase/ExtYaleB.html
2We also observe the mean and variance of the randomly selected neu-

rons in each layer, which also have the similar behaviours.

2807

50

100
epochs

150

200

50

100
epochs

150

200

(a) training error

(b) test error

Figure 3. Performances evaluation on MLP architecture over
permutation-invariant SVHN.

0.8

0.6

0.4

0.2

r
o
r
r
e
 
g
n
i
n
i
a
r
t

0
0

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
NNN
WN
CWN

BN
NNN+BN
WN+BN
CWN+BN
CWN

r
o
r
r
e
 
t
s
e
t

0.7

0.6

0.5

0.4

0.3

0.2

0.1
0

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
NNN
WN
CWN

plain
NNN
WN
CWN

50

100
epochs

150

200

50

100
epochs

150

200

(a) combining batch normalization

(b) using Adam optimization

Figure 4. Comparison of different methods by (a) combining batch
normalization and (b) using Adam optimization on permutation-
invariant SVHN. We evaluate training error (solid lines) and test
error (lines marked with plus) with respect to the training epochs.

and its condition number is cond(Fθ) = |λ(Fθ)|max
|λ(Fθ)|min
where |λ(Fθ)|max and |λ(Fθ)|min are the maximum and
minimum of the absolute values of Fθ’ eigenvalues. We
evaluated the condition number of relative FIM [32] with
respect to the last two layers, regarding the feasibility in
computation. We compared our methods with ‘plain’ and
the other two re-parameterization methods: Natural Neural
Network (NNN) [6] and Weight Normalization (WN) [28].
The results of the second last layer are shown in Figure 2
(c) and the last layer also has similar results, from which we
can ﬁnd that CWN, WN, NNN achieve better conditioning
compared to ‘plain’, and thus converges faster as shown in
Figure 2 (d). This indicates that re-parameterization serves
as an effective way to make the optimization problem easier
if good properties are designed carefully. Compared to WN,
our proposed CWN method further signiﬁcantly improves
the conditioning and speed up convergence. This observa-
tion is consistent with our analysis in Section 3.2 that the
proposed zero-mean property of input weight contributes
to making the optimization problem easier. At the testing
stage, the proposed CWN method achieves the lowest test
error as shown in Table 3, which means that CWN not only
accelerates and stabilizes training, but also has great poten-
tial to improve generalization of the neural network.

4.2. MLP architecture

In this part, we comprehensively evaluate our proposed
method on MLP architecture. We investigate both the train-
ing and test performances on SVHN [23]. SVHN consist-
s of 32 × 32 color images of house numbers collected by
Google Street View. It has 73,257 train images and 26,032
test images. We train a 6 layers MLP with 128 neurons cor-

Table 1. Comparison of test errors (%) averaged over 5 indepen-
dent runs on Yale-B and permutation-invariant SVHN.

plain
Yale-B
6.16
SVHN 18.98

NNN WN
4.68
6.58
17.12
17.99

CWN
4.20
16.16

respondingly in each hidden layer. Note that here we mainly
focus on validating the effectiveness of our proposed meth-
ods on MLP architecture, and under this situation, SVHN
dataset is permutation invariant.

We employ the stochastic gradient descent algorith-
m with mini-batch of size 1024.
Hyper-parameters
are selected by grid search, based on the test er-
ror on validation set (5% samples of the training set)
learning rates in
with the following grid speciﬁcations:
{0.1, 0.2, 0.5, 1}; the natural re-parameterization interval T
in {20, 50, 100, 200, 500} and the revised term ε within the
range of {0.01, 0.01, 0.1, 1} for NNN method.

Figure 3 shows the training and test error with respect
to the number of epochs. We can ﬁnd that WN, NNN and
CWN converge faster compared to ‘plain’, which indicates
that re-parameterization serves as an effective way to accel-
erate the training. Among all re-parametrization methods,
CWN converges fastest. Besides, we observe that both the
training and test error of CWN do not ﬂuctuate after a few
number of epochs in the training, which means that CWN is
much more stable when it reaches the optimal regions. We
can conjecture that the following functions of CWN as dis-
cussed in Section 3.2 contributes to this phenomenon: (1)
CWN can stabilize the distribution of pre-activation; (2) the
gradient ∂L/∂v of CWN can self-stabilize its norm. Table
3 presents the test error for different methods, where we fur-
ther ﬁnd that CWN achieves the best performances. These
observations together show the great potential of CWN in
improving the generalization of the neural network.

Combining batch normalization Batch normalization
[16] has shown to be very helpful for speeding up the train-
ing of deep networks. Combined with batch normalization,
previous re-parameterization methods [6] have shown suc-
cess in improving the performance. Here we show that over
the networks equipped with batch normalization, our pro-
posed CWN still outperforms others in all cases.
In this
experiment, Batch Normalization (BN) is plugged in be-
fore the nonlinearity in neural networks as suggested in
[16]. With batch normalization, we build different net-
works using the re-parameterized linear mapping including
WN, NNN and CWN, named ‘WN+BN’, ‘NNN+BN’ and
‘CWN+BN’ respectively.

Figure 4 (a) shows their experimental results on SVHN
dataset. We ﬁnd that ‘WN+BN’ and ‘NNN+BN’ have no
advantages compared to ‘BN’, while ‘CWN+BN’ signiﬁ-
cantly speeds up the training and achieves better test per-

2808

Table 2. Comparison of test errors (%) averaged over 5 inde-
pendent runs on Yale-B and permutation-invariant SVHN with
Xavier-Init and He-Init.

Table 3. Comparison of test errors (%) averaged over 3 inde-
pendent runs on 56 layers residual network over CIFAR-10 and
CIFAR-100 datasets.

Xavier-Init

He-Init

method Yale-B SVHN Yale-B SVHN
18.35
17.78
plain
18.14
17.83
NNN
WN
18.06
17.74
16.71
16.38
CWN

5.47
5.58
5.58
4.21

6.47
5.47
4.68
3.84

formance. Indeed, batch normalization is invariant to the
weights scaling as analyzed in [24] and [4]. However, batch
normalization is not re-centering invariant [4]. Therefore,
our CWN can further improve the performance of batch nor-
malization by centering the weights.

Amazingly, CWN itself achieves remarkably better per-
formance in terms of both the training speed and the
test error than BN, and even better than ‘CWN+BN’.
This is mainly due to the following reasons. Batch nor-
malization can well stabilize the distribution of the pre-
activation/activation, however it also introduces noises
stemming from the forceful transformation based on mini-
batch. Besides, during test the means and variances are es-
timated based on the moving averages of mini-batch means
and variances [16, 3, 4]. These ﬂaws may have a detrimental
effect on models. Our CWN can stabilize the distribution of
pre-activation implicitly and is deterministic without intro-
ducing stochastic noise. Moreover, the properties described
in Proposition 2 can improve the conditioning, self-stabilize
the gradient’s norm, and thus achieve better performance.

Adam optimization We consider an alternative optimiza-
tion method, Adam [17], to evaluate the performance. The
initial learning rate is set to {0.001, 0.002, 0.005, 0.01}, and
the best results for all methods on SVHN dataset are shown
in Figure 4 (b). Again, we ﬁnd that CWN can reach the
lowest errors at both training and test stages.

Different initialization We also have tried different ini-
tialization methods: Xavier-Init [9] and He-Init [13]. The
experimental setup is the same as in previous experiments.
The ﬁnal test errors both on Yale-B and SVHN datasets are
shown in Table 2. We get similar conclusions as the ran-
dom initialization [19] in previous experiments that CWN
achieves the best test performance.

4.3. CNN architectures on CIFAR dataset

In this part, we highlight that the proposed centered
weight normalization method also works well on several
popular state-of-the-art CNN architectures3, including VG-
G [31], GoogLeNet [34], and residual network [14, 15]. We

3The details of the used CNN architectures are shown in the supple-

mentary materials.

Methods
plain
WN
CWN

CIFAR-10
7.34 ± 0.52
7.58 ± 0.40
6.85 ± 0.26

CIFAR-100
29.38 ± 0.14
29.85 ± 0.66
29.23 ± 0.14

evaluate it over CIFAR-10 and CIFAR-100 datasets [18],
which consists of 50,000 training images and 10,000 test
images from 10 and 100 classes respectively. Each input
image consists of 32 × 32 pixels.

VGG-A We ﬁrst evaluate the proposed methods on the
VGG-A architecture [31] where we set the number of neu-
rons to 512 in the fully connected layer and use batch nor-
malization in the ﬁrst fully connected layer. The exper-
iments run on CIFAR-10 dataset and we preprocess the
dataset by subtracting the mean and dividing the variance.

We employ the stochastic gradient descent as the op-
timization method with mini-batch size of 256, and use
momentum of 0.9, weight decay of 0.0005. The learn-
ing rate decays with each K iterations halving the learn-
ing rate. We initialize the learning rate lr = {0.5, 1, 2, 4},
and K = {1000, 2000, 4000, 8000}. The hyper-parameters
are chosen over the validation set of 5,000 examples from
the training set by grid search. We report the results of
‘plain’, WN and CWN in Figure 5 (a). It is easy to see that
with the proposed CWN the training converges much faster
and promises the lowest test error of 13.06%, compared
to WN of 15.89% and ‘plain’ of 14.33%. Note that here
WN obtains even worse performance compared to ‘plain’,
which is mainly because that WN can not ensure the pre-
activations nearly zero-mean, and thus degenerate the per-
formance when the network is deep.

GoogleLeNet We conduct the experiments on Google-
LeNet. Here GoogleLeNet is equipped with the batch nor-
malization [16], plugged after the linear mappings. The
dataset is preprocessed as described in [10] with global con-
trast normalization and ZCA whitening. We follow the sim-
ple data augmentation that 4 pixels are padded on each side,
and a 32 × 32 crop is randomly sampled from the padded
image or its horizontal ﬂip. The models are trained with
a mini-batch size of 64, momentum of 0.9 and weight de-
cay of 0.0005. The learning rate starts from 0.1 and ends
at 0.001, and decays every two epochs with exponentially
decaying until the end of the training with 100 epochs. The
results on CIFAR-100 are reported in Figure 5 (b), from
which we can ﬁnd that CWN obtains slight speedup com-
pared to WN and ‘plain’. Moreover, CWN can attain the
lowest test error of 24.45%, compare to ‘plain’ of 25.52%
and WN of 25.39%. We also obtain similar observations on
CIFAR-10 (see the supplementary materials).

2809

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
WN
CWN

0.8

0.6

0.4

0.2

r
o
r
r
e
 
t
s
e
t

0.8

0.6

0.4

0.2

r
o
r
r
e

0
0

plain
WN
CWN

0.8

0.6

0.4

0.2

r
o
r
r
e

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
WN
CWN

plain
WN
CWN

plain
WN
CWN

20

40

epochs

(a)

epochs

(b)

60

0
0

20

40

60

80

100

50

100

epochs

150

50

100

epochs

150

(c)

(d)

Figure 5. Performance comparison on various architecture over CIFAR datasets. We evaluate the training error (solid lines) and test error
(lines marked with plus) with respect to the training epochs. (a) VGG-A architecture over CIFAR-10; (b) GoogLeNet architecture over
CIFAR-100; (c) residual network architecture over CIFAR-10; (d) residual network architecture over CIFAR-100.

Residual network We also apply our CWN module to the
residual network [14] with 56 layers. We follow the same
experimental protocol as described in [14] and the publicly
available Torch implementation for residual network 4. Fig-
ure 5 (c) and (d) show the training error and test error with
respect to epochs on CIFAR-10 and CIFAR-100 dataset re-
spectively, and the test errors after training are shown in Ta-
ble 3. We can ﬁnd that CWN converges slightly faster than
WN and ‘plain’ and achieves the best test performance.

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
i
a
r
t

Computational cost We implement our CWN module
based on Torch, and the re-parameterization is wrapped in
the fastest SpatialConvolution of cudnn package 5. We com-
pared the wall clock time of CWN module with the standard
convolution of cudnn. We use 3 × 3 convolution, 32 × 32
feature map with size 128 and mini-batch size of 64. The re-
sults are averaged over 100 runs. CWN costs 18.1ms while
the standard one takes 17.3ms.

4.4. Large scale classiﬁcation on ImageNet dataset

To show the scalability of our proposed method, we ap-
ply it to the large-scale ImageNet-2012 dataset [27] using
the GoogLeNet architecture. ImageNet-2012 includes im-
ages of 1,000 classes, and is split into training sets with
1.2M images, validation set with 50k images, and test set
with 100k images. We employ the validation set as the test
set, and evaluate the classiﬁcation performance based on
top-1 and top-5 error. We use single scale and single crop
test for simplifying discussion. Here we again use stochas-
tic gradient descent with mini-batch size of 64, momentum
of 0.9 and weight decay of 0.0001. The learning rate starts
from 0.1 and ends at 0.0001, and decays with exponentially
decaying until the end of the training with 90 epochs. The
top-5 training and test error curves are shown in Figure 6
and the ﬁnal test errors are shown in Table 4, where we can
ﬁnd that the deep network with CWN can converge to a low-
er training error faster, and obtain lower test error. We argue
that our CWN module draws its strength in improving the
performance of deep neural networks.

4https://github.com/facebook/fb.resnet.torch
5https://developer.nvidia.com/cudnn

plain
WN
CWN

0
0

20

40
epochs

60

80

0
0

20

40
epochs

60

80

(a) training error

(b) test error

Figure 6. Top-5 training and test error curves on GoogLeNet ar-
chitecture over ImageNet-2012 dataset.

Table 4. Comparison of test errors (%) on GoogLeNet over
ImageNet-2012 dataset.

Methods
plain
WN
CWN

Top-1 error
30.78
28.64
26.1

Top-5 error
11.14
9.7
8.35

5. Conclusions

re-
In this paper we introduced a new efﬁcient
parameterization method named centered weight normal-
ization, which improves the conditioning and accelerates
the convergence of the deep neural networks training. We
validate its effectiveness in image classiﬁcation tasks over
both multi-layer perceptron and convolutional neural net-
work architectures. The re-parameterization method is able
to stabilize the distribution and accelerate the training for
a number of popular networks. More importantly, it has
very low computational overhead and can be wrapped as a
linear module suitable for different types of networks. We
argue that the centered weight normalization module owns
the potential to replace the standard linear module, and con-
tributes to new deep architectures with easier optimization
and better generalization.

Acknowledgement

This work was partially supported by NSFC-61370125,
NSFC-61402026, SKLSDE-2017ZX-03, FL-170100117,
DP-140102164, LP-150100671 and the Innovation Foun-
dation of BUAA for PhD Graduates.

2810

References

[1] S.-I. Amari. Natural gradient works efﬁciently in learning.

Neural Comput., 10(2):251–276, Feb. 1998. 1

[2] M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution

recurrent neural networks. In ICML, 2016. 1, 2

[3] D. Arpit, Y. Zhou, B. U. Kota, and V. Govindaraju. Normal-
ization propagation: A parametric technique for removing
internal covariate shift in deep networks. In ICML, 2016. 7
[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.

CoRR, abs/1607.06450, 2016. 1, 2, 7

[5] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. D. Fre-
In Advances
itas. Predicting parameters in deep learning.
in Neural Information Processing Systems 26, pages 2148–
2156, 2013. 2

[6] G. Desjardins, K.

Simonyan,

R.

Pascanu,

K. Kavukcuoglu. Natural neural networks.
2015. 1, 2, 6

and
In NIPS,

[7] Z. Ding, M. Shao, and Y. Fu. Deep robust encoder through
In ECCV, pages

locality preserving low-rank dictionary.
567–582, 2016. 1

[8] V. Dorobantu, P. A. Stromhaug, and J. Renteria. Dizzyrn-
n: Reparameterizing recurrent neural networks for norm-
preserving backpropagation. CoRR, abs/1612.04035, 2016.
2

[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
2010. 1, 3, 7

[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. C.
Courville, and Y. Bengio. Maxout networks. In ICML, 2013.
7

[11] R. B. Grosse and J. Martens. A kronecker-factored approxi-
mate ﬁsher matrix for convolution layers. In ICML, 2016. 1,
2

[12] R. B. Grosse and R. Salakhutdinov. Scaling up natural gra-
dient by sparsely factorizing the inverse ﬁsher matrix.
In
ICML, 2015. 1, 2

[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 1, 3, 7

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 7, 8

[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. CoRR, abs/1603.05027, 2016. 7
[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 1, 2, 4, 6, 7

[17] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 2, 3, 7

[18] A. Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, 2009. 7

[19] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁicient
backprop. In Neural Networks: Tricks of the Trade, 1998. 1,
3, 5, 7

[20] J. Martens and I. Sutskever. Training deep and recurrent net-
works with hessian-free optimization. In Neural Networks:
Tricks of the Trade (2nd ed.). 2012. 1, 2

[21] D. Mishkin and J. Matas. All you need is a good init.

In

ICLR, 2016. 1

[22] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 5

[23] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning, 2011. 6

[24] B. Neyshabur, R. Tomioka, R. Salakhutdinov, and N. Sre-
bro. Data-dependent path normalization in neural networks.
CoRR, abs/1511.06747, 2015. 7

[25] G. Qi. Hierarchically gated deep networks for semantic seg-

mentation. In CVPR, pages 2267–2275, 2016. 1

[26] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made
easier by linear transformations in perceptrons. In AISTATS,
2012. 2

[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), 115(3):211–252, 2015. 8

[28] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In NIPS, 2016. 1, 2, 4, 6

[29] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact so-
lutions to the nonlinear dynamics of learning in deep linear
neural networks. CoRR, abs/1312.6120, 2013. 1

[30] N. N. Schraudolph. Accelerated gradient descent by factor-

centering decomposition. Technical report, 1998. 4

[31] K. Simonyan and A. Zisserman. Very deep convolution-
al networks for large-scale image recognition. CoRR, ab-
s/1409.1556, 2014. 7

[32] K. Sun and F. Nielsen. Relative natural gradient for learning
large complex models. CoRR, abs/1606.06069, 2016. 6
[33] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
pages 1988–1996, 2014. 1

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014. 7

[35] Y. Tian, P. Luo, X. Wang, and X. Tang. Deep learning strong
parts for pedestrian detection. In ICCV, pages 1904–1912,
2015. 1

[36] S. Wiesler, A. Richard, R. Schl¨uter, and H. Ney. Mean-
normalized stochastic gradient for large-scale deep learning.
In ICASSP, 2014. 1, 2

[37] S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas.
In NIPS,

Full-capacity unitary recurrent neural networks.
pages 4880–4888. 2016. 2

[38] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, 2015.
2

2811

Centered Weight Normalization
in Accelerating Training of Deep Neural Networks

Lei Huang† Xianglong Liu†∗ Yang Liu† Bo Lang† Dacheng Tao‡
†State Key Laboratory of Software Development Environment, Beihang University, P.R.China
‡UBTECH Sydney AI Centre, School of IT, FEIT, The University of Sydney, Australia
{huanglei, xlliu, blonster, langbo}@nlsde.buaa.edu.cn, dacheng.tao@sydney.edu.au

Abstract

Training deep neural networks is difﬁcult for the patho-
logical curvature problem. Re-parameterization is an effec-
tive way to relieve the problem by learning the curvature
approximately or constraining the solutions of weights with
good properties for optimization. This paper proposes to re-
parameterize the input weight of each neuron in deep neural
networks by normalizing it with zero-mean and unit-norm,
followed by a learnable scalar parameter to adjust the norm
of the weight. This technique effectively stabilizes the dis-
tribution implicitly. Besides, it improves the conditioning
of the optimization problem and thus accelerates the train-
ing of deep neural networks. It can be wrapped as a linear
module in practice and plugged in any architecture to re-
place the standard linear module. We highlight the beneﬁts
of our method on both multi-layer perceptrons and convolu-
tional neural networks, and demonstrate its scalability and
efﬁciency on SVHN, CIFAR-10, CIFAR-100 and ImageNet
datasets.

1. Introduction

Recently, deep neural networks have achieved grand
success across a broad range of applications, e.g.,
im-
age classiﬁcation, speech recognition and object detection
[33, 35, 14, 36, 7, 25]. Neural networks typically are com-
posed of stacked layers, and the transformation between
layers consists of linear mapping with learnable parameters,
in which each neuron computes a weighted sum over its in-
puts and adds a bias term, and followed by an element-wise
nonlinear activation. The stacked structure endows a neu-
ral network learning feature hierarchies with features from
higher levels formed by the composition of lower level fea-
tures. Further, deep architectures provide neural network-
s powerful representation capacity of learning complicated
functions that can represent high-level abstractions.

∗Corresponding author

While the deep and complex structure enjoys appeal-
ing advantages, it also makes learning difﬁcult.
Indeed,
many explanations for the difﬁculty of deep learning have
been explored, such as the problem of vanishing and ex-
ploding gradients [2] , internal covariate shift [16], and
the pathological curvature [20]. To address these prob-
lems, various studies such as ﬁnely weight initialization
[19, 9, 13, 29, 21], normalization of internal activation
[16, 4], and sophistic optimization methods have been pro-
posed accordingly [20, 12, 11].

Our work is dedicated to the problem of pathological
curvature [20], i.e. the condition number of the Hessian ma-
trix of the objective function is low at the optimum region-
s [28], which makes learning extremely difﬁcult via ﬁrst
order stochastic gradient descent. Several studies [12, 11]
recently have attempted to use the pre-conditioning tech-
niques to improve the conditioning of the cost curvature.
However, these methods introduce too much overhead and
are not convenient to be applied.

An alternative track to facilitate the optimization
progress is the transformation of parameterization space of
a model [1], which is called re-parameterization. The mo-
tivation of re-parameterization is that there may be various
equivalent ways to parameterize the same model, some of
which are much easier to optimize than others [28]. There-
fore, exploring good ways of parameterizing neural net-
works [6, 28] are essentially important in training deep neu-
ral networks.

Inspired by the practical trick that weights are sampled
from a distribution with zero mean and a standard deviation
for initialization [19, 9, 13], in this paper we propose to con-
strain the input weight of each neuron with zero mean and
unit norm by re-parameterization during the course of train-
ing, followed by a learnable scalar parameter to adjust the
norm of the input weight. We use proxy parameters and per-
form gradient updating on these proxy parameters by back-
propagating the gradient information through the normal-
ization process (Figure 1). By introducing this process, the
summed input of each neuron is more likely to possess the

12803

properties of zero mean and stable variance. Besides, this
technique can effectively improve the conditioning of the
optimization problem and thus can accelerate the training
of deep neural networks.

We wrap the proposed re-parameterization method into
a linear module in practice, which can be plugged in any
architecture as a substitute for the standard linear module.
The technique we present is generic and can be applied
to a broad range of models. Our method is also orthogo-
nal and complementary to recent advances in deep learn-
ing, such as Adam optimization [17] and batch normaliza-
tion [16]. We conduct comprehensive experiments on Yale-
B, SVHN, CIFAR-10, CIFAR-100 and ImageNet datasets
over multi-layer perceptron and convolutional neural net-
work architectures. The results show that centered weight
normalization draws its strength in improving the perfor-
mance of deep neural networks. Our code is available at
https://github.com/huangleiBuaa/CenteredWN.

2. Related work

Training deep neural network via ﬁrst order stochastic
gradient descent is difﬁcult in practice, mainly due to the
pathological curvature problem. Several works have tried
the preconditioning techniques to accelerate the training.
Martens and Sutskever [20] developed a second-order opti-
mization method based on the Hessian-free approach. Oth-
er studies [12, 11] explicitly pre-multiply the cost gradient
by an approximate inverse of the Fisher information ma-
trix, thereby expecting to obtain an approximate natural
gradient. The approximate inverse can be obtained by us-
ing Cholesky factorization [12] or Kronecker-factored ap-
proximation [11]. These methods usually introduce much
overhead. Besides, their optimization procedures are usu-
ally coupled with the preconditioning, and thus can not be
easily applied.

Some works addressed the beneﬁts of centered activa-
tions [36] and gradients [26]. They show that these transfor-
mations make the Fisher information matrix approximate
block diagonal, and improve the optimization performance.
Batch normalization [16] further standardizes the activa-
tion with centering and scaling based on mini-batch, and
includes normalization as a part of the model architecture.
Ba et al. [4] computed the layer normalization statics over
all the hidden units in the same layers, targeting at the sce-
nario that the size of mini-batch is limited. These methods
focus on normalizing the activation of the neurons explicit-
ly while our method works by re-parameterizing the model,
and is expected to achieve better conditioning and stabilize
the activations implicitly.

Re-parameterization is an effective technique to facili-
tate the optimization progress [6, 28]. Guillaume et al. [6]
tried to estimate the expected projection matrix, and implic-
itly whitening the activations. However, the operation of

(cid:3039)
(cid:2032)(cid:4666)(cid:2204)(cid:3036)

(cid:4667)

(cid:3039)
(cid:2204)(cid:3036)

(cid:3039)
(cid:2205)(cid:3036)

. . .

. . .

. . .

(cid:1486)(cid:4666)

(cid:2034)(cid:1838)
(cid:3039)(cid:4667)
(cid:2034)(cid:2205)(cid:3036)

.
 
.
 
.

=

(cid:3039)
(cid:1878)(cid:3036)

(cid:3039)
(cid:2205)(cid:3036)

(cid:2190)

(cid:4666)(cid:3039)(cid:2879)(cid:2869)(cid:4667)

.
 
.
 
.

. . .

. . .

. . .

.
 
.
 
.

.
 
.
 
.

(cid:4666)(cid:3039)(cid:2879)(cid:2869)(cid:4667)

(cid:3039)
(cid:2190)

(cid:3013)
(cid:2190)

(cid:2207)

(cid:2190)

(cid:2206)

Figure 1. An illustrative example of layered neural networks with
re-parameterization (for brevity, we leave out the bias nodes). The
proposed re-parameterization method ﬁnely constructs a transfor-
mation ψ over the proxy parameter v to ensure that the trans-
formed weight w has certain beneﬁcial properties for the training
of neural network. Gradient updating is executed on the proxy pa-
rameter v by back-propagating the gradient information through
the normalization process.

updating the projection matrix is still coupled with the opti-
mization. Salimans and Kingma [28] designed weight nor-
malization [28] as a part of the model architecture. The pro-
posed weight normalization addresses the normalization of
the input weight for each neuron and decouples the length of
those weight vectors from their directions. Our work further
powers weight normalization by centering the input weight,
and we argue that it can ulteriorly improve the conditioning
and speed up the training for deep neural networks.

There exist studies constructing orthogonal matrix in
recurrent neural networks (RNN) [2, 37, 8] by using re-
parameterization to avoid the gradient vanish and explosion
problem. However, these methods are limited for the hid-
den to hidden transformation in RNN, because they require
the weight matrix to be square matrix. Other alternative
methods [5, 38] focus on reducing the storage and compu-
tation costs by re-parameterization. Different from them,
our work aims to design a general re-parameterized linear
module to accelerate training and improve the performance
of deep neural networks, and make it an alternative for the
standard linear module.

3. Centered weight normalization

We follow the matrix notation that the vector is in col-
umn form, except that the derivative is a row vector. Given
training data D = {(xi, yi), i = 1, 2, ..., M } where x de-
notes the input and y the target. A neural network is a func-
tion f (x; θ) parameterized by θ that is expected to ﬁt well
the training data and have good generalization for the test
data. The function f (x; θ) adopted by neural network usu-
ally consists of stacked layers. The transformation between
layers consists of a linear mapping zl = (Wl)T hl−1 + bl
with learnable parameters Wl ∈ Rd×n and bl ∈ Rn, and
followed by an element-wise nonlinearity: hl = ϕ(sl),
where l ∈ {1, 2, ..., L} indexes the layer and L denotes the

2804

total number of layers. By convention, h0 corresponds to
the input x and hL corresponds to the output of the net-
work f (x; θ). For clariﬁcation, we refer to z and h as pre-
activation and activation respectively. Under the denotation,
the learnable parameters are θ = {Wl, bl|l = 1, 2, . . . , L}.
Training the neural networks can be viewed as tuning the
parameters to minimize the discrepancy between the desired
output y and the predicted output f (x; θ). This discrepancy
is usually described by a loss function L(y, f (x; θ)), and
thus the objective is to optimize the parameters θ by mini-
mizing the loss as follows:

θ∗ = arg min

E

(x,y)∈D[L(y, f (x; θ))].

(1)

θ

Stochastic gradient descent (SGD) has proved to be an
effective way to train deep networks, in which the gradien-
t of the loss function with respect to the parameters ∂L
∂θ is
approximated by the mini-batch x1...m of size m at each
iteration, by computing ∂L
∂θ = 1

∂L(yi,f (xi;θ))
∂θ

m Σm
i=1

.

3.1. Methodology

Despite the fact that SGD can guarantee a local optimum,
it is also well known the practical success of SGD is highly
dependent on the curvature of the objective to be optimized.
Deep neural network usually exhibits pathological curva-
ture problem, which makes the learning difﬁcult. An alter-
native track to relieve the pathological curvature issue and
thus facilitate the optimization progress is the transforma-
tion of parameterization space of a network model, which
is called re-parameterization.

In the literatures, there exist practical tricks for weight
initialization where weights are sampled from a distribu-
tion with zero-mean and a standard deviation [19, 9, 13].
These initialization techniques effectively can avoid expo-
nentially reducing/magnifying the magnitudes of input sig-
nals or back-propagation signals in the initial phases, by
constructing stable and proper variances of the activations
among different layers. Motivated by this observation, we
propose to constrain the input weight of each neuron with
zero mean and unit norm during the course of training by
re-parameterization, which enjoys stable and fast training
of deep neural networks.

For simplicity, we start by considering one certain neu-
ron i in layer l, whose pre-activation zl
i)T h(l−1)+bl
i.
We denote wl
i as the input weight of the neuron, as shown
in Figure 1. We have left out the superscript l and subscript
i for brevity in the following discusses.

i = (wl

Standardize weight We ﬁrst re-parameterize the input
weight w of each neuron and make sure that it has the fol-
lowing properties: (1) zero-mean, i.e. wT 1 = 0 where 1 is
a column vector of all ones; (2) unit-norm, i.e. (cid:3)w(cid:3) = 1
where (cid:3)w(cid:3) denotes the Euclidean norm of w. To achieve

Algorithm 1 Forward pass of linear mapping with centered
weight normalization.

1: Input:

the mini-batch input data X ∈ Rd×m and
parameters to be learned: g ∈ Rn×1, b ∈ Rn×1,
V ∈ Rd×n.

2: Output: pre-activation Z ∈ Rn×m.
3: compute centered weight: ˆV = V − 1
4: for i = 1 to n do
5:

d 1d(1T

d V).

calculate normalized weight with respect to the i-th
neuron: wi = ˆvi
(cid:4)ˆvi(cid:4)

6: end for
7: calculate: ˆZ = WT X.
8: calculate pre-activation: Z = (g1T

m) ⊙ ˆZ + b1T
m.

the goal, we express the input weight w in terms of the prox-
y parameter v (Figure 1) using

w =

v − 1
(cid:3)v − 1

d 1(1T v)
d 1(1T v)(cid:3)

(2)

where d is the dimension of the input weight, and the s-
tochastic gradient descent or other alternative techniques
such as Adam [17] can be directly applied to the optimiza-
tion with respect to the proxy parameter v. We refer to the
proposed re-parametrization as centered weight normaliza-
tion, that is, we center and scale the proxy parameter v to
ensure that the input weight w has the desired zero-mean
and unit-norm properties as discussed before.

In our centered weight normalization, the parameter up-
dating is completed based on the proxy parameters, and
thus the gradient signal should back-propagate through the
normalization process. Speciﬁcally, given the derivative of
∂L/∂w, we can get the derivative of loss with respect to the
proxy parameter v as follows:

∂L
∂v

=

1
(cid:3)ˆv(cid:3)

[

∂L
∂w

− (

∂L
∂w

w)wT −

1)1T ]

(3)

1
d

(

∂L
∂w

where ˆv = v − 1
ter.

d 1(1T v) is the centered auxiliary parame-

Adjustable weight scale Our method can be viewed ef-
fectively as a solution to the constrained optimization prob-
lem over neural networks:

θ∗ = arg minθ E

(x,y)∈D[L(y, f (x; θ))]

s.t. wT 1 = 0 and (cid:3)w(cid:3) = 1

(4)

where w indicates the input weight for each neuron in each
layer. Therefore, we regularize the network with 2n con-
straints in each layer where n is the number of ﬁlters in cer-
tain layer and the optimization is over the embedded sub-
manifold of the original weight space. While these con-
straints provide regularization, they also may reduce the

2805

Algorithm 2 Back-propagation pass of linear mapping with
centered weight normalization.
1: Input: pre-activation derivative { ∂L

∂Z ∈ Rn×m}. Oth-
er auxiliary variables from respective forward pass: ˆV,
W, ˆZ, X, g.

2: Output: the gradients with respect to the inputs { ∂L
∂g ∈ R1×n, ∂L

∂X ∈
∂b ∈

5:

4:

3:

∂Z ⊙ ˆZ)T

Rd×m} and learnable parameters: ∂L
∂V ∈ Rd×n.
R1×n, ∂L
m( ∂L
∂L
∂g = 1T
T
∂L
∂L
∂b = 1T
m
∂Z
= ∂L
∂L
∂Z ⊙ (g1T
m)
∂ ˆZ
∂X = W ∂L
∂L
∂ ˆZ
∂L
∂W = X ∂L
7:
∂ ˆZ
8: for i = 1 to n do
∂L
∂vi = 1
9:
10: end for

∂wi − ( ∂L

∂wi wi)wT

(cid:4)ˆvi(cid:4) ( ∂L

6:

T

i − 1

d ( ∂L

∂wi 1d)1T
d )

representation capacity of the networks. To address it, we
simply introduce a learnable scalar parameter g to ﬁne tune
the norm of w. Similar idea has been introduced in [28],
and proved useful in practice. We initialize it to 1, and ex-
pect the network learning process can ﬁnd a proper scalar
under the supervision signals. To summarize, we rewrite
the pre-activation z of each neuron in the following way

z = g(

v − 1
(cid:3)v − 1

d 1(1T v)
d 1(1T v)(cid:3)

)T h + b.

(5)

Wrapped module with re-parameterization We can
wrap our proposed method as a common module and plug-
in it in the neural networks as a substitute for the ordinary
linear module. To achieve this goal, the key is to implemen-
t the forward and back-propagation passes. Based on Eqn.
(5), it is readily to extend for multiple output neurons of size
n. We describe the details of the forward pass in Algorithm
1, and the back-propagation pass in Algorithm 2. In the al-
gorithms, the ‘⊙’ operator represents element-wise matrix
multiplication, 1d indicates a column vector of all ones of
size d, and X ∈ Rd×m is the mini-batch input data feeded
into this module, where d is the dimension of the input and
m is the size of the mini-batch. vi is the i-th column of V,
and W = (w1, w2, ..., wn).

layer For the convolutional

Convolutional
the
weight of each feature map is wc ∈ Rd×Fh×Fw where Fh
and Fw indicate the height and width of the ﬁlter, and d is
the number of the input feature maps. We unroll wc as a
FhFwd dimension vector w, then the same normalization
can be directly executed over the unrolled w.

layer,

3.2. Analysis and discussions

Next, we will show our centered weight normalization
enjoys certain appealing properties. It can stabilize the dis-
tribution of pre-activation z with respect to each neuron. To
illustrate this point, we introduce the following proposition
where we omit the bias term b to simplify the discussion.

Proposition 1. Let z = wT h, where wT 1 = 0 and (cid:3)w(cid:3) =
1. Assume h has Gaussian distribution with the mean:
Eh[h] = μ1, and covariance matrix: cov(h) = σ2I, where
μ ∈ R and σ2 ∈ R. We have Ez[z] = 0, var(z) = σ2.

The proof of this proposition is provided in the supple-
mentary materials. Such a proposition tells us that for each
neuron the pre-activation z has zero-mean and the same
variance as the activations fed in, when the assumption is
satisﬁed. This property does not strictly hold in practice due
to the issue of nonlinearities in the hidden layers, however,
we still empirically ﬁnd that our proposed centered weight
normalization approximately holds.

Note that our method is similar to batch normalization
[16]. However, batch normalization focuses on normalizing
the pre-activation compulsively such that the pre-activation
of current mini-batch data is zero-mean and unit-variance,
which is a data dependent normalization. Our method nor-
malizes the weights and is expected to have the effect of
zero-mean and stable variance implicitly, which is a data
independent normalization. Actually, our method can work
well by combining batch normalization as described in sub-
sequent experiments.

Besides the good property guaranteed by Proposition
1, another remarkable observation is that the proposed re-
parameterization method can make the optimization prob-
lem easier, which is supported by the following proposition.

Proposition 2. Regarding to the proxy parameter v, cen-
tered weight normalization makes that the gradient ∂L
∂v has
following properties: (1) zero-mean, i.e. ∂L
∂v · 1 = 0; (2)
orthogonal to the parameters w, i.e. ∂L

∂v · w = 0.

The derivative of Proposition 2 is also given in the sup-
plementary materials. The zero-mean property of ∂L/∂v
usually has an effect that the leading eigenvalue of the Hes-
sian is smaller, and therefore the optimization problem is
likely to become better-conditioned [30]. This promises the
network to learn at a higher rate, and hence converge much
faster. Meanwhile, the gradient is orthogonal to the param-
eters w, and therefore is orthogonal to ˆv. Following the
analysis in [28], the gradient ∂L/∂v can self-stabilize it-
s norm, which thus makes optimization of neural networks
robust to the value of the learning rate.

Computation complexity Given mini-batch input data
{X ∈ Rd×m}, regarding n neurons, both the forward pass
(Algorithm 1) and back-propagation pass (Algorithm 2) of

2806

0.5

0

n
a
e
m

−0.5

plain−L1
plain−L2
CWN−L1
CWN−L2

plain−L1
plain−L2
CWN−L1
CWN−L2

4

3

2

1

e
c
n
a
i
r
a
v

1080

1060

1040

1020

M
I
F
 
f
o
 
r
e
b
m
u
n
 
n
o
i
t
i
d
n
o
c

plain
NNN
WN
CWN

4

3

2

1

s
s
o
l
 
g
n
i
n
i
a
r
t

plain
NNN
WN
CWN

−1
0

200
400
updates (x10)

600

0
0

200
400
updates (x10)

600

100

0

20
40
updates (x100)

60

0
0

2000

4000

6000

updates

(a)

(b)

(c)

(d)

Figure 2. A case study on Yale-B dataset (‘-Ll’ indicates the l-th layer). With respect to the training updates, we analyze (a) the mean
over the mini-batch data and all neurons; (b) the variance calculated over the mini-batch data and all neurons; (c) the condition number
(log-scale) of relative FIM in the second last layer; and (d) the training loss.

the centered weight normalization have the computational
complexity of O(mnd + nd). This means that our centered
weight normalization has the same computational complex-
ity as the standard linear module, since the extra cost O(nd)
of centered weight normalization is negligible to O(mnd).
For a convolution layer with ﬁlters W ∈ Rn×d×Fh×Fw ,
given m mini-batch data {xi ∈ Rd×h×w, i = 1, 2, ..., m},
where h and w represent the height and width of the feature
map, the computational complexity of the standard convo-
lution layer is O(nmdhwFhFw), and while our proposed
method is O(mndhwFhFw + ndFhFw). The extra cost
O(ndFhFw) of the proposed normalization is also negligi-
ble compared to the convolution operation.

4. Experiments

In this section, we begin with a set of ablation study to
support the preliminary analysis of the proposed method.
Then in the following three subsections, we show the ef-
fectiveness of our proposed method on the general network
architectures, including (1) Multi-Layer Perceptron (MLP)
architecture for face and digit recognition tasks; and (2)
Convolutional Neural Network (CNN) models for large s-
cale image classiﬁcation.

Experimental protocols For all experiments, we adop-
t ReLUs [22] as the nonlinear activation and the negative
log-likelihood as the loss function, where (x, y) represents
the (input image, target class) pair. We choose the random
weight initialization by default as described in [19] if we
do not specify weight initialization methods. For the exper-
iments on MLP architecture, the input images are resized
and transformed to 1,024 dimensional vectors in gray scale,
with mean subtracted and variance divided.

4.1. Case study

As discussed in Section 3.2,

the proposed Centered
Weight Normalization (CWN) method can stabilize the
distribution of the pre-activations during training under
certain assumptions. A network with the proposed re-
parameterization technique is likely to become better-
conditioned. In this part, we conduct experiments to vali-
date the conclusions empirically.

We conduct the experiments on Yale-B dataset1 for face
recognition task. Yale-B dataset has 2,414 images with 38
classes. We randomly sample 380 images (i.e., 10 images
per class) as the test set and the others as training set, where
all images are resized as 32 × 32 pixels. We train a 5-layer
MLP with {128, 64, 48, 48} neurons in the hidden layers.
We compare our CWN with the plain network without re-
parameterization (named ‘plain’ for short). In the training,
we use stochastic gradient descent with batch size of 32.
For each method, the best results by tuning different learn-
ing rates in {0.1, 0.2, 0.5, 1} are reported.

Stabilize activation We investigate the mean and vari-
ance of pre-activations in each layer during the course of
training. Figure 2 (a) and (b) respectively show the mean
and variance from the ﬁrst and second layers, with respect
to different rounds of training updates. The mean and vari-
ance are calculated over the mini-batch data and all neuron-
s2 in the corresponding layers. We ﬁnd that the distribution
of pre-activation in plain network varies remarkably. For in-
stance, the mean decreases and the variance increases grad-
ually, even though the network nearly converges to the mini-
mal loss. The unstable distribution of pre-activation may re-
sult in an unstable learning, as shown in Figure 2 (d), where
the loss of plain network ﬂuctuates sharply. By comparing
the loss curve of CWN and the others, we ﬁnd that the pre-
activation of the network with CWN re-parameterization
has a stable mean and variance as shown in Figure 2 (a) and
(b). Especially, in the ﬁrst layer the pre-activation is nearly
zero-mean, which empirically supports the fact in Proposi-
tion 1. These beneﬁcial properties make the network with
CWN re-parameterization converge to an optimum regions
in a stable and fast way.

Information Matrix (FIM)

analysis The

Conditioning
the Fisher
dicator
to
lem is easy to solve.
Ex∼π{Ey∼P (y|x,θ)[( ∂ log P (y|x,θ)

show whether

∂θ

condition

of
number
is a good in-
optimization
prob-
FIM is deﬁned as Fθ =

the

)( ∂ log P (y|x,θ)
∂θ

)T ]},

1http://vision.ucsd.edu/ leekc/ExtYaleDatabase/ExtYaleB.html
2We also observe the mean and variance of the randomly selected neu-

rons in each layer, which also have the similar behaviours.

2807

50

100
epochs

150

200

50

100
epochs

150

200

(a) training error

(b) test error

Figure 3. Performances evaluation on MLP architecture over
permutation-invariant SVHN.

0.8

0.6

0.4

0.2

r
o
r
r
e
 
g
n
i
n
i
a
r
t

0
0

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
NNN
WN
CWN

BN
NNN+BN
WN+BN
CWN+BN
CWN

r
o
r
r
e
 
t
s
e
t

0.7

0.6

0.5

0.4

0.3

0.2

0.1
0

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
NNN
WN
CWN

plain
NNN
WN
CWN

50

100
epochs

150

200

50

100
epochs

150

200

(a) combining batch normalization

(b) using Adam optimization

Figure 4. Comparison of different methods by (a) combining batch
normalization and (b) using Adam optimization on permutation-
invariant SVHN. We evaluate training error (solid lines) and test
error (lines marked with plus) with respect to the training epochs.

and its condition number is cond(Fθ) = |λ(Fθ)|max
|λ(Fθ)|min
where |λ(Fθ)|max and |λ(Fθ)|min are the maximum and
minimum of the absolute values of Fθ’ eigenvalues. We
evaluated the condition number of relative FIM [32] with
respect to the last two layers, regarding the feasibility in
computation. We compared our methods with ‘plain’ and
the other two re-parameterization methods: Natural Neural
Network (NNN) [6] and Weight Normalization (WN) [28].
The results of the second last layer are shown in Figure 2
(c) and the last layer also has similar results, from which we
can ﬁnd that CWN, WN, NNN achieve better conditioning
compared to ‘plain’, and thus converges faster as shown in
Figure 2 (d). This indicates that re-parameterization serves
as an effective way to make the optimization problem easier
if good properties are designed carefully. Compared to WN,
our proposed CWN method further signiﬁcantly improves
the conditioning and speed up convergence. This observa-
tion is consistent with our analysis in Section 3.2 that the
proposed zero-mean property of input weight contributes
to making the optimization problem easier. At the testing
stage, the proposed CWN method achieves the lowest test
error as shown in Table 3, which means that CWN not only
accelerates and stabilizes training, but also has great poten-
tial to improve generalization of the neural network.

4.2. MLP architecture

In this part, we comprehensively evaluate our proposed
method on MLP architecture. We investigate both the train-
ing and test performances on SVHN [23]. SVHN consist-
s of 32 × 32 color images of house numbers collected by
Google Street View. It has 73,257 train images and 26,032
test images. We train a 6 layers MLP with 128 neurons cor-

Table 1. Comparison of test errors (%) averaged over 5 indepen-
dent runs on Yale-B and permutation-invariant SVHN.

plain
Yale-B
6.16
SVHN 18.98

NNN WN
4.68
6.58
17.12
17.99

CWN
4.20
16.16

respondingly in each hidden layer. Note that here we mainly
focus on validating the effectiveness of our proposed meth-
ods on MLP architecture, and under this situation, SVHN
dataset is permutation invariant.

We employ the stochastic gradient descent algorith-
m with mini-batch of size 1024.
Hyper-parameters
are selected by grid search, based on the test er-
ror on validation set (5% samples of the training set)
learning rates in
with the following grid speciﬁcations:
{0.1, 0.2, 0.5, 1}; the natural re-parameterization interval T
in {20, 50, 100, 200, 500} and the revised term ε within the
range of {0.01, 0.01, 0.1, 1} for NNN method.

Figure 3 shows the training and test error with respect
to the number of epochs. We can ﬁnd that WN, NNN and
CWN converge faster compared to ‘plain’, which indicates
that re-parameterization serves as an effective way to accel-
erate the training. Among all re-parametrization methods,
CWN converges fastest. Besides, we observe that both the
training and test error of CWN do not ﬂuctuate after a few
number of epochs in the training, which means that CWN is
much more stable when it reaches the optimal regions. We
can conjecture that the following functions of CWN as dis-
cussed in Section 3.2 contributes to this phenomenon: (1)
CWN can stabilize the distribution of pre-activation; (2) the
gradient ∂L/∂v of CWN can self-stabilize its norm. Table
3 presents the test error for different methods, where we fur-
ther ﬁnd that CWN achieves the best performances. These
observations together show the great potential of CWN in
improving the generalization of the neural network.

Combining batch normalization Batch normalization
[16] has shown to be very helpful for speeding up the train-
ing of deep networks. Combined with batch normalization,
previous re-parameterization methods [6] have shown suc-
cess in improving the performance. Here we show that over
the networks equipped with batch normalization, our pro-
posed CWN still outperforms others in all cases.
In this
experiment, Batch Normalization (BN) is plugged in be-
fore the nonlinearity in neural networks as suggested in
[16]. With batch normalization, we build different net-
works using the re-parameterized linear mapping including
WN, NNN and CWN, named ‘WN+BN’, ‘NNN+BN’ and
‘CWN+BN’ respectively.

Figure 4 (a) shows their experimental results on SVHN
dataset. We ﬁnd that ‘WN+BN’ and ‘NNN+BN’ have no
advantages compared to ‘BN’, while ‘CWN+BN’ signiﬁ-
cantly speeds up the training and achieves better test per-

2808

Table 2. Comparison of test errors (%) averaged over 5 inde-
pendent runs on Yale-B and permutation-invariant SVHN with
Xavier-Init and He-Init.

Table 3. Comparison of test errors (%) averaged over 3 inde-
pendent runs on 56 layers residual network over CIFAR-10 and
CIFAR-100 datasets.

Xavier-Init

He-Init

method Yale-B SVHN Yale-B SVHN
18.35
17.78
plain
18.14
17.83
NNN
WN
18.06
17.74
16.71
16.38
CWN

5.47
5.58
5.58
4.21

6.47
5.47
4.68
3.84

formance. Indeed, batch normalization is invariant to the
weights scaling as analyzed in [24] and [4]. However, batch
normalization is not re-centering invariant [4]. Therefore,
our CWN can further improve the performance of batch nor-
malization by centering the weights.

Amazingly, CWN itself achieves remarkably better per-
formance in terms of both the training speed and the
test error than BN, and even better than ‘CWN+BN’.
This is mainly due to the following reasons. Batch nor-
malization can well stabilize the distribution of the pre-
activation/activation, however it also introduces noises
stemming from the forceful transformation based on mini-
batch. Besides, during test the means and variances are es-
timated based on the moving averages of mini-batch means
and variances [16, 3, 4]. These ﬂaws may have a detrimental
effect on models. Our CWN can stabilize the distribution of
pre-activation implicitly and is deterministic without intro-
ducing stochastic noise. Moreover, the properties described
in Proposition 2 can improve the conditioning, self-stabilize
the gradient’s norm, and thus achieve better performance.

Adam optimization We consider an alternative optimiza-
tion method, Adam [17], to evaluate the performance. The
initial learning rate is set to {0.001, 0.002, 0.005, 0.01}, and
the best results for all methods on SVHN dataset are shown
in Figure 4 (b). Again, we ﬁnd that CWN can reach the
lowest errors at both training and test stages.

Different initialization We also have tried different ini-
tialization methods: Xavier-Init [9] and He-Init [13]. The
experimental setup is the same as in previous experiments.
The ﬁnal test errors both on Yale-B and SVHN datasets are
shown in Table 2. We get similar conclusions as the ran-
dom initialization [19] in previous experiments that CWN
achieves the best test performance.

4.3. CNN architectures on CIFAR dataset

In this part, we highlight that the proposed centered
weight normalization method also works well on several
popular state-of-the-art CNN architectures3, including VG-
G [31], GoogLeNet [34], and residual network [14, 15]. We

3The details of the used CNN architectures are shown in the supple-

mentary materials.

Methods
plain
WN
CWN

CIFAR-10
7.34 ± 0.52
7.58 ± 0.40
6.85 ± 0.26

CIFAR-100
29.38 ± 0.14
29.85 ± 0.66
29.23 ± 0.14

evaluate it over CIFAR-10 and CIFAR-100 datasets [18],
which consists of 50,000 training images and 10,000 test
images from 10 and 100 classes respectively. Each input
image consists of 32 × 32 pixels.

VGG-A We ﬁrst evaluate the proposed methods on the
VGG-A architecture [31] where we set the number of neu-
rons to 512 in the fully connected layer and use batch nor-
malization in the ﬁrst fully connected layer. The exper-
iments run on CIFAR-10 dataset and we preprocess the
dataset by subtracting the mean and dividing the variance.

We employ the stochastic gradient descent as the op-
timization method with mini-batch size of 256, and use
momentum of 0.9, weight decay of 0.0005. The learn-
ing rate decays with each K iterations halving the learn-
ing rate. We initialize the learning rate lr = {0.5, 1, 2, 4},
and K = {1000, 2000, 4000, 8000}. The hyper-parameters
are chosen over the validation set of 5,000 examples from
the training set by grid search. We report the results of
‘plain’, WN and CWN in Figure 5 (a). It is easy to see that
with the proposed CWN the training converges much faster
and promises the lowest test error of 13.06%, compared
to WN of 15.89% and ‘plain’ of 14.33%. Note that here
WN obtains even worse performance compared to ‘plain’,
which is mainly because that WN can not ensure the pre-
activations nearly zero-mean, and thus degenerate the per-
formance when the network is deep.

GoogleLeNet We conduct the experiments on Google-
LeNet. Here GoogleLeNet is equipped with the batch nor-
malization [16], plugged after the linear mappings. The
dataset is preprocessed as described in [10] with global con-
trast normalization and ZCA whitening. We follow the sim-
ple data augmentation that 4 pixels are padded on each side,
and a 32 × 32 crop is randomly sampled from the padded
image or its horizontal ﬂip. The models are trained with
a mini-batch size of 64, momentum of 0.9 and weight de-
cay of 0.0005. The learning rate starts from 0.1 and ends
at 0.001, and decays every two epochs with exponentially
decaying until the end of the training with 100 epochs. The
results on CIFAR-100 are reported in Figure 5 (b), from
which we can ﬁnd that CWN obtains slight speedup com-
pared to WN and ‘plain’. Moreover, CWN can attain the
lowest test error of 24.45%, compare to ‘plain’ of 25.52%
and WN of 25.39%. We also obtain similar observations on
CIFAR-10 (see the supplementary materials).

2809

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
WN
CWN

0.8

0.6

0.4

0.2

r
o
r
r
e
 
t
s
e
t

0.8

0.6

0.4

0.2

r
o
r
r
e

0
0

plain
WN
CWN

0.8

0.6

0.4

0.2

r
o
r
r
e

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
WN
CWN

plain
WN
CWN

plain
WN
CWN

20

40

epochs

(a)

epochs

(b)

60

0
0

20

40

60

80

100

50

100

epochs

150

50

100

epochs

150

(c)

(d)

Figure 5. Performance comparison on various architecture over CIFAR datasets. We evaluate the training error (solid lines) and test error
(lines marked with plus) with respect to the training epochs. (a) VGG-A architecture over CIFAR-10; (b) GoogLeNet architecture over
CIFAR-100; (c) residual network architecture over CIFAR-10; (d) residual network architecture over CIFAR-100.

Residual network We also apply our CWN module to the
residual network [14] with 56 layers. We follow the same
experimental protocol as described in [14] and the publicly
available Torch implementation for residual network 4. Fig-
ure 5 (c) and (d) show the training error and test error with
respect to epochs on CIFAR-10 and CIFAR-100 dataset re-
spectively, and the test errors after training are shown in Ta-
ble 3. We can ﬁnd that CWN converges slightly faster than
WN and ‘plain’ and achieves the best test performance.

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
i
a
r
t

Computational cost We implement our CWN module
based on Torch, and the re-parameterization is wrapped in
the fastest SpatialConvolution of cudnn package 5. We com-
pared the wall clock time of CWN module with the standard
convolution of cudnn. We use 3 × 3 convolution, 32 × 32
feature map with size 128 and mini-batch size of 64. The re-
sults are averaged over 100 runs. CWN costs 18.1ms while
the standard one takes 17.3ms.

4.4. Large scale classiﬁcation on ImageNet dataset

To show the scalability of our proposed method, we ap-
ply it to the large-scale ImageNet-2012 dataset [27] using
the GoogLeNet architecture. ImageNet-2012 includes im-
ages of 1,000 classes, and is split into training sets with
1.2M images, validation set with 50k images, and test set
with 100k images. We employ the validation set as the test
set, and evaluate the classiﬁcation performance based on
top-1 and top-5 error. We use single scale and single crop
test for simplifying discussion. Here we again use stochas-
tic gradient descent with mini-batch size of 64, momentum
of 0.9 and weight decay of 0.0001. The learning rate starts
from 0.1 and ends at 0.0001, and decays with exponentially
decaying until the end of the training with 90 epochs. The
top-5 training and test error curves are shown in Figure 6
and the ﬁnal test errors are shown in Table 4, where we can
ﬁnd that the deep network with CWN can converge to a low-
er training error faster, and obtain lower test error. We argue
that our CWN module draws its strength in improving the
performance of deep neural networks.

4https://github.com/facebook/fb.resnet.torch
5https://developer.nvidia.com/cudnn

plain
WN
CWN

0
0

20

40
epochs

60

80

0
0

20

40
epochs

60

80

(a) training error

(b) test error

Figure 6. Top-5 training and test error curves on GoogLeNet ar-
chitecture over ImageNet-2012 dataset.

Table 4. Comparison of test errors (%) on GoogLeNet over
ImageNet-2012 dataset.

Methods
plain
WN
CWN

Top-1 error
30.78
28.64
26.1

Top-5 error
11.14
9.7
8.35

5. Conclusions

re-
In this paper we introduced a new efﬁcient
parameterization method named centered weight normal-
ization, which improves the conditioning and accelerates
the convergence of the deep neural networks training. We
validate its effectiveness in image classiﬁcation tasks over
both multi-layer perceptron and convolutional neural net-
work architectures. The re-parameterization method is able
to stabilize the distribution and accelerate the training for
a number of popular networks. More importantly, it has
very low computational overhead and can be wrapped as a
linear module suitable for different types of networks. We
argue that the centered weight normalization module owns
the potential to replace the standard linear module, and con-
tributes to new deep architectures with easier optimization
and better generalization.

Acknowledgement

This work was partially supported by NSFC-61370125,
NSFC-61402026, SKLSDE-2017ZX-03, FL-170100117,
DP-140102164, LP-150100671 and the Innovation Foun-
dation of BUAA for PhD Graduates.

2810

References

[1] S.-I. Amari. Natural gradient works efﬁciently in learning.

Neural Comput., 10(2):251–276, Feb. 1998. 1

[2] M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution

recurrent neural networks. In ICML, 2016. 1, 2

[3] D. Arpit, Y. Zhou, B. U. Kota, and V. Govindaraju. Normal-
ization propagation: A parametric technique for removing
internal covariate shift in deep networks. In ICML, 2016. 7
[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.

CoRR, abs/1607.06450, 2016. 1, 2, 7

[5] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. D. Fre-
In Advances
itas. Predicting parameters in deep learning.
in Neural Information Processing Systems 26, pages 2148–
2156, 2013. 2

[6] G. Desjardins, K.

Simonyan,

R.

Pascanu,

K. Kavukcuoglu. Natural neural networks.
2015. 1, 2, 6

and
In NIPS,

[7] Z. Ding, M. Shao, and Y. Fu. Deep robust encoder through
In ECCV, pages

locality preserving low-rank dictionary.
567–582, 2016. 1

[8] V. Dorobantu, P. A. Stromhaug, and J. Renteria. Dizzyrn-
n: Reparameterizing recurrent neural networks for norm-
preserving backpropagation. CoRR, abs/1612.04035, 2016.
2

[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
2010. 1, 3, 7

[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. C.
Courville, and Y. Bengio. Maxout networks. In ICML, 2013.
7

[11] R. B. Grosse and J. Martens. A kronecker-factored approxi-
mate ﬁsher matrix for convolution layers. In ICML, 2016. 1,
2

[12] R. B. Grosse and R. Salakhutdinov. Scaling up natural gra-
dient by sparsely factorizing the inverse ﬁsher matrix.
In
ICML, 2015. 1, 2

[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 1, 3, 7

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 7, 8

[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. CoRR, abs/1603.05027, 2016. 7
[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 1, 2, 4, 6, 7

[17] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 2, 3, 7

[18] A. Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, 2009. 7

[19] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁicient
backprop. In Neural Networks: Tricks of the Trade, 1998. 1,
3, 5, 7

[20] J. Martens and I. Sutskever. Training deep and recurrent net-
works with hessian-free optimization. In Neural Networks:
Tricks of the Trade (2nd ed.). 2012. 1, 2

[21] D. Mishkin and J. Matas. All you need is a good init.

In

ICLR, 2016. 1

[22] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 5

[23] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning, 2011. 6

[24] B. Neyshabur, R. Tomioka, R. Salakhutdinov, and N. Sre-
bro. Data-dependent path normalization in neural networks.
CoRR, abs/1511.06747, 2015. 7

[25] G. Qi. Hierarchically gated deep networks for semantic seg-

mentation. In CVPR, pages 2267–2275, 2016. 1

[26] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made
easier by linear transformations in perceptrons. In AISTATS,
2012. 2

[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), 115(3):211–252, 2015. 8

[28] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In NIPS, 2016. 1, 2, 4, 6

[29] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact so-
lutions to the nonlinear dynamics of learning in deep linear
neural networks. CoRR, abs/1312.6120, 2013. 1

[30] N. N. Schraudolph. Accelerated gradient descent by factor-

centering decomposition. Technical report, 1998. 4

[31] K. Simonyan and A. Zisserman. Very deep convolution-
al networks for large-scale image recognition. CoRR, ab-
s/1409.1556, 2014. 7

[32] K. Sun and F. Nielsen. Relative natural gradient for learning
large complex models. CoRR, abs/1606.06069, 2016. 6
[33] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
pages 1988–1996, 2014. 1

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014. 7

[35] Y. Tian, P. Luo, X. Wang, and X. Tang. Deep learning strong
parts for pedestrian detection. In ICCV, pages 1904–1912,
2015. 1

[36] S. Wiesler, A. Richard, R. Schl¨uter, and H. Ney. Mean-
normalized stochastic gradient for large-scale deep learning.
In ICASSP, 2014. 1, 2

[37] S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas.
In NIPS,

Full-capacity unitary recurrent neural networks.
pages 4880–4888. 2016. 2

[38] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, 2015.
2

2811

Centered Weight Normalization
in Accelerating Training of Deep Neural Networks

Lei Huang† Xianglong Liu†∗ Yang Liu† Bo Lang† Dacheng Tao‡
†State Key Laboratory of Software Development Environment, Beihang University, P.R.China
‡UBTECH Sydney AI Centre, School of IT, FEIT, The University of Sydney, Australia
{huanglei, xlliu, blonster, langbo}@nlsde.buaa.edu.cn, dacheng.tao@sydney.edu.au

Abstract

Training deep neural networks is difﬁcult for the patho-
logical curvature problem. Re-parameterization is an effec-
tive way to relieve the problem by learning the curvature
approximately or constraining the solutions of weights with
good properties for optimization. This paper proposes to re-
parameterize the input weight of each neuron in deep neural
networks by normalizing it with zero-mean and unit-norm,
followed by a learnable scalar parameter to adjust the norm
of the weight. This technique effectively stabilizes the dis-
tribution implicitly. Besides, it improves the conditioning
of the optimization problem and thus accelerates the train-
ing of deep neural networks. It can be wrapped as a linear
module in practice and plugged in any architecture to re-
place the standard linear module. We highlight the beneﬁts
of our method on both multi-layer perceptrons and convolu-
tional neural networks, and demonstrate its scalability and
efﬁciency on SVHN, CIFAR-10, CIFAR-100 and ImageNet
datasets.

1. Introduction

Recently, deep neural networks have achieved grand
success across a broad range of applications, e.g.,
im-
age classiﬁcation, speech recognition and object detection
[33, 35, 14, 36, 7, 25]. Neural networks typically are com-
posed of stacked layers, and the transformation between
layers consists of linear mapping with learnable parameters,
in which each neuron computes a weighted sum over its in-
puts and adds a bias term, and followed by an element-wise
nonlinear activation. The stacked structure endows a neu-
ral network learning feature hierarchies with features from
higher levels formed by the composition of lower level fea-
tures. Further, deep architectures provide neural network-
s powerful representation capacity of learning complicated
functions that can represent high-level abstractions.

∗Corresponding author

While the deep and complex structure enjoys appeal-
ing advantages, it also makes learning difﬁcult.
Indeed,
many explanations for the difﬁculty of deep learning have
been explored, such as the problem of vanishing and ex-
ploding gradients [2] , internal covariate shift [16], and
the pathological curvature [20]. To address these prob-
lems, various studies such as ﬁnely weight initialization
[19, 9, 13, 29, 21], normalization of internal activation
[16, 4], and sophistic optimization methods have been pro-
posed accordingly [20, 12, 11].

Our work is dedicated to the problem of pathological
curvature [20], i.e. the condition number of the Hessian ma-
trix of the objective function is low at the optimum region-
s [28], which makes learning extremely difﬁcult via ﬁrst
order stochastic gradient descent. Several studies [12, 11]
recently have attempted to use the pre-conditioning tech-
niques to improve the conditioning of the cost curvature.
However, these methods introduce too much overhead and
are not convenient to be applied.

An alternative track to facilitate the optimization
progress is the transformation of parameterization space of
a model [1], which is called re-parameterization. The mo-
tivation of re-parameterization is that there may be various
equivalent ways to parameterize the same model, some of
which are much easier to optimize than others [28]. There-
fore, exploring good ways of parameterizing neural net-
works [6, 28] are essentially important in training deep neu-
ral networks.

Inspired by the practical trick that weights are sampled
from a distribution with zero mean and a standard deviation
for initialization [19, 9, 13], in this paper we propose to con-
strain the input weight of each neuron with zero mean and
unit norm by re-parameterization during the course of train-
ing, followed by a learnable scalar parameter to adjust the
norm of the input weight. We use proxy parameters and per-
form gradient updating on these proxy parameters by back-
propagating the gradient information through the normal-
ization process (Figure 1). By introducing this process, the
summed input of each neuron is more likely to possess the

12803

properties of zero mean and stable variance. Besides, this
technique can effectively improve the conditioning of the
optimization problem and thus can accelerate the training
of deep neural networks.

We wrap the proposed re-parameterization method into
a linear module in practice, which can be plugged in any
architecture as a substitute for the standard linear module.
The technique we present is generic and can be applied
to a broad range of models. Our method is also orthogo-
nal and complementary to recent advances in deep learn-
ing, such as Adam optimization [17] and batch normaliza-
tion [16]. We conduct comprehensive experiments on Yale-
B, SVHN, CIFAR-10, CIFAR-100 and ImageNet datasets
over multi-layer perceptron and convolutional neural net-
work architectures. The results show that centered weight
normalization draws its strength in improving the perfor-
mance of deep neural networks. Our code is available at
https://github.com/huangleiBuaa/CenteredWN.

2. Related work

Training deep neural network via ﬁrst order stochastic
gradient descent is difﬁcult in practice, mainly due to the
pathological curvature problem. Several works have tried
the preconditioning techniques to accelerate the training.
Martens and Sutskever [20] developed a second-order opti-
mization method based on the Hessian-free approach. Oth-
er studies [12, 11] explicitly pre-multiply the cost gradient
by an approximate inverse of the Fisher information ma-
trix, thereby expecting to obtain an approximate natural
gradient. The approximate inverse can be obtained by us-
ing Cholesky factorization [12] or Kronecker-factored ap-
proximation [11]. These methods usually introduce much
overhead. Besides, their optimization procedures are usu-
ally coupled with the preconditioning, and thus can not be
easily applied.

Some works addressed the beneﬁts of centered activa-
tions [36] and gradients [26]. They show that these transfor-
mations make the Fisher information matrix approximate
block diagonal, and improve the optimization performance.
Batch normalization [16] further standardizes the activa-
tion with centering and scaling based on mini-batch, and
includes normalization as a part of the model architecture.
Ba et al. [4] computed the layer normalization statics over
all the hidden units in the same layers, targeting at the sce-
nario that the size of mini-batch is limited. These methods
focus on normalizing the activation of the neurons explicit-
ly while our method works by re-parameterizing the model,
and is expected to achieve better conditioning and stabilize
the activations implicitly.

Re-parameterization is an effective technique to facili-
tate the optimization progress [6, 28]. Guillaume et al. [6]
tried to estimate the expected projection matrix, and implic-
itly whitening the activations. However, the operation of

(cid:3039)
(cid:2032)(cid:4666)(cid:2204)(cid:3036)

(cid:4667)

(cid:3039)
(cid:2204)(cid:3036)

(cid:3039)
(cid:2205)(cid:3036)

. . .

. . .

. . .

(cid:1486)(cid:4666)

(cid:2034)(cid:1838)
(cid:3039)(cid:4667)
(cid:2034)(cid:2205)(cid:3036)

.
 
.
 
.

=

(cid:3039)
(cid:1878)(cid:3036)

(cid:3039)
(cid:2205)(cid:3036)

(cid:2190)

(cid:4666)(cid:3039)(cid:2879)(cid:2869)(cid:4667)

.
 
.
 
.

. . .

. . .

. . .

.
 
.
 
.

.
 
.
 
.

(cid:4666)(cid:3039)(cid:2879)(cid:2869)(cid:4667)

(cid:3039)
(cid:2190)

(cid:3013)
(cid:2190)

(cid:2207)

(cid:2190)

(cid:2206)

Figure 1. An illustrative example of layered neural networks with
re-parameterization (for brevity, we leave out the bias nodes). The
proposed re-parameterization method ﬁnely constructs a transfor-
mation ψ over the proxy parameter v to ensure that the trans-
formed weight w has certain beneﬁcial properties for the training
of neural network. Gradient updating is executed on the proxy pa-
rameter v by back-propagating the gradient information through
the normalization process.

updating the projection matrix is still coupled with the opti-
mization. Salimans and Kingma [28] designed weight nor-
malization [28] as a part of the model architecture. The pro-
posed weight normalization addresses the normalization of
the input weight for each neuron and decouples the length of
those weight vectors from their directions. Our work further
powers weight normalization by centering the input weight,
and we argue that it can ulteriorly improve the conditioning
and speed up the training for deep neural networks.

There exist studies constructing orthogonal matrix in
recurrent neural networks (RNN) [2, 37, 8] by using re-
parameterization to avoid the gradient vanish and explosion
problem. However, these methods are limited for the hid-
den to hidden transformation in RNN, because they require
the weight matrix to be square matrix. Other alternative
methods [5, 38] focus on reducing the storage and compu-
tation costs by re-parameterization. Different from them,
our work aims to design a general re-parameterized linear
module to accelerate training and improve the performance
of deep neural networks, and make it an alternative for the
standard linear module.

3. Centered weight normalization

We follow the matrix notation that the vector is in col-
umn form, except that the derivative is a row vector. Given
training data D = {(xi, yi), i = 1, 2, ..., M } where x de-
notes the input and y the target. A neural network is a func-
tion f (x; θ) parameterized by θ that is expected to ﬁt well
the training data and have good generalization for the test
data. The function f (x; θ) adopted by neural network usu-
ally consists of stacked layers. The transformation between
layers consists of a linear mapping zl = (Wl)T hl−1 + bl
with learnable parameters Wl ∈ Rd×n and bl ∈ Rn, and
followed by an element-wise nonlinearity: hl = ϕ(sl),
where l ∈ {1, 2, ..., L} indexes the layer and L denotes the

2804

total number of layers. By convention, h0 corresponds to
the input x and hL corresponds to the output of the net-
work f (x; θ). For clariﬁcation, we refer to z and h as pre-
activation and activation respectively. Under the denotation,
the learnable parameters are θ = {Wl, bl|l = 1, 2, . . . , L}.
Training the neural networks can be viewed as tuning the
parameters to minimize the discrepancy between the desired
output y and the predicted output f (x; θ). This discrepancy
is usually described by a loss function L(y, f (x; θ)), and
thus the objective is to optimize the parameters θ by mini-
mizing the loss as follows:

θ∗ = arg min

E

(x,y)∈D[L(y, f (x; θ))].

(1)

θ

Stochastic gradient descent (SGD) has proved to be an
effective way to train deep networks, in which the gradien-
t of the loss function with respect to the parameters ∂L
∂θ is
approximated by the mini-batch x1...m of size m at each
iteration, by computing ∂L
∂θ = 1

∂L(yi,f (xi;θ))
∂θ

m Σm
i=1

.

3.1. Methodology

Despite the fact that SGD can guarantee a local optimum,
it is also well known the practical success of SGD is highly
dependent on the curvature of the objective to be optimized.
Deep neural network usually exhibits pathological curva-
ture problem, which makes the learning difﬁcult. An alter-
native track to relieve the pathological curvature issue and
thus facilitate the optimization progress is the transforma-
tion of parameterization space of a network model, which
is called re-parameterization.

In the literatures, there exist practical tricks for weight
initialization where weights are sampled from a distribu-
tion with zero-mean and a standard deviation [19, 9, 13].
These initialization techniques effectively can avoid expo-
nentially reducing/magnifying the magnitudes of input sig-
nals or back-propagation signals in the initial phases, by
constructing stable and proper variances of the activations
among different layers. Motivated by this observation, we
propose to constrain the input weight of each neuron with
zero mean and unit norm during the course of training by
re-parameterization, which enjoys stable and fast training
of deep neural networks.

For simplicity, we start by considering one certain neu-
ron i in layer l, whose pre-activation zl
i)T h(l−1)+bl
i.
We denote wl
i as the input weight of the neuron, as shown
in Figure 1. We have left out the superscript l and subscript
i for brevity in the following discusses.

i = (wl

Standardize weight We ﬁrst re-parameterize the input
weight w of each neuron and make sure that it has the fol-
lowing properties: (1) zero-mean, i.e. wT 1 = 0 where 1 is
a column vector of all ones; (2) unit-norm, i.e. (cid:3)w(cid:3) = 1
where (cid:3)w(cid:3) denotes the Euclidean norm of w. To achieve

Algorithm 1 Forward pass of linear mapping with centered
weight normalization.

1: Input:

the mini-batch input data X ∈ Rd×m and
parameters to be learned: g ∈ Rn×1, b ∈ Rn×1,
V ∈ Rd×n.

2: Output: pre-activation Z ∈ Rn×m.
3: compute centered weight: ˆV = V − 1
4: for i = 1 to n do
5:

d 1d(1T

d V).

calculate normalized weight with respect to the i-th
neuron: wi = ˆvi
(cid:4)ˆvi(cid:4)

6: end for
7: calculate: ˆZ = WT X.
8: calculate pre-activation: Z = (g1T

m) ⊙ ˆZ + b1T
m.

the goal, we express the input weight w in terms of the prox-
y parameter v (Figure 1) using

w =

v − 1
(cid:3)v − 1

d 1(1T v)
d 1(1T v)(cid:3)

(2)

where d is the dimension of the input weight, and the s-
tochastic gradient descent or other alternative techniques
such as Adam [17] can be directly applied to the optimiza-
tion with respect to the proxy parameter v. We refer to the
proposed re-parametrization as centered weight normaliza-
tion, that is, we center and scale the proxy parameter v to
ensure that the input weight w has the desired zero-mean
and unit-norm properties as discussed before.

In our centered weight normalization, the parameter up-
dating is completed based on the proxy parameters, and
thus the gradient signal should back-propagate through the
normalization process. Speciﬁcally, given the derivative of
∂L/∂w, we can get the derivative of loss with respect to the
proxy parameter v as follows:

∂L
∂v

=

1
(cid:3)ˆv(cid:3)

[

∂L
∂w

− (

∂L
∂w

w)wT −

1)1T ]

(3)

1
d

(

∂L
∂w

where ˆv = v − 1
ter.

d 1(1T v) is the centered auxiliary parame-

Adjustable weight scale Our method can be viewed ef-
fectively as a solution to the constrained optimization prob-
lem over neural networks:

θ∗ = arg minθ E

(x,y)∈D[L(y, f (x; θ))]

s.t. wT 1 = 0 and (cid:3)w(cid:3) = 1

(4)

where w indicates the input weight for each neuron in each
layer. Therefore, we regularize the network with 2n con-
straints in each layer where n is the number of ﬁlters in cer-
tain layer and the optimization is over the embedded sub-
manifold of the original weight space. While these con-
straints provide regularization, they also may reduce the

2805

Algorithm 2 Back-propagation pass of linear mapping with
centered weight normalization.
1: Input: pre-activation derivative { ∂L

∂Z ∈ Rn×m}. Oth-
er auxiliary variables from respective forward pass: ˆV,
W, ˆZ, X, g.

2: Output: the gradients with respect to the inputs { ∂L
∂g ∈ R1×n, ∂L

∂X ∈
∂b ∈

5:

4:

3:

∂Z ⊙ ˆZ)T

Rd×m} and learnable parameters: ∂L
∂V ∈ Rd×n.
R1×n, ∂L
m( ∂L
∂L
∂g = 1T
T
∂L
∂L
∂b = 1T
m
∂Z
= ∂L
∂L
∂Z ⊙ (g1T
m)
∂ ˆZ
∂X = W ∂L
∂L
∂ ˆZ
∂L
∂W = X ∂L
7:
∂ ˆZ
8: for i = 1 to n do
∂L
∂vi = 1
9:
10: end for

∂wi − ( ∂L

∂wi wi)wT

(cid:4)ˆvi(cid:4) ( ∂L

6:

T

i − 1

d ( ∂L

∂wi 1d)1T
d )

representation capacity of the networks. To address it, we
simply introduce a learnable scalar parameter g to ﬁne tune
the norm of w. Similar idea has been introduced in [28],
and proved useful in practice. We initialize it to 1, and ex-
pect the network learning process can ﬁnd a proper scalar
under the supervision signals. To summarize, we rewrite
the pre-activation z of each neuron in the following way

z = g(

v − 1
(cid:3)v − 1

d 1(1T v)
d 1(1T v)(cid:3)

)T h + b.

(5)

Wrapped module with re-parameterization We can
wrap our proposed method as a common module and plug-
in it in the neural networks as a substitute for the ordinary
linear module. To achieve this goal, the key is to implemen-
t the forward and back-propagation passes. Based on Eqn.
(5), it is readily to extend for multiple output neurons of size
n. We describe the details of the forward pass in Algorithm
1, and the back-propagation pass in Algorithm 2. In the al-
gorithms, the ‘⊙’ operator represents element-wise matrix
multiplication, 1d indicates a column vector of all ones of
size d, and X ∈ Rd×m is the mini-batch input data feeded
into this module, where d is the dimension of the input and
m is the size of the mini-batch. vi is the i-th column of V,
and W = (w1, w2, ..., wn).

layer For the convolutional

Convolutional
the
weight of each feature map is wc ∈ Rd×Fh×Fw where Fh
and Fw indicate the height and width of the ﬁlter, and d is
the number of the input feature maps. We unroll wc as a
FhFwd dimension vector w, then the same normalization
can be directly executed over the unrolled w.

layer,

3.2. Analysis and discussions

Next, we will show our centered weight normalization
enjoys certain appealing properties. It can stabilize the dis-
tribution of pre-activation z with respect to each neuron. To
illustrate this point, we introduce the following proposition
where we omit the bias term b to simplify the discussion.

Proposition 1. Let z = wT h, where wT 1 = 0 and (cid:3)w(cid:3) =
1. Assume h has Gaussian distribution with the mean:
Eh[h] = μ1, and covariance matrix: cov(h) = σ2I, where
μ ∈ R and σ2 ∈ R. We have Ez[z] = 0, var(z) = σ2.

The proof of this proposition is provided in the supple-
mentary materials. Such a proposition tells us that for each
neuron the pre-activation z has zero-mean and the same
variance as the activations fed in, when the assumption is
satisﬁed. This property does not strictly hold in practice due
to the issue of nonlinearities in the hidden layers, however,
we still empirically ﬁnd that our proposed centered weight
normalization approximately holds.

Note that our method is similar to batch normalization
[16]. However, batch normalization focuses on normalizing
the pre-activation compulsively such that the pre-activation
of current mini-batch data is zero-mean and unit-variance,
which is a data dependent normalization. Our method nor-
malizes the weights and is expected to have the effect of
zero-mean and stable variance implicitly, which is a data
independent normalization. Actually, our method can work
well by combining batch normalization as described in sub-
sequent experiments.

Besides the good property guaranteed by Proposition
1, another remarkable observation is that the proposed re-
parameterization method can make the optimization prob-
lem easier, which is supported by the following proposition.

Proposition 2. Regarding to the proxy parameter v, cen-
tered weight normalization makes that the gradient ∂L
∂v has
following properties: (1) zero-mean, i.e. ∂L
∂v · 1 = 0; (2)
orthogonal to the parameters w, i.e. ∂L

∂v · w = 0.

The derivative of Proposition 2 is also given in the sup-
plementary materials. The zero-mean property of ∂L/∂v
usually has an effect that the leading eigenvalue of the Hes-
sian is smaller, and therefore the optimization problem is
likely to become better-conditioned [30]. This promises the
network to learn at a higher rate, and hence converge much
faster. Meanwhile, the gradient is orthogonal to the param-
eters w, and therefore is orthogonal to ˆv. Following the
analysis in [28], the gradient ∂L/∂v can self-stabilize it-
s norm, which thus makes optimization of neural networks
robust to the value of the learning rate.

Computation complexity Given mini-batch input data
{X ∈ Rd×m}, regarding n neurons, both the forward pass
(Algorithm 1) and back-propagation pass (Algorithm 2) of

2806

0.5

0

n
a
e
m

−0.5

plain−L1
plain−L2
CWN−L1
CWN−L2

plain−L1
plain−L2
CWN−L1
CWN−L2

4

3

2

1

e
c
n
a
i
r
a
v

1080

1060

1040

1020

M
I
F
 
f
o
 
r
e
b
m
u
n
 
n
o
i
t
i
d
n
o
c

plain
NNN
WN
CWN

4

3

2

1

s
s
o
l
 
g
n
i
n
i
a
r
t

plain
NNN
WN
CWN

−1
0

200
400
updates (x10)

600

0
0

200
400
updates (x10)

600

100

0

20
40
updates (x100)

60

0
0

2000

4000

6000

updates

(a)

(b)

(c)

(d)

Figure 2. A case study on Yale-B dataset (‘-Ll’ indicates the l-th layer). With respect to the training updates, we analyze (a) the mean
over the mini-batch data and all neurons; (b) the variance calculated over the mini-batch data and all neurons; (c) the condition number
(log-scale) of relative FIM in the second last layer; and (d) the training loss.

the centered weight normalization have the computational
complexity of O(mnd + nd). This means that our centered
weight normalization has the same computational complex-
ity as the standard linear module, since the extra cost O(nd)
of centered weight normalization is negligible to O(mnd).
For a convolution layer with ﬁlters W ∈ Rn×d×Fh×Fw ,
given m mini-batch data {xi ∈ Rd×h×w, i = 1, 2, ..., m},
where h and w represent the height and width of the feature
map, the computational complexity of the standard convo-
lution layer is O(nmdhwFhFw), and while our proposed
method is O(mndhwFhFw + ndFhFw). The extra cost
O(ndFhFw) of the proposed normalization is also negligi-
ble compared to the convolution operation.

4. Experiments

In this section, we begin with a set of ablation study to
support the preliminary analysis of the proposed method.
Then in the following three subsections, we show the ef-
fectiveness of our proposed method on the general network
architectures, including (1) Multi-Layer Perceptron (MLP)
architecture for face and digit recognition tasks; and (2)
Convolutional Neural Network (CNN) models for large s-
cale image classiﬁcation.

Experimental protocols For all experiments, we adop-
t ReLUs [22] as the nonlinear activation and the negative
log-likelihood as the loss function, where (x, y) represents
the (input image, target class) pair. We choose the random
weight initialization by default as described in [19] if we
do not specify weight initialization methods. For the exper-
iments on MLP architecture, the input images are resized
and transformed to 1,024 dimensional vectors in gray scale,
with mean subtracted and variance divided.

4.1. Case study

As discussed in Section 3.2,

the proposed Centered
Weight Normalization (CWN) method can stabilize the
distribution of the pre-activations during training under
certain assumptions. A network with the proposed re-
parameterization technique is likely to become better-
conditioned. In this part, we conduct experiments to vali-
date the conclusions empirically.

We conduct the experiments on Yale-B dataset1 for face
recognition task. Yale-B dataset has 2,414 images with 38
classes. We randomly sample 380 images (i.e., 10 images
per class) as the test set and the others as training set, where
all images are resized as 32 × 32 pixels. We train a 5-layer
MLP with {128, 64, 48, 48} neurons in the hidden layers.
We compare our CWN with the plain network without re-
parameterization (named ‘plain’ for short). In the training,
we use stochastic gradient descent with batch size of 32.
For each method, the best results by tuning different learn-
ing rates in {0.1, 0.2, 0.5, 1} are reported.

Stabilize activation We investigate the mean and vari-
ance of pre-activations in each layer during the course of
training. Figure 2 (a) and (b) respectively show the mean
and variance from the ﬁrst and second layers, with respect
to different rounds of training updates. The mean and vari-
ance are calculated over the mini-batch data and all neuron-
s2 in the corresponding layers. We ﬁnd that the distribution
of pre-activation in plain network varies remarkably. For in-
stance, the mean decreases and the variance increases grad-
ually, even though the network nearly converges to the mini-
mal loss. The unstable distribution of pre-activation may re-
sult in an unstable learning, as shown in Figure 2 (d), where
the loss of plain network ﬂuctuates sharply. By comparing
the loss curve of CWN and the others, we ﬁnd that the pre-
activation of the network with CWN re-parameterization
has a stable mean and variance as shown in Figure 2 (a) and
(b). Especially, in the ﬁrst layer the pre-activation is nearly
zero-mean, which empirically supports the fact in Proposi-
tion 1. These beneﬁcial properties make the network with
CWN re-parameterization converge to an optimum regions
in a stable and fast way.

Information Matrix (FIM)

analysis The

Conditioning
the Fisher
dicator
to
lem is easy to solve.
Ex∼π{Ey∼P (y|x,θ)[( ∂ log P (y|x,θ)

show whether

∂θ

condition

of
number
is a good in-
optimization
prob-
FIM is deﬁned as Fθ =

the

)( ∂ log P (y|x,θ)
∂θ

)T ]},

1http://vision.ucsd.edu/ leekc/ExtYaleDatabase/ExtYaleB.html
2We also observe the mean and variance of the randomly selected neu-

rons in each layer, which also have the similar behaviours.

2807

50

100
epochs

150

200

50

100
epochs

150

200

(a) training error

(b) test error

Figure 3. Performances evaluation on MLP architecture over
permutation-invariant SVHN.

0.8

0.6

0.4

0.2

r
o
r
r
e
 
g
n
i
n
i
a
r
t

0
0

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
NNN
WN
CWN

BN
NNN+BN
WN+BN
CWN+BN
CWN

r
o
r
r
e
 
t
s
e
t

0.7

0.6

0.5

0.4

0.3

0.2

0.1
0

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
NNN
WN
CWN

plain
NNN
WN
CWN

50

100
epochs

150

200

50

100
epochs

150

200

(a) combining batch normalization

(b) using Adam optimization

Figure 4. Comparison of different methods by (a) combining batch
normalization and (b) using Adam optimization on permutation-
invariant SVHN. We evaluate training error (solid lines) and test
error (lines marked with plus) with respect to the training epochs.

and its condition number is cond(Fθ) = |λ(Fθ)|max
|λ(Fθ)|min
where |λ(Fθ)|max and |λ(Fθ)|min are the maximum and
minimum of the absolute values of Fθ’ eigenvalues. We
evaluated the condition number of relative FIM [32] with
respect to the last two layers, regarding the feasibility in
computation. We compared our methods with ‘plain’ and
the other two re-parameterization methods: Natural Neural
Network (NNN) [6] and Weight Normalization (WN) [28].
The results of the second last layer are shown in Figure 2
(c) and the last layer also has similar results, from which we
can ﬁnd that CWN, WN, NNN achieve better conditioning
compared to ‘plain’, and thus converges faster as shown in
Figure 2 (d). This indicates that re-parameterization serves
as an effective way to make the optimization problem easier
if good properties are designed carefully. Compared to WN,
our proposed CWN method further signiﬁcantly improves
the conditioning and speed up convergence. This observa-
tion is consistent with our analysis in Section 3.2 that the
proposed zero-mean property of input weight contributes
to making the optimization problem easier. At the testing
stage, the proposed CWN method achieves the lowest test
error as shown in Table 3, which means that CWN not only
accelerates and stabilizes training, but also has great poten-
tial to improve generalization of the neural network.

4.2. MLP architecture

In this part, we comprehensively evaluate our proposed
method on MLP architecture. We investigate both the train-
ing and test performances on SVHN [23]. SVHN consist-
s of 32 × 32 color images of house numbers collected by
Google Street View. It has 73,257 train images and 26,032
test images. We train a 6 layers MLP with 128 neurons cor-

Table 1. Comparison of test errors (%) averaged over 5 indepen-
dent runs on Yale-B and permutation-invariant SVHN.

plain
Yale-B
6.16
SVHN 18.98

NNN WN
4.68
6.58
17.12
17.99

CWN
4.20
16.16

respondingly in each hidden layer. Note that here we mainly
focus on validating the effectiveness of our proposed meth-
ods on MLP architecture, and under this situation, SVHN
dataset is permutation invariant.

We employ the stochastic gradient descent algorith-
m with mini-batch of size 1024.
Hyper-parameters
are selected by grid search, based on the test er-
ror on validation set (5% samples of the training set)
learning rates in
with the following grid speciﬁcations:
{0.1, 0.2, 0.5, 1}; the natural re-parameterization interval T
in {20, 50, 100, 200, 500} and the revised term ε within the
range of {0.01, 0.01, 0.1, 1} for NNN method.

Figure 3 shows the training and test error with respect
to the number of epochs. We can ﬁnd that WN, NNN and
CWN converge faster compared to ‘plain’, which indicates
that re-parameterization serves as an effective way to accel-
erate the training. Among all re-parametrization methods,
CWN converges fastest. Besides, we observe that both the
training and test error of CWN do not ﬂuctuate after a few
number of epochs in the training, which means that CWN is
much more stable when it reaches the optimal regions. We
can conjecture that the following functions of CWN as dis-
cussed in Section 3.2 contributes to this phenomenon: (1)
CWN can stabilize the distribution of pre-activation; (2) the
gradient ∂L/∂v of CWN can self-stabilize its norm. Table
3 presents the test error for different methods, where we fur-
ther ﬁnd that CWN achieves the best performances. These
observations together show the great potential of CWN in
improving the generalization of the neural network.

Combining batch normalization Batch normalization
[16] has shown to be very helpful for speeding up the train-
ing of deep networks. Combined with batch normalization,
previous re-parameterization methods [6] have shown suc-
cess in improving the performance. Here we show that over
the networks equipped with batch normalization, our pro-
posed CWN still outperforms others in all cases.
In this
experiment, Batch Normalization (BN) is plugged in be-
fore the nonlinearity in neural networks as suggested in
[16]. With batch normalization, we build different net-
works using the re-parameterized linear mapping including
WN, NNN and CWN, named ‘WN+BN’, ‘NNN+BN’ and
‘CWN+BN’ respectively.

Figure 4 (a) shows their experimental results on SVHN
dataset. We ﬁnd that ‘WN+BN’ and ‘NNN+BN’ have no
advantages compared to ‘BN’, while ‘CWN+BN’ signiﬁ-
cantly speeds up the training and achieves better test per-

2808

Table 2. Comparison of test errors (%) averaged over 5 inde-
pendent runs on Yale-B and permutation-invariant SVHN with
Xavier-Init and He-Init.

Table 3. Comparison of test errors (%) averaged over 3 inde-
pendent runs on 56 layers residual network over CIFAR-10 and
CIFAR-100 datasets.

Xavier-Init

He-Init

method Yale-B SVHN Yale-B SVHN
18.35
17.78
plain
18.14
17.83
NNN
WN
18.06
17.74
16.71
16.38
CWN

5.47
5.58
5.58
4.21

6.47
5.47
4.68
3.84

formance. Indeed, batch normalization is invariant to the
weights scaling as analyzed in [24] and [4]. However, batch
normalization is not re-centering invariant [4]. Therefore,
our CWN can further improve the performance of batch nor-
malization by centering the weights.

Amazingly, CWN itself achieves remarkably better per-
formance in terms of both the training speed and the
test error than BN, and even better than ‘CWN+BN’.
This is mainly due to the following reasons. Batch nor-
malization can well stabilize the distribution of the pre-
activation/activation, however it also introduces noises
stemming from the forceful transformation based on mini-
batch. Besides, during test the means and variances are es-
timated based on the moving averages of mini-batch means
and variances [16, 3, 4]. These ﬂaws may have a detrimental
effect on models. Our CWN can stabilize the distribution of
pre-activation implicitly and is deterministic without intro-
ducing stochastic noise. Moreover, the properties described
in Proposition 2 can improve the conditioning, self-stabilize
the gradient’s norm, and thus achieve better performance.

Adam optimization We consider an alternative optimiza-
tion method, Adam [17], to evaluate the performance. The
initial learning rate is set to {0.001, 0.002, 0.005, 0.01}, and
the best results for all methods on SVHN dataset are shown
in Figure 4 (b). Again, we ﬁnd that CWN can reach the
lowest errors at both training and test stages.

Different initialization We also have tried different ini-
tialization methods: Xavier-Init [9] and He-Init [13]. The
experimental setup is the same as in previous experiments.
The ﬁnal test errors both on Yale-B and SVHN datasets are
shown in Table 2. We get similar conclusions as the ran-
dom initialization [19] in previous experiments that CWN
achieves the best test performance.

4.3. CNN architectures on CIFAR dataset

In this part, we highlight that the proposed centered
weight normalization method also works well on several
popular state-of-the-art CNN architectures3, including VG-
G [31], GoogLeNet [34], and residual network [14, 15]. We

3The details of the used CNN architectures are shown in the supple-

mentary materials.

Methods
plain
WN
CWN

CIFAR-10
7.34 ± 0.52
7.58 ± 0.40
6.85 ± 0.26

CIFAR-100
29.38 ± 0.14
29.85 ± 0.66
29.23 ± 0.14

evaluate it over CIFAR-10 and CIFAR-100 datasets [18],
which consists of 50,000 training images and 10,000 test
images from 10 and 100 classes respectively. Each input
image consists of 32 × 32 pixels.

VGG-A We ﬁrst evaluate the proposed methods on the
VGG-A architecture [31] where we set the number of neu-
rons to 512 in the fully connected layer and use batch nor-
malization in the ﬁrst fully connected layer. The exper-
iments run on CIFAR-10 dataset and we preprocess the
dataset by subtracting the mean and dividing the variance.

We employ the stochastic gradient descent as the op-
timization method with mini-batch size of 256, and use
momentum of 0.9, weight decay of 0.0005. The learn-
ing rate decays with each K iterations halving the learn-
ing rate. We initialize the learning rate lr = {0.5, 1, 2, 4},
and K = {1000, 2000, 4000, 8000}. The hyper-parameters
are chosen over the validation set of 5,000 examples from
the training set by grid search. We report the results of
‘plain’, WN and CWN in Figure 5 (a). It is easy to see that
with the proposed CWN the training converges much faster
and promises the lowest test error of 13.06%, compared
to WN of 15.89% and ‘plain’ of 14.33%. Note that here
WN obtains even worse performance compared to ‘plain’,
which is mainly because that WN can not ensure the pre-
activations nearly zero-mean, and thus degenerate the per-
formance when the network is deep.

GoogleLeNet We conduct the experiments on Google-
LeNet. Here GoogleLeNet is equipped with the batch nor-
malization [16], plugged after the linear mappings. The
dataset is preprocessed as described in [10] with global con-
trast normalization and ZCA whitening. We follow the sim-
ple data augmentation that 4 pixels are padded on each side,
and a 32 × 32 crop is randomly sampled from the padded
image or its horizontal ﬂip. The models are trained with
a mini-batch size of 64, momentum of 0.9 and weight de-
cay of 0.0005. The learning rate starts from 0.1 and ends
at 0.001, and decays every two epochs with exponentially
decaying until the end of the training with 100 epochs. The
results on CIFAR-100 are reported in Figure 5 (b), from
which we can ﬁnd that CWN obtains slight speedup com-
pared to WN and ‘plain’. Moreover, CWN can attain the
lowest test error of 24.45%, compare to ‘plain’ of 25.52%
and WN of 25.39%. We also obtain similar observations on
CIFAR-10 (see the supplementary materials).

2809

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
WN
CWN

0.8

0.6

0.4

0.2

r
o
r
r
e
 
t
s
e
t

0.8

0.6

0.4

0.2

r
o
r
r
e

0
0

plain
WN
CWN

0.8

0.6

0.4

0.2

r
o
r
r
e

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
WN
CWN

plain
WN
CWN

plain
WN
CWN

20

40

epochs

(a)

epochs

(b)

60

0
0

20

40

60

80

100

50

100

epochs

150

50

100

epochs

150

(c)

(d)

Figure 5. Performance comparison on various architecture over CIFAR datasets. We evaluate the training error (solid lines) and test error
(lines marked with plus) with respect to the training epochs. (a) VGG-A architecture over CIFAR-10; (b) GoogLeNet architecture over
CIFAR-100; (c) residual network architecture over CIFAR-10; (d) residual network architecture over CIFAR-100.

Residual network We also apply our CWN module to the
residual network [14] with 56 layers. We follow the same
experimental protocol as described in [14] and the publicly
available Torch implementation for residual network 4. Fig-
ure 5 (c) and (d) show the training error and test error with
respect to epochs on CIFAR-10 and CIFAR-100 dataset re-
spectively, and the test errors after training are shown in Ta-
ble 3. We can ﬁnd that CWN converges slightly faster than
WN and ‘plain’ and achieves the best test performance.

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
i
a
r
t

Computational cost We implement our CWN module
based on Torch, and the re-parameterization is wrapped in
the fastest SpatialConvolution of cudnn package 5. We com-
pared the wall clock time of CWN module with the standard
convolution of cudnn. We use 3 × 3 convolution, 32 × 32
feature map with size 128 and mini-batch size of 64. The re-
sults are averaged over 100 runs. CWN costs 18.1ms while
the standard one takes 17.3ms.

4.4. Large scale classiﬁcation on ImageNet dataset

To show the scalability of our proposed method, we ap-
ply it to the large-scale ImageNet-2012 dataset [27] using
the GoogLeNet architecture. ImageNet-2012 includes im-
ages of 1,000 classes, and is split into training sets with
1.2M images, validation set with 50k images, and test set
with 100k images. We employ the validation set as the test
set, and evaluate the classiﬁcation performance based on
top-1 and top-5 error. We use single scale and single crop
test for simplifying discussion. Here we again use stochas-
tic gradient descent with mini-batch size of 64, momentum
of 0.9 and weight decay of 0.0001. The learning rate starts
from 0.1 and ends at 0.0001, and decays with exponentially
decaying until the end of the training with 90 epochs. The
top-5 training and test error curves are shown in Figure 6
and the ﬁnal test errors are shown in Table 4, where we can
ﬁnd that the deep network with CWN can converge to a low-
er training error faster, and obtain lower test error. We argue
that our CWN module draws its strength in improving the
performance of deep neural networks.

4https://github.com/facebook/fb.resnet.torch
5https://developer.nvidia.com/cudnn

plain
WN
CWN

0
0

20

40
epochs

60

80

0
0

20

40
epochs

60

80

(a) training error

(b) test error

Figure 6. Top-5 training and test error curves on GoogLeNet ar-
chitecture over ImageNet-2012 dataset.

Table 4. Comparison of test errors (%) on GoogLeNet over
ImageNet-2012 dataset.

Methods
plain
WN
CWN

Top-1 error
30.78
28.64
26.1

Top-5 error
11.14
9.7
8.35

5. Conclusions

re-
In this paper we introduced a new efﬁcient
parameterization method named centered weight normal-
ization, which improves the conditioning and accelerates
the convergence of the deep neural networks training. We
validate its effectiveness in image classiﬁcation tasks over
both multi-layer perceptron and convolutional neural net-
work architectures. The re-parameterization method is able
to stabilize the distribution and accelerate the training for
a number of popular networks. More importantly, it has
very low computational overhead and can be wrapped as a
linear module suitable for different types of networks. We
argue that the centered weight normalization module owns
the potential to replace the standard linear module, and con-
tributes to new deep architectures with easier optimization
and better generalization.

Acknowledgement

This work was partially supported by NSFC-61370125,
NSFC-61402026, SKLSDE-2017ZX-03, FL-170100117,
DP-140102164, LP-150100671 and the Innovation Foun-
dation of BUAA for PhD Graduates.

2810

References

[1] S.-I. Amari. Natural gradient works efﬁciently in learning.

Neural Comput., 10(2):251–276, Feb. 1998. 1

[2] M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution

recurrent neural networks. In ICML, 2016. 1, 2

[3] D. Arpit, Y. Zhou, B. U. Kota, and V. Govindaraju. Normal-
ization propagation: A parametric technique for removing
internal covariate shift in deep networks. In ICML, 2016. 7
[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.

CoRR, abs/1607.06450, 2016. 1, 2, 7

[5] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. D. Fre-
In Advances
itas. Predicting parameters in deep learning.
in Neural Information Processing Systems 26, pages 2148–
2156, 2013. 2

[6] G. Desjardins, K.

Simonyan,

R.

Pascanu,

K. Kavukcuoglu. Natural neural networks.
2015. 1, 2, 6

and
In NIPS,

[7] Z. Ding, M. Shao, and Y. Fu. Deep robust encoder through
In ECCV, pages

locality preserving low-rank dictionary.
567–582, 2016. 1

[8] V. Dorobantu, P. A. Stromhaug, and J. Renteria. Dizzyrn-
n: Reparameterizing recurrent neural networks for norm-
preserving backpropagation. CoRR, abs/1612.04035, 2016.
2

[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
2010. 1, 3, 7

[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. C.
Courville, and Y. Bengio. Maxout networks. In ICML, 2013.
7

[11] R. B. Grosse and J. Martens. A kronecker-factored approxi-
mate ﬁsher matrix for convolution layers. In ICML, 2016. 1,
2

[12] R. B. Grosse and R. Salakhutdinov. Scaling up natural gra-
dient by sparsely factorizing the inverse ﬁsher matrix.
In
ICML, 2015. 1, 2

[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 1, 3, 7

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 7, 8

[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. CoRR, abs/1603.05027, 2016. 7
[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 1, 2, 4, 6, 7

[17] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 2, 3, 7

[18] A. Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, 2009. 7

[19] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁicient
backprop. In Neural Networks: Tricks of the Trade, 1998. 1,
3, 5, 7

[20] J. Martens and I. Sutskever. Training deep and recurrent net-
works with hessian-free optimization. In Neural Networks:
Tricks of the Trade (2nd ed.). 2012. 1, 2

[21] D. Mishkin and J. Matas. All you need is a good init.

In

ICLR, 2016. 1

[22] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 5

[23] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning, 2011. 6

[24] B. Neyshabur, R. Tomioka, R. Salakhutdinov, and N. Sre-
bro. Data-dependent path normalization in neural networks.
CoRR, abs/1511.06747, 2015. 7

[25] G. Qi. Hierarchically gated deep networks for semantic seg-

mentation. In CVPR, pages 2267–2275, 2016. 1

[26] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made
easier by linear transformations in perceptrons. In AISTATS,
2012. 2

[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), 115(3):211–252, 2015. 8

[28] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In NIPS, 2016. 1, 2, 4, 6

[29] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact so-
lutions to the nonlinear dynamics of learning in deep linear
neural networks. CoRR, abs/1312.6120, 2013. 1

[30] N. N. Schraudolph. Accelerated gradient descent by factor-

centering decomposition. Technical report, 1998. 4

[31] K. Simonyan and A. Zisserman. Very deep convolution-
al networks for large-scale image recognition. CoRR, ab-
s/1409.1556, 2014. 7

[32] K. Sun and F. Nielsen. Relative natural gradient for learning
large complex models. CoRR, abs/1606.06069, 2016. 6
[33] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
pages 1988–1996, 2014. 1

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014. 7

[35] Y. Tian, P. Luo, X. Wang, and X. Tang. Deep learning strong
parts for pedestrian detection. In ICCV, pages 1904–1912,
2015. 1

[36] S. Wiesler, A. Richard, R. Schl¨uter, and H. Ney. Mean-
normalized stochastic gradient for large-scale deep learning.
In ICASSP, 2014. 1, 2

[37] S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas.
In NIPS,

Full-capacity unitary recurrent neural networks.
pages 4880–4888. 2016. 2

[38] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, 2015.
2

2811

Centered Weight Normalization
in Accelerating Training of Deep Neural Networks

Lei Huang† Xianglong Liu†∗ Yang Liu† Bo Lang† Dacheng Tao‡
†State Key Laboratory of Software Development Environment, Beihang University, P.R.China
‡UBTECH Sydney AI Centre, School of IT, FEIT, The University of Sydney, Australia
{huanglei, xlliu, blonster, langbo}@nlsde.buaa.edu.cn, dacheng.tao@sydney.edu.au

Abstract

Training deep neural networks is difﬁcult for the patho-
logical curvature problem. Re-parameterization is an effec-
tive way to relieve the problem by learning the curvature
approximately or constraining the solutions of weights with
good properties for optimization. This paper proposes to re-
parameterize the input weight of each neuron in deep neural
networks by normalizing it with zero-mean and unit-norm,
followed by a learnable scalar parameter to adjust the norm
of the weight. This technique effectively stabilizes the dis-
tribution implicitly. Besides, it improves the conditioning
of the optimization problem and thus accelerates the train-
ing of deep neural networks. It can be wrapped as a linear
module in practice and plugged in any architecture to re-
place the standard linear module. We highlight the beneﬁts
of our method on both multi-layer perceptrons and convolu-
tional neural networks, and demonstrate its scalability and
efﬁciency on SVHN, CIFAR-10, CIFAR-100 and ImageNet
datasets.

1. Introduction

Recently, deep neural networks have achieved grand
success across a broad range of applications, e.g.,
im-
age classiﬁcation, speech recognition and object detection
[33, 35, 14, 36, 7, 25]. Neural networks typically are com-
posed of stacked layers, and the transformation between
layers consists of linear mapping with learnable parameters,
in which each neuron computes a weighted sum over its in-
puts and adds a bias term, and followed by an element-wise
nonlinear activation. The stacked structure endows a neu-
ral network learning feature hierarchies with features from
higher levels formed by the composition of lower level fea-
tures. Further, deep architectures provide neural network-
s powerful representation capacity of learning complicated
functions that can represent high-level abstractions.

∗Corresponding author

While the deep and complex structure enjoys appeal-
ing advantages, it also makes learning difﬁcult.
Indeed,
many explanations for the difﬁculty of deep learning have
been explored, such as the problem of vanishing and ex-
ploding gradients [2] , internal covariate shift [16], and
the pathological curvature [20]. To address these prob-
lems, various studies such as ﬁnely weight initialization
[19, 9, 13, 29, 21], normalization of internal activation
[16, 4], and sophistic optimization methods have been pro-
posed accordingly [20, 12, 11].

Our work is dedicated to the problem of pathological
curvature [20], i.e. the condition number of the Hessian ma-
trix of the objective function is low at the optimum region-
s [28], which makes learning extremely difﬁcult via ﬁrst
order stochastic gradient descent. Several studies [12, 11]
recently have attempted to use the pre-conditioning tech-
niques to improve the conditioning of the cost curvature.
However, these methods introduce too much overhead and
are not convenient to be applied.

An alternative track to facilitate the optimization
progress is the transformation of parameterization space of
a model [1], which is called re-parameterization. The mo-
tivation of re-parameterization is that there may be various
equivalent ways to parameterize the same model, some of
which are much easier to optimize than others [28]. There-
fore, exploring good ways of parameterizing neural net-
works [6, 28] are essentially important in training deep neu-
ral networks.

Inspired by the practical trick that weights are sampled
from a distribution with zero mean and a standard deviation
for initialization [19, 9, 13], in this paper we propose to con-
strain the input weight of each neuron with zero mean and
unit norm by re-parameterization during the course of train-
ing, followed by a learnable scalar parameter to adjust the
norm of the input weight. We use proxy parameters and per-
form gradient updating on these proxy parameters by back-
propagating the gradient information through the normal-
ization process (Figure 1). By introducing this process, the
summed input of each neuron is more likely to possess the

12803

properties of zero mean and stable variance. Besides, this
technique can effectively improve the conditioning of the
optimization problem and thus can accelerate the training
of deep neural networks.

We wrap the proposed re-parameterization method into
a linear module in practice, which can be plugged in any
architecture as a substitute for the standard linear module.
The technique we present is generic and can be applied
to a broad range of models. Our method is also orthogo-
nal and complementary to recent advances in deep learn-
ing, such as Adam optimization [17] and batch normaliza-
tion [16]. We conduct comprehensive experiments on Yale-
B, SVHN, CIFAR-10, CIFAR-100 and ImageNet datasets
over multi-layer perceptron and convolutional neural net-
work architectures. The results show that centered weight
normalization draws its strength in improving the perfor-
mance of deep neural networks. Our code is available at
https://github.com/huangleiBuaa/CenteredWN.

2. Related work

Training deep neural network via ﬁrst order stochastic
gradient descent is difﬁcult in practice, mainly due to the
pathological curvature problem. Several works have tried
the preconditioning techniques to accelerate the training.
Martens and Sutskever [20] developed a second-order opti-
mization method based on the Hessian-free approach. Oth-
er studies [12, 11] explicitly pre-multiply the cost gradient
by an approximate inverse of the Fisher information ma-
trix, thereby expecting to obtain an approximate natural
gradient. The approximate inverse can be obtained by us-
ing Cholesky factorization [12] or Kronecker-factored ap-
proximation [11]. These methods usually introduce much
overhead. Besides, their optimization procedures are usu-
ally coupled with the preconditioning, and thus can not be
easily applied.

Some works addressed the beneﬁts of centered activa-
tions [36] and gradients [26]. They show that these transfor-
mations make the Fisher information matrix approximate
block diagonal, and improve the optimization performance.
Batch normalization [16] further standardizes the activa-
tion with centering and scaling based on mini-batch, and
includes normalization as a part of the model architecture.
Ba et al. [4] computed the layer normalization statics over
all the hidden units in the same layers, targeting at the sce-
nario that the size of mini-batch is limited. These methods
focus on normalizing the activation of the neurons explicit-
ly while our method works by re-parameterizing the model,
and is expected to achieve better conditioning and stabilize
the activations implicitly.

Re-parameterization is an effective technique to facili-
tate the optimization progress [6, 28]. Guillaume et al. [6]
tried to estimate the expected projection matrix, and implic-
itly whitening the activations. However, the operation of

(cid:3039)
(cid:2032)(cid:4666)(cid:2204)(cid:3036)

(cid:4667)

(cid:3039)
(cid:2204)(cid:3036)

(cid:3039)
(cid:2205)(cid:3036)

. . .

. . .

. . .

(cid:1486)(cid:4666)

(cid:2034)(cid:1838)
(cid:3039)(cid:4667)
(cid:2034)(cid:2205)(cid:3036)

.
 
.
 
.

=

(cid:3039)
(cid:1878)(cid:3036)

(cid:3039)
(cid:2205)(cid:3036)

(cid:2190)

(cid:4666)(cid:3039)(cid:2879)(cid:2869)(cid:4667)

.
 
.
 
.

. . .

. . .

. . .

.
 
.
 
.

.
 
.
 
.

(cid:4666)(cid:3039)(cid:2879)(cid:2869)(cid:4667)

(cid:3039)
(cid:2190)

(cid:3013)
(cid:2190)

(cid:2207)

(cid:2190)

(cid:2206)

Figure 1. An illustrative example of layered neural networks with
re-parameterization (for brevity, we leave out the bias nodes). The
proposed re-parameterization method ﬁnely constructs a transfor-
mation ψ over the proxy parameter v to ensure that the trans-
formed weight w has certain beneﬁcial properties for the training
of neural network. Gradient updating is executed on the proxy pa-
rameter v by back-propagating the gradient information through
the normalization process.

updating the projection matrix is still coupled with the opti-
mization. Salimans and Kingma [28] designed weight nor-
malization [28] as a part of the model architecture. The pro-
posed weight normalization addresses the normalization of
the input weight for each neuron and decouples the length of
those weight vectors from their directions. Our work further
powers weight normalization by centering the input weight,
and we argue that it can ulteriorly improve the conditioning
and speed up the training for deep neural networks.

There exist studies constructing orthogonal matrix in
recurrent neural networks (RNN) [2, 37, 8] by using re-
parameterization to avoid the gradient vanish and explosion
problem. However, these methods are limited for the hid-
den to hidden transformation in RNN, because they require
the weight matrix to be square matrix. Other alternative
methods [5, 38] focus on reducing the storage and compu-
tation costs by re-parameterization. Different from them,
our work aims to design a general re-parameterized linear
module to accelerate training and improve the performance
of deep neural networks, and make it an alternative for the
standard linear module.

3. Centered weight normalization

We follow the matrix notation that the vector is in col-
umn form, except that the derivative is a row vector. Given
training data D = {(xi, yi), i = 1, 2, ..., M } where x de-
notes the input and y the target. A neural network is a func-
tion f (x; θ) parameterized by θ that is expected to ﬁt well
the training data and have good generalization for the test
data. The function f (x; θ) adopted by neural network usu-
ally consists of stacked layers. The transformation between
layers consists of a linear mapping zl = (Wl)T hl−1 + bl
with learnable parameters Wl ∈ Rd×n and bl ∈ Rn, and
followed by an element-wise nonlinearity: hl = ϕ(sl),
where l ∈ {1, 2, ..., L} indexes the layer and L denotes the

2804

total number of layers. By convention, h0 corresponds to
the input x and hL corresponds to the output of the net-
work f (x; θ). For clariﬁcation, we refer to z and h as pre-
activation and activation respectively. Under the denotation,
the learnable parameters are θ = {Wl, bl|l = 1, 2, . . . , L}.
Training the neural networks can be viewed as tuning the
parameters to minimize the discrepancy between the desired
output y and the predicted output f (x; θ). This discrepancy
is usually described by a loss function L(y, f (x; θ)), and
thus the objective is to optimize the parameters θ by mini-
mizing the loss as follows:

θ∗ = arg min

E

(x,y)∈D[L(y, f (x; θ))].

(1)

θ

Stochastic gradient descent (SGD) has proved to be an
effective way to train deep networks, in which the gradien-
t of the loss function with respect to the parameters ∂L
∂θ is
approximated by the mini-batch x1...m of size m at each
iteration, by computing ∂L
∂θ = 1

∂L(yi,f (xi;θ))
∂θ

m Σm
i=1

.

3.1. Methodology

Despite the fact that SGD can guarantee a local optimum,
it is also well known the practical success of SGD is highly
dependent on the curvature of the objective to be optimized.
Deep neural network usually exhibits pathological curva-
ture problem, which makes the learning difﬁcult. An alter-
native track to relieve the pathological curvature issue and
thus facilitate the optimization progress is the transforma-
tion of parameterization space of a network model, which
is called re-parameterization.

In the literatures, there exist practical tricks for weight
initialization where weights are sampled from a distribu-
tion with zero-mean and a standard deviation [19, 9, 13].
These initialization techniques effectively can avoid expo-
nentially reducing/magnifying the magnitudes of input sig-
nals or back-propagation signals in the initial phases, by
constructing stable and proper variances of the activations
among different layers. Motivated by this observation, we
propose to constrain the input weight of each neuron with
zero mean and unit norm during the course of training by
re-parameterization, which enjoys stable and fast training
of deep neural networks.

For simplicity, we start by considering one certain neu-
ron i in layer l, whose pre-activation zl
i)T h(l−1)+bl
i.
We denote wl
i as the input weight of the neuron, as shown
in Figure 1. We have left out the superscript l and subscript
i for brevity in the following discusses.

i = (wl

Standardize weight We ﬁrst re-parameterize the input
weight w of each neuron and make sure that it has the fol-
lowing properties: (1) zero-mean, i.e. wT 1 = 0 where 1 is
a column vector of all ones; (2) unit-norm, i.e. (cid:3)w(cid:3) = 1
where (cid:3)w(cid:3) denotes the Euclidean norm of w. To achieve

Algorithm 1 Forward pass of linear mapping with centered
weight normalization.

1: Input:

the mini-batch input data X ∈ Rd×m and
parameters to be learned: g ∈ Rn×1, b ∈ Rn×1,
V ∈ Rd×n.

2: Output: pre-activation Z ∈ Rn×m.
3: compute centered weight: ˆV = V − 1
4: for i = 1 to n do
5:

d 1d(1T

d V).

calculate normalized weight with respect to the i-th
neuron: wi = ˆvi
(cid:4)ˆvi(cid:4)

6: end for
7: calculate: ˆZ = WT X.
8: calculate pre-activation: Z = (g1T

m) ⊙ ˆZ + b1T
m.

the goal, we express the input weight w in terms of the prox-
y parameter v (Figure 1) using

w =

v − 1
(cid:3)v − 1

d 1(1T v)
d 1(1T v)(cid:3)

(2)

where d is the dimension of the input weight, and the s-
tochastic gradient descent or other alternative techniques
such as Adam [17] can be directly applied to the optimiza-
tion with respect to the proxy parameter v. We refer to the
proposed re-parametrization as centered weight normaliza-
tion, that is, we center and scale the proxy parameter v to
ensure that the input weight w has the desired zero-mean
and unit-norm properties as discussed before.

In our centered weight normalization, the parameter up-
dating is completed based on the proxy parameters, and
thus the gradient signal should back-propagate through the
normalization process. Speciﬁcally, given the derivative of
∂L/∂w, we can get the derivative of loss with respect to the
proxy parameter v as follows:

∂L
∂v

=

1
(cid:3)ˆv(cid:3)

[

∂L
∂w

− (

∂L
∂w

w)wT −

1)1T ]

(3)

1
d

(

∂L
∂w

where ˆv = v − 1
ter.

d 1(1T v) is the centered auxiliary parame-

Adjustable weight scale Our method can be viewed ef-
fectively as a solution to the constrained optimization prob-
lem over neural networks:

θ∗ = arg minθ E

(x,y)∈D[L(y, f (x; θ))]

s.t. wT 1 = 0 and (cid:3)w(cid:3) = 1

(4)

where w indicates the input weight for each neuron in each
layer. Therefore, we regularize the network with 2n con-
straints in each layer where n is the number of ﬁlters in cer-
tain layer and the optimization is over the embedded sub-
manifold of the original weight space. While these con-
straints provide regularization, they also may reduce the

2805

Algorithm 2 Back-propagation pass of linear mapping with
centered weight normalization.
1: Input: pre-activation derivative { ∂L

∂Z ∈ Rn×m}. Oth-
er auxiliary variables from respective forward pass: ˆV,
W, ˆZ, X, g.

2: Output: the gradients with respect to the inputs { ∂L
∂g ∈ R1×n, ∂L

∂X ∈
∂b ∈

5:

4:

3:

∂Z ⊙ ˆZ)T

Rd×m} and learnable parameters: ∂L
∂V ∈ Rd×n.
R1×n, ∂L
m( ∂L
∂L
∂g = 1T
T
∂L
∂L
∂b = 1T
m
∂Z
= ∂L
∂L
∂Z ⊙ (g1T
m)
∂ ˆZ
∂X = W ∂L
∂L
∂ ˆZ
∂L
∂W = X ∂L
7:
∂ ˆZ
8: for i = 1 to n do
∂L
∂vi = 1
9:
10: end for

∂wi − ( ∂L

∂wi wi)wT

(cid:4)ˆvi(cid:4) ( ∂L

6:

T

i − 1

d ( ∂L

∂wi 1d)1T
d )

representation capacity of the networks. To address it, we
simply introduce a learnable scalar parameter g to ﬁne tune
the norm of w. Similar idea has been introduced in [28],
and proved useful in practice. We initialize it to 1, and ex-
pect the network learning process can ﬁnd a proper scalar
under the supervision signals. To summarize, we rewrite
the pre-activation z of each neuron in the following way

z = g(

v − 1
(cid:3)v − 1

d 1(1T v)
d 1(1T v)(cid:3)

)T h + b.

(5)

Wrapped module with re-parameterization We can
wrap our proposed method as a common module and plug-
in it in the neural networks as a substitute for the ordinary
linear module. To achieve this goal, the key is to implemen-
t the forward and back-propagation passes. Based on Eqn.
(5), it is readily to extend for multiple output neurons of size
n. We describe the details of the forward pass in Algorithm
1, and the back-propagation pass in Algorithm 2. In the al-
gorithms, the ‘⊙’ operator represents element-wise matrix
multiplication, 1d indicates a column vector of all ones of
size d, and X ∈ Rd×m is the mini-batch input data feeded
into this module, where d is the dimension of the input and
m is the size of the mini-batch. vi is the i-th column of V,
and W = (w1, w2, ..., wn).

layer For the convolutional

Convolutional
the
weight of each feature map is wc ∈ Rd×Fh×Fw where Fh
and Fw indicate the height and width of the ﬁlter, and d is
the number of the input feature maps. We unroll wc as a
FhFwd dimension vector w, then the same normalization
can be directly executed over the unrolled w.

layer,

3.2. Analysis and discussions

Next, we will show our centered weight normalization
enjoys certain appealing properties. It can stabilize the dis-
tribution of pre-activation z with respect to each neuron. To
illustrate this point, we introduce the following proposition
where we omit the bias term b to simplify the discussion.

Proposition 1. Let z = wT h, where wT 1 = 0 and (cid:3)w(cid:3) =
1. Assume h has Gaussian distribution with the mean:
Eh[h] = μ1, and covariance matrix: cov(h) = σ2I, where
μ ∈ R and σ2 ∈ R. We have Ez[z] = 0, var(z) = σ2.

The proof of this proposition is provided in the supple-
mentary materials. Such a proposition tells us that for each
neuron the pre-activation z has zero-mean and the same
variance as the activations fed in, when the assumption is
satisﬁed. This property does not strictly hold in practice due
to the issue of nonlinearities in the hidden layers, however,
we still empirically ﬁnd that our proposed centered weight
normalization approximately holds.

Note that our method is similar to batch normalization
[16]. However, batch normalization focuses on normalizing
the pre-activation compulsively such that the pre-activation
of current mini-batch data is zero-mean and unit-variance,
which is a data dependent normalization. Our method nor-
malizes the weights and is expected to have the effect of
zero-mean and stable variance implicitly, which is a data
independent normalization. Actually, our method can work
well by combining batch normalization as described in sub-
sequent experiments.

Besides the good property guaranteed by Proposition
1, another remarkable observation is that the proposed re-
parameterization method can make the optimization prob-
lem easier, which is supported by the following proposition.

Proposition 2. Regarding to the proxy parameter v, cen-
tered weight normalization makes that the gradient ∂L
∂v has
following properties: (1) zero-mean, i.e. ∂L
∂v · 1 = 0; (2)
orthogonal to the parameters w, i.e. ∂L

∂v · w = 0.

The derivative of Proposition 2 is also given in the sup-
plementary materials. The zero-mean property of ∂L/∂v
usually has an effect that the leading eigenvalue of the Hes-
sian is smaller, and therefore the optimization problem is
likely to become better-conditioned [30]. This promises the
network to learn at a higher rate, and hence converge much
faster. Meanwhile, the gradient is orthogonal to the param-
eters w, and therefore is orthogonal to ˆv. Following the
analysis in [28], the gradient ∂L/∂v can self-stabilize it-
s norm, which thus makes optimization of neural networks
robust to the value of the learning rate.

Computation complexity Given mini-batch input data
{X ∈ Rd×m}, regarding n neurons, both the forward pass
(Algorithm 1) and back-propagation pass (Algorithm 2) of

2806

0.5

0

n
a
e
m

−0.5

plain−L1
plain−L2
CWN−L1
CWN−L2

plain−L1
plain−L2
CWN−L1
CWN−L2

4

3

2

1

e
c
n
a
i
r
a
v

1080

1060

1040

1020

M
I
F
 
f
o
 
r
e
b
m
u
n
 
n
o
i
t
i
d
n
o
c

plain
NNN
WN
CWN

4

3

2

1

s
s
o
l
 
g
n
i
n
i
a
r
t

plain
NNN
WN
CWN

−1
0

200
400
updates (x10)

600

0
0

200
400
updates (x10)

600

100

0

20
40
updates (x100)

60

0
0

2000

4000

6000

updates

(a)

(b)

(c)

(d)

Figure 2. A case study on Yale-B dataset (‘-Ll’ indicates the l-th layer). With respect to the training updates, we analyze (a) the mean
over the mini-batch data and all neurons; (b) the variance calculated over the mini-batch data and all neurons; (c) the condition number
(log-scale) of relative FIM in the second last layer; and (d) the training loss.

the centered weight normalization have the computational
complexity of O(mnd + nd). This means that our centered
weight normalization has the same computational complex-
ity as the standard linear module, since the extra cost O(nd)
of centered weight normalization is negligible to O(mnd).
For a convolution layer with ﬁlters W ∈ Rn×d×Fh×Fw ,
given m mini-batch data {xi ∈ Rd×h×w, i = 1, 2, ..., m},
where h and w represent the height and width of the feature
map, the computational complexity of the standard convo-
lution layer is O(nmdhwFhFw), and while our proposed
method is O(mndhwFhFw + ndFhFw). The extra cost
O(ndFhFw) of the proposed normalization is also negligi-
ble compared to the convolution operation.

4. Experiments

In this section, we begin with a set of ablation study to
support the preliminary analysis of the proposed method.
Then in the following three subsections, we show the ef-
fectiveness of our proposed method on the general network
architectures, including (1) Multi-Layer Perceptron (MLP)
architecture for face and digit recognition tasks; and (2)
Convolutional Neural Network (CNN) models for large s-
cale image classiﬁcation.

Experimental protocols For all experiments, we adop-
t ReLUs [22] as the nonlinear activation and the negative
log-likelihood as the loss function, where (x, y) represents
the (input image, target class) pair. We choose the random
weight initialization by default as described in [19] if we
do not specify weight initialization methods. For the exper-
iments on MLP architecture, the input images are resized
and transformed to 1,024 dimensional vectors in gray scale,
with mean subtracted and variance divided.

4.1. Case study

As discussed in Section 3.2,

the proposed Centered
Weight Normalization (CWN) method can stabilize the
distribution of the pre-activations during training under
certain assumptions. A network with the proposed re-
parameterization technique is likely to become better-
conditioned. In this part, we conduct experiments to vali-
date the conclusions empirically.

We conduct the experiments on Yale-B dataset1 for face
recognition task. Yale-B dataset has 2,414 images with 38
classes. We randomly sample 380 images (i.e., 10 images
per class) as the test set and the others as training set, where
all images are resized as 32 × 32 pixels. We train a 5-layer
MLP with {128, 64, 48, 48} neurons in the hidden layers.
We compare our CWN with the plain network without re-
parameterization (named ‘plain’ for short). In the training,
we use stochastic gradient descent with batch size of 32.
For each method, the best results by tuning different learn-
ing rates in {0.1, 0.2, 0.5, 1} are reported.

Stabilize activation We investigate the mean and vari-
ance of pre-activations in each layer during the course of
training. Figure 2 (a) and (b) respectively show the mean
and variance from the ﬁrst and second layers, with respect
to different rounds of training updates. The mean and vari-
ance are calculated over the mini-batch data and all neuron-
s2 in the corresponding layers. We ﬁnd that the distribution
of pre-activation in plain network varies remarkably. For in-
stance, the mean decreases and the variance increases grad-
ually, even though the network nearly converges to the mini-
mal loss. The unstable distribution of pre-activation may re-
sult in an unstable learning, as shown in Figure 2 (d), where
the loss of plain network ﬂuctuates sharply. By comparing
the loss curve of CWN and the others, we ﬁnd that the pre-
activation of the network with CWN re-parameterization
has a stable mean and variance as shown in Figure 2 (a) and
(b). Especially, in the ﬁrst layer the pre-activation is nearly
zero-mean, which empirically supports the fact in Proposi-
tion 1. These beneﬁcial properties make the network with
CWN re-parameterization converge to an optimum regions
in a stable and fast way.

Information Matrix (FIM)

analysis The

Conditioning
the Fisher
dicator
to
lem is easy to solve.
Ex∼π{Ey∼P (y|x,θ)[( ∂ log P (y|x,θ)

show whether

∂θ

condition

of
number
is a good in-
optimization
prob-
FIM is deﬁned as Fθ =

the

)( ∂ log P (y|x,θ)
∂θ

)T ]},

1http://vision.ucsd.edu/ leekc/ExtYaleDatabase/ExtYaleB.html
2We also observe the mean and variance of the randomly selected neu-

rons in each layer, which also have the similar behaviours.

2807

50

100
epochs

150

200

50

100
epochs

150

200

(a) training error

(b) test error

Figure 3. Performances evaluation on MLP architecture over
permutation-invariant SVHN.

0.8

0.6

0.4

0.2

r
o
r
r
e
 
g
n
i
n
i
a
r
t

0
0

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
NNN
WN
CWN

BN
NNN+BN
WN+BN
CWN+BN
CWN

r
o
r
r
e
 
t
s
e
t

0.7

0.6

0.5

0.4

0.3

0.2

0.1
0

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
NNN
WN
CWN

plain
NNN
WN
CWN

50

100
epochs

150

200

50

100
epochs

150

200

(a) combining batch normalization

(b) using Adam optimization

Figure 4. Comparison of different methods by (a) combining batch
normalization and (b) using Adam optimization on permutation-
invariant SVHN. We evaluate training error (solid lines) and test
error (lines marked with plus) with respect to the training epochs.

and its condition number is cond(Fθ) = |λ(Fθ)|max
|λ(Fθ)|min
where |λ(Fθ)|max and |λ(Fθ)|min are the maximum and
minimum of the absolute values of Fθ’ eigenvalues. We
evaluated the condition number of relative FIM [32] with
respect to the last two layers, regarding the feasibility in
computation. We compared our methods with ‘plain’ and
the other two re-parameterization methods: Natural Neural
Network (NNN) [6] and Weight Normalization (WN) [28].
The results of the second last layer are shown in Figure 2
(c) and the last layer also has similar results, from which we
can ﬁnd that CWN, WN, NNN achieve better conditioning
compared to ‘plain’, and thus converges faster as shown in
Figure 2 (d). This indicates that re-parameterization serves
as an effective way to make the optimization problem easier
if good properties are designed carefully. Compared to WN,
our proposed CWN method further signiﬁcantly improves
the conditioning and speed up convergence. This observa-
tion is consistent with our analysis in Section 3.2 that the
proposed zero-mean property of input weight contributes
to making the optimization problem easier. At the testing
stage, the proposed CWN method achieves the lowest test
error as shown in Table 3, which means that CWN not only
accelerates and stabilizes training, but also has great poten-
tial to improve generalization of the neural network.

4.2. MLP architecture

In this part, we comprehensively evaluate our proposed
method on MLP architecture. We investigate both the train-
ing and test performances on SVHN [23]. SVHN consist-
s of 32 × 32 color images of house numbers collected by
Google Street View. It has 73,257 train images and 26,032
test images. We train a 6 layers MLP with 128 neurons cor-

Table 1. Comparison of test errors (%) averaged over 5 indepen-
dent runs on Yale-B and permutation-invariant SVHN.

plain
Yale-B
6.16
SVHN 18.98

NNN WN
4.68
6.58
17.12
17.99

CWN
4.20
16.16

respondingly in each hidden layer. Note that here we mainly
focus on validating the effectiveness of our proposed meth-
ods on MLP architecture, and under this situation, SVHN
dataset is permutation invariant.

We employ the stochastic gradient descent algorith-
m with mini-batch of size 1024.
Hyper-parameters
are selected by grid search, based on the test er-
ror on validation set (5% samples of the training set)
learning rates in
with the following grid speciﬁcations:
{0.1, 0.2, 0.5, 1}; the natural re-parameterization interval T
in {20, 50, 100, 200, 500} and the revised term ε within the
range of {0.01, 0.01, 0.1, 1} for NNN method.

Figure 3 shows the training and test error with respect
to the number of epochs. We can ﬁnd that WN, NNN and
CWN converge faster compared to ‘plain’, which indicates
that re-parameterization serves as an effective way to accel-
erate the training. Among all re-parametrization methods,
CWN converges fastest. Besides, we observe that both the
training and test error of CWN do not ﬂuctuate after a few
number of epochs in the training, which means that CWN is
much more stable when it reaches the optimal regions. We
can conjecture that the following functions of CWN as dis-
cussed in Section 3.2 contributes to this phenomenon: (1)
CWN can stabilize the distribution of pre-activation; (2) the
gradient ∂L/∂v of CWN can self-stabilize its norm. Table
3 presents the test error for different methods, where we fur-
ther ﬁnd that CWN achieves the best performances. These
observations together show the great potential of CWN in
improving the generalization of the neural network.

Combining batch normalization Batch normalization
[16] has shown to be very helpful for speeding up the train-
ing of deep networks. Combined with batch normalization,
previous re-parameterization methods [6] have shown suc-
cess in improving the performance. Here we show that over
the networks equipped with batch normalization, our pro-
posed CWN still outperforms others in all cases.
In this
experiment, Batch Normalization (BN) is plugged in be-
fore the nonlinearity in neural networks as suggested in
[16]. With batch normalization, we build different net-
works using the re-parameterized linear mapping including
WN, NNN and CWN, named ‘WN+BN’, ‘NNN+BN’ and
‘CWN+BN’ respectively.

Figure 4 (a) shows their experimental results on SVHN
dataset. We ﬁnd that ‘WN+BN’ and ‘NNN+BN’ have no
advantages compared to ‘BN’, while ‘CWN+BN’ signiﬁ-
cantly speeds up the training and achieves better test per-

2808

Table 2. Comparison of test errors (%) averaged over 5 inde-
pendent runs on Yale-B and permutation-invariant SVHN with
Xavier-Init and He-Init.

Table 3. Comparison of test errors (%) averaged over 3 inde-
pendent runs on 56 layers residual network over CIFAR-10 and
CIFAR-100 datasets.

Xavier-Init

He-Init

method Yale-B SVHN Yale-B SVHN
18.35
17.78
plain
18.14
17.83
NNN
WN
18.06
17.74
16.71
16.38
CWN

5.47
5.58
5.58
4.21

6.47
5.47
4.68
3.84

formance. Indeed, batch normalization is invariant to the
weights scaling as analyzed in [24] and [4]. However, batch
normalization is not re-centering invariant [4]. Therefore,
our CWN can further improve the performance of batch nor-
malization by centering the weights.

Amazingly, CWN itself achieves remarkably better per-
formance in terms of both the training speed and the
test error than BN, and even better than ‘CWN+BN’.
This is mainly due to the following reasons. Batch nor-
malization can well stabilize the distribution of the pre-
activation/activation, however it also introduces noises
stemming from the forceful transformation based on mini-
batch. Besides, during test the means and variances are es-
timated based on the moving averages of mini-batch means
and variances [16, 3, 4]. These ﬂaws may have a detrimental
effect on models. Our CWN can stabilize the distribution of
pre-activation implicitly and is deterministic without intro-
ducing stochastic noise. Moreover, the properties described
in Proposition 2 can improve the conditioning, self-stabilize
the gradient’s norm, and thus achieve better performance.

Adam optimization We consider an alternative optimiza-
tion method, Adam [17], to evaluate the performance. The
initial learning rate is set to {0.001, 0.002, 0.005, 0.01}, and
the best results for all methods on SVHN dataset are shown
in Figure 4 (b). Again, we ﬁnd that CWN can reach the
lowest errors at both training and test stages.

Different initialization We also have tried different ini-
tialization methods: Xavier-Init [9] and He-Init [13]. The
experimental setup is the same as in previous experiments.
The ﬁnal test errors both on Yale-B and SVHN datasets are
shown in Table 2. We get similar conclusions as the ran-
dom initialization [19] in previous experiments that CWN
achieves the best test performance.

4.3. CNN architectures on CIFAR dataset

In this part, we highlight that the proposed centered
weight normalization method also works well on several
popular state-of-the-art CNN architectures3, including VG-
G [31], GoogLeNet [34], and residual network [14, 15]. We

3The details of the used CNN architectures are shown in the supple-

mentary materials.

Methods
plain
WN
CWN

CIFAR-10
7.34 ± 0.52
7.58 ± 0.40
6.85 ± 0.26

CIFAR-100
29.38 ± 0.14
29.85 ± 0.66
29.23 ± 0.14

evaluate it over CIFAR-10 and CIFAR-100 datasets [18],
which consists of 50,000 training images and 10,000 test
images from 10 and 100 classes respectively. Each input
image consists of 32 × 32 pixels.

VGG-A We ﬁrst evaluate the proposed methods on the
VGG-A architecture [31] where we set the number of neu-
rons to 512 in the fully connected layer and use batch nor-
malization in the ﬁrst fully connected layer. The exper-
iments run on CIFAR-10 dataset and we preprocess the
dataset by subtracting the mean and dividing the variance.

We employ the stochastic gradient descent as the op-
timization method with mini-batch size of 256, and use
momentum of 0.9, weight decay of 0.0005. The learn-
ing rate decays with each K iterations halving the learn-
ing rate. We initialize the learning rate lr = {0.5, 1, 2, 4},
and K = {1000, 2000, 4000, 8000}. The hyper-parameters
are chosen over the validation set of 5,000 examples from
the training set by grid search. We report the results of
‘plain’, WN and CWN in Figure 5 (a). It is easy to see that
with the proposed CWN the training converges much faster
and promises the lowest test error of 13.06%, compared
to WN of 15.89% and ‘plain’ of 14.33%. Note that here
WN obtains even worse performance compared to ‘plain’,
which is mainly because that WN can not ensure the pre-
activations nearly zero-mean, and thus degenerate the per-
formance when the network is deep.

GoogleLeNet We conduct the experiments on Google-
LeNet. Here GoogleLeNet is equipped with the batch nor-
malization [16], plugged after the linear mappings. The
dataset is preprocessed as described in [10] with global con-
trast normalization and ZCA whitening. We follow the sim-
ple data augmentation that 4 pixels are padded on each side,
and a 32 × 32 crop is randomly sampled from the padded
image or its horizontal ﬂip. The models are trained with
a mini-batch size of 64, momentum of 0.9 and weight de-
cay of 0.0005. The learning rate starts from 0.1 and ends
at 0.001, and decays every two epochs with exponentially
decaying until the end of the training with 100 epochs. The
results on CIFAR-100 are reported in Figure 5 (b), from
which we can ﬁnd that CWN obtains slight speedup com-
pared to WN and ‘plain’. Moreover, CWN can attain the
lowest test error of 24.45%, compare to ‘plain’ of 25.52%
and WN of 25.39%. We also obtain similar observations on
CIFAR-10 (see the supplementary materials).

2809

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
WN
CWN

0.8

0.6

0.4

0.2

r
o
r
r
e
 
t
s
e
t

0.8

0.6

0.4

0.2

r
o
r
r
e

0
0

plain
WN
CWN

0.8

0.6

0.4

0.2

r
o
r
r
e

r
o
r
r
e

0.5

0.4

0.3

0.2

0.1

0
0

plain
WN
CWN

plain
WN
CWN

plain
WN
CWN

20

40

epochs

(a)

epochs

(b)

60

0
0

20

40

60

80

100

50

100

epochs

150

50

100

epochs

150

(c)

(d)

Figure 5. Performance comparison on various architecture over CIFAR datasets. We evaluate the training error (solid lines) and test error
(lines marked with plus) with respect to the training epochs. (a) VGG-A architecture over CIFAR-10; (b) GoogLeNet architecture over
CIFAR-100; (c) residual network architecture over CIFAR-10; (d) residual network architecture over CIFAR-100.

Residual network We also apply our CWN module to the
residual network [14] with 56 layers. We follow the same
experimental protocol as described in [14] and the publicly
available Torch implementation for residual network 4. Fig-
ure 5 (c) and (d) show the training error and test error with
respect to epochs on CIFAR-10 and CIFAR-100 dataset re-
spectively, and the test errors after training are shown in Ta-
ble 3. We can ﬁnd that CWN converges slightly faster than
WN and ‘plain’ and achieves the best test performance.

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
i
a
r
t

Computational cost We implement our CWN module
based on Torch, and the re-parameterization is wrapped in
the fastest SpatialConvolution of cudnn package 5. We com-
pared the wall clock time of CWN module with the standard
convolution of cudnn. We use 3 × 3 convolution, 32 × 32
feature map with size 128 and mini-batch size of 64. The re-
sults are averaged over 100 runs. CWN costs 18.1ms while
the standard one takes 17.3ms.

4.4. Large scale classiﬁcation on ImageNet dataset

To show the scalability of our proposed method, we ap-
ply it to the large-scale ImageNet-2012 dataset [27] using
the GoogLeNet architecture. ImageNet-2012 includes im-
ages of 1,000 classes, and is split into training sets with
1.2M images, validation set with 50k images, and test set
with 100k images. We employ the validation set as the test
set, and evaluate the classiﬁcation performance based on
top-1 and top-5 error. We use single scale and single crop
test for simplifying discussion. Here we again use stochas-
tic gradient descent with mini-batch size of 64, momentum
of 0.9 and weight decay of 0.0001. The learning rate starts
from 0.1 and ends at 0.0001, and decays with exponentially
decaying until the end of the training with 90 epochs. The
top-5 training and test error curves are shown in Figure 6
and the ﬁnal test errors are shown in Table 4, where we can
ﬁnd that the deep network with CWN can converge to a low-
er training error faster, and obtain lower test error. We argue
that our CWN module draws its strength in improving the
performance of deep neural networks.

4https://github.com/facebook/fb.resnet.torch
5https://developer.nvidia.com/cudnn

plain
WN
CWN

0
0

20

40
epochs

60

80

0
0

20

40
epochs

60

80

(a) training error

(b) test error

Figure 6. Top-5 training and test error curves on GoogLeNet ar-
chitecture over ImageNet-2012 dataset.

Table 4. Comparison of test errors (%) on GoogLeNet over
ImageNet-2012 dataset.

Methods
plain
WN
CWN

Top-1 error
30.78
28.64
26.1

Top-5 error
11.14
9.7
8.35

5. Conclusions

re-
In this paper we introduced a new efﬁcient
parameterization method named centered weight normal-
ization, which improves the conditioning and accelerates
the convergence of the deep neural networks training. We
validate its effectiveness in image classiﬁcation tasks over
both multi-layer perceptron and convolutional neural net-
work architectures. The re-parameterization method is able
to stabilize the distribution and accelerate the training for
a number of popular networks. More importantly, it has
very low computational overhead and can be wrapped as a
linear module suitable for different types of networks. We
argue that the centered weight normalization module owns
the potential to replace the standard linear module, and con-
tributes to new deep architectures with easier optimization
and better generalization.

Acknowledgement

This work was partially supported by NSFC-61370125,
NSFC-61402026, SKLSDE-2017ZX-03, FL-170100117,
DP-140102164, LP-150100671 and the Innovation Foun-
dation of BUAA for PhD Graduates.

2810

References

[1] S.-I. Amari. Natural gradient works efﬁciently in learning.

Neural Comput., 10(2):251–276, Feb. 1998. 1

[2] M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution

recurrent neural networks. In ICML, 2016. 1, 2

[3] D. Arpit, Y. Zhou, B. U. Kota, and V. Govindaraju. Normal-
ization propagation: A parametric technique for removing
internal covariate shift in deep networks. In ICML, 2016. 7
[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization.

CoRR, abs/1607.06450, 2016. 1, 2, 7

[5] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. D. Fre-
In Advances
itas. Predicting parameters in deep learning.
in Neural Information Processing Systems 26, pages 2148–
2156, 2013. 2

[6] G. Desjardins, K.

Simonyan,

R.

Pascanu,

K. Kavukcuoglu. Natural neural networks.
2015. 1, 2, 6

and
In NIPS,

[7] Z. Ding, M. Shao, and Y. Fu. Deep robust encoder through
In ECCV, pages

locality preserving low-rank dictionary.
567–582, 2016. 1

[8] V. Dorobantu, P. A. Stromhaug, and J. Renteria. Dizzyrn-
n: Reparameterizing recurrent neural networks for norm-
preserving backpropagation. CoRR, abs/1612.04035, 2016.
2

[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of
In AISTATS,

training deep feedforward neural networks.
2010. 1, 3, 7

[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. C.
Courville, and Y. Bengio. Maxout networks. In ICML, 2013.
7

[11] R. B. Grosse and J. Martens. A kronecker-factored approxi-
mate ﬁsher matrix for convolution layers. In ICML, 2016. 1,
2

[12] R. B. Grosse and R. Salakhutdinov. Scaling up natural gra-
dient by sparsely factorizing the inverse ﬁsher matrix.
In
ICML, 2015. 1, 2

[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In ICCV, 2015. 1, 3, 7

[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 7, 8

[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. CoRR, abs/1603.05027, 2016. 7
[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 1, 2, 4, 6, 7

[17] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 2, 3, 7

[18] A. Krizhevsky. Learning multiple layers of features from

tiny images. Technical report, 2009. 7

[19] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁicient
backprop. In Neural Networks: Tricks of the Trade, 1998. 1,
3, 5, 7

[20] J. Martens and I. Sutskever. Training deep and recurrent net-
works with hessian-free optimization. In Neural Networks:
Tricks of the Trade (2nd ed.). 2012. 1, 2

[21] D. Mishkin and J. Matas. All you need is a good init.

In

ICLR, 2016. 1

[22] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 5

[23] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS Workshop on Deep Learning and Un-
supervised Feature Learning, 2011. 6

[24] B. Neyshabur, R. Tomioka, R. Salakhutdinov, and N. Sre-
bro. Data-dependent path normalization in neural networks.
CoRR, abs/1511.06747, 2015. 7

[25] G. Qi. Hierarchically gated deep networks for semantic seg-

mentation. In CVPR, pages 2267–2275, 2016. 1

[26] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made
easier by linear transformations in perceptrons. In AISTATS,
2012. 2

[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), 115(3):211–252, 2015. 8

[28] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In NIPS, 2016. 1, 2, 4, 6

[29] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact so-
lutions to the nonlinear dynamics of learning in deep linear
neural networks. CoRR, abs/1312.6120, 2013. 1

[30] N. N. Schraudolph. Accelerated gradient descent by factor-

centering decomposition. Technical report, 1998. 4

[31] K. Simonyan and A. Zisserman. Very deep convolution-
al networks for large-scale image recognition. CoRR, ab-
s/1409.1556, 2014. 7

[32] K. Sun and F. Nielsen. Relative natural gradient for learning
large complex models. CoRR, abs/1606.06069, 2016. 6
[33] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
pages 1988–1996, 2014. 1

[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CoRR, abs/1409.4842,
2014. 7

[35] Y. Tian, P. Luo, X. Wang, and X. Tang. Deep learning strong
parts for pedestrian detection. In ICCV, pages 1904–1912,
2015. 1

[36] S. Wiesler, A. Richard, R. Schl¨uter, and H. Ney. Mean-
normalized stochastic gradient for large-scale deep learning.
In ICASSP, 2014. 1, 2

[37] S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas.
In NIPS,

Full-capacity unitary recurrent neural networks.
pages 4880–4888. 2016. 2

[38] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola,
L. Song, and Z. Wang. Deep fried convnets. In ICCV, 2015.
2

2811

