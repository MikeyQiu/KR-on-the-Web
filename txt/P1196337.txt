Adversarially Regularized Autoencoders

8
1
0
2
 
n
u
J
 
9
2
 
 
]

G
L
.
s
c
[
 
 
3
v
3
2
2
4
0
.
6
0
7
1
:
v
i
X
r
a

Jake (Junbo) Zhao * 1 2 Yoon Kim * 3 Kelly Zhang 1 Alexander M. Rush 3 Yann LeCun 1 2

Abstract
Deep latent variable models, trained using varia-
tional autoencoders or generative adversarial net-
works, are now a key technique for representa-
tion learning of continuous structures. However,
applying similar methods to discrete structures,
such as text sequences or discretized images, has
proven to be more challenging. In this work, we
propose a ﬂexible method for training deep latent
variable models of discrete structures. Our ap-
proach is based on the recently-proposed Wasser-
stein autoencoder (WAE) which formalizes the ad-
versarial autoencoder (AAE) as an optimal trans-
port problem. We ﬁrst extend this framework to
model discrete sequences, and then further ex-
plore different learned priors targeting a control-
lable representation. This adversarially regular-
ized autoencoder (ARAE) allows us to generate
natural textual outputs as well as perform manipu-
lations in the latent space to induce change in the
output space. Finally we show that the latent rep-
resentation can be trained to perform unaligned
textual style transfer, giving improvements both in
automatic/human evaluation compared to existing
methods.

1. Introduction
Recent work on deep latent variable models, such as vari-
ational autoencoders (Kingma & Welling, 2014) and gen-
erative adversarial networks (Goodfellow et al., 2014), has
shown signiﬁcant progress in learning smooth representa-
tions of complex, high-dimensional continuous data such as
images. These latent variable representations facilitate the
ability to apply smooth transformations in latent space in or-
der to produce complex modiﬁcations of generated outputs,
while still remaining on the data manifold.

Unfortunately, learning similar latent variable models of

*Equal contribution 1Department of Computer Science, New
York University 2Facebook AI Research 3School of Engineering
and Applied Sciences, Harvard University. Correspondence to:
Jake Zhao <jakezhao@cs.nyu.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

discrete structures, such as text sequences or discretized
images, remains a challenging problem. Initial work on
VAEs for text has shown that optimization is difﬁcult, as the
generative model can easily degenerate into a unconditional
language model (Bowman et al., 2016). Recent work on
generative adversarial networks (GANs) for text has mostly
focused on dealing with the non-differentiable objective
either through policy gradient methods (Che et al., 2017;
Hjelm et al., 2018; Yu et al., 2017) or with the Gumbel-
Softmax distribution (Kusner & Hernandez-Lobato, 2016).
However, neither approach can yet produce robust represen-
tations directly.

In this work, we extend the adversarial autoencoder (AAE)
(Makhzani et al., 2015) to discrete sequences/structures.
Similar to the AAE, our model learns an encoder from an
input space to an adversarially regularized continuous latent
space. However unlike the AAE which utilizes a ﬁxed
prior, we instead learn a parameterized prior as a GAN.
Like sequence VAEs, the model does not require using
policy gradients or continuous relaxations. Like GANs,
the model provides ﬂexibility in learning a prior through a
parameterized generator.

This adversarially regularized autoencoder (ARAE) can fur-
ther be formalized under the recently-introduced Wasser-
stein autoencoder (WAE) framework (Tolstikhin et al.,
2018), which also generalizes the adversarial autoencoder.
This framework connects regularized autoencoders to an
optimal transport objective for an implicit generative model.
We extend this class of latent variable models to the case of
discrete output, speciﬁcally showing that the autoencoder
cross-entropy loss upper-bounds the total variational dis-
tance between the model/data distributions. Under this
setup, commonly-used discrete decoders such as RNNs, can
be incorporated into the model. Finally to handle non-trivial
sequence examples, we consider several different (ﬁxed
and learned) prior distributions. These include a standard
Gaussian prior used in image models and in the AAE/WAE
models, a learned parametric generator acting as a GAN in
latent variable space, and a transfer-based parametric gener-
ator that is trained to ignore targeted attributes of the input.
The last prior can be directly used for unaligned transfer
tasks such as sentiment or style transfer.

Experiments apply ARAE to discretized images and text

Adversarially Regularized Autoencoders

sequences. The latent variable model is able to gener-
ate varied samples that can be quantitatively shown to
cover the input spaces and to generate consistent image
and sentence manipulations by moving around in the la-
tent space via interpolation and offset vector arithmetic.
When the ARAE model is trained with task-speciﬁc ad-
versarial regularization, the model improves upon strong
results on sentiment transfer reported in Shen et al. (2017)
and produces compelling outputs on a topic transfer task
using only a single shared space. Code is available at
https://github.com/jakezhaojb/ARAE.

X

=

2. Background and Notation
n to be a set of
Discrete Autoencoder Deﬁne
discrete sequences where
is a vocabulary of symbols.
V
Our discrete autoencoder will consist of two parameterized
functions: a deterministic encoder function encφ :
X (cid:55)→ Z
with parameters φ that maps from input space to code space,
and a conditional decoder pψ(x
with
parameters ψ. The parameters are trained based on the
cross-entropy reconstruction loss:

z) over structures

X

V

|

rec(φ, ψ) =

log pψ(x

encφ(x))

−

|

L

The choice of the encoder and decoder parameterization is
problem-speciﬁc, for example we use RNNs for sequences.
encφ(x)) for the
We use the notation, ˆx = arg maxx pψ(x
decoder mode, and call the model distribution Pψ.

|

Generative Adversarial Networks GANs are a class of
parameterized implicit generative models (Goodfellow et al.,
2014). The method approximates drawing samples from
a true distribution z
P∗ by instead employing a noise
sample s and a parameterized generator function ˜z = gθ(s)
to produce ˜z
Pz. Initial work on GANs implicitly min-
imized the Jensen-Shannon divergence between the distri-
butions. Recent work on Wasserstein GAN (WGAN) (Ar-
jovsky et al., 2017), replaces this with the Earth-Mover
(Wasserstein-1) distance.

∼

∼

GAN training utilizes two separate models: a generator
gθ(s) maps a latent vector from some easy-to-sample noise
distribution to a sample from a more complex distribution,
and a critic/discriminator fw(z) aims to distinguish real data
and generated samples from gθ. Informally, the generator
is trained to fool the critic, and the critic to tell real from
generated. WGAN training uses the following min-max
optimization over generator θ and critic w,

min
θ

max
w∈W

Ez∼P∗ [fw(z)]

E˜z∼Pz [fw(˜z)],

−

Z (cid:55)→

where fw :
R denotes the critic function, ˜z is ob-
tained from the generator, ˜z = gθ(s), and P∗ and Pz are
real and generated distributions. If the critic parameters w
are restricted to an 1-Lipschitz function set
, this term cor-
respond to minimizing Wasserstein-1 distance W (P∗, Pz).

W

We use a naive approximation to enforce this property by
(cid:15), (cid:15)]d (Arjovsky et al., 2017).1
weight-clipping, i.e. w = [

−
3. Adversarially Regularized Autoencoder
ARAE combines a discrete autoencoder with a GAN-
regularized latent representation. The full model is shown
in Figure 1, which produces a learned distribution over the
discrete space Pψ. Intuitively, this method aims to provide
smoother hidden encoding for discrete sequences with a
ﬂexible prior. In the next section we show how this sim-
ple network can be formally interpreted as a latent variable
model under the Wasserstein autoencoder framework.

The model consists of a discrete autoencoder regularized
with a prior distribution,

min
φ,ψ

L

rec(φ, ψ) + λ(1)W (PQ, Pz)

Here W is the Wasserstein distance between PQ, the distri-
bution from a discrete encoder model (i.e. encφ(x) where
P(cid:63)), and Pz, a prior distribution. As above, the W func-
x
tion is computed with an embedded critic function which is
optimized adversarially to the generator and encoder.2

∼

The model is trained with coordinate descent across: (1)
the encoder and decoder to minimize reconstruction, (2) the
critic function to approximate the W term, (3) the encoder
adversarially to the critic to minimize W :

1) min
φ,ψ

2) max
w∈W
3) min

φ

Lrec(φ, ψ) = Ex∼P(cid:63) [− log pψ(x | encφ(x))]

Lcri(w) = Ex∼P(cid:63) [fw(encφ(x))] − E˜z∼Pz [fw(˜z)]

Lenc(φ) = Ex∼P(cid:63) [fw(encφ(x))] − E˜z∼Pz [fw(˜z)]

The full training algorithm is shown in Algorithm 1.
Empirically we found that the choice of the prior distribu-
tion Pz strongly impacted the performance of the model.
The simplest choice is to use a ﬁxed distribution such as
a Gaussian
(0, I), which yields a discrete version of the
adversarial autoencoder (AAE). However in practice this
choice is seemingly too constrained and suffers from mode-
collapse.3

N

Instead we exploit the adversarial setup and use learned
prior parameterized through a generator model. This is
analogous to the use of learned priors in VAEs (Chen et al.,
2017; Tomczak & Welling, 2018). Speciﬁcally we introduce
(0, I) to act as an
a generator model, gθ(s) over noise s

∼ N

1While we did not experiment with enforcing the Lipschitz
constraint via gradient penalty (Gulrajani et al., 2017) or spectral
normalization (Miyato et al., 2018), other researchers have found
slight improvements by training ARAE with the gradient-penalty
version of WGAN (private correspondence).

2Other GANs could be used for this optimization. Experimen-
tally we found that WGANs to be more stable than other models.
3We note that recent work has successfully utilized AAE for

text by instead employing a spherical prior (Cífka et al., 2018).

Adversarially Regularized Autoencoders

where we want to change an attribute of a discrete input
without aligned examples, e.g. to change the topic or senti-
ment of a sentence. Deﬁne this attribute as y and redeﬁne
the decoder to be conditional pψ(x

z, y).

|

To adapt ARAE to this setup, we modify the objective to
learn to remove attribute distinctions from the prior (i.e.
we want the prior to encode all the relevant information
except about y). Following similar techniques from other
domains, notably in images (Lample et al., 2017) and video
modeling (Denton & Birodkar, 2017), we introduce a latent
space attribute classiﬁer:

min
φ,ψ,θ L

rec(φ, ψ) + λ(1)W (PQ, Pz)

λ(2)

class(φ, u)

−

L

L

where
z) from
class(φ, u) is the loss of a classiﬁer pu(y
latent variable to labels (in our experiments we always set
λ(2) = 1). This requires two more update steps: (2b) train-
ing the classiﬁer, and (3b) adversarially training the encoder
to this classiﬁer. This algorithm is shown in Algorithm 2.

|

4. Theoretical Properties

Standard GANs implicitly minimize a divergence measure
(e.g. f -divergence or Wasserstein distance) between the
true/model distributions. In our case however, we implicitly
minimize the divergence between learned code distributions,
and it is not clear if this training objective is matching the
distributions in the original discrete space. Tolstikhin et al.
(2018) recently showed that this style of training is minimiz-
ing the Wasserstein distance between the data distribution
P(cid:63) and the model distribution Pψ with latent variables (with
density pψ(x) = (cid:82)
In this section we apply the above result to the discrete case
and show that the ARAE loss minimizes an upper bound on
the total variation distance between P(cid:63) and Pψ.
Deﬁnition 1 (Kantorovich’s formulation of optimal trans-
port). Let P(cid:63), Pψ be distributions over
, and further let
R+ be a cost function. Then the optimal
c(x, y) :
transport (OT) problem is given by

z) p(z) dz).

X ×X →

z pψ(x

X

|

Wc(P(cid:63), Pψ) =

inf
Γ∈P(x∼P(cid:63),y∼Pψ)

Ex,y∼Γ[c(x, y)]

(x

where
∼
tions of (x, y) with marginals P(cid:63) and Pψ.

P(cid:63), y

∼

P

Pψ) is the set of all joint distribu-

1
p

y

∼

−

Pz, x

Pψ(x

p
p then Wc(P(cid:63), Pψ)
(cid:107)

In particular, if c(x, y) =
x
(cid:107)
is the Wasserstein-p distance between P(cid:63) and Pψ. Now
suppose we utilize a latent variable model to ﬁt the data, i.e.
z). Then Tolstikhin et al. (2018) prove
z
∼
the following theorem:
Theorem 1. Let Gψ :
tion (parameterized by ψ) from the latent space
space

be a deterministic func-
to data
z) on
x) be

Z
that induces a dirac distribution Pψ(x
. Let Q(z
}

z) = 1
{

X
, i.e. pψ(x

x = Gψ(z)

Z → X

X

|
|

|

|

Figure 1: ARAE architecture. A discrete sequence x is encoded
and decoded to produce ˆx. A noise sample s is passed though a
generator gθ (possibly the identity) to produce a prior. The critic
function fw is only used at training to enforce regularization W .
The model produce discrete samples x from noise s. Section 5
relates these samples x ∼ Pψ to x ∼ P(cid:63).
Algorithm 1 ARAE Training

for each training iteration do

(1) Train the encoder/decoder for reconstruction (φ, ψ)
i=1 ∼ P(cid:63) and compute z(i) = encφ(x(i))
Sample {x(i)}m
Backprop loss, Lrec = − 1
m

i=1 log pψ(x(i) | z(i))

(cid:80)m

i=1 ∼ P(cid:63) and {s(i)}m

(2) Train the critic (w)
Sample {x(i)}m
Compute z(i) = encφ(x(i)) and ˜z(i) = gθ(z(i))
(cid:80)m
Backprop loss − 1
m
Clip critic w to [−(cid:15), (cid:15)]d.

i=1 fw(z(i)) + 1

(cid:80)m

i=1 ∼ N (0, I)

m

i=1 fw(˜z(i))

(3) Train the encoder/generator adversarially (φ, θ)
i=1 ∼ P(cid:63) and {s(i)}m
Sample {x(i)}m
Compute z(i) = encφ(x(i)) and ˜z(i) = gθ(s(i)).
Backprop loss 1
m

i=1 fw(z(i)) − 1

i=1 ∼ N (0, I)

i=1 fw(˜z(i))

(cid:80)m

(cid:80)m

m

end for

implicit prior distribution Pz.4 We optimize its parameters
θ as part of training in Step 3.

Algorithm 2 ARAE Transfer Extension

Each loop additionally:

(2b) Train attribute classiﬁer (u)
Sample
encφ(x(i))
Backprop loss

m
i=1 ∼
1
m

x(i)
{

i=1 log pu(y(i)

(cid:80)m

}

z(i))

|

P(cid:63), lookup y(i), and compute z(i) =

(3b) Train the encoder adversarially (φ)
Sample
encφ(x(i))
Backprop loss

m
i=1 ∼
1
m

i=1 log pu(1

x(i)
{

(cid:80)m

y(i)

}

P(cid:63), lookup y(i), and compute z(i) =

z(i))

−

|

−

−

Extension: Unaligned Transfer Regularization of the la-
tent space makes it more adaptable for direct continuous
optimization that would be difﬁcult over discrete sequences.
For example, consider the problem of unaligned transfer,

4The downside of this approach is that the latent variable z is
now much less constrained. However we ﬁnd experimentally that
using a a simple MLP for gθ signiﬁcantly regularizes the encoder
RNN.

Adversarially Regularized Autoencoders

x).
any conditional distribution on
Deﬁne its marginal to be PQ, which has density pQ(x) =
(cid:82)
x pQ(z

x) p(cid:63)(x)dx. Then,

with density pQ(z

Z

|

|
Wc(P(cid:63), Pψ) =

inf
Q(z | x):PQ=Pz

EP(cid:63) EQ(z | x)[c(x, Gψ(z))]

Theorem 1 essentially says that learning an autoencoder can
be interpreted as learning a generative model with latent vari-
ables, as long as we ensure that the marginalized encoded
space is the same as the prior. This provides theoretical justi-
ﬁcation for adversarial autoencoders (Makhzani et al., 2015),
and Tolstikhin et al. (2018) used the above to train deep gen-
erative models of images by minimizing the Wasserstein-2
distance (i.e. squared loss between real/generated images).
We now apply Theorem 1 to discrete autoencoders trained
with cross-entropy loss.
Corollary 1 (Discrete case). Suppose x
X
is the set of all one-hot vectors of length n, and let fψ :
∆n−1 be a deterministic function that goes from the
Z →
1 dimensional simplex ∆n−1.
latent space
−
Z
Further let Gψ :
be a deterministic function such
that Gψ(z) = arg maxw∈X w(cid:62)fψ(z), and as above let
z) be the dirac distribution derived from Gψ such
Pψ(x
. Then the following is an
that pψ(x
}
upper bound on
TV, the total variation distance
between P(cid:63) and Pψ:

x = Gψ(z)
{
P(cid:63)(cid:107)
Pψ −

to the n

z) = 1

Z → X

where

∈ X

(cid:107)

|

|

inf
Q(z | x):PQ=Pz

EP(cid:63) EQ(z | x)

(cid:104)

2
log 2

−

(cid:105)
log x(cid:62)fψ(z)

|V|

m and therefore

The proof is in Appendix A. For natural language we have
is the set of sentences of length
n =
m, where m is the maximum sentence length (shorter sen-
tences are padded if necessary). Then the total variational
(TV) distance is given by

X

Pψ −

(cid:107)

P(cid:63)(cid:107)

TV =

pψ(x)

p(cid:63)(x)
|

−

1
2

(cid:88)
x∈Vm |

|

−

−

log pψ(x

This is an interesting alternative to the usual maximum
likelihood approach which instead minimizes KL(P(cid:63), Pψ).5
log x(cid:62)fψ(z) =
It is also clear that
z), the
standard autoencoder cross-entropy loss at the sentence level
with fψ as the decoder. As the above objective is hard to
minimize directly, we follow Tolstikhin et al. (2018) and
consider an easier objective by (i) restricting Q(z
x) to a
family of distributions induced by a deterministic encoder
parameterized by φ, and (ii) using a Langrangian relaxation
of the constraint PQ = Pz. In particular, letting Q(z
x) =
1
be the dirac distribution induced by a
deterministic encoder (with associated marginal Pφ), the
objective is given by

z = encφ(x)
{

}

|

|

5The relationship between KL-divergence and total variation
distance is also given by Pinsker’s inquality, which states that
2(cid:107)Pψ − P(cid:63)(cid:107)2

TV ≤ KL(P(cid:63), Pψ).

min
φ,ψ

EP(cid:63) [

−

|

log pψ(x

encφ(z))] + λW (Pφ, Pz)

Note that our minimizing the Wasserstein distance in the
latent space W (Pφ, Pz) is independent from the Wassertein
distance minimization in the output space in WAEs. Finally,
instead of using a ﬁxed prior (which led to mode-collapse
in our experiments) we parameterize Pz implicitly by trans-
forming a simple random variable with a generator (i.e.
(0, I), z = gθ(s)). This recovers the ARAE objec-
s
tive from the previous section.

∼ N

We conclude this section by noting that while the theoretical
formalization of the AAE as a latent variable model was an
important step, in practice there are many approximations
made to the actual optimal transport objective. Meaning-
fully quantifying (and reducing) such approximation gaps
remains an avenue for future work.
5. Methods and Architectures

We experiment with ARAE on three setups: (1) a small
model using discretized images trained on the binarized
version of MNIST, (2) a model for text sequences trained
on the Stanford Natural Language Inference (SNLI) corpus
(Bowman et al., 2015), and (3) a model trained for text
transfer trained on the Yelp/Yahoo datasets for unaligned
sentiment/topic transfer. For experiments using a learned
prior, the generator architecture uses a low dimensional s
with a Gaussian prior s
(0, I), and maps it to z using
an MLP gθ. The critic fw is also parameterized as an MLP.

∼ N

=

0, 1
{

The image model encodes/decodes binarized images. Here
n where n is the image size. The encoder
}
X
Rm, encφ(x) =
used is an MLP mapping from
0, 1
}
{
MLP(x; φ) = z. The decoder predicts each pixel in x
with as a parameterized logistic regression, pψ(x
z) =
(cid:81)n

|
σ(h))1−xj where h = MLP(z; ψ).

j=1 σ(h)xj (1

(cid:55)→

n

−

V

X

=

The text model uses a recurrent neural network (RNN) for
n where n is the
both the encoder and decoder. Here
V
is the vocabulary of the underlying
sentence length and
language. We deﬁne encφ(x) = z to be the last hidden
state of an encoder RNN. For decoding we feed z as an
additional input to the decoder RNN at each time step, and
at each time step via soft-
calculate the distribution over
j=1 softmax(Whj + b)xj where W
max, pψ(x
and b are parameters (part of ψ) and hj is the decoder RNN
hidden state. To be consistent with Corollary 1 we need to
ﬁnd the highest-scoring sequence ˆx under this distribution
during decoding, which is intractable in general. Instead
we approximate this with greedy search. The text transfer
model uses the same architecture as the text model but ex-
tends it with a classiﬁer pu(y
z) which is modeled using
an MLP and trained to minimize cross-entropy.

z) = (cid:81)n

V

|

|

We further compare our approach with a standard autoen-
coder (AE) and the cross-aligned autoencoder (Shen et al.,

Adversarially Regularized Autoencoders

Positive
⇒ ARAE
⇒ Cross-AE

great indoor mall .
no smoking mall .
terrible outdoor urine .

Positive
⇒ ARAE
⇒ Cross-AE

it has a great atmosphere , with wonderful service .
it has no taste , with a complete jerk .
it has a great horrible food and run out service .

we came on the recommendation of a bell boy and the food was amazing .
we came on the recommendation and the food was a joke .
we went on the car of the time and the chicken was awful .

Positive
⇒ ARAE
⇒ Cross-AE

Negative
⇒ ARAE
⇒ Cross-AE

hell no !
hell great !
incredible pork !

Negative
⇒ ARAE
⇒ Cross-AE

small , smokey , dark and rude management .
small , intimate , and cozy friendly staff .
great , , , chips and wine .

Negative
⇒ ARAE
⇒ Cross-AE

the people who ordered off the menu did n’t seem to do much better .
the people who work there are super friendly and the menu is good .
the place , one of the ofﬁce is always worth you do a business .

Table 2: Sentiment transfer results, where we transfer from posi-
tive to negative sentiment (Top) and negative to positive sentiment
(Bottom). Original sentence and transferred output (from ARAE
and the Cross-Aligned AE (from Shen et al. (2017)) of 6 randomly-
drawn examples.

vided a starting point for image generation models. Here
we use a similar method for text generation, which we call
reverse perplexity. We generate 100k samples from each
of the models, train an RNN language model on generated
samples and evaluate perplexity on held-out data.7 While
similar metrics for images (e.g. Parzen windows) have been
shown to be problematic, we argue that this is less of an
issue for text as RNN language models achieve state-of-the-
art perplexities on text datasets. We also calculate the usual
“forward” perplexity by training an RNN language model on
real data and testing on generated data. This measures the
ﬂuency of the generated samples, but cannot detect mode-
collapse, a common issue in training GANs (Arjovsky &
Bottou, 2017; Hu et al., 2018).

Table 1 shows these metrics for (i) ARAE, (ii) an autoen-
coder (AE),8 (iii) an RNN language model (LM), and (iv)
the real training set. We further ﬁnd that with a ﬁxed
prior, the reverse perplexity of an AAE-style text model
(Makhzani et al., 2015) was quite high (980) due to mode-
collapse. All models are of the same size to allow for fair
comparison. Training directly on real data (understand-
ably) outperforms training on generated data by a large
margin. Surprisingly however, training on ARAE samples
outperforms training on LM/AE samples in terms of reverse
perplexity.

6.2. Unaligned Text Style Transfer

Next we evaluate the model in the context of a learned adver-
sarial prior, as described in Section 3. We experiment with
two unaligned text transfer tasks: (i) transfer of sentiment on
the Yelp corpus, and (ii) topic on the Yahoo corpus (Zhang

7We also found this metric to be helpful for early-stopping.
8To “sample” from an AE we ﬁt a multivariate Gaussian to
the code space after training and generate code vectors from this
Gaussian to decode back into sentence space.

Figure 2: Image samples. The top block shows output generation
of the decoder for random noise samples; the bottom block shows
sample interpolation results.

Data

Reverse PPL

Forward PPL

Real data
LM samples
AE samples
ARAE samples

27.4
90.6
97.3
82.2

-
18.8
87.8
44.3

Table 1: Reverse PPL: Perplexity of language models trained on
the synthetic samples from a ARAE/AE/LM, and evaluated on real
data. Forward PPL: Perplexity of a language model trained on real
data and evaluated on synthetic samples.

1, 1)n by the tanh function at output layer.

2017) for transfer. In both our ARAE and standard AE
experiments, the encoder output is normalized to lie on the
unit sphere, and the generator output is bounded to lie in
(
−
Note, learning deep latent variable models for text sequences
has been a signiﬁcantly more challenging empirical problem
than for images. Standard models such as VAEs suffer from
optimization issues that have been widely documented. We
performed experiments with recurrent VAE, introduced by
(Bowman et al., 2016), as well as the adversarial autoen-
coder (AAE) (Makhzani et al., 2015), both with Gaussian
priors. We found that neither model was able to learn mean-
ingful latent representations—the VAE simply ignored the
latent code and the AAE experienced mode-collapse and
repeatedly generated the same samples.6 Appendix F in-
cludes detailed descriptions of the hyperparameters, model
architecture, and training regimes.

6. Experiments

6.1. Distributional Coverage
Section 4 argues that Pψ is trained to approximate the true
data distribution over discrete sequences P(cid:63). While it is
difﬁcult to test for this property directly (as is the case with
most GAN models), we can take samples from model to test
the ﬁdelity and coverage of the data space. Figure 2 shows
a set of samples from discretized MNIST and Appendix C
shows a set of generations from the text ARAE.

A common quantitative measure of sample quality for gener-
ative models is to evaluate a strong surrogate model trained
on its generated samples. While there are pitfalls of this
style of evaluation methods (Theis et al., 2016), it has pro-

6However there have been some recent successes training such

models, as noted in the related works section

Adversarially Regularized Autoencoders

Automatic Evaluation

Model

Transfer

BLEU Forward Reverse

Cross-Aligned AE
AE
ARAE, λ(1)
a
ARAE, λ(1)

b

77.1%
59.3%
73.4%
81.8%

17.75
37.28
31.15
20.18

65.9
31.9
29.7
27.7

124.2
68.9
70.1
77.0

Model

Transfer

Similarity Naturalness

Human Evaluation

Cross-Aligned AE
ARAE, λ(1)

b

57%
74%

3.8
3.7

2.7
3.8

Table 3: Sentiment transfer. (Top) Automatic metrics (Trans-
fer/BLEU/Forward PPL/Reverse PPL), (Bottom) Human evalua-
tion metrics (Transfer/Similarity/Naturalness). Cross-Aligned AE
is from Shen et al. (2017)

|

et al., 2015). For sentiment we follow the setup of Shen et al.
(2017) and split the Yelp corpus into two sets of unaligned
positive and negative reviews. We train ARAE with two
separate decoder RNNs, one for positive, p(x
z, y = 1),
|
and one for negative sentiment p(x
z, y = 0), and incorpo-
rate adversarial training of the encoder to remove sentiment
information from the prior. Transfer corresponds to encod-
ing sentences of one class and decoding, greedily, with the
opposite decoder. Experiments compare against the cross-
aligned AE of Shen et al. (2017) and also an AE trained
without the adversarial regularization. For ARAE, we exper-
imented with different λ(1) weighting on the adversarial loss
(see section 4) with λ(1)
b = 10. Both use λ(2) = 1.
Empirically the adversarial regularization enhances trans-
fer and perplexity, but tends to make the transferred text
less similar to the original, compared to the AE. Randomly
selected example sentences are shown in Table 2 and addi-
tional outputs are available in Appendix G.

a = 1, λ(1)

Table 3 (top) shows quantitative evaluation. We use four
automatic metrics: (i) Transfer: how successful the model
is at altering sentiment based on an automatic classiﬁer
(we use the fastText library (Joulin et al., 2017)); (ii)
BLEU: the consistency between the transferred text and the
original; (iii) Forward PPL: the ﬂuency of the generated
text; (iv) Reverse PPL: measuring the extent to which the
generations are representative of the underlying data distri-
bution. Both perplexity numbers are obtained by training
an RNN language model. Table 3 (bottom) shows human
evaluations on the cross-aligned AE and our best ARAE
model. We randomly select 1000 sentences (500/500 posi-
tive/negative), obtain the corresponding transfers from both
models, and ask crowdworkers to evaluate the sentiment
(Positive/Neutral/Negative) and naturalness (1-5, 5 being
most natural) of the transferred sentences. We create a sepa-
rate task in which we show the original and the transferred
sentences, and ask them to evaluate the similarity based on
sentence structure (1-5, 5 being most similar). We explicitly
requested that the reader disregard sentiment in similarity

Science
⇒ Music
⇒ Politics

Science
⇒ Music
⇒ Politics

Science
⇒ Music
⇒ Politics

Music
⇒ Science
⇒ Politics

Music
⇒ Science

⇒ Politics

Music
⇒ Science
⇒ Politics

Politics
⇒ Science
⇒ Music

Politics
⇒ Science
⇒ Music

Politics
⇒ Science
⇒ Music

what is an event horizon with regards to black holes ?
what is your favorite sitcom with adam sandler ?
what is an event with black people ?

take 1ml of hcl ( concentrated ) and dilute it to 50ml .
take em to you and shout it to me
take bribes to islam and it will be punished .

just multiply the numerator of one fraction by that of the other .
just multiply the fraction of the other one that &apos;s just like it .
just multiply the same fraction of other countries .

do you know a website that you can ﬁnd people who want to join bands ?
do you know a website that can help me with science ?
do you think that you can ﬁnd a person who is in prison ?

all three are fabulous artists , with just incredible talent ! !
all three are genetically bonded with water , but just as many substances ,
are capable of producing a special case .
all three are competing with the government , just as far as i can .

but there are so many more i can &apos;t think of !
but there are so many more of the number of questions .
but there are so many more of the can i think of today .

republicans : would you vote for a cheney / satan ticket in 2008 ?
guys : how would you solve this question ?
guys : would you rather be a good movie ?

4 years of an idiot in ofﬁce + electing the idiot again = ?
4 years of an idiot in the ofﬁce of science ?
4 ) <unk> in an idiot , the idiot is the best of the two points ever !

anyone who doesnt have a billion dollars for all the publicity cant win .
anyone who doesnt have a decent chance is the same for all the other .
anyone who doesnt have a lot of the show for the publicity .

Table 4: Topic Transfer. Random samples from the Yahoo dataset.
Note the ﬁrst row is from ARAE trained on titles while the follow-
ing ones are from replies.

Model

Medium Small

Tiny

Supervised Encoder
Semi-Supervised AE
Semi-Supervised ARAE

65.9%
68.5%
70.9%

62.5% 57.9%
64.6% 59.9%
66.8% 62.5%

Table 5: Semi-Supervised accuracy on the natural language infer-
ence (SNLI) test set, respectively using 22.2% (medium), 10.8%
(small), 5.25% (tiny) of the supervised labels of the full SNLI
training set (rest used for unlabeled AE training).

assessment.

The same method can be applied to other style transfer
tasks, for instance the more challenging Yahoo QA data
(Zhang et al., 2015). For Yahoo we chose 3 relatively dis-
tinct topic classes for transfer: SCIENCE & MATH, ENTER-
TAINMENT & MUSIC, and POLITICS & GOVERNMENT.
As the dataset contains both questions and answers, we sep-
arated our experiments into titles (questions) and replies
(answers). Randomly-selected generations are shown in Ta-
ble 4. See Appendix G for additional generation examples.

6.3. Semi-Supervised Training

Latent variable models can also provide an easy method
for semi-supervised training. We use a natural language in-
ference task to compare semi-supervised ARAE with other
training methods. Results are shown in Table 5. The full
SNLI training set contains 543k sentence pairs, and we use
supervised sets of 120k (Medium), 59k (Small), and 28k
(Tiny) and use the rest of the training set for unlabeled train-
ing. As a baseline we use an AE trained on the additional

Adversarially Regularized Autoencoders

Figure 3: Left: (cid:96)2 norm of encoder output z and generator output ˜z during ARAE training. (z is normalized, whereas the generator learns
to match). Middle: Sum of the dimension-wise variances of z and generator codes ˜z as well as reference AE. Right: Average cosine
similarity of nearby sentences (by word edit-distance) for the ARAE and AE during training.

AE

ARAE

Model

Samples

k

0
1
2
3
4

1.06
4.51
6.61
9.14
9.97

2.19
4.07
5.39
6.86
7.47

Original
Noised
AE
ARAE

Original
Noised
AE
ARAE

A woman wearing sunglasses
A woman sunglasses wearing
A woman sunglasses wearing sunglasses
A woman wearing sunglasses

Pets galloping down the street
Pets down the galloping street
Pets riding the down galloping
Pets congregate down the street near a ravine

Figure 4: Reconstruction error (negative log-likelihood averaged
over sentences) of the original sentence from a corrupted sentence.
Here k is the number of swaps performed on the original sentence.

data, similar to the setting explored in Dai & Le (2015).
For ARAE we use the subset of unsupervised data of length
< 15 (i.e. ARAE is trained on less data than AE for unsuper-
vised training). The results are shown in Table 5. Training
on unlabeled data with an AE objective improves upon a
model just trained on labeled data. Training with adversarial
regularization provides further gains.

7. Discussion

Impact of Regularization on Discrete Encoding We
further examine the impact of adversarial regularization
on the encoded representation produced by the model as
it is trained. Figure 3 (left), shows a sanity check that the
(cid:96)2 norm of encoder output z and prior samples ˜z converge
quickly in ARAE training. The middle plot compares the
trace of the covariance matrix between these terms as train-
ing progresses. It shows that variance of the encoder and
the prior match after several epochs.

Smoothness and Reconstruction We can also assess the
“smoothness” of the encoder model learned ARAE (Rifai
et al., 2011). We start with a simple proxy that a smooth
encoder model should map similar sentences to similar z
values. For 250 sentences, we calculate the average co-
sine similarity of 100 randomly-selected sentences within
an edit-distance of at most 5 to the original. The graph in
Figure 3 (right) shows that the cosine similarity of nearby
sentences is quite high for ARAE compared to a standard
AE and increases in early rounds of training. To further test
this property, we feed noised discrete input to the encoder
and (i) calculate the score given to the original input, and

(ii) compare the resulting reconstructions. Figure 4 (right)
shows results for text where k words are ﬁrst permuted in
each sentence. We observe that ARAE is able to map a
noised sentence to a natural sentence (though not necessar-
ily the denoised sentence). Figure 4 (left) shows empirical
results for these experiments. We obtain the reconstruction
error (negative log likelihood) of the original non-noised
sentence under the decoder, utilizing the noised code. We
ﬁnd that when k = 0 (i.e. no swaps), the regular AE better
reconstructs the exact input. However, as the number of
swaps pushes the input further away, ARAE is more likely
to produce the original sentence. (Note that unlike denois-
ing autoencoders which require a domain-speciﬁc noising
function (Hill et al., 2016; Vincent et al., 2008), the ARAE
is not explicitly trained to denoise an input.)

Manipulation through the Prior An interesting property
of latent variable models such as VAEs and GANs is the
ability to manipulate output samples through the prior. In
particular, for ARAE, the Gaussian form of the noise sam-
ple s induces the ability to smoothly interpolate between
outputs by exploiting the structure. While language models
may provide a better estimate of the underlying probability
space, constructing this style of interpolation would require
combinatorial search, which makes this a useful feature of
latent variable text models. In Appendix D we show inter-
polations from for the text model, while Figure 2 (bottom)
shows the interpolations for discretized MNIST ARAE.

A related property of GANs is the ability to move in the
latent space via offset vectors.9 To experiment with this
property we generate sentences from the ARAE and com-
pute vector transforms in this space to attempt to change
main verbs, subjects and modiﬁer (details in Appendix E).
Some examples of successful transformations are shown in
Figure 5 (bottom). Quantitative evaluation of the success of
the vector transformations is given in Figure 5 (top).

9Similar to the case with word vectors (Mikolov et al., 2013),
Radford et al. (2016) observe that when the mean latent vector
for “men with glasses” is subtracted from the mean latent vector
for “men without glasses” and applied to an image of a “woman
without glasses”, the resulting image is that of a “woman with
glasses”.

Adversarially Regularized Autoencoders

Transform

Match %

walking
man
two
dog
standing
several

85
92
86
88
89
70

Prec

79.5
80.2
74.1
77.0
79.3
67.0

A man in a tie is sleeping and clapping on balloons .
A man in a tie is clapping and walking dogs .

The jewish boy is trying to stay out of his skateboard .
The jewish man is trying to stay out of his horse .

Some child head a playing plastic with drink .
Two children playing a head with plastic drink .

The people shine or looks into an area .
The dog arrives or looks into an area .

A women are walking outside near a man .
Three women are standing near a man walking .

⇒walking

⇒man

⇒Two

⇒dog

⇒standing

A side child listening to a piece with steps playing on a table . ⇒Several
Several child playing a guitar on side with a table .

Figure 5: Top: Quantitative evaluation of transformations. Match
% refers to the % of samples where at least one decoder samples
(per 100) had the desired transformation in the output, while Prec.
measures the average precision of the output against the original
sentence. Bottom: Examples where the offset vectors produced
successful transformations of the original sentence. See Appendix
E for the full methodology.

8. Related Work

While ideally autoencoders would learn latent spaces which
compactly capture useful features that explain the observed
data, in practice they often learn a degenerate identity map-
ping where the latent code space is free of any structure,
necessitating the need for some regularization on the la-
tent space. A popular approach is to regularize through an
explicit prior on the code space and use a variational approx-
imation to the posterior, leading to a family of models called
variational autoencoders (VAE) (Kingma & Welling, 2014;
Rezende et al., 2014). Unfortunately VAEs for discrete text
sequences can be challenging to train—for example, if the
training procedure is not carefully tuned with techniques
like word dropout and KL annealing (Bowman et al., 2016),
the decoder simply becomes a language model and ignores
the latent code. However there have been some recent suc-
cesses through employing convolutional decoders (Yang
et al., 2017; Semeniuta et al., 2017), training the latent rep-
resentation as a topic model (Dieng et al., 2017; Wang et al.,
2018), using the von Mises–Fisher distribution (Guu et al.,
2017), and combining VAE with iterative inference (Kim
et al., 2018). There has also been some work on making
the prior more ﬂexible through explicit parameterization
(Chen et al., 2017; Tomczak & Welling, 2018). A notable
technique is adversarial autoencoders (AAE) (Makhzani
et al., 2015) which attempt to imbue the model with a more
ﬂexible prior implicitly through adversarial training. Recent
work on Wasserstein autoencoders (Tolstikhin et al., 2018)
provides a theoretical foundation for the AAE and shows
that AAE minimizes the Wasserstein distance between the
data/model distributions.

The success of GANs on images have led many researchers
to consider applying GANs to discrete data such as text.
Policy gradient methods are a natural way to deal with the
resulting non-differentiable generator objective when train-
ing directly in discrete space (Glynn, 1987; Williams, 1992).
When trained on text data however, such methods often re-
quire pre-training/co-training with a maximum likelihood
(i.e.
language modeling) objective (Che et al., 2017; Yu
et al., 2017; Li et al., 2017). Another direction of work
has been through reparameterizing the categorical distri-
bution with the Gumbel-Softmax trick (Jang et al., 2017;
Maddison et al., 2017)—while initial experiments were en-
couraging on a synthetic task (Kusner & Hernandez-Lobato,
2016), scaling them to work on natural language is a chal-
lenging open problem. There have also been recent related
approaches that work directly with the soft outputs from
a generator (Gulrajani et al., 2017; Rajeswar et al., 2017;
Shen et al., 2017; Press et al., 2017). For example, Shen
et al. (2017) exploits adversarial loss for unaligned style
transfer between text by having the discriminator act on the
RNN hidden states and using the soft outputs at each step
as input to an RNN generator. Our approach instead works
entirely in ﬁxed-dimensional continuous space and does not
require utilizing RNN hidden states directly. It is therefore
also different from methods that discriminate in the joint
latent/data space, such as ALI (Vincent Dumoulin, 2017)
and BiGAN (Donahue et al., 2017). Finally, our work adds
to the recent line of work on unaligned style transfer for
text (Hu et al., 2017; Mueller et al., 2017; Li et al., 2018;
Prabhumoye et al., 2018; Yang et al., 2018).

9. Conclusion

We present adversarially regularized autoencoders (ARAE)
as a simple approach for training a discrete structure au-
toencoder jointly with a code-space generative adversarial
network. Utilizing the Wasserstein autoencoder framework
(Tolstikhin et al., 2018), we also interpret ARAE as learning
a latent variable model that minimizes an upper bound on the
total variation distance between the data/model distributions.
We ﬁnd that the model learns an improved autoencoder and
exhibits a smooth latent space, as demonstrated by semi-
supervised experiments, improvements on text style transfer,
and manipulations in the latent space.

We note that (as has been frequently observed when training
GANs) the proposed model seemed to be quite sensitive to
hyperparameters, and that we only tested our model on sim-
ple structures such as binarized digits and short sentences.
Cífka et al. (2018) recently evaluated a suite of sentence
generation models and found that models are quite sensitive
to their training setup, and that different models do well
on different metrics. Training deep latent variable models
that can robustly model complex discrete structures (e.g.
documents) remains an important open issue in the ﬁeld.

Adversarially Regularized Autoencoders

Acknowledgements

We thank Sam Wiseman, Kyunghyun Cho, Sam Bowman,
Joan Bruna, Yacine Jernite, Martín Arjovsky, Mikael Henaff,
and Michael Mathieu for fruitful discussions. We are partic-
ularly grateful to Tianxiao Shen for providing the results for
style transfer. We also thank the NVIDIA Corporation for
the donation of a Titan X Pascal GPU that was used for this
research. Yoon Kim was supported by a gift from Amazon
AWS Machine Learning Research.

References

Arjovsky, M. and Bottou, L. Towards Principled Methods for
Training Generative Adversarial Networks. In Proceedings of
ICML, 2017.

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein GAN. In

Proceedings of ICML, 2017.

Bowman, S. R., Angeli, G., Potts, C., and Manning., C. D. A large
annotated corpus for learning natural language inference. In
Proceedings of EMNLP, 2015.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R.,
and Bengio, S. Generating Sentences from a Continuous Space.
2016.

Che, T., Li, Y., Zhang, R., Hjelm, R. D., Li, W., Song, Y., and
Bengio, Y. Maximum-Likelihood Augment Discrete Generative
Adversarial Networks. arXiv:1702.07983, 2017.

Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P.,
Schulman, J., Sutskever, I., and Abbeel, P. Variational Lossy
Autoencoder. In Proceedings of ICLR, 2017.

Cífka, O., Severyn, A., Alfonseca, E., and Filippova, K. Eval all,
trust a few, do wrong to none: Comparing sentence generation
models. arXiv:1804.07972, 2018.

Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. In

Proceedings of NIPS, 2015.

Denton, E. and Birodkar, V. Unsupervised learning of disentangled
representations from video. In Proceedings of NIPS, 2017.

Dieng, A. B., Wang, C., Gao, J., , and Paisley, J. TopicRNN: A
Recurrent Neural Network With Long-Range Semantic Depen-
dency. In Proceedings of ICLR, 2017.

Donahue, J., Krahenbühl, P., and Darrell, T. Adversarial Feature

Learning. In Proceedings of ICLR, 2017.

Glynn, P. Likelihood Ratio Gradient Estimation: An Overview. In

Proceedings of Winter Simulation Conference, 1987.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-
Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative
adversarial nets. In Proceedings of NIPS, 2014.

Gozlan, N. and Léonard, C. Transport Inequalities. A Survey.

arXiv:1003.3852, 2010.

Guu, K., Hashimoto, T. B., Oren, Y., and Liang, P. Generating
Sentences by Editing Prototypes. arXiv:1709.08878, 2017.

Hill, F., Cho, K., and Korhonen, A. Learning distributed represen-
tations of sentences from unlabelled data. In Proceedings of
NAACL, 2016.

Hjelm, R. D., Jacob, A. P., Che, T., Cho, K., and Bengio, Y.
Boundary-Seeking Generative Adversarial Networks. In Pro-
ceedings of ICLR, 2018.

Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., and Xing, E. P.
Controllable Text Generation. In Proceedings of ICML, 2017.

Hu, Z., Yang, Z., Salakhutdinov, R., and Xing, E. P. On Unifying

Deep Generative Models. In Proceedings of ICLR, 2018.

Jang, E., Gu, S., and Poole, B. Categorical Reparameterization

with Gumbel-Softmax. In Proceedings of ICLR, 2017.

Joulin, A., Grave, E., Bojanowski, P., and Mikolov, T. Bag of
Tricks for Efﬁcient Text Classiﬁcation. In Proceedings of ACL,
2017.

Kim, Y., Wiseman, S., Miller, A. C., Sontag, D., and Rush, A. M.
Semi-Amortized Variational Autoencoders. In Proceedings of
ICML, 2018.

Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes.

In Proceedings of ICLR, 2014.

Kusner, M. and Hernandez-Lobato, J. M. GANs for Sequences
of Discrete Elements with the Gumbel-Softmax Distribution.
arXiv:1611.04051, 2016.

Lample, G., Zeghidour, N., Usuniera, N., Bordes, A., Denoyer,
L., and Ranzato, M. Fader networks: Manipulating images by
sliding attributes. In Proceedings of NIPS, 2017.

Li, J., Monroe, W., Shi, T., Jean, S., Ritter, A., and Jurafsky,
D. Adversarial Learning for Neural Dialogue Generation. In
Proceedings of EMNLP, 2017.

Li, J., Jia, R., He, H., and Liang, P. Delete, Retrieve, Gener-
ate: A Simple Approach to Sentiment and Style Transfer. In
Proceedings of NAACL, 2018.

Maddison, C. J., Mnih, A., and Teh, Y. W. The Concrete Distribu-
tion: A Continuous Relaxation of Discrete Random Variables.
In Proceedings of ICLR, 2017.

Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey, B.

Adversarial Autoencoders. arXiv:1511.05644, 2015.

Mikolov, T., tau Yih, S. W., and Zweig, G. Linguistic Regularities
in Continuous Space Word Representations. In Proceedings of
NAACL, 2013.

Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral
Normalization For Generative Adversarial Networks. In Pro-
ceedings of ICLR, 2018.

Mueller, J., Gifford, D., and Jaakkola, T. Sequence to Better
Sequence: Continuous Revision of Combinatorial Structures.
In Proceedings of ICML, 2017.

Gulrajani, I., Ahmed, F., Arjovsky, M., and Vincent Dumoulin,
A. C. Improved Training of Wasserstein GANs. In Proceedings
of NIPS, 2017.

Prabhumoye, S., Tsvetkov, Y., Salakhutdinov, R., and Black, A. W.
Style Transfer Through Back-Translation. In Proceedings of
ACL, 2018.

Adversarially Regularized Autoencoders

Press, O., Bar, A., Bogin, B., Berant, J., and Wolf, L. Language
Generation with Recurrent Generative Adversarial Networks
without Pre-training. arXiv:1706.01399, 2017.

Radford, A., Metz, L., and Chintala, S. Unsupervised Representa-
tion Learning with Deep Convolutional Generative Adversarial
Networks. In Proceedings of ICLR, 2016.

Rajeswar, S., Subramanian, S., Dutil, F., Pal, C., and
Courville, A. Adversarial Generation of Natural Language.
arXiv:1705.10929, 2017.

Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic Back-
propagation and Approximate Inference in Deep Generative
Models. In Proceedings of ICML, 2014.

Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. Con-
tractive Auto-Encoders: Explicit Invariance During Feature
Extraction. In Proceedings of ICML, 2011.

Semeniuta, S., Severyn, A., and Barth, E. A Hybrid Convolutional
Variational Autoencoder for Text Generation. In Proceedings
of EMNLP, 2017.

Shen, T., Lei, T., Barzilay, R., and Jaakkola, T. Style Transfer
from Non-Parallel Text by Cross-Alignment. In Proceedings of
NIPS, 2017.

Theis, L., van den Oord, A., and Bethge, M. A note on the
evaluation of generative models. In Proceedings of ICLR, 2016.

Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. Wasser-

stein Auto-Encoders. In Proceedings of ICLR, 2018.

Tomczak, J. M. and Welling, M. VAE with a VampPrior.

In

Proceedings of AISTATS, 2018.

Villani, C. Optimal transport: old and new, volume 338. Springer

Science & Business Media, 2008.

Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Ex-
tracting and Composing Robust Features with Denoising Au-
toencoders. In Proceedings of ICML, 2008.

Vincent Dumoulin, Ishmael Belghazi, B. P. O. M. A. L. M. A.
A. C. Adversarially Learned Inference. In Proceedings of ICLR,
2017.

Wang, W., Gan, Z., Wang, W., Shen, D., Huang, J., Ping, W.,
Satheesh, S., and Carin, L. Topic Compositional Neural Lan-
guage Model. In Proceedings of AISTATS, 2018.

Williams, R. J. Simple Statistical Gradient-following Algorithms
for Connectionist Reinforcement Learning. Machine Learning,
8, 1992.

Yang, Z., Hu, Z., Salakhutdinov, R., and Berg-Kirkpatrick, T.
Improved Variational Autoencoders for Text Modeling using
Dilated Convolutions. In Proceedings of ICML, 2017.

Yang, Z., Hu, Z., Dyer, C., Xing, E. P., and Berg-Kirkpatrick, T.
Unsupervised Text Style Transfer using Language Models as
Discriminators. arXiv:1805.11749, 2018.

Yu, L., Zhang, W., Wang, J., and Yu, Y. SeqGAN: Sequence Gen-
erative Adversarial Nets with Policy Gradient. In Proceedings
of AAAI, 2017.

Zhang, X., Zhao, J., and LeCun, Y. Character-level Convolutional
Networks for Text Classiﬁcation. In Proceedings of NIPS, 2015.

Adversarially Regularized Autoencoders

A. Proof of Corollary 1

∈ X

is the

where

to the n

Corollary (Discrete case). Suppose x
X
set of all one-hot vectors of length n, and let fψ :
Z →
∆n−1 be a deterministic function that goes from the latent
1 dimensional simplex ∆n−1. Fur-
space
Z
ther let Gψ :
be a deterministic function such
that Gψ(z) = arg maxw∈X w(cid:62)fψ(z), and as above let
z) be the dirac distribution derived from Gψ such
Pψ(x
. Then the following is an
that pψ(x
}
upper bound on
TV, the total variation distance
between P(cid:63) and Pψ.

x = Gψ(z)
{
P(cid:63)(cid:107)
Pψ −
(cid:107)

−
Z → X

z) = 1

|

|

inf
Q(z | x):PQ=Pz

EP(cid:63) EQ(z | x)

(cid:104)

2
log 2

−

(cid:105)
log x(cid:62)fψ(z)

Proof. Let our cost function be c(x, y) = 1
ﬁrst note that for all x, z

x
{

= y

. We

}

input to the generator is from a simple distribution (e.g.
spherical Gaussian) and the generator possesses less capac-
ity than the encoder. However, it is not so simple that it is
overly restrictive (e.g. as in VAEs). Empirically we observe
that the ﬁrst and second moments do indeed converge as
training progresses (Section 7).
Proposition 1. Let P be a distribution on a compact set χ,
and (Pn)n∈N be a sequence of distributions on χ. Further
suppose that W (Pn, P)
0. Then the following statements
hold:

→

(i) Pn (cid:32) P (i.e. convergence in distribution).
(ii) All moments converge, i.e. for all k > 1, k

N,

∈

EX∼Pn

(cid:104) d
(cid:89)

i=1

(cid:105)

X pi
i

EX∼P

→

(cid:104) d
(cid:89)

i=1

(cid:105)

X pi
i

log 21
x
{

= arg max
w∈X

w(cid:62)fψ(z)
}

<

−

log x(cid:62)fψ(z)

for all p1, . . . , pd such that (cid:80)d

i=1 pi = k

x

= arg maxw∈X w(cid:62)fψ(z)
}

This holds since if 1
{
1, we have x(cid:62)fψ(z) < 0.5, and

=
log x(cid:62)fψ(z) >
If on the other hand x =
−
arg maxw∈X w(cid:62)fψ(z), then the LHS is 0 and RHS is al-
ways postive since fψ(z)

log 0.5 = log 2.

∆n−1. Then,

−

∈

2
log 2

log x(cid:62)fψ(z)]

w(cid:62)fψ(z)

]
}

x
{

x
{

= arg max
w∈X
= Gψ(z)
}

]

EP(cid:63) EQ(z | x)[

−
EP(cid:63) EQ(z | x)[21

EP(cid:63) EQ(z | x)[1

inf
Q:PQ=Pz

> inf

Q:PQ=Pz

=2

inf
Q:PQ=Pz

=2

inf
Q:PQ=Pz

= 2Wc(P(cid:63), Pψ)
=
P(cid:63) −

Pψ(cid:107)

(cid:107)

TV

EP(cid:63) EQ(z | x)[c(x, Gψ(z))]

The ﬁfth line follows from Theorem 1, and the last equality
uses the well-known correspondence between total varia-
tion distance and optimal transport with the indicator cost
function (Gozlan & Léonard, 2010).

B. Optimality Property

One can interpret the ARAE framework as a dual pathway
network mapping two distinct distributions into a similar
one; encφ and gθ both output code vectors that are kept
similar in terms of Wasserstein distance as measured by the
critic. We provide the following proposition showing that
under our parameterization of the encoder and the generator,
as the Wasserstein distance converges, the encoder distribu-
tion (PQ) converges to the generator distribution (Pz), and
further, their moments converge.

Proof. (i) has been proved in (Villani, 2008) Theorem 6.9.

For (ii), using The Portmanteau Theorem, (i) is equivalent
to the following statement:

EX∼Pn [f (X)]
uous function f : Rd
random variable.

→

→

EX∼P[f (X)] for all bounded and contin-
R, where d is the dimension of the

The k-th moment of a distribution is given by

(cid:104) d
(cid:89)

E

(cid:105)

X pi
i

i=1

such that

pi = k

d
(cid:88)

i=1

Our encoded code is bounded as we normalize the encoder
output to lie on the unit sphere, and our generated code is
1, 1)n by the tanh function. Hence
also bounded to lie in (
f (X) = (cid:81)d
is a bounded continuous function for all
qi ≥

i=1 X qi
i
0. Therefore,

−

EX∼Pn

(cid:104) d
(cid:89)

i=1

(cid:105)

X pi
i

EX∼P

→

(cid:104) d
(cid:89)

i=1

(cid:105)

X pi
i

where (cid:80)d

i=1 pi = k

C. Sample Generations

In Figure 6 we show some generated samples from the
ARAE, AE, and a LM.

D. Sentence Interpolations

This is ideal since under our setting the generated distribu-
tion is simpler than the encoded distribution, because the

In Figure 7 we show generations from interpolated latent
vectors. Speciﬁcally, we sample two points z0 and z1 from

Adversarially Regularized Autoencoders

ARAE Samples
A woman preparing three
ﬁsh .
A woman is seeing a man
in the river .
There passes a woman near
birds in the air .
Some ten people is sitting
through their ofﬁce .
The man got stolen with
young dinner bag .
Monks are running in court
.
The Two boys in glasses
are all girl .
The man is small sitting in
two men that tell a children
.
The two children are eating
the balloon animal .
A woman is trying on a
microscope .
The dogs are sleeping in
bed .

AE Samples
Two Three woman in a cart
tearing over of a tree .
A man is hugging and art .
The fancy skier is starting
under the drag cup in .
A dog are <unk> a
A man is not standing .
The Boys in their swim-
ming .
A surfer and a couple wait-
ing for a show .
A couple is a kids at a bar-
becue .
The motorcycles is in the
ocean loading
I ’s bike is on empty
The actor was walking in a
a small dog area .
no dog is young their
mother

LM Samples
a man walking outside on
a dirt road , sitting on the
dock .
A large group of people is
taking a photo for Christ-
mas and at night .
Someone is avoiding a soc-
cer game .
The man and woman are
dressed for a movie .
Person in an empty stadium
pointing at a mountain .
Two children and a little
boy are <unk> a man in a
blue shirt .
A boy rides a bicycle .
A girl is running another in
the forest .
the man is an indian women
.

Figure 6: Text samples generated from ARAE, a simple AE, and
from a baseline LM trained on the same data. To generate from an
AE we ﬁt a multivariate Gaussian to the learned code space and
generate code vectors from this Gaussian.

A man is on the corner in a
sport area .
A man is on corner in a road
all .
A lady is on outside a race-
track .
A lady is outside on a race-
track .
A lot of people is outdoors
in an urban setting .
A lot of people is outdoors
in an urban setting .
A lot of people is outdoors
in an urban setting .

A man is on a ship path with
the woman .
A man is on a ship path with
the woman .
A man is passing on a
bridge with the girl .
A man is passing on a
bridge with the girl .
A man is passing on a
bridge with the girl .
A man is passing on a
bridge with the dogs .
A man is passing on a
bridge with the dogs .

A man in a cave is used an
escalator .

A man in a cave is used
an escalator
A man in a cave is used
chairs .
A man in a number is used
many equipment
A man in a number is pos-
ing so on a big rock .
People are posing in a rural
area .
People are posing in a rural
area.

p(z) and construct intermediary points zλ = λz1 + (1
λ)z0. For each we generate the argmax output ˜xλ.

−

E. Vector Arithmetic

We generate 1 million sentences from the ARAE and parse
the sentences to obtain the main verb, subject, and modiﬁer.
Then for a given sentence, to change the main verb we sub-
tract the mean latent vector (t) for all other sentences with
the same main verb (in the ﬁrst example in Figure 5 this
would correspond to all sentences that had “sleeping” as the
main verb) and add the mean latent vector for all sentences
that have the desired transformation (with the running ex-
ample this would be all sentences whose main verb was
“walking”). We do the same to transform the subject and
the modiﬁer. We decode back into sentence space with the
transformed latent vector via sampling from pψ(g(z + t)).
Some examples of successful transformations are shown
in Figure 5 (right). Quantitative evaluation of the success
of the vector transformations is given in Figure 5 (left).
For each original vector z we sample 100 sentences from
pψ(g(z + t)) over the transformed new latent vector and

consider it a match if any of the sentences demonstrate the
desired transformation. Match % is proportion of original
vectors that yield a match post transformation. As we ideally
want the generated samples to only differ in the speciﬁed
transformation, we also calculate the average word precision
against the original sentence (Prec) for any match.

F. Experimental Details

MNIST experiments

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

The
is
encoder
784-800-400-100.

a

three-layer MLP,

Additive Gaussian noise is injected into c then gradu-
ally decayed to 0.

The
a
100-400-800-1000-784

decoder

is

four-layer

MLP,

The autoencoder is optimized by Adam, with learning
rate 5e-04.

An MLP generator 32-64-100-150-100.

An MLP critic 100-100-60-20-1 with weight clip-
ping (cid:15) = 0.05. The critic is trained 10 iterations in
every loop.

GAN is optimized by Adam, with learning rate 5e-04
on the generator and 5e-05 on the critic.

Weighing factor λ(1) = 0.2.

The encoder is an one-layer LSTM with 300 hidden
units.

Additive Gaussian noise is injected into c then gradu-
ally decayed to 0.

The decoder is an one-layer LSTM with 300 hidden
units.

The LSTM state vector is augmented by the hidden
code c at every decoding time step, before forwarding
into the output softmax layer.

The word embedding is of size 300.

The autoencoder is optimized by SGD with learning
rate 1. A grad clipping on the autoencoder, with max
grad_norm set to 1.

An MLP generator 100-300-300.

An MLP critic 300-300-1 with weight clipping (cid:15) =
0.01. The critic is trained 5 iterations in every loop.

GAN is optimized by Adam, with learning rate 5e-05
on the generator, and 1e-05 on the critic.

Figure 7: Sample interpolations from the ARAE. Constructed by
linearly interpolating in the latent space and decoding to the output
space. Word changes are highlighted in black.

Text experiments

Adversarially Regularized Autoencoders

Semi-supervised experiments

The following changes are made based on the SNLI experi-
ments:

•

•

•

an MLP
Larger network to GAN components:
generator 100-150-300-500 and an MLP critic
500-500-150-80-20-1 with weight clipping fac-
tor (cid:15) = 0.02.

Yelp/Yahoo transfer

An MLP style adversarial classiﬁer 300-200-100,
trained by SGD learning rate 0.1.

Weighing factor from both adversarial forces λ(1)
λ(1)
b = 10.

a = 1,

G. Style Transfer Samples

In the following pages we show randomly sampled style
transfers from the Yelp/Yahoo corpus.

Adversarially Regularized Autoencoders

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
Cross-AE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Yelp Sentiment Transfer

Positive to Negative

great indoor mall .
no smoking mall .
terrible outdoor urine .

great blooming onion .
no receipt onion .
terrible of pie .

i really enjoyed getting my nails done by peter .
i really needed getting my nails done by now .
i really really told my nails done with these things .

deﬁnitely a great choice for sushi in las vegas !
deﬁnitely a _num_ star rating for _num_ sushi in las vegas .
not a great choice for breakfast in las vegas vegas !

the best piece of meat i have ever had !
the worst piece of meat i have ever been to !
the worst part of that i have ever had had !

it has a great atmosphere , with wonderful service .
it has no taste , with a complete jerk .
it has a great horrible food and run out service .

their menu is extensive , even have italian food .
their menu is limited , even if i have an option .
their menu is decent , i have gotten italian food .

everyone who works there is incredibly friendly as well .
everyone who works there is incredibly rude as well .
everyone who works there is extremely clean and as well .

there are a couple decent places to drink and eat in here as well .
there are a couple slices of options and _num_ wings in the place .
there are a few night places to eat the car here are a crowd .

if you ’re in the mood to be adventurous , this is your place !
if you ’re in the mood to be disappointed , this is not the place .
if you ’re in the drive to the work , this is my place !

Negative to Positive

hell no !
hell great !
incredible pork !

highly disappointed !
highly recommended !
highly clean !

bad products .
good products .
good prices .

i was so very disappointed today at lunch .
i highly recommend this place today .
i was so very pleased to this .

i have n’t received any response to anything .
i have n’t received any problems to please .
i have always the desert vet .

small , smokey , dark and rude management .
small , intimate , and cozy friendly staff .
great , , , chips and wine .

the restaurant did n’t meet our standard though .
the restaurant did n’t disappoint our expectations though .
the restaurant is always happy and knowledge .

you could not see the stage at all !
you could see the difference at the counter !
you could deﬁnitely get the fuss !

room is void of all personality , no pictures or any sort of decorations .
room is eclectic , lots of ﬂavor and all of the best .
it ’s a nice that amazing , that one ’s some of ﬂavor .

waited in line to see how long a wait would be for three people .
waited in line for a long wait and totally worth it .
another great job to see and a lot going to be from dinner .

really good food , super casual and really friendly .
really bad food , really generally really low and decent food .
really good food , super horrible and not the price .

all the ﬁxes were minor and the bill ?
all the barbers were entertaining and the bill did n’t disappoint .
all the ﬂavors were especially and one !

we came on the recommendation of a bell boy and the food was amazing .
we came on the recommendation and the food was a joke .
we went on the car of the time and the chicken was awful .

the people who ordered off the menu did n’t seem to do much better .
the people who work there are super friendly and the menu is good .
the place , one of the ofﬁce is always worth you do a business .

service is good but not quick , just enjoy the wine and your company .
service is good but not quick , but the service is horrible .
service is good , and horrible , is the same and worst time ever .

the steak was really juicy with my side of salsa to balance the ﬂavor .
the steak was really bland with the sauce and mashed potatoes .
the ﬁsh was so much , the most of sauce had got the ﬂavor .

other than that one hell hole of a star bucks they ’re all great !
other than that one star rating the toilet they ’re not allowed .
a wonder our one came in a _num_ months , you ’re so better !

they told us in the beginning to make sure they do n’t eat anything .
they told us in the mood to make sure they do great food .
they ’re us in the next for us as you do n’t eat .

the person who was teaching me how to control my horse was pretty rude .
the person who was able to give me a pretty good price .
the owner ’s was gorgeous when i had a table and was friendly .

he was cleaning the table next to us with gloves on and a rag .
he was prompt and patient with us and the staff is awesome .
he was like the only thing to get some with with my hair .

Figure 8: Full sheet of sentiment transfer result on the Yelp corpus.

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Adversarially Regularized Autoencoders

Yahoo Topic Transfer on Questions

from Science

from Music

from Politics

Original

Music

what is an event horizon with regards to black holes
?
what is your favorite sitcom with adam sandler ?

Original

Science

republicans : would you vote for a cheney / satan
ticket in 2008 ?
guys : how would you solve this question ?

Original

Science

Politics

do you know a website that you can ﬁnd people
who want to join bands ?
do you know a website that can help me with sci-
ence ?
do you think that you can ﬁnd a person who is in
prison ?

Politics

what is an event with black people ?

Music

guys : would you rather be a good movie ?

Original

Music

what did john paul jones do in the american revolu-
tion ?
what did john lennon do in the new york family ?

Original

Science

Politics

what did john mccain do in the next election ?

Politics

Original

Music

Politics

can anybody suggest a good topic for a statistical
survey ?
can anybody suggest a good site for a techno ?

can anybody suggest a good topic for a student visa
?

Original

Music

can a kidney infection effect a woman &apos;s
<unk> cycle ?
can anyone give me a good ﬁlm <unk> ?

Politics

can a landlord ofﬁcer have a <unk> <unk> ?

Original

Music

Politics

Original

Music

Politics

sweating <unk>

where does the term &quot;
&quot; come from ?
where does the term &quot; <unk> &quot; come
from ?
where does the term &quot; <unk> &quot; come
from ?

what other <unk> sources are there than burning
fossil fuels .
what other <unk> are / who are the greatest gui-
tarist currently on tv today ?
what other <unk> are there for veterans who lives
?

Original

Science

Politics

Original

Science

Politics

Original

Science

Politics

Original

Science

Politics

do people who quote entire poems or song lyrics
ever actually get chosen best answer ?
do you think that scientists learn about human
anatomy and physiology of life ?
do people who knows anything about the recent is-
sue of <unk> leadership ?

from big brother , what is the girls name who had
<unk> in her apt ?
in big bang what is the <unk> of <unk> , what is
the difference between <unk> and <unk> ?
is big brother in the <unk> what do you think of
her ?

where is the tickets for the ﬁlming of the suite life
of zack and cody ?
where is the best place of the blood stream for the
production of the cell ?
where is the best place of the navy and the senate
of the union ?

the <unk> singers was a band in 1963 who had a
hit called <unk> man ?
the <unk> river in a <unk> was created by a <unk>
who was born in the last century ?
the <unk> are <unk> in a <unk> who was shot an
<unk> ?

Original

Science

Music

Original

Science

Music

Original

Science

Music

Original

Science

Music

if i move to the usa do i lose my pension in canada
?
if i move the <unk> in the air i have to do my math
homework ?
if i move to the music do you think i feel better ?

what is your reﬂection on what will be our organi-
zations in the future ?
what is your opinion on what will be the future in
our future ?
what is your favorite music videos on the may i ﬁnd
?

wouldn &apos;t it be fun if we the people veto or
passed bills ?
isnt it possible to be cloned if we put the moon or
it ?
isnt it possible or if we &apos;re getting married ?

can anyone tell me how i could go about interview-
ing north vietnamese soldiers ?
can anyone tell me how i could ﬁnd how to build a
robot ?
can anyone tell me how i could ﬁnd out about my
parents ?

what is the ﬁrst metal band in the early 60 &apos;s
..... ? ? ? ?
what is the ﬁrst country in the universe ?

Original

Science

if the us did not exist would the world be a better
place ?
if the world did not exist , would it be possible ?

who is the ﬁrst president in the usa ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?

Music

if you could not have a thing who would it be ?

Figure 9: Full sheet of Yahoo topic transfer on titles.

Adversarially Regularized Autoencoders

Yahoo Topic Transfer on Answers

from Science

from Music

from Politics

Original

Music

take 1ml of hcl ( concentrated ) and dilute it to 50ml
.
take em to you and shout it to me

Original

Science

Politics

take bribes to islam and it will be punished .

Politics

Original

oils do not do this , they do not &quot; set &quot; .

Original

Music

Politics

cucumbers do not do this , they do not &quot; do
&quot; .
corporations do not do this , but they do not .

Science

Politics

all three are fabulous artists , with just incredible
talent ! !
all three are genetically bonded with water , but just
as many substances , are capable of producing a
special case .
all three are competing with the government , just
as far as i can .

Original

Science

4 years of an idiot in ofﬁce + electing the idiot again
= ?
4 years of an idiot in the ofﬁce of science ?

Music

4 ) <unk> in an idiot , the idiot is the best of the
two points ever !

she , too , wondered about the underwear outside
the clothes .
she , too , i know , the clothes outside the clothes .

Original

Science

send me $ 100 and i &apos;ll send you a copy -
honest .
send me an email and i &apos;ll send you a copy .

she , too , i think that the cops are the only thing
about the outside of the u.s. .

Music

send me $ 100 and i &apos;ll send you a copy .

Original

Music

Politics

the average high temps in jan and feb are about 48
deg .
the average high school in seattle and is about 15
minutes .
the average high infantry division is in afghanistan
and alaska .

Original

Science

Politics

i like rammstein and i don &apos;t speak or under-
stand german .
i like googling and i don &apos;t understand or
speak .
i like mccain and i don &apos;t care about it .

Original

Science

Music

wills can be <unk> , or typed and signed without
needing an attorney .
euler can be <unk> , and without any type of oper-
ations , or <unk> .
madonna can be <unk> , and signed without open-
ing or <unk> .

Original

Music

the light from you lamps would move away from
you at light speed
the light from you tube would move away from you

Science

Original

mark is great , but the guest hosts were cool too !

Original

hungary : 20 january 1945 , ( formerly a member
of the axis )
nh3 : 20 january , 78 ( a )

Science

Politics

the light from you could go away from your state

Politics

Music

1966 - 20 january 1961 ( a ) 1983 song

mark is great , but the water will be too busy for the
same reason .
mark twain , but the great lakes , the united states
of america is too busy .

they all offer terriﬁc information about the cast and
characters , ...
they all offer insight about the characteristics of the
earth , and are composed of many stars .
they all offer legitimate information about the inva-
sion of iraq and the u.s. , and all aspects of history
.

Original

Music

Politics

van <unk> , on the other hand , had some serious
issues ...
van <unk> on the other hand , had some serious
issues .
van <unk> , on the other hand , had some serious
issues .

Original

Science

Politics

Original

Science

bulgaria : 8 september 1944 , ( formerly a member
of the axis )
moreover , 8 ˆ3 + ( x + 7 ) ( x ˆ2 ) = ( a ˆ2 )

Music

harrison : 8 september 1961 ( a ) ( 1995 )

Original

Music

Politics

just multiply the numerator of one fraction by that
of the other .
just multiply the fraction of the other one that
&apos;s just like it .
just multiply the same fraction of other countries .

Original

Music

civil engineering is still an umbrella ﬁeld com-
prised of many related specialties .
civil rights is still an art union .

Politics

civil law is still an issue .

Original

but there are so many more i can &apos;t think of !

Original

Science

Politics

but there are so many more of the number of ques-
tions .
but there are so many more of the can i think of
today .

Science

Music

Original

i love zach he is sooo sweet in his own way !

Original

Science

Politics

the answer is he &apos;s deﬁnitely in his own way
!
i love letting he is sooo smart in his own way !

Science

Music

anyone who doesnt have a billion dollars for all the
publicity cant win .
anyone who doesnt have a decent chance is the
same for all the other .
anyone who doesnt have a lot of the show for the
publicity .

the theory is that cats don &apos;t take to being tied
up but thats <unk> .
the theory is that cats don &apos;t grow up to
<unk> .
the theory is that dumb but don &apos;t play <unk>
to <unk> .

Original

Music

Politics

Original

Music
Politics

Original

Music

Politics

h2o2 ( hydrogen peroxide ) naturally decomposes
to form o2 and water .
jackie and brad pitt both great albums and they are
my fav .
kennedy and blair hate america to invade them .

Original

Science

Politics

remember the industry is very shady so keep your
eyes open !
remember the amount of water is so very important
.
remember the amount of time the politicians are
open your mind .

Original

Science

Music

the fear they are trying to instill in the common man
is based on what ?
the fear they are trying to ﬁnd the common ancestor
in the world .
the fear they are trying to ﬁnd out what is wrong in
the song .

the quieter it gets , the more white noise you can
here .
the fray it gets , the more you can hear .
the gop gets it , the more you can here .

Original

but can you fake it , for just one more show ?

Original

Science
Politics

but can you fake it , just for more than one ?
but can you fake it for more than one ?

Science
Music

h2co3 ( carbonic acid ) naturally decomposes to
form water and co2 .
phoebe and jack , he &apos;s gorgeous and she
loves to get him !
nixon ( captured ) he lied and voted for bush to
cause his country .

Original

Science

i am going to introduce you to the internet movie
database .
i am going to investigate the internet to google .

Original

Science

Politics

i am going to skip the internet to get you checked .

Music

think about how much planning and people would
have to be involved in what happened .
think about how much time would you have to do .
think about how much money and what would be
<unk> about in the world ?

this restricts the availability of cash to them and
other countries too start banning them .
this reduces the intake of the other molecules to pro-
duce them and thus are too large .
this is the cheapest package of them too .

Figure 10: Full sheet of Yahoo topic transfer on answers.

Adversarially Regularized Autoencoders

8
1
0
2
 
n
u
J
 
9
2
 
 
]

G
L
.
s
c
[
 
 
3
v
3
2
2
4
0
.
6
0
7
1
:
v
i
X
r
a

Jake (Junbo) Zhao * 1 2 Yoon Kim * 3 Kelly Zhang 1 Alexander M. Rush 3 Yann LeCun 1 2

Abstract
Deep latent variable models, trained using varia-
tional autoencoders or generative adversarial net-
works, are now a key technique for representa-
tion learning of continuous structures. However,
applying similar methods to discrete structures,
such as text sequences or discretized images, has
proven to be more challenging. In this work, we
propose a ﬂexible method for training deep latent
variable models of discrete structures. Our ap-
proach is based on the recently-proposed Wasser-
stein autoencoder (WAE) which formalizes the ad-
versarial autoencoder (AAE) as an optimal trans-
port problem. We ﬁrst extend this framework to
model discrete sequences, and then further ex-
plore different learned priors targeting a control-
lable representation. This adversarially regular-
ized autoencoder (ARAE) allows us to generate
natural textual outputs as well as perform manipu-
lations in the latent space to induce change in the
output space. Finally we show that the latent rep-
resentation can be trained to perform unaligned
textual style transfer, giving improvements both in
automatic/human evaluation compared to existing
methods.

1. Introduction
Recent work on deep latent variable models, such as vari-
ational autoencoders (Kingma & Welling, 2014) and gen-
erative adversarial networks (Goodfellow et al., 2014), has
shown signiﬁcant progress in learning smooth representa-
tions of complex, high-dimensional continuous data such as
images. These latent variable representations facilitate the
ability to apply smooth transformations in latent space in or-
der to produce complex modiﬁcations of generated outputs,
while still remaining on the data manifold.

Unfortunately, learning similar latent variable models of

*Equal contribution 1Department of Computer Science, New
York University 2Facebook AI Research 3School of Engineering
and Applied Sciences, Harvard University. Correspondence to:
Jake Zhao <jakezhao@cs.nyu.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

discrete structures, such as text sequences or discretized
images, remains a challenging problem. Initial work on
VAEs for text has shown that optimization is difﬁcult, as the
generative model can easily degenerate into a unconditional
language model (Bowman et al., 2016). Recent work on
generative adversarial networks (GANs) for text has mostly
focused on dealing with the non-differentiable objective
either through policy gradient methods (Che et al., 2017;
Hjelm et al., 2018; Yu et al., 2017) or with the Gumbel-
Softmax distribution (Kusner & Hernandez-Lobato, 2016).
However, neither approach can yet produce robust represen-
tations directly.

In this work, we extend the adversarial autoencoder (AAE)
(Makhzani et al., 2015) to discrete sequences/structures.
Similar to the AAE, our model learns an encoder from an
input space to an adversarially regularized continuous latent
space. However unlike the AAE which utilizes a ﬁxed
prior, we instead learn a parameterized prior as a GAN.
Like sequence VAEs, the model does not require using
policy gradients or continuous relaxations. Like GANs,
the model provides ﬂexibility in learning a prior through a
parameterized generator.

This adversarially regularized autoencoder (ARAE) can fur-
ther be formalized under the recently-introduced Wasser-
stein autoencoder (WAE) framework (Tolstikhin et al.,
2018), which also generalizes the adversarial autoencoder.
This framework connects regularized autoencoders to an
optimal transport objective for an implicit generative model.
We extend this class of latent variable models to the case of
discrete output, speciﬁcally showing that the autoencoder
cross-entropy loss upper-bounds the total variational dis-
tance between the model/data distributions. Under this
setup, commonly-used discrete decoders such as RNNs, can
be incorporated into the model. Finally to handle non-trivial
sequence examples, we consider several different (ﬁxed
and learned) prior distributions. These include a standard
Gaussian prior used in image models and in the AAE/WAE
models, a learned parametric generator acting as a GAN in
latent variable space, and a transfer-based parametric gener-
ator that is trained to ignore targeted attributes of the input.
The last prior can be directly used for unaligned transfer
tasks such as sentiment or style transfer.

Experiments apply ARAE to discretized images and text

Adversarially Regularized Autoencoders

sequences. The latent variable model is able to gener-
ate varied samples that can be quantitatively shown to
cover the input spaces and to generate consistent image
and sentence manipulations by moving around in the la-
tent space via interpolation and offset vector arithmetic.
When the ARAE model is trained with task-speciﬁc ad-
versarial regularization, the model improves upon strong
results on sentiment transfer reported in Shen et al. (2017)
and produces compelling outputs on a topic transfer task
using only a single shared space. Code is available at
https://github.com/jakezhaojb/ARAE.

X

=

2. Background and Notation
n to be a set of
Discrete Autoencoder Deﬁne
discrete sequences where
is a vocabulary of symbols.
V
Our discrete autoencoder will consist of two parameterized
functions: a deterministic encoder function encφ :
X (cid:55)→ Z
with parameters φ that maps from input space to code space,
and a conditional decoder pψ(x
with
parameters ψ. The parameters are trained based on the
cross-entropy reconstruction loss:

z) over structures

X

V

|

rec(φ, ψ) =

log pψ(x

encφ(x))

−

|

L

The choice of the encoder and decoder parameterization is
problem-speciﬁc, for example we use RNNs for sequences.
encφ(x)) for the
We use the notation, ˆx = arg maxx pψ(x
decoder mode, and call the model distribution Pψ.

|

Generative Adversarial Networks GANs are a class of
parameterized implicit generative models (Goodfellow et al.,
2014). The method approximates drawing samples from
a true distribution z
P∗ by instead employing a noise
sample s and a parameterized generator function ˜z = gθ(s)
to produce ˜z
Pz. Initial work on GANs implicitly min-
imized the Jensen-Shannon divergence between the distri-
butions. Recent work on Wasserstein GAN (WGAN) (Ar-
jovsky et al., 2017), replaces this with the Earth-Mover
(Wasserstein-1) distance.

∼

∼

GAN training utilizes two separate models: a generator
gθ(s) maps a latent vector from some easy-to-sample noise
distribution to a sample from a more complex distribution,
and a critic/discriminator fw(z) aims to distinguish real data
and generated samples from gθ. Informally, the generator
is trained to fool the critic, and the critic to tell real from
generated. WGAN training uses the following min-max
optimization over generator θ and critic w,

min
θ

max
w∈W

Ez∼P∗ [fw(z)]

E˜z∼Pz [fw(˜z)],

−

Z (cid:55)→

where fw :
R denotes the critic function, ˜z is ob-
tained from the generator, ˜z = gθ(s), and P∗ and Pz are
real and generated distributions. If the critic parameters w
are restricted to an 1-Lipschitz function set
, this term cor-
respond to minimizing Wasserstein-1 distance W (P∗, Pz).

W

We use a naive approximation to enforce this property by
(cid:15), (cid:15)]d (Arjovsky et al., 2017).1
weight-clipping, i.e. w = [

−
3. Adversarially Regularized Autoencoder
ARAE combines a discrete autoencoder with a GAN-
regularized latent representation. The full model is shown
in Figure 1, which produces a learned distribution over the
discrete space Pψ. Intuitively, this method aims to provide
smoother hidden encoding for discrete sequences with a
ﬂexible prior. In the next section we show how this sim-
ple network can be formally interpreted as a latent variable
model under the Wasserstein autoencoder framework.

The model consists of a discrete autoencoder regularized
with a prior distribution,

min
φ,ψ

L

rec(φ, ψ) + λ(1)W (PQ, Pz)

Here W is the Wasserstein distance between PQ, the distri-
bution from a discrete encoder model (i.e. encφ(x) where
P(cid:63)), and Pz, a prior distribution. As above, the W func-
x
tion is computed with an embedded critic function which is
optimized adversarially to the generator and encoder.2

∼

The model is trained with coordinate descent across: (1)
the encoder and decoder to minimize reconstruction, (2) the
critic function to approximate the W term, (3) the encoder
adversarially to the critic to minimize W :

1) min
φ,ψ

2) max
w∈W
3) min

φ

Lrec(φ, ψ) = Ex∼P(cid:63) [− log pψ(x | encφ(x))]

Lcri(w) = Ex∼P(cid:63) [fw(encφ(x))] − E˜z∼Pz [fw(˜z)]

Lenc(φ) = Ex∼P(cid:63) [fw(encφ(x))] − E˜z∼Pz [fw(˜z)]

The full training algorithm is shown in Algorithm 1.
Empirically we found that the choice of the prior distribu-
tion Pz strongly impacted the performance of the model.
The simplest choice is to use a ﬁxed distribution such as
a Gaussian
(0, I), which yields a discrete version of the
adversarial autoencoder (AAE). However in practice this
choice is seemingly too constrained and suffers from mode-
collapse.3

N

Instead we exploit the adversarial setup and use learned
prior parameterized through a generator model. This is
analogous to the use of learned priors in VAEs (Chen et al.,
2017; Tomczak & Welling, 2018). Speciﬁcally we introduce
(0, I) to act as an
a generator model, gθ(s) over noise s

∼ N

1While we did not experiment with enforcing the Lipschitz
constraint via gradient penalty (Gulrajani et al., 2017) or spectral
normalization (Miyato et al., 2018), other researchers have found
slight improvements by training ARAE with the gradient-penalty
version of WGAN (private correspondence).

2Other GANs could be used for this optimization. Experimen-
tally we found that WGANs to be more stable than other models.
3We note that recent work has successfully utilized AAE for

text by instead employing a spherical prior (Cífka et al., 2018).

Adversarially Regularized Autoencoders

where we want to change an attribute of a discrete input
without aligned examples, e.g. to change the topic or senti-
ment of a sentence. Deﬁne this attribute as y and redeﬁne
the decoder to be conditional pψ(x

z, y).

|

To adapt ARAE to this setup, we modify the objective to
learn to remove attribute distinctions from the prior (i.e.
we want the prior to encode all the relevant information
except about y). Following similar techniques from other
domains, notably in images (Lample et al., 2017) and video
modeling (Denton & Birodkar, 2017), we introduce a latent
space attribute classiﬁer:

min
φ,ψ,θ L

rec(φ, ψ) + λ(1)W (PQ, Pz)

λ(2)

class(φ, u)

−

L

L

where
z) from
class(φ, u) is the loss of a classiﬁer pu(y
latent variable to labels (in our experiments we always set
λ(2) = 1). This requires two more update steps: (2b) train-
ing the classiﬁer, and (3b) adversarially training the encoder
to this classiﬁer. This algorithm is shown in Algorithm 2.

|

4. Theoretical Properties

Standard GANs implicitly minimize a divergence measure
(e.g. f -divergence or Wasserstein distance) between the
true/model distributions. In our case however, we implicitly
minimize the divergence between learned code distributions,
and it is not clear if this training objective is matching the
distributions in the original discrete space. Tolstikhin et al.
(2018) recently showed that this style of training is minimiz-
ing the Wasserstein distance between the data distribution
P(cid:63) and the model distribution Pψ with latent variables (with
density pψ(x) = (cid:82)
In this section we apply the above result to the discrete case
and show that the ARAE loss minimizes an upper bound on
the total variation distance between P(cid:63) and Pψ.
Deﬁnition 1 (Kantorovich’s formulation of optimal trans-
port). Let P(cid:63), Pψ be distributions over
, and further let
R+ be a cost function. Then the optimal
c(x, y) :
transport (OT) problem is given by

z) p(z) dz).

X ×X →

z pψ(x

X

|

Wc(P(cid:63), Pψ) =

inf
Γ∈P(x∼P(cid:63),y∼Pψ)

Ex,y∼Γ[c(x, y)]

(x

where
∼
tions of (x, y) with marginals P(cid:63) and Pψ.

P(cid:63), y

∼

P

Pψ) is the set of all joint distribu-

1
p

y

−

∼

Pz, x

Pψ(x

p
p then Wc(P(cid:63), Pψ)
(cid:107)

In particular, if c(x, y) =
x
(cid:107)
is the Wasserstein-p distance between P(cid:63) and Pψ. Now
suppose we utilize a latent variable model to ﬁt the data, i.e.
z). Then Tolstikhin et al. (2018) prove
z
∼
the following theorem:
Theorem 1. Let Gψ :
tion (parameterized by ψ) from the latent space
space

be a deterministic func-
to data
z) on
x) be

Z
that induces a dirac distribution Pψ(x
. Let Q(z
}

z) = 1
{

X
, i.e. pψ(x

x = Gψ(z)

Z → X

X

|
|

|

|

Figure 1: ARAE architecture. A discrete sequence x is encoded
and decoded to produce ˆx. A noise sample s is passed though a
generator gθ (possibly the identity) to produce a prior. The critic
function fw is only used at training to enforce regularization W .
The model produce discrete samples x from noise s. Section 5
relates these samples x ∼ Pψ to x ∼ P(cid:63).
Algorithm 1 ARAE Training

for each training iteration do

(1) Train the encoder/decoder for reconstruction (φ, ψ)
i=1 ∼ P(cid:63) and compute z(i) = encφ(x(i))
Sample {x(i)}m
Backprop loss, Lrec = − 1
m

i=1 log pψ(x(i) | z(i))

(cid:80)m

i=1 ∼ P(cid:63) and {s(i)}m

(2) Train the critic (w)
Sample {x(i)}m
Compute z(i) = encφ(x(i)) and ˜z(i) = gθ(z(i))
(cid:80)m
Backprop loss − 1
m
Clip critic w to [−(cid:15), (cid:15)]d.

i=1 fw(z(i)) + 1

(cid:80)m

i=1 ∼ N (0, I)

m

i=1 fw(˜z(i))

(3) Train the encoder/generator adversarially (φ, θ)
i=1 ∼ P(cid:63) and {s(i)}m
Sample {x(i)}m
Compute z(i) = encφ(x(i)) and ˜z(i) = gθ(s(i)).
Backprop loss 1
m

i=1 fw(z(i)) − 1

i=1 ∼ N (0, I)

i=1 fw(˜z(i))

(cid:80)m

(cid:80)m

m

end for

implicit prior distribution Pz.4 We optimize its parameters
θ as part of training in Step 3.

Algorithm 2 ARAE Transfer Extension

Each loop additionally:

(2b) Train attribute classiﬁer (u)
Sample
encφ(x(i))
Backprop loss

m
i=1 ∼
1
m

x(i)
{

i=1 log pu(y(i)

(cid:80)m

}

z(i))

|

P(cid:63), lookup y(i), and compute z(i) =

(3b) Train the encoder adversarially (φ)
Sample
encφ(x(i))
Backprop loss

m
i=1 ∼
1
m

i=1 log pu(1

x(i)
{

(cid:80)m

y(i)

}

P(cid:63), lookup y(i), and compute z(i) =

z(i))

−

|

−

−

Extension: Unaligned Transfer Regularization of the la-
tent space makes it more adaptable for direct continuous
optimization that would be difﬁcult over discrete sequences.
For example, consider the problem of unaligned transfer,

4The downside of this approach is that the latent variable z is
now much less constrained. However we ﬁnd experimentally that
using a a simple MLP for gθ signiﬁcantly regularizes the encoder
RNN.

Adversarially Regularized Autoencoders

x).
any conditional distribution on
Deﬁne its marginal to be PQ, which has density pQ(x) =
(cid:82)
x pQ(z

x) p(cid:63)(x)dx. Then,

with density pQ(z

Z

|

|
Wc(P(cid:63), Pψ) =

inf
Q(z | x):PQ=Pz

EP(cid:63) EQ(z | x)[c(x, Gψ(z))]

Theorem 1 essentially says that learning an autoencoder can
be interpreted as learning a generative model with latent vari-
ables, as long as we ensure that the marginalized encoded
space is the same as the prior. This provides theoretical justi-
ﬁcation for adversarial autoencoders (Makhzani et al., 2015),
and Tolstikhin et al. (2018) used the above to train deep gen-
erative models of images by minimizing the Wasserstein-2
distance (i.e. squared loss between real/generated images).
We now apply Theorem 1 to discrete autoencoders trained
with cross-entropy loss.
Corollary 1 (Discrete case). Suppose x
X
is the set of all one-hot vectors of length n, and let fψ :
∆n−1 be a deterministic function that goes from the
Z →
1 dimensional simplex ∆n−1.
latent space
−
Z
Further let Gψ :
be a deterministic function such
that Gψ(z) = arg maxw∈X w(cid:62)fψ(z), and as above let
z) be the dirac distribution derived from Gψ such
Pψ(x
. Then the following is an
that pψ(x
}
upper bound on
TV, the total variation distance
between P(cid:63) and Pψ:

x = Gψ(z)
{
P(cid:63)(cid:107)
Pψ −

to the n

z) = 1

Z → X

where

∈ X

(cid:107)

|

|

inf
Q(z | x):PQ=Pz

EP(cid:63) EQ(z | x)

(cid:104)

2
log 2

−

(cid:105)
log x(cid:62)fψ(z)

|V|

m and therefore

The proof is in Appendix A. For natural language we have
is the set of sentences of length
n =
m, where m is the maximum sentence length (shorter sen-
tences are padded if necessary). Then the total variational
(TV) distance is given by

X

Pψ −

(cid:107)

P(cid:63)(cid:107)

TV =

pψ(x)

p(cid:63)(x)
|

−

1
2

(cid:88)
x∈Vm |

|

−

−

log pψ(x

This is an interesting alternative to the usual maximum
likelihood approach which instead minimizes KL(P(cid:63), Pψ).5
log x(cid:62)fψ(z) =
It is also clear that
z), the
standard autoencoder cross-entropy loss at the sentence level
with fψ as the decoder. As the above objective is hard to
minimize directly, we follow Tolstikhin et al. (2018) and
consider an easier objective by (i) restricting Q(z
x) to a
family of distributions induced by a deterministic encoder
parameterized by φ, and (ii) using a Langrangian relaxation
of the constraint PQ = Pz. In particular, letting Q(z
x) =
1
be the dirac distribution induced by a
deterministic encoder (with associated marginal Pφ), the
objective is given by

z = encφ(x)
{

}

|

|

5The relationship between KL-divergence and total variation
distance is also given by Pinsker’s inquality, which states that
2(cid:107)Pψ − P(cid:63)(cid:107)2

TV ≤ KL(P(cid:63), Pψ).

min
φ,ψ

EP(cid:63) [

−

|

log pψ(x

encφ(z))] + λW (Pφ, Pz)

Note that our minimizing the Wasserstein distance in the
latent space W (Pφ, Pz) is independent from the Wassertein
distance minimization in the output space in WAEs. Finally,
instead of using a ﬁxed prior (which led to mode-collapse
in our experiments) we parameterize Pz implicitly by trans-
forming a simple random variable with a generator (i.e.
(0, I), z = gθ(s)). This recovers the ARAE objec-
s
tive from the previous section.

∼ N

We conclude this section by noting that while the theoretical
formalization of the AAE as a latent variable model was an
important step, in practice there are many approximations
made to the actual optimal transport objective. Meaning-
fully quantifying (and reducing) such approximation gaps
remains an avenue for future work.
5. Methods and Architectures

We experiment with ARAE on three setups: (1) a small
model using discretized images trained on the binarized
version of MNIST, (2) a model for text sequences trained
on the Stanford Natural Language Inference (SNLI) corpus
(Bowman et al., 2015), and (3) a model trained for text
transfer trained on the Yelp/Yahoo datasets for unaligned
sentiment/topic transfer. For experiments using a learned
prior, the generator architecture uses a low dimensional s
with a Gaussian prior s
(0, I), and maps it to z using
an MLP gθ. The critic fw is also parameterized as an MLP.

∼ N

=

0, 1
{

The image model encodes/decodes binarized images. Here
n where n is the image size. The encoder
}
X
Rm, encφ(x) =
used is an MLP mapping from
0, 1
}
{
MLP(x; φ) = z. The decoder predicts each pixel in x
with as a parameterized logistic regression, pψ(x
z) =
(cid:81)n

|
σ(h))1−xj where h = MLP(z; ψ).

j=1 σ(h)xj (1

(cid:55)→

n

−

V

X

=

The text model uses a recurrent neural network (RNN) for
n where n is the
both the encoder and decoder. Here
V
is the vocabulary of the underlying
sentence length and
language. We deﬁne encφ(x) = z to be the last hidden
state of an encoder RNN. For decoding we feed z as an
additional input to the decoder RNN at each time step, and
at each time step via soft-
calculate the distribution over
j=1 softmax(Whj + b)xj where W
max, pψ(x
and b are parameters (part of ψ) and hj is the decoder RNN
hidden state. To be consistent with Corollary 1 we need to
ﬁnd the highest-scoring sequence ˆx under this distribution
during decoding, which is intractable in general. Instead
we approximate this with greedy search. The text transfer
model uses the same architecture as the text model but ex-
tends it with a classiﬁer pu(y
z) which is modeled using
an MLP and trained to minimize cross-entropy.

z) = (cid:81)n

V

|

|

We further compare our approach with a standard autoen-
coder (AE) and the cross-aligned autoencoder (Shen et al.,

Adversarially Regularized Autoencoders

Positive
⇒ ARAE
⇒ Cross-AE

great indoor mall .
no smoking mall .
terrible outdoor urine .

Positive
⇒ ARAE
⇒ Cross-AE

it has a great atmosphere , with wonderful service .
it has no taste , with a complete jerk .
it has a great horrible food and run out service .

we came on the recommendation of a bell boy and the food was amazing .
we came on the recommendation and the food was a joke .
we went on the car of the time and the chicken was awful .

Positive
⇒ ARAE
⇒ Cross-AE

Negative
⇒ ARAE
⇒ Cross-AE

hell no !
hell great !
incredible pork !

Negative
⇒ ARAE
⇒ Cross-AE

small , smokey , dark and rude management .
small , intimate , and cozy friendly staff .
great , , , chips and wine .

Negative
⇒ ARAE
⇒ Cross-AE

the people who ordered off the menu did n’t seem to do much better .
the people who work there are super friendly and the menu is good .
the place , one of the ofﬁce is always worth you do a business .

Table 2: Sentiment transfer results, where we transfer from posi-
tive to negative sentiment (Top) and negative to positive sentiment
(Bottom). Original sentence and transferred output (from ARAE
and the Cross-Aligned AE (from Shen et al. (2017)) of 6 randomly-
drawn examples.

vided a starting point for image generation models. Here
we use a similar method for text generation, which we call
reverse perplexity. We generate 100k samples from each
of the models, train an RNN language model on generated
samples and evaluate perplexity on held-out data.7 While
similar metrics for images (e.g. Parzen windows) have been
shown to be problematic, we argue that this is less of an
issue for text as RNN language models achieve state-of-the-
art perplexities on text datasets. We also calculate the usual
“forward” perplexity by training an RNN language model on
real data and testing on generated data. This measures the
ﬂuency of the generated samples, but cannot detect mode-
collapse, a common issue in training GANs (Arjovsky &
Bottou, 2017; Hu et al., 2018).

Table 1 shows these metrics for (i) ARAE, (ii) an autoen-
coder (AE),8 (iii) an RNN language model (LM), and (iv)
the real training set. We further ﬁnd that with a ﬁxed
prior, the reverse perplexity of an AAE-style text model
(Makhzani et al., 2015) was quite high (980) due to mode-
collapse. All models are of the same size to allow for fair
comparison. Training directly on real data (understand-
ably) outperforms training on generated data by a large
margin. Surprisingly however, training on ARAE samples
outperforms training on LM/AE samples in terms of reverse
perplexity.

6.2. Unaligned Text Style Transfer

Next we evaluate the model in the context of a learned adver-
sarial prior, as described in Section 3. We experiment with
two unaligned text transfer tasks: (i) transfer of sentiment on
the Yelp corpus, and (ii) topic on the Yahoo corpus (Zhang

7We also found this metric to be helpful for early-stopping.
8To “sample” from an AE we ﬁt a multivariate Gaussian to
the code space after training and generate code vectors from this
Gaussian to decode back into sentence space.

Figure 2: Image samples. The top block shows output generation
of the decoder for random noise samples; the bottom block shows
sample interpolation results.

Data

Reverse PPL

Forward PPL

Real data
LM samples
AE samples
ARAE samples

27.4
90.6
97.3
82.2

-
18.8
87.8
44.3

Table 1: Reverse PPL: Perplexity of language models trained on
the synthetic samples from a ARAE/AE/LM, and evaluated on real
data. Forward PPL: Perplexity of a language model trained on real
data and evaluated on synthetic samples.

1, 1)n by the tanh function at output layer.

2017) for transfer. In both our ARAE and standard AE
experiments, the encoder output is normalized to lie on the
unit sphere, and the generator output is bounded to lie in
(
−
Note, learning deep latent variable models for text sequences
has been a signiﬁcantly more challenging empirical problem
than for images. Standard models such as VAEs suffer from
optimization issues that have been widely documented. We
performed experiments with recurrent VAE, introduced by
(Bowman et al., 2016), as well as the adversarial autoen-
coder (AAE) (Makhzani et al., 2015), both with Gaussian
priors. We found that neither model was able to learn mean-
ingful latent representations—the VAE simply ignored the
latent code and the AAE experienced mode-collapse and
repeatedly generated the same samples.6 Appendix F in-
cludes detailed descriptions of the hyperparameters, model
architecture, and training regimes.

6. Experiments

6.1. Distributional Coverage
Section 4 argues that Pψ is trained to approximate the true
data distribution over discrete sequences P(cid:63). While it is
difﬁcult to test for this property directly (as is the case with
most GAN models), we can take samples from model to test
the ﬁdelity and coverage of the data space. Figure 2 shows
a set of samples from discretized MNIST and Appendix C
shows a set of generations from the text ARAE.

A common quantitative measure of sample quality for gener-
ative models is to evaluate a strong surrogate model trained
on its generated samples. While there are pitfalls of this
style of evaluation methods (Theis et al., 2016), it has pro-

6However there have been some recent successes training such

models, as noted in the related works section

Adversarially Regularized Autoencoders

Automatic Evaluation

Model

Transfer

BLEU Forward Reverse

Cross-Aligned AE
AE
ARAE, λ(1)
a
ARAE, λ(1)

b

77.1%
59.3%
73.4%
81.8%

17.75
37.28
31.15
20.18

65.9
31.9
29.7
27.7

124.2
68.9
70.1
77.0

Model

Transfer

Similarity Naturalness

Human Evaluation

Cross-Aligned AE
ARAE, λ(1)

b

57%
74%

3.8
3.7

2.7
3.8

Table 3: Sentiment transfer. (Top) Automatic metrics (Trans-
fer/BLEU/Forward PPL/Reverse PPL), (Bottom) Human evalua-
tion metrics (Transfer/Similarity/Naturalness). Cross-Aligned AE
is from Shen et al. (2017)

|

et al., 2015). For sentiment we follow the setup of Shen et al.
(2017) and split the Yelp corpus into two sets of unaligned
positive and negative reviews. We train ARAE with two
separate decoder RNNs, one for positive, p(x
z, y = 1),
|
and one for negative sentiment p(x
z, y = 0), and incorpo-
rate adversarial training of the encoder to remove sentiment
information from the prior. Transfer corresponds to encod-
ing sentences of one class and decoding, greedily, with the
opposite decoder. Experiments compare against the cross-
aligned AE of Shen et al. (2017) and also an AE trained
without the adversarial regularization. For ARAE, we exper-
imented with different λ(1) weighting on the adversarial loss
(see section 4) with λ(1)
b = 10. Both use λ(2) = 1.
Empirically the adversarial regularization enhances trans-
fer and perplexity, but tends to make the transferred text
less similar to the original, compared to the AE. Randomly
selected example sentences are shown in Table 2 and addi-
tional outputs are available in Appendix G.

a = 1, λ(1)

Table 3 (top) shows quantitative evaluation. We use four
automatic metrics: (i) Transfer: how successful the model
is at altering sentiment based on an automatic classiﬁer
(we use the fastText library (Joulin et al., 2017)); (ii)
BLEU: the consistency between the transferred text and the
original; (iii) Forward PPL: the ﬂuency of the generated
text; (iv) Reverse PPL: measuring the extent to which the
generations are representative of the underlying data distri-
bution. Both perplexity numbers are obtained by training
an RNN language model. Table 3 (bottom) shows human
evaluations on the cross-aligned AE and our best ARAE
model. We randomly select 1000 sentences (500/500 posi-
tive/negative), obtain the corresponding transfers from both
models, and ask crowdworkers to evaluate the sentiment
(Positive/Neutral/Negative) and naturalness (1-5, 5 being
most natural) of the transferred sentences. We create a sepa-
rate task in which we show the original and the transferred
sentences, and ask them to evaluate the similarity based on
sentence structure (1-5, 5 being most similar). We explicitly
requested that the reader disregard sentiment in similarity

Science
⇒ Music
⇒ Politics

Science
⇒ Music
⇒ Politics

Science
⇒ Music
⇒ Politics

Music
⇒ Science
⇒ Politics

Music
⇒ Science

⇒ Politics

Music
⇒ Science
⇒ Politics

Politics
⇒ Science
⇒ Music

Politics
⇒ Science
⇒ Music

Politics
⇒ Science
⇒ Music

what is an event horizon with regards to black holes ?
what is your favorite sitcom with adam sandler ?
what is an event with black people ?

take 1ml of hcl ( concentrated ) and dilute it to 50ml .
take em to you and shout it to me
take bribes to islam and it will be punished .

just multiply the numerator of one fraction by that of the other .
just multiply the fraction of the other one that &apos;s just like it .
just multiply the same fraction of other countries .

do you know a website that you can ﬁnd people who want to join bands ?
do you know a website that can help me with science ?
do you think that you can ﬁnd a person who is in prison ?

all three are fabulous artists , with just incredible talent ! !
all three are genetically bonded with water , but just as many substances ,
are capable of producing a special case .
all three are competing with the government , just as far as i can .

but there are so many more i can &apos;t think of !
but there are so many more of the number of questions .
but there are so many more of the can i think of today .

republicans : would you vote for a cheney / satan ticket in 2008 ?
guys : how would you solve this question ?
guys : would you rather be a good movie ?

4 years of an idiot in ofﬁce + electing the idiot again = ?
4 years of an idiot in the ofﬁce of science ?
4 ) <unk> in an idiot , the idiot is the best of the two points ever !

anyone who doesnt have a billion dollars for all the publicity cant win .
anyone who doesnt have a decent chance is the same for all the other .
anyone who doesnt have a lot of the show for the publicity .

Table 4: Topic Transfer. Random samples from the Yahoo dataset.
Note the ﬁrst row is from ARAE trained on titles while the follow-
ing ones are from replies.

Model

Medium Small

Tiny

Supervised Encoder
Semi-Supervised AE
Semi-Supervised ARAE

65.9%
68.5%
70.9%

62.5% 57.9%
64.6% 59.9%
66.8% 62.5%

Table 5: Semi-Supervised accuracy on the natural language infer-
ence (SNLI) test set, respectively using 22.2% (medium), 10.8%
(small), 5.25% (tiny) of the supervised labels of the full SNLI
training set (rest used for unlabeled AE training).

assessment.

The same method can be applied to other style transfer
tasks, for instance the more challenging Yahoo QA data
(Zhang et al., 2015). For Yahoo we chose 3 relatively dis-
tinct topic classes for transfer: SCIENCE & MATH, ENTER-
TAINMENT & MUSIC, and POLITICS & GOVERNMENT.
As the dataset contains both questions and answers, we sep-
arated our experiments into titles (questions) and replies
(answers). Randomly-selected generations are shown in Ta-
ble 4. See Appendix G for additional generation examples.

6.3. Semi-Supervised Training

Latent variable models can also provide an easy method
for semi-supervised training. We use a natural language in-
ference task to compare semi-supervised ARAE with other
training methods. Results are shown in Table 5. The full
SNLI training set contains 543k sentence pairs, and we use
supervised sets of 120k (Medium), 59k (Small), and 28k
(Tiny) and use the rest of the training set for unlabeled train-
ing. As a baseline we use an AE trained on the additional

Adversarially Regularized Autoencoders

Figure 3: Left: (cid:96)2 norm of encoder output z and generator output ˜z during ARAE training. (z is normalized, whereas the generator learns
to match). Middle: Sum of the dimension-wise variances of z and generator codes ˜z as well as reference AE. Right: Average cosine
similarity of nearby sentences (by word edit-distance) for the ARAE and AE during training.

AE

ARAE

Model

Samples

k

0
1
2
3
4

1.06
4.51
6.61
9.14
9.97

2.19
4.07
5.39
6.86
7.47

Original
Noised
AE
ARAE

Original
Noised
AE
ARAE

A woman wearing sunglasses
A woman sunglasses wearing
A woman sunglasses wearing sunglasses
A woman wearing sunglasses

Pets galloping down the street
Pets down the galloping street
Pets riding the down galloping
Pets congregate down the street near a ravine

Figure 4: Reconstruction error (negative log-likelihood averaged
over sentences) of the original sentence from a corrupted sentence.
Here k is the number of swaps performed on the original sentence.

data, similar to the setting explored in Dai & Le (2015).
For ARAE we use the subset of unsupervised data of length
< 15 (i.e. ARAE is trained on less data than AE for unsuper-
vised training). The results are shown in Table 5. Training
on unlabeled data with an AE objective improves upon a
model just trained on labeled data. Training with adversarial
regularization provides further gains.

7. Discussion

Impact of Regularization on Discrete Encoding We
further examine the impact of adversarial regularization
on the encoded representation produced by the model as
it is trained. Figure 3 (left), shows a sanity check that the
(cid:96)2 norm of encoder output z and prior samples ˜z converge
quickly in ARAE training. The middle plot compares the
trace of the covariance matrix between these terms as train-
ing progresses. It shows that variance of the encoder and
the prior match after several epochs.

Smoothness and Reconstruction We can also assess the
“smoothness” of the encoder model learned ARAE (Rifai
et al., 2011). We start with a simple proxy that a smooth
encoder model should map similar sentences to similar z
values. For 250 sentences, we calculate the average co-
sine similarity of 100 randomly-selected sentences within
an edit-distance of at most 5 to the original. The graph in
Figure 3 (right) shows that the cosine similarity of nearby
sentences is quite high for ARAE compared to a standard
AE and increases in early rounds of training. To further test
this property, we feed noised discrete input to the encoder
and (i) calculate the score given to the original input, and

(ii) compare the resulting reconstructions. Figure 4 (right)
shows results for text where k words are ﬁrst permuted in
each sentence. We observe that ARAE is able to map a
noised sentence to a natural sentence (though not necessar-
ily the denoised sentence). Figure 4 (left) shows empirical
results for these experiments. We obtain the reconstruction
error (negative log likelihood) of the original non-noised
sentence under the decoder, utilizing the noised code. We
ﬁnd that when k = 0 (i.e. no swaps), the regular AE better
reconstructs the exact input. However, as the number of
swaps pushes the input further away, ARAE is more likely
to produce the original sentence. (Note that unlike denois-
ing autoencoders which require a domain-speciﬁc noising
function (Hill et al., 2016; Vincent et al., 2008), the ARAE
is not explicitly trained to denoise an input.)

Manipulation through the Prior An interesting property
of latent variable models such as VAEs and GANs is the
ability to manipulate output samples through the prior. In
particular, for ARAE, the Gaussian form of the noise sam-
ple s induces the ability to smoothly interpolate between
outputs by exploiting the structure. While language models
may provide a better estimate of the underlying probability
space, constructing this style of interpolation would require
combinatorial search, which makes this a useful feature of
latent variable text models. In Appendix D we show inter-
polations from for the text model, while Figure 2 (bottom)
shows the interpolations for discretized MNIST ARAE.

A related property of GANs is the ability to move in the
latent space via offset vectors.9 To experiment with this
property we generate sentences from the ARAE and com-
pute vector transforms in this space to attempt to change
main verbs, subjects and modiﬁer (details in Appendix E).
Some examples of successful transformations are shown in
Figure 5 (bottom). Quantitative evaluation of the success of
the vector transformations is given in Figure 5 (top).

9Similar to the case with word vectors (Mikolov et al., 2013),
Radford et al. (2016) observe that when the mean latent vector
for “men with glasses” is subtracted from the mean latent vector
for “men without glasses” and applied to an image of a “woman
without glasses”, the resulting image is that of a “woman with
glasses”.

Adversarially Regularized Autoencoders

Transform

Match %

walking
man
two
dog
standing
several

85
92
86
88
89
70

Prec

79.5
80.2
74.1
77.0
79.3
67.0

A man in a tie is sleeping and clapping on balloons .
A man in a tie is clapping and walking dogs .

The jewish boy is trying to stay out of his skateboard .
The jewish man is trying to stay out of his horse .

Some child head a playing plastic with drink .
Two children playing a head with plastic drink .

The people shine or looks into an area .
The dog arrives or looks into an area .

A women are walking outside near a man .
Three women are standing near a man walking .

⇒walking

⇒man

⇒Two

⇒dog

⇒standing

A side child listening to a piece with steps playing on a table . ⇒Several
Several child playing a guitar on side with a table .

Figure 5: Top: Quantitative evaluation of transformations. Match
% refers to the % of samples where at least one decoder samples
(per 100) had the desired transformation in the output, while Prec.
measures the average precision of the output against the original
sentence. Bottom: Examples where the offset vectors produced
successful transformations of the original sentence. See Appendix
E for the full methodology.

8. Related Work

While ideally autoencoders would learn latent spaces which
compactly capture useful features that explain the observed
data, in practice they often learn a degenerate identity map-
ping where the latent code space is free of any structure,
necessitating the need for some regularization on the la-
tent space. A popular approach is to regularize through an
explicit prior on the code space and use a variational approx-
imation to the posterior, leading to a family of models called
variational autoencoders (VAE) (Kingma & Welling, 2014;
Rezende et al., 2014). Unfortunately VAEs for discrete text
sequences can be challenging to train—for example, if the
training procedure is not carefully tuned with techniques
like word dropout and KL annealing (Bowman et al., 2016),
the decoder simply becomes a language model and ignores
the latent code. However there have been some recent suc-
cesses through employing convolutional decoders (Yang
et al., 2017; Semeniuta et al., 2017), training the latent rep-
resentation as a topic model (Dieng et al., 2017; Wang et al.,
2018), using the von Mises–Fisher distribution (Guu et al.,
2017), and combining VAE with iterative inference (Kim
et al., 2018). There has also been some work on making
the prior more ﬂexible through explicit parameterization
(Chen et al., 2017; Tomczak & Welling, 2018). A notable
technique is adversarial autoencoders (AAE) (Makhzani
et al., 2015) which attempt to imbue the model with a more
ﬂexible prior implicitly through adversarial training. Recent
work on Wasserstein autoencoders (Tolstikhin et al., 2018)
provides a theoretical foundation for the AAE and shows
that AAE minimizes the Wasserstein distance between the
data/model distributions.

The success of GANs on images have led many researchers
to consider applying GANs to discrete data such as text.
Policy gradient methods are a natural way to deal with the
resulting non-differentiable generator objective when train-
ing directly in discrete space (Glynn, 1987; Williams, 1992).
When trained on text data however, such methods often re-
quire pre-training/co-training with a maximum likelihood
(i.e.
language modeling) objective (Che et al., 2017; Yu
et al., 2017; Li et al., 2017). Another direction of work
has been through reparameterizing the categorical distri-
bution with the Gumbel-Softmax trick (Jang et al., 2017;
Maddison et al., 2017)—while initial experiments were en-
couraging on a synthetic task (Kusner & Hernandez-Lobato,
2016), scaling them to work on natural language is a chal-
lenging open problem. There have also been recent related
approaches that work directly with the soft outputs from
a generator (Gulrajani et al., 2017; Rajeswar et al., 2017;
Shen et al., 2017; Press et al., 2017). For example, Shen
et al. (2017) exploits adversarial loss for unaligned style
transfer between text by having the discriminator act on the
RNN hidden states and using the soft outputs at each step
as input to an RNN generator. Our approach instead works
entirely in ﬁxed-dimensional continuous space and does not
require utilizing RNN hidden states directly. It is therefore
also different from methods that discriminate in the joint
latent/data space, such as ALI (Vincent Dumoulin, 2017)
and BiGAN (Donahue et al., 2017). Finally, our work adds
to the recent line of work on unaligned style transfer for
text (Hu et al., 2017; Mueller et al., 2017; Li et al., 2018;
Prabhumoye et al., 2018; Yang et al., 2018).

9. Conclusion

We present adversarially regularized autoencoders (ARAE)
as a simple approach for training a discrete structure au-
toencoder jointly with a code-space generative adversarial
network. Utilizing the Wasserstein autoencoder framework
(Tolstikhin et al., 2018), we also interpret ARAE as learning
a latent variable model that minimizes an upper bound on the
total variation distance between the data/model distributions.
We ﬁnd that the model learns an improved autoencoder and
exhibits a smooth latent space, as demonstrated by semi-
supervised experiments, improvements on text style transfer,
and manipulations in the latent space.

We note that (as has been frequently observed when training
GANs) the proposed model seemed to be quite sensitive to
hyperparameters, and that we only tested our model on sim-
ple structures such as binarized digits and short sentences.
Cífka et al. (2018) recently evaluated a suite of sentence
generation models and found that models are quite sensitive
to their training setup, and that different models do well
on different metrics. Training deep latent variable models
that can robustly model complex discrete structures (e.g.
documents) remains an important open issue in the ﬁeld.

Adversarially Regularized Autoencoders

Acknowledgements

We thank Sam Wiseman, Kyunghyun Cho, Sam Bowman,
Joan Bruna, Yacine Jernite, Martín Arjovsky, Mikael Henaff,
and Michael Mathieu for fruitful discussions. We are partic-
ularly grateful to Tianxiao Shen for providing the results for
style transfer. We also thank the NVIDIA Corporation for
the donation of a Titan X Pascal GPU that was used for this
research. Yoon Kim was supported by a gift from Amazon
AWS Machine Learning Research.

References

Arjovsky, M. and Bottou, L. Towards Principled Methods for
Training Generative Adversarial Networks. In Proceedings of
ICML, 2017.

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein GAN. In

Proceedings of ICML, 2017.

Bowman, S. R., Angeli, G., Potts, C., and Manning., C. D. A large
annotated corpus for learning natural language inference. In
Proceedings of EMNLP, 2015.

Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R.,
and Bengio, S. Generating Sentences from a Continuous Space.
2016.

Che, T., Li, Y., Zhang, R., Hjelm, R. D., Li, W., Song, Y., and
Bengio, Y. Maximum-Likelihood Augment Discrete Generative
Adversarial Networks. arXiv:1702.07983, 2017.

Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P.,
Schulman, J., Sutskever, I., and Abbeel, P. Variational Lossy
Autoencoder. In Proceedings of ICLR, 2017.

Cífka, O., Severyn, A., Alfonseca, E., and Filippova, K. Eval all,
trust a few, do wrong to none: Comparing sentence generation
models. arXiv:1804.07972, 2018.

Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. In

Proceedings of NIPS, 2015.

Denton, E. and Birodkar, V. Unsupervised learning of disentangled
representations from video. In Proceedings of NIPS, 2017.

Dieng, A. B., Wang, C., Gao, J., , and Paisley, J. TopicRNN: A
Recurrent Neural Network With Long-Range Semantic Depen-
dency. In Proceedings of ICLR, 2017.

Donahue, J., Krahenbühl, P., and Darrell, T. Adversarial Feature

Learning. In Proceedings of ICLR, 2017.

Glynn, P. Likelihood Ratio Gradient Estimation: An Overview. In

Proceedings of Winter Simulation Conference, 1987.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-
Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative
adversarial nets. In Proceedings of NIPS, 2014.

Gozlan, N. and Léonard, C. Transport Inequalities. A Survey.

arXiv:1003.3852, 2010.

Guu, K., Hashimoto, T. B., Oren, Y., and Liang, P. Generating
Sentences by Editing Prototypes. arXiv:1709.08878, 2017.

Hill, F., Cho, K., and Korhonen, A. Learning distributed represen-
tations of sentences from unlabelled data. In Proceedings of
NAACL, 2016.

Hjelm, R. D., Jacob, A. P., Che, T., Cho, K., and Bengio, Y.
Boundary-Seeking Generative Adversarial Networks. In Pro-
ceedings of ICLR, 2018.

Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., and Xing, E. P.
Controllable Text Generation. In Proceedings of ICML, 2017.

Hu, Z., Yang, Z., Salakhutdinov, R., and Xing, E. P. On Unifying

Deep Generative Models. In Proceedings of ICLR, 2018.

Jang, E., Gu, S., and Poole, B. Categorical Reparameterization

with Gumbel-Softmax. In Proceedings of ICLR, 2017.

Joulin, A., Grave, E., Bojanowski, P., and Mikolov, T. Bag of
Tricks for Efﬁcient Text Classiﬁcation. In Proceedings of ACL,
2017.

Kim, Y., Wiseman, S., Miller, A. C., Sontag, D., and Rush, A. M.
Semi-Amortized Variational Autoencoders. In Proceedings of
ICML, 2018.

Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes.

In Proceedings of ICLR, 2014.

Kusner, M. and Hernandez-Lobato, J. M. GANs for Sequences
of Discrete Elements with the Gumbel-Softmax Distribution.
arXiv:1611.04051, 2016.

Lample, G., Zeghidour, N., Usuniera, N., Bordes, A., Denoyer,
L., and Ranzato, M. Fader networks: Manipulating images by
sliding attributes. In Proceedings of NIPS, 2017.

Li, J., Monroe, W., Shi, T., Jean, S., Ritter, A., and Jurafsky,
D. Adversarial Learning for Neural Dialogue Generation. In
Proceedings of EMNLP, 2017.

Li, J., Jia, R., He, H., and Liang, P. Delete, Retrieve, Gener-
ate: A Simple Approach to Sentiment and Style Transfer. In
Proceedings of NAACL, 2018.

Maddison, C. J., Mnih, A., and Teh, Y. W. The Concrete Distribu-
tion: A Continuous Relaxation of Discrete Random Variables.
In Proceedings of ICLR, 2017.

Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey, B.

Adversarial Autoencoders. arXiv:1511.05644, 2015.

Mikolov, T., tau Yih, S. W., and Zweig, G. Linguistic Regularities
in Continuous Space Word Representations. In Proceedings of
NAACL, 2013.

Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral
Normalization For Generative Adversarial Networks. In Pro-
ceedings of ICLR, 2018.

Mueller, J., Gifford, D., and Jaakkola, T. Sequence to Better
Sequence: Continuous Revision of Combinatorial Structures.
In Proceedings of ICML, 2017.

Gulrajani, I., Ahmed, F., Arjovsky, M., and Vincent Dumoulin,
A. C. Improved Training of Wasserstein GANs. In Proceedings
of NIPS, 2017.

Prabhumoye, S., Tsvetkov, Y., Salakhutdinov, R., and Black, A. W.
Style Transfer Through Back-Translation. In Proceedings of
ACL, 2018.

Adversarially Regularized Autoencoders

Press, O., Bar, A., Bogin, B., Berant, J., and Wolf, L. Language
Generation with Recurrent Generative Adversarial Networks
without Pre-training. arXiv:1706.01399, 2017.

Radford, A., Metz, L., and Chintala, S. Unsupervised Representa-
tion Learning with Deep Convolutional Generative Adversarial
Networks. In Proceedings of ICLR, 2016.

Rajeswar, S., Subramanian, S., Dutil, F., Pal, C., and
Courville, A. Adversarial Generation of Natural Language.
arXiv:1705.10929, 2017.

Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic Back-
propagation and Approximate Inference in Deep Generative
Models. In Proceedings of ICML, 2014.

Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. Con-
tractive Auto-Encoders: Explicit Invariance During Feature
Extraction. In Proceedings of ICML, 2011.

Semeniuta, S., Severyn, A., and Barth, E. A Hybrid Convolutional
Variational Autoencoder for Text Generation. In Proceedings
of EMNLP, 2017.

Shen, T., Lei, T., Barzilay, R., and Jaakkola, T. Style Transfer
from Non-Parallel Text by Cross-Alignment. In Proceedings of
NIPS, 2017.

Theis, L., van den Oord, A., and Bethge, M. A note on the
evaluation of generative models. In Proceedings of ICLR, 2016.

Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. Wasser-

stein Auto-Encoders. In Proceedings of ICLR, 2018.

Tomczak, J. M. and Welling, M. VAE with a VampPrior.

In

Proceedings of AISTATS, 2018.

Villani, C. Optimal transport: old and new, volume 338. Springer

Science & Business Media, 2008.

Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Ex-
tracting and Composing Robust Features with Denoising Au-
toencoders. In Proceedings of ICML, 2008.

Vincent Dumoulin, Ishmael Belghazi, B. P. O. M. A. L. M. A.
A. C. Adversarially Learned Inference. In Proceedings of ICLR,
2017.

Wang, W., Gan, Z., Wang, W., Shen, D., Huang, J., Ping, W.,
Satheesh, S., and Carin, L. Topic Compositional Neural Lan-
guage Model. In Proceedings of AISTATS, 2018.

Williams, R. J. Simple Statistical Gradient-following Algorithms
for Connectionist Reinforcement Learning. Machine Learning,
8, 1992.

Yang, Z., Hu, Z., Salakhutdinov, R., and Berg-Kirkpatrick, T.
Improved Variational Autoencoders for Text Modeling using
Dilated Convolutions. In Proceedings of ICML, 2017.

Yang, Z., Hu, Z., Dyer, C., Xing, E. P., and Berg-Kirkpatrick, T.
Unsupervised Text Style Transfer using Language Models as
Discriminators. arXiv:1805.11749, 2018.

Yu, L., Zhang, W., Wang, J., and Yu, Y. SeqGAN: Sequence Gen-
erative Adversarial Nets with Policy Gradient. In Proceedings
of AAAI, 2017.

Zhang, X., Zhao, J., and LeCun, Y. Character-level Convolutional
Networks for Text Classiﬁcation. In Proceedings of NIPS, 2015.

Adversarially Regularized Autoencoders

A. Proof of Corollary 1

∈ X

is the

where

to the n

Corollary (Discrete case). Suppose x
X
set of all one-hot vectors of length n, and let fψ :
Z →
∆n−1 be a deterministic function that goes from the latent
1 dimensional simplex ∆n−1. Fur-
space
Z
ther let Gψ :
be a deterministic function such
that Gψ(z) = arg maxw∈X w(cid:62)fψ(z), and as above let
z) be the dirac distribution derived from Gψ such
Pψ(x
. Then the following is an
that pψ(x
}
upper bound on
TV, the total variation distance
between P(cid:63) and Pψ.

x = Gψ(z)
{
P(cid:63)(cid:107)
Pψ −
(cid:107)

−
Z → X

z) = 1

|

|

inf
Q(z | x):PQ=Pz

EP(cid:63) EQ(z | x)

(cid:104)

2
log 2

−

(cid:105)
log x(cid:62)fψ(z)

Proof. Let our cost function be c(x, y) = 1
ﬁrst note that for all x, z

x
{

= y

. We

}

input to the generator is from a simple distribution (e.g.
spherical Gaussian) and the generator possesses less capac-
ity than the encoder. However, it is not so simple that it is
overly restrictive (e.g. as in VAEs). Empirically we observe
that the ﬁrst and second moments do indeed converge as
training progresses (Section 7).
Proposition 1. Let P be a distribution on a compact set χ,
and (Pn)n∈N be a sequence of distributions on χ. Further
suppose that W (Pn, P)
0. Then the following statements
hold:

→

(i) Pn (cid:32) P (i.e. convergence in distribution).
(ii) All moments converge, i.e. for all k > 1, k

N,

∈

EX∼Pn

(cid:104) d
(cid:89)

i=1

(cid:105)

X pi
i

EX∼P

→

(cid:104) d
(cid:89)

i=1

(cid:105)

X pi
i

log 21
x
{

= arg max
w∈X

w(cid:62)fψ(z)
}

<

−

log x(cid:62)fψ(z)

for all p1, . . . , pd such that (cid:80)d

i=1 pi = k

x

= arg maxw∈X w(cid:62)fψ(z)
}

This holds since if 1
{
1, we have x(cid:62)fψ(z) < 0.5, and

=
log x(cid:62)fψ(z) >
If on the other hand x =
−
arg maxw∈X w(cid:62)fψ(z), then the LHS is 0 and RHS is al-
ways postive since fψ(z)

log 0.5 = log 2.

∆n−1. Then,

−

∈

2
log 2

log x(cid:62)fψ(z)]

w(cid:62)fψ(z)

]
}

x
{

x
{

= arg max
w∈X
= Gψ(z)
}

]

EP(cid:63) EQ(z | x)[

−
EP(cid:63) EQ(z | x)[21

EP(cid:63) EQ(z | x)[1

inf
Q:PQ=Pz

> inf

Q:PQ=Pz

=2

inf
Q:PQ=Pz

=2

inf
Q:PQ=Pz

= 2Wc(P(cid:63), Pψ)
=
P(cid:63) −

Pψ(cid:107)

(cid:107)

TV

EP(cid:63) EQ(z | x)[c(x, Gψ(z))]

The ﬁfth line follows from Theorem 1, and the last equality
uses the well-known correspondence between total varia-
tion distance and optimal transport with the indicator cost
function (Gozlan & Léonard, 2010).

B. Optimality Property

One can interpret the ARAE framework as a dual pathway
network mapping two distinct distributions into a similar
one; encφ and gθ both output code vectors that are kept
similar in terms of Wasserstein distance as measured by the
critic. We provide the following proposition showing that
under our parameterization of the encoder and the generator,
as the Wasserstein distance converges, the encoder distribu-
tion (PQ) converges to the generator distribution (Pz), and
further, their moments converge.

Proof. (i) has been proved in (Villani, 2008) Theorem 6.9.

For (ii), using The Portmanteau Theorem, (i) is equivalent
to the following statement:

EX∼Pn [f (X)]
uous function f : Rd
random variable.

→

→

EX∼P[f (X)] for all bounded and contin-
R, where d is the dimension of the

The k-th moment of a distribution is given by

(cid:104) d
(cid:89)

E

(cid:105)

X pi
i

i=1

such that

pi = k

d
(cid:88)

i=1

Our encoded code is bounded as we normalize the encoder
output to lie on the unit sphere, and our generated code is
1, 1)n by the tanh function. Hence
also bounded to lie in (
f (X) = (cid:81)d
is a bounded continuous function for all
qi ≥

i=1 X qi
i
0. Therefore,

−

EX∼Pn

(cid:104) d
(cid:89)

i=1

(cid:105)

X pi
i

EX∼P

→

(cid:104) d
(cid:89)

i=1

(cid:105)

X pi
i

where (cid:80)d

i=1 pi = k

C. Sample Generations

In Figure 6 we show some generated samples from the
ARAE, AE, and a LM.

D. Sentence Interpolations

This is ideal since under our setting the generated distribu-
tion is simpler than the encoded distribution, because the

In Figure 7 we show generations from interpolated latent
vectors. Speciﬁcally, we sample two points z0 and z1 from

Adversarially Regularized Autoencoders

ARAE Samples
A woman preparing three
ﬁsh .
A woman is seeing a man
in the river .
There passes a woman near
birds in the air .
Some ten people is sitting
through their ofﬁce .
The man got stolen with
young dinner bag .
Monks are running in court
.
The Two boys in glasses
are all girl .
The man is small sitting in
two men that tell a children
.
The two children are eating
the balloon animal .
A woman is trying on a
microscope .
The dogs are sleeping in
bed .

AE Samples
Two Three woman in a cart
tearing over of a tree .
A man is hugging and art .
The fancy skier is starting
under the drag cup in .
A dog are <unk> a
A man is not standing .
The Boys in their swim-
ming .
A surfer and a couple wait-
ing for a show .
A couple is a kids at a bar-
becue .
The motorcycles is in the
ocean loading
I ’s bike is on empty
The actor was walking in a
a small dog area .
no dog is young their
mother

LM Samples
a man walking outside on
a dirt road , sitting on the
dock .
A large group of people is
taking a photo for Christ-
mas and at night .
Someone is avoiding a soc-
cer game .
The man and woman are
dressed for a movie .
Person in an empty stadium
pointing at a mountain .
Two children and a little
boy are <unk> a man in a
blue shirt .
A boy rides a bicycle .
A girl is running another in
the forest .
the man is an indian women
.

Figure 6: Text samples generated from ARAE, a simple AE, and
from a baseline LM trained on the same data. To generate from an
AE we ﬁt a multivariate Gaussian to the learned code space and
generate code vectors from this Gaussian.

A man is on the corner in a
sport area .
A man is on corner in a road
all .
A lady is on outside a race-
track .
A lady is outside on a race-
track .
A lot of people is outdoors
in an urban setting .
A lot of people is outdoors
in an urban setting .
A lot of people is outdoors
in an urban setting .

A man is on a ship path with
the woman .
A man is on a ship path with
the woman .
A man is passing on a
bridge with the girl .
A man is passing on a
bridge with the girl .
A man is passing on a
bridge with the girl .
A man is passing on a
bridge with the dogs .
A man is passing on a
bridge with the dogs .

A man in a cave is used an
escalator .

A man in a cave is used
an escalator
A man in a cave is used
chairs .
A man in a number is used
many equipment
A man in a number is pos-
ing so on a big rock .
People are posing in a rural
area .
People are posing in a rural
area.

p(z) and construct intermediary points zλ = λz1 + (1
λ)z0. For each we generate the argmax output ˜xλ.

−

E. Vector Arithmetic

We generate 1 million sentences from the ARAE and parse
the sentences to obtain the main verb, subject, and modiﬁer.
Then for a given sentence, to change the main verb we sub-
tract the mean latent vector (t) for all other sentences with
the same main verb (in the ﬁrst example in Figure 5 this
would correspond to all sentences that had “sleeping” as the
main verb) and add the mean latent vector for all sentences
that have the desired transformation (with the running ex-
ample this would be all sentences whose main verb was
“walking”). We do the same to transform the subject and
the modiﬁer. We decode back into sentence space with the
transformed latent vector via sampling from pψ(g(z + t)).
Some examples of successful transformations are shown
in Figure 5 (right). Quantitative evaluation of the success
of the vector transformations is given in Figure 5 (left).
For each original vector z we sample 100 sentences from
pψ(g(z + t)) over the transformed new latent vector and

consider it a match if any of the sentences demonstrate the
desired transformation. Match % is proportion of original
vectors that yield a match post transformation. As we ideally
want the generated samples to only differ in the speciﬁed
transformation, we also calculate the average word precision
against the original sentence (Prec) for any match.

F. Experimental Details

MNIST experiments

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

The
is
encoder
784-800-400-100.

a

three-layer MLP,

Additive Gaussian noise is injected into c then gradu-
ally decayed to 0.

The
a
100-400-800-1000-784

decoder

is

four-layer

MLP,

The autoencoder is optimized by Adam, with learning
rate 5e-04.

An MLP generator 32-64-100-150-100.

An MLP critic 100-100-60-20-1 with weight clip-
ping (cid:15) = 0.05. The critic is trained 10 iterations in
every loop.

GAN is optimized by Adam, with learning rate 5e-04
on the generator and 5e-05 on the critic.

Weighing factor λ(1) = 0.2.

The encoder is an one-layer LSTM with 300 hidden
units.

Additive Gaussian noise is injected into c then gradu-
ally decayed to 0.

The decoder is an one-layer LSTM with 300 hidden
units.

The LSTM state vector is augmented by the hidden
code c at every decoding time step, before forwarding
into the output softmax layer.

The word embedding is of size 300.

The autoencoder is optimized by SGD with learning
rate 1. A grad clipping on the autoencoder, with max
grad_norm set to 1.

An MLP generator 100-300-300.

An MLP critic 300-300-1 with weight clipping (cid:15) =
0.01. The critic is trained 5 iterations in every loop.

GAN is optimized by Adam, with learning rate 5e-05
on the generator, and 1e-05 on the critic.

Figure 7: Sample interpolations from the ARAE. Constructed by
linearly interpolating in the latent space and decoding to the output
space. Word changes are highlighted in black.

Text experiments

Adversarially Regularized Autoencoders

Semi-supervised experiments

The following changes are made based on the SNLI experi-
ments:

•

•

•

an MLP
Larger network to GAN components:
generator 100-150-300-500 and an MLP critic
500-500-150-80-20-1 with weight clipping fac-
tor (cid:15) = 0.02.

Yelp/Yahoo transfer

An MLP style adversarial classiﬁer 300-200-100,
trained by SGD learning rate 0.1.

Weighing factor from both adversarial forces λ(1)
λ(1)
b = 10.

a = 1,

G. Style Transfer Samples

In the following pages we show randomly sampled style
transfers from the Yelp/Yahoo corpus.

Adversarially Regularized Autoencoders

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
Cross-AE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Yelp Sentiment Transfer

Positive to Negative

great indoor mall .
no smoking mall .
terrible outdoor urine .

great blooming onion .
no receipt onion .
terrible of pie .

i really enjoyed getting my nails done by peter .
i really needed getting my nails done by now .
i really really told my nails done with these things .

deﬁnitely a great choice for sushi in las vegas !
deﬁnitely a _num_ star rating for _num_ sushi in las vegas .
not a great choice for breakfast in las vegas vegas !

the best piece of meat i have ever had !
the worst piece of meat i have ever been to !
the worst part of that i have ever had had !

it has a great atmosphere , with wonderful service .
it has no taste , with a complete jerk .
it has a great horrible food and run out service .

their menu is extensive , even have italian food .
their menu is limited , even if i have an option .
their menu is decent , i have gotten italian food .

everyone who works there is incredibly friendly as well .
everyone who works there is incredibly rude as well .
everyone who works there is extremely clean and as well .

there are a couple decent places to drink and eat in here as well .
there are a couple slices of options and _num_ wings in the place .
there are a few night places to eat the car here are a crowd .

if you ’re in the mood to be adventurous , this is your place !
if you ’re in the mood to be disappointed , this is not the place .
if you ’re in the drive to the work , this is my place !

Negative to Positive

hell no !
hell great !
incredible pork !

highly disappointed !
highly recommended !
highly clean !

bad products .
good products .
good prices .

i was so very disappointed today at lunch .
i highly recommend this place today .
i was so very pleased to this .

i have n’t received any response to anything .
i have n’t received any problems to please .
i have always the desert vet .

small , smokey , dark and rude management .
small , intimate , and cozy friendly staff .
great , , , chips and wine .

the restaurant did n’t meet our standard though .
the restaurant did n’t disappoint our expectations though .
the restaurant is always happy and knowledge .

you could not see the stage at all !
you could see the difference at the counter !
you could deﬁnitely get the fuss !

room is void of all personality , no pictures or any sort of decorations .
room is eclectic , lots of ﬂavor and all of the best .
it ’s a nice that amazing , that one ’s some of ﬂavor .

waited in line to see how long a wait would be for three people .
waited in line for a long wait and totally worth it .
another great job to see and a lot going to be from dinner .

really good food , super casual and really friendly .
really bad food , really generally really low and decent food .
really good food , super horrible and not the price .

all the ﬁxes were minor and the bill ?
all the barbers were entertaining and the bill did n’t disappoint .
all the ﬂavors were especially and one !

we came on the recommendation of a bell boy and the food was amazing .
we came on the recommendation and the food was a joke .
we went on the car of the time and the chicken was awful .

the people who ordered off the menu did n’t seem to do much better .
the people who work there are super friendly and the menu is good .
the place , one of the ofﬁce is always worth you do a business .

service is good but not quick , just enjoy the wine and your company .
service is good but not quick , but the service is horrible .
service is good , and horrible , is the same and worst time ever .

the steak was really juicy with my side of salsa to balance the ﬂavor .
the steak was really bland with the sauce and mashed potatoes .
the ﬁsh was so much , the most of sauce had got the ﬂavor .

other than that one hell hole of a star bucks they ’re all great !
other than that one star rating the toilet they ’re not allowed .
a wonder our one came in a _num_ months , you ’re so better !

they told us in the beginning to make sure they do n’t eat anything .
they told us in the mood to make sure they do great food .
they ’re us in the next for us as you do n’t eat .

the person who was teaching me how to control my horse was pretty rude .
the person who was able to give me a pretty good price .
the owner ’s was gorgeous when i had a table and was friendly .

he was cleaning the table next to us with gloves on and a rag .
he was prompt and patient with us and the staff is awesome .
he was like the only thing to get some with with my hair .

Figure 8: Full sheet of sentiment transfer result on the Yelp corpus.

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Original
ARAE
Cross-AE

Adversarially Regularized Autoencoders

Yahoo Topic Transfer on Questions

from Science

from Music

from Politics

Original

Music

what is an event horizon with regards to black holes
?
what is your favorite sitcom with adam sandler ?

Original

Science

republicans : would you vote for a cheney / satan
ticket in 2008 ?
guys : how would you solve this question ?

Original

Science

Politics

do you know a website that you can ﬁnd people
who want to join bands ?
do you know a website that can help me with sci-
ence ?
do you think that you can ﬁnd a person who is in
prison ?

Politics

what is an event with black people ?

Music

guys : would you rather be a good movie ?

Original

Music

what did john paul jones do in the american revolu-
tion ?
what did john lennon do in the new york family ?

Original

Science

Politics

what did john mccain do in the next election ?

Politics

Original

Music

Politics

can anybody suggest a good topic for a statistical
survey ?
can anybody suggest a good site for a techno ?

can anybody suggest a good topic for a student visa
?

Original

Music

can a kidney infection effect a woman &apos;s
<unk> cycle ?
can anyone give me a good ﬁlm <unk> ?

Politics

can a landlord ofﬁcer have a <unk> <unk> ?

Original

Music

Politics

Original

Music

Politics

sweating <unk>

where does the term &quot;
&quot; come from ?
where does the term &quot; <unk> &quot; come
from ?
where does the term &quot; <unk> &quot; come
from ?

what other <unk> sources are there than burning
fossil fuels .
what other <unk> are / who are the greatest gui-
tarist currently on tv today ?
what other <unk> are there for veterans who lives
?

Original

Science

Politics

Original

Science

Politics

Original

Science

Politics

Original

Science

Politics

do people who quote entire poems or song lyrics
ever actually get chosen best answer ?
do you think that scientists learn about human
anatomy and physiology of life ?
do people who knows anything about the recent is-
sue of <unk> leadership ?

from big brother , what is the girls name who had
<unk> in her apt ?
in big bang what is the <unk> of <unk> , what is
the difference between <unk> and <unk> ?
is big brother in the <unk> what do you think of
her ?

where is the tickets for the ﬁlming of the suite life
of zack and cody ?
where is the best place of the blood stream for the
production of the cell ?
where is the best place of the navy and the senate
of the union ?

the <unk> singers was a band in 1963 who had a
hit called <unk> man ?
the <unk> river in a <unk> was created by a <unk>
who was born in the last century ?
the <unk> are <unk> in a <unk> who was shot an
<unk> ?

Original

Science

Music

Original

Science

Music

Original

Science

Music

Original

Science

Music

if i move to the usa do i lose my pension in canada
?
if i move the <unk> in the air i have to do my math
homework ?
if i move to the music do you think i feel better ?

what is your reﬂection on what will be our organi-
zations in the future ?
what is your opinion on what will be the future in
our future ?
what is your favorite music videos on the may i ﬁnd
?

wouldn &apos;t it be fun if we the people veto or
passed bills ?
isnt it possible to be cloned if we put the moon or
it ?
isnt it possible or if we &apos;re getting married ?

can anyone tell me how i could go about interview-
ing north vietnamese soldiers ?
can anyone tell me how i could ﬁnd how to build a
robot ?
can anyone tell me how i could ﬁnd out about my
parents ?

what is the ﬁrst metal band in the early 60 &apos;s
..... ? ? ? ?
what is the ﬁrst country in the universe ?

Original

Science

if the us did not exist would the world be a better
place ?
if the world did not exist , would it be possible ?

who is the ﬁrst president in the usa ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?

Music

if you could not have a thing who would it be ?

Figure 9: Full sheet of Yahoo topic transfer on titles.

Adversarially Regularized Autoencoders

Yahoo Topic Transfer on Answers

from Science

from Music

from Politics

Original

Music

take 1ml of hcl ( concentrated ) and dilute it to 50ml
.
take em to you and shout it to me

Original

Science

Politics

take bribes to islam and it will be punished .

Politics

Original

oils do not do this , they do not &quot; set &quot; .

Original

Music

Politics

cucumbers do not do this , they do not &quot; do
&quot; .
corporations do not do this , but they do not .

Science

Politics

all three are fabulous artists , with just incredible
talent ! !
all three are genetically bonded with water , but just
as many substances , are capable of producing a
special case .
all three are competing with the government , just
as far as i can .

Original

Science

4 years of an idiot in ofﬁce + electing the idiot again
= ?
4 years of an idiot in the ofﬁce of science ?

Music

4 ) <unk> in an idiot , the idiot is the best of the
two points ever !

she , too , wondered about the underwear outside
the clothes .
she , too , i know , the clothes outside the clothes .

Original

Science

send me $ 100 and i &apos;ll send you a copy -
honest .
send me an email and i &apos;ll send you a copy .

she , too , i think that the cops are the only thing
about the outside of the u.s. .

Music

send me $ 100 and i &apos;ll send you a copy .

Original

Music

Politics

the average high temps in jan and feb are about 48
deg .
the average high school in seattle and is about 15
minutes .
the average high infantry division is in afghanistan
and alaska .

Original

Science

Politics

i like rammstein and i don &apos;t speak or under-
stand german .
i like googling and i don &apos;t understand or
speak .
i like mccain and i don &apos;t care about it .

Original

Science

Music

wills can be <unk> , or typed and signed without
needing an attorney .
euler can be <unk> , and without any type of oper-
ations , or <unk> .
madonna can be <unk> , and signed without open-
ing or <unk> .

Original

Music

the light from you lamps would move away from
you at light speed
the light from you tube would move away from you

Science

Original

mark is great , but the guest hosts were cool too !

Original

hungary : 20 january 1945 , ( formerly a member
of the axis )
nh3 : 20 january , 78 ( a )

Science

Politics

the light from you could go away from your state

Politics

Music

1966 - 20 january 1961 ( a ) 1983 song

mark is great , but the water will be too busy for the
same reason .
mark twain , but the great lakes , the united states
of america is too busy .

they all offer terriﬁc information about the cast and
characters , ...
they all offer insight about the characteristics of the
earth , and are composed of many stars .
they all offer legitimate information about the inva-
sion of iraq and the u.s. , and all aspects of history
.

Original

Music

Politics

van <unk> , on the other hand , had some serious
issues ...
van <unk> on the other hand , had some serious
issues .
van <unk> , on the other hand , had some serious
issues .

Original

Science

Politics

Original

Science

bulgaria : 8 september 1944 , ( formerly a member
of the axis )
moreover , 8 ˆ3 + ( x + 7 ) ( x ˆ2 ) = ( a ˆ2 )

Music

harrison : 8 september 1961 ( a ) ( 1995 )

Original

Music

Politics

just multiply the numerator of one fraction by that
of the other .
just multiply the fraction of the other one that
&apos;s just like it .
just multiply the same fraction of other countries .

Original

Music

civil engineering is still an umbrella ﬁeld com-
prised of many related specialties .
civil rights is still an art union .

Politics

civil law is still an issue .

Original

but there are so many more i can &apos;t think of !

Original

Science

Politics

but there are so many more of the number of ques-
tions .
but there are so many more of the can i think of
today .

Science

Music

Original

i love zach he is sooo sweet in his own way !

Original

Science

Politics

the answer is he &apos;s deﬁnitely in his own way
!
i love letting he is sooo smart in his own way !

Science

Music

anyone who doesnt have a billion dollars for all the
publicity cant win .
anyone who doesnt have a decent chance is the
same for all the other .
anyone who doesnt have a lot of the show for the
publicity .

the theory is that cats don &apos;t take to being tied
up but thats <unk> .
the theory is that cats don &apos;t grow up to
<unk> .
the theory is that dumb but don &apos;t play <unk>
to <unk> .

Original

Music

Politics

Original

Music
Politics

Original

Music

Politics

h2o2 ( hydrogen peroxide ) naturally decomposes
to form o2 and water .
jackie and brad pitt both great albums and they are
my fav .
kennedy and blair hate america to invade them .

Original

Science

Politics

remember the industry is very shady so keep your
eyes open !
remember the amount of water is so very important
.
remember the amount of time the politicians are
open your mind .

Original

Science

Music

the fear they are trying to instill in the common man
is based on what ?
the fear they are trying to ﬁnd the common ancestor
in the world .
the fear they are trying to ﬁnd out what is wrong in
the song .

the quieter it gets , the more white noise you can
here .
the fray it gets , the more you can hear .
the gop gets it , the more you can here .

Original

but can you fake it , for just one more show ?

Original

Science
Politics

but can you fake it , just for more than one ?
but can you fake it for more than one ?

Science
Music

h2co3 ( carbonic acid ) naturally decomposes to
form water and co2 .
phoebe and jack , he &apos;s gorgeous and she
loves to get him !
nixon ( captured ) he lied and voted for bush to
cause his country .

Original

Science

i am going to introduce you to the internet movie
database .
i am going to investigate the internet to google .

Original

Science

Politics

i am going to skip the internet to get you checked .

Music

think about how much planning and people would
have to be involved in what happened .
think about how much time would you have to do .
think about how much money and what would be
<unk> about in the world ?

this restricts the availability of cash to them and
other countries too start banning them .
this reduces the intake of the other molecules to pro-
duce them and thus are too large .
this is the cheapest package of them too .

Figure 10: Full sheet of Yahoo topic transfer on answers.

