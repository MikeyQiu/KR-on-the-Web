6
1
0
2
 
t
c
O
 
5
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
8
7
4
0
.
0
1
6
1
:
v
i
X
r
a

An Adaptive Test of Independence with Analytic Kernel Embeddings

Wittawat Jitkrittum,1

Zoltán Szabó,2∗ Arthur Gretton1

1Gatsby Computational Neuroscience Unit, University College London
2Department of Applied Mathematics, CMAP, École Polytechnique

wittawat@gatsby.ucl.ac.uk
zoltan.szabo@polytechnique.edu
arthur.gretton@gmail.com

October 18, 2016

Abstract

A new computationally eﬃcient dependence measure, and
an adaptive statistical test of independence, are proposed.
The dependence measure is the diﬀerence between ana-
lytic embeddings of the joint distribution and the product
of the marginals, evaluated at a ﬁnite set of locations
(features). These features are chosen so as to maximize a
lower bound on the test power, resulting in a test that
is data-eﬃcient, and that runs in linear time (with re-
spect to the sample size n). The optimized features can
be interpreted as evidence to reject the null hypothesis,
indicating regions in the joint domain where the joint
distribution and the product of the marginals diﬀer most.
Consistency of the independence test is established, for
an appropriate choice of features. In real-world bench-
marks, independence tests using the optimized features
perform comparably to the state-of-the-art quadratic-
time HSIC test, and outperform competing O(n) and
O(n log n) tests.

1

Introduction

We consider the design of adaptive, nonparametric sta-
tistical tests of dependence: that is, tests of whether
a joint distribution Pxy factorizes into the product of
marginals PxPy. While classical tests of dependence,
such as Pearson’s correlation and Kendall’s τ , are able to
detect monotonic relations between univariate variables,
more modern tests can address complex interactions, for
instance changes in variance of X with the value of Y .
Key to many recent tests is to examine covariance or
correlation between data features. These interactions
become signiﬁcantly harder to detect, and the features

∗Zoltán

Szabó’s
0000-0001-6183-7603.

ORCID

ID:

http://orcid.org/

are more diﬃcult to design when the data reside in high
dimensions.

A basic nonlinear dependence measure is the Hilbert-
Schmidt Independence Criterion (HSIC), which is the
Hilbert-Schmidt norm of the covariance operator between
feature mappings of the random variables [Gretton et al.,
2005, 2008]. Each random variable X and Y is mapped to
a respective reproducing kernel Hilbert space Hk and Hl.
For suﬃciently rich mappings, the covariance operator
norm is zero if and only if the variables are indepen-
dent. A second basic nonlinear dependence measure is
the smoothed diﬀerence between the characteristic func-
tion of the joint distribution, and that of the product
of marginals. When a particular smoothing function is
used, the statistic corresponds to the covariance between
distances of X and Y variable pairs [Feuerverger, 1993,
Székely et al., 2007, Székely and Rizzo, 2009], yielding
a simple test statistic. It has been shown by Sejdinovic
et al. [2013] that the distance covariance (and its gener-
alization to semi-metrics) is an instance of HSIC for an
appropriate choice of kernels. A disadvantage of these
feature covariance statistics, however, is that they require
quadratic time to compute (besides in the special case of
the distance covariance with univariate real-valued vari-
ables, where Huo and Székely [2014] achieve an O(n log n)
cost). Moreover, the feature covariance statistics have
intractable null distributions, and either a permutation
approach or the solution of an expensive eigenvalue prob-
lem [e.g. Zhang et al., 2011] is required for consistent
estimation of the quantiles. Several approaches were
proposed by Zhang et al. [2016] to obtain faster tests
along the lines of HSIC. These include computing HSIC
on ﬁnite-dimensional feature mappings chosen as ran-
dom Fourier features (RFFs) [Rahimi and Recht, 2008], a
block-averaged statistic, and a Nyström approximation to
the statistic. Key to each of these approaches is a more
eﬃcient computation of the statistic and its threshold un-

1

der the null distribution: for RFFs, the null distribution
is a ﬁnite weighted sum of χ2 variables; for the block-
averaged statistic, the null distribution is asymptotically
normal; for Nyström, either a permutation approach is
employed, or the spectrum of the Nyström approximation
to the kernel matrix is used in approximating the null
distribution. Each of these methods costs signiﬁcantly
less than the O(n2) cost of the full HSIC (the cost is
linear in n, but also depends quadratically on the number
of features retained). A potential disadvantage of the
Nyström and Fourier approaches is that the features are
not optimized to maximize test power, but are chosen
randomly. The block statistic performs worse than both,
due to the large variance of the statistic under the null
(which can be mitigated by observing more data).

In addition to feature covariances, correlation measures
have also been developed in inﬁnite dimensional feature
spaces: in particular, Bach and Jordan [2002], Fukumizu
et al. [2008] proposed statistics on the correlation operator
in a reproducing kernel Hilbert space. While convergence
has been established for certain of these statistics, their
computational cost is high at O(n3), and test thresholds
have relied on permutation. A number of much faster
approaches to testing based on feature correlations have
been proposed, however. For instance, Dauxois and Nkiet
[1998] compute statistics of the correlation between ﬁnite
sets of basis functions, chosen for instance to be step func-
tions or low order B-splines. The cost of this approach is
O(n). This idea was extended by Lopez-Paz et al. [2013],
who computed the canonical correlation between ﬁnite
sets of basis functions chosen as random Fourier features;
in addition, they performed a copula transform on the
inputs, with a total cost of O(n log n). Finally, space
partitioning approaches have also been proposed, based
on statistics such as the KL divergence, however these
apply only to univariate variables [Heller et al., 2016], or
to multivariate variables of low dimension [Gretton and
Györﬁ, 2010] (that said, these tests have other advantages
of theoretical interest, notably distribution-independent
test thresholds).

The approach we take is most closely related to HSIC
on a ﬁnite set of features. Our simplest test statistic,
the Finite Set Independence Criterion (FSIC), is an av-
erage of covariances of analytic functions (i.e., features)
deﬁned on each of X and Y . A normalized version of
the statistic (NFSIC) yields a distribution-independent
asymptotic test threshold. We show that our test is con-
sistent, despite a ﬁnite number of analytic features being
used, via a generalization of arguments in Chwialkowski
et al. [2015]. As in recent work on two-sample testing
by Jitkrittum et al. [2016], our test is adaptive in the
sense that we choose our features on a held-out valida-
tion set to optimize a lower bound on the test power.
The design of features for independence testing turns out

to be quite diﬀerent to the case of two-sample testing,
however: the task is to ﬁnd correlated feature pairs on
the respective marginal domains, rather than attempting
to ﬁnd a single, high-dimensional feature representation
for the entire (x, y) (as we would need to do if we were
comparing distributions Pxy and Qxy, rather than testing
a speciﬁc property of Pxy). We demonstrate the perfor-
mance of our tests on several challenging artiﬁcial and
real-world datasets, including detection of dependence
between music and its year of appearance, and between
videos and captions. In these experiments, we outperform
competing linear and O(n log n) time tests.

2

Independence Criteria and Sta-
tistical Tests

We introduce two test statistics: ﬁrst, the Finite Set
Independence Criterion (FSIC), which builds on the prin-
ciple that dependence can be measured in terms of the
covariance between data features. Next, we propose a nor-
malized version of this statistic (NFSIC), with a simpler
asymptotic distribution when Pxy = PxPy. We show how
to select features for the latter statistic to maximize a
lower bound on the power of its corresponding statistical
test.

2.1 The Finite Set Independence Crite-

rion

We begin by introducing the Hilbert-Schmidt Indepen-
dence Criterion (HSIC) as proposed in Gretton et al.
[2005], since our unnormalized statistic is built along sim-
ilar lines. Consider two random variables X ∈ X ⊂ Rdx
and Y ∈ Y ⊂ Rdy . Denote by Pxy the joint distribution
between X and Y ; Px and Py are the marginal distribu-
tions of X and Y . Let ⊗ denote the tensor product, such
that (a ⊗ b) c = a (cid:104)b, c(cid:105). Assume that k : X × X → R
and l : Y × Y → R are positive deﬁnite kernels associated
with reproducing kernel Hilbert spaces (RKHS) Hk and
Hl, respectively. Let (cid:107) · (cid:107)HS be the norm on the space
of Hl → Hk Hilbert-Schmidt operators. Then, HSIC
between X and Y is deﬁned as
HSIC(X, Y ) = (cid:13)
(cid:13)
2
(cid:13)µxy − µx ⊗ µy
(cid:13)
HS
= E(x,y),(x(cid:48),y(cid:48)) [k(x, x(cid:48))l(y, y(cid:48))]

+ ExEx(cid:48)[k(x, x(cid:48))]EyEy(cid:48)[l(y, y(cid:48))]
− 2E(x,y) [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] ,

(1)

is an independent copy of x.

where Ex := Ex∼Px , Ey := Ey∼Py , E(x,y) := E(x,y)∼Pxy ,
and x(cid:48)
The mean
embedding of Pxy belongs to the space of Hilbert-
Schmidt operators from Hl to Hk, µxy := (cid:82)
X ×Y k(x, ·) ⊗
l(y, ·) dPxy(x, y) ∈ HS(Hl, Hk), and the marginal mean

2

embeddings are µx := (cid:82)
X k(x, ·) dPx(x) ∈ Hk and
µy := (cid:82)
Y l(y, ·) dPy(y) ∈ Hl [Smola et al., 2007]. Gret-
ton et al. [2005, Theorem 4] show that if the kernels k
and l are universal [Steinwart and Christmann, 2008] on
compact domains X and Y, then HSIC(X, Y ) = 0 if and
only if X and Y are independent. Alternatively, Gretton
[2015] shows that it is suﬃcient for each of k and l to be
characteristic to their respective domains (meaning that
distribution embeddings are injective in each marginal
domain: see Sriperumbudur et al. [2010]). Given a joint
sample Zn = {(xi, yi)}n
i=1 ∼ Pxy, an empirical estimator
of HSIC can be computed in O(n2) time by replacing the
population expectations in (1) with their corresponding
empirical expectations based on Zn.

(cid:80)J

We now propose our new linear-time dependence mea-
sure, the Finite Set Independence Criterion (FSIC). Let
X ⊂ Rdx and Y ⊂ Rdy be open sets. Deﬁne the empir-
ical measure ν := 1
i=1 δ(vi,wi) over J test locations
J
VJ := {(vi, wi)}J
i=1 ⊂ X × Y where δt denotes the Dirac
measure centered on t, and (vi, wi) are realizations from
an absolutely continuous distribution (wrt the Lebesgue
measure). Write Exy for E(x,y)∼Pxy . The idea is to
see µxy(v, w) = Exy[k(x, v)l(y, w)], µx(v) = Ex[k(x, v)]
and µy(w) = Ey[l(y, w)] as smooth functions, and con-
sider an L2(X × Y, ν) distance between µxy and µxµy
instead of a Hilbert-Schmidt distance as in HSIC [Gret-
ton et al., 2005]. Let µxµy(x, y) := µx(x)µy(y). FSIC is
deﬁned as

FSIC2(X, Y ) := (cid:107)µxy − µxµy(cid:107)2

L2(X ×Y,ν)

(µxy(x, y) − µx(x)µy(y))2 dν(x, y)

=

:=

(cid:90)

(cid:90)

X

1
J

Y
J
(cid:88)

i=1

u(vi, wi)2 =

(cid:107)u(cid:107)2

2, where

1
J

u(v, w) := µxy(v, w) − µx(v)µy(w)
= Exy[k(x, v)l(y, w)] − Ex[k(x, v)]Ey[l(y, w)],
= covxy[k(x, v), l(y, w)],

(2)

and u := (u(v1, w1), . . . , u(vJ , wJ ))(cid:62).

Our ﬁrst result in Proposition 2 states that FSIC(X, Y )
almost surely deﬁnes a dependence measure for the ran-
dom variables X and Y , provided that the product kernel
on the joint space X × Y is characteristic and analytic
(see Deﬁnition 1).

Deﬁnition 1 (Analytic kernels [Chwialkowski et al.,
2015]). Let X be an open set in Rd. A positive deﬁ-
nite kernel k : X × X → R is said to be analytic on its
domain X × X if for all v ∈ X , f (x) := k(x, v) is an
analytic function on X .

Assumption A. The kernels k : X × X → R and l :
Y × Y → R are bounded by Bk and Bl respectively, and

the product kernel g((x, y), (x(cid:48), y(cid:48))) := k(x, x(cid:48))l(y, y(cid:48)) is
characteristic [Sriperumbudur et al., 2010, Deﬁnition 6],
and analytic (Deﬁnition 1) on (X × Y) × (X × Y).

Proposition 2 (FSIC is a dependence measure). Assume
that
1. Assumption A holds.
2. The test locations VJ = {(vi, wi)}J

i=1 are drawn from

an absolutely continuous distribution.

Then, almost surely, FSIC(X, Y ) = 1√
J
only if X and Y are independent.

(cid:107)u(cid:107)2 = 0 if and

Proof. Since g is characteristic, the mean embedding map
Πg : P (cid:55)→ E(x,y)∼P [g((x, y), ·)] is injective [Sriperum-
budur et al., 2010, Section 3], where P is a probability
distribution on X × Y. Since g is analytic, by Lemma 10
(Appendix), µxy and µxµy are analytic functions. Thus,
Lemma 11 (Appendix, setting Λ = Πg) guarantees that
FSIC(X, Y ) = 0 ⇐⇒ Pxy = PxPy ⇐⇒ X and Y are
independent almost surely.

FSIC uses µxy as a proxy for Pxy, and µxµy as a proxy
for PxPy. Proposition 2 suggests that, to detect the
dependence between X and Y , it is suﬃcient to evaluate at
a ﬁnite number of locations (deﬁned by VJ ) the diﬀerence
of the population joint embedding µxy and the embedding
of the product of the marginal distributions µxµy. A brief
explanation to justify this property is as follows. If Pxy =
PxPy, then ρ(v, w) := µxy(v, w)−µxµy(v, w) is zero, and
FSIC(X, Y ) = 0 for any VJ . If Pxy (cid:54)= PxPy, then ρ will
not be a zero function, since the mean embedding map is
injective (require the product kernel to be characteristic).
Using the same argument as in Chwialkowski et al. [2015],
since k and l are analytic, ρ is also analytic, and the
set of roots R := {(v, w) | ρ(v, w) = 0} has Lebesgue
measure zero. Thus, it is suﬃcient to draw (v, w) from an
absolutely continuous distribution, as we are guaranteed
that (v, w) /∈ R giving FSIC(X, Y ) > 0.

For FSIC to be a dependence measure, the product
kernel is required to be characteristic and analytic. We
next show in Proposition 3 that Gaussian kernels k and l
yield such a product kernel.

3

is

=

(A

characteristic

product
and
exp (cid:0)−(x − x(cid:48))(cid:62)A(x − x(cid:48))(cid:1)

Gaussian
of
Proposition
analytic). Let
kernels
k(x, x(cid:48))
and
l(y, y(cid:48)) = exp (cid:0)−(y − y(cid:48))(cid:62)B(y − y(cid:48))(cid:1) be Gaussian
kernels on Rdx × Rdx and Rdy × Rdy respectively,
for positive deﬁnite matrices A and B.
Then,
g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteristic and
analytic on (Rdx × Rdy ) × (Rdx × Rdy ).
Proof (sketch). The main idea is to use the fact a Gaus-
sian kernel is analytic, and a product of Gaussian kernels
is a Gaussian kernel on the pair of variables. See the full
proof in Appendix D.

3

i=1

Plug-in Estimator We now give an empirical estima-
tor of FSIC. Assume that we observe a joint sample Zn :=
i.i.d.∼ Pxy. Unbiased estimators of µxy(v, w)
{(xi, yi)}n
(cid:80)n
and µxµy(v, w) are ˆµxy(v, w) := 1
i=1 k(xi, v)l(yi, w)
n
(cid:80)
and (cid:91)µxµy(v, w) :=
j(cid:54)=i k(xi, v)l(yj, w),
respectively. A straightforward empirical estimator of
FSIC2 is then given by

1
n(n−1)

(cid:80)n

i=1

(cid:92)FSIC2(Zn) =

ˆu(vi, wi)2,

1
J

J
(cid:88)

i=1

ˆu(v, w) := ˆµxy(v, w) − (cid:91)µxµy(v, w)

=

2
n(n − 1)

(cid:88)

i<j

h(v,w)((xi, yi), (xj, yj)),

(3)

(4)

J ˆu(cid:62) ˆu.

1
h(v,w)((x, y), (x(cid:48), y(cid:48)))
2 (k(x, v) −
where
k(x(cid:48), v))(l(y, w) − l(y(cid:48), w)).
For conciseness, we
deﬁne ˆu := (ˆu1, . . . , ˆuJ )(cid:62) ∈ RJ where ˆui := ˆu(vi, wi) so
that (cid:92)FSIC2(Zn) = 1

:=

(cid:92)FSIC2 can be eﬃciently computed in O((dx + dy)Jn)
time [see (3)], assuming that the runtime complexity of
evaluating k(x, v) is O(dx) and that of l(y, w) is O(dy).
The unbiasedness of (cid:91)µxµy is necessary for (4) to be a U-
statistic. This fact and the rewriting of (cid:92)FSIC2 in terms of
h(v,w)((x, y), (x(cid:48), y(cid:48))) will be exploited when the asymp-
totic distribution of ˆu is derived (Proposition 4).

Since FSIC satisﬁes FSIC(X, Y ) = 0 ⇐⇒ X ⊥ Y , in
principle its empirical estimator can be used as a test
statistic for an independence test proposing a null hy-
pothesis H0 : “X and Y are independent” against an
alternative H1 : “X and Y are dependent”. The null
distribution (i.e., distribution of the test statistic assum-
ing that H0 is true) is challenging to obtain, however
and depends on the unknown Pxy. This prompts us to
consider a normalized version of FSIC whose asymptotic
null distribution of a convenient form. We ﬁrst derive the
asymptotic distribution of ˆu in Proposition 4, which we
use to derive the normalized test statistic in Theorem 5.
As a shorthand, we write z := (x, y), and t := (v, w).

Proposition 4 (Asymptotic distribution of ˆu). Deﬁne
˜k(x, v) := k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) := l(y, w) −
Ey(cid:48)l(y(cid:48), w). Then, under both H0 and H1, for any ﬁxed
locations t and t(cid:48),
covz[ˆu(t), ˆu(t(cid:48))] n→∞−−−−→ covz[˜k(x, v)˜l(y, w), ˜k(x, v(cid:48))˜l(y, w(cid:48))]
= Exy[(cid:0)˜k(x, v)˜l(y, w) − u(t)(cid:1)(cid:0)˜k(x, v(cid:48))˜l(y, w(cid:48)) − u(t(cid:48))(cid:1)],

where u(v, w) is given in (2), and ˆu(v, w) is deﬁned
in (4). Second, if 0 < covz[ˆu(ti), ˆu(ti)] < ∞ for i =
n(ˆu − u) d→ N (0, Σ) as n → ∞, where
1, . . . , J, then
Σij = cov[ˆu(ti), ˆu(tj)] and u := (u(t1), . . . , u(tJ ))(cid:62).

√

Proof. We ﬁrst note that for a ﬁxed t = (v, w), ˆu(v, w) is
a one-sample second-order U-statistic [Serﬂing, 2009, Sec-
tion 5.1.3] with a U-statistic kernel ht where ht(a, b) =

ht(b, a). Thus, by Kowalski and Tu [2008, Section 5.1,
Theorem 1], it follows directly that cov[ˆu(t), ˆu(t(cid:48))] =
4covz[Eaht(z, a), Ebht(cid:48)(z, b)]. Substituting ht with its
deﬁnition yields the ﬁrst claim, where we note that
Exy[˜k(x, v)˜l(y, w)] = u(v, w).

For the second claim, since ˆu is a multivariate one-
sample U-statistic, by Lehmann [1999, Theorem 6.1.6]
and Kowalski and Tu [2008, Section 5.1, Theorem 1], it
n(ˆu − u) d→ N (0, Σ) as n → ∞, where

√

follows that
Σij = cov[ˆu(ti), ˆu(tj)].

Recall from Proposition 2 that u = 0 holds almost
surely under H0. The asymptotic normality in the second
claim of Proposition 4 implies that n(cid:92)FSIC2 = n
J ˆu(cid:62) ˆu con-
verges in distribution to a sum of J dependent weighted
χ2 random variables. The dependence comes from the
fact that the coordinates ˆu1 . . . , ˆuJ of ˆu all depend on
the sample Zn. This null distribution is not analytically
tractable, and requires a large number of simulations to
compute the rejection threshold Tα for a given signiﬁcance
value α.

2.2 Normalized FSIC and Adaptive Test

For the purpose of an independence test, we will consider
a normalized variant of (cid:92)FSIC2, which we call (cid:92)NFSIC2,
whose tractable asymptotic null distribution is χ2(J), the
chi-squared distribution with J degrees of freedom. We
then show that the independence test deﬁned by (cid:92)NFSIC2
is consistent. These results are given in Theorem 5.
Theorem 5 (Independence test using (cid:92)NFSIC2 is con-
sistent). Let ˆΣ be a consistent estimate of Σ based on
the joint sample Zn. The (cid:92)NFSIC2 statistic is deﬁned as
ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu where γn ≥ 0 is a regulariza-
tion parameter. Assume that

(cid:17)−1

1. Assumption A holds.

2. Σ is invertible almost surely with respect to VJ =
i=1 drawn from an absolutely continuous

{(vi, wi)}J
distribution.

3. limn→∞ γn = 0.
Then, for any k, l and VJ satisfying the assumptions,
1. Under H0, ˆλn
2. Under H1, for any r ∈ R, limn→∞ P

= 1
almost surely. That is, the independence test based on
(cid:92)NFSIC2 is consistent.

d→ χ2(J) as n → ∞.

(cid:16)ˆλn ≥ r

(cid:17)

Proof (sketch) . Under H0, nˆu(cid:62)( ˆΣ + γnI)−1 ˆu asymptot-
ically follows χ2(J) because
nˆu is asymptotically nor-
mally distributed (see Proposition 4). Claim 2 builds on
the result in Proposition 2 stating that u (cid:54)= 0 under H1;

√

4

it follows using the convergence of ˆu to u. The full proof
can be found in Appendix E.

Theorem 5 states that if H1 holds, the statistic can
be arbitrarily large as n increases, allowing H0 to be
rejected for any ﬁxed threshold. Asymptotically the test
threshold Tα is given by the (1 − α)-quantile of χ2(J) and
is independent of n. The assumption on the consistency
of ˆΣ is required to obtain the asymptotic chi-squared
distribution. The regularization parameter γn is to ensure
that ( ˆΣ + γnI)−1 can be stably computed. In practice,
γn requires no tuning, and can be set to be a very small
constant.

The next proposition states that the computational
complexity of the (cid:92)NFSIC2 estimator is linear in both
the input dimension and sample size, and that it can
be expressed in terms of the K =[K ij] = [k(vi, xj)] ∈
RJ×n, L = [Lij] = [l(wi, yj)] ∈ RJ×n matrices.

Proposition 6 (An empirical estimator of (cid:92)NFSIC2). Let
1n := (1, . . . , 1)(cid:62) ∈ Rn. Denote by ◦ the element-wise
matrix product. Then,
1. ˆu = (K◦L)1n

n−1 − (K1n)◦(L1n)

n(n−1)

.

2. A consistent estimator for Σ is ˆΣ = ΓΓ(cid:62)

n where

Γ := (K − n−1K1n1(cid:62)
n ) ◦ (L − n−1L1n1(cid:62)
ˆub = n−1 (K ◦ L) 1n − n−2 (K1n) ◦ (L1n) .

n ) − ˆub1(cid:62)
n ,

Assume that the complexity of the kernel evaluation is
linear in the input dimension. Then the test statistic
ˆλn = nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu can be computed in O(J 3 +
J 2n + (dx + dy)Jn) time.

(cid:17)−1

Proof (sketch). Claim 1 for ˆu is straightforward. The
expression for ˆΣ in claim 2 follows directly from the
asymptotic covariance expression in Proposition 4. The
consistency of ˆΣ can be obtained by noting that the
ﬁnite sample bound for P((cid:107) ˆΣ − Σ(cid:107)F > t) decreases as
n increases. This is implicitly shown in Appendix F.2.2
and its following sections.

Although the dependency of the estimator on J is cubic,
we empirically observe that only a small value of J is
required (see Section 3). The number of test locations
J relates to the number of regions in X × Y of pxy and
pxpy that diﬀer (see Figure 1). In particular, J need not
increase with n for test consistency.

Our ﬁnal theoretical result gives a lower bound on
the test power of (cid:92)NFSIC2 i.e., the probability of correctly
rejecting H0. We will use this lower bound as the objective
function to determine VJ and the kernel parameters. Let
(cid:107) · (cid:107)F be the Frobenius norm.

Theorem 7 (A lower bound on the test power). Let
NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let K be a kernel
class for k, L be a kernel class for l, and V be a collection
with each element being a set of J locations. Assume that
that
1. There
and

ﬁnite Bk
supk∈K supx,x(cid:48)∈X |k(x, x(cid:48))|
supl∈L supy,y(cid:48)∈Y |l(y, y(cid:48))| ≤ Bl.

and Bl
≤

such
Bk

exist

2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)Σ−1(cid:107)F < ∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test
power satisﬁes P

≥ L(λn) where

(cid:16)ˆλn ≥ r

(cid:17)

L(λn) = 1 − 62e−ξ1γ2

− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

n(λn−r)2/n − 2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

1

2JB2,
(cid:98)·(cid:99) is the ﬂoor function, ξ1 :=
B := BkBl, ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 :=
28B4J 2c2
J ˜c, and B∗ is
a constant depending on only Bk and Bl. Moreover, for
suﬃciently large ﬁxed n, L(λn) is increasing in λn.

1J 2B∗ , ξ2 := 72c2

1, c1 := 4B2J

J ˜c, c2 := 4B

32c2

√

√

(cid:17)

(cid:17)

(cid:16)

(cid:111)

(cid:110)

y,u]

y ∈ [σ2

x ∈ [σ2

y,l < σ2

(x, v) (cid:55)→ exp

(y, w) (cid:55)→ exp

the kernels k and l,

− (cid:107)x−v(cid:107)2
2σ2
x
x,l < σ2
| σ2

in Appendix F. To put
We provide the proof
let θx and θy be the
Theorem 7 into perspective,
parameters of
respectively.
We denote by θ = {θx, θy, VJ } the collection of
Assume that
all tuning parameters of the test.
x,l, σ2
| σ2
x,u]
K =
=:
Kg for some 0 < σ2
x,u < ∞ and L =
(cid:111)
(cid:110)
(cid:16)
− (cid:107)y−w(cid:107)2
y,l, σ2
=: Lg for
2σ2
y
some 0 < σ2
y,u < ∞ are Gaussian kernel classes.
Then, in Theorem 7, B = Bk = Bl = 1, and B∗ = 2.
The assumption ˜c < ∞ is a technical condition to guar-
antee that the test power lower bound is ﬁnite for all
θ deﬁned by the feasible sets K, L, and V. Let V(cid:15),r :=
(cid:8)VJ | (cid:107)vi(cid:107)2, (cid:107)wi(cid:107)2 ≤ r and (cid:107)vi − vj(cid:107)2
2 ≥
(cid:15), for all i (cid:54)= j(cid:9). If we set K = Kg, L = Lg, and V = V(cid:15),r
for some (cid:15), r > 0, then ˜c < ∞ as Kg, Lg, and V(cid:15),r are
compact. In practice, these conditions do not necessarily
create restrictions as they almost always hold implicitly.
We show in Appendix C that the objective function used
to choose VJ will discourage any two locations to be in
the same neighborhood.

2 + (cid:107)wi − wj(cid:107)2

Parameter Tuning The test power lower bound
L(λn) in Theorem 7 is a function of λn = nu(cid:62)Σ−1u
which is the population counterpart of the test statistic
ˆλn. As in FSIC, it can be shown that λn = 0 if and
only if X are Y are independent (from Proposition 2).
If X and Y are dependent, then λn > 0. According to
Theorem 7, for a suﬃciently large n, the test power lower
bound is increasing in λn. One can therefore think of
λn (a function of θ) as representing how easily the test
rejects H0 given a problem Pxy. The higher the λn, the

5

3 Experiments

In this section, we empirically study the performance
of the proposed method on both toy (Section 3.1) and
real-life problems (Section 3.2). Our interest is in the
performance of linear-time tests on challenging problems
which require a large sample size to be able to accurately
reveal the dependence. All the code is available at https:
//github.com/wittawatj/fsic-test.

We compare the proposed NFSIC with optimization
(NFSIC-opt) to ﬁve multivariate nonparametric tests.
The (cid:92)NFSIC2 test without optimization (NFSIC-med) acts
as a baseline, allowing the eﬀect of parameter optimization
to be clearly seen. For pedagogical reason, we consider
the original HSIC test of Gretton et al. [2005] denoted by
QHSIC, which is a quadratic-time test. Nyström HSIC
(NyHSIC) uses a Nyström approximation to the kernel
matrices of X and Y when computing the HSIC statistic.
FHSIC is another variant of HSIC in which a random
Fourier feature approximation [Rahimi and Recht, 2008]
to the kernel is used. NyHSIC and FHSIC are studied in
Zhang et al. [2016] and can be computed in O(n), with
quadratic dependency on the number of inducing points
in NyHSIC, and quadratic dependency in the number
of random features in FHSIC. Finally, the Randomized
Dependence Coeﬃcient (RDC) proposed in Lopez-Paz
et al. [2013] is also considered. The RDC can be seen as
the primal form (with random Fourier features) of the
kernel canonical correlation analysis of Bach and Jordan
[2002] on copula-transformed data. We consider RDC
as a linear-time test even though preprocessing by an
empirical copula transform costs O((dx + dy)n log n).

We use Gaussian kernel classes Kg and Lg for both
X and Y in all the methods. Except NFSIC-opt, all
other tests use full sample to conduct the indepen-
dence test, where the Gaussian widths σx and σy are
set according to the widely used median heuristic i.e.,
σx = median ({(cid:107)xi − xj(cid:107)2 | 1 ≤ i < j ≤ n}), and σy is
set in the same way using {yi}n
i=1. The J locations for
NFSIC-med are randomly drawn from the standard multi-
variate normal distribution in each trial. For a sample of
size n, NFSIC-opt uses half the sample for parameter tun-
ing, and the other disjoint half for the test. We permute
the sample 300 times in RDC1 and HSIC to simulate
from the null distribution and compute the test threshold.
The null distributions for FHSIC and NyHSIC are given
by a ﬁnite sum of weighted χ2(1) random variables given
in Eq. 8 of Zhang et al. [2016]. Unless stated otherwise,
we set the test threshold of the two NFSIC tests to be the
(1 − α)-quantile of χ2(J). To provide a fair comparison,
we set J = 10, use 10 inducing points in NyHSIC, and 10

1We use a permutation test for RDC, following the authors’
(https://github.com/lopezpaz/randomized_

implementation
dependence_coefficient, referred commit: b0ac6c0).

(a) ˆµxy(v, w)

(b) (cid:92)µxµy(v, w)

(c) (cid:98)Σ(v, w)

(d) Statistic ˆλn(v, w)

Figure 1: Illustration of (cid:92)NFSIC2.

greater the lower bound on the test power, and thus the
more likely it is that the test will reject H0 when it is
false.

In light of this reasoning, we propose setting θ to θ∗ =
arg maxθ λn. That this procedure is also valid under H0
can be seen as follows. Under H0, θ∗ = arg maxθ 0 will be
d→ χ2(J)
arbitrary. Since Theorem 7 guarantees that ˆλn
as n → ∞ for any θ, the asymptotic null distribution does
not change by using θ∗. In practice, λn is a population
quantity which is unknown. We propose dividing the
sample Zn into two disjoint sets: training and test sets.
The training set is used to optimize for θ∗, and the test set
is used for the actual independence test with the optimized
θ∗. The splitting is to guarantee the independence of θ∗
and the test sample, which is an assumption of Theorem
5.

we

To

better

under (cid:92)NFSIC2,
visualize
ˆµxy(v, w), (cid:91)µxµy(v, w) and ˆΣ(v, w) as a function of one
In this
test location (v, w) on a simple toy problem.
problem, Y = −X + Z where Z ∼ N (0, 0.32). As we con-
sider only one location (J = 1), ˆΣ(v, w) is a scalar. The
(ˆµxy(v,w)−(cid:92)µxµy(v,w))2
statistic can be written as ˆλn = n
.
ˆΣ(v,w)
These components are shown in Figure 1, where we use
Gaussian kernels for both X and Y , and the horizontal
and vertical axes correspond to v ∈ R and w ∈ R,
respectively.

Intuitively, ˆu(v, w) = ˆµxy(v, w) − (cid:91)µxµy(v, w) captures
the diﬀerence of the joint distribution and the product of
the marginals as a function of (v, w). Squaring ˆu(v, w)
and dividing it by the variance shown in Figure 1c gives
the statistic (also the parameter tuning objective) shown
in Figure 1d. The latter ﬁgure suggests that the parameter
tuning objective function can be non-convex. However,
we note that the non-convexity arises since there are
multiple ways to detect the diﬀerence between the joint
distribution and the product of the marginals. In this case,
the lower left and upper right regions equally indicate the
largest diﬀerence.

6

(a) SG (α = 0.05)

(b) SG (α = 0.05)

(c) Sin

(d) GSign

Figure 2: (a): Runtime. (b): Probability of rejecting H0 as problem parameters vary. Fix n = 4000.

random Fourier features in FHSIC and RDC.

Optimization of NFSIC-opt The parameters of
NFSIC-opt are σx, σy, and J locations of size (dx +
dy)J. We treat all the parameters as a long vector in
R2+(dx+dy)J and use gradient ascent to optimize ˆλn/2.
We observe that initializing VJ by randomly picking J
points from the training sample yields good performance.
The regularization parameter γn in NFSIC is ﬁxed to a
small value, and is not optimized. It is worth emphasizing
that the complexity of the optimization procedure is still
linear in n.

Since FSIC, NyHFSIC and RDC rely on a ﬁnite-
dimensional kernel approximation, these tests are consis-
tent only if both the number of features increases with n.
By constrast, the proposed NFSIC requires only n to go
to inﬁnity to achieve consistency i.e., J can be ﬁxed. We
refer the reader to Appendix C for a brief investigation
of the test power vs. increasing J. The test power does
not necessarily monotonically increase with J.

3.1 Toy Problems

We consider three toy problems: Same Gaussian (SG),
Sinusoid (Sin), and Gaussian Sign (GSign).

1. Same Gaussian (SG). The two variables are inde-
pendently drawn from the standard multivariate normal
distribution i.e., X ∼ N (0, Idx ) and Y ∼ N (0, Idy ) where
Id is the d × d identity matrix. This problem represents
a case in which H0 holds.

2. Sinusoid (Sin). Let pxy be the probability density
of Pxy.
In the Sinusoid problem, the dependency of
X and Y is characterized by (X, Y ) ∼ pxy(x, y) ∝ 1 +
sin(ωx) sin(ωy), where the domains of X , Y = (−π, π)
and ω is the frequency of the sinusoid. As the frequency
ω increases, the drawn sample becomes more similar to
a sample drawn from Uniform((−π, π)2). That is, the
higher ω, the harder to detect the dependency between
X and Y . This problem was studied in Sejdinovic et al.
[2013]. Plots of the density for a few values of ω are
shown in Figures 6 and 7 in the appendix. The main
characteristic of interest in this problem is the local change
in the density function.

3. Gaussian Sign (GSign). In this problem, Y =

|Z| (cid:81)dx
i=1 sgn(Xi), where X ∼ N (0, Idx ), sgn(·) is the sign
function, and Z ∼ N (0, 1) serves as a source of noise. The
full interaction of X = (X1, . . . , Xdx ) is what makes the
problem challenging. That is, Y is dependent on X, yet
it is independent of any proper subset of {X1, . . . , Xd}.
Thus, simultaneous consideration of all the coordinates
of X is required to successfully detect the dependency.

We ﬁx n = 4000 and vary the problem parameters.
Each problem is repeated for 300 trials, and the sample is
redrawn each time. The signiﬁcance level α is set to 0.05.
The results are shown in Figure 2. It can be seen that
in the SG problem (Figure 2b) where H0 holds, all the
tests achieve roughly correct type-I errors at α = 0.05.
In the Sin problem, NFSIC-opt achieves the highest test
power for all considered ω = 1, . . . , 6, highlighting its
strength in detecting local changes in the joint density.
The performance of NFSIC-med is signiﬁcantly lower than
that of NFSIC-opt. This phenomenon clearly emphasizes
the importance of the optimization to place the locations
at the relevant regions in X × Y. RDC has a remarkably
high performance in both Sin and GSign (Figure 2c, 2d)
despite no parameter tuning. Interestingly, both NFSIC-
opt and RDC outperform the quadratic-time QHSIC
in these two problems. The ability to simultaneously
consider interacting features of NFSIC-opt is indicated
by its superior test power in GSign, especially at the
challenging settings of dx = 5, 6. An average trial runtime
for each test in the SG problem is shown in Figure 2a. We
observe that the runtime does not increase with dimension,
as the complexity of all the tests is linear in the dimension
of the input. All the tests are implemented in Python
using a common SciPy Stack.

To investigate the sample eﬃciency of all the tests,
we ﬁx dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in
GSign, and increase n. Figure 3 shows the results. The
quadratic dependency on n in QHSIC makes it infeasible
both in terms of memory and runtime to consider n larger
than 6000 (Figure 3a). In constrast, although not the
most time-eﬃcient, NFSIC-opt has the highest sample-
eﬃciency for GSign, and for Sin in the low-sample regime,
signiﬁcantly outperforming QHSIC. Despite the small
additional overhead from the optimization, we are yet
able to conduct an accurate test with n = 105, dx = dy =

7

(a) SG. dx = dy = 250.

(b) SG. dx = dy = 250.

(c) Sin. ω = 4.

(d) GSign. dx = 4.

Figure 3: (a) Runtime. (b): Probability of rejecting H0 as n increases in the toy problems.

250 in less than 100 seconds. We observe in Figure 3b
that the two NFSIC variants have correct type-I errors
across all sample sizes, indicating that the asymptotic null
distribution approximately holds by the time n reaches
1000. We recall from Theorem 5 that the NFSIC test
with random test locations will asymptotically reject H0
if it is false. A demonstration of this property is given in
Figure 3c, where the test power of NFSIC-med eventually
reaches 1 with n higher than 105.

3.2 Real Problems

We now examine the performance of our proposed test
on real problems.

Million Song Data (MSD) We consider a subset of
the Million Song Data2 [Bertin-Mahieux et al., 2011], in
which each song (X) out of 515,345 is represented by 90
features, of which 12 features are timbre average (over
all segments) of the song, and 78 features are timbre
covariance. Most of the songs are western commercial
tracks from 1922 to 2011. The goal is to detect the
dependency between each song and its year of release
(Y ). We set α = 0.01, and repeat for 300 trials where
the full sample is randomly subsampled to n points in
each trial. Other settings are the same as in the toy
problems. To make sure that the type-I error is correct,
we use the permutation approach in the NFSIC tests to
compute the threshold. Figure 4b shows the test powers
as n increases from 500 to 2000. To simulate the case
where H0 holds in the problem, we permute the sample
to break the dependency of X and Y . The results are
shown in Figure 5 in the appendix.

Evidently, NFSIC-opt has the highest test power among
all the linear-time tests for all the sample sizes. Its test
power is second to only QHSIC. We recall that NFSIC-opt
uses half of the sample for parameter tuning. Thus, at
n = 500, the actual sample for testing is 250, which is
relatively small. The fact that there is a vast power gain
from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500
suggests that the optimization procedure can perform
well even at a lower sample sizes.

(a) MSD problem.

(b) Videos & Captions problem.
Figure 4: Probability of rejecting H0 as n increases in
the two real problems. α = 0.01.

Videos and Captions Our last problem is based on
the VideoStory46K3 dataset [Habibian et al., 2014]. The
dataset contains 45,826 Youtube videos (X) of an average
length of roughly one minute, and their corresponding
text captions (Y ) uploaded by the users. Each video is
represented as a dx = 2000 dimensional Fisher vector en-
coding of motion boundary histograms (MBH) descriptors
of Wang and Schmid [2013]. Each caption is represented
as a bag of words with each feature being the frequency
of one word. After ﬁltering only words which occur in at
least six video captions, we obtain dy = 1878 words. We
examine the test powers as n increases from 2000 to 8000.
The results are given in Figure 4. The problem is suﬃ-
ciently challenging that all linear-time tests achieve a low
power at n = 2000. QHSIC performs exceptionally well
on this problem, achieving a maximum power throughout.
NFSIC-opt has the highest sample eﬃciency among the
linear-time tests, showing that the optimization procedure
is also practical in a high dimensional setting.

Acknowledgement

We thank the Gatsby Charitable Foundation for the ﬁ-
nancial support. The major part of this work was carried
out while Zoltán Szabó was a research associate at the
Gatsby Computational Neuroscience Unit, University Col-
lege London.

2Million Song Data subset: https://archive.ics.uci.edu/ml/

3VideoStory46K dataset:

https://ivi.fnwi.uva.nl/isis/

datasets/YearPredictionMSD.

mediamill/datasets/videostory.php.

8

References

T. W. Anderson. An Introduction to Multivariate Statis-

tical Analysis. Wiley, 2003.

F. R. Bach and M. I. Jordan. Kernel independent compo-
nent analysis. Journal of Machine Learning Research,
3:1–48, 2002.

T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and
P. Lamere. The million song dataset. In International
Conference on Music Information Retrieval (ISMIR),
2011.

K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and
A. Gretton. Fast Two-Sample Testing with Analytic
Representations of Probability Measures. In Advances
in Neural Information Processing Systems (NIPS),
pages 1981–1989. 2015.

X. Huo and G. J. Székely. Fast computing for distance
covariance. Technical report, 2014. URL https://
arxiv.org/abs/1410.1503.

W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gret-
ton. Interpretable Distribution Features with Maximum
Testing Power. 2016. URL http://arxiv.org/abs/
1605.06796.

J. Kowalski and X. M. Tu. Modern Applied U-Statistics.

John Wiley & Sons, 2008.

E. L. Lehmann. Elements of Large-Sample Theory.

Springer Science & Business Media, 1999.

D. Lopez-Paz, P. Hennig, and B. Schölkopf. The Random-
ized Dependence Coeﬃcient. In Advances in Neural
Information Processing Systems (NIPS), pages 1–9.
2013.

J. Dauxois and G. M. Nkiet. Nonlinear canonical analysis
and independence tests. The Annals of Statistics, 26
(4):1254–1278, 1998.

A. Rahimi and B. Recht. Random features for large-scale
kernel machines. In Advances in Neural Information
Processing Systems (NIPS), pages 1177–1184. 2008.

A. Feuerverger. A consistent test for bivariate dependence.
International Statistical Review, 61(3):419–433, 1993.

K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf. Ker-
nel measures of conditional dependence. In Advances in
Neural Information Processing Systems (NIPS), pages
489–496, 2008.

A. Gretton. A simpler condition for consistency of a
kernel independence test. Technical report, 2015. URL
http://arxiv.org/abs/1501.06103.

A. Gretton and L. Györﬁ. Consistent nonparametric
tests of independence. Journal of Machine Learning
Research, 11:1391–1423, 2010.

A. Gretton, O. Bousquet, A. Smola, and B. Schölkopf.
Measuring Statistical Dependence with Hilbert-
Schmidt Norms.
In Algorithmic Learning Theory
(ALT), pages 63–77. 2005.

A. Gretton, K. Fukumizu, C. H. Teo, L. Song,
B. Schölkopf, and A. J. Smola. A Kernel Statistical Test
of Independence. In Advances in Neural Information
Processing Systems (NIPS), pages 585–592. 2008.

A. Habibian, T. Mensink, and C. G. Snoek. Videostory:
A new multimedia embedding for few-example recogni-
tion and translation of events. In ACM International
Conference on Multimedia, pages 17–26, 2014.

R. Heller, Y. Heller, S. Kaufman, B. Brill, and M. Gorﬁne.
Consistent distribution-free k-sample and independence
tests for univariate random variables. Journal of Ma-
chine Learning Research, 17(29):1–54, 2016.

D. Sejdinovic, B. Sriperumbudur, A. Gretton, and
K. Fukumizu. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. The Annals of
Statistics, 41(5):2263–2291, 2013.

R. J. Serﬂing. Approximation Theorems of Mathematical

Statistics. John Wiley & Sons, 2009.

A. Smola, A. Gretton, L. Song, and B. Schölkopf. A
hilbert space embedding for distributions. In Inter-
national Conference on Algorithmic Learning Theory
(ALT), pages 13–31, 2007.

B. K. Sriperumbudur, A. Gretton, K. Fukumizu,
B. Schölkopf, and G. R. G. Lanckriet. Hilbert Space Em-
beddings and Metrics on Probability Measures. Journal
of Machine Learning Research, 11:1517–1561, 2010.

I. Steinwart and A. Christmann. Support vector machines.

Springer Science & Business Media, 2008.

G. J. Székely and M. L. Rizzo. Brownian distance covari-
ance. The Annals of Applied Statistics, 3(4):1236–1265,
2009.

G. J. Székely, M. L. Rizzo, and N. K. Bakirov. Measuring
and testing dependence by correlation of distances. The
Annals of Statistics, 35(6):2769–2794, 2007.

A. W. v. d. Vaart. Asymptotic Statistics. Cambridge

University Press, 2000.

H. Wang and C. Schmid. Action recognition with im-
proved trajectories. In IEEE International Conference
on Computer Vision (ICCV), pages 3551–3558, 2013.

9

K. Zhang, J. Peters, D. Janzing, B., and B. Schölkopf.
Kernel-based conditional independence test and appli-
cation in causal discovery. In Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pages 804–813, 2011.

Q. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. Large-
Scale Kernel Methods for Independence Testing. 2016.
URL http://arxiv.org/abs/1606.07892.

10

An Adaptive Test of Independence with Analytic Kernel Embeddings
Supplementary Material

A Type-I Errors

In this section, we show that all the tests have correct type-I errors (i.e., the probability of reject H0 when it is true)
in real problems. We permute the joint sample so that the dependency is broken to simulate cases in which H0
holds. The results are shown in Figure 5.

(a) MSD problem (permuted).

(a) Videos & Captions problem with shuf-
ﬂed sample.

Figure 5: Probability of rejecting H0 as n increases in the Million Song problem. α = 0.01.

B Redundant Test Locations

Here, we provide a simple illustration to show that two locations t1 = (v1, w1) and t2 = (v2, w2) which are too
close to each other will reduce the optimization objective. We consider the Sinusoid problem described in Section
3.1 with ω = 1, and use J = 2 test locations. In Figure 6, t1 is ﬁxed at the red star, while t2 is varied along the
horizontal line. The objective value ˆλn as a function of (t1, t2) is shown in the bottom ﬁgure. It can be seen that ˆλn
decreases sharply when t2 is in the neighborhood of t1. This property implies that two locations which are too close
will not maximize the objective function (i.e., the second feature contains no additional information when it matches
the ﬁrst). For J > 2, the objective sharply decreases if any two locations are in the same neighborhood.

Figure 6: Plot of optimization objective values as location t2 moves along the green line. The objective sharply
drops when the two locations are in the same neighborhood.

C Test Power vs. J

It might seem intuitive that as the number of locations J increases, the test power should also increase. Here, we
empirically show that this statement is not always true. Consider the Sinusoid toy example described in Section 3.1
with ω = 2 (also see the left ﬁgure of Figure 7). By construction, X and Y are dependent in this problem. We run

11

NFSIC test with a sample size of n = 800, varying J from 1 to 600. For each value of J, the test is repeated for 500
times. In each trial, the sample is redrawn and the J test locations are drawn from Uniform((−π, π)2). There is no
optimization of the test locations. We use Gaussian kernels for both X and Y , and use the median heuristic to set
the Gaussian widths to 1.8. Figure 7 shows the test power as J increases.

Figure 7: The Sinusoid problem and the plot of test power vs. the number of test locations.

We observe that the test power does not monotonically increase as J increases. When J = 1, the diﬀerence of pxy
and pxpy cannot be adequately captured, resulting in a low power. The power increases rapidly to roughly 0.8 at
J = 10, and stays at the maximum until about J = 100. Then, the power starts to drop sharply when J is higher
than 400 in this problem.

Unlike random Fourier features, the number of test locations in NFSIC is not the number of Monte Carlo particles
used to approximate an expectation. There is a tradeoﬀ: if the test locations are in key regions (i.e., regions in
which there is a big diﬀerence between pxy and pxpy), then they increase power; yet the statistic gains in variance
(thus reducing test power) as J increases. As can be seen in Figure 7, there are eight key regions (in blue) that can
reveal the diﬀerence of pxy and pxpy. Using an unnecessarily high J not only makes the covariance matrix ˆΣ harder
to estimate accurately, it also increases the computation as the complexity on J is O(J 3).

We note that NFSIC is not intended to be used with a large J. In practice, it should be set to be large enough so
as to capture the key regions as stated. As a practical guide, with optimization of the test locations, a good starting
point is J = 5 or 10.

D Proof of Proposition 3

Recall Proposition 3,
Proposition (A product of Gaussian kernels is characteristic and analytic). Let k(x, x(cid:48)) = exp (cid:0)−(x − x(cid:48))(cid:62)A(x − x(cid:48))(cid:1)
and l(y, y(cid:48)) = exp (cid:0)−(y − y(cid:48))(cid:62)B(y − y(cid:48))(cid:1) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for
positive deﬁnite matrices A and B. Then, g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteristic and analytic on
(Rdx × Rdy ) × (Rdx × Rdy ).

Proof. Let z := (x(cid:62), y(cid:62))(cid:62) and z(cid:48) := (x(cid:48)(cid:62), y(cid:48)(cid:62))(cid:62) be vectors in Rdx+dy . We prove by reducing the product kernel to
one Gaussian kernel with g(z, z(cid:48)) = exp (cid:0)−(z − z(cid:48))(cid:62)C(z − z(cid:48))(cid:1) where C :=
. Write g(z, z(cid:48)) = Ψ(z − z(cid:48))
where Ψ(t) := exp (cid:0)−t(cid:62)Ct(cid:1). Since C is positive deﬁnite, we see that the ﬁnite measure ζ corresponding to Ψ as
deﬁned in Lemma 12 has support everywhere in Rdx+dy . Thus, Sriperumbudur et al. [2010, Theorem 9] implies that
g is characteristic.

(cid:18) A 0
0 B

(cid:19)

To see that g is analytic, we observe that for each z(cid:48) ∈ Rdx+dy , z (cid:55)→ −(z − z(cid:48))(cid:62)C(z − z(cid:48)) is a multivariate
polynomial in z, which is known to be analytic. Using the fact that t (cid:55)→ exp(t) is analytic on R, and that a
composition of analytic functions is analytic, we see that z (cid:55)→ exp (cid:0)−(z − z(cid:48))(cid:62)C(z − z(cid:48))(cid:1) is analytic on Rdx+dy for
each z(cid:48).

E Proof of Theorem 5

Recall Theorem 5,

12

Theorem 5 (Independence test using (cid:92)NFSIC2 is consistent). Let ˆΣ be a consistent estimate of Σ based on the
joint sample Zn. The (cid:92)NFSIC2 statistic is deﬁned as ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu where γn ≥ 0 is a regularization
parameter. Assume that

(cid:17)−1

1. Assumption A holds.
2. Σ is invertible almost surely with respect to VJ = {(vi, wi)}J

i=1 drawn from an absolutely continuous distribution.

3. limn→∞ γn = 0.

Then, for any k, l and VJ satisfying the assumptions,
1. Under H0, ˆλn

d→ χ2(J) as n → ∞.

2. Under H1, for any r ∈ R, limn→∞ P

(cid:16)ˆλn ≥ r

(cid:17)

(cid:92)NFSIC2 is consistent.

= 1 almost surely. That is, the independence test based on

(cid:17)−1 p
Proof. Assume that H0 holds. The consistency of ˆΣ and the continuous mapping theorem imply that
→
Σ−1 which is a constant. Let a be a random vector in RJ following N (0, Σ). By Vaart [2000, Theorem 2.7 (v)], it
nˆu d→ N (0, Σ)

d→ (cid:2)a, Σ−1(cid:3) where u = 0 almost surely by Proposition 2, and

(cid:16) ˆΣ + γnI

follows that

(cid:17)−1(cid:21)

(cid:16) ˆΣ + γnI

(cid:20)√

nˆu,

√

by Proposition 4. Since f (x, S) := x(cid:62)Sx is continuous, f
nˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

ˆu d→ a(cid:62)Σ−1a ∼ χ2(J) by Anderson [2003, Theorem 3.3.3]. This proves the ﬁrst claim.

(cid:18)√

(cid:16) ˆΣ + γnI

nˆu,

(cid:17)−1(cid:19)

d→ f (a, Σ−1). Equivalently,

The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al.
[2015]. Assume that H1 holds. Then, u (cid:54)= 0 almost surely by Proposition 2. Since k and l are bounded, it follows that
|ht(z, z(cid:48))| ≤ 2BkBl for any z, z(cid:48) (see (8)), and we have that ˆu a.s.→ u by Serﬂing [2009, Section 5.4, Theorem A]. Thus,
ˆu(cid:62) (cid:16) ˆΣ + γnI
d→ u(cid:62)Σ−1u by the continuous mapping theorem, and the consistency of ˆΣ. Consequently,

(cid:17)−1

ˆu − r
n

P

lim
n→∞

(cid:17)

(cid:16)ˆλn ≥ r
(cid:18)

= 1 − lim
n→∞

P

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

ˆu −

< 0

(cid:19)

r
n

(a)

= 1 − P (cid:0)u(cid:62)Σ−1u < 0(cid:1) (b)

= 1,

where at (a) we use the Portmanteau theorem [Vaart, 2000, Lemma 2.2 (i)] guaranteeing that xn
→ x if and only if
P(xn < t) → P(x < t) for all continuity points of t (cid:55)→ P(x < t). Step (b) is justiﬁed by noting that the covariance
matrix Σ is positive deﬁnite so that u(cid:62)Σ−1u > 0, and t (cid:55)→ P(u(cid:62)Σ−1u < t) (a step function) is continuous at 0.

d

F Proof of Theorem 7

Recall Theorem 7,

Theorem 7 (A lower bound on the test power). Let NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let K be a kernel class
for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that
1. There exist ﬁnite Bk and Bl such that supk∈K supx,x(cid:48)∈X |k(x, x(cid:48))| ≤ Bk and supl∈L supy,y(cid:48)∈Y |l(y, y(cid:48))| ≤ Bl.
2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)Σ−1(cid:107)F < ∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test power satisﬁes P

≥ L(λn) where

(cid:16)ˆλn ≥ r

(cid:17)

L(λn) = 1 − 62e−ξ1γ2

− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

n(λn−r)2/n − 2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

13

(cid:98)·(cid:99) is the ﬂoor function, ξ1 :=
c1 := 4B2J
ﬁxed n, L(λn) is increasing in λn.

J ˜c, c2 := 4B

√

√

1

32c2

1J 2B∗ , ξ2 := 72c2

2JB2, B := BkBl, ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 := 28B4J 2c2
1,
J ˜c, and B∗ is a constant depending on only Bk and Bl. Moreover, for suﬃciently large

Overview of the proof We ﬁrst derive a probabilistic bound for |ˆλn − λn|/n. The bound is in turn upper
bounded by an expression involving (cid:107)ˆu − u(cid:107)2 and (cid:107) ˆΣ − Σ(cid:107)F . The diﬀerence (cid:107)ˆu − u(cid:107)2 can be bounded by applying
the bound for U-statistics given in Serﬂing [2009, Theorem A, p. 201]. For (cid:107) ˆΣ − Σ(cid:107)F , we decompose it into a sum
of smaller components, and bound each term with a product variant of the Hoeﬀding’s inequality (Lemma 9). L(λn)
is obtained by combining all the bounds with the union bound.

F.1 Notations
Let (cid:104)A, B(cid:105)F := tr(A(cid:62)B) denote the Frobenius inner product, and (cid:107)A(cid:107)F := (cid:112)tr(A(cid:62)A) be the Frobenius norm.
Write z := (x, y) to denote a pair of points from X × Y. We write t := (v, w) to denote a pair of test locations
from X × Y. For brevity, an expectation over (x, y) (i.e., E(x,y)∼Pxy ) will be written as Ez or Exy. Deﬁne
˜k(x, v) := k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) := l(y, w) − Ey(cid:48)l(y(cid:48), w). Let B2(r) := {x | (cid:107)x(cid:107)2 ≤ r} be a closed ball
with radius r centered at the origin. Similarly, deﬁne BF (r) := {A | (cid:107)A(cid:107)F ≤ r} to be a closed ball with radius r of
J × J matrices under the Frobenius norm. Denote the max operation by (x1, . . . , xm)+ = max(x1, . . . , xm).
we write (cid:91)µxµy(v, w)

j(cid:54)=i k(xi, v)l(yj, w) to denote the unbiased plug-in estimator, and write ˆµx(v)ˆµy(w)

:=
:=
j=1 l(yj, w) which is a biased estimator. Deﬁne ˆub(v, w) := ˆµxy(v, w) − ˆµx(v)ˆµy(w)
where the superscript b stands for “biased”. To avoid confusing with a positive

i=1
i=1 k(xi, v) 1
n

For
1
n(n−1)
(cid:80)n
1
n

of marginal mean

so that ˆub := (cid:0)ˆub(t1), . . . , ˆub(tJ )(cid:1)(cid:62)
deﬁnite kernel, we will refer to a U-statistic kernel as a core.

product
(cid:80)

µx(v)µy(w),

embeddings

a
(cid:80)n

(cid:80)n

F.2 Proof
We will ﬁrst derive a bound for P(|ˆλn − λn| ≥ t), which will then be reparametrized to get a bound for the target
quantity P(ˆλn ≥ r). We closely follow the proof in Jitkrittum et al. [2016, Section C.1] up to (12), then we diverge.
We start by considering |ˆλn − λn|/n.

|ˆλn − λn|/n =

(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:17)−1
ˆu(cid:62) (cid:16) ˆΣ + γnI
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

=

≤

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u + u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)

+

:= ((cid:70))1 + ((cid:70))2 .

We next bound ((cid:70)1) and ((cid:70)2) separately.

((cid:70))1 =

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

≤

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

(cid:17)−1

ˆu(cid:62) (cid:16) ˆΣ + γnI
(cid:28)

(cid:12)
(cid:12)
ˆu − ˆu(cid:62) (Σ + γnI)−1 ˆu + ˆu(cid:62) (Σ + γnI)−1 ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)ˆu(cid:62) (Σ + γnI)−1 ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:68)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ˆu − ˆu(cid:62) (Σ + γnI)−1 ˆu
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

+

(cid:29)

=

(cid:17)−1

ˆuˆu(cid:62),

(cid:16) ˆΣ + γnI

− (Σ + γnI)−1

(cid:12)
(cid:12)
(cid:12)
F
≤ (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1 − (Σ + γnI)−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1(cid:107)F
= (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1[(Σ + γnI) − ( ˆΣ + γnI)](Σ + γnI)−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − ˆuu(cid:62) + ˆuu(cid:62) − uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1(cid:107)F

ˆuˆu(cid:62) − uu(cid:62), (Σ + γnI)−1(cid:69)

F

14

(a)
≤ (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1(cid:107)F (cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − ˆuu(cid:62) + ˆuu(cid:62) − uu(cid:62)(cid:107)F (cid:107)Σ−1(cid:107)F
(b)
≤

2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + (cid:0)(cid:107)ˆu(ˆu − u)(cid:62)(cid:107)F + (cid:107)(ˆu − u)u(cid:62)(cid:107)F

(cid:1) (cid:107)Σ−1(cid:107)F

(cid:107)ˆu(cid:107)2

√

≤

(cid:107)ˆu(cid:107)2

2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + ((cid:107)ˆu(cid:107)2 + (cid:107)u(cid:107)2) (cid:107)ˆu − u(cid:107)2(cid:107)Σ−1(cid:107)F ,

J
γn
√
J
γn

where at (a) we used (cid:107)(Σ + γnI)−1(cid:107)F ≤ (cid:107)Σ−1(cid:107)F , at (b) we used (cid:107)( ˆΣ + γnI)−1(cid:107)F ≤

J(cid:107)( ˆΣ + γnI)−1(cid:107)2 ≤

J/γn.

For ((cid:70))2, we have

(cid:12)
(cid:12)
(cid:12)u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
((cid:70))2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (cid:12)
(cid:10)uu(cid:62), (Σ + γnI)−1 − Σ−1(cid:11)
(cid:12)
(cid:12)
≤ (cid:107)uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1 − Σ−1(cid:107)F
= (cid:107)u(cid:107)2
≤ γn(cid:107)u(cid:107)2
(a)
≤ γn(cid:107)u(cid:107)2

2(cid:107)(Σ + γnI)−1(cid:107)F (cid:107)Σ−1(cid:107)F

2(cid:107)Σ−1(cid:107)2
F ,

F

2(cid:107)(Σ + γnI)−1 [Σ − (Σ + γnI)] Σ−1(cid:107)F

where at (a) we used (cid:107)(Σ + γnI)−1(cid:107)F ≤ (cid:107)Σ−1(cid:107)F .

Combining (5) and (6), we have

(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)

√

J
γn

≤

(cid:107)ˆu(cid:107)2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + ((cid:107)ˆu(cid:107)2 + (cid:107)u(cid:107)2) (cid:107)ˆu − u(cid:107)2(cid:107)Σ−1(cid:107)F + γn(cid:107)u(cid:107)2

2(cid:107)Σ−1(cid:107)2
F .

(7)

2 and (cid:107)u(cid:107)2

Bounding (cid:107)ˆu(cid:107)2
2 is
bounded. Recall that supx,x(cid:48)∈X |k(x, x(cid:48))| ≤ Bk, supy,y(cid:48) |l(y, y(cid:48))| ≤ Bl, our notation t = (v, w) for the test locations,
and zi := (xi, yi). We ﬁrst show that the U-statistic core h is bounded.

2 Here, we show that by the boundedness of the kernels k and l, it follows that (cid:107)ˆu(cid:107)2

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we deﬁne B := BkBl. It follows that

|ht((x, y), (x(cid:48), y(cid:48)))| =

(k(x, v) − k(x(cid:48), v))(l(y, w) − l(y(cid:48), w))

(cid:12)
1
(cid:12)
(cid:12)
2
(cid:12)
1
2

≤

(|k(x, v)| + |k(x(cid:48), v)|) (|l(y, w)| + |l(y(cid:48), w)|)

≤ 2BkBl := 2B,

(cid:107)ˆu(cid:107)2

2 =

htm (zi, zj)



≤

[2BkBl]2 = 4B2J,

2
n(n − 1)

(cid:88)

i<j


2

J
(cid:88)

m=1

(cid:107)u(cid:107)2

2 =

[EzEz(cid:48)htm(z, z(cid:48))]2 ≤ 4B2J.

Using the upper bounds on (cid:107)ˆu(cid:107)2

2, (cid:107)u(cid:107)2

2 ,(7) and the deﬁnition of ˜c, we have
(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
√

√

4B2J ˜c(cid:107)Σ − ˆΣ(cid:107)F + 4B

J ˜c(cid:107)ˆu − u(cid:107)2 + 4B2J ˜c2γn

(cid:107)Σ − ˆΣ(cid:107)F + c2(cid:107)ˆu − u(cid:107)2 + c3γn,

15





J
(cid:88)

m=1

J
(cid:88)

m=1

≤

=:

J
γn
c1
γn

(5)

√

(6)

(8)

(9)

(10)

(11)

where we deﬁne c1 := 4B2J

J ˜c, c2 := 4B

√

√

|ˆλn − λn| ≤

J ˜c, and c3 := 4B2J ˜c2. This upper bound implies that
c1
γn

n(cid:107)Σ − ˆΣ(cid:107)F + c2n(cid:107)ˆu − u(cid:107)2 + c3nγn.

(12)

We will separately upper bound (cid:107)Σ − ˆΣ(cid:107)F and (cid:107)ˆu − u(cid:107)2, and combine them with a union bound.

F.2.1 Bounding (cid:107)ˆu − u(cid:107)2
Let t∗ = arg maxt∈{t1,...,tJ } |ˆu(t) − u(t)|. Recall that u = (u(t1), . . . , u(tJ ))(cid:62) = (u1, . . . , uJ )(cid:62).

(cid:107)ˆu − u(cid:107)2 = sup

(cid:104)b, ˆu − u(cid:105)2 ≤ sup

|bj||ˆu(tj) − u(tj)|

b∈B2(1)

b∈B2(1)

j=1

J
(cid:88)

≤ |ˆu(t∗) − u(t∗)|

sup
b∈B2(1)

J
(cid:88)

j=1

|bj|

√

(a)
≤

√

J|ˆu(t∗) − u(t∗)|

sup
b∈B2(1)

(cid:107)b(cid:107)2

=

J|ˆu(t∗) − u(t∗)|,

J(cid:107)a(cid:107)2 for any a ∈ RJ . From (13), it can be seen that bounding (cid:107)ˆu − u(cid:107)2 amounts to
where at (a) we used (cid:107)a(cid:107)1 ≤
bounding the diﬀerence of a U-statistic ˆu(t∗) (see (4)) to its expectation u(t∗). Combining (13) and (12), we have

√

|ˆλn − λn| ≤

n(cid:107)Σ − ˆΣ(cid:107)F + c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

√

c1
γn

F.2.2 Bounding (cid:107) ˆΣ − Σ(cid:107)F
The plan is to write ˆΣ = ˆS − ˆub ˆub(cid:62), Σ = S − uu(cid:62), so that (cid:107) ˆΣ − Σ(cid:107)F ≤ (cid:107)ˆS − S(cid:107)F + (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F and bound
separately (cid:107)ˆS − S(cid:107)F and (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

Recall that Σij = η(ti, tj), η(t, t(cid:48)) = Exy[(cid:0)˜k(x, v)˜l(y, w) − u(v, w)(cid:1)(cid:0)˜k(x, v(cid:48))˜l(y, w(cid:48)) − u(v(cid:48), w(cid:48))(cid:1)] where ˜k(x, v) =
k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) = l(y, w) − Ey(cid:48)l(y(cid:48), w). Its empirical estimator (see Proposition 6) is ˆΣij = ˆη(ti, tj)
where

ˆη(t, t(cid:48)) =

[(cid:0)k(xi, v)l(yi, w) − ˆub(v, w)(cid:1)(cid:0)k(xi, v(cid:48))l(yi, w(cid:48)) − ˆub(v(cid:48), w(cid:48))(cid:1)]

=

k(xi, v)l(yi, w)k(xi, v(cid:48))l(yi, w(cid:48)) − ˆub(v, w)ˆub(v(cid:48), w(cid:48)),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:80)n

:=

that

k(x, v) − 1
and
n
i=1 k(xi, v)l(yi, w) = ˆub(v, w).

We
:=
m=1 k(xm, vi)l(ym, wi)k(xm, vj)l(yi, wj), and deﬁne similarly its population counterpart S such that

k(x, v)
note
1
n
Sij := Exy[˜k(x, v)˜l(y, w)˜k(x, v(cid:48))˜l(y, w(cid:48))]. We have

l(y, w) − 1
n
ˆS
RJ×J

i=1 l(yi, w).
such that

i=1 k(xi, v),

We deﬁne

l(y, w)

ˆSij

(cid:80)n

(cid:80)n

:=

1
n

∈

(cid:80)n

ˆΣ = ˆS − ˆub ˆub(cid:62),
Σ = S − uu(cid:62),

(cid:107) ˆΣ − Σ(cid:107)F = (cid:107)ˆS − S − (ˆub ˆub(cid:62) − uu(cid:62))(cid:107)F

≤ (cid:107)ˆS − S(cid:107)F + (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

With (16), (14) becomes

|ˆλn − λn| ≤

(cid:107)ˆS − S(cid:107)F +

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F + c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1n
γn

c1n
γn

√

We will further separately bound (cid:107)ˆS − S(cid:107)F and (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

16

(13)

(14)

(15)

(16)

(17)

F.2.3 Bounding (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F = (cid:107)ˆub ˆub(cid:62) − ˆubu(cid:62) + ˆubu(cid:62) − uu(cid:62)(cid:107)F
≤ (cid:107)ˆub(ˆub − u)(cid:62)(cid:107)F + (cid:107)(ˆub − u)u(cid:62)(cid:107)F
= (cid:107)ˆub(cid:107)2(cid:107)ˆub − u(cid:107)2 + (cid:107)ˆub − u(cid:107)2(cid:107)u(cid:107)2

√

≤ 4B

J(cid:107)ˆub − u(cid:107)2,

√

where we used (10) and the fact that (cid:107)ˆub(cid:107)2 ≤ 2B

J which can be shown similarly to (9) as

(cid:107)ˆub(cid:107)2

2 =

[ˆµxy(vm, wm) − ˆµx(vm)ˆµy(wm)]2 =

htm (zi, zj)



≤

[2BkBl]2 = 4B2J.

J
(cid:88)

n
(cid:88)

n
(cid:88)





1
n2

m=1

i=1

j=1


2

J
(cid:88)

m=1

J
(cid:88)

m=1

Let (˜v, ˜w) := ˜t = arg maxt∈{t1,...,tJ } |ˆub(t) − u(t)|. We bound (cid:107)ˆub − u(cid:107)2 by

(cid:107)ˆub − u(cid:107)2

√

(a)
≤

√

√

√

√

J|ˆub(˜t) − u(˜t)|
(cid:12)ˆµxy(˜t) − ˆµx(˜v)ˆµy( ˜w) − u(˜t)(cid:12)
J (cid:12)
(cid:12)
(cid:12)ˆµxy(˜t) − (cid:91)µxµy(˜t) + (cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w) − u(˜t)(cid:12)
J (cid:12)
(cid:12)
(cid:12)ˆµxy(˜t) − (cid:91)µxµy(˜t) − u(˜t)(cid:12)
J (cid:12)
(cid:12) +
(cid:12)ˆu(˜t) − u(˜t)(cid:12)
J (cid:12)
(cid:12) +

J (cid:12)
(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12)
(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12) ,

J (cid:12)

√

√

=

=

≤

=

where at (a) we used the same reasoning as in (13). The bias (cid:12)
bounded as

(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)

(cid:12) in the second term can be

k(xi, ˜v)l(yj, ˜w) −

k(xi, ˜v)l(yi, ˜w) −

k(xi, ˜v)l(yj, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:18)

(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12)
(cid:12)

=

=

1
n(n − 1)

1
n(n − 1)

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

n
(cid:88)

n
(cid:88)

i=1

j=1

n
n − 1

(cid:19) 1
n2

n
n − 1

(cid:19) 1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1

j=1

≤

1 −

≤

B
n − 1

+

B
n − 1

=

2B
n − 1

.

k(xi, ˜v)l(yj, ˜w) −

k(xi, ˜v)l(yj, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

1
n(n − 1)

n
(cid:88)

i=1

1
n2

n
(cid:88)

n
(cid:88)

j=1

1
n(n − 1)

i=1
(cid:12)
(cid:12)
(cid:12)
k(xi, ˜v)l(yi, ˜w)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
k(xi, ˜v)l(yj, ˜w)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(n − 1)

n
(cid:88)

i=1

k(xi, ˜v)l(yi, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

1 −

k(xi, ˜v)l(yj, ˜w) +

(18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Combining this upper bound with (18), we have

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F ≤ 4BJ (cid:12)

(cid:12)ˆu(˜t) − u(˜t)(cid:12)

(cid:12) +

(19)

With (19), (17) becomes

|ˆλn − λn| ≤

(cid:107)ˆS − S(cid:107)F +

c1n
γn

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12) +

c1n
γn

8B2J
n − 1

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

(20)

8B2J
n − 1

.

√

17

F.2.4 Bounding (cid:107)ˆS − S(cid:107)F
Recall that VJ = {t1, . . . , tJ }, ˆSij = ˆS(ti, tj) = 1
n
S(ti, tj) = Exy[˜k(x, vi)˜l(y, wi)˜k(x, vj)˜l(y, wj)]. Let (t(1), t(2)) = arg max(s,t)∈VJ ×VJ | ˆS(s, t) − S(s, t)|.

m=1 k(xm, vi)l(ym, wi)k(xm, vj)l(ym, wj), and Sij =

(cid:80)n

(cid:107)ˆS − S(cid:107)F = sup

B∈BF (1)

(cid:68)

(cid:69)
B, ˆS − S

F

J
(cid:88)

J
(cid:88)

≤ sup

B∈BF (1)

i=1

j=1

|Bij|| ˆSij − Sij|

≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12)

sup
B∈BF (1)

J
(cid:88)

J
(cid:88)

i=1

j=1

|Bij|

(a)
≤ J

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12)

sup
B∈BF (1)

(cid:107)B(cid:107)F

= J

ˆS(t(1), t(2)) − S(t(1), t(2))

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ,

where at (a) we used (cid:80)J

(cid:80)J

i=1

j=1 |Aij| ≤ J(cid:107)A(cid:107)F for any matrix A ∈ RJ×J . We arrive at

|ˆλn − λn| ≤

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12) +

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)
(cid:12)

√

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1Jn
γn
c1n
γn

(cid:12)
(cid:12)
(cid:12)
8B2J
n − 1

+

F.2.5 Bounding

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12)

Having an upper bound for
will deﬁne the following shorthands.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12) will allow us to bound (22). To keep the notations uncluttered, we

(21)

(22)

Expression

Shorthand

Expression

Shorthand

k(x, v)

k(x, v(cid:48))

k(xi, v)
k(xi, v(cid:48))
Ex∼Px k(x, v)
Ex∼Px k(x, v(cid:48))
(cid:80)n
1
i=1 k(xi, v)
n
1
i=1 k(xi, v(cid:48))
n

(cid:80)n

a

a(cid:48)

ai
a(cid:48)
i
˜a

˜a(cid:48)

a
a(cid:48)

l(y, w)

l(y, w(cid:48))

l(yi, w)
l(yi, w(cid:48))
Ey∼Py l(y, w)
Ey∼Py l(y, w(cid:48))
(cid:80)n
1
i=1 l(yi, w)
n
i=1 l(yi, w(cid:48))

(cid:80)n

1
n

b

b(cid:48)

bi
b(cid:48)
i
˜b
˜b(cid:48)

b
(cid:48)

b

We will also use · to denote a empirical expectation over x, or y, or (x, y). The argument under · will
i=1 k(xi, v)k(xi, v(cid:48)) and
i=1 k(xi, v)l(yi, w)k(xi, v(cid:48)), and so on. We deﬁne in the same way for the population expectation using

determine the variable over which we take the expectation. For instance, aa(cid:48) = 1
n
aba(cid:48) = 1
n
(cid:101)· i.e., (cid:102)aa(cid:48) = Ex [k(x, v)k(x, v(cid:48))] and (cid:103)aba(cid:48) = Exy [k(x, v)l(y, w)k(x, v(cid:48))].

(cid:80)n

(cid:80)n

With these shorthands, we can rewrite ˆS(t, t(cid:48)) and S(t, t(cid:48)) as

ˆS(t, t(cid:48)) =

(ai − a)(bi − b)(a(cid:48)

i − a(cid:48))(b(cid:48)

(cid:48)
i − b

),

1
n

n
(cid:88)

i=1

18

By expanding S(t, t(cid:48)), we have

S(t, t(cid:48)) = Exy

(cid:104)

(cid:105)
(a − ˜a)(b − ˜b)(a(cid:48) − ˜a(cid:48))(b(cid:48) − ˜b(cid:48))

.

S(t, t(cid:48)) = Exy

(cid:2) + aba(cid:48)b(cid:48) − aba(cid:48)˜b(cid:48) − ab˜a(cid:48)b(cid:48) + ab˜a(cid:48)˜b(cid:48)
− a˜ba(cid:48)b(cid:48) + a˜ba(cid:48)˜b(cid:48) + a˜b˜a(cid:48)b(cid:48) − a˜b˜a(cid:48)˜b(cid:48)
− ˜aba(cid:48)b(cid:48) + ˜aba(cid:48)˜b(cid:48) + ˜ab˜a(cid:48)b(cid:48) − ˜ab˜a(cid:48)˜b(cid:48)
+ ˜a˜ba(cid:48)b(cid:48) − ˜a˜ba(cid:48)˜b(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48) + ˜a˜b˜a(cid:48)˜b(cid:48)(cid:3)

= +(cid:94)aba(cid:48)b(cid:48) − (cid:103)aba(cid:48)˜b(cid:48) − (cid:103)abb(cid:48)˜a(cid:48) + (cid:101)ab˜a(cid:48)˜b(cid:48)
− (cid:93)aa(cid:48)b(cid:48)˜b + (cid:102)aa(cid:48)˜b˜b(cid:48) + (cid:102)ab(cid:48)˜a(cid:48)˜b − ˜a˜b˜a(cid:48)˜b(cid:48)
− (cid:103)a(cid:48)bb(cid:48)˜a + (cid:102)a(cid:48)b˜a˜b(cid:48) + ˜a˜a(cid:48) (cid:102)bb(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48)
+ (cid:103)a(cid:48)b(cid:48)˜a˜b − ˜a˜b˜a(cid:48)˜b(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48) + ˜a˜b˜a(cid:48)˜b(cid:48)
= +(cid:94)aba(cid:48)b(cid:48) − (cid:103)aba(cid:48)˜b(cid:48) − (cid:103)abb(cid:48)˜a(cid:48) + (cid:101)ab˜a(cid:48)˜b(cid:48)
− (cid:93)aa(cid:48)b(cid:48)˜b + (cid:102)aa(cid:48)˜b˜b(cid:48) + (cid:102)ab(cid:48)˜a(cid:48)˜b + (cid:103)a(cid:48)b(cid:48)˜a˜b
− (cid:103)a(cid:48)bb(cid:48)˜a + (cid:102)a(cid:48)b˜a˜b(cid:48) + ˜a˜a(cid:48) (cid:102)bb(cid:48) − 3˜a˜b˜a(cid:48)˜b(cid:48).

The expansion of ˆS(t, t(cid:48)) can be done in the same way. By the triangle inequality, we have

(cid:12)
(cid:12)
(cid:12)

ˆS(t, t(cid:48)) − S(t, t(cid:48))

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)aa(cid:48)b(cid:48) b − (cid:93)aa(cid:48)b(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bb(cid:48)a − (cid:103)a(cid:48)bb(cid:48)˜a
(cid:12) +

(cid:48)

(cid:12)
(cid:12)
(cid:12)aba(cid:48) b
(cid:12)
(cid:12)
(cid:12)aa(cid:48) b b
(cid:12)
(cid:12)
(cid:12)a(cid:48)bab

(cid:48)

(cid:48)

(cid:12)
(cid:12)

(cid:12)abb(cid:48)a(cid:48) − (cid:103)abb(cid:48)˜a(cid:48)(cid:12)
− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12) +
(cid:12)
(cid:12)
− (cid:102)aa(cid:48)˜b˜b(cid:48)(cid:12)
(cid:12)ab(cid:48)a(cid:48)b − (cid:102)ab(cid:48)˜a(cid:48)˜b
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12) +
(cid:12)
(cid:12)
− (cid:102)a(cid:48)b˜a˜b(cid:48)(cid:12)
(cid:12)a a(cid:48)bb(cid:48) − ˜a˜a(cid:48) (cid:102)bb(cid:48)
(cid:12)
(cid:12)
(cid:12)
(cid:12) + 3
(cid:12) +

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)aba(cid:48)b
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a(cid:48)b(cid:48)ab − (cid:103)a(cid:48)b(cid:48)˜a˜b
(cid:12)
(cid:12)
(cid:12)
− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)aba(cid:48)b
(cid:12)
(cid:12)
(cid:12) .

(cid:48)

(cid:48)

The ﬁrst term
by applying Lemma 9. Recall that we write (x1, . . . , xm)+ for max(x1, . . . , xm).

(cid:12)
(cid:12)
(cid:12) can be bounded by applying the Hoeﬀding’s inequality. Other terms can be bounded

(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

Bounding
have

(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (1st term). Since −B2 ≤ aba(cid:48)b(cid:48) ≤ B2, by the Hoeﬀding’s inequality (Lemma 14), we

P

(cid:16)(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 2 exp

−

(cid:18)

(cid:19)

.

nt2
2B4

Bounding
We note that |f1(x, y)| ≤ (BBk, Bl)+ and |f2(y)| ≤ (BBk, Bl)+. Thus, by Lemma 9 with E = 2, we have

− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (2nd term). Let f1(x, y) = aba(cid:48) = k(x, v)l(y, w)k(x, v(cid:48)) and f2(y) = b(cid:48) = l(y, w(cid:48)).

(cid:12)
(cid:12)
(cid:12)aba(cid:48) b

(cid:48)

P

(cid:16)(cid:12)
(cid:12)
(cid:12)aba(cid:48) b

(cid:48)

− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t

≥ 1 − 4 exp

−

(cid:18)

nt2
8(BBk, Bl)4
+

(cid:19)

.

Bounding
b(cid:48) = l(y, w(cid:48)). We can see that |f1(x, y)|, |f2(x)|, |f3(y)| ≤ (B, Bk, Bl)+. Thus, by Lemma 9 with E = 3, we have

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (4th term). Let f1(x, y) = ab = k(x, v)l(y, w), f2(x) = a(cid:48) = k(x, v(cid:48)) and f3(y) =

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b

(cid:48)

P

(cid:16)(cid:12)
(cid:12)aba(cid:48)b
(cid:12)

(cid:48)

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 6 exp

−

(cid:18)

nt2
18(B, Bk, Bl)6
+

(cid:19)

.

19

(cid:48)

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b

− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (last term). Let f1(x) = a = k(x, v), f2(y) = b = l(y, w), f3(x) = a(cid:48) = k(x, v(cid:48)) and
Bounding
f4(y) = b(cid:48) = l(y, w(cid:48)). It can be seen that |f1(x)|, |f2(y)|, |f3(x)|, |f4(y)| ≤ (Bk, Bl)+. Thus, by Lemma 9 with
E = 4, we have

(cid:16)
3

P

(cid:12)
(cid:12)aba(cid:48)b
(cid:12)

(cid:48)

− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 8 exp

−

(cid:18)

nt2
32 · 32(Bk, Bl)8
+

(cid:19)

.

Bounds for other terms can be derived in a similar way to yield

(3rd term) P

(5th term) P

(6th term) P

(7th term) P

(8th term) P

(9th term) P

(10th term) P

(11th term) P

(cid:48)

(cid:17)

(cid:16)(cid:12)
(cid:12)

(cid:12)abb(cid:48)a(cid:48) − (cid:103)abb(cid:48)˜a(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t
(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)aa(cid:48)b(cid:48) b − (cid:93)aa(cid:48)b(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
− (cid:102)aa(cid:48)˜b˜b(cid:48)(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)aa(cid:48) b b
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:12)ab(cid:48)a(cid:48)b − (cid:102)ab(cid:48)˜a(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:12)a(cid:48)b(cid:48)ab − (cid:103)a(cid:48)b(cid:48)˜a˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bb(cid:48)a − (cid:103)a(cid:48)bb(cid:48)˜a
(cid:12) ≤ t
(cid:16)(cid:12)
− (cid:102)a(cid:48)b˜a˜b(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bab
(cid:12) ≤ t
(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t

(cid:16)(cid:12)
(cid:12)a a(cid:48)bb(cid:48) − ˜a˜a(cid:48) (cid:102)bb(cid:48)
(cid:12)

(cid:17)

(cid:48)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

≥ 1 − 4 exp

−

≥ 1 − 4 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

≥ 1 − 4 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

,

,

,

(cid:19)

(cid:19)

18(B2

nt2
8(BBl, Bk)4
+
nt2
8(BBk, Bl)4
+
(cid:19)
nt2
k, Bl)6
+
nt2
18(B, Bk, Bl)6
+
nt2
18(B, Bk, Bl)6
+
(cid:19)
nt2
8(BBl, Bk)4
+
nt2
18(B, Bk, Bl)6
+
(cid:19)
nt2
18(Bk, B2

l )6
+

.

(cid:19)

(cid:19)

,

(cid:19)

,

,

,

By the union bound, we have

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

ˆS(t, t(cid:48)) − S(t, t(cid:48))

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ 12t
(cid:19)

(cid:18)

≥ 1 −

2 exp

−

+ 4 exp

−

nt2
2B4

(cid:18)

(cid:19)

(cid:19)

(cid:19)

nt2
18(B, Bk, Bl)6
+
(cid:18)

+ 6 exp

−

(cid:18)

+ 8 exp

−

(cid:19)

nt2
18(B, Bk, Bl)6
+
nt2
32 · 32(Bk, Bl)8
+
(cid:19)

(cid:19) (cid:21)

(cid:19)

(cid:18)

+ 24 exp

−

nt2
18(B, Bk, Bl)6
+

(cid:19)

(cid:18)

+ 4 exp

−

(cid:19)

nt2
8(BBl, Bk)4
+

+ 6 exp

−

= 1 −

2 exp

−

+ 8 exp

−

+ 8 exp

−

(cid:20)

(cid:20)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

4 exp

−

4 exp

−

(cid:18)

+ 6 exp

−

(cid:19)

(cid:19)

nt2
8(BBk, Bl)4
+
nt2
8(BBl, Bk)4
+
(cid:19)
nt2
2B4

nt2
18(B2
k, Bl)6
+
(cid:19)

nt2
8(BBk, Bl)4
+
(cid:18)

+ 6 exp

−

(cid:18)

+ 6 exp

−

(cid:19)

18(B2

nt2
k, Bl)6
+
nt2
18(B, Bk, Bl)6
+

(cid:18)

(cid:19)

(cid:19)

nt2
8(BBk, Bl)4
+

(cid:18)

+ 6 exp

−

nt2
18(Bk, B2
l )6
+
(cid:18)
(cid:19)

+ 8 exp

−

+ 8 exp

(cid:18)

+ 8 exp

−

(cid:19)

+ 6 exp

122nt2
B∗
122nt2
B∗

−

(cid:18)

(cid:19)

(cid:20)

(cid:18)

≥ 1 −

2 exp

−

+ 6 exp

(cid:18)

= 1 − 62 exp

−

(cid:18)

122nt2
B∗
122nt2
B∗
122nt2
B∗

−

(cid:19)

,

nt2
18(B, Bk, Bl)6
+
(cid:18)

nt2
18(Bk, B2

l )6
+

(cid:18)

+ 6 exp

−

(cid:19)

(cid:18)

(cid:19)

+ 6 exp

−

nt2
8(BBl, Bk)4
+
(cid:18)

+ 8 exp

−

(cid:19) (cid:21)

nt2
32 · 32(Bk, Bl)8
+
(cid:19)
122nt2
B∗

−

(cid:18)

(cid:19)

122nt2
B∗
122nt2
B∗

−

(cid:18)

+ 24 exp

(cid:19) (cid:21)

where

B∗ :=

1
122 max(2B4, 8(BBk, Bl)4

+, 8(BBl, Bk)4

+, 18(B, Bk, Bl)6

+, 18(B2

k, Bl)6

+, 18(Bk, B2

l )6

+, 32 · 32(Bk, Bl)8

+).

20

By reparameterization, it follows that

P

(cid:18) c1Jn
γn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12) ≤ t

(cid:19)

≥ 1 − 62 exp

−

(cid:18)

nt2
γ2
c2
1J 2nB∗

(cid:19)

.

(23)

F.2.6 Union Bound for

(cid:12)
(cid:12)
(cid:12)

ˆλn − λn

(cid:12)
(cid:12)
(cid:12) and Final Lower Bound

Recall from (22) that

|ˆλn − λn| ≤

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12) +

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)
(cid:12)

√

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1Jn
γn
c1n
γn

(cid:12)
(cid:12)
(cid:12)
8B2J
n − 1

+

We will bound terms in (22) separately and combine all the bounds with the union bound. As shown in (8), the
U-statistic core h is bounded between −2B and 2B. Thus, by Lemma 13 (with m = 2), we have

√

(cid:16)

P

c2n

J|ˆu(t∗) − u(t∗)| ≤ t

≥ 1 − 2 exp

−

(cid:17)

(cid:18)

(cid:98)0.5n(cid:99)t2
8c2
2n2JB2

(cid:19)

.

Bounding c1n
γn

8B2J
n−1 + c3nγn + 4BJc1n

γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12). By Lemma 13 (with m = 2), it follows that

P

(cid:18) c1n
γn

8B2J
n − 1

≥ 1 − 2 exp

+ c3nγn +

(cid:104)

(cid:98)0.5n(cid:99)γ2
n

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12) ≤ t

(cid:19)

t − c1n
γn
27B4J 2c2

8B2J
n−1 − c3nγn
1n2

(cid:105)2









−

(cid:32)

= 1 − 2 exp

−

(a)
≥ 1 − 2 exp

(cid:32)

−

(cid:98)0.5n(cid:99) (cid:2)tγn(n − 1) − 8c1B2nJ − c3n(n − 1)γ2

n

(cid:3)2

(cid:33)

27B4J 2c2
(cid:2)tγn(n − 1) − 8c1B2nJ − c3n(n − 1)γ2

1n2(n − 1)2

n

(cid:3)2

(cid:33)

,

28B4J 2c2

1n2(n − 1)

(24)

(25)

where at (a) we used (cid:98)0.5n(cid:99) ≥ (n − 1)/2. Combining (23), (24), and (25) with the union bound (set T = 3t), we can
bound (22) with

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

ˆλn − λn

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ T

≥ 1 − 62 exp

(cid:32)

− 2 exp

−

(cid:18)

(cid:19)

(cid:18)

−

− 2 exp

nT 2
γ2
32c2
1J 2nB∗

(cid:98)0.5n(cid:99)T 2
72c2
2n2JB2
nn(n − 1)(cid:3)2
(cid:2)T γn(n − 1)/3 − 8c1B2nJ − c3γ2
28B4J 2c2

1n2(n − 1)

−

(cid:19)

(cid:33)

.

Since

ˆλn − λn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ T implies ˆλn ≥ λn − T , a reparametrization with r = λn − T gives
(cid:12)

P

(cid:16)ˆλn ≥ r

(cid:17)

≥ 1 − 62 exp

−

(cid:18)

(cid:19)

n(λn − r)2
γ2
32c2
1J 2nB∗

(cid:18)

(cid:98)0.5n(cid:99)(λn − r)2

(cid:19)

− 2 exp

−

72c2

2n2JB2

(cid:32)

− 2 exp

−

(cid:2)(λn − r)γn(n − 1)/3 − 8c1B2nJ − c3γ2
28B4J 2c2

1n2(n − 1)

nn(n − 1)(cid:3)2

(cid:33)

Grouping constants into ξ1, . . . ξ5 gives the result.

:= L(λn).

21

The lower bound L(λn) takes the form

1 − 62 exp (cid:0)−C1(λn − Tα)2(cid:1) − 2 exp (cid:0)−C2(λn − Tα)2(cid:1) − 2 exp

(cid:18)

−

[(λn − Tα)C3 − C4]2
C5

(cid:19)

,

where C1, . . . , C5 are positive constants. For ﬁxed large enough n such that λn > Tα, and ﬁxed signiﬁcance level α,
increasing λn will increase L(λn). Speciﬁcally, since n is ﬁxed, increasing u(cid:62)Σ−1u in λn = nu(cid:62)Σ−1u will increase
L(λn).

G Helper Lemmas

This section contains lemmas used to prove the main results in this work.

Lemma 8 (Product to sum). Assume that |ai| ≤ B, |bi| ≤ B for i = 1, . . . , E. Then
BE−1 (cid:80)E

j=1 |aj − bj|.

(cid:12)
(cid:81)E
(cid:12)
(cid:12)

i=1 ai − (cid:81)E

i=1 bi

(cid:12)
(cid:12)
(cid:12) ≤

Proof.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

i=1

E
(cid:89)

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

i=1

ai −

bj

≤

ai −

aibE

+

aibE −

aibE−1bE

+ . . . +

a1

bj −

bj

E−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E−1
(cid:89)

i=1

E−2
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:12)
(cid:32)E−2
(cid:12)
(cid:89)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

j=2

E
(cid:89)

j=1

E
(cid:89)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j=2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ |aE − bE|

ai

+ |aE−1 − bE−1|

ai

bE

+ . . . + |a1 − b1|

bj

≤ |aE − bE|BE−1 + |aE−1 − bE−1| BE−1 + . . . + |a1 − b1| BE−1

= BE−1

|aj − bj|

E
(cid:88)

j=1

applying triangle inequality, and the boundedness of ai and bi-s.

Lemma 9 (Product variant of the Hoeﬀding’s inequality). For i = 1, . . . , E, let {x(i)
j=1 ⊂ Xi be an i.i.d. sample
from a distribution Pi, and fi : Xi (cid:55)→ R be a measurable function. Note that it is possible that P1 = P2 = · · · = PE
and {x(1)
j=1. Assume that |fi(x)| ≤ B < ∞ for all x ∈ Xi and i = 1, . . . , E. Write ˆPi to denote
an empirical distribution based on the sample {x(i)

j=1 = · · · = {x(E)

j }nE

j }n1

j }ni

j }ni

j=1. Then,

P

(cid:32)(cid:12)
(cid:34) E
(cid:12)
(cid:89)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:35)
fi(x(i))

−

(cid:34) E
(cid:89)

E

x(i)∼ ˆPi

i=1

E
x(i)∼Pifi(x(i))

≤ T

≥ 1 − 2

exp

−

(cid:33)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:88)

i=1

(cid:18)

niT 2
2E2B2E

(cid:19)

.

Proof. By Lemma 8, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34) E
(cid:89)

E

i=1

x(i)∼ ˆPi

(cid:35)
fi(x(i))

−

(cid:34) E
(cid:89)

i=1

E
x(i)∼Pifi(x(i))

≤ BE−1

x(i)∼ ˆPi

fi(x(i)) − E

(cid:12)
x(i)∼Pi fi(x(i))
(cid:12)
(cid:12) .

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)

E

By
(cid:16)(cid:12)
P
E
(cid:12)
(cid:12)

applying

the
fi(x(i)) − E

x(i)∼ ˆPi

Hoeﬀding’s
(cid:12)
(cid:12)
x(i)∼Pifi(x(i))
(cid:12) ≤ t

inequality
(cid:17)

≥ 1 − 2 exp

to
(cid:16)
− 2nit2
4B2

each
(cid:17)

term

the
. The result is obtained with a union bound.

sum,

we

in

have

H External Lemmas

In this section, we provide known results referred to in this work.

Lemma 10 (Chwialkowski et al. [2015, Lemma 1]). If k is a bounded, analytic kernel (in the sense given in
Deﬁnition 1) on Rd × Rd, then all functions in the RKHS deﬁned by k are analytic.

22

Lemma 11 (Chwialkowski et al. [2015, Lemma 3]). Let Λ be an injective mapping from the space of probability
measures into a space of analytic functions on Rd. Deﬁne

d2
VJ

(P, Q) =

|[ΛP ](vj) − [ΛQ](vj)|2 ,

J
(cid:88)

j=1

where VJ = {vi}J
respect to the Lebesgue measure. Then, dVJ (P, Q) is almost surely (w.r.t. VJ ) a metric.

i=1 are vector-valued i.i.d. random variables from a distribution which is absolutely continuous with

Lemma 12 (Bochner’s theorem [Rudin, 2011]). A continuous function Ψ : Rd → R is positive deﬁnite if and only if
it is the Fourier transform of a ﬁnite nonnegative Borel measure ζ on Rd, that is, Ψ(x) = (cid:82)
Rd e−ix(cid:62)ω dζ(ω), x ∈ Rd.

Lemma 13 (A bound for U-statistics [Serﬂing, 2009, Theorem A, p. 201]). Let h(x1, . . . , xm) be a U-
statistic kernel for an m-order U-statistic such that h(x1, . . . , xm) ∈ [a, b] where a ≤ b < ∞. Let Un =
(cid:0) n
h(xi1 , . . . , xim ) be a U-statistic computed with a sample of size n, where the summation is over the
m
(cid:0) n
m

(cid:1)−1 (cid:80)
(cid:1) combinations of m distinct elements {i1, . . . , im} from {1, . . . , n}. Then, for t > 0 and n ≥ m,

i1<···<im

P(Un − Eh(x1, . . . , xm) ≥ t) ≤ exp (cid:0)−2(cid:98)n/m(cid:99)t2/(b − a)2(cid:1) ,
P(|Un − Eh(x1, . . . , xm)| ≥ t) ≤ 2 exp (cid:0)−2(cid:98)n/m(cid:99)t2/(b − a)2(cid:1) ,

where (cid:98)x(cid:99) denotes the greatest integer which is smaller than or equal to x. Hoeﬃnd’s inequality is a special case
when m = 1.

Lemma 14 (Hoeﬀding’s inequality). Let X1, . . . , Xn be i.i.d. random variables such that a ≤ Xi ≤ b almost surely.
Deﬁne X := 1
n

i=1 Xi. Then,

(cid:80)n

P (cid:0)(cid:12)

(cid:12)X − E[X](cid:12)

(cid:12) ≤ α(cid:1) ≥ 1 − 2 exp

(cid:18)

−

2nα2
(b − a)2

(cid:19)

.

References

[sup4] K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton. Fast Two-Sample Testing with Analytic
Representations of Probability Measures. In Advances in Neural Information Processing Systems (NIPS), pages
1981–1989. 2015.

[sup15] W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gretton. Interpretable Distribution Features with Maximum

Testing Power. 2016. URL http://arxiv.org/abs/1605.06796.

[sup3] W. Rudin. Fourier analysis on groups. John Wiley & Sons, 2011.

[sup21] R. J. Serﬂing. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 2009.

23

6
1
0
2
 
t
c
O
 
5
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
8
7
4
0
.
0
1
6
1
:
v
i
X
r
a

An Adaptive Test of Independence with Analytic Kernel Embeddings

Wittawat Jitkrittum,1

Zoltán Szabó,2∗ Arthur Gretton1

1Gatsby Computational Neuroscience Unit, University College London
2Department of Applied Mathematics, CMAP, École Polytechnique

wittawat@gatsby.ucl.ac.uk
zoltan.szabo@polytechnique.edu
arthur.gretton@gmail.com

October 18, 2016

Abstract

A new computationally eﬃcient dependence measure, and
an adaptive statistical test of independence, are proposed.
The dependence measure is the diﬀerence between ana-
lytic embeddings of the joint distribution and the product
of the marginals, evaluated at a ﬁnite set of locations
(features). These features are chosen so as to maximize a
lower bound on the test power, resulting in a test that
is data-eﬃcient, and that runs in linear time (with re-
spect to the sample size n). The optimized features can
be interpreted as evidence to reject the null hypothesis,
indicating regions in the joint domain where the joint
distribution and the product of the marginals diﬀer most.
Consistency of the independence test is established, for
an appropriate choice of features. In real-world bench-
marks, independence tests using the optimized features
perform comparably to the state-of-the-art quadratic-
time HSIC test, and outperform competing O(n) and
O(n log n) tests.

1

Introduction

We consider the design of adaptive, nonparametric sta-
tistical tests of dependence: that is, tests of whether
a joint distribution Pxy factorizes into the product of
marginals PxPy. While classical tests of dependence,
such as Pearson’s correlation and Kendall’s τ , are able to
detect monotonic relations between univariate variables,
more modern tests can address complex interactions, for
instance changes in variance of X with the value of Y .
Key to many recent tests is to examine covariance or
correlation between data features. These interactions
become signiﬁcantly harder to detect, and the features

∗Zoltán

Szabó’s
0000-0001-6183-7603.

ORCID

ID:

http://orcid.org/

are more diﬃcult to design when the data reside in high
dimensions.

A basic nonlinear dependence measure is the Hilbert-
Schmidt Independence Criterion (HSIC), which is the
Hilbert-Schmidt norm of the covariance operator between
feature mappings of the random variables [Gretton et al.,
2005, 2008]. Each random variable X and Y is mapped to
a respective reproducing kernel Hilbert space Hk and Hl.
For suﬃciently rich mappings, the covariance operator
norm is zero if and only if the variables are indepen-
dent. A second basic nonlinear dependence measure is
the smoothed diﬀerence between the characteristic func-
tion of the joint distribution, and that of the product
of marginals. When a particular smoothing function is
used, the statistic corresponds to the covariance between
distances of X and Y variable pairs [Feuerverger, 1993,
Székely et al., 2007, Székely and Rizzo, 2009], yielding
a simple test statistic. It has been shown by Sejdinovic
et al. [2013] that the distance covariance (and its gener-
alization to semi-metrics) is an instance of HSIC for an
appropriate choice of kernels. A disadvantage of these
feature covariance statistics, however, is that they require
quadratic time to compute (besides in the special case of
the distance covariance with univariate real-valued vari-
ables, where Huo and Székely [2014] achieve an O(n log n)
cost). Moreover, the feature covariance statistics have
intractable null distributions, and either a permutation
approach or the solution of an expensive eigenvalue prob-
lem [e.g. Zhang et al., 2011] is required for consistent
estimation of the quantiles. Several approaches were
proposed by Zhang et al. [2016] to obtain faster tests
along the lines of HSIC. These include computing HSIC
on ﬁnite-dimensional feature mappings chosen as ran-
dom Fourier features (RFFs) [Rahimi and Recht, 2008], a
block-averaged statistic, and a Nyström approximation to
the statistic. Key to each of these approaches is a more
eﬃcient computation of the statistic and its threshold un-

1

der the null distribution: for RFFs, the null distribution
is a ﬁnite weighted sum of χ2 variables; for the block-
averaged statistic, the null distribution is asymptotically
normal; for Nyström, either a permutation approach is
employed, or the spectrum of the Nyström approximation
to the kernel matrix is used in approximating the null
distribution. Each of these methods costs signiﬁcantly
less than the O(n2) cost of the full HSIC (the cost is
linear in n, but also depends quadratically on the number
of features retained). A potential disadvantage of the
Nyström and Fourier approaches is that the features are
not optimized to maximize test power, but are chosen
randomly. The block statistic performs worse than both,
due to the large variance of the statistic under the null
(which can be mitigated by observing more data).

In addition to feature covariances, correlation measures
have also been developed in inﬁnite dimensional feature
spaces: in particular, Bach and Jordan [2002], Fukumizu
et al. [2008] proposed statistics on the correlation operator
in a reproducing kernel Hilbert space. While convergence
has been established for certain of these statistics, their
computational cost is high at O(n3), and test thresholds
have relied on permutation. A number of much faster
approaches to testing based on feature correlations have
been proposed, however. For instance, Dauxois and Nkiet
[1998] compute statistics of the correlation between ﬁnite
sets of basis functions, chosen for instance to be step func-
tions or low order B-splines. The cost of this approach is
O(n). This idea was extended by Lopez-Paz et al. [2013],
who computed the canonical correlation between ﬁnite
sets of basis functions chosen as random Fourier features;
in addition, they performed a copula transform on the
inputs, with a total cost of O(n log n). Finally, space
partitioning approaches have also been proposed, based
on statistics such as the KL divergence, however these
apply only to univariate variables [Heller et al., 2016], or
to multivariate variables of low dimension [Gretton and
Györﬁ, 2010] (that said, these tests have other advantages
of theoretical interest, notably distribution-independent
test thresholds).

The approach we take is most closely related to HSIC
on a ﬁnite set of features. Our simplest test statistic,
the Finite Set Independence Criterion (FSIC), is an av-
erage of covariances of analytic functions (i.e., features)
deﬁned on each of X and Y . A normalized version of
the statistic (NFSIC) yields a distribution-independent
asymptotic test threshold. We show that our test is con-
sistent, despite a ﬁnite number of analytic features being
used, via a generalization of arguments in Chwialkowski
et al. [2015]. As in recent work on two-sample testing
by Jitkrittum et al. [2016], our test is adaptive in the
sense that we choose our features on a held-out valida-
tion set to optimize a lower bound on the test power.
The design of features for independence testing turns out

to be quite diﬀerent to the case of two-sample testing,
however: the task is to ﬁnd correlated feature pairs on
the respective marginal domains, rather than attempting
to ﬁnd a single, high-dimensional feature representation
for the entire (x, y) (as we would need to do if we were
comparing distributions Pxy and Qxy, rather than testing
a speciﬁc property of Pxy). We demonstrate the perfor-
mance of our tests on several challenging artiﬁcial and
real-world datasets, including detection of dependence
between music and its year of appearance, and between
videos and captions. In these experiments, we outperform
competing linear and O(n log n) time tests.

2

Independence Criteria and Sta-
tistical Tests

We introduce two test statistics: ﬁrst, the Finite Set
Independence Criterion (FSIC), which builds on the prin-
ciple that dependence can be measured in terms of the
covariance between data features. Next, we propose a nor-
malized version of this statistic (NFSIC), with a simpler
asymptotic distribution when Pxy = PxPy. We show how
to select features for the latter statistic to maximize a
lower bound on the power of its corresponding statistical
test.

2.1 The Finite Set Independence Crite-

rion

We begin by introducing the Hilbert-Schmidt Indepen-
dence Criterion (HSIC) as proposed in Gretton et al.
[2005], since our unnormalized statistic is built along sim-
ilar lines. Consider two random variables X ∈ X ⊂ Rdx
and Y ∈ Y ⊂ Rdy . Denote by Pxy the joint distribution
between X and Y ; Px and Py are the marginal distribu-
tions of X and Y . Let ⊗ denote the tensor product, such
that (a ⊗ b) c = a (cid:104)b, c(cid:105). Assume that k : X × X → R
and l : Y × Y → R are positive deﬁnite kernels associated
with reproducing kernel Hilbert spaces (RKHS) Hk and
Hl, respectively. Let (cid:107) · (cid:107)HS be the norm on the space
of Hl → Hk Hilbert-Schmidt operators. Then, HSIC
between X and Y is deﬁned as
HSIC(X, Y ) = (cid:13)
(cid:13)
2
(cid:13)µxy − µx ⊗ µy
(cid:13)
HS
= E(x,y),(x(cid:48),y(cid:48)) [k(x, x(cid:48))l(y, y(cid:48))]

+ ExEx(cid:48)[k(x, x(cid:48))]EyEy(cid:48)[l(y, y(cid:48))]
− 2E(x,y) [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] ,

(1)

is an independent copy of x.

where Ex := Ex∼Px , Ey := Ey∼Py , E(x,y) := E(x,y)∼Pxy ,
and x(cid:48)
The mean
embedding of Pxy belongs to the space of Hilbert-
Schmidt operators from Hl to Hk, µxy := (cid:82)
X ×Y k(x, ·) ⊗
l(y, ·) dPxy(x, y) ∈ HS(Hl, Hk), and the marginal mean

2

embeddings are µx := (cid:82)
X k(x, ·) dPx(x) ∈ Hk and
µy := (cid:82)
Y l(y, ·) dPy(y) ∈ Hl [Smola et al., 2007]. Gret-
ton et al. [2005, Theorem 4] show that if the kernels k
and l are universal [Steinwart and Christmann, 2008] on
compact domains X and Y, then HSIC(X, Y ) = 0 if and
only if X and Y are independent. Alternatively, Gretton
[2015] shows that it is suﬃcient for each of k and l to be
characteristic to their respective domains (meaning that
distribution embeddings are injective in each marginal
domain: see Sriperumbudur et al. [2010]). Given a joint
sample Zn = {(xi, yi)}n
i=1 ∼ Pxy, an empirical estimator
of HSIC can be computed in O(n2) time by replacing the
population expectations in (1) with their corresponding
empirical expectations based on Zn.

(cid:80)J

We now propose our new linear-time dependence mea-
sure, the Finite Set Independence Criterion (FSIC). Let
X ⊂ Rdx and Y ⊂ Rdy be open sets. Deﬁne the empir-
ical measure ν := 1
i=1 δ(vi,wi) over J test locations
J
VJ := {(vi, wi)}J
i=1 ⊂ X × Y where δt denotes the Dirac
measure centered on t, and (vi, wi) are realizations from
an absolutely continuous distribution (wrt the Lebesgue
measure). Write Exy for E(x,y)∼Pxy . The idea is to
see µxy(v, w) = Exy[k(x, v)l(y, w)], µx(v) = Ex[k(x, v)]
and µy(w) = Ey[l(y, w)] as smooth functions, and con-
sider an L2(X × Y, ν) distance between µxy and µxµy
instead of a Hilbert-Schmidt distance as in HSIC [Gret-
ton et al., 2005]. Let µxµy(x, y) := µx(x)µy(y). FSIC is
deﬁned as

FSIC2(X, Y ) := (cid:107)µxy − µxµy(cid:107)2

L2(X ×Y,ν)

(µxy(x, y) − µx(x)µy(y))2 dν(x, y)

=

:=

(cid:90)

(cid:90)

X

1
J

Y
J
(cid:88)

i=1

u(vi, wi)2 =

(cid:107)u(cid:107)2

2, where

1
J

u(v, w) := µxy(v, w) − µx(v)µy(w)
= Exy[k(x, v)l(y, w)] − Ex[k(x, v)]Ey[l(y, w)],
= covxy[k(x, v), l(y, w)],

(2)

and u := (u(v1, w1), . . . , u(vJ , wJ ))(cid:62).

Our ﬁrst result in Proposition 2 states that FSIC(X, Y )
almost surely deﬁnes a dependence measure for the ran-
dom variables X and Y , provided that the product kernel
on the joint space X × Y is characteristic and analytic
(see Deﬁnition 1).

Deﬁnition 1 (Analytic kernels [Chwialkowski et al.,
2015]). Let X be an open set in Rd. A positive deﬁ-
nite kernel k : X × X → R is said to be analytic on its
domain X × X if for all v ∈ X , f (x) := k(x, v) is an
analytic function on X .

Assumption A. The kernels k : X × X → R and l :
Y × Y → R are bounded by Bk and Bl respectively, and

the product kernel g((x, y), (x(cid:48), y(cid:48))) := k(x, x(cid:48))l(y, y(cid:48)) is
characteristic [Sriperumbudur et al., 2010, Deﬁnition 6],
and analytic (Deﬁnition 1) on (X × Y) × (X × Y).

Proposition 2 (FSIC is a dependence measure). Assume
that
1. Assumption A holds.
2. The test locations VJ = {(vi, wi)}J

i=1 are drawn from

an absolutely continuous distribution.

Then, almost surely, FSIC(X, Y ) = 1√
J
only if X and Y are independent.

(cid:107)u(cid:107)2 = 0 if and

Proof. Since g is characteristic, the mean embedding map
Πg : P (cid:55)→ E(x,y)∼P [g((x, y), ·)] is injective [Sriperum-
budur et al., 2010, Section 3], where P is a probability
distribution on X × Y. Since g is analytic, by Lemma 10
(Appendix), µxy and µxµy are analytic functions. Thus,
Lemma 11 (Appendix, setting Λ = Πg) guarantees that
FSIC(X, Y ) = 0 ⇐⇒ Pxy = PxPy ⇐⇒ X and Y are
independent almost surely.

FSIC uses µxy as a proxy for Pxy, and µxµy as a proxy
for PxPy. Proposition 2 suggests that, to detect the
dependence between X and Y , it is suﬃcient to evaluate at
a ﬁnite number of locations (deﬁned by VJ ) the diﬀerence
of the population joint embedding µxy and the embedding
of the product of the marginal distributions µxµy. A brief
explanation to justify this property is as follows. If Pxy =
PxPy, then ρ(v, w) := µxy(v, w)−µxµy(v, w) is zero, and
FSIC(X, Y ) = 0 for any VJ . If Pxy (cid:54)= PxPy, then ρ will
not be a zero function, since the mean embedding map is
injective (require the product kernel to be characteristic).
Using the same argument as in Chwialkowski et al. [2015],
since k and l are analytic, ρ is also analytic, and the
set of roots R := {(v, w) | ρ(v, w) = 0} has Lebesgue
measure zero. Thus, it is suﬃcient to draw (v, w) from an
absolutely continuous distribution, as we are guaranteed
that (v, w) /∈ R giving FSIC(X, Y ) > 0.

For FSIC to be a dependence measure, the product
kernel is required to be characteristic and analytic. We
next show in Proposition 3 that Gaussian kernels k and l
yield such a product kernel.

3

is

=

(A

characteristic

product
and
exp (cid:0)−(x − x(cid:48))(cid:62)A(x − x(cid:48))(cid:1)

Gaussian
of
Proposition
analytic). Let
kernels
k(x, x(cid:48))
and
l(y, y(cid:48)) = exp (cid:0)−(y − y(cid:48))(cid:62)B(y − y(cid:48))(cid:1) be Gaussian
kernels on Rdx × Rdx and Rdy × Rdy respectively,
for positive deﬁnite matrices A and B.
Then,
g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteristic and
analytic on (Rdx × Rdy ) × (Rdx × Rdy ).
Proof (sketch). The main idea is to use the fact a Gaus-
sian kernel is analytic, and a product of Gaussian kernels
is a Gaussian kernel on the pair of variables. See the full
proof in Appendix D.

3

i=1

Plug-in Estimator We now give an empirical estima-
tor of FSIC. Assume that we observe a joint sample Zn :=
i.i.d.∼ Pxy. Unbiased estimators of µxy(v, w)
{(xi, yi)}n
(cid:80)n
and µxµy(v, w) are ˆµxy(v, w) := 1
i=1 k(xi, v)l(yi, w)
n
(cid:80)
and (cid:91)µxµy(v, w) :=
j(cid:54)=i k(xi, v)l(yj, w),
respectively. A straightforward empirical estimator of
FSIC2 is then given by

1
n(n−1)

(cid:80)n

i=1

(cid:92)FSIC2(Zn) =

ˆu(vi, wi)2,

1
J

J
(cid:88)

i=1

ˆu(v, w) := ˆµxy(v, w) − (cid:91)µxµy(v, w)

=

2
n(n − 1)

(cid:88)

i<j

h(v,w)((xi, yi), (xj, yj)),

(3)

(4)

J ˆu(cid:62) ˆu.

1
h(v,w)((x, y), (x(cid:48), y(cid:48)))
2 (k(x, v) −
where
k(x(cid:48), v))(l(y, w) − l(y(cid:48), w)).
For conciseness, we
deﬁne ˆu := (ˆu1, . . . , ˆuJ )(cid:62) ∈ RJ where ˆui := ˆu(vi, wi) so
that (cid:92)FSIC2(Zn) = 1

:=

(cid:92)FSIC2 can be eﬃciently computed in O((dx + dy)Jn)
time [see (3)], assuming that the runtime complexity of
evaluating k(x, v) is O(dx) and that of l(y, w) is O(dy).
The unbiasedness of (cid:91)µxµy is necessary for (4) to be a U-
statistic. This fact and the rewriting of (cid:92)FSIC2 in terms of
h(v,w)((x, y), (x(cid:48), y(cid:48))) will be exploited when the asymp-
totic distribution of ˆu is derived (Proposition 4).

Since FSIC satisﬁes FSIC(X, Y ) = 0 ⇐⇒ X ⊥ Y , in
principle its empirical estimator can be used as a test
statistic for an independence test proposing a null hy-
pothesis H0 : “X and Y are independent” against an
alternative H1 : “X and Y are dependent”. The null
distribution (i.e., distribution of the test statistic assum-
ing that H0 is true) is challenging to obtain, however
and depends on the unknown Pxy. This prompts us to
consider a normalized version of FSIC whose asymptotic
null distribution of a convenient form. We ﬁrst derive the
asymptotic distribution of ˆu in Proposition 4, which we
use to derive the normalized test statistic in Theorem 5.
As a shorthand, we write z := (x, y), and t := (v, w).

Proposition 4 (Asymptotic distribution of ˆu). Deﬁne
˜k(x, v) := k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) := l(y, w) −
Ey(cid:48)l(y(cid:48), w). Then, under both H0 and H1, for any ﬁxed
locations t and t(cid:48),
covz[ˆu(t), ˆu(t(cid:48))] n→∞−−−−→ covz[˜k(x, v)˜l(y, w), ˜k(x, v(cid:48))˜l(y, w(cid:48))]
= Exy[(cid:0)˜k(x, v)˜l(y, w) − u(t)(cid:1)(cid:0)˜k(x, v(cid:48))˜l(y, w(cid:48)) − u(t(cid:48))(cid:1)],

where u(v, w) is given in (2), and ˆu(v, w) is deﬁned
in (4). Second, if 0 < covz[ˆu(ti), ˆu(ti)] < ∞ for i =
n(ˆu − u) d→ N (0, Σ) as n → ∞, where
1, . . . , J, then
Σij = cov[ˆu(ti), ˆu(tj)] and u := (u(t1), . . . , u(tJ ))(cid:62).

√

Proof. We ﬁrst note that for a ﬁxed t = (v, w), ˆu(v, w) is
a one-sample second-order U-statistic [Serﬂing, 2009, Sec-
tion 5.1.3] with a U-statistic kernel ht where ht(a, b) =

ht(b, a). Thus, by Kowalski and Tu [2008, Section 5.1,
Theorem 1], it follows directly that cov[ˆu(t), ˆu(t(cid:48))] =
4covz[Eaht(z, a), Ebht(cid:48)(z, b)]. Substituting ht with its
deﬁnition yields the ﬁrst claim, where we note that
Exy[˜k(x, v)˜l(y, w)] = u(v, w).

For the second claim, since ˆu is a multivariate one-
sample U-statistic, by Lehmann [1999, Theorem 6.1.6]
and Kowalski and Tu [2008, Section 5.1, Theorem 1], it
n(ˆu − u) d→ N (0, Σ) as n → ∞, where

√

follows that
Σij = cov[ˆu(ti), ˆu(tj)].

Recall from Proposition 2 that u = 0 holds almost
surely under H0. The asymptotic normality in the second
claim of Proposition 4 implies that n(cid:92)FSIC2 = n
J ˆu(cid:62) ˆu con-
verges in distribution to a sum of J dependent weighted
χ2 random variables. The dependence comes from the
fact that the coordinates ˆu1 . . . , ˆuJ of ˆu all depend on
the sample Zn. This null distribution is not analytically
tractable, and requires a large number of simulations to
compute the rejection threshold Tα for a given signiﬁcance
value α.

2.2 Normalized FSIC and Adaptive Test

For the purpose of an independence test, we will consider
a normalized variant of (cid:92)FSIC2, which we call (cid:92)NFSIC2,
whose tractable asymptotic null distribution is χ2(J), the
chi-squared distribution with J degrees of freedom. We
then show that the independence test deﬁned by (cid:92)NFSIC2
is consistent. These results are given in Theorem 5.
Theorem 5 (Independence test using (cid:92)NFSIC2 is con-
sistent). Let ˆΣ be a consistent estimate of Σ based on
the joint sample Zn. The (cid:92)NFSIC2 statistic is deﬁned as
ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu where γn ≥ 0 is a regulariza-
tion parameter. Assume that

(cid:17)−1

1. Assumption A holds.

2. Σ is invertible almost surely with respect to VJ =
i=1 drawn from an absolutely continuous

{(vi, wi)}J
distribution.

3. limn→∞ γn = 0.
Then, for any k, l and VJ satisfying the assumptions,
1. Under H0, ˆλn
2. Under H1, for any r ∈ R, limn→∞ P

= 1
almost surely. That is, the independence test based on
(cid:92)NFSIC2 is consistent.

d→ χ2(J) as n → ∞.

(cid:16)ˆλn ≥ r

(cid:17)

Proof (sketch) . Under H0, nˆu(cid:62)( ˆΣ + γnI)−1 ˆu asymptot-
ically follows χ2(J) because
nˆu is asymptotically nor-
mally distributed (see Proposition 4). Claim 2 builds on
the result in Proposition 2 stating that u (cid:54)= 0 under H1;

√

4

it follows using the convergence of ˆu to u. The full proof
can be found in Appendix E.

Theorem 5 states that if H1 holds, the statistic can
be arbitrarily large as n increases, allowing H0 to be
rejected for any ﬁxed threshold. Asymptotically the test
threshold Tα is given by the (1 − α)-quantile of χ2(J) and
is independent of n. The assumption on the consistency
of ˆΣ is required to obtain the asymptotic chi-squared
distribution. The regularization parameter γn is to ensure
that ( ˆΣ + γnI)−1 can be stably computed. In practice,
γn requires no tuning, and can be set to be a very small
constant.

The next proposition states that the computational
complexity of the (cid:92)NFSIC2 estimator is linear in both
the input dimension and sample size, and that it can
be expressed in terms of the K =[K ij] = [k(vi, xj)] ∈
RJ×n, L = [Lij] = [l(wi, yj)] ∈ RJ×n matrices.

Proposition 6 (An empirical estimator of (cid:92)NFSIC2). Let
1n := (1, . . . , 1)(cid:62) ∈ Rn. Denote by ◦ the element-wise
matrix product. Then,
1. ˆu = (K◦L)1n

n−1 − (K1n)◦(L1n)

n(n−1)

.

2. A consistent estimator for Σ is ˆΣ = ΓΓ(cid:62)

n where

Γ := (K − n−1K1n1(cid:62)
n ) ◦ (L − n−1L1n1(cid:62)
ˆub = n−1 (K ◦ L) 1n − n−2 (K1n) ◦ (L1n) .

n ) − ˆub1(cid:62)
n ,

Assume that the complexity of the kernel evaluation is
linear in the input dimension. Then the test statistic
ˆλn = nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu can be computed in O(J 3 +
J 2n + (dx + dy)Jn) time.

(cid:17)−1

Proof (sketch). Claim 1 for ˆu is straightforward. The
expression for ˆΣ in claim 2 follows directly from the
asymptotic covariance expression in Proposition 4. The
consistency of ˆΣ can be obtained by noting that the
ﬁnite sample bound for P((cid:107) ˆΣ − Σ(cid:107)F > t) decreases as
n increases. This is implicitly shown in Appendix F.2.2
and its following sections.

Although the dependency of the estimator on J is cubic,
we empirically observe that only a small value of J is
required (see Section 3). The number of test locations
J relates to the number of regions in X × Y of pxy and
pxpy that diﬀer (see Figure 1). In particular, J need not
increase with n for test consistency.

Our ﬁnal theoretical result gives a lower bound on
the test power of (cid:92)NFSIC2 i.e., the probability of correctly
rejecting H0. We will use this lower bound as the objective
function to determine VJ and the kernel parameters. Let
(cid:107) · (cid:107)F be the Frobenius norm.

Theorem 7 (A lower bound on the test power). Let
NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let K be a kernel
class for k, L be a kernel class for l, and V be a collection
with each element being a set of J locations. Assume that
that
1. There
and

ﬁnite Bk
supk∈K supx,x(cid:48)∈X |k(x, x(cid:48))|
supl∈L supy,y(cid:48)∈Y |l(y, y(cid:48))| ≤ Bl.

and Bl
≤

such
Bk

exist

2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)Σ−1(cid:107)F < ∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test
power satisﬁes P

≥ L(λn) where

(cid:16)ˆλn ≥ r

(cid:17)

L(λn) = 1 − 62e−ξ1γ2

− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

n(λn−r)2/n − 2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

1

2JB2,
(cid:98)·(cid:99) is the ﬂoor function, ξ1 :=
B := BkBl, ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 :=
28B4J 2c2
J ˜c, and B∗ is
a constant depending on only Bk and Bl. Moreover, for
suﬃciently large ﬁxed n, L(λn) is increasing in λn.

1J 2B∗ , ξ2 := 72c2

1, c1 := 4B2J

J ˜c, c2 := 4B

32c2

√

√

(cid:17)

(cid:17)

(cid:16)

(cid:111)

(cid:110)

y,u]

y ∈ [σ2

x ∈ [σ2

y,l < σ2

(x, v) (cid:55)→ exp

(y, w) (cid:55)→ exp

the kernels k and l,

− (cid:107)x−v(cid:107)2
2σ2
x
x,l < σ2
| σ2

in Appendix F. To put
We provide the proof
let θx and θy be the
Theorem 7 into perspective,
parameters of
respectively.
We denote by θ = {θx, θy, VJ } the collection of
Assume that
all tuning parameters of the test.
x,l, σ2
| σ2
x,u]
K =
=:
Kg for some 0 < σ2
x,u < ∞ and L =
(cid:111)
(cid:110)
(cid:16)
− (cid:107)y−w(cid:107)2
y,l, σ2
=: Lg for
2σ2
y
some 0 < σ2
y,u < ∞ are Gaussian kernel classes.
Then, in Theorem 7, B = Bk = Bl = 1, and B∗ = 2.
The assumption ˜c < ∞ is a technical condition to guar-
antee that the test power lower bound is ﬁnite for all
θ deﬁned by the feasible sets K, L, and V. Let V(cid:15),r :=
(cid:8)VJ | (cid:107)vi(cid:107)2, (cid:107)wi(cid:107)2 ≤ r and (cid:107)vi − vj(cid:107)2
2 ≥
(cid:15), for all i (cid:54)= j(cid:9). If we set K = Kg, L = Lg, and V = V(cid:15),r
for some (cid:15), r > 0, then ˜c < ∞ as Kg, Lg, and V(cid:15),r are
compact. In practice, these conditions do not necessarily
create restrictions as they almost always hold implicitly.
We show in Appendix C that the objective function used
to choose VJ will discourage any two locations to be in
the same neighborhood.

2 + (cid:107)wi − wj(cid:107)2

Parameter Tuning The test power lower bound
L(λn) in Theorem 7 is a function of λn = nu(cid:62)Σ−1u
which is the population counterpart of the test statistic
ˆλn. As in FSIC, it can be shown that λn = 0 if and
only if X are Y are independent (from Proposition 2).
If X and Y are dependent, then λn > 0. According to
Theorem 7, for a suﬃciently large n, the test power lower
bound is increasing in λn. One can therefore think of
λn (a function of θ) as representing how easily the test
rejects H0 given a problem Pxy. The higher the λn, the

5

3 Experiments

In this section, we empirically study the performance
of the proposed method on both toy (Section 3.1) and
real-life problems (Section 3.2). Our interest is in the
performance of linear-time tests on challenging problems
which require a large sample size to be able to accurately
reveal the dependence. All the code is available at https:
//github.com/wittawatj/fsic-test.

We compare the proposed NFSIC with optimization
(NFSIC-opt) to ﬁve multivariate nonparametric tests.
The (cid:92)NFSIC2 test without optimization (NFSIC-med) acts
as a baseline, allowing the eﬀect of parameter optimization
to be clearly seen. For pedagogical reason, we consider
the original HSIC test of Gretton et al. [2005] denoted by
QHSIC, which is a quadratic-time test. Nyström HSIC
(NyHSIC) uses a Nyström approximation to the kernel
matrices of X and Y when computing the HSIC statistic.
FHSIC is another variant of HSIC in which a random
Fourier feature approximation [Rahimi and Recht, 2008]
to the kernel is used. NyHSIC and FHSIC are studied in
Zhang et al. [2016] and can be computed in O(n), with
quadratic dependency on the number of inducing points
in NyHSIC, and quadratic dependency in the number
of random features in FHSIC. Finally, the Randomized
Dependence Coeﬃcient (RDC) proposed in Lopez-Paz
et al. [2013] is also considered. The RDC can be seen as
the primal form (with random Fourier features) of the
kernel canonical correlation analysis of Bach and Jordan
[2002] on copula-transformed data. We consider RDC
as a linear-time test even though preprocessing by an
empirical copula transform costs O((dx + dy)n log n).

We use Gaussian kernel classes Kg and Lg for both
X and Y in all the methods. Except NFSIC-opt, all
other tests use full sample to conduct the indepen-
dence test, where the Gaussian widths σx and σy are
set according to the widely used median heuristic i.e.,
σx = median ({(cid:107)xi − xj(cid:107)2 | 1 ≤ i < j ≤ n}), and σy is
set in the same way using {yi}n
i=1. The J locations for
NFSIC-med are randomly drawn from the standard multi-
variate normal distribution in each trial. For a sample of
size n, NFSIC-opt uses half the sample for parameter tun-
ing, and the other disjoint half for the test. We permute
the sample 300 times in RDC1 and HSIC to simulate
from the null distribution and compute the test threshold.
The null distributions for FHSIC and NyHSIC are given
by a ﬁnite sum of weighted χ2(1) random variables given
in Eq. 8 of Zhang et al. [2016]. Unless stated otherwise,
we set the test threshold of the two NFSIC tests to be the
(1 − α)-quantile of χ2(J). To provide a fair comparison,
we set J = 10, use 10 inducing points in NyHSIC, and 10

1We use a permutation test for RDC, following the authors’
(https://github.com/lopezpaz/randomized_

implementation
dependence_coefficient, referred commit: b0ac6c0).

(a) ˆµxy(v, w)

(b) (cid:92)µxµy(v, w)

(c) (cid:98)Σ(v, w)

(d) Statistic ˆλn(v, w)

Figure 1: Illustration of (cid:92)NFSIC2.

greater the lower bound on the test power, and thus the
more likely it is that the test will reject H0 when it is
false.

In light of this reasoning, we propose setting θ to θ∗ =
arg maxθ λn. That this procedure is also valid under H0
can be seen as follows. Under H0, θ∗ = arg maxθ 0 will be
d→ χ2(J)
arbitrary. Since Theorem 7 guarantees that ˆλn
as n → ∞ for any θ, the asymptotic null distribution does
not change by using θ∗. In practice, λn is a population
quantity which is unknown. We propose dividing the
sample Zn into two disjoint sets: training and test sets.
The training set is used to optimize for θ∗, and the test set
is used for the actual independence test with the optimized
θ∗. The splitting is to guarantee the independence of θ∗
and the test sample, which is an assumption of Theorem
5.

we

To

better

under (cid:92)NFSIC2,
visualize
ˆµxy(v, w), (cid:91)µxµy(v, w) and ˆΣ(v, w) as a function of one
In this
test location (v, w) on a simple toy problem.
problem, Y = −X + Z where Z ∼ N (0, 0.32). As we con-
sider only one location (J = 1), ˆΣ(v, w) is a scalar. The
(ˆµxy(v,w)−(cid:92)µxµy(v,w))2
statistic can be written as ˆλn = n
.
ˆΣ(v,w)
These components are shown in Figure 1, where we use
Gaussian kernels for both X and Y , and the horizontal
and vertical axes correspond to v ∈ R and w ∈ R,
respectively.

Intuitively, ˆu(v, w) = ˆµxy(v, w) − (cid:91)µxµy(v, w) captures
the diﬀerence of the joint distribution and the product of
the marginals as a function of (v, w). Squaring ˆu(v, w)
and dividing it by the variance shown in Figure 1c gives
the statistic (also the parameter tuning objective) shown
in Figure 1d. The latter ﬁgure suggests that the parameter
tuning objective function can be non-convex. However,
we note that the non-convexity arises since there are
multiple ways to detect the diﬀerence between the joint
distribution and the product of the marginals. In this case,
the lower left and upper right regions equally indicate the
largest diﬀerence.

6

(a) SG (α = 0.05)

(b) SG (α = 0.05)

(c) Sin

(d) GSign

Figure 2: (a): Runtime. (b): Probability of rejecting H0 as problem parameters vary. Fix n = 4000.

random Fourier features in FHSIC and RDC.

Optimization of NFSIC-opt The parameters of
NFSIC-opt are σx, σy, and J locations of size (dx +
dy)J. We treat all the parameters as a long vector in
R2+(dx+dy)J and use gradient ascent to optimize ˆλn/2.
We observe that initializing VJ by randomly picking J
points from the training sample yields good performance.
The regularization parameter γn in NFSIC is ﬁxed to a
small value, and is not optimized. It is worth emphasizing
that the complexity of the optimization procedure is still
linear in n.

Since FSIC, NyHFSIC and RDC rely on a ﬁnite-
dimensional kernel approximation, these tests are consis-
tent only if both the number of features increases with n.
By constrast, the proposed NFSIC requires only n to go
to inﬁnity to achieve consistency i.e., J can be ﬁxed. We
refer the reader to Appendix C for a brief investigation
of the test power vs. increasing J. The test power does
not necessarily monotonically increase with J.

3.1 Toy Problems

We consider three toy problems: Same Gaussian (SG),
Sinusoid (Sin), and Gaussian Sign (GSign).

1. Same Gaussian (SG). The two variables are inde-
pendently drawn from the standard multivariate normal
distribution i.e., X ∼ N (0, Idx ) and Y ∼ N (0, Idy ) where
Id is the d × d identity matrix. This problem represents
a case in which H0 holds.

2. Sinusoid (Sin). Let pxy be the probability density
of Pxy.
In the Sinusoid problem, the dependency of
X and Y is characterized by (X, Y ) ∼ pxy(x, y) ∝ 1 +
sin(ωx) sin(ωy), where the domains of X , Y = (−π, π)
and ω is the frequency of the sinusoid. As the frequency
ω increases, the drawn sample becomes more similar to
a sample drawn from Uniform((−π, π)2). That is, the
higher ω, the harder to detect the dependency between
X and Y . This problem was studied in Sejdinovic et al.
[2013]. Plots of the density for a few values of ω are
shown in Figures 6 and 7 in the appendix. The main
characteristic of interest in this problem is the local change
in the density function.

3. Gaussian Sign (GSign). In this problem, Y =

|Z| (cid:81)dx
i=1 sgn(Xi), where X ∼ N (0, Idx ), sgn(·) is the sign
function, and Z ∼ N (0, 1) serves as a source of noise. The
full interaction of X = (X1, . . . , Xdx ) is what makes the
problem challenging. That is, Y is dependent on X, yet
it is independent of any proper subset of {X1, . . . , Xd}.
Thus, simultaneous consideration of all the coordinates
of X is required to successfully detect the dependency.

We ﬁx n = 4000 and vary the problem parameters.
Each problem is repeated for 300 trials, and the sample is
redrawn each time. The signiﬁcance level α is set to 0.05.
The results are shown in Figure 2. It can be seen that
in the SG problem (Figure 2b) where H0 holds, all the
tests achieve roughly correct type-I errors at α = 0.05.
In the Sin problem, NFSIC-opt achieves the highest test
power for all considered ω = 1, . . . , 6, highlighting its
strength in detecting local changes in the joint density.
The performance of NFSIC-med is signiﬁcantly lower than
that of NFSIC-opt. This phenomenon clearly emphasizes
the importance of the optimization to place the locations
at the relevant regions in X × Y. RDC has a remarkably
high performance in both Sin and GSign (Figure 2c, 2d)
despite no parameter tuning. Interestingly, both NFSIC-
opt and RDC outperform the quadratic-time QHSIC
in these two problems. The ability to simultaneously
consider interacting features of NFSIC-opt is indicated
by its superior test power in GSign, especially at the
challenging settings of dx = 5, 6. An average trial runtime
for each test in the SG problem is shown in Figure 2a. We
observe that the runtime does not increase with dimension,
as the complexity of all the tests is linear in the dimension
of the input. All the tests are implemented in Python
using a common SciPy Stack.

To investigate the sample eﬃciency of all the tests,
we ﬁx dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in
GSign, and increase n. Figure 3 shows the results. The
quadratic dependency on n in QHSIC makes it infeasible
both in terms of memory and runtime to consider n larger
than 6000 (Figure 3a). In constrast, although not the
most time-eﬃcient, NFSIC-opt has the highest sample-
eﬃciency for GSign, and for Sin in the low-sample regime,
signiﬁcantly outperforming QHSIC. Despite the small
additional overhead from the optimization, we are yet
able to conduct an accurate test with n = 105, dx = dy =

7

(a) SG. dx = dy = 250.

(b) SG. dx = dy = 250.

(c) Sin. ω = 4.

(d) GSign. dx = 4.

Figure 3: (a) Runtime. (b): Probability of rejecting H0 as n increases in the toy problems.

250 in less than 100 seconds. We observe in Figure 3b
that the two NFSIC variants have correct type-I errors
across all sample sizes, indicating that the asymptotic null
distribution approximately holds by the time n reaches
1000. We recall from Theorem 5 that the NFSIC test
with random test locations will asymptotically reject H0
if it is false. A demonstration of this property is given in
Figure 3c, where the test power of NFSIC-med eventually
reaches 1 with n higher than 105.

3.2 Real Problems

We now examine the performance of our proposed test
on real problems.

Million Song Data (MSD) We consider a subset of
the Million Song Data2 [Bertin-Mahieux et al., 2011], in
which each song (X) out of 515,345 is represented by 90
features, of which 12 features are timbre average (over
all segments) of the song, and 78 features are timbre
covariance. Most of the songs are western commercial
tracks from 1922 to 2011. The goal is to detect the
dependency between each song and its year of release
(Y ). We set α = 0.01, and repeat for 300 trials where
the full sample is randomly subsampled to n points in
each trial. Other settings are the same as in the toy
problems. To make sure that the type-I error is correct,
we use the permutation approach in the NFSIC tests to
compute the threshold. Figure 4b shows the test powers
as n increases from 500 to 2000. To simulate the case
where H0 holds in the problem, we permute the sample
to break the dependency of X and Y . The results are
shown in Figure 5 in the appendix.

Evidently, NFSIC-opt has the highest test power among
all the linear-time tests for all the sample sizes. Its test
power is second to only QHSIC. We recall that NFSIC-opt
uses half of the sample for parameter tuning. Thus, at
n = 500, the actual sample for testing is 250, which is
relatively small. The fact that there is a vast power gain
from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500
suggests that the optimization procedure can perform
well even at a lower sample sizes.

(a) MSD problem.

(b) Videos & Captions problem.
Figure 4: Probability of rejecting H0 as n increases in
the two real problems. α = 0.01.

Videos and Captions Our last problem is based on
the VideoStory46K3 dataset [Habibian et al., 2014]. The
dataset contains 45,826 Youtube videos (X) of an average
length of roughly one minute, and their corresponding
text captions (Y ) uploaded by the users. Each video is
represented as a dx = 2000 dimensional Fisher vector en-
coding of motion boundary histograms (MBH) descriptors
of Wang and Schmid [2013]. Each caption is represented
as a bag of words with each feature being the frequency
of one word. After ﬁltering only words which occur in at
least six video captions, we obtain dy = 1878 words. We
examine the test powers as n increases from 2000 to 8000.
The results are given in Figure 4. The problem is suﬃ-
ciently challenging that all linear-time tests achieve a low
power at n = 2000. QHSIC performs exceptionally well
on this problem, achieving a maximum power throughout.
NFSIC-opt has the highest sample eﬃciency among the
linear-time tests, showing that the optimization procedure
is also practical in a high dimensional setting.

Acknowledgement

We thank the Gatsby Charitable Foundation for the ﬁ-
nancial support. The major part of this work was carried
out while Zoltán Szabó was a research associate at the
Gatsby Computational Neuroscience Unit, University Col-
lege London.

2Million Song Data subset: https://archive.ics.uci.edu/ml/

3VideoStory46K dataset:

https://ivi.fnwi.uva.nl/isis/

datasets/YearPredictionMSD.

mediamill/datasets/videostory.php.

8

References

T. W. Anderson. An Introduction to Multivariate Statis-

tical Analysis. Wiley, 2003.

F. R. Bach and M. I. Jordan. Kernel independent compo-
nent analysis. Journal of Machine Learning Research,
3:1–48, 2002.

T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and
P. Lamere. The million song dataset. In International
Conference on Music Information Retrieval (ISMIR),
2011.

K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and
A. Gretton. Fast Two-Sample Testing with Analytic
Representations of Probability Measures. In Advances
in Neural Information Processing Systems (NIPS),
pages 1981–1989. 2015.

X. Huo and G. J. Székely. Fast computing for distance
covariance. Technical report, 2014. URL https://
arxiv.org/abs/1410.1503.

W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gret-
ton. Interpretable Distribution Features with Maximum
Testing Power. 2016. URL http://arxiv.org/abs/
1605.06796.

J. Kowalski and X. M. Tu. Modern Applied U-Statistics.

John Wiley & Sons, 2008.

E. L. Lehmann. Elements of Large-Sample Theory.

Springer Science & Business Media, 1999.

D. Lopez-Paz, P. Hennig, and B. Schölkopf. The Random-
ized Dependence Coeﬃcient. In Advances in Neural
Information Processing Systems (NIPS), pages 1–9.
2013.

J. Dauxois and G. M. Nkiet. Nonlinear canonical analysis
and independence tests. The Annals of Statistics, 26
(4):1254–1278, 1998.

A. Rahimi and B. Recht. Random features for large-scale
kernel machines. In Advances in Neural Information
Processing Systems (NIPS), pages 1177–1184. 2008.

A. Feuerverger. A consistent test for bivariate dependence.
International Statistical Review, 61(3):419–433, 1993.

K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf. Ker-
nel measures of conditional dependence. In Advances in
Neural Information Processing Systems (NIPS), pages
489–496, 2008.

A. Gretton. A simpler condition for consistency of a
kernel independence test. Technical report, 2015. URL
http://arxiv.org/abs/1501.06103.

A. Gretton and L. Györﬁ. Consistent nonparametric
tests of independence. Journal of Machine Learning
Research, 11:1391–1423, 2010.

A. Gretton, O. Bousquet, A. Smola, and B. Schölkopf.
Measuring Statistical Dependence with Hilbert-
Schmidt Norms.
In Algorithmic Learning Theory
(ALT), pages 63–77. 2005.

A. Gretton, K. Fukumizu, C. H. Teo, L. Song,
B. Schölkopf, and A. J. Smola. A Kernel Statistical Test
of Independence. In Advances in Neural Information
Processing Systems (NIPS), pages 585–592. 2008.

A. Habibian, T. Mensink, and C. G. Snoek. Videostory:
A new multimedia embedding for few-example recogni-
tion and translation of events. In ACM International
Conference on Multimedia, pages 17–26, 2014.

R. Heller, Y. Heller, S. Kaufman, B. Brill, and M. Gorﬁne.
Consistent distribution-free k-sample and independence
tests for univariate random variables. Journal of Ma-
chine Learning Research, 17(29):1–54, 2016.

D. Sejdinovic, B. Sriperumbudur, A. Gretton, and
K. Fukumizu. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. The Annals of
Statistics, 41(5):2263–2291, 2013.

R. J. Serﬂing. Approximation Theorems of Mathematical

Statistics. John Wiley & Sons, 2009.

A. Smola, A. Gretton, L. Song, and B. Schölkopf. A
hilbert space embedding for distributions. In Inter-
national Conference on Algorithmic Learning Theory
(ALT), pages 13–31, 2007.

B. K. Sriperumbudur, A. Gretton, K. Fukumizu,
B. Schölkopf, and G. R. G. Lanckriet. Hilbert Space Em-
beddings and Metrics on Probability Measures. Journal
of Machine Learning Research, 11:1517–1561, 2010.

I. Steinwart and A. Christmann. Support vector machines.

Springer Science & Business Media, 2008.

G. J. Székely and M. L. Rizzo. Brownian distance covari-
ance. The Annals of Applied Statistics, 3(4):1236–1265,
2009.

G. J. Székely, M. L. Rizzo, and N. K. Bakirov. Measuring
and testing dependence by correlation of distances. The
Annals of Statistics, 35(6):2769–2794, 2007.

A. W. v. d. Vaart. Asymptotic Statistics. Cambridge

University Press, 2000.

H. Wang and C. Schmid. Action recognition with im-
proved trajectories. In IEEE International Conference
on Computer Vision (ICCV), pages 3551–3558, 2013.

9

K. Zhang, J. Peters, D. Janzing, B., and B. Schölkopf.
Kernel-based conditional independence test and appli-
cation in causal discovery. In Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pages 804–813, 2011.

Q. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. Large-
Scale Kernel Methods for Independence Testing. 2016.
URL http://arxiv.org/abs/1606.07892.

10

An Adaptive Test of Independence with Analytic Kernel Embeddings
Supplementary Material

A Type-I Errors

In this section, we show that all the tests have correct type-I errors (i.e., the probability of reject H0 when it is true)
in real problems. We permute the joint sample so that the dependency is broken to simulate cases in which H0
holds. The results are shown in Figure 5.

(a) MSD problem (permuted).

(a) Videos & Captions problem with shuf-
ﬂed sample.

Figure 5: Probability of rejecting H0 as n increases in the Million Song problem. α = 0.01.

B Redundant Test Locations

Here, we provide a simple illustration to show that two locations t1 = (v1, w1) and t2 = (v2, w2) which are too
close to each other will reduce the optimization objective. We consider the Sinusoid problem described in Section
3.1 with ω = 1, and use J = 2 test locations. In Figure 6, t1 is ﬁxed at the red star, while t2 is varied along the
horizontal line. The objective value ˆλn as a function of (t1, t2) is shown in the bottom ﬁgure. It can be seen that ˆλn
decreases sharply when t2 is in the neighborhood of t1. This property implies that two locations which are too close
will not maximize the objective function (i.e., the second feature contains no additional information when it matches
the ﬁrst). For J > 2, the objective sharply decreases if any two locations are in the same neighborhood.

Figure 6: Plot of optimization objective values as location t2 moves along the green line. The objective sharply
drops when the two locations are in the same neighborhood.

C Test Power vs. J

It might seem intuitive that as the number of locations J increases, the test power should also increase. Here, we
empirically show that this statement is not always true. Consider the Sinusoid toy example described in Section 3.1
with ω = 2 (also see the left ﬁgure of Figure 7). By construction, X and Y are dependent in this problem. We run

11

NFSIC test with a sample size of n = 800, varying J from 1 to 600. For each value of J, the test is repeated for 500
times. In each trial, the sample is redrawn and the J test locations are drawn from Uniform((−π, π)2). There is no
optimization of the test locations. We use Gaussian kernels for both X and Y , and use the median heuristic to set
the Gaussian widths to 1.8. Figure 7 shows the test power as J increases.

Figure 7: The Sinusoid problem and the plot of test power vs. the number of test locations.

We observe that the test power does not monotonically increase as J increases. When J = 1, the diﬀerence of pxy
and pxpy cannot be adequately captured, resulting in a low power. The power increases rapidly to roughly 0.8 at
J = 10, and stays at the maximum until about J = 100. Then, the power starts to drop sharply when J is higher
than 400 in this problem.

Unlike random Fourier features, the number of test locations in NFSIC is not the number of Monte Carlo particles
used to approximate an expectation. There is a tradeoﬀ: if the test locations are in key regions (i.e., regions in
which there is a big diﬀerence between pxy and pxpy), then they increase power; yet the statistic gains in variance
(thus reducing test power) as J increases. As can be seen in Figure 7, there are eight key regions (in blue) that can
reveal the diﬀerence of pxy and pxpy. Using an unnecessarily high J not only makes the covariance matrix ˆΣ harder
to estimate accurately, it also increases the computation as the complexity on J is O(J 3).

We note that NFSIC is not intended to be used with a large J. In practice, it should be set to be large enough so
as to capture the key regions as stated. As a practical guide, with optimization of the test locations, a good starting
point is J = 5 or 10.

D Proof of Proposition 3

Recall Proposition 3,
Proposition (A product of Gaussian kernels is characteristic and analytic). Let k(x, x(cid:48)) = exp (cid:0)−(x − x(cid:48))(cid:62)A(x − x(cid:48))(cid:1)
and l(y, y(cid:48)) = exp (cid:0)−(y − y(cid:48))(cid:62)B(y − y(cid:48))(cid:1) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for
positive deﬁnite matrices A and B. Then, g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteristic and analytic on
(Rdx × Rdy ) × (Rdx × Rdy ).

Proof. Let z := (x(cid:62), y(cid:62))(cid:62) and z(cid:48) := (x(cid:48)(cid:62), y(cid:48)(cid:62))(cid:62) be vectors in Rdx+dy . We prove by reducing the product kernel to
one Gaussian kernel with g(z, z(cid:48)) = exp (cid:0)−(z − z(cid:48))(cid:62)C(z − z(cid:48))(cid:1) where C :=
. Write g(z, z(cid:48)) = Ψ(z − z(cid:48))
where Ψ(t) := exp (cid:0)−t(cid:62)Ct(cid:1). Since C is positive deﬁnite, we see that the ﬁnite measure ζ corresponding to Ψ as
deﬁned in Lemma 12 has support everywhere in Rdx+dy . Thus, Sriperumbudur et al. [2010, Theorem 9] implies that
g is characteristic.

(cid:18) A 0
0 B

(cid:19)

To see that g is analytic, we observe that for each z(cid:48) ∈ Rdx+dy , z (cid:55)→ −(z − z(cid:48))(cid:62)C(z − z(cid:48)) is a multivariate
polynomial in z, which is known to be analytic. Using the fact that t (cid:55)→ exp(t) is analytic on R, and that a
composition of analytic functions is analytic, we see that z (cid:55)→ exp (cid:0)−(z − z(cid:48))(cid:62)C(z − z(cid:48))(cid:1) is analytic on Rdx+dy for
each z(cid:48).

E Proof of Theorem 5

Recall Theorem 5,

12

Theorem 5 (Independence test using (cid:92)NFSIC2 is consistent). Let ˆΣ be a consistent estimate of Σ based on the
joint sample Zn. The (cid:92)NFSIC2 statistic is deﬁned as ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu where γn ≥ 0 is a regularization
parameter. Assume that

(cid:17)−1

1. Assumption A holds.
2. Σ is invertible almost surely with respect to VJ = {(vi, wi)}J

i=1 drawn from an absolutely continuous distribution.

3. limn→∞ γn = 0.

Then, for any k, l and VJ satisfying the assumptions,
1. Under H0, ˆλn

d→ χ2(J) as n → ∞.

2. Under H1, for any r ∈ R, limn→∞ P

(cid:16)ˆλn ≥ r

(cid:17)

(cid:92)NFSIC2 is consistent.

= 1 almost surely. That is, the independence test based on

(cid:17)−1 p
Proof. Assume that H0 holds. The consistency of ˆΣ and the continuous mapping theorem imply that
→
Σ−1 which is a constant. Let a be a random vector in RJ following N (0, Σ). By Vaart [2000, Theorem 2.7 (v)], it
nˆu d→ N (0, Σ)

d→ (cid:2)a, Σ−1(cid:3) where u = 0 almost surely by Proposition 2, and

(cid:16) ˆΣ + γnI

follows that

(cid:17)−1(cid:21)

(cid:16) ˆΣ + γnI

(cid:20)√

nˆu,

√

by Proposition 4. Since f (x, S) := x(cid:62)Sx is continuous, f
nˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

ˆu d→ a(cid:62)Σ−1a ∼ χ2(J) by Anderson [2003, Theorem 3.3.3]. This proves the ﬁrst claim.

(cid:18)√

(cid:16) ˆΣ + γnI

nˆu,

(cid:17)−1(cid:19)

d→ f (a, Σ−1). Equivalently,

The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al.
[2015]. Assume that H1 holds. Then, u (cid:54)= 0 almost surely by Proposition 2. Since k and l are bounded, it follows that
|ht(z, z(cid:48))| ≤ 2BkBl for any z, z(cid:48) (see (8)), and we have that ˆu a.s.→ u by Serﬂing [2009, Section 5.4, Theorem A]. Thus,
ˆu(cid:62) (cid:16) ˆΣ + γnI
d→ u(cid:62)Σ−1u by the continuous mapping theorem, and the consistency of ˆΣ. Consequently,

(cid:17)−1

ˆu − r
n

P

lim
n→∞

(cid:17)

(cid:16)ˆλn ≥ r
(cid:18)

= 1 − lim
n→∞

P

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

ˆu −

< 0

(cid:19)

r
n

(a)

= 1 − P (cid:0)u(cid:62)Σ−1u < 0(cid:1) (b)

= 1,

where at (a) we use the Portmanteau theorem [Vaart, 2000, Lemma 2.2 (i)] guaranteeing that xn
→ x if and only if
P(xn < t) → P(x < t) for all continuity points of t (cid:55)→ P(x < t). Step (b) is justiﬁed by noting that the covariance
matrix Σ is positive deﬁnite so that u(cid:62)Σ−1u > 0, and t (cid:55)→ P(u(cid:62)Σ−1u < t) (a step function) is continuous at 0.

d

F Proof of Theorem 7

Recall Theorem 7,

Theorem 7 (A lower bound on the test power). Let NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let K be a kernel class
for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that
1. There exist ﬁnite Bk and Bl such that supk∈K supx,x(cid:48)∈X |k(x, x(cid:48))| ≤ Bk and supl∈L supy,y(cid:48)∈Y |l(y, y(cid:48))| ≤ Bl.
2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)Σ−1(cid:107)F < ∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test power satisﬁes P

≥ L(λn) where

(cid:16)ˆλn ≥ r

(cid:17)

L(λn) = 1 − 62e−ξ1γ2

− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

n(λn−r)2/n − 2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

13

(cid:98)·(cid:99) is the ﬂoor function, ξ1 :=
c1 := 4B2J
ﬁxed n, L(λn) is increasing in λn.

J ˜c, c2 := 4B

√

√

1

32c2

1J 2B∗ , ξ2 := 72c2

2JB2, B := BkBl, ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 := 28B4J 2c2
1,
J ˜c, and B∗ is a constant depending on only Bk and Bl. Moreover, for suﬃciently large

Overview of the proof We ﬁrst derive a probabilistic bound for |ˆλn − λn|/n. The bound is in turn upper
bounded by an expression involving (cid:107)ˆu − u(cid:107)2 and (cid:107) ˆΣ − Σ(cid:107)F . The diﬀerence (cid:107)ˆu − u(cid:107)2 can be bounded by applying
the bound for U-statistics given in Serﬂing [2009, Theorem A, p. 201]. For (cid:107) ˆΣ − Σ(cid:107)F , we decompose it into a sum
of smaller components, and bound each term with a product variant of the Hoeﬀding’s inequality (Lemma 9). L(λn)
is obtained by combining all the bounds with the union bound.

F.1 Notations
Let (cid:104)A, B(cid:105)F := tr(A(cid:62)B) denote the Frobenius inner product, and (cid:107)A(cid:107)F := (cid:112)tr(A(cid:62)A) be the Frobenius norm.
Write z := (x, y) to denote a pair of points from X × Y. We write t := (v, w) to denote a pair of test locations
from X × Y. For brevity, an expectation over (x, y) (i.e., E(x,y)∼Pxy ) will be written as Ez or Exy. Deﬁne
˜k(x, v) := k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) := l(y, w) − Ey(cid:48)l(y(cid:48), w). Let B2(r) := {x | (cid:107)x(cid:107)2 ≤ r} be a closed ball
with radius r centered at the origin. Similarly, deﬁne BF (r) := {A | (cid:107)A(cid:107)F ≤ r} to be a closed ball with radius r of
J × J matrices under the Frobenius norm. Denote the max operation by (x1, . . . , xm)+ = max(x1, . . . , xm).
we write (cid:91)µxµy(v, w)

j(cid:54)=i k(xi, v)l(yj, w) to denote the unbiased plug-in estimator, and write ˆµx(v)ˆµy(w)

:=
:=
j=1 l(yj, w) which is a biased estimator. Deﬁne ˆub(v, w) := ˆµxy(v, w) − ˆµx(v)ˆµy(w)
where the superscript b stands for “biased”. To avoid confusing with a positive

i=1
i=1 k(xi, v) 1
n

For
1
n(n−1)
(cid:80)n
1
n

of marginal mean

so that ˆub := (cid:0)ˆub(t1), . . . , ˆub(tJ )(cid:1)(cid:62)
deﬁnite kernel, we will refer to a U-statistic kernel as a core.

product
(cid:80)

µx(v)µy(w),

embeddings

a
(cid:80)n

(cid:80)n

F.2 Proof
We will ﬁrst derive a bound for P(|ˆλn − λn| ≥ t), which will then be reparametrized to get a bound for the target
quantity P(ˆλn ≥ r). We closely follow the proof in Jitkrittum et al. [2016, Section C.1] up to (12), then we diverge.
We start by considering |ˆλn − λn|/n.

|ˆλn − λn|/n =

(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:17)−1
ˆu(cid:62) (cid:16) ˆΣ + γnI
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

=

≤

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u + u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)

+

:= ((cid:70))1 + ((cid:70))2 .

We next bound ((cid:70)1) and ((cid:70)2) separately.

((cid:70))1 =

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

≤

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

(cid:17)−1

ˆu(cid:62) (cid:16) ˆΣ + γnI
(cid:28)

(cid:12)
(cid:12)
ˆu − ˆu(cid:62) (Σ + γnI)−1 ˆu + ˆu(cid:62) (Σ + γnI)−1 ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)ˆu(cid:62) (Σ + γnI)−1 ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:68)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ˆu − ˆu(cid:62) (Σ + γnI)−1 ˆu
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:29)

=

(cid:17)−1

ˆuˆu(cid:62),

(cid:16) ˆΣ + γnI

− (Σ + γnI)−1

(cid:12)
(cid:12)
(cid:12)
F
≤ (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1 − (Σ + γnI)−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1(cid:107)F
= (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1[(Σ + γnI) − ( ˆΣ + γnI)](Σ + γnI)−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − ˆuu(cid:62) + ˆuu(cid:62) − uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1(cid:107)F

ˆuˆu(cid:62) − uu(cid:62), (Σ + γnI)−1(cid:69)

+

F

14

(a)
≤ (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1(cid:107)F (cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − ˆuu(cid:62) + ˆuu(cid:62) − uu(cid:62)(cid:107)F (cid:107)Σ−1(cid:107)F
(b)
≤

2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + (cid:0)(cid:107)ˆu(ˆu − u)(cid:62)(cid:107)F + (cid:107)(ˆu − u)u(cid:62)(cid:107)F

(cid:1) (cid:107)Σ−1(cid:107)F

(cid:107)ˆu(cid:107)2

√

≤

(cid:107)ˆu(cid:107)2

2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + ((cid:107)ˆu(cid:107)2 + (cid:107)u(cid:107)2) (cid:107)ˆu − u(cid:107)2(cid:107)Σ−1(cid:107)F ,

J
γn
√
J
γn

where at (a) we used (cid:107)(Σ + γnI)−1(cid:107)F ≤ (cid:107)Σ−1(cid:107)F , at (b) we used (cid:107)( ˆΣ + γnI)−1(cid:107)F ≤

J(cid:107)( ˆΣ + γnI)−1(cid:107)2 ≤

J/γn.

For ((cid:70))2, we have

(cid:12)
(cid:12)
(cid:12)u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
((cid:70))2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (cid:12)
(cid:10)uu(cid:62), (Σ + γnI)−1 − Σ−1(cid:11)
(cid:12)
(cid:12)
≤ (cid:107)uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1 − Σ−1(cid:107)F
= (cid:107)u(cid:107)2
≤ γn(cid:107)u(cid:107)2
(a)
≤ γn(cid:107)u(cid:107)2

2(cid:107)(Σ + γnI)−1(cid:107)F (cid:107)Σ−1(cid:107)F

2(cid:107)Σ−1(cid:107)2
F ,

F

2(cid:107)(Σ + γnI)−1 [Σ − (Σ + γnI)] Σ−1(cid:107)F

where at (a) we used (cid:107)(Σ + γnI)−1(cid:107)F ≤ (cid:107)Σ−1(cid:107)F .

Combining (5) and (6), we have

(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)

√

J
γn

≤

(cid:107)ˆu(cid:107)2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + ((cid:107)ˆu(cid:107)2 + (cid:107)u(cid:107)2) (cid:107)ˆu − u(cid:107)2(cid:107)Σ−1(cid:107)F + γn(cid:107)u(cid:107)2

2(cid:107)Σ−1(cid:107)2
F .

(7)

2 and (cid:107)u(cid:107)2

Bounding (cid:107)ˆu(cid:107)2
2 is
bounded. Recall that supx,x(cid:48)∈X |k(x, x(cid:48))| ≤ Bk, supy,y(cid:48) |l(y, y(cid:48))| ≤ Bl, our notation t = (v, w) for the test locations,
and zi := (xi, yi). We ﬁrst show that the U-statistic core h is bounded.

2 Here, we show that by the boundedness of the kernels k and l, it follows that (cid:107)ˆu(cid:107)2

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we deﬁne B := BkBl. It follows that

|ht((x, y), (x(cid:48), y(cid:48)))| =

(k(x, v) − k(x(cid:48), v))(l(y, w) − l(y(cid:48), w))

(cid:12)
1
(cid:12)
(cid:12)
2
(cid:12)
1
2

≤

(|k(x, v)| + |k(x(cid:48), v)|) (|l(y, w)| + |l(y(cid:48), w)|)

≤ 2BkBl := 2B,

(cid:107)ˆu(cid:107)2

2 =

htm (zi, zj)



≤

[2BkBl]2 = 4B2J,

2
n(n − 1)

(cid:88)

i<j


2

J
(cid:88)

m=1

(cid:107)u(cid:107)2

2 =

[EzEz(cid:48)htm(z, z(cid:48))]2 ≤ 4B2J.

Using the upper bounds on (cid:107)ˆu(cid:107)2

2, (cid:107)u(cid:107)2

2 ,(7) and the deﬁnition of ˜c, we have
(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
√

√

4B2J ˜c(cid:107)Σ − ˆΣ(cid:107)F + 4B

J ˜c(cid:107)ˆu − u(cid:107)2 + 4B2J ˜c2γn

(cid:107)Σ − ˆΣ(cid:107)F + c2(cid:107)ˆu − u(cid:107)2 + c3γn,

15





J
(cid:88)

m=1

J
(cid:88)

m=1

≤

=:

J
γn
c1
γn

(5)

√

(6)

(8)

(9)

(10)

(11)

where we deﬁne c1 := 4B2J

J ˜c, c2 := 4B

√

√

|ˆλn − λn| ≤

J ˜c, and c3 := 4B2J ˜c2. This upper bound implies that
c1
γn

n(cid:107)Σ − ˆΣ(cid:107)F + c2n(cid:107)ˆu − u(cid:107)2 + c3nγn.

(12)

We will separately upper bound (cid:107)Σ − ˆΣ(cid:107)F and (cid:107)ˆu − u(cid:107)2, and combine them with a union bound.

F.2.1 Bounding (cid:107)ˆu − u(cid:107)2
Let t∗ = arg maxt∈{t1,...,tJ } |ˆu(t) − u(t)|. Recall that u = (u(t1), . . . , u(tJ ))(cid:62) = (u1, . . . , uJ )(cid:62).

(cid:107)ˆu − u(cid:107)2 = sup

(cid:104)b, ˆu − u(cid:105)2 ≤ sup

|bj||ˆu(tj) − u(tj)|

b∈B2(1)

b∈B2(1)

j=1

J
(cid:88)

≤ |ˆu(t∗) − u(t∗)|

sup
b∈B2(1)

J
(cid:88)

j=1

|bj|

√

(a)
≤

√

J|ˆu(t∗) − u(t∗)|

sup
b∈B2(1)

(cid:107)b(cid:107)2

=

J|ˆu(t∗) − u(t∗)|,

J(cid:107)a(cid:107)2 for any a ∈ RJ . From (13), it can be seen that bounding (cid:107)ˆu − u(cid:107)2 amounts to
where at (a) we used (cid:107)a(cid:107)1 ≤
bounding the diﬀerence of a U-statistic ˆu(t∗) (see (4)) to its expectation u(t∗). Combining (13) and (12), we have

√

|ˆλn − λn| ≤

n(cid:107)Σ − ˆΣ(cid:107)F + c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

√

c1
γn

F.2.2 Bounding (cid:107) ˆΣ − Σ(cid:107)F
The plan is to write ˆΣ = ˆS − ˆub ˆub(cid:62), Σ = S − uu(cid:62), so that (cid:107) ˆΣ − Σ(cid:107)F ≤ (cid:107)ˆS − S(cid:107)F + (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F and bound
separately (cid:107)ˆS − S(cid:107)F and (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

Recall that Σij = η(ti, tj), η(t, t(cid:48)) = Exy[(cid:0)˜k(x, v)˜l(y, w) − u(v, w)(cid:1)(cid:0)˜k(x, v(cid:48))˜l(y, w(cid:48)) − u(v(cid:48), w(cid:48))(cid:1)] where ˜k(x, v) =
k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) = l(y, w) − Ey(cid:48)l(y(cid:48), w). Its empirical estimator (see Proposition 6) is ˆΣij = ˆη(ti, tj)
where

ˆη(t, t(cid:48)) =

[(cid:0)k(xi, v)l(yi, w) − ˆub(v, w)(cid:1)(cid:0)k(xi, v(cid:48))l(yi, w(cid:48)) − ˆub(v(cid:48), w(cid:48))(cid:1)]

=

k(xi, v)l(yi, w)k(xi, v(cid:48))l(yi, w(cid:48)) − ˆub(v, w)ˆub(v(cid:48), w(cid:48)),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:80)n

:=

that

k(x, v) − 1
and
n
i=1 k(xi, v)l(yi, w) = ˆub(v, w).

We
:=
m=1 k(xm, vi)l(ym, wi)k(xm, vj)l(yi, wj), and deﬁne similarly its population counterpart S such that

k(x, v)
note
1
n
Sij := Exy[˜k(x, v)˜l(y, w)˜k(x, v(cid:48))˜l(y, w(cid:48))]. We have

l(y, w) − 1
n
ˆS
RJ×J

i=1 l(yi, w).
such that

i=1 k(xi, v),

We deﬁne

l(y, w)

ˆSij

(cid:80)n

(cid:80)n

:=

1
n

∈

(cid:80)n

ˆΣ = ˆS − ˆub ˆub(cid:62),
Σ = S − uu(cid:62),

(cid:107) ˆΣ − Σ(cid:107)F = (cid:107)ˆS − S − (ˆub ˆub(cid:62) − uu(cid:62))(cid:107)F

≤ (cid:107)ˆS − S(cid:107)F + (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

With (16), (14) becomes

|ˆλn − λn| ≤

(cid:107)ˆS − S(cid:107)F +

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F + c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1n
γn

c1n
γn

√

We will further separately bound (cid:107)ˆS − S(cid:107)F and (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

16

(13)

(14)

(15)

(16)

(17)

F.2.3 Bounding (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F = (cid:107)ˆub ˆub(cid:62) − ˆubu(cid:62) + ˆubu(cid:62) − uu(cid:62)(cid:107)F
≤ (cid:107)ˆub(ˆub − u)(cid:62)(cid:107)F + (cid:107)(ˆub − u)u(cid:62)(cid:107)F
= (cid:107)ˆub(cid:107)2(cid:107)ˆub − u(cid:107)2 + (cid:107)ˆub − u(cid:107)2(cid:107)u(cid:107)2

√

≤ 4B

J(cid:107)ˆub − u(cid:107)2,

√

where we used (10) and the fact that (cid:107)ˆub(cid:107)2 ≤ 2B

J which can be shown similarly to (9) as

(cid:107)ˆub(cid:107)2

2 =

[ˆµxy(vm, wm) − ˆµx(vm)ˆµy(wm)]2 =

htm (zi, zj)



≤

[2BkBl]2 = 4B2J.

J
(cid:88)

n
(cid:88)

n
(cid:88)





1
n2

m=1

i=1

j=1


2

J
(cid:88)

m=1

J
(cid:88)

m=1

Let (˜v, ˜w) := ˜t = arg maxt∈{t1,...,tJ } |ˆub(t) − u(t)|. We bound (cid:107)ˆub − u(cid:107)2 by

(cid:107)ˆub − u(cid:107)2

√

(a)
≤

√

√

√

√

J|ˆub(˜t) − u(˜t)|
(cid:12)ˆµxy(˜t) − ˆµx(˜v)ˆµy( ˜w) − u(˜t)(cid:12)
J (cid:12)
(cid:12)
(cid:12)ˆµxy(˜t) − (cid:91)µxµy(˜t) + (cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w) − u(˜t)(cid:12)
J (cid:12)
(cid:12)
(cid:12)ˆµxy(˜t) − (cid:91)µxµy(˜t) − u(˜t)(cid:12)
J (cid:12)
(cid:12) +
(cid:12)ˆu(˜t) − u(˜t)(cid:12)
J (cid:12)
(cid:12) +

J (cid:12)
(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12)
(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12) ,

J (cid:12)

√

√

=

=

≤

=

where at (a) we used the same reasoning as in (13). The bias (cid:12)
bounded as

(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)

(cid:12) in the second term can be

k(xi, ˜v)l(yj, ˜w) −

k(xi, ˜v)l(yi, ˜w) −

k(xi, ˜v)l(yj, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:18)

(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12)
(cid:12)

=

=

1
n(n − 1)

1
n(n − 1)

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

n
(cid:88)

n
(cid:88)

i=1

j=1

n
n − 1

(cid:19) 1
n2

n
n − 1

(cid:19) 1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1

j=1

≤

1 −

≤

B
n − 1

+

B
n − 1

=

2B
n − 1

.

k(xi, ˜v)l(yj, ˜w) −

k(xi, ˜v)l(yj, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

1
n(n − 1)

n
(cid:88)

i=1

1
n2

n
(cid:88)

n
(cid:88)

j=1

1
n(n − 1)

i=1
(cid:12)
(cid:12)
(cid:12)
k(xi, ˜v)l(yi, ˜w)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
k(xi, ˜v)l(yj, ˜w)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(n − 1)

n
(cid:88)

i=1

k(xi, ˜v)l(yi, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

1 −

k(xi, ˜v)l(yj, ˜w) +

(18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Combining this upper bound with (18), we have

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F ≤ 4BJ (cid:12)

(cid:12)ˆu(˜t) − u(˜t)(cid:12)

(cid:12) +

(19)

With (19), (17) becomes

|ˆλn − λn| ≤

(cid:107)ˆS − S(cid:107)F +

c1n
γn

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12) +

c1n
γn

8B2J
n − 1

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

(20)

8B2J
n − 1

.

√

17

F.2.4 Bounding (cid:107)ˆS − S(cid:107)F
Recall that VJ = {t1, . . . , tJ }, ˆSij = ˆS(ti, tj) = 1
n
S(ti, tj) = Exy[˜k(x, vi)˜l(y, wi)˜k(x, vj)˜l(y, wj)]. Let (t(1), t(2)) = arg max(s,t)∈VJ ×VJ | ˆS(s, t) − S(s, t)|.

m=1 k(xm, vi)l(ym, wi)k(xm, vj)l(ym, wj), and Sij =

(cid:80)n

(cid:107)ˆS − S(cid:107)F = sup

B∈BF (1)

(cid:68)

(cid:69)
B, ˆS − S

F

J
(cid:88)

J
(cid:88)

≤ sup

B∈BF (1)

i=1

j=1

|Bij|| ˆSij − Sij|

≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12)

sup
B∈BF (1)

J
(cid:88)

J
(cid:88)

i=1

j=1

|Bij|

(a)
≤ J

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12)

sup
B∈BF (1)

(cid:107)B(cid:107)F

= J

ˆS(t(1), t(2)) − S(t(1), t(2))

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ,

where at (a) we used (cid:80)J

(cid:80)J

i=1

j=1 |Aij| ≤ J(cid:107)A(cid:107)F for any matrix A ∈ RJ×J . We arrive at

|ˆλn − λn| ≤

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12) +

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)
(cid:12)

√

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1Jn
γn
c1n
γn

(cid:12)
(cid:12)
(cid:12)
8B2J
n − 1

+

F.2.5 Bounding

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12)

Having an upper bound for
will deﬁne the following shorthands.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12) will allow us to bound (22). To keep the notations uncluttered, we

(21)

(22)

Expression

Shorthand

Expression

Shorthand

k(x, v)

k(x, v(cid:48))

k(xi, v)
k(xi, v(cid:48))
Ex∼Px k(x, v)
Ex∼Px k(x, v(cid:48))
(cid:80)n
1
i=1 k(xi, v)
n
1
i=1 k(xi, v(cid:48))
n

(cid:80)n

a

a(cid:48)

ai
a(cid:48)
i
˜a

˜a(cid:48)

a
a(cid:48)

l(y, w)

l(y, w(cid:48))

l(yi, w)
l(yi, w(cid:48))
Ey∼Py l(y, w)
Ey∼Py l(y, w(cid:48))
(cid:80)n
1
i=1 l(yi, w)
n
i=1 l(yi, w(cid:48))

(cid:80)n

1
n

b

b(cid:48)

bi
b(cid:48)
i
˜b
˜b(cid:48)

b
(cid:48)

b

We will also use · to denote a empirical expectation over x, or y, or (x, y). The argument under · will
i=1 k(xi, v)k(xi, v(cid:48)) and
i=1 k(xi, v)l(yi, w)k(xi, v(cid:48)), and so on. We deﬁne in the same way for the population expectation using

determine the variable over which we take the expectation. For instance, aa(cid:48) = 1
n
aba(cid:48) = 1
n
(cid:101)· i.e., (cid:102)aa(cid:48) = Ex [k(x, v)k(x, v(cid:48))] and (cid:103)aba(cid:48) = Exy [k(x, v)l(y, w)k(x, v(cid:48))].

(cid:80)n

(cid:80)n

With these shorthands, we can rewrite ˆS(t, t(cid:48)) and S(t, t(cid:48)) as

ˆS(t, t(cid:48)) =

(ai − a)(bi − b)(a(cid:48)

i − a(cid:48))(b(cid:48)

(cid:48)
i − b

),

1
n

n
(cid:88)

i=1

18

By expanding S(t, t(cid:48)), we have

S(t, t(cid:48)) = Exy

(cid:104)

(cid:105)
(a − ˜a)(b − ˜b)(a(cid:48) − ˜a(cid:48))(b(cid:48) − ˜b(cid:48))

.

S(t, t(cid:48)) = Exy

(cid:2) + aba(cid:48)b(cid:48) − aba(cid:48)˜b(cid:48) − ab˜a(cid:48)b(cid:48) + ab˜a(cid:48)˜b(cid:48)
− a˜ba(cid:48)b(cid:48) + a˜ba(cid:48)˜b(cid:48) + a˜b˜a(cid:48)b(cid:48) − a˜b˜a(cid:48)˜b(cid:48)
− ˜aba(cid:48)b(cid:48) + ˜aba(cid:48)˜b(cid:48) + ˜ab˜a(cid:48)b(cid:48) − ˜ab˜a(cid:48)˜b(cid:48)
+ ˜a˜ba(cid:48)b(cid:48) − ˜a˜ba(cid:48)˜b(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48) + ˜a˜b˜a(cid:48)˜b(cid:48)(cid:3)

= +(cid:94)aba(cid:48)b(cid:48) − (cid:103)aba(cid:48)˜b(cid:48) − (cid:103)abb(cid:48)˜a(cid:48) + (cid:101)ab˜a(cid:48)˜b(cid:48)
− (cid:93)aa(cid:48)b(cid:48)˜b + (cid:102)aa(cid:48)˜b˜b(cid:48) + (cid:102)ab(cid:48)˜a(cid:48)˜b − ˜a˜b˜a(cid:48)˜b(cid:48)
− (cid:103)a(cid:48)bb(cid:48)˜a + (cid:102)a(cid:48)b˜a˜b(cid:48) + ˜a˜a(cid:48) (cid:102)bb(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48)
+ (cid:103)a(cid:48)b(cid:48)˜a˜b − ˜a˜b˜a(cid:48)˜b(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48) + ˜a˜b˜a(cid:48)˜b(cid:48)
= +(cid:94)aba(cid:48)b(cid:48) − (cid:103)aba(cid:48)˜b(cid:48) − (cid:103)abb(cid:48)˜a(cid:48) + (cid:101)ab˜a(cid:48)˜b(cid:48)
− (cid:93)aa(cid:48)b(cid:48)˜b + (cid:102)aa(cid:48)˜b˜b(cid:48) + (cid:102)ab(cid:48)˜a(cid:48)˜b + (cid:103)a(cid:48)b(cid:48)˜a˜b
− (cid:103)a(cid:48)bb(cid:48)˜a + (cid:102)a(cid:48)b˜a˜b(cid:48) + ˜a˜a(cid:48) (cid:102)bb(cid:48) − 3˜a˜b˜a(cid:48)˜b(cid:48).

The expansion of ˆS(t, t(cid:48)) can be done in the same way. By the triangle inequality, we have

(cid:12)
(cid:12)
(cid:12)

ˆS(t, t(cid:48)) − S(t, t(cid:48))

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)aa(cid:48)b(cid:48) b − (cid:93)aa(cid:48)b(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bb(cid:48)a − (cid:103)a(cid:48)bb(cid:48)˜a
(cid:12) +

(cid:48)

(cid:12)
(cid:12)
(cid:12)aba(cid:48) b
(cid:12)
(cid:12)
(cid:12)aa(cid:48) b b
(cid:12)
(cid:12)
(cid:12)a(cid:48)bab

(cid:48)

(cid:48)

(cid:12)
(cid:12)

(cid:12)abb(cid:48)a(cid:48) − (cid:103)abb(cid:48)˜a(cid:48)(cid:12)
− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12) +
(cid:12)
(cid:12)
− (cid:102)aa(cid:48)˜b˜b(cid:48)(cid:12)
(cid:12)ab(cid:48)a(cid:48)b − (cid:102)ab(cid:48)˜a(cid:48)˜b
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12) +
(cid:12)
(cid:12)
− (cid:102)a(cid:48)b˜a˜b(cid:48)(cid:12)
(cid:12)a a(cid:48)bb(cid:48) − ˜a˜a(cid:48) (cid:102)bb(cid:48)
(cid:12)
(cid:12)
(cid:12)
(cid:12) + 3
(cid:12) +

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)aba(cid:48)b
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a(cid:48)b(cid:48)ab − (cid:103)a(cid:48)b(cid:48)˜a˜b
(cid:12)
(cid:12)
(cid:12)
− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)aba(cid:48)b
(cid:12)
(cid:12)
(cid:12) .

(cid:48)

(cid:48)

The ﬁrst term
by applying Lemma 9. Recall that we write (x1, . . . , xm)+ for max(x1, . . . , xm).

(cid:12)
(cid:12)
(cid:12) can be bounded by applying the Hoeﬀding’s inequality. Other terms can be bounded

(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

Bounding
have

(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (1st term). Since −B2 ≤ aba(cid:48)b(cid:48) ≤ B2, by the Hoeﬀding’s inequality (Lemma 14), we

P

(cid:16)(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 2 exp

−

(cid:18)

(cid:19)

.

nt2
2B4

Bounding
We note that |f1(x, y)| ≤ (BBk, Bl)+ and |f2(y)| ≤ (BBk, Bl)+. Thus, by Lemma 9 with E = 2, we have

− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (2nd term). Let f1(x, y) = aba(cid:48) = k(x, v)l(y, w)k(x, v(cid:48)) and f2(y) = b(cid:48) = l(y, w(cid:48)).

(cid:12)
(cid:12)
(cid:12)aba(cid:48) b

(cid:48)

P

(cid:16)(cid:12)
(cid:12)
(cid:12)aba(cid:48) b

(cid:48)

− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t

≥ 1 − 4 exp

−

(cid:18)

nt2
8(BBk, Bl)4
+

(cid:19)

.

Bounding
b(cid:48) = l(y, w(cid:48)). We can see that |f1(x, y)|, |f2(x)|, |f3(y)| ≤ (B, Bk, Bl)+. Thus, by Lemma 9 with E = 3, we have

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (4th term). Let f1(x, y) = ab = k(x, v)l(y, w), f2(x) = a(cid:48) = k(x, v(cid:48)) and f3(y) =

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b

(cid:48)

P

(cid:16)(cid:12)
(cid:12)aba(cid:48)b
(cid:12)

(cid:48)

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 6 exp

−

(cid:18)

nt2
18(B, Bk, Bl)6
+

(cid:19)

.

19

(cid:48)

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b

− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (last term). Let f1(x) = a = k(x, v), f2(y) = b = l(y, w), f3(x) = a(cid:48) = k(x, v(cid:48)) and
Bounding
f4(y) = b(cid:48) = l(y, w(cid:48)). It can be seen that |f1(x)|, |f2(y)|, |f3(x)|, |f4(y)| ≤ (Bk, Bl)+. Thus, by Lemma 9 with
E = 4, we have

(cid:16)
3

P

(cid:12)
(cid:12)aba(cid:48)b
(cid:12)

(cid:48)

− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 8 exp

−

(cid:18)

nt2
32 · 32(Bk, Bl)8
+

(cid:19)

.

Bounds for other terms can be derived in a similar way to yield

(3rd term) P

(5th term) P

(6th term) P

(7th term) P

(8th term) P

(9th term) P

(10th term) P

(11th term) P

(cid:48)

(cid:17)

(cid:16)(cid:12)
(cid:12)

(cid:12)abb(cid:48)a(cid:48) − (cid:103)abb(cid:48)˜a(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t
(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)aa(cid:48)b(cid:48) b − (cid:93)aa(cid:48)b(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
− (cid:102)aa(cid:48)˜b˜b(cid:48)(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)aa(cid:48) b b
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:12)ab(cid:48)a(cid:48)b − (cid:102)ab(cid:48)˜a(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:12)a(cid:48)b(cid:48)ab − (cid:103)a(cid:48)b(cid:48)˜a˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bb(cid:48)a − (cid:103)a(cid:48)bb(cid:48)˜a
(cid:12) ≤ t
(cid:16)(cid:12)
− (cid:102)a(cid:48)b˜a˜b(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bab
(cid:12) ≤ t
(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t

(cid:16)(cid:12)
(cid:12)a a(cid:48)bb(cid:48) − ˜a˜a(cid:48) (cid:102)bb(cid:48)
(cid:12)

(cid:17)

(cid:48)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

≥ 1 − 4 exp

−

≥ 1 − 4 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

≥ 1 − 4 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

,

,

,

(cid:19)

(cid:19)

18(B2

nt2
8(BBl, Bk)4
+
nt2
8(BBk, Bl)4
+
(cid:19)
nt2
k, Bl)6
+
nt2
18(B, Bk, Bl)6
+
nt2
18(B, Bk, Bl)6
+
(cid:19)
nt2
8(BBl, Bk)4
+
nt2
18(B, Bk, Bl)6
+
(cid:19)
nt2
18(Bk, B2

l )6
+

.

(cid:19)

(cid:19)

,

(cid:19)

,

,

,

By the union bound, we have

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

ˆS(t, t(cid:48)) − S(t, t(cid:48))

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ 12t
(cid:19)

(cid:18)

≥ 1 −

2 exp

−

+ 4 exp

−

nt2
2B4

(cid:18)

(cid:19)

(cid:19)

(cid:19)

nt2
18(B, Bk, Bl)6
+
(cid:18)

+ 6 exp

−

(cid:18)

+ 8 exp

−

(cid:19)

nt2
18(B, Bk, Bl)6
+
nt2
32 · 32(Bk, Bl)8
+
(cid:19)

(cid:19) (cid:21)

(cid:19)

(cid:18)

+ 24 exp

−

nt2
18(B, Bk, Bl)6
+

(cid:19)

(cid:18)

+ 4 exp

−

(cid:19)

nt2
8(BBl, Bk)4
+

+ 6 exp

−

= 1 −

2 exp

−

+ 8 exp

−

+ 8 exp

−

(cid:20)

(cid:20)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

4 exp

−

4 exp

−

(cid:18)

+ 6 exp

−

(cid:19)

(cid:19)

nt2
8(BBk, Bl)4
+
nt2
8(BBl, Bk)4
+
(cid:19)
nt2
2B4

nt2
18(B2
k, Bl)6
+
(cid:19)

nt2
8(BBk, Bl)4
+
(cid:18)

+ 6 exp

−

(cid:18)

+ 6 exp

−

(cid:19)

18(B2

nt2
k, Bl)6
+
nt2
18(B, Bk, Bl)6
+

(cid:18)

(cid:19)

(cid:19)

nt2
8(BBk, Bl)4
+

(cid:18)

+ 6 exp

−

nt2
18(Bk, B2
l )6
+
(cid:18)
(cid:19)

+ 8 exp

−

+ 8 exp

(cid:18)

+ 8 exp

−

(cid:19)

+ 6 exp

122nt2
B∗
122nt2
B∗

−

(cid:18)

(cid:19)

(cid:20)

(cid:18)

≥ 1 −

2 exp

−

+ 6 exp

(cid:18)

= 1 − 62 exp

−

(cid:18)

122nt2
B∗
122nt2
B∗
122nt2
B∗

−

(cid:19)

,

nt2
18(B, Bk, Bl)6
+
(cid:18)

nt2
18(Bk, B2

l )6
+

(cid:18)

+ 6 exp

−

(cid:19)

(cid:18)

(cid:19)

+ 6 exp

−

nt2
8(BBl, Bk)4
+
(cid:18)

+ 8 exp

−

(cid:19) (cid:21)

nt2
32 · 32(Bk, Bl)8
+
(cid:19)
122nt2
B∗

−

(cid:18)

(cid:19)

122nt2
B∗
122nt2
B∗

−

(cid:18)

+ 24 exp

(cid:19) (cid:21)

where

B∗ :=

1
122 max(2B4, 8(BBk, Bl)4

+, 8(BBl, Bk)4

+, 18(B, Bk, Bl)6

+, 18(B2

k, Bl)6

+, 18(Bk, B2

l )6

+, 32 · 32(Bk, Bl)8

+).

20

By reparameterization, it follows that

P

(cid:18) c1Jn
γn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12) ≤ t

(cid:19)

≥ 1 − 62 exp

−

(cid:18)

nt2
γ2
c2
1J 2nB∗

(cid:19)

.

(23)

F.2.6 Union Bound for

(cid:12)
(cid:12)
(cid:12)

ˆλn − λn

(cid:12)
(cid:12)
(cid:12) and Final Lower Bound

Recall from (22) that

|ˆλn − λn| ≤

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12) +

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)
(cid:12)

√

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1Jn
γn
c1n
γn

(cid:12)
(cid:12)
(cid:12)
8B2J
n − 1

+

We will bound terms in (22) separately and combine all the bounds with the union bound. As shown in (8), the
U-statistic core h is bounded between −2B and 2B. Thus, by Lemma 13 (with m = 2), we have

√

(cid:16)

P

c2n

J|ˆu(t∗) − u(t∗)| ≤ t

≥ 1 − 2 exp

−

(cid:17)

(cid:18)

(cid:98)0.5n(cid:99)t2
8c2
2n2JB2

(cid:19)

.

Bounding c1n
γn

8B2J
n−1 + c3nγn + 4BJc1n

γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12). By Lemma 13 (with m = 2), it follows that

P

(cid:18) c1n
γn

8B2J
n − 1

≥ 1 − 2 exp

+ c3nγn +

(cid:104)

(cid:98)0.5n(cid:99)γ2
n

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12) ≤ t

(cid:19)

t − c1n
γn
27B4J 2c2

8B2J
n−1 − c3nγn
1n2

(cid:105)2









−

(cid:32)

= 1 − 2 exp

−

(a)
≥ 1 − 2 exp

(cid:32)

−

(cid:98)0.5n(cid:99) (cid:2)tγn(n − 1) − 8c1B2nJ − c3n(n − 1)γ2

n

(cid:3)2

(cid:33)

27B4J 2c2
(cid:2)tγn(n − 1) − 8c1B2nJ − c3n(n − 1)γ2

1n2(n − 1)2

n

(cid:3)2

(cid:33)

,

28B4J 2c2

1n2(n − 1)

(24)

(25)

where at (a) we used (cid:98)0.5n(cid:99) ≥ (n − 1)/2. Combining (23), (24), and (25) with the union bound (set T = 3t), we can
bound (22) with

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

ˆλn − λn

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ T

≥ 1 − 62 exp

(cid:32)

− 2 exp

−

(cid:18)

(cid:19)

(cid:18)

−

− 2 exp

nT 2
γ2
32c2
1J 2nB∗

(cid:98)0.5n(cid:99)T 2
72c2
2n2JB2
nn(n − 1)(cid:3)2
(cid:2)T γn(n − 1)/3 − 8c1B2nJ − c3γ2
28B4J 2c2

1n2(n − 1)

−

(cid:19)

(cid:33)

.

Since

ˆλn − λn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ T implies ˆλn ≥ λn − T , a reparametrization with r = λn − T gives
(cid:12)

P

(cid:16)ˆλn ≥ r

(cid:17)

≥ 1 − 62 exp

−

(cid:18)

(cid:19)

n(λn − r)2
γ2
32c2
1J 2nB∗

(cid:18)

(cid:98)0.5n(cid:99)(λn − r)2

(cid:19)

− 2 exp

−

72c2

2n2JB2

(cid:32)

− 2 exp

−

(cid:2)(λn − r)γn(n − 1)/3 − 8c1B2nJ − c3γ2
28B4J 2c2

1n2(n − 1)

nn(n − 1)(cid:3)2

(cid:33)

Grouping constants into ξ1, . . . ξ5 gives the result.

:= L(λn).

21

The lower bound L(λn) takes the form

1 − 62 exp (cid:0)−C1(λn − Tα)2(cid:1) − 2 exp (cid:0)−C2(λn − Tα)2(cid:1) − 2 exp

(cid:18)

−

[(λn − Tα)C3 − C4]2
C5

(cid:19)

,

where C1, . . . , C5 are positive constants. For ﬁxed large enough n such that λn > Tα, and ﬁxed signiﬁcance level α,
increasing λn will increase L(λn). Speciﬁcally, since n is ﬁxed, increasing u(cid:62)Σ−1u in λn = nu(cid:62)Σ−1u will increase
L(λn).

G Helper Lemmas

This section contains lemmas used to prove the main results in this work.

Lemma 8 (Product to sum). Assume that |ai| ≤ B, |bi| ≤ B for i = 1, . . . , E. Then
BE−1 (cid:80)E

j=1 |aj − bj|.

(cid:12)
(cid:81)E
(cid:12)
(cid:12)

i=1 ai − (cid:81)E

i=1 bi

(cid:12)
(cid:12)
(cid:12) ≤

Proof.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

i=1

E
(cid:89)

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

i=1

ai −

bj

≤

ai −

aibE

+

aibE −

aibE−1bE

+ . . . +

a1

bj −

bj

E−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E−1
(cid:89)

i=1

E−2
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:12)
(cid:32)E−2
(cid:12)
(cid:89)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

j=2

E
(cid:89)

j=1

E
(cid:89)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j=2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ |aE − bE|

ai

+ |aE−1 − bE−1|

ai

bE

+ . . . + |a1 − b1|

bj

≤ |aE − bE|BE−1 + |aE−1 − bE−1| BE−1 + . . . + |a1 − b1| BE−1

= BE−1

|aj − bj|

E
(cid:88)

j=1

applying triangle inequality, and the boundedness of ai and bi-s.

Lemma 9 (Product variant of the Hoeﬀding’s inequality). For i = 1, . . . , E, let {x(i)
j=1 ⊂ Xi be an i.i.d. sample
from a distribution Pi, and fi : Xi (cid:55)→ R be a measurable function. Note that it is possible that P1 = P2 = · · · = PE
and {x(1)
j=1. Assume that |fi(x)| ≤ B < ∞ for all x ∈ Xi and i = 1, . . . , E. Write ˆPi to denote
an empirical distribution based on the sample {x(i)

j=1 = · · · = {x(E)

j }nE

j }n1

j }ni

j }ni

j=1. Then,

P

(cid:32)(cid:12)
(cid:34) E
(cid:12)
(cid:89)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:35)
fi(x(i))

−

(cid:34) E
(cid:89)

E

x(i)∼ ˆPi

i=1

E
x(i)∼Pifi(x(i))

≤ T

≥ 1 − 2

exp

−

(cid:33)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:88)

i=1

(cid:18)

niT 2
2E2B2E

(cid:19)

.

Proof. By Lemma 8, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34) E
(cid:89)

E

i=1

x(i)∼ ˆPi

(cid:35)
fi(x(i))

−

(cid:34) E
(cid:89)

i=1

E
x(i)∼Pifi(x(i))

≤ BE−1

x(i)∼ ˆPi

fi(x(i)) − E

(cid:12)
x(i)∼Pi fi(x(i))
(cid:12)
(cid:12) .

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)

E

By
(cid:16)(cid:12)
P
E
(cid:12)
(cid:12)

applying

the
fi(x(i)) − E

x(i)∼ ˆPi

Hoeﬀding’s
(cid:12)
(cid:12)
x(i)∼Pifi(x(i))
(cid:12) ≤ t

inequality
(cid:17)

≥ 1 − 2 exp

to
(cid:16)
− 2nit2
4B2

each
(cid:17)

term

the
. The result is obtained with a union bound.

sum,

we

in

have

H External Lemmas

In this section, we provide known results referred to in this work.

Lemma 10 (Chwialkowski et al. [2015, Lemma 1]). If k is a bounded, analytic kernel (in the sense given in
Deﬁnition 1) on Rd × Rd, then all functions in the RKHS deﬁned by k are analytic.

22

Lemma 11 (Chwialkowski et al. [2015, Lemma 3]). Let Λ be an injective mapping from the space of probability
measures into a space of analytic functions on Rd. Deﬁne

d2
VJ

(P, Q) =

|[ΛP ](vj) − [ΛQ](vj)|2 ,

J
(cid:88)

j=1

where VJ = {vi}J
respect to the Lebesgue measure. Then, dVJ (P, Q) is almost surely (w.r.t. VJ ) a metric.

i=1 are vector-valued i.i.d. random variables from a distribution which is absolutely continuous with

Lemma 12 (Bochner’s theorem [Rudin, 2011]). A continuous function Ψ : Rd → R is positive deﬁnite if and only if
it is the Fourier transform of a ﬁnite nonnegative Borel measure ζ on Rd, that is, Ψ(x) = (cid:82)
Rd e−ix(cid:62)ω dζ(ω), x ∈ Rd.

Lemma 13 (A bound for U-statistics [Serﬂing, 2009, Theorem A, p. 201]). Let h(x1, . . . , xm) be a U-
statistic kernel for an m-order U-statistic such that h(x1, . . . , xm) ∈ [a, b] where a ≤ b < ∞. Let Un =
(cid:0) n
h(xi1 , . . . , xim ) be a U-statistic computed with a sample of size n, where the summation is over the
m
(cid:0) n
m

(cid:1)−1 (cid:80)
(cid:1) combinations of m distinct elements {i1, . . . , im} from {1, . . . , n}. Then, for t > 0 and n ≥ m,

i1<···<im

P(Un − Eh(x1, . . . , xm) ≥ t) ≤ exp (cid:0)−2(cid:98)n/m(cid:99)t2/(b − a)2(cid:1) ,
P(|Un − Eh(x1, . . . , xm)| ≥ t) ≤ 2 exp (cid:0)−2(cid:98)n/m(cid:99)t2/(b − a)2(cid:1) ,

where (cid:98)x(cid:99) denotes the greatest integer which is smaller than or equal to x. Hoeﬃnd’s inequality is a special case
when m = 1.

Lemma 14 (Hoeﬀding’s inequality). Let X1, . . . , Xn be i.i.d. random variables such that a ≤ Xi ≤ b almost surely.
Deﬁne X := 1
n

i=1 Xi. Then,

(cid:80)n

P (cid:0)(cid:12)

(cid:12)X − E[X](cid:12)

(cid:12) ≤ α(cid:1) ≥ 1 − 2 exp

(cid:18)

−

2nα2
(b − a)2

(cid:19)

.

References

[sup4] K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton. Fast Two-Sample Testing with Analytic
Representations of Probability Measures. In Advances in Neural Information Processing Systems (NIPS), pages
1981–1989. 2015.

[sup15] W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gretton. Interpretable Distribution Features with Maximum

Testing Power. 2016. URL http://arxiv.org/abs/1605.06796.

[sup3] W. Rudin. Fourier analysis on groups. John Wiley & Sons, 2011.

[sup21] R. J. Serﬂing. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 2009.

23

6
1
0
2
 
t
c
O
 
5
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
8
7
4
0
.
0
1
6
1
:
v
i
X
r
a

An Adaptive Test of Independence with Analytic Kernel Embeddings

Wittawat Jitkrittum,1

Zoltán Szabó,2∗ Arthur Gretton1

1Gatsby Computational Neuroscience Unit, University College London
2Department of Applied Mathematics, CMAP, École Polytechnique

wittawat@gatsby.ucl.ac.uk
zoltan.szabo@polytechnique.edu
arthur.gretton@gmail.com

October 18, 2016

Abstract

A new computationally eﬃcient dependence measure, and
an adaptive statistical test of independence, are proposed.
The dependence measure is the diﬀerence between ana-
lytic embeddings of the joint distribution and the product
of the marginals, evaluated at a ﬁnite set of locations
(features). These features are chosen so as to maximize a
lower bound on the test power, resulting in a test that
is data-eﬃcient, and that runs in linear time (with re-
spect to the sample size n). The optimized features can
be interpreted as evidence to reject the null hypothesis,
indicating regions in the joint domain where the joint
distribution and the product of the marginals diﬀer most.
Consistency of the independence test is established, for
an appropriate choice of features. In real-world bench-
marks, independence tests using the optimized features
perform comparably to the state-of-the-art quadratic-
time HSIC test, and outperform competing O(n) and
O(n log n) tests.

1

Introduction

We consider the design of adaptive, nonparametric sta-
tistical tests of dependence: that is, tests of whether
a joint distribution Pxy factorizes into the product of
marginals PxPy. While classical tests of dependence,
such as Pearson’s correlation and Kendall’s τ , are able to
detect monotonic relations between univariate variables,
more modern tests can address complex interactions, for
instance changes in variance of X with the value of Y .
Key to many recent tests is to examine covariance or
correlation between data features. These interactions
become signiﬁcantly harder to detect, and the features

∗Zoltán

Szabó’s
0000-0001-6183-7603.

ORCID

ID:

http://orcid.org/

are more diﬃcult to design when the data reside in high
dimensions.

A basic nonlinear dependence measure is the Hilbert-
Schmidt Independence Criterion (HSIC), which is the
Hilbert-Schmidt norm of the covariance operator between
feature mappings of the random variables [Gretton et al.,
2005, 2008]. Each random variable X and Y is mapped to
a respective reproducing kernel Hilbert space Hk and Hl.
For suﬃciently rich mappings, the covariance operator
norm is zero if and only if the variables are indepen-
dent. A second basic nonlinear dependence measure is
the smoothed diﬀerence between the characteristic func-
tion of the joint distribution, and that of the product
of marginals. When a particular smoothing function is
used, the statistic corresponds to the covariance between
distances of X and Y variable pairs [Feuerverger, 1993,
Székely et al., 2007, Székely and Rizzo, 2009], yielding
a simple test statistic. It has been shown by Sejdinovic
et al. [2013] that the distance covariance (and its gener-
alization to semi-metrics) is an instance of HSIC for an
appropriate choice of kernels. A disadvantage of these
feature covariance statistics, however, is that they require
quadratic time to compute (besides in the special case of
the distance covariance with univariate real-valued vari-
ables, where Huo and Székely [2014] achieve an O(n log n)
cost). Moreover, the feature covariance statistics have
intractable null distributions, and either a permutation
approach or the solution of an expensive eigenvalue prob-
lem [e.g. Zhang et al., 2011] is required for consistent
estimation of the quantiles. Several approaches were
proposed by Zhang et al. [2016] to obtain faster tests
along the lines of HSIC. These include computing HSIC
on ﬁnite-dimensional feature mappings chosen as ran-
dom Fourier features (RFFs) [Rahimi and Recht, 2008], a
block-averaged statistic, and a Nyström approximation to
the statistic. Key to each of these approaches is a more
eﬃcient computation of the statistic and its threshold un-

1

der the null distribution: for RFFs, the null distribution
is a ﬁnite weighted sum of χ2 variables; for the block-
averaged statistic, the null distribution is asymptotically
normal; for Nyström, either a permutation approach is
employed, or the spectrum of the Nyström approximation
to the kernel matrix is used in approximating the null
distribution. Each of these methods costs signiﬁcantly
less than the O(n2) cost of the full HSIC (the cost is
linear in n, but also depends quadratically on the number
of features retained). A potential disadvantage of the
Nyström and Fourier approaches is that the features are
not optimized to maximize test power, but are chosen
randomly. The block statistic performs worse than both,
due to the large variance of the statistic under the null
(which can be mitigated by observing more data).

In addition to feature covariances, correlation measures
have also been developed in inﬁnite dimensional feature
spaces: in particular, Bach and Jordan [2002], Fukumizu
et al. [2008] proposed statistics on the correlation operator
in a reproducing kernel Hilbert space. While convergence
has been established for certain of these statistics, their
computational cost is high at O(n3), and test thresholds
have relied on permutation. A number of much faster
approaches to testing based on feature correlations have
been proposed, however. For instance, Dauxois and Nkiet
[1998] compute statistics of the correlation between ﬁnite
sets of basis functions, chosen for instance to be step func-
tions or low order B-splines. The cost of this approach is
O(n). This idea was extended by Lopez-Paz et al. [2013],
who computed the canonical correlation between ﬁnite
sets of basis functions chosen as random Fourier features;
in addition, they performed a copula transform on the
inputs, with a total cost of O(n log n). Finally, space
partitioning approaches have also been proposed, based
on statistics such as the KL divergence, however these
apply only to univariate variables [Heller et al., 2016], or
to multivariate variables of low dimension [Gretton and
Györﬁ, 2010] (that said, these tests have other advantages
of theoretical interest, notably distribution-independent
test thresholds).

The approach we take is most closely related to HSIC
on a ﬁnite set of features. Our simplest test statistic,
the Finite Set Independence Criterion (FSIC), is an av-
erage of covariances of analytic functions (i.e., features)
deﬁned on each of X and Y . A normalized version of
the statistic (NFSIC) yields a distribution-independent
asymptotic test threshold. We show that our test is con-
sistent, despite a ﬁnite number of analytic features being
used, via a generalization of arguments in Chwialkowski
et al. [2015]. As in recent work on two-sample testing
by Jitkrittum et al. [2016], our test is adaptive in the
sense that we choose our features on a held-out valida-
tion set to optimize a lower bound on the test power.
The design of features for independence testing turns out

to be quite diﬀerent to the case of two-sample testing,
however: the task is to ﬁnd correlated feature pairs on
the respective marginal domains, rather than attempting
to ﬁnd a single, high-dimensional feature representation
for the entire (x, y) (as we would need to do if we were
comparing distributions Pxy and Qxy, rather than testing
a speciﬁc property of Pxy). We demonstrate the perfor-
mance of our tests on several challenging artiﬁcial and
real-world datasets, including detection of dependence
between music and its year of appearance, and between
videos and captions. In these experiments, we outperform
competing linear and O(n log n) time tests.

2

Independence Criteria and Sta-
tistical Tests

We introduce two test statistics: ﬁrst, the Finite Set
Independence Criterion (FSIC), which builds on the prin-
ciple that dependence can be measured in terms of the
covariance between data features. Next, we propose a nor-
malized version of this statistic (NFSIC), with a simpler
asymptotic distribution when Pxy = PxPy. We show how
to select features for the latter statistic to maximize a
lower bound on the power of its corresponding statistical
test.

2.1 The Finite Set Independence Crite-

rion

We begin by introducing the Hilbert-Schmidt Indepen-
dence Criterion (HSIC) as proposed in Gretton et al.
[2005], since our unnormalized statistic is built along sim-
ilar lines. Consider two random variables X ∈ X ⊂ Rdx
and Y ∈ Y ⊂ Rdy . Denote by Pxy the joint distribution
between X and Y ; Px and Py are the marginal distribu-
tions of X and Y . Let ⊗ denote the tensor product, such
that (a ⊗ b) c = a (cid:104)b, c(cid:105). Assume that k : X × X → R
and l : Y × Y → R are positive deﬁnite kernels associated
with reproducing kernel Hilbert spaces (RKHS) Hk and
Hl, respectively. Let (cid:107) · (cid:107)HS be the norm on the space
of Hl → Hk Hilbert-Schmidt operators. Then, HSIC
between X and Y is deﬁned as
HSIC(X, Y ) = (cid:13)
(cid:13)
2
(cid:13)µxy − µx ⊗ µy
(cid:13)
HS
= E(x,y),(x(cid:48),y(cid:48)) [k(x, x(cid:48))l(y, y(cid:48))]

+ ExEx(cid:48)[k(x, x(cid:48))]EyEy(cid:48)[l(y, y(cid:48))]
− 2E(x,y) [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] ,

(1)

is an independent copy of x.

where Ex := Ex∼Px , Ey := Ey∼Py , E(x,y) := E(x,y)∼Pxy ,
and x(cid:48)
The mean
embedding of Pxy belongs to the space of Hilbert-
Schmidt operators from Hl to Hk, µxy := (cid:82)
X ×Y k(x, ·) ⊗
l(y, ·) dPxy(x, y) ∈ HS(Hl, Hk), and the marginal mean

2

embeddings are µx := (cid:82)
X k(x, ·) dPx(x) ∈ Hk and
µy := (cid:82)
Y l(y, ·) dPy(y) ∈ Hl [Smola et al., 2007]. Gret-
ton et al. [2005, Theorem 4] show that if the kernels k
and l are universal [Steinwart and Christmann, 2008] on
compact domains X and Y, then HSIC(X, Y ) = 0 if and
only if X and Y are independent. Alternatively, Gretton
[2015] shows that it is suﬃcient for each of k and l to be
characteristic to their respective domains (meaning that
distribution embeddings are injective in each marginal
domain: see Sriperumbudur et al. [2010]). Given a joint
sample Zn = {(xi, yi)}n
i=1 ∼ Pxy, an empirical estimator
of HSIC can be computed in O(n2) time by replacing the
population expectations in (1) with their corresponding
empirical expectations based on Zn.

(cid:80)J

We now propose our new linear-time dependence mea-
sure, the Finite Set Independence Criterion (FSIC). Let
X ⊂ Rdx and Y ⊂ Rdy be open sets. Deﬁne the empir-
ical measure ν := 1
i=1 δ(vi,wi) over J test locations
J
VJ := {(vi, wi)}J
i=1 ⊂ X × Y where δt denotes the Dirac
measure centered on t, and (vi, wi) are realizations from
an absolutely continuous distribution (wrt the Lebesgue
measure). Write Exy for E(x,y)∼Pxy . The idea is to
see µxy(v, w) = Exy[k(x, v)l(y, w)], µx(v) = Ex[k(x, v)]
and µy(w) = Ey[l(y, w)] as smooth functions, and con-
sider an L2(X × Y, ν) distance between µxy and µxµy
instead of a Hilbert-Schmidt distance as in HSIC [Gret-
ton et al., 2005]. Let µxµy(x, y) := µx(x)µy(y). FSIC is
deﬁned as

FSIC2(X, Y ) := (cid:107)µxy − µxµy(cid:107)2

L2(X ×Y,ν)

(µxy(x, y) − µx(x)µy(y))2 dν(x, y)

=

:=

(cid:90)

(cid:90)

X

1
J

Y
J
(cid:88)

i=1

u(vi, wi)2 =

(cid:107)u(cid:107)2

2, where

1
J

u(v, w) := µxy(v, w) − µx(v)µy(w)
= Exy[k(x, v)l(y, w)] − Ex[k(x, v)]Ey[l(y, w)],
= covxy[k(x, v), l(y, w)],

(2)

and u := (u(v1, w1), . . . , u(vJ , wJ ))(cid:62).

Our ﬁrst result in Proposition 2 states that FSIC(X, Y )
almost surely deﬁnes a dependence measure for the ran-
dom variables X and Y , provided that the product kernel
on the joint space X × Y is characteristic and analytic
(see Deﬁnition 1).

Deﬁnition 1 (Analytic kernels [Chwialkowski et al.,
2015]). Let X be an open set in Rd. A positive deﬁ-
nite kernel k : X × X → R is said to be analytic on its
domain X × X if for all v ∈ X , f (x) := k(x, v) is an
analytic function on X .

Assumption A. The kernels k : X × X → R and l :
Y × Y → R are bounded by Bk and Bl respectively, and

the product kernel g((x, y), (x(cid:48), y(cid:48))) := k(x, x(cid:48))l(y, y(cid:48)) is
characteristic [Sriperumbudur et al., 2010, Deﬁnition 6],
and analytic (Deﬁnition 1) on (X × Y) × (X × Y).

Proposition 2 (FSIC is a dependence measure). Assume
that
1. Assumption A holds.
2. The test locations VJ = {(vi, wi)}J

i=1 are drawn from

an absolutely continuous distribution.

Then, almost surely, FSIC(X, Y ) = 1√
J
only if X and Y are independent.

(cid:107)u(cid:107)2 = 0 if and

Proof. Since g is characteristic, the mean embedding map
Πg : P (cid:55)→ E(x,y)∼P [g((x, y), ·)] is injective [Sriperum-
budur et al., 2010, Section 3], where P is a probability
distribution on X × Y. Since g is analytic, by Lemma 10
(Appendix), µxy and µxµy are analytic functions. Thus,
Lemma 11 (Appendix, setting Λ = Πg) guarantees that
FSIC(X, Y ) = 0 ⇐⇒ Pxy = PxPy ⇐⇒ X and Y are
independent almost surely.

FSIC uses µxy as a proxy for Pxy, and µxµy as a proxy
for PxPy. Proposition 2 suggests that, to detect the
dependence between X and Y , it is suﬃcient to evaluate at
a ﬁnite number of locations (deﬁned by VJ ) the diﬀerence
of the population joint embedding µxy and the embedding
of the product of the marginal distributions µxµy. A brief
explanation to justify this property is as follows. If Pxy =
PxPy, then ρ(v, w) := µxy(v, w)−µxµy(v, w) is zero, and
FSIC(X, Y ) = 0 for any VJ . If Pxy (cid:54)= PxPy, then ρ will
not be a zero function, since the mean embedding map is
injective (require the product kernel to be characteristic).
Using the same argument as in Chwialkowski et al. [2015],
since k and l are analytic, ρ is also analytic, and the
set of roots R := {(v, w) | ρ(v, w) = 0} has Lebesgue
measure zero. Thus, it is suﬃcient to draw (v, w) from an
absolutely continuous distribution, as we are guaranteed
that (v, w) /∈ R giving FSIC(X, Y ) > 0.

For FSIC to be a dependence measure, the product
kernel is required to be characteristic and analytic. We
next show in Proposition 3 that Gaussian kernels k and l
yield such a product kernel.

3

is

=

(A

characteristic

product
and
exp (cid:0)−(x − x(cid:48))(cid:62)A(x − x(cid:48))(cid:1)

Gaussian
of
Proposition
analytic). Let
kernels
k(x, x(cid:48))
and
l(y, y(cid:48)) = exp (cid:0)−(y − y(cid:48))(cid:62)B(y − y(cid:48))(cid:1) be Gaussian
kernels on Rdx × Rdx and Rdy × Rdy respectively,
for positive deﬁnite matrices A and B.
Then,
g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteristic and
analytic on (Rdx × Rdy ) × (Rdx × Rdy ).
Proof (sketch). The main idea is to use the fact a Gaus-
sian kernel is analytic, and a product of Gaussian kernels
is a Gaussian kernel on the pair of variables. See the full
proof in Appendix D.

3

i=1

Plug-in Estimator We now give an empirical estima-
tor of FSIC. Assume that we observe a joint sample Zn :=
i.i.d.∼ Pxy. Unbiased estimators of µxy(v, w)
{(xi, yi)}n
(cid:80)n
and µxµy(v, w) are ˆµxy(v, w) := 1
i=1 k(xi, v)l(yi, w)
n
(cid:80)
and (cid:91)µxµy(v, w) :=
j(cid:54)=i k(xi, v)l(yj, w),
respectively. A straightforward empirical estimator of
FSIC2 is then given by

1
n(n−1)

(cid:80)n

i=1

(cid:92)FSIC2(Zn) =

ˆu(vi, wi)2,

1
J

J
(cid:88)

i=1

ˆu(v, w) := ˆµxy(v, w) − (cid:91)µxµy(v, w)

=

2
n(n − 1)

(cid:88)

i<j

h(v,w)((xi, yi), (xj, yj)),

(3)

(4)

J ˆu(cid:62) ˆu.

1
h(v,w)((x, y), (x(cid:48), y(cid:48)))
2 (k(x, v) −
where
k(x(cid:48), v))(l(y, w) − l(y(cid:48), w)).
For conciseness, we
deﬁne ˆu := (ˆu1, . . . , ˆuJ )(cid:62) ∈ RJ where ˆui := ˆu(vi, wi) so
that (cid:92)FSIC2(Zn) = 1

:=

(cid:92)FSIC2 can be eﬃciently computed in O((dx + dy)Jn)
time [see (3)], assuming that the runtime complexity of
evaluating k(x, v) is O(dx) and that of l(y, w) is O(dy).
The unbiasedness of (cid:91)µxµy is necessary for (4) to be a U-
statistic. This fact and the rewriting of (cid:92)FSIC2 in terms of
h(v,w)((x, y), (x(cid:48), y(cid:48))) will be exploited when the asymp-
totic distribution of ˆu is derived (Proposition 4).

Since FSIC satisﬁes FSIC(X, Y ) = 0 ⇐⇒ X ⊥ Y , in
principle its empirical estimator can be used as a test
statistic for an independence test proposing a null hy-
pothesis H0 : “X and Y are independent” against an
alternative H1 : “X and Y are dependent”. The null
distribution (i.e., distribution of the test statistic assum-
ing that H0 is true) is challenging to obtain, however
and depends on the unknown Pxy. This prompts us to
consider a normalized version of FSIC whose asymptotic
null distribution of a convenient form. We ﬁrst derive the
asymptotic distribution of ˆu in Proposition 4, which we
use to derive the normalized test statistic in Theorem 5.
As a shorthand, we write z := (x, y), and t := (v, w).

Proposition 4 (Asymptotic distribution of ˆu). Deﬁne
˜k(x, v) := k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) := l(y, w) −
Ey(cid:48)l(y(cid:48), w). Then, under both H0 and H1, for any ﬁxed
locations t and t(cid:48),
covz[ˆu(t), ˆu(t(cid:48))] n→∞−−−−→ covz[˜k(x, v)˜l(y, w), ˜k(x, v(cid:48))˜l(y, w(cid:48))]
= Exy[(cid:0)˜k(x, v)˜l(y, w) − u(t)(cid:1)(cid:0)˜k(x, v(cid:48))˜l(y, w(cid:48)) − u(t(cid:48))(cid:1)],

where u(v, w) is given in (2), and ˆu(v, w) is deﬁned
in (4). Second, if 0 < covz[ˆu(ti), ˆu(ti)] < ∞ for i =
n(ˆu − u) d→ N (0, Σ) as n → ∞, where
1, . . . , J, then
Σij = cov[ˆu(ti), ˆu(tj)] and u := (u(t1), . . . , u(tJ ))(cid:62).

√

Proof. We ﬁrst note that for a ﬁxed t = (v, w), ˆu(v, w) is
a one-sample second-order U-statistic [Serﬂing, 2009, Sec-
tion 5.1.3] with a U-statistic kernel ht where ht(a, b) =

ht(b, a). Thus, by Kowalski and Tu [2008, Section 5.1,
Theorem 1], it follows directly that cov[ˆu(t), ˆu(t(cid:48))] =
4covz[Eaht(z, a), Ebht(cid:48)(z, b)]. Substituting ht with its
deﬁnition yields the ﬁrst claim, where we note that
Exy[˜k(x, v)˜l(y, w)] = u(v, w).

For the second claim, since ˆu is a multivariate one-
sample U-statistic, by Lehmann [1999, Theorem 6.1.6]
and Kowalski and Tu [2008, Section 5.1, Theorem 1], it
n(ˆu − u) d→ N (0, Σ) as n → ∞, where

√

follows that
Σij = cov[ˆu(ti), ˆu(tj)].

Recall from Proposition 2 that u = 0 holds almost
surely under H0. The asymptotic normality in the second
claim of Proposition 4 implies that n(cid:92)FSIC2 = n
J ˆu(cid:62) ˆu con-
verges in distribution to a sum of J dependent weighted
χ2 random variables. The dependence comes from the
fact that the coordinates ˆu1 . . . , ˆuJ of ˆu all depend on
the sample Zn. This null distribution is not analytically
tractable, and requires a large number of simulations to
compute the rejection threshold Tα for a given signiﬁcance
value α.

2.2 Normalized FSIC and Adaptive Test

For the purpose of an independence test, we will consider
a normalized variant of (cid:92)FSIC2, which we call (cid:92)NFSIC2,
whose tractable asymptotic null distribution is χ2(J), the
chi-squared distribution with J degrees of freedom. We
then show that the independence test deﬁned by (cid:92)NFSIC2
is consistent. These results are given in Theorem 5.
Theorem 5 (Independence test using (cid:92)NFSIC2 is con-
sistent). Let ˆΣ be a consistent estimate of Σ based on
the joint sample Zn. The (cid:92)NFSIC2 statistic is deﬁned as
ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu where γn ≥ 0 is a regulariza-
tion parameter. Assume that

(cid:17)−1

1. Assumption A holds.

2. Σ is invertible almost surely with respect to VJ =
i=1 drawn from an absolutely continuous

{(vi, wi)}J
distribution.

3. limn→∞ γn = 0.
Then, for any k, l and VJ satisfying the assumptions,
1. Under H0, ˆλn
2. Under H1, for any r ∈ R, limn→∞ P

= 1
almost surely. That is, the independence test based on
(cid:92)NFSIC2 is consistent.

d→ χ2(J) as n → ∞.

(cid:16)ˆλn ≥ r

(cid:17)

Proof (sketch) . Under H0, nˆu(cid:62)( ˆΣ + γnI)−1 ˆu asymptot-
ically follows χ2(J) because
nˆu is asymptotically nor-
mally distributed (see Proposition 4). Claim 2 builds on
the result in Proposition 2 stating that u (cid:54)= 0 under H1;

√

4

it follows using the convergence of ˆu to u. The full proof
can be found in Appendix E.

Theorem 5 states that if H1 holds, the statistic can
be arbitrarily large as n increases, allowing H0 to be
rejected for any ﬁxed threshold. Asymptotically the test
threshold Tα is given by the (1 − α)-quantile of χ2(J) and
is independent of n. The assumption on the consistency
of ˆΣ is required to obtain the asymptotic chi-squared
distribution. The regularization parameter γn is to ensure
that ( ˆΣ + γnI)−1 can be stably computed. In practice,
γn requires no tuning, and can be set to be a very small
constant.

The next proposition states that the computational
complexity of the (cid:92)NFSIC2 estimator is linear in both
the input dimension and sample size, and that it can
be expressed in terms of the K =[K ij] = [k(vi, xj)] ∈
RJ×n, L = [Lij] = [l(wi, yj)] ∈ RJ×n matrices.

Proposition 6 (An empirical estimator of (cid:92)NFSIC2). Let
1n := (1, . . . , 1)(cid:62) ∈ Rn. Denote by ◦ the element-wise
matrix product. Then,
1. ˆu = (K◦L)1n

n−1 − (K1n)◦(L1n)

n(n−1)

.

2. A consistent estimator for Σ is ˆΣ = ΓΓ(cid:62)

n where

Γ := (K − n−1K1n1(cid:62)
n ) ◦ (L − n−1L1n1(cid:62)
ˆub = n−1 (K ◦ L) 1n − n−2 (K1n) ◦ (L1n) .

n ) − ˆub1(cid:62)
n ,

Assume that the complexity of the kernel evaluation is
linear in the input dimension. Then the test statistic
ˆλn = nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu can be computed in O(J 3 +
J 2n + (dx + dy)Jn) time.

(cid:17)−1

Proof (sketch). Claim 1 for ˆu is straightforward. The
expression for ˆΣ in claim 2 follows directly from the
asymptotic covariance expression in Proposition 4. The
consistency of ˆΣ can be obtained by noting that the
ﬁnite sample bound for P((cid:107) ˆΣ − Σ(cid:107)F > t) decreases as
n increases. This is implicitly shown in Appendix F.2.2
and its following sections.

Although the dependency of the estimator on J is cubic,
we empirically observe that only a small value of J is
required (see Section 3). The number of test locations
J relates to the number of regions in X × Y of pxy and
pxpy that diﬀer (see Figure 1). In particular, J need not
increase with n for test consistency.

Our ﬁnal theoretical result gives a lower bound on
the test power of (cid:92)NFSIC2 i.e., the probability of correctly
rejecting H0. We will use this lower bound as the objective
function to determine VJ and the kernel parameters. Let
(cid:107) · (cid:107)F be the Frobenius norm.

Theorem 7 (A lower bound on the test power). Let
NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let K be a kernel
class for k, L be a kernel class for l, and V be a collection
with each element being a set of J locations. Assume that
that
1. There
and

ﬁnite Bk
supk∈K supx,x(cid:48)∈X |k(x, x(cid:48))|
supl∈L supy,y(cid:48)∈Y |l(y, y(cid:48))| ≤ Bl.

and Bl
≤

such
Bk

exist

2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)Σ−1(cid:107)F < ∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test
power satisﬁes P

≥ L(λn) where

(cid:16)ˆλn ≥ r

(cid:17)

L(λn) = 1 − 62e−ξ1γ2

− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

n(λn−r)2/n − 2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

1

2JB2,
(cid:98)·(cid:99) is the ﬂoor function, ξ1 :=
B := BkBl, ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 :=
28B4J 2c2
J ˜c, and B∗ is
a constant depending on only Bk and Bl. Moreover, for
suﬃciently large ﬁxed n, L(λn) is increasing in λn.

1J 2B∗ , ξ2 := 72c2

1, c1 := 4B2J

J ˜c, c2 := 4B

32c2

√

√

(cid:17)

(cid:17)

(cid:16)

(cid:111)

(cid:110)

y,u]

y ∈ [σ2

x ∈ [σ2

y,l < σ2

(x, v) (cid:55)→ exp

(y, w) (cid:55)→ exp

the kernels k and l,

− (cid:107)x−v(cid:107)2
2σ2
x
x,l < σ2
| σ2

in Appendix F. To put
We provide the proof
let θx and θy be the
Theorem 7 into perspective,
parameters of
respectively.
We denote by θ = {θx, θy, VJ } the collection of
Assume that
all tuning parameters of the test.
x,l, σ2
| σ2
x,u]
K =
=:
Kg for some 0 < σ2
x,u < ∞ and L =
(cid:111)
(cid:110)
(cid:16)
− (cid:107)y−w(cid:107)2
y,l, σ2
=: Lg for
2σ2
y
some 0 < σ2
y,u < ∞ are Gaussian kernel classes.
Then, in Theorem 7, B = Bk = Bl = 1, and B∗ = 2.
The assumption ˜c < ∞ is a technical condition to guar-
antee that the test power lower bound is ﬁnite for all
θ deﬁned by the feasible sets K, L, and V. Let V(cid:15),r :=
(cid:8)VJ | (cid:107)vi(cid:107)2, (cid:107)wi(cid:107)2 ≤ r and (cid:107)vi − vj(cid:107)2
2 ≥
(cid:15), for all i (cid:54)= j(cid:9). If we set K = Kg, L = Lg, and V = V(cid:15),r
for some (cid:15), r > 0, then ˜c < ∞ as Kg, Lg, and V(cid:15),r are
compact. In practice, these conditions do not necessarily
create restrictions as they almost always hold implicitly.
We show in Appendix C that the objective function used
to choose VJ will discourage any two locations to be in
the same neighborhood.

2 + (cid:107)wi − wj(cid:107)2

Parameter Tuning The test power lower bound
L(λn) in Theorem 7 is a function of λn = nu(cid:62)Σ−1u
which is the population counterpart of the test statistic
ˆλn. As in FSIC, it can be shown that λn = 0 if and
only if X are Y are independent (from Proposition 2).
If X and Y are dependent, then λn > 0. According to
Theorem 7, for a suﬃciently large n, the test power lower
bound is increasing in λn. One can therefore think of
λn (a function of θ) as representing how easily the test
rejects H0 given a problem Pxy. The higher the λn, the

5

3 Experiments

In this section, we empirically study the performance
of the proposed method on both toy (Section 3.1) and
real-life problems (Section 3.2). Our interest is in the
performance of linear-time tests on challenging problems
which require a large sample size to be able to accurately
reveal the dependence. All the code is available at https:
//github.com/wittawatj/fsic-test.

We compare the proposed NFSIC with optimization
(NFSIC-opt) to ﬁve multivariate nonparametric tests.
The (cid:92)NFSIC2 test without optimization (NFSIC-med) acts
as a baseline, allowing the eﬀect of parameter optimization
to be clearly seen. For pedagogical reason, we consider
the original HSIC test of Gretton et al. [2005] denoted by
QHSIC, which is a quadratic-time test. Nyström HSIC
(NyHSIC) uses a Nyström approximation to the kernel
matrices of X and Y when computing the HSIC statistic.
FHSIC is another variant of HSIC in which a random
Fourier feature approximation [Rahimi and Recht, 2008]
to the kernel is used. NyHSIC and FHSIC are studied in
Zhang et al. [2016] and can be computed in O(n), with
quadratic dependency on the number of inducing points
in NyHSIC, and quadratic dependency in the number
of random features in FHSIC. Finally, the Randomized
Dependence Coeﬃcient (RDC) proposed in Lopez-Paz
et al. [2013] is also considered. The RDC can be seen as
the primal form (with random Fourier features) of the
kernel canonical correlation analysis of Bach and Jordan
[2002] on copula-transformed data. We consider RDC
as a linear-time test even though preprocessing by an
empirical copula transform costs O((dx + dy)n log n).

We use Gaussian kernel classes Kg and Lg for both
X and Y in all the methods. Except NFSIC-opt, all
other tests use full sample to conduct the indepen-
dence test, where the Gaussian widths σx and σy are
set according to the widely used median heuristic i.e.,
σx = median ({(cid:107)xi − xj(cid:107)2 | 1 ≤ i < j ≤ n}), and σy is
set in the same way using {yi}n
i=1. The J locations for
NFSIC-med are randomly drawn from the standard multi-
variate normal distribution in each trial. For a sample of
size n, NFSIC-opt uses half the sample for parameter tun-
ing, and the other disjoint half for the test. We permute
the sample 300 times in RDC1 and HSIC to simulate
from the null distribution and compute the test threshold.
The null distributions for FHSIC and NyHSIC are given
by a ﬁnite sum of weighted χ2(1) random variables given
in Eq. 8 of Zhang et al. [2016]. Unless stated otherwise,
we set the test threshold of the two NFSIC tests to be the
(1 − α)-quantile of χ2(J). To provide a fair comparison,
we set J = 10, use 10 inducing points in NyHSIC, and 10

1We use a permutation test for RDC, following the authors’
(https://github.com/lopezpaz/randomized_

implementation
dependence_coefficient, referred commit: b0ac6c0).

(a) ˆµxy(v, w)

(b) (cid:92)µxµy(v, w)

(c) (cid:98)Σ(v, w)

(d) Statistic ˆλn(v, w)

Figure 1: Illustration of (cid:92)NFSIC2.

greater the lower bound on the test power, and thus the
more likely it is that the test will reject H0 when it is
false.

In light of this reasoning, we propose setting θ to θ∗ =
arg maxθ λn. That this procedure is also valid under H0
can be seen as follows. Under H0, θ∗ = arg maxθ 0 will be
d→ χ2(J)
arbitrary. Since Theorem 7 guarantees that ˆλn
as n → ∞ for any θ, the asymptotic null distribution does
not change by using θ∗. In practice, λn is a population
quantity which is unknown. We propose dividing the
sample Zn into two disjoint sets: training and test sets.
The training set is used to optimize for θ∗, and the test set
is used for the actual independence test with the optimized
θ∗. The splitting is to guarantee the independence of θ∗
and the test sample, which is an assumption of Theorem
5.

we

To

better

under (cid:92)NFSIC2,
visualize
ˆµxy(v, w), (cid:91)µxµy(v, w) and ˆΣ(v, w) as a function of one
In this
test location (v, w) on a simple toy problem.
problem, Y = −X + Z where Z ∼ N (0, 0.32). As we con-
sider only one location (J = 1), ˆΣ(v, w) is a scalar. The
(ˆµxy(v,w)−(cid:92)µxµy(v,w))2
statistic can be written as ˆλn = n
.
ˆΣ(v,w)
These components are shown in Figure 1, where we use
Gaussian kernels for both X and Y , and the horizontal
and vertical axes correspond to v ∈ R and w ∈ R,
respectively.

Intuitively, ˆu(v, w) = ˆµxy(v, w) − (cid:91)µxµy(v, w) captures
the diﬀerence of the joint distribution and the product of
the marginals as a function of (v, w). Squaring ˆu(v, w)
and dividing it by the variance shown in Figure 1c gives
the statistic (also the parameter tuning objective) shown
in Figure 1d. The latter ﬁgure suggests that the parameter
tuning objective function can be non-convex. However,
we note that the non-convexity arises since there are
multiple ways to detect the diﬀerence between the joint
distribution and the product of the marginals. In this case,
the lower left and upper right regions equally indicate the
largest diﬀerence.

6

(a) SG (α = 0.05)

(b) SG (α = 0.05)

(c) Sin

(d) GSign

Figure 2: (a): Runtime. (b): Probability of rejecting H0 as problem parameters vary. Fix n = 4000.

random Fourier features in FHSIC and RDC.

Optimization of NFSIC-opt The parameters of
NFSIC-opt are σx, σy, and J locations of size (dx +
dy)J. We treat all the parameters as a long vector in
R2+(dx+dy)J and use gradient ascent to optimize ˆλn/2.
We observe that initializing VJ by randomly picking J
points from the training sample yields good performance.
The regularization parameter γn in NFSIC is ﬁxed to a
small value, and is not optimized. It is worth emphasizing
that the complexity of the optimization procedure is still
linear in n.

Since FSIC, NyHFSIC and RDC rely on a ﬁnite-
dimensional kernel approximation, these tests are consis-
tent only if both the number of features increases with n.
By constrast, the proposed NFSIC requires only n to go
to inﬁnity to achieve consistency i.e., J can be ﬁxed. We
refer the reader to Appendix C for a brief investigation
of the test power vs. increasing J. The test power does
not necessarily monotonically increase with J.

3.1 Toy Problems

We consider three toy problems: Same Gaussian (SG),
Sinusoid (Sin), and Gaussian Sign (GSign).

1. Same Gaussian (SG). The two variables are inde-
pendently drawn from the standard multivariate normal
distribution i.e., X ∼ N (0, Idx ) and Y ∼ N (0, Idy ) where
Id is the d × d identity matrix. This problem represents
a case in which H0 holds.

2. Sinusoid (Sin). Let pxy be the probability density
of Pxy.
In the Sinusoid problem, the dependency of
X and Y is characterized by (X, Y ) ∼ pxy(x, y) ∝ 1 +
sin(ωx) sin(ωy), where the domains of X , Y = (−π, π)
and ω is the frequency of the sinusoid. As the frequency
ω increases, the drawn sample becomes more similar to
a sample drawn from Uniform((−π, π)2). That is, the
higher ω, the harder to detect the dependency between
X and Y . This problem was studied in Sejdinovic et al.
[2013]. Plots of the density for a few values of ω are
shown in Figures 6 and 7 in the appendix. The main
characteristic of interest in this problem is the local change
in the density function.

3. Gaussian Sign (GSign). In this problem, Y =

|Z| (cid:81)dx
i=1 sgn(Xi), where X ∼ N (0, Idx ), sgn(·) is the sign
function, and Z ∼ N (0, 1) serves as a source of noise. The
full interaction of X = (X1, . . . , Xdx ) is what makes the
problem challenging. That is, Y is dependent on X, yet
it is independent of any proper subset of {X1, . . . , Xd}.
Thus, simultaneous consideration of all the coordinates
of X is required to successfully detect the dependency.

We ﬁx n = 4000 and vary the problem parameters.
Each problem is repeated for 300 trials, and the sample is
redrawn each time. The signiﬁcance level α is set to 0.05.
The results are shown in Figure 2. It can be seen that
in the SG problem (Figure 2b) where H0 holds, all the
tests achieve roughly correct type-I errors at α = 0.05.
In the Sin problem, NFSIC-opt achieves the highest test
power for all considered ω = 1, . . . , 6, highlighting its
strength in detecting local changes in the joint density.
The performance of NFSIC-med is signiﬁcantly lower than
that of NFSIC-opt. This phenomenon clearly emphasizes
the importance of the optimization to place the locations
at the relevant regions in X × Y. RDC has a remarkably
high performance in both Sin and GSign (Figure 2c, 2d)
despite no parameter tuning. Interestingly, both NFSIC-
opt and RDC outperform the quadratic-time QHSIC
in these two problems. The ability to simultaneously
consider interacting features of NFSIC-opt is indicated
by its superior test power in GSign, especially at the
challenging settings of dx = 5, 6. An average trial runtime
for each test in the SG problem is shown in Figure 2a. We
observe that the runtime does not increase with dimension,
as the complexity of all the tests is linear in the dimension
of the input. All the tests are implemented in Python
using a common SciPy Stack.

To investigate the sample eﬃciency of all the tests,
we ﬁx dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in
GSign, and increase n. Figure 3 shows the results. The
quadratic dependency on n in QHSIC makes it infeasible
both in terms of memory and runtime to consider n larger
than 6000 (Figure 3a). In constrast, although not the
most time-eﬃcient, NFSIC-opt has the highest sample-
eﬃciency for GSign, and for Sin in the low-sample regime,
signiﬁcantly outperforming QHSIC. Despite the small
additional overhead from the optimization, we are yet
able to conduct an accurate test with n = 105, dx = dy =

7

(a) SG. dx = dy = 250.

(b) SG. dx = dy = 250.

(c) Sin. ω = 4.

(d) GSign. dx = 4.

Figure 3: (a) Runtime. (b): Probability of rejecting H0 as n increases in the toy problems.

250 in less than 100 seconds. We observe in Figure 3b
that the two NFSIC variants have correct type-I errors
across all sample sizes, indicating that the asymptotic null
distribution approximately holds by the time n reaches
1000. We recall from Theorem 5 that the NFSIC test
with random test locations will asymptotically reject H0
if it is false. A demonstration of this property is given in
Figure 3c, where the test power of NFSIC-med eventually
reaches 1 with n higher than 105.

3.2 Real Problems

We now examine the performance of our proposed test
on real problems.

Million Song Data (MSD) We consider a subset of
the Million Song Data2 [Bertin-Mahieux et al., 2011], in
which each song (X) out of 515,345 is represented by 90
features, of which 12 features are timbre average (over
all segments) of the song, and 78 features are timbre
covariance. Most of the songs are western commercial
tracks from 1922 to 2011. The goal is to detect the
dependency between each song and its year of release
(Y ). We set α = 0.01, and repeat for 300 trials where
the full sample is randomly subsampled to n points in
each trial. Other settings are the same as in the toy
problems. To make sure that the type-I error is correct,
we use the permutation approach in the NFSIC tests to
compute the threshold. Figure 4b shows the test powers
as n increases from 500 to 2000. To simulate the case
where H0 holds in the problem, we permute the sample
to break the dependency of X and Y . The results are
shown in Figure 5 in the appendix.

Evidently, NFSIC-opt has the highest test power among
all the linear-time tests for all the sample sizes. Its test
power is second to only QHSIC. We recall that NFSIC-opt
uses half of the sample for parameter tuning. Thus, at
n = 500, the actual sample for testing is 250, which is
relatively small. The fact that there is a vast power gain
from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500
suggests that the optimization procedure can perform
well even at a lower sample sizes.

(a) MSD problem.

(b) Videos & Captions problem.
Figure 4: Probability of rejecting H0 as n increases in
the two real problems. α = 0.01.

Videos and Captions Our last problem is based on
the VideoStory46K3 dataset [Habibian et al., 2014]. The
dataset contains 45,826 Youtube videos (X) of an average
length of roughly one minute, and their corresponding
text captions (Y ) uploaded by the users. Each video is
represented as a dx = 2000 dimensional Fisher vector en-
coding of motion boundary histograms (MBH) descriptors
of Wang and Schmid [2013]. Each caption is represented
as a bag of words with each feature being the frequency
of one word. After ﬁltering only words which occur in at
least six video captions, we obtain dy = 1878 words. We
examine the test powers as n increases from 2000 to 8000.
The results are given in Figure 4. The problem is suﬃ-
ciently challenging that all linear-time tests achieve a low
power at n = 2000. QHSIC performs exceptionally well
on this problem, achieving a maximum power throughout.
NFSIC-opt has the highest sample eﬃciency among the
linear-time tests, showing that the optimization procedure
is also practical in a high dimensional setting.

Acknowledgement

We thank the Gatsby Charitable Foundation for the ﬁ-
nancial support. The major part of this work was carried
out while Zoltán Szabó was a research associate at the
Gatsby Computational Neuroscience Unit, University Col-
lege London.

2Million Song Data subset: https://archive.ics.uci.edu/ml/

3VideoStory46K dataset:

https://ivi.fnwi.uva.nl/isis/

datasets/YearPredictionMSD.

mediamill/datasets/videostory.php.

8

References

T. W. Anderson. An Introduction to Multivariate Statis-

tical Analysis. Wiley, 2003.

F. R. Bach and M. I. Jordan. Kernel independent compo-
nent analysis. Journal of Machine Learning Research,
3:1–48, 2002.

T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and
P. Lamere. The million song dataset. In International
Conference on Music Information Retrieval (ISMIR),
2011.

K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and
A. Gretton. Fast Two-Sample Testing with Analytic
Representations of Probability Measures. In Advances
in Neural Information Processing Systems (NIPS),
pages 1981–1989. 2015.

X. Huo and G. J. Székely. Fast computing for distance
covariance. Technical report, 2014. URL https://
arxiv.org/abs/1410.1503.

W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gret-
ton. Interpretable Distribution Features with Maximum
Testing Power. 2016. URL http://arxiv.org/abs/
1605.06796.

J. Kowalski and X. M. Tu. Modern Applied U-Statistics.

John Wiley & Sons, 2008.

E. L. Lehmann. Elements of Large-Sample Theory.

Springer Science & Business Media, 1999.

D. Lopez-Paz, P. Hennig, and B. Schölkopf. The Random-
ized Dependence Coeﬃcient. In Advances in Neural
Information Processing Systems (NIPS), pages 1–9.
2013.

J. Dauxois and G. M. Nkiet. Nonlinear canonical analysis
and independence tests. The Annals of Statistics, 26
(4):1254–1278, 1998.

A. Rahimi and B. Recht. Random features for large-scale
kernel machines. In Advances in Neural Information
Processing Systems (NIPS), pages 1177–1184. 2008.

A. Feuerverger. A consistent test for bivariate dependence.
International Statistical Review, 61(3):419–433, 1993.

K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf. Ker-
nel measures of conditional dependence. In Advances in
Neural Information Processing Systems (NIPS), pages
489–496, 2008.

A. Gretton. A simpler condition for consistency of a
kernel independence test. Technical report, 2015. URL
http://arxiv.org/abs/1501.06103.

A. Gretton and L. Györﬁ. Consistent nonparametric
tests of independence. Journal of Machine Learning
Research, 11:1391–1423, 2010.

A. Gretton, O. Bousquet, A. Smola, and B. Schölkopf.
Measuring Statistical Dependence with Hilbert-
Schmidt Norms.
In Algorithmic Learning Theory
(ALT), pages 63–77. 2005.

A. Gretton, K. Fukumizu, C. H. Teo, L. Song,
B. Schölkopf, and A. J. Smola. A Kernel Statistical Test
of Independence. In Advances in Neural Information
Processing Systems (NIPS), pages 585–592. 2008.

A. Habibian, T. Mensink, and C. G. Snoek. Videostory:
A new multimedia embedding for few-example recogni-
tion and translation of events. In ACM International
Conference on Multimedia, pages 17–26, 2014.

R. Heller, Y. Heller, S. Kaufman, B. Brill, and M. Gorﬁne.
Consistent distribution-free k-sample and independence
tests for univariate random variables. Journal of Ma-
chine Learning Research, 17(29):1–54, 2016.

D. Sejdinovic, B. Sriperumbudur, A. Gretton, and
K. Fukumizu. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. The Annals of
Statistics, 41(5):2263–2291, 2013.

R. J. Serﬂing. Approximation Theorems of Mathematical

Statistics. John Wiley & Sons, 2009.

A. Smola, A. Gretton, L. Song, and B. Schölkopf. A
hilbert space embedding for distributions. In Inter-
national Conference on Algorithmic Learning Theory
(ALT), pages 13–31, 2007.

B. K. Sriperumbudur, A. Gretton, K. Fukumizu,
B. Schölkopf, and G. R. G. Lanckriet. Hilbert Space Em-
beddings and Metrics on Probability Measures. Journal
of Machine Learning Research, 11:1517–1561, 2010.

I. Steinwart and A. Christmann. Support vector machines.

Springer Science & Business Media, 2008.

G. J. Székely and M. L. Rizzo. Brownian distance covari-
ance. The Annals of Applied Statistics, 3(4):1236–1265,
2009.

G. J. Székely, M. L. Rizzo, and N. K. Bakirov. Measuring
and testing dependence by correlation of distances. The
Annals of Statistics, 35(6):2769–2794, 2007.

A. W. v. d. Vaart. Asymptotic Statistics. Cambridge

University Press, 2000.

H. Wang and C. Schmid. Action recognition with im-
proved trajectories. In IEEE International Conference
on Computer Vision (ICCV), pages 3551–3558, 2013.

9

K. Zhang, J. Peters, D. Janzing, B., and B. Schölkopf.
Kernel-based conditional independence test and appli-
cation in causal discovery. In Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pages 804–813, 2011.

Q. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. Large-
Scale Kernel Methods for Independence Testing. 2016.
URL http://arxiv.org/abs/1606.07892.

10

An Adaptive Test of Independence with Analytic Kernel Embeddings
Supplementary Material

A Type-I Errors

In this section, we show that all the tests have correct type-I errors (i.e., the probability of reject H0 when it is true)
in real problems. We permute the joint sample so that the dependency is broken to simulate cases in which H0
holds. The results are shown in Figure 5.

(a) MSD problem (permuted).

(a) Videos & Captions problem with shuf-
ﬂed sample.

Figure 5: Probability of rejecting H0 as n increases in the Million Song problem. α = 0.01.

B Redundant Test Locations

Here, we provide a simple illustration to show that two locations t1 = (v1, w1) and t2 = (v2, w2) which are too
close to each other will reduce the optimization objective. We consider the Sinusoid problem described in Section
3.1 with ω = 1, and use J = 2 test locations. In Figure 6, t1 is ﬁxed at the red star, while t2 is varied along the
horizontal line. The objective value ˆλn as a function of (t1, t2) is shown in the bottom ﬁgure. It can be seen that ˆλn
decreases sharply when t2 is in the neighborhood of t1. This property implies that two locations which are too close
will not maximize the objective function (i.e., the second feature contains no additional information when it matches
the ﬁrst). For J > 2, the objective sharply decreases if any two locations are in the same neighborhood.

Figure 6: Plot of optimization objective values as location t2 moves along the green line. The objective sharply
drops when the two locations are in the same neighborhood.

C Test Power vs. J

It might seem intuitive that as the number of locations J increases, the test power should also increase. Here, we
empirically show that this statement is not always true. Consider the Sinusoid toy example described in Section 3.1
with ω = 2 (also see the left ﬁgure of Figure 7). By construction, X and Y are dependent in this problem. We run

11

NFSIC test with a sample size of n = 800, varying J from 1 to 600. For each value of J, the test is repeated for 500
times. In each trial, the sample is redrawn and the J test locations are drawn from Uniform((−π, π)2). There is no
optimization of the test locations. We use Gaussian kernels for both X and Y , and use the median heuristic to set
the Gaussian widths to 1.8. Figure 7 shows the test power as J increases.

Figure 7: The Sinusoid problem and the plot of test power vs. the number of test locations.

We observe that the test power does not monotonically increase as J increases. When J = 1, the diﬀerence of pxy
and pxpy cannot be adequately captured, resulting in a low power. The power increases rapidly to roughly 0.8 at
J = 10, and stays at the maximum until about J = 100. Then, the power starts to drop sharply when J is higher
than 400 in this problem.

Unlike random Fourier features, the number of test locations in NFSIC is not the number of Monte Carlo particles
used to approximate an expectation. There is a tradeoﬀ: if the test locations are in key regions (i.e., regions in
which there is a big diﬀerence between pxy and pxpy), then they increase power; yet the statistic gains in variance
(thus reducing test power) as J increases. As can be seen in Figure 7, there are eight key regions (in blue) that can
reveal the diﬀerence of pxy and pxpy. Using an unnecessarily high J not only makes the covariance matrix ˆΣ harder
to estimate accurately, it also increases the computation as the complexity on J is O(J 3).

We note that NFSIC is not intended to be used with a large J. In practice, it should be set to be large enough so
as to capture the key regions as stated. As a practical guide, with optimization of the test locations, a good starting
point is J = 5 or 10.

D Proof of Proposition 3

Recall Proposition 3,
Proposition (A product of Gaussian kernels is characteristic and analytic). Let k(x, x(cid:48)) = exp (cid:0)−(x − x(cid:48))(cid:62)A(x − x(cid:48))(cid:1)
and l(y, y(cid:48)) = exp (cid:0)−(y − y(cid:48))(cid:62)B(y − y(cid:48))(cid:1) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for
positive deﬁnite matrices A and B. Then, g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteristic and analytic on
(Rdx × Rdy ) × (Rdx × Rdy ).

Proof. Let z := (x(cid:62), y(cid:62))(cid:62) and z(cid:48) := (x(cid:48)(cid:62), y(cid:48)(cid:62))(cid:62) be vectors in Rdx+dy . We prove by reducing the product kernel to
one Gaussian kernel with g(z, z(cid:48)) = exp (cid:0)−(z − z(cid:48))(cid:62)C(z − z(cid:48))(cid:1) where C :=
. Write g(z, z(cid:48)) = Ψ(z − z(cid:48))
where Ψ(t) := exp (cid:0)−t(cid:62)Ct(cid:1). Since C is positive deﬁnite, we see that the ﬁnite measure ζ corresponding to Ψ as
deﬁned in Lemma 12 has support everywhere in Rdx+dy . Thus, Sriperumbudur et al. [2010, Theorem 9] implies that
g is characteristic.

(cid:18) A 0
0 B

(cid:19)

To see that g is analytic, we observe that for each z(cid:48) ∈ Rdx+dy , z (cid:55)→ −(z − z(cid:48))(cid:62)C(z − z(cid:48)) is a multivariate
polynomial in z, which is known to be analytic. Using the fact that t (cid:55)→ exp(t) is analytic on R, and that a
composition of analytic functions is analytic, we see that z (cid:55)→ exp (cid:0)−(z − z(cid:48))(cid:62)C(z − z(cid:48))(cid:1) is analytic on Rdx+dy for
each z(cid:48).

E Proof of Theorem 5

Recall Theorem 5,

12

Theorem 5 (Independence test using (cid:92)NFSIC2 is consistent). Let ˆΣ be a consistent estimate of Σ based on the
joint sample Zn. The (cid:92)NFSIC2 statistic is deﬁned as ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu where γn ≥ 0 is a regularization
parameter. Assume that

(cid:17)−1

1. Assumption A holds.
2. Σ is invertible almost surely with respect to VJ = {(vi, wi)}J

i=1 drawn from an absolutely continuous distribution.

3. limn→∞ γn = 0.

Then, for any k, l and VJ satisfying the assumptions,
1. Under H0, ˆλn

d→ χ2(J) as n → ∞.

2. Under H1, for any r ∈ R, limn→∞ P

(cid:16)ˆλn ≥ r

(cid:17)

(cid:92)NFSIC2 is consistent.

= 1 almost surely. That is, the independence test based on

(cid:17)−1 p
Proof. Assume that H0 holds. The consistency of ˆΣ and the continuous mapping theorem imply that
→
Σ−1 which is a constant. Let a be a random vector in RJ following N (0, Σ). By Vaart [2000, Theorem 2.7 (v)], it
nˆu d→ N (0, Σ)

d→ (cid:2)a, Σ−1(cid:3) where u = 0 almost surely by Proposition 2, and

(cid:16) ˆΣ + γnI

follows that

(cid:17)−1(cid:21)

(cid:16) ˆΣ + γnI

(cid:20)√

nˆu,

√

by Proposition 4. Since f (x, S) := x(cid:62)Sx is continuous, f
nˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

ˆu d→ a(cid:62)Σ−1a ∼ χ2(J) by Anderson [2003, Theorem 3.3.3]. This proves the ﬁrst claim.

(cid:18)√

(cid:16) ˆΣ + γnI

nˆu,

(cid:17)−1(cid:19)

d→ f (a, Σ−1). Equivalently,

The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al.
[2015]. Assume that H1 holds. Then, u (cid:54)= 0 almost surely by Proposition 2. Since k and l are bounded, it follows that
|ht(z, z(cid:48))| ≤ 2BkBl for any z, z(cid:48) (see (8)), and we have that ˆu a.s.→ u by Serﬂing [2009, Section 5.4, Theorem A]. Thus,
ˆu(cid:62) (cid:16) ˆΣ + γnI
d→ u(cid:62)Σ−1u by the continuous mapping theorem, and the consistency of ˆΣ. Consequently,

(cid:17)−1

ˆu − r
n

P

lim
n→∞

(cid:17)

(cid:16)ˆλn ≥ r
(cid:18)

= 1 − lim
n→∞

P

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

ˆu −

< 0

(cid:19)

r
n

(a)

= 1 − P (cid:0)u(cid:62)Σ−1u < 0(cid:1) (b)

= 1,

where at (a) we use the Portmanteau theorem [Vaart, 2000, Lemma 2.2 (i)] guaranteeing that xn
→ x if and only if
P(xn < t) → P(x < t) for all continuity points of t (cid:55)→ P(x < t). Step (b) is justiﬁed by noting that the covariance
matrix Σ is positive deﬁnite so that u(cid:62)Σ−1u > 0, and t (cid:55)→ P(u(cid:62)Σ−1u < t) (a step function) is continuous at 0.

d

F Proof of Theorem 7

Recall Theorem 7,

Theorem 7 (A lower bound on the test power). Let NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let K be a kernel class
for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that
1. There exist ﬁnite Bk and Bl such that supk∈K supx,x(cid:48)∈X |k(x, x(cid:48))| ≤ Bk and supl∈L supy,y(cid:48)∈Y |l(y, y(cid:48))| ≤ Bl.
2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)Σ−1(cid:107)F < ∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test power satisﬁes P

≥ L(λn) where

(cid:16)ˆλn ≥ r

(cid:17)

L(λn) = 1 − 62e−ξ1γ2

− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

n(λn−r)2/n − 2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

13

(cid:98)·(cid:99) is the ﬂoor function, ξ1 :=
c1 := 4B2J
ﬁxed n, L(λn) is increasing in λn.

J ˜c, c2 := 4B

√

√

1

32c2

1J 2B∗ , ξ2 := 72c2

2JB2, B := BkBl, ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 := 28B4J 2c2
1,
J ˜c, and B∗ is a constant depending on only Bk and Bl. Moreover, for suﬃciently large

Overview of the proof We ﬁrst derive a probabilistic bound for |ˆλn − λn|/n. The bound is in turn upper
bounded by an expression involving (cid:107)ˆu − u(cid:107)2 and (cid:107) ˆΣ − Σ(cid:107)F . The diﬀerence (cid:107)ˆu − u(cid:107)2 can be bounded by applying
the bound for U-statistics given in Serﬂing [2009, Theorem A, p. 201]. For (cid:107) ˆΣ − Σ(cid:107)F , we decompose it into a sum
of smaller components, and bound each term with a product variant of the Hoeﬀding’s inequality (Lemma 9). L(λn)
is obtained by combining all the bounds with the union bound.

F.1 Notations
Let (cid:104)A, B(cid:105)F := tr(A(cid:62)B) denote the Frobenius inner product, and (cid:107)A(cid:107)F := (cid:112)tr(A(cid:62)A) be the Frobenius norm.
Write z := (x, y) to denote a pair of points from X × Y. We write t := (v, w) to denote a pair of test locations
from X × Y. For brevity, an expectation over (x, y) (i.e., E(x,y)∼Pxy ) will be written as Ez or Exy. Deﬁne
˜k(x, v) := k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) := l(y, w) − Ey(cid:48)l(y(cid:48), w). Let B2(r) := {x | (cid:107)x(cid:107)2 ≤ r} be a closed ball
with radius r centered at the origin. Similarly, deﬁne BF (r) := {A | (cid:107)A(cid:107)F ≤ r} to be a closed ball with radius r of
J × J matrices under the Frobenius norm. Denote the max operation by (x1, . . . , xm)+ = max(x1, . . . , xm).
we write (cid:91)µxµy(v, w)

j(cid:54)=i k(xi, v)l(yj, w) to denote the unbiased plug-in estimator, and write ˆµx(v)ˆµy(w)

:=
:=
j=1 l(yj, w) which is a biased estimator. Deﬁne ˆub(v, w) := ˆµxy(v, w) − ˆµx(v)ˆµy(w)
where the superscript b stands for “biased”. To avoid confusing with a positive

i=1
i=1 k(xi, v) 1
n

For
1
n(n−1)
(cid:80)n
1
n

of marginal mean

so that ˆub := (cid:0)ˆub(t1), . . . , ˆub(tJ )(cid:1)(cid:62)
deﬁnite kernel, we will refer to a U-statistic kernel as a core.

product
(cid:80)

µx(v)µy(w),

embeddings

a
(cid:80)n

(cid:80)n

F.2 Proof
We will ﬁrst derive a bound for P(|ˆλn − λn| ≥ t), which will then be reparametrized to get a bound for the target
quantity P(ˆλn ≥ r). We closely follow the proof in Jitkrittum et al. [2016, Section C.1] up to (12), then we diverge.
We start by considering |ˆλn − λn|/n.

|ˆλn − λn|/n =

(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:17)−1
ˆu(cid:62) (cid:16) ˆΣ + γnI
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

=

≤

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u + u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)

+

:= ((cid:70))1 + ((cid:70))2 .

We next bound ((cid:70)1) and ((cid:70)2) separately.

((cid:70))1 =

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

≤

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

(cid:17)−1

ˆu(cid:62) (cid:16) ˆΣ + γnI
(cid:28)

(cid:12)
(cid:12)
ˆu − ˆu(cid:62) (Σ + γnI)−1 ˆu + ˆu(cid:62) (Σ + γnI)−1 ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)ˆu(cid:62) (Σ + γnI)−1 ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:68)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ˆu − ˆu(cid:62) (Σ + γnI)−1 ˆu
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:29)

=

(cid:17)−1

ˆuˆu(cid:62),

(cid:16) ˆΣ + γnI

− (Σ + γnI)−1

(cid:12)
(cid:12)
(cid:12)
F
≤ (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1 − (Σ + γnI)−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1(cid:107)F
= (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1[(Σ + γnI) − ( ˆΣ + γnI)](Σ + γnI)−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − ˆuu(cid:62) + ˆuu(cid:62) − uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1(cid:107)F

ˆuˆu(cid:62) − uu(cid:62), (Σ + γnI)−1(cid:69)

+

F

14

(a)
≤ (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1(cid:107)F (cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − ˆuu(cid:62) + ˆuu(cid:62) − uu(cid:62)(cid:107)F (cid:107)Σ−1(cid:107)F
(b)
≤

2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + (cid:0)(cid:107)ˆu(ˆu − u)(cid:62)(cid:107)F + (cid:107)(ˆu − u)u(cid:62)(cid:107)F

(cid:1) (cid:107)Σ−1(cid:107)F

(cid:107)ˆu(cid:107)2

√

≤

(cid:107)ˆu(cid:107)2

2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + ((cid:107)ˆu(cid:107)2 + (cid:107)u(cid:107)2) (cid:107)ˆu − u(cid:107)2(cid:107)Σ−1(cid:107)F ,

J
γn
√
J
γn

where at (a) we used (cid:107)(Σ + γnI)−1(cid:107)F ≤ (cid:107)Σ−1(cid:107)F , at (b) we used (cid:107)( ˆΣ + γnI)−1(cid:107)F ≤

J(cid:107)( ˆΣ + γnI)−1(cid:107)2 ≤

J/γn.

For ((cid:70))2, we have

(cid:12)
(cid:12)
(cid:12)u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
((cid:70))2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (cid:12)
(cid:10)uu(cid:62), (Σ + γnI)−1 − Σ−1(cid:11)
(cid:12)
(cid:12)
≤ (cid:107)uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1 − Σ−1(cid:107)F
= (cid:107)u(cid:107)2
≤ γn(cid:107)u(cid:107)2
(a)
≤ γn(cid:107)u(cid:107)2

2(cid:107)(Σ + γnI)−1(cid:107)F (cid:107)Σ−1(cid:107)F

2(cid:107)Σ−1(cid:107)2
F ,

F

2(cid:107)(Σ + γnI)−1 [Σ − (Σ + γnI)] Σ−1(cid:107)F

where at (a) we used (cid:107)(Σ + γnI)−1(cid:107)F ≤ (cid:107)Σ−1(cid:107)F .

Combining (5) and (6), we have

(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)

√

J
γn

≤

(cid:107)ˆu(cid:107)2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + ((cid:107)ˆu(cid:107)2 + (cid:107)u(cid:107)2) (cid:107)ˆu − u(cid:107)2(cid:107)Σ−1(cid:107)F + γn(cid:107)u(cid:107)2

2(cid:107)Σ−1(cid:107)2
F .

(7)

2 and (cid:107)u(cid:107)2

Bounding (cid:107)ˆu(cid:107)2
2 is
bounded. Recall that supx,x(cid:48)∈X |k(x, x(cid:48))| ≤ Bk, supy,y(cid:48) |l(y, y(cid:48))| ≤ Bl, our notation t = (v, w) for the test locations,
and zi := (xi, yi). We ﬁrst show that the U-statistic core h is bounded.

2 Here, we show that by the boundedness of the kernels k and l, it follows that (cid:107)ˆu(cid:107)2

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we deﬁne B := BkBl. It follows that

|ht((x, y), (x(cid:48), y(cid:48)))| =

(k(x, v) − k(x(cid:48), v))(l(y, w) − l(y(cid:48), w))

(cid:12)
1
(cid:12)
(cid:12)
2
(cid:12)
1
2

≤

(|k(x, v)| + |k(x(cid:48), v)|) (|l(y, w)| + |l(y(cid:48), w)|)

≤ 2BkBl := 2B,

(cid:107)ˆu(cid:107)2

2 =

htm (zi, zj)



≤

[2BkBl]2 = 4B2J,

2
n(n − 1)

(cid:88)

i<j


2

J
(cid:88)

m=1

(cid:107)u(cid:107)2

2 =

[EzEz(cid:48)htm(z, z(cid:48))]2 ≤ 4B2J.

Using the upper bounds on (cid:107)ˆu(cid:107)2

2, (cid:107)u(cid:107)2

2 ,(7) and the deﬁnition of ˜c, we have
(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
√

√

4B2J ˜c(cid:107)Σ − ˆΣ(cid:107)F + 4B

J ˜c(cid:107)ˆu − u(cid:107)2 + 4B2J ˜c2γn

(cid:107)Σ − ˆΣ(cid:107)F + c2(cid:107)ˆu − u(cid:107)2 + c3γn,

15





J
(cid:88)

m=1

J
(cid:88)

m=1

≤

=:

J
γn
c1
γn

(5)

√

(6)

(8)

(9)

(10)

(11)

where we deﬁne c1 := 4B2J

J ˜c, c2 := 4B

√

√

|ˆλn − λn| ≤

J ˜c, and c3 := 4B2J ˜c2. This upper bound implies that
c1
γn

n(cid:107)Σ − ˆΣ(cid:107)F + c2n(cid:107)ˆu − u(cid:107)2 + c3nγn.

(12)

We will separately upper bound (cid:107)Σ − ˆΣ(cid:107)F and (cid:107)ˆu − u(cid:107)2, and combine them with a union bound.

F.2.1 Bounding (cid:107)ˆu − u(cid:107)2
Let t∗ = arg maxt∈{t1,...,tJ } |ˆu(t) − u(t)|. Recall that u = (u(t1), . . . , u(tJ ))(cid:62) = (u1, . . . , uJ )(cid:62).

(cid:107)ˆu − u(cid:107)2 = sup

(cid:104)b, ˆu − u(cid:105)2 ≤ sup

|bj||ˆu(tj) − u(tj)|

b∈B2(1)

b∈B2(1)

j=1

J
(cid:88)

≤ |ˆu(t∗) − u(t∗)|

sup
b∈B2(1)

J
(cid:88)

j=1

|bj|

√

(a)
≤

√

J|ˆu(t∗) − u(t∗)|

sup
b∈B2(1)

(cid:107)b(cid:107)2

=

J|ˆu(t∗) − u(t∗)|,

J(cid:107)a(cid:107)2 for any a ∈ RJ . From (13), it can be seen that bounding (cid:107)ˆu − u(cid:107)2 amounts to
where at (a) we used (cid:107)a(cid:107)1 ≤
bounding the diﬀerence of a U-statistic ˆu(t∗) (see (4)) to its expectation u(t∗). Combining (13) and (12), we have

√

|ˆλn − λn| ≤

n(cid:107)Σ − ˆΣ(cid:107)F + c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

√

c1
γn

F.2.2 Bounding (cid:107) ˆΣ − Σ(cid:107)F
The plan is to write ˆΣ = ˆS − ˆub ˆub(cid:62), Σ = S − uu(cid:62), so that (cid:107) ˆΣ − Σ(cid:107)F ≤ (cid:107)ˆS − S(cid:107)F + (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F and bound
separately (cid:107)ˆS − S(cid:107)F and (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

Recall that Σij = η(ti, tj), η(t, t(cid:48)) = Exy[(cid:0)˜k(x, v)˜l(y, w) − u(v, w)(cid:1)(cid:0)˜k(x, v(cid:48))˜l(y, w(cid:48)) − u(v(cid:48), w(cid:48))(cid:1)] where ˜k(x, v) =
k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) = l(y, w) − Ey(cid:48)l(y(cid:48), w). Its empirical estimator (see Proposition 6) is ˆΣij = ˆη(ti, tj)
where

ˆη(t, t(cid:48)) =

[(cid:0)k(xi, v)l(yi, w) − ˆub(v, w)(cid:1)(cid:0)k(xi, v(cid:48))l(yi, w(cid:48)) − ˆub(v(cid:48), w(cid:48))(cid:1)]

=

k(xi, v)l(yi, w)k(xi, v(cid:48))l(yi, w(cid:48)) − ˆub(v, w)ˆub(v(cid:48), w(cid:48)),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:80)n

:=

that

k(x, v) − 1
and
n
i=1 k(xi, v)l(yi, w) = ˆub(v, w).

We
:=
m=1 k(xm, vi)l(ym, wi)k(xm, vj)l(yi, wj), and deﬁne similarly its population counterpart S such that

k(x, v)
note
1
n
Sij := Exy[˜k(x, v)˜l(y, w)˜k(x, v(cid:48))˜l(y, w(cid:48))]. We have

l(y, w) − 1
n
ˆS
RJ×J

i=1 l(yi, w).
such that

i=1 k(xi, v),

We deﬁne

l(y, w)

ˆSij

(cid:80)n

(cid:80)n

:=

1
n

∈

(cid:80)n

ˆΣ = ˆS − ˆub ˆub(cid:62),
Σ = S − uu(cid:62),

(cid:107) ˆΣ − Σ(cid:107)F = (cid:107)ˆS − S − (ˆub ˆub(cid:62) − uu(cid:62))(cid:107)F

≤ (cid:107)ˆS − S(cid:107)F + (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

With (16), (14) becomes

|ˆλn − λn| ≤

(cid:107)ˆS − S(cid:107)F +

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F + c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1n
γn

c1n
γn

√

We will further separately bound (cid:107)ˆS − S(cid:107)F and (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

16

(13)

(14)

(15)

(16)

(17)

F.2.3 Bounding (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F = (cid:107)ˆub ˆub(cid:62) − ˆubu(cid:62) + ˆubu(cid:62) − uu(cid:62)(cid:107)F
≤ (cid:107)ˆub(ˆub − u)(cid:62)(cid:107)F + (cid:107)(ˆub − u)u(cid:62)(cid:107)F
= (cid:107)ˆub(cid:107)2(cid:107)ˆub − u(cid:107)2 + (cid:107)ˆub − u(cid:107)2(cid:107)u(cid:107)2

√

≤ 4B

J(cid:107)ˆub − u(cid:107)2,

√

where we used (10) and the fact that (cid:107)ˆub(cid:107)2 ≤ 2B

J which can be shown similarly to (9) as

(cid:107)ˆub(cid:107)2

2 =

[ˆµxy(vm, wm) − ˆµx(vm)ˆµy(wm)]2 =

htm (zi, zj)



≤

[2BkBl]2 = 4B2J.

J
(cid:88)

n
(cid:88)

n
(cid:88)





1
n2

m=1

i=1

j=1


2

J
(cid:88)

m=1

J
(cid:88)

m=1

Let (˜v, ˜w) := ˜t = arg maxt∈{t1,...,tJ } |ˆub(t) − u(t)|. We bound (cid:107)ˆub − u(cid:107)2 by

(cid:107)ˆub − u(cid:107)2

√

(a)
≤

√

√

√

√

J|ˆub(˜t) − u(˜t)|
(cid:12)ˆµxy(˜t) − ˆµx(˜v)ˆµy( ˜w) − u(˜t)(cid:12)
J (cid:12)
(cid:12)
(cid:12)ˆµxy(˜t) − (cid:91)µxµy(˜t) + (cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w) − u(˜t)(cid:12)
J (cid:12)
(cid:12)
(cid:12)ˆµxy(˜t) − (cid:91)µxµy(˜t) − u(˜t)(cid:12)
J (cid:12)
(cid:12) +
(cid:12)ˆu(˜t) − u(˜t)(cid:12)
J (cid:12)
(cid:12) +

J (cid:12)
(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12)
(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12) ,

J (cid:12)

√

√

=

=

≤

=

where at (a) we used the same reasoning as in (13). The bias (cid:12)
bounded as

(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)

(cid:12) in the second term can be

k(xi, ˜v)l(yj, ˜w) −

k(xi, ˜v)l(yi, ˜w) −

k(xi, ˜v)l(yj, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:18)

(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12)
(cid:12)

=

=

1
n(n − 1)

1
n(n − 1)

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

n
(cid:88)

n
(cid:88)

i=1

j=1

n
n − 1

(cid:19) 1
n2

n
n − 1

(cid:19) 1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1

j=1

≤

1 −

≤

B
n − 1

+

B
n − 1

=

2B
n − 1

.

k(xi, ˜v)l(yj, ˜w) −

k(xi, ˜v)l(yj, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

1
n(n − 1)

n
(cid:88)

i=1

1
n2

n
(cid:88)

n
(cid:88)

j=1

1
n(n − 1)

i=1
(cid:12)
(cid:12)
(cid:12)
k(xi, ˜v)l(yi, ˜w)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
k(xi, ˜v)l(yj, ˜w)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(n − 1)

n
(cid:88)

i=1

k(xi, ˜v)l(yi, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

1 −

k(xi, ˜v)l(yj, ˜w) +

(18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Combining this upper bound with (18), we have

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F ≤ 4BJ (cid:12)

(cid:12)ˆu(˜t) − u(˜t)(cid:12)

(cid:12) +

(19)

With (19), (17) becomes

|ˆλn − λn| ≤

(cid:107)ˆS − S(cid:107)F +

c1n
γn

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12) +

c1n
γn

8B2J
n − 1

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

(20)

8B2J
n − 1

.

√

17

F.2.4 Bounding (cid:107)ˆS − S(cid:107)F
Recall that VJ = {t1, . . . , tJ }, ˆSij = ˆS(ti, tj) = 1
n
S(ti, tj) = Exy[˜k(x, vi)˜l(y, wi)˜k(x, vj)˜l(y, wj)]. Let (t(1), t(2)) = arg max(s,t)∈VJ ×VJ | ˆS(s, t) − S(s, t)|.

m=1 k(xm, vi)l(ym, wi)k(xm, vj)l(ym, wj), and Sij =

(cid:80)n

(cid:107)ˆS − S(cid:107)F = sup

B∈BF (1)

(cid:68)

(cid:69)
B, ˆS − S

F

J
(cid:88)

J
(cid:88)

≤ sup

B∈BF (1)

i=1

j=1

|Bij|| ˆSij − Sij|

≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12)

sup
B∈BF (1)

J
(cid:88)

J
(cid:88)

i=1

j=1

|Bij|

(a)
≤ J

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12)

sup
B∈BF (1)

(cid:107)B(cid:107)F

= J

ˆS(t(1), t(2)) − S(t(1), t(2))

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ,

where at (a) we used (cid:80)J

(cid:80)J

i=1

j=1 |Aij| ≤ J(cid:107)A(cid:107)F for any matrix A ∈ RJ×J . We arrive at

|ˆλn − λn| ≤

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12) +

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)
(cid:12)

√

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1Jn
γn
c1n
γn

(cid:12)
(cid:12)
(cid:12)
8B2J
n − 1

+

F.2.5 Bounding

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12)

Having an upper bound for
will deﬁne the following shorthands.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12) will allow us to bound (22). To keep the notations uncluttered, we

(21)

(22)

Expression

Shorthand

Expression

Shorthand

k(x, v)

k(x, v(cid:48))

k(xi, v)
k(xi, v(cid:48))
Ex∼Px k(x, v)
Ex∼Px k(x, v(cid:48))
(cid:80)n
1
i=1 k(xi, v)
n
1
i=1 k(xi, v(cid:48))
n

(cid:80)n

a

a(cid:48)

ai
a(cid:48)
i
˜a

˜a(cid:48)

a
a(cid:48)

l(y, w)

l(y, w(cid:48))

l(yi, w)
l(yi, w(cid:48))
Ey∼Py l(y, w)
Ey∼Py l(y, w(cid:48))
(cid:80)n
1
i=1 l(yi, w)
n
i=1 l(yi, w(cid:48))

(cid:80)n

1
n

b

b(cid:48)

bi
b(cid:48)
i
˜b
˜b(cid:48)

b
(cid:48)

b

We will also use · to denote a empirical expectation over x, or y, or (x, y). The argument under · will
i=1 k(xi, v)k(xi, v(cid:48)) and
i=1 k(xi, v)l(yi, w)k(xi, v(cid:48)), and so on. We deﬁne in the same way for the population expectation using

determine the variable over which we take the expectation. For instance, aa(cid:48) = 1
n
aba(cid:48) = 1
n
(cid:101)· i.e., (cid:102)aa(cid:48) = Ex [k(x, v)k(x, v(cid:48))] and (cid:103)aba(cid:48) = Exy [k(x, v)l(y, w)k(x, v(cid:48))].

(cid:80)n

(cid:80)n

With these shorthands, we can rewrite ˆS(t, t(cid:48)) and S(t, t(cid:48)) as

ˆS(t, t(cid:48)) =

(ai − a)(bi − b)(a(cid:48)

i − a(cid:48))(b(cid:48)

(cid:48)
i − b

),

1
n

n
(cid:88)

i=1

18

By expanding S(t, t(cid:48)), we have

S(t, t(cid:48)) = Exy

(cid:104)

(cid:105)
(a − ˜a)(b − ˜b)(a(cid:48) − ˜a(cid:48))(b(cid:48) − ˜b(cid:48))

.

S(t, t(cid:48)) = Exy

(cid:2) + aba(cid:48)b(cid:48) − aba(cid:48)˜b(cid:48) − ab˜a(cid:48)b(cid:48) + ab˜a(cid:48)˜b(cid:48)
− a˜ba(cid:48)b(cid:48) + a˜ba(cid:48)˜b(cid:48) + a˜b˜a(cid:48)b(cid:48) − a˜b˜a(cid:48)˜b(cid:48)
− ˜aba(cid:48)b(cid:48) + ˜aba(cid:48)˜b(cid:48) + ˜ab˜a(cid:48)b(cid:48) − ˜ab˜a(cid:48)˜b(cid:48)
+ ˜a˜ba(cid:48)b(cid:48) − ˜a˜ba(cid:48)˜b(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48) + ˜a˜b˜a(cid:48)˜b(cid:48)(cid:3)

= +(cid:94)aba(cid:48)b(cid:48) − (cid:103)aba(cid:48)˜b(cid:48) − (cid:103)abb(cid:48)˜a(cid:48) + (cid:101)ab˜a(cid:48)˜b(cid:48)
− (cid:93)aa(cid:48)b(cid:48)˜b + (cid:102)aa(cid:48)˜b˜b(cid:48) + (cid:102)ab(cid:48)˜a(cid:48)˜b − ˜a˜b˜a(cid:48)˜b(cid:48)
− (cid:103)a(cid:48)bb(cid:48)˜a + (cid:102)a(cid:48)b˜a˜b(cid:48) + ˜a˜a(cid:48) (cid:102)bb(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48)
+ (cid:103)a(cid:48)b(cid:48)˜a˜b − ˜a˜b˜a(cid:48)˜b(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48) + ˜a˜b˜a(cid:48)˜b(cid:48)
= +(cid:94)aba(cid:48)b(cid:48) − (cid:103)aba(cid:48)˜b(cid:48) − (cid:103)abb(cid:48)˜a(cid:48) + (cid:101)ab˜a(cid:48)˜b(cid:48)
− (cid:93)aa(cid:48)b(cid:48)˜b + (cid:102)aa(cid:48)˜b˜b(cid:48) + (cid:102)ab(cid:48)˜a(cid:48)˜b + (cid:103)a(cid:48)b(cid:48)˜a˜b
− (cid:103)a(cid:48)bb(cid:48)˜a + (cid:102)a(cid:48)b˜a˜b(cid:48) + ˜a˜a(cid:48) (cid:102)bb(cid:48) − 3˜a˜b˜a(cid:48)˜b(cid:48).

The expansion of ˆS(t, t(cid:48)) can be done in the same way. By the triangle inequality, we have

(cid:12)
(cid:12)
(cid:12)

ˆS(t, t(cid:48)) − S(t, t(cid:48))

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)aa(cid:48)b(cid:48) b − (cid:93)aa(cid:48)b(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bb(cid:48)a − (cid:103)a(cid:48)bb(cid:48)˜a
(cid:12) +

(cid:48)

(cid:12)
(cid:12)
(cid:12)aba(cid:48) b
(cid:12)
(cid:12)
(cid:12)aa(cid:48) b b
(cid:12)
(cid:12)
(cid:12)a(cid:48)bab

(cid:48)

(cid:48)

(cid:12)
(cid:12)

(cid:12)abb(cid:48)a(cid:48) − (cid:103)abb(cid:48)˜a(cid:48)(cid:12)
− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12) +
(cid:12)
(cid:12)
− (cid:102)aa(cid:48)˜b˜b(cid:48)(cid:12)
(cid:12)ab(cid:48)a(cid:48)b − (cid:102)ab(cid:48)˜a(cid:48)˜b
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12) +
(cid:12)
(cid:12)
− (cid:102)a(cid:48)b˜a˜b(cid:48)(cid:12)
(cid:12)a a(cid:48)bb(cid:48) − ˜a˜a(cid:48) (cid:102)bb(cid:48)
(cid:12)
(cid:12)
(cid:12)
(cid:12) + 3
(cid:12) +

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)aba(cid:48)b
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a(cid:48)b(cid:48)ab − (cid:103)a(cid:48)b(cid:48)˜a˜b
(cid:12)
(cid:12)
(cid:12)
− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)aba(cid:48)b
(cid:12)
(cid:12)
(cid:12) .

(cid:48)

(cid:48)

The ﬁrst term
by applying Lemma 9. Recall that we write (x1, . . . , xm)+ for max(x1, . . . , xm).

(cid:12)
(cid:12)
(cid:12) can be bounded by applying the Hoeﬀding’s inequality. Other terms can be bounded

(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

Bounding
have

(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (1st term). Since −B2 ≤ aba(cid:48)b(cid:48) ≤ B2, by the Hoeﬀding’s inequality (Lemma 14), we

P

(cid:16)(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 2 exp

−

(cid:18)

(cid:19)

.

nt2
2B4

Bounding
We note that |f1(x, y)| ≤ (BBk, Bl)+ and |f2(y)| ≤ (BBk, Bl)+. Thus, by Lemma 9 with E = 2, we have

− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (2nd term). Let f1(x, y) = aba(cid:48) = k(x, v)l(y, w)k(x, v(cid:48)) and f2(y) = b(cid:48) = l(y, w(cid:48)).

(cid:12)
(cid:12)
(cid:12)aba(cid:48) b

(cid:48)

P

(cid:16)(cid:12)
(cid:12)
(cid:12)aba(cid:48) b

(cid:48)

− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t

≥ 1 − 4 exp

−

(cid:18)

nt2
8(BBk, Bl)4
+

(cid:19)

.

Bounding
b(cid:48) = l(y, w(cid:48)). We can see that |f1(x, y)|, |f2(x)|, |f3(y)| ≤ (B, Bk, Bl)+. Thus, by Lemma 9 with E = 3, we have

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (4th term). Let f1(x, y) = ab = k(x, v)l(y, w), f2(x) = a(cid:48) = k(x, v(cid:48)) and f3(y) =

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b

(cid:48)

P

(cid:16)(cid:12)
(cid:12)aba(cid:48)b
(cid:12)

(cid:48)

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 6 exp

−

(cid:18)

nt2
18(B, Bk, Bl)6
+

(cid:19)

.

19

(cid:48)

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b

− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (last term). Let f1(x) = a = k(x, v), f2(y) = b = l(y, w), f3(x) = a(cid:48) = k(x, v(cid:48)) and
Bounding
f4(y) = b(cid:48) = l(y, w(cid:48)). It can be seen that |f1(x)|, |f2(y)|, |f3(x)|, |f4(y)| ≤ (Bk, Bl)+. Thus, by Lemma 9 with
E = 4, we have

(cid:16)
3

P

(cid:12)
(cid:12)aba(cid:48)b
(cid:12)

(cid:48)

− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 8 exp

−

(cid:18)

nt2
32 · 32(Bk, Bl)8
+

(cid:19)

.

Bounds for other terms can be derived in a similar way to yield

(3rd term) P

(5th term) P

(6th term) P

(7th term) P

(8th term) P

(9th term) P

(10th term) P

(11th term) P

(cid:48)

(cid:17)

(cid:16)(cid:12)
(cid:12)

(cid:12)abb(cid:48)a(cid:48) − (cid:103)abb(cid:48)˜a(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t
(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)aa(cid:48)b(cid:48) b − (cid:93)aa(cid:48)b(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
− (cid:102)aa(cid:48)˜b˜b(cid:48)(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)aa(cid:48) b b
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:12)ab(cid:48)a(cid:48)b − (cid:102)ab(cid:48)˜a(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:12)a(cid:48)b(cid:48)ab − (cid:103)a(cid:48)b(cid:48)˜a˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bb(cid:48)a − (cid:103)a(cid:48)bb(cid:48)˜a
(cid:12) ≤ t
(cid:16)(cid:12)
− (cid:102)a(cid:48)b˜a˜b(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bab
(cid:12) ≤ t
(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t

(cid:16)(cid:12)
(cid:12)a a(cid:48)bb(cid:48) − ˜a˜a(cid:48) (cid:102)bb(cid:48)
(cid:12)

(cid:17)

(cid:48)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

≥ 1 − 4 exp

−

≥ 1 − 4 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

≥ 1 − 4 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

,

,

,

(cid:19)

(cid:19)

18(B2

nt2
8(BBl, Bk)4
+
nt2
8(BBk, Bl)4
+
(cid:19)
nt2
k, Bl)6
+
nt2
18(B, Bk, Bl)6
+
nt2
18(B, Bk, Bl)6
+
(cid:19)
nt2
8(BBl, Bk)4
+
nt2
18(B, Bk, Bl)6
+
(cid:19)
nt2
18(Bk, B2

l )6
+

.

(cid:19)

(cid:19)

,

(cid:19)

,

,

,

By the union bound, we have

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

ˆS(t, t(cid:48)) − S(t, t(cid:48))

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ 12t
(cid:19)

(cid:18)

≥ 1 −

2 exp

−

+ 4 exp

−

nt2
2B4

(cid:18)

(cid:19)

(cid:19)

(cid:19)

nt2
18(B, Bk, Bl)6
+
(cid:18)

+ 6 exp

−

(cid:18)

+ 8 exp

−

(cid:19)

nt2
18(B, Bk, Bl)6
+
nt2
32 · 32(Bk, Bl)8
+
(cid:19)

(cid:19) (cid:21)

(cid:19)

(cid:18)

+ 24 exp

−

nt2
18(B, Bk, Bl)6
+

(cid:19)

(cid:18)

+ 4 exp

−

(cid:19)

nt2
8(BBl, Bk)4
+

+ 6 exp

−

= 1 −

2 exp

−

+ 8 exp

−

+ 8 exp

−

(cid:20)

(cid:20)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

4 exp

−

4 exp

−

(cid:18)

+ 6 exp

−

(cid:19)

(cid:19)

nt2
8(BBk, Bl)4
+
nt2
8(BBl, Bk)4
+
(cid:19)
nt2
2B4

nt2
18(B2
k, Bl)6
+
(cid:19)

nt2
8(BBk, Bl)4
+
(cid:18)

+ 6 exp

−

(cid:18)

+ 6 exp

−

(cid:19)

18(B2

nt2
k, Bl)6
+
nt2
18(B, Bk, Bl)6
+

(cid:18)

(cid:19)

(cid:19)

nt2
8(BBk, Bl)4
+

(cid:18)

+ 6 exp

−

nt2
18(Bk, B2
l )6
+
(cid:18)
(cid:19)

+ 8 exp

−

+ 8 exp

(cid:18)

+ 8 exp

−

(cid:19)

+ 6 exp

122nt2
B∗
122nt2
B∗

−

(cid:18)

(cid:19)

(cid:20)

(cid:18)

≥ 1 −

2 exp

−

+ 6 exp

(cid:18)

= 1 − 62 exp

−

(cid:18)

122nt2
B∗
122nt2
B∗
122nt2
B∗

−

(cid:19)

,

nt2
18(B, Bk, Bl)6
+
(cid:18)

nt2
18(Bk, B2

l )6
+

(cid:18)

+ 6 exp

−

(cid:19)

(cid:18)

(cid:19)

+ 6 exp

−

nt2
8(BBl, Bk)4
+
(cid:18)

+ 8 exp

−

(cid:19) (cid:21)

nt2
32 · 32(Bk, Bl)8
+
(cid:19)
122nt2
B∗

−

(cid:18)

(cid:19)

122nt2
B∗
122nt2
B∗

−

(cid:18)

+ 24 exp

(cid:19) (cid:21)

where

B∗ :=

1
122 max(2B4, 8(BBk, Bl)4

+, 8(BBl, Bk)4

+, 18(B, Bk, Bl)6

+, 18(B2

k, Bl)6

+, 18(Bk, B2

l )6

+, 32 · 32(Bk, Bl)8

+).

20

By reparameterization, it follows that

P

(cid:18) c1Jn
γn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12) ≤ t

(cid:19)

≥ 1 − 62 exp

−

(cid:18)

nt2
γ2
c2
1J 2nB∗

(cid:19)

.

(23)

F.2.6 Union Bound for

(cid:12)
(cid:12)
(cid:12)

ˆλn − λn

(cid:12)
(cid:12)
(cid:12) and Final Lower Bound

Recall from (22) that

|ˆλn − λn| ≤

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12) +

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)
(cid:12)

√

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1Jn
γn
c1n
γn

(cid:12)
(cid:12)
(cid:12)
8B2J
n − 1

+

We will bound terms in (22) separately and combine all the bounds with the union bound. As shown in (8), the
U-statistic core h is bounded between −2B and 2B. Thus, by Lemma 13 (with m = 2), we have

√

(cid:16)

P

c2n

J|ˆu(t∗) − u(t∗)| ≤ t

≥ 1 − 2 exp

−

(cid:17)

(cid:18)

(cid:98)0.5n(cid:99)t2
8c2
2n2JB2

(cid:19)

.

Bounding c1n
γn

8B2J
n−1 + c3nγn + 4BJc1n

γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12). By Lemma 13 (with m = 2), it follows that

P

(cid:18) c1n
γn

8B2J
n − 1

≥ 1 − 2 exp

+ c3nγn +

(cid:104)

(cid:98)0.5n(cid:99)γ2
n

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12) ≤ t

(cid:19)

t − c1n
γn
27B4J 2c2

8B2J
n−1 − c3nγn
1n2

(cid:105)2









−

(cid:32)

= 1 − 2 exp

−

(a)
≥ 1 − 2 exp

(cid:32)

−

(cid:98)0.5n(cid:99) (cid:2)tγn(n − 1) − 8c1B2nJ − c3n(n − 1)γ2

n

(cid:3)2

(cid:33)

27B4J 2c2
(cid:2)tγn(n − 1) − 8c1B2nJ − c3n(n − 1)γ2

1n2(n − 1)2

n

(cid:3)2

(cid:33)

,

28B4J 2c2

1n2(n − 1)

(24)

(25)

where at (a) we used (cid:98)0.5n(cid:99) ≥ (n − 1)/2. Combining (23), (24), and (25) with the union bound (set T = 3t), we can
bound (22) with

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

ˆλn − λn

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ T

≥ 1 − 62 exp

(cid:32)

− 2 exp

−

(cid:18)

(cid:19)

(cid:18)

−

− 2 exp

nT 2
γ2
32c2
1J 2nB∗

(cid:98)0.5n(cid:99)T 2
72c2
2n2JB2
nn(n − 1)(cid:3)2
(cid:2)T γn(n − 1)/3 − 8c1B2nJ − c3γ2
28B4J 2c2

1n2(n − 1)

−

(cid:19)

(cid:33)

.

Since

ˆλn − λn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ T implies ˆλn ≥ λn − T , a reparametrization with r = λn − T gives
(cid:12)

P

(cid:16)ˆλn ≥ r

(cid:17)

≥ 1 − 62 exp

−

(cid:18)

(cid:19)

n(λn − r)2
γ2
32c2
1J 2nB∗

(cid:18)

(cid:98)0.5n(cid:99)(λn − r)2

(cid:19)

− 2 exp

−

72c2

2n2JB2

(cid:32)

− 2 exp

−

(cid:2)(λn − r)γn(n − 1)/3 − 8c1B2nJ − c3γ2
28B4J 2c2

1n2(n − 1)

nn(n − 1)(cid:3)2

(cid:33)

Grouping constants into ξ1, . . . ξ5 gives the result.

:= L(λn).

21

The lower bound L(λn) takes the form

1 − 62 exp (cid:0)−C1(λn − Tα)2(cid:1) − 2 exp (cid:0)−C2(λn − Tα)2(cid:1) − 2 exp

(cid:18)

−

[(λn − Tα)C3 − C4]2
C5

(cid:19)

,

where C1, . . . , C5 are positive constants. For ﬁxed large enough n such that λn > Tα, and ﬁxed signiﬁcance level α,
increasing λn will increase L(λn). Speciﬁcally, since n is ﬁxed, increasing u(cid:62)Σ−1u in λn = nu(cid:62)Σ−1u will increase
L(λn).

G Helper Lemmas

This section contains lemmas used to prove the main results in this work.

Lemma 8 (Product to sum). Assume that |ai| ≤ B, |bi| ≤ B for i = 1, . . . , E. Then
BE−1 (cid:80)E

j=1 |aj − bj|.

(cid:12)
(cid:81)E
(cid:12)
(cid:12)

i=1 ai − (cid:81)E

i=1 bi

(cid:12)
(cid:12)
(cid:12) ≤

Proof.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

i=1

E
(cid:89)

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

i=1

ai −

bj

≤

ai −

aibE

+

aibE −

aibE−1bE

+ . . . +

a1

bj −

bj

E−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E−1
(cid:89)

i=1

E−2
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:12)
(cid:32)E−2
(cid:12)
(cid:89)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

j=2

E
(cid:89)

j=1

E
(cid:89)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j=2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ |aE − bE|

ai

+ |aE−1 − bE−1|

ai

bE

+ . . . + |a1 − b1|

bj

≤ |aE − bE|BE−1 + |aE−1 − bE−1| BE−1 + . . . + |a1 − b1| BE−1

= BE−1

|aj − bj|

E
(cid:88)

j=1

applying triangle inequality, and the boundedness of ai and bi-s.

Lemma 9 (Product variant of the Hoeﬀding’s inequality). For i = 1, . . . , E, let {x(i)
j=1 ⊂ Xi be an i.i.d. sample
from a distribution Pi, and fi : Xi (cid:55)→ R be a measurable function. Note that it is possible that P1 = P2 = · · · = PE
and {x(1)
j=1. Assume that |fi(x)| ≤ B < ∞ for all x ∈ Xi and i = 1, . . . , E. Write ˆPi to denote
an empirical distribution based on the sample {x(i)

j=1 = · · · = {x(E)

j }nE

j }n1

j }ni

j }ni

j=1. Then,

P

(cid:32)(cid:12)
(cid:34) E
(cid:12)
(cid:89)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:35)
fi(x(i))

−

(cid:34) E
(cid:89)

E

x(i)∼ ˆPi

i=1

E
x(i)∼Pifi(x(i))

≤ T

≥ 1 − 2

exp

−

(cid:33)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:88)

i=1

(cid:18)

niT 2
2E2B2E

(cid:19)

.

Proof. By Lemma 8, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34) E
(cid:89)

E

i=1

x(i)∼ ˆPi

(cid:35)
fi(x(i))

−

(cid:34) E
(cid:89)

i=1

E
x(i)∼Pifi(x(i))

≤ BE−1

x(i)∼ ˆPi

fi(x(i)) − E

(cid:12)
x(i)∼Pi fi(x(i))
(cid:12)
(cid:12) .

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)

E

By
(cid:16)(cid:12)
P
E
(cid:12)
(cid:12)

applying

the
fi(x(i)) − E

x(i)∼ ˆPi

Hoeﬀding’s
(cid:12)
(cid:12)
x(i)∼Pifi(x(i))
(cid:12) ≤ t

inequality
(cid:17)

≥ 1 − 2 exp

to
(cid:16)
− 2nit2
4B2

each
(cid:17)

term

the
. The result is obtained with a union bound.

sum,

we

in

have

H External Lemmas

In this section, we provide known results referred to in this work.

Lemma 10 (Chwialkowski et al. [2015, Lemma 1]). If k is a bounded, analytic kernel (in the sense given in
Deﬁnition 1) on Rd × Rd, then all functions in the RKHS deﬁned by k are analytic.

22

Lemma 11 (Chwialkowski et al. [2015, Lemma 3]). Let Λ be an injective mapping from the space of probability
measures into a space of analytic functions on Rd. Deﬁne

d2
VJ

(P, Q) =

|[ΛP ](vj) − [ΛQ](vj)|2 ,

J
(cid:88)

j=1

where VJ = {vi}J
respect to the Lebesgue measure. Then, dVJ (P, Q) is almost surely (w.r.t. VJ ) a metric.

i=1 are vector-valued i.i.d. random variables from a distribution which is absolutely continuous with

Lemma 12 (Bochner’s theorem [Rudin, 2011]). A continuous function Ψ : Rd → R is positive deﬁnite if and only if
it is the Fourier transform of a ﬁnite nonnegative Borel measure ζ on Rd, that is, Ψ(x) = (cid:82)
Rd e−ix(cid:62)ω dζ(ω), x ∈ Rd.

Lemma 13 (A bound for U-statistics [Serﬂing, 2009, Theorem A, p. 201]). Let h(x1, . . . , xm) be a U-
statistic kernel for an m-order U-statistic such that h(x1, . . . , xm) ∈ [a, b] where a ≤ b < ∞. Let Un =
(cid:0) n
h(xi1 , . . . , xim ) be a U-statistic computed with a sample of size n, where the summation is over the
m
(cid:0) n
m

(cid:1)−1 (cid:80)
(cid:1) combinations of m distinct elements {i1, . . . , im} from {1, . . . , n}. Then, for t > 0 and n ≥ m,

i1<···<im

P(Un − Eh(x1, . . . , xm) ≥ t) ≤ exp (cid:0)−2(cid:98)n/m(cid:99)t2/(b − a)2(cid:1) ,
P(|Un − Eh(x1, . . . , xm)| ≥ t) ≤ 2 exp (cid:0)−2(cid:98)n/m(cid:99)t2/(b − a)2(cid:1) ,

where (cid:98)x(cid:99) denotes the greatest integer which is smaller than or equal to x. Hoeﬃnd’s inequality is a special case
when m = 1.

Lemma 14 (Hoeﬀding’s inequality). Let X1, . . . , Xn be i.i.d. random variables such that a ≤ Xi ≤ b almost surely.
Deﬁne X := 1
n

i=1 Xi. Then,

(cid:80)n

P (cid:0)(cid:12)

(cid:12)X − E[X](cid:12)

(cid:12) ≤ α(cid:1) ≥ 1 − 2 exp

(cid:18)

−

2nα2
(b − a)2

(cid:19)

.

References

[sup4] K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton. Fast Two-Sample Testing with Analytic
Representations of Probability Measures. In Advances in Neural Information Processing Systems (NIPS), pages
1981–1989. 2015.

[sup15] W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gretton. Interpretable Distribution Features with Maximum

Testing Power. 2016. URL http://arxiv.org/abs/1605.06796.

[sup3] W. Rudin. Fourier analysis on groups. John Wiley & Sons, 2011.

[sup21] R. J. Serﬂing. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 2009.

23

6
1
0
2
 
t
c
O
 
5
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
8
7
4
0
.
0
1
6
1
:
v
i
X
r
a

An Adaptive Test of Independence with Analytic Kernel Embeddings

Wittawat Jitkrittum,1

Zoltán Szabó,2∗ Arthur Gretton1

1Gatsby Computational Neuroscience Unit, University College London
2Department of Applied Mathematics, CMAP, École Polytechnique

wittawat@gatsby.ucl.ac.uk
zoltan.szabo@polytechnique.edu
arthur.gretton@gmail.com

October 18, 2016

Abstract

A new computationally eﬃcient dependence measure, and
an adaptive statistical test of independence, are proposed.
The dependence measure is the diﬀerence between ana-
lytic embeddings of the joint distribution and the product
of the marginals, evaluated at a ﬁnite set of locations
(features). These features are chosen so as to maximize a
lower bound on the test power, resulting in a test that
is data-eﬃcient, and that runs in linear time (with re-
spect to the sample size n). The optimized features can
be interpreted as evidence to reject the null hypothesis,
indicating regions in the joint domain where the joint
distribution and the product of the marginals diﬀer most.
Consistency of the independence test is established, for
an appropriate choice of features. In real-world bench-
marks, independence tests using the optimized features
perform comparably to the state-of-the-art quadratic-
time HSIC test, and outperform competing O(n) and
O(n log n) tests.

1

Introduction

We consider the design of adaptive, nonparametric sta-
tistical tests of dependence: that is, tests of whether
a joint distribution Pxy factorizes into the product of
marginals PxPy. While classical tests of dependence,
such as Pearson’s correlation and Kendall’s τ , are able to
detect monotonic relations between univariate variables,
more modern tests can address complex interactions, for
instance changes in variance of X with the value of Y .
Key to many recent tests is to examine covariance or
correlation between data features. These interactions
become signiﬁcantly harder to detect, and the features

∗Zoltán

Szabó’s
0000-0001-6183-7603.

ORCID

ID:

http://orcid.org/

are more diﬃcult to design when the data reside in high
dimensions.

A basic nonlinear dependence measure is the Hilbert-
Schmidt Independence Criterion (HSIC), which is the
Hilbert-Schmidt norm of the covariance operator between
feature mappings of the random variables [Gretton et al.,
2005, 2008]. Each random variable X and Y is mapped to
a respective reproducing kernel Hilbert space Hk and Hl.
For suﬃciently rich mappings, the covariance operator
norm is zero if and only if the variables are indepen-
dent. A second basic nonlinear dependence measure is
the smoothed diﬀerence between the characteristic func-
tion of the joint distribution, and that of the product
of marginals. When a particular smoothing function is
used, the statistic corresponds to the covariance between
distances of X and Y variable pairs [Feuerverger, 1993,
Székely et al., 2007, Székely and Rizzo, 2009], yielding
a simple test statistic. It has been shown by Sejdinovic
et al. [2013] that the distance covariance (and its gener-
alization to semi-metrics) is an instance of HSIC for an
appropriate choice of kernels. A disadvantage of these
feature covariance statistics, however, is that they require
quadratic time to compute (besides in the special case of
the distance covariance with univariate real-valued vari-
ables, where Huo and Székely [2014] achieve an O(n log n)
cost). Moreover, the feature covariance statistics have
intractable null distributions, and either a permutation
approach or the solution of an expensive eigenvalue prob-
lem [e.g. Zhang et al., 2011] is required for consistent
estimation of the quantiles. Several approaches were
proposed by Zhang et al. [2016] to obtain faster tests
along the lines of HSIC. These include computing HSIC
on ﬁnite-dimensional feature mappings chosen as ran-
dom Fourier features (RFFs) [Rahimi and Recht, 2008], a
block-averaged statistic, and a Nyström approximation to
the statistic. Key to each of these approaches is a more
eﬃcient computation of the statistic and its threshold un-

1

der the null distribution: for RFFs, the null distribution
is a ﬁnite weighted sum of χ2 variables; for the block-
averaged statistic, the null distribution is asymptotically
normal; for Nyström, either a permutation approach is
employed, or the spectrum of the Nyström approximation
to the kernel matrix is used in approximating the null
distribution. Each of these methods costs signiﬁcantly
less than the O(n2) cost of the full HSIC (the cost is
linear in n, but also depends quadratically on the number
of features retained). A potential disadvantage of the
Nyström and Fourier approaches is that the features are
not optimized to maximize test power, but are chosen
randomly. The block statistic performs worse than both,
due to the large variance of the statistic under the null
(which can be mitigated by observing more data).

In addition to feature covariances, correlation measures
have also been developed in inﬁnite dimensional feature
spaces: in particular, Bach and Jordan [2002], Fukumizu
et al. [2008] proposed statistics on the correlation operator
in a reproducing kernel Hilbert space. While convergence
has been established for certain of these statistics, their
computational cost is high at O(n3), and test thresholds
have relied on permutation. A number of much faster
approaches to testing based on feature correlations have
been proposed, however. For instance, Dauxois and Nkiet
[1998] compute statistics of the correlation between ﬁnite
sets of basis functions, chosen for instance to be step func-
tions or low order B-splines. The cost of this approach is
O(n). This idea was extended by Lopez-Paz et al. [2013],
who computed the canonical correlation between ﬁnite
sets of basis functions chosen as random Fourier features;
in addition, they performed a copula transform on the
inputs, with a total cost of O(n log n). Finally, space
partitioning approaches have also been proposed, based
on statistics such as the KL divergence, however these
apply only to univariate variables [Heller et al., 2016], or
to multivariate variables of low dimension [Gretton and
Györﬁ, 2010] (that said, these tests have other advantages
of theoretical interest, notably distribution-independent
test thresholds).

The approach we take is most closely related to HSIC
on a ﬁnite set of features. Our simplest test statistic,
the Finite Set Independence Criterion (FSIC), is an av-
erage of covariances of analytic functions (i.e., features)
deﬁned on each of X and Y . A normalized version of
the statistic (NFSIC) yields a distribution-independent
asymptotic test threshold. We show that our test is con-
sistent, despite a ﬁnite number of analytic features being
used, via a generalization of arguments in Chwialkowski
et al. [2015]. As in recent work on two-sample testing
by Jitkrittum et al. [2016], our test is adaptive in the
sense that we choose our features on a held-out valida-
tion set to optimize a lower bound on the test power.
The design of features for independence testing turns out

to be quite diﬀerent to the case of two-sample testing,
however: the task is to ﬁnd correlated feature pairs on
the respective marginal domains, rather than attempting
to ﬁnd a single, high-dimensional feature representation
for the entire (x, y) (as we would need to do if we were
comparing distributions Pxy and Qxy, rather than testing
a speciﬁc property of Pxy). We demonstrate the perfor-
mance of our tests on several challenging artiﬁcial and
real-world datasets, including detection of dependence
between music and its year of appearance, and between
videos and captions. In these experiments, we outperform
competing linear and O(n log n) time tests.

2

Independence Criteria and Sta-
tistical Tests

We introduce two test statistics: ﬁrst, the Finite Set
Independence Criterion (FSIC), which builds on the prin-
ciple that dependence can be measured in terms of the
covariance between data features. Next, we propose a nor-
malized version of this statistic (NFSIC), with a simpler
asymptotic distribution when Pxy = PxPy. We show how
to select features for the latter statistic to maximize a
lower bound on the power of its corresponding statistical
test.

2.1 The Finite Set Independence Crite-

rion

We begin by introducing the Hilbert-Schmidt Indepen-
dence Criterion (HSIC) as proposed in Gretton et al.
[2005], since our unnormalized statistic is built along sim-
ilar lines. Consider two random variables X ∈ X ⊂ Rdx
and Y ∈ Y ⊂ Rdy . Denote by Pxy the joint distribution
between X and Y ; Px and Py are the marginal distribu-
tions of X and Y . Let ⊗ denote the tensor product, such
that (a ⊗ b) c = a (cid:104)b, c(cid:105). Assume that k : X × X → R
and l : Y × Y → R are positive deﬁnite kernels associated
with reproducing kernel Hilbert spaces (RKHS) Hk and
Hl, respectively. Let (cid:107) · (cid:107)HS be the norm on the space
of Hl → Hk Hilbert-Schmidt operators. Then, HSIC
between X and Y is deﬁned as
HSIC(X, Y ) = (cid:13)
(cid:13)
2
(cid:13)µxy − µx ⊗ µy
(cid:13)
HS
= E(x,y),(x(cid:48),y(cid:48)) [k(x, x(cid:48))l(y, y(cid:48))]

+ ExEx(cid:48)[k(x, x(cid:48))]EyEy(cid:48)[l(y, y(cid:48))]
− 2E(x,y) [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] ,

(1)

is an independent copy of x.

where Ex := Ex∼Px , Ey := Ey∼Py , E(x,y) := E(x,y)∼Pxy ,
and x(cid:48)
The mean
embedding of Pxy belongs to the space of Hilbert-
Schmidt operators from Hl to Hk, µxy := (cid:82)
X ×Y k(x, ·) ⊗
l(y, ·) dPxy(x, y) ∈ HS(Hl, Hk), and the marginal mean

2

embeddings are µx := (cid:82)
X k(x, ·) dPx(x) ∈ Hk and
µy := (cid:82)
Y l(y, ·) dPy(y) ∈ Hl [Smola et al., 2007]. Gret-
ton et al. [2005, Theorem 4] show that if the kernels k
and l are universal [Steinwart and Christmann, 2008] on
compact domains X and Y, then HSIC(X, Y ) = 0 if and
only if X and Y are independent. Alternatively, Gretton
[2015] shows that it is suﬃcient for each of k and l to be
characteristic to their respective domains (meaning that
distribution embeddings are injective in each marginal
domain: see Sriperumbudur et al. [2010]). Given a joint
sample Zn = {(xi, yi)}n
i=1 ∼ Pxy, an empirical estimator
of HSIC can be computed in O(n2) time by replacing the
population expectations in (1) with their corresponding
empirical expectations based on Zn.

(cid:80)J

We now propose our new linear-time dependence mea-
sure, the Finite Set Independence Criterion (FSIC). Let
X ⊂ Rdx and Y ⊂ Rdy be open sets. Deﬁne the empir-
ical measure ν := 1
i=1 δ(vi,wi) over J test locations
J
VJ := {(vi, wi)}J
i=1 ⊂ X × Y where δt denotes the Dirac
measure centered on t, and (vi, wi) are realizations from
an absolutely continuous distribution (wrt the Lebesgue
measure). Write Exy for E(x,y)∼Pxy . The idea is to
see µxy(v, w) = Exy[k(x, v)l(y, w)], µx(v) = Ex[k(x, v)]
and µy(w) = Ey[l(y, w)] as smooth functions, and con-
sider an L2(X × Y, ν) distance between µxy and µxµy
instead of a Hilbert-Schmidt distance as in HSIC [Gret-
ton et al., 2005]. Let µxµy(x, y) := µx(x)µy(y). FSIC is
deﬁned as

FSIC2(X, Y ) := (cid:107)µxy − µxµy(cid:107)2

L2(X ×Y,ν)

(µxy(x, y) − µx(x)µy(y))2 dν(x, y)

=

:=

(cid:90)

(cid:90)

X

1
J

Y
J
(cid:88)

i=1

u(vi, wi)2 =

(cid:107)u(cid:107)2

2, where

1
J

u(v, w) := µxy(v, w) − µx(v)µy(w)
= Exy[k(x, v)l(y, w)] − Ex[k(x, v)]Ey[l(y, w)],
= covxy[k(x, v), l(y, w)],

(2)

and u := (u(v1, w1), . . . , u(vJ , wJ ))(cid:62).

Our ﬁrst result in Proposition 2 states that FSIC(X, Y )
almost surely deﬁnes a dependence measure for the ran-
dom variables X and Y , provided that the product kernel
on the joint space X × Y is characteristic and analytic
(see Deﬁnition 1).

Deﬁnition 1 (Analytic kernels [Chwialkowski et al.,
2015]). Let X be an open set in Rd. A positive deﬁ-
nite kernel k : X × X → R is said to be analytic on its
domain X × X if for all v ∈ X , f (x) := k(x, v) is an
analytic function on X .

Assumption A. The kernels k : X × X → R and l :
Y × Y → R are bounded by Bk and Bl respectively, and

the product kernel g((x, y), (x(cid:48), y(cid:48))) := k(x, x(cid:48))l(y, y(cid:48)) is
characteristic [Sriperumbudur et al., 2010, Deﬁnition 6],
and analytic (Deﬁnition 1) on (X × Y) × (X × Y).

Proposition 2 (FSIC is a dependence measure). Assume
that
1. Assumption A holds.
2. The test locations VJ = {(vi, wi)}J

i=1 are drawn from

an absolutely continuous distribution.

Then, almost surely, FSIC(X, Y ) = 1√
J
only if X and Y are independent.

(cid:107)u(cid:107)2 = 0 if and

Proof. Since g is characteristic, the mean embedding map
Πg : P (cid:55)→ E(x,y)∼P [g((x, y), ·)] is injective [Sriperum-
budur et al., 2010, Section 3], where P is a probability
distribution on X × Y. Since g is analytic, by Lemma 10
(Appendix), µxy and µxµy are analytic functions. Thus,
Lemma 11 (Appendix, setting Λ = Πg) guarantees that
FSIC(X, Y ) = 0 ⇐⇒ Pxy = PxPy ⇐⇒ X and Y are
independent almost surely.

FSIC uses µxy as a proxy for Pxy, and µxµy as a proxy
for PxPy. Proposition 2 suggests that, to detect the
dependence between X and Y , it is suﬃcient to evaluate at
a ﬁnite number of locations (deﬁned by VJ ) the diﬀerence
of the population joint embedding µxy and the embedding
of the product of the marginal distributions µxµy. A brief
explanation to justify this property is as follows. If Pxy =
PxPy, then ρ(v, w) := µxy(v, w)−µxµy(v, w) is zero, and
FSIC(X, Y ) = 0 for any VJ . If Pxy (cid:54)= PxPy, then ρ will
not be a zero function, since the mean embedding map is
injective (require the product kernel to be characteristic).
Using the same argument as in Chwialkowski et al. [2015],
since k and l are analytic, ρ is also analytic, and the
set of roots R := {(v, w) | ρ(v, w) = 0} has Lebesgue
measure zero. Thus, it is suﬃcient to draw (v, w) from an
absolutely continuous distribution, as we are guaranteed
that (v, w) /∈ R giving FSIC(X, Y ) > 0.

For FSIC to be a dependence measure, the product
kernel is required to be characteristic and analytic. We
next show in Proposition 3 that Gaussian kernels k and l
yield such a product kernel.

3

is

=

(A

characteristic

product
and
exp (cid:0)−(x − x(cid:48))(cid:62)A(x − x(cid:48))(cid:1)

Gaussian
of
Proposition
analytic). Let
kernels
k(x, x(cid:48))
and
l(y, y(cid:48)) = exp (cid:0)−(y − y(cid:48))(cid:62)B(y − y(cid:48))(cid:1) be Gaussian
kernels on Rdx × Rdx and Rdy × Rdy respectively,
for positive deﬁnite matrices A and B.
Then,
g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteristic and
analytic on (Rdx × Rdy ) × (Rdx × Rdy ).
Proof (sketch). The main idea is to use the fact a Gaus-
sian kernel is analytic, and a product of Gaussian kernels
is a Gaussian kernel on the pair of variables. See the full
proof in Appendix D.

3

i=1

Plug-in Estimator We now give an empirical estima-
tor of FSIC. Assume that we observe a joint sample Zn :=
i.i.d.∼ Pxy. Unbiased estimators of µxy(v, w)
{(xi, yi)}n
(cid:80)n
and µxµy(v, w) are ˆµxy(v, w) := 1
i=1 k(xi, v)l(yi, w)
n
(cid:80)
and (cid:91)µxµy(v, w) :=
j(cid:54)=i k(xi, v)l(yj, w),
respectively. A straightforward empirical estimator of
FSIC2 is then given by

1
n(n−1)

(cid:80)n

i=1

(cid:92)FSIC2(Zn) =

ˆu(vi, wi)2,

1
J

J
(cid:88)

i=1

ˆu(v, w) := ˆµxy(v, w) − (cid:91)µxµy(v, w)

=

2
n(n − 1)

(cid:88)

i<j

h(v,w)((xi, yi), (xj, yj)),

(3)

(4)

J ˆu(cid:62) ˆu.

1
h(v,w)((x, y), (x(cid:48), y(cid:48)))
2 (k(x, v) −
where
k(x(cid:48), v))(l(y, w) − l(y(cid:48), w)).
For conciseness, we
deﬁne ˆu := (ˆu1, . . . , ˆuJ )(cid:62) ∈ RJ where ˆui := ˆu(vi, wi) so
that (cid:92)FSIC2(Zn) = 1

:=

(cid:92)FSIC2 can be eﬃciently computed in O((dx + dy)Jn)
time [see (3)], assuming that the runtime complexity of
evaluating k(x, v) is O(dx) and that of l(y, w) is O(dy).
The unbiasedness of (cid:91)µxµy is necessary for (4) to be a U-
statistic. This fact and the rewriting of (cid:92)FSIC2 in terms of
h(v,w)((x, y), (x(cid:48), y(cid:48))) will be exploited when the asymp-
totic distribution of ˆu is derived (Proposition 4).

Since FSIC satisﬁes FSIC(X, Y ) = 0 ⇐⇒ X ⊥ Y , in
principle its empirical estimator can be used as a test
statistic for an independence test proposing a null hy-
pothesis H0 : “X and Y are independent” against an
alternative H1 : “X and Y are dependent”. The null
distribution (i.e., distribution of the test statistic assum-
ing that H0 is true) is challenging to obtain, however
and depends on the unknown Pxy. This prompts us to
consider a normalized version of FSIC whose asymptotic
null distribution of a convenient form. We ﬁrst derive the
asymptotic distribution of ˆu in Proposition 4, which we
use to derive the normalized test statistic in Theorem 5.
As a shorthand, we write z := (x, y), and t := (v, w).

Proposition 4 (Asymptotic distribution of ˆu). Deﬁne
˜k(x, v) := k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) := l(y, w) −
Ey(cid:48)l(y(cid:48), w). Then, under both H0 and H1, for any ﬁxed
locations t and t(cid:48),
covz[ˆu(t), ˆu(t(cid:48))] n→∞−−−−→ covz[˜k(x, v)˜l(y, w), ˜k(x, v(cid:48))˜l(y, w(cid:48))]
= Exy[(cid:0)˜k(x, v)˜l(y, w) − u(t)(cid:1)(cid:0)˜k(x, v(cid:48))˜l(y, w(cid:48)) − u(t(cid:48))(cid:1)],

where u(v, w) is given in (2), and ˆu(v, w) is deﬁned
in (4). Second, if 0 < covz[ˆu(ti), ˆu(ti)] < ∞ for i =
n(ˆu − u) d→ N (0, Σ) as n → ∞, where
1, . . . , J, then
Σij = cov[ˆu(ti), ˆu(tj)] and u := (u(t1), . . . , u(tJ ))(cid:62).

√

Proof. We ﬁrst note that for a ﬁxed t = (v, w), ˆu(v, w) is
a one-sample second-order U-statistic [Serﬂing, 2009, Sec-
tion 5.1.3] with a U-statistic kernel ht where ht(a, b) =

ht(b, a). Thus, by Kowalski and Tu [2008, Section 5.1,
Theorem 1], it follows directly that cov[ˆu(t), ˆu(t(cid:48))] =
4covz[Eaht(z, a), Ebht(cid:48)(z, b)]. Substituting ht with its
deﬁnition yields the ﬁrst claim, where we note that
Exy[˜k(x, v)˜l(y, w)] = u(v, w).

For the second claim, since ˆu is a multivariate one-
sample U-statistic, by Lehmann [1999, Theorem 6.1.6]
and Kowalski and Tu [2008, Section 5.1, Theorem 1], it
n(ˆu − u) d→ N (0, Σ) as n → ∞, where

√

follows that
Σij = cov[ˆu(ti), ˆu(tj)].

Recall from Proposition 2 that u = 0 holds almost
surely under H0. The asymptotic normality in the second
claim of Proposition 4 implies that n(cid:92)FSIC2 = n
J ˆu(cid:62) ˆu con-
verges in distribution to a sum of J dependent weighted
χ2 random variables. The dependence comes from the
fact that the coordinates ˆu1 . . . , ˆuJ of ˆu all depend on
the sample Zn. This null distribution is not analytically
tractable, and requires a large number of simulations to
compute the rejection threshold Tα for a given signiﬁcance
value α.

2.2 Normalized FSIC and Adaptive Test

For the purpose of an independence test, we will consider
a normalized variant of (cid:92)FSIC2, which we call (cid:92)NFSIC2,
whose tractable asymptotic null distribution is χ2(J), the
chi-squared distribution with J degrees of freedom. We
then show that the independence test deﬁned by (cid:92)NFSIC2
is consistent. These results are given in Theorem 5.
Theorem 5 (Independence test using (cid:92)NFSIC2 is con-
sistent). Let ˆΣ be a consistent estimate of Σ based on
the joint sample Zn. The (cid:92)NFSIC2 statistic is deﬁned as
ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu where γn ≥ 0 is a regulariza-
tion parameter. Assume that

(cid:17)−1

1. Assumption A holds.

2. Σ is invertible almost surely with respect to VJ =
i=1 drawn from an absolutely continuous

{(vi, wi)}J
distribution.

3. limn→∞ γn = 0.
Then, for any k, l and VJ satisfying the assumptions,
1. Under H0, ˆλn
2. Under H1, for any r ∈ R, limn→∞ P

= 1
almost surely. That is, the independence test based on
(cid:92)NFSIC2 is consistent.

d→ χ2(J) as n → ∞.

(cid:16)ˆλn ≥ r

(cid:17)

Proof (sketch) . Under H0, nˆu(cid:62)( ˆΣ + γnI)−1 ˆu asymptot-
ically follows χ2(J) because
nˆu is asymptotically nor-
mally distributed (see Proposition 4). Claim 2 builds on
the result in Proposition 2 stating that u (cid:54)= 0 under H1;

√

4

it follows using the convergence of ˆu to u. The full proof
can be found in Appendix E.

Theorem 5 states that if H1 holds, the statistic can
be arbitrarily large as n increases, allowing H0 to be
rejected for any ﬁxed threshold. Asymptotically the test
threshold Tα is given by the (1 − α)-quantile of χ2(J) and
is independent of n. The assumption on the consistency
of ˆΣ is required to obtain the asymptotic chi-squared
distribution. The regularization parameter γn is to ensure
that ( ˆΣ + γnI)−1 can be stably computed. In practice,
γn requires no tuning, and can be set to be a very small
constant.

The next proposition states that the computational
complexity of the (cid:92)NFSIC2 estimator is linear in both
the input dimension and sample size, and that it can
be expressed in terms of the K =[K ij] = [k(vi, xj)] ∈
RJ×n, L = [Lij] = [l(wi, yj)] ∈ RJ×n matrices.

Proposition 6 (An empirical estimator of (cid:92)NFSIC2). Let
1n := (1, . . . , 1)(cid:62) ∈ Rn. Denote by ◦ the element-wise
matrix product. Then,
1. ˆu = (K◦L)1n

n−1 − (K1n)◦(L1n)

n(n−1)

.

2. A consistent estimator for Σ is ˆΣ = ΓΓ(cid:62)

n where

Γ := (K − n−1K1n1(cid:62)
n ) ◦ (L − n−1L1n1(cid:62)
ˆub = n−1 (K ◦ L) 1n − n−2 (K1n) ◦ (L1n) .

n ) − ˆub1(cid:62)
n ,

Assume that the complexity of the kernel evaluation is
linear in the input dimension. Then the test statistic
ˆλn = nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu can be computed in O(J 3 +
J 2n + (dx + dy)Jn) time.

(cid:17)−1

Proof (sketch). Claim 1 for ˆu is straightforward. The
expression for ˆΣ in claim 2 follows directly from the
asymptotic covariance expression in Proposition 4. The
consistency of ˆΣ can be obtained by noting that the
ﬁnite sample bound for P((cid:107) ˆΣ − Σ(cid:107)F > t) decreases as
n increases. This is implicitly shown in Appendix F.2.2
and its following sections.

Although the dependency of the estimator on J is cubic,
we empirically observe that only a small value of J is
required (see Section 3). The number of test locations
J relates to the number of regions in X × Y of pxy and
pxpy that diﬀer (see Figure 1). In particular, J need not
increase with n for test consistency.

Our ﬁnal theoretical result gives a lower bound on
the test power of (cid:92)NFSIC2 i.e., the probability of correctly
rejecting H0. We will use this lower bound as the objective
function to determine VJ and the kernel parameters. Let
(cid:107) · (cid:107)F be the Frobenius norm.

Theorem 7 (A lower bound on the test power). Let
NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let K be a kernel
class for k, L be a kernel class for l, and V be a collection
with each element being a set of J locations. Assume that
that
1. There
and

ﬁnite Bk
supk∈K supx,x(cid:48)∈X |k(x, x(cid:48))|
supl∈L supy,y(cid:48)∈Y |l(y, y(cid:48))| ≤ Bl.

and Bl
≤

such
Bk

exist

2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)Σ−1(cid:107)F < ∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test
power satisﬁes P

≥ L(λn) where

(cid:16)ˆλn ≥ r

(cid:17)

L(λn) = 1 − 62e−ξ1γ2

− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

n(λn−r)2/n − 2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

1

2JB2,
(cid:98)·(cid:99) is the ﬂoor function, ξ1 :=
B := BkBl, ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 :=
28B4J 2c2
J ˜c, and B∗ is
a constant depending on only Bk and Bl. Moreover, for
suﬃciently large ﬁxed n, L(λn) is increasing in λn.

1J 2B∗ , ξ2 := 72c2

1, c1 := 4B2J

J ˜c, c2 := 4B

32c2

√

√

(cid:17)

(cid:17)

(cid:16)

(cid:111)

(cid:110)

y,u]

y ∈ [σ2

x ∈ [σ2

y,l < σ2

(x, v) (cid:55)→ exp

(y, w) (cid:55)→ exp

the kernels k and l,

− (cid:107)x−v(cid:107)2
2σ2
x
x,l < σ2
| σ2

in Appendix F. To put
We provide the proof
let θx and θy be the
Theorem 7 into perspective,
parameters of
respectively.
We denote by θ = {θx, θy, VJ } the collection of
Assume that
all tuning parameters of the test.
x,l, σ2
| σ2
x,u]
K =
=:
Kg for some 0 < σ2
x,u < ∞ and L =
(cid:111)
(cid:110)
(cid:16)
− (cid:107)y−w(cid:107)2
y,l, σ2
=: Lg for
2σ2
y
some 0 < σ2
y,u < ∞ are Gaussian kernel classes.
Then, in Theorem 7, B = Bk = Bl = 1, and B∗ = 2.
The assumption ˜c < ∞ is a technical condition to guar-
antee that the test power lower bound is ﬁnite for all
θ deﬁned by the feasible sets K, L, and V. Let V(cid:15),r :=
(cid:8)VJ | (cid:107)vi(cid:107)2, (cid:107)wi(cid:107)2 ≤ r and (cid:107)vi − vj(cid:107)2
2 ≥
(cid:15), for all i (cid:54)= j(cid:9). If we set K = Kg, L = Lg, and V = V(cid:15),r
for some (cid:15), r > 0, then ˜c < ∞ as Kg, Lg, and V(cid:15),r are
compact. In practice, these conditions do not necessarily
create restrictions as they almost always hold implicitly.
We show in Appendix C that the objective function used
to choose VJ will discourage any two locations to be in
the same neighborhood.

2 + (cid:107)wi − wj(cid:107)2

Parameter Tuning The test power lower bound
L(λn) in Theorem 7 is a function of λn = nu(cid:62)Σ−1u
which is the population counterpart of the test statistic
ˆλn. As in FSIC, it can be shown that λn = 0 if and
only if X are Y are independent (from Proposition 2).
If X and Y are dependent, then λn > 0. According to
Theorem 7, for a suﬃciently large n, the test power lower
bound is increasing in λn. One can therefore think of
λn (a function of θ) as representing how easily the test
rejects H0 given a problem Pxy. The higher the λn, the

5

3 Experiments

In this section, we empirically study the performance
of the proposed method on both toy (Section 3.1) and
real-life problems (Section 3.2). Our interest is in the
performance of linear-time tests on challenging problems
which require a large sample size to be able to accurately
reveal the dependence. All the code is available at https:
//github.com/wittawatj/fsic-test.

We compare the proposed NFSIC with optimization
(NFSIC-opt) to ﬁve multivariate nonparametric tests.
The (cid:92)NFSIC2 test without optimization (NFSIC-med) acts
as a baseline, allowing the eﬀect of parameter optimization
to be clearly seen. For pedagogical reason, we consider
the original HSIC test of Gretton et al. [2005] denoted by
QHSIC, which is a quadratic-time test. Nyström HSIC
(NyHSIC) uses a Nyström approximation to the kernel
matrices of X and Y when computing the HSIC statistic.
FHSIC is another variant of HSIC in which a random
Fourier feature approximation [Rahimi and Recht, 2008]
to the kernel is used. NyHSIC and FHSIC are studied in
Zhang et al. [2016] and can be computed in O(n), with
quadratic dependency on the number of inducing points
in NyHSIC, and quadratic dependency in the number
of random features in FHSIC. Finally, the Randomized
Dependence Coeﬃcient (RDC) proposed in Lopez-Paz
et al. [2013] is also considered. The RDC can be seen as
the primal form (with random Fourier features) of the
kernel canonical correlation analysis of Bach and Jordan
[2002] on copula-transformed data. We consider RDC
as a linear-time test even though preprocessing by an
empirical copula transform costs O((dx + dy)n log n).

We use Gaussian kernel classes Kg and Lg for both
X and Y in all the methods. Except NFSIC-opt, all
other tests use full sample to conduct the indepen-
dence test, where the Gaussian widths σx and σy are
set according to the widely used median heuristic i.e.,
σx = median ({(cid:107)xi − xj(cid:107)2 | 1 ≤ i < j ≤ n}), and σy is
set in the same way using {yi}n
i=1. The J locations for
NFSIC-med are randomly drawn from the standard multi-
variate normal distribution in each trial. For a sample of
size n, NFSIC-opt uses half the sample for parameter tun-
ing, and the other disjoint half for the test. We permute
the sample 300 times in RDC1 and HSIC to simulate
from the null distribution and compute the test threshold.
The null distributions for FHSIC and NyHSIC are given
by a ﬁnite sum of weighted χ2(1) random variables given
in Eq. 8 of Zhang et al. [2016]. Unless stated otherwise,
we set the test threshold of the two NFSIC tests to be the
(1 − α)-quantile of χ2(J). To provide a fair comparison,
we set J = 10, use 10 inducing points in NyHSIC, and 10

1We use a permutation test for RDC, following the authors’
(https://github.com/lopezpaz/randomized_

implementation
dependence_coefficient, referred commit: b0ac6c0).

(a) ˆµxy(v, w)

(b) (cid:92)µxµy(v, w)

(c) (cid:98)Σ(v, w)

(d) Statistic ˆλn(v, w)

Figure 1: Illustration of (cid:92)NFSIC2.

greater the lower bound on the test power, and thus the
more likely it is that the test will reject H0 when it is
false.

In light of this reasoning, we propose setting θ to θ∗ =
arg maxθ λn. That this procedure is also valid under H0
can be seen as follows. Under H0, θ∗ = arg maxθ 0 will be
d→ χ2(J)
arbitrary. Since Theorem 7 guarantees that ˆλn
as n → ∞ for any θ, the asymptotic null distribution does
not change by using θ∗. In practice, λn is a population
quantity which is unknown. We propose dividing the
sample Zn into two disjoint sets: training and test sets.
The training set is used to optimize for θ∗, and the test set
is used for the actual independence test with the optimized
θ∗. The splitting is to guarantee the independence of θ∗
and the test sample, which is an assumption of Theorem
5.

we

To

better

under (cid:92)NFSIC2,
visualize
ˆµxy(v, w), (cid:91)µxµy(v, w) and ˆΣ(v, w) as a function of one
In this
test location (v, w) on a simple toy problem.
problem, Y = −X + Z where Z ∼ N (0, 0.32). As we con-
sider only one location (J = 1), ˆΣ(v, w) is a scalar. The
(ˆµxy(v,w)−(cid:92)µxµy(v,w))2
statistic can be written as ˆλn = n
.
ˆΣ(v,w)
These components are shown in Figure 1, where we use
Gaussian kernels for both X and Y , and the horizontal
and vertical axes correspond to v ∈ R and w ∈ R,
respectively.

Intuitively, ˆu(v, w) = ˆµxy(v, w) − (cid:91)µxµy(v, w) captures
the diﬀerence of the joint distribution and the product of
the marginals as a function of (v, w). Squaring ˆu(v, w)
and dividing it by the variance shown in Figure 1c gives
the statistic (also the parameter tuning objective) shown
in Figure 1d. The latter ﬁgure suggests that the parameter
tuning objective function can be non-convex. However,
we note that the non-convexity arises since there are
multiple ways to detect the diﬀerence between the joint
distribution and the product of the marginals. In this case,
the lower left and upper right regions equally indicate the
largest diﬀerence.

6

(a) SG (α = 0.05)

(b) SG (α = 0.05)

(c) Sin

(d) GSign

Figure 2: (a): Runtime. (b): Probability of rejecting H0 as problem parameters vary. Fix n = 4000.

random Fourier features in FHSIC and RDC.

Optimization of NFSIC-opt The parameters of
NFSIC-opt are σx, σy, and J locations of size (dx +
dy)J. We treat all the parameters as a long vector in
R2+(dx+dy)J and use gradient ascent to optimize ˆλn/2.
We observe that initializing VJ by randomly picking J
points from the training sample yields good performance.
The regularization parameter γn in NFSIC is ﬁxed to a
small value, and is not optimized. It is worth emphasizing
that the complexity of the optimization procedure is still
linear in n.

Since FSIC, NyHFSIC and RDC rely on a ﬁnite-
dimensional kernel approximation, these tests are consis-
tent only if both the number of features increases with n.
By constrast, the proposed NFSIC requires only n to go
to inﬁnity to achieve consistency i.e., J can be ﬁxed. We
refer the reader to Appendix C for a brief investigation
of the test power vs. increasing J. The test power does
not necessarily monotonically increase with J.

3.1 Toy Problems

We consider three toy problems: Same Gaussian (SG),
Sinusoid (Sin), and Gaussian Sign (GSign).

1. Same Gaussian (SG). The two variables are inde-
pendently drawn from the standard multivariate normal
distribution i.e., X ∼ N (0, Idx ) and Y ∼ N (0, Idy ) where
Id is the d × d identity matrix. This problem represents
a case in which H0 holds.

2. Sinusoid (Sin). Let pxy be the probability density
of Pxy.
In the Sinusoid problem, the dependency of
X and Y is characterized by (X, Y ) ∼ pxy(x, y) ∝ 1 +
sin(ωx) sin(ωy), where the domains of X , Y = (−π, π)
and ω is the frequency of the sinusoid. As the frequency
ω increases, the drawn sample becomes more similar to
a sample drawn from Uniform((−π, π)2). That is, the
higher ω, the harder to detect the dependency between
X and Y . This problem was studied in Sejdinovic et al.
[2013]. Plots of the density for a few values of ω are
shown in Figures 6 and 7 in the appendix. The main
characteristic of interest in this problem is the local change
in the density function.

3. Gaussian Sign (GSign). In this problem, Y =

|Z| (cid:81)dx
i=1 sgn(Xi), where X ∼ N (0, Idx ), sgn(·) is the sign
function, and Z ∼ N (0, 1) serves as a source of noise. The
full interaction of X = (X1, . . . , Xdx ) is what makes the
problem challenging. That is, Y is dependent on X, yet
it is independent of any proper subset of {X1, . . . , Xd}.
Thus, simultaneous consideration of all the coordinates
of X is required to successfully detect the dependency.

We ﬁx n = 4000 and vary the problem parameters.
Each problem is repeated for 300 trials, and the sample is
redrawn each time. The signiﬁcance level α is set to 0.05.
The results are shown in Figure 2. It can be seen that
in the SG problem (Figure 2b) where H0 holds, all the
tests achieve roughly correct type-I errors at α = 0.05.
In the Sin problem, NFSIC-opt achieves the highest test
power for all considered ω = 1, . . . , 6, highlighting its
strength in detecting local changes in the joint density.
The performance of NFSIC-med is signiﬁcantly lower than
that of NFSIC-opt. This phenomenon clearly emphasizes
the importance of the optimization to place the locations
at the relevant regions in X × Y. RDC has a remarkably
high performance in both Sin and GSign (Figure 2c, 2d)
despite no parameter tuning. Interestingly, both NFSIC-
opt and RDC outperform the quadratic-time QHSIC
in these two problems. The ability to simultaneously
consider interacting features of NFSIC-opt is indicated
by its superior test power in GSign, especially at the
challenging settings of dx = 5, 6. An average trial runtime
for each test in the SG problem is shown in Figure 2a. We
observe that the runtime does not increase with dimension,
as the complexity of all the tests is linear in the dimension
of the input. All the tests are implemented in Python
using a common SciPy Stack.

To investigate the sample eﬃciency of all the tests,
we ﬁx dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in
GSign, and increase n. Figure 3 shows the results. The
quadratic dependency on n in QHSIC makes it infeasible
both in terms of memory and runtime to consider n larger
than 6000 (Figure 3a). In constrast, although not the
most time-eﬃcient, NFSIC-opt has the highest sample-
eﬃciency for GSign, and for Sin in the low-sample regime,
signiﬁcantly outperforming QHSIC. Despite the small
additional overhead from the optimization, we are yet
able to conduct an accurate test with n = 105, dx = dy =

7

(a) SG. dx = dy = 250.

(b) SG. dx = dy = 250.

(c) Sin. ω = 4.

(d) GSign. dx = 4.

Figure 3: (a) Runtime. (b): Probability of rejecting H0 as n increases in the toy problems.

250 in less than 100 seconds. We observe in Figure 3b
that the two NFSIC variants have correct type-I errors
across all sample sizes, indicating that the asymptotic null
distribution approximately holds by the time n reaches
1000. We recall from Theorem 5 that the NFSIC test
with random test locations will asymptotically reject H0
if it is false. A demonstration of this property is given in
Figure 3c, where the test power of NFSIC-med eventually
reaches 1 with n higher than 105.

3.2 Real Problems

We now examine the performance of our proposed test
on real problems.

Million Song Data (MSD) We consider a subset of
the Million Song Data2 [Bertin-Mahieux et al., 2011], in
which each song (X) out of 515,345 is represented by 90
features, of which 12 features are timbre average (over
all segments) of the song, and 78 features are timbre
covariance. Most of the songs are western commercial
tracks from 1922 to 2011. The goal is to detect the
dependency between each song and its year of release
(Y ). We set α = 0.01, and repeat for 300 trials where
the full sample is randomly subsampled to n points in
each trial. Other settings are the same as in the toy
problems. To make sure that the type-I error is correct,
we use the permutation approach in the NFSIC tests to
compute the threshold. Figure 4b shows the test powers
as n increases from 500 to 2000. To simulate the case
where H0 holds in the problem, we permute the sample
to break the dependency of X and Y . The results are
shown in Figure 5 in the appendix.

Evidently, NFSIC-opt has the highest test power among
all the linear-time tests for all the sample sizes. Its test
power is second to only QHSIC. We recall that NFSIC-opt
uses half of the sample for parameter tuning. Thus, at
n = 500, the actual sample for testing is 250, which is
relatively small. The fact that there is a vast power gain
from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500
suggests that the optimization procedure can perform
well even at a lower sample sizes.

(a) MSD problem.

(b) Videos & Captions problem.
Figure 4: Probability of rejecting H0 as n increases in
the two real problems. α = 0.01.

Videos and Captions Our last problem is based on
the VideoStory46K3 dataset [Habibian et al., 2014]. The
dataset contains 45,826 Youtube videos (X) of an average
length of roughly one minute, and their corresponding
text captions (Y ) uploaded by the users. Each video is
represented as a dx = 2000 dimensional Fisher vector en-
coding of motion boundary histograms (MBH) descriptors
of Wang and Schmid [2013]. Each caption is represented
as a bag of words with each feature being the frequency
of one word. After ﬁltering only words which occur in at
least six video captions, we obtain dy = 1878 words. We
examine the test powers as n increases from 2000 to 8000.
The results are given in Figure 4. The problem is suﬃ-
ciently challenging that all linear-time tests achieve a low
power at n = 2000. QHSIC performs exceptionally well
on this problem, achieving a maximum power throughout.
NFSIC-opt has the highest sample eﬃciency among the
linear-time tests, showing that the optimization procedure
is also practical in a high dimensional setting.

Acknowledgement

We thank the Gatsby Charitable Foundation for the ﬁ-
nancial support. The major part of this work was carried
out while Zoltán Szabó was a research associate at the
Gatsby Computational Neuroscience Unit, University Col-
lege London.

2Million Song Data subset: https://archive.ics.uci.edu/ml/

3VideoStory46K dataset:

https://ivi.fnwi.uva.nl/isis/

datasets/YearPredictionMSD.

mediamill/datasets/videostory.php.

8

References

T. W. Anderson. An Introduction to Multivariate Statis-

tical Analysis. Wiley, 2003.

F. R. Bach and M. I. Jordan. Kernel independent compo-
nent analysis. Journal of Machine Learning Research,
3:1–48, 2002.

T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and
P. Lamere. The million song dataset. In International
Conference on Music Information Retrieval (ISMIR),
2011.

K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and
A. Gretton. Fast Two-Sample Testing with Analytic
Representations of Probability Measures. In Advances
in Neural Information Processing Systems (NIPS),
pages 1981–1989. 2015.

X. Huo and G. J. Székely. Fast computing for distance
covariance. Technical report, 2014. URL https://
arxiv.org/abs/1410.1503.

W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gret-
ton. Interpretable Distribution Features with Maximum
Testing Power. 2016. URL http://arxiv.org/abs/
1605.06796.

J. Kowalski and X. M. Tu. Modern Applied U-Statistics.

John Wiley & Sons, 2008.

E. L. Lehmann. Elements of Large-Sample Theory.

Springer Science & Business Media, 1999.

D. Lopez-Paz, P. Hennig, and B. Schölkopf. The Random-
ized Dependence Coeﬃcient. In Advances in Neural
Information Processing Systems (NIPS), pages 1–9.
2013.

J. Dauxois and G. M. Nkiet. Nonlinear canonical analysis
and independence tests. The Annals of Statistics, 26
(4):1254–1278, 1998.

A. Rahimi and B. Recht. Random features for large-scale
kernel machines. In Advances in Neural Information
Processing Systems (NIPS), pages 1177–1184. 2008.

A. Feuerverger. A consistent test for bivariate dependence.
International Statistical Review, 61(3):419–433, 1993.

K. Fukumizu, A. Gretton, X. Sun, and B. Schölkopf. Ker-
nel measures of conditional dependence. In Advances in
Neural Information Processing Systems (NIPS), pages
489–496, 2008.

A. Gretton. A simpler condition for consistency of a
kernel independence test. Technical report, 2015. URL
http://arxiv.org/abs/1501.06103.

A. Gretton and L. Györﬁ. Consistent nonparametric
tests of independence. Journal of Machine Learning
Research, 11:1391–1423, 2010.

A. Gretton, O. Bousquet, A. Smola, and B. Schölkopf.
Measuring Statistical Dependence with Hilbert-
Schmidt Norms.
In Algorithmic Learning Theory
(ALT), pages 63–77. 2005.

A. Gretton, K. Fukumizu, C. H. Teo, L. Song,
B. Schölkopf, and A. J. Smola. A Kernel Statistical Test
of Independence. In Advances in Neural Information
Processing Systems (NIPS), pages 585–592. 2008.

A. Habibian, T. Mensink, and C. G. Snoek. Videostory:
A new multimedia embedding for few-example recogni-
tion and translation of events. In ACM International
Conference on Multimedia, pages 17–26, 2014.

R. Heller, Y. Heller, S. Kaufman, B. Brill, and M. Gorﬁne.
Consistent distribution-free k-sample and independence
tests for univariate random variables. Journal of Ma-
chine Learning Research, 17(29):1–54, 2016.

D. Sejdinovic, B. Sriperumbudur, A. Gretton, and
K. Fukumizu. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. The Annals of
Statistics, 41(5):2263–2291, 2013.

R. J. Serﬂing. Approximation Theorems of Mathematical

Statistics. John Wiley & Sons, 2009.

A. Smola, A. Gretton, L. Song, and B. Schölkopf. A
hilbert space embedding for distributions. In Inter-
national Conference on Algorithmic Learning Theory
(ALT), pages 13–31, 2007.

B. K. Sriperumbudur, A. Gretton, K. Fukumizu,
B. Schölkopf, and G. R. G. Lanckriet. Hilbert Space Em-
beddings and Metrics on Probability Measures. Journal
of Machine Learning Research, 11:1517–1561, 2010.

I. Steinwart and A. Christmann. Support vector machines.

Springer Science & Business Media, 2008.

G. J. Székely and M. L. Rizzo. Brownian distance covari-
ance. The Annals of Applied Statistics, 3(4):1236–1265,
2009.

G. J. Székely, M. L. Rizzo, and N. K. Bakirov. Measuring
and testing dependence by correlation of distances. The
Annals of Statistics, 35(6):2769–2794, 2007.

A. W. v. d. Vaart. Asymptotic Statistics. Cambridge

University Press, 2000.

H. Wang and C. Schmid. Action recognition with im-
proved trajectories. In IEEE International Conference
on Computer Vision (ICCV), pages 3551–3558, 2013.

9

K. Zhang, J. Peters, D. Janzing, B., and B. Schölkopf.
Kernel-based conditional independence test and appli-
cation in causal discovery. In Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pages 804–813, 2011.

Q. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. Large-
Scale Kernel Methods for Independence Testing. 2016.
URL http://arxiv.org/abs/1606.07892.

10

An Adaptive Test of Independence with Analytic Kernel Embeddings
Supplementary Material

A Type-I Errors

In this section, we show that all the tests have correct type-I errors (i.e., the probability of reject H0 when it is true)
in real problems. We permute the joint sample so that the dependency is broken to simulate cases in which H0
holds. The results are shown in Figure 5.

(a) MSD problem (permuted).

(a) Videos & Captions problem with shuf-
ﬂed sample.

Figure 5: Probability of rejecting H0 as n increases in the Million Song problem. α = 0.01.

B Redundant Test Locations

Here, we provide a simple illustration to show that two locations t1 = (v1, w1) and t2 = (v2, w2) which are too
close to each other will reduce the optimization objective. We consider the Sinusoid problem described in Section
3.1 with ω = 1, and use J = 2 test locations. In Figure 6, t1 is ﬁxed at the red star, while t2 is varied along the
horizontal line. The objective value ˆλn as a function of (t1, t2) is shown in the bottom ﬁgure. It can be seen that ˆλn
decreases sharply when t2 is in the neighborhood of t1. This property implies that two locations which are too close
will not maximize the objective function (i.e., the second feature contains no additional information when it matches
the ﬁrst). For J > 2, the objective sharply decreases if any two locations are in the same neighborhood.

Figure 6: Plot of optimization objective values as location t2 moves along the green line. The objective sharply
drops when the two locations are in the same neighborhood.

C Test Power vs. J

It might seem intuitive that as the number of locations J increases, the test power should also increase. Here, we
empirically show that this statement is not always true. Consider the Sinusoid toy example described in Section 3.1
with ω = 2 (also see the left ﬁgure of Figure 7). By construction, X and Y are dependent in this problem. We run

11

NFSIC test with a sample size of n = 800, varying J from 1 to 600. For each value of J, the test is repeated for 500
times. In each trial, the sample is redrawn and the J test locations are drawn from Uniform((−π, π)2). There is no
optimization of the test locations. We use Gaussian kernels for both X and Y , and use the median heuristic to set
the Gaussian widths to 1.8. Figure 7 shows the test power as J increases.

Figure 7: The Sinusoid problem and the plot of test power vs. the number of test locations.

We observe that the test power does not monotonically increase as J increases. When J = 1, the diﬀerence of pxy
and pxpy cannot be adequately captured, resulting in a low power. The power increases rapidly to roughly 0.8 at
J = 10, and stays at the maximum until about J = 100. Then, the power starts to drop sharply when J is higher
than 400 in this problem.

Unlike random Fourier features, the number of test locations in NFSIC is not the number of Monte Carlo particles
used to approximate an expectation. There is a tradeoﬀ: if the test locations are in key regions (i.e., regions in
which there is a big diﬀerence between pxy and pxpy), then they increase power; yet the statistic gains in variance
(thus reducing test power) as J increases. As can be seen in Figure 7, there are eight key regions (in blue) that can
reveal the diﬀerence of pxy and pxpy. Using an unnecessarily high J not only makes the covariance matrix ˆΣ harder
to estimate accurately, it also increases the computation as the complexity on J is O(J 3).

We note that NFSIC is not intended to be used with a large J. In practice, it should be set to be large enough so
as to capture the key regions as stated. As a practical guide, with optimization of the test locations, a good starting
point is J = 5 or 10.

D Proof of Proposition 3

Recall Proposition 3,
Proposition (A product of Gaussian kernels is characteristic and analytic). Let k(x, x(cid:48)) = exp (cid:0)−(x − x(cid:48))(cid:62)A(x − x(cid:48))(cid:1)
and l(y, y(cid:48)) = exp (cid:0)−(y − y(cid:48))(cid:62)B(y − y(cid:48))(cid:1) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for
positive deﬁnite matrices A and B. Then, g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteristic and analytic on
(Rdx × Rdy ) × (Rdx × Rdy ).

Proof. Let z := (x(cid:62), y(cid:62))(cid:62) and z(cid:48) := (x(cid:48)(cid:62), y(cid:48)(cid:62))(cid:62) be vectors in Rdx+dy . We prove by reducing the product kernel to
one Gaussian kernel with g(z, z(cid:48)) = exp (cid:0)−(z − z(cid:48))(cid:62)C(z − z(cid:48))(cid:1) where C :=
. Write g(z, z(cid:48)) = Ψ(z − z(cid:48))
where Ψ(t) := exp (cid:0)−t(cid:62)Ct(cid:1). Since C is positive deﬁnite, we see that the ﬁnite measure ζ corresponding to Ψ as
deﬁned in Lemma 12 has support everywhere in Rdx+dy . Thus, Sriperumbudur et al. [2010, Theorem 9] implies that
g is characteristic.

(cid:18) A 0
0 B

(cid:19)

To see that g is analytic, we observe that for each z(cid:48) ∈ Rdx+dy , z (cid:55)→ −(z − z(cid:48))(cid:62)C(z − z(cid:48)) is a multivariate
polynomial in z, which is known to be analytic. Using the fact that t (cid:55)→ exp(t) is analytic on R, and that a
composition of analytic functions is analytic, we see that z (cid:55)→ exp (cid:0)−(z − z(cid:48))(cid:62)C(z − z(cid:48))(cid:1) is analytic on Rdx+dy for
each z(cid:48).

E Proof of Theorem 5

Recall Theorem 5,

12

Theorem 5 (Independence test using (cid:92)NFSIC2 is consistent). Let ˆΣ be a consistent estimate of Σ based on the
joint sample Zn. The (cid:92)NFSIC2 statistic is deﬁned as ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
ˆu where γn ≥ 0 is a regularization
parameter. Assume that

(cid:17)−1

1. Assumption A holds.
2. Σ is invertible almost surely with respect to VJ = {(vi, wi)}J

i=1 drawn from an absolutely continuous distribution.

3. limn→∞ γn = 0.

Then, for any k, l and VJ satisfying the assumptions,
1. Under H0, ˆλn

d→ χ2(J) as n → ∞.

2. Under H1, for any r ∈ R, limn→∞ P

(cid:16)ˆλn ≥ r

(cid:17)

(cid:92)NFSIC2 is consistent.

= 1 almost surely. That is, the independence test based on

(cid:17)−1 p
Proof. Assume that H0 holds. The consistency of ˆΣ and the continuous mapping theorem imply that
→
Σ−1 which is a constant. Let a be a random vector in RJ following N (0, Σ). By Vaart [2000, Theorem 2.7 (v)], it
nˆu d→ N (0, Σ)

d→ (cid:2)a, Σ−1(cid:3) where u = 0 almost surely by Proposition 2, and

(cid:16) ˆΣ + γnI

follows that

(cid:17)−1(cid:21)

(cid:16) ˆΣ + γnI

(cid:20)√

nˆu,

√

by Proposition 4. Since f (x, S) := x(cid:62)Sx is continuous, f
nˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

ˆu d→ a(cid:62)Σ−1a ∼ χ2(J) by Anderson [2003, Theorem 3.3.3]. This proves the ﬁrst claim.

(cid:18)√

(cid:16) ˆΣ + γnI

nˆu,

(cid:17)−1(cid:19)

d→ f (a, Σ−1). Equivalently,

The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al.
[2015]. Assume that H1 holds. Then, u (cid:54)= 0 almost surely by Proposition 2. Since k and l are bounded, it follows that
|ht(z, z(cid:48))| ≤ 2BkBl for any z, z(cid:48) (see (8)), and we have that ˆu a.s.→ u by Serﬂing [2009, Section 5.4, Theorem A]. Thus,
ˆu(cid:62) (cid:16) ˆΣ + γnI
d→ u(cid:62)Σ−1u by the continuous mapping theorem, and the consistency of ˆΣ. Consequently,

(cid:17)−1

ˆu − r
n

P

lim
n→∞

(cid:17)

(cid:16)ˆλn ≥ r
(cid:18)

= 1 − lim
n→∞

P

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

ˆu −

< 0

(cid:19)

r
n

(a)

= 1 − P (cid:0)u(cid:62)Σ−1u < 0(cid:1) (b)

= 1,

where at (a) we use the Portmanteau theorem [Vaart, 2000, Lemma 2.2 (i)] guaranteeing that xn
→ x if and only if
P(xn < t) → P(x < t) for all continuity points of t (cid:55)→ P(x < t). Step (b) is justiﬁed by noting that the covariance
matrix Σ is positive deﬁnite so that u(cid:62)Σ−1u > 0, and t (cid:55)→ P(u(cid:62)Σ−1u < t) (a step function) is continuous at 0.

d

F Proof of Theorem 7

Recall Theorem 7,

Theorem 7 (A lower bound on the test power). Let NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let K be a kernel class
for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that
1. There exist ﬁnite Bk and Bl such that supk∈K supx,x(cid:48)∈X |k(x, x(cid:48))| ≤ Bk and supl∈L supy,y(cid:48)∈Y |l(y, y(cid:48))| ≤ Bl.
2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)Σ−1(cid:107)F < ∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test power satisﬁes P

≥ L(λn) where

(cid:16)ˆλn ≥ r

(cid:17)

L(λn) = 1 − 62e−ξ1γ2

− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

n(λn−r)2/n − 2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

13

(cid:98)·(cid:99) is the ﬂoor function, ξ1 :=
c1 := 4B2J
ﬁxed n, L(λn) is increasing in λn.

J ˜c, c2 := 4B

√

√

1

32c2

1J 2B∗ , ξ2 := 72c2

2JB2, B := BkBl, ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 := 28B4J 2c2
1,
J ˜c, and B∗ is a constant depending on only Bk and Bl. Moreover, for suﬃciently large

Overview of the proof We ﬁrst derive a probabilistic bound for |ˆλn − λn|/n. The bound is in turn upper
bounded by an expression involving (cid:107)ˆu − u(cid:107)2 and (cid:107) ˆΣ − Σ(cid:107)F . The diﬀerence (cid:107)ˆu − u(cid:107)2 can be bounded by applying
the bound for U-statistics given in Serﬂing [2009, Theorem A, p. 201]. For (cid:107) ˆΣ − Σ(cid:107)F , we decompose it into a sum
of smaller components, and bound each term with a product variant of the Hoeﬀding’s inequality (Lemma 9). L(λn)
is obtained by combining all the bounds with the union bound.

F.1 Notations
Let (cid:104)A, B(cid:105)F := tr(A(cid:62)B) denote the Frobenius inner product, and (cid:107)A(cid:107)F := (cid:112)tr(A(cid:62)A) be the Frobenius norm.
Write z := (x, y) to denote a pair of points from X × Y. We write t := (v, w) to denote a pair of test locations
from X × Y. For brevity, an expectation over (x, y) (i.e., E(x,y)∼Pxy ) will be written as Ez or Exy. Deﬁne
˜k(x, v) := k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) := l(y, w) − Ey(cid:48)l(y(cid:48), w). Let B2(r) := {x | (cid:107)x(cid:107)2 ≤ r} be a closed ball
with radius r centered at the origin. Similarly, deﬁne BF (r) := {A | (cid:107)A(cid:107)F ≤ r} to be a closed ball with radius r of
J × J matrices under the Frobenius norm. Denote the max operation by (x1, . . . , xm)+ = max(x1, . . . , xm).
we write (cid:91)µxµy(v, w)

j(cid:54)=i k(xi, v)l(yj, w) to denote the unbiased plug-in estimator, and write ˆµx(v)ˆµy(w)

:=
:=
j=1 l(yj, w) which is a biased estimator. Deﬁne ˆub(v, w) := ˆµxy(v, w) − ˆµx(v)ˆµy(w)
where the superscript b stands for “biased”. To avoid confusing with a positive

i=1
i=1 k(xi, v) 1
n

For
1
n(n−1)
(cid:80)n
1
n

of marginal mean

so that ˆub := (cid:0)ˆub(t1), . . . , ˆub(tJ )(cid:1)(cid:62)
deﬁnite kernel, we will refer to a U-statistic kernel as a core.

product
(cid:80)

µx(v)µy(w),

embeddings

a
(cid:80)n

(cid:80)n

F.2 Proof
We will ﬁrst derive a bound for P(|ˆλn − λn| ≥ t), which will then be reparametrized to get a bound for the target
quantity P(ˆλn ≥ r). We closely follow the proof in Jitkrittum et al. [2016, Section C.1] up to (12), then we diverge.
We start by considering |ˆλn − λn|/n.

|ˆλn − λn|/n =

(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:17)−1
ˆu(cid:62) (cid:16) ˆΣ + γnI
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

=

≤

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u + u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)

+

:= ((cid:70))1 + ((cid:70))2 .

We next bound ((cid:70)1) and ((cid:70)2) separately.

((cid:70))1 =

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

(cid:12)
(cid:12)
ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

≤

ˆu(cid:62) (cid:16) ˆΣ + γnI

(cid:17)−1

(cid:17)−1

ˆu(cid:62) (cid:16) ˆΣ + γnI
(cid:28)

(cid:12)
(cid:12)
ˆu − ˆu(cid:62) (Σ + γnI)−1 ˆu + ˆu(cid:62) (Σ + γnI)−1 ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)ˆu(cid:62) (Σ + γnI)−1 ˆu − u(cid:62) (Σ + γnI)−1 u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:68)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
ˆu − ˆu(cid:62) (Σ + γnI)−1 ˆu
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:29)

=

(cid:17)−1

ˆuˆu(cid:62),

(cid:16) ˆΣ + γnI

− (Σ + γnI)−1

(cid:12)
(cid:12)
(cid:12)
F
≤ (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1 − (Σ + γnI)−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1(cid:107)F
= (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1[(Σ + γnI) − ( ˆΣ + γnI)](Σ + γnI)−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − ˆuu(cid:62) + ˆuu(cid:62) − uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1(cid:107)F

ˆuˆu(cid:62) − uu(cid:62), (Σ + γnI)−1(cid:69)

+

F

14

(a)
≤ (cid:107)ˆuˆu(cid:62)(cid:107)F (cid:107)( ˆΣ + γnI)−1(cid:107)F (cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + (cid:107)ˆuˆu(cid:62) − ˆuu(cid:62) + ˆuu(cid:62) − uu(cid:62)(cid:107)F (cid:107)Σ−1(cid:107)F
(b)
≤

2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + (cid:0)(cid:107)ˆu(ˆu − u)(cid:62)(cid:107)F + (cid:107)(ˆu − u)u(cid:62)(cid:107)F

(cid:1) (cid:107)Σ−1(cid:107)F

(cid:107)ˆu(cid:107)2

√

≤

(cid:107)ˆu(cid:107)2

2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + ((cid:107)ˆu(cid:107)2 + (cid:107)u(cid:107)2) (cid:107)ˆu − u(cid:107)2(cid:107)Σ−1(cid:107)F ,

J
γn
√
J
γn

where at (a) we used (cid:107)(Σ + γnI)−1(cid:107)F ≤ (cid:107)Σ−1(cid:107)F , at (b) we used (cid:107)( ˆΣ + γnI)−1(cid:107)F ≤

J(cid:107)( ˆΣ + γnI)−1(cid:107)2 ≤

J/γn.

For ((cid:70))2, we have

(cid:12)
(cid:12)
(cid:12)u(cid:62) (Σ + γnI)−1 u − u(cid:62)Σ−1u
((cid:70))2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (cid:12)
(cid:10)uu(cid:62), (Σ + γnI)−1 − Σ−1(cid:11)
(cid:12)
(cid:12)
≤ (cid:107)uu(cid:62)(cid:107)F (cid:107)(Σ + γnI)−1 − Σ−1(cid:107)F
= (cid:107)u(cid:107)2
≤ γn(cid:107)u(cid:107)2
(a)
≤ γn(cid:107)u(cid:107)2

2(cid:107)(Σ + γnI)−1(cid:107)F (cid:107)Σ−1(cid:107)F

2(cid:107)Σ−1(cid:107)2
F ,

F

2(cid:107)(Σ + γnI)−1 [Σ − (Σ + γnI)] Σ−1(cid:107)F

where at (a) we used (cid:107)(Σ + γnI)−1(cid:107)F ≤ (cid:107)Σ−1(cid:107)F .

Combining (5) and (6), we have

(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)

√

J
γn

≤

(cid:107)ˆu(cid:107)2(cid:107)Σ − ˆΣ(cid:107)F (cid:107)Σ−1(cid:107)F + ((cid:107)ˆu(cid:107)2 + (cid:107)u(cid:107)2) (cid:107)ˆu − u(cid:107)2(cid:107)Σ−1(cid:107)F + γn(cid:107)u(cid:107)2

2(cid:107)Σ−1(cid:107)2
F .

(7)

2 and (cid:107)u(cid:107)2

Bounding (cid:107)ˆu(cid:107)2
2 is
bounded. Recall that supx,x(cid:48)∈X |k(x, x(cid:48))| ≤ Bk, supy,y(cid:48) |l(y, y(cid:48))| ≤ Bl, our notation t = (v, w) for the test locations,
and zi := (xi, yi). We ﬁrst show that the U-statistic core h is bounded.

2 Here, we show that by the boundedness of the kernels k and l, it follows that (cid:107)ˆu(cid:107)2

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we deﬁne B := BkBl. It follows that

|ht((x, y), (x(cid:48), y(cid:48)))| =

(k(x, v) − k(x(cid:48), v))(l(y, w) − l(y(cid:48), w))

(cid:12)
1
(cid:12)
(cid:12)
2
(cid:12)
1
2

≤

(|k(x, v)| + |k(x(cid:48), v)|) (|l(y, w)| + |l(y(cid:48), w)|)

≤ 2BkBl := 2B,

(cid:107)ˆu(cid:107)2

2 =

htm (zi, zj)



≤

[2BkBl]2 = 4B2J,

2
n(n − 1)

(cid:88)

i<j


2

J
(cid:88)

m=1

(cid:107)u(cid:107)2

2 =

[EzEz(cid:48)htm(z, z(cid:48))]2 ≤ 4B2J.

Using the upper bounds on (cid:107)ˆu(cid:107)2

2, (cid:107)u(cid:107)2

2 ,(7) and the deﬁnition of ˜c, we have
(cid:12)
(cid:12)
(cid:12)ˆu(cid:62)( ˆΣ + γnI)−1 ˆu − u(cid:62)Σ−1u
(cid:12)
(cid:12)
(cid:12)
√

√

4B2J ˜c(cid:107)Σ − ˆΣ(cid:107)F + 4B

J ˜c(cid:107)ˆu − u(cid:107)2 + 4B2J ˜c2γn

(cid:107)Σ − ˆΣ(cid:107)F + c2(cid:107)ˆu − u(cid:107)2 + c3γn,

15





J
(cid:88)

m=1

J
(cid:88)

m=1

≤

=:

J
γn
c1
γn

(5)

√

(6)

(8)

(9)

(10)

(11)

where we deﬁne c1 := 4B2J

J ˜c, c2 := 4B

√

√

|ˆλn − λn| ≤

J ˜c, and c3 := 4B2J ˜c2. This upper bound implies that
c1
γn

n(cid:107)Σ − ˆΣ(cid:107)F + c2n(cid:107)ˆu − u(cid:107)2 + c3nγn.

(12)

We will separately upper bound (cid:107)Σ − ˆΣ(cid:107)F and (cid:107)ˆu − u(cid:107)2, and combine them with a union bound.

F.2.1 Bounding (cid:107)ˆu − u(cid:107)2
Let t∗ = arg maxt∈{t1,...,tJ } |ˆu(t) − u(t)|. Recall that u = (u(t1), . . . , u(tJ ))(cid:62) = (u1, . . . , uJ )(cid:62).

(cid:107)ˆu − u(cid:107)2 = sup

(cid:104)b, ˆu − u(cid:105)2 ≤ sup

|bj||ˆu(tj) − u(tj)|

b∈B2(1)

b∈B2(1)

j=1

J
(cid:88)

≤ |ˆu(t∗) − u(t∗)|

sup
b∈B2(1)

J
(cid:88)

j=1

|bj|

√

(a)
≤

√

J|ˆu(t∗) − u(t∗)|

sup
b∈B2(1)

(cid:107)b(cid:107)2

=

J|ˆu(t∗) − u(t∗)|,

J(cid:107)a(cid:107)2 for any a ∈ RJ . From (13), it can be seen that bounding (cid:107)ˆu − u(cid:107)2 amounts to
where at (a) we used (cid:107)a(cid:107)1 ≤
bounding the diﬀerence of a U-statistic ˆu(t∗) (see (4)) to its expectation u(t∗). Combining (13) and (12), we have

√

|ˆλn − λn| ≤

n(cid:107)Σ − ˆΣ(cid:107)F + c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

√

c1
γn

F.2.2 Bounding (cid:107) ˆΣ − Σ(cid:107)F
The plan is to write ˆΣ = ˆS − ˆub ˆub(cid:62), Σ = S − uu(cid:62), so that (cid:107) ˆΣ − Σ(cid:107)F ≤ (cid:107)ˆS − S(cid:107)F + (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F and bound
separately (cid:107)ˆS − S(cid:107)F and (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

Recall that Σij = η(ti, tj), η(t, t(cid:48)) = Exy[(cid:0)˜k(x, v)˜l(y, w) − u(v, w)(cid:1)(cid:0)˜k(x, v(cid:48))˜l(y, w(cid:48)) − u(v(cid:48), w(cid:48))(cid:1)] where ˜k(x, v) =
k(x, v) − Ex(cid:48)k(x(cid:48), v), and ˜l(y, w) = l(y, w) − Ey(cid:48)l(y(cid:48), w). Its empirical estimator (see Proposition 6) is ˆΣij = ˆη(ti, tj)
where

ˆη(t, t(cid:48)) =

[(cid:0)k(xi, v)l(yi, w) − ˆub(v, w)(cid:1)(cid:0)k(xi, v(cid:48))l(yi, w(cid:48)) − ˆub(v(cid:48), w(cid:48))(cid:1)]

=

k(xi, v)l(yi, w)k(xi, v(cid:48))l(yi, w(cid:48)) − ˆub(v, w)ˆub(v(cid:48), w(cid:48)),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:80)n

:=

that

k(x, v) − 1
and
n
i=1 k(xi, v)l(yi, w) = ˆub(v, w).

We
:=
m=1 k(xm, vi)l(ym, wi)k(xm, vj)l(yi, wj), and deﬁne similarly its population counterpart S such that

k(x, v)
note
1
n
Sij := Exy[˜k(x, v)˜l(y, w)˜k(x, v(cid:48))˜l(y, w(cid:48))]. We have

l(y, w) − 1
n
ˆS
RJ×J

i=1 l(yi, w).
such that

i=1 k(xi, v),

We deﬁne

l(y, w)

ˆSij

(cid:80)n

(cid:80)n

:=

1
n

∈

(cid:80)n

ˆΣ = ˆS − ˆub ˆub(cid:62),
Σ = S − uu(cid:62),

(cid:107) ˆΣ − Σ(cid:107)F = (cid:107)ˆS − S − (ˆub ˆub(cid:62) − uu(cid:62))(cid:107)F

≤ (cid:107)ˆS − S(cid:107)F + (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

With (16), (14) becomes

|ˆλn − λn| ≤

(cid:107)ˆS − S(cid:107)F +

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F + c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1n
γn

c1n
γn

√

We will further separately bound (cid:107)ˆS − S(cid:107)F and (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F .

16

(13)

(14)

(15)

(16)

(17)

F.2.3 Bounding (cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F = (cid:107)ˆub ˆub(cid:62) − ˆubu(cid:62) + ˆubu(cid:62) − uu(cid:62)(cid:107)F
≤ (cid:107)ˆub(ˆub − u)(cid:62)(cid:107)F + (cid:107)(ˆub − u)u(cid:62)(cid:107)F
= (cid:107)ˆub(cid:107)2(cid:107)ˆub − u(cid:107)2 + (cid:107)ˆub − u(cid:107)2(cid:107)u(cid:107)2

√

≤ 4B

J(cid:107)ˆub − u(cid:107)2,

√

where we used (10) and the fact that (cid:107)ˆub(cid:107)2 ≤ 2B

J which can be shown similarly to (9) as

(cid:107)ˆub(cid:107)2

2 =

[ˆµxy(vm, wm) − ˆµx(vm)ˆµy(wm)]2 =

htm (zi, zj)



≤

[2BkBl]2 = 4B2J.

J
(cid:88)

n
(cid:88)

n
(cid:88)





1
n2

m=1

i=1

j=1


2

J
(cid:88)

m=1

J
(cid:88)

m=1

Let (˜v, ˜w) := ˜t = arg maxt∈{t1,...,tJ } |ˆub(t) − u(t)|. We bound (cid:107)ˆub − u(cid:107)2 by

(cid:107)ˆub − u(cid:107)2

√

(a)
≤

√

√

√

√

J|ˆub(˜t) − u(˜t)|
(cid:12)ˆµxy(˜t) − ˆµx(˜v)ˆµy( ˜w) − u(˜t)(cid:12)
J (cid:12)
(cid:12)
(cid:12)ˆµxy(˜t) − (cid:91)µxµy(˜t) + (cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w) − u(˜t)(cid:12)
J (cid:12)
(cid:12)
(cid:12)ˆµxy(˜t) − (cid:91)µxµy(˜t) − u(˜t)(cid:12)
J (cid:12)
(cid:12) +
(cid:12)ˆu(˜t) − u(˜t)(cid:12)
J (cid:12)
(cid:12) +

J (cid:12)
(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12)
(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12) ,

J (cid:12)

√

√

=

=

≤

=

where at (a) we used the same reasoning as in (13). The bias (cid:12)
bounded as

(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)

(cid:12) in the second term can be

k(xi, ˜v)l(yj, ˜w) −

k(xi, ˜v)l(yi, ˜w) −

k(xi, ˜v)l(yj, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:18)

(cid:12)(cid:91)µxµy(˜t) − ˆµx(˜v)ˆµy( ˜w)(cid:12)
(cid:12)
(cid:12)

=

=

1
n(n − 1)

1
n(n − 1)

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

n
(cid:88)

n
(cid:88)

i=1

j=1

n
n − 1

(cid:19) 1
n2

n
n − 1

(cid:19) 1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1

j=1

≤

1 −

≤

B
n − 1

+

B
n − 1

=

2B
n − 1

.

k(xi, ˜v)l(yj, ˜w) −

k(xi, ˜v)l(yj, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

1
n(n − 1)

n
(cid:88)

i=1

1
n2

n
(cid:88)

n
(cid:88)

j=1

1
n(n − 1)

i=1
(cid:12)
(cid:12)
(cid:12)
k(xi, ˜v)l(yi, ˜w)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
k(xi, ˜v)l(yj, ˜w)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(n − 1)

n
(cid:88)

i=1

k(xi, ˜v)l(yi, ˜w)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

1 −

k(xi, ˜v)l(yj, ˜w) +

(18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Combining this upper bound with (18), we have

(cid:107)ˆub ˆub(cid:62) − uu(cid:62)(cid:107)F ≤ 4BJ (cid:12)

(cid:12)ˆu(˜t) − u(˜t)(cid:12)

(cid:12) +

(19)

With (19), (17) becomes

|ˆλn − λn| ≤

(cid:107)ˆS − S(cid:107)F +

c1n
γn

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12) +

c1n
γn

8B2J
n − 1

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

(20)

8B2J
n − 1

.

√

17

F.2.4 Bounding (cid:107)ˆS − S(cid:107)F
Recall that VJ = {t1, . . . , tJ }, ˆSij = ˆS(ti, tj) = 1
n
S(ti, tj) = Exy[˜k(x, vi)˜l(y, wi)˜k(x, vj)˜l(y, wj)]. Let (t(1), t(2)) = arg max(s,t)∈VJ ×VJ | ˆS(s, t) − S(s, t)|.

m=1 k(xm, vi)l(ym, wi)k(xm, vj)l(ym, wj), and Sij =

(cid:80)n

(cid:107)ˆS − S(cid:107)F = sup

B∈BF (1)

(cid:68)

(cid:69)
B, ˆS − S

F

J
(cid:88)

J
(cid:88)

≤ sup

B∈BF (1)

i=1

j=1

|Bij|| ˆSij − Sij|

≤

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12)

sup
B∈BF (1)

J
(cid:88)

J
(cid:88)

i=1

j=1

|Bij|

(a)
≤ J

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12)

sup
B∈BF (1)

(cid:107)B(cid:107)F

= J

ˆS(t(1), t(2)) − S(t(1), t(2))

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ,

where at (a) we used (cid:80)J

(cid:80)J

i=1

j=1 |Aij| ≤ J(cid:107)A(cid:107)F for any matrix A ∈ RJ×J . We arrive at

|ˆλn − λn| ≤

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12) +

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)
(cid:12)

√

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1Jn
γn
c1n
γn

(cid:12)
(cid:12)
(cid:12)
8B2J
n − 1

+

F.2.5 Bounding

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12)

Having an upper bound for
will deﬁne the following shorthands.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12) will allow us to bound (22). To keep the notations uncluttered, we

(21)

(22)

Expression

Shorthand

Expression

Shorthand

k(x, v)

k(x, v(cid:48))

k(xi, v)
k(xi, v(cid:48))
Ex∼Px k(x, v)
Ex∼Px k(x, v(cid:48))
(cid:80)n
1
i=1 k(xi, v)
n
1
i=1 k(xi, v(cid:48))
n

(cid:80)n

a

a(cid:48)

ai
a(cid:48)
i
˜a

˜a(cid:48)

a
a(cid:48)

l(y, w)

l(y, w(cid:48))

l(yi, w)
l(yi, w(cid:48))
Ey∼Py l(y, w)
Ey∼Py l(y, w(cid:48))
(cid:80)n
1
i=1 l(yi, w)
n
i=1 l(yi, w(cid:48))

(cid:80)n

1
n

b

b(cid:48)

bi
b(cid:48)
i
˜b
˜b(cid:48)

b
(cid:48)

b

We will also use · to denote a empirical expectation over x, or y, or (x, y). The argument under · will
i=1 k(xi, v)k(xi, v(cid:48)) and
i=1 k(xi, v)l(yi, w)k(xi, v(cid:48)), and so on. We deﬁne in the same way for the population expectation using

determine the variable over which we take the expectation. For instance, aa(cid:48) = 1
n
aba(cid:48) = 1
n
(cid:101)· i.e., (cid:102)aa(cid:48) = Ex [k(x, v)k(x, v(cid:48))] and (cid:103)aba(cid:48) = Exy [k(x, v)l(y, w)k(x, v(cid:48))].

(cid:80)n

(cid:80)n

With these shorthands, we can rewrite ˆS(t, t(cid:48)) and S(t, t(cid:48)) as

ˆS(t, t(cid:48)) =

(ai − a)(bi − b)(a(cid:48)

i − a(cid:48))(b(cid:48)

(cid:48)
i − b

),

1
n

n
(cid:88)

i=1

18

By expanding S(t, t(cid:48)), we have

S(t, t(cid:48)) = Exy

(cid:104)

(cid:105)
(a − ˜a)(b − ˜b)(a(cid:48) − ˜a(cid:48))(b(cid:48) − ˜b(cid:48))

.

S(t, t(cid:48)) = Exy

(cid:2) + aba(cid:48)b(cid:48) − aba(cid:48)˜b(cid:48) − ab˜a(cid:48)b(cid:48) + ab˜a(cid:48)˜b(cid:48)
− a˜ba(cid:48)b(cid:48) + a˜ba(cid:48)˜b(cid:48) + a˜b˜a(cid:48)b(cid:48) − a˜b˜a(cid:48)˜b(cid:48)
− ˜aba(cid:48)b(cid:48) + ˜aba(cid:48)˜b(cid:48) + ˜ab˜a(cid:48)b(cid:48) − ˜ab˜a(cid:48)˜b(cid:48)
+ ˜a˜ba(cid:48)b(cid:48) − ˜a˜ba(cid:48)˜b(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48) + ˜a˜b˜a(cid:48)˜b(cid:48)(cid:3)

= +(cid:94)aba(cid:48)b(cid:48) − (cid:103)aba(cid:48)˜b(cid:48) − (cid:103)abb(cid:48)˜a(cid:48) + (cid:101)ab˜a(cid:48)˜b(cid:48)
− (cid:93)aa(cid:48)b(cid:48)˜b + (cid:102)aa(cid:48)˜b˜b(cid:48) + (cid:102)ab(cid:48)˜a(cid:48)˜b − ˜a˜b˜a(cid:48)˜b(cid:48)
− (cid:103)a(cid:48)bb(cid:48)˜a + (cid:102)a(cid:48)b˜a˜b(cid:48) + ˜a˜a(cid:48) (cid:102)bb(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48)
+ (cid:103)a(cid:48)b(cid:48)˜a˜b − ˜a˜b˜a(cid:48)˜b(cid:48) − ˜a˜b˜a(cid:48)˜b(cid:48) + ˜a˜b˜a(cid:48)˜b(cid:48)
= +(cid:94)aba(cid:48)b(cid:48) − (cid:103)aba(cid:48)˜b(cid:48) − (cid:103)abb(cid:48)˜a(cid:48) + (cid:101)ab˜a(cid:48)˜b(cid:48)
− (cid:93)aa(cid:48)b(cid:48)˜b + (cid:102)aa(cid:48)˜b˜b(cid:48) + (cid:102)ab(cid:48)˜a(cid:48)˜b + (cid:103)a(cid:48)b(cid:48)˜a˜b
− (cid:103)a(cid:48)bb(cid:48)˜a + (cid:102)a(cid:48)b˜a˜b(cid:48) + ˜a˜a(cid:48) (cid:102)bb(cid:48) − 3˜a˜b˜a(cid:48)˜b(cid:48).

The expansion of ˆS(t, t(cid:48)) can be done in the same way. By the triangle inequality, we have

(cid:12)
(cid:12)
(cid:12)

ˆS(t, t(cid:48)) − S(t, t(cid:48))

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)aa(cid:48)b(cid:48) b − (cid:93)aa(cid:48)b(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bb(cid:48)a − (cid:103)a(cid:48)bb(cid:48)˜a
(cid:12) +

(cid:48)

(cid:12)
(cid:12)
(cid:12)aba(cid:48) b
(cid:12)
(cid:12)
(cid:12)aa(cid:48) b b
(cid:12)
(cid:12)
(cid:12)a(cid:48)bab

(cid:48)

(cid:48)

(cid:12)
(cid:12)

(cid:12)abb(cid:48)a(cid:48) − (cid:103)abb(cid:48)˜a(cid:48)(cid:12)
− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12) +
(cid:12)
(cid:12)
− (cid:102)aa(cid:48)˜b˜b(cid:48)(cid:12)
(cid:12)ab(cid:48)a(cid:48)b − (cid:102)ab(cid:48)˜a(cid:48)˜b
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12) +
(cid:12)
(cid:12)
− (cid:102)a(cid:48)b˜a˜b(cid:48)(cid:12)
(cid:12)a a(cid:48)bb(cid:48) − ˜a˜a(cid:48) (cid:102)bb(cid:48)
(cid:12)
(cid:12)
(cid:12)
(cid:12) + 3
(cid:12) +

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)aba(cid:48)b
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a(cid:48)b(cid:48)ab − (cid:103)a(cid:48)b(cid:48)˜a˜b
(cid:12)
(cid:12)
(cid:12)
− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12)aba(cid:48)b
(cid:12)
(cid:12)
(cid:12) .

(cid:48)

(cid:48)

The ﬁrst term
by applying Lemma 9. Recall that we write (x1, . . . , xm)+ for max(x1, . . . , xm).

(cid:12)
(cid:12)
(cid:12) can be bounded by applying the Hoeﬀding’s inequality. Other terms can be bounded

(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

Bounding
have

(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

(cid:12)
(cid:12)
(cid:12) (1st term). Since −B2 ≤ aba(cid:48)b(cid:48) ≤ B2, by the Hoeﬀding’s inequality (Lemma 14), we

P

(cid:16)(cid:12)
(cid:12)aba(cid:48)b(cid:48) − (cid:94)aba(cid:48)b(cid:48)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 2 exp

−

(cid:18)

(cid:19)

.

nt2
2B4

Bounding
We note that |f1(x, y)| ≤ (BBk, Bl)+ and |f2(y)| ≤ (BBk, Bl)+. Thus, by Lemma 9 with E = 2, we have

− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (2nd term). Let f1(x, y) = aba(cid:48) = k(x, v)l(y, w)k(x, v(cid:48)) and f2(y) = b(cid:48) = l(y, w(cid:48)).

(cid:12)
(cid:12)
(cid:12)aba(cid:48) b

(cid:48)

P

(cid:16)(cid:12)
(cid:12)
(cid:12)aba(cid:48) b

(cid:48)

− (cid:103)aba(cid:48)˜b(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t

≥ 1 − 4 exp

−

(cid:18)

nt2
8(BBk, Bl)4
+

(cid:19)

.

Bounding
b(cid:48) = l(y, w(cid:48)). We can see that |f1(x, y)|, |f2(x)|, |f3(y)| ≤ (B, Bk, Bl)+. Thus, by Lemma 9 with E = 3, we have

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (4th term). Let f1(x, y) = ab = k(x, v)l(y, w), f2(x) = a(cid:48) = k(x, v(cid:48)) and f3(y) =

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b

(cid:48)

P

(cid:16)(cid:12)
(cid:12)aba(cid:48)b
(cid:12)

(cid:48)

− (cid:101)ab˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 6 exp

−

(cid:18)

nt2
18(B, Bk, Bl)6
+

(cid:19)

.

19

(cid:48)

(cid:12)
(cid:12)
(cid:12)aba(cid:48)b

− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) (last term). Let f1(x) = a = k(x, v), f2(y) = b = l(y, w), f3(x) = a(cid:48) = k(x, v(cid:48)) and
Bounding
f4(y) = b(cid:48) = l(y, w(cid:48)). It can be seen that |f1(x)|, |f2(y)|, |f3(x)|, |f4(y)| ≤ (Bk, Bl)+. Thus, by Lemma 9 with
E = 4, we have

(cid:16)
3

P

(cid:12)
(cid:12)aba(cid:48)b
(cid:12)

(cid:48)

− ˜a˜b˜a(cid:48)˜b(cid:48)(cid:12)
(cid:12)
(cid:12) ≤ t

(cid:17)

≥ 1 − 8 exp

−

(cid:18)

nt2
32 · 32(Bk, Bl)8
+

(cid:19)

.

Bounds for other terms can be derived in a similar way to yield

(3rd term) P

(5th term) P

(6th term) P

(7th term) P

(8th term) P

(9th term) P

(10th term) P

(11th term) P

(cid:48)

(cid:17)

(cid:16)(cid:12)
(cid:12)

(cid:12)abb(cid:48)a(cid:48) − (cid:103)abb(cid:48)˜a(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t
(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)aa(cid:48)b(cid:48) b − (cid:93)aa(cid:48)b(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
− (cid:102)aa(cid:48)˜b˜b(cid:48)(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)aa(cid:48) b b
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:12)ab(cid:48)a(cid:48)b − (cid:102)ab(cid:48)˜a(cid:48)˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:12)a(cid:48)b(cid:48)ab − (cid:103)a(cid:48)b(cid:48)˜a˜b
(cid:12)
(cid:12)
(cid:12) ≤ t
(cid:16)(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bb(cid:48)a − (cid:103)a(cid:48)bb(cid:48)˜a
(cid:12) ≤ t
(cid:16)(cid:12)
− (cid:102)a(cid:48)b˜a˜b(cid:48)(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)a(cid:48)bab
(cid:12) ≤ t
(cid:12)
(cid:17)
(cid:12)
(cid:12) ≤ t

(cid:16)(cid:12)
(cid:12)a a(cid:48)bb(cid:48) − ˜a˜a(cid:48) (cid:102)bb(cid:48)
(cid:12)

(cid:17)

(cid:48)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

≥ 1 − 4 exp

−

≥ 1 − 4 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

≥ 1 − 4 exp

−

≥ 1 − 6 exp

−

≥ 1 − 6 exp

−

,

,

,

(cid:19)

(cid:19)

18(B2

nt2
8(BBl, Bk)4
+
nt2
8(BBk, Bl)4
+
(cid:19)
nt2
k, Bl)6
+
nt2
18(B, Bk, Bl)6
+
nt2
18(B, Bk, Bl)6
+
(cid:19)
nt2
8(BBl, Bk)4
+
nt2
18(B, Bk, Bl)6
+
(cid:19)
nt2
18(Bk, B2

l )6
+

.

(cid:19)

(cid:19)

,

(cid:19)

,

,

,

By the union bound, we have

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

ˆS(t, t(cid:48)) − S(t, t(cid:48))

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ 12t
(cid:19)

(cid:18)

≥ 1 −

2 exp

−

+ 4 exp

−

nt2
2B4

(cid:18)

(cid:19)

(cid:19)

(cid:19)

nt2
18(B, Bk, Bl)6
+
(cid:18)

+ 6 exp

−

(cid:18)

+ 8 exp

−

(cid:19)

nt2
18(B, Bk, Bl)6
+
nt2
32 · 32(Bk, Bl)8
+
(cid:19)

(cid:19) (cid:21)

(cid:19)

(cid:18)

+ 24 exp

−

nt2
18(B, Bk, Bl)6
+

(cid:19)

(cid:18)

+ 4 exp

−

(cid:19)

nt2
8(BBl, Bk)4
+

+ 6 exp

−

= 1 −

2 exp

−

+ 8 exp

−

+ 8 exp

−

(cid:20)

(cid:20)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

4 exp

−

4 exp

−

(cid:18)

+ 6 exp

−

(cid:19)

(cid:19)

nt2
8(BBk, Bl)4
+
nt2
8(BBl, Bk)4
+
(cid:19)
nt2
2B4

nt2
18(B2
k, Bl)6
+
(cid:19)

nt2
8(BBk, Bl)4
+
(cid:18)

+ 6 exp

−

(cid:18)

+ 6 exp

−

(cid:19)

18(B2

nt2
k, Bl)6
+
nt2
18(B, Bk, Bl)6
+

(cid:18)

(cid:19)

(cid:19)

nt2
8(BBk, Bl)4
+

(cid:18)

+ 6 exp

−

nt2
18(Bk, B2
l )6
+
(cid:18)
(cid:19)

+ 8 exp

−

+ 8 exp

(cid:18)

+ 8 exp

−

(cid:19)

+ 6 exp

122nt2
B∗
122nt2
B∗

−

(cid:18)

(cid:19)

(cid:20)

(cid:18)

≥ 1 −

2 exp

−

+ 6 exp

(cid:18)

= 1 − 62 exp

−

(cid:18)

122nt2
B∗
122nt2
B∗
122nt2
B∗

−

(cid:19)

,

nt2
18(B, Bk, Bl)6
+
(cid:18)

nt2
18(Bk, B2

l )6
+

(cid:18)

+ 6 exp

−

(cid:19)

(cid:18)

(cid:19)

+ 6 exp

−

nt2
8(BBl, Bk)4
+
(cid:18)

+ 8 exp

−

(cid:19) (cid:21)

nt2
32 · 32(Bk, Bl)8
+
(cid:19)
122nt2
B∗

−

(cid:18)

(cid:19)

122nt2
B∗
122nt2
B∗

−

(cid:18)

+ 24 exp

(cid:19) (cid:21)

where

B∗ :=

1
122 max(2B4, 8(BBk, Bl)4

+, 8(BBl, Bk)4

+, 18(B, Bk, Bl)6

+, 18(B2

k, Bl)6

+, 18(Bk, B2

l )6

+, 32 · 32(Bk, Bl)8

+).

20

By reparameterization, it follows that

P

(cid:18) c1Jn
γn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆS(t, t(cid:48)) − S(t, t(cid:48))
(cid:12)
(cid:12) ≤ t

(cid:19)

≥ 1 − 62 exp

−

(cid:18)

nt2
γ2
c2
1J 2nB∗

(cid:19)

.

(23)

F.2.6 Union Bound for

(cid:12)
(cid:12)
(cid:12)

ˆλn − λn

(cid:12)
(cid:12)
(cid:12) and Final Lower Bound

Recall from (22) that

|ˆλn − λn| ≤

(cid:12)
ˆS(t(1), t(2)) − S(t(1), t(2))
(cid:12)
(cid:12) +

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)
(cid:12)

√

+ c2n

J|ˆu(t∗) − u(t∗)| + c3nγn.

c1Jn
γn
c1n
γn

(cid:12)
(cid:12)
(cid:12)
8B2J
n − 1

+

We will bound terms in (22) separately and combine all the bounds with the union bound. As shown in (8), the
U-statistic core h is bounded between −2B and 2B. Thus, by Lemma 13 (with m = 2), we have

√

(cid:16)

P

c2n

J|ˆu(t∗) − u(t∗)| ≤ t

≥ 1 − 2 exp

−

(cid:17)

(cid:18)

(cid:98)0.5n(cid:99)t2
8c2
2n2JB2

(cid:19)

.

Bounding c1n
γn

8B2J
n−1 + c3nγn + 4BJc1n

γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12). By Lemma 13 (with m = 2), it follows that

P

(cid:18) c1n
γn

8B2J
n − 1

≥ 1 − 2 exp

+ c3nγn +

(cid:104)

(cid:98)0.5n(cid:99)γ2
n

4BJc1n
γn

(cid:12)ˆu(˜t) − u(˜t)(cid:12)
(cid:12)

(cid:12) ≤ t

(cid:19)

t − c1n
γn
27B4J 2c2

8B2J
n−1 − c3nγn
1n2

(cid:105)2









−

(cid:32)

= 1 − 2 exp

−

(a)
≥ 1 − 2 exp

(cid:32)

−

(cid:98)0.5n(cid:99) (cid:2)tγn(n − 1) − 8c1B2nJ − c3n(n − 1)γ2

n

(cid:3)2

(cid:33)

27B4J 2c2
(cid:2)tγn(n − 1) − 8c1B2nJ − c3n(n − 1)γ2

1n2(n − 1)2

n

(cid:3)2

(cid:33)

,

28B4J 2c2

1n2(n − 1)

(24)

(25)

where at (a) we used (cid:98)0.5n(cid:99) ≥ (n − 1)/2. Combining (23), (24), and (25) with the union bound (set T = 3t), we can
bound (22) with

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

ˆλn − λn

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ T

≥ 1 − 62 exp

(cid:32)

− 2 exp

−

(cid:18)

(cid:19)

(cid:18)

−

− 2 exp

nT 2
γ2
32c2
1J 2nB∗

(cid:98)0.5n(cid:99)T 2
72c2
2n2JB2
nn(n − 1)(cid:3)2
(cid:2)T γn(n − 1)/3 − 8c1B2nJ − c3γ2
28B4J 2c2

1n2(n − 1)

−

(cid:19)

(cid:33)

.

Since

ˆλn − λn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ T implies ˆλn ≥ λn − T , a reparametrization with r = λn − T gives
(cid:12)

P

(cid:16)ˆλn ≥ r

(cid:17)

≥ 1 − 62 exp

−

(cid:18)

(cid:19)

n(λn − r)2
γ2
32c2
1J 2nB∗

(cid:18)

(cid:98)0.5n(cid:99)(λn − r)2

(cid:19)

− 2 exp

−

72c2

2n2JB2

(cid:32)

− 2 exp

−

(cid:2)(λn − r)γn(n − 1)/3 − 8c1B2nJ − c3γ2
28B4J 2c2

1n2(n − 1)

nn(n − 1)(cid:3)2

(cid:33)

Grouping constants into ξ1, . . . ξ5 gives the result.

:= L(λn).

21

The lower bound L(λn) takes the form

1 − 62 exp (cid:0)−C1(λn − Tα)2(cid:1) − 2 exp (cid:0)−C2(λn − Tα)2(cid:1) − 2 exp

(cid:18)

−

[(λn − Tα)C3 − C4]2
C5

(cid:19)

,

where C1, . . . , C5 are positive constants. For ﬁxed large enough n such that λn > Tα, and ﬁxed signiﬁcance level α,
increasing λn will increase L(λn). Speciﬁcally, since n is ﬁxed, increasing u(cid:62)Σ−1u in λn = nu(cid:62)Σ−1u will increase
L(λn).

G Helper Lemmas

This section contains lemmas used to prove the main results in this work.

Lemma 8 (Product to sum). Assume that |ai| ≤ B, |bi| ≤ B for i = 1, . . . , E. Then
BE−1 (cid:80)E

j=1 |aj − bj|.

(cid:12)
(cid:81)E
(cid:12)
(cid:12)

i=1 ai − (cid:81)E

i=1 bi

(cid:12)
(cid:12)
(cid:12) ≤

Proof.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

i=1

E
(cid:89)

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

i=1

ai −

bj

≤

ai −

aibE

+

aibE −

aibE−1bE

+ . . . +

a1

bj −

bj

E−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E−1
(cid:89)

i=1

E−2
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:12)
(cid:32)E−2
(cid:12)
(cid:89)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:89)

j=2

E
(cid:89)

j=1

E
(cid:89)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j=2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ |aE − bE|

ai

+ |aE−1 − bE−1|

ai

bE

+ . . . + |a1 − b1|

bj

≤ |aE − bE|BE−1 + |aE−1 − bE−1| BE−1 + . . . + |a1 − b1| BE−1

= BE−1

|aj − bj|

E
(cid:88)

j=1

applying triangle inequality, and the boundedness of ai and bi-s.

Lemma 9 (Product variant of the Hoeﬀding’s inequality). For i = 1, . . . , E, let {x(i)
j=1 ⊂ Xi be an i.i.d. sample
from a distribution Pi, and fi : Xi (cid:55)→ R be a measurable function. Note that it is possible that P1 = P2 = · · · = PE
and {x(1)
j=1. Assume that |fi(x)| ≤ B < ∞ for all x ∈ Xi and i = 1, . . . , E. Write ˆPi to denote
an empirical distribution based on the sample {x(i)

j=1 = · · · = {x(E)

j }nE

j }n1

j }ni

j }ni

j=1. Then,

P

(cid:32)(cid:12)
(cid:34) E
(cid:12)
(cid:89)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:35)
fi(x(i))

−

(cid:34) E
(cid:89)

E

x(i)∼ ˆPi

i=1

E
x(i)∼Pifi(x(i))

≤ T

≥ 1 − 2

exp

−

(cid:33)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:88)

i=1

(cid:18)

niT 2
2E2B2E

(cid:19)

.

Proof. By Lemma 8, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34) E
(cid:89)

E

i=1

x(i)∼ ˆPi

(cid:35)
fi(x(i))

−

(cid:34) E
(cid:89)

i=1

E
x(i)∼Pifi(x(i))

≤ BE−1

x(i)∼ ˆPi

fi(x(i)) − E

(cid:12)
x(i)∼Pi fi(x(i))
(cid:12)
(cid:12) .

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)

E

By
(cid:16)(cid:12)
P
E
(cid:12)
(cid:12)

applying

the
fi(x(i)) − E

x(i)∼ ˆPi

Hoeﬀding’s
(cid:12)
(cid:12)
x(i)∼Pifi(x(i))
(cid:12) ≤ t

inequality
(cid:17)

≥ 1 − 2 exp

to
(cid:16)
− 2nit2
4B2

each
(cid:17)

term

the
. The result is obtained with a union bound.

sum,

we

in

have

H External Lemmas

In this section, we provide known results referred to in this work.

Lemma 10 (Chwialkowski et al. [2015, Lemma 1]). If k is a bounded, analytic kernel (in the sense given in
Deﬁnition 1) on Rd × Rd, then all functions in the RKHS deﬁned by k are analytic.

22

Lemma 11 (Chwialkowski et al. [2015, Lemma 3]). Let Λ be an injective mapping from the space of probability
measures into a space of analytic functions on Rd. Deﬁne

d2
VJ

(P, Q) =

|[ΛP ](vj) − [ΛQ](vj)|2 ,

J
(cid:88)

j=1

where VJ = {vi}J
respect to the Lebesgue measure. Then, dVJ (P, Q) is almost surely (w.r.t. VJ ) a metric.

i=1 are vector-valued i.i.d. random variables from a distribution which is absolutely continuous with

Lemma 12 (Bochner’s theorem [Rudin, 2011]). A continuous function Ψ : Rd → R is positive deﬁnite if and only if
it is the Fourier transform of a ﬁnite nonnegative Borel measure ζ on Rd, that is, Ψ(x) = (cid:82)
Rd e−ix(cid:62)ω dζ(ω), x ∈ Rd.

Lemma 13 (A bound for U-statistics [Serﬂing, 2009, Theorem A, p. 201]). Let h(x1, . . . , xm) be a U-
statistic kernel for an m-order U-statistic such that h(x1, . . . , xm) ∈ [a, b] where a ≤ b < ∞. Let Un =
(cid:0) n
h(xi1 , . . . , xim ) be a U-statistic computed with a sample of size n, where the summation is over the
m
(cid:0) n
m

(cid:1)−1 (cid:80)
(cid:1) combinations of m distinct elements {i1, . . . , im} from {1, . . . , n}. Then, for t > 0 and n ≥ m,

i1<···<im

P(Un − Eh(x1, . . . , xm) ≥ t) ≤ exp (cid:0)−2(cid:98)n/m(cid:99)t2/(b − a)2(cid:1) ,
P(|Un − Eh(x1, . . . , xm)| ≥ t) ≤ 2 exp (cid:0)−2(cid:98)n/m(cid:99)t2/(b − a)2(cid:1) ,

where (cid:98)x(cid:99) denotes the greatest integer which is smaller than or equal to x. Hoeﬃnd’s inequality is a special case
when m = 1.

Lemma 14 (Hoeﬀding’s inequality). Let X1, . . . , Xn be i.i.d. random variables such that a ≤ Xi ≤ b almost surely.
Deﬁne X := 1
n

i=1 Xi. Then,

(cid:80)n

P (cid:0)(cid:12)

(cid:12)X − E[X](cid:12)

(cid:12) ≤ α(cid:1) ≥ 1 − 2 exp

(cid:18)

−

2nα2
(b − a)2

(cid:19)

.

References

[sup4] K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton. Fast Two-Sample Testing with Analytic
Representations of Probability Measures. In Advances in Neural Information Processing Systems (NIPS), pages
1981–1989. 2015.

[sup15] W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gretton. Interpretable Distribution Features with Maximum

Testing Power. 2016. URL http://arxiv.org/abs/1605.06796.

[sup3] W. Rudin. Fourier analysis on groups. John Wiley & Sons, 2011.

[sup21] R. J. Serﬂing. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 2009.

23

