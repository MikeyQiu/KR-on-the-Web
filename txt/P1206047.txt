8
1
0
2
 
p
e
S
 
6
2
 
 
]

V
C
.
s
c
[
 
 
3
v
4
6
9
2
0
.
6
0
8
1
:
v
i
X
r
a

BSN: Boundary Sensitive Network for Temporal Action
Proposal Generation

Tianwei Lin1, Xu Zhao(cid:63)1, Haisheng Su1, Chongjing Wang2, Ming Yang1

1Department of Automation, Shanghai Jiao Tong University, China
2China Academy of Information and Communications Technology, China
wzmsltw,zhaoxu,suhaisheng,mingyang@sjtu.edu.cn
wangchongjing@caict.ac.cn

Abstract. Temporal action proposal generation is an important yet challeng-
ing problem, since temporal proposals with rich action content are indispens-
able for analysing real-world videos with long duration and high proportion ir-
relevant content. This problem requires methods not only generating proposals
with precise temporal boundaries, but also retrieving proposals to cover truth
action instances with high recall and high overlap using relatively fewer pro-
posals. To address these difﬁculties, we introduce an effective proposal genera-
tion method, named Boundary-Sensitive Network (BSN), which adopts “local to
global” fashion. Locally, BSN ﬁrst locates temporal boundaries with high prob-
abilities, then directly combines these boundaries as proposals. Globally, with
Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the
conﬁdence of whether a proposal contains an action within its region. We con-
duct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14,
where BSN outperforms other state-of-the-art temporal action proposal genera-
tion methods with high recall and high temporal precision. Finally, further ex-
periments demonstrate that by combining existing action classiﬁers, our method
signiﬁcantly improves the state-of-the-art temporal action detection performance.

Keywords: Temporal action proposal generation · Temporal action detection ·
Temporal convolution · Untrimmed video

1 Introduction

Nowadays, with fast development of digital cameras and Internet, the number of videos
is continuously booming, making automatic video content analysis methods widely re-
quired. One major branch of video analysis is action recognition, which aims to classify
manually trimmed video clips containing only one action instance. However, videos in
real scenarios are usually long, untrimmed and contain multiple action instances along
with irrelevant contents. This problem requires algorithms for another challenging task:
temporal action detection, which aims to detect action instances in untrimmed video
including both temporal boundaries and action classes. It can be applied in many areas
such as video recommendation and smart surveillance.

Similar with object detection in spatial domain, temporal action detection task can
be divided into two stages: proposal and classiﬁcation. Proposal generation stage aims

(cid:63) Corresponding author.

2

Tianwei Lin et al.

Fig. 1: Overview of our approach. Given an untrimmed video, (1) we evaluate bound-
aries and actionness probabilities of each temporal location and generate proposals
based on boundary probabilities, and (2) we evaluate the conﬁdence scores of proposals
with proposal-level feature to get retrieved proposals.

to generate temporal video regions which may contain action instances, and classiﬁca-
tion stage aims to classify classes of candidate proposals. Although classiﬁcation meth-
ods have reached convincing performance, the detection precision is still low in many
benchmarks [1,2]. Thus recently temporal action proposal generation has received much
attention [3,4,5,6], aiming to improve the detection performance by improving the qual-
ity of proposals. High quality proposals should come up with two key properties: (1)
proposals can cover truth action regions with both high recall and high temporal over-
lap, (2) proposals are retrieved so that high recall and high overlap can be achieved
using fewer proposals to reduce the computation cost of succeeding steps.

To achieve high proposal quality, a proposal generation method should generate pro-
posals with ﬂexible temporal durations and precise temporal boundaries, then retrieve
proposals with reliable conﬁdence scores, which indicate the probability of a proposal
containing an action instance. Most recently proposal generation methods [3,4,5,7] gen-
erate proposals via sliding temporal windows of multiple durations in video with regular
interval, then train a model to evaluate the conﬁdence scores of generated proposals for
proposals retrieving, while there is also method [6] making external boundaries regres-
sion. However, proposals generated with pre-deﬁned durations and intervals may have
some major drawbacks: (1) usually not temporally precise; (2) not ﬂexible enough to
cover variable temporal durations of ground truth action instances, especially when the
range of temporal durations is large.

To address these issues and generate high quality proposals, we propose the Boundary-

Sensitive Network (BSN), which adopts “local to global” fashion to locally combine
high probability boundaries as proposals and globally retrieve candidate proposals us-
ing proposal-level feature as shown in Fig 1. In detail, BSN generates proposals in three
steps. First, BSN evaluates the probabilities of each temporal location in video whether
it is inside or outside, at or not at the boundaries of ground truth action instances, to
generate starting, ending and actionness probabilities sequences as local information.

Boundary Sensitive Network

3

Second, BSN generates proposals via directly combining temporal locations with high
starting and ending probabilities separately. Using this bottom-up fashion, BSN can
generate proposals with ﬂexible durations and precise boundaries. Finally, using fea-
tures composed by actionness scores within and around proposal, BSN retrieves pro-
posals by evaluating the conﬁdence of whether a proposal contains an action. These
proposal-level features offer global information for better evaluation.
In summary, the main contributions of our work are three-folds:
(1) We introduce a new architecture (BSN) based on “local to global” fashion to
generate high quality temporal action proposals, which locally locates high bound-
ary probability locations to achieve precise proposal boundaries and globally evaluates
proposal-level feature to achieve reliable proposal conﬁdence scores for retrieving.

(2) Extensive experiments demonstrate that our method achieves signiﬁcantly bet-
ter proposal quality than other state-of-the-art proposal generation methods, and can
generate proposals in unseen action classes with comparative quality.

(3) Integrating our method with existing action classiﬁer into detection framework

leads to signiﬁcantly improved performance on temporal action detection task.

2 Related work

Action recognition. Action recognition is an important branch of video related research
areas and has been extensively studied. Earlier methods such as improved Dense Tra-
jectory (iDT) [8,9] mainly adopt hand-crafted features such as HOF, HOG and MBH. In
recent years, convolutional networks are widely adopted in many works [10,11,12,13]
and have achieved great performance. Typically, two-stream network [10,11,13] learns
appearance and motion features based on RGB frame and optical ﬂow ﬁeld separately.
C3D network [12] adopts 3D convolutional layers to directly capture both appearance
and motion features from raw frames volume. Action recognition models can be used
for extracting frame or snippet level visual features in long and untrimmed videos.
Object detection and proposals. Recent years, the performance of object detection has
been signiﬁcantly improved with deep learning methods. R-CNN [14] and its variations
[15,16] construct an important branch of object detection methods, which adopt “detec-
tion by classifying proposals” framework. For proposal generation stage, besides sliding
windows [17], earlier works also attempt to generate proposals by exploiting low-level
cues such as HOG and Canny edge [18,19]. Recently some methods [16,20,21] adopt
deep learning model to generate proposals with faster speed and stronger modelling ca-
pacity. In this work, we combine the properties of these methods via evaluating bound-
aries and actionness probabilities of each location using neural network and adopting
“local to global” fashion to generate proposals with high recall and accuracy.

Boundary probabilities are also adopted in LocNet [22] for revising the horizon-
tal and vertical boundaries of existing proposals. Our method differs in (1) BSN aims
to generate while LocNet aims to revise proposals and (2) boundary probabilities are
calculated repeatedly for all boxes in LocNet but only once for a video in BSN.
Temporal action detection and proposals. Temporal action detection task aims to
detect action instances in untrimmed videos including temporal boundaries and action
classes, and can be divided into proposal and classiﬁcation stages. Most detection meth-
ods [7,23,24] take these two stages separately, while there is also method [25,26] taking

4

Tianwei Lin et al.

these two stages jointly. For proposal generation, earlier works [27,28,29] directly use
sliding windows as proposals. Recently some methods [3,4,5,6,7] generate proposals
with pre-deﬁned temporal durations and intervals, and use multiple methods to evaluate
the conﬁdence score of proposals, such as dictionary learning [4] and recurrent neural
network [5]. TAG method [24] adopts watershed algorithm to generate proposals with
ﬂexible boundaries and durations in local fashion, but without global proposal-level
conﬁdence evaluation for retrieving. In our work, BSN can generate proposals with
ﬂexible boundaries meanwhile reliable conﬁdence scores for retrieving.

Recently temporal action detection method [30] detects action instances based on
class-wise start, middle and end probabilities of each location. Our method is superior
than [30] in two aspects: (1) BSN evaluates probabilities score using temporal convolu-
tion to better capture temporal information and (2) “local to global” fashion adopted in
BSN brings more precise boundaries and better retrieving quality.

3 Our Approach

3.1 Problem Deﬁnition

An untrimmed video sequence can be denoted as X = {xn}lv
n=1 with lv frames, where
xn is the n-th frame in X. Annotation of video X is composed by a set of action in-
stances Ψg = {ϕn = (ts,n, te,n)}Ng
n=1, where Ng is the number of truth action instances
in video X, and ts,n, te,n are starting and ending time of action instance ϕn separately.
Unlike detection task, classes of action instances are not considered in temporal ac-
tion proposal generation. Annotation set Ψg is used during training. During prediction,
generated proposals set Ψp should cover Ψg with high recall and high temporal overlap.

3.2 Video Features Encoding

To generate proposals of input video, ﬁrst we need to extract feature to encode visual
content of video. In our framework, we adopt two-stream network [11] as visual en-
coder, since this architecture has shown great performance in action recognition task
[31] and has been widely adopted in temporal action detection and proposal generation
tasks [24,25,32]. Two-stream network contains two branches: spatial network operates
on single RGB frame to capture appearance feature, and temporal network operates on
stacked optical ﬂow ﬁeld to capture motion information.

To extract two-stream features, as shown in Fig 2(a), ﬁrst we compose a snippets
sequence S = {sn}ls
n=1 from video X, where ls is the length of snippets sequence. A
snippet sn = (xtn , otn) includes two parts: xtn is the tn-th RGB frame in X and otn is
stacked optical ﬂow ﬁeld derived around center frame xtn . To reduce the computation
cost, we extract snippets with a regular frame interval σ, therefore ls = lv/σ. Given
a snippet sn, we concatenate output scores in top layer of both spatial and temporal
networks to form the encoded feature vector ftn = (fS,tn , fT,tn ), where fS,tn , fT,tn
are output scores from spatial and temporal networks separately. Thus given a snippets
sequence S with length ls, we can extract a feature sequence F = {ftn }ls
n=1. These
two-stream feature sequences are used as the input of BSN.

Boundary Sensitive Network

5

Fig. 2: The framework of our approach. (a) Two-stream network is used for encoding
visual features in snippet-level. (b) The architecture of Boundary-Sensitive Network:
temporal evaluation module handles the input feature sequence, and evaluates start-
ing, ending and actionness probabilities of each temporal location; proposal genera-
tion module generates proposals with high starting and ending probabilities, and con-
struct Boundary-Sensitive Proposal (BSP) feature for each proposal; proposal evalua-
tion module evaluates conﬁdence score of each proposal using BSP feature. (c) Finally,
we use Soft-NMS algorithm to suppress redundant proposals by decaying their scores.

3.3 Boundary-Sensitive Network

To achieve high proposal quality with both precise temporal boundaries and reliable
conﬁdence scores, we adopt “local to global” fashion to generate proposals. In BSN, we
ﬁrst generate candidate boundary locations, then combine these locations as proposals
and evaluate conﬁdence score of each proposal with proposal-level feature.
Network architecture. The architecture of BSN is presented in Fig 2(b), which con-
tains three modules: temporal evaluation, proposal generation and proposal evaluation.
Temporal evaluation module is a three layers temporal convolutional neural network,
which takes the two-stream feature sequences as input, and evaluates probabilities of
each temporal location in video whether it is inside or outside, at or not at boundaries
of ground truth action instances, to generate sequences of starting, ending and action-
ness probabilities respectively. Proposal generation module ﬁrst combines the temporal
locations with separately high starting and ending probabilities as candidate proposals,
then constructs Boundary-Sensitive Proposal (BSP) feature for each candidate proposal
based on actionness probabilities sequence. Finally, proposal evaluation module, a mul-
tilayer perceptron model with one hidden layer, evaluates the conﬁdence score of each
candidate proposal based on BSP feature. Conﬁdence score and boundary probabilities
of each proposal are fused as the ﬁnal conﬁdence score for retrieving.
Temporal evaluation module. The goal of temporal evaluation module is to evaluate
starting, ending and actionness probabilities of each temporal location, where three bi-
nary classiﬁers are needed. In this module, we adopt temporal convolutional layers upon

6

Tianwei Lin et al.

(a) Generate proposals

(b) Construct BSP feature

Fig. 3: Details of proposal generation module. (a) Generate proposals. First, to generate
candidate boundary locations, we choose temporal locations with high boundary prob-
ability or being a probability peak. Then, we combine candidate starting and ending
locations as proposals when their duration satisfying condition. (b) Construct BSP fea-
ture. Given a proposal and actionness probabilities sequence, we can sample actionness
sequence in starting, center and ending regions of proposal to construct BSP feature.

feature sequence, with good modelling capacity to capture local semantic information
such as boundaries and actionness probabilities.

A temporal convolutional layer can be simply denoted as Conv(cf , ck, Act), where
cf , ck and Act are ﬁlter numbers, kernel size and activation function of temporal con-
volutional layer separately. As shown in Fig 2(b), the temporal evaluation module can
be deﬁned as Conv(512, 3, Relu) → Conv(512, 3, Relu) → Conv(3, 1, Sigmoid),
where the three layers have same stride size 1. Three ﬁlters with sigmoid activation in
the last layer are used as classiﬁers to generate starting, ending and actionness prob-
abilities separately. For convenience of computation, we divide feature sequence into
non-overlapped windows as the input of temporal evaluation module. Given a fea-
ture sequence F , temporal evaluation module can generate three probability sequences
PS = (cid:8)ps
(cid:9)ls
n=1 and PA = (cid:8)pa
(cid:9)ls
n=1, where ps
tn are
tn
respectively starting, ending and actionness probabilities in time tn.
Proposal generation module. The goal of proposal generation module is to generate
candidate proposals and construct corresponding proposal-level feature. We achieve this
goal in two steps. First we locate temporal locations with high boundary probabilities,
and combine these locations to form proposals. Then for each proposal, we construct
Boundary-Sensitive Proposal (BSP) feature.

n=1, PE = (cid:8)pe
(cid:9)ls

tn and pa

tn, pe

tn

tn

> 0.9 or (2) is a probability peak: ps
tn

As shown in Fig 3(a), to locate where an action likely to start, for starting proba-
bilities sequence PS, we record all temporal location tn where ps
tn (1) has high score:
> ps
ps
tn+1. These locations
tn
are grouped into candidate starting locations set BS = {ts,i}NS
i=1, where NS is the num-
ber of candidate starting locations. Using same rules, we can generate candidate ending
locations set BE from ending probabilities sequence PE. Then, we generate temporal
regions via combing each starting location ts from BS and each ending location te from
BS. Any temporal region [ts, te] satisfying d = te − ts ∈ [dmin, dmax] is denoted as
a candidate proposal ϕ, where dmin and dmax are minimum and maximum durations

tn−1 and ps
tn

> ps

Boundary Sensitive Network

7

of ground truth action instances in dataset. Thus we can get candidate proposals set
Ψp = {ϕi}Np

i=1, where Np is the number of proposals.

To construct proposal-level feature as shown in Fig 3(b), for a candidate proposal
ϕ, we denote its center region as rC = [ts, te] and its starting and ending region as
rS = [ts − d/5, ts + d/5] and rE = [te − d/5, te + d/5] separately. Then, we sample
the actionness sequence PA within rc as f A
c by linear interpolation with 16 points. In
starting and ending regions, we also sample actionness sequence with 8 linear interpo-
s and f A
lation points and get f A
e separately. Concatenating these vectors, we can get
Boundary-Sensitive Proposal (BSP) feature fBSP = (f A
s ,f A
e ) of proposal ϕ. BSP
feature is highly compact and contains rich semantic information about corresponding
proposal. Then we can represent a proposal as ϕ = (ts, te, fBSP ).
Proposal evaluation module. The goal of proposal evaluation module is to evaluate
the conﬁdence score of each proposal whether it contains an action instance within
its duration using BSP feature. We adopt a simple multilayer perceptron model with
one hidden layer as shown in Fig 2(b). Hidden layer with 512 units handles the input
of BSP feature fBSP with Relu activation. The output layer outputs conﬁdence score
pconf with sigmoid activation, which estimates the overlap extent between candidate
proposal and ground truth action instances. Thus, a generated proposal can be denoted
as ϕ = (ts, te, pconf , ps
ts and pe
te are starting and ending probabilities
ts
in ts and te separately. These scores are fused to generate ﬁnal score during prediction.

), where ps

c ,f A

, pe
te

3.4 Training of BSN

In BSN, temporal evaluation module is trained to learn local boundary and actionness
probabilities from video features simultaneously. Then based on probabilities sequence
generated by trained temporal evaluation module, we can generate proposals and corre-
sponding BSP features and train the proposal evaluation module to learn the conﬁdence
score of proposals. The training details are introduced in this section.
Temporal evaluation module. Given a video X, we compose a snippets sequence S
with length ls and extract feature sequence F from it. Then we slide windows with
length lw = 100 in feature sequence without overlap. A window is denoted as ω =
{Fω, Ψω}, where Fω and Ψω are feature sequence and annotations within the window
separately. For ground truth action instance ϕg = (ts, te) in Ψω, we denote its region
as action region ra
g = [ts − dg/10, ts + dg/10]
and re

g = [te − dg/10, te + dg/10] separately, where dg = te − ts.

g and its starting and ending region as rs

tn , pe

tn and pa

Taking Fω as input, temporal evaluation module generates probabilities sequence
PS,ω, PE,ω and PA,ω with same length lw. For each temporal location tn within Fω,
we denote its region as rtn = [tn − ds/2, tn + ds/2] and get corresponding probability
scores ps
tn from PS,ω, PE,ω and PA,ω separately, where ds = tn − tn−1
is temporal interval between two snippets. Then for each rtn , we calculate its IoP ra-
tio with ra
g of all ϕg in Ψω separately, where IoP is deﬁned as the overlap
ratio with groundtruth proportional to the duration of this proposal. Thus we can rep-
resent information of tn as φn = (pa
tn , ge
, ps
tn are
tn
tn
maximum matching overlap IoP of action, starting and ending regions separately.

), where ga

g and re

tn , gs

g , rs

, pe
tn

, ge
tn

, ga
tn

, gs
tn

8

Tianwei Lin et al.

Given a window of matching information as Φω = {φn}ls

n=1, we can deﬁne training
objective of this module as a three-task loss function. The overall loss function consists
of actionness loss, starting loss and ending loss:

(1)

(2)

LT EM = λ · Laction

bl

+ Lstart

bl + Lend

bl

,

where λ is the weight term and is set to 2 in BSN. We adopt the sum of binary logistic
regression loss function Lbl for all three tasks, which can be denoted as:

Lbl =

(cid:0)α+ · bi · log(pi) + α− · (1 − bi) · log(1 − pi)(cid:1) ,

1
lw

lw(cid:88)

i=1

where bi = sign(gi − θIoP ) is a two-values function for converting matching score
gi to {0, 1} based on threshold θIoP , which is set to 0.5 in BSN. Let l+ = (cid:80) gi and
l− = lw − l+, we can set α+ = lw
l− and α− = lw
l+ , which are used for balancing the
effect of positive and negative samples during training.
Proposal evaluation module. Using probabilities sequences generated by trained tem-
poral evaluation module, we can generate proposals using proposal generation mod-
ule: Ψp = {ϕn = (ts, te, fBSP )}Np
n=1. Taking fBSP as input, for a proposal ϕ, con-
ﬁdence score pconf is generated by proposal evaluation module. Then we calculate its
Intersection-over-Union (IoU) with all ϕg in Ψg, and denote the maximum overlap score
as giou. Thus we can represent proposals set as Ψp = {ϕn = {ts, te, pconf , giou}}Np
n=1.
We split Ψp into two parts based on giou: Ψ pos
for giou < 0.3.
p
p
For data balancing, we take all proposals in Ψ pos
and randomly sample the proposals in
p
Ψ neg
p

to insure the ratio between two sets be nearly 1:2.

for giou > 0.7 and Ψ neg

The training objective of this module is a simple regression loss, which is used to

train a precise conﬁdence score prediction based on IoU overlap. We can deﬁne it as:

LP EM =

(pconf,i − giou,i)2,

(3)

1
Ntrain

Ntrain(cid:88)

i=1

where Ntrain is the number of proposals used for training.

3.5 Prediction and Post-processing

During prediction, we use BSN with same procedures described in training to generate
proposals set Ψp = (cid:8)ϕn = (ts, te, pconf , ps
n=1, where Np is the number of pro-
ts
posals. To get ﬁnal proposals set, we need to make score fusion to get ﬁnal conﬁdence
score, then suppress redundant proposals based on these score.
Score fusion for retrieving. To achieve better retrieving performance, for each candi-
date proposal ϕ, we fuse its conﬁdence score with its boundary probabilities by multi-
plication to get the ﬁnal conﬁdence score pf :

)(cid:9)Np

, pe
te

pf = pconf · ps
ts

· pe
te

.

(4)

After score fusion, we can get generated proposals set Ψp = {ϕn = (ts, te, pf )}Np
n=1,
where pf is used for proposals retrieving. In section 4.2, we explore the recall perfor-
mance with and without conﬁdence score generated by proposal evaluation module.

Boundary Sensitive Network

9

Redundant proposals suppression. Around a ground truth action instance, we may
generate multiple proposals with different temporal overlap. Thus we need to suppress
redundant proposals to obtain higher recall with fewer proposals.

Soft-NMS [33] is a recently proposed non-maximum suppression (NMS) algorithm
which suppresses redundant results using a score decaying function. First all proposals
are sorted by their scores. Then proposal ϕm with maximum score is used for calcu-
lating overlap IoU with other proposals, where scores of highly overlapped proposals
is decayed. This step is recursively applied to the remaining proposals to generate re-
scored proposals set. The Gaussian decaying function of Soft-NMS can be denoted as:

p(cid:48)
f,i =

(cid:40)
pf,i,
pf,i · e− iou(ϕm ,ϕi)2

ε

iou(ϕm, ϕi) < θ
, iou(ϕm, ϕi) ≥ θ

(5)

where ε is parameter of Gaussian function and θ is pre-ﬁxed threshold. After suppres-
sion, we get the ﬁnal proposals set Ψ (cid:48)

ϕn = (ts, te, p(cid:48)

(cid:111)Np

(cid:110)

.

p =

f )

n=1

4 Experiments

4.1 Dataset and setup

Dataset. ActivityNet-1.3 [1] is a large dataset for general temporal action proposal gen-
eration and detection, which contains 19994 videos with 200 action classes annotated
and was used in the ActivityNet Challenge 2016 and 2017. ActivityNet-1.3 is divided
into training, validation and testing sets by ratio of 2:1:1. THUMOS14 [2] dataset con-
tains 200 and 213 temporal annotated untrimmed videos with 20 action classes in vali-
dation and testing sets separately. The training set of THUMOS14 is the UCF-101 [34],
which contains trimmed videos for action recognition task. In this section, we compare
our method with state-of-the-art methods on both ActivityNet-1.3 and THUMOS14.
Evaluation metrics. In temporal action proposal generation task, Average Recall (AR)
calculated with multiple IoU thresholds is usually used as evaluation metrics. Follow-
ing conventions, we use IoU thresholds set [0.5 : 0.05 : 0.95] in ActivityNet-1.3 and
[0.5 : 0.05 : 1.0] in THUMOS14. To evaluate the relation between recall and proposals
number, we evaluate AR with Average Number of proposals (AN) on both datasets,
which is denoted as AR@AN. On ActivityNet-1.3, area under the AR vs. AN curve
(AUC) is also used as metrics, where AN varies from 0 to 100.

In temporal action detection task, mean Average Precision (mAP) is used as evalua-
tion metric, where Average Precision (AP) is calculated on each action class respec-
tively. On ActivityNet-1.3, mAP with IoU thresholds {0.5, 0.75, 0.95} and average
mAP with IoU thresholds set [0.5 : 0.05 : 0.95] are used. On THUMOS14, mAP
with IoU thresholds {0.3, 0.4, 0.5, 0.6, 0.7} is used.
Implementation details. For visual feature encoding, we use the two-stream network
[11] with architecture described in [35], where BN-Inception network [36] is used as
temporal network and ResNet network [37] is used as spatial network. Two-stream
network is implemented using Caffe [38] and pre-trained on ActivityNet-1.3 training
set. During feature extraction, the interval σ of snippets is set to 16 on ActivityNet-1.3
and is set to 5 on THUMOS14.

10

Tianwei Lin et al.

Fig. 4: Comparison of our proposal generation method with other state-of-the-art meth-
ods in THUMOS14 dataset. (left) BSN can achieve signiﬁcant performance gains with
relatively few proposals. (center) Recall with 100 proposals vs tIoU ﬁgure shows that
with few proposals, BSN gets performance improvements in both low and high tIoU.
(right) Recall with 1000 proposals vs tIoU ﬁgure shows that with large number of pro-
posals, BSN achieves improvements mainly while tIoU > 0.8.

On ActivityNet-1.3, since the duration of videos are limited, we follow [39] to
rescale the feature sequence of each video to new length lw = 100 by linear interpo-
lation, and the duration of corresponding annotations to range [0,1]. In BSN, temporal
evaluation module and proposal evaluation module are both implemented using Tensor-
ﬂow [40]. On both datasets, temporal evaluation module is trained with batch size 16
and learning rate 0.001 for 10 epochs, then 0.0001 for another 10 epochs, and proposal
evaluation module is trained with batch size 256 and same learning rate. For Soft-NMS,
we set the threshold θ to 0.8 on ActivityNet-1.3 and 0.65 on THUMOS14 by empirical
validation, while ε in Gaussian function is set to 0.75 on both datasets.

4.2 Temporal Proposal Generation

Taking a video as input, proposal generation method aims to generate temporal propos-
als where action instances likely to occur. In this section, we compare our method with
state-of-the-art methods and make external experiments to verify effectiveness of BSN.
Comparison with state-of-the-art methods. As aforementioned, a good proposal gen-
eration method should generate and retrieve proposals to cover ground truth action in-
stances with high recall and high temporal overlap using relatively few proposals. We
evaluate these methods in two aspects.

First we evaluate the ability of our method to generate and retrieve proposals with
high recall, which is measured by average recall with different number of proposals
(AR@AN) and area under AR-AN curve (AUC). We list the comparison results of
ActivityNet-1.3 and THUMOS14 in Table 1 and Table 2 respectively, and plot the av-
erage recall against average number of proposals curve of THUMOS14 in Fig 4 (left).
On THUMOS14, our method outperforms other state-of-the-art proposal methods when
proposal number varies from 10 to 1000. Especially, when average number of propos-
als is 50, our method signiﬁcantly improves average recall from 21.86% to 37.46% by
15.60%. On ActivityNet-1.3, our method outperforms other state-of-the-art proposal
generation methods on both validation and testing set.

Second, we evaluate the ability of our method to generate and retrieve proposals
with high temporal overlap, which is measured by recall of multiple IoU thresholds.
We plot the recall against IoU thresholds curve with 100 and 1000 proposals in Fig 4

Boundary Sensitive Network

11

Table 1: Comparison between our method with other state-of-the-art proposal genera-
tion methods on validation set of ActivityNet-1.3 in terms of AR@AN and AUC.
BSN
74.16
66.17
66.26

Method
AR@100 (val)
AUC (val)
AUC (test)

Zhao et al. [24] Dai et al. [42] Yao et al. [43] Lin et al. [39]

73.01
64.40
64.80

-
59.58
61.56

-
63.12
64.18

63.52
53.02
-

Table 2: Comparison between our method with other state-of-the-art proposal genera-
tion methods on THUMOS14 in terms of AR@AN.

Feature
C3D
C3D
C3D
C3D
C3D
C3D

Method
DAPs [5]
SCNN-prop [7]
SST [3]
TURN [6]
BSN + Greedy-NMS
BSN + Soft-NMS

Flow

2-Stream TAG [24]
TURN [6]
2-Stream BSN + Greedy-NMS
2-Stream BSN + Soft-NMS

@50
13.56
17.22
19.90
19.63
27.19
29.58

18.55
21.86
35.41
37.46

@100 @200 @500 @1000
57.64
23.83
58.20
26.17
60.27
28.36
60.75
27.96
59.50
35.38
37.38
59.48

49.29
51.57
51.58
53.52
53.77
54.67

33.96
37.01
37.90
38.34
43.61
45.55

29.00
31.89
43.55
46.06

39.61
43.02
52.23
53.21

-
57.63
61.35
60.64

-
64.17
65.10
64.52

(center) and (right) separately. Fig 4 (center) suggests that our method achieves signif-
icant higher recall than other methods with 100 proposals when IoU threshold varied
from 0.5 to 1.0. And Fig 4 (right) suggests that with 1000 proposals, our method obtains
the largest recall improvements when IoU threshold is higher than 0.8.

Furthermore, we make some controlled experiments to conﬁrm the contribution of
BSN itself in Table 2. For video feature encoding, except for two-stream network, C3D
network [12] is also adopted in some works [3,5,6,7]. For NMS method, most previ-
ous work adopt Greedy-NMS [41] for redundant proposals suppression. Thus, for fair
comparison, we train BSN with feature extracted by C3D network [12] pre-trained on
UCF-101 dataset, then perform Greedy-NMS and Soft-NMS on C3D-BSN and origi-
nal 2Stream-BSN respectively. Results in Table 2 show that (1) C3D-BSN still outper-
forms other C3D-based methods especially with small proposals number, (2) Soft-NMS
only brings small performance promotion than Greedy-NMS, while Greedy-NMS also
works well with BSN. These results suggest that the architecture of BSN itself is the
main reason for performance promotion rather than input feature and NMS method.

These results suggest the effectiveness of BSN. And BSN achieves the salient per-
formance since it can generate proposals with (1) ﬂexible temporal duration to cover
ground truth action instances with various durations; (2) precise temporal boundary via
learning starting and ending probability using temporal convolutional network, which
brings high overlap between generated proposals and ground truth action instances; (3)
reliable conﬁdence score using BSP feature, which retrieves proposals properly so that
high recall and high overlap can be achieved using relatively few proposals. Qualitative
examples on THUMOS14 and ActivityNet-1.3 datasets are shown in Fig 5.
Generalizability of proposals. Another key property of a proposal generation method
is the ability to generate proposals for unseen action classes. To evaluate this property,
we choose two semantically different action subsets on ActivityNet-1.3: “Sports, Ex-

12

Tianwei Lin et al.

Table 3: Generalization evaluation of BSN on ActivityNet-1.3. Seen subset: “Sports,
Exercise, and Recreation”; Unseen subset: “Socializing, Relaxing, and Leisure”.

BSN trained with Seen + Unseen (training)
BSN trained with Seen (training)

Seen (validation)
AUC
63.80
64.02

AR@100
72.40
72.42

Unseen (validation)
AUC
AR@100
63.99
71.84
63.38
71.32

ercise, and Recreation” and “Socializing, Relaxing, and Leisure” as seen and unseen
subsets separately. Seen subset contains 87 action classes with 4455 training and 2198
validation videos, and unseen subset contains 38 action classes with 1903 training and
896 validation videos. To guarantee the experiment effectiveness, instead of two-stream
network, here we adopt C3D network [44] trained on Sports-1M dataset [45] for video
features encoding. Using C3D feature, we train BSN with seen and seen+unseen videos
on training set separately, then evaluate both models on seen and unseen validation
videos separately. As shown in Table 3, there is only slight performance drop in unseen
classes, which demonstrates that BSN has great generalizability and can learn a generic
concept of temporal action proposal even in semantically different unseen actions.
Effectiveness of modules in BSN. To evaluate the effectiveness of temporal evalua-
tion module (TEM) and proposal evaluation module (PEM) in BSN, we demonstrate
experiment results of BSN with and without PEM in Table 4, where TEM is used in
both results. These results show that: (1) using only TEM without PEM, BSN can also
reach considerable recall performance over state-of-the-art methods; (2) PEM can bring
considerable further performance promotion in BSN. These observations suggest that
TEM and PEM are both effective and indispensable in BSN.
Boundary-Sensitive Proposal feature. BSP feature is used in proposal evaluation
module to evaluate the conﬁdence scores of proposals. In Table 4, we also make ab-
lation studies of the contribution of each component in BSP. These results suggest that
although BSP feature constructed from boundary regions contributes less improvements
than center region, best recall performance is achieved while PEM is trained with BSP
constructed from both boundary and center region.

4.3 Action Detection with Our Proposals

To further evaluate the quality of proposals generated by BSN, we put BSN propos-
als into “detection by classifying proposals” temporal action detection framework with
state-of-the-art action classiﬁer, where temporal boundaries of detection results are pro-
vided by our proposals. On ActivityNet-1.3, we use top-2 video-level class generated
by classiﬁcation model [46]1 for all proposals in a video. On THUMOS14, we use
top-2 video-level classes generated by UntrimmedNet [48] for proposals generated by
BSN and other methods.Following previous works, on THUMOS14, we also implement
SCNN-classiﬁer on BSN proposals for proposal-level classiﬁcation and adopt Greedy
NMS as [7]. We use 100 and 200 proposals per video on ActivityNet-1.3 and THU-
MOS14 datasets separately.

1 Previously, we adopted classiﬁcation results from result ﬁles of [47]. Recently we found that
the classiﬁcation accuracy of these results are unexpected high. Thus we replace it with clas-
siﬁcation results of [46] and updated all related experiments accordingly.

Boundary Sensitive Network

13

Table 4: Study of effectiveness of modules in BSN and contribution of components in
BSP feature on THUMOS14, where PEM is trained with BSP feature constructed by
Boundary region (f A

c ) independently and jointly.

e ) and Center region (f A

s , f A

BSN without PEM

BSN with PEM

Boundary Center

!

!

!
!

@50
30.72
35.61
36.80
37.46

@100 @200 @500 @1000
63.04
40.52
64.17
44.86
64.22
45.65
64.52
46.06

48.63
52.46
52.63
53.21

57.78
60.00
60.18
60.64

Table 5: Action detection results on validation and testing set of ActivityNet-1.3 in
terms of mAP@tIoU and average mAP, where our proposals are combined with video-
level classiﬁcation results generated by [46].

Method
Wang et al. [47]
SCC [49]
CDC [50]
TCN [42]
SSN [51]
Lin et al. [39]
BSN + [46]

validation

0.5
42.28
40.00
43.83
-
39.12
44.39
46.45

0.75
3.76
17.90
25.88
-
23.48
29.65
29.96

0.95
0.05
4.70
0.21
-
5.49
7.09
8.02

Average
14.85
21.70
22.77
-
23.98
29.17
30.03

testing
Average
14.62
19.30
22.90
23.58
28.28
32.26
32.84

Table 6: Action detection results on testing set of THUMOS14 in terms of mAP@tIoU ,
where classiﬁcation results generated by UntrimmedNet [48] and SCNN-classiﬁer [7]
are combined with proposals generated by BSN and other methods.

Action Detection Methods

Detection Method
SCNN [7]
SMS [30]
CDC [50]
SSAD [25]
TCN [42]
R-C3D [52]
SS-TAD [26]
SSN [51]
CBR [32]

Proposal method Classiﬁer
SCNN-cls
SST [3]
SCNN-cls
TURN [6]
UNet
SST [3]
UNet
TURN [6]
SCNN-cls
BSN
UNet
BSN

0.7
5.3
-
8.8
7.7
9.0
9.3
9.6
-
9.9

0.7
-
7.7
4.7
6.3
15.0
20.0

0.6
10.3
-
14.3
15.3
15.9
19.1
-
-
19.1

0.6
-
14.6
10.9
14.1
22.4
28.4

0.5
19.0
17.8
24.7
24.6
25.6
28.9
29.2
29.1
31.0

0.5
23.0
25.6
20.0
24.5
29.4
36.9

0.4
28.7
27.8
30.7
35.0
33.3
35.6
-
40.8
41.3

0.4
-
33.2
31.5
35.3
36.6
45.0

0.3
36.3
36.5
41.3
43.0
-
44.8
45.7
50.6
50.1

0.3
-
44.1
41.2
46.3
43.1
53.5

Proposal Generation Methods + Action Classiﬁer

14

Tianwei Lin et al.

Fig. 5: Qualitative examples of proposals generated by BSN on THUMOS14 (top
and middle) and ActivityNet-1.3 (bottom), where proposals are retrieved using post-
processed conﬁdence score.

The comparison results of ActivityNet-1.3 shown in Table 5 suggest that detec-
tion framework based on our proposals outperforms other state-of-the-art methods. The
comparison results of THUMOS14 shown in Table 6 suggest that (1) using same ac-
tion classiﬁer, our method achieves signiﬁcantly better performance than other proposal
generation methods; (2) comparing with proposal-level classiﬁer [7], video-level classi-
ﬁer [48] achieves better performance on BSN proposals and worse performance on [3]
and [6] proposals, which indicates that conﬁdence scores generated by BSN are more
reliable than scores generated by proposal-level classiﬁer, and are reliable enough for
retrieving detection results in action detection task; (3) detection framework based on
our proposals signiﬁcantly outperforms state-of-the-art action detection methods, espe-
cially when the overlap threshold is high. These results conﬁrm that proposals generated
by BSN have high quality and work generally well in detection frameworks.

5 Conclusion

In this paper, we have introduced the Boundary-Sensitive Network (BSN) for tempo-
ral action proposal generation. Our method can generate proposals with ﬂexible dura-
tions and precise boundaries via directly combing locations with high boundary prob-
abilities, and make accurate retrieving via evaluating proposal conﬁdence score with
proposal-level features. Thus BSN can achieve high recall and high temporal overlap
with relatively few proposals. In experiments, we demonstrate that BSN signiﬁcantly
outperforms other state-of-the-art proposal generation methods on both THUMOS14
and ActivityNet-1.3 datasets. And BSN can signiﬁcantly improve the detection perfor-
mance when used as the proposal stage of a full detection framework.

Boundary Sensitive Network

15

References

1. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A large-scale
video benchmark for human activity understanding. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. (2015) 961–970

2. Jiang, Y.G., Liu, J., Zamir, A.R., Toderici, G., Laptev, I., Shah, M., Sukthankar, R.: Thumos
challenge: Action recognition with a large number of classes. In: ECCV Workshop. (2014)
3. Buch, S., Escorcia, V., Shen, C., Ghanem, B., Niebles, J.C.: SST: Single-stream temporal

action proposals. In: IEEE International Conference on Computer Vision. (2017)

4. Caba Heilbron, F., Carlos Niebles, J., Ghanem, B.: Fast temporal activity proposals for
In: Proceedings of the IEEE

efﬁcient detection of human actions in untrimmed videos.
Conference on Computer Vision and Pattern Recognition. (2016) 1914–1923

5. Escorcia, V., Heilbron, F.C., Niebles, J.C., Ghanem, B.: Daps: Deep action proposals for
action understanding. In: European Conference on Computer Vision, Springer (2016) 768–
784

6. Gao, J., Yang, Z., Sun, C., Chen, K., Nevatia, R.: Turn tap: Temporal unit regression network

for temporal action proposals. arXiv preprint arXiv:1703.06189 (2017)

7. Shou, Z., Wang, D., Chang, S.F.: Temporal action localization in untrimmed videos via
multi-stage cnns. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2016) 1049–1058

8. Wang, H., Kl¨aser, A., Schmid, C., Liu, C.L.: Action recognition by dense trajectories. In:
Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, IEEE (2011)
3169–3176

9. Wang, H., Schmid, C.: Action recognition with improved trajectories. In: Proceedings of the

IEEE International Conference on Computer Vision. (2013) 3551–3558

10. Feichtenhofer, C., Pinz, A., Zisserman, A.: Convolutional two-stream network fusion for
video action recognition. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2016) 1933–1941

11. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recognition in

videos. In: Advances in Neural Information Processing Systems. (2014) 568–576

12. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotemporal features
with 3d convolutional networks. In: Proceedings of the IEEE International Conference on
Computer Vision. (2015) 4489–4497

13. Wang, L., Xiong, Y., Wang, Z., Qiao, Y.: Towards good practices for very deep two-stream

convnets. arXiv preprint arXiv:1507.02159 (2015)

14. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object
detection and semantic segmentation. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. (2014) 580–587

15. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference on Computer

Vision. (2015) 1440–1448

16. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with
region proposal networks. In: Advances in neural information processing systems. (2015)
91–99

17. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection with dis-
criminatively trained part-based models. IEEE transactions on pattern analysis and machine
intelligence 32(9) (2010) 1627–1645

18. Uijlings, J.R., van de Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search for object

recognition. International journal of computer vision 104(2) (2013) 154–171

19. Zitnick, C.L., Doll´ar, P.: Edge boxes: Locating object proposals from edges. In: European

Conference on Computer Vision, Springer (2014) 391–405

16

Tianwei Lin et al.

20. Kuo, W., Hariharan, B., Malik, J.: Deepbox: Learning objectness with convolutional net-
works. In: Proceedings of the IEEE International Conference on Computer Vision. (2015)
2479–2487

21. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid

networks for object detection. arXiv preprint arXiv:1612.03144 (2016)

22. Gidaris, S., Komodakis, N.: Locnet: Improving localization accuracy for object detection. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2016)
789–798

23. Singh, G., Cuzzolin, F.: Untrimmed video classiﬁcation for activity detection: submission to

activitynet challenge. arXiv preprint arXiv:1607.01979 (2016)

24. Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Lin, D., Tang, X.: Temporal action detection with

structured segment networks. arXiv preprint arXiv:1704.06228 (2017)

25. Lin, T., Zhao, X., Shou, Z.: Single shot temporal action detection. In: Proceedings of the

25nd ACM international conference on Multimedia. (2017)

26. Buch, S., Escorcia, V., Ghanem, B., Fei-Fei, L., Niebles, J.C.: End-to-end, single-stream
In: Proceedings of the British Machine

temporal action detection in untrimmed videos.
Vision Conference. (2017)

27. Karaman, S., Seidenari, L., Del Bimbo, A.: Fast saliency based pooling of ﬁsher encoded

dense trajectories. In: ECCV THUMOS Workshop. (2014)

28. Oneata, D., Verbeek, J., Schmid, C.: The lear submission at thumos 2014. ECCV THUMOS

Workshop (2014)

29. Wang, L., Qiao, Y., Tang, X.: Action recognition and detection by combining motion and

appearance features. THUMOS14 Action Recognition Challenge 1 (2014) 2

30. Yuan, Z., Stroud, J.C., Lu, T., Deng, J.: Temporal action localization by structured maximal

sums. arXiv preprint arXiv:1704.04671 (2017)

31. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment
networks: towards good practices for deep action recognition. In: European Conference on
Computer Vision, Springer (2016) 20–36

32. Gao, J., Yang, Z., Nevatia, R.: Cascaded boundary regression for temporal action detection.

arXiv preprint arXiv:1705.01180 (2017)

33. Bodla, N., Singh, B., Chellappa, R., Davis, .L.S.: Improving object detection with one line

of code. arXiv preprint arXiv:1704.04503 (2017)

34. Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes from

videos in the wild. arXiv preprint arXiv:1212.0402 (2012)

35. Xiong, Y., Wang, L., Wang, Z., Zhang, B., Song, H., Li, W., Lin, D., Qiao, Y., Gool, L.V.,
Tang, X.: Cuhk & ethz & siat submission to activitynet challenge 2016. arXiv preprint
arXiv:1608.00797 (2016)

37. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

36. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In: International Conference on Machine Learning. (2015) 448–456
In:
Proceedings of the IEEE conference on computer vision and pattern recognition. (2016)
770–778

38. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.,
Darrell, T.: Caffe: Convolutional architecture for fast feature embedding. In: Proceedings of
the 22nd ACM international conference on Multimedia, ACM (2014) 675–678

39. Lin, T., Zhao, X., Shou, Z.: Temporal convolution based action proposal: Submission to

activitynet 2017. arXiv preprint arXiv:1707.06750 (2017)

40. Abadi, M., Agarwal, A., Barham, P., et al.: Tensorﬂow: Large-scale machine learning on

heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016)
41. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection.
Conference on Computer Vision and Pattern Recognition. (2005) 886–893

In: IEEE

Boundary Sensitive Network

17

42. Dai, X., Singh, B., Zhang, G., Davis, L.S., Chen, Y.Q.: Temporal context network for activity
localization in videos. In: 2017 IEEE International Conference on Computer Vision (ICCV),
IEEE (2017) 5727–5736

43. Ghanem, B., Niebles, J.C., Snoek, C., Heilbron, F.C., Alwassel, H., Khrisna, R., Escorcia, V.,
Hata, K., Buch, S.: Activitynet challenge 2017 summary. arXiv preprint arXiv:1710.08011
(2017)

44. Tran, D., Ray, J., Shou, Z., Chang, S.F., Paluri, M.: Convnet architecture search for spa-

tiotemporal feature learning. arXiv preprint arXiv:1708.05038 (2017)

45. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale
video classiﬁcation with convolutional neural networks. In: Proceedings of the IEEE confer-
ence on Computer Vision and Pattern Recognition. (2014) 1725–1732

46. Zhao, Y., Zhang, B., Wu, Z., Yang, S., Zhou, L., Yan, S., Wang, L., Xiong, Y., Lin, D., Qiao,
Y., Tang, X.: Cuhk & ethz & siat submission to activitynet challenge 2017. arXiv preprint
arXiv:1710.08011 (2017)

47. Wang, R., Tao, D.: Uts at activitynet 2016. AcitivityNet Large Scale Activity Recognition

Challenge 2016 (2016) 8

48. Wang, L., Xiong, Y., Lin, D., Van Gool, L.: Untrimmednets for weakly supervised action

recognition and detection. arXiv preprint arXiv:1703.03329 (2017)

49. Heilbron, F.C., Barrios, W., Escorcia, V., Ghanem, B.: Scc: Semantic context cascade for
efﬁcient action detection. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). Volume 2. (2017)

50. Shou, Z., Chan, J., Zareian, A., Miyazawa, K., Chang, S.F.: Cdc: Convolutional-de-
convolutional networks for precise temporal action localization in untrimmed videos. arXiv
preprint arXiv:1703.01515 (2017)

51. Xiong, Y., Zhao, Y., Wang, L., Lin, D., Tang, X.: A pursuit of temporal accuracy in general

activity detection. arXiv preprint arXiv:1703.02716 (2017)

52. Xu, H., Das, A., Saenko, K.: R-c3d: Region convolutional 3d network for temporal activity

detection. arXiv preprint arXiv:1703.07814 (2017)

8
1
0
2
 
p
e
S
 
6
2
 
 
]

V
C
.
s
c
[
 
 
3
v
4
6
9
2
0
.
6
0
8
1
:
v
i
X
r
a

BSN: Boundary Sensitive Network for Temporal Action
Proposal Generation

Tianwei Lin1, Xu Zhao(cid:63)1, Haisheng Su1, Chongjing Wang2, Ming Yang1

1Department of Automation, Shanghai Jiao Tong University, China
2China Academy of Information and Communications Technology, China
wzmsltw,zhaoxu,suhaisheng,mingyang@sjtu.edu.cn
wangchongjing@caict.ac.cn

Abstract. Temporal action proposal generation is an important yet challeng-
ing problem, since temporal proposals with rich action content are indispens-
able for analysing real-world videos with long duration and high proportion ir-
relevant content. This problem requires methods not only generating proposals
with precise temporal boundaries, but also retrieving proposals to cover truth
action instances with high recall and high overlap using relatively fewer pro-
posals. To address these difﬁculties, we introduce an effective proposal genera-
tion method, named Boundary-Sensitive Network (BSN), which adopts “local to
global” fashion. Locally, BSN ﬁrst locates temporal boundaries with high prob-
abilities, then directly combines these boundaries as proposals. Globally, with
Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the
conﬁdence of whether a proposal contains an action within its region. We con-
duct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14,
where BSN outperforms other state-of-the-art temporal action proposal genera-
tion methods with high recall and high temporal precision. Finally, further ex-
periments demonstrate that by combining existing action classiﬁers, our method
signiﬁcantly improves the state-of-the-art temporal action detection performance.

Keywords: Temporal action proposal generation · Temporal action detection ·
Temporal convolution · Untrimmed video

1 Introduction

Nowadays, with fast development of digital cameras and Internet, the number of videos
is continuously booming, making automatic video content analysis methods widely re-
quired. One major branch of video analysis is action recognition, which aims to classify
manually trimmed video clips containing only one action instance. However, videos in
real scenarios are usually long, untrimmed and contain multiple action instances along
with irrelevant contents. This problem requires algorithms for another challenging task:
temporal action detection, which aims to detect action instances in untrimmed video
including both temporal boundaries and action classes. It can be applied in many areas
such as video recommendation and smart surveillance.

Similar with object detection in spatial domain, temporal action detection task can
be divided into two stages: proposal and classiﬁcation. Proposal generation stage aims

(cid:63) Corresponding author.

2

Tianwei Lin et al.

Fig. 1: Overview of our approach. Given an untrimmed video, (1) we evaluate bound-
aries and actionness probabilities of each temporal location and generate proposals
based on boundary probabilities, and (2) we evaluate the conﬁdence scores of proposals
with proposal-level feature to get retrieved proposals.

to generate temporal video regions which may contain action instances, and classiﬁca-
tion stage aims to classify classes of candidate proposals. Although classiﬁcation meth-
ods have reached convincing performance, the detection precision is still low in many
benchmarks [1,2]. Thus recently temporal action proposal generation has received much
attention [3,4,5,6], aiming to improve the detection performance by improving the qual-
ity of proposals. High quality proposals should come up with two key properties: (1)
proposals can cover truth action regions with both high recall and high temporal over-
lap, (2) proposals are retrieved so that high recall and high overlap can be achieved
using fewer proposals to reduce the computation cost of succeeding steps.

To achieve high proposal quality, a proposal generation method should generate pro-
posals with ﬂexible temporal durations and precise temporal boundaries, then retrieve
proposals with reliable conﬁdence scores, which indicate the probability of a proposal
containing an action instance. Most recently proposal generation methods [3,4,5,7] gen-
erate proposals via sliding temporal windows of multiple durations in video with regular
interval, then train a model to evaluate the conﬁdence scores of generated proposals for
proposals retrieving, while there is also method [6] making external boundaries regres-
sion. However, proposals generated with pre-deﬁned durations and intervals may have
some major drawbacks: (1) usually not temporally precise; (2) not ﬂexible enough to
cover variable temporal durations of ground truth action instances, especially when the
range of temporal durations is large.

To address these issues and generate high quality proposals, we propose the Boundary-

Sensitive Network (BSN), which adopts “local to global” fashion to locally combine
high probability boundaries as proposals and globally retrieve candidate proposals us-
ing proposal-level feature as shown in Fig 1. In detail, BSN generates proposals in three
steps. First, BSN evaluates the probabilities of each temporal location in video whether
it is inside or outside, at or not at the boundaries of ground truth action instances, to
generate starting, ending and actionness probabilities sequences as local information.

Boundary Sensitive Network

3

Second, BSN generates proposals via directly combining temporal locations with high
starting and ending probabilities separately. Using this bottom-up fashion, BSN can
generate proposals with ﬂexible durations and precise boundaries. Finally, using fea-
tures composed by actionness scores within and around proposal, BSN retrieves pro-
posals by evaluating the conﬁdence of whether a proposal contains an action. These
proposal-level features offer global information for better evaluation.
In summary, the main contributions of our work are three-folds:
(1) We introduce a new architecture (BSN) based on “local to global” fashion to
generate high quality temporal action proposals, which locally locates high bound-
ary probability locations to achieve precise proposal boundaries and globally evaluates
proposal-level feature to achieve reliable proposal conﬁdence scores for retrieving.

(2) Extensive experiments demonstrate that our method achieves signiﬁcantly bet-
ter proposal quality than other state-of-the-art proposal generation methods, and can
generate proposals in unseen action classes with comparative quality.

(3) Integrating our method with existing action classiﬁer into detection framework

leads to signiﬁcantly improved performance on temporal action detection task.

2 Related work

Action recognition. Action recognition is an important branch of video related research
areas and has been extensively studied. Earlier methods such as improved Dense Tra-
jectory (iDT) [8,9] mainly adopt hand-crafted features such as HOF, HOG and MBH. In
recent years, convolutional networks are widely adopted in many works [10,11,12,13]
and have achieved great performance. Typically, two-stream network [10,11,13] learns
appearance and motion features based on RGB frame and optical ﬂow ﬁeld separately.
C3D network [12] adopts 3D convolutional layers to directly capture both appearance
and motion features from raw frames volume. Action recognition models can be used
for extracting frame or snippet level visual features in long and untrimmed videos.
Object detection and proposals. Recent years, the performance of object detection has
been signiﬁcantly improved with deep learning methods. R-CNN [14] and its variations
[15,16] construct an important branch of object detection methods, which adopt “detec-
tion by classifying proposals” framework. For proposal generation stage, besides sliding
windows [17], earlier works also attempt to generate proposals by exploiting low-level
cues such as HOG and Canny edge [18,19]. Recently some methods [16,20,21] adopt
deep learning model to generate proposals with faster speed and stronger modelling ca-
pacity. In this work, we combine the properties of these methods via evaluating bound-
aries and actionness probabilities of each location using neural network and adopting
“local to global” fashion to generate proposals with high recall and accuracy.

Boundary probabilities are also adopted in LocNet [22] for revising the horizon-
tal and vertical boundaries of existing proposals. Our method differs in (1) BSN aims
to generate while LocNet aims to revise proposals and (2) boundary probabilities are
calculated repeatedly for all boxes in LocNet but only once for a video in BSN.
Temporal action detection and proposals. Temporal action detection task aims to
detect action instances in untrimmed videos including temporal boundaries and action
classes, and can be divided into proposal and classiﬁcation stages. Most detection meth-
ods [7,23,24] take these two stages separately, while there is also method [25,26] taking

4

Tianwei Lin et al.

these two stages jointly. For proposal generation, earlier works [27,28,29] directly use
sliding windows as proposals. Recently some methods [3,4,5,6,7] generate proposals
with pre-deﬁned temporal durations and intervals, and use multiple methods to evaluate
the conﬁdence score of proposals, such as dictionary learning [4] and recurrent neural
network [5]. TAG method [24] adopts watershed algorithm to generate proposals with
ﬂexible boundaries and durations in local fashion, but without global proposal-level
conﬁdence evaluation for retrieving. In our work, BSN can generate proposals with
ﬂexible boundaries meanwhile reliable conﬁdence scores for retrieving.

Recently temporal action detection method [30] detects action instances based on
class-wise start, middle and end probabilities of each location. Our method is superior
than [30] in two aspects: (1) BSN evaluates probabilities score using temporal convolu-
tion to better capture temporal information and (2) “local to global” fashion adopted in
BSN brings more precise boundaries and better retrieving quality.

3 Our Approach

3.1 Problem Deﬁnition

An untrimmed video sequence can be denoted as X = {xn}lv
n=1 with lv frames, where
xn is the n-th frame in X. Annotation of video X is composed by a set of action in-
stances Ψg = {ϕn = (ts,n, te,n)}Ng
n=1, where Ng is the number of truth action instances
in video X, and ts,n, te,n are starting and ending time of action instance ϕn separately.
Unlike detection task, classes of action instances are not considered in temporal ac-
tion proposal generation. Annotation set Ψg is used during training. During prediction,
generated proposals set Ψp should cover Ψg with high recall and high temporal overlap.

3.2 Video Features Encoding

To generate proposals of input video, ﬁrst we need to extract feature to encode visual
content of video. In our framework, we adopt two-stream network [11] as visual en-
coder, since this architecture has shown great performance in action recognition task
[31] and has been widely adopted in temporal action detection and proposal generation
tasks [24,25,32]. Two-stream network contains two branches: spatial network operates
on single RGB frame to capture appearance feature, and temporal network operates on
stacked optical ﬂow ﬁeld to capture motion information.

To extract two-stream features, as shown in Fig 2(a), ﬁrst we compose a snippets
sequence S = {sn}ls
n=1 from video X, where ls is the length of snippets sequence. A
snippet sn = (xtn , otn) includes two parts: xtn is the tn-th RGB frame in X and otn is
stacked optical ﬂow ﬁeld derived around center frame xtn . To reduce the computation
cost, we extract snippets with a regular frame interval σ, therefore ls = lv/σ. Given
a snippet sn, we concatenate output scores in top layer of both spatial and temporal
networks to form the encoded feature vector ftn = (fS,tn , fT,tn ), where fS,tn , fT,tn
are output scores from spatial and temporal networks separately. Thus given a snippets
sequence S with length ls, we can extract a feature sequence F = {ftn }ls
n=1. These
two-stream feature sequences are used as the input of BSN.

Boundary Sensitive Network

5

Fig. 2: The framework of our approach. (a) Two-stream network is used for encoding
visual features in snippet-level. (b) The architecture of Boundary-Sensitive Network:
temporal evaluation module handles the input feature sequence, and evaluates start-
ing, ending and actionness probabilities of each temporal location; proposal genera-
tion module generates proposals with high starting and ending probabilities, and con-
struct Boundary-Sensitive Proposal (BSP) feature for each proposal; proposal evalua-
tion module evaluates conﬁdence score of each proposal using BSP feature. (c) Finally,
we use Soft-NMS algorithm to suppress redundant proposals by decaying their scores.

3.3 Boundary-Sensitive Network

To achieve high proposal quality with both precise temporal boundaries and reliable
conﬁdence scores, we adopt “local to global” fashion to generate proposals. In BSN, we
ﬁrst generate candidate boundary locations, then combine these locations as proposals
and evaluate conﬁdence score of each proposal with proposal-level feature.
Network architecture. The architecture of BSN is presented in Fig 2(b), which con-
tains three modules: temporal evaluation, proposal generation and proposal evaluation.
Temporal evaluation module is a three layers temporal convolutional neural network,
which takes the two-stream feature sequences as input, and evaluates probabilities of
each temporal location in video whether it is inside or outside, at or not at boundaries
of ground truth action instances, to generate sequences of starting, ending and action-
ness probabilities respectively. Proposal generation module ﬁrst combines the temporal
locations with separately high starting and ending probabilities as candidate proposals,
then constructs Boundary-Sensitive Proposal (BSP) feature for each candidate proposal
based on actionness probabilities sequence. Finally, proposal evaluation module, a mul-
tilayer perceptron model with one hidden layer, evaluates the conﬁdence score of each
candidate proposal based on BSP feature. Conﬁdence score and boundary probabilities
of each proposal are fused as the ﬁnal conﬁdence score for retrieving.
Temporal evaluation module. The goal of temporal evaluation module is to evaluate
starting, ending and actionness probabilities of each temporal location, where three bi-
nary classiﬁers are needed. In this module, we adopt temporal convolutional layers upon

6

Tianwei Lin et al.

(a) Generate proposals

(b) Construct BSP feature

Fig. 3: Details of proposal generation module. (a) Generate proposals. First, to generate
candidate boundary locations, we choose temporal locations with high boundary prob-
ability or being a probability peak. Then, we combine candidate starting and ending
locations as proposals when their duration satisfying condition. (b) Construct BSP fea-
ture. Given a proposal and actionness probabilities sequence, we can sample actionness
sequence in starting, center and ending regions of proposal to construct BSP feature.

feature sequence, with good modelling capacity to capture local semantic information
such as boundaries and actionness probabilities.

A temporal convolutional layer can be simply denoted as Conv(cf , ck, Act), where
cf , ck and Act are ﬁlter numbers, kernel size and activation function of temporal con-
volutional layer separately. As shown in Fig 2(b), the temporal evaluation module can
be deﬁned as Conv(512, 3, Relu) → Conv(512, 3, Relu) → Conv(3, 1, Sigmoid),
where the three layers have same stride size 1. Three ﬁlters with sigmoid activation in
the last layer are used as classiﬁers to generate starting, ending and actionness prob-
abilities separately. For convenience of computation, we divide feature sequence into
non-overlapped windows as the input of temporal evaluation module. Given a fea-
ture sequence F , temporal evaluation module can generate three probability sequences
PS = (cid:8)ps
(cid:9)ls
n=1 and PA = (cid:8)pa
(cid:9)ls
n=1, where ps
tn are
tn
respectively starting, ending and actionness probabilities in time tn.
Proposal generation module. The goal of proposal generation module is to generate
candidate proposals and construct corresponding proposal-level feature. We achieve this
goal in two steps. First we locate temporal locations with high boundary probabilities,
and combine these locations to form proposals. Then for each proposal, we construct
Boundary-Sensitive Proposal (BSP) feature.

n=1, PE = (cid:8)pe
(cid:9)ls

tn and pa

tn, pe

tn

tn

> 0.9 or (2) is a probability peak: ps
tn

As shown in Fig 3(a), to locate where an action likely to start, for starting proba-
bilities sequence PS, we record all temporal location tn where ps
tn (1) has high score:
> ps
ps
tn+1. These locations
tn
are grouped into candidate starting locations set BS = {ts,i}NS
i=1, where NS is the num-
ber of candidate starting locations. Using same rules, we can generate candidate ending
locations set BE from ending probabilities sequence PE. Then, we generate temporal
regions via combing each starting location ts from BS and each ending location te from
BS. Any temporal region [ts, te] satisfying d = te − ts ∈ [dmin, dmax] is denoted as
a candidate proposal ϕ, where dmin and dmax are minimum and maximum durations

tn−1 and ps
tn

> ps

Boundary Sensitive Network

7

of ground truth action instances in dataset. Thus we can get candidate proposals set
Ψp = {ϕi}Np

i=1, where Np is the number of proposals.

To construct proposal-level feature as shown in Fig 3(b), for a candidate proposal
ϕ, we denote its center region as rC = [ts, te] and its starting and ending region as
rS = [ts − d/5, ts + d/5] and rE = [te − d/5, te + d/5] separately. Then, we sample
the actionness sequence PA within rc as f A
c by linear interpolation with 16 points. In
starting and ending regions, we also sample actionness sequence with 8 linear interpo-
s and f A
lation points and get f A
e separately. Concatenating these vectors, we can get
Boundary-Sensitive Proposal (BSP) feature fBSP = (f A
s ,f A
e ) of proposal ϕ. BSP
feature is highly compact and contains rich semantic information about corresponding
proposal. Then we can represent a proposal as ϕ = (ts, te, fBSP ).
Proposal evaluation module. The goal of proposal evaluation module is to evaluate
the conﬁdence score of each proposal whether it contains an action instance within
its duration using BSP feature. We adopt a simple multilayer perceptron model with
one hidden layer as shown in Fig 2(b). Hidden layer with 512 units handles the input
of BSP feature fBSP with Relu activation. The output layer outputs conﬁdence score
pconf with sigmoid activation, which estimates the overlap extent between candidate
proposal and ground truth action instances. Thus, a generated proposal can be denoted
as ϕ = (ts, te, pconf , ps
ts and pe
te are starting and ending probabilities
ts
in ts and te separately. These scores are fused to generate ﬁnal score during prediction.

), where ps

c ,f A

, pe
te

3.4 Training of BSN

In BSN, temporal evaluation module is trained to learn local boundary and actionness
probabilities from video features simultaneously. Then based on probabilities sequence
generated by trained temporal evaluation module, we can generate proposals and corre-
sponding BSP features and train the proposal evaluation module to learn the conﬁdence
score of proposals. The training details are introduced in this section.
Temporal evaluation module. Given a video X, we compose a snippets sequence S
with length ls and extract feature sequence F from it. Then we slide windows with
length lw = 100 in feature sequence without overlap. A window is denoted as ω =
{Fω, Ψω}, where Fω and Ψω are feature sequence and annotations within the window
separately. For ground truth action instance ϕg = (ts, te) in Ψω, we denote its region
as action region ra
g = [ts − dg/10, ts + dg/10]
and re

g = [te − dg/10, te + dg/10] separately, where dg = te − ts.

g and its starting and ending region as rs

tn , pe

tn and pa

Taking Fω as input, temporal evaluation module generates probabilities sequence
PS,ω, PE,ω and PA,ω with same length lw. For each temporal location tn within Fω,
we denote its region as rtn = [tn − ds/2, tn + ds/2] and get corresponding probability
scores ps
tn from PS,ω, PE,ω and PA,ω separately, where ds = tn − tn−1
is temporal interval between two snippets. Then for each rtn , we calculate its IoP ra-
tio with ra
g of all ϕg in Ψω separately, where IoP is deﬁned as the overlap
ratio with groundtruth proportional to the duration of this proposal. Thus we can rep-
resent information of tn as φn = (pa
tn , ge
, ps
tn are
tn
tn
maximum matching overlap IoP of action, starting and ending regions separately.

), where ga

g and re

tn , gs

g , rs

, pe
tn

, ge
tn

, ga
tn

, gs
tn

8

Tianwei Lin et al.

Given a window of matching information as Φω = {φn}ls

n=1, we can deﬁne training
objective of this module as a three-task loss function. The overall loss function consists
of actionness loss, starting loss and ending loss:

(1)

(2)

LT EM = λ · Laction

bl

+ Lstart

bl + Lend

bl

,

where λ is the weight term and is set to 2 in BSN. We adopt the sum of binary logistic
regression loss function Lbl for all three tasks, which can be denoted as:

Lbl =

(cid:0)α+ · bi · log(pi) + α− · (1 − bi) · log(1 − pi)(cid:1) ,

1
lw

lw(cid:88)

i=1

where bi = sign(gi − θIoP ) is a two-values function for converting matching score
gi to {0, 1} based on threshold θIoP , which is set to 0.5 in BSN. Let l+ = (cid:80) gi and
l− = lw − l+, we can set α+ = lw
l− and α− = lw
l+ , which are used for balancing the
effect of positive and negative samples during training.
Proposal evaluation module. Using probabilities sequences generated by trained tem-
poral evaluation module, we can generate proposals using proposal generation mod-
ule: Ψp = {ϕn = (ts, te, fBSP )}Np
n=1. Taking fBSP as input, for a proposal ϕ, con-
ﬁdence score pconf is generated by proposal evaluation module. Then we calculate its
Intersection-over-Union (IoU) with all ϕg in Ψg, and denote the maximum overlap score
as giou. Thus we can represent proposals set as Ψp = {ϕn = {ts, te, pconf , giou}}Np
n=1.
We split Ψp into two parts based on giou: Ψ pos
for giou < 0.3.
p
p
For data balancing, we take all proposals in Ψ pos
and randomly sample the proposals in
p
Ψ neg
p

to insure the ratio between two sets be nearly 1:2.

for giou > 0.7 and Ψ neg

The training objective of this module is a simple regression loss, which is used to

train a precise conﬁdence score prediction based on IoU overlap. We can deﬁne it as:

LP EM =

(pconf,i − giou,i)2,

(3)

1
Ntrain

Ntrain(cid:88)

i=1

where Ntrain is the number of proposals used for training.

3.5 Prediction and Post-processing

During prediction, we use BSN with same procedures described in training to generate
proposals set Ψp = (cid:8)ϕn = (ts, te, pconf , ps
n=1, where Np is the number of pro-
ts
posals. To get ﬁnal proposals set, we need to make score fusion to get ﬁnal conﬁdence
score, then suppress redundant proposals based on these score.
Score fusion for retrieving. To achieve better retrieving performance, for each candi-
date proposal ϕ, we fuse its conﬁdence score with its boundary probabilities by multi-
plication to get the ﬁnal conﬁdence score pf :

)(cid:9)Np

, pe
te

pf = pconf · ps
ts

· pe
te

.

(4)

After score fusion, we can get generated proposals set Ψp = {ϕn = (ts, te, pf )}Np
n=1,
where pf is used for proposals retrieving. In section 4.2, we explore the recall perfor-
mance with and without conﬁdence score generated by proposal evaluation module.

Boundary Sensitive Network

9

Redundant proposals suppression. Around a ground truth action instance, we may
generate multiple proposals with different temporal overlap. Thus we need to suppress
redundant proposals to obtain higher recall with fewer proposals.

Soft-NMS [33] is a recently proposed non-maximum suppression (NMS) algorithm
which suppresses redundant results using a score decaying function. First all proposals
are sorted by their scores. Then proposal ϕm with maximum score is used for calcu-
lating overlap IoU with other proposals, where scores of highly overlapped proposals
is decayed. This step is recursively applied to the remaining proposals to generate re-
scored proposals set. The Gaussian decaying function of Soft-NMS can be denoted as:

p(cid:48)
f,i =

(cid:40)
pf,i,
pf,i · e− iou(ϕm ,ϕi)2

ε

iou(ϕm, ϕi) < θ
, iou(ϕm, ϕi) ≥ θ

(5)

where ε is parameter of Gaussian function and θ is pre-ﬁxed threshold. After suppres-
sion, we get the ﬁnal proposals set Ψ (cid:48)

ϕn = (ts, te, p(cid:48)

(cid:111)Np

(cid:110)

.

p =

f )

n=1

4 Experiments

4.1 Dataset and setup

Dataset. ActivityNet-1.3 [1] is a large dataset for general temporal action proposal gen-
eration and detection, which contains 19994 videos with 200 action classes annotated
and was used in the ActivityNet Challenge 2016 and 2017. ActivityNet-1.3 is divided
into training, validation and testing sets by ratio of 2:1:1. THUMOS14 [2] dataset con-
tains 200 and 213 temporal annotated untrimmed videos with 20 action classes in vali-
dation and testing sets separately. The training set of THUMOS14 is the UCF-101 [34],
which contains trimmed videos for action recognition task. In this section, we compare
our method with state-of-the-art methods on both ActivityNet-1.3 and THUMOS14.
Evaluation metrics. In temporal action proposal generation task, Average Recall (AR)
calculated with multiple IoU thresholds is usually used as evaluation metrics. Follow-
ing conventions, we use IoU thresholds set [0.5 : 0.05 : 0.95] in ActivityNet-1.3 and
[0.5 : 0.05 : 1.0] in THUMOS14. To evaluate the relation between recall and proposals
number, we evaluate AR with Average Number of proposals (AN) on both datasets,
which is denoted as AR@AN. On ActivityNet-1.3, area under the AR vs. AN curve
(AUC) is also used as metrics, where AN varies from 0 to 100.

In temporal action detection task, mean Average Precision (mAP) is used as evalua-
tion metric, where Average Precision (AP) is calculated on each action class respec-
tively. On ActivityNet-1.3, mAP with IoU thresholds {0.5, 0.75, 0.95} and average
mAP with IoU thresholds set [0.5 : 0.05 : 0.95] are used. On THUMOS14, mAP
with IoU thresholds {0.3, 0.4, 0.5, 0.6, 0.7} is used.
Implementation details. For visual feature encoding, we use the two-stream network
[11] with architecture described in [35], where BN-Inception network [36] is used as
temporal network and ResNet network [37] is used as spatial network. Two-stream
network is implemented using Caffe [38] and pre-trained on ActivityNet-1.3 training
set. During feature extraction, the interval σ of snippets is set to 16 on ActivityNet-1.3
and is set to 5 on THUMOS14.

10

Tianwei Lin et al.

Fig. 4: Comparison of our proposal generation method with other state-of-the-art meth-
ods in THUMOS14 dataset. (left) BSN can achieve signiﬁcant performance gains with
relatively few proposals. (center) Recall with 100 proposals vs tIoU ﬁgure shows that
with few proposals, BSN gets performance improvements in both low and high tIoU.
(right) Recall with 1000 proposals vs tIoU ﬁgure shows that with large number of pro-
posals, BSN achieves improvements mainly while tIoU > 0.8.

On ActivityNet-1.3, since the duration of videos are limited, we follow [39] to
rescale the feature sequence of each video to new length lw = 100 by linear interpo-
lation, and the duration of corresponding annotations to range [0,1]. In BSN, temporal
evaluation module and proposal evaluation module are both implemented using Tensor-
ﬂow [40]. On both datasets, temporal evaluation module is trained with batch size 16
and learning rate 0.001 for 10 epochs, then 0.0001 for another 10 epochs, and proposal
evaluation module is trained with batch size 256 and same learning rate. For Soft-NMS,
we set the threshold θ to 0.8 on ActivityNet-1.3 and 0.65 on THUMOS14 by empirical
validation, while ε in Gaussian function is set to 0.75 on both datasets.

4.2 Temporal Proposal Generation

Taking a video as input, proposal generation method aims to generate temporal propos-
als where action instances likely to occur. In this section, we compare our method with
state-of-the-art methods and make external experiments to verify effectiveness of BSN.
Comparison with state-of-the-art methods. As aforementioned, a good proposal gen-
eration method should generate and retrieve proposals to cover ground truth action in-
stances with high recall and high temporal overlap using relatively few proposals. We
evaluate these methods in two aspects.

First we evaluate the ability of our method to generate and retrieve proposals with
high recall, which is measured by average recall with different number of proposals
(AR@AN) and area under AR-AN curve (AUC). We list the comparison results of
ActivityNet-1.3 and THUMOS14 in Table 1 and Table 2 respectively, and plot the av-
erage recall against average number of proposals curve of THUMOS14 in Fig 4 (left).
On THUMOS14, our method outperforms other state-of-the-art proposal methods when
proposal number varies from 10 to 1000. Especially, when average number of propos-
als is 50, our method signiﬁcantly improves average recall from 21.86% to 37.46% by
15.60%. On ActivityNet-1.3, our method outperforms other state-of-the-art proposal
generation methods on both validation and testing set.

Second, we evaluate the ability of our method to generate and retrieve proposals
with high temporal overlap, which is measured by recall of multiple IoU thresholds.
We plot the recall against IoU thresholds curve with 100 and 1000 proposals in Fig 4

Boundary Sensitive Network

11

Table 1: Comparison between our method with other state-of-the-art proposal genera-
tion methods on validation set of ActivityNet-1.3 in terms of AR@AN and AUC.
BSN
74.16
66.17
66.26

Method
AR@100 (val)
AUC (val)
AUC (test)

Zhao et al. [24] Dai et al. [42] Yao et al. [43] Lin et al. [39]

73.01
64.40
64.80

-
59.58
61.56

63.52
53.02
-

-
63.12
64.18

Table 2: Comparison between our method with other state-of-the-art proposal genera-
tion methods on THUMOS14 in terms of AR@AN.

Feature
C3D
C3D
C3D
C3D
C3D
C3D

Method
DAPs [5]
SCNN-prop [7]
SST [3]
TURN [6]
BSN + Greedy-NMS
BSN + Soft-NMS

Flow

2-Stream TAG [24]
TURN [6]
2-Stream BSN + Greedy-NMS
2-Stream BSN + Soft-NMS

@50
13.56
17.22
19.90
19.63
27.19
29.58

18.55
21.86
35.41
37.46

@100 @200 @500 @1000
57.64
23.83
58.20
26.17
60.27
28.36
60.75
27.96
59.50
35.38
37.38
59.48

49.29
51.57
51.58
53.52
53.77
54.67

33.96
37.01
37.90
38.34
43.61
45.55

29.00
31.89
43.55
46.06

39.61
43.02
52.23
53.21

-
57.63
61.35
60.64

-
64.17
65.10
64.52

(center) and (right) separately. Fig 4 (center) suggests that our method achieves signif-
icant higher recall than other methods with 100 proposals when IoU threshold varied
from 0.5 to 1.0. And Fig 4 (right) suggests that with 1000 proposals, our method obtains
the largest recall improvements when IoU threshold is higher than 0.8.

Furthermore, we make some controlled experiments to conﬁrm the contribution of
BSN itself in Table 2. For video feature encoding, except for two-stream network, C3D
network [12] is also adopted in some works [3,5,6,7]. For NMS method, most previ-
ous work adopt Greedy-NMS [41] for redundant proposals suppression. Thus, for fair
comparison, we train BSN with feature extracted by C3D network [12] pre-trained on
UCF-101 dataset, then perform Greedy-NMS and Soft-NMS on C3D-BSN and origi-
nal 2Stream-BSN respectively. Results in Table 2 show that (1) C3D-BSN still outper-
forms other C3D-based methods especially with small proposals number, (2) Soft-NMS
only brings small performance promotion than Greedy-NMS, while Greedy-NMS also
works well with BSN. These results suggest that the architecture of BSN itself is the
main reason for performance promotion rather than input feature and NMS method.

These results suggest the effectiveness of BSN. And BSN achieves the salient per-
formance since it can generate proposals with (1) ﬂexible temporal duration to cover
ground truth action instances with various durations; (2) precise temporal boundary via
learning starting and ending probability using temporal convolutional network, which
brings high overlap between generated proposals and ground truth action instances; (3)
reliable conﬁdence score using BSP feature, which retrieves proposals properly so that
high recall and high overlap can be achieved using relatively few proposals. Qualitative
examples on THUMOS14 and ActivityNet-1.3 datasets are shown in Fig 5.
Generalizability of proposals. Another key property of a proposal generation method
is the ability to generate proposals for unseen action classes. To evaluate this property,
we choose two semantically different action subsets on ActivityNet-1.3: “Sports, Ex-

12

Tianwei Lin et al.

Table 3: Generalization evaluation of BSN on ActivityNet-1.3. Seen subset: “Sports,
Exercise, and Recreation”; Unseen subset: “Socializing, Relaxing, and Leisure”.

BSN trained with Seen + Unseen (training)
BSN trained with Seen (training)

Seen (validation)
AUC
63.80
64.02

AR@100
72.40
72.42

Unseen (validation)
AUC
AR@100
63.99
71.84
63.38
71.32

ercise, and Recreation” and “Socializing, Relaxing, and Leisure” as seen and unseen
subsets separately. Seen subset contains 87 action classes with 4455 training and 2198
validation videos, and unseen subset contains 38 action classes with 1903 training and
896 validation videos. To guarantee the experiment effectiveness, instead of two-stream
network, here we adopt C3D network [44] trained on Sports-1M dataset [45] for video
features encoding. Using C3D feature, we train BSN with seen and seen+unseen videos
on training set separately, then evaluate both models on seen and unseen validation
videos separately. As shown in Table 3, there is only slight performance drop in unseen
classes, which demonstrates that BSN has great generalizability and can learn a generic
concept of temporal action proposal even in semantically different unseen actions.
Effectiveness of modules in BSN. To evaluate the effectiveness of temporal evalua-
tion module (TEM) and proposal evaluation module (PEM) in BSN, we demonstrate
experiment results of BSN with and without PEM in Table 4, where TEM is used in
both results. These results show that: (1) using only TEM without PEM, BSN can also
reach considerable recall performance over state-of-the-art methods; (2) PEM can bring
considerable further performance promotion in BSN. These observations suggest that
TEM and PEM are both effective and indispensable in BSN.
Boundary-Sensitive Proposal feature. BSP feature is used in proposal evaluation
module to evaluate the conﬁdence scores of proposals. In Table 4, we also make ab-
lation studies of the contribution of each component in BSP. These results suggest that
although BSP feature constructed from boundary regions contributes less improvements
than center region, best recall performance is achieved while PEM is trained with BSP
constructed from both boundary and center region.

4.3 Action Detection with Our Proposals

To further evaluate the quality of proposals generated by BSN, we put BSN propos-
als into “detection by classifying proposals” temporal action detection framework with
state-of-the-art action classiﬁer, where temporal boundaries of detection results are pro-
vided by our proposals. On ActivityNet-1.3, we use top-2 video-level class generated
by classiﬁcation model [46]1 for all proposals in a video. On THUMOS14, we use
top-2 video-level classes generated by UntrimmedNet [48] for proposals generated by
BSN and other methods.Following previous works, on THUMOS14, we also implement
SCNN-classiﬁer on BSN proposals for proposal-level classiﬁcation and adopt Greedy
NMS as [7]. We use 100 and 200 proposals per video on ActivityNet-1.3 and THU-
MOS14 datasets separately.

1 Previously, we adopted classiﬁcation results from result ﬁles of [47]. Recently we found that
the classiﬁcation accuracy of these results are unexpected high. Thus we replace it with clas-
siﬁcation results of [46] and updated all related experiments accordingly.

Boundary Sensitive Network

13

Table 4: Study of effectiveness of modules in BSN and contribution of components in
BSP feature on THUMOS14, where PEM is trained with BSP feature constructed by
Boundary region (f A

c ) independently and jointly.

e ) and Center region (f A

s , f A

BSN without PEM

BSN with PEM

Boundary Center

!

!

!
!

@50
30.72
35.61
36.80
37.46

@100 @200 @500 @1000
63.04
40.52
64.17
44.86
64.22
45.65
64.52
46.06

48.63
52.46
52.63
53.21

57.78
60.00
60.18
60.64

Table 5: Action detection results on validation and testing set of ActivityNet-1.3 in
terms of mAP@tIoU and average mAP, where our proposals are combined with video-
level classiﬁcation results generated by [46].

Method
Wang et al. [47]
SCC [49]
CDC [50]
TCN [42]
SSN [51]
Lin et al. [39]
BSN + [46]

validation

0.5
42.28
40.00
43.83
-
39.12
44.39
46.45

0.75
3.76
17.90
25.88
-
23.48
29.65
29.96

0.95
0.05
4.70
0.21
-
5.49
7.09
8.02

Average
14.85
21.70
22.77
-
23.98
29.17
30.03

testing
Average
14.62
19.30
22.90
23.58
28.28
32.26
32.84

Table 6: Action detection results on testing set of THUMOS14 in terms of mAP@tIoU ,
where classiﬁcation results generated by UntrimmedNet [48] and SCNN-classiﬁer [7]
are combined with proposals generated by BSN and other methods.

Action Detection Methods

Detection Method
SCNN [7]
SMS [30]
CDC [50]
SSAD [25]
TCN [42]
R-C3D [52]
SS-TAD [26]
SSN [51]
CBR [32]

Proposal method Classiﬁer
SCNN-cls
SST [3]
SCNN-cls
TURN [6]
UNet
SST [3]
UNet
TURN [6]
SCNN-cls
BSN
UNet
BSN

0.7
5.3
-
8.8
7.7
9.0
9.3
9.6
-
9.9

0.7
-
7.7
4.7
6.3
15.0
20.0

0.6
10.3
-
14.3
15.3
15.9
19.1
-
-
19.1

0.6
-
14.6
10.9
14.1
22.4
28.4

0.5
19.0
17.8
24.7
24.6
25.6
28.9
29.2
29.1
31.0

0.5
23.0
25.6
20.0
24.5
29.4
36.9

0.4
28.7
27.8
30.7
35.0
33.3
35.6
-
40.8
41.3

0.4
-
33.2
31.5
35.3
36.6
45.0

0.3
36.3
36.5
41.3
43.0
-
44.8
45.7
50.6
50.1

0.3
-
44.1
41.2
46.3
43.1
53.5

Proposal Generation Methods + Action Classiﬁer

14

Tianwei Lin et al.

Fig. 5: Qualitative examples of proposals generated by BSN on THUMOS14 (top
and middle) and ActivityNet-1.3 (bottom), where proposals are retrieved using post-
processed conﬁdence score.

The comparison results of ActivityNet-1.3 shown in Table 5 suggest that detec-
tion framework based on our proposals outperforms other state-of-the-art methods. The
comparison results of THUMOS14 shown in Table 6 suggest that (1) using same ac-
tion classiﬁer, our method achieves signiﬁcantly better performance than other proposal
generation methods; (2) comparing with proposal-level classiﬁer [7], video-level classi-
ﬁer [48] achieves better performance on BSN proposals and worse performance on [3]
and [6] proposals, which indicates that conﬁdence scores generated by BSN are more
reliable than scores generated by proposal-level classiﬁer, and are reliable enough for
retrieving detection results in action detection task; (3) detection framework based on
our proposals signiﬁcantly outperforms state-of-the-art action detection methods, espe-
cially when the overlap threshold is high. These results conﬁrm that proposals generated
by BSN have high quality and work generally well in detection frameworks.

5 Conclusion

In this paper, we have introduced the Boundary-Sensitive Network (BSN) for tempo-
ral action proposal generation. Our method can generate proposals with ﬂexible dura-
tions and precise boundaries via directly combing locations with high boundary prob-
abilities, and make accurate retrieving via evaluating proposal conﬁdence score with
proposal-level features. Thus BSN can achieve high recall and high temporal overlap
with relatively few proposals. In experiments, we demonstrate that BSN signiﬁcantly
outperforms other state-of-the-art proposal generation methods on both THUMOS14
and ActivityNet-1.3 datasets. And BSN can signiﬁcantly improve the detection perfor-
mance when used as the proposal stage of a full detection framework.

Boundary Sensitive Network

15

References

1. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A large-scale
video benchmark for human activity understanding. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. (2015) 961–970

2. Jiang, Y.G., Liu, J., Zamir, A.R., Toderici, G., Laptev, I., Shah, M., Sukthankar, R.: Thumos
challenge: Action recognition with a large number of classes. In: ECCV Workshop. (2014)
3. Buch, S., Escorcia, V., Shen, C., Ghanem, B., Niebles, J.C.: SST: Single-stream temporal

action proposals. In: IEEE International Conference on Computer Vision. (2017)

4. Caba Heilbron, F., Carlos Niebles, J., Ghanem, B.: Fast temporal activity proposals for
In: Proceedings of the IEEE

efﬁcient detection of human actions in untrimmed videos.
Conference on Computer Vision and Pattern Recognition. (2016) 1914–1923

5. Escorcia, V., Heilbron, F.C., Niebles, J.C., Ghanem, B.: Daps: Deep action proposals for
action understanding. In: European Conference on Computer Vision, Springer (2016) 768–
784

6. Gao, J., Yang, Z., Sun, C., Chen, K., Nevatia, R.: Turn tap: Temporal unit regression network

for temporal action proposals. arXiv preprint arXiv:1703.06189 (2017)

7. Shou, Z., Wang, D., Chang, S.F.: Temporal action localization in untrimmed videos via
multi-stage cnns. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2016) 1049–1058

8. Wang, H., Kl¨aser, A., Schmid, C., Liu, C.L.: Action recognition by dense trajectories. In:
Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, IEEE (2011)
3169–3176

9. Wang, H., Schmid, C.: Action recognition with improved trajectories. In: Proceedings of the

IEEE International Conference on Computer Vision. (2013) 3551–3558

10. Feichtenhofer, C., Pinz, A., Zisserman, A.: Convolutional two-stream network fusion for
video action recognition. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2016) 1933–1941

11. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recognition in

videos. In: Advances in Neural Information Processing Systems. (2014) 568–576

12. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotemporal features
with 3d convolutional networks. In: Proceedings of the IEEE International Conference on
Computer Vision. (2015) 4489–4497

13. Wang, L., Xiong, Y., Wang, Z., Qiao, Y.: Towards good practices for very deep two-stream

convnets. arXiv preprint arXiv:1507.02159 (2015)

14. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object
detection and semantic segmentation. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. (2014) 580–587

15. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference on Computer

Vision. (2015) 1440–1448

16. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with
region proposal networks. In: Advances in neural information processing systems. (2015)
91–99

17. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection with dis-
criminatively trained part-based models. IEEE transactions on pattern analysis and machine
intelligence 32(9) (2010) 1627–1645

18. Uijlings, J.R., van de Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search for object

recognition. International journal of computer vision 104(2) (2013) 154–171

19. Zitnick, C.L., Doll´ar, P.: Edge boxes: Locating object proposals from edges. In: European

Conference on Computer Vision, Springer (2014) 391–405

16

Tianwei Lin et al.

20. Kuo, W., Hariharan, B., Malik, J.: Deepbox: Learning objectness with convolutional net-
works. In: Proceedings of the IEEE International Conference on Computer Vision. (2015)
2479–2487

21. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid

networks for object detection. arXiv preprint arXiv:1612.03144 (2016)

22. Gidaris, S., Komodakis, N.: Locnet: Improving localization accuracy for object detection. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2016)
789–798

23. Singh, G., Cuzzolin, F.: Untrimmed video classiﬁcation for activity detection: submission to

activitynet challenge. arXiv preprint arXiv:1607.01979 (2016)

24. Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Lin, D., Tang, X.: Temporal action detection with

structured segment networks. arXiv preprint arXiv:1704.06228 (2017)

25. Lin, T., Zhao, X., Shou, Z.: Single shot temporal action detection. In: Proceedings of the

25nd ACM international conference on Multimedia. (2017)

26. Buch, S., Escorcia, V., Ghanem, B., Fei-Fei, L., Niebles, J.C.: End-to-end, single-stream
In: Proceedings of the British Machine

temporal action detection in untrimmed videos.
Vision Conference. (2017)

27. Karaman, S., Seidenari, L., Del Bimbo, A.: Fast saliency based pooling of ﬁsher encoded

dense trajectories. In: ECCV THUMOS Workshop. (2014)

28. Oneata, D., Verbeek, J., Schmid, C.: The lear submission at thumos 2014. ECCV THUMOS

Workshop (2014)

29. Wang, L., Qiao, Y., Tang, X.: Action recognition and detection by combining motion and

appearance features. THUMOS14 Action Recognition Challenge 1 (2014) 2

30. Yuan, Z., Stroud, J.C., Lu, T., Deng, J.: Temporal action localization by structured maximal

sums. arXiv preprint arXiv:1704.04671 (2017)

31. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment
networks: towards good practices for deep action recognition. In: European Conference on
Computer Vision, Springer (2016) 20–36

32. Gao, J., Yang, Z., Nevatia, R.: Cascaded boundary regression for temporal action detection.

arXiv preprint arXiv:1705.01180 (2017)

33. Bodla, N., Singh, B., Chellappa, R., Davis, .L.S.: Improving object detection with one line

of code. arXiv preprint arXiv:1704.04503 (2017)

34. Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes from

videos in the wild. arXiv preprint arXiv:1212.0402 (2012)

35. Xiong, Y., Wang, L., Wang, Z., Zhang, B., Song, H., Li, W., Lin, D., Qiao, Y., Gool, L.V.,
Tang, X.: Cuhk & ethz & siat submission to activitynet challenge 2016. arXiv preprint
arXiv:1608.00797 (2016)

37. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

36. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In: International Conference on Machine Learning. (2015) 448–456
In:
Proceedings of the IEEE conference on computer vision and pattern recognition. (2016)
770–778

38. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.,
Darrell, T.: Caffe: Convolutional architecture for fast feature embedding. In: Proceedings of
the 22nd ACM international conference on Multimedia, ACM (2014) 675–678

39. Lin, T., Zhao, X., Shou, Z.: Temporal convolution based action proposal: Submission to

activitynet 2017. arXiv preprint arXiv:1707.06750 (2017)

40. Abadi, M., Agarwal, A., Barham, P., et al.: Tensorﬂow: Large-scale machine learning on

heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016)
41. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection.
Conference on Computer Vision and Pattern Recognition. (2005) 886–893

In: IEEE

Boundary Sensitive Network

17

42. Dai, X., Singh, B., Zhang, G., Davis, L.S., Chen, Y.Q.: Temporal context network for activity
localization in videos. In: 2017 IEEE International Conference on Computer Vision (ICCV),
IEEE (2017) 5727–5736

43. Ghanem, B., Niebles, J.C., Snoek, C., Heilbron, F.C., Alwassel, H., Khrisna, R., Escorcia, V.,
Hata, K., Buch, S.: Activitynet challenge 2017 summary. arXiv preprint arXiv:1710.08011
(2017)

44. Tran, D., Ray, J., Shou, Z., Chang, S.F., Paluri, M.: Convnet architecture search for spa-

tiotemporal feature learning. arXiv preprint arXiv:1708.05038 (2017)

45. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-scale
video classiﬁcation with convolutional neural networks. In: Proceedings of the IEEE confer-
ence on Computer Vision and Pattern Recognition. (2014) 1725–1732

46. Zhao, Y., Zhang, B., Wu, Z., Yang, S., Zhou, L., Yan, S., Wang, L., Xiong, Y., Lin, D., Qiao,
Y., Tang, X.: Cuhk & ethz & siat submission to activitynet challenge 2017. arXiv preprint
arXiv:1710.08011 (2017)

47. Wang, R., Tao, D.: Uts at activitynet 2016. AcitivityNet Large Scale Activity Recognition

Challenge 2016 (2016) 8

48. Wang, L., Xiong, Y., Lin, D., Van Gool, L.: Untrimmednets for weakly supervised action

recognition and detection. arXiv preprint arXiv:1703.03329 (2017)

49. Heilbron, F.C., Barrios, W., Escorcia, V., Ghanem, B.: Scc: Semantic context cascade for
efﬁcient action detection. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR). Volume 2. (2017)

50. Shou, Z., Chan, J., Zareian, A., Miyazawa, K., Chang, S.F.: Cdc: Convolutional-de-
convolutional networks for precise temporal action localization in untrimmed videos. arXiv
preprint arXiv:1703.01515 (2017)

51. Xiong, Y., Zhao, Y., Wang, L., Lin, D., Tang, X.: A pursuit of temporal accuracy in general

activity detection. arXiv preprint arXiv:1703.02716 (2017)

52. Xu, H., Das, A., Saenko, K.: R-c3d: Region convolutional 3d network for temporal activity

detection. arXiv preprint arXiv:1703.07814 (2017)

