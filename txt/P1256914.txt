Training Classiﬁers with Natural Language Explanations

Braden Hancock
Computer Science Dept.
Stanford University
bradenjh@cs.stanford.edu

Paroma Varma
Electrical Engineering Dept.
Stanford University
paroma@stanford.edu

Stephanie Wang
Computer Science Dept.
Stanford University
steph17@stanford.edu

Martin Bringmann
OccamzRazor
San Francisco, CA
martin@occamzrazor.com

Percy Liang
Computer Science Dept.
Stanford University
pliang@cs.stanford.edu

Christopher R´e
Computer Science Dept.
Stanford University
chrismre@cs.stanford.edu

8
1
0
2
 
g
u
A
 
5
2
 
 
]
L
C
.
s
c
[
 
 
4
v
8
1
8
3
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Training accurate classiﬁers requires many
labels, but each label provides only
limited information (one bit for binary
classiﬁcation).
In this work, we propose
BabbleLabble, a framework for training
classiﬁers in which an annotator provides
a natural language explanation for each
A semantic parser
labeling decision.
converts these explanations into program-
matic labeling functions that generate
noisy labels for an arbitrary amount of
unlabeled data, which is used to train a
classiﬁer. On three relation extraction
tasks, we ﬁnd that users are able to
train classiﬁers with comparable F1 scores
from 5–100× faster by providing explana-
tions instead of just labels. Furthermore,
given the inherent imperfection of labeling
functions, we ﬁnd that a simple rule-based
semantic parser sufﬁces.

1

Introduction

The standard protocol for obtaining a labeled
dataset is to have a human annotator view each
example, assess its relevance, and provide a label
(e.g., positive or negative for binary classiﬁcation).
However, this only provides one bit of information
per example. This invites the question: how can
we get more information per example, given that
the annotator has already spent the effort reading
and understanding an example?

Previous works have relied on identifying rel-
evant parts of the input such as labeling features
(Druck et al., 2009; Raghavan et al., 2005; Liang
et al., 2009), highlighting rationale phrases in

Figure 1:
In BabbleLabble, the user provides
a natural language explanation for each label-
ing decision. These explanations are parsed into
labeling functions that convert unlabeled data into
a large labeled dataset for training a classiﬁer.

text (Zaidan and Eisner, 2008; Arora and Nyberg,
2009), or marking relevant regions in images (Ahn
et al., 2006). But there are certain types of infor-
mation which cannot be easily reduced to annotat-
ing a portion of the input, such as the absence of a
certain word, or the presence of at least two words.
In this work, we tap into the power of natural lan-
guage and allow annotators to provide supervision
to a classiﬁer via natural language explanations.

Speciﬁcally, we propose a framework in which
annotators provide a natural language explanation
for each label they assign to an example (see Fig-
ure 1). These explanations are parsed into log-
ical forms representing labeling functions (LFs),
functions that heuristically map examples to labels
(Ratner et al., 2016). The labeling functions are

Figure 2: Natural language explanations are parsed into candidate labeling functions (LFs). Many
incorrect LFs are ﬁltered out automatically by the ﬁlter bank. The remaining functions provide heuristic
labels over the unlabeled dataset, which are aggregated into one noisy label per example, yielding a large,
noisily-labeled training set for a classiﬁer.

then executed on many unlabeled examples, re-
sulting in a large, weakly-supervised training set
that is then used to train a classiﬁer.

Semantic parsing of natural language into log-
ical forms is recognized as a challenging prob-
lem and has been studied extensively (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Liang et al., 2011; Liang, 2016). One of our ma-
jor ﬁndings is that in our setting, even a simple
rule-based semantic parser sufﬁces for three rea-
sons: First, we ﬁnd that the majority of incorrect
LFs can be automatically ﬁltered out either seman-
tically (e.g., is it consistent with the associated ex-
ample?) or pragmatically (e.g., does it avoid as-
signing the same label to the entire training set?).
Second, LFs near the gold LF in the space of logi-
cal forms are often just as accurate (and sometimes
even more accurate). Third, techniques for com-
bining weak supervision sources are built to toler-
ate some noise (Alfonseca et al., 2012; Takamatsu
et al., 2012; Ratner et al., 2018). The signiﬁcance
of this is that we can deploy the same semantic
parser across tasks without task-speciﬁc training.
We show how we can tackle a real-world biomedi-
cal application with the same semantic parser used
to extract instances of spouses.

simple rule-based parser. In Section 4, we ﬁnd that
in our weak supervision framework, the rule-based
semantic parser and the perfect parser yield nearly
identical downstream performance. Second, while
they use the logical forms of explanations to pro-
duce features that are fed directly to a classiﬁer, we
use them as functions for labeling a much larger
training set. In Section 4, we show that using func-
tions yields a 9.5 F1 improvement (26% relative
improvement) over features, and that the F1 score
scales with the amount of available unlabeled data.
We validate our approach on two existing
datasets from the literature (extracting spouses
from news articles and disease-causing chemi-
cals from biomedical abstracts) and one real-
world use case with our biomedical collabora-
tors at OccamzRazor to extract protein-kinase
interactions related to Parkinson’s disease from
text. We ﬁnd empirically that users are able to
train classiﬁers with comparable F1 scores up to
100× faster when they provide natural language
explanations instead of individual labels. Our
code is available at https://github.com/
HazyResearch/babble.

2 The BabbleLabble Framework

Our work is most similar to that of Srivastava
et al. (2017), who also use natural language expla-
nations to train a classiﬁer, but with two important
differences. First, they jointly train a task-speciﬁc
semantic parser and classiﬁer, whereas we use a

The BabbleLabble framework converts natural
language explanations and unlabeled data into a
noisily-labeled training set (see Figure 2). There
are three key components: a semantic parser, a
ﬁlter bank, and a label aggregator. The semantic

Figure 3: Valid parses are found by iterating over increasingly large subspans of the input looking for
matches among the right hand sides of the rules in the grammar. Rules are either lexical (converting
tokens into symbols), unary (converting one symbol into another symbol), or compositional (combining
many symbols into a single higher-order symbol). A rule may optionally ignore unrecognized tokens in
a span (denoted here with a dashed line).

parser converts natural language explanations into
a set of logical forms representing labeling func-
tions (LFs). The ﬁlter bank removes as many in-
correct LFs as possible without requiring ground
truth labels. The remaining LFs are applied to un-
labeled examples to produce a matrix of labels.
This label matrix is passed into the label aggre-
gator, which combines these potentially conﬂict-
ing and overlapping labels into one label for each
example. The resulting labeled examples are then
used to train an arbitrary discriminative model.

2.1 Explanations

To create the input explanations, the user views a
subset S of an unlabeled dataset D (where |S| (cid:28)
|D|) and provides for each input xi ∈ S a label
yi and a natural language explanation ei, a sen-
tence explaining why the example should receive
that label. The explanation ei generally refers to
speciﬁc aspects of the example (e.g., in Figure 2,
the location of a speciﬁc string “his wife”).

2.2 Semantic Parser

The semantic parser takes a natural language ex-
planation ei and returns a set of LFs (logical forms
or labeling functions) {f1, . . . , fk} of the form
fi
: X → {−1, 0, 1} in a binary classiﬁcation
setting, with 0 representing abstention. We em-
phasize that the goal of this semantic parser is not
to generate the single correct parse, but rather to
have coverage over many potentially useful LFs.1

1Indeed, we ﬁnd empirically that an incorrect LF nearby
the correct one in the space of logical forms actually has
higher end-task accuracy 57% of the time (see Section 4.2).

We choose a simple rule-based semantic parser
that can be used without any training. Formally,
the parser uses a set of rules of the form α → β,
where α can be replaced by the token(s) in β (see
Figure 3 for example rules). To identify candidate
LFs, we recursively construct a set of valid parses
for each span of the explanation, based on the sub-
stitutions deﬁned by the grammar rules. At the
end, the parser returns all valid parses (LFs in our
case) corresponding to the entire explanation.

We also allow an arbitrary number of tokens in
a given span to be ignored when looking for a
matching rule. This improves the ability of the
parser to handle unexpected input, such as un-
known words or typos, since the portions of the
input that are parseable can still result in a valid
parse. For example, in Figure 3, the word “per-
son” is ignored.

All predicates included in our grammar (sum-
marized in Table 1) are provided to annota-
tors, with minimal examples of each in use
(Appendix A).
Importantly, all rules are do-
main independent (e.g., all three relation extrac-
tion tasks that we tested used the same grammar),
making the semantic parser easily transferrable to
new domains. Additionally, while this paper fo-
cuses on the task of relation extraction, in princi-
ple the BabbleLabble framework can be applied
to other tasks or settings by extending the grammar
with the necessary primitives (e.g., adding primi-
tives for rows and columns to enable explanations
about the alignments of words in tables). To guide
the construction of the grammar, we collected 500
explanations for the Spouse domain from workers

Predicate

Description

bool, string,
int, float, tuple,
list, set
and, or, not, any,
all, none
=, (cid:54)=, <, ≤, >, ≥
lower, upper,
capital, all caps
starts with,
ends with,
substring
person, location,
date, number,
organization
alias

count, contains,
intersection

map, filter

word distance,
character distance

left, right,
between, within

Standard primitive data types

Standard logic operators

Standard comparison operators
Return True for strings of the
corresponding case
Return True if the ﬁrst string
starts/ends with or contains the
second
Return True if a string has the
corresponding NER tag

A frequently used list of words
may be predeﬁned and referred
to with an alias
Operators for checking size,
membership, or common ele-
ments of a list/set
Apply a functional primitive to
each member of list/set to
transform or ﬁlter the elements
Return the distance between
two strings by words or charac-
ters
Return as a string the text that is
left/right/within some distance
of a string or between two des-
ignated strings

Table 1: Predicates in the grammar supported by
BabbleLabble’s rule-based semantic parser.

on Amazon Mechanical Turk and added support
for the most commonly used predicates. These
were added before the experiments described in
Section 4. The grammar contains a total of 200
rule templates.

2.3 Filter Bank

The input to the ﬁlter bank is a set of candidate
LFs produced by the semantic parser. The pur-
pose of the ﬁlter bank is to discard as many incor-
rect LFs as possible without requiring additional
labels. It consists of two classes of ﬁlters: seman-
tic and pragmatic.

Recall that each explanation ei is collected in
the context of a speciﬁc labeled example (xi, yi).
The semantic ﬁlter checks for LFs that are in-
consistent with their corresponding example; for-
mally, any LF f for which f (xi) (cid:54)= yi is discarded.
For example, in the ﬁrst explanation in Figure 2,
the word “right” can be interpreted as either “im-
mediately” (as in “right before”) or simply “to the

right.” The latter interpretation results in a func-
tion that is inconsistent with the associated exam-
ple (since “his wife” is actually to the left of person
2), so it can be safely removed.

The pragmatic ﬁlters removes LFs that are con-
stant, redundant, or correlated. For example, in
Figure 2, LF 2a is constant, as it labels every ex-
ample positively (since all examples contain two
people from the same sentence). LF 3b is redun-
dant, since even though it has a different syntax
tree from LF 3a, it labels the training set identi-
cally and therefore provides no new signal.

Finally, out of all LFs from the same explana-
tion that pass all the other ﬁlters, we keep only
the most speciﬁc (lowest coverage) LF. This pre-
vents multiple correlated LFs from a single exam-
ple from dominating.

As we show in Section 4, over three tasks, the
ﬁlter bank removes over 95% of incorrect parses,
and the incorrect ones that remain have average
end-task accuracy within 2.5 points of the corre-
sponding correct parses.

2.4 Label Aggregator

The label aggregator combines multiple (poten-
tially conﬂicting) suggested labels from the LFs
and combines them into a single probabilistic la-
if m LFs pass
bel per example. Concretely,
the ﬁlter bank and are applied to n examples,
the label aggregator implements a function f :
{−1, 0, 1}m×n → [0, 1]n.

A naive solution would be to use a simple ma-
jority vote, but this fails to account for the fact
that LFs can vary widely in accuracy and cover-
Instead, we use data programming (Ratner
age.
et al., 2016), which models the relationship be-
tween the true labels and the output of the label-
ing functions as a factor graph. More speciﬁcally,
given the true labels Y ∈ {−1, 1}n (latent) and la-
bel matrix Λ ∈ {−1, 0, 1}m×n (observed) where
Λi,j = LFi(xj), we deﬁne two types of factors
representing labeling propensity and accuracy:

i,j (Λ, Y ) = 1{Λi,j (cid:54)= 0}
φLab
i,j (Λ, Y ) = 1{Λi,j = yj}.
φAcc

(1)

(2)

Denoting the vector of factors pertaining to a given
data point xj as φj(Λ, Y ) ∈ Rm, deﬁne the model:

pw(Λ, Y ) = Z−1

w exp

w · φj(Λ, Y )

(3)

(cid:17)

,

(cid:16) n
(cid:88)

j=1

Figure 4: An example and explanation for each of the three datasets.

where w ∈ R2m is the weight vector and Zw is
the normalization constant. To learn this model
without knowing the true labels Y , we minimize
the negative log marginal likelihood given the ob-
served labels Λ:

ˆw = arg min

− log

pw(Λ, Y )

(4)

w

(cid:88)

Y

using SGD and Gibbs sampling for inference, and
then use the marginals p ˆw(Y | Λ) as probabilistic
training labels.

Intuitively, we infer accuracies of the LFs based
on the way they overlap and conﬂict with one an-
other. Since noisier LFs are more likely to have
high conﬂict rates with others, their correspond-
ing accuracy weights in w will be smaller, reduc-
ing their inﬂuence on the aggregated labels.

2.5 Discriminative Model

The noisy training set that the label aggregator
outputs is used to train an arbitrary discriminative
model. One advantage of training a discriminative
model on the task instead of using the label ag-
gregator as a classiﬁer directly is that the label ag-
gregator only takes into account those signals in-
cluded in the LFs. A discriminative model, on the
other hand, can incorporate features that were not
identiﬁed by the user but are nevertheless informa-
tive.2 Consequently, even examples for which all
LFs abstained can still be classiﬁed correctly. Ad-
ditionally, passing supervision information from
the user to the model in the form of a dataset—
rather than hard rules—promotes generalization in
the new model (rather than memorization), similar
to distant supervision (Mintz et al., 2009). On the
three tasks we evaluate, using the discriminative
model averages 4.3 F1 points higher than using the
label aggregator directly.

For the results reported in this paper, our dis-
criminative model is a simple logistic regression

Task

Train

Dev

Test % Pos.

Spouse
Disease
Protein

22195
6667
5546

2796
773
1011

2697
4101
1058

8%
23%
22%

Table 2: The total number of unlabeled train-
ing examples (a pair of annotated entities in a
sentence), labeled development examples (for hy-
perparameter tuning), labeled test examples (for
assessment), and the fraction of positive labels in
the test split.

classiﬁer with generic features deﬁned over depen-
dency paths.3 These features include unigrams,
bigrams, and trigrams of lemmas, dependency la-
bels, and part of speech tags found in the siblings,
parents, and nodes between the entities in the de-
pendency parse of the sentence. We found this to
perform better on average than a biLSTM, particu-
larly for the traditional supervision baselines with
small training set sizes; it also provided easily in-
terpretable features for analysis.

3 Experimental Setup

We evaluate the accuracy of BabbleLabble on
three relation extraction tasks, which we refer to
as Spouse, Disease, and Protein. The goal
of each task is to train a classiﬁer for predicting
whether the two entities in an example are partic-
ipating in the relationship of interest, as described
below.

3.1 Datasets

Statistics for each dataset are reported in Ta-
ble 2, with one example and one explanation for
each given in Figure 4 and additional explanations
shown in Appendix B.

In the Spouse task, annotators were shown a
sentence with two highlighted names and asked to

2We give an example of two such features in Section 4.3.

3https://github.com/HazyResearch/treedlib

# Inputs

Spouse
Disease
Protein

Average

BL

30

50.1
42.3
47.3

46.6

30

15.5
32.1
39.3

28.9

60

15.9
32.6
42.1

30.2

150

16.4
34.4
46.8

32.5

TS

300

17.2
37.5
51.0

35.2

1,000

3,000

10,000

22.8
41.9
57.6

40.8

41.8
44.5
-

43.2

55.0
-
-

55.0

Table 3: F1 scores obtained by a classiﬁer trained with BabbleLabble (BL) using 30 explanations
or with traditional supervision (TS) using the speciﬁed number of individually labeled examples.
BabbleLabble achieves the same F1 score as traditional supervision while using fewer user inputs
by a factor of over 5 (Protein) to over 100 (Spouse).

label whether the sentence suggests that the two
people are spouses. Sentences were pulled from
the Signal Media dataset of news articles (Corney
et al., 2016). Ground truth data was collected from
Amazon Mechanical Turk workers, accepting the
majority label over three annotations. The 30 ex-
planations we report on were sampled randomly
from a pool of 200 that were generated by 10 grad-
uate students unfamiliar with BabbleLabble.

In the Disease task, annotators were shown a
sentence with highlighted names of a chemical and
a disease and asked to label whether the sentence
suggests that the chemical causes the disease. Sen-
tences and ground truth labels came from a por-
tion of the 2015 BioCreative chemical-disease re-
lation dataset (Wei et al., 2015), which contains
abstracts from PubMed. Because this task re-
quires specialized domain expertise, we obtained
explanations by having someone unfamiliar with
BabbleLabble translate from Python to natural
language labeling functions from an existing pub-
lication that explored applying weak supervision
to this task (Ratner et al., 2018).

The Protein task was completed in conjunc-
tion with OccamzRazor, a neuroscience company
targeting biological pathways of Parkinson’s dis-
ease. For this task, annotators were shown a sen-
tence from the relevant biomedical literature with
highlighted names of a protein and a kinase and
asked to label whether or not the kinase inﬂu-
ences the protein in terms of a physical interac-
tion or phosphorylation. The annotators had do-
main expertise but minimal programming experi-
ence, making BabbleLabble a natural ﬁt for their
use case.

3.2 Experimental Settings

Text documents are tokenized with spaCy.4 The
semantic parser is built on top of the Python-based
implementation SippyCup.5 On a single core,
parsing 360 explanations takes approximately two
seconds. We use existing implementations of the
label aggregator, feature library, and discrimina-
tive classiﬁer described in Sections 2.4–2.5 pro-
vided by the open-source project Snorkel (Ratner
et al., 2018).

Hyperparameters for all methods we report
were selected via random search over thirty con-
ﬁgurations on the same held-out development set.
We searched over learning rate, batch size, L2 reg-
ularization, and the subsampling rate (for improv-
ing balance between classes).6 All reported F1
scores are the average value of 40 runs with ran-
dom seeds and otherwise identical settings.

4 Experimental Results

We evaluate the performance of BabbleLabble
with respect to its rate of improvement by number
of user inputs, its dependence on correctly parsed
logical forms, and the mechanism by which it uti-
lizes logical forms.

4.1 High Bandwidth Supervision

In Table 3 we report the average F1 score of a
classiﬁer trained with BabbleLabble using 30 ex-
planations or traditional supervision with the indi-
cated number of labels. On average, it took the
same amount of time to collect 30 explanations

4https://github.com/explosion/spaCy
5https://github.com/wcmac/sippycup
6Hyperparameter ranges:

learning rate (1e-2 to 1e-4),
batch size (32 to 128), L2 regularization (0 to 100), subsam-
pling rate (0 to 0.5)

Pre-ﬁlters

Discarded

Post-ﬁlters

LFs Correct

Sem. Prag.

LFs Correct

Spouse 156
Disease 102
Protein 122

10%
23%
14%

19
34
44

118
40
58

19
28
20

84%
89%
85%

Table 4: The number of LFs generated from 30
explanations (pre-ﬁlters), discarded by the ﬁlter
bank, and remaining (post-ﬁlters), along with the
percentage of LFs that were correctly parsed from
their corresponding explanations.

as 60 labels.7 We observe that in all three tasks,
BabbleLabble achieves a given F1 score with far
fewer user inputs than traditional supervision, by
as much as 100 times in the case of the Spouse
task. Because explanations are applied to many
unlabeled examples, each individual input from
the user can implicitly contribute many (noisy) la-
bels to the learning algorithm.

We also observe, however, that once the num-
ber of labeled examples is sufﬁciently large, tra-
ditional supervision once again dominates, since
ground truth labels are preferable to noisy ones
generated by labeling functions. However, in do-
mains where there is much more unlabeled data
available than labeled data (which in our experi-
ence is most domains), we can gain in supervision
efﬁciency from using BabbleLabble.

Of those explanations that did not produce a
correct LF, 4% were caused by the explanation re-
ferring to unsupported concepts (e.g., one expla-
nation referred to “the subject of the sentence,”
which our simple parser doesn’t support). An-
other 2% were caused by human errors (the cor-
rect LF for the explanation was inconsistent with
the example). The remainder were due to unrecog-
nized paraphrases (e.g., the explanation said “the
order of appearance is X, Y” instead of a sup-
ported phrasing like “X comes before Y”).

4.2 Utility of Incorrect Parses

In Table 4, we report LF summary statistics be-
fore and after ﬁltering. LF correctness is based
on exact match with a manually generated parse
for each explanation. Surprisingly, the simple
heuristic-based ﬁlter bank successfully removes
over 95% of incorrect LFs in all three tasks, re-
sulting in ﬁnal LF sets that are 86% correct on av-

7Zaidan and Eisner (2008) also found that collecting an-
notator rationales in the form of highlighted substrings from
the sentence only doubled annotation time.

Spouse
Disease
Protein

Average

BL-FB

15.7
39.8
38.2

31.2

BL

50.1
42.3
47.3

46.6

BL+PP

49.8
43.2
47.4

46.8

Table 5: F1 scores obtained using BabbleLabble
with no ﬁlter bank (BL-FB), as normal (BL), and
with a perfect parser (BL+PP) simulated by hand.

erage. Furthermore, among those LFs that pass
through the ﬁlter bank, we found that the aver-
age difference in end-task accuracy between cor-
rect and incorrect parses is less than 2.5%. Intu-
itively, the ﬁlters are effective because it is quite
difﬁcult for an LF to be parsed from the explana-
tion, label its own example correctly (passing the
semantic ﬁlter), and not label all examples in the
training set with the same label or identically to
another LF (passing the pragmatic ﬁlter).

We went one step further: using the LFs that
would be produced by a perfect semantic parser as
starting points, we searched for “nearby” LFs (LFs
differing by only one predicate) with higher end-
task accuracy on the test set and succeeded 57%
of the time (see Figure 5 for an example). In other
words, when users provide explanations, the sig-
nals they describe provide good starting points, but
they are actually unlikely to be optimal. This ob-
servation is further supported by Table 5, which
shows that the ﬁlter bank is necessary to remove
clearly irrelevant LFs, but with that in place, the
simple rule-based semantic parser and a perfect
parser have nearly identical average F1 scores.

4.3 Using LFs as Functions or Features

Once we have relevant logical forms from user-
provided explanations, we have multiple options
for how to use them. Srivastava et al. (2017) pro-
pose using these logical forms as features in a lin-
ear classiﬁer, essentially using a traditional super-
vision approach with user-speciﬁed features. We
choose instead to use them as functions for weakly
supervising the creation of a larger training set via
data programming (Ratner et al., 2016).
In Ta-
ble 6, we compare the two approaches directly,
ﬁnding that the the data programming approach
outperforms a feature-based one by 9.5 F1 points
on average with the rule-based parser, and by 4.5
points with a perfect parser.

We attribute this difference primarily to the abil-
ity of data programming to utilize a larger feature

Figure 5: Incorrect LFs often still provide useful signal. On top is an incorrect LF produced for the
Disease task that had the same accuracy as the correct LF. On bottom is a correct LF from the Spouse
task and a more accurate incorrect LF discovered by randomly perturbing one predicate at a time as
described in Section 4.2. (Person 2 is always the second person in the sentence).

BL-DM

BL BL+PP

Feat Feat+PP

Spouse
Disease
Protein

Average

46.5
39.7
40.6

42.3

50.1
42.3
47.3

46.6

49.8
43.2
47.4

46.8

33.9
40.8
36.7

37.1

39.2
43.8
44.0

42.3

Table 6: F1 scores obtained using explanations as
functions for data programming (BL) or features
(Feat), optionally with no discriminative model
(-DM) or using a perfect parser (+PP).

5 Related Work and Discussion

Our work has two themes: modeling natural lan-
guage explanations/instructions and learning from
weak supervision. The closest body of work is
on “learning from natural language.” As men-
tioned earlier, Srivastava et al. (2017) convert nat-
ural language explanations into classiﬁer features
(whereas we convert them into labeling functions).
Goldwasser and Roth (2011) convert natural lan-
guage into concepts (e.g.,
the rules of a card
game). Ling and Fidler (2017) use natural lan-
guage explanations to assist in supervising an im-
age captioning model. Weston (2016); Li et al.
(2016) learn from natural language feedback in a
dialogue. Wang et al. (2017) convert natural lan-
guage deﬁnitions to rules in a semantic parser to
build up progressively higher-level concepts.

We lean on the formalism of semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Liang, 2016). One notable trend is
to learn semantic parsers from weak supervision
(Clarke et al., 2010; Liang et al., 2011), whereas
our goal is to obtain weak supervision signal from
semantic parsers.

The broader topic of weak supervision has re-
ceived much attention; we mention some works
most related to relation extraction. In distant su-
pervision (Craven et al., 1999; Mintz et al., 2009)

Figure 6: When logical forms of natural language
explanations are used as functions for data pro-
gramming (as they are in BabbleLabble), perfor-
mance can improve with the addition of unlabeled
data, whereas using them as features does not ben-
eﬁt from unlabeled data.

set and unlabeled data. In Figure 6, we show how
the data programming approach improves with the
number of unlabeled examples, even as the num-
ber of LFs remains constant. We also observe
qualitatively that data programming exposes the
classiﬁer to additional patterns that are correlated
with our explanations but not mentioned directly.
For example, in the Disease task, two of the fea-
tures weighted most highly by the discriminative
model were the presence of the trigrams “could
produce a” or “support diagnosis of” between the
chemical and disease, despite none of these words
occurring in the explanations for that task. In Ta-
ble 6 we see a 4.3 F1 point improvement (10%)
when we use the discriminative model that can
take advantage of these features rather than apply-
ing the LFs directly to the test set and making pre-
dictions based on the label aggregator’s outputs.

and multi-instance learning (Riedel et al., 2010;
Hoffmann et al., 2011), an existing knowledge
base is used to (probabilistically) impute a train-
ing set. Various extensions have focused on aggre-
gating a variety of supervision sources by learn-
ing generative models from noisy labels (Alfon-
seca et al., 2012; Takamatsu et al., 2012; Roth and
Klakow, 2013; Ratner et al., 2016; Varma et al.,
2017).

Finally, while we have used natural language
explanations as input to train models, they can also
be output to interpret models (Krening et al., 2017;
Lei et al., 2016). More generally, from a machine
learning perspective, labels are the primary asset,
but they are a low bandwidth signal between an-
notators and the learning algorithm. Natural lan-
guage opens up a much higher-bandwidth commu-
nication channel. We have shown promising re-
sults in relation extraction (where one explanation
can be “worth” 100 labels), and it would be inter-
esting to extend our framework to other tasks and
more interactive settings.

Reproducibility

lowship under Grant No. DGE-114747, the Stan-
ford Finch Family Fellowship, the Joseph W. and
Hon Mai Goodman Stanford Graduate Fellowship,
an NSF CAREER Award IIS-1552635, and the
members of the Stanford DAWN project: Face-
book, Google, Intel, Microsoft, NEC, Teradata,
and VMware.

We thank Alex Ratner for his assistance with
data programming, Jason Fries and the many
members of the Hazy Research group and Stan-
ford NLP group who provided feedback and tested
early prototyptes, Kaya Tilev from the Stanford
Graduate School of Business for helpful discus-
sions early on, and the OccamzRazor team: Tarik
Koc, Benjamin Angulo, Katharina S. Volz, and
Charlotte Brzozowski.

The U.S. Government is authorized to repro-
duce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect
the views, policies, or endorsements, either ex-
pressed or implied, of DARPA, DOE, NIH, ONR,
AFOSR, NSF, or the U.S. Government.

The code, data, and experiments for this paper
are available on the CodaLab platform at https:
//worksheets.codalab.org/worksheets/
0x900e7e41deaa4ec5b2fe41dc50594548/.

References

Refactored code with simpliﬁed dependencies,
performance and speed improvements, and inter-
active tutorials can be found on Github:
https://github.com/HazyResearch/babble.

Acknowledgments

We gratefully acknowledge the support of the
following organizations: DARPA under No.
N66001-15-C-4043 (SIMPLEX), No. FA8750-
17-2-0095 (D3M), No.
FA8750-12-2-0335
(XDATA), and No. FA8750-13-2-0039 (DEFT),
108845, NIH under No.
DOE under No.
U54EB020405 (Mobilize), ONR under No.
N000141712266 and No.
N000141310129,
AFOSR under No. 580K753, the Intel/NSF CPS
Security grant No. 1505728, the Michael J. Fox
Foundation for Parkinsons Research under Grant
No. 14672, the Secure Internet of Things Project,
Qualcomm, Ericsson, Analog Devices, the Moore
Foundation, the Okawa Research Grant, Ameri-
can Family Insurance, Accenture, Toshiba, the Na-
tional Science Foundation Graduate Research Fel-

L. V. Ahn, R. Liu, and M. Blum. 2006. Peekaboom: a
game for locating objects in images. In Conference
on Human Factors in Computing Systems (CHI).
pages 55–64.

E. Alfonseca, K. Filippova, J. Delort, and G. Garrido.
2012. Pattern learning for relation extraction with a
hierarchical topic model. In Association for Compu-
tational Linguistics (ACL). pages 54–59.

S. Arora and E. Nyberg. 2009. Interactive annotation
learning with indirect feature voting. In Association
for Computational Linguistics (ACL). pages 55–60.

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL). pages 18–27.

D. Corney, D. Albakour, M. Martinez-Alvarez, and
S. Moussa. 2016. What do a million news articles
look like? In NewsIR@ ECIR. pages 42–47.

M. Craven, J. Kumlien, et al. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In ISMB. pages 77–86.

G. Druck, B. Settles, and A. McCallum. 2009. Active
learning by labeling features. In Empirical Methods
in Natural Language Processing (EMNLP). pages
81–90.

B. Roth and D. Klakow. 2013. Combining generative
and discriminative model scores for distant supervi-
In Empirical Methods in Natural Language
sion.
Processing (EMNLP). pages 24–29.

S. Srivastava, I. Labutov, and T. Mitchell. 2017. Joint
concept learning and semantic parsing from natu-
In Empirical Methods
ral language explanations.
in Natural Language Processing (EMNLP). pages
1528–1537.

S. Takamatsu, I. Sato, and H. Nakagawa. 2012. Reduc-
ing wrong labels in distant supervision for relation
In Association for Computational Lin-
extraction.
guistics (ACL). pages 721–729.

P. Varma, B. He, D. Iter, P. Xu, R. Yu, C. D. Sa, and
C. R’e. 2017. Socratic learning: Augmenting gener-
ative models to incorporate latent subsets in training
data. arXiv preprint arXiv:1610.08123 .

S. I. Wang, S. Ginn, P. Liang, and C. D. Manning.
2017. Naturalizing a programming language via in-
teractive learning. In Association for Computational
Linguistics (ACL).

C. Wei, Y. Peng, R. Leaman, A. P. Davis, C. J.
Mattingly, J. Li, T. C. Wiegers, and Z. Lu. 2015.
Overview of the biocreative V chemical disease re-
lation (cdr) task. In Proceedings of the ﬁfth BioCre-
ative challenge evaluation workshop. pages 154–
166.

J. E. Weston. 2016. Dialog-based language learning.
In Advances in Neural Information Processing Sys-
tems (NIPS). pages 829–837.

O. F. Zaidan and J. Eisner. 2008. Modeling annotators:
A generative approach to learning from annotator ra-
tionales. In Empirical Methods in Natural Language
Processing (EMNLP).

M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic program-
ming. In Association for the Advancement of Arti-
ﬁcial Intelligence (AAAI). pages 1050–1055.

L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classiﬁca-
tion with probabilistic categorial grammars. In Un-
certainty in Artiﬁcial Intelligence (UAI). pages 658–
666.

D. Goldwasser and D. Roth. 2011. Learning from nat-
ural instructions. In International Joint Conference
on Artiﬁcial Intelligence (IJCAI). pages 1794–1800.

R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer,
and D. S. Weld. 2011. Knowledge-based weak su-
pervision for information extraction of overlapping
In Association for Computational Lin-
relations.
guistics (ACL). pages 541–550.

S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell,
M. Riedl, and A. Thomaz. 2017. Learning from ex-
planations using sentiment and advice in RL. IEEE
Transactions on Cognitive and Developmental Sys-
tems 9(1):44–55.

T. Lei, R. Barzilay, and T. Jaakkola. 2016. Rational-
In Empirical Methods in

izing neural predictions.
Natural Language Processing (EMNLP).

J. Li, A. H. Miller, S. Chopra, M. Ranzato, and J. We-
ston. 2016. Learning through dialogue interactions.
arXiv preprint arXiv:1612.04936 .

P. Liang. 2016. Learning executable semantic parsers
for natural language understanding. Communica-
tions of the ACM 59.

P. Liang, M. I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families.
In International Conference on Machine Learning
(ICML).

P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
pages 590–599.

H. Ling and S. Fidler. 2017. Teaching machines to
describe images via natural language feedback. In
Advances in Neural Information Processing Systems
(NIPS).

M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Association for Computational Lin-
guistics (ACL). pages 1003–1011.

H. Raghavan, O. Madani, and R. Jones. 2005.

Inter-
active feature selection. In International Joint Con-
ference on Artiﬁcial Intelligence (IJCAI). volume 5,
pages 841–846.

A. J. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu,
and C. R’e. 2018. Snorkel: Rapid training data cre-
In Very Large Data
ation with weak supervision.
Bases (VLDB).

A. J. Ratner, C. M. D. Sa, S. Wu, D. Selsam, and C. R’e.
2016. Data programming: Creating large training
In Advances in Neural Information
sets, quickly.
Processing Systems (NIPS). pages 3567–3575.

S. Riedel, L. Yao, and A. McCallum. 2010. Model-
ing relations and their mentions without labeled text.
In Machine Learning and Knowledge Discovery in
Databases (ECML PKDD). pages 148–163.

A Predicate Examples

Below are the predicates in the rule-based semantic parser grammar, each of which may have many
supported paraphrases, only one of which is listed here in a minimal example.

Logic
and:
or:
not:
any:
all:
none:

X is true and Y is true

X is true or Y is true

X is not true
Any of X or Y or Z is true
All of X and Y and Z are true
None of X or Y or Z is true

Comparison
=:
(cid:54)=:
<:
≤:
>:
≥:

X is equal to Y
X is not Y
X is smaller than Y
X is no more than Y
X is larger than Y
X is at least Y

X is lowercase
X is upper case

Syntax
lower:
upper:
capital: X is capitalized
all caps: X is in all caps
starts with: X starts with "cardio"
ends with: X ends with "itis"
substring: X contains "-induced"

Named-entity Tags
person: A person is between X and Y
location: A place is within two words of X
date:
number: There are three numbers in the sentence
organization: An organization is right after X

A date is between X and Y

(X, Y) is in Z

X, Y, and Z are true

There is one word between X and Y

Lists
list:
set:
count:
contains: X is in Y
intersection: At least two of X are in Y
map:
filter: There are three capitalized words to the left of X
alias:

X is at the start of a word in Y

A spouse word is in the sentence (“spouse” is a predeﬁned list from the user)

Position
word distance: X is two words before Y
char distance: X is twenty characters after Y
left:
right:
between: X is between Y and Z
within: X is within five words of Y

X is before Y
X is after Y

B Sample Explanations

The following are a sample of the explanations provided by users for each task.

Spouse
Users referred to the ﬁrst person in the sentence as “X” and the second as “Y”.

Label true because "and" occurs between X and Y and "marriage" occurs
one word after person1.

Label true because person Y is preceded by ‘beau’.

Label false because the words "married", "spouse", "husband", and
"wife" do not occur in the sentence.

Label false because there are more than 2 people in the sentence and
"actor" or "actress" is left of person1 or person2.

Label true because the disease is immediately after the chemical and
’induc’ or ’assoc’ is in the chemical name.

Label true because a word containing ’develop’ appears somewhere
before the chemical, and the word ’following’ is between the disease
and the chemical.

Label true because "induced by", "caused by", or "due to" appears
between the chemical and the disease."

Label false because "none", "not", or "no" is within 30 characters to
the left of the disease.

Disease

Protein

Label true because "Ser" or "Tyr" are within 10 characters of the
protein.

Label true because the words "by" or "with" are between the protein
and kinase and the words "no", "not" or "none" are not in between
the protein and kinase and the total number of words between them is
smaller than 10.

Label false because the sentence contains "mRNA", "DNA", or "RNA".

Label false because there are two "," between the protein and the
kinase with less than 30 characters between them.

Training Classiﬁers with Natural Language Explanations

Braden Hancock
Computer Science Dept.
Stanford University
bradenjh@cs.stanford.edu

Paroma Varma
Electrical Engineering Dept.
Stanford University
paroma@stanford.edu

Stephanie Wang
Computer Science Dept.
Stanford University
steph17@stanford.edu

Martin Bringmann
OccamzRazor
San Francisco, CA
martin@occamzrazor.com

Percy Liang
Computer Science Dept.
Stanford University
pliang@cs.stanford.edu

Christopher R´e
Computer Science Dept.
Stanford University
chrismre@cs.stanford.edu

8
1
0
2
 
g
u
A
 
5
2
 
 
]
L
C
.
s
c
[
 
 
4
v
8
1
8
3
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Training accurate classiﬁers requires many
labels, but each label provides only
limited information (one bit for binary
classiﬁcation).
In this work, we propose
BabbleLabble, a framework for training
classiﬁers in which an annotator provides
a natural language explanation for each
A semantic parser
labeling decision.
converts these explanations into program-
matic labeling functions that generate
noisy labels for an arbitrary amount of
unlabeled data, which is used to train a
classiﬁer. On three relation extraction
tasks, we ﬁnd that users are able to
train classiﬁers with comparable F1 scores
from 5–100× faster by providing explana-
tions instead of just labels. Furthermore,
given the inherent imperfection of labeling
functions, we ﬁnd that a simple rule-based
semantic parser sufﬁces.

1

Introduction

The standard protocol for obtaining a labeled
dataset is to have a human annotator view each
example, assess its relevance, and provide a label
(e.g., positive or negative for binary classiﬁcation).
However, this only provides one bit of information
per example. This invites the question: how can
we get more information per example, given that
the annotator has already spent the effort reading
and understanding an example?

Previous works have relied on identifying rel-
evant parts of the input such as labeling features
(Druck et al., 2009; Raghavan et al., 2005; Liang
et al., 2009), highlighting rationale phrases in

Figure 1:
In BabbleLabble, the user provides
a natural language explanation for each label-
ing decision. These explanations are parsed into
labeling functions that convert unlabeled data into
a large labeled dataset for training a classiﬁer.

text (Zaidan and Eisner, 2008; Arora and Nyberg,
2009), or marking relevant regions in images (Ahn
et al., 2006). But there are certain types of infor-
mation which cannot be easily reduced to annotat-
ing a portion of the input, such as the absence of a
certain word, or the presence of at least two words.
In this work, we tap into the power of natural lan-
guage and allow annotators to provide supervision
to a classiﬁer via natural language explanations.

Speciﬁcally, we propose a framework in which
annotators provide a natural language explanation
for each label they assign to an example (see Fig-
ure 1). These explanations are parsed into log-
ical forms representing labeling functions (LFs),
functions that heuristically map examples to labels
(Ratner et al., 2016). The labeling functions are

Figure 2: Natural language explanations are parsed into candidate labeling functions (LFs). Many
incorrect LFs are ﬁltered out automatically by the ﬁlter bank. The remaining functions provide heuristic
labels over the unlabeled dataset, which are aggregated into one noisy label per example, yielding a large,
noisily-labeled training set for a classiﬁer.

then executed on many unlabeled examples, re-
sulting in a large, weakly-supervised training set
that is then used to train a classiﬁer.

Semantic parsing of natural language into log-
ical forms is recognized as a challenging prob-
lem and has been studied extensively (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Liang et al., 2011; Liang, 2016). One of our ma-
jor ﬁndings is that in our setting, even a simple
rule-based semantic parser sufﬁces for three rea-
sons: First, we ﬁnd that the majority of incorrect
LFs can be automatically ﬁltered out either seman-
tically (e.g., is it consistent with the associated ex-
ample?) or pragmatically (e.g., does it avoid as-
signing the same label to the entire training set?).
Second, LFs near the gold LF in the space of logi-
cal forms are often just as accurate (and sometimes
even more accurate). Third, techniques for com-
bining weak supervision sources are built to toler-
ate some noise (Alfonseca et al., 2012; Takamatsu
et al., 2012; Ratner et al., 2018). The signiﬁcance
of this is that we can deploy the same semantic
parser across tasks without task-speciﬁc training.
We show how we can tackle a real-world biomedi-
cal application with the same semantic parser used
to extract instances of spouses.

simple rule-based parser. In Section 4, we ﬁnd that
in our weak supervision framework, the rule-based
semantic parser and the perfect parser yield nearly
identical downstream performance. Second, while
they use the logical forms of explanations to pro-
duce features that are fed directly to a classiﬁer, we
use them as functions for labeling a much larger
training set. In Section 4, we show that using func-
tions yields a 9.5 F1 improvement (26% relative
improvement) over features, and that the F1 score
scales with the amount of available unlabeled data.
We validate our approach on two existing
datasets from the literature (extracting spouses
from news articles and disease-causing chemi-
cals from biomedical abstracts) and one real-
world use case with our biomedical collabora-
tors at OccamzRazor to extract protein-kinase
interactions related to Parkinson’s disease from
text. We ﬁnd empirically that users are able to
train classiﬁers with comparable F1 scores up to
100× faster when they provide natural language
explanations instead of individual labels. Our
code is available at https://github.com/
HazyResearch/babble.

2 The BabbleLabble Framework

Our work is most similar to that of Srivastava
et al. (2017), who also use natural language expla-
nations to train a classiﬁer, but with two important
differences. First, they jointly train a task-speciﬁc
semantic parser and classiﬁer, whereas we use a

The BabbleLabble framework converts natural
language explanations and unlabeled data into a
noisily-labeled training set (see Figure 2). There
are three key components: a semantic parser, a
ﬁlter bank, and a label aggregator. The semantic

Figure 3: Valid parses are found by iterating over increasingly large subspans of the input looking for
matches among the right hand sides of the rules in the grammar. Rules are either lexical (converting
tokens into symbols), unary (converting one symbol into another symbol), or compositional (combining
many symbols into a single higher-order symbol). A rule may optionally ignore unrecognized tokens in
a span (denoted here with a dashed line).

parser converts natural language explanations into
a set of logical forms representing labeling func-
tions (LFs). The ﬁlter bank removes as many in-
correct LFs as possible without requiring ground
truth labels. The remaining LFs are applied to un-
labeled examples to produce a matrix of labels.
This label matrix is passed into the label aggre-
gator, which combines these potentially conﬂict-
ing and overlapping labels into one label for each
example. The resulting labeled examples are then
used to train an arbitrary discriminative model.

2.1 Explanations

To create the input explanations, the user views a
subset S of an unlabeled dataset D (where |S| (cid:28)
|D|) and provides for each input xi ∈ S a label
yi and a natural language explanation ei, a sen-
tence explaining why the example should receive
that label. The explanation ei generally refers to
speciﬁc aspects of the example (e.g., in Figure 2,
the location of a speciﬁc string “his wife”).

2.2 Semantic Parser

The semantic parser takes a natural language ex-
planation ei and returns a set of LFs (logical forms
or labeling functions) {f1, . . . , fk} of the form
fi
: X → {−1, 0, 1} in a binary classiﬁcation
setting, with 0 representing abstention. We em-
phasize that the goal of this semantic parser is not
to generate the single correct parse, but rather to
have coverage over many potentially useful LFs.1

1Indeed, we ﬁnd empirically that an incorrect LF nearby
the correct one in the space of logical forms actually has
higher end-task accuracy 57% of the time (see Section 4.2).

We choose a simple rule-based semantic parser
that can be used without any training. Formally,
the parser uses a set of rules of the form α → β,
where α can be replaced by the token(s) in β (see
Figure 3 for example rules). To identify candidate
LFs, we recursively construct a set of valid parses
for each span of the explanation, based on the sub-
stitutions deﬁned by the grammar rules. At the
end, the parser returns all valid parses (LFs in our
case) corresponding to the entire explanation.

We also allow an arbitrary number of tokens in
a given span to be ignored when looking for a
matching rule. This improves the ability of the
parser to handle unexpected input, such as un-
known words or typos, since the portions of the
input that are parseable can still result in a valid
parse. For example, in Figure 3, the word “per-
son” is ignored.

All predicates included in our grammar (sum-
marized in Table 1) are provided to annota-
tors, with minimal examples of each in use
(Appendix A).
Importantly, all rules are do-
main independent (e.g., all three relation extrac-
tion tasks that we tested used the same grammar),
making the semantic parser easily transferrable to
new domains. Additionally, while this paper fo-
cuses on the task of relation extraction, in princi-
ple the BabbleLabble framework can be applied
to other tasks or settings by extending the grammar
with the necessary primitives (e.g., adding primi-
tives for rows and columns to enable explanations
about the alignments of words in tables). To guide
the construction of the grammar, we collected 500
explanations for the Spouse domain from workers

Predicate

Description

bool, string,
int, float, tuple,
list, set
and, or, not, any,
all, none
=, (cid:54)=, <, ≤, >, ≥
lower, upper,
capital, all caps
starts with,
ends with,
substring
person, location,
date, number,
organization
alias

count, contains,
intersection

map, filter

word distance,
character distance

left, right,
between, within

Standard primitive data types

Standard logic operators

Standard comparison operators
Return True for strings of the
corresponding case
Return True if the ﬁrst string
starts/ends with or contains the
second
Return True if a string has the
corresponding NER tag

A frequently used list of words
may be predeﬁned and referred
to with an alias
Operators for checking size,
membership, or common ele-
ments of a list/set
Apply a functional primitive to
each member of list/set to
transform or ﬁlter the elements
Return the distance between
two strings by words or charac-
ters
Return as a string the text that is
left/right/within some distance
of a string or between two des-
ignated strings

Table 1: Predicates in the grammar supported by
BabbleLabble’s rule-based semantic parser.

on Amazon Mechanical Turk and added support
for the most commonly used predicates. These
were added before the experiments described in
Section 4. The grammar contains a total of 200
rule templates.

2.3 Filter Bank

The input to the ﬁlter bank is a set of candidate
LFs produced by the semantic parser. The pur-
pose of the ﬁlter bank is to discard as many incor-
rect LFs as possible without requiring additional
labels. It consists of two classes of ﬁlters: seman-
tic and pragmatic.

Recall that each explanation ei is collected in
the context of a speciﬁc labeled example (xi, yi).
The semantic ﬁlter checks for LFs that are in-
consistent with their corresponding example; for-
mally, any LF f for which f (xi) (cid:54)= yi is discarded.
For example, in the ﬁrst explanation in Figure 2,
the word “right” can be interpreted as either “im-
mediately” (as in “right before”) or simply “to the

right.” The latter interpretation results in a func-
tion that is inconsistent with the associated exam-
ple (since “his wife” is actually to the left of person
2), so it can be safely removed.

The pragmatic ﬁlters removes LFs that are con-
stant, redundant, or correlated. For example, in
Figure 2, LF 2a is constant, as it labels every ex-
ample positively (since all examples contain two
people from the same sentence). LF 3b is redun-
dant, since even though it has a different syntax
tree from LF 3a, it labels the training set identi-
cally and therefore provides no new signal.

Finally, out of all LFs from the same explana-
tion that pass all the other ﬁlters, we keep only
the most speciﬁc (lowest coverage) LF. This pre-
vents multiple correlated LFs from a single exam-
ple from dominating.

As we show in Section 4, over three tasks, the
ﬁlter bank removes over 95% of incorrect parses,
and the incorrect ones that remain have average
end-task accuracy within 2.5 points of the corre-
sponding correct parses.

2.4 Label Aggregator

The label aggregator combines multiple (poten-
tially conﬂicting) suggested labels from the LFs
and combines them into a single probabilistic la-
if m LFs pass
bel per example. Concretely,
the ﬁlter bank and are applied to n examples,
the label aggregator implements a function f :
{−1, 0, 1}m×n → [0, 1]n.

A naive solution would be to use a simple ma-
jority vote, but this fails to account for the fact
that LFs can vary widely in accuracy and cover-
Instead, we use data programming (Ratner
age.
et al., 2016), which models the relationship be-
tween the true labels and the output of the label-
ing functions as a factor graph. More speciﬁcally,
given the true labels Y ∈ {−1, 1}n (latent) and la-
bel matrix Λ ∈ {−1, 0, 1}m×n (observed) where
Λi,j = LFi(xj), we deﬁne two types of factors
representing labeling propensity and accuracy:

i,j (Λ, Y ) = 1{Λi,j (cid:54)= 0}
φLab
i,j (Λ, Y ) = 1{Λi,j = yj}.
φAcc

(1)

(2)

Denoting the vector of factors pertaining to a given
data point xj as φj(Λ, Y ) ∈ Rm, deﬁne the model:

pw(Λ, Y ) = Z−1

w exp

w · φj(Λ, Y )

(3)

(cid:17)

,

(cid:16) n
(cid:88)

j=1

Figure 4: An example and explanation for each of the three datasets.

where w ∈ R2m is the weight vector and Zw is
the normalization constant. To learn this model
without knowing the true labels Y , we minimize
the negative log marginal likelihood given the ob-
served labels Λ:

ˆw = arg min

− log

pw(Λ, Y )

(4)

w

(cid:88)

Y

using SGD and Gibbs sampling for inference, and
then use the marginals p ˆw(Y | Λ) as probabilistic
training labels.

Intuitively, we infer accuracies of the LFs based
on the way they overlap and conﬂict with one an-
other. Since noisier LFs are more likely to have
high conﬂict rates with others, their correspond-
ing accuracy weights in w will be smaller, reduc-
ing their inﬂuence on the aggregated labels.

2.5 Discriminative Model

The noisy training set that the label aggregator
outputs is used to train an arbitrary discriminative
model. One advantage of training a discriminative
model on the task instead of using the label ag-
gregator as a classiﬁer directly is that the label ag-
gregator only takes into account those signals in-
cluded in the LFs. A discriminative model, on the
other hand, can incorporate features that were not
identiﬁed by the user but are nevertheless informa-
tive.2 Consequently, even examples for which all
LFs abstained can still be classiﬁed correctly. Ad-
ditionally, passing supervision information from
the user to the model in the form of a dataset—
rather than hard rules—promotes generalization in
the new model (rather than memorization), similar
to distant supervision (Mintz et al., 2009). On the
three tasks we evaluate, using the discriminative
model averages 4.3 F1 points higher than using the
label aggregator directly.

For the results reported in this paper, our dis-
criminative model is a simple logistic regression

Task

Train

Dev

Test % Pos.

Spouse
Disease
Protein

22195
6667
5546

2796
773
1011

2697
4101
1058

8%
23%
22%

Table 2: The total number of unlabeled train-
ing examples (a pair of annotated entities in a
sentence), labeled development examples (for hy-
perparameter tuning), labeled test examples (for
assessment), and the fraction of positive labels in
the test split.

classiﬁer with generic features deﬁned over depen-
dency paths.3 These features include unigrams,
bigrams, and trigrams of lemmas, dependency la-
bels, and part of speech tags found in the siblings,
parents, and nodes between the entities in the de-
pendency parse of the sentence. We found this to
perform better on average than a biLSTM, particu-
larly for the traditional supervision baselines with
small training set sizes; it also provided easily in-
terpretable features for analysis.

3 Experimental Setup

We evaluate the accuracy of BabbleLabble on
three relation extraction tasks, which we refer to
as Spouse, Disease, and Protein. The goal
of each task is to train a classiﬁer for predicting
whether the two entities in an example are partic-
ipating in the relationship of interest, as described
below.

3.1 Datasets

Statistics for each dataset are reported in Ta-
ble 2, with one example and one explanation for
each given in Figure 4 and additional explanations
shown in Appendix B.

In the Spouse task, annotators were shown a
sentence with two highlighted names and asked to

2We give an example of two such features in Section 4.3.

3https://github.com/HazyResearch/treedlib

# Inputs

Spouse
Disease
Protein

Average

BL

30

50.1
42.3
47.3

46.6

30

15.5
32.1
39.3

28.9

60

15.9
32.6
42.1

30.2

150

16.4
34.4
46.8

32.5

TS

300

17.2
37.5
51.0

35.2

1,000

3,000

10,000

22.8
41.9
57.6

40.8

41.8
44.5
-

43.2

55.0
-
-

55.0

Table 3: F1 scores obtained by a classiﬁer trained with BabbleLabble (BL) using 30 explanations
or with traditional supervision (TS) using the speciﬁed number of individually labeled examples.
BabbleLabble achieves the same F1 score as traditional supervision while using fewer user inputs
by a factor of over 5 (Protein) to over 100 (Spouse).

label whether the sentence suggests that the two
people are spouses. Sentences were pulled from
the Signal Media dataset of news articles (Corney
et al., 2016). Ground truth data was collected from
Amazon Mechanical Turk workers, accepting the
majority label over three annotations. The 30 ex-
planations we report on were sampled randomly
from a pool of 200 that were generated by 10 grad-
uate students unfamiliar with BabbleLabble.

In the Disease task, annotators were shown a
sentence with highlighted names of a chemical and
a disease and asked to label whether the sentence
suggests that the chemical causes the disease. Sen-
tences and ground truth labels came from a por-
tion of the 2015 BioCreative chemical-disease re-
lation dataset (Wei et al., 2015), which contains
abstracts from PubMed. Because this task re-
quires specialized domain expertise, we obtained
explanations by having someone unfamiliar with
BabbleLabble translate from Python to natural
language labeling functions from an existing pub-
lication that explored applying weak supervision
to this task (Ratner et al., 2018).

The Protein task was completed in conjunc-
tion with OccamzRazor, a neuroscience company
targeting biological pathways of Parkinson’s dis-
ease. For this task, annotators were shown a sen-
tence from the relevant biomedical literature with
highlighted names of a protein and a kinase and
asked to label whether or not the kinase inﬂu-
ences the protein in terms of a physical interac-
tion or phosphorylation. The annotators had do-
main expertise but minimal programming experi-
ence, making BabbleLabble a natural ﬁt for their
use case.

3.2 Experimental Settings

Text documents are tokenized with spaCy.4 The
semantic parser is built on top of the Python-based
implementation SippyCup.5 On a single core,
parsing 360 explanations takes approximately two
seconds. We use existing implementations of the
label aggregator, feature library, and discrimina-
tive classiﬁer described in Sections 2.4–2.5 pro-
vided by the open-source project Snorkel (Ratner
et al., 2018).

Hyperparameters for all methods we report
were selected via random search over thirty con-
ﬁgurations on the same held-out development set.
We searched over learning rate, batch size, L2 reg-
ularization, and the subsampling rate (for improv-
ing balance between classes).6 All reported F1
scores are the average value of 40 runs with ran-
dom seeds and otherwise identical settings.

4 Experimental Results

We evaluate the performance of BabbleLabble
with respect to its rate of improvement by number
of user inputs, its dependence on correctly parsed
logical forms, and the mechanism by which it uti-
lizes logical forms.

4.1 High Bandwidth Supervision

In Table 3 we report the average F1 score of a
classiﬁer trained with BabbleLabble using 30 ex-
planations or traditional supervision with the indi-
cated number of labels. On average, it took the
same amount of time to collect 30 explanations

4https://github.com/explosion/spaCy
5https://github.com/wcmac/sippycup
6Hyperparameter ranges:

learning rate (1e-2 to 1e-4),
batch size (32 to 128), L2 regularization (0 to 100), subsam-
pling rate (0 to 0.5)

Pre-ﬁlters

Discarded

Post-ﬁlters

LFs Correct

Sem. Prag.

LFs Correct

Spouse 156
Disease 102
Protein 122

10%
23%
14%

19
34
44

118
40
58

19
28
20

84%
89%
85%

Table 4: The number of LFs generated from 30
explanations (pre-ﬁlters), discarded by the ﬁlter
bank, and remaining (post-ﬁlters), along with the
percentage of LFs that were correctly parsed from
their corresponding explanations.

as 60 labels.7 We observe that in all three tasks,
BabbleLabble achieves a given F1 score with far
fewer user inputs than traditional supervision, by
as much as 100 times in the case of the Spouse
task. Because explanations are applied to many
unlabeled examples, each individual input from
the user can implicitly contribute many (noisy) la-
bels to the learning algorithm.

We also observe, however, that once the num-
ber of labeled examples is sufﬁciently large, tra-
ditional supervision once again dominates, since
ground truth labels are preferable to noisy ones
generated by labeling functions. However, in do-
mains where there is much more unlabeled data
available than labeled data (which in our experi-
ence is most domains), we can gain in supervision
efﬁciency from using BabbleLabble.

Of those explanations that did not produce a
correct LF, 4% were caused by the explanation re-
ferring to unsupported concepts (e.g., one expla-
nation referred to “the subject of the sentence,”
which our simple parser doesn’t support). An-
other 2% were caused by human errors (the cor-
rect LF for the explanation was inconsistent with
the example). The remainder were due to unrecog-
nized paraphrases (e.g., the explanation said “the
order of appearance is X, Y” instead of a sup-
ported phrasing like “X comes before Y”).

4.2 Utility of Incorrect Parses

In Table 4, we report LF summary statistics be-
fore and after ﬁltering. LF correctness is based
on exact match with a manually generated parse
for each explanation. Surprisingly, the simple
heuristic-based ﬁlter bank successfully removes
over 95% of incorrect LFs in all three tasks, re-
sulting in ﬁnal LF sets that are 86% correct on av-

7Zaidan and Eisner (2008) also found that collecting an-
notator rationales in the form of highlighted substrings from
the sentence only doubled annotation time.

Spouse
Disease
Protein

Average

BL-FB

15.7
39.8
38.2

31.2

BL

50.1
42.3
47.3

46.6

BL+PP

49.8
43.2
47.4

46.8

Table 5: F1 scores obtained using BabbleLabble
with no ﬁlter bank (BL-FB), as normal (BL), and
with a perfect parser (BL+PP) simulated by hand.

erage. Furthermore, among those LFs that pass
through the ﬁlter bank, we found that the aver-
age difference in end-task accuracy between cor-
rect and incorrect parses is less than 2.5%. Intu-
itively, the ﬁlters are effective because it is quite
difﬁcult for an LF to be parsed from the explana-
tion, label its own example correctly (passing the
semantic ﬁlter), and not label all examples in the
training set with the same label or identically to
another LF (passing the pragmatic ﬁlter).

We went one step further: using the LFs that
would be produced by a perfect semantic parser as
starting points, we searched for “nearby” LFs (LFs
differing by only one predicate) with higher end-
task accuracy on the test set and succeeded 57%
of the time (see Figure 5 for an example). In other
words, when users provide explanations, the sig-
nals they describe provide good starting points, but
they are actually unlikely to be optimal. This ob-
servation is further supported by Table 5, which
shows that the ﬁlter bank is necessary to remove
clearly irrelevant LFs, but with that in place, the
simple rule-based semantic parser and a perfect
parser have nearly identical average F1 scores.

4.3 Using LFs as Functions or Features

Once we have relevant logical forms from user-
provided explanations, we have multiple options
for how to use them. Srivastava et al. (2017) pro-
pose using these logical forms as features in a lin-
ear classiﬁer, essentially using a traditional super-
vision approach with user-speciﬁed features. We
choose instead to use them as functions for weakly
supervising the creation of a larger training set via
data programming (Ratner et al., 2016).
In Ta-
ble 6, we compare the two approaches directly,
ﬁnding that the the data programming approach
outperforms a feature-based one by 9.5 F1 points
on average with the rule-based parser, and by 4.5
points with a perfect parser.

We attribute this difference primarily to the abil-
ity of data programming to utilize a larger feature

Figure 5: Incorrect LFs often still provide useful signal. On top is an incorrect LF produced for the
Disease task that had the same accuracy as the correct LF. On bottom is a correct LF from the Spouse
task and a more accurate incorrect LF discovered by randomly perturbing one predicate at a time as
described in Section 4.2. (Person 2 is always the second person in the sentence).

BL-DM

BL BL+PP

Feat Feat+PP

Spouse
Disease
Protein

Average

46.5
39.7
40.6

42.3

50.1
42.3
47.3

46.6

49.8
43.2
47.4

46.8

33.9
40.8
36.7

37.1

39.2
43.8
44.0

42.3

Table 6: F1 scores obtained using explanations as
functions for data programming (BL) or features
(Feat), optionally with no discriminative model
(-DM) or using a perfect parser (+PP).

5 Related Work and Discussion

Our work has two themes: modeling natural lan-
guage explanations/instructions and learning from
weak supervision. The closest body of work is
on “learning from natural language.” As men-
tioned earlier, Srivastava et al. (2017) convert nat-
ural language explanations into classiﬁer features
(whereas we convert them into labeling functions).
Goldwasser and Roth (2011) convert natural lan-
guage into concepts (e.g.,
the rules of a card
game). Ling and Fidler (2017) use natural lan-
guage explanations to assist in supervising an im-
age captioning model. Weston (2016); Li et al.
(2016) learn from natural language feedback in a
dialogue. Wang et al. (2017) convert natural lan-
guage deﬁnitions to rules in a semantic parser to
build up progressively higher-level concepts.

We lean on the formalism of semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Liang, 2016). One notable trend is
to learn semantic parsers from weak supervision
(Clarke et al., 2010; Liang et al., 2011), whereas
our goal is to obtain weak supervision signal from
semantic parsers.

The broader topic of weak supervision has re-
ceived much attention; we mention some works
most related to relation extraction. In distant su-
pervision (Craven et al., 1999; Mintz et al., 2009)

Figure 6: When logical forms of natural language
explanations are used as functions for data pro-
gramming (as they are in BabbleLabble), perfor-
mance can improve with the addition of unlabeled
data, whereas using them as features does not ben-
eﬁt from unlabeled data.

set and unlabeled data. In Figure 6, we show how
the data programming approach improves with the
number of unlabeled examples, even as the num-
ber of LFs remains constant. We also observe
qualitatively that data programming exposes the
classiﬁer to additional patterns that are correlated
with our explanations but not mentioned directly.
For example, in the Disease task, two of the fea-
tures weighted most highly by the discriminative
model were the presence of the trigrams “could
produce a” or “support diagnosis of” between the
chemical and disease, despite none of these words
occurring in the explanations for that task. In Ta-
ble 6 we see a 4.3 F1 point improvement (10%)
when we use the discriminative model that can
take advantage of these features rather than apply-
ing the LFs directly to the test set and making pre-
dictions based on the label aggregator’s outputs.

and multi-instance learning (Riedel et al., 2010;
Hoffmann et al., 2011), an existing knowledge
base is used to (probabilistically) impute a train-
ing set. Various extensions have focused on aggre-
gating a variety of supervision sources by learn-
ing generative models from noisy labels (Alfon-
seca et al., 2012; Takamatsu et al., 2012; Roth and
Klakow, 2013; Ratner et al., 2016; Varma et al.,
2017).

Finally, while we have used natural language
explanations as input to train models, they can also
be output to interpret models (Krening et al., 2017;
Lei et al., 2016). More generally, from a machine
learning perspective, labels are the primary asset,
but they are a low bandwidth signal between an-
notators and the learning algorithm. Natural lan-
guage opens up a much higher-bandwidth commu-
nication channel. We have shown promising re-
sults in relation extraction (where one explanation
can be “worth” 100 labels), and it would be inter-
esting to extend our framework to other tasks and
more interactive settings.

Reproducibility

lowship under Grant No. DGE-114747, the Stan-
ford Finch Family Fellowship, the Joseph W. and
Hon Mai Goodman Stanford Graduate Fellowship,
an NSF CAREER Award IIS-1552635, and the
members of the Stanford DAWN project: Face-
book, Google, Intel, Microsoft, NEC, Teradata,
and VMware.

We thank Alex Ratner for his assistance with
data programming, Jason Fries and the many
members of the Hazy Research group and Stan-
ford NLP group who provided feedback and tested
early prototyptes, Kaya Tilev from the Stanford
Graduate School of Business for helpful discus-
sions early on, and the OccamzRazor team: Tarik
Koc, Benjamin Angulo, Katharina S. Volz, and
Charlotte Brzozowski.

The U.S. Government is authorized to repro-
duce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect
the views, policies, or endorsements, either ex-
pressed or implied, of DARPA, DOE, NIH, ONR,
AFOSR, NSF, or the U.S. Government.

The code, data, and experiments for this paper
are available on the CodaLab platform at https:
//worksheets.codalab.org/worksheets/
0x900e7e41deaa4ec5b2fe41dc50594548/.

References

Refactored code with simpliﬁed dependencies,
performance and speed improvements, and inter-
active tutorials can be found on Github:
https://github.com/HazyResearch/babble.

Acknowledgments

We gratefully acknowledge the support of the
following organizations: DARPA under No.
N66001-15-C-4043 (SIMPLEX), No. FA8750-
17-2-0095 (D3M), No.
FA8750-12-2-0335
(XDATA), and No. FA8750-13-2-0039 (DEFT),
108845, NIH under No.
DOE under No.
U54EB020405 (Mobilize), ONR under No.
N000141712266 and No.
N000141310129,
AFOSR under No. 580K753, the Intel/NSF CPS
Security grant No. 1505728, the Michael J. Fox
Foundation for Parkinsons Research under Grant
No. 14672, the Secure Internet of Things Project,
Qualcomm, Ericsson, Analog Devices, the Moore
Foundation, the Okawa Research Grant, Ameri-
can Family Insurance, Accenture, Toshiba, the Na-
tional Science Foundation Graduate Research Fel-

L. V. Ahn, R. Liu, and M. Blum. 2006. Peekaboom: a
game for locating objects in images. In Conference
on Human Factors in Computing Systems (CHI).
pages 55–64.

E. Alfonseca, K. Filippova, J. Delort, and G. Garrido.
2012. Pattern learning for relation extraction with a
hierarchical topic model. In Association for Compu-
tational Linguistics (ACL). pages 54–59.

S. Arora and E. Nyberg. 2009. Interactive annotation
learning with indirect feature voting. In Association
for Computational Linguistics (ACL). pages 55–60.

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL). pages 18–27.

D. Corney, D. Albakour, M. Martinez-Alvarez, and
S. Moussa. 2016. What do a million news articles
look like? In NewsIR@ ECIR. pages 42–47.

M. Craven, J. Kumlien, et al. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In ISMB. pages 77–86.

G. Druck, B. Settles, and A. McCallum. 2009. Active
learning by labeling features. In Empirical Methods
in Natural Language Processing (EMNLP). pages
81–90.

B. Roth and D. Klakow. 2013. Combining generative
and discriminative model scores for distant supervi-
In Empirical Methods in Natural Language
sion.
Processing (EMNLP). pages 24–29.

S. Srivastava, I. Labutov, and T. Mitchell. 2017. Joint
concept learning and semantic parsing from natu-
In Empirical Methods
ral language explanations.
in Natural Language Processing (EMNLP). pages
1528–1537.

S. Takamatsu, I. Sato, and H. Nakagawa. 2012. Reduc-
ing wrong labels in distant supervision for relation
In Association for Computational Lin-
extraction.
guistics (ACL). pages 721–729.

P. Varma, B. He, D. Iter, P. Xu, R. Yu, C. D. Sa, and
C. R’e. 2017. Socratic learning: Augmenting gener-
ative models to incorporate latent subsets in training
data. arXiv preprint arXiv:1610.08123 .

S. I. Wang, S. Ginn, P. Liang, and C. D. Manning.
2017. Naturalizing a programming language via in-
teractive learning. In Association for Computational
Linguistics (ACL).

C. Wei, Y. Peng, R. Leaman, A. P. Davis, C. J.
Mattingly, J. Li, T. C. Wiegers, and Z. Lu. 2015.
Overview of the biocreative V chemical disease re-
lation (cdr) task. In Proceedings of the ﬁfth BioCre-
ative challenge evaluation workshop. pages 154–
166.

J. E. Weston. 2016. Dialog-based language learning.
In Advances in Neural Information Processing Sys-
tems (NIPS). pages 829–837.

O. F. Zaidan and J. Eisner. 2008. Modeling annotators:
A generative approach to learning from annotator ra-
tionales. In Empirical Methods in Natural Language
Processing (EMNLP).

M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic program-
ming. In Association for the Advancement of Arti-
ﬁcial Intelligence (AAAI). pages 1050–1055.

L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classiﬁca-
tion with probabilistic categorial grammars. In Un-
certainty in Artiﬁcial Intelligence (UAI). pages 658–
666.

D. Goldwasser and D. Roth. 2011. Learning from nat-
ural instructions. In International Joint Conference
on Artiﬁcial Intelligence (IJCAI). pages 1794–1800.

R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer,
and D. S. Weld. 2011. Knowledge-based weak su-
pervision for information extraction of overlapping
In Association for Computational Lin-
relations.
guistics (ACL). pages 541–550.

S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell,
M. Riedl, and A. Thomaz. 2017. Learning from ex-
planations using sentiment and advice in RL. IEEE
Transactions on Cognitive and Developmental Sys-
tems 9(1):44–55.

T. Lei, R. Barzilay, and T. Jaakkola. 2016. Rational-
In Empirical Methods in

izing neural predictions.
Natural Language Processing (EMNLP).

J. Li, A. H. Miller, S. Chopra, M. Ranzato, and J. We-
ston. 2016. Learning through dialogue interactions.
arXiv preprint arXiv:1612.04936 .

P. Liang. 2016. Learning executable semantic parsers
for natural language understanding. Communica-
tions of the ACM 59.

P. Liang, M. I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families.
In International Conference on Machine Learning
(ICML).

P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
pages 590–599.

H. Ling and S. Fidler. 2017. Teaching machines to
describe images via natural language feedback. In
Advances in Neural Information Processing Systems
(NIPS).

M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Association for Computational Lin-
guistics (ACL). pages 1003–1011.

H. Raghavan, O. Madani, and R. Jones. 2005.

Inter-
active feature selection. In International Joint Con-
ference on Artiﬁcial Intelligence (IJCAI). volume 5,
pages 841–846.

A. J. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu,
and C. R’e. 2018. Snorkel: Rapid training data cre-
In Very Large Data
ation with weak supervision.
Bases (VLDB).

A. J. Ratner, C. M. D. Sa, S. Wu, D. Selsam, and C. R’e.
2016. Data programming: Creating large training
In Advances in Neural Information
sets, quickly.
Processing Systems (NIPS). pages 3567–3575.

S. Riedel, L. Yao, and A. McCallum. 2010. Model-
ing relations and their mentions without labeled text.
In Machine Learning and Knowledge Discovery in
Databases (ECML PKDD). pages 148–163.

A Predicate Examples

Below are the predicates in the rule-based semantic parser grammar, each of which may have many
supported paraphrases, only one of which is listed here in a minimal example.

Logic
and:
or:
not:
any:
all:
none:

X is true and Y is true

X is true or Y is true

X is not true
Any of X or Y or Z is true
All of X and Y and Z are true
None of X or Y or Z is true

Comparison
=:
(cid:54)=:
<:
≤:
>:
≥:

X is equal to Y
X is not Y
X is smaller than Y
X is no more than Y
X is larger than Y
X is at least Y

X is lowercase
X is upper case

Syntax
lower:
upper:
capital: X is capitalized
all caps: X is in all caps
starts with: X starts with "cardio"
ends with: X ends with "itis"
substring: X contains "-induced"

Named-entity Tags
person: A person is between X and Y
location: A place is within two words of X
date:
number: There are three numbers in the sentence
organization: An organization is right after X

A date is between X and Y

(X, Y) is in Z

X, Y, and Z are true

There is one word between X and Y

Lists
list:
set:
count:
contains: X is in Y
intersection: At least two of X are in Y
map:
filter: There are three capitalized words to the left of X
alias:

X is at the start of a word in Y

A spouse word is in the sentence (“spouse” is a predeﬁned list from the user)

Position
word distance: X is two words before Y
char distance: X is twenty characters after Y
left:
right:
between: X is between Y and Z
within: X is within five words of Y

X is before Y
X is after Y

B Sample Explanations

The following are a sample of the explanations provided by users for each task.

Spouse
Users referred to the ﬁrst person in the sentence as “X” and the second as “Y”.

Label true because "and" occurs between X and Y and "marriage" occurs
one word after person1.

Label true because person Y is preceded by ‘beau’.

Label false because the words "married", "spouse", "husband", and
"wife" do not occur in the sentence.

Label false because there are more than 2 people in the sentence and
"actor" or "actress" is left of person1 or person2.

Label true because the disease is immediately after the chemical and
’induc’ or ’assoc’ is in the chemical name.

Label true because a word containing ’develop’ appears somewhere
before the chemical, and the word ’following’ is between the disease
and the chemical.

Label true because "induced by", "caused by", or "due to" appears
between the chemical and the disease."

Label false because "none", "not", or "no" is within 30 characters to
the left of the disease.

Disease

Protein

Label true because "Ser" or "Tyr" are within 10 characters of the
protein.

Label true because the words "by" or "with" are between the protein
and kinase and the words "no", "not" or "none" are not in between
the protein and kinase and the total number of words between them is
smaller than 10.

Label false because the sentence contains "mRNA", "DNA", or "RNA".

Label false because there are two "," between the protein and the
kinase with less than 30 characters between them.

Training Classiﬁers with Natural Language Explanations

Braden Hancock
Computer Science Dept.
Stanford University
bradenjh@cs.stanford.edu

Paroma Varma
Electrical Engineering Dept.
Stanford University
paroma@stanford.edu

Stephanie Wang
Computer Science Dept.
Stanford University
steph17@stanford.edu

Martin Bringmann
OccamzRazor
San Francisco, CA
martin@occamzrazor.com

Percy Liang
Computer Science Dept.
Stanford University
pliang@cs.stanford.edu

Christopher R´e
Computer Science Dept.
Stanford University
chrismre@cs.stanford.edu

8
1
0
2
 
g
u
A
 
5
2
 
 
]
L
C
.
s
c
[
 
 
4
v
8
1
8
3
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Training accurate classiﬁers requires many
labels, but each label provides only
limited information (one bit for binary
classiﬁcation).
In this work, we propose
BabbleLabble, a framework for training
classiﬁers in which an annotator provides
a natural language explanation for each
A semantic parser
labeling decision.
converts these explanations into program-
matic labeling functions that generate
noisy labels for an arbitrary amount of
unlabeled data, which is used to train a
classiﬁer. On three relation extraction
tasks, we ﬁnd that users are able to
train classiﬁers with comparable F1 scores
from 5–100× faster by providing explana-
tions instead of just labels. Furthermore,
given the inherent imperfection of labeling
functions, we ﬁnd that a simple rule-based
semantic parser sufﬁces.

1

Introduction

The standard protocol for obtaining a labeled
dataset is to have a human annotator view each
example, assess its relevance, and provide a label
(e.g., positive or negative for binary classiﬁcation).
However, this only provides one bit of information
per example. This invites the question: how can
we get more information per example, given that
the annotator has already spent the effort reading
and understanding an example?

Previous works have relied on identifying rel-
evant parts of the input such as labeling features
(Druck et al., 2009; Raghavan et al., 2005; Liang
et al., 2009), highlighting rationale phrases in

Figure 1:
In BabbleLabble, the user provides
a natural language explanation for each label-
ing decision. These explanations are parsed into
labeling functions that convert unlabeled data into
a large labeled dataset for training a classiﬁer.

text (Zaidan and Eisner, 2008; Arora and Nyberg,
2009), or marking relevant regions in images (Ahn
et al., 2006). But there are certain types of infor-
mation which cannot be easily reduced to annotat-
ing a portion of the input, such as the absence of a
certain word, or the presence of at least two words.
In this work, we tap into the power of natural lan-
guage and allow annotators to provide supervision
to a classiﬁer via natural language explanations.

Speciﬁcally, we propose a framework in which
annotators provide a natural language explanation
for each label they assign to an example (see Fig-
ure 1). These explanations are parsed into log-
ical forms representing labeling functions (LFs),
functions that heuristically map examples to labels
(Ratner et al., 2016). The labeling functions are

Figure 2: Natural language explanations are parsed into candidate labeling functions (LFs). Many
incorrect LFs are ﬁltered out automatically by the ﬁlter bank. The remaining functions provide heuristic
labels over the unlabeled dataset, which are aggregated into one noisy label per example, yielding a large,
noisily-labeled training set for a classiﬁer.

then executed on many unlabeled examples, re-
sulting in a large, weakly-supervised training set
that is then used to train a classiﬁer.

Semantic parsing of natural language into log-
ical forms is recognized as a challenging prob-
lem and has been studied extensively (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Liang et al., 2011; Liang, 2016). One of our ma-
jor ﬁndings is that in our setting, even a simple
rule-based semantic parser sufﬁces for three rea-
sons: First, we ﬁnd that the majority of incorrect
LFs can be automatically ﬁltered out either seman-
tically (e.g., is it consistent with the associated ex-
ample?) or pragmatically (e.g., does it avoid as-
signing the same label to the entire training set?).
Second, LFs near the gold LF in the space of logi-
cal forms are often just as accurate (and sometimes
even more accurate). Third, techniques for com-
bining weak supervision sources are built to toler-
ate some noise (Alfonseca et al., 2012; Takamatsu
et al., 2012; Ratner et al., 2018). The signiﬁcance
of this is that we can deploy the same semantic
parser across tasks without task-speciﬁc training.
We show how we can tackle a real-world biomedi-
cal application with the same semantic parser used
to extract instances of spouses.

simple rule-based parser. In Section 4, we ﬁnd that
in our weak supervision framework, the rule-based
semantic parser and the perfect parser yield nearly
identical downstream performance. Second, while
they use the logical forms of explanations to pro-
duce features that are fed directly to a classiﬁer, we
use them as functions for labeling a much larger
training set. In Section 4, we show that using func-
tions yields a 9.5 F1 improvement (26% relative
improvement) over features, and that the F1 score
scales with the amount of available unlabeled data.
We validate our approach on two existing
datasets from the literature (extracting spouses
from news articles and disease-causing chemi-
cals from biomedical abstracts) and one real-
world use case with our biomedical collabora-
tors at OccamzRazor to extract protein-kinase
interactions related to Parkinson’s disease from
text. We ﬁnd empirically that users are able to
train classiﬁers with comparable F1 scores up to
100× faster when they provide natural language
explanations instead of individual labels. Our
code is available at https://github.com/
HazyResearch/babble.

2 The BabbleLabble Framework

Our work is most similar to that of Srivastava
et al. (2017), who also use natural language expla-
nations to train a classiﬁer, but with two important
differences. First, they jointly train a task-speciﬁc
semantic parser and classiﬁer, whereas we use a

The BabbleLabble framework converts natural
language explanations and unlabeled data into a
noisily-labeled training set (see Figure 2). There
are three key components: a semantic parser, a
ﬁlter bank, and a label aggregator. The semantic

Figure 3: Valid parses are found by iterating over increasingly large subspans of the input looking for
matches among the right hand sides of the rules in the grammar. Rules are either lexical (converting
tokens into symbols), unary (converting one symbol into another symbol), or compositional (combining
many symbols into a single higher-order symbol). A rule may optionally ignore unrecognized tokens in
a span (denoted here with a dashed line).

parser converts natural language explanations into
a set of logical forms representing labeling func-
tions (LFs). The ﬁlter bank removes as many in-
correct LFs as possible without requiring ground
truth labels. The remaining LFs are applied to un-
labeled examples to produce a matrix of labels.
This label matrix is passed into the label aggre-
gator, which combines these potentially conﬂict-
ing and overlapping labels into one label for each
example. The resulting labeled examples are then
used to train an arbitrary discriminative model.

2.1 Explanations

To create the input explanations, the user views a
subset S of an unlabeled dataset D (where |S| (cid:28)
|D|) and provides for each input xi ∈ S a label
yi and a natural language explanation ei, a sen-
tence explaining why the example should receive
that label. The explanation ei generally refers to
speciﬁc aspects of the example (e.g., in Figure 2,
the location of a speciﬁc string “his wife”).

2.2 Semantic Parser

The semantic parser takes a natural language ex-
planation ei and returns a set of LFs (logical forms
or labeling functions) {f1, . . . , fk} of the form
fi
: X → {−1, 0, 1} in a binary classiﬁcation
setting, with 0 representing abstention. We em-
phasize that the goal of this semantic parser is not
to generate the single correct parse, but rather to
have coverage over many potentially useful LFs.1

1Indeed, we ﬁnd empirically that an incorrect LF nearby
the correct one in the space of logical forms actually has
higher end-task accuracy 57% of the time (see Section 4.2).

We choose a simple rule-based semantic parser
that can be used without any training. Formally,
the parser uses a set of rules of the form α → β,
where α can be replaced by the token(s) in β (see
Figure 3 for example rules). To identify candidate
LFs, we recursively construct a set of valid parses
for each span of the explanation, based on the sub-
stitutions deﬁned by the grammar rules. At the
end, the parser returns all valid parses (LFs in our
case) corresponding to the entire explanation.

We also allow an arbitrary number of tokens in
a given span to be ignored when looking for a
matching rule. This improves the ability of the
parser to handle unexpected input, such as un-
known words or typos, since the portions of the
input that are parseable can still result in a valid
parse. For example, in Figure 3, the word “per-
son” is ignored.

All predicates included in our grammar (sum-
marized in Table 1) are provided to annota-
tors, with minimal examples of each in use
(Appendix A).
Importantly, all rules are do-
main independent (e.g., all three relation extrac-
tion tasks that we tested used the same grammar),
making the semantic parser easily transferrable to
new domains. Additionally, while this paper fo-
cuses on the task of relation extraction, in princi-
ple the BabbleLabble framework can be applied
to other tasks or settings by extending the grammar
with the necessary primitives (e.g., adding primi-
tives for rows and columns to enable explanations
about the alignments of words in tables). To guide
the construction of the grammar, we collected 500
explanations for the Spouse domain from workers

Predicate

Description

bool, string,
int, float, tuple,
list, set
and, or, not, any,
all, none
=, (cid:54)=, <, ≤, >, ≥
lower, upper,
capital, all caps
starts with,
ends with,
substring
person, location,
date, number,
organization
alias

count, contains,
intersection

map, filter

word distance,
character distance

left, right,
between, within

Standard primitive data types

Standard logic operators

Standard comparison operators
Return True for strings of the
corresponding case
Return True if the ﬁrst string
starts/ends with or contains the
second
Return True if a string has the
corresponding NER tag

A frequently used list of words
may be predeﬁned and referred
to with an alias
Operators for checking size,
membership, or common ele-
ments of a list/set
Apply a functional primitive to
each member of list/set to
transform or ﬁlter the elements
Return the distance between
two strings by words or charac-
ters
Return as a string the text that is
left/right/within some distance
of a string or between two des-
ignated strings

Table 1: Predicates in the grammar supported by
BabbleLabble’s rule-based semantic parser.

on Amazon Mechanical Turk and added support
for the most commonly used predicates. These
were added before the experiments described in
Section 4. The grammar contains a total of 200
rule templates.

2.3 Filter Bank

The input to the ﬁlter bank is a set of candidate
LFs produced by the semantic parser. The pur-
pose of the ﬁlter bank is to discard as many incor-
rect LFs as possible without requiring additional
labels. It consists of two classes of ﬁlters: seman-
tic and pragmatic.

Recall that each explanation ei is collected in
the context of a speciﬁc labeled example (xi, yi).
The semantic ﬁlter checks for LFs that are in-
consistent with their corresponding example; for-
mally, any LF f for which f (xi) (cid:54)= yi is discarded.
For example, in the ﬁrst explanation in Figure 2,
the word “right” can be interpreted as either “im-
mediately” (as in “right before”) or simply “to the

right.” The latter interpretation results in a func-
tion that is inconsistent with the associated exam-
ple (since “his wife” is actually to the left of person
2), so it can be safely removed.

The pragmatic ﬁlters removes LFs that are con-
stant, redundant, or correlated. For example, in
Figure 2, LF 2a is constant, as it labels every ex-
ample positively (since all examples contain two
people from the same sentence). LF 3b is redun-
dant, since even though it has a different syntax
tree from LF 3a, it labels the training set identi-
cally and therefore provides no new signal.

Finally, out of all LFs from the same explana-
tion that pass all the other ﬁlters, we keep only
the most speciﬁc (lowest coverage) LF. This pre-
vents multiple correlated LFs from a single exam-
ple from dominating.

As we show in Section 4, over three tasks, the
ﬁlter bank removes over 95% of incorrect parses,
and the incorrect ones that remain have average
end-task accuracy within 2.5 points of the corre-
sponding correct parses.

2.4 Label Aggregator

The label aggregator combines multiple (poten-
tially conﬂicting) suggested labels from the LFs
and combines them into a single probabilistic la-
if m LFs pass
bel per example. Concretely,
the ﬁlter bank and are applied to n examples,
the label aggregator implements a function f :
{−1, 0, 1}m×n → [0, 1]n.

A naive solution would be to use a simple ma-
jority vote, but this fails to account for the fact
that LFs can vary widely in accuracy and cover-
Instead, we use data programming (Ratner
age.
et al., 2016), which models the relationship be-
tween the true labels and the output of the label-
ing functions as a factor graph. More speciﬁcally,
given the true labels Y ∈ {−1, 1}n (latent) and la-
bel matrix Λ ∈ {−1, 0, 1}m×n (observed) where
Λi,j = LFi(xj), we deﬁne two types of factors
representing labeling propensity and accuracy:

i,j (Λ, Y ) = 1{Λi,j (cid:54)= 0}
φLab
i,j (Λ, Y ) = 1{Λi,j = yj}.
φAcc

(1)

(2)

Denoting the vector of factors pertaining to a given
data point xj as φj(Λ, Y ) ∈ Rm, deﬁne the model:

pw(Λ, Y ) = Z−1

w exp

w · φj(Λ, Y )

(3)

(cid:17)

,

(cid:16) n
(cid:88)

j=1

Figure 4: An example and explanation for each of the three datasets.

where w ∈ R2m is the weight vector and Zw is
the normalization constant. To learn this model
without knowing the true labels Y , we minimize
the negative log marginal likelihood given the ob-
served labels Λ:

ˆw = arg min

− log

pw(Λ, Y )

(4)

w

(cid:88)

Y

using SGD and Gibbs sampling for inference, and
then use the marginals p ˆw(Y | Λ) as probabilistic
training labels.

Intuitively, we infer accuracies of the LFs based
on the way they overlap and conﬂict with one an-
other. Since noisier LFs are more likely to have
high conﬂict rates with others, their correspond-
ing accuracy weights in w will be smaller, reduc-
ing their inﬂuence on the aggregated labels.

2.5 Discriminative Model

The noisy training set that the label aggregator
outputs is used to train an arbitrary discriminative
model. One advantage of training a discriminative
model on the task instead of using the label ag-
gregator as a classiﬁer directly is that the label ag-
gregator only takes into account those signals in-
cluded in the LFs. A discriminative model, on the
other hand, can incorporate features that were not
identiﬁed by the user but are nevertheless informa-
tive.2 Consequently, even examples for which all
LFs abstained can still be classiﬁed correctly. Ad-
ditionally, passing supervision information from
the user to the model in the form of a dataset—
rather than hard rules—promotes generalization in
the new model (rather than memorization), similar
to distant supervision (Mintz et al., 2009). On the
three tasks we evaluate, using the discriminative
model averages 4.3 F1 points higher than using the
label aggregator directly.

For the results reported in this paper, our dis-
criminative model is a simple logistic regression

Task

Train

Dev

Test % Pos.

Spouse
Disease
Protein

22195
6667
5546

2796
773
1011

2697
4101
1058

8%
23%
22%

Table 2: The total number of unlabeled train-
ing examples (a pair of annotated entities in a
sentence), labeled development examples (for hy-
perparameter tuning), labeled test examples (for
assessment), and the fraction of positive labels in
the test split.

classiﬁer with generic features deﬁned over depen-
dency paths.3 These features include unigrams,
bigrams, and trigrams of lemmas, dependency la-
bels, and part of speech tags found in the siblings,
parents, and nodes between the entities in the de-
pendency parse of the sentence. We found this to
perform better on average than a biLSTM, particu-
larly for the traditional supervision baselines with
small training set sizes; it also provided easily in-
terpretable features for analysis.

3 Experimental Setup

We evaluate the accuracy of BabbleLabble on
three relation extraction tasks, which we refer to
as Spouse, Disease, and Protein. The goal
of each task is to train a classiﬁer for predicting
whether the two entities in an example are partic-
ipating in the relationship of interest, as described
below.

3.1 Datasets

Statistics for each dataset are reported in Ta-
ble 2, with one example and one explanation for
each given in Figure 4 and additional explanations
shown in Appendix B.

In the Spouse task, annotators were shown a
sentence with two highlighted names and asked to

2We give an example of two such features in Section 4.3.

3https://github.com/HazyResearch/treedlib

# Inputs

Spouse
Disease
Protein

Average

BL

30

50.1
42.3
47.3

46.6

30

15.5
32.1
39.3

28.9

60

15.9
32.6
42.1

30.2

150

16.4
34.4
46.8

32.5

TS

300

17.2
37.5
51.0

35.2

1,000

3,000

10,000

22.8
41.9
57.6

40.8

41.8
44.5
-

43.2

55.0
-
-

55.0

Table 3: F1 scores obtained by a classiﬁer trained with BabbleLabble (BL) using 30 explanations
or with traditional supervision (TS) using the speciﬁed number of individually labeled examples.
BabbleLabble achieves the same F1 score as traditional supervision while using fewer user inputs
by a factor of over 5 (Protein) to over 100 (Spouse).

label whether the sentence suggests that the two
people are spouses. Sentences were pulled from
the Signal Media dataset of news articles (Corney
et al., 2016). Ground truth data was collected from
Amazon Mechanical Turk workers, accepting the
majority label over three annotations. The 30 ex-
planations we report on were sampled randomly
from a pool of 200 that were generated by 10 grad-
uate students unfamiliar with BabbleLabble.

In the Disease task, annotators were shown a
sentence with highlighted names of a chemical and
a disease and asked to label whether the sentence
suggests that the chemical causes the disease. Sen-
tences and ground truth labels came from a por-
tion of the 2015 BioCreative chemical-disease re-
lation dataset (Wei et al., 2015), which contains
abstracts from PubMed. Because this task re-
quires specialized domain expertise, we obtained
explanations by having someone unfamiliar with
BabbleLabble translate from Python to natural
language labeling functions from an existing pub-
lication that explored applying weak supervision
to this task (Ratner et al., 2018).

The Protein task was completed in conjunc-
tion with OccamzRazor, a neuroscience company
targeting biological pathways of Parkinson’s dis-
ease. For this task, annotators were shown a sen-
tence from the relevant biomedical literature with
highlighted names of a protein and a kinase and
asked to label whether or not the kinase inﬂu-
ences the protein in terms of a physical interac-
tion or phosphorylation. The annotators had do-
main expertise but minimal programming experi-
ence, making BabbleLabble a natural ﬁt for their
use case.

3.2 Experimental Settings

Text documents are tokenized with spaCy.4 The
semantic parser is built on top of the Python-based
implementation SippyCup.5 On a single core,
parsing 360 explanations takes approximately two
seconds. We use existing implementations of the
label aggregator, feature library, and discrimina-
tive classiﬁer described in Sections 2.4–2.5 pro-
vided by the open-source project Snorkel (Ratner
et al., 2018).

Hyperparameters for all methods we report
were selected via random search over thirty con-
ﬁgurations on the same held-out development set.
We searched over learning rate, batch size, L2 reg-
ularization, and the subsampling rate (for improv-
ing balance between classes).6 All reported F1
scores are the average value of 40 runs with ran-
dom seeds and otherwise identical settings.

4 Experimental Results

We evaluate the performance of BabbleLabble
with respect to its rate of improvement by number
of user inputs, its dependence on correctly parsed
logical forms, and the mechanism by which it uti-
lizes logical forms.

4.1 High Bandwidth Supervision

In Table 3 we report the average F1 score of a
classiﬁer trained with BabbleLabble using 30 ex-
planations or traditional supervision with the indi-
cated number of labels. On average, it took the
same amount of time to collect 30 explanations

4https://github.com/explosion/spaCy
5https://github.com/wcmac/sippycup
6Hyperparameter ranges:

learning rate (1e-2 to 1e-4),
batch size (32 to 128), L2 regularization (0 to 100), subsam-
pling rate (0 to 0.5)

Pre-ﬁlters

Discarded

Post-ﬁlters

LFs Correct

Sem. Prag.

LFs Correct

Spouse 156
Disease 102
Protein 122

10%
23%
14%

19
34
44

118
40
58

19
28
20

84%
89%
85%

Table 4: The number of LFs generated from 30
explanations (pre-ﬁlters), discarded by the ﬁlter
bank, and remaining (post-ﬁlters), along with the
percentage of LFs that were correctly parsed from
their corresponding explanations.

as 60 labels.7 We observe that in all three tasks,
BabbleLabble achieves a given F1 score with far
fewer user inputs than traditional supervision, by
as much as 100 times in the case of the Spouse
task. Because explanations are applied to many
unlabeled examples, each individual input from
the user can implicitly contribute many (noisy) la-
bels to the learning algorithm.

We also observe, however, that once the num-
ber of labeled examples is sufﬁciently large, tra-
ditional supervision once again dominates, since
ground truth labels are preferable to noisy ones
generated by labeling functions. However, in do-
mains where there is much more unlabeled data
available than labeled data (which in our experi-
ence is most domains), we can gain in supervision
efﬁciency from using BabbleLabble.

Of those explanations that did not produce a
correct LF, 4% were caused by the explanation re-
ferring to unsupported concepts (e.g., one expla-
nation referred to “the subject of the sentence,”
which our simple parser doesn’t support). An-
other 2% were caused by human errors (the cor-
rect LF for the explanation was inconsistent with
the example). The remainder were due to unrecog-
nized paraphrases (e.g., the explanation said “the
order of appearance is X, Y” instead of a sup-
ported phrasing like “X comes before Y”).

4.2 Utility of Incorrect Parses

In Table 4, we report LF summary statistics be-
fore and after ﬁltering. LF correctness is based
on exact match with a manually generated parse
for each explanation. Surprisingly, the simple
heuristic-based ﬁlter bank successfully removes
over 95% of incorrect LFs in all three tasks, re-
sulting in ﬁnal LF sets that are 86% correct on av-

7Zaidan and Eisner (2008) also found that collecting an-
notator rationales in the form of highlighted substrings from
the sentence only doubled annotation time.

Spouse
Disease
Protein

Average

BL-FB

15.7
39.8
38.2

31.2

BL

50.1
42.3
47.3

46.6

BL+PP

49.8
43.2
47.4

46.8

Table 5: F1 scores obtained using BabbleLabble
with no ﬁlter bank (BL-FB), as normal (BL), and
with a perfect parser (BL+PP) simulated by hand.

erage. Furthermore, among those LFs that pass
through the ﬁlter bank, we found that the aver-
age difference in end-task accuracy between cor-
rect and incorrect parses is less than 2.5%. Intu-
itively, the ﬁlters are effective because it is quite
difﬁcult for an LF to be parsed from the explana-
tion, label its own example correctly (passing the
semantic ﬁlter), and not label all examples in the
training set with the same label or identically to
another LF (passing the pragmatic ﬁlter).

We went one step further: using the LFs that
would be produced by a perfect semantic parser as
starting points, we searched for “nearby” LFs (LFs
differing by only one predicate) with higher end-
task accuracy on the test set and succeeded 57%
of the time (see Figure 5 for an example). In other
words, when users provide explanations, the sig-
nals they describe provide good starting points, but
they are actually unlikely to be optimal. This ob-
servation is further supported by Table 5, which
shows that the ﬁlter bank is necessary to remove
clearly irrelevant LFs, but with that in place, the
simple rule-based semantic parser and a perfect
parser have nearly identical average F1 scores.

4.3 Using LFs as Functions or Features

Once we have relevant logical forms from user-
provided explanations, we have multiple options
for how to use them. Srivastava et al. (2017) pro-
pose using these logical forms as features in a lin-
ear classiﬁer, essentially using a traditional super-
vision approach with user-speciﬁed features. We
choose instead to use them as functions for weakly
supervising the creation of a larger training set via
data programming (Ratner et al., 2016).
In Ta-
ble 6, we compare the two approaches directly,
ﬁnding that the the data programming approach
outperforms a feature-based one by 9.5 F1 points
on average with the rule-based parser, and by 4.5
points with a perfect parser.

We attribute this difference primarily to the abil-
ity of data programming to utilize a larger feature

Figure 5: Incorrect LFs often still provide useful signal. On top is an incorrect LF produced for the
Disease task that had the same accuracy as the correct LF. On bottom is a correct LF from the Spouse
task and a more accurate incorrect LF discovered by randomly perturbing one predicate at a time as
described in Section 4.2. (Person 2 is always the second person in the sentence).

BL-DM

BL BL+PP

Feat Feat+PP

Spouse
Disease
Protein

Average

46.5
39.7
40.6

42.3

50.1
42.3
47.3

46.6

49.8
43.2
47.4

46.8

33.9
40.8
36.7

37.1

39.2
43.8
44.0

42.3

Table 6: F1 scores obtained using explanations as
functions for data programming (BL) or features
(Feat), optionally with no discriminative model
(-DM) or using a perfect parser (+PP).

5 Related Work and Discussion

Our work has two themes: modeling natural lan-
guage explanations/instructions and learning from
weak supervision. The closest body of work is
on “learning from natural language.” As men-
tioned earlier, Srivastava et al. (2017) convert nat-
ural language explanations into classiﬁer features
(whereas we convert them into labeling functions).
Goldwasser and Roth (2011) convert natural lan-
guage into concepts (e.g.,
the rules of a card
game). Ling and Fidler (2017) use natural lan-
guage explanations to assist in supervising an im-
age captioning model. Weston (2016); Li et al.
(2016) learn from natural language feedback in a
dialogue. Wang et al. (2017) convert natural lan-
guage deﬁnitions to rules in a semantic parser to
build up progressively higher-level concepts.

We lean on the formalism of semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Liang, 2016). One notable trend is
to learn semantic parsers from weak supervision
(Clarke et al., 2010; Liang et al., 2011), whereas
our goal is to obtain weak supervision signal from
semantic parsers.

The broader topic of weak supervision has re-
ceived much attention; we mention some works
most related to relation extraction. In distant su-
pervision (Craven et al., 1999; Mintz et al., 2009)

Figure 6: When logical forms of natural language
explanations are used as functions for data pro-
gramming (as they are in BabbleLabble), perfor-
mance can improve with the addition of unlabeled
data, whereas using them as features does not ben-
eﬁt from unlabeled data.

set and unlabeled data. In Figure 6, we show how
the data programming approach improves with the
number of unlabeled examples, even as the num-
ber of LFs remains constant. We also observe
qualitatively that data programming exposes the
classiﬁer to additional patterns that are correlated
with our explanations but not mentioned directly.
For example, in the Disease task, two of the fea-
tures weighted most highly by the discriminative
model were the presence of the trigrams “could
produce a” or “support diagnosis of” between the
chemical and disease, despite none of these words
occurring in the explanations for that task. In Ta-
ble 6 we see a 4.3 F1 point improvement (10%)
when we use the discriminative model that can
take advantage of these features rather than apply-
ing the LFs directly to the test set and making pre-
dictions based on the label aggregator’s outputs.

and multi-instance learning (Riedel et al., 2010;
Hoffmann et al., 2011), an existing knowledge
base is used to (probabilistically) impute a train-
ing set. Various extensions have focused on aggre-
gating a variety of supervision sources by learn-
ing generative models from noisy labels (Alfon-
seca et al., 2012; Takamatsu et al., 2012; Roth and
Klakow, 2013; Ratner et al., 2016; Varma et al.,
2017).

Finally, while we have used natural language
explanations as input to train models, they can also
be output to interpret models (Krening et al., 2017;
Lei et al., 2016). More generally, from a machine
learning perspective, labels are the primary asset,
but they are a low bandwidth signal between an-
notators and the learning algorithm. Natural lan-
guage opens up a much higher-bandwidth commu-
nication channel. We have shown promising re-
sults in relation extraction (where one explanation
can be “worth” 100 labels), and it would be inter-
esting to extend our framework to other tasks and
more interactive settings.

Reproducibility

lowship under Grant No. DGE-114747, the Stan-
ford Finch Family Fellowship, the Joseph W. and
Hon Mai Goodman Stanford Graduate Fellowship,
an NSF CAREER Award IIS-1552635, and the
members of the Stanford DAWN project: Face-
book, Google, Intel, Microsoft, NEC, Teradata,
and VMware.

We thank Alex Ratner for his assistance with
data programming, Jason Fries and the many
members of the Hazy Research group and Stan-
ford NLP group who provided feedback and tested
early prototyptes, Kaya Tilev from the Stanford
Graduate School of Business for helpful discus-
sions early on, and the OccamzRazor team: Tarik
Koc, Benjamin Angulo, Katharina S. Volz, and
Charlotte Brzozowski.

The U.S. Government is authorized to repro-
duce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect
the views, policies, or endorsements, either ex-
pressed or implied, of DARPA, DOE, NIH, ONR,
AFOSR, NSF, or the U.S. Government.

The code, data, and experiments for this paper
are available on the CodaLab platform at https:
//worksheets.codalab.org/worksheets/
0x900e7e41deaa4ec5b2fe41dc50594548/.

References

Refactored code with simpliﬁed dependencies,
performance and speed improvements, and inter-
active tutorials can be found on Github:
https://github.com/HazyResearch/babble.

Acknowledgments

We gratefully acknowledge the support of the
following organizations: DARPA under No.
N66001-15-C-4043 (SIMPLEX), No. FA8750-
17-2-0095 (D3M), No.
FA8750-12-2-0335
(XDATA), and No. FA8750-13-2-0039 (DEFT),
108845, NIH under No.
DOE under No.
U54EB020405 (Mobilize), ONR under No.
N000141712266 and No.
N000141310129,
AFOSR under No. 580K753, the Intel/NSF CPS
Security grant No. 1505728, the Michael J. Fox
Foundation for Parkinsons Research under Grant
No. 14672, the Secure Internet of Things Project,
Qualcomm, Ericsson, Analog Devices, the Moore
Foundation, the Okawa Research Grant, Ameri-
can Family Insurance, Accenture, Toshiba, the Na-
tional Science Foundation Graduate Research Fel-

L. V. Ahn, R. Liu, and M. Blum. 2006. Peekaboom: a
game for locating objects in images. In Conference
on Human Factors in Computing Systems (CHI).
pages 55–64.

E. Alfonseca, K. Filippova, J. Delort, and G. Garrido.
2012. Pattern learning for relation extraction with a
hierarchical topic model. In Association for Compu-
tational Linguistics (ACL). pages 54–59.

S. Arora and E. Nyberg. 2009. Interactive annotation
learning with indirect feature voting. In Association
for Computational Linguistics (ACL). pages 55–60.

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL). pages 18–27.

D. Corney, D. Albakour, M. Martinez-Alvarez, and
S. Moussa. 2016. What do a million news articles
look like? In NewsIR@ ECIR. pages 42–47.

M. Craven, J. Kumlien, et al. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In ISMB. pages 77–86.

G. Druck, B. Settles, and A. McCallum. 2009. Active
learning by labeling features. In Empirical Methods
in Natural Language Processing (EMNLP). pages
81–90.

B. Roth and D. Klakow. 2013. Combining generative
and discriminative model scores for distant supervi-
In Empirical Methods in Natural Language
sion.
Processing (EMNLP). pages 24–29.

S. Srivastava, I. Labutov, and T. Mitchell. 2017. Joint
concept learning and semantic parsing from natu-
In Empirical Methods
ral language explanations.
in Natural Language Processing (EMNLP). pages
1528–1537.

S. Takamatsu, I. Sato, and H. Nakagawa. 2012. Reduc-
ing wrong labels in distant supervision for relation
In Association for Computational Lin-
extraction.
guistics (ACL). pages 721–729.

P. Varma, B. He, D. Iter, P. Xu, R. Yu, C. D. Sa, and
C. R’e. 2017. Socratic learning: Augmenting gener-
ative models to incorporate latent subsets in training
data. arXiv preprint arXiv:1610.08123 .

S. I. Wang, S. Ginn, P. Liang, and C. D. Manning.
2017. Naturalizing a programming language via in-
teractive learning. In Association for Computational
Linguistics (ACL).

C. Wei, Y. Peng, R. Leaman, A. P. Davis, C. J.
Mattingly, J. Li, T. C. Wiegers, and Z. Lu. 2015.
Overview of the biocreative V chemical disease re-
lation (cdr) task. In Proceedings of the ﬁfth BioCre-
ative challenge evaluation workshop. pages 154–
166.

J. E. Weston. 2016. Dialog-based language learning.
In Advances in Neural Information Processing Sys-
tems (NIPS). pages 829–837.

O. F. Zaidan and J. Eisner. 2008. Modeling annotators:
A generative approach to learning from annotator ra-
tionales. In Empirical Methods in Natural Language
Processing (EMNLP).

M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic program-
ming. In Association for the Advancement of Arti-
ﬁcial Intelligence (AAAI). pages 1050–1055.

L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classiﬁca-
tion with probabilistic categorial grammars. In Un-
certainty in Artiﬁcial Intelligence (UAI). pages 658–
666.

D. Goldwasser and D. Roth. 2011. Learning from nat-
ural instructions. In International Joint Conference
on Artiﬁcial Intelligence (IJCAI). pages 1794–1800.

R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer,
and D. S. Weld. 2011. Knowledge-based weak su-
pervision for information extraction of overlapping
In Association for Computational Lin-
relations.
guistics (ACL). pages 541–550.

S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell,
M. Riedl, and A. Thomaz. 2017. Learning from ex-
planations using sentiment and advice in RL. IEEE
Transactions on Cognitive and Developmental Sys-
tems 9(1):44–55.

T. Lei, R. Barzilay, and T. Jaakkola. 2016. Rational-
In Empirical Methods in

izing neural predictions.
Natural Language Processing (EMNLP).

J. Li, A. H. Miller, S. Chopra, M. Ranzato, and J. We-
ston. 2016. Learning through dialogue interactions.
arXiv preprint arXiv:1612.04936 .

P. Liang. 2016. Learning executable semantic parsers
for natural language understanding. Communica-
tions of the ACM 59.

P. Liang, M. I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families.
In International Conference on Machine Learning
(ICML).

P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
pages 590–599.

H. Ling and S. Fidler. 2017. Teaching machines to
describe images via natural language feedback. In
Advances in Neural Information Processing Systems
(NIPS).

M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Association for Computational Lin-
guistics (ACL). pages 1003–1011.

H. Raghavan, O. Madani, and R. Jones. 2005.

Inter-
active feature selection. In International Joint Con-
ference on Artiﬁcial Intelligence (IJCAI). volume 5,
pages 841–846.

A. J. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu,
and C. R’e. 2018. Snorkel: Rapid training data cre-
In Very Large Data
ation with weak supervision.
Bases (VLDB).

A. J. Ratner, C. M. D. Sa, S. Wu, D. Selsam, and C. R’e.
2016. Data programming: Creating large training
In Advances in Neural Information
sets, quickly.
Processing Systems (NIPS). pages 3567–3575.

S. Riedel, L. Yao, and A. McCallum. 2010. Model-
ing relations and their mentions without labeled text.
In Machine Learning and Knowledge Discovery in
Databases (ECML PKDD). pages 148–163.

A Predicate Examples

Below are the predicates in the rule-based semantic parser grammar, each of which may have many
supported paraphrases, only one of which is listed here in a minimal example.

Logic
and:
or:
not:
any:
all:
none:

X is true and Y is true

X is true or Y is true

X is not true
Any of X or Y or Z is true
All of X and Y and Z are true
None of X or Y or Z is true

Comparison
=:
(cid:54)=:
<:
≤:
>:
≥:

X is equal to Y
X is not Y
X is smaller than Y
X is no more than Y
X is larger than Y
X is at least Y

X is lowercase
X is upper case

Syntax
lower:
upper:
capital: X is capitalized
all caps: X is in all caps
starts with: X starts with "cardio"
ends with: X ends with "itis"
substring: X contains "-induced"

Named-entity Tags
person: A person is between X and Y
location: A place is within two words of X
date:
number: There are three numbers in the sentence
organization: An organization is right after X

A date is between X and Y

(X, Y) is in Z

X, Y, and Z are true

There is one word between X and Y

Lists
list:
set:
count:
contains: X is in Y
intersection: At least two of X are in Y
map:
filter: There are three capitalized words to the left of X
alias:

X is at the start of a word in Y

A spouse word is in the sentence (“spouse” is a predeﬁned list from the user)

Position
word distance: X is two words before Y
char distance: X is twenty characters after Y
left:
right:
between: X is between Y and Z
within: X is within five words of Y

X is before Y
X is after Y

B Sample Explanations

The following are a sample of the explanations provided by users for each task.

Spouse
Users referred to the ﬁrst person in the sentence as “X” and the second as “Y”.

Label true because "and" occurs between X and Y and "marriage" occurs
one word after person1.

Label true because person Y is preceded by ‘beau’.

Label false because the words "married", "spouse", "husband", and
"wife" do not occur in the sentence.

Label false because there are more than 2 people in the sentence and
"actor" or "actress" is left of person1 or person2.

Label true because the disease is immediately after the chemical and
’induc’ or ’assoc’ is in the chemical name.

Label true because a word containing ’develop’ appears somewhere
before the chemical, and the word ’following’ is between the disease
and the chemical.

Label true because "induced by", "caused by", or "due to" appears
between the chemical and the disease."

Label false because "none", "not", or "no" is within 30 characters to
the left of the disease.

Disease

Protein

Label true because "Ser" or "Tyr" are within 10 characters of the
protein.

Label true because the words "by" or "with" are between the protein
and kinase and the words "no", "not" or "none" are not in between
the protein and kinase and the total number of words between them is
smaller than 10.

Label false because the sentence contains "mRNA", "DNA", or "RNA".

Label false because there are two "," between the protein and the
kinase with less than 30 characters between them.

Training Classiﬁers with Natural Language Explanations

Braden Hancock
Computer Science Dept.
Stanford University
bradenjh@cs.stanford.edu

Paroma Varma
Electrical Engineering Dept.
Stanford University
paroma@stanford.edu

Stephanie Wang
Computer Science Dept.
Stanford University
steph17@stanford.edu

Martin Bringmann
OccamzRazor
San Francisco, CA
martin@occamzrazor.com

Percy Liang
Computer Science Dept.
Stanford University
pliang@cs.stanford.edu

Christopher R´e
Computer Science Dept.
Stanford University
chrismre@cs.stanford.edu

8
1
0
2
 
g
u
A
 
5
2
 
 
]
L
C
.
s
c
[
 
 
4
v
8
1
8
3
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Training accurate classiﬁers requires many
labels, but each label provides only
limited information (one bit for binary
classiﬁcation).
In this work, we propose
BabbleLabble, a framework for training
classiﬁers in which an annotator provides
a natural language explanation for each
A semantic parser
labeling decision.
converts these explanations into program-
matic labeling functions that generate
noisy labels for an arbitrary amount of
unlabeled data, which is used to train a
classiﬁer. On three relation extraction
tasks, we ﬁnd that users are able to
train classiﬁers with comparable F1 scores
from 5–100× faster by providing explana-
tions instead of just labels. Furthermore,
given the inherent imperfection of labeling
functions, we ﬁnd that a simple rule-based
semantic parser sufﬁces.

1

Introduction

The standard protocol for obtaining a labeled
dataset is to have a human annotator view each
example, assess its relevance, and provide a label
(e.g., positive or negative for binary classiﬁcation).
However, this only provides one bit of information
per example. This invites the question: how can
we get more information per example, given that
the annotator has already spent the effort reading
and understanding an example?

Previous works have relied on identifying rel-
evant parts of the input such as labeling features
(Druck et al., 2009; Raghavan et al., 2005; Liang
et al., 2009), highlighting rationale phrases in

Figure 1:
In BabbleLabble, the user provides
a natural language explanation for each label-
ing decision. These explanations are parsed into
labeling functions that convert unlabeled data into
a large labeled dataset for training a classiﬁer.

text (Zaidan and Eisner, 2008; Arora and Nyberg,
2009), or marking relevant regions in images (Ahn
et al., 2006). But there are certain types of infor-
mation which cannot be easily reduced to annotat-
ing a portion of the input, such as the absence of a
certain word, or the presence of at least two words.
In this work, we tap into the power of natural lan-
guage and allow annotators to provide supervision
to a classiﬁer via natural language explanations.

Speciﬁcally, we propose a framework in which
annotators provide a natural language explanation
for each label they assign to an example (see Fig-
ure 1). These explanations are parsed into log-
ical forms representing labeling functions (LFs),
functions that heuristically map examples to labels
(Ratner et al., 2016). The labeling functions are

Figure 2: Natural language explanations are parsed into candidate labeling functions (LFs). Many
incorrect LFs are ﬁltered out automatically by the ﬁlter bank. The remaining functions provide heuristic
labels over the unlabeled dataset, which are aggregated into one noisy label per example, yielding a large,
noisily-labeled training set for a classiﬁer.

then executed on many unlabeled examples, re-
sulting in a large, weakly-supervised training set
that is then used to train a classiﬁer.

Semantic parsing of natural language into log-
ical forms is recognized as a challenging prob-
lem and has been studied extensively (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Liang et al., 2011; Liang, 2016). One of our ma-
jor ﬁndings is that in our setting, even a simple
rule-based semantic parser sufﬁces for three rea-
sons: First, we ﬁnd that the majority of incorrect
LFs can be automatically ﬁltered out either seman-
tically (e.g., is it consistent with the associated ex-
ample?) or pragmatically (e.g., does it avoid as-
signing the same label to the entire training set?).
Second, LFs near the gold LF in the space of logi-
cal forms are often just as accurate (and sometimes
even more accurate). Third, techniques for com-
bining weak supervision sources are built to toler-
ate some noise (Alfonseca et al., 2012; Takamatsu
et al., 2012; Ratner et al., 2018). The signiﬁcance
of this is that we can deploy the same semantic
parser across tasks without task-speciﬁc training.
We show how we can tackle a real-world biomedi-
cal application with the same semantic parser used
to extract instances of spouses.

simple rule-based parser. In Section 4, we ﬁnd that
in our weak supervision framework, the rule-based
semantic parser and the perfect parser yield nearly
identical downstream performance. Second, while
they use the logical forms of explanations to pro-
duce features that are fed directly to a classiﬁer, we
use them as functions for labeling a much larger
training set. In Section 4, we show that using func-
tions yields a 9.5 F1 improvement (26% relative
improvement) over features, and that the F1 score
scales with the amount of available unlabeled data.
We validate our approach on two existing
datasets from the literature (extracting spouses
from news articles and disease-causing chemi-
cals from biomedical abstracts) and one real-
world use case with our biomedical collabora-
tors at OccamzRazor to extract protein-kinase
interactions related to Parkinson’s disease from
text. We ﬁnd empirically that users are able to
train classiﬁers with comparable F1 scores up to
100× faster when they provide natural language
explanations instead of individual labels. Our
code is available at https://github.com/
HazyResearch/babble.

2 The BabbleLabble Framework

Our work is most similar to that of Srivastava
et al. (2017), who also use natural language expla-
nations to train a classiﬁer, but with two important
differences. First, they jointly train a task-speciﬁc
semantic parser and classiﬁer, whereas we use a

The BabbleLabble framework converts natural
language explanations and unlabeled data into a
noisily-labeled training set (see Figure 2). There
are three key components: a semantic parser, a
ﬁlter bank, and a label aggregator. The semantic

Figure 3: Valid parses are found by iterating over increasingly large subspans of the input looking for
matches among the right hand sides of the rules in the grammar. Rules are either lexical (converting
tokens into symbols), unary (converting one symbol into another symbol), or compositional (combining
many symbols into a single higher-order symbol). A rule may optionally ignore unrecognized tokens in
a span (denoted here with a dashed line).

parser converts natural language explanations into
a set of logical forms representing labeling func-
tions (LFs). The ﬁlter bank removes as many in-
correct LFs as possible without requiring ground
truth labels. The remaining LFs are applied to un-
labeled examples to produce a matrix of labels.
This label matrix is passed into the label aggre-
gator, which combines these potentially conﬂict-
ing and overlapping labels into one label for each
example. The resulting labeled examples are then
used to train an arbitrary discriminative model.

2.1 Explanations

To create the input explanations, the user views a
subset S of an unlabeled dataset D (where |S| (cid:28)
|D|) and provides for each input xi ∈ S a label
yi and a natural language explanation ei, a sen-
tence explaining why the example should receive
that label. The explanation ei generally refers to
speciﬁc aspects of the example (e.g., in Figure 2,
the location of a speciﬁc string “his wife”).

2.2 Semantic Parser

The semantic parser takes a natural language ex-
planation ei and returns a set of LFs (logical forms
or labeling functions) {f1, . . . , fk} of the form
fi
: X → {−1, 0, 1} in a binary classiﬁcation
setting, with 0 representing abstention. We em-
phasize that the goal of this semantic parser is not
to generate the single correct parse, but rather to
have coverage over many potentially useful LFs.1

1Indeed, we ﬁnd empirically that an incorrect LF nearby
the correct one in the space of logical forms actually has
higher end-task accuracy 57% of the time (see Section 4.2).

We choose a simple rule-based semantic parser
that can be used without any training. Formally,
the parser uses a set of rules of the form α → β,
where α can be replaced by the token(s) in β (see
Figure 3 for example rules). To identify candidate
LFs, we recursively construct a set of valid parses
for each span of the explanation, based on the sub-
stitutions deﬁned by the grammar rules. At the
end, the parser returns all valid parses (LFs in our
case) corresponding to the entire explanation.

We also allow an arbitrary number of tokens in
a given span to be ignored when looking for a
matching rule. This improves the ability of the
parser to handle unexpected input, such as un-
known words or typos, since the portions of the
input that are parseable can still result in a valid
parse. For example, in Figure 3, the word “per-
son” is ignored.

All predicates included in our grammar (sum-
marized in Table 1) are provided to annota-
tors, with minimal examples of each in use
(Appendix A).
Importantly, all rules are do-
main independent (e.g., all three relation extrac-
tion tasks that we tested used the same grammar),
making the semantic parser easily transferrable to
new domains. Additionally, while this paper fo-
cuses on the task of relation extraction, in princi-
ple the BabbleLabble framework can be applied
to other tasks or settings by extending the grammar
with the necessary primitives (e.g., adding primi-
tives for rows and columns to enable explanations
about the alignments of words in tables). To guide
the construction of the grammar, we collected 500
explanations for the Spouse domain from workers

Predicate

Description

bool, string,
int, float, tuple,
list, set
and, or, not, any,
all, none
=, (cid:54)=, <, ≤, >, ≥
lower, upper,
capital, all caps
starts with,
ends with,
substring
person, location,
date, number,
organization
alias

count, contains,
intersection

map, filter

word distance,
character distance

left, right,
between, within

Standard primitive data types

Standard logic operators

Standard comparison operators
Return True for strings of the
corresponding case
Return True if the ﬁrst string
starts/ends with or contains the
second
Return True if a string has the
corresponding NER tag

A frequently used list of words
may be predeﬁned and referred
to with an alias
Operators for checking size,
membership, or common ele-
ments of a list/set
Apply a functional primitive to
each member of list/set to
transform or ﬁlter the elements
Return the distance between
two strings by words or charac-
ters
Return as a string the text that is
left/right/within some distance
of a string or between two des-
ignated strings

Table 1: Predicates in the grammar supported by
BabbleLabble’s rule-based semantic parser.

on Amazon Mechanical Turk and added support
for the most commonly used predicates. These
were added before the experiments described in
Section 4. The grammar contains a total of 200
rule templates.

2.3 Filter Bank

The input to the ﬁlter bank is a set of candidate
LFs produced by the semantic parser. The pur-
pose of the ﬁlter bank is to discard as many incor-
rect LFs as possible without requiring additional
labels. It consists of two classes of ﬁlters: seman-
tic and pragmatic.

Recall that each explanation ei is collected in
the context of a speciﬁc labeled example (xi, yi).
The semantic ﬁlter checks for LFs that are in-
consistent with their corresponding example; for-
mally, any LF f for which f (xi) (cid:54)= yi is discarded.
For example, in the ﬁrst explanation in Figure 2,
the word “right” can be interpreted as either “im-
mediately” (as in “right before”) or simply “to the

right.” The latter interpretation results in a func-
tion that is inconsistent with the associated exam-
ple (since “his wife” is actually to the left of person
2), so it can be safely removed.

The pragmatic ﬁlters removes LFs that are con-
stant, redundant, or correlated. For example, in
Figure 2, LF 2a is constant, as it labels every ex-
ample positively (since all examples contain two
people from the same sentence). LF 3b is redun-
dant, since even though it has a different syntax
tree from LF 3a, it labels the training set identi-
cally and therefore provides no new signal.

Finally, out of all LFs from the same explana-
tion that pass all the other ﬁlters, we keep only
the most speciﬁc (lowest coverage) LF. This pre-
vents multiple correlated LFs from a single exam-
ple from dominating.

As we show in Section 4, over three tasks, the
ﬁlter bank removes over 95% of incorrect parses,
and the incorrect ones that remain have average
end-task accuracy within 2.5 points of the corre-
sponding correct parses.

2.4 Label Aggregator

The label aggregator combines multiple (poten-
tially conﬂicting) suggested labels from the LFs
and combines them into a single probabilistic la-
if m LFs pass
bel per example. Concretely,
the ﬁlter bank and are applied to n examples,
the label aggregator implements a function f :
{−1, 0, 1}m×n → [0, 1]n.

A naive solution would be to use a simple ma-
jority vote, but this fails to account for the fact
that LFs can vary widely in accuracy and cover-
Instead, we use data programming (Ratner
age.
et al., 2016), which models the relationship be-
tween the true labels and the output of the label-
ing functions as a factor graph. More speciﬁcally,
given the true labels Y ∈ {−1, 1}n (latent) and la-
bel matrix Λ ∈ {−1, 0, 1}m×n (observed) where
Λi,j = LFi(xj), we deﬁne two types of factors
representing labeling propensity and accuracy:

i,j (Λ, Y ) = 1{Λi,j (cid:54)= 0}
φLab
i,j (Λ, Y ) = 1{Λi,j = yj}.
φAcc

(1)

(2)

Denoting the vector of factors pertaining to a given
data point xj as φj(Λ, Y ) ∈ Rm, deﬁne the model:

pw(Λ, Y ) = Z−1

w exp

w · φj(Λ, Y )

(3)

(cid:17)

,

(cid:16) n
(cid:88)

j=1

Figure 4: An example and explanation for each of the three datasets.

where w ∈ R2m is the weight vector and Zw is
the normalization constant. To learn this model
without knowing the true labels Y , we minimize
the negative log marginal likelihood given the ob-
served labels Λ:

ˆw = arg min

− log

pw(Λ, Y )

(4)

w

(cid:88)

Y

using SGD and Gibbs sampling for inference, and
then use the marginals p ˆw(Y | Λ) as probabilistic
training labels.

Intuitively, we infer accuracies of the LFs based
on the way they overlap and conﬂict with one an-
other. Since noisier LFs are more likely to have
high conﬂict rates with others, their correspond-
ing accuracy weights in w will be smaller, reduc-
ing their inﬂuence on the aggregated labels.

2.5 Discriminative Model

The noisy training set that the label aggregator
outputs is used to train an arbitrary discriminative
model. One advantage of training a discriminative
model on the task instead of using the label ag-
gregator as a classiﬁer directly is that the label ag-
gregator only takes into account those signals in-
cluded in the LFs. A discriminative model, on the
other hand, can incorporate features that were not
identiﬁed by the user but are nevertheless informa-
tive.2 Consequently, even examples for which all
LFs abstained can still be classiﬁed correctly. Ad-
ditionally, passing supervision information from
the user to the model in the form of a dataset—
rather than hard rules—promotes generalization in
the new model (rather than memorization), similar
to distant supervision (Mintz et al., 2009). On the
three tasks we evaluate, using the discriminative
model averages 4.3 F1 points higher than using the
label aggregator directly.

For the results reported in this paper, our dis-
criminative model is a simple logistic regression

Task

Train

Dev

Test % Pos.

Spouse
Disease
Protein

22195
6667
5546

2796
773
1011

2697
4101
1058

8%
23%
22%

Table 2: The total number of unlabeled train-
ing examples (a pair of annotated entities in a
sentence), labeled development examples (for hy-
perparameter tuning), labeled test examples (for
assessment), and the fraction of positive labels in
the test split.

classiﬁer with generic features deﬁned over depen-
dency paths.3 These features include unigrams,
bigrams, and trigrams of lemmas, dependency la-
bels, and part of speech tags found in the siblings,
parents, and nodes between the entities in the de-
pendency parse of the sentence. We found this to
perform better on average than a biLSTM, particu-
larly for the traditional supervision baselines with
small training set sizes; it also provided easily in-
terpretable features for analysis.

3 Experimental Setup

We evaluate the accuracy of BabbleLabble on
three relation extraction tasks, which we refer to
as Spouse, Disease, and Protein. The goal
of each task is to train a classiﬁer for predicting
whether the two entities in an example are partic-
ipating in the relationship of interest, as described
below.

3.1 Datasets

Statistics for each dataset are reported in Ta-
ble 2, with one example and one explanation for
each given in Figure 4 and additional explanations
shown in Appendix B.

In the Spouse task, annotators were shown a
sentence with two highlighted names and asked to

2We give an example of two such features in Section 4.3.

3https://github.com/HazyResearch/treedlib

# Inputs

Spouse
Disease
Protein

Average

BL

30

50.1
42.3
47.3

46.6

30

15.5
32.1
39.3

28.9

60

15.9
32.6
42.1

30.2

150

16.4
34.4
46.8

32.5

TS

300

17.2
37.5
51.0

35.2

1,000

3,000

10,000

22.8
41.9
57.6

40.8

41.8
44.5
-

43.2

55.0
-
-

55.0

Table 3: F1 scores obtained by a classiﬁer trained with BabbleLabble (BL) using 30 explanations
or with traditional supervision (TS) using the speciﬁed number of individually labeled examples.
BabbleLabble achieves the same F1 score as traditional supervision while using fewer user inputs
by a factor of over 5 (Protein) to over 100 (Spouse).

label whether the sentence suggests that the two
people are spouses. Sentences were pulled from
the Signal Media dataset of news articles (Corney
et al., 2016). Ground truth data was collected from
Amazon Mechanical Turk workers, accepting the
majority label over three annotations. The 30 ex-
planations we report on were sampled randomly
from a pool of 200 that were generated by 10 grad-
uate students unfamiliar with BabbleLabble.

In the Disease task, annotators were shown a
sentence with highlighted names of a chemical and
a disease and asked to label whether the sentence
suggests that the chemical causes the disease. Sen-
tences and ground truth labels came from a por-
tion of the 2015 BioCreative chemical-disease re-
lation dataset (Wei et al., 2015), which contains
abstracts from PubMed. Because this task re-
quires specialized domain expertise, we obtained
explanations by having someone unfamiliar with
BabbleLabble translate from Python to natural
language labeling functions from an existing pub-
lication that explored applying weak supervision
to this task (Ratner et al., 2018).

The Protein task was completed in conjunc-
tion with OccamzRazor, a neuroscience company
targeting biological pathways of Parkinson’s dis-
ease. For this task, annotators were shown a sen-
tence from the relevant biomedical literature with
highlighted names of a protein and a kinase and
asked to label whether or not the kinase inﬂu-
ences the protein in terms of a physical interac-
tion or phosphorylation. The annotators had do-
main expertise but minimal programming experi-
ence, making BabbleLabble a natural ﬁt for their
use case.

3.2 Experimental Settings

Text documents are tokenized with spaCy.4 The
semantic parser is built on top of the Python-based
implementation SippyCup.5 On a single core,
parsing 360 explanations takes approximately two
seconds. We use existing implementations of the
label aggregator, feature library, and discrimina-
tive classiﬁer described in Sections 2.4–2.5 pro-
vided by the open-source project Snorkel (Ratner
et al., 2018).

Hyperparameters for all methods we report
were selected via random search over thirty con-
ﬁgurations on the same held-out development set.
We searched over learning rate, batch size, L2 reg-
ularization, and the subsampling rate (for improv-
ing balance between classes).6 All reported F1
scores are the average value of 40 runs with ran-
dom seeds and otherwise identical settings.

4 Experimental Results

We evaluate the performance of BabbleLabble
with respect to its rate of improvement by number
of user inputs, its dependence on correctly parsed
logical forms, and the mechanism by which it uti-
lizes logical forms.

4.1 High Bandwidth Supervision

In Table 3 we report the average F1 score of a
classiﬁer trained with BabbleLabble using 30 ex-
planations or traditional supervision with the indi-
cated number of labels. On average, it took the
same amount of time to collect 30 explanations

4https://github.com/explosion/spaCy
5https://github.com/wcmac/sippycup
6Hyperparameter ranges:

learning rate (1e-2 to 1e-4),
batch size (32 to 128), L2 regularization (0 to 100), subsam-
pling rate (0 to 0.5)

Pre-ﬁlters

Discarded

Post-ﬁlters

LFs Correct

Sem. Prag.

LFs Correct

Spouse 156
Disease 102
Protein 122

10%
23%
14%

19
34
44

118
40
58

19
28
20

84%
89%
85%

Table 4: The number of LFs generated from 30
explanations (pre-ﬁlters), discarded by the ﬁlter
bank, and remaining (post-ﬁlters), along with the
percentage of LFs that were correctly parsed from
their corresponding explanations.

as 60 labels.7 We observe that in all three tasks,
BabbleLabble achieves a given F1 score with far
fewer user inputs than traditional supervision, by
as much as 100 times in the case of the Spouse
task. Because explanations are applied to many
unlabeled examples, each individual input from
the user can implicitly contribute many (noisy) la-
bels to the learning algorithm.

We also observe, however, that once the num-
ber of labeled examples is sufﬁciently large, tra-
ditional supervision once again dominates, since
ground truth labels are preferable to noisy ones
generated by labeling functions. However, in do-
mains where there is much more unlabeled data
available than labeled data (which in our experi-
ence is most domains), we can gain in supervision
efﬁciency from using BabbleLabble.

Of those explanations that did not produce a
correct LF, 4% were caused by the explanation re-
ferring to unsupported concepts (e.g., one expla-
nation referred to “the subject of the sentence,”
which our simple parser doesn’t support). An-
other 2% were caused by human errors (the cor-
rect LF for the explanation was inconsistent with
the example). The remainder were due to unrecog-
nized paraphrases (e.g., the explanation said “the
order of appearance is X, Y” instead of a sup-
ported phrasing like “X comes before Y”).

4.2 Utility of Incorrect Parses

In Table 4, we report LF summary statistics be-
fore and after ﬁltering. LF correctness is based
on exact match with a manually generated parse
for each explanation. Surprisingly, the simple
heuristic-based ﬁlter bank successfully removes
over 95% of incorrect LFs in all three tasks, re-
sulting in ﬁnal LF sets that are 86% correct on av-

7Zaidan and Eisner (2008) also found that collecting an-
notator rationales in the form of highlighted substrings from
the sentence only doubled annotation time.

Spouse
Disease
Protein

Average

BL-FB

15.7
39.8
38.2

31.2

BL

50.1
42.3
47.3

46.6

BL+PP

49.8
43.2
47.4

46.8

Table 5: F1 scores obtained using BabbleLabble
with no ﬁlter bank (BL-FB), as normal (BL), and
with a perfect parser (BL+PP) simulated by hand.

erage. Furthermore, among those LFs that pass
through the ﬁlter bank, we found that the aver-
age difference in end-task accuracy between cor-
rect and incorrect parses is less than 2.5%. Intu-
itively, the ﬁlters are effective because it is quite
difﬁcult for an LF to be parsed from the explana-
tion, label its own example correctly (passing the
semantic ﬁlter), and not label all examples in the
training set with the same label or identically to
another LF (passing the pragmatic ﬁlter).

We went one step further: using the LFs that
would be produced by a perfect semantic parser as
starting points, we searched for “nearby” LFs (LFs
differing by only one predicate) with higher end-
task accuracy on the test set and succeeded 57%
of the time (see Figure 5 for an example). In other
words, when users provide explanations, the sig-
nals they describe provide good starting points, but
they are actually unlikely to be optimal. This ob-
servation is further supported by Table 5, which
shows that the ﬁlter bank is necessary to remove
clearly irrelevant LFs, but with that in place, the
simple rule-based semantic parser and a perfect
parser have nearly identical average F1 scores.

4.3 Using LFs as Functions or Features

Once we have relevant logical forms from user-
provided explanations, we have multiple options
for how to use them. Srivastava et al. (2017) pro-
pose using these logical forms as features in a lin-
ear classiﬁer, essentially using a traditional super-
vision approach with user-speciﬁed features. We
choose instead to use them as functions for weakly
supervising the creation of a larger training set via
data programming (Ratner et al., 2016).
In Ta-
ble 6, we compare the two approaches directly,
ﬁnding that the the data programming approach
outperforms a feature-based one by 9.5 F1 points
on average with the rule-based parser, and by 4.5
points with a perfect parser.

We attribute this difference primarily to the abil-
ity of data programming to utilize a larger feature

Figure 5: Incorrect LFs often still provide useful signal. On top is an incorrect LF produced for the
Disease task that had the same accuracy as the correct LF. On bottom is a correct LF from the Spouse
task and a more accurate incorrect LF discovered by randomly perturbing one predicate at a time as
described in Section 4.2. (Person 2 is always the second person in the sentence).

BL-DM

BL BL+PP

Feat Feat+PP

Spouse
Disease
Protein

Average

46.5
39.7
40.6

42.3

50.1
42.3
47.3

46.6

49.8
43.2
47.4

46.8

33.9
40.8
36.7

37.1

39.2
43.8
44.0

42.3

Table 6: F1 scores obtained using explanations as
functions for data programming (BL) or features
(Feat), optionally with no discriminative model
(-DM) or using a perfect parser (+PP).

5 Related Work and Discussion

Our work has two themes: modeling natural lan-
guage explanations/instructions and learning from
weak supervision. The closest body of work is
on “learning from natural language.” As men-
tioned earlier, Srivastava et al. (2017) convert nat-
ural language explanations into classiﬁer features
(whereas we convert them into labeling functions).
Goldwasser and Roth (2011) convert natural lan-
guage into concepts (e.g.,
the rules of a card
game). Ling and Fidler (2017) use natural lan-
guage explanations to assist in supervising an im-
age captioning model. Weston (2016); Li et al.
(2016) learn from natural language feedback in a
dialogue. Wang et al. (2017) convert natural lan-
guage deﬁnitions to rules in a semantic parser to
build up progressively higher-level concepts.

We lean on the formalism of semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Liang, 2016). One notable trend is
to learn semantic parsers from weak supervision
(Clarke et al., 2010; Liang et al., 2011), whereas
our goal is to obtain weak supervision signal from
semantic parsers.

The broader topic of weak supervision has re-
ceived much attention; we mention some works
most related to relation extraction. In distant su-
pervision (Craven et al., 1999; Mintz et al., 2009)

Figure 6: When logical forms of natural language
explanations are used as functions for data pro-
gramming (as they are in BabbleLabble), perfor-
mance can improve with the addition of unlabeled
data, whereas using them as features does not ben-
eﬁt from unlabeled data.

set and unlabeled data. In Figure 6, we show how
the data programming approach improves with the
number of unlabeled examples, even as the num-
ber of LFs remains constant. We also observe
qualitatively that data programming exposes the
classiﬁer to additional patterns that are correlated
with our explanations but not mentioned directly.
For example, in the Disease task, two of the fea-
tures weighted most highly by the discriminative
model were the presence of the trigrams “could
produce a” or “support diagnosis of” between the
chemical and disease, despite none of these words
occurring in the explanations for that task. In Ta-
ble 6 we see a 4.3 F1 point improvement (10%)
when we use the discriminative model that can
take advantage of these features rather than apply-
ing the LFs directly to the test set and making pre-
dictions based on the label aggregator’s outputs.

and multi-instance learning (Riedel et al., 2010;
Hoffmann et al., 2011), an existing knowledge
base is used to (probabilistically) impute a train-
ing set. Various extensions have focused on aggre-
gating a variety of supervision sources by learn-
ing generative models from noisy labels (Alfon-
seca et al., 2012; Takamatsu et al., 2012; Roth and
Klakow, 2013; Ratner et al., 2016; Varma et al.,
2017).

Finally, while we have used natural language
explanations as input to train models, they can also
be output to interpret models (Krening et al., 2017;
Lei et al., 2016). More generally, from a machine
learning perspective, labels are the primary asset,
but they are a low bandwidth signal between an-
notators and the learning algorithm. Natural lan-
guage opens up a much higher-bandwidth commu-
nication channel. We have shown promising re-
sults in relation extraction (where one explanation
can be “worth” 100 labels), and it would be inter-
esting to extend our framework to other tasks and
more interactive settings.

Reproducibility

lowship under Grant No. DGE-114747, the Stan-
ford Finch Family Fellowship, the Joseph W. and
Hon Mai Goodman Stanford Graduate Fellowship,
an NSF CAREER Award IIS-1552635, and the
members of the Stanford DAWN project: Face-
book, Google, Intel, Microsoft, NEC, Teradata,
and VMware.

We thank Alex Ratner for his assistance with
data programming, Jason Fries and the many
members of the Hazy Research group and Stan-
ford NLP group who provided feedback and tested
early prototyptes, Kaya Tilev from the Stanford
Graduate School of Business for helpful discus-
sions early on, and the OccamzRazor team: Tarik
Koc, Benjamin Angulo, Katharina S. Volz, and
Charlotte Brzozowski.

The U.S. Government is authorized to repro-
duce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect
the views, policies, or endorsements, either ex-
pressed or implied, of DARPA, DOE, NIH, ONR,
AFOSR, NSF, or the U.S. Government.

The code, data, and experiments for this paper
are available on the CodaLab platform at https:
//worksheets.codalab.org/worksheets/
0x900e7e41deaa4ec5b2fe41dc50594548/.

References

Refactored code with simpliﬁed dependencies,
performance and speed improvements, and inter-
active tutorials can be found on Github:
https://github.com/HazyResearch/babble.

Acknowledgments

We gratefully acknowledge the support of the
following organizations: DARPA under No.
N66001-15-C-4043 (SIMPLEX), No. FA8750-
17-2-0095 (D3M), No.
FA8750-12-2-0335
(XDATA), and No. FA8750-13-2-0039 (DEFT),
108845, NIH under No.
DOE under No.
U54EB020405 (Mobilize), ONR under No.
N000141712266 and No.
N000141310129,
AFOSR under No. 580K753, the Intel/NSF CPS
Security grant No. 1505728, the Michael J. Fox
Foundation for Parkinsons Research under Grant
No. 14672, the Secure Internet of Things Project,
Qualcomm, Ericsson, Analog Devices, the Moore
Foundation, the Okawa Research Grant, Ameri-
can Family Insurance, Accenture, Toshiba, the Na-
tional Science Foundation Graduate Research Fel-

L. V. Ahn, R. Liu, and M. Blum. 2006. Peekaboom: a
game for locating objects in images. In Conference
on Human Factors in Computing Systems (CHI).
pages 55–64.

E. Alfonseca, K. Filippova, J. Delort, and G. Garrido.
2012. Pattern learning for relation extraction with a
hierarchical topic model. In Association for Compu-
tational Linguistics (ACL). pages 54–59.

S. Arora and E. Nyberg. 2009. Interactive annotation
learning with indirect feature voting. In Association
for Computational Linguistics (ACL). pages 55–60.

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL). pages 18–27.

D. Corney, D. Albakour, M. Martinez-Alvarez, and
S. Moussa. 2016. What do a million news articles
look like? In NewsIR@ ECIR. pages 42–47.

M. Craven, J. Kumlien, et al. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In ISMB. pages 77–86.

G. Druck, B. Settles, and A. McCallum. 2009. Active
learning by labeling features. In Empirical Methods
in Natural Language Processing (EMNLP). pages
81–90.

B. Roth and D. Klakow. 2013. Combining generative
and discriminative model scores for distant supervi-
In Empirical Methods in Natural Language
sion.
Processing (EMNLP). pages 24–29.

S. Srivastava, I. Labutov, and T. Mitchell. 2017. Joint
concept learning and semantic parsing from natu-
In Empirical Methods
ral language explanations.
in Natural Language Processing (EMNLP). pages
1528–1537.

S. Takamatsu, I. Sato, and H. Nakagawa. 2012. Reduc-
ing wrong labels in distant supervision for relation
In Association for Computational Lin-
extraction.
guistics (ACL). pages 721–729.

P. Varma, B. He, D. Iter, P. Xu, R. Yu, C. D. Sa, and
C. R’e. 2017. Socratic learning: Augmenting gener-
ative models to incorporate latent subsets in training
data. arXiv preprint arXiv:1610.08123 .

S. I. Wang, S. Ginn, P. Liang, and C. D. Manning.
2017. Naturalizing a programming language via in-
teractive learning. In Association for Computational
Linguistics (ACL).

C. Wei, Y. Peng, R. Leaman, A. P. Davis, C. J.
Mattingly, J. Li, T. C. Wiegers, and Z. Lu. 2015.
Overview of the biocreative V chemical disease re-
lation (cdr) task. In Proceedings of the ﬁfth BioCre-
ative challenge evaluation workshop. pages 154–
166.

J. E. Weston. 2016. Dialog-based language learning.
In Advances in Neural Information Processing Sys-
tems (NIPS). pages 829–837.

O. F. Zaidan and J. Eisner. 2008. Modeling annotators:
A generative approach to learning from annotator ra-
tionales. In Empirical Methods in Natural Language
Processing (EMNLP).

M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic program-
ming. In Association for the Advancement of Arti-
ﬁcial Intelligence (AAAI). pages 1050–1055.

L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classiﬁca-
tion with probabilistic categorial grammars. In Un-
certainty in Artiﬁcial Intelligence (UAI). pages 658–
666.

D. Goldwasser and D. Roth. 2011. Learning from nat-
ural instructions. In International Joint Conference
on Artiﬁcial Intelligence (IJCAI). pages 1794–1800.

R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer,
and D. S. Weld. 2011. Knowledge-based weak su-
pervision for information extraction of overlapping
In Association for Computational Lin-
relations.
guistics (ACL). pages 541–550.

S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell,
M. Riedl, and A. Thomaz. 2017. Learning from ex-
planations using sentiment and advice in RL. IEEE
Transactions on Cognitive and Developmental Sys-
tems 9(1):44–55.

T. Lei, R. Barzilay, and T. Jaakkola. 2016. Rational-
In Empirical Methods in

izing neural predictions.
Natural Language Processing (EMNLP).

J. Li, A. H. Miller, S. Chopra, M. Ranzato, and J. We-
ston. 2016. Learning through dialogue interactions.
arXiv preprint arXiv:1612.04936 .

P. Liang. 2016. Learning executable semantic parsers
for natural language understanding. Communica-
tions of the ACM 59.

P. Liang, M. I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families.
In International Conference on Machine Learning
(ICML).

P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
pages 590–599.

H. Ling and S. Fidler. 2017. Teaching machines to
describe images via natural language feedback. In
Advances in Neural Information Processing Systems
(NIPS).

M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Association for Computational Lin-
guistics (ACL). pages 1003–1011.

H. Raghavan, O. Madani, and R. Jones. 2005.

Inter-
active feature selection. In International Joint Con-
ference on Artiﬁcial Intelligence (IJCAI). volume 5,
pages 841–846.

A. J. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu,
and C. R’e. 2018. Snorkel: Rapid training data cre-
In Very Large Data
ation with weak supervision.
Bases (VLDB).

A. J. Ratner, C. M. D. Sa, S. Wu, D. Selsam, and C. R’e.
2016. Data programming: Creating large training
In Advances in Neural Information
sets, quickly.
Processing Systems (NIPS). pages 3567–3575.

S. Riedel, L. Yao, and A. McCallum. 2010. Model-
ing relations and their mentions without labeled text.
In Machine Learning and Knowledge Discovery in
Databases (ECML PKDD). pages 148–163.

A Predicate Examples

Below are the predicates in the rule-based semantic parser grammar, each of which may have many
supported paraphrases, only one of which is listed here in a minimal example.

Logic
and:
or:
not:
any:
all:
none:

X is true and Y is true

X is true or Y is true

X is not true
Any of X or Y or Z is true
All of X and Y and Z are true
None of X or Y or Z is true

Comparison
=:
(cid:54)=:
<:
≤:
>:
≥:

X is equal to Y
X is not Y
X is smaller than Y
X is no more than Y
X is larger than Y
X is at least Y

X is lowercase
X is upper case

Syntax
lower:
upper:
capital: X is capitalized
all caps: X is in all caps
starts with: X starts with "cardio"
ends with: X ends with "itis"
substring: X contains "-induced"

Named-entity Tags
person: A person is between X and Y
location: A place is within two words of X
date:
number: There are three numbers in the sentence
organization: An organization is right after X

A date is between X and Y

(X, Y) is in Z

X, Y, and Z are true

There is one word between X and Y

Lists
list:
set:
count:
contains: X is in Y
intersection: At least two of X are in Y
map:
filter: There are three capitalized words to the left of X
alias:

X is at the start of a word in Y

A spouse word is in the sentence (“spouse” is a predeﬁned list from the user)

Position
word distance: X is two words before Y
char distance: X is twenty characters after Y
left:
right:
between: X is between Y and Z
within: X is within five words of Y

X is before Y
X is after Y

B Sample Explanations

The following are a sample of the explanations provided by users for each task.

Spouse
Users referred to the ﬁrst person in the sentence as “X” and the second as “Y”.

Label true because "and" occurs between X and Y and "marriage" occurs
one word after person1.

Label true because person Y is preceded by ‘beau’.

Label false because the words "married", "spouse", "husband", and
"wife" do not occur in the sentence.

Label false because there are more than 2 people in the sentence and
"actor" or "actress" is left of person1 or person2.

Label true because the disease is immediately after the chemical and
’induc’ or ’assoc’ is in the chemical name.

Label true because a word containing ’develop’ appears somewhere
before the chemical, and the word ’following’ is between the disease
and the chemical.

Label true because "induced by", "caused by", or "due to" appears
between the chemical and the disease."

Label false because "none", "not", or "no" is within 30 characters to
the left of the disease.

Disease

Protein

Label true because "Ser" or "Tyr" are within 10 characters of the
protein.

Label true because the words "by" or "with" are between the protein
and kinase and the words "no", "not" or "none" are not in between
the protein and kinase and the total number of words between them is
smaller than 10.

Label false because the sentence contains "mRNA", "DNA", or "RNA".

Label false because there are two "," between the protein and the
kinase with less than 30 characters between them.

