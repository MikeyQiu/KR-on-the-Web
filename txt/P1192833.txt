Zero-Shot Open Entity Typing as Type-Compatible Grounding

EMNLP’18

Ben Zhou1, Daniel Khashabi2, Chen-Tse Tsai3, Dan Roth2

1University of Illinois, Urbana-Champaign,

2University of Pennsylvania,

3Bloomberg LP

xzhou45@illinois.edu, {danielkh,danroth}@cis.upenn.edu, ctsai54@bloomberg.net

9
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
1
v
8
2
2
3
0
.
7
0
9
1
:
v
i
X
r
a

Abstract

The problem of entity-typing has been stud-
ied predominantly in supervised learning fash-
ion, mostly with task-speciﬁc annotations
(for coarse types) and sometimes with dis-
tant supervision (for ﬁne types). While such
approaches have strong performance within
datasets, they often lack the ﬂexibility to trans-
fer across text genres and to generalize to new
type taxonomies. In this work we propose a
zero-shot entity typing approach that requires
no annotated data and can ﬂexibly identify
newly deﬁned types.

Given a type taxonomy deﬁned as Boolean
functions of FREEBASE “types”, we ground
a given mention to a set of type-compatible
Wikipedia entries and then infer the target
mention’s types using an inference algorithm
that makes use of the types of these entries.
We evaluate our system on a broad range of
datasets, including standard ﬁne-grained and
coarse-grained entity typing datasets, and also
a dataset in the biological domain. Our system
is shown to be competitive with state-of-the-
art supervised NER systems and outperforms
them on out-of-domain datasets. We also show
that our system signiﬁcantly outperforms other
zero-shot ﬁne typing systems.

1

Introduction

Entity type classiﬁcation is the task of connect-
ing an entity mention to a given set of seman-
tic types. The commonly used type sets range in
size and level of granularity, from a small num-
ber of coarse-grained types (Tjong Kim Sang and
De Meulder, 2003) to over a hundred ﬁne-grained
types (Ling and Weld, 2012). It is understood that
semantic typing is a key component in many natu-
ral language understanding tasks, including Ques-
tion Answering (Toral et al., 2005; Li and Roth,
2005) and Textual Entailment (Dagan et al., 2010,
2013). Consequently, the ability to type mentions

semantically across domains and text genres, and
to use a ﬂexible type hierarchy, is essential for
solving many important challenges.

used

semantic

commonly
for

Nevertheless, most
and systems

ap-
typing
proaches
(e.g., CORENLP (Manning et al., 2014), COG-
COMPNLP (Khashabi et al., 2018), NLTK (Loper
and Bird, 2002), SPACY) are trained in a super-
vised fashion and rely on high quality,
task-
speciﬁc annotation. Scaling such systems to other
domains and to a larger set of entity types faces
fundamental restrictions.

Coarse typing systems, which are mostly fully
supervised, are known to ﬁt a single dataset very
well. However, their performance drops signiﬁ-
cantly on different text genres and even new data
sets. Moreover, adding a new coarse type re-
quires manual annotation and retraining. For ﬁne-
typing systems, people have adopted a distant-
supervision approach. Nevertheless, the number
of types used is small:
the distantly-supervised
FIGER dataset covers only 113 types, a small
fraction of most-conservative estimates of the
number of types in the English language (the
FREEBASE (Bollacker et al., 2008) and WORD-
NET (Miller, 1995) hierarchies consist of more
than 1k and 1.5k unique types, respectively).
More importantly, adapting these systems, once
trained, to new type taxonomies cannot be done
ﬂexibly.

As was argued in Roth (2017), there is a need to
develop new training paradigms that support scal-
able semantic processing; speciﬁcally, there is a
need to scale semantic typing to ﬂexible type tax-
onomies and to multiple domains.

In this work, we introduce ZOE, a zero-shot
entity typing system, with open type deﬁnitions.
Given a mention in a sentence and a taxonomy of
entity types with their deﬁnitions, ZOE identiﬁes
a set of types that are appropriate for the mention

Figure 1: ZOE maps a given mention to its type-compatible entities in Wikipedia and infers a collection of
types using this set of entities. While the mention “Oarnniwsf,” a football player in the U. of Washington,
does not exist in Wikipedia, we ground it to other entities with approximately the same types (§3).

in this context. ZOE does not require any training,
and it makes use of existing data resources (e.g.,
Wikipedia) and tools developed without any task-
speciﬁc annotation. The key idea is to ground each
mention to a set of type-compatible Wikipedia en-
tities. The beneﬁt of using a set of Wikipedia titles
as an intermediate representation for a mention is
that there is much human-curated information in
Wikipedia – categories associated with each page,
FREEBASE types, and DBpedia types. These were
put there independently of the task at hand and can
be harnessed for many tasks: in particular, for de-
termining the semantic types of a given mention
In this grounding step, the guid-
in its context.
ing principle is that type-compatible entities often
appear in similar contexts. We rely on contextual
signals and, when available, surface forms, to rank
Wikipedia titles and choose those that are more
compatible with a given mention.

Importantly, our algorithm does not require a
given mention to be in Wikipedia; in fact, in many
cases (such as nominal mentions) the mentions are
not available in Wikipedia. We hypothesize that
any entity possible in English corresponds to some
type-compatible entities in Wikipedia. We can
then rely mostly on the context to reveal a set of
compatible titles, those that are likely to share se-
mantic types with the target mention. The fact that
our system is not required to ground to the exact
concept is a key difference between our ground-
ing and “standard” Wikiﬁcation approaches (Mi-
halcea and Csomai, 2007; Ratinov et al., 2011).
As a consequence, while entity linking approaches
rely heavily on priors associated with the surface
forms and do not consider those that do not link to
Wikipedia titles, our system mostly relies on con-
text, regardless of whether the grounding actually
exists or not.

Figure 1 shows a high-level visualization of our
system. Given a mention, our system grounds it
into type-compatible entries in Wikipedia. The

target mention “Oarnniwsf,” is not in Wikipedia,
yet it is grounded to entities with approximately
In addition, while some of the
correct types.
grounded Wikipedia entries are inaccurate in
terms of entity types, the resulting aggregated de-
cision is correct.

ZOE is an open type system, since it is not re-
stricted to a closed set of types.
In our experi-
ments, we build on FREEBASE types as primitive
types and use them to deﬁne types across seven
different datasets. Note, however, that our ap-
proach is not fundamentally restricted to FREE-
BASE types; in particular, we allow types to be
deﬁned as Boolean formulae over these primitives
(considering a type to be a set of entities). Further-
more, we support other primitives, e.g., DBPedia
or Wikipedia entries. Consequently, our system
can be used across type taxonomies; there is no
need to restrict to previously observed types or re-
train with annotations of new types. If one wants
to use types that are outside our current vocabu-
lary, one only needs to deﬁne the target type tax-
onomy in terms of the primitives used in this work.
In summary, our contributions are as follows:
• We propose a zero-shot open entity typing
framework1 that does not require training on
entity-typing-speciﬁc supervised data.

• The proposed system outperforms existing

zero-shot entity typing systems.

• Our

system is competitive with fully-
supervised systems in their respective do-
mains across a broad range of coarse- and
ﬁne-grained typing datasets, and it outper-
forms these systems in out-of-domain set-
tings.

2 Related Work

Named Entity Recognition (NER), for which the
goal is to discover mention-boundaries in addi-
tion to typing, often using a small set of mutu-

1https://github.com/CogComp/zoe

ally exclusive types, has a considerable amount of
work (Grishman and Sundheim, 1996; Mikheev
et al., 1999; Tjong Kim Sang and De Meulder,
2003; Florian et al., 2003; Ratinov and Roth,
2009).

There have been many proposals to scale the
systems to support a bigger type space (Fleis-
chman and Hovy, 2002; Sekine et al., 2002).
This direction was followed by the introduction
of datasets with large label-sets, either manually
annotated like BBN (Weischedel and Brunstein,
2005) or distantly supervised like FIGER (Ling
and Weld, 2012). With larger datasets avail-
able, supervised-learning systems were proposed
to learn from the data (Yosef et al., 2012; Ab-
hishek et al., 2017; Shimaoka et al., 2017; Xu and
Barbosa, 2018; Choi et al., 2018). Such systems
have achieved remarkable success, mostly when
restricted to their observed domain and labels.

There is a handful of works aiming to pave
the road towards zero-shot typing by address-
ing ways to extract cheap signals, often to help
the supervised algorithms: e.g., by generating
gazetteers (Nadeau et al., 2006), or using the an-
chor texts in Wikipedia (Nothman et al., 2008,
2009). Ren et al. (2016) project labels in high-
dimensional space and use label correlations to
suppress noise and better model their relations. In
our work, we choose not to use the supervised-
learning paradigm and instead merely rely on a
general entity linking corpus and the signals in
Wikipedia. Prior work has already shown the im-
portance of Wikipedia information for NER. Tsai
et al. (2016a) use a cross-lingual WIKIFIER to fa-
cilitate cross-lingual NER. However, they do not
explicitly address the case where the target entity
does not exist in Wikipedia.

The zero-shot paradigm for entity typing has
only recently been studied. Yogatama et al. (2015)
proposed an embedding representation for user-
deﬁned features and labels, which facilitates in-
formation sharing among labels and reduces the
dependence on the labels observed in the train-
ing set. The work of Yuan and Downey (2018)
can also be seen in the same spirit, i.e., systems
that rely on a form of representation of the labels.
In a broader sense, such works–including ours–
are part of a more general line of work on zero-
shot learning (Chang et al., 2008; Palatucci et al.,
2009; Norouzi et al., 2013; Romera-Paredes and
Torr, 2015; Song and Roth, 2014). Our work can

Approach

Zero-shot?

Use labeled
data?

ATTENTIVE
(Shimaoka et al., 2017)

AAA
(Abhishek et al., 2017)

NFETC-HIER(R)
(Xu and Barbosa, 2018)

AFET
(Ren et al., 2016)

PROTOLE
(Ma et al., 2016)

(Huang et al., 2016)

ZOE (ours)

No

No

No

No

Yes
Prototype Embedding

Yes
Concept-embedding
Clustering

Yes
Type-Compatible
Concepts

Yes

Yes

Yes

Yes
(partial)

Yes
(partial)

Yes
(partial)

No

No

OTYPER
(Yuan and Downey, 2018)

Yes
Word Embedding

Table 1: Comparison of recent work on entity typ-
ing. Our system does not require any labeled
data for entity typing; therefore it works on new
datasets without retraining.

be thought of as the continuation of the same re-
search direction.

A critical step in the design of zero-shot sys-
tems is the characterization of the output space.
For supervised systems, the output representations
are trivial, as they are just indices. For zero-shot
systems, the output space is often represented in
a high-dimensional space that encodes the seman-
tics of the labels. In OTYPER (Yuan and Downey,
2018), each type embedding is computed by av-
eraging the word embeddings of the words com-
prising the type. The same idea is also used in
PROTOLE (Ma et al., 2016), except that averag-
ing is done only for a few prototypical instances
In our work, we choose to deﬁne
of each type.
types using information in Wikipedia. This ﬂex-
ibility allows our system to perform well across
several datasets without retraining. On a concep-
tual level, the work of Lin et al. (2012) and Huang
et al. (2016) are close to our approach. The gov-
erning idea in these works is to cluster mentions,
followed by propagating type information from
representative mentions.

Table 1 compares our proposed system with

several recently proposed models.

3 Zero-Shot Open Entity Typing

Types are conceptual containers that bind entities
together to form a coherent group. Among the en-
tities of the same type, type-compatibility creates
a network of loosely connected entities:

Deﬁnition 1 (Weak Type Compatibility) Two
entities are type-compatible if they share at least
one type with respect to a type taxonomy and the
contexts in which they appear.

In our approach, given a mention in a sentence,
we aim to discover type-compatible entities in
Wikipedia and then infer the mention’s types us-
ing all the type-compatible entities together. The
advantage of using Wikipedia entries is that the
rich information associated with them allows us to
infer the types more easily. Note that this prob-
lem is different from the standard entity linking or
Wikiﬁcation problem in which the goal is to ﬁnd
the corresponding entity in Wikipedia. Wikipedia
does not contain all entities in the world, but an
entity is likely to have at least one type-compatible
entity in Wikipedia.

In order to ﬁnd the type-compatible entities, we
use the context of mentions as a proxy. Deﬁning it
formally:

Deﬁnition 2 (Context Consistency) A mention
m (in a context sentence s) is context-consistent
with another well-deﬁned mention m(cid:48), if m can
be replaced by m(cid:48) in the context s, and the new
sentence still makes logical sense.

Hypothesis 1 Context consistency is a strong
proxy for type compatibility.

Based on this hypothesis, given a mention m
in a sentence s, we ﬁnd other context-compatible
mentions in a Wikiﬁed corpus. Since the men-
tions in the Wikiﬁed corpus are linked to the cor-
responding Wikipedia entries, we can infer m’s
types by aggregating information associated with
these Wikipedia entries.

Figure 2 shows the high-level architecture of
our proposed system. The inputs to the system are
a mention m in a sentence s, and a type deﬁnition
T . The output of the system is a set of types
{tTarget} ⊆ T in the target taxonomy that best
represents the given mention. The type deﬁnitions
characterize the target entity-type space.
In our
experiments, we choose to use FREEBASE types
to deﬁne the types across 7 datasets; that is, T is
a mapping from the set of FREEBASE types to
the set of target types: T : {tFB} → {tTarget}.
This deﬁnition comprises many atomic deﬁ-
for example, we can deﬁne the type
nitions;
the disjunction of FREEBASE
location as
like FB.location and FB.geography:
types

Figure 2: A high-level view of our approach. The
inputs to the system are a mention m in a context
s, and type deﬁnitions T . The output
is set of
types {t} in the type deﬁnition. The ﬁgure also
highlights the input resources , as well as ofﬂine
and online processes .

The type deﬁnitions of a dataset reﬂect the un-
derstanding of a domain expert and the assump-
tions made in dataset design. Such deﬁnitions
are often much cheaper to deﬁne, than to anno-
tate full-ﬂedged supervised datasets. It is impor-
tant to emphasize that, to use our system on differ-
ent datasets, one does not need to retrain it; there
is one single system used across different datasets,
working with different type deﬁnitions.

For notational simplicity, we deﬁne a few con-
ventions for the rest of the paper. The notation
t ∈ T , simply means t is a member of the image of
the map T (i.e., t is a member of the target types).
For a ﬁxed concept c, the notation T (c) is the ap-
plication of T (.) on the FREEBASE types attached
to the concept c. For a collection of concepts C,
T (C) is deﬁned as (cid:83)
c∈C T (c). We use Tcoarse(.)
to refer to the subset of coarse types of T (.), while
Tﬁne(.) deﬁnes the ﬁne type subset.

Components in Figure 2 are described in the fol-

lowing sections.

Figure 3: Extraction of topically relevant concepts. Word-concept map is pre-computed using WIK-
ILINKS and used to retrieve the most relevant concepts for a given mention (see §3.1).

3.1

Initial Concept Candidate Generation

CESA which is passed to the next step.

Given a mention, the goal of this step is to quickly
generate a set of Wikipedia entries based on other
words in the sentence. Since there are millions
of entries in Wikipedia,
it is extremely inefﬁ-
cient to go through all entries for each mention.
We adopt ideas from explicit semantic analysis
(ESA) (Gabrilovich and Markovitch, 2007), an
approach to representing words with a vector of
Wikipedia concepts, and to providing fast retrieval
of the relevant Wikipedia concepts via inverted in-
dexing.

In our

the WIK-
construction we use
ILINKS (Singh et al., 2012) corpus, which
contains a total of 40 million mentions over 3
million concepts. Each mention in WIKILINKS
is associated with a Wikipedia concept. To char-
acterize it formally, in the WIKILINKS corpus,
for each concept c, there are example sentences
sent(c) = {si}.

Ofﬂine computation: The ﬁrst step is to con-
struct an ESA representation for each word in the
WIKILINKS corpus. We create a mapping from
each word in the corpus to the relevant concepts
associated with it. The result is a map S from to-
kens to concepts: S : w → {c, score(c|w)} (see
Figure 3), where score(c|w) denotes the associa-
tion of the word w with concept c, calculated as
the sum of the TF-IDF values of the word w in the
sentences describing c:

score(c|w) (cid:44) (cid:88)

(cid:88)

tf-idf(w, s).

s∈sent(c)

w∈s

That is, we treat each sentence as a document and
compute TF-IDF scores for the words in it.

3.2 Context-Consistent Re-Ranking

After quick retrieval of the initial concept candi-
dates, we re-rank concepts in CESA based on con-
text consistency between the input mention and
concept mentions in WIKILINKS.

For this step, assume we have a representation
that encodes the sentential information anchored
on the mention. We denote this mention-aware
context representation as SentRep(s|m). We de-
ﬁne a measure of consistency between a concept c
and a mention m in a sentence s:

Consistency(c, s, m) =

cosine(SentRep(s|m), ConceptRep(c)),
(1)
where ConceptRep(c) is representation of a con-
cept:

ConceptRep(c) (cid:44)

(cid:16)

avgs

SentRep(s|c)

(cid:12)
(cid:12)
(cid:12)s ∈ WIKILINKS, c ∈ s)

(cid:17)

,

which is the average vector of the representation
of all the sentences in WIKILINKS that describe
the given concept.

We use pre-trained ELMO (Peters et al.,
2018), a state-of-the-art contextual and mention-
aware word representation.
In order to gener-
ate SentRep(s|m), we run ELMO on sentence s,
where the tokens of the mention m are concate-
nated with “ ”, and retrieve its ELMO vector as
SentRep(s|m).

According to the consistency measure, we se-
lect the top (cid:96)ELMO concepts for each mention. We
call this set of concepts CELMO.

3.3 Surface-Based Concept Generation

Online computation: For a given mention m
and its sentence context s, we use our ofﬂine word-
concept map S to ﬁnd the concepts associated with
each word, and aggregate them to create a single
list of weighted concepts; i.e., (cid:80)
w∈s S(w). The
resulting concepts are sorted by the corresponding
weights, and the top (cid:96)ESA candidates form a set

While context often is a key signal for typing, one
should not ignore the information included in the
surface form of the mentions. If the corresponding
concept or entity exists in Wikipedia, many men-
tions can be accurately grounded with only trivial
prior probability Pr(concept|surface). The prior
distribution is pre-computed by calculating the fre-

quency of the times a certain surface string refers
to different concepts within Wikipedia.

In the test time, for a given mention, we use the
pre-computed probability distribution to obtain the
most likely concept, csurf = arg maxc Pr(c|m),
for the given mention m.

3.4 Type Inference

Our inference algorithm starts with selection of
concepts, followed by inference of coarse and ﬁne
types. Our approach is outlined in Algorithm 1
and explained below.

Concept inference. To integrate surface-based
and context-based concepts, we follow a simple
rule: if the prior probability of the surface-based
concept (csurf) has conﬁdence below a threshold
λ, we ignore it; otherwise we include it among the
concepts selected from context (CELMO), and only
choose coarse and ﬁne types from csurf.

To map the selected concepts to the target entity
types, we retrieve the FREEBASE-types of each
concept and then apply the type deﬁnition T (de-
ﬁned just before §3.1). In Algorithm 1, the set of
target types of a concept c is denoted as T (c). This
is followed by an aggregation step for selection of
a coarse type tcoarse ∈ Tcoarse(.), and ends with the
selection of a set of ﬁne types {tﬁne} ⊆ Tﬁne(.).

Coarse type inference. Our type inference al-
gorithm works in a relatively simple conﬁdence
To this end, we deﬁne
analysis procedure.
Count(t; C) to be the number of occurrences of
type t in the collection of concepts C:

Count(t; C) := |{c : c ∈ C and t ∈ T (c)}|.

In theory, for a sensible type t, the count of
context-consistent concepts that have this type
should be higher than that of the initial concept
candidates. In other words, Count(t;CELMO)/(cid:96)ELMO
>
Count(t;CESA)/(cid:96)ESA
1. We select the ﬁrst concept (in the CELMO rank-
ing) which has some coarse type that matches this
criterion. If there is no such concept, we use the
coarse types of the highest scoring concept. To se-
lect one of the coarse types of the selected concept,
we let each concept of CELMO vote based on its
consistency score. We name this voting-based pro-
cedure SelectCoarse(c), which selects one coarse
type from a given concept:

SelectCoarse(c) (cid:44)

argmax
t

(cid:88)

(cid:88)

˜c∈CELMO

t∈Tcoarse(˜c)

Consistency(˜c, s, m),

Algorithm 1: Type inference algorithm
Input mention m in sentence s, retrieved concepts

CESA, CELMO, csurf, and type deﬁnition T
Output Inferred types tcoarse and {tﬁne}.
Deﬁne, r(t, t(cid:48); C, C (cid:48)) := Count(t;C)/|C|
Count(t(cid:48);C(cid:48))/|C(cid:48)| ,
r(t; C, C (cid:48)) := r(t, t; C, C (cid:48)),
r(t, t(cid:48); C, ) := r(t, t(cid:48); C, C).

τsurf ← {t|t ∈ Tcoarse(csurf), r(t; CELMO, CESA) > 1}
if Pr(csurf|m) ≥ λ and τsurf (cid:54)= ∅ then
tcoarse ← SelectCoarse(csurf)
˜C ← {csurf} ∪ CELMO



tf ∈ Tﬁne(csurf),
compatible w/ tcoarse and,
r(tf , tcoarse; ˜C) ≥ ηs

{tﬁne} ←

tf






else

end

c ∈ CELMO, ∃t ∈ Tcoarse(c)
r(t; CELMO, CESA) > 1

(cid:27)

c

˜CELMO ←
if ˜CELMO = ø then

else

˜c ← argmaxc∈CELMO

Consistency(c, s, m)

˜c ← argmaxc∈ ˜CELMO

Consistency(c, s, m)

end
tcoarse ← SelectCoarse(˜c)

{tﬁne} ←

tf






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

tf ∈ Tﬁne(CELMO),
compatible w/ tcoarse and,
r(tf , tcoarse; CELMO) ≥ ηc






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)



(cid:26)

where consistency is deﬁned in Equation (1).

Fine type inference. With the selected coarse
type, we take only the ﬁne types that are compati-
ble with the selected coarse type (e.g., the ﬁne type
/people/athlete and the coarse type /people
are compatible).

Among the compatible ﬁne types, we further ﬁl-
ter the ones that have better support from the con-
text. Therefore, we select the ﬁne types tf such
that Count(tf ;CELMO)
Count(tc;CELMO) ≥ η, where tc is the previously
selected coarse type which is compatible with tf .
Intuitively, the fraction ﬁlters out the ﬁne-grained
candidate types that don’t have enough support
compared to the selected coarse type.

4 Experiments

Empirically, we study the behavior of our system
compared to published results. All the results are
reproduced except the ones indicated by ∗, which
are directly cited from their corresponding papers.

In our experiments, we use a wide

Datasets.
range of typing datasets:
• For coarse entity typing, we use MUC (Gr-
ishman and Sundheim, 1996), CoNLL (Tjong
Kim Sang and De Meulder, 2003),
and
OntoNotes (Hovy et al., 2006).

Approach

Trained on

F 1ma

F 1mi Acc.

F 1ma

F 1mi

Acc.

F 1ma

F 1mi

FIGER

BBN

OntoNotesﬁne

e
p
y
T
d
e
s
o
l
C

e
p
y
T
n
e
p
O

AFET* (Ren et al., 2016)
NFETC-HIER(R)*(Xu and Barbosa, 2018)

AFET* (Ren et al., 2016)
AAA* (Abhishek et al., 2017)

AFET* (Ren et al., 2016)
NFETC-HIER(R)* (Xu and Barbosa, 2018)

OTYPER

(Yuan and Downey, 2018)

ELMONN
WIKIFIERTYPER
ZOE (ours)

FIGER
FIGER

BBN
BBN

OntoNotesﬁne
OntoNotesﬁne

FIGER
BBN
OntoNotesﬁne

×
×
×

Acc.

53.3
68.9

-
-

-
-

47.2
5.3
0.4

21.5
17.2
58.8

69.3
81.9

66.4
79.0

-
-

-
-

69.1
11.5
15.6

57.7
33.3
74.8

-
-

-
-

67.2
11.5
16.8

53.8
46.2
71.3

68.3
73.3

74.4
79.1

74.7
79.2

-
-

-
-

-
-

-
-

-
-

-
-

27
29
23.6

49.3
45.8
61.8

50.3
54.4
51.1

68.4
52.3
74.6

49.5
48.8
47.9

66.2
66.1
74.9

-
-

-
-

55.1
60.2

31.6
2.5
31.8

0.5
47.8
50.7

-
-

-
-

71.1
76.4

34.5
5.1
39.1

21.2
65.6
66.9

-
-

-
-

64.7
70.2

32.1
5.4
36

21.8
58.2
60.8

Table 2: Evaluation of ﬁne-grained entity-typing: we compare our system with state-of-the-art systems (§4.1) For each col-
umn, the best zero-shot and overall results are bold-faced and underlined, respectively. Numbers are F 1 in percentage. For
supervised systems, we report their in-domain performances, since they do not transfer to other datasets with different labels.
For OTYPER, cells with gray color indicate in-domain evaluation, which is the setting in which it has the best performance.
Our system outperforms all the other zero-shot baselines, and achieves competitive results compared to the best supervised
systems.

OntoNotes

CoNLL

MUC

System

Trained on

PER

LOC ORG PER

LOC ORG PER LOC ORG

COGCOMPNLP OntoNotes
COGCOMPNLP
ZOE (ours)

CoNLL
×

98.4
94.4
88.4

91.9
59.1
70.0

97.7
87.8
85.6

83.7
95.6
90.1

70.1
92.9
80.1

68.3
90.5
73.9

82.5
90.8
87.8

76.9
90.8
90.9

86.7
90.9
91.2

Table 3: Evaluation of coarse entity-typing (§4.2): we compare two supervised entity-typers with our system. For the su-
pervised systems, cells with gray color indicate in-domain evaluation. For each column, the best, out-of-domain and overall
results are bold-faced and underlined, respectively. Numbers are F 1 in percentage. In most of the out-of-domain settings our
system outperforms the supervised system.

• For ﬁne typing, we focus on FIGER (Ling and
Weld, 2012), BBN (Weischedel and Brunstein,
2005), and OntoNotesﬁne (Gillick et al., 2014).
• In addition to the news NER, we use the BB3
dataset (Del˙eger et al., 2016), with contain men-
tions of bacteria or other notions, extracted
from sentences of scientiﬁc papers.

ZOE’s parameters. We use different type deﬁ-
nitions for each dataset. In order to design type
deﬁnitions for each dataset, we follow in the foot-
steps of Abhishek et al. (2017) and randomly sam-
ple 10% of the test set. For the experiments, we
exclude the sampled set. For completeness, we
have included the type deﬁnitions of the major ex-
periments in Appendix D.

The parameters are set universally across dif-
ferent experiments. For parameters that deter-
mine the number of extracted concepts, we use
(cid:96)ESA = 300 and (cid:96)ELMO = 20, which are based
on the upper-bound analysis in Appendix A. For
other parameters, we set λ = 0.5, ηs = 0.8 and
ηc = 0.3, based on the FIGER dev set. We em-
phasize that these parameters are universal across
our evaluations.

Evaluation metrics. Given a collection of men-
tions M , denote the set of gold types and predicted
types of a mention m ∈ M as Tg(m) and Tp(m)
respectively. We deﬁne the following metrics for
our evaluations:
• Strict Accuracy (Acc.): |{m|Tg(m)=Tp(m)}|
• Macro F1 (F 1ma): Macro Precision is deﬁned as
. With this, the deﬁni-

|Tp(m)∩Tg(m)|
1
|M |
|Tp(m)|
tions of Macro recall and F1 follow.

m∈M

(cid:80)

|M |

.

(cid:80)

(cid:80)

• Micro F1 (F 1mi): The precision is deﬁned as
, and the Micro recall and

m∈M |Tp(m)∩Tg(m)|
m∈M |Tp(m)|

F1 follow the same pattern.
In the experiment in §4.3, to evaluate systems
on unseen types we used modiﬁed versions of met-
rics. Let G(t) be the number of mentions with
gold type t, P (t) be the number of mentions pre-
dicted to have type t, C(t) be the number of men-
tions correctly predicted to have type t:
• The precision corresponding to F 1type
is de-
ma
recall follows the

G(t)

(cid:80)

t(cid:48) G(t(cid:48)) ;

ﬁned as (cid:80)
t
same pattern.

C(t)
P (t)

• The precision corresponding to F 1type
mi

is de-
t C(t)
t P (t) ; recall follows the same pattern.

ﬁned as

(cid:80)
(cid:80)

Approach

ELMONN
WIKIFIERTYPER
OTYPER (Yuan and Downey, 2018)
ZOE (ours)

F 1type
ma

63.1
53.0
50.6
71.7

F 1type
mi
53.8
43.9
23.4
71.1

Table 4: Comparing systems where no labels
(types) are seen a priori (§4.3).

System

Bacteria

not-Bacteria Overall

WIKIFIERTYPER
ELMONN
ZOE (ours)

54.8
67.6
68.1

86.2
81.2
84.2

70.5
74.4
76.2

Table 5: Results of the system classifying mentions
to “bacteria” or something else (§4.4). Numbers
are F 1 in percentage.

Baselines. To add to the best published results
on each dataset, we create two simple and effec-
tive baselines. The ﬁrst baseline, ELMONN, se-
lects the nearest neighbor types to a given men-
tion, where mentions and types are represented by
ELMO vectors. To create a representation for each
type t, we average the representation of the WIK-
ILINKS sentences that contain mentions of type t
(as explained in §3.2). Our other baseline, WIK-
IFIERTYPER, uses Wikiﬁer (Tsai et al., 2016b) to
map the mention to a Wikipedia concept, followed
by mapping to FREEBASE types, and ﬁnally pro-
jecting them to the target types, via type deﬁni-
tion function T (.). Additionally, to compare with
published zero-shot systems, we compare our sys-
tem to OTYPER, a recently published open-typing
system. Unfortunately, to the best of our knowl-
edge, the systems proposed by Ma et al.; Huang
et al. (2016; 2016) are not available online for em-
pirical comparison.

4.1 Fine-Grained Entity Typing

for

We evaluate our system for ﬁne-grained named-
Table 2 shows the evaluation
entity typing.
result
three datasets, FIGER, BBN, and
OntoNotesﬁne. We report our system’s perfor-
mance, our zero-shot baselines, and two super-
vised systems (AFET, plus the-state-of-the-art),
for each dataset. There is no easy way to trans-
fer the supervised systems across datasets, hence
no out-of-domain numbers for such systems. For
each dataset, we train OTYPER and evaluate on the
test sets of all the three datasets. In order to run
OTYPER on different datasets, we disabled orig-
inal dataset-speciﬁc entity and type features. As
a result, among the open typing systems, our sys-
In addition,
tem has signiﬁcantly better results.
our system has competitive scores compared to the
supervised systems.

4.2 Coarse Entity Typing

In Table 3 we study entity typing for the coarse
types on three datasets. We focus on three types

that are shared among the datasets: PER, LOC,
ORG. In coarse-entity typing, the best available
systems are heavily supervised. In this evaluation,
we use gold mention spans; i.e., we force the de-
coding algorithm of the supervised systems to se-
lect the best of the three classes for each gold men-
tion. As expected, the supervised systems have
strong in-domain performance. However, they
suffer a signiﬁcant drop when evaluated in a dif-
ferent domain. Our system, while not trained on
any supervised data, achieves better or comparable
performance compared to other supervised base-
lines in the out-of-domain evaluations.

4.3 Typing of Unseen Types within Domain

We compare the quality of open typing, in which
the target type(s) have not been seen before. We
compare our system to OTYPER, which relies on
supervised data to create representations for each
type; however, it is not restricted to the observed
types. We follow a similar setting to Yuan and
Downey (2018) and split the FIGER test in folds
(one fold per type) and do cross-validations. For
each fold, mentions of only one type are used
for evaluation, and the rest are used for training
OTYPER. To be able to evaluate on unseen types
(only for this experiment), we use modiﬁed met-
rics F 1type
that measure per type qual-
ity of the system (§4). In this experiment, we focus
on a within-domain setting, and show the results
of transfer across genres in the next experiments.
The results are summarized in Table 4. We ob-
serve a signiﬁcant margin between ZOE and other
systems, including OTYPER.

ma and F 1type
mi

4.4 Biology Entity Typing

We go beyond the scope of popular entity-typing
tasks, and evaluate the quality of our system on a
dataset that contains sentences from scientiﬁc pa-
pers (Del˙eger et al., 2016), which makes it differ-
ent from other entity-typing datasets. The men-
tions refer to either “bacteria”, or some miscella-
neous class (two class typing). As indicated in Ta-

FIGER

BBN

OntoNotesﬁne

Approach

ZOE (ours)

no surface-based concepts
no context-based concepts

Acc.

58.8

-8.8
-39.3

F 1ma

F 1mi

74.8

71.3

-7.5
-42.1

-9.2
-25.4

Acc.

61.8

-12.9
-36.4

F 1ma

F 1mi

74.6

74.9

-7.0
-31.0

-8.6
-13.9

Acc.

50.7

-1.8
-10.0

F 1ma

F 1mi

66.9

-1.2
-12.3

60.8

-0.1
-7.4

Table 6: Ablation study of different ways in which concepts are generated in our system (§4.5). The ﬁrst row shows perfor-
mance of our system on each dataset, followed by the change in the performance upon dropping a component. While both
signals are crucial, contextual information is playing more important role than the mention-surface signal.

ble 5, our system’s overall scores are higher than
our baselines.

cepts. This is the major reason behind 56.6%
of errors.

4.5 Ablation Study

We carry out ablation studies that quantify the con-
tribution of surface information (§3.3) and context
information (§3.2). As Table 6 shows, both fac-
tors are crucial and complementary for the sys-
tem. However, the contextual information seems
to have a bigger role overall.

We complement our qualitative analysis with
the quantitative share of each component.
In
69.3%, 54.6%, and 69.7% of mentions, our system
uses the context information (and ignores the sur-
face), in FIGER, BBN, and OntoNotesﬁne datasets,
respectively, underscoring the importance of con-
textual information.

4.6 Error Analysis

We provide insights into speciﬁc reasons for the
mistakes made by the system. For our analysis,
we use the erroneous decisions in the FIGER dev
set. Two independent annotators label the cause(s)
of the mistakes, resulting in 83% agreement be-
tween the annotators. The disagreements are later
reconciled by an adjudication step.
1. Incompatible concept, due to context informa-
tion: Ambiguous contexts, or short ones, of-
ten contribute to the inaccurate mapping to con-
cepts. In our manual annotations, 23.3% of er-
rors are caused, at least partly, by this issue.
2. Incompatible concept, due to surface informa-
tion: Although the prior probability is high, the
surface-based concept could be wrong. About
26% of the errors are partly due to the surface
signal errors.

3. Incorrect type, due to type inference: Even
when the system is able to ﬁnd several type-
compatible concepts, it can fail due to inference
errors. This could happen if the types attached
to the type-compatible concepts are not the ma-
jority among other types attached to other con-

4. Incorrect type, due to type deﬁnition: Some er-
rors are caused by the inaccurate deﬁnition of
the type mapping function T . About 23% of
the mistakes are partly caused by this issue.
Note that each mistake could be caused by mul-
tiple factors; in other words, the above categories
are not mutually disjoint events. A slightly more
detailed analysis is included in Appendix C.

5 Conclusion

Moving beyond a fully supervised paradigm and
scaling entity-typing systems to support bigger
type sets is a crucial challenge for NLP. In this
work, we have presented ZOE, a zero-shot open
entity typing framework. The signiﬁcance of this
work is threefold. First, the proposed system does
not require task-speciﬁc labeled data. Our sys-
tem relies on type deﬁnitions, which are much
cheaper to obtain than annotating thousands of ex-
amples. Second, our system outperforms exist-
ing state-of-the-art zero-shot systems by a signiﬁ-
cant margin. Third, we show that without reliance
on task-speciﬁc supervision, one can achieve rela-
tively robust transfer across datasets.

6 Acknowledgement

The authors would like to thank Jennifer Shefﬁeld,
Stephen Mayhew, and Qiang Ning for invaluable
suggestions. This work was supported by Con-
tract HR0011-15-2-0025 with the US Defense Ad-
vanced Research Projects Agency (DARPA). Ap-
proved for Public Release, Distribution Unlimited.
The views expressed are those of the authors and
do not reﬂect the ofﬁcial policy or position of the
Department of Defense or the U.S. Government.
Additionally, this research is supported by grants
from Google and the Allen Institute for Artiﬁcial
Intelligence (allenai.org).

References

Abhishek Abhishek, Ashish Anand, and Amit Awekar.
2017.
Fine-grained entity type classiﬁcation by
jointly learning representations and label embed-
dings. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, pages
797–807, Valencia, Spain. Association for Compu-
tational Linguistics.

Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD.

Ming-Wei Chang, Lev-Arie Ratinov, Dan Roth, and
Vivek Srikumar. 2008. Importance of semantic rep-
resentation: Dataless classiﬁcation. In Proceedings
of The Conference on Artiﬁcial Intelligence (AAAI).

Eunsol Choi, Omer Levy, Yejin Choi, and Luke S.
Zettlemoyer. 2018. Ultra-ﬁne entity typing. In An-
nual Meeting of the Association for Computational
Linguistics (ACL).

Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ra-
tional, evaluation and approaches–erratum. Natural
Language Engineering, 16(01):105–105.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzoto. 2013. Recognizing textual entail-
ment: Models and applications.

Louise Del˙eger, Robert Bossy, Estelle Chaix,
Mouhamadou Ba, Arnaud Ferr˙e, Philippe Bessieres,
and Claire N˙edellec. 2016. Overview of the bacteria
In Pro-
biotope task at bionlp shared task 2016.
ceedings of the 4th BioNLP Shared Task Workshop,
pages 12–22.

Michael Fleischman and Eduard H. Hovy. 2002. Fine
In COL-

grained classiﬁcation of named entities.
ING.

Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Named entity recognition
Tong Zhang. 2003.
In Proceedings of
through classiﬁer combination.
the seventh conference on Natural language learn-
ing at HLT-NAACL 2003-Volume 4, pages 168–171.
Association for Computational Linguistics.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJcAI, volume 7,
pages 1606–1611.

Daniel Gillick, Nevena Lazic, Kuzman Ganchev, Jesse
Kirchner, and David Huynh. 2014.
Context-
dependent ﬁne-grained entity type tagging. CoRR,
abs/1412.1820.

Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference-6: A brief history.

In COLING 1996 Volume 1: The 16th Interna-
tional Conference on Computational Linguistics,
volume 1.

Eduard H. Hovy, Mitchell P. Marcus, Martha Palmer,
Lance A. Ramshaw, and Ralph M. Weischedel.
In HLT-
2006. Ontonotes: The 90% solution.
NAACL.

Lifu Huang, Jonathan May, Xiaoman Pan, and Heng Ji.
2016. Building a ﬁne-grained entity typing system
overnight for a new x (x= language, domain, genre).
arXiv preprint arXiv:1603.03112.

Daniel Khashabi, Mark Sammons, Ben Zhou, Tom
Redman, Christos Christodoulopoulos, Vivek Sriku-
mar, Nicholas Rizzolo, Lev Ratinov, Guanheng Luo,
Quang Do, Chen-Tse Tsai, Subhro Roy, Stephen
Mayhew, Zhilli Feng, John Wieting, Xiaodong Yu,
Yangqiu Song, Shashank Gupta, Shyam Upadhyay,
Naveen Arivazhagan, Qiang Ning, Shaoshi Ling,
and Dan Roth. 2018. CogCompNLP: your swiss
army knife for nlp. In Proceedings of International
Conference on Language Resources and Evaluation
(LREC).

Xin Li and Dan Roth. 2005. Learning question classi-
ﬁers: The role of semantic information. Journal of
Natural Language Engineering, 11(4).

Thomas Lin, Oren Etzioni, et al. 2012. No noun phrase
left behind: detecting and typing unlinkable enti-
In Proceedings of the 2012 joint conference
ties.
on empirical methods in natural language process-
ing and computational natural language learning,
pages 893–903. Association for Computational Lin-
guistics.

Xiao Ling and Daniel S Weld. 2012. Fine-grained en-
tity recognition. In Proceedings of The Conference
on Artiﬁcial Intelligence (AAAI).

Edward Loper and Steven Bird. 2002. Nltk: The natu-
ral language toolkit. In Proceedings of the ACL-02
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Com-
putational Linguistics - Volume 1, ETMTNLP ’02,
pages 63–70, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Yukun Ma, Erik Cambria, and Sa Gao. 2016. La-
bel embedding for zero-shot ﬁne-grained named en-
In Proceedings of COLING 2016, the
tity typing.
26th International Conference on Computational
Linguistics: Technical Papers, pages 171–180.

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In ACL (System Demon-
strations), pages 55–60.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
In
linking documents to encyclopedic knowledge.
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233–242. ACM.

Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of the ninth conference on European
chapter of the Association for Computational Lin-
guistics, pages 1–8. Association for Computational
Linguistics.

George Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

David Nadeau, Peter D Turney, and Stan Matwin. 2006.
Unsupervised named-entity recognition: Generating
gazetteers and resolving ambiguity. In Conference
of the Canadian Society for Computational Studies
of Intelligence, pages 266–277. Springer.

Mohammad Norouzi, Tomas Mikolov, Samy Bengio,
Yoram Singer, Jonathon Shlens, Andrea Frome,
Gregory S. Corrado, and Jeffrey Dean. 2013. Zero-
shot learning by convex combination of semantic
embeddings. In International Conference on Learn-
ing Representations (ICLR).

Joel Nothman, James R Curran, and Tara Murphy.
2008. Transforming wikipedia into named entity
In Proceedings of the Australasian
training data.
Language Technology Association Workshop 2008,
pages 124–132.

Joel Nothman, Tara Murphy, and James R Curran.
2009. Analysing wikipedia and gold-standard cor-
In Proceedings of the 12th
pora for ner training.
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 612–620.
Association for Computational Linguistics.

Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton,
and Tom M. Mitchell. 2009. Zero-shot learning with
semantic output codes. In Advances in Neural Infor-
mation Processing Systems (NIPS).

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. Proceedings of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. of the Conference on Computational Natural
Language Learning (CoNLL).

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
In Proc. of the Annual
ambiguation to wikipedia.
Meeting of the Association for Computational Lin-
guistics (ACL).

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji,
and Jiawei Han. 2016. Afet: Automatic ﬁne-grained
entity typing by hierarchical partial-label embed-
ding. In Proceedings of the conference on empirical
methods in natural language processing (EMNLP).

Bernardino Romera-Paredes and Philip H. S. Torr.
2015. An embarrassingly simple approach to zero-
In Proceedings of the International
shot learning.
Conference on Machine Learning (ICML).

Dan Roth. 2017. Incidental supervision: Moving be-
In Proc. of the Confer-

yond supervised learning.
ence on Artiﬁcial Intelligence (AAAI).

Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
In Pro-
2002. Extended named entity hierarchy.
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC).

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2017. Neural architectures for
In Proceed-
ﬁne-grained entity type classiﬁcation.
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics: Volume 1, Long Papers, volume 1, pages 1271–
1280.

Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2012. Wikilinks:
A large-scale cross-document coreference corpus la-
beled via links to wikipedia. University of Mas-
sachusetts, Amherst, Tech. Rep. UM-CS-2012, 15.

Yangqiu Song and Dan Roth. 2014. On dataless hier-
In Proceedings of The

archical text classiﬁcation.
Conference on Artiﬁcial Intelligence (AAAI).

Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147. Association for Computational Lin-
guistics.

Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Munoz. 2005.
Improving question answer-
ing using named entity recognition. In International
Conference on Application of Natural Language to
Information Systems, pages 181–191. Springer.

Chen-Tse Tsai, Stephen Mayhew, and Dan Roth.
2016a. Cross-lingual named entity recognition via
wikiﬁcation. In Proc. of the Conference on Compu-
tational Natural Language Learning (CoNLL).

Chen-Tse Tsai, Stephen Mayhew, and Dan Roth.
2016b. Cross-lingual named entity recognition via
In Proceedings of The 20th SIGNLL
wikiﬁcation.
Conference on Computational Natural Language
Learning, pages 219–228.

Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Linguistic
Data Consortium, Philadelphia, 112.

Peng Xu and Denilson Barbosa. 2018. Neural ﬁne-
grained entity type classiﬁcation with hierarchy-
aware loss. In Proceedings of the 16th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, page 9pp.

Dani Yogatama, Daniel Gillick, and Nevena Lazic.
2015. Embedding methods for ﬁne grained entity
type classiﬁcation. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), volume 2, pages 291–296.

Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2012.
Hyena: Hierarchical type classiﬁcation for entity
names. Proceedings of COLING 2012: Posters,
pages 1361–1370.

Zheng Yuan and Doug Downey. 2018. Otyper: A neu-
In
ral architecture for open named entity typing.
Proceedings of The Conference on Artiﬁcial Intel-
ligence (AAAI).

Supplemental Material for:
“Zero-Shot Open Entity Typing as
Type-Compatible Grounding”

A Coverage Analysis for Concept

Selection

To better understand the behavior of our concept
retrieval, we perform an upper-bound analysis.
Assuming that our type inference given concepts
is perfect (an oracle inference), we want to esti-
mate the best achievable score (upper-bound anal-
ysis). We perform this analysis in the output of
two stages in the system,
1. Step 1: After initial concepts extraction (§3.1).
2. Step 2: After ELMO reranking (§3.2).

Figure 4 shows a summary of the upper-bound
curves for the output of the Step 1 and the Step
2. From these analysis we set the parameters
(cid:96)ESA and (cid:96)ELMO. From the blue curve it is ev-
ident that after about 200 concepts, there is al-
most high-coverage. With this, we set the pa-
rameter (cid:96)ESA = 300. Furthermore, the red curve
shows the strong coverage, even with a few dozen
candidates.
In our experiments, we choose top
(cid:96)ELMO = 20 concepts in the output of Step 2.

Figure 4: Upper-bound analysis: The y-axis shows
the percentage of the instances with their types
covered by the retrieved concepts. The x-axis
shows the number of concepts retrieved, with a)
Step 1 (dashed), b) Step 2 (solid).

B Output for Each Step: An Example

To give a better intuition about every single step in
our algorithm, we plot the outputs of each step in
our construction in Figure 5. The details of how
each step is done is included in §3.

C Error Analysis

We expand a little bit on the errors analysis intro-
duced in §4.6. The ﬁrst two categories consist of
the scenarios when context and mention-surface
information result in a type-compatible concept
(hence incorrect type). The last two categories
are due to inaccurate mapping of (mostly type-
compatible) concepts to a collection of types.

1. Inconsistent concept, due to context informa-
tion: A short and ambiguous context could
result in noisy concepts. In the following ex-
ample, the majority of the selected concepts
based on context are of type /politician,
however the correct label is /event:
[The Fellows Forum], concerned in part with the in-

duction of newly elected fellows, is just one event of the

associations annual meeting.

2. Inconsistent concept, due to surface infor-
mation: When a mention used in a sense
other than its most-popular sense, it could re-
sult in mistakes. In the following example,
while from the context it is clear that “Utah”
is a sports team, the surface-string has a
stronger association with Colorado, which
incorrectly results in the type /location:
The biggest cause for concern for McGuff is the bruised

hamstring Regina Rogers suffered against [Utah] last

Saturday.

type,

3. Incorrect type, due to type inference: even
when the system is able to ﬁnd type-
it still fails to infer
compatible concepts,
the correct
if the types attached to
the type-compatible are the majority among
other types. In the following example, while
there are concepts of type /person, the over-
all decision is incorrectly dominated by other
concepts of type /location:
After years of wanting to curate such an exhibition,

Wieczorek has collaborated with the Henry Art Gallery

to feature 26 pieces of [Balth]’s “Videowatercolors.”
The following is another example where
the system fails to ﬁnd the correct ﬁne-
type /person/musician, since the overall
decision is dominated by other concepts
with type /person, but not a musician (e.g.
Blair Waldorf).
“That’s what I love about [Balth]’s art.”

4. Incorrect type, due to type deﬁnition: Some
errors are caused by inaccurate deﬁnition for

Figure 5: The output of each step in our system.

the type mapping function T .
the follow-
ing example, the mention gets mapped to an
approximately correct concept infantry ,
but the system fails to map it to the correct
type due to the limitations of the type deﬁni-
tions.
“When he left the [Army], Spencer got a job in Boze-

man, where he used acupuncture to save a dog that

couldn’t walk anymore.”

D Type Deﬁnitions

We include the type deﬁnitions used in the exper-
iments here for completeness. The types on the
left are for the target dataset, and the types on the
right, are the types from FREEBASE, combined
with logical operators AND (&&), OR (||), NOT
(!), and any-type wildcard character (*).

For the type deﬁnitions of the FIGER dataset,
we use the rules provided by (Ling and Weld,
2012), in addition to a few more deﬁnitions (List-
ing 1).

Listing 1: Additional deﬁnition used for FIGER
beyond given mapping
/ORGANIZATION := /organization/organization
/LOCATION := /LOCATION || /BUILDING || /

transportation/road

/ORGANIZATION/COMPANY := (/ORGANIZATION/

COMPANY || /NEWS AGENCY) && !(/
ORGANIZATION/EDUCATIONAL INSTITUTION
|| /ORGANIZATION/SPORTS LEAGUE)
/WRITTEN WORK := /WRITTEN WORK && !/

NEWS AGENCY

/ART := /ART || /WRITTEN WORK

Listing 2: Type deﬁnition used of the BBN set.

/ORGANIZATION := /ORGANIZATION/

ORGANIZATION

/PERSON := /PEOPLE/PERSON
/PLANT := /BASE/PLANTS/PLANT
/BUILDING := /ARCHITECTURE/BUILDING
/DISEASE := /MEDICINE/DISEASE
/LANGUAGE := /LANGUAGE/HUMAN LANGUAGE
/LAW := /LAW && !/ORGANIZATION
/ANIMAL := /BIOLOGY/ANIMAL
/GPE/CITY := /LOCATION/CITYTOWN
/GPE/COUNTRY := /LOCATION/COUNTRY
/GPE/STATE PROVINCE := /LOCATION/

CN PROVINCE || /BASE/LOCATIONS/
STATES AND PROVENCES
/LOCATION := /LOCATION/LOCATION
/LOCATION/CONTINENT := /LOCATION/CONTINENT

|| /BASE/LOCATIONS/CONTINENTS
/LOCATION/RIVER := /GEOGRAPHY/RIVER
/LOCATION/LAKE SEA OCEAN := /GEOGRAPHY/
BODY OF WATER && !/LOCATION/RIVER

/LOCATION/REGION := /LOCATION/

STATISTICAL REGION

/FAC/AIRPORT := /AVIATION/AIRPORT
/FAC/HIGHWAY STREET := /TRANSPORTATION/

ROAD

/FAC/BRIDGE := /TRANSPORTATION/BRIDGE
/GAME := /CVG/COMPUTER VIDEOGAME
/PRODUCT/VEHICLE := /AUTOMOTIVE/MODEL || /

AVIATION/AIRCRAFT MODEL

/PRODUCT/WEAPON := /LAW/INVENTION
/WORK OF ART/BOOK := /BOOK/WRITTEN WORK
/WORK OF ART/SONG := /MUSIC/COMPOSITION
/WORK OF ART/PAINTING := /VISUAL ART/

ARTWORK

/WORK OF ART/PLAY := /THEATER/PLAY
/EVENT := /TIME/EVENT
/EVENT/WAR := /MILITARY/WAR

/EVENT/HURRICANE := /METEOROLOGY/

/ORGANIZATION/COMPANY/NEWS := /BASE/

TROPICAL CYCLONE

/SUBSTANCE/FOOD := /FOOD/FOOD
/SUBSTANCE/DRUG := /MEDICINE/DRUG
/SUBSTANCE/CHEMICAL := /CHEMISTRY/

CHEMICAL COMPOUND

/ORGANIZATION/HOTEL := /TRAVEL/HOTEL
/ORGANIZATION/HOSPITAL := /MEDICINE/

HOSPITAL

/ORGANIZATION/CORPORATION := /BUSINESS/

EMPLOYER && !/ORGANIZATION/
GOVERNMENT

/ORGANIZATION/POLITICAL := /GOVERNMENT/

POLITICAL PARTY

/ORGANIZATION/RELIGIOUS := /RELIGION/

RELIGION && !/LOCATION/CONTINENT
/ORGANIZATION/EDUCATIONAL := /EDUCATION/
ACADEMIC INSTITUTION || /EDUCATION/
EDUCATIONAL INSTITUTION

/ORGANIZATION/GOVERNMENT := /GOVERNMENT/
GOVERNMENT AGENCY || /GOVERNMENT/
GOVERNMENTAL BODY || /GOVERNMENT/
GOVERNMENT

the

Type deﬁnition used of

Listing 3:
OntoNotesﬁne set.
/PERSON := /PEOPLE/PERSON
/PERSON/ARTIST/AUTHOR := /BOOK/AUTHOR
/PERSON/ARTIST/ACTOR := /FILM/ACTOR
/PERSON/ARTIST/MUSIC := /MUSIC/ARTIST
/PERSON/ATHLETE := /SPORTS/PRO ATHLETE
/PERSON/DOCTOR /MEDICINE/PHYSICIAN
/PERSON/POLITICAL FIGURE := /GOVERNMENT/

POLITICIAN

/PERSON/LEGAL := /BASE/CRIME/

CRIMINAL DEFENCE ATTORNEY || /BASE/
CRIME/LAWYER TYPE || /LAW/JUDGE
/PERSON/TITLE := /FICTIONAL UNIVERSE/

FICTIONAL JOB TITLE || /BUSINESS/JOB TITLE
|| /GOVERNMENT/
GOVERNMENT OFFICE OR TITLE || /
GOVERNMENT/
GOVERNMENT OFFICE CATEGORY

/LOCATION/STRUCTURE/AIRPORT := /AVIATION/

/LOCATION/STRUCTURE := /ARCHITECTURE/

AIRPORT

BUILDING

/LOCATION/STRUCTURE/HOTEL := /TRAVEL/HOTEL
/LOCATION/STRUCTURE/SPORTS FACILITY := /

SPORTS/SPORTS FACILITY

/LOCATION/GEOGRAPHY/BODY OF WATER := /

GEOGRAPHY/BODY OF WATER
/LOCATION/GEOGRAPHY/MOUNTAIN := /

GEOGRAPHY/MOUNTAIN

/LOCATION/GEOGRAPHY := /GEOGRAPHY/∗
/LOCATION/TRANSIT/BRIDGE := /TRANSPORTATION

NEWSEVENTS/
NEWS REPORTING ORGANISATION || /BOOK/
PUBLISHING COMPANY

/ORGANIZATION/COMPANY/BROADCAST := /

BROADCAST/PRODUCER

/ORGANIZATION/COMPANY := /BUSINESS/

EMPLOYER

/ORGANIZATION/EDUCATION := /EDUCATION/
ACADEMIC INSTITUTION || /EDUCATION/
EDUCATIONAL INSTITUTION

/ORGANIZATION/GOVERNMENT := /GOVERNMENT/
GOVERNMENT AGENCY || /GOVERNMENT/
GOVERNMENT || /GOVERNMENT/
GOVERNMENTAL BODY

/ORGANIZATION/MILITARY := /MILITARY/

MILITARY UNIT

/ORGANIZATION/POLITICAL PARTY := /
GOVERNMENT/POLITICAL PARTY
/ORGANIZATION/SPORTS TEAM := /SPORTS/

/ORGANIZATION/STOCK EXCHANGE := /FINANCE/

SPORTS TEAM

STOCK EXCHANGE

/OTHER/ART/BROADCAST := /TV/TV PROGRAM
/OTHER/ART/FILM := /FILM/FILM
/OTHER/ART/MUSIC := /MUSIC/ALBUM || /MUSIC/

/OTHER/ART/STAGE := /THEATER/PLAY || /OPERA/

COMPOSITION

OPERA

/OTHER/ART/WRITING := /BOOK/WRITTEN WORK ||
/BOOK/SHORT STORY || /BOOK/POEM || /BOOK/
LITERARY SERIES || /BOOK/PUBLICATION

/OTHER/EVENT := /TIME/EVENT
/OTHER/EVENT/HOLIDAY := /TIME/HOLIDAY
/OTHER/EVENT/VIOLENT CONFLICT := /MILITARY/

MILITARY CONFLICT

/OTHER/HEALTH/TREATMENT := /MEDICINE/

MEDICAL TREATMENT

/OTHER/AWARD := /AWARD/AWARD
/OTHER/BODY PART := /MEDICINE/
ANATOMICAL STRUCTURE

/OTHER/CURRENCY := /FINANCE/CURRENCY
/OTHER/LIVING THING/ANIMAL := /BIOLOGY/

ANIMAL

/OTHER/LIVING THING := /BASE/PLANTS/PLANT
/OTHER/PRODUCT/WEAPON := /LAW/INVENTION
/OTHER/PRODUCT/VEHICLE := /AUTOMOTIVE/

MODEL || /AVIATION/AIRCRAFT MODEL
/OTHER/PRODUCT/COMPUTER := /COMPUTER/∗
/OTHER/PRODUCT/SOFTWARE := /COMPUTER/

SOFTWARE

/OTHER/FOOD := /FOOD/FOOD
/OTHER/RELIGION := /RELIGION/RELIGION
/OTHER/HERITAGE := /PEOPLE/ETHNICITY
/OTHER/LEGAL := /USER/SPROCKETONLINE/
ECONOMICS/LEGISLATION || /USER/
TSEGARAN/LEGAL/ACT OF CONGRESS || /
USER/SKUD/LEGAL/TREATY || /LAW/
CONSTITUTIONAL AMENDMENT

/OTHER := !ALL TYPES EXLUCDING OTHER∗ || /

/LOCATION/TRANSIT/RAILWAY := /RAIL/RAILWAY
/LOCATION/TRANSIT/ROAD := /TRANSPORTATION/

OTHER∗

/BRIDGE

ROAD

/LOCATION/CITY := /LOCATION/CITYTOWN
/LOCATION/COUNTRY := /LOCATION/COUNTRY
/LOCATION/PARK := /AMUSEMENT PARKS/PARK || /
BASE/USNATIONALPARKS/US NATIONAL PARK

/LOCATION := /LOCATION/LOCATION
/ORGANIZATION := /ORGANIZATION/

ORGANIZATION TYPE || /ORGANIZATION/
ORGANIZATION

Listing 4: Type deﬁnition used of the OntoNotes
set.
/PERSON := /PEOPLE/∗ && /MUSIC/ARTIST
/LOC := /LOCATION/LOCATION
/ORG := /ORGANIZATION/∗ || /GOVERNMENT/

GOVERNMENT BODY || /BUSINESS/EMPLOYER

|| /BOOK/NEWSPAPER || /RELIGION/RELIGION ||
/MILITARY/MILITARY COMBATANT

Listing 5: Type deﬁnition used of the MUC set.
/PER := /PEOPLE/∗ && /MUSIC/ARTIST
/LOC := /LOCATION/LOCATION
/ORG := /ORGANIZATION/∗ || /GOVERNMENT/

GOVERNMENT BODY || /BOOK/NEWSPAPER || /
RELIGION/RELIGION

Listing 6: Type deﬁnition used of the CoNLL set.
/PER := /PEOPLE/PERSON
/LOC := /LOCATION/LOCATION
/ORG := /ORGANIZATION/ORGANIZATION

Listing 7: Type deﬁnition used of the BB3 set.

/BACTERIA := /∗/MICROORGANISM/∗

