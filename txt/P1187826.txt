Exploring Domain Shift in Extractive Text Summarization

Danqing Wang∗, Pengfei Liu∗, Ming Zhong, Jie Fu†, Xipeng Qiu†, Xuanjing Huang
School of Computer Science, Fudan University
& ‡Mila and Polytechnique Montreal
{dqwang18,pfliu14,mzhong18,xpqiu,xjhuang}@fudan.edu.cn
jie.fu@polymtl.ca

9
1
0
2
 
g
u
A
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
4
6
6
1
1
.
8
0
9
1
:
v
i
X
r
a

Abstract

Although domain shift has been well explored
in many NLP applications, it still has received
little attention in the domain of extractive text
summarization. As a result, the model is under-
utilizing the nature of the training data due
to ignoring the difference in the distribution
of training sets and shows poor generalization
on the unseen domain. With the above limi-
tation in mind, in this paper, we ﬁrst extend
the conventional deﬁnition of the domain from
categories into data sources for the text sum-
marization task. Then we re-purpose a multi-
domain summarization dataset and verify how
the gap between different domains inﬂuences
the performance of neural summarization mod-
els. Furthermore, we investigate four learning
strategies and examine their abilities to deal
with the domain shift problem. Experimental
results on three different settings show their
different characteristics in our new testbed.

Our source code including BERT-based, meta-
learning methods for multi-domain summa-
rization learning and the re-purposed dataset
MULTI-SUM will be available on our project:
http://pfliu.com/TransferSum/.

1

Introduction

Text summarization has been an important research
topic due to its widespread applications. Exist-
ing research works for summarization mainly re-
volve around the exploration of neural architectures
(Cheng and Lapata, 2016; Nallapati et al., 2017)
and design of training constraints (Paulus et al.,
2017; Wu and Hu, 2018). Apart from these, sev-
eral works try to integrate document characteristics
(e.g. domain) to enhance the model performance
(Haghighi and Vanderwende, 2009; Cheung and
Penn, 2013a; Cao et al., 2017; Isonuma et al., 2017;

∗ These two authors contributed equally.
† Corresponding author.

Wang et al., 2018; Narayan et al., 2018a) or make
interpretable analysis towards existing neural sum-
marization models (Zhong et al., 2019).

Despite their success, only a few literature (Che-
ung and Penn, 2013b; Hua and Wang, 2017) probes
into the exact inﬂuence domain can bring, while
none of them investigates the problem of domain
shift, which has been well explored in many other
NLP tasks. This absence poses some challenges for
current neural summarization models: 1) How will
the domain shift exactly affect the performance of
existing neural architectures? 2) How to take better
advantage of the domain information to improve
the performance for current models? 3) Whenever
a new model is built which can perform well on its
test set, it should also be employed to unseen do-
mains to make sure that it learns something useful
for summarization, instead of overﬁtting its source
domains.

The most important reason for the lack of ap-
proaches that deal with domain shift might lay in
the unawareness of different domain deﬁnitions in
text summarization. Most literature limits the con-
cept of the domain into the document categories or
latent topics and uses it as the extra loss (Cao et al.,
2017; Isonuma et al., 2017) or feature embeddings
(Wang et al., 2018; Narayan et al., 2018a). This
deﬁnition presumes that category information will
affect how summaries should be formulated. How-
ever, such information may not always be obtained
easily and accurately. Among the most popular ﬁve
summarization datasets, only two of them have this
information and only one can be used for training.1
Besides, the semantic categories do not have a clear

1 The ﬁve datasets are DUC, Gigward(Napoles et al.,
2012), CNN/Daily Mail(Hermann et al., 2015), The New York
Times Annotated Corpus (NYT)(Sandhaus, 2008) and News-
room(Grusky et al., 2018). Only DUC and NYT are annotated
with document categories, and DUC is designed only for com-
petition test.

deﬁnition.2 Both of these prevent previous work
from the full use of domains in existing datasets or
building a new multi-domain dataset that not only
can be used for multi-domain learning but also is
easy to explore domain connection across datasets.
In this paper, we focus on the extractive summa-
rization and demonstrate that news publications can
cause data distribution differences, which means
that they can also be deﬁned as domains. Based
on this, we re-purpose a multi-domain summariza-
tion dataset MULTI-SUM and further explore the
issue of domain shift.

Methodologically, we employ four types of mod-
els with their characteristics under different set-
tings. The ﬁrst model is inspired by the joint train-
ing strategy, and the second one builds the connec-
tion between large-scale pre-trained models and
multi-domain learning. The third model directly
constructs a domain-aware model by introducing
domain type information explicitly. Lastly, we addi-
tionally explore the effectiveness of meta-learning
methods to get better generalization. By analyz-
ing their performance under IN-DOMAIN, OUT-OF-
DOMAIN, and CROSS-DATASET, we provide a pre-
liminary guideline in Section 5.2 for future research
in multi-domain learning of summarization tasks.
Our contributions can be summarized as follows:

• We analyze the limitation of the current do-
main deﬁnition in summarization tasks and
extend it into article publications. We then re-
purpose a dataset MULTI-SUM to provide a
sufﬁcient multi-domain testbed (IN-DOMAIN
and OUT-OF-DOMAIN).

• To the best of our knowledge, this is the ﬁrst
work that introduces domain shift to text sum-
marization. We also demonstrate how domain
shift affects the current system by designing a
veriﬁcation experiment.

• Instead of pursuing a uniﬁed model, we aim
to analyze how different choices of model de-
signs inﬂuence the generalization ability of
dealing with the domain shift problem, shed-
ding light on the practical challenges and pro-
vide a set of guidelines for future researchers.

2 Domains in Text Summarization

In this section, we ﬁrst describe similar concepts
used as the domain in summarization tasks. Then

2 For example, “Dining and Wine” in NYT refers to “Food

and Drink” in DUC 2002.

we extend the deﬁnition into article sources and
verify its rationality through several indicators that
illustrate the data distribution on our re-purposed
multi-domain summarization dataset.

2.1 Common Domain Deﬁnition

Although a domain is often deﬁned by the content
category of a text (Li and Zong, 2008; Blitzer et al.,
2007) or image (Saenko et al., 2010), the initial
motivation for a domain is a metadata attribute
which is used in order to divide the data into parts
with different distributions (Joshi et al., 2012).

For text summarization, the differences between
data distribution are often attributed to the docu-
ment categories, such as sports or business, or the
latent topics within articles, which can be caught
by classical topic models like Latent Dirichlet Al-
location (LDA) (Blei et al., 2003). Although pre-
vious works have shown that taking consideration
of those distribution differences can improve sum-
marization models performance (Isonuma et al.,
2017; Wang et al., 2018), few related them with the
concept of the domain and investigated the summa-
rization tasks from a perspective of multi-domain
learning. 3

2.2 Publications as Domain

In this paper, we extend the concept into the article
sources, which can be easily obtained and clearly
deﬁned4.

Three Measures We assume that the publica-
tions of news may also affect data distribution and
thus inﬂuence the summarization styles. In order
to verify our hypothesis, we make use of three
indicators (COVERAGE, DENSITY and COMPRES-
SION) deﬁned by Grusky et al. (2018) to measure
the overlap and compression between the (docu-
ment, summary) pair. The coverage and the density
are the word and the longest common subsequence
(LCS) overlaps, respectively. The compression is
the length ratio between the document and the sum-
mary.

Two Baselines We also calculate two strong sum-
marization baselines for each publication. The

3 Hua and Wang (2017) studied domain adaptation be-
tween news stories and opinion articles from NYT. How-
ever, their model was just trained in a single domain and
was adapted to another, which was different from our multi-
domain training and evaluation settings.

4 Most existing benchmark datasets are a mixture of multi-
ple publications with the idea of collecting a larger amount of
data, such as CNN/DailyMail, Gigward and Newsroom.

Statistics

Measures

Ext-Oralce

Train

Valid

Test

Cov.

Den.

Comp.

R-1

R-2

R-L

R-1

R-2

R-L

FN
CNN
MA
NYT
WTP

Avg

NYDN
WSJ
USAT
TG
TIME

Avg

78,760
43,466
31,896
152,959
95,379

80,492

55,653
49,967
44,919
58,057
42,200

50,159

8,423
4,563
3,414
16,488
9,939

8,565

6,057
5,449
4,628
6,376
4,761

5,454

8,392
4,619
3,316
16,620
10,072

8,604

5,904
5,462
4,781
6,273
4,702

5,424

0.90
0.85
0.84
0.85
0.76

0.84

0.93
0.80
0.78
0.80
0.75

0.81

16.18
12.46
6.66
9.19
6.04

10.11

14.57
8.45
6.35
2.75
4.87

7.40

35.58
38.28
28.93
42.30
63.52

41.72

21.25
23.64
31.17
40.35
47.67

32.81

40.30
35.56
29.38
28.24
20.75

45.25
35.21
25.11
21.66
19.80

Lead

33.90
25.60
19.15
16.62
10.57

37.69
23.70
15.52
8.02
10.83

38.74
33.25
27.17
25.20
18.56

43.64
32.26
23.03
18.24
17.94

73.61
59.99
55.35
52.25
43.00

74.05
57.21
47.22
41.23
41.37

65.53
46.66
40.97
36.14
27.14

64.84
43.08
33.43
21.56
26.04

71.50
56.64
51.97
47.73
39.48

72.13
53.31
44.05
35.90
37.87

29.70

19.74

27.30

55.32

41.28

51.72

29.90

19.54

27.48

52.79

38.33

49.20

Table 1: The statistics of the MULTI-SUM dataset. Three measures refer to COVERAGE, DENSITY and COM-
PRESSION respectively. LEAD and EXT-ORACLE are two common baselines for summarization. All measures and
baselines are calculated on the test set of the corresponding publication. The top ﬁve publication are used as source
domains for training and the bottom ones are viewed as OUT-OF-DOMAIN.

LEAD baseline concatenates the ﬁrst few sen-
tences as the summary and calculates its ROUGE
score. This baseline shows the lead bias of the
dataset, which is an essential factor in news ar-
ticles. The EXT-ORACLE baseline evaluates the
performance of the ground truth labels and can be
viewed as the upper bound of the extractive sum-
marization models (Nallapati et al., 2017; Narayan
et al., 2018a).

MULTI-SUM The recently proposed dataset
Newsroom (Grusky et al., 2018) is used, which
was scraped from 38 major news publications. We
select top ten publications (NYTimes, Washington-
Post, FoxNews, TheGuardian, NYDailyNews, WSJ,
USAToday, CNN, Time and Mashable) and process
them in the way of See et al. (2017). To obtain the
ground truth labels for extractive summarization
task, we follow the greedy approach introduced
by Nallapati et al. (2017). Finally, we randomly
divide ten domains into two groups, one for train-
ing and the other for test. We call this re-purposed
subset of Newsroom MULTI-SUM to indicate it
is specially designed for multi-domain learning in
summarization tasks.

From Table 1, we can ﬁnd that data from those
news publications vary in indicators that are closely
relevant to summarization. This means that (docu-
ment, summary) pairs from different publications
will have unique summarization formation, and
models might need to learn different semantic fea-
tures for different publications. Furthermore, we
follow the simple experiment by Torralba et al.

(2011) to train a classiﬁer for the top ﬁve domains.
A simple classiﬁcation model with GloVe initializ-
ing words can also achieve 74.84% accuracy (the
chance is 20%), which ensures us that there is a
built-in bias in each publication. Therefore, it is
reasonable to view one publication as a domain
and use our multi-publication MULTI-SUM as a
multi-domain dataset.

3 Analytical Experiment for Domain

Shift

Domain shift refers to the phenomenon that a
model trained on one domain performs poorly on
a different domain(Saenko et al., 2010; Gopalan
et al., 2011). To clearly verify the existence of do-
main shift in the text summarization, we design a
simple experiment on MULTI-SUM dataset.

Concretely, we take turns choosing one domain
and use its training data to train the basic model.
Then, we use the testing data of the remaining
domains to evaluate the model with the automatic
metric ROUGE (Lin and Hovy, 2003)

Basic Model Like a few recent approaches, we
deﬁne extractive summarization as a sequence la-
beling task. Formally, given a document S con-
sisting of n sentences s1, · · · , sn, the summaries
are extracted by predicting a sequence of label
Y = y1, · · · , yn (yi ∈ {0, 1}) for the document,
where yi = 1 represents the i-th sentence in the
document should be included in the summaries.

5ROUGE-2 and ROUGE-L show similar trends and their

results are attached in Appendix.

FN

CNN

MA

NYT WTP

NYDN WSJ

USAT

TG

TIME

FN
CNN
MA
NYT
WTP
NYDN
WSJ
USAT
TG
TIME

48.84
-0.92
-2.59
-3.13
-1.92
-1.96
-4.66
-2.04
-5.47
-0.82

-1.23
41.22
-6.62
-4.46
-3.03
-2.31
-7.07
-3.55
-8.63
-3.05

-1.76
-1.93
35.19
-2.41
-2.00
-1.91
-3.10
-2.56
-3.50
-1.64

-0.70
-1.49
-1.37
29.65
-0.61
-0.57
-1.04
-2.77
-1.50
-1.36

-0.27
-1.01
-1.45
-0.97
23.01
-0.53
-1.57
-1.86
-1.74
-1.06

-2.29
-3.83
-5.11
-3.95
-3.28
51.36
-6.37
-6.09
-5.82
-4.65

-0.31
-0.55
-0.25
-0.33
-0.43
-0.41
38.60
-0.92
-1.43
-0.47

-0.57
-1.00
-1.54
-1.16
-0.57
-1.20
-1.71
29.11
-2.19
-1.05

-0.27
-0.80
-0.17
-0.17
-0.01
-0.36
-0.58
-0.68
23.93
-0.24

-0.02
-0.06
-0.71
-0.86
-0.28
-0.05
-1.00
-1.27
-1.66
21.90

Table 2: Results (Matrix V ) of the veriﬁcation experiment based on the MULTI-SUM dataset. The ROUGE-1
scores 5 of the model which is trained and tested on the same domain Rii are shown on the diagonal line. It is
regarded as benchmark scores. The other cells Vij = Rij − Rjj, i (cid:54)= j, which represents that for the same test
domain j, how many improvements we obtained when we switch from training domain i to j. Positive values are
higher than the benchmark, and negative values are less than the benchmark.

In this paper, we implement a simple but pow-
erful model based on the encoder-decoder archi-
tecture. We choose CNN as the sentence encoder
following prior works (Chen and Bansal, 2018) and
employ the popular modular Transformer (Vaswani
et al., 2017) as the document encoder. The detailed
settings are described in Section 5.1.

Results From Table 2, we ﬁnd that the values are
negative except the diagonal, which indicates mod-
els trained and tested on the same domain show
the great advantage to those trained on other do-
mains. The signiﬁcant performance drops demon-
strate that the domain shift problem is quite se-
rious in extractive summarization tasks, and thus
pose challenges to current well-performed mod-
els, which are trained and evaluated particularly
under the strong hypothesis: training and test data
instances are drawn from the identical data distribu-
tion. Motivated by this vulnerability, we investigate
the domain shift problem under both multi-domain
training and evaluation settings.

4 Multi-domain Summarization

With the above observations in mind, we are seek-
ing an approach which can alleviate the domain
shift problem effectively in text summarization.
Speciﬁcally, the model should not only perform
well on source domains where it is trained on, but
also show advantage on the unseen target domains.
This involves the tasks of multi-domain learning
and domain adaptation. Here, we begin with sev-
eral simple approaches for multi-domain summa-
rization based on multi-domain learning.

4.1 Four Learning Strategies

, Y (k)
i

To facilitate the following description, we ﬁrst
set up mathematical notations. Assuming that
there are K related domains, we refer to Dk as
a dataset with Nk samples for domain k. Dk =
{(S(k)
represent
i
a sequence of sentences and the corresponding la-
bel sequence from a document of domain k, re-
spectively. The goal is to estimate the conditional
probability P (Y |S) by utilizing the complementar-
ities among different domains.

i=1, where S(k)

and Y (k)
i

)}Nk

i

ModelI
Base This is a simple but effective model
for multi-domain learning, in which all domains
are aggregated together and will be further used
for training a set of shared parameters. Notably,
domains in this model are not explicitly informed
of their differences.

Therefore, the loss function of each domain can

be written as:

L(k)
I = L(BASIC(S(k), θ(s)), Y(k))
(1)
where BASIC denotes our CNN-Transformer en-
coder framework (As described in Section 3). θ(s)
means that all domains share the same parameters.
Analysis: The above model beneﬁts from the
joint training strategy, which can allow a mono-
lithic model to learn shared features from different
domains. However, it is not sufﬁcient to alleviate
the domain shift problem, because two potential
limitations remain: 1) The joint model is not aware
of the differences across domains, which would
lead to poor performance on in-task evaluation
since some task-speciﬁc features shared by other
tasks. 2) Negative transferring might happened on
new domains. Next, we will study three different
approaches to address the above problems.

ModelII
BERT More recently, unsupervised pre-
training has achieved massive success in NLP com-
munity (Devlin et al., 2018; Peters et al., 2018),
which usually provides tremendous external knowl-
edge. However, there are few works on building the
connection between large-scale pre-trained mod-
els and multi-domain learning. In this model, we
explore how the external knowledge unsupervised
pre-trained models bring can contribute to multi-
domain learning and new domain adaption 6.

We achieve this by pre-training our basic model
M odelI
Base with BERT (Devlin et al., 2018), which
is one of the most successful learning frameworks.
Then we investigate if BERT can provide domain
information and bring the model good domain
adaptability. To avoid introducing new structures,
we use the feature-based BERT with its parameters
ﬁxed.

Analysis: This model instructs the processing
of multi-domain learning by utilizing external pre-
trained knowledge. Another perspective is to ad-
dress this problem algorithmically.

ModelIII
T ag The domain type can also be intro-
duced directly as a feature vector, which can aug-
ment learned representations with domain-aware
ability.

Speciﬁcally, each domain tag C(k) will be em-
bedded into a low dimensional real-valued vector
and then be concatenated with sentence embedding
s(k)
. The loss function can be formulated as:
i
L(k)
III = L(BASIC(S(k), C(k), θ(s)), Y(k))
(2)
It is worth noting that, on unseen domains, the in-
formation of real domain tags is not available. Thus
we design a domain tag ‘X’ for unknown domains
and randomly relabeled examples with it during
training. Since the real tag of the data tagged with
‘X’ may be any source domain, this embedding
will force the model to learn the shared features
and makes it more adaptive to unseen domains. In
the experiment, this improves the performance on
both source domains and target domains.

Analysis: This domain-aware model makes it
possible to learn domain-speciﬁc features, while
it still suffers from the negative transfer problem
since private and shared features are entangled in
shared space (Bousmalis et al., 2016; Liu et al.,
2017). Speciﬁcally, each domain has permission to

6Concurrent with our work, Radford et al. (2019) also
apply pre-trained language model to a wide range of NLP
tasks in a zero-shot setting. We will discuss the differences in
the related work section.

Figure 1: The gradient update mechanism of the meta
learning strategy of ModelIV

M eta.

modify shared parameters, which makes it easier
to update parameters along different directions.

M eta

ModelIV
In order to overcome the above lim-
itations, we try to bridge the communication gap
between different domains when updating shared
parameters via meta-learning (Finn et al., 2017; Li
et al., 2017; Liu and Huang, 2018).

Here, the introduced communicating protocol
claims that each domain should tell others what
its updating details (gradients) are. Through its
different updating behaviors of different domains
can be more consistent.

Formally, given a main domain A and an aux-
iliary domain B, the model will ﬁrst compute the
gradients of A ∇θLA with regard to the model pa-
rameters θ. Then the model will be updated with
the gradients and calculate the gradients of B.

Our objective is to produce maximal perfor-

mance on sample (S(B), Y (B)):

LA←B = min

L(S(B), Y (B), ∇θLA)

(3)

θ

So, the loss function for each domain can be

ﬁnally written as:
L(k)
IV = γL(k) + (1 − γ)

Lk←j

(4)

(cid:88)

j(cid:54)=k

where γ (0 ≤ γ ≤ 1) is the weight coefﬁcient and
L can be instantiated as LI (Eqn. 1), LII or LIII
(Eqn. 2).

Analysis: To address the multi-domain learn-
ing task and the adaptation to new domains,
BERT , ModelIII
T ag, ModelIV
ModelII
M eta take different
angles. Speciﬁcally, ModelII
BERT utilizes a large-
scale pre-trained model while ModelIII
T ag proposes
to introduce domain type information explicitly.
Lastly, ModelIV
M eta is designed to update parame-
ters more consistently, by adjusting the gradient
direction of the main domain A with the auxiliary
domain B during training. This mechanism indeed
puriﬁes the shared feature space via ﬁltering out
the domain-speciﬁc features which only beneﬁt A.

5 Experiment

Domains ModelI

Basic ModelII

BERT ModelIII

T ag ModelIV

M eta

We investigate the effectiveness of the above four
strategies under three evaluation settings:
IN-
DOMAIN, OUT-OF-DOMAIN and CROSS-DATASET.
These settings make it possible to explicitly eval-
uate models both on the quality of domain-aware
text representation and on their adaptation ability
to derive reasonable representations in unfamiliar
domains.

5.1 Experiment Setup

We perform our experiments mainly on our multi-
domain MULTI-SUM dataset. Source domains
are deﬁned as the ﬁrst ﬁve domains (IN-DOMAIN)
in Table 1 and the other domains (OUT-OF-
DOMAIN) are totally invisible during training. The
evaluation under the IN-DOMAIN setting tests the
model ability to learn different domain distribution
on a multi-domain set and later OUT-OF-DOMAIN
investigates how models perform on unseen do-
mains. We further make use of CNN/DailyMail
as a CROSS-DATASET evaluation environment to
provide a larger distribution gap.

We use ModelI

Basic as a baseline model,
build ModelII
BERT with feature-based BERT and
ModelIII
T ag with domain embedding on it. We
further develop ModelIII
T ag as the instantiation of
ModelIV
M eta. For the detailed dataset statistics,
model settings and hyper-parameters, the reader
can refer to Appendix.

5.2 Quantitative Results

We compare our models by ROUGE-1 scores
in Table 3. Note that we select two sentences
for MULTI-SUM domains and three sentences
for CNN/Daily Mail due to the different average
lengths of reference summaries.
Basic vs ModelIII

ModelI
T ag From Table 3, we ob-
serve that the domain-aware model outperforms the
monolithic model under both IN-DOMAIN and OUT-
OF-DOMAIN settings. The signiﬁcant improvement
of IN-DOMAIN demonstrates domain information
is effective for summarization models trained on
multiple domains. Meanwhile, the superior perfor-
mance on OUT-OF-DOMAIN further illustrates that,
the awareness of domain difference also beneﬁts
under the zero-shot setting. This might suggest that
the domain-aware model could capture domain-
speciﬁc features by domain tags and have learned
domain-invariant features at the same time, which
can be transferred to unseen domains.

IN-DOMAIN SETTING

FN
CNN
MA
NYT
WTP

Average

NTDN
WSJ
USAT
TG
TIME

Average

49.13
41.84
35.40
30.68
23.88

34.24

48.62
38.55
28.85
24.15
21.67

32.78

OUT-OF-DOMAIN SETTING

49.70
41.59
36.46
31.92
24.47

35.04

49.37
39.63
29.57
24.89
22.45

33.68

49.54
42.14
35.51
30.70
24.02

34.40

49.44
39.00
29.09
24.10
22.20

33.17

49.06
41.73
34.85
30.20
23.51

33.95

49.47
38.93
28.99
24.18
22.12

33.24

∆R

1.47 ↓

1.35 ↓

1.24 ↓

0.71 ↓

CROSS-DATASET SETTING

CNN/DM 40.11

39.82

40.28

40.30

Table 3: Rouge-1 performance of our four learning
strategies on the MULTI-SUM dataset. ∆R =
|Rouge(IN-DOMAIN) − Rouge(OUT-OF-DOMAIN)| .
A smaller ∆R indicates corresponding model has a
better generalization ability. Bold numbers are the
best results, and red ones indicate the minimum per-
formance gap between source and target domains. The
grey rows show the models’ average performance un-
der three evaluation settings.
Basic vs ModelIV
ModelI
M eta Despite a little drop
under IN-DOMAIN setting, the narrowed perfor-
mance gap, as shown in ∆R of Table 3, indicates
ModelIV
M eta has better generalization ability as a
compensation. The performance decline mainly
lies in the more consistent way to update param-
eters, which puriﬁes shared feature space at the
expense of ﬁltering out some domain-speciﬁc fea-
tures. The excellent results under CROSS-DATASET
settings further suggest the meta-learning strategy
successfully improve the model transferability not
only among the domains of MULTI-SUM but also
across different datasets.

ModelII
Supported by the smaller ∆R com-
BERT
pared with ModelI
Base, we can draw the conclu-
sion that BERT shows some domain generalization
ability 7 within MULTI-SUM. However, this abil-
ity is inferior to ModelIII
M eta, which
further leads to the worse performance on CROSS-
DATASET. Thus we cannot attribute its success
in MULTI-SUM to the ability to address multi-
domain learning nor domain adaptation. Instead,

T ag and ModelIV

7We give a speciﬁc experiment and analyze why
BERT with BERT can achieve domain generalization

ModelII
in Appendix.

Figure 2: Relative position of selected sentence in the original document across ﬁve source domains. We overlap
the ground truth labels with the model results in order to highlight the differences. The two rows correspond to
Model-I and Model-III in Section 4.

Model

R-1

R-2

R-L

Lead-3 (See et al., 2017)

40.34

17.70

36.57

Narayan et al. (2018b)
Zhang et al. (2018)
Chen and Bansal (2018)
Dong et al. (2018)
Zhou et al. (2018)

Our basic model
Basic model + Tag
Basic model + Meta
Basic model + BERT
Basic model + BERT + Tag

40.00
41.05
41.47
41.50
41.59

41.33
0.13↑
0.07↓
0.93↑
0.97↑

18.20
18.77
18.72
18.70
19.01

18.83
0.05↑
0.02↓
0.90↑
0.93↑

36.60
37.54
37.76
37.60
37.98

37.65
0.10↑
0.05↓
0.97↑
1.01↑

Table 4: Comparison between our strategies with other
extractive summarization models on non-anonymized
CNN/Daily Mail provided by See et al. (2017). The
red up-arrows indicate performance improvement over
our base model, and the green down-arrows denote the
degradation.

we suppose the vast external knowledge of BERT
provides its superior ability for feature extraction.
That causes ModelII
BERT to overﬁt MULTI-SUM
and perform excellently across all domains, but
fails on the more different dataset CNN/Daily Mail.
This observation also suggests that although un-
supervised pre-trained models are powerful enough
(Radford et al., 2019), still, it can not take place the
role of supervised learning methods (i.e. ModelIII
T ag
and ModelIV
M eta), which is designed speciﬁcally for
addressing multi-domain learning and new domain
adaptation.

the best generalization ability at the cost of rela-
tively lower in-domain performance. Therefore,
using ModelIV
M eta is not a good choice if in-domain
performance matters for end users. ModelII
BERT
can achieve the best performance under in-domain
settings at expense of training time and shows
worse generalization ability than ModelIV
M eta. If
the training time is not an issue, ModelII
BERT could
be a good supplement for other methods.

5.3 Results on CNN/DailyMail

Inspired by such observations, we further em-
ploy our four learning strategies to the mainstream
summarization dataset CNN/DailyMail (See et al.,
2017), which also includes two different data
sources: CNN and DailyMail. We use the publica-
tion as the domain and train our models on its 28w
training set. As Table 4 shows, our basic model has
comparable performance with other extractive sum-
marization models. Besides, the publication tags
can improve ROUGE scores signiﬁcantly by 0.13
points in ROUGE-1 and the meta learning strat-
egy does not show many advantages when dealing
with in-domain examples, what we have expected.
BERT with tags achieves the best performance, al-
though the performance increment is not as much
as what publication tags bring to the basic model,
which we suppose that BERT itself has contained
some degree of domain information.

Analysis of Different Model Choices To sum-
marize, ModelIII
T ag is a simple and efﬁcient method,
which can achieve good performance under in-
domain setting and shows certain generalization
ability on the unseen domain. ModelIV
M eta shows

5.4 Qualitative Analysis

We furthermore design several experiments to
probe into some potential factors that might con-
tribute to the superior performance of domain-
aware models over the monolithic basic model.

the import of the auxiliary domain hurts the model
ability to learn domain-speciﬁc features. However,
results under both OUT-OF-DOMAIN and CROSS-
DATASET settings indicate the loss of B, which is
informed of A’s gradient information, helps the
model to learn more general features, thus improv-
ing the generalization ability.

6 Related Work

We brieﬂy outline connections and differences to
the following related lines of research.

Domains in Summarization There have been
several works in summarization exploring the con-
cepts of domains. Cheung and Penn (2013b) ex-
plored domain-speciﬁc knowledge and associated
it as template information. Hua and Wang (2017)
investigated domain adaptation in abstractive sum-
marization and found the content selection is trans-
ferable to a new domain. Gehrmann et al. (2018)
trained a selection mask for abstractive summa-
rization and proved it has excellent adaptability.
However, previous works just investigated mod-
els trained on a single domain and did not explore
multi-domain learning in summarization.

Multi-domain Learning (MDL) & Domain
Adaptation (DA) We focus on the testbed that
requires both training and evaluating performance
on a set of domains. Therefore, we care about two
questions: 1) how to learn a model when the train-
ing set contains multiple domains – involving MDL.
2) how to adapt the multi-domain model to new do-
mains – involving DA. Beyond the investigation of
some effective approaches like existing works, we
have ﬁrst veriﬁed how domain shift inﬂuences the
summarization tasks.

Semi-supervised Pre-training for Zero-shot
Transfer
It has a long history of ﬁne-tuning
downstream tasks with supervised or unsupervised
pre-trained models (Le and Mikolov, 2014; De-
vlin et al., 2018; Peters et al., 2018). However,
there is a rising interest in applying large-scale pre-
trained models to zero-shot transfer learning (Rad-
ford et al., 2019). Different from the above works,
we focus on addressing domain shift and general-
ization problem. One of our explored methods is
semi-supervised pre-training, which combines su-
pervised and unsupervised approaches to achieve
zero-shot transfer.

(a) In-domain

(b) Out-of-domain (c) Cross-Dataset

Figure 3: Loss weight coefﬁcients γ for Model-IV. The
y-axis is the mean score of ROUGE-1, ROUGE-2 and
ROUGE-L and different bins correspond to different γ
values.

Label Position Sentence position is a well
known and powerful feature, especially for extrac-
tive summarization (Kedzie et al., 2018) 8. We com-
pare the relative position of sentences selected by
our models with the ground truth labels on source
domains to investigate how well these models ﬁt
the distribution and whether they can distinguish
between domains. We select the most representa-
tive models ModelI
T ag illustrated
in Figure 2 9.

Base and ModelIII

The percentage of the ﬁrst sentence on FoxNews
is signiﬁcantly higher than others: (1) Unaware of
different domains, ModelI
Base learns a similar dis-
tribution for all domains and is seriously affected
In its density his-
by this extreme distribution.
togram, the probability of the ﬁrst sentence being
selected is much higher than the ground truth on the
other four domains. (2) Compared with ModelI
Base,
domain-aware models are more robust by learning
different relative distributions for different domains.
ModelIII
T ag constrains the extreme trend especially
obviously on CNN and Mashable.
Weight γ for ModelIV
M eta We investigate sev-
eral γ to further probe into the performance of
ModelIV
M eta. In Eqn. 4, γ is the weight coefﬁcient
of main domain A. When γ = 0, the model ignores
A and focuses on the auxiliary domain B and when
γ = 1 it is trained only on the loss of main do-
main A (the same as the instantiation ModelIII
T ag).
As Figure 3 shows, with the increase of γ, the
Rouge scores rise on IN-DOMAIN while decline on
OUT-OF-DOMAIN and CROSS-DATASET. The per-
formances under IN-DOMAIN settings prove that

8We plot the density histogram of the relative locations of
ground truth labels for both source and target domains and
attach it in Appendix. Compared with Table 2, we can ﬁnd that
the relative position of ground truth labels is closely related to
ROUGE performance of the basic model.

9The whole picture in the Appendix illustrates the four

models performance.

7 Conclusion

In this paper, we explore publication in the con-
text of the domain and investigate the domain shift
problem in summarization. When veriﬁed its exis-
tence, we propose to build a multi-domain testbed
for summarization that requires both training and
measuring performance on a set of domains. Un-
der these new settings, we propose four learning
schemes to give a preliminary explore in character-
istics of different learning strategies when dealing
with multi-domain summarization tasks.

Acknowledgment

We thank Jackie Chi Kit Cheung for useful com-
ments and discussions. The research work is sup-
ported by National Natural Science Foundation
of China (No. 61751201 and 61672162), Shang-
hai Municipal Science and Technology Commis-
sion (16JC1420401 and 17JC1404100), Shang-
hai Municipal Science and Technology Major
Project(No.2018SHZDZX01)and ZJLab.

References

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
In
Domain adaptation for sentiment classiﬁcation.
Proceedings of the 45th annual meeting of the asso-
ciation of computational linguistics, pages 440–447.

Konstantinos Bousmalis, George Trigeorgis, Nathan
Silberman, Dilip Krishnan, and Dumitru Erhan.
2016. Domain separation networks. In Advances in
Neural Information Processing Systems, pages 343–
351.

Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2017.
Improving Multi-Document Summarization via Text
Classiﬁcation. Proceedings of the 31th Conference
on Artiﬁcial Intelligence (AAAI 2017).

Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-
tive summarization with reinforce-selected sentence
rewriting. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 675–686.

Jianpeng Cheng and Mirella Lapata. 2016. Neural sum-
In
marization by extracting sentences and words.
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 484–494.

Jackie Chi Kit Cheung and Gerald Penn. 2013a. Prob-
abilistic domain modelling with contextualized dis-
tributional semantic vectors. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 392–401.

Jackie Chi Kit Cheung and Gerald Penn. 2013b. To-
wards robust abstractive multi-document summa-
rization: A caseframe analysis of centrality and do-
main. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 1233–1242.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Yue Dong, Yikang Shen, Eric Crawford, Herke van
Hoof, and Jackie Chi Kit Cheung. 2018. BanditSum:
Extractive Summarization as a Contextual Bandit.
In Empirical Methods in Natural Language Process-
ing (EMNLP).

Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Ma-
chine Learning, pages 1126–1135.

Sebastian Gehrmann, Yuntian Deng, and Alexander M.
Rush. 2018. Bottom-Up Abstractive Summarization.
In Empirical Methods in Natural Language Process-
ing (EMNLP).

Raghuraman Gopalan, Ruonan Li, and Rama Chel-
lappa. 2011. Domain adaptation for object recog-
In 2011 inter-
nition: An unsupervised approach.
national conference on computer vision, pages 999–
1006. IEEE.

Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Newsroom: A dataset of 1.3 million summaries with
diverse extractive strategies. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), volume 1, pages 708–719.

Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362–370. Association for
Computational Linguistics.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
In Advances in Neural Informa-
and comprehend.
tion Processing Systems, pages 1693–1701.

Xinyu Hua and Lu Wang. 2017. A pilot study of do-
main adaptation effect for neural abstractive summa-
rization. arXiv preprint arXiv:1707.07062.

Masaru Isonuma, Toru Fujino, Junichiro Mori, Yutaka
Matsuo, and Ichiro Sakata. 2017. Extractive sum-
marization using multi-task learning with document
In Proceedings of the 2017 Confer-
classiﬁcation.
ence on Empirical Methods in Natural Language
Processing, pages 2101–2110.

Mahesh Joshi, William W Cohen, Mark Dredze, and
Carolyn P Ros´e. 2012. Multi-domain learning:
In Proceedings of the
when do domains matter?
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1302–1312. Associa-
tion for Computational Linguistics.

Chris Kedzie, Kathleen Mckeown, and Hal Daum.
2018. Content Selection in Deep Learning Models
of Summarization. In Empirical Methods in Natural
Language Processing (EMNLP).

Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M
Hospedales. 2017. Learning to generalize: Meta-
learning for domain generalization. arXiv preprint
arXiv:1710.03463.

Shoushan Li and Chengqing Zong. 2008. Multi-
domain sentiment classiﬁcation. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics on Human Language Tech-
nologies: Short Papers, pages 257–260. Association
for Computational Linguistics.

Chin-Yew Lin and Eduard Hovy. 2003.

Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003 Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.

Pengfei Liu and Xuanjing Huang. 2018. Meta-
learning multi-task communication. arXiv preprint
arXiv:1810.09988.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classiﬁca-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 1–10.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of docu-
ments. In Thirty-First AAAI Conference on Artiﬁcial
Intelligence.

Courtney Napoles, Matthew Gormley, and Benjamin
In Pro-
Van Durme. 2012. Annotated gigaword.
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, pages 95–100. Association for Computa-
tional Linguistics.

Shashi Narayan, Shay B Cohen, and Mirella Lap-
just the
ata. 2018a. Don’t give me the details,
summary!
topic-aware convolutional neural net-
works for extreme summarization. arXiv preprint
arXiv:1808.08745.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018b. Ranking Sentences for Extractive Summa-
rization with Reinforcement Learning.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 2227–2237.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
Models are Unsupervised Multitask Learners.

Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Dar-
rell. 2010. Adapting visual category models to new
In European conference on computer vi-
domains.
sion, pages 213–226. Springer.

Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Antonio Torralba, Alexei A Efros, et al. 2011. Unbi-
ased look at dataset bias. In CVPR, volume 1, page 7.
Citeseer.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Li Wang, Junlin Yao, Yunzhe Tao, Li Zhong, Wei
Liu, and Qiang Du. 2018. A reinforced topic-
aware convolutional sequence-to-sequence model
for abstractive text summarization. arXiv preprint
arXiv:1805.03616.

Yuxiang Wu and Baotian Hu. 2018. Learning to extract
coherent summary via deep reinforcement learning.
In Thirty-Second AAAI Conference on Artiﬁcial In-
telligence.

Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming
Zhou. 2018. Neural Latent Extractive Document
Summarization.

Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu,
and Xuan-Jing Huang. 2019. Searching for effec-
tive neural extractive summarization: What works
and whats next. In Proceedings of the 57th Confer-
ence of the Association for Computational Linguis-
tics, pages 1049–1058.

Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,
Ming Zhou, and Tiejun Zhao. 2018. Neural docu-
ment summarization by jointly learning to score and
select sentences. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
654–663.

