7
1
0
2
 
b
e
F
 
7
 
 
]

G
L
.
s
c
[
 
 
3
v
0
7
7
2
0
.
1
1
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

DELVING INTO TRANSFERABLE ADVERSARIAL EX-
AMPLES AND BLACK-BOX ATTACKS

Yanpei Liu∗, Xinyun Chen∗
Shanghai Jiao Tong University

Chang Liu, Dawn Song
University of the California, Berkeley

ABSTRACT

An intriguing property of deep neural networks is the existence of adversarial ex-
amples, which can transfer among different architectures. These transferable ad-
versarial examples may severely hinder deep neural network-based applications.
Previous works mostly study the transferability using small scale datasets. In this
work, we are the ﬁrst to conduct an extensive study of the transferability over
large models and a large scale dataset, and we are also the ﬁrst to study the trans-
ferability of targeted adversarial examples with their target labels. We study both
non-targeted and targeted adversarial examples, and show that while transferable
non-targeted adversarial examples are easy to ﬁnd, targeted adversarial examples
generated using existing approaches almost never transfer with their target labels.
Therefore, we propose novel ensemble-based approaches to generating transfer-
able adversarial examples. Using such approaches, we observe a large proportion
of targeted adversarial examples that are able to transfer with their target labels for
the ﬁrst time. We also present some geometric studies to help understanding the
transferable adversarial examples. Finally, we show that the adversarial examples
generated using ensemble-based approaches can successfully attack Clarifai.com,
which is a black-box image classiﬁcation system.

1

INTRODUCTION

Recent research has demonstrated that for a deep architecture, it is easy to generate adversarial
examples, which are close to the original ones but are misclassiﬁed by the deep architecture (Szegedy
et al. (2013); Goodfellow et al. (2014)). The existence of such adversarial examples may have severe
consequences, which hinders vision-understanding-based applications, such as autonomous driving.
Most of these studies require explicit knowledge of the underlying models.
It remains an open
question how to efﬁciently ﬁnd adversarial examples for a black-box model.

Several works have demonstrated that some adversarial examples generated for one model may
also be misclassiﬁed by another model. Such a property is referred to as transferability, which
can be leveraged to perform black-box attacks. This property has been exploited by constructing
a substitute of the black-box model, and generating adversarial instances against the substitute to
attack the black-box system (Papernot et al. (2016a;b)). However, so far, transferability is mostly
examined over small datasets, such as MNIST (LeCun et al. (1998)) and CIFAR-10 (Krizhevsky &
Hinton (2009)). It has yet to be better understood transferability over large scale datasets, such as
ImageNet (Russakovsky et al. (2015)).

In this work, we are the ﬁrst to conduct an extensive study of the transferability of different adver-
sarial instance generation strategies applied to different state-of-the-art models trained over a large
scale dataset. In particular, we study two types of adversarial examples: (1) non-targeted adversar-
ial examples, which can be misclassiﬁed by a network, regardless of what the misclassiﬁed labels
may be; and (2) targeted adversarial examples, which can be classiﬁed by a network as a target
label. We examine several existing approaches searching for adversarial examples based on a single
model. While non-targeted adversarial examples are more likely to transfer, we observe few targeted
adversarial examples that are able to transfer with their target labels.

∗Work is done while visiting UC Berkeley.

1

Published as a conference paper at ICLR 2017

We further propose a novel strategy to generate transferable adversarial images using an ensemble
of multiple models. In our evaluation, we observe that this new strategy can generate non-targeted
adversarial instances with better transferability than other methods examined in this work. Also, for
the ﬁrst time, we observe a large proportion of targeted adversarial examples that are able to transfer
with their target labels.

We study geometric properties of the models in our evaluation.
In particular, we show that the
gradient directions of different models are orthogonal to each other. We also show that decision
boundaries of different models align well with each other, which partially illustrates why adversarial
examples can transfer.

Last, we study whether generated adversarial images can attack Clarifai.com, a commercial com-
pany providing state-of-the-art image classiﬁcation services. We have no knowledge about the train-
ing dataset and the types of models used by Clarifai.com; meanwhile, the label set of Clarifai.com
is quite different from ImageNet’s. We show that even in this case, both non-targeted and targeted
adversarial images transfer to Clarifai.com. This is the ﬁrst work documenting the success of gen-
erating both non-targeted and targeted adversarial examples for a black-box state-of-the-art online
image classiﬁcation system, whose model and training dataset are unknown to the attacker.

Contributions and organization. We summarize our main contributions as follows:

• For ImageNet models, we show that while existing approaches are effective to generate
non-targeted transferable adversarial examples (Section 3), only few targeted adversarial
examples generated by existing methods can transfer (Section 4).

• We propose novel ensemble-based approaches to generate adversarial examples (Sec-
tion 5). Our approaches enable a large portion of targeted adversarial examples to transfer
among multiple models for the ﬁrst time.

• We are the ﬁrst to present that targeted adversarial examples generated for models trained
on ImageNet can transfer to a black-box system, i.e., Clarifai.com, whose model, training
data, and label set is unknown to us (Section 7). In particular, Clarifai.com’s label set is
very different from ImageNet’s.

• We conduct the ﬁrst analysis of geometric properties for large models trained over Ima-
geNet (Section 6), and the results reveal several interesting ﬁndings, such as the gradient
directions of different models are orthogonal to each other.

In the following, we ﬁrst discuss related work, and then present the background knowledge and
experiment setup in Section 2. Then we present each of our experiments and conclusions in the
corresponding section as mentioned above.

Related work. Transferability of adversarial examples was ﬁrst examined by Szegedy et al.
(2013), which studied the transferability (1) between different models trained over the same dataset;
and (2) between the same or different model trained over disjoint subsets of a dataset; However,
Szegedy et al. (2013) only studied MNIST.

The study of transferability was followed by Goodfellow et al. (2014), which attributed the phe-
nomenon of transferability to the reason that the adversarial perturbation is highly aligned with the
weight vector of the model. Again, this hypothesis was tested using MNIST and CIFAR-10 datasets.
We show that this is not the case for models trained over ImageNet.

Papernot et al. (2016a;b) examined constructing a substitute model to attack a black-box target
model. To train the substitute model, they developed a technique that synthesizes a training set and
annotates it by querying the target model for labels. They demonstrate that using this approach,
black-box attacks are feasible towards machine learning services hosted by Amazon, Google, and
MetaMind. Further, Papernot et al. (2016a) studied the transferability between deep neural networks
and other models such as decision tree, kNN, etc.

Our work differs from Papernot et al. (2016a;b) in three aspects. First, in these works, only the model
and the training process are a black box, but the training set and the test set are controlled by the
attacker; in contrast, we attack Clarifai.com, whose model, training data, training process, and even
the test label set are unknown to the attacker. Second, the datasets studied in these works are small

2

Published as a conference paper at ICLR 2017

scale, i.e., MNIST and GTSRB (Stallkamp et al. (2012)); in our work, we study the transferability
over larger models and a larger dataset, i.e., ImageNet. Third, to attack black-box machine learning
systems, we do not query the systems for constructing the substitute model ourselves.

In a concurrent and independent work, Moosavi-Dezfooli et al. (2016) showed the existence of a
universal perturbation for each model, which can transfer across different images. They also show
that the adversarial images generated using these universal perturbations can transfer across different
models on ImageNet. However, they only examine the non-targeted transferability, while our work
studies both non-targeted and targeted transferability over ImageNet.

2 ADVERSARIAL DEEP LEARNING AND TRANSFERABILITY

2.1 THE ADVERSARIAL DEEP LEARNING PROBLEM

We assume a classiﬁer fθ(x) outputs a category (or a label) as the prediction. Given an original
image x, with ground truth label y, the adversarial deep learning problem is to seek for adversarial
examples for the classiﬁer fθ(x). Speciﬁcally, we consider two classes of adversarial examples.
A non-targeted adversarial example x(cid:63) is an instance that is close to x, in which case x(cid:63) should
have the same ground truth as x, while fθ(x(cid:63)) (cid:54)= y. For the problem to be non-trivial, we assume
fθ(x) = y without loss of generality. A targeted adversarial example x(cid:63) is close to x and satisﬁes
fθ(x(cid:63)) = y(cid:63), where y(cid:63) is a target label speciﬁed by the adversary, and y(cid:63) (cid:54)= y.

2.2 APPROACHES FOR GENERATING ADVERSARIAL EXAMPLES

In this work, we consider three classes of approaches for generating adversarial examples:
optimization-based approaches, fast gradient approaches, and fast gradient sign approaches. Each
class has non-targeted and targeted versions respectively.

2.2.1 APPROACHES FOR GENERATING NON-TARGETED ADVERSARIAL EXAMPLES

Formally, given an image x with ground truth y = fθ(x), searching for a non-targeted adversarial
example can be modeled as searching for an instance x(cid:63) to satisfy the following constraints:

fθ(x(cid:63)) (cid:54)= y
d(x, x(cid:63)) ≤ B

(1)
(2)

where d(·, ·) is a metric to quantify the distance between an original image and its adversarial coun-
terpart, and B, called distortion, is an upper bound placed on this distance. Without loss of gener-
ality, we consider model f is composed of a network Jθ(x), which outputs the probability for each
category, so that f outputs the category with the highest probability.

Optimization-based approach. One approach is to approximate the solution to the following
optimization problem:

argminx(cid:63) λd(x, x(cid:63)) − (cid:96)(1y, Jθ(x(cid:63)))
(3)
where 1y is the one-hot encoding of the ground truth label y, (cid:96) is a loss function to measure the
distance between the prediction and the ground truth, and λ is a constant to balance constraints (2)
and (1), which is empirically determined. Here, loss function (cid:96) is used to approximate constraint (1),
and its choice can affect the effectiveness of searching for an adversarial example. In this work, we
choose (cid:96)(u, v) = log (1 − u · v), which is shown to be effective by Carlini & Wagner (2016).

Fast gradient sign (FGS). Goodfellow et al. (2014) proposed the fast gradient sign (FGS) method
so that the gradient needs be computed only once to generate an adversarial example. FGS can be
used to generate adversarial images to meet the L∞ norm bound. Formally, non-targeted adversarial
examples are constructed as

x(cid:63) ← clip(x + Bsgn(∇x(cid:96)(1y, Jθ(x))))
Here, clip(x) is used to clip each dimension of x to the range of pixel values, i.e., [0, 255] in this
work. We make a slight variation to choose (cid:96)(u, v) = log (1 − u · v), which is the same as used in
the optimization-based approach.

3

Published as a conference paper at ICLR 2017

Fast gradient (FG). The fast gradient approach (FG) is similar to FGS, but instead of moving
along the gradient sign direction, FG moves along the gradient direction. In particular, we have

x(cid:63) ← clip(x + B

∇x(cid:96)(1y, Jθ(x))
||∇x(cid:96)(1y, Jθ(x))||

))

Here, we assume the distance metric in constraint (2), d(x, x(cid:63)) = ||x − x(cid:63)|| is a norm of x − x(cid:63).
The term sgn(∇x(cid:96)) in FGS is replaced by ∇x(cid:96)

||∇x(cid:96)|| to meet this distance constraint.

We call both FGS and FG fast gradient-based approaches.

2.2.2 APPROACHES FOR GENERATING TARGETED ADVERSARIAL EXAMPLES

A targeted adversarial image x(cid:63) is similar to a non-targeted one, but constraint (1) is replaced by
fθ(x(cid:63)) = y(cid:63)

(4)
where y(cid:63) is the target label given by the adversary. For the optimization-based approach, we ap-
proximate the solution by solving the following dual objective:

argminx(cid:63) λd(x, x(cid:63)) + (cid:96)(cid:48)(1y(cid:63) , Jθ(x(cid:63)))

(5)

In this work, we choose the standard cross entropy loss (cid:96)(cid:48)(u, v) = − (cid:80)
i

ui log vi.

For FGS and FG, we construct adversarial examples as follows:

x(cid:63) ← clip(x − Bsgn(∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))))

(FGS)

x(cid:63) ← clip(x − B

∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))
||∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))||

)

(FG)

where (cid:96)(cid:48) is the same as the one used for the optimization-based approach.

2.3 EVALUATION METHODOLOGY

For the rest of the paper, we focus on examining the transferability among state-of-the-art models
trained over ImageNet (Russakovsky et al. (2015)).
In this section, we detail the models to be
examined, the dataset to be evaluated, and the measurements to be used.

Models. We examine ﬁve networks, ResNet-50, ResNet-101, ResNet-152 (He et al. (2015))1,
GoogLeNet (Szegedy et al. (2014))2, and VGG-16 (Simonyan & Zisserman (2014))3. We retrieve
the pre-trained models for each network online. The performance of these models on the ILSVRC
2012 (Russakovsky et al. (2015)) validation set are presented in the appendix (Table 7). We choose
these models to study the transferability between homogeneous architectures (i.e., ResNet models)
and heterogeneous architectures.

Dataset.
It is less meaningful to examine the transferability of an adversarial image between two
models which cannot classify the original image correctly. Therefore, from the ILSVRC 2012 val-
idation set, we randomly choose 100 images, which can be classiﬁed correctly by all ﬁve models
in our examination. These 100 images form our test set. To perform targeted attacks, we manually
choose a target label for each image, so that its semantics is far from the ground truth. The images
and target labels in our evaluation can be found on website4.

Measuring transferability. Given two models, we measure the non-targeted transferability by
computing the percentage of the adversarial examples generated for one model that can be classiﬁed
correctly for the other. We refer to this percentage as accuracy. A lower accuracy means better
non-targeted transferability. We measure the targeted transferability by computing the percentage of
the adversarial examples generated for one model that are classiﬁed as the target label by the other
model. We refer to this percentage as matching rate. A higher matching rate means better targeted
transferability. For clarity, the reported results are only based on top-1 accuracy. Top-5 accuracy’s
counterparts can be found in the appendix.

1https://github.com/KaimingHe/deep-residual-networks
2https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet
3https://gist.github.com/ksimonyan/211839e770f7b538e2d8
4https://github.com/sunblaze-ucb/transferability-advdnn-pub

4

Published as a conference paper at ICLR 2017

Distortion. Besides transferability, another important factor is the distortion between adversarial
images and the original ones. We measure the distortion by root mean square deviation, i.e., RMSD,
which is computed as d(x(cid:63), x) = (cid:112)(cid:80)
i − xi)2/N , where x(cid:63) and x are the vector representations
of an adversarial image and the original one respectively, N is the dimensionality of x and x(cid:63), and
xi denotes the pixel value of the i-th dimension of x, within range [0, 255], and similar for x(cid:63)
i .

i(x(cid:63)

3 NON-TARGETED ADVERSARIAL EXAMPLES

In this section, we examine different approaches for generating non-targeted adversarial images.

3.1 OPTIMIZATION-BASED APPROACH

To apply the optimization-based approach for a single model, we initialize x(cid:63) to be x and use Adam
Optimizer (Kingma & Ba (2014)) to optimize Objective (3) . We ﬁnd that we can tune the RMSD
by adjusting the learning rate of Adam and λ. We ﬁnd that, for each model, we can use a small
learning rate to generate adversarial images with small RMSD, i.e. < 2, with any λ. In fact, we ﬁnd
that when initializing x(cid:63) with x, Adam Optimizer will search for an adversarial example around x,
even when we set λ to be 0, i.e., not restricting the distance between x(cid:63) and x. Therefore, we set
λ to be 0 for all experiments using optimization-based approaches throughout the paper. Although
these adversarial examples with small distortions can successfully fool the target model, however,
they cannot transfer well to other models (see Table 15 and 16 in the appendix for details).

We increase the learning rate to allow the optimization algorithm to search for adversarial images
with larger distortion. In particular, we set the learning rate to be 4. We run Adam Optimizer for 100
iterations to generate the adversarial images. We observe that the loss converges after 100 iterations.
An alternative optimization-based approach leading to similar results can be found in the appendix.

Non-targeted adversarial examples transfer. We generate non-targeted adversarial examples on
one network, but evaluate them on another, and Table 1 Panel A presents the results. From the table,
we can observe that

• The diagonal contains all 0 values. This says that all adversarial images generated for one

model can mislead the same model.

• A large proportion of non-targeted adversarial images generated for one model using the

optimization-based approach can transfer to another.

• Although the three ResNet models share similar architectures which differ only in the hy-
perparameters, adversarial examples generated against a ResNet model do not necessarily
transfer to another ResNet model better than other non-ResNet models. For example, the
adversarial examples generated for VGG-16 have lower accuracy on ResNet-50 than those
generated for ResNet-152 or ResNet-101.

3.2 FAST GRADIENT-BASED APPROACHES

We then examine the effectiveness of fast gradient-based approaches. A good property of fast
gradient-based approaches is that all generated adversarial examples lie in a 1-D subspace. There-
fore, we can easily approximate the minimal distortion in this subspace of transferable adversarial
examples between two models. In the following, we ﬁrst control the RMSD to study fast gradient-
based approaches’ effectiveness. Second, we study the transferable minimal distortions of fast
gradient-based approaches.

3.2.1 EFFECTIVENESS AND TRANSFERABILITY OF THE FAST GRADIENT-BASED

APPROACHES

Since the distortion B and the RMSD of the generated adversarial images are highly correlated, we
can choose this hyperparameter B to generate adversarial images with a given RMSD. In Table 1
Panel B, we generate adversarial images using FG such that the average RMSD is almost the same
as those generated using the optimization-based approach. We observe that the diagonal values in

5

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
22.83
23.81
22.86
22.51
22.58

ResNet-101
13%
0%
20%
17%
38%
Panel A: Optimization-based approach

ResNet-50 VGG-16 GoogLeNet
19%
21%
21%
0%
19%

0%
19%
23%
22%
39%

18%
21%
0%
17%
34%

11%
12%
18%
5%
0%

RMSD
23.45
23.49
23.49
23.73
23.45

ResNet-152
ResNet-101
4%
13%
13%
19%
4%
11%
25%
19%
5%
20%
16%
15%
17%
25%
25%
Panel B: Fast gradient approach

ResNet-50 VGG-16 GoogLeNet
20%
23%
25%
1%
19%

12%
13%
14%
7%
1%

Table 1: Transferability of non-targeted adversarial images generated between pairs of models. The
ﬁrst column indicates the average RMSD of all adversarial images generated for the model in the
corresponding row. The cell (i, j) indicates the accuracy of the adversarial images generated for
model i (row) evaluated over model j (column). Results of top-5 accuracy can be found in the
appendix (Table 8 and Table 10).

the table are all positive, which means that FG cannot fully mislead the models. A potential reason
is that, FG can be viewed as approximating the optimization, but is tailored for speed over accuracy.

On the other hand, the values of non-diagonal cells in the table, which correspond to the accuracies
of adversarial images generated for one model but evaluated on another, are comparable with or less
than their counterparts in the optimization-based approach. This shows that non-targeted adversarial
examples generated by FG exhibit transferability as well.

We also evaluate FGS, but the transferability of the generated images is worse than the ones gener-
ated using either FG or optimization-based approaches. The results are in the appendix (Table 19
and 20). It shows that when RMSD is around 23, the accuracies of the adversarial images generated
by FGS is greater than their counterparts for FG. We hypothesize the reason why transferability of
FGS is worse to this fact.

3.2.2 ADVERSARIAL IMAGES WITH MINIMAL TRANSFERABLE RMSD

For an image x and two models M1, M2, we can approximate the minimal distortion B along a
direction δ, such that xB = x + Bδ generated for M1 is adversarial for both M1 and M2. Here δ is
the direction, i.e., sgn(∇x(cid:96)) for FGS, and ∇x(cid:96)/||∇x(cid:96)|| for FG.

We refer to the minimal transferable RMSD from M1 to M2 using FG (or FGS) as the RMSD of
a transferable adversarial example xB with the minimal transferable distortion B from M1 to M2
using FG (or FGS). The minimal transferable RMSD can illustrate the tradeoff between distortion
and transferability.

In the following, we approximate the minimal transferable RMSD through a linear search by sam-
pling B every 0.1 step. We choose the linear-search method rather than binary-search method to de-
termine the minimal transferable RMSD because the adversarial images generated from an original
image may come from multiple intervals. The experiment can be found in the appendix (Figure 6).

Minimal transferable RMSD using FG and FGS. Figure 1 plots the cumulative distribution
function (CDF) of the minimal transferable RMSD from VGG-16 to ResNet-152 using non-targeted
FG (Figure 1a) and FGS (Figure 1b). From the ﬁgures, we observe that both FG and FGS can ﬁnd
100% transferable adversarial images with RMSD less than 80.91 and 86.56 respectively. Further,
the FG method can generate transferable attacks with smaller RMSD than FGS. A potential rea-
son is that while FGS minimizes the distortion’s L∞ norm, FG minimizes its L2 norm, which is
proportional to RMSD.

6

Published as a conference paper at ICLR 2017

(a) Fast Gradient

(b) Fast Gradient Sign

Figure 1: The CDF of the minimal transferable RMSD from VGG-16 to ResNet-152 using FG (a)
and FGS (b). The green line labels the median minimal transferable RMSD, while the red line labels
the minimal transferable RMSD to reach 90% percentage.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.13
23.16
23.06
23.59
22.87

100%
3%
4%
2%
1%

ResNet-101
2%
100%
2%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
2%
1%
100%
1%

1%
1%
1%
1%
100%

1%
3%
100%
2%
0%

Table 2: The matching rate of targeted adversarial images generated using the optimization-based
approach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) indicates that matching rate of the targeted adversarial images generated for model i (row)
when evaluated on model j (column). The top-5 results can be found in the appendix (Table 12).

3.3 COMPARISON WITH RANDOM PERTURBATIONS

We also evaluate the test accuracy when we add a Gaussian noise to the 100 images in our test
set. The concrete results can be found in the appendix, and we show the conclusion that the “trans-
ferability” of this approach is signiﬁcantly worse than either optimization-based approaches or fast
gradient-based approaches.

4 TARGETED ADVERSARIAL EXAMPLES

In this section, we examine the transferability of targeted adversarial images. Table 2 presents
the results for using optimization-based approach. We observe that (1) the prediction of targeted
adversarial images can match the target labels when evaluated on the same model that is used to
generate the adversarial examples; but (2) the targeted adversarial images can be rarely predicted
as the target labels by a different model. We call the latter that the target labels do not transfer.
Even when we increase the distortion, we still do not observe improvements on making target label
transfer. Some results can be found in the appendix (Table 17). Even if we compute the matching
rate based on top-5 accuracy, the highest matching rate is only 10%. The results can be found in the
appendix (Table 18).

We also examine the targeted adversarial images generated by fast gradient-based approaches, and
we observe that the target labels do not transfer as well. The results are deferred to the appendix
(Table 25). In fact, most targeted adversarial images cannot mislead the model, for which the ad-
versarial images are generated, to predict the target labels, regardless of how large the distortion is
used. We attribute it to the fact that the fast gradient-based approaches only search for attacks in
a 1-D subspace. In this subspace, the total possible predictions may contain a small subset of all
labels, which usually does not contain the target label. In Section 6, we study decision boundaries
regarding this issue.

We also evaluate the matching rate of images added with Gaussian noise, as described in Section 3.3.
However, we observe that the matching rate of any of the 5 models is 0%. Therefore, we conclude

7

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.68
30.76
30.26
31.13
29.70

38%
75%
84%
74%
90%

ResNet-101
76%
43%
81%
78%
87%

ResNet-50 VGG-16 GoogLeNet
97%
98%
99%
24%
99%

70%
69%
46%
68%
83%

76%
73%
77%
63%
11%

Table 3: The matching rate of targeted adversarial images generated using the optimization-based
approach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) indicates that percentage of the targeted adversarial images generated for the ensemble of the
four models except model i (row) is predicted as the target label by model j (column). In each row,
the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Results of top-5 matching rate can be found in the appendix (Table 13).

that by adding Gaussian noise, the attacker cannot generate successful targeted adversarial examples
at all, let alone targeted transferability.

5 ENSEMBLE-BASED APPROACHES

We hypothesize that if an adversarial image remains adversarial for multiple models, then it is more
likely to transfer to other models as well. We develop techniques to generate adversarial images for
multiple models. The basic idea is to generate adversarial images for the ensemble of the models.
Formally, given k white-box models with softmax outputs being J1, ..., Jk, an original image x,
and its ground truth y, the ensemble-based approach solves the following optimization problem (for
targeted attack):

argminx(cid:63) − log (cid:0)(

αiJi(x(cid:63))) · 1y(cid:63)

(cid:1) + λd(x, x(cid:63))

(6)

k
(cid:88)

i=1

where y(cid:63) is the target label speciﬁed by the adversary, (cid:80) αiJi(x(cid:63)) is the ensemble model, and αi
are the ensemble weights, (cid:80)k
i=1 αi = 1. Note that (6) is the targeted objective. The non-targeted
counterpart can be derived similarly. In doing so, we hope the generated adversarial images remain
adversarial for an additional black-box model Jk+1.

We evaluate the effectiveness of the ensemble-based approach. For each of the ﬁve models, we treat
it as the black-box model to attack, and generate adversarial images for the ensemble of the rest
four, which is considered as white-box. We evaluate the generated adversarial images over all ﬁve
models. Throughout the rest of the paper, we refer to the approaches evaluated in Section 3 and 4 as
the approaches using a single model, and to the ensemble-based approaches discussed in this section
as the approaches using an ensemble model.

Optimization-based approach. We use Adam to optimize the objective (6) with equal ensemble
weights across all models in the ensemble to generate targeted adversarial examples. In particular,
we set the learning rate of Adam to be 8 for each model. In each iteration, we compute the Adam
update for each model, sum up the four updates, and add the aggregation onto the image. We run 100
iterations of updates, and we observe that the loss converges after 100 iterations. By doing so, for the
ﬁrst time, we observe a large proportion of the targeted adversarial images whose target labels can
transfer. The results are presented in Table 3. We observe that not all targeted adversarial images
can be misclassiﬁed to the target labels by the models used in the ensemble. This suggests that
while searching for an adversarial example for the ensemble model, there is no direct supervision to
mislead any individual model in the ensemble to predict the target label. Further, from the diagonal
numbers of the table, we observe that the transferability to ResNet models is better than to VGG-16
or GoogLeNet, when adversarial examples are generated against all models except the target model.

We also evaluate non-targeted adversarial images generated by the ensemble-based approach. We
observe that the generated adversarial images have almost perfect transferability. We use the same
procedure as for the targeted version, except the objective to generate the adversarial images. We
evaluate the generated adversarial images over all models. The results are presented in Table 4.
The generated adversarial images all have RMSDs around 17, which are lower than 22 to 23 of

8

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.17
17.25
17.25
17.80
17.41

0%
0%
0%
0%
0%

ResNet-101
0%
1%
0%
0%
0%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
6%
0%

0%
0%
2%
0%
0%

0%
0%
0%
0%
5%

Table 4: Accuracy of non-targeted adversarial images generated using the optimization-based ap-
proach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) corresponds to the accuracy of the attack generated using four models except model i (row)
when evaluated over model j (column). In each row, the minus sign “−” indicates that the model
of the row is not used when generating the attacks. Results of top-5 accuracy can be found in the
appendix (Table 14).

Model
# of labels

VGG-16 ResNet-50 ResNet-101 ResNet-152 GoogLeNet

10

9

21

10

21

Table 5: The number of all possible predicted labels for each model in the same plane described in Figure 3.

the optimization-based approach using a single model (See Table 1 for comparison). When the
adversarial images are evaluated over models which are not used to generate the attack, the accuracy
is no greater than 6%. For a reference, the corresponding accuracies for all approaches evaluated in
Section 3 using one single model are at least 12%. Our experiments demonstrate that the ensemble-
based approaches can generate almost perfectly transferable adversarial images.

Fast gradient-based approach. The results for non-targeted fast gradient-based approaches ap-
plied to the ensemble can be found in the appendix (Table 21, 22, 23 and 24). We observe that the
diagonal values are not zero, which is the same as we observed in the results for FG and FGS applied
to a single model. We hypothesize a potential reason is that the gradient directions of different mod-
els in the ensemble are orthogonal to each other, as we will illustrate in Section 6. In this case, the
gradient direction of the ensemble is almost orthogonal to the one of each model in the ensemble.
Therefore searching along this direction may require large distortion to reach adversarial examples.

For targeted adversarial examples generated using FG and FGS based on an ensemble model, their
transferability is no better than the ones generated using a single model. The results can be found in
the appendix (Table 28, 29, 30 and 31). We hypothesize the same reason to explain this: there are
only few possible target labels in total in the 1-D subspace.

6 GEOMETRIC PROPERTIES OF DIFFERENT MODELS

In this section, we show some geometric properties of the models to try to better understand transfer-
able adversarial examples. Prior works also try to understand the geometic properties of adversarial
examples theoretically (Fawzi et al. (2016)) or empirically (Goodfellow et al. (2014)). In this work,
we examine large models trained over a large dataset with 1000 labels, whose geometric properties
are never examined before. This allows us to make new observations to better understand the models
and their adversarial examples.

The gradient directions of different models in our evaluation are almost orthogonal to each
other. We study whether the adversarial directions of different models align with each other. We
calculate cosine value of the angle between gradient directions of different models, and the results
can be found in the appendix (Table 33). We observe that all non-diagonal values are close to 0,
which indicates that for most images, their gradient directions with respect to different models are
orthogonal to each other.

Decision boundaries of the non-targeted approaches using a single model. We study the deci-
sion boundary of different models to understand why adversarial examples transfer. We choose two
normalized orthogonal directions δ1, δ2, one being the gradient direction of VGG-16 and the other
being randomly chosen. Each point (u, v) in this 2-D plane corresponds to the image x + uδ1 + vδ2,

9

Published as a conference paper at ICLR 2017

Figure 2: The example image to study the decision boundary. Its ID in ILSVRC 2012 validation set
is 49443, and its ground truth label is “anemone ﬁsh.”

VGG-16

ResNet-50

ResNet-101

ResNet-152

GoogLeNet

n
i
-

m
o
o
Z

t
u
o
-
m
o
o
Z

Figure 3: Decision regions of different models. We pick the same two directions for all plots: one is
the gradient direction of VGG-16 (x-axis), and the other is a random orthogonal direction (y-axis).
Each point in the span plane shows the predicted label of the image generated by adding a noise to
the original image (e.g., the origin corresponds to the predicted label of the original image). The
units of both axises are 1 pixel values. All sub-ﬁgure plots the regions on the span plane using the
same color for the same label. The image is in Figure 2.

where x is the pixel value vector of the original image. For each model, we plot the label of the
image corresponding to each point, and get Figure 3 using the image in Figure 2.

We can observe that for all models, the region that each model can predict the image correctly
is limited to the central area. Also, along the gradient direction, the classiﬁers are soon misled.
One interesting ﬁnding is that along this gradient direction, the ﬁrst misclassiﬁed label for the three
ResNet models (corresponding to the light green region) is the label “orange”. A more detailed study
can be found in the appendix (Table 9, Table 26 and 27). When we look at the zoom-out ﬁgures,
however, the labels of images that are far away from the original one are different for different
models, even among ResNet models.

On the other hand, in Table 5, we show the total number of regions in each plane. In fact, for each
plane, there are at most 21 different regions in all planes. Compared with the 1,000 total categories
in ImageNet, this is only 2.1% of all categories. That means, for all other 97.9% labels, no targeted
adversarial example exists in each plane. Such a phenomenon partially explains why fast gradient-
based approaches can hardly ﬁnd targeted adversarial images.

Further, in Figure 4, we draw the decision boundaries of all models on the same plane as described
above. We can observe that

• The boundaries align with each other very well. This partially explains why non-targeted

adversarial images can transfer among models.

10

Published as a conference paper at ICLR 2017

Figure 4: The decision boundary to sep-
arate the region within which all points
are classiﬁed as the ground truth label
(encircled by each closed curve) from
others. The plane is the same one de-
scribed in Figure 3.
The origin of
the coordinate plane corresponds to the
original image. The units of both axises
are 1 pixel values.

Figure 5: The decision boundary to separate the
region within which all points are classiﬁed as the
target label (encircled by each closed curve) from
others. The plane is spanned by the targeted ad-
versarial direction and a random orthogonal di-
rection. The targeted adversarial direction is com-
puted as the difference between the original image
in Figure 2 and the adversarial image generated by
the optimization-based approach for an ensemble.
The ensemble contains all models except ResNet-
101. The origin of the coordinate plane corre-
sponds to the original image. The units of both
axises are 1 pixel values.

• The boundary diameters along the gradient direction is less than the ones along the ran-
dom direction. A potential reason is that moving a variable along its gradient direction
can change the loss function (i.e., the probability of the ground truth label) signiﬁcantly.
Therefore along the gradient direction it will take fewer steps to move out of the ground
truth region than a random direction.

• An interesting ﬁnding is that even though we move left along the x-axis, which is equivalent
to maximizing the ground truth’s prediction probability, it also reaches the boundary much
sooner than moving along a random direction. We attribute this to the non-linearity of the
loss function: when the distortion is larger, the gradient direction also changes dramatically.
In this case, moving along the original gradient direction no longer increases the probability
to predict the ground truth label (see Figure 7 in the appendix).

• As for VGG-16 model, there is a small hole within the region corresponding to the ground
truth. This may partially explain why non-targeted adversarial images with small distortion
exist, but do not transfer well. This hole does not exist in other models’ decision planes. In
this case, non-targeted adversarial images in this hole do not transfer.

Decision boundaries of the targeted ensemble-based approaches.
In addition, we choose the
targeted adversarial direction of the ensemble of all models except ResNet-101 and a random or-
thogonal direction, and we plot decision boundaries on the plane spanned by these two direction
vectors in Figure 5. We observe that the regions of images, which are predicted as the target label,
align well for the four models in the ensemble. However, for the model not used to generate the
adversarial image, i.e., ResNet-101, it also has a non-empty region such that the prediction is suc-
cessfully misled to the target label, although the area is much smaller. Meanwhile, the region within
each closed curve of the models almost has the same center.

7 REAL WORLD EXAMPLE: ADVERSARIAL EXAMPLES FOR CLARIFAI.COM

Clarifai.com is a commercial company providing state-of-the-art image classiﬁcation services. We
have no knowledge about the dataset and types of models used behind Clarifai.com, except that we
have black-box access to the services. The labels returned from Clarifai.com are also different from

11

Published as a conference paper at ICLR 2017

the categories in ILSVRC 2012. We submit all 100 original images to Clarifai.com and the returned
labels are correct based on a subjective measure.

We also submit 400 adversarial images in total, where 200 of them are targeted adversarial examples,
and the rest 200 are non-targeted ones. As for the 200 targeted adversarial images, 100 of them
are generated using the optimization-based approach based on VGG-16 (the same ones evaluated
in Table 2), and the rest 100 are generated using the optimization-based approach based on an
ensemble of all models except ResNet-152 (the same ones evaluated in Table 3). The 200 non-
targeted adversarial examples are generated similarly (the same ones evaluated in Table 1 and 4).

For non-targeted adversarial examples, we observe that for both the ones generated using VGG-16
and those generated using the ensemble, most of them can transfer to Clarifai.com.

More importantly, a large proportion of our targeted adversarial examples are misclassiﬁed by Clari-
fai.com as well. We observe that 57% of the targeted adversarial examples generated using VGG-16,
and 76% of the ones generated using the ensemble can mislead Clarifai.com to predict labels irrele-
vant to the ground truth.

Further, our experiment shows that for targeted adversarial examples, 18% of those generated us-
ing the ensemble model can be predicted as labels close to the target label by Clarifai.com. The
corresponding number for the targeted adversarial examples generated using VGG-16 is 2%. Con-
sidering that in the case of attacking Clarifai.com, the labels given by the target model are different
from those given by our models, it is fairly surprising to see that when using the ensemble-based
approach, there is still a considerable proportion of our targeted adversarial examples that can mis-
lead this black-box model to make predictions semantically similar to our target labels. All these
numbers are computed based on a subjective measure, and we include some examples in Table 6.
More examples can be found in the appendix (Table 34).

original
image

true
label

Clarifai.com
results of
original image

target
label

targeted
adversarial
example

Clarifai.com results
of targeted
adversarial example

window,
wall,
old,
decoration,
design

Buddha,
gold,
temple,
celebration,
artistic

cherry,
branch,
fruit,
food,
season

sea seal,
ocean,
head,
sea,
cute

viaduct

hip, rose
hip,
rosehip

bridge,
sight,
arch,
river,
sky

fruit,
fall,
food,
little,
wildlife

dogsled,
dog
sled,
dog
sleigh

group together,
four,
sledge,
sled,
enjoyment

pug,
pug-dog

pug,
friendship,
adorable,
purebred,
sit

window
screen

stupa,
tope

hip, rose
hip,
rosehip

sea lion

12

Published as a conference paper at ICLR 2017

Old
English
sheep-
dog,
bobtail

maillot,
tank suit

patas,
hussar
monkey,
Erythro-
cebus
patas

poodle,
retriever,
loyalty,
sit,
two

beach,
woman,
adult,
wear,
portrait

primate,
monkey,
safari,
sit,
looking

abaya

amphib-
ian,
amphibi-
ous
vehicle

bee eater

veil,
spirituality,
religion,
people,
illustration

transportation
system,
vehicle,
man,
print,
retro

ornithology,
avian,
beak,
wing,
feather

Table 6: Original images and adversarial images evaluated over Clarifai.com. For labels returned
from Clarifai.com, we sort the labels ﬁrstly by rareness: how many times a label appears in the
Clarifai.com results for all adversarial images and original images, and secondly by conﬁdence.
Only top 5 labels are provided.

8 CONCLUSION

In this work, we are the ﬁrst to conduct an extensive study of the transferability of both non-targeted
and targeted adversarial examples generated using different approaches over large models and a
large scale dataset. Our results conﬁrm that the transferability for non-targeted adversarial exam-
ples are prominent even for large models and a large scale dataset. On the other hand, we ﬁnd that
it is hard to use existing approaches to generate targeted adversarial examples whose target labels
can transfer. We develop novel ensemble-based approaches, and demonstrate that they can gen-
erate transferable targeted adversarial examples with a high success rate. Meanwhile, these new
approaches exhibit better performance on generating non-targeted transferable adversarial examples
than previous work. We also show that both non-targeted and targeted adversarial examples gen-
erated using our new approaches can successfully attack Clarifai.com, which is a black-box image
classiﬁcation system. Furthermore, we study some geometric properties to better understand the
transferable adversarial examples.

This material is in part based upon work supported by the National Science Foundation under Grant
No. TWC-1409915. Any opinions, ﬁndings, and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily reﬂect the views of the National Science
Foundation.

ACKNOWLEDGMENTS

REFERENCES

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. arXiv

preprint arXiv:1608.04644, 2016.

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classiﬁers:
from adversarial to random noise. In Advances in Neural Information Processing Systems, pp.
1624–1632, 2016.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014.

13

Published as a conference paper at ICLR 2017

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-

nition. arXiv preprint arXiv:1512.03385, 2015.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,

abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. arXiv preprint arXiv:1610.08401, 2016.

Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against deep learning systems using adversarial examples.
arXiv preprint arXiv:1602.02697, 2016b.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.

J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine
learning algorithms for trafﬁc sign recognition. Neural Networks, (0):–, 2012. ISSN 0893-6080.
doi: 10.1016/j.neunet.2012.02.016. URL http://www.sciencedirect.com/science/
article/pii/S0893608012000457.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.

14

Published as a conference paper at ICLR 2017

APPENDIX

Top-1 accuracy
Top-5 accuracy

ResNet-50
72.5%
91.0%

ResNet-101
73.8%
91.7%

ResNet-152 GoogLeNet VGG-16
68.3%
88.3%

74.6%
92.1%

68.7%
89.0%

Table 7: Top-1 and top-5 accuracy of the studied models over the ILSVRC 2012 validation dataset.

An alternative optimization-based approach to generate adversarial examples. An alternative
method to generate non-targeted adversarial examples with large distortion is to revise the optimiza-
tion objective to incorporate this distortion constraint. For example, for non-targeted adversarial
image searching, we can optimize for the following objective.

argminx(cid:63) − log (1 − 1y · Jθ(x(cid:63))) + λ1ReLU(τ − d(x, x(cid:63))) + λ2ReLU(d(x, x(cid:63)) − τ )

Optimizing for
(1) minimizing
− log (1 − 1y · Jθ(x(cid:63))); (2) Penalizing the solution if d(x, x(cid:63)) is no more than a threshold τ (too
low); and (3) Penalizing the solution if d(x, x(cid:63)) is too high.

the following three effects:

the above objective has

In our preliminary evaluation, we found that the solutions computed from the two approaches have
similar transferability. We thus omit the results for this alternative approach.

Transferable non-targeted adversarial images are classiﬁed as the same wrong labels. Previ-
ous work Goodfellow et al. (2014) reported the phenomenon that when evaluating the adversarial
images, different models tend to make the same wrong predictions. This conclusion was only exam-
ined over datasets with 10 categories. In our evaluation, however, we observe the same phenomenon,
albeit we have 1000 possible categories. We refer to this effect as the same mistake effect.

Table 9 presents the results based on the adversarial examples generated for VGG-16. For each pair
of models, among all adversarial examples that both models make wrong predictions, we compute
the percentage that both models make the same mistake. These percentage numbers are from 12% to
40%, which are surprisingly high, since there are 999 possible categories to be misclassiﬁed into (see
Table 32 in the appendix for the wrong predicted label distribution of these adversarial examples).
Later in Section 6, we try to explain this phenomenon using decision boundaries.

Adversarial images may come from multiple intervals along the gradient direction.
In Fig-
ure 6, we show that along the gradient direction of the non-targeted objective (3) for VGG-16, when
evaluating the adversarial image xB on ResNet-152, xB will soon become adversarial for small B.
While B increases, however, the prediction changes back to be the correct label, then a wrong label,
then the correct label again, and ﬁnally wrong labels. This indicates that along the gradient of this
image, the distortions that can cause the corresponding adversarial images to mislead ResNet-152
form four intervals.

Comparison with random perturbations. For comparison, we evaluate the test accuracy when
we add a Gaussian noise to the 100 images in our test set. We vary the standard deviation of the
Gaussian noise from 5 to 40 with a step size of 5. For each speciﬁc standard deviation, we generate

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
22.83
23.81
22.86
22.51
22.58

7%
40%
48%
36%
66%

ResNet-101
43%
6%
44%
33%
71%

ResNet-50 VGG-16 GoogLeNet
39%
42%
42%
0%
49%

43%
41%
3%
33%
62%

31%
34%
32%
15%
2%

Table 8: Top-5 accuracy of Table 1 Panel A. Transferability between pairs of models using non-
targeted optimization-based approach with a learning rate of 4. The ﬁrst column indicates the av-
erage RMSD of all adversarial images generated for the model in the corresponding row. The cell
(i, j) indicates the top-5 accuracy of the adversarial images generated for model i (row) evaluated
over model j (column). Lower value indicates better transferability.

15

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
28.57%
29.87%
19.23%
12.82%

ResNet-101
−
100.00%
40.00%
18.07%
20.48%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
22.89% 100.00%
16.84%
18.07%

GoogLeNet
−
−
−
−
100.00%

Table 9: When using the optimization-based approach for VGG-16 model to generate non-targeted adversarial
images, cell (i, j) is the percentage of all transferable adversarial images that are predicted as the same wrong
labels by both models i and j over all adversarial images that are misclassiﬁed by both models i and j. Notice
that this table is symmetric.

(a) The original image

(b) Prediction correctness

Figure 6: We add Bδ onto the original image in Figure 6a, where δ is the normalized gradient of
the non-targeted objective (3) for VGG-16. When xB is evaluated on ResNet-152, we plot 1 if the
prediction matches the ground truth, or 0 otherwise.

100 random noises and add them to each image, resulting in 10,000 noisy images in total. Then
we evaluate the accuracy of each model on these 10,000 images, and the results are presented in
Table 11. Notice that when setting standard deviation to be 25, the average RMSD is 23.59, which
is comparable to that of non-targeted adversarial examples generated by either optimization-based
approaches or fast gradient-based approaches. However, each model can still achieve an accuracy
more than 66%. This shows that adding random noise is not an effective way to generate adversarial
examples, hence the “transferability” of this approach is signiﬁcantly worse than either optimization-
based approaches or fast gradient-based approaches.

More examples submitted to Clarifai.com. We present more results from Clarifai.com by sub-
mitting original and adversarial examples in Table 34.

original
image

true
label

Clarifai.com
results of
original image

target
label

targeted
adv-example

Clarifai.com result
of targeted
adversarial example

broom

jacamar

dust,
brick,
rustic,
stone,
dirty

frost,
sparrow,
pigeon,
ice,
frosty

heavy,
bulldozer,
exert,
track,
plow

junco,
snow-
bird

har-
vester,
reaper

eel

prairie
chicken,
prairie
grouse,
prairie
fowl

16

feather,
beautiful,
bird,
leaf,
ﬂora

swimming,
underwater,
ﬁsh,
water,
one

wildlife,
animal,
illustration,
nature,
color

Published as a conference paper at ICLR 2017

broccoli

hamster

mon-
goose

holster

yurt

water
buffalo,
water
ox,
Asiatic
buffalo,
Bubalus
bubalis

zebra

French
bulldog

curly-
coated
retriever

fox,
rodent,
predator,
fur,
park

pistol,
force,
bullet,
protection,
cartridge

wooden,
scenic,
snow,
rural,
landscape

kind,
cauliﬂower,
vitamin,
carrot,
cabbage

herd,
milk,
beef cattle,
farmland,
cow

equid,
stripe,
savanna,
zebra,
safari

bulldog,
studio,
boxer,
eye,
bull

eye,
looking,
pet,
canine,
dog

maillot

ground
beetle,
carabid
beetle

comic
book

rugby ball

apiary,
bee house

kite

ﬂy

17

motley,
shape,
horizontal,
abstract,
bright

shell,
shell (food),
antenna,
insect,
invertebrate

grafﬁti,
religion,
people,
painting,
culture

bird,
bright,
texture,
animal,
decoration

pastime,
print,
illustration,
art

wood,
people,
outdoors,
nature

visuals,
feather,
wing,
pet,
print

graphic,
shape,
insect,
artistic,
image

Table 34: Original images and adversarial images evaluated over Clarifai.com. For labels returned
from Clarifai.com, we sort the labels ﬁrstly by the occurrence of a label from the Clarifai.com results,
and secondly by conﬁdence. Only top 5 labels are provided.

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.45
23.49
23.49
23.73
23.45

23%
35%
40%
40%
53%

ResNet-101
29%
20%
39%
35%
46%

ResNet-50 VGG-16 GoogLeNet
39%
43%
39%
8%
38%

31%
33%
18%
33%
37%

33%
28%
33%
19%
7%

Table 10: Top-5 accuracy of Table 1 Panel B. Transferability between pairs of models using non-
targeted FG. The ﬁrst column indicates the average RMSD of all adversarial images generated for
the model in the corresponding row. The cell (i, j) indicates the top-5 accuracy of the adversarial
images generated for model i (row) evaluated over model j (column). Lower value indicates better
transferability.

Standard Deviation
5
10
15
20
25
30
35
40

RMSD ResNet-152
4.91
9.72
14.44
19.07
23.59
28.01
32.32
36.52

97.41%
95.53%
91.19%
86.56%
83.10%
78.95%
73.60%
66.53%

ResNet-101
98.96%
96.72%
94.22%
90.38%
85.53%
79.04%
70.89%
63.09%

ResNet-50 VGG-16 GoogLeNet
98.74% 97.47%
96.81% 92.13%
92.16% 87.86%
84.07% 82.30%
78.33% 73.57%
71.66% 65.33%
62.03% 58.55%
50.96% 51.85%

99.29%
95.94%
88.50%
77.84%
66.84%
54.93%
45.13%
35.61%

Table 11: Accuracy of images with random perturbation. The ﬁrst column reports the standard
deviation of the Gaussian noise added to each image. The second column reports the average RMSD
over all generated images with the respective standard deviation. For each of the rest column j, the
cell (i, j) reports model j’s accuracy of the noisy images when a Gaussian noise with the respective
standard deviation speciﬁed in row i is added to each image.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.13
23.16
23.06
23.59
22.87

100%
9%
10%
3%
1%

ResNet-101
11%
100%
9%
5%
2%

ResNet-50 VGG-16 GoogLeNet
3%
2%
2%
100%
3%

1%
1%
3%
4%
100%

5%
7%
100%
5%
1%

Table 12: Top-5 matching rate of Table 2. The adversarial images are generated using the targeted
optimization-based approach with a learning rate of 4. The ﬁrst column indicates the average RMSD
of all adversarial images generated for the model in the corresponding row. Cell (i, j) indicates that
top-5 matching rate of the targeted adversarial images generated for model i (row) when evaluated
on model j (column). Higher value indicates more successful transferable target labels.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.68
30.76
30.26
31.13
29.70

87%
100%
99%
100%
100%

ResNet-101
100%
88%
99%
100%
100%

ResNet-50 VGG-16 GoogLeNet
100%
100%
99%
51%
100%

100%
100%
86%
100%
100%

100%
100%
99%
100%
32%

Table 13: The top-5 matching rate of Table 3. Matching rate of adversarial images generated using
targeted optimization-based approach. The ﬁrst column indicates the average RMSD of all adver-
sarial images generated for the model in the corresponding row. Cell (i, j) indicates that top-5
matching rate of the targeted adversarial images generated using the ensemble of the four models
except model i (row) is predicted as the target label by model j (column). In each row, the minus
sign “−” indicates that the model of the row is not used when generating the attacks. Higher value
indicates more successful transferable target labels.

18

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.17
17.25
17.25
17.80
17.41

13%
2%
4%
4%
3%

ResNet-101
4%
11%
5%
7%
2%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
20%
0%

4%
3%
11%
5%
3%

3%
3%
2%
4%
15%

Table 14: Top-5 accuracy of Table 4. The ﬁrst column indicates the average RMSD of all adversarial
images generated for the model in the corresponding row. Cell (i, j) indicates that top-5 accuracy
of the non-targeted adversarial images generated using the ensemble of the four models except
model i (row) when evaluated over model j (column). In each row, the minus sign “−” indicates
that the model of the row is not used when generating the attacks. Lower value indicates better
transferability.

RMSD ResNet-152

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

1.25
1.24
1.21
1.55
1.27

0%
84%
90%
89%
94%

ResNet-101
86%
0%
91%
94%
97%

ResNet-50 VGG-16 GoogLeNet
93%
95%
91%
0%
91%

96%
100%
97%
84%
0%

87%
93%
0%
92%
98%

Table 15: Transferability between pairs of models using non-targeted optimization-based approach
with a learning rate of 0.125. The ﬁrst column indicates the average RMSD of all adversarial images
generated for the model in the corresponding row. The cell (i, j) indicates the top-1 accuracy of the
adversarial images generated for model i (row) evaluated over model j (column). Lower value
indicates better transferability. Results of top-5 accuracy can be found in Table 16.

RMSD ResNet-152

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

1.25
1.24
1.21
1.55
1.27

24%
100%
100%
98%
100%

ResNet-101
100%
22%
99%
100%
100%

ResNet-50 VGG-16 GoogLeNet
99%
100%
100%
15%
100%

100%
100%
100%
100%
18%

100%
99%
25%
100%
100%

Table 16: Top-5 accuracy of Table 15. Transferability between pairs of models using non-targeted
optimization-based approach with a learning rate of 0.125. The ﬁrst column indicates the average
RMSD of all adversarial images generated for the model in the corresponding row. The cell (i, j)
indicates the top-5 accuracy of the adversarial images generated for model i (row) evaluated over
model j (column). Lower value indicates better transferability.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
31.35
31.11
31.32
31.50
30.67

98%
5%
3%
1%
2%

ResNet-101
1%
98%
2%
1%
1%

ResNet-50 VGG-16 GoogLeNet
2%
2%
1%
97%
2%

0%
0%
1%
1%
97%

2%
1%
99%
2%
0%

Table 17: The adversarial images are generated using the targeted optimization-based approach
with a larger learning rate. The ﬁrst column indicates the average RMSD of all adversarial images
generated for the model in the corresponding row. Cell (i, j) indicates that top-1 matching rate of
the targeted adversarial images generated for model i (row) when evaluated on model j (column).
Higher value indicates more successful transferable target labels. We used a larger learning rate to
achieve larger RMSD. Results of top-5 matching rate can be found in Table 18.

19

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
31.35
31.11
31.32
31.50
30.67

98%
10%
6%
4%
3%

ResNet-101
8%
98%
7%
4%
1%

ResNet-50 VGG-16 GoogLeNet
3%
3%
5%
97%
3%

1%
1%
1%
4%
97%

6%
5%
99%
6%
2%

Table 18: The top-5 matching rate of Table 17. The adversarial images are generated using the
targeted optimization-based approach with a larger learning rate. The ﬁrst column indicates the
average RMSD of all adversarial images generated for the model in the corresponding row. Cell
(i, j) indicates that top-5 matching rate of the targeted adversarial images generated for model i
(row) when evaluated on model j (column). Higher value indicates more successful transferable
target labels. We used a larger learning rate to achieve larger RMSD.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.11
23.11
23.11
23.12
23.11

12%
29%
34%
25%
46%

ResNet-101
27%
13%
28%
20%
41%

ResNet-50 VGG-16 GoogLeNet
22%
29%
25%
0%
25%

25%
29%
10%
23%
40%

15%
16%
23%
8%
2%

Table 19: Transferability between pairs of models using non-targeted FGS. The ﬁrst column indi-
cates the average RMSD of all adversarial images generated for the model in the corresponding row.
The cell (i, j) indicates the top-1 accuracy of the adversarial images generated for model i (row)
evaluated over model j (column). Lower value indicates better transferability. Results of top-5
accuracy can be found in Table 20.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.11
23.11
23.11
23.12
23.11

32%
56%
59%
42%
71%

ResNet-101
55%
33%
53%
39%
74%

ResNet-50 VGG-16 GoogLeNet
47%
46%
47%
5%
53%

53%
50%
29%
41%
62%

36%
40%
38%
21%
11%

Table 20: Top-5 accuracy of Table 19. Transferability between pairs of models using non-targeted
FGS. The ﬁrst column indicates the average RMSD of all adversarial images generated for the
model in the corresponding row. The cell (i, j) indicates the top-5 accuracy of the adversarial
images generated for model i (row) evaluated over model j (column). Lower value indicates better
transferability.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.25
17.24
17.24
17.24
17.24

23%
15%
15%
17%
14%

ResNet-101
12%
19%
13%
15%
11%

ResNet-50 VGG-16 GoogLeNet
1%
2%
2%
23%
2%

11%
11%
19%
12%
10%

7%
6%
8%
7%
19%

Table 21: Transferability between pairs of models using non-targeted ensemble FG. The ﬁrst column
indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that top-1
accuracy of the non-targeted adversarial images generated using the ensemble of the four models
except model i (row) when evaluated over model j (column). In each row, the minus sign “−”
indicates that the model of the row is not used when generating the attacks. Lower value indicates
better transferability. Results of top-5 accuracy can be found in Table 22.

20

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.25
17.24
17.24
17.24
17.24

47%
37%
38%
37%
32%

ResNet-101
33%
42%
32%
36%
30%

ResNet-50 VGG-16 GoogLeNet
15%
14%
13%
44%
13%

28%
30%
39%
37%
28%

21%
26%
23%
32%
46%

Table 22: Top-5 accuracy of Table 21. Transferability between pairs of models using non-targeted
ensemble FG. The ﬁrst column indicates the average RMSD of the generated adversarial images.
Cell (i, j) indicates that top-5 accuracy of the non-targeted adversarial images generated using the
ensemble of the four models except model i (row) when evaluated over model j (column). In each
row, the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Lower value indicates better transferability.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.41
17.40
17.40
17.40
17.40

26%
18%
15%
17%
15%

ResNet-101
10%
21%
13%
16%
12%

ResNet-50 VGG-16 GoogLeNet
2%
2%
4%
23%
3%

11%
13%
20%
11%
10%

7%
4%
8%
7%
22%

Table 23: Transferability between pairs of models using non-targeted ensemble FGS. The ﬁrst col-
umn indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that top-1
accuracy of the non-targeted adversarial images generated using the ensemble of the four models
except model i (row) when evaluated over model j (column). In each row, the minus sign “−” indi-
cates that the model of the row is not used when generating the attacks. Lower value indicates better
transferability. Results of top-5 accuracy can be found in Table 24.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.41
17.40
17.40
17.40
17.40

50%
38%
38%
37%
35%

ResNet-101
35%
48%
36%
36%
29%

ResNet-50 VGG-16 GoogLeNet
20%
19%
18%
48%
19%

26%
26%
28%
33%
53%

30%
33%
41%
36%
31%

Table 24: Top-5 accuracy of Table 23. Transferability between pairs of models using non-targeted
ensemble FGS. The ﬁrst column indicates the average RMSD of the generated adversarial images.
Cell (i, j) indicates that top-5 accuracy of the non-targeted adversarial images generated using the
ensemble of the four models except model i (row) when evaluated over model j (column). In each
row, the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Lower value indicates better transferability.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.55
23.56
23.56
23.95
23.63

1%
1%
1%
1%
1%

ResNet-101
2%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
1%
1%

0%
0%
1%
0%
0%

1%
1%
0%
1%
1%

Table 25: The adversarial images are generated using the targeted FG. The ﬁrst column indicates the
average RMSD of all adversarial images generated for the model in the corresponding row. The ﬁrst
column indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that
top-1 matching rate of the targeted adversarial images generated for model i (row) when evaluated
on model j (column). Higher value indicates more successful transferable target labels.

21

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
33.33%
24.00%
17.50%
15.38%

ResNet-101
−
100.00%
35.00%
19.05%
14.81%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
21.18% 100.00%
15.05%
13.10%

GoogLeNet
−
−
−
−
100.00%

Table 26: When using non-targeted FG for VGG-16 model to generate adversarial images, cell (i, j)
is the percentage of all transferable adversarial images that are predicted as the same wrong labels
by both models i and j over all adversarial images that are misclassiﬁed by both models i and j.
Notice that the table is symmetric.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
33.78%
36.62%
16.00%
14.86%

ResNet-101
−
100.00%
43.24%
20.00%
20.25%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
23.38% 100.00%
19.57%
13.16%

GoogLeNet
−
−
−
−
100.00%

Table 27: When using non-targeted FGS for VGG-16 model to generate adversarial images, cell
(i, j) is the percentage of all transferable adversarial images that are predicted as the same wrong
labels by both models i and j over all adversarial images that are misclassiﬁed by both models i and
j. Notice that the table is symmetric.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
31.05
30.94
31.12
30.57
30.47

1%
1%
1%
1%
1%

ResNet-101
1%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
1%

1%
0%
1%
1%
0%

0%
0%
0%
0%
0%

Table 28: Transferability between pairs of models using targeted ensemble FG. The ﬁrst column
indicates the average RMSD of all adversarial images generated for the model in the corresponding
row. Cell (i, j) indicates that top-1 matching rate of the targeted adversarial images generated using
the ensemble of the four models except model i (row) is predicted as the target label by model j
(column). In each row, the minus sign “−” indicates that the model of the row is not used when
generating the attacks. Higher value indicates more successful transferable target labels. Results of
top-5 matching rate can be found in Table 29.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
31.05
30.94
31.12
30.57
30.47

1%
1%
1%
2%
1%

ResNet-101
2%
1%
2%
2%
1%

ResNet-50 VGG-16 GoogLeNet
3%
1%
3%
1%
2%

2%
2%
2%
1%
2%

1%
1%
1%
2%
1%

Table 29: Top-5 matching rate of Table 28. Transferability between pairs of models using targeted
ensemble FG. The ﬁrst column indicates the average RMSD of all adversarial images generated
for the model in the corresponding row. Cell (i, j) indicates that top-5 matching rate of the targeted
adversarial images generated using the ensemble of the four models except model i (row) is predicted
as the target label by model j (column). In each row, the minus sign “−” indicates that the model of
the row is not used when generating the attacks. Higher value indicates more successful transferable
target labels.

22

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.42
30.42
30.42
30.41
30.42

1%
1%
1%
1%
1%

ResNet-101
1%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
1%

1%
1%
0%
0%
1%

1%
1%
1%
1%
1%

Table 30: Transferability between pairs of models using targeted ensemble FGS. The ﬁrst column
indicates the average RMSD of all adversarial images generated for the model in the corresponding
row. Cell (i, j) indicates that top-1 matching rate of the targeted adversarial images generated using
the ensemble of the four models except model i (row) is predicted as the target label by model j
(column). In each row, the minus sign “−” indicates that the model of the row is not used when
generating the attacks. Higher value indicates more successful transferable target labels. Results are
top-5 matching rate can be found in Table 31.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.42
30.42
30.42
30.41
30.42

2%
2%
2%
3%
2%

ResNet-101
1%
1%
1%
2%
2%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
2%

2%
1%
1%
2%
2%

2%
2%
2%
1%
2%

Table 31: Top-5 matching rate of Table 30. Transferability between pairs of models using targeted
ensemble FGS. The ﬁrst column indicates the average RMSD of all adversarial images generated
for the model in the corresponding row. Cell (i, j) indicates that top-5 matching rate of the targeted
adversarial images generated using the ensemble of the four models except model i (row) is predicted
as the target label by model j (column). In each row, the minus sign “−” indicates that the model of
the row is not used when generating the attacks. Higher value indicates more successful transferable
target labels.

ResNet-152
jigsaw puzzle(8%)
acorn(3%)
lycaenid(2%)
ram(2%)
maze(2%)

ResNet-101
jigsaw puzzle(12%)
starﬁsh(3%)
strawberry(2%)
wild boar(2%)
dishrag(2%)

ResNet-50
jigsaw puzzle(12%)

VGG-16
jigsaw puzzle(15%)

African chameleon(2%) African chameleon(7%)

strawberry(2%)
starﬁsh(2%)
greenhouse(2%)

prayer rug(5%)
apron(4%)
sarong(3%)

GoogleNet
prayer rug(12%)
jigsaw puzzle(7%)
stole(6%)
African chameleon(4%)
mitten(3%)

Table 32: When using non-targeted optimization-based approach for VGG-16 model to generate
adversarial images, column i indicates the top 5 common incorrect labels predicted by model i. The
value in the parentheses is the percentage of the predicted label.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
1.00
0.04
0.03
0.02
0.01

ResNet-101
−
1.00
0.03
0.02
0.01

ResNet-50 VGG-16 GoogLeNet
−
−
−
1.00
0.02

−
−
1.00
0.02
0.01

−
−
−
−
1.00

Table 33: Average cosine value of the angle between gradient directions of two models. Notice
that the dot-product of two normalized vectors is the cosine value of the angle between them, for
each image, we compute the dot-product of normalized gradient directions with respect to model i
(row) and model j (column), and the value in cell (i, j) is the average over dot-product values of all
images. Notice that this table is symmetric.

23

Published as a conference paper at ICLR 2017

(a) GoogLeNet

(b) VGG-16

(c) ResNet-152

(d) ResNet-101

(e) ResNet-50

Figure 7: Linearity of different models. Each line plots the classiﬁcation layer’s softmax input vs
distortion B when fast gradient noises are used. Solid red line is the ground truth label, and other
lines are the top 10 labels predicted on the original image.

24

7
1
0
2
 
b
e
F
 
7
 
 
]

G
L
.
s
c
[
 
 
3
v
0
7
7
2
0
.
1
1
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

DELVING INTO TRANSFERABLE ADVERSARIAL EX-
AMPLES AND BLACK-BOX ATTACKS

Yanpei Liu∗, Xinyun Chen∗
Shanghai Jiao Tong University

Chang Liu, Dawn Song
University of the California, Berkeley

ABSTRACT

An intriguing property of deep neural networks is the existence of adversarial ex-
amples, which can transfer among different architectures. These transferable ad-
versarial examples may severely hinder deep neural network-based applications.
Previous works mostly study the transferability using small scale datasets. In this
work, we are the ﬁrst to conduct an extensive study of the transferability over
large models and a large scale dataset, and we are also the ﬁrst to study the trans-
ferability of targeted adversarial examples with their target labels. We study both
non-targeted and targeted adversarial examples, and show that while transferable
non-targeted adversarial examples are easy to ﬁnd, targeted adversarial examples
generated using existing approaches almost never transfer with their target labels.
Therefore, we propose novel ensemble-based approaches to generating transfer-
able adversarial examples. Using such approaches, we observe a large proportion
of targeted adversarial examples that are able to transfer with their target labels for
the ﬁrst time. We also present some geometric studies to help understanding the
transferable adversarial examples. Finally, we show that the adversarial examples
generated using ensemble-based approaches can successfully attack Clarifai.com,
which is a black-box image classiﬁcation system.

1

INTRODUCTION

Recent research has demonstrated that for a deep architecture, it is easy to generate adversarial
examples, which are close to the original ones but are misclassiﬁed by the deep architecture (Szegedy
et al. (2013); Goodfellow et al. (2014)). The existence of such adversarial examples may have severe
consequences, which hinders vision-understanding-based applications, such as autonomous driving.
Most of these studies require explicit knowledge of the underlying models.
It remains an open
question how to efﬁciently ﬁnd adversarial examples for a black-box model.

Several works have demonstrated that some adversarial examples generated for one model may
also be misclassiﬁed by another model. Such a property is referred to as transferability, which
can be leveraged to perform black-box attacks. This property has been exploited by constructing
a substitute of the black-box model, and generating adversarial instances against the substitute to
attack the black-box system (Papernot et al. (2016a;b)). However, so far, transferability is mostly
examined over small datasets, such as MNIST (LeCun et al. (1998)) and CIFAR-10 (Krizhevsky &
Hinton (2009)). It has yet to be better understood transferability over large scale datasets, such as
ImageNet (Russakovsky et al. (2015)).

In this work, we are the ﬁrst to conduct an extensive study of the transferability of different adver-
sarial instance generation strategies applied to different state-of-the-art models trained over a large
scale dataset. In particular, we study two types of adversarial examples: (1) non-targeted adversar-
ial examples, which can be misclassiﬁed by a network, regardless of what the misclassiﬁed labels
may be; and (2) targeted adversarial examples, which can be classiﬁed by a network as a target
label. We examine several existing approaches searching for adversarial examples based on a single
model. While non-targeted adversarial examples are more likely to transfer, we observe few targeted
adversarial examples that are able to transfer with their target labels.

∗Work is done while visiting UC Berkeley.

1

Published as a conference paper at ICLR 2017

We further propose a novel strategy to generate transferable adversarial images using an ensemble
of multiple models. In our evaluation, we observe that this new strategy can generate non-targeted
adversarial instances with better transferability than other methods examined in this work. Also, for
the ﬁrst time, we observe a large proportion of targeted adversarial examples that are able to transfer
with their target labels.

We study geometric properties of the models in our evaluation.
In particular, we show that the
gradient directions of different models are orthogonal to each other. We also show that decision
boundaries of different models align well with each other, which partially illustrates why adversarial
examples can transfer.

Last, we study whether generated adversarial images can attack Clarifai.com, a commercial com-
pany providing state-of-the-art image classiﬁcation services. We have no knowledge about the train-
ing dataset and the types of models used by Clarifai.com; meanwhile, the label set of Clarifai.com
is quite different from ImageNet’s. We show that even in this case, both non-targeted and targeted
adversarial images transfer to Clarifai.com. This is the ﬁrst work documenting the success of gen-
erating both non-targeted and targeted adversarial examples for a black-box state-of-the-art online
image classiﬁcation system, whose model and training dataset are unknown to the attacker.

Contributions and organization. We summarize our main contributions as follows:

• For ImageNet models, we show that while existing approaches are effective to generate
non-targeted transferable adversarial examples (Section 3), only few targeted adversarial
examples generated by existing methods can transfer (Section 4).

• We propose novel ensemble-based approaches to generate adversarial examples (Sec-
tion 5). Our approaches enable a large portion of targeted adversarial examples to transfer
among multiple models for the ﬁrst time.

• We are the ﬁrst to present that targeted adversarial examples generated for models trained
on ImageNet can transfer to a black-box system, i.e., Clarifai.com, whose model, training
data, and label set is unknown to us (Section 7). In particular, Clarifai.com’s label set is
very different from ImageNet’s.

• We conduct the ﬁrst analysis of geometric properties for large models trained over Ima-
geNet (Section 6), and the results reveal several interesting ﬁndings, such as the gradient
directions of different models are orthogonal to each other.

In the following, we ﬁrst discuss related work, and then present the background knowledge and
experiment setup in Section 2. Then we present each of our experiments and conclusions in the
corresponding section as mentioned above.

Related work. Transferability of adversarial examples was ﬁrst examined by Szegedy et al.
(2013), which studied the transferability (1) between different models trained over the same dataset;
and (2) between the same or different model trained over disjoint subsets of a dataset; However,
Szegedy et al. (2013) only studied MNIST.

The study of transferability was followed by Goodfellow et al. (2014), which attributed the phe-
nomenon of transferability to the reason that the adversarial perturbation is highly aligned with the
weight vector of the model. Again, this hypothesis was tested using MNIST and CIFAR-10 datasets.
We show that this is not the case for models trained over ImageNet.

Papernot et al. (2016a;b) examined constructing a substitute model to attack a black-box target
model. To train the substitute model, they developed a technique that synthesizes a training set and
annotates it by querying the target model for labels. They demonstrate that using this approach,
black-box attacks are feasible towards machine learning services hosted by Amazon, Google, and
MetaMind. Further, Papernot et al. (2016a) studied the transferability between deep neural networks
and other models such as decision tree, kNN, etc.

Our work differs from Papernot et al. (2016a;b) in three aspects. First, in these works, only the model
and the training process are a black box, but the training set and the test set are controlled by the
attacker; in contrast, we attack Clarifai.com, whose model, training data, training process, and even
the test label set are unknown to the attacker. Second, the datasets studied in these works are small

2

Published as a conference paper at ICLR 2017

scale, i.e., MNIST and GTSRB (Stallkamp et al. (2012)); in our work, we study the transferability
over larger models and a larger dataset, i.e., ImageNet. Third, to attack black-box machine learning
systems, we do not query the systems for constructing the substitute model ourselves.

In a concurrent and independent work, Moosavi-Dezfooli et al. (2016) showed the existence of a
universal perturbation for each model, which can transfer across different images. They also show
that the adversarial images generated using these universal perturbations can transfer across different
models on ImageNet. However, they only examine the non-targeted transferability, while our work
studies both non-targeted and targeted transferability over ImageNet.

2 ADVERSARIAL DEEP LEARNING AND TRANSFERABILITY

2.1 THE ADVERSARIAL DEEP LEARNING PROBLEM

We assume a classiﬁer fθ(x) outputs a category (or a label) as the prediction. Given an original
image x, with ground truth label y, the adversarial deep learning problem is to seek for adversarial
examples for the classiﬁer fθ(x). Speciﬁcally, we consider two classes of adversarial examples.
A non-targeted adversarial example x(cid:63) is an instance that is close to x, in which case x(cid:63) should
have the same ground truth as x, while fθ(x(cid:63)) (cid:54)= y. For the problem to be non-trivial, we assume
fθ(x) = y without loss of generality. A targeted adversarial example x(cid:63) is close to x and satisﬁes
fθ(x(cid:63)) = y(cid:63), where y(cid:63) is a target label speciﬁed by the adversary, and y(cid:63) (cid:54)= y.

2.2 APPROACHES FOR GENERATING ADVERSARIAL EXAMPLES

In this work, we consider three classes of approaches for generating adversarial examples:
optimization-based approaches, fast gradient approaches, and fast gradient sign approaches. Each
class has non-targeted and targeted versions respectively.

2.2.1 APPROACHES FOR GENERATING NON-TARGETED ADVERSARIAL EXAMPLES

Formally, given an image x with ground truth y = fθ(x), searching for a non-targeted adversarial
example can be modeled as searching for an instance x(cid:63) to satisfy the following constraints:

fθ(x(cid:63)) (cid:54)= y
d(x, x(cid:63)) ≤ B

(1)
(2)

where d(·, ·) is a metric to quantify the distance between an original image and its adversarial coun-
terpart, and B, called distortion, is an upper bound placed on this distance. Without loss of gener-
ality, we consider model f is composed of a network Jθ(x), which outputs the probability for each
category, so that f outputs the category with the highest probability.

Optimization-based approach. One approach is to approximate the solution to the following
optimization problem:

argminx(cid:63) λd(x, x(cid:63)) − (cid:96)(1y, Jθ(x(cid:63)))
(3)
where 1y is the one-hot encoding of the ground truth label y, (cid:96) is a loss function to measure the
distance between the prediction and the ground truth, and λ is a constant to balance constraints (2)
and (1), which is empirically determined. Here, loss function (cid:96) is used to approximate constraint (1),
and its choice can affect the effectiveness of searching for an adversarial example. In this work, we
choose (cid:96)(u, v) = log (1 − u · v), which is shown to be effective by Carlini & Wagner (2016).

Fast gradient sign (FGS). Goodfellow et al. (2014) proposed the fast gradient sign (FGS) method
so that the gradient needs be computed only once to generate an adversarial example. FGS can be
used to generate adversarial images to meet the L∞ norm bound. Formally, non-targeted adversarial
examples are constructed as

x(cid:63) ← clip(x + Bsgn(∇x(cid:96)(1y, Jθ(x))))
Here, clip(x) is used to clip each dimension of x to the range of pixel values, i.e., [0, 255] in this
work. We make a slight variation to choose (cid:96)(u, v) = log (1 − u · v), which is the same as used in
the optimization-based approach.

3

Published as a conference paper at ICLR 2017

Fast gradient (FG). The fast gradient approach (FG) is similar to FGS, but instead of moving
along the gradient sign direction, FG moves along the gradient direction. In particular, we have

x(cid:63) ← clip(x + B

∇x(cid:96)(1y, Jθ(x))
||∇x(cid:96)(1y, Jθ(x))||

))

Here, we assume the distance metric in constraint (2), d(x, x(cid:63)) = ||x − x(cid:63)|| is a norm of x − x(cid:63).
The term sgn(∇x(cid:96)) in FGS is replaced by ∇x(cid:96)

||∇x(cid:96)|| to meet this distance constraint.

We call both FGS and FG fast gradient-based approaches.

2.2.2 APPROACHES FOR GENERATING TARGETED ADVERSARIAL EXAMPLES

A targeted adversarial image x(cid:63) is similar to a non-targeted one, but constraint (1) is replaced by
fθ(x(cid:63)) = y(cid:63)

(4)
where y(cid:63) is the target label given by the adversary. For the optimization-based approach, we ap-
proximate the solution by solving the following dual objective:

argminx(cid:63) λd(x, x(cid:63)) + (cid:96)(cid:48)(1y(cid:63) , Jθ(x(cid:63)))

(5)

In this work, we choose the standard cross entropy loss (cid:96)(cid:48)(u, v) = − (cid:80)
i

ui log vi.

For FGS and FG, we construct adversarial examples as follows:

x(cid:63) ← clip(x − Bsgn(∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))))

(FGS)

x(cid:63) ← clip(x − B

∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))
||∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))||

)

(FG)

where (cid:96)(cid:48) is the same as the one used for the optimization-based approach.

2.3 EVALUATION METHODOLOGY

For the rest of the paper, we focus on examining the transferability among state-of-the-art models
trained over ImageNet (Russakovsky et al. (2015)).
In this section, we detail the models to be
examined, the dataset to be evaluated, and the measurements to be used.

Models. We examine ﬁve networks, ResNet-50, ResNet-101, ResNet-152 (He et al. (2015))1,
GoogLeNet (Szegedy et al. (2014))2, and VGG-16 (Simonyan & Zisserman (2014))3. We retrieve
the pre-trained models for each network online. The performance of these models on the ILSVRC
2012 (Russakovsky et al. (2015)) validation set are presented in the appendix (Table 7). We choose
these models to study the transferability between homogeneous architectures (i.e., ResNet models)
and heterogeneous architectures.

Dataset.
It is less meaningful to examine the transferability of an adversarial image between two
models which cannot classify the original image correctly. Therefore, from the ILSVRC 2012 val-
idation set, we randomly choose 100 images, which can be classiﬁed correctly by all ﬁve models
in our examination. These 100 images form our test set. To perform targeted attacks, we manually
choose a target label for each image, so that its semantics is far from the ground truth. The images
and target labels in our evaluation can be found on website4.

Measuring transferability. Given two models, we measure the non-targeted transferability by
computing the percentage of the adversarial examples generated for one model that can be classiﬁed
correctly for the other. We refer to this percentage as accuracy. A lower accuracy means better
non-targeted transferability. We measure the targeted transferability by computing the percentage of
the adversarial examples generated for one model that are classiﬁed as the target label by the other
model. We refer to this percentage as matching rate. A higher matching rate means better targeted
transferability. For clarity, the reported results are only based on top-1 accuracy. Top-5 accuracy’s
counterparts can be found in the appendix.

1https://github.com/KaimingHe/deep-residual-networks
2https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet
3https://gist.github.com/ksimonyan/211839e770f7b538e2d8
4https://github.com/sunblaze-ucb/transferability-advdnn-pub

4

Published as a conference paper at ICLR 2017

Distortion. Besides transferability, another important factor is the distortion between adversarial
images and the original ones. We measure the distortion by root mean square deviation, i.e., RMSD,
which is computed as d(x(cid:63), x) = (cid:112)(cid:80)
i − xi)2/N , where x(cid:63) and x are the vector representations
of an adversarial image and the original one respectively, N is the dimensionality of x and x(cid:63), and
xi denotes the pixel value of the i-th dimension of x, within range [0, 255], and similar for x(cid:63)
i .

i(x(cid:63)

3 NON-TARGETED ADVERSARIAL EXAMPLES

In this section, we examine different approaches for generating non-targeted adversarial images.

3.1 OPTIMIZATION-BASED APPROACH

To apply the optimization-based approach for a single model, we initialize x(cid:63) to be x and use Adam
Optimizer (Kingma & Ba (2014)) to optimize Objective (3) . We ﬁnd that we can tune the RMSD
by adjusting the learning rate of Adam and λ. We ﬁnd that, for each model, we can use a small
learning rate to generate adversarial images with small RMSD, i.e. < 2, with any λ. In fact, we ﬁnd
that when initializing x(cid:63) with x, Adam Optimizer will search for an adversarial example around x,
even when we set λ to be 0, i.e., not restricting the distance between x(cid:63) and x. Therefore, we set
λ to be 0 for all experiments using optimization-based approaches throughout the paper. Although
these adversarial examples with small distortions can successfully fool the target model, however,
they cannot transfer well to other models (see Table 15 and 16 in the appendix for details).

We increase the learning rate to allow the optimization algorithm to search for adversarial images
with larger distortion. In particular, we set the learning rate to be 4. We run Adam Optimizer for 100
iterations to generate the adversarial images. We observe that the loss converges after 100 iterations.
An alternative optimization-based approach leading to similar results can be found in the appendix.

Non-targeted adversarial examples transfer. We generate non-targeted adversarial examples on
one network, but evaluate them on another, and Table 1 Panel A presents the results. From the table,
we can observe that

• The diagonal contains all 0 values. This says that all adversarial images generated for one

model can mislead the same model.

• A large proportion of non-targeted adversarial images generated for one model using the

optimization-based approach can transfer to another.

• Although the three ResNet models share similar architectures which differ only in the hy-
perparameters, adversarial examples generated against a ResNet model do not necessarily
transfer to another ResNet model better than other non-ResNet models. For example, the
adversarial examples generated for VGG-16 have lower accuracy on ResNet-50 than those
generated for ResNet-152 or ResNet-101.

3.2 FAST GRADIENT-BASED APPROACHES

We then examine the effectiveness of fast gradient-based approaches. A good property of fast
gradient-based approaches is that all generated adversarial examples lie in a 1-D subspace. There-
fore, we can easily approximate the minimal distortion in this subspace of transferable adversarial
examples between two models. In the following, we ﬁrst control the RMSD to study fast gradient-
based approaches’ effectiveness. Second, we study the transferable minimal distortions of fast
gradient-based approaches.

3.2.1 EFFECTIVENESS AND TRANSFERABILITY OF THE FAST GRADIENT-BASED

APPROACHES

Since the distortion B and the RMSD of the generated adversarial images are highly correlated, we
can choose this hyperparameter B to generate adversarial images with a given RMSD. In Table 1
Panel B, we generate adversarial images using FG such that the average RMSD is almost the same
as those generated using the optimization-based approach. We observe that the diagonal values in

5

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
22.83
23.81
22.86
22.51
22.58

ResNet-101
13%
0%
20%
17%
38%
Panel A: Optimization-based approach

ResNet-50 VGG-16 GoogLeNet
19%
21%
21%
0%
19%

0%
19%
23%
22%
39%

18%
21%
0%
17%
34%

11%
12%
18%
5%
0%

RMSD
23.45
23.49
23.49
23.73
23.45

ResNet-152
ResNet-101
4%
13%
13%
19%
4%
11%
25%
19%
5%
20%
16%
15%
17%
25%
25%
Panel B: Fast gradient approach

ResNet-50 VGG-16 GoogLeNet
20%
23%
25%
1%
19%

12%
13%
14%
7%
1%

Table 1: Transferability of non-targeted adversarial images generated between pairs of models. The
ﬁrst column indicates the average RMSD of all adversarial images generated for the model in the
corresponding row. The cell (i, j) indicates the accuracy of the adversarial images generated for
model i (row) evaluated over model j (column). Results of top-5 accuracy can be found in the
appendix (Table 8 and Table 10).

the table are all positive, which means that FG cannot fully mislead the models. A potential reason
is that, FG can be viewed as approximating the optimization, but is tailored for speed over accuracy.

On the other hand, the values of non-diagonal cells in the table, which correspond to the accuracies
of adversarial images generated for one model but evaluated on another, are comparable with or less
than their counterparts in the optimization-based approach. This shows that non-targeted adversarial
examples generated by FG exhibit transferability as well.

We also evaluate FGS, but the transferability of the generated images is worse than the ones gener-
ated using either FG or optimization-based approaches. The results are in the appendix (Table 19
and 20). It shows that when RMSD is around 23, the accuracies of the adversarial images generated
by FGS is greater than their counterparts for FG. We hypothesize the reason why transferability of
FGS is worse to this fact.

3.2.2 ADVERSARIAL IMAGES WITH MINIMAL TRANSFERABLE RMSD

For an image x and two models M1, M2, we can approximate the minimal distortion B along a
direction δ, such that xB = x + Bδ generated for M1 is adversarial for both M1 and M2. Here δ is
the direction, i.e., sgn(∇x(cid:96)) for FGS, and ∇x(cid:96)/||∇x(cid:96)|| for FG.

We refer to the minimal transferable RMSD from M1 to M2 using FG (or FGS) as the RMSD of
a transferable adversarial example xB with the minimal transferable distortion B from M1 to M2
using FG (or FGS). The minimal transferable RMSD can illustrate the tradeoff between distortion
and transferability.

In the following, we approximate the minimal transferable RMSD through a linear search by sam-
pling B every 0.1 step. We choose the linear-search method rather than binary-search method to de-
termine the minimal transferable RMSD because the adversarial images generated from an original
image may come from multiple intervals. The experiment can be found in the appendix (Figure 6).

Minimal transferable RMSD using FG and FGS. Figure 1 plots the cumulative distribution
function (CDF) of the minimal transferable RMSD from VGG-16 to ResNet-152 using non-targeted
FG (Figure 1a) and FGS (Figure 1b). From the ﬁgures, we observe that both FG and FGS can ﬁnd
100% transferable adversarial images with RMSD less than 80.91 and 86.56 respectively. Further,
the FG method can generate transferable attacks with smaller RMSD than FGS. A potential rea-
son is that while FGS minimizes the distortion’s L∞ norm, FG minimizes its L2 norm, which is
proportional to RMSD.

6

Published as a conference paper at ICLR 2017

(a) Fast Gradient

(b) Fast Gradient Sign

Figure 1: The CDF of the minimal transferable RMSD from VGG-16 to ResNet-152 using FG (a)
and FGS (b). The green line labels the median minimal transferable RMSD, while the red line labels
the minimal transferable RMSD to reach 90% percentage.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.13
23.16
23.06
23.59
22.87

100%
3%
4%
2%
1%

ResNet-101
2%
100%
2%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
2%
1%
100%
1%

1%
1%
1%
1%
100%

1%
3%
100%
2%
0%

Table 2: The matching rate of targeted adversarial images generated using the optimization-based
approach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) indicates that matching rate of the targeted adversarial images generated for model i (row)
when evaluated on model j (column). The top-5 results can be found in the appendix (Table 12).

3.3 COMPARISON WITH RANDOM PERTURBATIONS

We also evaluate the test accuracy when we add a Gaussian noise to the 100 images in our test
set. The concrete results can be found in the appendix, and we show the conclusion that the “trans-
ferability” of this approach is signiﬁcantly worse than either optimization-based approaches or fast
gradient-based approaches.

4 TARGETED ADVERSARIAL EXAMPLES

In this section, we examine the transferability of targeted adversarial images. Table 2 presents
the results for using optimization-based approach. We observe that (1) the prediction of targeted
adversarial images can match the target labels when evaluated on the same model that is used to
generate the adversarial examples; but (2) the targeted adversarial images can be rarely predicted
as the target labels by a different model. We call the latter that the target labels do not transfer.
Even when we increase the distortion, we still do not observe improvements on making target label
transfer. Some results can be found in the appendix (Table 17). Even if we compute the matching
rate based on top-5 accuracy, the highest matching rate is only 10%. The results can be found in the
appendix (Table 18).

We also examine the targeted adversarial images generated by fast gradient-based approaches, and
we observe that the target labels do not transfer as well. The results are deferred to the appendix
(Table 25). In fact, most targeted adversarial images cannot mislead the model, for which the ad-
versarial images are generated, to predict the target labels, regardless of how large the distortion is
used. We attribute it to the fact that the fast gradient-based approaches only search for attacks in
a 1-D subspace. In this subspace, the total possible predictions may contain a small subset of all
labels, which usually does not contain the target label. In Section 6, we study decision boundaries
regarding this issue.

We also evaluate the matching rate of images added with Gaussian noise, as described in Section 3.3.
However, we observe that the matching rate of any of the 5 models is 0%. Therefore, we conclude

7

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.68
30.76
30.26
31.13
29.70

38%
75%
84%
74%
90%

ResNet-101
76%
43%
81%
78%
87%

ResNet-50 VGG-16 GoogLeNet
97%
98%
99%
24%
99%

76%
73%
77%
63%
11%

70%
69%
46%
68%
83%

Table 3: The matching rate of targeted adversarial images generated using the optimization-based
approach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) indicates that percentage of the targeted adversarial images generated for the ensemble of the
four models except model i (row) is predicted as the target label by model j (column). In each row,
the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Results of top-5 matching rate can be found in the appendix (Table 13).

that by adding Gaussian noise, the attacker cannot generate successful targeted adversarial examples
at all, let alone targeted transferability.

5 ENSEMBLE-BASED APPROACHES

We hypothesize that if an adversarial image remains adversarial for multiple models, then it is more
likely to transfer to other models as well. We develop techniques to generate adversarial images for
multiple models. The basic idea is to generate adversarial images for the ensemble of the models.
Formally, given k white-box models with softmax outputs being J1, ..., Jk, an original image x,
and its ground truth y, the ensemble-based approach solves the following optimization problem (for
targeted attack):

argminx(cid:63) − log (cid:0)(

αiJi(x(cid:63))) · 1y(cid:63)

(cid:1) + λd(x, x(cid:63))

(6)

k
(cid:88)

i=1

where y(cid:63) is the target label speciﬁed by the adversary, (cid:80) αiJi(x(cid:63)) is the ensemble model, and αi
are the ensemble weights, (cid:80)k
i=1 αi = 1. Note that (6) is the targeted objective. The non-targeted
counterpart can be derived similarly. In doing so, we hope the generated adversarial images remain
adversarial for an additional black-box model Jk+1.

We evaluate the effectiveness of the ensemble-based approach. For each of the ﬁve models, we treat
it as the black-box model to attack, and generate adversarial images for the ensemble of the rest
four, which is considered as white-box. We evaluate the generated adversarial images over all ﬁve
models. Throughout the rest of the paper, we refer to the approaches evaluated in Section 3 and 4 as
the approaches using a single model, and to the ensemble-based approaches discussed in this section
as the approaches using an ensemble model.

Optimization-based approach. We use Adam to optimize the objective (6) with equal ensemble
weights across all models in the ensemble to generate targeted adversarial examples. In particular,
we set the learning rate of Adam to be 8 for each model. In each iteration, we compute the Adam
update for each model, sum up the four updates, and add the aggregation onto the image. We run 100
iterations of updates, and we observe that the loss converges after 100 iterations. By doing so, for the
ﬁrst time, we observe a large proportion of the targeted adversarial images whose target labels can
transfer. The results are presented in Table 3. We observe that not all targeted adversarial images
can be misclassiﬁed to the target labels by the models used in the ensemble. This suggests that
while searching for an adversarial example for the ensemble model, there is no direct supervision to
mislead any individual model in the ensemble to predict the target label. Further, from the diagonal
numbers of the table, we observe that the transferability to ResNet models is better than to VGG-16
or GoogLeNet, when adversarial examples are generated against all models except the target model.

We also evaluate non-targeted adversarial images generated by the ensemble-based approach. We
observe that the generated adversarial images have almost perfect transferability. We use the same
procedure as for the targeted version, except the objective to generate the adversarial images. We
evaluate the generated adversarial images over all models. The results are presented in Table 4.
The generated adversarial images all have RMSDs around 17, which are lower than 22 to 23 of

8

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.17
17.25
17.25
17.80
17.41

0%
0%
0%
0%
0%

ResNet-101
0%
1%
0%
0%
0%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
6%
0%

0%
0%
2%
0%
0%

0%
0%
0%
0%
5%

Table 4: Accuracy of non-targeted adversarial images generated using the optimization-based ap-
proach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) corresponds to the accuracy of the attack generated using four models except model i (row)
when evaluated over model j (column). In each row, the minus sign “−” indicates that the model
of the row is not used when generating the attacks. Results of top-5 accuracy can be found in the
appendix (Table 14).

Model
# of labels

VGG-16 ResNet-50 ResNet-101 ResNet-152 GoogLeNet

10

9

21

10

21

Table 5: The number of all possible predicted labels for each model in the same plane described in Figure 3.

the optimization-based approach using a single model (See Table 1 for comparison). When the
adversarial images are evaluated over models which are not used to generate the attack, the accuracy
is no greater than 6%. For a reference, the corresponding accuracies for all approaches evaluated in
Section 3 using one single model are at least 12%. Our experiments demonstrate that the ensemble-
based approaches can generate almost perfectly transferable adversarial images.

Fast gradient-based approach. The results for non-targeted fast gradient-based approaches ap-
plied to the ensemble can be found in the appendix (Table 21, 22, 23 and 24). We observe that the
diagonal values are not zero, which is the same as we observed in the results for FG and FGS applied
to a single model. We hypothesize a potential reason is that the gradient directions of different mod-
els in the ensemble are orthogonal to each other, as we will illustrate in Section 6. In this case, the
gradient direction of the ensemble is almost orthogonal to the one of each model in the ensemble.
Therefore searching along this direction may require large distortion to reach adversarial examples.

For targeted adversarial examples generated using FG and FGS based on an ensemble model, their
transferability is no better than the ones generated using a single model. The results can be found in
the appendix (Table 28, 29, 30 and 31). We hypothesize the same reason to explain this: there are
only few possible target labels in total in the 1-D subspace.

6 GEOMETRIC PROPERTIES OF DIFFERENT MODELS

In this section, we show some geometric properties of the models to try to better understand transfer-
able adversarial examples. Prior works also try to understand the geometic properties of adversarial
examples theoretically (Fawzi et al. (2016)) or empirically (Goodfellow et al. (2014)). In this work,
we examine large models trained over a large dataset with 1000 labels, whose geometric properties
are never examined before. This allows us to make new observations to better understand the models
and their adversarial examples.

The gradient directions of different models in our evaluation are almost orthogonal to each
other. We study whether the adversarial directions of different models align with each other. We
calculate cosine value of the angle between gradient directions of different models, and the results
can be found in the appendix (Table 33). We observe that all non-diagonal values are close to 0,
which indicates that for most images, their gradient directions with respect to different models are
orthogonal to each other.

Decision boundaries of the non-targeted approaches using a single model. We study the deci-
sion boundary of different models to understand why adversarial examples transfer. We choose two
normalized orthogonal directions δ1, δ2, one being the gradient direction of VGG-16 and the other
being randomly chosen. Each point (u, v) in this 2-D plane corresponds to the image x + uδ1 + vδ2,

9

Published as a conference paper at ICLR 2017

Figure 2: The example image to study the decision boundary. Its ID in ILSVRC 2012 validation set
is 49443, and its ground truth label is “anemone ﬁsh.”

VGG-16

ResNet-50

ResNet-101

ResNet-152

GoogLeNet

n
i
-

m
o
o
Z

t
u
o
-
m
o
o
Z

Figure 3: Decision regions of different models. We pick the same two directions for all plots: one is
the gradient direction of VGG-16 (x-axis), and the other is a random orthogonal direction (y-axis).
Each point in the span plane shows the predicted label of the image generated by adding a noise to
the original image (e.g., the origin corresponds to the predicted label of the original image). The
units of both axises are 1 pixel values. All sub-ﬁgure plots the regions on the span plane using the
same color for the same label. The image is in Figure 2.

where x is the pixel value vector of the original image. For each model, we plot the label of the
image corresponding to each point, and get Figure 3 using the image in Figure 2.

We can observe that for all models, the region that each model can predict the image correctly
is limited to the central area. Also, along the gradient direction, the classiﬁers are soon misled.
One interesting ﬁnding is that along this gradient direction, the ﬁrst misclassiﬁed label for the three
ResNet models (corresponding to the light green region) is the label “orange”. A more detailed study
can be found in the appendix (Table 9, Table 26 and 27). When we look at the zoom-out ﬁgures,
however, the labels of images that are far away from the original one are different for different
models, even among ResNet models.

On the other hand, in Table 5, we show the total number of regions in each plane. In fact, for each
plane, there are at most 21 different regions in all planes. Compared with the 1,000 total categories
in ImageNet, this is only 2.1% of all categories. That means, for all other 97.9% labels, no targeted
adversarial example exists in each plane. Such a phenomenon partially explains why fast gradient-
based approaches can hardly ﬁnd targeted adversarial images.

Further, in Figure 4, we draw the decision boundaries of all models on the same plane as described
above. We can observe that

• The boundaries align with each other very well. This partially explains why non-targeted

adversarial images can transfer among models.

10

Published as a conference paper at ICLR 2017

Figure 4: The decision boundary to sep-
arate the region within which all points
are classiﬁed as the ground truth label
(encircled by each closed curve) from
others. The plane is the same one de-
scribed in Figure 3.
The origin of
the coordinate plane corresponds to the
original image. The units of both axises
are 1 pixel values.

Figure 5: The decision boundary to separate the
region within which all points are classiﬁed as the
target label (encircled by each closed curve) from
others. The plane is spanned by the targeted ad-
versarial direction and a random orthogonal di-
rection. The targeted adversarial direction is com-
puted as the difference between the original image
in Figure 2 and the adversarial image generated by
the optimization-based approach for an ensemble.
The ensemble contains all models except ResNet-
101. The origin of the coordinate plane corre-
sponds to the original image. The units of both
axises are 1 pixel values.

• The boundary diameters along the gradient direction is less than the ones along the ran-
dom direction. A potential reason is that moving a variable along its gradient direction
can change the loss function (i.e., the probability of the ground truth label) signiﬁcantly.
Therefore along the gradient direction it will take fewer steps to move out of the ground
truth region than a random direction.

• An interesting ﬁnding is that even though we move left along the x-axis, which is equivalent
to maximizing the ground truth’s prediction probability, it also reaches the boundary much
sooner than moving along a random direction. We attribute this to the non-linearity of the
loss function: when the distortion is larger, the gradient direction also changes dramatically.
In this case, moving along the original gradient direction no longer increases the probability
to predict the ground truth label (see Figure 7 in the appendix).

• As for VGG-16 model, there is a small hole within the region corresponding to the ground
truth. This may partially explain why non-targeted adversarial images with small distortion
exist, but do not transfer well. This hole does not exist in other models’ decision planes. In
this case, non-targeted adversarial images in this hole do not transfer.

Decision boundaries of the targeted ensemble-based approaches.
In addition, we choose the
targeted adversarial direction of the ensemble of all models except ResNet-101 and a random or-
thogonal direction, and we plot decision boundaries on the plane spanned by these two direction
vectors in Figure 5. We observe that the regions of images, which are predicted as the target label,
align well for the four models in the ensemble. However, for the model not used to generate the
adversarial image, i.e., ResNet-101, it also has a non-empty region such that the prediction is suc-
cessfully misled to the target label, although the area is much smaller. Meanwhile, the region within
each closed curve of the models almost has the same center.

7 REAL WORLD EXAMPLE: ADVERSARIAL EXAMPLES FOR CLARIFAI.COM

Clarifai.com is a commercial company providing state-of-the-art image classiﬁcation services. We
have no knowledge about the dataset and types of models used behind Clarifai.com, except that we
have black-box access to the services. The labels returned from Clarifai.com are also different from

11

Published as a conference paper at ICLR 2017

the categories in ILSVRC 2012. We submit all 100 original images to Clarifai.com and the returned
labels are correct based on a subjective measure.

We also submit 400 adversarial images in total, where 200 of them are targeted adversarial examples,
and the rest 200 are non-targeted ones. As for the 200 targeted adversarial images, 100 of them
are generated using the optimization-based approach based on VGG-16 (the same ones evaluated
in Table 2), and the rest 100 are generated using the optimization-based approach based on an
ensemble of all models except ResNet-152 (the same ones evaluated in Table 3). The 200 non-
targeted adversarial examples are generated similarly (the same ones evaluated in Table 1 and 4).

For non-targeted adversarial examples, we observe that for both the ones generated using VGG-16
and those generated using the ensemble, most of them can transfer to Clarifai.com.

More importantly, a large proportion of our targeted adversarial examples are misclassiﬁed by Clari-
fai.com as well. We observe that 57% of the targeted adversarial examples generated using VGG-16,
and 76% of the ones generated using the ensemble can mislead Clarifai.com to predict labels irrele-
vant to the ground truth.

Further, our experiment shows that for targeted adversarial examples, 18% of those generated us-
ing the ensemble model can be predicted as labels close to the target label by Clarifai.com. The
corresponding number for the targeted adversarial examples generated using VGG-16 is 2%. Con-
sidering that in the case of attacking Clarifai.com, the labels given by the target model are different
from those given by our models, it is fairly surprising to see that when using the ensemble-based
approach, there is still a considerable proportion of our targeted adversarial examples that can mis-
lead this black-box model to make predictions semantically similar to our target labels. All these
numbers are computed based on a subjective measure, and we include some examples in Table 6.
More examples can be found in the appendix (Table 34).

original
image

true
label

Clarifai.com
results of
original image

target
label

targeted
adversarial
example

Clarifai.com results
of targeted
adversarial example

window,
wall,
old,
decoration,
design

Buddha,
gold,
temple,
celebration,
artistic

cherry,
branch,
fruit,
food,
season

sea seal,
ocean,
head,
sea,
cute

viaduct

hip, rose
hip,
rosehip

bridge,
sight,
arch,
river,
sky

fruit,
fall,
food,
little,
wildlife

dogsled,
dog
sled,
dog
sleigh

group together,
four,
sledge,
sled,
enjoyment

pug,
pug-dog

pug,
friendship,
adorable,
purebred,
sit

window
screen

stupa,
tope

hip, rose
hip,
rosehip

sea lion

12

Published as a conference paper at ICLR 2017

Old
English
sheep-
dog,
bobtail

maillot,
tank suit

patas,
hussar
monkey,
Erythro-
cebus
patas

poodle,
retriever,
loyalty,
sit,
two

beach,
woman,
adult,
wear,
portrait

primate,
monkey,
safari,
sit,
looking

abaya

amphib-
ian,
amphibi-
ous
vehicle

bee eater

veil,
spirituality,
religion,
people,
illustration

transportation
system,
vehicle,
man,
print,
retro

ornithology,
avian,
beak,
wing,
feather

Table 6: Original images and adversarial images evaluated over Clarifai.com. For labels returned
from Clarifai.com, we sort the labels ﬁrstly by rareness: how many times a label appears in the
Clarifai.com results for all adversarial images and original images, and secondly by conﬁdence.
Only top 5 labels are provided.

8 CONCLUSION

In this work, we are the ﬁrst to conduct an extensive study of the transferability of both non-targeted
and targeted adversarial examples generated using different approaches over large models and a
large scale dataset. Our results conﬁrm that the transferability for non-targeted adversarial exam-
ples are prominent even for large models and a large scale dataset. On the other hand, we ﬁnd that
it is hard to use existing approaches to generate targeted adversarial examples whose target labels
can transfer. We develop novel ensemble-based approaches, and demonstrate that they can gen-
erate transferable targeted adversarial examples with a high success rate. Meanwhile, these new
approaches exhibit better performance on generating non-targeted transferable adversarial examples
than previous work. We also show that both non-targeted and targeted adversarial examples gen-
erated using our new approaches can successfully attack Clarifai.com, which is a black-box image
classiﬁcation system. Furthermore, we study some geometric properties to better understand the
transferable adversarial examples.

This material is in part based upon work supported by the National Science Foundation under Grant
No. TWC-1409915. Any opinions, ﬁndings, and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily reﬂect the views of the National Science
Foundation.

ACKNOWLEDGMENTS

REFERENCES

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. arXiv

preprint arXiv:1608.04644, 2016.

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classiﬁers:
from adversarial to random noise. In Advances in Neural Information Processing Systems, pp.
1624–1632, 2016.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014.

13

Published as a conference paper at ICLR 2017

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-

nition. arXiv preprint arXiv:1512.03385, 2015.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,

abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. arXiv preprint arXiv:1610.08401, 2016.

Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against deep learning systems using adversarial examples.
arXiv preprint arXiv:1602.02697, 2016b.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.

J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine
learning algorithms for trafﬁc sign recognition. Neural Networks, (0):–, 2012. ISSN 0893-6080.
doi: 10.1016/j.neunet.2012.02.016. URL http://www.sciencedirect.com/science/
article/pii/S0893608012000457.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.

14

Published as a conference paper at ICLR 2017

APPENDIX

Top-1 accuracy
Top-5 accuracy

ResNet-50
72.5%
91.0%

ResNet-101
73.8%
91.7%

ResNet-152 GoogLeNet VGG-16
68.3%
88.3%

74.6%
92.1%

68.7%
89.0%

Table 7: Top-1 and top-5 accuracy of the studied models over the ILSVRC 2012 validation dataset.

An alternative optimization-based approach to generate adversarial examples. An alternative
method to generate non-targeted adversarial examples with large distortion is to revise the optimiza-
tion objective to incorporate this distortion constraint. For example, for non-targeted adversarial
image searching, we can optimize for the following objective.

argminx(cid:63) − log (1 − 1y · Jθ(x(cid:63))) + λ1ReLU(τ − d(x, x(cid:63))) + λ2ReLU(d(x, x(cid:63)) − τ )

Optimizing for
(1) minimizing
− log (1 − 1y · Jθ(x(cid:63))); (2) Penalizing the solution if d(x, x(cid:63)) is no more than a threshold τ (too
low); and (3) Penalizing the solution if d(x, x(cid:63)) is too high.

the following three effects:

the above objective has

In our preliminary evaluation, we found that the solutions computed from the two approaches have
similar transferability. We thus omit the results for this alternative approach.

Transferable non-targeted adversarial images are classiﬁed as the same wrong labels. Previ-
ous work Goodfellow et al. (2014) reported the phenomenon that when evaluating the adversarial
images, different models tend to make the same wrong predictions. This conclusion was only exam-
ined over datasets with 10 categories. In our evaluation, however, we observe the same phenomenon,
albeit we have 1000 possible categories. We refer to this effect as the same mistake effect.

Table 9 presents the results based on the adversarial examples generated for VGG-16. For each pair
of models, among all adversarial examples that both models make wrong predictions, we compute
the percentage that both models make the same mistake. These percentage numbers are from 12% to
40%, which are surprisingly high, since there are 999 possible categories to be misclassiﬁed into (see
Table 32 in the appendix for the wrong predicted label distribution of these adversarial examples).
Later in Section 6, we try to explain this phenomenon using decision boundaries.

Adversarial images may come from multiple intervals along the gradient direction.
In Fig-
ure 6, we show that along the gradient direction of the non-targeted objective (3) for VGG-16, when
evaluating the adversarial image xB on ResNet-152, xB will soon become adversarial for small B.
While B increases, however, the prediction changes back to be the correct label, then a wrong label,
then the correct label again, and ﬁnally wrong labels. This indicates that along the gradient of this
image, the distortions that can cause the corresponding adversarial images to mislead ResNet-152
form four intervals.

Comparison with random perturbations. For comparison, we evaluate the test accuracy when
we add a Gaussian noise to the 100 images in our test set. We vary the standard deviation of the
Gaussian noise from 5 to 40 with a step size of 5. For each speciﬁc standard deviation, we generate

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
22.83
23.81
22.86
22.51
22.58

7%
40%
48%
36%
66%

ResNet-101
43%
6%
44%
33%
71%

ResNet-50 VGG-16 GoogLeNet
39%
42%
42%
0%
49%

31%
34%
32%
15%
2%

43%
41%
3%
33%
62%

Table 8: Top-5 accuracy of Table 1 Panel A. Transferability between pairs of models using non-
targeted optimization-based approach with a learning rate of 4. The ﬁrst column indicates the av-
erage RMSD of all adversarial images generated for the model in the corresponding row. The cell
(i, j) indicates the top-5 accuracy of the adversarial images generated for model i (row) evaluated
over model j (column). Lower value indicates better transferability.

15

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
28.57%
29.87%
19.23%
12.82%

ResNet-101
−
100.00%
40.00%
18.07%
20.48%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
22.89% 100.00%
16.84%
18.07%

GoogLeNet
−
−
−
−
100.00%

Table 9: When using the optimization-based approach for VGG-16 model to generate non-targeted adversarial
images, cell (i, j) is the percentage of all transferable adversarial images that are predicted as the same wrong
labels by both models i and j over all adversarial images that are misclassiﬁed by both models i and j. Notice
that this table is symmetric.

(a) The original image

(b) Prediction correctness

Figure 6: We add Bδ onto the original image in Figure 6a, where δ is the normalized gradient of
the non-targeted objective (3) for VGG-16. When xB is evaluated on ResNet-152, we plot 1 if the
prediction matches the ground truth, or 0 otherwise.

100 random noises and add them to each image, resulting in 10,000 noisy images in total. Then
we evaluate the accuracy of each model on these 10,000 images, and the results are presented in
Table 11. Notice that when setting standard deviation to be 25, the average RMSD is 23.59, which
is comparable to that of non-targeted adversarial examples generated by either optimization-based
approaches or fast gradient-based approaches. However, each model can still achieve an accuracy
more than 66%. This shows that adding random noise is not an effective way to generate adversarial
examples, hence the “transferability” of this approach is signiﬁcantly worse than either optimization-
based approaches or fast gradient-based approaches.

More examples submitted to Clarifai.com. We present more results from Clarifai.com by sub-
mitting original and adversarial examples in Table 34.

original
image

true
label

Clarifai.com
results of
original image

target
label

targeted
adv-example

Clarifai.com result
of targeted
adversarial example

broom

jacamar

dust,
brick,
rustic,
stone,
dirty

frost,
sparrow,
pigeon,
ice,
frosty

heavy,
bulldozer,
exert,
track,
plow

junco,
snow-
bird

har-
vester,
reaper

eel

prairie
chicken,
prairie
grouse,
prairie
fowl

16

feather,
beautiful,
bird,
leaf,
ﬂora

swimming,
underwater,
ﬁsh,
water,
one

wildlife,
animal,
illustration,
nature,
color

Published as a conference paper at ICLR 2017

broccoli

hamster

mon-
goose

holster

yurt

water
buffalo,
water
ox,
Asiatic
buffalo,
Bubalus
bubalis

zebra

French
bulldog

curly-
coated
retriever

fox,
rodent,
predator,
fur,
park

pistol,
force,
bullet,
protection,
cartridge

wooden,
scenic,
snow,
rural,
landscape

kind,
cauliﬂower,
vitamin,
carrot,
cabbage

herd,
milk,
beef cattle,
farmland,
cow

equid,
stripe,
savanna,
zebra,
safari

bulldog,
studio,
boxer,
eye,
bull

eye,
looking,
pet,
canine,
dog

maillot

ground
beetle,
carabid
beetle

comic
book

rugby ball

apiary,
bee house

kite

ﬂy

17

motley,
shape,
horizontal,
abstract,
bright

shell,
shell (food),
antenna,
insect,
invertebrate

grafﬁti,
religion,
people,
painting,
culture

bird,
bright,
texture,
animal,
decoration

pastime,
print,
illustration,
art

wood,
people,
outdoors,
nature

visuals,
feather,
wing,
pet,
print

graphic,
shape,
insect,
artistic,
image

Table 34: Original images and adversarial images evaluated over Clarifai.com. For labels returned
from Clarifai.com, we sort the labels ﬁrstly by the occurrence of a label from the Clarifai.com results,
and secondly by conﬁdence. Only top 5 labels are provided.

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.45
23.49
23.49
23.73
23.45

23%
35%
40%
40%
53%

ResNet-101
29%
20%
39%
35%
46%

ResNet-50 VGG-16 GoogLeNet
39%
43%
39%
8%
38%

31%
33%
18%
33%
37%

33%
28%
33%
19%
7%

Table 10: Top-5 accuracy of Table 1 Panel B. Transferability between pairs of models using non-
targeted FG. The ﬁrst column indicates the average RMSD of all adversarial images generated for
the model in the corresponding row. The cell (i, j) indicates the top-5 accuracy of the adversarial
images generated for model i (row) evaluated over model j (column). Lower value indicates better
transferability.

Standard Deviation
5
10
15
20
25
30
35
40

RMSD ResNet-152
4.91
9.72
14.44
19.07
23.59
28.01
32.32
36.52

97.41%
95.53%
91.19%
86.56%
83.10%
78.95%
73.60%
66.53%

ResNet-101
98.96%
96.72%
94.22%
90.38%
85.53%
79.04%
70.89%
63.09%

ResNet-50 VGG-16 GoogLeNet
98.74% 97.47%
96.81% 92.13%
92.16% 87.86%
84.07% 82.30%
78.33% 73.57%
71.66% 65.33%
62.03% 58.55%
50.96% 51.85%

99.29%
95.94%
88.50%
77.84%
66.84%
54.93%
45.13%
35.61%

Table 11: Accuracy of images with random perturbation. The ﬁrst column reports the standard
deviation of the Gaussian noise added to each image. The second column reports the average RMSD
over all generated images with the respective standard deviation. For each of the rest column j, the
cell (i, j) reports model j’s accuracy of the noisy images when a Gaussian noise with the respective
standard deviation speciﬁed in row i is added to each image.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.13
23.16
23.06
23.59
22.87

100%
9%
10%
3%
1%

ResNet-101
11%
100%
9%
5%
2%

ResNet-50 VGG-16 GoogLeNet
3%
2%
2%
100%
3%

1%
1%
3%
4%
100%

5%
7%
100%
5%
1%

Table 12: Top-5 matching rate of Table 2. The adversarial images are generated using the targeted
optimization-based approach with a learning rate of 4. The ﬁrst column indicates the average RMSD
of all adversarial images generated for the model in the corresponding row. Cell (i, j) indicates that
top-5 matching rate of the targeted adversarial images generated for model i (row) when evaluated
on model j (column). Higher value indicates more successful transferable target labels.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.68
30.76
30.26
31.13
29.70

87%
100%
99%
100%
100%

ResNet-101
100%
88%
99%
100%
100%

ResNet-50 VGG-16 GoogLeNet
100%
100%
99%
51%
100%

100%
100%
99%
100%
32%

100%
100%
86%
100%
100%

Table 13: The top-5 matching rate of Table 3. Matching rate of adversarial images generated using
targeted optimization-based approach. The ﬁrst column indicates the average RMSD of all adver-
sarial images generated for the model in the corresponding row. Cell (i, j) indicates that top-5
matching rate of the targeted adversarial images generated using the ensemble of the four models
except model i (row) is predicted as the target label by model j (column). In each row, the minus
sign “−” indicates that the model of the row is not used when generating the attacks. Higher value
indicates more successful transferable target labels.

18

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.17
17.25
17.25
17.80
17.41

13%
2%
4%
4%
3%

ResNet-101
4%
11%
5%
7%
2%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
20%
0%

3%
3%
2%
4%
15%

4%
3%
11%
5%
3%

Table 14: Top-5 accuracy of Table 4. The ﬁrst column indicates the average RMSD of all adversarial
images generated for the model in the corresponding row. Cell (i, j) indicates that top-5 accuracy
of the non-targeted adversarial images generated using the ensemble of the four models except
model i (row) when evaluated over model j (column). In each row, the minus sign “−” indicates
that the model of the row is not used when generating the attacks. Lower value indicates better
transferability.

RMSD ResNet-152

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

1.25
1.24
1.21
1.55
1.27

0%
84%
90%
89%
94%

ResNet-101
86%
0%
91%
94%
97%

ResNet-50 VGG-16 GoogLeNet
93%
95%
91%
0%
91%

96%
100%
97%
84%
0%

87%
93%
0%
92%
98%

Table 15: Transferability between pairs of models using non-targeted optimization-based approach
with a learning rate of 0.125. The ﬁrst column indicates the average RMSD of all adversarial images
generated for the model in the corresponding row. The cell (i, j) indicates the top-1 accuracy of the
adversarial images generated for model i (row) evaluated over model j (column). Lower value
indicates better transferability. Results of top-5 accuracy can be found in Table 16.

RMSD ResNet-152

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

1.25
1.24
1.21
1.55
1.27

24%
100%
100%
98%
100%

ResNet-101
100%
22%
99%
100%
100%

ResNet-50 VGG-16 GoogLeNet
99%
100%
100%
15%
100%

100%
100%
100%
100%
18%

100%
99%
25%
100%
100%

Table 16: Top-5 accuracy of Table 15. Transferability between pairs of models using non-targeted
optimization-based approach with a learning rate of 0.125. The ﬁrst column indicates the average
RMSD of all adversarial images generated for the model in the corresponding row. The cell (i, j)
indicates the top-5 accuracy of the adversarial images generated for model i (row) evaluated over
model j (column). Lower value indicates better transferability.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
31.35
31.11
31.32
31.50
30.67

98%
5%
3%
1%
2%

ResNet-101
1%
98%
2%
1%
1%

ResNet-50 VGG-16 GoogLeNet
2%
2%
1%
97%
2%

2%
1%
99%
2%
0%

0%
0%
1%
1%
97%

Table 17: The adversarial images are generated using the targeted optimization-based approach
with a larger learning rate. The ﬁrst column indicates the average RMSD of all adversarial images
generated for the model in the corresponding row. Cell (i, j) indicates that top-1 matching rate of
the targeted adversarial images generated for model i (row) when evaluated on model j (column).
Higher value indicates more successful transferable target labels. We used a larger learning rate to
achieve larger RMSD. Results of top-5 matching rate can be found in Table 18.

19

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
31.35
31.11
31.32
31.50
30.67

98%
10%
6%
4%
3%

ResNet-101
8%
98%
7%
4%
1%

ResNet-50 VGG-16 GoogLeNet
3%
3%
5%
97%
3%

6%
5%
99%
6%
2%

1%
1%
1%
4%
97%

Table 18: The top-5 matching rate of Table 17. The adversarial images are generated using the
targeted optimization-based approach with a larger learning rate. The ﬁrst column indicates the
average RMSD of all adversarial images generated for the model in the corresponding row. Cell
(i, j) indicates that top-5 matching rate of the targeted adversarial images generated for model i
(row) when evaluated on model j (column). Higher value indicates more successful transferable
target labels. We used a larger learning rate to achieve larger RMSD.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.11
23.11
23.11
23.12
23.11

12%
29%
34%
25%
46%

ResNet-101
27%
13%
28%
20%
41%

ResNet-50 VGG-16 GoogLeNet
22%
29%
25%
0%
25%

25%
29%
10%
23%
40%

15%
16%
23%
8%
2%

Table 19: Transferability between pairs of models using non-targeted FGS. The ﬁrst column indi-
cates the average RMSD of all adversarial images generated for the model in the corresponding row.
The cell (i, j) indicates the top-1 accuracy of the adversarial images generated for model i (row)
evaluated over model j (column). Lower value indicates better transferability. Results of top-5
accuracy can be found in Table 20.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.11
23.11
23.11
23.12
23.11

32%
56%
59%
42%
71%

ResNet-101
55%
33%
53%
39%
74%

ResNet-50 VGG-16 GoogLeNet
47%
46%
47%
5%
53%

53%
50%
29%
41%
62%

36%
40%
38%
21%
11%

Table 20: Top-5 accuracy of Table 19. Transferability between pairs of models using non-targeted
FGS. The ﬁrst column indicates the average RMSD of all adversarial images generated for the
model in the corresponding row. The cell (i, j) indicates the top-5 accuracy of the adversarial
images generated for model i (row) evaluated over model j (column). Lower value indicates better
transferability.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.25
17.24
17.24
17.24
17.24

23%
15%
15%
17%
14%

ResNet-101
12%
19%
13%
15%
11%

ResNet-50 VGG-16 GoogLeNet
1%
2%
2%
23%
2%

11%
11%
19%
12%
10%

7%
6%
8%
7%
19%

Table 21: Transferability between pairs of models using non-targeted ensemble FG. The ﬁrst column
indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that top-1
accuracy of the non-targeted adversarial images generated using the ensemble of the four models
except model i (row) when evaluated over model j (column). In each row, the minus sign “−”
indicates that the model of the row is not used when generating the attacks. Lower value indicates
better transferability. Results of top-5 accuracy can be found in Table 22.

20

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.25
17.24
17.24
17.24
17.24

47%
37%
38%
37%
32%

ResNet-101
33%
42%
32%
36%
30%

ResNet-50 VGG-16 GoogLeNet
15%
14%
13%
44%
13%

28%
30%
39%
37%
28%

21%
26%
23%
32%
46%

Table 22: Top-5 accuracy of Table 21. Transferability between pairs of models using non-targeted
ensemble FG. The ﬁrst column indicates the average RMSD of the generated adversarial images.
Cell (i, j) indicates that top-5 accuracy of the non-targeted adversarial images generated using the
ensemble of the four models except model i (row) when evaluated over model j (column). In each
row, the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Lower value indicates better transferability.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.41
17.40
17.40
17.40
17.40

26%
18%
15%
17%
15%

ResNet-101
10%
21%
13%
16%
12%

ResNet-50 VGG-16 GoogLeNet
2%
2%
4%
23%
3%

7%
4%
8%
7%
22%

11%
13%
20%
11%
10%

Table 23: Transferability between pairs of models using non-targeted ensemble FGS. The ﬁrst col-
umn indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that top-1
accuracy of the non-targeted adversarial images generated using the ensemble of the four models
except model i (row) when evaluated over model j (column). In each row, the minus sign “−” indi-
cates that the model of the row is not used when generating the attacks. Lower value indicates better
transferability. Results of top-5 accuracy can be found in Table 24.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.41
17.40
17.40
17.40
17.40

50%
38%
38%
37%
35%

ResNet-101
35%
48%
36%
36%
29%

ResNet-50 VGG-16 GoogLeNet
20%
19%
18%
48%
19%

26%
26%
28%
33%
53%

30%
33%
41%
36%
31%

Table 24: Top-5 accuracy of Table 23. Transferability between pairs of models using non-targeted
ensemble FGS. The ﬁrst column indicates the average RMSD of the generated adversarial images.
Cell (i, j) indicates that top-5 accuracy of the non-targeted adversarial images generated using the
ensemble of the four models except model i (row) when evaluated over model j (column). In each
row, the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Lower value indicates better transferability.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.55
23.56
23.56
23.95
23.63

1%
1%
1%
1%
1%

ResNet-101
2%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
1%
1%

0%
0%
1%
0%
0%

1%
1%
0%
1%
1%

Table 25: The adversarial images are generated using the targeted FG. The ﬁrst column indicates the
average RMSD of all adversarial images generated for the model in the corresponding row. The ﬁrst
column indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that
top-1 matching rate of the targeted adversarial images generated for model i (row) when evaluated
on model j (column). Higher value indicates more successful transferable target labels.

21

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
33.33%
24.00%
17.50%
15.38%

ResNet-101
−
100.00%
35.00%
19.05%
14.81%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
21.18% 100.00%
15.05%
13.10%

GoogLeNet
−
−
−
−
100.00%

Table 26: When using non-targeted FG for VGG-16 model to generate adversarial images, cell (i, j)
is the percentage of all transferable adversarial images that are predicted as the same wrong labels
by both models i and j over all adversarial images that are misclassiﬁed by both models i and j.
Notice that the table is symmetric.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
33.78%
36.62%
16.00%
14.86%

ResNet-101
−
100.00%
43.24%
20.00%
20.25%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
23.38% 100.00%
19.57%
13.16%

GoogLeNet
−
−
−
−
100.00%

Table 27: When using non-targeted FGS for VGG-16 model to generate adversarial images, cell
(i, j) is the percentage of all transferable adversarial images that are predicted as the same wrong
labels by both models i and j over all adversarial images that are misclassiﬁed by both models i and
j. Notice that the table is symmetric.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
31.05
30.94
31.12
30.57
30.47

1%
1%
1%
1%
1%

ResNet-101
1%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
1%

0%
0%
0%
0%
0%

1%
0%
1%
1%
0%

Table 28: Transferability between pairs of models using targeted ensemble FG. The ﬁrst column
indicates the average RMSD of all adversarial images generated for the model in the corresponding
row. Cell (i, j) indicates that top-1 matching rate of the targeted adversarial images generated using
the ensemble of the four models except model i (row) is predicted as the target label by model j
(column). In each row, the minus sign “−” indicates that the model of the row is not used when
generating the attacks. Higher value indicates more successful transferable target labels. Results of
top-5 matching rate can be found in Table 29.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
31.05
30.94
31.12
30.57
30.47

1%
1%
1%
2%
1%

ResNet-101
2%
1%
2%
2%
1%

ResNet-50 VGG-16 GoogLeNet
3%
1%
3%
1%
2%

1%
1%
1%
2%
1%

2%
2%
2%
1%
2%

Table 29: Top-5 matching rate of Table 28. Transferability between pairs of models using targeted
ensemble FG. The ﬁrst column indicates the average RMSD of all adversarial images generated
for the model in the corresponding row. Cell (i, j) indicates that top-5 matching rate of the targeted
adversarial images generated using the ensemble of the four models except model i (row) is predicted
as the target label by model j (column). In each row, the minus sign “−” indicates that the model of
the row is not used when generating the attacks. Higher value indicates more successful transferable
target labels.

22

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.42
30.42
30.42
30.41
30.42

1%
1%
1%
1%
1%

ResNet-101
1%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
1%

1%
1%
0%
0%
1%

1%
1%
1%
1%
1%

Table 30: Transferability between pairs of models using targeted ensemble FGS. The ﬁrst column
indicates the average RMSD of all adversarial images generated for the model in the corresponding
row. Cell (i, j) indicates that top-1 matching rate of the targeted adversarial images generated using
the ensemble of the four models except model i (row) is predicted as the target label by model j
(column). In each row, the minus sign “−” indicates that the model of the row is not used when
generating the attacks. Higher value indicates more successful transferable target labels. Results are
top-5 matching rate can be found in Table 31.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.42
30.42
30.42
30.41
30.42

2%
2%
2%
3%
2%

ResNet-101
1%
1%
1%
2%
2%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
2%

2%
2%
2%
1%
2%

2%
1%
1%
2%
2%

Table 31: Top-5 matching rate of Table 30. Transferability between pairs of models using targeted
ensemble FGS. The ﬁrst column indicates the average RMSD of all adversarial images generated
for the model in the corresponding row. Cell (i, j) indicates that top-5 matching rate of the targeted
adversarial images generated using the ensemble of the four models except model i (row) is predicted
as the target label by model j (column). In each row, the minus sign “−” indicates that the model of
the row is not used when generating the attacks. Higher value indicates more successful transferable
target labels.

ResNet-152
jigsaw puzzle(8%)
acorn(3%)
lycaenid(2%)
ram(2%)
maze(2%)

ResNet-101
jigsaw puzzle(12%)
starﬁsh(3%)
strawberry(2%)
wild boar(2%)
dishrag(2%)

ResNet-50
jigsaw puzzle(12%)

VGG-16
jigsaw puzzle(15%)

African chameleon(2%) African chameleon(7%)

strawberry(2%)
starﬁsh(2%)
greenhouse(2%)

prayer rug(5%)
apron(4%)
sarong(3%)

GoogleNet
prayer rug(12%)
jigsaw puzzle(7%)
stole(6%)
African chameleon(4%)
mitten(3%)

Table 32: When using non-targeted optimization-based approach for VGG-16 model to generate
adversarial images, column i indicates the top 5 common incorrect labels predicted by model i. The
value in the parentheses is the percentage of the predicted label.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
1.00
0.04
0.03
0.02
0.01

ResNet-101
−
1.00
0.03
0.02
0.01

ResNet-50 VGG-16 GoogLeNet
−
−
−
1.00
0.02

−
−
−
−
1.00

−
−
1.00
0.02
0.01

Table 33: Average cosine value of the angle between gradient directions of two models. Notice
that the dot-product of two normalized vectors is the cosine value of the angle between them, for
each image, we compute the dot-product of normalized gradient directions with respect to model i
(row) and model j (column), and the value in cell (i, j) is the average over dot-product values of all
images. Notice that this table is symmetric.

23

Published as a conference paper at ICLR 2017

(a) GoogLeNet

(b) VGG-16

(c) ResNet-152

(d) ResNet-101

(e) ResNet-50

Figure 7: Linearity of different models. Each line plots the classiﬁcation layer’s softmax input vs
distortion B when fast gradient noises are used. Solid red line is the ground truth label, and other
lines are the top 10 labels predicted on the original image.

24

7
1
0
2
 
b
e
F
 
7
 
 
]

G
L
.
s
c
[
 
 
3
v
0
7
7
2
0
.
1
1
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

DELVING INTO TRANSFERABLE ADVERSARIAL EX-
AMPLES AND BLACK-BOX ATTACKS

Yanpei Liu∗, Xinyun Chen∗
Shanghai Jiao Tong University

Chang Liu, Dawn Song
University of the California, Berkeley

ABSTRACT

An intriguing property of deep neural networks is the existence of adversarial ex-
amples, which can transfer among different architectures. These transferable ad-
versarial examples may severely hinder deep neural network-based applications.
Previous works mostly study the transferability using small scale datasets. In this
work, we are the ﬁrst to conduct an extensive study of the transferability over
large models and a large scale dataset, and we are also the ﬁrst to study the trans-
ferability of targeted adversarial examples with their target labels. We study both
non-targeted and targeted adversarial examples, and show that while transferable
non-targeted adversarial examples are easy to ﬁnd, targeted adversarial examples
generated using existing approaches almost never transfer with their target labels.
Therefore, we propose novel ensemble-based approaches to generating transfer-
able adversarial examples. Using such approaches, we observe a large proportion
of targeted adversarial examples that are able to transfer with their target labels for
the ﬁrst time. We also present some geometric studies to help understanding the
transferable adversarial examples. Finally, we show that the adversarial examples
generated using ensemble-based approaches can successfully attack Clarifai.com,
which is a black-box image classiﬁcation system.

1

INTRODUCTION

Recent research has demonstrated that for a deep architecture, it is easy to generate adversarial
examples, which are close to the original ones but are misclassiﬁed by the deep architecture (Szegedy
et al. (2013); Goodfellow et al. (2014)). The existence of such adversarial examples may have severe
consequences, which hinders vision-understanding-based applications, such as autonomous driving.
Most of these studies require explicit knowledge of the underlying models.
It remains an open
question how to efﬁciently ﬁnd adversarial examples for a black-box model.

Several works have demonstrated that some adversarial examples generated for one model may
also be misclassiﬁed by another model. Such a property is referred to as transferability, which
can be leveraged to perform black-box attacks. This property has been exploited by constructing
a substitute of the black-box model, and generating adversarial instances against the substitute to
attack the black-box system (Papernot et al. (2016a;b)). However, so far, transferability is mostly
examined over small datasets, such as MNIST (LeCun et al. (1998)) and CIFAR-10 (Krizhevsky &
Hinton (2009)). It has yet to be better understood transferability over large scale datasets, such as
ImageNet (Russakovsky et al. (2015)).

In this work, we are the ﬁrst to conduct an extensive study of the transferability of different adver-
sarial instance generation strategies applied to different state-of-the-art models trained over a large
scale dataset. In particular, we study two types of adversarial examples: (1) non-targeted adversar-
ial examples, which can be misclassiﬁed by a network, regardless of what the misclassiﬁed labels
may be; and (2) targeted adversarial examples, which can be classiﬁed by a network as a target
label. We examine several existing approaches searching for adversarial examples based on a single
model. While non-targeted adversarial examples are more likely to transfer, we observe few targeted
adversarial examples that are able to transfer with their target labels.

∗Work is done while visiting UC Berkeley.

1

Published as a conference paper at ICLR 2017

We further propose a novel strategy to generate transferable adversarial images using an ensemble
of multiple models. In our evaluation, we observe that this new strategy can generate non-targeted
adversarial instances with better transferability than other methods examined in this work. Also, for
the ﬁrst time, we observe a large proportion of targeted adversarial examples that are able to transfer
with their target labels.

We study geometric properties of the models in our evaluation.
In particular, we show that the
gradient directions of different models are orthogonal to each other. We also show that decision
boundaries of different models align well with each other, which partially illustrates why adversarial
examples can transfer.

Last, we study whether generated adversarial images can attack Clarifai.com, a commercial com-
pany providing state-of-the-art image classiﬁcation services. We have no knowledge about the train-
ing dataset and the types of models used by Clarifai.com; meanwhile, the label set of Clarifai.com
is quite different from ImageNet’s. We show that even in this case, both non-targeted and targeted
adversarial images transfer to Clarifai.com. This is the ﬁrst work documenting the success of gen-
erating both non-targeted and targeted adversarial examples for a black-box state-of-the-art online
image classiﬁcation system, whose model and training dataset are unknown to the attacker.

Contributions and organization. We summarize our main contributions as follows:

• For ImageNet models, we show that while existing approaches are effective to generate
non-targeted transferable adversarial examples (Section 3), only few targeted adversarial
examples generated by existing methods can transfer (Section 4).

• We propose novel ensemble-based approaches to generate adversarial examples (Sec-
tion 5). Our approaches enable a large portion of targeted adversarial examples to transfer
among multiple models for the ﬁrst time.

• We are the ﬁrst to present that targeted adversarial examples generated for models trained
on ImageNet can transfer to a black-box system, i.e., Clarifai.com, whose model, training
data, and label set is unknown to us (Section 7). In particular, Clarifai.com’s label set is
very different from ImageNet’s.

• We conduct the ﬁrst analysis of geometric properties for large models trained over Ima-
geNet (Section 6), and the results reveal several interesting ﬁndings, such as the gradient
directions of different models are orthogonal to each other.

In the following, we ﬁrst discuss related work, and then present the background knowledge and
experiment setup in Section 2. Then we present each of our experiments and conclusions in the
corresponding section as mentioned above.

Related work. Transferability of adversarial examples was ﬁrst examined by Szegedy et al.
(2013), which studied the transferability (1) between different models trained over the same dataset;
and (2) between the same or different model trained over disjoint subsets of a dataset; However,
Szegedy et al. (2013) only studied MNIST.

The study of transferability was followed by Goodfellow et al. (2014), which attributed the phe-
nomenon of transferability to the reason that the adversarial perturbation is highly aligned with the
weight vector of the model. Again, this hypothesis was tested using MNIST and CIFAR-10 datasets.
We show that this is not the case for models trained over ImageNet.

Papernot et al. (2016a;b) examined constructing a substitute model to attack a black-box target
model. To train the substitute model, they developed a technique that synthesizes a training set and
annotates it by querying the target model for labels. They demonstrate that using this approach,
black-box attacks are feasible towards machine learning services hosted by Amazon, Google, and
MetaMind. Further, Papernot et al. (2016a) studied the transferability between deep neural networks
and other models such as decision tree, kNN, etc.

Our work differs from Papernot et al. (2016a;b) in three aspects. First, in these works, only the model
and the training process are a black box, but the training set and the test set are controlled by the
attacker; in contrast, we attack Clarifai.com, whose model, training data, training process, and even
the test label set are unknown to the attacker. Second, the datasets studied in these works are small

2

Published as a conference paper at ICLR 2017

scale, i.e., MNIST and GTSRB (Stallkamp et al. (2012)); in our work, we study the transferability
over larger models and a larger dataset, i.e., ImageNet. Third, to attack black-box machine learning
systems, we do not query the systems for constructing the substitute model ourselves.

In a concurrent and independent work, Moosavi-Dezfooli et al. (2016) showed the existence of a
universal perturbation for each model, which can transfer across different images. They also show
that the adversarial images generated using these universal perturbations can transfer across different
models on ImageNet. However, they only examine the non-targeted transferability, while our work
studies both non-targeted and targeted transferability over ImageNet.

2 ADVERSARIAL DEEP LEARNING AND TRANSFERABILITY

2.1 THE ADVERSARIAL DEEP LEARNING PROBLEM

We assume a classiﬁer fθ(x) outputs a category (or a label) as the prediction. Given an original
image x, with ground truth label y, the adversarial deep learning problem is to seek for adversarial
examples for the classiﬁer fθ(x). Speciﬁcally, we consider two classes of adversarial examples.
A non-targeted adversarial example x(cid:63) is an instance that is close to x, in which case x(cid:63) should
have the same ground truth as x, while fθ(x(cid:63)) (cid:54)= y. For the problem to be non-trivial, we assume
fθ(x) = y without loss of generality. A targeted adversarial example x(cid:63) is close to x and satisﬁes
fθ(x(cid:63)) = y(cid:63), where y(cid:63) is a target label speciﬁed by the adversary, and y(cid:63) (cid:54)= y.

2.2 APPROACHES FOR GENERATING ADVERSARIAL EXAMPLES

In this work, we consider three classes of approaches for generating adversarial examples:
optimization-based approaches, fast gradient approaches, and fast gradient sign approaches. Each
class has non-targeted and targeted versions respectively.

2.2.1 APPROACHES FOR GENERATING NON-TARGETED ADVERSARIAL EXAMPLES

Formally, given an image x with ground truth y = fθ(x), searching for a non-targeted adversarial
example can be modeled as searching for an instance x(cid:63) to satisfy the following constraints:

fθ(x(cid:63)) (cid:54)= y
d(x, x(cid:63)) ≤ B

(1)
(2)

where d(·, ·) is a metric to quantify the distance between an original image and its adversarial coun-
terpart, and B, called distortion, is an upper bound placed on this distance. Without loss of gener-
ality, we consider model f is composed of a network Jθ(x), which outputs the probability for each
category, so that f outputs the category with the highest probability.

Optimization-based approach. One approach is to approximate the solution to the following
optimization problem:

argminx(cid:63) λd(x, x(cid:63)) − (cid:96)(1y, Jθ(x(cid:63)))
(3)
where 1y is the one-hot encoding of the ground truth label y, (cid:96) is a loss function to measure the
distance between the prediction and the ground truth, and λ is a constant to balance constraints (2)
and (1), which is empirically determined. Here, loss function (cid:96) is used to approximate constraint (1),
and its choice can affect the effectiveness of searching for an adversarial example. In this work, we
choose (cid:96)(u, v) = log (1 − u · v), which is shown to be effective by Carlini & Wagner (2016).

Fast gradient sign (FGS). Goodfellow et al. (2014) proposed the fast gradient sign (FGS) method
so that the gradient needs be computed only once to generate an adversarial example. FGS can be
used to generate adversarial images to meet the L∞ norm bound. Formally, non-targeted adversarial
examples are constructed as

x(cid:63) ← clip(x + Bsgn(∇x(cid:96)(1y, Jθ(x))))
Here, clip(x) is used to clip each dimension of x to the range of pixel values, i.e., [0, 255] in this
work. We make a slight variation to choose (cid:96)(u, v) = log (1 − u · v), which is the same as used in
the optimization-based approach.

3

Published as a conference paper at ICLR 2017

Fast gradient (FG). The fast gradient approach (FG) is similar to FGS, but instead of moving
along the gradient sign direction, FG moves along the gradient direction. In particular, we have

x(cid:63) ← clip(x + B

∇x(cid:96)(1y, Jθ(x))
||∇x(cid:96)(1y, Jθ(x))||

))

Here, we assume the distance metric in constraint (2), d(x, x(cid:63)) = ||x − x(cid:63)|| is a norm of x − x(cid:63).
The term sgn(∇x(cid:96)) in FGS is replaced by ∇x(cid:96)

||∇x(cid:96)|| to meet this distance constraint.

We call both FGS and FG fast gradient-based approaches.

2.2.2 APPROACHES FOR GENERATING TARGETED ADVERSARIAL EXAMPLES

A targeted adversarial image x(cid:63) is similar to a non-targeted one, but constraint (1) is replaced by
fθ(x(cid:63)) = y(cid:63)

(4)
where y(cid:63) is the target label given by the adversary. For the optimization-based approach, we ap-
proximate the solution by solving the following dual objective:

argminx(cid:63) λd(x, x(cid:63)) + (cid:96)(cid:48)(1y(cid:63) , Jθ(x(cid:63)))

(5)

In this work, we choose the standard cross entropy loss (cid:96)(cid:48)(u, v) = − (cid:80)
i

ui log vi.

For FGS and FG, we construct adversarial examples as follows:

x(cid:63) ← clip(x − Bsgn(∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))))

(FGS)

x(cid:63) ← clip(x − B

∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))
||∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))||

)

(FG)

where (cid:96)(cid:48) is the same as the one used for the optimization-based approach.

2.3 EVALUATION METHODOLOGY

For the rest of the paper, we focus on examining the transferability among state-of-the-art models
trained over ImageNet (Russakovsky et al. (2015)).
In this section, we detail the models to be
examined, the dataset to be evaluated, and the measurements to be used.

Models. We examine ﬁve networks, ResNet-50, ResNet-101, ResNet-152 (He et al. (2015))1,
GoogLeNet (Szegedy et al. (2014))2, and VGG-16 (Simonyan & Zisserman (2014))3. We retrieve
the pre-trained models for each network online. The performance of these models on the ILSVRC
2012 (Russakovsky et al. (2015)) validation set are presented in the appendix (Table 7). We choose
these models to study the transferability between homogeneous architectures (i.e., ResNet models)
and heterogeneous architectures.

Dataset.
It is less meaningful to examine the transferability of an adversarial image between two
models which cannot classify the original image correctly. Therefore, from the ILSVRC 2012 val-
idation set, we randomly choose 100 images, which can be classiﬁed correctly by all ﬁve models
in our examination. These 100 images form our test set. To perform targeted attacks, we manually
choose a target label for each image, so that its semantics is far from the ground truth. The images
and target labels in our evaluation can be found on website4.

Measuring transferability. Given two models, we measure the non-targeted transferability by
computing the percentage of the adversarial examples generated for one model that can be classiﬁed
correctly for the other. We refer to this percentage as accuracy. A lower accuracy means better
non-targeted transferability. We measure the targeted transferability by computing the percentage of
the adversarial examples generated for one model that are classiﬁed as the target label by the other
model. We refer to this percentage as matching rate. A higher matching rate means better targeted
transferability. For clarity, the reported results are only based on top-1 accuracy. Top-5 accuracy’s
counterparts can be found in the appendix.

1https://github.com/KaimingHe/deep-residual-networks
2https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet
3https://gist.github.com/ksimonyan/211839e770f7b538e2d8
4https://github.com/sunblaze-ucb/transferability-advdnn-pub

4

Published as a conference paper at ICLR 2017

Distortion. Besides transferability, another important factor is the distortion between adversarial
images and the original ones. We measure the distortion by root mean square deviation, i.e., RMSD,
which is computed as d(x(cid:63), x) = (cid:112)(cid:80)
i − xi)2/N , where x(cid:63) and x are the vector representations
of an adversarial image and the original one respectively, N is the dimensionality of x and x(cid:63), and
xi denotes the pixel value of the i-th dimension of x, within range [0, 255], and similar for x(cid:63)
i .

i(x(cid:63)

3 NON-TARGETED ADVERSARIAL EXAMPLES

In this section, we examine different approaches for generating non-targeted adversarial images.

3.1 OPTIMIZATION-BASED APPROACH

To apply the optimization-based approach for a single model, we initialize x(cid:63) to be x and use Adam
Optimizer (Kingma & Ba (2014)) to optimize Objective (3) . We ﬁnd that we can tune the RMSD
by adjusting the learning rate of Adam and λ. We ﬁnd that, for each model, we can use a small
learning rate to generate adversarial images with small RMSD, i.e. < 2, with any λ. In fact, we ﬁnd
that when initializing x(cid:63) with x, Adam Optimizer will search for an adversarial example around x,
even when we set λ to be 0, i.e., not restricting the distance between x(cid:63) and x. Therefore, we set
λ to be 0 for all experiments using optimization-based approaches throughout the paper. Although
these adversarial examples with small distortions can successfully fool the target model, however,
they cannot transfer well to other models (see Table 15 and 16 in the appendix for details).

We increase the learning rate to allow the optimization algorithm to search for adversarial images
with larger distortion. In particular, we set the learning rate to be 4. We run Adam Optimizer for 100
iterations to generate the adversarial images. We observe that the loss converges after 100 iterations.
An alternative optimization-based approach leading to similar results can be found in the appendix.

Non-targeted adversarial examples transfer. We generate non-targeted adversarial examples on
one network, but evaluate them on another, and Table 1 Panel A presents the results. From the table,
we can observe that

• The diagonal contains all 0 values. This says that all adversarial images generated for one

model can mislead the same model.

• A large proportion of non-targeted adversarial images generated for one model using the

optimization-based approach can transfer to another.

• Although the three ResNet models share similar architectures which differ only in the hy-
perparameters, adversarial examples generated against a ResNet model do not necessarily
transfer to another ResNet model better than other non-ResNet models. For example, the
adversarial examples generated for VGG-16 have lower accuracy on ResNet-50 than those
generated for ResNet-152 or ResNet-101.

3.2 FAST GRADIENT-BASED APPROACHES

We then examine the effectiveness of fast gradient-based approaches. A good property of fast
gradient-based approaches is that all generated adversarial examples lie in a 1-D subspace. There-
fore, we can easily approximate the minimal distortion in this subspace of transferable adversarial
examples between two models. In the following, we ﬁrst control the RMSD to study fast gradient-
based approaches’ effectiveness. Second, we study the transferable minimal distortions of fast
gradient-based approaches.

3.2.1 EFFECTIVENESS AND TRANSFERABILITY OF THE FAST GRADIENT-BASED

APPROACHES

Since the distortion B and the RMSD of the generated adversarial images are highly correlated, we
can choose this hyperparameter B to generate adversarial images with a given RMSD. In Table 1
Panel B, we generate adversarial images using FG such that the average RMSD is almost the same
as those generated using the optimization-based approach. We observe that the diagonal values in

5

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
22.83
23.81
22.86
22.51
22.58

ResNet-101
13%
0%
20%
17%
38%
Panel A: Optimization-based approach

ResNet-50 VGG-16 GoogLeNet
19%
21%
21%
0%
19%

0%
19%
23%
22%
39%

18%
21%
0%
17%
34%

11%
12%
18%
5%
0%

RMSD
23.45
23.49
23.49
23.73
23.45

ResNet-152
ResNet-101
4%
13%
13%
19%
4%
11%
25%
19%
5%
20%
16%
15%
17%
25%
25%
Panel B: Fast gradient approach

ResNet-50 VGG-16 GoogLeNet
20%
23%
25%
1%
19%

12%
13%
14%
7%
1%

Table 1: Transferability of non-targeted adversarial images generated between pairs of models. The
ﬁrst column indicates the average RMSD of all adversarial images generated for the model in the
corresponding row. The cell (i, j) indicates the accuracy of the adversarial images generated for
model i (row) evaluated over model j (column). Results of top-5 accuracy can be found in the
appendix (Table 8 and Table 10).

the table are all positive, which means that FG cannot fully mislead the models. A potential reason
is that, FG can be viewed as approximating the optimization, but is tailored for speed over accuracy.

On the other hand, the values of non-diagonal cells in the table, which correspond to the accuracies
of adversarial images generated for one model but evaluated on another, are comparable with or less
than their counterparts in the optimization-based approach. This shows that non-targeted adversarial
examples generated by FG exhibit transferability as well.

We also evaluate FGS, but the transferability of the generated images is worse than the ones gener-
ated using either FG or optimization-based approaches. The results are in the appendix (Table 19
and 20). It shows that when RMSD is around 23, the accuracies of the adversarial images generated
by FGS is greater than their counterparts for FG. We hypothesize the reason why transferability of
FGS is worse to this fact.

3.2.2 ADVERSARIAL IMAGES WITH MINIMAL TRANSFERABLE RMSD

For an image x and two models M1, M2, we can approximate the minimal distortion B along a
direction δ, such that xB = x + Bδ generated for M1 is adversarial for both M1 and M2. Here δ is
the direction, i.e., sgn(∇x(cid:96)) for FGS, and ∇x(cid:96)/||∇x(cid:96)|| for FG.

We refer to the minimal transferable RMSD from M1 to M2 using FG (or FGS) as the RMSD of
a transferable adversarial example xB with the minimal transferable distortion B from M1 to M2
using FG (or FGS). The minimal transferable RMSD can illustrate the tradeoff between distortion
and transferability.

In the following, we approximate the minimal transferable RMSD through a linear search by sam-
pling B every 0.1 step. We choose the linear-search method rather than binary-search method to de-
termine the minimal transferable RMSD because the adversarial images generated from an original
image may come from multiple intervals. The experiment can be found in the appendix (Figure 6).

Minimal transferable RMSD using FG and FGS. Figure 1 plots the cumulative distribution
function (CDF) of the minimal transferable RMSD from VGG-16 to ResNet-152 using non-targeted
FG (Figure 1a) and FGS (Figure 1b). From the ﬁgures, we observe that both FG and FGS can ﬁnd
100% transferable adversarial images with RMSD less than 80.91 and 86.56 respectively. Further,
the FG method can generate transferable attacks with smaller RMSD than FGS. A potential rea-
son is that while FGS minimizes the distortion’s L∞ norm, FG minimizes its L2 norm, which is
proportional to RMSD.

6

Published as a conference paper at ICLR 2017

(a) Fast Gradient

(b) Fast Gradient Sign

Figure 1: The CDF of the minimal transferable RMSD from VGG-16 to ResNet-152 using FG (a)
and FGS (b). The green line labels the median minimal transferable RMSD, while the red line labels
the minimal transferable RMSD to reach 90% percentage.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.13
23.16
23.06
23.59
22.87

100%
3%
4%
2%
1%

ResNet-101
2%
100%
2%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
2%
1%
100%
1%

1%
3%
100%
2%
0%

1%
1%
1%
1%
100%

Table 2: The matching rate of targeted adversarial images generated using the optimization-based
approach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) indicates that matching rate of the targeted adversarial images generated for model i (row)
when evaluated on model j (column). The top-5 results can be found in the appendix (Table 12).

3.3 COMPARISON WITH RANDOM PERTURBATIONS

We also evaluate the test accuracy when we add a Gaussian noise to the 100 images in our test
set. The concrete results can be found in the appendix, and we show the conclusion that the “trans-
ferability” of this approach is signiﬁcantly worse than either optimization-based approaches or fast
gradient-based approaches.

4 TARGETED ADVERSARIAL EXAMPLES

In this section, we examine the transferability of targeted adversarial images. Table 2 presents
the results for using optimization-based approach. We observe that (1) the prediction of targeted
adversarial images can match the target labels when evaluated on the same model that is used to
generate the adversarial examples; but (2) the targeted adversarial images can be rarely predicted
as the target labels by a different model. We call the latter that the target labels do not transfer.
Even when we increase the distortion, we still do not observe improvements on making target label
transfer. Some results can be found in the appendix (Table 17). Even if we compute the matching
rate based on top-5 accuracy, the highest matching rate is only 10%. The results can be found in the
appendix (Table 18).

We also examine the targeted adversarial images generated by fast gradient-based approaches, and
we observe that the target labels do not transfer as well. The results are deferred to the appendix
(Table 25). In fact, most targeted adversarial images cannot mislead the model, for which the ad-
versarial images are generated, to predict the target labels, regardless of how large the distortion is
used. We attribute it to the fact that the fast gradient-based approaches only search for attacks in
a 1-D subspace. In this subspace, the total possible predictions may contain a small subset of all
labels, which usually does not contain the target label. In Section 6, we study decision boundaries
regarding this issue.

We also evaluate the matching rate of images added with Gaussian noise, as described in Section 3.3.
However, we observe that the matching rate of any of the 5 models is 0%. Therefore, we conclude

7

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.68
30.76
30.26
31.13
29.70

38%
75%
84%
74%
90%

ResNet-101
76%
43%
81%
78%
87%

ResNet-50 VGG-16 GoogLeNet
97%
98%
99%
24%
99%

70%
69%
46%
68%
83%

76%
73%
77%
63%
11%

Table 3: The matching rate of targeted adversarial images generated using the optimization-based
approach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) indicates that percentage of the targeted adversarial images generated for the ensemble of the
four models except model i (row) is predicted as the target label by model j (column). In each row,
the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Results of top-5 matching rate can be found in the appendix (Table 13).

that by adding Gaussian noise, the attacker cannot generate successful targeted adversarial examples
at all, let alone targeted transferability.

5 ENSEMBLE-BASED APPROACHES

We hypothesize that if an adversarial image remains adversarial for multiple models, then it is more
likely to transfer to other models as well. We develop techniques to generate adversarial images for
multiple models. The basic idea is to generate adversarial images for the ensemble of the models.
Formally, given k white-box models with softmax outputs being J1, ..., Jk, an original image x,
and its ground truth y, the ensemble-based approach solves the following optimization problem (for
targeted attack):

argminx(cid:63) − log (cid:0)(

αiJi(x(cid:63))) · 1y(cid:63)

(cid:1) + λd(x, x(cid:63))

(6)

k
(cid:88)

i=1

where y(cid:63) is the target label speciﬁed by the adversary, (cid:80) αiJi(x(cid:63)) is the ensemble model, and αi
are the ensemble weights, (cid:80)k
i=1 αi = 1. Note that (6) is the targeted objective. The non-targeted
counterpart can be derived similarly. In doing so, we hope the generated adversarial images remain
adversarial for an additional black-box model Jk+1.

We evaluate the effectiveness of the ensemble-based approach. For each of the ﬁve models, we treat
it as the black-box model to attack, and generate adversarial images for the ensemble of the rest
four, which is considered as white-box. We evaluate the generated adversarial images over all ﬁve
models. Throughout the rest of the paper, we refer to the approaches evaluated in Section 3 and 4 as
the approaches using a single model, and to the ensemble-based approaches discussed in this section
as the approaches using an ensemble model.

Optimization-based approach. We use Adam to optimize the objective (6) with equal ensemble
weights across all models in the ensemble to generate targeted adversarial examples. In particular,
we set the learning rate of Adam to be 8 for each model. In each iteration, we compute the Adam
update for each model, sum up the four updates, and add the aggregation onto the image. We run 100
iterations of updates, and we observe that the loss converges after 100 iterations. By doing so, for the
ﬁrst time, we observe a large proportion of the targeted adversarial images whose target labels can
transfer. The results are presented in Table 3. We observe that not all targeted adversarial images
can be misclassiﬁed to the target labels by the models used in the ensemble. This suggests that
while searching for an adversarial example for the ensemble model, there is no direct supervision to
mislead any individual model in the ensemble to predict the target label. Further, from the diagonal
numbers of the table, we observe that the transferability to ResNet models is better than to VGG-16
or GoogLeNet, when adversarial examples are generated against all models except the target model.

We also evaluate non-targeted adversarial images generated by the ensemble-based approach. We
observe that the generated adversarial images have almost perfect transferability. We use the same
procedure as for the targeted version, except the objective to generate the adversarial images. We
evaluate the generated adversarial images over all models. The results are presented in Table 4.
The generated adversarial images all have RMSDs around 17, which are lower than 22 to 23 of

8

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.17
17.25
17.25
17.80
17.41

0%
0%
0%
0%
0%

ResNet-101
0%
1%
0%
0%
0%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
6%
0%

0%
0%
0%
0%
5%

0%
0%
2%
0%
0%

Table 4: Accuracy of non-targeted adversarial images generated using the optimization-based ap-
proach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) corresponds to the accuracy of the attack generated using four models except model i (row)
when evaluated over model j (column). In each row, the minus sign “−” indicates that the model
of the row is not used when generating the attacks. Results of top-5 accuracy can be found in the
appendix (Table 14).

Model
# of labels

VGG-16 ResNet-50 ResNet-101 ResNet-152 GoogLeNet

10

9

21

10

21

Table 5: The number of all possible predicted labels for each model in the same plane described in Figure 3.

the optimization-based approach using a single model (See Table 1 for comparison). When the
adversarial images are evaluated over models which are not used to generate the attack, the accuracy
is no greater than 6%. For a reference, the corresponding accuracies for all approaches evaluated in
Section 3 using one single model are at least 12%. Our experiments demonstrate that the ensemble-
based approaches can generate almost perfectly transferable adversarial images.

Fast gradient-based approach. The results for non-targeted fast gradient-based approaches ap-
plied to the ensemble can be found in the appendix (Table 21, 22, 23 and 24). We observe that the
diagonal values are not zero, which is the same as we observed in the results for FG and FGS applied
to a single model. We hypothesize a potential reason is that the gradient directions of different mod-
els in the ensemble are orthogonal to each other, as we will illustrate in Section 6. In this case, the
gradient direction of the ensemble is almost orthogonal to the one of each model in the ensemble.
Therefore searching along this direction may require large distortion to reach adversarial examples.

For targeted adversarial examples generated using FG and FGS based on an ensemble model, their
transferability is no better than the ones generated using a single model. The results can be found in
the appendix (Table 28, 29, 30 and 31). We hypothesize the same reason to explain this: there are
only few possible target labels in total in the 1-D subspace.

6 GEOMETRIC PROPERTIES OF DIFFERENT MODELS

In this section, we show some geometric properties of the models to try to better understand transfer-
able adversarial examples. Prior works also try to understand the geometic properties of adversarial
examples theoretically (Fawzi et al. (2016)) or empirically (Goodfellow et al. (2014)). In this work,
we examine large models trained over a large dataset with 1000 labels, whose geometric properties
are never examined before. This allows us to make new observations to better understand the models
and their adversarial examples.

The gradient directions of different models in our evaluation are almost orthogonal to each
other. We study whether the adversarial directions of different models align with each other. We
calculate cosine value of the angle between gradient directions of different models, and the results
can be found in the appendix (Table 33). We observe that all non-diagonal values are close to 0,
which indicates that for most images, their gradient directions with respect to different models are
orthogonal to each other.

Decision boundaries of the non-targeted approaches using a single model. We study the deci-
sion boundary of different models to understand why adversarial examples transfer. We choose two
normalized orthogonal directions δ1, δ2, one being the gradient direction of VGG-16 and the other
being randomly chosen. Each point (u, v) in this 2-D plane corresponds to the image x + uδ1 + vδ2,

9

Published as a conference paper at ICLR 2017

Figure 2: The example image to study the decision boundary. Its ID in ILSVRC 2012 validation set
is 49443, and its ground truth label is “anemone ﬁsh.”

VGG-16

ResNet-50

ResNet-101

ResNet-152

GoogLeNet

n
i
-

m
o
o
Z

t
u
o
-
m
o
o
Z

Figure 3: Decision regions of different models. We pick the same two directions for all plots: one is
the gradient direction of VGG-16 (x-axis), and the other is a random orthogonal direction (y-axis).
Each point in the span plane shows the predicted label of the image generated by adding a noise to
the original image (e.g., the origin corresponds to the predicted label of the original image). The
units of both axises are 1 pixel values. All sub-ﬁgure plots the regions on the span plane using the
same color for the same label. The image is in Figure 2.

where x is the pixel value vector of the original image. For each model, we plot the label of the
image corresponding to each point, and get Figure 3 using the image in Figure 2.

We can observe that for all models, the region that each model can predict the image correctly
is limited to the central area. Also, along the gradient direction, the classiﬁers are soon misled.
One interesting ﬁnding is that along this gradient direction, the ﬁrst misclassiﬁed label for the three
ResNet models (corresponding to the light green region) is the label “orange”. A more detailed study
can be found in the appendix (Table 9, Table 26 and 27). When we look at the zoom-out ﬁgures,
however, the labels of images that are far away from the original one are different for different
models, even among ResNet models.

On the other hand, in Table 5, we show the total number of regions in each plane. In fact, for each
plane, there are at most 21 different regions in all planes. Compared with the 1,000 total categories
in ImageNet, this is only 2.1% of all categories. That means, for all other 97.9% labels, no targeted
adversarial example exists in each plane. Such a phenomenon partially explains why fast gradient-
based approaches can hardly ﬁnd targeted adversarial images.

Further, in Figure 4, we draw the decision boundaries of all models on the same plane as described
above. We can observe that

• The boundaries align with each other very well. This partially explains why non-targeted

adversarial images can transfer among models.

10

Published as a conference paper at ICLR 2017

Figure 4: The decision boundary to sep-
arate the region within which all points
are classiﬁed as the ground truth label
(encircled by each closed curve) from
others. The plane is the same one de-
scribed in Figure 3.
The origin of
the coordinate plane corresponds to the
original image. The units of both axises
are 1 pixel values.

Figure 5: The decision boundary to separate the
region within which all points are classiﬁed as the
target label (encircled by each closed curve) from
others. The plane is spanned by the targeted ad-
versarial direction and a random orthogonal di-
rection. The targeted adversarial direction is com-
puted as the difference between the original image
in Figure 2 and the adversarial image generated by
the optimization-based approach for an ensemble.
The ensemble contains all models except ResNet-
101. The origin of the coordinate plane corre-
sponds to the original image. The units of both
axises are 1 pixel values.

• The boundary diameters along the gradient direction is less than the ones along the ran-
dom direction. A potential reason is that moving a variable along its gradient direction
can change the loss function (i.e., the probability of the ground truth label) signiﬁcantly.
Therefore along the gradient direction it will take fewer steps to move out of the ground
truth region than a random direction.

• An interesting ﬁnding is that even though we move left along the x-axis, which is equivalent
to maximizing the ground truth’s prediction probability, it also reaches the boundary much
sooner than moving along a random direction. We attribute this to the non-linearity of the
loss function: when the distortion is larger, the gradient direction also changes dramatically.
In this case, moving along the original gradient direction no longer increases the probability
to predict the ground truth label (see Figure 7 in the appendix).

• As for VGG-16 model, there is a small hole within the region corresponding to the ground
truth. This may partially explain why non-targeted adversarial images with small distortion
exist, but do not transfer well. This hole does not exist in other models’ decision planes. In
this case, non-targeted adversarial images in this hole do not transfer.

Decision boundaries of the targeted ensemble-based approaches.
In addition, we choose the
targeted adversarial direction of the ensemble of all models except ResNet-101 and a random or-
thogonal direction, and we plot decision boundaries on the plane spanned by these two direction
vectors in Figure 5. We observe that the regions of images, which are predicted as the target label,
align well for the four models in the ensemble. However, for the model not used to generate the
adversarial image, i.e., ResNet-101, it also has a non-empty region such that the prediction is suc-
cessfully misled to the target label, although the area is much smaller. Meanwhile, the region within
each closed curve of the models almost has the same center.

7 REAL WORLD EXAMPLE: ADVERSARIAL EXAMPLES FOR CLARIFAI.COM

Clarifai.com is a commercial company providing state-of-the-art image classiﬁcation services. We
have no knowledge about the dataset and types of models used behind Clarifai.com, except that we
have black-box access to the services. The labels returned from Clarifai.com are also different from

11

Published as a conference paper at ICLR 2017

the categories in ILSVRC 2012. We submit all 100 original images to Clarifai.com and the returned
labels are correct based on a subjective measure.

We also submit 400 adversarial images in total, where 200 of them are targeted adversarial examples,
and the rest 200 are non-targeted ones. As for the 200 targeted adversarial images, 100 of them
are generated using the optimization-based approach based on VGG-16 (the same ones evaluated
in Table 2), and the rest 100 are generated using the optimization-based approach based on an
ensemble of all models except ResNet-152 (the same ones evaluated in Table 3). The 200 non-
targeted adversarial examples are generated similarly (the same ones evaluated in Table 1 and 4).

For non-targeted adversarial examples, we observe that for both the ones generated using VGG-16
and those generated using the ensemble, most of them can transfer to Clarifai.com.

More importantly, a large proportion of our targeted adversarial examples are misclassiﬁed by Clari-
fai.com as well. We observe that 57% of the targeted adversarial examples generated using VGG-16,
and 76% of the ones generated using the ensemble can mislead Clarifai.com to predict labels irrele-
vant to the ground truth.

Further, our experiment shows that for targeted adversarial examples, 18% of those generated us-
ing the ensemble model can be predicted as labels close to the target label by Clarifai.com. The
corresponding number for the targeted adversarial examples generated using VGG-16 is 2%. Con-
sidering that in the case of attacking Clarifai.com, the labels given by the target model are different
from those given by our models, it is fairly surprising to see that when using the ensemble-based
approach, there is still a considerable proportion of our targeted adversarial examples that can mis-
lead this black-box model to make predictions semantically similar to our target labels. All these
numbers are computed based on a subjective measure, and we include some examples in Table 6.
More examples can be found in the appendix (Table 34).

original
image

true
label

Clarifai.com
results of
original image

target
label

targeted
adversarial
example

Clarifai.com results
of targeted
adversarial example

window,
wall,
old,
decoration,
design

Buddha,
gold,
temple,
celebration,
artistic

cherry,
branch,
fruit,
food,
season

sea seal,
ocean,
head,
sea,
cute

viaduct

hip, rose
hip,
rosehip

bridge,
sight,
arch,
river,
sky

fruit,
fall,
food,
little,
wildlife

dogsled,
dog
sled,
dog
sleigh

group together,
four,
sledge,
sled,
enjoyment

pug,
pug-dog

pug,
friendship,
adorable,
purebred,
sit

window
screen

stupa,
tope

hip, rose
hip,
rosehip

sea lion

12

Published as a conference paper at ICLR 2017

Old
English
sheep-
dog,
bobtail

maillot,
tank suit

patas,
hussar
monkey,
Erythro-
cebus
patas

poodle,
retriever,
loyalty,
sit,
two

beach,
woman,
adult,
wear,
portrait

primate,
monkey,
safari,
sit,
looking

abaya

amphib-
ian,
amphibi-
ous
vehicle

bee eater

veil,
spirituality,
religion,
people,
illustration

transportation
system,
vehicle,
man,
print,
retro

ornithology,
avian,
beak,
wing,
feather

Table 6: Original images and adversarial images evaluated over Clarifai.com. For labels returned
from Clarifai.com, we sort the labels ﬁrstly by rareness: how many times a label appears in the
Clarifai.com results for all adversarial images and original images, and secondly by conﬁdence.
Only top 5 labels are provided.

8 CONCLUSION

In this work, we are the ﬁrst to conduct an extensive study of the transferability of both non-targeted
and targeted adversarial examples generated using different approaches over large models and a
large scale dataset. Our results conﬁrm that the transferability for non-targeted adversarial exam-
ples are prominent even for large models and a large scale dataset. On the other hand, we ﬁnd that
it is hard to use existing approaches to generate targeted adversarial examples whose target labels
can transfer. We develop novel ensemble-based approaches, and demonstrate that they can gen-
erate transferable targeted adversarial examples with a high success rate. Meanwhile, these new
approaches exhibit better performance on generating non-targeted transferable adversarial examples
than previous work. We also show that both non-targeted and targeted adversarial examples gen-
erated using our new approaches can successfully attack Clarifai.com, which is a black-box image
classiﬁcation system. Furthermore, we study some geometric properties to better understand the
transferable adversarial examples.

This material is in part based upon work supported by the National Science Foundation under Grant
No. TWC-1409915. Any opinions, ﬁndings, and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily reﬂect the views of the National Science
Foundation.

ACKNOWLEDGMENTS

REFERENCES

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. arXiv

preprint arXiv:1608.04644, 2016.

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classiﬁers:
from adversarial to random noise. In Advances in Neural Information Processing Systems, pp.
1624–1632, 2016.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014.

13

Published as a conference paper at ICLR 2017

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-

nition. arXiv preprint arXiv:1512.03385, 2015.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,

abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. arXiv preprint arXiv:1610.08401, 2016.

Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against deep learning systems using adversarial examples.
arXiv preprint arXiv:1602.02697, 2016b.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.

J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine
learning algorithms for trafﬁc sign recognition. Neural Networks, (0):–, 2012. ISSN 0893-6080.
doi: 10.1016/j.neunet.2012.02.016. URL http://www.sciencedirect.com/science/
article/pii/S0893608012000457.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.

14

Published as a conference paper at ICLR 2017

APPENDIX

Top-1 accuracy
Top-5 accuracy

ResNet-50
72.5%
91.0%

ResNet-101
73.8%
91.7%

ResNet-152 GoogLeNet VGG-16
68.3%
88.3%

68.7%
89.0%

74.6%
92.1%

Table 7: Top-1 and top-5 accuracy of the studied models over the ILSVRC 2012 validation dataset.

An alternative optimization-based approach to generate adversarial examples. An alternative
method to generate non-targeted adversarial examples with large distortion is to revise the optimiza-
tion objective to incorporate this distortion constraint. For example, for non-targeted adversarial
image searching, we can optimize for the following objective.

argminx(cid:63) − log (1 − 1y · Jθ(x(cid:63))) + λ1ReLU(τ − d(x, x(cid:63))) + λ2ReLU(d(x, x(cid:63)) − τ )

Optimizing for
(1) minimizing
− log (1 − 1y · Jθ(x(cid:63))); (2) Penalizing the solution if d(x, x(cid:63)) is no more than a threshold τ (too
low); and (3) Penalizing the solution if d(x, x(cid:63)) is too high.

the following three effects:

the above objective has

In our preliminary evaluation, we found that the solutions computed from the two approaches have
similar transferability. We thus omit the results for this alternative approach.

Transferable non-targeted adversarial images are classiﬁed as the same wrong labels. Previ-
ous work Goodfellow et al. (2014) reported the phenomenon that when evaluating the adversarial
images, different models tend to make the same wrong predictions. This conclusion was only exam-
ined over datasets with 10 categories. In our evaluation, however, we observe the same phenomenon,
albeit we have 1000 possible categories. We refer to this effect as the same mistake effect.

Table 9 presents the results based on the adversarial examples generated for VGG-16. For each pair
of models, among all adversarial examples that both models make wrong predictions, we compute
the percentage that both models make the same mistake. These percentage numbers are from 12% to
40%, which are surprisingly high, since there are 999 possible categories to be misclassiﬁed into (see
Table 32 in the appendix for the wrong predicted label distribution of these adversarial examples).
Later in Section 6, we try to explain this phenomenon using decision boundaries.

Adversarial images may come from multiple intervals along the gradient direction.
In Fig-
ure 6, we show that along the gradient direction of the non-targeted objective (3) for VGG-16, when
evaluating the adversarial image xB on ResNet-152, xB will soon become adversarial for small B.
While B increases, however, the prediction changes back to be the correct label, then a wrong label,
then the correct label again, and ﬁnally wrong labels. This indicates that along the gradient of this
image, the distortions that can cause the corresponding adversarial images to mislead ResNet-152
form four intervals.

Comparison with random perturbations. For comparison, we evaluate the test accuracy when
we add a Gaussian noise to the 100 images in our test set. We vary the standard deviation of the
Gaussian noise from 5 to 40 with a step size of 5. For each speciﬁc standard deviation, we generate

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
22.83
23.81
22.86
22.51
22.58

7%
40%
48%
36%
66%

ResNet-101
43%
6%
44%
33%
71%

ResNet-50 VGG-16 GoogLeNet
39%
42%
42%
0%
49%

43%
41%
3%
33%
62%

31%
34%
32%
15%
2%

Table 8: Top-5 accuracy of Table 1 Panel A. Transferability between pairs of models using non-
targeted optimization-based approach with a learning rate of 4. The ﬁrst column indicates the av-
erage RMSD of all adversarial images generated for the model in the corresponding row. The cell
(i, j) indicates the top-5 accuracy of the adversarial images generated for model i (row) evaluated
over model j (column). Lower value indicates better transferability.

15

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
28.57%
29.87%
19.23%
12.82%

ResNet-101
−
100.00%
40.00%
18.07%
20.48%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
22.89% 100.00%
16.84%
18.07%

GoogLeNet
−
−
−
−
100.00%

Table 9: When using the optimization-based approach for VGG-16 model to generate non-targeted adversarial
images, cell (i, j) is the percentage of all transferable adversarial images that are predicted as the same wrong
labels by both models i and j over all adversarial images that are misclassiﬁed by both models i and j. Notice
that this table is symmetric.

(a) The original image

(b) Prediction correctness

Figure 6: We add Bδ onto the original image in Figure 6a, where δ is the normalized gradient of
the non-targeted objective (3) for VGG-16. When xB is evaluated on ResNet-152, we plot 1 if the
prediction matches the ground truth, or 0 otherwise.

100 random noises and add them to each image, resulting in 10,000 noisy images in total. Then
we evaluate the accuracy of each model on these 10,000 images, and the results are presented in
Table 11. Notice that when setting standard deviation to be 25, the average RMSD is 23.59, which
is comparable to that of non-targeted adversarial examples generated by either optimization-based
approaches or fast gradient-based approaches. However, each model can still achieve an accuracy
more than 66%. This shows that adding random noise is not an effective way to generate adversarial
examples, hence the “transferability” of this approach is signiﬁcantly worse than either optimization-
based approaches or fast gradient-based approaches.

More examples submitted to Clarifai.com. We present more results from Clarifai.com by sub-
mitting original and adversarial examples in Table 34.

original
image

true
label

Clarifai.com
results of
original image

target
label

targeted
adv-example

Clarifai.com result
of targeted
adversarial example

broom

jacamar

dust,
brick,
rustic,
stone,
dirty

frost,
sparrow,
pigeon,
ice,
frosty

heavy,
bulldozer,
exert,
track,
plow

junco,
snow-
bird

har-
vester,
reaper

eel

prairie
chicken,
prairie
grouse,
prairie
fowl

16

feather,
beautiful,
bird,
leaf,
ﬂora

swimming,
underwater,
ﬁsh,
water,
one

wildlife,
animal,
illustration,
nature,
color

Published as a conference paper at ICLR 2017

broccoli

hamster

mon-
goose

holster

yurt

water
buffalo,
water
ox,
Asiatic
buffalo,
Bubalus
bubalis

zebra

French
bulldog

curly-
coated
retriever

fox,
rodent,
predator,
fur,
park

pistol,
force,
bullet,
protection,
cartridge

wooden,
scenic,
snow,
rural,
landscape

kind,
cauliﬂower,
vitamin,
carrot,
cabbage

herd,
milk,
beef cattle,
farmland,
cow

equid,
stripe,
savanna,
zebra,
safari

bulldog,
studio,
boxer,
eye,
bull

eye,
looking,
pet,
canine,
dog

maillot

ground
beetle,
carabid
beetle

comic
book

rugby ball

apiary,
bee house

kite

ﬂy

17

motley,
shape,
horizontal,
abstract,
bright

shell,
shell (food),
antenna,
insect,
invertebrate

grafﬁti,
religion,
people,
painting,
culture

bird,
bright,
texture,
animal,
decoration

pastime,
print,
illustration,
art

wood,
people,
outdoors,
nature

visuals,
feather,
wing,
pet,
print

graphic,
shape,
insect,
artistic,
image

Table 34: Original images and adversarial images evaluated over Clarifai.com. For labels returned
from Clarifai.com, we sort the labels ﬁrstly by the occurrence of a label from the Clarifai.com results,
and secondly by conﬁdence. Only top 5 labels are provided.

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.45
23.49
23.49
23.73
23.45

23%
35%
40%
40%
53%

ResNet-101
29%
20%
39%
35%
46%

ResNet-50 VGG-16 GoogLeNet
39%
43%
39%
8%
38%

33%
28%
33%
19%
7%

31%
33%
18%
33%
37%

Table 10: Top-5 accuracy of Table 1 Panel B. Transferability between pairs of models using non-
targeted FG. The ﬁrst column indicates the average RMSD of all adversarial images generated for
the model in the corresponding row. The cell (i, j) indicates the top-5 accuracy of the adversarial
images generated for model i (row) evaluated over model j (column). Lower value indicates better
transferability.

Standard Deviation
5
10
15
20
25
30
35
40

RMSD ResNet-152
4.91
9.72
14.44
19.07
23.59
28.01
32.32
36.52

97.41%
95.53%
91.19%
86.56%
83.10%
78.95%
73.60%
66.53%

ResNet-101
98.96%
96.72%
94.22%
90.38%
85.53%
79.04%
70.89%
63.09%

ResNet-50 VGG-16 GoogLeNet
98.74% 97.47%
96.81% 92.13%
92.16% 87.86%
84.07% 82.30%
78.33% 73.57%
71.66% 65.33%
62.03% 58.55%
50.96% 51.85%

99.29%
95.94%
88.50%
77.84%
66.84%
54.93%
45.13%
35.61%

Table 11: Accuracy of images with random perturbation. The ﬁrst column reports the standard
deviation of the Gaussian noise added to each image. The second column reports the average RMSD
over all generated images with the respective standard deviation. For each of the rest column j, the
cell (i, j) reports model j’s accuracy of the noisy images when a Gaussian noise with the respective
standard deviation speciﬁed in row i is added to each image.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.13
23.16
23.06
23.59
22.87

100%
9%
10%
3%
1%

ResNet-101
11%
100%
9%
5%
2%

ResNet-50 VGG-16 GoogLeNet
3%
2%
2%
100%
3%

5%
7%
100%
5%
1%

1%
1%
3%
4%
100%

Table 12: Top-5 matching rate of Table 2. The adversarial images are generated using the targeted
optimization-based approach with a learning rate of 4. The ﬁrst column indicates the average RMSD
of all adversarial images generated for the model in the corresponding row. Cell (i, j) indicates that
top-5 matching rate of the targeted adversarial images generated for model i (row) when evaluated
on model j (column). Higher value indicates more successful transferable target labels.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.68
30.76
30.26
31.13
29.70

87%
100%
99%
100%
100%

ResNet-101
100%
88%
99%
100%
100%

ResNet-50 VGG-16 GoogLeNet
100%
100%
99%
51%
100%

100%
100%
86%
100%
100%

100%
100%
99%
100%
32%

Table 13: The top-5 matching rate of Table 3. Matching rate of adversarial images generated using
targeted optimization-based approach. The ﬁrst column indicates the average RMSD of all adver-
sarial images generated for the model in the corresponding row. Cell (i, j) indicates that top-5
matching rate of the targeted adversarial images generated using the ensemble of the four models
except model i (row) is predicted as the target label by model j (column). In each row, the minus
sign “−” indicates that the model of the row is not used when generating the attacks. Higher value
indicates more successful transferable target labels.

18

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.17
17.25
17.25
17.80
17.41

13%
2%
4%
4%
3%

ResNet-101
4%
11%
5%
7%
2%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
20%
0%

3%
3%
2%
4%
15%

4%
3%
11%
5%
3%

Table 14: Top-5 accuracy of Table 4. The ﬁrst column indicates the average RMSD of all adversarial
images generated for the model in the corresponding row. Cell (i, j) indicates that top-5 accuracy
of the non-targeted adversarial images generated using the ensemble of the four models except
model i (row) when evaluated over model j (column). In each row, the minus sign “−” indicates
that the model of the row is not used when generating the attacks. Lower value indicates better
transferability.

RMSD ResNet-152

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

1.25
1.24
1.21
1.55
1.27

0%
84%
90%
89%
94%

ResNet-101
86%
0%
91%
94%
97%

ResNet-50 VGG-16 GoogLeNet
93%
95%
91%
0%
91%

96%
100%
97%
84%
0%

87%
93%
0%
92%
98%

Table 15: Transferability between pairs of models using non-targeted optimization-based approach
with a learning rate of 0.125. The ﬁrst column indicates the average RMSD of all adversarial images
generated for the model in the corresponding row. The cell (i, j) indicates the top-1 accuracy of the
adversarial images generated for model i (row) evaluated over model j (column). Lower value
indicates better transferability. Results of top-5 accuracy can be found in Table 16.

RMSD ResNet-152

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

1.25
1.24
1.21
1.55
1.27

24%
100%
100%
98%
100%

ResNet-101
100%
22%
99%
100%
100%

ResNet-50 VGG-16 GoogLeNet
99%
100%
100%
15%
100%

100%
100%
100%
100%
18%

100%
99%
25%
100%
100%

Table 16: Top-5 accuracy of Table 15. Transferability between pairs of models using non-targeted
optimization-based approach with a learning rate of 0.125. The ﬁrst column indicates the average
RMSD of all adversarial images generated for the model in the corresponding row. The cell (i, j)
indicates the top-5 accuracy of the adversarial images generated for model i (row) evaluated over
model j (column). Lower value indicates better transferability.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
31.35
31.11
31.32
31.50
30.67

98%
5%
3%
1%
2%

ResNet-101
1%
98%
2%
1%
1%

ResNet-50 VGG-16 GoogLeNet
2%
2%
1%
97%
2%

2%
1%
99%
2%
0%

0%
0%
1%
1%
97%

Table 17: The adversarial images are generated using the targeted optimization-based approach
with a larger learning rate. The ﬁrst column indicates the average RMSD of all adversarial images
generated for the model in the corresponding row. Cell (i, j) indicates that top-1 matching rate of
the targeted adversarial images generated for model i (row) when evaluated on model j (column).
Higher value indicates more successful transferable target labels. We used a larger learning rate to
achieve larger RMSD. Results of top-5 matching rate can be found in Table 18.

19

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
31.35
31.11
31.32
31.50
30.67

98%
10%
6%
4%
3%

ResNet-101
8%
98%
7%
4%
1%

ResNet-50 VGG-16 GoogLeNet
3%
3%
5%
97%
3%

6%
5%
99%
6%
2%

1%
1%
1%
4%
97%

Table 18: The top-5 matching rate of Table 17. The adversarial images are generated using the
targeted optimization-based approach with a larger learning rate. The ﬁrst column indicates the
average RMSD of all adversarial images generated for the model in the corresponding row. Cell
(i, j) indicates that top-5 matching rate of the targeted adversarial images generated for model i
(row) when evaluated on model j (column). Higher value indicates more successful transferable
target labels. We used a larger learning rate to achieve larger RMSD.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.11
23.11
23.11
23.12
23.11

12%
29%
34%
25%
46%

ResNet-101
27%
13%
28%
20%
41%

ResNet-50 VGG-16 GoogLeNet
22%
29%
25%
0%
25%

15%
16%
23%
8%
2%

25%
29%
10%
23%
40%

Table 19: Transferability between pairs of models using non-targeted FGS. The ﬁrst column indi-
cates the average RMSD of all adversarial images generated for the model in the corresponding row.
The cell (i, j) indicates the top-1 accuracy of the adversarial images generated for model i (row)
evaluated over model j (column). Lower value indicates better transferability. Results of top-5
accuracy can be found in Table 20.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.11
23.11
23.11
23.12
23.11

32%
56%
59%
42%
71%

ResNet-101
55%
33%
53%
39%
74%

ResNet-50 VGG-16 GoogLeNet
47%
46%
47%
5%
53%

36%
40%
38%
21%
11%

53%
50%
29%
41%
62%

Table 20: Top-5 accuracy of Table 19. Transferability between pairs of models using non-targeted
FGS. The ﬁrst column indicates the average RMSD of all adversarial images generated for the
model in the corresponding row. The cell (i, j) indicates the top-5 accuracy of the adversarial
images generated for model i (row) evaluated over model j (column). Lower value indicates better
transferability.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.25
17.24
17.24
17.24
17.24

23%
15%
15%
17%
14%

ResNet-101
12%
19%
13%
15%
11%

ResNet-50 VGG-16 GoogLeNet
1%
2%
2%
23%
2%

11%
11%
19%
12%
10%

7%
6%
8%
7%
19%

Table 21: Transferability between pairs of models using non-targeted ensemble FG. The ﬁrst column
indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that top-1
accuracy of the non-targeted adversarial images generated using the ensemble of the four models
except model i (row) when evaluated over model j (column). In each row, the minus sign “−”
indicates that the model of the row is not used when generating the attacks. Lower value indicates
better transferability. Results of top-5 accuracy can be found in Table 22.

20

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.25
17.24
17.24
17.24
17.24

47%
37%
38%
37%
32%

ResNet-101
33%
42%
32%
36%
30%

ResNet-50 VGG-16 GoogLeNet
15%
14%
13%
44%
13%

28%
30%
39%
37%
28%

21%
26%
23%
32%
46%

Table 22: Top-5 accuracy of Table 21. Transferability between pairs of models using non-targeted
ensemble FG. The ﬁrst column indicates the average RMSD of the generated adversarial images.
Cell (i, j) indicates that top-5 accuracy of the non-targeted adversarial images generated using the
ensemble of the four models except model i (row) when evaluated over model j (column). In each
row, the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Lower value indicates better transferability.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.41
17.40
17.40
17.40
17.40

26%
18%
15%
17%
15%

ResNet-101
10%
21%
13%
16%
12%

ResNet-50 VGG-16 GoogLeNet
2%
2%
4%
23%
3%

11%
13%
20%
11%
10%

7%
4%
8%
7%
22%

Table 23: Transferability between pairs of models using non-targeted ensemble FGS. The ﬁrst col-
umn indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that top-1
accuracy of the non-targeted adversarial images generated using the ensemble of the four models
except model i (row) when evaluated over model j (column). In each row, the minus sign “−” indi-
cates that the model of the row is not used when generating the attacks. Lower value indicates better
transferability. Results of top-5 accuracy can be found in Table 24.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.41
17.40
17.40
17.40
17.40

50%
38%
38%
37%
35%

ResNet-101
35%
48%
36%
36%
29%

ResNet-50 VGG-16 GoogLeNet
20%
19%
18%
48%
19%

26%
26%
28%
33%
53%

30%
33%
41%
36%
31%

Table 24: Top-5 accuracy of Table 23. Transferability between pairs of models using non-targeted
ensemble FGS. The ﬁrst column indicates the average RMSD of the generated adversarial images.
Cell (i, j) indicates that top-5 accuracy of the non-targeted adversarial images generated using the
ensemble of the four models except model i (row) when evaluated over model j (column). In each
row, the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Lower value indicates better transferability.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.55
23.56
23.56
23.95
23.63

1%
1%
1%
1%
1%

ResNet-101
2%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
1%
1%

0%
0%
1%
0%
0%

1%
1%
0%
1%
1%

Table 25: The adversarial images are generated using the targeted FG. The ﬁrst column indicates the
average RMSD of all adversarial images generated for the model in the corresponding row. The ﬁrst
column indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that
top-1 matching rate of the targeted adversarial images generated for model i (row) when evaluated
on model j (column). Higher value indicates more successful transferable target labels.

21

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
33.33%
24.00%
17.50%
15.38%

ResNet-101
−
100.00%
35.00%
19.05%
14.81%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
21.18% 100.00%
15.05%
13.10%

GoogLeNet
−
−
−
−
100.00%

Table 26: When using non-targeted FG for VGG-16 model to generate adversarial images, cell (i, j)
is the percentage of all transferable adversarial images that are predicted as the same wrong labels
by both models i and j over all adversarial images that are misclassiﬁed by both models i and j.
Notice that the table is symmetric.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
33.78%
36.62%
16.00%
14.86%

ResNet-101
−
100.00%
43.24%
20.00%
20.25%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
23.38% 100.00%
19.57%
13.16%

GoogLeNet
−
−
−
−
100.00%

Table 27: When using non-targeted FGS for VGG-16 model to generate adversarial images, cell
(i, j) is the percentage of all transferable adversarial images that are predicted as the same wrong
labels by both models i and j over all adversarial images that are misclassiﬁed by both models i and
j. Notice that the table is symmetric.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
31.05
30.94
31.12
30.57
30.47

1%
1%
1%
1%
1%

ResNet-101
1%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
1%

1%
0%
1%
1%
0%

0%
0%
0%
0%
0%

Table 28: Transferability between pairs of models using targeted ensemble FG. The ﬁrst column
indicates the average RMSD of all adversarial images generated for the model in the corresponding
row. Cell (i, j) indicates that top-1 matching rate of the targeted adversarial images generated using
the ensemble of the four models except model i (row) is predicted as the target label by model j
(column). In each row, the minus sign “−” indicates that the model of the row is not used when
generating the attacks. Higher value indicates more successful transferable target labels. Results of
top-5 matching rate can be found in Table 29.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
31.05
30.94
31.12
30.57
30.47

1%
1%
1%
2%
1%

ResNet-101
2%
1%
2%
2%
1%

ResNet-50 VGG-16 GoogLeNet
3%
1%
3%
1%
2%

2%
2%
2%
1%
2%

1%
1%
1%
2%
1%

Table 29: Top-5 matching rate of Table 28. Transferability between pairs of models using targeted
ensemble FG. The ﬁrst column indicates the average RMSD of all adversarial images generated
for the model in the corresponding row. Cell (i, j) indicates that top-5 matching rate of the targeted
adversarial images generated using the ensemble of the four models except model i (row) is predicted
as the target label by model j (column). In each row, the minus sign “−” indicates that the model of
the row is not used when generating the attacks. Higher value indicates more successful transferable
target labels.

22

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.42
30.42
30.42
30.41
30.42

1%
1%
1%
1%
1%

ResNet-101
1%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
1%

1%
1%
1%
1%
1%

1%
1%
0%
0%
1%

Table 30: Transferability between pairs of models using targeted ensemble FGS. The ﬁrst column
indicates the average RMSD of all adversarial images generated for the model in the corresponding
row. Cell (i, j) indicates that top-1 matching rate of the targeted adversarial images generated using
the ensemble of the four models except model i (row) is predicted as the target label by model j
(column). In each row, the minus sign “−” indicates that the model of the row is not used when
generating the attacks. Higher value indicates more successful transferable target labels. Results are
top-5 matching rate can be found in Table 31.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.42
30.42
30.42
30.41
30.42

2%
2%
2%
3%
2%

ResNet-101
1%
1%
1%
2%
2%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
2%

2%
1%
1%
2%
2%

2%
2%
2%
1%
2%

Table 31: Top-5 matching rate of Table 30. Transferability between pairs of models using targeted
ensemble FGS. The ﬁrst column indicates the average RMSD of all adversarial images generated
for the model in the corresponding row. Cell (i, j) indicates that top-5 matching rate of the targeted
adversarial images generated using the ensemble of the four models except model i (row) is predicted
as the target label by model j (column). In each row, the minus sign “−” indicates that the model of
the row is not used when generating the attacks. Higher value indicates more successful transferable
target labels.

ResNet-152
jigsaw puzzle(8%)
acorn(3%)
lycaenid(2%)
ram(2%)
maze(2%)

ResNet-101
jigsaw puzzle(12%)
starﬁsh(3%)
strawberry(2%)
wild boar(2%)
dishrag(2%)

ResNet-50
jigsaw puzzle(12%)

VGG-16
jigsaw puzzle(15%)

African chameleon(2%) African chameleon(7%)

strawberry(2%)
starﬁsh(2%)
greenhouse(2%)

prayer rug(5%)
apron(4%)
sarong(3%)

GoogleNet
prayer rug(12%)
jigsaw puzzle(7%)
stole(6%)
African chameleon(4%)
mitten(3%)

Table 32: When using non-targeted optimization-based approach for VGG-16 model to generate
adversarial images, column i indicates the top 5 common incorrect labels predicted by model i. The
value in the parentheses is the percentage of the predicted label.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
1.00
0.04
0.03
0.02
0.01

ResNet-101
−
1.00
0.03
0.02
0.01

ResNet-50 VGG-16 GoogLeNet
−
−
−
1.00
0.02

−
−
−
−
1.00

−
−
1.00
0.02
0.01

Table 33: Average cosine value of the angle between gradient directions of two models. Notice
that the dot-product of two normalized vectors is the cosine value of the angle between them, for
each image, we compute the dot-product of normalized gradient directions with respect to model i
(row) and model j (column), and the value in cell (i, j) is the average over dot-product values of all
images. Notice that this table is symmetric.

23

Published as a conference paper at ICLR 2017

(a) GoogLeNet

(b) VGG-16

(c) ResNet-152

(d) ResNet-101

(e) ResNet-50

Figure 7: Linearity of different models. Each line plots the classiﬁcation layer’s softmax input vs
distortion B when fast gradient noises are used. Solid red line is the ground truth label, and other
lines are the top 10 labels predicted on the original image.

24

7
1
0
2
 
b
e
F
 
7
 
 
]

G
L
.
s
c
[
 
 
3
v
0
7
7
2
0
.
1
1
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

DELVING INTO TRANSFERABLE ADVERSARIAL EX-
AMPLES AND BLACK-BOX ATTACKS

Yanpei Liu∗, Xinyun Chen∗
Shanghai Jiao Tong University

Chang Liu, Dawn Song
University of the California, Berkeley

ABSTRACT

An intriguing property of deep neural networks is the existence of adversarial ex-
amples, which can transfer among different architectures. These transferable ad-
versarial examples may severely hinder deep neural network-based applications.
Previous works mostly study the transferability using small scale datasets. In this
work, we are the ﬁrst to conduct an extensive study of the transferability over
large models and a large scale dataset, and we are also the ﬁrst to study the trans-
ferability of targeted adversarial examples with their target labels. We study both
non-targeted and targeted adversarial examples, and show that while transferable
non-targeted adversarial examples are easy to ﬁnd, targeted adversarial examples
generated using existing approaches almost never transfer with their target labels.
Therefore, we propose novel ensemble-based approaches to generating transfer-
able adversarial examples. Using such approaches, we observe a large proportion
of targeted adversarial examples that are able to transfer with their target labels for
the ﬁrst time. We also present some geometric studies to help understanding the
transferable adversarial examples. Finally, we show that the adversarial examples
generated using ensemble-based approaches can successfully attack Clarifai.com,
which is a black-box image classiﬁcation system.

1

INTRODUCTION

Recent research has demonstrated that for a deep architecture, it is easy to generate adversarial
examples, which are close to the original ones but are misclassiﬁed by the deep architecture (Szegedy
et al. (2013); Goodfellow et al. (2014)). The existence of such adversarial examples may have severe
consequences, which hinders vision-understanding-based applications, such as autonomous driving.
Most of these studies require explicit knowledge of the underlying models.
It remains an open
question how to efﬁciently ﬁnd adversarial examples for a black-box model.

Several works have demonstrated that some adversarial examples generated for one model may
also be misclassiﬁed by another model. Such a property is referred to as transferability, which
can be leveraged to perform black-box attacks. This property has been exploited by constructing
a substitute of the black-box model, and generating adversarial instances against the substitute to
attack the black-box system (Papernot et al. (2016a;b)). However, so far, transferability is mostly
examined over small datasets, such as MNIST (LeCun et al. (1998)) and CIFAR-10 (Krizhevsky &
Hinton (2009)). It has yet to be better understood transferability over large scale datasets, such as
ImageNet (Russakovsky et al. (2015)).

In this work, we are the ﬁrst to conduct an extensive study of the transferability of different adver-
sarial instance generation strategies applied to different state-of-the-art models trained over a large
scale dataset. In particular, we study two types of adversarial examples: (1) non-targeted adversar-
ial examples, which can be misclassiﬁed by a network, regardless of what the misclassiﬁed labels
may be; and (2) targeted adversarial examples, which can be classiﬁed by a network as a target
label. We examine several existing approaches searching for adversarial examples based on a single
model. While non-targeted adversarial examples are more likely to transfer, we observe few targeted
adversarial examples that are able to transfer with their target labels.

∗Work is done while visiting UC Berkeley.

1

Published as a conference paper at ICLR 2017

We further propose a novel strategy to generate transferable adversarial images using an ensemble
of multiple models. In our evaluation, we observe that this new strategy can generate non-targeted
adversarial instances with better transferability than other methods examined in this work. Also, for
the ﬁrst time, we observe a large proportion of targeted adversarial examples that are able to transfer
with their target labels.

We study geometric properties of the models in our evaluation.
In particular, we show that the
gradient directions of different models are orthogonal to each other. We also show that decision
boundaries of different models align well with each other, which partially illustrates why adversarial
examples can transfer.

Last, we study whether generated adversarial images can attack Clarifai.com, a commercial com-
pany providing state-of-the-art image classiﬁcation services. We have no knowledge about the train-
ing dataset and the types of models used by Clarifai.com; meanwhile, the label set of Clarifai.com
is quite different from ImageNet’s. We show that even in this case, both non-targeted and targeted
adversarial images transfer to Clarifai.com. This is the ﬁrst work documenting the success of gen-
erating both non-targeted and targeted adversarial examples for a black-box state-of-the-art online
image classiﬁcation system, whose model and training dataset are unknown to the attacker.

Contributions and organization. We summarize our main contributions as follows:

• For ImageNet models, we show that while existing approaches are effective to generate
non-targeted transferable adversarial examples (Section 3), only few targeted adversarial
examples generated by existing methods can transfer (Section 4).

• We propose novel ensemble-based approaches to generate adversarial examples (Sec-
tion 5). Our approaches enable a large portion of targeted adversarial examples to transfer
among multiple models for the ﬁrst time.

• We are the ﬁrst to present that targeted adversarial examples generated for models trained
on ImageNet can transfer to a black-box system, i.e., Clarifai.com, whose model, training
data, and label set is unknown to us (Section 7). In particular, Clarifai.com’s label set is
very different from ImageNet’s.

• We conduct the ﬁrst analysis of geometric properties for large models trained over Ima-
geNet (Section 6), and the results reveal several interesting ﬁndings, such as the gradient
directions of different models are orthogonal to each other.

In the following, we ﬁrst discuss related work, and then present the background knowledge and
experiment setup in Section 2. Then we present each of our experiments and conclusions in the
corresponding section as mentioned above.

Related work. Transferability of adversarial examples was ﬁrst examined by Szegedy et al.
(2013), which studied the transferability (1) between different models trained over the same dataset;
and (2) between the same or different model trained over disjoint subsets of a dataset; However,
Szegedy et al. (2013) only studied MNIST.

The study of transferability was followed by Goodfellow et al. (2014), which attributed the phe-
nomenon of transferability to the reason that the adversarial perturbation is highly aligned with the
weight vector of the model. Again, this hypothesis was tested using MNIST and CIFAR-10 datasets.
We show that this is not the case for models trained over ImageNet.

Papernot et al. (2016a;b) examined constructing a substitute model to attack a black-box target
model. To train the substitute model, they developed a technique that synthesizes a training set and
annotates it by querying the target model for labels. They demonstrate that using this approach,
black-box attacks are feasible towards machine learning services hosted by Amazon, Google, and
MetaMind. Further, Papernot et al. (2016a) studied the transferability between deep neural networks
and other models such as decision tree, kNN, etc.

Our work differs from Papernot et al. (2016a;b) in three aspects. First, in these works, only the model
and the training process are a black box, but the training set and the test set are controlled by the
attacker; in contrast, we attack Clarifai.com, whose model, training data, training process, and even
the test label set are unknown to the attacker. Second, the datasets studied in these works are small

2

Published as a conference paper at ICLR 2017

scale, i.e., MNIST and GTSRB (Stallkamp et al. (2012)); in our work, we study the transferability
over larger models and a larger dataset, i.e., ImageNet. Third, to attack black-box machine learning
systems, we do not query the systems for constructing the substitute model ourselves.

In a concurrent and independent work, Moosavi-Dezfooli et al. (2016) showed the existence of a
universal perturbation for each model, which can transfer across different images. They also show
that the adversarial images generated using these universal perturbations can transfer across different
models on ImageNet. However, they only examine the non-targeted transferability, while our work
studies both non-targeted and targeted transferability over ImageNet.

2 ADVERSARIAL DEEP LEARNING AND TRANSFERABILITY

2.1 THE ADVERSARIAL DEEP LEARNING PROBLEM

We assume a classiﬁer fθ(x) outputs a category (or a label) as the prediction. Given an original
image x, with ground truth label y, the adversarial deep learning problem is to seek for adversarial
examples for the classiﬁer fθ(x). Speciﬁcally, we consider two classes of adversarial examples.
A non-targeted adversarial example x(cid:63) is an instance that is close to x, in which case x(cid:63) should
have the same ground truth as x, while fθ(x(cid:63)) (cid:54)= y. For the problem to be non-trivial, we assume
fθ(x) = y without loss of generality. A targeted adversarial example x(cid:63) is close to x and satisﬁes
fθ(x(cid:63)) = y(cid:63), where y(cid:63) is a target label speciﬁed by the adversary, and y(cid:63) (cid:54)= y.

2.2 APPROACHES FOR GENERATING ADVERSARIAL EXAMPLES

In this work, we consider three classes of approaches for generating adversarial examples:
optimization-based approaches, fast gradient approaches, and fast gradient sign approaches. Each
class has non-targeted and targeted versions respectively.

2.2.1 APPROACHES FOR GENERATING NON-TARGETED ADVERSARIAL EXAMPLES

Formally, given an image x with ground truth y = fθ(x), searching for a non-targeted adversarial
example can be modeled as searching for an instance x(cid:63) to satisfy the following constraints:

fθ(x(cid:63)) (cid:54)= y
d(x, x(cid:63)) ≤ B

(1)
(2)

where d(·, ·) is a metric to quantify the distance between an original image and its adversarial coun-
terpart, and B, called distortion, is an upper bound placed on this distance. Without loss of gener-
ality, we consider model f is composed of a network Jθ(x), which outputs the probability for each
category, so that f outputs the category with the highest probability.

Optimization-based approach. One approach is to approximate the solution to the following
optimization problem:

argminx(cid:63) λd(x, x(cid:63)) − (cid:96)(1y, Jθ(x(cid:63)))
(3)
where 1y is the one-hot encoding of the ground truth label y, (cid:96) is a loss function to measure the
distance between the prediction and the ground truth, and λ is a constant to balance constraints (2)
and (1), which is empirically determined. Here, loss function (cid:96) is used to approximate constraint (1),
and its choice can affect the effectiveness of searching for an adversarial example. In this work, we
choose (cid:96)(u, v) = log (1 − u · v), which is shown to be effective by Carlini & Wagner (2016).

Fast gradient sign (FGS). Goodfellow et al. (2014) proposed the fast gradient sign (FGS) method
so that the gradient needs be computed only once to generate an adversarial example. FGS can be
used to generate adversarial images to meet the L∞ norm bound. Formally, non-targeted adversarial
examples are constructed as

x(cid:63) ← clip(x + Bsgn(∇x(cid:96)(1y, Jθ(x))))
Here, clip(x) is used to clip each dimension of x to the range of pixel values, i.e., [0, 255] in this
work. We make a slight variation to choose (cid:96)(u, v) = log (1 − u · v), which is the same as used in
the optimization-based approach.

3

Published as a conference paper at ICLR 2017

Fast gradient (FG). The fast gradient approach (FG) is similar to FGS, but instead of moving
along the gradient sign direction, FG moves along the gradient direction. In particular, we have

x(cid:63) ← clip(x + B

∇x(cid:96)(1y, Jθ(x))
||∇x(cid:96)(1y, Jθ(x))||

))

Here, we assume the distance metric in constraint (2), d(x, x(cid:63)) = ||x − x(cid:63)|| is a norm of x − x(cid:63).
The term sgn(∇x(cid:96)) in FGS is replaced by ∇x(cid:96)

||∇x(cid:96)|| to meet this distance constraint.

We call both FGS and FG fast gradient-based approaches.

2.2.2 APPROACHES FOR GENERATING TARGETED ADVERSARIAL EXAMPLES

A targeted adversarial image x(cid:63) is similar to a non-targeted one, but constraint (1) is replaced by
fθ(x(cid:63)) = y(cid:63)

(4)
where y(cid:63) is the target label given by the adversary. For the optimization-based approach, we ap-
proximate the solution by solving the following dual objective:

argminx(cid:63) λd(x, x(cid:63)) + (cid:96)(cid:48)(1y(cid:63) , Jθ(x(cid:63)))

(5)

In this work, we choose the standard cross entropy loss (cid:96)(cid:48)(u, v) = − (cid:80)
i

ui log vi.

For FGS and FG, we construct adversarial examples as follows:

x(cid:63) ← clip(x − Bsgn(∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))))

(FGS)

x(cid:63) ← clip(x − B

∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))
||∇x(cid:96)(cid:48)(1y(cid:63) , Jθ(x))||

)

(FG)

where (cid:96)(cid:48) is the same as the one used for the optimization-based approach.

2.3 EVALUATION METHODOLOGY

For the rest of the paper, we focus on examining the transferability among state-of-the-art models
trained over ImageNet (Russakovsky et al. (2015)).
In this section, we detail the models to be
examined, the dataset to be evaluated, and the measurements to be used.

Models. We examine ﬁve networks, ResNet-50, ResNet-101, ResNet-152 (He et al. (2015))1,
GoogLeNet (Szegedy et al. (2014))2, and VGG-16 (Simonyan & Zisserman (2014))3. We retrieve
the pre-trained models for each network online. The performance of these models on the ILSVRC
2012 (Russakovsky et al. (2015)) validation set are presented in the appendix (Table 7). We choose
these models to study the transferability between homogeneous architectures (i.e., ResNet models)
and heterogeneous architectures.

Dataset.
It is less meaningful to examine the transferability of an adversarial image between two
models which cannot classify the original image correctly. Therefore, from the ILSVRC 2012 val-
idation set, we randomly choose 100 images, which can be classiﬁed correctly by all ﬁve models
in our examination. These 100 images form our test set. To perform targeted attacks, we manually
choose a target label for each image, so that its semantics is far from the ground truth. The images
and target labels in our evaluation can be found on website4.

Measuring transferability. Given two models, we measure the non-targeted transferability by
computing the percentage of the adversarial examples generated for one model that can be classiﬁed
correctly for the other. We refer to this percentage as accuracy. A lower accuracy means better
non-targeted transferability. We measure the targeted transferability by computing the percentage of
the adversarial examples generated for one model that are classiﬁed as the target label by the other
model. We refer to this percentage as matching rate. A higher matching rate means better targeted
transferability. For clarity, the reported results are only based on top-1 accuracy. Top-5 accuracy’s
counterparts can be found in the appendix.

1https://github.com/KaimingHe/deep-residual-networks
2https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet
3https://gist.github.com/ksimonyan/211839e770f7b538e2d8
4https://github.com/sunblaze-ucb/transferability-advdnn-pub

4

Published as a conference paper at ICLR 2017

Distortion. Besides transferability, another important factor is the distortion between adversarial
images and the original ones. We measure the distortion by root mean square deviation, i.e., RMSD,
which is computed as d(x(cid:63), x) = (cid:112)(cid:80)
i − xi)2/N , where x(cid:63) and x are the vector representations
of an adversarial image and the original one respectively, N is the dimensionality of x and x(cid:63), and
xi denotes the pixel value of the i-th dimension of x, within range [0, 255], and similar for x(cid:63)
i .

i(x(cid:63)

3 NON-TARGETED ADVERSARIAL EXAMPLES

In this section, we examine different approaches for generating non-targeted adversarial images.

3.1 OPTIMIZATION-BASED APPROACH

To apply the optimization-based approach for a single model, we initialize x(cid:63) to be x and use Adam
Optimizer (Kingma & Ba (2014)) to optimize Objective (3) . We ﬁnd that we can tune the RMSD
by adjusting the learning rate of Adam and λ. We ﬁnd that, for each model, we can use a small
learning rate to generate adversarial images with small RMSD, i.e. < 2, with any λ. In fact, we ﬁnd
that when initializing x(cid:63) with x, Adam Optimizer will search for an adversarial example around x,
even when we set λ to be 0, i.e., not restricting the distance between x(cid:63) and x. Therefore, we set
λ to be 0 for all experiments using optimization-based approaches throughout the paper. Although
these adversarial examples with small distortions can successfully fool the target model, however,
they cannot transfer well to other models (see Table 15 and 16 in the appendix for details).

We increase the learning rate to allow the optimization algorithm to search for adversarial images
with larger distortion. In particular, we set the learning rate to be 4. We run Adam Optimizer for 100
iterations to generate the adversarial images. We observe that the loss converges after 100 iterations.
An alternative optimization-based approach leading to similar results can be found in the appendix.

Non-targeted adversarial examples transfer. We generate non-targeted adversarial examples on
one network, but evaluate them on another, and Table 1 Panel A presents the results. From the table,
we can observe that

• The diagonal contains all 0 values. This says that all adversarial images generated for one

model can mislead the same model.

• A large proportion of non-targeted adversarial images generated for one model using the

optimization-based approach can transfer to another.

• Although the three ResNet models share similar architectures which differ only in the hy-
perparameters, adversarial examples generated against a ResNet model do not necessarily
transfer to another ResNet model better than other non-ResNet models. For example, the
adversarial examples generated for VGG-16 have lower accuracy on ResNet-50 than those
generated for ResNet-152 or ResNet-101.

3.2 FAST GRADIENT-BASED APPROACHES

We then examine the effectiveness of fast gradient-based approaches. A good property of fast
gradient-based approaches is that all generated adversarial examples lie in a 1-D subspace. There-
fore, we can easily approximate the minimal distortion in this subspace of transferable adversarial
examples between two models. In the following, we ﬁrst control the RMSD to study fast gradient-
based approaches’ effectiveness. Second, we study the transferable minimal distortions of fast
gradient-based approaches.

3.2.1 EFFECTIVENESS AND TRANSFERABILITY OF THE FAST GRADIENT-BASED

APPROACHES

Since the distortion B and the RMSD of the generated adversarial images are highly correlated, we
can choose this hyperparameter B to generate adversarial images with a given RMSD. In Table 1
Panel B, we generate adversarial images using FG such that the average RMSD is almost the same
as those generated using the optimization-based approach. We observe that the diagonal values in

5

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
22.83
23.81
22.86
22.51
22.58

ResNet-101
13%
0%
20%
17%
38%
Panel A: Optimization-based approach

ResNet-50 VGG-16 GoogLeNet
19%
21%
21%
0%
19%

0%
19%
23%
22%
39%

18%
21%
0%
17%
34%

11%
12%
18%
5%
0%

RMSD
23.45
23.49
23.49
23.73
23.45

ResNet-152
ResNet-101
4%
13%
13%
19%
4%
11%
25%
19%
5%
20%
16%
15%
17%
25%
25%
Panel B: Fast gradient approach

ResNet-50 VGG-16 GoogLeNet
20%
23%
25%
1%
19%

12%
13%
14%
7%
1%

Table 1: Transferability of non-targeted adversarial images generated between pairs of models. The
ﬁrst column indicates the average RMSD of all adversarial images generated for the model in the
corresponding row. The cell (i, j) indicates the accuracy of the adversarial images generated for
model i (row) evaluated over model j (column). Results of top-5 accuracy can be found in the
appendix (Table 8 and Table 10).

the table are all positive, which means that FG cannot fully mislead the models. A potential reason
is that, FG can be viewed as approximating the optimization, but is tailored for speed over accuracy.

On the other hand, the values of non-diagonal cells in the table, which correspond to the accuracies
of adversarial images generated for one model but evaluated on another, are comparable with or less
than their counterparts in the optimization-based approach. This shows that non-targeted adversarial
examples generated by FG exhibit transferability as well.

We also evaluate FGS, but the transferability of the generated images is worse than the ones gener-
ated using either FG or optimization-based approaches. The results are in the appendix (Table 19
and 20). It shows that when RMSD is around 23, the accuracies of the adversarial images generated
by FGS is greater than their counterparts for FG. We hypothesize the reason why transferability of
FGS is worse to this fact.

3.2.2 ADVERSARIAL IMAGES WITH MINIMAL TRANSFERABLE RMSD

For an image x and two models M1, M2, we can approximate the minimal distortion B along a
direction δ, such that xB = x + Bδ generated for M1 is adversarial for both M1 and M2. Here δ is
the direction, i.e., sgn(∇x(cid:96)) for FGS, and ∇x(cid:96)/||∇x(cid:96)|| for FG.

We refer to the minimal transferable RMSD from M1 to M2 using FG (or FGS) as the RMSD of
a transferable adversarial example xB with the minimal transferable distortion B from M1 to M2
using FG (or FGS). The minimal transferable RMSD can illustrate the tradeoff between distortion
and transferability.

In the following, we approximate the minimal transferable RMSD through a linear search by sam-
pling B every 0.1 step. We choose the linear-search method rather than binary-search method to de-
termine the minimal transferable RMSD because the adversarial images generated from an original
image may come from multiple intervals. The experiment can be found in the appendix (Figure 6).

Minimal transferable RMSD using FG and FGS. Figure 1 plots the cumulative distribution
function (CDF) of the minimal transferable RMSD from VGG-16 to ResNet-152 using non-targeted
FG (Figure 1a) and FGS (Figure 1b). From the ﬁgures, we observe that both FG and FGS can ﬁnd
100% transferable adversarial images with RMSD less than 80.91 and 86.56 respectively. Further,
the FG method can generate transferable attacks with smaller RMSD than FGS. A potential rea-
son is that while FGS minimizes the distortion’s L∞ norm, FG minimizes its L2 norm, which is
proportional to RMSD.

6

Published as a conference paper at ICLR 2017

(a) Fast Gradient

(b) Fast Gradient Sign

Figure 1: The CDF of the minimal transferable RMSD from VGG-16 to ResNet-152 using FG (a)
and FGS (b). The green line labels the median minimal transferable RMSD, while the red line labels
the minimal transferable RMSD to reach 90% percentage.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.13
23.16
23.06
23.59
22.87

100%
3%
4%
2%
1%

ResNet-101
2%
100%
2%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
2%
1%
100%
1%

1%
1%
1%
1%
100%

1%
3%
100%
2%
0%

Table 2: The matching rate of targeted adversarial images generated using the optimization-based
approach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) indicates that matching rate of the targeted adversarial images generated for model i (row)
when evaluated on model j (column). The top-5 results can be found in the appendix (Table 12).

3.3 COMPARISON WITH RANDOM PERTURBATIONS

We also evaluate the test accuracy when we add a Gaussian noise to the 100 images in our test
set. The concrete results can be found in the appendix, and we show the conclusion that the “trans-
ferability” of this approach is signiﬁcantly worse than either optimization-based approaches or fast
gradient-based approaches.

4 TARGETED ADVERSARIAL EXAMPLES

In this section, we examine the transferability of targeted adversarial images. Table 2 presents
the results for using optimization-based approach. We observe that (1) the prediction of targeted
adversarial images can match the target labels when evaluated on the same model that is used to
generate the adversarial examples; but (2) the targeted adversarial images can be rarely predicted
as the target labels by a different model. We call the latter that the target labels do not transfer.
Even when we increase the distortion, we still do not observe improvements on making target label
transfer. Some results can be found in the appendix (Table 17). Even if we compute the matching
rate based on top-5 accuracy, the highest matching rate is only 10%. The results can be found in the
appendix (Table 18).

We also examine the targeted adversarial images generated by fast gradient-based approaches, and
we observe that the target labels do not transfer as well. The results are deferred to the appendix
(Table 25). In fact, most targeted adversarial images cannot mislead the model, for which the ad-
versarial images are generated, to predict the target labels, regardless of how large the distortion is
used. We attribute it to the fact that the fast gradient-based approaches only search for attacks in
a 1-D subspace. In this subspace, the total possible predictions may contain a small subset of all
labels, which usually does not contain the target label. In Section 6, we study decision boundaries
regarding this issue.

We also evaluate the matching rate of images added with Gaussian noise, as described in Section 3.3.
However, we observe that the matching rate of any of the 5 models is 0%. Therefore, we conclude

7

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.68
30.76
30.26
31.13
29.70

38%
75%
84%
74%
90%

ResNet-101
76%
43%
81%
78%
87%

ResNet-50 VGG-16 GoogLeNet
97%
98%
99%
24%
99%

70%
69%
46%
68%
83%

76%
73%
77%
63%
11%

Table 3: The matching rate of targeted adversarial images generated using the optimization-based
approach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) indicates that percentage of the targeted adversarial images generated for the ensemble of the
four models except model i (row) is predicted as the target label by model j (column). In each row,
the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Results of top-5 matching rate can be found in the appendix (Table 13).

that by adding Gaussian noise, the attacker cannot generate successful targeted adversarial examples
at all, let alone targeted transferability.

5 ENSEMBLE-BASED APPROACHES

We hypothesize that if an adversarial image remains adversarial for multiple models, then it is more
likely to transfer to other models as well. We develop techniques to generate adversarial images for
multiple models. The basic idea is to generate adversarial images for the ensemble of the models.
Formally, given k white-box models with softmax outputs being J1, ..., Jk, an original image x,
and its ground truth y, the ensemble-based approach solves the following optimization problem (for
targeted attack):

argminx(cid:63) − log (cid:0)(

αiJi(x(cid:63))) · 1y(cid:63)

(cid:1) + λd(x, x(cid:63))

(6)

k
(cid:88)

i=1

where y(cid:63) is the target label speciﬁed by the adversary, (cid:80) αiJi(x(cid:63)) is the ensemble model, and αi
are the ensemble weights, (cid:80)k
i=1 αi = 1. Note that (6) is the targeted objective. The non-targeted
counterpart can be derived similarly. In doing so, we hope the generated adversarial images remain
adversarial for an additional black-box model Jk+1.

We evaluate the effectiveness of the ensemble-based approach. For each of the ﬁve models, we treat
it as the black-box model to attack, and generate adversarial images for the ensemble of the rest
four, which is considered as white-box. We evaluate the generated adversarial images over all ﬁve
models. Throughout the rest of the paper, we refer to the approaches evaluated in Section 3 and 4 as
the approaches using a single model, and to the ensemble-based approaches discussed in this section
as the approaches using an ensemble model.

Optimization-based approach. We use Adam to optimize the objective (6) with equal ensemble
weights across all models in the ensemble to generate targeted adversarial examples. In particular,
we set the learning rate of Adam to be 8 for each model. In each iteration, we compute the Adam
update for each model, sum up the four updates, and add the aggregation onto the image. We run 100
iterations of updates, and we observe that the loss converges after 100 iterations. By doing so, for the
ﬁrst time, we observe a large proportion of the targeted adversarial images whose target labels can
transfer. The results are presented in Table 3. We observe that not all targeted adversarial images
can be misclassiﬁed to the target labels by the models used in the ensemble. This suggests that
while searching for an adversarial example for the ensemble model, there is no direct supervision to
mislead any individual model in the ensemble to predict the target label. Further, from the diagonal
numbers of the table, we observe that the transferability to ResNet models is better than to VGG-16
or GoogLeNet, when adversarial examples are generated against all models except the target model.

We also evaluate non-targeted adversarial images generated by the ensemble-based approach. We
observe that the generated adversarial images have almost perfect transferability. We use the same
procedure as for the targeted version, except the objective to generate the adversarial images. We
evaluate the generated adversarial images over all models. The results are presented in Table 4.
The generated adversarial images all have RMSDs around 17, which are lower than 22 to 23 of

8

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.17
17.25
17.25
17.80
17.41

0%
0%
0%
0%
0%

ResNet-101
0%
1%
0%
0%
0%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
6%
0%

0%
0%
2%
0%
0%

0%
0%
0%
0%
5%

Table 4: Accuracy of non-targeted adversarial images generated using the optimization-based ap-
proach. The ﬁrst column indicates the average RMSD of the generated adversarial images. Cell
(i, j) corresponds to the accuracy of the attack generated using four models except model i (row)
when evaluated over model j (column). In each row, the minus sign “−” indicates that the model
of the row is not used when generating the attacks. Results of top-5 accuracy can be found in the
appendix (Table 14).

Model
# of labels

VGG-16 ResNet-50 ResNet-101 ResNet-152 GoogLeNet

10

9

21

10

21

Table 5: The number of all possible predicted labels for each model in the same plane described in Figure 3.

the optimization-based approach using a single model (See Table 1 for comparison). When the
adversarial images are evaluated over models which are not used to generate the attack, the accuracy
is no greater than 6%. For a reference, the corresponding accuracies for all approaches evaluated in
Section 3 using one single model are at least 12%. Our experiments demonstrate that the ensemble-
based approaches can generate almost perfectly transferable adversarial images.

Fast gradient-based approach. The results for non-targeted fast gradient-based approaches ap-
plied to the ensemble can be found in the appendix (Table 21, 22, 23 and 24). We observe that the
diagonal values are not zero, which is the same as we observed in the results for FG and FGS applied
to a single model. We hypothesize a potential reason is that the gradient directions of different mod-
els in the ensemble are orthogonal to each other, as we will illustrate in Section 6. In this case, the
gradient direction of the ensemble is almost orthogonal to the one of each model in the ensemble.
Therefore searching along this direction may require large distortion to reach adversarial examples.

For targeted adversarial examples generated using FG and FGS based on an ensemble model, their
transferability is no better than the ones generated using a single model. The results can be found in
the appendix (Table 28, 29, 30 and 31). We hypothesize the same reason to explain this: there are
only few possible target labels in total in the 1-D subspace.

6 GEOMETRIC PROPERTIES OF DIFFERENT MODELS

In this section, we show some geometric properties of the models to try to better understand transfer-
able adversarial examples. Prior works also try to understand the geometic properties of adversarial
examples theoretically (Fawzi et al. (2016)) or empirically (Goodfellow et al. (2014)). In this work,
we examine large models trained over a large dataset with 1000 labels, whose geometric properties
are never examined before. This allows us to make new observations to better understand the models
and their adversarial examples.

The gradient directions of different models in our evaluation are almost orthogonal to each
other. We study whether the adversarial directions of different models align with each other. We
calculate cosine value of the angle between gradient directions of different models, and the results
can be found in the appendix (Table 33). We observe that all non-diagonal values are close to 0,
which indicates that for most images, their gradient directions with respect to different models are
orthogonal to each other.

Decision boundaries of the non-targeted approaches using a single model. We study the deci-
sion boundary of different models to understand why adversarial examples transfer. We choose two
normalized orthogonal directions δ1, δ2, one being the gradient direction of VGG-16 and the other
being randomly chosen. Each point (u, v) in this 2-D plane corresponds to the image x + uδ1 + vδ2,

9

Published as a conference paper at ICLR 2017

Figure 2: The example image to study the decision boundary. Its ID in ILSVRC 2012 validation set
is 49443, and its ground truth label is “anemone ﬁsh.”

VGG-16

ResNet-50

ResNet-101

ResNet-152

GoogLeNet

n
i
-

m
o
o
Z

t
u
o
-
m
o
o
Z

Figure 3: Decision regions of different models. We pick the same two directions for all plots: one is
the gradient direction of VGG-16 (x-axis), and the other is a random orthogonal direction (y-axis).
Each point in the span plane shows the predicted label of the image generated by adding a noise to
the original image (e.g., the origin corresponds to the predicted label of the original image). The
units of both axises are 1 pixel values. All sub-ﬁgure plots the regions on the span plane using the
same color for the same label. The image is in Figure 2.

where x is the pixel value vector of the original image. For each model, we plot the label of the
image corresponding to each point, and get Figure 3 using the image in Figure 2.

We can observe that for all models, the region that each model can predict the image correctly
is limited to the central area. Also, along the gradient direction, the classiﬁers are soon misled.
One interesting ﬁnding is that along this gradient direction, the ﬁrst misclassiﬁed label for the three
ResNet models (corresponding to the light green region) is the label “orange”. A more detailed study
can be found in the appendix (Table 9, Table 26 and 27). When we look at the zoom-out ﬁgures,
however, the labels of images that are far away from the original one are different for different
models, even among ResNet models.

On the other hand, in Table 5, we show the total number of regions in each plane. In fact, for each
plane, there are at most 21 different regions in all planes. Compared with the 1,000 total categories
in ImageNet, this is only 2.1% of all categories. That means, for all other 97.9% labels, no targeted
adversarial example exists in each plane. Such a phenomenon partially explains why fast gradient-
based approaches can hardly ﬁnd targeted adversarial images.

Further, in Figure 4, we draw the decision boundaries of all models on the same plane as described
above. We can observe that

• The boundaries align with each other very well. This partially explains why non-targeted

adversarial images can transfer among models.

10

Published as a conference paper at ICLR 2017

Figure 4: The decision boundary to sep-
arate the region within which all points
are classiﬁed as the ground truth label
(encircled by each closed curve) from
others. The plane is the same one de-
scribed in Figure 3.
The origin of
the coordinate plane corresponds to the
original image. The units of both axises
are 1 pixel values.

Figure 5: The decision boundary to separate the
region within which all points are classiﬁed as the
target label (encircled by each closed curve) from
others. The plane is spanned by the targeted ad-
versarial direction and a random orthogonal di-
rection. The targeted adversarial direction is com-
puted as the difference between the original image
in Figure 2 and the adversarial image generated by
the optimization-based approach for an ensemble.
The ensemble contains all models except ResNet-
101. The origin of the coordinate plane corre-
sponds to the original image. The units of both
axises are 1 pixel values.

• The boundary diameters along the gradient direction is less than the ones along the ran-
dom direction. A potential reason is that moving a variable along its gradient direction
can change the loss function (i.e., the probability of the ground truth label) signiﬁcantly.
Therefore along the gradient direction it will take fewer steps to move out of the ground
truth region than a random direction.

• An interesting ﬁnding is that even though we move left along the x-axis, which is equivalent
to maximizing the ground truth’s prediction probability, it also reaches the boundary much
sooner than moving along a random direction. We attribute this to the non-linearity of the
loss function: when the distortion is larger, the gradient direction also changes dramatically.
In this case, moving along the original gradient direction no longer increases the probability
to predict the ground truth label (see Figure 7 in the appendix).

• As for VGG-16 model, there is a small hole within the region corresponding to the ground
truth. This may partially explain why non-targeted adversarial images with small distortion
exist, but do not transfer well. This hole does not exist in other models’ decision planes. In
this case, non-targeted adversarial images in this hole do not transfer.

Decision boundaries of the targeted ensemble-based approaches.
In addition, we choose the
targeted adversarial direction of the ensemble of all models except ResNet-101 and a random or-
thogonal direction, and we plot decision boundaries on the plane spanned by these two direction
vectors in Figure 5. We observe that the regions of images, which are predicted as the target label,
align well for the four models in the ensemble. However, for the model not used to generate the
adversarial image, i.e., ResNet-101, it also has a non-empty region such that the prediction is suc-
cessfully misled to the target label, although the area is much smaller. Meanwhile, the region within
each closed curve of the models almost has the same center.

7 REAL WORLD EXAMPLE: ADVERSARIAL EXAMPLES FOR CLARIFAI.COM

Clarifai.com is a commercial company providing state-of-the-art image classiﬁcation services. We
have no knowledge about the dataset and types of models used behind Clarifai.com, except that we
have black-box access to the services. The labels returned from Clarifai.com are also different from

11

Published as a conference paper at ICLR 2017

the categories in ILSVRC 2012. We submit all 100 original images to Clarifai.com and the returned
labels are correct based on a subjective measure.

We also submit 400 adversarial images in total, where 200 of them are targeted adversarial examples,
and the rest 200 are non-targeted ones. As for the 200 targeted adversarial images, 100 of them
are generated using the optimization-based approach based on VGG-16 (the same ones evaluated
in Table 2), and the rest 100 are generated using the optimization-based approach based on an
ensemble of all models except ResNet-152 (the same ones evaluated in Table 3). The 200 non-
targeted adversarial examples are generated similarly (the same ones evaluated in Table 1 and 4).

For non-targeted adversarial examples, we observe that for both the ones generated using VGG-16
and those generated using the ensemble, most of them can transfer to Clarifai.com.

More importantly, a large proportion of our targeted adversarial examples are misclassiﬁed by Clari-
fai.com as well. We observe that 57% of the targeted adversarial examples generated using VGG-16,
and 76% of the ones generated using the ensemble can mislead Clarifai.com to predict labels irrele-
vant to the ground truth.

Further, our experiment shows that for targeted adversarial examples, 18% of those generated us-
ing the ensemble model can be predicted as labels close to the target label by Clarifai.com. The
corresponding number for the targeted adversarial examples generated using VGG-16 is 2%. Con-
sidering that in the case of attacking Clarifai.com, the labels given by the target model are different
from those given by our models, it is fairly surprising to see that when using the ensemble-based
approach, there is still a considerable proportion of our targeted adversarial examples that can mis-
lead this black-box model to make predictions semantically similar to our target labels. All these
numbers are computed based on a subjective measure, and we include some examples in Table 6.
More examples can be found in the appendix (Table 34).

original
image

true
label

Clarifai.com
results of
original image

target
label

targeted
adversarial
example

Clarifai.com results
of targeted
adversarial example

window,
wall,
old,
decoration,
design

Buddha,
gold,
temple,
celebration,
artistic

cherry,
branch,
fruit,
food,
season

sea seal,
ocean,
head,
sea,
cute

viaduct

hip, rose
hip,
rosehip

bridge,
sight,
arch,
river,
sky

fruit,
fall,
food,
little,
wildlife

dogsled,
dog
sled,
dog
sleigh

group together,
four,
sledge,
sled,
enjoyment

pug,
pug-dog

pug,
friendship,
adorable,
purebred,
sit

window
screen

stupa,
tope

hip, rose
hip,
rosehip

sea lion

12

Published as a conference paper at ICLR 2017

Old
English
sheep-
dog,
bobtail

maillot,
tank suit

patas,
hussar
monkey,
Erythro-
cebus
patas

poodle,
retriever,
loyalty,
sit,
two

beach,
woman,
adult,
wear,
portrait

primate,
monkey,
safari,
sit,
looking

abaya

amphib-
ian,
amphibi-
ous
vehicle

bee eater

veil,
spirituality,
religion,
people,
illustration

transportation
system,
vehicle,
man,
print,
retro

ornithology,
avian,
beak,
wing,
feather

Table 6: Original images and adversarial images evaluated over Clarifai.com. For labels returned
from Clarifai.com, we sort the labels ﬁrstly by rareness: how many times a label appears in the
Clarifai.com results for all adversarial images and original images, and secondly by conﬁdence.
Only top 5 labels are provided.

8 CONCLUSION

In this work, we are the ﬁrst to conduct an extensive study of the transferability of both non-targeted
and targeted adversarial examples generated using different approaches over large models and a
large scale dataset. Our results conﬁrm that the transferability for non-targeted adversarial exam-
ples are prominent even for large models and a large scale dataset. On the other hand, we ﬁnd that
it is hard to use existing approaches to generate targeted adversarial examples whose target labels
can transfer. We develop novel ensemble-based approaches, and demonstrate that they can gen-
erate transferable targeted adversarial examples with a high success rate. Meanwhile, these new
approaches exhibit better performance on generating non-targeted transferable adversarial examples
than previous work. We also show that both non-targeted and targeted adversarial examples gen-
erated using our new approaches can successfully attack Clarifai.com, which is a black-box image
classiﬁcation system. Furthermore, we study some geometric properties to better understand the
transferable adversarial examples.

This material is in part based upon work supported by the National Science Foundation under Grant
No. TWC-1409915. Any opinions, ﬁndings, and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily reﬂect the views of the National Science
Foundation.

ACKNOWLEDGMENTS

REFERENCES

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. arXiv

preprint arXiv:1608.04644, 2016.

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classiﬁers:
from adversarial to random noise. In Advances in Neural Information Processing Systems, pp.
1624–1632, 2016.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014.

13

Published as a conference paper at ICLR 2017

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-

nition. arXiv preprint arXiv:1512.03385, 2015.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,

abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. arXiv preprint arXiv:1610.08401, 2016.

Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against deep learning systems using adversarial examples.
arXiv preprint arXiv:1602.02697, 2016b.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.

J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine
learning algorithms for trafﬁc sign recognition. Neural Networks, (0):–, 2012. ISSN 0893-6080.
doi: 10.1016/j.neunet.2012.02.016. URL http://www.sciencedirect.com/science/
article/pii/S0893608012000457.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.

14

Published as a conference paper at ICLR 2017

APPENDIX

Top-1 accuracy
Top-5 accuracy

ResNet-50
72.5%
91.0%

ResNet-101
73.8%
91.7%

ResNet-152 GoogLeNet VGG-16
68.3%
88.3%

68.7%
89.0%

74.6%
92.1%

Table 7: Top-1 and top-5 accuracy of the studied models over the ILSVRC 2012 validation dataset.

An alternative optimization-based approach to generate adversarial examples. An alternative
method to generate non-targeted adversarial examples with large distortion is to revise the optimiza-
tion objective to incorporate this distortion constraint. For example, for non-targeted adversarial
image searching, we can optimize for the following objective.

argminx(cid:63) − log (1 − 1y · Jθ(x(cid:63))) + λ1ReLU(τ − d(x, x(cid:63))) + λ2ReLU(d(x, x(cid:63)) − τ )

Optimizing for
(1) minimizing
− log (1 − 1y · Jθ(x(cid:63))); (2) Penalizing the solution if d(x, x(cid:63)) is no more than a threshold τ (too
low); and (3) Penalizing the solution if d(x, x(cid:63)) is too high.

the following three effects:

the above objective has

In our preliminary evaluation, we found that the solutions computed from the two approaches have
similar transferability. We thus omit the results for this alternative approach.

Transferable non-targeted adversarial images are classiﬁed as the same wrong labels. Previ-
ous work Goodfellow et al. (2014) reported the phenomenon that when evaluating the adversarial
images, different models tend to make the same wrong predictions. This conclusion was only exam-
ined over datasets with 10 categories. In our evaluation, however, we observe the same phenomenon,
albeit we have 1000 possible categories. We refer to this effect as the same mistake effect.

Table 9 presents the results based on the adversarial examples generated for VGG-16. For each pair
of models, among all adversarial examples that both models make wrong predictions, we compute
the percentage that both models make the same mistake. These percentage numbers are from 12% to
40%, which are surprisingly high, since there are 999 possible categories to be misclassiﬁed into (see
Table 32 in the appendix for the wrong predicted label distribution of these adversarial examples).
Later in Section 6, we try to explain this phenomenon using decision boundaries.

Adversarial images may come from multiple intervals along the gradient direction.
In Fig-
ure 6, we show that along the gradient direction of the non-targeted objective (3) for VGG-16, when
evaluating the adversarial image xB on ResNet-152, xB will soon become adversarial for small B.
While B increases, however, the prediction changes back to be the correct label, then a wrong label,
then the correct label again, and ﬁnally wrong labels. This indicates that along the gradient of this
image, the distortions that can cause the corresponding adversarial images to mislead ResNet-152
form four intervals.

Comparison with random perturbations. For comparison, we evaluate the test accuracy when
we add a Gaussian noise to the 100 images in our test set. We vary the standard deviation of the
Gaussian noise from 5 to 40 with a step size of 5. For each speciﬁc standard deviation, we generate

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
22.83
23.81
22.86
22.51
22.58

7%
40%
48%
36%
66%

ResNet-101
43%
6%
44%
33%
71%

ResNet-50 VGG-16 GoogLeNet
39%
42%
42%
0%
49%

43%
41%
3%
33%
62%

31%
34%
32%
15%
2%

Table 8: Top-5 accuracy of Table 1 Panel A. Transferability between pairs of models using non-
targeted optimization-based approach with a learning rate of 4. The ﬁrst column indicates the av-
erage RMSD of all adversarial images generated for the model in the corresponding row. The cell
(i, j) indicates the top-5 accuracy of the adversarial images generated for model i (row) evaluated
over model j (column). Lower value indicates better transferability.

15

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
28.57%
29.87%
19.23%
12.82%

ResNet-101
−
100.00%
40.00%
18.07%
20.48%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
22.89% 100.00%
16.84%
18.07%

GoogLeNet
−
−
−
−
100.00%

Table 9: When using the optimization-based approach for VGG-16 model to generate non-targeted adversarial
images, cell (i, j) is the percentage of all transferable adversarial images that are predicted as the same wrong
labels by both models i and j over all adversarial images that are misclassiﬁed by both models i and j. Notice
that this table is symmetric.

(a) The original image

(b) Prediction correctness

Figure 6: We add Bδ onto the original image in Figure 6a, where δ is the normalized gradient of
the non-targeted objective (3) for VGG-16. When xB is evaluated on ResNet-152, we plot 1 if the
prediction matches the ground truth, or 0 otherwise.

100 random noises and add them to each image, resulting in 10,000 noisy images in total. Then
we evaluate the accuracy of each model on these 10,000 images, and the results are presented in
Table 11. Notice that when setting standard deviation to be 25, the average RMSD is 23.59, which
is comparable to that of non-targeted adversarial examples generated by either optimization-based
approaches or fast gradient-based approaches. However, each model can still achieve an accuracy
more than 66%. This shows that adding random noise is not an effective way to generate adversarial
examples, hence the “transferability” of this approach is signiﬁcantly worse than either optimization-
based approaches or fast gradient-based approaches.

More examples submitted to Clarifai.com. We present more results from Clarifai.com by sub-
mitting original and adversarial examples in Table 34.

original
image

true
label

Clarifai.com
results of
original image

target
label

targeted
adv-example

Clarifai.com result
of targeted
adversarial example

broom

jacamar

dust,
brick,
rustic,
stone,
dirty

frost,
sparrow,
pigeon,
ice,
frosty

heavy,
bulldozer,
exert,
track,
plow

junco,
snow-
bird

har-
vester,
reaper

eel

prairie
chicken,
prairie
grouse,
prairie
fowl

16

feather,
beautiful,
bird,
leaf,
ﬂora

swimming,
underwater,
ﬁsh,
water,
one

wildlife,
animal,
illustration,
nature,
color

Published as a conference paper at ICLR 2017

broccoli

hamster

mon-
goose

holster

yurt

water
buffalo,
water
ox,
Asiatic
buffalo,
Bubalus
bubalis

zebra

French
bulldog

curly-
coated
retriever

fox,
rodent,
predator,
fur,
park

pistol,
force,
bullet,
protection,
cartridge

wooden,
scenic,
snow,
rural,
landscape

kind,
cauliﬂower,
vitamin,
carrot,
cabbage

herd,
milk,
beef cattle,
farmland,
cow

equid,
stripe,
savanna,
zebra,
safari

bulldog,
studio,
boxer,
eye,
bull

eye,
looking,
pet,
canine,
dog

maillot

ground
beetle,
carabid
beetle

comic
book

rugby ball

apiary,
bee house

kite

ﬂy

17

motley,
shape,
horizontal,
abstract,
bright

shell,
shell (food),
antenna,
insect,
invertebrate

grafﬁti,
religion,
people,
painting,
culture

bird,
bright,
texture,
animal,
decoration

pastime,
print,
illustration,
art

wood,
people,
outdoors,
nature

visuals,
feather,
wing,
pet,
print

graphic,
shape,
insect,
artistic,
image

Table 34: Original images and adversarial images evaluated over Clarifai.com. For labels returned
from Clarifai.com, we sort the labels ﬁrstly by the occurrence of a label from the Clarifai.com results,
and secondly by conﬁdence. Only top 5 labels are provided.

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.45
23.49
23.49
23.73
23.45

23%
35%
40%
40%
53%

ResNet-101
29%
20%
39%
35%
46%

ResNet-50 VGG-16 GoogLeNet
39%
43%
39%
8%
38%

33%
28%
33%
19%
7%

31%
33%
18%
33%
37%

Table 10: Top-5 accuracy of Table 1 Panel B. Transferability between pairs of models using non-
targeted FG. The ﬁrst column indicates the average RMSD of all adversarial images generated for
the model in the corresponding row. The cell (i, j) indicates the top-5 accuracy of the adversarial
images generated for model i (row) evaluated over model j (column). Lower value indicates better
transferability.

Standard Deviation
5
10
15
20
25
30
35
40

RMSD ResNet-152
4.91
9.72
14.44
19.07
23.59
28.01
32.32
36.52

97.41%
95.53%
91.19%
86.56%
83.10%
78.95%
73.60%
66.53%

ResNet-101
98.96%
96.72%
94.22%
90.38%
85.53%
79.04%
70.89%
63.09%

ResNet-50 VGG-16 GoogLeNet
98.74% 97.47%
96.81% 92.13%
92.16% 87.86%
84.07% 82.30%
78.33% 73.57%
71.66% 65.33%
62.03% 58.55%
50.96% 51.85%

99.29%
95.94%
88.50%
77.84%
66.84%
54.93%
45.13%
35.61%

Table 11: Accuracy of images with random perturbation. The ﬁrst column reports the standard
deviation of the Gaussian noise added to each image. The second column reports the average RMSD
over all generated images with the respective standard deviation. For each of the rest column j, the
cell (i, j) reports model j’s accuracy of the noisy images when a Gaussian noise with the respective
standard deviation speciﬁed in row i is added to each image.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.13
23.16
23.06
23.59
22.87

100%
9%
10%
3%
1%

ResNet-101
11%
100%
9%
5%
2%

ResNet-50 VGG-16 GoogLeNet
3%
2%
2%
100%
3%

5%
7%
100%
5%
1%

1%
1%
3%
4%
100%

Table 12: Top-5 matching rate of Table 2. The adversarial images are generated using the targeted
optimization-based approach with a learning rate of 4. The ﬁrst column indicates the average RMSD
of all adversarial images generated for the model in the corresponding row. Cell (i, j) indicates that
top-5 matching rate of the targeted adversarial images generated for model i (row) when evaluated
on model j (column). Higher value indicates more successful transferable target labels.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.68
30.76
30.26
31.13
29.70

87%
100%
99%
100%
100%

ResNet-101
100%
88%
99%
100%
100%

ResNet-50 VGG-16 GoogLeNet
100%
100%
99%
51%
100%

100%
100%
99%
100%
32%

100%
100%
86%
100%
100%

Table 13: The top-5 matching rate of Table 3. Matching rate of adversarial images generated using
targeted optimization-based approach. The ﬁrst column indicates the average RMSD of all adver-
sarial images generated for the model in the corresponding row. Cell (i, j) indicates that top-5
matching rate of the targeted adversarial images generated using the ensemble of the four models
except model i (row) is predicted as the target label by model j (column). In each row, the minus
sign “−” indicates that the model of the row is not used when generating the attacks. Higher value
indicates more successful transferable target labels.

18

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.17
17.25
17.25
17.80
17.41

13%
2%
4%
4%
3%

ResNet-101
4%
11%
5%
7%
2%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
20%
0%

4%
3%
11%
5%
3%

3%
3%
2%
4%
15%

Table 14: Top-5 accuracy of Table 4. The ﬁrst column indicates the average RMSD of all adversarial
images generated for the model in the corresponding row. Cell (i, j) indicates that top-5 accuracy
of the non-targeted adversarial images generated using the ensemble of the four models except
model i (row) when evaluated over model j (column). In each row, the minus sign “−” indicates
that the model of the row is not used when generating the attacks. Lower value indicates better
transferability.

RMSD ResNet-152

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

1.25
1.24
1.21
1.55
1.27

0%
84%
90%
89%
94%

ResNet-101
86%
0%
91%
94%
97%

ResNet-50 VGG-16 GoogLeNet
93%
95%
91%
0%
91%

96%
100%
97%
84%
0%

87%
93%
0%
92%
98%

Table 15: Transferability between pairs of models using non-targeted optimization-based approach
with a learning rate of 0.125. The ﬁrst column indicates the average RMSD of all adversarial images
generated for the model in the corresponding row. The cell (i, j) indicates the top-1 accuracy of the
adversarial images generated for model i (row) evaluated over model j (column). Lower value
indicates better transferability. Results of top-5 accuracy can be found in Table 16.

RMSD ResNet-152

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

1.25
1.24
1.21
1.55
1.27

24%
100%
100%
98%
100%

ResNet-101
100%
22%
99%
100%
100%

ResNet-50 VGG-16 GoogLeNet
99%
100%
100%
15%
100%

100%
100%
100%
100%
18%

100%
99%
25%
100%
100%

Table 16: Top-5 accuracy of Table 15. Transferability between pairs of models using non-targeted
optimization-based approach with a learning rate of 0.125. The ﬁrst column indicates the average
RMSD of all adversarial images generated for the model in the corresponding row. The cell (i, j)
indicates the top-5 accuracy of the adversarial images generated for model i (row) evaluated over
model j (column). Lower value indicates better transferability.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
31.35
31.11
31.32
31.50
30.67

98%
5%
3%
1%
2%

ResNet-101
1%
98%
2%
1%
1%

ResNet-50 VGG-16 GoogLeNet
2%
2%
1%
97%
2%

2%
1%
99%
2%
0%

0%
0%
1%
1%
97%

Table 17: The adversarial images are generated using the targeted optimization-based approach
with a larger learning rate. The ﬁrst column indicates the average RMSD of all adversarial images
generated for the model in the corresponding row. Cell (i, j) indicates that top-1 matching rate of
the targeted adversarial images generated for model i (row) when evaluated on model j (column).
Higher value indicates more successful transferable target labels. We used a larger learning rate to
achieve larger RMSD. Results of top-5 matching rate can be found in Table 18.

19

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
31.35
31.11
31.32
31.50
30.67

98%
10%
6%
4%
3%

ResNet-101
8%
98%
7%
4%
1%

ResNet-50 VGG-16 GoogLeNet
3%
3%
5%
97%
3%

6%
5%
99%
6%
2%

1%
1%
1%
4%
97%

Table 18: The top-5 matching rate of Table 17. The adversarial images are generated using the
targeted optimization-based approach with a larger learning rate. The ﬁrst column indicates the
average RMSD of all adversarial images generated for the model in the corresponding row. Cell
(i, j) indicates that top-5 matching rate of the targeted adversarial images generated for model i
(row) when evaluated on model j (column). Higher value indicates more successful transferable
target labels. We used a larger learning rate to achieve larger RMSD.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.11
23.11
23.11
23.12
23.11

12%
29%
34%
25%
46%

ResNet-101
27%
13%
28%
20%
41%

ResNet-50 VGG-16 GoogLeNet
22%
29%
25%
0%
25%

15%
16%
23%
8%
2%

25%
29%
10%
23%
40%

Table 19: Transferability between pairs of models using non-targeted FGS. The ﬁrst column indi-
cates the average RMSD of all adversarial images generated for the model in the corresponding row.
The cell (i, j) indicates the top-1 accuracy of the adversarial images generated for model i (row)
evaluated over model j (column). Lower value indicates better transferability. Results of top-5
accuracy can be found in Table 20.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.11
23.11
23.11
23.12
23.11

32%
56%
59%
42%
71%

ResNet-101
55%
33%
53%
39%
74%

ResNet-50 VGG-16 GoogLeNet
47%
46%
47%
5%
53%

53%
50%
29%
41%
62%

36%
40%
38%
21%
11%

Table 20: Top-5 accuracy of Table 19. Transferability between pairs of models using non-targeted
FGS. The ﬁrst column indicates the average RMSD of all adversarial images generated for the
model in the corresponding row. The cell (i, j) indicates the top-5 accuracy of the adversarial
images generated for model i (row) evaluated over model j (column). Lower value indicates better
transferability.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.25
17.24
17.24
17.24
17.24

23%
15%
15%
17%
14%

ResNet-101
12%
19%
13%
15%
11%

ResNet-50 VGG-16 GoogLeNet
1%
2%
2%
23%
2%

7%
6%
8%
7%
19%

11%
11%
19%
12%
10%

Table 21: Transferability between pairs of models using non-targeted ensemble FG. The ﬁrst column
indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that top-1
accuracy of the non-targeted adversarial images generated using the ensemble of the four models
except model i (row) when evaluated over model j (column). In each row, the minus sign “−”
indicates that the model of the row is not used when generating the attacks. Lower value indicates
better transferability. Results of top-5 accuracy can be found in Table 22.

20

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.25
17.24
17.24
17.24
17.24

47%
37%
38%
37%
32%

ResNet-101
33%
42%
32%
36%
30%

ResNet-50 VGG-16 GoogLeNet
15%
14%
13%
44%
13%

28%
30%
39%
37%
28%

21%
26%
23%
32%
46%

Table 22: Top-5 accuracy of Table 21. Transferability between pairs of models using non-targeted
ensemble FG. The ﬁrst column indicates the average RMSD of the generated adversarial images.
Cell (i, j) indicates that top-5 accuracy of the non-targeted adversarial images generated using the
ensemble of the four models except model i (row) when evaluated over model j (column). In each
row, the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Lower value indicates better transferability.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.41
17.40
17.40
17.40
17.40

26%
18%
15%
17%
15%

ResNet-101
10%
21%
13%
16%
12%

ResNet-50 VGG-16 GoogLeNet
2%
2%
4%
23%
3%

11%
13%
20%
11%
10%

7%
4%
8%
7%
22%

Table 23: Transferability between pairs of models using non-targeted ensemble FGS. The ﬁrst col-
umn indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that top-1
accuracy of the non-targeted adversarial images generated using the ensemble of the four models
except model i (row) when evaluated over model j (column). In each row, the minus sign “−” indi-
cates that the model of the row is not used when generating the attacks. Lower value indicates better
transferability. Results of top-5 accuracy can be found in Table 24.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
17.41
17.40
17.40
17.40
17.40

50%
38%
38%
37%
35%

ResNet-101
35%
48%
36%
36%
29%

ResNet-50 VGG-16 GoogLeNet
20%
19%
18%
48%
19%

26%
26%
28%
33%
53%

30%
33%
41%
36%
31%

Table 24: Top-5 accuracy of Table 23. Transferability between pairs of models using non-targeted
ensemble FGS. The ﬁrst column indicates the average RMSD of the generated adversarial images.
Cell (i, j) indicates that top-5 accuracy of the non-targeted adversarial images generated using the
ensemble of the four models except model i (row) when evaluated over model j (column). In each
row, the minus sign “−” indicates that the model of the row is not used when generating the attacks.
Lower value indicates better transferability.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

RMSD ResNet-152
23.55
23.56
23.56
23.95
23.63

1%
1%
1%
1%
1%

ResNet-101
2%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
0%
0%
0%
1%
1%

0%
0%
1%
0%
0%

1%
1%
0%
1%
1%

Table 25: The adversarial images are generated using the targeted FG. The ﬁrst column indicates the
average RMSD of all adversarial images generated for the model in the corresponding row. The ﬁrst
column indicates the average RMSD of the generated adversarial images. Cell (i, j) indicates that
top-1 matching rate of the targeted adversarial images generated for model i (row) when evaluated
on model j (column). Higher value indicates more successful transferable target labels.

21

Published as a conference paper at ICLR 2017

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
33.33%
24.00%
17.50%
15.38%

ResNet-101
−
100.00%
35.00%
19.05%
14.81%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
21.18% 100.00%
15.05%
13.10%

GoogLeNet
−
−
−
−
100.00%

Table 26: When using non-targeted FG for VGG-16 model to generate adversarial images, cell (i, j)
is the percentage of all transferable adversarial images that are predicted as the same wrong labels
by both models i and j over all adversarial images that are misclassiﬁed by both models i and j.
Notice that the table is symmetric.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
100.00%
33.78%
36.62%
16.00%
14.86%

ResNet-101
−
100.00%
43.24%
20.00%
20.25%

VGG-16
−
−
−

ResNet-50
−
−
100.00%
23.38% 100.00%
19.57%
13.16%

GoogLeNet
−
−
−
−
100.00%

Table 27: When using non-targeted FGS for VGG-16 model to generate adversarial images, cell
(i, j) is the percentage of all transferable adversarial images that are predicted as the same wrong
labels by both models i and j over all adversarial images that are misclassiﬁed by both models i and
j. Notice that the table is symmetric.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
31.05
30.94
31.12
30.57
30.47

1%
1%
1%
1%
1%

ResNet-101
1%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
1%

0%
0%
0%
0%
0%

1%
0%
1%
1%
0%

Table 28: Transferability between pairs of models using targeted ensemble FG. The ﬁrst column
indicates the average RMSD of all adversarial images generated for the model in the corresponding
row. Cell (i, j) indicates that top-1 matching rate of the targeted adversarial images generated using
the ensemble of the four models except model i (row) is predicted as the target label by model j
(column). In each row, the minus sign “−” indicates that the model of the row is not used when
generating the attacks. Higher value indicates more successful transferable target labels. Results of
top-5 matching rate can be found in Table 29.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
31.05
30.94
31.12
30.57
30.47

1%
1%
1%
2%
1%

ResNet-101
2%
1%
2%
2%
1%

ResNet-50 VGG-16 GoogLeNet
3%
1%
3%
1%
2%

2%
2%
2%
1%
2%

1%
1%
1%
2%
1%

Table 29: Top-5 matching rate of Table 28. Transferability between pairs of models using targeted
ensemble FG. The ﬁrst column indicates the average RMSD of all adversarial images generated
for the model in the corresponding row. Cell (i, j) indicates that top-5 matching rate of the targeted
adversarial images generated using the ensemble of the four models except model i (row) is predicted
as the target label by model j (column). In each row, the minus sign “−” indicates that the model of
the row is not used when generating the attacks. Higher value indicates more successful transferable
target labels.

22

Published as a conference paper at ICLR 2017

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.42
30.42
30.42
30.41
30.42

1%
1%
1%
1%
1%

ResNet-101
1%
1%
1%
1%
1%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
1%

1%
1%
0%
0%
1%

1%
1%
1%
1%
1%

Table 30: Transferability between pairs of models using targeted ensemble FGS. The ﬁrst column
indicates the average RMSD of all adversarial images generated for the model in the corresponding
row. Cell (i, j) indicates that top-1 matching rate of the targeted adversarial images generated using
the ensemble of the four models except model i (row) is predicted as the target label by model j
(column). In each row, the minus sign “−” indicates that the model of the row is not used when
generating the attacks. Higher value indicates more successful transferable target labels. Results are
top-5 matching rate can be found in Table 31.

-ResNet-152
-ResNet-101
-ResNet-50
-VGG-16
-GoogLeNet

RMSD ResNet-152
30.42
30.42
30.42
30.41
30.42

2%
2%
2%
3%
2%

ResNet-101
1%
1%
1%
2%
2%

ResNet-50 VGG-16 GoogLeNet
1%
1%
1%
1%
2%

2%
1%
1%
2%
2%

2%
2%
2%
1%
2%

Table 31: Top-5 matching rate of Table 30. Transferability between pairs of models using targeted
ensemble FGS. The ﬁrst column indicates the average RMSD of all adversarial images generated
for the model in the corresponding row. Cell (i, j) indicates that top-5 matching rate of the targeted
adversarial images generated using the ensemble of the four models except model i (row) is predicted
as the target label by model j (column). In each row, the minus sign “−” indicates that the model of
the row is not used when generating the attacks. Higher value indicates more successful transferable
target labels.

ResNet-152
jigsaw puzzle(8%)
acorn(3%)
lycaenid(2%)
ram(2%)
maze(2%)

ResNet-101
jigsaw puzzle(12%)
starﬁsh(3%)
strawberry(2%)
wild boar(2%)
dishrag(2%)

ResNet-50
jigsaw puzzle(12%)

VGG-16
jigsaw puzzle(15%)

African chameleon(2%) African chameleon(7%)

strawberry(2%)
starﬁsh(2%)
greenhouse(2%)

prayer rug(5%)
apron(4%)
sarong(3%)

GoogleNet
prayer rug(12%)
jigsaw puzzle(7%)
stole(6%)
African chameleon(4%)
mitten(3%)

Table 32: When using non-targeted optimization-based approach for VGG-16 model to generate
adversarial images, column i indicates the top 5 common incorrect labels predicted by model i. The
value in the parentheses is the percentage of the predicted label.

ResNet-152
ResNet-101
ResNet-50
VGG-16
GoogLeNet

ResNet-152
1.00
0.04
0.03
0.02
0.01

ResNet-101
−
1.00
0.03
0.02
0.01

ResNet-50 VGG-16 GoogLeNet
−
−
−
1.00
0.02

−
−
1.00
0.02
0.01

−
−
−
−
1.00

Table 33: Average cosine value of the angle between gradient directions of two models. Notice
that the dot-product of two normalized vectors is the cosine value of the angle between them, for
each image, we compute the dot-product of normalized gradient directions with respect to model i
(row) and model j (column), and the value in cell (i, j) is the average over dot-product values of all
images. Notice that this table is symmetric.

23

Published as a conference paper at ICLR 2017

(a) GoogLeNet

(b) VGG-16

(c) ResNet-152

(d) ResNet-101

(e) ResNet-50

Figure 7: Linearity of different models. Each line plots the classiﬁcation layer’s softmax input vs
distortion B when fast gradient noises are used. Solid red line is the ground truth label, and other
lines are the top 10 labels predicted on the original image.

24

