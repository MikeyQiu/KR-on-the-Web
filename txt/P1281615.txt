Rumor Detection on Twitter with Tree-structured Recursive Neural
Networks

Jing Ma1, Wei Gao2, Kam-Fai Wong1,3
1The Chinese University of Hong Kong, Hong Kong SAR
2Victoria University of Wellington, New Zealand
3MoE Key Laboratory of High Conﬁdence Software Technologies, China
1{majing,kfwong}@se.cuhk.edu.hk, 2wei.gao@vuw.ac.nz

Abstract

Automatic rumor detection is technically
very challenging. In this work, we try to
learn discriminative features from tweets
content by following their non-sequential
propagation structure and generate more
powerful representations for identifying
different type of rumors. We propose
two recursive neural models based on a
bottom-up and a top-down tree-structured
neural networks for rumor representation
learning and classiﬁcation, which natu-
rally conform to the propagation layout
of tweets. Results on two public Twit-
ter datasets demonstrate that our recursive
neural models 1) achieve much better per-
formance than state-of-the-art approaches;
2) demonstrate superior capacity on de-
tecting rumors at very early stage.

1

Introduction

Rumors have always been a social disease. In re-
cent years, it has become unprecedentedly conve-
nient for the “evil-doers” to create and disseminate
rumors in massive scale with low cost thanks to
the popularity of social media outlets on Twitter,
Facebook, etc. The worst effect of false rumors
could be devastating to individual and/or society.
Research pertaining rumors spans multiple dis-
ciplines, such as philosophy and humanities (Di-
Fonzo and Bordia, 2007; Donovan, 2007), social
psychology (Allport and Postman, 1965; Jaeger
et al., 1980; Rosnow and Foster, 2005), politi-
cal studies (Allport and Postman, 1946; Berin-
sky, 2017), management science (DiFonzo et al.,
1994; Kimmel, 2004) and recently computer sci-
ence and artiﬁcial intelligence (Qazvinian et al.,
2011; Ratkiewicz et al., 2011; Castillo et al., 2011;
Hannak et al., 2014; Zhao et al., 2015; Ma et al.,

2015). Rumor is commonly deﬁned as informa-
tion that emerge and spread among people whose
truth value is unveriﬁed or intentionally false (Di-
Fonzo and Bordia, 2007; Qazvinian et al., 2011).
Analysis shows that people tend to stop spread-
ing a rumor if it is known as false (Zubiaga et al.,
2016b). However, identifying such misinforma-
tion is non-trivial and needs investigative jour-
nalism to fact check the suspected claim, which
is labor-intensive and time-consuming. The pro-
liferation of social media makes it worse due to
the ever-increasing information load and dynam-
ics. Therefore, it is necessary to develop automatic
and assistant approaches to facilitate real-time ru-
mor tracking and debunking.

For automating rumor detection, most of the
previous studies focused on text mining from se-
quential microblog streams using supervised mod-
els based on feature engineering (Castillo et al.,
2011; Kwon et al., 2013; Liu et al., 2015; Ma
et al., 2015), and more recently deep neural mod-
els (Ma et al., 2016; Chen et al., 2017; Ruchan-
sky et al., 2017). These methods largely ignore
or oversimplify the structural information asso-
ciated with message propagation which however
has been shown conducive to provide useful clues
for identifying rumors. Kernel-based method (Wu
et al., 2015; Ma et al., 2017) was thus proposed
to model the structure as propagation trees in or-
der to differentiate rumorous and non-rumorous
claims by comparing their tree-based similarities.
But such kind of approach cannot directly classify
a tree without pairwise comparison with all other
trees imposing unnecessary overhead, and it also
cannot automatically learn any high-level feature
representations out of the noisy surface features.

In this paper, we present a neural rumor de-
tection approach based on recursive neural net-
works (RvNN) to bridge the content semantics
and propagation clues. RvNN and its variants

were originally used to compose phrase or sen-
tence representation for syntactic and semantic
parsing (Socher et al., 2011, 2012). Unlike pars-
ing, the input into our model is a propagation tree
rooted from a source post rather than the parse tree
of an individual sentence, and each tree node is
a responsive post instead of an individual words.
The content semantics of posts and the responsive
relationship among them can be jointly captured
via the recursive feature learning process along the
tree structure.

So, why can such neural model do better for
the task? Analysis has generally found that Twit-
ter could “self-correct” some inaccurate informa-
tion as users share opinions, conjectures and evi-
dences (Zubiaga et al., 2017). To illustrate our in-
tuition, Figure 1 exempliﬁes the propagation trees
of two rumors in our dataset, one being false and
the other being true1. Structure-insensitive meth-
ods basically relying on the relative ratio of differ-
ent stances in the text cannot do well when such
clue is unclear like this example. However, it can
be seen that when a post denies the false rumor,
it tends to spark supportive or afﬁrmative replies
conﬁrming the denial; in contrast, denial to a true
rumor tends to trigger question or denial in its
replies. This observation may suggest a more gen-
eral hypothesis that the repliers tend to disagree
with (or question) who support a false rumor or
deny a true rumor, and also they tend to agree with
who deny a false rumor or support a true rumor.
Meanwhile, a reply, rather than directly respond-
ing to the source tweet (i.e., the root), is usually re-
sponsive to its immediate ancestor (Lukasik et al.,
2016; Zubiaga et al., 2016a), suggesting obvious
local characteristic of the interaction. The recur-
sive network naturally models such structures for
learning to capture the rumor indicative signals
and enhance the representation by recursively ag-
gregating the signals from different branches.

To this end, we extend the standard RvNN into
two variants, i.e., a bottom-up (BU) model and a
top-down (TD) model, which represent the propa-
gation tree structure from different angles, in order
to visit the nodes and combine their representa-
tions following distinct directions. The important
merit of such architecture is that the node features
can be selectively reﬁned by the recursion given
the connection and direction of all paths of the

(a) False rumor

(b) True rumor

Figure 1: Propagation trees of two rumorous
source tweets. Nodes may express stances on their
parent as commenting, supporting, questioning or
denying. The edge arrow indicates the direction
from a response to its responded node, and the po-
larity is marked as ‘+’ (‘-’) for support (denial).
The same node color indicates the same stance on
the veracity of root node (i.e., source tweet).

tree. As a result, it can be expected that the dis-
criminative signals are better embedded into the
learned representations.

We evaluate our proposed approach based on
two public Twitter datasets. The results show that
our method outperforms strong rumor detection
baselines with large margin and also demonstrate
much higher effectiveness for detection at early
stage of propagation, which is promising for real-
time intervention and debunking. Our contribu-
tions are summarized as follows in three folds:

• This is the ﬁrst study that deeply integrates
both structure and content semantics based
on tree-structured recursive neural networks
for detecting rumors from microblog posts.

• We propose two variants of RvNN models
based on bottom-up and top-down tree struc-
tures to generate better integrated representa-
tions for a claim by capturing both structural
and textural properties signaling rumors.

• Our experiments based on real-world Twitter
datasets achieve superior improvements over
state-of-the-art baselines on both rumor clas-
siﬁcation and early detection tasks. We make
the source codes in our experiments publicly
accessible 2.

2 Related Work

Most previous automatic approaches for rumor de-
tection (Castillo et al., 2011; Yang et al., 2012; Liu

1False (true) rumor means the veracity of the rumorous

2https://github.com/majingCUHK/Rumor_

claim is false (true).

RvNN

et al., 2015) intended to learn a supervised classi-
ﬁer by utilizing a wide range of features crafted
from post contents, user proﬁles and propagation
patterns. Subsequent studies were then conducted
to engineer new features such as those represent-
ing rumor diffusion and cascades (Friggeri et al.,
2014; Hannak et al., 2014) characterized by com-
ments with links to debunking websites. Kwon
et al. (2013) introduced a time-series-ﬁtting model
based on the volume of tweets over time. Ma et al.
(2015) extended their model with more chronolog-
ical social context features. These approaches typ-
ically require heavy preprocessing and feature en-
gineering.

Zhao et al. (2015) alleviated the engineering ef-
fort by using a set of regular expressions (such
as “really?”, “not true”, etc) to ﬁnd questing and
denying tweets, but the approach was oversimpli-
ﬁed and suffered from very low recall. Ma et al.
(2016) used recurrent neural networks (RNN)
to learn automatically the representations from
tweets content based on time series. Recently, they
studied to mutually reinforce stance detection and
rumor classiﬁcation in a neural multi-task learn-
ing framework (Ma et al., 2018). However, the
approaches cannot embed features reﬂecting how
the posts are propagated and requires careful data
segmentation to prepare for time sequence.

Some kernel-based methods were exploited to
model the propagation structure. Wu et al. (2015)
proposed a hybrid SVM classiﬁer which combines
a RBF kernel and a random-walk-based graph ker-
nel to capture both ﬂat and propagation patterns
for detecting rumors on Sina Weibo. Ma et al.
(2017) used tree kernel to capture the similarity
of propagation trees by counting their similar sub-
structures in order to identify different types of ru-
mors on Twitter. Compared to their studies, our
model can learn the useful features via a more nat-
ural and general approach, i.e., the tree-structured
neural network, to jointly generate representations
from both structure and content.

RvNN has demonstrated state-of-the-art perfor-
mances in a variety of tasks, e.g., images seg-
mentation (Socher et al., 2011), phrase represen-
tation from word vectors (Socher et al., 2012),
and sentiment classiﬁcation in sentences (Socher
et al., 2013). More recently, a deep RvNN was
proposed to model the compositionality in natu-
ral language for ﬁne-grained sentiment classiﬁca-
tion by stacking multiple recursive layers (Irsoy

and Cardie, 2014). In order to avoid gradient van-
ishing, some studies integrated Long Short Term
Memory (LSTM) (Hochreiter and Schmidhuber,
1997) to RvNN (Zhu et al., 2015; Tai et al., 2015).
Mou et al. (2015) used a convolutional network
over tree structures for syntactic tree parsing of
natural language sentences.

3 Problem Statement

We deﬁne a Twitter rumor detection dataset as
a set of claims C = {C1, C2, · · · , C|C|}, where
each claim Ci corresponds to a source tweet ri
which consists of ideally all its relevant respon-
i.e., Ci =
sive tweets in chronological order,
{ri, xi1, xi2, · · · , xim} where each xi∗ is a respon-
sive tweet of the root ri. Note that although the
tweets are notated sequentially, there are connec-
tions among them based on their reply or repost
relationships, which can form a propagation tree
structure (Wu et al., 2015; Ma et al., 2017) with ri
being the root node.

We formulate this task as a supervised classiﬁ-
cation problem, which learns a classiﬁer f from
labeled claims, that is f : Ci → Yi, where Yi takes
one of the four ﬁner-grained classes: non-rumor,
false rumor, true rumor, and unveriﬁed rumor that
are introduced in the literature (Ma et al., 2017;
Zubiaga et al., 2016b).

An important issue of the tree structure is con-
cerned about the direction of edges, which can re-
sult in two different architectures of the model: 1)
a bottom-up tree; 2) a top-down tree, which are
deﬁned as follows:

• Bottom-up tree takes the similar shape as
shown in Figure 1, where responsive nodes
always point to their responded nodes and
leaf nodes not having any response are laid
out at the furthest level. We represent a tree
as Ti = (cid:104)Vi, Ei(cid:105), where Vi = Ci which con-
sists of all relevant posts as nodes, and Ei de-
notes a set of all directed links, where for any
u, v ∈ Vi, u ← v exists if v responses to u.
This structure is similar to a citation network
where a response mimics a reference.

• Top-down tree naturally conforms to the di-
rection of information propagation, in which
a link u → v means the information ﬂows
from u to v and v sees it and provides a re-
sponse to u. This structure reverses bottom-
up tree and simulates how information cas-

function with W and b as parameters. This compu-
tation is done recursively over all tree nodes; the
learned hidden vectors of the nodes can then be
used for various classiﬁcation tasks.

4.2 Bottom-up RvNN

The core idea of bottom-up model is to generate a
feature vector for each subtree by recursively visit-
ing every node from the leaves at the bottom to the
root at the top. In this way, the subtrees with sim-
ilar contexts, such as those subtrees having a de-
nial parent and a set of supportive children, will be
projected into the proximity in the representation
space. And thus such local rumor indicative fea-
tures are aggregated along different branches into
some global representation of the whole tree.

For this purpose, we make a natural extension
to the original RvNN. The overall structure of our
proposed bottom-up model is illustrated in Fig-
ure 3(b), taking a bottom-up tree (see Figure 3(a))
as input. Different from the standard RvNN, the
input of each node in the bottom-up model is a
post represented as a vector of words in the vocab-
ulary in terms of tf idf values. Here, every node
has an input vector, and the number of children of
nodes varies signiﬁcantly3.

In rumor detection, long short-term memory
(LSTM) (Hochreiter and Schmidhuber, 1997) and
gated recurrent units (GRU) (Cho et al., 2014)
were used to learn textual representation, which
adopts memory units to store information over
long time steps (Ma et al., 2016). In this paper,
we choose to extend GRU as hidden unit to model
long-distance interactions over the tree nodes be-
cause it is more efﬁcient due to fewer parameters.
Let S(j) denote the set of direct children of the
node j. The transition equations of node j in the
bottom-up model are formulated as follows:

˜xj = xjE
(cid:88)

hS =

hs

s∈S(j)

rj = σ (Wr ˜xj + UrhS)
zj = σ (Wz ˜xj + UzhS)
˜hj = tanh (Wh ˜xj + Uh(hS (cid:12) rj))
hj = (1 − zj) (cid:12) hS + zj (cid:12) ˜hj

(1)

3In standard RvNN, since an input instance is the parse
tree of a sentence, only leaf nodes have input vector, each
node representing a word of the input sentence, and the non-
leaf nodes are constituents of the sentence, and thus the num-
ber of children of a node is limited.

Figure 2: A binarized sentence parse tree (left) and
its corresponding RvNN architecture (right).

cades from a source tweet, i.e., the root, to
all its receivers, i.e., the decedents, which is
similar as (Wu et al., 2015; Ma et al., 2017).

4 RvNN-based Rumor Detection

The core idea of our method is to strengthen the
high-level representation of tree nodes by the re-
cursion following the propagation structure over
different branches in the tree. For instance, the re-
sponsive nodes conﬁrming or supporting a node
(e.g., “I agree”, “be right”, etc) can further rein-
force the stance of that node while denial or ques-
tioning responses (e.g., “disagree, “really?!) oth-
erwise weaken its stance. Compared to the kernel-
based method using propagation tree (Wu et al.,
2015; Ma et al., 2017), our method does not need
pairwise comparison among large number of sub-
trees, and can learn much stronger representation
of content following the response structure.

In this section, we will describe our extension
to the standard RvNN for modeling rumor detec-
tion based on the bottom-up and top-down archi-
tectures presented in Section 3.

4.1 Standard Recursive Neural Networks

RvNN is a type of tree-structured neural networks.
The original version of RvNN utilized binarized
sentence parse trees (Socher et al., 2012), in which
the representation associated with each node of
a parse tree is computed from its direct children.
The overall structure of the standard RvNN is il-
lustrated as the right side of Figure 2, correspond-
ing to the input parse tree at the left side.

Leaf nodes are the words in an input sen-
tence, each represented by a low-dimensional
word embedding. Non-leaf nodes are sentence
constituents, computed by recursion based on the
presentations of child nodes. Let p be the feature
vector of a parent node whose children are c1 and
c2, the representation of the parent is computed by
p = f (W ·[c1; c2]+b), where f (·) is the activation

(a) Bottom-up/Top-down tree

(b) Bottom-up RvNN model

(c) Top-down RvNN model

Figure 3: A bottom-up/top-down propagation tree and the corresponding RvNN-based models. The
black-color and red-color edges differentiate the bottom-up and top-down tree in Figure 3(a).

where xj is the original input vector of node j,
E denotes the parameter matrix for transforming
this input post, ˜xj is the transformed representa-
tion of j, [W∗, U∗] are the weight connections in-
side GRU, and hj and hs refer to the hidden state
of j and its s-th child. Thus hS denotes the sum
of the hidden state of all the children of j assum-
ing that all children are equally important to j. As
with the standard GRU, (cid:12) denotes element-wise
multiplication; a reset gate rj determines how to
combine the current input ˜xj with the memory of
children, and an update gate zj deﬁnes how much
memory from the children is cascaded into the cur-
rent node; and ˜hj denotes the candidate activation
of the hidden state of the current node. Different
from the standard GRU unit, the gating vectors in
our variant of GRU are dependent on the states of
many child units, allowing our model to incorpo-
rate representations from different children.

After recursive aggregation from bottom to up,
the state of root node (i.e., source tweet) can be re-
gard as the representation of the whole tree which
is used for supervised classiﬁcation. So, an output
layer is connected to the root node for predicting
the class of the tree using a softmax function:

ˆy = Sof tmax(Vh0 + b)

(2)

where h0 is the learned hidden vector of root node;
V and b are the weights and bias in output layer.

4.3 Top-down RvNN

This model is designed to leverage the structure
of top-down tree to capture complex propagation
patterns for classifying rumorous claims, which is
shown in Figure 3(c). It models how the informa-

tion ﬂows from source post to the current node.
The idea of this top-down approach is to generate
a strengthened feature vector for each post consid-
ering its propagation path, where rumor-indicative
features are aggregated along the propagation his-
tory in the path. For example, if current post agree
with its parent’s stance which denies the source
post, the denial stance from the root node down to
the current node on this path should be reinforced.
Due to different branches of any non-leaf node, the
top-down visit to its subtree nodes is also recur-
sive. However, the nature of top-down tree lends
this model different from the bottom-up one. The
representation of each node is computed by com-
bining its own input and its parent node instead of
its children nodes. This process proceeds recur-
sively from the root node to its children until all
leaf nodes are reached.

Suppose that the hidden state of a non-leaf node
can be passed synchronously to all its child nodes
without loss. Then the hidden state hj of a node
j can be computed by combining the hidden state
hP(j) of its parent node P(j) and its own input
vector xj. Therefore, the transition equations of
node j can be formulated as a standard GRU:

˜xj = xjE
rj = σ (cid:0)Wr ˜xj + UrhP(j)
(cid:1)
zj = σ (cid:0)Wz ˜xj + UzhP(j)
˜hj = tanh (cid:0)Wh ˜xj + Uh(hP(j) (cid:12) rj)(cid:1)
hj = (1 − zj) (cid:12) hP(j) + zj (cid:12) ˜hj

(cid:1)

(3)

Through the top-down recursion, the learned
representations are eventually embedded into the
hidden vector of all the leaf nodes. Since the num-

ber of leaf nodes varies, the resulting vectors can-
not be directly fed into a ﬁxed-size neural layer
for output. Therefore, we add a max-pooling layer
to take the maximum value of each dimension of
the vectors over all the leaf nodes. This can also
help capture the most appealing indicative features
from all the propagation paths.

Based on the pooling result, we ﬁnally use a
softmax function in the output layer to predict the
label of the tree:

ˆy = Sof tmax(Vh∞ + b)

(4)

where h∞ is the pooling vector over all leaf nodes,
V and b are parameters in the output layer.

Although both of the two RvNN models aim
to capture the structural properties by recursively
visiting all nodes, we can conjecture that the top-
down model would be better. The hypothesis is
that in the bottom-up case the ﬁnal output relies on
the representation of single root, and its informa-
tion loss can be larger than the top-down one since
in the top-down case the representations embed-
ded into all leaf nodes along different propagation
paths can be incorporated via pooling holistically.

4.4 Model Training

The model is trained to minimize the squared error
between the probability distributions of the predic-
tions and the ground truth:

L(y, ˆy) =

(yc − ˆyc)2 + λ||θ||2
2

(5)

N
(cid:88)

C
(cid:88)

n=1

c=1

where yc is the ground truth and ˆyc is the pre-
diction probability of a class, N is the number of
training claims, C is the number of classes, ||.||2 is
the L2 regularization term over all model parame-
ters θ, and λ is the trade-off coefﬁcient.

During training, all the model parameters are
updated using efﬁcient back-propagation through
structure (Goller and Kuchler, 1996; Socher et al.,
2013), and the optimization is gradient-based fol-
lowing the Ada-grad update rule (Duchi et al.,
2011) to speed up the convergence. We empiri-
cally initialize the model parameters with uniform
distribution and set the vocabulary size as 5,000,
the size of embedding and hidden units as 100. We
iterate over all the training examples in each epoch
and continue until the loss value converges or the
maximum epoch number is met.

5 Experiments and Results

5.1 Datasets

For experimental evaluation, we use two publicly
available Twitter datasets released by Ma et al.
(2017), namely Twitter15 and Twitter164, which
respectively contains 1,381 and 1,181 propagation
trees (see (Ma et al., 2017) for detailed statistics).
In each dataset, a group of wide spread source
tweets along with their propagation threads, i.e.,
replies and retweets, are provided in the form of
tree structure. Each tree is annotated with one
of the four class labels, i.e., non-rumor, false ru-
mor, true rumor and unveriﬁed rumor. We remove
the retweets from the trees since they do not pro-
vide any extra information or evidence content-
wise. We build two versions for each tree, one for
the bottom-up tree and the other for the top-down
tree, by ﬂipping the edges’ direction.

5.2 Experimental Setup

We make comprehensive comparisons between
our models and some state-of-the-art baselines on
rumor classiﬁcation and early detection tasks.

- DTR: Zhao et al. (2015) proposed a Decision-
Tree-based Ranking model to identify trending ru-
mors by searching for inquiry phrases.

- DTC: The information credibility model using
a Decision-Tree Classiﬁer (Castillo et al., 2011)
based on manually engineering various statistical
features of the tweets.

- RFC: The Random Forest Classier using 3 ﬁt-
ting parameters as temporal properties and a set of
handcrafted features on user, linguistic and struc-
tural properties (Kwon et al., 2013).

- SVM-TS: A linear SVM classiﬁer that uses
time-series to model the variation of handcrafted
social context features (Ma et al., 2015).

- SVM-BOW: A naive baseline we built by rep-
resenting text content using bag-of-words and us-
ing linear SVM for rumor classiﬁcation.

- SVM-TK and SVM-HK: SVM classiﬁer uses
a Tree Kernel (Ma et al., 2017) and that uses a Hy-
brid Kernel (Wu et al., 2015), respectively, both of
which model propagation structures with kernels.
- GRU-RNN: A detection model based on re-
current neural networks (Ma et al., 2016) with
GRU units for learning rumor representations by
modeling sequential structure of relevant posts.

4https://www.dropbox.com/s/

7ewzdrbelpmrnxu/rumdetect2017.zip?dl=0

(a) Twitter15 dataset

Method

Acc.
0.409
DTR
0.454
DTC
RFC
0.565
0.544
SVM-TS
SVM-BOW 0.548
0.493
SVM-HK
0.667
SVM-TK
0.641
GRU-RNN
0.708
BU-RvNN
0.723
TD-RvNN

Method

Acc.
0.414
DTR
0.465
DTC
0.585
RFC
SVM-TS
0.574
SVM-BOW 0.585
0.511
SVM-HK
0.662
SVM-TK
GRU-RNN
0.633
0.718
BU-RvNN
0.737
TD-RvNN

NR
F1
0.501
0.733
0.810
0.796
0.564
0.650
0.619
0.684
0.695
0.682

NR
F1
0.394
0.643
0.752
0.755
0.553
0.648
0.643
0.617
0.723
0.662

FR
F1
0.311
0.355
0.422
0.472
0.524
0.439
0.669
0.634
0.728
0.758

FR
F1
0.273
0.393
0.415
0.420
0.556
0.434
0.623
0.715
0.712
0.743

TR
F1
0.364
0.317
0.401
0.404
0.582
0.342
0.772
0.688
0.759
0.821

TR
F1
0.630
0.419
0.547
0.571
0.655
0.473
0.783
0.577
0.779
0.835

UR
F1
0.473
0.415
0.543
0.483
0.512
0.336
0.645
0.571
0.653
0.654

UR
F1
0.344
0.403
0.563
0.526
0.578
0.451
0.655
0.527
0.659
0.708

(b) Twitter16 dataset

Table 1: Results of rumor detection.
(NR: non-
rumor; FR: false rumor; TR: true rumor; UR: un-
veriﬁed rumor)

- BU-RvNN and TD-RvNN: Our bottom-up

and top-down RvNN models, respectively.

We implement DTC and RFC using Weka5,
SVM-based models using LibSVM6 and all
neural-network-based models with Theano7. We
conduct 5-fold cross-validation on the datasets and
use accuracy over all the four categories and F1
measure on each class to evaluate the performance
of models.

5.3 Rumor Classiﬁcation Performance

As shown in Table 1, our proposed models ba-
sically yield much better performance than other
methods on both datasets via the modeling of in-
teraction structures of posts in the propagation.

It is observed that the performance of the 4
baselines in the ﬁrst group based on handcrafted
features is obviously poor, varying between 0.409
and 0.585 in accuracy, indicating that they fail to
generalize due to the lack of capacity capturing
helpful features. Among these baselines, SVM-
TS and RFC perform relatively better because they

5www.cs.waikato.ac.nz/ml/weka
6www.csie.ntu.edu.tw/˜cjlin/libsvm
7deeplearning.net/software/theano

use additional temporal traits, but they are still
clearly worse than the models not relying on fea-
ture engineering. DTR uses a set of regular ex-
pressions indicative of stances. However, only
19.6% and 22.2% tweets in the two datasets con-
tain strings covered by these regular expressions,
rendering unsatisfactory result.

Among the two kernel methods that are based
on comparing propagation structures, we observe
that SVM-TK is much more effective than SVM-
HK. There are two reasons: 1) SVM-HK was
originally proposed and experimented on Sina
Weibo (Wu et al., 2015), which may not be gener-
alize well on Twitter. 2) SVM-HK loosely couples
two separate kernels: a RBF kernel based on hand-
crafted features, plus a random walk-based ker-
nel which relies on a set of pre-deﬁned keywords
for jumping over the nodes probabilistically. This
under utilizes the propagation information due to
such oversimpliﬁed treatment of tree structure. In
contrast, SVM-TK is an integrated kernel and can
fully utilize the structure by comparing the trees
based on both textual and structural similarities.

It appears that using bag-of-words is already a
decent model evidenced as the fairly good perfor-
mance of SVM-BOW which is even better than
SVM-HK. This is because the features of SVM-
HK are handcrafted for binary classiﬁcation (i.e.,
non-rumor vs rumor), ignoring the importance of
indicative words or units that beneﬁt ﬁner-grained
classiﬁcation which can be captured more effec-
tively by SVM-BOW.

The sequential neural model GRU-RNN per-
forms slightly worse than SVM-TK, but much
worse than our recursive models. This is because
it is a special case of the recursive model where
each non-leaf node has only one child. It has to
rely on a linear chain as input, which missed out
valuable structural information. However, it does
learn high-level features from the post content via
hidden units of the neural model while SVM-TK
cannot which can only evaluates similarities based
on the overlapping words among subtrees. Our re-
cursive models are inherently tree-structured and
take advantages of representation learning follow-
ing the propagation structure, thus beats SVM-TK.

In the two recursive models, TD-RvNN outper-
forms BU-RvNN, which indicates that the bottom-
up model may suffer from larger information loss
than the top-down one. This veriﬁes the hypothe-
sis we made in Section 4.3 that the pooling layer

(a) Twitter15 (elapsed time)

(b) Twitter16 (elapsed time)

(c) Twitter15 (tweets count)

(d) Twitter16 (tweets count)

Figure 4: Early rumor detection accuracy at different checkpoints in terms of elapsed time (tweets count).

Figure 5: A correctly detected false rumor at early stage by both of our models, where propagation paths
are marked with relevant stances. Note that edge direction is not shown as it applies to either case.

in the top-down model can effectively select im-
portant features embedded into the leaf nodes.

For only the non-rumor class, it seems that our
method does not perform so well as some feature-
engineering baselines. This can be explained by
the fact that these baselines are trained with ad-
ditional features such as user information (e.g.,
proﬁle, veriﬁcation status, etc) which may contain
clues for differentiating non-rumors from rumors.
Also,
the responses to non-rumors are usually
much more diverse with little informative indi-
cation, making identiﬁcation of non-rumors more
difﬁcult based on content even with the structure.

5.4 Early Rumor Detection Performance

Detecting rumors at early state of propagation is
important so that interventions can be made in a
timely manner. We compared different methods
in term of different time delays measured by ei-
ther tweet count received or time elapsed since the
source tweet is posted. The performance is evalu-
ated by the accuracy obtained when we incremen-
tally add test data up to the check point given the
targeted time delay or tweets volume.

Figure 4 shows that the performance of our re-
cursive models climbs more rapidly and starts to
supersede the other models at the early stage. Al-
though all the methods are getting to their best per-

formance in the end, TD-RvNN and BU-RvNN
only need around 8 hours or about 90 tweets to
achieve the comparable performance of the best
baseline model, i.e., SVM-TK, which needs about
36 hours or around 300 posts, indicating superior
early detection performance of our method.

Figure 5 shows a sample tree at the early stage
of propagation that has been correctly classiﬁed as
a false rumor by both recursive models. We can
see that this false rumor demonstrates typical pat-
terns in subtrees and propagation paths indicative
of the falsehood, where a set of responses sup-
porting the parent posts that deny or question the
source post are captured by our bottom-up model.
Similarly, some patterns of propagation from the
root to leaf nodes like “support→deny→support”
are also seized by our top-down model. In com-
parison, sequential models may be confused be-
cause the supportive key terms such as “be right”,
“yeah”, “exactly!” dominate the responses, and
the SVM-TK may miss similar subtrees by just
comparing the surface words.

6 Conclusions and Future Work

We propose a bottom-up and a top-down tree-
structured model based on recursive neural net-
works for rumor detection on Twitter. The inher-

ent nature of recursive models allows them using
propagation tree to guide the learning of represen-
tations from tweets content, such as embedding
various indicative signals hidden in the structure,
for better identifying rumors. Results on two pub-
lic Twitter datasets show that our method improves
rumor detection performance in very large mar-
gins as compared to state-of-the-art baselines.

In our future work, we plan to integrate other
types of information such as user properties into
the structured neural models to further enhance
representation learning and detect rumor spread-
ers at the same time. We also plan to use unsuper-
vised models for the task by exploiting structural
information.

Acknowledgment

This work is partly supported by Innovation and
Technology Fund (ITF) Project No.
6904333,
and General Research Fund (GRF) Project No.
14232816 (12183516). We would like to thank
anonymous reviewers for the insightful comments.

References

Gordon W Allport and Leo Postman. 1946. An analy-
sis of rumor. Public Opinion Quarterly 10(4):501–
517.

G.W. Allport and L.J. Postman. 1965. The psychology

of rumor. Russell & Russell.

Adam J. Berinsky. 2017. Rumors and health care
reform: Experiments in political misinformation.
British Journal of Political Science 47(2):241262.

Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter. In
Proceedings of WWW. pages 675–684.

Tong Chen, Lin Wu, Xue Li, Jun Zhang, Hongzhi
Yin, and Yang Wang. 2017. Call attention to ru-
mors: Deep attention based recurrent neural net-
arXiv preprint
works for early rumor detection.
arXiv:1704.05973 .

Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .

Nicholas DiFonzo and Prashant Bordia. 2007. Rumor,
gossip and urban legends. Diogenes 54(1):19–35.

Nicholas DiFonzo, Prashant Bordia, and Ralph L Ros-
now. 1994. Reining in rumors. Organizational Dy-
namics 23(1):47–62.

Pamela Donovan. 2007. How idle is idle talk? one hun-
dred years of rumor research. Diogenes 54(1):59–
82.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
Journal of Machine
and stochastic optimization.
Learning Research 12(Jul):2121–2159.

Adrien Friggeri, Lada A Adamic, Dean Eckles, and
Justin Cheng. 2014. Rumor cascades. In Proceed-
ings of ICWSM.

Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Neural Networks,
1996., IEEE International Conference on. IEEE,
volume 1, pages 347–352.

Aniko Hannak, Drew Margolin, Brian Keegan, and In-
gmar Weber. 2014. Get back! you don’t know me
like that: The social mediation of fact checking in-
terventions in twitter conversations. In Proceedings
of ICWSM.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Proceedings of the 27th International Conference
on Neural Information Processing Systems - Volume
2. NIPS’14, pages 2096–2104.

Marianne E Jaeger, Susan Anthony, and Ralph L Ros-
now. 1980. Who hears what from whom and with
what effect: A study of rumor. Personality and So-
cial Psychology Bulletin 6(3):473–478.

Allan J Kimmel. 2004. Rumors and rumor control: A
manager’s guide to understanding and combatting
rumors. Routledge.

Sejeong Kwon, Meeyoung Cha, Kyomin Jung, Wei
Chen, and Yajun Wang. 2013. Prominent features of
rumor propagation in online social media. In Pro-
ceedings of ICDM. pages 1103–1108.

Xiaomo Liu, Armineh Nourbakhsh, Quanzhi Li, Rui
Fang, and Sameena Shah. 2015. Real-time ru-
In Proceedings of the
mor debunking on twitter.
24th ACM International on Conference on Informa-
tion and Knowledge Management. CIKM ’15, pages
1867–1870.

Michal Lukasik, PK Srijith, Duy Vu, Kalina
Bontcheva, Arkaitz Zubiaga, and Trevor Cohn.
2016. Hawkes processes for continuous time se-
quence classiﬁcation:
an application to rumour
stance classiﬁcation in twitter. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers). vol-
ume 2, pages 393–398.

Jing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,
Bernard J Jansen, Kam-Fai Wong, and Meeyoung
Cha. 2016. Detecting rumors from microblogs with
In Proceedings of the
recurrent neural networks.
Twenty-Fifth International Joint Conference on Ar-
tiﬁcial Intelligence. IJCAI’16, pages 3818–3824.

Jing Ma, Wei Gao, Zhongyu Wei, Yueming Lu, and
Kam-Fai Wong. 2015. Detect rumors using time se-
ries of social context information on microblogging
In Proceedings of the 24th ACM Inter-
websites.
national on Conference on Information and Knowl-
edge Management. CIKM ’15, pages 1751–1754.

Jing Ma, Wei Gao, and Kam-Fai Wong. 2017. De-
tect rumors in microblog posts using propagation
structure via kernel learning. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). vol-
ume 1, pages 708–717.

Jing Ma, Wei Gao, and Kam-Fai Wong. 2018. De-
tect rumor and stance jointly by neural multi-task
learning. In Companion Proceedings of the The Web
Conference 2018. WWW ’18, pages 585–593.

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin. 2015. Discriminative neural sentence mod-
arXiv preprint
eling by tree-based convolution.
arXiv:1504.01106 .

Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it: Iden-
tifying misinformation in microblogs. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing. EMNLP ’11, pages
1589–1599.

Jacob Ratkiewicz, Michael Conover, Mark Meiss,
Bruno Gonc¸alves, Snehal Patil, Alessandro Flam-
mini, and Filippo Menczer. 2011. Truthy: mapping
the spread of astroturf in microblog streams. In Pro-
ceedings of the 20th International Conference Com-
panion on World Wide Web. WWW ’11, pages 249–
252.

Ralph L Rosnow and Eric K Foster. 2005. Rumor
and gossip research. Psychological Science Agenda
19(4).

Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017.
Csi: A hybrid deep model for fake news detec-
tion. In Proceedings of the 2017 ACM on Confer-
ence on Information and Knowledge Management.
CIKM ’17, pages 797–806.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning. EMNLP-
CoNLL ’12, pages 1201–1211.

Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11). pages 129–136.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
In Proceedings of the 2013 conference on
bank.
empirical methods in natural language processing.
pages 1631–1642.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075 .

Ke Wu, Song Yang, and Kenny Q Zhu. 2015. False ru-
mors detection on sina weibo by propagation struc-
tures. In Data Engineering (ICDE), 2015 IEEE 31st
International Conference on. IEEE, pages 651–662.

Fan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.
Automatic detection of rumor on sina weibo. In Pro-
ceedings of the ACM SIGKDD Workshop on Mining
Data Semantics. MDS ’12, pages 13:1–13:7.

Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. En-
quiring minds: Early detection of rumors in social
In Proceedings of the
media from enquiry posts.
24th International Conference on World Wide Web.
WWW ’15, pages 1395–1405.

Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo.
2015.
Long short-term memory over recursive
structures. In Proceedings of the 32nd International
Conference on Machine Learning. pages 1604–
1612.

Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva,
Maria Liakata, and Rob Procter. 2017. Detection
and resolution of rumours in social media: A survey.
arXiv preprint arXiv:1704.00656 .

Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob
Procter, and Michal Lukasik. 2016a. Stance classiﬁ-
cation in rumours as a sequential task exploiting the
tree structure of social media conversations. In Pro-
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers. pages 2438–2448.

Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geral-
dine Wong Sak Hoi, and Peter Tolmie. 2016b.
Analysing how people orient to and spread rumours
in social media by looking at conversational threads.
PloS one 11(3):e0150989.

Rumor Detection on Twitter with Tree-structured Recursive Neural
Networks

Jing Ma1, Wei Gao2, Kam-Fai Wong1,3
1The Chinese University of Hong Kong, Hong Kong SAR
2Victoria University of Wellington, New Zealand
3MoE Key Laboratory of High Conﬁdence Software Technologies, China
1{majing,kfwong}@se.cuhk.edu.hk, 2wei.gao@vuw.ac.nz

Abstract

Automatic rumor detection is technically
very challenging. In this work, we try to
learn discriminative features from tweets
content by following their non-sequential
propagation structure and generate more
powerful representations for identifying
different type of rumors. We propose
two recursive neural models based on a
bottom-up and a top-down tree-structured
neural networks for rumor representation
learning and classiﬁcation, which natu-
rally conform to the propagation layout
of tweets. Results on two public Twit-
ter datasets demonstrate that our recursive
neural models 1) achieve much better per-
formance than state-of-the-art approaches;
2) demonstrate superior capacity on de-
tecting rumors at very early stage.

1

Introduction

Rumors have always been a social disease. In re-
cent years, it has become unprecedentedly conve-
nient for the “evil-doers” to create and disseminate
rumors in massive scale with low cost thanks to
the popularity of social media outlets on Twitter,
Facebook, etc. The worst effect of false rumors
could be devastating to individual and/or society.
Research pertaining rumors spans multiple dis-
ciplines, such as philosophy and humanities (Di-
Fonzo and Bordia, 2007; Donovan, 2007), social
psychology (Allport and Postman, 1965; Jaeger
et al., 1980; Rosnow and Foster, 2005), politi-
cal studies (Allport and Postman, 1946; Berin-
sky, 2017), management science (DiFonzo et al.,
1994; Kimmel, 2004) and recently computer sci-
ence and artiﬁcial intelligence (Qazvinian et al.,
2011; Ratkiewicz et al., 2011; Castillo et al., 2011;
Hannak et al., 2014; Zhao et al., 2015; Ma et al.,

2015). Rumor is commonly deﬁned as informa-
tion that emerge and spread among people whose
truth value is unveriﬁed or intentionally false (Di-
Fonzo and Bordia, 2007; Qazvinian et al., 2011).
Analysis shows that people tend to stop spread-
ing a rumor if it is known as false (Zubiaga et al.,
2016b). However, identifying such misinforma-
tion is non-trivial and needs investigative jour-
nalism to fact check the suspected claim, which
is labor-intensive and time-consuming. The pro-
liferation of social media makes it worse due to
the ever-increasing information load and dynam-
ics. Therefore, it is necessary to develop automatic
and assistant approaches to facilitate real-time ru-
mor tracking and debunking.

For automating rumor detection, most of the
previous studies focused on text mining from se-
quential microblog streams using supervised mod-
els based on feature engineering (Castillo et al.,
2011; Kwon et al., 2013; Liu et al., 2015; Ma
et al., 2015), and more recently deep neural mod-
els (Ma et al., 2016; Chen et al., 2017; Ruchan-
sky et al., 2017). These methods largely ignore
or oversimplify the structural information asso-
ciated with message propagation which however
has been shown conducive to provide useful clues
for identifying rumors. Kernel-based method (Wu
et al., 2015; Ma et al., 2017) was thus proposed
to model the structure as propagation trees in or-
der to differentiate rumorous and non-rumorous
claims by comparing their tree-based similarities.
But such kind of approach cannot directly classify
a tree without pairwise comparison with all other
trees imposing unnecessary overhead, and it also
cannot automatically learn any high-level feature
representations out of the noisy surface features.

In this paper, we present a neural rumor de-
tection approach based on recursive neural net-
works (RvNN) to bridge the content semantics
and propagation clues. RvNN and its variants

were originally used to compose phrase or sen-
tence representation for syntactic and semantic
parsing (Socher et al., 2011, 2012). Unlike pars-
ing, the input into our model is a propagation tree
rooted from a source post rather than the parse tree
of an individual sentence, and each tree node is
a responsive post instead of an individual words.
The content semantics of posts and the responsive
relationship among them can be jointly captured
via the recursive feature learning process along the
tree structure.

So, why can such neural model do better for
the task? Analysis has generally found that Twit-
ter could “self-correct” some inaccurate informa-
tion as users share opinions, conjectures and evi-
dences (Zubiaga et al., 2017). To illustrate our in-
tuition, Figure 1 exempliﬁes the propagation trees
of two rumors in our dataset, one being false and
the other being true1. Structure-insensitive meth-
ods basically relying on the relative ratio of differ-
ent stances in the text cannot do well when such
clue is unclear like this example. However, it can
be seen that when a post denies the false rumor,
it tends to spark supportive or afﬁrmative replies
conﬁrming the denial; in contrast, denial to a true
rumor tends to trigger question or denial in its
replies. This observation may suggest a more gen-
eral hypothesis that the repliers tend to disagree
with (or question) who support a false rumor or
deny a true rumor, and also they tend to agree with
who deny a false rumor or support a true rumor.
Meanwhile, a reply, rather than directly respond-
ing to the source tweet (i.e., the root), is usually re-
sponsive to its immediate ancestor (Lukasik et al.,
2016; Zubiaga et al., 2016a), suggesting obvious
local characteristic of the interaction. The recur-
sive network naturally models such structures for
learning to capture the rumor indicative signals
and enhance the representation by recursively ag-
gregating the signals from different branches.

To this end, we extend the standard RvNN into
two variants, i.e., a bottom-up (BU) model and a
top-down (TD) model, which represent the propa-
gation tree structure from different angles, in order
to visit the nodes and combine their representa-
tions following distinct directions. The important
merit of such architecture is that the node features
can be selectively reﬁned by the recursion given
the connection and direction of all paths of the

(a) False rumor

(b) True rumor

Figure 1: Propagation trees of two rumorous
source tweets. Nodes may express stances on their
parent as commenting, supporting, questioning or
denying. The edge arrow indicates the direction
from a response to its responded node, and the po-
larity is marked as ‘+’ (‘-’) for support (denial).
The same node color indicates the same stance on
the veracity of root node (i.e., source tweet).

tree. As a result, it can be expected that the dis-
criminative signals are better embedded into the
learned representations.

We evaluate our proposed approach based on
two public Twitter datasets. The results show that
our method outperforms strong rumor detection
baselines with large margin and also demonstrate
much higher effectiveness for detection at early
stage of propagation, which is promising for real-
time intervention and debunking. Our contribu-
tions are summarized as follows in three folds:

• This is the ﬁrst study that deeply integrates
both structure and content semantics based
on tree-structured recursive neural networks
for detecting rumors from microblog posts.

• We propose two variants of RvNN models
based on bottom-up and top-down tree struc-
tures to generate better integrated representa-
tions for a claim by capturing both structural
and textural properties signaling rumors.

• Our experiments based on real-world Twitter
datasets achieve superior improvements over
state-of-the-art baselines on both rumor clas-
siﬁcation and early detection tasks. We make
the source codes in our experiments publicly
accessible 2.

2 Related Work

Most previous automatic approaches for rumor de-
tection (Castillo et al., 2011; Yang et al., 2012; Liu

1False (true) rumor means the veracity of the rumorous

2https://github.com/majingCUHK/Rumor_

claim is false (true).

RvNN

et al., 2015) intended to learn a supervised classi-
ﬁer by utilizing a wide range of features crafted
from post contents, user proﬁles and propagation
patterns. Subsequent studies were then conducted
to engineer new features such as those represent-
ing rumor diffusion and cascades (Friggeri et al.,
2014; Hannak et al., 2014) characterized by com-
ments with links to debunking websites. Kwon
et al. (2013) introduced a time-series-ﬁtting model
based on the volume of tweets over time. Ma et al.
(2015) extended their model with more chronolog-
ical social context features. These approaches typ-
ically require heavy preprocessing and feature en-
gineering.

Zhao et al. (2015) alleviated the engineering ef-
fort by using a set of regular expressions (such
as “really?”, “not true”, etc) to ﬁnd questing and
denying tweets, but the approach was oversimpli-
ﬁed and suffered from very low recall. Ma et al.
(2016) used recurrent neural networks (RNN)
to learn automatically the representations from
tweets content based on time series. Recently, they
studied to mutually reinforce stance detection and
rumor classiﬁcation in a neural multi-task learn-
ing framework (Ma et al., 2018). However, the
approaches cannot embed features reﬂecting how
the posts are propagated and requires careful data
segmentation to prepare for time sequence.

Some kernel-based methods were exploited to
model the propagation structure. Wu et al. (2015)
proposed a hybrid SVM classiﬁer which combines
a RBF kernel and a random-walk-based graph ker-
nel to capture both ﬂat and propagation patterns
for detecting rumors on Sina Weibo. Ma et al.
(2017) used tree kernel to capture the similarity
of propagation trees by counting their similar sub-
structures in order to identify different types of ru-
mors on Twitter. Compared to their studies, our
model can learn the useful features via a more nat-
ural and general approach, i.e., the tree-structured
neural network, to jointly generate representations
from both structure and content.

RvNN has demonstrated state-of-the-art perfor-
mances in a variety of tasks, e.g., images seg-
mentation (Socher et al., 2011), phrase represen-
tation from word vectors (Socher et al., 2012),
and sentiment classiﬁcation in sentences (Socher
et al., 2013). More recently, a deep RvNN was
proposed to model the compositionality in natu-
ral language for ﬁne-grained sentiment classiﬁca-
tion by stacking multiple recursive layers (Irsoy

and Cardie, 2014). In order to avoid gradient van-
ishing, some studies integrated Long Short Term
Memory (LSTM) (Hochreiter and Schmidhuber,
1997) to RvNN (Zhu et al., 2015; Tai et al., 2015).
Mou et al. (2015) used a convolutional network
over tree structures for syntactic tree parsing of
natural language sentences.

3 Problem Statement

We deﬁne a Twitter rumor detection dataset as
a set of claims C = {C1, C2, · · · , C|C|}, where
each claim Ci corresponds to a source tweet ri
which consists of ideally all its relevant respon-
i.e., Ci =
sive tweets in chronological order,
{ri, xi1, xi2, · · · , xim} where each xi∗ is a respon-
sive tweet of the root ri. Note that although the
tweets are notated sequentially, there are connec-
tions among them based on their reply or repost
relationships, which can form a propagation tree
structure (Wu et al., 2015; Ma et al., 2017) with ri
being the root node.

We formulate this task as a supervised classiﬁ-
cation problem, which learns a classiﬁer f from
labeled claims, that is f : Ci → Yi, where Yi takes
one of the four ﬁner-grained classes: non-rumor,
false rumor, true rumor, and unveriﬁed rumor that
are introduced in the literature (Ma et al., 2017;
Zubiaga et al., 2016b).

An important issue of the tree structure is con-
cerned about the direction of edges, which can re-
sult in two different architectures of the model: 1)
a bottom-up tree; 2) a top-down tree, which are
deﬁned as follows:

• Bottom-up tree takes the similar shape as
shown in Figure 1, where responsive nodes
always point to their responded nodes and
leaf nodes not having any response are laid
out at the furthest level. We represent a tree
as Ti = (cid:104)Vi, Ei(cid:105), where Vi = Ci which con-
sists of all relevant posts as nodes, and Ei de-
notes a set of all directed links, where for any
u, v ∈ Vi, u ← v exists if v responses to u.
This structure is similar to a citation network
where a response mimics a reference.

• Top-down tree naturally conforms to the di-
rection of information propagation, in which
a link u → v means the information ﬂows
from u to v and v sees it and provides a re-
sponse to u. This structure reverses bottom-
up tree and simulates how information cas-

function with W and b as parameters. This compu-
tation is done recursively over all tree nodes; the
learned hidden vectors of the nodes can then be
used for various classiﬁcation tasks.

4.2 Bottom-up RvNN

The core idea of bottom-up model is to generate a
feature vector for each subtree by recursively visit-
ing every node from the leaves at the bottom to the
root at the top. In this way, the subtrees with sim-
ilar contexts, such as those subtrees having a de-
nial parent and a set of supportive children, will be
projected into the proximity in the representation
space. And thus such local rumor indicative fea-
tures are aggregated along different branches into
some global representation of the whole tree.

For this purpose, we make a natural extension
to the original RvNN. The overall structure of our
proposed bottom-up model is illustrated in Fig-
ure 3(b), taking a bottom-up tree (see Figure 3(a))
as input. Different from the standard RvNN, the
input of each node in the bottom-up model is a
post represented as a vector of words in the vocab-
ulary in terms of tf idf values. Here, every node
has an input vector, and the number of children of
nodes varies signiﬁcantly3.

In rumor detection, long short-term memory
(LSTM) (Hochreiter and Schmidhuber, 1997) and
gated recurrent units (GRU) (Cho et al., 2014)
were used to learn textual representation, which
adopts memory units to store information over
long time steps (Ma et al., 2016). In this paper,
we choose to extend GRU as hidden unit to model
long-distance interactions over the tree nodes be-
cause it is more efﬁcient due to fewer parameters.
Let S(j) denote the set of direct children of the
node j. The transition equations of node j in the
bottom-up model are formulated as follows:

˜xj = xjE
(cid:88)

hS =

hs

s∈S(j)

rj = σ (Wr ˜xj + UrhS)
zj = σ (Wz ˜xj + UzhS)
˜hj = tanh (Wh ˜xj + Uh(hS (cid:12) rj))
hj = (1 − zj) (cid:12) hS + zj (cid:12) ˜hj

(1)

3In standard RvNN, since an input instance is the parse
tree of a sentence, only leaf nodes have input vector, each
node representing a word of the input sentence, and the non-
leaf nodes are constituents of the sentence, and thus the num-
ber of children of a node is limited.

Figure 2: A binarized sentence parse tree (left) and
its corresponding RvNN architecture (right).

cades from a source tweet, i.e., the root, to
all its receivers, i.e., the decedents, which is
similar as (Wu et al., 2015; Ma et al., 2017).

4 RvNN-based Rumor Detection

The core idea of our method is to strengthen the
high-level representation of tree nodes by the re-
cursion following the propagation structure over
different branches in the tree. For instance, the re-
sponsive nodes conﬁrming or supporting a node
(e.g., “I agree”, “be right”, etc) can further rein-
force the stance of that node while denial or ques-
tioning responses (e.g., “disagree, “really?!) oth-
erwise weaken its stance. Compared to the kernel-
based method using propagation tree (Wu et al.,
2015; Ma et al., 2017), our method does not need
pairwise comparison among large number of sub-
trees, and can learn much stronger representation
of content following the response structure.

In this section, we will describe our extension
to the standard RvNN for modeling rumor detec-
tion based on the bottom-up and top-down archi-
tectures presented in Section 3.

4.1 Standard Recursive Neural Networks

RvNN is a type of tree-structured neural networks.
The original version of RvNN utilized binarized
sentence parse trees (Socher et al., 2012), in which
the representation associated with each node of
a parse tree is computed from its direct children.
The overall structure of the standard RvNN is il-
lustrated as the right side of Figure 2, correspond-
ing to the input parse tree at the left side.

Leaf nodes are the words in an input sen-
tence, each represented by a low-dimensional
word embedding. Non-leaf nodes are sentence
constituents, computed by recursion based on the
presentations of child nodes. Let p be the feature
vector of a parent node whose children are c1 and
c2, the representation of the parent is computed by
p = f (W ·[c1; c2]+b), where f (·) is the activation

(a) Bottom-up/Top-down tree

(b) Bottom-up RvNN model

(c) Top-down RvNN model

Figure 3: A bottom-up/top-down propagation tree and the corresponding RvNN-based models. The
black-color and red-color edges differentiate the bottom-up and top-down tree in Figure 3(a).

where xj is the original input vector of node j,
E denotes the parameter matrix for transforming
this input post, ˜xj is the transformed representa-
tion of j, [W∗, U∗] are the weight connections in-
side GRU, and hj and hs refer to the hidden state
of j and its s-th child. Thus hS denotes the sum
of the hidden state of all the children of j assum-
ing that all children are equally important to j. As
with the standard GRU, (cid:12) denotes element-wise
multiplication; a reset gate rj determines how to
combine the current input ˜xj with the memory of
children, and an update gate zj deﬁnes how much
memory from the children is cascaded into the cur-
rent node; and ˜hj denotes the candidate activation
of the hidden state of the current node. Different
from the standard GRU unit, the gating vectors in
our variant of GRU are dependent on the states of
many child units, allowing our model to incorpo-
rate representations from different children.

After recursive aggregation from bottom to up,
the state of root node (i.e., source tweet) can be re-
gard as the representation of the whole tree which
is used for supervised classiﬁcation. So, an output
layer is connected to the root node for predicting
the class of the tree using a softmax function:

ˆy = Sof tmax(Vh0 + b)

(2)

where h0 is the learned hidden vector of root node;
V and b are the weights and bias in output layer.

4.3 Top-down RvNN

This model is designed to leverage the structure
of top-down tree to capture complex propagation
patterns for classifying rumorous claims, which is
shown in Figure 3(c). It models how the informa-

tion ﬂows from source post to the current node.
The idea of this top-down approach is to generate
a strengthened feature vector for each post consid-
ering its propagation path, where rumor-indicative
features are aggregated along the propagation his-
tory in the path. For example, if current post agree
with its parent’s stance which denies the source
post, the denial stance from the root node down to
the current node on this path should be reinforced.
Due to different branches of any non-leaf node, the
top-down visit to its subtree nodes is also recur-
sive. However, the nature of top-down tree lends
this model different from the bottom-up one. The
representation of each node is computed by com-
bining its own input and its parent node instead of
its children nodes. This process proceeds recur-
sively from the root node to its children until all
leaf nodes are reached.

Suppose that the hidden state of a non-leaf node
can be passed synchronously to all its child nodes
without loss. Then the hidden state hj of a node
j can be computed by combining the hidden state
hP(j) of its parent node P(j) and its own input
vector xj. Therefore, the transition equations of
node j can be formulated as a standard GRU:

˜xj = xjE
rj = σ (cid:0)Wr ˜xj + UrhP(j)
(cid:1)
zj = σ (cid:0)Wz ˜xj + UzhP(j)
˜hj = tanh (cid:0)Wh ˜xj + Uh(hP(j) (cid:12) rj)(cid:1)
hj = (1 − zj) (cid:12) hP(j) + zj (cid:12) ˜hj

(cid:1)

(3)

Through the top-down recursion, the learned
representations are eventually embedded into the
hidden vector of all the leaf nodes. Since the num-

ber of leaf nodes varies, the resulting vectors can-
not be directly fed into a ﬁxed-size neural layer
for output. Therefore, we add a max-pooling layer
to take the maximum value of each dimension of
the vectors over all the leaf nodes. This can also
help capture the most appealing indicative features
from all the propagation paths.

Based on the pooling result, we ﬁnally use a
softmax function in the output layer to predict the
label of the tree:

ˆy = Sof tmax(Vh∞ + b)

(4)

where h∞ is the pooling vector over all leaf nodes,
V and b are parameters in the output layer.

Although both of the two RvNN models aim
to capture the structural properties by recursively
visiting all nodes, we can conjecture that the top-
down model would be better. The hypothesis is
that in the bottom-up case the ﬁnal output relies on
the representation of single root, and its informa-
tion loss can be larger than the top-down one since
in the top-down case the representations embed-
ded into all leaf nodes along different propagation
paths can be incorporated via pooling holistically.

4.4 Model Training

The model is trained to minimize the squared error
between the probability distributions of the predic-
tions and the ground truth:

L(y, ˆy) =

(yc − ˆyc)2 + λ||θ||2
2

(5)

N
(cid:88)

C
(cid:88)

n=1

c=1

where yc is the ground truth and ˆyc is the pre-
diction probability of a class, N is the number of
training claims, C is the number of classes, ||.||2 is
the L2 regularization term over all model parame-
ters θ, and λ is the trade-off coefﬁcient.

During training, all the model parameters are
updated using efﬁcient back-propagation through
structure (Goller and Kuchler, 1996; Socher et al.,
2013), and the optimization is gradient-based fol-
lowing the Ada-grad update rule (Duchi et al.,
2011) to speed up the convergence. We empiri-
cally initialize the model parameters with uniform
distribution and set the vocabulary size as 5,000,
the size of embedding and hidden units as 100. We
iterate over all the training examples in each epoch
and continue until the loss value converges or the
maximum epoch number is met.

5 Experiments and Results

5.1 Datasets

For experimental evaluation, we use two publicly
available Twitter datasets released by Ma et al.
(2017), namely Twitter15 and Twitter164, which
respectively contains 1,381 and 1,181 propagation
trees (see (Ma et al., 2017) for detailed statistics).
In each dataset, a group of wide spread source
tweets along with their propagation threads, i.e.,
replies and retweets, are provided in the form of
tree structure. Each tree is annotated with one
of the four class labels, i.e., non-rumor, false ru-
mor, true rumor and unveriﬁed rumor. We remove
the retweets from the trees since they do not pro-
vide any extra information or evidence content-
wise. We build two versions for each tree, one for
the bottom-up tree and the other for the top-down
tree, by ﬂipping the edges’ direction.

5.2 Experimental Setup

We make comprehensive comparisons between
our models and some state-of-the-art baselines on
rumor classiﬁcation and early detection tasks.

- DTR: Zhao et al. (2015) proposed a Decision-
Tree-based Ranking model to identify trending ru-
mors by searching for inquiry phrases.

- DTC: The information credibility model using
a Decision-Tree Classiﬁer (Castillo et al., 2011)
based on manually engineering various statistical
features of the tweets.

- RFC: The Random Forest Classier using 3 ﬁt-
ting parameters as temporal properties and a set of
handcrafted features on user, linguistic and struc-
tural properties (Kwon et al., 2013).

- SVM-TS: A linear SVM classiﬁer that uses
time-series to model the variation of handcrafted
social context features (Ma et al., 2015).

- SVM-BOW: A naive baseline we built by rep-
resenting text content using bag-of-words and us-
ing linear SVM for rumor classiﬁcation.

- SVM-TK and SVM-HK: SVM classiﬁer uses
a Tree Kernel (Ma et al., 2017) and that uses a Hy-
brid Kernel (Wu et al., 2015), respectively, both of
which model propagation structures with kernels.
- GRU-RNN: A detection model based on re-
current neural networks (Ma et al., 2016) with
GRU units for learning rumor representations by
modeling sequential structure of relevant posts.

4https://www.dropbox.com/s/

7ewzdrbelpmrnxu/rumdetect2017.zip?dl=0

(a) Twitter15 dataset

Method

Acc.
0.409
DTR
0.454
DTC
RFC
0.565
0.544
SVM-TS
SVM-BOW 0.548
0.493
SVM-HK
0.667
SVM-TK
0.641
GRU-RNN
0.708
BU-RvNN
0.723
TD-RvNN

Method

Acc.
0.414
DTR
0.465
DTC
0.585
RFC
SVM-TS
0.574
SVM-BOW 0.585
0.511
SVM-HK
0.662
SVM-TK
GRU-RNN
0.633
0.718
BU-RvNN
0.737
TD-RvNN

NR
F1
0.501
0.733
0.810
0.796
0.564
0.650
0.619
0.684
0.695
0.682

NR
F1
0.394
0.643
0.752
0.755
0.553
0.648
0.643
0.617
0.723
0.662

FR
F1
0.311
0.355
0.422
0.472
0.524
0.439
0.669
0.634
0.728
0.758

FR
F1
0.273
0.393
0.415
0.420
0.556
0.434
0.623
0.715
0.712
0.743

TR
F1
0.364
0.317
0.401
0.404
0.582
0.342
0.772
0.688
0.759
0.821

TR
F1
0.630
0.419
0.547
0.571
0.655
0.473
0.783
0.577
0.779
0.835

UR
F1
0.473
0.415
0.543
0.483
0.512
0.336
0.645
0.571
0.653
0.654

UR
F1
0.344
0.403
0.563
0.526
0.578
0.451
0.655
0.527
0.659
0.708

(b) Twitter16 dataset

Table 1: Results of rumor detection.
(NR: non-
rumor; FR: false rumor; TR: true rumor; UR: un-
veriﬁed rumor)

- BU-RvNN and TD-RvNN: Our bottom-up

and top-down RvNN models, respectively.

We implement DTC and RFC using Weka5,
SVM-based models using LibSVM6 and all
neural-network-based models with Theano7. We
conduct 5-fold cross-validation on the datasets and
use accuracy over all the four categories and F1
measure on each class to evaluate the performance
of models.

5.3 Rumor Classiﬁcation Performance

As shown in Table 1, our proposed models ba-
sically yield much better performance than other
methods on both datasets via the modeling of in-
teraction structures of posts in the propagation.

It is observed that the performance of the 4
baselines in the ﬁrst group based on handcrafted
features is obviously poor, varying between 0.409
and 0.585 in accuracy, indicating that they fail to
generalize due to the lack of capacity capturing
helpful features. Among these baselines, SVM-
TS and RFC perform relatively better because they

5www.cs.waikato.ac.nz/ml/weka
6www.csie.ntu.edu.tw/˜cjlin/libsvm
7deeplearning.net/software/theano

use additional temporal traits, but they are still
clearly worse than the models not relying on fea-
ture engineering. DTR uses a set of regular ex-
pressions indicative of stances. However, only
19.6% and 22.2% tweets in the two datasets con-
tain strings covered by these regular expressions,
rendering unsatisfactory result.

Among the two kernel methods that are based
on comparing propagation structures, we observe
that SVM-TK is much more effective than SVM-
HK. There are two reasons: 1) SVM-HK was
originally proposed and experimented on Sina
Weibo (Wu et al., 2015), which may not be gener-
alize well on Twitter. 2) SVM-HK loosely couples
two separate kernels: a RBF kernel based on hand-
crafted features, plus a random walk-based ker-
nel which relies on a set of pre-deﬁned keywords
for jumping over the nodes probabilistically. This
under utilizes the propagation information due to
such oversimpliﬁed treatment of tree structure. In
contrast, SVM-TK is an integrated kernel and can
fully utilize the structure by comparing the trees
based on both textual and structural similarities.

It appears that using bag-of-words is already a
decent model evidenced as the fairly good perfor-
mance of SVM-BOW which is even better than
SVM-HK. This is because the features of SVM-
HK are handcrafted for binary classiﬁcation (i.e.,
non-rumor vs rumor), ignoring the importance of
indicative words or units that beneﬁt ﬁner-grained
classiﬁcation which can be captured more effec-
tively by SVM-BOW.

The sequential neural model GRU-RNN per-
forms slightly worse than SVM-TK, but much
worse than our recursive models. This is because
it is a special case of the recursive model where
each non-leaf node has only one child. It has to
rely on a linear chain as input, which missed out
valuable structural information. However, it does
learn high-level features from the post content via
hidden units of the neural model while SVM-TK
cannot which can only evaluates similarities based
on the overlapping words among subtrees. Our re-
cursive models are inherently tree-structured and
take advantages of representation learning follow-
ing the propagation structure, thus beats SVM-TK.

In the two recursive models, TD-RvNN outper-
forms BU-RvNN, which indicates that the bottom-
up model may suffer from larger information loss
than the top-down one. This veriﬁes the hypothe-
sis we made in Section 4.3 that the pooling layer

(a) Twitter15 (elapsed time)

(b) Twitter16 (elapsed time)

(c) Twitter15 (tweets count)

(d) Twitter16 (tweets count)

Figure 4: Early rumor detection accuracy at different checkpoints in terms of elapsed time (tweets count).

Figure 5: A correctly detected false rumor at early stage by both of our models, where propagation paths
are marked with relevant stances. Note that edge direction is not shown as it applies to either case.

in the top-down model can effectively select im-
portant features embedded into the leaf nodes.

For only the non-rumor class, it seems that our
method does not perform so well as some feature-
engineering baselines. This can be explained by
the fact that these baselines are trained with ad-
ditional features such as user information (e.g.,
proﬁle, veriﬁcation status, etc) which may contain
clues for differentiating non-rumors from rumors.
Also,
the responses to non-rumors are usually
much more diverse with little informative indi-
cation, making identiﬁcation of non-rumors more
difﬁcult based on content even with the structure.

5.4 Early Rumor Detection Performance

Detecting rumors at early state of propagation is
important so that interventions can be made in a
timely manner. We compared different methods
in term of different time delays measured by ei-
ther tweet count received or time elapsed since the
source tweet is posted. The performance is evalu-
ated by the accuracy obtained when we incremen-
tally add test data up to the check point given the
targeted time delay or tweets volume.

Figure 4 shows that the performance of our re-
cursive models climbs more rapidly and starts to
supersede the other models at the early stage. Al-
though all the methods are getting to their best per-

formance in the end, TD-RvNN and BU-RvNN
only need around 8 hours or about 90 tweets to
achieve the comparable performance of the best
baseline model, i.e., SVM-TK, which needs about
36 hours or around 300 posts, indicating superior
early detection performance of our method.

Figure 5 shows a sample tree at the early stage
of propagation that has been correctly classiﬁed as
a false rumor by both recursive models. We can
see that this false rumor demonstrates typical pat-
terns in subtrees and propagation paths indicative
of the falsehood, where a set of responses sup-
porting the parent posts that deny or question the
source post are captured by our bottom-up model.
Similarly, some patterns of propagation from the
root to leaf nodes like “support→deny→support”
are also seized by our top-down model. In com-
parison, sequential models may be confused be-
cause the supportive key terms such as “be right”,
“yeah”, “exactly!” dominate the responses, and
the SVM-TK may miss similar subtrees by just
comparing the surface words.

6 Conclusions and Future Work

We propose a bottom-up and a top-down tree-
structured model based on recursive neural net-
works for rumor detection on Twitter. The inher-

ent nature of recursive models allows them using
propagation tree to guide the learning of represen-
tations from tweets content, such as embedding
various indicative signals hidden in the structure,
for better identifying rumors. Results on two pub-
lic Twitter datasets show that our method improves
rumor detection performance in very large mar-
gins as compared to state-of-the-art baselines.

In our future work, we plan to integrate other
types of information such as user properties into
the structured neural models to further enhance
representation learning and detect rumor spread-
ers at the same time. We also plan to use unsuper-
vised models for the task by exploiting structural
information.

Acknowledgment

This work is partly supported by Innovation and
Technology Fund (ITF) Project No.
6904333,
and General Research Fund (GRF) Project No.
14232816 (12183516). We would like to thank
anonymous reviewers for the insightful comments.

References

Gordon W Allport and Leo Postman. 1946. An analy-
sis of rumor. Public Opinion Quarterly 10(4):501–
517.

G.W. Allport and L.J. Postman. 1965. The psychology

of rumor. Russell & Russell.

Adam J. Berinsky. 2017. Rumors and health care
reform: Experiments in political misinformation.
British Journal of Political Science 47(2):241262.

Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter. In
Proceedings of WWW. pages 675–684.

Tong Chen, Lin Wu, Xue Li, Jun Zhang, Hongzhi
Yin, and Yang Wang. 2017. Call attention to ru-
mors: Deep attention based recurrent neural net-
arXiv preprint
works for early rumor detection.
arXiv:1704.05973 .

Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .

Nicholas DiFonzo and Prashant Bordia. 2007. Rumor,
gossip and urban legends. Diogenes 54(1):19–35.

Nicholas DiFonzo, Prashant Bordia, and Ralph L Ros-
now. 1994. Reining in rumors. Organizational Dy-
namics 23(1):47–62.

Pamela Donovan. 2007. How idle is idle talk? one hun-
dred years of rumor research. Diogenes 54(1):59–
82.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
Journal of Machine
and stochastic optimization.
Learning Research 12(Jul):2121–2159.

Adrien Friggeri, Lada A Adamic, Dean Eckles, and
Justin Cheng. 2014. Rumor cascades. In Proceed-
ings of ICWSM.

Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Neural Networks,
1996., IEEE International Conference on. IEEE,
volume 1, pages 347–352.

Aniko Hannak, Drew Margolin, Brian Keegan, and In-
gmar Weber. 2014. Get back! you don’t know me
like that: The social mediation of fact checking in-
terventions in twitter conversations. In Proceedings
of ICWSM.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Proceedings of the 27th International Conference
on Neural Information Processing Systems - Volume
2. NIPS’14, pages 2096–2104.

Marianne E Jaeger, Susan Anthony, and Ralph L Ros-
now. 1980. Who hears what from whom and with
what effect: A study of rumor. Personality and So-
cial Psychology Bulletin 6(3):473–478.

Allan J Kimmel. 2004. Rumors and rumor control: A
manager’s guide to understanding and combatting
rumors. Routledge.

Sejeong Kwon, Meeyoung Cha, Kyomin Jung, Wei
Chen, and Yajun Wang. 2013. Prominent features of
rumor propagation in online social media. In Pro-
ceedings of ICDM. pages 1103–1108.

Xiaomo Liu, Armineh Nourbakhsh, Quanzhi Li, Rui
Fang, and Sameena Shah. 2015. Real-time ru-
In Proceedings of the
mor debunking on twitter.
24th ACM International on Conference on Informa-
tion and Knowledge Management. CIKM ’15, pages
1867–1870.

Michal Lukasik, PK Srijith, Duy Vu, Kalina
Bontcheva, Arkaitz Zubiaga, and Trevor Cohn.
2016. Hawkes processes for continuous time se-
quence classiﬁcation:
an application to rumour
stance classiﬁcation in twitter. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers). vol-
ume 2, pages 393–398.

Jing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,
Bernard J Jansen, Kam-Fai Wong, and Meeyoung
Cha. 2016. Detecting rumors from microblogs with
In Proceedings of the
recurrent neural networks.
Twenty-Fifth International Joint Conference on Ar-
tiﬁcial Intelligence. IJCAI’16, pages 3818–3824.

Jing Ma, Wei Gao, Zhongyu Wei, Yueming Lu, and
Kam-Fai Wong. 2015. Detect rumors using time se-
ries of social context information on microblogging
In Proceedings of the 24th ACM Inter-
websites.
national on Conference on Information and Knowl-
edge Management. CIKM ’15, pages 1751–1754.

Jing Ma, Wei Gao, and Kam-Fai Wong. 2017. De-
tect rumors in microblog posts using propagation
structure via kernel learning. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). vol-
ume 1, pages 708–717.

Jing Ma, Wei Gao, and Kam-Fai Wong. 2018. De-
tect rumor and stance jointly by neural multi-task
learning. In Companion Proceedings of the The Web
Conference 2018. WWW ’18, pages 585–593.

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin. 2015. Discriminative neural sentence mod-
arXiv preprint
eling by tree-based convolution.
arXiv:1504.01106 .

Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it: Iden-
tifying misinformation in microblogs. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing. EMNLP ’11, pages
1589–1599.

Jacob Ratkiewicz, Michael Conover, Mark Meiss,
Bruno Gonc¸alves, Snehal Patil, Alessandro Flam-
mini, and Filippo Menczer. 2011. Truthy: mapping
the spread of astroturf in microblog streams. In Pro-
ceedings of the 20th International Conference Com-
panion on World Wide Web. WWW ’11, pages 249–
252.

Ralph L Rosnow and Eric K Foster. 2005. Rumor
and gossip research. Psychological Science Agenda
19(4).

Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017.
Csi: A hybrid deep model for fake news detec-
tion. In Proceedings of the 2017 ACM on Confer-
ence on Information and Knowledge Management.
CIKM ’17, pages 797–806.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning. EMNLP-
CoNLL ’12, pages 1201–1211.

Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11). pages 129–136.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
In Proceedings of the 2013 conference on
bank.
empirical methods in natural language processing.
pages 1631–1642.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075 .

Ke Wu, Song Yang, and Kenny Q Zhu. 2015. False ru-
mors detection on sina weibo by propagation struc-
tures. In Data Engineering (ICDE), 2015 IEEE 31st
International Conference on. IEEE, pages 651–662.

Fan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.
Automatic detection of rumor on sina weibo. In Pro-
ceedings of the ACM SIGKDD Workshop on Mining
Data Semantics. MDS ’12, pages 13:1–13:7.

Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. En-
quiring minds: Early detection of rumors in social
In Proceedings of the
media from enquiry posts.
24th International Conference on World Wide Web.
WWW ’15, pages 1395–1405.

Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo.
2015.
Long short-term memory over recursive
structures. In Proceedings of the 32nd International
Conference on Machine Learning. pages 1604–
1612.

Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva,
Maria Liakata, and Rob Procter. 2017. Detection
and resolution of rumours in social media: A survey.
arXiv preprint arXiv:1704.00656 .

Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob
Procter, and Michal Lukasik. 2016a. Stance classiﬁ-
cation in rumours as a sequential task exploiting the
tree structure of social media conversations. In Pro-
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers. pages 2438–2448.

Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geral-
dine Wong Sak Hoi, and Peter Tolmie. 2016b.
Analysing how people orient to and spread rumours
in social media by looking at conversational threads.
PloS one 11(3):e0150989.

