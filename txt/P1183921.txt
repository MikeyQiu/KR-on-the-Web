8
1
0
2
 
n
a
J
 
1
3

 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
8
5
9
0
1
.
5
0
7
1
:
v
i
X
r
a

FALKON: An Optimal Large Scale Kernel Method

Alessandro Rudi ∗
INRIA – Sierra Project-team,
´Ecole Normale Sup´erieure, Paris

Luigi Carratino
University of Genoa
Genova, Italy

Lorenzo Rosasco
University of Genoa,
LCSL, IIT & MIT

February 1, 2018

Abstract

Kernel methods provide a principled way to perform non linear, nonparametric
learning. They rely on solid functional analytic foundations and enjoy optimal statis-
tical properties. However, at least in their basic form, they have limited applicability
in large scale scenarios because of stringent computational requirements in terms of
time and especially memory. In this paper, we take a substantial step in scaling up
kernel methods, proposing FALKON, a novel algorithm that allows to eﬃciently pro-
cess millions of points. FALKON is derived combining several algorithmic principles,
namely stochastic subsampling, iterative solvers and preconditioning. Our theoretical
analysis shows that optimal statistical accuracy is achieved requiring essentially O(n)
memory and O(n√n) time. An extensive experimental analysis on large scale datasets
shows that, even with a single machine, FALKON outperforms previous state of the
art solutions, which exploit parallel/distributed architectures.

1 Introduction

The goal in supervised learning is to learn from examples a function that predicts well new
data. Nonparametric methods are often crucial since the functions to be learned can be
non-linear and complex Kernel methods are probably the most popular among nonpara-
metric learning methods, but despite excellent theoretical properties, they have limited
applications in large scale learning because of time and memory requirements, typically
at least quadratic in the number of data points.
Overcoming these limitations has motivated a variety of practical approaches including
gradient methods, as well accelerated, stochastic and preconditioned extensions, to im-
prove time complexity [1, 2, 3, 4, 5, 6]. Random projections provide an approach to
reduce memory requirements, popular methods including Nystr¨om [7, 8], random features
[9], and their numerous extensions. From a theoretical perspective a key question has
become to characterize statistical and computational trade-oﬀs, that is if, or under which
conditions, computational gains come at the expense of statistical accuracy. In particular,
recent results considering least squares, show that there are large class of problems for
which, by combining Nystr¨om or random features approaches [10, 11, 12, 13, 14, 15] with

∗

E-mail: alessandro.rudi@inria.fr. This work was done when A.R. was working at Laboratory of

Computational and Statistical Learning (Istituto Italiano di Tecnologia).

1

ridge regression, it is possible to substantially reduce computations, while preserving the
same optimal statistical accuracy of exact kernel ridge regression (KRR). While statisti-
cal lower bounds exist for this setting, there are no corresponding computational lower
bounds. The state of the art approximation of KRR, for which optimal statistical bounds
are known, typically requires complexities that are roughly O(n2) in time and memory (or
possibly O(n) in memory, if kernel computations are made on the ﬂy).

In this paper, we propose and study FALKON, a new algorithm that, to the best of
our knowledge, has the best known theoretical guarantees. At the same time FALKON
provides an eﬃcient approach to apply kernel methods on millions of points, and tested on
a variety of large scale problems outperform previously proposed methods while utilizing
only a fraction of computational resources. More precisely, we take a substantial step
in provably reducing the computational requirements, showing that, up to logarithmic
factors, a time/memory complexity of O(n√n) and O(n) is suﬃcient for optimal statistical
accuracy. Our new algorithm, exploits the idea of using Nystr¨om methods to approximate
the KRR problem, but also to eﬃciently compute a preconditioning to be used in conjugate
gradient. To the best of our knowledge this is the ﬁrst time all these ideas are combined
and put to fruition. Our theoretical analysis derives optimal statistical rates both in a
basic setting and under benign conditions for which fast rates are possible. The potential
beneﬁts of diﬀerent sampling strategies are also analyzed. Most importantly, the empirical
performances are thoroughly tested on available large scale data-sets. Our results show
that, even on a single machine, FALKON can outperforms state of the art methods on
most problems both in terms of time eﬃciency and prediction accuracy. In particular, our
results suggest that FALKON could be a viable kernel alternative to deep fully connected
neural networks for large scale problems.

The rest of the paper is organized as follows. In Sect. 2 we give some background on
kernel methods. In Sect. 3 we introduce FALKON, while in Sect. 4 we present and discuss
the main technical results. Finally in Sect. 5 we present experimental results.

2 Statistical and Computational Trade-oﬀs

in Kernel Methods

We consider the supervised learning problem of estimating a function from random noisy
samples. In statistical learning theory, this can be formalized as the problem of solving

inf
f ∈H E

(f ),

E

(f ) =

(f (x)

y)2dρ(x, y),

−

Z

given samples (xi, yi)n
candidate solutions. Ideally, a good empirical solution

i=1 from ρ, which is ﬁxed but unknown and where,

f should have small excess risk

is a space of

H

(1)

(2)

f ) =
(
R

f )
(

E

(f ),
inf
b
f ∈H E

−

since this implies it will generalize/predict well new data. In this paper, we are interested
b
In particular, we
in both computational and statistical aspects of the above problem.
investigate the computational resources needed to achieve optimal statistical accuracy, i.e.
minimal excess risk. Our focus is on the most popular class of nonparametric methods,
namely kernel methods.

b

2

Kernel methods and ridge regression. Kernel methods consider a space
tions

H

of func-

where K is a positive deﬁnite kernel 1. The coeﬃcients α1, . . . , αn are typically derived
from a convex optimization problem, that for the square loss is

fn,λ = argmin

(f (xi)

f ∈H

yi)2 + λ

f
k

2
H,
k

−

and deﬁnes the so called kernel ridge regression (KRR) estimator [16]. An advantage of
least squares approaches is that they reduce computations to a linear system

b

f (x) =

αjK(x, xi),

n

Xi=1

1
n

n

Xi=1

(Knn + λnI) α =

y,

(3)

(4)

(5)

where Knn is an n
n matrix deﬁned by (Knn)ij = K(xi, xj) and
next comment on computational and statistical properties of KRR.

×

b

y = (y1, . . . yn). We

Solving Eq. (5) for large datasets is challenging. A direct approach re-
Computations.
quires O(n2) in space, to allocate Knn, O(n2) kernel evaluations, and O(n2cK + n3) in
time, to compute and invert Knn (cK is the kernel evaluation cost assumed constant and
omitted throughout).

b

fλn) = O(n−1/2), for
Statistics. Under basic assumptions, KRR achieves an error
(
R
λn = n−1/2, which is optimal in a minimax sense and can be improved only under more
stringent assumptions [17, 18].

b

The question is then if it is possible to achieve the statistical properties of KRR, with

less computations.

Gradient methods and early stopping. A natural idea is to consider iterative solvers
and in particular gradient methods, because of their simplicity and low iteration cost. A
basic example is computing the coeﬃcients in (3) by

for a suitable step-size choice τ .

−

b

αt = αt−1 + τ [(Knnαt−1

y) + λnαt−1] ,

(6)

Computations.
In this case, if t is the number of iterations, gradient methods require
O(n2t) in time, O(n2) in memory and O(n2) in kernel evaluations, if the kernel matrix
is stored. Note that, the kernel matrix can also be computed on the ﬂy with only O(n)
memory, but O(n2t) kernel evaluations are required. We note that, beyond the above
simple iteration, several variants have been considered including accelerated [1, 19] and

1K is positive deﬁnite, if the matrix with entries K(xi, xj) is positive semideﬁnite ∀x1, . . . , xN , N ∈

N [16]

3

stochastic extensions [20].

Statistics. The statistical properties of iterative approaches are well studied and also in
the case where λ is set to zero, and regularization is performed by choosing a suitable
stopping time [21]. In this latter case, the number of iterations can roughly be thought of
1/λ and O(√n) iterations are needed for basic gradient descent, O(n1/4) for accelerated
methods and possible O(1) iterations/epochs for stochastic methods. Importantly, we note
that unlike most optimization studies, here we are considering the number of iterations
needed to solve (1), rather than (4).

While the time complexity of these methods dramatically improves over KRR, and
computations can be done in blocks, memory requirements (or number of kernel eval-
uations) still makes the application to large scale setting cumbersome. Randomization
provides an approach to tackle this challenge.

Random projections. The rough idea is to use random projections to compute Knn
only approximately. The most popular examples in this class of approaches are Nystr¨om
[7, 8] and random features [9] methods. In the following we focus in particular on a basic
Nystr¨om approach based on considering functions of the form

M

Xi=1

fλ,M (x) =

αiK(x,

xi), with

x1, . . . ,
{

xM } ⊆ {

,
x1, . . . , xn}

(7)

e

deﬁned considering only a subset of M training points sampled uniformly. In this case,
there are only M coeﬃcients that, following the approach in (4), can be derived considering
the linear system

e

e

e

e

H

α = z,

where H = K ⊤

nM KnM + λnKM M ,

z = K ⊤

nM ˆy.

(8)

Here KnM is the n
×
with (KM M )ij = K(
can be seen as a particular form of random projections.
e

M matrix with (KnM )ij = K(xi,
xi,

M matrix
xj). This method consists in subsampling the columns of Knn and

xj) and KM M is the M

×

e

e

e

Computations. Direct methods for solving (8) require O(nM 2) in time to form K ⊤
nM KnM
and O(M 3) for solving the linear system, and only O(nM ) kernel evaluations. The naive
memory requirement is O(nM ) to store KnM , however if K ⊤
nM KnM is computed in blocks
of dimension at most M
Iterative approaches as
in (6) can also be combined with random projections [22, 23, 24] to slightly reduce time
requirements (see Table. 1, or Sect. F in the appendix, for more details).

M only O(M 2) memory is needed.

×

Statistics. The key point though, is that random projections allow to dramatically reduce
memory requirements as soon as M
n and the question arises of whether this comes
at expenses of statistical accuracy. Interestingly, recent results considering this question
show that there are large classes of problems for which M = ˜O(√n) suﬃces for the same
optimal statistical accuracy of the exact KRR [11, 12, 13].

≪

In summary, in this case the computations needed for optimal statistical accuracy
are reduced from O(n2) to O(n√n) kernel evaluations, but the best time complexity is

4

basically O(n2). In the rest of the paper we discuss how this requirement can indeed be
dramatically reduced.

3 FALKON

Our approach is based on a novel combination of randomized projections with iterative
solvers plus preconditioning. The main novelty is that we use random projections to
approximate both the problem and the preconditioning.

Preliminaries: preconditioning and KRR. We begin recalling the basic idea behind
preconditioning. The key quantity is the condition number, that for a linear system is the
ratio between the largest and smallest singular values of the matrix deﬁning the problem
[25]. For example, for problem (5) the condition number is given by

cond(Knn + λnI) = (σmax + λn)/(σmin + λn),

with σmax, σmin largest and smallest eigenvalues of Knn, respectively. The importance of
the condition number is that it captures the time complexity of iteratively solving the
corresponding linear system. For example, if a simple gradient descent (6) is used, the
number of iterations needed for an ǫ accurate solution of problem (5) is

t = O(cond(Knn + λnI) log(1/ǫ)).

≈

It is shown in [23] that in this case t = √n log n are needed to achieve a solution with
ǫ ) are needed
good statistical properties. Indeed, it can be shown that roughly t
where λ = 1/√n and ǫ = 1/n. The idea behind preconditioning is to use a suitable matrix
B to deﬁne an equivalent linear system with better condition number. For (5), an ideal
choice is B such that

1/λ log( 1

BB⊤ = (Knn + λnI)−1
(9)
and B⊤(Knn + λnI)B β = B⊤ ˆy. Clearly, if β∗ solves the latter problem, α∗ = Bβ∗ is a
solution of problem (5). Using a preconditioner B as in (9) one iteration is suﬃcient, but
computing the B is typically as hard as the original problem. The problem is to derive
preconditioning such that (9) might hold only approximately, but that can be computed
eﬃciently. Derivation of eﬃcient preconditioners for the exact KRR problem (5) has been
the subject of recent studies, [3, 4, 26, 5, 6]. In particular, [4, 26, 5, 6] consider random
projections to approximately compute a preconditioner. Clearly, while preconditioning (5)
leads to computational speed ups in terms of the number of iterations, requirements in
terms of memory/kernel evaluation are the same as standard kernel ridge regression.

The key idea to tackle this problem is to consider an eﬃcient preconditioning approach

for problem (8) rather than (5).

Basic FALKON algorithm. We begin illustrating a basic version of our approach.
The key ingredient is the following preconditioner for Eq. (8),

BB⊤ =

K 2

M M + λnKM M

(10)

n
M

(cid:16)

5

−1

,

(cid:17)

Algorithm 1 MATLAB code for FALKON. It requires O(nM t + M 3) in time and O(M 2)
in memory. See Sect. A and Alg. 2 in the appendixes for the complete algorithm.

Input: Dataset X = (xi)n
i=1 ∈
KernelMatrix computing the kernel matrix given two sets of points, regularization parameter λ,
number of iterations t.
Output: Nystr¨om coeﬃcients α.

Rn, centers C = (˜xj)M

D, ˆy = (yi)n

j=1 ∈

i=1 ∈

RM

Rn

D,

×

×

function alpha = FALKON(X, C, Y, KernelMatrix, lambda, t)
n = size(X,1); M = size(C,1); KMM = KernelMatrix(C,C);
T = chol(KMM + eps*M*eye(M));
A = chol(T*T’/M + lambda*eye(M));

function w = KnM_times_vector(u, v)
w = zeros(M,1); ms = ceil(linspace(0, n, ceil(n/M)+1));
for i=1:ceil(n/M)
Kr = KernelMatrix( X(ms(i)+1:ms(i+1),:), C );
w = w + Kr’*(Kr*u + v(ms(i)+1:ms(i+1),:));
end
end

BHB = @(u) A’\(T’\(KnM_times_vector(T\(A\u), zeros(n,1))/n) + lambda*(A\u));
r = A’\(T’\KnM_times_vector(zeros(M,1), Y/n));
alpha = T\(A\conjgrad(BHB, r, t));
end

which is itself based on a Nystr¨om approximation2. The above preconditioning is a natural
approximation of the ideal preconditioning of problem (8) that corresponds to BB⊤ =
nM KnM + λnKM M )−1 and reduces to it if M = n. Our theoretical analysis, shows
(K ⊤
n suﬃces for deriving optimal statistical rates. In its basic form FALKON is
that M
derived combining the above preconditioning and gradient descent,

≪

M

fλ,M,t(x) =

αt,iK(x,

xi), with αt = Bβt

and

b

Xi=1
βk = βk−1

−
N, β0 = 0 and 1

τ
n

h

B⊤
e

K ⊤

nM (KnM (Bβk−1)

−

y) + λnKM M (Bβk−1)
i

,

∈

t and a suitable chosen τ . In practice, a reﬁned version
for t
of FALKON is preferable where a faster gradient iteration is used and additional care is
taken in organizing computations.

≤

≤

b

k

(11)

(12)

FALKON. The actual version of FALKON we propose is Alg. 1 (see Sect. A, Alg. 2 for
the complete algorithm). It consists in solving the system B⊤HBβ = B⊤z via conjugate
gradient [25], since it is a fast gradient method and does not require to specify the step-size.
Moreover, to compute B quickly, with reduced numerical errors, we consider the following

2 For the sake of simplicity, here we assume KMM to be invertible and the Nystr¨om centers selected with
uniform sampling from the training set, see Sect. A and Alg. 2 in the appendix for the general algorithm.

6

strategy

B =

T −1A−1,

1
√n

T = chol(KM M ), A = chol

T T ⊤ + λI

,

(13)

1
M

(cid:18)

(cid:19)

where chol() is the Cholesky decomposition (in Sect. A the strategy for non invertible
KM M ).

Computations.
in Alg. 1, B is never built explicitly and A, T are two upper-triangular
matrices, so A−⊤u, A−1u for a vector u costs M 2, and the same for T . The cost of
computing the preconditioner is only 4
3 M 3 ﬂoating point operations (consisting in two
Cholesky decompositions and one product of two triangular matrices). Then FALKON
requires O(nM t + M 3) in time and the same O(M 2) memory requirement of the basic
Nystr¨om method, if matrix/vector multiplications at each iteration are performed in blocks.
This implies O(nM t) kernel evaluations are needed.

The question remains to characterize M and the number of iterations needed for good
statistical accuracy. Indeed, in the next section we show that roughly O(n√n) computa-
tions and O(n) memory are suﬃcient for optimal accuracy. This implies that FALKON is
currently the most eﬃcient kernel method with the same optimal statistical accuracy of
KRR, see Table 1.

4 Theoretical Analysis

In this section, we characterize the generalization properties of FALKON showing it
achieves the optimal generalization error of KRR, with dramatically reduced computations.
This result is given in Thm. 3 and derived in two steps. First, we study the diﬀerence
between the excess risk of FALKON and that of the basic Nystr¨om (8), showing it depends
on the condition number induced by the preconditioning, hence on M (see Thm.1). De-
riving these results requires some care, since diﬀerently to standard optimization results,
our goal is to solve (1) i.e. achieve small excess risk, not to minimize the empirical error.
O(1/λ) allows to make this diﬀerence as small as
Second, we show that choosing M =
e−t/2 (see Thm.2). Finally, recalling that the basic Nystr¨om for λ = 1/√n has essentially
the same statistical properties of KRR [13], we answer the question posed at the end of
the last section and show that roughly log n iterations are suﬃcient for optimal statistical
accuracy. Following the discussion in the previous section this means that the computa-
O(n)
tional requirements for optimal accuracy are
in space. Later in this section faster rates under further regularity assumptions are also
e
derived and the eﬀect of diﬀerent selection methods for the Nystr¨om centers considered.
The proofs for this section are provided in Sect. E of the appendixes.

O(n√n) in time/kernel evaluations and

e

e

4.1 Main Result

The ﬁrst result is interesting in its own right since it corresponds to translating optimiza-
tion guarantees into statistical results. In particular, we derive a relation the excess risk of
the FALKON algorithm
fλ,M from Eq. (8)
with uniform sampling.

fλ,M,t from Alg. 1 and the Nystr¨om estimator

b

e

7

Algorithm

kernel evaluations memory

test time

SVM / KRR + direct method
KRR + iterative [1, 2]
Doubly stochastic [22]
Pegasos / KRR + sgd [27]
KRR + iter + precond [3, 28, 4, 5, 6]
Divide & Conquer [29]
Nystr¨om, random features [7, 8, 9]
Nystr¨om + iterative [23, 24]
Nystr¨om + sgd [20]
FALKON (see Thm. 3)

train time
n3
n2 4√n
n2√n
n2
n2
n2
n2
n2
n2
n√n

n2
n2
n2√n
n2
n2
n√n
n√n
n√n
n√n
n√n

n2
n2
n
n
n
n
n
n
n
n

n
n
n
n
n
n
√n
√n
√n
√n

Table 1: Computational complexity required by diﬀerent algorithms, for optimal general-
ization. Logarithmic terms are not showed.

Theorem 1. Let n, M
1 such that K(x, x)
κ
δ
probability 1

≥

≥
≤

−

N, 0 < λ

3, t
∈
κ2 for any x

λ1 and δ

(0, 1]. Assume there exists
X. Then, the following inequality holds with

≤

∈

∈

fλ,M,t)1/2
(
R

fλ,M )1/2 + 4
(
≤ R

v e−νt

1 +

r
i and ν = log(1 + 2/(cond (B⊤HB)

e

1/2

b

n

v2 = 1
n

where

1)), with cond (B⊤HB)
the condition number of B⊤HB. Note that λ1 > 0 is a constant not depending on
λ, n, M, δ, t.
b

P

−

b

i=1 y2

9κ2
λn

log

n
δ

,

The additive term in the bound above decreases exponentially in the number of it-
erations. If the condition number of B⊤HB is smaller than a small universal constant
(e.g. 17), then ν > 1/2 and the additive term decreases as e− t
2 . Next, theorems derive
a condition on M that allows to control cond (B⊤HB), and derive such an exponential
decay.

Theorem 2. Under the same conditions of Thm. 1, if

M

5

1 +

≥

(cid:20)

14κ2
λ

log

(cid:21)

8κ2
λδ

.

then the exponent ν in Thm. 1 satisﬁes ν

1/2.

≥

The above result gives the desired exponential bound showing that after log n iterations

the excess risk of FALKON is controlled by that of the basic Nystr¨om, more precisely

fλ,M,t)
(
R

≤

fλ,M ) when t
(
2
R

≥

log

fλ,M ) + log
(
R

1 +

log

9κ2
λn

n
δ

(cid:19)

(cid:18)

+ log

16

v2

.

(cid:0)

(cid:1)

e

b
Finally, we derive an excess risk bound for FALKON. By the no-free-lunch theorem,
this requires some conditions on the learning problem. We ﬁrst consider a standard basic
setting where we only assume it exists fH

(fH) = inf f ∈H

such that

(f ).

b

e

E

E

∈ H

8

(0, 1]. Assume there exists κ
2 ], almost surely, a > 0. There exist n0

≥

∈
2 , a
a

1 such that K(x, x)

κ2 for any
N such that for any

≤

∈

M

75 √n log

log(n) + 5 + 2 log(a + 3κ),

48κ2n
δ

,

t

≥

1
2

Theorem 3. Let δ
[
X, and y
x
−
n0, if
n

∈

∈
≥

λ =

1
√n

,

then with probability 1

δ,

≥

−

fλ,M,t )
(
R

≤

c0 log2 24
δ
√n

.

In particular n0, c0 do not depend on λ, M, n, t and c0 do not depend on δ.
b

The above result provides the desired bound, and all the constants are given in the
appendix. The obtained learning rate is the same as the full KRR estimator and is known
to be optimal in a minmax sense [17], hence not improvable. As mentioned before, the
same bound is also achieved by the basic Nystr¨om method but with much worse time
complexity. Indeed, as discussed before, using a simple iterative solver typically requires
O(√n log n) iterations, while we need only O(log n). Considering the choice for M this
leads to a computational time of O(nM t) = O(n√n) for optimal generalization (omitting
logarithmic terms). To the best of our knowledge FALKON currently provides the best
time/space complexity to achieve the statistical accuracy of KRR. Beyond the basic setting
considered above, in the next section we show that FALKON can achieve much faster rates
under reﬁned regularity assumptions and also consider the potential beneﬁts of leverage
score sampling.

4.2 Fast learning rates and Nystr¨om with approximate leverage scores

≤

li(λ)

qli(λ),

Considering fast rates and Nystr¨om with more general sampling is considerably more
technical and a heavier notation is needed. Our analysis apply to any approximation
[30, 12, 31]) satisfying the deﬁnition of q-approximate leverage scores [13],
scheme (e.g.
satisfying q−1li(λ)
. Here λ > 0, li(λ) = (Knn(Knn +
∈ {
}
∀
≤
λnI)−1)ii are the leverage scores and q
1 controls the quality of the approximation. In
≥
particular, given λ, the Nystr¨om points are sampled independently from the dataset with
X
li(λ). We need a few more deﬁnitions. Let Kx = K(x,
probability pi ∝
the reproducing kernel Hilbert space [32] of functions with inner product deﬁned
and
·iH deﬁned by
= span
by
H
to be the linear operator
Kx, Kx′
h
f, Cg
h

b
Kx |
{
X f (x)g(x)dρX (x), for all f, g

X
∈
iH = K(x, x′), for all x, x′

and closed with respect to the inner product

. Finally deﬁne the following quantities,

) for any x
·

X. Deﬁne C :

iH =

H → H

1, . . . , n

∈ H

,
h·

H

∈

∈

x

b

}

i

R

N

∞(λ) = sup

x∈Xk

(C + λI)−1/2Kxk
H,

N

(λ) = Tr(C(C + λI)−1).

The latter quantity is known as degrees of freedom or eﬀective dimension, can be seen as
∞(λ) can be seen to provide a uniform bound
a measure of the size of
κ2
λ [13]. We can now
on the leverage scores. In particular note that
provide a reﬁned version of Thm. 2.

. The quantity

∞(λ)

≤ N

(λ)

H

N

N

≤

9

1

0.95

0.9

MSE
0.85

0.8

0.75

Nystrom SGD
Nystrom GD
NYTRO GD
NYTRO SGD
Nystrom CG
NYTRO CG
FALKON

0

7

20

40

60

80

100

Iterates/epochs

Figure 1: Falkon is compared to stochastic gradient, gradient descent and conjugate gra-
dient applied to Problem (8), while NYTRO refer to the variants described in [23]. The
107 examples) with respect to the
graph shows the test error on the HIGGS dataset (1.1
×
number of iterations (epochs for stochastic algorithms).

≥
8κ2
λδ

.

Theorem 4. Under the same conditions of Thm. 1, the exponent ν in Thm. 1 satisﬁes
ν

1/2, when

≥
1. either Nystr¨om uniform sampling is used with M
≥
2. or Nystr¨om q-approx. lev. scores [13] is used, with λ

70 [1 +
N
19κ2
n log n

∞(λ)] log 8κ2
λδ .
405κ2 log 12κ2
2δ , n
δ

,

≥

M

215

2 + q2

(λ)

log

N
We then recall the standard, albeit technical, assumptions leading to fast rates [17, 18].

≥

(cid:3)

(cid:2)

∈

(0, 1] and Q

The capacity condition requires the existence of γ
≤
Q2λ−γ. Note that this condition is always satisﬁed with Q = κ and γ = 1. The source
, such that fH = C r−1/2g.
condition requires the existence of r
Intuitively, the capacity condition measures the size of
is small and
rates are faster. The source condition measures the regularity of fH, if r is big fH is regular
and rates are faster. The case r = 1/2 and γ = D/(2s) (for a kernel with smoothness
s and input space RD) recovers the classic Sobolev condition. For further discussions on
the interpretation of the conditions above see [17, 18, 11, 13]. We can then state our main
result on fast rates

∈ H
, if γ is small then

[1/2, 1] and g

0, such that

(λ)

H

H

N

≥

∈

Theorem 5. Let δ
x
X, and y
∈
any n

[
−

∈

(0, 1]. Assume there exists κ
2 ], almost surely, with a > 0. There exist an n0

1 such that K(x, x)

≥

∈
2 , a
a

κ2 for any
N such that for

≤

∈

≥

n0 the following holds. When
λ = n− 1

2r+γ ,

t

log(n) + 5 + 2 log(a + 3κ2),

≥

1. and either Nystr¨om uniform sampling is used with M
2. or Nystr¨om q-approx. lev. scores [13] is used with M

70 [1 +
220

N
2 + q2

∞(λ)] log 8κ2
λδ ,
log 8κ2
(λ)
λδ ,

N

≥
≥

(cid:2)

(cid:3)

10

Table 2: Architectures:
machines,
128GB RAM, ⋆ cluster with IBM POWER8 12-core processor, 512 GB RAM,
platform.

cluster 8 EC2 r3.8xlarge
single machine with two Intel Xeon E5-2620, one Nvidia GTX Titan X GPU,
unknown

cluster 128 EC2 r3.2xlarge machines,

∗

†

‡

≀

MillionSongs

YELP

TIMIT

MSE Relative error Time(s) RMSE Time(m)

c-err

Time(h)

FALKON
Prec. KRR [4]
Hierarchical [34]
D&C [29]
Rand. Feat. [29]
Nystr¨om [29]
ADMM R. F.[4]
BCD R. F. [24]
BCD Nystr¨om [24]
EigenPro [6]
KRR [34] [24]
Deep NN [35]
Sparse Kernels [35]
Ensemble [36]

80.10
-
-
80.35
80.93
80.38
-
-
-
-
-
-
-
-

4.51
4.58
4.56

10−3
10−3
10−3

5.01

10−3

4.55

10−3

×
×
×
-
-
-

×
-
-
-

×
-
-
-

55
289†
293⋆
737∗
772∗
876∗
958†
-
-
-
-
-
-
-

0.833
-
-
-
-
-
-
0.949
0.861
-
0.854
-
-
-

20
-
-
-
-
-
-
42‡
60‡
-
500‡
-
-
-

32.3%
-
-
-
-
-
-
34.0%
33.7%
32.6%
33.5%
32.4%
30.9%
33.5%

1.5
-
-
-
-
-
-
1.7‡
1.7‡
3.9≀
8.3‡
-
-
-

then with probability 1

δ,

−

fλ,M,t)
(
R

≤

c0 log2 24
δ

n− 2r

2r+γ .

where
fλ,M,t is the FALKON estimator (Sect. 3, Alg. 1 and Sect. A, Alg. 2 in the appendix
for the complete version). In particular n0, c0 do not depend on λ, M, n, t and c0 do not
b
depend on δ.

b

The above result shows that FALKON achieves the same fast rates as KRR, under
the same conditions [17]. For r = 1/2, γ = 1, the rate in Thm. 3 is recovered. If γ <
1, r > 1/2, FALKON achieves a rate close to O(1/n). By selecting the Nystr¨om points
with uniform sampling, a bigger M could be needed for fast rates (albeit always less than
n). However, when approximate leverage scores are used M , smaller than nγ/2
√n is
always enough for optimal generalization. This shows that FALKON with approximate
leverage scores is the ﬁrst algorithm to achieve fast rates with a computational complexity
that is O(n

(λ)) = O(n

2 ) in time.

O(n1+ γ

2r+γ )

1+ γ

≪

N

≤

5 Experiments

We present FALKON’s performance on a range of large scale datasets.
As shown in
Table 2, 3, FALKON achieves state of the art accuracy and typically outperforms previ-
ous approaches in all the considered large scale datasets including IMAGENET. This is
remarkable considering FALKON required only a fraction of the competitor’s computa-
tional resources. Indeed we used a single machine equipped with two Intel Xeon E5-2630

11

Table 3: Architectures:
machine with two Intel Xeon E5-2620, one Nvidia GTX Titan X GPU, 128GB RAM,
single machine [37]

cluster with IBM POWER8 12-core cpu, 512 GB RAM,

†

≀

single

‡

SUSY

HIGGS

IMAGENET

c-err

AUC Time(m) AUC Time(h)

c-err

Time(h)

FALKON
EigenPro [6]
Hierarchical [34]
Boosted Decision Tree [38]
Neural Network [38]
Deep Neural Network [38]
Inception-V4 [39]

19.6% 0.877
19.8%
20.1%
-
-
-
-

-
-
0.863
0.875
0.879
-

4
6≀
40†
-
-
4680‡
-

0.833
-
-
0.810
0.816
0.885
-

3
-
-
-
-
78‡
-

20.7%
-
-
-
-
-
20.0%

4
-
-
-
-
-
-

v3, one NVIDIA Tesla K40c and 128 GB of RAM and a basic MATLAB FALKON im-
plementation, while typically the results for competing algorithm have been performed on
clusters of GPU workstations (accuracies, times and used architectures are cited from the
corresponding papers).

A minimal MATLAB implementation of FALKON is presented in Appendix G. The
code necessary to reproduce the following experiments, plus a FALKON version that is
able to use the GPU, is available on GitHub at https://github.com/LCSL/FALKON_paper
The error is measured with MSE, RMSE or relative error for regression problems, and
with classiﬁcation error (c-err) or AUC for the classiﬁcation problems, to be consistent
with the literature. For datasets which do not have a ﬁxed test set, we set apart 20%
of the data for testing. For all datasets, but YELP and IMAGENET, we normalize the
features by their z-score. From now on we denote with n the cardinality of the dataset, d
the dimensionality. A comparison of FALKON with respect to other methods to compute
the Nystr¨om estimator, in terms of the MSE test error on the HIGGS dataset, is given in
Figure 1.

MillionSongs [33] (Table 2, n = 4.6
kernel with σ = 6, λ = 10−6 and 104 Nystr¨om centers. Moreover with 5
FALKON achieves a 79.20 MSE, and 4.49

105, d = 90, regression). We used a Gaussian
104 center,

10−3 rel. error in 630 sec.

×

×

106, d = 440, multiclass classiﬁcation). We used the same
TIMIT (Table 2, n = 1.2
preprocessed dataset of [6] and Gaussian Kernel with σ = 15, λ = 10−9 and 105 Nystr¨om
centers.

×

106, d = 6.52

107, regression). We used the same dataset
YELP (Table 2, n = 1.5
×
of [24]. We extracted the 3-grams from the plain text with the same pipeline as [24], then
we mapped them in a sparse binary vector which records if the 3-gram is present or not
104 Nystr¨om centers. With 105 centers,
in the example. We used a linear kernel with 5
we get a RMSE of 0.828 in 50 minutes.

×

×

SUSY (Table 3, n = 5

106, d = 18, binary classiﬁcation). We used a Gaussian kernel

×

×

12

with σ = 4, λ = 10−6 and 104 Nystr¨om centers.

107, d = 28, binary classiﬁcation). Each feature has been
HIGGS (Table 3, n = 1.1
normalized subtracting its mean and dividing for its variance. We used a Gaussian ker-
nel with diagonal matrix width learned with cross validation on a small validation set,
λ = 10−8 and 105 Nystr¨om centers. If we use a single σ = 5 we reach an AUC of 0.825.

×

106, d = 1536, multiclass classiﬁcation). We report the
IMAGENET (Table 3, n = 1.3
top 1 c-err over the validation set of ILSVRC 2012 with a single crop. The features are
obtained from the convolutional layers of pre-trained Inception-V4 [39]. We used Gaussian
kernel with σ = 19, λ = 10−9 and 5
104 Nystr¨om centers. Note that with linear kernel
we achieve c-err = 22.2%.

×

×

Acknowledgments. The authors would like to thank Mikhail Belkin, Benjamin Recht
and Siyuan Ma, Eric Fosler-Lussier, Shivaram Venkataraman, Stephen L. Tu, for providing
their features of the TIMIT and YELP datasets, and NVIDIA Corporation for the donation
of the Tesla K40c GPU used for this research. This work is funded by the Air Force project
FA9550-17-1-0390 (European Oﬃce of Aerospace Research and Development) and by the
FIRB project RBFR12M3AC (Italian Ministry of Education, University and Research).

References

[1] A. Caponnetto and Yuan Yao. Adaptive rates for regularization operators in learning

theory. Analysis and Applications, 08, 2010.

[2] L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro
Verri. Spectral Algorithms for Supervised Learning. Neural Computation, 20(7):1873–
1897, 2008.

[3] Gregory E Fasshauer and Michael J McCourt. Stable evaluation of gaussian radial
basis function interpolants. SIAM Journal on Scientiﬁc Computing, 34(2):A737–A762,
2012.

[4] Haim Avron, Kenneth L Clarkson, and David P Woodruﬀ. Faster kernel ridge regres-
sion using sketching and preconditioning. arXiv preprint arXiv:1611.03220, 2016.

[5] Alon Gonen, Francesco Orabona, and Shai Shalev-Shwartz. Solving ridge regression

using sketched preconditioned svrg. arXiv preprint arXiv:1602.02350, 2016.

[6] Siyuan Ma and Mikhail Belkin. Diving into the shallows: a computational perspective

on large-scale shallow learning. arXiv preprint arXiv:1703.10622, 2017.

[7] Christopher Williams and Matthias Seeger. Using the Nystr¨om Method to Speed Up

Kernel Machines. In NIPS, pages 682–688. MIT Press, 2000.

[8] Alex J. Smola and Bernhard Sch¨olkopf. Sparse Greedy Matrix Approximation for

Machine Learning. In ICML, pages 911–918. Morgan Kaufmann, 2000.

13

[9] Ali Rahimi and Benjamin Recht. Random Features for Large-Scale Kernel Machines.

In NIPS, pages 1177–1184. Curran Associates, Inc., 2007.

[10] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing
In Advances in neural information

minimization with randomization in learning.
processing systems, pages 1313–1320, 2009.

[11] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In COLT,

volume 30 of JMLR Proceedings, pages 185–209. JMLR.org, 2013.

[12] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression
with statistical guarantees. In Advances in Neural Information Processing Systems
28, pages 775–783. 2015.

[13] Alessandro Rudi, Raﬀaello Camoriano, and Lorenzo Rosasco. Less is more: Nystr¨om
computational regularization. In Advances in Neural Information Processing Systems,
pages 1648–1656, 2015.

[14] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with

random features. arXiv preprint arXiv:1602.04474, 2016.

[15] Francis Bach. On the equivalence between kernel quadrature rules and random feature

expansions. Journal of Machine Learning Research, 18(21):1–38, 2017.

[16] Bernhard Sch¨olkopf and Alexander J. Smola. Learning with Kernels: Support Vec-
tor Machines, Regularization, Optimization, and Beyond (Adaptive Computation and
Machine Learning). MIT Press, 2002.

[17] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-
squares algorithm. Foundations of Computational Mathematics, 7(3):331–368, 2007.

[18] Ingo Steinwart, Don R Hush, Clint Scovel, et al. Optimal rates for regularized least

squares regression. In COLT, 2009.

[19] F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning

theory. Journal of complexity, 23(1):52–72, 2007.

[20] Aymeric Dieuleveut and Francis Bach. Non-parametric stochastic approximation with

large step sizes. arXiv preprint arXiv:1408.0361, 2014.

[21] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient

descent learning. Constructive Approximation, 26(2):289–315, 2007.

[22] Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and
Le Song. Scalable kernel methods via doubly stochastic gradients. In Advances in
Neural Information Processing Systems, pages 3041–3049, 2014.

[23] Raﬀaello Camoriano, Tom´as Angles, Alessandro Rudi, and Lorenzo Rosasco. Nytro:
In Proceedings of the 19th International

When subsampling meets early stopping.
Conference on Artiﬁcial Intelligence and Statistics, pages 1403–1411, 2016.

14

[24] Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, and Benjamin Recht. Large
scale kernel learning using block coordinate descent. arXiv preprint arXiv:1602.05310,
2016.

[25] Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003.

[26] Kurt Cutajar, Michael Osborne, John Cunningham, and Maurizio Filippone. Precon-
ditioning kernel matrices. In International Conference on Machine Learning, pages
2529–2538, 2016.

[27] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos:
Primal estimated sub-gradient solver for svm. Mathematical programming, 127(1):3–
30, 2011.

[28] Yun Yang, Mert Pilanci, and Martin J Wainwright. Randomized sketches for kernels:
Fast and optimal non-parametric regression. arXiv preprint arXiv:1501.06195, 2015.

[29] Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. Divide and Conquer
Kernel Ridge Regression. In COLT, volume 30 of JMLR Proceedings, pages 592–617.
JMLR.org, 2013.

[30] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruﬀ.
Fast approximation of matrix coherence and statistical leverage. JMLR, 13:3475–3506,
2012.

[31] Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng,
and Aaron Sidford. Uniform Sampling for Matrix Approximation. In ITCS, pages
181–190. ACM, 2015.

[32] I. Steinwart and A. Christmann. Support Vector Machines. Information Science and

Statistics. Springer New York, 2008.

[33] Thierry Bertin-Mahieux, Daniel P. W. Ellis, Brian Whitman, and Paul Lamere. The

million song dataset. In ISMIR, 2011.

[34] Jie Chen, Haim Avron, and Vikas Sindhwani. Hierarchically compositional kernels

for scalable nonparametric learning. CoRR, abs/1608.00860, 2016.

[35] Avner May, Alireza Bagheri Garakani, Zhiyun Lu, Dong Guo, Kuan Liu, Aure-
lien Bellet, Linxi Fan, Michael Collins, Daniel J. Hsu, Brian Kingsbury, Michael
Picheny, and Fei Sha. Kernel approximation methods for speech recognition. CoRR,
abs/1701.03577, 2017.

[36] Po-Sen Huang, Haim Avron, Tara N. Sainath, Vikas Sindhwani, and Bhuvana Ram-
abhadran. Kernel methods match deep neural networks on timit. 2014 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
205–209, 2014.

[37] Alexandre Alves. Stacking machine learning classiﬁers to identify higgs bosons at the

lhc. CoRR, abs/1612.07725, 2016.

15

[38] Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in

high-energy physics with deep learning. Nature communications, 5, 2014.

[39] Christian Szegedy, Sergey Ioﬀe, Vincent Vanhoucke, and Alexander A Alemi.
Inception-v4, inception-resnet and the impact of residual connections on learning.
pages 4278–4284, 2017.

[40] Michael Reed and Barry Simon. Methods of Modern Mathematical Physics: Vol.: 1.:

Functional Analysis. Academic press, 1980.

[41] Ernesto D Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto D Giovannini, and
In Journal of

Francesca Odone. Learning from examples as an inverse problem.
Machine Learning Research, pages 883–904, 2005.

[42] Alessandro Rudi, Guillermo D Canas, and Lorenzo Rosasco. On the Sample Com-

plexity of Subspace Learning. In NIPS, pages 2067–2075, 2013.

[43] St´ephane Boucheron, G´abor Lugosi, and Olivier Bousquet. Concentration inequalities.

In Advanced Lectures on Machine Learning. 2004.

16

FALKON: An Optimal Large Scale Kernel Method
Supplementary Materials

A. FALKON: General Algorithm

where a generalized version of FALKON able to deal with both invertible and non-
invertible KM M is provided.

B. Deﬁnitions and Notation

where the notation, required by the proofs, is given and the basic operators are
deﬁned.

C. Analytic Decompositions

where the condition number of the FALKON system is controlled and the excess risk
of FALKON is decomposed in terms of functional analytic quantities.

D. Probabilistic Estimates

where the quantities of the previous section are bounded in probability.

E. Proof of the Main Results

where the results of the previous sections are collected and the proofs of the main
theorems of the paper are provided

F. Longer Comparison with the Literature

where some more details on previous works on the topic are given.

G. MATLAB Code for FALKON

where a minimal working implementation of FALKON is provided.

A FALKON: General Algorithm

In this section we deﬁne a generalized version of FALKON. In particular we provide a precondi-
tioner able to deal with non invertible KMM and with Nystr¨om centers selected by using approx-
imate leverage scores. In Def. 3 we state the properties that such preconditioner must satisfy. In
Example 1 we show that the preconditioner in Sect. 3 satisﬁes Def. 3 when KMM .
First we recall some ways to sample Nystr¨om centers, from the training set.
Nystr¨om with uniform sampling. Let n, M

N with 1

M

n. Let x1, . . . , xn be the
≤
xM are a random subset of cardinality M uniformly

≤

∈

x1, . . . ,

training set. The Nystr¨om centers
sampled from the training set.

Nystr¨om with approximate leverage scores. We recall the deﬁnition of approximate
N, λ > 0. Let
n.
i, j

leverage scores, from [13] and then the sampling method based on them. Let n
x1, . . . , xn be the training points and deﬁne Knn ∈
The exact leverage scores are deﬁned by

n as (Knn)ij = K(xi, xj ) for 1

Rn

≤

≤

∈

e

e

×

for any i
following deﬁnition is denoted as approximate leverage scores.

1, . . . , n. Any bi-lipschitz approximation of the exact leverage scores, satisfying the

∈

(cid:0)

lλ(i) =

Knn(Knn + λnI)−

1

ii ,
(cid:1)

17

Deﬁnition 1 (Nystr¨om with (q, λ0, δ)-approximate leverage scores [13]). Let δ
). A (random) sequence (
and q
when the following holds with probability at least 1

(0, 1] and λ0 > 0
i=1 is denoted as (q, λ0, δ)-approximate leverage scores

lλ(i))n

[1,

∞

∈

∈

δ

1
q

lλ(i)

lλ(i)

q lλ(i),

≤

λ

∀

≥

λ0, t

∈ {

1, . . . , n

.

}

b

≤

−

scores (

In particular, given n

lλ(i))n

N training points x1, . . . , xn, and a sequence of approximate leverage
, with

i=1, the Nystr¨om centers are selected in the following way. Let pi = blλ(i)
with probability (pi)n

1, . . . , n

j=1 blλ(j)

i=1. Then

n. Let i1, . . . , iM be independently sampled from
b

Pn

∈

b

{

}

i

1
≤
x1 := xi1 , . . . ,

≤

xM := xiM .

Now we deﬁne a diagonal matrix depending on the used sampling scheme that will be needed

for the general preconditioner.
e
e

RM

Deﬁnition 2. Let D
uniform sampling, then Djj = 1, for 1
Otherwhise, let i1, . . . , iM ∈ {
leverage scores. Then for 1
≤

1, . . . , n
}
M ,
j

≤

∈

×

M be a diagonal matrix.

If the Nystr¨om centers are selected via

j

M .

≤
be the indexes of the training points sampled via approximate

≤

Djj =

1
npij

.

s

We note here that by deﬁnition D is a diagonal matrix with strictly positive and ﬁnite diagonal.
Indeed it is true in the uniform case. In the leverage scores case, let 1
M . Note that since
the index ij has been sampled, it implies that the probability pij is strictly larger than zero. Then,
since 0 < pij ≤

1 then 0 < Djj <

a.s. .

∞

≤

≤

j

A.1 Generalized FALKON Algorithm

We now introduce some matrices needed for the deﬁnition of a generalized version of FALKON,
able to deal with non invertible KMM and with diﬀerent sampling schemes, for the Nystr¨om centers.
Finally in Def. 4, we deﬁne a general form of the algorithm, that will be used in the rest of the
appendix.

×

M with (KMM )ij = K(

Deﬁnition 3 (The generalized preconditioner). Let M
RM
≤
strictly positive diagonal, deﬁned according to Def 2.
RM

xj), for 1

M be the rank of KMM , Q
e

xi,

i, j

≤

e

×

∈

M . Let D

N. Let
RM

∈

x1, . . . ,
×
e

q a triangular matrix. Moreover Q, T satisfy the following equation

Let λ > 0, q
Rq

×

≤

T

∈

X and KMM ∈
M be a diagonal matrix with

xM ∈

e
q a partial isometry such that Q⊤Q = I and

Finally let A

Rq

×

q be a triangular matrix such that

∈

Then the generalized preconditioner is deﬁned as

DKMM D = QT ⊤T Q⊤.

A⊤A =

T T ⊤ + λI.

B =

DQT −

1A−

1.

1
√n

∈

1
M

18

Note that B is right invertible, indeed D is invertible, since is a diagonal matrix, with strictly
positive diagonal, T, A are invertible since they are square and full rank and Q is a partial isometry,

1 and BB−

1 = √nAT Q⊤D−

1 = I. Now we provide two ways to compute Q, T, A. We recall
so B−
M ,
that the Cholesky algorithm, denoted by chol, given a square positive deﬁnite matrix, B
RM
M such that B = R⊤R. While the pivoted (or rank
produces an upper triangular matrix R
revealing) QR decomposition, denoted by qr, given a square matrix B, with rank q, produces a
M
partial isometry Q
such that B = QR.

q with the same range of M and an upper trapezoidal matrix R

RM

RM

Rq

∈

∈

∈

∈

×

×

×

×

Example 1 (precoditioner satisfying Def. 3). Let λ > 0, and KMM , D as in Def. 3.

1. When KMM is full rank (q = M ), then the following Q, T, A satisfy Def. 3

Q = I,

T = chol(DKMM D), A = chol

T T ⊤ + λI

.

1
M

(cid:18)

2. When KMM is of any rank (q

M ), then the following Q, T, A satisfy Def. 3

≤

(Q, R) = qr(DKMM D),

T = chol(Q⊤DKMM DQ), A = chol

T T ⊤ + λI

.

(cid:19)

1
M

(cid:18)

(cid:19)

Proof. In the ﬁrst case, Q, T, A satisfy Def. 3 by construction.
In the second case, since QQ⊤
is the projection matrix on the range of DKMM D, then QQ⊤DKMM D = DKMM D and, since
DKMM D is symmetric, DKMM DQQ⊤ = DKMM D, so

QT ⊤T Q⊤ = QQ⊤DKMM DQQ⊤ = DKMM D.

Moreover note that, since the rank of KMM is q, then the range of DKMM D is q, and so Q⊤Q = I,
since it is a partial isometry with dimension RM
×

q. Finally A satisﬁes Def. 3 by construction.

Instead of rank-revealing QR decomposition, eigen-decomposition can be used.

Example 2 (preconditioner for the deﬁcient rank case, using eig instead of qr). Let λ > 0,
M be respectively the eigenvalues and the associ-
and KMM , D as in Def. 3. Let (λi, ui)1
ated eigenvectors from the eigendecomposition of DKMM D, with λ1 ≥ · · · ≥
0. So
λM ≥
the following Q, T, A satisfy Def. 3, Q = (u1, . . . , uq) and T = diag(√λ1, . . . ,
λq), while A =

i
≤

≤

diag

λ + 1

M λ1, . . . ,

λ + 1

M λq

.

p

(cid:16)q
We recall that this approach to compute Q, T, A is conceptually simpler than the one with QR
decomposition, but slower, since the hidden constants in the eigendecomposition are larger than
the one of QR.

q

(cid:17)

The following is the general form of the algorithm.

Deﬁnition 4 (Generalized FALKON algorithm). Let λ > 0, t
The generalized FALKON estimator is deﬁned as follows

∈

N and q, Q, T, A as in Def. 3.

fλ,M,t(x) =

αiK(x, ˜xi), with α = Bβt,

M

i=1
X

and βt ∈
applied to the following linear system

b

Rq denotes the vector resulting from t iterations of the conjugate gradient algorithm

W β = b, where W = B⊤(K ⊤nM KnM + λnKMM )B,

b = B⊤K ⊤nM

y.

(14)

b

19

B Deﬁnitions and Notation

Here we recall some basic facts on linear operators and give some notation that will be used in
the rest of the appendix, then we deﬁne the necessary operators to deal with the excess risk of
FALKON via functional analytic tools.

Notation Let
the associated inner product. We denote with
A, deﬁned as
= sup
k
particular

H=1k
k

A
k

Af

H

k

k

f

be an Hilbert space, we denote with

, the associated norm and with

,

k·kH

·iH
the operator norm for a bounded linear operator
the tensor product, in

. Moreover we will denote with

k·k

h·

⊗

(u

v)z = u

⊗

v, z
h

iH

,

u, v, z

∀

.

∈ H

is separable, we denote with Tr the trace, that is Tr(A) =

R,
In the rest of the appendix A + λI is often denoted by Aλ where A is linear operator and λ
moreover we denote with A∗ the adjoint of the linear operator A, we will use A⊤ if A is a matrix.
for any
When
linear operator A :
is the dimensionality of

D
j=1 h
H
∪ {∞}
k·kHS the Hilbert-Schmidt norm, that is
2
HS = Tr(A∗A), for a linear operator A.
A
k
In the next proposition we recall the spectral theorem for compact self-adjoint operators on a

j=1 is an orthogonal basis for

ui, AuiiH
N
∈

. Moreover we denote with

H → H
H

, where (ui)D

and D

P

H

∈

k

Hilbert space.

Proposition 1 (Spectral Theorem for compact self-adjoint operators). Let A be a compact self-
R,
adjoint operator on a separable Hilbert space
and an orthogonal basis of

. Then there exists a sequence (λj )D
is the dimensionality of

H
j=1 where D

(uj)D

N

j=1 with λj ∈
, such that
H

∈

∪ {∞}

H

A =

λj uj ⊗

uj.

D

j=1
X

(15)

Proof. Thm. VI.16, pag. 203 of [40].

Let

H

be a separable Hilbert space (for the sake of simplicity assume D =

), and A be a
that admits a spectral decomposition as in Eq. 15. Then the

∞

bounded self-adjoint operator on
largest and the smallest eigenvalues of A are denoted by

H

λmax(A) = sup
1
≥

j

λj ,

λmin(A) = inf
1
≥

j

λj .

In the next proposition we recall a basic fact about bounded symmetric linear operators on a
separable Hilbert space

.

H

, that admits a spectral decomposition

Proposition 2. Let A be a bounded self-adjoint operator on
as in Eq. 15. Then

H

A

λmin(A)

λmax(A)

≤
2
Proof. By deﬁnition of operator norm, we have that
H ≤ k
be an eigendecomposition of A, with D the dimensionality of
any j

1, we have

Ax
k

k ≤

−k

k

≤ k

.

A
k
2

≥

where we used the fact that Auj = λjuj and that

λ2
j =

Auj, Auji
h

=

k

Aujk
ujkH
k

2,

A
k

2
H ≤ k
= 1.

x
k

2
H ∀

A
j=1
k
, according to Prop. 1, then, for
H

. Let (λj, uj)D

∈ H

x

k

20

B.1 Deﬁnitions

Let X be a measurable and separable space and
We denote with ρX the marginal probability of ρ on X and with ρ(y
measure on
endowed with the inner product

R.
x) the conditional probability
given X. Let L2(X, ρX ) be the Lebesgue space of ρX square integrable functions,

= R. Let ρ be a probability measure on X

×

Y

Y

|

φ, ψ

iρ =

h

Z

φ(x)ψ(x)dρX (x),

φ, ψ

∀

∈

L2(X, ρX ),

k

ψ

ψ, ψ

kρ =
and norm
iρ for any ψ
∈
X
space of functions. Let K : X
→
×
), for which K(x, x)
(0,
bounded, i.e. there exists κ
the function K(x,
,
,
product induced by K, deﬁned by

L2(X, ρX). We now introduce the kernel and its associated
R be a positive deﬁnite kernel, measurable and uniformly
κ2 almost surely. We denote with Kx
), the Hilbert space of functions with the associated inner

∈
) and with (

h
p

∞
h·

·iH

H

≤

·

= span
{

Kx |

x

X

,
}

∈

H

Kx, Kx′
h

iH

= K(x, x′),

x, x′

X.

∀

∈

Now we deﬁne the linear operators used in the rest of the appendix

Deﬁnition 5. Under the assumptions above, for any f

S :

H →

S∗ : L2(X, ρX )
L : L2(X, ρX )

→ H

→

L2(X, ρX ), such that Sf =

f, K(

)

(cid:10)
, such that S∗φ =

L2(X, ρX ), such that L = SS∗ and

R

·

H ∈
(cid:11)
φ(x)KxdρX (x)

.

∈ H

, φ

L2(X, ρX)

∈

∈ H
L2(X, ρX ), with adjoint

, such that C = S∗S.

H → H

C :

•
Let xi ∈

i
≤
following linear operators

X with 1

≤

∈

xj ∈

e

Deﬁnition 6. Under the assumptions above, for any f

Sn :

H →

S∗n : Rn
b

Rn, such that

Snf = 1

, such that

S∗nv = 1
b
√n

→ H

√n (
h

)n
f, Kxii
i=1 ∈
i=1 viKxi ∈ H

n

.

, v

Rn, w

RM ,

∈
∈
∈ H
Rn, with adjoint

n and n

N, and

X for 1

j

M and M

N. We deﬁne the

≤

≤

∈

Cn :
b

H → H

SM :
b
H →
S∗M : RM
b

P

Sn.

, such that

S∗n
Cn =
b
RM , such that
SM f = 1
b
b
√M
S∗M w = 1
b
√M

, such that

b

→ H

CM :
b

GM :
b

H → H

H → H

, such that

CM =
b

, such that

GM =
b

P

SM .

S∗M
S∗M D2
b
b

RM , with adjoint

(
h

)M
i=1 ∈

f, K˜xii
i=1 viK˜xi ∈ H

M

.

We now recall some basic facts about L, C, S, Knn,

Cn,

Sn, KnM and KMM .

b

b

b

b

SM , with D deﬁned in Def. 3 (see also Def. 2).

•

•

•

•

•

•

•

•

•

•

b

b

21

Proposition 3. With the notation introduced above,

1. KnM = √nM

Sn

S∗M ,

KMM = M

SM

S∗M ,

Knn = n

Sn

S∗n

C =

KxdρX (x),
Kx ⊗
b
b

Tr(C) = Tr(L) =
b

b

2
HS =

S

k

k

dρX (x)

2

H

κ2,

≤

Tr(

Cn) = Tr(Knn/n) =

Kxik

k

2
H ≤

κ2,

Tr(

CM ) = Tr(KMM /M ) =

2
HS =

1
M

K˜xik

k

2
H ≤

κ2,

b

b

Kxk
b
b
ZX k
1
2
HS =
n

n

i=1
X
m

i=1
X

k

Snk
b
SM k
b

k

2.

3.

4.

5.

n

ZX
1
n

i=1
X
M

1
M

1
M

i=1
X
M

i=1
X

Cn =

b
CM =

b
GM =

b
⊗

Kxi ⊗

Kxi,

K˜xi ⊗

K˜xi,

D2
iiK˜xi ⊗

K˜xi.

where

denotes the tensor product.

j

≤

≤

Proof. Note that (KnM )ij = K(xi, ˜xj) =
M , thus KnM = √nM
Sn
1
second equation, by deﬁnition of C = S∗S we have that, for each h, h′
b
b
h, KxiH h

Sh, Sh′
h

h, Ch′
h

dρX (x) =

Kxi, K˜xj

iρ =

Kx, h′

Sn

iH

iH

h,

=

b

H

(cid:11)

(cid:10)

b
∈ H

,

n,
S∗M . The same reasoning holds for KMM and Knn. For the

S∗M )ij , for any 1

= (√nM

≤

≤

i

dρX (x)

(cid:17)EH

ZX h
Kx

=

h,

Kx ⊗

ZX

D

(cid:16)

(cid:17)

EH

h′

dρX (x) =

h,

Note that, since K is bounded almost surely, then

Kx h
(cid:16)
D
KxdρX (x)

ZX
Kx ⊗

κ for any x

∈

Kx, h′

iH

h′

.

(cid:29)H
(cid:17)
X, thus

(cid:28)

(cid:16) ZX
KxkH ≤

k

Tr(C) =

Tr(Kx ⊗

Kx)dρX (x) =

ZX

Kxk

2

H

dρX (x)

κ2

≤

ZX k

by linearity of the trace. Thus Tr(C) <

and so

∞

Tr(C) = Tr(S∗S) =

2
HS = Tr(SS∗) = Tr(L).

S

k

k

The proof for the rest of equations is analogous to the one for the second.

Now we recall a standard characterization of the excess risk

Proposition 4. When

y2dρ <

, then there exist fρ ∈

∞

L2(X, ρX ) deﬁned by

Y
R

almost everywhere. Moreover, for any ˆf

we have,

fρ(x) =

ydρ(y

x),

Z

∈ H
inf
f
∈H

E

f )
(

E

−

b

(f ) =

S

f

k

P fρk

−

2
ρX ,

|

b

where P : L2(X, ρX )
of the range of S.

→

Proof. Page 890 of [41].

L2(X, ρX ) is the projection operator whose range is the closure in L2(X, ρX )

22

C Analytic results

The section of analytic results is divided in two subsections, where we bound the condition number
of the FALKON preconditioned linear system (14) and we decompose the excess risk of FALKON,
with respect to analytical quantities that will be controlled in probability in the following sections.

C.1 Analytic results (I): Controlling condition number of W

First we characterize the matrix W deﬁning the FALKON preconditioned linear system (14), with
respect to the operators deﬁned in Def. 6 (see next lemma) and in particular we characterize its
condition number with respect to the norm of an auxiliary operator deﬁned in Lemma 2. Finally
we bound the norm of such operator with respect to analytical quantities more amenable to be
bounded in probability (Lemma 3).

Lemma 1 (Characterization of W ). Let λ

R. The matrix W in Def. 4 is characterized by

∈

W = A−⊤V ∗(

Cn + λI)V A−

1, with V = √nM

S∗M BA.

Moreover V is a partial isometry such that V ∗V = Iq

b

q and V V ∗ with the same range of
b

S∗M .

×

Proof. By the characterization of KnM , KMM and

Cn in Prop. 3, we have

b

K ⊤nM KnM + λKMM = nM (

S∗M )
S∗M = nM
b
Now note that, by deﬁnition of B in Def. 3 and of V , we have

b
Sn + λI)
b
b

S∗n
S∗n
b

S∗M + λ

SM (
b

= nM

SM

SM

Sn

b

SM (

Cn + λI)

S∗M .

b

b
b
S∗M B = √nM

√nM

S∗M BAA−

1 = V A−

1,

b

b

b

b

so

b

b

W = B⊤(K ⊤nM KnM + λKMM )B = nM B⊤

SM (

Cn + λI)

S∗M B = A−⊤V ∗(

Cn + λI)V A−

1.

The last step is to prove that V is a partial isometry. First we need a characterization of V that
b
is obtained by expanding the deﬁnition of B,

b

b

b

V = √nM

S∗M BA = √nM

S∗M

DQT −

1A−

1A = √M

S∗M DQT −

1.

(16)

1
√n

By the characterization of V , the characterization of KMM in Prop. 3 and the deﬁnition of Q, T
in terms of DKMM D in Def. 3 , we have

b

b

b

V ∗V = M T −⊤Q⊤D

SM

S∗M DQT −

1 = T −⊤Q⊤ DKMM D QT −

1 = T −⊤Q⊤QT ⊤T Q⊤QT −

1 = I.

Moreover, by the characterization of V , of DKMM D with respect to
Def. 3),

b

b

SM , and of Q, T (Prop. 3 and

V V ∗

S∗M D = M

S∗M DQT −

1T −⊤Q⊤D

SM

S∗M =

=

S∗M DQT −
b

1T −⊤Q⊤QT ⊤T Q⊤ =

b

b
where the last step is due to the fact that the range of QQ⊤ is the one of DKMM D by deﬁnition
SM . Note ﬁnally
(see Def. 3), and since DKMM D = M D
SM
S∗M since D is a diagonal matrix with strictly positive
S∗M D is the same of
that the range of
b
elements on the diagonal (see Def. 3).
b
b

S∗M D by Prop. 3, it is the same of D

b

b

b

b

b

b

b

b
1T −⊤Q⊤DKMM D

S∗M DQT −
S∗M DQQ⊤ =

S∗M D,

23

Lemma 2. Let λ > 0 and W be as in Eq. 14. Let E = A−⊤V ∗(
in Lemma 1. Then W is characterized by

GM )V A−

1, with V deﬁned

Cn −
b

b

In particular, when

< 1,

E

k

k

W = I + E.

cond (W )

1 +
1

k
− k

E
E

k
k

.

≤

Proof. Let Q, T, A, D as in Def. 3, and V as in Lemma 1. According to Lemma 1 we have

W = A−⊤V ∗(

Cn + λI)V A−

1 = A−⊤(V ∗

CnV + λI)A−

1.

Now we bound the largest and the smallest eigenvalue of W . First of all note that

b

b

A−⊤(V ∗

CnV + λI)A−

1 = A−⊤(V ∗

GM V + λI)A−

1 + A−⊤V ∗(

GM )V A−

1,

(17)

b

GM is deﬁned in Def. 6. To study the ﬁrst term, we need a preliminary result, which simpliﬁes
where
b
SM (Prop. 3), the
SM V . By using the deﬁnition of V , the characterization of KMM in terms of
deﬁnition of B (Def. 3), and ﬁnally the characterization of DKMM D in terms of Q, T (Def. 3), we
have
b

b

b

b

Cn −
b

D

SM V = √nM D

SM

S∗M BA =

DKMM BA =

DKMM D QT −

1

b

=

1
√M

b
QT ⊤T Q⊤QT −

b

QT ⊤.

n
M
1
√M

r
1 =

1
√M

Now we can simplify the ﬁrst term. We express
above on D
Def. 3, obtaining

b

SM V and on its transpose, ﬁnally we recall the identity A⊤A = 1

GM with respect to

SM , then we apply the identity
M T T ⊤ + λI from

A−⊤(V ∗

GM V + λI)A−

1 = A−⊤(V ∗

S∗M D2

SM V + λI)A−

1 = A−⊤(

T Q⊤QT ⊤ + λI)A−

(18)

1

= A−⊤(

T T ⊤ + λI)A−
b
b

1 = A−⊤A⊤AA−

(19)

b

1
M
1 = I.

b

b

So, by deﬁning E := A−⊤V ∗(

GM )V A−

1, we have

1
M

Cn −
b

b

W = I + E.

Note that E is compact and self-adjoint, by deﬁnition. Then, by Prop. 1, 2 we have that W admits
a spectral decomposition as in Eq. 15. Let λmax(W ) and λmin(W ) be respectively the largest and
the smallest eigenvalues of W , by Prop. 2, and considering that
(see Prop. 1)
we have

λj(E)

≤ k

k ≤

−k

E

E

k

λmax(W ) = sup
N

∈
λmin(W ) = inf
N
∈

j

j

1 + λj(E) = 1 + sup
N

∈
1 + λj (E) = 1 + inf
N
∈

j

j

λj(E) = 1 + λmax(E)

1 +

E

,

λj (E) = 1 + λmin(E)

1

≤

k

E

k

.

≥

− k

k

Since W is self-adjoint and positive, when

< 1, by deﬁnition of condition number, we have

cond (W ) =

λmax(W )
λmin(W ) ≤

1 +
1

k
− k

E
E

k
k

.

E

k

k

24

Lemma 3. Let E be deﬁned as in Lemma 2 and let

GM as in Def. 6, then

Proof. By multiplying and dividing by

GMλ =

b

E

k

k ≤ k

G−

1/2
Mλ (

G−

GM )
b

1/2
Mλ k

Cn −
GM + λI we have
b

b

b

.

=

A−⊤V ∗

E

k

k

A−⊤V ∗

k

≤ k

G−

G1/2
Mλ
G1/2
b

Mλk

b
2
b
k

1/2
Cn −
Mλ (
b
1/2
Cn −
Mλ (
G−
b
b
b

b

b

GM )

G−

Mλ

GM )
b

G−

b

b

1/2

G1/2

MλV A−
1/2
.
Mλ k
b

1

k

Now, considering that V ∗V = I and the identity in Eq. (18), we have

(20)

A−⊤V ∗

k

G1/2

Mλk

2 =

k

A−⊤V ∗(

GM + λI)V A−

=

A−⊤(V ∗

GM V + λI)A−

= 1.

(21)

1

k

k

1

k

b

b

b

C.2 Analytic results (II): The computational oracle inequality

In this subsection (Lemma 8) we bound the excess risk of FALKON with respect to the one of the
exact Nystr¨om estimator. First we prove that FALKON is equal to the exact Nystr¨om estimator
as the iterations go to inﬁnity (Lemma 4, 5). Then in Lemma 8 (via Lemma 6, 7) we use functional
analytic tools, together with results from operator theory to relate the weak convergence result of
the conjugate gradient method on the chosen preconditioned problem, with the excess risk.

Lemma 4 (Representation of the FALKON estimator as vector in
as in Def. 3. The FALKON estimator as in Def. 4 is characterized by the vector

). Let λ > 0, M, t

H

f

N and B
∈
as follows,

fλ,M,t = √M

S∗M Bβt,

∈ H

b

(22)

where βt ∈
b
applied to the linear system in Def. 4.

Rq denotes the vector resulting from t iterations of the conjugate gradient algorithm
b

Proof. According to the deﬁnition of
Def. 6, denoting with α

fλ,M,t(
·
RM the vector Bβt, we have that

) in Def.4 and the deﬁnition of the operator

SM in

∈

M

i=1
X

b

*

M

i=1
X

fλ,M,t(x) =

αiK(x, ˜xi) =

Kx,

αiK˜xi

=

Kx, √M

S∗M α

,

for any x

X. Then the vector in

representing the function

H

b

∈

+

H

D

EH

b

fλ,M,t(
·

) is

b

fλ,M,t = √M

S∗M α = √M

S∗M Bβt.
b

b

b

b

N, and
Lemma 5 (Representation of the Nystr¨om estimator as a vector in
B as in Def. 3. The exact Nystr¨om estimator, in Eq.(7) and Eq. (8) is characterized by the vector
f

). Let λ > 0, M

as follows

H

∈

fλ,M = √M

S∗M Bβ

,

∞

(23)

where β
algorithm applied to the linear system in Eq. (14).

= W −

1B⊤K ⊤nM

∞

e

b

y is the vector resulting from inﬁnite iterations of the conjugate gradient

∈ H

e

b

25

Proof. For the same reasoning in the proof of Lemma 4, we have that the FALKON estimator with
inﬁnite iterations is characterized by the following vector in

fλ,M = √M

S∗M Bβ

.

H

∞
= W −

To complete the proof, we need to prove 1) that β
b
∞
corresponds to the exact Nystr¨om estimator, as in Eq. (8).

e

1B⊤KnM

y and 2) that

fλ,M above,

Now we characterize β

. First, by the characterization of W in Lemma 1 and the fact that

e

b

V ∗V = I, we have

∞

W = A−⊤V ∗(

Cn + λI)V A−

1 = A−⊤(V ∗

CnV + λI)A−

1.

(24)

Since
Cn is a positive operator (see Def. 6) A is invertible and λ > 0, then W is a symmetric and
positive deﬁnite matrix. The positive deﬁniteness of W implies that it is invertible and that is has
a ﬁnite condition number, making the conjugate gradient algorithm to converge to the solution of
, by
the system in Eq. (14) (Thm. 6.6 of [25] and Eq. 6.107). So we can explicitly characterize β
the solution of the system in Eq. (14), that is

∞

b

b

b

= W −

1B⊤K ⊤nM

y.

β

∞

(25)

(26)

(27)

(28)

(29)

(30)

, with the above characterization of β

, corresponds to FALKON
fλ,M ∈ H
So we proved that
fλ,M is equal to the Nystr¨om estimator given in [13].
with inﬁnite iterations. Now we show that
e
First we need to study
SM . By the characterization of W in Eq. (24), the identity
S∗M BW −
1, valid for any A, B, C bounded invertible operators, and the deﬁnition
1A−
1B−
1 = C−
(ABC)−
of V (Lemma 1),
b

1B⊤

∞

b

e

b

S∗M BW −

1B⊤

SM =

S∗M B

A−⊤(V ∗

CnV + λI)A−

B⊤

SM

1

1

−

b

(cid:16)
S∗M BA(V ∗
b

CnV + λI)−
b

(cid:17)
1A⊤B⊤
SM

b

V (V ∗

CnV + λI)−
b

1V ∗.

b

=

=

1
b
M n

b

∞

By expanding β

, KnM (see Lemma 3) in

fλ,M ,
b

fλ,M = √M

S∗M Bβ

= √M

∞

S∗M BW −
e

1B⊤K ⊤nM

y = √nM

S∗M BW −

1B⊤

SM

S∗n

y

e

=

1
√n

V (V ∗
b

1V ∗
CnV + λI)−
b

S∗n

y.

b

b

b

b

b

Now by Lemma 2 of [13] with Zm =
by the vector ¯f

b

∈ H

deﬁned as follows
b
1
√n

¯f =

¯V ( ¯V ∗

Cn ¯V + λI)−

1 ¯V ∗

S∗n

y,

SM , we know that the exact Nystr¨om solution is characterized

b

b

with ¯V a partial isometry, such that ¯V ∗ ¯V = I and ¯V ¯V ∗ with the same range of
S∗M . Note that, by
deﬁnition of V in Lemma 1, we have that it is a partial isometry such that V ∗V = I and V V ∗ with
S∗M . This implies that ¯V = V G, for an orthogonal matrix G
q. Finally,
the same range of
b
1 = G⊤, that GG⊤ = G⊤G = I and that for three invertible matrices
exploiting the fact that G−
A, B, C we have (ABC)−

1 = C−

1B−

1A−

Rq

1,

∈

b

b

b

b

×

¯f =

¯V ( ¯V ∗

Cn ¯V + λI)−

1 ¯V ∗

S∗n

y =

V G

G⊤(V ∗

CnV + λI)G

G⊤V ∗

S∗n

y

=

b
V GG⊤

V ∗

CnV + λI

GG⊤V ∗

S∗n

V ∗

CnV + λI

V ∗

b
S∗n

y =
b

fλ,M .

1
√n
1
√n

(cid:16)

b

1
√n

b
−

1
b

(cid:17)

1

−

(cid:17)

1

−

(cid:17)

b

b

e

(cid:16)
y =

1
√n

b
V

(cid:16)

b

b

b

26

The next lemma is necessary to prove Lemma 8.

Lemma 6. When λ > 0 and B is as in Def. 3. then

b

1
n k

k

1/2

2.

k

Proof. By the fact that identity
in Eq. 26, we have

2 =
b

k

k

k

ZZ ∗

k

√M

S

S∗M BW −

1/2

1/2

n−

S

C−

1/2
nλ k

.

k
Z

k ≤
valid for any bounded operator Z and the identity

k

M

S

S∗M BW −

1/2

2 = M

S

S∗M BW −

1B⊤

SM S∗

=

SV (V ∗

CnV + λI)−

1V ∗S∗

k

k

b

k

k

1
n k

=

SV (V ∗
b

CnV + λI)−

b

b

Denote with

Cnλ the operator

Cn + λI, by dividing and multiplying for

b

1/2

C−
nλ

, we have

b

SV (V ∗

CnV + λI)−
b

1/2 = S

1/2

C−
nλ

C1/2

nλ V (V ∗

CnV + λI)−

b

1/2.

The second term is equal to 1, indeed, since V ∗
b
bounded operator Z, we have

b

CnλV = V ∗
b

CnV + λI, and

b

Z

2 =

k

k

k

Z ∗Z

, for any

k

C1/2

nλ V (V ∗

CnV + λI)−

1/2

2 =

(V ∗

CnV + λI)−

b

b
CnλV (V ∗

1/2V ∗
1/2(V ∗
CnV + λI)(V ∗
b

b

CnV + λI)−

1/2

k

CnV + λI)−

1/2

k

(V ∗

CnV + λI)−
b

k

k

=
k
= 1.

(31)

(32)
(33)

b

b

k

b

Finally

b

b

1
√n k
1
√n k
1/2

n−

≤

≤

√M

S

S∗M BW −

1/2

k

=

k

SV (V ∗

CnV + λI)−

1/2

k

b

C1/2

nλ V (V ∗

CnV + λI)−

1/2

k

b

S

C−

1/2
b
nλ kk
1/2
.
b
nλ k

C−

S
b

k

b

The next lemma is necessary to prove Lemma 8.

Lemma 7. For any λ > 0, let β
gradient algorithm applied to the linear system in Eq. (14). Then

∞

be the vector resulting from inﬁnite iterations of the conjugate

Proof. First we recall the characterization of β

W 1/2β

k

Rq

∞k

y

Rn .

≤ k

k
from Lemma 5,

b
∞
1B⊤K ⊤nM

y.

β

∞

= W −

So, by the characterization of KnM in terms of

Sn,

SM (Prop. 3),
b

W 1/2β

= W 1/2W −

1B⊤K ⊤nM
b

y = √nM W −

1/2B⊤

b

SM

S∗n

y.

∞

Then, by applying the characterization of

S∗M BW −
b

1B⊤

SM in terms of V , in Eq. 26

b

b

b

W 1/2β

2
Rq = nM

k

∞k

W −

k
SnV (V ∗

1/2B⊤

SM
b

y
S∗n
k
1V ∗
CnV + λI)−
b
b

b

2
Rq = nM
b

S∗n

y =

1B⊤

y⊤

Sn

S∗M BW −
CnV + λI)−
(V ∗
k
b
b

b

SM
1/2V ∗
b

y

S∗n
y
S∗n
k
b
b

2
Rq .

=

y⊤

b

b

b

b

b

27

b

b

b

Finally

Note that

indeed

(V ∗

CnV + λI)−

1/2

S∗n

y

(V ∗

CnV + λI)−

1/2

Rq

k

≤ k

k

b
CnV + λI)−
(V ∗

b

k

1,

b
1/2
S∗nk ≤
b
CnV + λI)−

1/2

C1/2

(V ∗

CnV + λI)−

1/2

nλ kk
and the ﬁrst term is equal to 1 by Eq. (31), moreover by deﬁnition of

k

(V ∗

b
S∗nk ≤ k
b

b

b

b

b

y

Rn .

k

S∗nkk
b

b

1/2

,

C−
nλ

S∗nk
Cn (Def. 6),
b
b
σ

1,

σ + λ ≤

k

k

k

=

1/2

1/2

1/2

1/2
nλ k

Cn

C−

Cn)

2 =

C−
nλ

C−
nλ

C−
nλ

b
[0,

where σ(

C1/2
n k

b
b
] is the set of eigenvalues of

S∗nk
b
Cnk
N, λ > 0 and B satisfying Def. 4. Let
b
∈
N iterations and
fλ,M the exact Nystr¨om estimator as in Eq. 7, 8. Let c0 ≥
e

2 = sup
b
σ( bCn)
σ

Lemma 8. Let M
b
t

Cn.

C−

c0,

⊂

∈

S

b

b

b

b

b

k

∈

1/2
nλ k ≤

k

then

fλ,M,t)1/2
(
R

b
fλ,M )1/2 + 2c0
(
≤ R

where

v2 = 1
n

b
n
i=1 y2
i .

e

2

cond(W) + 1 !

t

,

1
 

−

v

b

p

Proof of Lemma 8. By Prop. 4 we have that for any f

P

b

(f )

(
E

−

inf
f
∈H

E

(f ))1/2 =

∈ H
Sf

k

P fρkρX ,

−

fλ,M,t be the FALKON estimator after

0 such that

L2(X, ρX ) the orthogonal projection operator whose range is the closure
with P : L2(X, ρX )
of the range of S in L2(X, ρX ). Let
be respectively the Hilbert vector
fλ,M,t ∈ H
representation of the FALKON estimator and of the exact Nystr¨om estimator (Lemma 4 and
b
Lemma 5). By adding and subtracting

fλ,M ∈ H
e

fλ,M we have

and

→

f )
(

|E

inf
f
∈H

(f )
|

E

−

1/2 =

k

b

S

P fρkρX =
fλ,M,t −
e
fλ,M )
fλ,M,t −
S(
b
fλ,M,t −
b
b

k
kρX +
kρX +

fλ,M )
e

S(

≤ k
=

k

fλ,M ) + (S

S(

fλ,M,t −
P fρkρX
fλ,M −
S
e
b
inf
fλ,M )
(
e
E
−
f
∈H

k

|E

(f )
|

fλ,M −
e
1/2.

P fρ)

kρX

e
In particular, by expanding the deﬁnition of

e

fλ,M,t,

fλ,M from Lemma 4 and Lemma 5, we have

k

S(

kρX = √M
b

fλ,M )

Rq and β

fλ,M,t −
S∗M B(βt −
Rq denote respectively the vector resulting from t iterations and inﬁnite
where βt ∈
b
b
iterations of the conjugate gradient algorithm applied to the linear system in Eq. (14). Since
W is symmetric positive deﬁnite when λ > 0 (see proof of Lemma 5), we can apply the standard
convergence results for the conjugate gradient algorithm (Thm. 6.6 of [25], in particular Eq. 6.107),
that is the following

kρX ,
)

S
e
k

∞ ∈

β

∞

e

W 1/2(βt −

k

β

Rq

)
k

∞

q(W, t)
k

≤

W 1/2β

∞k

Rq , with q(W, t) = 2

1

2

t

.

−

 

cond(W) + 1 !

p

28

1/2W 1/2(βt −

β

)
kρX

∞

S(

So by dividing and multiplying by W 1/2 we have
kρX = √M
√M
S
k
≤
q(W, t) √M

S∗M B(βt −
S∗M BW −
b

fλ,M,t −
b

fλ,M )

1/2

S

S

β

∞

e

k

k

kk
S∗M BW −

S

kρX = √M
)
k
W 1/2(βt −
β
∞
W 1/2β
kk

1/2

S∗M BW −
)
k
b

Rq

Rq .

∞k

Finally, the term √M

S

is bounded in Lemma 6 as

b

≤
S∗M BW −

b
1/2

k

k

k

b

√M

S

S∗M BW −

1/2

k

1
√n k

S

C−

1/2
nλ k ≤

c0
√n

,

k ≤

while, for the term

W 1/2β

b

Rq , by Lemma 7, we have

b

k

∞k

W 1/2β

k

Rq

∞k

≤ k

k

y

Rn = (

i )1/2 = √n
y2

i=1
X

b

n
i=1 y2
i
n

r P

= √n

v.

b

D Probabilistic Estimates

In Lemma 9, 10 we provide probabilistic estimates of
, the quantity needed to bound the
condition number of the preconditioned linear system of FALKON (see Lemma 1, 3). In particular
Lemma 9, analyzes the case when the Nystr¨om centers are selected with uniform sampling, while
Lemma 10, considers the case when the Nystr¨om centers are selected via approximate leverage
scores sampling.

E

k

k

Now we are ready to provide probabilistic estimates for uniform sampling.

Lemma 9. Let η
sampling (see Sect. A), 0 < λ

[0, 1) and δ

∈

(0, 1]. When ˜x1, . . . , ˜xM are selected via Nystr¨om uniform
, M

n and

C

∈
k

≤ k

M

4

≥

1
2

"

+

+

1
η

(cid:18)

≤

3 + 7η
3 + 3η

(cid:19) (cid:18)

−

1 +

2

2
η

(cid:19)

(λ)

log

N∞

#

8κ2
λδ

,

(34)

then the following hold with probability at least 1

δ,

1/2

(C

C−
λ

1/2

Cn)C−
λ

< η,

G−

1/2
Mλ (

GM )

G−

1/2
Mλ k

< η.

k

k
Proof. First of all, note that since the Nystr¨om centers are selected by uniform sampling. Then
˜x1, . . . , ˜xM are independently and identically distributed according to ρX and moreover D is the
identity matrix. So

−

b

b

b

b

k

Cn −
b

SM =

k

G−

GM =

1/2
Mλ (

S∗M D2
Note that, by multiplying and dividing by Cλ,
b
b
b
1/2
CM )
Cn −
Mλ (
C−
1/2
Mλ C1/2
1/2
λ C−
C−
λ
b
b
b
Mλ C1/2
2
C−
C−
λ
λ k
b
≤ k
λmax(C−
(1
λ
b
−

Cn −
b

1/2
Mλ k

GM )

k
1/2

G−

1/2

=

=

≤

b

b

b

k

k

(C

S∗M

SM =

CM .

b

b
b
1/2
C−
Mλ k
Cn −
(
b
1/2
Cn −
(
b
b
CM )C−
λ
b
b

−

CM )C−
λ

1/2

C1/2
λ
1/2

C−

1/2
Mλ k

CM )C−
λ
1/2

1

))−

b
k
1/2
C−
λ

k

where the last step is due to Prop. 9 of [14]. Moreover note that

b

CM )C−
λ

1/2

k

Cn −
(
b

b

λmax(C−
λ

1/2

(C

CM )C−
λ

1/2

)

1/2

(C

C−
λ

CM )C−
λ

−

1/2

.

k

≤ k

−

b

29

b

Let µ = δ

2 . Note that

vi with vi the random variable vi = K˜xi (see Prop. 3)
and, since ˜x1, . . . , ˜xM are i.i.d. w.r.t. ρX , by the characterization of C in Prop. 3, we have for any
1

i=1 vi ⊗

M ,

i

CM = 1
M

b

M

≤

≤

P
Evi ⊗
=

vi =

Kx ⊗

KxdρX (x) = C.

ZX
κ2 a. e., we can apply Prop. 7 of [14], obtaining

Then, by considering that

v

k

k

Kxk ≤

k

1/2

(C

C−
λ

k

−

CM )C−
λ

1/2

k ≤

2d(1 +

(λ))

2d

(λ)

N∞

3M

+

r

N∞
3M

,

d = log

4κ2
λµ

,

with probability at least 1
η/(2 + η). By repeating the same reasoning for Cn, we have

µ. Note that, when M satisﬁes Eq (34), we have

C−
λ

k

b
−

1/2

(C

CM )C−
λ

1/2

−

<

k

1/2

(C

C−
λ

k

−

1/2

Cn)C−
λ

k ≤

2d(1 +

(λ))

2d

(λ)

N∞
3n

+

r

N∞
3n

,

d = log

b

4κ2
λµ

,

M and M satisfying Eq. (34), we have automatically that

with probability 1
Cn)C−
(C
λ

C−
λ

1/2

−
1/2

µ. Since n

b

≥
< η/(2 + η).

k

−

k

Finally note that, by adding and subtracting C,

b
1/2

C−
λ

k

Cn −
(
b

CM )C−
λ

1/2

=

k

1/2

1/2

k

C−
λ

C−
λ

Cn −
((
(C
b
−

C) + (C

1/2

Cn)C−
λ

−
+

CM ))C−
λ
1/2

C−
λ

b
k

1/2

k

(C

CM )C−
λ

1/2

.

k

−
So by performing the intersection bound of the two previous events, we have

≤ k

k

b

b

C−

1/2
Mλ (

k

CM )

1/2

Cn −
b
×

C−
λ
b

C−

1/2
Mλ k ≤
(C
b

−

Cn)C−

(1

− k
1
+
λ k

1/2

C−
λ

C−
λ

k

(C

1/2

−
(C

Cn)C−
λ

1/2

1

)−

k
1/2
CM )C−
λ

b
−

< η,

k
(cid:16)
with probability at least 1

b

2µ. The last step consists in substituting µ with δ/2.

b

b

−
The next lemma gives probabilistic estimates for

, that is the quantity needed to bound
the condition number of the preconditioned linear system of FALKON (see Lemma 1, 3), when the
Nystr¨om centers are selected via approximate leverage scores sampling.

E

k

k

Lemma 10. Let η > 0, δ
1 and λ0 > 0. Let x1, . . . , xn be independently
≥
and identically distributed according to ρX . Let ˜x1, . . . , ˜xM be randomly selected from x1 . . . , xn,
by using the (q, λ0, δ)-approximate leverage scores (see Def. 1 and discussion below), with λ0 ∨
67κ2 log 12κ2
19κ2
n log n
δ

(0, 1], n, M

. When n

405κ2

and

C

∈

∈

λ

N, q

2δ ≤

≤ k

k

≥

M

2 +

+

≥

(cid:20)

2
η

∨
18(η2 + 5η + 4)q2
η2

N

8κ2
λδ

,

(λ)

log

(cid:21)

(35)

b

×

k

(cid:17)

then the following hold with probability at least 1

δ,

−

Proof. By multiplying and dividing by
b
b

b

Cnλ =

Cn + λI, we have

b

G−

1/2
Mλ (

k

GM )

G−

1/2
Mλ k

< η,

1/2

(C

C−
λ

k

−

1/2

Cn)C−
λ

< η.

k

Cn −
b

G−

1/2
Mλ (

k

GM )

G−

1/2
Mλ k

b

b

b

Cn −
b

C−
nλ

1/2
b

=

G−

Mλ

k

Mλ

G−
b
≤ k
(1
b
−

≤

1/2

1/2

C1/2
b
nλ
C1/2
2
nλ k
b
b
C−
λmax(
nλ
b

C−
nλ

GM )
Cn −
(
1/2
Cn −
(
b
b
k
1/2
GM )
Cn −
(
b
b
b
30

b

b

b

C−
nλ

1/2

C−

C1/2
nλ
1/2
nλ k
b
1/2
1
))−
b

k

GM )
b
C−
nλ
b

G−

1/2
Mλ k

b
C−
nλ

1/2

GM )

C−

1/2
nλ k

Cn −
(
b

b

b

b

where the last step is due to Prop. 9 of [14]. Note that

thus

λmax(

C−
nλ

1/2

GM )

C−
nλ

1/2

)

1/2

C−
nλ

≤ k

GM )

C−

1/2
nλ k

,

b

Cn −
(
b
G−

b
1/2
Mλ (

k
1/2
b
nλ k

b
Cn −
b

GM )

G−

b
1/2
Mλ k ≤

b

b

Cn −
(
b

t

,

t

1

−

1/2

k

C−
nλ

GM )

with t =
lλ(j), respectively
the leverage scores and the (q, λ0, δ)-approximate leverage score associated to the point xj, as in
Def. 1 and discussion above. First we need some considerations on the leverage scores. By the
spectral theorem and the fact that Knn = n

. Now we bound t. We denote with lλ(j),

Cn −
(
b

S∗n (see Prop. 3), we have

C−

Sn

b

b

b

b

b

b

lλ(j) = (Knn(Knn + λnI)−

1)jj = e⊤j

S∗n + λI)−

1ej = e⊤j

Sn(

S∗n

Sn + λI)−

S∗nej

1

Kxj ,

C−

1
nλ Kxj

=

b

b

b

b

for any 1

D
n. Moreover, by the characterization of

E

b

Cn in Prop. 3, we have

1
n k

Sn

S∗n(
Sn
b
b
1/2
2.
nλ Kxj k
C−
b
b
b
b

b

lλ(j) =

Kxj , (

Cn + λ)−

1Kxj

Tr((

Cn + λ)−

1(Kxj ⊗

Kxj ))

n

=

1
b
n

j=1
X

EH

= Tr((

Cn + λ)−

(Kxj ⊗

Kxj )) = Tr(

1
C−
nλ

n

b
1 1
n

j=1
X

b
Cn).

b

b

n

1
n

j=1 D
X

b

Since the Nystr¨om points are selected by using the (q, λ0, δ)-approximate leverage scores, then
is the sequence of indexes obtained by
xt = xit for 1
approximate leverage scores sampling (see Sect. A). Note that i1, . . . , iM are independent random
indexes, distributed as follows: for 1
e

M , where i1, . . . , iM ∈ {

1, . . . , n

M ,

≤

≤

}

t

t

≤

≤

=

1
n

j

n

≤

≤
1
n

j=1
X

it = j, with probability pj =

,

1

j

∀

≤

≤

n.

lλ(j)
n
h=1
b

lλ(h)

b

Then, by recalling the deﬁnition of
Prop. 3 we have,

GM with respect to the matrix D deﬁned as in Def. 2 and by

P

GM =

b
S∗M D2

SM =

1
M

M

t=1
X

1
npit

Kxit ⊗

Kxit .

Consequently
in the following way

GM = 1
M

b
b
b
M
vi, where (vi)M
i=1 vi ⊗

i=1 are independent random variables distributed

b

P

vi =

1
√pjn

Kxj , with probability pj,

1

j

∀

≤

≤

n.

Now we study the moments of

GM as a sum of independent random matrices, to apply non-

commutative Bernstein inequality (e.g. Prop. 7 of [14]). We have that, for any 1

i

M

≤

≤

Evi ⊗

vi =

pj

n

j=1
X

b
1
pjn

(cid:18)

k

Kxj ⊗

Kxj

=

Cn,

(cid:19)

vi,

C−

1
nλ vi

D

b

≤

1

EH

sup
n
j

≤

n

≤
1
n

q

≤

2

1/2

nλ Kxj k
C−
pjn
b

lλ(h)

q2 1
n

≤

b

lλ(j)
pjn

= sup
n
1

j

≤

≤

n

= sup
n
1

j

≤

≤

lλ(j)
lλ(j)

1
n

lλ(h) = q2 Tr(

1
C−
nλ

b
Cn),

n

lλ(h)

Xh=1

b

Xh=1

b

Xh=1

31

b

b

N

b

n. Denote with

(λ), the quantity Tr(

Cn), by applying Prop. 7 of [14], we

for all 1
have

j

≤

≤

N

b

k

1/2

C−
nλ

GM )

Cn −
(
b
(λ) with respect to intrinsic dimension

b
with probability at least 1

1/2
nλ k ≤

b
−

C−

b

2d(1 + q2
3M

(λ))

N

b

µ. The ﬁnal step consist in bounding the empirical intrinsic dimension

(λ), for which we use Prop. 1 of [13], obtaining

1
C−
nλ

b

+

s

b
2dq2

(λ)

N
M
b

,

d = log

κ2
λµ

.

N
(λ)

N

b
≥

2.65

(λ),

≤
405κ2

N
67κ2 log 6κ2

∨

with probability at least 1
intersecting the events, we have

−

µ, when n

µ and 19κ2

n log n

λ

C

. By

4µ ≤

≤ k

k

1/2

C−
nλ

k

Cn −
(
b

GM )

C−

1/2
nλ k ≤

5.3d(1 + q2
3M

N

(λ))

+

(λ)

5.3dq2
M

N

,

d = log

κ2
λµ

.

r

with probability at least 1
selecting M as in Eq. 35, we have

b

b

b
2µ. The last step consist in substituting µ with µ = δ/2. Thus, by
−

That implies,

t =

1/2

C−
nλ

k

GM )

C−

1/2
nλ k

<

η
1 + η

.

Cn −
(
b

b

b

b

G−

1/2
Mλ (

k

GM )

G−

1/2
Mλ k

<

Cn −
b

b

b

b

t

1

t

−

< η.

E Proof of Main Results

In this section we prove the main results of the paper. This section is divided in three subsections.
In the ﬁrst, we specify the computational oracle inequality for Nystr¨om with uniform sampling, in
the second we specify the computational oracle inequality for Nystr¨om with approximate leverage
scores sampling (see Sect. D for a deﬁnition), while the third subsection contains the proof of the
main theorem presented in the paper.

Now we give a short sketch of the structure of the proofs. The deﬁnition of the general version
of the FALKON algorithm (taking into account leverage scores and non invertible KMM ) is given
in Sect. A. In Sect. B the notation and basic deﬁnition required for the rest of the analisys are
provided.

Our starting point is the analysis of the basic Nystr¨om estimator given in [13]. The key
novelty is the quantiﬁcation of the approximations induced by the preconditioned iterative solver
by relating its excess risk to the one of the basic Nystr¨om estimator.

A computational oracle inequality. First we prove that FALKON is equal to the exact Nystr¨om
in Lemma 8 (see also
estimator as the iterations go to inﬁnity (Lemma 5, Sect. C). Then,
Lemma 6, 7, Sect. C) we show how optimization guarantees can be used to derive statistical
results. More precisely, while optimization results in machine learning typically derives guarantees
on empirical minimization problems, we show, using analytic and probabilistic tools, how these
results can be turned into guarantees on the expected risks. Finally, in the proof of Thm. 1 we
concentrate the terms of the inequality. The other key point is the study of the behavior of the
condition number of B⊤HB with B given in (10).

Controlling the condition number of B⊤HB. Let Cn, CM be the empirical correlation operators
Kxi,

associated respectively to the training set and the Nystr¨om points Cn = 1
n

n

in

H

i=1 Kxi ⊗

P

32

CM = 1
M
λI)V A−

M
j=1 K

K

xj . In Lemma 1, Sect. C, we prove that B⊤HB is equivalent to A−⊤V ∗(Cn+
e
1 for a suitable partial isometry V . Then in Lemma 2, Sect. C, we split it in two components
P

xj ⊗
e

B⊤HB = A−⊤V ∗(CM + λI)V A−

1 + A−⊤V ∗(Cn −

CM )V A−

1,

(36)

k

< 1. In Lemma 3 we prove that

and prove that the ﬁrst component is just the identity matrix. By denoting the second component
with E, Eq. (36), Sect. C, implies that the condition number of B⊤HB is bounded by (1+

−
is analytically bounded by a suitable distance
E
), when
E
k
k
k
CM and in Lemma 9, 10, Sect. D, we bound in probability such distance, when the
between Cn −
Nystr¨om centers are selected uniformly at random and with approximate leverage scores. Finally
in Lemma 11, 12, Sect. D, we give a condition on M for the two kind of sampling, such that the
t/2, leading
condition number is controlled and the error term in the oracle inequality decays as e−
to Thm. 2, 4.

)/(1

E

E

k

k

k

k

Now we provide the preliminary result necessary to prove a computational oracle inequality

for FALKON.

Theorem 1 Let 0

C
k
estimator, with preconditioner B, after t iterations Def. 4 and let
estimator as in Eq. 8. Let δ

, B as in Def. 3 and n, M, t

(0, 1] and n

≤ k

≤

∈

λ

3, then following holds with probability 1

δ

b

N. Let

fλ,M,t be the FALKON
fλ,M be the exact Nystr¨om

∈

≥

−

fλ,M,t)1/2
(
R

fλ,M )1/2 + 4
(
≤ R

νt

v e−

1 +

log

r

n
δ

,

b
n
i=1 y2
i and ν = log

e
√cond (W )+1
√cond (W )
1

.

−

b

In particular ν

1/2, when cond (W )

≤

e
9κ2
λn

≥

v2 = 1
n

where
( e1/2+1
e1/2
−

1 )2.
b

P

Proof. By applying Lemma 8, we have

fλ,M,t)1/2
(
R

fλ,M )1/2 + 2c0k
(
≤ R
e
S

S

C−

1/2
nλ k
1/2
C−
b
nλ k

.

v e−

νt.

To complete the theorem we need to study the quantity
9κ2
δ . By dividing and multiplying for C1/2
n log n
nλ0 , we have

b

k

In particular, deﬁne λ0 =
b

Now, for the ﬁrst term, since

b

Z ∗Z

, and the fact that C = S∗S (see Prop. 3), we have

b
1/2
nλ0 kk

SC−

C1/2

nλ0C−

1/2
nλ k

.

S

C−

1/2
nλ k

k

=

SC−

1/2

nλ0 C1/2

nλ0 C−

1/2

nλ k ≤ k

k

Z

2 =

k

k
1/2
nλ0 k

k
2 =

k

SC−

k

k
1/2

C−

nλ0 CC−

1/2
nλ0 k

=

C1/2C−

1/2
nλ0 k

,

k

moreover by Lemma 5 of [13] (or Lemma 7.6 of [42]), we have

C1/2C−

1/2
nλ0 k ≤

2,

k

with probability 1
C, recalling that σ(C)

−

⊂

[0, κ2] (see Prop. 3), we have

δ. Finally, by denoting with σ(C) the set of eigenvalues of the positive operator

C1/2

nλ0 C−

1/2
nλ k

k

= sup
σ

σ(C) r

∈

σ + λ0
σ + λ ≤

sup
[0,κ2] r

σ

∈

σ + λ0
σ + λ ≤ r

1 +

λ0
λ

.

33

E.1 Main Result (I): computational oracle inequality for FALKON with

uniform sampling

Lemma 11. Let δ
Def. 3 and the Nystr¨om centers selected via uniform sampling. When

(0, 1], 0 < λ

, n, M

≤ k

C

∈

∈

k

N, the matrix W as in Eq. 14 with B satisfying

(37)

then the following holds with probability 1

δ

Proof. By Lemma 1 we have that

M

5 [1 + 14

(λ)] log

≥

N∞

8κ2
λδ

,

−

cond (W )

2

.

e1/2 + 1
e1/2
1

−

(cid:19)

≤

(cid:18)

cond (W )

1 +
1

k
− k

E
E

k
k

,

≤

with the operator E deﬁned in the same lemma. By Lemma 3, we have

1/2
Mλ (

k

E

G−

k ≤ k

Cn −
Lemma 9 proves that when the Nystr¨om centers are selected with uniform sampling and M satisﬁes
b
Eq. (34) for a given parameter η
η, with probability
(0, 1], then
GM )
∈
e+1 . The condition on M in Eq. (37) is derived by Eq. (34) by
1
−
substituting η with 2e1/2
e+1 .

δ. In particular we select η = 2e1/2

b
Cn −
b

1/2
Mλ k ≤

b
1/2
Mλ (

GM )

G−

G−

G−

b

b

b

b

k

.

1/2
Mλ k

Theorem 6. Let δ
∈
uniform sampling. Let
be the exact Nystr¨om estimator in Eq. (8). When
b

≤ k

C

(0, 1], 0 < λ
fλ,M,t be the FALKON estimator, after t iterations (Def. 4) and let

N and the Nystr¨om centers be selected via
fλ,M

, n, M

∈

k

e

M

5 [1 + 14

(λ)] log

≥

N∞

8κ2
λδ

,

then, with probability 1

2δ,

−

fλ,M,t)1/2
(
R

fλ,M )1/2 + 4
(
≤ R

v e−

t
2

1 +

log

9κ2
λn

n
δ

,

r

b
Proof. By applying Lemma 11 we have that

e

b

cond (W )

(e1/2 + 1)2/(e1/2

1)2,

≤

−

with probability 1
in Thm. 1 and take the union bound of the two events.

−

δ under the condition on M . Then apply the computational oracle inequality

Theorem 2. Under the same conditions of Thm. 1, the exponent ν in Thm. 1 satisﬁes ν

1/2,
2δ, when the Nystr¨om centers are selected via uniform sampling (see Sect. A),

≥

with probability 1
and

−

Proof. It is a direct application of Thm. 6. Indeed note that

(λ)

N∞

≤

κ2
λ by deﬁnition.

M

5

1 +

≥

(cid:20)

14κ2
λ

log

8κ2
λδ

.

(cid:21)

34

E.2 Main Result (II): computational oracle inequality for FALKON with

leverage scores

Lemma 12. Let δ
(0, 1] and the matrix W be as in Eq. 14 with B satisfying Def. 3 and
the Nystr¨om centers selected via (q, λ0, δ)-approximated leverage scores sampling (see Def. 1 and
discussion below), with λ0 = 19κ2

405κ2

and

, n

C

∈

λ

67κ2 log 12κ2
δ

n log n

≤ k

≥

∨

(38)

(cid:2)
then the following holds with probability 1

2δ . When λ0 ≤
1 + 43q2
5

M

≥

k
8κ2
λδ

,

(λ)

log

N

(cid:3)

δ

−

cond (W )

2

.

e1/2 + 1
e1/2
1

−

(cid:19)

≤

(cid:18)

cond (W )

1 +
1

k
− k

E
E

k
k

,

≤

Proof. By Lemma 1 we have that

with the operator E deﬁned in the same lemma. By Lemma 3 we have

1/2
Mλ (

k

E

G−

Cn −
Lemma 10 proves that when the Nystr¨om centers are selected via q-approximate leverage scores
b
and M satisﬁes Eq. (35) for a given parameter η
η, with
Cn −
∈
δ. In particular we select η = 2e1/2
e+1 . The condition on M in Eq. (38) is derived by
probability 1
b
Eq. (35) by substituting η with 2e1/2
e+1 .

1/2
Mλ k ≤

(0, 1], then

1/2
Mλ (

k ≤ k

GM )

GM )

G−

G−

G−

−

b

b

b

b

b

b

k

.

1/2
Mλ k

∈

∈

(0, 1], M, n

Theorem 7. Let δ
leverage scores sampling (see Def. 1 and discussion below), with λ0 = 19κ2
fλ,M,t be the FALKON estimator, after t iterations (Def. 4) and let
67κ2 log 12κ2
λ
estimator in Eq. (8). When λ0 ≤
δ
b
8κ2
λδ

N and the Nystr¨om centers be selected via (q, λ0, δ)-approximated
N. Let
2δ . Let t
fλ,M be the exact Nystr¨om
and
e

k
≥
1 + 43q2

n log n

405κ2

≤ k

(λ)

log

, n

M

C

∨

∈

5

,

N

≥

then, with probability 1

2δ,

(cid:2)

−

fλ,M,t)1/2
(
R

fλ,M )1/2 + 4
(
≤ R

v e−

t
2

1 +

log

9κ2
λn

n
δ

,

r

Proof. By applying Lemma 12 we have that
b

e

(cid:3)

b

cond (W )

(e1/2 + 1)2/(e1/2

1)2,

≤

−

with probability 1
inequality in Thm. 1 and take the union bound of the two events.

δ under the conditions on λ, n, M . Then apply the computational oracle

−

Theorem 4. Under the same conditions of Thm. 1, the exponent ν in Thm. 1 satisﬁes ν

1/2,

≥

with probability 1

2δ, when

−

1. either Nystr¨om uniform sampling (see Sect. A) is used with M

70 [1 +

2. or Nystr¨om (q, λ0, δ)-appr.

lev. scores (see Sect. A) is used, with λ

≥

N∞

(λ)] log 8κ2
λδ .
19κ2
n log n
2δ , n

≥

≥

405κ2 log 12κ2
δ

, and

Proof. It is a merge of Thm. 6 and Thm. 7.

(cid:2)

215

2 + q2

(λ)

log

M

≥

N

(cid:3)

8κ2
λδ

.

35

E.3 Main Results (III): Optimal Generalization Bounds

First we recall the standard assumptions to study generalization rates for the non-parametric
supervised learning setting, with square loss function. Then we provide Thm. 8, from which we
obtain Thm. 3 and Thm. 5.

There exists κ
) = inf f

(f

≥

E
moments of y: there exist σ, b satisfying 0

∈H E

H

(f ). Moreover, we assume that ρ(y

≤

κ2 for any x

X. There exists f

, such that
x) has sub-exponential tails, i.e. in terms of
X, the following holds

b, such that, for any x

H ∈ H

∈

|

1 such that K(x, x)

σ

≤

≤

∈

E [

y

|

−

p

f

(x)
|

H

x]

|

≤

1
2

p!σ2bp

2,

−

p

∀

≥

2.

Note that the assumption above is satisﬁed, when y is supported in an interval or when it has sub-
gaussian or sub-exponential tails. The last assumption is known as source condition [17]. There
exist r

[1/2, 1] and g

, such that

∈

∈ H

= Cr

1/2g,

−

f

H

where C is the correlation operator deﬁned in Def. 5. Finally deﬁne R
Note that assuming the existence of f
f
and R = max(1,

, the source condition is always satisﬁed with r = 1/2, g = f
), however if it is satisﬁed with larger r it leads to faster learning rates.

1 such that R

≥ k

≥

H

g

kH
H

.

HkH

k
Theorem 8. Let δ
19κ2
n log 24n

λ

δ ≤

∈
≤ k

k

(0, 1]. Let n, λ, M satisfy n
C

≥
fλ,M,t be the FALKON estimator in Def. 4, after t

1655κ2 + 223κ2 log 24κ2
δ

. Let

, M

≥

334 log 192n

and
δ
N iterations.

Under the assumptions in Eq. (39), (40), the following holds with probability at least 1

δ,

∈

−

b

fλ,M,t)1/2
(
R

≤

6R

b

 

p

(λ)

+

N∞
n

σ2

(λ)
N
n !

r

log

+ 7Rλr,

24
δ

1. either, when the Nystr¨om points are selected uniformly sampled (see Sect. A) and

b

M

70

1 +

(λ)

log

≥

h

N∞

i

48κ2
λδ

,

2 log

t

≥

8(b + κ

)

,

HkH

f
k
Rλr

2. or, when the Nystr¨om points are selected by means of (q, λ0, δ)-approximate leverage scores
δ and

(see Sect. A), with q

1, λ0 = 19κ2

n log 48n

≥

h

M

215

1 + q2

(λ)

log

≥

N

i

192κ2n
λδ

,

2 log

t

≥

8(b + κ

)

.

HkH

f
k
Rλr

Proof. Let µ = δ/4. By Proposition 2 of [13], under the assumptions in Eq. 39 and Eq. 40, when
µ , and 19κ2
, we have with probability
n
1

1655κ2+223κ2 log 6κ2
µ

n log 6n

334 log 48n

µ , M

µ ≤

≤ k

≥

C

λ

k

≥
−

(39)

(40)

(41)

(42)

(43)

fλ,M )1/2
(
R

≤

6R

b

 

p

(λ)

+

N∞
n

σ2

(λ)
N
n !

r

6
µ

log

+ 3R

(M )r + 3Rλr,

C

where

e

(M ) = min

t > 0

(67 + 5

(t)) log

N∞

C

(cid:26)

12κ2
tµ ≤

M

,

(cid:27)

when the Nystr¨om centers are selected with uniform sampling, otherwise

(M ) = min

C

t

λ0 ≤
(cid:26)

C

≤ k

k

78q2

(t) log

N

48n
µ ≤

M

,

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
36

when the Nystr¨om centers are selected via approximate sampling, with λ0 = 19κ2
In
λ, in both cases, when M satisﬁes Eq. (42) for uniform sampling, or
particular, note that
Eq. (43) for approximate leverage scores. Now, by applying the computational oracle inequality
in Thm. 6, for uniform sampling, or Thm. 7, for approximate leverage scores, the following holds
with probability 1

n log 12n
µ .

(M )

2µ

≤

C

−

R(

fλ,M,t)1/2

fλ,M )1/2 + 4
(
≤ R

v e−

t
2

1 +

log

9κ2
λn

n
µ

,

s

with

v2 := 1
n

n
i=1 y2

i . In particular, note that, since we require λ

e

b

b

19κ2
n log 12n

µ , we have

≥

P

b

4

1 +

s

9κ2
λn

log

n
µ ≤

5.

Now, we choose t such that 5
Rλr . The last step consists in bounding
v in probability. Since it depends on the random variables y1, . . . , yn we bound it in the following
X, we have
way. By recalling that

for any x

ve−

≤

=

Rλr, that is t

v
2 log 5
b

t/2

κ

f

f

f

Kx, f

| h

HiH | ≤ k

HkH ≤

k

HkH

∈

≥
KxkHk

b

(x)
b
|

H

|

n

v =

1
√n k

y

(yi −

f
H
n

(xi))2

+

n

f

H

(xi)2
n

n

(xi))2

(yi −

f
H
n

b

b

i=1
X

k ≤ v
u
u
t
Since the training set examples (xi, yi)n
i=1 are i.i.d. with probability ρ we can apply the Bernstein
(xi))2 (since
inequality [43] to the random variables zi = (yi −
xi, yi are i.i.d. each zi has the same distribution and so the same expected value s). In particular,
we need to bound the moments of zi’s. By the assumption in Eq. 39, zi are zero mean and

s, with s = E(yi −

≤ v
u
u
t

(xi))2

v
u
u
t

i=1
X

i=1
X

HkH

−

k

f

f

H

H

+ κ

f

.

2p

E
|

zi|

≤

1
2

(2p)!σ2b2p

−

2

p!(4σb)2(4b2)p

2,

−

1
2

≤

p

2

≥

and so, by applying the Bernstein inequality, the following holds with probability 1

µ

−

n

zi
n (cid:12)
(cid:12)
i=1
(cid:12)
(cid:12)
X
(cid:12)
(cid:12)
(cid:12)
(cid:12)
where the last step is due to the fact that we require n
(cid:12)
(cid:12)
by deﬁnition. So, by noting that s

σ2

s

8b2 log 2
µ
3n

≤

+

8σ2b2 log 2
µ
n

1
4

b2,

≤

223κ2 log 6

µ , that b

≥

σ and that κ

1

≥

≥

b2 (see Eq. 39), we have

≤

≤

n

zi
n ≤

v

κ

f

≤

k

HkH

+

s +

κ

f

+ √s +

b + κ

f

,

v
u
u
t
µ. Now by taking the intersection of the three events, the following

i=1
X

HkH

HkH

≤

k

k

1
2

b

3
2

b
with probability at least 1
holds with probability at least 1

−

4µ

−

fλ,M,t)1/2
(
R

≤

6R

b

 

p

(λ)

+

N∞
n

σ2

(λ)
N
n !

r

6
µ

log

+ 7Rλr.

b

Now we provide the generalization error bounds for the setting where we only assume the

existence of f

.

H

37

Theorem. 3 Let δ

For any n

max( 1
C
k

k

≥

∈

(0, 1]. Let the outputs y be bounded in [
−
)2 the following holds. When

, 82κ2 log 373κ2
√δ

a

2 , a

2 ], almost surely, with a > 0.

M

5(67 + 20√n) log

log(n) + 5 + 2 log(a + 3κ),

48κ2n
δ

,

t

≥

1
2

λ =

1
√n

,

then with probability 1

δ,

≥

−

fλ,M,t )
(
R

≤

c0 log2 24
δ
√n

,

fλ,M,t is the FALKON estimator in Def. 4 (see also Sect. 3 Alg. 1) with Nystr¨om uniform

where
sampling, and the constant c0 = 49

b
2

f

k

Hk

H

b

Proof. Here we assume y

(1 + aκ + 2κ2

f

)2.

k

HkH

E[

y

|

−

p

f

(x)
|

H

|

x]

≤

where we used the fact that
(x)
|
is satisﬁed with r = 1/2 and g = f
H

f

H

|

∈
E[2p

−

a

2 , a

−

[

1

p

y

|

|

|

1

−

f

x] + 2p

2 ] a.s., so Eq. 39 is satisﬁed with σ = b = a + 2κ
1
2
=
Kxkk
HiH | ≤ k
f
, while R = max(1,
HkH
k

f
Hk ≤
).

(ap + 2pκp

(x)
|

Kx, f

HkH

Hk

1
2

| h

≤

≤

κ

k

k

f

f

H

H

)

p

p

|

f

k

HkH

, indeed

p!(a + 2κ

f

)p,

k

HkH

. Moreover, Eq. (40)

To complete the proof we show that the assumptions on λ, M, n satisfy the condition re-
1/2 and deﬁne
n0, satisﬁes the condition on n required by

quired by Thm. 8, then we apply it and derive the ﬁnal bound. Set λ = n−
)2. The condition n
n0 = max(
k
1/2 and M

≥
75 √n log 48κ2n

1, 82κ2 log 373κ2
√δ
Thm. 8. Moreover both λ = n−
λ, M required by Thm. 8, when n
required by Thm. 8, indeed, since R = max(1,

satisfy respectively the conditions on
n0. Finally note that the condition on t implies the condition

), we have a/R

a and

1, so

/R

≥

≥

C

k

f

f

−

δ

≤

k

HkH

≤

2 log

8(a + κ

f
k
Rλr

)

HkH

= log

64

"

(cid:18)

k

+

kH
f
3κ
HkH
R

k

a
R

2

(cid:19)

√n

#

log(64(a + 3κ)2√n)

log 64 +

log n + 2 log(a + 3κ).

≤

≤

1
2

So, by applying Thm. 8 with R, r deﬁned as above and recalling that
have

(λ)

N

(λ)

≤ N∞

fλ,M,t)1/2
(
R

≤

6R

b

(λ)

+

N∞
n

 

p

r

σ2

= 6Rbκ(1 + n−

1/2)n−

1/4 log

+ 7Rn−

1/4

(λ)
N
n !
24
δ

log

+ 7Rλr

6R

24
δ

bκ
√λn

+

σκ
√λn (cid:19)

≤

(cid:18)
7R(bκ + 1) log 24
δ
n1/4

.

≤

κ2
λ , we

≤

24
δ

log

+ 7Rλ

1
2

with probability 1
n

n0, and that log 24

−

δ. For the last step we used the fact that b = σ, that 6(1 + n−
δ > 1.

1/2)

7, since

≤

To state the result for fast rates, we need to deﬁne explicitly the capacity condition on the

intrinsic dimension. There exists Q > 0, γ

(0, 1] such that

b

≥

∈

≤

(λ)

N

Q2λ−

γ,

λ

∀

≥

0.

(44)

Note that, by deﬁnition of
Theorem 5 Let δ

N

(λ), the assumption above is always satisﬁed with Q = κ and γ = 1.
2 ], almost surely, with a > 0.
, with s = 2r + γ, the

(0, 1]. Let the outputs y be bounded in [
102κ2s

s
s−1

2 , a
−
log 912
δ

C

−

a

1

s

s

∈

Under the assumptions in Eq. 40, 44 and n
following holds. When

≥ k

k

∨

−

(cid:16)

(cid:17)

λ = n−

1
2r+γ ,

log(n) + 5 + 2 log(a + 3κ2),

t

≥

38

and

b

1. and either Nystr¨om uniform sampling is used with

M

70 [1 +

(λ)]

log

≥

N∞

8κ2
λδ

,

2. and or Nystr¨om (q, λ0, δ)-approximate leverage scores (Def. 1), with q

1, λ0 = 19κ2

n log 48n

δ

≥

(45)

(46)

then with probability 1

δ,

−

M

215

1 + q2

(λ)

log

≥

(cid:2)

N

(cid:3)

fλ,M,t)
(
R

≤

c0 log2 24
δ

n−

2r
2r+γ ,

8κ2
λδ

,

fλ,M,t is the FALKON estimator in Sect. 3 (Alg. 1). In particular n0, c0 do not depend on

where
λ, M, n and c0 do not depend on δ.

b

Proof. The proof is similar to the one for the slow learning rate (Thm. 3), here we take into account
the additional assumption in Eq. (40), 44 and the fact that r may be bigger than 1/2. Moreover
we assume y

2 ] a.s., so Eq. 39 is satisﬁed with σ = b = a + 2κ

, indeed

2 , a

f

a

[

∈
p

−

x]

(x)
|

E[

y

f

|

H

−

|
where we used the fact that

≤

E[2p

−

1

x] + 2p

−

1

f

p

|

y

|
f

|
(x)
|

=

|
H
Kx, f

1
2
Kxkk

f

k

HkH
p

)

H

k
κ

Hk
f

≤
.

1
2

p

(x)
|

≤

(ap + 2pκp

f

p!(a + 2κ

f

)p,

k

HkH

HiH | ≤ k
To complete the proof we show that the assumptions on λ, M, n satisfy the required conditions
1/(2r+γ) and deﬁne

to apply Thm. 8. Then we apply it and derive the ﬁnal bound. Set λ = n−

Hk ≤

HkH

| h

k

H

|

s

102κ2s

log 912
δ

s

1

−

−

k

k

∨

C

n0 =

, with s = 2r+γ. Since 1 < s

n0, satisﬁes
≥
1/(2r+γ) and M
the condition on n required to apply Thm. 8. Moreover, for any n
satisfying Eq. (45) for Nystr¨om uniform sampling, and Eq. 46 for Nystr¨om leverage scores, satisfy
respectively the conditions on λ, M required to apply Thm. 8. Finally note that the condition on
t implies the condition required by Thm. 8, indeed, since 2r/(2r + γ)

3, the condition n

n0, both λ = n−

1,

≤

≥

(cid:16)

(cid:17)

s
s−1

2 log

8(b + κ

f
k
Rλr

)

HkH

= log

64

a
R

+

3κ

k

f
HkH
R

"

(cid:18)

≤

2

2r
2r+γ

n

(cid:19)

#
2r
2r + γ

log 64 + 2 log

HkH

+

log n

f

f

a + 3κ
k
R
a + 3κ
k
R

log 64 + 2 log

HkH

+ log n,

log 64 + 2 log(a + 3κ2) + log n.

≤

≤

≤

where the last step is due to the fact that a/R
R := max(1,
g
HkH ≤ k
kH
deﬁned as above and recalling that
N∞
capacity condition in Eq. (44), we have

kk
≤

), and

(λ)

Cr

1/2

k

k

f

g

−

k

/R

fhkH

κ, since
1 and
−
, by deﬁnition. So, by applying Thm. 8 with R, r
γ by the

≤
kH
κ2
λ by construction and that

Q2λ−

k ≤ k

≤ k

(λ)

≤

C

k

Cr

1/2

1/2

N

≤

fλ,M,t)1/2
(
R

≤

6R

b

 

p

(λ)

+

N∞
n

σ2

(λ)
N
n !

r

log

+ 7Rλr

6R

bκ
√λn

+

Qσ
√λγn (cid:19)

(cid:18)

log

+ 7Rλr

24
δ

b

= 6Rb

κn−

2r+γ + Q

n−

2r+γ log

+ 7Rn−

r+γ−1/2

r

≤

r
2r+γ

(cid:16)

7R(b(κ + Q) + 1) log

r

n−

2r+γ .

(cid:17)
24
δ

≤

with probability 1
r

−
1/2 by deﬁnition, and that log 24

δ > 1.

≥

δ. For the last step we used the fact that b = σ, that r + γ

1/2

0, since

−

≥

24
δ

24
δ

39

F Longer comparison with previous works

In the literature of KRR there are some papers that propose to solve Eq. 5 with iterative precon-
ditioned methods [3, 4, 26, 5, 6]. In particular the one of [3] is based, essentially, on an incomplete
singular value decomposition of the kernel matrix. Similarly, the ones proposed by [5, 6] are based
on singular value decomposition obtained via randomized linear algebra approaches. The ﬁrst
covers the linear case, while the second deals with the kernel case.
[4, 26] use a preconditioner
based on the solution of a randomized projection problem based respectively on random features
and Nystr¨om.

≈

log n) we should choose k such that σk(KnM )

While such preconditioners are suitable in the case of KRR, their computational cost becomes
too expensive when applied to the random projection case.
Indeed, performing an incomplete
svd of the matrix KnM even via randomized linear algebra approaches would require O(nM k)
where k is the number of singular values to compute. To achieve a good preconditioning level
λ. When the kernel function
(and so having t
is bounded, without further assumptions on the eigenvalue decay of the kernel matrix, we need
1/2, M = O(√n) to achieve optimal
k
√n and so the total cost of the incomplete svd preconditioner
generalization bounds, we have k
is O(n2). On the same lines, applying the preconditioner proposed by [4, 26] requires O(nM 2) to
be computed and there is no natural way to ﬁnd a similar sketched preconditioner as the one in
Eq. (10) in the case of [4], with reduced computational cost. In the case of [26], the preconditioner
1, whose computation amounts to solve the original problem in
they use is exactly the matrix H −
Eq. (8) with direct methods and requires O(nM 2).

1 [17, 13]. Since randomized projection requires λ = n−

λ−

≈

≈

≈

A similar reasoning hold for methods that solve the Nystr¨om linear system (8) with iterative
approaches [22, 23, 24]. Indeed on the positive side, they have a computational cost of O(nM t).
However they are aﬀected by the poor conditioning of the linear system in Eq. 8. Indeed, even
if H or KMM in Eq. 8 are invertible, their condition number can be arbitrarily large (while in
1), and so many iterations are often needed to achieve optimal
the KRR case it is bounded by λ−
λ−
generalization (E.g. by using early stopping in [23] they need t

1).

≈

G MATLAB Code for FALKON

40

Algorithm 2 Complete MATLAB code for FALKON. It requires O(nM t + M 3) in time and
O(M 2) in memory. See Sect. 3 for more details, and Sect. 4 for theoretical properties.
N numbers of Nystr¨om centers to
Input: Dataset X = (xi)n
select, lev scores
centers via uniform sampling), function KernelMatrix computing the Kernel matrix of two sets of
points, regularization parameter λ, number of iterations t.
Output: Nystr¨om coeﬃcients α.

Rn, M
Rn approximate leverage scores (set lev scores = [ ] for selecting Nystr¨om

D, ˆy = (yi)n

i=1 ∈

i=1 ∈

Rn

∈

∈

×

function alpha = FALKON(X, Y, lev_scores, M, KernelMatrix, lambda, t)

n = size(X,1);
[C, D] = selectNystromCenters(X, lev_scores, M, n);

KMM = KernelMatrix(C,C);
T = chol(D*KMM*D + eps*M*eye(M));
A = chol(T*T’/M + lambda*eye(M));

function w = KnMtimesVector(u, v)

w = zeros(M,1); ms = ceil(linspace(0, n, ceil(n/M)+1));
for i=1:ceil(n/M)

Kr = KernelMatrix( X(ms(i)+1:ms(i+1),:), C );
w = w + Kr’*(Kr*u + v(ms(i)+1:ms(i+1),:));

end

end

end

function w = BHB(u)

w = A’\(T’\(KnMtimesVector(T\(A\u), zeros(n,1))/n) + lambda*(A\u));

r = A’\(T’\KnMtimesVector(zeros(M,1), Y/n));

beta = conjgrad(@BHB, r, t);
alpha = T\(A\beta);

end

function beta = conjgrad(funA, r, tmax)

p = r; rsold = r’*r; beta = zeros(size(r,1), 1);

for i = 1:tmax

Ap = funA(p);
a = rsold/(p’*Ap);
beta = beta + a*p;
r = r - a*Ap; rsnew = r’*r;
p = r + (rsnew/rsold)*p;
rsold = rsnew;

end

end

end

end

end

function [C, D] = selectNystromCenters(X, lev_scores, M, n)

if isempty(lev_scores) %Uniform Nystrom

D = eye(M);
C = X(randperm(n,M),:);
else % Appr. Lev. Scores Nystrom

prob = lev_scores(:)./sum(lev_scores(:));
[count, ind] = discrete_prob_sample(M, prob);
D = diag(1./sqrt(n*prob(ind).*count));
C = X(ind,:);

function [count, ind] = discrete_prob_sample(M, prob)

bins = histcounts(rand(M,1), [0; cumsum(prob(:))]);
ind = find(bins > 0);
count = bins(ind);

41

