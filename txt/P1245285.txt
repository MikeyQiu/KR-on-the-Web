STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

1

Stochastic Subsampling for
Factorizing Huge Matrices

Arthur Mensch, Julien Mairal,
Bertrand Thirion, and Ga¨el Varoquaux

7
1
0
2
 
t
c
O
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
6
3
5
0
.
1
0
7
1
:
v
i
X
r
a

Abstract—We present a matrix-factorization algorithm that
scales to input matrices with both huge number of rows and
columns. Learned factors may be sparse or dense and/or non-
negative, which makes our algorithm suitable for dictionary
learning, sparse component analysis, and non-negative matrix
factorization. Our algorithm streams matrix columns while
subsampling them to iteratively learn the matrix factors. At
each iteration, the row dimension of a new sample is reduced by
subsampling, resulting in lower time complexity compared to a
simple streaming algorithm. Our method comes with convergence
guarantees to reach a stationary point of the matrix-factorization
problem. We demonstrate its efﬁciency on massive functional
Magnetic Resonance Imaging data (2 TB), and on patches ex-
tracted from hyperspectral images (103 GB). For both problems,
which involve different penalties on rows and columns, we obtain
signiﬁcant speed-ups compared to state-of-the-art algorithms.

Index Terms—Matrix factorization, dictionary learning, NMF,
stochastic optimization, majorization-minimization, randomized
methods, functional MRI, hyperspectral imaging

I. INTRODUCTION

Matrix factorization is a ﬂexible approach to uncover latent
factors in low-rank or sparse models. With sparse factors, it is
used in dictionary learning, and has proven very effective for
denoising and visual feature encoding in signal and computer
vision [see e.g., 1]. When the data admit a low-rank structure,
matrix factorization has proven very powerful for various tasks
such as matrix completion [2, 3], word embedding [4, 5], or
network models [6]. It is ﬂexible enough to accommodate a
large set of constraints and regularizations, and has gained sig-
niﬁcant attention in scientiﬁc domains where interpretability
is a key aspect, such as genetics [7] and neuroscience [8].
In this paper, our goal is to adapt matrix-factorization tech-
niques to huge-dimensional datasets, i.e., with large number
of columns n and large number of rows p. Speciﬁcally, our
work is motivated by the rapid increase in sensor resolution, as
in hyperspectral imaging or fMRI, and the challenge that the
resulting high-dimensional signals pose to current algorithms.
As a widely-used model, the literature on matrix factoriza-
tion is very rich and two main classes of formulations have

A. Mensch, B. Thirion, G. Varoquaux are with Parietal team, Inria, CEA,
Paris-Saclay University, Neurospin, at Gif-sur-Yvette, France. J. Mairal is with
Universit´e Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK at Grenoble,
France.

The research leading to these results was supported by the ANR (MAC-
ARON project, ANR-14-CE23-0003-01 — NiConnect project, ANR-11-
BINF-0004NiConnect). It has received funding from the European Union’s
Horizon 2020 Framework Programme for Research and Innovation under
Grant Agreement No 720270 (Human Brain Project SGA1).

Corresponding author: Arthur Mensch (arthur.mensch@m4x.org)

emerged. The ﬁrst one addresses a convex-optimization prob-
lem with a penalty promoting low-rank structures, such as the
trace or max norms [2]. This formulation has strong theoretical
guarantees [3], but lacks scalability for huge datasets or sparse
factors. For these reasons, our paper is focused on a second
type of approach, which relies on nonconvex optimization.
Stochastic (or online) optimization methods have been devel-
oped in this setting. Unlike classical alternate minimization
procedures, they learn matrix decompositions by observing
a single matrix column (or row) at each iteration. In other
words, they stream data along one matrix dimension. Their
cost per iteration is signiﬁcantly reduced, leading to faster
convergence in various practical contexts. More precisely,
two approaches have been particularly successful: stochastic
gradient descent [9] and stochastic majorization-minimization
methods [10, 11]. The former has been widely used for matrix
completion [see 12, 13, 14, and references therein], while the
latter has been used for dictionary learning with sparse and/or
structured regularization [15]. Despite those efforts, stochastic
algorithms for dictionary learning are currently unable to deal
efﬁciently with matrices that are large in both dimensions.

We propose a new matrix-factorization algorithm that
can handle such matrices.
It builds upon the stochastic
majorization-minimization framework of [10], which we gen-
the objective
eralize for our problem. In this framework,
function is minimized by iteratively improving an upper-bound
surrogate of the function (majorization step) and minimizing
it to obtain new estimates (minimization step). The core idea
of our algorithm is to approximate these steps to perform them
faster. We carefully introduce and control approximations,
so to extend convergence results of [10] when neither the
majorization nor the minimization step is performed exactly.
For this purpose, we borrow ideas from randomized meth-
ods in machine learning and signal processing. Indeed, quite
orthogonally to stochastic optimization, efﬁcient approaches to
tackle the growth of dataset dimension have exploited random
projections [16, 17] or sampling, reducing data dimension
while preserving signal content. Large-scale datasets often
have an intrinsic dimension which is signiﬁcantly smaller
than their ambient dimension. Good examples are biological
datasets [18] and physical acquisitions with an underlying
sparse structure enabling compressed sensing [19]. In this
context, models can be learned using only random data sum-
maries, also called sketches. For instance, randomized methods
[see 20, for a review] are efﬁcient to compute PCA [21], a
classic matrix-factorization approach, and to solve constrained
or penalized least-square problems [22, 23]. On a theoretical

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

2

level, recent works on sketching [24, 25] have provided bounds
on the risk of using random summaries in learning.

the matrix D is called the “dictionary” and A the sparse code.
We use this terminology throughout the paper.

Using random projections as a pre-processing step is not
appealing in our applicative context since factors learned on
reduced data are not
it
is possible to exploit random sampling to approximate the
steps of online matrix factorization. Factors are learned in
the original space whereas the dimension of each iteration is
reduced together with the computational cost per iteration.

interpretable. On the other hand,

Contribution: The contribution of this paper is both prac-
tical and theoretical. We introduce a new matrix factoriza-
tion algorithm, called subsampled online matrix factorization
(SOMF), which is faster than state-of-the-art algorithms by an
order of magnitude on large real-world datasets (hyperspectral
images, large fMRI data). It leverages random sampling with
stochastic optimization to learn sparse and dense factors more
efﬁciently. To prove the convergence of SOMF, we extend
the stochastic majorization-minimization framework [10] and
make it robust to some time-saving approximations. We then
show convergence guarantees for SOMF under reasonable
assumptions. Finally, we propose an extensive empirical vali-
dation of the subsampling approach.

In a ﬁrst version of this work [26] presented at the Interna-
tional Conference in Machine Learning (ICML), we proposed
an algorithm similar to SOMF, without any theoretical guaran-
tees. The algorithm that we present here has such guarantees,
which we express in a more general framework, stochastic
is validated for new sparsity
majorization-minimization. It
settings and a new domain of application. An open-source
efﬁcient Python package is provided.

Notations: Matrices are written using bold capital letters
and vectors using bold small letters (e.g., X, α). We use
superscript
to specify the column (sample or component)
number, and write X = [x(1), . . . , x(n)]. We use subscripts
to specify the iteration number, as in xt. The ﬂoating bar,
as in ¯gt, is used to stress that a given value is an average
over iterations, or an expectation. The superscript (cid:63) is used to
denote an exact value, when it has to be compared to an inexact
value, e.g., to compare α(cid:63)

t (exact) to αt (approximation).

II. PRIOR ART: MATRIX FACTORIZATION WITH
STOCHASTIC MAJORIZATION-MINIMIZATION

Below, we introduce the matrix-factorization problem and
recall a speciﬁc stochastic algorithm to solve it observing
one column (or a mini-batch) at every iteration. We cast this
algorithm in the stochastic majorization-minimization frame-
work [10], which we will use in the convergence analysis.

A. Problem statement

In our setting, the goal of matrix factorization is to decom-
Rp×n — typically n signals of dimension p

pose a matrix X
— as a product of two smaller matrices:

∈

X

DA with D

Rp×k and A

Rk×n,

≈

∈
with potential sparsity or structure requirements on D and A.
In signal processing, sparsity is often enforced on the code A,
in a problem called dictionary learning [27]. In such a case,

∈

Learning the factorization is typically performed by min-
imizing a quadratic data-ﬁtting term, with constraints and/or
penalties over the code and the dictionary:

min
D∈C
A∈Rk×n

n
(cid:88)

i=1

1
2

(cid:13)
(cid:13)x(i)

Dα(i)(cid:13)
2
2 + λ Ω(α(i)),
(cid:13)

(1)

−

C

→

where A (cid:44) [α(1), . . . , α(n)],
is a column-wise separable
C
R is a penalty over the
convex set of Rp×k and Ω : Rp
code. Both constraint set and penalty may enforce structure or
sparsity, though
has traditionally been used as a technical
requirement to ensure that the penalty on A does not vanish
with D growing arbitrarily large. Two choices of
and Ω are
of particular interest. The problem of dictionary learning sets
C
as the (cid:96)2 ball for each atom and Ω to be the (cid:96)1 norm. Due to
the sparsifying effect of (cid:96)1 penalty [28], the dataset admits
a sparse representation in the dictionary. On the opposite,
ﬁnding a sparse set in which to represent a given dataset,
with a goal akin to sparse PCA [29], requires to set as the
(cid:96)1 ball for each atom and Ω to be the (cid:96)2 norm. Our work
considers the elastic-net constraints and penalties [30], which
encompass both special cases. Fixing ν and µ in [0, 1], we
the elastic-net penalty in Rp and Rk:
denote by Ω(

C

α
ν)
(cid:107)

−
(cid:44) (1

(cid:107)
µ)

ν
2 (cid:107)

α

2
2,
(cid:107)
µ
1+
2 (cid:107)
(cid:107)

1 +

d(j)
(cid:107)

(2)
(cid:111)
1

.

d(j)

2
2 ≤
(cid:107)

(cid:44)

(cid:110)

D

Rp×k/
(cid:107)

d(j)

(cid:107)

∈

C
−
Following [15], we can also enforce the positivity of D
and/or A by replacing R by R+ in
, and adding positivity
constraints on A in (1), as in non-negative sparse coding [31].
We rewrite (1) as an empirical risk minimization problem
depending on the dictionary only. The matrix D solution of (1)
is indeed obtained by minimizing the empirical risk ¯f

C

D

argmin
D∈C

∈

n
(cid:88)

(cid:17)

f (D, x(i))

(cid:16) ¯f (D) (cid:44) 1
n
Dα(cid:13)
2
2 + λ Ω(α),
(cid:13)

(cid:13)
(cid:13)x

i=1

,

(3)

where

1
2
and the matrix A is obtained by solving the linear regression

f (D, x) (cid:44) min
α∈Rk

−

) and
·

(cid:107) · (cid:107)
Ω(α) (cid:44) (1

min
A∈Rk×n

n
(cid:88)

i=1

1
2

(cid:13)
(cid:13)x(i)

Dα(i)(cid:13)
2
2 + λ Ω(α(i)).
(cid:13)

−

(4)

The problem (1) is non-convex in the parameters (D, A), and
hence (3) is not convex. However, the problem (1) is convex in
both D and A when ﬁxing one variable and optimizing with
respect to the other. As such, it is naturally solved by alternate
minimization over D and A, which asymptotically provides
a stationary point of (3). Yet, X has typically to be observed
hundred of times before obtaining a good dictionary. Alternate
minimization is therefore not adapted to datasets with many
samples.

B. Online matrix factorization

When X has a large number of columns but a limited
number of rows, the stochastic optimization method of [15]

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

3

Algorithm 1 Online matrix factorization (OMF) [15]

Input: Initial iterate D0, sample stream (xt)t>0, number of
iterations T .
for t from 1 to T do

∼ P

Draw xt
.
Compute αt = argminα∈Rp
Update the parameters of aggregated surrogate ¯gt:
(cid:17)

Dt−1α(cid:13)
2
2+λ Ω(α).
(cid:13)

(cid:13)
(cid:13)xt

−

(cid:16)

1
2

1

¯Ct =

¯Ct−1 +

1
t
1
t
Compute (using block coordinate descent):

¯Bt−1 +

αtα(cid:62)
t .

xtα(cid:62)
t .

¯Bt =

1
t
1
t

−

−

(cid:16)

(cid:17)

1

(8)

Dt = argmin

Tr (D(cid:62)D ¯Ct)

Tr (D(cid:62) ¯Bt).

−

1
2

D∈C

Output: Final iterate DT .

Algorithm 2 Stochastic majorization-minimization [SMM 10]
Input: Initial iterate θ0, weight sequence (wt)t>0, sample
stream (xt)t>0, number of iteration T .
for t from 1 to T do

Draw xt
∈
Construct a surrogate of ft near θt−1, that meets

, get ft : θ

f (xt, θ).

∼ P

→

Θ

gt

ft,

gt(θt−1) = ft(θt−1).

(12)

≥

Update the aggregated surrogate:

¯gt = (1

wt)¯gt−1 + wtgt.

−

Compute

θt = argmin

¯gt(θ).

θ∈Θ

(13)

Output: Final iterate θT .

outputs a good dictionary much more rapidly than alternate-
minimization. In this setting [see 32], learning the dictionary
is naturally formalized as an expected risk minimization

min
D∈C

¯f (D) (cid:44) Ex[f (D, x)],

where x is drawn from the data distribution and forms an i.i.d.
stream (xt)t. In the ﬁnite-sample setting, (5) reduces to (3)
when xt is drawn uniformly at random from
.
We then write it the sample number selected at time t.

[1, n]
}

x(i), i

∈

{

The online matrix factorization algorithm proposed in [15]
is summarized in Alg. 1. It draws a sample xt at each iteration,
and uses it to improve the current iterate Dt−1. For this, it
ﬁrst computes the code αt associated to xt on the current
dictionary:

αt (cid:44) argmin
α∈Rk

1
2 (cid:107)

xt

−

Dt−1α

2
2 + λΩ(α).
(cid:107)

(6)

Then, it updates Dt to make it optimal in reconstructing past
samples (xs)s≤t from previously computed codes (αs)s≤t:

Dt

argmin
D∈C

∈

(cid:16)

¯gt(D) (cid:44) 1
t

t
(cid:88)

s=1

1
2

(cid:13)
(cid:13)xs

−

Dαs

(cid:17)
(cid:13)
2
2 + λΩ(αs)
(cid:13)

.

(7)
Importantly, minimizing ¯gt is equivalent to minimizing the
quadratic function

D

→

1
2

Tr (D(cid:62)D ¯C(cid:62)
t )

Tr (D(cid:62) ¯Bt),

(9)

−

where ¯Bt and ¯Ct are small matrices that summarize previously
seen samples and codes:

¯Bt =

xsα(cid:62)
s

¯Ct =

αsα(cid:62)
s .

(10)

1
t

t
(cid:88)

s=1

1
t

t
(cid:88)

s=1

The function ¯gt is an upper-bound surrogate of the true
current empirical risk, whose deﬁnition involves the regression
minima computed on current dictionary D:

¯ft(D) (cid:44) 1
t

t
(cid:88)

s=1

(5)

min
α∈Rp

(cid:13)
(cid:13)xs

1
2

−

Dα(cid:13)
2
2+ λΩ(α)
(cid:13)

≤

¯gt(D). (11)

Using empirical processes theory [33], it is possible to show
that minimizing ¯ft at each iteration asymptotically yields
a stationary point of the expected risk (5). Unfortunately,
minimizing (11) is expensive as it involves the computation of
optimal current codes for every previously seen sample at each
iteration, which boils down to naive alternate-minimization.

In contrast, ¯gt is much cheaper to minimize than ¯ft, using
block coordinate descent. It is possible to show that ¯gt con-
verges towards a locally tight upper-bound of the objective ¯ft
and that minimizing ¯gt at each iteration also asymptotically
yields a stationary point of the expected risk (5). This es-
tablishes the correctness of the online matrix factorization
algorithm (OMF). In practice, the OMF algorithm performs a
single pass of block coordinate descent: the minimization step
is inexact. This heuristic will be justiﬁed by our theoretical
contribution in Section IV.

}

t
∈ T

Extensions: For efﬁciency, it is essential to use mini-batches
xs, s
of size η instead of single samples in the
{
iterations [15]. The surrogate parameters ¯Bt, ¯Ct are then
s∈Tt over
updated by the mean value of
}
the batch. The optimal size of the mini-batches is usually
close to k. (8) uses the sequence of weights ( 1
t )t to update
parameters ¯Bt and ¯Ct. [15] replaces these weights with a
sequence (wt)t, which can decay more slowly to give more
importance to recent samples in ¯gt. These weights will prove
important in our analysis.

(xsα(cid:62)
{

s , αsα(cid:62)
s )

C. Stochastic majorization-minimization

C

As the constraints
have a separable structure per atom, [15]
uses projected block coordinate descent to minimize ¯gt. The
¯Bt, and it is there-
function gradient writes
fore enough to maintain ¯Bt and ¯Ct in memory to solve (7).
¯Bt and ¯Ct are updated online, using the rules (8) (Alg. 1).

¯gt(D) = D ¯Ct

∇

−

Online matrix factorization belongs to a wider category
of algorithms introduced in [10] that minimize locally tight
upper-bounding surrogates instead of a more complex objec-
tive, in order to solve an expected risk minimization prob-
lem. Generalizing online matrix factorization, we introduce

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

4

−

(cid:107)

in Alg. 2 the stochastic majorization-minimization (SMM)
algorithm, which is at the core of our theoretical contribution.
In online matrix factorization, the true empirical risk func-
tions ¯ft and their surrogates ¯gt follow the update rules, with
generalized weight (wt)t set to ( 1
¯ft (cid:44) (1

wt)¯gt−1 + wtgt, (14)

t )t in (7) – (11):

wt) ¯ft−1 + wtft,

¯gt (cid:44) (1

−

where the pointwise loss function and its surrogate are

ft(D) (cid:44) min
α∈Rk
gt(D) (cid:44) 1
2 (cid:107)

xt

1
2 (cid:107)

xt

−

Dα

2
2 + λΩ(α),

Dαt

2
2 + λΩ(αt).
(cid:107)

−

(15)

≥

ft, and gt

The function gt is a majorizing surrogate of ft: gt
is tangent to ft in Dt−1, i.e, gt(Dt−1) = ft(Dt−1) and
(gt
ft)(Dt−1) = 0. At each step of online matrix factorization:
• The surrogate gt is computed along with αt, using (6).
• The parameters ¯Bt, ¯Ct are updated following (8). They
deﬁne the aggregated surrogate ¯gt up to a constant.
• The quadratic function ¯gt is minimized efﬁciently by
block coordinate descent, using parameters ¯Bt and ¯Ct
to compute its gradient.

∇

−

iteration t, a surrogate gt of the loss ft

The stochastic majorization-minimization framework sim-
ply formalizes the three steps above, for a larger variety of
loss functions ft(θ) (cid:44) f (θ, xt), where θ is the parameter we
want to learn (D in the online matrix factorization setting).
is computed
At
to update the aggregated surrogate ¯gt following (14). The
surrogate functions (gt)t should be upper-bounds of loss
functions (ft)t, tight
in the current iterate θt−1 (e.g., the
dictionary Dt−1). This simply means that ft(θt−1) = gt(θt−1)
gt)(θt−1) = 0. Computing ¯gt can be done if gt is
and
∇
−
deﬁned simply, as in OMF where it is linearly parametrized by
t , xtα(cid:62)
(αtα(cid:62)
t ). ¯gt is then minimized to obtain a new iterate θt.
It can be shown following [10] that stochastic majorization-
minimization algorithms ﬁnd asymptotical stationary point of
the expected risk Ex[f (θ, x)] under mild assumptions recalled
in Section IV. SMM admits the same mini-batch and decaying
weight extensions (used in Alg. 2) as OMF.

(ft

In this work, we extend the SMM framework and allow both
majorization and minimization steps to be approximated. As a
side contribution, our extension proves that performing a single
pass of block coordinate descent to update the dictionary, an
important heuristic in [15], is indeed correct. We ﬁrst introduce
the new matrix factorization algorithm at the core of this paper
and then present the extended SMM framework.

III. STOCHASTIC SUBSAMPLING FOR HIGH DIMENSIONAL
DATA DECOMPOSITION

The online algorithm presented in Section II is very efﬁcient
to factorize matrices that have a large number of columns (i.e.,
with a large number of samples n), but a reasonable number
of rows — the dataset is not very high dimensional. However,
it is not designed to deal with very high number of rows: the
cost of a single iteration depends linearly on p. On terabyte-
105 features, the original
scale datasets from fMRI with p = 2
online algorithm requires one week to reach convergence. This

·

Fig. 1. Stochastic subsampling further improves online matrix factorization to
handle datasets with large number of columns and rows. X is the input p × n
matrix, Dt and At are respectively the dictionary and code at time t.

is a major motivation for designing new matrix factorization
algorithms that scale in both directions.

In the large-sample regime p

k, the underlying dimen-
sionality of columns may be much lower than the actual p:
the rows of a single column drawn at random are therefore
correlated and redundant. This guides us on how to scale
online matrix factorization with regard to the number of rows:

(cid:29)

• The online algorithm OMF uses a single column of (or
mini-batch) of X at each iteration to enrich the average
surrogate and update the whole dictionary.

• We go a step beyond and use a fraction of a single column

of X to reﬁne a fraction of the dictionary.

More precisely, we draw a column and observe only some
to reﬁne these rows of the
of its rows at each iteration,
dictionary, as illustrated in Figure 1. To take into account
all features from the dataset, rows are selected at random at
each iteration: we call this technique stochastic subsampling.
Stochastic subsampling reduces the efﬁciency of the dictionary
update per iteration, as less information is incorporated in the
current iterate Dt. On the other hand, with a correct design,
the cost of a single iteration can be considerably reduced, as it
grows with the number of observed features. Section V shows
that the proposed algorithm is an order of magnitude faster
than the original OMF on large and redundant datasets.

First, we formalize the idea of working with a fraction of
the p rows at a single iteration. We adapt the online matrix
factorization algorithm, to reduce the iteration cost by a factor
close to the ratio of selected rows. This deﬁnes a new on-
line algorithm, called subsampled online matrix factorization
(SOMF). At each iteration, it uses q rows of the column xt to
update the sequence of iterates (Dt)t. As in Section II, we
introduce a more general algorithm, stochastic approximate
majorization-minimization (SAMM), of which SOMF is an
instance. It extends the stochastic majorization-minimization
framework, with similar theoretical guarantees but potentially
faster convergence.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

5

A. Subsampled online matrix factorization

∈

[1, n]

Formally, as in online matrix factorization, we consider a
sample stream (xt)t in Rp that cycles onto a ﬁnite sample set
x(i), i
{
1) Stochastic subsampling and algorithm outline: We want
to reduce the time complexity of a single iteration. In the
original algorithm, the complexity depends linearly on the
sample dimension p in three aspects:

, and minimize the empirical risk (3).1
}

∈

∈

∈

Rp×k,

Rp is used to compute the code αt,
• xt
• it is used to update the surrogate parameters ¯Bt
• Dt

Rp×k is fully updated at each iteration.
Our algorithm reduces the dimensionality of these steps at
each iteration, such that p becomes q = p
r in the time
complexity analysis, where r > 1 is a reduction factor.
Formally, we randomly draw, at iteration t, a mask Mt that
“selects” a random subset of xt. We use it to drop a part of the
features of xt and to “freeze” these features in dictionary D
at iteration t.

It is convenient to consider Mt as a Rp×p random diagonal
matrix, such that each coefﬁcient is a Bernouilli variable with
parameter 1
1],

r , normalized to be 1 in expectation.

[0, p

j
∀
, P(cid:2)Mt[j, j] = 0(cid:3) = 1

∈
1
r

.

−

−

(16)

P(cid:2)Mt[j, j] = r(cid:3) =

1
r

Thus, r describes the average proportion of observed features
and Mtxt is a non-biased, low-dimensional estimator of xt:

E(cid:2)
Mtxt
(cid:107)

(cid:107)

(cid:3) =

0

p
r

= q

E(cid:2)Mtxt

(cid:3) = xt.

(17)

(cid:107) · (cid:107)

0 counting the number of non-zero coefﬁcients. We
with
deﬁne the pair of orthogonal projectors Pt
t ∈
R(p−q)×p that project Rp onto Im(Mt) and Ker(Mt). In other
Rp×y
words, PtY and P⊥
with rows respectively selected and not selected by Mt. In
Rq×n assigns the rows of Z to the
algorithms, PtY
rows of Y selected by Pt, by an abuse of notation.

t Y are the submatrices of Y

Rq×p and P⊥

←

Z

∈

∈

∈

In brief, subsampled online matrix factorization, deﬁned in
Alg. 3, follows the outer loop of online matrix factorization,
with the following major modiﬁcations at iteration t:

• it uses Mtxt and low-size statistics instead of xt to

estimate the code αt and the surrogate gt,

• it updates a subset of the dictionary PtDt−1 to reduce
the surrogate value ¯gt(D). Relevant parameters of ¯gt are
computed using Ptxt and αt only.

We now present SOMF in details. For comparison purpose,
we write all variables that would be computed following the
OMF rules at iteration t with a (cid:63) superscript. For simplicity, in
Alg. 3 and in the following paragraphs, we assume that we use
one sample per iteration —in practice, we use mini-batches of
size η. The next derivations are transposable when a batch It
is drawn at iteration t instead of a single sample it.

2) Code computation: In the OMF algorithm presented in
t is obtained by solving (6), namely

Section II, α(cid:63)

α(cid:63)

t ∈

argmin
α

1
2

α(cid:62)G(cid:63)

t α

α(cid:62)β(cid:63)

t + λΩ(α),

(21)

−

Algorithm 3 Subsampled online matrix factorization (SOMF)
(wt)t>0,

iterate D0, weight

sequences

Initial

Input:
(γc)c>0, sample set
{
for t from 1 to T do

x(i)

}i>0, number of iterations T .
Draw xt = x(i) at random and Mt following (16).
Update the regression parameters for sample i:

c(i)
β(i)
G(i)

c(i) + 1,

(1

←
t ←
t ←
Compute the approximate code for xt:

t−1 + γD(cid:62)
t−1 + γD(cid:62)

t−1Mtx(i),
t−1MtDt−1, Gt

γ)G(i)
γ)G(i)

(1

−

−

γ
←
βt ←
←

γc(i) .
β(i)
.
t
¯G(i)
t

.

αt

argmin
α∈Rk

←

1
2

α(cid:62)Gtα

α(cid:62)βt + λ Ω(α).

(18)

−

Update the parameters of the aggregated surrogate ¯gt:

¯Ct
Pt ¯Bt

(1

(1

−

−

←

←

wt) ¯Ct−1 + wtαtα(cid:62)
t .
wt)Pt ¯Bt−1 + wtPtxtα(cid:62)
t .

Compute simultaneously (using Alg. 4 for 1st line):

PtDt

P⊥
t

¯Bt

argmin
Dr∈Cr
(1

1
2
wt)P⊥
t

−

←

←

Tr (Dr (cid:62)Dr ¯Ct)

Tr (Dr (cid:62)Pt ¯Bt).

−
¯Bt−1 + wtP⊥
t xtα(cid:62)
t .

(19)

(20)

Output: Final iterate DT .

t = D(cid:62)

t = D(cid:62)

t−1Dt−1 and β(cid:63)
t and β(cid:63)

where G(cid:63)
t−1xt. For large p, the
computation of G(cid:63)
t dominates the complexity of the
regression step, which depends almost linearly on p. To reduce
this complexity, we use estimators for G(cid:63)
t , computed
at a cost proportional to the reduced dimension q. We propose
three kinds of estimators with different properties.

t and β(cid:63)

a) Masked loss: The most simple unbiased estimation
t and β(cid:63)
t whose computation cost depends on q is

of G(cid:63)
obtained by subsampling matrix products with Mt:
Gt = D(cid:62)
βt = D(cid:62)

t−1MtDt−1
t−1Mtxt.

(a)

This is the strategy proposed in [26]. We use Gt and βt
in (18), which amounts to minimize the masked loss

min
α∈Rk

1
2 (cid:107)

Mt(xt

D(cid:62)

t−1α)
(cid:107)

−

2
2 + λΩ(α).

(22)

t and β(cid:63)

Gt and βt are computed in a number of operations pro-
portional to q, which brings a speed-up factor of almost r
in the code computation for large p. On large data, using
estimators (a) instead of exact G(cid:63)
t proves very ef-
ﬁcient during the ﬁrst epochs (cycles over the columns).2
However, due to the masking, Gt and βt are not consistent
estimators: they do not converge to G(cid:63)
t for large t,
which breaks theoretical guarantees on the algorithm output.
Empirical results in Section V-E show that the sequence of
iterates approaches a critical point of the risk (3), but may
then oscillate around it.

t and β(cid:63)

1Note that we solve the fully observed problem despite the use of subsam-

2Estimators (a) are also available in the inﬁnite sample setting, when

pled data, unlike other recent work on low-rank factorization [34].

minimizing expected risk (5) from a i.i.d sample stream (xt)t.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

6

{

x(i)

b) Averaging over epochs: At iteration t, the sample xt
}i. This allows to
is drawn from a ﬁnite set of samples
average estimators over previously seen samples and address
the non-consistency issue of (a). Namely, we keep in memory
2n estimators, written (G(i)
t )1≤i≤n. We observe the
t
sample i = it at iteration t and use it to update the i-th
estimators ¯G(i)
t
β(i)
G(i)

following
γ)G(i)
γ)G(i)

t−1Mtx(i)
t−1MtD(i)

t−1 + γD(cid:62)
t−1 + γD(cid:62)

t = (1

t = (1

, ¯β(i)

, β(i)

(23)

−

,

t

t

−

where γ is a weight factor determined by the number of time
the one sample i has been previously observed at time t.
Precisely, given (γc)c a decreasing sequence of weights,
c(i)
t =

t, xs = x(i)(cid:111)(cid:12)
(cid:12)
(cid:12) .

γ = γc(i)

where

(cid:110)
s

(cid:12)
(cid:12)
(cid:12)

t

≤

All others estimators
iteration t
−
averaged estimators
Gt (cid:44) G(i)

t =

G(j)
t
{
G(i)
1. The set
t
{

, β(j)
t }j(cid:54)=i are left unchanged from
, β(i)
t }1≤i≤n is used to deﬁne the

(cid:88)

γ(i)
s,tD(cid:62)

s−1MsDs−1

βt

(cid:44) β(i)

t =

γ(i)
s,tD(cid:62)

s−1Msx(i),

(b)

s≤t,xs=x(i)
(cid:88)

s≤t,xs=x(i)

(cid:81)

s,t = γc(i)

where γ(i)
). Using βt and Gt
in (18), αt minimizes the masked loss averaged over the
previous iterations where sample i appeared:

s<t,xs=x(i) (1

γc(i)

−

s

t

γ(i)
s,t
2 (cid:107)

min
α∈Rk

(cid:88)

s≤t
xs=x(i)

Ms(x(i)

D(cid:62)

2
2 + λΩ(α).
s−1α)
(cid:107)

−

(24)

t )t and (β(cid:63)

The sequences (Gt)t and (βt)t are consistent estimations
of (G(cid:63)
t )t — consistency arises from the fact
that a single sample x(i) is observed with different masks
along iterations. Solving (24) is made closer and closer to
solving (21), to ensure the correctness of the algorithm (see
Section IV). Yet, computing the estimators (b) is no more
costly than computing (a) and still permits to speed up a single
iteration close to r times. In the mini-batch setting, for every
and β(i)
to compute α(i)
It, we use the estimators G(i)
i
.
t
t
t
(n k2). This is reasonable
This method has a memory cost of
k2.
compared to the dataset size3 if p

∈

c) Exact Gram computation: To reduce the memory
usage, another strategy is to use the true Gram matrix Gt
and the estimator βt from (b):
t = D(cid:62)

O
(cid:29)

Gt (cid:44) G(cid:63)
βt

(cid:44) (cid:88)

t−1Dt−1
γ(i)
s,tD(cid:62)

s−1Msx(i)

(c)

s≤t,xs=x(i)

As previously, the consistency of (βt)t ensures that (5) is
correctly solved despite the approximation in (αt)t computa-
tion. With the partial dictionary update step we propose, it is
possible to maintain Gt at a cost proportional to q. The time

3It is also possible to efﬁciently swap the estimators (G(i)

t )i on disk, as

they are only accessed for i = it at iteration t.

TABLE I
COMPARISON OF ESTIMATORS USED FOR CODE COMPUTATION

Est.

(a)
(b)
(c)

βt

Gt

Convergence

Extra
mem. cost

Masked
Averaged
Averaged

Masked
Averaged
Exact

(cid:88)
(cid:88)

n k2
n k

1st epoch
perform.
(cid:88)
(cid:88)

O

complexity of the coding step is thus similarly reduced when
replacing (b) or (c) estimators in (21), but the latter option
(n k). Although estimators (c) are
has a memory usage in
slightly less performant in the ﬁrst epochs, they are a good
compromise between resource usage and convergence. We
summarize the characteristics of the three estimators (a)–(c) in
Table I, anticipating their empirical comparison in Section V.
Surrogate computation: The computation of αt using one

of the estimators above deﬁnes a surrogate gt(D) (cid:44) 1
2 (cid:107)
−
2
Dαt
2 + λΩ(α), which we use to update the aggregated
(cid:107)
surrogate ¯gt (cid:44) (1
wt)¯gt−1 + wtgt, as in online matrix
factorization. We follow (8) (with weights (wt)t) to update the
matrices ¯Bt and ¯Ct, which deﬁne ¯gt up to constant factors.
The update of ¯Bt requires a number of operations proportional
to p. Fortunately, we will see in the next paragraph that it is
possible to update Pt ¯Bt in the main thread with a number
of operation proportional to q and to complete the update of
P⊥
t

¯Bt in parallel with the dictionary update step.
Weight sequences: Speciﬁc (wt)t and (γc)c in Alg. 3
are required. We provide then in Assumption (B) of the
( 11
analysis: wt = 1
12 , 1) and
cv , where u
2(cid:1) to ensure convergence. Weights have little
(cid:0) 3
4 , 3u
v
impact on convergence speed in practice.

tu and γc = 1

xt

−

−

∈

∈

3) Dictionary update: In the original online algorithm, the
whole dictionnary Dt−1 is updated at iteration t. To reduce the
time complexity of this step, we add a “freezing” constraint to
the minimization (7) of ¯gt. Every row r of D that corresponds
to an unseen row r at iteration r (such that Mt[r, r] = 0)
remains unchanged. This casts the problem (7) into a lower
dimensional space. Formally, the freezing operation comes out
as a additional constraint in (7):

Dt =

argmin
D∈C
t D=P⊥

P⊥

t Dt−1

1
2

Tr (D(cid:62)D ¯Ct)

Tr (D(cid:62) ¯Bt).

(25)

−

1 and P⊥

The constraints are separable into two blocks of rows. Re-
calling the notations of (2), for each atom d(j), the rules
t d(j)
d(j)
t d(j) = P⊥
t−1 can indeed be rewritten
(cid:40)
d(j)
− (cid:107)
t d(j)
t−1.

Ptd(j)
(cid:107)
t d(j)
P⊥

1
= P⊥

Ptd(j)
(cid:107)

(cid:44) r(j)
t

t−1(cid:107)

t−1(cid:107)

(cid:107) ≤

(26)

(cid:107) ≤

+

(cid:107)

Solving (25) is therefore equivalent to solving the following
(cid:44) PtBt,
problem in Rq×k, with Br
t

Dr

∈
r =

argmin
Dr∈Cr
Dr
{

∈

1
2
Rq×k/
j
∀

where

Tr (Dr (cid:62)Dr ¯Ct)

Tr (Dr (cid:62) ¯Br

t ) (27)

−

1],

[0, k

dr(j)
(cid:107)

r(j)
t }

.

C

∈
The rows of Dt selected by Pt are then replaced with Dr,
while the other rows of Dt are unchanged from iteration

(cid:107) ≤

−

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

7

Algorithm 4 Partial dictionary update

Gt

D(cid:62)

t−1PtDt−1.

Input: Dictionary Dt−1, projector Pt, statistics ¯Ct, ¯Bt,
norms (n(j)
t−1)0≤j<k, Gram matrix Gt (optional).
Dt−1,
Dt
Gt
←
←
−
permutation([1, k]) do
for j
∈
n(j)
r(j)
Ptd(j)
t−1 +
.
t ←
t−1(cid:107)
(cid:107)
¯Ct[j,j] (Pt ¯b(j)
Ptd(j)
t−1 + 1
u
←
enet projection(u, r(j)
Ptd(j)
Ptd(j)
n(j)
.
t (cid:107)
t ←
t PtDt.
←

Gt+1
Output: Dictionary Dt, norms (n(j)

t ←
r(j)
t − (cid:107)
Gt + D(cid:62)

PtDt¯c(j)
t ).

t )j, Gram matrix Gt+1.

(cid:46) in Rq
(cid:46) in Rq

t −

t ).

t

t

∈

−

t −

¯br(j)
t

and ¯br(j)

t Dt = P⊥

, at an average cost in

1. Formally, PtDt = Dr and P⊥

k operations, as it writes Dr¯c(j)

t Dt−1. We
t
solve (27) by a projected block coordinate descent (BCD)
similar to the one used in the original algorithm, but performed
in a subspace of size q. We compute each column j of the
gradient that we use in the block coordinate descent loop
Rq,
with q
×
where ¯c(j)
are the j-th columns of ¯Ct and ¯Br
t .
t
Each reduced atom dr(j) is projected onto the elastic-net ball
of radius r(j)
(q) following [15]. This
makes the complexity of a single-column update proportional
to q. Performing the projection requires to keep in memory the
d(j)
(cid:44) 1
values
t (cid:107)}j, which can be updated online at
{
a negligible cost.
We provide the reduced dictionary update step in Alg. 4,
where we use the function enet projection(u, r) that per-
Rq onto the elastic-net
forms the orthogonal projection of u
ball of radius r. As in the original algorithm, we perform a
single pass over columns to solve (27). Dictionary update is
now performed with a number of operations proportional to q,
instead of p in the original algorithm. Thanks to the random
nature of (Mt)t, updating Dt−1 into Dt reduces ¯gt enough
to ensure convergence.

n(j)
t

− (cid:107)

O

∈

t with a cost in

Gram matrix computation: Performing partial updates
of Dt makes it possible to maintain the full Gram matrix
Gt = G(cid:63)
(q k2) per iteration, as mentioned
O
in III-A2c. It is indeed enough to compute the reduced Gram
matrix D(cid:62)PtD before and after the dictionary update:
D(cid:62)
t PtD(cid:62)
t .

Gt+1 = D(cid:62)

t−1 + D(cid:62)

t−1PtD(cid:62)

t Dt = Gt

(28)

−

Parallel surrogate computation: Performing block coor-
t = Pt ¯Bt only.
t requires to access ¯Br
dinate descent on ¯gr
Assuming we may use use more than two threads, this allows
to parallelize the dictionary update step with the update
of P⊥
t
Pt ¯Bt

¯Bt. In the main thread, we compute Pt ¯Bt following

wt) ¯PtBt−1 + wtPtxtα(cid:62)
t .

(19 – Alg. 3)

(1

←

−

which has a cost proportional to q. Then, we update in parallel
the dictionary and the rows of ¯Bt that are not selected by Mt:
¯Bt−1 + wtP⊥

(20 – Alg. 3)

t xtα(cid:62)
t .

wt)P⊥
t

P⊥
t

¯Bt

(1

←

−

This update requires k(p
q)η operations (one matrix-matrix
product) for a mini-batch of size η. In contrast, with appropri-
ate implementation, the dictionary update step requires 4 k q2

−

∼
∼

to 6 k q2 operations, among which 2 k q2 come from slower
η, updating ¯Bt is faster
matrix-vector products. Assuming k
10, and performing (20)
than updating the dictionary up to r
on a second thread is seamless in term of wall-clock time.
More threads may be used for larger reduction or batch size.
4) Subsampling and time complexity: Subsampling may be
used in only some of the steps of Alg. 3, with the other
steps following Alg. 1. Whether to use subsampling or not in
each step depends on the trade-off between the computational
speed-up it brings and the approximations it makes. It is useful
to understand how complexity of OMF evolves with p. We
write s the average number of non-zero coefﬁcients in (αt)t
(s = k when Ω =

2
2). OMF complexity has three terms:

(i)

(ii)

(p k2): computation of the Gram matrix Gt, update of

O
the dictionary Dt with block coordinate descent,

(p k η): computation of βt = D(cid:62)

t−1xt and of ¯Bt

(cid:107) · (cid:107)

O
using xtα(cid:62)
t ,

(iii)

(k s2 η): computation of αt using Gt and βt, using

O
matrix inversion or elastic-net regression.

∼

Using subsampling turns p into q = p
r in the expressions
above. It improves single iteration time when the cost of re-
(k s2 η) is dominated by another term. This happens
gression
O
whenever p
r > s2, where r is the reduction factor used in the
algorithm. Subsampling can bring performance improvement
p
up to r
s2 . It can be introduced in either computations
from (i) or (ii), or both. When using small batch size, i.e.,
when η < k, computations from (i) dominates complexity, and
subsampling should be ﬁrst introduced in dictionary update (i),
and for code computation (ii) beyond a certain reduction ratio.
On the other hand, with large batch size η > k, subsampling
should be ﬁrst introduced in code computation, then in the
dictionary update step. The reasoning above ignore potentially
large constants. The best trade-offs in using subsampling must
be empirically determined, which we do in Section V.

B. Stochastic approximate majorization-minimization

The SOMF algorithm can be understood within the stochastic
majorization-minimization framework. The modiﬁcations that
we propose are indeed perturbations to the ﬁrst and third steps
of the SMM presented in Algorithm 2:

• The code is computed approximately:

the surrogate
is only an approximate majorizing surrogate of ft
near Dt−1.

• The surrogate objective is only reduced and not mini-
mized, due to the added constraint and the fact that we
perform only one pass of block coordinate descent.

We propose a new stochastic approximate majorization-
minimization (SAMM) framework handling these perturbations:
• A majorization step (12 – Alg. 2), computes an approx-
g(cid:63)
t , where gt is a

imate surrogate of ft near θt−1: gt
true upper-bounding surrogate of ¯ft.

≈

• A minimization step (13 – Alg. 2), ﬁnds θt by reducing
(cid:44) argminθ∈Θ ¯gt(θ),

θ(cid:63)
t

enough the objective ¯gt: θt
which implies ¯gt(θt) (cid:38) ¯gt(θ(cid:63)

≈
t ).

The SAMM framework is general, in the sense that approxima-
tions are not speciﬁed. The next section provides a theoretical

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

8

analysis of the approximation of SAMM and establishes how
SOMF is an instance of SAMM. It concludes by establishing
Proposition 1, which provides convergence guarantees for
SOMF, under the same assumptions made for OMF in [15].

IV. CONVERGENCE ANALYSIS
We establish the convergence of SOMF under reasonable
assumptions. For the sake of clarity, we ﬁrst state our principal
result (Proposition 1), that guarantees SOMF convergence. It is
a corollary of a more general result on SAMM algorithms. To
present this broader result, we recall the theoretical guarantees
of the stochastic majorization-minimization algorithm [10]
(Proposition 2); then, we show how the algorithm can with-
stand pertubations (Proposition 3). Proofs are reported in
Appendix A. SAMM convergence is proven before establishing
SOMF convergence as a corollary of this broader result.

A. Convergence of SOMF

Similar to [15, 34], we show that the sequence of iterates
(Dt)t asymptotically reaches a critical point of the empirical
risk (3). We introduce the same hypothesis on the code
covariance estimation ¯Ct as in [15] and a similar one on Gt —
they ensure strong convexity of the surrogate and boundedness
of (αt)t. They do not cause any loss of generality as they are
met in practice after a few iterations, if r is chosen reasonably
low, so that q > k. The following hypothesis can also be
guaranteed by adding small (cid:96)2 regularizations to ¯f .

(A) There exists ρ > 0 such that for all t > 0, ¯Ct, Gt

ρI.

(cid:31)

We further assume, that the weights (wt)t and (γc)c decay
at speciﬁc rates. We specify simple weight sequences, but the
proofs can be adapted for more complex ones.

(B) There exists u

(cid:0) 3
4 , 3u
for all t > 0, c > 0, wt = t−u, γc (cid:44) c−v.

( 11
12 , 1) and v

∈

∈

−

2) such that,

The following convergence result then applies to any se-
quence (Dt)t produced by SOMF, using estimators (b) or (c).
¯f is the empirical risk deﬁned in (3).

Proposition 1 (SOMF convergence). Under assumptions (A)
and (B), ¯f (Dt) converges with probability one and every limit
point D∞ of (Dt)t is a stationary point of ¯f : for all D

¯f (D∞, D

D∞)

0

∇

−
This result applies for any positive subsampling ratio r,
which may be set arbitrarily high. However, selecting a
reasonable ratio remains important for performance.

≥

Proposition 1 is a corollary of a stronger result on SAMM
algorithms. As it provides insights on the convergence mech-
anisms, we formalize this result in the following.

∈ C
(29)

B. Basic assumptions and results on SMM convergence

We ﬁrst recall the main results on stochastic majorization-
minimization algorithms, established in [10], under assump-
tions that we slightly tighten for our purpose. In our setting,
we consider the empirical risk minimization problem
(cid:16) ¯f (θ) (cid:44) 1
n

(cid:17)
f (θ, x(i))

min
θ∈Θ

n
(cid:88)

(30)

,

i=1

where f : RK

R is a loss function and

× X →

(C) Θ

RK and the support

of the data are compact.

⊂

X

This is a special case of (5) where the samples (xt)t are
i. The loss functions ft (cid:44)
x(i)
{
, xt) deﬁned on RK can be non-convex. We instead assume
·

drawn uniformly from the set
f (
that they meet reasonable regularity conditions:

}

(D) (ft)t is uniformly R-Lipschitz continuous on RK and

uniformly bounded on Θ.

(E) The directional derivatives [35]
¯f (θ, θ(cid:48)

∇
θ) exist for all θ and θ(cid:48) in RK.

ft(θ, θ(cid:48)

θ) and

−

∇

−

Assumption (E) allows to characterize the stationary points
0
Θ such that
of problem (30), namely θ
for all θ(cid:48)
Θ — intuitively a point is stationary when there
is no local direction in which the objective can be improved.
the deﬁnition of ﬁrst-order surrogate
functions used in the SMM algorithm. (gt)t are selected in
the set

ρ,L(ft, θt−1), hereby introduced.

Let us now recall

¯f (θ, θ(cid:48)

θ)

∇

−

≥

∈

∈

S

Deﬁnition 1 (First-order surrogate function). Given a function
f : RK
R, θ
ρ,L(f, θ) as
the set of functions g : RK

Θ and ρ, L > 0, we deﬁne

R such that

→

∈

S

→
• g is majorizing f on Θ and g is ρ-strongly convex,
• g and f are tight at θ — i.e., g(θ) = f (θ), g

differentiable,

f ) is L-Lipschitz,

(g

∇

−

(g

∇

−

In OMF, gt deﬁned in (15) is a variational surrogate4 of ft.
We refer the reader to [36] for further examples of ﬁrst-order
surrogates. We also ensure that ¯gt should be parametrized and
thus representable in memory. The following assumption is
met in OMF, as ¯gt is parametrized by the matrices ¯Ct and ¯Bt.

f is
−
f )(θ) = 0.

(F) Parametrized surrogates. The surrogates (¯gt)t are
RP . Namely,
such that ¯gt is unequivocally

parametrized by vectors in a compact set
for all t > 0, there exists κt
deﬁned as gt (cid:44) ¯gκt.

K ⊂

∈ K

Finally, we ensure that the weights (wt)t used in Alg. 2

decrease at a certain rate.

(G) There exists u

( 3
4 , 1) such that wt = t−u.

∈

When (θt)t is the sequence yielded by Alg. 2, the following
result (Proposition 3.4 in [10]) establishes the convergence
of ( ¯f (θt))t and states that θt is asymptotically a stationary
point of the ﬁnite sum problem (30), as a special case of the
expected risk minimization problem (5).

Proposition 2 (Convergence of SMM, from [10]). Under as-
sumptions (C) – (G), ( ¯f (θt))t≥1 converges with probability
one. Every limit point θ∞ of (θt)t is a stationary point of the
risk ¯f deﬁned in (30). That is,

θ
∀

∈

Θ,

∇

¯f (θ∞, θ

θ∞)

0.

≥

−

(31)

The correctness of the online matrix factorization algorithm

can be deduced from this proposition.

4In this case as in SOMF, gt is not ρ-strongly convex but ¯gt is, thanks to

assumption (A). This is sufﬁcient in the proofs of convergence.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

9

C. Convergence of SAMM

We now introduce assumptions on the approximations made
in SAMM, before extending the result of Proposition 2. We
make hypotheses on both the surrogate computation (ma-
jorization) step and the iterate update (minimization) step. The
principles of SAMM are illustrated in Figure 2, which provides
a geometric interpretation of the approximations introduced in
the following assumptions (H) and (I).

1) Approximate surrogate computation: The SMM algo-
rithm selects a surrogate for ft at point θt−1 within the set
ρ,L(ft, θt−1). Surrogates within this set are tight at θt−1
S
and greater than ft everywhere. In SAMM, we allow the use
of surrogates that are only approximately majorizing ft and
approximately tight at θt−1. This is indeed what SOMF does
when using estimators in the code computation step. For that
ρ,L(f, θ, (cid:15)), that contains all
purpose, we introduce the set
T
ρ,L(f, θ) for the (cid:96)∞-norm:
functions (cid:15)-close of a surrogate in

S

Deﬁnition 2 (Approximate ﬁrst-order surrogate function).
Given a function f : RK
R, θ
ρ,L(f, θ, (cid:15))
→
is the set of ρ-strongly convex functions g : RK
R
such that

Θ and (cid:15) > 0,

→

∈

T

• g is (cid:15)-majorizing f on Θ:
• g and f are (cid:15)-tight at θ — i.e., g(θ)

∈

κ

∀

Θ, g(κ)

−
f (θ)

f (κ)

is differentiable,

(g

∇

−

f ) is L-lipschitz.

−

(cid:15),
f

≥ −
(cid:15), g
−

≤

We assume that SAMM selects an approximative surrogate
ρ,L(ft, θt−1, (cid:15)t) at each iteration, where ((cid:15)t)t is a deter-
in
ministic or random non-negative sequence that vanishes at a
sufﬁcient rate.

T

∈ O

∞ 0 almost surely.

(H) For all t > 0, there exists (cid:15)t > 0 such that gt
∈
ρ,L(ft, θt−1, (cid:15)t). There exists a constant η > 0 such that
T
E[(cid:15)t]

(t2(u−1)−η) and (cid:15)t

→
As illustrated on Figure 2, given the OMF surrogate g(cid:63)
t ∈
ρ,L(ft, θt−1) deﬁned in (15), any function gt such that
S
ρ,L(ft, θt−1, (cid:15)) — e.g., where gt uses
gt
(cid:107)
an approximate αt in (15). This assumption can also be met in
matrix factorization settings with difﬁcult code regularizations,
that require to make code approximations.

∞ < (cid:15) is in

g(cid:63)
t (cid:107)

−

T

2) Approximate surrogate minimization: We do not re-
quire θt to be the minimizer of ¯gt any longer, but ensure that
the surrogate objective function ¯gt decreases “fast enough”.
Namely, θt obtained from partial minimization should be
closer to a minimizer of ¯gt than θt−1. We write (
t)t and
(
the ﬁltrations induced by the past of the algorithm,
Ft− 1
respectively up to the end of iteration t and up to the beginning
of the minimization step in iteration t. Then, we assume

F

)

t

2

(I) For all t > 0, ¯gt(θt) < ¯gt(θt−1). There exists µ > 0

such that, for all t > 0, where θ(cid:63)

t = argminθ∈Θ ¯gt(θ),

(1

µ)(¯gt(θt−1)

¯gt(θ(cid:63)

t )). (32)

E[¯gt(θt)

¯gt(θ(cid:63)
t )

]

|Ft− 1

2

≤

−

−
Assumption (I) is met by choosing an appropriate method
for the inner ¯gt minimization step — a large variety of
gradient-descent algorithms indeed have convergence rates of
the form (32). In SOMF, the block coordinate descent with
frozen coordinates indeed meet this property, relying on results

−

Fig. 2. Both steps of SAMM make well-behaved approximations. The
operations that are performed in exact SMM are in green and superscripted
by (cid:63), while the actual computed values are in orange. Light bands recall the
bounds on approximations assumed in (H) and (I).

from [37]. When both assumptions are met, SAMM enjoys the
same convergence guarantees as SMM.

3) Asymptotic convergence guarantee: The following
proposition guarantees that the stationary point condition of
Proposition 2 holds for the SAMM algorithm, despite the use
of approximate surrogates and approximate minimization.

Proposition 3 (Convergence of
tions (C) – (I),
for SAMM.

SAMM). Under assump-
the conclusion of Proposition 2 holds

Assumption (H) is essential to bound the errors introduced
by the sequence ((cid:15)t)t in the proof of Proposition 3, while (I)
is the key element to show that the sequence of iterates (θt)t
is stable enough to ensure convergence. The result holds for
any subsampling ratio r, provided that (A) remains true.

4) Proving SOMF convergence: Assumptions (A) and (B)
readily implies (C)–(G). With Proposition 3 at hand, proving
Proposition 1 reduces to ensure that the surrogate sequence of
SOMF meets (H) while its iterate sequence meets (I).

V. EXPERIMENTS

The SOMF algorithm is designed for datasets with large
number of samples n and large dimensionality p. Indeed,
as detailed in Section III-A, subsampling removes the com-
putational bottlenecks that arise from high dimensionality.
Proposition 1 establishes that the subsampling used in SOMF
is safe, as it enjoys the same guarantees as OMF. However,
as with OMF, no convergence rate is provided. We therefore
perform a strong empirical validation of subsampling.

We tackle two different problems, in functional Magnetic
Resonance Imaging (fMRI) and hyperspectral imaging. Both
involve the factorization of very large matrices X with sparse
factors. As the data we consider are huge, subsampling reduces
the time of a single iteration by a factor close to p
q . Yet it
is also much redundant: SOMF makes little approximations
and accessing only a fraction of the features per iteration
should not hinder much the reﬁnement of the dictionary. Hence
high speed-ups are expected — and indeed obtained. All
experiments can be reproduced using open-source code.

A. Problems and datasets

1) Functional MRI: Matrix factorization has long been
used on functional Magnetic Resonance Imaging [18]. Data
are temporal series of 3D images of brain activity and are

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

10

decomposed into spatial modes capturing regions that activate
synchronously. They form a matrix X where columns are
the 3D images, and rows corresponds to voxels. Interesting
dictionaries for neuroimaging capture spatially-localized com-
ponents, with a few brain regions. This can be obtained by
enforcing sparsity on the dictionary: we use an (cid:96)2 penalty and
the elastic-net constraint. SOMF streams subsampled 3D brain
records to learn the sparse dictionary D. Data can be huge:
106 (2000
we use the whole HCP dataset [38], with n = 2.4
105, totaling 2 TB
records, 1 200 time points) and p = 2
of dense data. For comparison, we also use a smaller public
dataset (ADHD200 [39]) with 40 records, n = 7000 samples
104 voxels. Historically, brain decomposition have
and p = 6
been obtained by minimizing the classical dictionary learning
objective on transposed data [40]: the code A holds sparse
spatial maps and voxel time-series are streamed. This is not a
natural streaming order for fMRI data as X is stored column-
wise on disk, which makes the sparse dictionary formulation
more appealing. Importantly, we seek a low-rank factorization,
to keep the decomposition interpretable — k

100

p.

·

·

·

2) Hyperspectral imaging: Hyperspectral cameras acquire
images with many channels that correspond to different spec-
tral bands. They are used heavily in remote sensing (satellite
imaging), and material study (microscopic imaging). They
yield digital images with around 1 million pixels, each as-
sociated with hundreds of spectral channels. Sparse matrix
factorization has been widely used on these data for image
classiﬁcation [41, 42] and denoising [43, 44]. All methods
rely on the extraction of full-band patches representing a local
image neighborhood with all channels included. These patches
are very high dimensional, due to the number of spectral
bands. From one image of the AVIRIS project [45], we extract
106 patches of size 16
n = 2
16 with 224 channels, hence
·
104. A dense dictionary is learned from these patches. It
p = 6
·
should allow a sparse representation of samples: we either use
the classical dictionary learning setting ((cid:96)1/elastic-net penalty),
or further add positive constraints to the dictionary and codes:
both methods may be used and deserved to be benchmarked.
p.
We seek a dictionary of reasonable size: we use k

256

×

∼

(cid:28)

∼

(cid:28)

B. Experimental design

To validate the introduction of subsampling and the useful-

ness of SOMF, we perform two major experiments.

• We measure the performance of SOMF when increasing
the reduction factor, and show beneﬁts of stochastic
dimension reduction on all datasets.

• We assess the importance of subsampling in each of
the steps of SOMF. We compare the different approaches
proposed for code computation.

Validation: We compute the objective function (3) over a
test set to rule out any overﬁtting effect — a dictionary should
be a good representation of unseen samples. This criterion is
always plotted against wall-clock time, as we are interested in
the performance of SOMF for practitioners.

Tools: To perform a valid benchmark, we implement OMF
and SOMF using Cython [46] We use coordinate descent [47]
to solve Lasso problems with optional positivity constraints.

TABLE II
SUMMARY OF EXPERIMENTAL SETTINGS

Field

Dataset

Functional MRI

Hyperspectral imaging

ADHD

HCP

Patches from AVIRIS

Factors
# samples n
# features p
X size
Use case ex.

D sparse, A dense
2 · 106
2 · 105
2 TB

7 · 103
6 · 104
2 GB
Extracting predictive feature

D dense, A sparse
2 · 106
6 · 104
103 GB
Recognition / denoising

Code computation is parallelized to handle mini-batches. Ex-
periments use scikit-learn [48] for numerics, and nilearn [49]
for handling fMRI data. We have released the code in an open-
source Python package5. Experiments were run on 3 cores of
¯Bt is
an Intel Xeon 2.6GHz, in which case computing P⊥
t
faster than updating PtDt.

Parameter setting: Setting the number of components k
and the amount of regularization λ is a hard problem in the
absence of ground truth. Those are typically set by cross-
validation when matrix factorization is part of a supervised
pipeline. For fMRI, we set k = 70 to obtain interpretable
networks, and set λ so that the decomposition approximately
covers the whole brain (i.e., every map is k
70 ) sparse). For
hyperspectral images, we set k = 256 and select λ to obtain
a dictionary on which codes are around 3% sparse. We cycle
randomly through the data (fMRI records, image patches) until
convergence, using mini-batches of size η = 200 for HCP
and AVIRIS, and η = 50 for ADHD (small number of sam-
ples). Hyperspectral patches are normalized in the dictionary
learning setting, but not in the non-negative setting — the
classical pre-conditioning for each case. We use u = 0.917
and v = 0.751 for weight sequences.

C. Reduction brings speed-up at all data scales

We benchmark SOMF for various reduction factors against
the original online matrix factorization algorithm OMF [15],
on the three presented datasets. We stream data in the same
order for all reduction factors. Using variant (c) (true Gram
matrix, averaged βt) performs slightly better on fMRI datasets,
whereas (b) (averaged Gram matrix and βt) is slightly faster
for hyperspectral decomposition. For comparison purpose, we
display results using estimators (b) only.

Figure 3 plots the test objective against CPU time. First,
we observe that all algorithms ﬁnd dictionaries with very
close objective function values for all reduction factors, on
each dataset. This is not a trivial observation as the matrix
factorization problem (3) is not convex and different runs of
OMF and SOMF may converge towards minima with different
values. Second, and most importantly, SOMF provides signif-
icant improvements in convergence speed for three different
sizes of data and three different factorization settings. Both ob-
servations conﬁrm the relevance of the subsampling approach.
Quantitatively, we summarize the speed-ups obtained in
Table III. On fMRI data, on both large and medium datasets,
SOMF provides more than an order of magnitude speed-up.

5https://github.com/arthurmensch/modl

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

11

Fig. 3. Subsampling provides signiﬁcant speed-ups on all fMRI and hyperspectral datasets. A reduction factor of 12 is a good overall choice. With larger
data, larger reduction factors can be used for better performance — convergence is reached 13× faster than state-of-the-art methods on the 2TB HCP dataset.

TABLE III
TIME TO REACH CONVERGENCE (< 1% TEST OBJECTIVE)

Dataset

ADHD

Algorithm OMF

SOMF OMF

AVIRIS (NMF) AVIRIS (DL)
SOMF OMF

SOMF OMF

SOMF

HCP

Conv. time 6 min 28 s 2 h 30 43 min 1 h 16 11 min 3 h 50 17 min
Speed-up

13.31

11.8

3.36

6.80

Fig. 5. Proﬁling OMF and SOMF on HCP. Partial dictionary update removes
the major bottleneck of online matrix factorization for small reductions. For
higher reduction, parameter update and code computation must be subsampled
to further reduce the iteration time.

Our ﬁrst experiment establishes the power of stochastic
subsampling as a whole. In the following two experiments, we
reﬁne our analysis to show that subsampling is indeed useful
in the three steps of online matrix factorization.

D. For each step of SOMF, subsampling removes a bottleneck

∼

In Section III, we have provided theoretical guidelines on
when to introduce subsampling in each of the three steps of
an iteration of SOMF. This analysis predicts that, for η
k,
we should ﬁrst use partial dictionary update, before using
approximate code computation and asynchronous parameter
aggregation. We verify this by measuring the time spent by
SOMF on each of the updates for various reduction factors,
on the HCP dataset. Results are presented in Figure 5. We
observe that block coordinate descent is indeed the bottle-
neck in OMF. Introducing partial dictionary update removes
this bottleneck, and as the reduction factor increases, code
computation and surrogate aggregation becomes the major
bottlenecks. Introducing subsampling as described in SOMF
overcomes these bottlenecks, which rationalizes all steps of
SOMF from a computational point of view.

E. Code subsampling becomes useful for high reduction

It remains to assess the performance of approximate code
computation and averaging techniques used in SOMF. Indeed,

Fig. 4. Given a 3 minute time budget, the atoms learned by SOMF are
more focal and less noisy that those learned by OMF. They are closer to
the dictionary of ﬁrst line, for which convergence has been reached.

Practitioners working on datasets akin to HCP can decompose
their data in 20 minutes instead of 4 h previously, while
working on a single machine. We obtain the highest speed-ups
for the largest dataset — accounting for the extra redundancy
that usually appears when dataset size increase. Up to r
8,
speed-up is of the order of r — subsampling induces little
noise in the iterate sequence, compared to OMF. Hyperspectral
decomposition is performed near 7
faster than with OMF in
the classical dictionary learning setting, and 3
in the non-
negative setting, which further demonstrates the versatility of
SOMF. Qualitatively, given a certain time budget, Figure 4
compares the results of OMF and the results of SOMF with
a subsampling ratio r = 24, in the non-negative setting. Our
algorithm yields a valid smooth bank of ﬁlters much faster.
The same comparison has been made for fMRI in [26].

∼

×

×

Comparison with stochastic gradient descent: It is possible
to solve (3) using the projected stochastic gradient (SGD)
algorithm [50]. On all
tested settings, for high precision
convergence, SGD (with the best step-size among a grid) is
slower than OMF and even slower than SOMF. In the dictionary
learning setting, SGD is somewhat faster than OMF but slower
than SOMF in the ﬁrst epochs. Compared to SOMF and OMF,
SGD further requires to select the step-size by grid search.

Limitations: Table III reports convergence time within 1%,
which is enough for application in practice. SOMF is less
beneﬁcial when setting very high precision: for convergence
within 0.01%, speed-up for HCP is 3.4. This is expected
as SOMF trades speed for approximation. For high precision
convergence, the reduction ratio can be reduced after a few
epochs. As expected, there exists an optimal reduction ratio,
depending on the problem and precision, beyond which per-
formance reduces: r = 12 yields better results than r = 24 on
AVIRIS (dictionary learning) and ADHD, for 1% precision.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

12

Fig. 6. Approximating code computation with the proposed subsampling method further accelerates the convergence of SOMF. Reﬁning code computation
using past iterations (averaged estimates) performs better than simply performing a subsampled linear regression as in [26]

subsampling for code computation introduces noise that may
undermine the computational speed-up. To understand the
impact of approximate code computation, we compare three
strategies to compute (αt)t on the HCP dataset. First, we
compute (α(cid:63)
t )t from (xt)t using (21). Subsampling is thus
used only in dictionary update. Second, we rely on masked,
non-consistent estimators (a), as in [26] — this breaks conver-
gence guarantees. Third, we use averaged estimators (βt, Gt)
from (c) to reduce the variance in (αt)t computation.

∈ {

Fig. 6 compares the three strategies for r

12, 24
.
}
Partial minimization at each step is the most important part
to accelerate convergence: subsampling the dictionary updates
already allows to outperforms OMF. This is expected, as
dictionary update constitutes the main bottleneck of OMF in
large-scale settings. Yet, for large reduction factors, using
subsampling in code computation is important
to further
accelerate convergence. This clearly appears when comparing
the plain and dashed black curves. Using past estimates to
better approximate (αt)t yields faster convergence than the
non-converging, masked loss strategy (a) proposed in [26].

VI. CONCLUSION

In this paper, we introduce SOMF, a matrix-factorization
algorithm that can handle input data with very large number of
rows and columns. It leverages subsampling within the inner
loop of a streaming algorithm to make iterations faster and
accelerate convergence. We show that SOMF provides a sta-
tionary point of the non-convex matrix factorization problem.
To prove this result, we extend the stochastic majorization-
minimization framework to two major approximations. We
assess the performance of SOMF on real-world large-scale
problems, with different sparsity/positivity requirements on
learned factors. In particular, on fMRI and hyperspectral data
decomposition, we show that
the use of subsampling can
speed-up decomposition up to 13 times. The larger the dataset,
the more SOMF outperforms state-of-the art techniques, which
is very promising for future applications. This calls for adap-
tation of our approach to learn more complex models.

APPENDIX A
PROOFS OF CONVERGENCE

This appendix contains the detailed proofs of Proposition 3
and Proposition 1. We ﬁrst introduce three lemmas that will be
crucial to prove SAMM convergence, before establishing it by
proving Proposition 3. Finally, we show that SOMF is indeed

∇

an instance of SAMM (i.e. meets the assumptions (C)–(I)),
proving Proposition 1.

A. Basic properties of the surrogates, estimate stability

We derive an important result on the stability and optimality
of the sequence (θt)t, formalized in Lemma 3 — introduced
in the main text. We ﬁrst introduce a numerical lemma on
the boundedness of well-behaved determistic and random
sequence. The proof is detailed in Appendix B.

R, t0

Lemma 1 (Bounded quasi-geometric sequences). Let (xt)t be
a sequence in R+, u : R
R
N and α
[0, 1)
→
αxt−1 + u(xt, xt−1), where
such that, for all t
. Then (xt)t is bounded.
u(x, y)

≤
→ ∞
Let now (Xt)t be a random sequence in R+, such that
t)t the ﬁltration adapted to (Xt)t.

∈
E[Xt] <
If, for all t > t0, there exists a σ-algebra

o(x + y) for x, y

. We deﬁne (

t(cid:48) such that

×
t0, xt

∞

t−1

≥

F

∈

∈

F

F

⊆

t(cid:48)

F

⊆ F

t and

E[Xt

t(cid:48)]

αXt−1 + u(Xt, Xt−1),

(33)

|F
then (Xt)t is bounded almost surely.

≤

We ﬁrst derive some properties of the approximate surrogate

functions used in SAMM. The proof is adapted from [10].

Lemma 2 (Basic properties of approximate surrogate func-
tions). Consider any sequence of iterates (θt)t and assume
L,ρ(ft, θt−1, (cid:15)) for all
there exists (cid:15) > 0 such that gt
1, ¯h0 (cid:44) h0 and
t
≥
¯ht (cid:44) (1
(i) (

∈ T
ft for all t
wt)¯ht−1 + wtht. Under assumptions (D) – (G),
−
ht(θt−1))t>0 is uniformly bounded and there exists

1. Deﬁne ht (cid:44) gt

≥

−

(ii) (ht)t and (¯ht)t are uniformly R(cid:48)-Lipschitz, (gt)t and

ht

}t is uniformly bounded by R(cid:48).

∇
R(cid:48) such that

{∇

(¯gt)t are uniformly (R + R(cid:48))-Lipschitz.

Proof. We ﬁrst prove (i). We set α > 0 and deﬁne θ(cid:48) =
. As ht has a L-Lipschitz gradient on RK,
θt
using Taylor’s inequality (see Appendix B)

α ∇ht(θt)
(cid:107)∇ht(θt)(cid:107)2

−

(34)

ht(θ(cid:48))

ht(θt)

α

−

≤

Lα2
2

2 +

ht(θt)
(cid:107)∇
(cid:107)
ht(θ(cid:48))) +

(cid:107)∇

1
α

ht(θt)
2
(cid:107)

≤
where we use ht(θt) < (cid:15) and
assumption gt

Lα
Lα
2
2 ≤
ht(θ(cid:48)
(cid:15) from the
t)
L,ρ(ft, θt−1, (cid:15)). Moreover, by deﬁnition,

(ht(θt)

2
α

(cid:15) +

−

−

≤

,

ht exists and is L-lipschitz for all t. Therefore,

t

1,

∈ T

ht(θ)
2
(cid:107)

(cid:107)∇

≤ (cid:107)∇

2 + L
ht(θt)
(cid:107)
(cid:107)

θt−1

−

≥

∀
(cid:107)2

θ

(35)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

13

Since Θ is compact and (

(cid:107)2)t≥1 is bounded in (34),
ht(θt)
ht is bounded by R(cid:48) independent of t. (ii) follows by basic

(cid:107)∇

where the second inequality holds for the same reasons as
in (41). Injecting (40) and (42) in (43), we obtain

∇
considerations on Lipschitz functions.

Finally, we prove a result on the stability of the estimates,
that derives from combining the properties of (gt)t and the
geometric decrease assumption (I).

Lemma 3 (Estimate stability under SAMM approximation). In
the same setting as Lemma 2, with the additional assump-
tion (I) (expected linear decrease of ¯gt suboptimality), the
2 converges to 0 as fast as (wt)t, and θt
sequence
(cid:107)
is asymptotically an exact minimizer. Namely, almost surely,

θt
(cid:107)

θt−1

−

θt
(cid:107)

−

θt−1

(cid:107)2 ∈ O

(wt) and ¯gt(θt)

¯gt(θ(cid:63)
t )

−

∈ O

(w2

t ).

(36)

Proof. We ﬁrst establish the result when a deterministic ver-
sion of (I) holds, as it makes derivations simpler to follow.

1) Determistic decrease rate: We temporarily assume that

decays are deterministic.

(Idet) For all t > 0, ¯gt(θt) < ¯gt(θt−1). Moreover, there

exists µ > 0 such that, for all t > 0
¯gt(θ(cid:63)
t )
(1
θ(cid:63)
t = argmin

−
where

¯gt(θt)

≤

−

µ)(¯gt(θt−1)
¯gt(θ),

θ∈Θ

¯gt(θ(cid:63)

t ))

−

We introduce the following auxiliary positive values, that

we will seek to bound in the proof:

At (cid:44)
θt
(cid:107)
−
θ(cid:63)
θ(cid:63)
t−1(cid:107)
t −
(cid:107)

Ct (cid:44)

2, Bt (cid:44)
θt
θt−1
(cid:107)
(cid:107)
2, Dt (cid:44) ¯gt(θt)

θ(cid:63)
2,
t (cid:107)
−
¯gt(θ(cid:63)
t ).

−

Our goal is to bound At. We ﬁrst relate it to Ct and Bt using
convexity of (cid:96)2 norm:

A2

t + 3B2
t is the minimizer of ¯gt, by strong convexity of (¯gt)t,

t−1 + 3C 2
t .

3B2

t ≤

(39)

As θ(cid:63)

ρ
2
while we also have
ρ
θ(cid:63)
2
2 ≤
t−1(cid:107)
2 (cid:107)
wt)(cid:0)¯gt−1(θ(cid:63)

θ(cid:63)
t −
(1

≤

−

B2

t =

ρ
2 (cid:107)

θt

2
θ(cid:63)
2 ≤
t (cid:107)

−

Dt,

¯gt(θ(cid:63)

t−1)

t−1)

−

¯gt(θ(cid:63)
t )
−
t )(cid:1)+wt
¯gt−1(θ(cid:63)

(cid:0)gt(θ(cid:63)

gt(θ(cid:63)

t )(cid:1)

(41)

t−1)
2Q
ρ

−
.

2, and thus Ct

wt

wt(R + R(cid:48))

θ(cid:63)
t −
(cid:107)

θ(cid:63)
t−1(cid:107)

≤
≤
The second inequalities holds because θ(cid:63)
t−1 is a minimizer
of ¯gt−1 and gt is Q-Lipschitz, where Q (cid:44) R + R(cid:48), using
Lemma 2. Replacing (40) and (41) in (39) yields

A2

(Dt + Dt−1) +

t ≤
and we are left to show that Dt
this, we decompose the inequality from (Idet) into

∈ O

w2
t ,

12Q2
ρ
(w2
t ) to conclude. For

(42)

6
ρ

Dt

≤
= (1

(1

µ)(¯gt(θt−1)
−
(cid:16)
(cid:0)gt(θt−1)

−
µ)

t ))

¯gt(θ(cid:63)
gt(θt)(cid:1) + wt

(cid:0)gt(θt)

gt(θ(cid:63)

t )(cid:1)(cid:17)

−
+ (1

µ)

(1

−

−
+ (1

−

−
wt)(cid:0)¯gt−1(θt−1)
wt)(cid:0)¯gt−1(θ(cid:63)

−
t−1)

¯gt−1(θ(cid:63)

−
t−1)(cid:1)
t )(cid:1)(cid:17)
¯gt−1(θ(cid:63)

−

wt
(cid:16)

(1

µ)(wtQ(At + Bt) + Dt−1),

≤

−

(37)

(38)

(40)

w2
t−1
w2
t

(44)

˜Dt

(1

µ) ˜Dt−1

+ u( ˜Dt, ˜Dt−1),

−

≤
where we deﬁne ˜Dt (cid:44) Dt
w2
t
gebraic details in Appendix B) that
u( ˜Dt, ˜Dt−1)
determistictic result of Lemma 1,
bounded, which combined with (40) allows to conclude.

is easy to show (see al-
the perturbation term
. Using the
is

o( ˜Dt + ˜Dt−1) if ˜Dt

this ensures that ˜Dt

→ ∞

. It

∈

2) Stochastic decrease rates: In the general case (I), the
inequalities (40), (41) and (42) holds, and (44) is replaced by

E[ ˜Dt

|Ft− 1

2

]

≤

(1

−

µ) ˜Dt−1

w2
t−1
w2
t

+ u( ˜Dt, ˜Dt−1),

(45)

Taking the expectation of this inequality and using Jensen
inequality, we show that (43) holds when replacing ˜Dt by
E[ ˜Dt]. This shows that E[Dt]
.
∞
The result follows from Lemma 1, that applies as
⊆
Ft− 1

t ) and thus E[Dt] <
t−1

2 ⊆ F

(w2

∈ O

F

t.

B. Convergence of SAMM — Proof of Proposition 3

We now proceed to prove the Proposition 3, that extends
the stochastic majorization-minimization framework to allow
approximations in both majorization and minimizations steps.

Proof of Proposition 3. We adapt the proof of Proposition 3.3
from [10] (reproduced as Proposition 2 in our work). Relaxing
tightness and majorizing hypotheseses introduces some extra
error terms in the derivations. Assumption (H) allows to
control these extra terms without breaking convergence. The
stability Lemma 3 is important in steps 3 and 5.

1) Almost sure convergence of (¯gt(θt)): We control the
positive expected variation of (gt(θt))t
is
a converging quasi-martingale. By construction of ¯gt and
ρ,L(ft, θt−1, (cid:15)t), where (cid:15)t
properties of the surrogates gt
is a non-negative sequence that meets (H),

to show that

∈ T

it

¯gt−1(θt−1)

¯gt(θt)
= (¯gt(θt)

−

wt(gt(θt−1)
wt(gt(θt−1)
+ wt( ¯ft−1(θt−1)
wt(ft(θt−1)

−

−

≤

≤

≤

−

¯gt(θt−1)) + wt(gt(θt−1)
¯gt−1(θt−1))
ft(θt−1)) + wt(ft(θt−1)

−

−

¯gt−1(θt−1))

¯ft−1(θt−1))

−

¯gt−1(θt−1))

−

¯ft−1(θt−1)) + wt(¯(cid:15)t−1 + (cid:15)t),

(46)

where the average error sequence (¯(cid:15)t)t is deﬁned recursively:
¯(cid:15)0 (cid:44) (cid:15)0 and ¯(cid:15)t (cid:44) (1
wt)(cid:15)t−1 +wt(cid:15)t. The ﬁrst inequality uses
¯gt(θt−1). To obtain the forth inequality we observe
¯gt(θt)
≤
ft(θt−1) < (cid:15)t by deﬁnition of (cid:15)t and ¯ft(θt−1)
gt(θt−1)
−
¯(cid:15)t, which can easily be shown by induction on t.
¯gt(θt−1)
t−1,
Then, taking the conditional expectation with respect to

−
≤

−

F

E[¯gt(θt)

−
wt sup
θ∈Θ |

≤

¯gt−1(θt−1)
f (θ)

|F
¯ft−1(θ)

t−1]

−

|

+ wt(¯(cid:15)t−1 + E[(cid:15)t

t−1]).

(47)

|F

We have used the fact that (cid:15)t−1 is deterministic with respect
t−1. To ensure convergence, we must bound both terms
to
in (47): the ﬁrst term is the same as in the original proof

F

(43)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

14

with exact surrogate, while the second is the perturbative
term introduced by the approximation sequence ((cid:15)t)t. We use
Lemma B.7 from [10], issued from the theory of empirical
processes: E[supθ∈Θ |
(wtt1/2), and thus
wtE[sup
f (θ)
θ∈Θ |

¯ft−1(θ)
] =
O
|
∞
(cid:88)
t1/2w2

−
¯ft−1(θ)
] < C
|

∞
(cid:88)

f (θ)

t <

(48)

∞

−

t=1

t=1

where C is a constant, as t1/2w2
t = t1/2−2u and u > 3/4
from (G). Let us now focus on the second term of (47).
Deﬁning, for all 1

t, wt

wj),

(cid:81)t

i

i = wi

≤

≤
t
(cid:88)

i=1

E[¯(cid:15)t] =

wt
i

E[(cid:15)t]

wt

≤

−

j=i+1(1
t
(cid:88)

E[(cid:15)t].

i=1

(49)

−

1)

η >

We set η > 0 so that 2(u
ensures E[(cid:15)t]
partial sum (cid:80)t

1. Assumption (H)
−
(t2(u−1)−η), which allows to bound the
∈ O
E[(cid:15)i]
i=1
wtE[¯(cid:15)t−1 + E[(cid:15)t
(cid:16) t
(cid:88)

−
(t2u−1−η). Therefore
t−1]] = wtE[(cid:15)t−1] + wtE[(cid:15)t]

∈ O

|F
(cid:17)
E[(cid:15)t]

+ wtE[(cid:15)t]

(50)

At2u−2u−1−η + Bt2u−u−2−η

Ct−1−η,

≤

where we use u < 1 on the third line and the deﬁnition of
(wt)t on the second line. Thus (cid:80)∞
t−1]] <
. We use quasi-martingale theory to conclude, as in [10]. We
∞
deﬁne the variable δt to be 1 if E[¯gt(θt)
¯gt−1(θt−1)
0, and 0 otherwise. As all terms of (47) are positive:
∞
(cid:88)

t=1 wtE[¯(cid:15)t−1+E[(cid:15)t

t−1]

|F

|F

≥

−

E[δt(¯gt(θt)

¯gt−1(θt−1))]

−

w2
t

i=1

≤

≤

t=1

=

∞
(cid:88)

t=1
∞
(cid:88)

E[δtE[¯gt(θt)

¯gt−1(θt−1)

t−1]]

−

|F

(51)

+ ¯(cid:15)t−1 + E[(cid:15)t

t−1]

] <
|

.

wtE[sup
θ∈Θ |

f (θ)

¯ft−1(θ)
|

−

t=1

|F

≤
∞
As ¯gt are bounded from below ( ¯ft is bounded from (D) and
we easily show that ¯(cid:15)t is bounded), we can apply Theorem
A.1 from [10], that is a quasi-martingale convergence theorem
originally found in [51]. It ensures that (gt(θt))t≥1 converges
almost surely to an integrable random variable g(cid:63), and that
(cid:80)∞

t=1
|F
2) Almost sure convergence of ¯f (θt): We rewrite the second

E[¯gt(θt)
E[
|

almost surely.

¯gt−1(θt−1)

] <
|

t−1]

∞

−

inequality of (46), adding ¯(cid:15)t on both sides:
(cid:1)
(cid:0)ft(θt−1)

¯ft−1(θt−1) + ¯(cid:15)t−1

0

−
ft(θt−1)(cid:1) + wt
¯gt(θt)(cid:1) + wt¯(cid:15)t−1
−
¯ft−1(θt−1)(cid:1) + (cid:0)¯gt−1(θt−1)

−

≤

(cid:0)¯gt−1(θt−1)
wt
(cid:0)gt(θt−1)
wt
≤
−
+ (cid:0)¯gt−1(θt−1)
(cid:0)ft(θt−1)
wt
+ wt((cid:15)t + ¯(cid:15)t−1),

−

≤

¯ft−1(θt−1)(cid:1)

¯gt(θt)(cid:1)

−

(52)

where the left side bound has been obtained in the last
paragraph by induction and the right side bound arises from
the deﬁnition of (cid:15)t. Taking the expectation of (52) conditioned
on

t−1, almost surely,

F

0

≤

−

wt(f (θt−1)
E[¯gt(θt)

¯ft−1(θt−1))
−
¯gt−1(θt−1)

−

|F

t−1] + wt(¯(cid:15)t−1 + E[(cid:15)t

(53)
t−1]),

|F

t−1]
|

|F
(cid:0)f (θt−1)

We separately study the three terms of the previous upper
bound. The ﬁrst two terms can undergo the same analysis as
E(cid:2)
in [10]. First, almost sure convergence of (cid:80)∞
E[¯gt(θt)
t=1
−
|
(cid:3) implies that E(cid:2)¯gt(θt)
(cid:3)
¯gt−1(θt−1)
¯gt−1(θt−1)
t−1
is the summand of an almost surely converging sum. Second,
¯ft−1(θt−1)(cid:1) is the summand of an absolutely
wt
converging sum with probability one, less it would contra-
dict (48). To bound the third term, we have once more
the perturbation introduced by ((cid:15)t)t. We have
to control
(cid:80)∞
almost surely, otherwise

t=1 wt¯(cid:15)t−1 + wtE[(cid:15)t

t−1] <

|F

−

−

Fubini’s theorem would invalidate (50).

|F

∞

As the three terms are the summand of absolutely converg-
¯ft−1(θt−1)+¯(cid:15)t−1)
ing sums, the positive term wt(¯gt−1(θt−1)
is the summand of an almost surely convergent sum. This is
not enough to prove that ¯ht(θt) (cid:44) ¯gt(θt)
∞ 0, hence
we follow [10] and make use of its Lemma A.6. We deﬁne
Xt (cid:44) ¯ht−1(θt−1) + ¯(cid:15)t−1. As (H) holds, we use Lemma 3,
which ensures that (¯ht)t≥1 are uniformly R(cid:48)-Lipschitz and
θt
(cid:107)

−
¯ft(θt)

θt−1

2 =

→

O

−

−
Xt+1
|
R(cid:48)

≤

≤ O

(cid:107)
Xt

−
θt
(cid:107)
−
(wt) +

| ≤ |
θt−1
¯(cid:15)t
|

−

(wt). Hence,
¯ht(θt)
−
¯(cid:15)t
2 +
|
(cid:107)
,
¯(cid:15)t−1

−

|

¯ht−1(θt−1)
¯(cid:15)t−1
,
|
θt
as
(cid:107)

−

+

¯(cid:15)t

¯(cid:15)t−1

|

|

−

|
as ¯ht is R(cid:48)-Lipschitz
θt−1

(wt)

2 =
(cid:107)

O

(54)

From assumption (H), ((cid:15)t)t and (¯(cid:15)t)t are bounded. Therefore
)
¯(cid:15)t

(wt) and hence

¯(cid:15)t−1

¯(cid:15)t−1

+

(cid:15)t

|

−

wt(
|

| ≤

|
|
Xt+1
|

∈ O

|
Xt

−

| ≤ O

(wt).

(55)

Lemma A.6 from [10] then ensures that Xt converges
to zero with probability one. Assumption (H) ensures that
∞ 0 almost surely, from which we can easily deduce
(cid:15)t
∞ 0 almost surely. Therefore ¯ht(θt)
0 with probability
¯(cid:15)t
one and ( ¯ft(θt))t≥1 converges almost surely to g(cid:63).

→
→
3) Almost sure convergence of ¯f (θt): Lemma B.7 of [10],
based on empirical process theory [33], ensures that ¯ft uni-
formly converges to ¯f . Therefore, ( ¯f (θt))t≥1 converges almost
surely to g(cid:63).

→

4) Asymptotic stationary point condition: Preliminary to
the ﬁnal result, we establish the asymptotic stationary point
condition (57) as in [10]. This requires to adapt the original
proof to take into account the errors in surrogate computation
¯ht is L-
and minimization. We set α > 0. By deﬁnition,
Lipschitz over RK. Following the same computation as in (34),
we obtain, for all α > 0,

∇

≤

2
(cid:107)

2
α

¯(cid:15)t +

¯ht(θt)

(cid:107)∇
¯ht(θ)
where we use
|
the inequality (56) is true for all α,
surely. From the strong convexity of ¯gt and Lemma 3,
θ(cid:63)
t (cid:107)

2 converges to zero, which ensures

Lα
2
RK. As ¯(cid:15)t
∈
¯ht(θt)
2
(cid:107)
(cid:107)∇

¯(cid:15)t for all θ

| ≤

→

,

0 and
∞ 0 almost

→

θt

(cid:107)

−

(56)

¯ht(θ(cid:63)
t )

2
(cid:107)

¯
∇

ht(θt)

θt
2 + L
(cid:107)
(cid:107)

θ(cid:63)
2
t (cid:107)

−

≤ (cid:107)

(cid:107)∇
5) Parametrized surrogates: We use assumption (F) to
ﬁnally prove the property, adapting the proof of Proposition 3.4
in [10]. We ﬁrst recall the derivations of [10] for obtaining (58)
We deﬁne (κt)t such that ¯gt = gκt for all t > 0. We assume
that θ∞ is a limit point of (θt)t. As Θ is compact, there

→

∞ 0.

(57)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

15

K

exists an increasing sequence (tk)k such that (θtk )k converges
toward θ∞. As
is compact, a converging subsequence of
(κtk )k can be extracted, that converges towards κ∞
.
∈ K
From the sake of simplicity, we drop subindices and assume
κ∞.
without loss of generality that θt
From the compact parametrization assumption, we easily show
that (¯gκt)t uniformly converges towards ¯g∞ (cid:44) ¯gκ∞. Then,
deﬁning ¯h∞ = ¯g∞

¯f , for all θ

θ∞ and κt

Θ,

→

→

¯h∞(θ∞, θ

− ∇

−

θ∞)
(58)
Θ. We
∈
θ(cid:63)
2
t (cid:107)
−

¯f (θ∞, θ

∇

−

θ∞) =

¯g∞(θ∞, θ

θ∞)

−

∇
¯f (θ∞, θ

∈

−

≥

−

∇

t →

We ﬁrst show that
consider the sequence (θ(cid:63)
0, which implies θ(cid:63)
¯g∞, which implies (¯gt(θ(cid:63)
minimizes ¯gt, for all t > 0 and θ
This implies ¯g∞(θ∞)
for t

θ∞)
0 for all θ
t )t. From Lemma 3,
θt
→
(cid:107)
θ∞. ¯gt converges uniformly towards
¯g∞(θ∞). Furthermore, as θ(cid:63)
t ))t →
t
Θ, ¯gt(θ(cid:63)
¯gt(θ).
t )
inf θ∈Θ ¯g∞(θ) by taking the limit
. Therefore θ∞ is the minimizer of ¯g∞ and thus

∇
around θ(cid:63)

→ ∞
¯g∞(θ∞, θ
Adapting [10], we perform the ﬁrst-order expansion of ¯ht
t (instead of θt in the original proof) and show that
0

θ∞) = 0, as ¯ht differentiable,
θ∞. This is sufﬁcient to conclude.

∇
and θ(cid:63)

¯ht(θ(cid:63)
t )

θ∞)

(cid:107)∇

2
(cid:107)

→

≤

−

≥

−

≤

0.

∈

¯h∞(θ∞, θ
t →

C. Convergence of SOMF — Proof of Proposition 1
Proof of Proposition 1. From assumption (D), (xt)t
is (cid:96)2-
bounded by a constant X. With assumption (A), it implies
that (αt)t is (cid:96)2-bounded by a constant A. This is enough to
show that (gt)t and (θt)t meet basic assumptions (C)–(F).
Assumption (G) immediately implies (B). It remains to show
that (gt)t and (θt)t meet the assumptions (H) and (I). This will
allow to cast SOMF as an instance of SAMM and conclude.
1) The computation of Dt veriﬁes (I): We deﬁne D(cid:63)
t =
argminD∈C ¯gt(D). We show that performing subsampled
block coordinate descent on ¯gt is sufﬁcient to meet assump-
tion (I), where θt = Dt. We separately analyse the exceptional
case where no subsampling is done and the general case.

First, with small but non-zero probability, Mt = Ip
and Alg. 4 performs a single pass of simple block coordi-
nate descent on ¯gt. In this case, as ¯gt is strongly convex
from (A), [52, 37] ensures that the sub-optimality decreases
at least of factor 1
µ with a single pass of block coordinate
descent, where µ > 0 is a constant independent of t. We
provide an explicit µ in Appendix B.

−

In the general case, the function value decreases determin-
¯gt(Dt−1). As
≤
¯gt(Dt−1). Fur-
≤
t ) are deterministic with respect
= Ip] = ¯gt(D(cid:63)
t ).
the sub-optimality

istically at each minimization step: ¯gt(Dt)
a consequence, E[¯gt(Dt)
= Ip]
|Ft− 1
thermore, ¯gt and hence ¯gt(D(cid:63)
, which implies E[¯gt(D(cid:63)
t )
to
Deﬁning d (cid:44) P[Mt = Ip], we split
expectation and combine the analysis of both cases:

|Ft− 1

Ft− 1

, Mt

, Mt

2

2

2

E[¯gt(Dt)
= dE[¯gt(Dt)

, Mt = Ip]

]

2

−

¯gt(D(cid:63)
t )
|Ft− 1
¯gt(D(cid:63)
t )
−
d)E[¯gt(Dt)
µ) + (1
−
dµ(cid:1)(¯gt(Dt−1)

−

2

|Ft− 1
¯gt(D(cid:63)
t )
−
d)(cid:1)(¯gt(Dt−1)
¯gt(D(cid:63)
t )).

|Ft− 1
−

2

−

+ (1
−
(cid:0)d(1

≤
= (cid:0)1

−

= Ip]

, Mt
¯gt(D(cid:63)

t ))

(59)

2) The surrogates (gt)t verify (H): We deﬁne g(cid:63)
t ∈
ρ,L(ft, Dt−1) the surrogate used in OMF at iteration t, which
S
depends on the exact computation of α(cid:63)
t , while the surrogate
gt used in SOMF relies on approximated αt. Formally, using
the loss function (cid:96)(α, G, β) (cid:44) 1
α(cid:62)β + λΩ(α), we
recall the deﬁnitions

2 α(cid:62)Gα

−

α(cid:63)
t

(cid:44) argmin
α∈Rk
t (D) (cid:44) (cid:96)(α(cid:63)
g(cid:63)

t , β(cid:63)

(cid:96)(α, G(cid:63)

t ), αt(cid:44) argmin
α∈Rk
t , D(cid:62)D, D(cid:62)xt), gt(D) (cid:44) (cid:96)(αt, D(cid:62)D, D(cid:62)xt).

(cid:96)(α, Gt, βt),

(60)

gt

t , β(cid:63)

g(cid:63)
t −
(cid:107)

The matrices G(cid:63)
t are deﬁned in (21) and Gt, βt in either
the update rules (b) or (c). We deﬁne (cid:15)t (cid:44)
∞ to be
(cid:107)
the (cid:96)∞ difference between the approximate surrogate of SOMF
and the exact surrogate of OMF, as illustrated in Figure 2. By
ρ,L(ft, θt−1, (cid:15)t). We ﬁrst show that (cid:15)t can be
deﬁnition, gt
bounded by the Froebenius distance between the approximate
parameters Gt, βt and the exact parameters G(cid:63)
t . Using
Cauchy-Schwartz inequality, we ﬁrst show that there exists a
constant C (cid:48) > 0 such that for all D

t , β(cid:63)

∈ T

,

gt(D)
|

−

g(cid:63)
t (D)

C (cid:48)

| ≤

Then, we show that the distance

bounded: there exists C (cid:48)(cid:48) > 0 constant such that

∈ C
αt
(cid:107)

αt
(cid:107)

2.

(61)

α∗
t (cid:107)
α∗
t (cid:107)2 can itself be

−

−

αt

(cid:107)

−

α(cid:63)

t (cid:107)2 ≤

C (cid:48)(cid:48)(

G(cid:63)
(cid:107)

t −

Gt

(cid:107)F +

β(cid:63)
(cid:107)

t −

βt(cid:107)2).

(62)

We combine both equations and take the supremum over
D

, yielding

∈ C

(cid:15)t

C(
(cid:107)

≤

G(cid:63)

t −

Gt

(cid:107)F +

(cid:107)

β(cid:63)

t −

βt(cid:107)2),

(63)

where C is constant. Detailed derivation of (61) to (63) relies
on assumption (A) and are reported in Appendix B.

In a second step, we show that

βt(cid:107)
2
vanish almost surely, sufﬁciently fast. We focus on bounding
βt −
2 when the
(cid:107)
update rules (b) are used. For t > 0, we write i (cid:44) it. Then
(cid:88)

2 and proceed similarly for

F and
(cid:107)

G(cid:63)
t (cid:107)

β(cid:63)
t (cid:107)

t −

t −

Gt

Gt

−

(cid:107)

G(cid:63)
(cid:107)

β(cid:63)
(cid:107)

βt

(cid:44) β(i)

t =

γ(i)
s,tD(cid:62)

s−1Msx(i),

where γ(i)
(cid:12)
(cid:8)s
(cid:12)

s,t = γc(i)
t, xs = x(i)(cid:9)(cid:12)

t

≤
βt −

β(cid:63)

t =

s≤t,xs=x(i)

(cid:81)

s<t,xs=x(i)(1

γc(i)

s

−

) and c(i)
t =
t as

β(cid:63)

(cid:12). We can then decompose βt −
(cid:88)
γ(i)
s,t(Ds−1

Dt−1)(cid:62)Msx(i)

−

s≤t,xs=xt=x(i)
+ D(cid:62)

(cid:16) (cid:88)

t−1

s≤t,xs=xi)

γ(i)
s,tMs

−

(cid:17)

I

x(i).

(64)

The latter equation is composed of two terms: the ﬁrst one
captures the approximation made by using old dictionaries
in the computation of (βt)t, while the second captures how
the masking effect is averaged out as the number of epochs
increases. Assumption (B) allows to bound both terms at the
v(cid:1) > 0, a te-
same time. Setting η (cid:44) 1
2)
−
β(cid:63)
dious but elementary derivation indeed shows E[
βt−
t (cid:107)
(cid:107)

∈
0 almost surely — see Appendix B.
O
The SOMF algorithm therefore meets assumption (H) and is a
convergent SAMM algorithm. Proposition 1 follows.

(t2(u−1)−η) and (cid:15)t

2 min (cid:0)v

3
4 , (3u

→

−

−

2]

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

16

REFERENCES
[1] J. Mairal, “Sparse Modeling for Image and Vision Processing,” Foundations and
Trends in Computer Graphics and Vision, vol. 8, no. 2-3, pp. 85–283, 2014.
[2] N. Srebro, J. Rennie, and T. S. Jaakkola, “Maximum-margin matrix factorization,”
in Advances in Neural Information Processing Systems, 2004, pp. 1329–1336.
[3] E. J. Cand`es and B. Recht, “Exact matrix completion via convex optimization,”
Foundations of Computational Mathematics, vol. 9, no. 6, pp. 717–772, 2009.
[4] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global Vectors for Word

Representation.” in Proc. Conf. EMNLP, vol. 14, 2014, pp. 1532–43.

[5] O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix factoriza-
tion,” in Advances in Neural Information Processing Systems, 2014, pp. 2177–2185.
[6] Y. Zhang, M. Roughan, W. Willinger, and L. Qiu, “Spatio-Temporal Compressive

Sensing and Internet Trafﬁc Matrices,” 2009.

[7] H. Kim and H. Park, “Sparse non-negative matrix factorizations via alternating non-
negativity-constrained least squares for microarray data analysis,” Bioinformatics,
vol. 23, no. 12, pp. 1495–1502, 2007.

[8] G. Varoquaux et al., “Multi-subject dictionary learning to segment an atlas of brain

spontaneous activity,” in Proc. IPMI Conf., 2011, pp. 562–573.

[9] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” in

Proceedings of COMPSTAT, 2010, pp. 177–186.

[10] J. Mairal, “Stochastic majorization-minimization algorithms for large-scale opti-

mization,” in Adv. Neural Inform. Process. Syst., 2013, pp. 2283–2291.

[11] M. Razaviyayn, M. Hong, and Z.-Q. Luo, “A uniﬁed convergence analysis of block
successive minimization methods for nonsmooth optimization,” SIAM Journal on
Optimization, vol. 23, no. 2, pp. 1126–1153, 2013.

[12] S. Burer and R. D. C. Monteiro, “Local Minima and Convergence in Low-Rank
Semideﬁnite Programming,” Math. Program., vol. 103, no. 3, pp. 427–444, 2004.
[13] B. Recht and C. R´e, “Parallel stochastic gradient algorithms for large-scale matrix

completion,” Math. Program. Comput., vol. 5, no. 2, pp. 201–226, 2013.

[14] R. M. Bell and Y. Koren, “Lessons from the Netﬂix prize challenge,” ACM SIGKDD

Explorations Newsletter, vol. 9, no. 2, pp. 75–79, 2007.

[15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for matrix factorization
and sparse coding,” J. Machine Learning Research, vol. 11, pp. 19–60, 2010.
[16] W. B. Johnson and J. Lindenstrauss, “Extensions of Lipschitz mappings into a
Hilbert space,” Contemporary mathematics, vol. 26, no. 189-206, p. 1, 1984.
[17] E. Bingham and H. Mannila, “Random projection in dimensionality reduction:
Applications to image and text data,” in Proc. SIGKDD Conf., 2001, pp. 245–250.
[18] M. J. McKeown et al., “Analysis of fMRI Data by Blind Separation into Indepen-
dent Spatial Components,” Hum. Brain Mapp., vol. 6, no. 3, pp. 160–188, 1998.
[19] E. J. Cand`es and T. Tao, “Near-optimal signal recovery from random projections:
Universal encoding strategies?” IEEE Transactions on Information Theory, vol. 52,
no. 12, pp. 5406–5425, 2006.

[20] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions,”
SIAM review, vol. 53, no. 2, pp. 217–288, 2011.

[21] V. Rokhlin et al., “A randomized algorithm for principal component analysis,”

SIAM J. Matrix Anal. Appl., vol. 31, no. 3, pp. 1100–1124, 2009.

[22] T. Sarlos, “Improved approximation algorithms for large matrices via random

projections,” in Proc. IEEE Symp. Found. Comput. Science, 2006, pp. 143–152.

[23] Y. Lu et al., “Faster ridge regression via the subsampled randomized hadamard

transform,” in Adv. Neural Inform. Process. Syst., 2013, pp. 369–377.

[24] M. Pilanci and M. Wainwright, “Iterative hessian sketch: Fast and accurate solution
approximation for constrained least-squares,” JMLR, vol. 17, pp. 1–33, 2015.
[25] G. Raskutti and M. Mahoney, “Statistical and algorithmic perspectives on random-
ized sketching for ordinary least-squares,” in Proc. ICML, 2015, pp. 617–625.
[26] A. Mensch, J. Mairal, B. Thirion, and G. Varoquaux, “Dictionary learning for

massive matrix factorization,” in Proc. ICML, 2016, pp. 1737–1746.

[27] B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete basis set:
A strategy employed by V1?” Vision Res., vol. 37, no. 23, pp. 3311–3325, 1997.
[28] R. Tibshirani, “Regression shrinkage and selection via the lasso,” J. R. Stat. Soc.

Series B Stat. Methodol., vol. 58, no. 1, pp. 267–288, 1996.

[29] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” J.

Comput. Graph. Stat., vol. 15, no. 2, pp. 265–286, 2006.

[30] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,”
J. R. Stat. Soc. Series B Stat. Methodol., vol. 67, no. 2, pp. 301–320, 2005.
[31] P. O. Hoyer, “Non-negative matrix factorization with sparseness constraints,”

Journal of Machine Learning Research, vol. 5, pp. 1457–1469, 2004.

[32] L. Bottou, F. E. Curtis, and J. Nocedal, “Optimization Methods for Large-Scale

Machine Learning,” arXiv:1606.04838v2 [stat.ML], 2016.
[33] A. W. Van der Vaart, Asymptotic Statistics. CUP, 2000, vol. 3.
[34] M. Mardani et al., “Subspace Learning and Imputation for Streaming Big Data

Matrices and Tensors,” IEEE TSP, vol. 63, no. 10, pp. 2663–2677, 2015.

[35] J. M. Borwein and A. S. Lewis, Convex Analysis and Nonlinear Optimization:

Theory and Examples. Springer Science & Business Media, 2010.

[36] J. Mairal, “Optimization with ﬁrst-order surrogate functions,” in Proceedings of the

International Conference on Machine Learning, 2013, pp. 783–791.

[37] S. J. Wright, “Coordinate descent algorithms,” Mathematical Programming, vol.

[38] D. C. Van Essen et al., “The WU-Minn Human Connectome Project: An overview,”

151, no. 1, pp. 3–34, 2015.

NeuroImage, vol. 80, pp. 62–79, 2013.

[39] M. P. Milham et al., “The adhd-200 consortium: a model to advance the transla-
tional potential of neuroimaging in clinical neuroscience,” Front. Syst. Neurosci.,
vol. 6, no. 62, 2012.

[40] G. Varoquaux, Y. Schwartz, P. Pinel, and B. Thirion, “Cohort-level brain mapping:
Learning cognitive atoms to single out specialized regions,” in Proceedings of the
Information Processing in Medical Imaging Conference, 2013, pp. 438–449.

[41] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral image classiﬁcation
using dictionary-based sparse representation,” IEEE Transactions on Geoscience
and Remote Sensing, vol. 49, no. 10, pp. 3973–3985, 2011.

[42] A. Soltani-Farani, H. R. Rabiee, and S. A. Hosseini, “Spatial-Aware Dictionary
Learning for Hyperspectral Image Classiﬁcation,” IEEE Transactions on Geo-
science and Remote Sensing, vol. 53, no. 1, pp. 527–541, 2015.

[43] M. Maggioni, V. Katkovnik, K. Egiazarian, and A. Foi, “Nonlocal transform-
domain ﬁlter for volumetric data denoising and reconstruction,” IEEE Trans. Image
Process., vol. 22, no. 1, pp. 119–133, 2013.

[44] Y. Peng et al., “Decomposable nonlocal tensor dictionary learning for multispectral

image denoising,” in Proc. IEEE Conf. CVPR, 2014, pp. 2949–2956.

[45] G. Vane, “First results from the airborne visible/infrared imaging spectrometer

(AVIRIS),” in Ann. Tech. Symp. Int. Soc. Optics Photonics, 1987, pp. 166–175.

[46] S. Behnel et al., “Cython: The best of both worlds,” Computing in Science &

Engineering, vol. 13, no. 2, pp. 31–39, 2011.

[47] J. Friedman, T. Hastie, H. H¨oﬂing, and R. Tibshirani, “Pathwise coordinate
optimization,” The Annals of Applied Statistics, vol. 1, no. 2, pp. 302–332, 2007.
[48] F. Pedregosa et al., “Scikit-learn: Machine learning in Python,” Journal of Machine

Learning Research, vol. 12, pp. 2825–2830, 2011.

[49] A. Abraham et al., “Machine learning for neuroimaging with scikit-learn,” Frontiers

in Neuroinformatics, vol. 8, no. 14, 2014.

[50] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra, “Efﬁcient projections onto
the l1-ball for learning in high dimensions,” in Proceedings of the International
Conference on Machine Learning, 2008, pp. 272–279.

[51] M. M´etivier, Semimartingales: A Course on Stochastic Processes. Walter de

Gruyter, 1982, vol. 2.

[52] A. Beck and L. Tetruashvili, “On the convergence of block coordinate descent type

methods,” SIAM Journal on Optimization, vol. 23, no. 4, pp. 2037–2060, 2013.

[53] J. L. Doob, Stochastic processes.

John Wiley & Sons, 1990.

Arthur Mensch is a PhD candidate at Universit´e
Paris-Saclay and Inria. His main research interests
are related to large-scale stochastic optimization
and statistical learning, with speciﬁc applications to
functional neuroimaging and cognitive brain map-
ping. In 2015, he received a graduate degree from
Ecole Polytechnique, France, and a MSc degree in
applied mathematics from ´Ecole Normale Sup´erieure
de Cachan, France.

Julien Mairal is a research scientist at Inria. He
received a graduate degree from Ecole Polytech-
nique, France, in 2005, and a PhD degree from Ecole
Normale Superieure, Cachan, France, in 2010. Then,
he was a postdoctoral researcher at the statistics
department of UC Berkeley, before joining Inria in
2012. His research interests include machine learn-
ing, computer vision, mathematical optimiza- tion,
and statistical image and signal processing. In 2016,
he received a Starting Grant from the European
Research Council (ERC).

Ga¨el Varoquaux is a tenured computer-science
researcher at Inria. His research develops statistical-
learning tools for functional neuroimaging data with
application to cognitive mapping of the brain as
well as the study of brain pathologies. He is also
heavily invested in software development for data
science, as project-lead for scikit-learn, one of the
reference machine-learning toolboxes, and on joblib,
Mayavi, and nilearn. Varoquaux has a PhD in quan-
tum physics and is a graduate from Ecole Normale
Superieure, Paris.

Bertrand Thirion is the principal investigator of the
Parietal team (Inria-CEA) within the main French
Neuroimaging center, Neurospin. His main research
interests are related to the use of machine learning
and statistical analysis techniques for neuroimaging,
e.g. the modeling of brain variability in group stud-
ies, the mathematical study of functional connectiv-
ity and brain activity decoding; he addresses various
applications such as the study of vision through
neuroimaging and the classiﬁcation of brain images
for diagnosis or brain mapping

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

17

APPENDIX B
ALGEBRAIC DETAILS

A. Proof of Lemma 1

Proof. We ﬁrst focus on the deterministic case. Assume that (xt)t is not bounded. Then there exists a subsequence of (xt)t
that diverges towards +
and for all (cid:15) > 0,
∞
using the asymptotic bounds on u, there exists t1

. We assume without loss of generality that (xt)t → ∞

. Then, xt + xt−1

t0 such that

→ ∞

≥

t
∀

≥

t1, xt

and therefore xt

αxt−1 + (cid:15)(xt + xt−1)
α + (cid:15)
(cid:15)
1

xt−1.

≤

≤

−

Setting (cid:15) small enough, we obtain that xt is bounded by a geometrically decreasing sequence after t1, and converges to 0,
which contradicts our hypothesis. This is enough to conclude.

t−1

diverges to +

In the random case, we consider a realization of (Xt)t that is not bounded, and assumes without loss of generality that it
βXt−1, where
βXt−1, as Xt−1 is deterministic conditioned
, Doob’s forward convergence lemma on
cannot happen

. Following the reasoning above, there exists β < 1, t1 > 0, such that for all t > t1, E[Xt
t. Taking the expectation conditioned on

⊆ F
F
t−1. Therefore Xt is a supermartingale beyond a certain time. As E[Xt] <
on
discrete martingales [53] ensures that (Xt)t converges almost surely. Therefore the event
on a set with non-zero probability, less it would lead to a contradiction. The lemma follows.

(Xt)t is not bounded
}
{

t−1, E[Xt

∞
⊆ F

t−1]

∞

|F

|F

t(cid:48)]

≤

≤

F

F

t(cid:48)

B. Taylor’s inequality for L-Lipschitz continuous functions

This inequality is useful in the demonstration of Lemma 2 and Proposition 3. Let f : Θ

L-Lipschitz gradient. That is, for all θ, θ(cid:48)

∈
f (θ(cid:48))

Θ,

f (θ)

(cid:107)∇

f (θ) +

∇

≤

f (θ(cid:48))

− ∇
f (θ)(cid:62)(θ(cid:48)

(cid:107)2 ≤

L

θ
(cid:107)

θ) +

−

⊂

RK
→
(cid:107)2. Then, for all θ, θ(cid:48)
∈
θ(cid:48)

2
2.
(cid:107)

θ(cid:48)

−
L
θ
2 (cid:107)

−

R be a function with
Θ,

(66)

C. Lemma 3: Detailed control of Dt in (44)

Injecting (40) and (42) in (43), we obtain

˜Dt

(1

≤

−

µ) ˜Dt−1

w2
t−1
w2
t

+ u( ˜Dt, ˜Dt−1), where u( ˜Dt, ˜Dt−1) (cid:44) (1

µ) ˜Q

3( ˜Dt + ˜Dt−1

) + ˜Q +

˜Dt

.

(67)

(cid:18)(cid:115)

−

(cid:113)

(cid:19)

w2
t−1
w2
t

From assumption (G),
Using the determistictic result of Lemma 1, this ensures that ˜Dt is bounded.

1, and we have, from elementary comparisons, that u( ˜Dt, ˜Dt−1)

∈

w2
t−1
w2
t →

o( ˜Dt + ˜Dt−1) if Dt

.
→ ∞

D. Detailed derivations in the proof of Proposition 1

Let us ﬁrst exhibit a scaler µ > 0 independent of t, for which (I) is met
1) Geometric rate for single pass subsampled block coordinate descent: . For D(j)

Rp×k any matrix with non-zero j-th

column d(j) and zero elsewhere

¯gt(D + D(j))

∇

− ∇

∈
¯gt(D + D(j)) = ¯Ct[j, j]d(j)

¯gt has coordinate Lipschitz constant Lmax (cid:44) max0≤j<k ¯Ct[j, j]

and hence ¯gt gradient has component Lipschitz constant Lj = ¯Ct[j, j] for component j, as already noted in [15]. Using [37]
A2, as (αt)t is
terminology,
√kLmax. Moreover,
bounded from (A). As a consequence, ¯gt gradient is also L-Lipschitz continuous, where [37] note that L
¯gt is strongly convex with strong convexity modulus ρ > 0 by hypothesis (A). Then, [52] ensures that after one cycle over the
k blocks

maxt>0,0≤j<k αt[j]2

∇

≤

≤

≤

E[¯gt(Dt)

¯gt(D(cid:63)
t )

t−1, Mt = Ip]

−

|F

(cid:0)1

(cid:0)1

−

−

≤

≤

(cid:1)(¯gt(Dt−1)

2Lmax(1 + kL2/L2
µ(cid:1)(¯gt(Dt−1)

¯gt(D(cid:63)

max)

−
t )) where µ (cid:44)

ρ

−

¯gt(D(cid:63)

t ))
ρ
2A2(1 + k2)

2) Controling (cid:15)t from (Gt, βt), (G(cid:63)

is met in the proof of SOMF convergence. We ﬁrst show that (αt)t is bounded. We choose D > 0 such that
for all j
condition, for all t > 0,

t ) — Equations 61–62: We detail the derivations that are required to show that (H)
D
. From assumption (A), using the second-order growth

, and X such that

X for all x

[k] and D

d(j)
(cid:107)

(cid:107)2 ≤

(cid:107)2 ≤

x
(cid:107)

∈ X

∈ C

∈

t , β(cid:63)

(65)

(68)

(69)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

We have successively used the fact that Ω(0) = 0, Ω(αt)
induction on the number of epochs. For all t > 0, from the deﬁnition of αt and α(cid:63)

βt(cid:107)2 ≤
(cid:107)

0, and

≥

√krDX, which can be shown by a simple

λΩ(0)

t Gtαt

α(cid:62)

t βt + λΩ(αt)

ρ
2 (cid:107)
1
2

2
2 ≤

αt

0
−
(cid:107)
t Gtαt

α(cid:62)

0 +

(cid:107)

≤

ρ
2 (cid:107)

αt

2
2 +
(cid:107)

−
αt

(

α(cid:62)

1
2
βt(cid:107)2,
αt
(cid:107)

(cid:107)2,

(cid:107)2(cid:107)

−

hence

αt

ρ
(cid:107)

2
2 ≤

(cid:107)

√krDX

and therefore

αt
(cid:107)

(cid:107)2 ≤

√krDX
ρ

(cid:44) A.

gt(D)

|

g(cid:63)
t (D)
|

−

=

t −

Tr D(cid:62)D(αtα(cid:62)

(cid:12)
1
(cid:12)
(cid:12)
2
1
t −
2 (cid:107)
(kD2A + √kDX)
αt
(cid:107)

αtα(cid:62)

D(cid:62)D

(cid:107)F (cid:107)

α(cid:63)

t α(cid:63)
t

(cid:62))

α(cid:63)

(cid:62)

t α(cid:63)
t
α(cid:63)
t (cid:107)2,

−

−
(cid:107)F +

≤

≤

t , for all D

:

∈ C

(αt

−

α(cid:63)

t )(cid:62)D(cid:62)xt

(cid:12)
(cid:12)
(cid:12)

D

(cid:107)

(cid:107)F (cid:107)

xt

(cid:107)2(cid:107)

αt

−

α(cid:63)

t (cid:107)2

where we use Cauchy-Schwartz inequality and elementary bounds on the Froebenius norm for the ﬁrst inequality, and use
αt, α(cid:63)
[k] to obtain the second inequality, which is (61) in the main text.
α(cid:63)
t (cid:107)2. We adapt the proof of Lemma B.6 from [36], that states the lipschitz continuity of the

αt
(cid:107)
minimizers of some parametrized functions. By deﬁnition,

A, xt
We now turn to control

X for all t > 0 and d(j)

D for all j

t ≤

≤

−

≤

∈

α(cid:63)

t = argmin
α∈Rk

(cid:96)(α, G(cid:63)

t , β(cid:63)
t )

αt = argmin
α∈Rk

(cid:96)(α, Gt, βt),

Assumption (A) ensures that Gt

ρIk, therefore we can write the second-order growth condition

(cid:31)

αt

αt

ρ
2 (cid:107)
ρ
2 (cid:107)
ρ

αt
(cid:107)

2
α(cid:63)
t (cid:107)
2 ≤
2
α(cid:63)
t (cid:107)
2 ≤
2
α(cid:63)
t (cid:107)
2 ≤

−

−

−

(cid:96)(αt, G(cid:63)

t , β(cid:63)
t )

(cid:96)(αt, Gt, βt)

−

(cid:96)(α(cid:63)

t , Gt, βt)
p(α(cid:63)

(cid:96)(α(cid:63)

t , G(cid:63)

t , β(cid:63)

t ),
and therefore
−
t ), where p(α) (cid:44) (cid:96)(α, Gt, βt)

p(αt)

−

−
Rk such that

(cid:96)(αt, G(cid:63)

t , β(cid:63)

t ).

α
(cid:107)

(cid:107)2 ≤

A,

p takes a simple form and can differentiated with respect to α. For all α

Therefore p is L-Lipschitz on the ball of size A where αt and α(cid:63)

p(α) =

1
2

α(cid:62)(Gt
G(cid:63)

A

−
Gt
(cid:107)

−

p(α) = (Gt

∇
p(α)

(cid:107)∇

(cid:107)2 ≤

−
t )α
G(cid:63)

G(cid:63)

∈
α(cid:62)(βt −
t )α
−
β(cid:63)
(βt −
t )
β(cid:63)
βt −
(cid:107)
t live, and

t (cid:107)2

−
t (cid:107)F +

β(cid:63)
t )

(cid:44) L

αt

ρ
(cid:107)

αt

(cid:107)

−

−

2
α(cid:63)
t (cid:107)
2 ≤
α(cid:63)

t (cid:107)2 ≤

L
(cid:107)
A
ρ (cid:107)

αt

−

Gt

−

α(cid:63)

t (cid:107)2
G(cid:63)
t (cid:107)F +

1
ρ (cid:107)

βt −

β(cid:63)
t (cid:107)2,

which is (62) in the main text. The bound (63) on (cid:15)t immediately follows.

3) Bounding

C are positive constants independent of t and we introduce the terms

βt −
(cid:107)

β(cid:63)
t (cid:107)2 in equation (64): Taking the (cid:96)2 norm in (64), we have

βt −
(cid:107)

β(cid:63)

t (cid:107)2 ≤

BLt + CRt, where B and

Lt (cid:44) (cid:88)

s≤t,xs=xt=x(i)

γ(i)
s,t(cid:107)

Ds−1

Dt−1

−

(cid:107)F ,

Rt (cid:44)

(cid:0) (cid:80)

s≤t,xs=x(i) γ(i)

s,tMs

(cid:1)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
I
(cid:13)F

.

−

{
∈

a) Conditioning on the sequence of drawn indices: We recall that (it)t is the sequence of indices that are used to draw
}i, namely such that xt = x(it). (it)t is a sequence of i.i.d random variables, whose law is uniform in [1, n].
b )b>0 that record the iterations at which sample (i) is drawn, i.e. such
t > 0 is the integer that counts the number of time sample (i) has
. These notations will help us understanding the behavior of (Lt)t
}

x(i)
(xt)t from
[n], we deﬁne the increasing sequence (t(i)
For each i
that itb = i for all b > 0. For t > 0, we recall that c(i)
appeared in the algorithm, i.e. c(i)
t
b ≤
and (Rt)t.

b > 0, t(i)
{

t = max

18

(70)

(71)

(72)

(73)

(74)

(75)

(76)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

19

(77)

(79)

(80)

(81)

b) Bounding Rt: The right term Rt takes its value into sequences that are running average of masking matrices. Formally,

Rt =

¯M(it)

(cid:107)

I

(cid:107)F , where we deﬁne for all i

∈

[n],

¯M(i)
t

γ(i)
b ,t(i)
t(i)

c

Mtb ,

which follows the recursion

t −
c(i)
t(cid:88)

(cid:44)

b=1






¯M(i)
t
¯M(i)
t
¯M(i)
0

γc(i)

) ¯M(i)
t−1 + γc(i)
= (1
−
= M(i)
= it
if i
t−1
[n]
= 0 for all i

t

t

Mt

if i = it

When sampling a sequence of indices (is)s>0, the n random matrix sequences [( ¯M(i)
law as the sampling is uniform. We therefore focus on controling ( ¯M(0)
the expectation over the sequence of indices (is)s,

∈
t )t≤0]
i∈[n] follows the same probability
] is

t )t. For simplicity, we write ct (cid:44) c(0)

t

. When E[
·

¯M(0)
E[
(cid:107)

t −

(cid:107)F ]2
I

( ¯M(0)
t

[j, j]

1)(cid:3) = pE[( ¯M(0)

t

−

[0, 0]

1)]

−

E(cid:2)

p
(cid:88)

j=1

≤

≤

C p(ct)1/2γct = C p(ct)1/2−v, where C is a constant independent of t.

(78)

We have simply bounded the Froebenius norm by the (cid:96)1 norm in the ﬁrst inequality and used the fact that all coefﬁcients
Mt[j, j] follows the same Bernouilli law for all t > 0, j
[p]. We then used Lemma B.7 from [10] for the last inequality. This
lemma applies as Mt[0, 0] follows the recursion (77). It remains to take the expectation of (78), over all possible sampling
trajectories (is)s>0:

∈

E[Rt] = E(cid:2)E[Rt

(is)s](cid:3) = E(cid:2)E[
M(it)
t −
(cid:107)F |
|
(cid:107)
CpE[(ct)2(u−1)−η].
= CpE[(ct)1/2−v]

I

≤

(is)s](cid:3) = E(cid:2)E[
(cid:107)

M(0)

I

t −

(cid:107)F |

(is)s](cid:3) = E[

M(0)
(cid:107)

t −

I

(cid:107)F ]

The last inequality arises from the deﬁnition of η (cid:44) 1
we successively have

2 min (cid:0)v

3
4 , (3u

2)

−

−

−

v(cid:1), as follows. First, η > 0 as u > 11

12 . Then,

5
2 −

2u <

<

as u >

2
3

3
4

,

11
12

,

3
4

v

≥

+ 2η >

2u + 2η,

5
2 −

1
2 −

v <

1
2 −

5
2

+ 2u

2η = 2(u

1)

2η < 2(u

1)

η, which allows to conclude.

−

−

−

−

−

I)t converges towards 0
Lemma B.7 from [10] also ensures that Mt[0, 0]
1 almost surely when t
almost surely, given any sample sequence (is)s. It thus converges almost surely when all random variables of the algorithm
are considered. This is also true for ( ¯M(i)

t −

→ ∞

→

I)t for all i

. Therefore ( ¯M(0)

c) Bounding Lt: As above, we deﬁne n sequences [(L(i)

t −

∈

[n] and hence for Rt.
t )t]i∈[n], such that Lt = L(it)

t

for all t > 0. Namely,

L(i)
t

(cid:44) (cid:88)
s≤t,
xs=xt=x(i)

γ(i)
s,t(cid:107)

Ds−1

Dt−1

−

(cid:107)F =

c(i)
t(cid:88)

b=1

γ(i)
b ,t(i)
t(i)

c

(i)
t

(cid:13)
(cid:13)Dtb−1

Dt

c

(i)
t

−

(cid:13)
(cid:13)

−1

.

F

Once again, the sequences (cid:2)(L(i)
t )t
focus on bounding (L(0)
From assumption (B) and the deﬁnition of η, we have v < ν < 1. We split the sum in two parts, around index dt (cid:44) ct
where

(cid:3)
i all follows the same distribution when sampling over sequence of indices (is)s. We thus
η.
,

t )t. Once again, we drop the (0) superscripts in the right expression for simplicity. We set ν (cid:44) 3u

takes the integer part of a real number. For simplicity, we write d (cid:44) dt and c (cid:44) ct in the following.

2
−
−
(ct)ν

−(cid:98)

(cid:99)

(cid:98)·(cid:99)

L(0)

t =

γtb,tc

(cid:13)
(cid:13)Dtb−1

Dtc−1

−

(cid:13)
(cid:13)F ≤

2√kD

c
(cid:88)

b=1

d
(cid:88)

b=1

c
(cid:88)

tc−1
(cid:88)

γtb,tc +

γtb,t

b=d+1

s=tb−1

ws (cid:44) 2√kDL(0)

t,1 + L(0)

t,2

(82)

On the left side, we have bounded
on

Ds

Dt

(cid:107)
We now study both L(0)

−

Dt
(cid:107)
t,1 and L(0)

(cid:107)F provided by Lemma 3, that applies here as (I) is met and (63) ensures that (
gt
(cid:107)

−

g(cid:63)
t (cid:107)∞)t is bounded.

(cid:107)F by √kD, where D is deﬁned in the previous section. The right part uses the bound

t,2 . First, for all t > 0,

L(0)
t,1

(cid:44)

d
(cid:88)

b=1

d
(cid:88)

c
(cid:89)

γtb,tc =

γb

(1

γp)

−

≤

p=b+1

d
(cid:88)

b=1

γb(1

−

γc)c−b

b=1
γc)(cid:98)cν (cid:99)
γc

≤

(1

−

≤

cv exp (cid:0)log(1

1

cv )cν(cid:1)

≤

−

C (cid:48)cv exp(cν−v)

Cc2(u−1)−η = C(ct)2(u−1)−η,

(83)

≤

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

20

where C and C (cid:48) are constants independent of t. We have used ν > v for the third inequality, which ensures that
exp (cid:0)log(1
(cν−v). Basic asymptotic comparison provides the last inequality, as ct
almost surely and
the right term decays exponentially in (ct)t, while the left decays polynomially. As a consequence, L(0)
0 almost surely.

cv )cν(cid:1)

∈ O

−

1

Secondly, the right term can be bounded as (wt)t decays sufﬁciently rapidly. Indeed, as (cid:80)c

→ ∞
t,1 →
b=1 γtb,t = 1, we have

L(0)
t,2

(cid:44)

γtb,t

c
(cid:88)

b=d

tc−1
(cid:88)

s=tb−1

ws

max
d≤b≤c

≤

s=tb−1

(cid:16) tc−1
(cid:88)

(cid:17)

ws

=

tc−1
(cid:88)

ws

wtd (tc

td) =

−

≤

s=td−1
ct
dt
−
(dt)u

tc
td
(td)u =
−

tc
ct

td
dt

(

dt
td

)u

−
−

from elementary comparisons. First, we use the deﬁnition of ν to draw

were we use the fast that η
−
expectation n. Therefore, as c

when t

n , and
0, from the strong law of large numbers and linearity of the expectation

tb follows a geometric law of parameter 1

−

(ct)ν

ct
dt
(dt)u ≤

−

(ct)u(1

cν−1
t

)u ≤

−
1 < 0. We note that for all b > 0, tb+1

C(ct)ν−u = C(ct)2(u−1)−η,

d

−

→ ∞
c−1
1
(cid:88)

d

→

tb

tb+1

n,

−

→

d−1
(cid:88)

td
d

=

1
d

tb+1

tb

n almost surely.

−

→

b=0

b=d

−
n1−u almost surely. This immediately shows L(0)
0 almost surely and therefore
βt −

almost surely.

β(cid:63)

(cid:107)

0

→

t,2 →

0 and thus L(0)

t →

0 almost surely.

tc
c

td
d

=

−
−
As a consequence, tc−td
ct−dt
As with Rt, this implies that Lt

( dt
td

)u

c

→

t (cid:107)2 →
( dt
td

−
−

Finally, from the dominated convergence theorem, E[ tc−td
ct−dt
and write

)u]

n1−u for t

→

→ ∞

. We can use Cauchy-Schartz inequality

E[L(0)

t,2 ] = E[

tc
td
(td)u ]
−

E[

ct
dt
(dt)u ]E[
−

tc
ct

≤

td
dt

(

dt
td

)u]

C (cid:48)E[

≤

ct
dt
(dt)u ]
−

≤

C C (cid:48)E[(ct)2(u−1)−η],

where C (cid:48) is a constant independant of t. Then

E[Lt] = E(cid:2)E[L(it)

(is)s](cid:3) = E[L(0)
(is)s](cid:3) = E(cid:2)E[L(0)
|
β(cid:63)
Combined with (79), this shows that E[
βt −
t (cid:107)2]
(cid:107)
∈ O
0. Therefore E[( ct
(t, 1

1
n almost surely when t

n ), ct

|

t

t

t

t →

]

≤

2√kDE[L(0)

t,1 ] + E[L(0)
t,2 ]
((ct)2(u−1)−η). As ct follows a binomial distribution of parameter
nη−2(u−1), and from Cauchy-Schwartz inequality,
t )2(u−1)−η)]
)2(u−1)−η)]t2(u−1)−η

((ct)2(u−1)−η).

(t2(u−1)−η).

∈ O

(89)

→

(90)

→
βt −
(cid:107)

E[

β(cid:63)
t (cid:107)2]

CE[(

ct
t

≤
We have reused the fact that converging sequences are bounded. This is enough to conclude.

∈ O

(84)

(85)

(86)

(87)

(88)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

1

Stochastic Subsampling for
Factorizing Huge Matrices

Arthur Mensch, Julien Mairal,
Bertrand Thirion, and Ga¨el Varoquaux

7
1
0
2
 
t
c
O
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
6
3
5
0
.
1
0
7
1
:
v
i
X
r
a

Abstract—We present a matrix-factorization algorithm that
scales to input matrices with both huge number of rows and
columns. Learned factors may be sparse or dense and/or non-
negative, which makes our algorithm suitable for dictionary
learning, sparse component analysis, and non-negative matrix
factorization. Our algorithm streams matrix columns while
subsampling them to iteratively learn the matrix factors. At
each iteration, the row dimension of a new sample is reduced by
subsampling, resulting in lower time complexity compared to a
simple streaming algorithm. Our method comes with convergence
guarantees to reach a stationary point of the matrix-factorization
problem. We demonstrate its efﬁciency on massive functional
Magnetic Resonance Imaging data (2 TB), and on patches ex-
tracted from hyperspectral images (103 GB). For both problems,
which involve different penalties on rows and columns, we obtain
signiﬁcant speed-ups compared to state-of-the-art algorithms.

Index Terms—Matrix factorization, dictionary learning, NMF,
stochastic optimization, majorization-minimization, randomized
methods, functional MRI, hyperspectral imaging

I. INTRODUCTION

Matrix factorization is a ﬂexible approach to uncover latent
factors in low-rank or sparse models. With sparse factors, it is
used in dictionary learning, and has proven very effective for
denoising and visual feature encoding in signal and computer
vision [see e.g., 1]. When the data admit a low-rank structure,
matrix factorization has proven very powerful for various tasks
such as matrix completion [2, 3], word embedding [4, 5], or
network models [6]. It is ﬂexible enough to accommodate a
large set of constraints and regularizations, and has gained sig-
niﬁcant attention in scientiﬁc domains where interpretability
is a key aspect, such as genetics [7] and neuroscience [8].
In this paper, our goal is to adapt matrix-factorization tech-
niques to huge-dimensional datasets, i.e., with large number
of columns n and large number of rows p. Speciﬁcally, our
work is motivated by the rapid increase in sensor resolution, as
in hyperspectral imaging or fMRI, and the challenge that the
resulting high-dimensional signals pose to current algorithms.
As a widely-used model, the literature on matrix factoriza-
tion is very rich and two main classes of formulations have

A. Mensch, B. Thirion, G. Varoquaux are with Parietal team, Inria, CEA,
Paris-Saclay University, Neurospin, at Gif-sur-Yvette, France. J. Mairal is with
Universit´e Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK at Grenoble,
France.

The research leading to these results was supported by the ANR (MAC-
ARON project, ANR-14-CE23-0003-01 — NiConnect project, ANR-11-
BINF-0004NiConnect). It has received funding from the European Union’s
Horizon 2020 Framework Programme for Research and Innovation under
Grant Agreement No 720270 (Human Brain Project SGA1).

Corresponding author: Arthur Mensch (arthur.mensch@m4x.org)

emerged. The ﬁrst one addresses a convex-optimization prob-
lem with a penalty promoting low-rank structures, such as the
trace or max norms [2]. This formulation has strong theoretical
guarantees [3], but lacks scalability for huge datasets or sparse
factors. For these reasons, our paper is focused on a second
type of approach, which relies on nonconvex optimization.
Stochastic (or online) optimization methods have been devel-
oped in this setting. Unlike classical alternate minimization
procedures, they learn matrix decompositions by observing
a single matrix column (or row) at each iteration. In other
words, they stream data along one matrix dimension. Their
cost per iteration is signiﬁcantly reduced, leading to faster
convergence in various practical contexts. More precisely,
two approaches have been particularly successful: stochastic
gradient descent [9] and stochastic majorization-minimization
methods [10, 11]. The former has been widely used for matrix
completion [see 12, 13, 14, and references therein], while the
latter has been used for dictionary learning with sparse and/or
structured regularization [15]. Despite those efforts, stochastic
algorithms for dictionary learning are currently unable to deal
efﬁciently with matrices that are large in both dimensions.

We propose a new matrix-factorization algorithm that
can handle such matrices.
It builds upon the stochastic
majorization-minimization framework of [10], which we gen-
the objective
eralize for our problem. In this framework,
function is minimized by iteratively improving an upper-bound
surrogate of the function (majorization step) and minimizing
it to obtain new estimates (minimization step). The core idea
of our algorithm is to approximate these steps to perform them
faster. We carefully introduce and control approximations,
so to extend convergence results of [10] when neither the
majorization nor the minimization step is performed exactly.
For this purpose, we borrow ideas from randomized meth-
ods in machine learning and signal processing. Indeed, quite
orthogonally to stochastic optimization, efﬁcient approaches to
tackle the growth of dataset dimension have exploited random
projections [16, 17] or sampling, reducing data dimension
while preserving signal content. Large-scale datasets often
have an intrinsic dimension which is signiﬁcantly smaller
than their ambient dimension. Good examples are biological
datasets [18] and physical acquisitions with an underlying
sparse structure enabling compressed sensing [19]. In this
context, models can be learned using only random data sum-
maries, also called sketches. For instance, randomized methods
[see 20, for a review] are efﬁcient to compute PCA [21], a
classic matrix-factorization approach, and to solve constrained
or penalized least-square problems [22, 23]. On a theoretical

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

2

level, recent works on sketching [24, 25] have provided bounds
on the risk of using random summaries in learning.

the matrix D is called the “dictionary” and A the sparse code.
We use this terminology throughout the paper.

Using random projections as a pre-processing step is not
appealing in our applicative context since factors learned on
reduced data are not
it
is possible to exploit random sampling to approximate the
steps of online matrix factorization. Factors are learned in
the original space whereas the dimension of each iteration is
reduced together with the computational cost per iteration.

interpretable. On the other hand,

Contribution: The contribution of this paper is both prac-
tical and theoretical. We introduce a new matrix factoriza-
tion algorithm, called subsampled online matrix factorization
(SOMF), which is faster than state-of-the-art algorithms by an
order of magnitude on large real-world datasets (hyperspectral
images, large fMRI data). It leverages random sampling with
stochastic optimization to learn sparse and dense factors more
efﬁciently. To prove the convergence of SOMF, we extend
the stochastic majorization-minimization framework [10] and
make it robust to some time-saving approximations. We then
show convergence guarantees for SOMF under reasonable
assumptions. Finally, we propose an extensive empirical vali-
dation of the subsampling approach.

In a ﬁrst version of this work [26] presented at the Interna-
tional Conference in Machine Learning (ICML), we proposed
an algorithm similar to SOMF, without any theoretical guaran-
tees. The algorithm that we present here has such guarantees,
which we express in a more general framework, stochastic
is validated for new sparsity
majorization-minimization. It
settings and a new domain of application. An open-source
efﬁcient Python package is provided.

Notations: Matrices are written using bold capital letters
and vectors using bold small letters (e.g., X, α). We use
superscript
to specify the column (sample or component)
number, and write X = [x(1), . . . , x(n)]. We use subscripts
to specify the iteration number, as in xt. The ﬂoating bar,
as in ¯gt, is used to stress that a given value is an average
over iterations, or an expectation. The superscript (cid:63) is used to
denote an exact value, when it has to be compared to an inexact
value, e.g., to compare α(cid:63)

t (exact) to αt (approximation).

II. PRIOR ART: MATRIX FACTORIZATION WITH
STOCHASTIC MAJORIZATION-MINIMIZATION

Below, we introduce the matrix-factorization problem and
recall a speciﬁc stochastic algorithm to solve it observing
one column (or a mini-batch) at every iteration. We cast this
algorithm in the stochastic majorization-minimization frame-
work [10], which we will use in the convergence analysis.

A. Problem statement

In our setting, the goal of matrix factorization is to decom-
Rp×n — typically n signals of dimension p

pose a matrix X
— as a product of two smaller matrices:

∈

X

DA with D

Rp×k and A

Rk×n,

≈

∈
with potential sparsity or structure requirements on D and A.
In signal processing, sparsity is often enforced on the code A,
in a problem called dictionary learning [27]. In such a case,

∈

Learning the factorization is typically performed by min-
imizing a quadratic data-ﬁtting term, with constraints and/or
penalties over the code and the dictionary:

min
D∈C
A∈Rk×n

n
(cid:88)

i=1

1
2

(cid:13)
(cid:13)x(i)

Dα(i)(cid:13)
2
2 + λ Ω(α(i)),
(cid:13)

(1)

−

C

→

where A (cid:44) [α(1), . . . , α(n)],
is a column-wise separable
C
R is a penalty over the
convex set of Rp×k and Ω : Rp
code. Both constraint set and penalty may enforce structure or
sparsity, though
has traditionally been used as a technical
requirement to ensure that the penalty on A does not vanish
with D growing arbitrarily large. Two choices of
and Ω are
of particular interest. The problem of dictionary learning sets
C
as the (cid:96)2 ball for each atom and Ω to be the (cid:96)1 norm. Due to
the sparsifying effect of (cid:96)1 penalty [28], the dataset admits
a sparse representation in the dictionary. On the opposite,
ﬁnding a sparse set in which to represent a given dataset,
with a goal akin to sparse PCA [29], requires to set as the
(cid:96)1 ball for each atom and Ω to be the (cid:96)2 norm. Our work
considers the elastic-net constraints and penalties [30], which
encompass both special cases. Fixing ν and µ in [0, 1], we
the elastic-net penalty in Rp and Rk:
denote by Ω(

C

α
ν)
(cid:107)

−
(cid:44) (1

(cid:107)
µ)

ν
2 (cid:107)

α

2
2,
(cid:107)
µ
1+
2 (cid:107)
(cid:107)

1 +

d(j)
(cid:107)

(2)
(cid:111)
1

.

d(j)

2
2 ≤
(cid:107)

(cid:44)

(cid:110)

D

Rp×k/
(cid:107)

d(j)

(cid:107)

∈

C
−
Following [15], we can also enforce the positivity of D
and/or A by replacing R by R+ in
, and adding positivity
constraints on A in (1), as in non-negative sparse coding [31].
We rewrite (1) as an empirical risk minimization problem
depending on the dictionary only. The matrix D solution of (1)
is indeed obtained by minimizing the empirical risk ¯f

C

D

argmin
D∈C

∈

n
(cid:88)

(cid:17)

f (D, x(i))

(cid:16) ¯f (D) (cid:44) 1
n
Dα(cid:13)
2
2 + λ Ω(α),
(cid:13)

(cid:13)
(cid:13)x

i=1

,

(3)

where

1
2
and the matrix A is obtained by solving the linear regression

f (D, x) (cid:44) min
α∈Rk

−

) and
·

(cid:107) · (cid:107)
Ω(α) (cid:44) (1

min
A∈Rk×n

n
(cid:88)

i=1

1
2

(cid:13)
(cid:13)x(i)

Dα(i)(cid:13)
2
2 + λ Ω(α(i)).
(cid:13)

−

(4)

The problem (1) is non-convex in the parameters (D, A), and
hence (3) is not convex. However, the problem (1) is convex in
both D and A when ﬁxing one variable and optimizing with
respect to the other. As such, it is naturally solved by alternate
minimization over D and A, which asymptotically provides
a stationary point of (3). Yet, X has typically to be observed
hundred of times before obtaining a good dictionary. Alternate
minimization is therefore not adapted to datasets with many
samples.

B. Online matrix factorization

When X has a large number of columns but a limited
number of rows, the stochastic optimization method of [15]

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

3

Algorithm 1 Online matrix factorization (OMF) [15]

Input: Initial iterate D0, sample stream (xt)t>0, number of
iterations T .
for t from 1 to T do

∼ P

Draw xt
.
Compute αt = argminα∈Rp
Update the parameters of aggregated surrogate ¯gt:
(cid:17)

Dt−1α(cid:13)
2
2+λ Ω(α).
(cid:13)

(cid:13)
(cid:13)xt

−

(cid:16)

1
2

1

¯Ct =

¯Ct−1 +

1
t
1
t
Compute (using block coordinate descent):

¯Bt−1 +

αtα(cid:62)
t .

xtα(cid:62)
t .

¯Bt =

1
t
1
t

−

−

(cid:16)

(cid:17)

1

(8)

Dt = argmin

Tr (D(cid:62)D ¯Ct)

Tr (D(cid:62) ¯Bt).

−

1
2

D∈C

Output: Final iterate DT .

Algorithm 2 Stochastic majorization-minimization [SMM 10]
Input: Initial iterate θ0, weight sequence (wt)t>0, sample
stream (xt)t>0, number of iteration T .
for t from 1 to T do

Draw xt
∈
Construct a surrogate of ft near θt−1, that meets

, get ft : θ

f (xt, θ).

∼ P

→

Θ

gt

ft,

gt(θt−1) = ft(θt−1).

(12)

≥

Update the aggregated surrogate:

¯gt = (1

wt)¯gt−1 + wtgt.

−

Compute

θt = argmin

¯gt(θ).

θ∈Θ

(13)

Output: Final iterate θT .

outputs a good dictionary much more rapidly than alternate-
minimization. In this setting [see 32], learning the dictionary
is naturally formalized as an expected risk minimization

min
D∈C

¯f (D) (cid:44) Ex[f (D, x)],

where x is drawn from the data distribution and forms an i.i.d.
stream (xt)t. In the ﬁnite-sample setting, (5) reduces to (3)
when xt is drawn uniformly at random from
.
We then write it the sample number selected at time t.

[1, n]
}

x(i), i

∈

{

The online matrix factorization algorithm proposed in [15]
is summarized in Alg. 1. It draws a sample xt at each iteration,
and uses it to improve the current iterate Dt−1. For this, it
ﬁrst computes the code αt associated to xt on the current
dictionary:

αt (cid:44) argmin
α∈Rk

1
2 (cid:107)

xt

−

Dt−1α

2
2 + λΩ(α).
(cid:107)

(6)

Then, it updates Dt to make it optimal in reconstructing past
samples (xs)s≤t from previously computed codes (αs)s≤t:

Dt

argmin
D∈C

∈

(cid:16)

¯gt(D) (cid:44) 1
t

t
(cid:88)

s=1

1
2

(cid:13)
(cid:13)xs

−

Dαs

(cid:17)
(cid:13)
2
2 + λΩ(αs)
(cid:13)

.

(7)
Importantly, minimizing ¯gt is equivalent to minimizing the
quadratic function

D

→

1
2

Tr (D(cid:62)D ¯C(cid:62)
t )

Tr (D(cid:62) ¯Bt),

(9)

−

where ¯Bt and ¯Ct are small matrices that summarize previously
seen samples and codes:

¯Bt =

xsα(cid:62)
s

¯Ct =

αsα(cid:62)
s .

(10)

1
t

t
(cid:88)

s=1

1
t

t
(cid:88)

s=1

The function ¯gt is an upper-bound surrogate of the true
current empirical risk, whose deﬁnition involves the regression
minima computed on current dictionary D:

¯ft(D) (cid:44) 1
t

t
(cid:88)

s=1

(5)

min
α∈Rp

(cid:13)
(cid:13)xs

1
2

−

Dα(cid:13)
2
2+ λΩ(α)
(cid:13)

≤

¯gt(D). (11)

Using empirical processes theory [33], it is possible to show
that minimizing ¯ft at each iteration asymptotically yields
a stationary point of the expected risk (5). Unfortunately,
minimizing (11) is expensive as it involves the computation of
optimal current codes for every previously seen sample at each
iteration, which boils down to naive alternate-minimization.

In contrast, ¯gt is much cheaper to minimize than ¯ft, using
block coordinate descent. It is possible to show that ¯gt con-
verges towards a locally tight upper-bound of the objective ¯ft
and that minimizing ¯gt at each iteration also asymptotically
yields a stationary point of the expected risk (5). This es-
tablishes the correctness of the online matrix factorization
algorithm (OMF). In practice, the OMF algorithm performs a
single pass of block coordinate descent: the minimization step
is inexact. This heuristic will be justiﬁed by our theoretical
contribution in Section IV.

}

t
∈ T

Extensions: For efﬁciency, it is essential to use mini-batches
xs, s
of size η instead of single samples in the
{
iterations [15]. The surrogate parameters ¯Bt, ¯Ct are then
s∈Tt over
updated by the mean value of
}
the batch. The optimal size of the mini-batches is usually
close to k. (8) uses the sequence of weights ( 1
t )t to update
parameters ¯Bt and ¯Ct. [15] replaces these weights with a
sequence (wt)t, which can decay more slowly to give more
importance to recent samples in ¯gt. These weights will prove
important in our analysis.

(xsα(cid:62)
{

s , αsα(cid:62)
s )

C. Stochastic majorization-minimization

C

As the constraints
have a separable structure per atom, [15]
uses projected block coordinate descent to minimize ¯gt. The
¯Bt, and it is there-
function gradient writes
fore enough to maintain ¯Bt and ¯Ct in memory to solve (7).
¯Bt and ¯Ct are updated online, using the rules (8) (Alg. 1).

¯gt(D) = D ¯Ct

∇

−

Online matrix factorization belongs to a wider category
of algorithms introduced in [10] that minimize locally tight
upper-bounding surrogates instead of a more complex objec-
tive, in order to solve an expected risk minimization prob-
lem. Generalizing online matrix factorization, we introduce

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

4

−

(cid:107)

in Alg. 2 the stochastic majorization-minimization (SMM)
algorithm, which is at the core of our theoretical contribution.
In online matrix factorization, the true empirical risk func-
tions ¯ft and their surrogates ¯gt follow the update rules, with
generalized weight (wt)t set to ( 1
¯ft (cid:44) (1

wt)¯gt−1 + wtgt, (14)

t )t in (7) – (11):

wt) ¯ft−1 + wtft,

¯gt (cid:44) (1

−

where the pointwise loss function and its surrogate are

ft(D) (cid:44) min
α∈Rk
gt(D) (cid:44) 1
2 (cid:107)

xt

1
2 (cid:107)

xt

−

Dα

2
2 + λΩ(α),

Dαt

2
2 + λΩ(αt).
(cid:107)

−

(15)

≥

ft, and gt

The function gt is a majorizing surrogate of ft: gt
is tangent to ft in Dt−1, i.e, gt(Dt−1) = ft(Dt−1) and
(gt
ft)(Dt−1) = 0. At each step of online matrix factorization:
• The surrogate gt is computed along with αt, using (6).
• The parameters ¯Bt, ¯Ct are updated following (8). They
deﬁne the aggregated surrogate ¯gt up to a constant.
• The quadratic function ¯gt is minimized efﬁciently by
block coordinate descent, using parameters ¯Bt and ¯Ct
to compute its gradient.

∇

−

iteration t, a surrogate gt of the loss ft

The stochastic majorization-minimization framework sim-
ply formalizes the three steps above, for a larger variety of
loss functions ft(θ) (cid:44) f (θ, xt), where θ is the parameter we
want to learn (D in the online matrix factorization setting).
is computed
At
to update the aggregated surrogate ¯gt following (14). The
surrogate functions (gt)t should be upper-bounds of loss
functions (ft)t, tight
in the current iterate θt−1 (e.g., the
dictionary Dt−1). This simply means that ft(θt−1) = gt(θt−1)
gt)(θt−1) = 0. Computing ¯gt can be done if gt is
and
∇
−
deﬁned simply, as in OMF where it is linearly parametrized by
t , xtα(cid:62)
(αtα(cid:62)
t ). ¯gt is then minimized to obtain a new iterate θt.
It can be shown following [10] that stochastic majorization-
minimization algorithms ﬁnd asymptotical stationary point of
the expected risk Ex[f (θ, x)] under mild assumptions recalled
in Section IV. SMM admits the same mini-batch and decaying
weight extensions (used in Alg. 2) as OMF.

(ft

In this work, we extend the SMM framework and allow both
majorization and minimization steps to be approximated. As a
side contribution, our extension proves that performing a single
pass of block coordinate descent to update the dictionary, an
important heuristic in [15], is indeed correct. We ﬁrst introduce
the new matrix factorization algorithm at the core of this paper
and then present the extended SMM framework.

III. STOCHASTIC SUBSAMPLING FOR HIGH DIMENSIONAL
DATA DECOMPOSITION

The online algorithm presented in Section II is very efﬁcient
to factorize matrices that have a large number of columns (i.e.,
with a large number of samples n), but a reasonable number
of rows — the dataset is not very high dimensional. However,
it is not designed to deal with very high number of rows: the
cost of a single iteration depends linearly on p. On terabyte-
105 features, the original
scale datasets from fMRI with p = 2
online algorithm requires one week to reach convergence. This

·

Fig. 1. Stochastic subsampling further improves online matrix factorization to
handle datasets with large number of columns and rows. X is the input p × n
matrix, Dt and At are respectively the dictionary and code at time t.

is a major motivation for designing new matrix factorization
algorithms that scale in both directions.

In the large-sample regime p

k, the underlying dimen-
sionality of columns may be much lower than the actual p:
the rows of a single column drawn at random are therefore
correlated and redundant. This guides us on how to scale
online matrix factorization with regard to the number of rows:

(cid:29)

• The online algorithm OMF uses a single column of (or
mini-batch) of X at each iteration to enrich the average
surrogate and update the whole dictionary.

• We go a step beyond and use a fraction of a single column

of X to reﬁne a fraction of the dictionary.

More precisely, we draw a column and observe only some
to reﬁne these rows of the
of its rows at each iteration,
dictionary, as illustrated in Figure 1. To take into account
all features from the dataset, rows are selected at random at
each iteration: we call this technique stochastic subsampling.
Stochastic subsampling reduces the efﬁciency of the dictionary
update per iteration, as less information is incorporated in the
current iterate Dt. On the other hand, with a correct design,
the cost of a single iteration can be considerably reduced, as it
grows with the number of observed features. Section V shows
that the proposed algorithm is an order of magnitude faster
than the original OMF on large and redundant datasets.

First, we formalize the idea of working with a fraction of
the p rows at a single iteration. We adapt the online matrix
factorization algorithm, to reduce the iteration cost by a factor
close to the ratio of selected rows. This deﬁnes a new on-
line algorithm, called subsampled online matrix factorization
(SOMF). At each iteration, it uses q rows of the column xt to
update the sequence of iterates (Dt)t. As in Section II, we
introduce a more general algorithm, stochastic approximate
majorization-minimization (SAMM), of which SOMF is an
instance. It extends the stochastic majorization-minimization
framework, with similar theoretical guarantees but potentially
faster convergence.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

5

A. Subsampled online matrix factorization

∈

[1, n]

Formally, as in online matrix factorization, we consider a
sample stream (xt)t in Rp that cycles onto a ﬁnite sample set
x(i), i
{
1) Stochastic subsampling and algorithm outline: We want
to reduce the time complexity of a single iteration. In the
original algorithm, the complexity depends linearly on the
sample dimension p in three aspects:

, and minimize the empirical risk (3).1
}

∈

∈

∈

Rp×k,

Rp is used to compute the code αt,
• xt
• it is used to update the surrogate parameters ¯Bt
• Dt

Rp×k is fully updated at each iteration.
Our algorithm reduces the dimensionality of these steps at
each iteration, such that p becomes q = p
r in the time
complexity analysis, where r > 1 is a reduction factor.
Formally, we randomly draw, at iteration t, a mask Mt that
“selects” a random subset of xt. We use it to drop a part of the
features of xt and to “freeze” these features in dictionary D
at iteration t.

It is convenient to consider Mt as a Rp×p random diagonal
matrix, such that each coefﬁcient is a Bernouilli variable with
parameter 1
1],

r , normalized to be 1 in expectation.

[0, p

j
∀
, P(cid:2)Mt[j, j] = 0(cid:3) = 1

∈
1
r

.

−

−

(16)

P(cid:2)Mt[j, j] = r(cid:3) =

1
r

Thus, r describes the average proportion of observed features
and Mtxt is a non-biased, low-dimensional estimator of xt:

E(cid:2)
Mtxt
(cid:107)

(cid:107)

(cid:3) =

0

p
r

= q

E(cid:2)Mtxt

(cid:3) = xt.

(17)

(cid:107) · (cid:107)

0 counting the number of non-zero coefﬁcients. We
with
deﬁne the pair of orthogonal projectors Pt
t ∈
R(p−q)×p that project Rp onto Im(Mt) and Ker(Mt). In other
Rp×y
words, PtY and P⊥
with rows respectively selected and not selected by Mt. In
Rq×n assigns the rows of Z to the
algorithms, PtY
rows of Y selected by Pt, by an abuse of notation.

t Y are the submatrices of Y

Rq×p and P⊥

←

Z

∈

∈

∈

In brief, subsampled online matrix factorization, deﬁned in
Alg. 3, follows the outer loop of online matrix factorization,
with the following major modiﬁcations at iteration t:

• it uses Mtxt and low-size statistics instead of xt to

estimate the code αt and the surrogate gt,

• it updates a subset of the dictionary PtDt−1 to reduce
the surrogate value ¯gt(D). Relevant parameters of ¯gt are
computed using Ptxt and αt only.

We now present SOMF in details. For comparison purpose,
we write all variables that would be computed following the
OMF rules at iteration t with a (cid:63) superscript. For simplicity, in
Alg. 3 and in the following paragraphs, we assume that we use
one sample per iteration —in practice, we use mini-batches of
size η. The next derivations are transposable when a batch It
is drawn at iteration t instead of a single sample it.

2) Code computation: In the OMF algorithm presented in
t is obtained by solving (6), namely

Section II, α(cid:63)

α(cid:63)

t ∈

argmin
α

1
2

α(cid:62)G(cid:63)

t α

α(cid:62)β(cid:63)

t + λΩ(α),

(21)

−

Algorithm 3 Subsampled online matrix factorization (SOMF)
(wt)t>0,

iterate D0, weight

sequences

Initial

Input:
(γc)c>0, sample set
{
for t from 1 to T do

x(i)

}i>0, number of iterations T .
Draw xt = x(i) at random and Mt following (16).
Update the regression parameters for sample i:

c(i)
β(i)
G(i)

c(i) + 1,

(1

←
t ←
t ←
Compute the approximate code for xt:

t−1 + γD(cid:62)
t−1 + γD(cid:62)

t−1Mtx(i),
t−1MtDt−1, Gt

γ)G(i)
γ)G(i)

(1

−

−

γ
←
βt ←
←

γc(i) .
β(i)
.
t
¯G(i)
t

.

αt

argmin
α∈Rk

←

1
2

α(cid:62)Gtα

α(cid:62)βt + λ Ω(α).

(18)

−

Update the parameters of the aggregated surrogate ¯gt:

¯Ct
Pt ¯Bt

(1

(1

−

−

←

←

wt) ¯Ct−1 + wtαtα(cid:62)
t .
wt)Pt ¯Bt−1 + wtPtxtα(cid:62)
t .

Compute simultaneously (using Alg. 4 for 1st line):

PtDt

P⊥
t

¯Bt

argmin
Dr∈Cr
(1

1
2
wt)P⊥
t

−

←

←

Tr (Dr (cid:62)Dr ¯Ct)

Tr (Dr (cid:62)Pt ¯Bt).

−
¯Bt−1 + wtP⊥
t xtα(cid:62)
t .

(19)

(20)

Output: Final iterate DT .

t = D(cid:62)

t = D(cid:62)

t−1Dt−1 and β(cid:63)
t and β(cid:63)

where G(cid:63)
t−1xt. For large p, the
computation of G(cid:63)
t dominates the complexity of the
regression step, which depends almost linearly on p. To reduce
this complexity, we use estimators for G(cid:63)
t , computed
at a cost proportional to the reduced dimension q. We propose
three kinds of estimators with different properties.

t and β(cid:63)

a) Masked loss: The most simple unbiased estimation
t and β(cid:63)
t whose computation cost depends on q is

of G(cid:63)
obtained by subsampling matrix products with Mt:
Gt = D(cid:62)
βt = D(cid:62)

t−1MtDt−1
t−1Mtxt.

(a)

This is the strategy proposed in [26]. We use Gt and βt
in (18), which amounts to minimize the masked loss

min
α∈Rk

1
2 (cid:107)

Mt(xt

D(cid:62)

t−1α)
(cid:107)

−

2
2 + λΩ(α).

(22)

t and β(cid:63)

Gt and βt are computed in a number of operations pro-
portional to q, which brings a speed-up factor of almost r
in the code computation for large p. On large data, using
estimators (a) instead of exact G(cid:63)
t proves very ef-
ﬁcient during the ﬁrst epochs (cycles over the columns).2
However, due to the masking, Gt and βt are not consistent
estimators: they do not converge to G(cid:63)
t for large t,
which breaks theoretical guarantees on the algorithm output.
Empirical results in Section V-E show that the sequence of
iterates approaches a critical point of the risk (3), but may
then oscillate around it.

t and β(cid:63)

1Note that we solve the fully observed problem despite the use of subsam-

2Estimators (a) are also available in the inﬁnite sample setting, when

pled data, unlike other recent work on low-rank factorization [34].

minimizing expected risk (5) from a i.i.d sample stream (xt)t.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

6

{

x(i)

b) Averaging over epochs: At iteration t, the sample xt
}i. This allows to
is drawn from a ﬁnite set of samples
average estimators over previously seen samples and address
the non-consistency issue of (a). Namely, we keep in memory
2n estimators, written (G(i)
t )1≤i≤n. We observe the
t
sample i = it at iteration t and use it to update the i-th
estimators ¯G(i)
t
β(i)
G(i)

following
γ)G(i)
γ)G(i)

t−1Mtx(i)
t−1MtD(i)

t−1 + γD(cid:62)
t−1 + γD(cid:62)

t = (1

t = (1

, ¯β(i)

, β(i)

(23)

−

,

t

t

−

where γ is a weight factor determined by the number of time
the one sample i has been previously observed at time t.
Precisely, given (γc)c a decreasing sequence of weights,
c(i)
t =

t, xs = x(i)(cid:111)(cid:12)
(cid:12)
(cid:12) .

γ = γc(i)

where

(cid:110)
s

(cid:12)
(cid:12)
(cid:12)

t

≤

All others estimators
iteration t
−
averaged estimators
Gt (cid:44) G(i)

t =

G(j)
t
{
G(i)
1. The set
t
{

, β(j)
t }j(cid:54)=i are left unchanged from
, β(i)
t }1≤i≤n is used to deﬁne the

(cid:88)

γ(i)
s,tD(cid:62)

s−1MsDs−1

βt

(cid:44) β(i)

t =

γ(i)
s,tD(cid:62)

s−1Msx(i),

(b)

s≤t,xs=x(i)
(cid:88)

s≤t,xs=x(i)

(cid:81)

s,t = γc(i)

where γ(i)
). Using βt and Gt
in (18), αt minimizes the masked loss averaged over the
previous iterations where sample i appeared:

s<t,xs=x(i) (1

γc(i)

−

s

t

γ(i)
s,t
2 (cid:107)

min
α∈Rk

(cid:88)

s≤t
xs=x(i)

Ms(x(i)

D(cid:62)

2
2 + λΩ(α).
s−1α)
(cid:107)

−

(24)

t )t and (β(cid:63)

The sequences (Gt)t and (βt)t are consistent estimations
of (G(cid:63)
t )t — consistency arises from the fact
that a single sample x(i) is observed with different masks
along iterations. Solving (24) is made closer and closer to
solving (21), to ensure the correctness of the algorithm (see
Section IV). Yet, computing the estimators (b) is no more
costly than computing (a) and still permits to speed up a single
iteration close to r times. In the mini-batch setting, for every
and β(i)
to compute α(i)
It, we use the estimators G(i)
i
.
t
t
t
(n k2). This is reasonable
This method has a memory cost of
k2.
compared to the dataset size3 if p

∈

c) Exact Gram computation: To reduce the memory
usage, another strategy is to use the true Gram matrix Gt
and the estimator βt from (b):
t = D(cid:62)

O
(cid:29)

Gt (cid:44) G(cid:63)
βt

(cid:44) (cid:88)

t−1Dt−1
γ(i)
s,tD(cid:62)

s−1Msx(i)

(c)

s≤t,xs=x(i)

As previously, the consistency of (βt)t ensures that (5) is
correctly solved despite the approximation in (αt)t computa-
tion. With the partial dictionary update step we propose, it is
possible to maintain Gt at a cost proportional to q. The time

3It is also possible to efﬁciently swap the estimators (G(i)

t )i on disk, as

they are only accessed for i = it at iteration t.

TABLE I
COMPARISON OF ESTIMATORS USED FOR CODE COMPUTATION

Est.

(a)
(b)
(c)

βt

Gt

Convergence

Extra
mem. cost

Masked
Averaged
Averaged

Masked
Averaged
Exact

(cid:88)
(cid:88)

n k2
n k

1st epoch
perform.
(cid:88)
(cid:88)

O

complexity of the coding step is thus similarly reduced when
replacing (b) or (c) estimators in (21), but the latter option
(n k). Although estimators (c) are
has a memory usage in
slightly less performant in the ﬁrst epochs, they are a good
compromise between resource usage and convergence. We
summarize the characteristics of the three estimators (a)–(c) in
Table I, anticipating their empirical comparison in Section V.
Surrogate computation: The computation of αt using one

of the estimators above deﬁnes a surrogate gt(D) (cid:44) 1
2 (cid:107)
−
2
Dαt
2 + λΩ(α), which we use to update the aggregated
(cid:107)
surrogate ¯gt (cid:44) (1
wt)¯gt−1 + wtgt, as in online matrix
factorization. We follow (8) (with weights (wt)t) to update the
matrices ¯Bt and ¯Ct, which deﬁne ¯gt up to constant factors.
The update of ¯Bt requires a number of operations proportional
to p. Fortunately, we will see in the next paragraph that it is
possible to update Pt ¯Bt in the main thread with a number
of operation proportional to q and to complete the update of
P⊥
t

¯Bt in parallel with the dictionary update step.
Weight sequences: Speciﬁc (wt)t and (γc)c in Alg. 3
are required. We provide then in Assumption (B) of the
( 11
analysis: wt = 1
12 , 1) and
cv , where u
2(cid:1) to ensure convergence. Weights have little
(cid:0) 3
4 , 3u
v
impact on convergence speed in practice.

tu and γc = 1

xt

−

−

∈

∈

3) Dictionary update: In the original online algorithm, the
whole dictionnary Dt−1 is updated at iteration t. To reduce the
time complexity of this step, we add a “freezing” constraint to
the minimization (7) of ¯gt. Every row r of D that corresponds
to an unseen row r at iteration r (such that Mt[r, r] = 0)
remains unchanged. This casts the problem (7) into a lower
dimensional space. Formally, the freezing operation comes out
as a additional constraint in (7):

Dt =

argmin
D∈C
t D=P⊥

P⊥

t Dt−1

1
2

Tr (D(cid:62)D ¯Ct)

Tr (D(cid:62) ¯Bt).

(25)

−

1 and P⊥

The constraints are separable into two blocks of rows. Re-
calling the notations of (2), for each atom d(j), the rules
t d(j)
d(j)
t d(j) = P⊥
t−1 can indeed be rewritten
(cid:40)
d(j)
− (cid:107)
t d(j)
t−1.

Ptd(j)
(cid:107)
t d(j)
P⊥

1
= P⊥

Ptd(j)
(cid:107)

(cid:44) r(j)
t

t−1(cid:107)

t−1(cid:107)

(cid:107) ≤

(26)

(cid:107) ≤

+

(cid:107)

Solving (25) is therefore equivalent to solving the following
(cid:44) PtBt,
problem in Rq×k, with Br
t

Dr

∈
r =

argmin
Dr∈Cr
Dr
{

∈

1
2
Rq×k/
j
∀

where

Tr (Dr (cid:62)Dr ¯Ct)

Tr (Dr (cid:62) ¯Br

t ) (27)

−

1],

[0, k

dr(j)
(cid:107)

r(j)
t }

.

C

∈
The rows of Dt selected by Pt are then replaced with Dr,
while the other rows of Dt are unchanged from iteration

(cid:107) ≤

−

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

7

Algorithm 4 Partial dictionary update

Gt

D(cid:62)

t−1PtDt−1.

Input: Dictionary Dt−1, projector Pt, statistics ¯Ct, ¯Bt,
norms (n(j)
t−1)0≤j<k, Gram matrix Gt (optional).
Dt−1,
Dt
Gt
←
←
−
permutation([1, k]) do
for j
∈
n(j)
r(j)
Ptd(j)
t−1 +
.
t ←
t−1(cid:107)
(cid:107)
¯Ct[j,j] (Pt ¯b(j)
Ptd(j)
t−1 + 1
u
←
enet projection(u, r(j)
Ptd(j)
Ptd(j)
n(j)
.
t (cid:107)
t ←
t PtDt.
←

Gt+1
Output: Dictionary Dt, norms (n(j)

t ←
r(j)
t − (cid:107)
Gt + D(cid:62)

PtDt¯c(j)
t ).

t )j, Gram matrix Gt+1.

(cid:46) in Rq
(cid:46) in Rq

t −

t ).

t

t

∈

−

t −

¯br(j)
t

and ¯br(j)

t Dt = P⊥

, at an average cost in

1. Formally, PtDt = Dr and P⊥

k operations, as it writes Dr¯c(j)

t Dt−1. We
t
solve (27) by a projected block coordinate descent (BCD)
similar to the one used in the original algorithm, but performed
in a subspace of size q. We compute each column j of the
gradient that we use in the block coordinate descent loop
Rq,
with q
×
where ¯c(j)
are the j-th columns of ¯Ct and ¯Br
t .
t
Each reduced atom dr(j) is projected onto the elastic-net ball
of radius r(j)
(q) following [15]. This
makes the complexity of a single-column update proportional
to q. Performing the projection requires to keep in memory the
d(j)
(cid:44) 1
values
t (cid:107)}j, which can be updated online at
{
a negligible cost.
We provide the reduced dictionary update step in Alg. 4,
where we use the function enet projection(u, r) that per-
Rq onto the elastic-net
forms the orthogonal projection of u
ball of radius r. As in the original algorithm, we perform a
single pass over columns to solve (27). Dictionary update is
now performed with a number of operations proportional to q,
instead of p in the original algorithm. Thanks to the random
nature of (Mt)t, updating Dt−1 into Dt reduces ¯gt enough
to ensure convergence.

n(j)
t

− (cid:107)

O

∈

t with a cost in

Gram matrix computation: Performing partial updates
of Dt makes it possible to maintain the full Gram matrix
Gt = G(cid:63)
(q k2) per iteration, as mentioned
O
in III-A2c. It is indeed enough to compute the reduced Gram
matrix D(cid:62)PtD before and after the dictionary update:
D(cid:62)
t PtD(cid:62)
t .

Gt+1 = D(cid:62)

t−1 + D(cid:62)

t−1PtD(cid:62)

t Dt = Gt

(28)

−

Parallel surrogate computation: Performing block coor-
t = Pt ¯Bt only.
t requires to access ¯Br
dinate descent on ¯gr
Assuming we may use use more than two threads, this allows
to parallelize the dictionary update step with the update
of P⊥
t
Pt ¯Bt

¯Bt. In the main thread, we compute Pt ¯Bt following

wt) ¯PtBt−1 + wtPtxtα(cid:62)
t .

(19 – Alg. 3)

(1

←

−

which has a cost proportional to q. Then, we update in parallel
the dictionary and the rows of ¯Bt that are not selected by Mt:
¯Bt−1 + wtP⊥

(20 – Alg. 3)

t xtα(cid:62)
t .

wt)P⊥
t

P⊥
t

¯Bt

(1

←

−

This update requires k(p
q)η operations (one matrix-matrix
product) for a mini-batch of size η. In contrast, with appropri-
ate implementation, the dictionary update step requires 4 k q2

−

∼
∼

to 6 k q2 operations, among which 2 k q2 come from slower
η, updating ¯Bt is faster
matrix-vector products. Assuming k
10, and performing (20)
than updating the dictionary up to r
on a second thread is seamless in term of wall-clock time.
More threads may be used for larger reduction or batch size.
4) Subsampling and time complexity: Subsampling may be
used in only some of the steps of Alg. 3, with the other
steps following Alg. 1. Whether to use subsampling or not in
each step depends on the trade-off between the computational
speed-up it brings and the approximations it makes. It is useful
to understand how complexity of OMF evolves with p. We
write s the average number of non-zero coefﬁcients in (αt)t
(s = k when Ω =

2
2). OMF complexity has three terms:

(i)

(ii)

(p k2): computation of the Gram matrix Gt, update of

O
the dictionary Dt with block coordinate descent,

(p k η): computation of βt = D(cid:62)

t−1xt and of ¯Bt

(cid:107) · (cid:107)

O
using xtα(cid:62)
t ,

(iii)

(k s2 η): computation of αt using Gt and βt, using

O
matrix inversion or elastic-net regression.

∼

Using subsampling turns p into q = p
r in the expressions
above. It improves single iteration time when the cost of re-
(k s2 η) is dominated by another term. This happens
gression
O
whenever p
r > s2, where r is the reduction factor used in the
algorithm. Subsampling can bring performance improvement
p
up to r
s2 . It can be introduced in either computations
from (i) or (ii), or both. When using small batch size, i.e.,
when η < k, computations from (i) dominates complexity, and
subsampling should be ﬁrst introduced in dictionary update (i),
and for code computation (ii) beyond a certain reduction ratio.
On the other hand, with large batch size η > k, subsampling
should be ﬁrst introduced in code computation, then in the
dictionary update step. The reasoning above ignore potentially
large constants. The best trade-offs in using subsampling must
be empirically determined, which we do in Section V.

B. Stochastic approximate majorization-minimization

The SOMF algorithm can be understood within the stochastic
majorization-minimization framework. The modiﬁcations that
we propose are indeed perturbations to the ﬁrst and third steps
of the SMM presented in Algorithm 2:

• The code is computed approximately:

the surrogate
is only an approximate majorizing surrogate of ft
near Dt−1.

• The surrogate objective is only reduced and not mini-
mized, due to the added constraint and the fact that we
perform only one pass of block coordinate descent.

We propose a new stochastic approximate majorization-
minimization (SAMM) framework handling these perturbations:
• A majorization step (12 – Alg. 2), computes an approx-
g(cid:63)
t , where gt is a

imate surrogate of ft near θt−1: gt
true upper-bounding surrogate of ¯ft.

≈

• A minimization step (13 – Alg. 2), ﬁnds θt by reducing
(cid:44) argminθ∈Θ ¯gt(θ),

θ(cid:63)
t

enough the objective ¯gt: θt
which implies ¯gt(θt) (cid:38) ¯gt(θ(cid:63)

≈
t ).

The SAMM framework is general, in the sense that approxima-
tions are not speciﬁed. The next section provides a theoretical

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

8

analysis of the approximation of SAMM and establishes how
SOMF is an instance of SAMM. It concludes by establishing
Proposition 1, which provides convergence guarantees for
SOMF, under the same assumptions made for OMF in [15].

IV. CONVERGENCE ANALYSIS
We establish the convergence of SOMF under reasonable
assumptions. For the sake of clarity, we ﬁrst state our principal
result (Proposition 1), that guarantees SOMF convergence. It is
a corollary of a more general result on SAMM algorithms. To
present this broader result, we recall the theoretical guarantees
of the stochastic majorization-minimization algorithm [10]
(Proposition 2); then, we show how the algorithm can with-
stand pertubations (Proposition 3). Proofs are reported in
Appendix A. SAMM convergence is proven before establishing
SOMF convergence as a corollary of this broader result.

A. Convergence of SOMF

Similar to [15, 34], we show that the sequence of iterates
(Dt)t asymptotically reaches a critical point of the empirical
risk (3). We introduce the same hypothesis on the code
covariance estimation ¯Ct as in [15] and a similar one on Gt —
they ensure strong convexity of the surrogate and boundedness
of (αt)t. They do not cause any loss of generality as they are
met in practice after a few iterations, if r is chosen reasonably
low, so that q > k. The following hypothesis can also be
guaranteed by adding small (cid:96)2 regularizations to ¯f .

(A) There exists ρ > 0 such that for all t > 0, ¯Ct, Gt

ρI.

(cid:31)

We further assume, that the weights (wt)t and (γc)c decay
at speciﬁc rates. We specify simple weight sequences, but the
proofs can be adapted for more complex ones.

(B) There exists u

(cid:0) 3
4 , 3u
for all t > 0, c > 0, wt = t−u, γc (cid:44) c−v.

( 11
12 , 1) and v

∈

∈

−

2) such that,

The following convergence result then applies to any se-
quence (Dt)t produced by SOMF, using estimators (b) or (c).
¯f is the empirical risk deﬁned in (3).

Proposition 1 (SOMF convergence). Under assumptions (A)
and (B), ¯f (Dt) converges with probability one and every limit
point D∞ of (Dt)t is a stationary point of ¯f : for all D

¯f (D∞, D

D∞)

0

∇

−
This result applies for any positive subsampling ratio r,
which may be set arbitrarily high. However, selecting a
reasonable ratio remains important for performance.

≥

Proposition 1 is a corollary of a stronger result on SAMM
algorithms. As it provides insights on the convergence mech-
anisms, we formalize this result in the following.

∈ C
(29)

B. Basic assumptions and results on SMM convergence

We ﬁrst recall the main results on stochastic majorization-
minimization algorithms, established in [10], under assump-
tions that we slightly tighten for our purpose. In our setting,
we consider the empirical risk minimization problem
(cid:16) ¯f (θ) (cid:44) 1
n

(cid:17)
f (θ, x(i))

min
θ∈Θ

n
(cid:88)

(30)

,

i=1

where f : RK

R is a loss function and

× X →

(C) Θ

RK and the support

of the data are compact.

⊂

X

This is a special case of (5) where the samples (xt)t are
i. The loss functions ft (cid:44)
x(i)
{
, xt) deﬁned on RK can be non-convex. We instead assume
·

drawn uniformly from the set
f (
that they meet reasonable regularity conditions:

}

(D) (ft)t is uniformly R-Lipschitz continuous on RK and

uniformly bounded on Θ.

(E) The directional derivatives [35]
¯f (θ, θ(cid:48)

∇
θ) exist for all θ and θ(cid:48) in RK.

ft(θ, θ(cid:48)

θ) and

−

∇

−

Assumption (E) allows to characterize the stationary points
0
Θ such that
of problem (30), namely θ
for all θ(cid:48)
Θ — intuitively a point is stationary when there
is no local direction in which the objective can be improved.
the deﬁnition of ﬁrst-order surrogate
functions used in the SMM algorithm. (gt)t are selected in
the set

ρ,L(ft, θt−1), hereby introduced.

Let us now recall

¯f (θ, θ(cid:48)

θ)

∇

−

≥

∈

∈

S

Deﬁnition 1 (First-order surrogate function). Given a function
f : RK
R, θ
ρ,L(f, θ) as
the set of functions g : RK

Θ and ρ, L > 0, we deﬁne

R such that

→

∈

S

→
• g is majorizing f on Θ and g is ρ-strongly convex,
• g and f are tight at θ — i.e., g(θ) = f (θ), g

differentiable,

f ) is L-Lipschitz,

(g

∇

−

(g

∇

−

In OMF, gt deﬁned in (15) is a variational surrogate4 of ft.
We refer the reader to [36] for further examples of ﬁrst-order
surrogates. We also ensure that ¯gt should be parametrized and
thus representable in memory. The following assumption is
met in OMF, as ¯gt is parametrized by the matrices ¯Ct and ¯Bt.

f is
−
f )(θ) = 0.

(F) Parametrized surrogates. The surrogates (¯gt)t are
RP . Namely,
such that ¯gt is unequivocally

parametrized by vectors in a compact set
for all t > 0, there exists κt
deﬁned as gt (cid:44) ¯gκt.

K ⊂

∈ K

Finally, we ensure that the weights (wt)t used in Alg. 2

decrease at a certain rate.

(G) There exists u

( 3
4 , 1) such that wt = t−u.

∈

When (θt)t is the sequence yielded by Alg. 2, the following
result (Proposition 3.4 in [10]) establishes the convergence
of ( ¯f (θt))t and states that θt is asymptotically a stationary
point of the ﬁnite sum problem (30), as a special case of the
expected risk minimization problem (5).

Proposition 2 (Convergence of SMM, from [10]). Under as-
sumptions (C) – (G), ( ¯f (θt))t≥1 converges with probability
one. Every limit point θ∞ of (θt)t is a stationary point of the
risk ¯f deﬁned in (30). That is,

θ
∀

∈

Θ,

∇

¯f (θ∞, θ

θ∞)

0.

≥

−

(31)

The correctness of the online matrix factorization algorithm

can be deduced from this proposition.

4In this case as in SOMF, gt is not ρ-strongly convex but ¯gt is, thanks to

assumption (A). This is sufﬁcient in the proofs of convergence.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

9

C. Convergence of SAMM

We now introduce assumptions on the approximations made
in SAMM, before extending the result of Proposition 2. We
make hypotheses on both the surrogate computation (ma-
jorization) step and the iterate update (minimization) step. The
principles of SAMM are illustrated in Figure 2, which provides
a geometric interpretation of the approximations introduced in
the following assumptions (H) and (I).

1) Approximate surrogate computation: The SMM algo-
rithm selects a surrogate for ft at point θt−1 within the set
ρ,L(ft, θt−1). Surrogates within this set are tight at θt−1
S
and greater than ft everywhere. In SAMM, we allow the use
of surrogates that are only approximately majorizing ft and
approximately tight at θt−1. This is indeed what SOMF does
when using estimators in the code computation step. For that
ρ,L(f, θ, (cid:15)), that contains all
purpose, we introduce the set
T
ρ,L(f, θ) for the (cid:96)∞-norm:
functions (cid:15)-close of a surrogate in

S

Deﬁnition 2 (Approximate ﬁrst-order surrogate function).
Given a function f : RK
R, θ
ρ,L(f, θ, (cid:15))
→
is the set of ρ-strongly convex functions g : RK
R
such that

Θ and (cid:15) > 0,

→

∈

T

• g is (cid:15)-majorizing f on Θ:
• g and f are (cid:15)-tight at θ — i.e., g(θ)

∈

κ

∀

Θ, g(κ)

−
f (θ)

f (κ)

is differentiable,

(g

∇

−

f ) is L-lipschitz.

−

(cid:15),
f

≥ −
(cid:15), g
−

≤

We assume that SAMM selects an approximative surrogate
ρ,L(ft, θt−1, (cid:15)t) at each iteration, where ((cid:15)t)t is a deter-
in
ministic or random non-negative sequence that vanishes at a
sufﬁcient rate.

T

∈ O

∞ 0 almost surely.

(H) For all t > 0, there exists (cid:15)t > 0 such that gt
∈
ρ,L(ft, θt−1, (cid:15)t). There exists a constant η > 0 such that
T
E[(cid:15)t]

(t2(u−1)−η) and (cid:15)t

→
As illustrated on Figure 2, given the OMF surrogate g(cid:63)
t ∈
ρ,L(ft, θt−1) deﬁned in (15), any function gt such that
S
ρ,L(ft, θt−1, (cid:15)) — e.g., where gt uses
gt
(cid:107)
an approximate αt in (15). This assumption can also be met in
matrix factorization settings with difﬁcult code regularizations,
that require to make code approximations.

∞ < (cid:15) is in

g(cid:63)
t (cid:107)

−

T

2) Approximate surrogate minimization: We do not re-
quire θt to be the minimizer of ¯gt any longer, but ensure that
the surrogate objective function ¯gt decreases “fast enough”.
Namely, θt obtained from partial minimization should be
closer to a minimizer of ¯gt than θt−1. We write (
t)t and
(
the ﬁltrations induced by the past of the algorithm,
Ft− 1
respectively up to the end of iteration t and up to the beginning
of the minimization step in iteration t. Then, we assume

F

)

t

2

(I) For all t > 0, ¯gt(θt) < ¯gt(θt−1). There exists µ > 0

such that, for all t > 0, where θ(cid:63)

t = argminθ∈Θ ¯gt(θ),

(1

µ)(¯gt(θt−1)

¯gt(θ(cid:63)

t )). (32)

E[¯gt(θt)

¯gt(θ(cid:63)
t )

]

|Ft− 1

2

≤

−

−
Assumption (I) is met by choosing an appropriate method
for the inner ¯gt minimization step — a large variety of
gradient-descent algorithms indeed have convergence rates of
the form (32). In SOMF, the block coordinate descent with
frozen coordinates indeed meet this property, relying on results

−

Fig. 2. Both steps of SAMM make well-behaved approximations. The
operations that are performed in exact SMM are in green and superscripted
by (cid:63), while the actual computed values are in orange. Light bands recall the
bounds on approximations assumed in (H) and (I).

from [37]. When both assumptions are met, SAMM enjoys the
same convergence guarantees as SMM.

3) Asymptotic convergence guarantee: The following
proposition guarantees that the stationary point condition of
Proposition 2 holds for the SAMM algorithm, despite the use
of approximate surrogates and approximate minimization.

Proposition 3 (Convergence of
tions (C) – (I),
for SAMM.

SAMM). Under assump-
the conclusion of Proposition 2 holds

Assumption (H) is essential to bound the errors introduced
by the sequence ((cid:15)t)t in the proof of Proposition 3, while (I)
is the key element to show that the sequence of iterates (θt)t
is stable enough to ensure convergence. The result holds for
any subsampling ratio r, provided that (A) remains true.

4) Proving SOMF convergence: Assumptions (A) and (B)
readily implies (C)–(G). With Proposition 3 at hand, proving
Proposition 1 reduces to ensure that the surrogate sequence of
SOMF meets (H) while its iterate sequence meets (I).

V. EXPERIMENTS

The SOMF algorithm is designed for datasets with large
number of samples n and large dimensionality p. Indeed,
as detailed in Section III-A, subsampling removes the com-
putational bottlenecks that arise from high dimensionality.
Proposition 1 establishes that the subsampling used in SOMF
is safe, as it enjoys the same guarantees as OMF. However,
as with OMF, no convergence rate is provided. We therefore
perform a strong empirical validation of subsampling.

We tackle two different problems, in functional Magnetic
Resonance Imaging (fMRI) and hyperspectral imaging. Both
involve the factorization of very large matrices X with sparse
factors. As the data we consider are huge, subsampling reduces
the time of a single iteration by a factor close to p
q . Yet it
is also much redundant: SOMF makes little approximations
and accessing only a fraction of the features per iteration
should not hinder much the reﬁnement of the dictionary. Hence
high speed-ups are expected — and indeed obtained. All
experiments can be reproduced using open-source code.

A. Problems and datasets

1) Functional MRI: Matrix factorization has long been
used on functional Magnetic Resonance Imaging [18]. Data
are temporal series of 3D images of brain activity and are

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

10

decomposed into spatial modes capturing regions that activate
synchronously. They form a matrix X where columns are
the 3D images, and rows corresponds to voxels. Interesting
dictionaries for neuroimaging capture spatially-localized com-
ponents, with a few brain regions. This can be obtained by
enforcing sparsity on the dictionary: we use an (cid:96)2 penalty and
the elastic-net constraint. SOMF streams subsampled 3D brain
records to learn the sparse dictionary D. Data can be huge:
106 (2000
we use the whole HCP dataset [38], with n = 2.4
105, totaling 2 TB
records, 1 200 time points) and p = 2
of dense data. For comparison, we also use a smaller public
dataset (ADHD200 [39]) with 40 records, n = 7000 samples
104 voxels. Historically, brain decomposition have
and p = 6
been obtained by minimizing the classical dictionary learning
objective on transposed data [40]: the code A holds sparse
spatial maps and voxel time-series are streamed. This is not a
natural streaming order for fMRI data as X is stored column-
wise on disk, which makes the sparse dictionary formulation
more appealing. Importantly, we seek a low-rank factorization,
to keep the decomposition interpretable — k

100

p.

·

·

·

2) Hyperspectral imaging: Hyperspectral cameras acquire
images with many channels that correspond to different spec-
tral bands. They are used heavily in remote sensing (satellite
imaging), and material study (microscopic imaging). They
yield digital images with around 1 million pixels, each as-
sociated with hundreds of spectral channels. Sparse matrix
factorization has been widely used on these data for image
classiﬁcation [41, 42] and denoising [43, 44]. All methods
rely on the extraction of full-band patches representing a local
image neighborhood with all channels included. These patches
are very high dimensional, due to the number of spectral
bands. From one image of the AVIRIS project [45], we extract
106 patches of size 16
n = 2
16 with 224 channels, hence
·
104. A dense dictionary is learned from these patches. It
p = 6
·
should allow a sparse representation of samples: we either use
the classical dictionary learning setting ((cid:96)1/elastic-net penalty),
or further add positive constraints to the dictionary and codes:
both methods may be used and deserved to be benchmarked.
p.
We seek a dictionary of reasonable size: we use k

256

×

∼

(cid:28)

∼

(cid:28)

B. Experimental design

To validate the introduction of subsampling and the useful-

ness of SOMF, we perform two major experiments.

• We measure the performance of SOMF when increasing
the reduction factor, and show beneﬁts of stochastic
dimension reduction on all datasets.

• We assess the importance of subsampling in each of
the steps of SOMF. We compare the different approaches
proposed for code computation.

Validation: We compute the objective function (3) over a
test set to rule out any overﬁtting effect — a dictionary should
be a good representation of unseen samples. This criterion is
always plotted against wall-clock time, as we are interested in
the performance of SOMF for practitioners.

Tools: To perform a valid benchmark, we implement OMF
and SOMF using Cython [46] We use coordinate descent [47]
to solve Lasso problems with optional positivity constraints.

TABLE II
SUMMARY OF EXPERIMENTAL SETTINGS

Field

Dataset

Functional MRI

Hyperspectral imaging

ADHD

HCP

Patches from AVIRIS

Factors
# samples n
# features p
X size
Use case ex.

D sparse, A dense
2 · 106
2 · 105
2 TB

7 · 103
6 · 104
2 GB
Extracting predictive feature

D dense, A sparse
2 · 106
6 · 104
103 GB
Recognition / denoising

Code computation is parallelized to handle mini-batches. Ex-
periments use scikit-learn [48] for numerics, and nilearn [49]
for handling fMRI data. We have released the code in an open-
source Python package5. Experiments were run on 3 cores of
¯Bt is
an Intel Xeon 2.6GHz, in which case computing P⊥
t
faster than updating PtDt.

Parameter setting: Setting the number of components k
and the amount of regularization λ is a hard problem in the
absence of ground truth. Those are typically set by cross-
validation when matrix factorization is part of a supervised
pipeline. For fMRI, we set k = 70 to obtain interpretable
networks, and set λ so that the decomposition approximately
covers the whole brain (i.e., every map is k
70 ) sparse). For
hyperspectral images, we set k = 256 and select λ to obtain
a dictionary on which codes are around 3% sparse. We cycle
randomly through the data (fMRI records, image patches) until
convergence, using mini-batches of size η = 200 for HCP
and AVIRIS, and η = 50 for ADHD (small number of sam-
ples). Hyperspectral patches are normalized in the dictionary
learning setting, but not in the non-negative setting — the
classical pre-conditioning for each case. We use u = 0.917
and v = 0.751 for weight sequences.

C. Reduction brings speed-up at all data scales

We benchmark SOMF for various reduction factors against
the original online matrix factorization algorithm OMF [15],
on the three presented datasets. We stream data in the same
order for all reduction factors. Using variant (c) (true Gram
matrix, averaged βt) performs slightly better on fMRI datasets,
whereas (b) (averaged Gram matrix and βt) is slightly faster
for hyperspectral decomposition. For comparison purpose, we
display results using estimators (b) only.

Figure 3 plots the test objective against CPU time. First,
we observe that all algorithms ﬁnd dictionaries with very
close objective function values for all reduction factors, on
each dataset. This is not a trivial observation as the matrix
factorization problem (3) is not convex and different runs of
OMF and SOMF may converge towards minima with different
values. Second, and most importantly, SOMF provides signif-
icant improvements in convergence speed for three different
sizes of data and three different factorization settings. Both ob-
servations conﬁrm the relevance of the subsampling approach.
Quantitatively, we summarize the speed-ups obtained in
Table III. On fMRI data, on both large and medium datasets,
SOMF provides more than an order of magnitude speed-up.

5https://github.com/arthurmensch/modl

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

11

Fig. 3. Subsampling provides signiﬁcant speed-ups on all fMRI and hyperspectral datasets. A reduction factor of 12 is a good overall choice. With larger
data, larger reduction factors can be used for better performance — convergence is reached 13× faster than state-of-the-art methods on the 2TB HCP dataset.

TABLE III
TIME TO REACH CONVERGENCE (< 1% TEST OBJECTIVE)

Dataset

ADHD

Algorithm OMF

SOMF OMF

AVIRIS (NMF) AVIRIS (DL)
SOMF OMF

SOMF OMF

SOMF

HCP

Conv. time 6 min 28 s 2 h 30 43 min 1 h 16 11 min 3 h 50 17 min
Speed-up

13.31

3.36

6.80

11.8

Fig. 5. Proﬁling OMF and SOMF on HCP. Partial dictionary update removes
the major bottleneck of online matrix factorization for small reductions. For
higher reduction, parameter update and code computation must be subsampled
to further reduce the iteration time.

Our ﬁrst experiment establishes the power of stochastic
subsampling as a whole. In the following two experiments, we
reﬁne our analysis to show that subsampling is indeed useful
in the three steps of online matrix factorization.

D. For each step of SOMF, subsampling removes a bottleneck

∼

In Section III, we have provided theoretical guidelines on
when to introduce subsampling in each of the three steps of
an iteration of SOMF. This analysis predicts that, for η
k,
we should ﬁrst use partial dictionary update, before using
approximate code computation and asynchronous parameter
aggregation. We verify this by measuring the time spent by
SOMF on each of the updates for various reduction factors,
on the HCP dataset. Results are presented in Figure 5. We
observe that block coordinate descent is indeed the bottle-
neck in OMF. Introducing partial dictionary update removes
this bottleneck, and as the reduction factor increases, code
computation and surrogate aggregation becomes the major
bottlenecks. Introducing subsampling as described in SOMF
overcomes these bottlenecks, which rationalizes all steps of
SOMF from a computational point of view.

E. Code subsampling becomes useful for high reduction

It remains to assess the performance of approximate code
computation and averaging techniques used in SOMF. Indeed,

Fig. 4. Given a 3 minute time budget, the atoms learned by SOMF are
more focal and less noisy that those learned by OMF. They are closer to
the dictionary of ﬁrst line, for which convergence has been reached.

Practitioners working on datasets akin to HCP can decompose
their data in 20 minutes instead of 4 h previously, while
working on a single machine. We obtain the highest speed-ups
for the largest dataset — accounting for the extra redundancy
that usually appears when dataset size increase. Up to r
8,
speed-up is of the order of r — subsampling induces little
noise in the iterate sequence, compared to OMF. Hyperspectral
decomposition is performed near 7
faster than with OMF in
the classical dictionary learning setting, and 3
in the non-
negative setting, which further demonstrates the versatility of
SOMF. Qualitatively, given a certain time budget, Figure 4
compares the results of OMF and the results of SOMF with
a subsampling ratio r = 24, in the non-negative setting. Our
algorithm yields a valid smooth bank of ﬁlters much faster.
The same comparison has been made for fMRI in [26].

∼

×

×

Comparison with stochastic gradient descent: It is possible
to solve (3) using the projected stochastic gradient (SGD)
algorithm [50]. On all
tested settings, for high precision
convergence, SGD (with the best step-size among a grid) is
slower than OMF and even slower than SOMF. In the dictionary
learning setting, SGD is somewhat faster than OMF but slower
than SOMF in the ﬁrst epochs. Compared to SOMF and OMF,
SGD further requires to select the step-size by grid search.

Limitations: Table III reports convergence time within 1%,
which is enough for application in practice. SOMF is less
beneﬁcial when setting very high precision: for convergence
within 0.01%, speed-up for HCP is 3.4. This is expected
as SOMF trades speed for approximation. For high precision
convergence, the reduction ratio can be reduced after a few
epochs. As expected, there exists an optimal reduction ratio,
depending on the problem and precision, beyond which per-
formance reduces: r = 12 yields better results than r = 24 on
AVIRIS (dictionary learning) and ADHD, for 1% precision.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

12

Fig. 6. Approximating code computation with the proposed subsampling method further accelerates the convergence of SOMF. Reﬁning code computation
using past iterations (averaged estimates) performs better than simply performing a subsampled linear regression as in [26]

subsampling for code computation introduces noise that may
undermine the computational speed-up. To understand the
impact of approximate code computation, we compare three
strategies to compute (αt)t on the HCP dataset. First, we
compute (α(cid:63)
t )t from (xt)t using (21). Subsampling is thus
used only in dictionary update. Second, we rely on masked,
non-consistent estimators (a), as in [26] — this breaks conver-
gence guarantees. Third, we use averaged estimators (βt, Gt)
from (c) to reduce the variance in (αt)t computation.

∈ {

Fig. 6 compares the three strategies for r

12, 24
.
}
Partial minimization at each step is the most important part
to accelerate convergence: subsampling the dictionary updates
already allows to outperforms OMF. This is expected, as
dictionary update constitutes the main bottleneck of OMF in
large-scale settings. Yet, for large reduction factors, using
subsampling in code computation is important
to further
accelerate convergence. This clearly appears when comparing
the plain and dashed black curves. Using past estimates to
better approximate (αt)t yields faster convergence than the
non-converging, masked loss strategy (a) proposed in [26].

VI. CONCLUSION

In this paper, we introduce SOMF, a matrix-factorization
algorithm that can handle input data with very large number of
rows and columns. It leverages subsampling within the inner
loop of a streaming algorithm to make iterations faster and
accelerate convergence. We show that SOMF provides a sta-
tionary point of the non-convex matrix factorization problem.
To prove this result, we extend the stochastic majorization-
minimization framework to two major approximations. We
assess the performance of SOMF on real-world large-scale
problems, with different sparsity/positivity requirements on
learned factors. In particular, on fMRI and hyperspectral data
decomposition, we show that
the use of subsampling can
speed-up decomposition up to 13 times. The larger the dataset,
the more SOMF outperforms state-of-the art techniques, which
is very promising for future applications. This calls for adap-
tation of our approach to learn more complex models.

APPENDIX A
PROOFS OF CONVERGENCE

This appendix contains the detailed proofs of Proposition 3
and Proposition 1. We ﬁrst introduce three lemmas that will be
crucial to prove SAMM convergence, before establishing it by
proving Proposition 3. Finally, we show that SOMF is indeed

∇

an instance of SAMM (i.e. meets the assumptions (C)–(I)),
proving Proposition 1.

A. Basic properties of the surrogates, estimate stability

We derive an important result on the stability and optimality
of the sequence (θt)t, formalized in Lemma 3 — introduced
in the main text. We ﬁrst introduce a numerical lemma on
the boundedness of well-behaved determistic and random
sequence. The proof is detailed in Appendix B.

R, t0

Lemma 1 (Bounded quasi-geometric sequences). Let (xt)t be
a sequence in R+, u : R
R
N and α
[0, 1)
→
αxt−1 + u(xt, xt−1), where
such that, for all t
. Then (xt)t is bounded.
u(x, y)

≤
→ ∞
Let now (Xt)t be a random sequence in R+, such that
t)t the ﬁltration adapted to (Xt)t.

∈
E[Xt] <
If, for all t > t0, there exists a σ-algebra

o(x + y) for x, y

. We deﬁne (

t(cid:48) such that

×
t0, xt

∞

t−1

≥

F

∈

∈

F

F

⊆

t(cid:48)

F

⊆ F

t and

E[Xt

t(cid:48)]

αXt−1 + u(Xt, Xt−1),

(33)

|F
then (Xt)t is bounded almost surely.

≤

We ﬁrst derive some properties of the approximate surrogate

functions used in SAMM. The proof is adapted from [10].

Lemma 2 (Basic properties of approximate surrogate func-
tions). Consider any sequence of iterates (θt)t and assume
L,ρ(ft, θt−1, (cid:15)) for all
there exists (cid:15) > 0 such that gt
1, ¯h0 (cid:44) h0 and
t
≥
¯ht (cid:44) (1
(i) (

∈ T
ft for all t
wt)¯ht−1 + wtht. Under assumptions (D) – (G),
−
ht(θt−1))t>0 is uniformly bounded and there exists

1. Deﬁne ht (cid:44) gt

≥

−

(ii) (ht)t and (¯ht)t are uniformly R(cid:48)-Lipschitz, (gt)t and

ht

}t is uniformly bounded by R(cid:48).

∇
R(cid:48) such that

{∇

(¯gt)t are uniformly (R + R(cid:48))-Lipschitz.

Proof. We ﬁrst prove (i). We set α > 0 and deﬁne θ(cid:48) =
. As ht has a L-Lipschitz gradient on RK,
θt
using Taylor’s inequality (see Appendix B)

α ∇ht(θt)
(cid:107)∇ht(θt)(cid:107)2

−

(34)

ht(θ(cid:48))

ht(θt)

α

−

≤

Lα2
2

2 +

ht(θt)
(cid:107)∇
(cid:107)
ht(θ(cid:48))) +

(cid:107)∇

1
α

ht(θt)
2
(cid:107)

≤
where we use ht(θt) < (cid:15) and
assumption gt

Lα
Lα
2
2 ≤
ht(θ(cid:48)
(cid:15) from the
t)
L,ρ(ft, θt−1, (cid:15)). Moreover, by deﬁnition,

(ht(θt)

2
α

(cid:15) +

−

−

≤

,

ht exists and is L-lipschitz for all t. Therefore,

t

1,

∈ T

ht(θ)
2
(cid:107)

(cid:107)∇

≤ (cid:107)∇

2 + L
ht(θt)
(cid:107)
(cid:107)

θt−1

−

≥

∀
(cid:107)2

θ

(35)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

13

Since Θ is compact and (

(cid:107)2)t≥1 is bounded in (34),
ht(θt)
ht is bounded by R(cid:48) independent of t. (ii) follows by basic

(cid:107)∇

where the second inequality holds for the same reasons as
in (41). Injecting (40) and (42) in (43), we obtain

∇
considerations on Lipschitz functions.

Finally, we prove a result on the stability of the estimates,
that derives from combining the properties of (gt)t and the
geometric decrease assumption (I).

Lemma 3 (Estimate stability under SAMM approximation). In
the same setting as Lemma 2, with the additional assump-
tion (I) (expected linear decrease of ¯gt suboptimality), the
2 converges to 0 as fast as (wt)t, and θt
sequence
(cid:107)
is asymptotically an exact minimizer. Namely, almost surely,

θt
(cid:107)

θt−1

−

θt
(cid:107)

−

θt−1

(cid:107)2 ∈ O

(wt) and ¯gt(θt)

¯gt(θ(cid:63)
t )

−

∈ O

(w2

t ).

(36)

Proof. We ﬁrst establish the result when a deterministic ver-
sion of (I) holds, as it makes derivations simpler to follow.

1) Determistic decrease rate: We temporarily assume that

decays are deterministic.

(Idet) For all t > 0, ¯gt(θt) < ¯gt(θt−1). Moreover, there

exists µ > 0 such that, for all t > 0
¯gt(θ(cid:63)
t )
(1
θ(cid:63)
t = argmin

−
where

¯gt(θt)

≤

−

µ)(¯gt(θt−1)
¯gt(θ),

θ∈Θ

¯gt(θ(cid:63)

t ))

−

We introduce the following auxiliary positive values, that

we will seek to bound in the proof:

At (cid:44)
θt
(cid:107)
−
θ(cid:63)
θ(cid:63)
t−1(cid:107)
t −
(cid:107)

Ct (cid:44)

2, Bt (cid:44)
θt
θt−1
(cid:107)
(cid:107)
2, Dt (cid:44) ¯gt(θt)

θ(cid:63)
2,
t (cid:107)
−
¯gt(θ(cid:63)
t ).

−

Our goal is to bound At. We ﬁrst relate it to Ct and Bt using
convexity of (cid:96)2 norm:

A2

t + 3B2
t is the minimizer of ¯gt, by strong convexity of (¯gt)t,

t−1 + 3C 2
t .

3B2

t ≤

(39)

As θ(cid:63)

ρ
2
while we also have
ρ
θ(cid:63)
2
2 ≤
t−1(cid:107)
2 (cid:107)
wt)(cid:0)¯gt−1(θ(cid:63)

θ(cid:63)
t −
(1

≤

−

B2

t =

ρ
2 (cid:107)

θt

2
θ(cid:63)
2 ≤
t (cid:107)

−

Dt,

¯gt(θ(cid:63)

t−1)

t−1)

−

¯gt(θ(cid:63)
t )
−
t )(cid:1)+wt
¯gt−1(θ(cid:63)

(cid:0)gt(θ(cid:63)

gt(θ(cid:63)

t )(cid:1)

(41)

t−1)
2Q
ρ

−
.

2, and thus Ct

wt

wt(R + R(cid:48))

θ(cid:63)
t −
(cid:107)

θ(cid:63)
t−1(cid:107)

≤
≤
The second inequalities holds because θ(cid:63)
t−1 is a minimizer
of ¯gt−1 and gt is Q-Lipschitz, where Q (cid:44) R + R(cid:48), using
Lemma 2. Replacing (40) and (41) in (39) yields

A2

(Dt + Dt−1) +

t ≤
and we are left to show that Dt
this, we decompose the inequality from (Idet) into

∈ O

w2
t ,

12Q2
ρ
(w2
t ) to conclude. For

(42)

6
ρ

Dt

≤
= (1

(1

µ)(¯gt(θt−1)
−
(cid:16)
(cid:0)gt(θt−1)

−
µ)

t ))

¯gt(θ(cid:63)
gt(θt)(cid:1) + wt

(cid:0)gt(θt)

gt(θ(cid:63)

t )(cid:1)(cid:17)

−
+ (1

µ)

(1

−

−
+ (1

−

−
wt)(cid:0)¯gt−1(θt−1)
wt)(cid:0)¯gt−1(θ(cid:63)

−
t−1)

¯gt−1(θ(cid:63)

−
t−1)(cid:1)
t )(cid:1)(cid:17)
¯gt−1(θ(cid:63)

−

wt
(cid:16)

(1

µ)(wtQ(At + Bt) + Dt−1),

≤

−

(37)

(38)

(40)

w2
t−1
w2
t

(44)

˜Dt

(1

µ) ˜Dt−1

+ u( ˜Dt, ˜Dt−1),

−

≤
where we deﬁne ˜Dt (cid:44) Dt
w2
t
gebraic details in Appendix B) that
u( ˜Dt, ˜Dt−1)
determistictic result of Lemma 1,
bounded, which combined with (40) allows to conclude.

is easy to show (see al-
the perturbation term
. Using the
is

o( ˜Dt + ˜Dt−1) if ˜Dt

this ensures that ˜Dt

→ ∞

. It

∈

2) Stochastic decrease rates: In the general case (I), the
inequalities (40), (41) and (42) holds, and (44) is replaced by

E[ ˜Dt

|Ft− 1

2

]

≤

(1

−

µ) ˜Dt−1

w2
t−1
w2
t

+ u( ˜Dt, ˜Dt−1),

(45)

Taking the expectation of this inequality and using Jensen
inequality, we show that (43) holds when replacing ˜Dt by
E[ ˜Dt]. This shows that E[Dt]
.
∞
The result follows from Lemma 1, that applies as
⊆
Ft− 1

t ) and thus E[Dt] <
t−1

2 ⊆ F

(w2

∈ O

F

t.

B. Convergence of SAMM — Proof of Proposition 3

We now proceed to prove the Proposition 3, that extends
the stochastic majorization-minimization framework to allow
approximations in both majorization and minimizations steps.

Proof of Proposition 3. We adapt the proof of Proposition 3.3
from [10] (reproduced as Proposition 2 in our work). Relaxing
tightness and majorizing hypotheseses introduces some extra
error terms in the derivations. Assumption (H) allows to
control these extra terms without breaking convergence. The
stability Lemma 3 is important in steps 3 and 5.

1) Almost sure convergence of (¯gt(θt)): We control the
positive expected variation of (gt(θt))t
is
a converging quasi-martingale. By construction of ¯gt and
ρ,L(ft, θt−1, (cid:15)t), where (cid:15)t
properties of the surrogates gt
is a non-negative sequence that meets (H),

to show that

∈ T

it

¯gt−1(θt−1)

¯gt(θt)
= (¯gt(θt)

−

wt(gt(θt−1)
wt(gt(θt−1)
+ wt( ¯ft−1(θt−1)
wt(ft(θt−1)

−

−

≤

≤

≤

−

¯gt(θt−1)) + wt(gt(θt−1)
¯gt−1(θt−1))
ft(θt−1)) + wt(ft(θt−1)

−

−

¯gt−1(θt−1))

¯ft−1(θt−1))

−

¯gt−1(θt−1))

−

¯ft−1(θt−1)) + wt(¯(cid:15)t−1 + (cid:15)t),

(46)

where the average error sequence (¯(cid:15)t)t is deﬁned recursively:
¯(cid:15)0 (cid:44) (cid:15)0 and ¯(cid:15)t (cid:44) (1
wt)(cid:15)t−1 +wt(cid:15)t. The ﬁrst inequality uses
¯gt(θt−1). To obtain the forth inequality we observe
¯gt(θt)
≤
ft(θt−1) < (cid:15)t by deﬁnition of (cid:15)t and ¯ft(θt−1)
gt(θt−1)
−
¯(cid:15)t, which can easily be shown by induction on t.
¯gt(θt−1)
t−1,
Then, taking the conditional expectation with respect to

−
≤

−

F

E[¯gt(θt)

−
wt sup
θ∈Θ |

≤

¯gt−1(θt−1)
f (θ)

|F
¯ft−1(θ)

t−1]

−

|

+ wt(¯(cid:15)t−1 + E[(cid:15)t

t−1]).

(47)

|F

We have used the fact that (cid:15)t−1 is deterministic with respect
t−1. To ensure convergence, we must bound both terms
to
in (47): the ﬁrst term is the same as in the original proof

F

(43)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

14

with exact surrogate, while the second is the perturbative
term introduced by the approximation sequence ((cid:15)t)t. We use
Lemma B.7 from [10], issued from the theory of empirical
processes: E[supθ∈Θ |
(wtt1/2), and thus
wtE[sup
f (θ)
θ∈Θ |

¯ft−1(θ)
] =
O
|
∞
(cid:88)
t1/2w2

−
¯ft−1(θ)
] < C
|

∞
(cid:88)

f (θ)

t <

(48)

∞

−

t=1

t=1

where C is a constant, as t1/2w2
t = t1/2−2u and u > 3/4
from (G). Let us now focus on the second term of (47).
Deﬁning, for all 1

t, wt

wj),

(cid:81)t

i

i = wi

≤

≤
t
(cid:88)

i=1

E[¯(cid:15)t] =

wt
i

E[(cid:15)t]

wt

≤

−

j=i+1(1
t
(cid:88)

E[(cid:15)t].

i=1

(49)

−

1)

η >

We set η > 0 so that 2(u
ensures E[(cid:15)t]
partial sum (cid:80)t

1. Assumption (H)
−
(t2(u−1)−η), which allows to bound the
∈ O
E[(cid:15)i]
i=1
wtE[¯(cid:15)t−1 + E[(cid:15)t
(cid:16) t
(cid:88)

−
(t2u−1−η). Therefore
t−1]] = wtE[(cid:15)t−1] + wtE[(cid:15)t]

∈ O

|F
(cid:17)
E[(cid:15)t]

+ wtE[(cid:15)t]

(50)

At2u−2u−1−η + Bt2u−u−2−η

Ct−1−η,

≤

where we use u < 1 on the third line and the deﬁnition of
(wt)t on the second line. Thus (cid:80)∞
t−1]] <
. We use quasi-martingale theory to conclude, as in [10]. We
∞
deﬁne the variable δt to be 1 if E[¯gt(θt)
¯gt−1(θt−1)
0, and 0 otherwise. As all terms of (47) are positive:
∞
(cid:88)

t=1 wtE[¯(cid:15)t−1+E[(cid:15)t

t−1]

|F

|F

≥

−

E[δt(¯gt(θt)

¯gt−1(θt−1))]

−

w2
t

i=1

≤

≤

t=1

=

∞
(cid:88)

t=1
∞
(cid:88)

E[δtE[¯gt(θt)

¯gt−1(θt−1)

t−1]]

−

|F

(51)

+ ¯(cid:15)t−1 + E[(cid:15)t

t−1]

] <
|

.

wtE[sup
θ∈Θ |

f (θ)

¯ft−1(θ)
|

−

t=1

|F

≤
∞
As ¯gt are bounded from below ( ¯ft is bounded from (D) and
we easily show that ¯(cid:15)t is bounded), we can apply Theorem
A.1 from [10], that is a quasi-martingale convergence theorem
originally found in [51]. It ensures that (gt(θt))t≥1 converges
almost surely to an integrable random variable g(cid:63), and that
(cid:80)∞

t=1
|F
2) Almost sure convergence of ¯f (θt): We rewrite the second

E[¯gt(θt)
E[
|

almost surely.

¯gt−1(θt−1)

] <
|

t−1]

∞

−

inequality of (46), adding ¯(cid:15)t on both sides:
(cid:1)
(cid:0)ft(θt−1)

¯ft−1(θt−1) + ¯(cid:15)t−1

0

−
ft(θt−1)(cid:1) + wt
¯gt(θt)(cid:1) + wt¯(cid:15)t−1
−
¯ft−1(θt−1)(cid:1) + (cid:0)¯gt−1(θt−1)

−

≤

(cid:0)¯gt−1(θt−1)
wt
(cid:0)gt(θt−1)
wt
≤
−
+ (cid:0)¯gt−1(θt−1)
(cid:0)ft(θt−1)
wt
+ wt((cid:15)t + ¯(cid:15)t−1),

−

≤

¯ft−1(θt−1)(cid:1)

¯gt(θt)(cid:1)

−

(52)

where the left side bound has been obtained in the last
paragraph by induction and the right side bound arises from
the deﬁnition of (cid:15)t. Taking the expectation of (52) conditioned
on

t−1, almost surely,

F

0

≤

−

wt(f (θt−1)
E[¯gt(θt)

¯ft−1(θt−1))
−
¯gt−1(θt−1)

−

|F

t−1] + wt(¯(cid:15)t−1 + E[(cid:15)t

(53)
t−1]),

|F

t−1]
|

|F
(cid:0)f (θt−1)

We separately study the three terms of the previous upper
bound. The ﬁrst two terms can undergo the same analysis as
E(cid:2)
in [10]. First, almost sure convergence of (cid:80)∞
E[¯gt(θt)
t=1
−
|
(cid:3) implies that E(cid:2)¯gt(θt)
(cid:3)
¯gt−1(θt−1)
¯gt−1(θt−1)
t−1
is the summand of an almost surely converging sum. Second,
¯ft−1(θt−1)(cid:1) is the summand of an absolutely
wt
converging sum with probability one, less it would contra-
dict (48). To bound the third term, we have once more
the perturbation introduced by ((cid:15)t)t. We have
to control
(cid:80)∞
almost surely, otherwise

t=1 wt¯(cid:15)t−1 + wtE[(cid:15)t

t−1] <

|F

−

−

Fubini’s theorem would invalidate (50).

|F

∞

As the three terms are the summand of absolutely converg-
¯ft−1(θt−1)+¯(cid:15)t−1)
ing sums, the positive term wt(¯gt−1(θt−1)
is the summand of an almost surely convergent sum. This is
not enough to prove that ¯ht(θt) (cid:44) ¯gt(θt)
∞ 0, hence
we follow [10] and make use of its Lemma A.6. We deﬁne
Xt (cid:44) ¯ht−1(θt−1) + ¯(cid:15)t−1. As (H) holds, we use Lemma 3,
which ensures that (¯ht)t≥1 are uniformly R(cid:48)-Lipschitz and
θt
(cid:107)

−
¯ft(θt)

θt−1

2 =

→

O

−

−
Xt+1
|
R(cid:48)

≤

≤ O

(cid:107)
Xt

−
θt
(cid:107)
−
(wt) +

| ≤ |
θt−1
¯(cid:15)t
|

−

(wt). Hence,
¯ht(θt)
−
¯(cid:15)t
2 +
|
(cid:107)
,
¯(cid:15)t−1

−

|

¯ht−1(θt−1)
¯(cid:15)t−1
,
|
θt
as
(cid:107)

−

+

¯(cid:15)t

¯(cid:15)t−1

|

|

−

|
as ¯ht is R(cid:48)-Lipschitz
θt−1

(wt)

2 =
(cid:107)

O

(54)

From assumption (H), ((cid:15)t)t and (¯(cid:15)t)t are bounded. Therefore
)
¯(cid:15)t

(wt) and hence

¯(cid:15)t−1

¯(cid:15)t−1

+

(cid:15)t

|

−

wt(
|

| ≤

|
|
Xt+1
|

∈ O

|
Xt

−

| ≤ O

(wt).

(55)

Lemma A.6 from [10] then ensures that Xt converges
to zero with probability one. Assumption (H) ensures that
∞ 0 almost surely, from which we can easily deduce
(cid:15)t
∞ 0 almost surely. Therefore ¯ht(θt)
0 with probability
¯(cid:15)t
one and ( ¯ft(θt))t≥1 converges almost surely to g(cid:63).

→
→
3) Almost sure convergence of ¯f (θt): Lemma B.7 of [10],
based on empirical process theory [33], ensures that ¯ft uni-
formly converges to ¯f . Therefore, ( ¯f (θt))t≥1 converges almost
surely to g(cid:63).

→

4) Asymptotic stationary point condition: Preliminary to
the ﬁnal result, we establish the asymptotic stationary point
condition (57) as in [10]. This requires to adapt the original
proof to take into account the errors in surrogate computation
¯ht is L-
and minimization. We set α > 0. By deﬁnition,
Lipschitz over RK. Following the same computation as in (34),
we obtain, for all α > 0,

∇

≤

2
(cid:107)

2
α

¯(cid:15)t +

¯ht(θt)

(cid:107)∇
¯ht(θ)
where we use
|
the inequality (56) is true for all α,
surely. From the strong convexity of ¯gt and Lemma 3,
θ(cid:63)
t (cid:107)

2 converges to zero, which ensures

Lα
2
RK. As ¯(cid:15)t
∈
¯ht(θt)
2
(cid:107)
(cid:107)∇

¯(cid:15)t for all θ

| ≤

→

,

0 and
∞ 0 almost

→

θt

(cid:107)

−

(56)

¯ht(θ(cid:63)
t )

2
(cid:107)

¯
∇

ht(θt)

θt
2 + L
(cid:107)
(cid:107)

θ(cid:63)
2
t (cid:107)

−

≤ (cid:107)

(cid:107)∇
5) Parametrized surrogates: We use assumption (F) to
ﬁnally prove the property, adapting the proof of Proposition 3.4
in [10]. We ﬁrst recall the derivations of [10] for obtaining (58)
We deﬁne (κt)t such that ¯gt = gκt for all t > 0. We assume
that θ∞ is a limit point of (θt)t. As Θ is compact, there

→

∞ 0.

(57)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

15

K

exists an increasing sequence (tk)k such that (θtk )k converges
toward θ∞. As
is compact, a converging subsequence of
(κtk )k can be extracted, that converges towards κ∞
.
∈ K
From the sake of simplicity, we drop subindices and assume
κ∞.
without loss of generality that θt
From the compact parametrization assumption, we easily show
that (¯gκt)t uniformly converges towards ¯g∞ (cid:44) ¯gκ∞. Then,
deﬁning ¯h∞ = ¯g∞

¯f , for all θ

θ∞ and κt

Θ,

→

→

¯h∞(θ∞, θ

− ∇

−

θ∞)
(58)
Θ. We
∈
θ(cid:63)
2
t (cid:107)
−

¯f (θ∞, θ

∇

−

θ∞) =

¯g∞(θ∞, θ

θ∞)

−

∇
¯f (θ∞, θ

∈

−

≥

−

∇

t →

We ﬁrst show that
consider the sequence (θ(cid:63)
0, which implies θ(cid:63)
¯g∞, which implies (¯gt(θ(cid:63)
minimizes ¯gt, for all t > 0 and θ
This implies ¯g∞(θ∞)
for t

θ∞)
0 for all θ
t )t. From Lemma 3,
θt
→
(cid:107)
θ∞. ¯gt converges uniformly towards
¯g∞(θ∞). Furthermore, as θ(cid:63)
t ))t →
t
Θ, ¯gt(θ(cid:63)
¯gt(θ).
t )
inf θ∈Θ ¯g∞(θ) by taking the limit
. Therefore θ∞ is the minimizer of ¯g∞ and thus

∇
around θ(cid:63)

→ ∞
¯g∞(θ∞, θ
Adapting [10], we perform the ﬁrst-order expansion of ¯ht
t (instead of θt in the original proof) and show that
0

θ∞) = 0, as ¯ht differentiable,
θ∞. This is sufﬁcient to conclude.

∇
and θ(cid:63)

¯ht(θ(cid:63)
t )

θ∞)

(cid:107)∇

2
(cid:107)

→

≤

−

≥

−

≤

0.

∈

¯h∞(θ∞, θ
t →

C. Convergence of SOMF — Proof of Proposition 1
Proof of Proposition 1. From assumption (D), (xt)t
is (cid:96)2-
bounded by a constant X. With assumption (A), it implies
that (αt)t is (cid:96)2-bounded by a constant A. This is enough to
show that (gt)t and (θt)t meet basic assumptions (C)–(F).
Assumption (G) immediately implies (B). It remains to show
that (gt)t and (θt)t meet the assumptions (H) and (I). This will
allow to cast SOMF as an instance of SAMM and conclude.
1) The computation of Dt veriﬁes (I): We deﬁne D(cid:63)
t =
argminD∈C ¯gt(D). We show that performing subsampled
block coordinate descent on ¯gt is sufﬁcient to meet assump-
tion (I), where θt = Dt. We separately analyse the exceptional
case where no subsampling is done and the general case.

First, with small but non-zero probability, Mt = Ip
and Alg. 4 performs a single pass of simple block coordi-
nate descent on ¯gt. In this case, as ¯gt is strongly convex
from (A), [52, 37] ensures that the sub-optimality decreases
at least of factor 1
µ with a single pass of block coordinate
descent, where µ > 0 is a constant independent of t. We
provide an explicit µ in Appendix B.

−

In the general case, the function value decreases determin-
¯gt(Dt−1). As
≤
¯gt(Dt−1). Fur-
≤
t ) are deterministic with respect
= Ip] = ¯gt(D(cid:63)
t ).
the sub-optimality

istically at each minimization step: ¯gt(Dt)
a consequence, E[¯gt(Dt)
= Ip]
|Ft− 1
thermore, ¯gt and hence ¯gt(D(cid:63)
, which implies E[¯gt(D(cid:63)
t )
to
Deﬁning d (cid:44) P[Mt = Ip], we split
expectation and combine the analysis of both cases:

|Ft− 1

Ft− 1

, Mt

, Mt

2

2

2

E[¯gt(Dt)
= dE[¯gt(Dt)

, Mt = Ip]

]

2

−

¯gt(D(cid:63)
t )
|Ft− 1
¯gt(D(cid:63)
t )
−
d)E[¯gt(Dt)
µ) + (1
−
dµ(cid:1)(¯gt(Dt−1)

−

2

|Ft− 1
¯gt(D(cid:63)
t )
−
d)(cid:1)(¯gt(Dt−1)
¯gt(D(cid:63)
t )).

|Ft− 1
−

2

−

+ (1
−
(cid:0)d(1

≤
= (cid:0)1

−

= Ip]

, Mt
¯gt(D(cid:63)

t ))

(59)

2) The surrogates (gt)t verify (H): We deﬁne g(cid:63)
t ∈
ρ,L(ft, Dt−1) the surrogate used in OMF at iteration t, which
S
depends on the exact computation of α(cid:63)
t , while the surrogate
gt used in SOMF relies on approximated αt. Formally, using
the loss function (cid:96)(α, G, β) (cid:44) 1
α(cid:62)β + λΩ(α), we
recall the deﬁnitions

2 α(cid:62)Gα

−

α(cid:63)
t

(cid:44) argmin
α∈Rk
t (D) (cid:44) (cid:96)(α(cid:63)
g(cid:63)

t , β(cid:63)

(cid:96)(α, G(cid:63)

t ), αt(cid:44) argmin
α∈Rk
t , D(cid:62)D, D(cid:62)xt), gt(D) (cid:44) (cid:96)(αt, D(cid:62)D, D(cid:62)xt).

(cid:96)(α, Gt, βt),

(60)

gt

t , β(cid:63)

g(cid:63)
t −
(cid:107)

The matrices G(cid:63)
t are deﬁned in (21) and Gt, βt in either
the update rules (b) or (c). We deﬁne (cid:15)t (cid:44)
∞ to be
(cid:107)
the (cid:96)∞ difference between the approximate surrogate of SOMF
and the exact surrogate of OMF, as illustrated in Figure 2. By
ρ,L(ft, θt−1, (cid:15)t). We ﬁrst show that (cid:15)t can be
deﬁnition, gt
bounded by the Froebenius distance between the approximate
parameters Gt, βt and the exact parameters G(cid:63)
t . Using
Cauchy-Schwartz inequality, we ﬁrst show that there exists a
constant C (cid:48) > 0 such that for all D

t , β(cid:63)

∈ T

,

gt(D)
|

−

g(cid:63)
t (D)

C (cid:48)

| ≤

Then, we show that the distance

bounded: there exists C (cid:48)(cid:48) > 0 constant such that

∈ C
αt
(cid:107)

αt
(cid:107)

2.

(61)

α∗
t (cid:107)
α∗
t (cid:107)2 can itself be

−

−

αt

(cid:107)

−

α(cid:63)

t (cid:107)2 ≤

C (cid:48)(cid:48)(

G(cid:63)
(cid:107)

t −

Gt

(cid:107)F +

β(cid:63)
(cid:107)

t −

βt(cid:107)2).

(62)

We combine both equations and take the supremum over
D

, yielding

∈ C

(cid:15)t

C(
(cid:107)

≤

G(cid:63)

t −

Gt

(cid:107)F +

(cid:107)

β(cid:63)

t −

βt(cid:107)2),

(63)

where C is constant. Detailed derivation of (61) to (63) relies
on assumption (A) and are reported in Appendix B.

In a second step, we show that

βt(cid:107)
2
vanish almost surely, sufﬁciently fast. We focus on bounding
βt −
2 when the
(cid:107)
update rules (b) are used. For t > 0, we write i (cid:44) it. Then
(cid:88)

2 and proceed similarly for

F and
(cid:107)

G(cid:63)
t (cid:107)

β(cid:63)
t (cid:107)

t −

t −

Gt

Gt

−

(cid:107)

G(cid:63)
(cid:107)

β(cid:63)
(cid:107)

βt

(cid:44) β(i)

t =

γ(i)
s,tD(cid:62)

s−1Msx(i),

where γ(i)
(cid:12)
(cid:8)s
(cid:12)

s,t = γc(i)
t, xs = x(i)(cid:9)(cid:12)

t

≤
βt −

β(cid:63)

t =

s≤t,xs=x(i)

(cid:81)

s<t,xs=x(i)(1

γc(i)

s

−

) and c(i)
t =
t as

β(cid:63)

(cid:12). We can then decompose βt −
(cid:88)
γ(i)
s,t(Ds−1

Dt−1)(cid:62)Msx(i)

−

s≤t,xs=xt=x(i)
+ D(cid:62)

(cid:16) (cid:88)

t−1

s≤t,xs=xi)

γ(i)
s,tMs

−

(cid:17)

I

x(i).

(64)

The latter equation is composed of two terms: the ﬁrst one
captures the approximation made by using old dictionaries
in the computation of (βt)t, while the second captures how
the masking effect is averaged out as the number of epochs
increases. Assumption (B) allows to bound both terms at the
v(cid:1) > 0, a te-
same time. Setting η (cid:44) 1
2)
−
β(cid:63)
dious but elementary derivation indeed shows E[
βt−
t (cid:107)
(cid:107)

∈
0 almost surely — see Appendix B.
O
The SOMF algorithm therefore meets assumption (H) and is a
convergent SAMM algorithm. Proposition 1 follows.

(t2(u−1)−η) and (cid:15)t

2 min (cid:0)v

3
4 , (3u

→

−

−

2]

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

16

REFERENCES
[1] J. Mairal, “Sparse Modeling for Image and Vision Processing,” Foundations and
Trends in Computer Graphics and Vision, vol. 8, no. 2-3, pp. 85–283, 2014.
[2] N. Srebro, J. Rennie, and T. S. Jaakkola, “Maximum-margin matrix factorization,”
in Advances in Neural Information Processing Systems, 2004, pp. 1329–1336.
[3] E. J. Cand`es and B. Recht, “Exact matrix completion via convex optimization,”
Foundations of Computational Mathematics, vol. 9, no. 6, pp. 717–772, 2009.
[4] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global Vectors for Word

Representation.” in Proc. Conf. EMNLP, vol. 14, 2014, pp. 1532–43.

[5] O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix factoriza-
tion,” in Advances in Neural Information Processing Systems, 2014, pp. 2177–2185.
[6] Y. Zhang, M. Roughan, W. Willinger, and L. Qiu, “Spatio-Temporal Compressive

Sensing and Internet Trafﬁc Matrices,” 2009.

[7] H. Kim and H. Park, “Sparse non-negative matrix factorizations via alternating non-
negativity-constrained least squares for microarray data analysis,” Bioinformatics,
vol. 23, no. 12, pp. 1495–1502, 2007.

[8] G. Varoquaux et al., “Multi-subject dictionary learning to segment an atlas of brain

spontaneous activity,” in Proc. IPMI Conf., 2011, pp. 562–573.

[9] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” in

Proceedings of COMPSTAT, 2010, pp. 177–186.

[10] J. Mairal, “Stochastic majorization-minimization algorithms for large-scale opti-

mization,” in Adv. Neural Inform. Process. Syst., 2013, pp. 2283–2291.

[11] M. Razaviyayn, M. Hong, and Z.-Q. Luo, “A uniﬁed convergence analysis of block
successive minimization methods for nonsmooth optimization,” SIAM Journal on
Optimization, vol. 23, no. 2, pp. 1126–1153, 2013.

[12] S. Burer and R. D. C. Monteiro, “Local Minima and Convergence in Low-Rank
Semideﬁnite Programming,” Math. Program., vol. 103, no. 3, pp. 427–444, 2004.
[13] B. Recht and C. R´e, “Parallel stochastic gradient algorithms for large-scale matrix

completion,” Math. Program. Comput., vol. 5, no. 2, pp. 201–226, 2013.

[14] R. M. Bell and Y. Koren, “Lessons from the Netﬂix prize challenge,” ACM SIGKDD

Explorations Newsletter, vol. 9, no. 2, pp. 75–79, 2007.

[15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for matrix factorization
and sparse coding,” J. Machine Learning Research, vol. 11, pp. 19–60, 2010.
[16] W. B. Johnson and J. Lindenstrauss, “Extensions of Lipschitz mappings into a
Hilbert space,” Contemporary mathematics, vol. 26, no. 189-206, p. 1, 1984.
[17] E. Bingham and H. Mannila, “Random projection in dimensionality reduction:
Applications to image and text data,” in Proc. SIGKDD Conf., 2001, pp. 245–250.
[18] M. J. McKeown et al., “Analysis of fMRI Data by Blind Separation into Indepen-
dent Spatial Components,” Hum. Brain Mapp., vol. 6, no. 3, pp. 160–188, 1998.
[19] E. J. Cand`es and T. Tao, “Near-optimal signal recovery from random projections:
Universal encoding strategies?” IEEE Transactions on Information Theory, vol. 52,
no. 12, pp. 5406–5425, 2006.

[20] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions,”
SIAM review, vol. 53, no. 2, pp. 217–288, 2011.

[21] V. Rokhlin et al., “A randomized algorithm for principal component analysis,”

SIAM J. Matrix Anal. Appl., vol. 31, no. 3, pp. 1100–1124, 2009.

[22] T. Sarlos, “Improved approximation algorithms for large matrices via random

projections,” in Proc. IEEE Symp. Found. Comput. Science, 2006, pp. 143–152.

[23] Y. Lu et al., “Faster ridge regression via the subsampled randomized hadamard

transform,” in Adv. Neural Inform. Process. Syst., 2013, pp. 369–377.

[24] M. Pilanci and M. Wainwright, “Iterative hessian sketch: Fast and accurate solution
approximation for constrained least-squares,” JMLR, vol. 17, pp. 1–33, 2015.
[25] G. Raskutti and M. Mahoney, “Statistical and algorithmic perspectives on random-
ized sketching for ordinary least-squares,” in Proc. ICML, 2015, pp. 617–625.
[26] A. Mensch, J. Mairal, B. Thirion, and G. Varoquaux, “Dictionary learning for

massive matrix factorization,” in Proc. ICML, 2016, pp. 1737–1746.

[27] B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete basis set:
A strategy employed by V1?” Vision Res., vol. 37, no. 23, pp. 3311–3325, 1997.
[28] R. Tibshirani, “Regression shrinkage and selection via the lasso,” J. R. Stat. Soc.

Series B Stat. Methodol., vol. 58, no. 1, pp. 267–288, 1996.

[29] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” J.

Comput. Graph. Stat., vol. 15, no. 2, pp. 265–286, 2006.

[30] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,”
J. R. Stat. Soc. Series B Stat. Methodol., vol. 67, no. 2, pp. 301–320, 2005.
[31] P. O. Hoyer, “Non-negative matrix factorization with sparseness constraints,”

Journal of Machine Learning Research, vol. 5, pp. 1457–1469, 2004.

[32] L. Bottou, F. E. Curtis, and J. Nocedal, “Optimization Methods for Large-Scale

Machine Learning,” arXiv:1606.04838v2 [stat.ML], 2016.
[33] A. W. Van der Vaart, Asymptotic Statistics. CUP, 2000, vol. 3.
[34] M. Mardani et al., “Subspace Learning and Imputation for Streaming Big Data

Matrices and Tensors,” IEEE TSP, vol. 63, no. 10, pp. 2663–2677, 2015.

[35] J. M. Borwein and A. S. Lewis, Convex Analysis and Nonlinear Optimization:

Theory and Examples. Springer Science & Business Media, 2010.

[36] J. Mairal, “Optimization with ﬁrst-order surrogate functions,” in Proceedings of the

International Conference on Machine Learning, 2013, pp. 783–791.

[37] S. J. Wright, “Coordinate descent algorithms,” Mathematical Programming, vol.

[38] D. C. Van Essen et al., “The WU-Minn Human Connectome Project: An overview,”

151, no. 1, pp. 3–34, 2015.

NeuroImage, vol. 80, pp. 62–79, 2013.

[39] M. P. Milham et al., “The adhd-200 consortium: a model to advance the transla-
tional potential of neuroimaging in clinical neuroscience,” Front. Syst. Neurosci.,
vol. 6, no. 62, 2012.

[40] G. Varoquaux, Y. Schwartz, P. Pinel, and B. Thirion, “Cohort-level brain mapping:
Learning cognitive atoms to single out specialized regions,” in Proceedings of the
Information Processing in Medical Imaging Conference, 2013, pp. 438–449.

[41] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral image classiﬁcation
using dictionary-based sparse representation,” IEEE Transactions on Geoscience
and Remote Sensing, vol. 49, no. 10, pp. 3973–3985, 2011.

[42] A. Soltani-Farani, H. R. Rabiee, and S. A. Hosseini, “Spatial-Aware Dictionary
Learning for Hyperspectral Image Classiﬁcation,” IEEE Transactions on Geo-
science and Remote Sensing, vol. 53, no. 1, pp. 527–541, 2015.

[43] M. Maggioni, V. Katkovnik, K. Egiazarian, and A. Foi, “Nonlocal transform-
domain ﬁlter for volumetric data denoising and reconstruction,” IEEE Trans. Image
Process., vol. 22, no. 1, pp. 119–133, 2013.

[44] Y. Peng et al., “Decomposable nonlocal tensor dictionary learning for multispectral

image denoising,” in Proc. IEEE Conf. CVPR, 2014, pp. 2949–2956.

[45] G. Vane, “First results from the airborne visible/infrared imaging spectrometer

(AVIRIS),” in Ann. Tech. Symp. Int. Soc. Optics Photonics, 1987, pp. 166–175.

[46] S. Behnel et al., “Cython: The best of both worlds,” Computing in Science &

Engineering, vol. 13, no. 2, pp. 31–39, 2011.

[47] J. Friedman, T. Hastie, H. H¨oﬂing, and R. Tibshirani, “Pathwise coordinate
optimization,” The Annals of Applied Statistics, vol. 1, no. 2, pp. 302–332, 2007.
[48] F. Pedregosa et al., “Scikit-learn: Machine learning in Python,” Journal of Machine

Learning Research, vol. 12, pp. 2825–2830, 2011.

[49] A. Abraham et al., “Machine learning for neuroimaging with scikit-learn,” Frontiers

in Neuroinformatics, vol. 8, no. 14, 2014.

[50] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra, “Efﬁcient projections onto
the l1-ball for learning in high dimensions,” in Proceedings of the International
Conference on Machine Learning, 2008, pp. 272–279.

[51] M. M´etivier, Semimartingales: A Course on Stochastic Processes. Walter de

Gruyter, 1982, vol. 2.

[52] A. Beck and L. Tetruashvili, “On the convergence of block coordinate descent type

methods,” SIAM Journal on Optimization, vol. 23, no. 4, pp. 2037–2060, 2013.

[53] J. L. Doob, Stochastic processes.

John Wiley & Sons, 1990.

Arthur Mensch is a PhD candidate at Universit´e
Paris-Saclay and Inria. His main research interests
are related to large-scale stochastic optimization
and statistical learning, with speciﬁc applications to
functional neuroimaging and cognitive brain map-
ping. In 2015, he received a graduate degree from
Ecole Polytechnique, France, and a MSc degree in
applied mathematics from ´Ecole Normale Sup´erieure
de Cachan, France.

Julien Mairal is a research scientist at Inria. He
received a graduate degree from Ecole Polytech-
nique, France, in 2005, and a PhD degree from Ecole
Normale Superieure, Cachan, France, in 2010. Then,
he was a postdoctoral researcher at the statistics
department of UC Berkeley, before joining Inria in
2012. His research interests include machine learn-
ing, computer vision, mathematical optimiza- tion,
and statistical image and signal processing. In 2016,
he received a Starting Grant from the European
Research Council (ERC).

Ga¨el Varoquaux is a tenured computer-science
researcher at Inria. His research develops statistical-
learning tools for functional neuroimaging data with
application to cognitive mapping of the brain as
well as the study of brain pathologies. He is also
heavily invested in software development for data
science, as project-lead for scikit-learn, one of the
reference machine-learning toolboxes, and on joblib,
Mayavi, and nilearn. Varoquaux has a PhD in quan-
tum physics and is a graduate from Ecole Normale
Superieure, Paris.

Bertrand Thirion is the principal investigator of the
Parietal team (Inria-CEA) within the main French
Neuroimaging center, Neurospin. His main research
interests are related to the use of machine learning
and statistical analysis techniques for neuroimaging,
e.g. the modeling of brain variability in group stud-
ies, the mathematical study of functional connectiv-
ity and brain activity decoding; he addresses various
applications such as the study of vision through
neuroimaging and the classiﬁcation of brain images
for diagnosis or brain mapping

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

17

APPENDIX B
ALGEBRAIC DETAILS

A. Proof of Lemma 1

Proof. We ﬁrst focus on the deterministic case. Assume that (xt)t is not bounded. Then there exists a subsequence of (xt)t
that diverges towards +
and for all (cid:15) > 0,
∞
using the asymptotic bounds on u, there exists t1

. We assume without loss of generality that (xt)t → ∞

. Then, xt + xt−1

t0 such that

→ ∞

≥

t
∀

≥

t1, xt

and therefore xt

αxt−1 + (cid:15)(xt + xt−1)
α + (cid:15)
(cid:15)
1

xt−1.

≤

≤

−

Setting (cid:15) small enough, we obtain that xt is bounded by a geometrically decreasing sequence after t1, and converges to 0,
which contradicts our hypothesis. This is enough to conclude.

t−1

diverges to +

In the random case, we consider a realization of (Xt)t that is not bounded, and assumes without loss of generality that it
βXt−1, where
βXt−1, as Xt−1 is deterministic conditioned
, Doob’s forward convergence lemma on
cannot happen

. Following the reasoning above, there exists β < 1, t1 > 0, such that for all t > t1, E[Xt
t. Taking the expectation conditioned on

⊆ F
F
t−1. Therefore Xt is a supermartingale beyond a certain time. As E[Xt] <
on
discrete martingales [53] ensures that (Xt)t converges almost surely. Therefore the event
on a set with non-zero probability, less it would lead to a contradiction. The lemma follows.

(Xt)t is not bounded
}
{

t−1, E[Xt

∞
⊆ F

t−1]

∞

|F

|F

t(cid:48)]

≤

≤

F

F

t(cid:48)

B. Taylor’s inequality for L-Lipschitz continuous functions

This inequality is useful in the demonstration of Lemma 2 and Proposition 3. Let f : Θ

L-Lipschitz gradient. That is, for all θ, θ(cid:48)

∈
f (θ(cid:48))

Θ,

f (θ)

(cid:107)∇

f (θ) +

∇

≤

f (θ(cid:48))

− ∇
f (θ)(cid:62)(θ(cid:48)

(cid:107)2 ≤

L

θ
(cid:107)

θ) +

−

⊂

RK
→
(cid:107)2. Then, for all θ, θ(cid:48)
∈
θ(cid:48)

2
2.
(cid:107)

θ(cid:48)

−
L
θ
2 (cid:107)

−

R be a function with
Θ,

(66)

C. Lemma 3: Detailed control of Dt in (44)

Injecting (40) and (42) in (43), we obtain

˜Dt

(1

≤

−

µ) ˜Dt−1

w2
t−1
w2
t

+ u( ˜Dt, ˜Dt−1), where u( ˜Dt, ˜Dt−1) (cid:44) (1

µ) ˜Q

3( ˜Dt + ˜Dt−1

) + ˜Q +

˜Dt

.

(67)

(cid:18)(cid:115)

−

(cid:113)

(cid:19)

w2
t−1
w2
t

From assumption (G),
Using the determistictic result of Lemma 1, this ensures that ˜Dt is bounded.

1, and we have, from elementary comparisons, that u( ˜Dt, ˜Dt−1)

∈

w2
t−1
w2
t →

o( ˜Dt + ˜Dt−1) if Dt

.
→ ∞

D. Detailed derivations in the proof of Proposition 1

Let us ﬁrst exhibit a scaler µ > 0 independent of t, for which (I) is met
1) Geometric rate for single pass subsampled block coordinate descent: . For D(j)

Rp×k any matrix with non-zero j-th

column d(j) and zero elsewhere

¯gt(D + D(j))

∇

− ∇

∈
¯gt(D + D(j)) = ¯Ct[j, j]d(j)

¯gt has coordinate Lipschitz constant Lmax (cid:44) max0≤j<k ¯Ct[j, j]

and hence ¯gt gradient has component Lipschitz constant Lj = ¯Ct[j, j] for component j, as already noted in [15]. Using [37]
A2, as (αt)t is
terminology,
√kLmax. Moreover,
bounded from (A). As a consequence, ¯gt gradient is also L-Lipschitz continuous, where [37] note that L
¯gt is strongly convex with strong convexity modulus ρ > 0 by hypothesis (A). Then, [52] ensures that after one cycle over the
k blocks

maxt>0,0≤j<k αt[j]2

∇

≤

≤

≤

E[¯gt(Dt)

¯gt(D(cid:63)
t )

t−1, Mt = Ip]

−

|F

(cid:0)1

(cid:0)1

−

−

≤

≤

(cid:1)(¯gt(Dt−1)

2Lmax(1 + kL2/L2
µ(cid:1)(¯gt(Dt−1)

¯gt(D(cid:63)

max)

−
t )) where µ (cid:44)

ρ

−

¯gt(D(cid:63)

t ))
ρ
2A2(1 + k2)

2) Controling (cid:15)t from (Gt, βt), (G(cid:63)

is met in the proof of SOMF convergence. We ﬁrst show that (αt)t is bounded. We choose D > 0 such that
for all j
condition, for all t > 0,

t ) — Equations 61–62: We detail the derivations that are required to show that (H)
D
. From assumption (A), using the second-order growth

, and X such that

X for all x

[k] and D

d(j)
(cid:107)

(cid:107)2 ≤

(cid:107)2 ≤

x
(cid:107)

∈ X

∈ C

∈

t , β(cid:63)

(65)

(68)

(69)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

We have successively used the fact that Ω(0) = 0, Ω(αt)
induction on the number of epochs. For all t > 0, from the deﬁnition of αt and α(cid:63)

βt(cid:107)2 ≤
(cid:107)

0, and

≥

√krDX, which can be shown by a simple

λΩ(0)

t Gtαt

α(cid:62)

t βt + λΩ(αt)

ρ
2 (cid:107)
1
2

2
2 ≤

αt

0
−
(cid:107)
t Gtαt

α(cid:62)

0 +

(cid:107)

≤

ρ
2 (cid:107)

αt

2
2 +
(cid:107)

−
αt

(

α(cid:62)

1
2
βt(cid:107)2,
αt
(cid:107)

(cid:107)2,

(cid:107)2(cid:107)

−

hence

αt

ρ
(cid:107)

2
2 ≤

(cid:107)

√krDX

and therefore

αt
(cid:107)

(cid:107)2 ≤

√krDX
ρ

(cid:44) A.

gt(D)

|

g(cid:63)
t (D)
|

−

=

t −

Tr D(cid:62)D(αtα(cid:62)

(cid:12)
1
(cid:12)
(cid:12)
2
1
t −
2 (cid:107)
(kD2A + √kDX)
αt
(cid:107)

αtα(cid:62)

D(cid:62)D

(cid:107)F (cid:107)

α(cid:63)

t α(cid:63)
t

(cid:62))

α(cid:63)

(cid:62)

t α(cid:63)
t
α(cid:63)
t (cid:107)2,

−

−
(cid:107)F +

≤

≤

t , for all D

:

∈ C

(αt

−

α(cid:63)

t )(cid:62)D(cid:62)xt

(cid:12)
(cid:12)
(cid:12)

D

(cid:107)

(cid:107)F (cid:107)

xt

(cid:107)2(cid:107)

αt

−

α(cid:63)

t (cid:107)2

where we use Cauchy-Schwartz inequality and elementary bounds on the Froebenius norm for the ﬁrst inequality, and use
αt, α(cid:63)
[k] to obtain the second inequality, which is (61) in the main text.
α(cid:63)
t (cid:107)2. We adapt the proof of Lemma B.6 from [36], that states the lipschitz continuity of the

αt
(cid:107)
minimizers of some parametrized functions. By deﬁnition,

A, xt
We now turn to control

X for all t > 0 and d(j)

D for all j

t ≤

≤

−

≤

∈

α(cid:63)

t = argmin
α∈Rk

(cid:96)(α, G(cid:63)

t , β(cid:63)
t )

αt = argmin
α∈Rk

(cid:96)(α, Gt, βt),

Assumption (A) ensures that Gt

ρIk, therefore we can write the second-order growth condition

(cid:31)

αt

αt

ρ
2 (cid:107)
ρ
2 (cid:107)
ρ

αt
(cid:107)

2
α(cid:63)
t (cid:107)
2 ≤
2
α(cid:63)
t (cid:107)
2 ≤
2
α(cid:63)
t (cid:107)
2 ≤

−

−

−

(cid:96)(αt, G(cid:63)

t , β(cid:63)
t )

(cid:96)(αt, Gt, βt)

−

(cid:96)(α(cid:63)

t , Gt, βt)
p(α(cid:63)

(cid:96)(α(cid:63)

t , G(cid:63)

t , β(cid:63)

t ),
and therefore
−
t ), where p(α) (cid:44) (cid:96)(α, Gt, βt)

p(αt)

−

−
Rk such that

(cid:96)(αt, G(cid:63)

t , β(cid:63)

t ).

α
(cid:107)

(cid:107)2 ≤

A,

p takes a simple form and can differentiated with respect to α. For all α

Therefore p is L-Lipschitz on the ball of size A where αt and α(cid:63)

p(α) =

1
2

α(cid:62)(Gt
G(cid:63)

A

−
Gt
(cid:107)

−

p(α) = (Gt

∇
p(α)

(cid:107)∇

(cid:107)2 ≤

−
t )α
G(cid:63)

G(cid:63)

∈
α(cid:62)(βt −
t )α
−
β(cid:63)
(βt −
t )
β(cid:63)
βt −
(cid:107)
t live, and

t (cid:107)2

−
t (cid:107)F +

β(cid:63)
t )

(cid:44) L

αt

ρ
(cid:107)

αt

(cid:107)

−

−

2
α(cid:63)
t (cid:107)
2 ≤
α(cid:63)

t (cid:107)2 ≤

L
(cid:107)
A
ρ (cid:107)

αt

−

Gt

−

α(cid:63)

t (cid:107)2
G(cid:63)
t (cid:107)F +

1
ρ (cid:107)

βt −

β(cid:63)
t (cid:107)2,

which is (62) in the main text. The bound (63) on (cid:15)t immediately follows.

3) Bounding

C are positive constants independent of t and we introduce the terms

βt −
(cid:107)

β(cid:63)
t (cid:107)2 in equation (64): Taking the (cid:96)2 norm in (64), we have

βt −
(cid:107)

β(cid:63)

t (cid:107)2 ≤

BLt + CRt, where B and

Lt (cid:44) (cid:88)

s≤t,xs=xt=x(i)

γ(i)
s,t(cid:107)

Ds−1

Dt−1

−

(cid:107)F ,

Rt (cid:44)

(cid:0) (cid:80)

s≤t,xs=x(i) γ(i)

s,tMs

(cid:1)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
I
(cid:13)F

.

−

{
∈

a) Conditioning on the sequence of drawn indices: We recall that (it)t is the sequence of indices that are used to draw
}i, namely such that xt = x(it). (it)t is a sequence of i.i.d random variables, whose law is uniform in [1, n].
b )b>0 that record the iterations at which sample (i) is drawn, i.e. such
t > 0 is the integer that counts the number of time sample (i) has
. These notations will help us understanding the behavior of (Lt)t
}

x(i)
(xt)t from
[n], we deﬁne the increasing sequence (t(i)
For each i
that itb = i for all b > 0. For t > 0, we recall that c(i)
appeared in the algorithm, i.e. c(i)
t
b ≤
and (Rt)t.

b > 0, t(i)
{

t = max

18

(70)

(71)

(72)

(73)

(74)

(75)

(76)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

19

(77)

(79)

(80)

(81)

b) Bounding Rt: The right term Rt takes its value into sequences that are running average of masking matrices. Formally,

Rt =

¯M(it)

(cid:107)

I

(cid:107)F , where we deﬁne for all i

∈

[n],

¯M(i)
t

γ(i)
b ,t(i)
t(i)

c

Mtb ,

which follows the recursion

t −
c(i)
t(cid:88)

(cid:44)

b=1






¯M(i)
t
¯M(i)
t
¯M(i)
0

γc(i)

) ¯M(i)
t−1 + γc(i)
= (1
−
= M(i)
= it
if i
t−1
[n]
= 0 for all i

t

t

Mt

if i = it

When sampling a sequence of indices (is)s>0, the n random matrix sequences [( ¯M(i)
law as the sampling is uniform. We therefore focus on controling ( ¯M(0)
the expectation over the sequence of indices (is)s,

∈
t )t≤0]
i∈[n] follows the same probability
] is

t )t. For simplicity, we write ct (cid:44) c(0)

t

. When E[
·

¯M(0)
E[
(cid:107)

t −

(cid:107)F ]2
I

( ¯M(0)
t

[j, j]

1)(cid:3) = pE[( ¯M(0)

t

−

[0, 0]

1)]

−

E(cid:2)

p
(cid:88)

j=1

≤

≤

C p(ct)1/2γct = C p(ct)1/2−v, where C is a constant independent of t.

(78)

We have simply bounded the Froebenius norm by the (cid:96)1 norm in the ﬁrst inequality and used the fact that all coefﬁcients
Mt[j, j] follows the same Bernouilli law for all t > 0, j
[p]. We then used Lemma B.7 from [10] for the last inequality. This
lemma applies as Mt[0, 0] follows the recursion (77). It remains to take the expectation of (78), over all possible sampling
trajectories (is)s>0:

∈

E[Rt] = E(cid:2)E[Rt

(is)s](cid:3) = E(cid:2)E[
M(it)
t −
(cid:107)F |
|
(cid:107)
CpE[(ct)2(u−1)−η].
= CpE[(ct)1/2−v]

I

≤

(is)s](cid:3) = E(cid:2)E[
(cid:107)

M(0)

I

t −

(cid:107)F |

(is)s](cid:3) = E[

M(0)
(cid:107)

t −

I

(cid:107)F ]

The last inequality arises from the deﬁnition of η (cid:44) 1
we successively have

2 min (cid:0)v

3
4 , (3u

2)

−

−

−

v(cid:1), as follows. First, η > 0 as u > 11

12 . Then,

5
2 −

2u <

<

as u >

2
3

3
4

,

11
12

,

3
4

v

≥

+ 2η >

2u + 2η,

5
2 −

1
2 −

v <

1
2 −

5
2

+ 2u

2η = 2(u

1)

2η < 2(u

1)

η, which allows to conclude.

−

−

−

−

−

I)t converges towards 0
Lemma B.7 from [10] also ensures that Mt[0, 0]
1 almost surely when t
almost surely, given any sample sequence (is)s. It thus converges almost surely when all random variables of the algorithm
are considered. This is also true for ( ¯M(i)

t −

→ ∞

→

I)t for all i

. Therefore ( ¯M(0)

c) Bounding Lt: As above, we deﬁne n sequences [(L(i)

t −

∈

[n] and hence for Rt.
t )t]i∈[n], such that Lt = L(it)

t

for all t > 0. Namely,

L(i)
t

(cid:44) (cid:88)
s≤t,
xs=xt=x(i)

γ(i)
s,t(cid:107)

Ds−1

Dt−1

−

(cid:107)F =

c(i)
t(cid:88)

b=1

γ(i)
b ,t(i)
t(i)

c

(i)
t

(cid:13)
(cid:13)Dtb−1

Dt

c

(i)
t

−

(cid:13)
(cid:13)

−1

.

F

Once again, the sequences (cid:2)(L(i)
t )t
focus on bounding (L(0)
From assumption (B) and the deﬁnition of η, we have v < ν < 1. We split the sum in two parts, around index dt (cid:44) ct
where

(cid:3)
i all follows the same distribution when sampling over sequence of indices (is)s. We thus
η.
,

t )t. Once again, we drop the (0) superscripts in the right expression for simplicity. We set ν (cid:44) 3u

takes the integer part of a real number. For simplicity, we write d (cid:44) dt and c (cid:44) ct in the following.

2
−
−
(ct)ν

−(cid:98)

(cid:99)

(cid:98)·(cid:99)

L(0)

t =

γtb,tc

(cid:13)
(cid:13)Dtb−1

Dtc−1

−

(cid:13)
(cid:13)F ≤

2√kD

c
(cid:88)

b=1

d
(cid:88)

b=1

c
(cid:88)

tc−1
(cid:88)

γtb,tc +

γtb,t

b=d+1

s=tb−1

ws (cid:44) 2√kDL(0)

t,1 + L(0)

t,2

(82)

On the left side, we have bounded
on

Ds

Dt

(cid:107)
We now study both L(0)

−

Dt
(cid:107)
t,1 and L(0)

(cid:107)F provided by Lemma 3, that applies here as (I) is met and (63) ensures that (
gt
(cid:107)

−

g(cid:63)
t (cid:107)∞)t is bounded.

(cid:107)F by √kD, where D is deﬁned in the previous section. The right part uses the bound

t,2 . First, for all t > 0,

L(0)
t,1

(cid:44)

d
(cid:88)

b=1

d
(cid:88)

c
(cid:89)

γtb,tc =

γb

(1

γp)

−

≤

p=b+1

d
(cid:88)

b=1

γb(1

−

γc)c−b

b=1
γc)(cid:98)cν (cid:99)
γc

≤

(1

−

≤

cv exp (cid:0)log(1

1

cv )cν(cid:1)

≤

−

C (cid:48)cv exp(cν−v)

Cc2(u−1)−η = C(ct)2(u−1)−η,

(83)

≤

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

20

where C and C (cid:48) are constants independent of t. We have used ν > v for the third inequality, which ensures that
exp (cid:0)log(1
(cν−v). Basic asymptotic comparison provides the last inequality, as ct
almost surely and
the right term decays exponentially in (ct)t, while the left decays polynomially. As a consequence, L(0)
0 almost surely.

cv )cν(cid:1)

∈ O

−

1

Secondly, the right term can be bounded as (wt)t decays sufﬁciently rapidly. Indeed, as (cid:80)c

→ ∞
t,1 →
b=1 γtb,t = 1, we have

L(0)
t,2

(cid:44)

γtb,t

c
(cid:88)

b=d

tc−1
(cid:88)

s=tb−1

ws

max
d≤b≤c

≤

s=tb−1

(cid:16) tc−1
(cid:88)

(cid:17)

ws

=

tc−1
(cid:88)

ws

wtd (tc

td) =

−

≤

s=td−1
ct
dt
−
(dt)u

tc
td
(td)u =
−

tc
ct

td
dt

(

dt
td

)u

−
−

from elementary comparisons. First, we use the deﬁnition of ν to draw

were we use the fast that η
−
expectation n. Therefore, as c

when t

n , and
0, from the strong law of large numbers and linearity of the expectation

tb follows a geometric law of parameter 1

−

(ct)ν

ct
dt
(dt)u ≤

−

(ct)u(1

cν−1
t

)u ≤

−
1 < 0. We note that for all b > 0, tb+1

C(ct)ν−u = C(ct)2(u−1)−η,

d

−

→ ∞
c−1
1
(cid:88)

d

→

tb

tb+1

n,

−

→

d−1
(cid:88)

td
d

=

1
d

tb+1

tb

n almost surely.

−

→

b=0

b=d

−
n1−u almost surely. This immediately shows L(0)
0 almost surely and therefore
βt −

almost surely.

β(cid:63)

0

(cid:107)

→

t,2 →

0 and thus L(0)

t →

0 almost surely.

tc
c

td
d

=

−
−
As a consequence, tc−td
ct−dt
As with Rt, this implies that Lt

( dt
td

)u

c

→

t (cid:107)2 →
( dt
td

−
−

Finally, from the dominated convergence theorem, E[ tc−td
ct−dt
and write

)u]

n1−u for t

→

→ ∞

. We can use Cauchy-Schartz inequality

E[L(0)

t,2 ] = E[

tc
td
(td)u ]
−

E[

ct
dt
(dt)u ]E[
−

tc
ct

≤

td
dt

(

dt
td

)u]

C (cid:48)E[

≤

ct
dt
(dt)u ]
−

≤

C C (cid:48)E[(ct)2(u−1)−η],

where C (cid:48) is a constant independant of t. Then

E[Lt] = E(cid:2)E[L(it)

(is)s](cid:3) = E[L(0)
(is)s](cid:3) = E(cid:2)E[L(0)
|
β(cid:63)
Combined with (79), this shows that E[
βt −
t (cid:107)2]
(cid:107)
∈ O
0. Therefore E[( ct
(t, 1

1
n almost surely when t

n ), ct

|

t

t

t →

t

]

≤

2√kDE[L(0)

t,1 ] + E[L(0)
t,2 ]
((ct)2(u−1)−η). As ct follows a binomial distribution of parameter
nη−2(u−1), and from Cauchy-Schwartz inequality,
t )2(u−1)−η)]
)2(u−1)−η)]t2(u−1)−η

((ct)2(u−1)−η).

(t2(u−1)−η).

∈ O

(89)

→

(90)

→
βt −
(cid:107)

E[

β(cid:63)
t (cid:107)2]

CE[(

ct
t

≤
We have reused the fact that converging sequences are bounded. This is enough to conclude.

∈ O

(84)

(85)

(86)

(87)

(88)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

1

Stochastic Subsampling for
Factorizing Huge Matrices

Arthur Mensch, Julien Mairal,
Bertrand Thirion, and Ga¨el Varoquaux

7
1
0
2
 
t
c
O
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
6
3
5
0
.
1
0
7
1
:
v
i
X
r
a

Abstract—We present a matrix-factorization algorithm that
scales to input matrices with both huge number of rows and
columns. Learned factors may be sparse or dense and/or non-
negative, which makes our algorithm suitable for dictionary
learning, sparse component analysis, and non-negative matrix
factorization. Our algorithm streams matrix columns while
subsampling them to iteratively learn the matrix factors. At
each iteration, the row dimension of a new sample is reduced by
subsampling, resulting in lower time complexity compared to a
simple streaming algorithm. Our method comes with convergence
guarantees to reach a stationary point of the matrix-factorization
problem. We demonstrate its efﬁciency on massive functional
Magnetic Resonance Imaging data (2 TB), and on patches ex-
tracted from hyperspectral images (103 GB). For both problems,
which involve different penalties on rows and columns, we obtain
signiﬁcant speed-ups compared to state-of-the-art algorithms.

Index Terms—Matrix factorization, dictionary learning, NMF,
stochastic optimization, majorization-minimization, randomized
methods, functional MRI, hyperspectral imaging

I. INTRODUCTION

Matrix factorization is a ﬂexible approach to uncover latent
factors in low-rank or sparse models. With sparse factors, it is
used in dictionary learning, and has proven very effective for
denoising and visual feature encoding in signal and computer
vision [see e.g., 1]. When the data admit a low-rank structure,
matrix factorization has proven very powerful for various tasks
such as matrix completion [2, 3], word embedding [4, 5], or
network models [6]. It is ﬂexible enough to accommodate a
large set of constraints and regularizations, and has gained sig-
niﬁcant attention in scientiﬁc domains where interpretability
is a key aspect, such as genetics [7] and neuroscience [8].
In this paper, our goal is to adapt matrix-factorization tech-
niques to huge-dimensional datasets, i.e., with large number
of columns n and large number of rows p. Speciﬁcally, our
work is motivated by the rapid increase in sensor resolution, as
in hyperspectral imaging or fMRI, and the challenge that the
resulting high-dimensional signals pose to current algorithms.
As a widely-used model, the literature on matrix factoriza-
tion is very rich and two main classes of formulations have

A. Mensch, B. Thirion, G. Varoquaux are with Parietal team, Inria, CEA,
Paris-Saclay University, Neurospin, at Gif-sur-Yvette, France. J. Mairal is with
Universit´e Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK at Grenoble,
France.

The research leading to these results was supported by the ANR (MAC-
ARON project, ANR-14-CE23-0003-01 — NiConnect project, ANR-11-
BINF-0004NiConnect). It has received funding from the European Union’s
Horizon 2020 Framework Programme for Research and Innovation under
Grant Agreement No 720270 (Human Brain Project SGA1).

Corresponding author: Arthur Mensch (arthur.mensch@m4x.org)

emerged. The ﬁrst one addresses a convex-optimization prob-
lem with a penalty promoting low-rank structures, such as the
trace or max norms [2]. This formulation has strong theoretical
guarantees [3], but lacks scalability for huge datasets or sparse
factors. For these reasons, our paper is focused on a second
type of approach, which relies on nonconvex optimization.
Stochastic (or online) optimization methods have been devel-
oped in this setting. Unlike classical alternate minimization
procedures, they learn matrix decompositions by observing
a single matrix column (or row) at each iteration. In other
words, they stream data along one matrix dimension. Their
cost per iteration is signiﬁcantly reduced, leading to faster
convergence in various practical contexts. More precisely,
two approaches have been particularly successful: stochastic
gradient descent [9] and stochastic majorization-minimization
methods [10, 11]. The former has been widely used for matrix
completion [see 12, 13, 14, and references therein], while the
latter has been used for dictionary learning with sparse and/or
structured regularization [15]. Despite those efforts, stochastic
algorithms for dictionary learning are currently unable to deal
efﬁciently with matrices that are large in both dimensions.

We propose a new matrix-factorization algorithm that
can handle such matrices.
It builds upon the stochastic
majorization-minimization framework of [10], which we gen-
the objective
eralize for our problem. In this framework,
function is minimized by iteratively improving an upper-bound
surrogate of the function (majorization step) and minimizing
it to obtain new estimates (minimization step). The core idea
of our algorithm is to approximate these steps to perform them
faster. We carefully introduce and control approximations,
so to extend convergence results of [10] when neither the
majorization nor the minimization step is performed exactly.
For this purpose, we borrow ideas from randomized meth-
ods in machine learning and signal processing. Indeed, quite
orthogonally to stochastic optimization, efﬁcient approaches to
tackle the growth of dataset dimension have exploited random
projections [16, 17] or sampling, reducing data dimension
while preserving signal content. Large-scale datasets often
have an intrinsic dimension which is signiﬁcantly smaller
than their ambient dimension. Good examples are biological
datasets [18] and physical acquisitions with an underlying
sparse structure enabling compressed sensing [19]. In this
context, models can be learned using only random data sum-
maries, also called sketches. For instance, randomized methods
[see 20, for a review] are efﬁcient to compute PCA [21], a
classic matrix-factorization approach, and to solve constrained
or penalized least-square problems [22, 23]. On a theoretical

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

2

level, recent works on sketching [24, 25] have provided bounds
on the risk of using random summaries in learning.

the matrix D is called the “dictionary” and A the sparse code.
We use this terminology throughout the paper.

Using random projections as a pre-processing step is not
appealing in our applicative context since factors learned on
reduced data are not
it
is possible to exploit random sampling to approximate the
steps of online matrix factorization. Factors are learned in
the original space whereas the dimension of each iteration is
reduced together with the computational cost per iteration.

interpretable. On the other hand,

Contribution: The contribution of this paper is both prac-
tical and theoretical. We introduce a new matrix factoriza-
tion algorithm, called subsampled online matrix factorization
(SOMF), which is faster than state-of-the-art algorithms by an
order of magnitude on large real-world datasets (hyperspectral
images, large fMRI data). It leverages random sampling with
stochastic optimization to learn sparse and dense factors more
efﬁciently. To prove the convergence of SOMF, we extend
the stochastic majorization-minimization framework [10] and
make it robust to some time-saving approximations. We then
show convergence guarantees for SOMF under reasonable
assumptions. Finally, we propose an extensive empirical vali-
dation of the subsampling approach.

In a ﬁrst version of this work [26] presented at the Interna-
tional Conference in Machine Learning (ICML), we proposed
an algorithm similar to SOMF, without any theoretical guaran-
tees. The algorithm that we present here has such guarantees,
which we express in a more general framework, stochastic
is validated for new sparsity
majorization-minimization. It
settings and a new domain of application. An open-source
efﬁcient Python package is provided.

Notations: Matrices are written using bold capital letters
and vectors using bold small letters (e.g., X, α). We use
superscript
to specify the column (sample or component)
number, and write X = [x(1), . . . , x(n)]. We use subscripts
to specify the iteration number, as in xt. The ﬂoating bar,
as in ¯gt, is used to stress that a given value is an average
over iterations, or an expectation. The superscript (cid:63) is used to
denote an exact value, when it has to be compared to an inexact
value, e.g., to compare α(cid:63)

t (exact) to αt (approximation).

II. PRIOR ART: MATRIX FACTORIZATION WITH
STOCHASTIC MAJORIZATION-MINIMIZATION

Below, we introduce the matrix-factorization problem and
recall a speciﬁc stochastic algorithm to solve it observing
one column (or a mini-batch) at every iteration. We cast this
algorithm in the stochastic majorization-minimization frame-
work [10], which we will use in the convergence analysis.

A. Problem statement

In our setting, the goal of matrix factorization is to decom-
Rp×n — typically n signals of dimension p

pose a matrix X
— as a product of two smaller matrices:

∈

X

DA with D

Rp×k and A

Rk×n,

≈

∈
with potential sparsity or structure requirements on D and A.
In signal processing, sparsity is often enforced on the code A,
in a problem called dictionary learning [27]. In such a case,

∈

Learning the factorization is typically performed by min-
imizing a quadratic data-ﬁtting term, with constraints and/or
penalties over the code and the dictionary:

min
D∈C
A∈Rk×n

n
(cid:88)

i=1

1
2

(cid:13)
(cid:13)x(i)

Dα(i)(cid:13)
2
2 + λ Ω(α(i)),
(cid:13)

(1)

−

C

→

where A (cid:44) [α(1), . . . , α(n)],
is a column-wise separable
C
R is a penalty over the
convex set of Rp×k and Ω : Rp
code. Both constraint set and penalty may enforce structure or
sparsity, though
has traditionally been used as a technical
requirement to ensure that the penalty on A does not vanish
with D growing arbitrarily large. Two choices of
and Ω are
of particular interest. The problem of dictionary learning sets
C
as the (cid:96)2 ball for each atom and Ω to be the (cid:96)1 norm. Due to
the sparsifying effect of (cid:96)1 penalty [28], the dataset admits
a sparse representation in the dictionary. On the opposite,
ﬁnding a sparse set in which to represent a given dataset,
with a goal akin to sparse PCA [29], requires to set as the
(cid:96)1 ball for each atom and Ω to be the (cid:96)2 norm. Our work
considers the elastic-net constraints and penalties [30], which
encompass both special cases. Fixing ν and µ in [0, 1], we
the elastic-net penalty in Rp and Rk:
denote by Ω(

C

α
ν)
(cid:107)

−
(cid:44) (1

(cid:107)
µ)

ν
2 (cid:107)

α

2
2,
(cid:107)
µ
1+
2 (cid:107)
(cid:107)

1 +

d(j)
(cid:107)

(2)
(cid:111)
1

.

d(j)

2
2 ≤
(cid:107)

(cid:44)

(cid:110)

D

Rp×k/
(cid:107)

d(j)

(cid:107)

∈

C
−
Following [15], we can also enforce the positivity of D
and/or A by replacing R by R+ in
, and adding positivity
constraints on A in (1), as in non-negative sparse coding [31].
We rewrite (1) as an empirical risk minimization problem
depending on the dictionary only. The matrix D solution of (1)
is indeed obtained by minimizing the empirical risk ¯f

C

D

argmin
D∈C

∈

n
(cid:88)

(cid:17)

f (D, x(i))

(cid:16) ¯f (D) (cid:44) 1
n
Dα(cid:13)
2
2 + λ Ω(α),
(cid:13)

(cid:13)
(cid:13)x

i=1

,

(3)

where

1
2
and the matrix A is obtained by solving the linear regression

f (D, x) (cid:44) min
α∈Rk

−

) and
·

(cid:107) · (cid:107)
Ω(α) (cid:44) (1

min
A∈Rk×n

n
(cid:88)

i=1

1
2

(cid:13)
(cid:13)x(i)

Dα(i)(cid:13)
2
2 + λ Ω(α(i)).
(cid:13)

−

(4)

The problem (1) is non-convex in the parameters (D, A), and
hence (3) is not convex. However, the problem (1) is convex in
both D and A when ﬁxing one variable and optimizing with
respect to the other. As such, it is naturally solved by alternate
minimization over D and A, which asymptotically provides
a stationary point of (3). Yet, X has typically to be observed
hundred of times before obtaining a good dictionary. Alternate
minimization is therefore not adapted to datasets with many
samples.

B. Online matrix factorization

When X has a large number of columns but a limited
number of rows, the stochastic optimization method of [15]

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

3

Algorithm 1 Online matrix factorization (OMF) [15]

Input: Initial iterate D0, sample stream (xt)t>0, number of
iterations T .
for t from 1 to T do

∼ P

Draw xt
.
Compute αt = argminα∈Rp
Update the parameters of aggregated surrogate ¯gt:
(cid:17)

Dt−1α(cid:13)
2
2+λ Ω(α).
(cid:13)

(cid:13)
(cid:13)xt

−

(cid:16)

1
2

1

¯Ct =

¯Ct−1 +

1
t
1
t
Compute (using block coordinate descent):

¯Bt−1 +

αtα(cid:62)
t .

xtα(cid:62)
t .

¯Bt =

1
t
1
t

−

−

(cid:16)

(cid:17)

1

(8)

Dt = argmin

Tr (D(cid:62)D ¯Ct)

Tr (D(cid:62) ¯Bt).

−

1
2

D∈C

Output: Final iterate DT .

Algorithm 2 Stochastic majorization-minimization [SMM 10]
Input: Initial iterate θ0, weight sequence (wt)t>0, sample
stream (xt)t>0, number of iteration T .
for t from 1 to T do

Draw xt
∈
Construct a surrogate of ft near θt−1, that meets

, get ft : θ

f (xt, θ).

∼ P

→

Θ

gt

ft,

gt(θt−1) = ft(θt−1).

(12)

≥

Update the aggregated surrogate:

¯gt = (1

wt)¯gt−1 + wtgt.

−

Compute

θt = argmin

¯gt(θ).

θ∈Θ

(13)

Output: Final iterate θT .

outputs a good dictionary much more rapidly than alternate-
minimization. In this setting [see 32], learning the dictionary
is naturally formalized as an expected risk minimization

min
D∈C

¯f (D) (cid:44) Ex[f (D, x)],

where x is drawn from the data distribution and forms an i.i.d.
stream (xt)t. In the ﬁnite-sample setting, (5) reduces to (3)
when xt is drawn uniformly at random from
.
We then write it the sample number selected at time t.

[1, n]
}

x(i), i

∈

{

The online matrix factorization algorithm proposed in [15]
is summarized in Alg. 1. It draws a sample xt at each iteration,
and uses it to improve the current iterate Dt−1. For this, it
ﬁrst computes the code αt associated to xt on the current
dictionary:

αt (cid:44) argmin
α∈Rk

1
2 (cid:107)

xt

−

Dt−1α

2
2 + λΩ(α).
(cid:107)

(6)

Then, it updates Dt to make it optimal in reconstructing past
samples (xs)s≤t from previously computed codes (αs)s≤t:

Dt

argmin
D∈C

∈

(cid:16)

¯gt(D) (cid:44) 1
t

t
(cid:88)

s=1

1
2

(cid:13)
(cid:13)xs

−

Dαs

(cid:17)
(cid:13)
2
2 + λΩ(αs)
(cid:13)

.

(7)
Importantly, minimizing ¯gt is equivalent to minimizing the
quadratic function

D

→

1
2

Tr (D(cid:62)D ¯C(cid:62)
t )

Tr (D(cid:62) ¯Bt),

(9)

−

where ¯Bt and ¯Ct are small matrices that summarize previously
seen samples and codes:

¯Bt =

xsα(cid:62)
s

¯Ct =

αsα(cid:62)
s .

(10)

1
t

t
(cid:88)

s=1

1
t

t
(cid:88)

s=1

The function ¯gt is an upper-bound surrogate of the true
current empirical risk, whose deﬁnition involves the regression
minima computed on current dictionary D:

¯ft(D) (cid:44) 1
t

t
(cid:88)

s=1

(5)

min
α∈Rp

(cid:13)
(cid:13)xs

1
2

−

Dα(cid:13)
2
2+ λΩ(α)
(cid:13)

≤

¯gt(D). (11)

Using empirical processes theory [33], it is possible to show
that minimizing ¯ft at each iteration asymptotically yields
a stationary point of the expected risk (5). Unfortunately,
minimizing (11) is expensive as it involves the computation of
optimal current codes for every previously seen sample at each
iteration, which boils down to naive alternate-minimization.

In contrast, ¯gt is much cheaper to minimize than ¯ft, using
block coordinate descent. It is possible to show that ¯gt con-
verges towards a locally tight upper-bound of the objective ¯ft
and that minimizing ¯gt at each iteration also asymptotically
yields a stationary point of the expected risk (5). This es-
tablishes the correctness of the online matrix factorization
algorithm (OMF). In practice, the OMF algorithm performs a
single pass of block coordinate descent: the minimization step
is inexact. This heuristic will be justiﬁed by our theoretical
contribution in Section IV.

}

t
∈ T

Extensions: For efﬁciency, it is essential to use mini-batches
xs, s
of size η instead of single samples in the
{
iterations [15]. The surrogate parameters ¯Bt, ¯Ct are then
s∈Tt over
updated by the mean value of
}
the batch. The optimal size of the mini-batches is usually
close to k. (8) uses the sequence of weights ( 1
t )t to update
parameters ¯Bt and ¯Ct. [15] replaces these weights with a
sequence (wt)t, which can decay more slowly to give more
importance to recent samples in ¯gt. These weights will prove
important in our analysis.

(xsα(cid:62)
{

s , αsα(cid:62)
s )

C. Stochastic majorization-minimization

C

As the constraints
have a separable structure per atom, [15]
uses projected block coordinate descent to minimize ¯gt. The
¯Bt, and it is there-
function gradient writes
fore enough to maintain ¯Bt and ¯Ct in memory to solve (7).
¯Bt and ¯Ct are updated online, using the rules (8) (Alg. 1).

¯gt(D) = D ¯Ct

∇

−

Online matrix factorization belongs to a wider category
of algorithms introduced in [10] that minimize locally tight
upper-bounding surrogates instead of a more complex objec-
tive, in order to solve an expected risk minimization prob-
lem. Generalizing online matrix factorization, we introduce

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

4

−

(cid:107)

in Alg. 2 the stochastic majorization-minimization (SMM)
algorithm, which is at the core of our theoretical contribution.
In online matrix factorization, the true empirical risk func-
tions ¯ft and their surrogates ¯gt follow the update rules, with
generalized weight (wt)t set to ( 1
¯ft (cid:44) (1

wt)¯gt−1 + wtgt, (14)

t )t in (7) – (11):

wt) ¯ft−1 + wtft,

¯gt (cid:44) (1

−

where the pointwise loss function and its surrogate are

ft(D) (cid:44) min
α∈Rk
gt(D) (cid:44) 1
2 (cid:107)

xt

1
2 (cid:107)

xt

−

Dα

2
2 + λΩ(α),

Dαt

2
2 + λΩ(αt).
(cid:107)

−

(15)

≥

ft, and gt

The function gt is a majorizing surrogate of ft: gt
is tangent to ft in Dt−1, i.e, gt(Dt−1) = ft(Dt−1) and
(gt
ft)(Dt−1) = 0. At each step of online matrix factorization:
• The surrogate gt is computed along with αt, using (6).
• The parameters ¯Bt, ¯Ct are updated following (8). They
deﬁne the aggregated surrogate ¯gt up to a constant.
• The quadratic function ¯gt is minimized efﬁciently by
block coordinate descent, using parameters ¯Bt and ¯Ct
to compute its gradient.

∇

−

iteration t, a surrogate gt of the loss ft

The stochastic majorization-minimization framework sim-
ply formalizes the three steps above, for a larger variety of
loss functions ft(θ) (cid:44) f (θ, xt), where θ is the parameter we
want to learn (D in the online matrix factorization setting).
is computed
At
to update the aggregated surrogate ¯gt following (14). The
surrogate functions (gt)t should be upper-bounds of loss
functions (ft)t, tight
in the current iterate θt−1 (e.g., the
dictionary Dt−1). This simply means that ft(θt−1) = gt(θt−1)
gt)(θt−1) = 0. Computing ¯gt can be done if gt is
and
∇
−
deﬁned simply, as in OMF where it is linearly parametrized by
t , xtα(cid:62)
(αtα(cid:62)
t ). ¯gt is then minimized to obtain a new iterate θt.
It can be shown following [10] that stochastic majorization-
minimization algorithms ﬁnd asymptotical stationary point of
the expected risk Ex[f (θ, x)] under mild assumptions recalled
in Section IV. SMM admits the same mini-batch and decaying
weight extensions (used in Alg. 2) as OMF.

(ft

In this work, we extend the SMM framework and allow both
majorization and minimization steps to be approximated. As a
side contribution, our extension proves that performing a single
pass of block coordinate descent to update the dictionary, an
important heuristic in [15], is indeed correct. We ﬁrst introduce
the new matrix factorization algorithm at the core of this paper
and then present the extended SMM framework.

III. STOCHASTIC SUBSAMPLING FOR HIGH DIMENSIONAL
DATA DECOMPOSITION

The online algorithm presented in Section II is very efﬁcient
to factorize matrices that have a large number of columns (i.e.,
with a large number of samples n), but a reasonable number
of rows — the dataset is not very high dimensional. However,
it is not designed to deal with very high number of rows: the
cost of a single iteration depends linearly on p. On terabyte-
105 features, the original
scale datasets from fMRI with p = 2
online algorithm requires one week to reach convergence. This

·

Fig. 1. Stochastic subsampling further improves online matrix factorization to
handle datasets with large number of columns and rows. X is the input p × n
matrix, Dt and At are respectively the dictionary and code at time t.

is a major motivation for designing new matrix factorization
algorithms that scale in both directions.

In the large-sample regime p

k, the underlying dimen-
sionality of columns may be much lower than the actual p:
the rows of a single column drawn at random are therefore
correlated and redundant. This guides us on how to scale
online matrix factorization with regard to the number of rows:

(cid:29)

• The online algorithm OMF uses a single column of (or
mini-batch) of X at each iteration to enrich the average
surrogate and update the whole dictionary.

• We go a step beyond and use a fraction of a single column

of X to reﬁne a fraction of the dictionary.

More precisely, we draw a column and observe only some
to reﬁne these rows of the
of its rows at each iteration,
dictionary, as illustrated in Figure 1. To take into account
all features from the dataset, rows are selected at random at
each iteration: we call this technique stochastic subsampling.
Stochastic subsampling reduces the efﬁciency of the dictionary
update per iteration, as less information is incorporated in the
current iterate Dt. On the other hand, with a correct design,
the cost of a single iteration can be considerably reduced, as it
grows with the number of observed features. Section V shows
that the proposed algorithm is an order of magnitude faster
than the original OMF on large and redundant datasets.

First, we formalize the idea of working with a fraction of
the p rows at a single iteration. We adapt the online matrix
factorization algorithm, to reduce the iteration cost by a factor
close to the ratio of selected rows. This deﬁnes a new on-
line algorithm, called subsampled online matrix factorization
(SOMF). At each iteration, it uses q rows of the column xt to
update the sequence of iterates (Dt)t. As in Section II, we
introduce a more general algorithm, stochastic approximate
majorization-minimization (SAMM), of which SOMF is an
instance. It extends the stochastic majorization-minimization
framework, with similar theoretical guarantees but potentially
faster convergence.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

5

A. Subsampled online matrix factorization

∈

[1, n]

Formally, as in online matrix factorization, we consider a
sample stream (xt)t in Rp that cycles onto a ﬁnite sample set
x(i), i
{
1) Stochastic subsampling and algorithm outline: We want
to reduce the time complexity of a single iteration. In the
original algorithm, the complexity depends linearly on the
sample dimension p in three aspects:

, and minimize the empirical risk (3).1
}

∈

∈

∈

Rp×k,

Rp is used to compute the code αt,
• xt
• it is used to update the surrogate parameters ¯Bt
• Dt

Rp×k is fully updated at each iteration.
Our algorithm reduces the dimensionality of these steps at
each iteration, such that p becomes q = p
r in the time
complexity analysis, where r > 1 is a reduction factor.
Formally, we randomly draw, at iteration t, a mask Mt that
“selects” a random subset of xt. We use it to drop a part of the
features of xt and to “freeze” these features in dictionary D
at iteration t.

It is convenient to consider Mt as a Rp×p random diagonal
matrix, such that each coefﬁcient is a Bernouilli variable with
parameter 1
1],

r , normalized to be 1 in expectation.

[0, p

j
∀
, P(cid:2)Mt[j, j] = 0(cid:3) = 1

∈
1
r

.

−

−

(16)

P(cid:2)Mt[j, j] = r(cid:3) =

1
r

Thus, r describes the average proportion of observed features
and Mtxt is a non-biased, low-dimensional estimator of xt:

E(cid:2)
Mtxt
(cid:107)

(cid:107)

(cid:3) =

0

p
r

= q

E(cid:2)Mtxt

(cid:3) = xt.

(17)

(cid:107) · (cid:107)

0 counting the number of non-zero coefﬁcients. We
with
deﬁne the pair of orthogonal projectors Pt
t ∈
R(p−q)×p that project Rp onto Im(Mt) and Ker(Mt). In other
Rp×y
words, PtY and P⊥
with rows respectively selected and not selected by Mt. In
Rq×n assigns the rows of Z to the
algorithms, PtY
rows of Y selected by Pt, by an abuse of notation.

t Y are the submatrices of Y

Rq×p and P⊥

←

Z

∈

∈

∈

In brief, subsampled online matrix factorization, deﬁned in
Alg. 3, follows the outer loop of online matrix factorization,
with the following major modiﬁcations at iteration t:

• it uses Mtxt and low-size statistics instead of xt to

estimate the code αt and the surrogate gt,

• it updates a subset of the dictionary PtDt−1 to reduce
the surrogate value ¯gt(D). Relevant parameters of ¯gt are
computed using Ptxt and αt only.

We now present SOMF in details. For comparison purpose,
we write all variables that would be computed following the
OMF rules at iteration t with a (cid:63) superscript. For simplicity, in
Alg. 3 and in the following paragraphs, we assume that we use
one sample per iteration —in practice, we use mini-batches of
size η. The next derivations are transposable when a batch It
is drawn at iteration t instead of a single sample it.

2) Code computation: In the OMF algorithm presented in
t is obtained by solving (6), namely

Section II, α(cid:63)

α(cid:63)

t ∈

argmin
α

1
2

α(cid:62)G(cid:63)

t α

α(cid:62)β(cid:63)

t + λΩ(α),

(21)

−

Algorithm 3 Subsampled online matrix factorization (SOMF)
(wt)t>0,

iterate D0, weight

sequences

Initial

Input:
(γc)c>0, sample set
{
for t from 1 to T do

x(i)

}i>0, number of iterations T .
Draw xt = x(i) at random and Mt following (16).
Update the regression parameters for sample i:

c(i)
β(i)
G(i)

c(i) + 1,

(1

←
t ←
t ←
Compute the approximate code for xt:

t−1 + γD(cid:62)
t−1 + γD(cid:62)

t−1Mtx(i),
t−1MtDt−1, Gt

γ)G(i)
γ)G(i)

(1

−

−

γ
←
βt ←
←

γc(i) .
β(i)
.
t
¯G(i)
t

.

αt

argmin
α∈Rk

←

1
2

α(cid:62)Gtα

α(cid:62)βt + λ Ω(α).

(18)

−

Update the parameters of the aggregated surrogate ¯gt:

¯Ct
Pt ¯Bt

(1

(1

−

−

←

←

wt) ¯Ct−1 + wtαtα(cid:62)
t .
wt)Pt ¯Bt−1 + wtPtxtα(cid:62)
t .

Compute simultaneously (using Alg. 4 for 1st line):

PtDt

P⊥
t

¯Bt

argmin
Dr∈Cr
(1

1
2
wt)P⊥
t

−

←

←

Tr (Dr (cid:62)Dr ¯Ct)

Tr (Dr (cid:62)Pt ¯Bt).

−
¯Bt−1 + wtP⊥
t xtα(cid:62)
t .

(19)

(20)

Output: Final iterate DT .

t = D(cid:62)

t = D(cid:62)

t−1Dt−1 and β(cid:63)
t and β(cid:63)

where G(cid:63)
t−1xt. For large p, the
computation of G(cid:63)
t dominates the complexity of the
regression step, which depends almost linearly on p. To reduce
this complexity, we use estimators for G(cid:63)
t , computed
at a cost proportional to the reduced dimension q. We propose
three kinds of estimators with different properties.

t and β(cid:63)

a) Masked loss: The most simple unbiased estimation
t and β(cid:63)
t whose computation cost depends on q is

of G(cid:63)
obtained by subsampling matrix products with Mt:
Gt = D(cid:62)
βt = D(cid:62)

t−1MtDt−1
t−1Mtxt.

(a)

This is the strategy proposed in [26]. We use Gt and βt
in (18), which amounts to minimize the masked loss

min
α∈Rk

1
2 (cid:107)

Mt(xt

D(cid:62)

t−1α)
(cid:107)

−

2
2 + λΩ(α).

(22)

t and β(cid:63)

Gt and βt are computed in a number of operations pro-
portional to q, which brings a speed-up factor of almost r
in the code computation for large p. On large data, using
estimators (a) instead of exact G(cid:63)
t proves very ef-
ﬁcient during the ﬁrst epochs (cycles over the columns).2
However, due to the masking, Gt and βt are not consistent
estimators: they do not converge to G(cid:63)
t for large t,
which breaks theoretical guarantees on the algorithm output.
Empirical results in Section V-E show that the sequence of
iterates approaches a critical point of the risk (3), but may
then oscillate around it.

t and β(cid:63)

1Note that we solve the fully observed problem despite the use of subsam-

2Estimators (a) are also available in the inﬁnite sample setting, when

pled data, unlike other recent work on low-rank factorization [34].

minimizing expected risk (5) from a i.i.d sample stream (xt)t.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

6

{

x(i)

b) Averaging over epochs: At iteration t, the sample xt
}i. This allows to
is drawn from a ﬁnite set of samples
average estimators over previously seen samples and address
the non-consistency issue of (a). Namely, we keep in memory
2n estimators, written (G(i)
t )1≤i≤n. We observe the
t
sample i = it at iteration t and use it to update the i-th
estimators ¯G(i)
t
β(i)
G(i)

following
γ)G(i)
γ)G(i)

t−1Mtx(i)
t−1MtD(i)

t−1 + γD(cid:62)
t−1 + γD(cid:62)

t = (1

t = (1

, ¯β(i)

, β(i)

(23)

−

,

t

t

−

where γ is a weight factor determined by the number of time
the one sample i has been previously observed at time t.
Precisely, given (γc)c a decreasing sequence of weights,
c(i)
t =

t, xs = x(i)(cid:111)(cid:12)
(cid:12)
(cid:12) .

γ = γc(i)

where

(cid:110)
s

(cid:12)
(cid:12)
(cid:12)

t

≤

All others estimators
iteration t
−
averaged estimators
Gt (cid:44) G(i)

t =

G(j)
t
{
G(i)
1. The set
t
{

, β(j)
t }j(cid:54)=i are left unchanged from
, β(i)
t }1≤i≤n is used to deﬁne the

(cid:88)

γ(i)
s,tD(cid:62)

s−1MsDs−1

βt

(cid:44) β(i)

t =

γ(i)
s,tD(cid:62)

s−1Msx(i),

(b)

s≤t,xs=x(i)
(cid:88)

s≤t,xs=x(i)

(cid:81)

s,t = γc(i)

where γ(i)
). Using βt and Gt
in (18), αt minimizes the masked loss averaged over the
previous iterations where sample i appeared:

s<t,xs=x(i) (1

γc(i)

−

s

t

γ(i)
s,t
2 (cid:107)

min
α∈Rk

(cid:88)

s≤t
xs=x(i)

Ms(x(i)

D(cid:62)

2
2 + λΩ(α).
s−1α)
(cid:107)

−

(24)

t )t and (β(cid:63)

The sequences (Gt)t and (βt)t are consistent estimations
of (G(cid:63)
t )t — consistency arises from the fact
that a single sample x(i) is observed with different masks
along iterations. Solving (24) is made closer and closer to
solving (21), to ensure the correctness of the algorithm (see
Section IV). Yet, computing the estimators (b) is no more
costly than computing (a) and still permits to speed up a single
iteration close to r times. In the mini-batch setting, for every
and β(i)
to compute α(i)
It, we use the estimators G(i)
i
.
t
t
t
(n k2). This is reasonable
This method has a memory cost of
k2.
compared to the dataset size3 if p

∈

c) Exact Gram computation: To reduce the memory
usage, another strategy is to use the true Gram matrix Gt
and the estimator βt from (b):
t = D(cid:62)

O
(cid:29)

Gt (cid:44) G(cid:63)
βt

(cid:44) (cid:88)

t−1Dt−1
γ(i)
s,tD(cid:62)

s−1Msx(i)

(c)

s≤t,xs=x(i)

As previously, the consistency of (βt)t ensures that (5) is
correctly solved despite the approximation in (αt)t computa-
tion. With the partial dictionary update step we propose, it is
possible to maintain Gt at a cost proportional to q. The time

3It is also possible to efﬁciently swap the estimators (G(i)

t )i on disk, as

they are only accessed for i = it at iteration t.

TABLE I
COMPARISON OF ESTIMATORS USED FOR CODE COMPUTATION

Est.

(a)
(b)
(c)

βt

Gt

Convergence

Extra
mem. cost

Masked
Averaged
Averaged

Masked
Averaged
Exact

(cid:88)
(cid:88)

n k2
n k

1st epoch
perform.
(cid:88)
(cid:88)

O

complexity of the coding step is thus similarly reduced when
replacing (b) or (c) estimators in (21), but the latter option
(n k). Although estimators (c) are
has a memory usage in
slightly less performant in the ﬁrst epochs, they are a good
compromise between resource usage and convergence. We
summarize the characteristics of the three estimators (a)–(c) in
Table I, anticipating their empirical comparison in Section V.
Surrogate computation: The computation of αt using one

of the estimators above deﬁnes a surrogate gt(D) (cid:44) 1
2 (cid:107)
−
2
Dαt
2 + λΩ(α), which we use to update the aggregated
(cid:107)
surrogate ¯gt (cid:44) (1
wt)¯gt−1 + wtgt, as in online matrix
factorization. We follow (8) (with weights (wt)t) to update the
matrices ¯Bt and ¯Ct, which deﬁne ¯gt up to constant factors.
The update of ¯Bt requires a number of operations proportional
to p. Fortunately, we will see in the next paragraph that it is
possible to update Pt ¯Bt in the main thread with a number
of operation proportional to q and to complete the update of
P⊥
t

¯Bt in parallel with the dictionary update step.
Weight sequences: Speciﬁc (wt)t and (γc)c in Alg. 3
are required. We provide then in Assumption (B) of the
( 11
analysis: wt = 1
12 , 1) and
cv , where u
2(cid:1) to ensure convergence. Weights have little
(cid:0) 3
4 , 3u
v
impact on convergence speed in practice.

tu and γc = 1

xt

−

−

∈

∈

3) Dictionary update: In the original online algorithm, the
whole dictionnary Dt−1 is updated at iteration t. To reduce the
time complexity of this step, we add a “freezing” constraint to
the minimization (7) of ¯gt. Every row r of D that corresponds
to an unseen row r at iteration r (such that Mt[r, r] = 0)
remains unchanged. This casts the problem (7) into a lower
dimensional space. Formally, the freezing operation comes out
as a additional constraint in (7):

Dt =

argmin
D∈C
t D=P⊥

P⊥

t Dt−1

1
2

Tr (D(cid:62)D ¯Ct)

Tr (D(cid:62) ¯Bt).

(25)

−

1 and P⊥

The constraints are separable into two blocks of rows. Re-
calling the notations of (2), for each atom d(j), the rules
t d(j)
d(j)
t d(j) = P⊥
t−1 can indeed be rewritten
(cid:40)
d(j)
− (cid:107)
t d(j)
t−1.

Ptd(j)
(cid:107)
t d(j)
P⊥

1
= P⊥

Ptd(j)
(cid:107)

(cid:44) r(j)
t

t−1(cid:107)

t−1(cid:107)

(cid:107) ≤

(26)

(cid:107) ≤

+

(cid:107)

Solving (25) is therefore equivalent to solving the following
(cid:44) PtBt,
problem in Rq×k, with Br
t

Dr

∈
r =

argmin
Dr∈Cr
Dr
{

∈

1
2
Rq×k/
j
∀

where

Tr (Dr (cid:62)Dr ¯Ct)

Tr (Dr (cid:62) ¯Br

t ) (27)

−

1],

[0, k

dr(j)
(cid:107)

r(j)
t }

.

C

∈
The rows of Dt selected by Pt are then replaced with Dr,
while the other rows of Dt are unchanged from iteration

(cid:107) ≤

−

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

7

Algorithm 4 Partial dictionary update

Gt

D(cid:62)

t−1PtDt−1.

Input: Dictionary Dt−1, projector Pt, statistics ¯Ct, ¯Bt,
norms (n(j)
t−1)0≤j<k, Gram matrix Gt (optional).
Dt−1,
Dt
Gt
←
←
−
permutation([1, k]) do
for j
∈
n(j)
r(j)
Ptd(j)
t−1 +
.
t ←
t−1(cid:107)
(cid:107)
¯Ct[j,j] (Pt ¯b(j)
Ptd(j)
t−1 + 1
u
←
enet projection(u, r(j)
Ptd(j)
Ptd(j)
n(j)
.
t (cid:107)
t ←
t PtDt.
←

Gt+1
Output: Dictionary Dt, norms (n(j)

t ←
r(j)
t − (cid:107)
Gt + D(cid:62)

PtDt¯c(j)
t ).

t )j, Gram matrix Gt+1.

(cid:46) in Rq
(cid:46) in Rq

t −

t ).

t

t

∈

−

t −

¯br(j)
t

and ¯br(j)

t Dt = P⊥

, at an average cost in

1. Formally, PtDt = Dr and P⊥

k operations, as it writes Dr¯c(j)

t Dt−1. We
t
solve (27) by a projected block coordinate descent (BCD)
similar to the one used in the original algorithm, but performed
in a subspace of size q. We compute each column j of the
gradient that we use in the block coordinate descent loop
Rq,
with q
×
where ¯c(j)
are the j-th columns of ¯Ct and ¯Br
t .
t
Each reduced atom dr(j) is projected onto the elastic-net ball
of radius r(j)
(q) following [15]. This
makes the complexity of a single-column update proportional
to q. Performing the projection requires to keep in memory the
d(j)
(cid:44) 1
values
t (cid:107)}j, which can be updated online at
{
a negligible cost.
We provide the reduced dictionary update step in Alg. 4,
where we use the function enet projection(u, r) that per-
Rq onto the elastic-net
forms the orthogonal projection of u
ball of radius r. As in the original algorithm, we perform a
single pass over columns to solve (27). Dictionary update is
now performed with a number of operations proportional to q,
instead of p in the original algorithm. Thanks to the random
nature of (Mt)t, updating Dt−1 into Dt reduces ¯gt enough
to ensure convergence.

n(j)
t

− (cid:107)

O

∈

t with a cost in

Gram matrix computation: Performing partial updates
of Dt makes it possible to maintain the full Gram matrix
Gt = G(cid:63)
(q k2) per iteration, as mentioned
O
in III-A2c. It is indeed enough to compute the reduced Gram
matrix D(cid:62)PtD before and after the dictionary update:
D(cid:62)
t PtD(cid:62)
t .

Gt+1 = D(cid:62)

t−1 + D(cid:62)

t−1PtD(cid:62)

t Dt = Gt

(28)

−

Parallel surrogate computation: Performing block coor-
t = Pt ¯Bt only.
t requires to access ¯Br
dinate descent on ¯gr
Assuming we may use use more than two threads, this allows
to parallelize the dictionary update step with the update
of P⊥
t
Pt ¯Bt

¯Bt. In the main thread, we compute Pt ¯Bt following

wt) ¯PtBt−1 + wtPtxtα(cid:62)
t .

(19 – Alg. 3)

(1

←

−

which has a cost proportional to q. Then, we update in parallel
the dictionary and the rows of ¯Bt that are not selected by Mt:
¯Bt−1 + wtP⊥

(20 – Alg. 3)

t xtα(cid:62)
t .

wt)P⊥
t

P⊥
t

¯Bt

(1

←

−

This update requires k(p
q)η operations (one matrix-matrix
product) for a mini-batch of size η. In contrast, with appropri-
ate implementation, the dictionary update step requires 4 k q2

−

∼
∼

to 6 k q2 operations, among which 2 k q2 come from slower
η, updating ¯Bt is faster
matrix-vector products. Assuming k
10, and performing (20)
than updating the dictionary up to r
on a second thread is seamless in term of wall-clock time.
More threads may be used for larger reduction or batch size.
4) Subsampling and time complexity: Subsampling may be
used in only some of the steps of Alg. 3, with the other
steps following Alg. 1. Whether to use subsampling or not in
each step depends on the trade-off between the computational
speed-up it brings and the approximations it makes. It is useful
to understand how complexity of OMF evolves with p. We
write s the average number of non-zero coefﬁcients in (αt)t
(s = k when Ω =

2
2). OMF complexity has three terms:

(i)

(ii)

(p k2): computation of the Gram matrix Gt, update of

O
the dictionary Dt with block coordinate descent,

(p k η): computation of βt = D(cid:62)

t−1xt and of ¯Bt

(cid:107) · (cid:107)

O
using xtα(cid:62)
t ,

(iii)

(k s2 η): computation of αt using Gt and βt, using

O
matrix inversion or elastic-net regression.

∼

Using subsampling turns p into q = p
r in the expressions
above. It improves single iteration time when the cost of re-
(k s2 η) is dominated by another term. This happens
gression
O
whenever p
r > s2, where r is the reduction factor used in the
algorithm. Subsampling can bring performance improvement
p
up to r
s2 . It can be introduced in either computations
from (i) or (ii), or both. When using small batch size, i.e.,
when η < k, computations from (i) dominates complexity, and
subsampling should be ﬁrst introduced in dictionary update (i),
and for code computation (ii) beyond a certain reduction ratio.
On the other hand, with large batch size η > k, subsampling
should be ﬁrst introduced in code computation, then in the
dictionary update step. The reasoning above ignore potentially
large constants. The best trade-offs in using subsampling must
be empirically determined, which we do in Section V.

B. Stochastic approximate majorization-minimization

The SOMF algorithm can be understood within the stochastic
majorization-minimization framework. The modiﬁcations that
we propose are indeed perturbations to the ﬁrst and third steps
of the SMM presented in Algorithm 2:

• The code is computed approximately:

the surrogate
is only an approximate majorizing surrogate of ft
near Dt−1.

• The surrogate objective is only reduced and not mini-
mized, due to the added constraint and the fact that we
perform only one pass of block coordinate descent.

We propose a new stochastic approximate majorization-
minimization (SAMM) framework handling these perturbations:
• A majorization step (12 – Alg. 2), computes an approx-
g(cid:63)
t , where gt is a

imate surrogate of ft near θt−1: gt
true upper-bounding surrogate of ¯ft.

≈

• A minimization step (13 – Alg. 2), ﬁnds θt by reducing
(cid:44) argminθ∈Θ ¯gt(θ),

θ(cid:63)
t

enough the objective ¯gt: θt
which implies ¯gt(θt) (cid:38) ¯gt(θ(cid:63)

≈
t ).

The SAMM framework is general, in the sense that approxima-
tions are not speciﬁed. The next section provides a theoretical

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

8

analysis of the approximation of SAMM and establishes how
SOMF is an instance of SAMM. It concludes by establishing
Proposition 1, which provides convergence guarantees for
SOMF, under the same assumptions made for OMF in [15].

IV. CONVERGENCE ANALYSIS
We establish the convergence of SOMF under reasonable
assumptions. For the sake of clarity, we ﬁrst state our principal
result (Proposition 1), that guarantees SOMF convergence. It is
a corollary of a more general result on SAMM algorithms. To
present this broader result, we recall the theoretical guarantees
of the stochastic majorization-minimization algorithm [10]
(Proposition 2); then, we show how the algorithm can with-
stand pertubations (Proposition 3). Proofs are reported in
Appendix A. SAMM convergence is proven before establishing
SOMF convergence as a corollary of this broader result.

A. Convergence of SOMF

Similar to [15, 34], we show that the sequence of iterates
(Dt)t asymptotically reaches a critical point of the empirical
risk (3). We introduce the same hypothesis on the code
covariance estimation ¯Ct as in [15] and a similar one on Gt —
they ensure strong convexity of the surrogate and boundedness
of (αt)t. They do not cause any loss of generality as they are
met in practice after a few iterations, if r is chosen reasonably
low, so that q > k. The following hypothesis can also be
guaranteed by adding small (cid:96)2 regularizations to ¯f .

(A) There exists ρ > 0 such that for all t > 0, ¯Ct, Gt

ρI.

(cid:31)

We further assume, that the weights (wt)t and (γc)c decay
at speciﬁc rates. We specify simple weight sequences, but the
proofs can be adapted for more complex ones.

(B) There exists u

(cid:0) 3
4 , 3u
for all t > 0, c > 0, wt = t−u, γc (cid:44) c−v.

( 11
12 , 1) and v

∈

∈

−

2) such that,

The following convergence result then applies to any se-
quence (Dt)t produced by SOMF, using estimators (b) or (c).
¯f is the empirical risk deﬁned in (3).

Proposition 1 (SOMF convergence). Under assumptions (A)
and (B), ¯f (Dt) converges with probability one and every limit
point D∞ of (Dt)t is a stationary point of ¯f : for all D

¯f (D∞, D

D∞)

0

∇

−
This result applies for any positive subsampling ratio r,
which may be set arbitrarily high. However, selecting a
reasonable ratio remains important for performance.

≥

Proposition 1 is a corollary of a stronger result on SAMM
algorithms. As it provides insights on the convergence mech-
anisms, we formalize this result in the following.

∈ C
(29)

B. Basic assumptions and results on SMM convergence

We ﬁrst recall the main results on stochastic majorization-
minimization algorithms, established in [10], under assump-
tions that we slightly tighten for our purpose. In our setting,
we consider the empirical risk minimization problem
(cid:16) ¯f (θ) (cid:44) 1
n

(cid:17)
f (θ, x(i))

min
θ∈Θ

n
(cid:88)

(30)

,

i=1

where f : RK

R is a loss function and

× X →

(C) Θ

RK and the support

of the data are compact.

⊂

X

This is a special case of (5) where the samples (xt)t are
i. The loss functions ft (cid:44)
x(i)
{
, xt) deﬁned on RK can be non-convex. We instead assume
·

drawn uniformly from the set
f (
that they meet reasonable regularity conditions:

}

(D) (ft)t is uniformly R-Lipschitz continuous on RK and

uniformly bounded on Θ.

(E) The directional derivatives [35]
¯f (θ, θ(cid:48)

∇
θ) exist for all θ and θ(cid:48) in RK.

ft(θ, θ(cid:48)

θ) and

−

∇

−

Assumption (E) allows to characterize the stationary points
0
Θ such that
of problem (30), namely θ
for all θ(cid:48)
Θ — intuitively a point is stationary when there
is no local direction in which the objective can be improved.
the deﬁnition of ﬁrst-order surrogate
functions used in the SMM algorithm. (gt)t are selected in
the set

ρ,L(ft, θt−1), hereby introduced.

Let us now recall

¯f (θ, θ(cid:48)

θ)

∇

−

≥

∈

∈

S

Deﬁnition 1 (First-order surrogate function). Given a function
f : RK
R, θ
ρ,L(f, θ) as
the set of functions g : RK

Θ and ρ, L > 0, we deﬁne

R such that

→

∈

S

→
• g is majorizing f on Θ and g is ρ-strongly convex,
• g and f are tight at θ — i.e., g(θ) = f (θ), g

differentiable,

f ) is L-Lipschitz,

(g

∇

−

(g

∇

−

In OMF, gt deﬁned in (15) is a variational surrogate4 of ft.
We refer the reader to [36] for further examples of ﬁrst-order
surrogates. We also ensure that ¯gt should be parametrized and
thus representable in memory. The following assumption is
met in OMF, as ¯gt is parametrized by the matrices ¯Ct and ¯Bt.

f is
−
f )(θ) = 0.

(F) Parametrized surrogates. The surrogates (¯gt)t are
RP . Namely,
such that ¯gt is unequivocally

parametrized by vectors in a compact set
for all t > 0, there exists κt
deﬁned as gt (cid:44) ¯gκt.

K ⊂

∈ K

Finally, we ensure that the weights (wt)t used in Alg. 2

decrease at a certain rate.

(G) There exists u

( 3
4 , 1) such that wt = t−u.

∈

When (θt)t is the sequence yielded by Alg. 2, the following
result (Proposition 3.4 in [10]) establishes the convergence
of ( ¯f (θt))t and states that θt is asymptotically a stationary
point of the ﬁnite sum problem (30), as a special case of the
expected risk minimization problem (5).

Proposition 2 (Convergence of SMM, from [10]). Under as-
sumptions (C) – (G), ( ¯f (θt))t≥1 converges with probability
one. Every limit point θ∞ of (θt)t is a stationary point of the
risk ¯f deﬁned in (30). That is,

θ
∀

∈

Θ,

∇

¯f (θ∞, θ

θ∞)

0.

≥

−

(31)

The correctness of the online matrix factorization algorithm

can be deduced from this proposition.

4In this case as in SOMF, gt is not ρ-strongly convex but ¯gt is, thanks to

assumption (A). This is sufﬁcient in the proofs of convergence.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

9

C. Convergence of SAMM

We now introduce assumptions on the approximations made
in SAMM, before extending the result of Proposition 2. We
make hypotheses on both the surrogate computation (ma-
jorization) step and the iterate update (minimization) step. The
principles of SAMM are illustrated in Figure 2, which provides
a geometric interpretation of the approximations introduced in
the following assumptions (H) and (I).

1) Approximate surrogate computation: The SMM algo-
rithm selects a surrogate for ft at point θt−1 within the set
ρ,L(ft, θt−1). Surrogates within this set are tight at θt−1
S
and greater than ft everywhere. In SAMM, we allow the use
of surrogates that are only approximately majorizing ft and
approximately tight at θt−1. This is indeed what SOMF does
when using estimators in the code computation step. For that
ρ,L(f, θ, (cid:15)), that contains all
purpose, we introduce the set
T
ρ,L(f, θ) for the (cid:96)∞-norm:
functions (cid:15)-close of a surrogate in

S

Deﬁnition 2 (Approximate ﬁrst-order surrogate function).
Given a function f : RK
R, θ
ρ,L(f, θ, (cid:15))
→
is the set of ρ-strongly convex functions g : RK
R
such that

Θ and (cid:15) > 0,

→

∈

T

• g is (cid:15)-majorizing f on Θ:
• g and f are (cid:15)-tight at θ — i.e., g(θ)

∈

κ

∀

Θ, g(κ)

−
f (θ)

f (κ)

is differentiable,

(g

∇

−

f ) is L-lipschitz.

−

(cid:15),
f

≥ −
(cid:15), g
−

≤

We assume that SAMM selects an approximative surrogate
ρ,L(ft, θt−1, (cid:15)t) at each iteration, where ((cid:15)t)t is a deter-
in
ministic or random non-negative sequence that vanishes at a
sufﬁcient rate.

T

∈ O

∞ 0 almost surely.

(H) For all t > 0, there exists (cid:15)t > 0 such that gt
∈
ρ,L(ft, θt−1, (cid:15)t). There exists a constant η > 0 such that
T
E[(cid:15)t]

(t2(u−1)−η) and (cid:15)t

→
As illustrated on Figure 2, given the OMF surrogate g(cid:63)
t ∈
ρ,L(ft, θt−1) deﬁned in (15), any function gt such that
S
ρ,L(ft, θt−1, (cid:15)) — e.g., where gt uses
gt
(cid:107)
an approximate αt in (15). This assumption can also be met in
matrix factorization settings with difﬁcult code regularizations,
that require to make code approximations.

∞ < (cid:15) is in

g(cid:63)
t (cid:107)

−

T

2) Approximate surrogate minimization: We do not re-
quire θt to be the minimizer of ¯gt any longer, but ensure that
the surrogate objective function ¯gt decreases “fast enough”.
Namely, θt obtained from partial minimization should be
closer to a minimizer of ¯gt than θt−1. We write (
t)t and
(
the ﬁltrations induced by the past of the algorithm,
Ft− 1
respectively up to the end of iteration t and up to the beginning
of the minimization step in iteration t. Then, we assume

F

)

t

2

(I) For all t > 0, ¯gt(θt) < ¯gt(θt−1). There exists µ > 0

such that, for all t > 0, where θ(cid:63)

t = argminθ∈Θ ¯gt(θ),

(1

µ)(¯gt(θt−1)

¯gt(θ(cid:63)

t )). (32)

E[¯gt(θt)

¯gt(θ(cid:63)
t )

]

|Ft− 1

2

≤

−

−
Assumption (I) is met by choosing an appropriate method
for the inner ¯gt minimization step — a large variety of
gradient-descent algorithms indeed have convergence rates of
the form (32). In SOMF, the block coordinate descent with
frozen coordinates indeed meet this property, relying on results

−

Fig. 2. Both steps of SAMM make well-behaved approximations. The
operations that are performed in exact SMM are in green and superscripted
by (cid:63), while the actual computed values are in orange. Light bands recall the
bounds on approximations assumed in (H) and (I).

from [37]. When both assumptions are met, SAMM enjoys the
same convergence guarantees as SMM.

3) Asymptotic convergence guarantee: The following
proposition guarantees that the stationary point condition of
Proposition 2 holds for the SAMM algorithm, despite the use
of approximate surrogates and approximate minimization.

Proposition 3 (Convergence of
tions (C) – (I),
for SAMM.

SAMM). Under assump-
the conclusion of Proposition 2 holds

Assumption (H) is essential to bound the errors introduced
by the sequence ((cid:15)t)t in the proof of Proposition 3, while (I)
is the key element to show that the sequence of iterates (θt)t
is stable enough to ensure convergence. The result holds for
any subsampling ratio r, provided that (A) remains true.

4) Proving SOMF convergence: Assumptions (A) and (B)
readily implies (C)–(G). With Proposition 3 at hand, proving
Proposition 1 reduces to ensure that the surrogate sequence of
SOMF meets (H) while its iterate sequence meets (I).

V. EXPERIMENTS

The SOMF algorithm is designed for datasets with large
number of samples n and large dimensionality p. Indeed,
as detailed in Section III-A, subsampling removes the com-
putational bottlenecks that arise from high dimensionality.
Proposition 1 establishes that the subsampling used in SOMF
is safe, as it enjoys the same guarantees as OMF. However,
as with OMF, no convergence rate is provided. We therefore
perform a strong empirical validation of subsampling.

We tackle two different problems, in functional Magnetic
Resonance Imaging (fMRI) and hyperspectral imaging. Both
involve the factorization of very large matrices X with sparse
factors. As the data we consider are huge, subsampling reduces
the time of a single iteration by a factor close to p
q . Yet it
is also much redundant: SOMF makes little approximations
and accessing only a fraction of the features per iteration
should not hinder much the reﬁnement of the dictionary. Hence
high speed-ups are expected — and indeed obtained. All
experiments can be reproduced using open-source code.

A. Problems and datasets

1) Functional MRI: Matrix factorization has long been
used on functional Magnetic Resonance Imaging [18]. Data
are temporal series of 3D images of brain activity and are

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

10

decomposed into spatial modes capturing regions that activate
synchronously. They form a matrix X where columns are
the 3D images, and rows corresponds to voxels. Interesting
dictionaries for neuroimaging capture spatially-localized com-
ponents, with a few brain regions. This can be obtained by
enforcing sparsity on the dictionary: we use an (cid:96)2 penalty and
the elastic-net constraint. SOMF streams subsampled 3D brain
records to learn the sparse dictionary D. Data can be huge:
106 (2000
we use the whole HCP dataset [38], with n = 2.4
105, totaling 2 TB
records, 1 200 time points) and p = 2
of dense data. For comparison, we also use a smaller public
dataset (ADHD200 [39]) with 40 records, n = 7000 samples
104 voxels. Historically, brain decomposition have
and p = 6
been obtained by minimizing the classical dictionary learning
objective on transposed data [40]: the code A holds sparse
spatial maps and voxel time-series are streamed. This is not a
natural streaming order for fMRI data as X is stored column-
wise on disk, which makes the sparse dictionary formulation
more appealing. Importantly, we seek a low-rank factorization,
to keep the decomposition interpretable — k

100

p.

·

·

·

2) Hyperspectral imaging: Hyperspectral cameras acquire
images with many channels that correspond to different spec-
tral bands. They are used heavily in remote sensing (satellite
imaging), and material study (microscopic imaging). They
yield digital images with around 1 million pixels, each as-
sociated with hundreds of spectral channels. Sparse matrix
factorization has been widely used on these data for image
classiﬁcation [41, 42] and denoising [43, 44]. All methods
rely on the extraction of full-band patches representing a local
image neighborhood with all channels included. These patches
are very high dimensional, due to the number of spectral
bands. From one image of the AVIRIS project [45], we extract
106 patches of size 16
n = 2
16 with 224 channels, hence
·
104. A dense dictionary is learned from these patches. It
p = 6
·
should allow a sparse representation of samples: we either use
the classical dictionary learning setting ((cid:96)1/elastic-net penalty),
or further add positive constraints to the dictionary and codes:
both methods may be used and deserved to be benchmarked.
p.
We seek a dictionary of reasonable size: we use k

256

×

∼

(cid:28)

∼

(cid:28)

B. Experimental design

To validate the introduction of subsampling and the useful-

ness of SOMF, we perform two major experiments.

• We measure the performance of SOMF when increasing
the reduction factor, and show beneﬁts of stochastic
dimension reduction on all datasets.

• We assess the importance of subsampling in each of
the steps of SOMF. We compare the different approaches
proposed for code computation.

Validation: We compute the objective function (3) over a
test set to rule out any overﬁtting effect — a dictionary should
be a good representation of unseen samples. This criterion is
always plotted against wall-clock time, as we are interested in
the performance of SOMF for practitioners.

Tools: To perform a valid benchmark, we implement OMF
and SOMF using Cython [46] We use coordinate descent [47]
to solve Lasso problems with optional positivity constraints.

TABLE II
SUMMARY OF EXPERIMENTAL SETTINGS

Field

Dataset

Functional MRI

Hyperspectral imaging

ADHD

HCP

Patches from AVIRIS

Factors
# samples n
# features p
X size
Use case ex.

D sparse, A dense
2 · 106
2 · 105
2 TB

7 · 103
6 · 104
2 GB
Extracting predictive feature

D dense, A sparse
2 · 106
6 · 104
103 GB
Recognition / denoising

Code computation is parallelized to handle mini-batches. Ex-
periments use scikit-learn [48] for numerics, and nilearn [49]
for handling fMRI data. We have released the code in an open-
source Python package5. Experiments were run on 3 cores of
¯Bt is
an Intel Xeon 2.6GHz, in which case computing P⊥
t
faster than updating PtDt.

Parameter setting: Setting the number of components k
and the amount of regularization λ is a hard problem in the
absence of ground truth. Those are typically set by cross-
validation when matrix factorization is part of a supervised
pipeline. For fMRI, we set k = 70 to obtain interpretable
networks, and set λ so that the decomposition approximately
covers the whole brain (i.e., every map is k
70 ) sparse). For
hyperspectral images, we set k = 256 and select λ to obtain
a dictionary on which codes are around 3% sparse. We cycle
randomly through the data (fMRI records, image patches) until
convergence, using mini-batches of size η = 200 for HCP
and AVIRIS, and η = 50 for ADHD (small number of sam-
ples). Hyperspectral patches are normalized in the dictionary
learning setting, but not in the non-negative setting — the
classical pre-conditioning for each case. We use u = 0.917
and v = 0.751 for weight sequences.

C. Reduction brings speed-up at all data scales

We benchmark SOMF for various reduction factors against
the original online matrix factorization algorithm OMF [15],
on the three presented datasets. We stream data in the same
order for all reduction factors. Using variant (c) (true Gram
matrix, averaged βt) performs slightly better on fMRI datasets,
whereas (b) (averaged Gram matrix and βt) is slightly faster
for hyperspectral decomposition. For comparison purpose, we
display results using estimators (b) only.

Figure 3 plots the test objective against CPU time. First,
we observe that all algorithms ﬁnd dictionaries with very
close objective function values for all reduction factors, on
each dataset. This is not a trivial observation as the matrix
factorization problem (3) is not convex and different runs of
OMF and SOMF may converge towards minima with different
values. Second, and most importantly, SOMF provides signif-
icant improvements in convergence speed for three different
sizes of data and three different factorization settings. Both ob-
servations conﬁrm the relevance of the subsampling approach.
Quantitatively, we summarize the speed-ups obtained in
Table III. On fMRI data, on both large and medium datasets,
SOMF provides more than an order of magnitude speed-up.

5https://github.com/arthurmensch/modl

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

11

Fig. 3. Subsampling provides signiﬁcant speed-ups on all fMRI and hyperspectral datasets. A reduction factor of 12 is a good overall choice. With larger
data, larger reduction factors can be used for better performance — convergence is reached 13× faster than state-of-the-art methods on the 2TB HCP dataset.

TABLE III
TIME TO REACH CONVERGENCE (< 1% TEST OBJECTIVE)

Dataset

ADHD

Algorithm OMF

SOMF OMF

AVIRIS (NMF) AVIRIS (DL)
SOMF OMF

SOMF OMF

SOMF

HCP

Conv. time 6 min 28 s 2 h 30 43 min 1 h 16 11 min 3 h 50 17 min
Speed-up

13.31

11.8

3.36

6.80

Fig. 5. Proﬁling OMF and SOMF on HCP. Partial dictionary update removes
the major bottleneck of online matrix factorization for small reductions. For
higher reduction, parameter update and code computation must be subsampled
to further reduce the iteration time.

Our ﬁrst experiment establishes the power of stochastic
subsampling as a whole. In the following two experiments, we
reﬁne our analysis to show that subsampling is indeed useful
in the three steps of online matrix factorization.

D. For each step of SOMF, subsampling removes a bottleneck

∼

In Section III, we have provided theoretical guidelines on
when to introduce subsampling in each of the three steps of
an iteration of SOMF. This analysis predicts that, for η
k,
we should ﬁrst use partial dictionary update, before using
approximate code computation and asynchronous parameter
aggregation. We verify this by measuring the time spent by
SOMF on each of the updates for various reduction factors,
on the HCP dataset. Results are presented in Figure 5. We
observe that block coordinate descent is indeed the bottle-
neck in OMF. Introducing partial dictionary update removes
this bottleneck, and as the reduction factor increases, code
computation and surrogate aggregation becomes the major
bottlenecks. Introducing subsampling as described in SOMF
overcomes these bottlenecks, which rationalizes all steps of
SOMF from a computational point of view.

E. Code subsampling becomes useful for high reduction

It remains to assess the performance of approximate code
computation and averaging techniques used in SOMF. Indeed,

Fig. 4. Given a 3 minute time budget, the atoms learned by SOMF are
more focal and less noisy that those learned by OMF. They are closer to
the dictionary of ﬁrst line, for which convergence has been reached.

Practitioners working on datasets akin to HCP can decompose
their data in 20 minutes instead of 4 h previously, while
working on a single machine. We obtain the highest speed-ups
for the largest dataset — accounting for the extra redundancy
that usually appears when dataset size increase. Up to r
8,
speed-up is of the order of r — subsampling induces little
noise in the iterate sequence, compared to OMF. Hyperspectral
decomposition is performed near 7
faster than with OMF in
the classical dictionary learning setting, and 3
in the non-
negative setting, which further demonstrates the versatility of
SOMF. Qualitatively, given a certain time budget, Figure 4
compares the results of OMF and the results of SOMF with
a subsampling ratio r = 24, in the non-negative setting. Our
algorithm yields a valid smooth bank of ﬁlters much faster.
The same comparison has been made for fMRI in [26].

∼

×

×

Comparison with stochastic gradient descent: It is possible
to solve (3) using the projected stochastic gradient (SGD)
algorithm [50]. On all
tested settings, for high precision
convergence, SGD (with the best step-size among a grid) is
slower than OMF and even slower than SOMF. In the dictionary
learning setting, SGD is somewhat faster than OMF but slower
than SOMF in the ﬁrst epochs. Compared to SOMF and OMF,
SGD further requires to select the step-size by grid search.

Limitations: Table III reports convergence time within 1%,
which is enough for application in practice. SOMF is less
beneﬁcial when setting very high precision: for convergence
within 0.01%, speed-up for HCP is 3.4. This is expected
as SOMF trades speed for approximation. For high precision
convergence, the reduction ratio can be reduced after a few
epochs. As expected, there exists an optimal reduction ratio,
depending on the problem and precision, beyond which per-
formance reduces: r = 12 yields better results than r = 24 on
AVIRIS (dictionary learning) and ADHD, for 1% precision.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

12

Fig. 6. Approximating code computation with the proposed subsampling method further accelerates the convergence of SOMF. Reﬁning code computation
using past iterations (averaged estimates) performs better than simply performing a subsampled linear regression as in [26]

subsampling for code computation introduces noise that may
undermine the computational speed-up. To understand the
impact of approximate code computation, we compare three
strategies to compute (αt)t on the HCP dataset. First, we
compute (α(cid:63)
t )t from (xt)t using (21). Subsampling is thus
used only in dictionary update. Second, we rely on masked,
non-consistent estimators (a), as in [26] — this breaks conver-
gence guarantees. Third, we use averaged estimators (βt, Gt)
from (c) to reduce the variance in (αt)t computation.

∈ {

Fig. 6 compares the three strategies for r

12, 24
.
}
Partial minimization at each step is the most important part
to accelerate convergence: subsampling the dictionary updates
already allows to outperforms OMF. This is expected, as
dictionary update constitutes the main bottleneck of OMF in
large-scale settings. Yet, for large reduction factors, using
subsampling in code computation is important
to further
accelerate convergence. This clearly appears when comparing
the plain and dashed black curves. Using past estimates to
better approximate (αt)t yields faster convergence than the
non-converging, masked loss strategy (a) proposed in [26].

VI. CONCLUSION

In this paper, we introduce SOMF, a matrix-factorization
algorithm that can handle input data with very large number of
rows and columns. It leverages subsampling within the inner
loop of a streaming algorithm to make iterations faster and
accelerate convergence. We show that SOMF provides a sta-
tionary point of the non-convex matrix factorization problem.
To prove this result, we extend the stochastic majorization-
minimization framework to two major approximations. We
assess the performance of SOMF on real-world large-scale
problems, with different sparsity/positivity requirements on
learned factors. In particular, on fMRI and hyperspectral data
decomposition, we show that
the use of subsampling can
speed-up decomposition up to 13 times. The larger the dataset,
the more SOMF outperforms state-of-the art techniques, which
is very promising for future applications. This calls for adap-
tation of our approach to learn more complex models.

APPENDIX A
PROOFS OF CONVERGENCE

This appendix contains the detailed proofs of Proposition 3
and Proposition 1. We ﬁrst introduce three lemmas that will be
crucial to prove SAMM convergence, before establishing it by
proving Proposition 3. Finally, we show that SOMF is indeed

∇

an instance of SAMM (i.e. meets the assumptions (C)–(I)),
proving Proposition 1.

A. Basic properties of the surrogates, estimate stability

We derive an important result on the stability and optimality
of the sequence (θt)t, formalized in Lemma 3 — introduced
in the main text. We ﬁrst introduce a numerical lemma on
the boundedness of well-behaved determistic and random
sequence. The proof is detailed in Appendix B.

R, t0

Lemma 1 (Bounded quasi-geometric sequences). Let (xt)t be
a sequence in R+, u : R
R
N and α
[0, 1)
→
αxt−1 + u(xt, xt−1), where
such that, for all t
. Then (xt)t is bounded.
u(x, y)

≤
→ ∞
Let now (Xt)t be a random sequence in R+, such that
t)t the ﬁltration adapted to (Xt)t.

∈
E[Xt] <
If, for all t > t0, there exists a σ-algebra

o(x + y) for x, y

. We deﬁne (

t(cid:48) such that

×
t0, xt

∞

t−1

≥

F

∈

∈

F

F

⊆

t(cid:48)

F

⊆ F

t and

E[Xt

t(cid:48)]

αXt−1 + u(Xt, Xt−1),

(33)

|F
then (Xt)t is bounded almost surely.

≤

We ﬁrst derive some properties of the approximate surrogate

functions used in SAMM. The proof is adapted from [10].

Lemma 2 (Basic properties of approximate surrogate func-
tions). Consider any sequence of iterates (θt)t and assume
L,ρ(ft, θt−1, (cid:15)) for all
there exists (cid:15) > 0 such that gt
1, ¯h0 (cid:44) h0 and
t
≥
¯ht (cid:44) (1
(i) (

∈ T
ft for all t
wt)¯ht−1 + wtht. Under assumptions (D) – (G),
−
ht(θt−1))t>0 is uniformly bounded and there exists

1. Deﬁne ht (cid:44) gt

−

≥

(ii) (ht)t and (¯ht)t are uniformly R(cid:48)-Lipschitz, (gt)t and

ht

}t is uniformly bounded by R(cid:48).

∇
R(cid:48) such that

{∇

(¯gt)t are uniformly (R + R(cid:48))-Lipschitz.

Proof. We ﬁrst prove (i). We set α > 0 and deﬁne θ(cid:48) =
. As ht has a L-Lipschitz gradient on RK,
θt
using Taylor’s inequality (see Appendix B)

α ∇ht(θt)
(cid:107)∇ht(θt)(cid:107)2

−

(34)

ht(θ(cid:48))

ht(θt)

α

−

≤

Lα2
2

2 +

ht(θt)
(cid:107)∇
(cid:107)
ht(θ(cid:48))) +

(cid:107)∇

1
α

ht(θt)
2
(cid:107)

≤
where we use ht(θt) < (cid:15) and
assumption gt

Lα
Lα
2
2 ≤
ht(θ(cid:48)
(cid:15) from the
t)
L,ρ(ft, θt−1, (cid:15)). Moreover, by deﬁnition,

(ht(θt)

2
α

(cid:15) +

−

−

≤

,

ht exists and is L-lipschitz for all t. Therefore,

t

1,

∈ T

ht(θ)
2
(cid:107)

(cid:107)∇

≤ (cid:107)∇

2 + L
ht(θt)
(cid:107)
(cid:107)

θt−1

−

≥

∀
(cid:107)2

θ

(35)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

13

Since Θ is compact and (

(cid:107)2)t≥1 is bounded in (34),
ht(θt)
ht is bounded by R(cid:48) independent of t. (ii) follows by basic

(cid:107)∇

where the second inequality holds for the same reasons as
in (41). Injecting (40) and (42) in (43), we obtain

∇
considerations on Lipschitz functions.

Finally, we prove a result on the stability of the estimates,
that derives from combining the properties of (gt)t and the
geometric decrease assumption (I).

Lemma 3 (Estimate stability under SAMM approximation). In
the same setting as Lemma 2, with the additional assump-
tion (I) (expected linear decrease of ¯gt suboptimality), the
2 converges to 0 as fast as (wt)t, and θt
sequence
(cid:107)
is asymptotically an exact minimizer. Namely, almost surely,

θt
(cid:107)

θt−1

−

θt
(cid:107)

−

θt−1

(cid:107)2 ∈ O

(wt) and ¯gt(θt)

¯gt(θ(cid:63)
t )

−

∈ O

(w2

t ).

(36)

Proof. We ﬁrst establish the result when a deterministic ver-
sion of (I) holds, as it makes derivations simpler to follow.

1) Determistic decrease rate: We temporarily assume that

decays are deterministic.

(Idet) For all t > 0, ¯gt(θt) < ¯gt(θt−1). Moreover, there

exists µ > 0 such that, for all t > 0
¯gt(θ(cid:63)
t )
(1
θ(cid:63)
t = argmin

−
where

¯gt(θt)

≤

−

µ)(¯gt(θt−1)
¯gt(θ),

θ∈Θ

¯gt(θ(cid:63)

t ))

−

We introduce the following auxiliary positive values, that

we will seek to bound in the proof:

At (cid:44)
θt
(cid:107)
−
θ(cid:63)
θ(cid:63)
t−1(cid:107)
t −
(cid:107)

Ct (cid:44)

2, Bt (cid:44)
θt
θt−1
(cid:107)
(cid:107)
2, Dt (cid:44) ¯gt(θt)

θ(cid:63)
2,
t (cid:107)
−
¯gt(θ(cid:63)
t ).

−

Our goal is to bound At. We ﬁrst relate it to Ct and Bt using
convexity of (cid:96)2 norm:

A2

t + 3B2
t is the minimizer of ¯gt, by strong convexity of (¯gt)t,

t−1 + 3C 2
t .

3B2

t ≤

(39)

As θ(cid:63)

ρ
2
while we also have
ρ
θ(cid:63)
2
2 ≤
t−1(cid:107)
2 (cid:107)
wt)(cid:0)¯gt−1(θ(cid:63)

θ(cid:63)
t −
(1

≤

−

B2

t =

ρ
2 (cid:107)

θt

2
θ(cid:63)
2 ≤
t (cid:107)

−

Dt,

¯gt(θ(cid:63)

t−1)

t−1)

−

¯gt(θ(cid:63)
t )
−
t )(cid:1)+wt
¯gt−1(θ(cid:63)

(cid:0)gt(θ(cid:63)

gt(θ(cid:63)

t )(cid:1)

(41)

t−1)
2Q
ρ

−
.

2, and thus Ct

wt

wt(R + R(cid:48))

θ(cid:63)
t −
(cid:107)

θ(cid:63)
t−1(cid:107)

≤
≤
The second inequalities holds because θ(cid:63)
t−1 is a minimizer
of ¯gt−1 and gt is Q-Lipschitz, where Q (cid:44) R + R(cid:48), using
Lemma 2. Replacing (40) and (41) in (39) yields

A2

(Dt + Dt−1) +

t ≤
and we are left to show that Dt
this, we decompose the inequality from (Idet) into

∈ O

w2
t ,

12Q2
ρ
(w2
t ) to conclude. For

(42)

6
ρ

Dt

≤
= (1

(1

µ)(¯gt(θt−1)
−
(cid:16)
(cid:0)gt(θt−1)

−
µ)

t ))

¯gt(θ(cid:63)
gt(θt)(cid:1) + wt

(cid:0)gt(θt)

gt(θ(cid:63)

t )(cid:1)(cid:17)

−
+ (1

µ)

(1

−

−
+ (1

−

−
wt)(cid:0)¯gt−1(θt−1)
wt)(cid:0)¯gt−1(θ(cid:63)

−
t−1)

¯gt−1(θ(cid:63)

−
t−1)(cid:1)
t )(cid:1)(cid:17)
¯gt−1(θ(cid:63)

−

wt
(cid:16)

(1

µ)(wtQ(At + Bt) + Dt−1),

≤

−

(37)

(38)

(40)

w2
t−1
w2
t

(44)

˜Dt

(1

µ) ˜Dt−1

+ u( ˜Dt, ˜Dt−1),

−

≤
where we deﬁne ˜Dt (cid:44) Dt
w2
t
gebraic details in Appendix B) that
u( ˜Dt, ˜Dt−1)
determistictic result of Lemma 1,
bounded, which combined with (40) allows to conclude.

is easy to show (see al-
the perturbation term
. Using the
is

o( ˜Dt + ˜Dt−1) if ˜Dt

this ensures that ˜Dt

→ ∞

. It

∈

2) Stochastic decrease rates: In the general case (I), the
inequalities (40), (41) and (42) holds, and (44) is replaced by

E[ ˜Dt

|Ft− 1

2

]

≤

(1

−

µ) ˜Dt−1

w2
t−1
w2
t

+ u( ˜Dt, ˜Dt−1),

(45)

Taking the expectation of this inequality and using Jensen
inequality, we show that (43) holds when replacing ˜Dt by
E[ ˜Dt]. This shows that E[Dt]
.
∞
The result follows from Lemma 1, that applies as
⊆
Ft− 1

t ) and thus E[Dt] <
t−1

2 ⊆ F

(w2

∈ O

F

t.

B. Convergence of SAMM — Proof of Proposition 3

We now proceed to prove the Proposition 3, that extends
the stochastic majorization-minimization framework to allow
approximations in both majorization and minimizations steps.

Proof of Proposition 3. We adapt the proof of Proposition 3.3
from [10] (reproduced as Proposition 2 in our work). Relaxing
tightness and majorizing hypotheseses introduces some extra
error terms in the derivations. Assumption (H) allows to
control these extra terms without breaking convergence. The
stability Lemma 3 is important in steps 3 and 5.

1) Almost sure convergence of (¯gt(θt)): We control the
positive expected variation of (gt(θt))t
is
a converging quasi-martingale. By construction of ¯gt and
ρ,L(ft, θt−1, (cid:15)t), where (cid:15)t
properties of the surrogates gt
is a non-negative sequence that meets (H),

to show that

∈ T

it

¯gt−1(θt−1)

¯gt(θt)
= (¯gt(θt)

−

−

wt(gt(θt−1)
wt(gt(θt−1)
+ wt( ¯ft−1(θt−1)
wt(ft(θt−1)

−

−

≤

≤

≤

−

¯gt(θt−1)) + wt(gt(θt−1)
¯gt−1(θt−1))
ft(θt−1)) + wt(ft(θt−1)

−

¯gt−1(θt−1))

¯ft−1(θt−1))

−

¯gt−1(θt−1))

−

¯ft−1(θt−1)) + wt(¯(cid:15)t−1 + (cid:15)t),

(46)

where the average error sequence (¯(cid:15)t)t is deﬁned recursively:
¯(cid:15)0 (cid:44) (cid:15)0 and ¯(cid:15)t (cid:44) (1
wt)(cid:15)t−1 +wt(cid:15)t. The ﬁrst inequality uses
¯gt(θt−1). To obtain the forth inequality we observe
¯gt(θt)
≤
ft(θt−1) < (cid:15)t by deﬁnition of (cid:15)t and ¯ft(θt−1)
gt(θt−1)
−
¯(cid:15)t, which can easily be shown by induction on t.
¯gt(θt−1)
t−1,
Then, taking the conditional expectation with respect to

−
≤

−

F

E[¯gt(θt)

−
wt sup
θ∈Θ |

≤

¯gt−1(θt−1)
f (θ)

|F
¯ft−1(θ)

t−1]

−

|

+ wt(¯(cid:15)t−1 + E[(cid:15)t

t−1]).

(47)

|F

We have used the fact that (cid:15)t−1 is deterministic with respect
t−1. To ensure convergence, we must bound both terms
to
in (47): the ﬁrst term is the same as in the original proof

F

(43)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

14

with exact surrogate, while the second is the perturbative
term introduced by the approximation sequence ((cid:15)t)t. We use
Lemma B.7 from [10], issued from the theory of empirical
processes: E[supθ∈Θ |
(wtt1/2), and thus
wtE[sup
f (θ)
θ∈Θ |

¯ft−1(θ)
] =
O
|
∞
(cid:88)
t1/2w2

−
¯ft−1(θ)
] < C
|

∞
(cid:88)

f (θ)

t <

(48)

∞

−

t=1

t=1

where C is a constant, as t1/2w2
t = t1/2−2u and u > 3/4
from (G). Let us now focus on the second term of (47).
Deﬁning, for all 1

t, wt

wj),

(cid:81)t

i

i = wi

≤

≤
t
(cid:88)

i=1

E[¯(cid:15)t] =

wt
i

E[(cid:15)t]

wt

≤

−

j=i+1(1
t
(cid:88)

E[(cid:15)t].

i=1

(49)

−

1)

η >

We set η > 0 so that 2(u
ensures E[(cid:15)t]
partial sum (cid:80)t

1. Assumption (H)
−
(t2(u−1)−η), which allows to bound the
∈ O
E[(cid:15)i]
i=1
wtE[¯(cid:15)t−1 + E[(cid:15)t
(cid:16) t
(cid:88)

−
(t2u−1−η). Therefore
t−1]] = wtE[(cid:15)t−1] + wtE[(cid:15)t]

∈ O

|F
(cid:17)
E[(cid:15)t]

+ wtE[(cid:15)t]

(50)

At2u−2u−1−η + Bt2u−u−2−η

Ct−1−η,

≤

where we use u < 1 on the third line and the deﬁnition of
(wt)t on the second line. Thus (cid:80)∞
t−1]] <
. We use quasi-martingale theory to conclude, as in [10]. We
∞
deﬁne the variable δt to be 1 if E[¯gt(θt)
¯gt−1(θt−1)
0, and 0 otherwise. As all terms of (47) are positive:
∞
(cid:88)

t=1 wtE[¯(cid:15)t−1+E[(cid:15)t

t−1]

|F

|F

≥

−

E[δt(¯gt(θt)

¯gt−1(θt−1))]

−

w2
t

i=1

≤

≤

t=1

=

∞
(cid:88)

t=1
∞
(cid:88)

E[δtE[¯gt(θt)

¯gt−1(θt−1)

t−1]]

−

|F

(51)

+ ¯(cid:15)t−1 + E[(cid:15)t

t−1]

] <
|

.

wtE[sup
θ∈Θ |

f (θ)

¯ft−1(θ)
|

−

t=1

|F

≤
∞
As ¯gt are bounded from below ( ¯ft is bounded from (D) and
we easily show that ¯(cid:15)t is bounded), we can apply Theorem
A.1 from [10], that is a quasi-martingale convergence theorem
originally found in [51]. It ensures that (gt(θt))t≥1 converges
almost surely to an integrable random variable g(cid:63), and that
(cid:80)∞

t=1
|F
2) Almost sure convergence of ¯f (θt): We rewrite the second

E[¯gt(θt)
E[
|

almost surely.

¯gt−1(θt−1)

] <
|

t−1]

∞

−

inequality of (46), adding ¯(cid:15)t on both sides:
(cid:1)
(cid:0)ft(θt−1)

¯ft−1(θt−1) + ¯(cid:15)t−1

0

−
ft(θt−1)(cid:1) + wt
¯gt(θt)(cid:1) + wt¯(cid:15)t−1
−
¯ft−1(θt−1)(cid:1) + (cid:0)¯gt−1(θt−1)

−

≤

(cid:0)¯gt−1(θt−1)
wt
(cid:0)gt(θt−1)
wt
≤
−
+ (cid:0)¯gt−1(θt−1)
(cid:0)ft(θt−1)
wt
+ wt((cid:15)t + ¯(cid:15)t−1),

−

≤

¯ft−1(θt−1)(cid:1)

¯gt(θt)(cid:1)

−

(52)

where the left side bound has been obtained in the last
paragraph by induction and the right side bound arises from
the deﬁnition of (cid:15)t. Taking the expectation of (52) conditioned
on

t−1, almost surely,

F

0

≤

−

wt(f (θt−1)
E[¯gt(θt)

¯ft−1(θt−1))
−
¯gt−1(θt−1)

−

|F

t−1] + wt(¯(cid:15)t−1 + E[(cid:15)t

(53)
t−1]),

|F

t−1]
|

|F
(cid:0)f (θt−1)

We separately study the three terms of the previous upper
bound. The ﬁrst two terms can undergo the same analysis as
E(cid:2)
in [10]. First, almost sure convergence of (cid:80)∞
E[¯gt(θt)
t=1
−
|
(cid:3) implies that E(cid:2)¯gt(θt)
(cid:3)
¯gt−1(θt−1)
¯gt−1(θt−1)
t−1
is the summand of an almost surely converging sum. Second,
¯ft−1(θt−1)(cid:1) is the summand of an absolutely
wt
converging sum with probability one, less it would contra-
dict (48). To bound the third term, we have once more
the perturbation introduced by ((cid:15)t)t. We have
to control
(cid:80)∞
almost surely, otherwise

t=1 wt¯(cid:15)t−1 + wtE[(cid:15)t

t−1] <

|F

−

−

Fubini’s theorem would invalidate (50).

|F

∞

As the three terms are the summand of absolutely converg-
¯ft−1(θt−1)+¯(cid:15)t−1)
ing sums, the positive term wt(¯gt−1(θt−1)
is the summand of an almost surely convergent sum. This is
not enough to prove that ¯ht(θt) (cid:44) ¯gt(θt)
∞ 0, hence
we follow [10] and make use of its Lemma A.6. We deﬁne
Xt (cid:44) ¯ht−1(θt−1) + ¯(cid:15)t−1. As (H) holds, we use Lemma 3,
which ensures that (¯ht)t≥1 are uniformly R(cid:48)-Lipschitz and
θt
(cid:107)

−
¯ft(θt)

θt−1

2 =

→

O

−

−
Xt+1
|
R(cid:48)

≤

≤ O

(cid:107)
Xt

−
θt
(cid:107)
−
(wt) +

| ≤ |
θt−1
¯(cid:15)t
|

−

(wt). Hence,
¯ht(θt)
−
¯(cid:15)t
2 +
|
(cid:107)
,
¯(cid:15)t−1

−

|

¯ht−1(θt−1)
¯(cid:15)t−1
,
|
θt
as
(cid:107)

−

+

¯(cid:15)t

¯(cid:15)t−1

|

|

−

|
as ¯ht is R(cid:48)-Lipschitz
θt−1

(wt)

2 =
(cid:107)

O

(54)

From assumption (H), ((cid:15)t)t and (¯(cid:15)t)t are bounded. Therefore
)
¯(cid:15)t

(wt) and hence

¯(cid:15)t−1

¯(cid:15)t−1

+

(cid:15)t

|

−

wt(
|

| ≤

|
|
Xt+1
|

∈ O

|
Xt

−

| ≤ O

(wt).

(55)

Lemma A.6 from [10] then ensures that Xt converges
to zero with probability one. Assumption (H) ensures that
∞ 0 almost surely, from which we can easily deduce
(cid:15)t
∞ 0 almost surely. Therefore ¯ht(θt)
0 with probability
¯(cid:15)t
one and ( ¯ft(θt))t≥1 converges almost surely to g(cid:63).

→
→
3) Almost sure convergence of ¯f (θt): Lemma B.7 of [10],
based on empirical process theory [33], ensures that ¯ft uni-
formly converges to ¯f . Therefore, ( ¯f (θt))t≥1 converges almost
surely to g(cid:63).

→

4) Asymptotic stationary point condition: Preliminary to
the ﬁnal result, we establish the asymptotic stationary point
condition (57) as in [10]. This requires to adapt the original
proof to take into account the errors in surrogate computation
¯ht is L-
and minimization. We set α > 0. By deﬁnition,
Lipschitz over RK. Following the same computation as in (34),
we obtain, for all α > 0,

∇

≤

2
(cid:107)

2
α

¯(cid:15)t +

¯ht(θt)

(cid:107)∇
¯ht(θ)
where we use
|
the inequality (56) is true for all α,
surely. From the strong convexity of ¯gt and Lemma 3,
θ(cid:63)
t (cid:107)

2 converges to zero, which ensures

Lα
2
RK. As ¯(cid:15)t
∈
¯ht(θt)
2
(cid:107)
(cid:107)∇

¯(cid:15)t for all θ

| ≤

→

,

0 and
∞ 0 almost

→

θt

(cid:107)

−

(56)

¯ht(θ(cid:63)
t )

2
(cid:107)

¯
∇

ht(θt)

θt
2 + L
(cid:107)
(cid:107)

θ(cid:63)
2
t (cid:107)

−

≤ (cid:107)

(cid:107)∇
5) Parametrized surrogates: We use assumption (F) to
ﬁnally prove the property, adapting the proof of Proposition 3.4
in [10]. We ﬁrst recall the derivations of [10] for obtaining (58)
We deﬁne (κt)t such that ¯gt = gκt for all t > 0. We assume
that θ∞ is a limit point of (θt)t. As Θ is compact, there

→

∞ 0.

(57)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

15

K

exists an increasing sequence (tk)k such that (θtk )k converges
toward θ∞. As
is compact, a converging subsequence of
(κtk )k can be extracted, that converges towards κ∞
.
∈ K
From the sake of simplicity, we drop subindices and assume
κ∞.
without loss of generality that θt
From the compact parametrization assumption, we easily show
that (¯gκt)t uniformly converges towards ¯g∞ (cid:44) ¯gκ∞. Then,
deﬁning ¯h∞ = ¯g∞

¯f , for all θ

θ∞ and κt

Θ,

→

→

¯h∞(θ∞, θ

− ∇

−

θ∞)
(58)
Θ. We
∈
θ(cid:63)
2
t (cid:107)
−

¯f (θ∞, θ

∇

−

θ∞) =

¯g∞(θ∞, θ

θ∞)

−

∇
¯f (θ∞, θ

∈

−

≥

−

∇

t →

We ﬁrst show that
consider the sequence (θ(cid:63)
0, which implies θ(cid:63)
¯g∞, which implies (¯gt(θ(cid:63)
minimizes ¯gt, for all t > 0 and θ
This implies ¯g∞(θ∞)
for t

θ∞)
0 for all θ
t )t. From Lemma 3,
θt
→
(cid:107)
θ∞. ¯gt converges uniformly towards
¯g∞(θ∞). Furthermore, as θ(cid:63)
t ))t →
t
Θ, ¯gt(θ(cid:63)
¯gt(θ).
t )
inf θ∈Θ ¯g∞(θ) by taking the limit
. Therefore θ∞ is the minimizer of ¯g∞ and thus

∇
around θ(cid:63)

→ ∞
¯g∞(θ∞, θ
Adapting [10], we perform the ﬁrst-order expansion of ¯ht
t (instead of θt in the original proof) and show that
0

θ∞) = 0, as ¯ht differentiable,
θ∞. This is sufﬁcient to conclude.

∇
and θ(cid:63)

¯ht(θ(cid:63)
t )

θ∞)

(cid:107)∇

2
(cid:107)

→

≤

−

≥

−

≤

0.

∈

¯h∞(θ∞, θ
t →

C. Convergence of SOMF — Proof of Proposition 1
Proof of Proposition 1. From assumption (D), (xt)t
is (cid:96)2-
bounded by a constant X. With assumption (A), it implies
that (αt)t is (cid:96)2-bounded by a constant A. This is enough to
show that (gt)t and (θt)t meet basic assumptions (C)–(F).
Assumption (G) immediately implies (B). It remains to show
that (gt)t and (θt)t meet the assumptions (H) and (I). This will
allow to cast SOMF as an instance of SAMM and conclude.
1) The computation of Dt veriﬁes (I): We deﬁne D(cid:63)
t =
argminD∈C ¯gt(D). We show that performing subsampled
block coordinate descent on ¯gt is sufﬁcient to meet assump-
tion (I), where θt = Dt. We separately analyse the exceptional
case where no subsampling is done and the general case.

First, with small but non-zero probability, Mt = Ip
and Alg. 4 performs a single pass of simple block coordi-
nate descent on ¯gt. In this case, as ¯gt is strongly convex
from (A), [52, 37] ensures that the sub-optimality decreases
at least of factor 1
µ with a single pass of block coordinate
descent, where µ > 0 is a constant independent of t. We
provide an explicit µ in Appendix B.

−

In the general case, the function value decreases determin-
¯gt(Dt−1). As
≤
¯gt(Dt−1). Fur-
≤
t ) are deterministic with respect
= Ip] = ¯gt(D(cid:63)
t ).
the sub-optimality

istically at each minimization step: ¯gt(Dt)
a consequence, E[¯gt(Dt)
= Ip]
|Ft− 1
thermore, ¯gt and hence ¯gt(D(cid:63)
, which implies E[¯gt(D(cid:63)
t )
to
Deﬁning d (cid:44) P[Mt = Ip], we split
expectation and combine the analysis of both cases:

|Ft− 1

Ft− 1

, Mt

, Mt

2

2

2

E[¯gt(Dt)
= dE[¯gt(Dt)

, Mt = Ip]

]

2

−

¯gt(D(cid:63)
t )
|Ft− 1
¯gt(D(cid:63)
t )
−
d)E[¯gt(Dt)
µ) + (1
−
dµ(cid:1)(¯gt(Dt−1)

−

2

|Ft− 1
¯gt(D(cid:63)
t )
−
d)(cid:1)(¯gt(Dt−1)
¯gt(D(cid:63)
t )).

|Ft− 1
−

2

−

+ (1
−
(cid:0)d(1

≤
= (cid:0)1

−

= Ip]

, Mt
¯gt(D(cid:63)

t ))

(59)

2) The surrogates (gt)t verify (H): We deﬁne g(cid:63)
t ∈
ρ,L(ft, Dt−1) the surrogate used in OMF at iteration t, which
S
depends on the exact computation of α(cid:63)
t , while the surrogate
gt used in SOMF relies on approximated αt. Formally, using
the loss function (cid:96)(α, G, β) (cid:44) 1
α(cid:62)β + λΩ(α), we
recall the deﬁnitions

2 α(cid:62)Gα

−

α(cid:63)
t

(cid:44) argmin
α∈Rk
t (D) (cid:44) (cid:96)(α(cid:63)
g(cid:63)

t , β(cid:63)

(cid:96)(α, G(cid:63)

t ), αt(cid:44) argmin
α∈Rk
t , D(cid:62)D, D(cid:62)xt), gt(D) (cid:44) (cid:96)(αt, D(cid:62)D, D(cid:62)xt).

(cid:96)(α, Gt, βt),

(60)

gt

t , β(cid:63)

g(cid:63)
t −
(cid:107)

The matrices G(cid:63)
t are deﬁned in (21) and Gt, βt in either
the update rules (b) or (c). We deﬁne (cid:15)t (cid:44)
∞ to be
(cid:107)
the (cid:96)∞ difference between the approximate surrogate of SOMF
and the exact surrogate of OMF, as illustrated in Figure 2. By
ρ,L(ft, θt−1, (cid:15)t). We ﬁrst show that (cid:15)t can be
deﬁnition, gt
bounded by the Froebenius distance between the approximate
parameters Gt, βt and the exact parameters G(cid:63)
t . Using
Cauchy-Schwartz inequality, we ﬁrst show that there exists a
constant C (cid:48) > 0 such that for all D

t , β(cid:63)

∈ T

,

gt(D)
|

−

g(cid:63)
t (D)

C (cid:48)

| ≤

Then, we show that the distance

bounded: there exists C (cid:48)(cid:48) > 0 constant such that

∈ C
αt
(cid:107)

αt
(cid:107)

2.

(61)

α∗
t (cid:107)
α∗
t (cid:107)2 can itself be

−

−

αt

(cid:107)

−

α(cid:63)

t (cid:107)2 ≤

C (cid:48)(cid:48)(

G(cid:63)
(cid:107)

t −

Gt

(cid:107)F +

β(cid:63)
(cid:107)

t −

βt(cid:107)2).

(62)

We combine both equations and take the supremum over
D

, yielding

∈ C

(cid:15)t

C(
(cid:107)

≤

G(cid:63)

t −

Gt

(cid:107)F +

(cid:107)

β(cid:63)

t −

βt(cid:107)2),

(63)

where C is constant. Detailed derivation of (61) to (63) relies
on assumption (A) and are reported in Appendix B.

In a second step, we show that

βt(cid:107)
2
vanish almost surely, sufﬁciently fast. We focus on bounding
βt −
2 when the
(cid:107)
update rules (b) are used. For t > 0, we write i (cid:44) it. Then
(cid:88)

2 and proceed similarly for

F and
(cid:107)

G(cid:63)
t (cid:107)

β(cid:63)
t (cid:107)

t −

t −

Gt

Gt

−

(cid:107)

G(cid:63)
(cid:107)

β(cid:63)
(cid:107)

βt

(cid:44) β(i)

t =

γ(i)
s,tD(cid:62)

s−1Msx(i),

where γ(i)
(cid:12)
(cid:8)s
(cid:12)

s,t = γc(i)
t, xs = x(i)(cid:9)(cid:12)

t

≤
βt −

β(cid:63)

t =

s≤t,xs=x(i)

(cid:81)

s<t,xs=x(i)(1

γc(i)

s

−

) and c(i)
t =
t as

β(cid:63)

(cid:12). We can then decompose βt −
(cid:88)
γ(i)
s,t(Ds−1

Dt−1)(cid:62)Msx(i)

−

s≤t,xs=xt=x(i)
+ D(cid:62)

(cid:16) (cid:88)

t−1

s≤t,xs=xi)

γ(i)
s,tMs

−

(cid:17)

I

x(i).

(64)

The latter equation is composed of two terms: the ﬁrst one
captures the approximation made by using old dictionaries
in the computation of (βt)t, while the second captures how
the masking effect is averaged out as the number of epochs
increases. Assumption (B) allows to bound both terms at the
v(cid:1) > 0, a te-
same time. Setting η (cid:44) 1
2)
−
β(cid:63)
dious but elementary derivation indeed shows E[
βt−
t (cid:107)
(cid:107)

∈
0 almost surely — see Appendix B.
O
The SOMF algorithm therefore meets assumption (H) and is a
convergent SAMM algorithm. Proposition 1 follows.

(t2(u−1)−η) and (cid:15)t

2 min (cid:0)v

3
4 , (3u

→

−

−

2]

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

16

REFERENCES
[1] J. Mairal, “Sparse Modeling for Image and Vision Processing,” Foundations and
Trends in Computer Graphics and Vision, vol. 8, no. 2-3, pp. 85–283, 2014.
[2] N. Srebro, J. Rennie, and T. S. Jaakkola, “Maximum-margin matrix factorization,”
in Advances in Neural Information Processing Systems, 2004, pp. 1329–1336.
[3] E. J. Cand`es and B. Recht, “Exact matrix completion via convex optimization,”
Foundations of Computational Mathematics, vol. 9, no. 6, pp. 717–772, 2009.
[4] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global Vectors for Word

Representation.” in Proc. Conf. EMNLP, vol. 14, 2014, pp. 1532–43.

[5] O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix factoriza-
tion,” in Advances in Neural Information Processing Systems, 2014, pp. 2177–2185.
[6] Y. Zhang, M. Roughan, W. Willinger, and L. Qiu, “Spatio-Temporal Compressive

Sensing and Internet Trafﬁc Matrices,” 2009.

[7] H. Kim and H. Park, “Sparse non-negative matrix factorizations via alternating non-
negativity-constrained least squares for microarray data analysis,” Bioinformatics,
vol. 23, no. 12, pp. 1495–1502, 2007.

[8] G. Varoquaux et al., “Multi-subject dictionary learning to segment an atlas of brain

spontaneous activity,” in Proc. IPMI Conf., 2011, pp. 562–573.

[9] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” in

Proceedings of COMPSTAT, 2010, pp. 177–186.

[10] J. Mairal, “Stochastic majorization-minimization algorithms for large-scale opti-

mization,” in Adv. Neural Inform. Process. Syst., 2013, pp. 2283–2291.

[11] M. Razaviyayn, M. Hong, and Z.-Q. Luo, “A uniﬁed convergence analysis of block
successive minimization methods for nonsmooth optimization,” SIAM Journal on
Optimization, vol. 23, no. 2, pp. 1126–1153, 2013.

[12] S. Burer and R. D. C. Monteiro, “Local Minima and Convergence in Low-Rank
Semideﬁnite Programming,” Math. Program., vol. 103, no. 3, pp. 427–444, 2004.
[13] B. Recht and C. R´e, “Parallel stochastic gradient algorithms for large-scale matrix

completion,” Math. Program. Comput., vol. 5, no. 2, pp. 201–226, 2013.

[14] R. M. Bell and Y. Koren, “Lessons from the Netﬂix prize challenge,” ACM SIGKDD

Explorations Newsletter, vol. 9, no. 2, pp. 75–79, 2007.

[15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for matrix factorization
and sparse coding,” J. Machine Learning Research, vol. 11, pp. 19–60, 2010.
[16] W. B. Johnson and J. Lindenstrauss, “Extensions of Lipschitz mappings into a
Hilbert space,” Contemporary mathematics, vol. 26, no. 189-206, p. 1, 1984.
[17] E. Bingham and H. Mannila, “Random projection in dimensionality reduction:
Applications to image and text data,” in Proc. SIGKDD Conf., 2001, pp. 245–250.
[18] M. J. McKeown et al., “Analysis of fMRI Data by Blind Separation into Indepen-
dent Spatial Components,” Hum. Brain Mapp., vol. 6, no. 3, pp. 160–188, 1998.
[19] E. J. Cand`es and T. Tao, “Near-optimal signal recovery from random projections:
Universal encoding strategies?” IEEE Transactions on Information Theory, vol. 52,
no. 12, pp. 5406–5425, 2006.

[20] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions,”
SIAM review, vol. 53, no. 2, pp. 217–288, 2011.

[21] V. Rokhlin et al., “A randomized algorithm for principal component analysis,”

SIAM J. Matrix Anal. Appl., vol. 31, no. 3, pp. 1100–1124, 2009.

[22] T. Sarlos, “Improved approximation algorithms for large matrices via random

projections,” in Proc. IEEE Symp. Found. Comput. Science, 2006, pp. 143–152.

[23] Y. Lu et al., “Faster ridge regression via the subsampled randomized hadamard

transform,” in Adv. Neural Inform. Process. Syst., 2013, pp. 369–377.

[24] M. Pilanci and M. Wainwright, “Iterative hessian sketch: Fast and accurate solution
approximation for constrained least-squares,” JMLR, vol. 17, pp. 1–33, 2015.
[25] G. Raskutti and M. Mahoney, “Statistical and algorithmic perspectives on random-
ized sketching for ordinary least-squares,” in Proc. ICML, 2015, pp. 617–625.
[26] A. Mensch, J. Mairal, B. Thirion, and G. Varoquaux, “Dictionary learning for

massive matrix factorization,” in Proc. ICML, 2016, pp. 1737–1746.

[27] B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete basis set:
A strategy employed by V1?” Vision Res., vol. 37, no. 23, pp. 3311–3325, 1997.
[28] R. Tibshirani, “Regression shrinkage and selection via the lasso,” J. R. Stat. Soc.

Series B Stat. Methodol., vol. 58, no. 1, pp. 267–288, 1996.

[29] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” J.

Comput. Graph. Stat., vol. 15, no. 2, pp. 265–286, 2006.

[30] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,”
J. R. Stat. Soc. Series B Stat. Methodol., vol. 67, no. 2, pp. 301–320, 2005.
[31] P. O. Hoyer, “Non-negative matrix factorization with sparseness constraints,”

Journal of Machine Learning Research, vol. 5, pp. 1457–1469, 2004.

[32] L. Bottou, F. E. Curtis, and J. Nocedal, “Optimization Methods for Large-Scale

Machine Learning,” arXiv:1606.04838v2 [stat.ML], 2016.
[33] A. W. Van der Vaart, Asymptotic Statistics. CUP, 2000, vol. 3.
[34] M. Mardani et al., “Subspace Learning and Imputation for Streaming Big Data

Matrices and Tensors,” IEEE TSP, vol. 63, no. 10, pp. 2663–2677, 2015.

[35] J. M. Borwein and A. S. Lewis, Convex Analysis and Nonlinear Optimization:

Theory and Examples. Springer Science & Business Media, 2010.

[36] J. Mairal, “Optimization with ﬁrst-order surrogate functions,” in Proceedings of the

International Conference on Machine Learning, 2013, pp. 783–791.

[37] S. J. Wright, “Coordinate descent algorithms,” Mathematical Programming, vol.

[38] D. C. Van Essen et al., “The WU-Minn Human Connectome Project: An overview,”

151, no. 1, pp. 3–34, 2015.

NeuroImage, vol. 80, pp. 62–79, 2013.

[39] M. P. Milham et al., “The adhd-200 consortium: a model to advance the transla-
tional potential of neuroimaging in clinical neuroscience,” Front. Syst. Neurosci.,
vol. 6, no. 62, 2012.

[40] G. Varoquaux, Y. Schwartz, P. Pinel, and B. Thirion, “Cohort-level brain mapping:
Learning cognitive atoms to single out specialized regions,” in Proceedings of the
Information Processing in Medical Imaging Conference, 2013, pp. 438–449.

[41] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral image classiﬁcation
using dictionary-based sparse representation,” IEEE Transactions on Geoscience
and Remote Sensing, vol. 49, no. 10, pp. 3973–3985, 2011.

[42] A. Soltani-Farani, H. R. Rabiee, and S. A. Hosseini, “Spatial-Aware Dictionary
Learning for Hyperspectral Image Classiﬁcation,” IEEE Transactions on Geo-
science and Remote Sensing, vol. 53, no. 1, pp. 527–541, 2015.

[43] M. Maggioni, V. Katkovnik, K. Egiazarian, and A. Foi, “Nonlocal transform-
domain ﬁlter for volumetric data denoising and reconstruction,” IEEE Trans. Image
Process., vol. 22, no. 1, pp. 119–133, 2013.

[44] Y. Peng et al., “Decomposable nonlocal tensor dictionary learning for multispectral

image denoising,” in Proc. IEEE Conf. CVPR, 2014, pp. 2949–2956.

[45] G. Vane, “First results from the airborne visible/infrared imaging spectrometer

(AVIRIS),” in Ann. Tech. Symp. Int. Soc. Optics Photonics, 1987, pp. 166–175.

[46] S. Behnel et al., “Cython: The best of both worlds,” Computing in Science &

Engineering, vol. 13, no. 2, pp. 31–39, 2011.

[47] J. Friedman, T. Hastie, H. H¨oﬂing, and R. Tibshirani, “Pathwise coordinate
optimization,” The Annals of Applied Statistics, vol. 1, no. 2, pp. 302–332, 2007.
[48] F. Pedregosa et al., “Scikit-learn: Machine learning in Python,” Journal of Machine

Learning Research, vol. 12, pp. 2825–2830, 2011.

[49] A. Abraham et al., “Machine learning for neuroimaging with scikit-learn,” Frontiers

in Neuroinformatics, vol. 8, no. 14, 2014.

[50] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra, “Efﬁcient projections onto
the l1-ball for learning in high dimensions,” in Proceedings of the International
Conference on Machine Learning, 2008, pp. 272–279.

[51] M. M´etivier, Semimartingales: A Course on Stochastic Processes. Walter de

Gruyter, 1982, vol. 2.

[52] A. Beck and L. Tetruashvili, “On the convergence of block coordinate descent type

methods,” SIAM Journal on Optimization, vol. 23, no. 4, pp. 2037–2060, 2013.

[53] J. L. Doob, Stochastic processes.

John Wiley & Sons, 1990.

Arthur Mensch is a PhD candidate at Universit´e
Paris-Saclay and Inria. His main research interests
are related to large-scale stochastic optimization
and statistical learning, with speciﬁc applications to
functional neuroimaging and cognitive brain map-
ping. In 2015, he received a graduate degree from
Ecole Polytechnique, France, and a MSc degree in
applied mathematics from ´Ecole Normale Sup´erieure
de Cachan, France.

Julien Mairal is a research scientist at Inria. He
received a graduate degree from Ecole Polytech-
nique, France, in 2005, and a PhD degree from Ecole
Normale Superieure, Cachan, France, in 2010. Then,
he was a postdoctoral researcher at the statistics
department of UC Berkeley, before joining Inria in
2012. His research interests include machine learn-
ing, computer vision, mathematical optimiza- tion,
and statistical image and signal processing. In 2016,
he received a Starting Grant from the European
Research Council (ERC).

Ga¨el Varoquaux is a tenured computer-science
researcher at Inria. His research develops statistical-
learning tools for functional neuroimaging data with
application to cognitive mapping of the brain as
well as the study of brain pathologies. He is also
heavily invested in software development for data
science, as project-lead for scikit-learn, one of the
reference machine-learning toolboxes, and on joblib,
Mayavi, and nilearn. Varoquaux has a PhD in quan-
tum physics and is a graduate from Ecole Normale
Superieure, Paris.

Bertrand Thirion is the principal investigator of the
Parietal team (Inria-CEA) within the main French
Neuroimaging center, Neurospin. His main research
interests are related to the use of machine learning
and statistical analysis techniques for neuroimaging,
e.g. the modeling of brain variability in group stud-
ies, the mathematical study of functional connectiv-
ity and brain activity decoding; he addresses various
applications such as the study of vision through
neuroimaging and the classiﬁcation of brain images
for diagnosis or brain mapping

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

17

APPENDIX B
ALGEBRAIC DETAILS

A. Proof of Lemma 1

Proof. We ﬁrst focus on the deterministic case. Assume that (xt)t is not bounded. Then there exists a subsequence of (xt)t
that diverges towards +
and for all (cid:15) > 0,
∞
using the asymptotic bounds on u, there exists t1

. We assume without loss of generality that (xt)t → ∞

. Then, xt + xt−1

t0 such that

→ ∞

≥

t
∀

≥

t1, xt

and therefore xt

αxt−1 + (cid:15)(xt + xt−1)
α + (cid:15)
(cid:15)
1

xt−1.

≤

≤

−

Setting (cid:15) small enough, we obtain that xt is bounded by a geometrically decreasing sequence after t1, and converges to 0,
which contradicts our hypothesis. This is enough to conclude.

t−1

diverges to +

In the random case, we consider a realization of (Xt)t that is not bounded, and assumes without loss of generality that it
βXt−1, where
βXt−1, as Xt−1 is deterministic conditioned
, Doob’s forward convergence lemma on
cannot happen

. Following the reasoning above, there exists β < 1, t1 > 0, such that for all t > t1, E[Xt
t. Taking the expectation conditioned on

⊆ F
F
t−1. Therefore Xt is a supermartingale beyond a certain time. As E[Xt] <
on
discrete martingales [53] ensures that (Xt)t converges almost surely. Therefore the event
on a set with non-zero probability, less it would lead to a contradiction. The lemma follows.

(Xt)t is not bounded
}
{

t−1, E[Xt

∞
⊆ F

t−1]

∞

|F

|F

t(cid:48)]

≤

≤

F

F

t(cid:48)

B. Taylor’s inequality for L-Lipschitz continuous functions

This inequality is useful in the demonstration of Lemma 2 and Proposition 3. Let f : Θ

L-Lipschitz gradient. That is, for all θ, θ(cid:48)

∈
f (θ(cid:48))

Θ,

f (θ)

(cid:107)∇

f (θ) +

∇

≤

f (θ(cid:48))

− ∇
f (θ)(cid:62)(θ(cid:48)

(cid:107)2 ≤

L

θ
(cid:107)

θ) +

−

⊂

RK
→
(cid:107)2. Then, for all θ, θ(cid:48)
∈
θ(cid:48)

2
2.
(cid:107)

θ(cid:48)

−
L
θ
2 (cid:107)

−

R be a function with
Θ,

(66)

C. Lemma 3: Detailed control of Dt in (44)

Injecting (40) and (42) in (43), we obtain

˜Dt

(1

≤

−

µ) ˜Dt−1

w2
t−1
w2
t

+ u( ˜Dt, ˜Dt−1), where u( ˜Dt, ˜Dt−1) (cid:44) (1

µ) ˜Q

3( ˜Dt + ˜Dt−1

) + ˜Q +

˜Dt

.

(67)

(cid:18)(cid:115)

−

(cid:113)

(cid:19)

w2
t−1
w2
t

From assumption (G),
Using the determistictic result of Lemma 1, this ensures that ˜Dt is bounded.

1, and we have, from elementary comparisons, that u( ˜Dt, ˜Dt−1)

∈

w2
t−1
w2
t →

o( ˜Dt + ˜Dt−1) if Dt

.
→ ∞

D. Detailed derivations in the proof of Proposition 1

Let us ﬁrst exhibit a scaler µ > 0 independent of t, for which (I) is met
1) Geometric rate for single pass subsampled block coordinate descent: . For D(j)

Rp×k any matrix with non-zero j-th

column d(j) and zero elsewhere

¯gt(D + D(j))

∇

− ∇

∈
¯gt(D + D(j)) = ¯Ct[j, j]d(j)

¯gt has coordinate Lipschitz constant Lmax (cid:44) max0≤j<k ¯Ct[j, j]

and hence ¯gt gradient has component Lipschitz constant Lj = ¯Ct[j, j] for component j, as already noted in [15]. Using [37]
A2, as (αt)t is
terminology,
√kLmax. Moreover,
bounded from (A). As a consequence, ¯gt gradient is also L-Lipschitz continuous, where [37] note that L
¯gt is strongly convex with strong convexity modulus ρ > 0 by hypothesis (A). Then, [52] ensures that after one cycle over the
k blocks

maxt>0,0≤j<k αt[j]2

∇

≤

≤

≤

E[¯gt(Dt)

¯gt(D(cid:63)
t )

t−1, Mt = Ip]

−

|F

(cid:0)1

(cid:0)1

−

−

≤

≤

(cid:1)(¯gt(Dt−1)

2Lmax(1 + kL2/L2
µ(cid:1)(¯gt(Dt−1)

¯gt(D(cid:63)

max)

−
t )) where µ (cid:44)

ρ

−

¯gt(D(cid:63)

t ))
ρ
2A2(1 + k2)

2) Controling (cid:15)t from (Gt, βt), (G(cid:63)

is met in the proof of SOMF convergence. We ﬁrst show that (αt)t is bounded. We choose D > 0 such that
for all j
condition, for all t > 0,

t ) — Equations 61–62: We detail the derivations that are required to show that (H)
D
. From assumption (A), using the second-order growth

, and X such that

X for all x

[k] and D

d(j)
(cid:107)

(cid:107)2 ≤

(cid:107)2 ≤

x
(cid:107)

∈ X

∈ C

∈

t , β(cid:63)

(65)

(68)

(69)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

We have successively used the fact that Ω(0) = 0, Ω(αt)
induction on the number of epochs. For all t > 0, from the deﬁnition of αt and α(cid:63)

βt(cid:107)2 ≤
(cid:107)

0, and

≥

√krDX, which can be shown by a simple

λΩ(0)

t Gtαt

α(cid:62)

t βt + λΩ(αt)

ρ
2 (cid:107)
1
2

2
2 ≤

αt

0
−
(cid:107)
t Gtαt

α(cid:62)

0 +

(cid:107)

≤

ρ
2 (cid:107)

αt

2
2 +
(cid:107)

−
αt

(

α(cid:62)

1
2
βt(cid:107)2,
αt
(cid:107)

(cid:107)2,

(cid:107)2(cid:107)

−

hence

αt

ρ
(cid:107)

2
2 ≤

(cid:107)

√krDX

and therefore

αt
(cid:107)

(cid:107)2 ≤

√krDX
ρ

(cid:44) A.

gt(D)

|

g(cid:63)
t (D)
|

−

=

t −

Tr D(cid:62)D(αtα(cid:62)

(cid:12)
1
(cid:12)
(cid:12)
2
1
t −
2 (cid:107)
(kD2A + √kDX)
αt
(cid:107)

αtα(cid:62)

D(cid:62)D

(cid:107)F (cid:107)

α(cid:63)

t α(cid:63)
t

(cid:62))

α(cid:63)

(cid:62)

t α(cid:63)
t
α(cid:63)
t (cid:107)2,

−

−
(cid:107)F +

≤

≤

t , for all D

:

∈ C

(αt

−

α(cid:63)

t )(cid:62)D(cid:62)xt

(cid:12)
(cid:12)
(cid:12)

D

(cid:107)

(cid:107)F (cid:107)

xt

(cid:107)2(cid:107)

αt

−

α(cid:63)

t (cid:107)2

where we use Cauchy-Schwartz inequality and elementary bounds on the Froebenius norm for the ﬁrst inequality, and use
αt, α(cid:63)
[k] to obtain the second inequality, which is (61) in the main text.
α(cid:63)
t (cid:107)2. We adapt the proof of Lemma B.6 from [36], that states the lipschitz continuity of the

αt
(cid:107)
minimizers of some parametrized functions. By deﬁnition,

A, xt
We now turn to control

X for all t > 0 and d(j)

D for all j

t ≤

≤

−

≤

∈

α(cid:63)

t = argmin
α∈Rk

(cid:96)(α, G(cid:63)

t , β(cid:63)
t )

αt = argmin
α∈Rk

(cid:96)(α, Gt, βt),

Assumption (A) ensures that Gt

ρIk, therefore we can write the second-order growth condition

(cid:31)

αt

αt

ρ
2 (cid:107)
ρ
2 (cid:107)
ρ

αt
(cid:107)

2
α(cid:63)
t (cid:107)
2 ≤
2
α(cid:63)
t (cid:107)
2 ≤
2
α(cid:63)
t (cid:107)
2 ≤

−

−

−

(cid:96)(αt, G(cid:63)

t , β(cid:63)
t )

(cid:96)(αt, Gt, βt)

−

(cid:96)(α(cid:63)

t , Gt, βt)
p(α(cid:63)

(cid:96)(α(cid:63)

t , G(cid:63)

t , β(cid:63)

t ),
and therefore
−
t ), where p(α) (cid:44) (cid:96)(α, Gt, βt)

p(αt)

−

−
Rk such that

(cid:96)(αt, G(cid:63)

t , β(cid:63)

t ).

α
(cid:107)

(cid:107)2 ≤

A,

p takes a simple form and can differentiated with respect to α. For all α

Therefore p is L-Lipschitz on the ball of size A where αt and α(cid:63)

p(α) =

1
2

α(cid:62)(Gt
G(cid:63)

A

−
Gt
(cid:107)

−

p(α) = (Gt

∇
p(α)

(cid:107)∇

(cid:107)2 ≤

−
t )α
G(cid:63)

G(cid:63)

∈
α(cid:62)(βt −
t )α
−
β(cid:63)
(βt −
t )
β(cid:63)
βt −
(cid:107)
t live, and

t (cid:107)2

−
t (cid:107)F +

β(cid:63)
t )

(cid:44) L

αt

ρ
(cid:107)

αt

(cid:107)

−

−

2
α(cid:63)
t (cid:107)
2 ≤
α(cid:63)

t (cid:107)2 ≤

L
(cid:107)
A
ρ (cid:107)

αt

−

Gt

−

α(cid:63)

t (cid:107)2
G(cid:63)
t (cid:107)F +

1
ρ (cid:107)

βt −

β(cid:63)
t (cid:107)2,

which is (62) in the main text. The bound (63) on (cid:15)t immediately follows.

3) Bounding

C are positive constants independent of t and we introduce the terms

βt −
(cid:107)

β(cid:63)
t (cid:107)2 in equation (64): Taking the (cid:96)2 norm in (64), we have

βt −
(cid:107)

β(cid:63)

t (cid:107)2 ≤

BLt + CRt, where B and

Lt (cid:44) (cid:88)

s≤t,xs=xt=x(i)

γ(i)
s,t(cid:107)

Ds−1

Dt−1

−

(cid:107)F ,

Rt (cid:44)

(cid:0) (cid:80)

s≤t,xs=x(i) γ(i)

s,tMs

(cid:1)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
I
(cid:13)F

.

−

{
∈

a) Conditioning on the sequence of drawn indices: We recall that (it)t is the sequence of indices that are used to draw
}i, namely such that xt = x(it). (it)t is a sequence of i.i.d random variables, whose law is uniform in [1, n].
b )b>0 that record the iterations at which sample (i) is drawn, i.e. such
t > 0 is the integer that counts the number of time sample (i) has
. These notations will help us understanding the behavior of (Lt)t
}

x(i)
(xt)t from
[n], we deﬁne the increasing sequence (t(i)
For each i
that itb = i for all b > 0. For t > 0, we recall that c(i)
appeared in the algorithm, i.e. c(i)
t
b ≤
and (Rt)t.

b > 0, t(i)
{

t = max

18

(70)

(71)

(72)

(73)

(74)

(75)

(76)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

19

(77)

(79)

(80)

(81)

b) Bounding Rt: The right term Rt takes its value into sequences that are running average of masking matrices. Formally,

Rt =

¯M(it)

(cid:107)

I

(cid:107)F , where we deﬁne for all i

∈

[n],

¯M(i)
t

γ(i)
b ,t(i)
t(i)

c

Mtb ,

which follows the recursion

t −
c(i)
t(cid:88)

(cid:44)

b=1






¯M(i)
t
¯M(i)
t
¯M(i)
0

γc(i)

) ¯M(i)
t−1 + γc(i)
= (1
−
= M(i)
= it
if i
t−1
[n]
= 0 for all i

t

t

Mt

if i = it

When sampling a sequence of indices (is)s>0, the n random matrix sequences [( ¯M(i)
law as the sampling is uniform. We therefore focus on controling ( ¯M(0)
the expectation over the sequence of indices (is)s,

∈
t )t≤0]
i∈[n] follows the same probability
] is

t )t. For simplicity, we write ct (cid:44) c(0)

t

. When E[
·

¯M(0)
E[
(cid:107)

t −

(cid:107)F ]2
I

( ¯M(0)
t

[j, j]

1)(cid:3) = pE[( ¯M(0)

t

−

[0, 0]

1)]

−

E(cid:2)

p
(cid:88)

j=1

≤

≤

C p(ct)1/2γct = C p(ct)1/2−v, where C is a constant independent of t.

(78)

We have simply bounded the Froebenius norm by the (cid:96)1 norm in the ﬁrst inequality and used the fact that all coefﬁcients
Mt[j, j] follows the same Bernouilli law for all t > 0, j
[p]. We then used Lemma B.7 from [10] for the last inequality. This
lemma applies as Mt[0, 0] follows the recursion (77). It remains to take the expectation of (78), over all possible sampling
trajectories (is)s>0:

∈

E[Rt] = E(cid:2)E[Rt

(is)s](cid:3) = E(cid:2)E[
M(it)
t −
(cid:107)F |
|
(cid:107)
CpE[(ct)2(u−1)−η].
= CpE[(ct)1/2−v]

I

≤

(is)s](cid:3) = E(cid:2)E[
(cid:107)

M(0)

I

t −

(cid:107)F |

(is)s](cid:3) = E[

M(0)
(cid:107)

t −

I

(cid:107)F ]

The last inequality arises from the deﬁnition of η (cid:44) 1
we successively have

2 min (cid:0)v

3
4 , (3u

2)

−

−

−

v(cid:1), as follows. First, η > 0 as u > 11

12 . Then,

5
2 −

2u <

<

as u >

2
3

3
4

,

11
12

,

3
4

v

≥

+ 2η >

2u + 2η,

5
2 −

1
2 −

v <

1
2 −

5
2

+ 2u

2η = 2(u

1)

2η < 2(u

1)

η, which allows to conclude.

−

−

−

−

−

I)t converges towards 0
Lemma B.7 from [10] also ensures that Mt[0, 0]
1 almost surely when t
almost surely, given any sample sequence (is)s. It thus converges almost surely when all random variables of the algorithm
are considered. This is also true for ( ¯M(i)

t −

→ ∞

→

I)t for all i

. Therefore ( ¯M(0)

c) Bounding Lt: As above, we deﬁne n sequences [(L(i)

t −

∈

[n] and hence for Rt.
t )t]i∈[n], such that Lt = L(it)

t

for all t > 0. Namely,

L(i)
t

(cid:44) (cid:88)
s≤t,
xs=xt=x(i)

γ(i)
s,t(cid:107)

Ds−1

Dt−1

−

(cid:107)F =

c(i)
t(cid:88)

b=1

γ(i)
b ,t(i)
t(i)

c

(i)
t

(cid:13)
(cid:13)Dtb−1

Dt

c

(i)
t

−

(cid:13)
(cid:13)

−1

.

F

Once again, the sequences (cid:2)(L(i)
t )t
focus on bounding (L(0)
From assumption (B) and the deﬁnition of η, we have v < ν < 1. We split the sum in two parts, around index dt (cid:44) ct
where

(cid:3)
i all follows the same distribution when sampling over sequence of indices (is)s. We thus
η.
,

t )t. Once again, we drop the (0) superscripts in the right expression for simplicity. We set ν (cid:44) 3u

takes the integer part of a real number. For simplicity, we write d (cid:44) dt and c (cid:44) ct in the following.

2
−
−
(ct)ν

−(cid:98)

(cid:99)

(cid:98)·(cid:99)

L(0)

t =

γtb,tc

(cid:13)
(cid:13)Dtb−1

Dtc−1

−

(cid:13)
(cid:13)F ≤

2√kD

c
(cid:88)

b=1

d
(cid:88)

b=1

c
(cid:88)

tc−1
(cid:88)

γtb,tc +

γtb,t

b=d+1

s=tb−1

ws (cid:44) 2√kDL(0)

t,1 + L(0)

t,2

(82)

On the left side, we have bounded
on

Ds

Dt

(cid:107)
We now study both L(0)

−

Dt
(cid:107)
t,1 and L(0)

(cid:107)F provided by Lemma 3, that applies here as (I) is met and (63) ensures that (
gt
(cid:107)

−

g(cid:63)
t (cid:107)∞)t is bounded.

(cid:107)F by √kD, where D is deﬁned in the previous section. The right part uses the bound

t,2 . First, for all t > 0,

L(0)
t,1

(cid:44)

d
(cid:88)

b=1

d
(cid:88)

c
(cid:89)

γtb,tc =

γb

(1

γp)

−

≤

p=b+1

d
(cid:88)

b=1

γb(1

−

γc)c−b

b=1
γc)(cid:98)cν (cid:99)
γc

≤

(1

−

≤

cv exp (cid:0)log(1

1

cv )cν(cid:1)

≤

−

C (cid:48)cv exp(cν−v)

Cc2(u−1)−η = C(ct)2(u−1)−η,

(83)

≤

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

20

where C and C (cid:48) are constants independent of t. We have used ν > v for the third inequality, which ensures that
exp (cid:0)log(1
(cν−v). Basic asymptotic comparison provides the last inequality, as ct
almost surely and
the right term decays exponentially in (ct)t, while the left decays polynomially. As a consequence, L(0)
0 almost surely.

cv )cν(cid:1)

∈ O

−

1

Secondly, the right term can be bounded as (wt)t decays sufﬁciently rapidly. Indeed, as (cid:80)c

→ ∞
t,1 →
b=1 γtb,t = 1, we have

L(0)
t,2

(cid:44)

γtb,t

c
(cid:88)

b=d

tc−1
(cid:88)

s=tb−1

ws

max
d≤b≤c

≤

s=tb−1

(cid:16) tc−1
(cid:88)

(cid:17)

ws

=

tc−1
(cid:88)

ws

wtd (tc

td) =

−

≤

s=td−1
ct
dt
−
(dt)u

tc
td
(td)u =
−

tc
ct

td
dt

(

dt
td

)u

−
−

from elementary comparisons. First, we use the deﬁnition of ν to draw

were we use the fast that η
−
expectation n. Therefore, as c

when t

n , and
0, from the strong law of large numbers and linearity of the expectation

tb follows a geometric law of parameter 1

−

(ct)ν

ct
dt
(dt)u ≤

−

(ct)u(1

cν−1
t

)u ≤

−
1 < 0. We note that for all b > 0, tb+1

C(ct)ν−u = C(ct)2(u−1)−η,

d

−

→ ∞
c−1
1
(cid:88)

d

→

tb

tb+1

n,

−

→

d−1
(cid:88)

td
d

=

1
d

tb+1

tb

n almost surely.

−

→

b=0

b=d

−
n1−u almost surely. This immediately shows L(0)
0 almost surely and therefore
βt −

almost surely.

β(cid:63)

0

(cid:107)

→

t,2 →

0 and thus L(0)

t →

0 almost surely.

tc
c

td
d

=

−
−
As a consequence, tc−td
ct−dt
As with Rt, this implies that Lt

( dt
td

)u

c

→

t (cid:107)2 →
( dt
td

−
−

Finally, from the dominated convergence theorem, E[ tc−td
ct−dt
and write

)u]

n1−u for t

→

→ ∞

. We can use Cauchy-Schartz inequality

E[L(0)

t,2 ] = E[

tc
td
(td)u ]
−

E[

ct
dt
(dt)u ]E[
−

tc
ct

≤

td
dt

(

dt
td

)u]

C (cid:48)E[

≤

ct
dt
(dt)u ]
−

≤

C C (cid:48)E[(ct)2(u−1)−η],

where C (cid:48) is a constant independant of t. Then

E[Lt] = E(cid:2)E[L(it)

(is)s](cid:3) = E[L(0)
(is)s](cid:3) = E(cid:2)E[L(0)
|
β(cid:63)
Combined with (79), this shows that E[
βt −
t (cid:107)2]
(cid:107)
∈ O
0. Therefore E[( ct
(t, 1

1
n almost surely when t

n ), ct

|

t

t

t →

t

]

≤

2√kDE[L(0)

t,1 ] + E[L(0)
t,2 ]
((ct)2(u−1)−η). As ct follows a binomial distribution of parameter
nη−2(u−1), and from Cauchy-Schwartz inequality,
t )2(u−1)−η)]
)2(u−1)−η)]t2(u−1)−η

((ct)2(u−1)−η).

(t2(u−1)−η).

∈ O

(89)

→

(90)

→
βt −
(cid:107)

E[

β(cid:63)
t (cid:107)2]

CE[(

ct
t

≤
We have reused the fact that converging sequences are bounded. This is enough to conclude.

∈ O

(84)

(85)

(86)

(87)

(88)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

1

Stochastic Subsampling for
Factorizing Huge Matrices

Arthur Mensch, Julien Mairal,
Bertrand Thirion, and Ga¨el Varoquaux

7
1
0
2
 
t
c
O
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
6
3
5
0
.
1
0
7
1
:
v
i
X
r
a

Abstract—We present a matrix-factorization algorithm that
scales to input matrices with both huge number of rows and
columns. Learned factors may be sparse or dense and/or non-
negative, which makes our algorithm suitable for dictionary
learning, sparse component analysis, and non-negative matrix
factorization. Our algorithm streams matrix columns while
subsampling them to iteratively learn the matrix factors. At
each iteration, the row dimension of a new sample is reduced by
subsampling, resulting in lower time complexity compared to a
simple streaming algorithm. Our method comes with convergence
guarantees to reach a stationary point of the matrix-factorization
problem. We demonstrate its efﬁciency on massive functional
Magnetic Resonance Imaging data (2 TB), and on patches ex-
tracted from hyperspectral images (103 GB). For both problems,
which involve different penalties on rows and columns, we obtain
signiﬁcant speed-ups compared to state-of-the-art algorithms.

Index Terms—Matrix factorization, dictionary learning, NMF,
stochastic optimization, majorization-minimization, randomized
methods, functional MRI, hyperspectral imaging

I. INTRODUCTION

Matrix factorization is a ﬂexible approach to uncover latent
factors in low-rank or sparse models. With sparse factors, it is
used in dictionary learning, and has proven very effective for
denoising and visual feature encoding in signal and computer
vision [see e.g., 1]. When the data admit a low-rank structure,
matrix factorization has proven very powerful for various tasks
such as matrix completion [2, 3], word embedding [4, 5], or
network models [6]. It is ﬂexible enough to accommodate a
large set of constraints and regularizations, and has gained sig-
niﬁcant attention in scientiﬁc domains where interpretability
is a key aspect, such as genetics [7] and neuroscience [8].
In this paper, our goal is to adapt matrix-factorization tech-
niques to huge-dimensional datasets, i.e., with large number
of columns n and large number of rows p. Speciﬁcally, our
work is motivated by the rapid increase in sensor resolution, as
in hyperspectral imaging or fMRI, and the challenge that the
resulting high-dimensional signals pose to current algorithms.
As a widely-used model, the literature on matrix factoriza-
tion is very rich and two main classes of formulations have

A. Mensch, B. Thirion, G. Varoquaux are with Parietal team, Inria, CEA,
Paris-Saclay University, Neurospin, at Gif-sur-Yvette, France. J. Mairal is with
Universit´e Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK at Grenoble,
France.

The research leading to these results was supported by the ANR (MAC-
ARON project, ANR-14-CE23-0003-01 — NiConnect project, ANR-11-
BINF-0004NiConnect). It has received funding from the European Union’s
Horizon 2020 Framework Programme for Research and Innovation under
Grant Agreement No 720270 (Human Brain Project SGA1).

Corresponding author: Arthur Mensch (arthur.mensch@m4x.org)

emerged. The ﬁrst one addresses a convex-optimization prob-
lem with a penalty promoting low-rank structures, such as the
trace or max norms [2]. This formulation has strong theoretical
guarantees [3], but lacks scalability for huge datasets or sparse
factors. For these reasons, our paper is focused on a second
type of approach, which relies on nonconvex optimization.
Stochastic (or online) optimization methods have been devel-
oped in this setting. Unlike classical alternate minimization
procedures, they learn matrix decompositions by observing
a single matrix column (or row) at each iteration. In other
words, they stream data along one matrix dimension. Their
cost per iteration is signiﬁcantly reduced, leading to faster
convergence in various practical contexts. More precisely,
two approaches have been particularly successful: stochastic
gradient descent [9] and stochastic majorization-minimization
methods [10, 11]. The former has been widely used for matrix
completion [see 12, 13, 14, and references therein], while the
latter has been used for dictionary learning with sparse and/or
structured regularization [15]. Despite those efforts, stochastic
algorithms for dictionary learning are currently unable to deal
efﬁciently with matrices that are large in both dimensions.

We propose a new matrix-factorization algorithm that
can handle such matrices.
It builds upon the stochastic
majorization-minimization framework of [10], which we gen-
the objective
eralize for our problem. In this framework,
function is minimized by iteratively improving an upper-bound
surrogate of the function (majorization step) and minimizing
it to obtain new estimates (minimization step). The core idea
of our algorithm is to approximate these steps to perform them
faster. We carefully introduce and control approximations,
so to extend convergence results of [10] when neither the
majorization nor the minimization step is performed exactly.
For this purpose, we borrow ideas from randomized meth-
ods in machine learning and signal processing. Indeed, quite
orthogonally to stochastic optimization, efﬁcient approaches to
tackle the growth of dataset dimension have exploited random
projections [16, 17] or sampling, reducing data dimension
while preserving signal content. Large-scale datasets often
have an intrinsic dimension which is signiﬁcantly smaller
than their ambient dimension. Good examples are biological
datasets [18] and physical acquisitions with an underlying
sparse structure enabling compressed sensing [19]. In this
context, models can be learned using only random data sum-
maries, also called sketches. For instance, randomized methods
[see 20, for a review] are efﬁcient to compute PCA [21], a
classic matrix-factorization approach, and to solve constrained
or penalized least-square problems [22, 23]. On a theoretical

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

2

level, recent works on sketching [24, 25] have provided bounds
on the risk of using random summaries in learning.

the matrix D is called the “dictionary” and A the sparse code.
We use this terminology throughout the paper.

Using random projections as a pre-processing step is not
appealing in our applicative context since factors learned on
reduced data are not
it
is possible to exploit random sampling to approximate the
steps of online matrix factorization. Factors are learned in
the original space whereas the dimension of each iteration is
reduced together with the computational cost per iteration.

interpretable. On the other hand,

Contribution: The contribution of this paper is both prac-
tical and theoretical. We introduce a new matrix factoriza-
tion algorithm, called subsampled online matrix factorization
(SOMF), which is faster than state-of-the-art algorithms by an
order of magnitude on large real-world datasets (hyperspectral
images, large fMRI data). It leverages random sampling with
stochastic optimization to learn sparse and dense factors more
efﬁciently. To prove the convergence of SOMF, we extend
the stochastic majorization-minimization framework [10] and
make it robust to some time-saving approximations. We then
show convergence guarantees for SOMF under reasonable
assumptions. Finally, we propose an extensive empirical vali-
dation of the subsampling approach.

In a ﬁrst version of this work [26] presented at the Interna-
tional Conference in Machine Learning (ICML), we proposed
an algorithm similar to SOMF, without any theoretical guaran-
tees. The algorithm that we present here has such guarantees,
which we express in a more general framework, stochastic
is validated for new sparsity
majorization-minimization. It
settings and a new domain of application. An open-source
efﬁcient Python package is provided.

Notations: Matrices are written using bold capital letters
and vectors using bold small letters (e.g., X, α). We use
superscript
to specify the column (sample or component)
number, and write X = [x(1), . . . , x(n)]. We use subscripts
to specify the iteration number, as in xt. The ﬂoating bar,
as in ¯gt, is used to stress that a given value is an average
over iterations, or an expectation. The superscript (cid:63) is used to
denote an exact value, when it has to be compared to an inexact
value, e.g., to compare α(cid:63)

t (exact) to αt (approximation).

II. PRIOR ART: MATRIX FACTORIZATION WITH
STOCHASTIC MAJORIZATION-MINIMIZATION

Below, we introduce the matrix-factorization problem and
recall a speciﬁc stochastic algorithm to solve it observing
one column (or a mini-batch) at every iteration. We cast this
algorithm in the stochastic majorization-minimization frame-
work [10], which we will use in the convergence analysis.

A. Problem statement

In our setting, the goal of matrix factorization is to decom-
Rp×n — typically n signals of dimension p

pose a matrix X
— as a product of two smaller matrices:

∈

X

DA with D

Rp×k and A

Rk×n,

≈

∈
with potential sparsity or structure requirements on D and A.
In signal processing, sparsity is often enforced on the code A,
in a problem called dictionary learning [27]. In such a case,

∈

Learning the factorization is typically performed by min-
imizing a quadratic data-ﬁtting term, with constraints and/or
penalties over the code and the dictionary:

min
D∈C
A∈Rk×n

n
(cid:88)

i=1

1
2

(cid:13)
(cid:13)x(i)

Dα(i)(cid:13)
2
2 + λ Ω(α(i)),
(cid:13)

(1)

−

C

→

where A (cid:44) [α(1), . . . , α(n)],
is a column-wise separable
C
R is a penalty over the
convex set of Rp×k and Ω : Rp
code. Both constraint set and penalty may enforce structure or
sparsity, though
has traditionally been used as a technical
requirement to ensure that the penalty on A does not vanish
with D growing arbitrarily large. Two choices of
and Ω are
of particular interest. The problem of dictionary learning sets
C
as the (cid:96)2 ball for each atom and Ω to be the (cid:96)1 norm. Due to
the sparsifying effect of (cid:96)1 penalty [28], the dataset admits
a sparse representation in the dictionary. On the opposite,
ﬁnding a sparse set in which to represent a given dataset,
with a goal akin to sparse PCA [29], requires to set as the
(cid:96)1 ball for each atom and Ω to be the (cid:96)2 norm. Our work
considers the elastic-net constraints and penalties [30], which
encompass both special cases. Fixing ν and µ in [0, 1], we
the elastic-net penalty in Rp and Rk:
denote by Ω(

C

α
ν)
(cid:107)

−
(cid:44) (1

(cid:107)
µ)

ν
2 (cid:107)

α

2
2,
(cid:107)
µ
1+
2 (cid:107)
(cid:107)

1 +

d(j)
(cid:107)

(2)
(cid:111)
1

.

d(j)

2
2 ≤
(cid:107)

(cid:44)

(cid:110)

D

Rp×k/
(cid:107)

d(j)

(cid:107)

∈

C
−
Following [15], we can also enforce the positivity of D
and/or A by replacing R by R+ in
, and adding positivity
constraints on A in (1), as in non-negative sparse coding [31].
We rewrite (1) as an empirical risk minimization problem
depending on the dictionary only. The matrix D solution of (1)
is indeed obtained by minimizing the empirical risk ¯f

C

D

argmin
D∈C

∈

n
(cid:88)

(cid:17)

f (D, x(i))

(cid:16) ¯f (D) (cid:44) 1
n
Dα(cid:13)
2
2 + λ Ω(α),
(cid:13)

i=1

,

(3)

where

1
2
and the matrix A is obtained by solving the linear regression

f (D, x) (cid:44) min
α∈Rk

(cid:13)
(cid:13)x

−

) and
·

(cid:107) · (cid:107)
Ω(α) (cid:44) (1

min
A∈Rk×n

n
(cid:88)

i=1

1
2

(cid:13)
(cid:13)x(i)

Dα(i)(cid:13)
2
2 + λ Ω(α(i)).
(cid:13)

−

(4)

The problem (1) is non-convex in the parameters (D, A), and
hence (3) is not convex. However, the problem (1) is convex in
both D and A when ﬁxing one variable and optimizing with
respect to the other. As such, it is naturally solved by alternate
minimization over D and A, which asymptotically provides
a stationary point of (3). Yet, X has typically to be observed
hundred of times before obtaining a good dictionary. Alternate
minimization is therefore not adapted to datasets with many
samples.

B. Online matrix factorization

When X has a large number of columns but a limited
number of rows, the stochastic optimization method of [15]

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

3

Algorithm 1 Online matrix factorization (OMF) [15]

Input: Initial iterate D0, sample stream (xt)t>0, number of
iterations T .
for t from 1 to T do

∼ P

Draw xt
.
Compute αt = argminα∈Rp
Update the parameters of aggregated surrogate ¯gt:
(cid:17)

Dt−1α(cid:13)
2
2+λ Ω(α).
(cid:13)

(cid:13)
(cid:13)xt

−

(cid:16)

1
2

1

¯Ct =

¯Ct−1 +

1
t
1
t
Compute (using block coordinate descent):

¯Bt−1 +

αtα(cid:62)
t .

xtα(cid:62)
t .

¯Bt =

1
t
1
t

−

−

(cid:17)

(cid:16)

1

(8)

Dt = argmin

Tr (D(cid:62)D ¯Ct)

Tr (D(cid:62) ¯Bt).

−

1
2

D∈C

Output: Final iterate DT .

Algorithm 2 Stochastic majorization-minimization [SMM 10]
Input: Initial iterate θ0, weight sequence (wt)t>0, sample
stream (xt)t>0, number of iteration T .
for t from 1 to T do

Draw xt
∈
Construct a surrogate of ft near θt−1, that meets

, get ft : θ

f (xt, θ).

∼ P

→

Θ

gt

ft,

gt(θt−1) = ft(θt−1).

(12)

≥

Update the aggregated surrogate:

¯gt = (1

wt)¯gt−1 + wtgt.

−

Compute

θt = argmin

¯gt(θ).

θ∈Θ

(13)

Output: Final iterate θT .

outputs a good dictionary much more rapidly than alternate-
minimization. In this setting [see 32], learning the dictionary
is naturally formalized as an expected risk minimization

min
D∈C

¯f (D) (cid:44) Ex[f (D, x)],

where x is drawn from the data distribution and forms an i.i.d.
stream (xt)t. In the ﬁnite-sample setting, (5) reduces to (3)
when xt is drawn uniformly at random from
.
We then write it the sample number selected at time t.

[1, n]
}

x(i), i

∈

{

The online matrix factorization algorithm proposed in [15]
is summarized in Alg. 1. It draws a sample xt at each iteration,
and uses it to improve the current iterate Dt−1. For this, it
ﬁrst computes the code αt associated to xt on the current
dictionary:

αt (cid:44) argmin
α∈Rk

1
2 (cid:107)

xt

−

Dt−1α

2
2 + λΩ(α).
(cid:107)

(6)

Then, it updates Dt to make it optimal in reconstructing past
samples (xs)s≤t from previously computed codes (αs)s≤t:

Dt

argmin
D∈C

∈

(cid:16)

¯gt(D) (cid:44) 1
t

t
(cid:88)

s=1

1
2

(cid:13)
(cid:13)xs

−

Dαs

(cid:17)
(cid:13)
2
2 + λΩ(αs)
(cid:13)

.

(7)
Importantly, minimizing ¯gt is equivalent to minimizing the
quadratic function

D

→

1
2

Tr (D(cid:62)D ¯C(cid:62)
t )

Tr (D(cid:62) ¯Bt),

(9)

−

where ¯Bt and ¯Ct are small matrices that summarize previously
seen samples and codes:

¯Bt =

xsα(cid:62)
s

¯Ct =

αsα(cid:62)
s .

(10)

1
t

t
(cid:88)

s=1

1
t

t
(cid:88)

s=1

The function ¯gt is an upper-bound surrogate of the true
current empirical risk, whose deﬁnition involves the regression
minima computed on current dictionary D:

¯ft(D) (cid:44) 1
t

t
(cid:88)

s=1

(5)

min
α∈Rp

(cid:13)
(cid:13)xs

1
2

−

Dα(cid:13)
2
2+ λΩ(α)
(cid:13)

≤

¯gt(D). (11)

Using empirical processes theory [33], it is possible to show
that minimizing ¯ft at each iteration asymptotically yields
a stationary point of the expected risk (5). Unfortunately,
minimizing (11) is expensive as it involves the computation of
optimal current codes for every previously seen sample at each
iteration, which boils down to naive alternate-minimization.

In contrast, ¯gt is much cheaper to minimize than ¯ft, using
block coordinate descent. It is possible to show that ¯gt con-
verges towards a locally tight upper-bound of the objective ¯ft
and that minimizing ¯gt at each iteration also asymptotically
yields a stationary point of the expected risk (5). This es-
tablishes the correctness of the online matrix factorization
algorithm (OMF). In practice, the OMF algorithm performs a
single pass of block coordinate descent: the minimization step
is inexact. This heuristic will be justiﬁed by our theoretical
contribution in Section IV.

}

t
∈ T

Extensions: For efﬁciency, it is essential to use mini-batches
xs, s
of size η instead of single samples in the
{
iterations [15]. The surrogate parameters ¯Bt, ¯Ct are then
s∈Tt over
updated by the mean value of
}
the batch. The optimal size of the mini-batches is usually
close to k. (8) uses the sequence of weights ( 1
t )t to update
parameters ¯Bt and ¯Ct. [15] replaces these weights with a
sequence (wt)t, which can decay more slowly to give more
importance to recent samples in ¯gt. These weights will prove
important in our analysis.

(xsα(cid:62)
{

s , αsα(cid:62)
s )

C. Stochastic majorization-minimization

C

As the constraints
have a separable structure per atom, [15]
uses projected block coordinate descent to minimize ¯gt. The
¯Bt, and it is there-
function gradient writes
fore enough to maintain ¯Bt and ¯Ct in memory to solve (7).
¯Bt and ¯Ct are updated online, using the rules (8) (Alg. 1).

¯gt(D) = D ¯Ct

∇

−

Online matrix factorization belongs to a wider category
of algorithms introduced in [10] that minimize locally tight
upper-bounding surrogates instead of a more complex objec-
tive, in order to solve an expected risk minimization prob-
lem. Generalizing online matrix factorization, we introduce

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

4

−

(cid:107)

in Alg. 2 the stochastic majorization-minimization (SMM)
algorithm, which is at the core of our theoretical contribution.
In online matrix factorization, the true empirical risk func-
tions ¯ft and their surrogates ¯gt follow the update rules, with
generalized weight (wt)t set to ( 1
¯ft (cid:44) (1

wt)¯gt−1 + wtgt, (14)

t )t in (7) – (11):

wt) ¯ft−1 + wtft,

¯gt (cid:44) (1

−

where the pointwise loss function and its surrogate are

ft(D) (cid:44) min
α∈Rk
gt(D) (cid:44) 1
2 (cid:107)

xt

1
2 (cid:107)

xt

−

Dα

2
2 + λΩ(α),

Dαt

2
2 + λΩ(αt).
(cid:107)

−

(15)

≥

ft, and gt

The function gt is a majorizing surrogate of ft: gt
is tangent to ft in Dt−1, i.e, gt(Dt−1) = ft(Dt−1) and
(gt
ft)(Dt−1) = 0. At each step of online matrix factorization:
• The surrogate gt is computed along with αt, using (6).
• The parameters ¯Bt, ¯Ct are updated following (8). They
deﬁne the aggregated surrogate ¯gt up to a constant.
• The quadratic function ¯gt is minimized efﬁciently by
block coordinate descent, using parameters ¯Bt and ¯Ct
to compute its gradient.

∇

−

iteration t, a surrogate gt of the loss ft

The stochastic majorization-minimization framework sim-
ply formalizes the three steps above, for a larger variety of
loss functions ft(θ) (cid:44) f (θ, xt), where θ is the parameter we
want to learn (D in the online matrix factorization setting).
is computed
At
to update the aggregated surrogate ¯gt following (14). The
surrogate functions (gt)t should be upper-bounds of loss
functions (ft)t, tight
in the current iterate θt−1 (e.g., the
dictionary Dt−1). This simply means that ft(θt−1) = gt(θt−1)
gt)(θt−1) = 0. Computing ¯gt can be done if gt is
and
∇
−
deﬁned simply, as in OMF where it is linearly parametrized by
t , xtα(cid:62)
(αtα(cid:62)
t ). ¯gt is then minimized to obtain a new iterate θt.
It can be shown following [10] that stochastic majorization-
minimization algorithms ﬁnd asymptotical stationary point of
the expected risk Ex[f (θ, x)] under mild assumptions recalled
in Section IV. SMM admits the same mini-batch and decaying
weight extensions (used in Alg. 2) as OMF.

(ft

In this work, we extend the SMM framework and allow both
majorization and minimization steps to be approximated. As a
side contribution, our extension proves that performing a single
pass of block coordinate descent to update the dictionary, an
important heuristic in [15], is indeed correct. We ﬁrst introduce
the new matrix factorization algorithm at the core of this paper
and then present the extended SMM framework.

III. STOCHASTIC SUBSAMPLING FOR HIGH DIMENSIONAL
DATA DECOMPOSITION

The online algorithm presented in Section II is very efﬁcient
to factorize matrices that have a large number of columns (i.e.,
with a large number of samples n), but a reasonable number
of rows — the dataset is not very high dimensional. However,
it is not designed to deal with very high number of rows: the
cost of a single iteration depends linearly on p. On terabyte-
105 features, the original
scale datasets from fMRI with p = 2
online algorithm requires one week to reach convergence. This

·

Fig. 1. Stochastic subsampling further improves online matrix factorization to
handle datasets with large number of columns and rows. X is the input p × n
matrix, Dt and At are respectively the dictionary and code at time t.

is a major motivation for designing new matrix factorization
algorithms that scale in both directions.

In the large-sample regime p

k, the underlying dimen-
sionality of columns may be much lower than the actual p:
the rows of a single column drawn at random are therefore
correlated and redundant. This guides us on how to scale
online matrix factorization with regard to the number of rows:

(cid:29)

• The online algorithm OMF uses a single column of (or
mini-batch) of X at each iteration to enrich the average
surrogate and update the whole dictionary.

• We go a step beyond and use a fraction of a single column

of X to reﬁne a fraction of the dictionary.

More precisely, we draw a column and observe only some
to reﬁne these rows of the
of its rows at each iteration,
dictionary, as illustrated in Figure 1. To take into account
all features from the dataset, rows are selected at random at
each iteration: we call this technique stochastic subsampling.
Stochastic subsampling reduces the efﬁciency of the dictionary
update per iteration, as less information is incorporated in the
current iterate Dt. On the other hand, with a correct design,
the cost of a single iteration can be considerably reduced, as it
grows with the number of observed features. Section V shows
that the proposed algorithm is an order of magnitude faster
than the original OMF on large and redundant datasets.

First, we formalize the idea of working with a fraction of
the p rows at a single iteration. We adapt the online matrix
factorization algorithm, to reduce the iteration cost by a factor
close to the ratio of selected rows. This deﬁnes a new on-
line algorithm, called subsampled online matrix factorization
(SOMF). At each iteration, it uses q rows of the column xt to
update the sequence of iterates (Dt)t. As in Section II, we
introduce a more general algorithm, stochastic approximate
majorization-minimization (SAMM), of which SOMF is an
instance. It extends the stochastic majorization-minimization
framework, with similar theoretical guarantees but potentially
faster convergence.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

5

A. Subsampled online matrix factorization

∈

[1, n]

Formally, as in online matrix factorization, we consider a
sample stream (xt)t in Rp that cycles onto a ﬁnite sample set
x(i), i
{
1) Stochastic subsampling and algorithm outline: We want
to reduce the time complexity of a single iteration. In the
original algorithm, the complexity depends linearly on the
sample dimension p in three aspects:

, and minimize the empirical risk (3).1
}

∈

∈

∈

Rp×k,

Rp is used to compute the code αt,
• xt
• it is used to update the surrogate parameters ¯Bt
• Dt

Rp×k is fully updated at each iteration.
Our algorithm reduces the dimensionality of these steps at
each iteration, such that p becomes q = p
r in the time
complexity analysis, where r > 1 is a reduction factor.
Formally, we randomly draw, at iteration t, a mask Mt that
“selects” a random subset of xt. We use it to drop a part of the
features of xt and to “freeze” these features in dictionary D
at iteration t.

It is convenient to consider Mt as a Rp×p random diagonal
matrix, such that each coefﬁcient is a Bernouilli variable with
parameter 1
1],

r , normalized to be 1 in expectation.

[0, p

j
∀
, P(cid:2)Mt[j, j] = 0(cid:3) = 1

∈
1
r

.

−

−

(16)

P(cid:2)Mt[j, j] = r(cid:3) =

1
r

Thus, r describes the average proportion of observed features
and Mtxt is a non-biased, low-dimensional estimator of xt:

E(cid:2)
Mtxt
(cid:107)

(cid:107)

(cid:3) =

0

p
r

= q

E(cid:2)Mtxt

(cid:3) = xt.

(17)

(cid:107) · (cid:107)

0 counting the number of non-zero coefﬁcients. We
with
deﬁne the pair of orthogonal projectors Pt
t ∈
R(p−q)×p that project Rp onto Im(Mt) and Ker(Mt). In other
Rp×y
words, PtY and P⊥
with rows respectively selected and not selected by Mt. In
Rq×n assigns the rows of Z to the
algorithms, PtY
rows of Y selected by Pt, by an abuse of notation.

t Y are the submatrices of Y

Rq×p and P⊥

←

Z

∈

∈

∈

In brief, subsampled online matrix factorization, deﬁned in
Alg. 3, follows the outer loop of online matrix factorization,
with the following major modiﬁcations at iteration t:

• it uses Mtxt and low-size statistics instead of xt to

estimate the code αt and the surrogate gt,

• it updates a subset of the dictionary PtDt−1 to reduce
the surrogate value ¯gt(D). Relevant parameters of ¯gt are
computed using Ptxt and αt only.

We now present SOMF in details. For comparison purpose,
we write all variables that would be computed following the
OMF rules at iteration t with a (cid:63) superscript. For simplicity, in
Alg. 3 and in the following paragraphs, we assume that we use
one sample per iteration —in practice, we use mini-batches of
size η. The next derivations are transposable when a batch It
is drawn at iteration t instead of a single sample it.

2) Code computation: In the OMF algorithm presented in
t is obtained by solving (6), namely

Section II, α(cid:63)

α(cid:63)

t ∈

argmin
α

1
2

α(cid:62)G(cid:63)

t α

α(cid:62)β(cid:63)

t + λΩ(α),

(21)

−

Algorithm 3 Subsampled online matrix factorization (SOMF)
(wt)t>0,

iterate D0, weight

sequences

Initial

Input:
(γc)c>0, sample set
{
for t from 1 to T do

x(i)

}i>0, number of iterations T .
Draw xt = x(i) at random and Mt following (16).
Update the regression parameters for sample i:

c(i)
β(i)
G(i)

c(i) + 1,

(1

←
t ←
t ←
Compute the approximate code for xt:

t−1 + γD(cid:62)
t−1 + γD(cid:62)

t−1Mtx(i),
t−1MtDt−1, Gt

γ)G(i)
γ)G(i)

(1

−

−

γ
←
βt ←
←

γc(i) .
β(i)
.
t
¯G(i)
t

.

αt

argmin
α∈Rk

←

1
2

α(cid:62)Gtα

α(cid:62)βt + λ Ω(α).

(18)

−

Update the parameters of the aggregated surrogate ¯gt:

¯Ct
Pt ¯Bt

(1

(1

−

−

←

←

wt) ¯Ct−1 + wtαtα(cid:62)
t .
wt)Pt ¯Bt−1 + wtPtxtα(cid:62)
t .

Compute simultaneously (using Alg. 4 for 1st line):

PtDt

P⊥
t

¯Bt

argmin
Dr∈Cr
(1

1
2
wt)P⊥
t

−

←

←

Tr (Dr (cid:62)Dr ¯Ct)

Tr (Dr (cid:62)Pt ¯Bt).

−
¯Bt−1 + wtP⊥
t xtα(cid:62)
t .

(19)

(20)

Output: Final iterate DT .

t = D(cid:62)

t = D(cid:62)

t−1Dt−1 and β(cid:63)
t and β(cid:63)

where G(cid:63)
t−1xt. For large p, the
computation of G(cid:63)
t dominates the complexity of the
regression step, which depends almost linearly on p. To reduce
this complexity, we use estimators for G(cid:63)
t , computed
at a cost proportional to the reduced dimension q. We propose
three kinds of estimators with different properties.

t and β(cid:63)

a) Masked loss: The most simple unbiased estimation
t and β(cid:63)
t whose computation cost depends on q is

of G(cid:63)
obtained by subsampling matrix products with Mt:
Gt = D(cid:62)
βt = D(cid:62)

t−1MtDt−1
t−1Mtxt.

(a)

This is the strategy proposed in [26]. We use Gt and βt
in (18), which amounts to minimize the masked loss

min
α∈Rk

1
2 (cid:107)

Mt(xt

D(cid:62)

t−1α)
(cid:107)

−

2
2 + λΩ(α).

(22)

t and β(cid:63)

Gt and βt are computed in a number of operations pro-
portional to q, which brings a speed-up factor of almost r
in the code computation for large p. On large data, using
estimators (a) instead of exact G(cid:63)
t proves very ef-
ﬁcient during the ﬁrst epochs (cycles over the columns).2
However, due to the masking, Gt and βt are not consistent
estimators: they do not converge to G(cid:63)
t for large t,
which breaks theoretical guarantees on the algorithm output.
Empirical results in Section V-E show that the sequence of
iterates approaches a critical point of the risk (3), but may
then oscillate around it.

t and β(cid:63)

1Note that we solve the fully observed problem despite the use of subsam-

2Estimators (a) are also available in the inﬁnite sample setting, when

pled data, unlike other recent work on low-rank factorization [34].

minimizing expected risk (5) from a i.i.d sample stream (xt)t.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

6

{

x(i)

b) Averaging over epochs: At iteration t, the sample xt
}i. This allows to
is drawn from a ﬁnite set of samples
average estimators over previously seen samples and address
the non-consistency issue of (a). Namely, we keep in memory
2n estimators, written (G(i)
t )1≤i≤n. We observe the
t
sample i = it at iteration t and use it to update the i-th
estimators ¯G(i)
t
β(i)
G(i)

following
γ)G(i)
γ)G(i)

t−1Mtx(i)
t−1MtD(i)

t−1 + γD(cid:62)
t−1 + γD(cid:62)

t = (1

t = (1

, ¯β(i)

, β(i)

(23)

−

,

t

t

−

where γ is a weight factor determined by the number of time
the one sample i has been previously observed at time t.
Precisely, given (γc)c a decreasing sequence of weights,
c(i)
t =

t, xs = x(i)(cid:111)(cid:12)
(cid:12)
(cid:12) .

γ = γc(i)

where

(cid:110)
s

(cid:12)
(cid:12)
(cid:12)

t

≤

All others estimators
iteration t
−
averaged estimators
Gt (cid:44) G(i)

t =

G(j)
t
{
G(i)
1. The set
t
{

, β(j)
t }j(cid:54)=i are left unchanged from
, β(i)
t }1≤i≤n is used to deﬁne the

(cid:88)

γ(i)
s,tD(cid:62)

s−1MsDs−1

βt

(cid:44) β(i)

t =

γ(i)
s,tD(cid:62)

s−1Msx(i),

(b)

s≤t,xs=x(i)
(cid:88)

s≤t,xs=x(i)

(cid:81)

s,t = γc(i)

where γ(i)
). Using βt and Gt
in (18), αt minimizes the masked loss averaged over the
previous iterations where sample i appeared:

s<t,xs=x(i) (1

γc(i)

−

s

t

γ(i)
s,t
2 (cid:107)

min
α∈Rk

(cid:88)

s≤t
xs=x(i)

Ms(x(i)

D(cid:62)

2
2 + λΩ(α).
s−1α)
(cid:107)

−

(24)

t )t and (β(cid:63)

The sequences (Gt)t and (βt)t are consistent estimations
of (G(cid:63)
t )t — consistency arises from the fact
that a single sample x(i) is observed with different masks
along iterations. Solving (24) is made closer and closer to
solving (21), to ensure the correctness of the algorithm (see
Section IV). Yet, computing the estimators (b) is no more
costly than computing (a) and still permits to speed up a single
iteration close to r times. In the mini-batch setting, for every
and β(i)
to compute α(i)
It, we use the estimators G(i)
i
.
t
t
t
(n k2). This is reasonable
This method has a memory cost of
k2.
compared to the dataset size3 if p

∈

c) Exact Gram computation: To reduce the memory
usage, another strategy is to use the true Gram matrix Gt
and the estimator βt from (b):
t = D(cid:62)

O
(cid:29)

Gt (cid:44) G(cid:63)
βt

(cid:44) (cid:88)

t−1Dt−1
γ(i)
s,tD(cid:62)

s−1Msx(i)

(c)

s≤t,xs=x(i)

As previously, the consistency of (βt)t ensures that (5) is
correctly solved despite the approximation in (αt)t computa-
tion. With the partial dictionary update step we propose, it is
possible to maintain Gt at a cost proportional to q. The time

3It is also possible to efﬁciently swap the estimators (G(i)

t )i on disk, as

they are only accessed for i = it at iteration t.

TABLE I
COMPARISON OF ESTIMATORS USED FOR CODE COMPUTATION

Est.

(a)
(b)
(c)

βt

Gt

Convergence

Extra
mem. cost

Masked
Averaged
Averaged

Masked
Averaged
Exact

(cid:88)
(cid:88)

n k2
n k

1st epoch
perform.
(cid:88)
(cid:88)

O

complexity of the coding step is thus similarly reduced when
replacing (b) or (c) estimators in (21), but the latter option
(n k). Although estimators (c) are
has a memory usage in
slightly less performant in the ﬁrst epochs, they are a good
compromise between resource usage and convergence. We
summarize the characteristics of the three estimators (a)–(c) in
Table I, anticipating their empirical comparison in Section V.
Surrogate computation: The computation of αt using one

of the estimators above deﬁnes a surrogate gt(D) (cid:44) 1
2 (cid:107)
−
2
Dαt
2 + λΩ(α), which we use to update the aggregated
(cid:107)
surrogate ¯gt (cid:44) (1
wt)¯gt−1 + wtgt, as in online matrix
factorization. We follow (8) (with weights (wt)t) to update the
matrices ¯Bt and ¯Ct, which deﬁne ¯gt up to constant factors.
The update of ¯Bt requires a number of operations proportional
to p. Fortunately, we will see in the next paragraph that it is
possible to update Pt ¯Bt in the main thread with a number
of operation proportional to q and to complete the update of
P⊥
t

¯Bt in parallel with the dictionary update step.
Weight sequences: Speciﬁc (wt)t and (γc)c in Alg. 3
are required. We provide then in Assumption (B) of the
( 11
analysis: wt = 1
12 , 1) and
cv , where u
2(cid:1) to ensure convergence. Weights have little
(cid:0) 3
4 , 3u
v
impact on convergence speed in practice.

tu and γc = 1

xt

−

−

∈

∈

3) Dictionary update: In the original online algorithm, the
whole dictionnary Dt−1 is updated at iteration t. To reduce the
time complexity of this step, we add a “freezing” constraint to
the minimization (7) of ¯gt. Every row r of D that corresponds
to an unseen row r at iteration r (such that Mt[r, r] = 0)
remains unchanged. This casts the problem (7) into a lower
dimensional space. Formally, the freezing operation comes out
as a additional constraint in (7):

Dt =

argmin
D∈C
t D=P⊥

P⊥

t Dt−1

1
2

Tr (D(cid:62)D ¯Ct)

Tr (D(cid:62) ¯Bt).

(25)

−

1 and P⊥

The constraints are separable into two blocks of rows. Re-
calling the notations of (2), for each atom d(j), the rules
t d(j)
d(j)
t d(j) = P⊥
t−1 can indeed be rewritten
(cid:40)
d(j)
− (cid:107)
t d(j)
t−1.

Ptd(j)
(cid:107)
t d(j)
P⊥

1
= P⊥

Ptd(j)
(cid:107)

(cid:44) r(j)
t

t−1(cid:107)

t−1(cid:107)

(cid:107) ≤

(26)

(cid:107) ≤

+

(cid:107)

Solving (25) is therefore equivalent to solving the following
(cid:44) PtBt,
problem in Rq×k, with Br
t

Dr

∈
r =

argmin
Dr∈Cr
Dr
{

∈

1
2
Rq×k/
j
∀

where

Tr (Dr (cid:62)Dr ¯Ct)

Tr (Dr (cid:62) ¯Br

t ) (27)

−

1],

[0, k

dr(j)
(cid:107)

r(j)
t }

.

C

∈
The rows of Dt selected by Pt are then replaced with Dr,
while the other rows of Dt are unchanged from iteration

(cid:107) ≤

−

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

7

Algorithm 4 Partial dictionary update

Gt

D(cid:62)

t−1PtDt−1.

Input: Dictionary Dt−1, projector Pt, statistics ¯Ct, ¯Bt,
norms (n(j)
t−1)0≤j<k, Gram matrix Gt (optional).
Dt−1,
Dt
Gt
←
←
−
permutation([1, k]) do
for j
∈
n(j)
r(j)
Ptd(j)
t−1 +
.
t ←
t−1(cid:107)
(cid:107)
¯Ct[j,j] (Pt ¯b(j)
Ptd(j)
t−1 + 1
u
←
enet projection(u, r(j)
Ptd(j)
Ptd(j)
n(j)
.
t (cid:107)
t ←
t PtDt.
←

Gt+1
Output: Dictionary Dt, norms (n(j)

t ←
r(j)
t − (cid:107)
Gt + D(cid:62)

PtDt¯c(j)
t ).

t )j, Gram matrix Gt+1.

(cid:46) in Rq
(cid:46) in Rq

t −

t ).

t

t

∈

−

t −

¯br(j)
t

and ¯br(j)

t Dt = P⊥

, at an average cost in

1. Formally, PtDt = Dr and P⊥

k operations, as it writes Dr¯c(j)

t Dt−1. We
t
solve (27) by a projected block coordinate descent (BCD)
similar to the one used in the original algorithm, but performed
in a subspace of size q. We compute each column j of the
gradient that we use in the block coordinate descent loop
Rq,
with q
×
where ¯c(j)
are the j-th columns of ¯Ct and ¯Br
t .
t
Each reduced atom dr(j) is projected onto the elastic-net ball
of radius r(j)
(q) following [15]. This
makes the complexity of a single-column update proportional
to q. Performing the projection requires to keep in memory the
d(j)
(cid:44) 1
values
t (cid:107)}j, which can be updated online at
{
a negligible cost.
We provide the reduced dictionary update step in Alg. 4,
where we use the function enet projection(u, r) that per-
Rq onto the elastic-net
forms the orthogonal projection of u
ball of radius r. As in the original algorithm, we perform a
single pass over columns to solve (27). Dictionary update is
now performed with a number of operations proportional to q,
instead of p in the original algorithm. Thanks to the random
nature of (Mt)t, updating Dt−1 into Dt reduces ¯gt enough
to ensure convergence.

n(j)
t

− (cid:107)

O

∈

t with a cost in

Gram matrix computation: Performing partial updates
of Dt makes it possible to maintain the full Gram matrix
Gt = G(cid:63)
(q k2) per iteration, as mentioned
O
in III-A2c. It is indeed enough to compute the reduced Gram
matrix D(cid:62)PtD before and after the dictionary update:
D(cid:62)
t PtD(cid:62)
t .

Gt+1 = D(cid:62)

t−1 + D(cid:62)

t−1PtD(cid:62)

t Dt = Gt

(28)

−

Parallel surrogate computation: Performing block coor-
t = Pt ¯Bt only.
t requires to access ¯Br
dinate descent on ¯gr
Assuming we may use use more than two threads, this allows
to parallelize the dictionary update step with the update
of P⊥
t
Pt ¯Bt

¯Bt. In the main thread, we compute Pt ¯Bt following

wt) ¯PtBt−1 + wtPtxtα(cid:62)
t .

(19 – Alg. 3)

(1

←

−

which has a cost proportional to q. Then, we update in parallel
the dictionary and the rows of ¯Bt that are not selected by Mt:
¯Bt−1 + wtP⊥

(20 – Alg. 3)

t xtα(cid:62)
t .

wt)P⊥
t

P⊥
t

¯Bt

(1

←

−

This update requires k(p
q)η operations (one matrix-matrix
product) for a mini-batch of size η. In contrast, with appropri-
ate implementation, the dictionary update step requires 4 k q2

−

∼
∼

to 6 k q2 operations, among which 2 k q2 come from slower
η, updating ¯Bt is faster
matrix-vector products. Assuming k
10, and performing (20)
than updating the dictionary up to r
on a second thread is seamless in term of wall-clock time.
More threads may be used for larger reduction or batch size.
4) Subsampling and time complexity: Subsampling may be
used in only some of the steps of Alg. 3, with the other
steps following Alg. 1. Whether to use subsampling or not in
each step depends on the trade-off between the computational
speed-up it brings and the approximations it makes. It is useful
to understand how complexity of OMF evolves with p. We
write s the average number of non-zero coefﬁcients in (αt)t
(s = k when Ω =

2
2). OMF complexity has three terms:

(i)

(ii)

(p k2): computation of the Gram matrix Gt, update of

O
the dictionary Dt with block coordinate descent,

(p k η): computation of βt = D(cid:62)

t−1xt and of ¯Bt

(cid:107) · (cid:107)

O
using xtα(cid:62)
t ,

(iii)

(k s2 η): computation of αt using Gt and βt, using

O
matrix inversion or elastic-net regression.

∼

Using subsampling turns p into q = p
r in the expressions
above. It improves single iteration time when the cost of re-
(k s2 η) is dominated by another term. This happens
gression
O
whenever p
r > s2, where r is the reduction factor used in the
algorithm. Subsampling can bring performance improvement
p
up to r
s2 . It can be introduced in either computations
from (i) or (ii), or both. When using small batch size, i.e.,
when η < k, computations from (i) dominates complexity, and
subsampling should be ﬁrst introduced in dictionary update (i),
and for code computation (ii) beyond a certain reduction ratio.
On the other hand, with large batch size η > k, subsampling
should be ﬁrst introduced in code computation, then in the
dictionary update step. The reasoning above ignore potentially
large constants. The best trade-offs in using subsampling must
be empirically determined, which we do in Section V.

B. Stochastic approximate majorization-minimization

The SOMF algorithm can be understood within the stochastic
majorization-minimization framework. The modiﬁcations that
we propose are indeed perturbations to the ﬁrst and third steps
of the SMM presented in Algorithm 2:

• The code is computed approximately:

the surrogate
is only an approximate majorizing surrogate of ft
near Dt−1.

• The surrogate objective is only reduced and not mini-
mized, due to the added constraint and the fact that we
perform only one pass of block coordinate descent.

We propose a new stochastic approximate majorization-
minimization (SAMM) framework handling these perturbations:
• A majorization step (12 – Alg. 2), computes an approx-
g(cid:63)
t , where gt is a

imate surrogate of ft near θt−1: gt
true upper-bounding surrogate of ¯ft.

≈

• A minimization step (13 – Alg. 2), ﬁnds θt by reducing
(cid:44) argminθ∈Θ ¯gt(θ),

θ(cid:63)
t

enough the objective ¯gt: θt
which implies ¯gt(θt) (cid:38) ¯gt(θ(cid:63)

≈
t ).

The SAMM framework is general, in the sense that approxima-
tions are not speciﬁed. The next section provides a theoretical

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

8

analysis of the approximation of SAMM and establishes how
SOMF is an instance of SAMM. It concludes by establishing
Proposition 1, which provides convergence guarantees for
SOMF, under the same assumptions made for OMF in [15].

IV. CONVERGENCE ANALYSIS
We establish the convergence of SOMF under reasonable
assumptions. For the sake of clarity, we ﬁrst state our principal
result (Proposition 1), that guarantees SOMF convergence. It is
a corollary of a more general result on SAMM algorithms. To
present this broader result, we recall the theoretical guarantees
of the stochastic majorization-minimization algorithm [10]
(Proposition 2); then, we show how the algorithm can with-
stand pertubations (Proposition 3). Proofs are reported in
Appendix A. SAMM convergence is proven before establishing
SOMF convergence as a corollary of this broader result.

A. Convergence of SOMF

Similar to [15, 34], we show that the sequence of iterates
(Dt)t asymptotically reaches a critical point of the empirical
risk (3). We introduce the same hypothesis on the code
covariance estimation ¯Ct as in [15] and a similar one on Gt —
they ensure strong convexity of the surrogate and boundedness
of (αt)t. They do not cause any loss of generality as they are
met in practice after a few iterations, if r is chosen reasonably
low, so that q > k. The following hypothesis can also be
guaranteed by adding small (cid:96)2 regularizations to ¯f .

(A) There exists ρ > 0 such that for all t > 0, ¯Ct, Gt

ρI.

(cid:31)

We further assume, that the weights (wt)t and (γc)c decay
at speciﬁc rates. We specify simple weight sequences, but the
proofs can be adapted for more complex ones.

(B) There exists u

(cid:0) 3
4 , 3u
for all t > 0, c > 0, wt = t−u, γc (cid:44) c−v.

( 11
12 , 1) and v

∈

∈

−

2) such that,

The following convergence result then applies to any se-
quence (Dt)t produced by SOMF, using estimators (b) or (c).
¯f is the empirical risk deﬁned in (3).

Proposition 1 (SOMF convergence). Under assumptions (A)
and (B), ¯f (Dt) converges with probability one and every limit
point D∞ of (Dt)t is a stationary point of ¯f : for all D

¯f (D∞, D

D∞)

0

∇

−
This result applies for any positive subsampling ratio r,
which may be set arbitrarily high. However, selecting a
reasonable ratio remains important for performance.

≥

Proposition 1 is a corollary of a stronger result on SAMM
algorithms. As it provides insights on the convergence mech-
anisms, we formalize this result in the following.

∈ C
(29)

B. Basic assumptions and results on SMM convergence

We ﬁrst recall the main results on stochastic majorization-
minimization algorithms, established in [10], under assump-
tions that we slightly tighten for our purpose. In our setting,
we consider the empirical risk minimization problem
(cid:16) ¯f (θ) (cid:44) 1
n

(cid:17)
f (θ, x(i))

min
θ∈Θ

n
(cid:88)

(30)

,

i=1

where f : RK

R is a loss function and

× X →

(C) Θ

RK and the support

of the data are compact.

⊂

X

This is a special case of (5) where the samples (xt)t are
i. The loss functions ft (cid:44)
x(i)
{
, xt) deﬁned on RK can be non-convex. We instead assume
·

drawn uniformly from the set
f (
that they meet reasonable regularity conditions:

}

(D) (ft)t is uniformly R-Lipschitz continuous on RK and

uniformly bounded on Θ.

(E) The directional derivatives [35]
¯f (θ, θ(cid:48)

∇
θ) exist for all θ and θ(cid:48) in RK.

ft(θ, θ(cid:48)

θ) and

−

∇

−

Assumption (E) allows to characterize the stationary points
0
Θ such that
of problem (30), namely θ
for all θ(cid:48)
Θ — intuitively a point is stationary when there
is no local direction in which the objective can be improved.
the deﬁnition of ﬁrst-order surrogate
functions used in the SMM algorithm. (gt)t are selected in
the set

ρ,L(ft, θt−1), hereby introduced.

Let us now recall

¯f (θ, θ(cid:48)

θ)

∇

−

≥

∈

∈

S

Deﬁnition 1 (First-order surrogate function). Given a function
f : RK
R, θ
ρ,L(f, θ) as
the set of functions g : RK

Θ and ρ, L > 0, we deﬁne

R such that

→

∈

S

→
• g is majorizing f on Θ and g is ρ-strongly convex,
• g and f are tight at θ — i.e., g(θ) = f (θ), g

differentiable,

f ) is L-Lipschitz,

(g

∇

−

(g

∇

−

In OMF, gt deﬁned in (15) is a variational surrogate4 of ft.
We refer the reader to [36] for further examples of ﬁrst-order
surrogates. We also ensure that ¯gt should be parametrized and
thus representable in memory. The following assumption is
met in OMF, as ¯gt is parametrized by the matrices ¯Ct and ¯Bt.

f is
−
f )(θ) = 0.

(F) Parametrized surrogates. The surrogates (¯gt)t are
RP . Namely,
such that ¯gt is unequivocally

parametrized by vectors in a compact set
for all t > 0, there exists κt
deﬁned as gt (cid:44) ¯gκt.

K ⊂

∈ K

Finally, we ensure that the weights (wt)t used in Alg. 2

decrease at a certain rate.

(G) There exists u

( 3
4 , 1) such that wt = t−u.

∈

When (θt)t is the sequence yielded by Alg. 2, the following
result (Proposition 3.4 in [10]) establishes the convergence
of ( ¯f (θt))t and states that θt is asymptotically a stationary
point of the ﬁnite sum problem (30), as a special case of the
expected risk minimization problem (5).

Proposition 2 (Convergence of SMM, from [10]). Under as-
sumptions (C) – (G), ( ¯f (θt))t≥1 converges with probability
one. Every limit point θ∞ of (θt)t is a stationary point of the
risk ¯f deﬁned in (30). That is,

θ
∀

∈

Θ,

∇

¯f (θ∞, θ

θ∞)

0.

≥

−

(31)

The correctness of the online matrix factorization algorithm

can be deduced from this proposition.

4In this case as in SOMF, gt is not ρ-strongly convex but ¯gt is, thanks to

assumption (A). This is sufﬁcient in the proofs of convergence.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

9

C. Convergence of SAMM

We now introduce assumptions on the approximations made
in SAMM, before extending the result of Proposition 2. We
make hypotheses on both the surrogate computation (ma-
jorization) step and the iterate update (minimization) step. The
principles of SAMM are illustrated in Figure 2, which provides
a geometric interpretation of the approximations introduced in
the following assumptions (H) and (I).

1) Approximate surrogate computation: The SMM algo-
rithm selects a surrogate for ft at point θt−1 within the set
ρ,L(ft, θt−1). Surrogates within this set are tight at θt−1
S
and greater than ft everywhere. In SAMM, we allow the use
of surrogates that are only approximately majorizing ft and
approximately tight at θt−1. This is indeed what SOMF does
when using estimators in the code computation step. For that
ρ,L(f, θ, (cid:15)), that contains all
purpose, we introduce the set
T
ρ,L(f, θ) for the (cid:96)∞-norm:
functions (cid:15)-close of a surrogate in

S

Deﬁnition 2 (Approximate ﬁrst-order surrogate function).
Given a function f : RK
R, θ
ρ,L(f, θ, (cid:15))
→
is the set of ρ-strongly convex functions g : RK
R
such that

Θ and (cid:15) > 0,

→

∈

T

• g is (cid:15)-majorizing f on Θ:
• g and f are (cid:15)-tight at θ — i.e., g(θ)

∈

κ

∀

Θ, g(κ)

−
f (θ)

f (κ)

is differentiable,

(g

∇

−

f ) is L-lipschitz.

−

(cid:15),
f

≥ −
(cid:15), g
−

≤

We assume that SAMM selects an approximative surrogate
ρ,L(ft, θt−1, (cid:15)t) at each iteration, where ((cid:15)t)t is a deter-
in
ministic or random non-negative sequence that vanishes at a
sufﬁcient rate.

T

∈ O

∞ 0 almost surely.

(H) For all t > 0, there exists (cid:15)t > 0 such that gt
∈
ρ,L(ft, θt−1, (cid:15)t). There exists a constant η > 0 such that
T
E[(cid:15)t]

(t2(u−1)−η) and (cid:15)t

→
As illustrated on Figure 2, given the OMF surrogate g(cid:63)
t ∈
ρ,L(ft, θt−1) deﬁned in (15), any function gt such that
S
ρ,L(ft, θt−1, (cid:15)) — e.g., where gt uses
gt
(cid:107)
an approximate αt in (15). This assumption can also be met in
matrix factorization settings with difﬁcult code regularizations,
that require to make code approximations.

∞ < (cid:15) is in

g(cid:63)
t (cid:107)

−

T

2) Approximate surrogate minimization: We do not re-
quire θt to be the minimizer of ¯gt any longer, but ensure that
the surrogate objective function ¯gt decreases “fast enough”.
Namely, θt obtained from partial minimization should be
closer to a minimizer of ¯gt than θt−1. We write (
t)t and
(
the ﬁltrations induced by the past of the algorithm,
Ft− 1
respectively up to the end of iteration t and up to the beginning
of the minimization step in iteration t. Then, we assume

F

)

t

2

(I) For all t > 0, ¯gt(θt) < ¯gt(θt−1). There exists µ > 0

such that, for all t > 0, where θ(cid:63)

t = argminθ∈Θ ¯gt(θ),

(1

µ)(¯gt(θt−1)

¯gt(θ(cid:63)

t )). (32)

E[¯gt(θt)

¯gt(θ(cid:63)
t )

]

|Ft− 1

2

≤

−

−
Assumption (I) is met by choosing an appropriate method
for the inner ¯gt minimization step — a large variety of
gradient-descent algorithms indeed have convergence rates of
the form (32). In SOMF, the block coordinate descent with
frozen coordinates indeed meet this property, relying on results

−

Fig. 2. Both steps of SAMM make well-behaved approximations. The
operations that are performed in exact SMM are in green and superscripted
by (cid:63), while the actual computed values are in orange. Light bands recall the
bounds on approximations assumed in (H) and (I).

from [37]. When both assumptions are met, SAMM enjoys the
same convergence guarantees as SMM.

3) Asymptotic convergence guarantee: The following
proposition guarantees that the stationary point condition of
Proposition 2 holds for the SAMM algorithm, despite the use
of approximate surrogates and approximate minimization.

Proposition 3 (Convergence of
tions (C) – (I),
for SAMM.

SAMM). Under assump-
the conclusion of Proposition 2 holds

Assumption (H) is essential to bound the errors introduced
by the sequence ((cid:15)t)t in the proof of Proposition 3, while (I)
is the key element to show that the sequence of iterates (θt)t
is stable enough to ensure convergence. The result holds for
any subsampling ratio r, provided that (A) remains true.

4) Proving SOMF convergence: Assumptions (A) and (B)
readily implies (C)–(G). With Proposition 3 at hand, proving
Proposition 1 reduces to ensure that the surrogate sequence of
SOMF meets (H) while its iterate sequence meets (I).

V. EXPERIMENTS

The SOMF algorithm is designed for datasets with large
number of samples n and large dimensionality p. Indeed,
as detailed in Section III-A, subsampling removes the com-
putational bottlenecks that arise from high dimensionality.
Proposition 1 establishes that the subsampling used in SOMF
is safe, as it enjoys the same guarantees as OMF. However,
as with OMF, no convergence rate is provided. We therefore
perform a strong empirical validation of subsampling.

We tackle two different problems, in functional Magnetic
Resonance Imaging (fMRI) and hyperspectral imaging. Both
involve the factorization of very large matrices X with sparse
factors. As the data we consider are huge, subsampling reduces
the time of a single iteration by a factor close to p
q . Yet it
is also much redundant: SOMF makes little approximations
and accessing only a fraction of the features per iteration
should not hinder much the reﬁnement of the dictionary. Hence
high speed-ups are expected — and indeed obtained. All
experiments can be reproduced using open-source code.

A. Problems and datasets

1) Functional MRI: Matrix factorization has long been
used on functional Magnetic Resonance Imaging [18]. Data
are temporal series of 3D images of brain activity and are

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

10

decomposed into spatial modes capturing regions that activate
synchronously. They form a matrix X where columns are
the 3D images, and rows corresponds to voxels. Interesting
dictionaries for neuroimaging capture spatially-localized com-
ponents, with a few brain regions. This can be obtained by
enforcing sparsity on the dictionary: we use an (cid:96)2 penalty and
the elastic-net constraint. SOMF streams subsampled 3D brain
records to learn the sparse dictionary D. Data can be huge:
106 (2000
we use the whole HCP dataset [38], with n = 2.4
105, totaling 2 TB
records, 1 200 time points) and p = 2
of dense data. For comparison, we also use a smaller public
dataset (ADHD200 [39]) with 40 records, n = 7000 samples
104 voxels. Historically, brain decomposition have
and p = 6
been obtained by minimizing the classical dictionary learning
objective on transposed data [40]: the code A holds sparse
spatial maps and voxel time-series are streamed. This is not a
natural streaming order for fMRI data as X is stored column-
wise on disk, which makes the sparse dictionary formulation
more appealing. Importantly, we seek a low-rank factorization,
to keep the decomposition interpretable — k

100

p.

·

·

·

2) Hyperspectral imaging: Hyperspectral cameras acquire
images with many channels that correspond to different spec-
tral bands. They are used heavily in remote sensing (satellite
imaging), and material study (microscopic imaging). They
yield digital images with around 1 million pixels, each as-
sociated with hundreds of spectral channels. Sparse matrix
factorization has been widely used on these data for image
classiﬁcation [41, 42] and denoising [43, 44]. All methods
rely on the extraction of full-band patches representing a local
image neighborhood with all channels included. These patches
are very high dimensional, due to the number of spectral
bands. From one image of the AVIRIS project [45], we extract
106 patches of size 16
n = 2
16 with 224 channels, hence
·
104. A dense dictionary is learned from these patches. It
p = 6
·
should allow a sparse representation of samples: we either use
the classical dictionary learning setting ((cid:96)1/elastic-net penalty),
or further add positive constraints to the dictionary and codes:
both methods may be used and deserved to be benchmarked.
p.
We seek a dictionary of reasonable size: we use k

256

×

∼

(cid:28)

∼

(cid:28)

B. Experimental design

To validate the introduction of subsampling and the useful-

ness of SOMF, we perform two major experiments.

• We measure the performance of SOMF when increasing
the reduction factor, and show beneﬁts of stochastic
dimension reduction on all datasets.

• We assess the importance of subsampling in each of
the steps of SOMF. We compare the different approaches
proposed for code computation.

Validation: We compute the objective function (3) over a
test set to rule out any overﬁtting effect — a dictionary should
be a good representation of unseen samples. This criterion is
always plotted against wall-clock time, as we are interested in
the performance of SOMF for practitioners.

Tools: To perform a valid benchmark, we implement OMF
and SOMF using Cython [46] We use coordinate descent [47]
to solve Lasso problems with optional positivity constraints.

TABLE II
SUMMARY OF EXPERIMENTAL SETTINGS

Field

Dataset

Functional MRI

Hyperspectral imaging

ADHD

HCP

Patches from AVIRIS

Factors
# samples n
# features p
X size
Use case ex.

D sparse, A dense
2 · 106
2 · 105
2 TB

7 · 103
6 · 104
2 GB
Extracting predictive feature

D dense, A sparse
2 · 106
6 · 104
103 GB
Recognition / denoising

Code computation is parallelized to handle mini-batches. Ex-
periments use scikit-learn [48] for numerics, and nilearn [49]
for handling fMRI data. We have released the code in an open-
source Python package5. Experiments were run on 3 cores of
¯Bt is
an Intel Xeon 2.6GHz, in which case computing P⊥
t
faster than updating PtDt.

Parameter setting: Setting the number of components k
and the amount of regularization λ is a hard problem in the
absence of ground truth. Those are typically set by cross-
validation when matrix factorization is part of a supervised
pipeline. For fMRI, we set k = 70 to obtain interpretable
networks, and set λ so that the decomposition approximately
covers the whole brain (i.e., every map is k
70 ) sparse). For
hyperspectral images, we set k = 256 and select λ to obtain
a dictionary on which codes are around 3% sparse. We cycle
randomly through the data (fMRI records, image patches) until
convergence, using mini-batches of size η = 200 for HCP
and AVIRIS, and η = 50 for ADHD (small number of sam-
ples). Hyperspectral patches are normalized in the dictionary
learning setting, but not in the non-negative setting — the
classical pre-conditioning for each case. We use u = 0.917
and v = 0.751 for weight sequences.

C. Reduction brings speed-up at all data scales

We benchmark SOMF for various reduction factors against
the original online matrix factorization algorithm OMF [15],
on the three presented datasets. We stream data in the same
order for all reduction factors. Using variant (c) (true Gram
matrix, averaged βt) performs slightly better on fMRI datasets,
whereas (b) (averaged Gram matrix and βt) is slightly faster
for hyperspectral decomposition. For comparison purpose, we
display results using estimators (b) only.

Figure 3 plots the test objective against CPU time. First,
we observe that all algorithms ﬁnd dictionaries with very
close objective function values for all reduction factors, on
each dataset. This is not a trivial observation as the matrix
factorization problem (3) is not convex and different runs of
OMF and SOMF may converge towards minima with different
values. Second, and most importantly, SOMF provides signif-
icant improvements in convergence speed for three different
sizes of data and three different factorization settings. Both ob-
servations conﬁrm the relevance of the subsampling approach.
Quantitatively, we summarize the speed-ups obtained in
Table III. On fMRI data, on both large and medium datasets,
SOMF provides more than an order of magnitude speed-up.

5https://github.com/arthurmensch/modl

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

11

Fig. 3. Subsampling provides signiﬁcant speed-ups on all fMRI and hyperspectral datasets. A reduction factor of 12 is a good overall choice. With larger
data, larger reduction factors can be used for better performance — convergence is reached 13× faster than state-of-the-art methods on the 2TB HCP dataset.

TABLE III
TIME TO REACH CONVERGENCE (< 1% TEST OBJECTIVE)

Dataset

ADHD

Algorithm OMF

SOMF OMF

AVIRIS (NMF) AVIRIS (DL)
SOMF OMF

SOMF OMF

SOMF

HCP

Conv. time 6 min 28 s 2 h 30 43 min 1 h 16 11 min 3 h 50 17 min
Speed-up

13.31

3.36

6.80

11.8

Fig. 5. Proﬁling OMF and SOMF on HCP. Partial dictionary update removes
the major bottleneck of online matrix factorization for small reductions. For
higher reduction, parameter update and code computation must be subsampled
to further reduce the iteration time.

Our ﬁrst experiment establishes the power of stochastic
subsampling as a whole. In the following two experiments, we
reﬁne our analysis to show that subsampling is indeed useful
in the three steps of online matrix factorization.

D. For each step of SOMF, subsampling removes a bottleneck

∼

In Section III, we have provided theoretical guidelines on
when to introduce subsampling in each of the three steps of
an iteration of SOMF. This analysis predicts that, for η
k,
we should ﬁrst use partial dictionary update, before using
approximate code computation and asynchronous parameter
aggregation. We verify this by measuring the time spent by
SOMF on each of the updates for various reduction factors,
on the HCP dataset. Results are presented in Figure 5. We
observe that block coordinate descent is indeed the bottle-
neck in OMF. Introducing partial dictionary update removes
this bottleneck, and as the reduction factor increases, code
computation and surrogate aggregation becomes the major
bottlenecks. Introducing subsampling as described in SOMF
overcomes these bottlenecks, which rationalizes all steps of
SOMF from a computational point of view.

E. Code subsampling becomes useful for high reduction

It remains to assess the performance of approximate code
computation and averaging techniques used in SOMF. Indeed,

Fig. 4. Given a 3 minute time budget, the atoms learned by SOMF are
more focal and less noisy that those learned by OMF. They are closer to
the dictionary of ﬁrst line, for which convergence has been reached.

Practitioners working on datasets akin to HCP can decompose
their data in 20 minutes instead of 4 h previously, while
working on a single machine. We obtain the highest speed-ups
for the largest dataset — accounting for the extra redundancy
that usually appears when dataset size increase. Up to r
8,
speed-up is of the order of r — subsampling induces little
noise in the iterate sequence, compared to OMF. Hyperspectral
decomposition is performed near 7
faster than with OMF in
the classical dictionary learning setting, and 3
in the non-
negative setting, which further demonstrates the versatility of
SOMF. Qualitatively, given a certain time budget, Figure 4
compares the results of OMF and the results of SOMF with
a subsampling ratio r = 24, in the non-negative setting. Our
algorithm yields a valid smooth bank of ﬁlters much faster.
The same comparison has been made for fMRI in [26].

∼

×

×

Comparison with stochastic gradient descent: It is possible
to solve (3) using the projected stochastic gradient (SGD)
algorithm [50]. On all
tested settings, for high precision
convergence, SGD (with the best step-size among a grid) is
slower than OMF and even slower than SOMF. In the dictionary
learning setting, SGD is somewhat faster than OMF but slower
than SOMF in the ﬁrst epochs. Compared to SOMF and OMF,
SGD further requires to select the step-size by grid search.

Limitations: Table III reports convergence time within 1%,
which is enough for application in practice. SOMF is less
beneﬁcial when setting very high precision: for convergence
within 0.01%, speed-up for HCP is 3.4. This is expected
as SOMF trades speed for approximation. For high precision
convergence, the reduction ratio can be reduced after a few
epochs. As expected, there exists an optimal reduction ratio,
depending on the problem and precision, beyond which per-
formance reduces: r = 12 yields better results than r = 24 on
AVIRIS (dictionary learning) and ADHD, for 1% precision.

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

12

Fig. 6. Approximating code computation with the proposed subsampling method further accelerates the convergence of SOMF. Reﬁning code computation
using past iterations (averaged estimates) performs better than simply performing a subsampled linear regression as in [26]

subsampling for code computation introduces noise that may
undermine the computational speed-up. To understand the
impact of approximate code computation, we compare three
strategies to compute (αt)t on the HCP dataset. First, we
compute (α(cid:63)
t )t from (xt)t using (21). Subsampling is thus
used only in dictionary update. Second, we rely on masked,
non-consistent estimators (a), as in [26] — this breaks conver-
gence guarantees. Third, we use averaged estimators (βt, Gt)
from (c) to reduce the variance in (αt)t computation.

∈ {

Fig. 6 compares the three strategies for r

12, 24
.
}
Partial minimization at each step is the most important part
to accelerate convergence: subsampling the dictionary updates
already allows to outperforms OMF. This is expected, as
dictionary update constitutes the main bottleneck of OMF in
large-scale settings. Yet, for large reduction factors, using
subsampling in code computation is important
to further
accelerate convergence. This clearly appears when comparing
the plain and dashed black curves. Using past estimates to
better approximate (αt)t yields faster convergence than the
non-converging, masked loss strategy (a) proposed in [26].

VI. CONCLUSION

In this paper, we introduce SOMF, a matrix-factorization
algorithm that can handle input data with very large number of
rows and columns. It leverages subsampling within the inner
loop of a streaming algorithm to make iterations faster and
accelerate convergence. We show that SOMF provides a sta-
tionary point of the non-convex matrix factorization problem.
To prove this result, we extend the stochastic majorization-
minimization framework to two major approximations. We
assess the performance of SOMF on real-world large-scale
problems, with different sparsity/positivity requirements on
learned factors. In particular, on fMRI and hyperspectral data
decomposition, we show that
the use of subsampling can
speed-up decomposition up to 13 times. The larger the dataset,
the more SOMF outperforms state-of-the art techniques, which
is very promising for future applications. This calls for adap-
tation of our approach to learn more complex models.

APPENDIX A
PROOFS OF CONVERGENCE

This appendix contains the detailed proofs of Proposition 3
and Proposition 1. We ﬁrst introduce three lemmas that will be
crucial to prove SAMM convergence, before establishing it by
proving Proposition 3. Finally, we show that SOMF is indeed

∇

an instance of SAMM (i.e. meets the assumptions (C)–(I)),
proving Proposition 1.

A. Basic properties of the surrogates, estimate stability

We derive an important result on the stability and optimality
of the sequence (θt)t, formalized in Lemma 3 — introduced
in the main text. We ﬁrst introduce a numerical lemma on
the boundedness of well-behaved determistic and random
sequence. The proof is detailed in Appendix B.

R, t0

Lemma 1 (Bounded quasi-geometric sequences). Let (xt)t be
a sequence in R+, u : R
R
N and α
[0, 1)
→
αxt−1 + u(xt, xt−1), where
such that, for all t
. Then (xt)t is bounded.
u(x, y)

≤
→ ∞
Let now (Xt)t be a random sequence in R+, such that
t)t the ﬁltration adapted to (Xt)t.

∈
E[Xt] <
If, for all t > t0, there exists a σ-algebra

o(x + y) for x, y

. We deﬁne (

t(cid:48) such that

×
t0, xt

∞

t−1

≥

F

∈

∈

F

F

⊆

t(cid:48)

F

⊆ F

t and

E[Xt

t(cid:48)]

αXt−1 + u(Xt, Xt−1),

(33)

|F
then (Xt)t is bounded almost surely.

≤

We ﬁrst derive some properties of the approximate surrogate

functions used in SAMM. The proof is adapted from [10].

Lemma 2 (Basic properties of approximate surrogate func-
tions). Consider any sequence of iterates (θt)t and assume
L,ρ(ft, θt−1, (cid:15)) for all
there exists (cid:15) > 0 such that gt
1, ¯h0 (cid:44) h0 and
t
≥
¯ht (cid:44) (1
(i) (

∈ T
ft for all t
wt)¯ht−1 + wtht. Under assumptions (D) – (G),
−
ht(θt−1))t>0 is uniformly bounded and there exists

1. Deﬁne ht (cid:44) gt

−

≥

(ii) (ht)t and (¯ht)t are uniformly R(cid:48)-Lipschitz, (gt)t and

ht

}t is uniformly bounded by R(cid:48).

∇
R(cid:48) such that

{∇

(¯gt)t are uniformly (R + R(cid:48))-Lipschitz.

Proof. We ﬁrst prove (i). We set α > 0 and deﬁne θ(cid:48) =
. As ht has a L-Lipschitz gradient on RK,
θt
using Taylor’s inequality (see Appendix B)

α ∇ht(θt)
(cid:107)∇ht(θt)(cid:107)2

−

(34)

ht(θ(cid:48))

ht(θt)

α

−

≤

Lα2
2

2 +

ht(θt)
(cid:107)∇
(cid:107)
ht(θ(cid:48))) +

(cid:107)∇

1
α

ht(θt)
2
(cid:107)

≤
where we use ht(θt) < (cid:15) and
assumption gt

Lα
Lα
2
2 ≤
ht(θ(cid:48)
(cid:15) from the
t)
L,ρ(ft, θt−1, (cid:15)). Moreover, by deﬁnition,

(ht(θt)

2
α

(cid:15) +

−

−

≤

,

ht exists and is L-lipschitz for all t. Therefore,

t

1,

∈ T

ht(θ)
2
(cid:107)

(cid:107)∇

≤ (cid:107)∇

2 + L
ht(θt)
(cid:107)
(cid:107)

θt−1

−

≥

∀
(cid:107)2

θ

(35)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

13

Since Θ is compact and (

(cid:107)2)t≥1 is bounded in (34),
ht(θt)
ht is bounded by R(cid:48) independent of t. (ii) follows by basic

(cid:107)∇

where the second inequality holds for the same reasons as
in (41). Injecting (40) and (42) in (43), we obtain

∇
considerations on Lipschitz functions.

Finally, we prove a result on the stability of the estimates,
that derives from combining the properties of (gt)t and the
geometric decrease assumption (I).

Lemma 3 (Estimate stability under SAMM approximation). In
the same setting as Lemma 2, with the additional assump-
tion (I) (expected linear decrease of ¯gt suboptimality), the
2 converges to 0 as fast as (wt)t, and θt
sequence
(cid:107)
is asymptotically an exact minimizer. Namely, almost surely,

θt
(cid:107)

θt−1

−

θt
(cid:107)

−

θt−1

(cid:107)2 ∈ O

(wt) and ¯gt(θt)

¯gt(θ(cid:63)
t )

−

∈ O

(w2

t ).

(36)

Proof. We ﬁrst establish the result when a deterministic ver-
sion of (I) holds, as it makes derivations simpler to follow.

1) Determistic decrease rate: We temporarily assume that

decays are deterministic.

(Idet) For all t > 0, ¯gt(θt) < ¯gt(θt−1). Moreover, there

exists µ > 0 such that, for all t > 0
¯gt(θ(cid:63)
t )
(1
θ(cid:63)
t = argmin

−
where

¯gt(θt)

≤

−

µ)(¯gt(θt−1)
¯gt(θ),

θ∈Θ

¯gt(θ(cid:63)

t ))

−

We introduce the following auxiliary positive values, that

we will seek to bound in the proof:

At (cid:44)
θt
(cid:107)
−
θ(cid:63)
θ(cid:63)
t−1(cid:107)
t −
(cid:107)

Ct (cid:44)

2, Bt (cid:44)
θt
θt−1
(cid:107)
(cid:107)
2, Dt (cid:44) ¯gt(θt)

θ(cid:63)
2,
t (cid:107)
−
¯gt(θ(cid:63)
t ).

−

Our goal is to bound At. We ﬁrst relate it to Ct and Bt using
convexity of (cid:96)2 norm:

A2

t + 3B2
t is the minimizer of ¯gt, by strong convexity of (¯gt)t,

t−1 + 3C 2
t .

3B2

t ≤

(39)

As θ(cid:63)

ρ
2
while we also have
ρ
θ(cid:63)
2
2 ≤
t−1(cid:107)
2 (cid:107)
wt)(cid:0)¯gt−1(θ(cid:63)

θ(cid:63)
t −
(1

≤

−

B2

t =

ρ
2 (cid:107)

θt

2
θ(cid:63)
2 ≤
t (cid:107)

−

Dt,

¯gt(θ(cid:63)

t−1)

t−1)

−

¯gt(θ(cid:63)
t )
−
t )(cid:1)+wt
¯gt−1(θ(cid:63)

(cid:0)gt(θ(cid:63)

gt(θ(cid:63)

t )(cid:1)

(41)

t−1)
2Q
ρ

−
.

2, and thus Ct

wt

wt(R + R(cid:48))

θ(cid:63)
t −
(cid:107)

θ(cid:63)
t−1(cid:107)

≤
≤
The second inequalities holds because θ(cid:63)
t−1 is a minimizer
of ¯gt−1 and gt is Q-Lipschitz, where Q (cid:44) R + R(cid:48), using
Lemma 2. Replacing (40) and (41) in (39) yields

A2

(Dt + Dt−1) +

t ≤
and we are left to show that Dt
this, we decompose the inequality from (Idet) into

∈ O

w2
t ,

12Q2
ρ
(w2
t ) to conclude. For

(42)

6
ρ

Dt

≤
= (1

(1

µ)(¯gt(θt−1)
−
(cid:16)
(cid:0)gt(θt−1)

−
µ)

t ))

¯gt(θ(cid:63)
gt(θt)(cid:1) + wt

(cid:0)gt(θt)

gt(θ(cid:63)

t )(cid:1)(cid:17)

−
+ (1

µ)

(1

−

−
+ (1

−

−
wt)(cid:0)¯gt−1(θt−1)
wt)(cid:0)¯gt−1(θ(cid:63)

−
t−1)

¯gt−1(θ(cid:63)

−
t−1)(cid:1)
t )(cid:1)(cid:17)
¯gt−1(θ(cid:63)

−

wt
(cid:16)

(1

µ)(wtQ(At + Bt) + Dt−1),

≤

−

(37)

(38)

(40)

w2
t−1
w2
t

(44)

˜Dt

(1

µ) ˜Dt−1

+ u( ˜Dt, ˜Dt−1),

−

≤
where we deﬁne ˜Dt (cid:44) Dt
w2
t
gebraic details in Appendix B) that
u( ˜Dt, ˜Dt−1)
determistictic result of Lemma 1,
bounded, which combined with (40) allows to conclude.

is easy to show (see al-
the perturbation term
. Using the
is

o( ˜Dt + ˜Dt−1) if ˜Dt

this ensures that ˜Dt

→ ∞

. It

∈

2) Stochastic decrease rates: In the general case (I), the
inequalities (40), (41) and (42) holds, and (44) is replaced by

E[ ˜Dt

|Ft− 1

2

]

≤

(1

−

µ) ˜Dt−1

w2
t−1
w2
t

+ u( ˜Dt, ˜Dt−1),

(45)

Taking the expectation of this inequality and using Jensen
inequality, we show that (43) holds when replacing ˜Dt by
E[ ˜Dt]. This shows that E[Dt]
.
∞
The result follows from Lemma 1, that applies as
⊆
Ft− 1

t ) and thus E[Dt] <
t−1

2 ⊆ F

(w2

∈ O

F

t.

B. Convergence of SAMM — Proof of Proposition 3

We now proceed to prove the Proposition 3, that extends
the stochastic majorization-minimization framework to allow
approximations in both majorization and minimizations steps.

Proof of Proposition 3. We adapt the proof of Proposition 3.3
from [10] (reproduced as Proposition 2 in our work). Relaxing
tightness and majorizing hypotheseses introduces some extra
error terms in the derivations. Assumption (H) allows to
control these extra terms without breaking convergence. The
stability Lemma 3 is important in steps 3 and 5.

1) Almost sure convergence of (¯gt(θt)): We control the
positive expected variation of (gt(θt))t
is
a converging quasi-martingale. By construction of ¯gt and
ρ,L(ft, θt−1, (cid:15)t), where (cid:15)t
properties of the surrogates gt
is a non-negative sequence that meets (H),

to show that

∈ T

it

¯gt−1(θt−1)

¯gt(θt)
= (¯gt(θt)

−

−

wt(gt(θt−1)
wt(gt(θt−1)
+ wt( ¯ft−1(θt−1)
wt(ft(θt−1)

−

−

≤

≤

≤

−

¯gt(θt−1)) + wt(gt(θt−1)
¯gt−1(θt−1))
ft(θt−1)) + wt(ft(θt−1)

−

¯gt−1(θt−1))

¯ft−1(θt−1))

−

¯gt−1(θt−1))

−

¯ft−1(θt−1)) + wt(¯(cid:15)t−1 + (cid:15)t),

(46)

where the average error sequence (¯(cid:15)t)t is deﬁned recursively:
¯(cid:15)0 (cid:44) (cid:15)0 and ¯(cid:15)t (cid:44) (1
wt)(cid:15)t−1 +wt(cid:15)t. The ﬁrst inequality uses
¯gt(θt−1). To obtain the forth inequality we observe
¯gt(θt)
≤
ft(θt−1) < (cid:15)t by deﬁnition of (cid:15)t and ¯ft(θt−1)
gt(θt−1)
−
¯(cid:15)t, which can easily be shown by induction on t.
¯gt(θt−1)
t−1,
Then, taking the conditional expectation with respect to

−
≤

−

F

E[¯gt(θt)

−
wt sup
θ∈Θ |

≤

¯gt−1(θt−1)
f (θ)

|F
¯ft−1(θ)

t−1]

−

|

+ wt(¯(cid:15)t−1 + E[(cid:15)t

t−1]).

(47)

|F

We have used the fact that (cid:15)t−1 is deterministic with respect
t−1. To ensure convergence, we must bound both terms
to
in (47): the ﬁrst term is the same as in the original proof

F

(43)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

14

with exact surrogate, while the second is the perturbative
term introduced by the approximation sequence ((cid:15)t)t. We use
Lemma B.7 from [10], issued from the theory of empirical
processes: E[supθ∈Θ |
(wtt1/2), and thus
wtE[sup
f (θ)
θ∈Θ |

¯ft−1(θ)
] =
O
|
∞
(cid:88)
t1/2w2

−
¯ft−1(θ)
] < C
|

∞
(cid:88)

f (θ)

t <

(48)

∞

−

t=1

t=1

where C is a constant, as t1/2w2
t = t1/2−2u and u > 3/4
from (G). Let us now focus on the second term of (47).
Deﬁning, for all 1

t, wt

wj),

(cid:81)t

i

i = wi

≤

≤
t
(cid:88)

i=1

E[¯(cid:15)t] =

wt
i

E[(cid:15)t]

wt

≤

−

j=i+1(1
t
(cid:88)

E[(cid:15)t].

i=1

(49)

−

1)

η >

We set η > 0 so that 2(u
ensures E[(cid:15)t]
partial sum (cid:80)t

1. Assumption (H)
−
(t2(u−1)−η), which allows to bound the
∈ O
E[(cid:15)i]
i=1
wtE[¯(cid:15)t−1 + E[(cid:15)t
(cid:16) t
(cid:88)

−
(t2u−1−η). Therefore
t−1]] = wtE[(cid:15)t−1] + wtE[(cid:15)t]

∈ O

|F
(cid:17)
E[(cid:15)t]

+ wtE[(cid:15)t]

(50)

At2u−2u−1−η + Bt2u−u−2−η

Ct−1−η,

≤

where we use u < 1 on the third line and the deﬁnition of
(wt)t on the second line. Thus (cid:80)∞
t−1]] <
. We use quasi-martingale theory to conclude, as in [10]. We
∞
deﬁne the variable δt to be 1 if E[¯gt(θt)
¯gt−1(θt−1)
0, and 0 otherwise. As all terms of (47) are positive:
∞
(cid:88)

t=1 wtE[¯(cid:15)t−1+E[(cid:15)t

t−1]

|F

|F

≥

−

E[δt(¯gt(θt)

¯gt−1(θt−1))]

−

w2
t

i=1

≤

≤

t=1

=

∞
(cid:88)

t=1
∞
(cid:88)

E[δtE[¯gt(θt)

¯gt−1(θt−1)

t−1]]

−

|F

(51)

+ ¯(cid:15)t−1 + E[(cid:15)t

t−1]

] <
|

.

wtE[sup
θ∈Θ |

f (θ)

¯ft−1(θ)
|

−

t=1

|F

≤
∞
As ¯gt are bounded from below ( ¯ft is bounded from (D) and
we easily show that ¯(cid:15)t is bounded), we can apply Theorem
A.1 from [10], that is a quasi-martingale convergence theorem
originally found in [51]. It ensures that (gt(θt))t≥1 converges
almost surely to an integrable random variable g(cid:63), and that
(cid:80)∞

t=1
|F
2) Almost sure convergence of ¯f (θt): We rewrite the second

E[¯gt(θt)
E[
|

almost surely.

¯gt−1(θt−1)

] <
|

t−1]

∞

−

inequality of (46), adding ¯(cid:15)t on both sides:
(cid:1)
(cid:0)ft(θt−1)

¯ft−1(θt−1) + ¯(cid:15)t−1

0

−
ft(θt−1)(cid:1) + wt
¯gt(θt)(cid:1) + wt¯(cid:15)t−1
−
¯ft−1(θt−1)(cid:1) + (cid:0)¯gt−1(θt−1)

−

≤

(cid:0)¯gt−1(θt−1)
wt
(cid:0)gt(θt−1)
wt
≤
−
+ (cid:0)¯gt−1(θt−1)
(cid:0)ft(θt−1)
wt
+ wt((cid:15)t + ¯(cid:15)t−1),

−

≤

¯ft−1(θt−1)(cid:1)

¯gt(θt)(cid:1)

−

(52)

where the left side bound has been obtained in the last
paragraph by induction and the right side bound arises from
the deﬁnition of (cid:15)t. Taking the expectation of (52) conditioned
on

t−1, almost surely,

F

0

≤

−

wt(f (θt−1)
E[¯gt(θt)

¯ft−1(θt−1))
−
¯gt−1(θt−1)

−

|F

t−1] + wt(¯(cid:15)t−1 + E[(cid:15)t

(53)
t−1]),

|F

t−1]
|

|F
(cid:0)f (θt−1)

We separately study the three terms of the previous upper
bound. The ﬁrst two terms can undergo the same analysis as
E(cid:2)
in [10]. First, almost sure convergence of (cid:80)∞
E[¯gt(θt)
t=1
−
|
(cid:3) implies that E(cid:2)¯gt(θt)
(cid:3)
¯gt−1(θt−1)
¯gt−1(θt−1)
t−1
is the summand of an almost surely converging sum. Second,
¯ft−1(θt−1)(cid:1) is the summand of an absolutely
wt
converging sum with probability one, less it would contra-
dict (48). To bound the third term, we have once more
the perturbation introduced by ((cid:15)t)t. We have
to control
(cid:80)∞
almost surely, otherwise

t=1 wt¯(cid:15)t−1 + wtE[(cid:15)t

t−1] <

|F

−

−

Fubini’s theorem would invalidate (50).

|F

∞

As the three terms are the summand of absolutely converg-
¯ft−1(θt−1)+¯(cid:15)t−1)
ing sums, the positive term wt(¯gt−1(θt−1)
is the summand of an almost surely convergent sum. This is
not enough to prove that ¯ht(θt) (cid:44) ¯gt(θt)
∞ 0, hence
we follow [10] and make use of its Lemma A.6. We deﬁne
Xt (cid:44) ¯ht−1(θt−1) + ¯(cid:15)t−1. As (H) holds, we use Lemma 3,
which ensures that (¯ht)t≥1 are uniformly R(cid:48)-Lipschitz and
θt
(cid:107)

−
¯ft(θt)

θt−1

2 =

→

O

−

−
Xt+1
|
R(cid:48)

≤

≤ O

(cid:107)
Xt

−
θt
(cid:107)
−
(wt) +

| ≤ |
θt−1
¯(cid:15)t
|

−

(wt). Hence,
¯ht(θt)
−
¯(cid:15)t
2 +
|
(cid:107)
,
¯(cid:15)t−1

−

|

¯ht−1(θt−1)
¯(cid:15)t−1
,
|
θt
as
(cid:107)

−

+

¯(cid:15)t

¯(cid:15)t−1

|

|

−

|
as ¯ht is R(cid:48)-Lipschitz
θt−1

(wt)

2 =
(cid:107)

O

(54)

From assumption (H), ((cid:15)t)t and (¯(cid:15)t)t are bounded. Therefore
)
¯(cid:15)t

(wt) and hence

¯(cid:15)t−1

¯(cid:15)t−1

+

(cid:15)t

|

−

wt(
|

| ≤

|
|
Xt+1
|

∈ O

|
Xt

−

| ≤ O

(wt).

(55)

Lemma A.6 from [10] then ensures that Xt converges
to zero with probability one. Assumption (H) ensures that
∞ 0 almost surely, from which we can easily deduce
(cid:15)t
∞ 0 almost surely. Therefore ¯ht(θt)
0 with probability
¯(cid:15)t
one and ( ¯ft(θt))t≥1 converges almost surely to g(cid:63).

→
→
3) Almost sure convergence of ¯f (θt): Lemma B.7 of [10],
based on empirical process theory [33], ensures that ¯ft uni-
formly converges to ¯f . Therefore, ( ¯f (θt))t≥1 converges almost
surely to g(cid:63).

→

4) Asymptotic stationary point condition: Preliminary to
the ﬁnal result, we establish the asymptotic stationary point
condition (57) as in [10]. This requires to adapt the original
proof to take into account the errors in surrogate computation
¯ht is L-
and minimization. We set α > 0. By deﬁnition,
Lipschitz over RK. Following the same computation as in (34),
we obtain, for all α > 0,

∇

≤

2
(cid:107)

2
α

¯(cid:15)t +

¯ht(θt)

(cid:107)∇
¯ht(θ)
where we use
|
the inequality (56) is true for all α,
surely. From the strong convexity of ¯gt and Lemma 3,
θ(cid:63)
t (cid:107)

2 converges to zero, which ensures

Lα
2
RK. As ¯(cid:15)t
∈
¯ht(θt)
2
(cid:107)
(cid:107)∇

¯(cid:15)t for all θ

| ≤

→

,

0 and
∞ 0 almost

→

θt

(cid:107)

−

(56)

¯ht(θ(cid:63)
t )

2
(cid:107)

¯
∇

ht(θt)

θt
2 + L
(cid:107)
(cid:107)

θ(cid:63)
2
t (cid:107)

−

≤ (cid:107)

(cid:107)∇
5) Parametrized surrogates: We use assumption (F) to
ﬁnally prove the property, adapting the proof of Proposition 3.4
in [10]. We ﬁrst recall the derivations of [10] for obtaining (58)
We deﬁne (κt)t such that ¯gt = gκt for all t > 0. We assume
that θ∞ is a limit point of (θt)t. As Θ is compact, there

→

∞ 0.

(57)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

15

K

exists an increasing sequence (tk)k such that (θtk )k converges
toward θ∞. As
is compact, a converging subsequence of
(κtk )k can be extracted, that converges towards κ∞
.
∈ K
From the sake of simplicity, we drop subindices and assume
κ∞.
without loss of generality that θt
From the compact parametrization assumption, we easily show
that (¯gκt)t uniformly converges towards ¯g∞ (cid:44) ¯gκ∞. Then,
deﬁning ¯h∞ = ¯g∞

¯f , for all θ

θ∞ and κt

Θ,

→

→

¯h∞(θ∞, θ

− ∇

−

θ∞)
(58)
Θ. We
∈
θ(cid:63)
2
t (cid:107)
−

¯f (θ∞, θ

∇

−

θ∞) =

¯g∞(θ∞, θ

θ∞)

−

∇
¯f (θ∞, θ

∈

−

≥

−

∇

t →

We ﬁrst show that
consider the sequence (θ(cid:63)
0, which implies θ(cid:63)
¯g∞, which implies (¯gt(θ(cid:63)
minimizes ¯gt, for all t > 0 and θ
This implies ¯g∞(θ∞)
for t

θ∞)
0 for all θ
t )t. From Lemma 3,
θt
→
(cid:107)
θ∞. ¯gt converges uniformly towards
¯g∞(θ∞). Furthermore, as θ(cid:63)
t ))t →
t
Θ, ¯gt(θ(cid:63)
¯gt(θ).
t )
inf θ∈Θ ¯g∞(θ) by taking the limit
. Therefore θ∞ is the minimizer of ¯g∞ and thus

∇
around θ(cid:63)

→ ∞
¯g∞(θ∞, θ
Adapting [10], we perform the ﬁrst-order expansion of ¯ht
t (instead of θt in the original proof) and show that
0

θ∞) = 0, as ¯ht differentiable,
θ∞. This is sufﬁcient to conclude.

∇
and θ(cid:63)

¯ht(θ(cid:63)
t )

θ∞)

(cid:107)∇

2
(cid:107)

→

≤

−

≥

−

≤

0.

∈

¯h∞(θ∞, θ
t →

C. Convergence of SOMF — Proof of Proposition 1
Proof of Proposition 1. From assumption (D), (xt)t
is (cid:96)2-
bounded by a constant X. With assumption (A), it implies
that (αt)t is (cid:96)2-bounded by a constant A. This is enough to
show that (gt)t and (θt)t meet basic assumptions (C)–(F).
Assumption (G) immediately implies (B). It remains to show
that (gt)t and (θt)t meet the assumptions (H) and (I). This will
allow to cast SOMF as an instance of SAMM and conclude.
1) The computation of Dt veriﬁes (I): We deﬁne D(cid:63)
t =
argminD∈C ¯gt(D). We show that performing subsampled
block coordinate descent on ¯gt is sufﬁcient to meet assump-
tion (I), where θt = Dt. We separately analyse the exceptional
case where no subsampling is done and the general case.

First, with small but non-zero probability, Mt = Ip
and Alg. 4 performs a single pass of simple block coordi-
nate descent on ¯gt. In this case, as ¯gt is strongly convex
from (A), [52, 37] ensures that the sub-optimality decreases
at least of factor 1
µ with a single pass of block coordinate
descent, where µ > 0 is a constant independent of t. We
provide an explicit µ in Appendix B.

−

In the general case, the function value decreases determin-
¯gt(Dt−1). As
≤
¯gt(Dt−1). Fur-
≤
t ) are deterministic with respect
= Ip] = ¯gt(D(cid:63)
t ).
the sub-optimality

istically at each minimization step: ¯gt(Dt)
a consequence, E[¯gt(Dt)
= Ip]
|Ft− 1
thermore, ¯gt and hence ¯gt(D(cid:63)
, which implies E[¯gt(D(cid:63)
t )
to
Deﬁning d (cid:44) P[Mt = Ip], we split
expectation and combine the analysis of both cases:

|Ft− 1

Ft− 1

, Mt

, Mt

2

2

2

E[¯gt(Dt)
= dE[¯gt(Dt)

, Mt = Ip]

]

2

−

¯gt(D(cid:63)
t )
|Ft− 1
¯gt(D(cid:63)
t )
−
d)E[¯gt(Dt)
µ) + (1
−
dµ(cid:1)(¯gt(Dt−1)

−

2

|Ft− 1
¯gt(D(cid:63)
t )
−
d)(cid:1)(¯gt(Dt−1)
¯gt(D(cid:63)
t )).

|Ft− 1
−

2

−

+ (1
−
(cid:0)d(1

≤
= (cid:0)1

−

= Ip]

, Mt
¯gt(D(cid:63)

t ))

(59)

2) The surrogates (gt)t verify (H): We deﬁne g(cid:63)
t ∈
ρ,L(ft, Dt−1) the surrogate used in OMF at iteration t, which
S
depends on the exact computation of α(cid:63)
t , while the surrogate
gt used in SOMF relies on approximated αt. Formally, using
the loss function (cid:96)(α, G, β) (cid:44) 1
α(cid:62)β + λΩ(α), we
recall the deﬁnitions

2 α(cid:62)Gα

−

α(cid:63)
t

(cid:44) argmin
α∈Rk
t (D) (cid:44) (cid:96)(α(cid:63)
g(cid:63)

t , β(cid:63)

(cid:96)(α, G(cid:63)

t ), αt(cid:44) argmin
α∈Rk
t , D(cid:62)D, D(cid:62)xt), gt(D) (cid:44) (cid:96)(αt, D(cid:62)D, D(cid:62)xt).

(cid:96)(α, Gt, βt),

(60)

gt

t , β(cid:63)

g(cid:63)
t −
(cid:107)

The matrices G(cid:63)
t are deﬁned in (21) and Gt, βt in either
the update rules (b) or (c). We deﬁne (cid:15)t (cid:44)
∞ to be
(cid:107)
the (cid:96)∞ difference between the approximate surrogate of SOMF
and the exact surrogate of OMF, as illustrated in Figure 2. By
ρ,L(ft, θt−1, (cid:15)t). We ﬁrst show that (cid:15)t can be
deﬁnition, gt
bounded by the Froebenius distance between the approximate
parameters Gt, βt and the exact parameters G(cid:63)
t . Using
Cauchy-Schwartz inequality, we ﬁrst show that there exists a
constant C (cid:48) > 0 such that for all D

t , β(cid:63)

∈ T

,

gt(D)
|

−

g(cid:63)
t (D)

C (cid:48)

| ≤

Then, we show that the distance

bounded: there exists C (cid:48)(cid:48) > 0 constant such that

∈ C
αt
(cid:107)

αt
(cid:107)

2.

(61)

α∗
t (cid:107)
α∗
t (cid:107)2 can itself be

−

−

αt

(cid:107)

−

α(cid:63)

t (cid:107)2 ≤

C (cid:48)(cid:48)(

G(cid:63)
(cid:107)

t −

Gt

(cid:107)F +

β(cid:63)
(cid:107)

t −

βt(cid:107)2).

(62)

We combine both equations and take the supremum over
D

, yielding

∈ C

(cid:15)t

C(
(cid:107)

≤

G(cid:63)

t −

Gt

(cid:107)F +

(cid:107)

β(cid:63)

t −

βt(cid:107)2),

(63)

where C is constant. Detailed derivation of (61) to (63) relies
on assumption (A) and are reported in Appendix B.

In a second step, we show that

βt(cid:107)
2
vanish almost surely, sufﬁciently fast. We focus on bounding
βt −
2 when the
(cid:107)
update rules (b) are used. For t > 0, we write i (cid:44) it. Then
(cid:88)

2 and proceed similarly for

F and
(cid:107)

G(cid:63)
t (cid:107)

β(cid:63)
t (cid:107)

t −

t −

Gt

Gt

−

(cid:107)

G(cid:63)
(cid:107)

β(cid:63)
(cid:107)

βt

(cid:44) β(i)

t =

γ(i)
s,tD(cid:62)

s−1Msx(i),

where γ(i)
(cid:12)
(cid:8)s
(cid:12)

s,t = γc(i)
t, xs = x(i)(cid:9)(cid:12)

t

≤
βt −

β(cid:63)

t =

s≤t,xs=x(i)

(cid:81)

s<t,xs=x(i)(1

γc(i)

s

−

) and c(i)
t =
t as

β(cid:63)

(cid:12). We can then decompose βt −
(cid:88)
γ(i)
s,t(Ds−1

Dt−1)(cid:62)Msx(i)

−

s≤t,xs=xt=x(i)
+ D(cid:62)

(cid:16) (cid:88)

t−1

s≤t,xs=xi)

γ(i)
s,tMs

−

(cid:17)

I

x(i).

(64)

The latter equation is composed of two terms: the ﬁrst one
captures the approximation made by using old dictionaries
in the computation of (βt)t, while the second captures how
the masking effect is averaged out as the number of epochs
increases. Assumption (B) allows to bound both terms at the
v(cid:1) > 0, a te-
same time. Setting η (cid:44) 1
2)
−
β(cid:63)
dious but elementary derivation indeed shows E[
βt−
t (cid:107)
(cid:107)

∈
0 almost surely — see Appendix B.
O
The SOMF algorithm therefore meets assumption (H) and is a
convergent SAMM algorithm. Proposition 1 follows.

(t2(u−1)−η) and (cid:15)t

2 min (cid:0)v

3
4 , (3u

→

−

−

2]

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

16

REFERENCES
[1] J. Mairal, “Sparse Modeling for Image and Vision Processing,” Foundations and
Trends in Computer Graphics and Vision, vol. 8, no. 2-3, pp. 85–283, 2014.
[2] N. Srebro, J. Rennie, and T. S. Jaakkola, “Maximum-margin matrix factorization,”
in Advances in Neural Information Processing Systems, 2004, pp. 1329–1336.
[3] E. J. Cand`es and B. Recht, “Exact matrix completion via convex optimization,”
Foundations of Computational Mathematics, vol. 9, no. 6, pp. 717–772, 2009.
[4] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global Vectors for Word

Representation.” in Proc. Conf. EMNLP, vol. 14, 2014, pp. 1532–43.

[5] O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix factoriza-
tion,” in Advances in Neural Information Processing Systems, 2014, pp. 2177–2185.
[6] Y. Zhang, M. Roughan, W. Willinger, and L. Qiu, “Spatio-Temporal Compressive

Sensing and Internet Trafﬁc Matrices,” 2009.

[7] H. Kim and H. Park, “Sparse non-negative matrix factorizations via alternating non-
negativity-constrained least squares for microarray data analysis,” Bioinformatics,
vol. 23, no. 12, pp. 1495–1502, 2007.

[8] G. Varoquaux et al., “Multi-subject dictionary learning to segment an atlas of brain

spontaneous activity,” in Proc. IPMI Conf., 2011, pp. 562–573.

[9] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” in

Proceedings of COMPSTAT, 2010, pp. 177–186.

[10] J. Mairal, “Stochastic majorization-minimization algorithms for large-scale opti-

mization,” in Adv. Neural Inform. Process. Syst., 2013, pp. 2283–2291.

[11] M. Razaviyayn, M. Hong, and Z.-Q. Luo, “A uniﬁed convergence analysis of block
successive minimization methods for nonsmooth optimization,” SIAM Journal on
Optimization, vol. 23, no. 2, pp. 1126–1153, 2013.

[12] S. Burer and R. D. C. Monteiro, “Local Minima and Convergence in Low-Rank
Semideﬁnite Programming,” Math. Program., vol. 103, no. 3, pp. 427–444, 2004.
[13] B. Recht and C. R´e, “Parallel stochastic gradient algorithms for large-scale matrix

completion,” Math. Program. Comput., vol. 5, no. 2, pp. 201–226, 2013.

[14] R. M. Bell and Y. Koren, “Lessons from the Netﬂix prize challenge,” ACM SIGKDD

Explorations Newsletter, vol. 9, no. 2, pp. 75–79, 2007.

[15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning for matrix factorization
and sparse coding,” J. Machine Learning Research, vol. 11, pp. 19–60, 2010.
[16] W. B. Johnson and J. Lindenstrauss, “Extensions of Lipschitz mappings into a
Hilbert space,” Contemporary mathematics, vol. 26, no. 189-206, p. 1, 1984.
[17] E. Bingham and H. Mannila, “Random projection in dimensionality reduction:
Applications to image and text data,” in Proc. SIGKDD Conf., 2001, pp. 245–250.
[18] M. J. McKeown et al., “Analysis of fMRI Data by Blind Separation into Indepen-
dent Spatial Components,” Hum. Brain Mapp., vol. 6, no. 3, pp. 160–188, 1998.
[19] E. J. Cand`es and T. Tao, “Near-optimal signal recovery from random projections:
Universal encoding strategies?” IEEE Transactions on Information Theory, vol. 52,
no. 12, pp. 5406–5425, 2006.

[20] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions,”
SIAM review, vol. 53, no. 2, pp. 217–288, 2011.

[21] V. Rokhlin et al., “A randomized algorithm for principal component analysis,”

SIAM J. Matrix Anal. Appl., vol. 31, no. 3, pp. 1100–1124, 2009.

[22] T. Sarlos, “Improved approximation algorithms for large matrices via random

projections,” in Proc. IEEE Symp. Found. Comput. Science, 2006, pp. 143–152.

[23] Y. Lu et al., “Faster ridge regression via the subsampled randomized hadamard

transform,” in Adv. Neural Inform. Process. Syst., 2013, pp. 369–377.

[24] M. Pilanci and M. Wainwright, “Iterative hessian sketch: Fast and accurate solution
approximation for constrained least-squares,” JMLR, vol. 17, pp. 1–33, 2015.
[25] G. Raskutti and M. Mahoney, “Statistical and algorithmic perspectives on random-
ized sketching for ordinary least-squares,” in Proc. ICML, 2015, pp. 617–625.
[26] A. Mensch, J. Mairal, B. Thirion, and G. Varoquaux, “Dictionary learning for

massive matrix factorization,” in Proc. ICML, 2016, pp. 1737–1746.

[27] B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete basis set:
A strategy employed by V1?” Vision Res., vol. 37, no. 23, pp. 3311–3325, 1997.
[28] R. Tibshirani, “Regression shrinkage and selection via the lasso,” J. R. Stat. Soc.

Series B Stat. Methodol., vol. 58, no. 1, pp. 267–288, 1996.

[29] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” J.

Comput. Graph. Stat., vol. 15, no. 2, pp. 265–286, 2006.

[30] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,”
J. R. Stat. Soc. Series B Stat. Methodol., vol. 67, no. 2, pp. 301–320, 2005.
[31] P. O. Hoyer, “Non-negative matrix factorization with sparseness constraints,”

Journal of Machine Learning Research, vol. 5, pp. 1457–1469, 2004.

[32] L. Bottou, F. E. Curtis, and J. Nocedal, “Optimization Methods for Large-Scale

Machine Learning,” arXiv:1606.04838v2 [stat.ML], 2016.
[33] A. W. Van der Vaart, Asymptotic Statistics. CUP, 2000, vol. 3.
[34] M. Mardani et al., “Subspace Learning and Imputation for Streaming Big Data

Matrices and Tensors,” IEEE TSP, vol. 63, no. 10, pp. 2663–2677, 2015.

[35] J. M. Borwein and A. S. Lewis, Convex Analysis and Nonlinear Optimization:

Theory and Examples. Springer Science & Business Media, 2010.

[36] J. Mairal, “Optimization with ﬁrst-order surrogate functions,” in Proceedings of the

International Conference on Machine Learning, 2013, pp. 783–791.

[37] S. J. Wright, “Coordinate descent algorithms,” Mathematical Programming, vol.

[38] D. C. Van Essen et al., “The WU-Minn Human Connectome Project: An overview,”

151, no. 1, pp. 3–34, 2015.

NeuroImage, vol. 80, pp. 62–79, 2013.

[39] M. P. Milham et al., “The adhd-200 consortium: a model to advance the transla-
tional potential of neuroimaging in clinical neuroscience,” Front. Syst. Neurosci.,
vol. 6, no. 62, 2012.

[40] G. Varoquaux, Y. Schwartz, P. Pinel, and B. Thirion, “Cohort-level brain mapping:
Learning cognitive atoms to single out specialized regions,” in Proceedings of the
Information Processing in Medical Imaging Conference, 2013, pp. 438–449.

[41] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral image classiﬁcation
using dictionary-based sparse representation,” IEEE Transactions on Geoscience
and Remote Sensing, vol. 49, no. 10, pp. 3973–3985, 2011.

[42] A. Soltani-Farani, H. R. Rabiee, and S. A. Hosseini, “Spatial-Aware Dictionary
Learning for Hyperspectral Image Classiﬁcation,” IEEE Transactions on Geo-
science and Remote Sensing, vol. 53, no. 1, pp. 527–541, 2015.

[43] M. Maggioni, V. Katkovnik, K. Egiazarian, and A. Foi, “Nonlocal transform-
domain ﬁlter for volumetric data denoising and reconstruction,” IEEE Trans. Image
Process., vol. 22, no. 1, pp. 119–133, 2013.

[44] Y. Peng et al., “Decomposable nonlocal tensor dictionary learning for multispectral

image denoising,” in Proc. IEEE Conf. CVPR, 2014, pp. 2949–2956.

[45] G. Vane, “First results from the airborne visible/infrared imaging spectrometer

(AVIRIS),” in Ann. Tech. Symp. Int. Soc. Optics Photonics, 1987, pp. 166–175.

[46] S. Behnel et al., “Cython: The best of both worlds,” Computing in Science &

Engineering, vol. 13, no. 2, pp. 31–39, 2011.

[47] J. Friedman, T. Hastie, H. H¨oﬂing, and R. Tibshirani, “Pathwise coordinate
optimization,” The Annals of Applied Statistics, vol. 1, no. 2, pp. 302–332, 2007.
[48] F. Pedregosa et al., “Scikit-learn: Machine learning in Python,” Journal of Machine

Learning Research, vol. 12, pp. 2825–2830, 2011.

[49] A. Abraham et al., “Machine learning for neuroimaging with scikit-learn,” Frontiers

in Neuroinformatics, vol. 8, no. 14, 2014.

[50] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra, “Efﬁcient projections onto
the l1-ball for learning in high dimensions,” in Proceedings of the International
Conference on Machine Learning, 2008, pp. 272–279.

[51] M. M´etivier, Semimartingales: A Course on Stochastic Processes. Walter de

Gruyter, 1982, vol. 2.

[52] A. Beck and L. Tetruashvili, “On the convergence of block coordinate descent type

methods,” SIAM Journal on Optimization, vol. 23, no. 4, pp. 2037–2060, 2013.

[53] J. L. Doob, Stochastic processes.

John Wiley & Sons, 1990.

Arthur Mensch is a PhD candidate at Universit´e
Paris-Saclay and Inria. His main research interests
are related to large-scale stochastic optimization
and statistical learning, with speciﬁc applications to
functional neuroimaging and cognitive brain map-
ping. In 2015, he received a graduate degree from
Ecole Polytechnique, France, and a MSc degree in
applied mathematics from ´Ecole Normale Sup´erieure
de Cachan, France.

Julien Mairal is a research scientist at Inria. He
received a graduate degree from Ecole Polytech-
nique, France, in 2005, and a PhD degree from Ecole
Normale Superieure, Cachan, France, in 2010. Then,
he was a postdoctoral researcher at the statistics
department of UC Berkeley, before joining Inria in
2012. His research interests include machine learn-
ing, computer vision, mathematical optimiza- tion,
and statistical image and signal processing. In 2016,
he received a Starting Grant from the European
Research Council (ERC).

Ga¨el Varoquaux is a tenured computer-science
researcher at Inria. His research develops statistical-
learning tools for functional neuroimaging data with
application to cognitive mapping of the brain as
well as the study of brain pathologies. He is also
heavily invested in software development for data
science, as project-lead for scikit-learn, one of the
reference machine-learning toolboxes, and on joblib,
Mayavi, and nilearn. Varoquaux has a PhD in quan-
tum physics and is a graduate from Ecole Normale
Superieure, Paris.

Bertrand Thirion is the principal investigator of the
Parietal team (Inria-CEA) within the main French
Neuroimaging center, Neurospin. His main research
interests are related to the use of machine learning
and statistical analysis techniques for neuroimaging,
e.g. the modeling of brain variability in group stud-
ies, the mathematical study of functional connectiv-
ity and brain activity decoding; he addresses various
applications such as the study of vision through
neuroimaging and the classiﬁcation of brain images
for diagnosis or brain mapping

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

17

APPENDIX B
ALGEBRAIC DETAILS

A. Proof of Lemma 1

Proof. We ﬁrst focus on the deterministic case. Assume that (xt)t is not bounded. Then there exists a subsequence of (xt)t
that diverges towards +
and for all (cid:15) > 0,
∞
using the asymptotic bounds on u, there exists t1

. We assume without loss of generality that (xt)t → ∞

. Then, xt + xt−1

t0 such that

→ ∞

≥

t
∀

≥

t1, xt

and therefore xt

αxt−1 + (cid:15)(xt + xt−1)
α + (cid:15)
(cid:15)
1

xt−1.

≤

≤

−

Setting (cid:15) small enough, we obtain that xt is bounded by a geometrically decreasing sequence after t1, and converges to 0,
which contradicts our hypothesis. This is enough to conclude.

t−1

diverges to +

In the random case, we consider a realization of (Xt)t that is not bounded, and assumes without loss of generality that it
βXt−1, where
βXt−1, as Xt−1 is deterministic conditioned
, Doob’s forward convergence lemma on
cannot happen

. Following the reasoning above, there exists β < 1, t1 > 0, such that for all t > t1, E[Xt
t. Taking the expectation conditioned on

⊆ F
F
t−1. Therefore Xt is a supermartingale beyond a certain time. As E[Xt] <
on
discrete martingales [53] ensures that (Xt)t converges almost surely. Therefore the event
on a set with non-zero probability, less it would lead to a contradiction. The lemma follows.

(Xt)t is not bounded
}
{

t−1, E[Xt

∞
⊆ F

t−1]

∞

|F

|F

t(cid:48)]

≤

≤

F

F

t(cid:48)

B. Taylor’s inequality for L-Lipschitz continuous functions

This inequality is useful in the demonstration of Lemma 2 and Proposition 3. Let f : Θ

L-Lipschitz gradient. That is, for all θ, θ(cid:48)

∈
f (θ(cid:48))

Θ,

f (θ)

(cid:107)∇

f (θ) +

∇

≤

f (θ(cid:48))

− ∇
f (θ)(cid:62)(θ(cid:48)

(cid:107)2 ≤

L

θ
(cid:107)

θ) +

−

⊂

RK
→
(cid:107)2. Then, for all θ, θ(cid:48)
∈
θ(cid:48)

2
2.
(cid:107)

θ(cid:48)

−
L
θ
2 (cid:107)

−

R be a function with
Θ,

(66)

C. Lemma 3: Detailed control of Dt in (44)

Injecting (40) and (42) in (43), we obtain

˜Dt

(1

≤

−

µ) ˜Dt−1

w2
t−1
w2
t

+ u( ˜Dt, ˜Dt−1), where u( ˜Dt, ˜Dt−1) (cid:44) (1

µ) ˜Q

3( ˜Dt + ˜Dt−1

) + ˜Q +

˜Dt

.

(67)

(cid:18)(cid:115)

−

(cid:113)

(cid:19)

w2
t−1
w2
t

From assumption (G),
Using the determistictic result of Lemma 1, this ensures that ˜Dt is bounded.

1, and we have, from elementary comparisons, that u( ˜Dt, ˜Dt−1)

∈

w2
t−1
w2
t →

o( ˜Dt + ˜Dt−1) if Dt

.
→ ∞

D. Detailed derivations in the proof of Proposition 1

Let us ﬁrst exhibit a scaler µ > 0 independent of t, for which (I) is met
1) Geometric rate for single pass subsampled block coordinate descent: . For D(j)

Rp×k any matrix with non-zero j-th

column d(j) and zero elsewhere

¯gt(D + D(j))

∇

− ∇

∈
¯gt(D + D(j)) = ¯Ct[j, j]d(j)

¯gt has coordinate Lipschitz constant Lmax (cid:44) max0≤j<k ¯Ct[j, j]

and hence ¯gt gradient has component Lipschitz constant Lj = ¯Ct[j, j] for component j, as already noted in [15]. Using [37]
A2, as (αt)t is
terminology,
√kLmax. Moreover,
bounded from (A). As a consequence, ¯gt gradient is also L-Lipschitz continuous, where [37] note that L
¯gt is strongly convex with strong convexity modulus ρ > 0 by hypothesis (A). Then, [52] ensures that after one cycle over the
k blocks

maxt>0,0≤j<k αt[j]2

∇

≤

≤

≤

E[¯gt(Dt)

¯gt(D(cid:63)
t )

t−1, Mt = Ip]

−

|F

(cid:0)1

(cid:0)1

−

−

≤

≤

(cid:1)(¯gt(Dt−1)

2Lmax(1 + kL2/L2
µ(cid:1)(¯gt(Dt−1)

¯gt(D(cid:63)

max)

−
t )) where µ (cid:44)

ρ

−

¯gt(D(cid:63)

t ))
ρ
2A2(1 + k2)

2) Controling (cid:15)t from (Gt, βt), (G(cid:63)

is met in the proof of SOMF convergence. We ﬁrst show that (αt)t is bounded. We choose D > 0 such that
for all j
condition, for all t > 0,

t ) — Equations 61–62: We detail the derivations that are required to show that (H)
D
. From assumption (A), using the second-order growth

, and X such that

X for all x

[k] and D

d(j)
(cid:107)

(cid:107)2 ≤

(cid:107)2 ≤

x
(cid:107)

∈ X

∈ C

∈

t , β(cid:63)

(65)

(68)

(69)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

We have successively used the fact that Ω(0) = 0, Ω(αt)
induction on the number of epochs. For all t > 0, from the deﬁnition of αt and α(cid:63)

βt(cid:107)2 ≤
(cid:107)

0, and

≥

√krDX, which can be shown by a simple

λΩ(0)

t Gtαt

α(cid:62)

t βt + λΩ(αt)

ρ
2 (cid:107)
1
2

2
2 ≤

αt

0
−
(cid:107)
t Gtαt

α(cid:62)

0 +

(cid:107)

≤

ρ
2 (cid:107)

αt

2
2 +
(cid:107)

−
αt

(

α(cid:62)

1
2
βt(cid:107)2,
αt
(cid:107)

(cid:107)2,

(cid:107)2(cid:107)

−

hence

αt

ρ
(cid:107)

2
2 ≤

(cid:107)

√krDX

and therefore

αt
(cid:107)

(cid:107)2 ≤

√krDX
ρ

(cid:44) A.

gt(D)

|

g(cid:63)
t (D)
|

−

=

t −

Tr D(cid:62)D(αtα(cid:62)

(cid:12)
1
(cid:12)
(cid:12)
2
1
t −
2 (cid:107)
(kD2A + √kDX)
αt
(cid:107)

αtα(cid:62)

D(cid:62)D

(cid:107)F (cid:107)

α(cid:63)

t α(cid:63)
t

(cid:62))

α(cid:63)

(cid:62)

t α(cid:63)
t
α(cid:63)
t (cid:107)2,

−

−
(cid:107)F +

≤

≤

t , for all D

:

∈ C

(αt

−

α(cid:63)

t )(cid:62)D(cid:62)xt

(cid:12)
(cid:12)
(cid:12)

D

(cid:107)

(cid:107)F (cid:107)

xt

(cid:107)2(cid:107)

αt

−

α(cid:63)

t (cid:107)2

where we use Cauchy-Schwartz inequality and elementary bounds on the Froebenius norm for the ﬁrst inequality, and use
αt, α(cid:63)
[k] to obtain the second inequality, which is (61) in the main text.
α(cid:63)
t (cid:107)2. We adapt the proof of Lemma B.6 from [36], that states the lipschitz continuity of the

αt
(cid:107)
minimizers of some parametrized functions. By deﬁnition,

A, xt
We now turn to control

X for all t > 0 and d(j)

D for all j

t ≤

≤

−

≤

∈

α(cid:63)

t = argmin
α∈Rk

(cid:96)(α, G(cid:63)

t , β(cid:63)
t )

αt = argmin
α∈Rk

(cid:96)(α, Gt, βt),

Assumption (A) ensures that Gt

ρIk, therefore we can write the second-order growth condition

(cid:31)

αt

αt

ρ
2 (cid:107)
ρ
2 (cid:107)
ρ

αt
(cid:107)

2
α(cid:63)
t (cid:107)
2 ≤
2
α(cid:63)
t (cid:107)
2 ≤
2
α(cid:63)
t (cid:107)
2 ≤

−

−

−

(cid:96)(αt, G(cid:63)

t , β(cid:63)
t )

(cid:96)(αt, Gt, βt)

−

(cid:96)(α(cid:63)

t , Gt, βt)
p(α(cid:63)

(cid:96)(α(cid:63)

t , G(cid:63)

t , β(cid:63)

t ),
and therefore
−
t ), where p(α) (cid:44) (cid:96)(α, Gt, βt)

p(αt)

−

−
Rk such that

(cid:96)(αt, G(cid:63)

t , β(cid:63)

t ).

α
(cid:107)

(cid:107)2 ≤

A,

p takes a simple form and can differentiated with respect to α. For all α

Therefore p is L-Lipschitz on the ball of size A where αt and α(cid:63)

p(α) =

1
2

α(cid:62)(Gt
G(cid:63)

A

−
Gt
(cid:107)

−

p(α) = (Gt

∇
p(α)

(cid:107)∇

(cid:107)2 ≤

−
t )α
G(cid:63)

G(cid:63)

∈
α(cid:62)(βt −
t )α
−
β(cid:63)
(βt −
t )
β(cid:63)
βt −
(cid:107)
t live, and

t (cid:107)2

−
t (cid:107)F +

β(cid:63)
t )

(cid:44) L

αt

ρ
(cid:107)

αt

(cid:107)

−

−

2
α(cid:63)
t (cid:107)
2 ≤
α(cid:63)

t (cid:107)2 ≤

L
(cid:107)
A
ρ (cid:107)

αt

−

Gt

−

α(cid:63)

t (cid:107)2
G(cid:63)
t (cid:107)F +

1
ρ (cid:107)

βt −

β(cid:63)
t (cid:107)2,

which is (62) in the main text. The bound (63) on (cid:15)t immediately follows.

3) Bounding

C are positive constants independent of t and we introduce the terms

βt −
(cid:107)

β(cid:63)
t (cid:107)2 in equation (64): Taking the (cid:96)2 norm in (64), we have

βt −
(cid:107)

β(cid:63)

t (cid:107)2 ≤

BLt + CRt, where B and

Lt (cid:44) (cid:88)

s≤t,xs=xt=x(i)

γ(i)
s,t(cid:107)

Ds−1

Dt−1

−

(cid:107)F ,

Rt (cid:44)

(cid:0) (cid:80)

s≤t,xs=x(i) γ(i)

s,tMs

(cid:1)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
I
(cid:13)F

.

−

{
∈

a) Conditioning on the sequence of drawn indices: We recall that (it)t is the sequence of indices that are used to draw
}i, namely such that xt = x(it). (it)t is a sequence of i.i.d random variables, whose law is uniform in [1, n].
b )b>0 that record the iterations at which sample (i) is drawn, i.e. such
t > 0 is the integer that counts the number of time sample (i) has
. These notations will help us understanding the behavior of (Lt)t
}

x(i)
(xt)t from
[n], we deﬁne the increasing sequence (t(i)
For each i
that itb = i for all b > 0. For t > 0, we recall that c(i)
appeared in the algorithm, i.e. c(i)
t
b ≤
and (Rt)t.

b > 0, t(i)
{

t = max

18

(70)

(71)

(72)

(73)

(74)

(75)

(76)

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

19

(77)

(79)

(80)

(81)

b) Bounding Rt: The right term Rt takes its value into sequences that are running average of masking matrices. Formally,

Rt =

¯M(it)

(cid:107)

I

(cid:107)F , where we deﬁne for all i

∈

[n],

¯M(i)
t

γ(i)
b ,t(i)
t(i)

c

Mtb ,

which follows the recursion

t −
c(i)
t(cid:88)

(cid:44)

b=1






¯M(i)
t
¯M(i)
t
¯M(i)
0

γc(i)

) ¯M(i)
t−1 + γc(i)
= (1
−
= M(i)
= it
if i
t−1
[n]
= 0 for all i

t

t

Mt

if i = it

When sampling a sequence of indices (is)s>0, the n random matrix sequences [( ¯M(i)
law as the sampling is uniform. We therefore focus on controling ( ¯M(0)
the expectation over the sequence of indices (is)s,

∈
t )t≤0]
i∈[n] follows the same probability
] is

t )t. For simplicity, we write ct (cid:44) c(0)

t

. When E[
·

¯M(0)
E[
(cid:107)

t −

(cid:107)F ]2
I

( ¯M(0)
t

[j, j]

1)(cid:3) = pE[( ¯M(0)

t

−

[0, 0]

1)]

−

E(cid:2)

p
(cid:88)

j=1

≤

≤

C p(ct)1/2γct = C p(ct)1/2−v, where C is a constant independent of t.

(78)

We have simply bounded the Froebenius norm by the (cid:96)1 norm in the ﬁrst inequality and used the fact that all coefﬁcients
Mt[j, j] follows the same Bernouilli law for all t > 0, j
[p]. We then used Lemma B.7 from [10] for the last inequality. This
lemma applies as Mt[0, 0] follows the recursion (77). It remains to take the expectation of (78), over all possible sampling
trajectories (is)s>0:

∈

E[Rt] = E(cid:2)E[Rt

(is)s](cid:3) = E(cid:2)E[
M(it)
t −
(cid:107)F |
|
(cid:107)
CpE[(ct)2(u−1)−η].
= CpE[(ct)1/2−v]

I

≤

(is)s](cid:3) = E(cid:2)E[
(cid:107)

M(0)

I

t −

(cid:107)F |

(is)s](cid:3) = E[

M(0)
(cid:107)

t −

I

(cid:107)F ]

The last inequality arises from the deﬁnition of η (cid:44) 1
we successively have

2 min (cid:0)v

3
4 , (3u

2)

−

−

−

v(cid:1), as follows. First, η > 0 as u > 11

12 . Then,

5
2 −

2u <

<

as u >

2
3

3
4

,

11
12

,

3
4

v

≥

+ 2η >

2u + 2η,

5
2 −

1
2 −

v <

1
2 −

5
2

+ 2u

2η = 2(u

1)

2η < 2(u

1)

η, which allows to conclude.

−

−

−

−

−

I)t converges towards 0
Lemma B.7 from [10] also ensures that Mt[0, 0]
1 almost surely when t
almost surely, given any sample sequence (is)s. It thus converges almost surely when all random variables of the algorithm
are considered. This is also true for ( ¯M(i)

t −

→ ∞

→

I)t for all i

. Therefore ( ¯M(0)

c) Bounding Lt: As above, we deﬁne n sequences [(L(i)

t −

∈

[n] and hence for Rt.
t )t]i∈[n], such that Lt = L(it)

t

for all t > 0. Namely,

L(i)
t

(cid:44) (cid:88)
s≤t,
xs=xt=x(i)

γ(i)
s,t(cid:107)

Ds−1

Dt−1

−

(cid:107)F =

c(i)
t(cid:88)

b=1

γ(i)
b ,t(i)
t(i)

c

(i)
t

(cid:13)
(cid:13)Dtb−1

Dt

c

(i)
t

−

(cid:13)
(cid:13)

−1

.

F

Once again, the sequences (cid:2)(L(i)
t )t
focus on bounding (L(0)
From assumption (B) and the deﬁnition of η, we have v < ν < 1. We split the sum in two parts, around index dt (cid:44) ct
where

(cid:3)
i all follows the same distribution when sampling over sequence of indices (is)s. We thus
η.
,

t )t. Once again, we drop the (0) superscripts in the right expression for simplicity. We set ν (cid:44) 3u

takes the integer part of a real number. For simplicity, we write d (cid:44) dt and c (cid:44) ct in the following.

2
−
−
(ct)ν

−(cid:98)

(cid:99)

(cid:98)·(cid:99)

L(0)

t =

γtb,tc

(cid:13)
(cid:13)Dtb−1

Dtc−1

−

(cid:13)
(cid:13)F ≤

2√kD

c
(cid:88)

b=1

d
(cid:88)

b=1

c
(cid:88)

tc−1
(cid:88)

γtb,tc +

γtb,t

b=d+1

s=tb−1

ws (cid:44) 2√kDL(0)

t,1 + L(0)

t,2

(82)

On the left side, we have bounded
on

Ds

Dt

(cid:107)
We now study both L(0)

−

Dt
(cid:107)
t,1 and L(0)

(cid:107)F provided by Lemma 3, that applies here as (I) is met and (63) ensures that (
gt
(cid:107)

−

g(cid:63)
t (cid:107)∞)t is bounded.

(cid:107)F by √kD, where D is deﬁned in the previous section. The right part uses the bound

t,2 . First, for all t > 0,

L(0)
t,1

(cid:44)

d
(cid:88)

b=1

d
(cid:88)

c
(cid:89)

γtb,tc =

γb

(1

γp)

−

≤

p=b+1

d
(cid:88)

b=1

γb(1

−

γc)c−b

b=1
γc)(cid:98)cν (cid:99)
γc

≤

(1

−

≤

cv exp (cid:0)log(1

1

cv )cν(cid:1)

≤

−

C (cid:48)cv exp(cν−v)

Cc2(u−1)−η = C(ct)2(u−1)−η,

(83)

≤

STOCHASTIC SUBSAMPLING FOR FACTORIZING HUGE MATRICES

20

where C and C (cid:48) are constants independent of t. We have used ν > v for the third inequality, which ensures that
exp (cid:0)log(1
(cν−v). Basic asymptotic comparison provides the last inequality, as ct
almost surely and
the right term decays exponentially in (ct)t, while the left decays polynomially. As a consequence, L(0)
0 almost surely.

cv )cν(cid:1)

∈ O

−

1

Secondly, the right term can be bounded as (wt)t decays sufﬁciently rapidly. Indeed, as (cid:80)c

→ ∞
t,1 →
b=1 γtb,t = 1, we have

L(0)
t,2

(cid:44)

γtb,t

c
(cid:88)

b=d

tc−1
(cid:88)

s=tb−1

ws

max
d≤b≤c

≤

s=tb−1

(cid:16) tc−1
(cid:88)

(cid:17)

ws

=

tc−1
(cid:88)

ws

wtd (tc

td) =

−

≤

s=td−1
ct
dt
−
(dt)u

tc
td
(td)u =
−

tc
ct

td
dt

(

dt
td

)u

−
−

from elementary comparisons. First, we use the deﬁnition of ν to draw

were we use the fast that η
−
expectation n. Therefore, as c

when t

n , and
0, from the strong law of large numbers and linearity of the expectation

tb follows a geometric law of parameter 1

−

(ct)ν

ct
dt
(dt)u ≤

−

(ct)u(1

cν−1
t

)u ≤

−
1 < 0. We note that for all b > 0, tb+1

C(ct)ν−u = C(ct)2(u−1)−η,

d

−

→ ∞
c−1
1
(cid:88)

d

→

tb

tb+1

n,

−

→

d−1
(cid:88)

td
d

=

1
d

tb+1

tb

n almost surely.

−

→

b=0

b=d

−
n1−u almost surely. This immediately shows L(0)
0 almost surely and therefore
βt −

almost surely.

β(cid:63)

0

(cid:107)

→

t,2 →

0 and thus L(0)

t →

0 almost surely.

tc
c

td
d

=

−
−
As a consequence, tc−td
ct−dt
As with Rt, this implies that Lt

( dt
td

)u

c

→

t (cid:107)2 →
( dt
td

−
−

Finally, from the dominated convergence theorem, E[ tc−td
ct−dt
and write

)u]

n1−u for t

→

→ ∞

. We can use Cauchy-Schartz inequality

E[L(0)

t,2 ] = E[

tc
td
(td)u ]
−

E[

ct
dt
(dt)u ]E[
−

tc
ct

≤

td
dt

(

dt
td

)u]

C (cid:48)E[

≤

ct
dt
(dt)u ]
−

≤

C C (cid:48)E[(ct)2(u−1)−η],

where C (cid:48) is a constant independant of t. Then

E[Lt] = E(cid:2)E[L(it)

(is)s](cid:3) = E[L(0)
(is)s](cid:3) = E(cid:2)E[L(0)
|
β(cid:63)
Combined with (79), this shows that E[
βt −
t (cid:107)2]
(cid:107)
∈ O
0. Therefore E[( ct
(t, 1

1
n almost surely when t

n ), ct

|

t

t

t →

t

]

≤

2√kDE[L(0)

t,1 ] + E[L(0)
t,2 ]
((ct)2(u−1)−η). As ct follows a binomial distribution of parameter
nη−2(u−1), and from Cauchy-Schwartz inequality,
t )2(u−1)−η)]
)2(u−1)−η)]t2(u−1)−η

((ct)2(u−1)−η).

(t2(u−1)−η).

∈ O

(89)

→

(90)

→
βt −
(cid:107)

E[

β(cid:63)
t (cid:107)2]

CE[(

ct
t

≤
We have reused the fact that converging sequences are bounded. This is enough to conclude.

∈ O

(84)

(85)

(86)

(87)

(88)

