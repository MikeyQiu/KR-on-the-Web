Fast Low-Rank Matrix Learning with Nonconvex Regularization

James T. Kwok Wenliang Zhong

Quanming Yao
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Hong Kong
{qyaoaa, jamesk, wzhong}@cse.ust.hk

5
1
0
2
 
c
e
D
 
3
 
 
]

A
N
.
s
c
[
 
 
1
v
4
8
9
0
0
.
2
1
5
1
:
v
i
X
r
a

Abstract—Low-rank modeling has a lot of important ap-
plications in machine learning, computer vision and social
network analysis. While the matrix rank is often approximated
by the convex nuclear norm, the use of nonconvex low-rank
regularizers has demonstrated better recovery performance.
However, the resultant optimization problem is much more
challenging. A very recent state-of-the-art is based on the
proximal gradient algorithm. However, it requires an expensive
full SVD in each proximal step. In this paper, we show that
for many commonly-used nonconvex low-rank regularizers, a
cutoff can be derived to automatically threshold the singular
values obtained from the proximal operator. This allows the use
of power method to approximate the SVD efﬁciently. Besides,
the proximal operator can be reduced to that of a much smaller
matrix projected onto this leading subspace. Convergence,
with a rate of O(1/T ) where T is the number of iterations,
can be guaranteed. Extensive experiments are performed on
matrix completion and robust principal component analysis.
The proposed method achieves signiﬁcant speedup over the
state-of-the-art. Moreover, the matrix solution obtained is more
accurate and has a lower rank than that of the traditional
nuclear norm regularizer. 1

Keywords-Low-rank matrix, Nonconvex optimization, Prox-

imal gradient, Matrix completion, Robust PCA

I. INTRODUCTION

The learning of low-rank matrices is a central

issue
in many machine learning problems. For example, matrix
completion [2], which is one of the most successful ap-
proaches in collaborative ﬁltering, assumes that the target
ratings matrix is low-rank. Besides collaborative ﬁltering,
matrix completion has also been used on tasks such as
sensor networks [3], social network analysis [4], and image
processing [5, 6].

Another important use of low-rank matrix learning is
robust principal component analysis (RPCA) [7], which
assumes the target matrix is low-rank and also corrupted
by sparse data noise. It is now popularly used in various
computer vision applications, such as shadow removal of
aligned faces and background modeling of surveillance
videos [7, 8]. Besides, low-rank minimization has also been
used in tasks such as multilabel learning [9] and multitask
learning [10].

1This is the long version of conference paper appeared in ICDM 2015

[1]; Code is available at: https://github.com/quanmingyao/FaNCL.

However, rank minimization is NP-hard. To alleviate this
problem, a common approach is to use instead a convex
surrogate such as the nuclear norm (which is the sum of
singular values of the matrix). It is known that the nuclear
norm is the tightest convex lower bound of the rank. Besides,
there are theoretical guarantees that the incomplete matrix
can be recovered with nuclear norm regularization [2, 7].
Moreover, though the nuclear norm is non-smooth, the re-
sultant optimization problem can often be solved efﬁciently
using modern tools such as accelerated proximal gradient
descent [11], Soft-Impute [12], and active subspace selection
methods [13].

Despite its success, recently there have been numerous
attempts that use nonconvex surrogates to better approx-
imate the rank function. The key idea is that the larger,
and thus more informative, singular values should be less
penalized. Example nonconvex low-rank regularizers in-
clude the capped-(cid:96)1 penalty [14], log-sum penalty (LSP)
[15], truncated nuclear norm (TNN) [16], smoothly clipped
absolute deviation (SCAD) [17], and minimax concave
penalty (MCP) [18]. Empirically, these nonconvex regular-
izers achieve better recovery performance than the convex
nuclear norm regularizer.

However, the resultant nonconvex optimization problem
is much more challenging. One approach is to use the
concave-convex procedure [19], which decomposes the non-
convex regularizer into a difference of convex functions
[14, 16]. However, a sequence of relaxed problems have
to be solved, and can be computationally expensive [20]. A
more efﬁcient method is the recently proposed iteratively re-
weighted nuclear norm (IRNN) algorithm [21]. It is based
on the observation that existing nonconvex regularizers are
all concave and their super-gradients are non-increasing.
Though IRNN still has to iterate, each of its iterations only
involves computing the super-gradient of the regularizer and
a singular value decomposition (SVD). However, performing
SVD on a m × n matrix (where m ≥ n) still takes O(mn2)
time, and can be expensive when the matrix is large.

Recently, the proximal gradient algorithm has also been
used on this nonconvex low-rank minimization problem
[8, 16, 21, 22]. However, it requires performing the full SVD
in each proximal step, which is expensive for large-scale
applications. To alleviate this problem, we ﬁrst observe that

for the commonly-used nonconvex low-rank regularizers, the
singular values obtained from the corresponding proximal
operator can be automatically thresholded. One then only
needs to ﬁnd the leading singular values/vectors in order to
generate the next iterate. By using the power method [23],
a fast and accurate approximation of such a subspace can
be obtained. Moreover, instead of computing the proximal
operator on a large matrix, one only needs to compute
that on its projection onto this leading subspace. The size
of the matrix is signiﬁcantly reduced and the proximal
operator can be made much more efﬁcient. In the context of
matrix completion problems, further speedup is possible by
exploiting a special “sparse plus low-rank” structure of the
matrix iterate.

The rest of the paper is organized as follows. Section II re-
views the related work. The proposed algorithm is presented
in Section III; Experimental results on matrix completion
and RPCA are shown in Section IV, and the last section
gives some concluding remarks.

In the sequel, the transpose of vector/matrix is denoted by
the superscript (·)(cid:62). For a m×n matrix X, tr(X) is its trace,
(cid:107)X(cid:107)F = tr(X (cid:62)X) is the Frobenius norm, and (cid:107)X(cid:107)∗ =
(cid:80)m
i=1 σi is the nuclear norm. Given x = [xi] ∈ Rm, Diag(x)
constructs a m × m diagonal matrix whose ith diagonal
element is xi. Moreover, I denotes the identity matrix. For
a differentiable function f , we use ∇f for its gradient. For
a nonsmooth function, we use ∂f for its subdifferential.

II. BACKGROUND

A. Proximal Gradient Algorithms

In this paper, we consider composite optimization prob-

lems of the form

min
x

F (x) ≡ f (x) + λr(x),

(1)

where f is smooth and r is nonsmooth. In many machine
learning problems, f is the loss and r a low-rank regularizer.
In particular, we make the following assumptions on f .
A1. f , not necessarily convex,

is differentiable with
ρ-Lipschitz continuous gradient,
i.e., (cid:107)∇f (X1) −
∇f (X2)(cid:107)F ≤ ρ(cid:107)X1 − X2(cid:107)F . Without loss of general-
ity, we assume that ρ ≤ 1.

A2. f is bounded below, i.e., inf f (X) > −∞.
In recent years, proximal gradient algorithms [24] have been
widely used for solving (1). At each iteration t, a quadratic
function is used to upper-bound the smooth f at the current
iterate xt, while leaving the nonsmooth r intact. For a given
stepsize τ , the next iterate xt+1 is obtained as

arg min

∇f (xt)(cid:62)(x − xt) +

(cid:107)x − xt(cid:107)2 + λr(x)

x

= arg min

(cid:107)x − zt(cid:107)2 +

1
2
where zt = xt − 1
τ ∇f (xt), and prox λ
τ r(·) is the proximal
operator [24]. Proximal gradient algorithms can be further

r(x) ≡ prox λ

τ r(zt),

x

τ
2
λ
τ

accelerated, by replacing zt with a proper linear combination
of xt and xt−1. In the sequel, as our focus is on learning
low-rank matrices, x in (1) becomes a m × n matrix X.2

B. Convex and Nonconvex Low-Rank Regularizers

An important factor for the success of proximal gradient
algorithms is that its proximal operator proxµr(·) can be
efﬁciently computed. For example, for the nuclear norm
(cid:107)X(cid:107)∗, the following Proposition shows that its proximal
operator has a closed-form solution.

(X) = U (Σ − µI)+ V (cid:62),
Proposition II.1. [25] proxµ(cid:107)·(cid:107)∗
where U ΣV (cid:62) is the SVD of X, and (Z)+ = [max(Zij, 0)].

While the convex nuclear norm makes the low-rank opti-
mization problem easier, it may not be a good approximation
of the matrix rank [8, 16, 21, 22]. As mentioned in Section I,
a number of nonconvex surrogates for the rank have been
recently proposed. In this paper, we make the following
assumption on the low-rank regularizer r in (1).
A3. r is possibly non-smooth and nonconvex, and of the
form r(X) = (cid:80)m
i=1 ˆr(σi), where σ1 ≥ · · · ≥ σm ≥ 0
are singular values of X, and ˆr(σ) is a concave and
non-decreasing function of σ ≥ 0 with ˆr(0) = 0.
All nonconvex low-rank regularizers introduced in Section I
satisfy this assumption. Their corresponding ˆr’s are shown
in Table I.

Table I
ˆr’S FOR SOME POPULAR NONCONVEX LOW-RANK REGULARIZERS. FOR
THE TNN REGULARIZER, θ ∈ {1, . . . , n} IS THE NUMBER OF LEADING
SINGULAR VALUES THAT ARE NOT PENALIZED.

capped-(cid:96)1
LSP

TNN

SCAD

MCP

µˆr(σi)
µ min(σi, θ), θ > 0
µ log (cid:0) σi
θ + 1(cid:1) , θ > 0
(cid:40)
i > θ
µσi
i ≤ θ
0
σi ≤ µ

i +2θµσi−µ2

2(θ−1)

µσi
−σ2






(θ+1)µ2
2
(cid:40)

µσi − σ2
θµ2
2

i
2θ

µ < σi ≤ θµ

, θ > 2

σi > θµ

σi ≤ θµ
σi > θµ

, θ > 0

The Iteratively Reweighted Nuclear Norm (IRNN) algo-
rithm [21] is a state-of-the-art solver for nonconvex low-
rank minimization. It is based on upper-bounding the non-
convex r, and approximates the matrix rank by a weighted
version of the nuclear norm (cid:107)X(cid:107)w = (cid:80)m
i=1 wiσi, where
0 ≤ w1 ≤ · · · ≤ wm, Intuitively, (cid:107)X(cid:107)w imposes a smaller
penalty on the larger (and more informative) singular values.
Other solvers that are designed only for speciﬁc nonconvex
low-rank regularizers include [8] (for the capped-(cid:96)1), [16]
(for the TNN), and [26] (for the MCP). All these (including

2In the following, we assume m ≤ n.

IRNN) need SVD in each iteration. It takes O(m2n) time,
and thus can be slow.

While proximal gradient algorithms have mostly been
used on convex problems, recently they are also applied to
nonconvex ones [8, 16, 21, 22]. In particular, in the very re-
cent generalized proximal gradient (GPG) algorithm [22], it
is shown that for any nonconvex r satisfying assumption A3,
its proximal operator can be computed by the following
generalized singular value thresholding (GSVT) operator.

Proposition II.2. [22] proxµr(X) = U Diag(y∗)V (cid:62), where
U ΣV (cid:62) is the SVD of X, and y∗ = [y∗

i ] with

y∗
i ∈ arg min
yi≥0

1
2

(yi − σi)2 + µˆr(yi).

(2)

In GPG, problem (2) is solved by a ﬁxed-point iteration
algorithm. Indeed, closed-form solutions exist for the regu-
larizers in Table I [20]. While the obtained proximal operator
can then be immediately plugged into a proximal gradient
algorithm, Proposition II.2 still involves SVD.

III. PROPOSED ALGORITHM

In this section, we show that the GSVT operator proxµr(·)
can be computed more efﬁciently. It is based on two ideas.
the singular values in proxµr(·) are automatically
First,
thresholded. Second, proxµr(·) can be obtained from the
proximal operator on a smaller matrix.

A. Automatic Thresholding of Singular Values
The following Proposition shows that y∗
i

in (2) becomes
zero when σi is smaller than a regularizer-speciﬁc threshold.
Because of the lack of space, proofs will be reported in a
longer version of this paper.

Proposition III.1. For any ˆr satisfying Assumption A3, there
exists a threshold γ > 0 such that y∗

i = 0 when σi ≤ γ.

By examining the optimality conditions of (2), simple
closed-form solutions can be obtained for the nonconvex
regularizers in Table I.

Corollary III.2. For the nonconvex regularizers in Table I,
their γ values are equal to

(cid:1);

• capped-(cid:96)1: γ = min (cid:0)µ, θ + µ
• LSP: γ = min (cid:0) µ
• TNN: γ = max (µ, σθ+1);
• SCAD: γ = µ;
√
• MCP: γ =

θ , θ(cid:1);

2

θµ if 0 < θ < 1, and µ otherwise.

Proposition III.1 suggests that in each proximal iteration t,
we only need to compute the leading singular values/vectors
of the matrix iterate Z t. The power method (Algorithm 1)
[23] is a fast and accurate algorithm for obtaining an ap-
proximation of such a subspace. Besides the power method,
algorithms such as PROPACK [27] have also been used
[28]. However, the power method is more efﬁcient than

Algorithm 1 Power method to obtain an approximate left
subspace of Z.
Require: matrix Z ∈ Rm×n, R ∈ Rn×k.
1: Y 1 ← ZR;
2: for t = 1, 2, . . . , Tpm do
3: Qt+1 = QR(Y t);
4:
5: end for
6: return QTpm+1.

Y t+1 = Z(Z (cid:62)Qt+1);

// QR decomposition

PROPACK [23]. It also allows warm-start, which is partic-
ularly useful because of the iterative nature of the proximal
gradient algorithm.

B. Proximal Operator on a Smaller Matrix

Assume that Z t has ˆk ≤ n singular values larger than γ,
and its rank-ˆk SVD is UˆkΣˆkV (cid:62)
. The following Proposition
ˆk
shows that proxµr(Z t) can be obtained from the proximal
operator on a smaller matrix.
Proposition III.3. Assume that Q ∈ Rm×k, where k ≥ ˆk, is
orthogonal and span(Uˆk) ⊆ span(Q). Then, proxµr(Z t) =
Q · proxµr(Q(cid:62)Z t).

Though SVD is still needed to obtain proxµr(Q(cid:62)Z t),
Q(cid:62)Z t is much smaller than Z t (k×n vs m×n). This smaller
SVD takes O(nk2) time, and the other matrix multiplication
steps take O(mnk) time. Thus, the time complexity for this
SVD step is reduced from O(m2n) to O((m + k)nk).

C. Complete Procedure

The complete procedure (Algorithm 2) will be called
FaNCL (Fast NonConvex Lowrank). The core steps are 9–
16. We ﬁrst use the power method to efﬁciently obtain an
approximate Q, whose singular values are then thresholded
according to Corollary III.2. With k ≥ ˆk, the rank of ˜X p
will be equal to that of proxµr(Z t). In each iteration, we
ensure a sufﬁcient decrease of the objective:

F (X t+1) ≤ F (X t) − c1(cid:107)X t+1 − X t(cid:107)2
F ,

(3)

where c1 = τ −ρ
4 ; otherwise, the power method is restarted.
Moreover, similar to [13, 28], steps 6-7 use the column
spaces of the previous iterates (V t and V t−1) to warm-
start the power method. For further speedup, we employ
a continuation strategy as in [12, 21, 28]. Speciﬁcally, λt is
initialized to a large value and then decreases gradually.

Algorithm 2 can also be used with the nuclear norm. It
can be shown that the threshold γ is equal to λ/τ , and y∗
i
in step 15 has the closed-form solution max(σi − λt/τ, 0).
However, since our focus is on nonconvex regularizers, using
Algorithm 2 for nuclear norm minimization will not be
further pursued in the sequel.

The power method has also been recently used to approx-
imate the SVT in nuclear norm minimization [13]. However,

Algorithm 2 FaNCL (Fast NonConvex Low-rank).
1: choose τ > ρ, c1 = τ −ρ
2: initialize V0, V1 ∈ Rn×k as random Gaussian matrices,

4 , λ0 > λ and ν ∈ (0, 1);

and X 1 = 0;

3: for t = 1, 2, . . . T do
4:
5:

λt ← (λt−1 − λ)ν + λ;
Z t ← X t − 1
τ ∇f (X t);
V t−1 ← V t−1 − V t(V t(cid:62)V t−1), and
remove any zero columns;

6:

7: R1 ← QR([V t, V t−1]);
for p = 1, 2, . . . do
8:
9:
10:

A, Σp

A, V p

A] ← SVD(Q(cid:62)Z t);

Q ← PowerMethod(Z t, Rp);
[U p
ˆk ← number of σA’s are > γ in Corollary III.2;
˜U p ← ˆk leading columns of U p
A;
˜V p ← ˆk leading columns of V p
A;
for i = 1, 2, . . . , ˆk do

obtain y∗

i from (2) with µ = 1/τ and λt;

end for
˜X p ← (Q ˜U p)Diag(y∗
)( ˜V p)(cid:62);
if F ( ˜X p) ≤ F (X t) − c1(cid:107) ˜X p − X t(cid:107)2

1, . . . , y∗
ˆk

F then

X t+1 ← ˜X p, V t+1 ← ˜V p;
break;

11:
12:

13:

14:
15:
16:
17:

18:
19:
20:

Rp+1 = V p
A;

else

21:
22:
23:
24:
25: end for
26: return X T +1.

end if
end for

[13] is based on active subspace selection (which uses SVT
to update the active row and column subspaces of the current
solution), and is thus very different from the proposed
algorithm (which is a proximal gradient algorithm). In Sec-
tion IV, it will be shown that the proposed method has better
empirical performance. Moreover, [13] is only designed for
nuclear norm minimization, and cannot be extended for the
nonconvex regularizers considered here.

A breakdown of the time complexity of Algorithm 2 is as
follows. For simplicity, assume that X t’s always have rank
k. Step 5 takes O(mn) time; step 6 and 7 take O(nk2) time;
step 9 and 10 take O(mnkTpm) time; step 17 takes O(mnk)
time; and step 18 takes O(mn) time. Thus, the per-iteration
time complexity is O(mnkpTpm). In the experiment, we set
Tpm = 3 and p = 1. Empirically, this setting is enough to
guarantee (3). In contrast, SVDs in GPG and IRNN take
O(m2n) time, and are thus much slower as k (cid:28) m.

D. Convergence Analysis

The following Proposition shows that {X t} from Algo-

rithm 2 converges to a limit point X∞ = limt→∞ X t.
Proposition III.4. (cid:80)∞

t=1 (cid:107)X t+1 − X t(cid:107)2

F < ∞.

The following shows that it is also a critical point.3

Theorem III.5. {X t} converges to a critical point X ∗ of
problem (1) in a ﬁnite number of iterations.

that (cid:107)X t+1 − X t(cid:107)2

By combining with Proposition III.4, the following shows
F converges to zero at a rate of O(1/T ).
≤

III.6. mint=1,...,T (cid:107)X t+1 − X t(cid:107)2
F

Corollary

(cid:2)F (X 1) − F (X ∗)(cid:3).

1
c1T
E. Further Speedup for Matrix Completion

In matrix completion, one attempts to recover a low-rank
matrix O ∈ Rm×n by observing only some of its elements.
Let the observed positions be indicated by Ω ∈ {0, 1}m×n,
such that Ωij = 1 if Oij is observed, and 0 otherwise. It
can be formulated as an optimization problem in (1), with
f (X) = 1
F , where [PΩ(A)]ij = Aij if Ωij =
1 and 0 otherwise, and r is a low-rank regularizer.

2 (cid:107)PΩ(X −O)(cid:107)2

It can be easily seen that step 5 in Algorithm 2 becomes
Z t = X t − 1
τ PΩ(X t −O). By observing that X t is low-rank
and 1
τ PΩ(X t − O) is sparse, Mazumder et al. [12] showed
that
this “sparse plus low-rank” structure allows matrix
multiplications of the form ZA and Z (cid:62)B to be efﬁciently
computed. Here, this trick can also be directly used to speed
up the computation of Z(Z (cid:62)Qt+1) in Algorithm 1. Since
(cid:107)Ω(cid:107)1 is very sparse, this step takes O(kTpm(cid:107)Ω(cid:107)1) time
instead of O(mnkTpm), thus is much faster.

The following Proposition shows that (cid:107) ˜X p − X t(cid:107)2

F in

step 18 of Algorithm 2 can also be easily computed.

Proposition III.7. Let the reduced SVD of X be U ΣV (cid:62),
and P, Q be orthogonal matrices such that span(U ) ⊆
span(P ) and span(V ) ⊆ span(Q). Then (cid:107)X(cid:107)F =
(cid:107)P (cid:62)XQ(cid:107)F .

Let

the reduced SVDs of

˜X p and X t be ˜U ˜Σ ˜V (cid:62)
and U tΣtV t(cid:62), respectively. Let P = QR([ ˜U , U t]) and
Q = QR([ ˜V , V t]). Using Proposition III.7, (cid:107) ˜X p −
X t(cid:107)F = (cid:107)P (cid:62)( ˜X p − X t)Q(cid:107)F = (cid:107)(P (cid:62) ˜U ) ˜Σ( ˜V (cid:62)Q) −
(P (cid:62)U t)Σt(V t(cid:62)Q)(cid:107)F . This takes O(nk2) instead of O(mn)
time. The per-iteration time complexity is reduced from
O(mnkTpm) to O((nk + Tpm|Ω|1)k) and is much faster.
Table II compares the per-iteration time complexities and
convergence rates for the various low-rank matrix comple-
tion solvers used in the experiments (Section IV-A).

F. Handling Multiple Matrix Variables

The proposed algorithm can be extended for optimization

problems involving N matrices X1, . . . , XN :

min F (X1, . . . , XN ) ≡ f (X1, . . . , XN )+

λiri(Xi).

(4)

N
(cid:88)

i=1

3Since r is nonconvex and its subdifferential for points in its domain
may be empty, we deﬁne X ∗ as a critical point by extending the deﬁnition
in [20], namely that 0 ∈ ∇f (X ∗) + λ∂r1(X ∗) − λ∂r2(X ∗), where
r(X) = r1(X) − r2(X), and r1 and r2 are convex.

Table II
COMPARISON OF THE PER-ITERATION TIME COMPLEXITIES AND
CONVERGENCE RATES OF VARIOUS MATRIX COMPLETION SOLVERS.
HERE, ν ∈ (0, 1) IS A CONSTANT.

regularizer
(convex)
nuclear
norm
ﬁxed-rank
factorization
nonconvex

method
APG [11, 28]
Soft-Impute [12]
active ALT [13]
LMaFit [29]
R1MP [30]
IRNN [21]
GPG [22]
FaNCL

complexity
O(mnk)
O(k(cid:107)Ω(cid:107)1)
O(kTin(cid:107)Ω(cid:107)1)
O(k(cid:107)Ω(cid:107)1)
O((cid:107)Ω(cid:107)1)
O(m2n)
O(m2n)
O(k(cid:107)Ω(cid:107)1)

rate
O(1/T 2)
O(1/T )
O(νT )
—
O(νT )
—
—
O(1/T )

Assumptions A1-A3 are analogously extended. In particular,
A1 now assumes that (cid:107)∇fi(X) − ∇fi(Y )(cid:107)F ≤ ρi(cid:107)X −
Y (cid:107)F for some ρi, where fi(X) is the function obtained by
keeping all the Xj’s (where i (cid:54)= j) in f ﬁxed.

Many machine learning problems can be cast into this
form. One example that will be considered in Section IV
is robust principal component analysis (RPCA) [7]. Given
a noisy data matrix O, RPCA assumes that O can be
approximated by the sum of a low-rank matrix X plus sparse
data noise Y . Mathematically, we have

min
X,Y

F (X, Y ) ≡ f (X, Y ) + λr(X) + β(cid:107)Y (cid:107)1,

(5)

where f (X, Y ) = 1
2 (cid:107)X + Y − O(cid:107)2
F , r is a low-rank
regularizer on X, and (cid:107)Y (cid:107)1 encourages Y to be sparse.
Since both r and the (cid:96)1 regularizer (cid:107) · (cid:107)1 are nonsmooth, (5)
does not ﬁt into formulation (1). Besides RPCA, problems
such as subspace clustering [31], multilabel learning [9] and
multitask learning [10] can also be cast as (4).

For simplicity, we focus on the case with two parameter
blocks. Extension to multiple blocks is straightforward. To
solve the two-block problem in (5), we perform alternating
proximal steps on X and Y at each iteration t:

X t+1 = arg minX
Y t+1 = arg minY

1

2 (cid:107)X − Z t
2 (cid:107)Y − Z t

X (cid:107)2
Y (cid:107)2

F + λ
F + β

τ r(X) = prox λ
τ (cid:107)Y (cid:107)1 = prox β

1

τ r(Z t

X ),
(Z t

Y ),

X = X t − 1

τ ∇f (X t, Y t), and Z t

τ (cid:107)·(cid:107)1
where Z t
Y = Y t −
τ ∇f (X t+1, Y t). Y t+1 can be easily obtained as Y t+1
1
ij =
(cid:17)
sign ([Z t
, where sign(x) denotes the
+
sign of x. Similar to (3), we ensure a sufﬁcient decrease of
the objective in each iteration:

Y ]ij| − β

Y ]ij)

|[Z t

(cid:16)

τ

FY t(X t+1) ≤ FY t(X t) − c1(cid:107)X t+1 − X t(cid:107)2
F ,
FX t+1(Y t+1) ≤ FX t+1(Y t) − c1(cid:107)Y t+1 − Y t(cid:107)2
F ,

here F is nonconvex. We extend the convergence results
in Section III-D to the following.

and

blocks

Theorem III.8. With N parameter
{(X t

1, . . . , X t

F < ∞;

i=1 (cid:107)X t+1

1) (cid:80)∞
t=1
2) {(X t
(X ∗

N )} generated by the algorithm, we have
(cid:80)N
1, . . . , X t
1 , . . . , X ∗
3) mint=1,...,T
c1T [F (X 1

i (cid:107)2
i − X t
N )} converges
to a critical point
N ) of (4) in a ﬁnite number of iterations;
(cid:80)N
≤
1 , . . . , X 1

X t
−
1 , . . . , X ∗
N ) − F (X ∗
IV. EXPERIMENTS

i=1 (cid:107)X t+1

i (cid:107)2
F
N )].

1

i

A. Matrix Completion

We compare a number of low-rank matrix completion
solvers, including models based on (i) the commonly used
(convex) nuclear norm regularizer; (ii) ﬁxed-rank factoriza-
tion models [29, 30], which decompose the observed matrix
O into a product of rank-k matrices U and V . Its opti-
1
mization problem can be written as: minU,V
2 (cid:107)PΩ(U V −
O)(cid:107)2
F ); and (iii) nonconvex regularizers,
including the capped-(cid:96)1 (with θ in Table I set to 2λ), LSP
(with θ =

λ), and TNN (with θ = 3).

2 ((cid:107)U (cid:107)2
√

F + (cid:107)V (cid:107)2

F + λ

The nuclear norm minimization algorithms to be com-

pared include:

1) Accelerated proximal gradient (APG)4 algorithm [11,

28], with the partial SVD by PROPACK [27];

2) Soft-Impute5 [12], which iteratively replaces the miss-
ing elements with those obtained from SVT. The
“sparse plus low-rank” structure of the matrix iterate
is utilized to speed up computation (Section III-E);

3) Active alternating minimization6

(denoted “active
ALT”) [13], which adds/removes rank-one subspaces
from the active set in each iteration. The nuclear norm
optimization problem is then reduced to a smaller
problem deﬁned only on this active set.

We do not compare with the Frank-Wolfe algorithm [33]
and stochastic gradient descent [34], as they have been
shown to be less efﬁcient [13]. For the ﬁxed-rank factor-
ization models (where the rank is tuned by the validation
set), we compare with the two state-of-the-art algorithms:

1) Low-rank matrix ﬁtting (LMaFit) algorithm7 [29]; and
2) Rank-one matrix pursuit (R1MP) [30], which pursues

a rank-one basis in each iteration.

We do not compare with the concave-convex procedure [14,
16], since it has been shown to be inferior to IRNN [20].

For models with nonconvex low-rank regularizers, we

compare the following solvers:

1) Iterative reweighted nuclear norm (IRNN)8 [21];

where FY (X) = f (X, Y ) + λr(X), and FX (Y ) =
f (X, Y ) + β(cid:107)Y (cid:107)1. The resultant algorithm is similar to
Algorithm 2.

When F is convex, convergence of this alternating min-
imization scheme has been well studied [32]. However,

4http://perception.csl.illinois.edu/matrix-rank/Files/apg partial.zip
5http://cran.r-project.org/web/packages/softImpute/index.html
6http://www.cs.utexas.edu/∼cjhsieh/nuclear active 1.1.zip
7http://www.caam.rice.edu/∼optimization/L1/LMaFit/download.html
8https://sites.google.com/site/canyilu/ﬁle/2014-CVPR-IRNN.zip?

attredirects=0&d=1

Table III
MATRIX COMPLETION PERFORMANCE ON THE SYNTHETIC DATA. HERE, NMSE IS SCALED BY ×10−2, AND CPU TIME IS IN SECONDS.

nuclear
norm

ﬁxed
rank
capped
(cid:96)1

LSP

TNN

APG
Soft-Impute
active ALT
LMaFit
R1MP
IRNN
GPG
FaNCL
IRNN
GPG
FaNCL
IRNN
GPG
FaNCL

m = 500
(observed: 12.43%)

m = 1000
(observed: 6.91%)

m = 1500
(observed: 4.88%)

m = 2000
(observed: 3.80%)

NMSE
3.95±0.16
3.95±0.16
3.95±0.16
2.63±0.10
22.72±0.63
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07

rank
49
49
49
5
39
5
5
5
5
5
5
5
5
5

time
4.8
64.9
17.1
0.6
0.3
8.5
8.5
0.3
21.8
21.2
0.5
8.5
8.3
0.3

NMSE
3.90±0.05
3.90±0.05
3.90±0.05
2.85±0.10
20.89±0.66
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04

rank
59
59
59
5
54
5
5
5
5
5
5
5
5
5

time
59.5
176.0
81.9
1.7
0.8
75.5
72.4
0.9
223.9
235.3
2.2
72.6
71.7
0.8

NMSE
3.74±0.02
3.74±0.02
3.74±0.02
2.54±0.09
20.04±0.66
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02

rank
71
71
71
5
62
5
5
5
5
5
5
5
5
5

time
469.3
464.4
343.8
4.5
1.4
510.8
497.0
2.6
720.9
687.4
3.3
650.7
655.3
2.7

NMSE
3.69±0.04
3.69±0.04
3.69±0.04
2.40±0.09
19.53±0.61
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02

rank
85
85
85
5
63
5
5
5
5
5
5
5
5
5

time
1383.3
1090.2
860.1
7.1
3.4
1112.3
1105.8
4.1
2635.0
2612.0
7.6
1104.1
1098.2
4.2

2) Generalized proximal gradient (GPG) algorithm [22],
with the underlying problem (2) solved more efﬁ-
ciently using the closed-form solutions in [20];
3) The proposed FaNCL algorithm (Tpm = 3, p = 1).
All algorithms are implemented in Matlab. The same
stopping criterion is used, namely that the algorithm stops
when the difference in objective values between consecutive
iterations is smaller than a given threshold. Experiments are
run on a PC with i7 4GHz CPU and 24GB memory.

1) Synthetic Data: The observed m × m matrix is gen-
erated as O = U V + G, where the elements of U ∈
Rm×k, V ∈ Rk×m (with k = 5) are sampled i.i.d. from
the normal distribution N (0, 1), and elements of G sampled
from N (0, 0.1). A total of (cid:107)Ω(cid:107)1 = 2mk log(m) random ele-
ments in O are observed. Half of them are used for training,
and the rest as validation set for parameter tuning. Testing
is performed on the non-observed (missing) elements.
(i)

performance
mean
(i,j)(cid:54)∈Ω(Xij − [U V ]ij)2/

use
evaluation, we
NMSE
error
squared
(i,j)(cid:54)∈Ω[U V ]2
ij,

the
=
where
X is the recovered matrix; (ii) rank of X; and (iii) training
CPU time. We vary m in the range {500, 1000, 1500, 2000}.
Each experiment is repeated ﬁve times.

normalized
(cid:113)(cid:80)

(cid:113)(cid:80)

For

Results are shown in Table III. As can be seen, the non-
convex regularizers (capped-(cid:96)1, LSP and TNN) lead to much
lower NMSE’s than the convex nuclear norm regularizer and
ﬁxed-rank factorization. Moreover, as is also observed in
[34], the nuclear norm needs to use a much higher rank than
the nonconvex ones. In terms of speed, FaNCL is the fastest
among the nonconvex low-rank solvers. Figure 1 shows its
speedup over GPG (which in turn is faster than IRNN). As
can be seen, the larger the matrix, the higher is the speedup.
Recall that the efﬁciency of the proposed algorithm comes
from (i) automatic singular value thresholding; (ii) comput-
ing the proximal operator on a smaller matrix; and (iii)
exploiting the “sparse plus low-rank” structure in matrix
completion. Their individual contributions are examined in
Table IV. The baseline is GPG, which uses none of these;

Figure 1. Speedup of FaNCL over GPG at different matrix sizes.

while the proposed FaNCL uses all. As all the variants
produce the same solution, the obtained NMSE and rank
values are not shown. As can be seen, tricks (i), (ii) and (iii)
lead to average speedups of about 6, 4, and 3, respectively;
and are particularly useful on the large data sets.

Table IV
EFFECTS OF THE THREE TRICKS ON CPU TIME (IN SECONDS) USING
THE SYNTHETIC DATA. (I) AUTOMATIC SINGULAR VALUE
THRESHOLDING; (II) COMPUTING THE PROXIMAL OPERATOR ON A
SMALLER MATRIX; AND (III) “SPARSE PLUS LOW-RANK” STRUCTURE.

capped
(cid:96)1

LSP

TNN

solver
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)

500
8.5
5.4
0.6
0.3
21.2
4.9
1.0
0.5
8.3
5.4
0.6
0.3

1000
72.4
37.6
3.2
0.9
235.3
44.0
9.7
2.2
71.7
32.5
2.8
0.8

1500
497.0
114.8
11.4
2.6
687.4
70.0
14.8
3.3
655.3
122.3
10.3
2.7

2000
1105.8
203.7
25.6
6.8
2612.0
154.9
31.1
8.2
1098.2
194.1
15.8
3.3

2) MovieLens: Experiment is performed on the popular
MovieLens9 data set (Table V), which contain ratings of
different users on movies. We follow the setup in [30],

9http://grouplens.org/datasets/movielens/

(a) capped-(cid:96)1.

(b) LSP.

(c) TNN.

Figure 2. Objective value vs CPU time for the various nonconvex low-rank regularizers on the MovieLens-100K data set.

and use 50% of the observed ratings for training, 25%
for validation and the rest for testing. For performance
evaluation, we use the root mean squared error on the test
set Ω: RMSE = (cid:112)(cid:107)PΩ(X − O)(cid:107)2
F /(cid:107)Ω(cid:107)1, where X is the
recovered matrix. The experiment is repeated ﬁve times.

Table V
RECOMMENDATION DATA SETS USED IN THE EXPERIMENTS.

MovieLens

100K
1M
10M

netﬂix
yahoo

#users
943
6,040
69,878
480,189
249,012

#movies
1,682
3,449
10,677
17,770
296,111

#ratings
100,000
999,714
10,000,054
100,480,507
62,551,438

Results are shown in Table VI. Again, nonconvex reg-
ularizers lead to the lowest RMSE’s. Moreover, FaNCL
is also the fastest among the nonconvex low-rank solvers,
even faster than the state-of-the-art. In particular, it is the
only solver (among those compared) that can be run on the
MovieLens-10M data. Table VII examines the usefulness of
the three tricks. The behavior is similar to that as observed in
Table IV. Figures 2 and 3 compare the objective and RMSE
vs CPU time for the various methods on the MovieLens-
100K data set. As can be seen, FaNCL decreases the
objective and RMSE much faster than the others.

Table VII
EFFECTS OF THE THREE TRICKS ON CPU TIME (IN SECONDS) ON THE
MOVIELENS DATA.

capped
(cid:96)1

LSP

TNN

solver
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)

100K
10M
1M
523.6 > 104 > 105
1920.5 > 105
212.2
> 105
29.2
288.8
634.6
29.4
3.2
192.8 > 104 > 105
2353.8 > 105
35.8
> 105
212.4
5.6
0.7
616.3
25.6
572.7 > 104 > 105
1944.8 > 105
116.9
> 105
256.1
15.4
710.7
25.8
1.9

Figure 3. RMSE vs CPU time on the MovieLens-100K data set.

(a) netﬂix.

(b) yahoo.
Figure 4. RMSE vs CPU time on the netﬂix and yahoo data sets.

3) Netﬂix and Yahoo: Next, we perform experiments on
two very large recommendation data sets, Netﬂix10 and

10http://archive.ics.uci.edu/ml/datasets/Netﬂix+Prize

Table VI
MATRIX COMPLETION RESULTS ON THE MOVIELENS DATA SETS (TIME IS IN SECONDS).

nuclear norm

ﬁxed rank

capped-(cid:96)1

LSP

TNN

APG
Soft-Impute
active ALT
LMaFit
R1MP
IRNN
GPG
FaNCL
IRNN
GPG
FaNCL
IRNN
GPG
FaNCL

MovieLens-100K

MovieLens-1M

MovieLens-10M

RMSE
0.879±0.001
0.879±0.001
0.879±0.001
0.884±0.001
0.924±0.003
0.863±0.003
0.863±0.003
0.863±0.003
0.855±0.002
0.855±0.002
0.855±0.002
0.862±0.003
0.862±0.003
0.862±0.003

rank
36
36
36
2
5
3
3
3
2
2
2
3
3
3

time
18.9
13.8
4.1
3.0
0.1
558.9
523.6
3.2
195.9
192.8
0.7
621.9
572.7
1.9

RMSE
0.818±0.001
0.818±0.001
0.818±0.001
0.818±0.001
0.862±0.004
—
—
0.797±0.001
—
—
0.786±0.001
—
—
0.797±0.004

time
735.8
311.8
133.4
39.2
2.9

rank
67
67
67
6
19
— > 104
— > 104
29.4
5
— > 104
— > 104
25.6
5
— > 104
— > 104
25.8
5

RMSE
—
—
0.813±0.001
0.795±0.001
0.850±0.008
—
—
0.783±0.002
—
—
0.777±0.001
—
—
0.783±0.002

rank
time
— > 105
— > 105
3675.2
119
650.1
9
29
37.3
— > 105
— > 105
634.6
8
— > 105
— > 105
616.3
9
— > 105
— > 105
710.7
8

Yahoo11 (Table V). We randomly use 50% of the observed
ratings for training, 25% for validation and the rest for
testing. Each experiment is repeated ﬁve times.

Results are shown in Table VIII. APG, Soft-Impute,
GPG and IRNN cannot be run as the data set is large.
Figure 4 shows the objective and RMSE vs time for the
compared methods.12 Again,
the nonconvex regularizers
converge faster, yield lower RMSE’s and solutions of much
lower ranks. Moreover, FaNCL is fast.

B. Robust Principal Component Analysis

1) Synthetic Data: In this section, we ﬁrst perform ex-
periments on a synthetic data set. The observed m × m
matrix is generated as O = U V + ˜Y + G, where elements
of U ∈ Rm×k, V ∈ Rk×m (with k = 0.01m) are sampled
i.i.d. from N (0, 1), and elements of G are sampled from
N (0, 0.1). Matrix ˜Y is sparse, with 1% of its elements
randomly set to 5(cid:107)U V (cid:107)∞ or −5(cid:107)U V (cid:107)∞ with equal prob-
abilities. The sparsity regularizer is the standard (cid:96)1, while
different convex/nonconvex low-rank regularizers are used.
For performance evaluation, we use (i) NMSE = (cid:107)(X +
Y ) − (U V + ˜Y )(cid:107)F /(cid:107)U V + ˜Y (cid:107)F , where X and Y are the
recovered low-rank and sparse components, respectively in
(5); (ii) accuracy on locating the sparse support of ˜Y (i.e.,
percentage of entries that both ˜Yij and Yij are nonzero or
zero together); and (iii) the recovered rank. We vary m in
{500, 1000, 1500, 2000}. Each experiment is repeated ﬁve
times.

Note that IRNN and the active subspace selection method
cannot be used here. Their objectives are of the form
“smooth function plus low-rank regularizer”, while RPCA
has a nonsmooth (cid:96)1 regularizer besides its low-rank regular-
izer. Similarly, Soft-Impute is for matrix completion only.

11http://webscope.sandbox.yahoo.com/catalog.php?datatype=c
12On these two data sets, R1MP easily overﬁts as the rank increases.
Hence, the validation set selects a rank which is small (relative to that
obtained by the nuclear norm) and R1MP stops earlier. However, as can be
seen, its RMSE is much worse.

Results are shown in Table IX. The accuracy on locating
the sparse support are always 100% for all methods, and
thus are not shown. As can be seen, while both convex
and nonconvex regularizers can perfectly recover the matrix
rank and sparse locations, the nonconvex regularizers have
lower NMSE’s. Moreover, as in matrix completion, FaNCL
is again much faster. The larger the matrix, the higher is the
speedup.

2) Background Removal on Videos: In this section, we
use RPCA to perform video denoising on background re-
moval of corrupted videos. Four benchmark videos13 in
[7, 8] are used (Table X), and example image frames are
shown in Figure 5. As discussed in [7], the stable image
background can be treated as low-rank, while the foreground
moving objects contribute to the sparse component.

Table X
VIDEOS USED IN THE EXPERIMENT.

#pixels / frame
total #frames

bootstrap
19,200
9,165

campus
20,480
4,317

escalator
20,800
10,251

hall
25,344
10,752

(a) bootstrap.

(b) campus.

(c) escalator.

(d) hall.

Figure 5. Example image frames in the videos.

Each image frame is reshaped as a column vector, and
all frames are then stacked together to form a matrix. The
pixel values are normalized to [0, 1], and Gaussian noise
from N (0, 0.15) is added. The experiment is repeated ﬁve
times.

For performance evaluation, we use the commonly used
peak signal-to-noise ratio [35]: PSNR = −10 log10(MSE),
j=1 (Xij − Oij)2, X ∈ Rm×n
where MSE = 1
mn
is the recovered video, and O ∈ Rm×n is the ground-truth.

(cid:80)m

(cid:80)n

i=1

13http://perception.i2r.a-star.edu.sg/bk model/bk index.html

Table VIII
RESULTS ON THE NETFLIX AND YAHOO DATA SETS (CPU TIME IS IN HOURS).

nuclear norm active ALT

ﬁxed rank

capped-(cid:96)1
LSP
TNN

LMaFit
R1MP
FaNCL
FaNCL
FaNCL

netﬂix

yahoo

RMSE
0.814 ± 0.001
0.813 ± 0.003
0.861 ± 0.006
0.799 ± 0.001
0.793 ± 0.002
0.798 ± 0.001

rank
399
16
31
15
13
17

time
47.6
2.4
0.2
2.5
1.9
3.3

RMSE
0.680 ± 0.001
0.667 ± 0.002
0.810 ± 0.005
0.650 ± 0.001
0.650 ± 0.001
0.655 ± 0.002

rank
221
10
92
8
9
8

time
118.9
6.6
0.3
5.9
6.1
6.2

Table IX
RPCA PERFORMANCE OF THE VARIOUS METHODS ON SYNTHETIC DATA. THE STANDARD DEVIATIONS OF NMSE ARE ALL SMALLER THAN 0.0002
AND SO NOT REPORTED. CPU TIME IS IN SECONDS.
m = 1000

m = 500

nuclear norm
capped-(cid:96)1

LSP

TNN

APG
GPG
FaNCL
GPG
FaNCL
GPG
FaNCL

NMSE
0.46
0.36
0.36
0.36
0.36
0.36
0.36

rank
5
5
5
5
5
5
5

time
1.5
0.9
0.2
2.7
0.4
0.8
0.2

NMSE
0.30
0.25
0.25
0.25
0.25
0.25
0.25

rank
10
10
10
10
10
10
10

time
9.7
6.7
1.4
18.5
1.8
6.0
1.2

m = 1500
rank
15
15
15
15
15
15
15

NMSE
0.25
0.21
0.21
0.21
0.21
0.21
0.21

time
33.9
18.7
2.7
111.2
3.9
23.1
2.9

m = 2000
rank
20
20
20
20
20
20
20

NMSE
0.18
0.15
0.15
0.15
0.15
0.15
0.15

time
94.7
60.4
6.5
250.2
7.1
51.4
5.8

(a) original.

(c) capped-(cid:96)1.
Figure 6. Example foreground images in bootstrap, as recovered by using various low-rank regularizers.

(b) nuclear norm.

(d) LSP.

(e) TNN.

Results are shown in Table XI. As can be seen, the non-
convex regularizers lead to better PSNR’s than the convex
nuclear norm. Moreover, FaNCL is more than 10 times faster
than GPG. Figure 6 shows an example of the recovered
foreground in the bootstrap video. As can been seen, the
nonconvex regularizers can better separate foreground from
background. Figure 7 shows the PSNR vs time on bootstrap.
Again, FaNCL converges much faster than others.

be automatically thresholded, and also that the proximal
operator can be computed on a smaller matrix. For matrix
completion, extra speedup can be achieved by exploiting
the “sparse plus low-rank” structure of the matrix estimate
in each iteration. The resultant algorithm is guaranteed to
converge to a critical point of the nonconvex optimization
problem. Extensive experiments on matrix completion and
RPCA show that the proposed algorithm is much faster
than the state-of-art convex and nonconvex low-rank solvers.
It also demonstrates that nonconvex low-rank regularizers
outperform the convex nuclear norm regularizer in terms of
recovery accuracy and the rank obtained.

ACKNOWLEDGMENT

This research was supported in part by the Research
Grants Council of the Hong Kong Special Administrative
Region (Grant 614513).

REFERENCES

[1] Q. Yao, J. T. Kwok, and W. Zhong, “Fast low-rank matrix learning

with nonconvex regularization,” 2015.

[2] E. J. Cand`es and B. Recht, “Exact matrix completion via convex
optimization,” Foundations of Computational Mathematics, vol. 9,
no. 6, pp. 717–772, 2009.

[3] P. Biswas, T.-C. Lian, T.-C. Wang, and Y. Ye, “Semideﬁnite pro-
gramming based algorithms for sensor network localization,” ACM
Transactions on Sensor Networks, vol. 2, no. 2, pp. 188–220, 2006.

Figure 7. PSNR vs CPU time on the bootstrap data set.

V. CONCLUSION

In this paper, we considered the challenging problem of
nonconvex low-rank matrix optimization. The key obser-
vations are that for the popular low-rank regularizers, the
singular values obtained from the proximal operator can

Table XI
PSNR (IN DB) AND CPU TIME (IN SECONDS) ON THE VIDEO BACKGROUND REMOVAL EXPERIMENT. FOR COMPARISON, THE PSNRS FOR ALL THE
INPUT VIDEOS ARE 16.47DB.
campus

bootstrap

escalator

hall

nuclear norm
capped-(cid:96)1

LSP

TNN

APG
GPG
FaNCL
GPG
FaNCL
GPG
FaNCL

PSNR
23.01±0.03
24.00±0.03
24.00±0.03
24.29±0.03
24.29±0.03
24.06±0.03
24.06±0.03

time
688.4
1009.3
60.4
1420.2
56.0
1047.5
86.3

PSNR
22.90±0.02
23.14±0.02
23.14±0.02
23.96±0.02
23.96±0.02
23.11±0.02
23.11±0.02

time
102.6
90.6
12.4
88.9
17.4
130.3
12.6

PSNR
23.56±0.01
24.33±0.02
24.33±0.02
24.13±0.01
24.13±0.01
24.29±0.01
24.29±0.01

time
942.5
1571.2
68.3
1523.1
54.5
1857.7
69.6

PSNR
23.62±0.01
24.95±0.02
24.95±0.02
25.08±0.01
25.08±0.01
24.98±0.02
24.98±0.02

time
437.7
620.0
34.7
683.9
35.8
626.2
37.4

[4] M. Kim and J. Leskovec, “The network completion problem: Inferring
missing nodes and edges in networks,” in Proceedings of the 11th
International Conference on Data Mining, 2011, pp. 47–58.

[5] J. Liu, P. Musialski, P. Wonka, and J. Ye, “Tensor completion for
estimating missing values in visual data,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 208–
220, 2013.

[6] Q. Yao and J. Kwok, “Colorization by patch-based local low-rank

matrix completion,” 2015.

[7] E. J. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal
component analysis?” Journal of the ACM, vol. 58, no. 3, p. 11,
2011.

[8] Q. Sun, S. Xiang, and J. Ye, “Robust principal component analysis via
capped norms,” in Proceedings of the 19th International Conference
on Knowledge Discovery and Data Mining, 2013, pp. 311–319.
[9] G. Zhu, S. Yan, and Y. Ma, “Image tag reﬁnement towards low-
rank, content-tag prior and error sparsity,” in Proceedings of the
International Conference on Multimedia, 2010, pp. 461–470.
[10] P. Gong, J. Ye, and C. Zhang, “Robust multi-task feature learning,”
in Proceedings of the 18th international Conference on Knowledge
Discovery and Data Mining, 2012, pp. 895–903.

[11] S. Ji and J. Ye, “An accelerated gradient method for trace norm
minimization,” in Proceedings of the 26th International Conference
on Machine Learning, 2009, pp. 457–464.

[12] R. Mazumder, T. Hastie, and R. Tibshirani, “Spectral regularization
algorithms for learning large incomplete matrices,” Journal of Ma-
chine Learning Research, vol. 11, pp. 2287–2322, 2010.

[13] C.-J. Hsieh and P. Olsen, “Nuclear norm minimization via active sub-
space selection,” in Proceedings of the 31st International Conference
on Machine Learning, 2014, pp. 575–583.

[14] T. Zhang, “Analysis of multi-stage convex relaxation for sparse
regularization,” Journal of Machine Learning Research, vol. 11, pp.
1081–1107, 2010.

4130–4137.

[22] C. Lu, C. Zhu, C. Xu, S. Yan, and Z. Lin, “Generalized singular
value thresholding,” in Proceedings of the 29th AAAI Conference on
Artiﬁcial Intelligence, 2015.

[23] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with
randomness: Probabilistic algorithms for constructing approximate
matrix decompositions,” SIAM Review, vol. 53, no. 2, pp. 217–288,
2011.

[24] N. Parikh and S. Boyd, “Proximal algorithms,” Foundations and

Trends in Optimization, vol. 1, no. 3, pp. 127–239, 2014.

[25] J.-F. Cai, E. J. Cand`es, and Z. Shen, “A singular value thresholding
algorithm for matrix completion,” SIAM Journal on Optimization,
vol. 20, no. 4, pp. 1956–1982, 2010.

[26] S. Wang, D. Liu, and Z. Zhang, “Nonconvex relaxation approaches
to robust matrix recovery,” in Proceedings of the 23rd International
Joint Conference on Artiﬁcial Intelligence, 2013, pp. 1764–1770.
[27] R. M. Larsen, “Lanczos bidiagonalization with partial reorthogonal-
ization,” Department of Computer Science, Aarhus University, DAIMI
PB-357, 1998.

[28] K.-C. Toh and S. Yun, “An accelerated proximal gradient algorithm
for nuclear norm regularized linear least squares problems,” Paciﬁc
Journal of Optimization, vol. 6, no. 615-640, p. 15, 2010.

[29] Z. Wen, W. Yin, and Y. Zhang, “Solving a low-rank factorization
model for matrix completion by a nonlinear successive over-relaxation
algorithm,” Mathematical Programming Computation, vol. 4, no. 4,
pp. 333–361, 2012.

[30] Z. Wang, M.-J. Lai, Z. Lu, W. Fan, H. Davulcu, and J. Ye, “Rank-
one matrix pursuit for matrix completion,” in Proceedings of the 31st
International Conference on Machine Learning, 2014, pp. 91–99.

[31] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of
subspace structures by low-rank representation,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 171–
184, 2013.

[15] E. J. Cand`es, M. B. Wakin, and S. P. Boyd, “Enhancing sparsity
by reweighted (cid:96)1 minimization,” Journal of Fourier Analysis and
Applications, vol. 14, no. 5-6, pp. 877–905, 2008.

[32] P. Tseng, “Convergence of a block coordinate descent method for
nondifferentiable minimization,” Journal of Optimization Theory and
Applications, vol. 109, no. 3, pp. 475–494, 2001.

[16] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He, “Fast and accurate
matrix completion via truncated nuclear norm regularization,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 35,
no. 9, pp. 2117–2130, 2013.

[17] J. Fan and R. Li, “Variable selection via nonconcave penalized like-
lihood and its oracle properties,” Journal of the American Statistical
Association, vol. 96, no. 456, pp. 1348–1360, 2001.

[33] X. Zhang, D. Schuurmans, and Y.-l. Yu, “Accelerated training for
matrix-norm regularization: A boosting approach,” in Advances in
Neural Information Processing Systems, 2012, pp. 2906–2914.
[34] H. Avron, S. Kale, V. Sindhwani, and S. P. Kasiviswanathan, “Efﬁ-
cient and practical stochastic subgradient descent for nuclear norm
regularization,” in Proceedings of the 29th International Conference
on Machine Learning, 2012, pp. 1231–1238.

[18] C.-H. Zhang, “Nearly unbiased variable selection under minimax
concave penalty,” Annals of Statistics, vol. 38, no. 2, pp. 894–942,
2010.

[35] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Image denoising
by sparse 3-D transform-domain collaborative ﬁltering,” IEEE Trans-
actions on Image Processing, vol. 16, no. 8, pp. 2080–2095, 2007.

[19] A. L. Yuille and A. Rangarajan, “The concave-convex procedure,”

Neural Computation, vol. 15, no. 4, pp. 915–936, 2003.

[20] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye, “A general iterative
shrinkage and thresholding algorithm for non-convex regularized
optimization problems,” in Proceedings of
the 30th International
Conference on Machine Learning, 2013, pp. 37–45.

[21] C. Lu, J. Tang, S. Yan, and Z. Lin, “Generalized nonconvex nons-
mooth low-rank minimization,” in Proceedings of the International
Conference on Computer Vision and Pattern Recognition, 2014, pp.

[36] S. Gu, L. Zhang, W. Zuo, and X. Feng, “Weighted nuclear norm
minimization with application to image denoising,” in Proceedings
of the International Conference on Computer Vision and Pattern
Recognition, 2014, pp. 2862–2869.

[37] S. Boyd and L. Vandenberghe, Convex optimization.

Cambridge

university press, 2004.

A. Proof of Proposition II.2

APPENDIX

If the optimal y∗
obviously y∗

i ≤ σi. Otherwise,

i is achieved at the boundary (i.e., y∗

i = 0),

This Proposition appears in [22, 36], for completeness, we
also present a proof here. First, note that for a matrix X ∈
Rm×n and any orthogonal projection matrices P ∈ Rm×m
and Q ∈ Rn×n (where P (cid:62)P = I, Q(cid:62)Q = I), X has the
same singular values with P (cid:62)XQ. Therefore, Assumption
A3 implies r is invariant to orthogonal projection, i.e.,

r(X) = r(P (cid:62)XQ)

0 ∈ y∗

i − σi + µ∂ ˆr(y∗

i ).

(8)

Since ˆr(x) is non-decreasing on x ≥ 0, its super-gradient
∂ ˆr(·) is non-negative on R+, and so y∗

i ≤ σi.

2) : Now, consider an (i, j) pair such that σj ≥ σi.

Assume that y∗

i ≥ 0 and y∗

j ≥ 0. From (8), we have

Then, we introduce the following Proposition in [37].

0 ∈ y∗

i − σi + µ∂ ˆr(y∗

i ) and 0 ∈ y∗

j − σj + µ∂ ˆr(y∗

j ).

Proposition A.1. Let (u∗, v∗) = arg maxu,v(u(cid:62)Xv)
:
(cid:107)u(cid:107)2 = (cid:107)v(cid:107)2 = 1. Then, u∗ (resp. v∗) is the left (resp.
right) singular vector of X, and the optimal objective value
is σ1, the largest singular value of X.

Let the SVD of X be P ΣX Q(cid:62). Since, (cid:107) · (cid:107)F and r(·) are

invariant to orthogonal projections,

P µ

r(·)(Z) = min
X

1
2

(cid:107)X − Z(cid:107)2

F + µr(X)

(6)

1
2
1
2

= min

P,ΣX ,Q

= min

P,ΣX ,Q
1
= min
2
ΣX
− max
P,Q

(cid:107)P ΣX Q(cid:62) − Z(cid:107)2

F + µr(P ΣX Q(cid:62))

(cid:107)ΣX − P (cid:62)ZQ(cid:107)2

F + µr(ΣX )

tr(Σ2

X + Z (cid:62)Z) + µr(ΣX )

tr(ΣX P (cid:62)ZQ).

Let pi (resp. qi) be the ith column of P (resp. Q). We have

max
P,Q

tr(ΣX P (cid:62)ZQ) =

[σX ]i max
pi,qi

p(cid:62)
i Zqi.

n
(cid:88)

i=1

Recall that the SVD of Z is U ΣV (cid:62). Using Proposition A.1,
σ1 = maxp1,q1 p(cid:62)
1 Zq1, and p1 = u1, q1 = v1 where ui
(resp. vi) is the ith column of U (resp. V ). Since pi
(cid:54)=
pj, qi (cid:54)= qj if i (cid:54)= j, again by Proposition A.1, we have
p2 = u2, q2 = v2, and so on. Hence, P = U, Q = V and
Σ = P (cid:62)ZQ, then (6) can then be rewritten as:

j > y∗

i , and thus ∂ ˆr(y∗

i , and thus ∂ ˆr(y∗
j ) ≥ ∂ ˆr(y∗

Again from assumption A3, since ˆr(x) is concave and
non-decreasing on x ≥ 0, its super-gradient ∂ ˆr(·) is thus also
non-increasing. If y∗
is not achieved at the boundary (i.e.,
i
is locally optimal), consider σj > σi > 0. To ensure that (8)
holds, we can ether (i) y∗
j ) ≤ ∂ ˆr(y∗
i );
or (ii) y∗
j < y∗
i ). However,
∂ ˆr(·) is non-negative, and thus lower-bounded. Hence, there
always exists y∗
to ensure (8). If multiple solutions
exist, we take the largest one. So, we must have y∗
j > y∗
i .
i and y∗
3) : Thus, the smaller the σi, the smaller is y∗
i ≤
i ) is non-increasing on R+, µˆr(y∗
σi. Since ˆr(y∗
i ) will not
become smaller. Thus, there must exists γ such that once
σi ≤ γ, (8) no longer holds, and y∗
is not locally optimal
i
and lies on the boundary (i.e., y∗
i = 0).

j > y∗
i

C. Proof of Proposition III.3

Since span(Uˆk) ⊆ span(Q) and Q is orthogonal, it can
be written as Q = [U=; U⊥]R where span(U=) = span(Uˆk),
⊥ Uˆk = 0 and R is a rotation matrix (RR(cid:62) = R(cid:62)R =
U (cid:62)
I). Thus, Q(cid:62)Z t = R(cid:62)[U=; U⊥](cid:62)Z t and its rank-ˆk SVD is
R(cid:62)[U=; 0](cid:62)UˆkΣV (cid:62)
ˆk

. Using Proposition II.2,

proxµ(r(·))Q(cid:62)Z t = R(cid:62)

(cid:21)

(cid:20)U (cid:62)
=
0

Uˆk

ˆΣV (cid:62)
ˆk

,

min
ΣX

1
2

(cid:107)ΣX − Σ(cid:107)2

F + µr(ΣX )

(7)

where ˆΣ = Diag(y∗
Then note that,

1, . . . , y∗
ˆk

) is the optimal solution in (2).

= min
y≥0

1
2

n
(cid:88)

i=1

which leads to (2).

(yi − σi)2 + µr(Diag(y)),

B. Proof of Proposition III.1
1) : First, we show y∗

i ≤ σi. By assumption A3, (7) (or,
equivalently, (2) with Proposition II.2) can be rewritten as

min
ΣX

1
2

(cid:107)ΣX − Σ(cid:107)2

F + µ

ˆr ([σX ]i)

n
(cid:88)

i=1

=

n
(cid:88)

i=1

min
yi≥0

1
2

(yi − σi)2 + µˆr(yi),

Qproxµ(r(·))Q(cid:62)Z t = [U=; U⊥]RR(cid:62)
(cid:20)U (cid:62)
=
0
ˆΣV (cid:62)
ˆk

= [U=; U⊥]

= U=U (cid:62)

= Uˆk

(cid:21)

.

(cid:21)

(cid:20)U (cid:62)
=
0

Uˆk

ˆΣV (cid:62)
ˆk

Uˆk

ˆΣV (cid:62)
ˆk

Since span(Uˆk) = span(U=), so U=U (cid:62)

= = UˆkU (cid:62)
ˆk

, and

U=U (cid:62)

= Uˆk

ˆΣV (cid:62)
ˆk

= Uˆk(U (cid:62)
ˆk

Uˆk) ˆΣV (cid:62)
ˆk

= Uˆk

ˆΣV (cid:62)
ˆk

,

which is proxµ(r(·))Z t.

D. Proof of Proposition III.4

Since (3) holds, we have

F (X t+1) ≤ F (X t) − c1(cid:107)X t+1 − X t(cid:107)F .

Sum it from 0 to T , we have

Since span(U=) = span(U ), we have U=U (cid:62)
ˆU = R(cid:62)[U=, 0](cid:62)U . Then,

= = U U (cid:62). Let

ˆU (cid:62) ˆU = U (cid:62)[U=, 0]RR(cid:62)[U=, 0](cid:62)U

= U (cid:62)U=U (cid:62)

= U = (U (cid:62)U )(U (cid:62)U ) = I.

F (X 0) − F (X T +1) ≥ c1

(cid:107)X t+1 − X t(cid:107)2
F .

(9)

Hence, ˆU ΣV (cid:62) is the reduced SVD of P (cid:62)X. Similarly, for
Q, we obtain that Σ is also the singular values of P (cid:62)XQ.

T
(cid:88)

t=1

By assumption A2, F (X) is bounded below. Thus, as

T → +∞, there exists a ﬁnite constant α such that

H. Proof of Theorem III.8

α =

(cid:107)X t+1 − X t(cid:107)2
F .

(10)

+∞
(cid:88)

t=1

Hence, we must have lim
t→∞

{X t} converges to a limit point X ∗.

(cid:107)X t+1 − X t(cid:107)2

F = 0, and thus

E. Proof of Theorem III.5

Next, we show that X ∗ is a critical point of (1). First, as in
[20], it is easy to see that r(·) here can also be decomposed
as the difference of two convex functions r1(X) and r2(X)
i.e., r(X) = r1(X)−r2(X). Consider the optimal conditions
in proximal step, we have

0 ∈ ∇f (X t) + X t+1 − X t

(11)

+ λ∂r1(X t+1) − λ∂r2(X t+1).

For limit point X t+1 = X t = X ∗, so X t+1 − X t = 0

and vanish. Thus,

0 ∈ ∇f (X ∗) + λ∂r1(X ∗) − λ∂r2(X ∗),

and X ∗ is a critical point of (1).

F. Proof of Corollary III.6

In Theorem III.5, we have shown Algorithm 2 can con-
verge to a critical point of (1). Then, from (9), rearrange
items we will have

min
t=1,··· ,T

(cid:107)X t+1 − X t(cid:107)2

F ≤

(cid:107)X t+1 − X t(cid:107)2
F

1
T

T
(cid:88)

t=1

≤

1
c1T

Here, we prove the case for two blocks of parameters
(5) as an example. Extension to multiple block is easily
obtained.

1) : Let ∆2

Y t = (cid:107)Y t+1 −
X t = (cid:107)X t+1 − X t(cid:107)F , and ∆2
Y t(cid:107)F . When sufﬁcient decrease holds for both X and Y ,
we have

F (X t+1, Y t+1) ≤ F (X t+1, Y t) − c1∆2
Y t

≤ F (X t, Y t) − c1∆2

X t − c1∆2
Y t

Summarize above from t = 0 to T , we get

F (X 1, Y 1) − F (X T +1, Y T +1) ≥ c1

(cid:0)∆2

X t + ∆2
Y t

(cid:1) . (12)

T
(cid:88)

t=0

Since F (X, Y ) is bounded below, L.H.S above is a ﬁnite

positive constant. Same as (10):

lim
t→∞

(cid:107)X t+1 − X t(cid:107)F = 0,

(cid:107)Y t+1 − Y t(cid:107)F = 0.

lim
t→∞

Thus, (cid:80)T
2) : From the optimal conditions of the proximal step,

X t + ∆2
Y t

(cid:1) ≤ +∞.

(cid:0)∆2

t=0

similar to (11), we have

0 ∈ ∇Y f (X ∗, Y ∗) + β∂(cid:107)Y ∗(cid:107)1,
0 ∈ ∇X f (X ∗, Y ∗) + λ∂r1(X ∗) − λ∂r2(X ∗).

Thus, (X ∗, Y ∗) is a critical point of (5).

3) : Finally, using same technique at proof of Corol-

lary III.6 and (12), it is easy to obtain

(cid:2)F (X 1) − F (X T +1)(cid:3) ,

min
t=1,··· ,T

(cid:0)∆2

X t + ∆2
Y t

(cid:1) ≤

1
c1T

(cid:2)F (X 1, Y 1) − F (X T +1, Y T +1)(cid:3) .

which proves the Corollary.

G. Proof of Proposition III.7

By deﬁnition of the Frobenoius norm, we only need
to show that P (cid:62)XQ has the same singular values as
X. Since U ⊆ P , we partition P as P = [U=, U⊥]R,
where span(U=) = span(U ), U⊥ is orthogonal to U (i.e.,
U (cid:62)

= U⊥ = 0), and R is a rotation matrix. Then,
P (cid:62)X = R(cid:62) [U=, U⊥](cid:62) U ΣV (cid:62) = R(cid:62)[U=, 0](cid:62)U ΣV (cid:62).

I. Solution of GSVT and details of Corollary III.2

To simplify notations, we write yi as y, and σi as σ. Our
focus here is γ and is derived based on GIST [20]. For
LSP, MCP and SCAD, the relationship between different
stationary points is ignored in [20], thus their solutions are
not necessarily the local optimal.

1) (cid:96)1-regularizer: The

at
Lemma II.1, as it can be seen that γ = µ for the nuclear
norm.

closed-form solution is

2) LSP: For LSP, (2) becomes

h(y) ≡

(y − σ)2 + µ log

1 +

min
y

(cid:16)

(cid:17)

.

y
θ

1
2

If σ = 0, obviously y∗ = 0. So we only need to consider
σ > 0. Now,

∇h(y) = y − σ +

µ
y + θ

.

Since θ, y > 0,

(θ + y)∇h(y) = (y + θ)(y − σ) + µ.

= y2 − (σ − θ)y + µ − θσ.

(13)

Case 1: ∆ ≡ (σ + θ)2 − 4µ ≤ 0: Then ∇h(y) ≥ 0 on
R+, and thus h(y) is non-deceasing on y ≥ 0. If 0 ≤ σ ≤
min (cid:0)0, −θ + 2

µ(cid:1), we have arg miny h(y) = 0.

√

Case 2: ∆ > 0. The square roots of y2 − (σ − θ)y + µ −

θσ = 0 in (13) are
1
2
1
2

ˆy1 =

ˆy2 =

(cid:16)

(cid:16)

(cid:17)
σ − θ − (cid:112)(σ + θ)2 − 4µ
(cid:17)
σ − θ + (cid:112)(σ + θ)2 − 4µ

,

.

Since h(y) has two stationary points, it is of the form in
Figure 8, and y∗ depends only on

h(0) =

σ2,

h(ˆy2) = h

(σ − θ +

∆)

.

√

(cid:19)

(cid:18) 1
2

1
2

Thus, if h(0) < h(ˆy2), y∗ = 0. When h(0) = h(ˆy2), we take
the largest one as y∗ = max (0, ˆy2) (and thus the solution
may not be unique). Finally, when h(0) > h(ˆy2), we have
y∗ = ˆy2.

Figure 8.

Illustration for Case 2.

However, obtaining γ by directly comparing h(0) and
h(ˆy2) is complicated and has no simple closed-form solu-
tion. Here, we take a simpler approach. Once ˆy2 ≤ 0, we
(cid:1), we have ˆy2 ≤ 0 and
have y∗ = 0. I.e., if σ ≤ min (cid:0)θ, µ
y∗ = 0.

θ

Finally, on combining both cases, the threshold for LSP

can be obtained as
(cid:110)

γ = max

= min

(cid:17)

, θ

.

(cid:16) µ
θ

min (0, −θ + 2

µ) , min

√

(cid:17)(cid:111)

(cid:16) µ
θ

, θ

Lemma A.2. When r(·) is the LSP, the optimal solution
of the corresponding proximal operator is proxµ(r(·))Z =
U Diag(y∗



n)V (cid:62) where

1, . . . , y∗

y∗
i =

σi ≤ min(0, −θ + 2
0
0
σi > min(0, −θ + 2
ˆy2 σi > min(0, −θ + 2



√
√
√

µ),
µ) and h(0) < h(ˆy2),
µ) and h(0) ≥ h(ˆy2).
2 (σi − θ + (cid:112)(σ + θ)2 − 4µ).

depends only on σi, and ˆy2 = 1

3) Capped (cid:96)1: Problem (2) then becomes

1
2
This can be written as

h(y) ≡

(y − σ)2 + µ min (y, θ) .

arg min h(y) =

(cid:26) arg min h1(y)
arg min h2(y)

y ≤ θ
y > θ

,

where h1(y) = 1
The optimal cases for h1(y) are:

2 (y −σ)2 +µy, and h2(y) = 1

2 (y −σ)2 +µθ.






h1(y∗ = 0) = 0,
h1(y∗ = σ − λ) = − 1
h1(y∗ = θ) = 1

2 µ2 + µσ,
2 (θ − σ)2 + µσ,

σ − µ ≤ 0,
0 < σ − µ < θ,
θ ≤ σ − µ,

And those for h2(y) are:

(cid:40)

h2(y∗ = θ) = 1
h2(y∗ = σ) = µθ,

2 (θ − σ)2 + µθ, σ ≤ θ,
σ > θ.

Consider cases of θ ≤ µ and θ > µ. Taking the minimum
over above functions, the optimal of x∗ is:
(cid:40)

y∗ =

(σ − µ)+ σ ≤ θ + 1
σ > θ + 1
θ

2 µ,
2 µ.

Thus, for the capped-(cid:96)1 regularizer, γ = min(µ, 1
2 λ + θ).
Combining with Proposition II.2, we obtain the following:

Lemma A.3. When r(·) is the capped-(cid:96)1,
mal solution of
proxµ(r(·))Z = U Diag(y∗

the opti-
the corresponding proximal operator is
n)V (cid:62), where

1, . . . , y∗

(cid:40)

y∗
i =

(σi − µ)+ σi ≤ θ + 1
σi > θ + 1
θ

2 µ
2 µ

,

and depends on σi.

4) TNN: For the TNN, it directly controls the number of
singular values. However, from Lemma A.4, it is to see that
γ = min (µ, σθ+1).

Lemma A.4. [16] When r(·) is the TNN regularizer, the
optimal solution of the proximal operator is

proxµ(r(·))Z = U

(cid:16)

Σ − µ ˜Iθ

(cid:17)

+

V (cid:62),

Using Proposition II.2, the optimal solution is shown in
Lemma A.2.

where ˜Ik is the square matrix with all zeros elements except
at positions [ ˜Iθ]ii = 1 for i > θ.

(3). If 0 < θ < 1, again it is a quadratic function, but the
coefﬁcient on the quadratic term is negative. Thus

Thus, we can get
(1). For h1(y), the optimal is

5) MCP: For MCP, again, y∗ for problem (2) becomes

(cid:40)

y∗ =

arg min h1(y) 0 ≤ y ≤ θµ
arg min h2(y)

y > θµ

where h1(y) and h2(y) are deﬁned as
1
2
1
2

1
θ
y2 − σy +

h1(y) =

h2(y) =

σ2 +

(1 −

1
2

1
2

1
2

θµ2

)y2 − (σ − µ)y +

σ2

For h1(y), the optimal depends on θ as:

(1). If θ = 1, then the optimal is:

y∗ =

0 ≤ σ ≤ µ

(cid:40)
0
µ σ > µ

(2). If θ > 1, note that it is a quadratic function and the

optimal depends on ¯y = θ(σ−µ)

θ−1 . As a result:

y∗ =






0 ≤ σ ≤ µ
µ < σ < θµ

0
¯y
θµ σ ≥ θµ

(cid:40)

y∗ =

0 ≤ σ ≤ 1

0
θµ σ > 1

2 θµ + 1
2 µ

2 θµ + 1

2 µ

Then, for h2(y), it is simple:

(cid:40)

y∗ =

θµ 0 ≤ σ ≤ θµ
σ > θµ
σ

Combine h1(y) and h2(y):
(1). If θ = 1, then

(cid:40)

y∗ =

0 ≤ σ ≤ µ

0
σ σ > µ

(2). If θ > 1, then (¯y = θ(σ−µ)

θ−1 ):

y∗ =





0 ≤ σ ≤ µ
0
¯y
µ < σ < θµ and h1(¯y) ≤ h2(θµ)
θµ µ < σ < θµ and h1(¯y) > h2(θµ)
σ

σ ≥ θµ

(14)

(15)

(3). If 0 < θ < 1, we need to compare h1(0) and h2(σ),

then we have:

(cid:40)

y∗ =

0
σ σ >

0 ≤ σ ≤
θµ

√

√

θµ

(16)

y∗
i =

Thus, γ for MCP is:

(cid:40)√

γ =

θµ 0 < θ < 1
θ ≥ 1

µ

Using Proposition II.2, the optimal solution is shown in
Lemma A.5.

Lemma A.5. When r(·) is the MCP, the optimal solution of
the corresponding proximal operator is:

proxµ(r(·))Z = U Diag(y∗

n)V (cid:62),
where y∗
i depends on θ and σi, i.e. if θ > 1, then y∗
by (14); then if θ = 1, then y∗
i
0 < θ < 1, y∗
i is given by (16).

i is given
is given by (15); ﬁnally, if

1, . . . , y∗

6) SCAD: Again, it can be written as (θ > 2)

y∗ =

0 ≤ y ≤ µ

arg min h1(y)
arg min h2(y) µ < y ≤ θµ
arg min h3(y)

θµ < y

where h1(y), h2(y) and h3(y) are deﬁned as

h1(y) =

(y − σ)2 + µy,

h2(y) =

(y − σ)2 +

h3(y) =

(y − σ)2 +

−y2 + 2θµy − µ2
2(θ − 1)

,

(θ + 1)µ2
2

.






1
2
1
2
1
2

(2). For h2(y), the optimal is

y∗ =

(cid:40)
0
σ − µ σ > µ

0 ≤ σ ≤ µ

y∗ =




2µ
(θ−1)σ−θµ
θ−2



θµ

0 ≤ σ ≤ 2µ
2µ < σ < θµ
σ ≥ θµ

(3). For h3(y), the optimal is

(cid:40)

y∗ =

θµ 0 ≤ σ ≤ θµ
σ > θµ
σ

To get γ, we need to compare h1(0), h2(µ) and h3(θµ),
it is easy to verify h1(0) is smallest, thus γ = µ. Finally,
using Proposition II.2, the optimal solution is shown in
Lemma A.6.

Lemma A.6. When r(·) is the SCAD, the optimal solution
of the corresponding proximal operator is proxµ(r(·))Z =
1, . . . , y∗
U Diag(y∗


n)V (cid:62) where




0 ≤ σi ≤ µ

0
σi − µ µ < σi ≤ 2µ
ˆyi
θµ
σi

2µ < σi < θµ and h2(ˆyi) ≤ h3(θµ)
2µ < σi < θµ and h2(ˆyi) > h3(θµ)
σi ≥ θµ

depends on σi and ˆyi = (θ−1)σi−θµ

.

θ−2

Fast Low-Rank Matrix Learning with Nonconvex Regularization

James T. Kwok Wenliang Zhong

Quanming Yao
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Hong Kong
{qyaoaa, jamesk, wzhong}@cse.ust.hk

5
1
0
2
 
c
e
D
 
3
 
 
]

A
N
.
s
c
[
 
 
1
v
4
8
9
0
0
.
2
1
5
1
:
v
i
X
r
a

Abstract—Low-rank modeling has a lot of important ap-
plications in machine learning, computer vision and social
network analysis. While the matrix rank is often approximated
by the convex nuclear norm, the use of nonconvex low-rank
regularizers has demonstrated better recovery performance.
However, the resultant optimization problem is much more
challenging. A very recent state-of-the-art is based on the
proximal gradient algorithm. However, it requires an expensive
full SVD in each proximal step. In this paper, we show that
for many commonly-used nonconvex low-rank regularizers, a
cutoff can be derived to automatically threshold the singular
values obtained from the proximal operator. This allows the use
of power method to approximate the SVD efﬁciently. Besides,
the proximal operator can be reduced to that of a much smaller
matrix projected onto this leading subspace. Convergence,
with a rate of O(1/T ) where T is the number of iterations,
can be guaranteed. Extensive experiments are performed on
matrix completion and robust principal component analysis.
The proposed method achieves signiﬁcant speedup over the
state-of-the-art. Moreover, the matrix solution obtained is more
accurate and has a lower rank than that of the traditional
nuclear norm regularizer. 1

Keywords-Low-rank matrix, Nonconvex optimization, Prox-

imal gradient, Matrix completion, Robust PCA

I. INTRODUCTION

The learning of low-rank matrices is a central

issue
in many machine learning problems. For example, matrix
completion [2], which is one of the most successful ap-
proaches in collaborative ﬁltering, assumes that the target
ratings matrix is low-rank. Besides collaborative ﬁltering,
matrix completion has also been used on tasks such as
sensor networks [3], social network analysis [4], and image
processing [5, 6].

Another important use of low-rank matrix learning is
robust principal component analysis (RPCA) [7], which
assumes the target matrix is low-rank and also corrupted
by sparse data noise. It is now popularly used in various
computer vision applications, such as shadow removal of
aligned faces and background modeling of surveillance
videos [7, 8]. Besides, low-rank minimization has also been
used in tasks such as multilabel learning [9] and multitask
learning [10].

1This is the long version of conference paper appeared in ICDM 2015

[1]; Code is available at: https://github.com/quanmingyao/FaNCL.

However, rank minimization is NP-hard. To alleviate this
problem, a common approach is to use instead a convex
surrogate such as the nuclear norm (which is the sum of
singular values of the matrix). It is known that the nuclear
norm is the tightest convex lower bound of the rank. Besides,
there are theoretical guarantees that the incomplete matrix
can be recovered with nuclear norm regularization [2, 7].
Moreover, though the nuclear norm is non-smooth, the re-
sultant optimization problem can often be solved efﬁciently
using modern tools such as accelerated proximal gradient
descent [11], Soft-Impute [12], and active subspace selection
methods [13].

Despite its success, recently there have been numerous
attempts that use nonconvex surrogates to better approx-
imate the rank function. The key idea is that the larger,
and thus more informative, singular values should be less
penalized. Example nonconvex low-rank regularizers in-
clude the capped-(cid:96)1 penalty [14], log-sum penalty (LSP)
[15], truncated nuclear norm (TNN) [16], smoothly clipped
absolute deviation (SCAD) [17], and minimax concave
penalty (MCP) [18]. Empirically, these nonconvex regular-
izers achieve better recovery performance than the convex
nuclear norm regularizer.

However, the resultant nonconvex optimization problem
is much more challenging. One approach is to use the
concave-convex procedure [19], which decomposes the non-
convex regularizer into a difference of convex functions
[14, 16]. However, a sequence of relaxed problems have
to be solved, and can be computationally expensive [20]. A
more efﬁcient method is the recently proposed iteratively re-
weighted nuclear norm (IRNN) algorithm [21]. It is based
on the observation that existing nonconvex regularizers are
all concave and their super-gradients are non-increasing.
Though IRNN still has to iterate, each of its iterations only
involves computing the super-gradient of the regularizer and
a singular value decomposition (SVD). However, performing
SVD on a m × n matrix (where m ≥ n) still takes O(mn2)
time, and can be expensive when the matrix is large.

Recently, the proximal gradient algorithm has also been
used on this nonconvex low-rank minimization problem
[8, 16, 21, 22]. However, it requires performing the full SVD
in each proximal step, which is expensive for large-scale
applications. To alleviate this problem, we ﬁrst observe that

for the commonly-used nonconvex low-rank regularizers, the
singular values obtained from the corresponding proximal
operator can be automatically thresholded. One then only
needs to ﬁnd the leading singular values/vectors in order to
generate the next iterate. By using the power method [23],
a fast and accurate approximation of such a subspace can
be obtained. Moreover, instead of computing the proximal
operator on a large matrix, one only needs to compute
that on its projection onto this leading subspace. The size
of the matrix is signiﬁcantly reduced and the proximal
operator can be made much more efﬁcient. In the context of
matrix completion problems, further speedup is possible by
exploiting a special “sparse plus low-rank” structure of the
matrix iterate.

The rest of the paper is organized as follows. Section II re-
views the related work. The proposed algorithm is presented
in Section III; Experimental results on matrix completion
and RPCA are shown in Section IV, and the last section
gives some concluding remarks.

In the sequel, the transpose of vector/matrix is denoted by
the superscript (·)(cid:62). For a m×n matrix X, tr(X) is its trace,
(cid:107)X(cid:107)F = tr(X (cid:62)X) is the Frobenius norm, and (cid:107)X(cid:107)∗ =
(cid:80)m
i=1 σi is the nuclear norm. Given x = [xi] ∈ Rm, Diag(x)
constructs a m × m diagonal matrix whose ith diagonal
element is xi. Moreover, I denotes the identity matrix. For
a differentiable function f , we use ∇f for its gradient. For
a nonsmooth function, we use ∂f for its subdifferential.

II. BACKGROUND

A. Proximal Gradient Algorithms

In this paper, we consider composite optimization prob-

lems of the form

min
x

F (x) ≡ f (x) + λr(x),

(1)

where f is smooth and r is nonsmooth. In many machine
learning problems, f is the loss and r a low-rank regularizer.
In particular, we make the following assumptions on f .
A1. f , not necessarily convex,

is differentiable with
ρ-Lipschitz continuous gradient,
i.e., (cid:107)∇f (X1) −
∇f (X2)(cid:107)F ≤ ρ(cid:107)X1 − X2(cid:107)F . Without loss of general-
ity, we assume that ρ ≤ 1.

A2. f is bounded below, i.e., inf f (X) > −∞.
In recent years, proximal gradient algorithms [24] have been
widely used for solving (1). At each iteration t, a quadratic
function is used to upper-bound the smooth f at the current
iterate xt, while leaving the nonsmooth r intact. For a given
stepsize τ , the next iterate xt+1 is obtained as

arg min

∇f (xt)(cid:62)(x − xt) +

(cid:107)x − xt(cid:107)2 + λr(x)

x

= arg min

(cid:107)x − zt(cid:107)2 +

1
2
where zt = xt − 1
τ ∇f (xt), and prox λ
τ r(·) is the proximal
operator [24]. Proximal gradient algorithms can be further

r(x) ≡ prox λ

τ r(zt),

x

τ
2
λ
τ

accelerated, by replacing zt with a proper linear combination
of xt and xt−1. In the sequel, as our focus is on learning
low-rank matrices, x in (1) becomes a m × n matrix X.2

B. Convex and Nonconvex Low-Rank Regularizers

An important factor for the success of proximal gradient
algorithms is that its proximal operator proxµr(·) can be
efﬁciently computed. For example, for the nuclear norm
(cid:107)X(cid:107)∗, the following Proposition shows that its proximal
operator has a closed-form solution.

(X) = U (Σ − µI)+ V (cid:62),
Proposition II.1. [25] proxµ(cid:107)·(cid:107)∗
where U ΣV (cid:62) is the SVD of X, and (Z)+ = [max(Zij, 0)].

While the convex nuclear norm makes the low-rank opti-
mization problem easier, it may not be a good approximation
of the matrix rank [8, 16, 21, 22]. As mentioned in Section I,
a number of nonconvex surrogates for the rank have been
recently proposed. In this paper, we make the following
assumption on the low-rank regularizer r in (1).
A3. r is possibly non-smooth and nonconvex, and of the
form r(X) = (cid:80)m
i=1 ˆr(σi), where σ1 ≥ · · · ≥ σm ≥ 0
are singular values of X, and ˆr(σ) is a concave and
non-decreasing function of σ ≥ 0 with ˆr(0) = 0.
All nonconvex low-rank regularizers introduced in Section I
satisfy this assumption. Their corresponding ˆr’s are shown
in Table I.

Table I
ˆr’S FOR SOME POPULAR NONCONVEX LOW-RANK REGULARIZERS. FOR
THE TNN REGULARIZER, θ ∈ {1, . . . , n} IS THE NUMBER OF LEADING
SINGULAR VALUES THAT ARE NOT PENALIZED.

capped-(cid:96)1
LSP

TNN

SCAD

MCP

µˆr(σi)
µ min(σi, θ), θ > 0
µ log (cid:0) σi
θ + 1(cid:1) , θ > 0
(cid:40)
i > θ
µσi
i ≤ θ
0
σi ≤ µ

i +2θµσi−µ2

2(θ−1)

µσi
−σ2






(θ+1)µ2
2
(cid:40)

µσi − σ2
θµ2
2

i
2θ

µ < σi ≤ θµ

, θ > 2

σi > θµ

σi ≤ θµ
σi > θµ

, θ > 0

The Iteratively Reweighted Nuclear Norm (IRNN) algo-
rithm [21] is a state-of-the-art solver for nonconvex low-
rank minimization. It is based on upper-bounding the non-
convex r, and approximates the matrix rank by a weighted
version of the nuclear norm (cid:107)X(cid:107)w = (cid:80)m
i=1 wiσi, where
0 ≤ w1 ≤ · · · ≤ wm, Intuitively, (cid:107)X(cid:107)w imposes a smaller
penalty on the larger (and more informative) singular values.
Other solvers that are designed only for speciﬁc nonconvex
low-rank regularizers include [8] (for the capped-(cid:96)1), [16]
(for the TNN), and [26] (for the MCP). All these (including

2In the following, we assume m ≤ n.

IRNN) need SVD in each iteration. It takes O(m2n) time,
and thus can be slow.

While proximal gradient algorithms have mostly been
used on convex problems, recently they are also applied to
nonconvex ones [8, 16, 21, 22]. In particular, in the very re-
cent generalized proximal gradient (GPG) algorithm [22], it
is shown that for any nonconvex r satisfying assumption A3,
its proximal operator can be computed by the following
generalized singular value thresholding (GSVT) operator.

Proposition II.2. [22] proxµr(X) = U Diag(y∗)V (cid:62), where
U ΣV (cid:62) is the SVD of X, and y∗ = [y∗

i ] with

y∗
i ∈ arg min
yi≥0

1
2

(yi − σi)2 + µˆr(yi).

(2)

In GPG, problem (2) is solved by a ﬁxed-point iteration
algorithm. Indeed, closed-form solutions exist for the regu-
larizers in Table I [20]. While the obtained proximal operator
can then be immediately plugged into a proximal gradient
algorithm, Proposition II.2 still involves SVD.

III. PROPOSED ALGORITHM

In this section, we show that the GSVT operator proxµr(·)
can be computed more efﬁciently. It is based on two ideas.
the singular values in proxµr(·) are automatically
First,
thresholded. Second, proxµr(·) can be obtained from the
proximal operator on a smaller matrix.

A. Automatic Thresholding of Singular Values
The following Proposition shows that y∗
i

in (2) becomes
zero when σi is smaller than a regularizer-speciﬁc threshold.
Because of the lack of space, proofs will be reported in a
longer version of this paper.

Proposition III.1. For any ˆr satisfying Assumption A3, there
exists a threshold γ > 0 such that y∗

i = 0 when σi ≤ γ.

By examining the optimality conditions of (2), simple
closed-form solutions can be obtained for the nonconvex
regularizers in Table I.

Corollary III.2. For the nonconvex regularizers in Table I,
their γ values are equal to

(cid:1);

• capped-(cid:96)1: γ = min (cid:0)µ, θ + µ
• LSP: γ = min (cid:0) µ
• TNN: γ = max (µ, σθ+1);
• SCAD: γ = µ;
√
• MCP: γ =

θ , θ(cid:1);

2

θµ if 0 < θ < 1, and µ otherwise.

Proposition III.1 suggests that in each proximal iteration t,
we only need to compute the leading singular values/vectors
of the matrix iterate Z t. The power method (Algorithm 1)
[23] is a fast and accurate algorithm for obtaining an ap-
proximation of such a subspace. Besides the power method,
algorithms such as PROPACK [27] have also been used
[28]. However, the power method is more efﬁcient than

Algorithm 1 Power method to obtain an approximate left
subspace of Z.
Require: matrix Z ∈ Rm×n, R ∈ Rn×k.
1: Y 1 ← ZR;
2: for t = 1, 2, . . . , Tpm do
3: Qt+1 = QR(Y t);
4:
5: end for
6: return QTpm+1.

Y t+1 = Z(Z (cid:62)Qt+1);

// QR decomposition

PROPACK [23]. It also allows warm-start, which is partic-
ularly useful because of the iterative nature of the proximal
gradient algorithm.

B. Proximal Operator on a Smaller Matrix

Assume that Z t has ˆk ≤ n singular values larger than γ,
and its rank-ˆk SVD is UˆkΣˆkV (cid:62)
. The following Proposition
ˆk
shows that proxµr(Z t) can be obtained from the proximal
operator on a smaller matrix.
Proposition III.3. Assume that Q ∈ Rm×k, where k ≥ ˆk, is
orthogonal and span(Uˆk) ⊆ span(Q). Then, proxµr(Z t) =
Q · proxµr(Q(cid:62)Z t).

Though SVD is still needed to obtain proxµr(Q(cid:62)Z t),
Q(cid:62)Z t is much smaller than Z t (k×n vs m×n). This smaller
SVD takes O(nk2) time, and the other matrix multiplication
steps take O(mnk) time. Thus, the time complexity for this
SVD step is reduced from O(m2n) to O((m + k)nk).

C. Complete Procedure

The complete procedure (Algorithm 2) will be called
FaNCL (Fast NonConvex Lowrank). The core steps are 9–
16. We ﬁrst use the power method to efﬁciently obtain an
approximate Q, whose singular values are then thresholded
according to Corollary III.2. With k ≥ ˆk, the rank of ˜X p
will be equal to that of proxµr(Z t). In each iteration, we
ensure a sufﬁcient decrease of the objective:

F (X t+1) ≤ F (X t) − c1(cid:107)X t+1 − X t(cid:107)2
F ,

(3)

where c1 = τ −ρ
4 ; otherwise, the power method is restarted.
Moreover, similar to [13, 28], steps 6-7 use the column
spaces of the previous iterates (V t and V t−1) to warm-
start the power method. For further speedup, we employ
a continuation strategy as in [12, 21, 28]. Speciﬁcally, λt is
initialized to a large value and then decreases gradually.

Algorithm 2 can also be used with the nuclear norm. It
can be shown that the threshold γ is equal to λ/τ , and y∗
i
in step 15 has the closed-form solution max(σi − λt/τ, 0).
However, since our focus is on nonconvex regularizers, using
Algorithm 2 for nuclear norm minimization will not be
further pursued in the sequel.

The power method has also been recently used to approx-
imate the SVT in nuclear norm minimization [13]. However,

Algorithm 2 FaNCL (Fast NonConvex Low-rank).
1: choose τ > ρ, c1 = τ −ρ
2: initialize V0, V1 ∈ Rn×k as random Gaussian matrices,

4 , λ0 > λ and ν ∈ (0, 1);

and X 1 = 0;

3: for t = 1, 2, . . . T do
4:
5:

λt ← (λt−1 − λ)ν + λ;
Z t ← X t − 1
τ ∇f (X t);
V t−1 ← V t−1 − V t(V t(cid:62)V t−1), and
remove any zero columns;

6:

7: R1 ← QR([V t, V t−1]);
for p = 1, 2, . . . do
8:
9:
10:

A, Σp

A, V p

A] ← SVD(Q(cid:62)Z t);

Q ← PowerMethod(Z t, Rp);
[U p
ˆk ← number of σA’s are > γ in Corollary III.2;
˜U p ← ˆk leading columns of U p
A;
˜V p ← ˆk leading columns of V p
A;
for i = 1, 2, . . . , ˆk do

obtain y∗

i from (2) with µ = 1/τ and λt;

end for
˜X p ← (Q ˜U p)Diag(y∗
)( ˜V p)(cid:62);
if F ( ˜X p) ≤ F (X t) − c1(cid:107) ˜X p − X t(cid:107)2

1, . . . , y∗
ˆk

F then

X t+1 ← ˜X p, V t+1 ← ˜V p;
break;

11:
12:

13:

14:
15:
16:
17:

18:
19:
20:

Rp+1 = V p
A;

else

21:
22:
23:
24:
25: end for
26: return X T +1.

end if
end for

[13] is based on active subspace selection (which uses SVT
to update the active row and column subspaces of the current
solution), and is thus very different from the proposed
algorithm (which is a proximal gradient algorithm). In Sec-
tion IV, it will be shown that the proposed method has better
empirical performance. Moreover, [13] is only designed for
nuclear norm minimization, and cannot be extended for the
nonconvex regularizers considered here.

A breakdown of the time complexity of Algorithm 2 is as
follows. For simplicity, assume that X t’s always have rank
k. Step 5 takes O(mn) time; step 6 and 7 take O(nk2) time;
step 9 and 10 take O(mnkTpm) time; step 17 takes O(mnk)
time; and step 18 takes O(mn) time. Thus, the per-iteration
time complexity is O(mnkpTpm). In the experiment, we set
Tpm = 3 and p = 1. Empirically, this setting is enough to
guarantee (3). In contrast, SVDs in GPG and IRNN take
O(m2n) time, and are thus much slower as k (cid:28) m.

D. Convergence Analysis

The following Proposition shows that {X t} from Algo-

rithm 2 converges to a limit point X∞ = limt→∞ X t.
Proposition III.4. (cid:80)∞

t=1 (cid:107)X t+1 − X t(cid:107)2

F < ∞.

The following shows that it is also a critical point.3

Theorem III.5. {X t} converges to a critical point X ∗ of
problem (1) in a ﬁnite number of iterations.

that (cid:107)X t+1 − X t(cid:107)2

By combining with Proposition III.4, the following shows
F converges to zero at a rate of O(1/T ).
≤

III.6. mint=1,...,T (cid:107)X t+1 − X t(cid:107)2
F

Corollary

(cid:2)F (X 1) − F (X ∗)(cid:3).

1
c1T
E. Further Speedup for Matrix Completion

In matrix completion, one attempts to recover a low-rank
matrix O ∈ Rm×n by observing only some of its elements.
Let the observed positions be indicated by Ω ∈ {0, 1}m×n,
such that Ωij = 1 if Oij is observed, and 0 otherwise. It
can be formulated as an optimization problem in (1), with
f (X) = 1
F , where [PΩ(A)]ij = Aij if Ωij =
1 and 0 otherwise, and r is a low-rank regularizer.

2 (cid:107)PΩ(X −O)(cid:107)2

It can be easily seen that step 5 in Algorithm 2 becomes
Z t = X t − 1
τ PΩ(X t −O). By observing that X t is low-rank
and 1
τ PΩ(X t − O) is sparse, Mazumder et al. [12] showed
that
this “sparse plus low-rank” structure allows matrix
multiplications of the form ZA and Z (cid:62)B to be efﬁciently
computed. Here, this trick can also be directly used to speed
up the computation of Z(Z (cid:62)Qt+1) in Algorithm 1. Since
(cid:107)Ω(cid:107)1 is very sparse, this step takes O(kTpm(cid:107)Ω(cid:107)1) time
instead of O(mnkTpm), thus is much faster.

The following Proposition shows that (cid:107) ˜X p − X t(cid:107)2

F in

step 18 of Algorithm 2 can also be easily computed.

Proposition III.7. Let the reduced SVD of X be U ΣV (cid:62),
and P, Q be orthogonal matrices such that span(U ) ⊆
span(P ) and span(V ) ⊆ span(Q). Then (cid:107)X(cid:107)F =
(cid:107)P (cid:62)XQ(cid:107)F .

Let

the reduced SVDs of

˜X p and X t be ˜U ˜Σ ˜V (cid:62)
and U tΣtV t(cid:62), respectively. Let P = QR([ ˜U , U t]) and
Q = QR([ ˜V , V t]). Using Proposition III.7, (cid:107) ˜X p −
X t(cid:107)F = (cid:107)P (cid:62)( ˜X p − X t)Q(cid:107)F = (cid:107)(P (cid:62) ˜U ) ˜Σ( ˜V (cid:62)Q) −
(P (cid:62)U t)Σt(V t(cid:62)Q)(cid:107)F . This takes O(nk2) instead of O(mn)
time. The per-iteration time complexity is reduced from
O(mnkTpm) to O((nk + Tpm|Ω|1)k) and is much faster.
Table II compares the per-iteration time complexities and
convergence rates for the various low-rank matrix comple-
tion solvers used in the experiments (Section IV-A).

F. Handling Multiple Matrix Variables

The proposed algorithm can be extended for optimization

problems involving N matrices X1, . . . , XN :

min F (X1, . . . , XN ) ≡ f (X1, . . . , XN )+

λiri(Xi).

(4)

N
(cid:88)

i=1

3Since r is nonconvex and its subdifferential for points in its domain
may be empty, we deﬁne X ∗ as a critical point by extending the deﬁnition
in [20], namely that 0 ∈ ∇f (X ∗) + λ∂r1(X ∗) − λ∂r2(X ∗), where
r(X) = r1(X) − r2(X), and r1 and r2 are convex.

Table II
COMPARISON OF THE PER-ITERATION TIME COMPLEXITIES AND
CONVERGENCE RATES OF VARIOUS MATRIX COMPLETION SOLVERS.
HERE, ν ∈ (0, 1) IS A CONSTANT.

regularizer
(convex)
nuclear
norm
ﬁxed-rank
factorization
nonconvex

method
APG [11, 28]
Soft-Impute [12]
active ALT [13]
LMaFit [29]
R1MP [30]
IRNN [21]
GPG [22]
FaNCL

complexity
O(mnk)
O(k(cid:107)Ω(cid:107)1)
O(kTin(cid:107)Ω(cid:107)1)
O(k(cid:107)Ω(cid:107)1)
O((cid:107)Ω(cid:107)1)
O(m2n)
O(m2n)
O(k(cid:107)Ω(cid:107)1)

rate
O(1/T 2)
O(1/T )
O(νT )
—
O(νT )
—
—
O(1/T )

Assumptions A1-A3 are analogously extended. In particular,
A1 now assumes that (cid:107)∇fi(X) − ∇fi(Y )(cid:107)F ≤ ρi(cid:107)X −
Y (cid:107)F for some ρi, where fi(X) is the function obtained by
keeping all the Xj’s (where i (cid:54)= j) in f ﬁxed.

Many machine learning problems can be cast into this
form. One example that will be considered in Section IV
is robust principal component analysis (RPCA) [7]. Given
a noisy data matrix O, RPCA assumes that O can be
approximated by the sum of a low-rank matrix X plus sparse
data noise Y . Mathematically, we have

min
X,Y

F (X, Y ) ≡ f (X, Y ) + λr(X) + β(cid:107)Y (cid:107)1,

(5)

where f (X, Y ) = 1
2 (cid:107)X + Y − O(cid:107)2
F , r is a low-rank
regularizer on X, and (cid:107)Y (cid:107)1 encourages Y to be sparse.
Since both r and the (cid:96)1 regularizer (cid:107) · (cid:107)1 are nonsmooth, (5)
does not ﬁt into formulation (1). Besides RPCA, problems
such as subspace clustering [31], multilabel learning [9] and
multitask learning [10] can also be cast as (4).

For simplicity, we focus on the case with two parameter
blocks. Extension to multiple blocks is straightforward. To
solve the two-block problem in (5), we perform alternating
proximal steps on X and Y at each iteration t:

X t+1 = arg minX
Y t+1 = arg minY

1

2 (cid:107)X − Z t
2 (cid:107)Y − Z t

X (cid:107)2
Y (cid:107)2

F + λ
F + β

τ r(X) = prox λ
τ (cid:107)Y (cid:107)1 = prox β

1

τ r(Z t

X ),
(Z t

Y ),

X = X t − 1

τ ∇f (X t, Y t), and Z t

τ (cid:107)·(cid:107)1
where Z t
Y = Y t −
τ ∇f (X t+1, Y t). Y t+1 can be easily obtained as Y t+1
1
ij =
(cid:17)
sign ([Z t
, where sign(x) denotes the
+
sign of x. Similar to (3), we ensure a sufﬁcient decrease of
the objective in each iteration:

Y ]ij| − β

Y ]ij)

|[Z t

(cid:16)

τ

FY t(X t+1) ≤ FY t(X t) − c1(cid:107)X t+1 − X t(cid:107)2
F ,
FX t+1(Y t+1) ≤ FX t+1(Y t) − c1(cid:107)Y t+1 − Y t(cid:107)2
F ,

here F is nonconvex. We extend the convergence results
in Section III-D to the following.

and

blocks

Theorem III.8. With N parameter
{(X t

1, . . . , X t

F < ∞;

i=1 (cid:107)X t+1

1) (cid:80)∞
t=1
2) {(X t
(X ∗

N )} generated by the algorithm, we have
(cid:80)N
1, . . . , X t
1 , . . . , X ∗
3) mint=1,...,T
c1T [F (X 1

i (cid:107)2
i − X t
N )} converges
to a critical point
N ) of (4) in a ﬁnite number of iterations;
(cid:80)N
≤
1 , . . . , X 1

X t
−
1 , . . . , X ∗
N ) − F (X ∗
IV. EXPERIMENTS

i=1 (cid:107)X t+1

i (cid:107)2
F
N )].

1

i

A. Matrix Completion

We compare a number of low-rank matrix completion
solvers, including models based on (i) the commonly used
(convex) nuclear norm regularizer; (ii) ﬁxed-rank factoriza-
tion models [29, 30], which decompose the observed matrix
O into a product of rank-k matrices U and V . Its opti-
1
mization problem can be written as: minU,V
2 (cid:107)PΩ(U V −
O)(cid:107)2
F ); and (iii) nonconvex regularizers,
including the capped-(cid:96)1 (with θ in Table I set to 2λ), LSP
(with θ =

λ), and TNN (with θ = 3).

2 ((cid:107)U (cid:107)2
√

F + (cid:107)V (cid:107)2

F + λ

The nuclear norm minimization algorithms to be com-

pared include:

1) Accelerated proximal gradient (APG)4 algorithm [11,

28], with the partial SVD by PROPACK [27];

2) Soft-Impute5 [12], which iteratively replaces the miss-
ing elements with those obtained from SVT. The
“sparse plus low-rank” structure of the matrix iterate
is utilized to speed up computation (Section III-E);

3) Active alternating minimization6

(denoted “active
ALT”) [13], which adds/removes rank-one subspaces
from the active set in each iteration. The nuclear norm
optimization problem is then reduced to a smaller
problem deﬁned only on this active set.

We do not compare with the Frank-Wolfe algorithm [33]
and stochastic gradient descent [34], as they have been
shown to be less efﬁcient [13]. For the ﬁxed-rank factor-
ization models (where the rank is tuned by the validation
set), we compare with the two state-of-the-art algorithms:

1) Low-rank matrix ﬁtting (LMaFit) algorithm7 [29]; and
2) Rank-one matrix pursuit (R1MP) [30], which pursues

a rank-one basis in each iteration.

We do not compare with the concave-convex procedure [14,
16], since it has been shown to be inferior to IRNN [20].

For models with nonconvex low-rank regularizers, we

compare the following solvers:

1) Iterative reweighted nuclear norm (IRNN)8 [21];

where FY (X) = f (X, Y ) + λr(X), and FX (Y ) =
f (X, Y ) + β(cid:107)Y (cid:107)1. The resultant algorithm is similar to
Algorithm 2.

When F is convex, convergence of this alternating min-
imization scheme has been well studied [32]. However,

4http://perception.csl.illinois.edu/matrix-rank/Files/apg partial.zip
5http://cran.r-project.org/web/packages/softImpute/index.html
6http://www.cs.utexas.edu/∼cjhsieh/nuclear active 1.1.zip
7http://www.caam.rice.edu/∼optimization/L1/LMaFit/download.html
8https://sites.google.com/site/canyilu/ﬁle/2014-CVPR-IRNN.zip?

attredirects=0&d=1

Table III
MATRIX COMPLETION PERFORMANCE ON THE SYNTHETIC DATA. HERE, NMSE IS SCALED BY ×10−2, AND CPU TIME IS IN SECONDS.

nuclear
norm

ﬁxed
rank
capped
(cid:96)1

LSP

TNN

APG
Soft-Impute
active ALT
LMaFit
R1MP
IRNN
GPG
FaNCL
IRNN
GPG
FaNCL
IRNN
GPG
FaNCL

m = 500
(observed: 12.43%)

m = 1000
(observed: 6.91%)

m = 1500
(observed: 4.88%)

m = 2000
(observed: 3.80%)

NMSE
3.95±0.16
3.95±0.16
3.95±0.16
2.63±0.10
22.72±0.63
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07
1.98±0.07

rank
49
49
49
5
39
5
5
5
5
5
5
5
5
5

time
4.8
64.9
17.1
0.6
0.3
8.5
8.5
0.3
21.8
21.2
0.5
8.5
8.3
0.3

NMSE
3.90±0.05
3.90±0.05
3.90±0.05
2.85±0.10
20.89±0.66
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04
1.89±0.04

rank
59
59
59
5
54
5
5
5
5
5
5
5
5
5

time
59.5
176.0
81.9
1.7
0.8
75.5
72.4
0.9
223.9
235.3
2.2
72.6
71.7
0.8

NMSE
3.74±0.02
3.74±0.02
3.74±0.02
2.54±0.09
20.04±0.66
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02
1.81±0.02

rank
71
71
71
5
62
5
5
5
5
5
5
5
5
5

time
469.3
464.4
343.8
4.5
1.4
510.8
497.0
2.6
720.9
687.4
3.3
650.7
655.3
2.7

NMSE
3.69±0.04
3.69±0.04
3.69±0.04
2.40±0.09
19.53±0.61
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02
1.80±0.02

rank
85
85
85
5
63
5
5
5
5
5
5
5
5
5

time
1383.3
1090.2
860.1
7.1
3.4
1112.3
1105.8
4.1
2635.0
2612.0
7.6
1104.1
1098.2
4.2

2) Generalized proximal gradient (GPG) algorithm [22],
with the underlying problem (2) solved more efﬁ-
ciently using the closed-form solutions in [20];
3) The proposed FaNCL algorithm (Tpm = 3, p = 1).
All algorithms are implemented in Matlab. The same
stopping criterion is used, namely that the algorithm stops
when the difference in objective values between consecutive
iterations is smaller than a given threshold. Experiments are
run on a PC with i7 4GHz CPU and 24GB memory.

1) Synthetic Data: The observed m × m matrix is gen-
erated as O = U V + G, where the elements of U ∈
Rm×k, V ∈ Rk×m (with k = 5) are sampled i.i.d. from
the normal distribution N (0, 1), and elements of G sampled
from N (0, 0.1). A total of (cid:107)Ω(cid:107)1 = 2mk log(m) random ele-
ments in O are observed. Half of them are used for training,
and the rest as validation set for parameter tuning. Testing
is performed on the non-observed (missing) elements.
(i)

performance
mean
(i,j)(cid:54)∈Ω(Xij − [U V ]ij)2/

use
evaluation, we
NMSE
error
squared
(i,j)(cid:54)∈Ω[U V ]2
ij,

the
=
where
X is the recovered matrix; (ii) rank of X; and (iii) training
CPU time. We vary m in the range {500, 1000, 1500, 2000}.
Each experiment is repeated ﬁve times.

normalized
(cid:113)(cid:80)

(cid:113)(cid:80)

For

Results are shown in Table III. As can be seen, the non-
convex regularizers (capped-(cid:96)1, LSP and TNN) lead to much
lower NMSE’s than the convex nuclear norm regularizer and
ﬁxed-rank factorization. Moreover, as is also observed in
[34], the nuclear norm needs to use a much higher rank than
the nonconvex ones. In terms of speed, FaNCL is the fastest
among the nonconvex low-rank solvers. Figure 1 shows its
speedup over GPG (which in turn is faster than IRNN). As
can be seen, the larger the matrix, the higher is the speedup.
Recall that the efﬁciency of the proposed algorithm comes
from (i) automatic singular value thresholding; (ii) comput-
ing the proximal operator on a smaller matrix; and (iii)
exploiting the “sparse plus low-rank” structure in matrix
completion. Their individual contributions are examined in
Table IV. The baseline is GPG, which uses none of these;

Figure 1. Speedup of FaNCL over GPG at different matrix sizes.

while the proposed FaNCL uses all. As all the variants
produce the same solution, the obtained NMSE and rank
values are not shown. As can be seen, tricks (i), (ii) and (iii)
lead to average speedups of about 6, 4, and 3, respectively;
and are particularly useful on the large data sets.

Table IV
EFFECTS OF THE THREE TRICKS ON CPU TIME (IN SECONDS) USING
THE SYNTHETIC DATA. (I) AUTOMATIC SINGULAR VALUE
THRESHOLDING; (II) COMPUTING THE PROXIMAL OPERATOR ON A
SMALLER MATRIX; AND (III) “SPARSE PLUS LOW-RANK” STRUCTURE.

capped
(cid:96)1

LSP

TNN

solver
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)

500
8.5
5.4
0.6
0.3
21.2
4.9
1.0
0.5
8.3
5.4
0.6
0.3

1000
72.4
37.6
3.2
0.9
235.3
44.0
9.7
2.2
71.7
32.5
2.8
0.8

1500
497.0
114.8
11.4
2.6
687.4
70.0
14.8
3.3
655.3
122.3
10.3
2.7

2000
1105.8
203.7
25.6
6.8
2612.0
154.9
31.1
8.2
1098.2
194.1
15.8
3.3

2) MovieLens: Experiment is performed on the popular
MovieLens9 data set (Table V), which contain ratings of
different users on movies. We follow the setup in [30],

9http://grouplens.org/datasets/movielens/

(a) capped-(cid:96)1.

(b) LSP.

(c) TNN.

Figure 2. Objective value vs CPU time for the various nonconvex low-rank regularizers on the MovieLens-100K data set.

and use 50% of the observed ratings for training, 25%
for validation and the rest for testing. For performance
evaluation, we use the root mean squared error on the test
set Ω: RMSE = (cid:112)(cid:107)PΩ(X − O)(cid:107)2
F /(cid:107)Ω(cid:107)1, where X is the
recovered matrix. The experiment is repeated ﬁve times.

Table V
RECOMMENDATION DATA SETS USED IN THE EXPERIMENTS.

MovieLens

100K
1M
10M

netﬂix
yahoo

#users
943
6,040
69,878
480,189
249,012

#movies
1,682
3,449
10,677
17,770
296,111

#ratings
100,000
999,714
10,000,054
100,480,507
62,551,438

Results are shown in Table VI. Again, nonconvex reg-
ularizers lead to the lowest RMSE’s. Moreover, FaNCL
is also the fastest among the nonconvex low-rank solvers,
even faster than the state-of-the-art. In particular, it is the
only solver (among those compared) that can be run on the
MovieLens-10M data. Table VII examines the usefulness of
the three tricks. The behavior is similar to that as observed in
Table IV. Figures 2 and 3 compare the objective and RMSE
vs CPU time for the various methods on the MovieLens-
100K data set. As can be seen, FaNCL decreases the
objective and RMSE much faster than the others.

Table VII
EFFECTS OF THE THREE TRICKS ON CPU TIME (IN SECONDS) ON THE
MOVIELENS DATA.

capped
(cid:96)1

LSP

TNN

solver
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)
baseline (GPG)
(cid:88) i
(cid:88) i, ii
(cid:88) i, ii, iii (FaNCL)

100K
10M
1M
523.6 > 104 > 105
1920.5 > 105
212.2
> 105
29.2
288.8
634.6
29.4
3.2
192.8 > 104 > 105
2353.8 > 105
35.8
> 105
212.4
5.6
0.7
616.3
25.6
572.7 > 104 > 105
1944.8 > 105
116.9
> 105
256.1
15.4
710.7
25.8
1.9

Figure 3. RMSE vs CPU time on the MovieLens-100K data set.

(a) netﬂix.

(b) yahoo.
Figure 4. RMSE vs CPU time on the netﬂix and yahoo data sets.

3) Netﬂix and Yahoo: Next, we perform experiments on
two very large recommendation data sets, Netﬂix10 and

10http://archive.ics.uci.edu/ml/datasets/Netﬂix+Prize

Table VI
MATRIX COMPLETION RESULTS ON THE MOVIELENS DATA SETS (TIME IS IN SECONDS).

nuclear norm

ﬁxed rank

capped-(cid:96)1

LSP

TNN

APG
Soft-Impute
active ALT
LMaFit
R1MP
IRNN
GPG
FaNCL
IRNN
GPG
FaNCL
IRNN
GPG
FaNCL

MovieLens-100K

MovieLens-1M

MovieLens-10M

RMSE
0.879±0.001
0.879±0.001
0.879±0.001
0.884±0.001
0.924±0.003
0.863±0.003
0.863±0.003
0.863±0.003
0.855±0.002
0.855±0.002
0.855±0.002
0.862±0.003
0.862±0.003
0.862±0.003

rank
36
36
36
2
5
3
3
3
2
2
2
3
3
3

time
18.9
13.8
4.1
3.0
0.1
558.9
523.6
3.2
195.9
192.8
0.7
621.9
572.7
1.9

RMSE
0.818±0.001
0.818±0.001
0.818±0.001
0.818±0.001
0.862±0.004
—
—
0.797±0.001
—
—
0.786±0.001
—
—
0.797±0.004

time
735.8
311.8
133.4
39.2
2.9

rank
67
67
67
6
19
— > 104
— > 104
29.4
5
— > 104
— > 104
25.6
5
— > 104
— > 104
25.8
5

RMSE
—
—
0.813±0.001
0.795±0.001
0.850±0.008
—
—
0.783±0.002
—
—
0.777±0.001
—
—
0.783±0.002

rank
time
— > 105
— > 105
3675.2
119
650.1
9
29
37.3
— > 105
— > 105
634.6
8
— > 105
— > 105
616.3
9
— > 105
— > 105
710.7
8

Yahoo11 (Table V). We randomly use 50% of the observed
ratings for training, 25% for validation and the rest for
testing. Each experiment is repeated ﬁve times.

Results are shown in Table VIII. APG, Soft-Impute,
GPG and IRNN cannot be run as the data set is large.
Figure 4 shows the objective and RMSE vs time for the
compared methods.12 Again,
the nonconvex regularizers
converge faster, yield lower RMSE’s and solutions of much
lower ranks. Moreover, FaNCL is fast.

B. Robust Principal Component Analysis

1) Synthetic Data: In this section, we ﬁrst perform ex-
periments on a synthetic data set. The observed m × m
matrix is generated as O = U V + ˜Y + G, where elements
of U ∈ Rm×k, V ∈ Rk×m (with k = 0.01m) are sampled
i.i.d. from N (0, 1), and elements of G are sampled from
N (0, 0.1). Matrix ˜Y is sparse, with 1% of its elements
randomly set to 5(cid:107)U V (cid:107)∞ or −5(cid:107)U V (cid:107)∞ with equal prob-
abilities. The sparsity regularizer is the standard (cid:96)1, while
different convex/nonconvex low-rank regularizers are used.
For performance evaluation, we use (i) NMSE = (cid:107)(X +
Y ) − (U V + ˜Y )(cid:107)F /(cid:107)U V + ˜Y (cid:107)F , where X and Y are the
recovered low-rank and sparse components, respectively in
(5); (ii) accuracy on locating the sparse support of ˜Y (i.e.,
percentage of entries that both ˜Yij and Yij are nonzero or
zero together); and (iii) the recovered rank. We vary m in
{500, 1000, 1500, 2000}. Each experiment is repeated ﬁve
times.

Note that IRNN and the active subspace selection method
cannot be used here. Their objectives are of the form
“smooth function plus low-rank regularizer”, while RPCA
has a nonsmooth (cid:96)1 regularizer besides its low-rank regular-
izer. Similarly, Soft-Impute is for matrix completion only.

11http://webscope.sandbox.yahoo.com/catalog.php?datatype=c
12On these two data sets, R1MP easily overﬁts as the rank increases.
Hence, the validation set selects a rank which is small (relative to that
obtained by the nuclear norm) and R1MP stops earlier. However, as can be
seen, its RMSE is much worse.

Results are shown in Table IX. The accuracy on locating
the sparse support are always 100% for all methods, and
thus are not shown. As can be seen, while both convex
and nonconvex regularizers can perfectly recover the matrix
rank and sparse locations, the nonconvex regularizers have
lower NMSE’s. Moreover, as in matrix completion, FaNCL
is again much faster. The larger the matrix, the higher is the
speedup.

2) Background Removal on Videos: In this section, we
use RPCA to perform video denoising on background re-
moval of corrupted videos. Four benchmark videos13 in
[7, 8] are used (Table X), and example image frames are
shown in Figure 5. As discussed in [7], the stable image
background can be treated as low-rank, while the foreground
moving objects contribute to the sparse component.

Table X
VIDEOS USED IN THE EXPERIMENT.

#pixels / frame
total #frames

bootstrap
19,200
9,165

campus
20,480
4,317

escalator
20,800
10,251

hall
25,344
10,752

(a) bootstrap.

(b) campus.

(c) escalator.

(d) hall.

Figure 5. Example image frames in the videos.

Each image frame is reshaped as a column vector, and
all frames are then stacked together to form a matrix. The
pixel values are normalized to [0, 1], and Gaussian noise
from N (0, 0.15) is added. The experiment is repeated ﬁve
times.

For performance evaluation, we use the commonly used
peak signal-to-noise ratio [35]: PSNR = −10 log10(MSE),
j=1 (Xij − Oij)2, X ∈ Rm×n
where MSE = 1
mn
is the recovered video, and O ∈ Rm×n is the ground-truth.

(cid:80)m

(cid:80)n

i=1

13http://perception.i2r.a-star.edu.sg/bk model/bk index.html

Table VIII
RESULTS ON THE NETFLIX AND YAHOO DATA SETS (CPU TIME IS IN HOURS).

nuclear norm active ALT

ﬁxed rank

capped-(cid:96)1
LSP
TNN

LMaFit
R1MP
FaNCL
FaNCL
FaNCL

netﬂix

yahoo

RMSE
0.814 ± 0.001
0.813 ± 0.003
0.861 ± 0.006
0.799 ± 0.001
0.793 ± 0.002
0.798 ± 0.001

rank
399
16
31
15
13
17

time
47.6
2.4
0.2
2.5
1.9
3.3

RMSE
0.680 ± 0.001
0.667 ± 0.002
0.810 ± 0.005
0.650 ± 0.001
0.650 ± 0.001
0.655 ± 0.002

rank
221
10
92
8
9
8

time
118.9
6.6
0.3
5.9
6.1
6.2

Table IX
RPCA PERFORMANCE OF THE VARIOUS METHODS ON SYNTHETIC DATA. THE STANDARD DEVIATIONS OF NMSE ARE ALL SMALLER THAN 0.0002
AND SO NOT REPORTED. CPU TIME IS IN SECONDS.
m = 1000

m = 500

nuclear norm
capped-(cid:96)1

LSP

TNN

APG
GPG
FaNCL
GPG
FaNCL
GPG
FaNCL

NMSE
0.46
0.36
0.36
0.36
0.36
0.36
0.36

rank
5
5
5
5
5
5
5

time
1.5
0.9
0.2
2.7
0.4
0.8
0.2

NMSE
0.30
0.25
0.25
0.25
0.25
0.25
0.25

rank
10
10
10
10
10
10
10

time
9.7
6.7
1.4
18.5
1.8
6.0
1.2

m = 1500
rank
15
15
15
15
15
15
15

NMSE
0.25
0.21
0.21
0.21
0.21
0.21
0.21

time
33.9
18.7
2.7
111.2
3.9
23.1
2.9

m = 2000
rank
20
20
20
20
20
20
20

NMSE
0.18
0.15
0.15
0.15
0.15
0.15
0.15

time
94.7
60.4
6.5
250.2
7.1
51.4
5.8

(a) original.

(c) capped-(cid:96)1.
Figure 6. Example foreground images in bootstrap, as recovered by using various low-rank regularizers.

(b) nuclear norm.

(d) LSP.

(e) TNN.

Results are shown in Table XI. As can be seen, the non-
convex regularizers lead to better PSNR’s than the convex
nuclear norm. Moreover, FaNCL is more than 10 times faster
than GPG. Figure 6 shows an example of the recovered
foreground in the bootstrap video. As can been seen, the
nonconvex regularizers can better separate foreground from
background. Figure 7 shows the PSNR vs time on bootstrap.
Again, FaNCL converges much faster than others.

be automatically thresholded, and also that the proximal
operator can be computed on a smaller matrix. For matrix
completion, extra speedup can be achieved by exploiting
the “sparse plus low-rank” structure of the matrix estimate
in each iteration. The resultant algorithm is guaranteed to
converge to a critical point of the nonconvex optimization
problem. Extensive experiments on matrix completion and
RPCA show that the proposed algorithm is much faster
than the state-of-art convex and nonconvex low-rank solvers.
It also demonstrates that nonconvex low-rank regularizers
outperform the convex nuclear norm regularizer in terms of
recovery accuracy and the rank obtained.

ACKNOWLEDGMENT

This research was supported in part by the Research
Grants Council of the Hong Kong Special Administrative
Region (Grant 614513).

REFERENCES

[1] Q. Yao, J. T. Kwok, and W. Zhong, “Fast low-rank matrix learning

with nonconvex regularization,” 2015.

[2] E. J. Cand`es and B. Recht, “Exact matrix completion via convex
optimization,” Foundations of Computational Mathematics, vol. 9,
no. 6, pp. 717–772, 2009.

[3] P. Biswas, T.-C. Lian, T.-C. Wang, and Y. Ye, “Semideﬁnite pro-
gramming based algorithms for sensor network localization,” ACM
Transactions on Sensor Networks, vol. 2, no. 2, pp. 188–220, 2006.

Figure 7. PSNR vs CPU time on the bootstrap data set.

V. CONCLUSION

In this paper, we considered the challenging problem of
nonconvex low-rank matrix optimization. The key obser-
vations are that for the popular low-rank regularizers, the
singular values obtained from the proximal operator can

Table XI
PSNR (IN DB) AND CPU TIME (IN SECONDS) ON THE VIDEO BACKGROUND REMOVAL EXPERIMENT. FOR COMPARISON, THE PSNRS FOR ALL THE
INPUT VIDEOS ARE 16.47DB.
campus

bootstrap

escalator

hall

nuclear norm
capped-(cid:96)1

LSP

TNN

APG
GPG
FaNCL
GPG
FaNCL
GPG
FaNCL

PSNR
23.01±0.03
24.00±0.03
24.00±0.03
24.29±0.03
24.29±0.03
24.06±0.03
24.06±0.03

time
688.4
1009.3
60.4
1420.2
56.0
1047.5
86.3

PSNR
22.90±0.02
23.14±0.02
23.14±0.02
23.96±0.02
23.96±0.02
23.11±0.02
23.11±0.02

time
102.6
90.6
12.4
88.9
17.4
130.3
12.6

PSNR
23.56±0.01
24.33±0.02
24.33±0.02
24.13±0.01
24.13±0.01
24.29±0.01
24.29±0.01

time
942.5
1571.2
68.3
1523.1
54.5
1857.7
69.6

PSNR
23.62±0.01
24.95±0.02
24.95±0.02
25.08±0.01
25.08±0.01
24.98±0.02
24.98±0.02

time
437.7
620.0
34.7
683.9
35.8
626.2
37.4

[4] M. Kim and J. Leskovec, “The network completion problem: Inferring
missing nodes and edges in networks,” in Proceedings of the 11th
International Conference on Data Mining, 2011, pp. 47–58.

[5] J. Liu, P. Musialski, P. Wonka, and J. Ye, “Tensor completion for
estimating missing values in visual data,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 208–
220, 2013.

[6] Q. Yao and J. Kwok, “Colorization by patch-based local low-rank

matrix completion,” 2015.

[7] E. J. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal
component analysis?” Journal of the ACM, vol. 58, no. 3, p. 11,
2011.

[8] Q. Sun, S. Xiang, and J. Ye, “Robust principal component analysis via
capped norms,” in Proceedings of the 19th International Conference
on Knowledge Discovery and Data Mining, 2013, pp. 311–319.
[9] G. Zhu, S. Yan, and Y. Ma, “Image tag reﬁnement towards low-
rank, content-tag prior and error sparsity,” in Proceedings of the
International Conference on Multimedia, 2010, pp. 461–470.
[10] P. Gong, J. Ye, and C. Zhang, “Robust multi-task feature learning,”
in Proceedings of the 18th international Conference on Knowledge
Discovery and Data Mining, 2012, pp. 895–903.

[11] S. Ji and J. Ye, “An accelerated gradient method for trace norm
minimization,” in Proceedings of the 26th International Conference
on Machine Learning, 2009, pp. 457–464.

[12] R. Mazumder, T. Hastie, and R. Tibshirani, “Spectral regularization
algorithms for learning large incomplete matrices,” Journal of Ma-
chine Learning Research, vol. 11, pp. 2287–2322, 2010.

[13] C.-J. Hsieh and P. Olsen, “Nuclear norm minimization via active sub-
space selection,” in Proceedings of the 31st International Conference
on Machine Learning, 2014, pp. 575–583.

[14] T. Zhang, “Analysis of multi-stage convex relaxation for sparse
regularization,” Journal of Machine Learning Research, vol. 11, pp.
1081–1107, 2010.

4130–4137.

[22] C. Lu, C. Zhu, C. Xu, S. Yan, and Z. Lin, “Generalized singular
value thresholding,” in Proceedings of the 29th AAAI Conference on
Artiﬁcial Intelligence, 2015.

[23] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with
randomness: Probabilistic algorithms for constructing approximate
matrix decompositions,” SIAM Review, vol. 53, no. 2, pp. 217–288,
2011.

[24] N. Parikh and S. Boyd, “Proximal algorithms,” Foundations and

Trends in Optimization, vol. 1, no. 3, pp. 127–239, 2014.

[25] J.-F. Cai, E. J. Cand`es, and Z. Shen, “A singular value thresholding
algorithm for matrix completion,” SIAM Journal on Optimization,
vol. 20, no. 4, pp. 1956–1982, 2010.

[26] S. Wang, D. Liu, and Z. Zhang, “Nonconvex relaxation approaches
to robust matrix recovery,” in Proceedings of the 23rd International
Joint Conference on Artiﬁcial Intelligence, 2013, pp. 1764–1770.
[27] R. M. Larsen, “Lanczos bidiagonalization with partial reorthogonal-
ization,” Department of Computer Science, Aarhus University, DAIMI
PB-357, 1998.

[28] K.-C. Toh and S. Yun, “An accelerated proximal gradient algorithm
for nuclear norm regularized linear least squares problems,” Paciﬁc
Journal of Optimization, vol. 6, no. 615-640, p. 15, 2010.

[29] Z. Wen, W. Yin, and Y. Zhang, “Solving a low-rank factorization
model for matrix completion by a nonlinear successive over-relaxation
algorithm,” Mathematical Programming Computation, vol. 4, no. 4,
pp. 333–361, 2012.

[30] Z. Wang, M.-J. Lai, Z. Lu, W. Fan, H. Davulcu, and J. Ye, “Rank-
one matrix pursuit for matrix completion,” in Proceedings of the 31st
International Conference on Machine Learning, 2014, pp. 91–99.

[31] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of
subspace structures by low-rank representation,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 171–
184, 2013.

[15] E. J. Cand`es, M. B. Wakin, and S. P. Boyd, “Enhancing sparsity
by reweighted (cid:96)1 minimization,” Journal of Fourier Analysis and
Applications, vol. 14, no. 5-6, pp. 877–905, 2008.

[32] P. Tseng, “Convergence of a block coordinate descent method for
nondifferentiable minimization,” Journal of Optimization Theory and
Applications, vol. 109, no. 3, pp. 475–494, 2001.

[16] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He, “Fast and accurate
matrix completion via truncated nuclear norm regularization,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 35,
no. 9, pp. 2117–2130, 2013.

[17] J. Fan and R. Li, “Variable selection via nonconcave penalized like-
lihood and its oracle properties,” Journal of the American Statistical
Association, vol. 96, no. 456, pp. 1348–1360, 2001.

[33] X. Zhang, D. Schuurmans, and Y.-l. Yu, “Accelerated training for
matrix-norm regularization: A boosting approach,” in Advances in
Neural Information Processing Systems, 2012, pp. 2906–2914.
[34] H. Avron, S. Kale, V. Sindhwani, and S. P. Kasiviswanathan, “Efﬁ-
cient and practical stochastic subgradient descent for nuclear norm
regularization,” in Proceedings of the 29th International Conference
on Machine Learning, 2012, pp. 1231–1238.

[18] C.-H. Zhang, “Nearly unbiased variable selection under minimax
concave penalty,” Annals of Statistics, vol. 38, no. 2, pp. 894–942,
2010.

[35] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Image denoising
by sparse 3-D transform-domain collaborative ﬁltering,” IEEE Trans-
actions on Image Processing, vol. 16, no. 8, pp. 2080–2095, 2007.

[19] A. L. Yuille and A. Rangarajan, “The concave-convex procedure,”

Neural Computation, vol. 15, no. 4, pp. 915–936, 2003.

[20] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye, “A general iterative
shrinkage and thresholding algorithm for non-convex regularized
optimization problems,” in Proceedings of
the 30th International
Conference on Machine Learning, 2013, pp. 37–45.

[21] C. Lu, J. Tang, S. Yan, and Z. Lin, “Generalized nonconvex nons-
mooth low-rank minimization,” in Proceedings of the International
Conference on Computer Vision and Pattern Recognition, 2014, pp.

[36] S. Gu, L. Zhang, W. Zuo, and X. Feng, “Weighted nuclear norm
minimization with application to image denoising,” in Proceedings
of the International Conference on Computer Vision and Pattern
Recognition, 2014, pp. 2862–2869.

[37] S. Boyd and L. Vandenberghe, Convex optimization.

Cambridge

university press, 2004.

A. Proof of Proposition II.2

APPENDIX

If the optimal y∗
obviously y∗

i ≤ σi. Otherwise,

i is achieved at the boundary (i.e., y∗

i = 0),

This Proposition appears in [22, 36], for completeness, we
also present a proof here. First, note that for a matrix X ∈
Rm×n and any orthogonal projection matrices P ∈ Rm×m
and Q ∈ Rn×n (where P (cid:62)P = I, Q(cid:62)Q = I), X has the
same singular values with P (cid:62)XQ. Therefore, Assumption
A3 implies r is invariant to orthogonal projection, i.e.,

r(X) = r(P (cid:62)XQ)

0 ∈ y∗

i − σi + µ∂ ˆr(y∗

i ).

(8)

Since ˆr(x) is non-decreasing on x ≥ 0, its super-gradient
∂ ˆr(·) is non-negative on R+, and so y∗

i ≤ σi.

2) : Now, consider an (i, j) pair such that σj ≥ σi.

Assume that y∗

i ≥ 0 and y∗

j ≥ 0. From (8), we have

Then, we introduce the following Proposition in [37].

0 ∈ y∗

i − σi + µ∂ ˆr(y∗

i ) and 0 ∈ y∗

j − σj + µ∂ ˆr(y∗

j ).

Proposition A.1. Let (u∗, v∗) = arg maxu,v(u(cid:62)Xv)
:
(cid:107)u(cid:107)2 = (cid:107)v(cid:107)2 = 1. Then, u∗ (resp. v∗) is the left (resp.
right) singular vector of X, and the optimal objective value
is σ1, the largest singular value of X.

Let the SVD of X be P ΣX Q(cid:62). Since, (cid:107) · (cid:107)F and r(·) are

invariant to orthogonal projections,

P µ

r(·)(Z) = min
X

1
2

(cid:107)X − Z(cid:107)2

F + µr(X)

(6)

1
2
1
2

= min

P,ΣX ,Q

= min

P,ΣX ,Q
1
= min
2
ΣX
− max
P,Q

(cid:107)P ΣX Q(cid:62) − Z(cid:107)2

F + µr(P ΣX Q(cid:62))

(cid:107)ΣX − P (cid:62)ZQ(cid:107)2

F + µr(ΣX )

tr(Σ2

X + Z (cid:62)Z) + µr(ΣX )

tr(ΣX P (cid:62)ZQ).

Let pi (resp. qi) be the ith column of P (resp. Q). We have

max
P,Q

tr(ΣX P (cid:62)ZQ) =

[σX ]i max
pi,qi

p(cid:62)
i Zqi.

n
(cid:88)

i=1

Recall that the SVD of Z is U ΣV (cid:62). Using Proposition A.1,
σ1 = maxp1,q1 p(cid:62)
1 Zq1, and p1 = u1, q1 = v1 where ui
(resp. vi) is the ith column of U (resp. V ). Since pi
(cid:54)=
pj, qi (cid:54)= qj if i (cid:54)= j, again by Proposition A.1, we have
p2 = u2, q2 = v2, and so on. Hence, P = U, Q = V and
Σ = P (cid:62)ZQ, then (6) can then be rewritten as:

j > y∗

i , and thus ∂ ˆr(y∗

i , and thus ∂ ˆr(y∗
j ) ≥ ∂ ˆr(y∗

Again from assumption A3, since ˆr(x) is concave and
non-decreasing on x ≥ 0, its super-gradient ∂ ˆr(·) is thus also
non-increasing. If y∗
is not achieved at the boundary (i.e.,
i
is locally optimal), consider σj > σi > 0. To ensure that (8)
holds, we can ether (i) y∗
j ) ≤ ∂ ˆr(y∗
i );
or (ii) y∗
j < y∗
i ). However,
∂ ˆr(·) is non-negative, and thus lower-bounded. Hence, there
always exists y∗
to ensure (8). If multiple solutions
exist, we take the largest one. So, we must have y∗
j > y∗
i .
i and y∗
3) : Thus, the smaller the σi, the smaller is y∗
i ≤
i ) is non-increasing on R+, µˆr(y∗
σi. Since ˆr(y∗
i ) will not
become smaller. Thus, there must exists γ such that once
σi ≤ γ, (8) no longer holds, and y∗
is not locally optimal
i
and lies on the boundary (i.e., y∗
i = 0).

j > y∗
i

C. Proof of Proposition III.3

Since span(Uˆk) ⊆ span(Q) and Q is orthogonal, it can
be written as Q = [U=; U⊥]R where span(U=) = span(Uˆk),
⊥ Uˆk = 0 and R is a rotation matrix (RR(cid:62) = R(cid:62)R =
U (cid:62)
I). Thus, Q(cid:62)Z t = R(cid:62)[U=; U⊥](cid:62)Z t and its rank-ˆk SVD is
R(cid:62)[U=; 0](cid:62)UˆkΣV (cid:62)
ˆk

. Using Proposition II.2,

proxµ(r(·))Q(cid:62)Z t = R(cid:62)

(cid:21)

(cid:20)U (cid:62)
=
0

Uˆk

ˆΣV (cid:62)
ˆk

,

min
ΣX

1
2

(cid:107)ΣX − Σ(cid:107)2

F + µr(ΣX )

(7)

where ˆΣ = Diag(y∗
Then note that,

1, . . . , y∗
ˆk

) is the optimal solution in (2).

= min
y≥0

1
2

n
(cid:88)

i=1

which leads to (2).

(yi − σi)2 + µr(Diag(y)),

B. Proof of Proposition III.1
1) : First, we show y∗

i ≤ σi. By assumption A3, (7) (or,
equivalently, (2) with Proposition II.2) can be rewritten as

min
ΣX

1
2

(cid:107)ΣX − Σ(cid:107)2

F + µ

ˆr ([σX ]i)

n
(cid:88)

i=1

=

n
(cid:88)

i=1

min
yi≥0

1
2

(yi − σi)2 + µˆr(yi),

Qproxµ(r(·))Q(cid:62)Z t = [U=; U⊥]RR(cid:62)
(cid:20)U (cid:62)
=
0
ˆΣV (cid:62)
ˆk

= [U=; U⊥]

= U=U (cid:62)

= Uˆk

(cid:21)

.

(cid:21)

(cid:20)U (cid:62)
=
0

Uˆk

ˆΣV (cid:62)
ˆk

Uˆk

ˆΣV (cid:62)
ˆk

Since span(Uˆk) = span(U=), so U=U (cid:62)

= = UˆkU (cid:62)
ˆk

, and

U=U (cid:62)

= Uˆk

ˆΣV (cid:62)
ˆk

= Uˆk(U (cid:62)
ˆk

Uˆk) ˆΣV (cid:62)
ˆk

= Uˆk

ˆΣV (cid:62)
ˆk

,

which is proxµ(r(·))Z t.

D. Proof of Proposition III.4

Since (3) holds, we have

F (X t+1) ≤ F (X t) − c1(cid:107)X t+1 − X t(cid:107)F .

Sum it from 0 to T , we have

Since span(U=) = span(U ), we have U=U (cid:62)
ˆU = R(cid:62)[U=, 0](cid:62)U . Then,

= = U U (cid:62). Let

ˆU (cid:62) ˆU = U (cid:62)[U=, 0]RR(cid:62)[U=, 0](cid:62)U

= U (cid:62)U=U (cid:62)

= U = (U (cid:62)U )(U (cid:62)U ) = I.

F (X 0) − F (X T +1) ≥ c1

(cid:107)X t+1 − X t(cid:107)2
F .

(9)

Hence, ˆU ΣV (cid:62) is the reduced SVD of P (cid:62)X. Similarly, for
Q, we obtain that Σ is also the singular values of P (cid:62)XQ.

T
(cid:88)

t=1

By assumption A2, F (X) is bounded below. Thus, as

T → +∞, there exists a ﬁnite constant α such that

H. Proof of Theorem III.8

α =

(cid:107)X t+1 − X t(cid:107)2
F .

(10)

+∞
(cid:88)

t=1

Hence, we must have lim
t→∞

{X t} converges to a limit point X ∗.

(cid:107)X t+1 − X t(cid:107)2

F = 0, and thus

E. Proof of Theorem III.5

Next, we show that X ∗ is a critical point of (1). First, as in
[20], it is easy to see that r(·) here can also be decomposed
as the difference of two convex functions r1(X) and r2(X)
i.e., r(X) = r1(X)−r2(X). Consider the optimal conditions
in proximal step, we have

0 ∈ ∇f (X t) + X t+1 − X t

(11)

+ λ∂r1(X t+1) − λ∂r2(X t+1).

For limit point X t+1 = X t = X ∗, so X t+1 − X t = 0

and vanish. Thus,

0 ∈ ∇f (X ∗) + λ∂r1(X ∗) − λ∂r2(X ∗),

and X ∗ is a critical point of (1).

F. Proof of Corollary III.6

In Theorem III.5, we have shown Algorithm 2 can con-
verge to a critical point of (1). Then, from (9), rearrange
items we will have

min
t=1,··· ,T

(cid:107)X t+1 − X t(cid:107)2

F ≤

(cid:107)X t+1 − X t(cid:107)2
F

1
T

T
(cid:88)

t=1

≤

1
c1T

Here, we prove the case for two blocks of parameters
(5) as an example. Extension to multiple block is easily
obtained.

1) : Let ∆2

Y t = (cid:107)Y t+1 −
X t = (cid:107)X t+1 − X t(cid:107)F , and ∆2
Y t(cid:107)F . When sufﬁcient decrease holds for both X and Y ,
we have

F (X t+1, Y t+1) ≤ F (X t+1, Y t) − c1∆2
Y t

≤ F (X t, Y t) − c1∆2

X t − c1∆2
Y t

Summarize above from t = 0 to T , we get

F (X 1, Y 1) − F (X T +1, Y T +1) ≥ c1

(cid:0)∆2

X t + ∆2
Y t

(cid:1) . (12)

T
(cid:88)

t=0

Since F (X, Y ) is bounded below, L.H.S above is a ﬁnite

positive constant. Same as (10):

lim
t→∞

(cid:107)X t+1 − X t(cid:107)F = 0,

(cid:107)Y t+1 − Y t(cid:107)F = 0.

lim
t→∞

Thus, (cid:80)T
2) : From the optimal conditions of the proximal step,

X t + ∆2
Y t

(cid:1) ≤ +∞.

(cid:0)∆2

t=0

similar to (11), we have

0 ∈ ∇Y f (X ∗, Y ∗) + β∂(cid:107)Y ∗(cid:107)1,
0 ∈ ∇X f (X ∗, Y ∗) + λ∂r1(X ∗) − λ∂r2(X ∗).

Thus, (X ∗, Y ∗) is a critical point of (5).

3) : Finally, using same technique at proof of Corol-

lary III.6 and (12), it is easy to obtain

(cid:2)F (X 1) − F (X T +1)(cid:3) ,

min
t=1,··· ,T

(cid:0)∆2

X t + ∆2
Y t

(cid:1) ≤

1
c1T

(cid:2)F (X 1, Y 1) − F (X T +1, Y T +1)(cid:3) .

which proves the Corollary.

G. Proof of Proposition III.7

By deﬁnition of the Frobenoius norm, we only need
to show that P (cid:62)XQ has the same singular values as
X. Since U ⊆ P , we partition P as P = [U=, U⊥]R,
where span(U=) = span(U ), U⊥ is orthogonal to U (i.e.,
U (cid:62)

= U⊥ = 0), and R is a rotation matrix. Then,
P (cid:62)X = R(cid:62) [U=, U⊥](cid:62) U ΣV (cid:62) = R(cid:62)[U=, 0](cid:62)U ΣV (cid:62).

I. Solution of GSVT and details of Corollary III.2

To simplify notations, we write yi as y, and σi as σ. Our
focus here is γ and is derived based on GIST [20]. For
LSP, MCP and SCAD, the relationship between different
stationary points is ignored in [20], thus their solutions are
not necessarily the local optimal.

1) (cid:96)1-regularizer: The

at
Lemma II.1, as it can be seen that γ = µ for the nuclear
norm.

closed-form solution is

2) LSP: For LSP, (2) becomes

h(y) ≡

(y − σ)2 + µ log

1 +

min
y

(cid:16)

(cid:17)

.

y
θ

1
2

If σ = 0, obviously y∗ = 0. So we only need to consider
σ > 0. Now,

∇h(y) = y − σ +

µ
y + θ

.

Since θ, y > 0,

(θ + y)∇h(y) = (y + θ)(y − σ) + µ.

= y2 − (σ − θ)y + µ − θσ.

(13)

Case 1: ∆ ≡ (σ + θ)2 − 4µ ≤ 0: Then ∇h(y) ≥ 0 on
R+, and thus h(y) is non-deceasing on y ≥ 0. If 0 ≤ σ ≤
min (cid:0)0, −θ + 2

µ(cid:1), we have arg miny h(y) = 0.

√

Case 2: ∆ > 0. The square roots of y2 − (σ − θ)y + µ −

θσ = 0 in (13) are
1
2
1
2

ˆy1 =

ˆy2 =

(cid:16)

(cid:16)

(cid:17)
σ − θ − (cid:112)(σ + θ)2 − 4µ
(cid:17)
σ − θ + (cid:112)(σ + θ)2 − 4µ

,

.

Since h(y) has two stationary points, it is of the form in
Figure 8, and y∗ depends only on

h(0) =

σ2,

h(ˆy2) = h

(σ − θ +

∆)

.

√

(cid:19)

(cid:18) 1
2

1
2

Thus, if h(0) < h(ˆy2), y∗ = 0. When h(0) = h(ˆy2), we take
the largest one as y∗ = max (0, ˆy2) (and thus the solution
may not be unique). Finally, when h(0) > h(ˆy2), we have
y∗ = ˆy2.

Figure 8.

Illustration for Case 2.

However, obtaining γ by directly comparing h(0) and
h(ˆy2) is complicated and has no simple closed-form solu-
tion. Here, we take a simpler approach. Once ˆy2 ≤ 0, we
(cid:1), we have ˆy2 ≤ 0 and
have y∗ = 0. I.e., if σ ≤ min (cid:0)θ, µ
y∗ = 0.

θ

Finally, on combining both cases, the threshold for LSP

can be obtained as
(cid:110)

γ = max

= min

(cid:17)

, θ

.

(cid:16) µ
θ

min (0, −θ + 2

µ) , min

√

(cid:17)(cid:111)

(cid:16) µ
θ

, θ

Lemma A.2. When r(·) is the LSP, the optimal solution
of the corresponding proximal operator is proxµ(r(·))Z =
U Diag(y∗



n)V (cid:62) where

1, . . . , y∗

y∗
i =

σi ≤ min(0, −θ + 2
0
0
σi > min(0, −θ + 2
ˆy2 σi > min(0, −θ + 2



√
√
√

µ),
µ) and h(0) < h(ˆy2),
µ) and h(0) ≥ h(ˆy2).
2 (σi − θ + (cid:112)(σ + θ)2 − 4µ).

depends only on σi, and ˆy2 = 1

3) Capped (cid:96)1: Problem (2) then becomes

1
2
This can be written as

h(y) ≡

(y − σ)2 + µ min (y, θ) .

arg min h(y) =

(cid:26) arg min h1(y)
arg min h2(y)

y ≤ θ
y > θ

,

where h1(y) = 1
The optimal cases for h1(y) are:

2 (y −σ)2 +µy, and h2(y) = 1

2 (y −σ)2 +µθ.






h1(y∗ = 0) = 0,
h1(y∗ = σ − λ) = − 1
h1(y∗ = θ) = 1

2 µ2 + µσ,
2 (θ − σ)2 + µσ,

σ − µ ≤ 0,
0 < σ − µ < θ,
θ ≤ σ − µ,

And those for h2(y) are:

(cid:40)

h2(y∗ = θ) = 1
h2(y∗ = σ) = µθ,

2 (θ − σ)2 + µθ, σ ≤ θ,
σ > θ.

Consider cases of θ ≤ µ and θ > µ. Taking the minimum
over above functions, the optimal of x∗ is:
(cid:40)

y∗ =

(σ − µ)+ σ ≤ θ + 1
σ > θ + 1
θ

2 µ,
2 µ.

Thus, for the capped-(cid:96)1 regularizer, γ = min(µ, 1
2 λ + θ).
Combining with Proposition II.2, we obtain the following:

Lemma A.3. When r(·) is the capped-(cid:96)1,
mal solution of
proxµ(r(·))Z = U Diag(y∗

the opti-
the corresponding proximal operator is
n)V (cid:62), where

1, . . . , y∗

(cid:40)

y∗
i =

(σi − µ)+ σi ≤ θ + 1
σi > θ + 1
θ

2 µ
2 µ

,

and depends on σi.

4) TNN: For the TNN, it directly controls the number of
singular values. However, from Lemma A.4, it is to see that
γ = min (µ, σθ+1).

Lemma A.4. [16] When r(·) is the TNN regularizer, the
optimal solution of the proximal operator is

proxµ(r(·))Z = U

(cid:16)

Σ − µ ˜Iθ

(cid:17)

+

V (cid:62),

Using Proposition II.2, the optimal solution is shown in
Lemma A.2.

where ˜Ik is the square matrix with all zeros elements except
at positions [ ˜Iθ]ii = 1 for i > θ.

(3). If 0 < θ < 1, again it is a quadratic function, but the
coefﬁcient on the quadratic term is negative. Thus

Thus, we can get
(1). For h1(y), the optimal is

5) MCP: For MCP, again, y∗ for problem (2) becomes

(cid:40)

y∗ =

arg min h1(y) 0 ≤ y ≤ θµ
arg min h2(y)

y > θµ

where h1(y) and h2(y) are deﬁned as
1
2
1
2

1
θ
y2 − σy +

h2(y) =

h1(y) =

σ2 +

(1 −

1
2

1
2

1
2

θµ2

)y2 − (σ − µ)y +

σ2

For h1(y), the optimal depends on θ as:

(1). If θ = 1, then the optimal is:

y∗ =

0 ≤ σ ≤ µ

(cid:40)
0
µ σ > µ

(2). If θ > 1, note that it is a quadratic function and the

optimal depends on ¯y = θ(σ−µ)

θ−1 . As a result:

y∗ =






0 ≤ σ ≤ µ
µ < σ < θµ

0
¯y
θµ σ ≥ θµ

(cid:40)

y∗ =

0 ≤ σ ≤ 1

0
θµ σ > 1

2 θµ + 1
2 µ

2 θµ + 1

2 µ

Then, for h2(y), it is simple:

(cid:40)

y∗ =

θµ 0 ≤ σ ≤ θµ
σ > θµ
σ

Combine h1(y) and h2(y):
(1). If θ = 1, then

(cid:40)

y∗ =

0 ≤ σ ≤ µ

0
σ σ > µ

(2). If θ > 1, then (¯y = θ(σ−µ)

θ−1 ):

y∗ =





0 ≤ σ ≤ µ
0
¯y
µ < σ < θµ and h1(¯y) ≤ h2(θµ)
θµ µ < σ < θµ and h1(¯y) > h2(θµ)
σ

σ ≥ θµ

(14)

(15)

(3). If 0 < θ < 1, we need to compare h1(0) and h2(σ),

then we have:

(cid:40)

y∗ =

0
σ σ >

0 ≤ σ ≤
θµ

√

√

θµ

(16)

y∗
i =

Thus, γ for MCP is:

(cid:40)√

γ =

θµ 0 < θ < 1
θ ≥ 1

µ

Using Proposition II.2, the optimal solution is shown in
Lemma A.5.

Lemma A.5. When r(·) is the MCP, the optimal solution of
the corresponding proximal operator is:

proxµ(r(·))Z = U Diag(y∗

n)V (cid:62),
where y∗
i depends on θ and σi, i.e. if θ > 1, then y∗
by (14); then if θ = 1, then y∗
i
0 < θ < 1, y∗
i is given by (16).

i is given
is given by (15); ﬁnally, if

1, . . . , y∗

6) SCAD: Again, it can be written as (θ > 2)

y∗ =

0 ≤ y ≤ µ

arg min h1(y)
arg min h2(y) µ < y ≤ θµ
arg min h3(y)

θµ < y

where h1(y), h2(y) and h3(y) are deﬁned as

h1(y) =

(y − σ)2 + µy,

h2(y) =

(y − σ)2 +

h3(y) =

(y − σ)2 +

−y2 + 2θµy − µ2
2(θ − 1)

,

(θ + 1)µ2
2

.






1
2
1
2
1
2

(2). For h2(y), the optimal is

y∗ =

(cid:40)
0
σ − µ σ > µ

0 ≤ σ ≤ µ

y∗ =




2µ
(θ−1)σ−θµ
θ−2



θµ

0 ≤ σ ≤ 2µ
2µ < σ < θµ
σ ≥ θµ

(3). For h3(y), the optimal is

(cid:40)

y∗ =

θµ 0 ≤ σ ≤ θµ
σ > θµ
σ

To get γ, we need to compare h1(0), h2(µ) and h3(θµ),
it is easy to verify h1(0) is smallest, thus γ = µ. Finally,
using Proposition II.2, the optimal solution is shown in
Lemma A.6.

Lemma A.6. When r(·) is the SCAD, the optimal solution
of the corresponding proximal operator is proxµ(r(·))Z =
1, . . . , y∗
U Diag(y∗


n)V (cid:62) where




0 ≤ σi ≤ µ

0
σi − µ µ < σi ≤ 2µ
ˆyi
θµ
σi

2µ < σi < θµ and h2(ˆyi) ≤ h3(θµ)
2µ < σi < θµ and h2(ˆyi) > h3(θµ)
σi ≥ θµ

depends on σi and ˆyi = (θ−1)σi−θµ

.

θ−2

