Exploiting Rich Syntactic Information for Semantic Parsing
with Graph-to-Sequence Model

Kun Xu1∗, Lingfei Wu2, Zhiguo Wang2, Mo Yu2, Liwei Chen3, Vadim Sheinin2
1Tencent AI Lab
2IBM Research
3Peking University, Beijing, China
{syxu828,zgw.tomorrow,gflfof}@gmail.com, lwu@email.wm.edu
chenliwei@pku.edu.cn,vadims@us.ibm.com

Abstract

Existing neural semantic parsers mainly utilize
a sequence encoder, i.e., a sequential LSTM,
to extract word order features while neglect-
ing other valuable syntactic information such
as dependency graph or constituent trees. In
this paper, we ﬁrst propose to use the syntac-
tic graph to represent three types of syntac-
tic information, i.e., word order, dependency
and constituency features. We further employ
a graph-to-sequence model to encode the syn-
tactic graph and decode a logical form. Exper-
imental results on benchmark datasets show
that our model is comparable to the state-of-
the-art on Jobs640, ATIS and Geo880. Experi-
mental results on adversarial examples demon-
strate the robustness of the model is also im-
proved by encoding more syntactic informa-
tion.

1

Introduction

The task of semantic parsing is to translate text to
its formal meaning representations, such as logical
forms or structured queries. Recent neural seman-
tic parsers approach this problem by learning soft
alignments between natural language and logical
forms from (text, logic) pairs (Jia and Liang, 2016;
Dong and Lapata, 2016; Krishnamurthy et al.,
2017). All these parsers follow the conventional
encoder-decoder architecture that ﬁrst encodes the
text into a distributional representation and then
decodes it to a logical form. These parsers may
differ in the choice of the decoders, such as se-
quence or tree decoders, but they utilize the same
encoder which is essentially a sequential Long
Short-Term Memory network (SeqLSTM). This
encoder only extracts word order features while
neglecting useful syntactic information, such as
dependency parse and constituency parse.

∗ This work is done when the author was in IBM Re-

search.

However,

the syntactic features capture im-
portant structural information of the natural lan-
guage input, which complements the simple word
sequence.
For example, a dependency graph
presents grammatical relations that hold among
the words; and a constituent tree provides a phrase
structure representation.
Intuitively, by incorpo-
rating such additional information, the encoder
could produce a more meaningful and robust sen-
tence representation. The combination of these
features (i.e., sequence + trees) forms a general
graph structure (see Figure 1). This inspires us
to apply a graph encoder to produce a represen-
tation of a graph-structured input. The graph en-
coder also has the advantages that it could simulta-
neously encode all types of syntactic contexts, and
incorporate multiple types of syntactic structures
in a uniﬁed way.

In this paper, we ﬁrst introduce a structure,
namely syntactic graph, to represent three types
of syntactic information, i.e., word order, depen-
dency and constituency features (see §2). We then
employ a novel graph-to-sequence (Graph2Seq)
model (Xu et al., 2018), which consists of a graph
encoder and a sequence decoder, to learn the rep-
resentation of the syntactic graph (see §3). Specif-
ically, the graph encoder learns the representation
of each node by aggregating information from its
K-hop neighbors. Given the learned node em-
beddings, the graph encoder uses a pooling-based
method to generate the graph embedding. On the
decoder side, a Recurrent Neural Network (RNN)
decoder takes the graph embedding as its initial
hidden state to generate the logical form while
employing an attention mechanism over the node
embeddings. Experimental results show that our
model achieves the competitive performance on
Jobs640, ATIS and Geo880 datasets.

Different from existing works, we also inves-
tigate the robustness of our model by evaluating

8
1
0
2
 
g
u
A
 
3
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
2
6
7
0
.
8
0
8
1
:
v
i
X
r
a

Figure 1: The syntactic graph for the Jobs640 question what are the jobs for programmer that has salary 50000 that uses c++
and not related with AI. Due to the space limitation, the constituent tree is partially shown here.

the model on two types of adversarial examples
(Belinkov and Bisk, 2017; Cheng et al., 2018).
Experimental results show that the model cou-
pling all syntactic features has the best robustness,
achieving the best performance. Our code and
data is available at https://github.com/IBM/
Text-to-LogicForm.

2 Syntactic Graph

In our model, we represent three types of syntac-
tic features, i.e., word order, dependency parse and
constituency parse, in the syntactic graph (see Fig-
ure 1).
• Word Order Features. Previous neural seman-
tic parsers mainly use these features by building a
SeqLSTM that works on the word sequence. Our
syntactic graph also incorporates this information
by generating a node for each word and connect-
ing them in the chain form. In order to capture the
forward and backward contextual information, we
link these nodes in two directions, that is, from left
to right and from right to left.
• Dependency Features. A dependency parse de-
scribes the grammatical relations that hold among
words. Reddy et al. (2016, 2017) have demon-
strated that the dependency parse tree could be
directly transformed to a logical form, which in-
dicates that the dependency information (i.e., tree
structure and dependency labels) is critical to the
semantic parsing task. We incorporate this infor-
mation into the syntactic graph by adding directed
edges between the word nodes and assign them
with dependency labels.
• Constituency Features. Similar to the depen-
dency parse, the constituency parse represents the
phrase structure, which is also important to the se-
mantic parsing task. Take Figure 1 as an example:
given the constituent tree that explicitly annotates
“not related with AI” (node #1) is a proposition

phrase, the model could learn a meaningful em-
bedding for this phrase by encoding this structure
into the model. Motivated by this observation, we
add the non-terminal nodes of the constituent tree
and the edges describing their parent-child rela-
tionships into the syntactic graph.

3 Graph-to-sequence Model for

Semantic Parsing

After building the syntactic graph for the input
text, we employ a novel graph-to-sequence model
(Xu et al., 2018), which includes a graph encoder
and a sequence decoder with attention mechanism,
to map the syntactic graph to the logical form.
Conceptually, the graph encoder generates node
embeddings for each node by accumulating infor-
mation from its K-hop neighbors, and then pro-
duces a graph embedding for the entire graph by
abstracting all these node embeddings. Next, the
sequence decoder takes the graph embedding as
the initial hidden state, and calculates the atten-
tion over all node embeddings on the encoder side
to generate logical forms. Note that this graph
encoder does not explicitly encode the edge label
information, therefore, for each labeled edge, we
add a node whose text attribute is the edge’s label.

Node Embedding. Given the syntactic graph
G = (V, E), we take the embedding generation
process for node v ∈ V as an example to explain
the node embedding generation algorithm1:
(1) We ﬁrst transform node v’s text attribute to a
feature vector, av, by looking up the embedding
matrix We;
(2) The neighbors of v are categorized into for-
ward neighbors N(cid:96)(v) and backward neighbors
N(cid:97)(v) according to the edge direction. In partic-
ular, N(cid:96)(v) returns the nodes that v directs to and

1Interested readers may refer to (Xu et al., 2018) for more

implementation details.

N(cid:97)(v) returns the nodes that direct to v;
(3) We aggregate the forward representations of
v’s forward neighbors {hk−1
u(cid:96) , ∀u ∈ N(cid:96)(v)} into
a single vector, hk
N(cid:96)(v), where k∈{1, ..., K} is the
iteration index. Speciﬁcally, this aggregator feeds
each neighbor’s vector to a fully-connected neural
network and applies an element-wise max-pooling
operation to capture different aspects of the neigh-
bor set. Notice that, at iteration k, this aggregator
only uses the representations generated at k − 1.
The initial forward representation of each node is
its feature vector calculated in step (1);
(4) We concatenate v’s current forward represen-
tation hk−1
v(cid:96) with the newly generated neighbor-
hood vector hk
N(cid:96)(v). This concatenated vector is
fed into a fully connected layer with nonlinear ac-
tivation function σ, which updates the forward
representation of v, hk
v(cid:96), to be used at the next it-
eration;
(5) We update the backward representation of v,
hk
v(cid:97) using the similar procedure as introduced in
step (3) and (4) except that operating on the back-
ward representations;
(6) We repeat steps (3)∼(5) K times, and the con-
catenation of the ﬁnal forward and backward rep-
resentation is used as the ﬁnal representation of
v. Since the neighbor information from different
hops may have different impact on the node em-
bedding, we learn a distinct aggregator at each it-
eration.
Graph Embedding. We feed the obtained node
embeddings into a fully-connected neural net-
work, and apply the element-wise max-pooling
operation on all node embeddings. We did not
ﬁnd substantial performance improvement using
mean-pooling.
Sequence Decoding. The decoder is an RNN
which predicts the next token yi given all the pre-
vious words y<i = y1, ..., yi−1, the RNN hidden
state si for time-step i and the context vector ci
that captures the attention of the encoder side. In
particular, the context vector ci depends on a set
of node representations (h1,...,hV ) to which the
encoder maps the input graph. The context vec-
tor ci is dynamically computed using an attention
mechanism over the node representations. The
whole model is jointly trained to maximize the
conditional log-probability of the correct descrip-
tion given a source graph. In the inference phase,
we use the beam search algorithm to generate a
description with beam size = 3.

Method
Zettlemoyer and Collins (2007)
Kwiatkowski et al. (2011)
Liang et al. (2011)
Kwiatkowski et al. (2013)
Wang et al. (2014)
Zhao and Huang (2015)
Jia and Liang (2016)
Dong and Lapata (2016)-Seq2Seq
Dong and Lapata (2016)-Seq2Tree
Rabinovich et al. (2017)
Graph2Seq

w/o word order features
w/o dependency features
w/o constituency features
w/ word order features

BASELINE

Jobs Geo ATIS
84.6
86.1
79.3
82.8
88.6
-
-
87.9
90.7
89.0
-
-
91.3
90.4
-
84.2
88.9
85.0
76.3
85.0
-
84.2
85.0
87.1
84.6
87.1
90.0
92.9
85.3
85.7
85.9
88.1
91.2
82.9
84.4
86.7
83.8
85.8
89.3
84.6
84.7
88.9
83.1
84.8
88.0
83.0
84.9
88.1

Table 1: Exact-match accuracy on Jobs640, Geo880 and
ATIS.

4 Experiments

We evaluate our model on three datasets: Jobs640,
a set of 640 queries to a database of job listings;
Geo880, a set of 880 queries to a database of
U.S. geography; and ATIS, a set of 5,410 queries
to a ﬂight booking system. We use the standard
train/development/test split as previous works, and
the logical form accuracy as our evaluation metric.
The model is trained using the Adam optimizer
(Kingma and Ba, 2014), with mini-batch size 30.
Our hyper-parameters are cross-validated on the
training set for Jobs640 and Geo880, and tuned on
the development set for ATIS. The learning rate is
set to 0.001. The decoder has 1 layer, and its hid-
den state size is 300. The dropout strategy (Sri-
vastava et al., 2014) with the ratio of 0.5 is ap-
plied at the decoder layer to avoid overﬁtting. We
is initialized using GloVe word vectors from Pen-
nington et al. (2014) and the dimension of word
embedding is 300. For the graph encoder, the hop
size K is set to 10, the non-linearity function σ
is implemented as ReLU (Glorot et al., 2011), the
parameters of the aggregators are randomly initial-
ized. We use the Stanford CoreNLP tool (Manning
et al., 2014) to generate the dependency and con-
stituent trees.

Results and Discussion. Table 1 summarizes
the results of our model and existing semantic
parsers on three datasets. Our model achieves
competitive performance on Jobs640, ATIS and
Geo880. Our work is the ﬁrst to use both multiple
trees and the word sequence for semantic parsing,
and it outperforms the Seq2Seq model reported in
Dong and Lapata (2016), which only uses limited
syntactic information.

Comparison with Baseline. To better demon-
strate that our work is an effective way to uti-
lize both multiple trees and the word sequence
for semantic parsing, we compare with an addi-
tional straightforward baseline method (referred as
BASELINE in Table 1). To deal with the graph
input, the BASELINE decomposes the graph em-
bedding to two steps and applies different types of
encoders sequentially: (1) a SeqLSTM to extract
word order features, which results in word embed-
dings, Wseq; (2) two TreeLSTMs (Tai et al., 2015)
to extract the dependency tree and constituency
features while taking Wseq as initial word embed-
dings. The resulted word embeddings and non-
terminal node embeddings (from TreeLSTMs) are
then fed into a sequence decoder.

We can see that our model signiﬁcantly outper-
forms the BASELINE. One possible reason is that
our graph encoder jointly extracts these features
in a uniﬁed model by propagating the dependency
and constituency information to all nodes in the
syntactic graph. However, BASELINE separately
models these features using two distinct TreeL-
STMs. As a result, the non-terminal tree nodes
only retain only one type of syntactic information
propagated from their descendants in the tree.

Ablation Study.
In Table 1, we also report the
results of three ablation variants of our model,
i.e., without word order
features/dependency
features/constituency features. We ﬁnd that
Graph2Seq is superior to Seq2Seq (Dong and Lap-
ata, 2016) which is expected since Graph2Seq ex-
ploits more syntactic information. Among these
features, the word order feature have more impact
on the performance than other two syntactic fea-
tures. By incorporating either the dependency or
the constituency features, the model could gain
further performance improvement, which under-
lines the importance of utilizing more aspects of
syntactic information. A natural question here is
on which type of queries our model could beneﬁt
from incorporating these parse features? By ana-
lyzing the queries and our predicted logical forms,
we ﬁnd that the parse features mainly improve the
prediction accuracy for the queries with complex
logical forms. Table 2 gives some running exam-
ples of complicated queries in three datasets. We
ﬁnd that the model that exploits three syntactic
information could correctly predict these logical
forms while the model that only uses word order
features may fail.

Complicated Query & Predicted Logical Forms
Jobs Q: what are the jobs for programmer that has salary
50000 that uses c++ and not related with AI

Pred: answer(J,(job(J),-((area(J,R),const(R,’ai’))),

language(J,L),const(L,’c++’), title(J,P),
const(P,’Programmer’),salary greater than(J,
50000,year)))).

Geo Q: which is the density of the state that the largest

river in the united states run through

Pred: answer(A,(density(B,A),state(B),

longest(C,(river(C),loc(C,D),const(D,id(usa)))),
traverse(C,B))))

ATIS Q: please ﬁnd a ﬂight round trip from los angeles

to tacoma washington with a stopover in san
francisco not exceeding the price of 300 dollars
for june tenth 1993

Pred: (lambda $0 e (and (ﬂight $0) (round trip $0)

(from $0 los angeles) (to $0 tacoma washington)
(stop $0 san francisco) (< (cost $0) 300)
(day number $0 tenth) (month $0 june)
(year $0 1993)))

Table 2: Examples of complicated query and predicted logi-
cal forms.

Robustness Study.
Different from previous
works, we evaluate the robustness of our model by
creating adversarial examples with the hope to in-
vestigate the impact of introducing more syntactic
information on robustness. Speciﬁcally, we cre-
ate two types of adversarial examples and con-
duct experiments on the ATIS dataset. Follow-
ing Belinkov and Bisk (2017), we ﬁrst experiment
with the synthetic noise, SWAP, which swaps
two letters (e.g. noise→nosie). It is common to
see such noisy information when typing quickly.
Given a text, we randomly perform swap on m
∈ {1, 2, 3, 4, 5} words that not correspond to the
operators or arguments in logical forms, ensuring
the meaning of the text is not changed. We train
Graph2Seq on the training data and ﬁrst evaluate
it on the original development data, Devori. Then
we use the same model but evaluate it on a vari-
ant of Devori, whose queries contain m swapped
words.

Figure 2 summarizes the results of our model
on the ﬁrst type of adversarial examples, i.e., the
ATIS development set with the SWAP noise. From
Figure 2, we can see that (1) the performance
of our model on all combinations of features de-
grade signiﬁcantly when increasing the number of
swapped words; (2) the model that uses three syn-
tactic features (our default model) always achieves
the best performance, and the performance gap
compared to others increases when rising the num-
ber of swapped words; (3) word order features are

result of our model on the original ATIS devel-
opment set. We can see that (1) no matter which
feature our model uses, the performance degrades
at least 2.5% on the paraphrased dataset; (2) the
model that only uses word order features achieves
the worst robustness to the paraphrased queries
while the dependency feature seems more robust
than other two features. (3) simultaneously utiliz-
ing three syntactic features could greatly enhance
the robustness of our model. These results again
demonstrate that our model could beneﬁt from in-
corporating more aspects of syntactic information.

5 Related Work

Existing works of generating text representation
has evolved into two main streams. The ﬁrst one is
based on the word order, that is, either generating
general purpose and domain independent embed-
dings of word sequences (Wu et al., 2018a; Arora
et al., 2017), or building Bi-directional LSTMs
over the text (Zhang et al., 2018). These methods
neglect other syntactic information, which, how-
ever, has been proved to be useful in shallow se-
mantic parsing, e.g., semantic role labeling. To
address this, recent works attempt to incorporate
these syntactic information into the text represen-
tation. For example, (Su and Yan, 2017; Gur
et al., 2018) leverages additional information such
as dialogue and paraphrasing for semantic pars-
ing while (Xu et al., 2016) builds separated neural
networks for different types of syntactic annota-
tion. (Gormley et al., 2015; Wu et al., 2018b) de-
compose a graph to simpler sub-graphs and embed
these sub-graphs independently.

6 Conclusions

Existing neural semantic parsers mainly leverage
word order features while neglecting other valu-
able syntactic information. To address this, we
propose to build a syntactic graph which repre-
sents three types of syntactic information, and fur-
ther apply a novel graph-to-sequence model to
map the syntactic graph to a logical form. Ex-
perimental results show that the robustness of our
model is improved due to the incorporating more
aspects of syntactic information, and our model
outperforms previous semantic parsing systems.

Figure 2: The logical form accuracy on the development set
of ATIS with various swapped words number.

the most sensitive to the word sequence while the
dependency and constituency features seem more
robust to such noisy information; (4) thanks to
the robustness of the dependency and constituency
features, the default model performs signiﬁcantly
better than the one that only uses word order
features on the noisy sentences. These ﬁndings
demonstrate that incorporating more aspects of
syntactic information could enhance the robust-
ness of the model.

We also experiment with the paraphrase of the
input text as the second type of adversarial exam-
ples. More speciﬁcally, we collect the paraphrase
of a text by ﬁrst translating it to the other language
such as Chinese and then translating it back to
English, using the Google Translate service. We
use this method to collect a new variant of Devori
whose queries are the paraphrases of the original
ones. By manually reading these queries, we ﬁnd
94% queries convey the same meaning as original
ones. Similar to the ﬁrst experiment, we still train
the model on Devori and evaluate it on the newly
created dataset.

Feature
Word Order
Dep
Cons
Dep + Cons
Word Order + Dep
Word Order + Cons
Word Order + Dep + Cons

Accori Accpara Diff.
-6.1
78.7
-3.4
80.1
-5.6
77.3
-3.3
80.7
-2.9
82.3
-5.0
79.9
-2.5
83.5

84.8
83.5
82.9
84.0
85.2
84.9
86.0

Table 3: Evaluation results on ATIS where Accori and
Accpara denote the accuracy on the original and paraphrased
development set of ATIS, respectively.

Table 3 shows the results of our model on the
second type of adversarial examples, i.e., the para-
phrased ATIS development set. We also report the

References

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In ICLR.

Yonatan Belinkov and Yonatan Bisk. 2017. Synthetic
and natural noise both break neural machine transla-
tion. arXiv preprint arXiv:1711.02173.

Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen,
Seq2sick: Evaluat-
and Cho-Jui Hsieh. 2018.
ing the robustness of sequence-to-sequence mod-
arXiv preprint
els with adversarial examples.
arXiv:1803.01128.

Li Dong and Mirella Lapata. 2016. Language to logical
form with neural attention. CoRR, abs/1601.01280.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
In
2011. Deep sparse rectiﬁer neural networks.
Proceedings of the Fourteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, AIS-
TATS 2011, Fort Lauderdale, USA, April 11-13,
2011, pages 315–323.

Matthew R Gormley, Mo Yu, and Mark Dredze. 2015.
Improved relation extraction with feature-rich com-
positional embedding models. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1774–1784.

Izzeddin Gur, Semih Yavuz, Yu Su, and Xifeng Yan.
2018. Dialsql: Dialogue based structured query
generation. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1339–
1349.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. CoRR, abs/1606.03622.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1516–1526.

Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Scaling semantic
In

Luke S. Zettlemoyer. 2013.
parsers with on-the-ﬂy ontology matching.
EMNLP.

Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical gener-
alization in CCG grammar induction for semantic
parsing. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2011, 27-31 July 2011, John McIntyre
Conference Centre, Edinburgh, UK, A meeting of
SIGDAT, a Special Interest Group of the ACL, pages
1512–1523.

Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39:389–446.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code genera-
In Proceedings of the
tion and semantic parsing.
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1139–1149.

Siva Reddy, Oscar T¨ackstr¨om, Michael Collins, Tom
Kwiatkowski, Dipanjan Das, Mark Steedman, and
Mirella Lapata. 2016. Transforming dependency
structures to logical forms for semantic parsing.
Transactions of the Association for Computational
Linguistics.

Siva Reddy, Oscar T¨ackstr¨om, Slav Petrov, Mark
Steedman, and Mirella Lapata. 2017. Universal se-
mantic parsing. arXiv preprint arXiv:1702.03196.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
Journal of Machine
networks from overﬁtting.
Learning Research, 15(1):1929–1958.

Yu Su and Xifeng Yan. 2017.

mantic parsing via paraphrasing.
arXiv:1704.05974.

Cross-domain se-
arXiv preprint

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

Adrienne Wang, Tom Kwiatkowski, and Luke S.
Zettlemoyer. 2014. Morpho-syntactic lexical gen-
In Proceed-
eralization for ccg semantic parsing.
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1284–1295.

Lingfei Wu, Ian E.H. Yen, Kun Xu, Fangli Xu, Avinash
Balakrishnan, Pin-Yu Chen, Pradeep Ravikumar,
and Michael J. Witbrock. 2018a. Word mover’s em-
bedding: From word2vec to document embedding.
In EMNLP.

Lingfei Wu, Ian En-Hsu Yen, Fangli Xu, Pradeep
Ravikuma, and Michael Witbrock. 2018b. D2ke:
arXiv
From distance to kernel and embedding.
preprint arXiv:1802.04956.

Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang,
and Dongyan Zhao. 2016. Question Answering on
Freebase via Relation Extraction and Textual Evi-
dence. In ACL 2016.

Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim
Sheinin. 2018. Graph2seq: Graph to sequence
learning with attention-based neural networks.
arXiv preprint arXiv:1804.00823.

Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL.

Yue Zhang, Qi Liu, and Linfeng Song. 2018. Sentence-
state lstm for text representation. arXiv preprint
arXiv:1805.02474.

Kai Zhao and Liang Huang. 2015. Type-driven in-
cremental semantic parsing with polymorphism. In
HLT-NAACL.

