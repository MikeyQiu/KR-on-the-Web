IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

1

Finding a sparse vector in a subspace: linear
sparsity using alternating directions
Qing Qu, Student Member, IEEE, Ju Sun, Student Member, IEEE, and John Wright, Member, IEEE

6
1
0
2
 
l
u
J
 
0
2
 
 
]
T
I
.
s
c
[
 
 
3
v
9
5
6
4
.
2
1
4
1
:
v
i
X
r
a

Abstract

Is it possible to ﬁnd the sparsest vector (direction) in a generic subspace S ⊆ Rp with dim (S) = n < p? This
problem can be considered a homogeneous variant of the sparse recovery problem, and ﬁnds connections to sparse
dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper,
we focus on a planted sparse model for the subspace: the target sparse vector is embedded in an otherwise random
subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of
nonzero entries in the target sparse vector substantially exceeds O(1/
n). In contrast, we exhibit a relatively simple
nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero
entries is Ω(1). To the best of our knowledge, this is the ﬁrst practical algorithm to achieve linear scaling under
the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g.,
sparse dictionary learning.

√

Sparse vector, Subspace modeling, Sparse recovery, Homogeneous recovery, Dictionary learning, Nonconvex

optimization, Alternating direction method

Index Terms

I. INTRODUCTION
Suppose that a linear subspace S embedded in Rp contains a sparse vector x0 (cid:54)= 0. Given an arbitrary basis of S,
can we efﬁciently recover x0 (up to scaling)? Equivalently, provided a matrix A ∈ R(p−n)×p with Null(A) = S, 1
can we efﬁciently ﬁnd a nonzero sparse vector x such that Ax = 0? In the language of sparse recovery, can we
solve

min
x

(cid:107)x(cid:107)0

s.t. Ax = 0, x (cid:54)= 0

?

In contrast to the standard sparse recovery problem (Ax = b, b (cid:54)= 0), for which convex relaxations perform nearly
optimally for broad classes of designs A [2, 3], the computational properties of problem (I.1) are not nearly as well
understood. It has been known for several decades that the basic formulation

(I.1)

(I.2)

min
x

(cid:107)x(cid:107)0 ,

s.t. x ∈ S \ {0},

is NP-hard for an arbitrary subspace [4, 5]. In this paper, we assume a speciﬁc random planted sparse model for
the subspace S: a target sparse vector is embedded in an otherwise random subspace. We will show that under the
speciﬁc random model, problem (I.2) is tractable by an efﬁcient algorithm based on nonconvex optimization.

A. Motivation

The general version of Problem (I.2), in which S can be an arbitrary subspace, takes several forms in numerical
computation and computer science, and underlies several important problems in modern signal processing and
machine learning. Below we provide a sample of these applications.
Sparse Null Space and Matrix Sparsiﬁcation: The sparse null space problem is ﬁnding the sparsest matrix N
whose columns span the null space of a given matrix A. The problem arises in the context of solving linear
equality problems in constrained optimization [5], null space methods for quadratic programming [6], and solving

This work was partially supported by grants ONR N00014-13-1-0492, NSF 1343282, NSF 1527809, and funding from the Moore and
Sloan Foundations. Q. Qu, J. Sun and J. Wright are all with the Electrical Engineering Department, Columbia University, New York, NY,
10027, USA (e-mail: {qq2105, js4038, jw2966}@columbia.edu). This paper is an extension of our previous conference version [1].

1 Null(A)

.
= {x ∈ Rp | Ax = 0} denotes the null space of A.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

2

underdetermined linear equations [7]. The matrix sparsiﬁcation problem is of similar ﬂavor, the task is ﬁnding the
sparsest matrix B which is equivalent to a given full rank matrix A under elementary column operations. Sparsity
helps simplify many fundamental matrix operations (see [8]), and the problem has applications in areas such as
machine learning [9] and in discovering cycle bases of graphs [10]. [11] discusses connections between the two
problems and also to other problems in complexity theory.

Sparse (Complete) Dictionary Learning: In dictionary learning, given a data matrix Y, one seeks an approximation
Y ≈ AX, such that A is a representation dictionary with certain desired structure and X collects the representation
coefﬁcients with maximal sparsity. Such compact representation naturally allows signal compression, and also
facilitates efﬁcient signal acquisition and classiﬁcation (see relevant discussion in [12]). When A is required to
be complete (i.e., square and invertible), by linear algebra, we have2 row(Y) = row(X) [13]. Then the problem
reduces to ﬁnding sparsest vectors (directions) in the known subspace row(Y), i.e. (I.2). Insights into this problem
have led to new theoretical developments on complete dictionary learning [13–15].

Sparse Principal Component Analysis (Sparse PCA): In geometric terms, Sparse PCA (see, e.g., [16–18] for
early developments and [19, 20] for discussion of recent results) concerns stable estimation of a linear subspace
spanned by a sparse basis, in the data-poor regime, i.e., when the available data are not numerous enough to allow
one to decouple the subspace estimation and sparsiﬁcation tasks. Formally, given a data matrix Z = U0X0 + E,3
where Z ∈ Rp×n collects column-wise n data points, U0 ∈ Rp×r is the sparse basis, and E is a noise matrix, one is
asked to estimate U0 (up to sign, scale, and permutation). Such a factorization ﬁnds applications in gene expression,
ﬁnancial data analysis and pattern recognition [24]. When the subspace is known (say by the PCA estimator with
enough data samples), the problem again reduces to instances of (I.2) and is already nontrivial4. The full geometric
sparse PCA can be treated as ﬁnding sparse vectors in a subspace that is subject to perturbation.

In addition, variants and generalizations of the problem (I.2) have also been studied in applications regarding
control and optimization [25], nonrigid structure from motion [26], spectral estimation and Prony’s problem [27],
outlier rejection in PCA [28], blind source separation [29], graphical model learning [30], and sparse coding on
manifolds [31]; see also [32] and the references therein.

B. Prior Arts

Despite these potential applications of problem (I.2), it is only very recently that efﬁcient computational surrogates
with nontrivial recovery guarantees have been discovered for some cases of practical interest. In the context of sparse
dictionary learning, Spielman et al. [13] introduced a convex relaxation which replaces the nonconvex problem (I.2)
with a sequence of linear programs:

(cid:96)1/(cid:96)∞ Relaxation:

min
x

(cid:107)x(cid:107)1 ,

s.t. x(i) = 1, x ∈ S, 1 ≤ i ≤ p.

(I.3)

They proved that when S is generated as a span of n random sparse vectors, with high probability (w.h.p.), the
relaxation recovers these vectors, provided the probability of an entry being nonzero is at most θ ∈ O (1/
n). In
the planted sparse model, in which S is formed as direct sum of a single sparse vector x0 and a “generic” subspace,
Hand and Demanet proved that (I.3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales
n) [14]. One might imagine improving these results by tightening the analyses. Unfortunately, the
as θ ∈ O (1/
results of [13, 14] are essentially sharp: when θ substantially exceeds Ω(1/
n), in both models the relaxation (I.3)
provably breaks down. Moreover, the most natural semideﬁnite programming (SDP) relaxation of (I.1),

√

√

√

min
X

(cid:107)X(cid:107)1 ,

s.t.

(cid:68)

A(cid:62)A, X

(cid:69)

= 0, trace[X] = 1, X (cid:23) 0.

(I.4)

also breaks down at exactly the same threshold of θ ∼ O(1/

√

n).5

2Here, row(·) denotes the row space.
3Variants of multiple-component formulations often add an additional orthonormality constraint on U0 but involve a different notation of

sparsity; see, e.g., [16, 21–23].

4[14] has also discussed this data-rich sparse PCA setting.
5This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (with b (cid:54)= 0), in which it is possible to

handle very large fractions of nonzeros (say, θ = Ω(1/ log n), or even θ = Ω(1)) using a very simple (cid:96)1 relaxation [2, 3]

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

3

TABLE I
COMPARISON OF EXISTING METHODS FOR RECOVERING A PLANTED SPARSE VECTOR IN A SUBSPACE

Method
(cid:96)1/(cid:96)∞ Relaxation [14]
SDP Relaxation
SOS Relaxation [34]
Spectral Method [35]
This work

Recovery Condition

θ ∈ O(1/
θ ∈ O(1/

√
√

n)
n)

p ≥ Ω(n2), θ ∈ O(1)
p ≥ Ω(n2poly log(n)), θ ∈ O(1)
p ≥ Ω(n4 log n), θ ∈ O(1)

Time Complexity6
O(n3p log(1/ε))
O (cid:0)p3.5 log (1/ε)(cid:1)
∼ O(p7 log(1/ε)) 7
O (np log(1/(cid:15)))
O(n5p2 log n + n3p log(1/ε))

√

One might naturally conjecture that this 1/

n threshold is simply an intrinsic price we must pay for having an
efﬁcient algorithm, even in these random models. Some evidence towards this conjecture might be borrowed from
the superﬁcial similarity of (I.2)-(I.4) and sparse PCA [16]. In sparse PCA, there is a substantial gap between what
can be achieved with currently available efﬁcient algorithms and the information theoretic optimum [19, 33]. Is
this also the case for recovering a sparse vector in a subspace? Is θ ∈ O (1/
n) simply the best we can do with
efﬁcient, guaranteed algorithms?

√

Remarkably, this is not the case. Recently, Barak et al. introduced a new rounding technique for sum-of-squares
relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p ≥ Ω (cid:0)n2(cid:1) and
θ = Ω(1) [34]. It is perhaps surprising that this is possible at all with a polynomial time algorithm. Unfortunately,
the runtime of this approach is a high-degree polynomial in p (see Table I); for machine learning problems in
which p is often either the feature dimension or the sample size, this algorithm is mostly of theoretical interest only.
However, it raises an interesting algorithmic question: Is there a practical algorithm that provably recovers a sparse
vector with θ (cid:29) 1/

n portion of nonzeros from a generic subspace S?

√

C. Contributions and Recent Developments

In this paper, we address the above problem under the planted sparse model. We allow x0 to have up to θ0p
nonzero entries, where θ0 ∈ (0, 1) is a constant. We provide a relatively simple algorithm which, w.h.p., exactly
recovers x0, provided that p ≥ Ω (cid:0)n4 log n(cid:1). A comparison of our results with existing methods is shown in Table
I. After initial submission of our paper, Hopkins et al. [35] proposed a different simple algorithm based on the
spectral method. This algorithm guarantees recovery of the planted sparse vector also up to linear sparsity, whenever
p ≥ Ω(n2polylog(n)), and comes with better time complexity.8

Our algorithm is based on alternating directions, with two special twists. First, we introduce a special data driven
initialization, which seems to be important for achieving θ = Ω(1). Second, our theoretical results require a second,
linear programming based rounding phase, which is similar to [13]. Our core algorithm has very simple iterations,
of linear complexity in the size of the data, and hence should be scalable to moderate-to-large scale problems.

Besides enjoying the θ ∼ Ω(1) guarantee that is out of the reach of previous practical algorithms, our algorithm
performs well in simulations – empirically succeeding with p ≥ Ω (n polylog(n)). It also performs well empirically
on more challenging data models, such as the complete dictionary learning model, in which the subspace of interest
contains not one, but n random target sparse vectors. This is encouraging, as breaking the O(1/
n) sparsity barrier
with a practical algorithm and optimal guarantee is an important problem in theoretical dictionary learning [36–40].
In this regard, our recent work [15] presents an efﬁcient algorithm based on Riemannian optimization that guarantees
recovery up to linear sparsity. However, the result is based on different ideas: a different nonconvex formulation,
optimization algorithm, and analysis methodology.

√

D. Paper Organization, Notations and Reproducible Research

The rest of the paper is organized as follows. In Section II, we provide a nonconvex formulation and show its
capability of recovering the sparse vector. Section III introduces the alternating direction algorithm. In Section IV,

6All estimates here are based on the standard interior point methods for solving linear and semideﬁnite programs. Customized solvers may

result in order-wise speedup for speciﬁc problems. ε is the desired numerical accuracy.

7Here our estimation is based on the degree-4 SOS hierarchy used in [34] to obtain an initial approximate recovery.
8Despite these improved guarantees in the planted sparse model, our method still produces more appealing results on real imagery data –

see Section V-B for examples.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

4

we present our main results and sketch the proof ideas. Experimental evaluation of our method is provided in
Section V. We conclude the paper by drawing connections to related work and discussing potential improvements
in Section VI. Full proofs are all deferred to the appendix sections.

For a matrix X, we use xi and xj to denote its i-th column and j-th row, respectively, all in column vector form.
.
Moreover, we use x(i) to denote the i-th component of a vector x. We use the compact notation [k]
= {1, . . . , k}
for any positive integer k, and use c or C, and their indexed versions to denote absolute numerical constants. The
scope of these constants are always local, namely within a particular lemma, proposition, or proof, such that the
apparently same constant in different contexts may carry different values. For probability events, sometimes we will
just say the event holds “with high probability” (w.h.p.) if the probability of failure is dominated by p−κ for some
κ > 0.

The codes to reproduce all the ﬁgures and experimental results can be found online at:

https://github.com/sunju/psv.

II. PROBLEM FORMULATION AND GLOBAL OPTIMALITY
We study the problem of recovering a sparse vector x0 (cid:54)= 0 (up to scale), which is an element of a known
subspace S ⊂ Rp of dimension n, provided an arbitrary orthonormal basis Y ∈ Rp×n for S. Our starting point is
the nonconvex formulation (I.2). Both the objective and the constraint set are nonconvex, and hence it is not easy to
optimize over. We relax (I.2) by replacing the (cid:96)0 norm with the (cid:96)1 norm. For the constraint x (cid:54)= 0, since in most
applications we only care about the solution up to scaling, it is natural to force x to live on the unit sphere Sn−1,
giving

min
x

(cid:107)x(cid:107)1 ,

s.t. x ∈ S, (cid:107)x(cid:107)2 = 1.

(II.1)

This formulation is still nonconvex, and for general nonconvex problems it is known to be NP-hard to ﬁnd even
a local minimizer [41]. Nevertheless, the geometry of the sphere is benign enough, such that for well-structured
inputs it actually will be possible to give algorithms that ﬁnd the global optimizer.

The formulation (II.1) can be contrasted with (I.3), in which effectively we optimize the (cid:96)1 norm subject to the
constraint (cid:107)x(cid:107)∞ = 1: because the set {x : (cid:107)x(cid:107)∞ = 1} is polyhedral, the (cid:96)∞-constrained problem immediately
yields a sequence of linear programs. This is very convenient for computation and analysis. However, it suffers
from the aforementioned breakdown behavior around (cid:107)x0(cid:107)0 ∼ p/
n. In contrast, though the sphere (cid:107)x(cid:107)2 = 1 is a
more complicated geometric constraint, it will allow much larger number of nonzeros in x0. Indeed, if we consider
the global optimizer of a reformulation of (II.1):

√

min
q∈Rn

(cid:107)Yq(cid:107)1 ,

s.t.

(cid:107)q(cid:107)2 = 1,

(II.2)

where Y is any orthonormal basis for S, the sufﬁcient condition that guarantees exact recovery under the planted
sparse model for the subspace is as follows:

Theorem II.1 ((cid:96)1/(cid:96)2 recovery, planted sparse model). There exists a constant θ0 > 0, such that if the subspace S
follows the planted sparse model

where gi ∼i.i.d. N (0, 1
n < θ < θ0, then the unique
(up to sign) optimizer q(cid:63) to (II.2), for any orthonormal basis Y of S, produces Yq(cid:63) = ξx0 for some ξ (cid:54)= 0 with
probability at least 1 − cp−2, provided p ≥ Cn. Here c and C are positive constants.

Ber(θ) are all jointly independent and 1/

p I), and x0 ∼i.i.d.

θp

S = span (x0, g1, . . . , gn−1) ⊂ Rp,
1√

√

Hence, if we could ﬁnd the global optimizer of (II.2), we would be able to recover x0 whose number of nonzero
entries is quite large – even linear in the dimension p (θ = Ω(1)). On the other hand, it is not obvious that this
should be possible: (II.2) is nonconvex. In the next section, we will describe a simple heuristic algorithm for
approximately solving a relaxed version of the (cid:96)1/(cid:96)2 problem (II.2). More surprisingly, we will then prove that
for a class of random problem instances, this algorithm, plus an auxiliary rounding technique, actually recovers
the global optimizer – the target sparse vector x0. The proof requires a detailed probabilistic analysis, which is
sketched in Section IV-B.

Before continuing, it is worth noting that the formulation (II.1) is in no way novel – see, e.g., the work of [29]

in blind source separation for precedent. However, our algorithms and subsequent analysis are novel.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

5

III. ALGORITHM BASED ON ALTERNATING DIRECTION METHOD (ADM)
To develop an algorithm for solving (II.2), it is useful to consider a slight relaxation of (II.2), in which we

introduce an auxiliary variable x ≈ Yq:

min
q,x

f (q, x)

.
=

1
2

(cid:107)Yq − x(cid:107)2

2 + λ (cid:107)x(cid:107)1 ,

s.t.

(cid:107)q(cid:107)2 = 1.

(III.1)

Here, λ > 0 is a penalty parameter. It is not difﬁcult to see that this problem is equivalent to minimizing the
Huber M-estimator over Yq. This relaxation makes it possible to apply the alternating direction method to this
problem. This method starts from some initial point q(0), alternates between optimizing with respect to (w.r.t.) x
and optimizing w.r.t. q:

where x(k) and q(k) denote the values of x and q in the k-th iteration. Both (III.2) and (III.3) have simple closed
form solutions:

x(k+1) = arg min

q(k+1) = arg min

1
2
1
2

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)Yq(k) − x
(cid:13)
(cid:13)Yq − x(k+1)(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
2

x

q

+ λ (cid:107)x(cid:107)1 ,

s.t. (cid:107)q(cid:107)2 = 1,

x(k+1) = Sλ[Yq(k)],

q(k+1) =

Y(cid:62)x(k+1)
(cid:13)Y(cid:62)x(k+1)(cid:13)
(cid:13)
(cid:13)2

,

(III.2)

(III.3)

(III.4)

where Sλ [x] = sign(x) max {|x| − λ, 0} is the soft-thresholding operator. The proposed ADM algorithm is
summarized in Algorithm 1.

Algorithm 1 Nonconvex ADM for solving (III.1)

A matrix Y ∈ Rp×n with Y(cid:62)Y = I, initialization q(0), threshold parameter λ > 0.

The recovered sparse vector ˆx0 = Yq(k)

Input:
Output:
1: for k = 0, . . . , O (cid:0)n4 log n(cid:1) do
x(k+1) = Sλ[Yq(k)],
2:
q(k+1) = Y(cid:62)x(k+1)
(cid:107)Y(cid:62)x(k+1)(cid:107)2

,

3:
4: end for

The algorithm is simple to state and easy to implement. However, if our goal is to recover the sparsest vector x0,

some additional tricks are needed.
Initialization. Because the problem (II.2) is nonconvex, an arbitrary or random initialization may not produce a
global minimizer.9 In fact, good initializations are critical for the proposed ADM algorithm to succeed in the linear
sparsity regime. For this purpose, we suggest using every normalized row of Y as initializations for q, and solving
a sequence of p nonconvex programs (II.2) by the ADM algorithm.

To get an intuition of why our initialization works, recall the planted sparse model S = span(x0, g1, . . . , gn−1)

and suppose

Y = [x0 | g1 | · · · | gn−1] ∈ Rp×n.

(III.5)
θp(cid:1). Meanwhile, the entries of
If we take a row yi of Y, in which x0(i) is nonzero, then x0(i) = Θ (cid:0)1/
√
p. Hence, when θ is not too large,
g1(i), . . . gn−1(i) are all N (0, 1/p), and so their magnitude have size about 1/
x0(i) will be somewhat bigger than most of the other entries in yi. Put another way, yi is biased towards the ﬁrst
Y ≈ I.10
standard basis vector e1. Now, under our probabilistic model assumptions, Y is very well conditioned: Y
Using the Gram-Schmidt process11, we can ﬁnd an orthonormal basis Y for S via:

√

(cid:62)

9More precisely, in our models, random initialization does work, but only when the subspace dimension n is extremely low compared to

the ambient dimension p.

10This is the common heuristic that “tall random matrices are well conditioned” [42].
11...QR decomposition in general with restriction that R11 = 1.

Y = YR,

(III.6)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

6

where R is upper triangular, and R is itself well-conditioned: R ≈ I. Since the i-th row yi of Y is biased in
the direction of e1 and R is well-conditioned, the i-th row yi of Y is also biased in the direction of e1. In other
words, with this canonical orthobasis Y for the subspace, the i-th row of Y is biased in the direction of the global
optimizer. The heuristic arguments are made rigorous in Appendix B and Appendix D.

What if we are handed some other basis (cid:98)Y = YU, where U is an arbitary orthogonal matrix? Suppose q(cid:63) is a
global optimizer to (II.2) with the input matrix Y, then it is easy to check that, U(cid:62)q(cid:63) is a global optimizer to
(II.2) with the input matrix (cid:98)Y. Because
(cid:68)

(cid:69)

(cid:68)

(cid:69)

(YU)(cid:62)ei, U(cid:62)q(cid:63)

=

Y(cid:62)ei, q(cid:63)

,

our initialization is invariant to any rotation of the orthobasis. Hence, even if we are handed an arbitrary orthobasis
for S, the i-th row is still biased in the direction of the global optimizer.

Rounding by linear programming (LP). Let q denote the output of Algorithm 1. As illustrated in Fig. 1, we
will prove that with our particular initialization and an appropriate choice of λ, ADM algorithm uniformly moves
towards the optimal over a large portion of the sphere, and its solution falls within a certain small radius of the
globally optimal solution q(cid:63) to (II.2). To exactly recover q(cid:63), or equivalently to recover the exact sparse vector
x0 = γYq(cid:63) for some γ (cid:54)= 0, we solve the linear program

min
q

(cid:107)Yq(cid:107)1

s.t.

(cid:104)r, q(cid:105) = 1

(III.7)

with r = q. Since the feasible set {q | (cid:104)q, q(cid:105) = 1} is essentially the tangent space of the sphere Sn−1 at q, whenever
q is close enough to q(cid:63), one should expect that the optimizer of (III.7) exactly recovers q(cid:63) and hence x0 up to
scale. We will prove that this is indeed true under appropriate conditions.

IV. MAIN RESULTS AND SKETCH OF ANALYSIS

A. Main Results

previous section succeeds.

In this section, we describe our main theoretical result, which shows that w.h.p. the algorithm described in the

Theorem IV.1. Suppose that S obeys the planted sparse model, and let the columns of Y form an arbitrary
orthonormal basis for the subspace S. Let y1, . . . , yp ∈ Rn denote the (transposes of) the rows of Y. Apply
(cid:13)2 , . . . , yp/ (cid:107)yp(cid:107)2, to produce outputs q1, . . . , qp.
Algorithm 1 with λ = 1/
Solve the linear program (III.7) with r = q1, . . . , qp, to produce (cid:98)q1, . . . , (cid:98)qp. Set i(cid:63) ∈ arg mini (cid:107)Y(cid:98)qi(cid:107)1. Then

p, using initializations q(0) = y1/ (cid:13)

(cid:13)y1(cid:13)

√

for some γ (cid:54)= 0 with probability at least 1 − cp−2, provided

Y(cid:98)qi(cid:63) = γx0,

p ≥ Cn4 log n,

and

1
√
n

≤ θ ≤ θ0.

Here C, c and θ0 are positive constants.

(IV.1)

(IV.2)

Remark IV.2. We can see that the result in Theorem IV.1 is suboptimal in sample complexity compared to the
global optimality result in Theorem II.1 and Barak et al.’s result [34] (and the subsequent work [35]). For successful
recovery, we require p ≥ Ω (cid:0)n4 log n(cid:1), while the global optimality and Barak et al. demand p ≥ Ω (n) and
p ≥ Ω (cid:0)n2(cid:1), respectively. Aside from possible deﬁciencies in our current analysis, compared to Barak et al., we
believe this is still the ﬁrst practical and efﬁcient method which is guaranteed to achieve θ ∼ Ω(1) rate. The lower
bound on θ in Theorem IV.1 is mostly for convenience in the proof; in fact, the LP rounding stage of our algorithm
already succeeds w.h.p. when θ ∈ O (1/

n).

√

B. A Sketch of Analysis

In this section, we brieﬂy sketch the main ideas of proving our main result in Theorem IV.1, to show that the
“initialization + ADM + LP rounding” pipeline recovers x0 under the stated technical conditions, as illustrated in

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

7

Fig. 1. An illustration of the proof sketch for our ADM algorithm.

Fig. 1. The proof of our main result requires rather detailed technical analysis of the iteration-by-iteration properties
of Algorithm 1, most of which is deferred to the appendices.

As noted in Section III, the ADM algorithm is invariant to change of basis. So w.l.o.g., let us assume Y =

[x0 | g1 | · · · | gn−1] and let Y to be its orthogonalization, i.e., 12

Y =

(cid:20) x0
(cid:107)x0(cid:107)2

(cid:16)

| Px⊥

0

G

G(cid:62)Px⊥

0

G

(cid:17)−1/2(cid:21)

.

When p is large, Y is nearly orthogonal, and hence Y is very close to Y. Thus, in our proofs, whenever convenient,
we make the arguments on Y ﬁrst and then “propagate” the quantitative results onto Y by perturbation arguments.
With that noted, let y1, · · · , yp be the transpose of the rows of Y, and note that these are all independent random
vectors. To prove the result of Theorem IV.1, we need the following results. First, given the speciﬁed Y, we show
that our initialization is biased towards the global optimum:

Proposition IV.3 (Good initialization). Suppose θ > 1/
that at least one of our p initialization vectors suggested in Section III, say q(0)

n and p ≥ Cn. It holds with probability at least 1 − cp−2
i = yi/ (cid:13)

(cid:13)yi(cid:13)

(cid:13)2, obeys

√

Here C, c are positive constants.

Proof: See Appendix D.

Second, we deﬁne a vector-valued random process Q(q) on q ∈ Sn−1, via

so that based on (III.4), one step of the ADM algorithm takes the form:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28) yi

(cid:107)yi(cid:107)2

, e1

≥

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√

.

10

θn

Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

,

1
p

p
(cid:88)

i=1

q(k+1) =

Q (cid:0)q(k)(cid:1)
(cid:13)Q (cid:0)q(k)(cid:1)(cid:13)
(cid:13)
(cid:13)2

(IV.3)

(IV.4)

(IV.5)

(IV.6)

This is a very favorable form for analysis: the term in the numerator Q (cid:0)q(k)(cid:1) is a sum of p independent random
vectors with q(k) viewed as ﬁxed. We study the behavior of the iteration (IV.6) through the random process Q (cid:0)q(k)(cid:1).

12Note that with probability one, the inverse matrix square-root in Y is well deﬁned. So Y is well deﬁned w.h.p. (i.e., except for x0 = 0).

See more quantitative characterization of Y in Appendix B.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

8

We want to show that w.h.p. the ADM iterate sequence q(k) converges to some small neighborhood of ±e1, so
that the ADM algorithm plus the LP rounding (described in Section III) successfully retrieves the sparse vector
x0/(cid:107)x0(cid:107) = Ye1. Thus, we hope that in general, Q(q) is more concentrated on the ﬁrst coordinate than q ∈ Sn−1. Let
us partition the vector q as q = [q1; q2], with q1 ∈ R and q2 ∈ Rn−1; and correspondingly Q(q) = [Q1(q); Q2(q)].
The inner product of Q(q)/ (cid:107)Q(q)(cid:107)2 and e1 is strictly larger than the inner product of q and e1 if and only if

|Q1(q)|
|q1|

>

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

.

In the following proposition, we show that w.h.p., this inequality holds uniformly over a signiﬁcant portion of the
sphere

(cid:26)

.
=

Γ

q ∈ Sn−1 |

1
√

10

nθ

√

≤ |q1| ≤ 3

θ, (cid:107)q2(cid:107)2 ≥

(cid:27)

,

1
10

so the algorithm moves in the correct direction. Let us deﬁne the gap G(q) between the two quantities |Q1(q)| / |q1|
and (cid:107)Q2(q)(cid:107)2 / (cid:107)q2(cid:107)2 as

(IV.7)

(IV.8)

and we show that the following result is true:

G(q)

.
=

|Q1(q)|
|q1|

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

,

Proposition IV.4 (Uniform lower bound for ﬁnite sample gap). There exists a constant θ0 ∈ (0, 1), such that when
p ≥ Cn4 log n, the estimate

G(q) ≥

inf
q∈Γ

1
104θ2np

√

holds with probability at least 1 − cp−2, provided θ ∈ (1/

n, θ0). Here C, c are positive constants.

Proof: See Appendix E.

√

Next, we show that whenever |q1| ≥ 3
enough for LP rounding (III.7) to succeed.

θ, w.h.p. the iterates stay in a “safe region” with |q1| ≥ 2

θ which is

√

Proposition IV.5 (Safe region for rounding). There exists a constant θ0 ∈ (0, 1), such that when p ≥ Cn4 log n, it
holds with probability at least 1 − cp−2 that

|Q1(q)|
(cid:107)Q(q)(cid:107)2

√
θ

≥ 2

√

√

for all q ∈ Sn−1 satisfying |q1| > 3

θ, provided θ ∈ (1/

n, θ0). Here C, c are positive constants.

Proof: See Appendix F.

In addition, the following result shows that the number of iterations for the ADM algorithm to reach the safe

region can be bounded grossly by O(n4 log n) w.h.p..

Proposition IV.6 (Iteration complexity of reaching the safe region). There is a constant θ0 ∈ (0, 1), such that
when p ≥ Cn4 log n, it holds with probability at least 1 − cp−2 that the ADM algorithm in Algorithm 1, with any
(cid:12)
initialization q(0) ∈ Sn−1 satisfying
(cid:12) ≥ 1
(cid:12)
θ at least once in
√
θn
10
√
at most O(n4 log n) iterations, provided θ ∈ (1/

, will produce some iterate q with |¯q1| > 3
n, θ0). Here C, c are positive constants.

(cid:12)
(cid:12)q(0)
(cid:12)

√

1

Proof: See Appendix G.

Moreover, we show that the LP rounding (III.7) with input r = q exactly recovers the optimal solution w.h.p.,

√

whenever the ADM algorithm returns a solution q with ﬁrst coordinate |q1| > 2

θ.

Proposition IV.7 (Success of rounding). There is a constant θ0 ∈ (0, 1), such that when p ≥ Cn, the following
holds with probability at least 1 − cp−2 provided θ ∈ (1/
n, θ0): Suppose the input basis is Y deﬁned in (IV.3)
and the ADM algorithm produces an output q ∈ Sn−1 with |q1| > 2
θ. Then the rounding procedure with r = q
returns the desired solution ±e1. Here C, c are positive constants.

√

√

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

9

Finally, given p ≥ Cn4 log n for a sufﬁciently large constant C, we combine all the results above to complete the

Proof: See Appendix H.

proof of Theorem IV.1.

Proof of Theorem IV.1:

W.l.o.g., let us again ﬁrst consider Y as deﬁned in (III.5) and its orthogonalization Y in a “natural/canonical”
form (IV.3). We show that w.h.p. our algorithmic pipeline described in Section III exactly recovers the optimal
solution up to scale, via the following argument:

1) Good initializers. Proposition IV.3 shows that w.h.p., at least one of the p initialization vectors, say q(0)

i =

yi/ (cid:13)

(cid:13)yi(cid:13)

(cid:13)2, obeys

(cid:12)
(cid:68)
(cid:12)
(cid:12)

q(0)
i

, e1

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥

1
√

,

10

θn

which implies that q(0)

i

is biased towards the global optimal solution.

√

2) Uniform progress away from the equator. By Proposition IV.4, for any θ ∈ (1/

n, θ0) with a constant

θ0 ∈ (0, 1),

w.h.p.,

G(q) =

|Q1(q)|
|q1|
holds uniformly for all q ∈ Sn−1 in the region
(cid:12)
(cid:12) ≥ 1
(cid:12)
q(0) such that
if sufﬁciently many iterations are allowed.

(cid:12)
(cid:12)q(0)
(cid:12)

θn

10

√

1

1
√

10

θn

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2

1
104θ2np

≥

√

, the ADM algorithm will eventually obtain a point q(k) for which (cid:12)

(cid:12)q(k)(cid:12)

(cid:12) ≥ 3

√

θ,

≤ |q1| ≤ 3

θ w.h.p.. This implies that with an input

(IV.9)

3) No jumps away from the caps. Proposition IV.5 shows that for any θ ∈ (1/

n, θ0) with a constant θ0 ∈ (0, 1),

Q1(q)
(cid:107)Q(q)(cid:107)2

√

≥ 2

θ

√

√

√

θ. This implies that once |q(k)
holds for all q ∈ Sn−1 with |q1| ≥ 3
θ for some iterate k, all the future
iterates produced by the ADM algorithm stay in a “spherical cap” region around the optimum with |q1| ≥ 2
θ.
4) Location of stopping points. As shown in Proposition IV.6, w.h.p., the strictly positive gap G(q) in (IV.9)
ensures that one needs to run at most O (cid:0)n4 log n(cid:1) iterations to ﬁrst encounter an iterate q(k) such that
|q(k)
θ. Hence, the steps above imply that, w.h.p., Algorithm 1 fed with the proposed initialization
θ after O (cid:0)n4 log n(cid:1) steps.
scheme successively produces iterates q ∈ Sn−1 with its ﬁrst coordinate |q1| ≥ 2
θ. Proposition IV.7 proves that w.h.p., the LP rounding (III.7) with an

5) Rounding succeeds when |r1| ≥ 2

1 | ≥ 3

1 | ≥ 3

√

√

√

√

input r = q produces the solution ±x0 up to scale.

Taken together, these claims imply that from at least one of the initializers q(0), the ADM algorithm will produce
an output q which is accurate enough for LP rounding to exactly return x0/(cid:107)x0(cid:107)2. On the other hand, our (cid:96)1/(cid:96)2
optimality theorem (Theorem II.1) implies that ±x0 are the unique vectors with the smallest (cid:96)1 norm among all
unit vectors in the subspace. Since w.h.p. x0/(cid:107)x0(cid:107)2 is among the p unit vectors (cid:98)q1, . . . , (cid:98)qp our p row initializers
ﬁnally produce, our minimal (cid:96)1 norm selector will successfully locate x0/(cid:107)x0(cid:107)2 vector.

For the general case when the input is an arbitrary orthonormal basis (cid:98)Y = YU for some orthogonal matrix U,

the target solution is U(cid:62)e1. The following technical pieces are perfectly parallel to the argument above for Y.

1) Discussion at the end of Appendix D implies that w.h.p., at least one row of (cid:98)Y provides an initial point q(0)

such that (cid:12)
(cid:12)

(cid:10)q(0), U(cid:62)e1

(cid:11)(cid:12)
(cid:12) ≥ 1
√
10

.

θn

2) Discussion following Proposition IV.4 in Appendix E indicates that for all q such that

≤ (cid:12)
(cid:10)q, U(cid:62)e1
(cid:12)
θ, there is a strictly positive gap, indicating steady progress towards a point q(k) such that (cid:12)
(cid:10)q(k), U(cid:62)e1
(cid:12)
θ.

√
3
√
3

1
√

θn

10

3) Discussion at the end of Appendix F implies that once q satisﬁes (cid:12)
(cid:12)

(cid:10)q, U(cid:62)e1

(cid:11)(cid:12)
(cid:12), the next iterate will not move

(cid:11)(cid:12)
(cid:12) ≤
(cid:11)(cid:12)
(cid:12) ≥

far away from the target:

(cid:68)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

Q

q; (cid:98)Y

(cid:16)

(cid:17)

(cid:13)
(cid:13)
(cid:13)Q

/

q; (cid:98)Y

(cid:17)(cid:13)
(cid:13)
(cid:13)2

, U(cid:62)e1

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥ 2

√

θ.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

10

4) Repeating the argument in Appendix G for general input (cid:98)Y shows it is enough to run the ADM algorithm
≤ (cid:12)
O (cid:0)n4 log n(cid:1) iterations to cross the range
θ. So the argument above together
(cid:12)
dictates that with the proposed initialization, w.h.p., the ADM algorithm produces an output q that satisﬁes
(cid:12)
(cid:10)q, U(cid:62)e1
(cid:12)

√
5) Since the ADM returns q satisfying (cid:12)
(cid:12)

θ, if we run at least O (cid:0)n4 log n(cid:1) iterations.
(cid:10)q, R(cid:62)e1

θ, discussion at the end of Appendix H implies that we
will obtain a solution q(cid:63) = ±U(cid:62)e1 up to scale as the optimizer of the rounding program, exactly the target
solution.

(cid:10)q, U(cid:62)e1

(cid:11)(cid:12)
(cid:12) ≤ 3

(cid:11)(cid:12)
(cid:12) ≥ 2

(cid:11)(cid:12)
(cid:12) ≥ 2

√

√

1
√

θn

10

Hence, we complete the proof.

Remark IV.8. Under the planted sparse model, in practice the ADM algorithm with the proposed initialization
converges to a global optimizer of (III.1) that correctly recovers x0. In fact, simple calculation shows such desired
point for successful recovery is indeed the only critical point of (III.1) near the pole in Fig. 1. Unfortunately,
using the current analytical framework, we did not succeed in proving such convergence in theory. Proposition IV.5
and IV.6 imply that after O(n4 log n) iterations, however, the ADM sequence will stay in a small neighborhood of
the target. Hence, we proposed to stop after O(n4 log n) steps, and then round the output using the LP that provable
recover the target, as implied by Proposition IV.5 and IV.7. So the LP rounding procedure is for the purpose of
completing the theory, and seems not necessary in practice. We suspect alternative analytical strategies, such as the
geometrical analysis that we will discuss in Section VI, can likely get around the artifact.

V. EXPERIMENTAL RESULTS

In this section, we show the performance of the proposed ADM algorithm on both synthetic and real datasets. On
the synthetic dataset, we show the phase transition of our algorithm on both the planted sparse and the dictionary
learning models; for the real dataset, we demonstrate how seeking sparse vectors can help discover interesting
patterns on face images.

A. Phase Transition on Synthetic Data

For the planted sparse model, for each pair of (k, p), we generate the n dimensional subspace S ⊂ Rp by direct
sum of x0 and G: x0 ∈ Rp is a k-sparse vector with uniformly random support and all nonzero entries equal to 1,
and G ∈ Rp×(n−1) is an i.i.d. Gaussian matrix distributed by N (0, 1/p). So one basis Y of the subspace S can
be constructed by Y = GS ([x0, G]) U, where GS (·) denotes the Gram-Schmidt orthonormalization operator and
U ∈ Rn×n is an arbitrary orthogonal matrix. For each p, we set the regularization parameter in (III.1) as λ = 1/
p,
use all the normalized rows of Y as initializations of q for the proposed ADM algorithm, and run the alternating
steps for 104 iterations. We determine the recovery to be successful whenever (cid:107)x0/ (cid:107)x0(cid:107)2 − Yq(cid:107)2 ≤ 10−2 for at
least one of the p trials (we set the tolerance relatively large as we have shown that LP rounding exactly recovers
the solutions with approximate input). To determine the empirical recovery performance of our ADM algorithm,
ﬁrst we ﬁx the relationship between n and p as p = 5n log n, and plot out the phase transition between k and p.
Next, we ﬁx the sparsity level θ = 0.2 (or k = 0.2p), and plot out the phase transition between p and n. For each
pair of (p, k) or (n, p), we repeat the simulation for 10 times. Fig. 2 shows both phase transition plots.

√

We also experiment with the complete dictionary learning model as in [13] (see also [15]). Speciﬁcally, the
observation is assumed to be Y = A0X0, where A0 is a square, invertible matrix, and X0 a n × p sparse matrix.
Since A0 is invertible, the row space of Y is the same as that of X0. For each pair of (k, n), we generate
X0 = [x1, · · · , xn](cid:62), where each vector xi ∈ Rp is k-sparse with every nonzero entry following i.i.d. Gaussian
distribution, and construct the observation by Y(cid:62) = GS (cid:0)X(cid:62)
(cid:1) U(cid:62). We repeat the same experiment as for the planted
0
sparse model described above. The only difference is that here we determine the recovery to be successful as long
as one sparse row of X0 is recovered by one of those p programs. Fig. 3 shows both phase transition plots.

Fig. 2(a) and Fig. 3(a) suggest our ADM algorithm could work into the linear sparsity regime for both models,
provided p ≥ Ω(n log n). Moreover, for both models, the log n factor seems necessary for working into the linear
sparsity regime, as suggested by Fig. 2(b) and Fig. 3(b): there are clear nonlinear transition boundaries between
success and failure regions. For both models, O(n log n) sample requirement is near optimal: for the planted sparse
model, obviously p ≥ Ω(n) is necessary; for the complete dictionary learning model, [13] proved that p ≥ Ω(n log n)
is required for exact recovery. For the planted sparse model, our result p ≥ Ω(n4 log n) is far from this much

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

11

Fig. 2. Phase transition for the planted sparse model using the ADM algorithm: (a) with ﬁxed relationship between p and n: p = 5n log n;
(b) with ﬁxed relationship between p and k: k = 0.2p. White indicates success and black indicates failure.

Fig. 3. Phase transition for the dictionary learning model using the ADM algorithm: (a) with ﬁxed relationship between p and n: p = 5n log n;
(b) with ﬁxed relationship between p and k: k = 0.2p. White indicates success and black indicates failure.

lower empirical requirement. Fig 2(b) further suggests that alternative reformulation and algorithm are needed to
solve (II.1) so that the optimal recovery guarantee as depicted in Theorem II.1 can be obtained.

B. Exploratory Experiments on Faces

It is well known in computer vision that the collection of images of a convex object only subject to illumination
changes can be well approximated by a low-dimensional subspaces in raw-pixel space [43]. We will play with face
subspaces here. First, we extract face images of one person (65 images) under different illumination conditions. Then
we apply robust principal component analysis [44] to the data and get a low dimensional subspace of dimension 10,
i.e., the basis Y ∈ R32256×10. We apply the ADM + LP algorithm to ﬁnd the sparsest elements in such a subspace,
by randomly selecting 10% rows of Y as initializations for q. We judge the sparsity in the (cid:96)1/(cid:96)2 sense, that is,
the sparsest vector (cid:98)x0 = Yq(cid:63) should produce the smallest (cid:107)Yq(cid:107)1 / (cid:107)Yq(cid:107)2 among all results. Once some sparse
vectors are found, we project the subspace onto orthogonal complement of the sparse vectors already found13, and
continue the seeking process in the projected subspace. Fig. 4(Top) shows the ﬁrst four sparse vectors we get from
the data. We can see they correspond well to different extreme illumination conditions. We also implemented the
spectral method (with the LP post-processing) proposed in [35] for comparison under the same protocol. The result
is presented as Fig. 4(Bottom): the ratios (cid:107)·(cid:107)(cid:96)1 / (cid:107)·(cid:107)(cid:96)2 are signiﬁcantly higher, and the ratios (cid:107)·(cid:107)(cid:96)4 / (cid:107)·(cid:107)(cid:96)2 (this is

13The idea is to build a sparse, orthonormal basis for the subspace in a greedy manner.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

12

the metric to be maximized in [35] to promote sparsity) are signiﬁcantly lower. By these two criteria the spectral
method with LP rounding consistently produces vectors with higher sparsity levels under our evaluation protocol.
Moreover, the resulting images are harder to interpret physically.

Fig. 4. The ﬁrst four sparse vectors extracted for one person in the Yale B database under different illuminations. (Top) by our ADM
algorithm; (Bottom) by the speeding-up SOS algorithm proposed in [35].

Second, we manually select ten different persons’ faces under the normal lighting condition. Again, the dimension
of the subspace is 10 and Y ∈ R32256×10. We repeat the same experiment as stated above. Fig. 5 shows four sparse
vectors we get from the data. Interestingly, the sparse vectors roughly correspond to differences of face images
concentrated around facial parts that different people tend to differ from each other, e.g., eye brows, forehead hair,
nose, etc. By comparison, the vectors returned by the spectral method [35] are relatively denser and the sparsity
patterns in the images are less structured physically.

In sum, our algorithm seems to ﬁnd useful sparse vectors for potential applications, such as peculiarity discovery
in ﬁrst setting, and locating differences in second setting. Nevertheless, the main goal of this experiment is to
invite readers to think about similar pattern discovery problems that might be cast as the problem of seeking sparse
vectors in a subspace. The experiment also demonstrates in a concrete way the practicality of our algorithm, both in
handling data sets of realistic size and in producing meaningful results even beyond the (idealized) planted sparse
model that we adopted for analysis.

VI. CONNECTIONS AND DISCUSSION

For the planted sparse model, there is a substantial performance gap in terms of p-n relationship between the our
optimality theorem (Theorem II.1), empirical simulations, and guarantees we have obtained via efﬁcient algorithm
(Theorem IV.1). More careful and tighter analysis based on decoupling [45] and chaining [46, 47] and geometrical
analysis described below can probably help bridge the gap between our theoretical and empirical results. Matching
the theoretical limit depicted in Theorem II.1 seems to require novel algorithmic ideas. The random models we
assume for the subspace can be extended to other random models, particularly for dictionary learning where all the
bases are sparse (e.g., Bernoulli-Gaussian random model).

This work is part of a recent surge of research efforts on deriving provable and practical nonconvex algorithms
to central problems in modern signal processing and machine learning. These problems include low-rank matrix
recovery/completion [48–56], tensor recovery/decomposition [57–61], phase retrieval [62–65], dictionary learning

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

13

Fig. 5. The ﬁrst four sparse vectors extracted for 10 persons in the Yale B database under normal illuminations. (Top) by our ADM
algorithm; (Bottom) by the speeding-up SOS algorithm proposed in [35].

[15, 36–40], and so on.14 Our approach, like the others, is to start with a carefully chosen, problem-speciﬁc
initialization, and then perform a local analysis of the subsequent iterates to guarantee convergence to a good solution.
In comparison, our subsequent work on complete dictionary learning [15] and generalized phase retrieval [65] has
taken a geometrical approach by characterizing the function landscape and designing efﬁcient algorithm accordingly.
The geometric approach has allowed provable recovery via efﬁcient algorithms, with an arbitrary initialization. The
article [66] summarizes the geometric approach and its applicability to several other problems of interest.

A hybrid of the initialization and the geometric approach discussed above is likely to be a powerful computational
framework. To see it in action for the current planted sparse vector problem, in Fig. 6 we provide the asymptotic
function landscape (i.e., p → ∞) of the Huber loss on the sphere S2 (aka the relaxed formulation we tried to
solve (III.1)). It is clear that with an initialization that is biased towards either the north or the south pole, we are
situated in a region where the gradients are always nonzero and points to the favorable directions such that many
reasonable optimization algorithms can take the gradient information and make steady progress towards the target.
This will probably ease the algorithm development and analysis, and help yield tight performance guarantees.

We provide a very efﬁcient algorithm for ﬁnding a sparse vector in a subspace, with strong guarantee. Our
algorithm is practical for handling large datasets—in the experiment on the face dataset, we successfully extracted
some meaningful features from the human face images. However, the potential of seeking sparse/structured element
in a subspace seems largely unexplored, despite the cases we mentioned at the start. We hope this work could
inspire more application ideas.

ACKNOWLEDGEMENT

JS thanks the Wei Family Private Foundation for their generous support. We thank Cun Mu, IEOR Department of
Columbia University, for helpful discussion and input regarding this work. We thank the anonymous reviewers for
their constructive comments that helped improve the manuscript. This work was partially supported by grants ONR
N00014-13-1-0492, NSF 1343282, NSF 1527809, and funding from the Moore and Sloan Foundations.

14The webpage http://sunju.org/research/nonconvex/ maintained by the second author contains pointers to the growing list of work in this

direction.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

14

Fig. 6. Function landscape of f (q) with θ = 0.4 for n = 3. (Left) f (q) over the sphere S2. Note that near the spherical caps around
the north and south poles, there are no critical points and the gradients are always nonzero; (Right) Projected function landscape by
3 (cid:55)→ R obtained via the reparameterization
projecting the upper hemisphere onto the equatorial plane. Mathematically the function g(w) : e⊥
q(w) = [w; (cid:112)1 − (cid:107)w(cid:107)2]. Corresponding to the left, there is no undesired critical point around 0 within a large radius.

APPENDIX A
TECHNICAL TOOLS AND PRELIMINARIES

In this appendix, we record several lemmas that are useful for our analysis.

Lemma A.1. Let ψ(x) and Ψ(x) to denote the probability density function (pdf) and the cumulative distribution
function (cdf) for the standard normal distribution:

(Standard Normal pdf) ψ(x) =

1
√
2π
1
√
2π
Suppose a random variable X ∼ N (0, σ2), with the pdf fσ(x) = 1

(Standard Normal cdf) Ψ(x) =

(cid:90) x

−∞

σ ψ (cid:0) x

σ

(cid:26)

exp

−

(cid:27)

x2
2
(cid:26)

exp

−

dt,

(cid:27)

t2
2

(cid:1), then for any t2 > t1 we have

(cid:90) t2

t1
(cid:90) t2

t1
(cid:90) t2

t1

fσ(x)dx = Ψ

(cid:19)

(cid:18) t2
σ
(cid:20)

(cid:20)

− Ψ

(cid:19)

(cid:19)

(cid:18) t2
σ
(cid:18) t2
σ

(cid:19)

,

(cid:18) t1
σ

(cid:19)(cid:21)

,

(cid:19)(cid:21)

(cid:18) t1
σ
(cid:18) t1
σ

xfσ(x)dx = −σ

ψ

− ψ

x2fσ(x)dx = σ2

Ψ

− Ψ

− σ

(cid:20)
t2ψ

(cid:19)

(cid:18) t2
σ

− t1ψ

(cid:19)(cid:21)

.

(cid:18) t1
σ

Lemma A.2 (Taylor Expansion of Standard Gaussian cdf and pdf ). Assume ψ(x) and Ψ(x) be deﬁned as above.
There exists some universal constant Cψ > 0 such that for any x0, x ∈ R,

|ψ(x) − [ψ(x0) − x0ψ (x0) (x − x0)]| ≤ Cψ(x − x0)2,
|Ψ(x) − [Ψ(x0) + ψ(x0)(x − x0)]| ≤ Cψ(x − x0)2.

Lemma A.3 (Matrix Induced Norms). For any matrix A ∈ Rp×n, the induced matrix norm from (cid:96)p → (cid:96)q is deﬁned
as

(cid:107)A(cid:107)(cid:96)p→(cid:96)q

.
= sup

(cid:107)x(cid:107)p=1

(cid:107)Ax(cid:107)q .

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

15

In particular, let A = [a1, · · · , an] = (cid:2)a1, · · · , ap(cid:3)(cid:62) , we have

(cid:107)A(cid:107)(cid:96)2→(cid:96)1 = sup
(cid:107)x(cid:107)2=1

p
(cid:88)

k=1

(cid:12)
(cid:12)a(cid:62)
(cid:12)
k x

(cid:12)
(cid:12)
(cid:12) ,

(cid:107)AB(cid:107)(cid:96)p→(cid:96)r ≤ (cid:107)A(cid:107)(cid:96)q→(cid:96)r (cid:107)B(cid:107)(cid:96)p→(cid:96)q ,

(cid:107)A(cid:107)(cid:96)2→(cid:96)∞ = max
1≤k≤p

(cid:13)
(cid:13)

(cid:13)ak(cid:13)
(cid:13)
(cid:13)2

,

and B is any matrix of size compatible with A.
Lemma A.4 (Moments of the Gaussian Random Variable). If X ∼ N (cid:0)0, σ2
X
that

(cid:1), then it holds for all integer m ≥ 1

E [|X|m] = σm

X (m − 1)!!

1m=2k+1 + 1m=2k

≤ σm

X (m − 1)!!, k = (cid:98)m/2(cid:99).

(cid:34)(cid:114) 2
π

(cid:35)

Lemma A.5 (Moments of the χ Random Variable). If X ∼ χ (n), i.e., X = (cid:107)x(cid:107)2 for x ∼ N (0, I), then it holds
for all integer m ≥ 1 that

E [X m] = 2m/2 Γ (m/2 + n/2)

≤ m!! nm/2.

Γ (n/2)

Lemma A.6 (Moments of the χ2 Random Variable). If X ∼ χ2 (n), i.e., X = (cid:107)x(cid:107)2
for all integer m ≥ 1 that

2 for x ∼ N (0, I), then it holds

E [X m] = 2m Γ (m + n/2)

=

Γ (n/2)

(n + 2k − 2) ≤

(2n)m.

m!
2

m
(cid:89)

k=1

Lemma A.7 (Moment-Control Bernstein’s Inequality for Random Variables [67]). Let X1, . . . , Xp be i.i.d. real-valued
random variables. Suppose that there exist some positive numbers R and σ2

X such that

m!
2

m!
2

Let S

.
= 1
p

(cid:80)p

k=1 Xk, then for all t > 0, it holds that

E [|Xk|m] ≤

X Rm−2,
σ2

for all integers m ≥ 2.

P [|S − E [S]| ≥ t] ≤ 2 exp

−

(cid:18)

pt2
X + 2Rt

2σ2

(cid:19)

.

Lemma A.8 (Moment-Control Bernstein’s Inequality for Random Vectors [15]). Let x1, . . . , xp ∈ Rd be i.i.d.
random vectors. Suppose there exist some positive number R and σ2

X such that

Let s = 1
p

(cid:80)p

k=1 xk, then for any t > 0, it holds that

E [(cid:107)xk(cid:107)m

2 ] ≤

X Rm−2,
σ2

for all integers m ≥ 2.

P [(cid:107)s − E [s](cid:107)2 ≥ t] ≤ 2(d + 1) exp

(cid:18)

−

pt2
X + 2Rt

2σ2

(cid:19)

.

Lemma A.9 (Gaussian Concentration Inequality). Let x ∼ N (0, Ip). Let f : Rp (cid:55)→ R be an L-Lipschitz function.
Then we have for all t > 0 that

P [f (X) − Ef (X) ≥ t] ≤ exp

−

(cid:18)

(cid:19)

.

t2
2L2

Lemma A.10 (Bounding Maximum Norm of Gaussian Vector Sequence). Let x1, . . . , xn1 be a sequence of (not
necessarily independent) standard Gaussian vectors in Rn2. It holds that

(cid:20)

P

max
i∈[n1]

(cid:107)xi(cid:107)2 >

√

(cid:21)
n2 + 2(cid:112)2 log(2n1)

≤ (2n1)−3.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

16

Proof: Since the function (cid:107)·(cid:107)2 is 1-Lipschitz, by Gaussian concentration inequality, for any i ∈ [n1], we have

(cid:20)
(cid:107)xi(cid:107)2 −

P

(cid:113)

E (cid:107)xi(cid:107)2

(cid:21)
2 > t

≤ P [(cid:107)xi(cid:107)2 − E (cid:107)xi(cid:107)2 > t] ≤ exp

(cid:19)

(cid:18)

−

t2
2

for all t > 0. Since E (cid:107)xi(cid:107)2

2 = n2, by a simple union bound, we obtain
t2
√
2

(cid:21)
n2 + t

(cid:107)xi(cid:107) >

max
i∈[n1]

≤ exp

−

(cid:18)

P

(cid:20)

(cid:19)

+ log n1

for all t > 0. Taking t = 2(cid:112)2 log(2n1) gives the claimed result.

Corollary A.11. Let Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1). It holds that
(cid:17)
n2 + 2(cid:112)2 log(2n1)

(cid:107)Φx(cid:107)∞ ≤

(cid:16)√

(cid:107)x(cid:107)2

for all x ∈ Rn2,

with probability at least 1 − (2n1)−3.

Proof: Let Φ = (cid:2)φ1, · · · , φn1(cid:3)(cid:62) . Without loss of generality, let us only consider x ∈ Sn2−1, we have
(cid:12)x(cid:62)φi(cid:12)

(cid:13)φi(cid:13)
(cid:13)

(cid:13)2 .

(cid:107)Φx(cid:107)∞ = max
i∈[n1]

(cid:12)
(cid:12) ≤ max
i∈[n1]

(cid:12)
(cid:12)

(A.1)

Invoking Lemma A.10 returns the claimed result.

Lemma A.12 (Covering Number of a Unit Sphere [42]). Let Sn−1 = {x ∈ Rn | (cid:107)x(cid:107)2 = 1} be the unit sphere. For
any ε ∈ (0, 1), there exists some ε cover of Sn−1 w.r.t. the (cid:96)2 norm, denoted as Nε, such that

|Nε| ≤

1 +

(cid:18)

(cid:19)n

2
ε

≤

(cid:19)n

.

(cid:18) 3
ε

Lemma A.13 (Spectrum of Gaussian Matrices, [42]). Let Φ ∈ Rn1×n2 (n1 > n2) contain i.i.d. standard normal
entries. Then for every t ≥ 0, with probability at least 1 − 2 exp (cid:0)−t2/2(cid:1), one has

√

√

n1 −

n2 − t ≤ σmin(Φ) ≤ σmax(Φ) ≤

n1 +

n2 + t.

√

√

Lemma A.14. For any ε ∈ (0, 1), there exists a constant C (ε) > 1, such that provided n1 > C (ε) n2, the random
matrix Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1) obeys

(cid:114) 2
(cid:114) 2
π
π
with probability at least 1 − 2 exp (−c (ε) n1) for some c (ε) > 0.

n1 (cid:107)x(cid:107)2 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + ε)

(1 − ε)

n1 (cid:107)x(cid:107)2

for all x ∈ Rn2,

Geometrically, this lemma roughly corresponds to the well known almost spherical section theorem [68, 69], see

also [70]. A slight variant of this version has been proved in [3], borrowing ideas from [71].

Proof: By homogeneity, it is enough to show that the bounds hold for every x of unit (cid:96)2 norm. For a ﬁxed
n1-Lipschitz, by concentration of

π n1. Note that (cid:107)·(cid:107)1 is

x0 with (cid:107)x0(cid:107)2 = 1, Φx0 ∼ N (0, I). So E (cid:107)Φx(cid:107)1 =
measure for Gaussian vectors in Lemma A.9, we have

(cid:113) 2

√

P [|(cid:107)Φx(cid:107)1 − E [(cid:107)Φx(cid:107)1]| > t] ≤ 2 exp

(cid:19)

(cid:18)

−

t2
2n1

for any t > 0. For a ﬁxed δ ∈ (0, 1), S n2−1 can be covered by a δ-net Nδ with cardinality #Nδ ≤ (1 + 2/δ)n2.
Now consider the event

(cid:40)

.
=

E

(cid:114) 2
π

(1 − δ)

n1 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + δ)

n1 ∀ x ∈ Nδ

.

(cid:114) 2
π

(cid:41)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

17

(cid:114) 2
π

∞
(cid:88)

k=0

∞
(cid:88)

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

A simple application of union bound yields

P [E c] ≤ 2 exp

−

+ n2 log

1 +

(cid:18)

δ2n1
π

(cid:18)

(cid:19)(cid:19)

.

2
δ

Choosing δ small enough such that

then conditioned on E, we can conclude that

(1 − 3δ) (1 − δ)−1 ≥ 1 − ε and (1 + δ) (1 − δ)−1 ≤ 1 + ε,

(1 − ε)

n1 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + ε)

n1 ∀ x ∈ Sn2−1.

(cid:114) 2
π

Indeed, suppose E holds. Then it can easily be seen that any z ∈ Sn2−1 can be written as

z =

λkxk,

with |λk| ≤ δk, xk ∈ Nδ for all k.

Hence we have

Similarly,

(cid:107)Φz(cid:107)1 =

Φ

λkxk

≤

δk (cid:107)Φxk(cid:107)1 ≤ (1 + δ) (1 − δ)−1

(cid:114) 2
π

n1.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

∞
(cid:88)

k=0

(cid:107)Φz(cid:107)1 =

Φ

λkxk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

k=0

(cid:104)

1 − δ − δ (1 + δ) (1 − δ)−1(cid:105) (cid:114) 2

≥

n1 = (1 − 3δ) (1 − δ)−1

π

(cid:114) 2
π

n1.

Hence, the choice of δ above leads to the claimed result. Finally, given n1 > Cn2, to make the probability P [E c]
decaying in n1, it is enough to set C = 2π

(cid:1). This completes the proof.

δ2 log (cid:0)1 + 2

δ

APPENDIX B
THE RANDOM BASIS VS. ITS ORTHONORMALIZED VERSION

In this appendix, we consider the planted sparse model

Y = [x0 | g1 | · · · | gn−1] = [x0 | G] ∈ Rp×n

as deﬁned in (III.5), where

1
√
θp

x0(k) ∼i.i.d.

Ber (θ) ,

g(cid:96) ∼i.i.d. N

0,

,

1 ≤ k ≤ p, 1 ≤ (cid:96) ≤ n − 1.

(B.1)

Recall that one “natural/canonical” orthonormal basis for the subspace spanned by columns of Y is

(cid:19)

1
p

I

(cid:18)

(cid:16)

Y =

(cid:20) x0
(cid:107)x0(cid:107)2

| Px⊥

0

G

G(cid:62)Px⊥

0

G

(cid:17)−1/2(cid:21)

,

G(cid:48)

.
= Px⊥

0

G

(cid:16)

G(cid:62)Px⊥

0

G

(cid:17)−1/2

which is well-deﬁned with high probability as Px⊥

0

G is well-conditioned (proved in Lemma B.2). We write

for convenience. When p is large, Y has nearly orthonormal columns, and so we expect that Y closely approximates
Y. In this section, we make this intuition rigorous. We prove several results that are needed for the proof of
Theorem II.1, and for translating results for Y to results for Y in Appendix E-D.

For any realization of x0, let I = supp(x0) = {i | x0(i) (cid:54)= 0}. By Bernstein’s inequality in Lemma A.7 with
σ2
X = 2θ and R = 1, the event

.
=

E0

(cid:26) 1
2

θp ≤ |I| ≤ 2θp

(cid:27)

(B.2)

(B.3)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

18

holds with probability at least 1 − 2 exp (−θp/16). Moreover, we show the following:

Lemma B.1. When p ≥ Cn and θ > 1/

n, the bound

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

4

2

(cid:115)

≤

5

n log p
θ2p

holds with probability at least 1 − cp−2. Here C, c are positive constants.

(B.4)

Proof: Because E

(cid:104)

(cid:107)x0(cid:107)2
2

(cid:105)

we have

for all t > 0, which implies

= 1, by Bernstein’s inequality in Lemma A.7 with σ2

X = 2/(θp2) and R = 1/(θp),

P

(cid:104)(cid:12)
(cid:12)(cid:107)x0(cid:107)2
(cid:12)

2 − E

(cid:104)

(cid:107)x0(cid:107)2
2

(cid:105)(cid:12)
(cid:105)
(cid:12)
(cid:12) > t

= P

(cid:104)(cid:12)
(cid:12)(cid:107)x0(cid:107)2
(cid:12)

(cid:12)
(cid:105)
(cid:12)
2 − 1
(cid:12) > t

≤ 2 exp

−

(cid:18)

(cid:19)

θpt2
4 + 2t

(cid:20)
|(cid:107)x0(cid:107)2 − 1| >

P

(cid:21)

t
(cid:107)x0(cid:107)2 + 1

= P [|(cid:107)x0(cid:107)2 − 1| ((cid:107)x0(cid:107)2 + 1) > t] ≤ 2 exp

(cid:18)

−

θpt2
4 + 2t

(cid:19)

.

On the intersection with E0, (cid:107)x0(cid:107)2 + 1 ≥ 1√
2

+ 1 ≥ 5/4 and setting t =

(cid:113) n log p

θ2p , we obtain

(cid:34)

(cid:115)

(cid:35)

4
5
Unconditionally, this implies that with probability at least 1 − 2 exp (−pθ/16) − 2 exp (cid:0)−

−(cid:112)np log p

|(cid:107)x0(cid:107)2 − 1| ≥

n log p
θ2p

≤ 2 exp

(cid:12)
(cid:12)
(cid:12) E0

P

(cid:16)

(cid:17)

.

√

np log p(cid:1), we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

|1 − (cid:107)x0(cid:107)2|
(cid:107)x0(cid:107)2

≤

√
4

2

(cid:115)

5

n log p
θ2p

,

as desired.
.
= (cid:0)G(cid:62)Px⊥

Let M

0

G(cid:1)−1/2

. Then G(cid:48) = GM − x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM. We show the following results hold:

Lemma B.2. Provided p ≥ Cn, it holds that

(cid:107)M(cid:107) ≤ 2,

(cid:107)M − I(cid:107) ≤ 4

+ 4

(cid:114) n
p

(cid:115)

log(2p)
p

with probability at least 1 − (2p)−2. Here C is a positive constant.

Proof: First observe that

(cid:16)

(cid:16)

(cid:107)M(cid:107) =

σmin

G(cid:62)Px⊥

0

G

(cid:17)(cid:17)−1/2

= σ−1
min

(cid:0)Px⊥

0

G(cid:1) .

Now suppose B is an orthonormal basis spanning x⊥
as that of B(cid:62)G ∈ R(p−1)×(n−1); in particular,

0 . Then it is not hard to see the spectrum of Px⊥

G is the same

0

σmin

(cid:0)Px⊥

0

G(cid:1) = σmin

(cid:16)

B(cid:62)G

(cid:17)

.

0, 1
Since each entry of G ∼i.i.d. N
p
spectrum results for Gaussian matrices in Lemma A.13 and obtain that

, and B(cid:62) has orthonormal rows, B(cid:62)G ∼i.i.d. N

(cid:16)

(cid:17)

(cid:17)

(cid:16)

0, 1
p

, we can invoke the

(cid:114) p − 1
p

(cid:114) n − 1
p

−

− 2

log (2p)
p

(cid:115)

≤ σmin

(cid:16)

(cid:17)

B(cid:62)G

≤ σmax

(cid:16)

(cid:17)

B(cid:62)G

≤

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

log(2p)
p

(cid:115)

with probability at least 1 − (2p)−2. Thus, when p ≥ C1n for some sufﬁciently large constant C1, by using the

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

19

results above we have

(cid:107)M(cid:107) = σ−1
min

(cid:16)

(cid:17)

B(cid:62)G

=

(cid:32)(cid:114) p − 1

(cid:114) n − 1
p

−

− 2

p

(cid:115)

(cid:33)−1

log (2p)
p

≤ 2,

(cid:107)I − M(cid:107) = max (|σmax (M) − 1| , |σmin (M) − 1|)
(cid:16)

(cid:17)

(cid:16)

= max

≤ max

B(cid:62)G

min

(cid:16)(cid:12)
(cid:12)σ−1
(cid:12)

(cid:32)(cid:114) p − 1


−

p

− 1

(cid:12)
(cid:12)
(cid:12) ,
(cid:114) n − 1
p

(cid:12)
(cid:12)σ−1
(cid:12)

max

(cid:17)

B(cid:62)G

(cid:115)

− 2

log (2p)
p

(cid:17)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:33)−1

− 1, 1 −

(cid:32)(cid:114) p − 1

(cid:114) n − 1
p

+

+ 2

p

(cid:115)

log(2p)
p

(cid:33)−1




(cid:32)







= max

1 −

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

log (2p)
p

(cid:33) (cid:32)(cid:114) p − 1

(cid:114) n − 1
p

−

− 2

(cid:33)−1

,

log (2p)
p

(cid:32)(cid:114) p − 1

− 1 +

p

(cid:114) n − 1
p

+ 2

log(2p)
p

(cid:33) (cid:32)(cid:114) p − 1

(cid:114) n − 1
p

+

+ 2

log(2p)
p

(cid:33)−1




(cid:115)

(cid:115)

(cid:115)

(cid:115)

p

p

(cid:32)

≤ 2

1 −

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

(cid:115)

(cid:33)

log (2p)
p

(cid:115)

≤ 4

+ 4

(cid:114) n
p

log(2p)
p

,

with probability at least 1 − (2p)−2.

Lemma B.3. Let YI be a submatrix of Y whose rows are indexed by the set I. There exists a constant C > 0,
such that when p ≥ Cn and 1/2 > θ > 1/

√

p,

n, the following
√
(cid:13)
(cid:13)Y(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 3
(cid:107)YI(cid:107)(cid:96)2→(cid:96)1 ≤ 7(cid:112)2θp,
(cid:13)G − G(cid:48)(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 4
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 20

√

(cid:13)
(cid:13)YI − YI

(cid:13)Y − Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 20

n + 7(cid:112)log(2p),
(cid:114)

(cid:114)

,

n log p
θ
n log p
θ

hold simultaneously with probability at least 1 − cp−2 for a positive constant c.

Proof: First of all, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

≤

1
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)(cid:96)2→(cid:96)1

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 GM
(cid:13)(cid:96)2→(cid:96)2

=

2
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)1

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

,

where in the last inequality we have applied the fact (cid:107)M(cid:107) ≤ 2 from Lemma B.2. Now x(cid:62)
0, (cid:107)x0(cid:107)2
vectors with each entry distributed as N
p
Lemma A.9, we have

0 G is an i.i.d. Gaussian
θp . So by Gaussian concentration inequality in

, where (cid:107)x0(cid:107)2

2 = |I|

(cid:16)

(cid:17)

2

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

≤ 2 (cid:107)x0(cid:107)2

(cid:115)

log(2p)
p

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

20

with probability at least 1 − c1p−2. On the intersection with E0, this implies
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2(cid:112)2θ log(2p),

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

with probability at least 1 − c2p−2 provided θ > 1/
that when p ≥ C1n,

n. Moreover, when intersected with E0, Lemma A.14 implies

(cid:107)G(cid:107)(cid:96)2→(cid:96)1 ≤

(cid:107)GI(cid:107)(cid:96)2→(cid:96)1 ≤ (cid:112)2θp

n. Hence, by Lemma B.2, when p > C2n,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1
√

√

p,
√

with probability at least 1 − c3p−2 provided θ > 1/
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)1 (cid:107)I − M(cid:107) +

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2
(cid:33)

√

p

≤

(cid:32)
(cid:114) n
4
p

(cid:115)

+ 4

log(2p)
p

(cid:13)Y(cid:13)
(cid:13)

(cid:13)
(cid:13)G(cid:48)
I

(cid:13)
(cid:13)GI − G(cid:48)
I

(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)x0(cid:107)(cid:96)2→(cid:96)1 + (cid:107)G(cid:107)(cid:96)2→(cid:96)1 ≤ (cid:107)x0(cid:107)1 +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)GI(cid:107)(cid:96)2→(cid:96)1 (cid:107)M(cid:107) +
(cid:13)
(cid:13)(cid:96)2→(cid:96)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)GI(cid:107)(cid:96)2→(cid:96)1 (cid:107)I − M(cid:107) +

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2
(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)
2
(cid:33)

GM

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:115)

≤ (cid:112)2θp

(cid:32)
4

(cid:114) n
p

+ 4

log(2p)
p

+ 2(cid:112)2θ log(2p) ≤ 4

n + 7(cid:112)log(2p),

√

√

p ≤ 2(cid:112)θp +

√

√

p ≤ 3

p,

≤ 2(cid:112)2θp + 2(cid:112)2θ log(2p) ≤ 4(cid:112)2θp,

+ 2(cid:112)2θ log(2p) ≤ 4

2θn + 6(cid:112)2θ log(2p),

√

(cid:107)YI(cid:107)(cid:96)2→(cid:96)1 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

+ (cid:13)

(cid:13)G(cid:48)
I

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤
√

(cid:107)x0(cid:107)1
(cid:107)x0(cid:107)2

+ 6(cid:112)2θp ≤ 7(cid:112)2θp

n. Finally, by Lemma B.1 and the results above, we obtain
(cid:114)

(cid:107)x0(cid:107)1 + (cid:13)

(cid:13)G − G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 20

(cid:107)x0(cid:107)1 + (cid:13)

(cid:13)GI − G(cid:48)
I

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 20

n log p
θ

,

(cid:114)

n log p
θ

,

with probability at least 1 − c4p−2 provided θ > 1/
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)x0(cid:107)2
1
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)YI − YI

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤

(cid:13)Y − Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1 −

holding with probability at least 1 − c5p−2.

Lemma B.4. Provided p ≥ Cn and θ > 1/

√

n, the following
(cid:115)

(cid:13)G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ 2

(cid:114) n
p

+ 8

√

2 log(2p)
p

,

21(cid:112)n log(2p)
p
hold simultaneously with probability at least 1 − cp−2 for some constant c > 0.

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤

2 log(2p)

4n
p

+

+

p

8

Proof: First of all, we have when p ≥ C1n, it holds with probability at least 1 − c2p−2 that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)∞

≤

1
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)(cid:96)2→(cid:96)∞

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 GM
(cid:13)(cid:96)2→(cid:96)2

≤

2
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)∞

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

,

where at the last inequality we have applied the fact (cid:107)M(cid:107) ≤ 2 from Lemma B.2. Moreover, from proof of Lemma B.3,
(cid:13)2 ≤ 2(cid:112)log(2p)/p (cid:107)x0(cid:107)2 with probability at least 1 − c3p−2 provided p ≥ C4n. Therefore,
we know that (cid:13)

0 G(cid:13)

(cid:13)x(cid:62)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

21

conditioned on E0, we obtain that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
holds with probability at least 1 − c5p−2 provided θ > 1/

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)∞

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

4 (cid:107)x0(cid:107)∞
(cid:107)x0(cid:107)2

GM

≤

√

(cid:115)

log(2p)
p

≤

4(cid:112)2 log(2p)
√
θp

n. Now by Corollary A.11, we have that

(cid:107)G(cid:107)(cid:96)2→(cid:96)∞ ≤

+ 2

(cid:114) n
p

(cid:115)

2 log(2p)
p

with probability at least 1 − c6p−2. Combining the above estimates and Lemma B.2, we have that with probability
at least 1 − c7p−2

where the last simpliﬁcation is provided that θ > 1/

n and p ≥ C8n for a sufﬁciently large C8. Similarly,

(cid:13)G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ (cid:107)GM(cid:107)(cid:96)2→(cid:96)∞ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

GM

(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
2
(cid:13)
(cid:13)
x0x(cid:62)
(cid:13)
(cid:13)
0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
(cid:13)
2
4(cid:112)2 log(2p)
√
θp

GM

+

≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)∞ (cid:107)M(cid:107) +

(cid:115)

≤ 2

+ 4

(cid:114) n
p

2 log(2p)
p

√

(cid:115)

≤ 2

+ 8

(cid:114) n
p

2 log(2p)
p

,

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)∞ (cid:107)I − M(cid:107) +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(8

GM

(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
2
√
2 + 4)(cid:112)n log(2p)

√
8

√
8

≤

≤

4n
p

4n
p

+

+

p

p

2 log(2p)

2 log(2p)

+

+

p

21(cid:112)n log(2p)
p

,

+

4(cid:112)2 log(2p)
√
θp

completing the proof.

APPENDIX C
PROOF OF (cid:96)1/(cid:96)2 GLOBAL OPTIMALITY

In this appendix, we prove the (cid:96)1/(cid:96)2 global optimality condition in Theorem II.1 of Section II.

Proof of Theorem II.1: We will ﬁrst analyze a canonical version, in which the input orthonormal basis is Y as

deﬁned in (III.6) of Section III:

Let q =

and let I be the support set of x0, we have

(cid:21)

(cid:20)q1
q2

min
q∈Rn

(cid:107)Yq(cid:107)1 ,

s.t. (cid:107)q(cid:107)2 = 1.

(cid:13)G(cid:48)

Iq2

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

I cq2

(cid:13)
(cid:13)1

− (cid:13)

≥ |q1|

(cid:107)Yq(cid:107)1 = (cid:107)YIq(cid:107)1 + (cid:107)YI cq(cid:107)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2
x0
(cid:107)x0(cid:107)2
x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≥ |q1|

≥ |q1|

− (cid:107)GIq2(cid:107)1 − (cid:13)
(cid:13)

(cid:0)GI − G(cid:48)

I

(cid:1) q2

(cid:13)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)
(cid:13)
(cid:0)GI c − G(cid:48)
(cid:13)

I c

(cid:1) q2

(cid:13)
(cid:13)1

− (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

(cid:13)G − G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2 ,

where G and G(cid:48) are deﬁned in (B.1) and (B.2) of Appendix B. By Lemma A.14 and intersecting with E0 deﬁned

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

22

in (B.3), we have that as long as p ≥ C1n,

(cid:107)GIq2(cid:107)1 ≤

(cid:107)q2(cid:107)2 = 2θ

p (cid:107)q2(cid:107)2 for all q2 ∈ Rn−1,

√

(cid:107)GI cq2(cid:107)1 ≥

(cid:107)q2(cid:107)2 =

p (1 − 2θ) (cid:107)q2(cid:107)2 for all q2 ∈ Rn−1,

√

1
2

2θp
√
p
p − 2θp
√
p

1
2

hold with probability at least 1 − c2p−2. Moreover, by Lemma B.3,

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 4

√

n + 7(cid:112)log(2p)
√

holds with probability at least 1 − c3p−2 when p ≥ C4n and θ > 1/
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107)Yq(cid:107)1 ≥ g(q)

+ (cid:107)q2(cid:107)2

.
= |q1|

(cid:18) 1
2

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

√

holds with probability at least 1 − c5p−2. Assuming E0, we observe

n. So we obtain that

p (1 − 2θ) − 2θ

p − 4

√

√

n − 7(cid:112)log(2p)

(cid:19)

Now g(q) is a linear function in |q1| and (cid:107)q2(cid:107)2. Thus, whenever θ is sufﬁciently small and p ≥ C6n such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≤ (cid:112)|I|

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ (cid:112)2θp.

(cid:112)2θp <

√

1
2

p (1 − 2θ) − 2θ

p − 4

√

√

n − 7(cid:112)log(2p),

±e1 are the unique minimizers of g(q) under the constraint q2
g(±e1), and we have

1 + (cid:107)q2(cid:107)2

2 = 1. In this case, because (cid:107)Y(±e1)(cid:107)1 =

(cid:107)Yq(cid:107)1 ≥ g(q) > g(±e1)

for all q (cid:54)= ±e1, ±e1 are the unique minimizers of (cid:107)Yq(cid:107)1 under the spherical constraint. Thus there exists a
universal constant θ0 > 0, such that for all 1/
n ≤ θ ≤ θ0, ±e1 are the only global minimizers of (II.2) if the
input basis is Y.

√

Any other input basis can be written as (cid:98)Y = YU, for some orthogonal matrix U. The program now is written as

which is equivalent to

which is obviously equivalent to the canonical program we analyzed above by a simple change of variable, i.e.,
q

.
= Uq, completing the proof.

min
q∈Rn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Yq
(cid:13)1

,

s.t. (cid:107)q(cid:107)2 = 1,

min
q∈Rn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Yq
(cid:13)1

,

s.t. (cid:107)Uq(cid:107)2 = 1,

APPENDIX D
GOOD INITIALIZATION

In this appendix, we prove Proposition IV.3. We show that the initializations produced by the procedure described

Proof of Proposition IV.3: Our previous calculation has shown that θp/2 ≤ |I| ≤ 2θp with probability at least
n. Let Y = (cid:2)y1, · · · , yp(cid:3)(cid:62) as deﬁned in (III.6). Consider any i ∈ I.

√

in Section III are biased towards the optimal.

1 − c1p−2 provided p ≥ C2n and θ > 1/
Then x0(i) = 1√
θp , and

(cid:10)e1, yi/ (cid:13)

(cid:13)yi(cid:13)
(cid:13)2

(cid:11) =

√

1/

θp

(cid:107)x0(cid:107)2 (cid:107)yi(cid:107)2

(cid:107)x0(cid:107)2 ((cid:107)x0(cid:107)∞ / (cid:107)x0(cid:107)2 + (cid:107)(g(cid:48))i(cid:107)2)

≥

≥

√

1/

θp

√

1/

θp

(cid:107)x0(cid:107)2 ((cid:107)x0(cid:107)∞ / (cid:107)x0(cid:107)2 + (cid:107)gi(cid:107)2 + (cid:107)G − G(cid:48)(cid:107)(cid:96)2→(cid:96)∞)

,

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

23

where gi and (g(cid:48))i are the i-th rows of G and G(cid:48), respectively. Since such gi’s are independent Gaussian vectors
in Rn−1 distributed as N (0, 1/p), by Gaussian concentration inequality and the fact that |I| ≥ pθ/2 w.h.p.,

provided p ≥ C5n and θ > 1/

n. Moreover,

(cid:104)

P

∃i ∈ I : (cid:13)
√

(cid:13)gi(cid:13)

(cid:13)2 ≤ 2(cid:112)n/p

(cid:105)

≥ 1 − exp (−c3nθp) ≤ c4p−2,

(cid:114)

(cid:114)

(cid:107)x0(cid:107)2 =

|I| ×

≤

2θp ×

1
θp

√

=

2.

1
θp

Combining the above estimates and result of Lemma B.4, we obtain that provided p ≥ C6n and θ > 1/
probability at least 1 − c7p−2, there exists an i ∈ [p], such that if we set q(0) = yi/ (cid:13)
(cid:13)2, it holds that

(cid:13)yi(cid:13)

(cid:12)
(cid:12)q(0)
(cid:12)

1

(cid:12)
(cid:12)
(cid:12) ≥

√

2(cid:112)n/p +

√

(cid:16)

2

4n/p + 8

2 log(2p)/p + 21(cid:112)n log(2p)/p

(cid:17)

√

1/

θp
√

√

n, with

(using p ≥ C6n to simpliﬁy the above line)

θp
√
2(cid:112)n/p

√

1/

√

1/

1 + 6

θp + 2
√

1/

θp + 6
1
√

√
2
1
√

2)

(1 + 6
1
√

10

θn

,

≥

=

≥

≥

θn

√

θn

√

(as θ > 1/

n)

completing the proof.

We will next show that for an arbitrary orthonormal basis (cid:98)Y

.
= YU the initialization still biases towards the target
solution. To see this, suppose w.l.o.g. (cid:0)yi(cid:1)(cid:62) is a row of Y with nonzero ﬁrst coordinate. We have shown above that
if Y is the input orthonormal basis. For Y, as x0 = Ye1 = YUU(cid:62)e1,
with high probability
10
we know q(cid:63) = U(cid:62)e1 is the target solution corresponding to (cid:98)Y. Observing that

(cid:69)(cid:12)
(cid:12) ≥ 1
(cid:12)

(cid:68) yi
(cid:107)yi(cid:107)2

, e1

(cid:12)
(cid:12)
(cid:12)

θn

√

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

U(cid:62)e1,

(cid:17)(cid:62)

(cid:16)

e(cid:62)
i (cid:98)Y

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

e(cid:62)
i (cid:98)Y

(cid:17)(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:12)
(cid:12)
(cid:43)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

U(cid:62)e1,

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

U(cid:62)Y(cid:62)ei
(cid:107)U(cid:62)Y(cid:62)ei(cid:107)2

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

e1,

(Y)(cid:62) ei
(cid:107)Y(cid:62)ei(cid:107)2

(cid:43)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

e1,

yi
(cid:107)yi(cid:107)2

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

1
√

,

10

nθ

corroborating our claim.

APPENDIX E
LOWER BOUNDING FINITE SAMPLE GAP G(q)

In this appendix, we prove Proposition IV.4. In particular, we show that the gap G(q) deﬁned in (IV.8) is strictly

positive over a large portion of the sphere Sn−1.

Proof of Proposition IV.4: Without loss of generality, we work with the “canonical” orthonormal basis Y
deﬁned in (III.6). Recall that Y is the orthogonalization of the planted sparse basis Y as deﬁned in (III.5). We
deﬁne the processes Q(q) and Q(q) on q ∈ Sn−1, via

Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

, Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

.

1
p

p
(cid:88)

i=1

1
p

p
(cid:88)

i=1

24

(E.1)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

Thus, we can separate Q(q) as Q(q) =

, where

(cid:21)

(cid:20) Q1(q)
Q2(q)

Q1(q) =

x0iSλ

(cid:104)

q(cid:62)yi(cid:105)

and Q2(q) =

(cid:104)

q(cid:62)yi(cid:105)

,

giSλ

1
p

p
(cid:88)

i=1

1
p

p
(cid:88)

i=1

and separate Q(q) correspondingly. Our task is to lower bound the gap G(q) for ﬁnite samples as deﬁned in (IV.8).
√
Since we can deterministically constrain |q1| and (cid:107)q2(cid:107)2 over the set Γ as deﬁned in (IV.7) (e.g.,
θ
and (cid:107)q2(cid:107)2 ≥ 1
10 for q2 is arbitrary here, as we can always take a sufﬁciently small θ), the
challenge lies in lower bounding |Q1 (q)| and upper bounding (cid:107)Q2 (q)(cid:107)2, which depend on the orthonormal basis
Y. The unnormalized basis Y is much easier to work with than Y. Our proof will follow the observation that
(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)

10 , where the choice of 1

≤ |q1| ≤ 3

1
√

nθ

10

|Q1 (q)| ≥ (cid:12)
(cid:107)Q2 (q)(cid:107) ≤ (cid:13)

(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)

(cid:12) − (cid:12)
(cid:13)2 + (cid:13)

(cid:12) − (cid:12)
(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)

(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12) ,
(cid:13)2 + (cid:13)

(cid:13)Q2 (q) − Q2 (q)(cid:13)

(cid:13)2 .

In particular, we show the following:

• Appendix E-A shows that the expected gap is lower bounded for all q ∈ Sn−1 with |q1| ≤ 3
(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

q2
1
θp

1
50

G (q)

.
=

≥

−

.

√

θ:

As |q1| ≥ 1
√
10

nθ

, we have

• Appendix E-B, as summarized in Proposition E.8, shows that whenever p ≥ Ω (cid:0)n4 log n(cid:1), it holds with high

probability that

(cid:12)
(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|

−

(cid:13)
(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

inf
q∈Γ

≥

1
5000

1
θ2np

.

(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

+

sup
q∈Γ

√

10

θn

≤

4 × 105θ5/2n3/2p

+

10
4 × 105θ2np

(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2
1
2 × 104θ2np

=

.

• Appendix E-D shows that whenever p ≥ Ω (cid:0)n4 log n(cid:1), it holds with high probability that
(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12)
(cid:12)
|q1|

(cid:13)Q2 (q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

sup
q∈Γ

+

√

10

θn

≤

4 × 105θ5/2n3/2p

+

10
4 × 105θ2np

=

1
2 × 104θ2np

.

Observing that

inf
q∈Γ

G(q) ≥ inf
q∈Γ

(cid:32) (cid:12)

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

−

(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|
(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12)
|q1|

(cid:32) (cid:12)

+

− sup
q∈Γ

(cid:13)Q2 (q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

(cid:33)

,

(cid:33)

(cid:32) (cid:12)

(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|

+

(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

(cid:33)

− sup
q∈Γ

we obtain the result as desired.

For the general case when the input orthonormal basis is (cid:98)Y = YU with target solution q(cid:63) = U(cid:62)e1, a

straightforward extension of the deﬁnition for the gap would be:
(cid:13)
(cid:13)
(cid:13)

, U(cid:62)e1

Q

(cid:69)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:68)

(cid:16)

(cid:16)

(cid:17)

G

q; (cid:98)Y = YU

(cid:17) .
=

−

q; (cid:98)Y
|(cid:104)q, U(cid:62)e1(cid:105)|

(cid:16)

(cid:0)I − U(cid:62)e1e(cid:62)

1 U(cid:1) Q
(cid:13)
(cid:0)I − U(cid:62)e1e(cid:62)
(cid:13)

q; (cid:98)Y
1 U(cid:1) q(cid:13)
(cid:13)2

(cid:17)(cid:13)
(cid:13)
(cid:13)2

.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

25

(cid:16)

(cid:17)

Since Q

q; (cid:98)Y

= 1
p

(cid:80)p

k=1 U(cid:62)ykSλ

(cid:0)q(cid:62)U(cid:62)yk(cid:1), we have

(cid:16)

(cid:17)

UQ

q; (cid:98)Y

=

UU(cid:62)ykSλ

(cid:16)

q(cid:62)U(cid:62)yk(cid:17)

=

ykSλ

(cid:104)

(Uq)(cid:62) yk(cid:105)

= Q (Uq; Y) .

(E.2)

1
p

p
(cid:88)

k=1

1
p

p
(cid:88)

k=1

Hence we have

(cid:16)

G

q; (cid:98)Y = YU

=

(cid:17)

|(cid:104)Q (Uq; Y) , e1(cid:105)|
|(cid:104)Uq, e1(cid:105)|

−

(cid:13)
(cid:0)I − e1e(cid:62)
(cid:13)

1

(cid:1) Q (Uq; Y)(cid:13)
(cid:13)2
(cid:1) Uq(cid:13)
(cid:13)2

1

.

(cid:13)
(cid:0)I − e1e(cid:62)
(cid:13)

Therefore, from Proposition IV.4 above, we conclude that under the same technical conditions as therein,

q∈Sn−1:

1
√

10

θn

inf
≤|(cid:104)Uq,e1(cid:105)|≤3

√

θ

(cid:16)

(cid:17)

G

q; (cid:98)Y

≥

1
104θ2np

with high probability.

A. Lower Bounding the Expected Gap G(q)

In this section, we provide a nontrivial lower bound for the gap

G(q) =

(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

−

(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

.

More speciﬁcally, we show that:

for all q ∈ Sn−1 with |q1| ≤ 3

θ.

√

G(q) ≥

1
50

q2
1
θp

Proposition E.1. There exists some numerical constant θ0 > 0, such that for all θ ∈ (0, θ0), it holds that

(E.3)

(E.4)

Estimating the gap G(q) requires delicate estimates for E (cid:2)Q1(q)(cid:3) and E (cid:2)Q2(q)(cid:3). We ﬁrst outline the main

proof in Appendix E-A1, and delay these detailed technical calculations to the subsequent subsections.

1) Sketch of the Proof: W.l.o.g., we only consider the situation that q1 > 0, because the case of q1 < 0 can be

similarly shown by symmetry. By (E.1), we have

E (cid:2)Q1(q)(cid:3) = E
E (cid:2)Q2(q)(cid:3) = E

(cid:104)

(cid:104)

(cid:104)

(cid:105)(cid:105)

,

x0Sλ
(cid:104)

gSλ

x0q1 + q(cid:62)
2 g
(cid:105)(cid:105)

x0q1 + q(cid:62)
2 g

,

where g ∼ N

(cid:16)

(cid:17)
0, 1
p I

, and x0 ∼ 1√

θp Ber(θ). Let us decompose

g = g(cid:107) + g⊥,

with g(cid:107) = P(cid:107)g = q2q(cid:62)
2
(cid:107)q2(cid:107)2
2

g, and g⊥ = (I − P(cid:107))g. In this notation, we have

E (cid:2)Q2(q)(cid:3) = E
= E

(cid:104)

(cid:104)

g(cid:107)Sλ

x0q1 + q(cid:62)

(cid:105)(cid:105)

(cid:104)

+ E

2 g(cid:107)
(cid:105)(cid:105)

(cid:104)

g⊥Sλ
(cid:104)

x0q1 + q(cid:62)

2 g(cid:107)

(cid:105)(cid:105)

(cid:104)

Sλ

x0q1 + q(cid:62)
2 g

(cid:105)(cid:105)

g(cid:107)Sλ
q2
(cid:107)q2(cid:107)2
2

E

=

x0q1 + q(cid:62)
2 g

+ E [g⊥] E
(cid:105)(cid:105)

(cid:104)

q(cid:62)

2 gSλ

x0q1 + q(cid:62)
2 g

,

(cid:104)

(cid:104)

(cid:104)

where we used the facts that q(cid:62)
and E [g⊥] = 0. Let Z

.
= g(cid:62)q2 ∼ N (0, σ2) with σ2 = (cid:107)q2(cid:107)2

2 g(cid:107), g⊥ and g(cid:107) are uncorrelated Gaussian vectors and therefore independent,
2 /p, by partial evaluation of the expectations with

2 g = q(cid:62)

26

(E.5)

(E.6)

(E.7)

(E.8)

(E.9)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

respect to x0, we get

E (cid:2)Q1(q)(cid:3) =

E (cid:2)Q2(q)(cid:3) =

(cid:21)(cid:21)

,

(cid:115)

(cid:20)
Sλ

E

θ
p

θq2
(cid:107)q2(cid:107)2
2

E

(cid:20) q1√
+ Z
θp
(cid:20) q1√
θp

(cid:20)
ZSλ

(cid:21)(cid:21)

+ Z

+

(1 − θ)q2
(cid:107)q2(cid:107)2
2

E [ZSλ [Z]] .

Straightforward integration based on Lemma A.1 gives a explicit form of the expectations as follows

E (cid:2)Q1(q)(cid:3) =

E (cid:2)Q2(q)(cid:3) =

(cid:115)

(cid:26)(cid:20)

θ
p

(cid:16)

αΨ

−

(cid:17)

α
σ

(cid:19)(cid:21)

(cid:20)

(cid:18)

(cid:19)

+ βΨ

+ σ

ψ

(cid:16)

− ψ

−

(cid:17)(cid:21)(cid:27)

,

α
σ

(cid:26) 2 (1 − θ)
p

(cid:18)

(cid:19)

Ψ

−

+

λ
σ

θ
p

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)(cid:27)

q2,

−

β
σ
(cid:18) β
σ

(cid:18) β
σ
(cid:20)

where the scalars α and β are deﬁned as

α =

+ λ,

β =

− λ,

q1√
θp

q1√
θp

and ψ (t) and Ψ (t) are pdf and cdf for standard normal distribution, respectively, as deﬁned in Lemma A.1. Plugging
(E.7) and (E.8) into (E.3), by some simpliﬁcations, we obtain

G(q) =

1
q1

+

(cid:115)

σ
q1

(cid:20)

θ
p
(cid:115)

(cid:16)

(cid:17)

αΨ

(cid:20)

ψ

θ
p

−

α
σ
(cid:18) β
σ

(cid:19)

(cid:16)

− ψ

−

(cid:17)(cid:21)

.

α
σ

+ βΨ

(cid:19)

(cid:18) β
σ

−

2q1√
θp

(cid:18)

(cid:19)(cid:21)

Ψ

−

λ
σ

−

θ
p

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

(cid:19)

(cid:18) β
σ

+ Ψ

− 2Ψ

−

(cid:18)

(cid:19)(cid:21)

λ
σ

With λ = 1/

p and σ2 = (cid:107)q2(cid:107)2

√

2 /p = (1 − q2
δ + 1
(cid:112)1 − q2

,

1)/p, we have
β
σ

=

= −

1

δ − 1
(cid:112)1 − q2

1

,

λ
σ

=

1
(cid:112)1 − q2

1

,

−

α
σ
√

√

θ for q1 ≤ 3

where δ = q1/
More speciﬁcally, we approximate Ψ (cid:0)− α
(cid:1) and ψ (cid:0)− α
σ
σ
−1 + δ. Applying the estimates for the relevant quantities established in Lemma E.2, we obtain

θ. To proceed, it is natural to consider estimating the gap G(q) by Taylor’s expansion.
(cid:1) around −1 − δ, and approximate Ψ
around

and ψ

(cid:16) β
σ

(cid:16) β
σ

(cid:17)

(cid:17)

G(q) ≥

Φ1(δ) −

Φ2(δ) +

1
δp

1 − θ
p

ψ(−1)q2

1 +

(cid:18)

√

σ

p +

(cid:19)

− 1

θ
2

1
p

(cid:2)1 + δ2 − θδ2 − σ (cid:0)1 + δ2(cid:1) √

p(cid:3) q2

1η1 (δ) +

η1 (δ) −

σ
√

δ

p

1 − θ
p

+

1
2δp

η2(δ)q2
1
√

θq3
1

5CT

p

(δ + 1)3 ,

where we deﬁne

Φ1(δ) = Ψ(−1 − δ) + Ψ(−1 + δ) − 2Ψ(−1),
η1(δ) = ψ(−1 + δ) − ψ(−1 − δ),

Φ2(δ) = Ψ(−1 + δ) − Ψ(−1 − δ),
η2(δ) = ψ(−1 + δ) + ψ(−1 − δ),

and CT is as deﬁned in Lemma E.2. Since 1 − σ
2p η2(δ), and (cid:0)1 + δ2(cid:1) (cid:0)1 − σ
θq2
1

p(cid:1) q2

√

1η1 (δ) / (2δp), and using the fact that δ = q1/

√

p ≥ 0, dropping those small positive terms q2

1

p (1 − θ)ψ(−1),

G(q) ≥

Φ1(δ) −

[Φ2(δ) − σ

pη1(δ)] −

(1 − σ

√

1 − θ
p
1 − θ
p

1
δp
1
δp

≥

Φ1(δ) −

[Φ2(δ) − η1(δ)] −

q2
1
p
η1 (δ)
δ

q2
1
p

√

p) η2(δ) −
(cid:18) 2θ
√
2π

+

2

√

θ
2p
3θ2
√
2π

−

q2
1
θp

q3
1η1 (δ) −
(cid:19)

+ C1θ2

,

√

θ, we obtain
√

C1

θq3
1

p

max

(cid:18) q3
1
θ3/2

(cid:19)

, 1

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

27

1 ≥
1 to simplify the expression. Substituting the estimates in Lemma E.4 and use the fact δ (cid:55)→ η1 (δ) /δ is bounded,

for some constant C1 > 0, where we have used q1 ≤ 3
1 − q2
we obtain

θ to simplify the bounds and the fact σ

√

p = (cid:112)1 − q2

√

G (p) ≥

(cid:18) 1
40
(cid:18) 1
40

1
p
q2
1
θp

−

θ

1
√
2π
1
√
2π

−

≥

(cid:19)

δ2 −

q2
1
θp

(cid:0)c1θ + c2θ2(cid:1)
(cid:19)

θ − c1θ − c2θ2

for some positive constants c1 and c2. We obtain the claimed result once θ0 is made sufﬁciently small.

θ. There exists some universal constant CT > 0 such that we have the follow polynomial

2) Auxiliary Results Used in the Proof:

√

.
Lemma E.2. Let δ
= q1/
approximations hold for all q1 ∈ (cid:0)0, 1
2
(cid:17)
α
σ
(cid:18) β
σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

ψ

ψ

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:1):

(cid:20)

−

1 −

(1 + δ)2q2
1

(cid:19)

(cid:20)

−

1 −

(δ − 1)2q2
1

≤ CT (1 + δ)2 q4
1,

≤ CT (δ − 1)2 q4
1,

1
2

1
2
1
2

1
2

(cid:21)

(cid:21)

(cid:12)
(cid:12)
ψ(−1 − δ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ψ(δ − 1)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

ψ(−1)q2
1

1
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

Ψ

−

(cid:17)

α
σ

(cid:20)

(cid:19)

(cid:20)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ψ

(cid:18) β
σ

−

Ψ(−1 − δ) −

ψ(−1 − δ)(1 + δ)q2
1

≤ CT (1 + δ)2 q4
1,

−

Ψ(δ − 1) +

ψ(δ − 1)(δ − 1)q2
1

≤ CT (δ − 1)2 q4
1,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:19)

(cid:20)

Ψ

−

−

Ψ(−1) −

λ
σ

≤ CT q4
1.

Proof: First observe that for any q1 ∈ (cid:0)0, 1

(cid:1) it holds that

2

0 ≤

1
(cid:112)1 − q2

1

(cid:18)

−

1 +

(cid:19)

q2
1
2

≤ q4
1.

Hence we have

So we have

(δ − 1)

1 +

≤ (δ − 1)

1 +

, when δ ≥ 1

−(1 + δ)

1 +

1 + q4
q2
1

≤ −

≤ −(1 + δ)

1 +

(cid:18)

(cid:18)

1
2
(cid:18)

1
2

(cid:19)

(cid:19)

(cid:19)

1
2

q2
1

α
σ

≤

≤

β
σ
β
σ

(cid:19)(cid:19)

(cid:18)

(cid:18)

(cid:19)

1
2

q2
1

,

(cid:19)

1 + q4
q2
1
(cid:19)

q2
1

(cid:18)

1
2
1
2

(cid:18)

(δ − 1)

1 +

1 + q4
q2
1

≤ (δ − 1)

1 +

, when δ ≤ 1.

(cid:18)

(cid:18)

ψ

−(1 + δ)

1 +

1
2

1 + q4
q2
1

(cid:16)

≤ ψ

−

(cid:17)

α
σ

≤ ψ

−(1 + δ)

1 +

(cid:18)

(cid:19)(cid:19)

.

1
2

q2
1

By Taylor expansion of the left and right sides of the above two-side inequality around −1 − δ using Lemma A.2,
we obtain

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

ψ

−

(cid:17)

α
σ

− ψ(−1 − δ) −

(1 + δ)2q2

≤ CT (1 + δ)2 q4
1,

1
2

(cid:12)
(cid:12)
1ψ(−1 − δ)
(cid:12)
(cid:12)

for some numerical constant CT > 0 sufﬁciently large. In the same way, we can obtain other claimed results.

Lemma E.3. For any δ ∈ [0, 3], it holds that

Φ2(δ) − η1(δ) ≥

η1 (3)
9

δ3 ≥

1
20

δ3.

(E.10)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

28

Proof: Let us deﬁne

h(δ) = Φ2(δ) − η1(δ) − Cδ3

for some C > 0 to be determined later. Then it is obvious that h(0) = 0. Direct calculation shows that

d
dδ

d
dδ

d
dδ

Φ1(δ) = η1(δ),

Φ2(δ) = η2(δ),

η1(δ) = η2(δ) − δη1(δ).

(E.11)

Thus, to show (E.10), it is sufﬁcient to show that h(cid:48)(δ) ≥ 0 for all δ ∈ [0, 3]. By differentiating h(δ) with respect to
δ and use the results in (E.11), it is sufﬁcient to have

h(cid:48)(δ) = δη1(δ) − 3Cδ2 ≥ 0 ⇐⇒ η1(δ) ≥ 3Cδ

for all δ ∈ [0, 3]. We obtain the claimed result by observing that δ (cid:55)→ η1 (δ) /3δ is monotonically decreasing over
δ ∈ [0, 3] as justiﬁed below.
Consider the function

To show it is monotonically decreasing, it is enough to show p(cid:48) (δ) is always nonpositive for δ ∈ (0, 3), or equivalently

p (δ)

.
=

η1 (δ)
3δ

=

1
√
2π

3

(cid:18)

exp

−

δ2 + 1
2

(cid:19) eδ − e−δ
δ

.

g (δ)

.
=

(cid:16)

eδ + e−δ(cid:17)

δ − (cid:0)δ2 + 1(cid:1) (cid:16)

eδ − e−δ(cid:17)

≤ 0

for all δ ∈ (0, 3), which can be easily veriﬁed by noticing that g (0) = 0 and g(cid:48) (δ) ≤ 0 for all δ ≥ 0.

Lemma E.4. For any δ ∈ [0, 3], we have

(1 − θ)Φ1(δ) −

[Φ2(δ) − η1(δ)] ≥

1
δ

(cid:18) 1
40

−

1
√
2π

θ

(cid:19)

δ2.

(E.12)

Proof: Let us deﬁne

g(δ) = (1 − θ)Φ1(δ) −

[Φ2(δ) − η1(δ)] − c0 (θ) δ2,

1
δ

where c0 (θ) > 0 is a function of θ. Thus, by the results in (E.11) and L’Hospital’s rule, we have

Φ2(δ)
δ

lim
δ→0

= lim
δ→0

η2 (δ) = 2ψ(−1),

[η2(δ) − δη1(δ)] = 2ψ(−1).

η1(δ)
δ

lim
δ→0

= lim
δ→0

Combined that with the fact that Φ1(0) = 0, we conclude g (0) = 0. Hence, to show (E.12), it is sufﬁcient to show
that g(cid:48)(δ) ≥ 0 for all δ ∈ [0, 3]. Direct calculation using the results in (E.11) shows that

Since η1 (δ) /δ is monotonically decreasing as shown in Lemma E.3, we have that for all δ ∈ (0, 3)

g(cid:48)(δ) =

1
δ2 [Φ2(δ) − η1(δ)] − θη1(δ) − 2c0 (θ) δ.

Using the above bound and the main result from Lemma E.3 again, we obtain

η1 (δ) ≤ δ lim
δ→0

η (δ)
δ

≤

2
√
2π

δ.

g(cid:48)(δ) ≥

1
20

δ −

2
√
2π

θδ − 2c0δ.

Choosing c0 (θ) = 1

40 − 1√

2π

θ completes the proof.

B. Finite Sample Concentration

In the following two subsections, we estimate the deviations around the expectations E (cid:2)Q1(q)(cid:3) and E (cid:2)Q2(q)(cid:3),
i.e., (cid:12)
(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:13)2, and show that the total deviations ﬁt into the gap G(q) we
derived in Appendix E-A. Our analysis is based on the scalar and vector Bernstein’s inequalities with moment
conditions. Finally, in Appendix E-C, we uniformize the bound by applying the classical discretization argument.

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:12) and (cid:13)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

29

1) Concentration for Q1(q):

Lemma E.5 (Bounding (cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12)). For each q ∈ Sn−1, it holds for all t > 0 that
(cid:18)

(cid:19)

P (cid:2)(cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) ≥ t(cid:3) ≤ 2 exp

−

θp3t2
8 + 4pt

.

Proof: By (E.1), we know that

Q1(q) =

X 1

k , X 1

k = x0(k)Sλ [x0(k)q1 + Zk]

1
p

p
(cid:88)

k=1

where Zk = q(cid:62)

2 gk ∼ N

(cid:16)

0, (cid:107)q2(cid:107)2
p

2

(cid:17)

. Thus, for any m ≥ 2, by Lemma A.4, we have

E (cid:2)(cid:12)

(cid:12)X 1
k

(cid:12)
(cid:12)

m(cid:3) ≤ θ

E

E

(cid:19)l

m(cid:21)

(cid:19)m

(cid:19)m m
(cid:88)

(cid:20)(cid:12)
(cid:12)
(cid:18) 1
q1√
(cid:12)
(cid:12)
√
+ Zk
(cid:12)
(cid:12)
θp
θp
(cid:12)
(cid:12)
(cid:19) (cid:18) q1√
(cid:18)m
(cid:18) 1
√
l
θp
θp
(cid:18) 1
(cid:19) (cid:18) q1√
(cid:18)m
√
l
θp
θp
(cid:19)m (cid:18) q1√
(cid:18) 1
(cid:107)q2(cid:107)2√
√
p
θp
θp
(cid:19)m−2
(cid:19)m

l=0
(cid:19)m m
(cid:88)

(cid:19)l

l=0

+

θ

(cid:19)m

= θ

= θ

≤

m!
2

(cid:104)

|Zk|m−l(cid:105)

(m − l − 1)!!

(cid:19)m−l

(cid:18) (cid:107)q2(cid:107)2√

p

(cid:18) 2
θp
X = 4/(θp2) and R = 2/(θp), apply Lemma A.7, we get

(cid:18) 2
θp

4
θp2

m!
2

m!
2

≤

=

θ

let σ2

P (cid:2)(cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) ≥ t(cid:3) ≤ 2 exp

(cid:18)

−

θp3t2
8 + 4pt

(cid:19)

.

as desired.

2) Concentration for Q2(q):

Lemma E.6 (Bounding (cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

P (cid:2)(cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2). For each q ∈ Sn−1, it holds for all t > 0 that
(cid:18)
(cid:13)2 > t(cid:3) ≤ 2(n + 1) exp

θp3t2
√
128n + 16

θnpt

−

(cid:19)

.

Before proving Lemma E.6, we record the following useful results.

Lemma E.7. For any positive integer s, l > 0, we have

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

s
(cid:13)
(cid:13)
2

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

≤

(l + s)!!
2

(cid:107)q2(cid:107)l
2

(2
(cid:0)√

√
n)s
p(cid:1)s+l .

In particular, when s = l, we have

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

l
(cid:13)
(cid:13)
2

(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

l!
2

(cid:107)q2(cid:107)l
2

√

(cid:18) 4

(cid:19)l

n

p

≤

(cid:17)

2

Proof: Let Pq(cid:107)

= q2q(cid:62)
2
(cid:107)q2(cid:107)2
2
complement, respectively. By Lemma A.4, we have
gk(cid:13)
(cid:13)
(cid:13)2

(cid:20)(cid:16)(cid:13)
(cid:13)
(cid:13)Pq(cid:107)

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

and Pq⊥

I − 1

≤ E

s
(cid:13)
(cid:13)
2

(cid:107)q2(cid:107)2
2

l(cid:21)

=

E

2

2

q2q(cid:62)
2

(cid:16)

+

(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)2

2 gk(cid:12)
(cid:17)s (cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

denote the projection operators onto q2 and its orthogonal

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

30

Using Lemma A.5 and the fact that (cid:13)

(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)2

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

s
(cid:13)
(cid:13)
2

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

≤ (cid:107)q2(cid:107)l
2

=

=

s
(cid:88)

i=0
s
(cid:88)

i=0

(cid:18)s
i
(cid:18)s
i

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)

(cid:21)

i

(cid:21)

i

2

2

E

E

≤ (cid:107)q2(cid:107)l
2

s
(cid:88)

i=0

(cid:21)

s−i

l (cid:13)
(cid:13)
(cid:13)Pq(cid:107)

2

gk(cid:13)
(cid:13)
(cid:13)
2

l+s−i(cid:21)

1
(cid:107)q2(cid:107)s−i
2

(cid:19)l+s−i

(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:21) (cid:18) 1
√
p

(l + s − i − 1)!!.

2

i

2

E

(cid:19)

(cid:19)i

s
(cid:88)

gk(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

(cid:13)2, we obtain
(cid:19) (cid:18) √
√

(cid:18)s
i
(cid:13)gk(cid:13)
≤ (cid:13)
(cid:18)s
i
(cid:19)l (l + s)!!
2
√
n)s
p(cid:1)s+l .

i=0
(cid:18) 1
√
p

(cid:107)q2(cid:107)l
2

(2
(cid:0)√

n
p

i!!

≤ (cid:107)q2(cid:107)l
2

≤

(l + s)!!
2

(l + s − i − 1)!!

(cid:19)l+s−i

(cid:18) 1
√
p

(cid:18) √
√

n
p

+

1
√
p

(cid:19)s

Now, we are ready to prove Lemma E.6,

Proof: By (E.1), note that

Q2 =

X2

k, X2

k = gkSλ [x0(k)q1 + Zk]

1
p

p
(cid:88)

k=1

where Zk = q(cid:62)

E (cid:2)(cid:13)

(cid:13)X2
k

(cid:13)
(cid:13)

m
2

2 gk. Thus, for any m ≥ 2, by Lemma E.7, we have
(cid:104)(cid:13)
(cid:13)

+ (1 − θ)E

(cid:3) ≤ θE

+ q(cid:62)

m(cid:21)

m

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

m

(cid:13)
(cid:13)

l (cid:13)
(cid:13)

2 gk
(cid:13)gk(cid:13)
(cid:19) (m + l)!!
2
q1√
θp

+

p

m

(cid:13)gk(cid:13)

(cid:13)
(cid:13)
2

m(cid:105)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:13)gk(cid:13)
(cid:104)(cid:13)
(cid:13)

(cid:13)
(cid:13)
2

m

m−l

q1√
θp

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (1 − θ)E

(cid:18) (cid:107)q2(cid:107)2√

p

(cid:19)l (cid:12)
(cid:12)
(cid:12)
(cid:12)

q1√
θp

(cid:12)
(cid:12)
(cid:12)
(cid:12)

m−l

(cid:19)m

+ (1 − θ)

m!
2

(cid:107)q2(cid:107)m
2

(cid:18) 4

√

n

p

m!
2
(cid:19)m

m(cid:105)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

+ (1 − θ)

(cid:107)q2(cid:107)m
2

√

(cid:18) 4

(cid:19)m

n

p

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:19)
(cid:18)m
l

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

m
(cid:88)

q1√
θp
(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

(cid:18)m
l

l=0

(cid:19)m (cid:18) (cid:107)q2(cid:107)2√

≤ θ

≤ θ

≤ θ

≤

(cid:19)m m
(cid:88)

l=0
√
(cid:18) 2
√

n
p
√
(cid:18) 4
√
√
(cid:18) 8
√

n
p
(cid:19)m
n
θp

m!
2
m!
2

.

√

√

Taking σ2

X = 64n/(θp2) and R = 8

n/(

θp) and using vector Bernstein’s inequality in Lemma A.8, we obtain

P (cid:2)(cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 ≥ t(cid:3) ≤ 2(n + 1) exp

(cid:18)

−

θp3t2
√
128n + 16

θnpt

(cid:19)

,

as desired.

C. Union Bound
Proposition E.8 (Uniformizing the Bounds). Suppose that θ > 1/
C (ξ), such that whenever p ≥ C (ξ) n4 log n, we have

√

n. Given any ξ > 0, there exists some constant

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤

,

2ξ
θ5/2n3/2p
2ξ
θ2np

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

31

hold uniformly for all q ∈ Sn−1, with probability at least 1 − c(ξ)p−2 for a positive constant c(ξ).

Proof: We apply the standard covering argument. For any ε ∈ (0, 1), by Lemma A.12, the unit hemisphere of

interest can be covered by an ε-net Nε of cardinality at most (3/ε)n. For any q ∈ Sn−1, it can be written as

, which is an independent copy of y = [x0, g](cid:62).

q = q(cid:48) + e

where q(cid:48) ∈ Nε and (cid:107)e(cid:107)2 ≤ ε. Let a row of Y be yk = (cid:2)x0(k), gk(cid:3)(cid:62)
By (E.1), we have
(cid:12)
(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)
(cid:110)

(cid:69)(cid:105)

(cid:104)(cid:68)

(cid:104)(cid:68)

(cid:104)

yk, q(cid:48) + e

− E

x0(k)Sλ

yk, q(cid:48) + e

x0(k)Sλ

(cid:69)(cid:105)(cid:105)(cid:111)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

p
(cid:88)

k=1

1
p

p
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
p
(cid:12)
(cid:12)E (cid:2)x0Sλ

k=1
p
(cid:88)

k=1

=

≤

+ (cid:12)

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48) + e

(cid:69)(cid:105)

−

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48)(cid:69)(cid:105)

+

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48)(cid:69)(cid:105)

− E (cid:2)x0Sλ

(cid:2)(cid:10)y, q(cid:48)(cid:11)(cid:3)(cid:3)

p
(cid:88)

1
p

(cid:2)(cid:10)y, q(cid:48)(cid:11)(cid:3)(cid:3) − E (cid:2)x0Sλ

k=1
(cid:2)(cid:10)y, q(cid:48) + e(cid:11)(cid:3)(cid:3)(cid:12)
(cid:12) .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Using Cauchy-Schwarz inequality and the fact that Sλ [·] is a nonexpansive operator, we have

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ (cid:12)

(cid:12)Q1(q(cid:48)) − E (cid:2)Q1(q(cid:48))(cid:3)(cid:12)

(cid:12) +

|x0(k)|

p
(cid:88)

(cid:32)

1
p

k=1
1
√
θp

(cid:33)

(cid:107)e(cid:107)2

(cid:13)
(cid:13)

(cid:13)yk(cid:13)
+ E [|x0| (cid:107)y(cid:107)2]
(cid:13)
(cid:13)2
(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)

≤ (cid:12)

(cid:12)Q1(q(cid:48)) − E (cid:2)Q1(q(cid:48))(cid:3)(cid:12)

(cid:18) 2
√
θp
(cid:13)2 ≤ (cid:112)n/p + 2(cid:112)2 log(2p)/p with probability at least 1 − c1p−3. Also E [(cid:107)g(cid:107)2] ≤
≤ (cid:112)n/p. Taking t = ξθ−5/2n−3/2p−1 in Lemma E.5 and applying a union bound with ε =

+ E [(cid:107)g(cid:107)2]

+ max
k∈[p]

(cid:13)gk(cid:13)
(cid:13)

(cid:12) + ε

(cid:19)

.

By Lemma A.10, maxk∈[p]
(cid:105)(cid:17)1/2
(cid:16)

(cid:104)

E

(cid:107)g(cid:107)2
2

ξθ−2n−2(log 2p)−1/2/7, and combining with the above estimates, we obtain that

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

ξ
θ5/2n3/2p

+

ξ
7θ5/2n2(cid:112)log(2p)p

√

(cid:16)
4

(cid:17)
n + 2(cid:112)2 log(2p)

≤

2ξ
θ5/2n3/2p

holds for all q ∈ Sn−1, with probability at least

1 − c1p−3 − 2 exp (cid:0)−c3 (ξ) p/(θ4n3) + c4 (ξ) n log n + c5(ξ)n log log(2p)(cid:1) .

Similarly, by (E.1), we have

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 =

(cid:104)(cid:68)

gkSλ

yk, q(cid:48) + e

− E (cid:2)gSλ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
p

p
(cid:88)

(cid:110)

k=1

(cid:69)(cid:105)

(cid:32)

(cid:13)
(cid:13)
(cid:2)(cid:10)y, q(cid:48) + e(cid:11)(cid:3)(cid:3)(cid:111)
(cid:13)
(cid:13)
(cid:13)2

≤ (cid:13)

(cid:13)Q2(q(cid:48)) − E (cid:2)Q2(q(cid:48))(cid:3)(cid:13)

≤ (cid:13)

(cid:13)Q2(q(cid:48)) − E (cid:2)Q2(q(cid:48))(cid:3)(cid:13)

(cid:33)

p
(cid:88)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

k=1

(cid:13)2 +

1
p
(cid:20)

+ E [(cid:107)g(cid:107)2 (cid:107)y(cid:107)2]

(cid:13)yk(cid:13)
(cid:13)
(cid:13)2
(cid:18) 1
√
θp
(cid:13)2, and taking t = ξθ−2n−1p−1 in Lemma E.6 and applying a union

(cid:13)gk(cid:13)
(cid:13)
(cid:13)2
(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

(cid:107)e(cid:107)2
√
√

(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

+ max
k∈[p]

(cid:13)2 + ε

max
k∈[p]

n
θp

n
p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

+

(cid:19)

(cid:21)

.

Applying the above estimates for maxk∈[p]
bound with ε = ξθ−2n−2 log−1(2p)/30, we obtain that

(cid:13)gk(cid:13)
(cid:13)

(cid:13)
(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 ≤

ξ
θ2np

ξ
θ2np

+

+

≤

ξ
30θ2n2 log(2p)

ξ
30θ2n2 log(2p)

+

(cid:32)(cid:114) n
p



4

(cid:26) 16 log(2p)
p

(cid:27)

+

10n
p

(cid:115)

2 log(2p)
p

(cid:33)2

+

2n
p






IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

32

≤

2ξ
θ2np

holds for all q ∈ Sn−1, with probability at least

1 − c1p−3 − exp (cid:0)−c6 (ξ) p/(θ3n3) + c7(ξ)n log n + c8(ξ)n log log(2p)(cid:1) .

Taking p ≥ C9(ξ)n4 log n and simplifying the probability terms complete the proof.

D. Q(q) approximates Q(q)

Proposition E.9. Suppose θ > 1/
p ≥ C (ξ) n4 log n, the following bounds

√

n. For any ξ > 0, there exists some constant C (ξ), such that whenever

(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)

(cid:12) ≤

sup
q∈Sn−1

sup
q∈Sn−1

(cid:13)
(cid:13)Q2(q) − Q2(q)(cid:13)

(cid:13)2 ≤

ξ
θ5/2n3/2p

ξ
θ2np

,

(E.13)

(E.14)

hold with probability at least 1 − c(ξ)p−2 for a positive constant c(ξ).

Proof: First, for any q ∈ Sn−1, from (E.1), we know that
(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)
(cid:12)
p
(cid:88)

p
(cid:88)

(cid:104)

(cid:104)

x0(k)Sλ

q(cid:62)yk(cid:105)

−

q(cid:62)yk(cid:105)

x0(k)
(cid:107)x0(cid:107)2

Sλ

=

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

1
p

1
p

k=1
p
(cid:88)

k=1
p
(cid:88)

k=1

1
p

1
p

k=1
p
(cid:88)

k=1
(cid:104)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

p
(cid:88)

k=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|x0(k)|

1 −

1
p

p
(cid:88)

k=1

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

−

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

+

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

−

|x0(k)|

(cid:12)
(cid:12)
(cid:12)Sλ

(cid:104)

q(cid:62)yk(cid:105)

− Sλ

q(cid:62)yk(cid:105)(cid:12)
(cid:12)
(cid:12) +

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)Sλ

(cid:104)

q(cid:62)yk(cid:105)(cid:12)
(cid:12)
(cid:12) .

1
p

p
(cid:88)

k=1

x0(k)
(cid:107)x0(cid:107)2

(cid:104)

q(cid:62)yk(cid:105)

Sλ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

For any I = supp(x0), using the fact that Sλ[·] is a nonexpansive operator, we have

(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)

(cid:12) ≤

sup
q∈Sn−1

1
p

sup
q∈Sn−1

(cid:88)

k∈I
(cid:18)

=

√

1
θp3/2

|x0(k)|

(cid:12)
(cid:12)

(cid:12)q(cid:62) (cid:16)

yk − yk(cid:17)(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

(cid:13)
(cid:13)YI − YI

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 +

1 −

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)YI(cid:107)(cid:96)2→(cid:96)1

.

(cid:88)

k∈I

sup
q∈Sn−1
(cid:19)

|x0(k)|

(cid:12)
(cid:12)

(cid:12)q(cid:62)yk(cid:12)

(cid:12)
(cid:12)

By Lemma B.1 and Lemma B.3 in Appendix B, we have the following holds

(cid:12)
(cid:12)Q1(q) − Q1(q)(cid:12)

(cid:12) ≤

sup
q∈Sn−1

√

1
θp3/2

(cid:32)

(cid:114)

20

√

4

2

(cid:115)

n log p
θ

+

5

n log p
θ2p

(cid:33)

× 7(cid:112)2θp

≤

32
θp3/2

(cid:112)n log p,

with probability at least 1 − c1p−2, provided p ≥ C2n and θ > 1/
n. Simple calculation shows that it is enough
to have p ≥ C3 (ξ) n4 log n for some sufﬁciently large C1 (ξ) to obtain the claimed result in (E.13). Similarly, by
Lemma B.3 and Lemma B.4 in Appendix B, we have

√

sup
q∈Sn−1

= sup

q∈Sn−1

≤ sup

q∈Sn−1

(cid:104)

1
p

p
(cid:88)

(cid:13)
(cid:13)Q2(q) − Q2(q)(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1
p
(cid:88)

gkSλ

gkSλ

1
p

(cid:104)

k=1

q(cid:62)yk(cid:105)

q(cid:62)yk(cid:105)

−

−

1
p

1
p

p
(cid:88)

k=1
p
(cid:88)

k=1

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
p

p
(cid:88)

k=1

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

−

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

1
p

p
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

33

p
(cid:88)

k=1

sup
q∈Sn−1
(cid:13)G − G(cid:48)(cid:13)
(cid:0)(cid:13)
(cid:32)

≤

≤

≤

1
p

1
p

1
p

(cid:12)
(cid:12)

(cid:13)
(cid:13)

(cid:12)q(cid:62)yk(cid:12)
(cid:13)gk − g(cid:48)k(cid:13)
(cid:12)
(cid:13)
(cid:12) +
(cid:13)2
(cid:13)(cid:96)2→(cid:96)1 + (cid:13)
(cid:13)Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞

1
p

sup
q∈Sn−1

(cid:13)G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)∞

(cid:12)
(cid:12)

(cid:12)q(cid:62) (cid:16)

yk − yk(cid:17)(cid:12)
(cid:12)
(cid:12)

p
(cid:88)

(cid:13)
(cid:13)

(cid:13)g(cid:48)k(cid:13)
(cid:13)
(cid:13)2
k=1
(cid:13)Y − Y(cid:13)
(cid:13)
√

(cid:1)

(cid:13)(cid:96)2→(cid:96)1
n, (cid:112)log(2p))

(cid:33)

120 max(n, log(2p))
√
p

+

300(cid:112)n log(2p) max(
θp

√

with probability at least 1 − c4p−2 provided p ≥ C4n and θ > 1/
obtain the claimed result (E.14).

≤

420(cid:112)n log(2p) max(
θ1/2p3/2

√

n, (cid:112)log(2p))

√

n. It is sufﬁcient to have p ≥ C5 (ξ) n4 log n to

APPENDIX F
LARGE |q1| ITERATES STAYING IN SAFE REGION FOR ROUNDING

In this appendix, we prove Proposition IV.5 in Section IV.

Proof of Proposition IV.5: For notational simplicity, w.l.o.g. we will proceed to prove assuming q1 > 0. The

proof for q1 < 0 is similar by symmetry. It is equivalent to show that

(cid:107)Q2 (q)(cid:107)2
|Q1 (q)|

<

(cid:114) 1
4θ

− 1,

which is implied by

for any q ∈ Sn−1 satisfying q1 > 3
(cid:115)

L (q)

.
=

(cid:13)
(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 + (cid:13)
E (cid:2)Q1 (q)(cid:3) − (cid:12)
√
θ. Recall from (E.7) that

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)2
(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)

(cid:114) 1
4θ

<

− 1

E (cid:2)Q1(q)(cid:3) =

(cid:26)(cid:20)

(cid:16)

αΨ

−

(cid:17)

α
σ

θ
p

+ βΨ

(cid:19)(cid:21)

(cid:20)

+ σ

ψ

(cid:18) β
σ

(cid:19)

(cid:18) β
σ

(cid:16)

− ψ

−

(cid:17)(cid:21)(cid:27)

,

α
σ

where

α =

1
√
p

(cid:18) q1√
θ

(cid:19)

+ 1

,

β =

1
√
p

(cid:18) q1√
θ

(cid:19)

− 1

,

σ = (cid:107)q2(cid:107)2 /

√

p.

Noticing the fact that
(cid:18) β
σ

ψ

(cid:19)

(cid:16)

− ψ

≥ 0,

(cid:17)

(cid:19)

−

α
σ
(cid:18) β
σ

we have

Ψ

= Ψ

− 1

≥ Ψ (2) ≥

for q1 > 3

θ,

(cid:32)

1
(cid:112)1 − q2

1

(cid:18) q1√
θ

(cid:19)(cid:33)

19
20

√

E (cid:2)Q1 (q)(cid:3) ≥

√

θ
p

(cid:26) q1√
θ

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)

(cid:18) β
σ

+ Ψ

−

− Ψ

(cid:16)

(cid:17)

α
σ

(cid:19)(cid:27)

≥

(cid:18) β
σ

√
2

θ

p

(cid:19)

(cid:18) β
σ

Ψ

≥

√

19
10

θ
p

.

Moreover, from (E.8), we have

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 = (cid:107)q2(cid:107)2

(cid:26) 2 (1 − θ)
p

(cid:18)

(cid:19)

Ψ

−

+

λ
σ

θ
p

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)(cid:27)

≤

2 (1 − θ)
p

θ
p

Ψ (−1) +

[Ψ (−1) + 1] ≤

Ψ (−1) +

≤

+

2
p

2
5p

θ
p

,

(cid:18) β
σ
θ
p

where we have used the fact that −λ/σ ≤ −1 and −α/σ ≤ −1. Moreover, from results in Proposition E.8 and
Proposition E.9 in Appendix E, we know that

sup
q∈Sn−1

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ sup
q∈Sn−1

(cid:12)
(cid:12)Q1(q) − Q1(q)(cid:12)

(cid:12) + sup
q∈Sn−1

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

1
2 × 105θ5/2n3/2p

,

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

34

sup
q∈Sn−1

(cid:13)Q(q) − E (cid:2)Q(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤ sup
q∈Sn−1

(cid:13)Q(q) − Q(q)(cid:13)
(cid:13)

(cid:13)2 + sup
q∈Sn−1

(cid:13)Q(q) − E (cid:2)Q(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤

1
2 × 105θ2np

hold with probability at least 1 − c1p−2 provided that p ≥ Ω (cid:0)n4 log n(cid:1). Hence, with high probability, we have

L (q) ≤

2/(5p) + θ/p + (2 × 105θ2np)−1
√

θ/(10p) − (2 × 105θ5/2n3/2p)−1

19

≤

3/5
√

18

θ/10

≤

1
√
3

θ

<

(cid:114) 1
4θ

− 1,

whenever θ is sufﬁciently small. This completes the proof.

Now, keep the notation in Appendix E for general orthonormal basis (cid:98)Y = YU. For any current iterate q ∈ Sn−1
(cid:11)(cid:12)
(cid:12) = |(cid:104)Uq, e1(cid:105)| ≥ 3
(cid:69)(cid:12)
(cid:12)
(cid:12)

that is close enough to the target solution, i.e., (cid:12)
(cid:12)
(cid:12)
(cid:68)
(cid:12)
(cid:12)

(cid:10)q, U(cid:62)e1
(cid:12)
(cid:16)
(cid:68)
(cid:12)
(cid:12)

θ, we have

(cid:69)(cid:12)
(cid:12)
(cid:12)

√

(cid:16)

(cid:17)

(cid:17)

q; (cid:98)Y
(cid:16)

Q
(cid:13)
(cid:13)
(cid:13)Q

q; (cid:98)Y

, U(cid:62)e1
(cid:17)(cid:13)
(cid:13)
(cid:13)2

=

UQ
(cid:13)
(cid:13)
(cid:13)UQ

q; (cid:98)Y
(cid:16)

q; (cid:98)Y

, e1
(cid:17)(cid:13)
(cid:13)
(cid:13)2

=

|(cid:104)Q (Uq; Y) , e1(cid:105)|
(cid:107)Q (Uq; Y)(cid:107)2

,

where we have applied the identity proved in (E.2). Taking Uq ∈ Sn−1 as the object of interest, by Proposition IV.5,
we conclude that

with high probability.

|(cid:104)Q (Uq; Y) , e1(cid:105)|
(cid:107)Q (Uq; Y)(cid:107)2

√

≥ 2

θ

APPENDIX G
BOUNDING ITERATION COMPLEXITY

In this appendix, we prove Proposition IV.6 in Section IV.

Proof of Proposition IV.6: Recall from Proposition IV.4 in Section IV, the gap

G(q) =

|Q1(q)|
|q1|

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2

≥

1
104θ2np

√

holds uniformly over q ∈ Sn−1 satisfying
p ≥ C2n4 log n. The gap G(q) implies that

1
√

10

θn

≤ |q1| ≤ 3

θ, with probability at least 1 − c1p−2, provided

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)

.
=

⇐⇒

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12) ≥

|Q1(q)|
(cid:107)Q (q)(cid:107)2
(cid:114)
|q1|
(cid:107)q2(cid:107)2

(cid:32)

≥

1 −

=⇒

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)

≥ |q1|2

1 +

+

|q1|
104θ2np (cid:107)Q (q)(cid:107)2
|q1|
104θ2np (cid:107)Q (q)(cid:107)2
(cid:33)

|q1| (cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2 (cid:107)Q (q)(cid:107)2
(cid:12)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)
(cid:107)q2(cid:107)2
2
108θ4n2p2 (cid:107)Q (q)(cid:107)2
2

+

.

Given the set Γ deﬁned in (IV.7), now we know that

sup
q∈Γ

(cid:107)Q (q)(cid:107)2 ≤ sup
q∈Γ

(cid:12)E (cid:2)Q1(q)(cid:3) − Q1 (q)(cid:12)
(cid:12)

(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:13)
(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)

+ sup
q∈Γ
(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

≤ sup
q∈Γ

(cid:12) + sup
q∈Sn−1
(cid:13)2 + sup
q∈Sn−1
(cid:12)E (cid:2)Q2(q)(cid:3)(cid:12)
(cid:12)

(cid:12) + sup
q∈Γ

(cid:12) +

1
pn

(cid:13)E (cid:2)Q2(q)(cid:3) − Q2 (q)(cid:13)
(cid:13)

(cid:12) + sup
q∈Sn−1
(cid:13)2 + sup
q∈Sn−1

(cid:12)Q1(q) − Q1 (q)(cid:12)
(cid:12)
(cid:12)
(cid:13)Q2(q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2

with probability at least 1 − c3p−2 provided p ≥ C4n4 log n and θ > 1/
n. Here we have used Proposition E.8 and
Proposition E.9 to bound the magnitudes of the four difference terms. To bound the magnitudes of the expectations,
we have

(cid:12)
(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) =

E

(cid:104)

x0(k)Sλ

x0(k)q1 + q(cid:62)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

1
p

p
(cid:88)

k=1

2 gk(cid:105)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
√
θp

(cid:18) 1
√
θp

(cid:19)

+ E [(cid:107)g(cid:107)2]

≤

√
3
√

n
θp

≤

3n
p

,

√

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

35

(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 =

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

1
p

p
(cid:88)

k=1

(cid:104)

gkSλ

x0(k)q1 + q(cid:62)

2 gk(cid:105)

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
n. Thus, we obtain that

1
√
θp

≤

√

hold uniformly for all q ∈ Γ, provided θ > 1/

E [(cid:107)g(cid:107)2] + E

(cid:107)g(cid:107)2
2

(cid:104)

(cid:105)

≤

3n
p

with probability at least 1 − c3p−2 provided p ≥ C4n4 log n and θ > 1/

n. So we conclude that

(cid:107)Q (q)(cid:107)2 ≤

sup
q∈Γ

3n
p

+

+

≤

3n
p

1
np

7n
p

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)
|q1|

(cid:114)

≥

1 +

1 − 9θ
108 × 72 × θ4n4 .
, we will need at most

Thus, starting with any q ∈ Sn−1 such that |q1| ≥ 1
√
10
√

θn

(cid:16)

3

(cid:17)

θ/

1
√

10

θn

T =

2 log
(cid:16)

log

1 +

1−9θ
108×72×θ4n4

(cid:17) =

log
√

√

n)

2 log (30θ
(cid:16)

1 +

1−9θ
108×72×θ4n4

(cid:17) ≤

√

2 log (30θ

n)

(log 2)

1−9θ
108×72×θ4n4

≤ C5n4 log n

steps to arrive at a q ∈ Sn−1 with | ¯q1| ≥ 3
that log (1 + x) ≥ x log 2 for x ∈ [0, 1] to simplify the ﬁnal result.

θ for the ﬁrst time. Here we have assumed θ0 < 1/9 and used the fact

APPENDIX H
ROUNDING TO THE DESIRED SOLUTION

min
q

(cid:107)Yq(cid:107)1 ,

s.t. (cid:104)q, q(cid:105) = 1.

In this appendix, we prove Proposition IV.7 in Section IV. For convenience, we will assume the notations we

used in Appendix B. Then the rounding scheme can be written as

(H.1)

(H.2)

(H.3)

We will show the rounding procedure get us to the desired solution with high probability, regardless of the particular
orthonormal basis used.

Proof of Proposition IV.7: The rounding program (H.1) can be written as

Consider its relaxation

inf
q

(cid:107)Yq(cid:107)1 ,

s.t. q1q1 + (cid:104)q2, q2(cid:105) = 1.

inf
q

(cid:107)Yq(cid:107)1 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1.

It is obvious that the feasible set of (H.3) contains that of (H.2). So if e1/q1 is the unique optimal solution (UOS)
of (H.3), it is also the UOS of (H.2). Let I = supp(x0), and consider a modiﬁed problem

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

|q1| − (cid:13)

(cid:13)G(cid:48)

Iq2

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

I cq2

(cid:13)
(cid:13)1 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1.

(H.4)

The objective value of (H.4) lower bounds the objective value of (H.3), and are equal when q = e1/q1. So if
q = e1/q1 is the UOS to (H.4), it is also UOS to (H.3), and hence UOS to (H.2) by the argument above. Now
(cid:13)1 ≥ − (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)
(cid:13)
(cid:0)G − G(cid:48)(cid:1) q2
(cid:13)
(cid:13)G − G(cid:48)(cid:13)
≥ − (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

(cid:13)
(cid:13)1
(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2 .

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

(cid:13)G(cid:48)

I cq2

− (cid:13)

Iq2

When p ≥ C1n, by Lemma A.14 and Lemma B.3, we know that

− (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

≥ −

(cid:114) 2
π

6
5

√

2θ

(cid:13)G − G(cid:48)(cid:13)
(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2
(cid:114) 2
24
π
25

p (cid:107)q2(cid:107)2 +

√

(1 − 2θ)

p (cid:107)q2(cid:107)2 − 4

√

n (cid:107)q2(cid:107)2 − 7(cid:112)log(2p) (cid:107)q2(cid:107)2

.
= ζ (cid:107)q2(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

holds with probability at least 1 − c2p−2. Thus, we make a further relaxation of problem (H.2) by

|q1| + ζ (cid:107)q2(cid:107)2 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1,

36

(H.5)

whose objective value lower bounds that of (H.4). By similar arguments, if e1/q1 is UOS to (H.5), it is UOS to (H.2).
At the optimal solution to (H.5), notice that it is necessary to have sign(q1) = sign(q1) and q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 = 1.
So (H.5) is equivalent to

|q1| + ζ (cid:107)q2(cid:107)2 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 = 1.

(H.6)

which is further equivalent to

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q1

|q1| + ζ

x0
(cid:107)x0(cid:107)2

1 − |q1| |q1|
(cid:107)q2(cid:107)2
Notice that the problem in (H.7) is linear in |q1| with a compact feasible set. Since the objective is also monotonic
in |q1|, it indicates that the optimal solution only occurs at the boundary points |q1| = 0 or |q1| = 1/ |q1| Therefore,
q = e1/q1 is the UOS of (H.7) if and only if
1
|q1|

1
|q1|

ζ
(cid:107)q2(cid:107)2

x0
(cid:107)x0(cid:107)2

|q1| ≤

(H.7)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

s.t.

<

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

.

.

,

Since

(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)1

√

≤

2θp conditioned on E0, it is sufﬁcient to have

√

2θp
√
θ
2

≤ ζ =

(cid:32)

(cid:114) 2
π

24
25

√

p

1 −

θ −

9
2

(cid:114) π
2

(cid:114) n
p

25
6

−

175
24

(cid:114) π
2

(cid:115)

(cid:33)

.

log(2p)
p

Therefore there exists a constant θ0 > 0, such that whenever θ ≤ θ0 and p ≥ C3(θ0)n, the rounding returns e1/q1.
A bit of thought suggests one can take a universal C3 for all possible choice of θ0, completing the proof.

When the input basis is (cid:98)Y = YU for some orthogonal matrix U (cid:54)= I, if the ADM algorithm produces some

√

q = U(cid:62)q(cid:48), such that q(cid:48)

1 > 2

θ. It is not hard to see that now the rounding (H.1) is equivalent to

min
q

(cid:107)YUq(cid:107)1 ,

s.t. (cid:10)q(cid:48), Uq(cid:11) = 1.

Renaming Uq, it follows from the above argument that at optimum q(cid:63) it holds that Uq(cid:63) = γe1 for some constant
γ with high probability.

REFERENCES

[1] Q. Qu, J. Sun, and J. Wright, “Finding a sparse vector in a subspace: Linear sparsity using alternating directions,”

in Advances in Neural Information Processing Systems, 2014.

[2] E. J. Cand`es and T. Tao, “Decoding by linear programming,” Information Theory, IEEE Transactions on,

vol. 51, no. 12, pp. 4203–4215, 2005.

[3] D. L. Donoho, “For most large underdetermined systems of linear equations the minimal (cid:96)1-norm solution is
also the sparsest solution,” Communications on pure and applied mathematics, vol. 59, no. 6, pp. 797–829,
2006.

[4] S. T. McCormick, “A combinatorial approach to some sparse matrix problems.,” tech. rep., DTIC Document,

1983.

[5] T. F. Coleman and A. Pothen, “The null space problem i. complexity,” SIAM Journal on Algebraic Discrete

Methods, vol. 7, no. 4, pp. 527–537, 1986.

[6] M. Berry, M. Heath, I. Kaneko, M. Lawo, R. Plemmons, and R. Ward, “An algorithm to compute a sparse

basis of the null space,” Numerische Mathematik, vol. 47, no. 4, pp. 483–504, 1985.

[7] J. R. Gilbert and M. T. Heath, “Computing a sparse basis for the null space,” SIAM Journal on Algebraic

Discrete Methods, vol. 8, no. 3, pp. 446–459, 1987.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

37

[8] I. S. Duff, A. M. Erisman, and J. K. Reid, Direct Methods for Sparse Matrices. New York, NY, USA: Oxford

[9] A. J. Smola and B. Schlkopf, “Sparse greedy matrix approximation for machine learning,” pp. 911–918, Morgan

University Press, Inc., 1986.

Kaufmann, 2000.

[10] T. Kavitha, K. Mehlhorn, D. Michail, and K. Paluch, “A faster algorithm for minimum cycle basis of graphs,”

in 31st International Colloquium on Automata, Languages and Programming, pp. 846–857, Springer, 2004.

[11] L.-A. Gottlieb and T. Neylon, “Matrix sparsiﬁcation and the sparse null space problem,” in Approximation,

Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 205–218, Springer, 2010.

[12] J. Mairal, F. Bach, and J. Ponce, “Sparse modeling for image and vision processing,” arXiv preprint

arXiv:1411.3230, 2014.

[13] D. A. Spielman, H. Wang, and J. Wright, “Exact recovery of sparsely-used dictionaries,” in Proceedings of the

25th Annual Conference on Learning Theory, 2012.

[14] P. Hand and L. Demanet, “Recovering the sparsest element in a subspace,” arXiv preprint arXiv:1310.1654,

[15] J. Sun, Q. Qu, and J. Wright, “Complete dictionary recovery over the sphere,” arXiv preprint arXiv:1504.06785,

[16] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” Journal of computational and

graphical statistics, vol. 15, no. 2, pp. 265–286, 2006.

[17] I. M. Johnstone and A. Y. Lu, “On consistency and sparsity for principal components analysis in high

dimensions,” Journal of the American Statistical Association, vol. 104, no. 486, 2009.

[18] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. Lanckriet, “A direct formulation for sparse pca using

semideﬁnite programming,” SIAM review, vol. 49, no. 3, pp. 434–448, 2007.

[19] R. Krauthgamer, B. Nadler, D. Vilenchik, et al., “Do semideﬁnite relaxations solve sparse PCA up to the

information limit?,” The Annals of Statistics, vol. 43, no. 3, pp. 1300–1322, 2015.

[20] T. Ma and A. Wigderson, “Sum-of-squares lower bounds for sparse pca,” arXiv preprint arXiv:1507.06370,

2013.

2015.

2015.

[21] V. Q. Vu, J. Cho, J. Lei, and K. Rohe, “Fantope projection and selection: A near-optimal convex relaxation of

sparse pca,” in Advances in Neural Information Processing Systems, pp. 2670–2678, 2013.

[22] J. Lei, V. Q. Vu, et al., “Sparsistency and agnostic inference in sparse pca,” The Annals of Statistics, vol. 43,

[23] Z. Wang, H. Lu, and H. Liu, “Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial

no. 1, pp. 299–322, 2015.

time,” arXiv preprint arXiv:1408.5352, 2014.

[24] A. d’Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet, “A direct formulation of sparse PCA using

semideﬁnite programming,” SIAM Review, vol. 49, no. 3, 2007.

[25] Y.-B. Zhao and M. Fukushima, “Rank-one solutions for homogeneous linear matrix equations over the positive

semideﬁnite cone,” Applied Mathematics and Computation, vol. 219, no. 10, pp. 5569–5583, 2013.

[26] Y. Dai, H. Li, and M. He, “A simple prior-free method for non-rigid structure-from-motion factorization,” in
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2018–2025, IEEE, 2012.
[27] G. Beylkin and L. Monz´on, “On approximation of functions by exponential sums,” Applied and Computational

Harmonic Analysis, vol. 19, no. 1, pp. 17–48, 2005.

[28] C. T. Manolis and V. Rene, “Dual principal component pursuit,” arXiv preprint arXiv:1510.04390, 2015.
[29] M. Zibulevsky and B. A. Pearlmutter, “Blind source separation by sparse decomposition in a signal dictionary,”

Neural computation, vol. 13, no. 4, pp. 863–882, 2001.

[30] A. Anandkumar, D. Hsu, M. Janzamin, and S. M. Kakade, “When are overcomplete topic models identiﬁable?
uniqueness of tensor tucker decompositions with structured sparsity,” in Advances in Neural Information
Processing Systems, pp. 1986–1994, 2013.

[31] J. Ho, Y. Xie, and B. Vemuri, “On a nonlinear generalization of sparse coding and dictionary learning,” in

Proceedings of The 30th International Conference on Machine Learning, pp. 1480–1488, 2013.

[32] Y. Nakatsukasa, T. Soma, and A. Uschmajew, “Finding a low-rank basis in a matrix subspace,” CoRR,

vol. abs/1503.08601, 2015.

[33] Q. Berthet and P. Rigollet, “Complexity theoretic lower bounds for sparse principal component detection,” in

Conference on Learning Theory, pp. 1046–1066, 2013.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

38

[34] B. Barak, J. Kelner, and D. Steurer, “Rounding sum-of-squares relaxations,” arXiv preprint arXiv:1312.6652,

2013.

[35] S. B. Hopkins, T. Schramm, J. Shi, and D. Steurer, “Speeding up sum-of-squares for tensor decomposition and

planted sparse vectors,” arXiv preprint arXiv:1512.02337, 2015.

[36] S. Arora, R. Ge, and A. Moitra, “New algorithms for learning incoherent and overcomplete dictionaries,” arXiv

[37] A. Agarwal, A. Anandkumar, and P. Netrapalli, “Exact recovery of sparsely used overcomplete dictionaries,”

preprint arXiv:1308.6273, 2013.

arXiv preprint arXiv:1309.1952, 2013.

[38] A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon, “Learning sparsely used overcomplete

dictionaries via alternating minimization,” arXiv preprint arXiv:1310.7991, 2013.

[39] S. Arora, A. Bhaskara, R. Ge, and T. Ma, “More algorithms for provable dictionary learning,” arXiv preprint

[40] S. Arora, R. Ge, T. Ma, and A. Moitra, “Simple, efﬁcient, and neural algorithms for sparse coding,” arXiv

arXiv:1401.0579, 2014.

preprint arXiv:1503.00778, 2015.

[41] K. G. Murty and S. N. Kabadi, “Some NP-complete problems in quadratic and nonlinear programming,”

Mathematical programming, vol. 39, no. 2, pp. 117–129, 1987.

[42] R. Vershynin, “Introduction to the non-asymptotic analysis of random matrices,” arXiv preprint arXiv:1011.3027,

2010.

May 2011.

[43] R. Basri and D. W. Jacobs, “Lambertian reﬂectance and linear subspaces,” Pattern Analysis and Machine

Intelligence, IEEE Transactions on, vol. 25, no. 2, pp. 218–233, 2003.

[44] E. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal component analysis?,” Journal of the ACM, vol. 58,

[45] V. De la Pena and E. Gin´e, Decoupling: from dependence to independence. Springer, 1999.
[46] M. Talagrand, Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems,

vol. 60. Springer Science & Business Media, 2014.

[47] K. Luh and V. Vu, “Dictionary learning with few samples and matrix concentration,” arXiv preprint

arXiv:1503.08854, 2015.

[48] P. Jain, P. Netrapalli, and S. Sanghavi, “Low-rank matrix completion using alternating minimization,” in
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pp. 665–674, ACM,
2013.

[49] M. Hardt, “On the provable convergence of alternating minimization for matrix completion,” arXiv preprint

arXiv:1312.0925, 2013.

[50] M. Hardt and M. Wootters, “Fast matrix completion without the condition number,” in Proceedings of The

27th Conference on Learning Theory, pp. 638–678, 2014.

[51] M. Hardt, “Understanding alternating minimization for matrix completion,” in Foundations of Computer Science

(FOCS), 2014 IEEE 55th Annual Symposium on, pp. 651–660, IEEE, 2014.

[52] P. Jain and P. Netrapalli, “Fast exact matrix completion with ﬁnite samples,” arXiv preprint arXiv:1411.1087,

2014.

[53] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain, “Non-convex robust pca,” in Advances in

Neural Information Processing Systems, pp. 1107–1115, 2014.

[54] Q. Zheng and J. Lafferty, “A convergent gradient descent algorithm for rank minimization and semideﬁnite

programming from random linear measurements,” arXiv preprint arXiv:1506.06081, 2015.

[55] S. Tu, R. Boczar, M. Soltanolkotabi, and B. Recht, “Low-rank solutions of linear matrix equations via procrustes

ﬂow,” arXiv preprint arXiv:1507.03566, 2015.

[56] Y. Chen and M. J. Wainwright, “Fast low-rank estimation by projected gradient descent: General statistical

and algorithmic guarantees,” arXiv preprint arXiv:1509.03025, 2015.

[57] P. Jain and S. Oh, “Provable tensor factorization with missing data,” in Advances in Neural Information

Processing Systems, pp. 1431–1439, 2014.

[58] A. Anandkumar, R. Ge, and M. Janzamin, “Guaranteed non-orthogonal tensor decomposition via alternating

rank-1 updates,” arXiv preprint arXiv:1402.5180, 2014.

[59] A. Anandkumar, R. Ge, and M. Janzamin, “Analyzing tensor power method dynamics: Applications to learning

overcomplete latent variable models,” arXiv preprint arXiv:1411.1488, 2014.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

39

[60] A. Anandkumar, P. Jain, Y. Shi, and U. Niranjan, “Tensor vs matrix methods: Robust tensor decomposition

under block sparse perturbations,” arXiv preprint arXiv:1510.04747, 2015.

[61] R. Ge, F. Huang, C. Jin, and Y. Yuan, “Escaping from saddle points—online stochastic gradient for tensor

decomposition,” in Proceedings of The 28th Conference on Learning Theory, pp. 797–842, 2015.

[62] P. Netrapalli, P. Jain, and S. Sanghavi, “Phase retrieval using alternating minimization,” in Advances in Neural

Information Processing Systems, pp. 2796–2804, 2013.

[63] E. J. Cand`es, X. Li, and M. Soltanolkotabi, “Phase retrieval via wirtinger ﬂow: Theory and algorithms,” arXiv

preprint arXiv:1407.1065, 2014.

[64] Y. Chen and E. J. Candes, “Solving random quadratic systems of equations is nearly as easy as solving linear

systems,” arXiv preprint arXiv:1505.05114, 2015.

[65] J. Sun, Q. Qu, and J. Wright, “A geometric analysis of phase retreival,” arXiv preprint arXiv:1602.06664,

[66] J. Sun, Q. Qu, and J. Wright, “When are nonconvex problems not scary?,” arXiv preprint arXiv:1510.06096,

[67] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive Sensing. Springer, 2013.
[68] T. Figiel, J. Lindenstrauss, and V. D. Milman, “The dimension of almost spherical sections of convex bodies,”

Acta Mathematica, vol. 139, no. 1, pp. 53–94, 1977.

[69] A. Y. Garnaev and E. D. Gluskin, “The widths of a euclidean ball,” in Dokl. Akad. Nauk SSSR, vol. 277,

pp. 1048–1052, 1984.

[70] E. Gluskin and V. Milman, “Note on the geometric-arithmetic mean inequality,” in Geometric aspects of

Functional analysis, pp. 131–135, Springer, 2003.

[71] G. Pisier, The volume of convex bodies and Banach space geometry, vol. 94. Cambridge University Press,

2016.

2015.

1999.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

1

Finding a sparse vector in a subspace: linear
sparsity using alternating directions
Qing Qu, Student Member, IEEE, Ju Sun, Student Member, IEEE, and John Wright, Member, IEEE

6
1
0
2
 
l
u
J
 
0
2
 
 
]
T
I
.
s
c
[
 
 
3
v
9
5
6
4
.
2
1
4
1
:
v
i
X
r
a

Abstract

Is it possible to ﬁnd the sparsest vector (direction) in a generic subspace S ⊆ Rp with dim (S) = n < p? This
problem can be considered a homogeneous variant of the sparse recovery problem, and ﬁnds connections to sparse
dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper,
we focus on a planted sparse model for the subspace: the target sparse vector is embedded in an otherwise random
subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of
nonzero entries in the target sparse vector substantially exceeds O(1/
n). In contrast, we exhibit a relatively simple
nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero
entries is Ω(1). To the best of our knowledge, this is the ﬁrst practical algorithm to achieve linear scaling under
the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g.,
sparse dictionary learning.

√

Sparse vector, Subspace modeling, Sparse recovery, Homogeneous recovery, Dictionary learning, Nonconvex

optimization, Alternating direction method

Index Terms

I. INTRODUCTION
Suppose that a linear subspace S embedded in Rp contains a sparse vector x0 (cid:54)= 0. Given an arbitrary basis of S,
can we efﬁciently recover x0 (up to scaling)? Equivalently, provided a matrix A ∈ R(p−n)×p with Null(A) = S, 1
can we efﬁciently ﬁnd a nonzero sparse vector x such that Ax = 0? In the language of sparse recovery, can we
solve

min
x

(cid:107)x(cid:107)0

s.t. Ax = 0, x (cid:54)= 0

?

In contrast to the standard sparse recovery problem (Ax = b, b (cid:54)= 0), for which convex relaxations perform nearly
optimally for broad classes of designs A [2, 3], the computational properties of problem (I.1) are not nearly as well
understood. It has been known for several decades that the basic formulation

(I.1)

(I.2)

min
x

(cid:107)x(cid:107)0 ,

s.t. x ∈ S \ {0},

is NP-hard for an arbitrary subspace [4, 5]. In this paper, we assume a speciﬁc random planted sparse model for
the subspace S: a target sparse vector is embedded in an otherwise random subspace. We will show that under the
speciﬁc random model, problem (I.2) is tractable by an efﬁcient algorithm based on nonconvex optimization.

A. Motivation

The general version of Problem (I.2), in which S can be an arbitrary subspace, takes several forms in numerical
computation and computer science, and underlies several important problems in modern signal processing and
machine learning. Below we provide a sample of these applications.
Sparse Null Space and Matrix Sparsiﬁcation: The sparse null space problem is ﬁnding the sparsest matrix N
whose columns span the null space of a given matrix A. The problem arises in the context of solving linear
equality problems in constrained optimization [5], null space methods for quadratic programming [6], and solving

This work was partially supported by grants ONR N00014-13-1-0492, NSF 1343282, NSF 1527809, and funding from the Moore and
Sloan Foundations. Q. Qu, J. Sun and J. Wright are all with the Electrical Engineering Department, Columbia University, New York, NY,
10027, USA (e-mail: {qq2105, js4038, jw2966}@columbia.edu). This paper is an extension of our previous conference version [1].

1 Null(A)

.
= {x ∈ Rp | Ax = 0} denotes the null space of A.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

2

underdetermined linear equations [7]. The matrix sparsiﬁcation problem is of similar ﬂavor, the task is ﬁnding the
sparsest matrix B which is equivalent to a given full rank matrix A under elementary column operations. Sparsity
helps simplify many fundamental matrix operations (see [8]), and the problem has applications in areas such as
machine learning [9] and in discovering cycle bases of graphs [10]. [11] discusses connections between the two
problems and also to other problems in complexity theory.

Sparse (Complete) Dictionary Learning: In dictionary learning, given a data matrix Y, one seeks an approximation
Y ≈ AX, such that A is a representation dictionary with certain desired structure and X collects the representation
coefﬁcients with maximal sparsity. Such compact representation naturally allows signal compression, and also
facilitates efﬁcient signal acquisition and classiﬁcation (see relevant discussion in [12]). When A is required to
be complete (i.e., square and invertible), by linear algebra, we have2 row(Y) = row(X) [13]. Then the problem
reduces to ﬁnding sparsest vectors (directions) in the known subspace row(Y), i.e. (I.2). Insights into this problem
have led to new theoretical developments on complete dictionary learning [13–15].

Sparse Principal Component Analysis (Sparse PCA): In geometric terms, Sparse PCA (see, e.g., [16–18] for
early developments and [19, 20] for discussion of recent results) concerns stable estimation of a linear subspace
spanned by a sparse basis, in the data-poor regime, i.e., when the available data are not numerous enough to allow
one to decouple the subspace estimation and sparsiﬁcation tasks. Formally, given a data matrix Z = U0X0 + E,3
where Z ∈ Rp×n collects column-wise n data points, U0 ∈ Rp×r is the sparse basis, and E is a noise matrix, one is
asked to estimate U0 (up to sign, scale, and permutation). Such a factorization ﬁnds applications in gene expression,
ﬁnancial data analysis and pattern recognition [24]. When the subspace is known (say by the PCA estimator with
enough data samples), the problem again reduces to instances of (I.2) and is already nontrivial4. The full geometric
sparse PCA can be treated as ﬁnding sparse vectors in a subspace that is subject to perturbation.

In addition, variants and generalizations of the problem (I.2) have also been studied in applications regarding
control and optimization [25], nonrigid structure from motion [26], spectral estimation and Prony’s problem [27],
outlier rejection in PCA [28], blind source separation [29], graphical model learning [30], and sparse coding on
manifolds [31]; see also [32] and the references therein.

B. Prior Arts

Despite these potential applications of problem (I.2), it is only very recently that efﬁcient computational surrogates
with nontrivial recovery guarantees have been discovered for some cases of practical interest. In the context of sparse
dictionary learning, Spielman et al. [13] introduced a convex relaxation which replaces the nonconvex problem (I.2)
with a sequence of linear programs:

(cid:96)1/(cid:96)∞ Relaxation:

min
x

(cid:107)x(cid:107)1 ,

s.t. x(i) = 1, x ∈ S, 1 ≤ i ≤ p.

(I.3)

They proved that when S is generated as a span of n random sparse vectors, with high probability (w.h.p.), the
relaxation recovers these vectors, provided the probability of an entry being nonzero is at most θ ∈ O (1/
n). In
the planted sparse model, in which S is formed as direct sum of a single sparse vector x0 and a “generic” subspace,
Hand and Demanet proved that (I.3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales
n) [14]. One might imagine improving these results by tightening the analyses. Unfortunately, the
as θ ∈ O (1/
results of [13, 14] are essentially sharp: when θ substantially exceeds Ω(1/
n), in both models the relaxation (I.3)
provably breaks down. Moreover, the most natural semideﬁnite programming (SDP) relaxation of (I.1),

√

√

√

min
X

(cid:107)X(cid:107)1 ,

s.t.

(cid:68)

A(cid:62)A, X

(cid:69)

= 0, trace[X] = 1, X (cid:23) 0.

(I.4)

also breaks down at exactly the same threshold of θ ∼ O(1/

√

n).5

2Here, row(·) denotes the row space.
3Variants of multiple-component formulations often add an additional orthonormality constraint on U0 but involve a different notation of

sparsity; see, e.g., [16, 21–23].

4[14] has also discussed this data-rich sparse PCA setting.
5This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (with b (cid:54)= 0), in which it is possible to

handle very large fractions of nonzeros (say, θ = Ω(1/ log n), or even θ = Ω(1)) using a very simple (cid:96)1 relaxation [2, 3]

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

3

TABLE I
COMPARISON OF EXISTING METHODS FOR RECOVERING A PLANTED SPARSE VECTOR IN A SUBSPACE

Method
(cid:96)1/(cid:96)∞ Relaxation [14]
SDP Relaxation
SOS Relaxation [34]
Spectral Method [35]
This work

Recovery Condition

θ ∈ O(1/
θ ∈ O(1/

√
√

n)
n)

p ≥ Ω(n2), θ ∈ O(1)
p ≥ Ω(n2poly log(n)), θ ∈ O(1)
p ≥ Ω(n4 log n), θ ∈ O(1)

Time Complexity6
O(n3p log(1/ε))
O (cid:0)p3.5 log (1/ε)(cid:1)
∼ O(p7 log(1/ε)) 7
O (np log(1/(cid:15)))
O(n5p2 log n + n3p log(1/ε))

√

One might naturally conjecture that this 1/

n threshold is simply an intrinsic price we must pay for having an
efﬁcient algorithm, even in these random models. Some evidence towards this conjecture might be borrowed from
the superﬁcial similarity of (I.2)-(I.4) and sparse PCA [16]. In sparse PCA, there is a substantial gap between what
can be achieved with currently available efﬁcient algorithms and the information theoretic optimum [19, 33]. Is
this also the case for recovering a sparse vector in a subspace? Is θ ∈ O (1/
n) simply the best we can do with
efﬁcient, guaranteed algorithms?

√

Remarkably, this is not the case. Recently, Barak et al. introduced a new rounding technique for sum-of-squares
relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p ≥ Ω (cid:0)n2(cid:1) and
θ = Ω(1) [34]. It is perhaps surprising that this is possible at all with a polynomial time algorithm. Unfortunately,
the runtime of this approach is a high-degree polynomial in p (see Table I); for machine learning problems in
which p is often either the feature dimension or the sample size, this algorithm is mostly of theoretical interest only.
However, it raises an interesting algorithmic question: Is there a practical algorithm that provably recovers a sparse
vector with θ (cid:29) 1/

n portion of nonzeros from a generic subspace S?

√

C. Contributions and Recent Developments

In this paper, we address the above problem under the planted sparse model. We allow x0 to have up to θ0p
nonzero entries, where θ0 ∈ (0, 1) is a constant. We provide a relatively simple algorithm which, w.h.p., exactly
recovers x0, provided that p ≥ Ω (cid:0)n4 log n(cid:1). A comparison of our results with existing methods is shown in Table
I. After initial submission of our paper, Hopkins et al. [35] proposed a different simple algorithm based on the
spectral method. This algorithm guarantees recovery of the planted sparse vector also up to linear sparsity, whenever
p ≥ Ω(n2polylog(n)), and comes with better time complexity.8

Our algorithm is based on alternating directions, with two special twists. First, we introduce a special data driven
initialization, which seems to be important for achieving θ = Ω(1). Second, our theoretical results require a second,
linear programming based rounding phase, which is similar to [13]. Our core algorithm has very simple iterations,
of linear complexity in the size of the data, and hence should be scalable to moderate-to-large scale problems.

Besides enjoying the θ ∼ Ω(1) guarantee that is out of the reach of previous practical algorithms, our algorithm
performs well in simulations – empirically succeeding with p ≥ Ω (n polylog(n)). It also performs well empirically
on more challenging data models, such as the complete dictionary learning model, in which the subspace of interest
contains not one, but n random target sparse vectors. This is encouraging, as breaking the O(1/
n) sparsity barrier
with a practical algorithm and optimal guarantee is an important problem in theoretical dictionary learning [36–40].
In this regard, our recent work [15] presents an efﬁcient algorithm based on Riemannian optimization that guarantees
recovery up to linear sparsity. However, the result is based on different ideas: a different nonconvex formulation,
optimization algorithm, and analysis methodology.

√

D. Paper Organization, Notations and Reproducible Research

The rest of the paper is organized as follows. In Section II, we provide a nonconvex formulation and show its
capability of recovering the sparse vector. Section III introduces the alternating direction algorithm. In Section IV,

6All estimates here are based on the standard interior point methods for solving linear and semideﬁnite programs. Customized solvers may

result in order-wise speedup for speciﬁc problems. ε is the desired numerical accuracy.

7Here our estimation is based on the degree-4 SOS hierarchy used in [34] to obtain an initial approximate recovery.
8Despite these improved guarantees in the planted sparse model, our method still produces more appealing results on real imagery data –

see Section V-B for examples.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

4

we present our main results and sketch the proof ideas. Experimental evaluation of our method is provided in
Section V. We conclude the paper by drawing connections to related work and discussing potential improvements
in Section VI. Full proofs are all deferred to the appendix sections.

For a matrix X, we use xi and xj to denote its i-th column and j-th row, respectively, all in column vector form.
.
Moreover, we use x(i) to denote the i-th component of a vector x. We use the compact notation [k]
= {1, . . . , k}
for any positive integer k, and use c or C, and their indexed versions to denote absolute numerical constants. The
scope of these constants are always local, namely within a particular lemma, proposition, or proof, such that the
apparently same constant in different contexts may carry different values. For probability events, sometimes we will
just say the event holds “with high probability” (w.h.p.) if the probability of failure is dominated by p−κ for some
κ > 0.

The codes to reproduce all the ﬁgures and experimental results can be found online at:

https://github.com/sunju/psv.

II. PROBLEM FORMULATION AND GLOBAL OPTIMALITY
We study the problem of recovering a sparse vector x0 (cid:54)= 0 (up to scale), which is an element of a known
subspace S ⊂ Rp of dimension n, provided an arbitrary orthonormal basis Y ∈ Rp×n for S. Our starting point is
the nonconvex formulation (I.2). Both the objective and the constraint set are nonconvex, and hence it is not easy to
optimize over. We relax (I.2) by replacing the (cid:96)0 norm with the (cid:96)1 norm. For the constraint x (cid:54)= 0, since in most
applications we only care about the solution up to scaling, it is natural to force x to live on the unit sphere Sn−1,
giving

min
x

(cid:107)x(cid:107)1 ,

s.t. x ∈ S, (cid:107)x(cid:107)2 = 1.

(II.1)

This formulation is still nonconvex, and for general nonconvex problems it is known to be NP-hard to ﬁnd even
a local minimizer [41]. Nevertheless, the geometry of the sphere is benign enough, such that for well-structured
inputs it actually will be possible to give algorithms that ﬁnd the global optimizer.

The formulation (II.1) can be contrasted with (I.3), in which effectively we optimize the (cid:96)1 norm subject to the
constraint (cid:107)x(cid:107)∞ = 1: because the set {x : (cid:107)x(cid:107)∞ = 1} is polyhedral, the (cid:96)∞-constrained problem immediately
yields a sequence of linear programs. This is very convenient for computation and analysis. However, it suffers
from the aforementioned breakdown behavior around (cid:107)x0(cid:107)0 ∼ p/
n. In contrast, though the sphere (cid:107)x(cid:107)2 = 1 is a
more complicated geometric constraint, it will allow much larger number of nonzeros in x0. Indeed, if we consider
the global optimizer of a reformulation of (II.1):

√

min
q∈Rn

(cid:107)Yq(cid:107)1 ,

s.t.

(cid:107)q(cid:107)2 = 1,

(II.2)

where Y is any orthonormal basis for S, the sufﬁcient condition that guarantees exact recovery under the planted
sparse model for the subspace is as follows:

Theorem II.1 ((cid:96)1/(cid:96)2 recovery, planted sparse model). There exists a constant θ0 > 0, such that if the subspace S
follows the planted sparse model

where gi ∼i.i.d. N (0, 1
n < θ < θ0, then the unique
(up to sign) optimizer q(cid:63) to (II.2), for any orthonormal basis Y of S, produces Yq(cid:63) = ξx0 for some ξ (cid:54)= 0 with
probability at least 1 − cp−2, provided p ≥ Cn. Here c and C are positive constants.

Ber(θ) are all jointly independent and 1/

p I), and x0 ∼i.i.d.

θp

S = span (x0, g1, . . . , gn−1) ⊂ Rp,
1√

√

Hence, if we could ﬁnd the global optimizer of (II.2), we would be able to recover x0 whose number of nonzero
entries is quite large – even linear in the dimension p (θ = Ω(1)). On the other hand, it is not obvious that this
should be possible: (II.2) is nonconvex. In the next section, we will describe a simple heuristic algorithm for
approximately solving a relaxed version of the (cid:96)1/(cid:96)2 problem (II.2). More surprisingly, we will then prove that
for a class of random problem instances, this algorithm, plus an auxiliary rounding technique, actually recovers
the global optimizer – the target sparse vector x0. The proof requires a detailed probabilistic analysis, which is
sketched in Section IV-B.

Before continuing, it is worth noting that the formulation (II.1) is in no way novel – see, e.g., the work of [29]

in blind source separation for precedent. However, our algorithms and subsequent analysis are novel.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

5

III. ALGORITHM BASED ON ALTERNATING DIRECTION METHOD (ADM)
To develop an algorithm for solving (II.2), it is useful to consider a slight relaxation of (II.2), in which we

introduce an auxiliary variable x ≈ Yq:

min
q,x

f (q, x)

.
=

1
2

(cid:107)Yq − x(cid:107)2

2 + λ (cid:107)x(cid:107)1 ,

s.t.

(cid:107)q(cid:107)2 = 1.

(III.1)

Here, λ > 0 is a penalty parameter. It is not difﬁcult to see that this problem is equivalent to minimizing the
Huber M-estimator over Yq. This relaxation makes it possible to apply the alternating direction method to this
problem. This method starts from some initial point q(0), alternates between optimizing with respect to (w.r.t.) x
and optimizing w.r.t. q:

where x(k) and q(k) denote the values of x and q in the k-th iteration. Both (III.2) and (III.3) have simple closed
form solutions:

x(k+1) = arg min

q(k+1) = arg min

1
2
1
2

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)Yq(k) − x
(cid:13)
(cid:13)Yq − x(k+1)(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
2

x

q

+ λ (cid:107)x(cid:107)1 ,

s.t. (cid:107)q(cid:107)2 = 1,

x(k+1) = Sλ[Yq(k)],

q(k+1) =

Y(cid:62)x(k+1)
(cid:13)Y(cid:62)x(k+1)(cid:13)
(cid:13)
(cid:13)2

,

(III.2)

(III.3)

(III.4)

where Sλ [x] = sign(x) max {|x| − λ, 0} is the soft-thresholding operator. The proposed ADM algorithm is
summarized in Algorithm 1.

Algorithm 1 Nonconvex ADM for solving (III.1)

A matrix Y ∈ Rp×n with Y(cid:62)Y = I, initialization q(0), threshold parameter λ > 0.

The recovered sparse vector ˆx0 = Yq(k)

Input:
Output:
1: for k = 0, . . . , O (cid:0)n4 log n(cid:1) do
x(k+1) = Sλ[Yq(k)],
2:
q(k+1) = Y(cid:62)x(k+1)
(cid:107)Y(cid:62)x(k+1)(cid:107)2

,

3:
4: end for

The algorithm is simple to state and easy to implement. However, if our goal is to recover the sparsest vector x0,

some additional tricks are needed.
Initialization. Because the problem (II.2) is nonconvex, an arbitrary or random initialization may not produce a
global minimizer.9 In fact, good initializations are critical for the proposed ADM algorithm to succeed in the linear
sparsity regime. For this purpose, we suggest using every normalized row of Y as initializations for q, and solving
a sequence of p nonconvex programs (II.2) by the ADM algorithm.

To get an intuition of why our initialization works, recall the planted sparse model S = span(x0, g1, . . . , gn−1)

and suppose

Y = [x0 | g1 | · · · | gn−1] ∈ Rp×n.

(III.5)
θp(cid:1). Meanwhile, the entries of
If we take a row yi of Y, in which x0(i) is nonzero, then x0(i) = Θ (cid:0)1/
√
p. Hence, when θ is not too large,
g1(i), . . . gn−1(i) are all N (0, 1/p), and so their magnitude have size about 1/
x0(i) will be somewhat bigger than most of the other entries in yi. Put another way, yi is biased towards the ﬁrst
Y ≈ I.10
standard basis vector e1. Now, under our probabilistic model assumptions, Y is very well conditioned: Y
Using the Gram-Schmidt process11, we can ﬁnd an orthonormal basis Y for S via:

√

(cid:62)

9More precisely, in our models, random initialization does work, but only when the subspace dimension n is extremely low compared to

the ambient dimension p.

10This is the common heuristic that “tall random matrices are well conditioned” [42].
11...QR decomposition in general with restriction that R11 = 1.

Y = YR,

(III.6)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

6

where R is upper triangular, and R is itself well-conditioned: R ≈ I. Since the i-th row yi of Y is biased in
the direction of e1 and R is well-conditioned, the i-th row yi of Y is also biased in the direction of e1. In other
words, with this canonical orthobasis Y for the subspace, the i-th row of Y is biased in the direction of the global
optimizer. The heuristic arguments are made rigorous in Appendix B and Appendix D.

What if we are handed some other basis (cid:98)Y = YU, where U is an arbitary orthogonal matrix? Suppose q(cid:63) is a
global optimizer to (II.2) with the input matrix Y, then it is easy to check that, U(cid:62)q(cid:63) is a global optimizer to
(II.2) with the input matrix (cid:98)Y. Because
(cid:68)

(cid:68)

(cid:69)

(cid:69)

(YU)(cid:62)ei, U(cid:62)q(cid:63)

=

Y(cid:62)ei, q(cid:63)

,

our initialization is invariant to any rotation of the orthobasis. Hence, even if we are handed an arbitrary orthobasis
for S, the i-th row is still biased in the direction of the global optimizer.

Rounding by linear programming (LP). Let q denote the output of Algorithm 1. As illustrated in Fig. 1, we
will prove that with our particular initialization and an appropriate choice of λ, ADM algorithm uniformly moves
towards the optimal over a large portion of the sphere, and its solution falls within a certain small radius of the
globally optimal solution q(cid:63) to (II.2). To exactly recover q(cid:63), or equivalently to recover the exact sparse vector
x0 = γYq(cid:63) for some γ (cid:54)= 0, we solve the linear program

min
q

(cid:107)Yq(cid:107)1

s.t.

(cid:104)r, q(cid:105) = 1

(III.7)

with r = q. Since the feasible set {q | (cid:104)q, q(cid:105) = 1} is essentially the tangent space of the sphere Sn−1 at q, whenever
q is close enough to q(cid:63), one should expect that the optimizer of (III.7) exactly recovers q(cid:63) and hence x0 up to
scale. We will prove that this is indeed true under appropriate conditions.

IV. MAIN RESULTS AND SKETCH OF ANALYSIS

A. Main Results

previous section succeeds.

In this section, we describe our main theoretical result, which shows that w.h.p. the algorithm described in the

Theorem IV.1. Suppose that S obeys the planted sparse model, and let the columns of Y form an arbitrary
orthonormal basis for the subspace S. Let y1, . . . , yp ∈ Rn denote the (transposes of) the rows of Y. Apply
(cid:13)2 , . . . , yp/ (cid:107)yp(cid:107)2, to produce outputs q1, . . . , qp.
Algorithm 1 with λ = 1/
Solve the linear program (III.7) with r = q1, . . . , qp, to produce (cid:98)q1, . . . , (cid:98)qp. Set i(cid:63) ∈ arg mini (cid:107)Y(cid:98)qi(cid:107)1. Then

p, using initializations q(0) = y1/ (cid:13)

(cid:13)y1(cid:13)

√

for some γ (cid:54)= 0 with probability at least 1 − cp−2, provided

Y(cid:98)qi(cid:63) = γx0,

p ≥ Cn4 log n,

and

1
√
n

≤ θ ≤ θ0.

Here C, c and θ0 are positive constants.

(IV.1)

(IV.2)

Remark IV.2. We can see that the result in Theorem IV.1 is suboptimal in sample complexity compared to the
global optimality result in Theorem II.1 and Barak et al.’s result [34] (and the subsequent work [35]). For successful
recovery, we require p ≥ Ω (cid:0)n4 log n(cid:1), while the global optimality and Barak et al. demand p ≥ Ω (n) and
p ≥ Ω (cid:0)n2(cid:1), respectively. Aside from possible deﬁciencies in our current analysis, compared to Barak et al., we
believe this is still the ﬁrst practical and efﬁcient method which is guaranteed to achieve θ ∼ Ω(1) rate. The lower
bound on θ in Theorem IV.1 is mostly for convenience in the proof; in fact, the LP rounding stage of our algorithm
already succeeds w.h.p. when θ ∈ O (1/

n).

√

B. A Sketch of Analysis

In this section, we brieﬂy sketch the main ideas of proving our main result in Theorem IV.1, to show that the
“initialization + ADM + LP rounding” pipeline recovers x0 under the stated technical conditions, as illustrated in

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

7

Fig. 1. An illustration of the proof sketch for our ADM algorithm.

Fig. 1. The proof of our main result requires rather detailed technical analysis of the iteration-by-iteration properties
of Algorithm 1, most of which is deferred to the appendices.

As noted in Section III, the ADM algorithm is invariant to change of basis. So w.l.o.g., let us assume Y =

[x0 | g1 | · · · | gn−1] and let Y to be its orthogonalization, i.e., 12

Y =

(cid:20) x0
(cid:107)x0(cid:107)2

(cid:16)

| Px⊥

0

G

G(cid:62)Px⊥

0

G

(cid:17)−1/2(cid:21)

.

When p is large, Y is nearly orthogonal, and hence Y is very close to Y. Thus, in our proofs, whenever convenient,
we make the arguments on Y ﬁrst and then “propagate” the quantitative results onto Y by perturbation arguments.
With that noted, let y1, · · · , yp be the transpose of the rows of Y, and note that these are all independent random
vectors. To prove the result of Theorem IV.1, we need the following results. First, given the speciﬁed Y, we show
that our initialization is biased towards the global optimum:

Proposition IV.3 (Good initialization). Suppose θ > 1/
that at least one of our p initialization vectors suggested in Section III, say q(0)

n and p ≥ Cn. It holds with probability at least 1 − cp−2
i = yi/ (cid:13)

(cid:13)yi(cid:13)

(cid:13)2, obeys

√

Here C, c are positive constants.

Proof: See Appendix D.

Second, we deﬁne a vector-valued random process Q(q) on q ∈ Sn−1, via

so that based on (III.4), one step of the ADM algorithm takes the form:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28) yi

(cid:107)yi(cid:107)2

, e1

≥

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√

.

10

θn

Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

,

1
p

p
(cid:88)

i=1

q(k+1) =

Q (cid:0)q(k)(cid:1)
(cid:13)Q (cid:0)q(k)(cid:1)(cid:13)
(cid:13)
(cid:13)2

(IV.3)

(IV.4)

(IV.5)

(IV.6)

This is a very favorable form for analysis: the term in the numerator Q (cid:0)q(k)(cid:1) is a sum of p independent random
vectors with q(k) viewed as ﬁxed. We study the behavior of the iteration (IV.6) through the random process Q (cid:0)q(k)(cid:1).

12Note that with probability one, the inverse matrix square-root in Y is well deﬁned. So Y is well deﬁned w.h.p. (i.e., except for x0 = 0).

See more quantitative characterization of Y in Appendix B.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

8

We want to show that w.h.p. the ADM iterate sequence q(k) converges to some small neighborhood of ±e1, so
that the ADM algorithm plus the LP rounding (described in Section III) successfully retrieves the sparse vector
x0/(cid:107)x0(cid:107) = Ye1. Thus, we hope that in general, Q(q) is more concentrated on the ﬁrst coordinate than q ∈ Sn−1. Let
us partition the vector q as q = [q1; q2], with q1 ∈ R and q2 ∈ Rn−1; and correspondingly Q(q) = [Q1(q); Q2(q)].
The inner product of Q(q)/ (cid:107)Q(q)(cid:107)2 and e1 is strictly larger than the inner product of q and e1 if and only if

|Q1(q)|
|q1|

>

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

.

In the following proposition, we show that w.h.p., this inequality holds uniformly over a signiﬁcant portion of the
sphere

(cid:26)

.
=

Γ

q ∈ Sn−1 |

1
√

10

nθ

√

≤ |q1| ≤ 3

θ, (cid:107)q2(cid:107)2 ≥

(cid:27)

,

1
10

so the algorithm moves in the correct direction. Let us deﬁne the gap G(q) between the two quantities |Q1(q)| / |q1|
and (cid:107)Q2(q)(cid:107)2 / (cid:107)q2(cid:107)2 as

(IV.7)

(IV.8)

and we show that the following result is true:

G(q)

.
=

|Q1(q)|
|q1|

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

,

Proposition IV.4 (Uniform lower bound for ﬁnite sample gap). There exists a constant θ0 ∈ (0, 1), such that when
p ≥ Cn4 log n, the estimate

G(q) ≥

inf
q∈Γ

1
104θ2np

√

holds with probability at least 1 − cp−2, provided θ ∈ (1/

n, θ0). Here C, c are positive constants.

Proof: See Appendix E.

√

Next, we show that whenever |q1| ≥ 3
enough for LP rounding (III.7) to succeed.

θ, w.h.p. the iterates stay in a “safe region” with |q1| ≥ 2

θ which is

√

Proposition IV.5 (Safe region for rounding). There exists a constant θ0 ∈ (0, 1), such that when p ≥ Cn4 log n, it
holds with probability at least 1 − cp−2 that

|Q1(q)|
(cid:107)Q(q)(cid:107)2

√
θ

≥ 2

√

√

for all q ∈ Sn−1 satisfying |q1| > 3

θ, provided θ ∈ (1/

n, θ0). Here C, c are positive constants.

Proof: See Appendix F.

In addition, the following result shows that the number of iterations for the ADM algorithm to reach the safe

region can be bounded grossly by O(n4 log n) w.h.p..

Proposition IV.6 (Iteration complexity of reaching the safe region). There is a constant θ0 ∈ (0, 1), such that
when p ≥ Cn4 log n, it holds with probability at least 1 − cp−2 that the ADM algorithm in Algorithm 1, with any
(cid:12)
initialization q(0) ∈ Sn−1 satisfying
(cid:12) ≥ 1
(cid:12)
θ at least once in
√
θn
10
√
at most O(n4 log n) iterations, provided θ ∈ (1/

, will produce some iterate q with |¯q1| > 3
n, θ0). Here C, c are positive constants.

(cid:12)
(cid:12)q(0)
(cid:12)

√

1

Proof: See Appendix G.

Moreover, we show that the LP rounding (III.7) with input r = q exactly recovers the optimal solution w.h.p.,

√

whenever the ADM algorithm returns a solution q with ﬁrst coordinate |q1| > 2

θ.

Proposition IV.7 (Success of rounding). There is a constant θ0 ∈ (0, 1), such that when p ≥ Cn, the following
holds with probability at least 1 − cp−2 provided θ ∈ (1/
n, θ0): Suppose the input basis is Y deﬁned in (IV.3)
and the ADM algorithm produces an output q ∈ Sn−1 with |q1| > 2
θ. Then the rounding procedure with r = q
returns the desired solution ±e1. Here C, c are positive constants.

√

√

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

9

Finally, given p ≥ Cn4 log n for a sufﬁciently large constant C, we combine all the results above to complete the

Proof: See Appendix H.

proof of Theorem IV.1.

Proof of Theorem IV.1:

W.l.o.g., let us again ﬁrst consider Y as deﬁned in (III.5) and its orthogonalization Y in a “natural/canonical”
form (IV.3). We show that w.h.p. our algorithmic pipeline described in Section III exactly recovers the optimal
solution up to scale, via the following argument:

1) Good initializers. Proposition IV.3 shows that w.h.p., at least one of the p initialization vectors, say q(0)

i =

yi/ (cid:13)

(cid:13)yi(cid:13)

(cid:13)2, obeys

(cid:12)
(cid:68)
(cid:12)
(cid:12)

q(0)
i

, e1

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥

1
√

,

10

θn

which implies that q(0)

i

is biased towards the global optimal solution.

√

2) Uniform progress away from the equator. By Proposition IV.4, for any θ ∈ (1/

n, θ0) with a constant

θ0 ∈ (0, 1),

w.h.p.,

G(q) =

|Q1(q)|
|q1|
holds uniformly for all q ∈ Sn−1 in the region
(cid:12)
(cid:12) ≥ 1
(cid:12)
q(0) such that
if sufﬁciently many iterations are allowed.

(cid:12)
(cid:12)q(0)
(cid:12)

θn

10

√

1

1
√

10

θn

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2

1
104θ2np

≥

√

, the ADM algorithm will eventually obtain a point q(k) for which (cid:12)

(cid:12)q(k)(cid:12)

(cid:12) ≥ 3

√

θ,

≤ |q1| ≤ 3

θ w.h.p.. This implies that with an input

(IV.9)

3) No jumps away from the caps. Proposition IV.5 shows that for any θ ∈ (1/

n, θ0) with a constant θ0 ∈ (0, 1),

Q1(q)
(cid:107)Q(q)(cid:107)2

√

≥ 2

θ

√

√

√

θ. This implies that once |q(k)
holds for all q ∈ Sn−1 with |q1| ≥ 3
θ for some iterate k, all the future
iterates produced by the ADM algorithm stay in a “spherical cap” region around the optimum with |q1| ≥ 2
θ.
4) Location of stopping points. As shown in Proposition IV.6, w.h.p., the strictly positive gap G(q) in (IV.9)
ensures that one needs to run at most O (cid:0)n4 log n(cid:1) iterations to ﬁrst encounter an iterate q(k) such that
|q(k)
θ. Hence, the steps above imply that, w.h.p., Algorithm 1 fed with the proposed initialization
θ after O (cid:0)n4 log n(cid:1) steps.
scheme successively produces iterates q ∈ Sn−1 with its ﬁrst coordinate |q1| ≥ 2
θ. Proposition IV.7 proves that w.h.p., the LP rounding (III.7) with an

5) Rounding succeeds when |r1| ≥ 2

1 | ≥ 3

1 | ≥ 3

√

√

√

√

input r = q produces the solution ±x0 up to scale.

Taken together, these claims imply that from at least one of the initializers q(0), the ADM algorithm will produce
an output q which is accurate enough for LP rounding to exactly return x0/(cid:107)x0(cid:107)2. On the other hand, our (cid:96)1/(cid:96)2
optimality theorem (Theorem II.1) implies that ±x0 are the unique vectors with the smallest (cid:96)1 norm among all
unit vectors in the subspace. Since w.h.p. x0/(cid:107)x0(cid:107)2 is among the p unit vectors (cid:98)q1, . . . , (cid:98)qp our p row initializers
ﬁnally produce, our minimal (cid:96)1 norm selector will successfully locate x0/(cid:107)x0(cid:107)2 vector.

For the general case when the input is an arbitrary orthonormal basis (cid:98)Y = YU for some orthogonal matrix U,

the target solution is U(cid:62)e1. The following technical pieces are perfectly parallel to the argument above for Y.

1) Discussion at the end of Appendix D implies that w.h.p., at least one row of (cid:98)Y provides an initial point q(0)

such that (cid:12)
(cid:12)

(cid:10)q(0), U(cid:62)e1

(cid:11)(cid:12)
(cid:12) ≥ 1
√
10

.

θn

2) Discussion following Proposition IV.4 in Appendix E indicates that for all q such that

≤ (cid:12)
(cid:10)q, U(cid:62)e1
(cid:12)
θ, there is a strictly positive gap, indicating steady progress towards a point q(k) such that (cid:12)
(cid:10)q(k), U(cid:62)e1
(cid:12)
θ.

√
3
√
3

1
√

θn

10

3) Discussion at the end of Appendix F implies that once q satisﬁes (cid:12)
(cid:12)

(cid:10)q, U(cid:62)e1

(cid:11)(cid:12)
(cid:12), the next iterate will not move

(cid:11)(cid:12)
(cid:12) ≤
(cid:11)(cid:12)
(cid:12) ≥

far away from the target:

(cid:68)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

Q

q; (cid:98)Y

(cid:16)

(cid:17)

(cid:13)
(cid:13)
(cid:13)Q

/

q; (cid:98)Y

(cid:17)(cid:13)
(cid:13)
(cid:13)2

, U(cid:62)e1

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥ 2

√

θ.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

10

4) Repeating the argument in Appendix G for general input (cid:98)Y shows it is enough to run the ADM algorithm
≤ (cid:12)
O (cid:0)n4 log n(cid:1) iterations to cross the range
θ. So the argument above together
(cid:12)
dictates that with the proposed initialization, w.h.p., the ADM algorithm produces an output q that satisﬁes
(cid:12)
(cid:10)q, U(cid:62)e1
(cid:12)

√
5) Since the ADM returns q satisfying (cid:12)
(cid:12)

θ, if we run at least O (cid:0)n4 log n(cid:1) iterations.
(cid:10)q, R(cid:62)e1

θ, discussion at the end of Appendix H implies that we
will obtain a solution q(cid:63) = ±U(cid:62)e1 up to scale as the optimizer of the rounding program, exactly the target
solution.

(cid:10)q, U(cid:62)e1

(cid:11)(cid:12)
(cid:12) ≤ 3

(cid:11)(cid:12)
(cid:12) ≥ 2

(cid:11)(cid:12)
(cid:12) ≥ 2

√

√

1
√

θn

10

Hence, we complete the proof.

Remark IV.8. Under the planted sparse model, in practice the ADM algorithm with the proposed initialization
converges to a global optimizer of (III.1) that correctly recovers x0. In fact, simple calculation shows such desired
point for successful recovery is indeed the only critical point of (III.1) near the pole in Fig. 1. Unfortunately,
using the current analytical framework, we did not succeed in proving such convergence in theory. Proposition IV.5
and IV.6 imply that after O(n4 log n) iterations, however, the ADM sequence will stay in a small neighborhood of
the target. Hence, we proposed to stop after O(n4 log n) steps, and then round the output using the LP that provable
recover the target, as implied by Proposition IV.5 and IV.7. So the LP rounding procedure is for the purpose of
completing the theory, and seems not necessary in practice. We suspect alternative analytical strategies, such as the
geometrical analysis that we will discuss in Section VI, can likely get around the artifact.

V. EXPERIMENTAL RESULTS

In this section, we show the performance of the proposed ADM algorithm on both synthetic and real datasets. On
the synthetic dataset, we show the phase transition of our algorithm on both the planted sparse and the dictionary
learning models; for the real dataset, we demonstrate how seeking sparse vectors can help discover interesting
patterns on face images.

A. Phase Transition on Synthetic Data

For the planted sparse model, for each pair of (k, p), we generate the n dimensional subspace S ⊂ Rp by direct
sum of x0 and G: x0 ∈ Rp is a k-sparse vector with uniformly random support and all nonzero entries equal to 1,
and G ∈ Rp×(n−1) is an i.i.d. Gaussian matrix distributed by N (0, 1/p). So one basis Y of the subspace S can
be constructed by Y = GS ([x0, G]) U, where GS (·) denotes the Gram-Schmidt orthonormalization operator and
U ∈ Rn×n is an arbitrary orthogonal matrix. For each p, we set the regularization parameter in (III.1) as λ = 1/
p,
use all the normalized rows of Y as initializations of q for the proposed ADM algorithm, and run the alternating
steps for 104 iterations. We determine the recovery to be successful whenever (cid:107)x0/ (cid:107)x0(cid:107)2 − Yq(cid:107)2 ≤ 10−2 for at
least one of the p trials (we set the tolerance relatively large as we have shown that LP rounding exactly recovers
the solutions with approximate input). To determine the empirical recovery performance of our ADM algorithm,
ﬁrst we ﬁx the relationship between n and p as p = 5n log n, and plot out the phase transition between k and p.
Next, we ﬁx the sparsity level θ = 0.2 (or k = 0.2p), and plot out the phase transition between p and n. For each
pair of (p, k) or (n, p), we repeat the simulation for 10 times. Fig. 2 shows both phase transition plots.

√

We also experiment with the complete dictionary learning model as in [13] (see also [15]). Speciﬁcally, the
observation is assumed to be Y = A0X0, where A0 is a square, invertible matrix, and X0 a n × p sparse matrix.
Since A0 is invertible, the row space of Y is the same as that of X0. For each pair of (k, n), we generate
X0 = [x1, · · · , xn](cid:62), where each vector xi ∈ Rp is k-sparse with every nonzero entry following i.i.d. Gaussian
distribution, and construct the observation by Y(cid:62) = GS (cid:0)X(cid:62)
(cid:1) U(cid:62). We repeat the same experiment as for the planted
0
sparse model described above. The only difference is that here we determine the recovery to be successful as long
as one sparse row of X0 is recovered by one of those p programs. Fig. 3 shows both phase transition plots.

Fig. 2(a) and Fig. 3(a) suggest our ADM algorithm could work into the linear sparsity regime for both models,
provided p ≥ Ω(n log n). Moreover, for both models, the log n factor seems necessary for working into the linear
sparsity regime, as suggested by Fig. 2(b) and Fig. 3(b): there are clear nonlinear transition boundaries between
success and failure regions. For both models, O(n log n) sample requirement is near optimal: for the planted sparse
model, obviously p ≥ Ω(n) is necessary; for the complete dictionary learning model, [13] proved that p ≥ Ω(n log n)
is required for exact recovery. For the planted sparse model, our result p ≥ Ω(n4 log n) is far from this much

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

11

Fig. 2. Phase transition for the planted sparse model using the ADM algorithm: (a) with ﬁxed relationship between p and n: p = 5n log n;
(b) with ﬁxed relationship between p and k: k = 0.2p. White indicates success and black indicates failure.

Fig. 3. Phase transition for the dictionary learning model using the ADM algorithm: (a) with ﬁxed relationship between p and n: p = 5n log n;
(b) with ﬁxed relationship between p and k: k = 0.2p. White indicates success and black indicates failure.

lower empirical requirement. Fig 2(b) further suggests that alternative reformulation and algorithm are needed to
solve (II.1) so that the optimal recovery guarantee as depicted in Theorem II.1 can be obtained.

B. Exploratory Experiments on Faces

It is well known in computer vision that the collection of images of a convex object only subject to illumination
changes can be well approximated by a low-dimensional subspaces in raw-pixel space [43]. We will play with face
subspaces here. First, we extract face images of one person (65 images) under different illumination conditions. Then
we apply robust principal component analysis [44] to the data and get a low dimensional subspace of dimension 10,
i.e., the basis Y ∈ R32256×10. We apply the ADM + LP algorithm to ﬁnd the sparsest elements in such a subspace,
by randomly selecting 10% rows of Y as initializations for q. We judge the sparsity in the (cid:96)1/(cid:96)2 sense, that is,
the sparsest vector (cid:98)x0 = Yq(cid:63) should produce the smallest (cid:107)Yq(cid:107)1 / (cid:107)Yq(cid:107)2 among all results. Once some sparse
vectors are found, we project the subspace onto orthogonal complement of the sparse vectors already found13, and
continue the seeking process in the projected subspace. Fig. 4(Top) shows the ﬁrst four sparse vectors we get from
the data. We can see they correspond well to different extreme illumination conditions. We also implemented the
spectral method (with the LP post-processing) proposed in [35] for comparison under the same protocol. The result
is presented as Fig. 4(Bottom): the ratios (cid:107)·(cid:107)(cid:96)1 / (cid:107)·(cid:107)(cid:96)2 are signiﬁcantly higher, and the ratios (cid:107)·(cid:107)(cid:96)4 / (cid:107)·(cid:107)(cid:96)2 (this is

13The idea is to build a sparse, orthonormal basis for the subspace in a greedy manner.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

12

the metric to be maximized in [35] to promote sparsity) are signiﬁcantly lower. By these two criteria the spectral
method with LP rounding consistently produces vectors with higher sparsity levels under our evaluation protocol.
Moreover, the resulting images are harder to interpret physically.

Fig. 4. The ﬁrst four sparse vectors extracted for one person in the Yale B database under different illuminations. (Top) by our ADM
algorithm; (Bottom) by the speeding-up SOS algorithm proposed in [35].

Second, we manually select ten different persons’ faces under the normal lighting condition. Again, the dimension
of the subspace is 10 and Y ∈ R32256×10. We repeat the same experiment as stated above. Fig. 5 shows four sparse
vectors we get from the data. Interestingly, the sparse vectors roughly correspond to differences of face images
concentrated around facial parts that different people tend to differ from each other, e.g., eye brows, forehead hair,
nose, etc. By comparison, the vectors returned by the spectral method [35] are relatively denser and the sparsity
patterns in the images are less structured physically.

In sum, our algorithm seems to ﬁnd useful sparse vectors for potential applications, such as peculiarity discovery
in ﬁrst setting, and locating differences in second setting. Nevertheless, the main goal of this experiment is to
invite readers to think about similar pattern discovery problems that might be cast as the problem of seeking sparse
vectors in a subspace. The experiment also demonstrates in a concrete way the practicality of our algorithm, both in
handling data sets of realistic size and in producing meaningful results even beyond the (idealized) planted sparse
model that we adopted for analysis.

VI. CONNECTIONS AND DISCUSSION

For the planted sparse model, there is a substantial performance gap in terms of p-n relationship between the our
optimality theorem (Theorem II.1), empirical simulations, and guarantees we have obtained via efﬁcient algorithm
(Theorem IV.1). More careful and tighter analysis based on decoupling [45] and chaining [46, 47] and geometrical
analysis described below can probably help bridge the gap between our theoretical and empirical results. Matching
the theoretical limit depicted in Theorem II.1 seems to require novel algorithmic ideas. The random models we
assume for the subspace can be extended to other random models, particularly for dictionary learning where all the
bases are sparse (e.g., Bernoulli-Gaussian random model).

This work is part of a recent surge of research efforts on deriving provable and practical nonconvex algorithms
to central problems in modern signal processing and machine learning. These problems include low-rank matrix
recovery/completion [48–56], tensor recovery/decomposition [57–61], phase retrieval [62–65], dictionary learning

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

13

Fig. 5. The ﬁrst four sparse vectors extracted for 10 persons in the Yale B database under normal illuminations. (Top) by our ADM
algorithm; (Bottom) by the speeding-up SOS algorithm proposed in [35].

[15, 36–40], and so on.14 Our approach, like the others, is to start with a carefully chosen, problem-speciﬁc
initialization, and then perform a local analysis of the subsequent iterates to guarantee convergence to a good solution.
In comparison, our subsequent work on complete dictionary learning [15] and generalized phase retrieval [65] has
taken a geometrical approach by characterizing the function landscape and designing efﬁcient algorithm accordingly.
The geometric approach has allowed provable recovery via efﬁcient algorithms, with an arbitrary initialization. The
article [66] summarizes the geometric approach and its applicability to several other problems of interest.

A hybrid of the initialization and the geometric approach discussed above is likely to be a powerful computational
framework. To see it in action for the current planted sparse vector problem, in Fig. 6 we provide the asymptotic
function landscape (i.e., p → ∞) of the Huber loss on the sphere S2 (aka the relaxed formulation we tried to
solve (III.1)). It is clear that with an initialization that is biased towards either the north or the south pole, we are
situated in a region where the gradients are always nonzero and points to the favorable directions such that many
reasonable optimization algorithms can take the gradient information and make steady progress towards the target.
This will probably ease the algorithm development and analysis, and help yield tight performance guarantees.

We provide a very efﬁcient algorithm for ﬁnding a sparse vector in a subspace, with strong guarantee. Our
algorithm is practical for handling large datasets—in the experiment on the face dataset, we successfully extracted
some meaningful features from the human face images. However, the potential of seeking sparse/structured element
in a subspace seems largely unexplored, despite the cases we mentioned at the start. We hope this work could
inspire more application ideas.

ACKNOWLEDGEMENT

JS thanks the Wei Family Private Foundation for their generous support. We thank Cun Mu, IEOR Department of
Columbia University, for helpful discussion and input regarding this work. We thank the anonymous reviewers for
their constructive comments that helped improve the manuscript. This work was partially supported by grants ONR
N00014-13-1-0492, NSF 1343282, NSF 1527809, and funding from the Moore and Sloan Foundations.

14The webpage http://sunju.org/research/nonconvex/ maintained by the second author contains pointers to the growing list of work in this

direction.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

14

Fig. 6. Function landscape of f (q) with θ = 0.4 for n = 3. (Left) f (q) over the sphere S2. Note that near the spherical caps around
the north and south poles, there are no critical points and the gradients are always nonzero; (Right) Projected function landscape by
3 (cid:55)→ R obtained via the reparameterization
projecting the upper hemisphere onto the equatorial plane. Mathematically the function g(w) : e⊥
q(w) = [w; (cid:112)1 − (cid:107)w(cid:107)2]. Corresponding to the left, there is no undesired critical point around 0 within a large radius.

APPENDIX A
TECHNICAL TOOLS AND PRELIMINARIES

In this appendix, we record several lemmas that are useful for our analysis.

Lemma A.1. Let ψ(x) and Ψ(x) to denote the probability density function (pdf) and the cumulative distribution
function (cdf) for the standard normal distribution:

(Standard Normal pdf) ψ(x) =

1
√
2π
1
√
2π
Suppose a random variable X ∼ N (0, σ2), with the pdf fσ(x) = 1

(Standard Normal cdf) Ψ(x) =

(cid:90) x

−∞

σ ψ (cid:0) x

σ

(cid:26)

exp

−

(cid:27)

x2
2
(cid:26)

exp

−

dt,

(cid:27)

t2
2

(cid:1), then for any t2 > t1 we have

(cid:90) t2

t1
(cid:90) t2

t1
(cid:90) t2

t1

fσ(x)dx = Ψ

(cid:19)

(cid:18) t2
σ
(cid:20)

(cid:20)

− Ψ

(cid:19)

(cid:19)

(cid:18) t2
σ
(cid:18) t2
σ

(cid:19)

,

(cid:18) t1
σ

(cid:19)(cid:21)

,

(cid:19)(cid:21)

(cid:18) t1
σ
(cid:18) t1
σ

xfσ(x)dx = −σ

ψ

− ψ

x2fσ(x)dx = σ2

Ψ

− Ψ

− σ

(cid:20)
t2ψ

(cid:19)

(cid:18) t2
σ

− t1ψ

(cid:19)(cid:21)

.

(cid:18) t1
σ

Lemma A.2 (Taylor Expansion of Standard Gaussian cdf and pdf ). Assume ψ(x) and Ψ(x) be deﬁned as above.
There exists some universal constant Cψ > 0 such that for any x0, x ∈ R,

|ψ(x) − [ψ(x0) − x0ψ (x0) (x − x0)]| ≤ Cψ(x − x0)2,
|Ψ(x) − [Ψ(x0) + ψ(x0)(x − x0)]| ≤ Cψ(x − x0)2.

Lemma A.3 (Matrix Induced Norms). For any matrix A ∈ Rp×n, the induced matrix norm from (cid:96)p → (cid:96)q is deﬁned
as

(cid:107)A(cid:107)(cid:96)p→(cid:96)q

.
= sup

(cid:107)x(cid:107)p=1

(cid:107)Ax(cid:107)q .

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

15

In particular, let A = [a1, · · · , an] = (cid:2)a1, · · · , ap(cid:3)(cid:62) , we have

(cid:107)A(cid:107)(cid:96)2→(cid:96)1 = sup
(cid:107)x(cid:107)2=1

p
(cid:88)

k=1

(cid:12)
(cid:12)a(cid:62)
(cid:12)
k x

(cid:12)
(cid:12)
(cid:12) ,

(cid:107)AB(cid:107)(cid:96)p→(cid:96)r ≤ (cid:107)A(cid:107)(cid:96)q→(cid:96)r (cid:107)B(cid:107)(cid:96)p→(cid:96)q ,

(cid:107)A(cid:107)(cid:96)2→(cid:96)∞ = max
1≤k≤p

(cid:13)
(cid:13)

(cid:13)ak(cid:13)
(cid:13)
(cid:13)2

,

and B is any matrix of size compatible with A.
Lemma A.4 (Moments of the Gaussian Random Variable). If X ∼ N (cid:0)0, σ2
X
that

(cid:1), then it holds for all integer m ≥ 1

E [|X|m] = σm

X (m − 1)!!

1m=2k+1 + 1m=2k

≤ σm

X (m − 1)!!, k = (cid:98)m/2(cid:99).

(cid:34)(cid:114) 2
π

(cid:35)

Lemma A.5 (Moments of the χ Random Variable). If X ∼ χ (n), i.e., X = (cid:107)x(cid:107)2 for x ∼ N (0, I), then it holds
for all integer m ≥ 1 that

E [X m] = 2m/2 Γ (m/2 + n/2)

≤ m!! nm/2.

Γ (n/2)

Lemma A.6 (Moments of the χ2 Random Variable). If X ∼ χ2 (n), i.e., X = (cid:107)x(cid:107)2
for all integer m ≥ 1 that

2 for x ∼ N (0, I), then it holds

E [X m] = 2m Γ (m + n/2)

=

Γ (n/2)

(n + 2k − 2) ≤

(2n)m.

m!
2

m
(cid:89)

k=1

Lemma A.7 (Moment-Control Bernstein’s Inequality for Random Variables [67]). Let X1, . . . , Xp be i.i.d. real-valued
random variables. Suppose that there exist some positive numbers R and σ2

X such that

m!
2

m!
2

Let S

.
= 1
p

(cid:80)p

k=1 Xk, then for all t > 0, it holds that

E [|Xk|m] ≤

X Rm−2,
σ2

for all integers m ≥ 2.

P [|S − E [S]| ≥ t] ≤ 2 exp

−

(cid:18)

pt2
X + 2Rt

2σ2

(cid:19)

.

Lemma A.8 (Moment-Control Bernstein’s Inequality for Random Vectors [15]). Let x1, . . . , xp ∈ Rd be i.i.d.
random vectors. Suppose there exist some positive number R and σ2

X such that

Let s = 1
p

(cid:80)p

k=1 xk, then for any t > 0, it holds that

E [(cid:107)xk(cid:107)m

2 ] ≤

X Rm−2,
σ2

for all integers m ≥ 2.

P [(cid:107)s − E [s](cid:107)2 ≥ t] ≤ 2(d + 1) exp

(cid:18)

−

pt2
X + 2Rt

2σ2

(cid:19)

.

Lemma A.9 (Gaussian Concentration Inequality). Let x ∼ N (0, Ip). Let f : Rp (cid:55)→ R be an L-Lipschitz function.
Then we have for all t > 0 that

P [f (X) − Ef (X) ≥ t] ≤ exp

−

(cid:18)

(cid:19)

.

t2
2L2

Lemma A.10 (Bounding Maximum Norm of Gaussian Vector Sequence). Let x1, . . . , xn1 be a sequence of (not
necessarily independent) standard Gaussian vectors in Rn2. It holds that

(cid:20)

P

max
i∈[n1]

(cid:107)xi(cid:107)2 >

√

(cid:21)
n2 + 2(cid:112)2 log(2n1)

≤ (2n1)−3.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

16

Proof: Since the function (cid:107)·(cid:107)2 is 1-Lipschitz, by Gaussian concentration inequality, for any i ∈ [n1], we have

(cid:20)
(cid:107)xi(cid:107)2 −

P

(cid:113)

E (cid:107)xi(cid:107)2

(cid:21)
2 > t

≤ P [(cid:107)xi(cid:107)2 − E (cid:107)xi(cid:107)2 > t] ≤ exp

(cid:19)

(cid:18)

−

t2
2

for all t > 0. Since E (cid:107)xi(cid:107)2

2 = n2, by a simple union bound, we obtain
t2
√
2

(cid:21)
n2 + t

(cid:107)xi(cid:107) >

max
i∈[n1]

≤ exp

−

(cid:18)

P

(cid:20)

(cid:19)

+ log n1

for all t > 0. Taking t = 2(cid:112)2 log(2n1) gives the claimed result.

Corollary A.11. Let Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1). It holds that
(cid:17)
n2 + 2(cid:112)2 log(2n1)

(cid:107)Φx(cid:107)∞ ≤

(cid:16)√

(cid:107)x(cid:107)2

for all x ∈ Rn2,

with probability at least 1 − (2n1)−3.

Proof: Let Φ = (cid:2)φ1, · · · , φn1(cid:3)(cid:62) . Without loss of generality, let us only consider x ∈ Sn2−1, we have
(cid:12)x(cid:62)φi(cid:12)

(cid:13)φi(cid:13)
(cid:13)

(cid:13)2 .

(cid:107)Φx(cid:107)∞ = max
i∈[n1]

(cid:12)
(cid:12) ≤ max
i∈[n1]

(cid:12)
(cid:12)

(A.1)

Invoking Lemma A.10 returns the claimed result.

Lemma A.12 (Covering Number of a Unit Sphere [42]). Let Sn−1 = {x ∈ Rn | (cid:107)x(cid:107)2 = 1} be the unit sphere. For
any ε ∈ (0, 1), there exists some ε cover of Sn−1 w.r.t. the (cid:96)2 norm, denoted as Nε, such that

|Nε| ≤

1 +

(cid:18)

(cid:19)n

2
ε

≤

(cid:19)n

.

(cid:18) 3
ε

Lemma A.13 (Spectrum of Gaussian Matrices, [42]). Let Φ ∈ Rn1×n2 (n1 > n2) contain i.i.d. standard normal
entries. Then for every t ≥ 0, with probability at least 1 − 2 exp (cid:0)−t2/2(cid:1), one has

√

√

n1 −

n2 − t ≤ σmin(Φ) ≤ σmax(Φ) ≤

n1 +

n2 + t.

√

√

Lemma A.14. For any ε ∈ (0, 1), there exists a constant C (ε) > 1, such that provided n1 > C (ε) n2, the random
matrix Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1) obeys

(cid:114) 2
(cid:114) 2
π
π
with probability at least 1 − 2 exp (−c (ε) n1) for some c (ε) > 0.

n1 (cid:107)x(cid:107)2 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + ε)

(1 − ε)

n1 (cid:107)x(cid:107)2

for all x ∈ Rn2,

Geometrically, this lemma roughly corresponds to the well known almost spherical section theorem [68, 69], see

also [70]. A slight variant of this version has been proved in [3], borrowing ideas from [71].

Proof: By homogeneity, it is enough to show that the bounds hold for every x of unit (cid:96)2 norm. For a ﬁxed
n1-Lipschitz, by concentration of

π n1. Note that (cid:107)·(cid:107)1 is

x0 with (cid:107)x0(cid:107)2 = 1, Φx0 ∼ N (0, I). So E (cid:107)Φx(cid:107)1 =
measure for Gaussian vectors in Lemma A.9, we have

(cid:113) 2

√

P [|(cid:107)Φx(cid:107)1 − E [(cid:107)Φx(cid:107)1]| > t] ≤ 2 exp

(cid:19)

(cid:18)

−

t2
2n1

for any t > 0. For a ﬁxed δ ∈ (0, 1), S n2−1 can be covered by a δ-net Nδ with cardinality #Nδ ≤ (1 + 2/δ)n2.
Now consider the event

(cid:40)

.
=

E

(cid:114) 2
π

(1 − δ)

n1 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + δ)

n1 ∀ x ∈ Nδ

.

(cid:114) 2
π

(cid:41)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

17

(cid:114) 2
π

∞
(cid:88)

k=0

∞
(cid:88)

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

A simple application of union bound yields

P [E c] ≤ 2 exp

−

+ n2 log

1 +

(cid:18)

δ2n1
π

(cid:18)

(cid:19)(cid:19)

.

2
δ

Choosing δ small enough such that

then conditioned on E, we can conclude that

(1 − 3δ) (1 − δ)−1 ≥ 1 − ε and (1 + δ) (1 − δ)−1 ≤ 1 + ε,

(1 − ε)

n1 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + ε)

n1 ∀ x ∈ Sn2−1.

(cid:114) 2
π

Indeed, suppose E holds. Then it can easily be seen that any z ∈ Sn2−1 can be written as

z =

λkxk,

with |λk| ≤ δk, xk ∈ Nδ for all k.

Hence we have

Similarly,

(cid:107)Φz(cid:107)1 =

Φ

λkxk

≤

δk (cid:107)Φxk(cid:107)1 ≤ (1 + δ) (1 − δ)−1

(cid:114) 2
π

n1.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

∞
(cid:88)

k=0

(cid:107)Φz(cid:107)1 =

Φ

λkxk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

k=0

(cid:104)

1 − δ − δ (1 + δ) (1 − δ)−1(cid:105) (cid:114) 2

≥

n1 = (1 − 3δ) (1 − δ)−1

π

(cid:114) 2
π

n1.

Hence, the choice of δ above leads to the claimed result. Finally, given n1 > Cn2, to make the probability P [E c]
decaying in n1, it is enough to set C = 2π

(cid:1). This completes the proof.

δ2 log (cid:0)1 + 2

δ

APPENDIX B
THE RANDOM BASIS VS. ITS ORTHONORMALIZED VERSION

In this appendix, we consider the planted sparse model

Y = [x0 | g1 | · · · | gn−1] = [x0 | G] ∈ Rp×n

as deﬁned in (III.5), where

1
√
θp

x0(k) ∼i.i.d.

Ber (θ) ,

g(cid:96) ∼i.i.d. N

0,

,

1 ≤ k ≤ p, 1 ≤ (cid:96) ≤ n − 1.

(B.1)

Recall that one “natural/canonical” orthonormal basis for the subspace spanned by columns of Y is

(cid:19)

1
p

I

(cid:18)

(cid:16)

Y =

(cid:20) x0
(cid:107)x0(cid:107)2

| Px⊥

0

G

G(cid:62)Px⊥

0

G

(cid:17)−1/2(cid:21)

,

G(cid:48)

.
= Px⊥

0

G

(cid:16)

G(cid:62)Px⊥

0

G

(cid:17)−1/2

which is well-deﬁned with high probability as Px⊥

0

G is well-conditioned (proved in Lemma B.2). We write

for convenience. When p is large, Y has nearly orthonormal columns, and so we expect that Y closely approximates
Y. In this section, we make this intuition rigorous. We prove several results that are needed for the proof of
Theorem II.1, and for translating results for Y to results for Y in Appendix E-D.

For any realization of x0, let I = supp(x0) = {i | x0(i) (cid:54)= 0}. By Bernstein’s inequality in Lemma A.7 with
σ2
X = 2θ and R = 1, the event

.
=

E0

(cid:26) 1
2

θp ≤ |I| ≤ 2θp

(cid:27)

(B.2)

(B.3)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

18

holds with probability at least 1 − 2 exp (−θp/16). Moreover, we show the following:

Lemma B.1. When p ≥ Cn and θ > 1/

n, the bound

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

4

2

(cid:115)

≤

5

n log p
θ2p

holds with probability at least 1 − cp−2. Here C, c are positive constants.

(B.4)

Proof: Because E

(cid:104)

(cid:107)x0(cid:107)2
2

(cid:105)

we have

for all t > 0, which implies

= 1, by Bernstein’s inequality in Lemma A.7 with σ2

X = 2/(θp2) and R = 1/(θp),

P

(cid:104)(cid:12)
(cid:12)(cid:107)x0(cid:107)2
(cid:12)

2 − E

(cid:104)

(cid:107)x0(cid:107)2
2

(cid:105)(cid:12)
(cid:105)
(cid:12)
(cid:12) > t

= P

(cid:104)(cid:12)
(cid:12)(cid:107)x0(cid:107)2
(cid:12)

(cid:12)
(cid:105)
(cid:12)
2 − 1
(cid:12) > t

≤ 2 exp

−

(cid:18)

(cid:19)

θpt2
4 + 2t

(cid:20)
|(cid:107)x0(cid:107)2 − 1| >

P

(cid:21)

t
(cid:107)x0(cid:107)2 + 1

= P [|(cid:107)x0(cid:107)2 − 1| ((cid:107)x0(cid:107)2 + 1) > t] ≤ 2 exp

(cid:18)

−

θpt2
4 + 2t

(cid:19)

.

On the intersection with E0, (cid:107)x0(cid:107)2 + 1 ≥ 1√
2

+ 1 ≥ 5/4 and setting t =

(cid:113) n log p

θ2p , we obtain

(cid:34)

(cid:115)

(cid:35)

4
5
Unconditionally, this implies that with probability at least 1 − 2 exp (−pθ/16) − 2 exp (cid:0)−

−(cid:112)np log p

|(cid:107)x0(cid:107)2 − 1| ≥

n log p
θ2p

≤ 2 exp

(cid:12)
(cid:12)
(cid:12) E0

P

(cid:17)

(cid:16)

.

√

np log p(cid:1), we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

|1 − (cid:107)x0(cid:107)2|
(cid:107)x0(cid:107)2

≤

√
4

2

(cid:115)

5

n log p
θ2p

,

as desired.
.
= (cid:0)G(cid:62)Px⊥

Let M

0

G(cid:1)−1/2

. Then G(cid:48) = GM − x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM. We show the following results hold:

Lemma B.2. Provided p ≥ Cn, it holds that

(cid:107)M(cid:107) ≤ 2,

(cid:107)M − I(cid:107) ≤ 4

+ 4

(cid:114) n
p

(cid:115)

log(2p)
p

with probability at least 1 − (2p)−2. Here C is a positive constant.

Proof: First observe that

(cid:16)

(cid:16)

(cid:107)M(cid:107) =

σmin

G(cid:62)Px⊥

0

G

(cid:17)(cid:17)−1/2

= σ−1
min

(cid:0)Px⊥

0

G(cid:1) .

Now suppose B is an orthonormal basis spanning x⊥
as that of B(cid:62)G ∈ R(p−1)×(n−1); in particular,

0 . Then it is not hard to see the spectrum of Px⊥

G is the same

0

σmin

(cid:0)Px⊥

0

G(cid:1) = σmin

(cid:16)

B(cid:62)G

(cid:17)

.

0, 1
Since each entry of G ∼i.i.d. N
p
spectrum results for Gaussian matrices in Lemma A.13 and obtain that

, and B(cid:62) has orthonormal rows, B(cid:62)G ∼i.i.d. N

(cid:16)

(cid:17)

(cid:17)

(cid:16)

0, 1
p

, we can invoke the

(cid:114) p − 1
p

(cid:114) n − 1
p

−

− 2

log (2p)
p

(cid:115)

≤ σmin

(cid:16)

(cid:17)

B(cid:62)G

≤ σmax

(cid:16)

(cid:17)

B(cid:62)G

≤

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

log(2p)
p

(cid:115)

with probability at least 1 − (2p)−2. Thus, when p ≥ C1n for some sufﬁciently large constant C1, by using the

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

19

results above we have

(cid:107)M(cid:107) = σ−1
min

(cid:16)

(cid:17)

B(cid:62)G

=

(cid:32)(cid:114) p − 1

(cid:114) n − 1
p

−

− 2

p

(cid:115)

(cid:33)−1

log (2p)
p

≤ 2,

(cid:107)I − M(cid:107) = max (|σmax (M) − 1| , |σmin (M) − 1|)
(cid:16)

(cid:17)

(cid:16)

= max

≤ max

B(cid:62)G

min

(cid:16)(cid:12)
(cid:12)σ−1
(cid:12)

(cid:32)(cid:114) p − 1


−

p

− 1

(cid:12)
(cid:12)
(cid:12) ,
(cid:114) n − 1
p

(cid:12)
(cid:12)σ−1
(cid:12)

max

(cid:17)

B(cid:62)G

(cid:115)

− 2

log (2p)
p

(cid:17)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:33)−1

− 1, 1 −

(cid:32)(cid:114) p − 1

(cid:114) n − 1
p

+

+ 2

p

(cid:115)

log(2p)
p

(cid:33)−1




(cid:32)







= max

1 −

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

log (2p)
p

(cid:33) (cid:32)(cid:114) p − 1

(cid:114) n − 1
p

−

− 2

(cid:33)−1

,

log (2p)
p

(cid:32)(cid:114) p − 1

− 1 +

p

(cid:114) n − 1
p

+ 2

log(2p)
p

(cid:33) (cid:32)(cid:114) p − 1

(cid:114) n − 1
p

+

+ 2

log(2p)
p

(cid:33)−1




(cid:115)

(cid:115)

(cid:115)

(cid:115)

p

p

(cid:32)

≤ 2

1 −

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

(cid:115)

(cid:33)

log (2p)
p

(cid:115)

≤ 4

+ 4

(cid:114) n
p

log(2p)
p

,

with probability at least 1 − (2p)−2.

Lemma B.3. Let YI be a submatrix of Y whose rows are indexed by the set I. There exists a constant C > 0,
such that when p ≥ Cn and 1/2 > θ > 1/

√

p,

n, the following
√
(cid:13)
(cid:13)Y(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 3
(cid:107)YI(cid:107)(cid:96)2→(cid:96)1 ≤ 7(cid:112)2θp,
(cid:13)G − G(cid:48)(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 4
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 20

√

(cid:13)
(cid:13)YI − YI

(cid:13)Y − Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 20

n + 7(cid:112)log(2p),
(cid:114)

(cid:114)

,

n log p
θ
n log p
θ

hold simultaneously with probability at least 1 − cp−2 for a positive constant c.

Proof: First of all, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

≤

1
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)(cid:96)2→(cid:96)1

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 GM
(cid:13)(cid:96)2→(cid:96)2

=

2
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)1

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

,

where in the last inequality we have applied the fact (cid:107)M(cid:107) ≤ 2 from Lemma B.2. Now x(cid:62)
0, (cid:107)x0(cid:107)2
vectors with each entry distributed as N
p
Lemma A.9, we have

0 G is an i.i.d. Gaussian
θp . So by Gaussian concentration inequality in

, where (cid:107)x0(cid:107)2

2 = |I|

(cid:16)

(cid:17)

2

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

≤ 2 (cid:107)x0(cid:107)2

(cid:115)

log(2p)
p

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

20

with probability at least 1 − c1p−2. On the intersection with E0, this implies
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2(cid:112)2θ log(2p),

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

with probability at least 1 − c2p−2 provided θ > 1/
that when p ≥ C1n,

n. Moreover, when intersected with E0, Lemma A.14 implies

(cid:107)G(cid:107)(cid:96)2→(cid:96)1 ≤

(cid:107)GI(cid:107)(cid:96)2→(cid:96)1 ≤ (cid:112)2θp

n. Hence, by Lemma B.2, when p > C2n,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1
√

√

p,
√

with probability at least 1 − c3p−2 provided θ > 1/
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)1 (cid:107)I − M(cid:107) +

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2
(cid:33)

√

p

≤

(cid:32)
(cid:114) n
4
p

(cid:115)

+ 4

log(2p)
p

(cid:13)Y(cid:13)
(cid:13)

(cid:13)
(cid:13)G(cid:48)
I

(cid:13)
(cid:13)GI − G(cid:48)
I

(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)x0(cid:107)(cid:96)2→(cid:96)1 + (cid:107)G(cid:107)(cid:96)2→(cid:96)1 ≤ (cid:107)x0(cid:107)1 +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)GI(cid:107)(cid:96)2→(cid:96)1 (cid:107)M(cid:107) +
(cid:13)
(cid:13)(cid:96)2→(cid:96)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)GI(cid:107)(cid:96)2→(cid:96)1 (cid:107)I − M(cid:107) +

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2
(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)
2
(cid:33)

GM

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:115)

≤ (cid:112)2θp

(cid:32)
4

(cid:114) n
p

+ 4

log(2p)
p

+ 2(cid:112)2θ log(2p) ≤ 4

n + 7(cid:112)log(2p),

√

√

p ≤ 2(cid:112)θp +

√

√

p ≤ 3

p,

≤ 2(cid:112)2θp + 2(cid:112)2θ log(2p) ≤ 4(cid:112)2θp,

+ 2(cid:112)2θ log(2p) ≤ 4

2θn + 6(cid:112)2θ log(2p),

√

(cid:107)YI(cid:107)(cid:96)2→(cid:96)1 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

+ (cid:13)

(cid:13)G(cid:48)
I

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤
√

(cid:107)x0(cid:107)1
(cid:107)x0(cid:107)2

+ 6(cid:112)2θp ≤ 7(cid:112)2θp

n. Finally, by Lemma B.1 and the results above, we obtain
(cid:114)

(cid:107)x0(cid:107)1 + (cid:13)

(cid:13)G − G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 20

(cid:107)x0(cid:107)1 + (cid:13)

(cid:13)GI − G(cid:48)
I

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 20

n log p
θ

,

(cid:114)

n log p
θ

,

with probability at least 1 − c4p−2 provided θ > 1/
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)x0(cid:107)2
1
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)YI − YI

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤

(cid:13)Y − Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1 −

holding with probability at least 1 − c5p−2.

Lemma B.4. Provided p ≥ Cn and θ > 1/

√

n, the following
(cid:115)

(cid:13)G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ 2

(cid:114) n
p

+ 8

√

2 log(2p)
p

,

21(cid:112)n log(2p)
p
hold simultaneously with probability at least 1 − cp−2 for some constant c > 0.

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤

2 log(2p)

4n
p

+

+

p

8

Proof: First of all, we have when p ≥ C1n, it holds with probability at least 1 − c2p−2 that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)∞

≤

1
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)(cid:96)2→(cid:96)∞

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 GM
(cid:13)(cid:96)2→(cid:96)2

≤

2
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)∞

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

,

where at the last inequality we have applied the fact (cid:107)M(cid:107) ≤ 2 from Lemma B.2. Moreover, from proof of Lemma B.3,
(cid:13)2 ≤ 2(cid:112)log(2p)/p (cid:107)x0(cid:107)2 with probability at least 1 − c3p−2 provided p ≥ C4n. Therefore,
we know that (cid:13)

0 G(cid:13)

(cid:13)x(cid:62)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

21

conditioned on E0, we obtain that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
holds with probability at least 1 − c5p−2 provided θ > 1/

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)∞

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

4 (cid:107)x0(cid:107)∞
(cid:107)x0(cid:107)2

GM

≤

√

(cid:115)

log(2p)
p

≤

4(cid:112)2 log(2p)
√
θp

n. Now by Corollary A.11, we have that

(cid:107)G(cid:107)(cid:96)2→(cid:96)∞ ≤

+ 2

(cid:114) n
p

(cid:115)

2 log(2p)
p

with probability at least 1 − c6p−2. Combining the above estimates and Lemma B.2, we have that with probability
at least 1 − c7p−2

where the last simpliﬁcation is provided that θ > 1/

n and p ≥ C8n for a sufﬁciently large C8. Similarly,

(cid:13)G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ (cid:107)GM(cid:107)(cid:96)2→(cid:96)∞ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

GM

(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
2
(cid:13)
(cid:13)
x0x(cid:62)
(cid:13)
(cid:13)
0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
(cid:13)
2
4(cid:112)2 log(2p)
√
θp

GM

+

≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)∞ (cid:107)M(cid:107) +

(cid:115)

≤ 2

+ 4

(cid:114) n
p

2 log(2p)
p

√

(cid:115)

≤ 2

+ 8

(cid:114) n
p

2 log(2p)
p

,

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)∞ (cid:107)I − M(cid:107) +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(8

GM

(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
2
√
2 + 4)(cid:112)n log(2p)

√
8

√
8

≤

≤

4n
p

4n
p

+

+

p

p

2 log(2p)

2 log(2p)

+

+

p

21(cid:112)n log(2p)
p

,

+

4(cid:112)2 log(2p)
√
θp

completing the proof.

APPENDIX C
PROOF OF (cid:96)1/(cid:96)2 GLOBAL OPTIMALITY

In this appendix, we prove the (cid:96)1/(cid:96)2 global optimality condition in Theorem II.1 of Section II.

Proof of Theorem II.1: We will ﬁrst analyze a canonical version, in which the input orthonormal basis is Y as

deﬁned in (III.6) of Section III:

Let q =

and let I be the support set of x0, we have

(cid:21)

(cid:20)q1
q2

min
q∈Rn

(cid:107)Yq(cid:107)1 ,

s.t. (cid:107)q(cid:107)2 = 1.

(cid:13)G(cid:48)

Iq2

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

I cq2

(cid:13)
(cid:13)1

− (cid:13)

≥ |q1|

(cid:107)Yq(cid:107)1 = (cid:107)YIq(cid:107)1 + (cid:107)YI cq(cid:107)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2
x0
(cid:107)x0(cid:107)2
x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≥ |q1|

≥ |q1|

− (cid:107)GIq2(cid:107)1 − (cid:13)
(cid:13)

(cid:0)GI − G(cid:48)

I

(cid:1) q2

(cid:13)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)
(cid:13)
(cid:0)GI c − G(cid:48)
(cid:13)

I c

(cid:1) q2

(cid:13)
(cid:13)1

− (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

(cid:13)G − G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2 ,

where G and G(cid:48) are deﬁned in (B.1) and (B.2) of Appendix B. By Lemma A.14 and intersecting with E0 deﬁned

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

22

in (B.3), we have that as long as p ≥ C1n,

(cid:107)GIq2(cid:107)1 ≤

(cid:107)q2(cid:107)2 = 2θ

p (cid:107)q2(cid:107)2 for all q2 ∈ Rn−1,

√

(cid:107)GI cq2(cid:107)1 ≥

(cid:107)q2(cid:107)2 =

p (1 − 2θ) (cid:107)q2(cid:107)2 for all q2 ∈ Rn−1,

√

1
2

2θp
√
p
p − 2θp
√
p

1
2

hold with probability at least 1 − c2p−2. Moreover, by Lemma B.3,

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 4

√

n + 7(cid:112)log(2p)
√

holds with probability at least 1 − c3p−2 when p ≥ C4n and θ > 1/
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107)Yq(cid:107)1 ≥ g(q)

+ (cid:107)q2(cid:107)2

.
= |q1|

(cid:18) 1
2

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

√

holds with probability at least 1 − c5p−2. Assuming E0, we observe

n. So we obtain that

p (1 − 2θ) − 2θ

p − 4

√

√

n − 7(cid:112)log(2p)

(cid:19)

Now g(q) is a linear function in |q1| and (cid:107)q2(cid:107)2. Thus, whenever θ is sufﬁciently small and p ≥ C6n such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≤ (cid:112)|I|

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ (cid:112)2θp.

(cid:112)2θp <

√

1
2

p (1 − 2θ) − 2θ

p − 4

√

√

n − 7(cid:112)log(2p),

±e1 are the unique minimizers of g(q) under the constraint q2
g(±e1), and we have

1 + (cid:107)q2(cid:107)2

2 = 1. In this case, because (cid:107)Y(±e1)(cid:107)1 =

(cid:107)Yq(cid:107)1 ≥ g(q) > g(±e1)

for all q (cid:54)= ±e1, ±e1 are the unique minimizers of (cid:107)Yq(cid:107)1 under the spherical constraint. Thus there exists a
universal constant θ0 > 0, such that for all 1/
n ≤ θ ≤ θ0, ±e1 are the only global minimizers of (II.2) if the
input basis is Y.

√

Any other input basis can be written as (cid:98)Y = YU, for some orthogonal matrix U. The program now is written as

which is equivalent to

which is obviously equivalent to the canonical program we analyzed above by a simple change of variable, i.e.,
q

.
= Uq, completing the proof.

min
q∈Rn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Yq
(cid:13)1

,

s.t. (cid:107)q(cid:107)2 = 1,

min
q∈Rn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Yq
(cid:13)1

,

s.t. (cid:107)Uq(cid:107)2 = 1,

APPENDIX D
GOOD INITIALIZATION

In this appendix, we prove Proposition IV.3. We show that the initializations produced by the procedure described

Proof of Proposition IV.3: Our previous calculation has shown that θp/2 ≤ |I| ≤ 2θp with probability at least
n. Let Y = (cid:2)y1, · · · , yp(cid:3)(cid:62) as deﬁned in (III.6). Consider any i ∈ I.

√

in Section III are biased towards the optimal.

1 − c1p−2 provided p ≥ C2n and θ > 1/
Then x0(i) = 1√
θp , and

(cid:10)e1, yi/ (cid:13)

(cid:13)yi(cid:13)
(cid:13)2

(cid:11) =

√

1/

θp

(cid:107)x0(cid:107)2 (cid:107)yi(cid:107)2

(cid:107)x0(cid:107)2 ((cid:107)x0(cid:107)∞ / (cid:107)x0(cid:107)2 + (cid:107)(g(cid:48))i(cid:107)2)

≥

≥

√

1/

θp

√

1/

θp

(cid:107)x0(cid:107)2 ((cid:107)x0(cid:107)∞ / (cid:107)x0(cid:107)2 + (cid:107)gi(cid:107)2 + (cid:107)G − G(cid:48)(cid:107)(cid:96)2→(cid:96)∞)

,

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

23

where gi and (g(cid:48))i are the i-th rows of G and G(cid:48), respectively. Since such gi’s are independent Gaussian vectors
in Rn−1 distributed as N (0, 1/p), by Gaussian concentration inequality and the fact that |I| ≥ pθ/2 w.h.p.,

provided p ≥ C5n and θ > 1/

n. Moreover,

(cid:104)

P

∃i ∈ I : (cid:13)
√

(cid:13)gi(cid:13)

(cid:13)2 ≤ 2(cid:112)n/p

(cid:105)

≥ 1 − exp (−c3nθp) ≤ c4p−2,

(cid:114)

(cid:114)

(cid:107)x0(cid:107)2 =

|I| ×

≤

2θp ×

1
θp

√

=

2.

1
θp

Combining the above estimates and result of Lemma B.4, we obtain that provided p ≥ C6n and θ > 1/
probability at least 1 − c7p−2, there exists an i ∈ [p], such that if we set q(0) = yi/ (cid:13)
(cid:13)2, it holds that

(cid:13)yi(cid:13)

(cid:12)
(cid:12)q(0)
(cid:12)

1

(cid:12)
(cid:12)
(cid:12) ≥

√

2(cid:112)n/p +

√

(cid:16)

2

4n/p + 8

2 log(2p)/p + 21(cid:112)n log(2p)/p

(cid:17)

√

1/

θp
√

√

n, with

(using p ≥ C6n to simpliﬁy the above line)

θp
√
2(cid:112)n/p

√

1/

√

1/

1 + 6

θp + 2
√

1/

θp + 6
1
√

√
2
1
√

2)

(1 + 6
1
√

10

θn

,

≥

=

≥

≥

θn

√

θn

√

(as θ > 1/

n)

completing the proof.

We will next show that for an arbitrary orthonormal basis (cid:98)Y

.
= YU the initialization still biases towards the target
solution. To see this, suppose w.l.o.g. (cid:0)yi(cid:1)(cid:62) is a row of Y with nonzero ﬁrst coordinate. We have shown above that
if Y is the input orthonormal basis. For Y, as x0 = Ye1 = YUU(cid:62)e1,
with high probability
10
we know q(cid:63) = U(cid:62)e1 is the target solution corresponding to (cid:98)Y. Observing that

(cid:69)(cid:12)
(cid:12) ≥ 1
(cid:12)

(cid:68) yi
(cid:107)yi(cid:107)2

, e1

(cid:12)
(cid:12)
(cid:12)

θn

√

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

U(cid:62)e1,

(cid:17)(cid:62)

(cid:16)

e(cid:62)
i (cid:98)Y

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

e(cid:62)
i (cid:98)Y

(cid:17)(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:12)
(cid:12)
(cid:43)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

U(cid:62)e1,

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

U(cid:62)Y(cid:62)ei
(cid:107)U(cid:62)Y(cid:62)ei(cid:107)2

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

e1,

(Y)(cid:62) ei
(cid:107)Y(cid:62)ei(cid:107)2

(cid:43)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

e1,

yi
(cid:107)yi(cid:107)2

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

1
√

,

10

nθ

corroborating our claim.

APPENDIX E
LOWER BOUNDING FINITE SAMPLE GAP G(q)

In this appendix, we prove Proposition IV.4. In particular, we show that the gap G(q) deﬁned in (IV.8) is strictly

positive over a large portion of the sphere Sn−1.

Proof of Proposition IV.4: Without loss of generality, we work with the “canonical” orthonormal basis Y
deﬁned in (III.6). Recall that Y is the orthogonalization of the planted sparse basis Y as deﬁned in (III.5). We
deﬁne the processes Q(q) and Q(q) on q ∈ Sn−1, via

Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

, Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

.

1
p

p
(cid:88)

i=1

1
p

p
(cid:88)

i=1

24

(E.1)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

Thus, we can separate Q(q) as Q(q) =

, where

(cid:21)

(cid:20) Q1(q)
Q2(q)

Q1(q) =

x0iSλ

(cid:104)

q(cid:62)yi(cid:105)

and Q2(q) =

(cid:104)

q(cid:62)yi(cid:105)

,

giSλ

1
p

p
(cid:88)

i=1

1
p

p
(cid:88)

i=1

and separate Q(q) correspondingly. Our task is to lower bound the gap G(q) for ﬁnite samples as deﬁned in (IV.8).
√
Since we can deterministically constrain |q1| and (cid:107)q2(cid:107)2 over the set Γ as deﬁned in (IV.7) (e.g.,
θ
and (cid:107)q2(cid:107)2 ≥ 1
10 for q2 is arbitrary here, as we can always take a sufﬁciently small θ), the
challenge lies in lower bounding |Q1 (q)| and upper bounding (cid:107)Q2 (q)(cid:107)2, which depend on the orthonormal basis
Y. The unnormalized basis Y is much easier to work with than Y. Our proof will follow the observation that
(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)

10 , where the choice of 1

≤ |q1| ≤ 3

1
√

nθ

10

|Q1 (q)| ≥ (cid:12)
(cid:107)Q2 (q)(cid:107) ≤ (cid:13)

(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)

(cid:12) − (cid:12)
(cid:13)2 + (cid:13)

(cid:12) − (cid:12)
(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)

(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12) ,
(cid:13)2 + (cid:13)

(cid:13)Q2 (q) − Q2 (q)(cid:13)

(cid:13)2 .

In particular, we show the following:

• Appendix E-A shows that the expected gap is lower bounded for all q ∈ Sn−1 with |q1| ≤ 3
(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

q2
1
θp

1
50

G (q)

.
=

−

≥

.

√

θ:

As |q1| ≥ 1
√
10

nθ

, we have

• Appendix E-B, as summarized in Proposition E.8, shows that whenever p ≥ Ω (cid:0)n4 log n(cid:1), it holds with high

probability that

(cid:12)
(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|

−

(cid:13)
(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

inf
q∈Γ

≥

1
5000

1
θ2np

.

(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

+

sup
q∈Γ

√

10

θn

≤

4 × 105θ5/2n3/2p

+

10
4 × 105θ2np

(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2
1
2 × 104θ2np

=

.

• Appendix E-D shows that whenever p ≥ Ω (cid:0)n4 log n(cid:1), it holds with high probability that
(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12)
(cid:12)
|q1|

(cid:13)Q2 (q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

sup
q∈Γ

+

√

10

θn

≤

4 × 105θ5/2n3/2p

+

10
4 × 105θ2np

=

1
2 × 104θ2np

.

Observing that

inf
q∈Γ

G(q) ≥ inf
q∈Γ

(cid:32) (cid:12)

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

−

(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|
(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12)
|q1|

(cid:32) (cid:12)

+

− sup
q∈Γ

(cid:13)Q2 (q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

(cid:33)

,

(cid:33)

(cid:32) (cid:12)

(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|

+

(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

(cid:33)

− sup
q∈Γ

we obtain the result as desired.

For the general case when the input orthonormal basis is (cid:98)Y = YU with target solution q(cid:63) = U(cid:62)e1, a

straightforward extension of the deﬁnition for the gap would be:
(cid:13)
(cid:13)
(cid:13)

, U(cid:62)e1

Q

(cid:69)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:68)

(cid:16)

(cid:16)

(cid:17)

G

q; (cid:98)Y = YU

(cid:17) .
=

−

q; (cid:98)Y
|(cid:104)q, U(cid:62)e1(cid:105)|

(cid:16)

(cid:0)I − U(cid:62)e1e(cid:62)

1 U(cid:1) Q
(cid:13)
(cid:0)I − U(cid:62)e1e(cid:62)
(cid:13)

q; (cid:98)Y
1 U(cid:1) q(cid:13)
(cid:13)2

(cid:17)(cid:13)
(cid:13)
(cid:13)2

.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

25

(cid:16)

(cid:17)

Since Q

q; (cid:98)Y

= 1
p

(cid:80)p

k=1 U(cid:62)ykSλ

(cid:0)q(cid:62)U(cid:62)yk(cid:1), we have

(cid:16)

(cid:17)

UQ

q; (cid:98)Y

=

UU(cid:62)ykSλ

(cid:16)

q(cid:62)U(cid:62)yk(cid:17)

=

ykSλ

(cid:104)

(Uq)(cid:62) yk(cid:105)

= Q (Uq; Y) .

(E.2)

1
p

p
(cid:88)

k=1

1
p

p
(cid:88)

k=1

Hence we have

(cid:16)

G

q; (cid:98)Y = YU

=

(cid:17)

|(cid:104)Q (Uq; Y) , e1(cid:105)|
|(cid:104)Uq, e1(cid:105)|

−

(cid:13)
(cid:0)I − e1e(cid:62)
(cid:13)

1

(cid:1) Q (Uq; Y)(cid:13)
(cid:13)2
(cid:1) Uq(cid:13)
(cid:13)2

1

.

(cid:13)
(cid:0)I − e1e(cid:62)
(cid:13)

Therefore, from Proposition IV.4 above, we conclude that under the same technical conditions as therein,

q∈Sn−1:

1
√

10

θn

inf
≤|(cid:104)Uq,e1(cid:105)|≤3

√

θ

(cid:16)

(cid:17)

G

q; (cid:98)Y

≥

1
104θ2np

with high probability.

A. Lower Bounding the Expected Gap G(q)

In this section, we provide a nontrivial lower bound for the gap

G(q) =

(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

−

(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

.

More speciﬁcally, we show that:

for all q ∈ Sn−1 with |q1| ≤ 3

θ.

√

G(q) ≥

1
50

q2
1
θp

Proposition E.1. There exists some numerical constant θ0 > 0, such that for all θ ∈ (0, θ0), it holds that

(E.3)

(E.4)

Estimating the gap G(q) requires delicate estimates for E (cid:2)Q1(q)(cid:3) and E (cid:2)Q2(q)(cid:3). We ﬁrst outline the main

proof in Appendix E-A1, and delay these detailed technical calculations to the subsequent subsections.

1) Sketch of the Proof: W.l.o.g., we only consider the situation that q1 > 0, because the case of q1 < 0 can be

similarly shown by symmetry. By (E.1), we have

E (cid:2)Q1(q)(cid:3) = E
E (cid:2)Q2(q)(cid:3) = E

(cid:104)

(cid:104)

(cid:104)

(cid:105)(cid:105)

,

x0Sλ
(cid:104)

gSλ

x0q1 + q(cid:62)
2 g
(cid:105)(cid:105)

x0q1 + q(cid:62)
2 g

,

where g ∼ N

(cid:16)

(cid:17)
0, 1
p I

, and x0 ∼ 1√

θp Ber(θ). Let us decompose

g = g(cid:107) + g⊥,

with g(cid:107) = P(cid:107)g = q2q(cid:62)
2
(cid:107)q2(cid:107)2
2

g, and g⊥ = (I − P(cid:107))g. In this notation, we have

E (cid:2)Q2(q)(cid:3) = E
= E

(cid:104)

(cid:104)

g(cid:107)Sλ

x0q1 + q(cid:62)

(cid:105)(cid:105)

(cid:104)

+ E

2 g(cid:107)
(cid:105)(cid:105)

(cid:104)

g⊥Sλ
(cid:104)

x0q1 + q(cid:62)

2 g(cid:107)

(cid:105)(cid:105)

(cid:104)

Sλ

x0q1 + q(cid:62)
2 g

(cid:105)(cid:105)

g(cid:107)Sλ
q2
(cid:107)q2(cid:107)2
2

E

=

x0q1 + q(cid:62)
2 g

+ E [g⊥] E
(cid:105)(cid:105)

(cid:104)

q(cid:62)

2 gSλ

x0q1 + q(cid:62)
2 g

,

(cid:104)

(cid:104)

(cid:104)

where we used the facts that q(cid:62)
and E [g⊥] = 0. Let Z

.
= g(cid:62)q2 ∼ N (0, σ2) with σ2 = (cid:107)q2(cid:107)2

2 g(cid:107), g⊥ and g(cid:107) are uncorrelated Gaussian vectors and therefore independent,
2 /p, by partial evaluation of the expectations with

2 g = q(cid:62)

26

(E.5)

(E.6)

(E.7)

(E.8)

(E.9)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

respect to x0, we get

E (cid:2)Q1(q)(cid:3) =

E (cid:2)Q2(q)(cid:3) =

(cid:21)(cid:21)

,

(cid:115)

(cid:20)
Sλ

E

θ
p

θq2
(cid:107)q2(cid:107)2
2

E

(cid:20) q1√
+ Z
θp
(cid:20) q1√
θp

(cid:20)
ZSλ

(cid:21)(cid:21)

+ Z

+

(1 − θ)q2
(cid:107)q2(cid:107)2
2

E [ZSλ [Z]] .

Straightforward integration based on Lemma A.1 gives a explicit form of the expectations as follows

E (cid:2)Q1(q)(cid:3) =

E (cid:2)Q2(q)(cid:3) =

(cid:115)

(cid:26)(cid:20)

θ
p

(cid:16)

αΨ

−

(cid:17)

α
σ

(cid:19)(cid:21)

(cid:20)

(cid:18)

(cid:19)

+ βΨ

+ σ

ψ

(cid:16)

− ψ

−

(cid:17)(cid:21)(cid:27)

,

α
σ

(cid:26) 2 (1 − θ)
p

(cid:18)

(cid:19)

Ψ

−

+

λ
σ

θ
p

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)(cid:27)

q2,

−

β
σ
(cid:18) β
σ

(cid:18) β
σ
(cid:20)

where the scalars α and β are deﬁned as

α =

+ λ,

β =

− λ,

q1√
θp

q1√
θp

and ψ (t) and Ψ (t) are pdf and cdf for standard normal distribution, respectively, as deﬁned in Lemma A.1. Plugging
(E.7) and (E.8) into (E.3), by some simpliﬁcations, we obtain

G(q) =

1
q1

+

(cid:115)

σ
q1

(cid:20)

θ
p
(cid:115)

(cid:16)

(cid:17)

αΨ

(cid:20)

ψ

θ
p

−

α
σ
(cid:18) β
σ

(cid:19)

(cid:16)

− ψ

−

(cid:17)(cid:21)

.

α
σ

+ βΨ

(cid:19)

(cid:18) β
σ

−

2q1√
θp

(cid:18)

(cid:19)(cid:21)

Ψ

−

λ
σ

−

θ
p

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

(cid:19)

(cid:18) β
σ

+ Ψ

− 2Ψ

−

(cid:18)

(cid:19)(cid:21)

λ
σ

With λ = 1/

p and σ2 = (cid:107)q2(cid:107)2

√

2 /p = (1 − q2
δ + 1
(cid:112)1 − q2

,

1)/p, we have
β
σ

=

= −

1

δ − 1
(cid:112)1 − q2

1

,

λ
σ

=

1
(cid:112)1 − q2

1

,

−

α
σ
√

√

θ for q1 ≤ 3

where δ = q1/
More speciﬁcally, we approximate Ψ (cid:0)− α
(cid:1) and ψ (cid:0)− α
σ
σ
−1 + δ. Applying the estimates for the relevant quantities established in Lemma E.2, we obtain

θ. To proceed, it is natural to consider estimating the gap G(q) by Taylor’s expansion.
(cid:1) around −1 − δ, and approximate Ψ
around

and ψ

(cid:16) β
σ

(cid:16) β
σ

(cid:17)

(cid:17)

G(q) ≥

Φ1(δ) −

Φ2(δ) +

1
δp

1 − θ
p

ψ(−1)q2

1 +

(cid:18)

√

σ

p +

(cid:19)

− 1

θ
2

1
p

(cid:2)1 + δ2 − θδ2 − σ (cid:0)1 + δ2(cid:1) √

p(cid:3) q2

1η1 (δ) +

η1 (δ) −

σ
√

δ

p

1 − θ
p

+

1
2δp

η2(δ)q2
1
√

θq3
1

5CT

p

(δ + 1)3 ,

where we deﬁne

Φ1(δ) = Ψ(−1 − δ) + Ψ(−1 + δ) − 2Ψ(−1),
η1(δ) = ψ(−1 + δ) − ψ(−1 − δ),

Φ2(δ) = Ψ(−1 + δ) − Ψ(−1 − δ),
η2(δ) = ψ(−1 + δ) + ψ(−1 − δ),

and CT is as deﬁned in Lemma E.2. Since 1 − σ
2p η2(δ), and (cid:0)1 + δ2(cid:1) (cid:0)1 − σ
θq2
1

p(cid:1) q2

√

1η1 (δ) / (2δp), and using the fact that δ = q1/

√

p ≥ 0, dropping those small positive terms q2

1

p (1 − θ)ψ(−1),

G(q) ≥

Φ1(δ) −

[Φ2(δ) − σ

pη1(δ)] −

(1 − σ

√

1 − θ
p
1 − θ
p

1
δp
1
δp

≥

Φ1(δ) −

[Φ2(δ) − η1(δ)] −

q2
1
p
η1 (δ)
δ

q2
1
p

√

p) η2(δ) −
(cid:18) 2θ
√
2π

+

2

√

θ
2p
3θ2
√
2π

−

q2
1
θp

q3
1η1 (δ) −
(cid:19)

+ C1θ2

,

√

θ, we obtain
√

C1

θq3
1

p

max

(cid:18) q3
1
θ3/2

(cid:19)

, 1

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

27

1 ≥
1 to simplify the expression. Substituting the estimates in Lemma E.4 and use the fact δ (cid:55)→ η1 (δ) /δ is bounded,

for some constant C1 > 0, where we have used q1 ≤ 3
1 − q2
we obtain

θ to simplify the bounds and the fact σ

√

p = (cid:112)1 − q2

√

G (p) ≥

(cid:18) 1
40
(cid:18) 1
40

1
p
q2
1
θp

−

θ

1
√
2π
1
√
2π

−

≥

(cid:19)

δ2 −

q2
1
θp

(cid:0)c1θ + c2θ2(cid:1)
(cid:19)

θ − c1θ − c2θ2

for some positive constants c1 and c2. We obtain the claimed result once θ0 is made sufﬁciently small.

θ. There exists some universal constant CT > 0 such that we have the follow polynomial

2) Auxiliary Results Used in the Proof:

√

.
Lemma E.2. Let δ
= q1/
approximations hold for all q1 ∈ (cid:0)0, 1
2
(cid:17)
α
σ
(cid:18) β
σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

ψ

ψ

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:1):

(cid:20)

−

1 −

(1 + δ)2q2
1

(cid:19)

(cid:20)

−

1 −

(δ − 1)2q2
1

≤ CT (1 + δ)2 q4
1,

≤ CT (δ − 1)2 q4
1,

1
2

1
2
1
2

1
2

(cid:21)

(cid:21)

(cid:12)
(cid:12)
ψ(−1 − δ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ψ(δ − 1)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

ψ(−1)q2
1

1
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

Ψ

−

(cid:17)

α
σ

(cid:20)

(cid:19)

(cid:20)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ψ

(cid:18) β
σ

−

Ψ(−1 − δ) −

ψ(−1 − δ)(1 + δ)q2
1

≤ CT (1 + δ)2 q4
1,

−

Ψ(δ − 1) +

ψ(δ − 1)(δ − 1)q2
1

≤ CT (δ − 1)2 q4
1,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:19)

(cid:20)

Ψ

−

−

Ψ(−1) −

λ
σ

≤ CT q4
1.

Proof: First observe that for any q1 ∈ (cid:0)0, 1

(cid:1) it holds that

2

0 ≤

1
(cid:112)1 − q2

1

(cid:18)

−

1 +

(cid:19)

q2
1
2

≤ q4
1.

Hence we have

So we have

(δ − 1)

1 +

≤ (δ − 1)

1 +

, when δ ≥ 1

−(1 + δ)

1 +

1 + q4
q2
1

≤ −

≤ −(1 + δ)

1 +

(cid:18)

(cid:18)

1
2
(cid:18)

1
2

(cid:19)

(cid:19)

(cid:19)

1
2

q2
1

α
σ

≤

≤

β
σ
β
σ

(cid:19)(cid:19)

(cid:18)

(cid:18)

(cid:19)

1
2

q2
1

,

(cid:19)

1 + q4
q2
1
(cid:19)

q2
1

(cid:18)

1
2
1
2

(cid:18)

(δ − 1)

1 +

1 + q4
q2
1

≤ (δ − 1)

1 +

, when δ ≤ 1.

(cid:18)

(cid:18)

ψ

−(1 + δ)

1 +

1
2

1 + q4
q2
1

(cid:16)

≤ ψ

−

(cid:17)

α
σ

≤ ψ

−(1 + δ)

1 +

(cid:18)

(cid:19)(cid:19)

.

1
2

q2
1

By Taylor expansion of the left and right sides of the above two-side inequality around −1 − δ using Lemma A.2,
we obtain

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

ψ

−

(cid:17)

α
σ

− ψ(−1 − δ) −

(1 + δ)2q2

≤ CT (1 + δ)2 q4
1,

1
2

(cid:12)
(cid:12)
1ψ(−1 − δ)
(cid:12)
(cid:12)

for some numerical constant CT > 0 sufﬁciently large. In the same way, we can obtain other claimed results.

Lemma E.3. For any δ ∈ [0, 3], it holds that

Φ2(δ) − η1(δ) ≥

η1 (3)
9

δ3 ≥

1
20

δ3.

(E.10)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

28

Proof: Let us deﬁne

h(δ) = Φ2(δ) − η1(δ) − Cδ3

for some C > 0 to be determined later. Then it is obvious that h(0) = 0. Direct calculation shows that

d
dδ

d
dδ

d
dδ

Φ1(δ) = η1(δ),

Φ2(δ) = η2(δ),

η1(δ) = η2(δ) − δη1(δ).

(E.11)

Thus, to show (E.10), it is sufﬁcient to show that h(cid:48)(δ) ≥ 0 for all δ ∈ [0, 3]. By differentiating h(δ) with respect to
δ and use the results in (E.11), it is sufﬁcient to have

h(cid:48)(δ) = δη1(δ) − 3Cδ2 ≥ 0 ⇐⇒ η1(δ) ≥ 3Cδ

for all δ ∈ [0, 3]. We obtain the claimed result by observing that δ (cid:55)→ η1 (δ) /3δ is monotonically decreasing over
δ ∈ [0, 3] as justiﬁed below.
Consider the function

To show it is monotonically decreasing, it is enough to show p(cid:48) (δ) is always nonpositive for δ ∈ (0, 3), or equivalently

p (δ)

.
=

η1 (δ)
3δ

=

1
√
2π

3

(cid:18)

exp

−

δ2 + 1
2

(cid:19) eδ − e−δ
δ

.

g (δ)

.
=

(cid:16)

eδ + e−δ(cid:17)

δ − (cid:0)δ2 + 1(cid:1) (cid:16)

eδ − e−δ(cid:17)

≤ 0

for all δ ∈ (0, 3), which can be easily veriﬁed by noticing that g (0) = 0 and g(cid:48) (δ) ≤ 0 for all δ ≥ 0.

Lemma E.4. For any δ ∈ [0, 3], we have

(1 − θ)Φ1(δ) −

[Φ2(δ) − η1(δ)] ≥

1
δ

(cid:18) 1
40

−

1
√
2π

θ

(cid:19)

δ2.

(E.12)

Proof: Let us deﬁne

g(δ) = (1 − θ)Φ1(δ) −

[Φ2(δ) − η1(δ)] − c0 (θ) δ2,

1
δ

where c0 (θ) > 0 is a function of θ. Thus, by the results in (E.11) and L’Hospital’s rule, we have

Φ2(δ)
δ

lim
δ→0

= lim
δ→0

η2 (δ) = 2ψ(−1),

[η2(δ) − δη1(δ)] = 2ψ(−1).

η1(δ)
δ

lim
δ→0

= lim
δ→0

Combined that with the fact that Φ1(0) = 0, we conclude g (0) = 0. Hence, to show (E.12), it is sufﬁcient to show
that g(cid:48)(δ) ≥ 0 for all δ ∈ [0, 3]. Direct calculation using the results in (E.11) shows that

Since η1 (δ) /δ is monotonically decreasing as shown in Lemma E.3, we have that for all δ ∈ (0, 3)

g(cid:48)(δ) =

1
δ2 [Φ2(δ) − η1(δ)] − θη1(δ) − 2c0 (θ) δ.

Using the above bound and the main result from Lemma E.3 again, we obtain

η1 (δ) ≤ δ lim
δ→0

η (δ)
δ

≤

2
√
2π

δ.

g(cid:48)(δ) ≥

1
20

δ −

2
√
2π

θδ − 2c0δ.

Choosing c0 (θ) = 1

40 − 1√

2π

θ completes the proof.

B. Finite Sample Concentration

In the following two subsections, we estimate the deviations around the expectations E (cid:2)Q1(q)(cid:3) and E (cid:2)Q2(q)(cid:3),
i.e., (cid:12)
(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:13)2, and show that the total deviations ﬁt into the gap G(q) we
derived in Appendix E-A. Our analysis is based on the scalar and vector Bernstein’s inequalities with moment
conditions. Finally, in Appendix E-C, we uniformize the bound by applying the classical discretization argument.

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:12) and (cid:13)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

29

1) Concentration for Q1(q):

Lemma E.5 (Bounding (cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12)). For each q ∈ Sn−1, it holds for all t > 0 that
(cid:18)

(cid:19)

P (cid:2)(cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) ≥ t(cid:3) ≤ 2 exp

−

θp3t2
8 + 4pt

.

Proof: By (E.1), we know that

Q1(q) =

X 1

k , X 1

k = x0(k)Sλ [x0(k)q1 + Zk]

1
p

p
(cid:88)

k=1

where Zk = q(cid:62)

2 gk ∼ N

(cid:16)

0, (cid:107)q2(cid:107)2
p

2

(cid:17)

. Thus, for any m ≥ 2, by Lemma A.4, we have

E (cid:2)(cid:12)

(cid:12)X 1
k

(cid:12)
(cid:12)

m(cid:3) ≤ θ

E

E

(cid:19)l

m(cid:21)

(cid:19)m

(cid:19)m m
(cid:88)

(cid:20)(cid:12)
(cid:12)
(cid:18) 1
q1√
(cid:12)
(cid:12)
√
+ Zk
(cid:12)
(cid:12)
θp
θp
(cid:12)
(cid:12)
(cid:19) (cid:18) q1√
(cid:18)m
(cid:18) 1
√
l
θp
θp
(cid:18) 1
(cid:19) (cid:18) q1√
(cid:18)m
√
l
θp
θp
(cid:19)m (cid:18) q1√
(cid:18) 1
(cid:107)q2(cid:107)2√
√
p
θp
θp
(cid:19)m−2
(cid:19)m

l=0
(cid:19)m m
(cid:88)

(cid:19)l

l=0

+

θ

(cid:19)m

= θ

= θ

≤

m!
2

(cid:104)

|Zk|m−l(cid:105)

(m − l − 1)!!

(cid:19)m−l

(cid:18) (cid:107)q2(cid:107)2√

p

(cid:18) 2
θp
X = 4/(θp2) and R = 2/(θp), apply Lemma A.7, we get

(cid:18) 2
θp

4
θp2

m!
2

m!
2

≤

=

θ

let σ2

P (cid:2)(cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) ≥ t(cid:3) ≤ 2 exp

(cid:18)

−

θp3t2
8 + 4pt

(cid:19)

.

as desired.

2) Concentration for Q2(q):

Lemma E.6 (Bounding (cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

P (cid:2)(cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2). For each q ∈ Sn−1, it holds for all t > 0 that
(cid:18)
(cid:13)2 > t(cid:3) ≤ 2(n + 1) exp

θp3t2
√
128n + 16

θnpt

−

(cid:19)

.

Before proving Lemma E.6, we record the following useful results.

Lemma E.7. For any positive integer s, l > 0, we have

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

s
(cid:13)
(cid:13)
2

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

≤

(l + s)!!
2

(cid:107)q2(cid:107)l
2

(2
(cid:0)√

√
n)s
p(cid:1)s+l .

In particular, when s = l, we have

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

l
(cid:13)
(cid:13)
2

(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

l!
2

(cid:107)q2(cid:107)l
2

√

(cid:18) 4

(cid:19)l

n

p

≤

(cid:17)

2

Proof: Let Pq(cid:107)

= q2q(cid:62)
2
(cid:107)q2(cid:107)2
2
complement, respectively. By Lemma A.4, we have
gk(cid:13)
(cid:13)
(cid:13)2

(cid:20)(cid:16)(cid:13)
(cid:13)
(cid:13)Pq(cid:107)

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

and Pq⊥

I − 1

≤ E

s
(cid:13)
(cid:13)
2

(cid:107)q2(cid:107)2
2

l(cid:21)

=

E

2

2

q2q(cid:62)
2

(cid:16)

+

(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)2

2 gk(cid:12)
(cid:17)s (cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

denote the projection operators onto q2 and its orthogonal

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

30

Using Lemma A.5 and the fact that (cid:13)

(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)2

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

s
(cid:13)
(cid:13)
2

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

≤ (cid:107)q2(cid:107)l
2

=

=

s
(cid:88)

i=0
s
(cid:88)

i=0

(cid:18)s
i
(cid:18)s
i

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)

(cid:21)

i

(cid:21)

i

2

2

E

E

≤ (cid:107)q2(cid:107)l
2

s
(cid:88)

i=0

(cid:21)

s−i

l (cid:13)
(cid:13)
(cid:13)Pq(cid:107)

2

gk(cid:13)
(cid:13)
(cid:13)
2

l+s−i(cid:21)

1
(cid:107)q2(cid:107)s−i
2

(cid:19)l+s−i

(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:21) (cid:18) 1
√
p

(l + s − i − 1)!!.

2

i

2

E

(cid:19)

(cid:19)i

s
(cid:88)

gk(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

(cid:13)2, we obtain
(cid:19) (cid:18) √
√

(cid:18)s
i
(cid:13)gk(cid:13)
≤ (cid:13)
(cid:18)s
i
(cid:19)l (l + s)!!
2
√
n)s
p(cid:1)s+l .

i=0
(cid:18) 1
√
p

(cid:107)q2(cid:107)l
2

(2
(cid:0)√

n
p

i!!

≤ (cid:107)q2(cid:107)l
2

≤

(l + s)!!
2

(l + s − i − 1)!!

(cid:19)l+s−i

(cid:18) 1
√
p

(cid:18) √
√

n
p

+

1
√
p

(cid:19)s

Now, we are ready to prove Lemma E.6,

Proof: By (E.1), note that

Q2 =

X2

k, X2

k = gkSλ [x0(k)q1 + Zk]

1
p

p
(cid:88)

k=1

where Zk = q(cid:62)

E (cid:2)(cid:13)

(cid:13)X2
k

(cid:13)
(cid:13)

m
2

2 gk. Thus, for any m ≥ 2, by Lemma E.7, we have
(cid:104)(cid:13)
(cid:13)

+ (1 − θ)E

(cid:3) ≤ θE

+ q(cid:62)

m(cid:21)

m

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

m

(cid:13)
(cid:13)

l (cid:13)
(cid:13)

2 gk
(cid:13)gk(cid:13)
(cid:19) (m + l)!!
2
q1√
θp

+

p

m

(cid:13)gk(cid:13)

(cid:13)
(cid:13)
2

m(cid:105)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:13)gk(cid:13)
(cid:104)(cid:13)
(cid:13)

(cid:13)
(cid:13)
2

m

m−l

q1√
θp

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (1 − θ)E

(cid:18) (cid:107)q2(cid:107)2√

p

(cid:19)l (cid:12)
(cid:12)
(cid:12)
(cid:12)

q1√
θp

(cid:12)
(cid:12)
(cid:12)
(cid:12)

m−l

(cid:19)m

+ (1 − θ)

m!
2

(cid:107)q2(cid:107)m
2

(cid:18) 4

√

n

p

m!
2
(cid:19)m

m(cid:105)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

+ (1 − θ)

(cid:107)q2(cid:107)m
2

√

(cid:18) 4

(cid:19)m

n

p

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:19)
(cid:18)m
l

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

m
(cid:88)

q1√
θp
(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

(cid:18)m
l

l=0

(cid:19)m (cid:18) (cid:107)q2(cid:107)2√

≤ θ

≤ θ

≤ θ

≤

(cid:19)m m
(cid:88)

l=0
√
(cid:18) 2
√

n
p
√
(cid:18) 4
√
√
(cid:18) 8
√

n
p
(cid:19)m
n
θp

m!
2
m!
2

.

√

√

Taking σ2

X = 64n/(θp2) and R = 8

n/(

θp) and using vector Bernstein’s inequality in Lemma A.8, we obtain

P (cid:2)(cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 ≥ t(cid:3) ≤ 2(n + 1) exp

(cid:18)

−

θp3t2
√
128n + 16

θnpt

(cid:19)

,

as desired.

C. Union Bound
Proposition E.8 (Uniformizing the Bounds). Suppose that θ > 1/
C (ξ), such that whenever p ≥ C (ξ) n4 log n, we have

√

n. Given any ξ > 0, there exists some constant

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤

,

2ξ
θ5/2n3/2p
2ξ
θ2np

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

31

hold uniformly for all q ∈ Sn−1, with probability at least 1 − c(ξ)p−2 for a positive constant c(ξ).

Proof: We apply the standard covering argument. For any ε ∈ (0, 1), by Lemma A.12, the unit hemisphere of

interest can be covered by an ε-net Nε of cardinality at most (3/ε)n. For any q ∈ Sn−1, it can be written as

, which is an independent copy of y = [x0, g](cid:62).

q = q(cid:48) + e

where q(cid:48) ∈ Nε and (cid:107)e(cid:107)2 ≤ ε. Let a row of Y be yk = (cid:2)x0(k), gk(cid:3)(cid:62)
By (E.1), we have
(cid:12)
(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)
(cid:110)

(cid:69)(cid:105)

(cid:104)(cid:68)

(cid:104)(cid:68)

(cid:104)

yk, q(cid:48) + e

− E

x0(k)Sλ

yk, q(cid:48) + e

x0(k)Sλ

(cid:69)(cid:105)(cid:105)(cid:111)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

p
(cid:88)

k=1

1
p

p
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
p
(cid:12)
(cid:12)E (cid:2)x0Sλ

k=1
p
(cid:88)

k=1

=

≤

+ (cid:12)

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48) + e

(cid:69)(cid:105)

−

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48)(cid:69)(cid:105)

+

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48)(cid:69)(cid:105)

− E (cid:2)x0Sλ

(cid:2)(cid:10)y, q(cid:48)(cid:11)(cid:3)(cid:3)

p
(cid:88)

1
p

(cid:2)(cid:10)y, q(cid:48)(cid:11)(cid:3)(cid:3) − E (cid:2)x0Sλ

k=1
(cid:2)(cid:10)y, q(cid:48) + e(cid:11)(cid:3)(cid:3)(cid:12)
(cid:12) .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Using Cauchy-Schwarz inequality and the fact that Sλ [·] is a nonexpansive operator, we have

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ (cid:12)

(cid:12)Q1(q(cid:48)) − E (cid:2)Q1(q(cid:48))(cid:3)(cid:12)

(cid:12) +

|x0(k)|

p
(cid:88)

(cid:32)

1
p

k=1
1
√
θp

(cid:33)

(cid:107)e(cid:107)2

(cid:13)
(cid:13)

(cid:13)yk(cid:13)
+ E [|x0| (cid:107)y(cid:107)2]
(cid:13)
(cid:13)2
(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)

≤ (cid:12)

(cid:12)Q1(q(cid:48)) − E (cid:2)Q1(q(cid:48))(cid:3)(cid:12)

(cid:18) 2
√
θp
(cid:13)2 ≤ (cid:112)n/p + 2(cid:112)2 log(2p)/p with probability at least 1 − c1p−3. Also E [(cid:107)g(cid:107)2] ≤
≤ (cid:112)n/p. Taking t = ξθ−5/2n−3/2p−1 in Lemma E.5 and applying a union bound with ε =

+ E [(cid:107)g(cid:107)2]

+ max
k∈[p]

(cid:13)gk(cid:13)
(cid:13)

(cid:12) + ε

(cid:19)

.

By Lemma A.10, maxk∈[p]
(cid:105)(cid:17)1/2
(cid:16)

(cid:104)

E

(cid:107)g(cid:107)2
2

ξθ−2n−2(log 2p)−1/2/7, and combining with the above estimates, we obtain that

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

ξ
θ5/2n3/2p

+

ξ
7θ5/2n2(cid:112)log(2p)p

√

(cid:16)
4

(cid:17)
n + 2(cid:112)2 log(2p)

≤

2ξ
θ5/2n3/2p

holds for all q ∈ Sn−1, with probability at least

1 − c1p−3 − 2 exp (cid:0)−c3 (ξ) p/(θ4n3) + c4 (ξ) n log n + c5(ξ)n log log(2p)(cid:1) .

Similarly, by (E.1), we have

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 =

(cid:104)(cid:68)

gkSλ

yk, q(cid:48) + e

− E (cid:2)gSλ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
p

p
(cid:88)

(cid:110)

k=1

(cid:69)(cid:105)

(cid:32)

(cid:13)
(cid:13)
(cid:2)(cid:10)y, q(cid:48) + e(cid:11)(cid:3)(cid:3)(cid:111)
(cid:13)
(cid:13)
(cid:13)2

≤ (cid:13)

(cid:13)Q2(q(cid:48)) − E (cid:2)Q2(q(cid:48))(cid:3)(cid:13)

≤ (cid:13)

(cid:13)Q2(q(cid:48)) − E (cid:2)Q2(q(cid:48))(cid:3)(cid:13)

(cid:33)

p
(cid:88)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

k=1

(cid:13)2 +

1
p
(cid:20)

+ E [(cid:107)g(cid:107)2 (cid:107)y(cid:107)2]

(cid:13)yk(cid:13)
(cid:13)
(cid:13)2
(cid:18) 1
√
θp
(cid:13)2, and taking t = ξθ−2n−1p−1 in Lemma E.6 and applying a union

(cid:13)gk(cid:13)
(cid:13)
(cid:13)2
(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

(cid:107)e(cid:107)2
√
√

(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

+ max
k∈[p]

(cid:13)2 + ε

max
k∈[p]

n
θp

n
p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

+

(cid:19)

(cid:21)

.

Applying the above estimates for maxk∈[p]
bound with ε = ξθ−2n−2 log−1(2p)/30, we obtain that

(cid:13)gk(cid:13)
(cid:13)

(cid:13)
(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 ≤

ξ
θ2np

ξ
θ2np

+

+

≤

ξ
30θ2n2 log(2p)

ξ
30θ2n2 log(2p)

+

(cid:32)(cid:114) n
p



4

(cid:26) 16 log(2p)
p

(cid:27)

+

10n
p

(cid:115)

2 log(2p)
p

(cid:33)2

+

2n
p






IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

32

≤

2ξ
θ2np

holds for all q ∈ Sn−1, with probability at least

1 − c1p−3 − exp (cid:0)−c6 (ξ) p/(θ3n3) + c7(ξ)n log n + c8(ξ)n log log(2p)(cid:1) .

Taking p ≥ C9(ξ)n4 log n and simplifying the probability terms complete the proof.

D. Q(q) approximates Q(q)

Proposition E.9. Suppose θ > 1/
p ≥ C (ξ) n4 log n, the following bounds

√

n. For any ξ > 0, there exists some constant C (ξ), such that whenever

(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)

(cid:12) ≤

sup
q∈Sn−1

sup
q∈Sn−1

(cid:13)
(cid:13)Q2(q) − Q2(q)(cid:13)

(cid:13)2 ≤

ξ
θ5/2n3/2p

ξ
θ2np

,

(E.13)

(E.14)

hold with probability at least 1 − c(ξ)p−2 for a positive constant c(ξ).

Proof: First, for any q ∈ Sn−1, from (E.1), we know that
(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)
(cid:12)
p
(cid:88)

p
(cid:88)

(cid:104)

(cid:104)

x0(k)Sλ

q(cid:62)yk(cid:105)

−

q(cid:62)yk(cid:105)

x0(k)
(cid:107)x0(cid:107)2

Sλ

=

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

1
p

1
p

k=1
p
(cid:88)

k=1
p
(cid:88)

k=1

1
p

1
p

k=1
p
(cid:88)

k=1
(cid:104)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

p
(cid:88)

k=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|x0(k)|

1 −

1
p

p
(cid:88)

k=1

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

−

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

+

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

−

|x0(k)|

(cid:12)
(cid:12)
(cid:12)Sλ

(cid:104)

q(cid:62)yk(cid:105)

− Sλ

q(cid:62)yk(cid:105)(cid:12)
(cid:12)
(cid:12) +

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)Sλ

(cid:104)

q(cid:62)yk(cid:105)(cid:12)
(cid:12)
(cid:12) .

1
p

p
(cid:88)

k=1

x0(k)
(cid:107)x0(cid:107)2

(cid:104)

q(cid:62)yk(cid:105)

Sλ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

For any I = supp(x0), using the fact that Sλ[·] is a nonexpansive operator, we have

(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)

(cid:12) ≤

sup
q∈Sn−1

1
p

sup
q∈Sn−1

(cid:88)

k∈I
(cid:18)

=

√

1
θp3/2

|x0(k)|

(cid:12)
(cid:12)

(cid:12)q(cid:62) (cid:16)

yk − yk(cid:17)(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

(cid:13)
(cid:13)YI − YI

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 +

1 −

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)YI(cid:107)(cid:96)2→(cid:96)1

.

(cid:88)

k∈I

sup
q∈Sn−1
(cid:19)

|x0(k)|

(cid:12)
(cid:12)

(cid:12)q(cid:62)yk(cid:12)

(cid:12)
(cid:12)

By Lemma B.1 and Lemma B.3 in Appendix B, we have the following holds

(cid:12)
(cid:12)Q1(q) − Q1(q)(cid:12)

(cid:12) ≤

sup
q∈Sn−1

√

1
θp3/2

(cid:32)

(cid:114)

20

√

4

2

(cid:115)

n log p
θ

+

5

n log p
θ2p

(cid:33)

× 7(cid:112)2θp

≤

32
θp3/2

(cid:112)n log p,

with probability at least 1 − c1p−2, provided p ≥ C2n and θ > 1/
n. Simple calculation shows that it is enough
to have p ≥ C3 (ξ) n4 log n for some sufﬁciently large C1 (ξ) to obtain the claimed result in (E.13). Similarly, by
Lemma B.3 and Lemma B.4 in Appendix B, we have

√

sup
q∈Sn−1

= sup

q∈Sn−1

≤ sup

q∈Sn−1

(cid:104)

1
p

p
(cid:88)

(cid:13)
(cid:13)Q2(q) − Q2(q)(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1
p
(cid:88)

gkSλ

gkSλ

1
p

(cid:104)

k=1

q(cid:62)yk(cid:105)

q(cid:62)yk(cid:105)

−

−

1
p

1
p

p
(cid:88)

k=1
p
(cid:88)

k=1

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
p

p
(cid:88)

k=1

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

−

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

1
p

p
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

33

p
(cid:88)

k=1

sup
q∈Sn−1
(cid:13)G − G(cid:48)(cid:13)
(cid:0)(cid:13)
(cid:32)

≤

≤

≤

1
p

1
p

1
p

(cid:12)
(cid:12)

(cid:13)
(cid:13)

(cid:12)q(cid:62)yk(cid:12)
(cid:13)gk − g(cid:48)k(cid:13)
(cid:12)
(cid:13)
(cid:12) +
(cid:13)2
(cid:13)(cid:96)2→(cid:96)1 + (cid:13)
(cid:13)Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞

1
p

sup
q∈Sn−1

(cid:13)G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)∞

(cid:12)
(cid:12)

(cid:12)q(cid:62) (cid:16)

yk − yk(cid:17)(cid:12)
(cid:12)
(cid:12)

p
(cid:88)

(cid:13)
(cid:13)

(cid:13)g(cid:48)k(cid:13)
(cid:13)
(cid:13)2
k=1
(cid:13)Y − Y(cid:13)
(cid:13)
√

(cid:1)

(cid:13)(cid:96)2→(cid:96)1
n, (cid:112)log(2p))

(cid:33)

120 max(n, log(2p))
√
p

+

300(cid:112)n log(2p) max(
θp

√

with probability at least 1 − c4p−2 provided p ≥ C4n and θ > 1/
obtain the claimed result (E.14).

≤

420(cid:112)n log(2p) max(
θ1/2p3/2

√

n, (cid:112)log(2p))

√

n. It is sufﬁcient to have p ≥ C5 (ξ) n4 log n to

APPENDIX F
LARGE |q1| ITERATES STAYING IN SAFE REGION FOR ROUNDING

In this appendix, we prove Proposition IV.5 in Section IV.

Proof of Proposition IV.5: For notational simplicity, w.l.o.g. we will proceed to prove assuming q1 > 0. The

proof for q1 < 0 is similar by symmetry. It is equivalent to show that

(cid:107)Q2 (q)(cid:107)2
|Q1 (q)|

<

(cid:114) 1
4θ

− 1,

which is implied by

for any q ∈ Sn−1 satisfying q1 > 3
(cid:115)

L (q)

.
=

(cid:13)
(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 + (cid:13)
E (cid:2)Q1 (q)(cid:3) − (cid:12)
√
θ. Recall from (E.7) that

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)2
(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)

(cid:114) 1
4θ

<

− 1

E (cid:2)Q1(q)(cid:3) =

(cid:26)(cid:20)

(cid:16)

αΨ

−

(cid:17)

α
σ

θ
p

+ βΨ

(cid:19)(cid:21)

(cid:20)

+ σ

ψ

(cid:18) β
σ

(cid:19)

(cid:18) β
σ

(cid:16)

− ψ

−

(cid:17)(cid:21)(cid:27)

,

α
σ

where

α =

1
√
p

(cid:18) q1√
θ

(cid:19)

+ 1

,

β =

1
√
p

(cid:18) q1√
θ

(cid:19)

− 1

,

σ = (cid:107)q2(cid:107)2 /

√

p.

Noticing the fact that
(cid:18) β
σ

ψ

(cid:19)

(cid:16)

− ψ

≥ 0,

(cid:17)

(cid:19)

−

α
σ
(cid:18) β
σ

we have

Ψ

= Ψ

− 1

≥ Ψ (2) ≥

for q1 > 3

θ,

(cid:32)

1
(cid:112)1 − q2

1

(cid:18) q1√
θ

(cid:19)(cid:33)

19
20

√

E (cid:2)Q1 (q)(cid:3) ≥

√

θ
p

(cid:26) q1√
θ

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)

(cid:18) β
σ

+ Ψ

−

− Ψ

(cid:16)

(cid:17)

α
σ

(cid:19)(cid:27)

≥

(cid:18) β
σ

√
2

θ

p

(cid:19)

(cid:18) β
σ

Ψ

≥

√

19
10

θ
p

.

Moreover, from (E.8), we have

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 = (cid:107)q2(cid:107)2

(cid:26) 2 (1 − θ)
p

(cid:18)

(cid:19)

Ψ

−

+

λ
σ

θ
p

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)(cid:27)

≤

2 (1 − θ)
p

θ
p

Ψ (−1) +

[Ψ (−1) + 1] ≤

Ψ (−1) +

≤

+

2
p

2
5p

θ
p

,

(cid:18) β
σ
θ
p

where we have used the fact that −λ/σ ≤ −1 and −α/σ ≤ −1. Moreover, from results in Proposition E.8 and
Proposition E.9 in Appendix E, we know that

sup
q∈Sn−1

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ sup
q∈Sn−1

(cid:12)
(cid:12)Q1(q) − Q1(q)(cid:12)

(cid:12) + sup
q∈Sn−1

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

1
2 × 105θ5/2n3/2p

,

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

34

sup
q∈Sn−1

(cid:13)Q(q) − E (cid:2)Q(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤ sup
q∈Sn−1

(cid:13)Q(q) − Q(q)(cid:13)
(cid:13)

(cid:13)2 + sup
q∈Sn−1

(cid:13)Q(q) − E (cid:2)Q(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤

1
2 × 105θ2np

hold with probability at least 1 − c1p−2 provided that p ≥ Ω (cid:0)n4 log n(cid:1). Hence, with high probability, we have

L (q) ≤

2/(5p) + θ/p + (2 × 105θ2np)−1
√

θ/(10p) − (2 × 105θ5/2n3/2p)−1

19

≤

3/5
√

18

θ/10

≤

1
√
3

θ

<

(cid:114) 1
4θ

− 1,

whenever θ is sufﬁciently small. This completes the proof.

Now, keep the notation in Appendix E for general orthonormal basis (cid:98)Y = YU. For any current iterate q ∈ Sn−1
(cid:11)(cid:12)
(cid:12) = |(cid:104)Uq, e1(cid:105)| ≥ 3
(cid:69)(cid:12)
(cid:12)
(cid:12)

that is close enough to the target solution, i.e., (cid:12)
(cid:12)
(cid:12)
(cid:68)
(cid:12)
(cid:12)

(cid:10)q, U(cid:62)e1
(cid:12)
(cid:16)
(cid:68)
(cid:12)
(cid:12)

θ, we have

(cid:69)(cid:12)
(cid:12)
(cid:12)

√

(cid:17)

(cid:16)

(cid:17)

q; (cid:98)Y
(cid:16)

Q
(cid:13)
(cid:13)
(cid:13)Q

q; (cid:98)Y

, U(cid:62)e1
(cid:17)(cid:13)
(cid:13)
(cid:13)2

=

UQ
(cid:13)
(cid:13)
(cid:13)UQ

q; (cid:98)Y
(cid:16)

q; (cid:98)Y

, e1
(cid:17)(cid:13)
(cid:13)
(cid:13)2

=

|(cid:104)Q (Uq; Y) , e1(cid:105)|
(cid:107)Q (Uq; Y)(cid:107)2

,

where we have applied the identity proved in (E.2). Taking Uq ∈ Sn−1 as the object of interest, by Proposition IV.5,
we conclude that

with high probability.

|(cid:104)Q (Uq; Y) , e1(cid:105)|
(cid:107)Q (Uq; Y)(cid:107)2

√

≥ 2

θ

APPENDIX G
BOUNDING ITERATION COMPLEXITY

In this appendix, we prove Proposition IV.6 in Section IV.

Proof of Proposition IV.6: Recall from Proposition IV.4 in Section IV, the gap

G(q) =

|Q1(q)|
|q1|

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2

≥

1
104θ2np

√

holds uniformly over q ∈ Sn−1 satisfying
p ≥ C2n4 log n. The gap G(q) implies that

1
√

10

θn

≤ |q1| ≤ 3

θ, with probability at least 1 − c1p−2, provided

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)

.
=

⇐⇒

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12) ≥

|Q1(q)|
(cid:107)Q (q)(cid:107)2
(cid:114)
|q1|
(cid:107)q2(cid:107)2

(cid:32)

≥

1 −

=⇒

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)

≥ |q1|2

1 +

+

|q1|
104θ2np (cid:107)Q (q)(cid:107)2
|q1|
104θ2np (cid:107)Q (q)(cid:107)2
(cid:33)

|q1| (cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2 (cid:107)Q (q)(cid:107)2
(cid:12)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)
(cid:107)q2(cid:107)2
2
108θ4n2p2 (cid:107)Q (q)(cid:107)2
2

+

.

Given the set Γ deﬁned in (IV.7), now we know that

sup
q∈Γ

(cid:107)Q (q)(cid:107)2 ≤ sup
q∈Γ

(cid:12)E (cid:2)Q1(q)(cid:3) − Q1 (q)(cid:12)
(cid:12)

(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:13)
(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)

+ sup
q∈Γ
(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

≤ sup
q∈Γ

(cid:12) + sup
q∈Sn−1
(cid:13)2 + sup
q∈Sn−1
(cid:12)E (cid:2)Q2(q)(cid:3)(cid:12)
(cid:12)

(cid:12) + sup
q∈Γ

(cid:12) +

1
pn

(cid:13)E (cid:2)Q2(q)(cid:3) − Q2 (q)(cid:13)
(cid:13)

(cid:12) + sup
q∈Sn−1
(cid:13)2 + sup
q∈Sn−1

(cid:12)Q1(q) − Q1 (q)(cid:12)
(cid:12)
(cid:12)
(cid:13)Q2(q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2

with probability at least 1 − c3p−2 provided p ≥ C4n4 log n and θ > 1/
n. Here we have used Proposition E.8 and
Proposition E.9 to bound the magnitudes of the four difference terms. To bound the magnitudes of the expectations,
we have

(cid:12)
(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) =

E

(cid:104)

x0(k)Sλ

x0(k)q1 + q(cid:62)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

1
p

p
(cid:88)

k=1

2 gk(cid:105)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
√
θp

(cid:18) 1
√
θp

(cid:19)

+ E [(cid:107)g(cid:107)2]

≤

√
3
√

n
θp

≤

3n
p

,

√

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

35

(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 =

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

1
p

p
(cid:88)

k=1

(cid:104)

gkSλ

x0(k)q1 + q(cid:62)

2 gk(cid:105)

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
n. Thus, we obtain that

1
√
θp

≤

√

hold uniformly for all q ∈ Γ, provided θ > 1/

E [(cid:107)g(cid:107)2] + E

(cid:107)g(cid:107)2
2

(cid:104)

(cid:105)

≤

3n
p

with probability at least 1 − c3p−2 provided p ≥ C4n4 log n and θ > 1/

n. So we conclude that

(cid:107)Q (q)(cid:107)2 ≤

sup
q∈Γ

3n
p

+

+

≤

3n
p

1
np

7n
p

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)
|q1|

(cid:114)

≥

1 +

1 − 9θ
108 × 72 × θ4n4 .
, we will need at most

Thus, starting with any q ∈ Sn−1 such that |q1| ≥ 1
√
10
√

θn

(cid:16)

3

(cid:17)

θ/

1
√

10

θn

T =

2 log
(cid:16)

log

1 +

1−9θ
108×72×θ4n4

(cid:17) =

log
√

√

n)

2 log (30θ
(cid:16)

1 +

1−9θ
108×72×θ4n4

(cid:17) ≤

√

2 log (30θ

n)

(log 2)

1−9θ
108×72×θ4n4

≤ C5n4 log n

steps to arrive at a q ∈ Sn−1 with | ¯q1| ≥ 3
that log (1 + x) ≥ x log 2 for x ∈ [0, 1] to simplify the ﬁnal result.

θ for the ﬁrst time. Here we have assumed θ0 < 1/9 and used the fact

APPENDIX H
ROUNDING TO THE DESIRED SOLUTION

min
q

(cid:107)Yq(cid:107)1 ,

s.t. (cid:104)q, q(cid:105) = 1.

In this appendix, we prove Proposition IV.7 in Section IV. For convenience, we will assume the notations we

used in Appendix B. Then the rounding scheme can be written as

(H.1)

(H.2)

(H.3)

We will show the rounding procedure get us to the desired solution with high probability, regardless of the particular
orthonormal basis used.

Proof of Proposition IV.7: The rounding program (H.1) can be written as

Consider its relaxation

inf
q

(cid:107)Yq(cid:107)1 ,

s.t. q1q1 + (cid:104)q2, q2(cid:105) = 1.

inf
q

(cid:107)Yq(cid:107)1 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1.

It is obvious that the feasible set of (H.3) contains that of (H.2). So if e1/q1 is the unique optimal solution (UOS)
of (H.3), it is also the UOS of (H.2). Let I = supp(x0), and consider a modiﬁed problem

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

|q1| − (cid:13)

(cid:13)G(cid:48)

Iq2

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

I cq2

(cid:13)
(cid:13)1 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1.

(H.4)

The objective value of (H.4) lower bounds the objective value of (H.3), and are equal when q = e1/q1. So if
q = e1/q1 is the UOS to (H.4), it is also UOS to (H.3), and hence UOS to (H.2) by the argument above. Now
(cid:13)1 ≥ − (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)
(cid:13)
(cid:0)G − G(cid:48)(cid:1) q2
(cid:13)
(cid:13)G − G(cid:48)(cid:13)
≥ − (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

(cid:13)
(cid:13)1
(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2 .

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

(cid:13)G(cid:48)

I cq2

− (cid:13)

Iq2

When p ≥ C1n, by Lemma A.14 and Lemma B.3, we know that

− (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

≥ −

(cid:114) 2
π

6
5

√

2θ

(cid:13)G − G(cid:48)(cid:13)
(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2
(cid:114) 2
24
π
25

p (cid:107)q2(cid:107)2 +

√

(1 − 2θ)

p (cid:107)q2(cid:107)2 − 4

√

n (cid:107)q2(cid:107)2 − 7(cid:112)log(2p) (cid:107)q2(cid:107)2

.
= ζ (cid:107)q2(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

holds with probability at least 1 − c2p−2. Thus, we make a further relaxation of problem (H.2) by

|q1| + ζ (cid:107)q2(cid:107)2 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1,

36

(H.5)

whose objective value lower bounds that of (H.4). By similar arguments, if e1/q1 is UOS to (H.5), it is UOS to (H.2).
At the optimal solution to (H.5), notice that it is necessary to have sign(q1) = sign(q1) and q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 = 1.
So (H.5) is equivalent to

|q1| + ζ (cid:107)q2(cid:107)2 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 = 1.

(H.6)

which is further equivalent to

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q1

|q1| + ζ

x0
(cid:107)x0(cid:107)2

1 − |q1| |q1|
(cid:107)q2(cid:107)2
Notice that the problem in (H.7) is linear in |q1| with a compact feasible set. Since the objective is also monotonic
in |q1|, it indicates that the optimal solution only occurs at the boundary points |q1| = 0 or |q1| = 1/ |q1| Therefore,
q = e1/q1 is the UOS of (H.7) if and only if
1
|q1|

1
|q1|

ζ
(cid:107)q2(cid:107)2

x0
(cid:107)x0(cid:107)2

|q1| ≤

(H.7)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

s.t.

<

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

.

.

,

Since

(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)1

√

≤

2θp conditioned on E0, it is sufﬁcient to have

√

2θp
√
θ
2

≤ ζ =

(cid:32)

(cid:114) 2
π

24
25

√

p

1 −

θ −

9
2

(cid:114) π
2

(cid:114) n
p

25
6

−

175
24

(cid:114) π
2

(cid:115)

(cid:33)

.

log(2p)
p

Therefore there exists a constant θ0 > 0, such that whenever θ ≤ θ0 and p ≥ C3(θ0)n, the rounding returns e1/q1.
A bit of thought suggests one can take a universal C3 for all possible choice of θ0, completing the proof.

When the input basis is (cid:98)Y = YU for some orthogonal matrix U (cid:54)= I, if the ADM algorithm produces some

√

q = U(cid:62)q(cid:48), such that q(cid:48)

1 > 2

θ. It is not hard to see that now the rounding (H.1) is equivalent to

min
q

(cid:107)YUq(cid:107)1 ,

s.t. (cid:10)q(cid:48), Uq(cid:11) = 1.

Renaming Uq, it follows from the above argument that at optimum q(cid:63) it holds that Uq(cid:63) = γe1 for some constant
γ with high probability.

REFERENCES

[1] Q. Qu, J. Sun, and J. Wright, “Finding a sparse vector in a subspace: Linear sparsity using alternating directions,”

in Advances in Neural Information Processing Systems, 2014.

[2] E. J. Cand`es and T. Tao, “Decoding by linear programming,” Information Theory, IEEE Transactions on,

vol. 51, no. 12, pp. 4203–4215, 2005.

[3] D. L. Donoho, “For most large underdetermined systems of linear equations the minimal (cid:96)1-norm solution is
also the sparsest solution,” Communications on pure and applied mathematics, vol. 59, no. 6, pp. 797–829,
2006.

[4] S. T. McCormick, “A combinatorial approach to some sparse matrix problems.,” tech. rep., DTIC Document,

1983.

[5] T. F. Coleman and A. Pothen, “The null space problem i. complexity,” SIAM Journal on Algebraic Discrete

Methods, vol. 7, no. 4, pp. 527–537, 1986.

[6] M. Berry, M. Heath, I. Kaneko, M. Lawo, R. Plemmons, and R. Ward, “An algorithm to compute a sparse

basis of the null space,” Numerische Mathematik, vol. 47, no. 4, pp. 483–504, 1985.

[7] J. R. Gilbert and M. T. Heath, “Computing a sparse basis for the null space,” SIAM Journal on Algebraic

Discrete Methods, vol. 8, no. 3, pp. 446–459, 1987.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

37

[8] I. S. Duff, A. M. Erisman, and J. K. Reid, Direct Methods for Sparse Matrices. New York, NY, USA: Oxford

[9] A. J. Smola and B. Schlkopf, “Sparse greedy matrix approximation for machine learning,” pp. 911–918, Morgan

University Press, Inc., 1986.

Kaufmann, 2000.

[10] T. Kavitha, K. Mehlhorn, D. Michail, and K. Paluch, “A faster algorithm for minimum cycle basis of graphs,”

in 31st International Colloquium on Automata, Languages and Programming, pp. 846–857, Springer, 2004.

[11] L.-A. Gottlieb and T. Neylon, “Matrix sparsiﬁcation and the sparse null space problem,” in Approximation,

Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 205–218, Springer, 2010.

[12] J. Mairal, F. Bach, and J. Ponce, “Sparse modeling for image and vision processing,” arXiv preprint

arXiv:1411.3230, 2014.

[13] D. A. Spielman, H. Wang, and J. Wright, “Exact recovery of sparsely-used dictionaries,” in Proceedings of the

25th Annual Conference on Learning Theory, 2012.

[14] P. Hand and L. Demanet, “Recovering the sparsest element in a subspace,” arXiv preprint arXiv:1310.1654,

[15] J. Sun, Q. Qu, and J. Wright, “Complete dictionary recovery over the sphere,” arXiv preprint arXiv:1504.06785,

[16] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” Journal of computational and

graphical statistics, vol. 15, no. 2, pp. 265–286, 2006.

[17] I. M. Johnstone and A. Y. Lu, “On consistency and sparsity for principal components analysis in high

dimensions,” Journal of the American Statistical Association, vol. 104, no. 486, 2009.

[18] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. Lanckriet, “A direct formulation for sparse pca using

semideﬁnite programming,” SIAM review, vol. 49, no. 3, pp. 434–448, 2007.

[19] R. Krauthgamer, B. Nadler, D. Vilenchik, et al., “Do semideﬁnite relaxations solve sparse PCA up to the

information limit?,” The Annals of Statistics, vol. 43, no. 3, pp. 1300–1322, 2015.

[20] T. Ma and A. Wigderson, “Sum-of-squares lower bounds for sparse pca,” arXiv preprint arXiv:1507.06370,

2013.

2015.

2015.

[21] V. Q. Vu, J. Cho, J. Lei, and K. Rohe, “Fantope projection and selection: A near-optimal convex relaxation of

sparse pca,” in Advances in Neural Information Processing Systems, pp. 2670–2678, 2013.

[22] J. Lei, V. Q. Vu, et al., “Sparsistency and agnostic inference in sparse pca,” The Annals of Statistics, vol. 43,

[23] Z. Wang, H. Lu, and H. Liu, “Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial

no. 1, pp. 299–322, 2015.

time,” arXiv preprint arXiv:1408.5352, 2014.

[24] A. d’Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet, “A direct formulation of sparse PCA using

semideﬁnite programming,” SIAM Review, vol. 49, no. 3, 2007.

[25] Y.-B. Zhao and M. Fukushima, “Rank-one solutions for homogeneous linear matrix equations over the positive

semideﬁnite cone,” Applied Mathematics and Computation, vol. 219, no. 10, pp. 5569–5583, 2013.

[26] Y. Dai, H. Li, and M. He, “A simple prior-free method for non-rigid structure-from-motion factorization,” in
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2018–2025, IEEE, 2012.
[27] G. Beylkin and L. Monz´on, “On approximation of functions by exponential sums,” Applied and Computational

Harmonic Analysis, vol. 19, no. 1, pp. 17–48, 2005.

[28] C. T. Manolis and V. Rene, “Dual principal component pursuit,” arXiv preprint arXiv:1510.04390, 2015.
[29] M. Zibulevsky and B. A. Pearlmutter, “Blind source separation by sparse decomposition in a signal dictionary,”

Neural computation, vol. 13, no. 4, pp. 863–882, 2001.

[30] A. Anandkumar, D. Hsu, M. Janzamin, and S. M. Kakade, “When are overcomplete topic models identiﬁable?
uniqueness of tensor tucker decompositions with structured sparsity,” in Advances in Neural Information
Processing Systems, pp. 1986–1994, 2013.

[31] J. Ho, Y. Xie, and B. Vemuri, “On a nonlinear generalization of sparse coding and dictionary learning,” in

Proceedings of The 30th International Conference on Machine Learning, pp. 1480–1488, 2013.

[32] Y. Nakatsukasa, T. Soma, and A. Uschmajew, “Finding a low-rank basis in a matrix subspace,” CoRR,

vol. abs/1503.08601, 2015.

[33] Q. Berthet and P. Rigollet, “Complexity theoretic lower bounds for sparse principal component detection,” in

Conference on Learning Theory, pp. 1046–1066, 2013.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

38

[34] B. Barak, J. Kelner, and D. Steurer, “Rounding sum-of-squares relaxations,” arXiv preprint arXiv:1312.6652,

2013.

[35] S. B. Hopkins, T. Schramm, J. Shi, and D. Steurer, “Speeding up sum-of-squares for tensor decomposition and

planted sparse vectors,” arXiv preprint arXiv:1512.02337, 2015.

[36] S. Arora, R. Ge, and A. Moitra, “New algorithms for learning incoherent and overcomplete dictionaries,” arXiv

[37] A. Agarwal, A. Anandkumar, and P. Netrapalli, “Exact recovery of sparsely used overcomplete dictionaries,”

preprint arXiv:1308.6273, 2013.

arXiv preprint arXiv:1309.1952, 2013.

[38] A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon, “Learning sparsely used overcomplete

dictionaries via alternating minimization,” arXiv preprint arXiv:1310.7991, 2013.

[39] S. Arora, A. Bhaskara, R. Ge, and T. Ma, “More algorithms for provable dictionary learning,” arXiv preprint

[40] S. Arora, R. Ge, T. Ma, and A. Moitra, “Simple, efﬁcient, and neural algorithms for sparse coding,” arXiv

arXiv:1401.0579, 2014.

preprint arXiv:1503.00778, 2015.

[41] K. G. Murty and S. N. Kabadi, “Some NP-complete problems in quadratic and nonlinear programming,”

Mathematical programming, vol. 39, no. 2, pp. 117–129, 1987.

[42] R. Vershynin, “Introduction to the non-asymptotic analysis of random matrices,” arXiv preprint arXiv:1011.3027,

2010.

May 2011.

[43] R. Basri and D. W. Jacobs, “Lambertian reﬂectance and linear subspaces,” Pattern Analysis and Machine

Intelligence, IEEE Transactions on, vol. 25, no. 2, pp. 218–233, 2003.

[44] E. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal component analysis?,” Journal of the ACM, vol. 58,

[45] V. De la Pena and E. Gin´e, Decoupling: from dependence to independence. Springer, 1999.
[46] M. Talagrand, Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems,

vol. 60. Springer Science & Business Media, 2014.

[47] K. Luh and V. Vu, “Dictionary learning with few samples and matrix concentration,” arXiv preprint

arXiv:1503.08854, 2015.

[48] P. Jain, P. Netrapalli, and S. Sanghavi, “Low-rank matrix completion using alternating minimization,” in
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pp. 665–674, ACM,
2013.

[49] M. Hardt, “On the provable convergence of alternating minimization for matrix completion,” arXiv preprint

arXiv:1312.0925, 2013.

[50] M. Hardt and M. Wootters, “Fast matrix completion without the condition number,” in Proceedings of The

27th Conference on Learning Theory, pp. 638–678, 2014.

[51] M. Hardt, “Understanding alternating minimization for matrix completion,” in Foundations of Computer Science

(FOCS), 2014 IEEE 55th Annual Symposium on, pp. 651–660, IEEE, 2014.

[52] P. Jain and P. Netrapalli, “Fast exact matrix completion with ﬁnite samples,” arXiv preprint arXiv:1411.1087,

2014.

[53] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain, “Non-convex robust pca,” in Advances in

Neural Information Processing Systems, pp. 1107–1115, 2014.

[54] Q. Zheng and J. Lafferty, “A convergent gradient descent algorithm for rank minimization and semideﬁnite

programming from random linear measurements,” arXiv preprint arXiv:1506.06081, 2015.

[55] S. Tu, R. Boczar, M. Soltanolkotabi, and B. Recht, “Low-rank solutions of linear matrix equations via procrustes

ﬂow,” arXiv preprint arXiv:1507.03566, 2015.

[56] Y. Chen and M. J. Wainwright, “Fast low-rank estimation by projected gradient descent: General statistical

and algorithmic guarantees,” arXiv preprint arXiv:1509.03025, 2015.

[57] P. Jain and S. Oh, “Provable tensor factorization with missing data,” in Advances in Neural Information

Processing Systems, pp. 1431–1439, 2014.

[58] A. Anandkumar, R. Ge, and M. Janzamin, “Guaranteed non-orthogonal tensor decomposition via alternating

rank-1 updates,” arXiv preprint arXiv:1402.5180, 2014.

[59] A. Anandkumar, R. Ge, and M. Janzamin, “Analyzing tensor power method dynamics: Applications to learning

overcomplete latent variable models,” arXiv preprint arXiv:1411.1488, 2014.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

39

[60] A. Anandkumar, P. Jain, Y. Shi, and U. Niranjan, “Tensor vs matrix methods: Robust tensor decomposition

under block sparse perturbations,” arXiv preprint arXiv:1510.04747, 2015.

[61] R. Ge, F. Huang, C. Jin, and Y. Yuan, “Escaping from saddle points—online stochastic gradient for tensor

decomposition,” in Proceedings of The 28th Conference on Learning Theory, pp. 797–842, 2015.

[62] P. Netrapalli, P. Jain, and S. Sanghavi, “Phase retrieval using alternating minimization,” in Advances in Neural

Information Processing Systems, pp. 2796–2804, 2013.

[63] E. J. Cand`es, X. Li, and M. Soltanolkotabi, “Phase retrieval via wirtinger ﬂow: Theory and algorithms,” arXiv

preprint arXiv:1407.1065, 2014.

[64] Y. Chen and E. J. Candes, “Solving random quadratic systems of equations is nearly as easy as solving linear

systems,” arXiv preprint arXiv:1505.05114, 2015.

[65] J. Sun, Q. Qu, and J. Wright, “A geometric analysis of phase retreival,” arXiv preprint arXiv:1602.06664,

[66] J. Sun, Q. Qu, and J. Wright, “When are nonconvex problems not scary?,” arXiv preprint arXiv:1510.06096,

[67] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive Sensing. Springer, 2013.
[68] T. Figiel, J. Lindenstrauss, and V. D. Milman, “The dimension of almost spherical sections of convex bodies,”

Acta Mathematica, vol. 139, no. 1, pp. 53–94, 1977.

[69] A. Y. Garnaev and E. D. Gluskin, “The widths of a euclidean ball,” in Dokl. Akad. Nauk SSSR, vol. 277,

pp. 1048–1052, 1984.

[70] E. Gluskin and V. Milman, “Note on the geometric-arithmetic mean inequality,” in Geometric aspects of

Functional analysis, pp. 131–135, Springer, 2003.

[71] G. Pisier, The volume of convex bodies and Banach space geometry, vol. 94. Cambridge University Press,

2016.

2015.

1999.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

1

Finding a sparse vector in a subspace: linear
sparsity using alternating directions
Qing Qu, Student Member, IEEE, Ju Sun, Student Member, IEEE, and John Wright, Member, IEEE

6
1
0
2
 
l
u
J
 
0
2
 
 
]
T
I
.
s
c
[
 
 
3
v
9
5
6
4
.
2
1
4
1
:
v
i
X
r
a

Abstract

Is it possible to ﬁnd the sparsest vector (direction) in a generic subspace S ⊆ Rp with dim (S) = n < p? This
problem can be considered a homogeneous variant of the sparse recovery problem, and ﬁnds connections to sparse
dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper,
we focus on a planted sparse model for the subspace: the target sparse vector is embedded in an otherwise random
subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of
nonzero entries in the target sparse vector substantially exceeds O(1/
n). In contrast, we exhibit a relatively simple
nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero
entries is Ω(1). To the best of our knowledge, this is the ﬁrst practical algorithm to achieve linear scaling under
the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g.,
sparse dictionary learning.

√

Sparse vector, Subspace modeling, Sparse recovery, Homogeneous recovery, Dictionary learning, Nonconvex

optimization, Alternating direction method

Index Terms

I. INTRODUCTION
Suppose that a linear subspace S embedded in Rp contains a sparse vector x0 (cid:54)= 0. Given an arbitrary basis of S,
can we efﬁciently recover x0 (up to scaling)? Equivalently, provided a matrix A ∈ R(p−n)×p with Null(A) = S, 1
can we efﬁciently ﬁnd a nonzero sparse vector x such that Ax = 0? In the language of sparse recovery, can we
solve

min
x

(cid:107)x(cid:107)0

s.t. Ax = 0, x (cid:54)= 0

?

In contrast to the standard sparse recovery problem (Ax = b, b (cid:54)= 0), for which convex relaxations perform nearly
optimally for broad classes of designs A [2, 3], the computational properties of problem (I.1) are not nearly as well
understood. It has been known for several decades that the basic formulation

(I.1)

(I.2)

min
x

(cid:107)x(cid:107)0 ,

s.t. x ∈ S \ {0},

is NP-hard for an arbitrary subspace [4, 5]. In this paper, we assume a speciﬁc random planted sparse model for
the subspace S: a target sparse vector is embedded in an otherwise random subspace. We will show that under the
speciﬁc random model, problem (I.2) is tractable by an efﬁcient algorithm based on nonconvex optimization.

A. Motivation

The general version of Problem (I.2), in which S can be an arbitrary subspace, takes several forms in numerical
computation and computer science, and underlies several important problems in modern signal processing and
machine learning. Below we provide a sample of these applications.
Sparse Null Space and Matrix Sparsiﬁcation: The sparse null space problem is ﬁnding the sparsest matrix N
whose columns span the null space of a given matrix A. The problem arises in the context of solving linear
equality problems in constrained optimization [5], null space methods for quadratic programming [6], and solving

This work was partially supported by grants ONR N00014-13-1-0492, NSF 1343282, NSF 1527809, and funding from the Moore and
Sloan Foundations. Q. Qu, J. Sun and J. Wright are all with the Electrical Engineering Department, Columbia University, New York, NY,
10027, USA (e-mail: {qq2105, js4038, jw2966}@columbia.edu). This paper is an extension of our previous conference version [1].

1 Null(A)

.
= {x ∈ Rp | Ax = 0} denotes the null space of A.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

2

underdetermined linear equations [7]. The matrix sparsiﬁcation problem is of similar ﬂavor, the task is ﬁnding the
sparsest matrix B which is equivalent to a given full rank matrix A under elementary column operations. Sparsity
helps simplify many fundamental matrix operations (see [8]), and the problem has applications in areas such as
machine learning [9] and in discovering cycle bases of graphs [10]. [11] discusses connections between the two
problems and also to other problems in complexity theory.

Sparse (Complete) Dictionary Learning: In dictionary learning, given a data matrix Y, one seeks an approximation
Y ≈ AX, such that A is a representation dictionary with certain desired structure and X collects the representation
coefﬁcients with maximal sparsity. Such compact representation naturally allows signal compression, and also
facilitates efﬁcient signal acquisition and classiﬁcation (see relevant discussion in [12]). When A is required to
be complete (i.e., square and invertible), by linear algebra, we have2 row(Y) = row(X) [13]. Then the problem
reduces to ﬁnding sparsest vectors (directions) in the known subspace row(Y), i.e. (I.2). Insights into this problem
have led to new theoretical developments on complete dictionary learning [13–15].

Sparse Principal Component Analysis (Sparse PCA): In geometric terms, Sparse PCA (see, e.g., [16–18] for
early developments and [19, 20] for discussion of recent results) concerns stable estimation of a linear subspace
spanned by a sparse basis, in the data-poor regime, i.e., when the available data are not numerous enough to allow
one to decouple the subspace estimation and sparsiﬁcation tasks. Formally, given a data matrix Z = U0X0 + E,3
where Z ∈ Rp×n collects column-wise n data points, U0 ∈ Rp×r is the sparse basis, and E is a noise matrix, one is
asked to estimate U0 (up to sign, scale, and permutation). Such a factorization ﬁnds applications in gene expression,
ﬁnancial data analysis and pattern recognition [24]. When the subspace is known (say by the PCA estimator with
enough data samples), the problem again reduces to instances of (I.2) and is already nontrivial4. The full geometric
sparse PCA can be treated as ﬁnding sparse vectors in a subspace that is subject to perturbation.

In addition, variants and generalizations of the problem (I.2) have also been studied in applications regarding
control and optimization [25], nonrigid structure from motion [26], spectral estimation and Prony’s problem [27],
outlier rejection in PCA [28], blind source separation [29], graphical model learning [30], and sparse coding on
manifolds [31]; see also [32] and the references therein.

B. Prior Arts

Despite these potential applications of problem (I.2), it is only very recently that efﬁcient computational surrogates
with nontrivial recovery guarantees have been discovered for some cases of practical interest. In the context of sparse
dictionary learning, Spielman et al. [13] introduced a convex relaxation which replaces the nonconvex problem (I.2)
with a sequence of linear programs:

(cid:96)1/(cid:96)∞ Relaxation:

min
x

(cid:107)x(cid:107)1 ,

s.t. x(i) = 1, x ∈ S, 1 ≤ i ≤ p.

(I.3)

They proved that when S is generated as a span of n random sparse vectors, with high probability (w.h.p.), the
relaxation recovers these vectors, provided the probability of an entry being nonzero is at most θ ∈ O (1/
n). In
the planted sparse model, in which S is formed as direct sum of a single sparse vector x0 and a “generic” subspace,
Hand and Demanet proved that (I.3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales
n) [14]. One might imagine improving these results by tightening the analyses. Unfortunately, the
as θ ∈ O (1/
results of [13, 14] are essentially sharp: when θ substantially exceeds Ω(1/
n), in both models the relaxation (I.3)
provably breaks down. Moreover, the most natural semideﬁnite programming (SDP) relaxation of (I.1),

√

√

√

min
X

(cid:107)X(cid:107)1 ,

s.t.

(cid:68)

A(cid:62)A, X

(cid:69)

= 0, trace[X] = 1, X (cid:23) 0.

(I.4)

also breaks down at exactly the same threshold of θ ∼ O(1/

√

n).5

2Here, row(·) denotes the row space.
3Variants of multiple-component formulations often add an additional orthonormality constraint on U0 but involve a different notation of

sparsity; see, e.g., [16, 21–23].

4[14] has also discussed this data-rich sparse PCA setting.
5This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (with b (cid:54)= 0), in which it is possible to

handle very large fractions of nonzeros (say, θ = Ω(1/ log n), or even θ = Ω(1)) using a very simple (cid:96)1 relaxation [2, 3]

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

3

TABLE I
COMPARISON OF EXISTING METHODS FOR RECOVERING A PLANTED SPARSE VECTOR IN A SUBSPACE

Method
(cid:96)1/(cid:96)∞ Relaxation [14]
SDP Relaxation
SOS Relaxation [34]
Spectral Method [35]
This work

Recovery Condition

θ ∈ O(1/
θ ∈ O(1/

√
√

n)
n)

p ≥ Ω(n2), θ ∈ O(1)
p ≥ Ω(n2poly log(n)), θ ∈ O(1)
p ≥ Ω(n4 log n), θ ∈ O(1)

Time Complexity6
O(n3p log(1/ε))
O (cid:0)p3.5 log (1/ε)(cid:1)
∼ O(p7 log(1/ε)) 7
O (np log(1/(cid:15)))
O(n5p2 log n + n3p log(1/ε))

√

One might naturally conjecture that this 1/

n threshold is simply an intrinsic price we must pay for having an
efﬁcient algorithm, even in these random models. Some evidence towards this conjecture might be borrowed from
the superﬁcial similarity of (I.2)-(I.4) and sparse PCA [16]. In sparse PCA, there is a substantial gap between what
can be achieved with currently available efﬁcient algorithms and the information theoretic optimum [19, 33]. Is
this also the case for recovering a sparse vector in a subspace? Is θ ∈ O (1/
n) simply the best we can do with
efﬁcient, guaranteed algorithms?

√

Remarkably, this is not the case. Recently, Barak et al. introduced a new rounding technique for sum-of-squares
relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p ≥ Ω (cid:0)n2(cid:1) and
θ = Ω(1) [34]. It is perhaps surprising that this is possible at all with a polynomial time algorithm. Unfortunately,
the runtime of this approach is a high-degree polynomial in p (see Table I); for machine learning problems in
which p is often either the feature dimension or the sample size, this algorithm is mostly of theoretical interest only.
However, it raises an interesting algorithmic question: Is there a practical algorithm that provably recovers a sparse
vector with θ (cid:29) 1/

n portion of nonzeros from a generic subspace S?

√

C. Contributions and Recent Developments

In this paper, we address the above problem under the planted sparse model. We allow x0 to have up to θ0p
nonzero entries, where θ0 ∈ (0, 1) is a constant. We provide a relatively simple algorithm which, w.h.p., exactly
recovers x0, provided that p ≥ Ω (cid:0)n4 log n(cid:1). A comparison of our results with existing methods is shown in Table
I. After initial submission of our paper, Hopkins et al. [35] proposed a different simple algorithm based on the
spectral method. This algorithm guarantees recovery of the planted sparse vector also up to linear sparsity, whenever
p ≥ Ω(n2polylog(n)), and comes with better time complexity.8

Our algorithm is based on alternating directions, with two special twists. First, we introduce a special data driven
initialization, which seems to be important for achieving θ = Ω(1). Second, our theoretical results require a second,
linear programming based rounding phase, which is similar to [13]. Our core algorithm has very simple iterations,
of linear complexity in the size of the data, and hence should be scalable to moderate-to-large scale problems.

Besides enjoying the θ ∼ Ω(1) guarantee that is out of the reach of previous practical algorithms, our algorithm
performs well in simulations – empirically succeeding with p ≥ Ω (n polylog(n)). It also performs well empirically
on more challenging data models, such as the complete dictionary learning model, in which the subspace of interest
contains not one, but n random target sparse vectors. This is encouraging, as breaking the O(1/
n) sparsity barrier
with a practical algorithm and optimal guarantee is an important problem in theoretical dictionary learning [36–40].
In this regard, our recent work [15] presents an efﬁcient algorithm based on Riemannian optimization that guarantees
recovery up to linear sparsity. However, the result is based on different ideas: a different nonconvex formulation,
optimization algorithm, and analysis methodology.

√

D. Paper Organization, Notations and Reproducible Research

The rest of the paper is organized as follows. In Section II, we provide a nonconvex formulation and show its
capability of recovering the sparse vector. Section III introduces the alternating direction algorithm. In Section IV,

6All estimates here are based on the standard interior point methods for solving linear and semideﬁnite programs. Customized solvers may

result in order-wise speedup for speciﬁc problems. ε is the desired numerical accuracy.

7Here our estimation is based on the degree-4 SOS hierarchy used in [34] to obtain an initial approximate recovery.
8Despite these improved guarantees in the planted sparse model, our method still produces more appealing results on real imagery data –

see Section V-B for examples.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

4

we present our main results and sketch the proof ideas. Experimental evaluation of our method is provided in
Section V. We conclude the paper by drawing connections to related work and discussing potential improvements
in Section VI. Full proofs are all deferred to the appendix sections.

For a matrix X, we use xi and xj to denote its i-th column and j-th row, respectively, all in column vector form.
.
Moreover, we use x(i) to denote the i-th component of a vector x. We use the compact notation [k]
= {1, . . . , k}
for any positive integer k, and use c or C, and their indexed versions to denote absolute numerical constants. The
scope of these constants are always local, namely within a particular lemma, proposition, or proof, such that the
apparently same constant in different contexts may carry different values. For probability events, sometimes we will
just say the event holds “with high probability” (w.h.p.) if the probability of failure is dominated by p−κ for some
κ > 0.

The codes to reproduce all the ﬁgures and experimental results can be found online at:

https://github.com/sunju/psv.

II. PROBLEM FORMULATION AND GLOBAL OPTIMALITY
We study the problem of recovering a sparse vector x0 (cid:54)= 0 (up to scale), which is an element of a known
subspace S ⊂ Rp of dimension n, provided an arbitrary orthonormal basis Y ∈ Rp×n for S. Our starting point is
the nonconvex formulation (I.2). Both the objective and the constraint set are nonconvex, and hence it is not easy to
optimize over. We relax (I.2) by replacing the (cid:96)0 norm with the (cid:96)1 norm. For the constraint x (cid:54)= 0, since in most
applications we only care about the solution up to scaling, it is natural to force x to live on the unit sphere Sn−1,
giving

min
x

(cid:107)x(cid:107)1 ,

s.t. x ∈ S, (cid:107)x(cid:107)2 = 1.

(II.1)

This formulation is still nonconvex, and for general nonconvex problems it is known to be NP-hard to ﬁnd even
a local minimizer [41]. Nevertheless, the geometry of the sphere is benign enough, such that for well-structured
inputs it actually will be possible to give algorithms that ﬁnd the global optimizer.

The formulation (II.1) can be contrasted with (I.3), in which effectively we optimize the (cid:96)1 norm subject to the
constraint (cid:107)x(cid:107)∞ = 1: because the set {x : (cid:107)x(cid:107)∞ = 1} is polyhedral, the (cid:96)∞-constrained problem immediately
yields a sequence of linear programs. This is very convenient for computation and analysis. However, it suffers
from the aforementioned breakdown behavior around (cid:107)x0(cid:107)0 ∼ p/
n. In contrast, though the sphere (cid:107)x(cid:107)2 = 1 is a
more complicated geometric constraint, it will allow much larger number of nonzeros in x0. Indeed, if we consider
the global optimizer of a reformulation of (II.1):

√

min
q∈Rn

(cid:107)Yq(cid:107)1 ,

s.t.

(cid:107)q(cid:107)2 = 1,

(II.2)

where Y is any orthonormal basis for S, the sufﬁcient condition that guarantees exact recovery under the planted
sparse model for the subspace is as follows:

Theorem II.1 ((cid:96)1/(cid:96)2 recovery, planted sparse model). There exists a constant θ0 > 0, such that if the subspace S
follows the planted sparse model

where gi ∼i.i.d. N (0, 1
n < θ < θ0, then the unique
(up to sign) optimizer q(cid:63) to (II.2), for any orthonormal basis Y of S, produces Yq(cid:63) = ξx0 for some ξ (cid:54)= 0 with
probability at least 1 − cp−2, provided p ≥ Cn. Here c and C are positive constants.

Ber(θ) are all jointly independent and 1/

p I), and x0 ∼i.i.d.

θp

S = span (x0, g1, . . . , gn−1) ⊂ Rp,
1√

√

Hence, if we could ﬁnd the global optimizer of (II.2), we would be able to recover x0 whose number of nonzero
entries is quite large – even linear in the dimension p (θ = Ω(1)). On the other hand, it is not obvious that this
should be possible: (II.2) is nonconvex. In the next section, we will describe a simple heuristic algorithm for
approximately solving a relaxed version of the (cid:96)1/(cid:96)2 problem (II.2). More surprisingly, we will then prove that
for a class of random problem instances, this algorithm, plus an auxiliary rounding technique, actually recovers
the global optimizer – the target sparse vector x0. The proof requires a detailed probabilistic analysis, which is
sketched in Section IV-B.

Before continuing, it is worth noting that the formulation (II.1) is in no way novel – see, e.g., the work of [29]

in blind source separation for precedent. However, our algorithms and subsequent analysis are novel.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

5

III. ALGORITHM BASED ON ALTERNATING DIRECTION METHOD (ADM)
To develop an algorithm for solving (II.2), it is useful to consider a slight relaxation of (II.2), in which we

introduce an auxiliary variable x ≈ Yq:

min
q,x

f (q, x)

.
=

1
2

(cid:107)Yq − x(cid:107)2

2 + λ (cid:107)x(cid:107)1 ,

s.t.

(cid:107)q(cid:107)2 = 1.

(III.1)

Here, λ > 0 is a penalty parameter. It is not difﬁcult to see that this problem is equivalent to minimizing the
Huber M-estimator over Yq. This relaxation makes it possible to apply the alternating direction method to this
problem. This method starts from some initial point q(0), alternates between optimizing with respect to (w.r.t.) x
and optimizing w.r.t. q:

where x(k) and q(k) denote the values of x and q in the k-th iteration. Both (III.2) and (III.3) have simple closed
form solutions:

x(k+1) = arg min

q(k+1) = arg min

1
2
1
2

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)Yq(k) − x
(cid:13)
(cid:13)Yq − x(k+1)(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
2

x

q

+ λ (cid:107)x(cid:107)1 ,

s.t. (cid:107)q(cid:107)2 = 1,

x(k+1) = Sλ[Yq(k)],

q(k+1) =

Y(cid:62)x(k+1)
(cid:13)Y(cid:62)x(k+1)(cid:13)
(cid:13)
(cid:13)2

,

(III.2)

(III.3)

(III.4)

where Sλ [x] = sign(x) max {|x| − λ, 0} is the soft-thresholding operator. The proposed ADM algorithm is
summarized in Algorithm 1.

Algorithm 1 Nonconvex ADM for solving (III.1)

A matrix Y ∈ Rp×n with Y(cid:62)Y = I, initialization q(0), threshold parameter λ > 0.

The recovered sparse vector ˆx0 = Yq(k)

Input:
Output:
1: for k = 0, . . . , O (cid:0)n4 log n(cid:1) do
x(k+1) = Sλ[Yq(k)],
2:
q(k+1) = Y(cid:62)x(k+1)
(cid:107)Y(cid:62)x(k+1)(cid:107)2

,

3:
4: end for

The algorithm is simple to state and easy to implement. However, if our goal is to recover the sparsest vector x0,

some additional tricks are needed.
Initialization. Because the problem (II.2) is nonconvex, an arbitrary or random initialization may not produce a
global minimizer.9 In fact, good initializations are critical for the proposed ADM algorithm to succeed in the linear
sparsity regime. For this purpose, we suggest using every normalized row of Y as initializations for q, and solving
a sequence of p nonconvex programs (II.2) by the ADM algorithm.

To get an intuition of why our initialization works, recall the planted sparse model S = span(x0, g1, . . . , gn−1)

and suppose

Y = [x0 | g1 | · · · | gn−1] ∈ Rp×n.

(III.5)
θp(cid:1). Meanwhile, the entries of
If we take a row yi of Y, in which x0(i) is nonzero, then x0(i) = Θ (cid:0)1/
√
p. Hence, when θ is not too large,
g1(i), . . . gn−1(i) are all N (0, 1/p), and so their magnitude have size about 1/
x0(i) will be somewhat bigger than most of the other entries in yi. Put another way, yi is biased towards the ﬁrst
Y ≈ I.10
standard basis vector e1. Now, under our probabilistic model assumptions, Y is very well conditioned: Y
Using the Gram-Schmidt process11, we can ﬁnd an orthonormal basis Y for S via:

√

(cid:62)

9More precisely, in our models, random initialization does work, but only when the subspace dimension n is extremely low compared to

the ambient dimension p.

10This is the common heuristic that “tall random matrices are well conditioned” [42].
11...QR decomposition in general with restriction that R11 = 1.

Y = YR,

(III.6)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

6

where R is upper triangular, and R is itself well-conditioned: R ≈ I. Since the i-th row yi of Y is biased in
the direction of e1 and R is well-conditioned, the i-th row yi of Y is also biased in the direction of e1. In other
words, with this canonical orthobasis Y for the subspace, the i-th row of Y is biased in the direction of the global
optimizer. The heuristic arguments are made rigorous in Appendix B and Appendix D.

What if we are handed some other basis (cid:98)Y = YU, where U is an arbitary orthogonal matrix? Suppose q(cid:63) is a
global optimizer to (II.2) with the input matrix Y, then it is easy to check that, U(cid:62)q(cid:63) is a global optimizer to
(II.2) with the input matrix (cid:98)Y. Because
(cid:68)

(cid:69)

(cid:68)

(cid:69)

(YU)(cid:62)ei, U(cid:62)q(cid:63)

=

Y(cid:62)ei, q(cid:63)

,

our initialization is invariant to any rotation of the orthobasis. Hence, even if we are handed an arbitrary orthobasis
for S, the i-th row is still biased in the direction of the global optimizer.

Rounding by linear programming (LP). Let q denote the output of Algorithm 1. As illustrated in Fig. 1, we
will prove that with our particular initialization and an appropriate choice of λ, ADM algorithm uniformly moves
towards the optimal over a large portion of the sphere, and its solution falls within a certain small radius of the
globally optimal solution q(cid:63) to (II.2). To exactly recover q(cid:63), or equivalently to recover the exact sparse vector
x0 = γYq(cid:63) for some γ (cid:54)= 0, we solve the linear program

min
q

(cid:107)Yq(cid:107)1

s.t.

(cid:104)r, q(cid:105) = 1

(III.7)

with r = q. Since the feasible set {q | (cid:104)q, q(cid:105) = 1} is essentially the tangent space of the sphere Sn−1 at q, whenever
q is close enough to q(cid:63), one should expect that the optimizer of (III.7) exactly recovers q(cid:63) and hence x0 up to
scale. We will prove that this is indeed true under appropriate conditions.

IV. MAIN RESULTS AND SKETCH OF ANALYSIS

A. Main Results

previous section succeeds.

In this section, we describe our main theoretical result, which shows that w.h.p. the algorithm described in the

Theorem IV.1. Suppose that S obeys the planted sparse model, and let the columns of Y form an arbitrary
orthonormal basis for the subspace S. Let y1, . . . , yp ∈ Rn denote the (transposes of) the rows of Y. Apply
(cid:13)2 , . . . , yp/ (cid:107)yp(cid:107)2, to produce outputs q1, . . . , qp.
Algorithm 1 with λ = 1/
Solve the linear program (III.7) with r = q1, . . . , qp, to produce (cid:98)q1, . . . , (cid:98)qp. Set i(cid:63) ∈ arg mini (cid:107)Y(cid:98)qi(cid:107)1. Then

p, using initializations q(0) = y1/ (cid:13)

(cid:13)y1(cid:13)

√

for some γ (cid:54)= 0 with probability at least 1 − cp−2, provided

Y(cid:98)qi(cid:63) = γx0,

p ≥ Cn4 log n,

and

1
√
n

≤ θ ≤ θ0.

Here C, c and θ0 are positive constants.

(IV.1)

(IV.2)

Remark IV.2. We can see that the result in Theorem IV.1 is suboptimal in sample complexity compared to the
global optimality result in Theorem II.1 and Barak et al.’s result [34] (and the subsequent work [35]). For successful
recovery, we require p ≥ Ω (cid:0)n4 log n(cid:1), while the global optimality and Barak et al. demand p ≥ Ω (n) and
p ≥ Ω (cid:0)n2(cid:1), respectively. Aside from possible deﬁciencies in our current analysis, compared to Barak et al., we
believe this is still the ﬁrst practical and efﬁcient method which is guaranteed to achieve θ ∼ Ω(1) rate. The lower
bound on θ in Theorem IV.1 is mostly for convenience in the proof; in fact, the LP rounding stage of our algorithm
already succeeds w.h.p. when θ ∈ O (1/

n).

√

B. A Sketch of Analysis

In this section, we brieﬂy sketch the main ideas of proving our main result in Theorem IV.1, to show that the
“initialization + ADM + LP rounding” pipeline recovers x0 under the stated technical conditions, as illustrated in

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

7

Fig. 1. An illustration of the proof sketch for our ADM algorithm.

Fig. 1. The proof of our main result requires rather detailed technical analysis of the iteration-by-iteration properties
of Algorithm 1, most of which is deferred to the appendices.

As noted in Section III, the ADM algorithm is invariant to change of basis. So w.l.o.g., let us assume Y =

[x0 | g1 | · · · | gn−1] and let Y to be its orthogonalization, i.e., 12

Y =

(cid:20) x0
(cid:107)x0(cid:107)2

(cid:16)

| Px⊥

0

G

G(cid:62)Px⊥

0

G

(cid:17)−1/2(cid:21)

.

When p is large, Y is nearly orthogonal, and hence Y is very close to Y. Thus, in our proofs, whenever convenient,
we make the arguments on Y ﬁrst and then “propagate” the quantitative results onto Y by perturbation arguments.
With that noted, let y1, · · · , yp be the transpose of the rows of Y, and note that these are all independent random
vectors. To prove the result of Theorem IV.1, we need the following results. First, given the speciﬁed Y, we show
that our initialization is biased towards the global optimum:

Proposition IV.3 (Good initialization). Suppose θ > 1/
that at least one of our p initialization vectors suggested in Section III, say q(0)

n and p ≥ Cn. It holds with probability at least 1 − cp−2
i = yi/ (cid:13)

(cid:13)yi(cid:13)

(cid:13)2, obeys

√

Here C, c are positive constants.

Proof: See Appendix D.

Second, we deﬁne a vector-valued random process Q(q) on q ∈ Sn−1, via

so that based on (III.4), one step of the ADM algorithm takes the form:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28) yi

(cid:107)yi(cid:107)2

, e1

≥

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√

.

10

θn

Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

,

1
p

p
(cid:88)

i=1

q(k+1) =

Q (cid:0)q(k)(cid:1)
(cid:13)Q (cid:0)q(k)(cid:1)(cid:13)
(cid:13)
(cid:13)2

(IV.3)

(IV.4)

(IV.5)

(IV.6)

This is a very favorable form for analysis: the term in the numerator Q (cid:0)q(k)(cid:1) is a sum of p independent random
vectors with q(k) viewed as ﬁxed. We study the behavior of the iteration (IV.6) through the random process Q (cid:0)q(k)(cid:1).

12Note that with probability one, the inverse matrix square-root in Y is well deﬁned. So Y is well deﬁned w.h.p. (i.e., except for x0 = 0).

See more quantitative characterization of Y in Appendix B.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

8

We want to show that w.h.p. the ADM iterate sequence q(k) converges to some small neighborhood of ±e1, so
that the ADM algorithm plus the LP rounding (described in Section III) successfully retrieves the sparse vector
x0/(cid:107)x0(cid:107) = Ye1. Thus, we hope that in general, Q(q) is more concentrated on the ﬁrst coordinate than q ∈ Sn−1. Let
us partition the vector q as q = [q1; q2], with q1 ∈ R and q2 ∈ Rn−1; and correspondingly Q(q) = [Q1(q); Q2(q)].
The inner product of Q(q)/ (cid:107)Q(q)(cid:107)2 and e1 is strictly larger than the inner product of q and e1 if and only if

|Q1(q)|
|q1|

>

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

.

In the following proposition, we show that w.h.p., this inequality holds uniformly over a signiﬁcant portion of the
sphere

(cid:26)

.
=

Γ

q ∈ Sn−1 |

1
√

10

nθ

√

≤ |q1| ≤ 3

θ, (cid:107)q2(cid:107)2 ≥

(cid:27)

,

1
10

so the algorithm moves in the correct direction. Let us deﬁne the gap G(q) between the two quantities |Q1(q)| / |q1|
and (cid:107)Q2(q)(cid:107)2 / (cid:107)q2(cid:107)2 as

(IV.7)

(IV.8)

and we show that the following result is true:

G(q)

.
=

|Q1(q)|
|q1|

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

,

Proposition IV.4 (Uniform lower bound for ﬁnite sample gap). There exists a constant θ0 ∈ (0, 1), such that when
p ≥ Cn4 log n, the estimate

G(q) ≥

inf
q∈Γ

1
104θ2np

√

holds with probability at least 1 − cp−2, provided θ ∈ (1/

n, θ0). Here C, c are positive constants.

Proof: See Appendix E.

√

Next, we show that whenever |q1| ≥ 3
enough for LP rounding (III.7) to succeed.

θ, w.h.p. the iterates stay in a “safe region” with |q1| ≥ 2

θ which is

√

Proposition IV.5 (Safe region for rounding). There exists a constant θ0 ∈ (0, 1), such that when p ≥ Cn4 log n, it
holds with probability at least 1 − cp−2 that

|Q1(q)|
(cid:107)Q(q)(cid:107)2

√
θ

≥ 2

√

√

for all q ∈ Sn−1 satisfying |q1| > 3

θ, provided θ ∈ (1/

n, θ0). Here C, c are positive constants.

Proof: See Appendix F.

In addition, the following result shows that the number of iterations for the ADM algorithm to reach the safe

region can be bounded grossly by O(n4 log n) w.h.p..

Proposition IV.6 (Iteration complexity of reaching the safe region). There is a constant θ0 ∈ (0, 1), such that
when p ≥ Cn4 log n, it holds with probability at least 1 − cp−2 that the ADM algorithm in Algorithm 1, with any
(cid:12)
initialization q(0) ∈ Sn−1 satisfying
(cid:12) ≥ 1
(cid:12)
θ at least once in
√
θn
10
√
at most O(n4 log n) iterations, provided θ ∈ (1/

, will produce some iterate q with |¯q1| > 3
n, θ0). Here C, c are positive constants.

(cid:12)
(cid:12)q(0)
(cid:12)

√

1

Proof: See Appendix G.

Moreover, we show that the LP rounding (III.7) with input r = q exactly recovers the optimal solution w.h.p.,

√

whenever the ADM algorithm returns a solution q with ﬁrst coordinate |q1| > 2

θ.

Proposition IV.7 (Success of rounding). There is a constant θ0 ∈ (0, 1), such that when p ≥ Cn, the following
holds with probability at least 1 − cp−2 provided θ ∈ (1/
n, θ0): Suppose the input basis is Y deﬁned in (IV.3)
and the ADM algorithm produces an output q ∈ Sn−1 with |q1| > 2
θ. Then the rounding procedure with r = q
returns the desired solution ±e1. Here C, c are positive constants.

√

√

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

9

Finally, given p ≥ Cn4 log n for a sufﬁciently large constant C, we combine all the results above to complete the

Proof: See Appendix H.

proof of Theorem IV.1.

Proof of Theorem IV.1:

W.l.o.g., let us again ﬁrst consider Y as deﬁned in (III.5) and its orthogonalization Y in a “natural/canonical”
form (IV.3). We show that w.h.p. our algorithmic pipeline described in Section III exactly recovers the optimal
solution up to scale, via the following argument:

1) Good initializers. Proposition IV.3 shows that w.h.p., at least one of the p initialization vectors, say q(0)

i =

yi/ (cid:13)

(cid:13)yi(cid:13)

(cid:13)2, obeys

(cid:12)
(cid:68)
(cid:12)
(cid:12)

q(0)
i

, e1

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥

1
√

,

10

θn

which implies that q(0)

i

is biased towards the global optimal solution.

√

2) Uniform progress away from the equator. By Proposition IV.4, for any θ ∈ (1/

n, θ0) with a constant

θ0 ∈ (0, 1),

w.h.p.,

G(q) =

|Q1(q)|
|q1|
holds uniformly for all q ∈ Sn−1 in the region
(cid:12)
(cid:12) ≥ 1
(cid:12)
q(0) such that
if sufﬁciently many iterations are allowed.

(cid:12)
(cid:12)q(0)
(cid:12)

θn

10

√

1

1
√

10

θn

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2

1
104θ2np

≥

√

, the ADM algorithm will eventually obtain a point q(k) for which (cid:12)

(cid:12)q(k)(cid:12)

(cid:12) ≥ 3

√

θ,

≤ |q1| ≤ 3

θ w.h.p.. This implies that with an input

(IV.9)

3) No jumps away from the caps. Proposition IV.5 shows that for any θ ∈ (1/

n, θ0) with a constant θ0 ∈ (0, 1),

Q1(q)
(cid:107)Q(q)(cid:107)2

√

≥ 2

θ

√

√

√

θ. This implies that once |q(k)
holds for all q ∈ Sn−1 with |q1| ≥ 3
θ for some iterate k, all the future
iterates produced by the ADM algorithm stay in a “spherical cap” region around the optimum with |q1| ≥ 2
θ.
4) Location of stopping points. As shown in Proposition IV.6, w.h.p., the strictly positive gap G(q) in (IV.9)
ensures that one needs to run at most O (cid:0)n4 log n(cid:1) iterations to ﬁrst encounter an iterate q(k) such that
|q(k)
θ. Hence, the steps above imply that, w.h.p., Algorithm 1 fed with the proposed initialization
θ after O (cid:0)n4 log n(cid:1) steps.
scheme successively produces iterates q ∈ Sn−1 with its ﬁrst coordinate |q1| ≥ 2
θ. Proposition IV.7 proves that w.h.p., the LP rounding (III.7) with an

5) Rounding succeeds when |r1| ≥ 2

1 | ≥ 3

1 | ≥ 3

√

√

√

√

input r = q produces the solution ±x0 up to scale.

Taken together, these claims imply that from at least one of the initializers q(0), the ADM algorithm will produce
an output q which is accurate enough for LP rounding to exactly return x0/(cid:107)x0(cid:107)2. On the other hand, our (cid:96)1/(cid:96)2
optimality theorem (Theorem II.1) implies that ±x0 are the unique vectors with the smallest (cid:96)1 norm among all
unit vectors in the subspace. Since w.h.p. x0/(cid:107)x0(cid:107)2 is among the p unit vectors (cid:98)q1, . . . , (cid:98)qp our p row initializers
ﬁnally produce, our minimal (cid:96)1 norm selector will successfully locate x0/(cid:107)x0(cid:107)2 vector.

For the general case when the input is an arbitrary orthonormal basis (cid:98)Y = YU for some orthogonal matrix U,

the target solution is U(cid:62)e1. The following technical pieces are perfectly parallel to the argument above for Y.

1) Discussion at the end of Appendix D implies that w.h.p., at least one row of (cid:98)Y provides an initial point q(0)

such that (cid:12)
(cid:12)

(cid:10)q(0), U(cid:62)e1

(cid:11)(cid:12)
(cid:12) ≥ 1
√
10

.

θn

2) Discussion following Proposition IV.4 in Appendix E indicates that for all q such that

≤ (cid:12)
(cid:10)q, U(cid:62)e1
(cid:12)
θ, there is a strictly positive gap, indicating steady progress towards a point q(k) such that (cid:12)
(cid:10)q(k), U(cid:62)e1
(cid:12)
θ.

√
3
√
3

1
√

θn

10

3) Discussion at the end of Appendix F implies that once q satisﬁes (cid:12)
(cid:12)

(cid:10)q, U(cid:62)e1

(cid:11)(cid:12)
(cid:12), the next iterate will not move

(cid:11)(cid:12)
(cid:12) ≤
(cid:11)(cid:12)
(cid:12) ≥

far away from the target:

(cid:68)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

Q

q; (cid:98)Y

(cid:16)

(cid:17)

(cid:13)
(cid:13)
(cid:13)Q

/

q; (cid:98)Y

(cid:17)(cid:13)
(cid:13)
(cid:13)2

, U(cid:62)e1

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥ 2

√

θ.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

10

4) Repeating the argument in Appendix G for general input (cid:98)Y shows it is enough to run the ADM algorithm
≤ (cid:12)
O (cid:0)n4 log n(cid:1) iterations to cross the range
θ. So the argument above together
(cid:12)
dictates that with the proposed initialization, w.h.p., the ADM algorithm produces an output q that satisﬁes
(cid:12)
(cid:10)q, U(cid:62)e1
(cid:12)

√
5) Since the ADM returns q satisfying (cid:12)
(cid:12)

θ, if we run at least O (cid:0)n4 log n(cid:1) iterations.
(cid:10)q, R(cid:62)e1

θ, discussion at the end of Appendix H implies that we
will obtain a solution q(cid:63) = ±U(cid:62)e1 up to scale as the optimizer of the rounding program, exactly the target
solution.

(cid:10)q, U(cid:62)e1

(cid:11)(cid:12)
(cid:12) ≤ 3

(cid:11)(cid:12)
(cid:12) ≥ 2

(cid:11)(cid:12)
(cid:12) ≥ 2

√

√

1
√

θn

10

Hence, we complete the proof.

Remark IV.8. Under the planted sparse model, in practice the ADM algorithm with the proposed initialization
converges to a global optimizer of (III.1) that correctly recovers x0. In fact, simple calculation shows such desired
point for successful recovery is indeed the only critical point of (III.1) near the pole in Fig. 1. Unfortunately,
using the current analytical framework, we did not succeed in proving such convergence in theory. Proposition IV.5
and IV.6 imply that after O(n4 log n) iterations, however, the ADM sequence will stay in a small neighborhood of
the target. Hence, we proposed to stop after O(n4 log n) steps, and then round the output using the LP that provable
recover the target, as implied by Proposition IV.5 and IV.7. So the LP rounding procedure is for the purpose of
completing the theory, and seems not necessary in practice. We suspect alternative analytical strategies, such as the
geometrical analysis that we will discuss in Section VI, can likely get around the artifact.

V. EXPERIMENTAL RESULTS

In this section, we show the performance of the proposed ADM algorithm on both synthetic and real datasets. On
the synthetic dataset, we show the phase transition of our algorithm on both the planted sparse and the dictionary
learning models; for the real dataset, we demonstrate how seeking sparse vectors can help discover interesting
patterns on face images.

A. Phase Transition on Synthetic Data

For the planted sparse model, for each pair of (k, p), we generate the n dimensional subspace S ⊂ Rp by direct
sum of x0 and G: x0 ∈ Rp is a k-sparse vector with uniformly random support and all nonzero entries equal to 1,
and G ∈ Rp×(n−1) is an i.i.d. Gaussian matrix distributed by N (0, 1/p). So one basis Y of the subspace S can
be constructed by Y = GS ([x0, G]) U, where GS (·) denotes the Gram-Schmidt orthonormalization operator and
U ∈ Rn×n is an arbitrary orthogonal matrix. For each p, we set the regularization parameter in (III.1) as λ = 1/
p,
use all the normalized rows of Y as initializations of q for the proposed ADM algorithm, and run the alternating
steps for 104 iterations. We determine the recovery to be successful whenever (cid:107)x0/ (cid:107)x0(cid:107)2 − Yq(cid:107)2 ≤ 10−2 for at
least one of the p trials (we set the tolerance relatively large as we have shown that LP rounding exactly recovers
the solutions with approximate input). To determine the empirical recovery performance of our ADM algorithm,
ﬁrst we ﬁx the relationship between n and p as p = 5n log n, and plot out the phase transition between k and p.
Next, we ﬁx the sparsity level θ = 0.2 (or k = 0.2p), and plot out the phase transition between p and n. For each
pair of (p, k) or (n, p), we repeat the simulation for 10 times. Fig. 2 shows both phase transition plots.

√

We also experiment with the complete dictionary learning model as in [13] (see also [15]). Speciﬁcally, the
observation is assumed to be Y = A0X0, where A0 is a square, invertible matrix, and X0 a n × p sparse matrix.
Since A0 is invertible, the row space of Y is the same as that of X0. For each pair of (k, n), we generate
X0 = [x1, · · · , xn](cid:62), where each vector xi ∈ Rp is k-sparse with every nonzero entry following i.i.d. Gaussian
distribution, and construct the observation by Y(cid:62) = GS (cid:0)X(cid:62)
(cid:1) U(cid:62). We repeat the same experiment as for the planted
0
sparse model described above. The only difference is that here we determine the recovery to be successful as long
as one sparse row of X0 is recovered by one of those p programs. Fig. 3 shows both phase transition plots.

Fig. 2(a) and Fig. 3(a) suggest our ADM algorithm could work into the linear sparsity regime for both models,
provided p ≥ Ω(n log n). Moreover, for both models, the log n factor seems necessary for working into the linear
sparsity regime, as suggested by Fig. 2(b) and Fig. 3(b): there are clear nonlinear transition boundaries between
success and failure regions. For both models, O(n log n) sample requirement is near optimal: for the planted sparse
model, obviously p ≥ Ω(n) is necessary; for the complete dictionary learning model, [13] proved that p ≥ Ω(n log n)
is required for exact recovery. For the planted sparse model, our result p ≥ Ω(n4 log n) is far from this much

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

11

Fig. 2. Phase transition for the planted sparse model using the ADM algorithm: (a) with ﬁxed relationship between p and n: p = 5n log n;
(b) with ﬁxed relationship between p and k: k = 0.2p. White indicates success and black indicates failure.

Fig. 3. Phase transition for the dictionary learning model using the ADM algorithm: (a) with ﬁxed relationship between p and n: p = 5n log n;
(b) with ﬁxed relationship between p and k: k = 0.2p. White indicates success and black indicates failure.

lower empirical requirement. Fig 2(b) further suggests that alternative reformulation and algorithm are needed to
solve (II.1) so that the optimal recovery guarantee as depicted in Theorem II.1 can be obtained.

B. Exploratory Experiments on Faces

It is well known in computer vision that the collection of images of a convex object only subject to illumination
changes can be well approximated by a low-dimensional subspaces in raw-pixel space [43]. We will play with face
subspaces here. First, we extract face images of one person (65 images) under different illumination conditions. Then
we apply robust principal component analysis [44] to the data and get a low dimensional subspace of dimension 10,
i.e., the basis Y ∈ R32256×10. We apply the ADM + LP algorithm to ﬁnd the sparsest elements in such a subspace,
by randomly selecting 10% rows of Y as initializations for q. We judge the sparsity in the (cid:96)1/(cid:96)2 sense, that is,
the sparsest vector (cid:98)x0 = Yq(cid:63) should produce the smallest (cid:107)Yq(cid:107)1 / (cid:107)Yq(cid:107)2 among all results. Once some sparse
vectors are found, we project the subspace onto orthogonal complement of the sparse vectors already found13, and
continue the seeking process in the projected subspace. Fig. 4(Top) shows the ﬁrst four sparse vectors we get from
the data. We can see they correspond well to different extreme illumination conditions. We also implemented the
spectral method (with the LP post-processing) proposed in [35] for comparison under the same protocol. The result
is presented as Fig. 4(Bottom): the ratios (cid:107)·(cid:107)(cid:96)1 / (cid:107)·(cid:107)(cid:96)2 are signiﬁcantly higher, and the ratios (cid:107)·(cid:107)(cid:96)4 / (cid:107)·(cid:107)(cid:96)2 (this is

13The idea is to build a sparse, orthonormal basis for the subspace in a greedy manner.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

12

the metric to be maximized in [35] to promote sparsity) are signiﬁcantly lower. By these two criteria the spectral
method with LP rounding consistently produces vectors with higher sparsity levels under our evaluation protocol.
Moreover, the resulting images are harder to interpret physically.

Fig. 4. The ﬁrst four sparse vectors extracted for one person in the Yale B database under different illuminations. (Top) by our ADM
algorithm; (Bottom) by the speeding-up SOS algorithm proposed in [35].

Second, we manually select ten different persons’ faces under the normal lighting condition. Again, the dimension
of the subspace is 10 and Y ∈ R32256×10. We repeat the same experiment as stated above. Fig. 5 shows four sparse
vectors we get from the data. Interestingly, the sparse vectors roughly correspond to differences of face images
concentrated around facial parts that different people tend to differ from each other, e.g., eye brows, forehead hair,
nose, etc. By comparison, the vectors returned by the spectral method [35] are relatively denser and the sparsity
patterns in the images are less structured physically.

In sum, our algorithm seems to ﬁnd useful sparse vectors for potential applications, such as peculiarity discovery
in ﬁrst setting, and locating differences in second setting. Nevertheless, the main goal of this experiment is to
invite readers to think about similar pattern discovery problems that might be cast as the problem of seeking sparse
vectors in a subspace. The experiment also demonstrates in a concrete way the practicality of our algorithm, both in
handling data sets of realistic size and in producing meaningful results even beyond the (idealized) planted sparse
model that we adopted for analysis.

VI. CONNECTIONS AND DISCUSSION

For the planted sparse model, there is a substantial performance gap in terms of p-n relationship between the our
optimality theorem (Theorem II.1), empirical simulations, and guarantees we have obtained via efﬁcient algorithm
(Theorem IV.1). More careful and tighter analysis based on decoupling [45] and chaining [46, 47] and geometrical
analysis described below can probably help bridge the gap between our theoretical and empirical results. Matching
the theoretical limit depicted in Theorem II.1 seems to require novel algorithmic ideas. The random models we
assume for the subspace can be extended to other random models, particularly for dictionary learning where all the
bases are sparse (e.g., Bernoulli-Gaussian random model).

This work is part of a recent surge of research efforts on deriving provable and practical nonconvex algorithms
to central problems in modern signal processing and machine learning. These problems include low-rank matrix
recovery/completion [48–56], tensor recovery/decomposition [57–61], phase retrieval [62–65], dictionary learning

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

13

Fig. 5. The ﬁrst four sparse vectors extracted for 10 persons in the Yale B database under normal illuminations. (Top) by our ADM
algorithm; (Bottom) by the speeding-up SOS algorithm proposed in [35].

[15, 36–40], and so on.14 Our approach, like the others, is to start with a carefully chosen, problem-speciﬁc
initialization, and then perform a local analysis of the subsequent iterates to guarantee convergence to a good solution.
In comparison, our subsequent work on complete dictionary learning [15] and generalized phase retrieval [65] has
taken a geometrical approach by characterizing the function landscape and designing efﬁcient algorithm accordingly.
The geometric approach has allowed provable recovery via efﬁcient algorithms, with an arbitrary initialization. The
article [66] summarizes the geometric approach and its applicability to several other problems of interest.

A hybrid of the initialization and the geometric approach discussed above is likely to be a powerful computational
framework. To see it in action for the current planted sparse vector problem, in Fig. 6 we provide the asymptotic
function landscape (i.e., p → ∞) of the Huber loss on the sphere S2 (aka the relaxed formulation we tried to
solve (III.1)). It is clear that with an initialization that is biased towards either the north or the south pole, we are
situated in a region where the gradients are always nonzero and points to the favorable directions such that many
reasonable optimization algorithms can take the gradient information and make steady progress towards the target.
This will probably ease the algorithm development and analysis, and help yield tight performance guarantees.

We provide a very efﬁcient algorithm for ﬁnding a sparse vector in a subspace, with strong guarantee. Our
algorithm is practical for handling large datasets—in the experiment on the face dataset, we successfully extracted
some meaningful features from the human face images. However, the potential of seeking sparse/structured element
in a subspace seems largely unexplored, despite the cases we mentioned at the start. We hope this work could
inspire more application ideas.

ACKNOWLEDGEMENT

JS thanks the Wei Family Private Foundation for their generous support. We thank Cun Mu, IEOR Department of
Columbia University, for helpful discussion and input regarding this work. We thank the anonymous reviewers for
their constructive comments that helped improve the manuscript. This work was partially supported by grants ONR
N00014-13-1-0492, NSF 1343282, NSF 1527809, and funding from the Moore and Sloan Foundations.

14The webpage http://sunju.org/research/nonconvex/ maintained by the second author contains pointers to the growing list of work in this

direction.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

14

Fig. 6. Function landscape of f (q) with θ = 0.4 for n = 3. (Left) f (q) over the sphere S2. Note that near the spherical caps around
the north and south poles, there are no critical points and the gradients are always nonzero; (Right) Projected function landscape by
3 (cid:55)→ R obtained via the reparameterization
projecting the upper hemisphere onto the equatorial plane. Mathematically the function g(w) : e⊥
q(w) = [w; (cid:112)1 − (cid:107)w(cid:107)2]. Corresponding to the left, there is no undesired critical point around 0 within a large radius.

APPENDIX A
TECHNICAL TOOLS AND PRELIMINARIES

In this appendix, we record several lemmas that are useful for our analysis.

Lemma A.1. Let ψ(x) and Ψ(x) to denote the probability density function (pdf) and the cumulative distribution
function (cdf) for the standard normal distribution:

(Standard Normal pdf) ψ(x) =

1
√
2π
1
√
2π
Suppose a random variable X ∼ N (0, σ2), with the pdf fσ(x) = 1

(Standard Normal cdf) Ψ(x) =

(cid:90) x

−∞

σ ψ (cid:0) x

σ

(cid:26)

exp

−

(cid:27)

x2
2
(cid:26)

exp

−

dt,

(cid:27)

t2
2

(cid:1), then for any t2 > t1 we have

(cid:90) t2

t1
(cid:90) t2

t1
(cid:90) t2

t1

fσ(x)dx = Ψ

(cid:19)

(cid:18) t2
σ
(cid:20)

(cid:20)

− Ψ

(cid:19)

(cid:19)

(cid:18) t2
σ
(cid:18) t2
σ

(cid:19)

,

(cid:18) t1
σ

(cid:19)(cid:21)

,

(cid:19)(cid:21)

(cid:18) t1
σ
(cid:18) t1
σ

xfσ(x)dx = −σ

ψ

− ψ

x2fσ(x)dx = σ2

Ψ

− Ψ

− σ

(cid:20)
t2ψ

(cid:19)

(cid:18) t2
σ

− t1ψ

(cid:19)(cid:21)

.

(cid:18) t1
σ

Lemma A.2 (Taylor Expansion of Standard Gaussian cdf and pdf ). Assume ψ(x) and Ψ(x) be deﬁned as above.
There exists some universal constant Cψ > 0 such that for any x0, x ∈ R,

|ψ(x) − [ψ(x0) − x0ψ (x0) (x − x0)]| ≤ Cψ(x − x0)2,
|Ψ(x) − [Ψ(x0) + ψ(x0)(x − x0)]| ≤ Cψ(x − x0)2.

Lemma A.3 (Matrix Induced Norms). For any matrix A ∈ Rp×n, the induced matrix norm from (cid:96)p → (cid:96)q is deﬁned
as

(cid:107)A(cid:107)(cid:96)p→(cid:96)q

.
= sup

(cid:107)x(cid:107)p=1

(cid:107)Ax(cid:107)q .

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

15

In particular, let A = [a1, · · · , an] = (cid:2)a1, · · · , ap(cid:3)(cid:62) , we have

(cid:107)A(cid:107)(cid:96)2→(cid:96)1 = sup
(cid:107)x(cid:107)2=1

p
(cid:88)

k=1

(cid:12)
(cid:12)a(cid:62)
(cid:12)
k x

(cid:12)
(cid:12)
(cid:12) ,

(cid:107)AB(cid:107)(cid:96)p→(cid:96)r ≤ (cid:107)A(cid:107)(cid:96)q→(cid:96)r (cid:107)B(cid:107)(cid:96)p→(cid:96)q ,

(cid:107)A(cid:107)(cid:96)2→(cid:96)∞ = max
1≤k≤p

(cid:13)
(cid:13)

(cid:13)ak(cid:13)
(cid:13)
(cid:13)2

,

and B is any matrix of size compatible with A.
Lemma A.4 (Moments of the Gaussian Random Variable). If X ∼ N (cid:0)0, σ2
X
that

(cid:1), then it holds for all integer m ≥ 1

E [|X|m] = σm

X (m − 1)!!

1m=2k+1 + 1m=2k

≤ σm

X (m − 1)!!, k = (cid:98)m/2(cid:99).

(cid:34)(cid:114) 2
π

(cid:35)

Lemma A.5 (Moments of the χ Random Variable). If X ∼ χ (n), i.e., X = (cid:107)x(cid:107)2 for x ∼ N (0, I), then it holds
for all integer m ≥ 1 that

E [X m] = 2m/2 Γ (m/2 + n/2)

≤ m!! nm/2.

Γ (n/2)

Lemma A.6 (Moments of the χ2 Random Variable). If X ∼ χ2 (n), i.e., X = (cid:107)x(cid:107)2
for all integer m ≥ 1 that

2 for x ∼ N (0, I), then it holds

E [X m] = 2m Γ (m + n/2)

=

Γ (n/2)

(n + 2k − 2) ≤

(2n)m.

m!
2

m
(cid:89)

k=1

Lemma A.7 (Moment-Control Bernstein’s Inequality for Random Variables [67]). Let X1, . . . , Xp be i.i.d. real-valued
random variables. Suppose that there exist some positive numbers R and σ2

X such that

m!
2

m!
2

Let S

.
= 1
p

(cid:80)p

k=1 Xk, then for all t > 0, it holds that

E [|Xk|m] ≤

X Rm−2,
σ2

for all integers m ≥ 2.

P [|S − E [S]| ≥ t] ≤ 2 exp

−

(cid:18)

pt2
X + 2Rt

2σ2

(cid:19)

.

Lemma A.8 (Moment-Control Bernstein’s Inequality for Random Vectors [15]). Let x1, . . . , xp ∈ Rd be i.i.d.
random vectors. Suppose there exist some positive number R and σ2

X such that

Let s = 1
p

(cid:80)p

k=1 xk, then for any t > 0, it holds that

E [(cid:107)xk(cid:107)m

2 ] ≤

X Rm−2,
σ2

for all integers m ≥ 2.

P [(cid:107)s − E [s](cid:107)2 ≥ t] ≤ 2(d + 1) exp

(cid:18)

−

pt2
X + 2Rt

2σ2

(cid:19)

.

Lemma A.9 (Gaussian Concentration Inequality). Let x ∼ N (0, Ip). Let f : Rp (cid:55)→ R be an L-Lipschitz function.
Then we have for all t > 0 that

P [f (X) − Ef (X) ≥ t] ≤ exp

−

(cid:18)

(cid:19)

.

t2
2L2

Lemma A.10 (Bounding Maximum Norm of Gaussian Vector Sequence). Let x1, . . . , xn1 be a sequence of (not
necessarily independent) standard Gaussian vectors in Rn2. It holds that

(cid:20)

P

max
i∈[n1]

(cid:107)xi(cid:107)2 >

√

(cid:21)
n2 + 2(cid:112)2 log(2n1)

≤ (2n1)−3.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

16

Proof: Since the function (cid:107)·(cid:107)2 is 1-Lipschitz, by Gaussian concentration inequality, for any i ∈ [n1], we have

(cid:20)
(cid:107)xi(cid:107)2 −

P

(cid:113)

E (cid:107)xi(cid:107)2

(cid:21)
2 > t

≤ P [(cid:107)xi(cid:107)2 − E (cid:107)xi(cid:107)2 > t] ≤ exp

(cid:19)

(cid:18)

−

t2
2

for all t > 0. Since E (cid:107)xi(cid:107)2

2 = n2, by a simple union bound, we obtain
t2
√
2

(cid:21)
n2 + t

(cid:107)xi(cid:107) >

max
i∈[n1]

≤ exp

−

(cid:18)

P

(cid:20)

(cid:19)

+ log n1

for all t > 0. Taking t = 2(cid:112)2 log(2n1) gives the claimed result.

Corollary A.11. Let Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1). It holds that
(cid:17)
n2 + 2(cid:112)2 log(2n1)

(cid:107)Φx(cid:107)∞ ≤

(cid:16)√

(cid:107)x(cid:107)2

for all x ∈ Rn2,

with probability at least 1 − (2n1)−3.

Proof: Let Φ = (cid:2)φ1, · · · , φn1(cid:3)(cid:62) . Without loss of generality, let us only consider x ∈ Sn2−1, we have
(cid:12)x(cid:62)φi(cid:12)

(cid:13)φi(cid:13)
(cid:13)

(cid:13)2 .

(cid:107)Φx(cid:107)∞ = max
i∈[n1]

(cid:12)
(cid:12) ≤ max
i∈[n1]

(cid:12)
(cid:12)

(A.1)

Invoking Lemma A.10 returns the claimed result.

Lemma A.12 (Covering Number of a Unit Sphere [42]). Let Sn−1 = {x ∈ Rn | (cid:107)x(cid:107)2 = 1} be the unit sphere. For
any ε ∈ (0, 1), there exists some ε cover of Sn−1 w.r.t. the (cid:96)2 norm, denoted as Nε, such that

|Nε| ≤

1 +

(cid:18)

(cid:19)n

2
ε

≤

(cid:19)n

.

(cid:18) 3
ε

Lemma A.13 (Spectrum of Gaussian Matrices, [42]). Let Φ ∈ Rn1×n2 (n1 > n2) contain i.i.d. standard normal
entries. Then for every t ≥ 0, with probability at least 1 − 2 exp (cid:0)−t2/2(cid:1), one has

√

√

n1 −

n2 − t ≤ σmin(Φ) ≤ σmax(Φ) ≤

n1 +

n2 + t.

√

√

Lemma A.14. For any ε ∈ (0, 1), there exists a constant C (ε) > 1, such that provided n1 > C (ε) n2, the random
matrix Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1) obeys

(cid:114) 2
(cid:114) 2
π
π
with probability at least 1 − 2 exp (−c (ε) n1) for some c (ε) > 0.

n1 (cid:107)x(cid:107)2 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + ε)

(1 − ε)

n1 (cid:107)x(cid:107)2

for all x ∈ Rn2,

Geometrically, this lemma roughly corresponds to the well known almost spherical section theorem [68, 69], see

also [70]. A slight variant of this version has been proved in [3], borrowing ideas from [71].

Proof: By homogeneity, it is enough to show that the bounds hold for every x of unit (cid:96)2 norm. For a ﬁxed
n1-Lipschitz, by concentration of

π n1. Note that (cid:107)·(cid:107)1 is

x0 with (cid:107)x0(cid:107)2 = 1, Φx0 ∼ N (0, I). So E (cid:107)Φx(cid:107)1 =
measure for Gaussian vectors in Lemma A.9, we have

(cid:113) 2

√

P [|(cid:107)Φx(cid:107)1 − E [(cid:107)Φx(cid:107)1]| > t] ≤ 2 exp

(cid:19)

(cid:18)

−

t2
2n1

for any t > 0. For a ﬁxed δ ∈ (0, 1), S n2−1 can be covered by a δ-net Nδ with cardinality #Nδ ≤ (1 + 2/δ)n2.
Now consider the event

(cid:40)

.
=

E

(cid:114) 2
π

(1 − δ)

n1 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + δ)

n1 ∀ x ∈ Nδ

.

(cid:114) 2
π

(cid:41)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

17

(cid:114) 2
π

∞
(cid:88)

k=0

∞
(cid:88)

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

A simple application of union bound yields

P [E c] ≤ 2 exp

−

+ n2 log

1 +

(cid:18)

δ2n1
π

(cid:18)

(cid:19)(cid:19)

.

2
δ

Choosing δ small enough such that

then conditioned on E, we can conclude that

(1 − 3δ) (1 − δ)−1 ≥ 1 − ε and (1 + δ) (1 − δ)−1 ≤ 1 + ε,

(1 − ε)

n1 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + ε)

n1 ∀ x ∈ Sn2−1.

(cid:114) 2
π

Indeed, suppose E holds. Then it can easily be seen that any z ∈ Sn2−1 can be written as

z =

λkxk,

with |λk| ≤ δk, xk ∈ Nδ for all k.

Hence we have

Similarly,

(cid:107)Φz(cid:107)1 =

Φ

λkxk

≤

δk (cid:107)Φxk(cid:107)1 ≤ (1 + δ) (1 − δ)−1

(cid:114) 2
π

n1.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

∞
(cid:88)

k=0

(cid:107)Φz(cid:107)1 =

Φ

λkxk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

k=0

(cid:104)

1 − δ − δ (1 + δ) (1 − δ)−1(cid:105) (cid:114) 2

≥

n1 = (1 − 3δ) (1 − δ)−1

π

(cid:114) 2
π

n1.

Hence, the choice of δ above leads to the claimed result. Finally, given n1 > Cn2, to make the probability P [E c]
decaying in n1, it is enough to set C = 2π

(cid:1). This completes the proof.

δ2 log (cid:0)1 + 2

δ

APPENDIX B
THE RANDOM BASIS VS. ITS ORTHONORMALIZED VERSION

In this appendix, we consider the planted sparse model

Y = [x0 | g1 | · · · | gn−1] = [x0 | G] ∈ Rp×n

as deﬁned in (III.5), where

1
√
θp

x0(k) ∼i.i.d.

Ber (θ) ,

g(cid:96) ∼i.i.d. N

0,

,

1 ≤ k ≤ p, 1 ≤ (cid:96) ≤ n − 1.

(B.1)

Recall that one “natural/canonical” orthonormal basis for the subspace spanned by columns of Y is

(cid:19)

1
p

I

(cid:18)

(cid:16)

Y =

(cid:20) x0
(cid:107)x0(cid:107)2

| Px⊥

0

G

G(cid:62)Px⊥

0

G

(cid:17)−1/2(cid:21)

,

G(cid:48)

.
= Px⊥

0

G

(cid:16)

G(cid:62)Px⊥

0

G

(cid:17)−1/2

which is well-deﬁned with high probability as Px⊥

0

G is well-conditioned (proved in Lemma B.2). We write

for convenience. When p is large, Y has nearly orthonormal columns, and so we expect that Y closely approximates
Y. In this section, we make this intuition rigorous. We prove several results that are needed for the proof of
Theorem II.1, and for translating results for Y to results for Y in Appendix E-D.

For any realization of x0, let I = supp(x0) = {i | x0(i) (cid:54)= 0}. By Bernstein’s inequality in Lemma A.7 with
σ2
X = 2θ and R = 1, the event

.
=

E0

(cid:26) 1
2

θp ≤ |I| ≤ 2θp

(cid:27)

(B.2)

(B.3)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

18

holds with probability at least 1 − 2 exp (−θp/16). Moreover, we show the following:

Lemma B.1. When p ≥ Cn and θ > 1/

n, the bound

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

4

2

(cid:115)

≤

5

n log p
θ2p

holds with probability at least 1 − cp−2. Here C, c are positive constants.

(B.4)

Proof: Because E

(cid:104)

(cid:107)x0(cid:107)2
2

(cid:105)

we have

for all t > 0, which implies

= 1, by Bernstein’s inequality in Lemma A.7 with σ2

X = 2/(θp2) and R = 1/(θp),

P

(cid:104)(cid:12)
(cid:12)(cid:107)x0(cid:107)2
(cid:12)

2 − E

(cid:104)

(cid:107)x0(cid:107)2
2

(cid:105)(cid:12)
(cid:105)
(cid:12)
(cid:12) > t

= P

(cid:104)(cid:12)
(cid:12)(cid:107)x0(cid:107)2
(cid:12)

(cid:12)
(cid:105)
(cid:12)
2 − 1
(cid:12) > t

≤ 2 exp

−

(cid:18)

(cid:19)

θpt2
4 + 2t

(cid:20)
|(cid:107)x0(cid:107)2 − 1| >

P

(cid:21)

t
(cid:107)x0(cid:107)2 + 1

= P [|(cid:107)x0(cid:107)2 − 1| ((cid:107)x0(cid:107)2 + 1) > t] ≤ 2 exp

(cid:18)

−

θpt2
4 + 2t

(cid:19)

.

On the intersection with E0, (cid:107)x0(cid:107)2 + 1 ≥ 1√
2

+ 1 ≥ 5/4 and setting t =

(cid:113) n log p

θ2p , we obtain

(cid:34)

(cid:115)

(cid:35)

4
5
Unconditionally, this implies that with probability at least 1 − 2 exp (−pθ/16) − 2 exp (cid:0)−

−(cid:112)np log p

|(cid:107)x0(cid:107)2 − 1| ≥

n log p
θ2p

≤ 2 exp

(cid:12)
(cid:12)
(cid:12) E0

P

(cid:16)

(cid:17)

.

√

np log p(cid:1), we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

|1 − (cid:107)x0(cid:107)2|
(cid:107)x0(cid:107)2

≤

√
4

2

(cid:115)

5

n log p
θ2p

,

as desired.
.
= (cid:0)G(cid:62)Px⊥

Let M

0

G(cid:1)−1/2

. Then G(cid:48) = GM − x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM. We show the following results hold:

Lemma B.2. Provided p ≥ Cn, it holds that

(cid:107)M(cid:107) ≤ 2,

(cid:107)M − I(cid:107) ≤ 4

+ 4

(cid:114) n
p

(cid:115)

log(2p)
p

with probability at least 1 − (2p)−2. Here C is a positive constant.

Proof: First observe that

(cid:16)

(cid:16)

(cid:107)M(cid:107) =

σmin

G(cid:62)Px⊥

0

G

(cid:17)(cid:17)−1/2

= σ−1
min

(cid:0)Px⊥

0

G(cid:1) .

Now suppose B is an orthonormal basis spanning x⊥
as that of B(cid:62)G ∈ R(p−1)×(n−1); in particular,

0 . Then it is not hard to see the spectrum of Px⊥

G is the same

0

σmin

(cid:0)Px⊥

0

G(cid:1) = σmin

(cid:16)

B(cid:62)G

(cid:17)

.

0, 1
Since each entry of G ∼i.i.d. N
p
spectrum results for Gaussian matrices in Lemma A.13 and obtain that

, and B(cid:62) has orthonormal rows, B(cid:62)G ∼i.i.d. N

(cid:16)

(cid:17)

(cid:17)

(cid:16)

0, 1
p

, we can invoke the

(cid:114) p − 1
p

(cid:114) n − 1
p

−

− 2

log (2p)
p

(cid:115)

≤ σmin

(cid:16)

(cid:17)

B(cid:62)G

≤ σmax

(cid:16)

(cid:17)

B(cid:62)G

≤

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

log(2p)
p

(cid:115)

with probability at least 1 − (2p)−2. Thus, when p ≥ C1n for some sufﬁciently large constant C1, by using the

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

19

results above we have

(cid:107)M(cid:107) = σ−1
min

(cid:16)

(cid:17)

B(cid:62)G

=

(cid:32)(cid:114) p − 1

(cid:114) n − 1
p

−

− 2

p

(cid:115)

(cid:33)−1

log (2p)
p

≤ 2,

(cid:107)I − M(cid:107) = max (|σmax (M) − 1| , |σmin (M) − 1|)
(cid:16)

(cid:17)

(cid:16)

= max

≤ max

B(cid:62)G

min

(cid:16)(cid:12)
(cid:12)σ−1
(cid:12)

(cid:32)(cid:114) p − 1


−

p

− 1

(cid:12)
(cid:12)
(cid:12) ,
(cid:114) n − 1
p

(cid:12)
(cid:12)σ−1
(cid:12)

max

(cid:17)

B(cid:62)G

(cid:115)

− 2

log (2p)
p

(cid:17)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:33)−1

− 1, 1 −

(cid:32)(cid:114) p − 1

(cid:114) n − 1
p

+

+ 2

p

(cid:115)

log(2p)
p

(cid:33)−1




(cid:32)







= max

1 −

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

log (2p)
p

(cid:33) (cid:32)(cid:114) p − 1

(cid:114) n − 1
p

−

− 2

(cid:33)−1

,

log (2p)
p

(cid:32)(cid:114) p − 1

− 1 +

p

(cid:114) n − 1
p

+ 2

log(2p)
p

(cid:33) (cid:32)(cid:114) p − 1

(cid:114) n − 1
p

+

+ 2

log(2p)
p

(cid:33)−1




(cid:115)

(cid:115)

(cid:115)

(cid:115)

p

p

(cid:32)

≤ 2

1 −

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

(cid:115)

(cid:33)

log (2p)
p

(cid:115)

≤ 4

+ 4

(cid:114) n
p

log(2p)
p

,

with probability at least 1 − (2p)−2.

Lemma B.3. Let YI be a submatrix of Y whose rows are indexed by the set I. There exists a constant C > 0,
such that when p ≥ Cn and 1/2 > θ > 1/

√

p,

n, the following
√
(cid:13)
(cid:13)Y(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 3
(cid:107)YI(cid:107)(cid:96)2→(cid:96)1 ≤ 7(cid:112)2θp,
(cid:13)G − G(cid:48)(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 4
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 20

√

(cid:13)
(cid:13)YI − YI

(cid:13)Y − Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 20

n + 7(cid:112)log(2p),
(cid:114)

(cid:114)

,

n log p
θ
n log p
θ

hold simultaneously with probability at least 1 − cp−2 for a positive constant c.

Proof: First of all, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

≤

1
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)(cid:96)2→(cid:96)1

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 GM
(cid:13)(cid:96)2→(cid:96)2

=

2
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)1

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

,

where in the last inequality we have applied the fact (cid:107)M(cid:107) ≤ 2 from Lemma B.2. Now x(cid:62)
0, (cid:107)x0(cid:107)2
vectors with each entry distributed as N
p
Lemma A.9, we have

0 G is an i.i.d. Gaussian
θp . So by Gaussian concentration inequality in

, where (cid:107)x0(cid:107)2

2 = |I|

(cid:16)

(cid:17)

2

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

≤ 2 (cid:107)x0(cid:107)2

(cid:115)

log(2p)
p

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

20

with probability at least 1 − c1p−2. On the intersection with E0, this implies
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2(cid:112)2θ log(2p),

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

with probability at least 1 − c2p−2 provided θ > 1/
that when p ≥ C1n,

n. Moreover, when intersected with E0, Lemma A.14 implies

(cid:107)G(cid:107)(cid:96)2→(cid:96)1 ≤

(cid:107)GI(cid:107)(cid:96)2→(cid:96)1 ≤ (cid:112)2θp

n. Hence, by Lemma B.2, when p > C2n,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1
√

√

p,
√

with probability at least 1 − c3p−2 provided θ > 1/
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)1 (cid:107)I − M(cid:107) +

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2
(cid:33)

√

p

≤

(cid:32)
(cid:114) n
4
p

(cid:115)

+ 4

log(2p)
p

(cid:13)Y(cid:13)
(cid:13)

(cid:13)
(cid:13)G(cid:48)
I

(cid:13)
(cid:13)GI − G(cid:48)
I

(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)x0(cid:107)(cid:96)2→(cid:96)1 + (cid:107)G(cid:107)(cid:96)2→(cid:96)1 ≤ (cid:107)x0(cid:107)1 +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)GI(cid:107)(cid:96)2→(cid:96)1 (cid:107)M(cid:107) +
(cid:13)
(cid:13)(cid:96)2→(cid:96)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)GI(cid:107)(cid:96)2→(cid:96)1 (cid:107)I − M(cid:107) +

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2
(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)
2
(cid:33)

GM

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:115)

≤ (cid:112)2θp

(cid:32)
4

(cid:114) n
p

+ 4

log(2p)
p

+ 2(cid:112)2θ log(2p) ≤ 4

n + 7(cid:112)log(2p),

√

√

p ≤ 2(cid:112)θp +

√

√

p ≤ 3

p,

≤ 2(cid:112)2θp + 2(cid:112)2θ log(2p) ≤ 4(cid:112)2θp,

+ 2(cid:112)2θ log(2p) ≤ 4

2θn + 6(cid:112)2θ log(2p),

√

(cid:107)YI(cid:107)(cid:96)2→(cid:96)1 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

+ (cid:13)

(cid:13)G(cid:48)
I

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤
√

(cid:107)x0(cid:107)1
(cid:107)x0(cid:107)2

+ 6(cid:112)2θp ≤ 7(cid:112)2θp

n. Finally, by Lemma B.1 and the results above, we obtain
(cid:114)

(cid:107)x0(cid:107)1 + (cid:13)

(cid:13)G − G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 20

(cid:107)x0(cid:107)1 + (cid:13)

(cid:13)GI − G(cid:48)
I

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 20

n log p
θ

,

(cid:114)

n log p
θ

,

with probability at least 1 − c4p−2 provided θ > 1/
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)x0(cid:107)2
1
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)YI − YI

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤

(cid:13)Y − Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1 −

holding with probability at least 1 − c5p−2.

Lemma B.4. Provided p ≥ Cn and θ > 1/

√

n, the following
(cid:115)

(cid:13)G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ 2

(cid:114) n
p

+ 8

√

2 log(2p)
p

,

21(cid:112)n log(2p)
p
hold simultaneously with probability at least 1 − cp−2 for some constant c > 0.

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤

2 log(2p)

4n
p

+

+

p

8

Proof: First of all, we have when p ≥ C1n, it holds with probability at least 1 − c2p−2 that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)∞

≤

1
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)(cid:96)2→(cid:96)∞

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 GM
(cid:13)(cid:96)2→(cid:96)2

≤

2
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)∞

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

,

where at the last inequality we have applied the fact (cid:107)M(cid:107) ≤ 2 from Lemma B.2. Moreover, from proof of Lemma B.3,
(cid:13)2 ≤ 2(cid:112)log(2p)/p (cid:107)x0(cid:107)2 with probability at least 1 − c3p−2 provided p ≥ C4n. Therefore,
we know that (cid:13)

0 G(cid:13)

(cid:13)x(cid:62)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

21

conditioned on E0, we obtain that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
holds with probability at least 1 − c5p−2 provided θ > 1/

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)∞

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

4 (cid:107)x0(cid:107)∞
(cid:107)x0(cid:107)2

GM

≤

√

(cid:115)

log(2p)
p

≤

4(cid:112)2 log(2p)
√
θp

n. Now by Corollary A.11, we have that

(cid:107)G(cid:107)(cid:96)2→(cid:96)∞ ≤

+ 2

(cid:114) n
p

(cid:115)

2 log(2p)
p

with probability at least 1 − c6p−2. Combining the above estimates and Lemma B.2, we have that with probability
at least 1 − c7p−2

where the last simpliﬁcation is provided that θ > 1/

n and p ≥ C8n for a sufﬁciently large C8. Similarly,

(cid:13)G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ (cid:107)GM(cid:107)(cid:96)2→(cid:96)∞ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

GM

(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
2
(cid:13)
(cid:13)
x0x(cid:62)
(cid:13)
(cid:13)
0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
(cid:13)
2
4(cid:112)2 log(2p)
√
θp

GM

+

≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)∞ (cid:107)M(cid:107) +

(cid:115)

≤ 2

+ 4

(cid:114) n
p

2 log(2p)
p

√

(cid:115)

≤ 2

+ 8

(cid:114) n
p

2 log(2p)
p

,

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)∞ (cid:107)I − M(cid:107) +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(8

GM

(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
2
√
2 + 4)(cid:112)n log(2p)

√
8

√
8

≤

≤

4n
p

4n
p

+

+

p

p

2 log(2p)

2 log(2p)

+

+

p

21(cid:112)n log(2p)
p

,

+

4(cid:112)2 log(2p)
√
θp

completing the proof.

APPENDIX C
PROOF OF (cid:96)1/(cid:96)2 GLOBAL OPTIMALITY

In this appendix, we prove the (cid:96)1/(cid:96)2 global optimality condition in Theorem II.1 of Section II.

Proof of Theorem II.1: We will ﬁrst analyze a canonical version, in which the input orthonormal basis is Y as

deﬁned in (III.6) of Section III:

Let q =

and let I be the support set of x0, we have

(cid:21)

(cid:20)q1
q2

min
q∈Rn

(cid:107)Yq(cid:107)1 ,

s.t. (cid:107)q(cid:107)2 = 1.

(cid:13)G(cid:48)

Iq2

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

I cq2

(cid:13)
(cid:13)1

− (cid:13)

≥ |q1|

(cid:107)Yq(cid:107)1 = (cid:107)YIq(cid:107)1 + (cid:107)YI cq(cid:107)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2
x0
(cid:107)x0(cid:107)2
x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≥ |q1|

≥ |q1|

− (cid:107)GIq2(cid:107)1 − (cid:13)
(cid:13)

(cid:0)GI − G(cid:48)

I

(cid:1) q2

(cid:13)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)
(cid:13)
(cid:0)GI c − G(cid:48)
(cid:13)

I c

(cid:1) q2

(cid:13)
(cid:13)1

− (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

(cid:13)G − G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2 ,

where G and G(cid:48) are deﬁned in (B.1) and (B.2) of Appendix B. By Lemma A.14 and intersecting with E0 deﬁned

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

22

in (B.3), we have that as long as p ≥ C1n,

(cid:107)GIq2(cid:107)1 ≤

(cid:107)q2(cid:107)2 = 2θ

p (cid:107)q2(cid:107)2 for all q2 ∈ Rn−1,

√

(cid:107)GI cq2(cid:107)1 ≥

(cid:107)q2(cid:107)2 =

p (1 − 2θ) (cid:107)q2(cid:107)2 for all q2 ∈ Rn−1,

√

1
2

2θp
√
p
p − 2θp
√
p

1
2

hold with probability at least 1 − c2p−2. Moreover, by Lemma B.3,

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 4

√

n + 7(cid:112)log(2p)
√

holds with probability at least 1 − c3p−2 when p ≥ C4n and θ > 1/
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107)Yq(cid:107)1 ≥ g(q)

+ (cid:107)q2(cid:107)2

.
= |q1|

(cid:18) 1
2

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

√

holds with probability at least 1 − c5p−2. Assuming E0, we observe

n. So we obtain that

p (1 − 2θ) − 2θ

p − 4

√

√

n − 7(cid:112)log(2p)

(cid:19)

Now g(q) is a linear function in |q1| and (cid:107)q2(cid:107)2. Thus, whenever θ is sufﬁciently small and p ≥ C6n such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≤ (cid:112)|I|

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ (cid:112)2θp.

(cid:112)2θp <

√

1
2

p (1 − 2θ) − 2θ

p − 4

√

√

n − 7(cid:112)log(2p),

±e1 are the unique minimizers of g(q) under the constraint q2
g(±e1), and we have

1 + (cid:107)q2(cid:107)2

2 = 1. In this case, because (cid:107)Y(±e1)(cid:107)1 =

(cid:107)Yq(cid:107)1 ≥ g(q) > g(±e1)

for all q (cid:54)= ±e1, ±e1 are the unique minimizers of (cid:107)Yq(cid:107)1 under the spherical constraint. Thus there exists a
universal constant θ0 > 0, such that for all 1/
n ≤ θ ≤ θ0, ±e1 are the only global minimizers of (II.2) if the
input basis is Y.

√

Any other input basis can be written as (cid:98)Y = YU, for some orthogonal matrix U. The program now is written as

which is equivalent to

which is obviously equivalent to the canonical program we analyzed above by a simple change of variable, i.e.,
q

.
= Uq, completing the proof.

min
q∈Rn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Yq
(cid:13)1

,

s.t. (cid:107)q(cid:107)2 = 1,

min
q∈Rn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Yq
(cid:13)1

,

s.t. (cid:107)Uq(cid:107)2 = 1,

APPENDIX D
GOOD INITIALIZATION

In this appendix, we prove Proposition IV.3. We show that the initializations produced by the procedure described

Proof of Proposition IV.3: Our previous calculation has shown that θp/2 ≤ |I| ≤ 2θp with probability at least
n. Let Y = (cid:2)y1, · · · , yp(cid:3)(cid:62) as deﬁned in (III.6). Consider any i ∈ I.

√

in Section III are biased towards the optimal.

1 − c1p−2 provided p ≥ C2n and θ > 1/
Then x0(i) = 1√
θp , and

(cid:10)e1, yi/ (cid:13)

(cid:13)yi(cid:13)
(cid:13)2

(cid:11) =

√

1/

θp

(cid:107)x0(cid:107)2 (cid:107)yi(cid:107)2

(cid:107)x0(cid:107)2 ((cid:107)x0(cid:107)∞ / (cid:107)x0(cid:107)2 + (cid:107)(g(cid:48))i(cid:107)2)

≥

≥

√

1/

θp

√

1/

θp

(cid:107)x0(cid:107)2 ((cid:107)x0(cid:107)∞ / (cid:107)x0(cid:107)2 + (cid:107)gi(cid:107)2 + (cid:107)G − G(cid:48)(cid:107)(cid:96)2→(cid:96)∞)

,

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

23

where gi and (g(cid:48))i are the i-th rows of G and G(cid:48), respectively. Since such gi’s are independent Gaussian vectors
in Rn−1 distributed as N (0, 1/p), by Gaussian concentration inequality and the fact that |I| ≥ pθ/2 w.h.p.,

provided p ≥ C5n and θ > 1/

n. Moreover,

(cid:104)

P

∃i ∈ I : (cid:13)
√

(cid:13)gi(cid:13)

(cid:13)2 ≤ 2(cid:112)n/p

(cid:105)

≥ 1 − exp (−c3nθp) ≤ c4p−2,

(cid:114)

(cid:114)

(cid:107)x0(cid:107)2 =

|I| ×

≤

2θp ×

1
θp

√

=

2.

1
θp

Combining the above estimates and result of Lemma B.4, we obtain that provided p ≥ C6n and θ > 1/
probability at least 1 − c7p−2, there exists an i ∈ [p], such that if we set q(0) = yi/ (cid:13)
(cid:13)2, it holds that

(cid:13)yi(cid:13)

(cid:12)
(cid:12)q(0)
(cid:12)

1

(cid:12)
(cid:12)
(cid:12) ≥

√

2(cid:112)n/p +

√

(cid:16)

2

4n/p + 8

2 log(2p)/p + 21(cid:112)n log(2p)/p

(cid:17)

√

1/

θp
√

√

n, with

(using p ≥ C6n to simpliﬁy the above line)

θp
√
2(cid:112)n/p

√

1/

√

1/

1 + 6

θp + 2
√

1/

θp + 6
1
√

√
2
1
√

2)

(1 + 6
1
√

10

θn

,

≥

=

≥

≥

θn

√

θn

√

(as θ > 1/

n)

completing the proof.

We will next show that for an arbitrary orthonormal basis (cid:98)Y

.
= YU the initialization still biases towards the target
solution. To see this, suppose w.l.o.g. (cid:0)yi(cid:1)(cid:62) is a row of Y with nonzero ﬁrst coordinate. We have shown above that
if Y is the input orthonormal basis. For Y, as x0 = Ye1 = YUU(cid:62)e1,
with high probability
10
we know q(cid:63) = U(cid:62)e1 is the target solution corresponding to (cid:98)Y. Observing that

(cid:69)(cid:12)
(cid:12) ≥ 1
(cid:12)

(cid:68) yi
(cid:107)yi(cid:107)2

, e1

(cid:12)
(cid:12)
(cid:12)

θn

√

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

U(cid:62)e1,

(cid:17)(cid:62)

(cid:16)

e(cid:62)
i (cid:98)Y

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

e(cid:62)
i (cid:98)Y

(cid:17)(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:12)
(cid:12)
(cid:43)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

U(cid:62)e1,

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

U(cid:62)Y(cid:62)ei
(cid:107)U(cid:62)Y(cid:62)ei(cid:107)2

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

e1,

(Y)(cid:62) ei
(cid:107)Y(cid:62)ei(cid:107)2

(cid:43)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

e1,

yi
(cid:107)yi(cid:107)2

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

1
√

,

10

nθ

corroborating our claim.

APPENDIX E
LOWER BOUNDING FINITE SAMPLE GAP G(q)

In this appendix, we prove Proposition IV.4. In particular, we show that the gap G(q) deﬁned in (IV.8) is strictly

positive over a large portion of the sphere Sn−1.

Proof of Proposition IV.4: Without loss of generality, we work with the “canonical” orthonormal basis Y
deﬁned in (III.6). Recall that Y is the orthogonalization of the planted sparse basis Y as deﬁned in (III.5). We
deﬁne the processes Q(q) and Q(q) on q ∈ Sn−1, via

Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

, Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

.

1
p

p
(cid:88)

i=1

1
p

p
(cid:88)

i=1

24

(E.1)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

Thus, we can separate Q(q) as Q(q) =

, where

(cid:21)

(cid:20) Q1(q)
Q2(q)

Q1(q) =

x0iSλ

(cid:104)

q(cid:62)yi(cid:105)

and Q2(q) =

(cid:104)

q(cid:62)yi(cid:105)

,

giSλ

1
p

p
(cid:88)

i=1

1
p

p
(cid:88)

i=1

and separate Q(q) correspondingly. Our task is to lower bound the gap G(q) for ﬁnite samples as deﬁned in (IV.8).
√
Since we can deterministically constrain |q1| and (cid:107)q2(cid:107)2 over the set Γ as deﬁned in (IV.7) (e.g.,
θ
and (cid:107)q2(cid:107)2 ≥ 1
10 for q2 is arbitrary here, as we can always take a sufﬁciently small θ), the
challenge lies in lower bounding |Q1 (q)| and upper bounding (cid:107)Q2 (q)(cid:107)2, which depend on the orthonormal basis
Y. The unnormalized basis Y is much easier to work with than Y. Our proof will follow the observation that
(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)

10 , where the choice of 1

≤ |q1| ≤ 3

1
√

nθ

10

|Q1 (q)| ≥ (cid:12)
(cid:107)Q2 (q)(cid:107) ≤ (cid:13)

(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)

(cid:12) − (cid:12)
(cid:13)2 + (cid:13)

(cid:12) − (cid:12)
(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)

(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12) ,
(cid:13)2 + (cid:13)

(cid:13)Q2 (q) − Q2 (q)(cid:13)

(cid:13)2 .

In particular, we show the following:

• Appendix E-A shows that the expected gap is lower bounded for all q ∈ Sn−1 with |q1| ≤ 3
(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

q2
1
θp

1
50

G (q)

.
=

≥

−

.

√

θ:

As |q1| ≥ 1
√
10

nθ

, we have

• Appendix E-B, as summarized in Proposition E.8, shows that whenever p ≥ Ω (cid:0)n4 log n(cid:1), it holds with high

probability that

(cid:12)
(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|

−

(cid:13)
(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

inf
q∈Γ

≥

1
5000

1
θ2np

.

(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

+

sup
q∈Γ

√

10

θn

≤

4 × 105θ5/2n3/2p

+

10
4 × 105θ2np

(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2
1
2 × 104θ2np

=

.

• Appendix E-D shows that whenever p ≥ Ω (cid:0)n4 log n(cid:1), it holds with high probability that
(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12)
(cid:12)
|q1|

(cid:13)Q2 (q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

sup
q∈Γ

+

√

10

θn

≤

4 × 105θ5/2n3/2p

+

10
4 × 105θ2np

=

1
2 × 104θ2np

.

Observing that

inf
q∈Γ

G(q) ≥ inf
q∈Γ

(cid:32) (cid:12)

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

−

(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|
(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12)
|q1|

(cid:32) (cid:12)

+

− sup
q∈Γ

(cid:13)Q2 (q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

(cid:33)

,

(cid:33)

(cid:32) (cid:12)

(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|

+

(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

(cid:33)

− sup
q∈Γ

we obtain the result as desired.

For the general case when the input orthonormal basis is (cid:98)Y = YU with target solution q(cid:63) = U(cid:62)e1, a

straightforward extension of the deﬁnition for the gap would be:
(cid:13)
(cid:13)
(cid:13)

, U(cid:62)e1

Q

(cid:69)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:68)

(cid:16)

(cid:16)

(cid:17)

G

q; (cid:98)Y = YU

(cid:17) .
=

−

q; (cid:98)Y
|(cid:104)q, U(cid:62)e1(cid:105)|

(cid:16)

(cid:0)I − U(cid:62)e1e(cid:62)

1 U(cid:1) Q
(cid:13)
(cid:0)I − U(cid:62)e1e(cid:62)
(cid:13)

q; (cid:98)Y
1 U(cid:1) q(cid:13)
(cid:13)2

(cid:17)(cid:13)
(cid:13)
(cid:13)2

.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

25

(cid:16)

(cid:17)

Since Q

q; (cid:98)Y

= 1
p

(cid:80)p

k=1 U(cid:62)ykSλ

(cid:0)q(cid:62)U(cid:62)yk(cid:1), we have

(cid:16)

(cid:17)

UQ

q; (cid:98)Y

=

UU(cid:62)ykSλ

(cid:16)

q(cid:62)U(cid:62)yk(cid:17)

=

ykSλ

(cid:104)

(Uq)(cid:62) yk(cid:105)

= Q (Uq; Y) .

(E.2)

1
p

p
(cid:88)

k=1

1
p

p
(cid:88)

k=1

Hence we have

(cid:16)

G

q; (cid:98)Y = YU

=

(cid:17)

|(cid:104)Q (Uq; Y) , e1(cid:105)|
|(cid:104)Uq, e1(cid:105)|

−

(cid:13)
(cid:0)I − e1e(cid:62)
(cid:13)

1

(cid:1) Q (Uq; Y)(cid:13)
(cid:13)2
(cid:1) Uq(cid:13)
(cid:13)2

1

.

(cid:13)
(cid:0)I − e1e(cid:62)
(cid:13)

Therefore, from Proposition IV.4 above, we conclude that under the same technical conditions as therein,

q∈Sn−1:

1
√

10

θn

inf
≤|(cid:104)Uq,e1(cid:105)|≤3

√

θ

(cid:16)

(cid:17)

G

q; (cid:98)Y

≥

1
104θ2np

with high probability.

A. Lower Bounding the Expected Gap G(q)

In this section, we provide a nontrivial lower bound for the gap

G(q) =

(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

−

(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

.

More speciﬁcally, we show that:

for all q ∈ Sn−1 with |q1| ≤ 3

θ.

√

G(q) ≥

1
50

q2
1
θp

Proposition E.1. There exists some numerical constant θ0 > 0, such that for all θ ∈ (0, θ0), it holds that

(E.3)

(E.4)

Estimating the gap G(q) requires delicate estimates for E (cid:2)Q1(q)(cid:3) and E (cid:2)Q2(q)(cid:3). We ﬁrst outline the main

proof in Appendix E-A1, and delay these detailed technical calculations to the subsequent subsections.

1) Sketch of the Proof: W.l.o.g., we only consider the situation that q1 > 0, because the case of q1 < 0 can be

similarly shown by symmetry. By (E.1), we have

E (cid:2)Q1(q)(cid:3) = E
E (cid:2)Q2(q)(cid:3) = E

(cid:104)

(cid:104)

(cid:104)

(cid:105)(cid:105)

,

x0Sλ
(cid:104)

gSλ

x0q1 + q(cid:62)
2 g
(cid:105)(cid:105)

x0q1 + q(cid:62)
2 g

,

where g ∼ N

(cid:16)

(cid:17)
0, 1
p I

, and x0 ∼ 1√

θp Ber(θ). Let us decompose

g = g(cid:107) + g⊥,

with g(cid:107) = P(cid:107)g = q2q(cid:62)
2
(cid:107)q2(cid:107)2
2

g, and g⊥ = (I − P(cid:107))g. In this notation, we have

E (cid:2)Q2(q)(cid:3) = E
= E

(cid:104)

(cid:104)

g(cid:107)Sλ

x0q1 + q(cid:62)

(cid:105)(cid:105)

(cid:104)

+ E

2 g(cid:107)
(cid:105)(cid:105)

(cid:104)

g⊥Sλ
(cid:104)

x0q1 + q(cid:62)

2 g(cid:107)

(cid:105)(cid:105)

(cid:104)

Sλ

x0q1 + q(cid:62)
2 g

(cid:105)(cid:105)

g(cid:107)Sλ
q2
(cid:107)q2(cid:107)2
2

E

=

x0q1 + q(cid:62)
2 g

+ E [g⊥] E
(cid:105)(cid:105)

(cid:104)

q(cid:62)

2 gSλ

x0q1 + q(cid:62)
2 g

,

(cid:104)

(cid:104)

(cid:104)

where we used the facts that q(cid:62)
and E [g⊥] = 0. Let Z

.
= g(cid:62)q2 ∼ N (0, σ2) with σ2 = (cid:107)q2(cid:107)2

2 g(cid:107), g⊥ and g(cid:107) are uncorrelated Gaussian vectors and therefore independent,
2 /p, by partial evaluation of the expectations with

2 g = q(cid:62)

26

(E.5)

(E.6)

(E.7)

(E.8)

(E.9)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

respect to x0, we get

E (cid:2)Q1(q)(cid:3) =

E (cid:2)Q2(q)(cid:3) =

(cid:21)(cid:21)

,

(cid:115)

(cid:20)
Sλ

E

θ
p

θq2
(cid:107)q2(cid:107)2
2

E

(cid:20) q1√
+ Z
θp
(cid:20) q1√
θp

(cid:20)
ZSλ

(cid:21)(cid:21)

+ Z

+

(1 − θ)q2
(cid:107)q2(cid:107)2
2

E [ZSλ [Z]] .

Straightforward integration based on Lemma A.1 gives a explicit form of the expectations as follows

E (cid:2)Q1(q)(cid:3) =

E (cid:2)Q2(q)(cid:3) =

(cid:115)

(cid:26)(cid:20)

θ
p

(cid:16)

αΨ

−

(cid:17)

α
σ

(cid:19)(cid:21)

(cid:20)

(cid:18)

(cid:19)

+ βΨ

+ σ

ψ

(cid:16)

− ψ

−

(cid:17)(cid:21)(cid:27)

,

α
σ

(cid:26) 2 (1 − θ)
p

(cid:18)

(cid:19)

Ψ

−

+

λ
σ

θ
p

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)(cid:27)

q2,

−

β
σ
(cid:18) β
σ

(cid:18) β
σ
(cid:20)

where the scalars α and β are deﬁned as

α =

+ λ,

β =

− λ,

q1√
θp

q1√
θp

and ψ (t) and Ψ (t) are pdf and cdf for standard normal distribution, respectively, as deﬁned in Lemma A.1. Plugging
(E.7) and (E.8) into (E.3), by some simpliﬁcations, we obtain

G(q) =

1
q1

+

(cid:115)

σ
q1

(cid:20)

θ
p
(cid:115)

(cid:16)

(cid:17)

αΨ

(cid:20)

ψ

θ
p

−

α
σ
(cid:18) β
σ

(cid:19)

(cid:16)

− ψ

−

(cid:17)(cid:21)

.

α
σ

+ βΨ

(cid:19)

(cid:18) β
σ

−

2q1√
θp

(cid:18)

(cid:19)(cid:21)

Ψ

−

λ
σ

−

θ
p

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

(cid:19)

(cid:18) β
σ

+ Ψ

− 2Ψ

−

(cid:18)

(cid:19)(cid:21)

λ
σ

With λ = 1/

p and σ2 = (cid:107)q2(cid:107)2

√

2 /p = (1 − q2
δ + 1
(cid:112)1 − q2

,

1)/p, we have
β
σ

=

= −

1

δ − 1
(cid:112)1 − q2

1

,

λ
σ

=

1
(cid:112)1 − q2

1

,

−

α
σ
√

√

θ for q1 ≤ 3

where δ = q1/
More speciﬁcally, we approximate Ψ (cid:0)− α
(cid:1) and ψ (cid:0)− α
σ
σ
−1 + δ. Applying the estimates for the relevant quantities established in Lemma E.2, we obtain

θ. To proceed, it is natural to consider estimating the gap G(q) by Taylor’s expansion.
(cid:1) around −1 − δ, and approximate Ψ
around

and ψ

(cid:16) β
σ

(cid:16) β
σ

(cid:17)

(cid:17)

G(q) ≥

Φ1(δ) −

Φ2(δ) +

1
δp

1 − θ
p

ψ(−1)q2

1 +

(cid:18)

√

σ

p +

(cid:19)

− 1

θ
2

1
p

(cid:2)1 + δ2 − θδ2 − σ (cid:0)1 + δ2(cid:1) √

p(cid:3) q2

1η1 (δ) +

η1 (δ) −

σ
√

δ

p

1 − θ
p

+

1
2δp

η2(δ)q2
1
√

θq3
1

5CT

p

(δ + 1)3 ,

where we deﬁne

Φ1(δ) = Ψ(−1 − δ) + Ψ(−1 + δ) − 2Ψ(−1),
η1(δ) = ψ(−1 + δ) − ψ(−1 − δ),

Φ2(δ) = Ψ(−1 + δ) − Ψ(−1 − δ),
η2(δ) = ψ(−1 + δ) + ψ(−1 − δ),

and CT is as deﬁned in Lemma E.2. Since 1 − σ
2p η2(δ), and (cid:0)1 + δ2(cid:1) (cid:0)1 − σ
θq2
1

p(cid:1) q2

√

1η1 (δ) / (2δp), and using the fact that δ = q1/

√

p ≥ 0, dropping those small positive terms q2

1

p (1 − θ)ψ(−1),

G(q) ≥

Φ1(δ) −

[Φ2(δ) − σ

pη1(δ)] −

(1 − σ

√

1 − θ
p
1 − θ
p

1
δp
1
δp

≥

Φ1(δ) −

[Φ2(δ) − η1(δ)] −

q2
1
p
η1 (δ)
δ

q2
1
p

√

p) η2(δ) −
(cid:18) 2θ
√
2π

+

2

√

θ
2p
3θ2
√
2π

−

q2
1
θp

q3
1η1 (δ) −
(cid:19)

+ C1θ2

,

√

θ, we obtain
√

C1

θq3
1

p

max

(cid:18) q3
1
θ3/2

(cid:19)

, 1

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

27

1 ≥
1 to simplify the expression. Substituting the estimates in Lemma E.4 and use the fact δ (cid:55)→ η1 (δ) /δ is bounded,

for some constant C1 > 0, where we have used q1 ≤ 3
1 − q2
we obtain

θ to simplify the bounds and the fact σ

√

p = (cid:112)1 − q2

√

G (p) ≥

(cid:18) 1
40
(cid:18) 1
40

1
p
q2
1
θp

−

θ

1
√
2π
1
√
2π

−

≥

(cid:19)

δ2 −

q2
1
θp

(cid:0)c1θ + c2θ2(cid:1)
(cid:19)

θ − c1θ − c2θ2

for some positive constants c1 and c2. We obtain the claimed result once θ0 is made sufﬁciently small.

θ. There exists some universal constant CT > 0 such that we have the follow polynomial

2) Auxiliary Results Used in the Proof:

√

.
Lemma E.2. Let δ
= q1/
approximations hold for all q1 ∈ (cid:0)0, 1
2
(cid:17)
α
σ
(cid:18) β
σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

ψ

ψ

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:1):

(cid:20)

−

1 −

(1 + δ)2q2
1

(cid:19)

(cid:20)

−

1 −

(δ − 1)2q2
1

≤ CT (1 + δ)2 q4
1,

≤ CT (δ − 1)2 q4
1,

1
2

1
2
1
2

1
2

(cid:21)

(cid:21)

(cid:12)
(cid:12)
ψ(−1 − δ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ψ(δ − 1)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

ψ(−1)q2
1

1
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

Ψ

−

(cid:17)

α
σ

(cid:20)

(cid:19)

(cid:20)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ψ

(cid:18) β
σ

−

Ψ(−1 − δ) −

ψ(−1 − δ)(1 + δ)q2
1

≤ CT (1 + δ)2 q4
1,

−

Ψ(δ − 1) +

ψ(δ − 1)(δ − 1)q2
1

≤ CT (δ − 1)2 q4
1,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:19)

(cid:20)

Ψ

−

−

Ψ(−1) −

λ
σ

≤ CT q4
1.

Proof: First observe that for any q1 ∈ (cid:0)0, 1

(cid:1) it holds that

2

0 ≤

1
(cid:112)1 − q2

1

(cid:18)

−

1 +

(cid:19)

q2
1
2

≤ q4
1.

Hence we have

So we have

(δ − 1)

1 +

≤ (δ − 1)

1 +

, when δ ≥ 1

−(1 + δ)

1 +

1 + q4
q2
1

≤ −

≤ −(1 + δ)

1 +

(cid:18)

(cid:18)

1
2
(cid:18)

1
2

(cid:19)

(cid:19)

(cid:19)

1
2

q2
1

α
σ

≤

≤

β
σ
β
σ

(cid:19)(cid:19)

(cid:18)

(cid:18)

(cid:19)

1
2

q2
1

,

(cid:19)

1 + q4
q2
1
(cid:19)

q2
1

(cid:18)

1
2
1
2

(cid:18)

(δ − 1)

1 +

1 + q4
q2
1

≤ (δ − 1)

1 +

, when δ ≤ 1.

(cid:18)

(cid:18)

ψ

−(1 + δ)

1 +

1
2

1 + q4
q2
1

(cid:16)

≤ ψ

−

(cid:17)

α
σ

≤ ψ

−(1 + δ)

1 +

(cid:18)

(cid:19)(cid:19)

.

1
2

q2
1

By Taylor expansion of the left and right sides of the above two-side inequality around −1 − δ using Lemma A.2,
we obtain

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

ψ

−

(cid:17)

α
σ

− ψ(−1 − δ) −

(1 + δ)2q2

≤ CT (1 + δ)2 q4
1,

1
2

(cid:12)
(cid:12)
1ψ(−1 − δ)
(cid:12)
(cid:12)

for some numerical constant CT > 0 sufﬁciently large. In the same way, we can obtain other claimed results.

Lemma E.3. For any δ ∈ [0, 3], it holds that

Φ2(δ) − η1(δ) ≥

η1 (3)
9

δ3 ≥

1
20

δ3.

(E.10)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

28

Proof: Let us deﬁne

h(δ) = Φ2(δ) − η1(δ) − Cδ3

for some C > 0 to be determined later. Then it is obvious that h(0) = 0. Direct calculation shows that

d
dδ

d
dδ

d
dδ

Φ1(δ) = η1(δ),

Φ2(δ) = η2(δ),

η1(δ) = η2(δ) − δη1(δ).

(E.11)

Thus, to show (E.10), it is sufﬁcient to show that h(cid:48)(δ) ≥ 0 for all δ ∈ [0, 3]. By differentiating h(δ) with respect to
δ and use the results in (E.11), it is sufﬁcient to have

h(cid:48)(δ) = δη1(δ) − 3Cδ2 ≥ 0 ⇐⇒ η1(δ) ≥ 3Cδ

for all δ ∈ [0, 3]. We obtain the claimed result by observing that δ (cid:55)→ η1 (δ) /3δ is monotonically decreasing over
δ ∈ [0, 3] as justiﬁed below.
Consider the function

To show it is monotonically decreasing, it is enough to show p(cid:48) (δ) is always nonpositive for δ ∈ (0, 3), or equivalently

p (δ)

.
=

η1 (δ)
3δ

=

1
√
2π

3

(cid:18)

exp

−

δ2 + 1
2

(cid:19) eδ − e−δ
δ

.

g (δ)

.
=

(cid:16)

eδ + e−δ(cid:17)

δ − (cid:0)δ2 + 1(cid:1) (cid:16)

eδ − e−δ(cid:17)

≤ 0

for all δ ∈ (0, 3), which can be easily veriﬁed by noticing that g (0) = 0 and g(cid:48) (δ) ≤ 0 for all δ ≥ 0.

Lemma E.4. For any δ ∈ [0, 3], we have

(1 − θ)Φ1(δ) −

[Φ2(δ) − η1(δ)] ≥

1
δ

(cid:18) 1
40

−

1
√
2π

θ

(cid:19)

δ2.

(E.12)

Proof: Let us deﬁne

g(δ) = (1 − θ)Φ1(δ) −

[Φ2(δ) − η1(δ)] − c0 (θ) δ2,

1
δ

where c0 (θ) > 0 is a function of θ. Thus, by the results in (E.11) and L’Hospital’s rule, we have

Φ2(δ)
δ

lim
δ→0

= lim
δ→0

η2 (δ) = 2ψ(−1),

[η2(δ) − δη1(δ)] = 2ψ(−1).

η1(δ)
δ

lim
δ→0

= lim
δ→0

Combined that with the fact that Φ1(0) = 0, we conclude g (0) = 0. Hence, to show (E.12), it is sufﬁcient to show
that g(cid:48)(δ) ≥ 0 for all δ ∈ [0, 3]. Direct calculation using the results in (E.11) shows that

Since η1 (δ) /δ is monotonically decreasing as shown in Lemma E.3, we have that for all δ ∈ (0, 3)

g(cid:48)(δ) =

1
δ2 [Φ2(δ) − η1(δ)] − θη1(δ) − 2c0 (θ) δ.

Using the above bound and the main result from Lemma E.3 again, we obtain

η1 (δ) ≤ δ lim
δ→0

η (δ)
δ

≤

2
√
2π

δ.

g(cid:48)(δ) ≥

1
20

δ −

2
√
2π

θδ − 2c0δ.

Choosing c0 (θ) = 1

40 − 1√

2π

θ completes the proof.

B. Finite Sample Concentration

In the following two subsections, we estimate the deviations around the expectations E (cid:2)Q1(q)(cid:3) and E (cid:2)Q2(q)(cid:3),
i.e., (cid:12)
(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:13)2, and show that the total deviations ﬁt into the gap G(q) we
derived in Appendix E-A. Our analysis is based on the scalar and vector Bernstein’s inequalities with moment
conditions. Finally, in Appendix E-C, we uniformize the bound by applying the classical discretization argument.

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:12) and (cid:13)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

29

1) Concentration for Q1(q):

Lemma E.5 (Bounding (cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12)). For each q ∈ Sn−1, it holds for all t > 0 that
(cid:18)

(cid:19)

P (cid:2)(cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) ≥ t(cid:3) ≤ 2 exp

−

θp3t2
8 + 4pt

.

Proof: By (E.1), we know that

Q1(q) =

X 1

k , X 1

k = x0(k)Sλ [x0(k)q1 + Zk]

1
p

p
(cid:88)

k=1

where Zk = q(cid:62)

2 gk ∼ N

(cid:16)

0, (cid:107)q2(cid:107)2
p

2

(cid:17)

. Thus, for any m ≥ 2, by Lemma A.4, we have

E (cid:2)(cid:12)

(cid:12)X 1
k

(cid:12)
(cid:12)

m(cid:3) ≤ θ

E

E

(cid:19)l

m(cid:21)

(cid:19)m

(cid:19)m m
(cid:88)

(cid:20)(cid:12)
(cid:12)
(cid:18) 1
q1√
(cid:12)
(cid:12)
√
+ Zk
(cid:12)
(cid:12)
θp
θp
(cid:12)
(cid:12)
(cid:19) (cid:18) q1√
(cid:18)m
(cid:18) 1
√
l
θp
θp
(cid:18) 1
(cid:19) (cid:18) q1√
(cid:18)m
√
l
θp
θp
(cid:19)m (cid:18) q1√
(cid:18) 1
(cid:107)q2(cid:107)2√
√
p
θp
θp
(cid:19)m−2
(cid:19)m

l=0
(cid:19)m m
(cid:88)

(cid:19)l

l=0

+

θ

(cid:19)m

= θ

= θ

≤

m!
2

(cid:104)

|Zk|m−l(cid:105)

(m − l − 1)!!

(cid:19)m−l

(cid:18) (cid:107)q2(cid:107)2√

p

(cid:18) 2
θp
X = 4/(θp2) and R = 2/(θp), apply Lemma A.7, we get

(cid:18) 2
θp

4
θp2

m!
2

m!
2

≤

=

θ

let σ2

P (cid:2)(cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) ≥ t(cid:3) ≤ 2 exp

(cid:18)

−

θp3t2
8 + 4pt

(cid:19)

.

as desired.

2) Concentration for Q2(q):

Lemma E.6 (Bounding (cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

P (cid:2)(cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2). For each q ∈ Sn−1, it holds for all t > 0 that
(cid:18)
(cid:13)2 > t(cid:3) ≤ 2(n + 1) exp

θp3t2
√
128n + 16

θnpt

−

(cid:19)

.

Before proving Lemma E.6, we record the following useful results.

Lemma E.7. For any positive integer s, l > 0, we have

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

s
(cid:13)
(cid:13)
2

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

≤

(l + s)!!
2

(cid:107)q2(cid:107)l
2

(2
(cid:0)√

√
n)s
p(cid:1)s+l .

In particular, when s = l, we have

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

l
(cid:13)
(cid:13)
2

(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

l!
2

(cid:107)q2(cid:107)l
2

√

(cid:18) 4

(cid:19)l

n

p

≤

(cid:17)

2

Proof: Let Pq(cid:107)

= q2q(cid:62)
2
(cid:107)q2(cid:107)2
2
complement, respectively. By Lemma A.4, we have
gk(cid:13)
(cid:13)
(cid:13)2

(cid:20)(cid:16)(cid:13)
(cid:13)
(cid:13)Pq(cid:107)

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

and Pq⊥

I − 1

≤ E

s
(cid:13)
(cid:13)
2

(cid:107)q2(cid:107)2
2

l(cid:21)

=

E

2

2

q2q(cid:62)
2

(cid:16)

+

(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)2

2 gk(cid:12)
(cid:17)s (cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

denote the projection operators onto q2 and its orthogonal

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

30

Using Lemma A.5 and the fact that (cid:13)

(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)2

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

s
(cid:13)
(cid:13)
2

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

≤ (cid:107)q2(cid:107)l
2

=

=

s
(cid:88)

i=0
s
(cid:88)

i=0

(cid:18)s
i
(cid:18)s
i

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)

(cid:21)

i

(cid:21)

i

2

2

E

E

≤ (cid:107)q2(cid:107)l
2

s
(cid:88)

i=0

(cid:21)

s−i

l (cid:13)
(cid:13)
(cid:13)Pq(cid:107)

2

gk(cid:13)
(cid:13)
(cid:13)
2

l+s−i(cid:21)

1
(cid:107)q2(cid:107)s−i
2

(cid:19)l+s−i

(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:21) (cid:18) 1
√
p

2

(l + s − i − 1)!!.

2

i

E

(cid:19)

(cid:19)i

s
(cid:88)

gk(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

(cid:13)2, we obtain
(cid:19) (cid:18) √
√

(cid:18)s
i
(cid:13)gk(cid:13)
≤ (cid:13)
(cid:18)s
i
(cid:19)l (l + s)!!
2
√
n)s
p(cid:1)s+l .

i=0
(cid:18) 1
√
p

(cid:107)q2(cid:107)l
2

(2
(cid:0)√

n
p

i!!

≤ (cid:107)q2(cid:107)l
2

≤

(l + s)!!
2

(l + s − i − 1)!!

(cid:19)l+s−i

(cid:18) 1
√
p

(cid:18) √
√

n
p

+

1
√
p

(cid:19)s

Now, we are ready to prove Lemma E.6,

Proof: By (E.1), note that

Q2 =

X2

k, X2

k = gkSλ [x0(k)q1 + Zk]

1
p

p
(cid:88)

k=1

where Zk = q(cid:62)

E (cid:2)(cid:13)

(cid:13)X2
k

(cid:13)
(cid:13)

m
2

2 gk. Thus, for any m ≥ 2, by Lemma E.7, we have
(cid:104)(cid:13)
(cid:13)

+ (1 − θ)E

(cid:3) ≤ θE

+ q(cid:62)

m(cid:21)

m

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

m

(cid:13)
(cid:13)

l (cid:13)
(cid:13)

2 gk
(cid:13)gk(cid:13)
(cid:19) (m + l)!!
2
q1√
θp

+

p

m

(cid:13)gk(cid:13)

(cid:13)
(cid:13)
2

m(cid:105)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:13)gk(cid:13)
(cid:104)(cid:13)
(cid:13)

(cid:13)
(cid:13)
2

m

m−l

q1√
θp

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (1 − θ)E

(cid:18) (cid:107)q2(cid:107)2√

p

(cid:19)l (cid:12)
(cid:12)
(cid:12)
(cid:12)

q1√
θp

(cid:12)
(cid:12)
(cid:12)
(cid:12)

m−l

(cid:19)m

+ (1 − θ)

m!
2

(cid:107)q2(cid:107)m
2

(cid:18) 4

√

n

p

m!
2
(cid:19)m

m(cid:105)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

+ (1 − θ)

(cid:107)q2(cid:107)m
2

√

(cid:18) 4

(cid:19)m

n

p

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:19)
(cid:18)m
l

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

m
(cid:88)

q1√
θp
(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

(cid:18)m
l

l=0

(cid:19)m (cid:18) (cid:107)q2(cid:107)2√

≤ θ

≤ θ

≤ θ

≤

(cid:19)m m
(cid:88)

l=0
√
(cid:18) 2
√

n
p
√
(cid:18) 4
√
√
(cid:18) 8
√

n
p
(cid:19)m
n
θp

m!
2
m!
2

.

√

√

Taking σ2

X = 64n/(θp2) and R = 8

n/(

θp) and using vector Bernstein’s inequality in Lemma A.8, we obtain

P (cid:2)(cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 ≥ t(cid:3) ≤ 2(n + 1) exp

(cid:18)

−

θp3t2
√
128n + 16

θnpt

(cid:19)

,

as desired.

C. Union Bound
Proposition E.8 (Uniformizing the Bounds). Suppose that θ > 1/
C (ξ), such that whenever p ≥ C (ξ) n4 log n, we have

√

n. Given any ξ > 0, there exists some constant

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤

,

2ξ
θ5/2n3/2p
2ξ
θ2np

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

31

hold uniformly for all q ∈ Sn−1, with probability at least 1 − c(ξ)p−2 for a positive constant c(ξ).

Proof: We apply the standard covering argument. For any ε ∈ (0, 1), by Lemma A.12, the unit hemisphere of

interest can be covered by an ε-net Nε of cardinality at most (3/ε)n. For any q ∈ Sn−1, it can be written as

, which is an independent copy of y = [x0, g](cid:62).

q = q(cid:48) + e

where q(cid:48) ∈ Nε and (cid:107)e(cid:107)2 ≤ ε. Let a row of Y be yk = (cid:2)x0(k), gk(cid:3)(cid:62)
By (E.1), we have
(cid:12)
(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)
(cid:110)

(cid:69)(cid:105)

(cid:104)(cid:68)

(cid:104)(cid:68)

(cid:104)

yk, q(cid:48) + e

− E

x0(k)Sλ

yk, q(cid:48) + e

x0(k)Sλ

(cid:69)(cid:105)(cid:105)(cid:111)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

p
(cid:88)

k=1

1
p

p
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
p
(cid:12)
(cid:12)E (cid:2)x0Sλ

k=1
p
(cid:88)

k=1

=

≤

+ (cid:12)

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48) + e

(cid:69)(cid:105)

−

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48)(cid:69)(cid:105)

+

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48)(cid:69)(cid:105)

− E (cid:2)x0Sλ

(cid:2)(cid:10)y, q(cid:48)(cid:11)(cid:3)(cid:3)

p
(cid:88)

1
p

(cid:2)(cid:10)y, q(cid:48)(cid:11)(cid:3)(cid:3) − E (cid:2)x0Sλ

k=1
(cid:2)(cid:10)y, q(cid:48) + e(cid:11)(cid:3)(cid:3)(cid:12)
(cid:12) .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Using Cauchy-Schwarz inequality and the fact that Sλ [·] is a nonexpansive operator, we have

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ (cid:12)

(cid:12)Q1(q(cid:48)) − E (cid:2)Q1(q(cid:48))(cid:3)(cid:12)

(cid:12) +

|x0(k)|

p
(cid:88)

(cid:32)

1
p

k=1
1
√
θp

(cid:33)

(cid:107)e(cid:107)2

(cid:13)
(cid:13)

(cid:13)yk(cid:13)
+ E [|x0| (cid:107)y(cid:107)2]
(cid:13)
(cid:13)2
(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)

≤ (cid:12)

(cid:12)Q1(q(cid:48)) − E (cid:2)Q1(q(cid:48))(cid:3)(cid:12)

(cid:18) 2
√
θp
(cid:13)2 ≤ (cid:112)n/p + 2(cid:112)2 log(2p)/p with probability at least 1 − c1p−3. Also E [(cid:107)g(cid:107)2] ≤
≤ (cid:112)n/p. Taking t = ξθ−5/2n−3/2p−1 in Lemma E.5 and applying a union bound with ε =

+ E [(cid:107)g(cid:107)2]

+ max
k∈[p]

(cid:13)gk(cid:13)
(cid:13)

(cid:12) + ε

(cid:19)

.

By Lemma A.10, maxk∈[p]
(cid:105)(cid:17)1/2
(cid:16)

(cid:104)

E

(cid:107)g(cid:107)2
2

ξθ−2n−2(log 2p)−1/2/7, and combining with the above estimates, we obtain that

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

ξ
θ5/2n3/2p

+

ξ
7θ5/2n2(cid:112)log(2p)p

√

(cid:16)
4

(cid:17)
n + 2(cid:112)2 log(2p)

≤

2ξ
θ5/2n3/2p

holds for all q ∈ Sn−1, with probability at least

1 − c1p−3 − 2 exp (cid:0)−c3 (ξ) p/(θ4n3) + c4 (ξ) n log n + c5(ξ)n log log(2p)(cid:1) .

Similarly, by (E.1), we have

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 =

(cid:104)(cid:68)

gkSλ

yk, q(cid:48) + e

− E (cid:2)gSλ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
p

p
(cid:88)

(cid:110)

k=1

(cid:69)(cid:105)

(cid:32)

(cid:13)
(cid:13)
(cid:2)(cid:10)y, q(cid:48) + e(cid:11)(cid:3)(cid:3)(cid:111)
(cid:13)
(cid:13)
(cid:13)2

≤ (cid:13)

(cid:13)Q2(q(cid:48)) − E (cid:2)Q2(q(cid:48))(cid:3)(cid:13)

≤ (cid:13)

(cid:13)Q2(q(cid:48)) − E (cid:2)Q2(q(cid:48))(cid:3)(cid:13)

(cid:33)

p
(cid:88)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

k=1

(cid:13)2 +

1
p
(cid:20)

+ E [(cid:107)g(cid:107)2 (cid:107)y(cid:107)2]

(cid:13)yk(cid:13)
(cid:13)
(cid:13)2
(cid:18) 1
√
θp
(cid:13)2, and taking t = ξθ−2n−1p−1 in Lemma E.6 and applying a union

(cid:13)gk(cid:13)
(cid:13)
(cid:13)2
(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

(cid:107)e(cid:107)2
√
√

(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

+ max
k∈[p]

(cid:13)2 + ε

max
k∈[p]

n
θp

n
p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

+

(cid:19)

(cid:21)

.

Applying the above estimates for maxk∈[p]
bound with ε = ξθ−2n−2 log−1(2p)/30, we obtain that

(cid:13)gk(cid:13)
(cid:13)

(cid:13)
(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 ≤

ξ
θ2np

ξ
θ2np

+

+

≤

ξ
30θ2n2 log(2p)

ξ
30θ2n2 log(2p)

+

(cid:32)(cid:114) n
p



4

(cid:26) 16 log(2p)
p

(cid:27)

+

10n
p

(cid:115)

2 log(2p)
p

(cid:33)2

+

2n
p






IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

32

≤

2ξ
θ2np

holds for all q ∈ Sn−1, with probability at least

1 − c1p−3 − exp (cid:0)−c6 (ξ) p/(θ3n3) + c7(ξ)n log n + c8(ξ)n log log(2p)(cid:1) .

Taking p ≥ C9(ξ)n4 log n and simplifying the probability terms complete the proof.

D. Q(q) approximates Q(q)

Proposition E.9. Suppose θ > 1/
p ≥ C (ξ) n4 log n, the following bounds

√

n. For any ξ > 0, there exists some constant C (ξ), such that whenever

(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)

(cid:12) ≤

sup
q∈Sn−1

sup
q∈Sn−1

(cid:13)
(cid:13)Q2(q) − Q2(q)(cid:13)

(cid:13)2 ≤

ξ
θ5/2n3/2p

ξ
θ2np

,

(E.13)

(E.14)

hold with probability at least 1 − c(ξ)p−2 for a positive constant c(ξ).

Proof: First, for any q ∈ Sn−1, from (E.1), we know that
(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)
(cid:12)
p
(cid:88)

p
(cid:88)

(cid:104)

(cid:104)

x0(k)Sλ

q(cid:62)yk(cid:105)

−

q(cid:62)yk(cid:105)

x0(k)
(cid:107)x0(cid:107)2

Sλ

=

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

1
p

1
p

k=1
p
(cid:88)

k=1
p
(cid:88)

k=1

1
p

1
p

k=1
p
(cid:88)

k=1
(cid:104)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

p
(cid:88)

k=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|x0(k)|

1 −

1
p

p
(cid:88)

k=1

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

−

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

+

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

−

|x0(k)|

(cid:12)
(cid:12)
(cid:12)Sλ

(cid:104)

q(cid:62)yk(cid:105)

− Sλ

q(cid:62)yk(cid:105)(cid:12)
(cid:12)
(cid:12) +

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)Sλ

(cid:104)

q(cid:62)yk(cid:105)(cid:12)
(cid:12)
(cid:12) .

1
p

p
(cid:88)

k=1

x0(k)
(cid:107)x0(cid:107)2

(cid:104)

q(cid:62)yk(cid:105)

Sλ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

For any I = supp(x0), using the fact that Sλ[·] is a nonexpansive operator, we have

(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)

(cid:12) ≤

sup
q∈Sn−1

1
p

sup
q∈Sn−1

(cid:88)

k∈I
(cid:18)

=

√

1
θp3/2

|x0(k)|

(cid:12)
(cid:12)

(cid:12)q(cid:62) (cid:16)

yk − yk(cid:17)(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

(cid:13)
(cid:13)YI − YI

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 +

1 −

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)YI(cid:107)(cid:96)2→(cid:96)1

.

(cid:88)

k∈I

sup
q∈Sn−1
(cid:19)

|x0(k)|

(cid:12)
(cid:12)

(cid:12)q(cid:62)yk(cid:12)

(cid:12)
(cid:12)

By Lemma B.1 and Lemma B.3 in Appendix B, we have the following holds

(cid:12)
(cid:12)Q1(q) − Q1(q)(cid:12)

(cid:12) ≤

sup
q∈Sn−1

√

1
θp3/2

(cid:32)

(cid:114)

20

√

4

2

(cid:115)

n log p
θ

+

5

n log p
θ2p

(cid:33)

× 7(cid:112)2θp

≤

32
θp3/2

(cid:112)n log p,

with probability at least 1 − c1p−2, provided p ≥ C2n and θ > 1/
n. Simple calculation shows that it is enough
to have p ≥ C3 (ξ) n4 log n for some sufﬁciently large C1 (ξ) to obtain the claimed result in (E.13). Similarly, by
Lemma B.3 and Lemma B.4 in Appendix B, we have

√

sup
q∈Sn−1

= sup

q∈Sn−1

≤ sup

q∈Sn−1

(cid:104)

1
p

p
(cid:88)

(cid:13)
(cid:13)Q2(q) − Q2(q)(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1
p
(cid:88)

gkSλ

gkSλ

1
p

(cid:104)

k=1

q(cid:62)yk(cid:105)

q(cid:62)yk(cid:105)

−

−

1
p

1
p

p
(cid:88)

k=1
p
(cid:88)

k=1

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
p

p
(cid:88)

k=1

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

−

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

1
p

p
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

33

p
(cid:88)

k=1

sup
q∈Sn−1
(cid:13)G − G(cid:48)(cid:13)
(cid:0)(cid:13)
(cid:32)

≤

≤

≤

1
p

1
p

1
p

(cid:12)
(cid:12)

(cid:13)
(cid:13)

(cid:12)q(cid:62)yk(cid:12)
(cid:13)gk − g(cid:48)k(cid:13)
(cid:12)
(cid:13)
(cid:12) +
(cid:13)2
(cid:13)(cid:96)2→(cid:96)1 + (cid:13)
(cid:13)Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞

1
p

sup
q∈Sn−1

(cid:13)G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)∞

(cid:12)
(cid:12)

(cid:12)q(cid:62) (cid:16)

yk − yk(cid:17)(cid:12)
(cid:12)
(cid:12)

p
(cid:88)

(cid:13)
(cid:13)

(cid:13)g(cid:48)k(cid:13)
(cid:13)
(cid:13)2
k=1
(cid:13)Y − Y(cid:13)
(cid:13)
√

(cid:1)

(cid:13)(cid:96)2→(cid:96)1
n, (cid:112)log(2p))

(cid:33)

120 max(n, log(2p))
√
p

+

300(cid:112)n log(2p) max(
θp

√

with probability at least 1 − c4p−2 provided p ≥ C4n and θ > 1/
obtain the claimed result (E.14).

≤

420(cid:112)n log(2p) max(
θ1/2p3/2

√

n, (cid:112)log(2p))

√

n. It is sufﬁcient to have p ≥ C5 (ξ) n4 log n to

APPENDIX F
LARGE |q1| ITERATES STAYING IN SAFE REGION FOR ROUNDING

In this appendix, we prove Proposition IV.5 in Section IV.

Proof of Proposition IV.5: For notational simplicity, w.l.o.g. we will proceed to prove assuming q1 > 0. The

proof for q1 < 0 is similar by symmetry. It is equivalent to show that

(cid:107)Q2 (q)(cid:107)2
|Q1 (q)|

<

(cid:114) 1
4θ

− 1,

which is implied by

for any q ∈ Sn−1 satisfying q1 > 3
(cid:115)

L (q)

.
=

(cid:13)
(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 + (cid:13)
E (cid:2)Q1 (q)(cid:3) − (cid:12)
√
θ. Recall from (E.7) that

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)2
(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)

(cid:114) 1
4θ

<

− 1

E (cid:2)Q1(q)(cid:3) =

(cid:26)(cid:20)

(cid:16)

αΨ

−

(cid:17)

α
σ

θ
p

+ βΨ

(cid:19)(cid:21)

(cid:20)

+ σ

ψ

(cid:18) β
σ

(cid:19)

(cid:18) β
σ

(cid:16)

− ψ

−

(cid:17)(cid:21)(cid:27)

,

α
σ

where

α =

1
√
p

(cid:18) q1√
θ

(cid:19)

+ 1

,

β =

1
√
p

(cid:18) q1√
θ

(cid:19)

− 1

,

σ = (cid:107)q2(cid:107)2 /

√

p.

Noticing the fact that
(cid:18) β
σ

ψ

(cid:19)

(cid:16)

− ψ

≥ 0,

(cid:17)

(cid:19)

−

α
σ
(cid:18) β
σ

we have

Ψ

= Ψ

− 1

≥ Ψ (2) ≥

for q1 > 3

θ,

(cid:32)

1
(cid:112)1 − q2

1

(cid:18) q1√
θ

(cid:19)(cid:33)

19
20

√

E (cid:2)Q1 (q)(cid:3) ≥

√

θ
p

(cid:26) q1√
θ

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)

(cid:18) β
σ

+ Ψ

−

− Ψ

(cid:16)

(cid:17)

α
σ

(cid:19)(cid:27)

≥

(cid:18) β
σ

√
2

θ

p

(cid:19)

(cid:18) β
σ

Ψ

≥

√

19
10

θ
p

.

Moreover, from (E.8), we have

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 = (cid:107)q2(cid:107)2

(cid:26) 2 (1 − θ)
p

(cid:18)

(cid:19)

Ψ

−

+

λ
σ

θ
p

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)(cid:27)

≤

2 (1 − θ)
p

θ
p

Ψ (−1) +

[Ψ (−1) + 1] ≤

Ψ (−1) +

≤

+

2
p

2
5p

θ
p

,

(cid:18) β
σ
θ
p

where we have used the fact that −λ/σ ≤ −1 and −α/σ ≤ −1. Moreover, from results in Proposition E.8 and
Proposition E.9 in Appendix E, we know that

sup
q∈Sn−1

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ sup
q∈Sn−1

(cid:12)
(cid:12)Q1(q) − Q1(q)(cid:12)

(cid:12) + sup
q∈Sn−1

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

1
2 × 105θ5/2n3/2p

,

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

34

sup
q∈Sn−1

(cid:13)Q(q) − E (cid:2)Q(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤ sup
q∈Sn−1

(cid:13)Q(q) − Q(q)(cid:13)
(cid:13)

(cid:13)2 + sup
q∈Sn−1

(cid:13)Q(q) − E (cid:2)Q(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤

1
2 × 105θ2np

hold with probability at least 1 − c1p−2 provided that p ≥ Ω (cid:0)n4 log n(cid:1). Hence, with high probability, we have

L (q) ≤

2/(5p) + θ/p + (2 × 105θ2np)−1
√

θ/(10p) − (2 × 105θ5/2n3/2p)−1

19

≤

3/5
√

18

θ/10

≤

1
√
3

θ

<

(cid:114) 1
4θ

− 1,

whenever θ is sufﬁciently small. This completes the proof.

Now, keep the notation in Appendix E for general orthonormal basis (cid:98)Y = YU. For any current iterate q ∈ Sn−1
(cid:11)(cid:12)
(cid:12) = |(cid:104)Uq, e1(cid:105)| ≥ 3
(cid:69)(cid:12)
(cid:12)
(cid:12)

that is close enough to the target solution, i.e., (cid:12)
(cid:12)
(cid:12)
(cid:68)
(cid:12)
(cid:12)

(cid:10)q, U(cid:62)e1
(cid:12)
(cid:16)
(cid:68)
(cid:12)
(cid:12)

θ, we have

(cid:69)(cid:12)
(cid:12)
(cid:12)

√

(cid:17)

(cid:16)

(cid:17)

q; (cid:98)Y
(cid:16)

Q
(cid:13)
(cid:13)
(cid:13)Q

q; (cid:98)Y

, U(cid:62)e1
(cid:17)(cid:13)
(cid:13)
(cid:13)2

=

UQ
(cid:13)
(cid:13)
(cid:13)UQ

q; (cid:98)Y
(cid:16)

q; (cid:98)Y

, e1
(cid:17)(cid:13)
(cid:13)
(cid:13)2

=

|(cid:104)Q (Uq; Y) , e1(cid:105)|
(cid:107)Q (Uq; Y)(cid:107)2

,

where we have applied the identity proved in (E.2). Taking Uq ∈ Sn−1 as the object of interest, by Proposition IV.5,
we conclude that

with high probability.

|(cid:104)Q (Uq; Y) , e1(cid:105)|
(cid:107)Q (Uq; Y)(cid:107)2

√

≥ 2

θ

APPENDIX G
BOUNDING ITERATION COMPLEXITY

In this appendix, we prove Proposition IV.6 in Section IV.

Proof of Proposition IV.6: Recall from Proposition IV.4 in Section IV, the gap

G(q) =

|Q1(q)|
|q1|

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2

≥

1
104θ2np

√

holds uniformly over q ∈ Sn−1 satisfying
p ≥ C2n4 log n. The gap G(q) implies that

1
√

10

θn

≤ |q1| ≤ 3

θ, with probability at least 1 − c1p−2, provided

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)

.
=

⇐⇒

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12) ≥

|Q1(q)|
(cid:107)Q (q)(cid:107)2
(cid:114)
|q1|
(cid:107)q2(cid:107)2

(cid:32)

≥

1 −

=⇒

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)

≥ |q1|2

1 +

+

|q1|
104θ2np (cid:107)Q (q)(cid:107)2
|q1|
104θ2np (cid:107)Q (q)(cid:107)2
(cid:33)

|q1| (cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2 (cid:107)Q (q)(cid:107)2
(cid:12)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)
(cid:107)q2(cid:107)2
2
108θ4n2p2 (cid:107)Q (q)(cid:107)2
2

+

.

Given the set Γ deﬁned in (IV.7), now we know that

sup
q∈Γ

(cid:107)Q (q)(cid:107)2 ≤ sup
q∈Γ

(cid:12)E (cid:2)Q1(q)(cid:3) − Q1 (q)(cid:12)
(cid:12)

(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:13)
(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)

+ sup
q∈Γ
(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

≤ sup
q∈Γ

(cid:12) + sup
q∈Sn−1
(cid:13)2 + sup
q∈Sn−1
(cid:12)E (cid:2)Q2(q)(cid:3)(cid:12)
(cid:12)

(cid:12) + sup
q∈Γ

(cid:12) +

1
pn

(cid:13)E (cid:2)Q2(q)(cid:3) − Q2 (q)(cid:13)
(cid:13)

(cid:12) + sup
q∈Sn−1
(cid:13)2 + sup
q∈Sn−1

(cid:12)Q1(q) − Q1 (q)(cid:12)
(cid:12)
(cid:12)
(cid:13)Q2(q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2

with probability at least 1 − c3p−2 provided p ≥ C4n4 log n and θ > 1/
n. Here we have used Proposition E.8 and
Proposition E.9 to bound the magnitudes of the four difference terms. To bound the magnitudes of the expectations,
we have

(cid:12)
(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) =

E

(cid:104)

x0(k)Sλ

x0(k)q1 + q(cid:62)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

1
p

p
(cid:88)

k=1

2 gk(cid:105)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
√
θp

(cid:18) 1
√
θp

(cid:19)

+ E [(cid:107)g(cid:107)2]

≤

√
3
√

n
θp

≤

3n
p

,

√

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

35

(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 =

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

1
p

p
(cid:88)

k=1

(cid:104)

gkSλ

x0(k)q1 + q(cid:62)

2 gk(cid:105)

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
n. Thus, we obtain that

1
√
θp

≤

√

hold uniformly for all q ∈ Γ, provided θ > 1/

E [(cid:107)g(cid:107)2] + E

(cid:107)g(cid:107)2
2

(cid:104)

(cid:105)

≤

3n
p

with probability at least 1 − c3p−2 provided p ≥ C4n4 log n and θ > 1/

n. So we conclude that

(cid:107)Q (q)(cid:107)2 ≤

sup
q∈Γ

3n
p

+

+

≤

3n
p

1
np

7n
p

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)
|q1|

(cid:114)

≥

1 +

1 − 9θ
108 × 72 × θ4n4 .
, we will need at most

Thus, starting with any q ∈ Sn−1 such that |q1| ≥ 1
√
10
√

θn

(cid:16)

3

(cid:17)

θ/

1
√

10

θn

T =

2 log
(cid:16)

log

1 +

1−9θ
108×72×θ4n4

(cid:17) =

log
√

√

n)

2 log (30θ
(cid:16)

1 +

1−9θ
108×72×θ4n4

(cid:17) ≤

√

2 log (30θ

n)

(log 2)

1−9θ
108×72×θ4n4

≤ C5n4 log n

steps to arrive at a q ∈ Sn−1 with | ¯q1| ≥ 3
that log (1 + x) ≥ x log 2 for x ∈ [0, 1] to simplify the ﬁnal result.

θ for the ﬁrst time. Here we have assumed θ0 < 1/9 and used the fact

APPENDIX H
ROUNDING TO THE DESIRED SOLUTION

min
q

(cid:107)Yq(cid:107)1 ,

s.t. (cid:104)q, q(cid:105) = 1.

In this appendix, we prove Proposition IV.7 in Section IV. For convenience, we will assume the notations we

used in Appendix B. Then the rounding scheme can be written as

(H.1)

(H.2)

(H.3)

We will show the rounding procedure get us to the desired solution with high probability, regardless of the particular
orthonormal basis used.

Proof of Proposition IV.7: The rounding program (H.1) can be written as

Consider its relaxation

inf
q

(cid:107)Yq(cid:107)1 ,

s.t. q1q1 + (cid:104)q2, q2(cid:105) = 1.

inf
q

(cid:107)Yq(cid:107)1 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1.

It is obvious that the feasible set of (H.3) contains that of (H.2). So if e1/q1 is the unique optimal solution (UOS)
of (H.3), it is also the UOS of (H.2). Let I = supp(x0), and consider a modiﬁed problem

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

|q1| − (cid:13)

(cid:13)G(cid:48)

Iq2

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

I cq2

(cid:13)
(cid:13)1 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1.

(H.4)

The objective value of (H.4) lower bounds the objective value of (H.3), and are equal when q = e1/q1. So if
q = e1/q1 is the UOS to (H.4), it is also UOS to (H.3), and hence UOS to (H.2) by the argument above. Now
(cid:13)1 ≥ − (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)
(cid:13)
(cid:0)G − G(cid:48)(cid:1) q2
(cid:13)
(cid:13)G − G(cid:48)(cid:13)
≥ − (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

(cid:13)
(cid:13)1
(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2 .

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

(cid:13)G(cid:48)

I cq2

− (cid:13)

Iq2

When p ≥ C1n, by Lemma A.14 and Lemma B.3, we know that

− (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

≥ −

(cid:114) 2
π

6
5

√

2θ

(cid:13)G − G(cid:48)(cid:13)
(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2
(cid:114) 2
24
π
25

p (cid:107)q2(cid:107)2 +

√

(1 − 2θ)

p (cid:107)q2(cid:107)2 − 4

√

n (cid:107)q2(cid:107)2 − 7(cid:112)log(2p) (cid:107)q2(cid:107)2

.
= ζ (cid:107)q2(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

holds with probability at least 1 − c2p−2. Thus, we make a further relaxation of problem (H.2) by

|q1| + ζ (cid:107)q2(cid:107)2 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1,

36

(H.5)

whose objective value lower bounds that of (H.4). By similar arguments, if e1/q1 is UOS to (H.5), it is UOS to (H.2).
At the optimal solution to (H.5), notice that it is necessary to have sign(q1) = sign(q1) and q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 = 1.
So (H.5) is equivalent to

|q1| + ζ (cid:107)q2(cid:107)2 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 = 1.

(H.6)

which is further equivalent to

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q1

|q1| + ζ

x0
(cid:107)x0(cid:107)2

1 − |q1| |q1|
(cid:107)q2(cid:107)2
Notice that the problem in (H.7) is linear in |q1| with a compact feasible set. Since the objective is also monotonic
in |q1|, it indicates that the optimal solution only occurs at the boundary points |q1| = 0 or |q1| = 1/ |q1| Therefore,
q = e1/q1 is the UOS of (H.7) if and only if
1
|q1|

1
|q1|

ζ
(cid:107)q2(cid:107)2

x0
(cid:107)x0(cid:107)2

|q1| ≤

(H.7)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

s.t.

<

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

.

.

,

Since

(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)1

√

≤

2θp conditioned on E0, it is sufﬁcient to have

√

2θp
√
θ
2

≤ ζ =

(cid:32)

(cid:114) 2
π

24
25

√

p

1 −

θ −

9
2

(cid:114) π
2

(cid:114) n
p

25
6

−

175
24

(cid:114) π
2

(cid:115)

(cid:33)

.

log(2p)
p

Therefore there exists a constant θ0 > 0, such that whenever θ ≤ θ0 and p ≥ C3(θ0)n, the rounding returns e1/q1.
A bit of thought suggests one can take a universal C3 for all possible choice of θ0, completing the proof.

When the input basis is (cid:98)Y = YU for some orthogonal matrix U (cid:54)= I, if the ADM algorithm produces some

√

q = U(cid:62)q(cid:48), such that q(cid:48)

1 > 2

θ. It is not hard to see that now the rounding (H.1) is equivalent to

min
q

(cid:107)YUq(cid:107)1 ,

s.t. (cid:10)q(cid:48), Uq(cid:11) = 1.

Renaming Uq, it follows from the above argument that at optimum q(cid:63) it holds that Uq(cid:63) = γe1 for some constant
γ with high probability.

REFERENCES

[1] Q. Qu, J. Sun, and J. Wright, “Finding a sparse vector in a subspace: Linear sparsity using alternating directions,”

in Advances in Neural Information Processing Systems, 2014.

[2] E. J. Cand`es and T. Tao, “Decoding by linear programming,” Information Theory, IEEE Transactions on,

vol. 51, no. 12, pp. 4203–4215, 2005.

[3] D. L. Donoho, “For most large underdetermined systems of linear equations the minimal (cid:96)1-norm solution is
also the sparsest solution,” Communications on pure and applied mathematics, vol. 59, no. 6, pp. 797–829,
2006.

[4] S. T. McCormick, “A combinatorial approach to some sparse matrix problems.,” tech. rep., DTIC Document,

1983.

[5] T. F. Coleman and A. Pothen, “The null space problem i. complexity,” SIAM Journal on Algebraic Discrete

Methods, vol. 7, no. 4, pp. 527–537, 1986.

[6] M. Berry, M. Heath, I. Kaneko, M. Lawo, R. Plemmons, and R. Ward, “An algorithm to compute a sparse

basis of the null space,” Numerische Mathematik, vol. 47, no. 4, pp. 483–504, 1985.

[7] J. R. Gilbert and M. T. Heath, “Computing a sparse basis for the null space,” SIAM Journal on Algebraic

Discrete Methods, vol. 8, no. 3, pp. 446–459, 1987.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

37

[8] I. S. Duff, A. M. Erisman, and J. K. Reid, Direct Methods for Sparse Matrices. New York, NY, USA: Oxford

[9] A. J. Smola and B. Schlkopf, “Sparse greedy matrix approximation for machine learning,” pp. 911–918, Morgan

University Press, Inc., 1986.

Kaufmann, 2000.

[10] T. Kavitha, K. Mehlhorn, D. Michail, and K. Paluch, “A faster algorithm for minimum cycle basis of graphs,”

in 31st International Colloquium on Automata, Languages and Programming, pp. 846–857, Springer, 2004.

[11] L.-A. Gottlieb and T. Neylon, “Matrix sparsiﬁcation and the sparse null space problem,” in Approximation,

Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 205–218, Springer, 2010.

[12] J. Mairal, F. Bach, and J. Ponce, “Sparse modeling for image and vision processing,” arXiv preprint

arXiv:1411.3230, 2014.

[13] D. A. Spielman, H. Wang, and J. Wright, “Exact recovery of sparsely-used dictionaries,” in Proceedings of the

25th Annual Conference on Learning Theory, 2012.

[14] P. Hand and L. Demanet, “Recovering the sparsest element in a subspace,” arXiv preprint arXiv:1310.1654,

[15] J. Sun, Q. Qu, and J. Wright, “Complete dictionary recovery over the sphere,” arXiv preprint arXiv:1504.06785,

[16] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” Journal of computational and

graphical statistics, vol. 15, no. 2, pp. 265–286, 2006.

[17] I. M. Johnstone and A. Y. Lu, “On consistency and sparsity for principal components analysis in high

dimensions,” Journal of the American Statistical Association, vol. 104, no. 486, 2009.

[18] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. Lanckriet, “A direct formulation for sparse pca using

semideﬁnite programming,” SIAM review, vol. 49, no. 3, pp. 434–448, 2007.

[19] R. Krauthgamer, B. Nadler, D. Vilenchik, et al., “Do semideﬁnite relaxations solve sparse PCA up to the

information limit?,” The Annals of Statistics, vol. 43, no. 3, pp. 1300–1322, 2015.

[20] T. Ma and A. Wigderson, “Sum-of-squares lower bounds for sparse pca,” arXiv preprint arXiv:1507.06370,

2013.

2015.

2015.

[21] V. Q. Vu, J. Cho, J. Lei, and K. Rohe, “Fantope projection and selection: A near-optimal convex relaxation of

sparse pca,” in Advances in Neural Information Processing Systems, pp. 2670–2678, 2013.

[22] J. Lei, V. Q. Vu, et al., “Sparsistency and agnostic inference in sparse pca,” The Annals of Statistics, vol. 43,

[23] Z. Wang, H. Lu, and H. Liu, “Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial

no. 1, pp. 299–322, 2015.

time,” arXiv preprint arXiv:1408.5352, 2014.

[24] A. d’Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet, “A direct formulation of sparse PCA using

semideﬁnite programming,” SIAM Review, vol. 49, no. 3, 2007.

[25] Y.-B. Zhao and M. Fukushima, “Rank-one solutions for homogeneous linear matrix equations over the positive

semideﬁnite cone,” Applied Mathematics and Computation, vol. 219, no. 10, pp. 5569–5583, 2013.

[26] Y. Dai, H. Li, and M. He, “A simple prior-free method for non-rigid structure-from-motion factorization,” in
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2018–2025, IEEE, 2012.
[27] G. Beylkin and L. Monz´on, “On approximation of functions by exponential sums,” Applied and Computational

Harmonic Analysis, vol. 19, no. 1, pp. 17–48, 2005.

[28] C. T. Manolis and V. Rene, “Dual principal component pursuit,” arXiv preprint arXiv:1510.04390, 2015.
[29] M. Zibulevsky and B. A. Pearlmutter, “Blind source separation by sparse decomposition in a signal dictionary,”

Neural computation, vol. 13, no. 4, pp. 863–882, 2001.

[30] A. Anandkumar, D. Hsu, M. Janzamin, and S. M. Kakade, “When are overcomplete topic models identiﬁable?
uniqueness of tensor tucker decompositions with structured sparsity,” in Advances in Neural Information
Processing Systems, pp. 1986–1994, 2013.

[31] J. Ho, Y. Xie, and B. Vemuri, “On a nonlinear generalization of sparse coding and dictionary learning,” in

Proceedings of The 30th International Conference on Machine Learning, pp. 1480–1488, 2013.

[32] Y. Nakatsukasa, T. Soma, and A. Uschmajew, “Finding a low-rank basis in a matrix subspace,” CoRR,

vol. abs/1503.08601, 2015.

[33] Q. Berthet and P. Rigollet, “Complexity theoretic lower bounds for sparse principal component detection,” in

Conference on Learning Theory, pp. 1046–1066, 2013.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

38

[34] B. Barak, J. Kelner, and D. Steurer, “Rounding sum-of-squares relaxations,” arXiv preprint arXiv:1312.6652,

2013.

[35] S. B. Hopkins, T. Schramm, J. Shi, and D. Steurer, “Speeding up sum-of-squares for tensor decomposition and

planted sparse vectors,” arXiv preprint arXiv:1512.02337, 2015.

[36] S. Arora, R. Ge, and A. Moitra, “New algorithms for learning incoherent and overcomplete dictionaries,” arXiv

[37] A. Agarwal, A. Anandkumar, and P. Netrapalli, “Exact recovery of sparsely used overcomplete dictionaries,”

preprint arXiv:1308.6273, 2013.

arXiv preprint arXiv:1309.1952, 2013.

[38] A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon, “Learning sparsely used overcomplete

dictionaries via alternating minimization,” arXiv preprint arXiv:1310.7991, 2013.

[39] S. Arora, A. Bhaskara, R. Ge, and T. Ma, “More algorithms for provable dictionary learning,” arXiv preprint

[40] S. Arora, R. Ge, T. Ma, and A. Moitra, “Simple, efﬁcient, and neural algorithms for sparse coding,” arXiv

arXiv:1401.0579, 2014.

preprint arXiv:1503.00778, 2015.

[41] K. G. Murty and S. N. Kabadi, “Some NP-complete problems in quadratic and nonlinear programming,”

Mathematical programming, vol. 39, no. 2, pp. 117–129, 1987.

[42] R. Vershynin, “Introduction to the non-asymptotic analysis of random matrices,” arXiv preprint arXiv:1011.3027,

2010.

May 2011.

[43] R. Basri and D. W. Jacobs, “Lambertian reﬂectance and linear subspaces,” Pattern Analysis and Machine

Intelligence, IEEE Transactions on, vol. 25, no. 2, pp. 218–233, 2003.

[44] E. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal component analysis?,” Journal of the ACM, vol. 58,

[45] V. De la Pena and E. Gin´e, Decoupling: from dependence to independence. Springer, 1999.
[46] M. Talagrand, Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems,

vol. 60. Springer Science & Business Media, 2014.

[47] K. Luh and V. Vu, “Dictionary learning with few samples and matrix concentration,” arXiv preprint

arXiv:1503.08854, 2015.

[48] P. Jain, P. Netrapalli, and S. Sanghavi, “Low-rank matrix completion using alternating minimization,” in
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pp. 665–674, ACM,
2013.

[49] M. Hardt, “On the provable convergence of alternating minimization for matrix completion,” arXiv preprint

arXiv:1312.0925, 2013.

[50] M. Hardt and M. Wootters, “Fast matrix completion without the condition number,” in Proceedings of The

27th Conference on Learning Theory, pp. 638–678, 2014.

[51] M. Hardt, “Understanding alternating minimization for matrix completion,” in Foundations of Computer Science

(FOCS), 2014 IEEE 55th Annual Symposium on, pp. 651–660, IEEE, 2014.

[52] P. Jain and P. Netrapalli, “Fast exact matrix completion with ﬁnite samples,” arXiv preprint arXiv:1411.1087,

2014.

[53] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain, “Non-convex robust pca,” in Advances in

Neural Information Processing Systems, pp. 1107–1115, 2014.

[54] Q. Zheng and J. Lafferty, “A convergent gradient descent algorithm for rank minimization and semideﬁnite

programming from random linear measurements,” arXiv preprint arXiv:1506.06081, 2015.

[55] S. Tu, R. Boczar, M. Soltanolkotabi, and B. Recht, “Low-rank solutions of linear matrix equations via procrustes

ﬂow,” arXiv preprint arXiv:1507.03566, 2015.

[56] Y. Chen and M. J. Wainwright, “Fast low-rank estimation by projected gradient descent: General statistical

and algorithmic guarantees,” arXiv preprint arXiv:1509.03025, 2015.

[57] P. Jain and S. Oh, “Provable tensor factorization with missing data,” in Advances in Neural Information

Processing Systems, pp. 1431–1439, 2014.

[58] A. Anandkumar, R. Ge, and M. Janzamin, “Guaranteed non-orthogonal tensor decomposition via alternating

rank-1 updates,” arXiv preprint arXiv:1402.5180, 2014.

[59] A. Anandkumar, R. Ge, and M. Janzamin, “Analyzing tensor power method dynamics: Applications to learning

overcomplete latent variable models,” arXiv preprint arXiv:1411.1488, 2014.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

39

[60] A. Anandkumar, P. Jain, Y. Shi, and U. Niranjan, “Tensor vs matrix methods: Robust tensor decomposition

under block sparse perturbations,” arXiv preprint arXiv:1510.04747, 2015.

[61] R. Ge, F. Huang, C. Jin, and Y. Yuan, “Escaping from saddle points—online stochastic gradient for tensor

decomposition,” in Proceedings of The 28th Conference on Learning Theory, pp. 797–842, 2015.

[62] P. Netrapalli, P. Jain, and S. Sanghavi, “Phase retrieval using alternating minimization,” in Advances in Neural

Information Processing Systems, pp. 2796–2804, 2013.

[63] E. J. Cand`es, X. Li, and M. Soltanolkotabi, “Phase retrieval via wirtinger ﬂow: Theory and algorithms,” arXiv

preprint arXiv:1407.1065, 2014.

[64] Y. Chen and E. J. Candes, “Solving random quadratic systems of equations is nearly as easy as solving linear

systems,” arXiv preprint arXiv:1505.05114, 2015.

[65] J. Sun, Q. Qu, and J. Wright, “A geometric analysis of phase retreival,” arXiv preprint arXiv:1602.06664,

[66] J. Sun, Q. Qu, and J. Wright, “When are nonconvex problems not scary?,” arXiv preprint arXiv:1510.06096,

[67] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive Sensing. Springer, 2013.
[68] T. Figiel, J. Lindenstrauss, and V. D. Milman, “The dimension of almost spherical sections of convex bodies,”

Acta Mathematica, vol. 139, no. 1, pp. 53–94, 1977.

[69] A. Y. Garnaev and E. D. Gluskin, “The widths of a euclidean ball,” in Dokl. Akad. Nauk SSSR, vol. 277,

pp. 1048–1052, 1984.

[70] E. Gluskin and V. Milman, “Note on the geometric-arithmetic mean inequality,” in Geometric aspects of

Functional analysis, pp. 131–135, Springer, 2003.

[71] G. Pisier, The volume of convex bodies and Banach space geometry, vol. 94. Cambridge University Press,

2016.

2015.

1999.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

1

Finding a sparse vector in a subspace: linear
sparsity using alternating directions
Qing Qu, Student Member, IEEE, Ju Sun, Student Member, IEEE, and John Wright, Member, IEEE

6
1
0
2
 
l
u
J
 
0
2
 
 
]
T
I
.
s
c
[
 
 
3
v
9
5
6
4
.
2
1
4
1
:
v
i
X
r
a

Abstract

Is it possible to ﬁnd the sparsest vector (direction) in a generic subspace S ⊆ Rp with dim (S) = n < p? This
problem can be considered a homogeneous variant of the sparse recovery problem, and ﬁnds connections to sparse
dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper,
we focus on a planted sparse model for the subspace: the target sparse vector is embedded in an otherwise random
subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of
nonzero entries in the target sparse vector substantially exceeds O(1/
n). In contrast, we exhibit a relatively simple
nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero
entries is Ω(1). To the best of our knowledge, this is the ﬁrst practical algorithm to achieve linear scaling under
the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g.,
sparse dictionary learning.

√

Sparse vector, Subspace modeling, Sparse recovery, Homogeneous recovery, Dictionary learning, Nonconvex

optimization, Alternating direction method

Index Terms

I. INTRODUCTION
Suppose that a linear subspace S embedded in Rp contains a sparse vector x0 (cid:54)= 0. Given an arbitrary basis of S,
can we efﬁciently recover x0 (up to scaling)? Equivalently, provided a matrix A ∈ R(p−n)×p with Null(A) = S, 1
can we efﬁciently ﬁnd a nonzero sparse vector x such that Ax = 0? In the language of sparse recovery, can we
solve

min
x

(cid:107)x(cid:107)0

s.t. Ax = 0, x (cid:54)= 0

?

In contrast to the standard sparse recovery problem (Ax = b, b (cid:54)= 0), for which convex relaxations perform nearly
optimally for broad classes of designs A [2, 3], the computational properties of problem (I.1) are not nearly as well
understood. It has been known for several decades that the basic formulation

(I.1)

(I.2)

min
x

(cid:107)x(cid:107)0 ,

s.t. x ∈ S \ {0},

is NP-hard for an arbitrary subspace [4, 5]. In this paper, we assume a speciﬁc random planted sparse model for
the subspace S: a target sparse vector is embedded in an otherwise random subspace. We will show that under the
speciﬁc random model, problem (I.2) is tractable by an efﬁcient algorithm based on nonconvex optimization.

A. Motivation

The general version of Problem (I.2), in which S can be an arbitrary subspace, takes several forms in numerical
computation and computer science, and underlies several important problems in modern signal processing and
machine learning. Below we provide a sample of these applications.
Sparse Null Space and Matrix Sparsiﬁcation: The sparse null space problem is ﬁnding the sparsest matrix N
whose columns span the null space of a given matrix A. The problem arises in the context of solving linear
equality problems in constrained optimization [5], null space methods for quadratic programming [6], and solving

This work was partially supported by grants ONR N00014-13-1-0492, NSF 1343282, NSF 1527809, and funding from the Moore and
Sloan Foundations. Q. Qu, J. Sun and J. Wright are all with the Electrical Engineering Department, Columbia University, New York, NY,
10027, USA (e-mail: {qq2105, js4038, jw2966}@columbia.edu). This paper is an extension of our previous conference version [1].

1 Null(A)

.
= {x ∈ Rp | Ax = 0} denotes the null space of A.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

2

underdetermined linear equations [7]. The matrix sparsiﬁcation problem is of similar ﬂavor, the task is ﬁnding the
sparsest matrix B which is equivalent to a given full rank matrix A under elementary column operations. Sparsity
helps simplify many fundamental matrix operations (see [8]), and the problem has applications in areas such as
machine learning [9] and in discovering cycle bases of graphs [10]. [11] discusses connections between the two
problems and also to other problems in complexity theory.

Sparse (Complete) Dictionary Learning: In dictionary learning, given a data matrix Y, one seeks an approximation
Y ≈ AX, such that A is a representation dictionary with certain desired structure and X collects the representation
coefﬁcients with maximal sparsity. Such compact representation naturally allows signal compression, and also
facilitates efﬁcient signal acquisition and classiﬁcation (see relevant discussion in [12]). When A is required to
be complete (i.e., square and invertible), by linear algebra, we have2 row(Y) = row(X) [13]. Then the problem
reduces to ﬁnding sparsest vectors (directions) in the known subspace row(Y), i.e. (I.2). Insights into this problem
have led to new theoretical developments on complete dictionary learning [13–15].

Sparse Principal Component Analysis (Sparse PCA): In geometric terms, Sparse PCA (see, e.g., [16–18] for
early developments and [19, 20] for discussion of recent results) concerns stable estimation of a linear subspace
spanned by a sparse basis, in the data-poor regime, i.e., when the available data are not numerous enough to allow
one to decouple the subspace estimation and sparsiﬁcation tasks. Formally, given a data matrix Z = U0X0 + E,3
where Z ∈ Rp×n collects column-wise n data points, U0 ∈ Rp×r is the sparse basis, and E is a noise matrix, one is
asked to estimate U0 (up to sign, scale, and permutation). Such a factorization ﬁnds applications in gene expression,
ﬁnancial data analysis and pattern recognition [24]. When the subspace is known (say by the PCA estimator with
enough data samples), the problem again reduces to instances of (I.2) and is already nontrivial4. The full geometric
sparse PCA can be treated as ﬁnding sparse vectors in a subspace that is subject to perturbation.

In addition, variants and generalizations of the problem (I.2) have also been studied in applications regarding
control and optimization [25], nonrigid structure from motion [26], spectral estimation and Prony’s problem [27],
outlier rejection in PCA [28], blind source separation [29], graphical model learning [30], and sparse coding on
manifolds [31]; see also [32] and the references therein.

B. Prior Arts

Despite these potential applications of problem (I.2), it is only very recently that efﬁcient computational surrogates
with nontrivial recovery guarantees have been discovered for some cases of practical interest. In the context of sparse
dictionary learning, Spielman et al. [13] introduced a convex relaxation which replaces the nonconvex problem (I.2)
with a sequence of linear programs:

(cid:96)1/(cid:96)∞ Relaxation:

min
x

(cid:107)x(cid:107)1 ,

s.t. x(i) = 1, x ∈ S, 1 ≤ i ≤ p.

(I.3)

They proved that when S is generated as a span of n random sparse vectors, with high probability (w.h.p.), the
relaxation recovers these vectors, provided the probability of an entry being nonzero is at most θ ∈ O (1/
n). In
the planted sparse model, in which S is formed as direct sum of a single sparse vector x0 and a “generic” subspace,
Hand and Demanet proved that (I.3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales
n) [14]. One might imagine improving these results by tightening the analyses. Unfortunately, the
as θ ∈ O (1/
results of [13, 14] are essentially sharp: when θ substantially exceeds Ω(1/
n), in both models the relaxation (I.3)
provably breaks down. Moreover, the most natural semideﬁnite programming (SDP) relaxation of (I.1),

√

√

√

min
X

(cid:107)X(cid:107)1 ,

s.t.

(cid:68)

A(cid:62)A, X

(cid:69)

= 0, trace[X] = 1, X (cid:23) 0.

(I.4)

also breaks down at exactly the same threshold of θ ∼ O(1/

√

n).5

2Here, row(·) denotes the row space.
3Variants of multiple-component formulations often add an additional orthonormality constraint on U0 but involve a different notation of

sparsity; see, e.g., [16, 21–23].

4[14] has also discussed this data-rich sparse PCA setting.
5This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (with b (cid:54)= 0), in which it is possible to

handle very large fractions of nonzeros (say, θ = Ω(1/ log n), or even θ = Ω(1)) using a very simple (cid:96)1 relaxation [2, 3]

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

3

TABLE I
COMPARISON OF EXISTING METHODS FOR RECOVERING A PLANTED SPARSE VECTOR IN A SUBSPACE

Method
(cid:96)1/(cid:96)∞ Relaxation [14]
SDP Relaxation
SOS Relaxation [34]
Spectral Method [35]
This work

Recovery Condition

θ ∈ O(1/
θ ∈ O(1/

√
√

n)
n)

p ≥ Ω(n2), θ ∈ O(1)
p ≥ Ω(n2poly log(n)), θ ∈ O(1)
p ≥ Ω(n4 log n), θ ∈ O(1)

Time Complexity6
O(n3p log(1/ε))
O (cid:0)p3.5 log (1/ε)(cid:1)
∼ O(p7 log(1/ε)) 7
O (np log(1/(cid:15)))
O(n5p2 log n + n3p log(1/ε))

√

One might naturally conjecture that this 1/

n threshold is simply an intrinsic price we must pay for having an
efﬁcient algorithm, even in these random models. Some evidence towards this conjecture might be borrowed from
the superﬁcial similarity of (I.2)-(I.4) and sparse PCA [16]. In sparse PCA, there is a substantial gap between what
can be achieved with currently available efﬁcient algorithms and the information theoretic optimum [19, 33]. Is
this also the case for recovering a sparse vector in a subspace? Is θ ∈ O (1/
n) simply the best we can do with
efﬁcient, guaranteed algorithms?

√

Remarkably, this is not the case. Recently, Barak et al. introduced a new rounding technique for sum-of-squares
relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p ≥ Ω (cid:0)n2(cid:1) and
θ = Ω(1) [34]. It is perhaps surprising that this is possible at all with a polynomial time algorithm. Unfortunately,
the runtime of this approach is a high-degree polynomial in p (see Table I); for machine learning problems in
which p is often either the feature dimension or the sample size, this algorithm is mostly of theoretical interest only.
However, it raises an interesting algorithmic question: Is there a practical algorithm that provably recovers a sparse
vector with θ (cid:29) 1/

n portion of nonzeros from a generic subspace S?

√

C. Contributions and Recent Developments

In this paper, we address the above problem under the planted sparse model. We allow x0 to have up to θ0p
nonzero entries, where θ0 ∈ (0, 1) is a constant. We provide a relatively simple algorithm which, w.h.p., exactly
recovers x0, provided that p ≥ Ω (cid:0)n4 log n(cid:1). A comparison of our results with existing methods is shown in Table
I. After initial submission of our paper, Hopkins et al. [35] proposed a different simple algorithm based on the
spectral method. This algorithm guarantees recovery of the planted sparse vector also up to linear sparsity, whenever
p ≥ Ω(n2polylog(n)), and comes with better time complexity.8

Our algorithm is based on alternating directions, with two special twists. First, we introduce a special data driven
initialization, which seems to be important for achieving θ = Ω(1). Second, our theoretical results require a second,
linear programming based rounding phase, which is similar to [13]. Our core algorithm has very simple iterations,
of linear complexity in the size of the data, and hence should be scalable to moderate-to-large scale problems.

Besides enjoying the θ ∼ Ω(1) guarantee that is out of the reach of previous practical algorithms, our algorithm
performs well in simulations – empirically succeeding with p ≥ Ω (n polylog(n)). It also performs well empirically
on more challenging data models, such as the complete dictionary learning model, in which the subspace of interest
contains not one, but n random target sparse vectors. This is encouraging, as breaking the O(1/
n) sparsity barrier
with a practical algorithm and optimal guarantee is an important problem in theoretical dictionary learning [36–40].
In this regard, our recent work [15] presents an efﬁcient algorithm based on Riemannian optimization that guarantees
recovery up to linear sparsity. However, the result is based on different ideas: a different nonconvex formulation,
optimization algorithm, and analysis methodology.

√

D. Paper Organization, Notations and Reproducible Research

The rest of the paper is organized as follows. In Section II, we provide a nonconvex formulation and show its
capability of recovering the sparse vector. Section III introduces the alternating direction algorithm. In Section IV,

6All estimates here are based on the standard interior point methods for solving linear and semideﬁnite programs. Customized solvers may

result in order-wise speedup for speciﬁc problems. ε is the desired numerical accuracy.

7Here our estimation is based on the degree-4 SOS hierarchy used in [34] to obtain an initial approximate recovery.
8Despite these improved guarantees in the planted sparse model, our method still produces more appealing results on real imagery data –

see Section V-B for examples.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

4

we present our main results and sketch the proof ideas. Experimental evaluation of our method is provided in
Section V. We conclude the paper by drawing connections to related work and discussing potential improvements
in Section VI. Full proofs are all deferred to the appendix sections.

For a matrix X, we use xi and xj to denote its i-th column and j-th row, respectively, all in column vector form.
.
Moreover, we use x(i) to denote the i-th component of a vector x. We use the compact notation [k]
= {1, . . . , k}
for any positive integer k, and use c or C, and their indexed versions to denote absolute numerical constants. The
scope of these constants are always local, namely within a particular lemma, proposition, or proof, such that the
apparently same constant in different contexts may carry different values. For probability events, sometimes we will
just say the event holds “with high probability” (w.h.p.) if the probability of failure is dominated by p−κ for some
κ > 0.

The codes to reproduce all the ﬁgures and experimental results can be found online at:

https://github.com/sunju/psv.

II. PROBLEM FORMULATION AND GLOBAL OPTIMALITY
We study the problem of recovering a sparse vector x0 (cid:54)= 0 (up to scale), which is an element of a known
subspace S ⊂ Rp of dimension n, provided an arbitrary orthonormal basis Y ∈ Rp×n for S. Our starting point is
the nonconvex formulation (I.2). Both the objective and the constraint set are nonconvex, and hence it is not easy to
optimize over. We relax (I.2) by replacing the (cid:96)0 norm with the (cid:96)1 norm. For the constraint x (cid:54)= 0, since in most
applications we only care about the solution up to scaling, it is natural to force x to live on the unit sphere Sn−1,
giving

min
x

(cid:107)x(cid:107)1 ,

s.t. x ∈ S, (cid:107)x(cid:107)2 = 1.

(II.1)

This formulation is still nonconvex, and for general nonconvex problems it is known to be NP-hard to ﬁnd even
a local minimizer [41]. Nevertheless, the geometry of the sphere is benign enough, such that for well-structured
inputs it actually will be possible to give algorithms that ﬁnd the global optimizer.

The formulation (II.1) can be contrasted with (I.3), in which effectively we optimize the (cid:96)1 norm subject to the
constraint (cid:107)x(cid:107)∞ = 1: because the set {x : (cid:107)x(cid:107)∞ = 1} is polyhedral, the (cid:96)∞-constrained problem immediately
yields a sequence of linear programs. This is very convenient for computation and analysis. However, it suffers
from the aforementioned breakdown behavior around (cid:107)x0(cid:107)0 ∼ p/
n. In contrast, though the sphere (cid:107)x(cid:107)2 = 1 is a
more complicated geometric constraint, it will allow much larger number of nonzeros in x0. Indeed, if we consider
the global optimizer of a reformulation of (II.1):

√

min
q∈Rn

(cid:107)Yq(cid:107)1 ,

s.t.

(cid:107)q(cid:107)2 = 1,

(II.2)

where Y is any orthonormal basis for S, the sufﬁcient condition that guarantees exact recovery under the planted
sparse model for the subspace is as follows:

Theorem II.1 ((cid:96)1/(cid:96)2 recovery, planted sparse model). There exists a constant θ0 > 0, such that if the subspace S
follows the planted sparse model

where gi ∼i.i.d. N (0, 1
n < θ < θ0, then the unique
(up to sign) optimizer q(cid:63) to (II.2), for any orthonormal basis Y of S, produces Yq(cid:63) = ξx0 for some ξ (cid:54)= 0 with
probability at least 1 − cp−2, provided p ≥ Cn. Here c and C are positive constants.

Ber(θ) are all jointly independent and 1/

p I), and x0 ∼i.i.d.

θp

S = span (x0, g1, . . . , gn−1) ⊂ Rp,
1√

√

Hence, if we could ﬁnd the global optimizer of (II.2), we would be able to recover x0 whose number of nonzero
entries is quite large – even linear in the dimension p (θ = Ω(1)). On the other hand, it is not obvious that this
should be possible: (II.2) is nonconvex. In the next section, we will describe a simple heuristic algorithm for
approximately solving a relaxed version of the (cid:96)1/(cid:96)2 problem (II.2). More surprisingly, we will then prove that
for a class of random problem instances, this algorithm, plus an auxiliary rounding technique, actually recovers
the global optimizer – the target sparse vector x0. The proof requires a detailed probabilistic analysis, which is
sketched in Section IV-B.

Before continuing, it is worth noting that the formulation (II.1) is in no way novel – see, e.g., the work of [29]

in blind source separation for precedent. However, our algorithms and subsequent analysis are novel.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

5

III. ALGORITHM BASED ON ALTERNATING DIRECTION METHOD (ADM)
To develop an algorithm for solving (II.2), it is useful to consider a slight relaxation of (II.2), in which we

introduce an auxiliary variable x ≈ Yq:

min
q,x

f (q, x)

.
=

1
2

(cid:107)Yq − x(cid:107)2

2 + λ (cid:107)x(cid:107)1 ,

s.t.

(cid:107)q(cid:107)2 = 1.

(III.1)

Here, λ > 0 is a penalty parameter. It is not difﬁcult to see that this problem is equivalent to minimizing the
Huber M-estimator over Yq. This relaxation makes it possible to apply the alternating direction method to this
problem. This method starts from some initial point q(0), alternates between optimizing with respect to (w.r.t.) x
and optimizing w.r.t. q:

where x(k) and q(k) denote the values of x and q in the k-th iteration. Both (III.2) and (III.3) have simple closed
form solutions:

x(k+1) = arg min

q(k+1) = arg min

1
2
1
2

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)Yq(k) − x
(cid:13)
(cid:13)Yq − x(k+1)(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
2

x

q

+ λ (cid:107)x(cid:107)1 ,

s.t. (cid:107)q(cid:107)2 = 1,

x(k+1) = Sλ[Yq(k)],

q(k+1) =

Y(cid:62)x(k+1)
(cid:13)Y(cid:62)x(k+1)(cid:13)
(cid:13)
(cid:13)2

,

(III.2)

(III.3)

(III.4)

where Sλ [x] = sign(x) max {|x| − λ, 0} is the soft-thresholding operator. The proposed ADM algorithm is
summarized in Algorithm 1.

Algorithm 1 Nonconvex ADM for solving (III.1)

A matrix Y ∈ Rp×n with Y(cid:62)Y = I, initialization q(0), threshold parameter λ > 0.

The recovered sparse vector ˆx0 = Yq(k)

Input:
Output:
1: for k = 0, . . . , O (cid:0)n4 log n(cid:1) do
x(k+1) = Sλ[Yq(k)],
2:
q(k+1) = Y(cid:62)x(k+1)
(cid:107)Y(cid:62)x(k+1)(cid:107)2

,

3:
4: end for

The algorithm is simple to state and easy to implement. However, if our goal is to recover the sparsest vector x0,

some additional tricks are needed.
Initialization. Because the problem (II.2) is nonconvex, an arbitrary or random initialization may not produce a
global minimizer.9 In fact, good initializations are critical for the proposed ADM algorithm to succeed in the linear
sparsity regime. For this purpose, we suggest using every normalized row of Y as initializations for q, and solving
a sequence of p nonconvex programs (II.2) by the ADM algorithm.

To get an intuition of why our initialization works, recall the planted sparse model S = span(x0, g1, . . . , gn−1)

and suppose

Y = [x0 | g1 | · · · | gn−1] ∈ Rp×n.

(III.5)
θp(cid:1). Meanwhile, the entries of
If we take a row yi of Y, in which x0(i) is nonzero, then x0(i) = Θ (cid:0)1/
√
p. Hence, when θ is not too large,
g1(i), . . . gn−1(i) are all N (0, 1/p), and so their magnitude have size about 1/
x0(i) will be somewhat bigger than most of the other entries in yi. Put another way, yi is biased towards the ﬁrst
Y ≈ I.10
standard basis vector e1. Now, under our probabilistic model assumptions, Y is very well conditioned: Y
Using the Gram-Schmidt process11, we can ﬁnd an orthonormal basis Y for S via:

√

(cid:62)

9More precisely, in our models, random initialization does work, but only when the subspace dimension n is extremely low compared to

the ambient dimension p.

10This is the common heuristic that “tall random matrices are well conditioned” [42].
11...QR decomposition in general with restriction that R11 = 1.

Y = YR,

(III.6)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

6

where R is upper triangular, and R is itself well-conditioned: R ≈ I. Since the i-th row yi of Y is biased in
the direction of e1 and R is well-conditioned, the i-th row yi of Y is also biased in the direction of e1. In other
words, with this canonical orthobasis Y for the subspace, the i-th row of Y is biased in the direction of the global
optimizer. The heuristic arguments are made rigorous in Appendix B and Appendix D.

What if we are handed some other basis (cid:98)Y = YU, where U is an arbitary orthogonal matrix? Suppose q(cid:63) is a
global optimizer to (II.2) with the input matrix Y, then it is easy to check that, U(cid:62)q(cid:63) is a global optimizer to
(II.2) with the input matrix (cid:98)Y. Because
(cid:68)

(cid:69)

(cid:68)

(cid:69)

(YU)(cid:62)ei, U(cid:62)q(cid:63)

=

Y(cid:62)ei, q(cid:63)

,

our initialization is invariant to any rotation of the orthobasis. Hence, even if we are handed an arbitrary orthobasis
for S, the i-th row is still biased in the direction of the global optimizer.

Rounding by linear programming (LP). Let q denote the output of Algorithm 1. As illustrated in Fig. 1, we
will prove that with our particular initialization and an appropriate choice of λ, ADM algorithm uniformly moves
towards the optimal over a large portion of the sphere, and its solution falls within a certain small radius of the
globally optimal solution q(cid:63) to (II.2). To exactly recover q(cid:63), or equivalently to recover the exact sparse vector
x0 = γYq(cid:63) for some γ (cid:54)= 0, we solve the linear program

min
q

(cid:107)Yq(cid:107)1

s.t.

(cid:104)r, q(cid:105) = 1

(III.7)

with r = q. Since the feasible set {q | (cid:104)q, q(cid:105) = 1} is essentially the tangent space of the sphere Sn−1 at q, whenever
q is close enough to q(cid:63), one should expect that the optimizer of (III.7) exactly recovers q(cid:63) and hence x0 up to
scale. We will prove that this is indeed true under appropriate conditions.

IV. MAIN RESULTS AND SKETCH OF ANALYSIS

A. Main Results

previous section succeeds.

In this section, we describe our main theoretical result, which shows that w.h.p. the algorithm described in the

Theorem IV.1. Suppose that S obeys the planted sparse model, and let the columns of Y form an arbitrary
orthonormal basis for the subspace S. Let y1, . . . , yp ∈ Rn denote the (transposes of) the rows of Y. Apply
(cid:13)2 , . . . , yp/ (cid:107)yp(cid:107)2, to produce outputs q1, . . . , qp.
Algorithm 1 with λ = 1/
Solve the linear program (III.7) with r = q1, . . . , qp, to produce (cid:98)q1, . . . , (cid:98)qp. Set i(cid:63) ∈ arg mini (cid:107)Y(cid:98)qi(cid:107)1. Then

p, using initializations q(0) = y1/ (cid:13)

(cid:13)y1(cid:13)

√

for some γ (cid:54)= 0 with probability at least 1 − cp−2, provided

Y(cid:98)qi(cid:63) = γx0,

p ≥ Cn4 log n,

and

1
√
n

≤ θ ≤ θ0.

Here C, c and θ0 are positive constants.

(IV.1)

(IV.2)

Remark IV.2. We can see that the result in Theorem IV.1 is suboptimal in sample complexity compared to the
global optimality result in Theorem II.1 and Barak et al.’s result [34] (and the subsequent work [35]). For successful
recovery, we require p ≥ Ω (cid:0)n4 log n(cid:1), while the global optimality and Barak et al. demand p ≥ Ω (n) and
p ≥ Ω (cid:0)n2(cid:1), respectively. Aside from possible deﬁciencies in our current analysis, compared to Barak et al., we
believe this is still the ﬁrst practical and efﬁcient method which is guaranteed to achieve θ ∼ Ω(1) rate. The lower
bound on θ in Theorem IV.1 is mostly for convenience in the proof; in fact, the LP rounding stage of our algorithm
already succeeds w.h.p. when θ ∈ O (1/

n).

√

B. A Sketch of Analysis

In this section, we brieﬂy sketch the main ideas of proving our main result in Theorem IV.1, to show that the
“initialization + ADM + LP rounding” pipeline recovers x0 under the stated technical conditions, as illustrated in

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

7

Fig. 1. An illustration of the proof sketch for our ADM algorithm.

Fig. 1. The proof of our main result requires rather detailed technical analysis of the iteration-by-iteration properties
of Algorithm 1, most of which is deferred to the appendices.

As noted in Section III, the ADM algorithm is invariant to change of basis. So w.l.o.g., let us assume Y =

[x0 | g1 | · · · | gn−1] and let Y to be its orthogonalization, i.e., 12

Y =

(cid:20) x0
(cid:107)x0(cid:107)2

(cid:16)

| Px⊥

0

G

G(cid:62)Px⊥

0

G

(cid:17)−1/2(cid:21)

.

When p is large, Y is nearly orthogonal, and hence Y is very close to Y. Thus, in our proofs, whenever convenient,
we make the arguments on Y ﬁrst and then “propagate” the quantitative results onto Y by perturbation arguments.
With that noted, let y1, · · · , yp be the transpose of the rows of Y, and note that these are all independent random
vectors. To prove the result of Theorem IV.1, we need the following results. First, given the speciﬁed Y, we show
that our initialization is biased towards the global optimum:

Proposition IV.3 (Good initialization). Suppose θ > 1/
that at least one of our p initialization vectors suggested in Section III, say q(0)

n and p ≥ Cn. It holds with probability at least 1 − cp−2
i = yi/ (cid:13)

(cid:13)yi(cid:13)

(cid:13)2, obeys

√

Here C, c are positive constants.

Proof: See Appendix D.

Second, we deﬁne a vector-valued random process Q(q) on q ∈ Sn−1, via

so that based on (III.4), one step of the ADM algorithm takes the form:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28) yi

(cid:107)yi(cid:107)2

, e1

≥

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√

.

10

θn

Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

,

1
p

p
(cid:88)

i=1

q(k+1) =

Q (cid:0)q(k)(cid:1)
(cid:13)Q (cid:0)q(k)(cid:1)(cid:13)
(cid:13)
(cid:13)2

(IV.3)

(IV.4)

(IV.5)

(IV.6)

This is a very favorable form for analysis: the term in the numerator Q (cid:0)q(k)(cid:1) is a sum of p independent random
vectors with q(k) viewed as ﬁxed. We study the behavior of the iteration (IV.6) through the random process Q (cid:0)q(k)(cid:1).

12Note that with probability one, the inverse matrix square-root in Y is well deﬁned. So Y is well deﬁned w.h.p. (i.e., except for x0 = 0).

See more quantitative characterization of Y in Appendix B.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

8

We want to show that w.h.p. the ADM iterate sequence q(k) converges to some small neighborhood of ±e1, so
that the ADM algorithm plus the LP rounding (described in Section III) successfully retrieves the sparse vector
x0/(cid:107)x0(cid:107) = Ye1. Thus, we hope that in general, Q(q) is more concentrated on the ﬁrst coordinate than q ∈ Sn−1. Let
us partition the vector q as q = [q1; q2], with q1 ∈ R and q2 ∈ Rn−1; and correspondingly Q(q) = [Q1(q); Q2(q)].
The inner product of Q(q)/ (cid:107)Q(q)(cid:107)2 and e1 is strictly larger than the inner product of q and e1 if and only if

|Q1(q)|
|q1|

>

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

.

In the following proposition, we show that w.h.p., this inequality holds uniformly over a signiﬁcant portion of the
sphere

(cid:26)

.
=

Γ

q ∈ Sn−1 |

1
√

10

nθ

√

≤ |q1| ≤ 3

θ, (cid:107)q2(cid:107)2 ≥

(cid:27)

,

1
10

so the algorithm moves in the correct direction. Let us deﬁne the gap G(q) between the two quantities |Q1(q)| / |q1|
and (cid:107)Q2(q)(cid:107)2 / (cid:107)q2(cid:107)2 as

(IV.7)

(IV.8)

and we show that the following result is true:

G(q)

.
=

|Q1(q)|
|q1|

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q2(cid:107)2

,

Proposition IV.4 (Uniform lower bound for ﬁnite sample gap). There exists a constant θ0 ∈ (0, 1), such that when
p ≥ Cn4 log n, the estimate

G(q) ≥

inf
q∈Γ

1
104θ2np

√

holds with probability at least 1 − cp−2, provided θ ∈ (1/

n, θ0). Here C, c are positive constants.

Proof: See Appendix E.

√

Next, we show that whenever |q1| ≥ 3
enough for LP rounding (III.7) to succeed.

θ, w.h.p. the iterates stay in a “safe region” with |q1| ≥ 2

θ which is

√

Proposition IV.5 (Safe region for rounding). There exists a constant θ0 ∈ (0, 1), such that when p ≥ Cn4 log n, it
holds with probability at least 1 − cp−2 that

|Q1(q)|
(cid:107)Q(q)(cid:107)2

√
θ

≥ 2

√

√

for all q ∈ Sn−1 satisfying |q1| > 3

θ, provided θ ∈ (1/

n, θ0). Here C, c are positive constants.

Proof: See Appendix F.

In addition, the following result shows that the number of iterations for the ADM algorithm to reach the safe

region can be bounded grossly by O(n4 log n) w.h.p..

Proposition IV.6 (Iteration complexity of reaching the safe region). There is a constant θ0 ∈ (0, 1), such that
when p ≥ Cn4 log n, it holds with probability at least 1 − cp−2 that the ADM algorithm in Algorithm 1, with any
(cid:12)
initialization q(0) ∈ Sn−1 satisfying
(cid:12) ≥ 1
(cid:12)
θ at least once in
√
θn
10
√
at most O(n4 log n) iterations, provided θ ∈ (1/

, will produce some iterate q with |¯q1| > 3
n, θ0). Here C, c are positive constants.

(cid:12)
(cid:12)q(0)
(cid:12)

√

1

Proof: See Appendix G.

Moreover, we show that the LP rounding (III.7) with input r = q exactly recovers the optimal solution w.h.p.,

√

whenever the ADM algorithm returns a solution q with ﬁrst coordinate |q1| > 2

θ.

Proposition IV.7 (Success of rounding). There is a constant θ0 ∈ (0, 1), such that when p ≥ Cn, the following
holds with probability at least 1 − cp−2 provided θ ∈ (1/
n, θ0): Suppose the input basis is Y deﬁned in (IV.3)
and the ADM algorithm produces an output q ∈ Sn−1 with |q1| > 2
θ. Then the rounding procedure with r = q
returns the desired solution ±e1. Here C, c are positive constants.

√

√

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

9

Finally, given p ≥ Cn4 log n for a sufﬁciently large constant C, we combine all the results above to complete the

Proof: See Appendix H.

proof of Theorem IV.1.

Proof of Theorem IV.1:

W.l.o.g., let us again ﬁrst consider Y as deﬁned in (III.5) and its orthogonalization Y in a “natural/canonical”
form (IV.3). We show that w.h.p. our algorithmic pipeline described in Section III exactly recovers the optimal
solution up to scale, via the following argument:

1) Good initializers. Proposition IV.3 shows that w.h.p., at least one of the p initialization vectors, say q(0)

i =

yi/ (cid:13)

(cid:13)yi(cid:13)

(cid:13)2, obeys

(cid:12)
(cid:68)
(cid:12)
(cid:12)

q(0)
i

, e1

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥

1
√

,

10

θn

which implies that q(0)

i

is biased towards the global optimal solution.

√

2) Uniform progress away from the equator. By Proposition IV.4, for any θ ∈ (1/

n, θ0) with a constant

θ0 ∈ (0, 1),

w.h.p.,

G(q) =

|Q1(q)|
|q1|
holds uniformly for all q ∈ Sn−1 in the region
(cid:12)
(cid:12) ≥ 1
(cid:12)
q(0) such that
if sufﬁciently many iterations are allowed.

(cid:12)
(cid:12)q(0)
(cid:12)

θn

10

√

1

1
√

10

θn

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2

1
104θ2np

≥

√

, the ADM algorithm will eventually obtain a point q(k) for which (cid:12)

(cid:12)q(k)(cid:12)

(cid:12) ≥ 3

√

θ,

≤ |q1| ≤ 3

θ w.h.p.. This implies that with an input

(IV.9)

3) No jumps away from the caps. Proposition IV.5 shows that for any θ ∈ (1/

n, θ0) with a constant θ0 ∈ (0, 1),

Q1(q)
(cid:107)Q(q)(cid:107)2

√

≥ 2

θ

√

√

√

θ. This implies that once |q(k)
holds for all q ∈ Sn−1 with |q1| ≥ 3
θ for some iterate k, all the future
iterates produced by the ADM algorithm stay in a “spherical cap” region around the optimum with |q1| ≥ 2
θ.
4) Location of stopping points. As shown in Proposition IV.6, w.h.p., the strictly positive gap G(q) in (IV.9)
ensures that one needs to run at most O (cid:0)n4 log n(cid:1) iterations to ﬁrst encounter an iterate q(k) such that
|q(k)
θ. Hence, the steps above imply that, w.h.p., Algorithm 1 fed with the proposed initialization
θ after O (cid:0)n4 log n(cid:1) steps.
scheme successively produces iterates q ∈ Sn−1 with its ﬁrst coordinate |q1| ≥ 2
θ. Proposition IV.7 proves that w.h.p., the LP rounding (III.7) with an

5) Rounding succeeds when |r1| ≥ 2

1 | ≥ 3

1 | ≥ 3

√

√

√

√

input r = q produces the solution ±x0 up to scale.

Taken together, these claims imply that from at least one of the initializers q(0), the ADM algorithm will produce
an output q which is accurate enough for LP rounding to exactly return x0/(cid:107)x0(cid:107)2. On the other hand, our (cid:96)1/(cid:96)2
optimality theorem (Theorem II.1) implies that ±x0 are the unique vectors with the smallest (cid:96)1 norm among all
unit vectors in the subspace. Since w.h.p. x0/(cid:107)x0(cid:107)2 is among the p unit vectors (cid:98)q1, . . . , (cid:98)qp our p row initializers
ﬁnally produce, our minimal (cid:96)1 norm selector will successfully locate x0/(cid:107)x0(cid:107)2 vector.

For the general case when the input is an arbitrary orthonormal basis (cid:98)Y = YU for some orthogonal matrix U,

the target solution is U(cid:62)e1. The following technical pieces are perfectly parallel to the argument above for Y.

1) Discussion at the end of Appendix D implies that w.h.p., at least one row of (cid:98)Y provides an initial point q(0)

such that (cid:12)
(cid:12)

(cid:10)q(0), U(cid:62)e1

(cid:11)(cid:12)
(cid:12) ≥ 1
√
10

.

θn

2) Discussion following Proposition IV.4 in Appendix E indicates that for all q such that

≤ (cid:12)
(cid:10)q, U(cid:62)e1
(cid:12)
θ, there is a strictly positive gap, indicating steady progress towards a point q(k) such that (cid:12)
(cid:10)q(k), U(cid:62)e1
(cid:12)
θ.

√
3
√
3

1
√

θn

10

3) Discussion at the end of Appendix F implies that once q satisﬁes (cid:12)
(cid:12)

(cid:10)q, U(cid:62)e1

(cid:11)(cid:12)
(cid:12), the next iterate will not move

(cid:11)(cid:12)
(cid:12) ≤
(cid:11)(cid:12)
(cid:12) ≥

far away from the target:

(cid:68)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

Q

q; (cid:98)Y

(cid:16)

(cid:17)

(cid:13)
(cid:13)
(cid:13)Q

/

q; (cid:98)Y

(cid:17)(cid:13)
(cid:13)
(cid:13)2

, U(cid:62)e1

(cid:69)(cid:12)
(cid:12)
(cid:12) ≥ 2

√

θ.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

10

4) Repeating the argument in Appendix G for general input (cid:98)Y shows it is enough to run the ADM algorithm
≤ (cid:12)
O (cid:0)n4 log n(cid:1) iterations to cross the range
θ. So the argument above together
(cid:12)
dictates that with the proposed initialization, w.h.p., the ADM algorithm produces an output q that satisﬁes
(cid:12)
(cid:10)q, U(cid:62)e1
(cid:12)

√
5) Since the ADM returns q satisfying (cid:12)
(cid:12)

θ, if we run at least O (cid:0)n4 log n(cid:1) iterations.
(cid:10)q, R(cid:62)e1

θ, discussion at the end of Appendix H implies that we
will obtain a solution q(cid:63) = ±U(cid:62)e1 up to scale as the optimizer of the rounding program, exactly the target
solution.

(cid:10)q, U(cid:62)e1

(cid:11)(cid:12)
(cid:12) ≤ 3

(cid:11)(cid:12)
(cid:12) ≥ 2

(cid:11)(cid:12)
(cid:12) ≥ 2

√

√

1
√

θn

10

Hence, we complete the proof.

Remark IV.8. Under the planted sparse model, in practice the ADM algorithm with the proposed initialization
converges to a global optimizer of (III.1) that correctly recovers x0. In fact, simple calculation shows such desired
point for successful recovery is indeed the only critical point of (III.1) near the pole in Fig. 1. Unfortunately,
using the current analytical framework, we did not succeed in proving such convergence in theory. Proposition IV.5
and IV.6 imply that after O(n4 log n) iterations, however, the ADM sequence will stay in a small neighborhood of
the target. Hence, we proposed to stop after O(n4 log n) steps, and then round the output using the LP that provable
recover the target, as implied by Proposition IV.5 and IV.7. So the LP rounding procedure is for the purpose of
completing the theory, and seems not necessary in practice. We suspect alternative analytical strategies, such as the
geometrical analysis that we will discuss in Section VI, can likely get around the artifact.

V. EXPERIMENTAL RESULTS

In this section, we show the performance of the proposed ADM algorithm on both synthetic and real datasets. On
the synthetic dataset, we show the phase transition of our algorithm on both the planted sparse and the dictionary
learning models; for the real dataset, we demonstrate how seeking sparse vectors can help discover interesting
patterns on face images.

A. Phase Transition on Synthetic Data

For the planted sparse model, for each pair of (k, p), we generate the n dimensional subspace S ⊂ Rp by direct
sum of x0 and G: x0 ∈ Rp is a k-sparse vector with uniformly random support and all nonzero entries equal to 1,
and G ∈ Rp×(n−1) is an i.i.d. Gaussian matrix distributed by N (0, 1/p). So one basis Y of the subspace S can
be constructed by Y = GS ([x0, G]) U, where GS (·) denotes the Gram-Schmidt orthonormalization operator and
U ∈ Rn×n is an arbitrary orthogonal matrix. For each p, we set the regularization parameter in (III.1) as λ = 1/
p,
use all the normalized rows of Y as initializations of q for the proposed ADM algorithm, and run the alternating
steps for 104 iterations. We determine the recovery to be successful whenever (cid:107)x0/ (cid:107)x0(cid:107)2 − Yq(cid:107)2 ≤ 10−2 for at
least one of the p trials (we set the tolerance relatively large as we have shown that LP rounding exactly recovers
the solutions with approximate input). To determine the empirical recovery performance of our ADM algorithm,
ﬁrst we ﬁx the relationship between n and p as p = 5n log n, and plot out the phase transition between k and p.
Next, we ﬁx the sparsity level θ = 0.2 (or k = 0.2p), and plot out the phase transition between p and n. For each
pair of (p, k) or (n, p), we repeat the simulation for 10 times. Fig. 2 shows both phase transition plots.

√

We also experiment with the complete dictionary learning model as in [13] (see also [15]). Speciﬁcally, the
observation is assumed to be Y = A0X0, where A0 is a square, invertible matrix, and X0 a n × p sparse matrix.
Since A0 is invertible, the row space of Y is the same as that of X0. For each pair of (k, n), we generate
X0 = [x1, · · · , xn](cid:62), where each vector xi ∈ Rp is k-sparse with every nonzero entry following i.i.d. Gaussian
distribution, and construct the observation by Y(cid:62) = GS (cid:0)X(cid:62)
(cid:1) U(cid:62). We repeat the same experiment as for the planted
0
sparse model described above. The only difference is that here we determine the recovery to be successful as long
as one sparse row of X0 is recovered by one of those p programs. Fig. 3 shows both phase transition plots.

Fig. 2(a) and Fig. 3(a) suggest our ADM algorithm could work into the linear sparsity regime for both models,
provided p ≥ Ω(n log n). Moreover, for both models, the log n factor seems necessary for working into the linear
sparsity regime, as suggested by Fig. 2(b) and Fig. 3(b): there are clear nonlinear transition boundaries between
success and failure regions. For both models, O(n log n) sample requirement is near optimal: for the planted sparse
model, obviously p ≥ Ω(n) is necessary; for the complete dictionary learning model, [13] proved that p ≥ Ω(n log n)
is required for exact recovery. For the planted sparse model, our result p ≥ Ω(n4 log n) is far from this much

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

11

Fig. 2. Phase transition for the planted sparse model using the ADM algorithm: (a) with ﬁxed relationship between p and n: p = 5n log n;
(b) with ﬁxed relationship between p and k: k = 0.2p. White indicates success and black indicates failure.

Fig. 3. Phase transition for the dictionary learning model using the ADM algorithm: (a) with ﬁxed relationship between p and n: p = 5n log n;
(b) with ﬁxed relationship between p and k: k = 0.2p. White indicates success and black indicates failure.

lower empirical requirement. Fig 2(b) further suggests that alternative reformulation and algorithm are needed to
solve (II.1) so that the optimal recovery guarantee as depicted in Theorem II.1 can be obtained.

B. Exploratory Experiments on Faces

It is well known in computer vision that the collection of images of a convex object only subject to illumination
changes can be well approximated by a low-dimensional subspaces in raw-pixel space [43]. We will play with face
subspaces here. First, we extract face images of one person (65 images) under different illumination conditions. Then
we apply robust principal component analysis [44] to the data and get a low dimensional subspace of dimension 10,
i.e., the basis Y ∈ R32256×10. We apply the ADM + LP algorithm to ﬁnd the sparsest elements in such a subspace,
by randomly selecting 10% rows of Y as initializations for q. We judge the sparsity in the (cid:96)1/(cid:96)2 sense, that is,
the sparsest vector (cid:98)x0 = Yq(cid:63) should produce the smallest (cid:107)Yq(cid:107)1 / (cid:107)Yq(cid:107)2 among all results. Once some sparse
vectors are found, we project the subspace onto orthogonal complement of the sparse vectors already found13, and
continue the seeking process in the projected subspace. Fig. 4(Top) shows the ﬁrst four sparse vectors we get from
the data. We can see they correspond well to different extreme illumination conditions. We also implemented the
spectral method (with the LP post-processing) proposed in [35] for comparison under the same protocol. The result
is presented as Fig. 4(Bottom): the ratios (cid:107)·(cid:107)(cid:96)1 / (cid:107)·(cid:107)(cid:96)2 are signiﬁcantly higher, and the ratios (cid:107)·(cid:107)(cid:96)4 / (cid:107)·(cid:107)(cid:96)2 (this is

13The idea is to build a sparse, orthonormal basis for the subspace in a greedy manner.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

12

the metric to be maximized in [35] to promote sparsity) are signiﬁcantly lower. By these two criteria the spectral
method with LP rounding consistently produces vectors with higher sparsity levels under our evaluation protocol.
Moreover, the resulting images are harder to interpret physically.

Fig. 4. The ﬁrst four sparse vectors extracted for one person in the Yale B database under different illuminations. (Top) by our ADM
algorithm; (Bottom) by the speeding-up SOS algorithm proposed in [35].

Second, we manually select ten different persons’ faces under the normal lighting condition. Again, the dimension
of the subspace is 10 and Y ∈ R32256×10. We repeat the same experiment as stated above. Fig. 5 shows four sparse
vectors we get from the data. Interestingly, the sparse vectors roughly correspond to differences of face images
concentrated around facial parts that different people tend to differ from each other, e.g., eye brows, forehead hair,
nose, etc. By comparison, the vectors returned by the spectral method [35] are relatively denser and the sparsity
patterns in the images are less structured physically.

In sum, our algorithm seems to ﬁnd useful sparse vectors for potential applications, such as peculiarity discovery
in ﬁrst setting, and locating differences in second setting. Nevertheless, the main goal of this experiment is to
invite readers to think about similar pattern discovery problems that might be cast as the problem of seeking sparse
vectors in a subspace. The experiment also demonstrates in a concrete way the practicality of our algorithm, both in
handling data sets of realistic size and in producing meaningful results even beyond the (idealized) planted sparse
model that we adopted for analysis.

VI. CONNECTIONS AND DISCUSSION

For the planted sparse model, there is a substantial performance gap in terms of p-n relationship between the our
optimality theorem (Theorem II.1), empirical simulations, and guarantees we have obtained via efﬁcient algorithm
(Theorem IV.1). More careful and tighter analysis based on decoupling [45] and chaining [46, 47] and geometrical
analysis described below can probably help bridge the gap between our theoretical and empirical results. Matching
the theoretical limit depicted in Theorem II.1 seems to require novel algorithmic ideas. The random models we
assume for the subspace can be extended to other random models, particularly for dictionary learning where all the
bases are sparse (e.g., Bernoulli-Gaussian random model).

This work is part of a recent surge of research efforts on deriving provable and practical nonconvex algorithms
to central problems in modern signal processing and machine learning. These problems include low-rank matrix
recovery/completion [48–56], tensor recovery/decomposition [57–61], phase retrieval [62–65], dictionary learning

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

13

Fig. 5. The ﬁrst four sparse vectors extracted for 10 persons in the Yale B database under normal illuminations. (Top) by our ADM
algorithm; (Bottom) by the speeding-up SOS algorithm proposed in [35].

[15, 36–40], and so on.14 Our approach, like the others, is to start with a carefully chosen, problem-speciﬁc
initialization, and then perform a local analysis of the subsequent iterates to guarantee convergence to a good solution.
In comparison, our subsequent work on complete dictionary learning [15] and generalized phase retrieval [65] has
taken a geometrical approach by characterizing the function landscape and designing efﬁcient algorithm accordingly.
The geometric approach has allowed provable recovery via efﬁcient algorithms, with an arbitrary initialization. The
article [66] summarizes the geometric approach and its applicability to several other problems of interest.

A hybrid of the initialization and the geometric approach discussed above is likely to be a powerful computational
framework. To see it in action for the current planted sparse vector problem, in Fig. 6 we provide the asymptotic
function landscape (i.e., p → ∞) of the Huber loss on the sphere S2 (aka the relaxed formulation we tried to
solve (III.1)). It is clear that with an initialization that is biased towards either the north or the south pole, we are
situated in a region where the gradients are always nonzero and points to the favorable directions such that many
reasonable optimization algorithms can take the gradient information and make steady progress towards the target.
This will probably ease the algorithm development and analysis, and help yield tight performance guarantees.

We provide a very efﬁcient algorithm for ﬁnding a sparse vector in a subspace, with strong guarantee. Our
algorithm is practical for handling large datasets—in the experiment on the face dataset, we successfully extracted
some meaningful features from the human face images. However, the potential of seeking sparse/structured element
in a subspace seems largely unexplored, despite the cases we mentioned at the start. We hope this work could
inspire more application ideas.

ACKNOWLEDGEMENT

JS thanks the Wei Family Private Foundation for their generous support. We thank Cun Mu, IEOR Department of
Columbia University, for helpful discussion and input regarding this work. We thank the anonymous reviewers for
their constructive comments that helped improve the manuscript. This work was partially supported by grants ONR
N00014-13-1-0492, NSF 1343282, NSF 1527809, and funding from the Moore and Sloan Foundations.

14The webpage http://sunju.org/research/nonconvex/ maintained by the second author contains pointers to the growing list of work in this

direction.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

14

Fig. 6. Function landscape of f (q) with θ = 0.4 for n = 3. (Left) f (q) over the sphere S2. Note that near the spherical caps around
the north and south poles, there are no critical points and the gradients are always nonzero; (Right) Projected function landscape by
3 (cid:55)→ R obtained via the reparameterization
projecting the upper hemisphere onto the equatorial plane. Mathematically the function g(w) : e⊥
q(w) = [w; (cid:112)1 − (cid:107)w(cid:107)2]. Corresponding to the left, there is no undesired critical point around 0 within a large radius.

APPENDIX A
TECHNICAL TOOLS AND PRELIMINARIES

In this appendix, we record several lemmas that are useful for our analysis.

Lemma A.1. Let ψ(x) and Ψ(x) to denote the probability density function (pdf) and the cumulative distribution
function (cdf) for the standard normal distribution:

(Standard Normal pdf) ψ(x) =

1
√
2π
1
√
2π
Suppose a random variable X ∼ N (0, σ2), with the pdf fσ(x) = 1

(Standard Normal cdf) Ψ(x) =

(cid:90) x

−∞

σ ψ (cid:0) x

σ

(cid:26)

exp

−

(cid:27)

x2
2
(cid:26)

exp

−

dt,

(cid:27)

t2
2

(cid:1), then for any t2 > t1 we have

(cid:90) t2

t1
(cid:90) t2

t1
(cid:90) t2

t1

fσ(x)dx = Ψ

(cid:19)

(cid:18) t2
σ
(cid:20)

(cid:20)

− Ψ

(cid:19)

(cid:19)

(cid:18) t2
σ
(cid:18) t2
σ

(cid:19)

,

(cid:18) t1
σ

(cid:19)(cid:21)

,

(cid:19)(cid:21)

(cid:18) t1
σ
(cid:18) t1
σ

xfσ(x)dx = −σ

ψ

− ψ

x2fσ(x)dx = σ2

Ψ

− Ψ

− σ

(cid:20)
t2ψ

(cid:19)

(cid:18) t2
σ

− t1ψ

(cid:19)(cid:21)

.

(cid:18) t1
σ

Lemma A.2 (Taylor Expansion of Standard Gaussian cdf and pdf ). Assume ψ(x) and Ψ(x) be deﬁned as above.
There exists some universal constant Cψ > 0 such that for any x0, x ∈ R,

|ψ(x) − [ψ(x0) − x0ψ (x0) (x − x0)]| ≤ Cψ(x − x0)2,
|Ψ(x) − [Ψ(x0) + ψ(x0)(x − x0)]| ≤ Cψ(x − x0)2.

Lemma A.3 (Matrix Induced Norms). For any matrix A ∈ Rp×n, the induced matrix norm from (cid:96)p → (cid:96)q is deﬁned
as

(cid:107)A(cid:107)(cid:96)p→(cid:96)q

.
= sup

(cid:107)x(cid:107)p=1

(cid:107)Ax(cid:107)q .

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

15

In particular, let A = [a1, · · · , an] = (cid:2)a1, · · · , ap(cid:3)(cid:62) , we have

(cid:107)A(cid:107)(cid:96)2→(cid:96)1 = sup
(cid:107)x(cid:107)2=1

p
(cid:88)

k=1

(cid:12)
(cid:12)a(cid:62)
(cid:12)
k x

(cid:12)
(cid:12)
(cid:12) ,

(cid:107)AB(cid:107)(cid:96)p→(cid:96)r ≤ (cid:107)A(cid:107)(cid:96)q→(cid:96)r (cid:107)B(cid:107)(cid:96)p→(cid:96)q ,

(cid:107)A(cid:107)(cid:96)2→(cid:96)∞ = max
1≤k≤p

(cid:13)
(cid:13)

(cid:13)ak(cid:13)
(cid:13)
(cid:13)2

,

and B is any matrix of size compatible with A.
Lemma A.4 (Moments of the Gaussian Random Variable). If X ∼ N (cid:0)0, σ2
X
that

(cid:1), then it holds for all integer m ≥ 1

E [|X|m] = σm

X (m − 1)!!

1m=2k+1 + 1m=2k

≤ σm

X (m − 1)!!, k = (cid:98)m/2(cid:99).

(cid:34)(cid:114) 2
π

(cid:35)

Lemma A.5 (Moments of the χ Random Variable). If X ∼ χ (n), i.e., X = (cid:107)x(cid:107)2 for x ∼ N (0, I), then it holds
for all integer m ≥ 1 that

E [X m] = 2m/2 Γ (m/2 + n/2)

≤ m!! nm/2.

Γ (n/2)

Lemma A.6 (Moments of the χ2 Random Variable). If X ∼ χ2 (n), i.e., X = (cid:107)x(cid:107)2
for all integer m ≥ 1 that

2 for x ∼ N (0, I), then it holds

E [X m] = 2m Γ (m + n/2)

=

Γ (n/2)

(n + 2k − 2) ≤

(2n)m.

m!
2

m
(cid:89)

k=1

Lemma A.7 (Moment-Control Bernstein’s Inequality for Random Variables [67]). Let X1, . . . , Xp be i.i.d. real-valued
random variables. Suppose that there exist some positive numbers R and σ2

X such that

m!
2

m!
2

Let S

.
= 1
p

(cid:80)p

k=1 Xk, then for all t > 0, it holds that

E [|Xk|m] ≤

X Rm−2,
σ2

for all integers m ≥ 2.

P [|S − E [S]| ≥ t] ≤ 2 exp

−

(cid:18)

pt2
X + 2Rt

2σ2

(cid:19)

.

Lemma A.8 (Moment-Control Bernstein’s Inequality for Random Vectors [15]). Let x1, . . . , xp ∈ Rd be i.i.d.
random vectors. Suppose there exist some positive number R and σ2

X such that

Let s = 1
p

(cid:80)p

k=1 xk, then for any t > 0, it holds that

E [(cid:107)xk(cid:107)m

2 ] ≤

X Rm−2,
σ2

for all integers m ≥ 2.

P [(cid:107)s − E [s](cid:107)2 ≥ t] ≤ 2(d + 1) exp

(cid:18)

−

pt2
X + 2Rt

2σ2

(cid:19)

.

Lemma A.9 (Gaussian Concentration Inequality). Let x ∼ N (0, Ip). Let f : Rp (cid:55)→ R be an L-Lipschitz function.
Then we have for all t > 0 that

P [f (X) − Ef (X) ≥ t] ≤ exp

−

(cid:18)

(cid:19)

.

t2
2L2

Lemma A.10 (Bounding Maximum Norm of Gaussian Vector Sequence). Let x1, . . . , xn1 be a sequence of (not
necessarily independent) standard Gaussian vectors in Rn2. It holds that

(cid:20)

P

max
i∈[n1]

(cid:107)xi(cid:107)2 >

√

(cid:21)
n2 + 2(cid:112)2 log(2n1)

≤ (2n1)−3.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

16

Proof: Since the function (cid:107)·(cid:107)2 is 1-Lipschitz, by Gaussian concentration inequality, for any i ∈ [n1], we have

(cid:20)
(cid:107)xi(cid:107)2 −

P

(cid:113)

E (cid:107)xi(cid:107)2

(cid:21)
2 > t

≤ P [(cid:107)xi(cid:107)2 − E (cid:107)xi(cid:107)2 > t] ≤ exp

(cid:19)

(cid:18)

−

t2
2

for all t > 0. Since E (cid:107)xi(cid:107)2

2 = n2, by a simple union bound, we obtain
t2
√
2

(cid:21)
n2 + t

(cid:107)xi(cid:107) >

max
i∈[n1]

≤ exp

−

(cid:18)

P

(cid:20)

(cid:19)

+ log n1

for all t > 0. Taking t = 2(cid:112)2 log(2n1) gives the claimed result.

Corollary A.11. Let Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1). It holds that
(cid:17)
n2 + 2(cid:112)2 log(2n1)

(cid:107)Φx(cid:107)∞ ≤

(cid:16)√

(cid:107)x(cid:107)2

for all x ∈ Rn2,

with probability at least 1 − (2n1)−3.

Proof: Let Φ = (cid:2)φ1, · · · , φn1(cid:3)(cid:62) . Without loss of generality, let us only consider x ∈ Sn2−1, we have
(cid:12)x(cid:62)φi(cid:12)

(cid:13)φi(cid:13)
(cid:13)

(cid:13)2 .

(cid:107)Φx(cid:107)∞ = max
i∈[n1]

(cid:12)
(cid:12) ≤ max
i∈[n1]

(cid:12)
(cid:12)

(A.1)

Invoking Lemma A.10 returns the claimed result.

Lemma A.12 (Covering Number of a Unit Sphere [42]). Let Sn−1 = {x ∈ Rn | (cid:107)x(cid:107)2 = 1} be the unit sphere. For
any ε ∈ (0, 1), there exists some ε cover of Sn−1 w.r.t. the (cid:96)2 norm, denoted as Nε, such that

|Nε| ≤

1 +

(cid:18)

(cid:19)n

2
ε

≤

(cid:19)n

.

(cid:18) 3
ε

Lemma A.13 (Spectrum of Gaussian Matrices, [42]). Let Φ ∈ Rn1×n2 (n1 > n2) contain i.i.d. standard normal
entries. Then for every t ≥ 0, with probability at least 1 − 2 exp (cid:0)−t2/2(cid:1), one has

√

√

n1 −

n2 − t ≤ σmin(Φ) ≤ σmax(Φ) ≤

n1 +

n2 + t.

√

√

Lemma A.14. For any ε ∈ (0, 1), there exists a constant C (ε) > 1, such that provided n1 > C (ε) n2, the random
matrix Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1) obeys

(cid:114) 2
(cid:114) 2
π
π
with probability at least 1 − 2 exp (−c (ε) n1) for some c (ε) > 0.

n1 (cid:107)x(cid:107)2 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + ε)

(1 − ε)

n1 (cid:107)x(cid:107)2

for all x ∈ Rn2,

Geometrically, this lemma roughly corresponds to the well known almost spherical section theorem [68, 69], see

also [70]. A slight variant of this version has been proved in [3], borrowing ideas from [71].

Proof: By homogeneity, it is enough to show that the bounds hold for every x of unit (cid:96)2 norm. For a ﬁxed
n1-Lipschitz, by concentration of

π n1. Note that (cid:107)·(cid:107)1 is

x0 with (cid:107)x0(cid:107)2 = 1, Φx0 ∼ N (0, I). So E (cid:107)Φx(cid:107)1 =
measure for Gaussian vectors in Lemma A.9, we have

(cid:113) 2

√

P [|(cid:107)Φx(cid:107)1 − E [(cid:107)Φx(cid:107)1]| > t] ≤ 2 exp

(cid:19)

(cid:18)

−

t2
2n1

for any t > 0. For a ﬁxed δ ∈ (0, 1), S n2−1 can be covered by a δ-net Nδ with cardinality #Nδ ≤ (1 + 2/δ)n2.
Now consider the event

(cid:40)

.
=

E

(cid:114) 2
π

(1 − δ)

n1 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + δ)

n1 ∀ x ∈ Nδ

.

(cid:114) 2
π

(cid:41)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

17

(cid:114) 2
π

∞
(cid:88)

k=0

∞
(cid:88)

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

A simple application of union bound yields

P [E c] ≤ 2 exp

−

+ n2 log

1 +

(cid:18)

δ2n1
π

(cid:18)

(cid:19)(cid:19)

.

2
δ

Choosing δ small enough such that

then conditioned on E, we can conclude that

(1 − 3δ) (1 − δ)−1 ≥ 1 − ε and (1 + δ) (1 − δ)−1 ≤ 1 + ε,

(1 − ε)

n1 ≤ (cid:107)Φx(cid:107)1 ≤ (1 + ε)

n1 ∀ x ∈ Sn2−1.

(cid:114) 2
π

Indeed, suppose E holds. Then it can easily be seen that any z ∈ Sn2−1 can be written as

z =

λkxk,

with |λk| ≤ δk, xk ∈ Nδ for all k.

Hence we have

Similarly,

(cid:107)Φz(cid:107)1 =

Φ

λkxk

≤

δk (cid:107)Φxk(cid:107)1 ≤ (1 + δ) (1 − δ)−1

(cid:114) 2
π

n1.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

∞
(cid:88)

k=0

(cid:107)Φz(cid:107)1 =

Φ

λkxk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

k=0

(cid:104)

1 − δ − δ (1 + δ) (1 − δ)−1(cid:105) (cid:114) 2

≥

n1 = (1 − 3δ) (1 − δ)−1

π

(cid:114) 2
π

n1.

Hence, the choice of δ above leads to the claimed result. Finally, given n1 > Cn2, to make the probability P [E c]
decaying in n1, it is enough to set C = 2π

(cid:1). This completes the proof.

δ2 log (cid:0)1 + 2

δ

APPENDIX B
THE RANDOM BASIS VS. ITS ORTHONORMALIZED VERSION

In this appendix, we consider the planted sparse model

Y = [x0 | g1 | · · · | gn−1] = [x0 | G] ∈ Rp×n

as deﬁned in (III.5), where

1
√
θp

x0(k) ∼i.i.d.

Ber (θ) ,

g(cid:96) ∼i.i.d. N

0,

,

1 ≤ k ≤ p, 1 ≤ (cid:96) ≤ n − 1.

(B.1)

Recall that one “natural/canonical” orthonormal basis for the subspace spanned by columns of Y is

(cid:19)

1
p

I

(cid:18)

(cid:16)

Y =

(cid:20) x0
(cid:107)x0(cid:107)2

| Px⊥

0

G

G(cid:62)Px⊥

0

G

(cid:17)−1/2(cid:21)

,

G(cid:48)

.
= Px⊥

0

G

(cid:16)

G(cid:62)Px⊥

0

G

(cid:17)−1/2

which is well-deﬁned with high probability as Px⊥

0

G is well-conditioned (proved in Lemma B.2). We write

for convenience. When p is large, Y has nearly orthonormal columns, and so we expect that Y closely approximates
Y. In this section, we make this intuition rigorous. We prove several results that are needed for the proof of
Theorem II.1, and for translating results for Y to results for Y in Appendix E-D.

For any realization of x0, let I = supp(x0) = {i | x0(i) (cid:54)= 0}. By Bernstein’s inequality in Lemma A.7 with
σ2
X = 2θ and R = 1, the event

.
=

E0

(cid:26) 1
2

θp ≤ |I| ≤ 2θp

(cid:27)

(B.2)

(B.3)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

18

holds with probability at least 1 − 2 exp (−θp/16). Moreover, we show the following:

Lemma B.1. When p ≥ Cn and θ > 1/

n, the bound

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

4

2

(cid:115)

≤

5

n log p
θ2p

holds with probability at least 1 − cp−2. Here C, c are positive constants.

(B.4)

Proof: Because E

(cid:104)

(cid:107)x0(cid:107)2
2

(cid:105)

we have

for all t > 0, which implies

= 1, by Bernstein’s inequality in Lemma A.7 with σ2

X = 2/(θp2) and R = 1/(θp),

P

(cid:104)(cid:12)
(cid:12)(cid:107)x0(cid:107)2
(cid:12)

2 − E

(cid:104)

(cid:107)x0(cid:107)2
2

(cid:105)(cid:12)
(cid:105)
(cid:12)
(cid:12) > t

= P

(cid:104)(cid:12)
(cid:12)(cid:107)x0(cid:107)2
(cid:12)

(cid:12)
(cid:105)
(cid:12)
2 − 1
(cid:12) > t

≤ 2 exp

−

(cid:18)

(cid:19)

θpt2
4 + 2t

(cid:20)
|(cid:107)x0(cid:107)2 − 1| >

P

(cid:21)

t
(cid:107)x0(cid:107)2 + 1

= P [|(cid:107)x0(cid:107)2 − 1| ((cid:107)x0(cid:107)2 + 1) > t] ≤ 2 exp

(cid:18)

−

θpt2
4 + 2t

(cid:19)

.

On the intersection with E0, (cid:107)x0(cid:107)2 + 1 ≥ 1√
2

+ 1 ≥ 5/4 and setting t =

(cid:113) n log p

θ2p , we obtain

(cid:34)

(cid:115)

(cid:35)

4
5
Unconditionally, this implies that with probability at least 1 − 2 exp (−pθ/16) − 2 exp (cid:0)−

−(cid:112)np log p

|(cid:107)x0(cid:107)2 − 1| ≥

n log p
θ2p

≤ 2 exp

(cid:12)
(cid:12)
(cid:12) E0

P

(cid:16)

(cid:17)

.

√

np log p(cid:1), we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

|1 − (cid:107)x0(cid:107)2|
(cid:107)x0(cid:107)2

≤

√
4

2

(cid:115)

5

n log p
θ2p

,

as desired.
.
= (cid:0)G(cid:62)Px⊥

Let M

0

G(cid:1)−1/2

. Then G(cid:48) = GM − x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM. We show the following results hold:

Lemma B.2. Provided p ≥ Cn, it holds that

(cid:107)M(cid:107) ≤ 2,

(cid:107)M − I(cid:107) ≤ 4

+ 4

(cid:114) n
p

(cid:115)

log(2p)
p

with probability at least 1 − (2p)−2. Here C is a positive constant.

Proof: First observe that

(cid:16)

(cid:16)

(cid:107)M(cid:107) =

σmin

G(cid:62)Px⊥

0

G

(cid:17)(cid:17)−1/2

= σ−1
min

(cid:0)Px⊥

0

G(cid:1) .

Now suppose B is an orthonormal basis spanning x⊥
as that of B(cid:62)G ∈ R(p−1)×(n−1); in particular,

0 . Then it is not hard to see the spectrum of Px⊥

G is the same

0

σmin

(cid:0)Px⊥

0

G(cid:1) = σmin

(cid:16)

B(cid:62)G

(cid:17)

.

0, 1
Since each entry of G ∼i.i.d. N
p
spectrum results for Gaussian matrices in Lemma A.13 and obtain that

, and B(cid:62) has orthonormal rows, B(cid:62)G ∼i.i.d. N

(cid:16)

(cid:17)

(cid:17)

(cid:16)

0, 1
p

, we can invoke the

(cid:114) p − 1
p

(cid:114) n − 1
p

−

− 2

log (2p)
p

(cid:115)

≤ σmin

(cid:16)

(cid:17)

B(cid:62)G

≤ σmax

(cid:16)

(cid:17)

B(cid:62)G

≤

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

log(2p)
p

(cid:115)

with probability at least 1 − (2p)−2. Thus, when p ≥ C1n for some sufﬁciently large constant C1, by using the

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

19

results above we have

(cid:107)M(cid:107) = σ−1
min

(cid:16)

(cid:17)

B(cid:62)G

=

(cid:32)(cid:114) p − 1

(cid:114) n − 1
p

−

− 2

p

(cid:115)

(cid:33)−1

log (2p)
p

≤ 2,

(cid:107)I − M(cid:107) = max (|σmax (M) − 1| , |σmin (M) − 1|)
(cid:16)

(cid:17)

(cid:16)

= max

≤ max

B(cid:62)G

min

(cid:16)(cid:12)
(cid:12)σ−1
(cid:12)

(cid:32)(cid:114) p − 1


−

p

− 1

(cid:12)
(cid:12)
(cid:12) ,
(cid:114) n − 1
p

(cid:12)
(cid:12)σ−1
(cid:12)

max

(cid:17)

B(cid:62)G

(cid:115)

− 2

log (2p)
p

(cid:17)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:33)−1

− 1, 1 −

(cid:32)(cid:114) p − 1

(cid:114) n − 1
p

+

+ 2

p

(cid:115)

log(2p)
p

(cid:33)−1




(cid:32)







= max

1 −

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

log (2p)
p

(cid:33) (cid:32)(cid:114) p − 1

(cid:114) n − 1
p

−

− 2

(cid:33)−1

,

log (2p)
p

(cid:32)(cid:114) p − 1

− 1 +

p

(cid:114) n − 1
p

+ 2

log(2p)
p

(cid:33) (cid:32)(cid:114) p − 1

(cid:114) n − 1
p

+

+ 2

log(2p)
p

(cid:33)−1




(cid:115)

(cid:115)

(cid:115)

(cid:115)

p

p

(cid:32)

≤ 2

1 −

(cid:114) p − 1
p

(cid:114) n − 1
p

+

+ 2

(cid:115)

(cid:33)

log (2p)
p

(cid:115)

≤ 4

+ 4

(cid:114) n
p

log(2p)
p

,

with probability at least 1 − (2p)−2.

Lemma B.3. Let YI be a submatrix of Y whose rows are indexed by the set I. There exists a constant C > 0,
such that when p ≥ Cn and 1/2 > θ > 1/

√

p,

n, the following
√
(cid:13)
(cid:13)Y(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 3
(cid:107)YI(cid:107)(cid:96)2→(cid:96)1 ≤ 7(cid:112)2θp,
(cid:13)G − G(cid:48)(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 4
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 20

√

(cid:13)
(cid:13)YI − YI

(cid:13)Y − Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 20

n + 7(cid:112)log(2p),
(cid:114)

(cid:114)

,

n log p
θ
n log p
θ

hold simultaneously with probability at least 1 − cp−2 for a positive constant c.

Proof: First of all, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

≤

1
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)(cid:96)2→(cid:96)1

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 GM
(cid:13)(cid:96)2→(cid:96)2

=

2
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)1

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

,

where in the last inequality we have applied the fact (cid:107)M(cid:107) ≤ 2 from Lemma B.2. Now x(cid:62)
0, (cid:107)x0(cid:107)2
vectors with each entry distributed as N
p
Lemma A.9, we have

0 G is an i.i.d. Gaussian
θp . So by Gaussian concentration inequality in

, where (cid:107)x0(cid:107)2

2 = |I|

(cid:16)

(cid:17)

2

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

≤ 2 (cid:107)x0(cid:107)2

(cid:115)

log(2p)
p

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

20

with probability at least 1 − c1p−2. On the intersection with E0, this implies
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2(cid:112)2θ log(2p),

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

with probability at least 1 − c2p−2 provided θ > 1/
that when p ≥ C1n,

n. Moreover, when intersected with E0, Lemma A.14 implies

(cid:107)G(cid:107)(cid:96)2→(cid:96)1 ≤

(cid:107)GI(cid:107)(cid:96)2→(cid:96)1 ≤ (cid:112)2θp

n. Hence, by Lemma B.2, when p > C2n,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1
√

√

p,
√

with probability at least 1 − c3p−2 provided θ > 1/
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)1 (cid:107)I − M(cid:107) +

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2
(cid:33)

√

p

≤

(cid:32)
(cid:114) n
4
p

(cid:115)

+ 4

log(2p)
p

(cid:13)Y(cid:13)
(cid:13)

(cid:13)
(cid:13)G(cid:48)
I

(cid:13)
(cid:13)GI − G(cid:48)
I

(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)x0(cid:107)(cid:96)2→(cid:96)1 + (cid:107)G(cid:107)(cid:96)2→(cid:96)1 ≤ (cid:107)x0(cid:107)1 +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)GI(cid:107)(cid:96)2→(cid:96)1 (cid:107)M(cid:107) +
(cid:13)
(cid:13)(cid:96)2→(cid:96)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ (cid:107)GI(cid:107)(cid:96)2→(cid:96)1 (cid:107)I − M(cid:107) +

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2
(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)
2
(cid:33)

GM

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:115)

≤ (cid:112)2θp

(cid:32)
4

(cid:114) n
p

+ 4

log(2p)
p

+ 2(cid:112)2θ log(2p) ≤ 4

n + 7(cid:112)log(2p),

√

√

p ≤ 2(cid:112)θp +

√

√

p ≤ 3

p,

≤ 2(cid:112)2θp + 2(cid:112)2θ log(2p) ≤ 4(cid:112)2θp,

+ 2(cid:112)2θ log(2p) ≤ 4

2θn + 6(cid:112)2θ log(2p),

√

(cid:107)YI(cid:107)(cid:96)2→(cid:96)1 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)1

+ (cid:13)

(cid:13)G(cid:48)
I

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤
√

(cid:107)x0(cid:107)1
(cid:107)x0(cid:107)2

+ 6(cid:112)2θp ≤ 7(cid:112)2θp

n. Finally, by Lemma B.1 and the results above, we obtain
(cid:114)

(cid:107)x0(cid:107)1 + (cid:13)

(cid:13)G − G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 20

(cid:107)x0(cid:107)1 + (cid:13)

(cid:13)GI − G(cid:48)
I

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤ 20

n log p
θ

,

(cid:114)

n log p
θ

,

with probability at least 1 − c4p−2 provided θ > 1/
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)x0(cid:107)2
1
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)YI − YI

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 ≤

(cid:13)Y − Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1 −

holding with probability at least 1 − c5p−2.

Lemma B.4. Provided p ≥ Cn and θ > 1/

√

n, the following
(cid:115)

(cid:13)G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ 2

(cid:114) n
p

+ 8

√

2 log(2p)
p

,

21(cid:112)n log(2p)
p
hold simultaneously with probability at least 1 − cp−2 for some constant c > 0.

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤

2 log(2p)

4n
p

+

+

p

8

Proof: First of all, we have when p ≥ C1n, it holds with probability at least 1 − c2p−2 that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

GM

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)∞

≤

1
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)(cid:96)2→(cid:96)∞

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 GM
(cid:13)(cid:96)2→(cid:96)2

≤

2
(cid:107)x0(cid:107)2
2

(cid:107)x0(cid:107)∞

(cid:13)
(cid:13)x(cid:62)
(cid:13)

(cid:13)
(cid:13)
0 G
(cid:13)2

,

where at the last inequality we have applied the fact (cid:107)M(cid:107) ≤ 2 from Lemma B.2. Moreover, from proof of Lemma B.3,
(cid:13)2 ≤ 2(cid:112)log(2p)/p (cid:107)x0(cid:107)2 with probability at least 1 − c3p−2 provided p ≥ C4n. Therefore,
we know that (cid:13)

0 G(cid:13)

(cid:13)x(cid:62)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

21

conditioned on E0, we obtain that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
holds with probability at least 1 − c5p−2 provided θ > 1/

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:96)2→(cid:96)∞

x0x(cid:62)
0
(cid:107)x0(cid:107)2
2

4 (cid:107)x0(cid:107)∞
(cid:107)x0(cid:107)2

GM

≤

√

(cid:115)

log(2p)
p

≤

4(cid:112)2 log(2p)
√
θp

n. Now by Corollary A.11, we have that

(cid:107)G(cid:107)(cid:96)2→(cid:96)∞ ≤

+ 2

(cid:114) n
p

(cid:115)

2 log(2p)
p

with probability at least 1 − c6p−2. Combining the above estimates and Lemma B.2, we have that with probability
at least 1 − c7p−2

where the last simpliﬁcation is provided that θ > 1/

n and p ≥ C8n for a sufﬁciently large C8. Similarly,

(cid:13)G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ (cid:107)GM(cid:107)(cid:96)2→(cid:96)∞ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

GM

(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
2
(cid:13)
(cid:13)
x0x(cid:62)
(cid:13)
(cid:13)
0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
(cid:13)
2
4(cid:112)2 log(2p)
√
θp

GM

+

≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)∞ (cid:107)M(cid:107) +

(cid:115)

≤ 2

+ 4

(cid:114) n
p

2 log(2p)
p

√

(cid:115)

≤ 2

+ 8

(cid:114) n
p

2 log(2p)
p

,

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞ ≤ (cid:107)G(cid:107)(cid:96)2→(cid:96)∞ (cid:107)I − M(cid:107) +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(8

GM

(cid:13)
x0x(cid:62)
(cid:13)
0
(cid:13)
(cid:13)
(cid:107)x0(cid:107)2
(cid:13)(cid:96)2→(cid:96)∞
2
√
2 + 4)(cid:112)n log(2p)

√
8

√
8

≤

≤

4n
p

4n
p

+

+

p

p

2 log(2p)

2 log(2p)

+

+

p

21(cid:112)n log(2p)
p

,

+

4(cid:112)2 log(2p)
√
θp

completing the proof.

APPENDIX C
PROOF OF (cid:96)1/(cid:96)2 GLOBAL OPTIMALITY

In this appendix, we prove the (cid:96)1/(cid:96)2 global optimality condition in Theorem II.1 of Section II.

Proof of Theorem II.1: We will ﬁrst analyze a canonical version, in which the input orthonormal basis is Y as

deﬁned in (III.6) of Section III:

Let q =

and let I be the support set of x0, we have

(cid:21)

(cid:20)q1
q2

min
q∈Rn

(cid:107)Yq(cid:107)1 ,

s.t. (cid:107)q(cid:107)2 = 1.

(cid:13)G(cid:48)

Iq2

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

I cq2

(cid:13)
(cid:13)1

− (cid:13)

≥ |q1|

(cid:107)Yq(cid:107)1 = (cid:107)YIq(cid:107)1 + (cid:107)YI cq(cid:107)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2
x0
(cid:107)x0(cid:107)2
x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≥ |q1|

≥ |q1|

− (cid:107)GIq2(cid:107)1 − (cid:13)
(cid:13)

(cid:0)GI − G(cid:48)

I

(cid:1) q2

(cid:13)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)
(cid:13)
(cid:0)GI c − G(cid:48)
(cid:13)

I c

(cid:1) q2

(cid:13)
(cid:13)1

− (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

(cid:13)G − G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2 ,

where G and G(cid:48) are deﬁned in (B.1) and (B.2) of Appendix B. By Lemma A.14 and intersecting with E0 deﬁned

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

22

in (B.3), we have that as long as p ≥ C1n,

(cid:107)GIq2(cid:107)1 ≤

(cid:107)q2(cid:107)2 = 2θ

p (cid:107)q2(cid:107)2 for all q2 ∈ Rn−1,

√

(cid:107)GI cq2(cid:107)1 ≥

(cid:107)q2(cid:107)2 =

p (1 − 2θ) (cid:107)q2(cid:107)2 for all q2 ∈ Rn−1,

√

1
2

2θp
√
p
p − 2θp
√
p

1
2

hold with probability at least 1 − c2p−2. Moreover, by Lemma B.3,

(cid:13)G − G(cid:48)(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)1 ≤ 4

√

n + 7(cid:112)log(2p)
√

holds with probability at least 1 − c3p−2 when p ≥ C4n and θ > 1/
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:107)Yq(cid:107)1 ≥ g(q)

+ (cid:107)q2(cid:107)2

.
= |q1|

(cid:18) 1
2

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

√

holds with probability at least 1 − c5p−2. Assuming E0, we observe

n. So we obtain that

p (1 − 2θ) − 2θ

p − 4

√

√

n − 7(cid:112)log(2p)

(cid:19)

Now g(q) is a linear function in |q1| and (cid:107)q2(cid:107)2. Thus, whenever θ is sufﬁciently small and p ≥ C6n such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≤ (cid:112)|I|

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ (cid:112)2θp.

(cid:112)2θp <

√

1
2

p (1 − 2θ) − 2θ

p − 4

√

√

n − 7(cid:112)log(2p),

±e1 are the unique minimizers of g(q) under the constraint q2
g(±e1), and we have

1 + (cid:107)q2(cid:107)2

2 = 1. In this case, because (cid:107)Y(±e1)(cid:107)1 =

(cid:107)Yq(cid:107)1 ≥ g(q) > g(±e1)

for all q (cid:54)= ±e1, ±e1 are the unique minimizers of (cid:107)Yq(cid:107)1 under the spherical constraint. Thus there exists a
universal constant θ0 > 0, such that for all 1/
n ≤ θ ≤ θ0, ±e1 are the only global minimizers of (II.2) if the
input basis is Y.

√

Any other input basis can be written as (cid:98)Y = YU, for some orthogonal matrix U. The program now is written as

which is equivalent to

which is obviously equivalent to the canonical program we analyzed above by a simple change of variable, i.e.,
q

.
= Uq, completing the proof.

min
q∈Rn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Yq
(cid:13)1

,

s.t. (cid:107)q(cid:107)2 = 1,

min
q∈Rn

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Yq
(cid:13)1

,

s.t. (cid:107)Uq(cid:107)2 = 1,

APPENDIX D
GOOD INITIALIZATION

In this appendix, we prove Proposition IV.3. We show that the initializations produced by the procedure described

Proof of Proposition IV.3: Our previous calculation has shown that θp/2 ≤ |I| ≤ 2θp with probability at least
n. Let Y = (cid:2)y1, · · · , yp(cid:3)(cid:62) as deﬁned in (III.6). Consider any i ∈ I.

√

in Section III are biased towards the optimal.

1 − c1p−2 provided p ≥ C2n and θ > 1/
Then x0(i) = 1√
θp , and

(cid:10)e1, yi/ (cid:13)

(cid:13)yi(cid:13)
(cid:13)2

(cid:11) =

√

1/

θp

(cid:107)x0(cid:107)2 (cid:107)yi(cid:107)2

(cid:107)x0(cid:107)2 ((cid:107)x0(cid:107)∞ / (cid:107)x0(cid:107)2 + (cid:107)(g(cid:48))i(cid:107)2)

≥

≥

√

1/

θp

√

1/

θp

(cid:107)x0(cid:107)2 ((cid:107)x0(cid:107)∞ / (cid:107)x0(cid:107)2 + (cid:107)gi(cid:107)2 + (cid:107)G − G(cid:48)(cid:107)(cid:96)2→(cid:96)∞)

,

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

23

where gi and (g(cid:48))i are the i-th rows of G and G(cid:48), respectively. Since such gi’s are independent Gaussian vectors
in Rn−1 distributed as N (0, 1/p), by Gaussian concentration inequality and the fact that |I| ≥ pθ/2 w.h.p.,

provided p ≥ C5n and θ > 1/

n. Moreover,

(cid:104)

P

∃i ∈ I : (cid:13)
√

(cid:13)gi(cid:13)

(cid:13)2 ≤ 2(cid:112)n/p

(cid:105)

≥ 1 − exp (−c3nθp) ≤ c4p−2,

(cid:114)

(cid:114)

(cid:107)x0(cid:107)2 =

|I| ×

≤

2θp ×

1
θp

√

=

2.

1
θp

Combining the above estimates and result of Lemma B.4, we obtain that provided p ≥ C6n and θ > 1/
probability at least 1 − c7p−2, there exists an i ∈ [p], such that if we set q(0) = yi/ (cid:13)
(cid:13)2, it holds that

(cid:13)yi(cid:13)

(cid:12)
(cid:12)q(0)
(cid:12)

1

(cid:12)
(cid:12)
(cid:12) ≥

√

2(cid:112)n/p +

√

(cid:16)

2

4n/p + 8

2 log(2p)/p + 21(cid:112)n log(2p)/p

(cid:17)

√

1/

θp
√

√

n, with

(using p ≥ C6n to simpliﬁy the above line)

θp
√
2(cid:112)n/p

√

1/

√

1/

1 + 6

θp + 2
√

1/

θp + 6
1
√

√
2
1
√

2)

(1 + 6
1
√

10

θn

,

≥

=

≥

≥

θn

√

θn

√

(as θ > 1/

n)

completing the proof.

We will next show that for an arbitrary orthonormal basis (cid:98)Y

.
= YU the initialization still biases towards the target
solution. To see this, suppose w.l.o.g. (cid:0)yi(cid:1)(cid:62) is a row of Y with nonzero ﬁrst coordinate. We have shown above that
if Y is the input orthonormal basis. For Y, as x0 = Ye1 = YUU(cid:62)e1,
with high probability
10
we know q(cid:63) = U(cid:62)e1 is the target solution corresponding to (cid:98)Y. Observing that

(cid:69)(cid:12)
(cid:12) ≥ 1
(cid:12)

(cid:68) yi
(cid:107)yi(cid:107)2

, e1

(cid:12)
(cid:12)
(cid:12)

θn

√

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

U(cid:62)e1,

(cid:17)(cid:62)

(cid:16)

e(cid:62)
i (cid:98)Y

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

e(cid:62)
i (cid:98)Y

(cid:17)(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:12)
(cid:12)
(cid:43)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

U(cid:62)e1,

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

U(cid:62)Y(cid:62)ei
(cid:107)U(cid:62)Y(cid:62)ei(cid:107)2

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

e1,

(Y)(cid:62) ei
(cid:107)Y(cid:62)ei(cid:107)2

(cid:43)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

e1,

yi
(cid:107)yi(cid:107)2

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

1
√

,

10

nθ

corroborating our claim.

APPENDIX E
LOWER BOUNDING FINITE SAMPLE GAP G(q)

In this appendix, we prove Proposition IV.4. In particular, we show that the gap G(q) deﬁned in (IV.8) is strictly

positive over a large portion of the sphere Sn−1.

Proof of Proposition IV.4: Without loss of generality, we work with the “canonical” orthonormal basis Y
deﬁned in (III.6). Recall that Y is the orthogonalization of the planted sparse basis Y as deﬁned in (III.5). We
deﬁne the processes Q(q) and Q(q) on q ∈ Sn−1, via

Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

, Q(q) =

yiSλ

(cid:104)

q(cid:62)yi(cid:105)

.

1
p

p
(cid:88)

i=1

1
p

p
(cid:88)

i=1

24

(E.1)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

Thus, we can separate Q(q) as Q(q) =

, where

(cid:21)

(cid:20) Q1(q)
Q2(q)

Q1(q) =

x0iSλ

(cid:104)

q(cid:62)yi(cid:105)

and Q2(q) =

(cid:104)

q(cid:62)yi(cid:105)

,

giSλ

1
p

p
(cid:88)

i=1

1
p

p
(cid:88)

i=1

and separate Q(q) correspondingly. Our task is to lower bound the gap G(q) for ﬁnite samples as deﬁned in (IV.8).
√
Since we can deterministically constrain |q1| and (cid:107)q2(cid:107)2 over the set Γ as deﬁned in (IV.7) (e.g.,
θ
and (cid:107)q2(cid:107)2 ≥ 1
10 for q2 is arbitrary here, as we can always take a sufﬁciently small θ), the
challenge lies in lower bounding |Q1 (q)| and upper bounding (cid:107)Q2 (q)(cid:107)2, which depend on the orthonormal basis
Y. The unnormalized basis Y is much easier to work with than Y. Our proof will follow the observation that
(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)

10 , where the choice of 1

≤ |q1| ≤ 3

1
√

nθ

10

|Q1 (q)| ≥ (cid:12)
(cid:107)Q2 (q)(cid:107) ≤ (cid:13)

(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)

(cid:12) − (cid:12)
(cid:13)2 + (cid:13)

(cid:12) − (cid:12)
(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)

(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12) ,
(cid:13)2 + (cid:13)

(cid:13)Q2 (q) − Q2 (q)(cid:13)

(cid:13)2 .

In particular, we show the following:

• Appendix E-A shows that the expected gap is lower bounded for all q ∈ Sn−1 with |q1| ≤ 3
(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

q2
1
θp

1
50

G (q)

.
=

≥

−

.

√

θ:

As |q1| ≥ 1
√
10

nθ

, we have

• Appendix E-B, as summarized in Proposition E.8, shows that whenever p ≥ Ω (cid:0)n4 log n(cid:1), it holds with high

probability that

(cid:12)
(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|

−

(cid:13)
(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

inf
q∈Γ

≥

1
5000

1
θ2np

.

(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

+

sup
q∈Γ

√

10

θn

≤

4 × 105θ5/2n3/2p

+

10
4 × 105θ2np

(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2
1
2 × 104θ2np

=

.

• Appendix E-D shows that whenever p ≥ Ω (cid:0)n4 log n(cid:1), it holds with high probability that
(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12)
(cid:12)
|q1|

(cid:13)Q2 (q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

sup
q∈Γ

+

√

10

θn

≤

4 × 105θ5/2n3/2p

+

10
4 × 105θ2np

=

1
2 × 104θ2np

.

Observing that

inf
q∈Γ

G(q) ≥ inf
q∈Γ

(cid:32) (cid:12)

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

−

(cid:12)E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|
(cid:12)Q1 (q) − Q1 (q)(cid:12)
(cid:12)
|q1|

(cid:32) (cid:12)

+

− sup
q∈Γ

(cid:13)Q2 (q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

(cid:33)

,

(cid:33)

(cid:32) (cid:12)

(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)
|q1|

+

(cid:13)Q2 (q) − E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

(cid:33)

− sup
q∈Γ

we obtain the result as desired.

For the general case when the input orthonormal basis is (cid:98)Y = YU with target solution q(cid:63) = U(cid:62)e1, a

straightforward extension of the deﬁnition for the gap would be:
(cid:13)
(cid:13)
(cid:13)

, U(cid:62)e1

Q

(cid:69)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:68)

(cid:16)

(cid:16)

(cid:17)

G

q; (cid:98)Y = YU

(cid:17) .
=

−

q; (cid:98)Y
|(cid:104)q, U(cid:62)e1(cid:105)|

(cid:16)

(cid:0)I − U(cid:62)e1e(cid:62)

1 U(cid:1) Q
(cid:13)
(cid:0)I − U(cid:62)e1e(cid:62)
(cid:13)

q; (cid:98)Y
1 U(cid:1) q(cid:13)
(cid:13)2

(cid:17)(cid:13)
(cid:13)
(cid:13)2

.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

25

(cid:16)

(cid:17)

Since Q

q; (cid:98)Y

= 1
p

(cid:80)p

k=1 U(cid:62)ykSλ

(cid:0)q(cid:62)U(cid:62)yk(cid:1), we have

(cid:16)

(cid:17)

UQ

q; (cid:98)Y

=

UU(cid:62)ykSλ

(cid:16)

q(cid:62)U(cid:62)yk(cid:17)

=

ykSλ

(cid:104)

(Uq)(cid:62) yk(cid:105)

= Q (Uq; Y) .

(E.2)

1
p

p
(cid:88)

k=1

1
p

p
(cid:88)

k=1

Hence we have

(cid:16)

G

q; (cid:98)Y = YU

=

(cid:17)

|(cid:104)Q (Uq; Y) , e1(cid:105)|
|(cid:104)Uq, e1(cid:105)|

−

(cid:13)
(cid:0)I − e1e(cid:62)
(cid:13)

1

(cid:1) Q (Uq; Y)(cid:13)
(cid:13)2
(cid:1) Uq(cid:13)
(cid:13)2

1

.

(cid:13)
(cid:0)I − e1e(cid:62)
(cid:13)

Therefore, from Proposition IV.4 above, we conclude that under the same technical conditions as therein,

q∈Sn−1:

1
√

10

θn

inf
≤|(cid:104)Uq,e1(cid:105)|≤3

√

θ

(cid:16)

(cid:17)

G

q; (cid:98)Y

≥

1
104θ2np

with high probability.

A. Lower Bounding the Expected Gap G(q)

In this section, we provide a nontrivial lower bound for the gap

G(q) =

(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)
(cid:12)
|q1|

−

(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)
(cid:13)2
(cid:107)q2(cid:107)2

.

More speciﬁcally, we show that:

for all q ∈ Sn−1 with |q1| ≤ 3

θ.

√

G(q) ≥

1
50

q2
1
θp

Proposition E.1. There exists some numerical constant θ0 > 0, such that for all θ ∈ (0, θ0), it holds that

(E.3)

(E.4)

Estimating the gap G(q) requires delicate estimates for E (cid:2)Q1(q)(cid:3) and E (cid:2)Q2(q)(cid:3). We ﬁrst outline the main

proof in Appendix E-A1, and delay these detailed technical calculations to the subsequent subsections.

1) Sketch of the Proof: W.l.o.g., we only consider the situation that q1 > 0, because the case of q1 < 0 can be

similarly shown by symmetry. By (E.1), we have

E (cid:2)Q1(q)(cid:3) = E
E (cid:2)Q2(q)(cid:3) = E

(cid:104)

(cid:104)

(cid:104)

(cid:105)(cid:105)

,

x0Sλ
(cid:104)

gSλ

x0q1 + q(cid:62)
2 g
(cid:105)(cid:105)

x0q1 + q(cid:62)
2 g

,

where g ∼ N

(cid:16)

(cid:17)
0, 1
p I

, and x0 ∼ 1√

θp Ber(θ). Let us decompose

g = g(cid:107) + g⊥,

with g(cid:107) = P(cid:107)g = q2q(cid:62)
2
(cid:107)q2(cid:107)2
2

g, and g⊥ = (I − P(cid:107))g. In this notation, we have

E (cid:2)Q2(q)(cid:3) = E
= E

(cid:104)

(cid:104)

g(cid:107)Sλ

x0q1 + q(cid:62)

(cid:105)(cid:105)

(cid:104)

+ E

2 g(cid:107)
(cid:105)(cid:105)

(cid:104)

g⊥Sλ
(cid:104)

x0q1 + q(cid:62)

2 g(cid:107)

(cid:105)(cid:105)

(cid:104)

Sλ

x0q1 + q(cid:62)
2 g

(cid:105)(cid:105)

g(cid:107)Sλ
q2
(cid:107)q2(cid:107)2
2

E

=

x0q1 + q(cid:62)
2 g

+ E [g⊥] E
(cid:105)(cid:105)

(cid:104)

q(cid:62)

2 gSλ

x0q1 + q(cid:62)
2 g

,

(cid:104)

(cid:104)

(cid:104)

where we used the facts that q(cid:62)
and E [g⊥] = 0. Let Z

.
= g(cid:62)q2 ∼ N (0, σ2) with σ2 = (cid:107)q2(cid:107)2

2 g(cid:107), g⊥ and g(cid:107) are uncorrelated Gaussian vectors and therefore independent,
2 /p, by partial evaluation of the expectations with

2 g = q(cid:62)

26

(E.5)

(E.6)

(E.7)

(E.8)

(E.9)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

respect to x0, we get

E (cid:2)Q1(q)(cid:3) =

E (cid:2)Q2(q)(cid:3) =

(cid:21)(cid:21)

,

(cid:115)

(cid:20)
Sλ

E

θ
p

θq2
(cid:107)q2(cid:107)2
2

E

(cid:20) q1√
+ Z
θp
(cid:20) q1√
θp

(cid:20)
ZSλ

(cid:21)(cid:21)

+ Z

+

(1 − θ)q2
(cid:107)q2(cid:107)2
2

E [ZSλ [Z]] .

Straightforward integration based on Lemma A.1 gives a explicit form of the expectations as follows

E (cid:2)Q1(q)(cid:3) =

E (cid:2)Q2(q)(cid:3) =

(cid:115)

(cid:26)(cid:20)

θ
p

(cid:16)

αΨ

−

(cid:17)

α
σ

(cid:19)(cid:21)

(cid:20)

(cid:18)

(cid:19)

+ βΨ

+ σ

ψ

(cid:16)

− ψ

−

(cid:17)(cid:21)(cid:27)

,

α
σ

(cid:26) 2 (1 − θ)
p

(cid:18)

(cid:19)

Ψ

−

+

λ
σ

θ
p

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)(cid:27)

q2,

−

β
σ
(cid:18) β
σ

(cid:18) β
σ
(cid:20)

where the scalars α and β are deﬁned as

α =

+ λ,

β =

− λ,

q1√
θp

q1√
θp

and ψ (t) and Ψ (t) are pdf and cdf for standard normal distribution, respectively, as deﬁned in Lemma A.1. Plugging
(E.7) and (E.8) into (E.3), by some simpliﬁcations, we obtain

G(q) =

1
q1

+

(cid:115)

σ
q1

(cid:20)

θ
p
(cid:115)

(cid:16)

(cid:17)

αΨ

(cid:20)

ψ

θ
p

−

α
σ
(cid:18) β
σ

(cid:19)

(cid:16)

− ψ

−

(cid:17)(cid:21)

.

α
σ

+ βΨ

(cid:19)

(cid:18) β
σ

−

2q1√
θp

(cid:18)

(cid:19)(cid:21)

Ψ

−

λ
σ

−

θ
p

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

(cid:19)

(cid:18) β
σ

+ Ψ

− 2Ψ

−

(cid:18)

(cid:19)(cid:21)

λ
σ

With λ = 1/

p and σ2 = (cid:107)q2(cid:107)2

√

2 /p = (1 − q2
δ + 1
(cid:112)1 − q2

,

1)/p, we have
β
σ

=

= −

1

δ − 1
(cid:112)1 − q2

1

,

λ
σ

=

1
(cid:112)1 − q2

1

,

−

α
σ
√

√

θ for q1 ≤ 3

where δ = q1/
More speciﬁcally, we approximate Ψ (cid:0)− α
(cid:1) and ψ (cid:0)− α
σ
σ
−1 + δ. Applying the estimates for the relevant quantities established in Lemma E.2, we obtain

θ. To proceed, it is natural to consider estimating the gap G(q) by Taylor’s expansion.
(cid:1) around −1 − δ, and approximate Ψ
around

and ψ

(cid:16) β
σ

(cid:16) β
σ

(cid:17)

(cid:17)

G(q) ≥

Φ1(δ) −

Φ2(δ) +

1
δp

1 − θ
p

ψ(−1)q2

1 +

(cid:18)

√

σ

p +

(cid:19)

− 1

θ
2

1
p

(cid:2)1 + δ2 − θδ2 − σ (cid:0)1 + δ2(cid:1) √

p(cid:3) q2

1η1 (δ) +

η1 (δ) −

σ
√

δ

p

1 − θ
p

+

1
2δp

η2(δ)q2
1
√

θq3
1

5CT

p

(δ + 1)3 ,

where we deﬁne

Φ1(δ) = Ψ(−1 − δ) + Ψ(−1 + δ) − 2Ψ(−1),
η1(δ) = ψ(−1 + δ) − ψ(−1 − δ),

Φ2(δ) = Ψ(−1 + δ) − Ψ(−1 − δ),
η2(δ) = ψ(−1 + δ) + ψ(−1 − δ),

and CT is as deﬁned in Lemma E.2. Since 1 − σ
2p η2(δ), and (cid:0)1 + δ2(cid:1) (cid:0)1 − σ
θq2
1

p(cid:1) q2

√

1η1 (δ) / (2δp), and using the fact that δ = q1/

√

p ≥ 0, dropping those small positive terms q2

1

p (1 − θ)ψ(−1),

G(q) ≥

Φ1(δ) −

[Φ2(δ) − σ

pη1(δ)] −

(1 − σ

√

1 − θ
p
1 − θ
p

1
δp
1
δp

≥

Φ1(δ) −

[Φ2(δ) − η1(δ)] −

q2
1
p
η1 (δ)
δ

q2
1
p

√

p) η2(δ) −
(cid:18) 2θ
√
2π

+

2

√

θ
2p
3θ2
√
2π

−

q2
1
θp

q3
1η1 (δ) −
(cid:19)

+ C1θ2

,

√

θ, we obtain
√

C1

θq3
1

p

max

(cid:18) q3
1
θ3/2

(cid:19)

, 1

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

27

1 ≥
1 to simplify the expression. Substituting the estimates in Lemma E.4 and use the fact δ (cid:55)→ η1 (δ) /δ is bounded,

for some constant C1 > 0, where we have used q1 ≤ 3
1 − q2
we obtain

θ to simplify the bounds and the fact σ

√

p = (cid:112)1 − q2

√

G (p) ≥

(cid:18) 1
40
(cid:18) 1
40

1
p
q2
1
θp

−

θ

1
√
2π
1
√
2π

−

≥

(cid:19)

δ2 −

q2
1
θp

(cid:0)c1θ + c2θ2(cid:1)
(cid:19)

θ − c1θ − c2θ2

for some positive constants c1 and c2. We obtain the claimed result once θ0 is made sufﬁciently small.

θ. There exists some universal constant CT > 0 such that we have the follow polynomial

2) Auxiliary Results Used in the Proof:

√

.
Lemma E.2. Let δ
= q1/
approximations hold for all q1 ∈ (cid:0)0, 1
2
(cid:17)
α
σ
(cid:18) β
σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

ψ

ψ

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:1):

(cid:20)

−

1 −

(1 + δ)2q2
1

(cid:19)

(cid:20)

−

1 −

(δ − 1)2q2
1

≤ CT (1 + δ)2 q4
1,

≤ CT (δ − 1)2 q4
1,

1
2

1
2
1
2

1
2

(cid:21)

(cid:21)

(cid:12)
(cid:12)
ψ(−1 − δ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ψ(δ − 1)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

ψ(−1)q2
1

1
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

Ψ

−

(cid:17)

α
σ

(cid:20)

(cid:19)

(cid:20)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ψ

(cid:18) β
σ

−

Ψ(−1 − δ) −

ψ(−1 − δ)(1 + δ)q2
1

≤ CT (1 + δ)2 q4
1,

−

Ψ(δ − 1) +

ψ(δ − 1)(δ − 1)q2
1

≤ CT (δ − 1)2 q4
1,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:19)

(cid:20)

Ψ

−

−

Ψ(−1) −

λ
σ

≤ CT q4
1.

Proof: First observe that for any q1 ∈ (cid:0)0, 1

(cid:1) it holds that

2

0 ≤

1
(cid:112)1 − q2

1

(cid:18)

−

1 +

(cid:19)

q2
1
2

≤ q4
1.

Hence we have

So we have

(δ − 1)

1 +

≤ (δ − 1)

1 +

, when δ ≥ 1

−(1 + δ)

1 +

1 + q4
q2
1

≤ −

≤ −(1 + δ)

1 +

(cid:18)

(cid:18)

1
2
(cid:18)

1
2

(cid:19)

(cid:19)

(cid:19)

1
2

q2
1

α
σ

≤

≤

β
σ
β
σ

(cid:19)(cid:19)

(cid:18)

(cid:18)

(cid:19)

1
2

q2
1

,

(cid:19)

1 + q4
q2
1
(cid:19)

q2
1

(cid:18)

1
2
1
2

(cid:18)

(δ − 1)

1 +

1 + q4
q2
1

≤ (δ − 1)

1 +

, when δ ≤ 1.

(cid:18)

(cid:18)

ψ

−(1 + δ)

1 +

1
2

1 + q4
q2
1

(cid:16)

≤ ψ

−

(cid:17)

α
σ

≤ ψ

−(1 + δ)

1 +

(cid:18)

(cid:19)(cid:19)

.

1
2

q2
1

By Taylor expansion of the left and right sides of the above two-side inequality around −1 − δ using Lemma A.2,
we obtain

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

ψ

−

(cid:17)

α
σ

− ψ(−1 − δ) −

(1 + δ)2q2

≤ CT (1 + δ)2 q4
1,

1
2

(cid:12)
(cid:12)
1ψ(−1 − δ)
(cid:12)
(cid:12)

for some numerical constant CT > 0 sufﬁciently large. In the same way, we can obtain other claimed results.

Lemma E.3. For any δ ∈ [0, 3], it holds that

Φ2(δ) − η1(δ) ≥

η1 (3)
9

δ3 ≥

1
20

δ3.

(E.10)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

28

Proof: Let us deﬁne

h(δ) = Φ2(δ) − η1(δ) − Cδ3

for some C > 0 to be determined later. Then it is obvious that h(0) = 0. Direct calculation shows that

d
dδ

d
dδ

d
dδ

Φ1(δ) = η1(δ),

Φ2(δ) = η2(δ),

η1(δ) = η2(δ) − δη1(δ).

(E.11)

Thus, to show (E.10), it is sufﬁcient to show that h(cid:48)(δ) ≥ 0 for all δ ∈ [0, 3]. By differentiating h(δ) with respect to
δ and use the results in (E.11), it is sufﬁcient to have

h(cid:48)(δ) = δη1(δ) − 3Cδ2 ≥ 0 ⇐⇒ η1(δ) ≥ 3Cδ

for all δ ∈ [0, 3]. We obtain the claimed result by observing that δ (cid:55)→ η1 (δ) /3δ is monotonically decreasing over
δ ∈ [0, 3] as justiﬁed below.
Consider the function

To show it is monotonically decreasing, it is enough to show p(cid:48) (δ) is always nonpositive for δ ∈ (0, 3), or equivalently

p (δ)

.
=

η1 (δ)
3δ

=

1
√
2π

3

(cid:18)

exp

−

δ2 + 1
2

(cid:19) eδ − e−δ
δ

.

g (δ)

.
=

(cid:16)

eδ + e−δ(cid:17)

δ − (cid:0)δ2 + 1(cid:1) (cid:16)

eδ − e−δ(cid:17)

≤ 0

for all δ ∈ (0, 3), which can be easily veriﬁed by noticing that g (0) = 0 and g(cid:48) (δ) ≤ 0 for all δ ≥ 0.

Lemma E.4. For any δ ∈ [0, 3], we have

(1 − θ)Φ1(δ) −

[Φ2(δ) − η1(δ)] ≥

1
δ

(cid:18) 1
40

−

1
√
2π

θ

(cid:19)

δ2.

(E.12)

Proof: Let us deﬁne

g(δ) = (1 − θ)Φ1(δ) −

[Φ2(δ) − η1(δ)] − c0 (θ) δ2,

1
δ

where c0 (θ) > 0 is a function of θ. Thus, by the results in (E.11) and L’Hospital’s rule, we have

Φ2(δ)
δ

lim
δ→0

= lim
δ→0

η2 (δ) = 2ψ(−1),

[η2(δ) − δη1(δ)] = 2ψ(−1).

η1(δ)
δ

lim
δ→0

= lim
δ→0

Combined that with the fact that Φ1(0) = 0, we conclude g (0) = 0. Hence, to show (E.12), it is sufﬁcient to show
that g(cid:48)(δ) ≥ 0 for all δ ∈ [0, 3]. Direct calculation using the results in (E.11) shows that

Since η1 (δ) /δ is monotonically decreasing as shown in Lemma E.3, we have that for all δ ∈ (0, 3)

g(cid:48)(δ) =

1
δ2 [Φ2(δ) − η1(δ)] − θη1(δ) − 2c0 (θ) δ.

Using the above bound and the main result from Lemma E.3 again, we obtain

η1 (δ) ≤ δ lim
δ→0

η (δ)
δ

≤

2
√
2π

δ.

g(cid:48)(δ) ≥

1
20

δ −

2
√
2π

θδ − 2c0δ.

Choosing c0 (θ) = 1

40 − 1√

2π

θ completes the proof.

B. Finite Sample Concentration

In the following two subsections, we estimate the deviations around the expectations E (cid:2)Q1(q)(cid:3) and E (cid:2)Q2(q)(cid:3),
i.e., (cid:12)
(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:13)2, and show that the total deviations ﬁt into the gap G(q) we
derived in Appendix E-A. Our analysis is based on the scalar and vector Bernstein’s inequalities with moment
conditions. Finally, in Appendix E-C, we uniformize the bound by applying the classical discretization argument.

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:12) and (cid:13)

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

29

1) Concentration for Q1(q):

Lemma E.5 (Bounding (cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12)). For each q ∈ Sn−1, it holds for all t > 0 that
(cid:18)

(cid:19)

P (cid:2)(cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) ≥ t(cid:3) ≤ 2 exp

−

θp3t2
8 + 4pt

.

Proof: By (E.1), we know that

Q1(q) =

X 1

k , X 1

k = x0(k)Sλ [x0(k)q1 + Zk]

1
p

p
(cid:88)

k=1

where Zk = q(cid:62)

2 gk ∼ N

(cid:16)

0, (cid:107)q2(cid:107)2
p

2

(cid:17)

. Thus, for any m ≥ 2, by Lemma A.4, we have

E (cid:2)(cid:12)

(cid:12)X 1
k

(cid:12)
(cid:12)

m(cid:3) ≤ θ

E

E

(cid:19)l

m(cid:21)

(cid:19)m

(cid:19)m m
(cid:88)

(cid:20)(cid:12)
(cid:12)
(cid:18) 1
q1√
(cid:12)
(cid:12)
√
+ Zk
(cid:12)
(cid:12)
θp
θp
(cid:12)
(cid:12)
(cid:19) (cid:18) q1√
(cid:18)m
(cid:18) 1
√
l
θp
θp
(cid:18) 1
(cid:19) (cid:18) q1√
(cid:18)m
√
l
θp
θp
(cid:19)m (cid:18) q1√
(cid:18) 1
(cid:107)q2(cid:107)2√
√
p
θp
θp
(cid:19)m−2
(cid:19)m

l=0
(cid:19)m m
(cid:88)

(cid:19)l

l=0

+

θ

(cid:19)m

= θ

= θ

≤

m!
2

(cid:104)

|Zk|m−l(cid:105)

(m − l − 1)!!

(cid:19)m−l

(cid:18) (cid:107)q2(cid:107)2√

p

(cid:18) 2
θp
X = 4/(θp2) and R = 2/(θp), apply Lemma A.7, we get

(cid:18) 2
θp

4
θp2

m!
2

m!
2

≤

=

θ

let σ2

P (cid:2)(cid:12)

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) ≥ t(cid:3) ≤ 2 exp

(cid:18)

−

θp3t2
8 + 4pt

(cid:19)

.

as desired.

2) Concentration for Q2(q):

Lemma E.6 (Bounding (cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

P (cid:2)(cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2). For each q ∈ Sn−1, it holds for all t > 0 that
(cid:18)
(cid:13)2 > t(cid:3) ≤ 2(n + 1) exp

θp3t2
√
128n + 16

θnpt

−

(cid:19)

.

Before proving Lemma E.6, we record the following useful results.

Lemma E.7. For any positive integer s, l > 0, we have

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

s
(cid:13)
(cid:13)
2

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

≤

(l + s)!!
2

(cid:107)q2(cid:107)l
2

(2
(cid:0)√

√
n)s
p(cid:1)s+l .

In particular, when s = l, we have

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

l
(cid:13)
(cid:13)
2

(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

l!
2

(cid:107)q2(cid:107)l
2

√

(cid:18) 4

(cid:19)l

n

p

≤

(cid:17)

2

Proof: Let Pq(cid:107)

= q2q(cid:62)
2
(cid:107)q2(cid:107)2
2
complement, respectively. By Lemma A.4, we have
gk(cid:13)
(cid:13)
(cid:13)2

(cid:20)(cid:16)(cid:13)
(cid:13)
(cid:13)Pq(cid:107)

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

and Pq⊥

I − 1

≤ E

s
(cid:13)
(cid:13)
2

(cid:107)q2(cid:107)2
2

l(cid:21)

=

E

2

2

q2q(cid:62)
2

(cid:16)

+

(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)2

2 gk(cid:12)
(cid:17)s (cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

denote the projection operators onto q2 and its orthogonal

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

30

Using Lemma A.5 and the fact that (cid:13)

(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)2

E

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)

s
(cid:13)
(cid:13)
2

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

l(cid:21)

≤ (cid:107)q2(cid:107)l
2

=

=

s
(cid:88)

i=0
s
(cid:88)

i=0

(cid:18)s
i
(cid:18)s
i

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

2

gk(cid:13)
(cid:13)
(cid:13)

(cid:21)

i

(cid:21)

i

2

2

E

E

≤ (cid:107)q2(cid:107)l
2

s
(cid:88)

i=0

(cid:21)

s−i

l (cid:13)
(cid:13)
(cid:13)Pq(cid:107)

2

gk(cid:13)
(cid:13)
(cid:13)
2

l+s−i(cid:21)

1
(cid:107)q2(cid:107)s−i
2

(cid:19)l+s−i

(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:21) (cid:18) 1
√
p

(l + s − i − 1)!!.

2

i

2

E

(cid:19)

(cid:19)i

s
(cid:88)

gk(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:13)
(cid:13)
(cid:13)Pq⊥

(cid:13)2, we obtain
(cid:19) (cid:18) √
√

(cid:18)s
i
(cid:13)gk(cid:13)
≤ (cid:13)
(cid:18)s
i
(cid:19)l (l + s)!!
2
√
n)s
p(cid:1)s+l .

i=0
(cid:18) 1
√
p

(cid:107)q2(cid:107)l
2

(2
(cid:0)√

n
p

i!!

≤ (cid:107)q2(cid:107)l
2

≤

(l + s)!!
2

(l + s − i − 1)!!

(cid:19)l+s−i

(cid:18) 1
√
p

(cid:18) √
√

n
p

+

1
√
p

(cid:19)s

Now, we are ready to prove Lemma E.6,

Proof: By (E.1), note that

Q2 =

X2

k, X2

k = gkSλ [x0(k)q1 + Zk]

1
p

p
(cid:88)

k=1

where Zk = q(cid:62)

E (cid:2)(cid:13)

(cid:13)X2
k

(cid:13)
(cid:13)

m
2

2 gk. Thus, for any m ≥ 2, by Lemma E.7, we have
(cid:104)(cid:13)
(cid:13)

+ (1 − θ)E

(cid:3) ≤ θE

+ q(cid:62)

m(cid:21)

m

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

m

(cid:13)
(cid:13)

l (cid:13)
(cid:13)

2 gk
(cid:13)gk(cid:13)
(cid:19) (m + l)!!
2
q1√
θp

+

p

m

(cid:13)gk(cid:13)

(cid:13)
(cid:13)
2

m(cid:105)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)
(cid:13)gk(cid:13)
(cid:104)(cid:13)
(cid:13)

(cid:13)
(cid:13)
2

m

m−l

q1√
θp

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (1 − θ)E

(cid:18) (cid:107)q2(cid:107)2√

p

(cid:19)l (cid:12)
(cid:12)
(cid:12)
(cid:12)

q1√
θp

(cid:12)
(cid:12)
(cid:12)
(cid:12)

m−l

(cid:19)m

+ (1 − θ)

m!
2

(cid:107)q2(cid:107)m
2

(cid:18) 4

√

n

p

m!
2
(cid:19)m

m(cid:105)

2 gk(cid:12)
(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

+ (1 − θ)

(cid:107)q2(cid:107)m
2

√

(cid:18) 4

(cid:19)m

n

p

(cid:20)(cid:13)
(cid:13)gk(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:19)
(cid:18)m
l

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

m
(cid:88)

q1√
θp
(cid:20)(cid:12)
2 gk(cid:12)
(cid:12)q(cid:62)
(cid:12)
(cid:12)
(cid:12)

(cid:18)m
l

l=0

(cid:19)m (cid:18) (cid:107)q2(cid:107)2√

≤ θ

≤ θ

≤ θ

≤

(cid:19)m m
(cid:88)

l=0
√
(cid:18) 2
√

n
p
√
(cid:18) 4
√
√
(cid:18) 8
√

n
p
(cid:19)m
n
θp

m!
2
m!
2

.

√

√

Taking σ2

X = 64n/(θp2) and R = 8

n/(

θp) and using vector Bernstein’s inequality in Lemma A.8, we obtain

P (cid:2)(cid:13)

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 ≥ t(cid:3) ≤ 2(n + 1) exp

(cid:18)

−

θp3t2
√
128n + 16

θnpt

(cid:19)

,

as desired.

C. Union Bound
Proposition E.8 (Uniformizing the Bounds). Suppose that θ > 1/
C (ξ), such that whenever p ≥ C (ξ) n4 log n, we have

√

n. Given any ξ > 0, there exists some constant

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤

,

2ξ
θ5/2n3/2p
2ξ
θ2np

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

31

hold uniformly for all q ∈ Sn−1, with probability at least 1 − c(ξ)p−2 for a positive constant c(ξ).

Proof: We apply the standard covering argument. For any ε ∈ (0, 1), by Lemma A.12, the unit hemisphere of

interest can be covered by an ε-net Nε of cardinality at most (3/ε)n. For any q ∈ Sn−1, it can be written as

, which is an independent copy of y = [x0, g](cid:62).

q = q(cid:48) + e

where q(cid:48) ∈ Nε and (cid:107)e(cid:107)2 ≤ ε. Let a row of Y be yk = (cid:2)x0(k), gk(cid:3)(cid:62)
By (E.1), we have
(cid:12)
(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)
(cid:110)

(cid:69)(cid:105)

(cid:104)(cid:68)

(cid:104)(cid:68)

(cid:104)

yk, q(cid:48) + e

− E

x0(k)Sλ

yk, q(cid:48) + e

x0(k)Sλ

(cid:69)(cid:105)(cid:105)(cid:111)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

p
(cid:88)

k=1

1
p

p
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
p
(cid:12)
(cid:12)E (cid:2)x0Sλ

k=1
p
(cid:88)

k=1

=

≤

+ (cid:12)

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48) + e

(cid:69)(cid:105)

−

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48)(cid:69)(cid:105)

+

x0(k)Sλ

(cid:104)(cid:68)

yk, q(cid:48)(cid:69)(cid:105)

− E (cid:2)x0Sλ

(cid:2)(cid:10)y, q(cid:48)(cid:11)(cid:3)(cid:3)

p
(cid:88)

1
p

(cid:2)(cid:10)y, q(cid:48)(cid:11)(cid:3)(cid:3) − E (cid:2)x0Sλ

k=1
(cid:2)(cid:10)y, q(cid:48) + e(cid:11)(cid:3)(cid:3)(cid:12)
(cid:12) .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Using Cauchy-Schwarz inequality and the fact that Sλ [·] is a nonexpansive operator, we have

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ (cid:12)

(cid:12)Q1(q(cid:48)) − E (cid:2)Q1(q(cid:48))(cid:3)(cid:12)

(cid:12) +

|x0(k)|

p
(cid:88)

(cid:32)

1
p

k=1
1
√
θp

(cid:33)

(cid:107)e(cid:107)2

(cid:13)
(cid:13)

(cid:13)yk(cid:13)
+ E [|x0| (cid:107)y(cid:107)2]
(cid:13)
(cid:13)2
(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

≤ (cid:12)

(cid:12)Q1(q(cid:48)) − E (cid:2)Q1(q(cid:48))(cid:3)(cid:12)

(cid:18) 2
√
θp
(cid:13)2 ≤ (cid:112)n/p + 2(cid:112)2 log(2p)/p with probability at least 1 − c1p−3. Also E [(cid:107)g(cid:107)2] ≤
≤ (cid:112)n/p. Taking t = ξθ−5/2n−3/2p−1 in Lemma E.5 and applying a union bound with ε =

+ E [(cid:107)g(cid:107)2]

+ max
k∈[p]

(cid:13)gk(cid:13)
(cid:13)

(cid:12) + ε

(cid:13)
(cid:13)

(cid:19)

.

By Lemma A.10, maxk∈[p]
(cid:105)(cid:17)1/2
(cid:16)

(cid:104)

E

(cid:107)g(cid:107)2
2

ξθ−2n−2(log 2p)−1/2/7, and combining with the above estimates, we obtain that

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

ξ
θ5/2n3/2p

+

ξ
7θ5/2n2(cid:112)log(2p)p

√

(cid:16)
4

(cid:17)
n + 2(cid:112)2 log(2p)

≤

2ξ
θ5/2n3/2p

holds for all q ∈ Sn−1, with probability at least

1 − c1p−3 − 2 exp (cid:0)−c3 (ξ) p/(θ4n3) + c4 (ξ) n log n + c5(ξ)n log log(2p)(cid:1) .

Similarly, by (E.1), we have

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 =

(cid:104)(cid:68)

gkSλ

yk, q(cid:48) + e

− E (cid:2)gSλ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
p

p
(cid:88)

(cid:110)

k=1

(cid:69)(cid:105)

(cid:32)

(cid:13)
(cid:13)
(cid:2)(cid:10)y, q(cid:48) + e(cid:11)(cid:3)(cid:3)(cid:111)
(cid:13)
(cid:13)
(cid:13)2

≤ (cid:13)

(cid:13)Q2(q(cid:48)) − E (cid:2)Q2(q(cid:48))(cid:3)(cid:13)

≤ (cid:13)

(cid:13)Q2(q(cid:48)) − E (cid:2)Q2(q(cid:48))(cid:3)(cid:13)

(cid:33)

p
(cid:88)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

k=1

(cid:13)2 +

1
p
(cid:20)

+ E [(cid:107)g(cid:107)2 (cid:107)y(cid:107)2]

(cid:13)yk(cid:13)
(cid:13)
(cid:13)2
(cid:18) 1
√
θp
(cid:13)2, and taking t = ξθ−2n−1p−1 in Lemma E.6 and applying a union

(cid:13)gk(cid:13)
(cid:13)
(cid:13)2
(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

(cid:107)e(cid:107)2
√
√

(cid:13)gk(cid:13)
(cid:13)
(cid:13)2

+ max
k∈[p]

(cid:13)2 + ε

max
k∈[p]

n
θp

n
p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

+

(cid:19)

(cid:21)

.

Applying the above estimates for maxk∈[p]
bound with ε = ξθ−2n−2 log−1(2p)/30, we obtain that

(cid:13)gk(cid:13)
(cid:13)

(cid:13)
(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 ≤

ξ
θ2np

ξ
θ2np

+

+

≤

ξ
30θ2n2 log(2p)

ξ
30θ2n2 log(2p)

+

(cid:32)(cid:114) n
p



4

(cid:26) 16 log(2p)
p

(cid:27)

+

10n
p

(cid:115)

2 log(2p)
p

(cid:33)2

+

2n
p






IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

32

≤

2ξ
θ2np

holds for all q ∈ Sn−1, with probability at least

1 − c1p−3 − exp (cid:0)−c6 (ξ) p/(θ3n3) + c7(ξ)n log n + c8(ξ)n log log(2p)(cid:1) .

Taking p ≥ C9(ξ)n4 log n and simplifying the probability terms complete the proof.

D. Q(q) approximates Q(q)

Proposition E.9. Suppose θ > 1/
p ≥ C (ξ) n4 log n, the following bounds

√

n. For any ξ > 0, there exists some constant C (ξ), such that whenever

(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)

(cid:12) ≤

sup
q∈Sn−1

sup
q∈Sn−1

(cid:13)
(cid:13)Q2(q) − Q2(q)(cid:13)

(cid:13)2 ≤

ξ
θ5/2n3/2p

ξ
θ2np

,

(E.13)

(E.14)

hold with probability at least 1 − c(ξ)p−2 for a positive constant c(ξ).

Proof: First, for any q ∈ Sn−1, from (E.1), we know that
(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)
(cid:12)
p
(cid:88)

p
(cid:88)

(cid:104)

(cid:104)

x0(k)Sλ

q(cid:62)yk(cid:105)

−

q(cid:62)yk(cid:105)

x0(k)
(cid:107)x0(cid:107)2

Sλ

=

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

1
p

1
p

k=1
p
(cid:88)

k=1
p
(cid:88)

k=1

1
p

1
p

k=1
p
(cid:88)

k=1
(cid:104)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

p
(cid:88)

k=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|x0(k)|

1 −

1
p

p
(cid:88)

k=1

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

−

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

+

x0(k)Sλ

(cid:104)

q(cid:62)yk(cid:105)

−

|x0(k)|

(cid:12)
(cid:12)
(cid:12)Sλ

(cid:104)

q(cid:62)yk(cid:105)

− Sλ

q(cid:62)yk(cid:105)(cid:12)
(cid:12)
(cid:12) +

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)Sλ

(cid:104)

q(cid:62)yk(cid:105)(cid:12)
(cid:12)
(cid:12) .

1
p

p
(cid:88)

k=1

x0(k)
(cid:107)x0(cid:107)2

(cid:104)

q(cid:62)yk(cid:105)

Sλ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

For any I = supp(x0), using the fact that Sλ[·] is a nonexpansive operator, we have

(cid:12)Q1(q) − Q1(q)(cid:12)
(cid:12)

(cid:12) ≤

sup
q∈Sn−1

1
p

sup
q∈Sn−1

(cid:88)

k∈I
(cid:18)

=

√

1
θp3/2

|x0(k)|

(cid:12)
(cid:12)

(cid:12)q(cid:62) (cid:16)

yk − yk(cid:17)(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

(cid:13)
(cid:13)YI − YI

(cid:13)
(cid:13)(cid:96)2→(cid:96)1 +

1 −

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)x0(cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)YI(cid:107)(cid:96)2→(cid:96)1

.

(cid:88)

k∈I

sup
q∈Sn−1
(cid:19)

|x0(k)|

(cid:12)
(cid:12)

(cid:12)q(cid:62)yk(cid:12)

(cid:12)
(cid:12)

By Lemma B.1 and Lemma B.3 in Appendix B, we have the following holds

(cid:12)
(cid:12)Q1(q) − Q1(q)(cid:12)

(cid:12) ≤

sup
q∈Sn−1

√

1
θp3/2

(cid:32)

(cid:114)

20

√

4

2

(cid:115)

n log p
θ

+

5

n log p
θ2p

(cid:33)

× 7(cid:112)2θp

≤

32
θp3/2

(cid:112)n log p,

with probability at least 1 − c1p−2, provided p ≥ C2n and θ > 1/
n. Simple calculation shows that it is enough
to have p ≥ C3 (ξ) n4 log n for some sufﬁciently large C1 (ξ) to obtain the claimed result in (E.13). Similarly, by
Lemma B.3 and Lemma B.4 in Appendix B, we have

√

sup
q∈Sn−1

= sup

q∈Sn−1

≤ sup

q∈Sn−1

(cid:104)

1
p

p
(cid:88)

(cid:13)
(cid:13)Q2(q) − Q2(q)(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1
p
(cid:88)

gkSλ

gkSλ

1
p

(cid:104)

k=1

q(cid:62)yk(cid:105)

q(cid:62)yk(cid:105)

−

−

1
p

1
p

p
(cid:88)

k=1
p
(cid:88)

k=1

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
p

p
(cid:88)

k=1

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

−

g(cid:48)kSλ

(cid:104)

q(cid:62)yk(cid:105)

1
p

p
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

33

p
(cid:88)

k=1

sup
q∈Sn−1
(cid:13)G − G(cid:48)(cid:13)
(cid:0)(cid:13)
(cid:32)

≤

≤

≤

1
p

1
p

1
p

(cid:12)
(cid:12)

(cid:13)
(cid:13)

(cid:12)q(cid:62)yk(cid:12)
(cid:13)gk − g(cid:48)k(cid:13)
(cid:12)
(cid:13)
(cid:12) +
(cid:13)2
(cid:13)(cid:96)2→(cid:96)1 + (cid:13)
(cid:13)Y(cid:13)
(cid:13)

(cid:13)(cid:96)2→(cid:96)∞

1
p

sup
q∈Sn−1

(cid:13)G(cid:48)(cid:13)

(cid:13)(cid:96)2→(cid:96)∞

(cid:12)
(cid:12)

(cid:12)q(cid:62) (cid:16)

yk − yk(cid:17)(cid:12)
(cid:12)
(cid:12)

p
(cid:88)

(cid:13)
(cid:13)

(cid:13)g(cid:48)k(cid:13)
(cid:13)
(cid:13)2
k=1
(cid:13)Y − Y(cid:13)
(cid:13)
√

(cid:1)

(cid:13)(cid:96)2→(cid:96)1
n, (cid:112)log(2p))

(cid:33)

120 max(n, log(2p))
√
p

+

300(cid:112)n log(2p) max(
θp

√

with probability at least 1 − c4p−2 provided p ≥ C4n and θ > 1/
obtain the claimed result (E.14).

≤

420(cid:112)n log(2p) max(
θ1/2p3/2

√

n, (cid:112)log(2p))

√

n. It is sufﬁcient to have p ≥ C5 (ξ) n4 log n to

APPENDIX F
LARGE |q1| ITERATES STAYING IN SAFE REGION FOR ROUNDING

In this appendix, we prove Proposition IV.5 in Section IV.

Proof of Proposition IV.5: For notational simplicity, w.l.o.g. we will proceed to prove assuming q1 > 0. The

proof for q1 < 0 is similar by symmetry. It is equivalent to show that

(cid:107)Q2 (q)(cid:107)2
|Q1 (q)|

<

(cid:114) 1
4θ

− 1,

which is implied by

for any q ∈ Sn−1 satisfying q1 > 3
(cid:115)

L (q)

.
=

(cid:13)
(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)

(cid:13)2 + (cid:13)
E (cid:2)Q1 (q)(cid:3) − (cid:12)
√
θ. Recall from (E.7) that

(cid:13)Q2(q) − E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)2
(cid:12)Q1 (q) − E (cid:2)Q1 (q)(cid:3)(cid:12)
(cid:12)

(cid:114) 1
4θ

<

− 1

E (cid:2)Q1(q)(cid:3) =

(cid:26)(cid:20)

(cid:16)

αΨ

−

(cid:17)

α
σ

θ
p

+ βΨ

(cid:19)(cid:21)

(cid:20)

+ σ

ψ

(cid:18) β
σ

(cid:19)

(cid:18) β
σ

(cid:16)

− ψ

−

(cid:17)(cid:21)(cid:27)

,

α
σ

where

α =

1
√
p

(cid:18) q1√
θ

(cid:19)

+ 1

,

β =

1
√
p

(cid:18) q1√
θ

(cid:19)

− 1

,

σ = (cid:107)q2(cid:107)2 /

√

p.

Noticing the fact that
(cid:18) β
σ

ψ

(cid:19)

(cid:16)

− ψ

≥ 0,

(cid:17)

(cid:19)

−

α
σ
(cid:18) β
σ

we have

Ψ

= Ψ

− 1

≥ Ψ (2) ≥

for q1 > 3

θ,

(cid:32)

1
(cid:112)1 − q2

1

(cid:18) q1√
θ

(cid:19)(cid:33)

19
20

√

E (cid:2)Q1 (q)(cid:3) ≥

√

θ
p

(cid:26) q1√
θ

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)

(cid:18) β
σ

+ Ψ

−

− Ψ

(cid:16)

(cid:17)

α
σ

(cid:19)(cid:27)

≥

(cid:18) β
σ

√
2

θ

p

(cid:19)

(cid:18) β
σ

Ψ

≥

√

19
10

θ
p

.

Moreover, from (E.8), we have

(cid:13)E (cid:2)Q2 (q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 = (cid:107)q2(cid:107)2

(cid:26) 2 (1 − θ)
p

(cid:18)

(cid:19)

Ψ

−

+

λ
σ

θ
p

(cid:20)

(cid:16)

Ψ

−

(cid:17)

α
σ

+ Ψ

(cid:19)(cid:21)(cid:27)

≤

2 (1 − θ)
p

θ
p

Ψ (−1) +

[Ψ (−1) + 1] ≤

Ψ (−1) +

≤

+

2
p

2
5p

θ
p

,

(cid:18) β
σ
θ
p

where we have used the fact that −λ/σ ≤ −1 and −α/σ ≤ −1. Moreover, from results in Proposition E.8 and
Proposition E.9 in Appendix E, we know that

sup
q∈Sn−1

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ sup
q∈Sn−1

(cid:12)
(cid:12)Q1(q) − Q1(q)(cid:12)

(cid:12) + sup
q∈Sn−1

(cid:12)Q1(q) − E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:12) ≤

1
2 × 105θ5/2n3/2p

,

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

34

sup
q∈Sn−1

(cid:13)Q(q) − E (cid:2)Q(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤ sup
q∈Sn−1

(cid:13)Q(q) − Q(q)(cid:13)
(cid:13)

(cid:13)2 + sup
q∈Sn−1

(cid:13)Q(q) − E (cid:2)Q(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 ≤

1
2 × 105θ2np

hold with probability at least 1 − c1p−2 provided that p ≥ Ω (cid:0)n4 log n(cid:1). Hence, with high probability, we have

L (q) ≤

2/(5p) + θ/p + (2 × 105θ2np)−1
√

θ/(10p) − (2 × 105θ5/2n3/2p)−1

19

≤

3/5
√

18

θ/10

≤

1
√
3

θ

<

(cid:114) 1
4θ

− 1,

whenever θ is sufﬁciently small. This completes the proof.

Now, keep the notation in Appendix E for general orthonormal basis (cid:98)Y = YU. For any current iterate q ∈ Sn−1
(cid:11)(cid:12)
(cid:12) = |(cid:104)Uq, e1(cid:105)| ≥ 3
(cid:69)(cid:12)
(cid:12)
(cid:12)

that is close enough to the target solution, i.e., (cid:12)
(cid:12)
(cid:12)
(cid:68)
(cid:12)
(cid:12)

(cid:10)q, U(cid:62)e1
(cid:12)
(cid:16)
(cid:68)
(cid:12)
(cid:12)

θ, we have

(cid:69)(cid:12)
(cid:12)
(cid:12)

√

(cid:17)

(cid:16)

(cid:17)

q; (cid:98)Y
(cid:16)

Q
(cid:13)
(cid:13)
(cid:13)Q

q; (cid:98)Y

, U(cid:62)e1
(cid:17)(cid:13)
(cid:13)
(cid:13)2

=

UQ
(cid:13)
(cid:13)
(cid:13)UQ

q; (cid:98)Y
(cid:16)

q; (cid:98)Y

, e1
(cid:17)(cid:13)
(cid:13)
(cid:13)2

=

|(cid:104)Q (Uq; Y) , e1(cid:105)|
(cid:107)Q (Uq; Y)(cid:107)2

,

where we have applied the identity proved in (E.2). Taking Uq ∈ Sn−1 as the object of interest, by Proposition IV.5,
we conclude that

with high probability.

|(cid:104)Q (Uq; Y) , e1(cid:105)|
(cid:107)Q (Uq; Y)(cid:107)2

√

≥ 2

θ

APPENDIX G
BOUNDING ITERATION COMPLEXITY

In this appendix, we prove Proposition IV.6 in Section IV.

Proof of Proposition IV.6: Recall from Proposition IV.4 in Section IV, the gap

G(q) =

|Q1(q)|
|q1|

−

(cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2

≥

1
104θ2np

√

holds uniformly over q ∈ Sn−1 satisfying
p ≥ C2n4 log n. The gap G(q) implies that

1
√

10

θn

≤ |q1| ≤ 3

θ, with probability at least 1 − c1p−2, provided

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)

.
=

⇐⇒

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12) ≥

|Q1(q)|
(cid:107)Q (q)(cid:107)2
(cid:114)
|q1|
(cid:107)q2(cid:107)2

(cid:32)

≥

1 −

=⇒

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)

≥ |q1|2

1 +

+

|q1|
104θ2np (cid:107)Q (q)(cid:107)2
|q1|
104θ2np (cid:107)Q (q)(cid:107)2
(cid:33)

|q1| (cid:107)Q2(q)(cid:107)2
(cid:107)q(cid:107)2 (cid:107)Q (q)(cid:107)2
(cid:12)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)
(cid:107)q2(cid:107)2
2
108θ4n2p2 (cid:107)Q (q)(cid:107)2
2

+

.

Given the set Γ deﬁned in (IV.7), now we know that

sup
q∈Γ

(cid:107)Q (q)(cid:107)2 ≤ sup
q∈Γ

(cid:12)E (cid:2)Q1(q)(cid:3) − Q1 (q)(cid:12)
(cid:12)

(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

(cid:13)
(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)

+ sup
q∈Γ
(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)
(cid:12)

≤ sup
q∈Γ

(cid:12) + sup
q∈Sn−1
(cid:13)2 + sup
q∈Sn−1
(cid:12)E (cid:2)Q2(q)(cid:3)(cid:12)
(cid:12)

(cid:12) + sup
q∈Γ

(cid:12) +

1
pn

(cid:13)E (cid:2)Q2(q)(cid:3) − Q2 (q)(cid:13)
(cid:13)

(cid:12) + sup
q∈Sn−1
(cid:13)2 + sup
q∈Sn−1

(cid:12)Q1(q) − Q1 (q)(cid:12)
(cid:12)
(cid:12)
(cid:13)Q2(q) − Q2 (q)(cid:13)
(cid:13)
(cid:13)2

with probability at least 1 − c3p−2 provided p ≥ C4n4 log n and θ > 1/
n. Here we have used Proposition E.8 and
Proposition E.9 to bound the magnitudes of the four difference terms. To bound the magnitudes of the expectations,
we have

(cid:12)
(cid:12)E (cid:2)Q1(q)(cid:3)(cid:12)

(cid:12) =

E

(cid:104)

x0(k)Sλ

x0(k)q1 + q(cid:62)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

1
p

p
(cid:88)

k=1

2 gk(cid:105)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
√
θp

(cid:18) 1
√
θp

(cid:19)

+ E [(cid:107)g(cid:107)2]

≤

√
3
√

n
θp

≤

3n
p

,

√

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

35

(cid:13)E (cid:2)Q2(q)(cid:3)(cid:13)
(cid:13)

(cid:13)2 =

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

1
p

p
(cid:88)

k=1

(cid:104)

gkSλ

x0(k)q1 + q(cid:62)

2 gk(cid:105)

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
n. Thus, we obtain that

1
√
θp

≤

√

hold uniformly for all q ∈ Γ, provided θ > 1/

E [(cid:107)g(cid:107)2] + E

(cid:107)g(cid:107)2
2

(cid:104)

(cid:105)

≤

3n
p

with probability at least 1 − c3p−2 provided p ≥ C4n4 log n and θ > 1/

n. So we conclude that

(cid:107)Q (q)(cid:107)2 ≤

sup
q∈Γ

3n
p

+

+

≤

3n
p

1
np

7n
p

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Q1 (q)
(cid:12)
|q1|

(cid:114)

≥

1 +

1 − 9θ
108 × 72 × θ4n4 .
, we will need at most

Thus, starting with any q ∈ Sn−1 such that |q1| ≥ 1
√
10
√

θn

(cid:16)

3

(cid:17)

θ/

1
√

10

θn

T =

2 log
(cid:16)

log

1 +

1−9θ
108×72×θ4n4

(cid:17) =

log
√

√

n)

2 log (30θ
(cid:16)

1 +

1−9θ
108×72×θ4n4

(cid:17) ≤

√

2 log (30θ

n)

(log 2)

1−9θ
108×72×θ4n4

≤ C5n4 log n

steps to arrive at a q ∈ Sn−1 with | ¯q1| ≥ 3
that log (1 + x) ≥ x log 2 for x ∈ [0, 1] to simplify the ﬁnal result.

θ for the ﬁrst time. Here we have assumed θ0 < 1/9 and used the fact

APPENDIX H
ROUNDING TO THE DESIRED SOLUTION

min
q

(cid:107)Yq(cid:107)1 ,

s.t. (cid:104)q, q(cid:105) = 1.

In this appendix, we prove Proposition IV.7 in Section IV. For convenience, we will assume the notations we

used in Appendix B. Then the rounding scheme can be written as

(H.1)

(H.2)

(H.3)

We will show the rounding procedure get us to the desired solution with high probability, regardless of the particular
orthonormal basis used.

Proof of Proposition IV.7: The rounding program (H.1) can be written as

Consider its relaxation

inf
q

(cid:107)Yq(cid:107)1 ,

s.t. q1q1 + (cid:104)q2, q2(cid:105) = 1.

inf
q

(cid:107)Yq(cid:107)1 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1.

It is obvious that the feasible set of (H.3) contains that of (H.2). So if e1/q1 is the unique optimal solution (UOS)
of (H.3), it is also the UOS of (H.2). Let I = supp(x0), and consider a modiﬁed problem

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

|q1| − (cid:13)

(cid:13)G(cid:48)

Iq2

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

I cq2

(cid:13)
(cid:13)1 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1.

(H.4)

The objective value of (H.4) lower bounds the objective value of (H.3), and are equal when q = e1/q1. So if
q = e1/q1 is the UOS to (H.4), it is also UOS to (H.3), and hence UOS to (H.2) by the argument above. Now
(cid:13)1 ≥ − (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)
(cid:13)
(cid:0)G − G(cid:48)(cid:1) q2
(cid:13)
(cid:13)G − G(cid:48)(cid:13)
≥ − (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

(cid:13)
(cid:13)1
(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2 .

(cid:13)1 + (cid:13)
(cid:13)

(cid:13)G(cid:48)

(cid:13)G(cid:48)

I cq2

− (cid:13)

Iq2

When p ≥ C1n, by Lemma A.14 and Lemma B.3, we know that

− (cid:107)GIq2(cid:107)1 + (cid:107)GI cq2(cid:107)1 − (cid:13)

≥ −

(cid:114) 2
π

6
5

√

2θ

(cid:13)G − G(cid:48)(cid:13)
(cid:13)(cid:96)2→(cid:96)1 (cid:107)q2(cid:107)2
(cid:114) 2
24
π
25

p (cid:107)q2(cid:107)2 +

√

(1 − 2θ)

p (cid:107)q2(cid:107)2 − 4

√

n (cid:107)q2(cid:107)2 − 7(cid:112)log(2p) (cid:107)q2(cid:107)2

.
= ζ (cid:107)q2(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

holds with probability at least 1 − c2p−2. Thus, we make a further relaxation of problem (H.2) by

|q1| + ζ (cid:107)q2(cid:107)2 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 ≥ 1,

36

(H.5)

whose objective value lower bounds that of (H.4). By similar arguments, if e1/q1 is UOS to (H.5), it is UOS to (H.2).
At the optimal solution to (H.5), notice that it is necessary to have sign(q1) = sign(q1) and q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 = 1.
So (H.5) is equivalent to

|q1| + ζ (cid:107)q2(cid:107)2 ,

s.t. q1q1 + (cid:107)q2(cid:107)2 (cid:107)q2(cid:107)2 = 1.

(H.6)

which is further equivalent to

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

inf
q1

|q1| + ζ

x0
(cid:107)x0(cid:107)2

1 − |q1| |q1|
(cid:107)q2(cid:107)2
Notice that the problem in (H.7) is linear in |q1| with a compact feasible set. Since the objective is also monotonic
in |q1|, it indicates that the optimal solution only occurs at the boundary points |q1| = 0 or |q1| = 1/ |q1| Therefore,
q = e1/q1 is the UOS of (H.7) if and only if
1
|q1|

1
|q1|

ζ
(cid:107)q2(cid:107)2

x0
(cid:107)x0(cid:107)2

|q1| ≤

(H.7)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

s.t.

<

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

.

.

,

Since

(cid:13)
(cid:13)
(cid:13)

x0
(cid:107)x0(cid:107)2

(cid:13)
(cid:13)
(cid:13)1

√

≤

2θp conditioned on E0, it is sufﬁcient to have

√

2θp
√
θ
2

≤ ζ =

(cid:32)

(cid:114) 2
π

24
25

√

p

1 −

θ −

9
2

(cid:114) π
2

(cid:114) n
p

25
6

−

175
24

(cid:114) π
2

(cid:115)

(cid:33)

.

log(2p)
p

Therefore there exists a constant θ0 > 0, such that whenever θ ≤ θ0 and p ≥ C3(θ0)n, the rounding returns e1/q1.
A bit of thought suggests one can take a universal C3 for all possible choice of θ0, completing the proof.

When the input basis is (cid:98)Y = YU for some orthogonal matrix U (cid:54)= I, if the ADM algorithm produces some

√

q = U(cid:62)q(cid:48), such that q(cid:48)

1 > 2

θ. It is not hard to see that now the rounding (H.1) is equivalent to

min
q

(cid:107)YUq(cid:107)1 ,

s.t. (cid:10)q(cid:48), Uq(cid:11) = 1.

Renaming Uq, it follows from the above argument that at optimum q(cid:63) it holds that Uq(cid:63) = γe1 for some constant
γ with high probability.

REFERENCES

[1] Q. Qu, J. Sun, and J. Wright, “Finding a sparse vector in a subspace: Linear sparsity using alternating directions,”

in Advances in Neural Information Processing Systems, 2014.

[2] E. J. Cand`es and T. Tao, “Decoding by linear programming,” Information Theory, IEEE Transactions on,

vol. 51, no. 12, pp. 4203–4215, 2005.

[3] D. L. Donoho, “For most large underdetermined systems of linear equations the minimal (cid:96)1-norm solution is
also the sparsest solution,” Communications on pure and applied mathematics, vol. 59, no. 6, pp. 797–829,
2006.

[4] S. T. McCormick, “A combinatorial approach to some sparse matrix problems.,” tech. rep., DTIC Document,

1983.

[5] T. F. Coleman and A. Pothen, “The null space problem i. complexity,” SIAM Journal on Algebraic Discrete

Methods, vol. 7, no. 4, pp. 527–537, 1986.

[6] M. Berry, M. Heath, I. Kaneko, M. Lawo, R. Plemmons, and R. Ward, “An algorithm to compute a sparse

basis of the null space,” Numerische Mathematik, vol. 47, no. 4, pp. 483–504, 1985.

[7] J. R. Gilbert and M. T. Heath, “Computing a sparse basis for the null space,” SIAM Journal on Algebraic

Discrete Methods, vol. 8, no. 3, pp. 446–459, 1987.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

37

[8] I. S. Duff, A. M. Erisman, and J. K. Reid, Direct Methods for Sparse Matrices. New York, NY, USA: Oxford

[9] A. J. Smola and B. Schlkopf, “Sparse greedy matrix approximation for machine learning,” pp. 911–918, Morgan

University Press, Inc., 1986.

Kaufmann, 2000.

[10] T. Kavitha, K. Mehlhorn, D. Michail, and K. Paluch, “A faster algorithm for minimum cycle basis of graphs,”

in 31st International Colloquium on Automata, Languages and Programming, pp. 846–857, Springer, 2004.

[11] L.-A. Gottlieb and T. Neylon, “Matrix sparsiﬁcation and the sparse null space problem,” in Approximation,

Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 205–218, Springer, 2010.

[12] J. Mairal, F. Bach, and J. Ponce, “Sparse modeling for image and vision processing,” arXiv preprint

arXiv:1411.3230, 2014.

[13] D. A. Spielman, H. Wang, and J. Wright, “Exact recovery of sparsely-used dictionaries,” in Proceedings of the

25th Annual Conference on Learning Theory, 2012.

[14] P. Hand and L. Demanet, “Recovering the sparsest element in a subspace,” arXiv preprint arXiv:1310.1654,

[15] J. Sun, Q. Qu, and J. Wright, “Complete dictionary recovery over the sphere,” arXiv preprint arXiv:1504.06785,

[16] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” Journal of computational and

graphical statistics, vol. 15, no. 2, pp. 265–286, 2006.

[17] I. M. Johnstone and A. Y. Lu, “On consistency and sparsity for principal components analysis in high

dimensions,” Journal of the American Statistical Association, vol. 104, no. 486, 2009.

[18] A. d’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. Lanckriet, “A direct formulation for sparse pca using

semideﬁnite programming,” SIAM review, vol. 49, no. 3, pp. 434–448, 2007.

[19] R. Krauthgamer, B. Nadler, D. Vilenchik, et al., “Do semideﬁnite relaxations solve sparse PCA up to the

information limit?,” The Annals of Statistics, vol. 43, no. 3, pp. 1300–1322, 2015.

[20] T. Ma and A. Wigderson, “Sum-of-squares lower bounds for sparse pca,” arXiv preprint arXiv:1507.06370,

2013.

2015.

2015.

[21] V. Q. Vu, J. Cho, J. Lei, and K. Rohe, “Fantope projection and selection: A near-optimal convex relaxation of

sparse pca,” in Advances in Neural Information Processing Systems, pp. 2670–2678, 2013.

[22] J. Lei, V. Q. Vu, et al., “Sparsistency and agnostic inference in sparse pca,” The Annals of Statistics, vol. 43,

[23] Z. Wang, H. Lu, and H. Liu, “Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial

no. 1, pp. 299–322, 2015.

time,” arXiv preprint arXiv:1408.5352, 2014.

[24] A. d’Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet, “A direct formulation of sparse PCA using

semideﬁnite programming,” SIAM Review, vol. 49, no. 3, 2007.

[25] Y.-B. Zhao and M. Fukushima, “Rank-one solutions for homogeneous linear matrix equations over the positive

semideﬁnite cone,” Applied Mathematics and Computation, vol. 219, no. 10, pp. 5569–5583, 2013.

[26] Y. Dai, H. Li, and M. He, “A simple prior-free method for non-rigid structure-from-motion factorization,” in
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2018–2025, IEEE, 2012.
[27] G. Beylkin and L. Monz´on, “On approximation of functions by exponential sums,” Applied and Computational

Harmonic Analysis, vol. 19, no. 1, pp. 17–48, 2005.

[28] C. T. Manolis and V. Rene, “Dual principal component pursuit,” arXiv preprint arXiv:1510.04390, 2015.
[29] M. Zibulevsky and B. A. Pearlmutter, “Blind source separation by sparse decomposition in a signal dictionary,”

Neural computation, vol. 13, no. 4, pp. 863–882, 2001.

[30] A. Anandkumar, D. Hsu, M. Janzamin, and S. M. Kakade, “When are overcomplete topic models identiﬁable?
uniqueness of tensor tucker decompositions with structured sparsity,” in Advances in Neural Information
Processing Systems, pp. 1986–1994, 2013.

[31] J. Ho, Y. Xie, and B. Vemuri, “On a nonlinear generalization of sparse coding and dictionary learning,” in

Proceedings of The 30th International Conference on Machine Learning, pp. 1480–1488, 2013.

[32] Y. Nakatsukasa, T. Soma, and A. Uschmajew, “Finding a low-rank basis in a matrix subspace,” CoRR,

vol. abs/1503.08601, 2015.

[33] Q. Berthet and P. Rigollet, “Complexity theoretic lower bounds for sparse principal component detection,” in

Conference on Learning Theory, pp. 1046–1066, 2013.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

38

[34] B. Barak, J. Kelner, and D. Steurer, “Rounding sum-of-squares relaxations,” arXiv preprint arXiv:1312.6652,

2013.

[35] S. B. Hopkins, T. Schramm, J. Shi, and D. Steurer, “Speeding up sum-of-squares for tensor decomposition and

planted sparse vectors,” arXiv preprint arXiv:1512.02337, 2015.

[36] S. Arora, R. Ge, and A. Moitra, “New algorithms for learning incoherent and overcomplete dictionaries,” arXiv

[37] A. Agarwal, A. Anandkumar, and P. Netrapalli, “Exact recovery of sparsely used overcomplete dictionaries,”

preprint arXiv:1308.6273, 2013.

arXiv preprint arXiv:1309.1952, 2013.

[38] A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon, “Learning sparsely used overcomplete

dictionaries via alternating minimization,” arXiv preprint arXiv:1310.7991, 2013.

[39] S. Arora, A. Bhaskara, R. Ge, and T. Ma, “More algorithms for provable dictionary learning,” arXiv preprint

[40] S. Arora, R. Ge, T. Ma, and A. Moitra, “Simple, efﬁcient, and neural algorithms for sparse coding,” arXiv

arXiv:1401.0579, 2014.

preprint arXiv:1503.00778, 2015.

[41] K. G. Murty and S. N. Kabadi, “Some NP-complete problems in quadratic and nonlinear programming,”

Mathematical programming, vol. 39, no. 2, pp. 117–129, 1987.

[42] R. Vershynin, “Introduction to the non-asymptotic analysis of random matrices,” arXiv preprint arXiv:1011.3027,

2010.

May 2011.

[43] R. Basri and D. W. Jacobs, “Lambertian reﬂectance and linear subspaces,” Pattern Analysis and Machine

Intelligence, IEEE Transactions on, vol. 25, no. 2, pp. 218–233, 2003.

[44] E. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal component analysis?,” Journal of the ACM, vol. 58,

[45] V. De la Pena and E. Gin´e, Decoupling: from dependence to independence. Springer, 1999.
[46] M. Talagrand, Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems,

vol. 60. Springer Science & Business Media, 2014.

[47] K. Luh and V. Vu, “Dictionary learning with few samples and matrix concentration,” arXiv preprint

arXiv:1503.08854, 2015.

[48] P. Jain, P. Netrapalli, and S. Sanghavi, “Low-rank matrix completion using alternating minimization,” in
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pp. 665–674, ACM,
2013.

[49] M. Hardt, “On the provable convergence of alternating minimization for matrix completion,” arXiv preprint

arXiv:1312.0925, 2013.

[50] M. Hardt and M. Wootters, “Fast matrix completion without the condition number,” in Proceedings of The

27th Conference on Learning Theory, pp. 638–678, 2014.

[51] M. Hardt, “Understanding alternating minimization for matrix completion,” in Foundations of Computer Science

(FOCS), 2014 IEEE 55th Annual Symposium on, pp. 651–660, IEEE, 2014.

[52] P. Jain and P. Netrapalli, “Fast exact matrix completion with ﬁnite samples,” arXiv preprint arXiv:1411.1087,

2014.

[53] P. Netrapalli, U. Niranjan, S. Sanghavi, A. Anandkumar, and P. Jain, “Non-convex robust pca,” in Advances in

Neural Information Processing Systems, pp. 1107–1115, 2014.

[54] Q. Zheng and J. Lafferty, “A convergent gradient descent algorithm for rank minimization and semideﬁnite

programming from random linear measurements,” arXiv preprint arXiv:1506.06081, 2015.

[55] S. Tu, R. Boczar, M. Soltanolkotabi, and B. Recht, “Low-rank solutions of linear matrix equations via procrustes

ﬂow,” arXiv preprint arXiv:1507.03566, 2015.

[56] Y. Chen and M. J. Wainwright, “Fast low-rank estimation by projected gradient descent: General statistical

and algorithmic guarantees,” arXiv preprint arXiv:1509.03025, 2015.

[57] P. Jain and S. Oh, “Provable tensor factorization with missing data,” in Advances in Neural Information

Processing Systems, pp. 1431–1439, 2014.

[58] A. Anandkumar, R. Ge, and M. Janzamin, “Guaranteed non-orthogonal tensor decomposition via alternating

rank-1 updates,” arXiv preprint arXiv:1402.5180, 2014.

[59] A. Anandkumar, R. Ge, and M. Janzamin, “Analyzing tensor power method dynamics: Applications to learning

overcomplete latent variable models,” arXiv preprint arXiv:1411.1488, 2014.

IEEE TRANSACTION ON INFORMATION THEORY, VOL. XX, NO. XX, XXXX 2015

39

[60] A. Anandkumar, P. Jain, Y. Shi, and U. Niranjan, “Tensor vs matrix methods: Robust tensor decomposition

under block sparse perturbations,” arXiv preprint arXiv:1510.04747, 2015.

[61] R. Ge, F. Huang, C. Jin, and Y. Yuan, “Escaping from saddle points—online stochastic gradient for tensor

decomposition,” in Proceedings of The 28th Conference on Learning Theory, pp. 797–842, 2015.

[62] P. Netrapalli, P. Jain, and S. Sanghavi, “Phase retrieval using alternating minimization,” in Advances in Neural

Information Processing Systems, pp. 2796–2804, 2013.

[63] E. J. Cand`es, X. Li, and M. Soltanolkotabi, “Phase retrieval via wirtinger ﬂow: Theory and algorithms,” arXiv

preprint arXiv:1407.1065, 2014.

[64] Y. Chen and E. J. Candes, “Solving random quadratic systems of equations is nearly as easy as solving linear

systems,” arXiv preprint arXiv:1505.05114, 2015.

[65] J. Sun, Q. Qu, and J. Wright, “A geometric analysis of phase retreival,” arXiv preprint arXiv:1602.06664,

[66] J. Sun, Q. Qu, and J. Wright, “When are nonconvex problems not scary?,” arXiv preprint arXiv:1510.06096,

[67] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive Sensing. Springer, 2013.
[68] T. Figiel, J. Lindenstrauss, and V. D. Milman, “The dimension of almost spherical sections of convex bodies,”

Acta Mathematica, vol. 139, no. 1, pp. 53–94, 1977.

[69] A. Y. Garnaev and E. D. Gluskin, “The widths of a euclidean ball,” in Dokl. Akad. Nauk SSSR, vol. 277,

pp. 1048–1052, 1984.

[70] E. Gluskin and V. Milman, “Note on the geometric-arithmetic mean inequality,” in Geometric aspects of

Functional analysis, pp. 131–135, Springer, 2003.

[71] G. Pisier, The volume of convex bodies and Banach space geometry, vol. 94. Cambridge University Press,

2016.

2015.

1999.

