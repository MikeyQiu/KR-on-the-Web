Syllable-aware Neural Language Models:
A Failure to Beat Character-aware Ones

Zhenisbek Assylbekov
School of Science and Technology
Nazarbayev University
zhassylbekov@nu.edu.kz

Rustem Takhanov
School of Science and Technology
Nazarbayev University
rustem.takhanov@nu.edu.kz

Bagdat Myrzakhmetov
National Laboratory Astana
Nazarbayev University
bagdat.myrzakhmetov@nu.edu.kz

Jonathan N. Washington
Linguistics Department
Swarthmore College
jonathan.washington
@swarthmore.edu

7
1
0
2
 
l
u
J
 
0
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
8
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Syllabiﬁcation does not seem to im-
prove word-level RNN language model-
ing quality when compared to character-
based segmentation. However, our best
syllable-aware language model, achieving
performance comparable to the competi-
tive character-aware model, has 18%–33%
fewer parameters and is trained 1.2–2.2
times faster.

1

Introduction

Recent advances in neural
language modeling
(NLM) are connected with character-aware mod-
els (Kim et al., 2016; Ling et al., 2015b; Verwimp
et al., 2017). This is a promising approach, and
we propose the following direction related to it:
We would like to make sure that in the pursuit of
the most ﬁne-grained representations one has not
missed possible intermediate ways of segmenta-
tion, e.g., by syllables. Syllables, in our opinion,
are better supported as linguistic units of language
than single characters. In most languages, words
can be naturally split into syllables:

ES: el par-la-men-to a-po-y´o la en-mien-da
RU: par-la-ment pod-der-ˇzal po-prav-ku
(EN: the parliament supported the amendment)

Based on this observation, we attempted to de-
termine whether syllable-aware NLM has any ad-
vantages over character-aware NLM. We exper-
imented with a variety of models but could not
ﬁnd any evidence to support this hypothesis: split-
ting words into syllables does not seem to improve
the language modeling quality when compared to
splitting into characters. However, there are some
positive ﬁndings: while our best syllable-aware

language model achieves performance comparable
to the competitive character-aware model, it has
18%–33% fewer parameters and is 1.2–2.2 times
faster to train.

2 Related Work

Much research has been done on subword-level
and subword-aware1 neural language modeling
when subwords are characters (Ling et al., 2015b;
Kim et al., 2016; Verwimp et al., 2017) or mor-
phemes (Botha and Blunsom, 2014; Qiu et al.,
2014; Cotterell and Sch¨utze, 2015). However,
not much work has been done on syllable-level or
syllable-aware NLM. Mikolov et al. (2012) show
that subword-level language models outperform
character-level ones.2 They keep the most fre-
quent words untouched and split all other words
into syllable-like units. Our approach differs
mainly in the following aspects: we make predic-
tions at the word level, use a more linguistically
sound syllabiﬁcation algorithm, and consider a va-
riety of more advanced neural architectures.

We have recently come across a concurrent pa-
per (Vania and Lopez, 2017) where the authors
systematically compare different subword units
(characters, character trigrams, BPE (Sennrich
et al., 2016), morphemes) and different represen-
tation models (CNN, Bi-LSTM, summation) on
languages with various morphological typology.
However, they do not consider syllables, and they
experiment with relatively small models on small
data sets (0.6M–1.4M tokens).

1Subword-level LMs rely on subword-level inputs and
make predictions at the level of subwords; subword-aware
LMs also rely on subword-level inputs but make predictions
at the level of words.

2Not to be confused with character-aware ones, see the

previous footnote.

unconstitutional conditions on

Highway layers (optional)

Syllable-aware word embedding model

stack of two
LSTMs

word vector

Syllable
embeddings

un con sti

tu tional

imposes unconstitutional conditions

Figure 1: Syllable-aware language model.

3 Syllable-aware word embeddings

Let W and S be ﬁnite vocabularies of words and
syllables respectively. We assume that both words
and syllables have already been converted into in-
dices. Let ES ∈ R|S|×dS be an embedding ma-
trix for syllables — i.e., it is a matrix in which the
sth row (denoted as s) corresponds to an embed-
ding of the syllable s ∈ S. Any word w ∈ W is
a sequence of its syllables (s1, s2, . . . , snw ), and
hence can be represented as a sequence of the cor-
responding syllable vectors:

[s1, s2, . . . , snw ].

(1)

The question is: How shall we pack the sequence
(1) into a single vector x ∈ RdW to produce a
better embedding of the word w?3
In our case
“better” means “better than a character-aware em-
bedding of w via the Char-CNN model of Kim
et al. (2016)”. Below we present several viable
approaches.

3.1 Recurrent sequential model (Syl-LSTM)

Since the syllables are coming in a sequence it is
natural to try a recurrent sequential model:

ht = f (st, ht−1),

h0 = 0,

(2)

which converts the sequence of syllable vectors (1)
into a sequence of state vectors h1:nw . The last

3The same question applies to any model that segments

words into a sequence of characters or other subword units.

state vector hnw is assumed to contain the infor-
mation on the whole sequence (1), and is there-
fore used as a word embedding for w. There is
a big variety of transformations from which one
can choose f in (2); however, a recent thorough
evaluation (Jozefowicz et al., 2015) shows that the
LSTM (Hochreiter and Schmidhuber, 1997) with
its forget bias initialized to 1 outperforms other
popular architectures on almost all tasks, and we
decided to use it for our experiments. We will re-
fer to this model as Syl-LSTM.

3.2 Convolutional model (Syl-CNN)

Inspired by recent work on character-aware neural
language models (Kim et al., 2016) we decided to
try this approach (Char-CNN) on syllables. Our
case differs mainly in the following two aspects:

1. The set of syllables S is usually bigger than
the set of characters C,4 and also the dimen-
sionality dS of syllable vectors is expected to
be greater than the dimensionality dC of char-
acter vectors. Both of these factors result in
allocating more parameters on syllable em-
beddings compared to character embeddings.
2. On average a word contains fewer syllables
than characters, and therefore we need nar-
rower convolutional ﬁlters for syllables. This
results in spending fewer parameters per con-
volution.

This means that by varying dS and the maximum
width of convolutional ﬁlters L we can still ﬁt the
parameter budget of Kim et al. (2016) to allow fair
comparison of the models.

Like in Char-CNN, our syllable-aware model,
which is referred to as Syl-CNN-[L], utilizes max-
pooling and highway layers (Srivastava et al.,
2015) to model interactions between the syllables.
The dimensionality of a highway layer is denoted
by dHW.

3.3 Linear combinations

We also considered using linear combinations of
syllable-vectors to represent the word embedding:

x = (cid:80)nw

t=1 αt(st) · st.

(3)

The choice for αt is motivated mainly by the ex-
isting approaches (discussed below) which proved
to be successful for other tasks.
Syl-Sum: Summing up syllable vectors to get a
word vector can be obtained by setting αt(st) = 1.

4In languages with alphabetic writing systems.

This approach was used by Botha and Blunsom
(2014) to combine a word and its morpheme em-
beddings into a single word vector.
Syl-Avg: A simple average of syllable vectors can
be obtained by setting αt(st) = 1/nw. This can
be also called a “continuous bag of syllables” in an
analogy to a CBOW model (Mikolov et al., 2013),
where vectors of neighboring words are averaged
to get a word embedding of the current word.
Syl-Avg-A: We let the weights αt in (3) be a
function of parameters (a1, . . . , an) of the model,
which are jointly trained together with other pa-
rameters. Here n = maxw{nw} is a maxi-
mum word length in syllables.
In order to have
a weighted average in (3) we apply a softmax nor-
malization:

αt = softmax(a)t =

exp(at)
τ =1 exp(aτ )

(cid:80)n

(4)

Syl-Avg-B: We can let αt depend on syllables and
their positions:

αt = αt(st) = softmax(ast + b)t
where A ∈ RdS ×n (with elements as,t) is a set of
parameters that determine the importance of each
syllable type in each (relative) position, b ∈ Rn
is a bias, which is conditioned only on the rela-
tive position. This approach is motivated by re-
cent work on using an attention mechanism in the
CBOW model (Ling et al., 2015a).

We feed the resulting x from (3) into a stack of
highway layers to allow interactions between the
syllables.

3.4 Concatenation (Syl-Concat)

In this model we simply concatenate syllable vec-
tors (1) into a single word vector:

x = [s1; s2; . . . ; snw ; 0; 0; . . . ; 0
]
(cid:125)

(cid:124)

(cid:123)(cid:122)
n−nw

We zero-pad x so that all word vectors have the
same length n · dS to allow batch processing, and
then we feed x into a stack of highway layers.

4 Word-level language model

Once we have word embeddings x1:k for a se-
quence of words w1:k we can use a word-level
RNN language model to produce a sequence of
states h1:k and then predict the next word accord-
ing to the probability distribution

where W ∈ RdLM×|W|, b ∈ R|W|, and dLM is the
hidden layer size of the RNN. Training the model
involves minimizing the negative log-likelihood
over the corpus w1:K:
− (cid:80)K

k=1 log Pr(wk|w1:k−1) −→ min

(5)

As was mentioned in Section 3.1 there is a huge
variety of RNN architectures to choose from. The
most advanced recurrent neural architectures, at
the time of this writing, are recurrent highway
networks (Zilly et al., 2017) and a novel model
which was obtained through a neural architecture
search with reinforcement learning (Zoph and Le,
2017). These models can be spiced up with the
most recent regularization techniques for RNNs
(Gal and Ghahramani, 2016) to reach state-of-the-
art. However, to make our results directly com-
parable to those of Kim et al. (2016) we select a
two-layer LSTM and regularize it as in Zaremba
et al. (2014).

5 Experimental Setup

We search for the best model in two steps: ﬁrst,
we block the word-level LSTM’s architecture and
pre-select the three best models under a small pa-
rameter budget (5M), and then we tune these three
best models’ hyperparameters under a larger bud-
get (20M).
Pre-selection: We ﬁx dLM (hidden layer size of
the word-level LSTM) at 300 units per layer and
run each syllable-aware word embedding method
from Section 3 on the English PTB data set (Mar-
cus et al., 1993), keeping the total parameter bud-
get at 5M. The architectural choices are speciﬁed
in Appendix A.
Hyperparameter tuning: The hyperparameters
of the three best-performing models from the pre-
selection step are then thoroughly tuned on the
same English PTB data through a random search
according to the marginal distributions:

• dS ∼ U (20, 650),5
• log(dHW) ∼ U (log(160), log(2000)),
• log(dLM) ∼ U (log(300), log(2000)),

with the restriction dS < dLM. The total parameter
budget is kept at 20M to allow for easy comparison
to the results of Kim et al. (2016). Then these three
best models (with their hyperparameters tuned on
PTB) are trained and evaluated on small- (DATA-
S) and medium-sized (DATA-L) data sets in six
languages.

Pr(wk+1|w1:k) = softmax(hkW + b),

5U (a, b) stands for a uniform distribution over (a, b).

Model
LSTM-Word
Syl-LSTM
Syl-CNN-2
Syl-CNN-3
Syl-CNN-4
Syl-Sum

PPL Model
88.0
88.7
86.6
84.6
86.8
84.6

Char-CNN
Syl-Avg
Syl-Avg-A
Syl-Avg-B
Syl-Concat

PPL
92.3
88.5
91.4
88.5
83.7

Table 1: Pre-selection results. PPL stands for test
set perplexity, all models have ≈ 5M parameters.

Model
Syl-CNN
Syl-Sum
Syl-Concat

dS
242
438
228

dHW
1170
1256
781

dLM
380
435
439

Size
PPL
15M 80.5
18M 80.3
13M 79.4

Table 2: Hyperparameters tuning.
In Syl-CNN,
dHW is a function of the primary hyperparameter
c = 195 (see Appendix A).

Optimizaton is performed in almost the same way
as in the work of Zaremba et al. (2014). See Ap-
pendix B for details.
Syllabiﬁcation: The true syllabiﬁcation of a word
requires its grapheme-to-phoneme conversion and
then splitting it into syllables based on some rules.
Since these are not always available for less-
resourced languages, we decided to utilize Liang’s
widely-used hyphenation algorithm (Liang, 1983).

6 Results

The results of the pre-selection are reported in
Table 1. All syllable-aware models comfortably
outperform the Char-CNN when the budget is
limited to 5M parameters. Surprisingly, a pure
word-level model,6 LSTM-Word, also beats the
character-aware one under such budget. The three
best conﬁgurations are Syl-Concat, Syl-Sum, and
Syl-CNN-3 (hereinafter referred to as Syl-CNN),
and tuning their hyperparameters under 20M pa-
rameter budget gives the architectures in Table
2. The results of evaluating these three models
on small (1M tokens) and medium-sized (17M–
57M tokens) data sets against Char-CNN for dif-
The
ferent languages are provided in Table 3.
models demonstrate similar performance on small
data, but Char-CNN scales signiﬁcantly better on
medium-sized data. From the three syllable-aware
models, Syl-Concat looks the most advantageous
as it demonstrates stable results and has the least

6When words are directly embedded into RdW through an

embedding matrix EW ∈ R|W|×dW .

7Syl-CNN results on DATA-L are not reported since com-
putational resources were insufﬁcient to run these conﬁgura-
tions.

Model
EN
Char-CNN 78.9
80.5
Syl-CNN
80.3
Syl-Sum
Syl-Concat
79.4
Char-CNN 160
Syl-CNN7
–
170
Syl-Sum
176
Syl-Concat

FR
184
191
193
188
124
–
141
139

ES DE CS RU
261
165
269
172
273
170
265
168
190
118
–
–
233
129
225
129

239
239
243
244
198
–
212
225

371
374
389
383
392
–
451
449

S
-
A
T
A
D

L
-
A
T
A
D

Table 3: Evaluation of the syllable-aware mod-
els against Char-CNN. In each case the smallest
model, Syl-Concat, has 18%–33% less parameters
than Char-CNN and is trained 1.2–2.2 times faster
(Appendix C).

number of parameters. Therefore in what follows
we will make a more detailed comparison of Syl-
Concat with Char-CNN.
Shared errors: It is interesting to see whether
Char-CNN and Syl-Concat are making similar er-
rors. We say that a model gives an error if it as-
signs a probability less than p∗ to a correct word
from the test set. Figure 2 shows the percentage of
errors which are shared by Syl-Concat and Char-
CNN depending on the value of p∗. We see that the

Figure 2: Percentage of errors shared by both
Syl-Concat and Char-CNN on DATA-S (left) and
DATA-L (right).

vast majority of errors are shared by both models
even when p∗ is small (0.01).
PPL breakdown by token frequency: To ﬁnd
out how Char-CNN outperforms Syl-Concat, we
partition the test sets on token frequency, as com-
puted on the training data. We can observe in Fig-
ure 3 that, on average, the more frequent the word
is, the bigger the advantage of Char-CNN over
Syl-Concat. The more Char-CNN sees a word
in different contexts, the more it can learn about
this word (due to its powerful CNN ﬁlters). Syl-
Concat, on the other hand, has limitations – it can-
not see below syllables, which prevents it from ex-
tracting the same amount of knowledge about the
word.

Model
RHN-Char-CNN
RHN-Syl-Concat
RHN-Syl-Concat

depth
8
8
8

dLM Size
650
439
650

PPL
20M 67.6
13M 72.0
20M 69.4

Table 5: Replacing LSTM with Variational RHN.

Lagus, 2007) in its default conﬁguration on the
PTB training data and used it instead of the syl-
labiﬁer in our models. Interestingly, we got ≈3K
unique morphemes, whereas the number of unique
syllables was ≈6K. We then trained all our models
on PTB under 5M parameter budget, keeping the
state size of the word-level LSTM at 300 (as in our
pre-selection step for syllable-aware models). The
reduction in number of subword types allowed us
to give them higher dimensionality dM = 100 (cf.
dS = 50).9

Convolutional

(Morph-CNN-3) and additive
(Morph-Sum) models performed better than oth-
ers with test set PPLs 83.0 and 83.9 respectively.
Due to limited amount of time, we did not perform
a thorough hyperparameter search under 20M bud-
get. Instead, we ran two conﬁgurations for Morph-
CNN-3 and two conﬁgurations for Morph-Sum
with hyperparameters close to those, which were
optimal for Syl-CNN-3 and Syl-Sum correspond-
ingly. All told, our best morpheme-aware model
is Morph-Sum with dM = 550, dHW = 1100,
dLM = 550, and test set PPL 79.5, which is
practically the same as the result of our best
syllable-aware model Syl-Concat (79.4). This
makes Morph-Sum a notable alternative to Char-
CNN and Syl-Concat, and we defer its thorough
study to future work.
Source code: The source code for the mod-
els discussed in this paper
is available at
https://github.com/zh3nis/lstm-syl.

7 Conclusion

It seems that syllable-aware language models fail
to outperform competitive character-aware ones.
However, usage of syllabiﬁcation can reduce the
total number of parameters and increase the train-
the expense of language-
ing speed, albeit at
dependent preprocessing. Morphological segmen-
tation is a noteworthy alternative to syllabiﬁcation:
a simple morpheme-aware model which sums
morpheme embeddings looks promising, and its
study is deferred to future work.

9M stands for morphemes.

Figure 3: PPL reduction by token frequency, Char-
CNN relative to Syl-Concat on DATA-L.

Model
Char-CNN 568
515
Syl-Concat

80% 90% 95% 99%
1038
1035

893
875

762
729

Table 4: Number of principle components when
PCA is applied to word embeddings produced by
each model, depending on % of variance to retain.

PCA of word embeddings: The intrinsic advan-
tage of Char-CNN over Syl-Concat is also sup-
ported by the following experiment: We took word
embeddings produced by both models on the En-
glish PTB, and applied PCA to them.8 Regard-
less of the threshold percentage of variance to re-
tain, the embeddings from Char-CNN always have
more principal components than the embeddings
from Syl-Concat (see Table 4). This means that
Char-CNN embeds words into higher dimensional
space than Syl-Concat, and thus can better distin-
guish them in different contexts.
LSTM limitations: During the hyperparameters
tuning we noticed that increasing dS, dHW and
dLM from the optimal values (in Table 2) did not
result in better performance for Syl-Concat. Could
it be due to the limitations of the word-level LSTM
(the topmost layer in Fig. 1)? To ﬁnd out whether
this was the case we replaced the LSTM by a
Variational RHN (Zilly et al., 2017), and that re-
sulted in a signiﬁcant reduction of perplexities on
PTB for both Char-CNN and Syl-Concat (Table
5). Moreover, increasing dLM from 439 to 650 did
result in better performance for Syl-Concat. Opti-
mization details are given in Appendix B.
Comparing syllable and morpheme embed-
dings: It is interesting to compare morphemes and
syllables. We trained Morfessor 2.0 (Creutz and

8We equalized highway layer sizes dHW in both models to
have same dimensions for embeddings. In both cases, word
vectors were standardized using the z-score transformation.

A Pre-selection

In all models with highway layers there are two of
them and the non-linear activation of any highway
layer is a ReLU.
LSTM-Word: dW = 108, dLM = 300.
Syl-LSTM: dS = 50, dLM = 300.
Syl-CNN-[L]: dS = 50, convolutional ﬁlter
widths are [1, . . . , L], the corresponding convolu-
tional ﬁlter depths are [c·l]L
l=1, dHW = c·(1+. . .+
L). We experimented with L = 2, 3, 4. The corre-
sponding values of c are chosen to be 120, 60, 35
to ﬁt the total parameter budget. CNN activation
is tanh.
Linear combinations: We give higher dimen-
sionality to syllable vectors here (compared to
other models) since the resulting word vector will
have the same size as syllable vectors (see (3)).
dS = 175, dHW = 175 in all models except the
Syl-Avg-B, where we have dS = 160, dHW =
160.
Syl-Concat: dS = 50, dHW = 300.

B Optimization

LSTM-based models: We perform the training
(5) by truncated BPTT (Werbos, 1990; Graves,
2013). We backpropagate for 70 time steps on
DATA-S and for 35 time steps on DATA-L using
stochastic gradient descent where the learning rate
is initially set to 1.0 and halved if the perplex-
ity does not decrease on the validation set after
an epoch. We use batch sizes of 20 for DATA-S
and 100 for DATA-L. We train for 50 epochs on
DATA-S and for 25 epochs on DATA-L, picking
the best-performing model on the validation set.
Parameters of the models are randomly initialized
uniformly in [−0.05, 0.05], except the forget bias
of the word-level LSTM, which is initialized to
1. For regularization we use dropout (Srivastava
et al., 2014) with probability 0.5 between word-
level LSTM layers and on the hidden-to-output
softmax layer. We clip the norm of the gradi-
ents (normalized by minibatch size) at 5. These
choices were guided by previous work on word-
level language modeling with LSTMs (Zaremba
et al., 2014).

To speed up training on DATA-L we use a sam-
pled softmax (Jean et al., 2015) with the number
of samples equal to 20% of the vocabulary size
(Chen et al., 2016). Although Kim et al. (2016)
used a hierarchical softmax (Morin and Bengio,
2005) for the same purpose, a recent study (Grave

et al., 2016) shows that it is outperformed by sam-
pled softmax on the Europarl corpus, from which
DATA-L was derived (Botha and Blunsom, 2014).
RHN-based models are optimized as in Zilly et al.
(2017), except that we unrolled the networks for
70 time steps in truncated BPTT, and dropout rates
were chosen to be as follows: 0.2 for the embed-
ding layer, 0.7 for the input to the gates, 0.7 for the
hidden units and 0.2 for the output activations.

C Sizes and speeds

On DATA-S, Syl-Concat has 28%–33% fewer pa-
rameters than Char-CNN, and on DATA-L the re-
duction is 18%–27% (see Fig. 4).

Figure 4: Model sizes on DATA-S (left) and
DATA-L, in millions of trainable variables.

Training speeds are provided in the Table 6. Mod-
els were implemented in TensorFlow, and were
run on NVIDIA Titan X (Pascal).

EN FR ES DE CS RU
Model
6
9
Char-CNN
9
Syl-Concat
14
4
Char-CNN 10
5
22
Syl-Concat

6
10
7
10

8
12
8
13

7
11
5
6

8
12
7
13

S

L

Table 6: Training speeds, in thousands of tokens
per second.

Acknowledgements

We gratefully acknowledge the NVIDIA Corpo-
ration for their donation of the Titan X Pascal
GPU used for this research. The work of Bag-
dat Myrzakhmetov has been funded by the Com-
mittee of Science of the Ministry of Education
and Science of the Republic of Kazakhstan un-
der the targeted program O.0743 (0115PK02473).
The authors would like to thank anonymous re-
viewers and Aibek Makazhanov for valuable feed-
back, Makat Tlebaliyev and Dmitriy Polynin for
IT support, and Yoon Kim for providing the pre-
processed datasets.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781.

Tom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
Subword language modeling with neu-
2012.
(http://www. ﬁt. vutbr.
ral networks.
cz/imikolov/rnnlm/char. pdf).

preprint

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of AISTATS.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In Proceedings of COL-
ING.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of ACL.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Rupesh K Srivastava, Klaus Greff, and J¨urgen Schmid-
huber. 2015. Training very deep networks. In Pro-
ceedings of NIPS.

Clara Vania and Adam Lopez. 2017. From characters
to words to in between: Do we capture morphology?
In Proceedings of ACL.

Lyan Verwimp, Joris Pelemans, Patrick Wambacq,
et al. 2017. Character-word lstm language models.
In Proceedings of EACL.

Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10).

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan
Koutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent
highway networks. In Proceedings of ICML.

Barret Zoph and Quoc V Le. 2017. Neural architecture
search with reinforcement learning. In Proceedings
of ICLR.

References

Jan Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proceedings of ICML.

Wenlin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proceedings of ACL.

Ryan Cotterell and Hinrich Sch¨utze. 2015. Morpho-
logical word-embeddings. In Proceedings of HLT-
NAACL.

Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of NIPS.

Edouard Grave, Armand Joulin, Moustapha Ciss´e,
David Grangier, and Herv´e J´egou. 2016. Efﬁcient
arXiv preprint
softmax approximation for gpus.
arXiv:1609.04309.

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
Proceedings of ACL-IJCNLP.

Rafal

Jozefowicz, Wojciech Zaremba,

and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of ICML.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI.

Franklin Mark Liang. 1983. Word Hy-phen-a-tion by

Com-put-er. Citeseer.

Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, and Sil-
vio Amir. 2015a. Not all contexts are created equal:
Better word representations with variable attention.
In Proceedings of EMNLP.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015b. Finding function in form:
Compositional character models for open vocab-
In Proceedings of
ulary word representation.
EMNLP.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Syllable-aware Neural Language Models:
A Failure to Beat Character-aware Ones

Zhenisbek Assylbekov
School of Science and Technology
Nazarbayev University
zhassylbekov@nu.edu.kz

Rustem Takhanov
School of Science and Technology
Nazarbayev University
rustem.takhanov@nu.edu.kz

Bagdat Myrzakhmetov
National Laboratory Astana
Nazarbayev University
bagdat.myrzakhmetov@nu.edu.kz

Jonathan N. Washington
Linguistics Department
Swarthmore College
jonathan.washington
@swarthmore.edu

7
1
0
2
 
l
u
J
 
0
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
8
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Syllabiﬁcation does not seem to im-
prove word-level RNN language model-
ing quality when compared to character-
based segmentation. However, our best
syllable-aware language model, achieving
performance comparable to the competi-
tive character-aware model, has 18%–33%
fewer parameters and is trained 1.2–2.2
times faster.

1

Introduction

Recent advances in neural
language modeling
(NLM) are connected with character-aware mod-
els (Kim et al., 2016; Ling et al., 2015b; Verwimp
et al., 2017). This is a promising approach, and
we propose the following direction related to it:
We would like to make sure that in the pursuit of
the most ﬁne-grained representations one has not
missed possible intermediate ways of segmenta-
tion, e.g., by syllables. Syllables, in our opinion,
are better supported as linguistic units of language
than single characters. In most languages, words
can be naturally split into syllables:

ES: el par-la-men-to a-po-y´o la en-mien-da
RU: par-la-ment pod-der-ˇzal po-prav-ku
(EN: the parliament supported the amendment)

Based on this observation, we attempted to de-
termine whether syllable-aware NLM has any ad-
vantages over character-aware NLM. We exper-
imented with a variety of models but could not
ﬁnd any evidence to support this hypothesis: split-
ting words into syllables does not seem to improve
the language modeling quality when compared to
splitting into characters. However, there are some
positive ﬁndings: while our best syllable-aware

language model achieves performance comparable
to the competitive character-aware model, it has
18%–33% fewer parameters and is 1.2–2.2 times
faster to train.

2 Related Work

Much research has been done on subword-level
and subword-aware1 neural language modeling
when subwords are characters (Ling et al., 2015b;
Kim et al., 2016; Verwimp et al., 2017) or mor-
phemes (Botha and Blunsom, 2014; Qiu et al.,
2014; Cotterell and Sch¨utze, 2015). However,
not much work has been done on syllable-level or
syllable-aware NLM. Mikolov et al. (2012) show
that subword-level language models outperform
character-level ones.2 They keep the most fre-
quent words untouched and split all other words
into syllable-like units. Our approach differs
mainly in the following aspects: we make predic-
tions at the word level, use a more linguistically
sound syllabiﬁcation algorithm, and consider a va-
riety of more advanced neural architectures.

We have recently come across a concurrent pa-
per (Vania and Lopez, 2017) where the authors
systematically compare different subword units
(characters, character trigrams, BPE (Sennrich
et al., 2016), morphemes) and different represen-
tation models (CNN, Bi-LSTM, summation) on
languages with various morphological typology.
However, they do not consider syllables, and they
experiment with relatively small models on small
data sets (0.6M–1.4M tokens).

1Subword-level LMs rely on subword-level inputs and
make predictions at the level of subwords; subword-aware
LMs also rely on subword-level inputs but make predictions
at the level of words.

2Not to be confused with character-aware ones, see the

previous footnote.

unconstitutional conditions on

Highway layers (optional)

Syllable-aware word embedding model

stack of two
LSTMs

word vector

Syllable
embeddings

un con sti

tu tional

imposes unconstitutional conditions

Figure 1: Syllable-aware language model.

3 Syllable-aware word embeddings

Let W and S be ﬁnite vocabularies of words and
syllables respectively. We assume that both words
and syllables have already been converted into in-
dices. Let ES ∈ R|S|×dS be an embedding ma-
trix for syllables — i.e., it is a matrix in which the
sth row (denoted as s) corresponds to an embed-
ding of the syllable s ∈ S. Any word w ∈ W is
a sequence of its syllables (s1, s2, . . . , snw ), and
hence can be represented as a sequence of the cor-
responding syllable vectors:

[s1, s2, . . . , snw ].

(1)

The question is: How shall we pack the sequence
(1) into a single vector x ∈ RdW to produce a
better embedding of the word w?3
In our case
“better” means “better than a character-aware em-
bedding of w via the Char-CNN model of Kim
et al. (2016)”. Below we present several viable
approaches.

3.1 Recurrent sequential model (Syl-LSTM)

Since the syllables are coming in a sequence it is
natural to try a recurrent sequential model:

ht = f (st, ht−1),

h0 = 0,

(2)

which converts the sequence of syllable vectors (1)
into a sequence of state vectors h1:nw . The last

3The same question applies to any model that segments

words into a sequence of characters or other subword units.

state vector hnw is assumed to contain the infor-
mation on the whole sequence (1), and is there-
fore used as a word embedding for w. There is
a big variety of transformations from which one
can choose f in (2); however, a recent thorough
evaluation (Jozefowicz et al., 2015) shows that the
LSTM (Hochreiter and Schmidhuber, 1997) with
its forget bias initialized to 1 outperforms other
popular architectures on almost all tasks, and we
decided to use it for our experiments. We will re-
fer to this model as Syl-LSTM.

3.2 Convolutional model (Syl-CNN)

Inspired by recent work on character-aware neural
language models (Kim et al., 2016) we decided to
try this approach (Char-CNN) on syllables. Our
case differs mainly in the following two aspects:

1. The set of syllables S is usually bigger than
the set of characters C,4 and also the dimen-
sionality dS of syllable vectors is expected to
be greater than the dimensionality dC of char-
acter vectors. Both of these factors result in
allocating more parameters on syllable em-
beddings compared to character embeddings.
2. On average a word contains fewer syllables
than characters, and therefore we need nar-
rower convolutional ﬁlters for syllables. This
results in spending fewer parameters per con-
volution.

This means that by varying dS and the maximum
width of convolutional ﬁlters L we can still ﬁt the
parameter budget of Kim et al. (2016) to allow fair
comparison of the models.

Like in Char-CNN, our syllable-aware model,
which is referred to as Syl-CNN-[L], utilizes max-
pooling and highway layers (Srivastava et al.,
2015) to model interactions between the syllables.
The dimensionality of a highway layer is denoted
by dHW.

3.3 Linear combinations

We also considered using linear combinations of
syllable-vectors to represent the word embedding:

x = (cid:80)nw

t=1 αt(st) · st.

(3)

The choice for αt is motivated mainly by the ex-
isting approaches (discussed below) which proved
to be successful for other tasks.
Syl-Sum: Summing up syllable vectors to get a
word vector can be obtained by setting αt(st) = 1.

4In languages with alphabetic writing systems.

This approach was used by Botha and Blunsom
(2014) to combine a word and its morpheme em-
beddings into a single word vector.
Syl-Avg: A simple average of syllable vectors can
be obtained by setting αt(st) = 1/nw. This can
be also called a “continuous bag of syllables” in an
analogy to a CBOW model (Mikolov et al., 2013),
where vectors of neighboring words are averaged
to get a word embedding of the current word.
Syl-Avg-A: We let the weights αt in (3) be a
function of parameters (a1, . . . , an) of the model,
which are jointly trained together with other pa-
rameters. Here n = maxw{nw} is a maxi-
mum word length in syllables.
In order to have
a weighted average in (3) we apply a softmax nor-
malization:

αt = softmax(a)t =

exp(at)
τ =1 exp(aτ )

(cid:80)n

(4)

Syl-Avg-B: We can let αt depend on syllables and
their positions:

αt = αt(st) = softmax(ast + b)t
where A ∈ RdS ×n (with elements as,t) is a set of
parameters that determine the importance of each
syllable type in each (relative) position, b ∈ Rn
is a bias, which is conditioned only on the rela-
tive position. This approach is motivated by re-
cent work on using an attention mechanism in the
CBOW model (Ling et al., 2015a).

We feed the resulting x from (3) into a stack of
highway layers to allow interactions between the
syllables.

3.4 Concatenation (Syl-Concat)

In this model we simply concatenate syllable vec-
tors (1) into a single word vector:

x = [s1; s2; . . . ; snw ; 0; 0; . . . ; 0
]
(cid:125)

(cid:124)

(cid:123)(cid:122)
n−nw

We zero-pad x so that all word vectors have the
same length n · dS to allow batch processing, and
then we feed x into a stack of highway layers.

4 Word-level language model

Once we have word embeddings x1:k for a se-
quence of words w1:k we can use a word-level
RNN language model to produce a sequence of
states h1:k and then predict the next word accord-
ing to the probability distribution

where W ∈ RdLM×|W|, b ∈ R|W|, and dLM is the
hidden layer size of the RNN. Training the model
involves minimizing the negative log-likelihood
over the corpus w1:K:
− (cid:80)K

k=1 log Pr(wk|w1:k−1) −→ min

(5)

As was mentioned in Section 3.1 there is a huge
variety of RNN architectures to choose from. The
most advanced recurrent neural architectures, at
the time of this writing, are recurrent highway
networks (Zilly et al., 2017) and a novel model
which was obtained through a neural architecture
search with reinforcement learning (Zoph and Le,
2017). These models can be spiced up with the
most recent regularization techniques for RNNs
(Gal and Ghahramani, 2016) to reach state-of-the-
art. However, to make our results directly com-
parable to those of Kim et al. (2016) we select a
two-layer LSTM and regularize it as in Zaremba
et al. (2014).

5 Experimental Setup

We search for the best model in two steps: ﬁrst,
we block the word-level LSTM’s architecture and
pre-select the three best models under a small pa-
rameter budget (5M), and then we tune these three
best models’ hyperparameters under a larger bud-
get (20M).
Pre-selection: We ﬁx dLM (hidden layer size of
the word-level LSTM) at 300 units per layer and
run each syllable-aware word embedding method
from Section 3 on the English PTB data set (Mar-
cus et al., 1993), keeping the total parameter bud-
get at 5M. The architectural choices are speciﬁed
in Appendix A.
Hyperparameter tuning: The hyperparameters
of the three best-performing models from the pre-
selection step are then thoroughly tuned on the
same English PTB data through a random search
according to the marginal distributions:

• dS ∼ U (20, 650),5
• log(dHW) ∼ U (log(160), log(2000)),
• log(dLM) ∼ U (log(300), log(2000)),

with the restriction dS < dLM. The total parameter
budget is kept at 20M to allow for easy comparison
to the results of Kim et al. (2016). Then these three
best models (with their hyperparameters tuned on
PTB) are trained and evaluated on small- (DATA-
S) and medium-sized (DATA-L) data sets in six
languages.

Pr(wk+1|w1:k) = softmax(hkW + b),

5U (a, b) stands for a uniform distribution over (a, b).

Model
LSTM-Word
Syl-LSTM
Syl-CNN-2
Syl-CNN-3
Syl-CNN-4
Syl-Sum

PPL Model
88.0
88.7
86.6
84.6
86.8
84.6

Char-CNN
Syl-Avg
Syl-Avg-A
Syl-Avg-B
Syl-Concat

PPL
92.3
88.5
91.4
88.5
83.7

Table 1: Pre-selection results. PPL stands for test
set perplexity, all models have ≈ 5M parameters.

Model
Syl-CNN
Syl-Sum
Syl-Concat

dS
242
438
228

dHW
1170
1256
781

dLM
380
435
439

Size
PPL
15M 80.5
18M 80.3
13M 79.4

Table 2: Hyperparameters tuning.
In Syl-CNN,
dHW is a function of the primary hyperparameter
c = 195 (see Appendix A).

Optimizaton is performed in almost the same way
as in the work of Zaremba et al. (2014). See Ap-
pendix B for details.
Syllabiﬁcation: The true syllabiﬁcation of a word
requires its grapheme-to-phoneme conversion and
then splitting it into syllables based on some rules.
Since these are not always available for less-
resourced languages, we decided to utilize Liang’s
widely-used hyphenation algorithm (Liang, 1983).

6 Results

The results of the pre-selection are reported in
Table 1. All syllable-aware models comfortably
outperform the Char-CNN when the budget is
limited to 5M parameters. Surprisingly, a pure
word-level model,6 LSTM-Word, also beats the
character-aware one under such budget. The three
best conﬁgurations are Syl-Concat, Syl-Sum, and
Syl-CNN-3 (hereinafter referred to as Syl-CNN),
and tuning their hyperparameters under 20M pa-
rameter budget gives the architectures in Table
2. The results of evaluating these three models
on small (1M tokens) and medium-sized (17M–
57M tokens) data sets against Char-CNN for dif-
The
ferent languages are provided in Table 3.
models demonstrate similar performance on small
data, but Char-CNN scales signiﬁcantly better on
medium-sized data. From the three syllable-aware
models, Syl-Concat looks the most advantageous
as it demonstrates stable results and has the least

6When words are directly embedded into RdW through an

embedding matrix EW ∈ R|W|×dW .

7Syl-CNN results on DATA-L are not reported since com-
putational resources were insufﬁcient to run these conﬁgura-
tions.

Model
EN
Char-CNN 78.9
80.5
Syl-CNN
80.3
Syl-Sum
Syl-Concat
79.4
Char-CNN 160
Syl-CNN7
–
170
Syl-Sum
176
Syl-Concat

FR
184
191
193
188
124
–
141
139

ES DE CS RU
261
165
269
172
273
170
265
168
190
118
–
–
233
129
225
129

239
239
243
244
198
–
212
225

371
374
389
383
392
–
451
449

S
-
A
T
A
D

L
-
A
T
A
D

Table 3: Evaluation of the syllable-aware mod-
els against Char-CNN. In each case the smallest
model, Syl-Concat, has 18%–33% less parameters
than Char-CNN and is trained 1.2–2.2 times faster
(Appendix C).

number of parameters. Therefore in what follows
we will make a more detailed comparison of Syl-
Concat with Char-CNN.
Shared errors: It is interesting to see whether
Char-CNN and Syl-Concat are making similar er-
rors. We say that a model gives an error if it as-
signs a probability less than p∗ to a correct word
from the test set. Figure 2 shows the percentage of
errors which are shared by Syl-Concat and Char-
CNN depending on the value of p∗. We see that the

Figure 2: Percentage of errors shared by both
Syl-Concat and Char-CNN on DATA-S (left) and
DATA-L (right).

vast majority of errors are shared by both models
even when p∗ is small (0.01).
PPL breakdown by token frequency: To ﬁnd
out how Char-CNN outperforms Syl-Concat, we
partition the test sets on token frequency, as com-
puted on the training data. We can observe in Fig-
ure 3 that, on average, the more frequent the word
is, the bigger the advantage of Char-CNN over
Syl-Concat. The more Char-CNN sees a word
in different contexts, the more it can learn about
this word (due to its powerful CNN ﬁlters). Syl-
Concat, on the other hand, has limitations – it can-
not see below syllables, which prevents it from ex-
tracting the same amount of knowledge about the
word.

Model
RHN-Char-CNN
RHN-Syl-Concat
RHN-Syl-Concat

depth
8
8
8

dLM Size
650
439
650

PPL
20M 67.6
13M 72.0
20M 69.4

Table 5: Replacing LSTM with Variational RHN.

Lagus, 2007) in its default conﬁguration on the
PTB training data and used it instead of the syl-
labiﬁer in our models. Interestingly, we got ≈3K
unique morphemes, whereas the number of unique
syllables was ≈6K. We then trained all our models
on PTB under 5M parameter budget, keeping the
state size of the word-level LSTM at 300 (as in our
pre-selection step for syllable-aware models). The
reduction in number of subword types allowed us
to give them higher dimensionality dM = 100 (cf.
dS = 50).9

Convolutional

(Morph-CNN-3) and additive
(Morph-Sum) models performed better than oth-
ers with test set PPLs 83.0 and 83.9 respectively.
Due to limited amount of time, we did not perform
a thorough hyperparameter search under 20M bud-
get. Instead, we ran two conﬁgurations for Morph-
CNN-3 and two conﬁgurations for Morph-Sum
with hyperparameters close to those, which were
optimal for Syl-CNN-3 and Syl-Sum correspond-
ingly. All told, our best morpheme-aware model
is Morph-Sum with dM = 550, dHW = 1100,
dLM = 550, and test set PPL 79.5, which is
practically the same as the result of our best
syllable-aware model Syl-Concat (79.4). This
makes Morph-Sum a notable alternative to Char-
CNN and Syl-Concat, and we defer its thorough
study to future work.
Source code: The source code for the mod-
els discussed in this paper
is available at
https://github.com/zh3nis/lstm-syl.

7 Conclusion

It seems that syllable-aware language models fail
to outperform competitive character-aware ones.
However, usage of syllabiﬁcation can reduce the
total number of parameters and increase the train-
the expense of language-
ing speed, albeit at
dependent preprocessing. Morphological segmen-
tation is a noteworthy alternative to syllabiﬁcation:
a simple morpheme-aware model which sums
morpheme embeddings looks promising, and its
study is deferred to future work.

9M stands for morphemes.

Figure 3: PPL reduction by token frequency, Char-
CNN relative to Syl-Concat on DATA-L.

Model
Char-CNN 568
515
Syl-Concat

80% 90% 95% 99%
1038
1035

893
875

762
729

Table 4: Number of principle components when
PCA is applied to word embeddings produced by
each model, depending on % of variance to retain.

PCA of word embeddings: The intrinsic advan-
tage of Char-CNN over Syl-Concat is also sup-
ported by the following experiment: We took word
embeddings produced by both models on the En-
glish PTB, and applied PCA to them.8 Regard-
less of the threshold percentage of variance to re-
tain, the embeddings from Char-CNN always have
more principal components than the embeddings
from Syl-Concat (see Table 4). This means that
Char-CNN embeds words into higher dimensional
space than Syl-Concat, and thus can better distin-
guish them in different contexts.
LSTM limitations: During the hyperparameters
tuning we noticed that increasing dS, dHW and
dLM from the optimal values (in Table 2) did not
result in better performance for Syl-Concat. Could
it be due to the limitations of the word-level LSTM
(the topmost layer in Fig. 1)? To ﬁnd out whether
this was the case we replaced the LSTM by a
Variational RHN (Zilly et al., 2017), and that re-
sulted in a signiﬁcant reduction of perplexities on
PTB for both Char-CNN and Syl-Concat (Table
5). Moreover, increasing dLM from 439 to 650 did
result in better performance for Syl-Concat. Opti-
mization details are given in Appendix B.
Comparing syllable and morpheme embed-
dings: It is interesting to compare morphemes and
syllables. We trained Morfessor 2.0 (Creutz and

8We equalized highway layer sizes dHW in both models to
have same dimensions for embeddings. In both cases, word
vectors were standardized using the z-score transformation.

A Pre-selection

In all models with highway layers there are two of
them and the non-linear activation of any highway
layer is a ReLU.
LSTM-Word: dW = 108, dLM = 300.
Syl-LSTM: dS = 50, dLM = 300.
Syl-CNN-[L]: dS = 50, convolutional ﬁlter
widths are [1, . . . , L], the corresponding convolu-
tional ﬁlter depths are [c·l]L
l=1, dHW = c·(1+. . .+
L). We experimented with L = 2, 3, 4. The corre-
sponding values of c are chosen to be 120, 60, 35
to ﬁt the total parameter budget. CNN activation
is tanh.
Linear combinations: We give higher dimen-
sionality to syllable vectors here (compared to
other models) since the resulting word vector will
have the same size as syllable vectors (see (3)).
dS = 175, dHW = 175 in all models except the
Syl-Avg-B, where we have dS = 160, dHW =
160.
Syl-Concat: dS = 50, dHW = 300.

B Optimization

LSTM-based models: We perform the training
(5) by truncated BPTT (Werbos, 1990; Graves,
2013). We backpropagate for 70 time steps on
DATA-S and for 35 time steps on DATA-L using
stochastic gradient descent where the learning rate
is initially set to 1.0 and halved if the perplex-
ity does not decrease on the validation set after
an epoch. We use batch sizes of 20 for DATA-S
and 100 for DATA-L. We train for 50 epochs on
DATA-S and for 25 epochs on DATA-L, picking
the best-performing model on the validation set.
Parameters of the models are randomly initialized
uniformly in [−0.05, 0.05], except the forget bias
of the word-level LSTM, which is initialized to
1. For regularization we use dropout (Srivastava
et al., 2014) with probability 0.5 between word-
level LSTM layers and on the hidden-to-output
softmax layer. We clip the norm of the gradi-
ents (normalized by minibatch size) at 5. These
choices were guided by previous work on word-
level language modeling with LSTMs (Zaremba
et al., 2014).

To speed up training on DATA-L we use a sam-
pled softmax (Jean et al., 2015) with the number
of samples equal to 20% of the vocabulary size
(Chen et al., 2016). Although Kim et al. (2016)
used a hierarchical softmax (Morin and Bengio,
2005) for the same purpose, a recent study (Grave

et al., 2016) shows that it is outperformed by sam-
pled softmax on the Europarl corpus, from which
DATA-L was derived (Botha and Blunsom, 2014).
RHN-based models are optimized as in Zilly et al.
(2017), except that we unrolled the networks for
70 time steps in truncated BPTT, and dropout rates
were chosen to be as follows: 0.2 for the embed-
ding layer, 0.7 for the input to the gates, 0.7 for the
hidden units and 0.2 for the output activations.

C Sizes and speeds

On DATA-S, Syl-Concat has 28%–33% fewer pa-
rameters than Char-CNN, and on DATA-L the re-
duction is 18%–27% (see Fig. 4).

Figure 4: Model sizes on DATA-S (left) and
DATA-L, in millions of trainable variables.

Training speeds are provided in the Table 6. Mod-
els were implemented in TensorFlow, and were
run on NVIDIA Titan X (Pascal).

EN FR ES DE CS RU
Model
6
9
Char-CNN
9
Syl-Concat
14
4
Char-CNN 10
5
22
Syl-Concat

6
10
7
10

8
12
8
13

7
11
5
6

8
12
7
13

S

L

Table 6: Training speeds, in thousands of tokens
per second.

Acknowledgements

We gratefully acknowledge the NVIDIA Corpo-
ration for their donation of the Titan X Pascal
GPU used for this research. The work of Bag-
dat Myrzakhmetov has been funded by the Com-
mittee of Science of the Ministry of Education
and Science of the Republic of Kazakhstan un-
der the targeted program O.0743 (0115PK02473).
The authors would like to thank anonymous re-
viewers and Aibek Makazhanov for valuable feed-
back, Makat Tlebaliyev and Dmitriy Polynin for
IT support, and Yoon Kim for providing the pre-
processed datasets.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781.

Tom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
Subword language modeling with neu-
2012.
(http://www. ﬁt. vutbr.
ral networks.
cz/imikolov/rnnlm/char. pdf).

preprint

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of AISTATS.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In Proceedings of COL-
ING.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of ACL.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Rupesh K Srivastava, Klaus Greff, and J¨urgen Schmid-
huber. 2015. Training very deep networks. In Pro-
ceedings of NIPS.

Clara Vania and Adam Lopez. 2017. From characters
to words to in between: Do we capture morphology?
In Proceedings of ACL.

Lyan Verwimp, Joris Pelemans, Patrick Wambacq,
et al. 2017. Character-word lstm language models.
In Proceedings of EACL.

Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10).

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan
Koutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent
highway networks. In Proceedings of ICML.

Barret Zoph and Quoc V Le. 2017. Neural architecture
search with reinforcement learning. In Proceedings
of ICLR.

References

Jan Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proceedings of ICML.

Wenlin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proceedings of ACL.

Ryan Cotterell and Hinrich Sch¨utze. 2015. Morpho-
logical word-embeddings. In Proceedings of HLT-
NAACL.

Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of NIPS.

Edouard Grave, Armand Joulin, Moustapha Ciss´e,
David Grangier, and Herv´e J´egou. 2016. Efﬁcient
arXiv preprint
softmax approximation for gpus.
arXiv:1609.04309.

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
Proceedings of ACL-IJCNLP.

Rafal

Jozefowicz, Wojciech Zaremba,

and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of ICML.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI.

Franklin Mark Liang. 1983. Word Hy-phen-a-tion by

Com-put-er. Citeseer.

Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, and Sil-
vio Amir. 2015a. Not all contexts are created equal:
Better word representations with variable attention.
In Proceedings of EMNLP.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015b. Finding function in form:
Compositional character models for open vocab-
In Proceedings of
ulary word representation.
EMNLP.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Syllable-aware Neural Language Models:
A Failure to Beat Character-aware Ones

Zhenisbek Assylbekov
School of Science and Technology
Nazarbayev University
zhassylbekov@nu.edu.kz

Rustem Takhanov
School of Science and Technology
Nazarbayev University
rustem.takhanov@nu.edu.kz

Bagdat Myrzakhmetov
National Laboratory Astana
Nazarbayev University
bagdat.myrzakhmetov@nu.edu.kz

Jonathan N. Washington
Linguistics Department
Swarthmore College
jonathan.washington
@swarthmore.edu

7
1
0
2
 
l
u
J
 
0
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
8
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Syllabiﬁcation does not seem to im-
prove word-level RNN language model-
ing quality when compared to character-
based segmentation. However, our best
syllable-aware language model, achieving
performance comparable to the competi-
tive character-aware model, has 18%–33%
fewer parameters and is trained 1.2–2.2
times faster.

1

Introduction

Recent advances in neural
language modeling
(NLM) are connected with character-aware mod-
els (Kim et al., 2016; Ling et al., 2015b; Verwimp
et al., 2017). This is a promising approach, and
we propose the following direction related to it:
We would like to make sure that in the pursuit of
the most ﬁne-grained representations one has not
missed possible intermediate ways of segmenta-
tion, e.g., by syllables. Syllables, in our opinion,
are better supported as linguistic units of language
than single characters. In most languages, words
can be naturally split into syllables:

ES: el par-la-men-to a-po-y´o la en-mien-da
RU: par-la-ment pod-der-ˇzal po-prav-ku
(EN: the parliament supported the amendment)

Based on this observation, we attempted to de-
termine whether syllable-aware NLM has any ad-
vantages over character-aware NLM. We exper-
imented with a variety of models but could not
ﬁnd any evidence to support this hypothesis: split-
ting words into syllables does not seem to improve
the language modeling quality when compared to
splitting into characters. However, there are some
positive ﬁndings: while our best syllable-aware

language model achieves performance comparable
to the competitive character-aware model, it has
18%–33% fewer parameters and is 1.2–2.2 times
faster to train.

2 Related Work

Much research has been done on subword-level
and subword-aware1 neural language modeling
when subwords are characters (Ling et al., 2015b;
Kim et al., 2016; Verwimp et al., 2017) or mor-
phemes (Botha and Blunsom, 2014; Qiu et al.,
2014; Cotterell and Sch¨utze, 2015). However,
not much work has been done on syllable-level or
syllable-aware NLM. Mikolov et al. (2012) show
that subword-level language models outperform
character-level ones.2 They keep the most fre-
quent words untouched and split all other words
into syllable-like units. Our approach differs
mainly in the following aspects: we make predic-
tions at the word level, use a more linguistically
sound syllabiﬁcation algorithm, and consider a va-
riety of more advanced neural architectures.

We have recently come across a concurrent pa-
per (Vania and Lopez, 2017) where the authors
systematically compare different subword units
(characters, character trigrams, BPE (Sennrich
et al., 2016), morphemes) and different represen-
tation models (CNN, Bi-LSTM, summation) on
languages with various morphological typology.
However, they do not consider syllables, and they
experiment with relatively small models on small
data sets (0.6M–1.4M tokens).

1Subword-level LMs rely on subword-level inputs and
make predictions at the level of subwords; subword-aware
LMs also rely on subword-level inputs but make predictions
at the level of words.

2Not to be confused with character-aware ones, see the

previous footnote.

unconstitutional conditions on

Highway layers (optional)

Syllable-aware word embedding model

stack of two
LSTMs

word vector

Syllable
embeddings

un con sti

tu tional

imposes unconstitutional conditions

Figure 1: Syllable-aware language model.

3 Syllable-aware word embeddings

Let W and S be ﬁnite vocabularies of words and
syllables respectively. We assume that both words
and syllables have already been converted into in-
dices. Let ES ∈ R|S|×dS be an embedding ma-
trix for syllables — i.e., it is a matrix in which the
sth row (denoted as s) corresponds to an embed-
ding of the syllable s ∈ S. Any word w ∈ W is
a sequence of its syllables (s1, s2, . . . , snw ), and
hence can be represented as a sequence of the cor-
responding syllable vectors:

[s1, s2, . . . , snw ].

(1)

The question is: How shall we pack the sequence
(1) into a single vector x ∈ RdW to produce a
better embedding of the word w?3
In our case
“better” means “better than a character-aware em-
bedding of w via the Char-CNN model of Kim
et al. (2016)”. Below we present several viable
approaches.

3.1 Recurrent sequential model (Syl-LSTM)

Since the syllables are coming in a sequence it is
natural to try a recurrent sequential model:

ht = f (st, ht−1),

h0 = 0,

(2)

which converts the sequence of syllable vectors (1)
into a sequence of state vectors h1:nw . The last

3The same question applies to any model that segments

words into a sequence of characters or other subword units.

state vector hnw is assumed to contain the infor-
mation on the whole sequence (1), and is there-
fore used as a word embedding for w. There is
a big variety of transformations from which one
can choose f in (2); however, a recent thorough
evaluation (Jozefowicz et al., 2015) shows that the
LSTM (Hochreiter and Schmidhuber, 1997) with
its forget bias initialized to 1 outperforms other
popular architectures on almost all tasks, and we
decided to use it for our experiments. We will re-
fer to this model as Syl-LSTM.

3.2 Convolutional model (Syl-CNN)

Inspired by recent work on character-aware neural
language models (Kim et al., 2016) we decided to
try this approach (Char-CNN) on syllables. Our
case differs mainly in the following two aspects:

1. The set of syllables S is usually bigger than
the set of characters C,4 and also the dimen-
sionality dS of syllable vectors is expected to
be greater than the dimensionality dC of char-
acter vectors. Both of these factors result in
allocating more parameters on syllable em-
beddings compared to character embeddings.
2. On average a word contains fewer syllables
than characters, and therefore we need nar-
rower convolutional ﬁlters for syllables. This
results in spending fewer parameters per con-
volution.

This means that by varying dS and the maximum
width of convolutional ﬁlters L we can still ﬁt the
parameter budget of Kim et al. (2016) to allow fair
comparison of the models.

Like in Char-CNN, our syllable-aware model,
which is referred to as Syl-CNN-[L], utilizes max-
pooling and highway layers (Srivastava et al.,
2015) to model interactions between the syllables.
The dimensionality of a highway layer is denoted
by dHW.

3.3 Linear combinations

We also considered using linear combinations of
syllable-vectors to represent the word embedding:

x = (cid:80)nw

t=1 αt(st) · st.

(3)

The choice for αt is motivated mainly by the ex-
isting approaches (discussed below) which proved
to be successful for other tasks.
Syl-Sum: Summing up syllable vectors to get a
word vector can be obtained by setting αt(st) = 1.

4In languages with alphabetic writing systems.

This approach was used by Botha and Blunsom
(2014) to combine a word and its morpheme em-
beddings into a single word vector.
Syl-Avg: A simple average of syllable vectors can
be obtained by setting αt(st) = 1/nw. This can
be also called a “continuous bag of syllables” in an
analogy to a CBOW model (Mikolov et al., 2013),
where vectors of neighboring words are averaged
to get a word embedding of the current word.
Syl-Avg-A: We let the weights αt in (3) be a
function of parameters (a1, . . . , an) of the model,
which are jointly trained together with other pa-
rameters. Here n = maxw{nw} is a maxi-
mum word length in syllables.
In order to have
a weighted average in (3) we apply a softmax nor-
malization:

αt = softmax(a)t =

exp(at)
τ =1 exp(aτ )

(cid:80)n

(4)

Syl-Avg-B: We can let αt depend on syllables and
their positions:

αt = αt(st) = softmax(ast + b)t
where A ∈ RdS ×n (with elements as,t) is a set of
parameters that determine the importance of each
syllable type in each (relative) position, b ∈ Rn
is a bias, which is conditioned only on the rela-
tive position. This approach is motivated by re-
cent work on using an attention mechanism in the
CBOW model (Ling et al., 2015a).

We feed the resulting x from (3) into a stack of
highway layers to allow interactions between the
syllables.

3.4 Concatenation (Syl-Concat)

In this model we simply concatenate syllable vec-
tors (1) into a single word vector:

x = [s1; s2; . . . ; snw ; 0; 0; . . . ; 0
]
(cid:125)

(cid:124)

(cid:123)(cid:122)
n−nw

We zero-pad x so that all word vectors have the
same length n · dS to allow batch processing, and
then we feed x into a stack of highway layers.

4 Word-level language model

Once we have word embeddings x1:k for a se-
quence of words w1:k we can use a word-level
RNN language model to produce a sequence of
states h1:k and then predict the next word accord-
ing to the probability distribution

where W ∈ RdLM×|W|, b ∈ R|W|, and dLM is the
hidden layer size of the RNN. Training the model
involves minimizing the negative log-likelihood
over the corpus w1:K:
− (cid:80)K

k=1 log Pr(wk|w1:k−1) −→ min

(5)

As was mentioned in Section 3.1 there is a huge
variety of RNN architectures to choose from. The
most advanced recurrent neural architectures, at
the time of this writing, are recurrent highway
networks (Zilly et al., 2017) and a novel model
which was obtained through a neural architecture
search with reinforcement learning (Zoph and Le,
2017). These models can be spiced up with the
most recent regularization techniques for RNNs
(Gal and Ghahramani, 2016) to reach state-of-the-
art. However, to make our results directly com-
parable to those of Kim et al. (2016) we select a
two-layer LSTM and regularize it as in Zaremba
et al. (2014).

5 Experimental Setup

We search for the best model in two steps: ﬁrst,
we block the word-level LSTM’s architecture and
pre-select the three best models under a small pa-
rameter budget (5M), and then we tune these three
best models’ hyperparameters under a larger bud-
get (20M).
Pre-selection: We ﬁx dLM (hidden layer size of
the word-level LSTM) at 300 units per layer and
run each syllable-aware word embedding method
from Section 3 on the English PTB data set (Mar-
cus et al., 1993), keeping the total parameter bud-
get at 5M. The architectural choices are speciﬁed
in Appendix A.
Hyperparameter tuning: The hyperparameters
of the three best-performing models from the pre-
selection step are then thoroughly tuned on the
same English PTB data through a random search
according to the marginal distributions:

• dS ∼ U (20, 650),5
• log(dHW) ∼ U (log(160), log(2000)),
• log(dLM) ∼ U (log(300), log(2000)),

with the restriction dS < dLM. The total parameter
budget is kept at 20M to allow for easy comparison
to the results of Kim et al. (2016). Then these three
best models (with their hyperparameters tuned on
PTB) are trained and evaluated on small- (DATA-
S) and medium-sized (DATA-L) data sets in six
languages.

Pr(wk+1|w1:k) = softmax(hkW + b),

5U (a, b) stands for a uniform distribution over (a, b).

Model
LSTM-Word
Syl-LSTM
Syl-CNN-2
Syl-CNN-3
Syl-CNN-4
Syl-Sum

PPL Model
88.0
88.7
86.6
84.6
86.8
84.6

Char-CNN
Syl-Avg
Syl-Avg-A
Syl-Avg-B
Syl-Concat

PPL
92.3
88.5
91.4
88.5
83.7

Table 1: Pre-selection results. PPL stands for test
set perplexity, all models have ≈ 5M parameters.

Model
Syl-CNN
Syl-Sum
Syl-Concat

dS
242
438
228

dHW
1170
1256
781

dLM
380
435
439

Size
PPL
15M 80.5
18M 80.3
13M 79.4

Table 2: Hyperparameters tuning.
In Syl-CNN,
dHW is a function of the primary hyperparameter
c = 195 (see Appendix A).

Optimizaton is performed in almost the same way
as in the work of Zaremba et al. (2014). See Ap-
pendix B for details.
Syllabiﬁcation: The true syllabiﬁcation of a word
requires its grapheme-to-phoneme conversion and
then splitting it into syllables based on some rules.
Since these are not always available for less-
resourced languages, we decided to utilize Liang’s
widely-used hyphenation algorithm (Liang, 1983).

6 Results

The results of the pre-selection are reported in
Table 1. All syllable-aware models comfortably
outperform the Char-CNN when the budget is
limited to 5M parameters. Surprisingly, a pure
word-level model,6 LSTM-Word, also beats the
character-aware one under such budget. The three
best conﬁgurations are Syl-Concat, Syl-Sum, and
Syl-CNN-3 (hereinafter referred to as Syl-CNN),
and tuning their hyperparameters under 20M pa-
rameter budget gives the architectures in Table
2. The results of evaluating these three models
on small (1M tokens) and medium-sized (17M–
57M tokens) data sets against Char-CNN for dif-
The
ferent languages are provided in Table 3.
models demonstrate similar performance on small
data, but Char-CNN scales signiﬁcantly better on
medium-sized data. From the three syllable-aware
models, Syl-Concat looks the most advantageous
as it demonstrates stable results and has the least

6When words are directly embedded into RdW through an

embedding matrix EW ∈ R|W|×dW .

7Syl-CNN results on DATA-L are not reported since com-
putational resources were insufﬁcient to run these conﬁgura-
tions.

Model
EN
Char-CNN 78.9
80.5
Syl-CNN
80.3
Syl-Sum
Syl-Concat
79.4
Char-CNN 160
Syl-CNN7
–
170
Syl-Sum
176
Syl-Concat

FR
184
191
193
188
124
–
141
139

ES DE CS RU
261
165
269
172
273
170
265
168
190
118
–
–
233
129
225
129

239
239
243
244
198
–
212
225

371
374
389
383
392
–
451
449

S
-
A
T
A
D

L
-
A
T
A
D

Table 3: Evaluation of the syllable-aware mod-
els against Char-CNN. In each case the smallest
model, Syl-Concat, has 18%–33% less parameters
than Char-CNN and is trained 1.2–2.2 times faster
(Appendix C).

number of parameters. Therefore in what follows
we will make a more detailed comparison of Syl-
Concat with Char-CNN.
Shared errors: It is interesting to see whether
Char-CNN and Syl-Concat are making similar er-
rors. We say that a model gives an error if it as-
signs a probability less than p∗ to a correct word
from the test set. Figure 2 shows the percentage of
errors which are shared by Syl-Concat and Char-
CNN depending on the value of p∗. We see that the

Figure 2: Percentage of errors shared by both
Syl-Concat and Char-CNN on DATA-S (left) and
DATA-L (right).

vast majority of errors are shared by both models
even when p∗ is small (0.01).
PPL breakdown by token frequency: To ﬁnd
out how Char-CNN outperforms Syl-Concat, we
partition the test sets on token frequency, as com-
puted on the training data. We can observe in Fig-
ure 3 that, on average, the more frequent the word
is, the bigger the advantage of Char-CNN over
Syl-Concat. The more Char-CNN sees a word
in different contexts, the more it can learn about
this word (due to its powerful CNN ﬁlters). Syl-
Concat, on the other hand, has limitations – it can-
not see below syllables, which prevents it from ex-
tracting the same amount of knowledge about the
word.

Model
RHN-Char-CNN
RHN-Syl-Concat
RHN-Syl-Concat

depth
8
8
8

dLM Size
650
439
650

PPL
20M 67.6
13M 72.0
20M 69.4

Table 5: Replacing LSTM with Variational RHN.

Lagus, 2007) in its default conﬁguration on the
PTB training data and used it instead of the syl-
labiﬁer in our models. Interestingly, we got ≈3K
unique morphemes, whereas the number of unique
syllables was ≈6K. We then trained all our models
on PTB under 5M parameter budget, keeping the
state size of the word-level LSTM at 300 (as in our
pre-selection step for syllable-aware models). The
reduction in number of subword types allowed us
to give them higher dimensionality dM = 100 (cf.
dS = 50).9

Convolutional

(Morph-CNN-3) and additive
(Morph-Sum) models performed better than oth-
ers with test set PPLs 83.0 and 83.9 respectively.
Due to limited amount of time, we did not perform
a thorough hyperparameter search under 20M bud-
get. Instead, we ran two conﬁgurations for Morph-
CNN-3 and two conﬁgurations for Morph-Sum
with hyperparameters close to those, which were
optimal for Syl-CNN-3 and Syl-Sum correspond-
ingly. All told, our best morpheme-aware model
is Morph-Sum with dM = 550, dHW = 1100,
dLM = 550, and test set PPL 79.5, which is
practically the same as the result of our best
syllable-aware model Syl-Concat (79.4). This
makes Morph-Sum a notable alternative to Char-
CNN and Syl-Concat, and we defer its thorough
study to future work.
Source code: The source code for the mod-
els discussed in this paper
is available at
https://github.com/zh3nis/lstm-syl.

7 Conclusion

It seems that syllable-aware language models fail
to outperform competitive character-aware ones.
However, usage of syllabiﬁcation can reduce the
total number of parameters and increase the train-
the expense of language-
ing speed, albeit at
dependent preprocessing. Morphological segmen-
tation is a noteworthy alternative to syllabiﬁcation:
a simple morpheme-aware model which sums
morpheme embeddings looks promising, and its
study is deferred to future work.

9M stands for morphemes.

Figure 3: PPL reduction by token frequency, Char-
CNN relative to Syl-Concat on DATA-L.

Model
Char-CNN 568
515
Syl-Concat

80% 90% 95% 99%
1038
1035

893
875

762
729

Table 4: Number of principle components when
PCA is applied to word embeddings produced by
each model, depending on % of variance to retain.

PCA of word embeddings: The intrinsic advan-
tage of Char-CNN over Syl-Concat is also sup-
ported by the following experiment: We took word
embeddings produced by both models on the En-
glish PTB, and applied PCA to them.8 Regard-
less of the threshold percentage of variance to re-
tain, the embeddings from Char-CNN always have
more principal components than the embeddings
from Syl-Concat (see Table 4). This means that
Char-CNN embeds words into higher dimensional
space than Syl-Concat, and thus can better distin-
guish them in different contexts.
LSTM limitations: During the hyperparameters
tuning we noticed that increasing dS, dHW and
dLM from the optimal values (in Table 2) did not
result in better performance for Syl-Concat. Could
it be due to the limitations of the word-level LSTM
(the topmost layer in Fig. 1)? To ﬁnd out whether
this was the case we replaced the LSTM by a
Variational RHN (Zilly et al., 2017), and that re-
sulted in a signiﬁcant reduction of perplexities on
PTB for both Char-CNN and Syl-Concat (Table
5). Moreover, increasing dLM from 439 to 650 did
result in better performance for Syl-Concat. Opti-
mization details are given in Appendix B.
Comparing syllable and morpheme embed-
dings: It is interesting to compare morphemes and
syllables. We trained Morfessor 2.0 (Creutz and

8We equalized highway layer sizes dHW in both models to
have same dimensions for embeddings. In both cases, word
vectors were standardized using the z-score transformation.

A Pre-selection

In all models with highway layers there are two of
them and the non-linear activation of any highway
layer is a ReLU.
LSTM-Word: dW = 108, dLM = 300.
Syl-LSTM: dS = 50, dLM = 300.
Syl-CNN-[L]: dS = 50, convolutional ﬁlter
widths are [1, . . . , L], the corresponding convolu-
tional ﬁlter depths are [c·l]L
l=1, dHW = c·(1+. . .+
L). We experimented with L = 2, 3, 4. The corre-
sponding values of c are chosen to be 120, 60, 35
to ﬁt the total parameter budget. CNN activation
is tanh.
Linear combinations: We give higher dimen-
sionality to syllable vectors here (compared to
other models) since the resulting word vector will
have the same size as syllable vectors (see (3)).
dS = 175, dHW = 175 in all models except the
Syl-Avg-B, where we have dS = 160, dHW =
160.
Syl-Concat: dS = 50, dHW = 300.

B Optimization

LSTM-based models: We perform the training
(5) by truncated BPTT (Werbos, 1990; Graves,
2013). We backpropagate for 70 time steps on
DATA-S and for 35 time steps on DATA-L using
stochastic gradient descent where the learning rate
is initially set to 1.0 and halved if the perplex-
ity does not decrease on the validation set after
an epoch. We use batch sizes of 20 for DATA-S
and 100 for DATA-L. We train for 50 epochs on
DATA-S and for 25 epochs on DATA-L, picking
the best-performing model on the validation set.
Parameters of the models are randomly initialized
uniformly in [−0.05, 0.05], except the forget bias
of the word-level LSTM, which is initialized to
1. For regularization we use dropout (Srivastava
et al., 2014) with probability 0.5 between word-
level LSTM layers and on the hidden-to-output
softmax layer. We clip the norm of the gradi-
ents (normalized by minibatch size) at 5. These
choices were guided by previous work on word-
level language modeling with LSTMs (Zaremba
et al., 2014).

To speed up training on DATA-L we use a sam-
pled softmax (Jean et al., 2015) with the number
of samples equal to 20% of the vocabulary size
(Chen et al., 2016). Although Kim et al. (2016)
used a hierarchical softmax (Morin and Bengio,
2005) for the same purpose, a recent study (Grave

et al., 2016) shows that it is outperformed by sam-
pled softmax on the Europarl corpus, from which
DATA-L was derived (Botha and Blunsom, 2014).
RHN-based models are optimized as in Zilly et al.
(2017), except that we unrolled the networks for
70 time steps in truncated BPTT, and dropout rates
were chosen to be as follows: 0.2 for the embed-
ding layer, 0.7 for the input to the gates, 0.7 for the
hidden units and 0.2 for the output activations.

C Sizes and speeds

On DATA-S, Syl-Concat has 28%–33% fewer pa-
rameters than Char-CNN, and on DATA-L the re-
duction is 18%–27% (see Fig. 4).

Figure 4: Model sizes on DATA-S (left) and
DATA-L, in millions of trainable variables.

Training speeds are provided in the Table 6. Mod-
els were implemented in TensorFlow, and were
run on NVIDIA Titan X (Pascal).

EN FR ES DE CS RU
Model
6
9
Char-CNN
9
Syl-Concat
14
4
Char-CNN 10
5
22
Syl-Concat

6
10
7
10

8
12
8
13

7
11
5
6

8
12
7
13

S

L

Table 6: Training speeds, in thousands of tokens
per second.

Acknowledgements

We gratefully acknowledge the NVIDIA Corpo-
ration for their donation of the Titan X Pascal
GPU used for this research. The work of Bag-
dat Myrzakhmetov has been funded by the Com-
mittee of Science of the Ministry of Education
and Science of the Republic of Kazakhstan un-
der the targeted program O.0743 (0115PK02473).
The authors would like to thank anonymous re-
viewers and Aibek Makazhanov for valuable feed-
back, Makat Tlebaliyev and Dmitriy Polynin for
IT support, and Yoon Kim for providing the pre-
processed datasets.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781.

Tom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
Subword language modeling with neu-
2012.
(http://www. ﬁt. vutbr.
ral networks.
cz/imikolov/rnnlm/char. pdf).

preprint

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of AISTATS.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In Proceedings of COL-
ING.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of ACL.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Rupesh K Srivastava, Klaus Greff, and J¨urgen Schmid-
huber. 2015. Training very deep networks. In Pro-
ceedings of NIPS.

Clara Vania and Adam Lopez. 2017. From characters
to words to in between: Do we capture morphology?
In Proceedings of ACL.

Lyan Verwimp, Joris Pelemans, Patrick Wambacq,
et al. 2017. Character-word lstm language models.
In Proceedings of EACL.

Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10).

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan
Koutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent
highway networks. In Proceedings of ICML.

Barret Zoph and Quoc V Le. 2017. Neural architecture
search with reinforcement learning. In Proceedings
of ICLR.

References

Jan Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proceedings of ICML.

Wenlin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proceedings of ACL.

Ryan Cotterell and Hinrich Sch¨utze. 2015. Morpho-
logical word-embeddings. In Proceedings of HLT-
NAACL.

Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of NIPS.

Edouard Grave, Armand Joulin, Moustapha Ciss´e,
David Grangier, and Herv´e J´egou. 2016. Efﬁcient
arXiv preprint
softmax approximation for gpus.
arXiv:1609.04309.

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
Proceedings of ACL-IJCNLP.

Rafal

Jozefowicz, Wojciech Zaremba,

and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of ICML.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI.

Franklin Mark Liang. 1983. Word Hy-phen-a-tion by

Com-put-er. Citeseer.

Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, and Sil-
vio Amir. 2015a. Not all contexts are created equal:
Better word representations with variable attention.
In Proceedings of EMNLP.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015b. Finding function in form:
Compositional character models for open vocab-
In Proceedings of
ulary word representation.
EMNLP.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Syllable-aware Neural Language Models:
A Failure to Beat Character-aware Ones

Zhenisbek Assylbekov
School of Science and Technology
Nazarbayev University
zhassylbekov@nu.edu.kz

Rustem Takhanov
School of Science and Technology
Nazarbayev University
rustem.takhanov@nu.edu.kz

Bagdat Myrzakhmetov
National Laboratory Astana
Nazarbayev University
bagdat.myrzakhmetov@nu.edu.kz

Jonathan N. Washington
Linguistics Department
Swarthmore College
jonathan.washington
@swarthmore.edu

7
1
0
2
 
l
u
J
 
0
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
8
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Abstract

Syllabiﬁcation does not seem to im-
prove word-level RNN language model-
ing quality when compared to character-
based segmentation. However, our best
syllable-aware language model, achieving
performance comparable to the competi-
tive character-aware model, has 18%–33%
fewer parameters and is trained 1.2–2.2
times faster.

1

Introduction

Recent advances in neural
language modeling
(NLM) are connected with character-aware mod-
els (Kim et al., 2016; Ling et al., 2015b; Verwimp
et al., 2017). This is a promising approach, and
we propose the following direction related to it:
We would like to make sure that in the pursuit of
the most ﬁne-grained representations one has not
missed possible intermediate ways of segmenta-
tion, e.g., by syllables. Syllables, in our opinion,
are better supported as linguistic units of language
than single characters. In most languages, words
can be naturally split into syllables:

ES: el par-la-men-to a-po-y´o la en-mien-da
RU: par-la-ment pod-der-ˇzal po-prav-ku
(EN: the parliament supported the amendment)

Based on this observation, we attempted to de-
termine whether syllable-aware NLM has any ad-
vantages over character-aware NLM. We exper-
imented with a variety of models but could not
ﬁnd any evidence to support this hypothesis: split-
ting words into syllables does not seem to improve
the language modeling quality when compared to
splitting into characters. However, there are some
positive ﬁndings: while our best syllable-aware

language model achieves performance comparable
to the competitive character-aware model, it has
18%–33% fewer parameters and is 1.2–2.2 times
faster to train.

2 Related Work

Much research has been done on subword-level
and subword-aware1 neural language modeling
when subwords are characters (Ling et al., 2015b;
Kim et al., 2016; Verwimp et al., 2017) or mor-
phemes (Botha and Blunsom, 2014; Qiu et al.,
2014; Cotterell and Sch¨utze, 2015). However,
not much work has been done on syllable-level or
syllable-aware NLM. Mikolov et al. (2012) show
that subword-level language models outperform
character-level ones.2 They keep the most fre-
quent words untouched and split all other words
into syllable-like units. Our approach differs
mainly in the following aspects: we make predic-
tions at the word level, use a more linguistically
sound syllabiﬁcation algorithm, and consider a va-
riety of more advanced neural architectures.

We have recently come across a concurrent pa-
per (Vania and Lopez, 2017) where the authors
systematically compare different subword units
(characters, character trigrams, BPE (Sennrich
et al., 2016), morphemes) and different represen-
tation models (CNN, Bi-LSTM, summation) on
languages with various morphological typology.
However, they do not consider syllables, and they
experiment with relatively small models on small
data sets (0.6M–1.4M tokens).

1Subword-level LMs rely on subword-level inputs and
make predictions at the level of subwords; subword-aware
LMs also rely on subword-level inputs but make predictions
at the level of words.

2Not to be confused with character-aware ones, see the

previous footnote.

unconstitutional conditions on

Highway layers (optional)

Syllable-aware word embedding model

stack of two
LSTMs

word vector

Syllable
embeddings

un con sti

tu tional

imposes unconstitutional conditions

Figure 1: Syllable-aware language model.

3 Syllable-aware word embeddings

Let W and S be ﬁnite vocabularies of words and
syllables respectively. We assume that both words
and syllables have already been converted into in-
dices. Let ES ∈ R|S|×dS be an embedding ma-
trix for syllables — i.e., it is a matrix in which the
sth row (denoted as s) corresponds to an embed-
ding of the syllable s ∈ S. Any word w ∈ W is
a sequence of its syllables (s1, s2, . . . , snw ), and
hence can be represented as a sequence of the cor-
responding syllable vectors:

[s1, s2, . . . , snw ].

(1)

The question is: How shall we pack the sequence
(1) into a single vector x ∈ RdW to produce a
better embedding of the word w?3
In our case
“better” means “better than a character-aware em-
bedding of w via the Char-CNN model of Kim
et al. (2016)”. Below we present several viable
approaches.

3.1 Recurrent sequential model (Syl-LSTM)

Since the syllables are coming in a sequence it is
natural to try a recurrent sequential model:

ht = f (st, ht−1),

h0 = 0,

(2)

which converts the sequence of syllable vectors (1)
into a sequence of state vectors h1:nw . The last

3The same question applies to any model that segments

words into a sequence of characters or other subword units.

state vector hnw is assumed to contain the infor-
mation on the whole sequence (1), and is there-
fore used as a word embedding for w. There is
a big variety of transformations from which one
can choose f in (2); however, a recent thorough
evaluation (Jozefowicz et al., 2015) shows that the
LSTM (Hochreiter and Schmidhuber, 1997) with
its forget bias initialized to 1 outperforms other
popular architectures on almost all tasks, and we
decided to use it for our experiments. We will re-
fer to this model as Syl-LSTM.

3.2 Convolutional model (Syl-CNN)

Inspired by recent work on character-aware neural
language models (Kim et al., 2016) we decided to
try this approach (Char-CNN) on syllables. Our
case differs mainly in the following two aspects:

1. The set of syllables S is usually bigger than
the set of characters C,4 and also the dimen-
sionality dS of syllable vectors is expected to
be greater than the dimensionality dC of char-
acter vectors. Both of these factors result in
allocating more parameters on syllable em-
beddings compared to character embeddings.
2. On average a word contains fewer syllables
than characters, and therefore we need nar-
rower convolutional ﬁlters for syllables. This
results in spending fewer parameters per con-
volution.

This means that by varying dS and the maximum
width of convolutional ﬁlters L we can still ﬁt the
parameter budget of Kim et al. (2016) to allow fair
comparison of the models.

Like in Char-CNN, our syllable-aware model,
which is referred to as Syl-CNN-[L], utilizes max-
pooling and highway layers (Srivastava et al.,
2015) to model interactions between the syllables.
The dimensionality of a highway layer is denoted
by dHW.

3.3 Linear combinations

We also considered using linear combinations of
syllable-vectors to represent the word embedding:

x = (cid:80)nw

t=1 αt(st) · st.

(3)

The choice for αt is motivated mainly by the ex-
isting approaches (discussed below) which proved
to be successful for other tasks.
Syl-Sum: Summing up syllable vectors to get a
word vector can be obtained by setting αt(st) = 1.

4In languages with alphabetic writing systems.

This approach was used by Botha and Blunsom
(2014) to combine a word and its morpheme em-
beddings into a single word vector.
Syl-Avg: A simple average of syllable vectors can
be obtained by setting αt(st) = 1/nw. This can
be also called a “continuous bag of syllables” in an
analogy to a CBOW model (Mikolov et al., 2013),
where vectors of neighboring words are averaged
to get a word embedding of the current word.
Syl-Avg-A: We let the weights αt in (3) be a
function of parameters (a1, . . . , an) of the model,
which are jointly trained together with other pa-
rameters. Here n = maxw{nw} is a maxi-
mum word length in syllables.
In order to have
a weighted average in (3) we apply a softmax nor-
malization:

αt = softmax(a)t =

exp(at)
τ =1 exp(aτ )

(cid:80)n

(4)

Syl-Avg-B: We can let αt depend on syllables and
their positions:

αt = αt(st) = softmax(ast + b)t
where A ∈ RdS ×n (with elements as,t) is a set of
parameters that determine the importance of each
syllable type in each (relative) position, b ∈ Rn
is a bias, which is conditioned only on the rela-
tive position. This approach is motivated by re-
cent work on using an attention mechanism in the
CBOW model (Ling et al., 2015a).

We feed the resulting x from (3) into a stack of
highway layers to allow interactions between the
syllables.

3.4 Concatenation (Syl-Concat)

In this model we simply concatenate syllable vec-
tors (1) into a single word vector:

x = [s1; s2; . . . ; snw ; 0; 0; . . . ; 0
]
(cid:125)

(cid:124)

(cid:123)(cid:122)
n−nw

We zero-pad x so that all word vectors have the
same length n · dS to allow batch processing, and
then we feed x into a stack of highway layers.

4 Word-level language model

Once we have word embeddings x1:k for a se-
quence of words w1:k we can use a word-level
RNN language model to produce a sequence of
states h1:k and then predict the next word accord-
ing to the probability distribution

where W ∈ RdLM×|W|, b ∈ R|W|, and dLM is the
hidden layer size of the RNN. Training the model
involves minimizing the negative log-likelihood
over the corpus w1:K:
− (cid:80)K

k=1 log Pr(wk|w1:k−1) −→ min

(5)

As was mentioned in Section 3.1 there is a huge
variety of RNN architectures to choose from. The
most advanced recurrent neural architectures, at
the time of this writing, are recurrent highway
networks (Zilly et al., 2017) and a novel model
which was obtained through a neural architecture
search with reinforcement learning (Zoph and Le,
2017). These models can be spiced up with the
most recent regularization techniques for RNNs
(Gal and Ghahramani, 2016) to reach state-of-the-
art. However, to make our results directly com-
parable to those of Kim et al. (2016) we select a
two-layer LSTM and regularize it as in Zaremba
et al. (2014).

5 Experimental Setup

We search for the best model in two steps: ﬁrst,
we block the word-level LSTM’s architecture and
pre-select the three best models under a small pa-
rameter budget (5M), and then we tune these three
best models’ hyperparameters under a larger bud-
get (20M).
Pre-selection: We ﬁx dLM (hidden layer size of
the word-level LSTM) at 300 units per layer and
run each syllable-aware word embedding method
from Section 3 on the English PTB data set (Mar-
cus et al., 1993), keeping the total parameter bud-
get at 5M. The architectural choices are speciﬁed
in Appendix A.
Hyperparameter tuning: The hyperparameters
of the three best-performing models from the pre-
selection step are then thoroughly tuned on the
same English PTB data through a random search
according to the marginal distributions:

• dS ∼ U (20, 650),5
• log(dHW) ∼ U (log(160), log(2000)),
• log(dLM) ∼ U (log(300), log(2000)),

with the restriction dS < dLM. The total parameter
budget is kept at 20M to allow for easy comparison
to the results of Kim et al. (2016). Then these three
best models (with their hyperparameters tuned on
PTB) are trained and evaluated on small- (DATA-
S) and medium-sized (DATA-L) data sets in six
languages.

Pr(wk+1|w1:k) = softmax(hkW + b),

5U (a, b) stands for a uniform distribution over (a, b).

Model
LSTM-Word
Syl-LSTM
Syl-CNN-2
Syl-CNN-3
Syl-CNN-4
Syl-Sum

PPL Model
88.0
88.7
86.6
84.6
86.8
84.6

Char-CNN
Syl-Avg
Syl-Avg-A
Syl-Avg-B
Syl-Concat

PPL
92.3
88.5
91.4
88.5
83.7

Table 1: Pre-selection results. PPL stands for test
set perplexity, all models have ≈ 5M parameters.

Model
Syl-CNN
Syl-Sum
Syl-Concat

dS
242
438
228

dHW
1170
1256
781

dLM
380
435
439

Size
PPL
15M 80.5
18M 80.3
13M 79.4

Table 2: Hyperparameters tuning.
In Syl-CNN,
dHW is a function of the primary hyperparameter
c = 195 (see Appendix A).

Optimizaton is performed in almost the same way
as in the work of Zaremba et al. (2014). See Ap-
pendix B for details.
Syllabiﬁcation: The true syllabiﬁcation of a word
requires its grapheme-to-phoneme conversion and
then splitting it into syllables based on some rules.
Since these are not always available for less-
resourced languages, we decided to utilize Liang’s
widely-used hyphenation algorithm (Liang, 1983).

6 Results

The results of the pre-selection are reported in
Table 1. All syllable-aware models comfortably
outperform the Char-CNN when the budget is
limited to 5M parameters. Surprisingly, a pure
word-level model,6 LSTM-Word, also beats the
character-aware one under such budget. The three
best conﬁgurations are Syl-Concat, Syl-Sum, and
Syl-CNN-3 (hereinafter referred to as Syl-CNN),
and tuning their hyperparameters under 20M pa-
rameter budget gives the architectures in Table
2. The results of evaluating these three models
on small (1M tokens) and medium-sized (17M–
57M tokens) data sets against Char-CNN for dif-
The
ferent languages are provided in Table 3.
models demonstrate similar performance on small
data, but Char-CNN scales signiﬁcantly better on
medium-sized data. From the three syllable-aware
models, Syl-Concat looks the most advantageous
as it demonstrates stable results and has the least

6When words are directly embedded into RdW through an

embedding matrix EW ∈ R|W|×dW .

7Syl-CNN results on DATA-L are not reported since com-
putational resources were insufﬁcient to run these conﬁgura-
tions.

Model
EN
Char-CNN 78.9
80.5
Syl-CNN
80.3
Syl-Sum
Syl-Concat
79.4
Char-CNN 160
Syl-CNN7
–
170
Syl-Sum
176
Syl-Concat

FR
184
191
193
188
124
–
141
139

ES DE CS RU
261
165
269
172
273
170
265
168
190
118
–
–
233
129
225
129

239
239
243
244
198
–
212
225

371
374
389
383
392
–
451
449

S
-
A
T
A
D

L
-
A
T
A
D

Table 3: Evaluation of the syllable-aware mod-
els against Char-CNN. In each case the smallest
model, Syl-Concat, has 18%–33% less parameters
than Char-CNN and is trained 1.2–2.2 times faster
(Appendix C).

number of parameters. Therefore in what follows
we will make a more detailed comparison of Syl-
Concat with Char-CNN.
Shared errors: It is interesting to see whether
Char-CNN and Syl-Concat are making similar er-
rors. We say that a model gives an error if it as-
signs a probability less than p∗ to a correct word
from the test set. Figure 2 shows the percentage of
errors which are shared by Syl-Concat and Char-
CNN depending on the value of p∗. We see that the

Figure 2: Percentage of errors shared by both
Syl-Concat and Char-CNN on DATA-S (left) and
DATA-L (right).

vast majority of errors are shared by both models
even when p∗ is small (0.01).
PPL breakdown by token frequency: To ﬁnd
out how Char-CNN outperforms Syl-Concat, we
partition the test sets on token frequency, as com-
puted on the training data. We can observe in Fig-
ure 3 that, on average, the more frequent the word
is, the bigger the advantage of Char-CNN over
Syl-Concat. The more Char-CNN sees a word
in different contexts, the more it can learn about
this word (due to its powerful CNN ﬁlters). Syl-
Concat, on the other hand, has limitations – it can-
not see below syllables, which prevents it from ex-
tracting the same amount of knowledge about the
word.

Model
RHN-Char-CNN
RHN-Syl-Concat
RHN-Syl-Concat

depth
8
8
8

dLM Size
650
439
650

PPL
20M 67.6
13M 72.0
20M 69.4

Table 5: Replacing LSTM with Variational RHN.

Lagus, 2007) in its default conﬁguration on the
PTB training data and used it instead of the syl-
labiﬁer in our models. Interestingly, we got ≈3K
unique morphemes, whereas the number of unique
syllables was ≈6K. We then trained all our models
on PTB under 5M parameter budget, keeping the
state size of the word-level LSTM at 300 (as in our
pre-selection step for syllable-aware models). The
reduction in number of subword types allowed us
to give them higher dimensionality dM = 100 (cf.
dS = 50).9

Convolutional

(Morph-CNN-3) and additive
(Morph-Sum) models performed better than oth-
ers with test set PPLs 83.0 and 83.9 respectively.
Due to limited amount of time, we did not perform
a thorough hyperparameter search under 20M bud-
get. Instead, we ran two conﬁgurations for Morph-
CNN-3 and two conﬁgurations for Morph-Sum
with hyperparameters close to those, which were
optimal for Syl-CNN-3 and Syl-Sum correspond-
ingly. All told, our best morpheme-aware model
is Morph-Sum with dM = 550, dHW = 1100,
dLM = 550, and test set PPL 79.5, which is
practically the same as the result of our best
syllable-aware model Syl-Concat (79.4). This
makes Morph-Sum a notable alternative to Char-
CNN and Syl-Concat, and we defer its thorough
study to future work.
Source code: The source code for the mod-
els discussed in this paper
is available at
https://github.com/zh3nis/lstm-syl.

7 Conclusion

It seems that syllable-aware language models fail
to outperform competitive character-aware ones.
However, usage of syllabiﬁcation can reduce the
total number of parameters and increase the train-
the expense of language-
ing speed, albeit at
dependent preprocessing. Morphological segmen-
tation is a noteworthy alternative to syllabiﬁcation:
a simple morpheme-aware model which sums
morpheme embeddings looks promising, and its
study is deferred to future work.

9M stands for morphemes.

Figure 3: PPL reduction by token frequency, Char-
CNN relative to Syl-Concat on DATA-L.

Model
Char-CNN 568
515
Syl-Concat

80% 90% 95% 99%
1038
1035

893
875

762
729

Table 4: Number of principle components when
PCA is applied to word embeddings produced by
each model, depending on % of variance to retain.

PCA of word embeddings: The intrinsic advan-
tage of Char-CNN over Syl-Concat is also sup-
ported by the following experiment: We took word
embeddings produced by both models on the En-
glish PTB, and applied PCA to them.8 Regard-
less of the threshold percentage of variance to re-
tain, the embeddings from Char-CNN always have
more principal components than the embeddings
from Syl-Concat (see Table 4). This means that
Char-CNN embeds words into higher dimensional
space than Syl-Concat, and thus can better distin-
guish them in different contexts.
LSTM limitations: During the hyperparameters
tuning we noticed that increasing dS, dHW and
dLM from the optimal values (in Table 2) did not
result in better performance for Syl-Concat. Could
it be due to the limitations of the word-level LSTM
(the topmost layer in Fig. 1)? To ﬁnd out whether
this was the case we replaced the LSTM by a
Variational RHN (Zilly et al., 2017), and that re-
sulted in a signiﬁcant reduction of perplexities on
PTB for both Char-CNN and Syl-Concat (Table
5). Moreover, increasing dLM from 439 to 650 did
result in better performance for Syl-Concat. Opti-
mization details are given in Appendix B.
Comparing syllable and morpheme embed-
dings: It is interesting to compare morphemes and
syllables. We trained Morfessor 2.0 (Creutz and

8We equalized highway layer sizes dHW in both models to
have same dimensions for embeddings. In both cases, word
vectors were standardized using the z-score transformation.

A Pre-selection

In all models with highway layers there are two of
them and the non-linear activation of any highway
layer is a ReLU.
LSTM-Word: dW = 108, dLM = 300.
Syl-LSTM: dS = 50, dLM = 300.
Syl-CNN-[L]: dS = 50, convolutional ﬁlter
widths are [1, . . . , L], the corresponding convolu-
tional ﬁlter depths are [c·l]L
l=1, dHW = c·(1+. . .+
L). We experimented with L = 2, 3, 4. The corre-
sponding values of c are chosen to be 120, 60, 35
to ﬁt the total parameter budget. CNN activation
is tanh.
Linear combinations: We give higher dimen-
sionality to syllable vectors here (compared to
other models) since the resulting word vector will
have the same size as syllable vectors (see (3)).
dS = 175, dHW = 175 in all models except the
Syl-Avg-B, where we have dS = 160, dHW =
160.
Syl-Concat: dS = 50, dHW = 300.

B Optimization

LSTM-based models: We perform the training
(5) by truncated BPTT (Werbos, 1990; Graves,
2013). We backpropagate for 70 time steps on
DATA-S and for 35 time steps on DATA-L using
stochastic gradient descent where the learning rate
is initially set to 1.0 and halved if the perplex-
ity does not decrease on the validation set after
an epoch. We use batch sizes of 20 for DATA-S
and 100 for DATA-L. We train for 50 epochs on
DATA-S and for 25 epochs on DATA-L, picking
the best-performing model on the validation set.
Parameters of the models are randomly initialized
uniformly in [−0.05, 0.05], except the forget bias
of the word-level LSTM, which is initialized to
1. For regularization we use dropout (Srivastava
et al., 2014) with probability 0.5 between word-
level LSTM layers and on the hidden-to-output
softmax layer. We clip the norm of the gradi-
ents (normalized by minibatch size) at 5. These
choices were guided by previous work on word-
level language modeling with LSTMs (Zaremba
et al., 2014).

To speed up training on DATA-L we use a sam-
pled softmax (Jean et al., 2015) with the number
of samples equal to 20% of the vocabulary size
(Chen et al., 2016). Although Kim et al. (2016)
used a hierarchical softmax (Morin and Bengio,
2005) for the same purpose, a recent study (Grave

et al., 2016) shows that it is outperformed by sam-
pled softmax on the Europarl corpus, from which
DATA-L was derived (Botha and Blunsom, 2014).
RHN-based models are optimized as in Zilly et al.
(2017), except that we unrolled the networks for
70 time steps in truncated BPTT, and dropout rates
were chosen to be as follows: 0.2 for the embed-
ding layer, 0.7 for the input to the gates, 0.7 for the
hidden units and 0.2 for the output activations.

C Sizes and speeds

On DATA-S, Syl-Concat has 28%–33% fewer pa-
rameters than Char-CNN, and on DATA-L the re-
duction is 18%–27% (see Fig. 4).

Figure 4: Model sizes on DATA-S (left) and
DATA-L, in millions of trainable variables.

Training speeds are provided in the Table 6. Mod-
els were implemented in TensorFlow, and were
run on NVIDIA Titan X (Pascal).

EN FR ES DE CS RU
Model
6
9
Char-CNN
9
Syl-Concat
14
4
Char-CNN 10
5
22
Syl-Concat

6
10
7
10

8
12
8
13

7
11
5
6

8
12
7
13

S

L

Table 6: Training speeds, in thousands of tokens
per second.

Acknowledgements

We gratefully acknowledge the NVIDIA Corpo-
ration for their donation of the Titan X Pascal
GPU used for this research. The work of Bag-
dat Myrzakhmetov has been funded by the Com-
mittee of Science of the Ministry of Education
and Science of the Republic of Kazakhstan un-
der the targeted program O.0743 (0115PK02473).
The authors would like to thank anonymous re-
viewers and Aibek Makazhanov for valuable feed-
back, Makat Tlebaliyev and Dmitriy Polynin for
IT support, and Yoon Kim for providing the pre-
processed datasets.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781.

Tom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
Subword language modeling with neu-
2012.
(http://www. ﬁt. vutbr.
ral networks.
cz/imikolov/rnnlm/char. pdf).

preprint

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of AISTATS.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In Proceedings of COL-
ING.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of ACL.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Rupesh K Srivastava, Klaus Greff, and J¨urgen Schmid-
huber. 2015. Training very deep networks. In Pro-
ceedings of NIPS.

Clara Vania and Adam Lopez. 2017. From characters
to words to in between: Do we capture morphology?
In Proceedings of ACL.

Lyan Verwimp, Joris Pelemans, Patrick Wambacq,
et al. 2017. Character-word lstm language models.
In Proceedings of EACL.

Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10).

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
Recurrent neural network regularization.

2014.
arXiv preprint arXiv:1409.2329.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan
Koutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent
highway networks. In Proceedings of ICML.

Barret Zoph and Quoc V Le. 2017. Neural architecture
search with reinforcement learning. In Proceedings
of ICLR.

References

Jan Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proceedings of ICML.

Wenlin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proceedings of ACL.

Ryan Cotterell and Hinrich Sch¨utze. 2015. Morpho-
logical word-embeddings. In Proceedings of HLT-
NAACL.

Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of NIPS.

Edouard Grave, Armand Joulin, Moustapha Ciss´e,
David Grangier, and Herv´e J´egou. 2016. Efﬁcient
arXiv preprint
softmax approximation for gpus.
arXiv:1609.04309.

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
Proceedings of ACL-IJCNLP.

Rafal

Jozefowicz, Wojciech Zaremba,

and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of ICML.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI.

Franklin Mark Liang. 1983. Word Hy-phen-a-tion by

Com-put-er. Citeseer.

Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, and Sil-
vio Amir. 2015a. Not all contexts are created equal:
Better word representations with variable attention.
In Proceedings of EMNLP.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015b. Finding function in form:
Compositional character models for open vocab-
In Proceedings of
ulary word representation.
EMNLP.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

