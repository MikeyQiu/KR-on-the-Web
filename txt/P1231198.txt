Data-Driven Tree Transforms and Metrics

Gal Mishne, Ronen Talmon, Israel Cohen, Ronald R. Coifman and Yuval Kluger

1

7
1
0
2
 
g
u
A
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
6
7
5
0
.
8
0
7
1
:
v
i
X
r
a

Abstract—We consider the analysis of high dimensional data
given in the form of a matrix with columns consisting of
observations and rows consisting of features. Often the data
is such that the observations do not reside on a regular grid,
and the given order of the features is arbitrary and does not
convey a notion of locality. Therefore, traditional transforms
and metrics cannot be used for data organization and analysis.
In this paper, our goal is to organize the data by deﬁning an
appropriate representation and metric such that they respect the
smoothness and structure underlying the data. We also aim to
generalize the joint clustering of observations and features in
the case the data does not fall into clear disjoint groups. For
this purpose, we propose multiscale data-driven transforms and
metrics based on trees. Their construction is implemented in an
iterative reﬁnement procedure that exploits the co-dependencies
between features and observations. Beyond the organization
of a single dataset, our approach enables us to transfer the
organization learned from one dataset to another and to integrate
several datasets together. We present an application to breast
cancer gene expression analysis: learning metrics on the genes to
cluster the tumor samples into cancer sub-types and validating
the joint organization of both the genes and the samples. We
demonstrate that using our approach to combine information
from multiple gene expression cohorts, acquired by different
proﬁling technologies, improves the clustering of tumor samples.

Index Terms—graph signal processing, multiscale representa-

tions, geometric analysis, partition trees, gene expression

I. INTRODUCTION

High-dimensional datasets are typically analyzed as a two-
dimensional matrix where, for example,
the rows consist
of features and the columns consist of observations. Signal
processing addresses the analysis of such data as residing on
a regular grid, such that the rows and columns are given
in a particular order,
indicating smoothness. For example,
the ordering in time-series data indicates temporal-frequency
smoothness, and the order in 2D images indicating spatial
smoothness. Non-Euclidean data that do not reside on a regular
grid, but rather on a graph, raise the more general problem of
matrix organization. In such datasets, the given ordering of the
rows (features) and columns (observations) does not indicate
any degree of smoothness.

However, in many applications, for example, analysis of
gene expression data, text documents, psychological question-

G. Mishne and R. R. Coifman are with the Department of Mathematics,
Yale University, New Haven, CT 06520 USA (e-mail: gal.mishne@yale.edu
; ronald.coifman@math.yale.edu.). R. Talmon and I. Cohen are with the
Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of
ico-
Technology, Haifa 32000,
hen@ee.technion.ac.il). Y. Kluger is with the Department of Pathology and
the Yale Cancer Center, Yale University School of Medicine, New Haven, CT
06511 USA (e-mail: yuval.kluger@yale.edu). This research was supported by
the Israel Science Foundation (grant no. 576/16), and by the United States-
Israel Binational Science Foundation and the United States National Science
Foundation (grant no. 2015582), and by the National Institutes of Health (grant
no. 1R01HG008383-01A1).

ronen@ee.technion.ac.il;

(e-mail:

Israel.

naires and recommendation systems [1]–[10], there is an un-
derlying structure to both the features and the observations. For
example, in gene expression subsets of samples (observations)
have similar genetic proﬁles, while subsets of genes (features)
have similar expressions across groups of samples. Thus, as
the observations are viewed as high-dimensional vectors of
features, one can swap the role of features and observations,
and treat the features as high-dimensional vectors of observa-
tions. This dual analysis reveals meaningful joint structures in
the data.

The problem of matrix organization considered here is
closely related to biclustering [1]–[5], [11], [12], where the
goal is to identify biclusters: joint subsets of features and
observations such that the matrix elements in each subset have
similar values. Matrix organization goes beyond the extraction
of joint clusters, yielding a joint reordering of the entire dataset
and not just the extraction of partial subsets of observations
and features that constitute bi-clusters. By recovering the
smooth joint organization of the features and observations,
one can apply signal processing and machine learning methods
such as denoising, data completion, clustering and classiﬁca-
tion, or extract meaningful patterns for exploratory analysis
and data visualization.

The application of traditional signal processing transforms
to data on graphs is not straightforward, as these transforms
rely almost exclusively on convolution with ﬁlters of ﬁnite
support, and thus are based on the assumption that the given
ordering of the data conveys smoothness. The ﬁeld of graph
signal processing adapts classical techniques to signals sup-
ported on a graph (or a network), such as ﬁltering and wavelets
in the graph domain [13]–[22]. Consider for example signals
(observations) acquired from a network of sensors (features).
The nodes of the graph are the sensors and the edges and
their weights are typically dictated by a-priori information
such as physical connectivity, geographical proximity, etc. The
samples collected from all sensors at a given time compose a
high-dimensional graph signal supported on this network. The
signal observations, acquired over time, are usually processed
separately and the connectivity between the observations is not
taken into account.

To address this issue, in this paper we propose to analyze
the data in a matrix organization setting as represented by
two graphs: one whose nodes are the observations and the
other whose nodes are the features, and our aim is a joint
unsupervised organization of these two graphs. Furthermore,
we do not ﬁx the edge weights by relying on a predetermined
structure or a-priori information. Instead, we calculate the edge
weights by taking into account the underlying dual structure
of the data and the coupling between the observations and the
features. This requires deﬁning two metrics, one between pairs
of observations and one between pairs of features.

2

Such an approach for matrix organization was introduced by
Gavish and Coifman [9], where the organization of the data
relies on the construction of a pair of hierarchical partition
trees on the observations and on the features. In previous
work [23], we extended this methodology to the organization
of a rank-3 tensor (or a 3D database), introducing a multiscale
averaging ﬁlterbank derived from partition trees.

Here we introduce a new formulation of the averaging ﬁlter-
bank as a tree-based linear transform on the data, and propose
a new tree-based difference transform. Together these yield
a multiscale representation of both the observations and the
features, in analogue to the Gaussian and Laplacian pyramid
transforms [24]. Our transforms can be seen as data-driven
multiscale ﬁlters on graphs, where in contrast to classical
signal processing, the support of the ﬁlters is non-local and
depends on the structure of the data. From the transforms,
we derive a metric in the transform space that incorporates
the multiscale structure revealed by the trees [25]. The trees
and the metrics are incorporated in an iterative bi-organization
procedure following [9]. We demonstrate that beyond the
organization of a single dataset, our metric enables us to
apply the organization learned from one dataset to another
and to integrate several datasets together. This is achieved by
generalizing our transform to a new multi-tree transform and
to a multi-tree metric, which integrate a set of multiple trees on
the features. Finally, the multi-tree transform inspires a local
reﬁnement of the partition trees, improving the bi-organization
of the data.

The remainder of the paper is organized as follows. In
Section II, we formulate the problem, present an overview of
our solution and review related background. In Section III, we
present the new tree-induced transforms and their properties.
In Section IV, we derive the metric in the transform space and
propose different extensions of the metric. We also propose a
local reﬁnement of the bi-organization approach. Section V
presents experimental results in the analysis of breast cancer
gene expression data.

non-binary tree is not uniquely deﬁned. Finally, since the
averaging transform is over-complete such that each ﬁlter
corresponds to a single folder in the tree, it is simple to design
weights on the transform coefﬁcients based on the properties
of the individual folders.

it

Filterbanks and multiscale transforms on trees and graphs
have been proposed in [18]–[21], yet differ from our ap-
proach in several aspects. While ﬁlterbanks construct a multi-
scale representation by using downsampling operators on the
data [18], [20], the multiscale nature of our transform arises
is
from partitioning of the data via the tree. In that,
most similar to [21], where the graph is decomposed into
subgraphs by partitioning. However, all these ﬁlterbanks on
graphs employ the eigen-decomposition of the graph Laplacian
to deﬁne either global ﬁlters on the full graph or local ﬁlters
on disjoint subgraphs. Our approach, conversely, employs the
eigen-decomposition of the graph Laplacian to construct the
partition tree, but the transforms (ﬁlters) are deﬁned by the
structure of the tree and not explicitly derived from the Lapla-
cian. In addition, we do not treat the structure of the graph as
ﬁxed, but rather iteratively update the Laplacian based on the
tree transform. Finally, while graph signal processing typically
addresses one dimension of the data (features or observations),
our approach addresses the construction of transforms on both
the observations and features of a dataset, and relies on the
coupling between the two to derive the transforms.

This work is also related to the matrix factorization pro-
posed by Shahid et al. [22], where the graph Laplacians of both
the features and the observation regularize the decomposition
of a dataset
into a low-rank matrix and a sparse matrix
representing noise. Then the observations are clustered using
k-means on the low-dimensional principal components of the
smooth low-rank matrix. Our work differs in that we preform
an iterative non-linear embedding of the observations and
features, not jointly, but alternating between the two while
updating the graph Laplacian of each in turn. In addition, we
provide a multiscale clustering of the data.

A. Related Work

II. BI-ORGANIZATION

Various methodologies have been proposed for the con-
struction of wavelets on graphs,
including Haar wavelets,
and wavelets based on spectral clustering and spanning tree
decompositions [13]–[16], [26], [27]. Our work deviates from
this path and presents an iterative construction of data-driven
tree-based transforms. In contrast to previous multiscale rep-
resentations of a single graph, our approach takes into account
the co-dependencies between observations and features by
incorporating two graph structures. Our motivation for the
proposed transforms is the tree-based Earth Mover’s Distance
(EMD) proposed in [25], which introduces a coupling between
observations and features, enabling an iterative procedure that
updates the trees and metrics in each iteration. The averaging
transform, in addition to being equipped with this metric, is
also easier to compute than a wavelet basis as it does not
require an orthogonalization procedure. In addition, given a
the averaging and difference transforms are
partition tree,
unique, whereas the tree-based wavelet transform [13] on a

A. Problem Formulation

Let Z be a high-dimensional dataset and let us denote its
set of nX features by X and denote its set of nY observations
by Y. For example, in gene expression data, X consists of
the genes and Y consists of individual samples. The element
Z(x, y) is the expression of gene x ∈ X in sample y ∈ Y. The
given ordering of the dataset is arbitrary such that adjacent
features and adjacent observations in the dataset are likely
dissimilar. We assume there exists a reordering of the features
and a reordering of the observations such that Z is smooth.

Deﬁnition 1: A matrix Z is smooth if it satisﬁes the mixed
H¨older condition [9], such that ∀x, x(cid:48) ∈ X and ∀y, y(cid:48) ∈ Y,
and for a pair of non-trivial metrics ρX on X and ρY on Y
and constants C > 0 and 0 < α ≤ 1:

|Z(x, y) − Z(x, y(cid:48)) − Z(x(cid:48), y) + Z(x(cid:48), y(cid:48))|

≤ CρX (x, x(cid:48))αρY (y, y(cid:48))α.

(1)

Note that we do not impose smoothness as an explicit con-
straint; instead it manifests itself implicitly in our data-driven
approach.

Although the given ordering of the dataset is not smooth, the
organization of the observations and the features by partition
trees following [9] constructs both local and global neigh-
borhoods of each feature and of each observation. Thus, the
structure of the tree organizes the data in a hierarchy of nested
clusters in which the data is smooth. Our aim is to deﬁne a
transform on the features and on the observations that conveys
the hierarchy of the trees, thus recovering the smoothness of
the data. We deﬁne a new metric in the transform space that
incorporates the hierarchical clustering of the data via the
trees. The notations in this paper follow these conventions:
matrices are denoted by bold uppercase and sets are denoted
by uppercase calligraphic.

B. Method Overview

The construction of the tree, which relies on a metric, and
the calculation of the metric, which is derived from a tree, lead
to an iterative bi-organization algorithm [9]. Each iteration
updates the pair of trees and metrics on the observations
and features as follows. First, an initial partition tree on the
features, denoted TX , is calculated based on an initial pairwise
afﬁnity between features. This initial afﬁnity is application
dependent. Based on a coarse-to-ﬁne decomposition of the
features implied by the partition tree on the features, we deﬁne
a new metric between pairs of observations: dTX (y, y(cid:48)). The
metric is then used to construct a new partition tree on the
observations TY . Thus, the construction of the tree on the
observations TY is based on a metric induced by the tree on the
features TX . The new tree on the observations TY then deﬁnes
a new metric between pairs of features dTY (x, x(cid:48)). Using this
metric, a new partition tree is constructed on the features TX ,
and a new iteration begins. Thus, this approach exploits the
strong coupling between the features and the observations.
This enables an iterative procedure in which the pair of trees
are reﬁned from iteration to iteration, providing in turn a more
accurate metric on the features and on the observations. We
will show that the resulting tree-based transform and corre-
sponding metric enable a multiscale analysis of the dataset,
reordering of the observations and features, and detection of
meaningful joint clusters in the data.

C. Partition trees

Given a dataset Z, we construct a hierarchical partitioning
of the observations and features deﬁned by a pair of trees.
Without loss of generality, we deﬁne the partition trees in this
section with respect to the features, and introduce relevant
notation.

Let TX be a partition tree on the features. The partition tree
is composed of L + 1 levels, where a partition Pl is deﬁned
for each level 0 ≤ l ≤ L. The partition Pl = {Il,1, ..., Il,n(l)}
at level l consists of n(l) mutually disjoint non-empty subsets
of indices in {1, ..., nX }, termed folders and denoted by Il,i,
i ∈ {1, ..., n(l)}. Note that we deﬁne the folders on the indices
of the set and not on the features themselves.

3

The partition tree TX has the following properties:
• The ﬁnest partition (l = 0) is composed of n(0) = nX
singleton folders, termed the “leaves”, where I0,i = {i}.
• The coarsest partition (l = L) is composed of a single
folder, PL = IL,1 = {1, ..., nX }, termed the “root”.
• The partitions are nested such that if I ∈ Pl, then I ⊆ J
for some J ∈ Pl+1, i.e., each folder at level l − 1 is a
subset of a folder from level l.

The partition tree is the set of all folders at all levels TX =
{Il,i | 0 ≤ l ≤ L, 1 ≤ i ≤ n(l)}, and the number of all
folders in the tree is denoted by NX = |TX |. The size, or
cardinality, of a folder I, i.e. the number of indices in that
folder, is denoted by |I|. In the remainder of the paper, for
compactness, we drop the subscript l denoting the level of a
folder, and denote a single folder by either I or Ii, such that
i ∈ {1, ..., NX } is an index over all folders in the tree.

Given a dataset, there are many methods to construct a
partition tree, including deterministic, random, agglomerative
(bottom-up) and divisive (top-down) [5], [13], [28]. For ex-
ample, in a bottom-up approach, we begin at the lowest level
of the tree and cluster the features into small folders. These
folders are then clustered into larger folders at higher levels
of the tree, until all folders are merged together at the root.

Some approaches take into account the geometric structure
and multiscale nature of the data by incorporating afﬁnity ma-
trices deﬁned on the data, and manifold embeddings [10], [13].
Ankenman [10] proposed “ﬂexible trees”, whose construction
requires an afﬁnity kernel deﬁned on the data, and is based on a
low-dimensional diffusion embedding of the data [29]. Given
a metric between features d(x, x(cid:48)), a local pairwise afﬁnity
kernel k(x, x(cid:48)) = exp{−d(x, x(cid:48))/σ2} is integrated into a
global representation on the data via a manifold embedding
representation Ψ, which minimizes

min

k(x, x(cid:48))(cid:107)Ψ(x) − Ψ(x(cid:48))(cid:107)2
2.

(2)

(cid:88)

x,x(cid:48)

The clustering of the folders in the ﬂexible tree algorithm is
based on the Euclidean distance between the embedding Ψ
of the features, which integrates the original metric d(x, x(cid:48)).
Thus, the construction of the tree does not rely directly on the
high-dimensional features but on the low-dimensional geomet-
ric representation underlying the data (see [10] for a detailed
description). The quality of this representation, and therefore,
of the constructed tree depends on the metric d(x, x(cid:48)). In our
approach, we propose to use the metric induced by the tree
on the observations d(x, x(cid:48)) = dTY (x, x(cid:48)). This introduces a
coupling between the observations and the features, as the
tree construction of one depends on the tree of the other.
Since our approach is based on an iterative procedure, the
tree construction is reﬁned from iteration to iteration, as both
the tree and the metric on the features are updated based
on the organization of the observations, and vice versa. This
also updates the afﬁnity kernel between observations and the
afﬁnity kernel between features, therefore updating the dual
graph structure of the dataset.

Note that while we apply ﬂexible trees in our experimental
results, the bi-organization approach is modular and different
tree construction algorithms can be applied, as in [9], [30].

While the deﬁnition of the proposed transforms and metrics
does not depend on properties of the ﬂexible trees algorithm,
the resulting bi-organization does depend on the tree con-
struction. Spin-cycling (averaging results over multiple trees)
as in [10] can be applied to stabilize the results. Instead,
we propose an iterative reﬁnement procedure that makes the
tree constructions.
algorithm less dependent on the initial
Convergence guarantees to smooth results from a family of
appropriate initial trees are lacking. This will be the subject
of future work.

III. TREE TRANSFORMS

Given partition trees TX and TY , deﬁned on the features
and observations, respectively, we propose several transforms
induced by the partition trees, which are deﬁned by a linear
transform matrix and generalizes the method proposed in [10].
In the following we focus on the feature set X , but the same
deﬁnitions and constructions apply to the observation set Y.
Note that while the proposed transforms are linear, the support
of the transform elements is derived in a non-linear manner
as it depends on the tree construction.

i.e.

The proposed transforms project the data onto a high di-
mensional space whose dimensionality is equal to the number
of folders in the tree, denoted by NX ,
the transform
maps T : RnX → RNX . Each transform is represented as
a matrix of size NX × nX , where nX is the number of
features. We denote the row indices of the transform matrices
by i, j ∈ {1, 2, ..., NX } indicating the unique index of the
folder in TX . We denote the column indices of the transform
matrices by x, x(cid:48) ∈ X (y, y(cid:48) ∈ Y), which are the indices
of the features (observations) in the data. We deﬁne 1I to
be the indicator function on the features x ∈ {1, ..., nX }
belonging to folder I ∈ TX . Tree transforms obtained from
TX are applied to the dataset as ˆZX = TX Z and tree
transforms obtained from TY are applied to the dataset as
ˆZY = ZTT
Y . We begin with transforms induced by a tree in
a single dimension (features or observations) analogously to a
typical one-dimensional linear transform. We then extend these
transforms to joint-tree transforms induced by a pair of trees
{TX , TY } on the observations and the features, analogously to
a two-dimensional linear transform. Finally, we propose multi-
tree transforms in the case that we have more than one tree
in a single dimension, for example we have constructed a set
of trees {TX } on the features X , each constructed from a
different dataset consisting of different observations with the
same features.

A. Averaging transform

4

(5)

(6)

Applying S to an observation vector y ∈ RnX yields a vector
of length NX where each element i ∈ {1, ..., NX } is the sum
of the elements y(x) for x ∈ Ii:

(Sy)[i] =

y(x)1Ii(x) =

y(x)

(4)

(cid:88)

x∈X

(cid:88)

x∈Ii

The sum of each row of S is the size of its corresponding
folder: (cid:80)
x S[i, x] = |Ii|. The sum of each column is the
number of levels in TX : (cid:80)
i S[i, x] = L + 1, since the folders
are disjoint at each level such that each feature belongs only
to a single folder at each level.

From S we derive the averaging transform denoted by M.
Let D ∈ RNX ×NX be a diagonal matrix whose elements are
the cardinality of each folder: D[i, i] = |Ii|. We calculate
M ∈ RNX ×nX by normalizing the rows of S, so the sum of
each row is 1:

M = D−1S.

Thus, the rows i of M are uniformly weighted indicators on
the indices of X for each folder Ii:

M[i, x] =

1Ii(x) =

1
|Ii|

(cid:26) 1

|Ii| , x ∈ Ii
0,
o.w.

Note that the matrix S and the averaging transform M share
the same structure, i.e. they differ only in the value of the their
non-zero elements.

Alternatively if we denote by m(y, I) the average value of

y(x) in folder I:

m(y, I) =

y(x),

(7)

1
|I|

(cid:88)

x∈I

then applying the averaging transform M to y yields a vector
ˆy of length NX such that each element i is the average value
of y in folder Ii

(7):

ˆy[i] = (My)[i] = m(y, Ii), 1 ≤ i ≤ NX .

(8)

The averaging transform reinterprets each folder in the tree
as applying a uniform averaging ﬁlter, whose support depends
on the size of the folder. Applying the feature-based transform
MX to the dataset Z yields ˆZX = MX Z ∈ RNX ×nY , a data-
driven multi-scale representation of the data. As opposed to
a multiscale representation deﬁned on a regular grid, here
the representation at each level
is obtained via non-local
averaging of the coefﬁcients from the level below. The ﬁnest
level of the representation is the data itself, which is then
averaged in increasing degree of coarseness and in a non-
local manner according to the clusters deﬁned by the hierarchy
in the partition tree. The columns of ˆZX are the multiscale
representation ˆy of each observation y. The rows of ˆZX are the
centroids of the folders I ∈ TX and can be seen as multiscale
meta-features of length nY :

Ci(y) =

M[i, x]Z[x, y], 1 ≤ y ≤ nY .

(9)

(cid:88)

x

Let S be an NX × nX matrix representing the structure of
a given tree TX , by having each row i of the matrix be the
indicator function of the corresponding folder Ii ∈ TX :

S[i, x] = 1Ii(x) =

(cid:26) 1,

x ∈ Ii
0, otherwise

In a similar fashion denote by ˆZY = ZMT
Y the application
of the observation-based transform to the entire dataset. For
additional properties of S and M see [31].

In Fig. 1, we display an illustration of a partition tree

(3)

5

(a) Partition tree T . (b) Averaging transform matrix M induced by
Fig. 1.
the tree and applied to column vector y(x). The color of the elements in the
output correspond to the color of the folders in the tree.

and the resulting averaging transform. Fig. 1(a) is a partition
tree TX constructed on X where nX = 8. Fig. 1(b) is the
averaging transform M corresponding to the partition tree
TX . For visualization purposes we construct M as having
columns whose order correspond to the leaves of the tree TX
(level 0). This reordering also needs to be applied to the data
vectors y, and is essentially one of the aims of our approach.
The lower part of the transform is just the identity matrix,
as it corresponds to the leaves of the tree. The number of
rows in the transform matrix is NX = |T | = 14, as the
number of folders in the tree. The transform is applied to
a (reordered) column y ∈ R8, yielding the coefﬁcient vector
ˆy = My ∈ R14. The coefﬁcients are colored according to the
corresponding folders in the tree.

To further demonstrate and visualize the transform, we
apply the averaging transform to an image in Fig. 2. We
treat a grayscale image as a high-dimensional dataset where
X is the set of rows and Y is the set of columns. We
calculate a partition tree TY on the columns. We then calculate
the averaging transform and apply it to the image yielding
ˆZY = ZMT
Y . The result is presented in Fig. 2(a). Each row
x has now been extended to a higher dimension NY , where
we separate the levels of the tree with colored borders lines
for visualization purposes. Each of the columns ˆZY is the
centroid of folder I in the tree. The right-most sub-matrix is
the original image and as we move left we have coarser and
coarser scales. The averaging is non-local and the folder sizes
vary, respecting the structure of the data. Thus on the second
level of the tree, the building on the right is more densely
compressed compared to the building on the left.

Fig. 2. Application of the averaging transform (a) and the difference transform
(b) to an image. The color of the border represents the level of the tree. The
non-local nature of the transforms and the varying support is apparent, for
example, in the building on the right. In the ﬁne-scale resolution the building
has 7 windows in the horizontal direction, which have been compressed into
5 windows on the next level.

B. Difference transform

The goal of our approach is to organize the data in nested
folders in which the features and the observations are smooth.
Thus, it is of value to determine how smooth is the hierarchical
structure of the tree, i.e. does the merging of folders on one
level into a single folder on the next level preserve smoothness.
Let ∆ be an NX × nX matrix, termed the multiscale differ-
ence transform. This transform yields the difference between
ˆy[i] and ˆy[j] where j is the index of the immediate parent of
folder i.

The matrix ∆ is obtained from the averaging matrix M as:

∆[i, x] = M[i, x] − M[j, x], Il,i ⊂ Il+1,j.

(10)

Applying ∆ to observation y yields a vector of length NX
whose element i is the difference between the average value
of y in folder Il,i and the average value in its immediate parent
folder Il+1,j:

(∆y)[i] =

(cid:26) m(y, IL,1),

Ii = IL,1
m(y, Il,i) − m(y, Il+1,j), Il,i ⊂ Il+1,j,

(11)
where for the root folder, we deﬁne (∆y)[i] to be the average
over all features. This choice leads to the deﬁnition of an
inverse transform below. Thus, the rows i of ∆ are given by:

∆[i, x] =





1
|Ii| ,
|Il,i| − 1
1
− 1
|Il+1,j | ,
0,

|Il+1,j | ,

Ii = IL,1
x ∈ Il,i ⊂ Il+1,j
x /∈ Il,i ⊂ Il+1,j, x ∈ Il+1,j
x /∈ Il,i, x /∈ Il+1,j

and the sum of the rows of ∆:

∆[i, x] =

(cid:26) 1, Ii = IL,1
otherwise

0,

(12)

(13)

The difference transform can be seen as revealing “edges”
in the data, however these edges are non-local. Since the
tree groups features together based on their similarity and
not based on their adjacency, the difference between folders
is not restricted to the given ordering of the features. This
demonstrated in Fig. 2(b) where the difference transform of
the column tree has been applied to the 2D image as Z∆T
Y .
Theorem 2: The data can be recovered from the difference

transform by:

y = ST (∆y)

(14)

Proof: An element (ST ∆y)[x] is given by
(cid:88)

1Ii(x) (m(y, Il,i) − m(y, Il+1,j)) + m(y, IL,1) =

Il,i∈TX
Il,i⊂Il+1,j
l<L

(cid:88)

=
Il,i∈TX
0≤l≤L

=

n(0)
(cid:88)

i=1

1Ii(x)m(y, Il,i) −

1Ii(x)m(y, Il,i) =

(15)

(cid:88)

Il,i∈TX
1≤l≤L

1Ii(x)m(y, I0,i) = y(x) (cid:3)

The ﬁrst equality is due to the folders on each level being
disjoint such that if x ∈ Il,i and Il,i ⊂ Il+1,j then x ∈
Il+1,j, and Il+1,j is the only folder containing x on level
l + 1. This enables us to process the data in the tree-based
transform domain and then reconstruct by:

ˆy = ST f (∆y),

(16)

X → RN

where f : RN
X is a function in the domain of the
tree folders. For example, we can threshold coefﬁcients based
on their energy or the size of their corresponding folder. This
scheme can be applied to denoising and compression of graphs
or matrix completion [18]–[21], however this is beyond the
scope of this paper and will be explored in future work.

Note that the difference transform differs from the tree-
based Haar-like basis introduced in [13]. The Haar-like basis
is an orthonormal basis spanned by nX vectors derived from
the tree by an orthogonalization procedure. The difference
transform is overcomplete and spanned by NX vectors, whose
construction does not require an orthogonalization procedure,
making it simpler to compute. Also, as each vector corre-
sponds to a single folder, it enables us to deﬁne a measure of
the homogeneity of a speciﬁc folder compared to its parent.

C. Joint-tree transforms

Given the matrix Z on X × Y, and the respective partition
trees TX and TY , we deﬁne joint-tree transforms that operate
on the features and observations of Z simultaneously. This is
analogous to typical 2D transforms. The joint-tree averaging
transform is applied as

ˆZX ,Y = MX ZMT
Y .

(17)

The resulting matrix of size NX × NY provides a multiscale
representation of the data matrix, admitting a block-like struc-
ture corresponding to the folders in both trees. On the ﬁnest
level we have Z and then on coarser and coarser scales we have
smoothed versions of Z, where the averaging is performed

6

Fig. 3.
difference transform applied to image.

(a) Joint-tree averaging transform applied to image. (b) Joint-tree

under the joint folders at each level. The coarsest level is of
size 1 × 1 corresponding to the joint root folder. This matrix is
analogous to a 2D Gaussian pyramid representation of the data,
popular in image processing [24]. However, as opposed to the
2D Gaussian pyramid in which each level is a reduction of both
dimensions, applying our transform yields all combinations
of ﬁne and coarse scales in both dimensions. The joint-tree
averaging transform yields a result similar to the directional
pyramids introduced in [32], however the “blur” and “sub-
sample” operations in our case are data-driven and non-local.
The joint-tree difference transform is applied as ∆X Z∆T
Y .
This matrix is analogous to a 2D Laplacian pyramid represen-
tation of the data, revealing “edges” in the data. As in applying
a 1D transform, the data can be recovered from the joint-tree
difference transform as Z = ST

X ∆X Z∆T

Y SY .

Figure 3 presents applying the joint-tree averaging transform
and joint-tree difference transform to the 2D image. Within the
red border we display “zooming in” on level l ≥ 1 in both
trees TX and TY .

D. Multi-tree transforms

At each level of the partition tree, the folders are grouped
into disjoint sets. A limitation of using partition trees, there-
fore, is that each folder is connected to a single “parent”.
However, it can be beneﬁcial to enable a folder on one level
to participate in several folders at the level above, such that
folders overlap, as in [33]. We propose an approach that
enables overlapping folders in the bi-organization framework
by constructing more than one tree on the features X , and
we extend the single tree transforms to multi-tree transforms.
This generalizes the partition tree such that each folder can be
connected to more than one folder in the above level, i.e. this
is no longer a tree because it is now cyclic but still a bipartite
graph. Note that in contrast to the joint-tree transform, which
incorporates a joint pair of trees over both the features and the
observations, here we are referring to a set of trees deﬁned for
only the features, or only the observations.

t |Tt|. Yet since all trees {Tt}nT

Given a set of nT different partition trees on X , denoted
{Tt}nT
t=1, we construct the multi-tree averaging transform. Let
(cid:102)MX be an (cid:101)NX × nX matrix, constructed by concatenation of
the averaging transform matrices MT induced by each of the
trees {Tt}nT
t=1. The number of rows in the multi-tree transform
matrix is denoted by (cid:101)NX and equal to the number of folders in
all of the trees (cid:80)
t=1 contain the
same root and leaves folders, we remove the multiple appear-
ance of the rows corresponding to these folders and include
them only once (then (cid:101)NX = (cid:80)
t |Tt| − (nT − 1)(1 + nX )).
Thus, the matrix of the multi-tree averaging transform now
represents a decomposition via a single root, a single set
of leaves and many multiscale folders that are no longer
disjoint. This implies that
instead of considering multiple
“independent” trees, we have a single hierarchical graph where
at each level we do not have disjoint folders, as in a tree, but
instead overlapping folders. In Sec. IV-C, we derive from these
transforms a new multi-tree metric. For additional properties
of the multi-tree transform see [31].

Ram, Elad and Cohen [26] also proposed a “generalized tree
transform” where folders are connected to multiple parents in
the level above, however their work differs in two aspects.
First, their proposed tree construction is a binary tree, whereas
ours admits general tree constructions. Second, their transform
relies on classic pre-determined wavelet ﬁlters such that the
support of the ﬁlter is ﬁxed across the dataset. Our formu-
lation on the other hand introduces data-driven ﬁlters whose
support is determined by the size of the folder, which can
vary across the tree. The Multiresolution Matrix Factorization
(MMF) [27] also yields a wavelet basis on graphs. MMF
uncovers a hierarchical organization of the graph that permits
overlapping clusters, by decomposition of a graph Laplacian
matrix via a sequence of sparse orthogonal matrices. However,
our transform is derived from a set of multiple hierarchical
trees, whereas their hierarchical structure is derived from the
wavelet transform.

The ﬁeld of community detection also addresses ﬁnd-
ing overlapping clusters in graphs [34]. Ahn, Bagrow and
Lehmann [33] construct multiscale overlapping clusters on
graphs by performing hierarchical clustering with a similarity
between edges of a graph, instead of its nodes. Their approach
focuses on the explicit construction of the hierarchy of the
overlapping clusters, whereas our focus is on employing a
transform and a metric derived from such a multiscale over-
lapping organization of the features. In contrast to clustering,
our approach allows for the organization and analysis of the
observations.

IV. TREE-BASED METRIC

The success of the data organization and the reﬁnement of
the partition trees depends on the metric used to construct
the trees. We assume that a good organization of the data
recovers smooth joint clusters of observations and features.
Therefore, a metric for comparing pairs of observations should
not only compare their values for individual features (as in
the Euclidean distance), but also across clusters of features,
which are expected to have similar values. Thus, we present a

7

metric dT in the multiscale representation yielded by the tree
transforms. Using this metric, the construction of the tree on
the features takes into account the structure of the underlying
graph on the observations as represented by its partition tree.
The partition tree on the observations in turn relies on the
graph structure of the features. In each iteration a new tree
is calculated based on the metric from the previous iteration,
and then a new metric is calculated based on the new tree.
This can be seen as updating the dual graph structure of the
data in each iteration. The iterative bi-organization algorithm
is presented in Alg. 1.

A. Tree-based EMD

Coifman and Leeb [25] deﬁne a tree-based metric approxi-
mating the EMD in the setting of hierarchical partition trees.
Given a 2D matrix Z, equipped with a partition tree on the
features TX , consider two observations y, y(cid:48) ∈ Y. The tree-
based metric between the observations is deﬁned as

dTX (y, y(cid:48)) =

|m(y − y(cid:48), I)|,

(18)

(cid:19)β

(cid:18) |I|
nX

(cid:88)

I∈TX

where β is a parameter that weights the folders in the tree
based on their size. Following our formulation of the trees
inducing linear transforms, this tree-based metric can be seen
as a weighted l1 distance in the space of the averaging
transform.

Theorem 3: [23, Theorem 4.1] Given a partition tree on
the features TX , deﬁne the NX × NX diagonal weight matrix
W[i, i] =
. Then the tree metric (18) between two
observations y, y(cid:48) ∈ RnX is equivalent to the weighted l1
distance between the averaging transform coefﬁcients:

(cid:16) |Ii|
nX

(cid:17)β

dTX (y, y(cid:48)) = (cid:107)W(ˆy − ˆy(cid:48))(cid:107)1.

(19)

Proof: An element of the vector W(ˆy − ˆy(cid:48)) is

(WM(y − y(cid:48)))[i] =

W[i, j](M(y − y(cid:48)))[j]

(cid:88)

j

= W[i, i](M(y − y(cid:48)))[i]

(20)

(cid:19)β

=

(cid:18) |Ii|
nX

m(y − y(cid:48), Ii).

Therefore:

(cid:107)W(ˆy − ˆy(cid:48))(cid:107)1 =

|m(y − y(cid:48), I)| (cid:3)

(21)

(cid:19)β

(cid:18) |Ii|
nX

(cid:88)

I∈T

Note that the proposed metric is equivalent to the l1 distance
between vectors of higher-dimensionality than the original di-
mension of the vectors. However, by weighting the coefﬁcients
with W, the effective dimension of the new vectors is typically
smaller than the original dimensionality, as the weights rapidly
decrease to zero based on the folder size and the choice
of β. For positive values of β, the entries corresponding to
the large folders dominate ˆy, while entries corresponding to
small folders tend to zero. This trend is reversed for negative
values of β, with elements corresponding to small folders
dominating ˆy while large folders are suppressed. In both cases,
a threshold can be applied to ˆy or ˆZ so as to discard entries

with low absolute values. Thus, the transforms project the
data onto a low-dimensional space of either coarse or ﬁne
structures. Also, note that interpreting the metric as the l1
distance in the averaging transform space enables us to apply
approximate nearest-neighbor search algorithms suitable for
the l1 distance [35], [36]. This allows to analyze larger datasets
via a sparse afﬁnity matrix.

Deﬁning the metric in the transform space enables us to
easily generalize the metric to a joint-tree metric deﬁned for a
joint pair of trees {TX , TY } (Sec. IV-B), to incorporate several
trees over the features {TX }nT in a multi-tree metric via the
multi-tree transform (Sec. IV-C), and to seamlessly introduce
weights on the transform coefﬁcients by setting the elements
of W (Sec. IV-E). Python code implementing our approach is
available at [37].

B. Joint-tree Metric

The tree-based transforms and metrics can be generalized to
analyzing rank-n tensor datasets. We brieﬂy present the joint-
tree metric to demonstrate that the proposed transforms are
not limited to just 2D matrices, but rather can be extended
to processing and organizing tensor datasets. An example of
such an application was presented in [23].

In [23] we proposed a 2D metric given a pair of partition
trees in the setting of organizing a rank-3 tensor. We reformu-
late this metric in the transform space by generalizing the tree-
based metric to a joint-tree metric using the coefﬁcients of the
joint-tree transform. Given a partition tree TX on the features
and a partition tree TY on the observations,
the distance
between two matrices Z1 and Z2 is deﬁned as

dTX ,TY (Z1, Z2) =

|m(Z1 − Z2, I × J )|

|I|βX |J |βY
nβX
βY
X nY

.

(cid:88)

I∈TX
J ∈TY

(22)
The value m(Z, I × J ) is the mean value of a matrix Z on
the joint folder I × J = {(x, y) | x ∈ I, y ∈ J }:

m(Z, I × J ) =

Z[x, y].

(23)

1
|I||J |

(cid:88)

x∈I,y∈J

Theorem 3 can be generalized to a 2D transform applied to
2D matrices.

Corollary 4: [23, Corollary 4.2] The joint-tree metric (22)
between two matrices given a partition tree TX on the features
and a partition tree TY on the observations is equivalent to the
l1 distance between the weighted 2D multiscale transform of
the two matrices:

dTX ,TY (Z1, Z2) = (cid:107)WX MX (Z1 − Z2)MT

YWY (cid:107)1.

(24)

C. Multi-tree Metric

The deﬁnition of the metric in the transform domain enables
a simple extension to a metric derived from a multi-tree
composition. Given a set of multiple trees {Tt}nT
t=1 deﬁned on
the features X as in Sec. III-D, we deﬁne a multi-tree metric
using the multi-tree averaging tree transform as:

d{T }(y, y(cid:48)) = (cid:107) (cid:102)W(cid:102)M(y − y(cid:48))(cid:107)1,

(25)

8

(cid:17)β

(cid:16) |Ii|
nX

where (cid:102)W is a diagonal matrix whose elements are
for all I ∈ T and for all trees in {Tt}nT
equivalent to averaging the single tree metrics:

t=1. This metric is

(cid:88)

d{T }(y, y(cid:48)) = (cid:107) (cid:102)W(cid:102)M(y − y(cid:48))(cid:107)1
1
nT
1
nT

dT (y, y(cid:48)).

T
(cid:88)

=

=

T

(cid:107)WT MT (y − y(cid:48))(cid:107)1

(26)

Note that in contrast to the joint-tree metric, which incorpo-
rates a pair of trees over both the features and the observations,
here we are referring to a set of trees deﬁned only for the
features, or only for the observations.

A question that arises is how to construct multiple trees? For
matrix denoising in a bi-organization setting, Ankenman [10]
applies a spin-cycling procedure: constructing many trees
by randomly varying the parameters in the partition tree
construction algorithm. Multiple trees can also be obtained by
initializing the bi-organization with different metric choices
for d(0)
X (x, x(cid:48)) (step 2 in Alg. 1), e.g., Euclidean, correlation,
etc. Another option, which we demonstrate experimentally on
real data in Sec. V, arises when we have multiple data sets
of observations with the same set of features, or multiple data
sets with the same observations but different features as in
multi-modal settings. In such cases, we construct a partition
tree for each dataset separately and then combine them using
the multi-tree metric.

D. Local Reﬁnement

We propose a new approach to constructing multiple trees,
leveraging the partition of the data during the bi-organization
procedure. This approach is based on a local reﬁnement of
the partition trees, which results in a smoother organization
of the data. The bi-organization method is effective when
correlations exist among both observations and features, by
revealing a hierarchical organization that is meaningful for all
the data together. Yet, since the bi-organization approach is
global and takes all observations and all features into account,
it needs to achieve the best organization on average. However,
the correlations between features may differ among sub-
populations in the data, i.e. the correlations between features
depend on the set of observations taken into account (and vice-
versa).

For example, consider a dataset of documents where the ob-
servations Y are documents belonging to different categories,
the features X are words and Z(x, y) indicates whether a
document y contained a word x. Grouping the words into
disjoint folders forces a single partition of the vocabulary that
disregards words that belong to more than one conceptual
group of words. These connections could be revealed by taking
into account the context, i.e. the subject of the documents.
By diving the documents into a few contextual clusters, and
calculating a local tree on the words TX for each such cluster,
the words are grouped together conceptually according to
the local category. The word “ﬁeld” for example will be
joined with different neighbors, depending on whether the

Algorithm 1 Bi-organization Algorithm [10, Sec. 5.3]
Initialization
Input Dataset Z of features X and observations Y

X , weight function on tree

1: Starting with features X
2:

Calculate initial metric d(0)
X (x, x(cid:48))
Calculate initial ﬂexible tree T (0)
X .

3:

Iterative analysis
Input Flexible tree on features T (0)

folders W[i, i] = ω(Ii)

4: for n ≥ 1 do
5:

X , calculate tree metric between obser-
(y, y(cid:48)) = (cid:107)WX MX (y − y(cid:48))(cid:107)1

Given tree T (n)
vations d(n)
TX
Calculate ﬂexible tree on the observations T (n)
Y .
Repeat steps 5-6 for the features X given T (n)
obtain T (n+1)

and

Y

.

X

8: end for

Algorithm 2 Bi-organization local reﬁnement
Input Dataset Z, observation tree TY

1: Choose level l in tree TY
2: for j ∈ {1, ..., n(l)} do
3:
4:

Set ω(Ji) = 1 ∀Ji ⊆ Jl,j, otherwise ω(Ji) = 0.
Calculate initial afﬁnity on features for subset of
observations as weighted tree-metric d(0)(x, x(cid:48)) =
dTY (x, x(cid:48); ω(Jj))
Calculate initial ﬂexible tree on features T (0)
Perform iterative analysis (steps 4-8 in Alg. 1) for Z
on X and (cid:101)Y = {y | y ∈ J ∈ TY }.

X

7: end for
8: Merge observation trees {T

}n(l) back into global tree

(cid:101)Yj

Output Reﬁned observation tree TY , Set of feature trees

TY

{TX }n(l)
i=1

6:

7:

5:
6:

analysis is applied to documents belonging to “agriculture”,
“mathematics” or “sports”.

Therefore, we propose to take advantage of the unsupervised
clustering obtained by the partition tree on the observations
TY , and apply a localized bi-organization to folders of ob-
servations. Formally, we apply the bi-organization algorithm
to a subset of Z containing all features X and a subset of
observations belonging to the same folder (cid:101)Y = {y | y ∈ J ∈
TY }. This local bi-organization results in a pair of trees: a
local tree T
(cid:101)Y organizing the subset of observations (cid:101)Y, and
a feature tree TX that organizes all the features X based on
this subset of observations that share the same local structure,
rather than the global structure of the data. This reveals the
correlations between features for this sub-population of the
data, and provides a localized visualization and exploratory
analysis for subsets of the data discovered in an unsupervised
manner. This is meaningful when the data is unbalanced and a
subset of the data differs drastically from the rest of the data,
e.g., due to anomalies.

We propose a local reﬁnement of the bi-organization as
follows. We select a single layer l of the observations tree

9

(cid:101)Yj

TY , and perform a separate localized organization for each
folder Jl,j ∈ Pl,
j ∈ {1, ..., n(l)}. We thus obtain n(l)
}n(l)
local observation trees {T
j=1, which we then merge back
into one global tree, with reﬁned partitioning. Merging is
performed by replacing the branch in TY whose root is Jl,j,
i.e. {J ∈ TY |J ⊆ Jl,j}, with the local observation tree T
.
(cid:101)Yj
In addition, we obtain a set of several corresponding trees on
the full set of features {TX }n(l), which we can use to calculate
a multi-tree metric (25). Our local reﬁnement algorithm is
presented in Alg. 2. Applying this algorithm to reﬁne the
global structures of both TY and TX results in a smoother
bi-organization of the data.

We typically apply the reﬁnement to a high level of the tree
since at these levels large clusters of distinct sub-populations
are grouped together, and their separate analysis will reveal
their local organization. The level can be chosen by applying
the difference transform and selecting a level at which the
folders grouped together are heterogeneous, i.e. their mean
signiﬁcantly differs from the mean of their parent folder.

Note that this approach is unsupervised and relies on the
data-driven organization of the data. However, this approach
can also be used in a supervised setting, when there are labels
on the observations. Then we calculate a different partition
tree on the features for each separate label (or sets of labels)
of the observations, revealing the hierarchical structure of the
features for each label. This will be explored in future work.

E. Weight Selection

The calculation of the metric depends on the weight attached
to each folder. We generalize the metric such that the weight
is W[i, i] = ω(Ii), where ω(Ii) > 0 is a weight function
associated with folder Ii. The weights can incorporate prior
smoothness assumptions on the data, and also enable to en-
hance either coarse or ﬁne structures in the similarity between
samples.

(cid:17)β

(cid:16) |Ii|
nX

The choice ω(Ii) =

in [25] makes the tree-based
metric (18) equivalent to EMD, i.e., the ratio of EMD to
the tree-based metric is always between two constants. The
parameter β weights the folder by its relative size in the tree,
where β > 0 emphasizes coarser scales of the data, while
β < 0 emphasizes differences in ﬁne structures.

Ankenman [10] proposed a slight variation to the weight

also encompassing the tree structure:

ω(Ii) = 2−αl(Ii)

(cid:18) |Ii|
nX

(cid:19)β

,

(27)

where α is a constant and l(Ii) is the level at which the folder
Ii is found in T . The constant α weights all folders in a
given level equally. Choosing α = 0 resorts to the original
weight. The structure of the trees can be seen as an analogue
to a frequency decomposition in signal processing, where
the support of a folder is analogous to a certain frequency.
Moreover, since high levels of the tree typically contain large
folders, they correspond to low-pass ﬁlters. Conversely, lower
levels of the tree correspond to high-pass ﬁlters as they contain
many small folders. Thus setting α > 0 corresponds to
emphasizing low frequencies whereas α < 0 corresponds to

10

enhancing high frequencies. In an unbalanced tree, where a
small folder of features remains separate for all levels of the
tree (an anomalous cluster of features), α can be used to
enhance the importance of this folder, as opposed to β, which
would decrease its importance based on its size.

We propose a different approach. Instead of weighting the
folders based on the structure of the tree, which requires a-
priori assumptions on the optimal scale of the features or the
observations, we set the folders weights based on their content.
By applying the difference transform to the data, we obtain a
measure for each folder deﬁning how homogeneous it is. This
reduces the number of parameters in the algorithm, which is
advantageous in the unsupervised problem of bi-organization.
We calculate for each folder, the norm of its difference on the
dataset Z:
(cid:32)

(cid:33)1/2

ω(Ii) =

((∆X Z)[i, y])2

(cid:88)

y



(cid:32)

(cid:88)

(cid:88)

=



y

x

(m(y(x), Il,i) − m(y(x), Il+1,j)

1/2

(cid:33)2


,

(28)
where Il,i ⊂ Il+1,j. This weight is high when Il,i (cid:28) Il+1,j.
This means that the parent folder joining Il,i with other folders
contains non-homogeneous “populations”. Therefore, assign-
ing a high weight to Il,i places importance on differentiating
these different populations.

The localized reﬁnement procedure in Alg. 2 can also be
formalized as assigning weights ω(I) in the tree metric. We
set all weights containing a branch of the tree (a folder and
all its sub-folders) to 1 and set all other weights to zero:
(cid:26) 1,
0,

Ii ⊆ Ij
otherwise,

ω(Ii) =

(29)

where Ij is the root folder of the branch. Thus, using these
weights, the metric is calculated based only on a subset of the
observations ˜Y. This metric can initialize a bi-organization
procedure of a subset of Z containing X and ˜Y.

F. Coherence

To assess the smoothness of the bi-organization stemming
from the constructed partition trees, a coherency criterion was
proposed in [9]. The coherency criterion is given by

C(Z; TX , TY ) =

(cid:107)ΨX ZΨT

Y (cid:107)1,

(30)

1
nX nY

where Ψ is a Haar-like orthonormal basis proposed by Gavish,
Nadler and Coifman [13] in the settings of partition trees,
and it depends on the structure of a given tree. This criterion
measures the decomposition of the data in a bi-Haar-like basis
induced by two partition trees TX and TY : ΨX ZΨT
Y . The
lower the value of C(Z; TX , TY ), the smoother the organiza-
tion is in terms of satisfying the mixed H¨older condition (1).
Minimizing the coherence can be used as a stopping con-
dition for the bi-organization algorithm presented in Alg. 1.
The bi-organization continues as long as C(Z; T (n)
Y ) <
C(Z; T (n−1)
) [9]. However, we have empirically
X

X , T (n)

, T (n−1)
Y

found that the iterative process typically converges within
only few iterations. Therefore, in our experimental results we
perform n = 2 iterations.

V. EXPERIMENTAL RESULTS

Analysis of cancer gene expression data is of critical impor-
tance in jointly identifying subtypes of cancerous tumors and
genes that can distinguish the subtypes or indicate a patient’s
long-term survival. Identifying a patient’s tumor subtype can
determine the course of treatment, such as recommendation of
hormone therapy in some subtypes of breast cancer, and is a
an important step toward the goal of personalized medicine.
Biclustering of breast cancer data has identiﬁed sets of genes
whose expression levels categorize tumors into ﬁve subtypes
with distinct survival outcomes [38]: Luminal A, Luminal
B, Triple negative/basal-like, HER2 type and “Normal-like”.
Related work has aimed to classify samples into each of
these subtypes or identify other types of signiﬁcant clusters
based on gene expression, clinical features and DNA copy
number analysis [39]–[42]. The clustered dendrogram obtained
by agglomerative hierarchical clustering of the genes and the
subjects is widely used in the analysis of gene expression data.
However, in contrast to our approach, hierarchical clustering
is usually applied with a metric, such as correlation, that
is global and linear, and does not
the
structure revealed by the multiscale tree structure of the other
dimension. Conversely, our approach enables us to iteratively
update both the tree and metric of the subjects based on the
metric for the genes, and update the tree and metric of the
genes based on the metric for the subjects.

take into account

We analyze three breast cancer gene expression datasets,
where the features are the genes and the observations are the
tumor samples. The ﬁrst dataset is the METABRIC dataset,
containing gene expression data for 1981 breast tumors [40]
collected with a gene expression microarray. We denote this
dataset ZM, and its set of samples YM. The second dataset,
ZT, is taken from The Cancer Genome Atlas (TCGA) Breast
Cancer cohort [43] and consists of 1218 samples, YT. This
dataset was proﬁled using RNA sequencing, which is a newer
and more advanced gene expression technology. The third
dataset ZB (BRCA-547) [41], comprising of 547 samples YB,
was acquired with microarray technology. These 547 samples
are also included in the TCGA cohort, but the gene expression
was proﬁled using a different technology.

We selected X to be the 2000 genes with the largest variance
in METABRIC from the original collection of ∼ 40000 gene
probes. In related work, the analyzed genes were selected in
a supervised manner based on prior knowledge or statistical
signiﬁcance in relation to patient survival
time [38]–[40],
[42], [44]. Here we present results of a purely unsupervised
approach aimed at exploratory analysis of high-dimensional
data, and we do not use the survival information or subtypes
labels in either applying our analysis or for gene selection, but
only in evaluating the results. In the remainder of this section
we present three approaches in which the tree transforms
and metrics are applied for the purpose of unsupervised
organization of gene expression data.

11

Fig. 4. Global bi-organization of the METABRIC dataset. The samples (columns) and genes (rows) have been reordered so they correspond to the leaves
of the two partition trees. Below the organized data are clinical details for each of the samples: two types of breast cancer subtype labels (reﬁned [42] and
PAM50 [39]) hormone receptor status (ER, PR) and HER2 status.

Regarding implementation, in this application we use ﬂex-
the partition trees in the bi-
ible trees [10] to construct
organization. We initialize the bi-organization with a corre-
lation afﬁnity on the genes (d(0)
X (x, x(cid:48)) in Alg. 1, Step 2),
which is commonly used in gene expression analysis.

A. Subject Clustering

We begin with a global analysis of all samples of the
METABRIC data using the bi-organization algorithm pre-
sented in Alg. 1. We perform two iterations of the bi-
organization using the tree-based metric with the data-driven
weights deﬁned in (28). The organized data and corresponding
trees on the samples and on the genes are shown in Fig. 4.
The samples and genes have been reordered such that they
correspond to the leaves of the two partition trees. Below
the organized data we provide clinical details for each of
the samples: two types of breast cancer subtype labels, the
reﬁned labels introduced in [42] and the standard PAM50
subtypes [39], hormone receptor status (ER, PR) and HER2
status. We analyze the folders of level l = 5 on the samples
tree, which divides the samples into ﬁve clusters (the folders
are marked with numbered colored circles).

In Fig. 5 we present histograms of the reﬁned subtype labels
for each of the numbered folders in the samples tree, and
plot the disease-speciﬁc survival curve of each folder in the
bottom graph. The histograms of each folder is surrounded by
a colored border corresponding to the colored circle indicating

TABLE I
METABRIC SELF ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)
DTC [46]
sparseBC [12]

RI
0.79
0.72
0.72
0.69
0.74
0.72
0.74
0.76

ARI VI
0.45
0.30
0.23
0.20
0.30
0.26
0.19
0.33

1.48
1.77
1.98
1.94
1.84
1.90
2.45
1.74

p-value
4.35 × 10−21
1.11 × 10−17
8.48 × 10−10
1.46 × 10−12
1.11 × 10−16
5.23 × 10−11
5.54 × 10−22
2.6 × 10−19

the relevant folder in the tree in Fig. 4. Note that the folders do
not just separate data according to subtype as in the dark blue
and light blue folders (Basal and Her2 respectively), but also
separate data according to the survival rates. If we compare the
orange and green folders that are grouped in the same parent
folder, both contain a mixture of Luminal A and Luminal B,
yet they have distinctive survival curves. The p-value of this
separation using the log-rank test [45] was 4.35 × 10−21.

We next compare our weighted metric (28) to the original
EMD-like metric (18), using different values of β and α in
(27). These values were chosen in order to place different em-
phasis of the transform coefﬁcients depending on the support
of the corresponding folders or the level of the tree. The values
of β enable to emphasize large folders (β = 1), small folders

12

the variation of information (VI) [49]. The RI and ARI
measure the similarity between two clusterings (or partitions)
of the data. Both measures indicate no agreement between the
partitions by 0 and perfect agreement by 1, however ARI can
return negative values for certain pairs of clusterings. The third
measure is an information theoretic criterion, where 0 indicates
perfect agreement between two partitions. Finally, we perform
survival analysis using Kaplan-Meier estimate [50] of disease-
speciﬁc survival rates of the samples, reporting the p-value of
the log-rank test [45]. A brief description of these statistics is
provided in Appendix II.

We select clusters by partitioning the samples into the
folders J of the samples tree TX , at a single level l of the tree
which divides the data into 4-6 clusters (typically level L − 2
in our experiments). This follows the property of ﬂexible trees
that the level at which folders are joined is meaningful across
the entire dataset, as for each level the distances between
joined folders are similar. For other types of tree construction
algorithms, alternative methods can be used to select clusters
in the tree, such as SigClust used in [41].

Results are presented in Table I for the METABRIC dataset
and in Table II for the BRCA-547 dataset. For the METABRIC
dataset, using the weighted metric achieves the best results
compared to the other weight selections, in terms of both
clustering relative to the ground-truth labels and the survival
curves of the different clusters (note these two criteria do
not always coincide). While DTC achieves the lowest p-
value overall, it has very poor clustering results compared
to the ground-truth labels (lowest ARI and highest VI). The
weighted metric out-performed the sparseBC method, which
has second-best performance for the clustering measures, and
third-lowest p-value. For the BRCA-547 dataset, the weighted
metric achieves the best clustering in terms of the ARI measure
and has the lowest p-value. For the VI measure, the clustering
by the weighted metric was slightly larger but comparable
to that of the lowest score. On this dataset, DTC performed
poorly with highest VI and p-value. The sparseBC method
achieved good clustering with highest RI and ARI measures,
but had a high p-value and VI compared to the performance
of our bi-organization method.

The results indicate that the data-driven weighting achieves
comparable if not better performance, than both using the tree-
dependent weights and competing biclustering methods. Thus,
the data-driven weighting provides an automatic method to
set appropriate weights on the transform coefﬁcients in the
metric. Our method is completely data-driven, as opposed to
the sparseBC method which requires as input the number of
features and observations to decompose the data into. (We used
the provided computationally expensive cross-validation pro-
cedure to select the best number of clusters in each dimension).
In addition, our approach provides a multiscale organization,
whereas sparseBC yields a single-scale decomposition of the
data. The DTC is a multiscale approach, however as it relies
on hierarchical clustering it does not take into account the
dendrogram in the other dimension. The performance may
be improved by using dendrograms in our iterative approach,
instead of the ﬂexible trees (this is further discussed below).

Fig. 5. (top) Histograms of folders in sample tree of METABRIC. The color
of the border corresponds to the circles in the tree. (bottom) Survival curves
for each folder.

TABLE II
BRCA-547 SELF ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)
DTC [46]
sparseBC [12]

RI
0.75
0.75
0.74
0.72
0.74
0.74
0.75
0.76

ARI VI
0.38
0.37
0.36
0.35
0.34
0.35
0.35
0.38

1.38
1.39
1.37
1.33
1.56
1.45
1.63
1.49

p-value
0.0004
0.0073
0.0028
0.0773
0.0010
0.0130
0.0853
0.0269

(β = −1) and weighting all folders equally (β = 0). The
values of α either emphasize high levels of the tree (α = 0.5),
low levels of the tree (α = −1) or weighting all levels equally
(α = 0).

We also compare to two other biclustering methods. The
ﬁrst is the dynamic tree cutting (DTC) [46] applied to a hier-
archical clustering dendrogram obtained using mean linkage
and correlation distance (a popular choice in gene expression
analysis). The second is the sparse biclustering method [12],
where the authors impose a sparse regularization on the mean
values of the estimated biclsuters (assuming the mean of
the dataset
is zero). Both algorithms are implemented in
R: package dynamicTreeCut and package sparseBC,
respectively.

We evaluate our approach by both measuring how well the
obtained clusters represent the cancer subtypes, and estimating
the statistical signiﬁcance of the survival curves of the clusters.
We compare the clustering of the samples relative to the
reﬁned subtype labels [42] using three measures: the Rand
index (RI) [47], the adjusted Rand index (ARI) [48], and

13

TABLE III
COHERENCY OF REFINED BI-ORGANIZATION

TABLE IV
METABRIC EXTERNAL ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)

Global TX
and TY
0.7039
0.7066
0.7051
0.7028
0.7051
0.7075

Reﬁned
TX
0.6103
0.6107
0.6118
0.6130
0.6119
0.6141

Reﬁned
TY
0.5908
0.5928
0.5921
0.5972
0.5927
0.5934

Reﬁned
TX , TY
0.5463
0.5480
0.5472
0.5668
0.5487
0.5497

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)

RI
0.74
0.73
0.72
0.73
0.72
0.73

ARI VI
0.30
0.29
0.26
0.28
0.27
0.25

1.77
1.87
1.87
1.83
1.89
1.98

p-value
3.71 × 10−19
7.78 × 10−16
1.77 × 10−16
4.25 × 10−14
7.02 × 10−6
3.33 × 10−16

B. Local reﬁnement

In Table III we demonstrate the improvement gained in the
organization by applying the local reﬁnement to the partition
trees, where we measure the smoothness of the organized data
using the coherency criterion (30). We perform bi-organization
for different values of β and α as well as the weighted
metric, and compare 4 organizations: 1) Global organization;
2) Reﬁned organization of only the genes tree TX ; 3) Reﬁned
organization of only the samples tree TY ; and 4) Reﬁned
organization of both the features and the samples (reﬁned TX
and TY ). Applying the reﬁned local organization to both the
genes and the samples, yields the best result with regard to
the smoothness of the bi-organization. We also examined the
the effect of the level of the tree on which the reﬁnement is
performed for l ∈ {5, 6, 7} for both trees, and the improvement
gained by reﬁnement was of the same order for all combina-
tions. The results demonstrate that regardless of the weighting
(data-driven or folder dependent), the reﬁnement procedure
improves the coherency of the organization.

C. Bi-organization with multiple datasets

Following the introduction of gene expression proﬁling by
RNA sequencing, an interesting scenario is that of two datasets
proﬁled using different technologies, one using microarray
and the other RNA sequencing. Consider, for example, the
METABRIC dataset ZM and the TCGA dataset ZT, which
share the same features X (in this case genes), but collected for
two different sample sets, YM and YT respectively. In this case,
the gene expression proﬁles have different dynamic range and
are normalized differently, and the samples cannot be analyzed
together simply by concatenating the datasets. However, the
hierarchical structure we learn on the genes, which deﬁnes a
multiscale clustering of the genes, is informative regardless of
the technique used to acquire the expression data.

Thus, the gene metric learned from one dataset can be
applied seamlessly to another dataset and used to organize
its samples due to the coupling between the genes and the
samples. We term this “external-organization”, and demon-
strate how it organizes the METABRIC dataset ZM using the
TCGA dataset ZT. We ﬁrst apply the bi-organization algorithm
to organize ZT, and then we derive the gene tree-based metric
dTX from the constructed tree on the genes TX . This metric
is then used to a construct a new tree TY on the samples set
YM of ZM.

In Table IV we compare the external organization of
METABRIC using our weighted metric to the original EMD-

like metric for different values of β and α. Our results
show that the data-driven weights achieve the best results,
reinforcing that learning the weights in a data-adaptive way
is more beneﬁcial than setting the weights based on the size
of the folders or the level of the tree. Applying external
organization enables us to assess which bi-organization of the
external dataset and corresponding learned metric were the
most meaningful. Note that for some of the parameter choices
(α = 0, β = 1 or β = −1), the external organization of
ZM using a gene tree learned from the dataset ZT was better
than the internal organization. Thus, via the organization of the
dataset ZM, we validate that the hierarchical organization of
the genes in ZT, and therefore, the corresponding metric, are
effective in clustering samples into cancer subtypes. This also
demonstrates that the hierarchical gene organization learned
from one dataset can be successfully applied to another dataset
to learn a meaningful sample organization, even though the
two were proﬁled using different technologies. This provides
motivation to integrate information from datasets together.

In our ﬁnal evaluation, we divide the METABRIC dataset
into its two original subsets: the discovery set comprising 997
tumors and the validation set comprising 995 tumors. Note
that the two sets have different sample distributions of cancer
subtypes. We compare three approaches for organizing the
data. We begin with the self-organization as in Sec. V-A.
We organize each of the two datasets separately and report
their clustering measures in the ﬁrst row in Table V for the
discovery cohort and in Table VI for the validation cohort.
Note that the organization achieved using half the data is
less meaningful in terms of the survival rates compared to
using all of the data. This is due to the different distribution
of subtypes and survival times between the discovery and
validation cohorts, and in addition, the p-value calculation
itself is dependent on the sample size used.

One of the important aspects in a practical application is the
ability to process new samples. Our approach naturally allows
for such a capability. Assume we have already performed bi-
organization on an existing dataset and we acquire a few new
test samples. Instead of having to reapply the bi-organization
procedure to all of the data, we can instead insert the new
samples into the existing organization. We demonstrate this
by using each subset of the METABRIC dataset to organize
the other. In contrast to the external organization example, here
we have two datasets proﬁled with the same technology. We
can treat this as a training and test set scenario: construct a
sample tree on the training set Ytrain and use the learned metric
on the genes dTX to insert samples from the test set Ytest into

the training sample tree TYtrain . First, we calculate the centroids
of the folders Jj of level l = 1 (the level above the leaves) in
the samples tree TYtrain:
(cid:88)

MY [j, y]Z[x, y], x ∈ {1, ..., nX },

l(Jj) = 1

Cj(x) =

y

(31)
These can be considered the representative sample of each
folder. We then assign each new sample y ∈ Ytest
to its
nearest centroid using the metric dTX (y, Cj) derived from
the gene tree TX . Thus, we reconstruct the sample hierarchy
on the test dataset Ytest by assigning each test sample to
the hierarchical clustering of the low-level centroids from the
training sample tree. This approach, therefore, validates the
sample organization as well as the gene organization, whereas
the external organization only enables to validate the gene
organization.

We perform this once treating the validation set as the
training set and the discovery set as the test set, and then vice-
versa. We report the clustering measures in the second row of
Table V and Table VI. Note that the measures are reported only
for the samples belonging to the given set in the table. Inserting
samples from one dataset into the sample tree of another
demonstrates an improved organization in some measures
compared to performing self-organization. For example, the
organization of the discovery set via the validation tree results
in a clustering with improved ARI and VI measures. This
serves as additional evidence for the importance of integrating
information from several datasets together.

Thus far in our experiments, we have gathered substantial
evidence for the importance of information stemming from
multiple data sets. Here, we harness the multiple tree met-
ric (25) to perform integration of datasets in a more systematic
manner. We generalize the external organization method to
several datasets, where we integrate all the learned trees on
the genes {TX } into a single metric via the multi-tree metric.
In addition to the gene tree from both METABRIC datasets,
we also obtain the gene trees from the TCGA and the BRCA-
547 datasets, ZT and ZB. We then calculate a multi-tree met-
ric (25) to construct the sample tree on either the discovery or
validation sets. We report the evaluation measures in the third
row of Table V and Table VI. Taking into account all measures,
the multi-tree metric incorporating four different datasets best
organizes both the discovery and validation datasets. Integrat-
ing information from multiple sources improves the accuracy
of the organization, as averaging the metrics emphasizes
genes that are consistently grouped together, representing the
intrinsic structure of the data. In addition, since the metric
integrates the organizations from several datasets, it is more
accurate than the internal organization of a dataset with few
samples or a non-uniform distribution of subtypes.

Our results show that external organization, via either both
single or multi-tree metric, enables us to learn a meaningful
multi-scale hierarchy on the genes and apply it as a metric to
organize the samples of a given dataset. Thus, we can apply
information from one dataset to another to recover a multi-
scale organization of the samples, even if they were proﬁled
in a different technique. In addition, we obtain a validation of

14

TABLE V
METABRIC DISCOVERY ORGANIZATION

discovery
Self-organization
Inserted into
validation tree
Multi-tree

RI
0.75

ARI VI
0.33

1.81

p-value
1.82 × 10−11

0.74

0.34

1.66

2.93 × 10−9

0.75

0.35

1.63

3.18 × 10−13

TABLE VI
METABRIC VALIDATION ORGANIZATION

validation
Self-organization
Inserted into
discovery tree
Multi-tree

RI
0.77

ARI VI
0.33

1.82

p-value
3.07 × 10−4

0.76

0.30

1.98

9.08 × 10−8

0.76

0.34

1.73

4.24 × 10−9

the gene organization of one dataset via another. This cannot
be accomplished with traditional hierarchical clustering in a
clustered dendrogram as the clustering of the samples does not
depend on the hierarchical structure of the genes dendrogram.
However, we can obtain an iterative hierarchical clustering
algorithm for biclustering using our approach. As our bi-
organization depends on a partition tree method, we can use
hierarchical clustering instead of ﬂexible trees in the iterative
bi-organization algorithm. Alternatively, as hierarchical clus-
tering depends on a metric, this can also be formulated as
deriving a transform from the dendrogram on the genes and
using its corresponding tree-metric instead of correlation as
the input metric to the hierarchical clustering algorithm on
the samples, and vice-versa.

In related work, Cheng, Yang and Anastassiou [51] analyzed
multiple datasets and identiﬁed consistent groups of genes
across datasets. Zhou et al. [52] integrate datasets in a platform
independent manner to identify groups of genes with the same
function across multiple datasets. The multi-tree transform can
also be used to identify such genes, however this is beyond
the scope of this paper and will be addressed in future work.

D. Sub-type labels

In breast cancer, PAM50 [39] is typically used to assign
intrinsic subtypes to the tumors. However, Milioli et al. [42]
recently proposed a reﬁned set of subtypes labels for the
METABRIC dataset, based on a supervised iterative approach
to ensure consistency of the labels using several classiﬁers.
Their labels are shown to have a better agreement with
the clinical markers and patients’ overall survival than those
provided by the PAM50 method. Therefore, the clustering
measures we reported on the METABRIC dataset were with
respect to the reﬁned labels.

Our unsupervised analysis demonstrated higher consistency
with the reﬁned labels than with PAM50. Thus, our unsuper-
vised approach provides an additional validation to the labeling
achieved in a supervised manner. We divided the data into
training and test sets and classiﬁed the test set using k-NN
nearest neighbors with majority voting using the tree-based
metric. For different parameters and increasing numbers of

genes (nX = 500, 1000, 2000), we had higher agreement with
the reﬁned labels than with PAM50, achieving a classiﬁcation
accuracy of 82% on average. Classifying with the PAM50
labels had classiﬁcation accuracy lower by an average of
10% ± 2%. This is also evident when examining the labels
in Fig. 4. Note that whereas PAM50 assigns a label based
on 50 genes and the reﬁned labels were learned using a
subset of genes found in a supervised manner, our approach
is unsupervised using the nX genes with the highest variance.

VI. CONCLUSIONS

In this paper we proposed new data-driven tree-based trans-
forms and metrics in a matrix organization setting. We pre-
sented partition trees as inducing a new multiscale transform
space that conveys the smooth organization of the data, and
derived a metric in the transform space. The trees and cor-
responding metrics are updated in an iterative bi-organization
approach, organizing the observations based on the multiscale
decomposition of the features, and organizing the features
based on the multiscale decomposition of the observations.
In addition, we generalized the transform and the metric
to incorporate multiple partition trees on the data, allowing
for the integration of several datasets. We applied our data-
driven approach to the organization of breast cancer gene
expression data, learning metrics on the genes to organize the
tumor samples in meaningful clusters of cancer sub-types. We
demonstrated how our approach can be used to validate the
hierarchical organization of both the genes and the samples
by taking into account several datasets of samples, even
when these datasets were proﬁled using different technolo-
gies. Finally, we employed our multi-tree metric to integrate
information from the organization of these multiple datasets
and achieved an improved organization of tumor samples.

In future work, we will explore several aspects of the
multiple tree setting. First, the multi-tree transform and metric
can be incorporated in the iterative framework for further
reﬁnement. Second, we will generalize the coherency measure
to incorporate multiple trees. Third, we will apply the multi-
tree framework to a multi-modal setting, where observations
are shared across datasets, as for example, in the joint samples
shared by the BRCA-547 and TCGA datasets. Finally, we will
reformulate the iterative procedure as an optimization problem,
enabling to explicitly introduce cost functions. In particular,
cost functions imposing the common structure of the multiple
trees across datasets will be considered.

ACKNOWLEDGMENTS

The authors thank the anonymous reviewers for their con-

structive comments and useful suggestions.

REFERENCES

15

[4] W. H. Yang, D. Q. Dai, and H. Yan, “Finding correlated biclusters from
gene expression data,” IEEE Trans. Knowl. Data Eng., vol. 23, no. 4,
pp. 568–584, April 2011.

[5] E. C. Chi, G. I. Allen, and R. G. Baraniuk, “Convex biclustering,”
Biometrics, 2016. [Online]. Available: http://dx.doi.org/10.1111/biom.
12540

[6] D. Jiang, C. Tang, and A. Zhang, “Cluster analysis for gene expression
data: a survey,” IEEE Trans. Knowl. Data Eng., vol. 16, no. 11, pp.
1370–1386, 2004.

[7] J. Bennett and S. Lanning, “The Netﬂix prize,” in Proceedings of KDD

cup and workshop, vol. 2007, 2007, p. 35.

[8] S. Busygin, O. Prokopyev, and P. M. Pardalos, “Biclustering in data
mining,” Computers & Operations Research, vol. 35, no. 9, pp. 2964 –
2987, 2008.

[9] M. Gavish and R. R. Coifman, “Sampling, denoising and compression
of matrices by coherent matrix organization,” Appl. Comput. Harmon.
Anal., vol. 33, no. 3, pp. 354 – 369, 2012.

[10] J.

I. Ankenman, “Geometry and analysis of dual networks on
questionnaires,” Ph.D. dissertation, Yale University, 2014. [Online].
Available: https://github.com/hgfalling/pyquest/blob/master/ankenman
diss.pdf

[11] Y. Kluger, R. Basri, J. T. Chang, and M. Gerstein, “Spectral biclus-
tering of microarray data: coclustering genes and conditions,” Genome
research, vol. 13, no. 4, pp. 703–716, 2003.

[12] K. M. Tan and D. M. Witten, “Sparse biclustering of transposable data,”

J. Comp. Graph. Stat., vol. 23, no. 4, pp. 985–1008, 2014.

[13] M. Gavish, B. Nadler, and R. R. Coifman, “Multiscale wavelets on
trees, graphs and high dimensional data: Theory and applications to
semi supervised learning,” in Proc. ICML, 2010, pp. 367–374.

[14] A. Singh, R. Nowak, and R. Calderbank, “Detecting weak but
hierarchically-structured patterns in networks,” in Proc. AISTATS, vol. 9,
May 2010, pp. 749–756.

[15] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on
graphs via spectral graph theory,” Appl. Comput. Harmon. Anal., vol. 30,
no. 2, pp. 129 – 150, 2011.

[16] J. Sharpnack, A. Singh, and A. Krishnamurthy, “Detecting activations
over graphs using spanning tree wavelet bases.” in AISTATS, 2013, pp.
536–544.

[17] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-
dergheynst, “The emerging ﬁeld of signal processing on graphs: Ex-
tending high-dimensional data analysis to networks and other irregular
domains,” IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83–98, 2013.
[18] S. K. Narang and A. Ortega, “Compact support biorthogonal wavelet
ﬁlterbanks for arbitrary undirected graphs,” IEEE Trans. Signal Process.,
vol. 61, no. 19, pp. 4673–4685, Oct 2013.

[19] A. Sakiyama, K. Watanabe, and Y. Tanaka, “Spectral graph wavelets
and ﬁlter banks with low approximation error,” IEEE Trans. Signal Inf.
Process. Netw., vol. 2, no. 3, pp. 230–245, Sept 2016.

[20] D. I. Shuman, M. J. Faraji, and P. Vandergheynst, “A multiscale pyramid
transform for graph signals,” IEEE Trans. Signal Process., vol. 64, no. 8,
pp. 2119–2134, April 2016.

[21] N. Tremblay and P. Borgnat, “Subgraph-based ﬁlterbanks for graph
signals,” IEEE Trans. Signal Process., vol. 64, no. 15, pp. 3827–3840,
Aug 2016.

[22] N. Shahid, N. Perraudin, V. Kalofolias, G. Puy, and P. Vandergheynst,
“Fast robust PCA on graphs,” IEEE J. Sel. Topics Signal Process.,
vol. 10, no. 4, pp. 740–756, June 2016.

[23] G. Mishne, R. Talmon, R. Meir, J. Schiller, U. Dubin, and R. R.
Coifman, “Hierarchical coupled-geometry analysis for neuronal structure
and activity pattern discovery,” IEEE J. Sel. Topics Signal Process.,
vol. 10, no. 7, pp. 1238–1253, Oct 2016.

[24] P. Burt and E. Adelson, “The Laplacian pyramid as a compact image
code,” IEEE Trans. Commun., vol. 31, no. 4, pp. 532–540, 1983.
[25] R. R. Coifman and W. E. Leeb, “Earth mover’s distance and equivalent
metrics for spaces with hierarchical partition trees,” Yale University,
Tech. Rep., 2013, technical report YALEU/DCS/TR1482.

[26] I. Ram, M. Elad, and I. Cohen, “Generalized tree-based wavelet trans-
form,” IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4199–4209, 2011.
[27] R. Kondor, N. Teneva, and V. Garg, “Multiresolution matrix factoriza-

[1] Y. Cheng and G. M. Church, “Biclustering of expression data,” in ISMB,

tion,” in Proc. ICML, 2014, pp. 1620–1628.

vol. 8, 2000, pp. 93–103.

[28] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp.

[2] C. Tang, L. Zhang, A. Zhang, and M. Ramanathan, “Interrelated two-
way clustering: an unsupervised approach for gene expression data
analysis,” in Proc. BIBE, 2001, pp. 41–48.

[3] M. Lee, H. Shen, J. Z. Huang, and J. S. Marron, “Biclustering via
sparse singular value decomposition,” Biometrics, vol. 66, no. 4, pp.
1087–1095, 2010.

5–32, 2001.

[29] R. R. Coifman and S. Lafon, “Diffusion maps,” Appl. Comput. Harmon.

Anal., vol. 21, no. 1, pp. 5–30, July 2006.

[30] R. R. Coifman and M. Gavish, “Harmonic analysis of digital data
bases,” in Wavelets and Multiscale Analysis, ser. Applied and Numerical
Harmonic Analysis. Birkh¨auser Boston, 2011, pp. 161–197.

[31] G. Mishne, “Diffusion nets and manifold learning for high-dimensional
data analysis in the presence of outliers,” Ph.D. dissertation, Technion,
2016.

[32] M. Zontak, I. Mosseri, and M. Irani, “Separating signal from noise using

patch recurrence across scales,” in Proc. CVPR, June 2013.

[33] Y.-Y. Ahn, J. P. Bagrow, and S. Lehmann, “Link communities reveal
multiscale complexity in networks,” Nature, vol. 466, no. 7307, pp. 761–
764, 2010.

[34] J. Xie, S. Kelley, and B. K. Szymanski, “Overlapping community
detection in networks: The state-of-the-art and comparative study,” ACM
Comput. Surv., vol. 45, no. 4, pp. 43:1–43:35, Aug. 2013.

[35] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu,
“An optimal algorithm for approximate nearest neighbor searching ﬁxed
dimensions,” J. ACM, vol. 45, no. 6, pp. 891–923, Nov. 1998.

[36] B.-K. Yi and C. Faloutsos, “Fast time sequence indexing for arbitrary

lp norms,” in Proc. VLDB, 2000.

[37] [Online]. Available: http://github.com/gmishne/pyquest
[38] T. Sørlie et al., “Gene expression patterns of breast carcinomas distin-
guish tumor subclasses with clinical implications,” Proc. Natl. Acad.
Sci., vol. 98, no. 19, pp. 10 869–10 874, 2001.

[39] J. S. Parker et al., “Supervised risk predictor of breast cancer based on
intrinsic subtypes,” Journal of Clinical Oncology, vol. 27, no. 8, pp.
1160–1167, 2009.

[40] C. Curtis et al., “The genomic and transcriptomic architecture of 2,000
breast tumours reveals novel subgroups,” Nature, vol. 486, no. 7403, pp.
346–352, 2012.

[41] Cancer Genome Atlas Network, “Comprehensive molecular portraits of

human breast tumours,” Nature, vol. 490, no. 7418, pp. 61–70, 2012.

[42] H. H. Milioli, R. Vimieiro, I. Tishchenko, C. Riveros, R. Berretta, and
P. Moscato, “Iteratively reﬁning breast cancer intrinsic subtypes in the
METABRIC dataset,” BioData Mining, vol. 9, no. 1, pp. 1–8, 2016.

[43] Cancer Genome Atlas Network. [Online]. Available: https://xenabrowser.
net/datapages/?cohort=TCGA%20Breast%20Cancer%20(BRCA)
[44] C. M. Perou et al., “Molecular portraits of human breast tumours,”

Nature, vol. 406, no. 6797, pp. 747–752, 2000.

[45] R. Peto and J. Peto, “Asymptotically efﬁcient rank invariant test proce-
dures,” Journal of the Royal Statistical Society. Series A (General), vol.
135, no. 2, pp. 185–207, 1972.

[46] P. Langfelder, B. Zhang, and S. Horvath, “Deﬁning clusters from a hier-
archical cluster tree: the dynamic tree cut package for r,” Bioinformatics,
vol. 24, no. 5, pp. 719–720, 2008.

[47] W. M. Rand, “Objective criteria for the evaluation of clustering meth-
ods,” Journal of the American Statistical Association, vol. 66, no. 336,
pp. 846–850, 1971.

[48] L. Hubert and P. Arabie, “Comparing partitions,” J. Classiﬁcation, vol. 2,

no. 1, pp. 193–218, 1985.

[49] M. Meil˘a, “Comparing clusterings - an information based distance,”
Journal of Multivariate Analysis, vol. 98, no. 5, pp. 873 – 895, 2007.
[50] E. L. Kaplan and P. Meier, “Nonparametric estimation from incomplete
observations,” Journal of the American statistical association, vol. 53,
no. 282, pp. 457–481, 1958.

[51] W.-Y. Cheng, T.-H. O. Yang, and D. Anastassiou, “Biomolecular events
in cancer revealed by attractor metagenes,” PLoS Comput Biol, vol. 9,
no. 2, pp. 1–14, 02 2013.

[52] X. J. Zhou et al., “Functional annotation and network reconstruction
through cross-platform integration of microarray data,” Nature biotech-
nology, vol. 23, no. 2, pp. 238–243, 2005.

[53] J. P. Klein and M. L. Moeschberger, Survival analysis: techniques for

censored and truncated data. SSBM, 2005.

APPENDIX I
FLEXIBLE TREES

We brieﬂy describe the ﬂexible trees algorithm, given the
feature set X and an afﬁnity matrix on the features denoted
KX . For a detailed description see [10].

1) Input: The set of features X , an afﬁnity matrix KX ∈

RnX ×nX , and a constant (cid:15).

2) Init: Set partition I0,i = {i} ∀ 1 ≤ i ≤ nX , set l = 1.
3) Given an afﬁnity on the data, we construct a low-

dimensional embedding on the data [29].

4) Calculate

level-dependent
d(l)(i, j) ∀ 1 ≤ i, j ≤ nX in the embedding space.

pairwise

the

distances

16

(cid:15) , where p = median (cid:0)d(l)(i, j)(cid:1).

5) Set a threshold p
6) For each index i which has not yet been added
to a folder, ﬁnd its minimal distance dmin(i) =
minj{d(l)(i, j)}.

• If dmin(i) < p

(cid:15) , i and j form a new folder if j
does not belong to a folder. If j is already part of a
folder I, then i is added to that folder if dmin(i) <
p
(cid:15) 2−|I|+1.
• If dmin(i) > p

(cid:15) , i remains as a singleton folder.
7) The partition Pl is set to be all the formed folders.
8) For l > 1 and while not all samples have been merged
together in a single folder, steps 4-7 are repeated for the
folders Il−1,i ∈ Pl−1. The distances between folders
depend on the level l, and on the samples in each of the
folders.

APPENDIX II
COMPARING SURVIVAL CURVES
The survival function S(t) is deﬁned as the probability that
a subject will survive past time t. Let T be a failure time
with probability density function f . The survival function is
S(t) = P (T > t), where the Kaplan-Meier method [50] is a
non-parametric estimate given by

ˆS(tj) =

P r(T > ti|T ≥ ti) =

j
(cid:89)

i=1

ˆS(tj−1)P r(T > tj|T ≥ tj).

(32)

Deﬁning ni as the number at risk just prior to time ti and
di as the number of failures at ti, then P (T > ti) = ni−di
.
For more information on estimating survival curves and taking
into account censored data see [53]

ni

Comparison of two survival curves can be done using a
statistical hypothesis test called the log-rank test [45]. It is
used to test the null hypothesis that there is no difference
between the population survival curves (i.e. the probability of
an event occurring at any time point is the same for each
population). Deﬁne nk,i as the number at risk in group k just
prior to time ti, such that ni = (cid:80)
k nk,i and dk,i as the number
of failures in group k at time ti such that di = (cid:80)
k dk,i. Then,
the expected number of failures in group k = 1, 2 is given by
(cid:88)

and the observed number of failures in group k = 1, 2 is

Ek =

di

nk,i
ni

(cid:88)

Ok =

dk,i.

i

i

(O2 − E2)2
Var(O2 − E2)

∼ χ2
1.

(33)

(34)

(35)

Under the null hypothesis of no difference between the two

groups, the log-rank test statistic is

The log-rank test can be extended to more than two
groups [53].

Data-Driven Tree Transforms and Metrics

Gal Mishne, Ronen Talmon, Israel Cohen, Ronald R. Coifman and Yuval Kluger

1

7
1
0
2
 
g
u
A
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
6
7
5
0
.
8
0
7
1
:
v
i
X
r
a

Abstract—We consider the analysis of high dimensional data
given in the form of a matrix with columns consisting of
observations and rows consisting of features. Often the data
is such that the observations do not reside on a regular grid,
and the given order of the features is arbitrary and does not
convey a notion of locality. Therefore, traditional transforms
and metrics cannot be used for data organization and analysis.
In this paper, our goal is to organize the data by deﬁning an
appropriate representation and metric such that they respect the
smoothness and structure underlying the data. We also aim to
generalize the joint clustering of observations and features in
the case the data does not fall into clear disjoint groups. For
this purpose, we propose multiscale data-driven transforms and
metrics based on trees. Their construction is implemented in an
iterative reﬁnement procedure that exploits the co-dependencies
between features and observations. Beyond the organization
of a single dataset, our approach enables us to transfer the
organization learned from one dataset to another and to integrate
several datasets together. We present an application to breast
cancer gene expression analysis: learning metrics on the genes to
cluster the tumor samples into cancer sub-types and validating
the joint organization of both the genes and the samples. We
demonstrate that using our approach to combine information
from multiple gene expression cohorts, acquired by different
proﬁling technologies, improves the clustering of tumor samples.

Index Terms—graph signal processing, multiscale representa-

tions, geometric analysis, partition trees, gene expression

I. INTRODUCTION

High-dimensional datasets are typically analyzed as a two-
dimensional matrix where, for example,
the rows consist
of features and the columns consist of observations. Signal
processing addresses the analysis of such data as residing on
a regular grid, such that the rows and columns are given
in a particular order,
indicating smoothness. For example,
the ordering in time-series data indicates temporal-frequency
smoothness, and the order in 2D images indicating spatial
smoothness. Non-Euclidean data that do not reside on a regular
grid, but rather on a graph, raise the more general problem of
matrix organization. In such datasets, the given ordering of the
rows (features) and columns (observations) does not indicate
any degree of smoothness.

However, in many applications, for example, analysis of
gene expression data, text documents, psychological question-

G. Mishne and R. R. Coifman are with the Department of Mathematics,
Yale University, New Haven, CT 06520 USA (e-mail: gal.mishne@yale.edu
; ronald.coifman@math.yale.edu.). R. Talmon and I. Cohen are with the
Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of
ico-
Technology, Haifa 32000,
hen@ee.technion.ac.il). Y. Kluger is with the Department of Pathology and
the Yale Cancer Center, Yale University School of Medicine, New Haven, CT
06511 USA (e-mail: yuval.kluger@yale.edu). This research was supported by
the Israel Science Foundation (grant no. 576/16), and by the United States-
Israel Binational Science Foundation and the United States National Science
Foundation (grant no. 2015582), and by the National Institutes of Health (grant
no. 1R01HG008383-01A1).

ronen@ee.technion.ac.il;

(e-mail:

Israel.

naires and recommendation systems [1]–[10], there is an un-
derlying structure to both the features and the observations. For
example, in gene expression subsets of samples (observations)
have similar genetic proﬁles, while subsets of genes (features)
have similar expressions across groups of samples. Thus, as
the observations are viewed as high-dimensional vectors of
features, one can swap the role of features and observations,
and treat the features as high-dimensional vectors of observa-
tions. This dual analysis reveals meaningful joint structures in
the data.

The problem of matrix organization considered here is
closely related to biclustering [1]–[5], [11], [12], where the
goal is to identify biclusters: joint subsets of features and
observations such that the matrix elements in each subset have
similar values. Matrix organization goes beyond the extraction
of joint clusters, yielding a joint reordering of the entire dataset
and not just the extraction of partial subsets of observations
and features that constitute bi-clusters. By recovering the
smooth joint organization of the features and observations,
one can apply signal processing and machine learning methods
such as denoising, data completion, clustering and classiﬁca-
tion, or extract meaningful patterns for exploratory analysis
and data visualization.

The application of traditional signal processing transforms
to data on graphs is not straightforward, as these transforms
rely almost exclusively on convolution with ﬁlters of ﬁnite
support, and thus are based on the assumption that the given
ordering of the data conveys smoothness. The ﬁeld of graph
signal processing adapts classical techniques to signals sup-
ported on a graph (or a network), such as ﬁltering and wavelets
in the graph domain [13]–[22]. Consider for example signals
(observations) acquired from a network of sensors (features).
The nodes of the graph are the sensors and the edges and
their weights are typically dictated by a-priori information
such as physical connectivity, geographical proximity, etc. The
samples collected from all sensors at a given time compose a
high-dimensional graph signal supported on this network. The
signal observations, acquired over time, are usually processed
separately and the connectivity between the observations is not
taken into account.

To address this issue, in this paper we propose to analyze
the data in a matrix organization setting as represented by
two graphs: one whose nodes are the observations and the
other whose nodes are the features, and our aim is a joint
unsupervised organization of these two graphs. Furthermore,
we do not ﬁx the edge weights by relying on a predetermined
structure or a-priori information. Instead, we calculate the edge
weights by taking into account the underlying dual structure
of the data and the coupling between the observations and the
features. This requires deﬁning two metrics, one between pairs
of observations and one between pairs of features.

2

Such an approach for matrix organization was introduced by
Gavish and Coifman [9], where the organization of the data
relies on the construction of a pair of hierarchical partition
trees on the observations and on the features. In previous
work [23], we extended this methodology to the organization
of a rank-3 tensor (or a 3D database), introducing a multiscale
averaging ﬁlterbank derived from partition trees.

Here we introduce a new formulation of the averaging ﬁlter-
bank as a tree-based linear transform on the data, and propose
a new tree-based difference transform. Together these yield
a multiscale representation of both the observations and the
features, in analogue to the Gaussian and Laplacian pyramid
transforms [24]. Our transforms can be seen as data-driven
multiscale ﬁlters on graphs, where in contrast to classical
signal processing, the support of the ﬁlters is non-local and
depends on the structure of the data. From the transforms,
we derive a metric in the transform space that incorporates
the multiscale structure revealed by the trees [25]. The trees
and the metrics are incorporated in an iterative bi-organization
procedure following [9]. We demonstrate that beyond the
organization of a single dataset, our metric enables us to
apply the organization learned from one dataset to another
and to integrate several datasets together. This is achieved by
generalizing our transform to a new multi-tree transform and
to a multi-tree metric, which integrate a set of multiple trees on
the features. Finally, the multi-tree transform inspires a local
reﬁnement of the partition trees, improving the bi-organization
of the data.

The remainder of the paper is organized as follows. In
Section II, we formulate the problem, present an overview of
our solution and review related background. In Section III, we
present the new tree-induced transforms and their properties.
In Section IV, we derive the metric in the transform space and
propose different extensions of the metric. We also propose a
local reﬁnement of the bi-organization approach. Section V
presents experimental results in the analysis of breast cancer
gene expression data.

non-binary tree is not uniquely deﬁned. Finally, since the
averaging transform is over-complete such that each ﬁlter
corresponds to a single folder in the tree, it is simple to design
weights on the transform coefﬁcients based on the properties
of the individual folders.

it

Filterbanks and multiscale transforms on trees and graphs
have been proposed in [18]–[21], yet differ from our ap-
proach in several aspects. While ﬁlterbanks construct a multi-
scale representation by using downsampling operators on the
data [18], [20], the multiscale nature of our transform arises
is
from partitioning of the data via the tree. In that,
most similar to [21], where the graph is decomposed into
subgraphs by partitioning. However, all these ﬁlterbanks on
graphs employ the eigen-decomposition of the graph Laplacian
to deﬁne either global ﬁlters on the full graph or local ﬁlters
on disjoint subgraphs. Our approach, conversely, employs the
eigen-decomposition of the graph Laplacian to construct the
partition tree, but the transforms (ﬁlters) are deﬁned by the
structure of the tree and not explicitly derived from the Lapla-
cian. In addition, we do not treat the structure of the graph as
ﬁxed, but rather iteratively update the Laplacian based on the
tree transform. Finally, while graph signal processing typically
addresses one dimension of the data (features or observations),
our approach addresses the construction of transforms on both
the observations and features of a dataset, and relies on the
coupling between the two to derive the transforms.

This work is also related to the matrix factorization pro-
posed by Shahid et al. [22], where the graph Laplacians of both
the features and the observation regularize the decomposition
of a dataset
into a low-rank matrix and a sparse matrix
representing noise. Then the observations are clustered using
k-means on the low-dimensional principal components of the
smooth low-rank matrix. Our work differs in that we preform
an iterative non-linear embedding of the observations and
features, not jointly, but alternating between the two while
updating the graph Laplacian of each in turn. In addition, we
provide a multiscale clustering of the data.

A. Related Work

II. BI-ORGANIZATION

Various methodologies have been proposed for the con-
struction of wavelets on graphs,
including Haar wavelets,
and wavelets based on spectral clustering and spanning tree
decompositions [13]–[16], [26], [27]. Our work deviates from
this path and presents an iterative construction of data-driven
tree-based transforms. In contrast to previous multiscale rep-
resentations of a single graph, our approach takes into account
the co-dependencies between observations and features by
incorporating two graph structures. Our motivation for the
proposed transforms is the tree-based Earth Mover’s Distance
(EMD) proposed in [25], which introduces a coupling between
observations and features, enabling an iterative procedure that
updates the trees and metrics in each iteration. The averaging
transform, in addition to being equipped with this metric, is
also easier to compute than a wavelet basis as it does not
require an orthogonalization procedure. In addition, given a
the averaging and difference transforms are
partition tree,
unique, whereas the tree-based wavelet transform [13] on a

A. Problem Formulation

Let Z be a high-dimensional dataset and let us denote its
set of nX features by X and denote its set of nY observations
by Y. For example, in gene expression data, X consists of
the genes and Y consists of individual samples. The element
Z(x, y) is the expression of gene x ∈ X in sample y ∈ Y. The
given ordering of the dataset is arbitrary such that adjacent
features and adjacent observations in the dataset are likely
dissimilar. We assume there exists a reordering of the features
and a reordering of the observations such that Z is smooth.

Deﬁnition 1: A matrix Z is smooth if it satisﬁes the mixed
H¨older condition [9], such that ∀x, x(cid:48) ∈ X and ∀y, y(cid:48) ∈ Y,
and for a pair of non-trivial metrics ρX on X and ρY on Y
and constants C > 0 and 0 < α ≤ 1:

|Z(x, y) − Z(x, y(cid:48)) − Z(x(cid:48), y) + Z(x(cid:48), y(cid:48))|

≤ CρX (x, x(cid:48))αρY (y, y(cid:48))α.

(1)

Note that we do not impose smoothness as an explicit con-
straint; instead it manifests itself implicitly in our data-driven
approach.

Although the given ordering of the dataset is not smooth, the
organization of the observations and the features by partition
trees following [9] constructs both local and global neigh-
borhoods of each feature and of each observation. Thus, the
structure of the tree organizes the data in a hierarchy of nested
clusters in which the data is smooth. Our aim is to deﬁne a
transform on the features and on the observations that conveys
the hierarchy of the trees, thus recovering the smoothness of
the data. We deﬁne a new metric in the transform space that
incorporates the hierarchical clustering of the data via the
trees. The notations in this paper follow these conventions:
matrices are denoted by bold uppercase and sets are denoted
by uppercase calligraphic.

B. Method Overview

The construction of the tree, which relies on a metric, and
the calculation of the metric, which is derived from a tree, lead
to an iterative bi-organization algorithm [9]. Each iteration
updates the pair of trees and metrics on the observations
and features as follows. First, an initial partition tree on the
features, denoted TX , is calculated based on an initial pairwise
afﬁnity between features. This initial afﬁnity is application
dependent. Based on a coarse-to-ﬁne decomposition of the
features implied by the partition tree on the features, we deﬁne
a new metric between pairs of observations: dTX (y, y(cid:48)). The
metric is then used to construct a new partition tree on the
observations TY . Thus, the construction of the tree on the
observations TY is based on a metric induced by the tree on the
features TX . The new tree on the observations TY then deﬁnes
a new metric between pairs of features dTY (x, x(cid:48)). Using this
metric, a new partition tree is constructed on the features TX ,
and a new iteration begins. Thus, this approach exploits the
strong coupling between the features and the observations.
This enables an iterative procedure in which the pair of trees
are reﬁned from iteration to iteration, providing in turn a more
accurate metric on the features and on the observations. We
will show that the resulting tree-based transform and corre-
sponding metric enable a multiscale analysis of the dataset,
reordering of the observations and features, and detection of
meaningful joint clusters in the data.

C. Partition trees

Given a dataset Z, we construct a hierarchical partitioning
of the observations and features deﬁned by a pair of trees.
Without loss of generality, we deﬁne the partition trees in this
section with respect to the features, and introduce relevant
notation.

Let TX be a partition tree on the features. The partition tree
is composed of L + 1 levels, where a partition Pl is deﬁned
for each level 0 ≤ l ≤ L. The partition Pl = {Il,1, ..., Il,n(l)}
at level l consists of n(l) mutually disjoint non-empty subsets
of indices in {1, ..., nX }, termed folders and denoted by Il,i,
i ∈ {1, ..., n(l)}. Note that we deﬁne the folders on the indices
of the set and not on the features themselves.

3

The partition tree TX has the following properties:
• The ﬁnest partition (l = 0) is composed of n(0) = nX
singleton folders, termed the “leaves”, where I0,i = {i}.
• The coarsest partition (l = L) is composed of a single
folder, PL = IL,1 = {1, ..., nX }, termed the “root”.
• The partitions are nested such that if I ∈ Pl, then I ⊆ J
for some J ∈ Pl+1, i.e., each folder at level l − 1 is a
subset of a folder from level l.

The partition tree is the set of all folders at all levels TX =
{Il,i | 0 ≤ l ≤ L, 1 ≤ i ≤ n(l)}, and the number of all
folders in the tree is denoted by NX = |TX |. The size, or
cardinality, of a folder I, i.e. the number of indices in that
folder, is denoted by |I|. In the remainder of the paper, for
compactness, we drop the subscript l denoting the level of a
folder, and denote a single folder by either I or Ii, such that
i ∈ {1, ..., NX } is an index over all folders in the tree.

Given a dataset, there are many methods to construct a
partition tree, including deterministic, random, agglomerative
(bottom-up) and divisive (top-down) [5], [13], [28]. For ex-
ample, in a bottom-up approach, we begin at the lowest level
of the tree and cluster the features into small folders. These
folders are then clustered into larger folders at higher levels
of the tree, until all folders are merged together at the root.

Some approaches take into account the geometric structure
and multiscale nature of the data by incorporating afﬁnity ma-
trices deﬁned on the data, and manifold embeddings [10], [13].
Ankenman [10] proposed “ﬂexible trees”, whose construction
requires an afﬁnity kernel deﬁned on the data, and is based on a
low-dimensional diffusion embedding of the data [29]. Given
a metric between features d(x, x(cid:48)), a local pairwise afﬁnity
kernel k(x, x(cid:48)) = exp{−d(x, x(cid:48))/σ2} is integrated into a
global representation on the data via a manifold embedding
representation Ψ, which minimizes

min

k(x, x(cid:48))(cid:107)Ψ(x) − Ψ(x(cid:48))(cid:107)2
2.

(2)

(cid:88)

x,x(cid:48)

The clustering of the folders in the ﬂexible tree algorithm is
based on the Euclidean distance between the embedding Ψ
of the features, which integrates the original metric d(x, x(cid:48)).
Thus, the construction of the tree does not rely directly on the
high-dimensional features but on the low-dimensional geomet-
ric representation underlying the data (see [10] for a detailed
description). The quality of this representation, and therefore,
of the constructed tree depends on the metric d(x, x(cid:48)). In our
approach, we propose to use the metric induced by the tree
on the observations d(x, x(cid:48)) = dTY (x, x(cid:48)). This introduces a
coupling between the observations and the features, as the
tree construction of one depends on the tree of the other.
Since our approach is based on an iterative procedure, the
tree construction is reﬁned from iteration to iteration, as both
the tree and the metric on the features are updated based
on the organization of the observations, and vice versa. This
also updates the afﬁnity kernel between observations and the
afﬁnity kernel between features, therefore updating the dual
graph structure of the dataset.

Note that while we apply ﬂexible trees in our experimental
results, the bi-organization approach is modular and different
tree construction algorithms can be applied, as in [9], [30].

While the deﬁnition of the proposed transforms and metrics
does not depend on properties of the ﬂexible trees algorithm,
the resulting bi-organization does depend on the tree con-
struction. Spin-cycling (averaging results over multiple trees)
as in [10] can be applied to stabilize the results. Instead,
we propose an iterative reﬁnement procedure that makes the
tree constructions.
algorithm less dependent on the initial
Convergence guarantees to smooth results from a family of
appropriate initial trees are lacking. This will be the subject
of future work.

III. TREE TRANSFORMS

Given partition trees TX and TY , deﬁned on the features
and observations, respectively, we propose several transforms
induced by the partition trees, which are deﬁned by a linear
transform matrix and generalizes the method proposed in [10].
In the following we focus on the feature set X , but the same
deﬁnitions and constructions apply to the observation set Y.
Note that while the proposed transforms are linear, the support
of the transform elements is derived in a non-linear manner
as it depends on the tree construction.

i.e.

The proposed transforms project the data onto a high di-
mensional space whose dimensionality is equal to the number
of folders in the tree, denoted by NX ,
the transform
maps T : RnX → RNX . Each transform is represented as
a matrix of size NX × nX , where nX is the number of
features. We denote the row indices of the transform matrices
by i, j ∈ {1, 2, ..., NX } indicating the unique index of the
folder in TX . We denote the column indices of the transform
matrices by x, x(cid:48) ∈ X (y, y(cid:48) ∈ Y), which are the indices
of the features (observations) in the data. We deﬁne 1I to
be the indicator function on the features x ∈ {1, ..., nX }
belonging to folder I ∈ TX . Tree transforms obtained from
TX are applied to the dataset as ˆZX = TX Z and tree
transforms obtained from TY are applied to the dataset as
ˆZY = ZTT
Y . We begin with transforms induced by a tree in
a single dimension (features or observations) analogously to a
typical one-dimensional linear transform. We then extend these
transforms to joint-tree transforms induced by a pair of trees
{TX , TY } on the observations and the features, analogously to
a two-dimensional linear transform. Finally, we propose multi-
tree transforms in the case that we have more than one tree
in a single dimension, for example we have constructed a set
of trees {TX } on the features X , each constructed from a
different dataset consisting of different observations with the
same features.

A. Averaging transform

4

(5)

(6)

Applying S to an observation vector y ∈ RnX yields a vector
of length NX where each element i ∈ {1, ..., NX } is the sum
of the elements y(x) for x ∈ Ii:

(Sy)[i] =

y(x)1Ii(x) =

y(x)

(4)

(cid:88)

x∈X

(cid:88)

x∈Ii

The sum of each row of S is the size of its corresponding
folder: (cid:80)
x S[i, x] = |Ii|. The sum of each column is the
number of levels in TX : (cid:80)
i S[i, x] = L + 1, since the folders
are disjoint at each level such that each feature belongs only
to a single folder at each level.

From S we derive the averaging transform denoted by M.
Let D ∈ RNX ×NX be a diagonal matrix whose elements are
the cardinality of each folder: D[i, i] = |Ii|. We calculate
M ∈ RNX ×nX by normalizing the rows of S, so the sum of
each row is 1:

M = D−1S.

Thus, the rows i of M are uniformly weighted indicators on
the indices of X for each folder Ii:

M[i, x] =

1Ii(x) =

1
|Ii|

(cid:26) 1

|Ii| , x ∈ Ii
0,
o.w.

Note that the matrix S and the averaging transform M share
the same structure, i.e. they differ only in the value of the their
non-zero elements.

Alternatively if we denote by m(y, I) the average value of

y(x) in folder I:

m(y, I) =

y(x),

(7)

1
|I|

(cid:88)

x∈I

then applying the averaging transform M to y yields a vector
ˆy of length NX such that each element i is the average value
of y in folder Ii

(7):

ˆy[i] = (My)[i] = m(y, Ii), 1 ≤ i ≤ NX .

(8)

The averaging transform reinterprets each folder in the tree
as applying a uniform averaging ﬁlter, whose support depends
on the size of the folder. Applying the feature-based transform
MX to the dataset Z yields ˆZX = MX Z ∈ RNX ×nY , a data-
driven multi-scale representation of the data. As opposed to
a multiscale representation deﬁned on a regular grid, here
the representation at each level
is obtained via non-local
averaging of the coefﬁcients from the level below. The ﬁnest
level of the representation is the data itself, which is then
averaged in increasing degree of coarseness and in a non-
local manner according to the clusters deﬁned by the hierarchy
in the partition tree. The columns of ˆZX are the multiscale
representation ˆy of each observation y. The rows of ˆZX are the
centroids of the folders I ∈ TX and can be seen as multiscale
meta-features of length nY :

Ci(y) =

M[i, x]Z[x, y], 1 ≤ y ≤ nY .

(9)

(cid:88)

x

Let S be an NX × nX matrix representing the structure of
a given tree TX , by having each row i of the matrix be the
indicator function of the corresponding folder Ii ∈ TX :

S[i, x] = 1Ii(x) =

(cid:26) 1,

x ∈ Ii
0, otherwise

In a similar fashion denote by ˆZY = ZMT
Y the application
of the observation-based transform to the entire dataset. For
additional properties of S and M see [31].

In Fig. 1, we display an illustration of a partition tree

(3)

5

(a) Partition tree T . (b) Averaging transform matrix M induced by
Fig. 1.
the tree and applied to column vector y(x). The color of the elements in the
output correspond to the color of the folders in the tree.

and the resulting averaging transform. Fig. 1(a) is a partition
tree TX constructed on X where nX = 8. Fig. 1(b) is the
averaging transform M corresponding to the partition tree
TX . For visualization purposes we construct M as having
columns whose order correspond to the leaves of the tree TX
(level 0). This reordering also needs to be applied to the data
vectors y, and is essentially one of the aims of our approach.
The lower part of the transform is just the identity matrix,
as it corresponds to the leaves of the tree. The number of
rows in the transform matrix is NX = |T | = 14, as the
number of folders in the tree. The transform is applied to
a (reordered) column y ∈ R8, yielding the coefﬁcient vector
ˆy = My ∈ R14. The coefﬁcients are colored according to the
corresponding folders in the tree.

To further demonstrate and visualize the transform, we
apply the averaging transform to an image in Fig. 2. We
treat a grayscale image as a high-dimensional dataset where
X is the set of rows and Y is the set of columns. We
calculate a partition tree TY on the columns. We then calculate
the averaging transform and apply it to the image yielding
ˆZY = ZMT
Y . The result is presented in Fig. 2(a). Each row
x has now been extended to a higher dimension NY , where
we separate the levels of the tree with colored borders lines
for visualization purposes. Each of the columns ˆZY is the
centroid of folder I in the tree. The right-most sub-matrix is
the original image and as we move left we have coarser and
coarser scales. The averaging is non-local and the folder sizes
vary, respecting the structure of the data. Thus on the second
level of the tree, the building on the right is more densely
compressed compared to the building on the left.

Fig. 2. Application of the averaging transform (a) and the difference transform
(b) to an image. The color of the border represents the level of the tree. The
non-local nature of the transforms and the varying support is apparent, for
example, in the building on the right. In the ﬁne-scale resolution the building
has 7 windows in the horizontal direction, which have been compressed into
5 windows on the next level.

B. Difference transform

The goal of our approach is to organize the data in nested
folders in which the features and the observations are smooth.
Thus, it is of value to determine how smooth is the hierarchical
structure of the tree, i.e. does the merging of folders on one
level into a single folder on the next level preserve smoothness.
Let ∆ be an NX × nX matrix, termed the multiscale differ-
ence transform. This transform yields the difference between
ˆy[i] and ˆy[j] where j is the index of the immediate parent of
folder i.

The matrix ∆ is obtained from the averaging matrix M as:

∆[i, x] = M[i, x] − M[j, x], Il,i ⊂ Il+1,j.

(10)

Applying ∆ to observation y yields a vector of length NX
whose element i is the difference between the average value
of y in folder Il,i and the average value in its immediate parent
folder Il+1,j:

(∆y)[i] =

(cid:26) m(y, IL,1),

Ii = IL,1
m(y, Il,i) − m(y, Il+1,j), Il,i ⊂ Il+1,j,

(11)
where for the root folder, we deﬁne (∆y)[i] to be the average
over all features. This choice leads to the deﬁnition of an
inverse transform below. Thus, the rows i of ∆ are given by:

∆[i, x] =





1
|Ii| ,
|Il,i| − 1
1
− 1
|Il+1,j | ,
0,

|Il+1,j | ,

Ii = IL,1
x ∈ Il,i ⊂ Il+1,j
x /∈ Il,i ⊂ Il+1,j, x ∈ Il+1,j
x /∈ Il,i, x /∈ Il+1,j

and the sum of the rows of ∆:

∆[i, x] =

(cid:26) 1, Ii = IL,1
otherwise

0,

(12)

(13)

The difference transform can be seen as revealing “edges”
in the data, however these edges are non-local. Since the
tree groups features together based on their similarity and
not based on their adjacency, the difference between folders
is not restricted to the given ordering of the features. This
demonstrated in Fig. 2(b) where the difference transform of
the column tree has been applied to the 2D image as Z∆T
Y .
Theorem 2: The data can be recovered from the difference

transform by:

y = ST (∆y)

(14)

Proof: An element (ST ∆y)[x] is given by
(cid:88)

1Ii(x) (m(y, Il,i) − m(y, Il+1,j)) + m(y, IL,1) =

Il,i∈TX
Il,i⊂Il+1,j
l<L

(cid:88)

=
Il,i∈TX
0≤l≤L

=

n(0)
(cid:88)

i=1

1Ii(x)m(y, Il,i) −

1Ii(x)m(y, Il,i) =

(15)

(cid:88)

Il,i∈TX
1≤l≤L

1Ii(x)m(y, I0,i) = y(x) (cid:3)

The ﬁrst equality is due to the folders on each level being
disjoint such that if x ∈ Il,i and Il,i ⊂ Il+1,j then x ∈
Il+1,j, and Il+1,j is the only folder containing x on level
l + 1. This enables us to process the data in the tree-based
transform domain and then reconstruct by:

ˆy = ST f (∆y),

(16)

X → RN

where f : RN
X is a function in the domain of the
tree folders. For example, we can threshold coefﬁcients based
on their energy or the size of their corresponding folder. This
scheme can be applied to denoising and compression of graphs
or matrix completion [18]–[21], however this is beyond the
scope of this paper and will be explored in future work.

Note that the difference transform differs from the tree-
based Haar-like basis introduced in [13]. The Haar-like basis
is an orthonormal basis spanned by nX vectors derived from
the tree by an orthogonalization procedure. The difference
transform is overcomplete and spanned by NX vectors, whose
construction does not require an orthogonalization procedure,
making it simpler to compute. Also, as each vector corre-
sponds to a single folder, it enables us to deﬁne a measure of
the homogeneity of a speciﬁc folder compared to its parent.

C. Joint-tree transforms

Given the matrix Z on X × Y, and the respective partition
trees TX and TY , we deﬁne joint-tree transforms that operate
on the features and observations of Z simultaneously. This is
analogous to typical 2D transforms. The joint-tree averaging
transform is applied as

ˆZX ,Y = MX ZMT
Y .

(17)

The resulting matrix of size NX × NY provides a multiscale
representation of the data matrix, admitting a block-like struc-
ture corresponding to the folders in both trees. On the ﬁnest
level we have Z and then on coarser and coarser scales we have
smoothed versions of Z, where the averaging is performed

6

Fig. 3.
difference transform applied to image.

(a) Joint-tree averaging transform applied to image. (b) Joint-tree

under the joint folders at each level. The coarsest level is of
size 1 × 1 corresponding to the joint root folder. This matrix is
analogous to a 2D Gaussian pyramid representation of the data,
popular in image processing [24]. However, as opposed to the
2D Gaussian pyramid in which each level is a reduction of both
dimensions, applying our transform yields all combinations
of ﬁne and coarse scales in both dimensions. The joint-tree
averaging transform yields a result similar to the directional
pyramids introduced in [32], however the “blur” and “sub-
sample” operations in our case are data-driven and non-local.
The joint-tree difference transform is applied as ∆X Z∆T
Y .
This matrix is analogous to a 2D Laplacian pyramid represen-
tation of the data, revealing “edges” in the data. As in applying
a 1D transform, the data can be recovered from the joint-tree
difference transform as Z = ST

X ∆X Z∆T

Y SY .

Figure 3 presents applying the joint-tree averaging transform
and joint-tree difference transform to the 2D image. Within the
red border we display “zooming in” on level l ≥ 1 in both
trees TX and TY .

D. Multi-tree transforms

At each level of the partition tree, the folders are grouped
into disjoint sets. A limitation of using partition trees, there-
fore, is that each folder is connected to a single “parent”.
However, it can be beneﬁcial to enable a folder on one level
to participate in several folders at the level above, such that
folders overlap, as in [33]. We propose an approach that
enables overlapping folders in the bi-organization framework
by constructing more than one tree on the features X , and
we extend the single tree transforms to multi-tree transforms.
This generalizes the partition tree such that each folder can be
connected to more than one folder in the above level, i.e. this
is no longer a tree because it is now cyclic but still a bipartite
graph. Note that in contrast to the joint-tree transform, which
incorporates a joint pair of trees over both the features and the
observations, here we are referring to a set of trees deﬁned for
only the features, or only the observations.

t |Tt|. Yet since all trees {Tt}nT

Given a set of nT different partition trees on X , denoted
{Tt}nT
t=1, we construct the multi-tree averaging transform. Let
(cid:102)MX be an (cid:101)NX × nX matrix, constructed by concatenation of
the averaging transform matrices MT induced by each of the
trees {Tt}nT
t=1. The number of rows in the multi-tree transform
matrix is denoted by (cid:101)NX and equal to the number of folders in
all of the trees (cid:80)
t=1 contain the
same root and leaves folders, we remove the multiple appear-
ance of the rows corresponding to these folders and include
them only once (then (cid:101)NX = (cid:80)
t |Tt| − (nT − 1)(1 + nX )).
Thus, the matrix of the multi-tree averaging transform now
represents a decomposition via a single root, a single set
of leaves and many multiscale folders that are no longer
disjoint. This implies that
instead of considering multiple
“independent” trees, we have a single hierarchical graph where
at each level we do not have disjoint folders, as in a tree, but
instead overlapping folders. In Sec. IV-C, we derive from these
transforms a new multi-tree metric. For additional properties
of the multi-tree transform see [31].

Ram, Elad and Cohen [26] also proposed a “generalized tree
transform” where folders are connected to multiple parents in
the level above, however their work differs in two aspects.
First, their proposed tree construction is a binary tree, whereas
ours admits general tree constructions. Second, their transform
relies on classic pre-determined wavelet ﬁlters such that the
support of the ﬁlter is ﬁxed across the dataset. Our formu-
lation on the other hand introduces data-driven ﬁlters whose
support is determined by the size of the folder, which can
vary across the tree. The Multiresolution Matrix Factorization
(MMF) [27] also yields a wavelet basis on graphs. MMF
uncovers a hierarchical organization of the graph that permits
overlapping clusters, by decomposition of a graph Laplacian
matrix via a sequence of sparse orthogonal matrices. However,
our transform is derived from a set of multiple hierarchical
trees, whereas their hierarchical structure is derived from the
wavelet transform.

The ﬁeld of community detection also addresses ﬁnd-
ing overlapping clusters in graphs [34]. Ahn, Bagrow and
Lehmann [33] construct multiscale overlapping clusters on
graphs by performing hierarchical clustering with a similarity
between edges of a graph, instead of its nodes. Their approach
focuses on the explicit construction of the hierarchy of the
overlapping clusters, whereas our focus is on employing a
transform and a metric derived from such a multiscale over-
lapping organization of the features. In contrast to clustering,
our approach allows for the organization and analysis of the
observations.

IV. TREE-BASED METRIC

The success of the data organization and the reﬁnement of
the partition trees depends on the metric used to construct
the trees. We assume that a good organization of the data
recovers smooth joint clusters of observations and features.
Therefore, a metric for comparing pairs of observations should
not only compare their values for individual features (as in
the Euclidean distance), but also across clusters of features,
which are expected to have similar values. Thus, we present a

7

metric dT in the multiscale representation yielded by the tree
transforms. Using this metric, the construction of the tree on
the features takes into account the structure of the underlying
graph on the observations as represented by its partition tree.
The partition tree on the observations in turn relies on the
graph structure of the features. In each iteration a new tree
is calculated based on the metric from the previous iteration,
and then a new metric is calculated based on the new tree.
This can be seen as updating the dual graph structure of the
data in each iteration. The iterative bi-organization algorithm
is presented in Alg. 1.

A. Tree-based EMD

Coifman and Leeb [25] deﬁne a tree-based metric approxi-
mating the EMD in the setting of hierarchical partition trees.
Given a 2D matrix Z, equipped with a partition tree on the
features TX , consider two observations y, y(cid:48) ∈ Y. The tree-
based metric between the observations is deﬁned as

dTX (y, y(cid:48)) =

|m(y − y(cid:48), I)|,

(18)

(cid:19)β

(cid:18) |I|
nX

(cid:88)

I∈TX

where β is a parameter that weights the folders in the tree
based on their size. Following our formulation of the trees
inducing linear transforms, this tree-based metric can be seen
as a weighted l1 distance in the space of the averaging
transform.

Theorem 3: [23, Theorem 4.1] Given a partition tree on
the features TX , deﬁne the NX × NX diagonal weight matrix
W[i, i] =
. Then the tree metric (18) between two
observations y, y(cid:48) ∈ RnX is equivalent to the weighted l1
distance between the averaging transform coefﬁcients:

(cid:16) |Ii|
nX

(cid:17)β

dTX (y, y(cid:48)) = (cid:107)W(ˆy − ˆy(cid:48))(cid:107)1.

(19)

Proof: An element of the vector W(ˆy − ˆy(cid:48)) is

(WM(y − y(cid:48)))[i] =

W[i, j](M(y − y(cid:48)))[j]

(cid:88)

j

= W[i, i](M(y − y(cid:48)))[i]

(20)

(cid:19)β

=

(cid:18) |Ii|
nX

m(y − y(cid:48), Ii).

Therefore:

(cid:107)W(ˆy − ˆy(cid:48))(cid:107)1 =

|m(y − y(cid:48), I)| (cid:3)

(21)

(cid:19)β

(cid:18) |Ii|
nX

(cid:88)

I∈T

Note that the proposed metric is equivalent to the l1 distance
between vectors of higher-dimensionality than the original di-
mension of the vectors. However, by weighting the coefﬁcients
with W, the effective dimension of the new vectors is typically
smaller than the original dimensionality, as the weights rapidly
decrease to zero based on the folder size and the choice
of β. For positive values of β, the entries corresponding to
the large folders dominate ˆy, while entries corresponding to
small folders tend to zero. This trend is reversed for negative
values of β, with elements corresponding to small folders
dominating ˆy while large folders are suppressed. In both cases,
a threshold can be applied to ˆy or ˆZ so as to discard entries

with low absolute values. Thus, the transforms project the
data onto a low-dimensional space of either coarse or ﬁne
structures. Also, note that interpreting the metric as the l1
distance in the averaging transform space enables us to apply
approximate nearest-neighbor search algorithms suitable for
the l1 distance [35], [36]. This allows to analyze larger datasets
via a sparse afﬁnity matrix.

Deﬁning the metric in the transform space enables us to
easily generalize the metric to a joint-tree metric deﬁned for a
joint pair of trees {TX , TY } (Sec. IV-B), to incorporate several
trees over the features {TX }nT in a multi-tree metric via the
multi-tree transform (Sec. IV-C), and to seamlessly introduce
weights on the transform coefﬁcients by setting the elements
of W (Sec. IV-E). Python code implementing our approach is
available at [37].

B. Joint-tree Metric

The tree-based transforms and metrics can be generalized to
analyzing rank-n tensor datasets. We brieﬂy present the joint-
tree metric to demonstrate that the proposed transforms are
not limited to just 2D matrices, but rather can be extended
to processing and organizing tensor datasets. An example of
such an application was presented in [23].

In [23] we proposed a 2D metric given a pair of partition
trees in the setting of organizing a rank-3 tensor. We reformu-
late this metric in the transform space by generalizing the tree-
based metric to a joint-tree metric using the coefﬁcients of the
joint-tree transform. Given a partition tree TX on the features
and a partition tree TY on the observations,
the distance
between two matrices Z1 and Z2 is deﬁned as

dTX ,TY (Z1, Z2) =

|m(Z1 − Z2, I × J )|

|I|βX |J |βY
nβX
βY
X nY

.

(cid:88)

I∈TX
J ∈TY

(22)
The value m(Z, I × J ) is the mean value of a matrix Z on
the joint folder I × J = {(x, y) | x ∈ I, y ∈ J }:

m(Z, I × J ) =

Z[x, y].

(23)

1
|I||J |

(cid:88)

x∈I,y∈J

Theorem 3 can be generalized to a 2D transform applied to
2D matrices.

Corollary 4: [23, Corollary 4.2] The joint-tree metric (22)
between two matrices given a partition tree TX on the features
and a partition tree TY on the observations is equivalent to the
l1 distance between the weighted 2D multiscale transform of
the two matrices:

dTX ,TY (Z1, Z2) = (cid:107)WX MX (Z1 − Z2)MT

YWY (cid:107)1.

(24)

C. Multi-tree Metric

The deﬁnition of the metric in the transform domain enables
a simple extension to a metric derived from a multi-tree
composition. Given a set of multiple trees {Tt}nT
t=1 deﬁned on
the features X as in Sec. III-D, we deﬁne a multi-tree metric
using the multi-tree averaging tree transform as:

d{T }(y, y(cid:48)) = (cid:107) (cid:102)W(cid:102)M(y − y(cid:48))(cid:107)1,

(25)

8

(cid:17)β

(cid:16) |Ii|
nX

where (cid:102)W is a diagonal matrix whose elements are
for all I ∈ T and for all trees in {Tt}nT
equivalent to averaging the single tree metrics:

t=1. This metric is

(cid:88)

d{T }(y, y(cid:48)) = (cid:107) (cid:102)W(cid:102)M(y − y(cid:48))(cid:107)1
1
nT
1
nT

dT (y, y(cid:48)).

T
(cid:88)

=

=

T

(cid:107)WT MT (y − y(cid:48))(cid:107)1

(26)

Note that in contrast to the joint-tree metric, which incorpo-
rates a pair of trees over both the features and the observations,
here we are referring to a set of trees deﬁned only for the
features, or only for the observations.

A question that arises is how to construct multiple trees? For
matrix denoising in a bi-organization setting, Ankenman [10]
applies a spin-cycling procedure: constructing many trees
by randomly varying the parameters in the partition tree
construction algorithm. Multiple trees can also be obtained by
initializing the bi-organization with different metric choices
for d(0)
X (x, x(cid:48)) (step 2 in Alg. 1), e.g., Euclidean, correlation,
etc. Another option, which we demonstrate experimentally on
real data in Sec. V, arises when we have multiple data sets
of observations with the same set of features, or multiple data
sets with the same observations but different features as in
multi-modal settings. In such cases, we construct a partition
tree for each dataset separately and then combine them using
the multi-tree metric.

D. Local Reﬁnement

We propose a new approach to constructing multiple trees,
leveraging the partition of the data during the bi-organization
procedure. This approach is based on a local reﬁnement of
the partition trees, which results in a smoother organization
of the data. The bi-organization method is effective when
correlations exist among both observations and features, by
revealing a hierarchical organization that is meaningful for all
the data together. Yet, since the bi-organization approach is
global and takes all observations and all features into account,
it needs to achieve the best organization on average. However,
the correlations between features may differ among sub-
populations in the data, i.e. the correlations between features
depend on the set of observations taken into account (and vice-
versa).

For example, consider a dataset of documents where the ob-
servations Y are documents belonging to different categories,
the features X are words and Z(x, y) indicates whether a
document y contained a word x. Grouping the words into
disjoint folders forces a single partition of the vocabulary that
disregards words that belong to more than one conceptual
group of words. These connections could be revealed by taking
into account the context, i.e. the subject of the documents.
By diving the documents into a few contextual clusters, and
calculating a local tree on the words TX for each such cluster,
the words are grouped together conceptually according to
the local category. The word “ﬁeld” for example will be
joined with different neighbors, depending on whether the

Algorithm 1 Bi-organization Algorithm [10, Sec. 5.3]
Initialization
Input Dataset Z of features X and observations Y

X , weight function on tree

1: Starting with features X
2:

Calculate initial metric d(0)
X (x, x(cid:48))
Calculate initial ﬂexible tree T (0)
X .

3:

Iterative analysis
Input Flexible tree on features T (0)

folders W[i, i] = ω(Ii)

4: for n ≥ 1 do
5:

X , calculate tree metric between obser-
(y, y(cid:48)) = (cid:107)WX MX (y − y(cid:48))(cid:107)1

Given tree T (n)
vations d(n)
TX
Calculate ﬂexible tree on the observations T (n)
Y .
Repeat steps 5-6 for the features X given T (n)
obtain T (n+1)

and

Y

.

X

8: end for

Algorithm 2 Bi-organization local reﬁnement
Input Dataset Z, observation tree TY

1: Choose level l in tree TY
2: for j ∈ {1, ..., n(l)} do
3:
4:

Set ω(Ji) = 1 ∀Ji ⊆ Jl,j, otherwise ω(Ji) = 0.
Calculate initial afﬁnity on features for subset of
observations as weighted tree-metric d(0)(x, x(cid:48)) =
dTY (x, x(cid:48); ω(Jj))
Calculate initial ﬂexible tree on features T (0)
Perform iterative analysis (steps 4-8 in Alg. 1) for Z
on X and (cid:101)Y = {y | y ∈ J ∈ TY }.

X

7: end for
8: Merge observation trees {T

}n(l) back into global tree

(cid:101)Yj

Output Reﬁned observation tree TY , Set of feature trees

TY

{TX }n(l)
i=1

6:

7:

5:
6:

analysis is applied to documents belonging to “agriculture”,
“mathematics” or “sports”.

Therefore, we propose to take advantage of the unsupervised
clustering obtained by the partition tree on the observations
TY , and apply a localized bi-organization to folders of ob-
servations. Formally, we apply the bi-organization algorithm
to a subset of Z containing all features X and a subset of
observations belonging to the same folder (cid:101)Y = {y | y ∈ J ∈
TY }. This local bi-organization results in a pair of trees: a
local tree T
(cid:101)Y organizing the subset of observations (cid:101)Y, and
a feature tree TX that organizes all the features X based on
this subset of observations that share the same local structure,
rather than the global structure of the data. This reveals the
correlations between features for this sub-population of the
data, and provides a localized visualization and exploratory
analysis for subsets of the data discovered in an unsupervised
manner. This is meaningful when the data is unbalanced and a
subset of the data differs drastically from the rest of the data,
e.g., due to anomalies.

We propose a local reﬁnement of the bi-organization as
follows. We select a single layer l of the observations tree

9

(cid:101)Yj

TY , and perform a separate localized organization for each
folder Jl,j ∈ Pl,
j ∈ {1, ..., n(l)}. We thus obtain n(l)
}n(l)
local observation trees {T
j=1, which we then merge back
into one global tree, with reﬁned partitioning. Merging is
performed by replacing the branch in TY whose root is Jl,j,
i.e. {J ∈ TY |J ⊆ Jl,j}, with the local observation tree T
.
(cid:101)Yj
In addition, we obtain a set of several corresponding trees on
the full set of features {TX }n(l), which we can use to calculate
a multi-tree metric (25). Our local reﬁnement algorithm is
presented in Alg. 2. Applying this algorithm to reﬁne the
global structures of both TY and TX results in a smoother
bi-organization of the data.

We typically apply the reﬁnement to a high level of the tree
since at these levels large clusters of distinct sub-populations
are grouped together, and their separate analysis will reveal
their local organization. The level can be chosen by applying
the difference transform and selecting a level at which the
folders grouped together are heterogeneous, i.e. their mean
signiﬁcantly differs from the mean of their parent folder.

Note that this approach is unsupervised and relies on the
data-driven organization of the data. However, this approach
can also be used in a supervised setting, when there are labels
on the observations. Then we calculate a different partition
tree on the features for each separate label (or sets of labels)
of the observations, revealing the hierarchical structure of the
features for each label. This will be explored in future work.

E. Weight Selection

The calculation of the metric depends on the weight attached
to each folder. We generalize the metric such that the weight
is W[i, i] = ω(Ii), where ω(Ii) > 0 is a weight function
associated with folder Ii. The weights can incorporate prior
smoothness assumptions on the data, and also enable to en-
hance either coarse or ﬁne structures in the similarity between
samples.

(cid:17)β

(cid:16) |Ii|
nX

The choice ω(Ii) =

in [25] makes the tree-based
metric (18) equivalent to EMD, i.e., the ratio of EMD to
the tree-based metric is always between two constants. The
parameter β weights the folder by its relative size in the tree,
where β > 0 emphasizes coarser scales of the data, while
β < 0 emphasizes differences in ﬁne structures.

Ankenman [10] proposed a slight variation to the weight

also encompassing the tree structure:

ω(Ii) = 2−αl(Ii)

(cid:18) |Ii|
nX

(cid:19)β

,

(27)

where α is a constant and l(Ii) is the level at which the folder
Ii is found in T . The constant α weights all folders in a
given level equally. Choosing α = 0 resorts to the original
weight. The structure of the trees can be seen as an analogue
to a frequency decomposition in signal processing, where
the support of a folder is analogous to a certain frequency.
Moreover, since high levels of the tree typically contain large
folders, they correspond to low-pass ﬁlters. Conversely, lower
levels of the tree correspond to high-pass ﬁlters as they contain
many small folders. Thus setting α > 0 corresponds to
emphasizing low frequencies whereas α < 0 corresponds to

10

enhancing high frequencies. In an unbalanced tree, where a
small folder of features remains separate for all levels of the
tree (an anomalous cluster of features), α can be used to
enhance the importance of this folder, as opposed to β, which
would decrease its importance based on its size.

We propose a different approach. Instead of weighting the
folders based on the structure of the tree, which requires a-
priori assumptions on the optimal scale of the features or the
observations, we set the folders weights based on their content.
By applying the difference transform to the data, we obtain a
measure for each folder deﬁning how homogeneous it is. This
reduces the number of parameters in the algorithm, which is
advantageous in the unsupervised problem of bi-organization.
We calculate for each folder, the norm of its difference on the
dataset Z:
(cid:32)

(cid:33)1/2

ω(Ii) =

((∆X Z)[i, y])2

(cid:88)

y



(cid:32)

(cid:88)

(cid:88)

=



y

x

(m(y(x), Il,i) − m(y(x), Il+1,j)

1/2

(cid:33)2


,

(28)
where Il,i ⊂ Il+1,j. This weight is high when Il,i (cid:28) Il+1,j.
This means that the parent folder joining Il,i with other folders
contains non-homogeneous “populations”. Therefore, assign-
ing a high weight to Il,i places importance on differentiating
these different populations.

The localized reﬁnement procedure in Alg. 2 can also be
formalized as assigning weights ω(I) in the tree metric. We
set all weights containing a branch of the tree (a folder and
all its sub-folders) to 1 and set all other weights to zero:
(cid:26) 1,
0,

Ii ⊆ Ij
otherwise,

ω(Ii) =

(29)

where Ij is the root folder of the branch. Thus, using these
weights, the metric is calculated based only on a subset of the
observations ˜Y. This metric can initialize a bi-organization
procedure of a subset of Z containing X and ˜Y.

F. Coherence

To assess the smoothness of the bi-organization stemming
from the constructed partition trees, a coherency criterion was
proposed in [9]. The coherency criterion is given by

C(Z; TX , TY ) =

(cid:107)ΨX ZΨT

Y (cid:107)1,

(30)

1
nX nY

where Ψ is a Haar-like orthonormal basis proposed by Gavish,
Nadler and Coifman [13] in the settings of partition trees,
and it depends on the structure of a given tree. This criterion
measures the decomposition of the data in a bi-Haar-like basis
induced by two partition trees TX and TY : ΨX ZΨT
Y . The
lower the value of C(Z; TX , TY ), the smoother the organiza-
tion is in terms of satisfying the mixed H¨older condition (1).
Minimizing the coherence can be used as a stopping con-
dition for the bi-organization algorithm presented in Alg. 1.
The bi-organization continues as long as C(Z; T (n)
Y ) <
C(Z; T (n−1)
) [9]. However, we have empirically
X

X , T (n)

, T (n−1)
Y

found that the iterative process typically converges within
only few iterations. Therefore, in our experimental results we
perform n = 2 iterations.

V. EXPERIMENTAL RESULTS

Analysis of cancer gene expression data is of critical impor-
tance in jointly identifying subtypes of cancerous tumors and
genes that can distinguish the subtypes or indicate a patient’s
long-term survival. Identifying a patient’s tumor subtype can
determine the course of treatment, such as recommendation of
hormone therapy in some subtypes of breast cancer, and is a
an important step toward the goal of personalized medicine.
Biclustering of breast cancer data has identiﬁed sets of genes
whose expression levels categorize tumors into ﬁve subtypes
with distinct survival outcomes [38]: Luminal A, Luminal
B, Triple negative/basal-like, HER2 type and “Normal-like”.
Related work has aimed to classify samples into each of
these subtypes or identify other types of signiﬁcant clusters
based on gene expression, clinical features and DNA copy
number analysis [39]–[42]. The clustered dendrogram obtained
by agglomerative hierarchical clustering of the genes and the
subjects is widely used in the analysis of gene expression data.
However, in contrast to our approach, hierarchical clustering
is usually applied with a metric, such as correlation, that
is global and linear, and does not
the
structure revealed by the multiscale tree structure of the other
dimension. Conversely, our approach enables us to iteratively
update both the tree and metric of the subjects based on the
metric for the genes, and update the tree and metric of the
genes based on the metric for the subjects.

take into account

We analyze three breast cancer gene expression datasets,
where the features are the genes and the observations are the
tumor samples. The ﬁrst dataset is the METABRIC dataset,
containing gene expression data for 1981 breast tumors [40]
collected with a gene expression microarray. We denote this
dataset ZM, and its set of samples YM. The second dataset,
ZT, is taken from The Cancer Genome Atlas (TCGA) Breast
Cancer cohort [43] and consists of 1218 samples, YT. This
dataset was proﬁled using RNA sequencing, which is a newer
and more advanced gene expression technology. The third
dataset ZB (BRCA-547) [41], comprising of 547 samples YB,
was acquired with microarray technology. These 547 samples
are also included in the TCGA cohort, but the gene expression
was proﬁled using a different technology.

We selected X to be the 2000 genes with the largest variance
in METABRIC from the original collection of ∼ 40000 gene
probes. In related work, the analyzed genes were selected in
a supervised manner based on prior knowledge or statistical
signiﬁcance in relation to patient survival
time [38]–[40],
[42], [44]. Here we present results of a purely unsupervised
approach aimed at exploratory analysis of high-dimensional
data, and we do not use the survival information or subtypes
labels in either applying our analysis or for gene selection, but
only in evaluating the results. In the remainder of this section
we present three approaches in which the tree transforms
and metrics are applied for the purpose of unsupervised
organization of gene expression data.

11

Fig. 4. Global bi-organization of the METABRIC dataset. The samples (columns) and genes (rows) have been reordered so they correspond to the leaves
of the two partition trees. Below the organized data are clinical details for each of the samples: two types of breast cancer subtype labels (reﬁned [42] and
PAM50 [39]) hormone receptor status (ER, PR) and HER2 status.

Regarding implementation, in this application we use ﬂex-
the partition trees in the bi-
ible trees [10] to construct
organization. We initialize the bi-organization with a corre-
lation afﬁnity on the genes (d(0)
X (x, x(cid:48)) in Alg. 1, Step 2),
which is commonly used in gene expression analysis.

A. Subject Clustering

We begin with a global analysis of all samples of the
METABRIC data using the bi-organization algorithm pre-
sented in Alg. 1. We perform two iterations of the bi-
organization using the tree-based metric with the data-driven
weights deﬁned in (28). The organized data and corresponding
trees on the samples and on the genes are shown in Fig. 4.
The samples and genes have been reordered such that they
correspond to the leaves of the two partition trees. Below
the organized data we provide clinical details for each of
the samples: two types of breast cancer subtype labels, the
reﬁned labels introduced in [42] and the standard PAM50
subtypes [39], hormone receptor status (ER, PR) and HER2
status. We analyze the folders of level l = 5 on the samples
tree, which divides the samples into ﬁve clusters (the folders
are marked with numbered colored circles).

In Fig. 5 we present histograms of the reﬁned subtype labels
for each of the numbered folders in the samples tree, and
plot the disease-speciﬁc survival curve of each folder in the
bottom graph. The histograms of each folder is surrounded by
a colored border corresponding to the colored circle indicating

TABLE I
METABRIC SELF ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)
DTC [46]
sparseBC [12]

RI
0.79
0.72
0.72
0.69
0.74
0.72
0.74
0.76

ARI VI
0.45
0.30
0.23
0.20
0.30
0.26
0.19
0.33

1.48
1.77
1.98
1.94
1.84
1.90
2.45
1.74

p-value
4.35 × 10−21
1.11 × 10−17
8.48 × 10−10
1.46 × 10−12
1.11 × 10−16
5.23 × 10−11
5.54 × 10−22
2.6 × 10−19

the relevant folder in the tree in Fig. 4. Note that the folders do
not just separate data according to subtype as in the dark blue
and light blue folders (Basal and Her2 respectively), but also
separate data according to the survival rates. If we compare the
orange and green folders that are grouped in the same parent
folder, both contain a mixture of Luminal A and Luminal B,
yet they have distinctive survival curves. The p-value of this
separation using the log-rank test [45] was 4.35 × 10−21.

We next compare our weighted metric (28) to the original
EMD-like metric (18), using different values of β and α in
(27). These values were chosen in order to place different em-
phasis of the transform coefﬁcients depending on the support
of the corresponding folders or the level of the tree. The values
of β enable to emphasize large folders (β = 1), small folders

12

the variation of information (VI) [49]. The RI and ARI
measure the similarity between two clusterings (or partitions)
of the data. Both measures indicate no agreement between the
partitions by 0 and perfect agreement by 1, however ARI can
return negative values for certain pairs of clusterings. The third
measure is an information theoretic criterion, where 0 indicates
perfect agreement between two partitions. Finally, we perform
survival analysis using Kaplan-Meier estimate [50] of disease-
speciﬁc survival rates of the samples, reporting the p-value of
the log-rank test [45]. A brief description of these statistics is
provided in Appendix II.

We select clusters by partitioning the samples into the
folders J of the samples tree TX , at a single level l of the tree
which divides the data into 4-6 clusters (typically level L − 2
in our experiments). This follows the property of ﬂexible trees
that the level at which folders are joined is meaningful across
the entire dataset, as for each level the distances between
joined folders are similar. For other types of tree construction
algorithms, alternative methods can be used to select clusters
in the tree, such as SigClust used in [41].

Results are presented in Table I for the METABRIC dataset
and in Table II for the BRCA-547 dataset. For the METABRIC
dataset, using the weighted metric achieves the best results
compared to the other weight selections, in terms of both
clustering relative to the ground-truth labels and the survival
curves of the different clusters (note these two criteria do
not always coincide). While DTC achieves the lowest p-
value overall, it has very poor clustering results compared
to the ground-truth labels (lowest ARI and highest VI). The
weighted metric out-performed the sparseBC method, which
has second-best performance for the clustering measures, and
third-lowest p-value. For the BRCA-547 dataset, the weighted
metric achieves the best clustering in terms of the ARI measure
and has the lowest p-value. For the VI measure, the clustering
by the weighted metric was slightly larger but comparable
to that of the lowest score. On this dataset, DTC performed
poorly with highest VI and p-value. The sparseBC method
achieved good clustering with highest RI and ARI measures,
but had a high p-value and VI compared to the performance
of our bi-organization method.

The results indicate that the data-driven weighting achieves
comparable if not better performance, than both using the tree-
dependent weights and competing biclustering methods. Thus,
the data-driven weighting provides an automatic method to
set appropriate weights on the transform coefﬁcients in the
metric. Our method is completely data-driven, as opposed to
the sparseBC method which requires as input the number of
features and observations to decompose the data into. (We used
the provided computationally expensive cross-validation pro-
cedure to select the best number of clusters in each dimension).
In addition, our approach provides a multiscale organization,
whereas sparseBC yields a single-scale decomposition of the
data. The DTC is a multiscale approach, however as it relies
on hierarchical clustering it does not take into account the
dendrogram in the other dimension. The performance may
be improved by using dendrograms in our iterative approach,
instead of the ﬂexible trees (this is further discussed below).

Fig. 5. (top) Histograms of folders in sample tree of METABRIC. The color
of the border corresponds to the circles in the tree. (bottom) Survival curves
for each folder.

TABLE II
BRCA-547 SELF ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)
DTC [46]
sparseBC [12]

RI
0.75
0.75
0.74
0.72
0.74
0.74
0.75
0.76

ARI VI
0.38
0.37
0.36
0.35
0.34
0.35
0.35
0.38

1.38
1.39
1.37
1.33
1.56
1.45
1.63
1.49

p-value
0.0004
0.0073
0.0028
0.0773
0.0010
0.0130
0.0853
0.0269

(β = −1) and weighting all folders equally (β = 0). The
values of α either emphasize high levels of the tree (α = 0.5),
low levels of the tree (α = −1) or weighting all levels equally
(α = 0).

We also compare to two other biclustering methods. The
ﬁrst is the dynamic tree cutting (DTC) [46] applied to a hier-
archical clustering dendrogram obtained using mean linkage
and correlation distance (a popular choice in gene expression
analysis). The second is the sparse biclustering method [12],
where the authors impose a sparse regularization on the mean
values of the estimated biclsuters (assuming the mean of
the dataset
is zero). Both algorithms are implemented in
R: package dynamicTreeCut and package sparseBC,
respectively.

We evaluate our approach by both measuring how well the
obtained clusters represent the cancer subtypes, and estimating
the statistical signiﬁcance of the survival curves of the clusters.
We compare the clustering of the samples relative to the
reﬁned subtype labels [42] using three measures: the Rand
index (RI) [47], the adjusted Rand index (ARI) [48], and

13

TABLE III
COHERENCY OF REFINED BI-ORGANIZATION

TABLE IV
METABRIC EXTERNAL ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)

Global TX
and TY
0.7039
0.7066
0.7051
0.7028
0.7051
0.7075

Reﬁned
TX
0.6103
0.6107
0.6118
0.6130
0.6119
0.6141

Reﬁned
TY
0.5908
0.5928
0.5921
0.5972
0.5927
0.5934

Reﬁned
TX , TY
0.5463
0.5480
0.5472
0.5668
0.5487
0.5497

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)

RI
0.74
0.73
0.72
0.73
0.72
0.73

ARI VI
0.30
0.29
0.26
0.28
0.27
0.25

1.77
1.87
1.87
1.83
1.89
1.98

p-value
3.71 × 10−19
7.78 × 10−16
1.77 × 10−16
4.25 × 10−14
7.02 × 10−6
3.33 × 10−16

B. Local reﬁnement

In Table III we demonstrate the improvement gained in the
organization by applying the local reﬁnement to the partition
trees, where we measure the smoothness of the organized data
using the coherency criterion (30). We perform bi-organization
for different values of β and α as well as the weighted
metric, and compare 4 organizations: 1) Global organization;
2) Reﬁned organization of only the genes tree TX ; 3) Reﬁned
organization of only the samples tree TY ; and 4) Reﬁned
organization of both the features and the samples (reﬁned TX
and TY ). Applying the reﬁned local organization to both the
genes and the samples, yields the best result with regard to
the smoothness of the bi-organization. We also examined the
the effect of the level of the tree on which the reﬁnement is
performed for l ∈ {5, 6, 7} for both trees, and the improvement
gained by reﬁnement was of the same order for all combina-
tions. The results demonstrate that regardless of the weighting
(data-driven or folder dependent), the reﬁnement procedure
improves the coherency of the organization.

C. Bi-organization with multiple datasets

Following the introduction of gene expression proﬁling by
RNA sequencing, an interesting scenario is that of two datasets
proﬁled using different technologies, one using microarray
and the other RNA sequencing. Consider, for example, the
METABRIC dataset ZM and the TCGA dataset ZT, which
share the same features X (in this case genes), but collected for
two different sample sets, YM and YT respectively. In this case,
the gene expression proﬁles have different dynamic range and
are normalized differently, and the samples cannot be analyzed
together simply by concatenating the datasets. However, the
hierarchical structure we learn on the genes, which deﬁnes a
multiscale clustering of the genes, is informative regardless of
the technique used to acquire the expression data.

Thus, the gene metric learned from one dataset can be
applied seamlessly to another dataset and used to organize
its samples due to the coupling between the genes and the
samples. We term this “external-organization”, and demon-
strate how it organizes the METABRIC dataset ZM using the
TCGA dataset ZT. We ﬁrst apply the bi-organization algorithm
to organize ZT, and then we derive the gene tree-based metric
dTX from the constructed tree on the genes TX . This metric
is then used to a construct a new tree TY on the samples set
YM of ZM.

In Table IV we compare the external organization of
METABRIC using our weighted metric to the original EMD-

like metric for different values of β and α. Our results
show that the data-driven weights achieve the best results,
reinforcing that learning the weights in a data-adaptive way
is more beneﬁcial than setting the weights based on the size
of the folders or the level of the tree. Applying external
organization enables us to assess which bi-organization of the
external dataset and corresponding learned metric were the
most meaningful. Note that for some of the parameter choices
(α = 0, β = 1 or β = −1), the external organization of
ZM using a gene tree learned from the dataset ZT was better
than the internal organization. Thus, via the organization of the
dataset ZM, we validate that the hierarchical organization of
the genes in ZT, and therefore, the corresponding metric, are
effective in clustering samples into cancer subtypes. This also
demonstrates that the hierarchical gene organization learned
from one dataset can be successfully applied to another dataset
to learn a meaningful sample organization, even though the
two were proﬁled using different technologies. This provides
motivation to integrate information from datasets together.

In our ﬁnal evaluation, we divide the METABRIC dataset
into its two original subsets: the discovery set comprising 997
tumors and the validation set comprising 995 tumors. Note
that the two sets have different sample distributions of cancer
subtypes. We compare three approaches for organizing the
data. We begin with the self-organization as in Sec. V-A.
We organize each of the two datasets separately and report
their clustering measures in the ﬁrst row in Table V for the
discovery cohort and in Table VI for the validation cohort.
Note that the organization achieved using half the data is
less meaningful in terms of the survival rates compared to
using all of the data. This is due to the different distribution
of subtypes and survival times between the discovery and
validation cohorts, and in addition, the p-value calculation
itself is dependent on the sample size used.

One of the important aspects in a practical application is the
ability to process new samples. Our approach naturally allows
for such a capability. Assume we have already performed bi-
organization on an existing dataset and we acquire a few new
test samples. Instead of having to reapply the bi-organization
procedure to all of the data, we can instead insert the new
samples into the existing organization. We demonstrate this
by using each subset of the METABRIC dataset to organize
the other. In contrast to the external organization example, here
we have two datasets proﬁled with the same technology. We
can treat this as a training and test set scenario: construct a
sample tree on the training set Ytrain and use the learned metric
on the genes dTX to insert samples from the test set Ytest into

the training sample tree TYtrain . First, we calculate the centroids
of the folders Jj of level l = 1 (the level above the leaves) in
the samples tree TYtrain:
(cid:88)

MY [j, y]Z[x, y], x ∈ {1, ..., nX },

l(Jj) = 1

Cj(x) =

y

(31)
These can be considered the representative sample of each
folder. We then assign each new sample y ∈ Ytest
to its
nearest centroid using the metric dTX (y, Cj) derived from
the gene tree TX . Thus, we reconstruct the sample hierarchy
on the test dataset Ytest by assigning each test sample to
the hierarchical clustering of the low-level centroids from the
training sample tree. This approach, therefore, validates the
sample organization as well as the gene organization, whereas
the external organization only enables to validate the gene
organization.

We perform this once treating the validation set as the
training set and the discovery set as the test set, and then vice-
versa. We report the clustering measures in the second row of
Table V and Table VI. Note that the measures are reported only
for the samples belonging to the given set in the table. Inserting
samples from one dataset into the sample tree of another
demonstrates an improved organization in some measures
compared to performing self-organization. For example, the
organization of the discovery set via the validation tree results
in a clustering with improved ARI and VI measures. This
serves as additional evidence for the importance of integrating
information from several datasets together.

Thus far in our experiments, we have gathered substantial
evidence for the importance of information stemming from
multiple data sets. Here, we harness the multiple tree met-
ric (25) to perform integration of datasets in a more systematic
manner. We generalize the external organization method to
several datasets, where we integrate all the learned trees on
the genes {TX } into a single metric via the multi-tree metric.
In addition to the gene tree from both METABRIC datasets,
we also obtain the gene trees from the TCGA and the BRCA-
547 datasets, ZT and ZB. We then calculate a multi-tree met-
ric (25) to construct the sample tree on either the discovery or
validation sets. We report the evaluation measures in the third
row of Table V and Table VI. Taking into account all measures,
the multi-tree metric incorporating four different datasets best
organizes both the discovery and validation datasets. Integrat-
ing information from multiple sources improves the accuracy
of the organization, as averaging the metrics emphasizes
genes that are consistently grouped together, representing the
intrinsic structure of the data. In addition, since the metric
integrates the organizations from several datasets, it is more
accurate than the internal organization of a dataset with few
samples or a non-uniform distribution of subtypes.

Our results show that external organization, via either both
single or multi-tree metric, enables us to learn a meaningful
multi-scale hierarchy on the genes and apply it as a metric to
organize the samples of a given dataset. Thus, we can apply
information from one dataset to another to recover a multi-
scale organization of the samples, even if they were proﬁled
in a different technique. In addition, we obtain a validation of

14

TABLE V
METABRIC DISCOVERY ORGANIZATION

discovery
Self-organization
Inserted into
validation tree
Multi-tree

RI
0.75

ARI VI
0.33

1.81

p-value
1.82 × 10−11

0.74

0.34

1.66

2.93 × 10−9

0.75

0.35

1.63

3.18 × 10−13

TABLE VI
METABRIC VALIDATION ORGANIZATION

validation
Self-organization
Inserted into
discovery tree
Multi-tree

RI
0.77

ARI VI
0.33

1.82

p-value
3.07 × 10−4

0.76

0.30

1.98

9.08 × 10−8

0.76

0.34

1.73

4.24 × 10−9

the gene organization of one dataset via another. This cannot
be accomplished with traditional hierarchical clustering in a
clustered dendrogram as the clustering of the samples does not
depend on the hierarchical structure of the genes dendrogram.
However, we can obtain an iterative hierarchical clustering
algorithm for biclustering using our approach. As our bi-
organization depends on a partition tree method, we can use
hierarchical clustering instead of ﬂexible trees in the iterative
bi-organization algorithm. Alternatively, as hierarchical clus-
tering depends on a metric, this can also be formulated as
deriving a transform from the dendrogram on the genes and
using its corresponding tree-metric instead of correlation as
the input metric to the hierarchical clustering algorithm on
the samples, and vice-versa.

In related work, Cheng, Yang and Anastassiou [51] analyzed
multiple datasets and identiﬁed consistent groups of genes
across datasets. Zhou et al. [52] integrate datasets in a platform
independent manner to identify groups of genes with the same
function across multiple datasets. The multi-tree transform can
also be used to identify such genes, however this is beyond
the scope of this paper and will be addressed in future work.

D. Sub-type labels

In breast cancer, PAM50 [39] is typically used to assign
intrinsic subtypes to the tumors. However, Milioli et al. [42]
recently proposed a reﬁned set of subtypes labels for the
METABRIC dataset, based on a supervised iterative approach
to ensure consistency of the labels using several classiﬁers.
Their labels are shown to have a better agreement with
the clinical markers and patients’ overall survival than those
provided by the PAM50 method. Therefore, the clustering
measures we reported on the METABRIC dataset were with
respect to the reﬁned labels.

Our unsupervised analysis demonstrated higher consistency
with the reﬁned labels than with PAM50. Thus, our unsuper-
vised approach provides an additional validation to the labeling
achieved in a supervised manner. We divided the data into
training and test sets and classiﬁed the test set using k-NN
nearest neighbors with majority voting using the tree-based
metric. For different parameters and increasing numbers of

genes (nX = 500, 1000, 2000), we had higher agreement with
the reﬁned labels than with PAM50, achieving a classiﬁcation
accuracy of 82% on average. Classifying with the PAM50
labels had classiﬁcation accuracy lower by an average of
10% ± 2%. This is also evident when examining the labels
in Fig. 4. Note that whereas PAM50 assigns a label based
on 50 genes and the reﬁned labels were learned using a
subset of genes found in a supervised manner, our approach
is unsupervised using the nX genes with the highest variance.

VI. CONCLUSIONS

In this paper we proposed new data-driven tree-based trans-
forms and metrics in a matrix organization setting. We pre-
sented partition trees as inducing a new multiscale transform
space that conveys the smooth organization of the data, and
derived a metric in the transform space. The trees and cor-
responding metrics are updated in an iterative bi-organization
approach, organizing the observations based on the multiscale
decomposition of the features, and organizing the features
based on the multiscale decomposition of the observations.
In addition, we generalized the transform and the metric
to incorporate multiple partition trees on the data, allowing
for the integration of several datasets. We applied our data-
driven approach to the organization of breast cancer gene
expression data, learning metrics on the genes to organize the
tumor samples in meaningful clusters of cancer sub-types. We
demonstrated how our approach can be used to validate the
hierarchical organization of both the genes and the samples
by taking into account several datasets of samples, even
when these datasets were proﬁled using different technolo-
gies. Finally, we employed our multi-tree metric to integrate
information from the organization of these multiple datasets
and achieved an improved organization of tumor samples.

In future work, we will explore several aspects of the
multiple tree setting. First, the multi-tree transform and metric
can be incorporated in the iterative framework for further
reﬁnement. Second, we will generalize the coherency measure
to incorporate multiple trees. Third, we will apply the multi-
tree framework to a multi-modal setting, where observations
are shared across datasets, as for example, in the joint samples
shared by the BRCA-547 and TCGA datasets. Finally, we will
reformulate the iterative procedure as an optimization problem,
enabling to explicitly introduce cost functions. In particular,
cost functions imposing the common structure of the multiple
trees across datasets will be considered.

ACKNOWLEDGMENTS

The authors thank the anonymous reviewers for their con-

structive comments and useful suggestions.

REFERENCES

15

[4] W. H. Yang, D. Q. Dai, and H. Yan, “Finding correlated biclusters from
gene expression data,” IEEE Trans. Knowl. Data Eng., vol. 23, no. 4,
pp. 568–584, April 2011.

[5] E. C. Chi, G. I. Allen, and R. G. Baraniuk, “Convex biclustering,”
Biometrics, 2016. [Online]. Available: http://dx.doi.org/10.1111/biom.
12540

[6] D. Jiang, C. Tang, and A. Zhang, “Cluster analysis for gene expression
data: a survey,” IEEE Trans. Knowl. Data Eng., vol. 16, no. 11, pp.
1370–1386, 2004.

[7] J. Bennett and S. Lanning, “The Netﬂix prize,” in Proceedings of KDD

cup and workshop, vol. 2007, 2007, p. 35.

[8] S. Busygin, O. Prokopyev, and P. M. Pardalos, “Biclustering in data
mining,” Computers & Operations Research, vol. 35, no. 9, pp. 2964 –
2987, 2008.

[9] M. Gavish and R. R. Coifman, “Sampling, denoising and compression
of matrices by coherent matrix organization,” Appl. Comput. Harmon.
Anal., vol. 33, no. 3, pp. 354 – 369, 2012.

[10] J.

I. Ankenman, “Geometry and analysis of dual networks on
questionnaires,” Ph.D. dissertation, Yale University, 2014. [Online].
Available: https://github.com/hgfalling/pyquest/blob/master/ankenman
diss.pdf

[11] Y. Kluger, R. Basri, J. T. Chang, and M. Gerstein, “Spectral biclus-
tering of microarray data: coclustering genes and conditions,” Genome
research, vol. 13, no. 4, pp. 703–716, 2003.

[12] K. M. Tan and D. M. Witten, “Sparse biclustering of transposable data,”

J. Comp. Graph. Stat., vol. 23, no. 4, pp. 985–1008, 2014.

[13] M. Gavish, B. Nadler, and R. R. Coifman, “Multiscale wavelets on
trees, graphs and high dimensional data: Theory and applications to
semi supervised learning,” in Proc. ICML, 2010, pp. 367–374.

[14] A. Singh, R. Nowak, and R. Calderbank, “Detecting weak but
hierarchically-structured patterns in networks,” in Proc. AISTATS, vol. 9,
May 2010, pp. 749–756.

[15] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on
graphs via spectral graph theory,” Appl. Comput. Harmon. Anal., vol. 30,
no. 2, pp. 129 – 150, 2011.

[16] J. Sharpnack, A. Singh, and A. Krishnamurthy, “Detecting activations
over graphs using spanning tree wavelet bases.” in AISTATS, 2013, pp.
536–544.

[17] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-
dergheynst, “The emerging ﬁeld of signal processing on graphs: Ex-
tending high-dimensional data analysis to networks and other irregular
domains,” IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83–98, 2013.
[18] S. K. Narang and A. Ortega, “Compact support biorthogonal wavelet
ﬁlterbanks for arbitrary undirected graphs,” IEEE Trans. Signal Process.,
vol. 61, no. 19, pp. 4673–4685, Oct 2013.

[19] A. Sakiyama, K. Watanabe, and Y. Tanaka, “Spectral graph wavelets
and ﬁlter banks with low approximation error,” IEEE Trans. Signal Inf.
Process. Netw., vol. 2, no. 3, pp. 230–245, Sept 2016.

[20] D. I. Shuman, M. J. Faraji, and P. Vandergheynst, “A multiscale pyramid
transform for graph signals,” IEEE Trans. Signal Process., vol. 64, no. 8,
pp. 2119–2134, April 2016.

[21] N. Tremblay and P. Borgnat, “Subgraph-based ﬁlterbanks for graph
signals,” IEEE Trans. Signal Process., vol. 64, no. 15, pp. 3827–3840,
Aug 2016.

[22] N. Shahid, N. Perraudin, V. Kalofolias, G. Puy, and P. Vandergheynst,
“Fast robust PCA on graphs,” IEEE J. Sel. Topics Signal Process.,
vol. 10, no. 4, pp. 740–756, June 2016.

[23] G. Mishne, R. Talmon, R. Meir, J. Schiller, U. Dubin, and R. R.
Coifman, “Hierarchical coupled-geometry analysis for neuronal structure
and activity pattern discovery,” IEEE J. Sel. Topics Signal Process.,
vol. 10, no. 7, pp. 1238–1253, Oct 2016.

[24] P. Burt and E. Adelson, “The Laplacian pyramid as a compact image
code,” IEEE Trans. Commun., vol. 31, no. 4, pp. 532–540, 1983.
[25] R. R. Coifman and W. E. Leeb, “Earth mover’s distance and equivalent
metrics for spaces with hierarchical partition trees,” Yale University,
Tech. Rep., 2013, technical report YALEU/DCS/TR1482.

[26] I. Ram, M. Elad, and I. Cohen, “Generalized tree-based wavelet trans-
form,” IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4199–4209, 2011.
[27] R. Kondor, N. Teneva, and V. Garg, “Multiresolution matrix factoriza-

[1] Y. Cheng and G. M. Church, “Biclustering of expression data,” in ISMB,

tion,” in Proc. ICML, 2014, pp. 1620–1628.

vol. 8, 2000, pp. 93–103.

[28] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp.

[2] C. Tang, L. Zhang, A. Zhang, and M. Ramanathan, “Interrelated two-
way clustering: an unsupervised approach for gene expression data
analysis,” in Proc. BIBE, 2001, pp. 41–48.

[3] M. Lee, H. Shen, J. Z. Huang, and J. S. Marron, “Biclustering via
sparse singular value decomposition,” Biometrics, vol. 66, no. 4, pp.
1087–1095, 2010.

5–32, 2001.

[29] R. R. Coifman and S. Lafon, “Diffusion maps,” Appl. Comput. Harmon.

Anal., vol. 21, no. 1, pp. 5–30, July 2006.

[30] R. R. Coifman and M. Gavish, “Harmonic analysis of digital data
bases,” in Wavelets and Multiscale Analysis, ser. Applied and Numerical
Harmonic Analysis. Birkh¨auser Boston, 2011, pp. 161–197.

[31] G. Mishne, “Diffusion nets and manifold learning for high-dimensional
data analysis in the presence of outliers,” Ph.D. dissertation, Technion,
2016.

[32] M. Zontak, I. Mosseri, and M. Irani, “Separating signal from noise using

patch recurrence across scales,” in Proc. CVPR, June 2013.

[33] Y.-Y. Ahn, J. P. Bagrow, and S. Lehmann, “Link communities reveal
multiscale complexity in networks,” Nature, vol. 466, no. 7307, pp. 761–
764, 2010.

[34] J. Xie, S. Kelley, and B. K. Szymanski, “Overlapping community
detection in networks: The state-of-the-art and comparative study,” ACM
Comput. Surv., vol. 45, no. 4, pp. 43:1–43:35, Aug. 2013.

[35] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu,
“An optimal algorithm for approximate nearest neighbor searching ﬁxed
dimensions,” J. ACM, vol. 45, no. 6, pp. 891–923, Nov. 1998.

[36] B.-K. Yi and C. Faloutsos, “Fast time sequence indexing for arbitrary

lp norms,” in Proc. VLDB, 2000.

[37] [Online]. Available: http://github.com/gmishne/pyquest
[38] T. Sørlie et al., “Gene expression patterns of breast carcinomas distin-
guish tumor subclasses with clinical implications,” Proc. Natl. Acad.
Sci., vol. 98, no. 19, pp. 10 869–10 874, 2001.

[39] J. S. Parker et al., “Supervised risk predictor of breast cancer based on
intrinsic subtypes,” Journal of Clinical Oncology, vol. 27, no. 8, pp.
1160–1167, 2009.

[40] C. Curtis et al., “The genomic and transcriptomic architecture of 2,000
breast tumours reveals novel subgroups,” Nature, vol. 486, no. 7403, pp.
346–352, 2012.

[41] Cancer Genome Atlas Network, “Comprehensive molecular portraits of

human breast tumours,” Nature, vol. 490, no. 7418, pp. 61–70, 2012.

[42] H. H. Milioli, R. Vimieiro, I. Tishchenko, C. Riveros, R. Berretta, and
P. Moscato, “Iteratively reﬁning breast cancer intrinsic subtypes in the
METABRIC dataset,” BioData Mining, vol. 9, no. 1, pp. 1–8, 2016.

[43] Cancer Genome Atlas Network. [Online]. Available: https://xenabrowser.
net/datapages/?cohort=TCGA%20Breast%20Cancer%20(BRCA)
[44] C. M. Perou et al., “Molecular portraits of human breast tumours,”

Nature, vol. 406, no. 6797, pp. 747–752, 2000.

[45] R. Peto and J. Peto, “Asymptotically efﬁcient rank invariant test proce-
dures,” Journal of the Royal Statistical Society. Series A (General), vol.
135, no. 2, pp. 185–207, 1972.

[46] P. Langfelder, B. Zhang, and S. Horvath, “Deﬁning clusters from a hier-
archical cluster tree: the dynamic tree cut package for r,” Bioinformatics,
vol. 24, no. 5, pp. 719–720, 2008.

[47] W. M. Rand, “Objective criteria for the evaluation of clustering meth-
ods,” Journal of the American Statistical Association, vol. 66, no. 336,
pp. 846–850, 1971.

[48] L. Hubert and P. Arabie, “Comparing partitions,” J. Classiﬁcation, vol. 2,

no. 1, pp. 193–218, 1985.

[49] M. Meil˘a, “Comparing clusterings - an information based distance,”
Journal of Multivariate Analysis, vol. 98, no. 5, pp. 873 – 895, 2007.
[50] E. L. Kaplan and P. Meier, “Nonparametric estimation from incomplete
observations,” Journal of the American statistical association, vol. 53,
no. 282, pp. 457–481, 1958.

[51] W.-Y. Cheng, T.-H. O. Yang, and D. Anastassiou, “Biomolecular events
in cancer revealed by attractor metagenes,” PLoS Comput Biol, vol. 9,
no. 2, pp. 1–14, 02 2013.

[52] X. J. Zhou et al., “Functional annotation and network reconstruction
through cross-platform integration of microarray data,” Nature biotech-
nology, vol. 23, no. 2, pp. 238–243, 2005.

[53] J. P. Klein and M. L. Moeschberger, Survival analysis: techniques for

censored and truncated data. SSBM, 2005.

APPENDIX I
FLEXIBLE TREES

We brieﬂy describe the ﬂexible trees algorithm, given the
feature set X and an afﬁnity matrix on the features denoted
KX . For a detailed description see [10].

1) Input: The set of features X , an afﬁnity matrix KX ∈

RnX ×nX , and a constant (cid:15).

2) Init: Set partition I0,i = {i} ∀ 1 ≤ i ≤ nX , set l = 1.
3) Given an afﬁnity on the data, we construct a low-

dimensional embedding on the data [29].

4) Calculate

level-dependent
d(l)(i, j) ∀ 1 ≤ i, j ≤ nX in the embedding space.

pairwise

the

distances

16

(cid:15) , where p = median (cid:0)d(l)(i, j)(cid:1).

5) Set a threshold p
6) For each index i which has not yet been added
to a folder, ﬁnd its minimal distance dmin(i) =
minj{d(l)(i, j)}.

• If dmin(i) < p

(cid:15) , i and j form a new folder if j
does not belong to a folder. If j is already part of a
folder I, then i is added to that folder if dmin(i) <
p
(cid:15) 2−|I|+1.
• If dmin(i) > p

(cid:15) , i remains as a singleton folder.
7) The partition Pl is set to be all the formed folders.
8) For l > 1 and while not all samples have been merged
together in a single folder, steps 4-7 are repeated for the
folders Il−1,i ∈ Pl−1. The distances between folders
depend on the level l, and on the samples in each of the
folders.

APPENDIX II
COMPARING SURVIVAL CURVES
The survival function S(t) is deﬁned as the probability that
a subject will survive past time t. Let T be a failure time
with probability density function f . The survival function is
S(t) = P (T > t), where the Kaplan-Meier method [50] is a
non-parametric estimate given by

ˆS(tj) =

P r(T > ti|T ≥ ti) =

j
(cid:89)

i=1

ˆS(tj−1)P r(T > tj|T ≥ tj).

(32)

Deﬁning ni as the number at risk just prior to time ti and
di as the number of failures at ti, then P (T > ti) = ni−di
.
For more information on estimating survival curves and taking
into account censored data see [53]

ni

Comparison of two survival curves can be done using a
statistical hypothesis test called the log-rank test [45]. It is
used to test the null hypothesis that there is no difference
between the population survival curves (i.e. the probability of
an event occurring at any time point is the same for each
population). Deﬁne nk,i as the number at risk in group k just
prior to time ti, such that ni = (cid:80)
k nk,i and dk,i as the number
of failures in group k at time ti such that di = (cid:80)
k dk,i. Then,
the expected number of failures in group k = 1, 2 is given by
(cid:88)

and the observed number of failures in group k = 1, 2 is

Ek =

di

nk,i
ni

(cid:88)

Ok =

dk,i.

i

i

(O2 − E2)2
Var(O2 − E2)

∼ χ2
1.

(33)

(34)

(35)

Under the null hypothesis of no difference between the two

groups, the log-rank test statistic is

The log-rank test can be extended to more than two
groups [53].

Data-Driven Tree Transforms and Metrics

Gal Mishne, Ronen Talmon, Israel Cohen, Ronald R. Coifman and Yuval Kluger

1

7
1
0
2
 
g
u
A
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
6
7
5
0
.
8
0
7
1
:
v
i
X
r
a

Abstract—We consider the analysis of high dimensional data
given in the form of a matrix with columns consisting of
observations and rows consisting of features. Often the data
is such that the observations do not reside on a regular grid,
and the given order of the features is arbitrary and does not
convey a notion of locality. Therefore, traditional transforms
and metrics cannot be used for data organization and analysis.
In this paper, our goal is to organize the data by deﬁning an
appropriate representation and metric such that they respect the
smoothness and structure underlying the data. We also aim to
generalize the joint clustering of observations and features in
the case the data does not fall into clear disjoint groups. For
this purpose, we propose multiscale data-driven transforms and
metrics based on trees. Their construction is implemented in an
iterative reﬁnement procedure that exploits the co-dependencies
between features and observations. Beyond the organization
of a single dataset, our approach enables us to transfer the
organization learned from one dataset to another and to integrate
several datasets together. We present an application to breast
cancer gene expression analysis: learning metrics on the genes to
cluster the tumor samples into cancer sub-types and validating
the joint organization of both the genes and the samples. We
demonstrate that using our approach to combine information
from multiple gene expression cohorts, acquired by different
proﬁling technologies, improves the clustering of tumor samples.

Index Terms—graph signal processing, multiscale representa-

tions, geometric analysis, partition trees, gene expression

I. INTRODUCTION

High-dimensional datasets are typically analyzed as a two-
dimensional matrix where, for example,
the rows consist
of features and the columns consist of observations. Signal
processing addresses the analysis of such data as residing on
a regular grid, such that the rows and columns are given
in a particular order,
indicating smoothness. For example,
the ordering in time-series data indicates temporal-frequency
smoothness, and the order in 2D images indicating spatial
smoothness. Non-Euclidean data that do not reside on a regular
grid, but rather on a graph, raise the more general problem of
matrix organization. In such datasets, the given ordering of the
rows (features) and columns (observations) does not indicate
any degree of smoothness.

However, in many applications, for example, analysis of
gene expression data, text documents, psychological question-

G. Mishne and R. R. Coifman are with the Department of Mathematics,
Yale University, New Haven, CT 06520 USA (e-mail: gal.mishne@yale.edu
; ronald.coifman@math.yale.edu.). R. Talmon and I. Cohen are with the
Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of
ico-
Technology, Haifa 32000,
hen@ee.technion.ac.il). Y. Kluger is with the Department of Pathology and
the Yale Cancer Center, Yale University School of Medicine, New Haven, CT
06511 USA (e-mail: yuval.kluger@yale.edu). This research was supported by
the Israel Science Foundation (grant no. 576/16), and by the United States-
Israel Binational Science Foundation and the United States National Science
Foundation (grant no. 2015582), and by the National Institutes of Health (grant
no. 1R01HG008383-01A1).

ronen@ee.technion.ac.il;

(e-mail:

Israel.

naires and recommendation systems [1]–[10], there is an un-
derlying structure to both the features and the observations. For
example, in gene expression subsets of samples (observations)
have similar genetic proﬁles, while subsets of genes (features)
have similar expressions across groups of samples. Thus, as
the observations are viewed as high-dimensional vectors of
features, one can swap the role of features and observations,
and treat the features as high-dimensional vectors of observa-
tions. This dual analysis reveals meaningful joint structures in
the data.

The problem of matrix organization considered here is
closely related to biclustering [1]–[5], [11], [12], where the
goal is to identify biclusters: joint subsets of features and
observations such that the matrix elements in each subset have
similar values. Matrix organization goes beyond the extraction
of joint clusters, yielding a joint reordering of the entire dataset
and not just the extraction of partial subsets of observations
and features that constitute bi-clusters. By recovering the
smooth joint organization of the features and observations,
one can apply signal processing and machine learning methods
such as denoising, data completion, clustering and classiﬁca-
tion, or extract meaningful patterns for exploratory analysis
and data visualization.

The application of traditional signal processing transforms
to data on graphs is not straightforward, as these transforms
rely almost exclusively on convolution with ﬁlters of ﬁnite
support, and thus are based on the assumption that the given
ordering of the data conveys smoothness. The ﬁeld of graph
signal processing adapts classical techniques to signals sup-
ported on a graph (or a network), such as ﬁltering and wavelets
in the graph domain [13]–[22]. Consider for example signals
(observations) acquired from a network of sensors (features).
The nodes of the graph are the sensors and the edges and
their weights are typically dictated by a-priori information
such as physical connectivity, geographical proximity, etc. The
samples collected from all sensors at a given time compose a
high-dimensional graph signal supported on this network. The
signal observations, acquired over time, are usually processed
separately and the connectivity between the observations is not
taken into account.

To address this issue, in this paper we propose to analyze
the data in a matrix organization setting as represented by
two graphs: one whose nodes are the observations and the
other whose nodes are the features, and our aim is a joint
unsupervised organization of these two graphs. Furthermore,
we do not ﬁx the edge weights by relying on a predetermined
structure or a-priori information. Instead, we calculate the edge
weights by taking into account the underlying dual structure
of the data and the coupling between the observations and the
features. This requires deﬁning two metrics, one between pairs
of observations and one between pairs of features.

2

Such an approach for matrix organization was introduced by
Gavish and Coifman [9], where the organization of the data
relies on the construction of a pair of hierarchical partition
trees on the observations and on the features. In previous
work [23], we extended this methodology to the organization
of a rank-3 tensor (or a 3D database), introducing a multiscale
averaging ﬁlterbank derived from partition trees.

Here we introduce a new formulation of the averaging ﬁlter-
bank as a tree-based linear transform on the data, and propose
a new tree-based difference transform. Together these yield
a multiscale representation of both the observations and the
features, in analogue to the Gaussian and Laplacian pyramid
transforms [24]. Our transforms can be seen as data-driven
multiscale ﬁlters on graphs, where in contrast to classical
signal processing, the support of the ﬁlters is non-local and
depends on the structure of the data. From the transforms,
we derive a metric in the transform space that incorporates
the multiscale structure revealed by the trees [25]. The trees
and the metrics are incorporated in an iterative bi-organization
procedure following [9]. We demonstrate that beyond the
organization of a single dataset, our metric enables us to
apply the organization learned from one dataset to another
and to integrate several datasets together. This is achieved by
generalizing our transform to a new multi-tree transform and
to a multi-tree metric, which integrate a set of multiple trees on
the features. Finally, the multi-tree transform inspires a local
reﬁnement of the partition trees, improving the bi-organization
of the data.

The remainder of the paper is organized as follows. In
Section II, we formulate the problem, present an overview of
our solution and review related background. In Section III, we
present the new tree-induced transforms and their properties.
In Section IV, we derive the metric in the transform space and
propose different extensions of the metric. We also propose a
local reﬁnement of the bi-organization approach. Section V
presents experimental results in the analysis of breast cancer
gene expression data.

non-binary tree is not uniquely deﬁned. Finally, since the
averaging transform is over-complete such that each ﬁlter
corresponds to a single folder in the tree, it is simple to design
weights on the transform coefﬁcients based on the properties
of the individual folders.

it

Filterbanks and multiscale transforms on trees and graphs
have been proposed in [18]–[21], yet differ from our ap-
proach in several aspects. While ﬁlterbanks construct a multi-
scale representation by using downsampling operators on the
data [18], [20], the multiscale nature of our transform arises
is
from partitioning of the data via the tree. In that,
most similar to [21], where the graph is decomposed into
subgraphs by partitioning. However, all these ﬁlterbanks on
graphs employ the eigen-decomposition of the graph Laplacian
to deﬁne either global ﬁlters on the full graph or local ﬁlters
on disjoint subgraphs. Our approach, conversely, employs the
eigen-decomposition of the graph Laplacian to construct the
partition tree, but the transforms (ﬁlters) are deﬁned by the
structure of the tree and not explicitly derived from the Lapla-
cian. In addition, we do not treat the structure of the graph as
ﬁxed, but rather iteratively update the Laplacian based on the
tree transform. Finally, while graph signal processing typically
addresses one dimension of the data (features or observations),
our approach addresses the construction of transforms on both
the observations and features of a dataset, and relies on the
coupling between the two to derive the transforms.

This work is also related to the matrix factorization pro-
posed by Shahid et al. [22], where the graph Laplacians of both
the features and the observation regularize the decomposition
of a dataset
into a low-rank matrix and a sparse matrix
representing noise. Then the observations are clustered using
k-means on the low-dimensional principal components of the
smooth low-rank matrix. Our work differs in that we preform
an iterative non-linear embedding of the observations and
features, not jointly, but alternating between the two while
updating the graph Laplacian of each in turn. In addition, we
provide a multiscale clustering of the data.

A. Related Work

II. BI-ORGANIZATION

Various methodologies have been proposed for the con-
struction of wavelets on graphs,
including Haar wavelets,
and wavelets based on spectral clustering and spanning tree
decompositions [13]–[16], [26], [27]. Our work deviates from
this path and presents an iterative construction of data-driven
tree-based transforms. In contrast to previous multiscale rep-
resentations of a single graph, our approach takes into account
the co-dependencies between observations and features by
incorporating two graph structures. Our motivation for the
proposed transforms is the tree-based Earth Mover’s Distance
(EMD) proposed in [25], which introduces a coupling between
observations and features, enabling an iterative procedure that
updates the trees and metrics in each iteration. The averaging
transform, in addition to being equipped with this metric, is
also easier to compute than a wavelet basis as it does not
require an orthogonalization procedure. In addition, given a
the averaging and difference transforms are
partition tree,
unique, whereas the tree-based wavelet transform [13] on a

A. Problem Formulation

Let Z be a high-dimensional dataset and let us denote its
set of nX features by X and denote its set of nY observations
by Y. For example, in gene expression data, X consists of
the genes and Y consists of individual samples. The element
Z(x, y) is the expression of gene x ∈ X in sample y ∈ Y. The
given ordering of the dataset is arbitrary such that adjacent
features and adjacent observations in the dataset are likely
dissimilar. We assume there exists a reordering of the features
and a reordering of the observations such that Z is smooth.

Deﬁnition 1: A matrix Z is smooth if it satisﬁes the mixed
H¨older condition [9], such that ∀x, x(cid:48) ∈ X and ∀y, y(cid:48) ∈ Y,
and for a pair of non-trivial metrics ρX on X and ρY on Y
and constants C > 0 and 0 < α ≤ 1:

|Z(x, y) − Z(x, y(cid:48)) − Z(x(cid:48), y) + Z(x(cid:48), y(cid:48))|

≤ CρX (x, x(cid:48))αρY (y, y(cid:48))α.

(1)

Note that we do not impose smoothness as an explicit con-
straint; instead it manifests itself implicitly in our data-driven
approach.

Although the given ordering of the dataset is not smooth, the
organization of the observations and the features by partition
trees following [9] constructs both local and global neigh-
borhoods of each feature and of each observation. Thus, the
structure of the tree organizes the data in a hierarchy of nested
clusters in which the data is smooth. Our aim is to deﬁne a
transform on the features and on the observations that conveys
the hierarchy of the trees, thus recovering the smoothness of
the data. We deﬁne a new metric in the transform space that
incorporates the hierarchical clustering of the data via the
trees. The notations in this paper follow these conventions:
matrices are denoted by bold uppercase and sets are denoted
by uppercase calligraphic.

B. Method Overview

The construction of the tree, which relies on a metric, and
the calculation of the metric, which is derived from a tree, lead
to an iterative bi-organization algorithm [9]. Each iteration
updates the pair of trees and metrics on the observations
and features as follows. First, an initial partition tree on the
features, denoted TX , is calculated based on an initial pairwise
afﬁnity between features. This initial afﬁnity is application
dependent. Based on a coarse-to-ﬁne decomposition of the
features implied by the partition tree on the features, we deﬁne
a new metric between pairs of observations: dTX (y, y(cid:48)). The
metric is then used to construct a new partition tree on the
observations TY . Thus, the construction of the tree on the
observations TY is based on a metric induced by the tree on the
features TX . The new tree on the observations TY then deﬁnes
a new metric between pairs of features dTY (x, x(cid:48)). Using this
metric, a new partition tree is constructed on the features TX ,
and a new iteration begins. Thus, this approach exploits the
strong coupling between the features and the observations.
This enables an iterative procedure in which the pair of trees
are reﬁned from iteration to iteration, providing in turn a more
accurate metric on the features and on the observations. We
will show that the resulting tree-based transform and corre-
sponding metric enable a multiscale analysis of the dataset,
reordering of the observations and features, and detection of
meaningful joint clusters in the data.

C. Partition trees

Given a dataset Z, we construct a hierarchical partitioning
of the observations and features deﬁned by a pair of trees.
Without loss of generality, we deﬁne the partition trees in this
section with respect to the features, and introduce relevant
notation.

Let TX be a partition tree on the features. The partition tree
is composed of L + 1 levels, where a partition Pl is deﬁned
for each level 0 ≤ l ≤ L. The partition Pl = {Il,1, ..., Il,n(l)}
at level l consists of n(l) mutually disjoint non-empty subsets
of indices in {1, ..., nX }, termed folders and denoted by Il,i,
i ∈ {1, ..., n(l)}. Note that we deﬁne the folders on the indices
of the set and not on the features themselves.

3

The partition tree TX has the following properties:
• The ﬁnest partition (l = 0) is composed of n(0) = nX
singleton folders, termed the “leaves”, where I0,i = {i}.
• The coarsest partition (l = L) is composed of a single
folder, PL = IL,1 = {1, ..., nX }, termed the “root”.
• The partitions are nested such that if I ∈ Pl, then I ⊆ J
for some J ∈ Pl+1, i.e., each folder at level l − 1 is a
subset of a folder from level l.

The partition tree is the set of all folders at all levels TX =
{Il,i | 0 ≤ l ≤ L, 1 ≤ i ≤ n(l)}, and the number of all
folders in the tree is denoted by NX = |TX |. The size, or
cardinality, of a folder I, i.e. the number of indices in that
folder, is denoted by |I|. In the remainder of the paper, for
compactness, we drop the subscript l denoting the level of a
folder, and denote a single folder by either I or Ii, such that
i ∈ {1, ..., NX } is an index over all folders in the tree.

Given a dataset, there are many methods to construct a
partition tree, including deterministic, random, agglomerative
(bottom-up) and divisive (top-down) [5], [13], [28]. For ex-
ample, in a bottom-up approach, we begin at the lowest level
of the tree and cluster the features into small folders. These
folders are then clustered into larger folders at higher levels
of the tree, until all folders are merged together at the root.

Some approaches take into account the geometric structure
and multiscale nature of the data by incorporating afﬁnity ma-
trices deﬁned on the data, and manifold embeddings [10], [13].
Ankenman [10] proposed “ﬂexible trees”, whose construction
requires an afﬁnity kernel deﬁned on the data, and is based on a
low-dimensional diffusion embedding of the data [29]. Given
a metric between features d(x, x(cid:48)), a local pairwise afﬁnity
kernel k(x, x(cid:48)) = exp{−d(x, x(cid:48))/σ2} is integrated into a
global representation on the data via a manifold embedding
representation Ψ, which minimizes

min

k(x, x(cid:48))(cid:107)Ψ(x) − Ψ(x(cid:48))(cid:107)2
2.

(2)

(cid:88)

x,x(cid:48)

The clustering of the folders in the ﬂexible tree algorithm is
based on the Euclidean distance between the embedding Ψ
of the features, which integrates the original metric d(x, x(cid:48)).
Thus, the construction of the tree does not rely directly on the
high-dimensional features but on the low-dimensional geomet-
ric representation underlying the data (see [10] for a detailed
description). The quality of this representation, and therefore,
of the constructed tree depends on the metric d(x, x(cid:48)). In our
approach, we propose to use the metric induced by the tree
on the observations d(x, x(cid:48)) = dTY (x, x(cid:48)). This introduces a
coupling between the observations and the features, as the
tree construction of one depends on the tree of the other.
Since our approach is based on an iterative procedure, the
tree construction is reﬁned from iteration to iteration, as both
the tree and the metric on the features are updated based
on the organization of the observations, and vice versa. This
also updates the afﬁnity kernel between observations and the
afﬁnity kernel between features, therefore updating the dual
graph structure of the dataset.

Note that while we apply ﬂexible trees in our experimental
results, the bi-organization approach is modular and different
tree construction algorithms can be applied, as in [9], [30].

While the deﬁnition of the proposed transforms and metrics
does not depend on properties of the ﬂexible trees algorithm,
the resulting bi-organization does depend on the tree con-
struction. Spin-cycling (averaging results over multiple trees)
as in [10] can be applied to stabilize the results. Instead,
we propose an iterative reﬁnement procedure that makes the
tree constructions.
algorithm less dependent on the initial
Convergence guarantees to smooth results from a family of
appropriate initial trees are lacking. This will be the subject
of future work.

III. TREE TRANSFORMS

Given partition trees TX and TY , deﬁned on the features
and observations, respectively, we propose several transforms
induced by the partition trees, which are deﬁned by a linear
transform matrix and generalizes the method proposed in [10].
In the following we focus on the feature set X , but the same
deﬁnitions and constructions apply to the observation set Y.
Note that while the proposed transforms are linear, the support
of the transform elements is derived in a non-linear manner
as it depends on the tree construction.

i.e.

The proposed transforms project the data onto a high di-
mensional space whose dimensionality is equal to the number
of folders in the tree, denoted by NX ,
the transform
maps T : RnX → RNX . Each transform is represented as
a matrix of size NX × nX , where nX is the number of
features. We denote the row indices of the transform matrices
by i, j ∈ {1, 2, ..., NX } indicating the unique index of the
folder in TX . We denote the column indices of the transform
matrices by x, x(cid:48) ∈ X (y, y(cid:48) ∈ Y), which are the indices
of the features (observations) in the data. We deﬁne 1I to
be the indicator function on the features x ∈ {1, ..., nX }
belonging to folder I ∈ TX . Tree transforms obtained from
TX are applied to the dataset as ˆZX = TX Z and tree
transforms obtained from TY are applied to the dataset as
ˆZY = ZTT
Y . We begin with transforms induced by a tree in
a single dimension (features or observations) analogously to a
typical one-dimensional linear transform. We then extend these
transforms to joint-tree transforms induced by a pair of trees
{TX , TY } on the observations and the features, analogously to
a two-dimensional linear transform. Finally, we propose multi-
tree transforms in the case that we have more than one tree
in a single dimension, for example we have constructed a set
of trees {TX } on the features X , each constructed from a
different dataset consisting of different observations with the
same features.

A. Averaging transform

4

(5)

(6)

Applying S to an observation vector y ∈ RnX yields a vector
of length NX where each element i ∈ {1, ..., NX } is the sum
of the elements y(x) for x ∈ Ii:

(Sy)[i] =

y(x)1Ii(x) =

y(x)

(4)

(cid:88)

x∈X

(cid:88)

x∈Ii

The sum of each row of S is the size of its corresponding
folder: (cid:80)
x S[i, x] = |Ii|. The sum of each column is the
number of levels in TX : (cid:80)
i S[i, x] = L + 1, since the folders
are disjoint at each level such that each feature belongs only
to a single folder at each level.

From S we derive the averaging transform denoted by M.
Let D ∈ RNX ×NX be a diagonal matrix whose elements are
the cardinality of each folder: D[i, i] = |Ii|. We calculate
M ∈ RNX ×nX by normalizing the rows of S, so the sum of
each row is 1:

M = D−1S.

Thus, the rows i of M are uniformly weighted indicators on
the indices of X for each folder Ii:

M[i, x] =

1Ii(x) =

1
|Ii|

(cid:26) 1

|Ii| , x ∈ Ii
0,
o.w.

Note that the matrix S and the averaging transform M share
the same structure, i.e. they differ only in the value of the their
non-zero elements.

Alternatively if we denote by m(y, I) the average value of

y(x) in folder I:

m(y, I) =

y(x),

(7)

1
|I|

(cid:88)

x∈I

then applying the averaging transform M to y yields a vector
ˆy of length NX such that each element i is the average value
of y in folder Ii

(7):

ˆy[i] = (My)[i] = m(y, Ii), 1 ≤ i ≤ NX .

(8)

The averaging transform reinterprets each folder in the tree
as applying a uniform averaging ﬁlter, whose support depends
on the size of the folder. Applying the feature-based transform
MX to the dataset Z yields ˆZX = MX Z ∈ RNX ×nY , a data-
driven multi-scale representation of the data. As opposed to
a multiscale representation deﬁned on a regular grid, here
the representation at each level
is obtained via non-local
averaging of the coefﬁcients from the level below. The ﬁnest
level of the representation is the data itself, which is then
averaged in increasing degree of coarseness and in a non-
local manner according to the clusters deﬁned by the hierarchy
in the partition tree. The columns of ˆZX are the multiscale
representation ˆy of each observation y. The rows of ˆZX are the
centroids of the folders I ∈ TX and can be seen as multiscale
meta-features of length nY :

Ci(y) =

M[i, x]Z[x, y], 1 ≤ y ≤ nY .

(9)

(cid:88)

x

Let S be an NX × nX matrix representing the structure of
a given tree TX , by having each row i of the matrix be the
indicator function of the corresponding folder Ii ∈ TX :

S[i, x] = 1Ii(x) =

(cid:26) 1,

x ∈ Ii
0, otherwise

In a similar fashion denote by ˆZY = ZMT
Y the application
of the observation-based transform to the entire dataset. For
additional properties of S and M see [31].

In Fig. 1, we display an illustration of a partition tree

(3)

5

(a) Partition tree T . (b) Averaging transform matrix M induced by
Fig. 1.
the tree and applied to column vector y(x). The color of the elements in the
output correspond to the color of the folders in the tree.

and the resulting averaging transform. Fig. 1(a) is a partition
tree TX constructed on X where nX = 8. Fig. 1(b) is the
averaging transform M corresponding to the partition tree
TX . For visualization purposes we construct M as having
columns whose order correspond to the leaves of the tree TX
(level 0). This reordering also needs to be applied to the data
vectors y, and is essentially one of the aims of our approach.
The lower part of the transform is just the identity matrix,
as it corresponds to the leaves of the tree. The number of
rows in the transform matrix is NX = |T | = 14, as the
number of folders in the tree. The transform is applied to
a (reordered) column y ∈ R8, yielding the coefﬁcient vector
ˆy = My ∈ R14. The coefﬁcients are colored according to the
corresponding folders in the tree.

To further demonstrate and visualize the transform, we
apply the averaging transform to an image in Fig. 2. We
treat a grayscale image as a high-dimensional dataset where
X is the set of rows and Y is the set of columns. We
calculate a partition tree TY on the columns. We then calculate
the averaging transform and apply it to the image yielding
ˆZY = ZMT
Y . The result is presented in Fig. 2(a). Each row
x has now been extended to a higher dimension NY , where
we separate the levels of the tree with colored borders lines
for visualization purposes. Each of the columns ˆZY is the
centroid of folder I in the tree. The right-most sub-matrix is
the original image and as we move left we have coarser and
coarser scales. The averaging is non-local and the folder sizes
vary, respecting the structure of the data. Thus on the second
level of the tree, the building on the right is more densely
compressed compared to the building on the left.

Fig. 2. Application of the averaging transform (a) and the difference transform
(b) to an image. The color of the border represents the level of the tree. The
non-local nature of the transforms and the varying support is apparent, for
example, in the building on the right. In the ﬁne-scale resolution the building
has 7 windows in the horizontal direction, which have been compressed into
5 windows on the next level.

B. Difference transform

The goal of our approach is to organize the data in nested
folders in which the features and the observations are smooth.
Thus, it is of value to determine how smooth is the hierarchical
structure of the tree, i.e. does the merging of folders on one
level into a single folder on the next level preserve smoothness.
Let ∆ be an NX × nX matrix, termed the multiscale differ-
ence transform. This transform yields the difference between
ˆy[i] and ˆy[j] where j is the index of the immediate parent of
folder i.

The matrix ∆ is obtained from the averaging matrix M as:

∆[i, x] = M[i, x] − M[j, x], Il,i ⊂ Il+1,j.

(10)

Applying ∆ to observation y yields a vector of length NX
whose element i is the difference between the average value
of y in folder Il,i and the average value in its immediate parent
folder Il+1,j:

(∆y)[i] =

(cid:26) m(y, IL,1),

Ii = IL,1
m(y, Il,i) − m(y, Il+1,j), Il,i ⊂ Il+1,j,

(11)
where for the root folder, we deﬁne (∆y)[i] to be the average
over all features. This choice leads to the deﬁnition of an
inverse transform below. Thus, the rows i of ∆ are given by:

∆[i, x] =





1
|Ii| ,
|Il,i| − 1
1
− 1
|Il+1,j | ,
0,

|Il+1,j | ,

Ii = IL,1
x ∈ Il,i ⊂ Il+1,j
x /∈ Il,i ⊂ Il+1,j, x ∈ Il+1,j
x /∈ Il,i, x /∈ Il+1,j

and the sum of the rows of ∆:

∆[i, x] =

(cid:26) 1, Ii = IL,1
otherwise

0,

(12)

(13)

The difference transform can be seen as revealing “edges”
in the data, however these edges are non-local. Since the
tree groups features together based on their similarity and
not based on their adjacency, the difference between folders
is not restricted to the given ordering of the features. This
demonstrated in Fig. 2(b) where the difference transform of
the column tree has been applied to the 2D image as Z∆T
Y .
Theorem 2: The data can be recovered from the difference

transform by:

y = ST (∆y)

(14)

Proof: An element (ST ∆y)[x] is given by
(cid:88)

1Ii(x) (m(y, Il,i) − m(y, Il+1,j)) + m(y, IL,1) =

Il,i∈TX
Il,i⊂Il+1,j
l<L

(cid:88)

=
Il,i∈TX
0≤l≤L

=

n(0)
(cid:88)

i=1

1Ii(x)m(y, Il,i) −

1Ii(x)m(y, Il,i) =

(15)

(cid:88)

Il,i∈TX
1≤l≤L

1Ii(x)m(y, I0,i) = y(x) (cid:3)

The ﬁrst equality is due to the folders on each level being
disjoint such that if x ∈ Il,i and Il,i ⊂ Il+1,j then x ∈
Il+1,j, and Il+1,j is the only folder containing x on level
l + 1. This enables us to process the data in the tree-based
transform domain and then reconstruct by:

ˆy = ST f (∆y),

(16)

X → RN

where f : RN
X is a function in the domain of the
tree folders. For example, we can threshold coefﬁcients based
on their energy or the size of their corresponding folder. This
scheme can be applied to denoising and compression of graphs
or matrix completion [18]–[21], however this is beyond the
scope of this paper and will be explored in future work.

Note that the difference transform differs from the tree-
based Haar-like basis introduced in [13]. The Haar-like basis
is an orthonormal basis spanned by nX vectors derived from
the tree by an orthogonalization procedure. The difference
transform is overcomplete and spanned by NX vectors, whose
construction does not require an orthogonalization procedure,
making it simpler to compute. Also, as each vector corre-
sponds to a single folder, it enables us to deﬁne a measure of
the homogeneity of a speciﬁc folder compared to its parent.

C. Joint-tree transforms

Given the matrix Z on X × Y, and the respective partition
trees TX and TY , we deﬁne joint-tree transforms that operate
on the features and observations of Z simultaneously. This is
analogous to typical 2D transforms. The joint-tree averaging
transform is applied as

ˆZX ,Y = MX ZMT
Y .

(17)

The resulting matrix of size NX × NY provides a multiscale
representation of the data matrix, admitting a block-like struc-
ture corresponding to the folders in both trees. On the ﬁnest
level we have Z and then on coarser and coarser scales we have
smoothed versions of Z, where the averaging is performed

6

Fig. 3.
difference transform applied to image.

(a) Joint-tree averaging transform applied to image. (b) Joint-tree

under the joint folders at each level. The coarsest level is of
size 1 × 1 corresponding to the joint root folder. This matrix is
analogous to a 2D Gaussian pyramid representation of the data,
popular in image processing [24]. However, as opposed to the
2D Gaussian pyramid in which each level is a reduction of both
dimensions, applying our transform yields all combinations
of ﬁne and coarse scales in both dimensions. The joint-tree
averaging transform yields a result similar to the directional
pyramids introduced in [32], however the “blur” and “sub-
sample” operations in our case are data-driven and non-local.
The joint-tree difference transform is applied as ∆X Z∆T
Y .
This matrix is analogous to a 2D Laplacian pyramid represen-
tation of the data, revealing “edges” in the data. As in applying
a 1D transform, the data can be recovered from the joint-tree
difference transform as Z = ST

X ∆X Z∆T

Y SY .

Figure 3 presents applying the joint-tree averaging transform
and joint-tree difference transform to the 2D image. Within the
red border we display “zooming in” on level l ≥ 1 in both
trees TX and TY .

D. Multi-tree transforms

At each level of the partition tree, the folders are grouped
into disjoint sets. A limitation of using partition trees, there-
fore, is that each folder is connected to a single “parent”.
However, it can be beneﬁcial to enable a folder on one level
to participate in several folders at the level above, such that
folders overlap, as in [33]. We propose an approach that
enables overlapping folders in the bi-organization framework
by constructing more than one tree on the features X , and
we extend the single tree transforms to multi-tree transforms.
This generalizes the partition tree such that each folder can be
connected to more than one folder in the above level, i.e. this
is no longer a tree because it is now cyclic but still a bipartite
graph. Note that in contrast to the joint-tree transform, which
incorporates a joint pair of trees over both the features and the
observations, here we are referring to a set of trees deﬁned for
only the features, or only the observations.

t |Tt|. Yet since all trees {Tt}nT

Given a set of nT different partition trees on X , denoted
{Tt}nT
t=1, we construct the multi-tree averaging transform. Let
(cid:102)MX be an (cid:101)NX × nX matrix, constructed by concatenation of
the averaging transform matrices MT induced by each of the
trees {Tt}nT
t=1. The number of rows in the multi-tree transform
matrix is denoted by (cid:101)NX and equal to the number of folders in
all of the trees (cid:80)
t=1 contain the
same root and leaves folders, we remove the multiple appear-
ance of the rows corresponding to these folders and include
them only once (then (cid:101)NX = (cid:80)
t |Tt| − (nT − 1)(1 + nX )).
Thus, the matrix of the multi-tree averaging transform now
represents a decomposition via a single root, a single set
of leaves and many multiscale folders that are no longer
disjoint. This implies that
instead of considering multiple
“independent” trees, we have a single hierarchical graph where
at each level we do not have disjoint folders, as in a tree, but
instead overlapping folders. In Sec. IV-C, we derive from these
transforms a new multi-tree metric. For additional properties
of the multi-tree transform see [31].

Ram, Elad and Cohen [26] also proposed a “generalized tree
transform” where folders are connected to multiple parents in
the level above, however their work differs in two aspects.
First, their proposed tree construction is a binary tree, whereas
ours admits general tree constructions. Second, their transform
relies on classic pre-determined wavelet ﬁlters such that the
support of the ﬁlter is ﬁxed across the dataset. Our formu-
lation on the other hand introduces data-driven ﬁlters whose
support is determined by the size of the folder, which can
vary across the tree. The Multiresolution Matrix Factorization
(MMF) [27] also yields a wavelet basis on graphs. MMF
uncovers a hierarchical organization of the graph that permits
overlapping clusters, by decomposition of a graph Laplacian
matrix via a sequence of sparse orthogonal matrices. However,
our transform is derived from a set of multiple hierarchical
trees, whereas their hierarchical structure is derived from the
wavelet transform.

The ﬁeld of community detection also addresses ﬁnd-
ing overlapping clusters in graphs [34]. Ahn, Bagrow and
Lehmann [33] construct multiscale overlapping clusters on
graphs by performing hierarchical clustering with a similarity
between edges of a graph, instead of its nodes. Their approach
focuses on the explicit construction of the hierarchy of the
overlapping clusters, whereas our focus is on employing a
transform and a metric derived from such a multiscale over-
lapping organization of the features. In contrast to clustering,
our approach allows for the organization and analysis of the
observations.

IV. TREE-BASED METRIC

The success of the data organization and the reﬁnement of
the partition trees depends on the metric used to construct
the trees. We assume that a good organization of the data
recovers smooth joint clusters of observations and features.
Therefore, a metric for comparing pairs of observations should
not only compare their values for individual features (as in
the Euclidean distance), but also across clusters of features,
which are expected to have similar values. Thus, we present a

7

metric dT in the multiscale representation yielded by the tree
transforms. Using this metric, the construction of the tree on
the features takes into account the structure of the underlying
graph on the observations as represented by its partition tree.
The partition tree on the observations in turn relies on the
graph structure of the features. In each iteration a new tree
is calculated based on the metric from the previous iteration,
and then a new metric is calculated based on the new tree.
This can be seen as updating the dual graph structure of the
data in each iteration. The iterative bi-organization algorithm
is presented in Alg. 1.

A. Tree-based EMD

Coifman and Leeb [25] deﬁne a tree-based metric approxi-
mating the EMD in the setting of hierarchical partition trees.
Given a 2D matrix Z, equipped with a partition tree on the
features TX , consider two observations y, y(cid:48) ∈ Y. The tree-
based metric between the observations is deﬁned as

dTX (y, y(cid:48)) =

|m(y − y(cid:48), I)|,

(18)

(cid:19)β

(cid:18) |I|
nX

(cid:88)

I∈TX

where β is a parameter that weights the folders in the tree
based on their size. Following our formulation of the trees
inducing linear transforms, this tree-based metric can be seen
as a weighted l1 distance in the space of the averaging
transform.

Theorem 3: [23, Theorem 4.1] Given a partition tree on
the features TX , deﬁne the NX × NX diagonal weight matrix
W[i, i] =
. Then the tree metric (18) between two
observations y, y(cid:48) ∈ RnX is equivalent to the weighted l1
distance between the averaging transform coefﬁcients:

(cid:16) |Ii|
nX

(cid:17)β

dTX (y, y(cid:48)) = (cid:107)W(ˆy − ˆy(cid:48))(cid:107)1.

(19)

Proof: An element of the vector W(ˆy − ˆy(cid:48)) is

(WM(y − y(cid:48)))[i] =

W[i, j](M(y − y(cid:48)))[j]

(cid:88)

j

= W[i, i](M(y − y(cid:48)))[i]

(20)

(cid:19)β

=

(cid:18) |Ii|
nX

m(y − y(cid:48), Ii).

Therefore:

(cid:107)W(ˆy − ˆy(cid:48))(cid:107)1 =

|m(y − y(cid:48), I)| (cid:3)

(21)

(cid:19)β

(cid:18) |Ii|
nX

(cid:88)

I∈T

Note that the proposed metric is equivalent to the l1 distance
between vectors of higher-dimensionality than the original di-
mension of the vectors. However, by weighting the coefﬁcients
with W, the effective dimension of the new vectors is typically
smaller than the original dimensionality, as the weights rapidly
decrease to zero based on the folder size and the choice
of β. For positive values of β, the entries corresponding to
the large folders dominate ˆy, while entries corresponding to
small folders tend to zero. This trend is reversed for negative
values of β, with elements corresponding to small folders
dominating ˆy while large folders are suppressed. In both cases,
a threshold can be applied to ˆy or ˆZ so as to discard entries

with low absolute values. Thus, the transforms project the
data onto a low-dimensional space of either coarse or ﬁne
structures. Also, note that interpreting the metric as the l1
distance in the averaging transform space enables us to apply
approximate nearest-neighbor search algorithms suitable for
the l1 distance [35], [36]. This allows to analyze larger datasets
via a sparse afﬁnity matrix.

Deﬁning the metric in the transform space enables us to
easily generalize the metric to a joint-tree metric deﬁned for a
joint pair of trees {TX , TY } (Sec. IV-B), to incorporate several
trees over the features {TX }nT in a multi-tree metric via the
multi-tree transform (Sec. IV-C), and to seamlessly introduce
weights on the transform coefﬁcients by setting the elements
of W (Sec. IV-E). Python code implementing our approach is
available at [37].

B. Joint-tree Metric

The tree-based transforms and metrics can be generalized to
analyzing rank-n tensor datasets. We brieﬂy present the joint-
tree metric to demonstrate that the proposed transforms are
not limited to just 2D matrices, but rather can be extended
to processing and organizing tensor datasets. An example of
such an application was presented in [23].

In [23] we proposed a 2D metric given a pair of partition
trees in the setting of organizing a rank-3 tensor. We reformu-
late this metric in the transform space by generalizing the tree-
based metric to a joint-tree metric using the coefﬁcients of the
joint-tree transform. Given a partition tree TX on the features
and a partition tree TY on the observations,
the distance
between two matrices Z1 and Z2 is deﬁned as

dTX ,TY (Z1, Z2) =

|m(Z1 − Z2, I × J )|

|I|βX |J |βY
nβX
βY
X nY

.

(cid:88)

I∈TX
J ∈TY

(22)
The value m(Z, I × J ) is the mean value of a matrix Z on
the joint folder I × J = {(x, y) | x ∈ I, y ∈ J }:

m(Z, I × J ) =

Z[x, y].

(23)

1
|I||J |

(cid:88)

x∈I,y∈J

Theorem 3 can be generalized to a 2D transform applied to
2D matrices.

Corollary 4: [23, Corollary 4.2] The joint-tree metric (22)
between two matrices given a partition tree TX on the features
and a partition tree TY on the observations is equivalent to the
l1 distance between the weighted 2D multiscale transform of
the two matrices:

dTX ,TY (Z1, Z2) = (cid:107)WX MX (Z1 − Z2)MT

YWY (cid:107)1.

(24)

C. Multi-tree Metric

The deﬁnition of the metric in the transform domain enables
a simple extension to a metric derived from a multi-tree
composition. Given a set of multiple trees {Tt}nT
t=1 deﬁned on
the features X as in Sec. III-D, we deﬁne a multi-tree metric
using the multi-tree averaging tree transform as:

d{T }(y, y(cid:48)) = (cid:107) (cid:102)W(cid:102)M(y − y(cid:48))(cid:107)1,

(25)

8

(cid:17)β

(cid:16) |Ii|
nX

where (cid:102)W is a diagonal matrix whose elements are
for all I ∈ T and for all trees in {Tt}nT
equivalent to averaging the single tree metrics:

t=1. This metric is

(cid:88)

d{T }(y, y(cid:48)) = (cid:107) (cid:102)W(cid:102)M(y − y(cid:48))(cid:107)1
1
nT
1
nT

dT (y, y(cid:48)).

T
(cid:88)

=

=

T

(cid:107)WT MT (y − y(cid:48))(cid:107)1

(26)

Note that in contrast to the joint-tree metric, which incorpo-
rates a pair of trees over both the features and the observations,
here we are referring to a set of trees deﬁned only for the
features, or only for the observations.

A question that arises is how to construct multiple trees? For
matrix denoising in a bi-organization setting, Ankenman [10]
applies a spin-cycling procedure: constructing many trees
by randomly varying the parameters in the partition tree
construction algorithm. Multiple trees can also be obtained by
initializing the bi-organization with different metric choices
for d(0)
X (x, x(cid:48)) (step 2 in Alg. 1), e.g., Euclidean, correlation,
etc. Another option, which we demonstrate experimentally on
real data in Sec. V, arises when we have multiple data sets
of observations with the same set of features, or multiple data
sets with the same observations but different features as in
multi-modal settings. In such cases, we construct a partition
tree for each dataset separately and then combine them using
the multi-tree metric.

D. Local Reﬁnement

We propose a new approach to constructing multiple trees,
leveraging the partition of the data during the bi-organization
procedure. This approach is based on a local reﬁnement of
the partition trees, which results in a smoother organization
of the data. The bi-organization method is effective when
correlations exist among both observations and features, by
revealing a hierarchical organization that is meaningful for all
the data together. Yet, since the bi-organization approach is
global and takes all observations and all features into account,
it needs to achieve the best organization on average. However,
the correlations between features may differ among sub-
populations in the data, i.e. the correlations between features
depend on the set of observations taken into account (and vice-
versa).

For example, consider a dataset of documents where the ob-
servations Y are documents belonging to different categories,
the features X are words and Z(x, y) indicates whether a
document y contained a word x. Grouping the words into
disjoint folders forces a single partition of the vocabulary that
disregards words that belong to more than one conceptual
group of words. These connections could be revealed by taking
into account the context, i.e. the subject of the documents.
By diving the documents into a few contextual clusters, and
calculating a local tree on the words TX for each such cluster,
the words are grouped together conceptually according to
the local category. The word “ﬁeld” for example will be
joined with different neighbors, depending on whether the

Algorithm 1 Bi-organization Algorithm [10, Sec. 5.3]
Initialization
Input Dataset Z of features X and observations Y

X , weight function on tree

1: Starting with features X
2:

Calculate initial metric d(0)
X (x, x(cid:48))
Calculate initial ﬂexible tree T (0)
X .

3:

Iterative analysis
Input Flexible tree on features T (0)

folders W[i, i] = ω(Ii)

4: for n ≥ 1 do
5:

X , calculate tree metric between obser-
(y, y(cid:48)) = (cid:107)WX MX (y − y(cid:48))(cid:107)1

Given tree T (n)
vations d(n)
TX
Calculate ﬂexible tree on the observations T (n)
Y .
Repeat steps 5-6 for the features X given T (n)
obtain T (n+1)

and

Y

.

X

8: end for

Algorithm 2 Bi-organization local reﬁnement
Input Dataset Z, observation tree TY

1: Choose level l in tree TY
2: for j ∈ {1, ..., n(l)} do
3:
4:

Set ω(Ji) = 1 ∀Ji ⊆ Jl,j, otherwise ω(Ji) = 0.
Calculate initial afﬁnity on features for subset of
observations as weighted tree-metric d(0)(x, x(cid:48)) =
dTY (x, x(cid:48); ω(Jj))
Calculate initial ﬂexible tree on features T (0)
Perform iterative analysis (steps 4-8 in Alg. 1) for Z
on X and (cid:101)Y = {y | y ∈ J ∈ TY }.

X

7: end for
8: Merge observation trees {T

}n(l) back into global tree

(cid:101)Yj

Output Reﬁned observation tree TY , Set of feature trees

TY

{TX }n(l)
i=1

6:

7:

5:
6:

analysis is applied to documents belonging to “agriculture”,
“mathematics” or “sports”.

Therefore, we propose to take advantage of the unsupervised
clustering obtained by the partition tree on the observations
TY , and apply a localized bi-organization to folders of ob-
servations. Formally, we apply the bi-organization algorithm
to a subset of Z containing all features X and a subset of
observations belonging to the same folder (cid:101)Y = {y | y ∈ J ∈
TY }. This local bi-organization results in a pair of trees: a
local tree T
(cid:101)Y organizing the subset of observations (cid:101)Y, and
a feature tree TX that organizes all the features X based on
this subset of observations that share the same local structure,
rather than the global structure of the data. This reveals the
correlations between features for this sub-population of the
data, and provides a localized visualization and exploratory
analysis for subsets of the data discovered in an unsupervised
manner. This is meaningful when the data is unbalanced and a
subset of the data differs drastically from the rest of the data,
e.g., due to anomalies.

We propose a local reﬁnement of the bi-organization as
follows. We select a single layer l of the observations tree

9

(cid:101)Yj

TY , and perform a separate localized organization for each
folder Jl,j ∈ Pl,
j ∈ {1, ..., n(l)}. We thus obtain n(l)
}n(l)
local observation trees {T
j=1, which we then merge back
into one global tree, with reﬁned partitioning. Merging is
performed by replacing the branch in TY whose root is Jl,j,
i.e. {J ∈ TY |J ⊆ Jl,j}, with the local observation tree T
.
(cid:101)Yj
In addition, we obtain a set of several corresponding trees on
the full set of features {TX }n(l), which we can use to calculate
a multi-tree metric (25). Our local reﬁnement algorithm is
presented in Alg. 2. Applying this algorithm to reﬁne the
global structures of both TY and TX results in a smoother
bi-organization of the data.

We typically apply the reﬁnement to a high level of the tree
since at these levels large clusters of distinct sub-populations
are grouped together, and their separate analysis will reveal
their local organization. The level can be chosen by applying
the difference transform and selecting a level at which the
folders grouped together are heterogeneous, i.e. their mean
signiﬁcantly differs from the mean of their parent folder.

Note that this approach is unsupervised and relies on the
data-driven organization of the data. However, this approach
can also be used in a supervised setting, when there are labels
on the observations. Then we calculate a different partition
tree on the features for each separate label (or sets of labels)
of the observations, revealing the hierarchical structure of the
features for each label. This will be explored in future work.

E. Weight Selection

The calculation of the metric depends on the weight attached
to each folder. We generalize the metric such that the weight
is W[i, i] = ω(Ii), where ω(Ii) > 0 is a weight function
associated with folder Ii. The weights can incorporate prior
smoothness assumptions on the data, and also enable to en-
hance either coarse or ﬁne structures in the similarity between
samples.

(cid:17)β

(cid:16) |Ii|
nX

The choice ω(Ii) =

in [25] makes the tree-based
metric (18) equivalent to EMD, i.e., the ratio of EMD to
the tree-based metric is always between two constants. The
parameter β weights the folder by its relative size in the tree,
where β > 0 emphasizes coarser scales of the data, while
β < 0 emphasizes differences in ﬁne structures.

Ankenman [10] proposed a slight variation to the weight

also encompassing the tree structure:

ω(Ii) = 2−αl(Ii)

(cid:18) |Ii|
nX

(cid:19)β

,

(27)

where α is a constant and l(Ii) is the level at which the folder
Ii is found in T . The constant α weights all folders in a
given level equally. Choosing α = 0 resorts to the original
weight. The structure of the trees can be seen as an analogue
to a frequency decomposition in signal processing, where
the support of a folder is analogous to a certain frequency.
Moreover, since high levels of the tree typically contain large
folders, they correspond to low-pass ﬁlters. Conversely, lower
levels of the tree correspond to high-pass ﬁlters as they contain
many small folders. Thus setting α > 0 corresponds to
emphasizing low frequencies whereas α < 0 corresponds to

10

enhancing high frequencies. In an unbalanced tree, where a
small folder of features remains separate for all levels of the
tree (an anomalous cluster of features), α can be used to
enhance the importance of this folder, as opposed to β, which
would decrease its importance based on its size.

We propose a different approach. Instead of weighting the
folders based on the structure of the tree, which requires a-
priori assumptions on the optimal scale of the features or the
observations, we set the folders weights based on their content.
By applying the difference transform to the data, we obtain a
measure for each folder deﬁning how homogeneous it is. This
reduces the number of parameters in the algorithm, which is
advantageous in the unsupervised problem of bi-organization.
We calculate for each folder, the norm of its difference on the
dataset Z:
(cid:32)

(cid:33)1/2

ω(Ii) =

((∆X Z)[i, y])2

(cid:88)

y



(cid:32)

(cid:88)

(cid:88)

=



y

x

(m(y(x), Il,i) − m(y(x), Il+1,j)

1/2

(cid:33)2


,

(28)
where Il,i ⊂ Il+1,j. This weight is high when Il,i (cid:28) Il+1,j.
This means that the parent folder joining Il,i with other folders
contains non-homogeneous “populations”. Therefore, assign-
ing a high weight to Il,i places importance on differentiating
these different populations.

The localized reﬁnement procedure in Alg. 2 can also be
formalized as assigning weights ω(I) in the tree metric. We
set all weights containing a branch of the tree (a folder and
all its sub-folders) to 1 and set all other weights to zero:
(cid:26) 1,
0,

Ii ⊆ Ij
otherwise,

ω(Ii) =

(29)

where Ij is the root folder of the branch. Thus, using these
weights, the metric is calculated based only on a subset of the
observations ˜Y. This metric can initialize a bi-organization
procedure of a subset of Z containing X and ˜Y.

F. Coherence

To assess the smoothness of the bi-organization stemming
from the constructed partition trees, a coherency criterion was
proposed in [9]. The coherency criterion is given by

C(Z; TX , TY ) =

(cid:107)ΨX ZΨT

Y (cid:107)1,

(30)

1
nX nY

where Ψ is a Haar-like orthonormal basis proposed by Gavish,
Nadler and Coifman [13] in the settings of partition trees,
and it depends on the structure of a given tree. This criterion
measures the decomposition of the data in a bi-Haar-like basis
induced by two partition trees TX and TY : ΨX ZΨT
Y . The
lower the value of C(Z; TX , TY ), the smoother the organiza-
tion is in terms of satisfying the mixed H¨older condition (1).
Minimizing the coherence can be used as a stopping con-
dition for the bi-organization algorithm presented in Alg. 1.
The bi-organization continues as long as C(Z; T (n)
Y ) <
C(Z; T (n−1)
) [9]. However, we have empirically
X

X , T (n)

, T (n−1)
Y

found that the iterative process typically converges within
only few iterations. Therefore, in our experimental results we
perform n = 2 iterations.

V. EXPERIMENTAL RESULTS

Analysis of cancer gene expression data is of critical impor-
tance in jointly identifying subtypes of cancerous tumors and
genes that can distinguish the subtypes or indicate a patient’s
long-term survival. Identifying a patient’s tumor subtype can
determine the course of treatment, such as recommendation of
hormone therapy in some subtypes of breast cancer, and is a
an important step toward the goal of personalized medicine.
Biclustering of breast cancer data has identiﬁed sets of genes
whose expression levels categorize tumors into ﬁve subtypes
with distinct survival outcomes [38]: Luminal A, Luminal
B, Triple negative/basal-like, HER2 type and “Normal-like”.
Related work has aimed to classify samples into each of
these subtypes or identify other types of signiﬁcant clusters
based on gene expression, clinical features and DNA copy
number analysis [39]–[42]. The clustered dendrogram obtained
by agglomerative hierarchical clustering of the genes and the
subjects is widely used in the analysis of gene expression data.
However, in contrast to our approach, hierarchical clustering
is usually applied with a metric, such as correlation, that
is global and linear, and does not
the
structure revealed by the multiscale tree structure of the other
dimension. Conversely, our approach enables us to iteratively
update both the tree and metric of the subjects based on the
metric for the genes, and update the tree and metric of the
genes based on the metric for the subjects.

take into account

We analyze three breast cancer gene expression datasets,
where the features are the genes and the observations are the
tumor samples. The ﬁrst dataset is the METABRIC dataset,
containing gene expression data for 1981 breast tumors [40]
collected with a gene expression microarray. We denote this
dataset ZM, and its set of samples YM. The second dataset,
ZT, is taken from The Cancer Genome Atlas (TCGA) Breast
Cancer cohort [43] and consists of 1218 samples, YT. This
dataset was proﬁled using RNA sequencing, which is a newer
and more advanced gene expression technology. The third
dataset ZB (BRCA-547) [41], comprising of 547 samples YB,
was acquired with microarray technology. These 547 samples
are also included in the TCGA cohort, but the gene expression
was proﬁled using a different technology.

We selected X to be the 2000 genes with the largest variance
in METABRIC from the original collection of ∼ 40000 gene
probes. In related work, the analyzed genes were selected in
a supervised manner based on prior knowledge or statistical
signiﬁcance in relation to patient survival
time [38]–[40],
[42], [44]. Here we present results of a purely unsupervised
approach aimed at exploratory analysis of high-dimensional
data, and we do not use the survival information or subtypes
labels in either applying our analysis or for gene selection, but
only in evaluating the results. In the remainder of this section
we present three approaches in which the tree transforms
and metrics are applied for the purpose of unsupervised
organization of gene expression data.

11

Fig. 4. Global bi-organization of the METABRIC dataset. The samples (columns) and genes (rows) have been reordered so they correspond to the leaves
of the two partition trees. Below the organized data are clinical details for each of the samples: two types of breast cancer subtype labels (reﬁned [42] and
PAM50 [39]) hormone receptor status (ER, PR) and HER2 status.

Regarding implementation, in this application we use ﬂex-
the partition trees in the bi-
ible trees [10] to construct
organization. We initialize the bi-organization with a corre-
lation afﬁnity on the genes (d(0)
X (x, x(cid:48)) in Alg. 1, Step 2),
which is commonly used in gene expression analysis.

A. Subject Clustering

We begin with a global analysis of all samples of the
METABRIC data using the bi-organization algorithm pre-
sented in Alg. 1. We perform two iterations of the bi-
organization using the tree-based metric with the data-driven
weights deﬁned in (28). The organized data and corresponding
trees on the samples and on the genes are shown in Fig. 4.
The samples and genes have been reordered such that they
correspond to the leaves of the two partition trees. Below
the organized data we provide clinical details for each of
the samples: two types of breast cancer subtype labels, the
reﬁned labels introduced in [42] and the standard PAM50
subtypes [39], hormone receptor status (ER, PR) and HER2
status. We analyze the folders of level l = 5 on the samples
tree, which divides the samples into ﬁve clusters (the folders
are marked with numbered colored circles).

In Fig. 5 we present histograms of the reﬁned subtype labels
for each of the numbered folders in the samples tree, and
plot the disease-speciﬁc survival curve of each folder in the
bottom graph. The histograms of each folder is surrounded by
a colored border corresponding to the colored circle indicating

TABLE I
METABRIC SELF ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)
DTC [46]
sparseBC [12]

RI
0.79
0.72
0.72
0.69
0.74
0.72
0.74
0.76

ARI VI
0.45
0.30
0.23
0.20
0.30
0.26
0.19
0.33

1.48
1.77
1.98
1.94
1.84
1.90
2.45
1.74

p-value
4.35 × 10−21
1.11 × 10−17
8.48 × 10−10
1.46 × 10−12
1.11 × 10−16
5.23 × 10−11
5.54 × 10−22
2.6 × 10−19

the relevant folder in the tree in Fig. 4. Note that the folders do
not just separate data according to subtype as in the dark blue
and light blue folders (Basal and Her2 respectively), but also
separate data according to the survival rates. If we compare the
orange and green folders that are grouped in the same parent
folder, both contain a mixture of Luminal A and Luminal B,
yet they have distinctive survival curves. The p-value of this
separation using the log-rank test [45] was 4.35 × 10−21.

We next compare our weighted metric (28) to the original
EMD-like metric (18), using different values of β and α in
(27). These values were chosen in order to place different em-
phasis of the transform coefﬁcients depending on the support
of the corresponding folders or the level of the tree. The values
of β enable to emphasize large folders (β = 1), small folders

12

the variation of information (VI) [49]. The RI and ARI
measure the similarity between two clusterings (or partitions)
of the data. Both measures indicate no agreement between the
partitions by 0 and perfect agreement by 1, however ARI can
return negative values for certain pairs of clusterings. The third
measure is an information theoretic criterion, where 0 indicates
perfect agreement between two partitions. Finally, we perform
survival analysis using Kaplan-Meier estimate [50] of disease-
speciﬁc survival rates of the samples, reporting the p-value of
the log-rank test [45]. A brief description of these statistics is
provided in Appendix II.

We select clusters by partitioning the samples into the
folders J of the samples tree TX , at a single level l of the tree
which divides the data into 4-6 clusters (typically level L − 2
in our experiments). This follows the property of ﬂexible trees
that the level at which folders are joined is meaningful across
the entire dataset, as for each level the distances between
joined folders are similar. For other types of tree construction
algorithms, alternative methods can be used to select clusters
in the tree, such as SigClust used in [41].

Results are presented in Table I for the METABRIC dataset
and in Table II for the BRCA-547 dataset. For the METABRIC
dataset, using the weighted metric achieves the best results
compared to the other weight selections, in terms of both
clustering relative to the ground-truth labels and the survival
curves of the different clusters (note these two criteria do
not always coincide). While DTC achieves the lowest p-
value overall, it has very poor clustering results compared
to the ground-truth labels (lowest ARI and highest VI). The
weighted metric out-performed the sparseBC method, which
has second-best performance for the clustering measures, and
third-lowest p-value. For the BRCA-547 dataset, the weighted
metric achieves the best clustering in terms of the ARI measure
and has the lowest p-value. For the VI measure, the clustering
by the weighted metric was slightly larger but comparable
to that of the lowest score. On this dataset, DTC performed
poorly with highest VI and p-value. The sparseBC method
achieved good clustering with highest RI and ARI measures,
but had a high p-value and VI compared to the performance
of our bi-organization method.

The results indicate that the data-driven weighting achieves
comparable if not better performance, than both using the tree-
dependent weights and competing biclustering methods. Thus,
the data-driven weighting provides an automatic method to
set appropriate weights on the transform coefﬁcients in the
metric. Our method is completely data-driven, as opposed to
the sparseBC method which requires as input the number of
features and observations to decompose the data into. (We used
the provided computationally expensive cross-validation pro-
cedure to select the best number of clusters in each dimension).
In addition, our approach provides a multiscale organization,
whereas sparseBC yields a single-scale decomposition of the
data. The DTC is a multiscale approach, however as it relies
on hierarchical clustering it does not take into account the
dendrogram in the other dimension. The performance may
be improved by using dendrograms in our iterative approach,
instead of the ﬂexible trees (this is further discussed below).

Fig. 5. (top) Histograms of folders in sample tree of METABRIC. The color
of the border corresponds to the circles in the tree. (bottom) Survival curves
for each folder.

TABLE II
BRCA-547 SELF ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)
DTC [46]
sparseBC [12]

RI
0.75
0.75
0.74
0.72
0.74
0.74
0.75
0.76

ARI VI
0.38
0.37
0.36
0.35
0.34
0.35
0.35
0.38

1.38
1.39
1.37
1.33
1.56
1.45
1.63
1.49

p-value
0.0004
0.0073
0.0028
0.0773
0.0010
0.0130
0.0853
0.0269

(β = −1) and weighting all folders equally (β = 0). The
values of α either emphasize high levels of the tree (α = 0.5),
low levels of the tree (α = −1) or weighting all levels equally
(α = 0).

We also compare to two other biclustering methods. The
ﬁrst is the dynamic tree cutting (DTC) [46] applied to a hier-
archical clustering dendrogram obtained using mean linkage
and correlation distance (a popular choice in gene expression
analysis). The second is the sparse biclustering method [12],
where the authors impose a sparse regularization on the mean
values of the estimated biclsuters (assuming the mean of
the dataset
is zero). Both algorithms are implemented in
R: package dynamicTreeCut and package sparseBC,
respectively.

We evaluate our approach by both measuring how well the
obtained clusters represent the cancer subtypes, and estimating
the statistical signiﬁcance of the survival curves of the clusters.
We compare the clustering of the samples relative to the
reﬁned subtype labels [42] using three measures: the Rand
index (RI) [47], the adjusted Rand index (ARI) [48], and

13

TABLE III
COHERENCY OF REFINED BI-ORGANIZATION

TABLE IV
METABRIC EXTERNAL ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)

Global TX
and TY
0.7039
0.7066
0.7051
0.7028
0.7051
0.7075

Reﬁned
TX
0.6103
0.6107
0.6118
0.6130
0.6119
0.6141

Reﬁned
TY
0.5908
0.5928
0.5921
0.5972
0.5927
0.5934

Reﬁned
TX , TY
0.5463
0.5480
0.5472
0.5668
0.5487
0.5497

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)

RI
0.74
0.73
0.72
0.73
0.72
0.73

ARI VI
0.30
0.29
0.26
0.28
0.27
0.25

1.77
1.87
1.87
1.83
1.89
1.98

p-value
3.71 × 10−19
7.78 × 10−16
1.77 × 10−16
4.25 × 10−14
7.02 × 10−6
3.33 × 10−16

B. Local reﬁnement

In Table III we demonstrate the improvement gained in the
organization by applying the local reﬁnement to the partition
trees, where we measure the smoothness of the organized data
using the coherency criterion (30). We perform bi-organization
for different values of β and α as well as the weighted
metric, and compare 4 organizations: 1) Global organization;
2) Reﬁned organization of only the genes tree TX ; 3) Reﬁned
organization of only the samples tree TY ; and 4) Reﬁned
organization of both the features and the samples (reﬁned TX
and TY ). Applying the reﬁned local organization to both the
genes and the samples, yields the best result with regard to
the smoothness of the bi-organization. We also examined the
the effect of the level of the tree on which the reﬁnement is
performed for l ∈ {5, 6, 7} for both trees, and the improvement
gained by reﬁnement was of the same order for all combina-
tions. The results demonstrate that regardless of the weighting
(data-driven or folder dependent), the reﬁnement procedure
improves the coherency of the organization.

C. Bi-organization with multiple datasets

Following the introduction of gene expression proﬁling by
RNA sequencing, an interesting scenario is that of two datasets
proﬁled using different technologies, one using microarray
and the other RNA sequencing. Consider, for example, the
METABRIC dataset ZM and the TCGA dataset ZT, which
share the same features X (in this case genes), but collected for
two different sample sets, YM and YT respectively. In this case,
the gene expression proﬁles have different dynamic range and
are normalized differently, and the samples cannot be analyzed
together simply by concatenating the datasets. However, the
hierarchical structure we learn on the genes, which deﬁnes a
multiscale clustering of the genes, is informative regardless of
the technique used to acquire the expression data.

Thus, the gene metric learned from one dataset can be
applied seamlessly to another dataset and used to organize
its samples due to the coupling between the genes and the
samples. We term this “external-organization”, and demon-
strate how it organizes the METABRIC dataset ZM using the
TCGA dataset ZT. We ﬁrst apply the bi-organization algorithm
to organize ZT, and then we derive the gene tree-based metric
dTX from the constructed tree on the genes TX . This metric
is then used to a construct a new tree TY on the samples set
YM of ZM.

In Table IV we compare the external organization of
METABRIC using our weighted metric to the original EMD-

like metric for different values of β and α. Our results
show that the data-driven weights achieve the best results,
reinforcing that learning the weights in a data-adaptive way
is more beneﬁcial than setting the weights based on the size
of the folders or the level of the tree. Applying external
organization enables us to assess which bi-organization of the
external dataset and corresponding learned metric were the
most meaningful. Note that for some of the parameter choices
(α = 0, β = 1 or β = −1), the external organization of
ZM using a gene tree learned from the dataset ZT was better
than the internal organization. Thus, via the organization of the
dataset ZM, we validate that the hierarchical organization of
the genes in ZT, and therefore, the corresponding metric, are
effective in clustering samples into cancer subtypes. This also
demonstrates that the hierarchical gene organization learned
from one dataset can be successfully applied to another dataset
to learn a meaningful sample organization, even though the
two were proﬁled using different technologies. This provides
motivation to integrate information from datasets together.

In our ﬁnal evaluation, we divide the METABRIC dataset
into its two original subsets: the discovery set comprising 997
tumors and the validation set comprising 995 tumors. Note
that the two sets have different sample distributions of cancer
subtypes. We compare three approaches for organizing the
data. We begin with the self-organization as in Sec. V-A.
We organize each of the two datasets separately and report
their clustering measures in the ﬁrst row in Table V for the
discovery cohort and in Table VI for the validation cohort.
Note that the organization achieved using half the data is
less meaningful in terms of the survival rates compared to
using all of the data. This is due to the different distribution
of subtypes and survival times between the discovery and
validation cohorts, and in addition, the p-value calculation
itself is dependent on the sample size used.

One of the important aspects in a practical application is the
ability to process new samples. Our approach naturally allows
for such a capability. Assume we have already performed bi-
organization on an existing dataset and we acquire a few new
test samples. Instead of having to reapply the bi-organization
procedure to all of the data, we can instead insert the new
samples into the existing organization. We demonstrate this
by using each subset of the METABRIC dataset to organize
the other. In contrast to the external organization example, here
we have two datasets proﬁled with the same technology. We
can treat this as a training and test set scenario: construct a
sample tree on the training set Ytrain and use the learned metric
on the genes dTX to insert samples from the test set Ytest into

the training sample tree TYtrain . First, we calculate the centroids
of the folders Jj of level l = 1 (the level above the leaves) in
the samples tree TYtrain:
(cid:88)

MY [j, y]Z[x, y], x ∈ {1, ..., nX },

l(Jj) = 1

Cj(x) =

y

(31)
These can be considered the representative sample of each
folder. We then assign each new sample y ∈ Ytest
to its
nearest centroid using the metric dTX (y, Cj) derived from
the gene tree TX . Thus, we reconstruct the sample hierarchy
on the test dataset Ytest by assigning each test sample to
the hierarchical clustering of the low-level centroids from the
training sample tree. This approach, therefore, validates the
sample organization as well as the gene organization, whereas
the external organization only enables to validate the gene
organization.

We perform this once treating the validation set as the
training set and the discovery set as the test set, and then vice-
versa. We report the clustering measures in the second row of
Table V and Table VI. Note that the measures are reported only
for the samples belonging to the given set in the table. Inserting
samples from one dataset into the sample tree of another
demonstrates an improved organization in some measures
compared to performing self-organization. For example, the
organization of the discovery set via the validation tree results
in a clustering with improved ARI and VI measures. This
serves as additional evidence for the importance of integrating
information from several datasets together.

Thus far in our experiments, we have gathered substantial
evidence for the importance of information stemming from
multiple data sets. Here, we harness the multiple tree met-
ric (25) to perform integration of datasets in a more systematic
manner. We generalize the external organization method to
several datasets, where we integrate all the learned trees on
the genes {TX } into a single metric via the multi-tree metric.
In addition to the gene tree from both METABRIC datasets,
we also obtain the gene trees from the TCGA and the BRCA-
547 datasets, ZT and ZB. We then calculate a multi-tree met-
ric (25) to construct the sample tree on either the discovery or
validation sets. We report the evaluation measures in the third
row of Table V and Table VI. Taking into account all measures,
the multi-tree metric incorporating four different datasets best
organizes both the discovery and validation datasets. Integrat-
ing information from multiple sources improves the accuracy
of the organization, as averaging the metrics emphasizes
genes that are consistently grouped together, representing the
intrinsic structure of the data. In addition, since the metric
integrates the organizations from several datasets, it is more
accurate than the internal organization of a dataset with few
samples or a non-uniform distribution of subtypes.

Our results show that external organization, via either both
single or multi-tree metric, enables us to learn a meaningful
multi-scale hierarchy on the genes and apply it as a metric to
organize the samples of a given dataset. Thus, we can apply
information from one dataset to another to recover a multi-
scale organization of the samples, even if they were proﬁled
in a different technique. In addition, we obtain a validation of

14

TABLE V
METABRIC DISCOVERY ORGANIZATION

discovery
Self-organization
Inserted into
validation tree
Multi-tree

RI
0.75

ARI VI
0.33

1.81

p-value
1.82 × 10−11

0.74

0.34

1.66

2.93 × 10−9

0.75

0.35

1.63

3.18 × 10−13

TABLE VI
METABRIC VALIDATION ORGANIZATION

validation
Self-organization
Inserted into
discovery tree
Multi-tree

RI
0.77

ARI VI
0.33

1.82

p-value
3.07 × 10−4

0.76

0.30

1.98

9.08 × 10−8

0.76

0.34

1.73

4.24 × 10−9

the gene organization of one dataset via another. This cannot
be accomplished with traditional hierarchical clustering in a
clustered dendrogram as the clustering of the samples does not
depend on the hierarchical structure of the genes dendrogram.
However, we can obtain an iterative hierarchical clustering
algorithm for biclustering using our approach. As our bi-
organization depends on a partition tree method, we can use
hierarchical clustering instead of ﬂexible trees in the iterative
bi-organization algorithm. Alternatively, as hierarchical clus-
tering depends on a metric, this can also be formulated as
deriving a transform from the dendrogram on the genes and
using its corresponding tree-metric instead of correlation as
the input metric to the hierarchical clustering algorithm on
the samples, and vice-versa.

In related work, Cheng, Yang and Anastassiou [51] analyzed
multiple datasets and identiﬁed consistent groups of genes
across datasets. Zhou et al. [52] integrate datasets in a platform
independent manner to identify groups of genes with the same
function across multiple datasets. The multi-tree transform can
also be used to identify such genes, however this is beyond
the scope of this paper and will be addressed in future work.

D. Sub-type labels

In breast cancer, PAM50 [39] is typically used to assign
intrinsic subtypes to the tumors. However, Milioli et al. [42]
recently proposed a reﬁned set of subtypes labels for the
METABRIC dataset, based on a supervised iterative approach
to ensure consistency of the labels using several classiﬁers.
Their labels are shown to have a better agreement with
the clinical markers and patients’ overall survival than those
provided by the PAM50 method. Therefore, the clustering
measures we reported on the METABRIC dataset were with
respect to the reﬁned labels.

Our unsupervised analysis demonstrated higher consistency
with the reﬁned labels than with PAM50. Thus, our unsuper-
vised approach provides an additional validation to the labeling
achieved in a supervised manner. We divided the data into
training and test sets and classiﬁed the test set using k-NN
nearest neighbors with majority voting using the tree-based
metric. For different parameters and increasing numbers of

genes (nX = 500, 1000, 2000), we had higher agreement with
the reﬁned labels than with PAM50, achieving a classiﬁcation
accuracy of 82% on average. Classifying with the PAM50
labels had classiﬁcation accuracy lower by an average of
10% ± 2%. This is also evident when examining the labels
in Fig. 4. Note that whereas PAM50 assigns a label based
on 50 genes and the reﬁned labels were learned using a
subset of genes found in a supervised manner, our approach
is unsupervised using the nX genes with the highest variance.

VI. CONCLUSIONS

In this paper we proposed new data-driven tree-based trans-
forms and metrics in a matrix organization setting. We pre-
sented partition trees as inducing a new multiscale transform
space that conveys the smooth organization of the data, and
derived a metric in the transform space. The trees and cor-
responding metrics are updated in an iterative bi-organization
approach, organizing the observations based on the multiscale
decomposition of the features, and organizing the features
based on the multiscale decomposition of the observations.
In addition, we generalized the transform and the metric
to incorporate multiple partition trees on the data, allowing
for the integration of several datasets. We applied our data-
driven approach to the organization of breast cancer gene
expression data, learning metrics on the genes to organize the
tumor samples in meaningful clusters of cancer sub-types. We
demonstrated how our approach can be used to validate the
hierarchical organization of both the genes and the samples
by taking into account several datasets of samples, even
when these datasets were proﬁled using different technolo-
gies. Finally, we employed our multi-tree metric to integrate
information from the organization of these multiple datasets
and achieved an improved organization of tumor samples.

In future work, we will explore several aspects of the
multiple tree setting. First, the multi-tree transform and metric
can be incorporated in the iterative framework for further
reﬁnement. Second, we will generalize the coherency measure
to incorporate multiple trees. Third, we will apply the multi-
tree framework to a multi-modal setting, where observations
are shared across datasets, as for example, in the joint samples
shared by the BRCA-547 and TCGA datasets. Finally, we will
reformulate the iterative procedure as an optimization problem,
enabling to explicitly introduce cost functions. In particular,
cost functions imposing the common structure of the multiple
trees across datasets will be considered.

ACKNOWLEDGMENTS

The authors thank the anonymous reviewers for their con-

structive comments and useful suggestions.

REFERENCES

15

[4] W. H. Yang, D. Q. Dai, and H. Yan, “Finding correlated biclusters from
gene expression data,” IEEE Trans. Knowl. Data Eng., vol. 23, no. 4,
pp. 568–584, April 2011.

[5] E. C. Chi, G. I. Allen, and R. G. Baraniuk, “Convex biclustering,”
Biometrics, 2016. [Online]. Available: http://dx.doi.org/10.1111/biom.
12540

[6] D. Jiang, C. Tang, and A. Zhang, “Cluster analysis for gene expression
data: a survey,” IEEE Trans. Knowl. Data Eng., vol. 16, no. 11, pp.
1370–1386, 2004.

[7] J. Bennett and S. Lanning, “The Netﬂix prize,” in Proceedings of KDD

cup and workshop, vol. 2007, 2007, p. 35.

[8] S. Busygin, O. Prokopyev, and P. M. Pardalos, “Biclustering in data
mining,” Computers & Operations Research, vol. 35, no. 9, pp. 2964 –
2987, 2008.

[9] M. Gavish and R. R. Coifman, “Sampling, denoising and compression
of matrices by coherent matrix organization,” Appl. Comput. Harmon.
Anal., vol. 33, no. 3, pp. 354 – 369, 2012.

[10] J.

I. Ankenman, “Geometry and analysis of dual networks on
questionnaires,” Ph.D. dissertation, Yale University, 2014. [Online].
Available: https://github.com/hgfalling/pyquest/blob/master/ankenman
diss.pdf

[11] Y. Kluger, R. Basri, J. T. Chang, and M. Gerstein, “Spectral biclus-
tering of microarray data: coclustering genes and conditions,” Genome
research, vol. 13, no. 4, pp. 703–716, 2003.

[12] K. M. Tan and D. M. Witten, “Sparse biclustering of transposable data,”

J. Comp. Graph. Stat., vol. 23, no. 4, pp. 985–1008, 2014.

[13] M. Gavish, B. Nadler, and R. R. Coifman, “Multiscale wavelets on
trees, graphs and high dimensional data: Theory and applications to
semi supervised learning,” in Proc. ICML, 2010, pp. 367–374.

[14] A. Singh, R. Nowak, and R. Calderbank, “Detecting weak but
hierarchically-structured patterns in networks,” in Proc. AISTATS, vol. 9,
May 2010, pp. 749–756.

[15] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on
graphs via spectral graph theory,” Appl. Comput. Harmon. Anal., vol. 30,
no. 2, pp. 129 – 150, 2011.

[16] J. Sharpnack, A. Singh, and A. Krishnamurthy, “Detecting activations
over graphs using spanning tree wavelet bases.” in AISTATS, 2013, pp.
536–544.

[17] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-
dergheynst, “The emerging ﬁeld of signal processing on graphs: Ex-
tending high-dimensional data analysis to networks and other irregular
domains,” IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83–98, 2013.
[18] S. K. Narang and A. Ortega, “Compact support biorthogonal wavelet
ﬁlterbanks for arbitrary undirected graphs,” IEEE Trans. Signal Process.,
vol. 61, no. 19, pp. 4673–4685, Oct 2013.

[19] A. Sakiyama, K. Watanabe, and Y. Tanaka, “Spectral graph wavelets
and ﬁlter banks with low approximation error,” IEEE Trans. Signal Inf.
Process. Netw., vol. 2, no. 3, pp. 230–245, Sept 2016.

[20] D. I. Shuman, M. J. Faraji, and P. Vandergheynst, “A multiscale pyramid
transform for graph signals,” IEEE Trans. Signal Process., vol. 64, no. 8,
pp. 2119–2134, April 2016.

[21] N. Tremblay and P. Borgnat, “Subgraph-based ﬁlterbanks for graph
signals,” IEEE Trans. Signal Process., vol. 64, no. 15, pp. 3827–3840,
Aug 2016.

[22] N. Shahid, N. Perraudin, V. Kalofolias, G. Puy, and P. Vandergheynst,
“Fast robust PCA on graphs,” IEEE J. Sel. Topics Signal Process.,
vol. 10, no. 4, pp. 740–756, June 2016.

[23] G. Mishne, R. Talmon, R. Meir, J. Schiller, U. Dubin, and R. R.
Coifman, “Hierarchical coupled-geometry analysis for neuronal structure
and activity pattern discovery,” IEEE J. Sel. Topics Signal Process.,
vol. 10, no. 7, pp. 1238–1253, Oct 2016.

[24] P. Burt and E. Adelson, “The Laplacian pyramid as a compact image
code,” IEEE Trans. Commun., vol. 31, no. 4, pp. 532–540, 1983.
[25] R. R. Coifman and W. E. Leeb, “Earth mover’s distance and equivalent
metrics for spaces with hierarchical partition trees,” Yale University,
Tech. Rep., 2013, technical report YALEU/DCS/TR1482.

[26] I. Ram, M. Elad, and I. Cohen, “Generalized tree-based wavelet trans-
form,” IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4199–4209, 2011.
[27] R. Kondor, N. Teneva, and V. Garg, “Multiresolution matrix factoriza-

[1] Y. Cheng and G. M. Church, “Biclustering of expression data,” in ISMB,

tion,” in Proc. ICML, 2014, pp. 1620–1628.

vol. 8, 2000, pp. 93–103.

[28] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp.

[2] C. Tang, L. Zhang, A. Zhang, and M. Ramanathan, “Interrelated two-
way clustering: an unsupervised approach for gene expression data
analysis,” in Proc. BIBE, 2001, pp. 41–48.

[3] M. Lee, H. Shen, J. Z. Huang, and J. S. Marron, “Biclustering via
sparse singular value decomposition,” Biometrics, vol. 66, no. 4, pp.
1087–1095, 2010.

5–32, 2001.

[29] R. R. Coifman and S. Lafon, “Diffusion maps,” Appl. Comput. Harmon.

Anal., vol. 21, no. 1, pp. 5–30, July 2006.

[30] R. R. Coifman and M. Gavish, “Harmonic analysis of digital data
bases,” in Wavelets and Multiscale Analysis, ser. Applied and Numerical
Harmonic Analysis. Birkh¨auser Boston, 2011, pp. 161–197.

[31] G. Mishne, “Diffusion nets and manifold learning for high-dimensional
data analysis in the presence of outliers,” Ph.D. dissertation, Technion,
2016.

[32] M. Zontak, I. Mosseri, and M. Irani, “Separating signal from noise using

patch recurrence across scales,” in Proc. CVPR, June 2013.

[33] Y.-Y. Ahn, J. P. Bagrow, and S. Lehmann, “Link communities reveal
multiscale complexity in networks,” Nature, vol. 466, no. 7307, pp. 761–
764, 2010.

[34] J. Xie, S. Kelley, and B. K. Szymanski, “Overlapping community
detection in networks: The state-of-the-art and comparative study,” ACM
Comput. Surv., vol. 45, no. 4, pp. 43:1–43:35, Aug. 2013.

[35] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu,
“An optimal algorithm for approximate nearest neighbor searching ﬁxed
dimensions,” J. ACM, vol. 45, no. 6, pp. 891–923, Nov. 1998.

[36] B.-K. Yi and C. Faloutsos, “Fast time sequence indexing for arbitrary

lp norms,” in Proc. VLDB, 2000.

[37] [Online]. Available: http://github.com/gmishne/pyquest
[38] T. Sørlie et al., “Gene expression patterns of breast carcinomas distin-
guish tumor subclasses with clinical implications,” Proc. Natl. Acad.
Sci., vol. 98, no. 19, pp. 10 869–10 874, 2001.

[39] J. S. Parker et al., “Supervised risk predictor of breast cancer based on
intrinsic subtypes,” Journal of Clinical Oncology, vol. 27, no. 8, pp.
1160–1167, 2009.

[40] C. Curtis et al., “The genomic and transcriptomic architecture of 2,000
breast tumours reveals novel subgroups,” Nature, vol. 486, no. 7403, pp.
346–352, 2012.

[41] Cancer Genome Atlas Network, “Comprehensive molecular portraits of

human breast tumours,” Nature, vol. 490, no. 7418, pp. 61–70, 2012.

[42] H. H. Milioli, R. Vimieiro, I. Tishchenko, C. Riveros, R. Berretta, and
P. Moscato, “Iteratively reﬁning breast cancer intrinsic subtypes in the
METABRIC dataset,” BioData Mining, vol. 9, no. 1, pp. 1–8, 2016.

[43] Cancer Genome Atlas Network. [Online]. Available: https://xenabrowser.
net/datapages/?cohort=TCGA%20Breast%20Cancer%20(BRCA)
[44] C. M. Perou et al., “Molecular portraits of human breast tumours,”

Nature, vol. 406, no. 6797, pp. 747–752, 2000.

[45] R. Peto and J. Peto, “Asymptotically efﬁcient rank invariant test proce-
dures,” Journal of the Royal Statistical Society. Series A (General), vol.
135, no. 2, pp. 185–207, 1972.

[46] P. Langfelder, B. Zhang, and S. Horvath, “Deﬁning clusters from a hier-
archical cluster tree: the dynamic tree cut package for r,” Bioinformatics,
vol. 24, no. 5, pp. 719–720, 2008.

[47] W. M. Rand, “Objective criteria for the evaluation of clustering meth-
ods,” Journal of the American Statistical Association, vol. 66, no. 336,
pp. 846–850, 1971.

[48] L. Hubert and P. Arabie, “Comparing partitions,” J. Classiﬁcation, vol. 2,

no. 1, pp. 193–218, 1985.

[49] M. Meil˘a, “Comparing clusterings - an information based distance,”
Journal of Multivariate Analysis, vol. 98, no. 5, pp. 873 – 895, 2007.
[50] E. L. Kaplan and P. Meier, “Nonparametric estimation from incomplete
observations,” Journal of the American statistical association, vol. 53,
no. 282, pp. 457–481, 1958.

[51] W.-Y. Cheng, T.-H. O. Yang, and D. Anastassiou, “Biomolecular events
in cancer revealed by attractor metagenes,” PLoS Comput Biol, vol. 9,
no. 2, pp. 1–14, 02 2013.

[52] X. J. Zhou et al., “Functional annotation and network reconstruction
through cross-platform integration of microarray data,” Nature biotech-
nology, vol. 23, no. 2, pp. 238–243, 2005.

[53] J. P. Klein and M. L. Moeschberger, Survival analysis: techniques for

censored and truncated data. SSBM, 2005.

APPENDIX I
FLEXIBLE TREES

We brieﬂy describe the ﬂexible trees algorithm, given the
feature set X and an afﬁnity matrix on the features denoted
KX . For a detailed description see [10].

1) Input: The set of features X , an afﬁnity matrix KX ∈

RnX ×nX , and a constant (cid:15).

2) Init: Set partition I0,i = {i} ∀ 1 ≤ i ≤ nX , set l = 1.
3) Given an afﬁnity on the data, we construct a low-

dimensional embedding on the data [29].

4) Calculate

level-dependent
d(l)(i, j) ∀ 1 ≤ i, j ≤ nX in the embedding space.

pairwise

the

distances

16

(cid:15) , where p = median (cid:0)d(l)(i, j)(cid:1).

5) Set a threshold p
6) For each index i which has not yet been added
to a folder, ﬁnd its minimal distance dmin(i) =
minj{d(l)(i, j)}.

• If dmin(i) < p

(cid:15) , i and j form a new folder if j
does not belong to a folder. If j is already part of a
folder I, then i is added to that folder if dmin(i) <
p
(cid:15) 2−|I|+1.
• If dmin(i) > p

(cid:15) , i remains as a singleton folder.
7) The partition Pl is set to be all the formed folders.
8) For l > 1 and while not all samples have been merged
together in a single folder, steps 4-7 are repeated for the
folders Il−1,i ∈ Pl−1. The distances between folders
depend on the level l, and on the samples in each of the
folders.

APPENDIX II
COMPARING SURVIVAL CURVES
The survival function S(t) is deﬁned as the probability that
a subject will survive past time t. Let T be a failure time
with probability density function f . The survival function is
S(t) = P (T > t), where the Kaplan-Meier method [50] is a
non-parametric estimate given by

ˆS(tj) =

P r(T > ti|T ≥ ti) =

j
(cid:89)

i=1

ˆS(tj−1)P r(T > tj|T ≥ tj).

(32)

Deﬁning ni as the number at risk just prior to time ti and
di as the number of failures at ti, then P (T > ti) = ni−di
.
For more information on estimating survival curves and taking
into account censored data see [53]

ni

Comparison of two survival curves can be done using a
statistical hypothesis test called the log-rank test [45]. It is
used to test the null hypothesis that there is no difference
between the population survival curves (i.e. the probability of
an event occurring at any time point is the same for each
population). Deﬁne nk,i as the number at risk in group k just
prior to time ti, such that ni = (cid:80)
k nk,i and dk,i as the number
of failures in group k at time ti such that di = (cid:80)
k dk,i. Then,
the expected number of failures in group k = 1, 2 is given by
(cid:88)

and the observed number of failures in group k = 1, 2 is

Ek =

di

nk,i
ni

(cid:88)

Ok =

dk,i.

i

i

(O2 − E2)2
Var(O2 − E2)

∼ χ2
1.

(33)

(34)

(35)

Under the null hypothesis of no difference between the two

groups, the log-rank test statistic is

The log-rank test can be extended to more than two
groups [53].

Data-Driven Tree Transforms and Metrics

Gal Mishne, Ronen Talmon, Israel Cohen, Ronald R. Coifman and Yuval Kluger

1

7
1
0
2
 
g
u
A
 
8
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
6
7
5
0
.
8
0
7
1
:
v
i
X
r
a

Abstract—We consider the analysis of high dimensional data
given in the form of a matrix with columns consisting of
observations and rows consisting of features. Often the data
is such that the observations do not reside on a regular grid,
and the given order of the features is arbitrary and does not
convey a notion of locality. Therefore, traditional transforms
and metrics cannot be used for data organization and analysis.
In this paper, our goal is to organize the data by deﬁning an
appropriate representation and metric such that they respect the
smoothness and structure underlying the data. We also aim to
generalize the joint clustering of observations and features in
the case the data does not fall into clear disjoint groups. For
this purpose, we propose multiscale data-driven transforms and
metrics based on trees. Their construction is implemented in an
iterative reﬁnement procedure that exploits the co-dependencies
between features and observations. Beyond the organization
of a single dataset, our approach enables us to transfer the
organization learned from one dataset to another and to integrate
several datasets together. We present an application to breast
cancer gene expression analysis: learning metrics on the genes to
cluster the tumor samples into cancer sub-types and validating
the joint organization of both the genes and the samples. We
demonstrate that using our approach to combine information
from multiple gene expression cohorts, acquired by different
proﬁling technologies, improves the clustering of tumor samples.

Index Terms—graph signal processing, multiscale representa-

tions, geometric analysis, partition trees, gene expression

I. INTRODUCTION

High-dimensional datasets are typically analyzed as a two-
dimensional matrix where, for example,
the rows consist
of features and the columns consist of observations. Signal
processing addresses the analysis of such data as residing on
a regular grid, such that the rows and columns are given
in a particular order,
indicating smoothness. For example,
the ordering in time-series data indicates temporal-frequency
smoothness, and the order in 2D images indicating spatial
smoothness. Non-Euclidean data that do not reside on a regular
grid, but rather on a graph, raise the more general problem of
matrix organization. In such datasets, the given ordering of the
rows (features) and columns (observations) does not indicate
any degree of smoothness.

However, in many applications, for example, analysis of
gene expression data, text documents, psychological question-

G. Mishne and R. R. Coifman are with the Department of Mathematics,
Yale University, New Haven, CT 06520 USA (e-mail: gal.mishne@yale.edu
; ronald.coifman@math.yale.edu.). R. Talmon and I. Cohen are with the
Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of
ico-
Technology, Haifa 32000,
hen@ee.technion.ac.il). Y. Kluger is with the Department of Pathology and
the Yale Cancer Center, Yale University School of Medicine, New Haven, CT
06511 USA (e-mail: yuval.kluger@yale.edu). This research was supported by
the Israel Science Foundation (grant no. 576/16), and by the United States-
Israel Binational Science Foundation and the United States National Science
Foundation (grant no. 2015582), and by the National Institutes of Health (grant
no. 1R01HG008383-01A1).

ronen@ee.technion.ac.il;

(e-mail:

Israel.

naires and recommendation systems [1]–[10], there is an un-
derlying structure to both the features and the observations. For
example, in gene expression subsets of samples (observations)
have similar genetic proﬁles, while subsets of genes (features)
have similar expressions across groups of samples. Thus, as
the observations are viewed as high-dimensional vectors of
features, one can swap the role of features and observations,
and treat the features as high-dimensional vectors of observa-
tions. This dual analysis reveals meaningful joint structures in
the data.

The problem of matrix organization considered here is
closely related to biclustering [1]–[5], [11], [12], where the
goal is to identify biclusters: joint subsets of features and
observations such that the matrix elements in each subset have
similar values. Matrix organization goes beyond the extraction
of joint clusters, yielding a joint reordering of the entire dataset
and not just the extraction of partial subsets of observations
and features that constitute bi-clusters. By recovering the
smooth joint organization of the features and observations,
one can apply signal processing and machine learning methods
such as denoising, data completion, clustering and classiﬁca-
tion, or extract meaningful patterns for exploratory analysis
and data visualization.

The application of traditional signal processing transforms
to data on graphs is not straightforward, as these transforms
rely almost exclusively on convolution with ﬁlters of ﬁnite
support, and thus are based on the assumption that the given
ordering of the data conveys smoothness. The ﬁeld of graph
signal processing adapts classical techniques to signals sup-
ported on a graph (or a network), such as ﬁltering and wavelets
in the graph domain [13]–[22]. Consider for example signals
(observations) acquired from a network of sensors (features).
The nodes of the graph are the sensors and the edges and
their weights are typically dictated by a-priori information
such as physical connectivity, geographical proximity, etc. The
samples collected from all sensors at a given time compose a
high-dimensional graph signal supported on this network. The
signal observations, acquired over time, are usually processed
separately and the connectivity between the observations is not
taken into account.

To address this issue, in this paper we propose to analyze
the data in a matrix organization setting as represented by
two graphs: one whose nodes are the observations and the
other whose nodes are the features, and our aim is a joint
unsupervised organization of these two graphs. Furthermore,
we do not ﬁx the edge weights by relying on a predetermined
structure or a-priori information. Instead, we calculate the edge
weights by taking into account the underlying dual structure
of the data and the coupling between the observations and the
features. This requires deﬁning two metrics, one between pairs
of observations and one between pairs of features.

2

Such an approach for matrix organization was introduced by
Gavish and Coifman [9], where the organization of the data
relies on the construction of a pair of hierarchical partition
trees on the observations and on the features. In previous
work [23], we extended this methodology to the organization
of a rank-3 tensor (or a 3D database), introducing a multiscale
averaging ﬁlterbank derived from partition trees.

Here we introduce a new formulation of the averaging ﬁlter-
bank as a tree-based linear transform on the data, and propose
a new tree-based difference transform. Together these yield
a multiscale representation of both the observations and the
features, in analogue to the Gaussian and Laplacian pyramid
transforms [24]. Our transforms can be seen as data-driven
multiscale ﬁlters on graphs, where in contrast to classical
signal processing, the support of the ﬁlters is non-local and
depends on the structure of the data. From the transforms,
we derive a metric in the transform space that incorporates
the multiscale structure revealed by the trees [25]. The trees
and the metrics are incorporated in an iterative bi-organization
procedure following [9]. We demonstrate that beyond the
organization of a single dataset, our metric enables us to
apply the organization learned from one dataset to another
and to integrate several datasets together. This is achieved by
generalizing our transform to a new multi-tree transform and
to a multi-tree metric, which integrate a set of multiple trees on
the features. Finally, the multi-tree transform inspires a local
reﬁnement of the partition trees, improving the bi-organization
of the data.

The remainder of the paper is organized as follows. In
Section II, we formulate the problem, present an overview of
our solution and review related background. In Section III, we
present the new tree-induced transforms and their properties.
In Section IV, we derive the metric in the transform space and
propose different extensions of the metric. We also propose a
local reﬁnement of the bi-organization approach. Section V
presents experimental results in the analysis of breast cancer
gene expression data.

non-binary tree is not uniquely deﬁned. Finally, since the
averaging transform is over-complete such that each ﬁlter
corresponds to a single folder in the tree, it is simple to design
weights on the transform coefﬁcients based on the properties
of the individual folders.

it

Filterbanks and multiscale transforms on trees and graphs
have been proposed in [18]–[21], yet differ from our ap-
proach in several aspects. While ﬁlterbanks construct a multi-
scale representation by using downsampling operators on the
data [18], [20], the multiscale nature of our transform arises
is
from partitioning of the data via the tree. In that,
most similar to [21], where the graph is decomposed into
subgraphs by partitioning. However, all these ﬁlterbanks on
graphs employ the eigen-decomposition of the graph Laplacian
to deﬁne either global ﬁlters on the full graph or local ﬁlters
on disjoint subgraphs. Our approach, conversely, employs the
eigen-decomposition of the graph Laplacian to construct the
partition tree, but the transforms (ﬁlters) are deﬁned by the
structure of the tree and not explicitly derived from the Lapla-
cian. In addition, we do not treat the structure of the graph as
ﬁxed, but rather iteratively update the Laplacian based on the
tree transform. Finally, while graph signal processing typically
addresses one dimension of the data (features or observations),
our approach addresses the construction of transforms on both
the observations and features of a dataset, and relies on the
coupling between the two to derive the transforms.

This work is also related to the matrix factorization pro-
posed by Shahid et al. [22], where the graph Laplacians of both
the features and the observation regularize the decomposition
of a dataset
into a low-rank matrix and a sparse matrix
representing noise. Then the observations are clustered using
k-means on the low-dimensional principal components of the
smooth low-rank matrix. Our work differs in that we preform
an iterative non-linear embedding of the observations and
features, not jointly, but alternating between the two while
updating the graph Laplacian of each in turn. In addition, we
provide a multiscale clustering of the data.

A. Related Work

II. BI-ORGANIZATION

Various methodologies have been proposed for the con-
struction of wavelets on graphs,
including Haar wavelets,
and wavelets based on spectral clustering and spanning tree
decompositions [13]–[16], [26], [27]. Our work deviates from
this path and presents an iterative construction of data-driven
tree-based transforms. In contrast to previous multiscale rep-
resentations of a single graph, our approach takes into account
the co-dependencies between observations and features by
incorporating two graph structures. Our motivation for the
proposed transforms is the tree-based Earth Mover’s Distance
(EMD) proposed in [25], which introduces a coupling between
observations and features, enabling an iterative procedure that
updates the trees and metrics in each iteration. The averaging
transform, in addition to being equipped with this metric, is
also easier to compute than a wavelet basis as it does not
require an orthogonalization procedure. In addition, given a
the averaging and difference transforms are
partition tree,
unique, whereas the tree-based wavelet transform [13] on a

A. Problem Formulation

Let Z be a high-dimensional dataset and let us denote its
set of nX features by X and denote its set of nY observations
by Y. For example, in gene expression data, X consists of
the genes and Y consists of individual samples. The element
Z(x, y) is the expression of gene x ∈ X in sample y ∈ Y. The
given ordering of the dataset is arbitrary such that adjacent
features and adjacent observations in the dataset are likely
dissimilar. We assume there exists a reordering of the features
and a reordering of the observations such that Z is smooth.

Deﬁnition 1: A matrix Z is smooth if it satisﬁes the mixed
H¨older condition [9], such that ∀x, x(cid:48) ∈ X and ∀y, y(cid:48) ∈ Y,
and for a pair of non-trivial metrics ρX on X and ρY on Y
and constants C > 0 and 0 < α ≤ 1:

|Z(x, y) − Z(x, y(cid:48)) − Z(x(cid:48), y) + Z(x(cid:48), y(cid:48))|

≤ CρX (x, x(cid:48))αρY (y, y(cid:48))α.

(1)

Note that we do not impose smoothness as an explicit con-
straint; instead it manifests itself implicitly in our data-driven
approach.

Although the given ordering of the dataset is not smooth, the
organization of the observations and the features by partition
trees following [9] constructs both local and global neigh-
borhoods of each feature and of each observation. Thus, the
structure of the tree organizes the data in a hierarchy of nested
clusters in which the data is smooth. Our aim is to deﬁne a
transform on the features and on the observations that conveys
the hierarchy of the trees, thus recovering the smoothness of
the data. We deﬁne a new metric in the transform space that
incorporates the hierarchical clustering of the data via the
trees. The notations in this paper follow these conventions:
matrices are denoted by bold uppercase and sets are denoted
by uppercase calligraphic.

B. Method Overview

The construction of the tree, which relies on a metric, and
the calculation of the metric, which is derived from a tree, lead
to an iterative bi-organization algorithm [9]. Each iteration
updates the pair of trees and metrics on the observations
and features as follows. First, an initial partition tree on the
features, denoted TX , is calculated based on an initial pairwise
afﬁnity between features. This initial afﬁnity is application
dependent. Based on a coarse-to-ﬁne decomposition of the
features implied by the partition tree on the features, we deﬁne
a new metric between pairs of observations: dTX (y, y(cid:48)). The
metric is then used to construct a new partition tree on the
observations TY . Thus, the construction of the tree on the
observations TY is based on a metric induced by the tree on the
features TX . The new tree on the observations TY then deﬁnes
a new metric between pairs of features dTY (x, x(cid:48)). Using this
metric, a new partition tree is constructed on the features TX ,
and a new iteration begins. Thus, this approach exploits the
strong coupling between the features and the observations.
This enables an iterative procedure in which the pair of trees
are reﬁned from iteration to iteration, providing in turn a more
accurate metric on the features and on the observations. We
will show that the resulting tree-based transform and corre-
sponding metric enable a multiscale analysis of the dataset,
reordering of the observations and features, and detection of
meaningful joint clusters in the data.

C. Partition trees

Given a dataset Z, we construct a hierarchical partitioning
of the observations and features deﬁned by a pair of trees.
Without loss of generality, we deﬁne the partition trees in this
section with respect to the features, and introduce relevant
notation.

Let TX be a partition tree on the features. The partition tree
is composed of L + 1 levels, where a partition Pl is deﬁned
for each level 0 ≤ l ≤ L. The partition Pl = {Il,1, ..., Il,n(l)}
at level l consists of n(l) mutually disjoint non-empty subsets
of indices in {1, ..., nX }, termed folders and denoted by Il,i,
i ∈ {1, ..., n(l)}. Note that we deﬁne the folders on the indices
of the set and not on the features themselves.

3

The partition tree TX has the following properties:
• The ﬁnest partition (l = 0) is composed of n(0) = nX
singleton folders, termed the “leaves”, where I0,i = {i}.
• The coarsest partition (l = L) is composed of a single
folder, PL = IL,1 = {1, ..., nX }, termed the “root”.
• The partitions are nested such that if I ∈ Pl, then I ⊆ J
for some J ∈ Pl+1, i.e., each folder at level l − 1 is a
subset of a folder from level l.

The partition tree is the set of all folders at all levels TX =
{Il,i | 0 ≤ l ≤ L, 1 ≤ i ≤ n(l)}, and the number of all
folders in the tree is denoted by NX = |TX |. The size, or
cardinality, of a folder I, i.e. the number of indices in that
folder, is denoted by |I|. In the remainder of the paper, for
compactness, we drop the subscript l denoting the level of a
folder, and denote a single folder by either I or Ii, such that
i ∈ {1, ..., NX } is an index over all folders in the tree.

Given a dataset, there are many methods to construct a
partition tree, including deterministic, random, agglomerative
(bottom-up) and divisive (top-down) [5], [13], [28]. For ex-
ample, in a bottom-up approach, we begin at the lowest level
of the tree and cluster the features into small folders. These
folders are then clustered into larger folders at higher levels
of the tree, until all folders are merged together at the root.

Some approaches take into account the geometric structure
and multiscale nature of the data by incorporating afﬁnity ma-
trices deﬁned on the data, and manifold embeddings [10], [13].
Ankenman [10] proposed “ﬂexible trees”, whose construction
requires an afﬁnity kernel deﬁned on the data, and is based on a
low-dimensional diffusion embedding of the data [29]. Given
a metric between features d(x, x(cid:48)), a local pairwise afﬁnity
kernel k(x, x(cid:48)) = exp{−d(x, x(cid:48))/σ2} is integrated into a
global representation on the data via a manifold embedding
representation Ψ, which minimizes

min

k(x, x(cid:48))(cid:107)Ψ(x) − Ψ(x(cid:48))(cid:107)2
2.

(2)

(cid:88)

x,x(cid:48)

The clustering of the folders in the ﬂexible tree algorithm is
based on the Euclidean distance between the embedding Ψ
of the features, which integrates the original metric d(x, x(cid:48)).
Thus, the construction of the tree does not rely directly on the
high-dimensional features but on the low-dimensional geomet-
ric representation underlying the data (see [10] for a detailed
description). The quality of this representation, and therefore,
of the constructed tree depends on the metric d(x, x(cid:48)). In our
approach, we propose to use the metric induced by the tree
on the observations d(x, x(cid:48)) = dTY (x, x(cid:48)). This introduces a
coupling between the observations and the features, as the
tree construction of one depends on the tree of the other.
Since our approach is based on an iterative procedure, the
tree construction is reﬁned from iteration to iteration, as both
the tree and the metric on the features are updated based
on the organization of the observations, and vice versa. This
also updates the afﬁnity kernel between observations and the
afﬁnity kernel between features, therefore updating the dual
graph structure of the dataset.

Note that while we apply ﬂexible trees in our experimental
results, the bi-organization approach is modular and different
tree construction algorithms can be applied, as in [9], [30].

While the deﬁnition of the proposed transforms and metrics
does not depend on properties of the ﬂexible trees algorithm,
the resulting bi-organization does depend on the tree con-
struction. Spin-cycling (averaging results over multiple trees)
as in [10] can be applied to stabilize the results. Instead,
we propose an iterative reﬁnement procedure that makes the
tree constructions.
algorithm less dependent on the initial
Convergence guarantees to smooth results from a family of
appropriate initial trees are lacking. This will be the subject
of future work.

III. TREE TRANSFORMS

Given partition trees TX and TY , deﬁned on the features
and observations, respectively, we propose several transforms
induced by the partition trees, which are deﬁned by a linear
transform matrix and generalizes the method proposed in [10].
In the following we focus on the feature set X , but the same
deﬁnitions and constructions apply to the observation set Y.
Note that while the proposed transforms are linear, the support
of the transform elements is derived in a non-linear manner
as it depends on the tree construction.

i.e.

The proposed transforms project the data onto a high di-
mensional space whose dimensionality is equal to the number
of folders in the tree, denoted by NX ,
the transform
maps T : RnX → RNX . Each transform is represented as
a matrix of size NX × nX , where nX is the number of
features. We denote the row indices of the transform matrices
by i, j ∈ {1, 2, ..., NX } indicating the unique index of the
folder in TX . We denote the column indices of the transform
matrices by x, x(cid:48) ∈ X (y, y(cid:48) ∈ Y), which are the indices
of the features (observations) in the data. We deﬁne 1I to
be the indicator function on the features x ∈ {1, ..., nX }
belonging to folder I ∈ TX . Tree transforms obtained from
TX are applied to the dataset as ˆZX = TX Z and tree
transforms obtained from TY are applied to the dataset as
ˆZY = ZTT
Y . We begin with transforms induced by a tree in
a single dimension (features or observations) analogously to a
typical one-dimensional linear transform. We then extend these
transforms to joint-tree transforms induced by a pair of trees
{TX , TY } on the observations and the features, analogously to
a two-dimensional linear transform. Finally, we propose multi-
tree transforms in the case that we have more than one tree
in a single dimension, for example we have constructed a set
of trees {TX } on the features X , each constructed from a
different dataset consisting of different observations with the
same features.

A. Averaging transform

4

(5)

(6)

Applying S to an observation vector y ∈ RnX yields a vector
of length NX where each element i ∈ {1, ..., NX } is the sum
of the elements y(x) for x ∈ Ii:

(Sy)[i] =

y(x)1Ii(x) =

y(x)

(4)

(cid:88)

x∈X

(cid:88)

x∈Ii

The sum of each row of S is the size of its corresponding
folder: (cid:80)
x S[i, x] = |Ii|. The sum of each column is the
number of levels in TX : (cid:80)
i S[i, x] = L + 1, since the folders
are disjoint at each level such that each feature belongs only
to a single folder at each level.

From S we derive the averaging transform denoted by M.
Let D ∈ RNX ×NX be a diagonal matrix whose elements are
the cardinality of each folder: D[i, i] = |Ii|. We calculate
M ∈ RNX ×nX by normalizing the rows of S, so the sum of
each row is 1:

M = D−1S.

Thus, the rows i of M are uniformly weighted indicators on
the indices of X for each folder Ii:

M[i, x] =

1Ii(x) =

1
|Ii|

(cid:26) 1

|Ii| , x ∈ Ii
0,
o.w.

Note that the matrix S and the averaging transform M share
the same structure, i.e. they differ only in the value of the their
non-zero elements.

Alternatively if we denote by m(y, I) the average value of

y(x) in folder I:

m(y, I) =

y(x),

(7)

1
|I|

(cid:88)

x∈I

then applying the averaging transform M to y yields a vector
ˆy of length NX such that each element i is the average value
of y in folder Ii

(7):

ˆy[i] = (My)[i] = m(y, Ii), 1 ≤ i ≤ NX .

(8)

The averaging transform reinterprets each folder in the tree
as applying a uniform averaging ﬁlter, whose support depends
on the size of the folder. Applying the feature-based transform
MX to the dataset Z yields ˆZX = MX Z ∈ RNX ×nY , a data-
driven multi-scale representation of the data. As opposed to
a multiscale representation deﬁned on a regular grid, here
the representation at each level
is obtained via non-local
averaging of the coefﬁcients from the level below. The ﬁnest
level of the representation is the data itself, which is then
averaged in increasing degree of coarseness and in a non-
local manner according to the clusters deﬁned by the hierarchy
in the partition tree. The columns of ˆZX are the multiscale
representation ˆy of each observation y. The rows of ˆZX are the
centroids of the folders I ∈ TX and can be seen as multiscale
meta-features of length nY :

Ci(y) =

M[i, x]Z[x, y], 1 ≤ y ≤ nY .

(9)

(cid:88)

x

Let S be an NX × nX matrix representing the structure of
a given tree TX , by having each row i of the matrix be the
indicator function of the corresponding folder Ii ∈ TX :

S[i, x] = 1Ii(x) =

(cid:26) 1,

x ∈ Ii
0, otherwise

In a similar fashion denote by ˆZY = ZMT
Y the application
of the observation-based transform to the entire dataset. For
additional properties of S and M see [31].

In Fig. 1, we display an illustration of a partition tree

(3)

5

(a) Partition tree T . (b) Averaging transform matrix M induced by
Fig. 1.
the tree and applied to column vector y(x). The color of the elements in the
output correspond to the color of the folders in the tree.

and the resulting averaging transform. Fig. 1(a) is a partition
tree TX constructed on X where nX = 8. Fig. 1(b) is the
averaging transform M corresponding to the partition tree
TX . For visualization purposes we construct M as having
columns whose order correspond to the leaves of the tree TX
(level 0). This reordering also needs to be applied to the data
vectors y, and is essentially one of the aims of our approach.
The lower part of the transform is just the identity matrix,
as it corresponds to the leaves of the tree. The number of
rows in the transform matrix is NX = |T | = 14, as the
number of folders in the tree. The transform is applied to
a (reordered) column y ∈ R8, yielding the coefﬁcient vector
ˆy = My ∈ R14. The coefﬁcients are colored according to the
corresponding folders in the tree.

To further demonstrate and visualize the transform, we
apply the averaging transform to an image in Fig. 2. We
treat a grayscale image as a high-dimensional dataset where
X is the set of rows and Y is the set of columns. We
calculate a partition tree TY on the columns. We then calculate
the averaging transform and apply it to the image yielding
ˆZY = ZMT
Y . The result is presented in Fig. 2(a). Each row
x has now been extended to a higher dimension NY , where
we separate the levels of the tree with colored borders lines
for visualization purposes. Each of the columns ˆZY is the
centroid of folder I in the tree. The right-most sub-matrix is
the original image and as we move left we have coarser and
coarser scales. The averaging is non-local and the folder sizes
vary, respecting the structure of the data. Thus on the second
level of the tree, the building on the right is more densely
compressed compared to the building on the left.

Fig. 2. Application of the averaging transform (a) and the difference transform
(b) to an image. The color of the border represents the level of the tree. The
non-local nature of the transforms and the varying support is apparent, for
example, in the building on the right. In the ﬁne-scale resolution the building
has 7 windows in the horizontal direction, which have been compressed into
5 windows on the next level.

B. Difference transform

The goal of our approach is to organize the data in nested
folders in which the features and the observations are smooth.
Thus, it is of value to determine how smooth is the hierarchical
structure of the tree, i.e. does the merging of folders on one
level into a single folder on the next level preserve smoothness.
Let ∆ be an NX × nX matrix, termed the multiscale differ-
ence transform. This transform yields the difference between
ˆy[i] and ˆy[j] where j is the index of the immediate parent of
folder i.

The matrix ∆ is obtained from the averaging matrix M as:

∆[i, x] = M[i, x] − M[j, x], Il,i ⊂ Il+1,j.

(10)

Applying ∆ to observation y yields a vector of length NX
whose element i is the difference between the average value
of y in folder Il,i and the average value in its immediate parent
folder Il+1,j:

(∆y)[i] =

(cid:26) m(y, IL,1),

Ii = IL,1
m(y, Il,i) − m(y, Il+1,j), Il,i ⊂ Il+1,j,

(11)
where for the root folder, we deﬁne (∆y)[i] to be the average
over all features. This choice leads to the deﬁnition of an
inverse transform below. Thus, the rows i of ∆ are given by:

∆[i, x] =





1
|Ii| ,
|Il,i| − 1
1
− 1
|Il+1,j | ,
0,

|Il+1,j | ,

Ii = IL,1
x ∈ Il,i ⊂ Il+1,j
x /∈ Il,i ⊂ Il+1,j, x ∈ Il+1,j
x /∈ Il,i, x /∈ Il+1,j

and the sum of the rows of ∆:

∆[i, x] =

(cid:26) 1, Ii = IL,1
otherwise

0,

(12)

(13)

The difference transform can be seen as revealing “edges”
in the data, however these edges are non-local. Since the
tree groups features together based on their similarity and
not based on their adjacency, the difference between folders
is not restricted to the given ordering of the features. This
demonstrated in Fig. 2(b) where the difference transform of
the column tree has been applied to the 2D image as Z∆T
Y .
Theorem 2: The data can be recovered from the difference

transform by:

y = ST (∆y)

(14)

Proof: An element (ST ∆y)[x] is given by
(cid:88)

1Ii(x) (m(y, Il,i) − m(y, Il+1,j)) + m(y, IL,1) =

Il,i∈TX
Il,i⊂Il+1,j
l<L

(cid:88)

=
Il,i∈TX
0≤l≤L

=

n(0)
(cid:88)

i=1

1Ii(x)m(y, Il,i) −

1Ii(x)m(y, Il,i) =

(15)

(cid:88)

Il,i∈TX
1≤l≤L

1Ii(x)m(y, I0,i) = y(x) (cid:3)

The ﬁrst equality is due to the folders on each level being
disjoint such that if x ∈ Il,i and Il,i ⊂ Il+1,j then x ∈
Il+1,j, and Il+1,j is the only folder containing x on level
l + 1. This enables us to process the data in the tree-based
transform domain and then reconstruct by:

ˆy = ST f (∆y),

(16)

X → RN

where f : RN
X is a function in the domain of the
tree folders. For example, we can threshold coefﬁcients based
on their energy or the size of their corresponding folder. This
scheme can be applied to denoising and compression of graphs
or matrix completion [18]–[21], however this is beyond the
scope of this paper and will be explored in future work.

Note that the difference transform differs from the tree-
based Haar-like basis introduced in [13]. The Haar-like basis
is an orthonormal basis spanned by nX vectors derived from
the tree by an orthogonalization procedure. The difference
transform is overcomplete and spanned by NX vectors, whose
construction does not require an orthogonalization procedure,
making it simpler to compute. Also, as each vector corre-
sponds to a single folder, it enables us to deﬁne a measure of
the homogeneity of a speciﬁc folder compared to its parent.

C. Joint-tree transforms

Given the matrix Z on X × Y, and the respective partition
trees TX and TY , we deﬁne joint-tree transforms that operate
on the features and observations of Z simultaneously. This is
analogous to typical 2D transforms. The joint-tree averaging
transform is applied as

ˆZX ,Y = MX ZMT
Y .

(17)

The resulting matrix of size NX × NY provides a multiscale
representation of the data matrix, admitting a block-like struc-
ture corresponding to the folders in both trees. On the ﬁnest
level we have Z and then on coarser and coarser scales we have
smoothed versions of Z, where the averaging is performed

6

Fig. 3.
difference transform applied to image.

(a) Joint-tree averaging transform applied to image. (b) Joint-tree

under the joint folders at each level. The coarsest level is of
size 1 × 1 corresponding to the joint root folder. This matrix is
analogous to a 2D Gaussian pyramid representation of the data,
popular in image processing [24]. However, as opposed to the
2D Gaussian pyramid in which each level is a reduction of both
dimensions, applying our transform yields all combinations
of ﬁne and coarse scales in both dimensions. The joint-tree
averaging transform yields a result similar to the directional
pyramids introduced in [32], however the “blur” and “sub-
sample” operations in our case are data-driven and non-local.
The joint-tree difference transform is applied as ∆X Z∆T
Y .
This matrix is analogous to a 2D Laplacian pyramid represen-
tation of the data, revealing “edges” in the data. As in applying
a 1D transform, the data can be recovered from the joint-tree
difference transform as Z = ST

X ∆X Z∆T

Y SY .

Figure 3 presents applying the joint-tree averaging transform
and joint-tree difference transform to the 2D image. Within the
red border we display “zooming in” on level l ≥ 1 in both
trees TX and TY .

D. Multi-tree transforms

At each level of the partition tree, the folders are grouped
into disjoint sets. A limitation of using partition trees, there-
fore, is that each folder is connected to a single “parent”.
However, it can be beneﬁcial to enable a folder on one level
to participate in several folders at the level above, such that
folders overlap, as in [33]. We propose an approach that
enables overlapping folders in the bi-organization framework
by constructing more than one tree on the features X , and
we extend the single tree transforms to multi-tree transforms.
This generalizes the partition tree such that each folder can be
connected to more than one folder in the above level, i.e. this
is no longer a tree because it is now cyclic but still a bipartite
graph. Note that in contrast to the joint-tree transform, which
incorporates a joint pair of trees over both the features and the
observations, here we are referring to a set of trees deﬁned for
only the features, or only the observations.

t |Tt|. Yet since all trees {Tt}nT

Given a set of nT different partition trees on X , denoted
{Tt}nT
t=1, we construct the multi-tree averaging transform. Let
(cid:102)MX be an (cid:101)NX × nX matrix, constructed by concatenation of
the averaging transform matrices MT induced by each of the
trees {Tt}nT
t=1. The number of rows in the multi-tree transform
matrix is denoted by (cid:101)NX and equal to the number of folders in
all of the trees (cid:80)
t=1 contain the
same root and leaves folders, we remove the multiple appear-
ance of the rows corresponding to these folders and include
them only once (then (cid:101)NX = (cid:80)
t |Tt| − (nT − 1)(1 + nX )).
Thus, the matrix of the multi-tree averaging transform now
represents a decomposition via a single root, a single set
of leaves and many multiscale folders that are no longer
disjoint. This implies that
instead of considering multiple
“independent” trees, we have a single hierarchical graph where
at each level we do not have disjoint folders, as in a tree, but
instead overlapping folders. In Sec. IV-C, we derive from these
transforms a new multi-tree metric. For additional properties
of the multi-tree transform see [31].

Ram, Elad and Cohen [26] also proposed a “generalized tree
transform” where folders are connected to multiple parents in
the level above, however their work differs in two aspects.
First, their proposed tree construction is a binary tree, whereas
ours admits general tree constructions. Second, their transform
relies on classic pre-determined wavelet ﬁlters such that the
support of the ﬁlter is ﬁxed across the dataset. Our formu-
lation on the other hand introduces data-driven ﬁlters whose
support is determined by the size of the folder, which can
vary across the tree. The Multiresolution Matrix Factorization
(MMF) [27] also yields a wavelet basis on graphs. MMF
uncovers a hierarchical organization of the graph that permits
overlapping clusters, by decomposition of a graph Laplacian
matrix via a sequence of sparse orthogonal matrices. However,
our transform is derived from a set of multiple hierarchical
trees, whereas their hierarchical structure is derived from the
wavelet transform.

The ﬁeld of community detection also addresses ﬁnd-
ing overlapping clusters in graphs [34]. Ahn, Bagrow and
Lehmann [33] construct multiscale overlapping clusters on
graphs by performing hierarchical clustering with a similarity
between edges of a graph, instead of its nodes. Their approach
focuses on the explicit construction of the hierarchy of the
overlapping clusters, whereas our focus is on employing a
transform and a metric derived from such a multiscale over-
lapping organization of the features. In contrast to clustering,
our approach allows for the organization and analysis of the
observations.

IV. TREE-BASED METRIC

The success of the data organization and the reﬁnement of
the partition trees depends on the metric used to construct
the trees. We assume that a good organization of the data
recovers smooth joint clusters of observations and features.
Therefore, a metric for comparing pairs of observations should
not only compare their values for individual features (as in
the Euclidean distance), but also across clusters of features,
which are expected to have similar values. Thus, we present a

7

metric dT in the multiscale representation yielded by the tree
transforms. Using this metric, the construction of the tree on
the features takes into account the structure of the underlying
graph on the observations as represented by its partition tree.
The partition tree on the observations in turn relies on the
graph structure of the features. In each iteration a new tree
is calculated based on the metric from the previous iteration,
and then a new metric is calculated based on the new tree.
This can be seen as updating the dual graph structure of the
data in each iteration. The iterative bi-organization algorithm
is presented in Alg. 1.

A. Tree-based EMD

Coifman and Leeb [25] deﬁne a tree-based metric approxi-
mating the EMD in the setting of hierarchical partition trees.
Given a 2D matrix Z, equipped with a partition tree on the
features TX , consider two observations y, y(cid:48) ∈ Y. The tree-
based metric between the observations is deﬁned as

dTX (y, y(cid:48)) =

|m(y − y(cid:48), I)|,

(18)

(cid:19)β

(cid:18) |I|
nX

(cid:88)

I∈TX

where β is a parameter that weights the folders in the tree
based on their size. Following our formulation of the trees
inducing linear transforms, this tree-based metric can be seen
as a weighted l1 distance in the space of the averaging
transform.

Theorem 3: [23, Theorem 4.1] Given a partition tree on
the features TX , deﬁne the NX × NX diagonal weight matrix
W[i, i] =
. Then the tree metric (18) between two
observations y, y(cid:48) ∈ RnX is equivalent to the weighted l1
distance between the averaging transform coefﬁcients:

(cid:16) |Ii|
nX

(cid:17)β

dTX (y, y(cid:48)) = (cid:107)W(ˆy − ˆy(cid:48))(cid:107)1.

(19)

Proof: An element of the vector W(ˆy − ˆy(cid:48)) is

(WM(y − y(cid:48)))[i] =

W[i, j](M(y − y(cid:48)))[j]

(cid:88)

j

= W[i, i](M(y − y(cid:48)))[i]

(20)

(cid:19)β

=

(cid:18) |Ii|
nX

m(y − y(cid:48), Ii).

Therefore:

(cid:107)W(ˆy − ˆy(cid:48))(cid:107)1 =

|m(y − y(cid:48), I)| (cid:3)

(21)

(cid:19)β

(cid:18) |Ii|
nX

(cid:88)

I∈T

Note that the proposed metric is equivalent to the l1 distance
between vectors of higher-dimensionality than the original di-
mension of the vectors. However, by weighting the coefﬁcients
with W, the effective dimension of the new vectors is typically
smaller than the original dimensionality, as the weights rapidly
decrease to zero based on the folder size and the choice
of β. For positive values of β, the entries corresponding to
the large folders dominate ˆy, while entries corresponding to
small folders tend to zero. This trend is reversed for negative
values of β, with elements corresponding to small folders
dominating ˆy while large folders are suppressed. In both cases,
a threshold can be applied to ˆy or ˆZ so as to discard entries

with low absolute values. Thus, the transforms project the
data onto a low-dimensional space of either coarse or ﬁne
structures. Also, note that interpreting the metric as the l1
distance in the averaging transform space enables us to apply
approximate nearest-neighbor search algorithms suitable for
the l1 distance [35], [36]. This allows to analyze larger datasets
via a sparse afﬁnity matrix.

Deﬁning the metric in the transform space enables us to
easily generalize the metric to a joint-tree metric deﬁned for a
joint pair of trees {TX , TY } (Sec. IV-B), to incorporate several
trees over the features {TX }nT in a multi-tree metric via the
multi-tree transform (Sec. IV-C), and to seamlessly introduce
weights on the transform coefﬁcients by setting the elements
of W (Sec. IV-E). Python code implementing our approach is
available at [37].

B. Joint-tree Metric

The tree-based transforms and metrics can be generalized to
analyzing rank-n tensor datasets. We brieﬂy present the joint-
tree metric to demonstrate that the proposed transforms are
not limited to just 2D matrices, but rather can be extended
to processing and organizing tensor datasets. An example of
such an application was presented in [23].

In [23] we proposed a 2D metric given a pair of partition
trees in the setting of organizing a rank-3 tensor. We reformu-
late this metric in the transform space by generalizing the tree-
based metric to a joint-tree metric using the coefﬁcients of the
joint-tree transform. Given a partition tree TX on the features
and a partition tree TY on the observations,
the distance
between two matrices Z1 and Z2 is deﬁned as

dTX ,TY (Z1, Z2) =

|m(Z1 − Z2, I × J )|

|I|βX |J |βY
nβX
βY
X nY

.

(cid:88)

I∈TX
J ∈TY

(22)
The value m(Z, I × J ) is the mean value of a matrix Z on
the joint folder I × J = {(x, y) | x ∈ I, y ∈ J }:

m(Z, I × J ) =

Z[x, y].

(23)

1
|I||J |

(cid:88)

x∈I,y∈J

Theorem 3 can be generalized to a 2D transform applied to
2D matrices.

Corollary 4: [23, Corollary 4.2] The joint-tree metric (22)
between two matrices given a partition tree TX on the features
and a partition tree TY on the observations is equivalent to the
l1 distance between the weighted 2D multiscale transform of
the two matrices:

dTX ,TY (Z1, Z2) = (cid:107)WX MX (Z1 − Z2)MT

YWY (cid:107)1.

(24)

C. Multi-tree Metric

The deﬁnition of the metric in the transform domain enables
a simple extension to a metric derived from a multi-tree
composition. Given a set of multiple trees {Tt}nT
t=1 deﬁned on
the features X as in Sec. III-D, we deﬁne a multi-tree metric
using the multi-tree averaging tree transform as:

d{T }(y, y(cid:48)) = (cid:107) (cid:102)W(cid:102)M(y − y(cid:48))(cid:107)1,

(25)

8

(cid:17)β

(cid:16) |Ii|
nX

where (cid:102)W is a diagonal matrix whose elements are
for all I ∈ T and for all trees in {Tt}nT
equivalent to averaging the single tree metrics:

t=1. This metric is

(cid:88)

d{T }(y, y(cid:48)) = (cid:107) (cid:102)W(cid:102)M(y − y(cid:48))(cid:107)1
1
nT
1
nT

dT (y, y(cid:48)).

T
(cid:88)

=

=

T

(cid:107)WT MT (y − y(cid:48))(cid:107)1

(26)

Note that in contrast to the joint-tree metric, which incorpo-
rates a pair of trees over both the features and the observations,
here we are referring to a set of trees deﬁned only for the
features, or only for the observations.

A question that arises is how to construct multiple trees? For
matrix denoising in a bi-organization setting, Ankenman [10]
applies a spin-cycling procedure: constructing many trees
by randomly varying the parameters in the partition tree
construction algorithm. Multiple trees can also be obtained by
initializing the bi-organization with different metric choices
for d(0)
X (x, x(cid:48)) (step 2 in Alg. 1), e.g., Euclidean, correlation,
etc. Another option, which we demonstrate experimentally on
real data in Sec. V, arises when we have multiple data sets
of observations with the same set of features, or multiple data
sets with the same observations but different features as in
multi-modal settings. In such cases, we construct a partition
tree for each dataset separately and then combine them using
the multi-tree metric.

D. Local Reﬁnement

We propose a new approach to constructing multiple trees,
leveraging the partition of the data during the bi-organization
procedure. This approach is based on a local reﬁnement of
the partition trees, which results in a smoother organization
of the data. The bi-organization method is effective when
correlations exist among both observations and features, by
revealing a hierarchical organization that is meaningful for all
the data together. Yet, since the bi-organization approach is
global and takes all observations and all features into account,
it needs to achieve the best organization on average. However,
the correlations between features may differ among sub-
populations in the data, i.e. the correlations between features
depend on the set of observations taken into account (and vice-
versa).

For example, consider a dataset of documents where the ob-
servations Y are documents belonging to different categories,
the features X are words and Z(x, y) indicates whether a
document y contained a word x. Grouping the words into
disjoint folders forces a single partition of the vocabulary that
disregards words that belong to more than one conceptual
group of words. These connections could be revealed by taking
into account the context, i.e. the subject of the documents.
By diving the documents into a few contextual clusters, and
calculating a local tree on the words TX for each such cluster,
the words are grouped together conceptually according to
the local category. The word “ﬁeld” for example will be
joined with different neighbors, depending on whether the

Algorithm 1 Bi-organization Algorithm [10, Sec. 5.3]
Initialization
Input Dataset Z of features X and observations Y

X , weight function on tree

1: Starting with features X
2:

Calculate initial metric d(0)
X (x, x(cid:48))
Calculate initial ﬂexible tree T (0)
X .

3:

Iterative analysis
Input Flexible tree on features T (0)

folders W[i, i] = ω(Ii)

4: for n ≥ 1 do
5:

X , calculate tree metric between obser-
(y, y(cid:48)) = (cid:107)WX MX (y − y(cid:48))(cid:107)1

Given tree T (n)
vations d(n)
TX
Calculate ﬂexible tree on the observations T (n)
Y .
Repeat steps 5-6 for the features X given T (n)
obtain T (n+1)

and

Y

.

X

8: end for

Algorithm 2 Bi-organization local reﬁnement
Input Dataset Z, observation tree TY

1: Choose level l in tree TY
2: for j ∈ {1, ..., n(l)} do
3:
4:

Set ω(Ji) = 1 ∀Ji ⊆ Jl,j, otherwise ω(Ji) = 0.
Calculate initial afﬁnity on features for subset of
observations as weighted tree-metric d(0)(x, x(cid:48)) =
dTY (x, x(cid:48); ω(Jj))
Calculate initial ﬂexible tree on features T (0)
Perform iterative analysis (steps 4-8 in Alg. 1) for Z
on X and (cid:101)Y = {y | y ∈ J ∈ TY }.

X

7: end for
8: Merge observation trees {T

}n(l) back into global tree

(cid:101)Yj

Output Reﬁned observation tree TY , Set of feature trees

TY

{TX }n(l)
i=1

6:

7:

5:
6:

analysis is applied to documents belonging to “agriculture”,
“mathematics” or “sports”.

Therefore, we propose to take advantage of the unsupervised
clustering obtained by the partition tree on the observations
TY , and apply a localized bi-organization to folders of ob-
servations. Formally, we apply the bi-organization algorithm
to a subset of Z containing all features X and a subset of
observations belonging to the same folder (cid:101)Y = {y | y ∈ J ∈
TY }. This local bi-organization results in a pair of trees: a
local tree T
(cid:101)Y organizing the subset of observations (cid:101)Y, and
a feature tree TX that organizes all the features X based on
this subset of observations that share the same local structure,
rather than the global structure of the data. This reveals the
correlations between features for this sub-population of the
data, and provides a localized visualization and exploratory
analysis for subsets of the data discovered in an unsupervised
manner. This is meaningful when the data is unbalanced and a
subset of the data differs drastically from the rest of the data,
e.g., due to anomalies.

We propose a local reﬁnement of the bi-organization as
follows. We select a single layer l of the observations tree

9

(cid:101)Yj

TY , and perform a separate localized organization for each
folder Jl,j ∈ Pl,
j ∈ {1, ..., n(l)}. We thus obtain n(l)
}n(l)
local observation trees {T
j=1, which we then merge back
into one global tree, with reﬁned partitioning. Merging is
performed by replacing the branch in TY whose root is Jl,j,
i.e. {J ∈ TY |J ⊆ Jl,j}, with the local observation tree T
.
(cid:101)Yj
In addition, we obtain a set of several corresponding trees on
the full set of features {TX }n(l), which we can use to calculate
a multi-tree metric (25). Our local reﬁnement algorithm is
presented in Alg. 2. Applying this algorithm to reﬁne the
global structures of both TY and TX results in a smoother
bi-organization of the data.

We typically apply the reﬁnement to a high level of the tree
since at these levels large clusters of distinct sub-populations
are grouped together, and their separate analysis will reveal
their local organization. The level can be chosen by applying
the difference transform and selecting a level at which the
folders grouped together are heterogeneous, i.e. their mean
signiﬁcantly differs from the mean of their parent folder.

Note that this approach is unsupervised and relies on the
data-driven organization of the data. However, this approach
can also be used in a supervised setting, when there are labels
on the observations. Then we calculate a different partition
tree on the features for each separate label (or sets of labels)
of the observations, revealing the hierarchical structure of the
features for each label. This will be explored in future work.

E. Weight Selection

The calculation of the metric depends on the weight attached
to each folder. We generalize the metric such that the weight
is W[i, i] = ω(Ii), where ω(Ii) > 0 is a weight function
associated with folder Ii. The weights can incorporate prior
smoothness assumptions on the data, and also enable to en-
hance either coarse or ﬁne structures in the similarity between
samples.

(cid:17)β

(cid:16) |Ii|
nX

The choice ω(Ii) =

in [25] makes the tree-based
metric (18) equivalent to EMD, i.e., the ratio of EMD to
the tree-based metric is always between two constants. The
parameter β weights the folder by its relative size in the tree,
where β > 0 emphasizes coarser scales of the data, while
β < 0 emphasizes differences in ﬁne structures.

Ankenman [10] proposed a slight variation to the weight

also encompassing the tree structure:

ω(Ii) = 2−αl(Ii)

(cid:18) |Ii|
nX

(cid:19)β

,

(27)

where α is a constant and l(Ii) is the level at which the folder
Ii is found in T . The constant α weights all folders in a
given level equally. Choosing α = 0 resorts to the original
weight. The structure of the trees can be seen as an analogue
to a frequency decomposition in signal processing, where
the support of a folder is analogous to a certain frequency.
Moreover, since high levels of the tree typically contain large
folders, they correspond to low-pass ﬁlters. Conversely, lower
levels of the tree correspond to high-pass ﬁlters as they contain
many small folders. Thus setting α > 0 corresponds to
emphasizing low frequencies whereas α < 0 corresponds to

10

enhancing high frequencies. In an unbalanced tree, where a
small folder of features remains separate for all levels of the
tree (an anomalous cluster of features), α can be used to
enhance the importance of this folder, as opposed to β, which
would decrease its importance based on its size.

We propose a different approach. Instead of weighting the
folders based on the structure of the tree, which requires a-
priori assumptions on the optimal scale of the features or the
observations, we set the folders weights based on their content.
By applying the difference transform to the data, we obtain a
measure for each folder deﬁning how homogeneous it is. This
reduces the number of parameters in the algorithm, which is
advantageous in the unsupervised problem of bi-organization.
We calculate for each folder, the norm of its difference on the
dataset Z:
(cid:32)

(cid:33)1/2

ω(Ii) =

((∆X Z)[i, y])2

(cid:88)

y



(cid:32)

(cid:88)

(cid:88)

=



y

x

(m(y(x), Il,i) − m(y(x), Il+1,j)

1/2

(cid:33)2


,

(28)
where Il,i ⊂ Il+1,j. This weight is high when Il,i (cid:28) Il+1,j.
This means that the parent folder joining Il,i with other folders
contains non-homogeneous “populations”. Therefore, assign-
ing a high weight to Il,i places importance on differentiating
these different populations.

The localized reﬁnement procedure in Alg. 2 can also be
formalized as assigning weights ω(I) in the tree metric. We
set all weights containing a branch of the tree (a folder and
all its sub-folders) to 1 and set all other weights to zero:
(cid:26) 1,
0,

Ii ⊆ Ij
otherwise,

ω(Ii) =

(29)

where Ij is the root folder of the branch. Thus, using these
weights, the metric is calculated based only on a subset of the
observations ˜Y. This metric can initialize a bi-organization
procedure of a subset of Z containing X and ˜Y.

F. Coherence

To assess the smoothness of the bi-organization stemming
from the constructed partition trees, a coherency criterion was
proposed in [9]. The coherency criterion is given by

C(Z; TX , TY ) =

(cid:107)ΨX ZΨT

Y (cid:107)1,

(30)

1
nX nY

where Ψ is a Haar-like orthonormal basis proposed by Gavish,
Nadler and Coifman [13] in the settings of partition trees,
and it depends on the structure of a given tree. This criterion
measures the decomposition of the data in a bi-Haar-like basis
induced by two partition trees TX and TY : ΨX ZΨT
Y . The
lower the value of C(Z; TX , TY ), the smoother the organiza-
tion is in terms of satisfying the mixed H¨older condition (1).
Minimizing the coherence can be used as a stopping con-
dition for the bi-organization algorithm presented in Alg. 1.
The bi-organization continues as long as C(Z; T (n)
Y ) <
C(Z; T (n−1)
) [9]. However, we have empirically
X

X , T (n)

, T (n−1)
Y

found that the iterative process typically converges within
only few iterations. Therefore, in our experimental results we
perform n = 2 iterations.

V. EXPERIMENTAL RESULTS

Analysis of cancer gene expression data is of critical impor-
tance in jointly identifying subtypes of cancerous tumors and
genes that can distinguish the subtypes or indicate a patient’s
long-term survival. Identifying a patient’s tumor subtype can
determine the course of treatment, such as recommendation of
hormone therapy in some subtypes of breast cancer, and is a
an important step toward the goal of personalized medicine.
Biclustering of breast cancer data has identiﬁed sets of genes
whose expression levels categorize tumors into ﬁve subtypes
with distinct survival outcomes [38]: Luminal A, Luminal
B, Triple negative/basal-like, HER2 type and “Normal-like”.
Related work has aimed to classify samples into each of
these subtypes or identify other types of signiﬁcant clusters
based on gene expression, clinical features and DNA copy
number analysis [39]–[42]. The clustered dendrogram obtained
by agglomerative hierarchical clustering of the genes and the
subjects is widely used in the analysis of gene expression data.
However, in contrast to our approach, hierarchical clustering
is usually applied with a metric, such as correlation, that
is global and linear, and does not
the
structure revealed by the multiscale tree structure of the other
dimension. Conversely, our approach enables us to iteratively
update both the tree and metric of the subjects based on the
metric for the genes, and update the tree and metric of the
genes based on the metric for the subjects.

take into account

We analyze three breast cancer gene expression datasets,
where the features are the genes and the observations are the
tumor samples. The ﬁrst dataset is the METABRIC dataset,
containing gene expression data for 1981 breast tumors [40]
collected with a gene expression microarray. We denote this
dataset ZM, and its set of samples YM. The second dataset,
ZT, is taken from The Cancer Genome Atlas (TCGA) Breast
Cancer cohort [43] and consists of 1218 samples, YT. This
dataset was proﬁled using RNA sequencing, which is a newer
and more advanced gene expression technology. The third
dataset ZB (BRCA-547) [41], comprising of 547 samples YB,
was acquired with microarray technology. These 547 samples
are also included in the TCGA cohort, but the gene expression
was proﬁled using a different technology.

We selected X to be the 2000 genes with the largest variance
in METABRIC from the original collection of ∼ 40000 gene
probes. In related work, the analyzed genes were selected in
a supervised manner based on prior knowledge or statistical
signiﬁcance in relation to patient survival
time [38]–[40],
[42], [44]. Here we present results of a purely unsupervised
approach aimed at exploratory analysis of high-dimensional
data, and we do not use the survival information or subtypes
labels in either applying our analysis or for gene selection, but
only in evaluating the results. In the remainder of this section
we present three approaches in which the tree transforms
and metrics are applied for the purpose of unsupervised
organization of gene expression data.

11

Fig. 4. Global bi-organization of the METABRIC dataset. The samples (columns) and genes (rows) have been reordered so they correspond to the leaves
of the two partition trees. Below the organized data are clinical details for each of the samples: two types of breast cancer subtype labels (reﬁned [42] and
PAM50 [39]) hormone receptor status (ER, PR) and HER2 status.

Regarding implementation, in this application we use ﬂex-
the partition trees in the bi-
ible trees [10] to construct
organization. We initialize the bi-organization with a corre-
lation afﬁnity on the genes (d(0)
X (x, x(cid:48)) in Alg. 1, Step 2),
which is commonly used in gene expression analysis.

A. Subject Clustering

We begin with a global analysis of all samples of the
METABRIC data using the bi-organization algorithm pre-
sented in Alg. 1. We perform two iterations of the bi-
organization using the tree-based metric with the data-driven
weights deﬁned in (28). The organized data and corresponding
trees on the samples and on the genes are shown in Fig. 4.
The samples and genes have been reordered such that they
correspond to the leaves of the two partition trees. Below
the organized data we provide clinical details for each of
the samples: two types of breast cancer subtype labels, the
reﬁned labels introduced in [42] and the standard PAM50
subtypes [39], hormone receptor status (ER, PR) and HER2
status. We analyze the folders of level l = 5 on the samples
tree, which divides the samples into ﬁve clusters (the folders
are marked with numbered colored circles).

In Fig. 5 we present histograms of the reﬁned subtype labels
for each of the numbered folders in the samples tree, and
plot the disease-speciﬁc survival curve of each folder in the
bottom graph. The histograms of each folder is surrounded by
a colored border corresponding to the colored circle indicating

TABLE I
METABRIC SELF ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)
DTC [46]
sparseBC [12]

RI
0.79
0.72
0.72
0.69
0.74
0.72
0.74
0.76

ARI VI
0.45
0.30
0.23
0.20
0.30
0.26
0.19
0.33

1.48
1.77
1.98
1.94
1.84
1.90
2.45
1.74

p-value
4.35 × 10−21
1.11 × 10−17
8.48 × 10−10
1.46 × 10−12
1.11 × 10−16
5.23 × 10−11
5.54 × 10−22
2.6 × 10−19

the relevant folder in the tree in Fig. 4. Note that the folders do
not just separate data according to subtype as in the dark blue
and light blue folders (Basal and Her2 respectively), but also
separate data according to the survival rates. If we compare the
orange and green folders that are grouped in the same parent
folder, both contain a mixture of Luminal A and Luminal B,
yet they have distinctive survival curves. The p-value of this
separation using the log-rank test [45] was 4.35 × 10−21.

We next compare our weighted metric (28) to the original
EMD-like metric (18), using different values of β and α in
(27). These values were chosen in order to place different em-
phasis of the transform coefﬁcients depending on the support
of the corresponding folders or the level of the tree. The values
of β enable to emphasize large folders (β = 1), small folders

12

the variation of information (VI) [49]. The RI and ARI
measure the similarity between two clusterings (or partitions)
of the data. Both measures indicate no agreement between the
partitions by 0 and perfect agreement by 1, however ARI can
return negative values for certain pairs of clusterings. The third
measure is an information theoretic criterion, where 0 indicates
perfect agreement between two partitions. Finally, we perform
survival analysis using Kaplan-Meier estimate [50] of disease-
speciﬁc survival rates of the samples, reporting the p-value of
the log-rank test [45]. A brief description of these statistics is
provided in Appendix II.

We select clusters by partitioning the samples into the
folders J of the samples tree TX , at a single level l of the tree
which divides the data into 4-6 clusters (typically level L − 2
in our experiments). This follows the property of ﬂexible trees
that the level at which folders are joined is meaningful across
the entire dataset, as for each level the distances between
joined folders are similar. For other types of tree construction
algorithms, alternative methods can be used to select clusters
in the tree, such as SigClust used in [41].

Results are presented in Table I for the METABRIC dataset
and in Table II for the BRCA-547 dataset. For the METABRIC
dataset, using the weighted metric achieves the best results
compared to the other weight selections, in terms of both
clustering relative to the ground-truth labels and the survival
curves of the different clusters (note these two criteria do
not always coincide). While DTC achieves the lowest p-
value overall, it has very poor clustering results compared
to the ground-truth labels (lowest ARI and highest VI). The
weighted metric out-performed the sparseBC method, which
has second-best performance for the clustering measures, and
third-lowest p-value. For the BRCA-547 dataset, the weighted
metric achieves the best clustering in terms of the ARI measure
and has the lowest p-value. For the VI measure, the clustering
by the weighted metric was slightly larger but comparable
to that of the lowest score. On this dataset, DTC performed
poorly with highest VI and p-value. The sparseBC method
achieved good clustering with highest RI and ARI measures,
but had a high p-value and VI compared to the performance
of our bi-organization method.

The results indicate that the data-driven weighting achieves
comparable if not better performance, than both using the tree-
dependent weights and competing biclustering methods. Thus,
the data-driven weighting provides an automatic method to
set appropriate weights on the transform coefﬁcients in the
metric. Our method is completely data-driven, as opposed to
the sparseBC method which requires as input the number of
features and observations to decompose the data into. (We used
the provided computationally expensive cross-validation pro-
cedure to select the best number of clusters in each dimension).
In addition, our approach provides a multiscale organization,
whereas sparseBC yields a single-scale decomposition of the
data. The DTC is a multiscale approach, however as it relies
on hierarchical clustering it does not take into account the
dendrogram in the other dimension. The performance may
be improved by using dendrograms in our iterative approach,
instead of the ﬂexible trees (this is further discussed below).

Fig. 5. (top) Histograms of folders in sample tree of METABRIC. The color
of the border corresponds to the circles in the tree. (bottom) Survival curves
for each folder.

TABLE II
BRCA-547 SELF ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)
DTC [46]
sparseBC [12]

RI
0.75
0.75
0.74
0.72
0.74
0.74
0.75
0.76

ARI VI
0.38
0.37
0.36
0.35
0.34
0.35
0.35
0.38

1.38
1.39
1.37
1.33
1.56
1.45
1.63
1.49

p-value
0.0004
0.0073
0.0028
0.0773
0.0010
0.0130
0.0853
0.0269

(β = −1) and weighting all folders equally (β = 0). The
values of α either emphasize high levels of the tree (α = 0.5),
low levels of the tree (α = −1) or weighting all levels equally
(α = 0).

We also compare to two other biclustering methods. The
ﬁrst is the dynamic tree cutting (DTC) [46] applied to a hier-
archical clustering dendrogram obtained using mean linkage
and correlation distance (a popular choice in gene expression
analysis). The second is the sparse biclustering method [12],
where the authors impose a sparse regularization on the mean
values of the estimated biclsuters (assuming the mean of
the dataset
is zero). Both algorithms are implemented in
R: package dynamicTreeCut and package sparseBC,
respectively.

We evaluate our approach by both measuring how well the
obtained clusters represent the cancer subtypes, and estimating
the statistical signiﬁcance of the survival curves of the clusters.
We compare the clustering of the samples relative to the
reﬁned subtype labels [42] using three measures: the Rand
index (RI) [47], the adjusted Rand index (ARI) [48], and

13

TABLE III
COHERENCY OF REFINED BI-ORGANIZATION

TABLE IV
METABRIC EXTERNAL ORGANIZATION

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)

Global TX
and TY
0.7039
0.7066
0.7051
0.7028
0.7051
0.7075

Reﬁned
TX
0.6103
0.6107
0.6118
0.6130
0.6119
0.6141

Reﬁned
TY
0.5908
0.5928
0.5921
0.5972
0.5927
0.5934

Reﬁned
TX , TY
0.5463
0.5480
0.5472
0.5668
0.5487
0.5497

weighted
(α, β) = (0, 0)
(α, β) = (0, −1)
(α, β) = (0, 1)
(α, β) = (−1, 0)
(α, β) = (0.5, 0)

RI
0.74
0.73
0.72
0.73
0.72
0.73

ARI VI
0.30
0.29
0.26
0.28
0.27
0.25

1.77
1.87
1.87
1.83
1.89
1.98

p-value
3.71 × 10−19
7.78 × 10−16
1.77 × 10−16
4.25 × 10−14
7.02 × 10−6
3.33 × 10−16

B. Local reﬁnement

In Table III we demonstrate the improvement gained in the
organization by applying the local reﬁnement to the partition
trees, where we measure the smoothness of the organized data
using the coherency criterion (30). We perform bi-organization
for different values of β and α as well as the weighted
metric, and compare 4 organizations: 1) Global organization;
2) Reﬁned organization of only the genes tree TX ; 3) Reﬁned
organization of only the samples tree TY ; and 4) Reﬁned
organization of both the features and the samples (reﬁned TX
and TY ). Applying the reﬁned local organization to both the
genes and the samples, yields the best result with regard to
the smoothness of the bi-organization. We also examined the
the effect of the level of the tree on which the reﬁnement is
performed for l ∈ {5, 6, 7} for both trees, and the improvement
gained by reﬁnement was of the same order for all combina-
tions. The results demonstrate that regardless of the weighting
(data-driven or folder dependent), the reﬁnement procedure
improves the coherency of the organization.

C. Bi-organization with multiple datasets

Following the introduction of gene expression proﬁling by
RNA sequencing, an interesting scenario is that of two datasets
proﬁled using different technologies, one using microarray
and the other RNA sequencing. Consider, for example, the
METABRIC dataset ZM and the TCGA dataset ZT, which
share the same features X (in this case genes), but collected for
two different sample sets, YM and YT respectively. In this case,
the gene expression proﬁles have different dynamic range and
are normalized differently, and the samples cannot be analyzed
together simply by concatenating the datasets. However, the
hierarchical structure we learn on the genes, which deﬁnes a
multiscale clustering of the genes, is informative regardless of
the technique used to acquire the expression data.

Thus, the gene metric learned from one dataset can be
applied seamlessly to another dataset and used to organize
its samples due to the coupling between the genes and the
samples. We term this “external-organization”, and demon-
strate how it organizes the METABRIC dataset ZM using the
TCGA dataset ZT. We ﬁrst apply the bi-organization algorithm
to organize ZT, and then we derive the gene tree-based metric
dTX from the constructed tree on the genes TX . This metric
is then used to a construct a new tree TY on the samples set
YM of ZM.

In Table IV we compare the external organization of
METABRIC using our weighted metric to the original EMD-

like metric for different values of β and α. Our results
show that the data-driven weights achieve the best results,
reinforcing that learning the weights in a data-adaptive way
is more beneﬁcial than setting the weights based on the size
of the folders or the level of the tree. Applying external
organization enables us to assess which bi-organization of the
external dataset and corresponding learned metric were the
most meaningful. Note that for some of the parameter choices
(α = 0, β = 1 or β = −1), the external organization of
ZM using a gene tree learned from the dataset ZT was better
than the internal organization. Thus, via the organization of the
dataset ZM, we validate that the hierarchical organization of
the genes in ZT, and therefore, the corresponding metric, are
effective in clustering samples into cancer subtypes. This also
demonstrates that the hierarchical gene organization learned
from one dataset can be successfully applied to another dataset
to learn a meaningful sample organization, even though the
two were proﬁled using different technologies. This provides
motivation to integrate information from datasets together.

In our ﬁnal evaluation, we divide the METABRIC dataset
into its two original subsets: the discovery set comprising 997
tumors and the validation set comprising 995 tumors. Note
that the two sets have different sample distributions of cancer
subtypes. We compare three approaches for organizing the
data. We begin with the self-organization as in Sec. V-A.
We organize each of the two datasets separately and report
their clustering measures in the ﬁrst row in Table V for the
discovery cohort and in Table VI for the validation cohort.
Note that the organization achieved using half the data is
less meaningful in terms of the survival rates compared to
using all of the data. This is due to the different distribution
of subtypes and survival times between the discovery and
validation cohorts, and in addition, the p-value calculation
itself is dependent on the sample size used.

One of the important aspects in a practical application is the
ability to process new samples. Our approach naturally allows
for such a capability. Assume we have already performed bi-
organization on an existing dataset and we acquire a few new
test samples. Instead of having to reapply the bi-organization
procedure to all of the data, we can instead insert the new
samples into the existing organization. We demonstrate this
by using each subset of the METABRIC dataset to organize
the other. In contrast to the external organization example, here
we have two datasets proﬁled with the same technology. We
can treat this as a training and test set scenario: construct a
sample tree on the training set Ytrain and use the learned metric
on the genes dTX to insert samples from the test set Ytest into

the training sample tree TYtrain . First, we calculate the centroids
of the folders Jj of level l = 1 (the level above the leaves) in
the samples tree TYtrain:
(cid:88)

MY [j, y]Z[x, y], x ∈ {1, ..., nX },

l(Jj) = 1

Cj(x) =

y

(31)
These can be considered the representative sample of each
folder. We then assign each new sample y ∈ Ytest
to its
nearest centroid using the metric dTX (y, Cj) derived from
the gene tree TX . Thus, we reconstruct the sample hierarchy
on the test dataset Ytest by assigning each test sample to
the hierarchical clustering of the low-level centroids from the
training sample tree. This approach, therefore, validates the
sample organization as well as the gene organization, whereas
the external organization only enables to validate the gene
organization.

We perform this once treating the validation set as the
training set and the discovery set as the test set, and then vice-
versa. We report the clustering measures in the second row of
Table V and Table VI. Note that the measures are reported only
for the samples belonging to the given set in the table. Inserting
samples from one dataset into the sample tree of another
demonstrates an improved organization in some measures
compared to performing self-organization. For example, the
organization of the discovery set via the validation tree results
in a clustering with improved ARI and VI measures. This
serves as additional evidence for the importance of integrating
information from several datasets together.

Thus far in our experiments, we have gathered substantial
evidence for the importance of information stemming from
multiple data sets. Here, we harness the multiple tree met-
ric (25) to perform integration of datasets in a more systematic
manner. We generalize the external organization method to
several datasets, where we integrate all the learned trees on
the genes {TX } into a single metric via the multi-tree metric.
In addition to the gene tree from both METABRIC datasets,
we also obtain the gene trees from the TCGA and the BRCA-
547 datasets, ZT and ZB. We then calculate a multi-tree met-
ric (25) to construct the sample tree on either the discovery or
validation sets. We report the evaluation measures in the third
row of Table V and Table VI. Taking into account all measures,
the multi-tree metric incorporating four different datasets best
organizes both the discovery and validation datasets. Integrat-
ing information from multiple sources improves the accuracy
of the organization, as averaging the metrics emphasizes
genes that are consistently grouped together, representing the
intrinsic structure of the data. In addition, since the metric
integrates the organizations from several datasets, it is more
accurate than the internal organization of a dataset with few
samples or a non-uniform distribution of subtypes.

Our results show that external organization, via either both
single or multi-tree metric, enables us to learn a meaningful
multi-scale hierarchy on the genes and apply it as a metric to
organize the samples of a given dataset. Thus, we can apply
information from one dataset to another to recover a multi-
scale organization of the samples, even if they were proﬁled
in a different technique. In addition, we obtain a validation of

14

TABLE V
METABRIC DISCOVERY ORGANIZATION

discovery
Self-organization
Inserted into
validation tree
Multi-tree

RI
0.75

ARI VI
0.33

1.81

p-value
1.82 × 10−11

0.74

0.34

1.66

2.93 × 10−9

0.75

0.35

1.63

3.18 × 10−13

TABLE VI
METABRIC VALIDATION ORGANIZATION

validation
Self-organization
Inserted into
discovery tree
Multi-tree

RI
0.77

ARI VI
0.33

1.82

p-value
3.07 × 10−4

0.76

0.30

1.98

9.08 × 10−8

0.76

0.34

1.73

4.24 × 10−9

the gene organization of one dataset via another. This cannot
be accomplished with traditional hierarchical clustering in a
clustered dendrogram as the clustering of the samples does not
depend on the hierarchical structure of the genes dendrogram.
However, we can obtain an iterative hierarchical clustering
algorithm for biclustering using our approach. As our bi-
organization depends on a partition tree method, we can use
hierarchical clustering instead of ﬂexible trees in the iterative
bi-organization algorithm. Alternatively, as hierarchical clus-
tering depends on a metric, this can also be formulated as
deriving a transform from the dendrogram on the genes and
using its corresponding tree-metric instead of correlation as
the input metric to the hierarchical clustering algorithm on
the samples, and vice-versa.

In related work, Cheng, Yang and Anastassiou [51] analyzed
multiple datasets and identiﬁed consistent groups of genes
across datasets. Zhou et al. [52] integrate datasets in a platform
independent manner to identify groups of genes with the same
function across multiple datasets. The multi-tree transform can
also be used to identify such genes, however this is beyond
the scope of this paper and will be addressed in future work.

D. Sub-type labels

In breast cancer, PAM50 [39] is typically used to assign
intrinsic subtypes to the tumors. However, Milioli et al. [42]
recently proposed a reﬁned set of subtypes labels for the
METABRIC dataset, based on a supervised iterative approach
to ensure consistency of the labels using several classiﬁers.
Their labels are shown to have a better agreement with
the clinical markers and patients’ overall survival than those
provided by the PAM50 method. Therefore, the clustering
measures we reported on the METABRIC dataset were with
respect to the reﬁned labels.

Our unsupervised analysis demonstrated higher consistency
with the reﬁned labels than with PAM50. Thus, our unsuper-
vised approach provides an additional validation to the labeling
achieved in a supervised manner. We divided the data into
training and test sets and classiﬁed the test set using k-NN
nearest neighbors with majority voting using the tree-based
metric. For different parameters and increasing numbers of

genes (nX = 500, 1000, 2000), we had higher agreement with
the reﬁned labels than with PAM50, achieving a classiﬁcation
accuracy of 82% on average. Classifying with the PAM50
labels had classiﬁcation accuracy lower by an average of
10% ± 2%. This is also evident when examining the labels
in Fig. 4. Note that whereas PAM50 assigns a label based
on 50 genes and the reﬁned labels were learned using a
subset of genes found in a supervised manner, our approach
is unsupervised using the nX genes with the highest variance.

VI. CONCLUSIONS

In this paper we proposed new data-driven tree-based trans-
forms and metrics in a matrix organization setting. We pre-
sented partition trees as inducing a new multiscale transform
space that conveys the smooth organization of the data, and
derived a metric in the transform space. The trees and cor-
responding metrics are updated in an iterative bi-organization
approach, organizing the observations based on the multiscale
decomposition of the features, and organizing the features
based on the multiscale decomposition of the observations.
In addition, we generalized the transform and the metric
to incorporate multiple partition trees on the data, allowing
for the integration of several datasets. We applied our data-
driven approach to the organization of breast cancer gene
expression data, learning metrics on the genes to organize the
tumor samples in meaningful clusters of cancer sub-types. We
demonstrated how our approach can be used to validate the
hierarchical organization of both the genes and the samples
by taking into account several datasets of samples, even
when these datasets were proﬁled using different technolo-
gies. Finally, we employed our multi-tree metric to integrate
information from the organization of these multiple datasets
and achieved an improved organization of tumor samples.

In future work, we will explore several aspects of the
multiple tree setting. First, the multi-tree transform and metric
can be incorporated in the iterative framework for further
reﬁnement. Second, we will generalize the coherency measure
to incorporate multiple trees. Third, we will apply the multi-
tree framework to a multi-modal setting, where observations
are shared across datasets, as for example, in the joint samples
shared by the BRCA-547 and TCGA datasets. Finally, we will
reformulate the iterative procedure as an optimization problem,
enabling to explicitly introduce cost functions. In particular,
cost functions imposing the common structure of the multiple
trees across datasets will be considered.

ACKNOWLEDGMENTS

The authors thank the anonymous reviewers for their con-

structive comments and useful suggestions.

REFERENCES

15

[4] W. H. Yang, D. Q. Dai, and H. Yan, “Finding correlated biclusters from
gene expression data,” IEEE Trans. Knowl. Data Eng., vol. 23, no. 4,
pp. 568–584, April 2011.

[5] E. C. Chi, G. I. Allen, and R. G. Baraniuk, “Convex biclustering,”
Biometrics, 2016. [Online]. Available: http://dx.doi.org/10.1111/biom.
12540

[6] D. Jiang, C. Tang, and A. Zhang, “Cluster analysis for gene expression
data: a survey,” IEEE Trans. Knowl. Data Eng., vol. 16, no. 11, pp.
1370–1386, 2004.

[7] J. Bennett and S. Lanning, “The Netﬂix prize,” in Proceedings of KDD

cup and workshop, vol. 2007, 2007, p. 35.

[8] S. Busygin, O. Prokopyev, and P. M. Pardalos, “Biclustering in data
mining,” Computers & Operations Research, vol. 35, no. 9, pp. 2964 –
2987, 2008.

[9] M. Gavish and R. R. Coifman, “Sampling, denoising and compression
of matrices by coherent matrix organization,” Appl. Comput. Harmon.
Anal., vol. 33, no. 3, pp. 354 – 369, 2012.

[10] J.

I. Ankenman, “Geometry and analysis of dual networks on
questionnaires,” Ph.D. dissertation, Yale University, 2014. [Online].
Available: https://github.com/hgfalling/pyquest/blob/master/ankenman
diss.pdf

[11] Y. Kluger, R. Basri, J. T. Chang, and M. Gerstein, “Spectral biclus-
tering of microarray data: coclustering genes and conditions,” Genome
research, vol. 13, no. 4, pp. 703–716, 2003.

[12] K. M. Tan and D. M. Witten, “Sparse biclustering of transposable data,”

J. Comp. Graph. Stat., vol. 23, no. 4, pp. 985–1008, 2014.

[13] M. Gavish, B. Nadler, and R. R. Coifman, “Multiscale wavelets on
trees, graphs and high dimensional data: Theory and applications to
semi supervised learning,” in Proc. ICML, 2010, pp. 367–374.

[14] A. Singh, R. Nowak, and R. Calderbank, “Detecting weak but
hierarchically-structured patterns in networks,” in Proc. AISTATS, vol. 9,
May 2010, pp. 749–756.

[15] D. K. Hammond, P. Vandergheynst, and R. Gribonval, “Wavelets on
graphs via spectral graph theory,” Appl. Comput. Harmon. Anal., vol. 30,
no. 2, pp. 129 – 150, 2011.

[16] J. Sharpnack, A. Singh, and A. Krishnamurthy, “Detecting activations
over graphs using spanning tree wavelet bases.” in AISTATS, 2013, pp.
536–544.

[17] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-
dergheynst, “The emerging ﬁeld of signal processing on graphs: Ex-
tending high-dimensional data analysis to networks and other irregular
domains,” IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83–98, 2013.
[18] S. K. Narang and A. Ortega, “Compact support biorthogonal wavelet
ﬁlterbanks for arbitrary undirected graphs,” IEEE Trans. Signal Process.,
vol. 61, no. 19, pp. 4673–4685, Oct 2013.

[19] A. Sakiyama, K. Watanabe, and Y. Tanaka, “Spectral graph wavelets
and ﬁlter banks with low approximation error,” IEEE Trans. Signal Inf.
Process. Netw., vol. 2, no. 3, pp. 230–245, Sept 2016.

[20] D. I. Shuman, M. J. Faraji, and P. Vandergheynst, “A multiscale pyramid
transform for graph signals,” IEEE Trans. Signal Process., vol. 64, no. 8,
pp. 2119–2134, April 2016.

[21] N. Tremblay and P. Borgnat, “Subgraph-based ﬁlterbanks for graph
signals,” IEEE Trans. Signal Process., vol. 64, no. 15, pp. 3827–3840,
Aug 2016.

[22] N. Shahid, N. Perraudin, V. Kalofolias, G. Puy, and P. Vandergheynst,
“Fast robust PCA on graphs,” IEEE J. Sel. Topics Signal Process.,
vol. 10, no. 4, pp. 740–756, June 2016.

[23] G. Mishne, R. Talmon, R. Meir, J. Schiller, U. Dubin, and R. R.
Coifman, “Hierarchical coupled-geometry analysis for neuronal structure
and activity pattern discovery,” IEEE J. Sel. Topics Signal Process.,
vol. 10, no. 7, pp. 1238–1253, Oct 2016.

[24] P. Burt and E. Adelson, “The Laplacian pyramid as a compact image
code,” IEEE Trans. Commun., vol. 31, no. 4, pp. 532–540, 1983.
[25] R. R. Coifman and W. E. Leeb, “Earth mover’s distance and equivalent
metrics for spaces with hierarchical partition trees,” Yale University,
Tech. Rep., 2013, technical report YALEU/DCS/TR1482.

[26] I. Ram, M. Elad, and I. Cohen, “Generalized tree-based wavelet trans-
form,” IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4199–4209, 2011.
[27] R. Kondor, N. Teneva, and V. Garg, “Multiresolution matrix factoriza-

[1] Y. Cheng and G. M. Church, “Biclustering of expression data,” in ISMB,

tion,” in Proc. ICML, 2014, pp. 1620–1628.

vol. 8, 2000, pp. 93–103.

[28] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp.

[2] C. Tang, L. Zhang, A. Zhang, and M. Ramanathan, “Interrelated two-
way clustering: an unsupervised approach for gene expression data
analysis,” in Proc. BIBE, 2001, pp. 41–48.

[3] M. Lee, H. Shen, J. Z. Huang, and J. S. Marron, “Biclustering via
sparse singular value decomposition,” Biometrics, vol. 66, no. 4, pp.
1087–1095, 2010.

5–32, 2001.

[29] R. R. Coifman and S. Lafon, “Diffusion maps,” Appl. Comput. Harmon.

Anal., vol. 21, no. 1, pp. 5–30, July 2006.

[30] R. R. Coifman and M. Gavish, “Harmonic analysis of digital data
bases,” in Wavelets and Multiscale Analysis, ser. Applied and Numerical
Harmonic Analysis. Birkh¨auser Boston, 2011, pp. 161–197.

[31] G. Mishne, “Diffusion nets and manifold learning for high-dimensional
data analysis in the presence of outliers,” Ph.D. dissertation, Technion,
2016.

[32] M. Zontak, I. Mosseri, and M. Irani, “Separating signal from noise using

patch recurrence across scales,” in Proc. CVPR, June 2013.

[33] Y.-Y. Ahn, J. P. Bagrow, and S. Lehmann, “Link communities reveal
multiscale complexity in networks,” Nature, vol. 466, no. 7307, pp. 761–
764, 2010.

[34] J. Xie, S. Kelley, and B. K. Szymanski, “Overlapping community
detection in networks: The state-of-the-art and comparative study,” ACM
Comput. Surv., vol. 45, no. 4, pp. 43:1–43:35, Aug. 2013.

[35] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu,
“An optimal algorithm for approximate nearest neighbor searching ﬁxed
dimensions,” J. ACM, vol. 45, no. 6, pp. 891–923, Nov. 1998.

[36] B.-K. Yi and C. Faloutsos, “Fast time sequence indexing for arbitrary

lp norms,” in Proc. VLDB, 2000.

[37] [Online]. Available: http://github.com/gmishne/pyquest
[38] T. Sørlie et al., “Gene expression patterns of breast carcinomas distin-
guish tumor subclasses with clinical implications,” Proc. Natl. Acad.
Sci., vol. 98, no. 19, pp. 10 869–10 874, 2001.

[39] J. S. Parker et al., “Supervised risk predictor of breast cancer based on
intrinsic subtypes,” Journal of Clinical Oncology, vol. 27, no. 8, pp.
1160–1167, 2009.

[40] C. Curtis et al., “The genomic and transcriptomic architecture of 2,000
breast tumours reveals novel subgroups,” Nature, vol. 486, no. 7403, pp.
346–352, 2012.

[41] Cancer Genome Atlas Network, “Comprehensive molecular portraits of

human breast tumours,” Nature, vol. 490, no. 7418, pp. 61–70, 2012.

[42] H. H. Milioli, R. Vimieiro, I. Tishchenko, C. Riveros, R. Berretta, and
P. Moscato, “Iteratively reﬁning breast cancer intrinsic subtypes in the
METABRIC dataset,” BioData Mining, vol. 9, no. 1, pp. 1–8, 2016.

[43] Cancer Genome Atlas Network. [Online]. Available: https://xenabrowser.
net/datapages/?cohort=TCGA%20Breast%20Cancer%20(BRCA)
[44] C. M. Perou et al., “Molecular portraits of human breast tumours,”

Nature, vol. 406, no. 6797, pp. 747–752, 2000.

[45] R. Peto and J. Peto, “Asymptotically efﬁcient rank invariant test proce-
dures,” Journal of the Royal Statistical Society. Series A (General), vol.
135, no. 2, pp. 185–207, 1972.

[46] P. Langfelder, B. Zhang, and S. Horvath, “Deﬁning clusters from a hier-
archical cluster tree: the dynamic tree cut package for r,” Bioinformatics,
vol. 24, no. 5, pp. 719–720, 2008.

[47] W. M. Rand, “Objective criteria for the evaluation of clustering meth-
ods,” Journal of the American Statistical Association, vol. 66, no. 336,
pp. 846–850, 1971.

[48] L. Hubert and P. Arabie, “Comparing partitions,” J. Classiﬁcation, vol. 2,

no. 1, pp. 193–218, 1985.

[49] M. Meil˘a, “Comparing clusterings - an information based distance,”
Journal of Multivariate Analysis, vol. 98, no. 5, pp. 873 – 895, 2007.
[50] E. L. Kaplan and P. Meier, “Nonparametric estimation from incomplete
observations,” Journal of the American statistical association, vol. 53,
no. 282, pp. 457–481, 1958.

[51] W.-Y. Cheng, T.-H. O. Yang, and D. Anastassiou, “Biomolecular events
in cancer revealed by attractor metagenes,” PLoS Comput Biol, vol. 9,
no. 2, pp. 1–14, 02 2013.

[52] X. J. Zhou et al., “Functional annotation and network reconstruction
through cross-platform integration of microarray data,” Nature biotech-
nology, vol. 23, no. 2, pp. 238–243, 2005.

[53] J. P. Klein and M. L. Moeschberger, Survival analysis: techniques for

censored and truncated data. SSBM, 2005.

APPENDIX I
FLEXIBLE TREES

We brieﬂy describe the ﬂexible trees algorithm, given the
feature set X and an afﬁnity matrix on the features denoted
KX . For a detailed description see [10].

1) Input: The set of features X , an afﬁnity matrix KX ∈

RnX ×nX , and a constant (cid:15).

2) Init: Set partition I0,i = {i} ∀ 1 ≤ i ≤ nX , set l = 1.
3) Given an afﬁnity on the data, we construct a low-

dimensional embedding on the data [29].

4) Calculate

level-dependent
d(l)(i, j) ∀ 1 ≤ i, j ≤ nX in the embedding space.

pairwise

the

distances

16

(cid:15) , where p = median (cid:0)d(l)(i, j)(cid:1).

5) Set a threshold p
6) For each index i which has not yet been added
to a folder, ﬁnd its minimal distance dmin(i) =
minj{d(l)(i, j)}.

• If dmin(i) < p

(cid:15) , i and j form a new folder if j
does not belong to a folder. If j is already part of a
folder I, then i is added to that folder if dmin(i) <
p
(cid:15) 2−|I|+1.
• If dmin(i) > p

(cid:15) , i remains as a singleton folder.
7) The partition Pl is set to be all the formed folders.
8) For l > 1 and while not all samples have been merged
together in a single folder, steps 4-7 are repeated for the
folders Il−1,i ∈ Pl−1. The distances between folders
depend on the level l, and on the samples in each of the
folders.

APPENDIX II
COMPARING SURVIVAL CURVES
The survival function S(t) is deﬁned as the probability that
a subject will survive past time t. Let T be a failure time
with probability density function f . The survival function is
S(t) = P (T > t), where the Kaplan-Meier method [50] is a
non-parametric estimate given by

ˆS(tj) =

P r(T > ti|T ≥ ti) =

j
(cid:89)

i=1

ˆS(tj−1)P r(T > tj|T ≥ tj).

(32)

Deﬁning ni as the number at risk just prior to time ti and
di as the number of failures at ti, then P (T > ti) = ni−di
.
For more information on estimating survival curves and taking
into account censored data see [53]

ni

Comparison of two survival curves can be done using a
statistical hypothesis test called the log-rank test [45]. It is
used to test the null hypothesis that there is no difference
between the population survival curves (i.e. the probability of
an event occurring at any time point is the same for each
population). Deﬁne nk,i as the number at risk in group k just
prior to time ti, such that ni = (cid:80)
k nk,i and dk,i as the number
of failures in group k at time ti such that di = (cid:80)
k dk,i. Then,
the expected number of failures in group k = 1, 2 is given by
(cid:88)

and the observed number of failures in group k = 1, 2 is

Ek =

di

nk,i
ni

(cid:88)

Ok =

dk,i.

i

i

(O2 − E2)2
Var(O2 − E2)

∼ χ2
1.

(33)

(34)

(35)

Under the null hypothesis of no difference between the two

groups, the log-rank test statistic is

The log-rank test can be extended to more than two
groups [53].

