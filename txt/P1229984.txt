Conditional Probability Models for Deep Image Compression

Fabian Mentzer∗ Eirikur Agustsson∗ Michael Tschannen
mentzerf@vision.ee.ethz.ch

aeirikur@vision.ee.ethz.ch

michaelt@nari.ee.ethz.ch

Radu Timofte
timofter@vision.ee.ethz.ch

Luc Van Gool
vangool@vision.ee.ethz.ch

ETH Z¨urich, Switzerland

9
1
0
2
 
n
u
J
 
4
 
 
]

V
C
.
s
c
[
 
 
4
v
0
6
2
4
0
.
1
0
8
1
:
v
i
X
r
a

Abstract

Deep Neural Networks trained as image auto-encoders
have recently emerged as a promising direction for advanc-
ing the state-of-the-art in image compression. The key chal-
lenge in learning such networks is twofold: To deal with
quantization, and to control the trade-off between recon-
struction error (distortion) and entropy (rate) of the latent
image representation. In this paper, we focus on the latter
challenge and propose a new technique to navigate the rate-
distortion trade-off for an image compression auto-encoder.
The main idea is to directly model the entropy of the latent
representation by using a context model: A 3D-CNN which
learns a conditional probability model of the latent distribu-
tion of the auto-encoder. During training, the auto-encoder
makes use of the context model to estimate the entropy of its
representation, and the context model is concurrently up-
dated to learn the dependencies between the symbols in the
latent representation. Our experiments show that this ap-
proach, when measured in MS-SSIM, yields a state-of-the-
art image compression system based on a simple convolu-
tional auto-encoder.

1. Introduction

Image compression refers to the task of representing im-
ages using as little storage (i.e., bits) as possible. While in
lossless image compression the compression rate is limited
by the requirement that the original image should be per-
fectly reconstructible, in lossy image compression, a greater
reduction in storage is enabled by allowing for some distor-
tion in the reconstructed image. This results in a so-called
rate-distortion trade-off, where a balance is found between
the bitrate R and the distortion d by minimizing d + βR,
where β > 0 balances the two competing objectives. Re-
cently, deep neural networks (DNNs) trained as image auto-
encoders for this task led to promising results, achieving
better performance than many traditional techniques for im-
age compression [19, 20, 17, 4, 2, 9]. Another advantage of

∗The ﬁrst two authors contributed equally.

Figure 1: State-of-the-art performance achieved by our sim-
ple compression system composed of a standard convolu-
tional auto-encoder and a 3D-CNN-based context model.

DNN-based learned compression systems is their adaptabil-
ity to speciﬁc target domains such as areal images or stereo
images, enabling even higher compression rates on these
domains. A key challenge in training such systems is to op-
timize the bitrate R of the latent image representation in the
auto-encoder. To encode the latent representation using a
ﬁnite number of bits, it needs to be discretized into symbols
(i.e., mapped to a stream of elements from some ﬁnite set
of values). Since discretization is non-differentiable, this
presents challenges for gradient-based optimization meth-
ods and many techniques have been proposed to address
them. After discretization, information theory tells us that
the correct measure for bitrate R is the entropy H of the
resulting symbols. Thus the challenge, and the focus of this
paper, is how to model H such that we can navigate the
trade-off d + βH during optimization of the auto-encoder.

Our proposed method is based on leveraging context
models, which were previously used as techniques to im-

1

prove coding rates for already-trained models [4, 20, 9, 14],
directly as an entropy term in the optimization. We concur-
rently train the auto-encoder and the context model with re-
spect to each other, where the context model learns a convo-
lutional probabilistic model of the image representation in
the auto-encoder, while the auto-encoder uses it for entropy
estimation to navigate the rate-distortion trade-off. Further-
more, we generalize our formulation to spatially-aware net-
works, which use an importance map to spatially attend the
bitrate representation to the most important regions in the
compressed representation. The proposed techniques lead
to a simple image compression system1, which achieves
state-of-the-art performance when measured with the pop-
ular multi-scale structural similarity index (MS-SSIM) dis-
tortion metric [23], while being straightforward to imple-
ment with standard deep-learning toolboxes.

2. Related work

Full-resolution image compression using DNNs has at-
tracted considerable attention recently. DNN architectures
commonly used for image compression are auto-encoders
[17, 4, 2, 9] and recurrent neural networks (RNNs) [19, 20].
The networks are typically trained to minimize the mean-
squared error (MSE) between original and decompressed
image [17, 4, 2, 9], or using perceptual metrics such as
MS-SSIM [20, 14]. Other notable techniques involve pro-
gressive encoding/decoding strategies [19, 20], adversarial
training [14], multi-scale image decompositions [14], and
generalized divisive normalization (GDN) layers [4, 3].

Context models and entropy estimation—the focus of
the present paper—have a long history in the context of
engineered compression methods, both lossless and lossy
[24, 12, 25, 13, 10]. Most of the recent DNN-based lossy
image compression approaches have also employed such
techniques in some form. [4] uses a binary context model
for adaptive binary arithmetic coding [11]. The works of
[20, 9, 14] use learned context models for improved cod-
ing performance on their trained models when using adap-
tive arithmetic coding. [17, 2] use non-adaptive arithmetic
coding but estimate the entropy term with an independence
assumption on the symbols.

Also related is the work of van den Oord et al. [22, 21],
who proposed PixelRNN and PixelCNN, powerful RNN-
and CNN-based context models for modeling the distribu-
tion of natural images in a lossless setting, which can be
used for (learned) lossless image compression as well as
image generation.

3. Proposed method

Given a set of training images

, we wish to learn a
compression system which consists of an encoder, a quan-

X

1https://github.com/fab-jul/imgcomp-cvpr

→

→ C

discretizes the coordinates of z to L =

Rm maps an
tizer, and a decoder. The encoder E : Rd
image x to a latent representation z = E(x). The quantizer
Q : R
cen-
ters, obtaining ˆz with ˆzi := Q(zi)
, which can be loss-
lessly encoded into a bitstream. The decoder D then forms
the reconstructed image ˆx = D(ˆz) from the quantized la-
tent representation ˆz, which is in turn (losslessy) decoded
from the bitstream. We want the encoded representation ˆz
to be compact when measured in bits, while at the same time
we want the distortion d(x, ˆx) to be small, where d is some
measure of reconstruction error, such as MSE or MS-SSIM.
This results in the so-called rate-distortion trade-off

∈ C

|C|

d(x, ˆx) + βH(ˆz),

(1)

where H denotes the cost of encoding ˆz to bits, i.e., the en-
tropy of ˆz. Our system is realized by modeling E and D as
convolutional neural networks (CNNs) (more speciﬁcally,
as the encoder and decoder, respectively, of a convolutional
auto-encoder) and minimizing (1) over the training set
,
X
where a large/small β draws the system towards low/high
average entropy H. In the next sections, we will discuss
how we quantize z and estimate the entropy H(ˆz). We note
that as E, D are CNNs, ˆz will be a 3D feature map, but for
simplicity of exposition we will denote it as a vector with
equally many elements. Thus, ˆzi refers to the i-th element
of the feature map, in raster scan order (row by column by
channel).

3.1. Quantization

We adopt the scalar variant of the quantization approach
proposed in [2] to quantize z, but simplify it using ideas
from [17]. Speciﬁcally, given centers
, cL} ⊂
· · ·
R, we use nearest neighbor assignments to compute
ˆzi = Q(zi) := arg minj(cid:107)

zi −
but rely on (differentiable) soft quantization

,
cj(cid:107)

c1,

(2)

=

C

{

˜zi =

L
(cid:88)

j=1

exp(
σ
−
l=1 exp(

(cid:80)L

)
zi −
cj(cid:107)
(cid:107)
)
cl(cid:107)
zi −
σ
(cid:107)
−

cj

(3)

C

to compute gradients during the backward pass. This com-
bines the beneﬁt of [2] where the quantization is restricted
(instead of the ﬁxed (non-
to a ﬁnite set of learned centers
learned) integer grid as in [17]) and the simplicity of [17],
where a differentiable approximation of quantization is only
used in the backward pass, avoiding the need to choose
an annealing strategy (i.e., a schedule for σ) as in [2] to
drive the soft quantization (3) to hard assignments (2) dur-
ing training. In TensorFlow, this is implemented as
¯zi = tf.stopgradient(ˆzi −

˜zi) + ˜zi.

(4)

We note that for forward pass computations, ¯zi = ˆzi, and
thus we will continue writing ˆzi for the latent representa-
tion.

2

3.2. Entropy estimation

the index I(ˆzi) of ˆzi in

with a cross entropy loss:

To model the entropy H(ˆz) we build on the approach
of PixelRNN [22] and factorize the distribution p(ˆz) as a
product of conditional distributions

p(ˆz) =

ˆzi
p(ˆzi|

−

1, . . . , ˆz1),

(5)

m
(cid:89)

i=1

where the 3D feature volume ˆz is indexed in raster
scan order. We then use a neural network P (ˆz), which
we refer to as a context model,
to estimate each term
ˆzi
p(ˆzi|

1, . . . , ˆz1):

−

−

−

≈

(6)

Pi,l(ˆz)

1, . . . , ˆz1),

ˆzi
p(ˆzi = cl|
where Pi,l speciﬁes for every 3D location i in ˆz the prob-
with l = 1, . . . , L. We re-
abilites of each symbol in
C
fer to the resulting approximate distribution as q(ˆz) :=
(cid:81)m
i=1 Pi,I(ˆzi)(ˆz), where I(ˆzi) denotes the index of ˆzi in
.
Since the conditional distributions p(ˆzi|
1, . . . , ˆz1)
ˆzi
1, . . . , ˆz1, this imposes a
only depend on previous values ˆzi
causality constraint on the network P : While P may com-
pute Pi,l in parallel for i = 1, . . . , m, l = 1, . . . , L, it needs
to make sure that each such term only depends on previous
values ˆzi

1, . . . , ˆz1.

C

−

−

The authors of PixelCNN [22, 21] study the use of 2D-
CNNs as causal conditional models over 2D images in a
lossless setting, i.e., treating the RGB pixels as symbols.
They show that the causality constraint can be efﬁciently en-
forced using masked ﬁlters in the convolution. Intuitively,
the idea is as follows: If for each layer the causality con-
dition is satisﬁed with respect to the spatial coordinates of
the layer before, then by induction the causality condition
will hold between the output layer and the input. Satisfying
the causality condition for each layer can be achieved with
proper masking of its weight tensor, and thus the entire net-
work can be made causal only through the masking of its
weights. Thus, the entire set of probabilities Pi,l for all (2D)
spatial locations i and symbol values l can be computed in
parallel with a fully convolutional network, as opposed to
modeling each term p(ˆzi|
1,
ˆzi
In our case, ˆz is a 3D symbol volume, with as much as
K = 64 channels. We therefore generalize the approach
of PixelCNN to 3D convolutions, using the same idea of
masking the ﬁlters properly in every layer of the network.
This enables us to model P efﬁciently, with a light-weight2
3D-CNN which slides over ˆz, while properly respecting the
causality constraint. We refer to the supplementary material
for more details.

, ˆz1) separately.

· · ·

−

As in [21], we learn P by training it for maximum like-
lihood, or equivalently (see [16]) by training Pi,: to classify

2We use a 4-layer network, compared to 15 layers in [22].

3

C

CE := Eˆz
∼

p(ˆz)[

m
(cid:88)

−

i=1

log Pi,I(ˆzi)].

(7)

Using the well-known property of cross entropy as the cod-
ing cost when using the wrong distribution q(ˆz) instead of
the true distribution p(ˆz), we can also view the CE loss as
an estimate of H(ˆz) since we learn P such that P = q
p.
That is, we can compute

≈

p(ˆz)[

log(p(ˆz))]

H(ˆz) = Eˆz
∼
= Eˆz
∼

p(ˆz)[

ˆzi
log p(ˆzi|

−

1,

−

· · ·

, ˆz1)]

Eˆz
∼

p(ˆz)[

≈

log q(ˆzi|

ˆzi

−

1,

−

· · ·

, ˆz1)]

−
m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

= Eˆz
∼

p(ˆz)[

−

i=1

log Pi,I(ˆzi)]

= CE

(8)

(9)

(10)

(11)

(12)

Therefore, when training the auto-encoder we can indirectly
minimize H(ˆz) through the cross entropy CE. We refer to
argument in the expectation of (7),

C(ˆz) :=

log Pi,I(ˆzi),

(13)

m
(cid:88)

−

i=1

as the coding cost of the latent image representation, since
this reﬂects the coding cost incurred when using P as a con-
text model with an adaptive arithmetic encoder [11]. From
the application perspective, minimizing the coding cost is
actually more important than the (unknown) true entropy,
since it reﬂects the bitrate obtained in practice.

To backpropagate through P (ˆz) we use the same ap-
proach as for the encoder (see (4)). Thus, like the decoder
D, P only sees the (discrete) ˆz in the forward pass, whereas
the gradient of the soft quantization ˜z is used for the back-
ward pass.

3.3. Concurrent optimization

Given an auto-encoder (E, D), we can train P to model
the dependencies of the entries of ˆz as described in the pre-
vious section by minimizing (7). On the other hand, us-
ing the model P , we can obtain an estimate of H(ˆz) as
in (12) and use this estimate to adjust (E, D) such that
d(x, D(Q(E(x)))) + βH(ˆz) is reduced, thereby navigating
the rate distortion trade-off. Therefore, it is natural to con-
currently learn P (with respect to its own loss), and (E, D)
(with respect to the rate distortion trade-off) during train-
ing, such that all models which the losses depend on are
continuously updated.

3.4. Importance map for spatial bit-allocation

8 ×

H
8 ×

Recall that since E and D are CNNs, ˆz is a 3D feature-
map. For example, if E has three stride-2 convolution layers
and the bottleneck has K channels, the dimensions of ˆz will
be W
K. A consequence of this formulation is that
we are using equally many symbols in ˆz for each spatial
location of the input image x. It is known, however, that in
practice there is great variability in the information content
across spatial locations (e.g., the uniform area of blue sky
vs. the ﬁne-grained structure of the leaves of a tree).

This can in principle be accounted for automatically in
the trade-off between the entropy and the distortion, where
the network would learn to output more predictable (i.e.,
low entropy) symbols for the low information regions, while
making room for the use of high entropy symbols for the
more complex regions. More precisely, the formulation in
(7) already allows for variable bit allocation for different
spatial regions through the context model P .

However, this arguably requires a quite sophisticated
(and hence computationally expensive) context model, and
we ﬁnd it beneﬁcial to follow Li et al. [9] instead by using
an importance map to help the CNN attend to different re-
gions of the image with different amounts of bits. While
[9] uses a separate network for this purpose, we consider a
simpliﬁed setting. We take the last layer of the encoder E,
1. We
and add a second single-channel output y
∈
H
K
expand this single channel y into a mask m
8 ×
of the same dimensionality as z as follows:

H
8 ×
8 ×

8 ×
R W

R W

∈

prior during training, i.e., costing each 1 bit to encode. The
importance map is thus their principal tool for controlling
the bitrate, since they thereby avoid encoding all the bits
in the representation. In contrast, we stick to the formula-
tion in (5) where the dependencies between the symbols are
modeled during training. We then use the importance map
as an architectural constraint and use their suggested cod-
ing strategy to obtain an alternative estimate for the entropy
H(ˆz), as follows.

(cid:100)

m

We observe that we can recover

from ˆz by count-
ing the number of consecutive zero symbols at the end of
each column ˆzi,j,:.3
is therefore a function of the
masked ˆz, i.e.,
m
as de-
(cid:101)
scribed, which means that we have for the conditional en-
tropy H(
(cid:100)

(cid:100)
= g(ˆz) for g recovering

ˆz) = 0. Now, we have

m
(cid:101)

m
(cid:101)

m

(cid:101)

(cid:100)

(cid:100)

(cid:101)|
H(ˆz) = H(
(cid:100)
= H(ˆz,
= H(ˆz

m

ˆz) + H(ˆz)

(15)

)

(cid:101)|
m
(cid:101)
) + H(
(cid:100)
If we treat the entropy of the mask, H(
), as constant
during optimization of the auto-encoder, we can then indi-
rectly minimize H(ˆz) through H(ˆz

m
(cid:101)
m
(cid:100)

(cid:100)
m
(cid:101)

(17)

(16)

|(cid:100)

).

(cid:101)

To estimate H(ˆz

m), we use the same factorization of p
|
m
as in (5), but since the mask
is known we have p(ˆzi =
(cid:100)
(cid:101)
c0) = 1 deterministic for the 3D locations i in ˆz where
the mask is zero. The logs of the corresponding terms in (9)
then evaluate to 0. The remaining terms, we can model with
the same context model Pi,l(ˆz), which results in

m).
|

mi,j,k =






1
(yi,j −
0

k)

if k < yi,j
if k
yi,j ≤
if k + 1 > yi,j

≤

k + 1

,

(14)

H(ˆz

m

)
(cid:101)

|(cid:100)

≈

Eˆz

p(ˆz)[

∼

mi(cid:101)

−(cid:100)

log Pi,I(ˆzi)],

(18)

m
(cid:88)

i=1

where yi,j denotes the value of y at spatial location (i, j).
The transition value for k
k + 1 is such that the
mask smoothly transitions from 0 to 1 for non-integer val-
ues of y.

yi,j ≤

≤

We then mask z by pointwise multiplication with the bi-
m
. Since the ceiling op-
(cid:101)
is not differentiable, as done by [17, 9], we use

narization of m, i.e., z
erator
identity for the backward pass.

(cid:12) (cid:100)

(cid:100)·(cid:101)

←

z

With this modiﬁcation, we have simply changed the ar-
chitecture of E slightly such that it can easily “zero out”
portions of columns zi,j,: of z (the rest of the network stays
the same, so that (2) still holds for example). As suggested
by [9], the so-obtained structure in z presents an alterna-
tive coding strategy: Instead of losslessly encoding the en-
tire symbol volume ˆz, we could ﬁrst (separately) encode the
m
, and then for each column ˆzi,j,: only encode the
mask
(cid:100)
(cid:101)
ﬁrst
+ 1 symbols, since the remaining ones are the
mi,j(cid:101)
constant Q(0), which we refer to as the zero symbol.

(cid:100)

Work [9] uses binary symbols (i.e.,

) and as-
sumes independence between the symbols and a uniform

0, 1
}
{

=

C

where mi denotes the i-th element of m (in the same raster
scan order as ˆz).

Similar to the coding cost (13), we refer to the argument

in the expectation in (18),

M C(ˆz) :=

mi(cid:101)

−(cid:100)

log Pi,I(ˆzi)

(19)

m
(cid:88)

i=1

as the masked coding cost of ˆz.

While the entropy estimate (18) is almost estimating the
same quantity as (7) (only differing by H(
)), it has the
beneﬁt of being weighted by mi. Therefore, the encoder E
has an obvious path to control the entropy of ˆz, by simply
increasing/decreasing the value of y for some spatial loca-
tion of x and thus obtaining fewer/more zero entries in m.
When the context model P (ˆz) is trained, however, we
still train it with respect to the formulation in (8), so it does

m
(cid:101)

(cid:100)

3If z contained zeros before it was masked, we might overestimate the
number of 0 entries in (cid:100)m(cid:101). However, we can redeﬁne those entries of m
as 0 and this will give the same result after masking.

4

not have direct access to the mask m and needs to learn the
dependencies on the entire masked symbol volume ˆz. This
means that when encoding an image, we can stick to stan-
dard adaptive arithmetic coding over the entire bottleneck,
without needing to resort to a two-step coding process as in
[9], where the mask is ﬁrst encoded and then the remaining
symbols. We emphasize that this approach hinges critically
on the context model P and the encoder E being trained
concurrently as this allows the encoder to learn a meaning-
ful (in terms of coding cost) mask with respect to P (see the
next section).

≈

m

) being ignored.
(cid:101)

In our experiments we observe that during training, the
two entropy losses (7) and (18) converge to almost the same
3.5% smaller due to
value, with the latter being around
H(
(cid:100)
While the importance map is not crucial for optimal rate-
distortion performance, if the channel depth K is adjusted
carefully, we found that we could more easily control the
entropy of ˆz through β when using a ﬁxed K, since the net-
work can easily learn to ignore some of the channels via the
importance map. Furthermore, in the supplementary mate-
rial we show that by using multiple importance maps for a
single network, one can obtain a single model that supports
multiple compression rates.

3.5. Putting the pieces together

We made an effort to carefully describe our formulation
and its motivation in detail. While the description is lengthy,
when putting the resulting pieces together we get a quite
straightforward pipeline for learned image compression, as
follows.

Given the set of training images

, we initialize (fully

convolutional) CNNs E, D, and P , as well as the centers
C
of the quantizer Q. Then, we train over minibatches
XB =
x(1),
. At each iteration, we
of crops from
{
take one gradient step for the auto-encoder (E, D) and the
quantizer Q, with respect to the rate-distortion trade-off

, x(B)

· · ·

X

}

X

2. Expand importance map y to mask m via (14)

3. Mask z, i.e., z

z

m

←

(cid:12) (cid:100)

(cid:101)

4. Quantize ˆz = Q(z)

5. Compute the context P (ˆz)

6. Decode ˆx = D(ˆz),

which can be computed in parallel over the minibatch on a
GPU since all the models are fully convolutional.

3.6. Relationship to previous methods

We are not the ﬁrst to use context models for adaptive
arithmetic coding to improve the performance in learned
deep image compression. Work [20] uses a PixelRNN-like
architecture [22] to train a recurrent network as a context
model for an RNN-based compression auto-encoder. Li et
al. [9] extract cuboid patches around each symbol in a bi-
nary feature map, and feed them to a convolutional context
model. Both these methods, however, only learn the context
model after training their system, as a post-processing step
to boost coding performance.

In contrast, our method directly incorporates the context
model as the entropy term for the rate-distortion term (1)
of the auto-encoder, and trains the two concurrently. This
is done at little overhead during training, since we adopt a
3D-CNN for the context model, using PixelCNN-inspired
[21] masking of the weights of each layer to ensure causal-
ity in the context model. Adopting the same approach to the
context models deployed by [20] or [9] would be non-trivial
since they are not designed for fast feed-forward computa-
tion. In particular, while the context model of [9] is also
convolutional, its causality is enforced through masking the
inputs to the network, as opposed to our masking of the
weights of the networks. This means their context model
needs to be run separately with a proper input cuboid for
each symbol in the volume (i.e., not fully convolutionally).

LE,D,Q =

1
B

B
(cid:88)

j=1

d(x(j), ˆx(j)) + βM C(ˆz(j)),

(20)

4. Experiments

which is obtained by combining (1) with the estimate (18)
& (19) and taking the batch sample average. Furthermore,
we take a gradient step for the context model P with respect
to its objective (see (7) & (13))

LP :=

1
B

B
(cid:88)

j=1

d(x(j), ˆx(j)) + βC(ˆz(j)).

(21)

To compute these two batch losses, we need to perform

the following computation for each x

∈ XB:

1. Obtain compressed (latent) representation z and im-
portance map y from the encoder: (z, y) = E(x)

Architecture Our auto-encoder has a similar architecture
as [17] but with more layers, and is described in Fig. 2. We
adapt the number of channels K in the latent representation
for different models. For the context model P , we use a
simple 4-layer 3D-CNN as described in Fig. 3.

Distortion measure Following [7, 14], we use the multi-
scale structural similarity index (MS-SSIM) [23] as mea-
sure of distortion d(x, ˆx) = 100
MS-SSIM(x, ˆx))
for our models. MS-SSIM reportedly correlates better with
human perception of distortion than mean squared error
(MSE). We train and test all our models using MS-SSIM.

(1

−

·

5

while using a moderately large β = 10. We use a small
regularization on the weights and note that we achieve very
stable training. We trained our models for 6 epochs, which
took around 24h per model on a single GPU. For P , we use
a LR of 10−

4 and the same decay schedule.

Datasets We train on the the ImageNet dataset from
the Large Scale Visual Recognition Challenge 2012
(ILSVRC2012) [15]. As a preprocessing step, we take ran-
dom 160
160 crops, and randomly ﬂip them. We set aside
100 images from ImageNet as a testing set, ImageNetTest.
Furthermore, we test our method on the widely used Ko-
dak [1] dataset. To asses performance on high-quality full-
resolution images, we also test on the datasets B100 [18]
and Urban100 [5], commonly used in super-resolution.

×

Other codecs We compare to JPEG, using libjpeg4, and
JPEG2000, using the Kakadu implementation5. We also
compare to the lesser known BPG6, which is based on
HEVC, the state-of-the-art in video compression, and which
outperforms JPEG and JPEG2000. We use BPG in the non-
default 4:4:4 chroma format, following [14].

Comparison Like [14], we proceed as follows to com-
pare to other methods. For each dataset, we compress each
image using all our models. This yields a set of (bpp, MS-
SSIM) points for each image, which we interpolate to get
a curve for each image. We ﬁx a grid of bpp values, and
average the curves for each image at each bpp grid value
(ignoring those images whose bpp range does not include
the grid value, i.e., we do not extrapolate). We do this for
our method, BPG, JPEG, and JPEG2000. Due to code being
unavailable for the related works in general, we digitize the
Kodak curve from Rippel & Bourdev [14], who have care-
fully collected the curves from the respective works. With
this, we also show the results of Rippel & Bourdev [14],
Johnston et al. [7], Ball´e et al. [4], and Theis et al. [17].
To validate that our estimated MS-SSIM is correctly im-
plemented, we independently generated the BPG curves for
Kodak and veriﬁed that they matched the one from [14].

Results Fig. 1 shows a comparison of the aforementioned
methods for Kodak. Our method outperforms BPG, JPEG,
and JPEG2000, as well as the neural network based ap-
proaches of Johnston et al. [7], Ball´e et al. [4], and Theis
et al. [17]. Furthermore, we achieve performance compara-
ble to that of Rippel & Bourdev [14]. This holds for all bpps
we tested, from 0.3 bpp to 0.9 bpp. We note that while Rip-
pel & Bourdev and Johnston et al. also train to maximize
(MS-)SSIM, the other methods minimize MSE.

4http://libjpeg.sourceforge.net/
5http://kakadusoftware.com/
6https://bellard.org/bpg/

Figure 2: The architecture of our auto-encoder. Dark gray
blocks represent residual units. The upper part represents
the encoder E, the lower part the decoder D. For the en-
coder, “k5 n64-2” represents a convolution layer with ker-
nel size 5, 64 output channels and a stride of 2. For the de-
coder it represents the equivalent deconvolution layer. All
convolution layers are normalized using batch norm [6], and
use SAME padding. Masked quantization is the quantiza-
tion described in Section 3.4. Normalize normalizes the in-
put to [0, 1] using a mean and variance obtained from a sub-
set of the training set. Denormalize is the inverse operation.

Figure 3:
The architecture of our context model.
“3D k3 n24” refers to a 3D masked convolution with ﬁl-
ter size 3 and 24 output channels. The last layer outputs L
values for each voxel in ˆz.

·

10−

Training We use the Adam optimizer [8] with a mini-
batch size of 30 to train seven models. Each model is trained
to maximize MS-SSIM directly. As a baseline, we used a
3 for each model, but found it
learning rate (LR) of 4
beneﬁcial to vary it slightly for different models. We set
σ = 1 in the smooth approximation (3) used for gradient
backpropagation through Q. To make the model more pre-
dictably land at a certain bitrate t when optimizing (1), we
found it helpful to clip the rate term (i.e., replace the entropy
term βH with max(t, βH)), such that the entropy term is
“switched off” when it is below t. We found this did not
hurt performance. We decay the learning rate by a factor 10
every two epochs. To obtain models for different bitrates,
we adapt the target bitrate t and the number of channels K,

6

Figure 4: Performance of our approach on ImageNetTest, B100, Urban100, where we out-
perform BPG, JPEG and JPEG2000 in MS-SSIM.

Ours 0.124bpp

0.147 bpp BPG

JPEG2000 0.134bpp

0.150bpp JPEG

Figure 5: Example image (kodim21) from the Kodak testing set, compressed with different methods.

In each of the other testing sets, we also outperform
BPG, JPEG, and JPEG2000 over the reported bitrates, as
shown in Fig. 4.

In Fig. 5, we compare our approach to BPG, JPEG,
and JPEG2000 visually, using very strong compression on
kodim21 from Kodak. It can be seen that the output of our
network is pleasant to look at. Soft structures like the clouds
are very well preserved. BPG appears to handle high fre-
quencies better (see, e.g., the fence) but loses structure in
the clouds and in the sea. Like JPEG2000, it produces block
artifacts. JPEG breaks down at this rate. We refer to the
supplementary material for further visual examples.

Ablation study: Context model
In order to show the ef-
fectiveness of the context model, we performed the follow-

ing ablation study. We trained the auto-encoder without en-
tropy loss, i.e., β = 0 in (20), using L = 6 centers and
K = 16 channels. On Kodak, this model yields an av-
erage MS-SSIM of 0.982, at an average rate of 0.646 bpp
(calculated assuming that we need log2(L) = 2.59 bits per
symbol). We then trained three different context models for
this auto-encoder, while keeping the auto-encoder ﬁxed: A
zeroth order context model which uses a histogram to esti-
mate the probability of each of the L symbols; a ﬁrst order
(one-step prediction) context model, which uses a condi-
tional histogram to estimate the probability of each of the
L symbols given the previous symbol (scanning ˆz in raster
order); and P , i.e., our proposed context model. The result-
ing average rates are shown in Table 1. Our context model

7

reduces the rate by 10 %, even though the auto-encoder was
optimized using a uniform prior (see supplementary mate-
rial for a detailed comparison of Table 1 and Fig. 1).

5. Discussion

rate
Model
0.646 bpp
Baseline (Uniform)
0.642 bpp
Zeroth order
First order
0.627 bpp
Our context model P 0.579 bpp

Table 1: Rates for different context models, for the same
architecture (E, D).

Importance map As described in detail in Section 3.4,
we use an importance map to dynamically alter the number
of channels used at different spatial locations to encode an
image. To visualize how this helps, we trained two auto-
encoders M and M (cid:48), where M uses an importance map
and at most K = 32 channels to compress an image, and
M (cid:48) compresses without importance map and with K = 16
channels (this yields a rate for M (cid:48) similar to that of M ). In
Fig. 6, we show an image from ImageNetTest along with
the same image compressed to 0.463 bpp by M and com-
pressed to 0.504 bpp by M (cid:48). Furthermore, Fig. 6 shows the
importance map produced by M , as well as ordered visual-
izations of all channels of the latent representation for both
M and M (cid:48). Note how for M , channels with larger index
are sparser, showing how the model can spatially adapt the
number of channels. M (cid:48) uses all channels similarly.

Input

Importance map of M

Output of M

Latent representation of M

Output of M (cid:48)

Latent representation of M (cid:48)

Our experiments showed that combining a convolutional
auto-encoder with a lightweight 3D-CNN as context model
and training the two networks concurrently leads to a highly
effective image compression system. Not only were we
able to clearly outperform state-of-the-art engineered com-
pression methods including BPG and JPEG2000 in terms
of MS-SSIM, but we also obtained performance compet-
itive with the current state-of-the-art learned compression
method from [14]. In particular, our method outperforms
BPG and JPEG2000 in MS-SSIM across four different
testing sets (ImageNetTest, Kodak, B100, Urban100), and
does so signiﬁcantly, i.e., the proposed method generalizes
well. We emphasize that our method relies on elemen-
tary techniques both in terms of the architecture (standard
convolutional auto-encoder with importance map, convo-
lutional context model) and training procedure (minimize
the rate-distortion trade-off and the negative log-likelihood
for the context model), while [14] uses highly specialized
techniques such as a pyramidal decomposition architecture,
adaptive codelength regularization, and multiscale adver-
sarial training.

The ablation study for the context model showed that our
3D-CNN-based context model is signiﬁcantly more power-
ful than the ﬁrst order (histogram) and second order (one-
step prediction) baseline context models. Further, our ex-
periments suggest that the importance map learns to con-
densate the image information in a reduced number of chan-
nels of the latent representation without relying on explicit
supervision. Notably, the importance map is learned as a
part of the image compression auto-encoder concurrently
with the auto-encoder and the context model, without in-
troducing any optimization difﬁculties. In contrast, in [9]
the importance map is computed using a separate network,
learned together with the auto-encoder, while the context
model is learned separately.

6. Conclusions

In this paper, we proposed the ﬁrst method for learning
a lossy image compression auto-encoder concurrently with
a lightweight context model by incorporating it into an en-
tropy loss for the optimization of the auto-encoder, leading
to performance competitive with the current state-of-the-art
in deep image compression [14].

Future works could explore heavier and more power-
ful context models, as those employed in [22, 21]. This
could further improve compression performance and allow
for sampling of natural images in a “lossy” manner, by sam-
pling ˆz according to the context model and then decoding.

Figure 6: Visualization of the latent representation of the
auto-encoder for a high-bpp operating point, with (M ) and
without (M (cid:48)) incorporating an importance map.

Acknowledgements This work was supported by ETH
Z¨urich and by NVIDIA through a GPU grant.

8

[16] C. Shalizi.

Lecture notes on stochastic processes.

http://www.stat.cmu.edu/˜cshalizi/754/
2006/notes/lecture-28.pdf, 2006.
accessed 15-Nov-2017]. 3

[Online;

[17] L. Theis, W. Shi, A. Cunningham, and F. Huszar. Lossy
image compression with compressive autoencoders. In ICLR
2017, 2017. 1, 2, 4, 5, 6

[18] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted An-
chored Neighborhood Regression for Fast Super-Resolution,
pages 111–126. Springer International Publishing, Cham,
2015. 6

[19] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent,
D. Minnen, S. Baluja, M. Covell, and R. Sukthankar. Vari-
able rate image compression with recurrent neural networks.
arXiv preprint arXiv:1511.06085, 2015. 1, 2

[20] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Min-
nen, J. Shor, and M. Covell. Full resolution image com-
arXiv preprint
pression with recurrent neural networks.
arXiv:1608.05148, 2016. 1, 2, 5

[21] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,
A. Graves, et al. Conditional image generation with pixel-
cnn decoders. In Advances in Neural Information Processing
Systems, pages 4790–4798, 2016. 2, 3, 5, 8

[22] A. Van Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel
In International Conference on

recurrent neural networks.
Machine Learning, pages 1747–1756, 2016. 2, 3, 5, 8
[23] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-
In Asilomar
tural similarity for image quality assessment.
Conference on Signals, Systems Computers, 2003, volume 2,
pages 1398–1402 Vol.2, Nov 2003. 2, 5

[24] M. J. Weinberger, J. J. Rissanen, and R. B. Arps. Applica-
tions of universal context modeling to lossless compression
of gray-scale images. IEEE Transactions on Image Process-
ing, 5(4):575–586, 1996. 2

[25] X. Wu, E. Barthel, and W. Zhang. Piecewise 2d autore-
gression for predictive image coding. In Image Processing,
1998. ICIP 98. Proceedings. 1998 International Conference
on, pages 901–904. IEEE, 1998. 2

References

kodak/. 6

[1] Kodak PhotoCD dataset. http://r0k.us/graphics/

[2] E. Agustsson, F. Mentzer, M. Tschannen, L. Cavigelli,
R. Timofte, L. Benini, and L. Van Gool. Soft-to-hard vector
quantization for end-to-end learning compressible represen-
tations. arXiv preprint arXiv:1704.00648, 2017. 1, 2

[3] J. Ball´e, V. Laparra, and E. P. Simoncelli. End-to-end opti-
mization of nonlinear transform codes for perceptual quality.
arXiv preprint arXiv:1607.05006, 2016. 2

[4] J. Ball´e, V. Laparra, and E. P. Simoncelli.

End-
arXiv preprint

to-end optimized image compression.
arXiv:1611.01704, 2016. 1, 2, 6

[5] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5197–5206, 2015. 6

[6] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, pages 448–
456, 2015. 6

[7] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh,
Im-
T. Chinen, S. Jin Hwang, J. Shor, and G. Toderici.
proved lossy image compression with priming and spatially
adaptive bit rates for recurrent networks. arXiv preprint
arXiv:1703.10114, 2017. 5, 6

[8] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 6

[9] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang. Learning con-
volutional networks for content-weighted image compres-
sion. arXiv preprint arXiv:1703.10553, 2017. 1, 2, 4, 5,
8

[10] D. Marpe, H. Schwarz, and T. Wiegand. Context-based adap-
tive binary arithmetic coding in the h. 264/avc video com-
IEEE Transactions on circuits and sys-
pression standard.
tems for video technology, 13(7):620–636, 2003. 2

[11] D. Marpe, H. Schwarz, and T. Wiegand. Context-based adap-
tive binary arithmetic coding in the h. 264/avc video com-
IEEE Transactions on circuits and sys-
pression standard.
tems for video technology, 13(7):620–636, 2003. 2, 3
[12] B. Meyer and P. Tischer. Tmw-a new method for lossless
image compression. ITG FACHBERICHT, pages 533–540,
1997. 2

[13] B. Meyer and P. E. Tischer. Glicbawls-grey level image com-
pression by adaptive weighted least squares. In Data Com-
pression Conference, volume 503, 2001. 2

[14] O. Rippel and L. Bourdev. Real-time adaptive image com-
In Proceedings of the 34th International Con-
pression.
ference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pages 2922–2930, Interna-
tional Convention Centre, Sydney, Australia, 06–11 Aug
2017. PMLR. 2, 5, 6, 8, 11

[15] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. CoRR, abs/1409.0575, 2014. 6

9

Figure 7: 2D vs. 3D CNNs

Algorithm 1 Constructing 3D Masks

Conditional Probability Models for Deep
Image Compression – Suppl. Material

3D probability classiﬁer As mentioned in Section 3.2,
we rely on masked 3D convolutions to enforce the causality
constraint in our probability classiﬁer P .
In a 2D-CNN,
standard 2D convolutions are used in ﬁlter banks, as shown
in Fig. 7 on the left: A W
Cin-dimensional tensor
×
Cout-dimensional tensor using
is mapped to a W (cid:48)
Cout banks of Cin 2D ﬁlters, i.e., ﬁlters can be represented
Cout-dimensional tensors. Note that
as fW ×
all Cin channels are used together, which violates causality:
When we encode, we proceed channel by channel.

fH ×

Cin

H (cid:48)

H

×

×

×

×

Using 3D convolutions, a depth dimension D is intro-
Cin-dimensional tensors
duced. In a 3D-CNN, W
D
H
×
×
×
Cout-dimensional tensors,
are mapped to W (cid:48)
D(cid:48)
H (cid:48)
×
×
Cout-dimensional ﬁlters. Thus,
with fW ×
Cin
fD ×
fH ×
a 3D-CNN slides over the depth dimension, as shown in
Fig. 7 on the right. We use such a 3D-CNN for P , where
we use as input our W
K-dimensional feature map
ˆz, using D = K, Cin = 1 for the ﬁrst layer.

H

×

×

×

×

Figure 8: Left shows a grid of symbols ˆzi, where the black
square denotes some context and the gray cells denote sym-
bols which where previously encoded. Right shows masks.

To explain how we mask the ﬁlters in P , consider the 2D
case in Fig. 8. We want to encode all values ˆzi by iterating
1, . . . , ˆz1).
in raster scan order and by computing p(ˆzi|
We simplify this by instead of relying on all previously en-
c-context around ˆzi (black
coded symbols, we use some c

ˆzi

−

×

10

square in Fig. 8). To satisfy the causality constraint, this
context may only contain values above ˆzi or in the same
row to the left of ˆzi (gray cells). By using the ﬁlter shown
in Fig. 8 in the top right for the ﬁrst layer of a CNN and
the ﬁlter shown in Fig. 8 in the bottom right for subsequent
ﬁlters, we can build a 2D-CNN with a c
c receptive ﬁeld
that forms such a context. We build our 3D-CNN P by gen-
eralizing this idea to 3D, where we construct the mask for
the ﬁlter of the ﬁrst layer as shown in pseudo-code Algo-
rithm 1. The mask for the subsequent layers is constructed
analoguously by replacing “<” in line 7 with “
”. We use
ﬁlter size fW = fH = fD = 3.

≤

×

1: central idx
2: current idx
3: mask
4: for d
5:
6:

←
∈ {
for h

fD)/2
(cid:101)

(fW ·
fH ·
← (cid:100)
1
←
fD-dimensional matrix of zeros
fH ×
fW ×
do
1, . . . , fD}
1, . . . , fH }
∈ {
for w
∈ {
if current idx < central idx then

do
1, . . . , fW }
mask(w, h, d) = 1

do

else

mask(w, h, d) = 0

current idx

current idx + 1

←

7:
8:
9:
10:

11:

fW ×

With this approach, we obtain a 3D-CNN P which oper-
fD-dimensional blocks. We can use P to
ates on fH ×
encode ˆz by iterating over ˆz in such blocks, exhausting ﬁrst
axis w, then axis h, and ﬁnally axis d (like in Algorithm 1).
For each such block, P yields the probability distribution of
the central symbol given the symbols in the block. Due to
the construction of the masks, this probability distribution
only depends on previously encoded symbols.

Multiple compression rates
It is quite straightforward to
obtain multiple operating points in a single network with
our framework: We can simply share the network but use
multiple importance maps. We did a simple experiment
where we trained an autoencoder with 5 different impor-
tance maps. In each iteration, a random importance map
t.
i was picked, and the target entropy was set to i/5
While not tuned for performance, this already yielded a
model competitive with BPG. The following shows the
output of the model for i = 1, 3, 5 (from left to right):

·

On the beneﬁt of 3DCNN and joint training We note
that the points from Table 1 (where we trained different en-

Figure 9: Performance on the Kodak dataset. See text.

tropy models non-jointly as a post-training step) are not di-
rectly comparable with the curve in Fig. 1. This is because
these points are obtained by taking the mean of the MS-
SSIM and bpp values over the Kodak images for a single
model. In contrast, the curve in Fig. 1 is obtained by fol-
lowing the approach of [14], constructing a MS-SSIM vs.
bpp curve per-image via interpolation (see Comparison in
Section 4). In Fig. 9, we show the black curve from Fig. 1,
as well as the mean (MS-SSIM, bpp) points achieved by
the underlying models (
). We also show the points from
◦
Tab. 1 (+). We can see that our masked 3DCNN with joint
training gives a signiﬁcant improvement over the separately
trained 3DCNN, i.e., a 22% reduction in bpp when compar-
ing mean points (the red point is estimated).

Non-realistic images
In Fig. 10, we compare our ap-
proach to BPG on an image from the Manga1097 dataset.
We can see that our approach preserves text well enough to
still be legible, but it is not as crip as BPG (left zoom). On
the other hand, our approach manages to preserve the ﬁne
texture on the face better than BPG (right zoom).

Visual examples The following pages show the ﬁrst four
images of each of our validation sets compressed to low bi-
trate, together with outputs from BPG, JPEG2000 and JPEG
compressed to similar bitrates. We ignored all header infor-
mation for all considered methods when computing the bi-
trate (here and throughout the paper). We note that the only
header our approach requires is the size of the image and an
identiﬁer, e.g., β, specifying the model.

Overall, our images look pleasant to the eye. We see
cases of over-blurring in our outputs, where BPG manages
to keep high frequencies due to its more local approach. An
example is the fences in front of the windows in Fig. 14, top,
or the text in Fig. 15, top. On the other hand, BPG tends to
discard low-contrast high frequencies where our approach
keeps them in the output, like in the door in Fig. 11, top,
or in the hair in Fig. 12, bottom. This may be explained by

7http://www.manga109.org/

Figure 10: Comparison on a non-realistic image. See text.

BPG being optimized for MSE as opposed to our approach
being optimized for MS-SSIM.

JPEG looks extremely blocky for most images due to the

very low bitrate.

11

Ours 0.239 bpp

0.246 bpp BPG

JPEG 2000 0.242 bpp

0.259 bpp JPEG

Ours 0.203 bpp

0.201 bpp BPG

JPEG 2000 0.197 bpp

0.205 bpp JPEG

Figure 11: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Kodak
dataset, along with bit rate.

12

Ours 0.165 bpp

0.164 bpp BPG

JPEG 2000 0.166 bpp

0.166 bpp JPEG

Ours 0.193 bpp

0.209 bpp BPG

JPEG 2000 0.194 bpp

0.203 bpp JPEG

Figure 12: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Kodak
dataset, along with bit rate.

13

Ours 0.385 bpp

0.394 bpp BPG

JPEG 2000 0.377 bpp

0.386 bpp JPEG

Ours 0.365 bpp

0.363 bpp BPG

JPEG 2000 0.363 bpp

0.372 bpp JPEG

Figure 13: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Ur-
ban100 dataset, along with bit rate.

14

Ours 0.435 bpp

0.479 bpp BPG

JPEG 2000 0.437 bpp

0.445 bpp JPEG

Ours 0.345 bpp

0.377 bpp BPG

JPEG 2000 0.349 bpp

0.357 bpp JPEG

Figure 14: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Ur-
ban100 dataset, along with bit rate.

15

Ours 0.355 bpp

0.394 bpp BPG

JPEG 2000 0.349 bpp

0.378 bpp JPEG

Ours 0.263 bpp

0.267 bpp BPG

JPEG 2000 0.254 bpp

0.266 bpp JPEG

Figure 15: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Ima-
geNetTest dataset, along with bit rate.

16

Ours 0.284 bpp

0.280 bpp BPG

JPEG 2000 0.287 bpp

0.288 bpp JPEG

Ours 0.247 bpp

0.253 bpp BPG

JPEG 2000 0.243 bpp

0.252 bpp JPEG

Figure 16: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Ima-
geNetTest dataset, along with bit rate.

17

Ours 0.494 bpp

0.501 bpp BPG

JPEG 2000 0.490 bpp

0.525 bpp JPEG

Ours 0.298 bpp

0.301 bpp BPG

JPEG 2000 0.293 bpp

0.315 bpp JPEG

Figure 17: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the B100
dataset, along with bit rate.

18

Ours 0.315 bpp

0.329 bpp BPG

JPEG 2000 0.311 bpp

0.321 bpp JPEG

Ours 0.363 bpp

0.397 bpp BPG

JPEG 2000 0.369 bpp

0.372 bpp JPEG

Figure 18: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the B100
dataset, along with bit rate.

19

Conditional Probability Models for Deep Image Compression

Fabian Mentzer∗ Eirikur Agustsson∗ Michael Tschannen
mentzerf@vision.ee.ethz.ch

aeirikur@vision.ee.ethz.ch

michaelt@nari.ee.ethz.ch

Radu Timofte
timofter@vision.ee.ethz.ch

Luc Van Gool
vangool@vision.ee.ethz.ch

ETH Z¨urich, Switzerland

9
1
0
2
 
n
u
J
 
4
 
 
]

V
C
.
s
c
[
 
 
4
v
0
6
2
4
0
.
1
0
8
1
:
v
i
X
r
a

Abstract

Deep Neural Networks trained as image auto-encoders
have recently emerged as a promising direction for advanc-
ing the state-of-the-art in image compression. The key chal-
lenge in learning such networks is twofold: To deal with
quantization, and to control the trade-off between recon-
struction error (distortion) and entropy (rate) of the latent
image representation. In this paper, we focus on the latter
challenge and propose a new technique to navigate the rate-
distortion trade-off for an image compression auto-encoder.
The main idea is to directly model the entropy of the latent
representation by using a context model: A 3D-CNN which
learns a conditional probability model of the latent distribu-
tion of the auto-encoder. During training, the auto-encoder
makes use of the context model to estimate the entropy of its
representation, and the context model is concurrently up-
dated to learn the dependencies between the symbols in the
latent representation. Our experiments show that this ap-
proach, when measured in MS-SSIM, yields a state-of-the-
art image compression system based on a simple convolu-
tional auto-encoder.

1. Introduction

Image compression refers to the task of representing im-
ages using as little storage (i.e., bits) as possible. While in
lossless image compression the compression rate is limited
by the requirement that the original image should be per-
fectly reconstructible, in lossy image compression, a greater
reduction in storage is enabled by allowing for some distor-
tion in the reconstructed image. This results in a so-called
rate-distortion trade-off, where a balance is found between
the bitrate R and the distortion d by minimizing d + βR,
where β > 0 balances the two competing objectives. Re-
cently, deep neural networks (DNNs) trained as image auto-
encoders for this task led to promising results, achieving
better performance than many traditional techniques for im-
age compression [19, 20, 17, 4, 2, 9]. Another advantage of

∗The ﬁrst two authors contributed equally.

Figure 1: State-of-the-art performance achieved by our sim-
ple compression system composed of a standard convolu-
tional auto-encoder and a 3D-CNN-based context model.

DNN-based learned compression systems is their adaptabil-
ity to speciﬁc target domains such as areal images or stereo
images, enabling even higher compression rates on these
domains. A key challenge in training such systems is to op-
timize the bitrate R of the latent image representation in the
auto-encoder. To encode the latent representation using a
ﬁnite number of bits, it needs to be discretized into symbols
(i.e., mapped to a stream of elements from some ﬁnite set
of values). Since discretization is non-differentiable, this
presents challenges for gradient-based optimization meth-
ods and many techniques have been proposed to address
them. After discretization, information theory tells us that
the correct measure for bitrate R is the entropy H of the
resulting symbols. Thus the challenge, and the focus of this
paper, is how to model H such that we can navigate the
trade-off d + βH during optimization of the auto-encoder.

Our proposed method is based on leveraging context
models, which were previously used as techniques to im-

1

prove coding rates for already-trained models [4, 20, 9, 14],
directly as an entropy term in the optimization. We concur-
rently train the auto-encoder and the context model with re-
spect to each other, where the context model learns a convo-
lutional probabilistic model of the image representation in
the auto-encoder, while the auto-encoder uses it for entropy
estimation to navigate the rate-distortion trade-off. Further-
more, we generalize our formulation to spatially-aware net-
works, which use an importance map to spatially attend the
bitrate representation to the most important regions in the
compressed representation. The proposed techniques lead
to a simple image compression system1, which achieves
state-of-the-art performance when measured with the pop-
ular multi-scale structural similarity index (MS-SSIM) dis-
tortion metric [23], while being straightforward to imple-
ment with standard deep-learning toolboxes.

2. Related work

Full-resolution image compression using DNNs has at-
tracted considerable attention recently. DNN architectures
commonly used for image compression are auto-encoders
[17, 4, 2, 9] and recurrent neural networks (RNNs) [19, 20].
The networks are typically trained to minimize the mean-
squared error (MSE) between original and decompressed
image [17, 4, 2, 9], or using perceptual metrics such as
MS-SSIM [20, 14]. Other notable techniques involve pro-
gressive encoding/decoding strategies [19, 20], adversarial
training [14], multi-scale image decompositions [14], and
generalized divisive normalization (GDN) layers [4, 3].

Context models and entropy estimation—the focus of
the present paper—have a long history in the context of
engineered compression methods, both lossless and lossy
[24, 12, 25, 13, 10]. Most of the recent DNN-based lossy
image compression approaches have also employed such
techniques in some form. [4] uses a binary context model
for adaptive binary arithmetic coding [11]. The works of
[20, 9, 14] use learned context models for improved cod-
ing performance on their trained models when using adap-
tive arithmetic coding. [17, 2] use non-adaptive arithmetic
coding but estimate the entropy term with an independence
assumption on the symbols.

Also related is the work of van den Oord et al. [22, 21],
who proposed PixelRNN and PixelCNN, powerful RNN-
and CNN-based context models for modeling the distribu-
tion of natural images in a lossless setting, which can be
used for (learned) lossless image compression as well as
image generation.

3. Proposed method

Given a set of training images

, we wish to learn a
compression system which consists of an encoder, a quan-

X

1https://github.com/fab-jul/imgcomp-cvpr

→

→ C

discretizes the coordinates of z to L =

Rm maps an
tizer, and a decoder. The encoder E : Rd
image x to a latent representation z = E(x). The quantizer
Q : R
cen-
ters, obtaining ˆz with ˆzi := Q(zi)
, which can be loss-
lessly encoded into a bitstream. The decoder D then forms
the reconstructed image ˆx = D(ˆz) from the quantized la-
tent representation ˆz, which is in turn (losslessy) decoded
from the bitstream. We want the encoded representation ˆz
to be compact when measured in bits, while at the same time
we want the distortion d(x, ˆx) to be small, where d is some
measure of reconstruction error, such as MSE or MS-SSIM.
This results in the so-called rate-distortion trade-off

∈ C

|C|

d(x, ˆx) + βH(ˆz),

(1)

where H denotes the cost of encoding ˆz to bits, i.e., the en-
tropy of ˆz. Our system is realized by modeling E and D as
convolutional neural networks (CNNs) (more speciﬁcally,
as the encoder and decoder, respectively, of a convolutional
auto-encoder) and minimizing (1) over the training set
,
X
where a large/small β draws the system towards low/high
average entropy H. In the next sections, we will discuss
how we quantize z and estimate the entropy H(ˆz). We note
that as E, D are CNNs, ˆz will be a 3D feature map, but for
simplicity of exposition we will denote it as a vector with
equally many elements. Thus, ˆzi refers to the i-th element
of the feature map, in raster scan order (row by column by
channel).

3.1. Quantization

We adopt the scalar variant of the quantization approach
proposed in [2] to quantize z, but simplify it using ideas
from [17]. Speciﬁcally, given centers
, cL} ⊂
· · ·
R, we use nearest neighbor assignments to compute
ˆzi = Q(zi) := arg minj(cid:107)

zi −
but rely on (differentiable) soft quantization

,
cj(cid:107)

c1,

(2)

=

C

{

˜zi =

L
(cid:88)

j=1

exp(
σ
−
l=1 exp(

(cid:80)L

)
zi −
cj(cid:107)
(cid:107)
)
cl(cid:107)
zi −
σ
(cid:107)
−

cj

(3)

C

to compute gradients during the backward pass. This com-
bines the beneﬁt of [2] where the quantization is restricted
(instead of the ﬁxed (non-
to a ﬁnite set of learned centers
learned) integer grid as in [17]) and the simplicity of [17],
where a differentiable approximation of quantization is only
used in the backward pass, avoiding the need to choose
an annealing strategy (i.e., a schedule for σ) as in [2] to
drive the soft quantization (3) to hard assignments (2) dur-
ing training. In TensorFlow, this is implemented as
¯zi = tf.stopgradient(ˆzi −

˜zi) + ˜zi.

(4)

We note that for forward pass computations, ¯zi = ˆzi, and
thus we will continue writing ˆzi for the latent representa-
tion.

2

3.2. Entropy estimation

the index I(ˆzi) of ˆzi in

with a cross entropy loss:

To model the entropy H(ˆz) we build on the approach
of PixelRNN [22] and factorize the distribution p(ˆz) as a
product of conditional distributions

p(ˆz) =

ˆzi
p(ˆzi|

−

1, . . . , ˆz1),

(5)

m
(cid:89)

i=1

where the 3D feature volume ˆz is indexed in raster
scan order. We then use a neural network P (ˆz), which
we refer to as a context model,
to estimate each term
ˆzi
p(ˆzi|

1, . . . , ˆz1):

−

−

−

≈

(6)

Pi,l(ˆz)

1, . . . , ˆz1),

ˆzi
p(ˆzi = cl|
where Pi,l speciﬁes for every 3D location i in ˆz the prob-
with l = 1, . . . , L. We re-
abilites of each symbol in
C
fer to the resulting approximate distribution as q(ˆz) :=
(cid:81)m
i=1 Pi,I(ˆzi)(ˆz), where I(ˆzi) denotes the index of ˆzi in
.
Since the conditional distributions p(ˆzi|
1, . . . , ˆz1)
ˆzi
1, . . . , ˆz1, this imposes a
only depend on previous values ˆzi
causality constraint on the network P : While P may com-
pute Pi,l in parallel for i = 1, . . . , m, l = 1, . . . , L, it needs
to make sure that each such term only depends on previous
values ˆzi

1, . . . , ˆz1.

C

−

−

The authors of PixelCNN [22, 21] study the use of 2D-
CNNs as causal conditional models over 2D images in a
lossless setting, i.e., treating the RGB pixels as symbols.
They show that the causality constraint can be efﬁciently en-
forced using masked ﬁlters in the convolution. Intuitively,
the idea is as follows: If for each layer the causality con-
dition is satisﬁed with respect to the spatial coordinates of
the layer before, then by induction the causality condition
will hold between the output layer and the input. Satisfying
the causality condition for each layer can be achieved with
proper masking of its weight tensor, and thus the entire net-
work can be made causal only through the masking of its
weights. Thus, the entire set of probabilities Pi,l for all (2D)
spatial locations i and symbol values l can be computed in
parallel with a fully convolutional network, as opposed to
modeling each term p(ˆzi|
1,
ˆzi
In our case, ˆz is a 3D symbol volume, with as much as
K = 64 channels. We therefore generalize the approach
of PixelCNN to 3D convolutions, using the same idea of
masking the ﬁlters properly in every layer of the network.
This enables us to model P efﬁciently, with a light-weight2
3D-CNN which slides over ˆz, while properly respecting the
causality constraint. We refer to the supplementary material
for more details.

, ˆz1) separately.

· · ·

−

As in [21], we learn P by training it for maximum like-
lihood, or equivalently (see [16]) by training Pi,: to classify

2We use a 4-layer network, compared to 15 layers in [22].

3

C

CE := Eˆz
∼

p(ˆz)[

m
(cid:88)

−

i=1

log Pi,I(ˆzi)].

(7)

Using the well-known property of cross entropy as the cod-
ing cost when using the wrong distribution q(ˆz) instead of
the true distribution p(ˆz), we can also view the CE loss as
an estimate of H(ˆz) since we learn P such that P = q
p.
That is, we can compute

≈

p(ˆz)[

log(p(ˆz))]

H(ˆz) = Eˆz
∼
= Eˆz
∼

p(ˆz)[

ˆzi
log p(ˆzi|

−

1,

−

· · ·

, ˆz1)]

Eˆz
∼

p(ˆz)[

≈

log q(ˆzi|

ˆzi

−

1,

−

· · ·

, ˆz1)]

−
m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

= Eˆz
∼

p(ˆz)[

−

i=1

log Pi,I(ˆzi)]

= CE

(8)

(9)

(10)

(11)

(12)

Therefore, when training the auto-encoder we can indirectly
minimize H(ˆz) through the cross entropy CE. We refer to
argument in the expectation of (7),

C(ˆz) :=

log Pi,I(ˆzi),

(13)

m
(cid:88)

−

i=1

as the coding cost of the latent image representation, since
this reﬂects the coding cost incurred when using P as a con-
text model with an adaptive arithmetic encoder [11]. From
the application perspective, minimizing the coding cost is
actually more important than the (unknown) true entropy,
since it reﬂects the bitrate obtained in practice.

To backpropagate through P (ˆz) we use the same ap-
proach as for the encoder (see (4)). Thus, like the decoder
D, P only sees the (discrete) ˆz in the forward pass, whereas
the gradient of the soft quantization ˜z is used for the back-
ward pass.

3.3. Concurrent optimization

Given an auto-encoder (E, D), we can train P to model
the dependencies of the entries of ˆz as described in the pre-
vious section by minimizing (7). On the other hand, us-
ing the model P , we can obtain an estimate of H(ˆz) as
in (12) and use this estimate to adjust (E, D) such that
d(x, D(Q(E(x)))) + βH(ˆz) is reduced, thereby navigating
the rate distortion trade-off. Therefore, it is natural to con-
currently learn P (with respect to its own loss), and (E, D)
(with respect to the rate distortion trade-off) during train-
ing, such that all models which the losses depend on are
continuously updated.

3.4. Importance map for spatial bit-allocation

8 ×

H
8 ×

Recall that since E and D are CNNs, ˆz is a 3D feature-
map. For example, if E has three stride-2 convolution layers
and the bottleneck has K channels, the dimensions of ˆz will
be W
K. A consequence of this formulation is that
we are using equally many symbols in ˆz for each spatial
location of the input image x. It is known, however, that in
practice there is great variability in the information content
across spatial locations (e.g., the uniform area of blue sky
vs. the ﬁne-grained structure of the leaves of a tree).

This can in principle be accounted for automatically in
the trade-off between the entropy and the distortion, where
the network would learn to output more predictable (i.e.,
low entropy) symbols for the low information regions, while
making room for the use of high entropy symbols for the
more complex regions. More precisely, the formulation in
(7) already allows for variable bit allocation for different
spatial regions through the context model P .

However, this arguably requires a quite sophisticated
(and hence computationally expensive) context model, and
we ﬁnd it beneﬁcial to follow Li et al. [9] instead by using
an importance map to help the CNN attend to different re-
gions of the image with different amounts of bits. While
[9] uses a separate network for this purpose, we consider a
simpliﬁed setting. We take the last layer of the encoder E,
1. We
and add a second single-channel output y
∈
H
K
expand this single channel y into a mask m
8 ×
of the same dimensionality as z as follows:

H
8 ×
8 ×

8 ×
R W

R W

∈

prior during training, i.e., costing each 1 bit to encode. The
importance map is thus their principal tool for controlling
the bitrate, since they thereby avoid encoding all the bits
in the representation. In contrast, we stick to the formula-
tion in (5) where the dependencies between the symbols are
modeled during training. We then use the importance map
as an architectural constraint and use their suggested cod-
ing strategy to obtain an alternative estimate for the entropy
H(ˆz), as follows.

(cid:100)

m

We observe that we can recover

from ˆz by count-
ing the number of consecutive zero symbols at the end of
each column ˆzi,j,:.3
is therefore a function of the
masked ˆz, i.e.,
m
as de-
(cid:101)
scribed, which means that we have for the conditional en-
tropy H(
(cid:100)

(cid:100)
= g(ˆz) for g recovering

ˆz) = 0. Now, we have

m
(cid:101)

m
(cid:101)

m

(cid:100)

(cid:101)

(cid:100)

(cid:101)|
H(ˆz) = H(
(cid:100)
= H(ˆz,
= H(ˆz

m

ˆz) + H(ˆz)

(15)

)

(cid:101)|
m
(cid:101)
) + H(
(cid:100)
If we treat the entropy of the mask, H(
), as constant
during optimization of the auto-encoder, we can then indi-
rectly minimize H(ˆz) through H(ˆz

m
(cid:101)
m
(cid:100)

(cid:100)
m
(cid:101)

(17)

(16)

|(cid:100)

).

(cid:101)

To estimate H(ˆz

m), we use the same factorization of p
|
m
as in (5), but since the mask
is known we have p(ˆzi =
(cid:100)
(cid:101)
c0) = 1 deterministic for the 3D locations i in ˆz where
the mask is zero. The logs of the corresponding terms in (9)
then evaluate to 0. The remaining terms, we can model with
the same context model Pi,l(ˆz), which results in

m).
|

mi,j,k =






1
(yi,j −
0

k)

if k < yi,j
if k
yi,j ≤
if k + 1 > yi,j

≤

k + 1

,

(14)

H(ˆz

m

)
(cid:101)

|(cid:100)

≈

Eˆz

p(ˆz)[

∼

mi(cid:101)

−(cid:100)

log Pi,I(ˆzi)],

(18)

m
(cid:88)

i=1

where yi,j denotes the value of y at spatial location (i, j).
The transition value for k
k + 1 is such that the
mask smoothly transitions from 0 to 1 for non-integer val-
ues of y.

yi,j ≤

≤

We then mask z by pointwise multiplication with the bi-
m
. Since the ceiling op-
(cid:101)
is not differentiable, as done by [17, 9], we use

narization of m, i.e., z
erator
identity for the backward pass.

(cid:12) (cid:100)

(cid:100)·(cid:101)

←

z

With this modiﬁcation, we have simply changed the ar-
chitecture of E slightly such that it can easily “zero out”
portions of columns zi,j,: of z (the rest of the network stays
the same, so that (2) still holds for example). As suggested
by [9], the so-obtained structure in z presents an alterna-
tive coding strategy: Instead of losslessly encoding the en-
tire symbol volume ˆz, we could ﬁrst (separately) encode the
m
, and then for each column ˆzi,j,: only encode the
mask
(cid:100)
(cid:101)
ﬁrst
+ 1 symbols, since the remaining ones are the
mi,j(cid:101)
constant Q(0), which we refer to as the zero symbol.

(cid:100)

Work [9] uses binary symbols (i.e.,

) and as-
sumes independence between the symbols and a uniform

0, 1
}
{

=

C

where mi denotes the i-th element of m (in the same raster
scan order as ˆz).

Similar to the coding cost (13), we refer to the argument

in the expectation in (18),

M C(ˆz) :=

mi(cid:101)

−(cid:100)

log Pi,I(ˆzi)

(19)

m
(cid:88)

i=1

as the masked coding cost of ˆz.

While the entropy estimate (18) is almost estimating the
same quantity as (7) (only differing by H(
)), it has the
beneﬁt of being weighted by mi. Therefore, the encoder E
has an obvious path to control the entropy of ˆz, by simply
increasing/decreasing the value of y for some spatial loca-
tion of x and thus obtaining fewer/more zero entries in m.
When the context model P (ˆz) is trained, however, we
still train it with respect to the formulation in (8), so it does

m
(cid:101)

(cid:100)

3If z contained zeros before it was masked, we might overestimate the
number of 0 entries in (cid:100)m(cid:101). However, we can redeﬁne those entries of m
as 0 and this will give the same result after masking.

4

not have direct access to the mask m and needs to learn the
dependencies on the entire masked symbol volume ˆz. This
means that when encoding an image, we can stick to stan-
dard adaptive arithmetic coding over the entire bottleneck,
without needing to resort to a two-step coding process as in
[9], where the mask is ﬁrst encoded and then the remaining
symbols. We emphasize that this approach hinges critically
on the context model P and the encoder E being trained
concurrently as this allows the encoder to learn a meaning-
ful (in terms of coding cost) mask with respect to P (see the
next section).

≈

m

) being ignored.
(cid:101)

In our experiments we observe that during training, the
two entropy losses (7) and (18) converge to almost the same
3.5% smaller due to
value, with the latter being around
H(
(cid:100)
While the importance map is not crucial for optimal rate-
distortion performance, if the channel depth K is adjusted
carefully, we found that we could more easily control the
entropy of ˆz through β when using a ﬁxed K, since the net-
work can easily learn to ignore some of the channels via the
importance map. Furthermore, in the supplementary mate-
rial we show that by using multiple importance maps for a
single network, one can obtain a single model that supports
multiple compression rates.

3.5. Putting the pieces together

We made an effort to carefully describe our formulation
and its motivation in detail. While the description is lengthy,
when putting the resulting pieces together we get a quite
straightforward pipeline for learned image compression, as
follows.

Given the set of training images

, we initialize (fully

convolutional) CNNs E, D, and P , as well as the centers
C
of the quantizer Q. Then, we train over minibatches
XB =
x(1),
. At each iteration, we
of crops from
{
take one gradient step for the auto-encoder (E, D) and the
quantizer Q, with respect to the rate-distortion trade-off

, x(B)

· · ·

X

}

X

2. Expand importance map y to mask m via (14)

3. Mask z, i.e., z

z

m

←

(cid:12) (cid:100)

(cid:101)

4. Quantize ˆz = Q(z)

5. Compute the context P (ˆz)

6. Decode ˆx = D(ˆz),

which can be computed in parallel over the minibatch on a
GPU since all the models are fully convolutional.

3.6. Relationship to previous methods

We are not the ﬁrst to use context models for adaptive
arithmetic coding to improve the performance in learned
deep image compression. Work [20] uses a PixelRNN-like
architecture [22] to train a recurrent network as a context
model for an RNN-based compression auto-encoder. Li et
al. [9] extract cuboid patches around each symbol in a bi-
nary feature map, and feed them to a convolutional context
model. Both these methods, however, only learn the context
model after training their system, as a post-processing step
to boost coding performance.

In contrast, our method directly incorporates the context
model as the entropy term for the rate-distortion term (1)
of the auto-encoder, and trains the two concurrently. This
is done at little overhead during training, since we adopt a
3D-CNN for the context model, using PixelCNN-inspired
[21] masking of the weights of each layer to ensure causal-
ity in the context model. Adopting the same approach to the
context models deployed by [20] or [9] would be non-trivial
since they are not designed for fast feed-forward computa-
tion. In particular, while the context model of [9] is also
convolutional, its causality is enforced through masking the
inputs to the network, as opposed to our masking of the
weights of the networks. This means their context model
needs to be run separately with a proper input cuboid for
each symbol in the volume (i.e., not fully convolutionally).

LE,D,Q =

1
B

B
(cid:88)

j=1

d(x(j), ˆx(j)) + βM C(ˆz(j)),

(20)

4. Experiments

which is obtained by combining (1) with the estimate (18)
& (19) and taking the batch sample average. Furthermore,
we take a gradient step for the context model P with respect
to its objective (see (7) & (13))

LP :=

1
B

B
(cid:88)

j=1

d(x(j), ˆx(j)) + βC(ˆz(j)).

(21)

To compute these two batch losses, we need to perform

the following computation for each x

∈ XB:

1. Obtain compressed (latent) representation z and im-
portance map y from the encoder: (z, y) = E(x)

Architecture Our auto-encoder has a similar architecture
as [17] but with more layers, and is described in Fig. 2. We
adapt the number of channels K in the latent representation
for different models. For the context model P , we use a
simple 4-layer 3D-CNN as described in Fig. 3.

Distortion measure Following [7, 14], we use the multi-
scale structural similarity index (MS-SSIM) [23] as mea-
sure of distortion d(x, ˆx) = 100
MS-SSIM(x, ˆx))
for our models. MS-SSIM reportedly correlates better with
human perception of distortion than mean squared error
(MSE). We train and test all our models using MS-SSIM.

(1

−

·

5

while using a moderately large β = 10. We use a small
regularization on the weights and note that we achieve very
stable training. We trained our models for 6 epochs, which
took around 24h per model on a single GPU. For P , we use
a LR of 10−

4 and the same decay schedule.

Datasets We train on the the ImageNet dataset from
the Large Scale Visual Recognition Challenge 2012
(ILSVRC2012) [15]. As a preprocessing step, we take ran-
dom 160
160 crops, and randomly ﬂip them. We set aside
100 images from ImageNet as a testing set, ImageNetTest.
Furthermore, we test our method on the widely used Ko-
dak [1] dataset. To asses performance on high-quality full-
resolution images, we also test on the datasets B100 [18]
and Urban100 [5], commonly used in super-resolution.

×

Other codecs We compare to JPEG, using libjpeg4, and
JPEG2000, using the Kakadu implementation5. We also
compare to the lesser known BPG6, which is based on
HEVC, the state-of-the-art in video compression, and which
outperforms JPEG and JPEG2000. We use BPG in the non-
default 4:4:4 chroma format, following [14].

Comparison Like [14], we proceed as follows to com-
pare to other methods. For each dataset, we compress each
image using all our models. This yields a set of (bpp, MS-
SSIM) points for each image, which we interpolate to get
a curve for each image. We ﬁx a grid of bpp values, and
average the curves for each image at each bpp grid value
(ignoring those images whose bpp range does not include
the grid value, i.e., we do not extrapolate). We do this for
our method, BPG, JPEG, and JPEG2000. Due to code being
unavailable for the related works in general, we digitize the
Kodak curve from Rippel & Bourdev [14], who have care-
fully collected the curves from the respective works. With
this, we also show the results of Rippel & Bourdev [14],
Johnston et al. [7], Ball´e et al. [4], and Theis et al. [17].
To validate that our estimated MS-SSIM is correctly im-
plemented, we independently generated the BPG curves for
Kodak and veriﬁed that they matched the one from [14].

Results Fig. 1 shows a comparison of the aforementioned
methods for Kodak. Our method outperforms BPG, JPEG,
and JPEG2000, as well as the neural network based ap-
proaches of Johnston et al. [7], Ball´e et al. [4], and Theis
et al. [17]. Furthermore, we achieve performance compara-
ble to that of Rippel & Bourdev [14]. This holds for all bpps
we tested, from 0.3 bpp to 0.9 bpp. We note that while Rip-
pel & Bourdev and Johnston et al. also train to maximize
(MS-)SSIM, the other methods minimize MSE.

4http://libjpeg.sourceforge.net/
5http://kakadusoftware.com/
6https://bellard.org/bpg/

Figure 2: The architecture of our auto-encoder. Dark gray
blocks represent residual units. The upper part represents
the encoder E, the lower part the decoder D. For the en-
coder, “k5 n64-2” represents a convolution layer with ker-
nel size 5, 64 output channels and a stride of 2. For the de-
coder it represents the equivalent deconvolution layer. All
convolution layers are normalized using batch norm [6], and
use SAME padding. Masked quantization is the quantiza-
tion described in Section 3.4. Normalize normalizes the in-
put to [0, 1] using a mean and variance obtained from a sub-
set of the training set. Denormalize is the inverse operation.

Figure 3:
The architecture of our context model.
“3D k3 n24” refers to a 3D masked convolution with ﬁl-
ter size 3 and 24 output channels. The last layer outputs L
values for each voxel in ˆz.

·

10−

Training We use the Adam optimizer [8] with a mini-
batch size of 30 to train seven models. Each model is trained
to maximize MS-SSIM directly. As a baseline, we used a
3 for each model, but found it
learning rate (LR) of 4
beneﬁcial to vary it slightly for different models. We set
σ = 1 in the smooth approximation (3) used for gradient
backpropagation through Q. To make the model more pre-
dictably land at a certain bitrate t when optimizing (1), we
found it helpful to clip the rate term (i.e., replace the entropy
term βH with max(t, βH)), such that the entropy term is
“switched off” when it is below t. We found this did not
hurt performance. We decay the learning rate by a factor 10
every two epochs. To obtain models for different bitrates,
we adapt the target bitrate t and the number of channels K,

6

Figure 4: Performance of our approach on ImageNetTest, B100, Urban100, where we out-
perform BPG, JPEG and JPEG2000 in MS-SSIM.

Ours 0.124bpp

0.147 bpp BPG

JPEG2000 0.134bpp

0.150bpp JPEG

Figure 5: Example image (kodim21) from the Kodak testing set, compressed with different methods.

In each of the other testing sets, we also outperform
BPG, JPEG, and JPEG2000 over the reported bitrates, as
shown in Fig. 4.

In Fig. 5, we compare our approach to BPG, JPEG,
and JPEG2000 visually, using very strong compression on
kodim21 from Kodak. It can be seen that the output of our
network is pleasant to look at. Soft structures like the clouds
are very well preserved. BPG appears to handle high fre-
quencies better (see, e.g., the fence) but loses structure in
the clouds and in the sea. Like JPEG2000, it produces block
artifacts. JPEG breaks down at this rate. We refer to the
supplementary material for further visual examples.

Ablation study: Context model
In order to show the ef-
fectiveness of the context model, we performed the follow-

ing ablation study. We trained the auto-encoder without en-
tropy loss, i.e., β = 0 in (20), using L = 6 centers and
K = 16 channels. On Kodak, this model yields an av-
erage MS-SSIM of 0.982, at an average rate of 0.646 bpp
(calculated assuming that we need log2(L) = 2.59 bits per
symbol). We then trained three different context models for
this auto-encoder, while keeping the auto-encoder ﬁxed: A
zeroth order context model which uses a histogram to esti-
mate the probability of each of the L symbols; a ﬁrst order
(one-step prediction) context model, which uses a condi-
tional histogram to estimate the probability of each of the
L symbols given the previous symbol (scanning ˆz in raster
order); and P , i.e., our proposed context model. The result-
ing average rates are shown in Table 1. Our context model

7

reduces the rate by 10 %, even though the auto-encoder was
optimized using a uniform prior (see supplementary mate-
rial for a detailed comparison of Table 1 and Fig. 1).

5. Discussion

rate
Model
0.646 bpp
Baseline (Uniform)
0.642 bpp
Zeroth order
First order
0.627 bpp
Our context model P 0.579 bpp

Table 1: Rates for different context models, for the same
architecture (E, D).

Importance map As described in detail in Section 3.4,
we use an importance map to dynamically alter the number
of channels used at different spatial locations to encode an
image. To visualize how this helps, we trained two auto-
encoders M and M (cid:48), where M uses an importance map
and at most K = 32 channels to compress an image, and
M (cid:48) compresses without importance map and with K = 16
channels (this yields a rate for M (cid:48) similar to that of M ). In
Fig. 6, we show an image from ImageNetTest along with
the same image compressed to 0.463 bpp by M and com-
pressed to 0.504 bpp by M (cid:48). Furthermore, Fig. 6 shows the
importance map produced by M , as well as ordered visual-
izations of all channels of the latent representation for both
M and M (cid:48). Note how for M , channels with larger index
are sparser, showing how the model can spatially adapt the
number of channels. M (cid:48) uses all channels similarly.

Input

Importance map of M

Output of M

Latent representation of M

Output of M (cid:48)

Latent representation of M (cid:48)

Our experiments showed that combining a convolutional
auto-encoder with a lightweight 3D-CNN as context model
and training the two networks concurrently leads to a highly
effective image compression system. Not only were we
able to clearly outperform state-of-the-art engineered com-
pression methods including BPG and JPEG2000 in terms
of MS-SSIM, but we also obtained performance compet-
itive with the current state-of-the-art learned compression
method from [14]. In particular, our method outperforms
BPG and JPEG2000 in MS-SSIM across four different
testing sets (ImageNetTest, Kodak, B100, Urban100), and
does so signiﬁcantly, i.e., the proposed method generalizes
well. We emphasize that our method relies on elemen-
tary techniques both in terms of the architecture (standard
convolutional auto-encoder with importance map, convo-
lutional context model) and training procedure (minimize
the rate-distortion trade-off and the negative log-likelihood
for the context model), while [14] uses highly specialized
techniques such as a pyramidal decomposition architecture,
adaptive codelength regularization, and multiscale adver-
sarial training.

The ablation study for the context model showed that our
3D-CNN-based context model is signiﬁcantly more power-
ful than the ﬁrst order (histogram) and second order (one-
step prediction) baseline context models. Further, our ex-
periments suggest that the importance map learns to con-
densate the image information in a reduced number of chan-
nels of the latent representation without relying on explicit
supervision. Notably, the importance map is learned as a
part of the image compression auto-encoder concurrently
with the auto-encoder and the context model, without in-
troducing any optimization difﬁculties. In contrast, in [9]
the importance map is computed using a separate network,
learned together with the auto-encoder, while the context
model is learned separately.

6. Conclusions

In this paper, we proposed the ﬁrst method for learning
a lossy image compression auto-encoder concurrently with
a lightweight context model by incorporating it into an en-
tropy loss for the optimization of the auto-encoder, leading
to performance competitive with the current state-of-the-art
in deep image compression [14].

Future works could explore heavier and more power-
ful context models, as those employed in [22, 21]. This
could further improve compression performance and allow
for sampling of natural images in a “lossy” manner, by sam-
pling ˆz according to the context model and then decoding.

Figure 6: Visualization of the latent representation of the
auto-encoder for a high-bpp operating point, with (M ) and
without (M (cid:48)) incorporating an importance map.

Acknowledgements This work was supported by ETH
Z¨urich and by NVIDIA through a GPU grant.

8

[16] C. Shalizi.

Lecture notes on stochastic processes.

http://www.stat.cmu.edu/˜cshalizi/754/
2006/notes/lecture-28.pdf, 2006.
accessed 15-Nov-2017]. 3

[Online;

[17] L. Theis, W. Shi, A. Cunningham, and F. Huszar. Lossy
image compression with compressive autoencoders. In ICLR
2017, 2017. 1, 2, 4, 5, 6

[18] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted An-
chored Neighborhood Regression for Fast Super-Resolution,
pages 111–126. Springer International Publishing, Cham,
2015. 6

[19] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent,
D. Minnen, S. Baluja, M. Covell, and R. Sukthankar. Vari-
able rate image compression with recurrent neural networks.
arXiv preprint arXiv:1511.06085, 2015. 1, 2

[20] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Min-
nen, J. Shor, and M. Covell. Full resolution image com-
arXiv preprint
pression with recurrent neural networks.
arXiv:1608.05148, 2016. 1, 2, 5

[21] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,
A. Graves, et al. Conditional image generation with pixel-
cnn decoders. In Advances in Neural Information Processing
Systems, pages 4790–4798, 2016. 2, 3, 5, 8

[22] A. Van Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel
In International Conference on

recurrent neural networks.
Machine Learning, pages 1747–1756, 2016. 2, 3, 5, 8
[23] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-
In Asilomar
tural similarity for image quality assessment.
Conference on Signals, Systems Computers, 2003, volume 2,
pages 1398–1402 Vol.2, Nov 2003. 2, 5

[24] M. J. Weinberger, J. J. Rissanen, and R. B. Arps. Applica-
tions of universal context modeling to lossless compression
of gray-scale images. IEEE Transactions on Image Process-
ing, 5(4):575–586, 1996. 2

[25] X. Wu, E. Barthel, and W. Zhang. Piecewise 2d autore-
gression for predictive image coding. In Image Processing,
1998. ICIP 98. Proceedings. 1998 International Conference
on, pages 901–904. IEEE, 1998. 2

References

kodak/. 6

[1] Kodak PhotoCD dataset. http://r0k.us/graphics/

[2] E. Agustsson, F. Mentzer, M. Tschannen, L. Cavigelli,
R. Timofte, L. Benini, and L. Van Gool. Soft-to-hard vector
quantization for end-to-end learning compressible represen-
tations. arXiv preprint arXiv:1704.00648, 2017. 1, 2

[3] J. Ball´e, V. Laparra, and E. P. Simoncelli. End-to-end opti-
mization of nonlinear transform codes for perceptual quality.
arXiv preprint arXiv:1607.05006, 2016. 2

[4] J. Ball´e, V. Laparra, and E. P. Simoncelli.

End-
arXiv preprint

to-end optimized image compression.
arXiv:1611.01704, 2016. 1, 2, 6

[5] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5197–5206, 2015. 6

[6] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, pages 448–
456, 2015. 6

[7] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh,
Im-
T. Chinen, S. Jin Hwang, J. Shor, and G. Toderici.
proved lossy image compression with priming and spatially
adaptive bit rates for recurrent networks. arXiv preprint
arXiv:1703.10114, 2017. 5, 6

[8] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 6

[9] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang. Learning con-
volutional networks for content-weighted image compres-
sion. arXiv preprint arXiv:1703.10553, 2017. 1, 2, 4, 5,
8

[10] D. Marpe, H. Schwarz, and T. Wiegand. Context-based adap-
tive binary arithmetic coding in the h. 264/avc video com-
IEEE Transactions on circuits and sys-
pression standard.
tems for video technology, 13(7):620–636, 2003. 2

[11] D. Marpe, H. Schwarz, and T. Wiegand. Context-based adap-
tive binary arithmetic coding in the h. 264/avc video com-
IEEE Transactions on circuits and sys-
pression standard.
tems for video technology, 13(7):620–636, 2003. 2, 3
[12] B. Meyer and P. Tischer. Tmw-a new method for lossless
image compression. ITG FACHBERICHT, pages 533–540,
1997. 2

[13] B. Meyer and P. E. Tischer. Glicbawls-grey level image com-
pression by adaptive weighted least squares. In Data Com-
pression Conference, volume 503, 2001. 2

[14] O. Rippel and L. Bourdev. Real-time adaptive image com-
In Proceedings of the 34th International Con-
pression.
ference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pages 2922–2930, Interna-
tional Convention Centre, Sydney, Australia, 06–11 Aug
2017. PMLR. 2, 5, 6, 8, 11

[15] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. CoRR, abs/1409.0575, 2014. 6

9

Figure 7: 2D vs. 3D CNNs

Algorithm 1 Constructing 3D Masks

Conditional Probability Models for Deep
Image Compression – Suppl. Material

3D probability classiﬁer As mentioned in Section 3.2,
we rely on masked 3D convolutions to enforce the causality
constraint in our probability classiﬁer P .
In a 2D-CNN,
standard 2D convolutions are used in ﬁlter banks, as shown
in Fig. 7 on the left: A W
Cin-dimensional tensor
×
Cout-dimensional tensor using
is mapped to a W (cid:48)
Cout banks of Cin 2D ﬁlters, i.e., ﬁlters can be represented
Cout-dimensional tensors. Note that
as fW ×
all Cin channels are used together, which violates causality:
When we encode, we proceed channel by channel.

fH ×

Cin

H (cid:48)

H

×

×

×

×

Using 3D convolutions, a depth dimension D is intro-
Cin-dimensional tensors
duced. In a 3D-CNN, W
D
H
×
×
×
Cout-dimensional tensors,
are mapped to W (cid:48)
D(cid:48)
H (cid:48)
×
×
Cout-dimensional ﬁlters. Thus,
with fW ×
Cin
fD ×
fH ×
a 3D-CNN slides over the depth dimension, as shown in
Fig. 7 on the right. We use such a 3D-CNN for P , where
we use as input our W
K-dimensional feature map
ˆz, using D = K, Cin = 1 for the ﬁrst layer.

H

×

×

×

×

Figure 8: Left shows a grid of symbols ˆzi, where the black
square denotes some context and the gray cells denote sym-
bols which where previously encoded. Right shows masks.

To explain how we mask the ﬁlters in P , consider the 2D
case in Fig. 8. We want to encode all values ˆzi by iterating
1, . . . , ˆz1).
in raster scan order and by computing p(ˆzi|
We simplify this by instead of relying on all previously en-
c-context around ˆzi (black
coded symbols, we use some c

ˆzi

−

×

10

square in Fig. 8). To satisfy the causality constraint, this
context may only contain values above ˆzi or in the same
row to the left of ˆzi (gray cells). By using the ﬁlter shown
in Fig. 8 in the top right for the ﬁrst layer of a CNN and
the ﬁlter shown in Fig. 8 in the bottom right for subsequent
ﬁlters, we can build a 2D-CNN with a c
c receptive ﬁeld
that forms such a context. We build our 3D-CNN P by gen-
eralizing this idea to 3D, where we construct the mask for
the ﬁlter of the ﬁrst layer as shown in pseudo-code Algo-
rithm 1. The mask for the subsequent layers is constructed
analoguously by replacing “<” in line 7 with “
”. We use
ﬁlter size fW = fH = fD = 3.

≤

×

1: central idx
2: current idx
3: mask
4: for d
5:
6:

←
∈ {
for h

fD)/2
(cid:101)

(fW ·
fH ·
← (cid:100)
1
←
fD-dimensional matrix of zeros
fH ×
fW ×
do
1, . . . , fD}
1, . . . , fH }
∈ {
for w
∈ {
if current idx < central idx then

do
1, . . . , fW }
mask(w, h, d) = 1

do

else

mask(w, h, d) = 0

current idx

current idx + 1

←

7:
8:
9:
10:

11:

fW ×

With this approach, we obtain a 3D-CNN P which oper-
fD-dimensional blocks. We can use P to
ates on fH ×
encode ˆz by iterating over ˆz in such blocks, exhausting ﬁrst
axis w, then axis h, and ﬁnally axis d (like in Algorithm 1).
For each such block, P yields the probability distribution of
the central symbol given the symbols in the block. Due to
the construction of the masks, this probability distribution
only depends on previously encoded symbols.

Multiple compression rates
It is quite straightforward to
obtain multiple operating points in a single network with
our framework: We can simply share the network but use
multiple importance maps. We did a simple experiment
where we trained an autoencoder with 5 different impor-
tance maps. In each iteration, a random importance map
t.
i was picked, and the target entropy was set to i/5
While not tuned for performance, this already yielded a
model competitive with BPG. The following shows the
output of the model for i = 1, 3, 5 (from left to right):

·

On the beneﬁt of 3DCNN and joint training We note
that the points from Table 1 (where we trained different en-

Figure 9: Performance on the Kodak dataset. See text.

tropy models non-jointly as a post-training step) are not di-
rectly comparable with the curve in Fig. 1. This is because
these points are obtained by taking the mean of the MS-
SSIM and bpp values over the Kodak images for a single
model. In contrast, the curve in Fig. 1 is obtained by fol-
lowing the approach of [14], constructing a MS-SSIM vs.
bpp curve per-image via interpolation (see Comparison in
Section 4). In Fig. 9, we show the black curve from Fig. 1,
as well as the mean (MS-SSIM, bpp) points achieved by
the underlying models (
). We also show the points from
◦
Tab. 1 (+). We can see that our masked 3DCNN with joint
training gives a signiﬁcant improvement over the separately
trained 3DCNN, i.e., a 22% reduction in bpp when compar-
ing mean points (the red point is estimated).

Non-realistic images
In Fig. 10, we compare our ap-
proach to BPG on an image from the Manga1097 dataset.
We can see that our approach preserves text well enough to
still be legible, but it is not as crip as BPG (left zoom). On
the other hand, our approach manages to preserve the ﬁne
texture on the face better than BPG (right zoom).

Visual examples The following pages show the ﬁrst four
images of each of our validation sets compressed to low bi-
trate, together with outputs from BPG, JPEG2000 and JPEG
compressed to similar bitrates. We ignored all header infor-
mation for all considered methods when computing the bi-
trate (here and throughout the paper). We note that the only
header our approach requires is the size of the image and an
identiﬁer, e.g., β, specifying the model.

Overall, our images look pleasant to the eye. We see
cases of over-blurring in our outputs, where BPG manages
to keep high frequencies due to its more local approach. An
example is the fences in front of the windows in Fig. 14, top,
or the text in Fig. 15, top. On the other hand, BPG tends to
discard low-contrast high frequencies where our approach
keeps them in the output, like in the door in Fig. 11, top,
or in the hair in Fig. 12, bottom. This may be explained by

7http://www.manga109.org/

Figure 10: Comparison on a non-realistic image. See text.

BPG being optimized for MSE as opposed to our approach
being optimized for MS-SSIM.

JPEG looks extremely blocky for most images due to the

very low bitrate.

11

Ours 0.239 bpp

0.246 bpp BPG

JPEG 2000 0.242 bpp

0.259 bpp JPEG

Ours 0.203 bpp

0.201 bpp BPG

JPEG 2000 0.197 bpp

0.205 bpp JPEG

Figure 11: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Kodak
dataset, along with bit rate.

12

Ours 0.165 bpp

0.164 bpp BPG

JPEG 2000 0.166 bpp

0.166 bpp JPEG

Ours 0.193 bpp

0.209 bpp BPG

JPEG 2000 0.194 bpp

0.203 bpp JPEG

Figure 12: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Kodak
dataset, along with bit rate.

13

Ours 0.385 bpp

0.394 bpp BPG

JPEG 2000 0.377 bpp

0.386 bpp JPEG

Ours 0.365 bpp

0.363 bpp BPG

JPEG 2000 0.363 bpp

0.372 bpp JPEG

Figure 13: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Ur-
ban100 dataset, along with bit rate.

14

Ours 0.435 bpp

0.479 bpp BPG

JPEG 2000 0.437 bpp

0.445 bpp JPEG

Ours 0.345 bpp

0.377 bpp BPG

JPEG 2000 0.349 bpp

0.357 bpp JPEG

Figure 14: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Ur-
ban100 dataset, along with bit rate.

15

Ours 0.355 bpp

0.394 bpp BPG

JPEG 2000 0.349 bpp

0.378 bpp JPEG

Ours 0.263 bpp

0.267 bpp BPG

JPEG 2000 0.254 bpp

0.266 bpp JPEG

Figure 15: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Ima-
geNetTest dataset, along with bit rate.

16

Ours 0.284 bpp

0.280 bpp BPG

JPEG 2000 0.287 bpp

0.288 bpp JPEG

Ours 0.247 bpp

0.253 bpp BPG

JPEG 2000 0.243 bpp

0.252 bpp JPEG

Figure 16: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Ima-
geNetTest dataset, along with bit rate.

17

Ours 0.494 bpp

0.501 bpp BPG

JPEG 2000 0.490 bpp

0.525 bpp JPEG

Ours 0.298 bpp

0.301 bpp BPG

JPEG 2000 0.293 bpp

0.315 bpp JPEG

Figure 17: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the B100
dataset, along with bit rate.

18

Ours 0.315 bpp

0.329 bpp BPG

JPEG 2000 0.311 bpp

0.321 bpp JPEG

Ours 0.363 bpp

0.397 bpp BPG

JPEG 2000 0.369 bpp

0.372 bpp JPEG

Figure 18: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the B100
dataset, along with bit rate.

19

Conditional Probability Models for Deep Image Compression

Fabian Mentzer∗ Eirikur Agustsson∗ Michael Tschannen
mentzerf@vision.ee.ethz.ch

aeirikur@vision.ee.ethz.ch

michaelt@nari.ee.ethz.ch

Radu Timofte
timofter@vision.ee.ethz.ch

Luc Van Gool
vangool@vision.ee.ethz.ch

ETH Z¨urich, Switzerland

9
1
0
2
 
n
u
J
 
4
 
 
]

V
C
.
s
c
[
 
 
4
v
0
6
2
4
0
.
1
0
8
1
:
v
i
X
r
a

Abstract

Deep Neural Networks trained as image auto-encoders
have recently emerged as a promising direction for advanc-
ing the state-of-the-art in image compression. The key chal-
lenge in learning such networks is twofold: To deal with
quantization, and to control the trade-off between recon-
struction error (distortion) and entropy (rate) of the latent
image representation. In this paper, we focus on the latter
challenge and propose a new technique to navigate the rate-
distortion trade-off for an image compression auto-encoder.
The main idea is to directly model the entropy of the latent
representation by using a context model: A 3D-CNN which
learns a conditional probability model of the latent distribu-
tion of the auto-encoder. During training, the auto-encoder
makes use of the context model to estimate the entropy of its
representation, and the context model is concurrently up-
dated to learn the dependencies between the symbols in the
latent representation. Our experiments show that this ap-
proach, when measured in MS-SSIM, yields a state-of-the-
art image compression system based on a simple convolu-
tional auto-encoder.

1. Introduction

Image compression refers to the task of representing im-
ages using as little storage (i.e., bits) as possible. While in
lossless image compression the compression rate is limited
by the requirement that the original image should be per-
fectly reconstructible, in lossy image compression, a greater
reduction in storage is enabled by allowing for some distor-
tion in the reconstructed image. This results in a so-called
rate-distortion trade-off, where a balance is found between
the bitrate R and the distortion d by minimizing d + βR,
where β > 0 balances the two competing objectives. Re-
cently, deep neural networks (DNNs) trained as image auto-
encoders for this task led to promising results, achieving
better performance than many traditional techniques for im-
age compression [19, 20, 17, 4, 2, 9]. Another advantage of

∗The ﬁrst two authors contributed equally.

Figure 1: State-of-the-art performance achieved by our sim-
ple compression system composed of a standard convolu-
tional auto-encoder and a 3D-CNN-based context model.

DNN-based learned compression systems is their adaptabil-
ity to speciﬁc target domains such as areal images or stereo
images, enabling even higher compression rates on these
domains. A key challenge in training such systems is to op-
timize the bitrate R of the latent image representation in the
auto-encoder. To encode the latent representation using a
ﬁnite number of bits, it needs to be discretized into symbols
(i.e., mapped to a stream of elements from some ﬁnite set
of values). Since discretization is non-differentiable, this
presents challenges for gradient-based optimization meth-
ods and many techniques have been proposed to address
them. After discretization, information theory tells us that
the correct measure for bitrate R is the entropy H of the
resulting symbols. Thus the challenge, and the focus of this
paper, is how to model H such that we can navigate the
trade-off d + βH during optimization of the auto-encoder.

Our proposed method is based on leveraging context
models, which were previously used as techniques to im-

1

prove coding rates for already-trained models [4, 20, 9, 14],
directly as an entropy term in the optimization. We concur-
rently train the auto-encoder and the context model with re-
spect to each other, where the context model learns a convo-
lutional probabilistic model of the image representation in
the auto-encoder, while the auto-encoder uses it for entropy
estimation to navigate the rate-distortion trade-off. Further-
more, we generalize our formulation to spatially-aware net-
works, which use an importance map to spatially attend the
bitrate representation to the most important regions in the
compressed representation. The proposed techniques lead
to a simple image compression system1, which achieves
state-of-the-art performance when measured with the pop-
ular multi-scale structural similarity index (MS-SSIM) dis-
tortion metric [23], while being straightforward to imple-
ment with standard deep-learning toolboxes.

2. Related work

Full-resolution image compression using DNNs has at-
tracted considerable attention recently. DNN architectures
commonly used for image compression are auto-encoders
[17, 4, 2, 9] and recurrent neural networks (RNNs) [19, 20].
The networks are typically trained to minimize the mean-
squared error (MSE) between original and decompressed
image [17, 4, 2, 9], or using perceptual metrics such as
MS-SSIM [20, 14]. Other notable techniques involve pro-
gressive encoding/decoding strategies [19, 20], adversarial
training [14], multi-scale image decompositions [14], and
generalized divisive normalization (GDN) layers [4, 3].

Context models and entropy estimation—the focus of
the present paper—have a long history in the context of
engineered compression methods, both lossless and lossy
[24, 12, 25, 13, 10]. Most of the recent DNN-based lossy
image compression approaches have also employed such
techniques in some form. [4] uses a binary context model
for adaptive binary arithmetic coding [11]. The works of
[20, 9, 14] use learned context models for improved cod-
ing performance on their trained models when using adap-
tive arithmetic coding. [17, 2] use non-adaptive arithmetic
coding but estimate the entropy term with an independence
assumption on the symbols.

Also related is the work of van den Oord et al. [22, 21],
who proposed PixelRNN and PixelCNN, powerful RNN-
and CNN-based context models for modeling the distribu-
tion of natural images in a lossless setting, which can be
used for (learned) lossless image compression as well as
image generation.

3. Proposed method

Given a set of training images

, we wish to learn a
compression system which consists of an encoder, a quan-

X

1https://github.com/fab-jul/imgcomp-cvpr

→

→ C

discretizes the coordinates of z to L =

Rm maps an
tizer, and a decoder. The encoder E : Rd
image x to a latent representation z = E(x). The quantizer
Q : R
cen-
ters, obtaining ˆz with ˆzi := Q(zi)
, which can be loss-
lessly encoded into a bitstream. The decoder D then forms
the reconstructed image ˆx = D(ˆz) from the quantized la-
tent representation ˆz, which is in turn (losslessy) decoded
from the bitstream. We want the encoded representation ˆz
to be compact when measured in bits, while at the same time
we want the distortion d(x, ˆx) to be small, where d is some
measure of reconstruction error, such as MSE or MS-SSIM.
This results in the so-called rate-distortion trade-off

∈ C

|C|

d(x, ˆx) + βH(ˆz),

(1)

where H denotes the cost of encoding ˆz to bits, i.e., the en-
tropy of ˆz. Our system is realized by modeling E and D as
convolutional neural networks (CNNs) (more speciﬁcally,
as the encoder and decoder, respectively, of a convolutional
auto-encoder) and minimizing (1) over the training set
,
X
where a large/small β draws the system towards low/high
average entropy H. In the next sections, we will discuss
how we quantize z and estimate the entropy H(ˆz). We note
that as E, D are CNNs, ˆz will be a 3D feature map, but for
simplicity of exposition we will denote it as a vector with
equally many elements. Thus, ˆzi refers to the i-th element
of the feature map, in raster scan order (row by column by
channel).

3.1. Quantization

We adopt the scalar variant of the quantization approach
proposed in [2] to quantize z, but simplify it using ideas
from [17]. Speciﬁcally, given centers
, cL} ⊂
· · ·
R, we use nearest neighbor assignments to compute
ˆzi = Q(zi) := arg minj(cid:107)

zi −
but rely on (differentiable) soft quantization

,
cj(cid:107)

c1,

(2)

=

C

{

˜zi =

L
(cid:88)

j=1

exp(
σ
−
l=1 exp(

(cid:80)L

)
zi −
cj(cid:107)
(cid:107)
)
cl(cid:107)
zi −
σ
(cid:107)
−

cj

(3)

C

to compute gradients during the backward pass. This com-
bines the beneﬁt of [2] where the quantization is restricted
(instead of the ﬁxed (non-
to a ﬁnite set of learned centers
learned) integer grid as in [17]) and the simplicity of [17],
where a differentiable approximation of quantization is only
used in the backward pass, avoiding the need to choose
an annealing strategy (i.e., a schedule for σ) as in [2] to
drive the soft quantization (3) to hard assignments (2) dur-
ing training. In TensorFlow, this is implemented as
¯zi = tf.stopgradient(ˆzi −

˜zi) + ˜zi.

(4)

We note that for forward pass computations, ¯zi = ˆzi, and
thus we will continue writing ˆzi for the latent representa-
tion.

2

3.2. Entropy estimation

the index I(ˆzi) of ˆzi in

with a cross entropy loss:

To model the entropy H(ˆz) we build on the approach
of PixelRNN [22] and factorize the distribution p(ˆz) as a
product of conditional distributions

p(ˆz) =

ˆzi
p(ˆzi|

−

1, . . . , ˆz1),

(5)

m
(cid:89)

i=1

where the 3D feature volume ˆz is indexed in raster
scan order. We then use a neural network P (ˆz), which
we refer to as a context model,
to estimate each term
ˆzi
p(ˆzi|

1, . . . , ˆz1):

−

−

−

≈

(6)

Pi,l(ˆz)

1, . . . , ˆz1),

ˆzi
p(ˆzi = cl|
where Pi,l speciﬁes for every 3D location i in ˆz the prob-
with l = 1, . . . , L. We re-
abilites of each symbol in
C
fer to the resulting approximate distribution as q(ˆz) :=
(cid:81)m
i=1 Pi,I(ˆzi)(ˆz), where I(ˆzi) denotes the index of ˆzi in
.
Since the conditional distributions p(ˆzi|
1, . . . , ˆz1)
ˆzi
1, . . . , ˆz1, this imposes a
only depend on previous values ˆzi
causality constraint on the network P : While P may com-
pute Pi,l in parallel for i = 1, . . . , m, l = 1, . . . , L, it needs
to make sure that each such term only depends on previous
values ˆzi

1, . . . , ˆz1.

C

−

−

The authors of PixelCNN [22, 21] study the use of 2D-
CNNs as causal conditional models over 2D images in a
lossless setting, i.e., treating the RGB pixels as symbols.
They show that the causality constraint can be efﬁciently en-
forced using masked ﬁlters in the convolution. Intuitively,
the idea is as follows: If for each layer the causality con-
dition is satisﬁed with respect to the spatial coordinates of
the layer before, then by induction the causality condition
will hold between the output layer and the input. Satisfying
the causality condition for each layer can be achieved with
proper masking of its weight tensor, and thus the entire net-
work can be made causal only through the masking of its
weights. Thus, the entire set of probabilities Pi,l for all (2D)
spatial locations i and symbol values l can be computed in
parallel with a fully convolutional network, as opposed to
modeling each term p(ˆzi|
1,
ˆzi
In our case, ˆz is a 3D symbol volume, with as much as
K = 64 channels. We therefore generalize the approach
of PixelCNN to 3D convolutions, using the same idea of
masking the ﬁlters properly in every layer of the network.
This enables us to model P efﬁciently, with a light-weight2
3D-CNN which slides over ˆz, while properly respecting the
causality constraint. We refer to the supplementary material
for more details.

, ˆz1) separately.

· · ·

−

As in [21], we learn P by training it for maximum like-
lihood, or equivalently (see [16]) by training Pi,: to classify

2We use a 4-layer network, compared to 15 layers in [22].

3

C

CE := Eˆz
∼

p(ˆz)[

m
(cid:88)

−

i=1

log Pi,I(ˆzi)].

(7)

Using the well-known property of cross entropy as the cod-
ing cost when using the wrong distribution q(ˆz) instead of
the true distribution p(ˆz), we can also view the CE loss as
an estimate of H(ˆz) since we learn P such that P = q
p.
That is, we can compute

≈

p(ˆz)[

log(p(ˆz))]

H(ˆz) = Eˆz
∼
= Eˆz
∼

p(ˆz)[

ˆzi
log p(ˆzi|

−

1,

−

· · ·

, ˆz1)]

Eˆz
∼

p(ˆz)[

≈

log q(ˆzi|

ˆzi

−

1,

−

· · ·

, ˆz1)]

−
m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

= Eˆz
∼

p(ˆz)[

−

i=1

log Pi,I(ˆzi)]

= CE

(8)

(9)

(10)

(11)

(12)

Therefore, when training the auto-encoder we can indirectly
minimize H(ˆz) through the cross entropy CE. We refer to
argument in the expectation of (7),

C(ˆz) :=

log Pi,I(ˆzi),

(13)

m
(cid:88)

−

i=1

as the coding cost of the latent image representation, since
this reﬂects the coding cost incurred when using P as a con-
text model with an adaptive arithmetic encoder [11]. From
the application perspective, minimizing the coding cost is
actually more important than the (unknown) true entropy,
since it reﬂects the bitrate obtained in practice.

To backpropagate through P (ˆz) we use the same ap-
proach as for the encoder (see (4)). Thus, like the decoder
D, P only sees the (discrete) ˆz in the forward pass, whereas
the gradient of the soft quantization ˜z is used for the back-
ward pass.

3.3. Concurrent optimization

Given an auto-encoder (E, D), we can train P to model
the dependencies of the entries of ˆz as described in the pre-
vious section by minimizing (7). On the other hand, us-
ing the model P , we can obtain an estimate of H(ˆz) as
in (12) and use this estimate to adjust (E, D) such that
d(x, D(Q(E(x)))) + βH(ˆz) is reduced, thereby navigating
the rate distortion trade-off. Therefore, it is natural to con-
currently learn P (with respect to its own loss), and (E, D)
(with respect to the rate distortion trade-off) during train-
ing, such that all models which the losses depend on are
continuously updated.

3.4. Importance map for spatial bit-allocation

8 ×

H
8 ×

Recall that since E and D are CNNs, ˆz is a 3D feature-
map. For example, if E has three stride-2 convolution layers
and the bottleneck has K channels, the dimensions of ˆz will
be W
K. A consequence of this formulation is that
we are using equally many symbols in ˆz for each spatial
location of the input image x. It is known, however, that in
practice there is great variability in the information content
across spatial locations (e.g., the uniform area of blue sky
vs. the ﬁne-grained structure of the leaves of a tree).

This can in principle be accounted for automatically in
the trade-off between the entropy and the distortion, where
the network would learn to output more predictable (i.e.,
low entropy) symbols for the low information regions, while
making room for the use of high entropy symbols for the
more complex regions. More precisely, the formulation in
(7) already allows for variable bit allocation for different
spatial regions through the context model P .

However, this arguably requires a quite sophisticated
(and hence computationally expensive) context model, and
we ﬁnd it beneﬁcial to follow Li et al. [9] instead by using
an importance map to help the CNN attend to different re-
gions of the image with different amounts of bits. While
[9] uses a separate network for this purpose, we consider a
simpliﬁed setting. We take the last layer of the encoder E,
1. We
and add a second single-channel output y
∈
H
K
expand this single channel y into a mask m
8 ×
of the same dimensionality as z as follows:

H
8 ×
8 ×

8 ×
R W

R W

∈

prior during training, i.e., costing each 1 bit to encode. The
importance map is thus their principal tool for controlling
the bitrate, since they thereby avoid encoding all the bits
in the representation. In contrast, we stick to the formula-
tion in (5) where the dependencies between the symbols are
modeled during training. We then use the importance map
as an architectural constraint and use their suggested cod-
ing strategy to obtain an alternative estimate for the entropy
H(ˆz), as follows.

(cid:100)

m

We observe that we can recover

from ˆz by count-
ing the number of consecutive zero symbols at the end of
each column ˆzi,j,:.3
is therefore a function of the
masked ˆz, i.e.,
m
as de-
(cid:101)
scribed, which means that we have for the conditional en-
tropy H(
(cid:100)

(cid:100)
= g(ˆz) for g recovering

ˆz) = 0. Now, we have

m
(cid:101)

m
(cid:101)

m

(cid:101)

(cid:100)

(cid:100)

(cid:101)|
H(ˆz) = H(
(cid:100)
= H(ˆz,
= H(ˆz

m

ˆz) + H(ˆz)

(15)

)

(cid:101)|
m
(cid:101)
) + H(
(cid:100)
If we treat the entropy of the mask, H(
), as constant
during optimization of the auto-encoder, we can then indi-
rectly minimize H(ˆz) through H(ˆz

m
(cid:101)
m
(cid:100)

(cid:100)
m
(cid:101)

(17)

(16)

|(cid:100)

).

(cid:101)

To estimate H(ˆz

m), we use the same factorization of p
|
m
as in (5), but since the mask
is known we have p(ˆzi =
(cid:100)
(cid:101)
c0) = 1 deterministic for the 3D locations i in ˆz where
the mask is zero. The logs of the corresponding terms in (9)
then evaluate to 0. The remaining terms, we can model with
the same context model Pi,l(ˆz), which results in

m).
|

mi,j,k =






1
(yi,j −
0

k)

if k < yi,j
if k
yi,j ≤
if k + 1 > yi,j

≤

k + 1

,

(14)

H(ˆz

m

)
(cid:101)

|(cid:100)

≈

Eˆz

p(ˆz)[

∼

mi(cid:101)

−(cid:100)

log Pi,I(ˆzi)],

(18)

m
(cid:88)

i=1

where yi,j denotes the value of y at spatial location (i, j).
The transition value for k
k + 1 is such that the
mask smoothly transitions from 0 to 1 for non-integer val-
ues of y.

yi,j ≤

≤

We then mask z by pointwise multiplication with the bi-
m
. Since the ceiling op-
(cid:101)
is not differentiable, as done by [17, 9], we use

narization of m, i.e., z
erator
identity for the backward pass.

(cid:12) (cid:100)

(cid:100)·(cid:101)

←

z

With this modiﬁcation, we have simply changed the ar-
chitecture of E slightly such that it can easily “zero out”
portions of columns zi,j,: of z (the rest of the network stays
the same, so that (2) still holds for example). As suggested
by [9], the so-obtained structure in z presents an alterna-
tive coding strategy: Instead of losslessly encoding the en-
tire symbol volume ˆz, we could ﬁrst (separately) encode the
m
, and then for each column ˆzi,j,: only encode the
mask
(cid:100)
(cid:101)
ﬁrst
+ 1 symbols, since the remaining ones are the
mi,j(cid:101)
constant Q(0), which we refer to as the zero symbol.

(cid:100)

Work [9] uses binary symbols (i.e.,

) and as-
sumes independence between the symbols and a uniform

0, 1
}
{

=

C

where mi denotes the i-th element of m (in the same raster
scan order as ˆz).

Similar to the coding cost (13), we refer to the argument

in the expectation in (18),

M C(ˆz) :=

mi(cid:101)

−(cid:100)

log Pi,I(ˆzi)

(19)

m
(cid:88)

i=1

as the masked coding cost of ˆz.

While the entropy estimate (18) is almost estimating the
same quantity as (7) (only differing by H(
)), it has the
beneﬁt of being weighted by mi. Therefore, the encoder E
has an obvious path to control the entropy of ˆz, by simply
increasing/decreasing the value of y for some spatial loca-
tion of x and thus obtaining fewer/more zero entries in m.
When the context model P (ˆz) is trained, however, we
still train it with respect to the formulation in (8), so it does

m
(cid:101)

(cid:100)

3If z contained zeros before it was masked, we might overestimate the
number of 0 entries in (cid:100)m(cid:101). However, we can redeﬁne those entries of m
as 0 and this will give the same result after masking.

4

not have direct access to the mask m and needs to learn the
dependencies on the entire masked symbol volume ˆz. This
means that when encoding an image, we can stick to stan-
dard adaptive arithmetic coding over the entire bottleneck,
without needing to resort to a two-step coding process as in
[9], where the mask is ﬁrst encoded and then the remaining
symbols. We emphasize that this approach hinges critically
on the context model P and the encoder E being trained
concurrently as this allows the encoder to learn a meaning-
ful (in terms of coding cost) mask with respect to P (see the
next section).

≈

m

) being ignored.
(cid:101)

In our experiments we observe that during training, the
two entropy losses (7) and (18) converge to almost the same
3.5% smaller due to
value, with the latter being around
H(
(cid:100)
While the importance map is not crucial for optimal rate-
distortion performance, if the channel depth K is adjusted
carefully, we found that we could more easily control the
entropy of ˆz through β when using a ﬁxed K, since the net-
work can easily learn to ignore some of the channels via the
importance map. Furthermore, in the supplementary mate-
rial we show that by using multiple importance maps for a
single network, one can obtain a single model that supports
multiple compression rates.

3.5. Putting the pieces together

We made an effort to carefully describe our formulation
and its motivation in detail. While the description is lengthy,
when putting the resulting pieces together we get a quite
straightforward pipeline for learned image compression, as
follows.

Given the set of training images

, we initialize (fully

convolutional) CNNs E, D, and P , as well as the centers
C
of the quantizer Q. Then, we train over minibatches
XB =
x(1),
. At each iteration, we
of crops from
{
take one gradient step for the auto-encoder (E, D) and the
quantizer Q, with respect to the rate-distortion trade-off

, x(B)

· · ·

X

}

X

2. Expand importance map y to mask m via (14)

3. Mask z, i.e., z

z

m

←

(cid:12) (cid:100)

(cid:101)

4. Quantize ˆz = Q(z)

5. Compute the context P (ˆz)

6. Decode ˆx = D(ˆz),

which can be computed in parallel over the minibatch on a
GPU since all the models are fully convolutional.

3.6. Relationship to previous methods

We are not the ﬁrst to use context models for adaptive
arithmetic coding to improve the performance in learned
deep image compression. Work [20] uses a PixelRNN-like
architecture [22] to train a recurrent network as a context
model for an RNN-based compression auto-encoder. Li et
al. [9] extract cuboid patches around each symbol in a bi-
nary feature map, and feed them to a convolutional context
model. Both these methods, however, only learn the context
model after training their system, as a post-processing step
to boost coding performance.

In contrast, our method directly incorporates the context
model as the entropy term for the rate-distortion term (1)
of the auto-encoder, and trains the two concurrently. This
is done at little overhead during training, since we adopt a
3D-CNN for the context model, using PixelCNN-inspired
[21] masking of the weights of each layer to ensure causal-
ity in the context model. Adopting the same approach to the
context models deployed by [20] or [9] would be non-trivial
since they are not designed for fast feed-forward computa-
tion. In particular, while the context model of [9] is also
convolutional, its causality is enforced through masking the
inputs to the network, as opposed to our masking of the
weights of the networks. This means their context model
needs to be run separately with a proper input cuboid for
each symbol in the volume (i.e., not fully convolutionally).

LE,D,Q =

1
B

B
(cid:88)

j=1

d(x(j), ˆx(j)) + βM C(ˆz(j)),

(20)

4. Experiments

which is obtained by combining (1) with the estimate (18)
& (19) and taking the batch sample average. Furthermore,
we take a gradient step for the context model P with respect
to its objective (see (7) & (13))

LP :=

1
B

B
(cid:88)

j=1

d(x(j), ˆx(j)) + βC(ˆz(j)).

(21)

To compute these two batch losses, we need to perform

the following computation for each x

∈ XB:

1. Obtain compressed (latent) representation z and im-
portance map y from the encoder: (z, y) = E(x)

Architecture Our auto-encoder has a similar architecture
as [17] but with more layers, and is described in Fig. 2. We
adapt the number of channels K in the latent representation
for different models. For the context model P , we use a
simple 4-layer 3D-CNN as described in Fig. 3.

Distortion measure Following [7, 14], we use the multi-
scale structural similarity index (MS-SSIM) [23] as mea-
sure of distortion d(x, ˆx) = 100
MS-SSIM(x, ˆx))
for our models. MS-SSIM reportedly correlates better with
human perception of distortion than mean squared error
(MSE). We train and test all our models using MS-SSIM.

(1

−

·

5

while using a moderately large β = 10. We use a small
regularization on the weights and note that we achieve very
stable training. We trained our models for 6 epochs, which
took around 24h per model on a single GPU. For P , we use
a LR of 10−

4 and the same decay schedule.

Datasets We train on the the ImageNet dataset from
the Large Scale Visual Recognition Challenge 2012
(ILSVRC2012) [15]. As a preprocessing step, we take ran-
dom 160
160 crops, and randomly ﬂip them. We set aside
100 images from ImageNet as a testing set, ImageNetTest.
Furthermore, we test our method on the widely used Ko-
dak [1] dataset. To asses performance on high-quality full-
resolution images, we also test on the datasets B100 [18]
and Urban100 [5], commonly used in super-resolution.

×

Other codecs We compare to JPEG, using libjpeg4, and
JPEG2000, using the Kakadu implementation5. We also
compare to the lesser known BPG6, which is based on
HEVC, the state-of-the-art in video compression, and which
outperforms JPEG and JPEG2000. We use BPG in the non-
default 4:4:4 chroma format, following [14].

Comparison Like [14], we proceed as follows to com-
pare to other methods. For each dataset, we compress each
image using all our models. This yields a set of (bpp, MS-
SSIM) points for each image, which we interpolate to get
a curve for each image. We ﬁx a grid of bpp values, and
average the curves for each image at each bpp grid value
(ignoring those images whose bpp range does not include
the grid value, i.e., we do not extrapolate). We do this for
our method, BPG, JPEG, and JPEG2000. Due to code being
unavailable for the related works in general, we digitize the
Kodak curve from Rippel & Bourdev [14], who have care-
fully collected the curves from the respective works. With
this, we also show the results of Rippel & Bourdev [14],
Johnston et al. [7], Ball´e et al. [4], and Theis et al. [17].
To validate that our estimated MS-SSIM is correctly im-
plemented, we independently generated the BPG curves for
Kodak and veriﬁed that they matched the one from [14].

Results Fig. 1 shows a comparison of the aforementioned
methods for Kodak. Our method outperforms BPG, JPEG,
and JPEG2000, as well as the neural network based ap-
proaches of Johnston et al. [7], Ball´e et al. [4], and Theis
et al. [17]. Furthermore, we achieve performance compara-
ble to that of Rippel & Bourdev [14]. This holds for all bpps
we tested, from 0.3 bpp to 0.9 bpp. We note that while Rip-
pel & Bourdev and Johnston et al. also train to maximize
(MS-)SSIM, the other methods minimize MSE.

4http://libjpeg.sourceforge.net/
5http://kakadusoftware.com/
6https://bellard.org/bpg/

Figure 2: The architecture of our auto-encoder. Dark gray
blocks represent residual units. The upper part represents
the encoder E, the lower part the decoder D. For the en-
coder, “k5 n64-2” represents a convolution layer with ker-
nel size 5, 64 output channels and a stride of 2. For the de-
coder it represents the equivalent deconvolution layer. All
convolution layers are normalized using batch norm [6], and
use SAME padding. Masked quantization is the quantiza-
tion described in Section 3.4. Normalize normalizes the in-
put to [0, 1] using a mean and variance obtained from a sub-
set of the training set. Denormalize is the inverse operation.

Figure 3:
The architecture of our context model.
“3D k3 n24” refers to a 3D masked convolution with ﬁl-
ter size 3 and 24 output channels. The last layer outputs L
values for each voxel in ˆz.

·

10−

Training We use the Adam optimizer [8] with a mini-
batch size of 30 to train seven models. Each model is trained
to maximize MS-SSIM directly. As a baseline, we used a
3 for each model, but found it
learning rate (LR) of 4
beneﬁcial to vary it slightly for different models. We set
σ = 1 in the smooth approximation (3) used for gradient
backpropagation through Q. To make the model more pre-
dictably land at a certain bitrate t when optimizing (1), we
found it helpful to clip the rate term (i.e., replace the entropy
term βH with max(t, βH)), such that the entropy term is
“switched off” when it is below t. We found this did not
hurt performance. We decay the learning rate by a factor 10
every two epochs. To obtain models for different bitrates,
we adapt the target bitrate t and the number of channels K,

6

Figure 4: Performance of our approach on ImageNetTest, B100, Urban100, where we out-
perform BPG, JPEG and JPEG2000 in MS-SSIM.

Ours 0.124bpp

0.147 bpp BPG

JPEG2000 0.134bpp

0.150bpp JPEG

Figure 5: Example image (kodim21) from the Kodak testing set, compressed with different methods.

In each of the other testing sets, we also outperform
BPG, JPEG, and JPEG2000 over the reported bitrates, as
shown in Fig. 4.

In Fig. 5, we compare our approach to BPG, JPEG,
and JPEG2000 visually, using very strong compression on
kodim21 from Kodak. It can be seen that the output of our
network is pleasant to look at. Soft structures like the clouds
are very well preserved. BPG appears to handle high fre-
quencies better (see, e.g., the fence) but loses structure in
the clouds and in the sea. Like JPEG2000, it produces block
artifacts. JPEG breaks down at this rate. We refer to the
supplementary material for further visual examples.

Ablation study: Context model
In order to show the ef-
fectiveness of the context model, we performed the follow-

ing ablation study. We trained the auto-encoder without en-
tropy loss, i.e., β = 0 in (20), using L = 6 centers and
K = 16 channels. On Kodak, this model yields an av-
erage MS-SSIM of 0.982, at an average rate of 0.646 bpp
(calculated assuming that we need log2(L) = 2.59 bits per
symbol). We then trained three different context models for
this auto-encoder, while keeping the auto-encoder ﬁxed: A
zeroth order context model which uses a histogram to esti-
mate the probability of each of the L symbols; a ﬁrst order
(one-step prediction) context model, which uses a condi-
tional histogram to estimate the probability of each of the
L symbols given the previous symbol (scanning ˆz in raster
order); and P , i.e., our proposed context model. The result-
ing average rates are shown in Table 1. Our context model

7

reduces the rate by 10 %, even though the auto-encoder was
optimized using a uniform prior (see supplementary mate-
rial for a detailed comparison of Table 1 and Fig. 1).

5. Discussion

rate
Model
0.646 bpp
Baseline (Uniform)
0.642 bpp
Zeroth order
First order
0.627 bpp
Our context model P 0.579 bpp

Table 1: Rates for different context models, for the same
architecture (E, D).

Importance map As described in detail in Section 3.4,
we use an importance map to dynamically alter the number
of channels used at different spatial locations to encode an
image. To visualize how this helps, we trained two auto-
encoders M and M (cid:48), where M uses an importance map
and at most K = 32 channels to compress an image, and
M (cid:48) compresses without importance map and with K = 16
channels (this yields a rate for M (cid:48) similar to that of M ). In
Fig. 6, we show an image from ImageNetTest along with
the same image compressed to 0.463 bpp by M and com-
pressed to 0.504 bpp by M (cid:48). Furthermore, Fig. 6 shows the
importance map produced by M , as well as ordered visual-
izations of all channels of the latent representation for both
M and M (cid:48). Note how for M , channels with larger index
are sparser, showing how the model can spatially adapt the
number of channels. M (cid:48) uses all channels similarly.

Input

Importance map of M

Output of M

Latent representation of M

Output of M (cid:48)

Latent representation of M (cid:48)

Our experiments showed that combining a convolutional
auto-encoder with a lightweight 3D-CNN as context model
and training the two networks concurrently leads to a highly
effective image compression system. Not only were we
able to clearly outperform state-of-the-art engineered com-
pression methods including BPG and JPEG2000 in terms
of MS-SSIM, but we also obtained performance compet-
itive with the current state-of-the-art learned compression
method from [14]. In particular, our method outperforms
BPG and JPEG2000 in MS-SSIM across four different
testing sets (ImageNetTest, Kodak, B100, Urban100), and
does so signiﬁcantly, i.e., the proposed method generalizes
well. We emphasize that our method relies on elemen-
tary techniques both in terms of the architecture (standard
convolutional auto-encoder with importance map, convo-
lutional context model) and training procedure (minimize
the rate-distortion trade-off and the negative log-likelihood
for the context model), while [14] uses highly specialized
techniques such as a pyramidal decomposition architecture,
adaptive codelength regularization, and multiscale adver-
sarial training.

The ablation study for the context model showed that our
3D-CNN-based context model is signiﬁcantly more power-
ful than the ﬁrst order (histogram) and second order (one-
step prediction) baseline context models. Further, our ex-
periments suggest that the importance map learns to con-
densate the image information in a reduced number of chan-
nels of the latent representation without relying on explicit
supervision. Notably, the importance map is learned as a
part of the image compression auto-encoder concurrently
with the auto-encoder and the context model, without in-
troducing any optimization difﬁculties. In contrast, in [9]
the importance map is computed using a separate network,
learned together with the auto-encoder, while the context
model is learned separately.

6. Conclusions

In this paper, we proposed the ﬁrst method for learning
a lossy image compression auto-encoder concurrently with
a lightweight context model by incorporating it into an en-
tropy loss for the optimization of the auto-encoder, leading
to performance competitive with the current state-of-the-art
in deep image compression [14].

Future works could explore heavier and more power-
ful context models, as those employed in [22, 21]. This
could further improve compression performance and allow
for sampling of natural images in a “lossy” manner, by sam-
pling ˆz according to the context model and then decoding.

Figure 6: Visualization of the latent representation of the
auto-encoder for a high-bpp operating point, with (M ) and
without (M (cid:48)) incorporating an importance map.

Acknowledgements This work was supported by ETH
Z¨urich and by NVIDIA through a GPU grant.

8

[16] C. Shalizi.

Lecture notes on stochastic processes.

http://www.stat.cmu.edu/˜cshalizi/754/
2006/notes/lecture-28.pdf, 2006.
accessed 15-Nov-2017]. 3

[Online;

[17] L. Theis, W. Shi, A. Cunningham, and F. Huszar. Lossy
image compression with compressive autoencoders. In ICLR
2017, 2017. 1, 2, 4, 5, 6

[18] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted An-
chored Neighborhood Regression for Fast Super-Resolution,
pages 111–126. Springer International Publishing, Cham,
2015. 6

[19] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent,
D. Minnen, S. Baluja, M. Covell, and R. Sukthankar. Vari-
able rate image compression with recurrent neural networks.
arXiv preprint arXiv:1511.06085, 2015. 1, 2

[20] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Min-
nen, J. Shor, and M. Covell. Full resolution image com-
arXiv preprint
pression with recurrent neural networks.
arXiv:1608.05148, 2016. 1, 2, 5

[21] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,
A. Graves, et al. Conditional image generation with pixel-
cnn decoders. In Advances in Neural Information Processing
Systems, pages 4790–4798, 2016. 2, 3, 5, 8

[22] A. Van Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel
In International Conference on

recurrent neural networks.
Machine Learning, pages 1747–1756, 2016. 2, 3, 5, 8
[23] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-
In Asilomar
tural similarity for image quality assessment.
Conference on Signals, Systems Computers, 2003, volume 2,
pages 1398–1402 Vol.2, Nov 2003. 2, 5

[24] M. J. Weinberger, J. J. Rissanen, and R. B. Arps. Applica-
tions of universal context modeling to lossless compression
of gray-scale images. IEEE Transactions on Image Process-
ing, 5(4):575–586, 1996. 2

[25] X. Wu, E. Barthel, and W. Zhang. Piecewise 2d autore-
gression for predictive image coding. In Image Processing,
1998. ICIP 98. Proceedings. 1998 International Conference
on, pages 901–904. IEEE, 1998. 2

References

kodak/. 6

[1] Kodak PhotoCD dataset. http://r0k.us/graphics/

[2] E. Agustsson, F. Mentzer, M. Tschannen, L. Cavigelli,
R. Timofte, L. Benini, and L. Van Gool. Soft-to-hard vector
quantization for end-to-end learning compressible represen-
tations. arXiv preprint arXiv:1704.00648, 2017. 1, 2

[3] J. Ball´e, V. Laparra, and E. P. Simoncelli. End-to-end opti-
mization of nonlinear transform codes for perceptual quality.
arXiv preprint arXiv:1607.05006, 2016. 2

[4] J. Ball´e, V. Laparra, and E. P. Simoncelli.

End-
arXiv preprint

to-end optimized image compression.
arXiv:1611.01704, 2016. 1, 2, 6

[5] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5197–5206, 2015. 6

[6] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, pages 448–
456, 2015. 6

[7] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh,
Im-
T. Chinen, S. Jin Hwang, J. Shor, and G. Toderici.
proved lossy image compression with priming and spatially
adaptive bit rates for recurrent networks. arXiv preprint
arXiv:1703.10114, 2017. 5, 6

[8] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 6

[9] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang. Learning con-
volutional networks for content-weighted image compres-
sion. arXiv preprint arXiv:1703.10553, 2017. 1, 2, 4, 5,
8

[10] D. Marpe, H. Schwarz, and T. Wiegand. Context-based adap-
tive binary arithmetic coding in the h. 264/avc video com-
IEEE Transactions on circuits and sys-
pression standard.
tems for video technology, 13(7):620–636, 2003. 2

[11] D. Marpe, H. Schwarz, and T. Wiegand. Context-based adap-
tive binary arithmetic coding in the h. 264/avc video com-
IEEE Transactions on circuits and sys-
pression standard.
tems for video technology, 13(7):620–636, 2003. 2, 3
[12] B. Meyer and P. Tischer. Tmw-a new method for lossless
image compression. ITG FACHBERICHT, pages 533–540,
1997. 2

[13] B. Meyer and P. E. Tischer. Glicbawls-grey level image com-
pression by adaptive weighted least squares. In Data Com-
pression Conference, volume 503, 2001. 2

[14] O. Rippel and L. Bourdev. Real-time adaptive image com-
In Proceedings of the 34th International Con-
pression.
ference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pages 2922–2930, Interna-
tional Convention Centre, Sydney, Australia, 06–11 Aug
2017. PMLR. 2, 5, 6, 8, 11

[15] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. CoRR, abs/1409.0575, 2014. 6

9

Figure 7: 2D vs. 3D CNNs

Algorithm 1 Constructing 3D Masks

Conditional Probability Models for Deep
Image Compression – Suppl. Material

3D probability classiﬁer As mentioned in Section 3.2,
we rely on masked 3D convolutions to enforce the causality
constraint in our probability classiﬁer P .
In a 2D-CNN,
standard 2D convolutions are used in ﬁlter banks, as shown
in Fig. 7 on the left: A W
Cin-dimensional tensor
×
Cout-dimensional tensor using
is mapped to a W (cid:48)
Cout banks of Cin 2D ﬁlters, i.e., ﬁlters can be represented
Cout-dimensional tensors. Note that
as fW ×
all Cin channels are used together, which violates causality:
When we encode, we proceed channel by channel.

fH ×

Cin

H (cid:48)

H

×

×

×

×

Using 3D convolutions, a depth dimension D is intro-
Cin-dimensional tensors
duced. In a 3D-CNN, W
D
H
×
×
×
Cout-dimensional tensors,
are mapped to W (cid:48)
D(cid:48)
H (cid:48)
×
×
Cout-dimensional ﬁlters. Thus,
with fW ×
Cin
fD ×
fH ×
a 3D-CNN slides over the depth dimension, as shown in
Fig. 7 on the right. We use such a 3D-CNN for P , where
we use as input our W
K-dimensional feature map
ˆz, using D = K, Cin = 1 for the ﬁrst layer.

H

×

×

×

×

Figure 8: Left shows a grid of symbols ˆzi, where the black
square denotes some context and the gray cells denote sym-
bols which where previously encoded. Right shows masks.

To explain how we mask the ﬁlters in P , consider the 2D
case in Fig. 8. We want to encode all values ˆzi by iterating
1, . . . , ˆz1).
in raster scan order and by computing p(ˆzi|
We simplify this by instead of relying on all previously en-
c-context around ˆzi (black
coded symbols, we use some c

ˆzi

−

×

10

square in Fig. 8). To satisfy the causality constraint, this
context may only contain values above ˆzi or in the same
row to the left of ˆzi (gray cells). By using the ﬁlter shown
in Fig. 8 in the top right for the ﬁrst layer of a CNN and
the ﬁlter shown in Fig. 8 in the bottom right for subsequent
ﬁlters, we can build a 2D-CNN with a c
c receptive ﬁeld
that forms such a context. We build our 3D-CNN P by gen-
eralizing this idea to 3D, where we construct the mask for
the ﬁlter of the ﬁrst layer as shown in pseudo-code Algo-
rithm 1. The mask for the subsequent layers is constructed
analoguously by replacing “<” in line 7 with “
”. We use
ﬁlter size fW = fH = fD = 3.

≤

×

1: central idx
2: current idx
3: mask
4: for d
5:
6:

←
∈ {
for h

fD)/2
(cid:101)

(fW ·
fH ·
← (cid:100)
1
←
fD-dimensional matrix of zeros
fH ×
fW ×
do
1, . . . , fD}
1, . . . , fH }
∈ {
for w
∈ {
if current idx < central idx then

do
1, . . . , fW }
mask(w, h, d) = 1

do

else

mask(w, h, d) = 0

current idx

current idx + 1

←

7:
8:
9:
10:

11:

fW ×

With this approach, we obtain a 3D-CNN P which oper-
fD-dimensional blocks. We can use P to
ates on fH ×
encode ˆz by iterating over ˆz in such blocks, exhausting ﬁrst
axis w, then axis h, and ﬁnally axis d (like in Algorithm 1).
For each such block, P yields the probability distribution of
the central symbol given the symbols in the block. Due to
the construction of the masks, this probability distribution
only depends on previously encoded symbols.

Multiple compression rates
It is quite straightforward to
obtain multiple operating points in a single network with
our framework: We can simply share the network but use
multiple importance maps. We did a simple experiment
where we trained an autoencoder with 5 different impor-
tance maps. In each iteration, a random importance map
t.
i was picked, and the target entropy was set to i/5
While not tuned for performance, this already yielded a
model competitive with BPG. The following shows the
output of the model for i = 1, 3, 5 (from left to right):

·

On the beneﬁt of 3DCNN and joint training We note
that the points from Table 1 (where we trained different en-

Figure 9: Performance on the Kodak dataset. See text.

tropy models non-jointly as a post-training step) are not di-
rectly comparable with the curve in Fig. 1. This is because
these points are obtained by taking the mean of the MS-
SSIM and bpp values over the Kodak images for a single
model. In contrast, the curve in Fig. 1 is obtained by fol-
lowing the approach of [14], constructing a MS-SSIM vs.
bpp curve per-image via interpolation (see Comparison in
Section 4). In Fig. 9, we show the black curve from Fig. 1,
as well as the mean (MS-SSIM, bpp) points achieved by
the underlying models (
). We also show the points from
◦
Tab. 1 (+). We can see that our masked 3DCNN with joint
training gives a signiﬁcant improvement over the separately
trained 3DCNN, i.e., a 22% reduction in bpp when compar-
ing mean points (the red point is estimated).

Non-realistic images
In Fig. 10, we compare our ap-
proach to BPG on an image from the Manga1097 dataset.
We can see that our approach preserves text well enough to
still be legible, but it is not as crip as BPG (left zoom). On
the other hand, our approach manages to preserve the ﬁne
texture on the face better than BPG (right zoom).

Visual examples The following pages show the ﬁrst four
images of each of our validation sets compressed to low bi-
trate, together with outputs from BPG, JPEG2000 and JPEG
compressed to similar bitrates. We ignored all header infor-
mation for all considered methods when computing the bi-
trate (here and throughout the paper). We note that the only
header our approach requires is the size of the image and an
identiﬁer, e.g., β, specifying the model.

Overall, our images look pleasant to the eye. We see
cases of over-blurring in our outputs, where BPG manages
to keep high frequencies due to its more local approach. An
example is the fences in front of the windows in Fig. 14, top,
or the text in Fig. 15, top. On the other hand, BPG tends to
discard low-contrast high frequencies where our approach
keeps them in the output, like in the door in Fig. 11, top,
or in the hair in Fig. 12, bottom. This may be explained by

7http://www.manga109.org/

Figure 10: Comparison on a non-realistic image. See text.

BPG being optimized for MSE as opposed to our approach
being optimized for MS-SSIM.

JPEG looks extremely blocky for most images due to the

very low bitrate.

11

Ours 0.239 bpp

0.246 bpp BPG

JPEG 2000 0.242 bpp

0.259 bpp JPEG

Ours 0.203 bpp

0.201 bpp BPG

JPEG 2000 0.197 bpp

0.205 bpp JPEG

Figure 11: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Kodak
dataset, along with bit rate.

12

Ours 0.165 bpp

0.164 bpp BPG

JPEG 2000 0.166 bpp

0.166 bpp JPEG

Ours 0.193 bpp

0.209 bpp BPG

JPEG 2000 0.194 bpp

0.203 bpp JPEG

Figure 12: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Kodak
dataset, along with bit rate.

13

Ours 0.385 bpp

0.394 bpp BPG

JPEG 2000 0.377 bpp

0.386 bpp JPEG

Ours 0.365 bpp

0.363 bpp BPG

JPEG 2000 0.363 bpp

0.372 bpp JPEG

Figure 13: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Ur-
ban100 dataset, along with bit rate.

14

Ours 0.435 bpp

0.479 bpp BPG

JPEG 2000 0.437 bpp

0.445 bpp JPEG

Ours 0.345 bpp

0.377 bpp BPG

JPEG 2000 0.349 bpp

0.357 bpp JPEG

Figure 14: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Ur-
ban100 dataset, along with bit rate.

15

Ours 0.355 bpp

0.394 bpp BPG

JPEG 2000 0.349 bpp

0.378 bpp JPEG

Ours 0.263 bpp

0.267 bpp BPG

JPEG 2000 0.254 bpp

0.266 bpp JPEG

Figure 15: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Ima-
geNetTest dataset, along with bit rate.

16

Ours 0.284 bpp

0.280 bpp BPG

JPEG 2000 0.287 bpp

0.288 bpp JPEG

Ours 0.247 bpp

0.253 bpp BPG

JPEG 2000 0.243 bpp

0.252 bpp JPEG

Figure 16: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Ima-
geNetTest dataset, along with bit rate.

17

Ours 0.494 bpp

0.501 bpp BPG

JPEG 2000 0.490 bpp

0.525 bpp JPEG

Ours 0.298 bpp

0.301 bpp BPG

JPEG 2000 0.293 bpp

0.315 bpp JPEG

Figure 17: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the B100
dataset, along with bit rate.

18

Ours 0.315 bpp

0.329 bpp BPG

JPEG 2000 0.311 bpp

0.321 bpp JPEG

Ours 0.363 bpp

0.397 bpp BPG

JPEG 2000 0.369 bpp

0.372 bpp JPEG

Figure 18: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the B100
dataset, along with bit rate.

19

Conditional Probability Models for Deep Image Compression

Fabian Mentzer∗ Eirikur Agustsson∗ Michael Tschannen
mentzerf@vision.ee.ethz.ch

aeirikur@vision.ee.ethz.ch

michaelt@nari.ee.ethz.ch

Radu Timofte
timofter@vision.ee.ethz.ch

Luc Van Gool
vangool@vision.ee.ethz.ch

ETH Z¨urich, Switzerland

9
1
0
2
 
n
u
J
 
4
 
 
]

V
C
.
s
c
[
 
 
4
v
0
6
2
4
0
.
1
0
8
1
:
v
i
X
r
a

Abstract

Deep Neural Networks trained as image auto-encoders
have recently emerged as a promising direction for advanc-
ing the state-of-the-art in image compression. The key chal-
lenge in learning such networks is twofold: To deal with
quantization, and to control the trade-off between recon-
struction error (distortion) and entropy (rate) of the latent
image representation. In this paper, we focus on the latter
challenge and propose a new technique to navigate the rate-
distortion trade-off for an image compression auto-encoder.
The main idea is to directly model the entropy of the latent
representation by using a context model: A 3D-CNN which
learns a conditional probability model of the latent distribu-
tion of the auto-encoder. During training, the auto-encoder
makes use of the context model to estimate the entropy of its
representation, and the context model is concurrently up-
dated to learn the dependencies between the symbols in the
latent representation. Our experiments show that this ap-
proach, when measured in MS-SSIM, yields a state-of-the-
art image compression system based on a simple convolu-
tional auto-encoder.

1. Introduction

Image compression refers to the task of representing im-
ages using as little storage (i.e., bits) as possible. While in
lossless image compression the compression rate is limited
by the requirement that the original image should be per-
fectly reconstructible, in lossy image compression, a greater
reduction in storage is enabled by allowing for some distor-
tion in the reconstructed image. This results in a so-called
rate-distortion trade-off, where a balance is found between
the bitrate R and the distortion d by minimizing d + βR,
where β > 0 balances the two competing objectives. Re-
cently, deep neural networks (DNNs) trained as image auto-
encoders for this task led to promising results, achieving
better performance than many traditional techniques for im-
age compression [19, 20, 17, 4, 2, 9]. Another advantage of

∗The ﬁrst two authors contributed equally.

Figure 1: State-of-the-art performance achieved by our sim-
ple compression system composed of a standard convolu-
tional auto-encoder and a 3D-CNN-based context model.

DNN-based learned compression systems is their adaptabil-
ity to speciﬁc target domains such as areal images or stereo
images, enabling even higher compression rates on these
domains. A key challenge in training such systems is to op-
timize the bitrate R of the latent image representation in the
auto-encoder. To encode the latent representation using a
ﬁnite number of bits, it needs to be discretized into symbols
(i.e., mapped to a stream of elements from some ﬁnite set
of values). Since discretization is non-differentiable, this
presents challenges for gradient-based optimization meth-
ods and many techniques have been proposed to address
them. After discretization, information theory tells us that
the correct measure for bitrate R is the entropy H of the
resulting symbols. Thus the challenge, and the focus of this
paper, is how to model H such that we can navigate the
trade-off d + βH during optimization of the auto-encoder.

Our proposed method is based on leveraging context
models, which were previously used as techniques to im-

1

prove coding rates for already-trained models [4, 20, 9, 14],
directly as an entropy term in the optimization. We concur-
rently train the auto-encoder and the context model with re-
spect to each other, where the context model learns a convo-
lutional probabilistic model of the image representation in
the auto-encoder, while the auto-encoder uses it for entropy
estimation to navigate the rate-distortion trade-off. Further-
more, we generalize our formulation to spatially-aware net-
works, which use an importance map to spatially attend the
bitrate representation to the most important regions in the
compressed representation. The proposed techniques lead
to a simple image compression system1, which achieves
state-of-the-art performance when measured with the pop-
ular multi-scale structural similarity index (MS-SSIM) dis-
tortion metric [23], while being straightforward to imple-
ment with standard deep-learning toolboxes.

2. Related work

Full-resolution image compression using DNNs has at-
tracted considerable attention recently. DNN architectures
commonly used for image compression are auto-encoders
[17, 4, 2, 9] and recurrent neural networks (RNNs) [19, 20].
The networks are typically trained to minimize the mean-
squared error (MSE) between original and decompressed
image [17, 4, 2, 9], or using perceptual metrics such as
MS-SSIM [20, 14]. Other notable techniques involve pro-
gressive encoding/decoding strategies [19, 20], adversarial
training [14], multi-scale image decompositions [14], and
generalized divisive normalization (GDN) layers [4, 3].

Context models and entropy estimation—the focus of
the present paper—have a long history in the context of
engineered compression methods, both lossless and lossy
[24, 12, 25, 13, 10]. Most of the recent DNN-based lossy
image compression approaches have also employed such
techniques in some form. [4] uses a binary context model
for adaptive binary arithmetic coding [11]. The works of
[20, 9, 14] use learned context models for improved cod-
ing performance on their trained models when using adap-
tive arithmetic coding. [17, 2] use non-adaptive arithmetic
coding but estimate the entropy term with an independence
assumption on the symbols.

Also related is the work of van den Oord et al. [22, 21],
who proposed PixelRNN and PixelCNN, powerful RNN-
and CNN-based context models for modeling the distribu-
tion of natural images in a lossless setting, which can be
used for (learned) lossless image compression as well as
image generation.

3. Proposed method

Given a set of training images

, we wish to learn a
compression system which consists of an encoder, a quan-

X

1https://github.com/fab-jul/imgcomp-cvpr

→

→ C

discretizes the coordinates of z to L =

Rm maps an
tizer, and a decoder. The encoder E : Rd
image x to a latent representation z = E(x). The quantizer
Q : R
cen-
ters, obtaining ˆz with ˆzi := Q(zi)
, which can be loss-
lessly encoded into a bitstream. The decoder D then forms
the reconstructed image ˆx = D(ˆz) from the quantized la-
tent representation ˆz, which is in turn (losslessy) decoded
from the bitstream. We want the encoded representation ˆz
to be compact when measured in bits, while at the same time
we want the distortion d(x, ˆx) to be small, where d is some
measure of reconstruction error, such as MSE or MS-SSIM.
This results in the so-called rate-distortion trade-off

∈ C

|C|

d(x, ˆx) + βH(ˆz),

(1)

where H denotes the cost of encoding ˆz to bits, i.e., the en-
tropy of ˆz. Our system is realized by modeling E and D as
convolutional neural networks (CNNs) (more speciﬁcally,
as the encoder and decoder, respectively, of a convolutional
auto-encoder) and minimizing (1) over the training set
,
X
where a large/small β draws the system towards low/high
average entropy H. In the next sections, we will discuss
how we quantize z and estimate the entropy H(ˆz). We note
that as E, D are CNNs, ˆz will be a 3D feature map, but for
simplicity of exposition we will denote it as a vector with
equally many elements. Thus, ˆzi refers to the i-th element
of the feature map, in raster scan order (row by column by
channel).

3.1. Quantization

We adopt the scalar variant of the quantization approach
proposed in [2] to quantize z, but simplify it using ideas
from [17]. Speciﬁcally, given centers
, cL} ⊂
· · ·
R, we use nearest neighbor assignments to compute
ˆzi = Q(zi) := arg minj(cid:107)

zi −
but rely on (differentiable) soft quantization

,
cj(cid:107)

c1,

(2)

=

C

{

˜zi =

L
(cid:88)

j=1

exp(
σ
−
l=1 exp(

(cid:80)L

)
zi −
cj(cid:107)
(cid:107)
)
cl(cid:107)
zi −
σ
(cid:107)
−

cj

(3)

C

to compute gradients during the backward pass. This com-
bines the beneﬁt of [2] where the quantization is restricted
(instead of the ﬁxed (non-
to a ﬁnite set of learned centers
learned) integer grid as in [17]) and the simplicity of [17],
where a differentiable approximation of quantization is only
used in the backward pass, avoiding the need to choose
an annealing strategy (i.e., a schedule for σ) as in [2] to
drive the soft quantization (3) to hard assignments (2) dur-
ing training. In TensorFlow, this is implemented as
¯zi = tf.stopgradient(ˆzi −

˜zi) + ˜zi.

(4)

We note that for forward pass computations, ¯zi = ˆzi, and
thus we will continue writing ˆzi for the latent representa-
tion.

2

3.2. Entropy estimation

the index I(ˆzi) of ˆzi in

with a cross entropy loss:

To model the entropy H(ˆz) we build on the approach
of PixelRNN [22] and factorize the distribution p(ˆz) as a
product of conditional distributions

p(ˆz) =

ˆzi
p(ˆzi|

−

1, . . . , ˆz1),

(5)

m
(cid:89)

i=1

where the 3D feature volume ˆz is indexed in raster
scan order. We then use a neural network P (ˆz), which
we refer to as a context model,
to estimate each term
ˆzi
p(ˆzi|

1, . . . , ˆz1):

−

−

−

≈

(6)

Pi,l(ˆz)

1, . . . , ˆz1),

ˆzi
p(ˆzi = cl|
where Pi,l speciﬁes for every 3D location i in ˆz the prob-
with l = 1, . . . , L. We re-
abilites of each symbol in
C
fer to the resulting approximate distribution as q(ˆz) :=
(cid:81)m
i=1 Pi,I(ˆzi)(ˆz), where I(ˆzi) denotes the index of ˆzi in
.
Since the conditional distributions p(ˆzi|
1, . . . , ˆz1)
ˆzi
1, . . . , ˆz1, this imposes a
only depend on previous values ˆzi
causality constraint on the network P : While P may com-
pute Pi,l in parallel for i = 1, . . . , m, l = 1, . . . , L, it needs
to make sure that each such term only depends on previous
values ˆzi

1, . . . , ˆz1.

C

−

−

The authors of PixelCNN [22, 21] study the use of 2D-
CNNs as causal conditional models over 2D images in a
lossless setting, i.e., treating the RGB pixels as symbols.
They show that the causality constraint can be efﬁciently en-
forced using masked ﬁlters in the convolution. Intuitively,
the idea is as follows: If for each layer the causality con-
dition is satisﬁed with respect to the spatial coordinates of
the layer before, then by induction the causality condition
will hold between the output layer and the input. Satisfying
the causality condition for each layer can be achieved with
proper masking of its weight tensor, and thus the entire net-
work can be made causal only through the masking of its
weights. Thus, the entire set of probabilities Pi,l for all (2D)
spatial locations i and symbol values l can be computed in
parallel with a fully convolutional network, as opposed to
modeling each term p(ˆzi|
1,
ˆzi
In our case, ˆz is a 3D symbol volume, with as much as
K = 64 channels. We therefore generalize the approach
of PixelCNN to 3D convolutions, using the same idea of
masking the ﬁlters properly in every layer of the network.
This enables us to model P efﬁciently, with a light-weight2
3D-CNN which slides over ˆz, while properly respecting the
causality constraint. We refer to the supplementary material
for more details.

, ˆz1) separately.

· · ·

−

As in [21], we learn P by training it for maximum like-
lihood, or equivalently (see [16]) by training Pi,: to classify

2We use a 4-layer network, compared to 15 layers in [22].

3

C

CE := Eˆz
∼

p(ˆz)[

m
(cid:88)

−

i=1

log Pi,I(ˆzi)].

(7)

Using the well-known property of cross entropy as the cod-
ing cost when using the wrong distribution q(ˆz) instead of
the true distribution p(ˆz), we can also view the CE loss as
an estimate of H(ˆz) since we learn P such that P = q
p.
That is, we can compute

≈

p(ˆz)[

log(p(ˆz))]

H(ˆz) = Eˆz
∼
= Eˆz
∼

p(ˆz)[

ˆzi
log p(ˆzi|

−

1,

−

· · ·

, ˆz1)]

Eˆz
∼

p(ˆz)[

≈

log q(ˆzi|

ˆzi

−

1,

−

· · ·

, ˆz1)]

−
m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

= Eˆz
∼

p(ˆz)[

−

i=1

log Pi,I(ˆzi)]

= CE

(8)

(9)

(10)

(11)

(12)

Therefore, when training the auto-encoder we can indirectly
minimize H(ˆz) through the cross entropy CE. We refer to
argument in the expectation of (7),

C(ˆz) :=

log Pi,I(ˆzi),

(13)

m
(cid:88)

−

i=1

as the coding cost of the latent image representation, since
this reﬂects the coding cost incurred when using P as a con-
text model with an adaptive arithmetic encoder [11]. From
the application perspective, minimizing the coding cost is
actually more important than the (unknown) true entropy,
since it reﬂects the bitrate obtained in practice.

To backpropagate through P (ˆz) we use the same ap-
proach as for the encoder (see (4)). Thus, like the decoder
D, P only sees the (discrete) ˆz in the forward pass, whereas
the gradient of the soft quantization ˜z is used for the back-
ward pass.

3.3. Concurrent optimization

Given an auto-encoder (E, D), we can train P to model
the dependencies of the entries of ˆz as described in the pre-
vious section by minimizing (7). On the other hand, us-
ing the model P , we can obtain an estimate of H(ˆz) as
in (12) and use this estimate to adjust (E, D) such that
d(x, D(Q(E(x)))) + βH(ˆz) is reduced, thereby navigating
the rate distortion trade-off. Therefore, it is natural to con-
currently learn P (with respect to its own loss), and (E, D)
(with respect to the rate distortion trade-off) during train-
ing, such that all models which the losses depend on are
continuously updated.

3.4. Importance map for spatial bit-allocation

8 ×

H
8 ×

Recall that since E and D are CNNs, ˆz is a 3D feature-
map. For example, if E has three stride-2 convolution layers
and the bottleneck has K channels, the dimensions of ˆz will
be W
K. A consequence of this formulation is that
we are using equally many symbols in ˆz for each spatial
location of the input image x. It is known, however, that in
practice there is great variability in the information content
across spatial locations (e.g., the uniform area of blue sky
vs. the ﬁne-grained structure of the leaves of a tree).

This can in principle be accounted for automatically in
the trade-off between the entropy and the distortion, where
the network would learn to output more predictable (i.e.,
low entropy) symbols for the low information regions, while
making room for the use of high entropy symbols for the
more complex regions. More precisely, the formulation in
(7) already allows for variable bit allocation for different
spatial regions through the context model P .

However, this arguably requires a quite sophisticated
(and hence computationally expensive) context model, and
we ﬁnd it beneﬁcial to follow Li et al. [9] instead by using
an importance map to help the CNN attend to different re-
gions of the image with different amounts of bits. While
[9] uses a separate network for this purpose, we consider a
simpliﬁed setting. We take the last layer of the encoder E,
1. We
and add a second single-channel output y
∈
H
K
expand this single channel y into a mask m
8 ×
of the same dimensionality as z as follows:

H
8 ×
8 ×

8 ×
R W

R W

∈

prior during training, i.e., costing each 1 bit to encode. The
importance map is thus their principal tool for controlling
the bitrate, since they thereby avoid encoding all the bits
in the representation. In contrast, we stick to the formula-
tion in (5) where the dependencies between the symbols are
modeled during training. We then use the importance map
as an architectural constraint and use their suggested cod-
ing strategy to obtain an alternative estimate for the entropy
H(ˆz), as follows.

(cid:100)

m

We observe that we can recover

from ˆz by count-
ing the number of consecutive zero symbols at the end of
each column ˆzi,j,:.3
is therefore a function of the
masked ˆz, i.e.,
m
as de-
(cid:101)
scribed, which means that we have for the conditional en-
tropy H(
(cid:100)

(cid:100)
= g(ˆz) for g recovering

ˆz) = 0. Now, we have

m
(cid:101)

m
(cid:101)

m

(cid:101)

(cid:100)

(cid:100)

(cid:101)|
H(ˆz) = H(
(cid:100)
= H(ˆz,
= H(ˆz

m

ˆz) + H(ˆz)

(15)

)

(cid:101)|
m
(cid:101)
) + H(
(cid:100)
If we treat the entropy of the mask, H(
), as constant
during optimization of the auto-encoder, we can then indi-
rectly minimize H(ˆz) through H(ˆz

m
(cid:101)
m
(cid:100)

(cid:100)
m
(cid:101)

(17)

(16)

|(cid:100)

).

(cid:101)

To estimate H(ˆz

m), we use the same factorization of p
|
m
as in (5), but since the mask
is known we have p(ˆzi =
(cid:100)
(cid:101)
c0) = 1 deterministic for the 3D locations i in ˆz where
the mask is zero. The logs of the corresponding terms in (9)
then evaluate to 0. The remaining terms, we can model with
the same context model Pi,l(ˆz), which results in

m).
|

mi,j,k =






1
(yi,j −
0

k)

if k < yi,j
if k
yi,j ≤
if k + 1 > yi,j

≤

k + 1

,

(14)

H(ˆz

m

)
(cid:101)

|(cid:100)

≈

Eˆz

p(ˆz)[

∼

mi(cid:101)

−(cid:100)

log Pi,I(ˆzi)],

(18)

m
(cid:88)

i=1

where yi,j denotes the value of y at spatial location (i, j).
The transition value for k
k + 1 is such that the
mask smoothly transitions from 0 to 1 for non-integer val-
ues of y.

yi,j ≤

≤

We then mask z by pointwise multiplication with the bi-
m
. Since the ceiling op-
(cid:101)
is not differentiable, as done by [17, 9], we use

narization of m, i.e., z
erator
identity for the backward pass.

(cid:12) (cid:100)

(cid:100)·(cid:101)

←

z

With this modiﬁcation, we have simply changed the ar-
chitecture of E slightly such that it can easily “zero out”
portions of columns zi,j,: of z (the rest of the network stays
the same, so that (2) still holds for example). As suggested
by [9], the so-obtained structure in z presents an alterna-
tive coding strategy: Instead of losslessly encoding the en-
tire symbol volume ˆz, we could ﬁrst (separately) encode the
m
, and then for each column ˆzi,j,: only encode the
mask
(cid:100)
(cid:101)
ﬁrst
+ 1 symbols, since the remaining ones are the
mi,j(cid:101)
constant Q(0), which we refer to as the zero symbol.

(cid:100)

Work [9] uses binary symbols (i.e.,

) and as-
sumes independence between the symbols and a uniform

0, 1
}
{

=

C

where mi denotes the i-th element of m (in the same raster
scan order as ˆz).

Similar to the coding cost (13), we refer to the argument

in the expectation in (18),

M C(ˆz) :=

mi(cid:101)

−(cid:100)

log Pi,I(ˆzi)

(19)

m
(cid:88)

i=1

as the masked coding cost of ˆz.

While the entropy estimate (18) is almost estimating the
same quantity as (7) (only differing by H(
)), it has the
beneﬁt of being weighted by mi. Therefore, the encoder E
has an obvious path to control the entropy of ˆz, by simply
increasing/decreasing the value of y for some spatial loca-
tion of x and thus obtaining fewer/more zero entries in m.
When the context model P (ˆz) is trained, however, we
still train it with respect to the formulation in (8), so it does

m
(cid:101)

(cid:100)

3If z contained zeros before it was masked, we might overestimate the
number of 0 entries in (cid:100)m(cid:101). However, we can redeﬁne those entries of m
as 0 and this will give the same result after masking.

4

not have direct access to the mask m and needs to learn the
dependencies on the entire masked symbol volume ˆz. This
means that when encoding an image, we can stick to stan-
dard adaptive arithmetic coding over the entire bottleneck,
without needing to resort to a two-step coding process as in
[9], where the mask is ﬁrst encoded and then the remaining
symbols. We emphasize that this approach hinges critically
on the context model P and the encoder E being trained
concurrently as this allows the encoder to learn a meaning-
ful (in terms of coding cost) mask with respect to P (see the
next section).

≈

m

) being ignored.
(cid:101)

In our experiments we observe that during training, the
two entropy losses (7) and (18) converge to almost the same
3.5% smaller due to
value, with the latter being around
H(
(cid:100)
While the importance map is not crucial for optimal rate-
distortion performance, if the channel depth K is adjusted
carefully, we found that we could more easily control the
entropy of ˆz through β when using a ﬁxed K, since the net-
work can easily learn to ignore some of the channels via the
importance map. Furthermore, in the supplementary mate-
rial we show that by using multiple importance maps for a
single network, one can obtain a single model that supports
multiple compression rates.

3.5. Putting the pieces together

We made an effort to carefully describe our formulation
and its motivation in detail. While the description is lengthy,
when putting the resulting pieces together we get a quite
straightforward pipeline for learned image compression, as
follows.

Given the set of training images

, we initialize (fully

convolutional) CNNs E, D, and P , as well as the centers
C
of the quantizer Q. Then, we train over minibatches
XB =
x(1),
. At each iteration, we
of crops from
{
take one gradient step for the auto-encoder (E, D) and the
quantizer Q, with respect to the rate-distortion trade-off

, x(B)

· · ·

X

}

X

2. Expand importance map y to mask m via (14)

3. Mask z, i.e., z

z

m

←

(cid:12) (cid:100)

(cid:101)

4. Quantize ˆz = Q(z)

5. Compute the context P (ˆz)

6. Decode ˆx = D(ˆz),

which can be computed in parallel over the minibatch on a
GPU since all the models are fully convolutional.

3.6. Relationship to previous methods

We are not the ﬁrst to use context models for adaptive
arithmetic coding to improve the performance in learned
deep image compression. Work [20] uses a PixelRNN-like
architecture [22] to train a recurrent network as a context
model for an RNN-based compression auto-encoder. Li et
al. [9] extract cuboid patches around each symbol in a bi-
nary feature map, and feed them to a convolutional context
model. Both these methods, however, only learn the context
model after training their system, as a post-processing step
to boost coding performance.

In contrast, our method directly incorporates the context
model as the entropy term for the rate-distortion term (1)
of the auto-encoder, and trains the two concurrently. This
is done at little overhead during training, since we adopt a
3D-CNN for the context model, using PixelCNN-inspired
[21] masking of the weights of each layer to ensure causal-
ity in the context model. Adopting the same approach to the
context models deployed by [20] or [9] would be non-trivial
since they are not designed for fast feed-forward computa-
tion. In particular, while the context model of [9] is also
convolutional, its causality is enforced through masking the
inputs to the network, as opposed to our masking of the
weights of the networks. This means their context model
needs to be run separately with a proper input cuboid for
each symbol in the volume (i.e., not fully convolutionally).

LE,D,Q =

1
B

B
(cid:88)

j=1

d(x(j), ˆx(j)) + βM C(ˆz(j)),

(20)

4. Experiments

which is obtained by combining (1) with the estimate (18)
& (19) and taking the batch sample average. Furthermore,
we take a gradient step for the context model P with respect
to its objective (see (7) & (13))

LP :=

1
B

B
(cid:88)

j=1

d(x(j), ˆx(j)) + βC(ˆz(j)).

(21)

To compute these two batch losses, we need to perform

the following computation for each x

∈ XB:

1. Obtain compressed (latent) representation z and im-
portance map y from the encoder: (z, y) = E(x)

Architecture Our auto-encoder has a similar architecture
as [17] but with more layers, and is described in Fig. 2. We
adapt the number of channels K in the latent representation
for different models. For the context model P , we use a
simple 4-layer 3D-CNN as described in Fig. 3.

Distortion measure Following [7, 14], we use the multi-
scale structural similarity index (MS-SSIM) [23] as mea-
sure of distortion d(x, ˆx) = 100
MS-SSIM(x, ˆx))
for our models. MS-SSIM reportedly correlates better with
human perception of distortion than mean squared error
(MSE). We train and test all our models using MS-SSIM.

(1

−

·

5

while using a moderately large β = 10. We use a small
regularization on the weights and note that we achieve very
stable training. We trained our models for 6 epochs, which
took around 24h per model on a single GPU. For P , we use
a LR of 10−

4 and the same decay schedule.

Datasets We train on the the ImageNet dataset from
the Large Scale Visual Recognition Challenge 2012
(ILSVRC2012) [15]. As a preprocessing step, we take ran-
dom 160
160 crops, and randomly ﬂip them. We set aside
100 images from ImageNet as a testing set, ImageNetTest.
Furthermore, we test our method on the widely used Ko-
dak [1] dataset. To asses performance on high-quality full-
resolution images, we also test on the datasets B100 [18]
and Urban100 [5], commonly used in super-resolution.

×

Other codecs We compare to JPEG, using libjpeg4, and
JPEG2000, using the Kakadu implementation5. We also
compare to the lesser known BPG6, which is based on
HEVC, the state-of-the-art in video compression, and which
outperforms JPEG and JPEG2000. We use BPG in the non-
default 4:4:4 chroma format, following [14].

Comparison Like [14], we proceed as follows to com-
pare to other methods. For each dataset, we compress each
image using all our models. This yields a set of (bpp, MS-
SSIM) points for each image, which we interpolate to get
a curve for each image. We ﬁx a grid of bpp values, and
average the curves for each image at each bpp grid value
(ignoring those images whose bpp range does not include
the grid value, i.e., we do not extrapolate). We do this for
our method, BPG, JPEG, and JPEG2000. Due to code being
unavailable for the related works in general, we digitize the
Kodak curve from Rippel & Bourdev [14], who have care-
fully collected the curves from the respective works. With
this, we also show the results of Rippel & Bourdev [14],
Johnston et al. [7], Ball´e et al. [4], and Theis et al. [17].
To validate that our estimated MS-SSIM is correctly im-
plemented, we independently generated the BPG curves for
Kodak and veriﬁed that they matched the one from [14].

Results Fig. 1 shows a comparison of the aforementioned
methods for Kodak. Our method outperforms BPG, JPEG,
and JPEG2000, as well as the neural network based ap-
proaches of Johnston et al. [7], Ball´e et al. [4], and Theis
et al. [17]. Furthermore, we achieve performance compara-
ble to that of Rippel & Bourdev [14]. This holds for all bpps
we tested, from 0.3 bpp to 0.9 bpp. We note that while Rip-
pel & Bourdev and Johnston et al. also train to maximize
(MS-)SSIM, the other methods minimize MSE.

4http://libjpeg.sourceforge.net/
5http://kakadusoftware.com/
6https://bellard.org/bpg/

Figure 2: The architecture of our auto-encoder. Dark gray
blocks represent residual units. The upper part represents
the encoder E, the lower part the decoder D. For the en-
coder, “k5 n64-2” represents a convolution layer with ker-
nel size 5, 64 output channels and a stride of 2. For the de-
coder it represents the equivalent deconvolution layer. All
convolution layers are normalized using batch norm [6], and
use SAME padding. Masked quantization is the quantiza-
tion described in Section 3.4. Normalize normalizes the in-
put to [0, 1] using a mean and variance obtained from a sub-
set of the training set. Denormalize is the inverse operation.

Figure 3:
The architecture of our context model.
“3D k3 n24” refers to a 3D masked convolution with ﬁl-
ter size 3 and 24 output channels. The last layer outputs L
values for each voxel in ˆz.

·

10−

Training We use the Adam optimizer [8] with a mini-
batch size of 30 to train seven models. Each model is trained
to maximize MS-SSIM directly. As a baseline, we used a
3 for each model, but found it
learning rate (LR) of 4
beneﬁcial to vary it slightly for different models. We set
σ = 1 in the smooth approximation (3) used for gradient
backpropagation through Q. To make the model more pre-
dictably land at a certain bitrate t when optimizing (1), we
found it helpful to clip the rate term (i.e., replace the entropy
term βH with max(t, βH)), such that the entropy term is
“switched off” when it is below t. We found this did not
hurt performance. We decay the learning rate by a factor 10
every two epochs. To obtain models for different bitrates,
we adapt the target bitrate t and the number of channels K,

6

Figure 4: Performance of our approach on ImageNetTest, B100, Urban100, where we out-
perform BPG, JPEG and JPEG2000 in MS-SSIM.

Ours 0.124bpp

0.147 bpp BPG

JPEG2000 0.134bpp

0.150bpp JPEG

Figure 5: Example image (kodim21) from the Kodak testing set, compressed with different methods.

In each of the other testing sets, we also outperform
BPG, JPEG, and JPEG2000 over the reported bitrates, as
shown in Fig. 4.

In Fig. 5, we compare our approach to BPG, JPEG,
and JPEG2000 visually, using very strong compression on
kodim21 from Kodak. It can be seen that the output of our
network is pleasant to look at. Soft structures like the clouds
are very well preserved. BPG appears to handle high fre-
quencies better (see, e.g., the fence) but loses structure in
the clouds and in the sea. Like JPEG2000, it produces block
artifacts. JPEG breaks down at this rate. We refer to the
supplementary material for further visual examples.

Ablation study: Context model
In order to show the ef-
fectiveness of the context model, we performed the follow-

ing ablation study. We trained the auto-encoder without en-
tropy loss, i.e., β = 0 in (20), using L = 6 centers and
K = 16 channels. On Kodak, this model yields an av-
erage MS-SSIM of 0.982, at an average rate of 0.646 bpp
(calculated assuming that we need log2(L) = 2.59 bits per
symbol). We then trained three different context models for
this auto-encoder, while keeping the auto-encoder ﬁxed: A
zeroth order context model which uses a histogram to esti-
mate the probability of each of the L symbols; a ﬁrst order
(one-step prediction) context model, which uses a condi-
tional histogram to estimate the probability of each of the
L symbols given the previous symbol (scanning ˆz in raster
order); and P , i.e., our proposed context model. The result-
ing average rates are shown in Table 1. Our context model

7

reduces the rate by 10 %, even though the auto-encoder was
optimized using a uniform prior (see supplementary mate-
rial for a detailed comparison of Table 1 and Fig. 1).

5. Discussion

rate
Model
0.646 bpp
Baseline (Uniform)
0.642 bpp
Zeroth order
First order
0.627 bpp
Our context model P 0.579 bpp

Table 1: Rates for different context models, for the same
architecture (E, D).

Importance map As described in detail in Section 3.4,
we use an importance map to dynamically alter the number
of channels used at different spatial locations to encode an
image. To visualize how this helps, we trained two auto-
encoders M and M (cid:48), where M uses an importance map
and at most K = 32 channels to compress an image, and
M (cid:48) compresses without importance map and with K = 16
channels (this yields a rate for M (cid:48) similar to that of M ). In
Fig. 6, we show an image from ImageNetTest along with
the same image compressed to 0.463 bpp by M and com-
pressed to 0.504 bpp by M (cid:48). Furthermore, Fig. 6 shows the
importance map produced by M , as well as ordered visual-
izations of all channels of the latent representation for both
M and M (cid:48). Note how for M , channels with larger index
are sparser, showing how the model can spatially adapt the
number of channels. M (cid:48) uses all channels similarly.

Input

Importance map of M

Output of M

Latent representation of M

Output of M (cid:48)

Latent representation of M (cid:48)

Our experiments showed that combining a convolutional
auto-encoder with a lightweight 3D-CNN as context model
and training the two networks concurrently leads to a highly
effective image compression system. Not only were we
able to clearly outperform state-of-the-art engineered com-
pression methods including BPG and JPEG2000 in terms
of MS-SSIM, but we also obtained performance compet-
itive with the current state-of-the-art learned compression
method from [14]. In particular, our method outperforms
BPG and JPEG2000 in MS-SSIM across four different
testing sets (ImageNetTest, Kodak, B100, Urban100), and
does so signiﬁcantly, i.e., the proposed method generalizes
well. We emphasize that our method relies on elemen-
tary techniques both in terms of the architecture (standard
convolutional auto-encoder with importance map, convo-
lutional context model) and training procedure (minimize
the rate-distortion trade-off and the negative log-likelihood
for the context model), while [14] uses highly specialized
techniques such as a pyramidal decomposition architecture,
adaptive codelength regularization, and multiscale adver-
sarial training.

The ablation study for the context model showed that our
3D-CNN-based context model is signiﬁcantly more power-
ful than the ﬁrst order (histogram) and second order (one-
step prediction) baseline context models. Further, our ex-
periments suggest that the importance map learns to con-
densate the image information in a reduced number of chan-
nels of the latent representation without relying on explicit
supervision. Notably, the importance map is learned as a
part of the image compression auto-encoder concurrently
with the auto-encoder and the context model, without in-
troducing any optimization difﬁculties. In contrast, in [9]
the importance map is computed using a separate network,
learned together with the auto-encoder, while the context
model is learned separately.

6. Conclusions

In this paper, we proposed the ﬁrst method for learning
a lossy image compression auto-encoder concurrently with
a lightweight context model by incorporating it into an en-
tropy loss for the optimization of the auto-encoder, leading
to performance competitive with the current state-of-the-art
in deep image compression [14].

Future works could explore heavier and more power-
ful context models, as those employed in [22, 21]. This
could further improve compression performance and allow
for sampling of natural images in a “lossy” manner, by sam-
pling ˆz according to the context model and then decoding.

Figure 6: Visualization of the latent representation of the
auto-encoder for a high-bpp operating point, with (M ) and
without (M (cid:48)) incorporating an importance map.

Acknowledgements This work was supported by ETH
Z¨urich and by NVIDIA through a GPU grant.

8

[16] C. Shalizi.

Lecture notes on stochastic processes.

http://www.stat.cmu.edu/˜cshalizi/754/
2006/notes/lecture-28.pdf, 2006.
accessed 15-Nov-2017]. 3

[Online;

[17] L. Theis, W. Shi, A. Cunningham, and F. Huszar. Lossy
image compression with compressive autoencoders. In ICLR
2017, 2017. 1, 2, 4, 5, 6

[18] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted An-
chored Neighborhood Regression for Fast Super-Resolution,
pages 111–126. Springer International Publishing, Cham,
2015. 6

[19] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent,
D. Minnen, S. Baluja, M. Covell, and R. Sukthankar. Vari-
able rate image compression with recurrent neural networks.
arXiv preprint arXiv:1511.06085, 2015. 1, 2

[20] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Min-
nen, J. Shor, and M. Covell. Full resolution image com-
arXiv preprint
pression with recurrent neural networks.
arXiv:1608.05148, 2016. 1, 2, 5

[21] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,
A. Graves, et al. Conditional image generation with pixel-
cnn decoders. In Advances in Neural Information Processing
Systems, pages 4790–4798, 2016. 2, 3, 5, 8

[22] A. Van Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel
In International Conference on

recurrent neural networks.
Machine Learning, pages 1747–1756, 2016. 2, 3, 5, 8
[23] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-
In Asilomar
tural similarity for image quality assessment.
Conference on Signals, Systems Computers, 2003, volume 2,
pages 1398–1402 Vol.2, Nov 2003. 2, 5

[24] M. J. Weinberger, J. J. Rissanen, and R. B. Arps. Applica-
tions of universal context modeling to lossless compression
of gray-scale images. IEEE Transactions on Image Process-
ing, 5(4):575–586, 1996. 2

[25] X. Wu, E. Barthel, and W. Zhang. Piecewise 2d autore-
gression for predictive image coding. In Image Processing,
1998. ICIP 98. Proceedings. 1998 International Conference
on, pages 901–904. IEEE, 1998. 2

References

kodak/. 6

[1] Kodak PhotoCD dataset. http://r0k.us/graphics/

[2] E. Agustsson, F. Mentzer, M. Tschannen, L. Cavigelli,
R. Timofte, L. Benini, and L. Van Gool. Soft-to-hard vector
quantization for end-to-end learning compressible represen-
tations. arXiv preprint arXiv:1704.00648, 2017. 1, 2

[3] J. Ball´e, V. Laparra, and E. P. Simoncelli. End-to-end opti-
mization of nonlinear transform codes for perceptual quality.
arXiv preprint arXiv:1607.05006, 2016. 2

[4] J. Ball´e, V. Laparra, and E. P. Simoncelli.

End-
arXiv preprint

to-end optimized image compression.
arXiv:1611.01704, 2016. 1, 2, 6

[5] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5197–5206, 2015. 6

[6] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
International Conference on Machine Learning, pages 448–
456, 2015. 6

[7] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh,
Im-
T. Chinen, S. Jin Hwang, J. Shor, and G. Toderici.
proved lossy image compression with priming and spatially
adaptive bit rates for recurrent networks. arXiv preprint
arXiv:1703.10114, 2017. 5, 6

[8] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014. 6

[9] M. Li, W. Zuo, S. Gu, D. Zhao, and D. Zhang. Learning con-
volutional networks for content-weighted image compres-
sion. arXiv preprint arXiv:1703.10553, 2017. 1, 2, 4, 5,
8

[10] D. Marpe, H. Schwarz, and T. Wiegand. Context-based adap-
tive binary arithmetic coding in the h. 264/avc video com-
IEEE Transactions on circuits and sys-
pression standard.
tems for video technology, 13(7):620–636, 2003. 2

[11] D. Marpe, H. Schwarz, and T. Wiegand. Context-based adap-
tive binary arithmetic coding in the h. 264/avc video com-
IEEE Transactions on circuits and sys-
pression standard.
tems for video technology, 13(7):620–636, 2003. 2, 3
[12] B. Meyer and P. Tischer. Tmw-a new method for lossless
image compression. ITG FACHBERICHT, pages 533–540,
1997. 2

[13] B. Meyer and P. E. Tischer. Glicbawls-grey level image com-
pression by adaptive weighted least squares. In Data Com-
pression Conference, volume 503, 2001. 2

[14] O. Rippel and L. Bourdev. Real-time adaptive image com-
In Proceedings of the 34th International Con-
pression.
ference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pages 2922–2930, Interna-
tional Convention Centre, Sydney, Australia, 06–11 Aug
2017. PMLR. 2, 5, 6, 8, 11

[15] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,
A. C. Berg, and F. Li. Imagenet large scale visual recognition
challenge. CoRR, abs/1409.0575, 2014. 6

9

Figure 7: 2D vs. 3D CNNs

Algorithm 1 Constructing 3D Masks

Conditional Probability Models for Deep
Image Compression – Suppl. Material

3D probability classiﬁer As mentioned in Section 3.2,
we rely on masked 3D convolutions to enforce the causality
constraint in our probability classiﬁer P .
In a 2D-CNN,
standard 2D convolutions are used in ﬁlter banks, as shown
in Fig. 7 on the left: A W
Cin-dimensional tensor
×
Cout-dimensional tensor using
is mapped to a W (cid:48)
Cout banks of Cin 2D ﬁlters, i.e., ﬁlters can be represented
Cout-dimensional tensors. Note that
as fW ×
all Cin channels are used together, which violates causality:
When we encode, we proceed channel by channel.

fH ×

Cin

H (cid:48)

H

×

×

×

×

Using 3D convolutions, a depth dimension D is intro-
Cin-dimensional tensors
duced. In a 3D-CNN, W
D
H
×
×
×
Cout-dimensional tensors,
are mapped to W (cid:48)
D(cid:48)
H (cid:48)
×
×
Cout-dimensional ﬁlters. Thus,
with fW ×
Cin
fD ×
fH ×
a 3D-CNN slides over the depth dimension, as shown in
Fig. 7 on the right. We use such a 3D-CNN for P , where
we use as input our W
K-dimensional feature map
ˆz, using D = K, Cin = 1 for the ﬁrst layer.

H

×

×

×

×

Figure 8: Left shows a grid of symbols ˆzi, where the black
square denotes some context and the gray cells denote sym-
bols which where previously encoded. Right shows masks.

To explain how we mask the ﬁlters in P , consider the 2D
case in Fig. 8. We want to encode all values ˆzi by iterating
1, . . . , ˆz1).
in raster scan order and by computing p(ˆzi|
We simplify this by instead of relying on all previously en-
c-context around ˆzi (black
coded symbols, we use some c

ˆzi

−

×

10

square in Fig. 8). To satisfy the causality constraint, this
context may only contain values above ˆzi or in the same
row to the left of ˆzi (gray cells). By using the ﬁlter shown
in Fig. 8 in the top right for the ﬁrst layer of a CNN and
the ﬁlter shown in Fig. 8 in the bottom right for subsequent
ﬁlters, we can build a 2D-CNN with a c
c receptive ﬁeld
that forms such a context. We build our 3D-CNN P by gen-
eralizing this idea to 3D, where we construct the mask for
the ﬁlter of the ﬁrst layer as shown in pseudo-code Algo-
rithm 1. The mask for the subsequent layers is constructed
analoguously by replacing “<” in line 7 with “
”. We use
ﬁlter size fW = fH = fD = 3.

≤

×

1: central idx
2: current idx
3: mask
4: for d
5:
6:

←
∈ {
for h

fD)/2
(cid:101)

(fW ·
fH ·
← (cid:100)
1
←
fD-dimensional matrix of zeros
fH ×
fW ×
do
1, . . . , fD}
1, . . . , fH }
∈ {
for w
∈ {
if current idx < central idx then

do
1, . . . , fW }
mask(w, h, d) = 1

do

else

mask(w, h, d) = 0

current idx

current idx + 1

←

7:
8:
9:
10:

11:

fW ×

With this approach, we obtain a 3D-CNN P which oper-
fD-dimensional blocks. We can use P to
ates on fH ×
encode ˆz by iterating over ˆz in such blocks, exhausting ﬁrst
axis w, then axis h, and ﬁnally axis d (like in Algorithm 1).
For each such block, P yields the probability distribution of
the central symbol given the symbols in the block. Due to
the construction of the masks, this probability distribution
only depends on previously encoded symbols.

Multiple compression rates
It is quite straightforward to
obtain multiple operating points in a single network with
our framework: We can simply share the network but use
multiple importance maps. We did a simple experiment
where we trained an autoencoder with 5 different impor-
tance maps. In each iteration, a random importance map
t.
i was picked, and the target entropy was set to i/5
While not tuned for performance, this already yielded a
model competitive with BPG. The following shows the
output of the model for i = 1, 3, 5 (from left to right):

·

On the beneﬁt of 3DCNN and joint training We note
that the points from Table 1 (where we trained different en-

Figure 9: Performance on the Kodak dataset. See text.

tropy models non-jointly as a post-training step) are not di-
rectly comparable with the curve in Fig. 1. This is because
these points are obtained by taking the mean of the MS-
SSIM and bpp values over the Kodak images for a single
model. In contrast, the curve in Fig. 1 is obtained by fol-
lowing the approach of [14], constructing a MS-SSIM vs.
bpp curve per-image via interpolation (see Comparison in
Section 4). In Fig. 9, we show the black curve from Fig. 1,
as well as the mean (MS-SSIM, bpp) points achieved by
the underlying models (
). We also show the points from
◦
Tab. 1 (+). We can see that our masked 3DCNN with joint
training gives a signiﬁcant improvement over the separately
trained 3DCNN, i.e., a 22% reduction in bpp when compar-
ing mean points (the red point is estimated).

Non-realistic images
In Fig. 10, we compare our ap-
proach to BPG on an image from the Manga1097 dataset.
We can see that our approach preserves text well enough to
still be legible, but it is not as crip as BPG (left zoom). On
the other hand, our approach manages to preserve the ﬁne
texture on the face better than BPG (right zoom).

Visual examples The following pages show the ﬁrst four
images of each of our validation sets compressed to low bi-
trate, together with outputs from BPG, JPEG2000 and JPEG
compressed to similar bitrates. We ignored all header infor-
mation for all considered methods when computing the bi-
trate (here and throughout the paper). We note that the only
header our approach requires is the size of the image and an
identiﬁer, e.g., β, specifying the model.

Overall, our images look pleasant to the eye. We see
cases of over-blurring in our outputs, where BPG manages
to keep high frequencies due to its more local approach. An
example is the fences in front of the windows in Fig. 14, top,
or the text in Fig. 15, top. On the other hand, BPG tends to
discard low-contrast high frequencies where our approach
keeps them in the output, like in the door in Fig. 11, top,
or in the hair in Fig. 12, bottom. This may be explained by

7http://www.manga109.org/

Figure 10: Comparison on a non-realistic image. See text.

BPG being optimized for MSE as opposed to our approach
being optimized for MS-SSIM.

JPEG looks extremely blocky for most images due to the

very low bitrate.

11

Ours 0.239 bpp

0.246 bpp BPG

JPEG 2000 0.242 bpp

0.259 bpp JPEG

Ours 0.203 bpp

0.201 bpp BPG

JPEG 2000 0.197 bpp

0.205 bpp JPEG

Figure 11: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Kodak
dataset, along with bit rate.

12

Ours 0.165 bpp

0.164 bpp BPG

JPEG 2000 0.166 bpp

0.166 bpp JPEG

Ours 0.193 bpp

0.209 bpp BPG

JPEG 2000 0.194 bpp

0.203 bpp JPEG

Figure 12: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Kodak
dataset, along with bit rate.

13

Ours 0.385 bpp

0.394 bpp BPG

JPEG 2000 0.377 bpp

0.386 bpp JPEG

Ours 0.365 bpp

0.363 bpp BPG

JPEG 2000 0.363 bpp

0.372 bpp JPEG

Figure 13: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Ur-
ban100 dataset, along with bit rate.

14

Ours 0.435 bpp

0.479 bpp BPG

JPEG 2000 0.437 bpp

0.445 bpp JPEG

Ours 0.345 bpp

0.377 bpp BPG

JPEG 2000 0.349 bpp

0.357 bpp JPEG

Figure 14: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Ur-
ban100 dataset, along with bit rate.

15

Ours 0.355 bpp

0.394 bpp BPG

JPEG 2000 0.349 bpp

0.378 bpp JPEG

Ours 0.263 bpp

0.267 bpp BPG

JPEG 2000 0.254 bpp

0.266 bpp JPEG

Figure 15: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the Ima-
geNetTest dataset, along with bit rate.

16

Ours 0.284 bpp

0.280 bpp BPG

JPEG 2000 0.287 bpp

0.288 bpp JPEG

Ours 0.247 bpp

0.253 bpp BPG

JPEG 2000 0.243 bpp

0.252 bpp JPEG

Figure 16: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the Ima-
geNetTest dataset, along with bit rate.

17

Ours 0.494 bpp

0.501 bpp BPG

JPEG 2000 0.490 bpp

0.525 bpp JPEG

Ours 0.298 bpp

0.301 bpp BPG

JPEG 2000 0.293 bpp

0.315 bpp JPEG

Figure 17: Our approach vs. BPG, JPEG and JPEG 2000 on the ﬁrst and second image of the B100
dataset, along with bit rate.

18

Ours 0.315 bpp

0.329 bpp BPG

JPEG 2000 0.311 bpp

0.321 bpp JPEG

Ours 0.363 bpp

0.397 bpp BPG

JPEG 2000 0.369 bpp

0.372 bpp JPEG

Figure 18: Our approach vs. BPG, JPEG and JPEG 2000 on the third and fourth image of the B100
dataset, along with bit rate.

19

