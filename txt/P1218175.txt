9
1
0
2
 
l
u
J
 
4
 
 
]

V
C
.
s
c
[
 
 
4
v
4
8
1
6
0
.
3
0
8
1
:
v
i
X
r
a

1

The ApolloScape Open Dataset for Autonomous
Driving and its Application

Xinyu Huang*, Peng Wang*, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, Ruigang Yang

Abstract—Autonomous driving has attracted tremendous attention especially in the past few years. The key techniques for a
self-driving car include solving tasks like 3D map construction, self-localization, parsing the driving road and understanding objects,
which enable vehicles to reason and act. However, large scale data set for training and system evaluation is still a bottleneck for
developing robust perception models. In this paper, we present the ApolloScape dataset [1] and its applications for autonomous
driving. Compared with existing public datasets from real scenes, e.g., KITTI [2] or Cityscapes [3], ApolloScape contains much large
and richer labelling including holistic semantic dense point cloud for each site, stereo, per-pixel semantic labelling, lanemark labelling,
instance segmentation, 3D car instance, high accurate location for every frame in various driving videos from multiple sites, cities and
daytimes. For each task, it contains at lease 15x larger amount of images than SOTA datasets. To label such a complete dataset, we
develop various tools and algorithms speciﬁed for each task to accelerate the labelling process, such as joint 3D-2D segment labeling,
active labelling in videos etc. Depend on ApolloScape, we are able to develop algorithms jointly consider the learning and inference of
multiple tasks. In this paper, we provide a sensor fusion scheme integrating camera videos, consumer-grade motion sensors
(GPS/IMU), and a 3D semantic map in order to achieve robust self-localization and semantic segmentation for autonomous driving. We
show that practically, sensor fusion and joint learning of multiple tasks are beneﬁcial to achieve a more robust and accurate system. We
expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor
fusion and multi-task learning in the ﬁeld of computer vision.

Index Terms—Autonomous Driving, Large-scale Datasets, Scene/Lane Parsing, Self Localization, 3D Understanding.

(cid:70)

of,

1 INTRODUCTION

A Successful self-driving vehicle that is widely applied must

include three essential components. Firstly, understanding
the environment, where commonly a 3D semantic HD map
at the back-end precisely recorded the environment. Secondly,
understanding self-location, where an on-the-ﬂy self-localization
system puts the vehicles accurately inside the 3D world, so that it
can plot a path to every target location. Thirdly, understanding
semantics in the view, where a 3D perceptual system detects
other moving objects, guidance signs and obstacles on the road,
in order to avoid collisions and perform correct actions. The
prevailing approaches for solving those tasks from self-driving
companies are mostly dependent on LIDAR [4], whereas vision-
based approaches, which have potentially very low-cost, are still
very challenging and under research. It requires solving tasks
such as learning to do visual 3D scene reconstruction [5], [6],
[7], [8], self-localization [9], [10], semantic parsing [11], [12],
semantic instance understanding [13], [14], object 3D instance
understanding [15], [16], [17], [18], [19] online in a self-driving
video etc. However, the SOTA datasets for supporting these tasks
either have limited amount, e.g., KITTI [2] only has 200 training
images for semantic understanding, or limited variation of tasks,
e.g., Cityscapes [3] only has discrete semantic labelled frames
without tasks like localization or 3D reconstruction. Therefore, in
order to have a holistic training and evaluation of a vision-based
self-driving system, in this paper, we build the Apolloscape [1]
for autonomous driving, which is a growing and uniﬁed dataset
extending previous ones both on the data scale, label density and
variation of tasks.

• X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng, and R. Yang are with

Baidu Research.

* Equal contribution

Speciﬁcally, in current stage, ApolloScape contains properties

1) dense semantics 3D point cloud for the environment (20+

2) stereo driving videos (100+ hours)
3) high accurate 6DoF camera pose. (translation ≤ 50mm,

4) videos at same site under different day times, (morning, noon,

5) dense per-pixel per-frame semantic labelling (35 classes,

6) per-pixel lanemark labelling (35 classes, 160K+ images)
7) semantic 2D instances segmentation (8 classes, 90K+ im-

driving site)

rotation ≤ 0.015◦)

144K+ images)

night)

ages)

8) 2D car keypoints and 3D car instance labelling (70K cars)

With these information, we have released several standard
benchmarks for scene parsing [20], instance segmentation [21],
lanemark parsing [22], self-localization [23] by withholding part
of the data as test set, and our toolkit for visualization and
evaluation has also published [24]. Here, for 3D car instance, we
list the car number we already labelled, and since it is still under
development, we will elaborate it in our future work. Fig. 1 shows
a glance of ApolloScape, which illustrates various information
from the dataset that is necessary for autonomous driving. Our
dataset is still growing and evolving, and will shortly contains new
tasks such as 3D car instance shape and pose, 3D car tracking etc.,
which are important for scene understanding with ﬁner granularity.
In addition, thanks to our efﬁcient labelling pipeline, we are able
to scale the dataset to multiple cities and sites, and we have already
contained 10 cities in China under various driving conditions.

Based on ApolloScape, we are able to develop algorithms

2

Fig. 1: A glance of ApolloScape with various properties. The images are cropped for better visualization.

for jointly considering 3D and 2D simultaneously with multi-
ple tasks like segmentation, reconstruction, self-localization etc.
These tasks are traditionally handled individually [9], [12], or
jointly handled ofﬂine with semantic SLAM [25] which could
be time consuming. However, from a more practical standpoint,
self-driving car needs to handle localization and parsing the
environment on-the-ﬂy efﬁciently. Therefore, in this paper, we
propose a deep learning based online algorithm jointly solving
localization and semantic scene parsing when a 3D semantic map
is available. In our system, we assume to have (a) GPS/IMU signal
to provide a coarse camera pose estimation; (b) a semantic 3D map
for the static environment. The GPS/IMU signals serve as a crucial
prior for our pose estimation system. The semantic 3D map, which
can synthesize a semantic view for a given camera pose, not only
provides strong guidance for scene parsing, but also helps maintain
temporal consistency.

With our framework, the camera poses and scene semantics
are mutually beneﬁcial. The camera poses help establish the
correspondences between the 3D semantic map and 2D semantic
label map. Conversely, scene semantics could help reﬁne camera
poses. Our uniﬁed framework yields better results, in terms of both
accuracy and speed, for both tasks than doing them individually. In
our experiments, using a single Titan Z GPU, the networks in our
system estimates the pose in 10ms with accuracy under 1 degree,
and segments the image 512×608 within 90ms with pixel accuracy
around 96% without model compression, which demonstrates its
efﬁciency and effectiveness.

In summary, the contributions of this work are in three folds,
1) We propose a large and rich dataset, named as ApolloScape,
which includes various tasks, e.g., 3D reconstruction, self-
localization, semantic parsing, instance segmentation etc.,
supporting the training and evaluation of vision-based au-
tonomous driving algorithms and systems.

2) For developing the dataset, we design an efﬁcient and scal-
able 2D/3D joint-labelling pipeline, where various tools are
developed for 2D segmentation, 3D instance understanding
etc. For example, compared with fully manual labelling,
our 3D/2D labelling pipeline saves 70% labeling time for
semantic segmentation.

3) Based on ApolloScape, we developed a deep learning based
joint self-localization and segmentation algorithm, which is

relying on a semantic 3D map. The system fuses sensors from
camera and customer-grad GPS/IMU, which runs efﬁciently
and improves the robustness and accuracy for camera local-
ization and scene parsing.
The structure of this paper is organized as follows. We provide
related work in Sec. 2, and elaborate the collection and labelling of
ApolloScape in Sec. 3. In Sec. 4, we explain the developed efﬁcient
joint segmentation and localization algorithm. Finally, we present
the evaluation results of our algorithms,
the benchmarks for
multiple tasks and corresponding baseline algorithms performed
on these tasks in Sec. 5.

2 RELATED WORKS
Autonomous driving datasets and related algorithms has been an
active research area for years. Here we summarize the related
works in aspects of datasets and most relevant algorithms without
enumerating them all due to space limitation.

2.1 Datasets for autonomous driving.

Most recently, various datasets targeting at solving each individual
visual task for robot navigation have been released such as 3D
geometry estimation [32], [33], localization [9], [34], instance
detection and segmentation [35], [36]. However, focusing on
autonomous driving, a set of comprehensive visual
tasks are
preferred to be collected consistently within a uniﬁed dataset
from driving videos, so that one may explore the mutual beneﬁts
between different problems.

In past years, lots of datasets have been collected in various
cities, aiming to increase variability and complexity of urban
street views for self-driving applications. The Cambridge-driving
Labeled Video database (CamVid) [26] is the ﬁrst dataset with se-
mantic annotated videos. The size of the dataset is small, contain-
ing 701 manually annotated images with 32 semantic classes. The
KITTI vision benchmark suite [2] is later collected and contains
multiple computer vision tasks such as stereo, optical ﬂow, 2D/3D
object detection and tracking. For semantics, it mainly focuses on
detection, where 7,481 training and 7,518 test images are anno-
tated by 2D and 3D bounding boxes, and each image contains up
to 15 cars and 30 pedestrians. Nevertheless, for segmentation, very
few images contain pixel-level annotations, yielding a relatively

3

Dataset

Real

Location Accuracy

Diversity

CamVid [26]

Kitti [2]

Cityscapes [3]

Toronto [27]

Mapillary [28]

BDD100K [29]

SYNTHIA [30]

P.F.B. [31]

ApolloScape

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

-

-

(cid:88)

cm

-

-

cm

meter

meter

-

-

cm

day time

day time

day time
50 cities

Toronto

various weather
day & night
6 continents

various weather
day
4 regions in US

various weather

various weather

80k 3D box

3D

no

no

no

no

box

box

focus on buildings and roads
exact numbers are not available1

pixel: 25k

2D / 2 classes

Annotation

2D

pixel: 701

box: 15k
pixel: 400

pixel: 25k

box: 100k
pixel: 10k

pixel:213k

pixel:250k

pixel: 140k

Video
(cid:88)

Lane

2D / 2 classes

-

-

-

-

(cid:88)

(cid:88)

(cid:88)

no

no

no

no

2D / 2 classes

3D / 2D Video
27 classes

various weather
day time
4 regions in China

3D semantic point
70K 3D ﬁtted cars

1 database is not open to public yet.

TABLE 1: Comparison between our dataset and the other street-view self-driving datasets published. “pixel” represents 2D pixel-level
annotations. “point” represents 3D point-level annotations. “box” represents bounding box-level annotations. “Video” indicates whether 2D
video sequences are annotated. “3D ﬁtted cars” gives the number of car instance we already ﬁtted in the images with a 3D mesh model, which
we will introduce in our future works.

weak benchmark for semantic segmentation. Most recently, the
Cityscapes dataset [3] is specially collected for 2D segmentation
which contains 30 semantic classes. In detail, 5,000 images have
detailed annotations, and 20,000 images have coarse annotations.
Although video frames are available, only one image out of each
video is manually labelled. Thus, tasks such as video segmentation
can not be performed. Similarly, the Mapillary Vistas dataset [28]
provides a larger set of images with ﬁne annotations, which
has 25,000 images with 66 object categories. The TorontoCity
benchmark [27] collects LIDAR data and images including stereo
and panoramas from both drones and moving vehicles. Although
the dataset scale is large, which covers the Toronto area. as
mentioned by authors, it is not possible to manually do per-pixel
labelling of each frame. Therefore, only two semantic classes,
i.e., building footprints and roads, are provided for benchmarks of
segmentation. BDD100K database [29] contains 100K raw video
sequences representing more than 1000 hours of driving hours
with more than 100 million images. Similarly with the Cityscapes,
one image is selected from each video clip for annotation. 100K
images are annotated in bounding box level and 10K images are
annotated in pixel level.

Real data collection is laborious, to avoid the difﬁculties in
real scene collection, several synthetic datasets are also proposes.
SYNTHIA [30] builds a virtual city with Unity development
platform [37], and Play for benchmark [31] extracts ground truth
with GTA game engine. Though large amount of data and ground
truth can be generated, there is still a domain gap [38] between
appearance of synthesized images and the real ones. In general,
models learned in real scenario still generalize better in real
applications such as object detection and segmentation [39], [40].
In Tab. 1, we compare the properties our dataset and other
SOTA datasets for autonomous driving, and show that Apol-
loScape is unique in terms of data scale, granularity of labelling,
task variations within real environments. Later in Sec. 3, we will
present more details about the dataset.

2.2 Self-localization and semantic scene parsing.

As discussed in Sec. 1, we also try to tackle real-time self-
localization and semantic scene parsing back on ApolloScape
given a video or a single image. These two problems have long
been center focus for computer vision. Here we summarize the
related works on outdoor cases with street-view images as input.

Visual self-localization. Traditionally, localizing an image given
a set of 3D points is formulated as a Perspective-n-Point (PnP)
problem [41], [42] by matching feature points in 2D and fea-
tures in 3D through cardinality maximization. Usually in a large
environment, a pose prior is required in order to obtain good
estimation [43], [44]. Campbell et al. [45] propose a global-
optimal solver which leverage the prior. In the case that geo-
tagged images are available, Sattler et al. [46] propose to use
image-retrieval to avoid matching large-scale point cloud. When
given a video, temporal information could be further modeled with
methods like SLAM [47] etc, which increases the localization
accuracy and speed.

Although these methods are effective in cases with distin-
guished feature points, they are still not practical for city-scale
environment with billions of points, and they may also fail in
areas with low texture, repeated structures, and occlusions. Thus,
recently, deep learned features with hierarchical representations
are proposed for localization. PoseNet [9], [48] takes a low-
resolution image as input, which can estimate pose in 10ms w.r.t.
a feature rich environment composed of distinguished landmarks.
LSTM-PoseNet [49] further captures a global spatial context
after CNN features. Given an video, later works incorporate Bi-
Directional LSTM [50] or Kalman ﬁlter LSTM [51] to obtain
better results with temporal information. Most recently, many
works [10], [52] also consider adding semantic cues as more
robust representation for localization. However, in street-view
in most cases,
scenario, considering a road with trees aside,
the visual
no signiﬁcant

landmark appears, which could fail

models. Thus, signals from GPS/IMU are a must-have for robust
localization in these cases [53], whereas the problem switched
to estimating the relative pose between the camera view from a
noisy pose and the real pose. For ﬁnding relative camera pose of
two views, recently, researchers [54], [55] propose to stack the
two images as a network input. In our case, we concatenate the
real image with an online rendered label map from the noisy pose,
which provides superior results in our experiments. Street scene

parsing. For parsing a single image of street views (e.g., these
from CityScapes [3]), most state-of-the-arts (SOTA) algorithms
are designed based on a FCN [11] and a multi-scale context
module with dilated convolution [12], pooling [56], CRF [57], or
spatial RNN [58]. However, they are dependent on a ResNet [59]
with hundreds of layers, which is too computationally expen-
sive for applications that require real-time performance. Some
researchers apply small models [60] or model compression [61]
for acceleration, with the cost of reduced accuracy. When the input
is a video, spatial-temporal informations are jointly considered,
Kundu et al. [62] use 3D dense CRF to get temporally consistent
results. Recently, optical ﬂow [63] between consecutive frames
is computed to transfer label or features [64], [65] from the
previous frame to current one. In our case, we connect consecutive
video frames through 3D information and camera poses, which
is a more compact representation for static background. In our
case, we propose the projection from 3D maps as an additional
input, which alleviates the difﬁculty of scene parsing solely from
image cues. Additionally, we adopt a light weighted network from
DeMoN [55] for inference efﬁciency.

Joint 2D-3D for video parsing. Our work is also related to joint
reconstruction, pose estimation and parsing [25], [66] through
embedding 2D-3D consistency. Traditionally, reliant on structure-
from-motion (SFM) [66] from feature or photometric matching,
those methods ﬁrst reconstruct a 3D map, and then perform
semantic parsing over 2D and 3D jointly, yielding geometrically
consistent segmentation between multiple frames. Most recently,
CNN-SLAM [67] replaces traditional 3D reconstruction module
with a single image depth network, and adopts a segment network
for image parsing. However, all these approaches are processed
off-line and only for static background, which do not satisfy our
online setting. Moreover, the quality of a reconstructed 3D model
is not comparable with the one collected with a 3D scanner.

3 BUILD ApolloScape
In this section, we introduce our acquisition system, speciﬁcations
about the collected data and efﬁcient labelling process for building
ApolloScape.

3.1 Acquisition system

In Fig. 2, we visualize our collection system. To collect static 3D
environment, we adopt Riegl VMX-1HA [68] as our acquisition
system that consists of two VUX-1HA laser scanners (360◦ FOV,
range from 1.2m up to 420m with target reﬂectivity larger than
80%), one VMX-CS6 camera system (two front cameras are
used with resolution 3384 × 2710), and a measuring head with
IMU/GNSS (position accuracy 20 ∼ 50mm, roll & pitch accuracy
0.005◦, and heading accuracy 0.015◦). The laser scanners utilizes
two laser beams to scan its surroundings vertically that are similar
to the push-broom cameras. Comparing with common-used Velo-
dyne HDL-64E [4], the scanners are able to acquire higher density

4

Fig. 2: Acquisition system consists of two laser scanners, up to six
video cameras, and a combined IMU/GNSS system.

Count

Kitti
(box)

Cityscapes
(pixel)

BDD100K
(box)

ApolloScape
(pixel)

total (×104)

average per image

person
vehicle

person
vehicle

0.6
3.0

0.8
4.1

2.4
4.1

7.0
11.8

12.9
110.2

1.3
11.0

54.3
198.9

m

6.2
24.0

e

1.1
12.7

h

16.9
38.1

TABLE 2: Total and average number of instances in KITTI [2],
Cityscapes [3], BDD100K [29], and ApolloScape (instance-level).
“pixel” represents 2D pixel-level annotations. “box” represents bound-
ing box-level annotations. The letters, e, m, and h, indicate easy,
moderate, and hard subsets in ApolloScape respectively.

of point clouds and obtain higher measuring accuracy / precision
(5mm / 3mm). The whole system has been internally calibrated
and synchronized, and is mounted on the top of a mid-size SUV.
Additionally, the system contains two high frontal camera
capturing with a resolution of 3384 × 2710, and is well cali-
brated with the LIDAR device. Finally, to obtain high accurate
GPS/IMU information, a temporary GPS basement is set up near
the collection site to make sure the localization of the camera is
sufﬁciently accurate for us to match the 2D image and 3D point
cloud. Commonly, our vehicle drives at the speed of 30km per
hour and the cameras are triggered once every meter, i.e., 30fps.

3.2 Speciﬁcations

Here, based on the acquisition system, we ﬁrst present the spec-
iﬁcations of Apolloscape w.r.t. different tasks, e.g., predeﬁned
semantic classes, lanemark classes and instance etc., to allow
better overview of the dataset. In Sec. 3.3, we will introduce our
active labelling pipeline which allows us to efﬁciently produce the
ground truth of multiple tasks simultaneously.

Semantic scene parsing. In our current version released on-
line [20], [21], we have 143,906 video frames and their cor-
responding pixel-level semantic labelling, from which 89,430
images contain instance-level annotations where movable objects
are further separated. Notice that our labelled images contains tem-
poral information which could also be useful for video semantic
and object segmentation.

To make the evaluation more comprehensive, similar to the
KITTI [2], we separate the recorded video with the level of easy,

5

Fig. 3: Examples with challenging environments for object detection and segmentation (Images are center-cropped for better visualization).
We highlight and zoom in the region of challenges in each image. (a) Objects with heavy occlusion and small scale. (b) Abnormal action by
cyclist drivers. (c) High contrast and overexposure due to shadows and strong sunlight. (d) Mirror reﬂection on bus glasses.

moderate, and heavy scene complexities based on the amount of
movable objects, such as person and vehicles. Tab. 2 compares the
scene complexities between ApolloScape, the Cityscapes [3] and
KITTI [2], where we show the statistics for each individual classes
of movable objects. ApolloScape contains more objects than others
in terms of both total number and average number of object
instances from images. More importantly, it contains stronger
challenging environments, as shown in Fig. 3. For instance, high
contrast regions due to sun light and large area of shadows from
the overpass. Mirror reﬂections of multiple nearby vehicles on a
bus glass due to highly crowded transportation. We hope these
case can help and motivate researchers to develop more robust
models against environment changes.

For semantic scene parsing, we annotate 24 different labels
in four groups. The speciﬁcations of the classes are partially
borrowed from the Cityscapes dataset. Fig. 4 gives the amount of
labelled pixels for each class. As expected, ApolloScape provides
much higher amount of average annotated pixels than Cityscapes,
especially for some rare classes, e.g.,
trafﬁc-light, pole. Here, we
add several new classes common in China. For instance, we add
“tricycle” that is one of the most popular means of transportation.
This class covers all kinds of three-wheeled vehicles that could
be both motorized and human-powered. The rider class in the
Cityscape is deﬁned as the person on means of transportation.
Here, we consider the person and the means of transportation as a
single moving object, and treat the two together as one class. The
three classes related to rider, i.e., bicycle, motorcycle, and tricycle,
represent means of transportation without rider and parked along
the roads.

Semantic lanemark segmentation. Automatically understanding
lane mark is perhaps the most important function for autonomous
driving since it is the guidance for possible actions. In Apol-
loScape, 27 different lane markings are used for evaluation as elab-
orated in Tab. 3 and Fig. 5. The labels are deﬁned based on lane
mark attributes including color (e.g., white and yellow) and type
(e.g., solid and broken). To be speciﬁc, 165949 images from 3 road
sites are labelled and released online [22], where 33760 images are
withheld for testing. Comparing to other public available datasets
such as KITTI [2] or the one from Tusimple [69], ApolloScape is
the ﬁrst large dataset containing rich semantic labelling for lane
marks with many variations.

Self-localization. Each frame of our recorded video is tagged
with high accurate GPS/IMU signal automatically. Therefore, the

Type

Color

Use

solid
solid
double solid
double solid
solid & broken
solid & broken

broken
broken
double broken

crosswalk

solid

solid
solid

solid

arrow
arrow
arrow
arrow
arrow
arrow
arrow
arrow

bump

diamond

rectangle

visible old marking
other markings

w
y
w
y
y
w

w
y
y

w

w
y

w

w

w
w
w
w
w
w
w
w

n/a

w/y

w/y

y/w
n/a

dividing
dividing
dividing, no pass
dividing, no pass
dividing, one-way pass
dividing, one-way pass

guiding
guiding
guiding

stopping

chevron
chevron

parking

zebra

u-turn
thru
thru & left turn
thru & right turn
left turn
right turn
left & right turn
left & u-turn

speed reduction

zebra attention

no parking

others
others

TABLE 3: Details of lane mark labels in our dataset (y: yellow,
w:white).

videos we released for segmentation are also available for self-
localization research. However, for setting up a benchmark, we
additionally collected a much larger amount of videos, which has
not been semantically labelled. Speciﬁcally, videos for localiza-
tion, as published at [23], contain 6 more roads at 4 different
cities, which include roughly 300k images, and road of 28km.

Our dataset has variations under different lighting, i.e., morn-
ing, noon and night, and driving conditions, i.e., rush and non-rush
hours, with stereo pair of images available. In addition, each road
has a survey-grade point cloud based 3D map that can be used
in ﬁnding matching pixels for both supervised and unsupervised

6

Fig. 4: 24 semantic classes and corresponding numbers of annotated pixels. Bar colors indicate different semantic groups.

Fig. 5: 27 lane mark labels and corresponding numbers of annotated pixels. Bar colors indicate 11 different lane mark usages. Here,“s w d”
is short for solid, white and dividing in Tab. 3 by combining the ﬁrst letter of type, color and usage respectively, and other classes are named
accordingly.

feature learning [70], [71] etc. Finally, we record each road
by driving from start-to-end and then end-to-start, which means
each position along a road will be looked at from two opposite
directions. This enables the research of camera localization with
large view changes such as that proposed in semantic visual
localization [10].

multiple rounds; 2) align these point clouds based on manually
selected control points; 3) remove the points based on the temporal
consistency. Formally, the condition to kept a point x in round j
is,

1(∃ xi ∈ Pi s.t. (cid:107)xi − xj(cid:107) < (cid:15)d)/r ≥ δ

(1)

r
(cid:88)

i=0

3.3 Labeling Process

In order to make our labeling of video frames accurate and efﬁ-
cient, we propose an active labelling pipeline by jointly consider
2D and 3D information, as shown in Fig. 6. The pipeline mainly
consists of two stages, 3D labeling and 2D labeling, to handle
static background/objects and moving objects respectively. The
basic idea of our pipeline is similar to the one described in [72],
which transfers the 3D labelled results to 2D images by camera
projection, while we need to handle much larger amount of data
and have different set up of the acquisition vehicle. Thus some
key techniques used in our pipeline are re-designed, which we
will elaborate later.

Moving object removal. As mentioned in Sec. 3.1, LIDAR
scanner Riegl is accurate in static background, while due to low
scanning rate, the point clouds of moving objects, such as vehi-
cles and pedestrians running on the road, could be compressed,
expanded, or completely missing in the captured point clouds as
illustrated in Fig. 8(b). Thus, we design to handle labelling static
background and moving object separately, as shown in Fig. 6.
Speciﬁcally, in the ﬁrst step, we do moving object removal from
our collected point clouds by 1) scan the same road segment

where δ = 0.6 and (cid:15)d = 0.025m in our setting, and 1() is an
indicator function. It indicates that a 3D point will be kept if it
appears with high frequency in many rounds of recording, i.e.,
60% of all times. We keep the remained point clouds as a static
background M for semantic labelling. 3D labelling. Next, for

labelling static background (3D Labeling), rather than label each
3D point and loading all the points, we ﬁrst separate the 3D points
into multiple parts, and over-segment each part of point clouds
into point clusters based on spatial distances and normal directions
using locally convex connected patches (LCCP) [73] implemented
with PCL [74]. Then, we label these point clusters manually using
our in-house developed 3D labelling tool as shown in Fig. 7, which
can easily do point cloud rotation, (inverse-)selection by polygons,
matching between point clouds and camera views, etc.. Notice at
this stage, there will be point clouds belonging to movable but
static objects such as bicycles and cars parking aside the road.
These point clouds are remained in our background, and also
labelled in 3D which are valuable to increase our label efﬁciency
of objects in 2D images.

To further improve 3D point cloud labelling efﬁciency, after
labelling of one road, we actively train a PointNet++ model [75]
to pre-label the over-segmented point cloud clusters of the next

7

Fig. 6: Our 2D/3D labeling pipeline that label static background/objects and moving objects separately. We also adopt active strategies for
accelerating the labelling process for scalability of the labelling process. For inputs, since GPS/IMU still has some errors, we manually add
control points to better align the point clouds and our image frames. In Sec. 3.3, we present the details of each components.

sc ∝

(cid:88)

1
|Pc|

min
t∈T

d(x, t)

(2)

x∈Pc
where Pc is the set of 3D points belong to class c, and T is
the set of ground truth camera poses. Then, given the relative
square size between different classes, we deﬁne an absolute range
to obtain the actual square size for splatting. This is non-trivial
since too large size will result in dilated edges, while too small
size will yield many holes. In our experiments, we set the range as
[0.025, 0.05], and ﬁnd that it provides the highest visual quality.
As shown in Fig. 8(e), invalid values in-between those projected
points are well in-painted, meanwhile the boundaries separating
different semantic classes are also well preserved, yielding the
both the background depth map and 2D labelled background. With
such a strategy, we increase labelling efﬁciency and accuracy for
video frames. For example, it could be very labor-intensive to
label texture-rich regions like trees, poles and trafﬁc lights further
away, especially when occlusion happens like fence on the road as
illustrated in Fig. 8(g).

2D labelling of objects and backgrounds. Finally, to generate
the ﬁnal labels (Fig. 8(f)), we need to label the moving objects in
the environments, and ﬁx missing parts at background like part of
building regions. Similar with 3D point cloud labelling, we also
developed an in-house 2D labelling tool with the same interface as
3D tool in Fig. 7. To speed up the 2D semantic labeling, we also
use a labelling strategy by training a CNN network for movable
objects and background [76] to pre-segment the 2D images. For
segmenting background, we test with original image resolution
collected by our camera, where the resolution is much higher
than that used in the original paper to increase the quality of
predicted region boundaries. For segment objects, similar with
MaskRCNN [13], we ﬁrst do 2D object detection with faster
RCNN [77], and segment object masks inside. However, since we
consider high requirements for object boundaries rather than class
accuracy, for each bounding box with high conﬁdence (≥ 0.9),
we enlarge the bounding box and crop out the object region with
context similar to [78]. Then, we upsample the cropped image to
a higher resolution by setting a minimum resolution of prediction
(minimum len greater than 512), and segment out the mask with
an actively trained mask CNN network with the same architecture
in [76]. The two networks for segmenting background and objects
are updated when images in one road is labelled. Here, the learning
parameters from these networks follow the original papers.

Finally, the segmented results from the networks are fused
with our rendered label map from the semantic 3D point clouds

Fig. 7: The user interface of our 3D labeling tool. At left-top, we
show the pre-deﬁned color code of different classes. At left-bottom,
we show the labelling logs which can be used to revert the labelling
when mistakes happen. At center part, labelled point cloud is shown
indicating the labelling progress.

road. Labellers are then asked to reﬁne and correct the results
by ﬁxing wrong annotations, which often occur near the object
boundaries. With the growing number of labelled point clouds,
our learned model can label new roads with increasing accuracy,
yielding accelerated labelling process, which scales up to various
cities and roads.

Splatting & Projection. Once the 3D annotations are generated,
the annotations of static background/objects for all the 2D image
frames are generated automatically by 3D-2D projections. In our
setting, the 3D map is a point cloud based environment. Although
the density of the point cloud is very high (one point per 25mm
within road regions), when the 3D points are far away from
the camera, the projected labels could be sparse, e.g., regions
of buildings shown in Fig. 8(c). Thus for each point
in the
environment, we adopt the point splatting technique, by enlarging
the 3D point to a square where the square size is determined by
its semantic class.

Formally, given a 6-DOF camera pose p = [q, t] ∈ SE(3),
where q ∈ SO(3) is the quaternion representation of rotation
and t ∈ R3 is translation, a label map can be rendered from
the semantic 3D map, where z-buffer is applied to ﬁnd the closest
point at each pixel. For a 3D point x belonging a class c, its square
size sc is set to be proportional to the class’ average distance to
the camera. Formally,

8

Fig. 8: A labelled example of the labelling pipeline for semantic parsing, a subset of color coded labels are shown below. (a) Image. (b)
Rendered label map with 3D point cloud projection, with an inaccurate moving object (rider) circled in blue. (c) Rendered label map with 3D
point cloud projection after points with low temporal consistency being removed. (d) & (e) Rendered depth map of background and rendered
label map after class dependent splatting in 3D point clouds (Sec. 3.3). (f) Merged label map with missing region in-painted, moving objects
and sky. (g) Another label map with very small trafﬁc lights. Details are zoomed out highlighting the details of our rendered label maps. Other
examples of our labeled videos is shown online [20].

following two rules: 1) for fusing segmented label map from
the background network, we ﬁll the predicted label in the pixels
without 3D projection, yielding a background semantic map. 2)
for fusing semantic object label segmented by object network, we
pasted the object mask over the fused background map, without
replacing the projected static movable object mask rendered from
3D points as mentioned in 3D labelling. We provided this fused
label map for labellers to further ﬁne tuning when error happens
especially around object boundary or occlusion from the object
masks. In addition, the user can omit any of the pre-segmented
results from CNNs to do relabelling if the segmented results are far
from satisfaction. Our label tool supports multiple actions such as
polygons and pasting brushes etc., which are commonly adopted
by many popular open source label tools 1.

A ﬁnal labelled example is shown in Fig. 8(f)&(g). Notice that
some background classes such as fence, trafﬁc light, and vege-
tation are annotated in details using our projection and missing
parts such as building glass can be ﬁll in. Thanks to 3D and active
learning, our overall pipeline save us signiﬁcant efforts in dense
per-pixel and per-frame semantic labelling for background and
objects. In practice, our labelling pipeline can reduce the time cost
of dense labelling task per-image from nearly 1 hour to around 10
minutes, with the guarantee of passing our quality control process.

Labelling of lane mark segments on road. In self-driving, lane
marks are information solely from static background. Fortunately,
our collected survey-grade 3D points not only have high density,
but also contain lighting intensity, dependent on which we can
distinguish the lane mark on the roads. Speciﬁcally, we perform
similar labelling process as 3D labelling of rigid background by
labelling each 3D point to pre-deﬁned lane mark labels listed in
Tab. 3.

Nevertheless, different from labelling 3D point clusters where
point clouds from buildings and trees are important, for lane
marks, we only need to consider points on the road. Therefore,
we take out the road point clouds based on normal directions,
and perform orthogonal projection of these points from the bird

1. https://github.com/topics/labeling-tool

view to a high resolution 2D image, as shown in Fig. 9, over
which labellers draw a polygon for each lane mark on the road. In
the meantime, our tool brings out the corresponding images, and
highlights the regions in 2D for each labelled polygon, where the
color and type of the labelled lanemark can be determined.

Labelling of instance segments. Thanks to an active labelling
component with detection, it is easy for us to generalize the
segmentation label map to produce instance masks given the
segmented results from the object detection and segmentation
networks. Speciﬁcally, we ask the labellers to reﬁne the boundary
between different instances when it is necessary, i.e., visually
signiﬁcantly not aligned with true object boundaries.

Control of label quality. Following the existing standard work
ﬂows of crowdsourcing object annotations [79], [80], [81], all our
2D/3D labeling tasks, e.g., 3D point cloud, 2D background, 2D
instance and 3D lanemark, contain veriﬁcation stages to control
the label quality. Speciﬁcally, for each task, we have a detailed
instruction to train our labellers, and a labeller is good to start
labelling after passing a designed quiz. We will publish all our
instructions on our website to beneﬁt the community upon the
publication of this paper.

After the labelling stage, we have a review stage, and each
reviewer is an experienced labeller had sufﬁcient labelled images
(over 500) passed our label quality veriﬁcation. The reviewer will
verify the quality and the coverage of labelled regions. In addition,
since we do video labelling, we also have reviewer to visually
verify the semantics are temporally consistent in the next frame.
Only if an image has passed two reviewers, it could be accepted
as a valid ground truth.

Existing issues. LiDAR scanners could fail on translucent and
highly reﬂective surfaces such as mirrors of buildings. Though we
ﬁxed this problem in part of our recorded videos, e.g., as shown
in Fig. 8), we found it is still over laborious to ﬁx every frame
in all our videos even with active labelling. Therefore, part of
video frames in our current release, pixels without 3D projection
or active labelling, e.g., sky and part of building in Fig. 1, are set

9

Fig. 9: Bird view of our projected road lane marks with labelling.

as void, so that they are ignored during the training and evaluation.
We leave labelling of these pixels to our future work.

4 DEEP LOCALIZATION AND SEGMENTATION
As discussed in introduction (Sec. 1), ApolloScape contains
various ground truth which enables multitask learning. In this
paper, we show such a case by creating a deep learning based
system for joint localization and semantic segmentation given a
semantic 3D map [82], which we call DeLS-3D, as illustrated
in Fig. 10. Speciﬁcally, at upper part, a pre-built 3D semantic
map is available. During testing, an online stream of images
and corresponding coarse camera poses from GPS/IMU are fed
into the system. Firstly, for each frame, a semantic label map is
rendered out given the input coarse camera pose, which is fed
into a pose CNN jointly with the respective RGB image. The
network calculates the relative rotation and translation, and yields
a corrected camera pose. To incorporate the temporal correlations,
the corrected poses from pose CNN are fed into a pose RNN
to further improves the estimation accuracy in the stream. Last,
given the rectiﬁed camera pose, a new label map is rendered out,
which is fed together with the image to a segment CNN. The
rendered label map helps to segment a spatially more accurate
and temporally more consistent result for the image stream of
video. In this system, since ApolloScape contains ground truth for
both camera poses and segments, it can be trained with strong
supervision at each end of outputs. The code for our system
has been released at https://github.com/pengwangucla/DeLS-3D.
In the following, we elaborate our network architectures and the
loss functions to train the whole system.

4.1 Camera localization with motion prior

Translation rectiﬁcation with road prior. One common lo-
calization priori for navigation is to use the 2D road map, by
constraining the GPS signals inside the road regions. We adopt a
similar strategy, since once the GPS signal is out of road regions,
the rendered label map will be totally different from the street-
view, and no correspondence can be found by the network.

To implement this constraint, ﬁrstly we render a 2D road map
image with a rasterization grid of 0.05m from our 3D semantic
map by using only road 3D points, i.e., points belong to car-lane,
pedestrian-lane and bike-lane etc. Then, at each pixel [x, y] ∈ Z2
in the 2D map, an offset value f (x, y) is pre-calculated indicating
its 2D offset to the closest pixel belongs to road through the breath-
ﬁrst-search (BFS) algorithm efﬁciently.

During online testing, given a noisy translation t = [tx, ty, tz],
we can ﬁnd the closest road points w.r.t. t using [tx, ty] +

f ((cid:98)tx(cid:99), (cid:98)ty(cid:99)) from our pre-calculated offset function. Then, a
label map is rendered based on the rectiﬁed camera pose, which is
fed to pose CNN.

CNN-GRU pose network architecture. As shown in Fig. 10,
our pose networks contain a pose CNN and a pose GRU-RNN.
Particularly, the CNN of our pose network takes as inputs an
image I and the rendered label map L from corresponding coarse
camera pose pc
i . It outputs a 7 dimension vector ˆpi representing
the relative pose between the image and rendered label map, and
we can get a corrected pose w.r.t. the 3D map by pi = pc
i + ˆpi.
For the network architecture of pose CNN, we follow the design
of DeMoN [55], which has large kernel to obtain bigger context
while keeping the amount of parameters and runtime manageable.
The convolutional kernel of this network consists a pair of 1D
ﬁlters in y and x-direction, and the encoder gradually reduces the
spatial resolution with stride of 2 while increasing the number of
channels. We list the details of the network in our implementation
details at Sec. 5.

Additionally, since the input is a stream of images, in order to
model the temporal dependency, after the pose CNN, a multi-layer
GRU with residual connection [83] is appended. More speciﬁcally,
we adopt a two layer GRU with 32 hidden states as illustrated in
Fig. 11. It includes high order interaction beyond nearby frames,
which is preferred for improve the pose estimation performance.
In traditional navigation applications of estimating 2D poses,
Kalman ﬁlter [84] is commonly applied by assuming either a
constant velocity or acceleration. In our case, because the vehicle
velocity is unknown, transition of camera poses is learned from
the training sequences, and in our experiments we show that the
motion predicted from RNN is better than using a Kalman ﬁlter
with a constant speed assumption, yielding further improvement
over the estimated ones from our pose CNN.

Pose loss. Following the PoseNet [48], we use the geometric
matching loss for training, which avoids the balancing factor
between rotation and translation. Formally, given a set of point
cloud in 3D P = {x}, and the loss for each image is written as,

L(p, p∗) =

ωlx|π(x, p) − π(x, p∗)|2

(3)

(cid:88)

x∈P

where p and p∗ are the estimated pose and ground truth pose
respectively. π() is a projective function that maps a 3D point x
to 2D image coordinates. lx is the semantic label of x and ωlx is
a weight factor dependent on the semantics. Here, we set stronger
weights for point cloud belong to certain classes like trafﬁc light,
and ﬁnd it helps pose CNN to achieve better performance. In [48],
only the 3D points visible to the current camera are applied to

10

Fig. 10: DeLS-3D overview. The black arrows show the testing process, and red arrows indicate the rendering (projection) operation in training
and inference. The yellow frustum shows the location of cameras inside the 3D map. The input of our system contains a sequence of images
and corresponding GPS/IMU signals. The outputs are the semantically segmented images, each with its reﬁned camera pose.

compute this loss to help the stability of training. However, the
amount of visible 3D points is still too large in practical for
us to apply the loss. Thus, we pre-render a depth map for each
training image with a resolution of 256 × 304 using the ground
truth camera pose, and use the back projected 3D points from the
depth map for training.

4.2 Video parsing with pose guidance

Having rectiﬁed pose at hand, one may direct render the semantic
3D world to the view of a camera, yielding a semantic parsing of
the current image. However, the estimated pose is not perfect, ﬁne
regions such as light poles can be completely misaligned. Other
issues also exist. For instance, many 3D points are missing due
to reﬂection, e.g., regions of glasses, and points can be sparse
at long distance. Last, dynamic objects in the input cannot be
represented by the projected label map, yielding incorrect labelling
at corresponding regions. Thus, we propose an additional segment
CNN to tackle these issues, while taking the rendered label map
as segmentation guidance.

Segment network architecture. As discussed in Sec. 2, heavily
parameterized networks such as ResNet are not efﬁcient enough

Fig. 11: The GRU RNN network architecture for modeling a sequence
of camera poses.

for our online application. Thus, as illustrated in Fig. 12, our
segment CNN is a light-weight network containing an encoder-
decoder network and a reﬁnement network, and both have similar
architecture with the corresponding ones used in DeMoN [55]
including 1D ﬁlters and mirror connections. However, since we
have a segment guidance from the 3D semantic map, we add
a residual stream (top part of Fig. 12), which encourages the
network to learn the differences between the rendered label map
and the ground truth. In [85], a full resolution stream is used to
keep spatial details, while here, we use the rendered label map to
keep the semantic spatial layout.

Another notable difference for encoder-decoder network from
DeMoN is that for network inputs, shown in Fig. 12, rather than
directly concatenate the label map with input image, we transform
the label map to a score map through one-hot operation, and
embed the score of each pixel to a 32 dimensional feature vector.
Then, we concatenate this feature vector with the ﬁrst layer output
from image, where the input channel imbalance between image
and label map is alleviated, which is shown to be useful by
previous works [86]. For reﬁnement network shown in Fig. 12,
we use the same strategy to handle the two inputs. Finally, the
segment network produces a score map, yielding the semantic
parsing of the given image.

We train the segment network ﬁrst with only RGB images, then
ﬁne-tune the network by adding the input of rendered label maps.
This is because our network is trained from scratch, therefore
it needs a large amount of data to learn effective features from
images. However, the rendered label map from the estimated pose
has on average 70% pixel accuracy, leaving only 30% of pixels
having effective gradients. This could easily drive the network to
over ﬁt to the rendered label map, while slowing down the process
towards learning features from images. Finally, for segmentation
loss, we use the standard softmax loss, and add intermediate
supervision right after the outputs from both the encoder and the
decoder as indicated in Fig. 12.

11

Fig. 12: Architecture of the segment CNN with rendered label map as a segmentation priori. At bottom of each convolutional block, we show
the ﬁlter size, and at top we mark the downsample rates of each block w.r.t. the input image size. The ‘softmax’ text box indicates the places a
loss is calculated. Details are in Sec. 4.2.

5 EXPERIMENTS

In this section, we ﬁrst evaluate our online deep localization and
segmentation algorithms (DeLS-3D) on two of our released roads,
which is a subset of our full data. We compare it against other
SOTA deep learning based visual localization, i.e., PoseNet [9],
and segmentation algorithms i.e., ResNet38 [76], which shows the
beneﬁts of multitask uniﬁcation.

Then, we elaborate the benchmarks setup online with Apol-
loScape and the current leading results, which follows many stan-
dard settings such as the ones from KITTI [2] and Cityscapes [3].
These tasks include semantic segmentation, semantic instance
segmentation, self-localization, lanemark segmentation. Due to the
“DeLS” algorithm proposed in this work does not follow those
standard experimental settings, we could not provide its results
for the benchmarks. Nevertheless, for each benchmark, we either
ran a baseline result with SOTA methods or launched a challenge
for other researchers, providing a reasonable estimation of the task
difﬁculties.

5.1 Evaluate DeLS-3D

In this section, we evaluate various settings for pose estimation and
segmentation to validate each component in the DeLS-3D system.
For GPS and IMU signal, despite we have multiple scans for the
same road segments, it is still very limited for training. Thus,
follow [53], we simulate noisy GPS and IMU by adding random
perturbation (cid:15) w.r.t. the ground truth pose following uniform
distributions. Speciﬁcally, translation and rotation noise are set
as (cid:15)t ∼ U (0, 7.5m) and (cid:15)r ∼ U (0◦, 15◦) respectively. We refer
to realistic data [87] for setting the noisy range of simulation.

Datasets. Two roads early collected at Beijing in China are used
in our evaluation. The ﬁrst one is inside a technology park, named
zhongguancun park (Zpark), and we scanned 6 rounds during
different daytimes. The 3D map generated has a road length
around 3km, and the distance between consecutive frames is
around 5m to 10m. We use 4 rounds of the video camera images
for training and 2 for testing, yielding 2242 training images and
756 testing images. The second one we scanned 10 rounds and
4km near a lake, named daoxianghu lake (Dlake), and the distance
between consecutive frames is around 1m to 3m. We use 8 rounds
of the video camera images for training and 2 for testing, yielding
17062 training images and 1973 testing images. The existing

semantic classes in the two datasets are shown in Tab. 5, which
are subsets from our full semantic classes.

Implementation details. To quickly render from the 3D map, we
adopt OpenGL to efﬁciently render a label map with the z-buffer
handling. A 512 × 608 image can be generated in 70ms with a
single Titan Z GPU, which is also the input size for both pose CNN
and segment CNN. For pose CNN, the ﬁlter sizes of all layers are
{32, 32, 64, 128, 256, 1024, 128, 7}, and the forward speed for
each frame is 9ms. For pose RNN, we sample sequences with
length of 100 from our data for training, and the speed for each
frame is 0.9ms on average. For segment CNN, we keep the size the
same as input, and the forward time is 90ms. Overall, the inference
speed is around 240ms per-image for performing joint localization
and segmentation. Both of the network is learned with ’Nadam’
optimizer [88] with a learning rate of 10−3. We sequentially train
these three models due to GPU memory limitation. Speciﬁcally,
for pose CNN and segment CNN, we stops at 150 epochs when
there is no performance gain, and for pose RNN, we stops at 200
epochs. For data augmentation, we use the imgaug2 library to add
lighting, blurring and ﬂipping variations. We keep a subset from
training images for validating the trained model from each epoch,
and choose the model performing best for evaluation.

i.e.,
For testing, since input GPS/IMU varies every time,
t = p∗ + (cid:15), we need to have a conﬁdence range of prediction
pc
for both camera pose and image segment, in order to verify the
improvement of each component we have is signiﬁcant. Speciﬁ-
cally, we report the standard variation of the results from a 10 time
simulation to obtain the conﬁdence range. Finally, we implement
all the networks by adopting the MXNet [89] platform.

Evaluation metrics. We use the median translation offset and
median relative angle [9]. For evaluating segment, we adopt the
commonly used pixel accuracy (Pix. Acc.), mean class accuracy
(mAcc.) and mean intersect-over-union (mIOU) as that from [76].

Pose Evaluation. We ﬁrst directly compare with the work of
PoseNet [9], [48], and use their published code and geometric
loss (Eq. (3)) to train a model on Zpark dataset. Due to scene
appearance similarity of the street-view, we did not obtain a
reasonable model with their methods [9], [48], i.e., results better
than the noisy GPS/IMU signal. Then, we experimented a leading
open-source monocular SLAM algorithm, i.e., ORB-SLAM [90],

2. https://github.com/aleju/imgaug

Data

k
r
a
p
Z

e
k
a
l
D

Method
Noisy pose
Pose CNN w/o semantic
Pose CNN w semantic
Noisy pose w KF
Pose RNN w/o CNN
Pose CNN w KF
Pose CNN-RNN

Pose CNN w semantic
Pose RNN w/o CNN
Pose CNN-RNN

Trans (m) ↓
3.45 ± 0.176
1.355 ± 0.052
1.331 ± 0.057
2.56 ± 0.16
1.282 ± 0.061
1.281 ± 0.06
1.005 ± 0.044

1.667 ± 0.05
1.385 ± 0.057
0.890 ± 0.037

Rot (◦)↓
7.87 ± 1.10
0.982 ± 0.023
0.727 ± 0.018
7.37 ± 1.01
1.731 ± 0.06
0.833 ± 0.03
0.719 ± 0.035

0.702 ± 0.015
1.222 ± 0.054
0.557± 0.021

Pix. Acc(%)↑
54.01 ± 1.5
70.99 ± 0.18
71.73 ± 0.18
55.1 ± 0.91
68.10 ± 0.32
72.00 ± 0.17
73.01 ± 0.16

87.83 ± 0.017
85.10 ± 0.03
88.55 ± 0.13

TABLE 4: Compare the accuracy of different settings for pose
estimation from the two datasets. Noisy pose indicates the noisy input
signal from GPS, IMU, and ’KF’ means kalman ﬁlter. The number
after ± indicates the standard deviation (S.D.) from 10 simulations.
↓ & ↑ means lower the better and higher the better respectively. We
can see the improvement is statistically signiﬁcant.

to do self-localization. However, it also provides no better result
than provided initial poses, since low-level ORB features fail to
match robustly due to many non-diffusion/reﬂective components
in Zpark, such as glass buildings and specular new roads, plus
repetitive appearance on trees. Therefore, in Tab. 4, we majorly list
the performance of estimated translation t and rotation r from our
model variations. At the 1st row, we show the median error of GPS
and IMU from our simulation. At the 2nd row, by using our pose
CNN with an additional input of projected label map, the model
can learn good relative pose between camera and GPS/IMU, which
signiﬁcantly reduces the error (60% for t, 85% for r). By adding
semantic cues, i.e., road priori and semantic weights in Eq. (3), the
pose errors are further reduced, especially for rotation (from 0.982
to 0.727 at the 3rd row). In fact, we found the most improvement
is from semantic weighting, while the road priori helps marginally.
In our future work, we would like to experiment larger noise and
more data variations, which will better validate different cues.

When having an video input, we ﬁrst evaluate a simple
baseline which reﬁnes the GPS/IMU with Kalman ﬁlter [84], i.e.,
’Noisy pose w KF’, which reasonably reduces the errors. Then,
we setup a baseline of performing RNN directly on the GPS/IMU
signal, and as shown at ’Pose RNN w/o CNN’, the estimated
t is even better than pose CNN, while r is comparably much
worse. This meets our expectation since the speed of camera is
easier to capture temporally than rotation. Another baseline we
adopt is performing to the output from Pose CNN by assuming a
constant speed which we set as the averaged speed from training
sequences. As shown at ’Pose CNN w KF’, it does improve
slightly for translation, but harms rotation, which means the ﬁlter
over smoothed the sequence. Finally when combining pose CNN
and RNN, it achieves the best pose estimation both for t and
r. We visualize some results at Fig. 13(a-c). Finally at bottom
of Tab. 4, we list corresponding results on Dlake dataset, which
draws similar conclusion with that from Zpark dataset.

Segment Evaluation. At top part of Tab. 5, we show the scene
parsing results of Zpark dataset. Firstly, we adopt one of the SOTA
parsing network on the CityScapes, i.e., ResNet38 [76], and train
it with Zpark dataset. It utilizes pre-trained parameters from the
CityScapes [3] dataset, and run with a 1.03s per-frame with our
resolution. As shown at the 1st row, it achieve reasonable accuracy
compare to our segment CNN (2nd row) when there is no pose
priori. However, our network is 10x faster. At 3rd row, we show
the results of rendered label map with the estimated pose after
pose RNN. Clearly, the results are much worse due to missing
pixels and object misalignment. At 4th row, we use the rendered
label map with ground truth pose as segment CNN guidance to

12

obtain an upper-bound for our segmentation performance. In this
case, the rendered label map aligns perfectly with the image, thus
signiﬁcantly improves the results by correct labelling most of
the static background. At 5th and 6th row, we show the results
trained with rendered label map with pose after pose CNN and
pose RNN respectively. We can see using pose CNN, the results
just improve slightly compare to the segment CNN. From our
observation, this is because the offset is still signiﬁcant for some
detailed structures, e.g., light-pole. However, when using the pose
after RNN, better alignment is achieved, and the segment accuracy
is improved signiﬁcantly especially for thin structured regions like
pole, as visualized in Fig. 13, which demonstrates the effectiveness
of our strategy.

Bottom part of Tab. 5 shows the results over the larger
Dlake dataset with more object labelling, where we see clearer
improvement, i.e., from 62.36 to 67.00, and here the rendered
label provides a background context for object segmentation,
which also improve the object parsing performance. In all classes,
we observe the performance of trafﬁc-light drops. In our opinion,
the majorly reason is trafﬁc-light only exists in intersection of
roads, which happens much fewer than objects such as light-pole,
yielding overﬁtting to the projected label maps from pose. We
may ﬁx this issue by training with even larger dataset or better
class balancing strategies, which is left to our future work.

In Fig. 13, we visualize several examples from our results
at
the view of camera. In the ﬁgure, we can see the noisy
pose (a), is progressively rectiﬁed by pose CNN (b) and pose
RNN (c) from view of camera. Additionally, at (d) and (e),
we compare the segment results without and with camera pose
respectively. As can be seen at the boxed regions, the segment
results with rendered label maps provide better accuracy in terms
of capturing region details at the boundary, discovering rare classes
and keeping correct scene layout. All of above could be important
for applications, e.g., ﬁguring out the trafﬁc signs and tele-poles
that are visually hard to detect. For additional visualization, please
check our demo videos online 34.

5.2 Benchmarks and baselines

With various tasks and large amount of labelled data we have
proposed, it would be non-practical for us to extensively explore
algorithms over all of them. Therefore, we release the data to
research community, and set up standard evaluation benchmarks.
Currently, four challenges have been set up online for evaluation
by withholding part of our labelled results as test set, which
include semantic segmentation [20], instance segmentation [21],
self-localization [23], lanemark segmentation [22].

For evaluation, in the tasks of semantic segmentation, lane-
mark segmentation, we adopt mean IoU, and in the task of
self-localization, we adopt median translation and rotation offset,
which are described in evaluation of DeLS-3D (Sec. 5.1). For
the task of instance segmentation, we use interpolated average
precision (AP) [91] under various IoU thresholds which is used
for the COCO challenge [36]. Later, we elaborate the split of each
dataset, the leading method on each benchmark currently.

Semantic segmentation. For video semantic segmentation, until
now, we haven’t receive valid results from the challenge. This
probably is due to the extremely large amount of training videos in

3. Zpark: https://www.youtube.com/watch?v=fqglYBipNfQ
4. Dlake https://www.youtube.com/watch?v=fqglYBipNfQ

mIO U

Pix. A cc

sky

car-lane

ped-lane

Data

k
r
a
p
Z

Method
64.66 95.87 93.6 98.5
ResNet38 [76]
68.35 95.61 94.2 98.6
SegCNN w/o Pose
91.7
32.61
Render PoseRNN
73.1
97.1
SegCNN w pose GT
96.1 99.4
79.37
SegCNN w Pose CNN 68.6
95.67 94.5 98.7
SegCNN w Pose RNN 69.93 95.98 94.9 98.8

-

82.9
83.8
50.4
92.5
84.3
85.3

bike-lane

87.2
89.5
62.1
93.9
89.3
90.2

t-cone

curb
61.8 46.1
69.3 47.5
16.9 6.6
81.4 68.8
69.0 46.8
71.9 45.7

t-stack

t-fence

light-pole

41.7
52.9
5.8
71.4
52.9
57.0

82.0
83.9
30.5
90.8
84.9
85.9

37.5
52.2
8.9
71.7
53.7
58.5

t-light

26.7
43.5
6.7
64.2
39.5
41.8

tele-pole

45.9
46.3
10.1
69.1
48.8
51.0

t-sign

49.5
52.9
16.3
72.2
50.4
52.2

billboard

te m p-build

building

67.3
69.2
29.4
76.2
69.9
70.9

sec.-stand

38.0
40.0
20.2
58.9
42.8
48.0

plants

89.2
88.6
73.5
91.6
88.5
89.3

85.1
87.0
70.6
91.3
87.5
88.5

Method

Data
e SegCNN w/o Pose
k
a
l
D

m I O
62.36
SegCNN w pose GT
73.10
SegCNN w pose RNN 67.00

U

P ix. A c c

96.7
97.7
97.1

s k y
95.3
96.8
95.8

c ar-la n e

96.8
97.5
97.2

p e d -la n e

12.8
41.3
30.0

t -sta c k

21.5
54.6
37.4

t -fe n c e

w all
81.9 53.0
87.5 70.5
84.2 62.6

lig ht-p ole

44.7
63.4
47.4

t -lig ht

65.8
77.6
65.5

tele-p ole

52.1
70.5
62.9

t -sig n

87.2
92.1
89.6

billb o ard

55.5
69.2
59.0

c y clist

pla nts

c ar
94.5 84.9 20.3
96.1 87.4 24.5
95.2 86.8 23.9

oto rbik e

m

28.9
43.8
34.4

tru c k
b u s
78.4 82.1
80.0 85.7
76.8 86.6

60.0
66.9
22.2
83.7
67.9
69.4

b uildin g

66.8
77.4
70.3

TABLE 5: Compare the accuracy of different segment networks setting over Zpark (top) and Dlake (bottom) dataset. t is short for ’trafﬁc’ in
the table. Here we drop the 10 times S.D. to save space because it is relatively small (≤ 0.1). Our results are especially good at parsing of
detailed structures and scene layouts, which is visualized in Fig. 13.

13

object

66.3
63.8
-
56.7
60.9
59.5

Fig. 13: Results from each intermediate stage out of the system over Zpark dataset. Label map is overlaid with the image. Improved regions
are boxed and zoomed out (best in color). More results are shown in the online videos for Zpark and Dlake.

ApolloScape, making training a model with SOTA deep learning
models such as ResNet [59] not-practical. Thus, we select a subset
from the whole data for comparison of one model performance
between ApolloScape and Cityscapes. Speciﬁcally, 5,378 training
images and 671 testing images are carefully selected from our
140K labelled semantic video frames for setting up the benchmark,
which maintains the diversity and objects appeared of the collected
scenes. The selected images will be released at our website [20].
We conducted our experiments using ResNet-38 network [76]
that trades depth for width comparing with the original ResNet
structure [59]. We ﬁne-tune their released model using our training
with initial learning rate 0.0001, standard SGD with momentum
0.9 and weight decay 0.0005, random crop with size 512 × 512,
10 times data augmentation that includes scaling and left-right
ﬂipping, and we train the network for 100 epochs. The predictions
are computed with the original image resolution without any post-
processing steps such as multi-scale ensemble etc.. Tab. 6 shows
the parsing results of classes in common for these two datasets.
Notice that using exactly same training procedure, the test IoU
with our dataset are much lower than that from the Cityscapes
mostly due to the challenges we have mentioned at Sec. 3.2,
especially for movable objects, where mIoU is 34.6% lower than
the corresponding one for the Cityscapes.

Here, we leave the training a model with the our full dataset

to the research community and our future work.

Instance segmentation. This task is an extension of semantic
object parsing by jointly considering detection and segmentation.
Speciﬁcally, we select 39212 training images and 1907 testing
images, and set up a challenge benchmark online [21] evaluating
7 objects in our dataset (Upper part of Tab. 6) to collect potential

Class

Cityscapes

ApolloScape

IoU

Group

movable
object

mIoU

surface

infrastructure

nature

car
motorcycle
bicycle
person
rider
truck
bus

road
sidewalk

fence
trafﬁc light
pole
trafﬁc sign
wall
building

vegetation

94.67
70.51
78.55
83.17
65.45
62.43
88.61

77.63

97.94
84.08

61.49
70.98
62.11
78.93
58.81
92.66

92.41

87.12
27.99
48.65
57.12
6.58
25.28
48.73

43.07

92.03
46.42

42.08
67.49
46.02
79.60
8.41
65.71

90.53

TABLE 6: Results of image parsing based on ResNet-38 network.

issues within autonomous driving scenario. During the past few
month, there are over 140 teams attended our challenge, which
reveals our community is much more interested in object level
understanding rather than scene segmentation.

The leading results from our participants are shown in Tab. 8,
where we can see in general the reported mAP of winning teams
are lower than those reported in Cityscapes benchmarks [92],
by using similar strategies [93] modiﬁed from MaskRCNN [13].
Based on the challenge reports from the winning team [94],
comparing to Cityscapes, ApolloScape contains more tiny and
occluded objects (60% object has scale less than 32 pixels), which

Method
ResNet38 [76] 40.0

mIOU s

48.6

d

w

d

y

s
53.1

d n

y

d s
57.8

g

w

b
52.1

g

y

b
22.7

s

w

s
36.4

p

w

s
18.7

z

y

w

c

59.1

w t

a
40.4

w tl

a
27.1

w tr

a
49.1

w l

r

w

a
a
57.4 20.9

w lr

a
0.01

sr

n

b

y

w

r

n p

n

n

m

o

0.9

36.1

40.5

TABLE 7: The IoU using one SOTA semantic segmentation architecture, i.e., ResNet38 [76]. Here the amount of class is less than that in
Tab. 3 due to zero accuracy over some predeﬁned labels.

14

Dataset

metric

1st

2nd

3nd

ApolloScape

Cityscapes

mAP

33.97 [94]

30.22 [95]

25.02 [96]

38.0

37.2

36.4 [93]

TABLE 8: Results of top ranked instance segmentation algorithms
in ApolloScape and Cityscapes (Numbers are obtained at the date of
submission).

Road ID

Trans (m) ↓

Rot (◦)↓

Road11
Road12
Road14
Road15
Road16
Road17

mean

0.0476
0.1115
0.0785
0.0711
0.1229
0.4934

0.1542

0.0452
0.0528
0.0825
0.1240
0.2063
0.3135

0.1374

reducing the issues from speed changing in real applications. In
the near future, hopefully, we can add more challenging scenarios
with more variations in driving speed and weathers.

In summary, from the dataset benchmarks we set up and eval-
uated algorithms, we found for low-level localization, the results
are impressively good, while for high level semantic understand-
ing, Apolloscape provides additional challenges and new issues,
yielding limited accuracy for SOTA algorithms, i.e., best mAP is
around 33% for instance segmentation, and best mIoU is around
40% for lane segmentation. Comparing to human perception,
visual based algorithms for autonomous driving deﬁnitely need
further research to handle extremely difﬁcult cases.

TABLE 9: Detailed localization accuracy of the leading results on our
benchmark from [97].

leads to signiﬁcant drop of performance when transfer models
trained on other datasets.

Lanemark segmentation. Lanemark segmentation task follows
the same metric as semantic segmentation, which contains 132189
training images and 33790 testing images. Our in-house challenge
benchmark [22] chooses to evaluate 35 most common lane mark
types on the road as listed in Tab. 3.

Until the submission of this paper, we only have one work
based on ResNet-38 network [76] evaluated, probably due to the
large amount of data (160K+ images). We show the corresponding
detailed results in Tab. 7, where we can see the mIoU of each class
are still very limited (40%) comparing to the accuracy of leading
semantic segmentation algorithms on general classes. We think
this is mostly because the high contrast, dimmed and broken lane
marks on the road such as the cases shown in Fig. 3.

Self-localization. We use the same metrics for evaluating camera
pose, i.e., median offset of translation and rotation, as described in
Sec. 5.1. This task contains driving videos in 6 sites from Beijing,
Guangzhou, Chengdu and Shanghai in China, under multiple
driving scenarios and day times. In total, we provide 153 training
videos and 71 testing videos including over 300k image frames,
and build an in-house challenge benchmark website [23] most
recently.

Currently, we also have few submissions, while the leading
one published is from one of the SOTA method for large-scale
image based localization [97]. The method is based on image
retrieval with learned deep features via various triplet losses. We
show their reported number in Tab. 9, where the localization
errors are surprisingly small, i.e.,
translation is around 15cm
and rotation error is around 0.14 degree. Originally, we believe
image appearance similarity on the street or highway can fail deep
network models. However, from the participant results, especially
designed features distinguish minor appearance changes and pro-
vide high accurate localization results. Another possibility is that
our acquisition vehicle always drives in a roughly constant speed,

6 CONCLUSION AND FUTURE WORK

In this paper, we present the ApolloScape, a large, diverse, and
multi-task dataset for autonomous driving, which includes high
density 3D point cloud map, per-pixel, per-frame semantic image
label, lane mark label, semantic instance segmentation for various
videos. Every frame of our videos is geo-tagged with high accurate
GPS/IMU device. ApolloScape is signiﬁcantly larger than existing
autonomous driving datasets, e.g., KITTI [2] and Cityscapes [3],
yielding more challenges for computer vision research ﬁeld. In
order to label such a large dataset, we developed an active 2D/3D
joint annotation pipeline, which effectively accelerate the labelling
process. Back on ApolloScape, we developed a joint localization
and segmentation algorithm with a 3D semantic map, which fuses
multi-sensors, is simple and runs efﬁciently, yielding strong results
in both tasks. We hope it may motivate researcher to develop
algorithms handling multiple tasks simultaneously by considering
their inner geometrical relationships. Finally, for each individual
task, we set up an online evaluation benchmark where different
algorithms can compete with a fair platform.

Last but not the least, ApolloScape is an evolving dataset,
not only in terms of data scale, but also in terms of various
driving conditions, tasks and acquisition devices. For example,
ﬁrstly, we plan to enlarge our dataset to contain more diversiﬁed
driving environments including snow, and foggy. Secondly, we
also released our labelled 3D cars [19], stereo images, 3D humans
and tracking [98] of objects in 3D recently. Thirdly, we plan to
mount a panoramic camera system, and Velodyne [4] in near future
to generate depth maps for objects and panoramic images.

ACKNOWLEDGMENTS

This work is supported by Baidu Inc.. We also thank the work of
Xibin Song, Binbin Cao, Jin Fang, He Jiang, Yu Zhang, Xiang
Gu, and Xiaofei Liu for their laborious efforts in organizing data,
helping writing label tools, checking labelled results and manage
the content of benchmark websites. We thank Alan L. Yuille,
Hongdong Li and Andreas Geiger for benchmark suggestions.

REFERENCES

“ApolloScape Website,” apolloscape.auto. 1

[1]
[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
The kitti dataset,” International Journal of Robotics Research (IJRR),
2013. 1, 2, 3, 4, 5, 11, 14

[3] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic
urban scene understanding,” in Proc. of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2016. 1, 3, 4, 5, 11, 12,
14

[4] Velodyne Lidar, “HDL-64E,” http://velodynelidar.com/, 2018, [Online;

accessed 01-March-2018]. 1, 4, 14

[5] A. Kar, C. H¨ane, and J. Malik, “Learning a multi-view stereo machine,”
in Advances in neural information processing systems, 2017, pp. 365–
376. 1

[6] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, “Deepmvs:
Learning multi-view stereopsis,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2018, pp. 2821–2830. 1

[7] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, “Mvsnet: Depth inference
for unstructured multi-view stereo,” in The European Conference on
Computer Vision (ECCV), September 2018. 1

[8] X. Cheng, P. Wang, and R. Yang, “Depth estimation via afﬁnity learned
with convolutional spatial propagation network,” European Conference
on Computer Vision, 2018. 1

[9] A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional
network for real-time 6-dof camera relocalization,” in Proceedings of the
IEEE international conference on computer vision, 2015, pp. 2938–2946.
1, 2, 3, 11

[10] J. L. Sch¨onberger, M. Pollefeys, A. Geiger, and T. Sattler, “Semantic
visual localization,” ISPRS Journal of Photogrammetry and Remote
Sensing (JPRS), 2018. 1, 3, 6

[11] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2015, pp. 3431–3440. 1, 4
[12] L. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking
atrous convolution for semantic image segmentation,” CoRR, vol.
abs/1706.05587, 2017. 1, 2, 4

[13] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” IEEE
transactions on pattern analysis and machine intelligence, 2018. 1, 7, 13
[14] L.-C. Chen, A. Hermans, G. Papandreou, F. Schroff, P. Wang, and
H. Adam, “Masklab: Instance segmentation by reﬁning object detection
with semantic and direction features,” arXiv preprint arXiv:1712.04837,
2017. 1

[15] Y. Xiang, R. Mottaghi, and S. Savarese, “Beyond pascal: A benchmark
for 3d object detection in the wild,” in Applications of Computer Vision
(WACV), 2014 IEEE Winter Conference on.
IEEE, 2014, pp. 75–82. 1
[16] A. Kar, S. Tulsiani, J. Carreira, and J. Malik, “Category-speciﬁc object
reconstruction from a single image,” in Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2015, pp. 1966–1974.
1

[17] F. Guney and A. Geiger, “Displets: Resolving stereo ambiguities using
object knowledge,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2015, pp. 4165–4175. 1

[18] A. Kundu, Y. Li, and J. M. Rehg, “3d-rcnn: Instance-level 3d object
reconstruction via render-and-compare,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
3559–3568. 1

[19] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su, H. Li, and
R. Yang, “Apollocar3d: A large 3d car instance understanding benchmark
for autonomous driving,” CVPR, 2019. 1, 14

[20] ApolloScape., “Semantic segmentation,” http://apolloscape.auto/scene.

html. 1, 4, 8, 12, 13
“Instance

[21] ——,

cvpr-2018-autonomous-driving. 1, 4, 12, 13

[22] ——,

“Lanemark
segmentation.html. 1, 5, 12, 14

segmentation,”

segmentation,”

https://www.kaggle.com/c/

http://apolloscape.auto/lane

[23] ——, “Localization,” http://apolloscape.auto/self localization.html. 1, 5,

[24] W. Peng et al., “ApolloScape API,” https://github.com/ApolloScapeAuto/

12, 14

dataset-api. 1

[25] A. Kundu, Y. Li, F. Dellaert, F. Li, and J. M. Rehg, “Joint semantic
segmentation and 3d reconstruction from monocular video,” in European
Conference on Computer Vision. Springer, 2014, pp. 703–718. 2, 4
[26] G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic object classes
in video: A high-deﬁnition ground truth database,” Pattern Recognition
Letters, vol. 30, no. 2, pp. 88–97, 2009. 2, 3

15

[27] S. Wang, M. Bai, G. Mattyus, H. Chu, W. Luo, B. Yang, J. Liang,
J. Cheverie, S. Fidler, and R. Urtasun, “Torontocity: Seeing the world
with a million eyes,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2017, pp. 3009–3017. 3

[28] G. Neuhold, T. Ollmann, S. R. Bulo, and P. Kontschieder, “The mapillary
vistas dataset for semantic understanding of street scenes,” in Proceed-
ings of the International Conference on Computer Vision (ICCV), Venice,
Italy, 2017, pp. 22–29. 3

[29] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,
“Bdd100k: A diverse driving video database with scalable annotation
tooling,” arXiv preprint arXiv:1805.04687, 2018. 3, 4

[30] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
synthia dataset: A large collection of synthetic images for semantic
segmentation of urban scenes,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 3234–3243. 3
[31] S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,” in

International Conference on Computer Vision (ICCV), 2017. 3

[32] D. Scharstein, H. Hirschm¨uller, Y. Kitajima, G. Krathwohl, N. Neˇsi´c,
X. Wang, and P. Westling, “High-resolution stereo datasets with subpixel-
accurate ground truth,” in German Conference on Pattern Recognition.
Springer, 2014, pp. 31–42. 2

[33] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
and support inference from rgbd images,” in European Conference on
Computer Vision. Springer, 2012, pp. 746–760. 2

[34] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
D. Safari, M. Okutomi, M. Pollefeys, J. Sivic et al., “Benchmarking 6dof
outdoor visual localization in changing conditions,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, vol. 1,
2018. 2

[35] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
“The pascal visual object classes (voc) challenge,” International journal
of computer vision, vol. 88, no. 2, pp. 303–338, 2010. 2

[36] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision. Springer, 2014,
pp. 740–755. 2, 12

[37] “Unity Development Platform,” https://unity3d.com/. 3
[38] J. Hoffman, D. Wang, F. Yu, and T. Darrell, “Fcns in the wild:
Pixel-level adversarial and constraint-based adaptation,” arXiv preprint
arXiv:1612.02649, 2016. 3

[39] Y. Zhang, P. David, and B. Gong, “Curriculum domain adaptation for
semantic segmentation of urban scenes,” in The IEEE International
Conference on Computer Vision (ICCV), vol. 2, no. 5, 2017, p. 6. 3
[40] Y. Chen, W. Li, and L. Van Gool, “Road: Reality oriented adaptation for
semantic segmentation of urban scenes,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
7892–7901. 3

[41] B. M. Haralick, C.-N. Lee, K. Ottenberg, and M. N¨olle, “Review and
analysis of solutions of the three point perspective pose estimation
problem,” IJCV, vol. 13, no. 3, pp. 331–356, 1994. 3

[42] L. Kneip, H. Li, and Y. Seo, “Upnp: An optimal o (n) solution to
the absolute pose problem with universal applicability,” in European
Conference on Computer Vision. Springer, 2014, pp. 127–142. 3
[43] P. David, D. Dementhon, R. Duraiswami, and H. Samet, “Softposit:
Simultaneous pose and correspondence determination,” IJCV, vol. 59,
no. 3, pp. 259–284, 2004. 3

[44] F. Moreno-Noguer, V. Lepetit, and P. Fua, “Pose priors for simultane-
ously solving alignment and correspondence,” European Conference on
Computer Vision, pp. 405–418, 2008. 3

[45] D. Campbell, L. Petersson, L. Kneip, and H. Li, “Globally-optimal
inlier set maximisation for simultaneous camera pose and feature corre-
spondence,” in The IEEE International Conference on Computer Vision
(ICCV), vol. 1, no. 3, 2017. 3

[46] T. Sattler, A. Torii, J. Sivic, M. Pollefeys, H. Taira, M. Okutomi, and
T. Pajdla, “Are large-scale 3d models really necessary for accurate visual
localization?” in 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

IEEE, 2017, pp. 6175–6184. 3

[47] J. Engel, T. Sch¨ops, and D. Cremers, “Lsd-slam: Large-scale di-
rect monocular slam,” in European Conference on Computer Vision.
Springer, 2014, pp. 834–849. 3

[48] A. Kendall, R. Cipolla et al., “Geometric loss functions for camera pose
regression with deep learning,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol. 3, 2017, p. 8.
3, 9, 10, 11

[49] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, and
D. Cremers, “Image-based localization using lstms for structured feature
correlation,” in Int. Conf. Comput. Vis.(ICCV), 2017, pp. 627–637. 3

[50] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen, “Vidloc:
A deep spatio-temporal model for 6-dof video-clip relocalization,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), vol. 3, 2017. 3

[51] H. Coskun, F. Achilles, R. DiPietro, N. Navab, and F. Tombari, “Long
short-term memory kalman ﬁlters: Recurrent neural estimators for pose
regularization,” in Proceedings of the International Conference on Com-
puter Vision (ICCV), 2017. 3

[52] K.-N. Lianos, J. L. Sch¨onberger, M. Pollefeys, and T. Sattler, “Vso:
Visual semantic odometry,” in Proceedings of the European Conference
on Computer Vision (ECCV), 2018, pp. 234–250. 3

[53] K. Vishal, C. Jawahar, and V. Chari, “Accurate localization by fusing
images and gps signals,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops, 2015, pp. 17–24.
4, 11

[54] Z. Laskar, I. Melekhov, S. Kalia, and J. Kannala, “Camera relocaliza-
tion by computing pairwise relative poses using convolutional neural
network,” Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2017. 4

[55] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy,
and T. Brox, “Demon: Depth and motion network for learning monocular
stereo,” in IEEE Conference on computer vision and pattern recognition
(CVPR), vol. 5, 2017, p. 6. 4, 9, 10

[56] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing

network,” CVPR, 2017. 4

[57] A. Arnab, S. Jayasumana, S. Zheng, and P. H. Torr, “Higher order condi-
tional random ﬁelds in deep neural networks,” in European Conference
on Computer Vision. Springer, 2016, pp. 524–540. 4

[58] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling with
lstm recurrent neural networks,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2015, pp. 3547–3555. 4

[59] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778. 4, 13

[60] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep
neural network architecture for real-time semantic segmentation,” CoRR,
vol. abs/1606.02147, 2016. 4

[61] H. Zhao, X. Qi, X. Shen, J. Shi, and J. Jia, “Icnet for real-time semantic
segmentation on high-resolution images,” CoRR, vol. abs/1704.08545,
2017. 4

[62] A. Kundu, V. Vineet, and V. Koltun, “Feature space optimization for
semantic video segmentation,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2016, pp. 3168–3175. 4

[63] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
P. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning
optical ﬂow with convolutional networks,” in Proceedings of the IEEE
International Conference on Computer Vision, 2015, pp. 2758–2766. 4

[64] R. Gadde, V. Jampani, and P. V. Gehler, “Semantic video cnns through
representation warping,” Proceedings of the International Conference on
Computer Vision (ICCV), 2017. 4

[65] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, “Deep feature ﬂow for
video recognition,” Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2017. 4

[66] C. Hane, C. Zach, A. Cohen, R. Angst, and M. Pollefeys, “Joint 3d
scene reconstruction and class segmentation,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2013, pp. 97–
104. 4

[67] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time
dense monocular slam with learned depth prediction,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), vol. 2, 2017. 4

[68] RIEGL, “VMX-1HA,” http://www.riegl.com/. 4
[69] TuSimple., “Lanemark segmentation,” http://benchmark.tusimple.ai/#/. 5
[70] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “Deepmatching:
Hierarchical deformable dense matching,” IJCV, vol. 120, no. 3, pp. 300–
323, 2016. 5

[71] C. Luo, Z. Yang, P. Wang, Y. Wang, W. Xu, R. Nevatia, and A. Yuille,
“Every pixel counts++: Joint learning of geometry and motion with 3d
holistic understanding,” arXiv preprint arXiv:1810.06125, 2018. 5
[72] J. Xie, M. Kiefel, M.-T. Sun, and A. Geiger, “Semantic instance anno-
tation of street scenes by 3d to 2d label transfer,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2016,
pp. 3688–3697. 6

[73] S. Christoph Stein, M. Schoeler, J. Papon, and F. Worgotter, “Object par-
titioning using local convexity,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2014, pp. 304–311. 6

[74] “Point Cloud Library,” pointclouds.org. 6

16

[75] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
feature learning on point sets in a metric space,” in Advances in Neural
Information Processing Systems, 2017, pp. 5105–5114. 6

[76] Z. Wu, C. Shen, and A. v. d. Hengel, “Wider or deeper: Revisiting the
resnet model for visual recognition,” arXiv preprint arXiv:1611.10080,
2016. 7, 11, 12, 13, 14

[77] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” in Advances in neural
information processing systems, 2015, pp. 91–99. 7

[78] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L. Yuille,
“Joint object and part segmentation using deep learned potentials,” in
Proceedings of the IEEE International Conference on Computer Vision,
2015, pp. 1573–1581. 7

[79] H. Su, J. Deng, and L. Fei-Fei, “Crowdsourcing annotations for visual
object detection,” in Workshops at the Twenty-Sixth AAAI Conference on
Artiﬁcial Intelligence, vol. 1, no. 2, 2012. 8

[80] A. Kovashka, O. Russakovsky, L. Fei-Fei, K. Grauman et al., “Crowd-
sourcing in computer vision,” Foundations and Trends R(cid:13) in Computer
Graphics and Vision, vol. 10, no. 3, pp. 177–243, 2016. 8

[81] G. Li, J. Wang, Y. Zheng, and M. J. Franklin, “Crowdsourced data
management: A survey,” IEEE Transactions on Knowledge and Data
Engineering, vol. 28, no. 9, pp. 2296–2319, 2016. 8

[82] P. Wang, R. Yang, B. Cao, W. Xu, and Y. Lin, “Dels-3d: Deep localization
and segmentation with a 3d semantic map,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
5860–5869. 9

[83] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,
M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., “Google’s neural ma-
chine translation system: Bridging the gap between human and machine
translation,” arXiv preprint arXiv:1609.08144, 2016. 9

[84] R. E. Kalman et al., “A new approach to linear ﬁltering and prediction
problems,” Journal of basic Engineering, vol. 82, no. 1, pp. 35–45, 1960.
9, 12

[85] T. Pohlen, A. Hermans, M. Mathias, and B. Leibe, “Full-resolution resid-
ual networks for semantic segmentation in street scenes,” Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
10

[86] D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic
labels with a common multi-scale convolutional architecture,” in Pro-
ceedings of the IEEE International Conference on Computer Vision,
2015, pp. 2650–2658. 10

[87] B.-H. Lee, J.-H. Song, J.-H. Im, S.-H. Im, M.-B. Heo, and G.-I. Jee,
“Gps/dr error estimation for autonomous vehicle localization,” Sensors,
vol. 15, no. 8, pp. 20 779–20 798, 2015. 11

[88] T. Dozat, “Incorporating nesterov momentum into adam,” 2016. 11
[89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,
C. Zhang, and Z. Zhang, “Mxnet: A ﬂexible and efﬁcient machine
learning library for heterogeneous distributed systems,” CoRR, vol.
abs/1512.01274, 2015. 11

[90] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a versatile
and accurate monocular slam system,” IEEE transactions on robotics,
vol. 31, no. 5, pp. 1147–1163, 2015. 11

[91] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik, “Simultaneous
detection and segmentation,” in European Conference on Computer
Vision. Springer, 2014, pp. 297–312. 12

[92] “Cityscapes

instance

https://www.
cityscapes-dataset.com/benchmarks/#instance-level-scene-labeling-task.
13

segmentation

benchmark,”

[93] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network
for instance segmentation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018, pp. 8759–8768. 13, 14
[94] Z. Yueqing, L. Zeming, and G. Yu, “Find tiny instance segmentation,”

http://www.skicyyu.org/WAD/wad ﬁnal.pdf, 2018. 13, 14

[95] Smart Vision SG,

place,”

2nd
Kaggle-CVPR-2018-WAD-Video-Segmentation-Challenge-Solution,
2018. 14

“Wad

segmentation
https://github.com/Computational-Camera/

instance

[96] SZU N606, “Wad instance segmentation 3rd place,” https://github.com/
2018.

wwoody827/cvpr-2018-autonomous-driving-autopilot-solution,
14

[97] L. Liu, H. Li, and Y. Dai, “Deep stochastic attraction and repulsion em-
bedding for image based localization,” arXiv preprint arXiv:1808.08779,
2018. 14

[98] Y. Ma, X. Zhu, S. Zhang, R. Yang, W. Wang, and D. Manocha,
“Trafﬁcpredict: Trajectory prediction for heterogeneous trafﬁc-agents,”
AAAI, 2019. 14

17

Ruigang Yang is the chief scientist for 3D vision
at Baidu. He is also a full professor at the Uni-
versity of Kentucky (on leave). His research in-
terests include 3D computer vision and 3D com-
puter graphics, in particular 3D modeling and 3D
data analysis. He has published over 100 papers
with an H-index of 48. He is an Associate Editor
for IEEE T-PAMI. He has been a program co-
chair for 3DIMPVT (now 3DV) 2011 and WACV
2014, and he has been area chairs for both ICCV
and CVPR multiple times.

Xinyu Huang is currently a senior research sci-
entist at Baidu Research and an associate pro-
fessor at the North Carolina Central University.
He obtained his Ph.D. degree from the University
of Kentucky and B.S. degree from Huazhong
University of Science and Technology. His re-
search areas include computer vision,
image
processing, pattern recognition, and machine
learning.

Peng Wang is a senior research scientist in
Baidu USA LLC. He obtained his Ph.D. degree
in University of California, Los Angeles, advised
by Prof. Alan Yuille. Before that, he received his
B.S. and M.S. from Peking University, China. His
research interest is image parsing and 3D under-
standing, and vision based autonomous driving
system. He has around 30 published papers in
ECCV/CVPR/ICCV/NIPS.

Xinjing Cheng is a research scientist with
Robotics and Autonomous Driving Lab, Baidu
Research, Baidu Inc., Beijing, China. Before
that, he was a research assistant with the In-
telligent Bionic Center, Shenzhen Institutes of
Advanced Technology (SIAT), Chinese Academy
of Sciences(CAS), Shenzhen, China. His current
research interests include computer vision, deep
learning, robotics and autonomous driving.

Dingfu Zhou a senior researcher at Robotics
and Autonomous Driving Laboratory (RAL) of
Baidu. Before joining in Baidu, he worked as
a PostDoc Researcher in the Research School
of Engineering at the Australian National Uni-
versity, Canberra, Australia. He obtained his
Ph.D degree in System and Control from Sor-
bonne Universit ´es, Universit ´e de Technologie de
Compi `egne, Compi `egne, France, in 2014. He
received the B.E. degree and M.E degree both
in signal and information processing from North-
western Polytechnical University, Xian, China. His research interests
include Simultaneous Localization and Mapping, Structure from Motion,
Classiﬁcation and their application in Autonomous Driving.

Qichuan Geng is a Ph.D. candidate, at State
Key Lab of Virtual Reality Technology and Sys-
tems, Beihang University, Beijing, China. He re-
ceived his B.S. degree from Beihang Univer-
sity in 2012. His main research interests in-
clude computer vision, semantic segmentation
and scene geometry recovery.

9
1
0
2
 
l
u
J
 
4
 
 
]

V
C
.
s
c
[
 
 
4
v
4
8
1
6
0
.
3
0
8
1
:
v
i
X
r
a

1

The ApolloScape Open Dataset for Autonomous
Driving and its Application

Xinyu Huang*, Peng Wang*, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, Ruigang Yang

Abstract—Autonomous driving has attracted tremendous attention especially in the past few years. The key techniques for a
self-driving car include solving tasks like 3D map construction, self-localization, parsing the driving road and understanding objects,
which enable vehicles to reason and act. However, large scale data set for training and system evaluation is still a bottleneck for
developing robust perception models. In this paper, we present the ApolloScape dataset [1] and its applications for autonomous
driving. Compared with existing public datasets from real scenes, e.g., KITTI [2] or Cityscapes [3], ApolloScape contains much large
and richer labelling including holistic semantic dense point cloud for each site, stereo, per-pixel semantic labelling, lanemark labelling,
instance segmentation, 3D car instance, high accurate location for every frame in various driving videos from multiple sites, cities and
daytimes. For each task, it contains at lease 15x larger amount of images than SOTA datasets. To label such a complete dataset, we
develop various tools and algorithms speciﬁed for each task to accelerate the labelling process, such as joint 3D-2D segment labeling,
active labelling in videos etc. Depend on ApolloScape, we are able to develop algorithms jointly consider the learning and inference of
multiple tasks. In this paper, we provide a sensor fusion scheme integrating camera videos, consumer-grade motion sensors
(GPS/IMU), and a 3D semantic map in order to achieve robust self-localization and semantic segmentation for autonomous driving. We
show that practically, sensor fusion and joint learning of multiple tasks are beneﬁcial to achieve a more robust and accurate system. We
expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor
fusion and multi-task learning in the ﬁeld of computer vision.

Index Terms—Autonomous Driving, Large-scale Datasets, Scene/Lane Parsing, Self Localization, 3D Understanding.

(cid:70)

of,

1 INTRODUCTION

A Successful self-driving vehicle that is widely applied must

include three essential components. Firstly, understanding
the environment, where commonly a 3D semantic HD map
at the back-end precisely recorded the environment. Secondly,
understanding self-location, where an on-the-ﬂy self-localization
system puts the vehicles accurately inside the 3D world, so that it
can plot a path to every target location. Thirdly, understanding
semantics in the view, where a 3D perceptual system detects
other moving objects, guidance signs and obstacles on the road,
in order to avoid collisions and perform correct actions. The
prevailing approaches for solving those tasks from self-driving
companies are mostly dependent on LIDAR [4], whereas vision-
based approaches, which have potentially very low-cost, are still
very challenging and under research. It requires solving tasks
such as learning to do visual 3D scene reconstruction [5], [6],
[7], [8], self-localization [9], [10], semantic parsing [11], [12],
semantic instance understanding [13], [14], object 3D instance
understanding [15], [16], [17], [18], [19] online in a self-driving
video etc. However, the SOTA datasets for supporting these tasks
either have limited amount, e.g., KITTI [2] only has 200 training
images for semantic understanding, or limited variation of tasks,
e.g., Cityscapes [3] only has discrete semantic labelled frames
without tasks like localization or 3D reconstruction. Therefore, in
order to have a holistic training and evaluation of a vision-based
self-driving system, in this paper, we build the Apolloscape [1]
for autonomous driving, which is a growing and uniﬁed dataset
extending previous ones both on the data scale, label density and
variation of tasks.

• X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng, and R. Yang are with

Baidu Research.

* Equal contribution

Speciﬁcally, in current stage, ApolloScape contains properties

1) dense semantics 3D point cloud for the environment (20+

2) stereo driving videos (100+ hours)
3) high accurate 6DoF camera pose. (translation ≤ 50mm,

4) videos at same site under different day times, (morning, noon,

5) dense per-pixel per-frame semantic labelling (35 classes,

6) per-pixel lanemark labelling (35 classes, 160K+ images)
7) semantic 2D instances segmentation (8 classes, 90K+ im-

driving site)

rotation ≤ 0.015◦)

144K+ images)

night)

ages)

8) 2D car keypoints and 3D car instance labelling (70K cars)

With these information, we have released several standard
benchmarks for scene parsing [20], instance segmentation [21],
lanemark parsing [22], self-localization [23] by withholding part
of the data as test set, and our toolkit for visualization and
evaluation has also published [24]. Here, for 3D car instance, we
list the car number we already labelled, and since it is still under
development, we will elaborate it in our future work. Fig. 1 shows
a glance of ApolloScape, which illustrates various information
from the dataset that is necessary for autonomous driving. Our
dataset is still growing and evolving, and will shortly contains new
tasks such as 3D car instance shape and pose, 3D car tracking etc.,
which are important for scene understanding with ﬁner granularity.
In addition, thanks to our efﬁcient labelling pipeline, we are able
to scale the dataset to multiple cities and sites, and we have already
contained 10 cities in China under various driving conditions.

Based on ApolloScape, we are able to develop algorithms

2

Fig. 1: A glance of ApolloScape with various properties. The images are cropped for better visualization.

for jointly considering 3D and 2D simultaneously with multi-
ple tasks like segmentation, reconstruction, self-localization etc.
These tasks are traditionally handled individually [9], [12], or
jointly handled ofﬂine with semantic SLAM [25] which could
be time consuming. However, from a more practical standpoint,
self-driving car needs to handle localization and parsing the
environment on-the-ﬂy efﬁciently. Therefore, in this paper, we
propose a deep learning based online algorithm jointly solving
localization and semantic scene parsing when a 3D semantic map
is available. In our system, we assume to have (a) GPS/IMU signal
to provide a coarse camera pose estimation; (b) a semantic 3D map
for the static environment. The GPS/IMU signals serve as a crucial
prior for our pose estimation system. The semantic 3D map, which
can synthesize a semantic view for a given camera pose, not only
provides strong guidance for scene parsing, but also helps maintain
temporal consistency.

With our framework, the camera poses and scene semantics
are mutually beneﬁcial. The camera poses help establish the
correspondences between the 3D semantic map and 2D semantic
label map. Conversely, scene semantics could help reﬁne camera
poses. Our uniﬁed framework yields better results, in terms of both
accuracy and speed, for both tasks than doing them individually. In
our experiments, using a single Titan Z GPU, the networks in our
system estimates the pose in 10ms with accuracy under 1 degree,
and segments the image 512×608 within 90ms with pixel accuracy
around 96% without model compression, which demonstrates its
efﬁciency and effectiveness.

In summary, the contributions of this work are in three folds,
1) We propose a large and rich dataset, named as ApolloScape,
which includes various tasks, e.g., 3D reconstruction, self-
localization, semantic parsing, instance segmentation etc.,
supporting the training and evaluation of vision-based au-
tonomous driving algorithms and systems.

2) For developing the dataset, we design an efﬁcient and scal-
able 2D/3D joint-labelling pipeline, where various tools are
developed for 2D segmentation, 3D instance understanding
etc. For example, compared with fully manual labelling,
our 3D/2D labelling pipeline saves 70% labeling time for
semantic segmentation.

3) Based on ApolloScape, we developed a deep learning based
joint self-localization and segmentation algorithm, which is

relying on a semantic 3D map. The system fuses sensors from
camera and customer-grad GPS/IMU, which runs efﬁciently
and improves the robustness and accuracy for camera local-
ization and scene parsing.
The structure of this paper is organized as follows. We provide
related work in Sec. 2, and elaborate the collection and labelling of
ApolloScape in Sec. 3. In Sec. 4, we explain the developed efﬁcient
joint segmentation and localization algorithm. Finally, we present
the evaluation results of our algorithms,
the benchmarks for
multiple tasks and corresponding baseline algorithms performed
on these tasks in Sec. 5.

2 RELATED WORKS
Autonomous driving datasets and related algorithms has been an
active research area for years. Here we summarize the related
works in aspects of datasets and most relevant algorithms without
enumerating them all due to space limitation.

2.1 Datasets for autonomous driving.

Most recently, various datasets targeting at solving each individual
visual task for robot navigation have been released such as 3D
geometry estimation [32], [33], localization [9], [34], instance
detection and segmentation [35], [36]. However, focusing on
autonomous driving, a set of comprehensive visual
tasks are
preferred to be collected consistently within a uniﬁed dataset
from driving videos, so that one may explore the mutual beneﬁts
between different problems.

In past years, lots of datasets have been collected in various
cities, aiming to increase variability and complexity of urban
street views for self-driving applications. The Cambridge-driving
Labeled Video database (CamVid) [26] is the ﬁrst dataset with se-
mantic annotated videos. The size of the dataset is small, contain-
ing 701 manually annotated images with 32 semantic classes. The
KITTI vision benchmark suite [2] is later collected and contains
multiple computer vision tasks such as stereo, optical ﬂow, 2D/3D
object detection and tracking. For semantics, it mainly focuses on
detection, where 7,481 training and 7,518 test images are anno-
tated by 2D and 3D bounding boxes, and each image contains up
to 15 cars and 30 pedestrians. Nevertheless, for segmentation, very
few images contain pixel-level annotations, yielding a relatively

3

Dataset

Real

Location Accuracy

Diversity

CamVid [26]

Kitti [2]

Cityscapes [3]

Toronto [27]

Mapillary [28]

BDD100K [29]

SYNTHIA [30]

P.F.B. [31]

ApolloScape

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

-

-

(cid:88)

cm

-

-

cm

meter

meter

-

-

cm

day time

day time

day time
50 cities

Toronto

various weather
day & night
6 continents

various weather
day
4 regions in US

various weather

various weather

80k 3D box

3D

no

no

no

no

box

box

focus on buildings and roads
exact numbers are not available1

pixel: 25k

2D / 2 classes

Annotation

2D

pixel: 701

box: 15k
pixel: 400

pixel: 25k

box: 100k
pixel: 10k

pixel:213k

pixel:250k

pixel: 140k

Video
(cid:88)

Lane

2D / 2 classes

-

-

-

-

(cid:88)

(cid:88)

(cid:88)

no

no

no

no

2D / 2 classes

3D / 2D Video
27 classes

various weather
day time
4 regions in China

3D semantic point
70K 3D ﬁtted cars

1 database is not open to public yet.

TABLE 1: Comparison between our dataset and the other street-view self-driving datasets published. “pixel” represents 2D pixel-level
annotations. “point” represents 3D point-level annotations. “box” represents bounding box-level annotations. “Video” indicates whether 2D
video sequences are annotated. “3D ﬁtted cars” gives the number of car instance we already ﬁtted in the images with a 3D mesh model, which
we will introduce in our future works.

weak benchmark for semantic segmentation. Most recently, the
Cityscapes dataset [3] is specially collected for 2D segmentation
which contains 30 semantic classes. In detail, 5,000 images have
detailed annotations, and 20,000 images have coarse annotations.
Although video frames are available, only one image out of each
video is manually labelled. Thus, tasks such as video segmentation
can not be performed. Similarly, the Mapillary Vistas dataset [28]
provides a larger set of images with ﬁne annotations, which
has 25,000 images with 66 object categories. The TorontoCity
benchmark [27] collects LIDAR data and images including stereo
and panoramas from both drones and moving vehicles. Although
the dataset scale is large, which covers the Toronto area. as
mentioned by authors, it is not possible to manually do per-pixel
labelling of each frame. Therefore, only two semantic classes,
i.e., building footprints and roads, are provided for benchmarks of
segmentation. BDD100K database [29] contains 100K raw video
sequences representing more than 1000 hours of driving hours
with more than 100 million images. Similarly with the Cityscapes,
one image is selected from each video clip for annotation. 100K
images are annotated in bounding box level and 10K images are
annotated in pixel level.

Real data collection is laborious, to avoid the difﬁculties in
real scene collection, several synthetic datasets are also proposes.
SYNTHIA [30] builds a virtual city with Unity development
platform [37], and Play for benchmark [31] extracts ground truth
with GTA game engine. Though large amount of data and ground
truth can be generated, there is still a domain gap [38] between
appearance of synthesized images and the real ones. In general,
models learned in real scenario still generalize better in real
applications such as object detection and segmentation [39], [40].
In Tab. 1, we compare the properties our dataset and other
SOTA datasets for autonomous driving, and show that Apol-
loScape is unique in terms of data scale, granularity of labelling,
task variations within real environments. Later in Sec. 3, we will
present more details about the dataset.

2.2 Self-localization and semantic scene parsing.

As discussed in Sec. 1, we also try to tackle real-time self-
localization and semantic scene parsing back on ApolloScape
given a video or a single image. These two problems have long
been center focus for computer vision. Here we summarize the
related works on outdoor cases with street-view images as input.

Visual self-localization. Traditionally, localizing an image given
a set of 3D points is formulated as a Perspective-n-Point (PnP)
problem [41], [42] by matching feature points in 2D and fea-
tures in 3D through cardinality maximization. Usually in a large
environment, a pose prior is required in order to obtain good
estimation [43], [44]. Campbell et al. [45] propose a global-
optimal solver which leverage the prior. In the case that geo-
tagged images are available, Sattler et al. [46] propose to use
image-retrieval to avoid matching large-scale point cloud. When
given a video, temporal information could be further modeled with
methods like SLAM [47] etc, which increases the localization
accuracy and speed.

Although these methods are effective in cases with distin-
guished feature points, they are still not practical for city-scale
environment with billions of points, and they may also fail in
areas with low texture, repeated structures, and occlusions. Thus,
recently, deep learned features with hierarchical representations
are proposed for localization. PoseNet [9], [48] takes a low-
resolution image as input, which can estimate pose in 10ms w.r.t.
a feature rich environment composed of distinguished landmarks.
LSTM-PoseNet [49] further captures a global spatial context
after CNN features. Given an video, later works incorporate Bi-
Directional LSTM [50] or Kalman ﬁlter LSTM [51] to obtain
better results with temporal information. Most recently, many
works [10], [52] also consider adding semantic cues as more
robust representation for localization. However, in street-view
in most cases,
scenario, considering a road with trees aside,
the visual
no signiﬁcant

landmark appears, which could fail

models. Thus, signals from GPS/IMU are a must-have for robust
localization in these cases [53], whereas the problem switched
to estimating the relative pose between the camera view from a
noisy pose and the real pose. For ﬁnding relative camera pose of
two views, recently, researchers [54], [55] propose to stack the
two images as a network input. In our case, we concatenate the
real image with an online rendered label map from the noisy pose,
which provides superior results in our experiments. Street scene

parsing. For parsing a single image of street views (e.g., these
from CityScapes [3]), most state-of-the-arts (SOTA) algorithms
are designed based on a FCN [11] and a multi-scale context
module with dilated convolution [12], pooling [56], CRF [57], or
spatial RNN [58]. However, they are dependent on a ResNet [59]
with hundreds of layers, which is too computationally expen-
sive for applications that require real-time performance. Some
researchers apply small models [60] or model compression [61]
for acceleration, with the cost of reduced accuracy. When the input
is a video, spatial-temporal informations are jointly considered,
Kundu et al. [62] use 3D dense CRF to get temporally consistent
results. Recently, optical ﬂow [63] between consecutive frames
is computed to transfer label or features [64], [65] from the
previous frame to current one. In our case, we connect consecutive
video frames through 3D information and camera poses, which
is a more compact representation for static background. In our
case, we propose the projection from 3D maps as an additional
input, which alleviates the difﬁculty of scene parsing solely from
image cues. Additionally, we adopt a light weighted network from
DeMoN [55] for inference efﬁciency.

Joint 2D-3D for video parsing. Our work is also related to joint
reconstruction, pose estimation and parsing [25], [66] through
embedding 2D-3D consistency. Traditionally, reliant on structure-
from-motion (SFM) [66] from feature or photometric matching,
those methods ﬁrst reconstruct a 3D map, and then perform
semantic parsing over 2D and 3D jointly, yielding geometrically
consistent segmentation between multiple frames. Most recently,
CNN-SLAM [67] replaces traditional 3D reconstruction module
with a single image depth network, and adopts a segment network
for image parsing. However, all these approaches are processed
off-line and only for static background, which do not satisfy our
online setting. Moreover, the quality of a reconstructed 3D model
is not comparable with the one collected with a 3D scanner.

3 BUILD ApolloScape
In this section, we introduce our acquisition system, speciﬁcations
about the collected data and efﬁcient labelling process for building
ApolloScape.

3.1 Acquisition system

In Fig. 2, we visualize our collection system. To collect static 3D
environment, we adopt Riegl VMX-1HA [68] as our acquisition
system that consists of two VUX-1HA laser scanners (360◦ FOV,
range from 1.2m up to 420m with target reﬂectivity larger than
80%), one VMX-CS6 camera system (two front cameras are
used with resolution 3384 × 2710), and a measuring head with
IMU/GNSS (position accuracy 20 ∼ 50mm, roll & pitch accuracy
0.005◦, and heading accuracy 0.015◦). The laser scanners utilizes
two laser beams to scan its surroundings vertically that are similar
to the push-broom cameras. Comparing with common-used Velo-
dyne HDL-64E [4], the scanners are able to acquire higher density

4

Fig. 2: Acquisition system consists of two laser scanners, up to six
video cameras, and a combined IMU/GNSS system.

Count

Kitti
(box)

Cityscapes
(pixel)

BDD100K
(box)

ApolloScape
(pixel)

total (×104)

average per image

person
vehicle

person
vehicle

0.6
3.0

0.8
4.1

2.4
4.1

7.0
11.8

12.9
110.2

1.3
11.0

54.3
198.9

m

6.2
24.0

e

1.1
12.7

h

16.9
38.1

TABLE 2: Total and average number of instances in KITTI [2],
Cityscapes [3], BDD100K [29], and ApolloScape (instance-level).
“pixel” represents 2D pixel-level annotations. “box” represents bound-
ing box-level annotations. The letters, e, m, and h, indicate easy,
moderate, and hard subsets in ApolloScape respectively.

of point clouds and obtain higher measuring accuracy / precision
(5mm / 3mm). The whole system has been internally calibrated
and synchronized, and is mounted on the top of a mid-size SUV.
Additionally, the system contains two high frontal camera
capturing with a resolution of 3384 × 2710, and is well cali-
brated with the LIDAR device. Finally, to obtain high accurate
GPS/IMU information, a temporary GPS basement is set up near
the collection site to make sure the localization of the camera is
sufﬁciently accurate for us to match the 2D image and 3D point
cloud. Commonly, our vehicle drives at the speed of 30km per
hour and the cameras are triggered once every meter, i.e., 30fps.

3.2 Speciﬁcations

Here, based on the acquisition system, we ﬁrst present the spec-
iﬁcations of Apolloscape w.r.t. different tasks, e.g., predeﬁned
semantic classes, lanemark classes and instance etc., to allow
better overview of the dataset. In Sec. 3.3, we will introduce our
active labelling pipeline which allows us to efﬁciently produce the
ground truth of multiple tasks simultaneously.

Semantic scene parsing. In our current version released on-
line [20], [21], we have 143,906 video frames and their cor-
responding pixel-level semantic labelling, from which 89,430
images contain instance-level annotations where movable objects
are further separated. Notice that our labelled images contains tem-
poral information which could also be useful for video semantic
and object segmentation.

To make the evaluation more comprehensive, similar to the
KITTI [2], we separate the recorded video with the level of easy,

5

Fig. 3: Examples with challenging environments for object detection and segmentation (Images are center-cropped for better visualization).
We highlight and zoom in the region of challenges in each image. (a) Objects with heavy occlusion and small scale. (b) Abnormal action by
cyclist drivers. (c) High contrast and overexposure due to shadows and strong sunlight. (d) Mirror reﬂection on bus glasses.

moderate, and heavy scene complexities based on the amount of
movable objects, such as person and vehicles. Tab. 2 compares the
scene complexities between ApolloScape, the Cityscapes [3] and
KITTI [2], where we show the statistics for each individual classes
of movable objects. ApolloScape contains more objects than others
in terms of both total number and average number of object
instances from images. More importantly, it contains stronger
challenging environments, as shown in Fig. 3. For instance, high
contrast regions due to sun light and large area of shadows from
the overpass. Mirror reﬂections of multiple nearby vehicles on a
bus glass due to highly crowded transportation. We hope these
case can help and motivate researchers to develop more robust
models against environment changes.

For semantic scene parsing, we annotate 24 different labels
in four groups. The speciﬁcations of the classes are partially
borrowed from the Cityscapes dataset. Fig. 4 gives the amount of
labelled pixels for each class. As expected, ApolloScape provides
much higher amount of average annotated pixels than Cityscapes,
especially for some rare classes, e.g.,
trafﬁc-light, pole. Here, we
add several new classes common in China. For instance, we add
“tricycle” that is one of the most popular means of transportation.
This class covers all kinds of three-wheeled vehicles that could
be both motorized and human-powered. The rider class in the
Cityscape is deﬁned as the person on means of transportation.
Here, we consider the person and the means of transportation as a
single moving object, and treat the two together as one class. The
three classes related to rider, i.e., bicycle, motorcycle, and tricycle,
represent means of transportation without rider and parked along
the roads.

Semantic lanemark segmentation. Automatically understanding
lane mark is perhaps the most important function for autonomous
driving since it is the guidance for possible actions. In Apol-
loScape, 27 different lane markings are used for evaluation as elab-
orated in Tab. 3 and Fig. 5. The labels are deﬁned based on lane
mark attributes including color (e.g., white and yellow) and type
(e.g., solid and broken). To be speciﬁc, 165949 images from 3 road
sites are labelled and released online [22], where 33760 images are
withheld for testing. Comparing to other public available datasets
such as KITTI [2] or the one from Tusimple [69], ApolloScape is
the ﬁrst large dataset containing rich semantic labelling for lane
marks with many variations.

Self-localization. Each frame of our recorded video is tagged
with high accurate GPS/IMU signal automatically. Therefore, the

Type

Color

Use

solid
solid
double solid
double solid
solid & broken
solid & broken

broken
broken
double broken

crosswalk

solid

solid
solid

solid

arrow
arrow
arrow
arrow
arrow
arrow
arrow
arrow

bump

diamond

rectangle

visible old marking
other markings

w
y
w
y
y
w

w
y
y

w

w
y

w

w

w
w
w
w
w
w
w
w

n/a

w/y

w/y

y/w
n/a

dividing
dividing
dividing, no pass
dividing, no pass
dividing, one-way pass
dividing, one-way pass

guiding
guiding
guiding

stopping

chevron
chevron

parking

zebra

u-turn
thru
thru & left turn
thru & right turn
left turn
right turn
left & right turn
left & u-turn

speed reduction

zebra attention

no parking

others
others

TABLE 3: Details of lane mark labels in our dataset (y: yellow,
w:white).

videos we released for segmentation are also available for self-
localization research. However, for setting up a benchmark, we
additionally collected a much larger amount of videos, which has
not been semantically labelled. Speciﬁcally, videos for localiza-
tion, as published at [23], contain 6 more roads at 4 different
cities, which include roughly 300k images, and road of 28km.

Our dataset has variations under different lighting, i.e., morn-
ing, noon and night, and driving conditions, i.e., rush and non-rush
hours, with stereo pair of images available. In addition, each road
has a survey-grade point cloud based 3D map that can be used
in ﬁnding matching pixels for both supervised and unsupervised

6

Fig. 4: 24 semantic classes and corresponding numbers of annotated pixels. Bar colors indicate different semantic groups.

Fig. 5: 27 lane mark labels and corresponding numbers of annotated pixels. Bar colors indicate 11 different lane mark usages. Here,“s w d”
is short for solid, white and dividing in Tab. 3 by combining the ﬁrst letter of type, color and usage respectively, and other classes are named
accordingly.

feature learning [70], [71] etc. Finally, we record each road
by driving from start-to-end and then end-to-start, which means
each position along a road will be looked at from two opposite
directions. This enables the research of camera localization with
large view changes such as that proposed in semantic visual
localization [10].

multiple rounds; 2) align these point clouds based on manually
selected control points; 3) remove the points based on the temporal
consistency. Formally, the condition to kept a point x in round j
is,

1(∃ xi ∈ Pi s.t. (cid:107)xi − xj(cid:107) < (cid:15)d)/r ≥ δ

(1)

r
(cid:88)

i=0

3.3 Labeling Process

In order to make our labeling of video frames accurate and efﬁ-
cient, we propose an active labelling pipeline by jointly consider
2D and 3D information, as shown in Fig. 6. The pipeline mainly
consists of two stages, 3D labeling and 2D labeling, to handle
static background/objects and moving objects respectively. The
basic idea of our pipeline is similar to the one described in [72],
which transfers the 3D labelled results to 2D images by camera
projection, while we need to handle much larger amount of data
and have different set up of the acquisition vehicle. Thus some
key techniques used in our pipeline are re-designed, which we
will elaborate later.

Moving object removal. As mentioned in Sec. 3.1, LIDAR
scanner Riegl is accurate in static background, while due to low
scanning rate, the point clouds of moving objects, such as vehi-
cles and pedestrians running on the road, could be compressed,
expanded, or completely missing in the captured point clouds as
illustrated in Fig. 8(b). Thus, we design to handle labelling static
background and moving object separately, as shown in Fig. 6.
Speciﬁcally, in the ﬁrst step, we do moving object removal from
our collected point clouds by 1) scan the same road segment

where δ = 0.6 and (cid:15)d = 0.025m in our setting, and 1() is an
indicator function. It indicates that a 3D point will be kept if it
appears with high frequency in many rounds of recording, i.e.,
60% of all times. We keep the remained point clouds as a static
background M for semantic labelling. 3D labelling. Next, for

labelling static background (3D Labeling), rather than label each
3D point and loading all the points, we ﬁrst separate the 3D points
into multiple parts, and over-segment each part of point clouds
into point clusters based on spatial distances and normal directions
using locally convex connected patches (LCCP) [73] implemented
with PCL [74]. Then, we label these point clusters manually using
our in-house developed 3D labelling tool as shown in Fig. 7, which
can easily do point cloud rotation, (inverse-)selection by polygons,
matching between point clouds and camera views, etc.. Notice at
this stage, there will be point clouds belonging to movable but
static objects such as bicycles and cars parking aside the road.
These point clouds are remained in our background, and also
labelled in 3D which are valuable to increase our label efﬁciency
of objects in 2D images.

To further improve 3D point cloud labelling efﬁciency, after
labelling of one road, we actively train a PointNet++ model [75]
to pre-label the over-segmented point cloud clusters of the next

7

Fig. 6: Our 2D/3D labeling pipeline that label static background/objects and moving objects separately. We also adopt active strategies for
accelerating the labelling process for scalability of the labelling process. For inputs, since GPS/IMU still has some errors, we manually add
control points to better align the point clouds and our image frames. In Sec. 3.3, we present the details of each components.

sc ∝

(cid:88)

1
|Pc|

min
t∈T

d(x, t)

(2)

x∈Pc
where Pc is the set of 3D points belong to class c, and T is
the set of ground truth camera poses. Then, given the relative
square size between different classes, we deﬁne an absolute range
to obtain the actual square size for splatting. This is non-trivial
since too large size will result in dilated edges, while too small
size will yield many holes. In our experiments, we set the range as
[0.025, 0.05], and ﬁnd that it provides the highest visual quality.
As shown in Fig. 8(e), invalid values in-between those projected
points are well in-painted, meanwhile the boundaries separating
different semantic classes are also well preserved, yielding the
both the background depth map and 2D labelled background. With
such a strategy, we increase labelling efﬁciency and accuracy for
video frames. For example, it could be very labor-intensive to
label texture-rich regions like trees, poles and trafﬁc lights further
away, especially when occlusion happens like fence on the road as
illustrated in Fig. 8(g).

2D labelling of objects and backgrounds. Finally, to generate
the ﬁnal labels (Fig. 8(f)), we need to label the moving objects in
the environments, and ﬁx missing parts at background like part of
building regions. Similar with 3D point cloud labelling, we also
developed an in-house 2D labelling tool with the same interface as
3D tool in Fig. 7. To speed up the 2D semantic labeling, we also
use a labelling strategy by training a CNN network for movable
objects and background [76] to pre-segment the 2D images. For
segmenting background, we test with original image resolution
collected by our camera, where the resolution is much higher
than that used in the original paper to increase the quality of
predicted region boundaries. For segment objects, similar with
MaskRCNN [13], we ﬁrst do 2D object detection with faster
RCNN [77], and segment object masks inside. However, since we
consider high requirements for object boundaries rather than class
accuracy, for each bounding box with high conﬁdence (≥ 0.9),
we enlarge the bounding box and crop out the object region with
context similar to [78]. Then, we upsample the cropped image to
a higher resolution by setting a minimum resolution of prediction
(minimum len greater than 512), and segment out the mask with
an actively trained mask CNN network with the same architecture
in [76]. The two networks for segmenting background and objects
are updated when images in one road is labelled. Here, the learning
parameters from these networks follow the original papers.

Finally, the segmented results from the networks are fused
with our rendered label map from the semantic 3D point clouds

Fig. 7: The user interface of our 3D labeling tool. At left-top, we
show the pre-deﬁned color code of different classes. At left-bottom,
we show the labelling logs which can be used to revert the labelling
when mistakes happen. At center part, labelled point cloud is shown
indicating the labelling progress.

road. Labellers are then asked to reﬁne and correct the results
by ﬁxing wrong annotations, which often occur near the object
boundaries. With the growing number of labelled point clouds,
our learned model can label new roads with increasing accuracy,
yielding accelerated labelling process, which scales up to various
cities and roads.

Splatting & Projection. Once the 3D annotations are generated,
the annotations of static background/objects for all the 2D image
frames are generated automatically by 3D-2D projections. In our
setting, the 3D map is a point cloud based environment. Although
the density of the point cloud is very high (one point per 25mm
within road regions), when the 3D points are far away from
the camera, the projected labels could be sparse, e.g., regions
of buildings shown in Fig. 8(c). Thus for each point
in the
environment, we adopt the point splatting technique, by enlarging
the 3D point to a square where the square size is determined by
its semantic class.

Formally, given a 6-DOF camera pose p = [q, t] ∈ SE(3),
where q ∈ SO(3) is the quaternion representation of rotation
and t ∈ R3 is translation, a label map can be rendered from
the semantic 3D map, where z-buffer is applied to ﬁnd the closest
point at each pixel. For a 3D point x belonging a class c, its square
size sc is set to be proportional to the class’ average distance to
the camera. Formally,

8

Fig. 8: A labelled example of the labelling pipeline for semantic parsing, a subset of color coded labels are shown below. (a) Image. (b)
Rendered label map with 3D point cloud projection, with an inaccurate moving object (rider) circled in blue. (c) Rendered label map with 3D
point cloud projection after points with low temporal consistency being removed. (d) & (e) Rendered depth map of background and rendered
label map after class dependent splatting in 3D point clouds (Sec. 3.3). (f) Merged label map with missing region in-painted, moving objects
and sky. (g) Another label map with very small trafﬁc lights. Details are zoomed out highlighting the details of our rendered label maps. Other
examples of our labeled videos is shown online [20].

following two rules: 1) for fusing segmented label map from
the background network, we ﬁll the predicted label in the pixels
without 3D projection, yielding a background semantic map. 2)
for fusing semantic object label segmented by object network, we
pasted the object mask over the fused background map, without
replacing the projected static movable object mask rendered from
3D points as mentioned in 3D labelling. We provided this fused
label map for labellers to further ﬁne tuning when error happens
especially around object boundary or occlusion from the object
masks. In addition, the user can omit any of the pre-segmented
results from CNNs to do relabelling if the segmented results are far
from satisfaction. Our label tool supports multiple actions such as
polygons and pasting brushes etc., which are commonly adopted
by many popular open source label tools 1.

A ﬁnal labelled example is shown in Fig. 8(f)&(g). Notice that
some background classes such as fence, trafﬁc light, and vege-
tation are annotated in details using our projection and missing
parts such as building glass can be ﬁll in. Thanks to 3D and active
learning, our overall pipeline save us signiﬁcant efforts in dense
per-pixel and per-frame semantic labelling for background and
objects. In practice, our labelling pipeline can reduce the time cost
of dense labelling task per-image from nearly 1 hour to around 10
minutes, with the guarantee of passing our quality control process.

Labelling of lane mark segments on road. In self-driving, lane
marks are information solely from static background. Fortunately,
our collected survey-grade 3D points not only have high density,
but also contain lighting intensity, dependent on which we can
distinguish the lane mark on the roads. Speciﬁcally, we perform
similar labelling process as 3D labelling of rigid background by
labelling each 3D point to pre-deﬁned lane mark labels listed in
Tab. 3.

Nevertheless, different from labelling 3D point clusters where
point clouds from buildings and trees are important, for lane
marks, we only need to consider points on the road. Therefore,
we take out the road point clouds based on normal directions,
and perform orthogonal projection of these points from the bird

1. https://github.com/topics/labeling-tool

view to a high resolution 2D image, as shown in Fig. 9, over
which labellers draw a polygon for each lane mark on the road. In
the meantime, our tool brings out the corresponding images, and
highlights the regions in 2D for each labelled polygon, where the
color and type of the labelled lanemark can be determined.

Labelling of instance segments. Thanks to an active labelling
component with detection, it is easy for us to generalize the
segmentation label map to produce instance masks given the
segmented results from the object detection and segmentation
networks. Speciﬁcally, we ask the labellers to reﬁne the boundary
between different instances when it is necessary, i.e., visually
signiﬁcantly not aligned with true object boundaries.

Control of label quality. Following the existing standard work
ﬂows of crowdsourcing object annotations [79], [80], [81], all our
2D/3D labeling tasks, e.g., 3D point cloud, 2D background, 2D
instance and 3D lanemark, contain veriﬁcation stages to control
the label quality. Speciﬁcally, for each task, we have a detailed
instruction to train our labellers, and a labeller is good to start
labelling after passing a designed quiz. We will publish all our
instructions on our website to beneﬁt the community upon the
publication of this paper.

After the labelling stage, we have a review stage, and each
reviewer is an experienced labeller had sufﬁcient labelled images
(over 500) passed our label quality veriﬁcation. The reviewer will
verify the quality and the coverage of labelled regions. In addition,
since we do video labelling, we also have reviewer to visually
verify the semantics are temporally consistent in the next frame.
Only if an image has passed two reviewers, it could be accepted
as a valid ground truth.

Existing issues. LiDAR scanners could fail on translucent and
highly reﬂective surfaces such as mirrors of buildings. Though we
ﬁxed this problem in part of our recorded videos, e.g., as shown
in Fig. 8), we found it is still over laborious to ﬁx every frame
in all our videos even with active labelling. Therefore, part of
video frames in our current release, pixels without 3D projection
or active labelling, e.g., sky and part of building in Fig. 1, are set

9

Fig. 9: Bird view of our projected road lane marks with labelling.

as void, so that they are ignored during the training and evaluation.
We leave labelling of these pixels to our future work.

4 DEEP LOCALIZATION AND SEGMENTATION
As discussed in introduction (Sec. 1), ApolloScape contains
various ground truth which enables multitask learning. In this
paper, we show such a case by creating a deep learning based
system for joint localization and semantic segmentation given a
semantic 3D map [82], which we call DeLS-3D, as illustrated
in Fig. 10. Speciﬁcally, at upper part, a pre-built 3D semantic
map is available. During testing, an online stream of images
and corresponding coarse camera poses from GPS/IMU are fed
into the system. Firstly, for each frame, a semantic label map is
rendered out given the input coarse camera pose, which is fed
into a pose CNN jointly with the respective RGB image. The
network calculates the relative rotation and translation, and yields
a corrected camera pose. To incorporate the temporal correlations,
the corrected poses from pose CNN are fed into a pose RNN
to further improves the estimation accuracy in the stream. Last,
given the rectiﬁed camera pose, a new label map is rendered out,
which is fed together with the image to a segment CNN. The
rendered label map helps to segment a spatially more accurate
and temporally more consistent result for the image stream of
video. In this system, since ApolloScape contains ground truth for
both camera poses and segments, it can be trained with strong
supervision at each end of outputs. The code for our system
has been released at https://github.com/pengwangucla/DeLS-3D.
In the following, we elaborate our network architectures and the
loss functions to train the whole system.

4.1 Camera localization with motion prior

Translation rectiﬁcation with road prior. One common lo-
calization priori for navigation is to use the 2D road map, by
constraining the GPS signals inside the road regions. We adopt a
similar strategy, since once the GPS signal is out of road regions,
the rendered label map will be totally different from the street-
view, and no correspondence can be found by the network.

To implement this constraint, ﬁrstly we render a 2D road map
image with a rasterization grid of 0.05m from our 3D semantic
map by using only road 3D points, i.e., points belong to car-lane,
pedestrian-lane and bike-lane etc. Then, at each pixel [x, y] ∈ Z2
in the 2D map, an offset value f (x, y) is pre-calculated indicating
its 2D offset to the closest pixel belongs to road through the breath-
ﬁrst-search (BFS) algorithm efﬁciently.

During online testing, given a noisy translation t = [tx, ty, tz],
we can ﬁnd the closest road points w.r.t. t using [tx, ty] +

f ((cid:98)tx(cid:99), (cid:98)ty(cid:99)) from our pre-calculated offset function. Then, a
label map is rendered based on the rectiﬁed camera pose, which is
fed to pose CNN.

CNN-GRU pose network architecture. As shown in Fig. 10,
our pose networks contain a pose CNN and a pose GRU-RNN.
Particularly, the CNN of our pose network takes as inputs an
image I and the rendered label map L from corresponding coarse
camera pose pc
i . It outputs a 7 dimension vector ˆpi representing
the relative pose between the image and rendered label map, and
we can get a corrected pose w.r.t. the 3D map by pi = pc
i + ˆpi.
For the network architecture of pose CNN, we follow the design
of DeMoN [55], which has large kernel to obtain bigger context
while keeping the amount of parameters and runtime manageable.
The convolutional kernel of this network consists a pair of 1D
ﬁlters in y and x-direction, and the encoder gradually reduces the
spatial resolution with stride of 2 while increasing the number of
channels. We list the details of the network in our implementation
details at Sec. 5.

Additionally, since the input is a stream of images, in order to
model the temporal dependency, after the pose CNN, a multi-layer
GRU with residual connection [83] is appended. More speciﬁcally,
we adopt a two layer GRU with 32 hidden states as illustrated in
Fig. 11. It includes high order interaction beyond nearby frames,
which is preferred for improve the pose estimation performance.
In traditional navigation applications of estimating 2D poses,
Kalman ﬁlter [84] is commonly applied by assuming either a
constant velocity or acceleration. In our case, because the vehicle
velocity is unknown, transition of camera poses is learned from
the training sequences, and in our experiments we show that the
motion predicted from RNN is better than using a Kalman ﬁlter
with a constant speed assumption, yielding further improvement
over the estimated ones from our pose CNN.

Pose loss. Following the PoseNet [48], we use the geometric
matching loss for training, which avoids the balancing factor
between rotation and translation. Formally, given a set of point
cloud in 3D P = {x}, and the loss for each image is written as,

L(p, p∗) =

ωlx|π(x, p) − π(x, p∗)|2

(3)

(cid:88)

x∈P

where p and p∗ are the estimated pose and ground truth pose
respectively. π() is a projective function that maps a 3D point x
to 2D image coordinates. lx is the semantic label of x and ωlx is
a weight factor dependent on the semantics. Here, we set stronger
weights for point cloud belong to certain classes like trafﬁc light,
and ﬁnd it helps pose CNN to achieve better performance. In [48],
only the 3D points visible to the current camera are applied to

10

Fig. 10: DeLS-3D overview. The black arrows show the testing process, and red arrows indicate the rendering (projection) operation in training
and inference. The yellow frustum shows the location of cameras inside the 3D map. The input of our system contains a sequence of images
and corresponding GPS/IMU signals. The outputs are the semantically segmented images, each with its reﬁned camera pose.

compute this loss to help the stability of training. However, the
amount of visible 3D points is still too large in practical for
us to apply the loss. Thus, we pre-render a depth map for each
training image with a resolution of 256 × 304 using the ground
truth camera pose, and use the back projected 3D points from the
depth map for training.

4.2 Video parsing with pose guidance

Having rectiﬁed pose at hand, one may direct render the semantic
3D world to the view of a camera, yielding a semantic parsing of
the current image. However, the estimated pose is not perfect, ﬁne
regions such as light poles can be completely misaligned. Other
issues also exist. For instance, many 3D points are missing due
to reﬂection, e.g., regions of glasses, and points can be sparse
at long distance. Last, dynamic objects in the input cannot be
represented by the projected label map, yielding incorrect labelling
at corresponding regions. Thus, we propose an additional segment
CNN to tackle these issues, while taking the rendered label map
as segmentation guidance.

Segment network architecture. As discussed in Sec. 2, heavily
parameterized networks such as ResNet are not efﬁcient enough

Fig. 11: The GRU RNN network architecture for modeling a sequence
of camera poses.

for our online application. Thus, as illustrated in Fig. 12, our
segment CNN is a light-weight network containing an encoder-
decoder network and a reﬁnement network, and both have similar
architecture with the corresponding ones used in DeMoN [55]
including 1D ﬁlters and mirror connections. However, since we
have a segment guidance from the 3D semantic map, we add
a residual stream (top part of Fig. 12), which encourages the
network to learn the differences between the rendered label map
and the ground truth. In [85], a full resolution stream is used to
keep spatial details, while here, we use the rendered label map to
keep the semantic spatial layout.

Another notable difference for encoder-decoder network from
DeMoN is that for network inputs, shown in Fig. 12, rather than
directly concatenate the label map with input image, we transform
the label map to a score map through one-hot operation, and
embed the score of each pixel to a 32 dimensional feature vector.
Then, we concatenate this feature vector with the ﬁrst layer output
from image, where the input channel imbalance between image
and label map is alleviated, which is shown to be useful by
previous works [86]. For reﬁnement network shown in Fig. 12,
we use the same strategy to handle the two inputs. Finally, the
segment network produces a score map, yielding the semantic
parsing of the given image.

We train the segment network ﬁrst with only RGB images, then
ﬁne-tune the network by adding the input of rendered label maps.
This is because our network is trained from scratch, therefore
it needs a large amount of data to learn effective features from
images. However, the rendered label map from the estimated pose
has on average 70% pixel accuracy, leaving only 30% of pixels
having effective gradients. This could easily drive the network to
over ﬁt to the rendered label map, while slowing down the process
towards learning features from images. Finally, for segmentation
loss, we use the standard softmax loss, and add intermediate
supervision right after the outputs from both the encoder and the
decoder as indicated in Fig. 12.

11

Fig. 12: Architecture of the segment CNN with rendered label map as a segmentation priori. At bottom of each convolutional block, we show
the ﬁlter size, and at top we mark the downsample rates of each block w.r.t. the input image size. The ‘softmax’ text box indicates the places a
loss is calculated. Details are in Sec. 4.2.

5 EXPERIMENTS

In this section, we ﬁrst evaluate our online deep localization and
segmentation algorithms (DeLS-3D) on two of our released roads,
which is a subset of our full data. We compare it against other
SOTA deep learning based visual localization, i.e., PoseNet [9],
and segmentation algorithms i.e., ResNet38 [76], which shows the
beneﬁts of multitask uniﬁcation.

Then, we elaborate the benchmarks setup online with Apol-
loScape and the current leading results, which follows many stan-
dard settings such as the ones from KITTI [2] and Cityscapes [3].
These tasks include semantic segmentation, semantic instance
segmentation, self-localization, lanemark segmentation. Due to the
“DeLS” algorithm proposed in this work does not follow those
standard experimental settings, we could not provide its results
for the benchmarks. Nevertheless, for each benchmark, we either
ran a baseline result with SOTA methods or launched a challenge
for other researchers, providing a reasonable estimation of the task
difﬁculties.

5.1 Evaluate DeLS-3D

In this section, we evaluate various settings for pose estimation and
segmentation to validate each component in the DeLS-3D system.
For GPS and IMU signal, despite we have multiple scans for the
same road segments, it is still very limited for training. Thus,
follow [53], we simulate noisy GPS and IMU by adding random
perturbation (cid:15) w.r.t. the ground truth pose following uniform
distributions. Speciﬁcally, translation and rotation noise are set
as (cid:15)t ∼ U (0, 7.5m) and (cid:15)r ∼ U (0◦, 15◦) respectively. We refer
to realistic data [87] for setting the noisy range of simulation.

Datasets. Two roads early collected at Beijing in China are used
in our evaluation. The ﬁrst one is inside a technology park, named
zhongguancun park (Zpark), and we scanned 6 rounds during
different daytimes. The 3D map generated has a road length
around 3km, and the distance between consecutive frames is
around 5m to 10m. We use 4 rounds of the video camera images
for training and 2 for testing, yielding 2242 training images and
756 testing images. The second one we scanned 10 rounds and
4km near a lake, named daoxianghu lake (Dlake), and the distance
between consecutive frames is around 1m to 3m. We use 8 rounds
of the video camera images for training and 2 for testing, yielding
17062 training images and 1973 testing images. The existing

semantic classes in the two datasets are shown in Tab. 5, which
are subsets from our full semantic classes.

Implementation details. To quickly render from the 3D map, we
adopt OpenGL to efﬁciently render a label map with the z-buffer
handling. A 512 × 608 image can be generated in 70ms with a
single Titan Z GPU, which is also the input size for both pose CNN
and segment CNN. For pose CNN, the ﬁlter sizes of all layers are
{32, 32, 64, 128, 256, 1024, 128, 7}, and the forward speed for
each frame is 9ms. For pose RNN, we sample sequences with
length of 100 from our data for training, and the speed for each
frame is 0.9ms on average. For segment CNN, we keep the size the
same as input, and the forward time is 90ms. Overall, the inference
speed is around 240ms per-image for performing joint localization
and segmentation. Both of the network is learned with ’Nadam’
optimizer [88] with a learning rate of 10−3. We sequentially train
these three models due to GPU memory limitation. Speciﬁcally,
for pose CNN and segment CNN, we stops at 150 epochs when
there is no performance gain, and for pose RNN, we stops at 200
epochs. For data augmentation, we use the imgaug2 library to add
lighting, blurring and ﬂipping variations. We keep a subset from
training images for validating the trained model from each epoch,
and choose the model performing best for evaluation.

i.e.,
For testing, since input GPS/IMU varies every time,
t = p∗ + (cid:15), we need to have a conﬁdence range of prediction
pc
for both camera pose and image segment, in order to verify the
improvement of each component we have is signiﬁcant. Speciﬁ-
cally, we report the standard variation of the results from a 10 time
simulation to obtain the conﬁdence range. Finally, we implement
all the networks by adopting the MXNet [89] platform.

Evaluation metrics. We use the median translation offset and
median relative angle [9]. For evaluating segment, we adopt the
commonly used pixel accuracy (Pix. Acc.), mean class accuracy
(mAcc.) and mean intersect-over-union (mIOU) as that from [76].

Pose Evaluation. We ﬁrst directly compare with the work of
PoseNet [9], [48], and use their published code and geometric
loss (Eq. (3)) to train a model on Zpark dataset. Due to scene
appearance similarity of the street-view, we did not obtain a
reasonable model with their methods [9], [48], i.e., results better
than the noisy GPS/IMU signal. Then, we experimented a leading
open-source monocular SLAM algorithm, i.e., ORB-SLAM [90],

2. https://github.com/aleju/imgaug

Data

k
r
a
p
Z

e
k
a
l
D

Method
Noisy pose
Pose CNN w/o semantic
Pose CNN w semantic
Noisy pose w KF
Pose RNN w/o CNN
Pose CNN w KF
Pose CNN-RNN

Pose CNN w semantic
Pose RNN w/o CNN
Pose CNN-RNN

Trans (m) ↓
3.45 ± 0.176
1.355 ± 0.052
1.331 ± 0.057
2.56 ± 0.16
1.282 ± 0.061
1.281 ± 0.06
1.005 ± 0.044

1.667 ± 0.05
1.385 ± 0.057
0.890 ± 0.037

Rot (◦)↓
7.87 ± 1.10
0.982 ± 0.023
0.727 ± 0.018
7.37 ± 1.01
1.731 ± 0.06
0.833 ± 0.03
0.719 ± 0.035

0.702 ± 0.015
1.222 ± 0.054
0.557± 0.021

Pix. Acc(%)↑
54.01 ± 1.5
70.99 ± 0.18
71.73 ± 0.18
55.1 ± 0.91
68.10 ± 0.32
72.00 ± 0.17
73.01 ± 0.16

87.83 ± 0.017
85.10 ± 0.03
88.55 ± 0.13

TABLE 4: Compare the accuracy of different settings for pose
estimation from the two datasets. Noisy pose indicates the noisy input
signal from GPS, IMU, and ’KF’ means kalman ﬁlter. The number
after ± indicates the standard deviation (S.D.) from 10 simulations.
↓ & ↑ means lower the better and higher the better respectively. We
can see the improvement is statistically signiﬁcant.

to do self-localization. However, it also provides no better result
than provided initial poses, since low-level ORB features fail to
match robustly due to many non-diffusion/reﬂective components
in Zpark, such as glass buildings and specular new roads, plus
repetitive appearance on trees. Therefore, in Tab. 4, we majorly list
the performance of estimated translation t and rotation r from our
model variations. At the 1st row, we show the median error of GPS
and IMU from our simulation. At the 2nd row, by using our pose
CNN with an additional input of projected label map, the model
can learn good relative pose between camera and GPS/IMU, which
signiﬁcantly reduces the error (60% for t, 85% for r). By adding
semantic cues, i.e., road priori and semantic weights in Eq. (3), the
pose errors are further reduced, especially for rotation (from 0.982
to 0.727 at the 3rd row). In fact, we found the most improvement
is from semantic weighting, while the road priori helps marginally.
In our future work, we would like to experiment larger noise and
more data variations, which will better validate different cues.

When having an video input, we ﬁrst evaluate a simple
baseline which reﬁnes the GPS/IMU with Kalman ﬁlter [84], i.e.,
’Noisy pose w KF’, which reasonably reduces the errors. Then,
we setup a baseline of performing RNN directly on the GPS/IMU
signal, and as shown at ’Pose RNN w/o CNN’, the estimated
t is even better than pose CNN, while r is comparably much
worse. This meets our expectation since the speed of camera is
easier to capture temporally than rotation. Another baseline we
adopt is performing to the output from Pose CNN by assuming a
constant speed which we set as the averaged speed from training
sequences. As shown at ’Pose CNN w KF’, it does improve
slightly for translation, but harms rotation, which means the ﬁlter
over smoothed the sequence. Finally when combining pose CNN
and RNN, it achieves the best pose estimation both for t and
r. We visualize some results at Fig. 13(a-c). Finally at bottom
of Tab. 4, we list corresponding results on Dlake dataset, which
draws similar conclusion with that from Zpark dataset.

Segment Evaluation. At top part of Tab. 5, we show the scene
parsing results of Zpark dataset. Firstly, we adopt one of the SOTA
parsing network on the CityScapes, i.e., ResNet38 [76], and train
it with Zpark dataset. It utilizes pre-trained parameters from the
CityScapes [3] dataset, and run with a 1.03s per-frame with our
resolution. As shown at the 1st row, it achieve reasonable accuracy
compare to our segment CNN (2nd row) when there is no pose
priori. However, our network is 10x faster. At 3rd row, we show
the results of rendered label map with the estimated pose after
pose RNN. Clearly, the results are much worse due to missing
pixels and object misalignment. At 4th row, we use the rendered
label map with ground truth pose as segment CNN guidance to

12

obtain an upper-bound for our segmentation performance. In this
case, the rendered label map aligns perfectly with the image, thus
signiﬁcantly improves the results by correct labelling most of
the static background. At 5th and 6th row, we show the results
trained with rendered label map with pose after pose CNN and
pose RNN respectively. We can see using pose CNN, the results
just improve slightly compare to the segment CNN. From our
observation, this is because the offset is still signiﬁcant for some
detailed structures, e.g., light-pole. However, when using the pose
after RNN, better alignment is achieved, and the segment accuracy
is improved signiﬁcantly especially for thin structured regions like
pole, as visualized in Fig. 13, which demonstrates the effectiveness
of our strategy.

Bottom part of Tab. 5 shows the results over the larger
Dlake dataset with more object labelling, where we see clearer
improvement, i.e., from 62.36 to 67.00, and here the rendered
label provides a background context for object segmentation,
which also improve the object parsing performance. In all classes,
we observe the performance of trafﬁc-light drops. In our opinion,
the majorly reason is trafﬁc-light only exists in intersection of
roads, which happens much fewer than objects such as light-pole,
yielding overﬁtting to the projected label maps from pose. We
may ﬁx this issue by training with even larger dataset or better
class balancing strategies, which is left to our future work.

In Fig. 13, we visualize several examples from our results
at
the view of camera. In the ﬁgure, we can see the noisy
pose (a), is progressively rectiﬁed by pose CNN (b) and pose
RNN (c) from view of camera. Additionally, at (d) and (e),
we compare the segment results without and with camera pose
respectively. As can be seen at the boxed regions, the segment
results with rendered label maps provide better accuracy in terms
of capturing region details at the boundary, discovering rare classes
and keeping correct scene layout. All of above could be important
for applications, e.g., ﬁguring out the trafﬁc signs and tele-poles
that are visually hard to detect. For additional visualization, please
check our demo videos online 34.

5.2 Benchmarks and baselines

With various tasks and large amount of labelled data we have
proposed, it would be non-practical for us to extensively explore
algorithms over all of them. Therefore, we release the data to
research community, and set up standard evaluation benchmarks.
Currently, four challenges have been set up online for evaluation
by withholding part of our labelled results as test set, which
include semantic segmentation [20], instance segmentation [21],
self-localization [23], lanemark segmentation [22].

For evaluation, in the tasks of semantic segmentation, lane-
mark segmentation, we adopt mean IoU, and in the task of
self-localization, we adopt median translation and rotation offset,
which are described in evaluation of DeLS-3D (Sec. 5.1). For
the task of instance segmentation, we use interpolated average
precision (AP) [91] under various IoU thresholds which is used
for the COCO challenge [36]. Later, we elaborate the split of each
dataset, the leading method on each benchmark currently.

Semantic segmentation. For video semantic segmentation, until
now, we haven’t receive valid results from the challenge. This
probably is due to the extremely large amount of training videos in

3. Zpark: https://www.youtube.com/watch?v=fqglYBipNfQ
4. Dlake https://www.youtube.com/watch?v=fqglYBipNfQ

mIO U

Pix. A cc

sky

car-lane

ped-lane

Data

k
r
a
p
Z

Method
64.66 95.87 93.6 98.5
ResNet38 [76]
68.35 95.61 94.2 98.6
SegCNN w/o Pose
91.7
32.61
Render PoseRNN
73.1
97.1
SegCNN w pose GT
96.1 99.4
79.37
SegCNN w Pose CNN 68.6
95.67 94.5 98.7
SegCNN w Pose RNN 69.93 95.98 94.9 98.8

-

82.9
83.8
50.4
92.5
84.3
85.3

bike-lane

87.2
89.5
62.1
93.9
89.3
90.2

t-cone

curb
61.8 46.1
69.3 47.5
16.9 6.6
81.4 68.8
69.0 46.8
71.9 45.7

t-stack

t-fence

light-pole

41.7
52.9
5.8
71.4
52.9
57.0

82.0
83.9
30.5
90.8
84.9
85.9

37.5
52.2
8.9
71.7
53.7
58.5

t-light

26.7
43.5
6.7
64.2
39.5
41.8

tele-pole

45.9
46.3
10.1
69.1
48.8
51.0

t-sign

49.5
52.9
16.3
72.2
50.4
52.2

billboard

te m p-build

building

67.3
69.2
29.4
76.2
69.9
70.9

sec.-stand

38.0
40.0
20.2
58.9
42.8
48.0

plants

89.2
88.6
73.5
91.6
88.5
89.3

85.1
87.0
70.6
91.3
87.5
88.5

Method

Data
e SegCNN w/o Pose
k
a
l
D

m I O
62.36
SegCNN w pose GT
73.10
SegCNN w pose RNN 67.00

U

P ix. A c c

96.7
97.7
97.1

s k y
95.3
96.8
95.8

c ar-la n e

96.8
97.5
97.2

p e d -la n e

12.8
41.3
30.0

t -sta c k

21.5
54.6
37.4

t -fe n c e

w all
81.9 53.0
87.5 70.5
84.2 62.6

lig ht-p ole

44.7
63.4
47.4

t -lig ht

65.8
77.6
65.5

tele-p ole

52.1
70.5
62.9

t -sig n

87.2
92.1
89.6

billb o ard

55.5
69.2
59.0

c y clist

pla nts

c ar
94.5 84.9 20.3
96.1 87.4 24.5
95.2 86.8 23.9

oto rbik e

m

28.9
43.8
34.4

tru c k
b u s
78.4 82.1
80.0 85.7
76.8 86.6

60.0
66.9
22.2
83.7
67.9
69.4

b uildin g

66.8
77.4
70.3

TABLE 5: Compare the accuracy of different segment networks setting over Zpark (top) and Dlake (bottom) dataset. t is short for ’trafﬁc’ in
the table. Here we drop the 10 times S.D. to save space because it is relatively small (≤ 0.1). Our results are especially good at parsing of
detailed structures and scene layouts, which is visualized in Fig. 13.

13

object

66.3
63.8
-
56.7
60.9
59.5

Fig. 13: Results from each intermediate stage out of the system over Zpark dataset. Label map is overlaid with the image. Improved regions
are boxed and zoomed out (best in color). More results are shown in the online videos for Zpark and Dlake.

ApolloScape, making training a model with SOTA deep learning
models such as ResNet [59] not-practical. Thus, we select a subset
from the whole data for comparison of one model performance
between ApolloScape and Cityscapes. Speciﬁcally, 5,378 training
images and 671 testing images are carefully selected from our
140K labelled semantic video frames for setting up the benchmark,
which maintains the diversity and objects appeared of the collected
scenes. The selected images will be released at our website [20].
We conducted our experiments using ResNet-38 network [76]
that trades depth for width comparing with the original ResNet
structure [59]. We ﬁne-tune their released model using our training
with initial learning rate 0.0001, standard SGD with momentum
0.9 and weight decay 0.0005, random crop with size 512 × 512,
10 times data augmentation that includes scaling and left-right
ﬂipping, and we train the network for 100 epochs. The predictions
are computed with the original image resolution without any post-
processing steps such as multi-scale ensemble etc.. Tab. 6 shows
the parsing results of classes in common for these two datasets.
Notice that using exactly same training procedure, the test IoU
with our dataset are much lower than that from the Cityscapes
mostly due to the challenges we have mentioned at Sec. 3.2,
especially for movable objects, where mIoU is 34.6% lower than
the corresponding one for the Cityscapes.

Here, we leave the training a model with the our full dataset

to the research community and our future work.

Instance segmentation. This task is an extension of semantic
object parsing by jointly considering detection and segmentation.
Speciﬁcally, we select 39212 training images and 1907 testing
images, and set up a challenge benchmark online [21] evaluating
7 objects in our dataset (Upper part of Tab. 6) to collect potential

Class

Cityscapes

ApolloScape

IoU

Group

movable
object

mIoU

surface

infrastructure

nature

car
motorcycle
bicycle
person
rider
truck
bus

road
sidewalk

fence
trafﬁc light
pole
trafﬁc sign
wall
building

vegetation

94.67
70.51
78.55
83.17
65.45
62.43
88.61

77.63

97.94
84.08

61.49
70.98
62.11
78.93
58.81
92.66

92.41

87.12
27.99
48.65
57.12
6.58
25.28
48.73

43.07

92.03
46.42

42.08
67.49
46.02
79.60
8.41
65.71

90.53

TABLE 6: Results of image parsing based on ResNet-38 network.

issues within autonomous driving scenario. During the past few
month, there are over 140 teams attended our challenge, which
reveals our community is much more interested in object level
understanding rather than scene segmentation.

The leading results from our participants are shown in Tab. 8,
where we can see in general the reported mAP of winning teams
are lower than those reported in Cityscapes benchmarks [92],
by using similar strategies [93] modiﬁed from MaskRCNN [13].
Based on the challenge reports from the winning team [94],
comparing to Cityscapes, ApolloScape contains more tiny and
occluded objects (60% object has scale less than 32 pixels), which

Method
ResNet38 [76] 40.0

mIOU s

48.6

d

w

d

y

s
53.1

d n

y

d s
57.8

g

w

b
52.1

g

y

b
22.7

s

w

s
36.4

p

w

s
18.7

z

y

w

c

59.1

w t

a
40.4

w tl

a
27.1

w tr

a
49.1

w l

r

w

a
a
57.4 20.9

w lr

a
0.01

sr

n

b

y

w

r

n p

n

n

m

o

0.9

36.1

40.5

TABLE 7: The IoU using one SOTA semantic segmentation architecture, i.e., ResNet38 [76]. Here the amount of class is less than that in
Tab. 3 due to zero accuracy over some predeﬁned labels.

14

Dataset

metric

1st

2nd

3nd

ApolloScape

Cityscapes

mAP

33.97 [94]

30.22 [95]

25.02 [96]

38.0

37.2

36.4 [93]

TABLE 8: Results of top ranked instance segmentation algorithms
in ApolloScape and Cityscapes (Numbers are obtained at the date of
submission).

Road ID

Trans (m) ↓

Rot (◦)↓

Road11
Road12
Road14
Road15
Road16
Road17

mean

0.0476
0.1115
0.0785
0.0711
0.1229
0.4934

0.1542

0.0452
0.0528
0.0825
0.1240
0.2063
0.3135

0.1374

reducing the issues from speed changing in real applications. In
the near future, hopefully, we can add more challenging scenarios
with more variations in driving speed and weathers.

In summary, from the dataset benchmarks we set up and eval-
uated algorithms, we found for low-level localization, the results
are impressively good, while for high level semantic understand-
ing, Apolloscape provides additional challenges and new issues,
yielding limited accuracy for SOTA algorithms, i.e., best mAP is
around 33% for instance segmentation, and best mIoU is around
40% for lane segmentation. Comparing to human perception,
visual based algorithms for autonomous driving deﬁnitely need
further research to handle extremely difﬁcult cases.

TABLE 9: Detailed localization accuracy of the leading results on our
benchmark from [97].

leads to signiﬁcant drop of performance when transfer models
trained on other datasets.

Lanemark segmentation. Lanemark segmentation task follows
the same metric as semantic segmentation, which contains 132189
training images and 33790 testing images. Our in-house challenge
benchmark [22] chooses to evaluate 35 most common lane mark
types on the road as listed in Tab. 3.

Until the submission of this paper, we only have one work
based on ResNet-38 network [76] evaluated, probably due to the
large amount of data (160K+ images). We show the corresponding
detailed results in Tab. 7, where we can see the mIoU of each class
are still very limited (40%) comparing to the accuracy of leading
semantic segmentation algorithms on general classes. We think
this is mostly because the high contrast, dimmed and broken lane
marks on the road such as the cases shown in Fig. 3.

Self-localization. We use the same metrics for evaluating camera
pose, i.e., median offset of translation and rotation, as described in
Sec. 5.1. This task contains driving videos in 6 sites from Beijing,
Guangzhou, Chengdu and Shanghai in China, under multiple
driving scenarios and day times. In total, we provide 153 training
videos and 71 testing videos including over 300k image frames,
and build an in-house challenge benchmark website [23] most
recently.

Currently, we also have few submissions, while the leading
one published is from one of the SOTA method for large-scale
image based localization [97]. The method is based on image
retrieval with learned deep features via various triplet losses. We
show their reported number in Tab. 9, where the localization
errors are surprisingly small, i.e.,
translation is around 15cm
and rotation error is around 0.14 degree. Originally, we believe
image appearance similarity on the street or highway can fail deep
network models. However, from the participant results, especially
designed features distinguish minor appearance changes and pro-
vide high accurate localization results. Another possibility is that
our acquisition vehicle always drives in a roughly constant speed,

6 CONCLUSION AND FUTURE WORK

In this paper, we present the ApolloScape, a large, diverse, and
multi-task dataset for autonomous driving, which includes high
density 3D point cloud map, per-pixel, per-frame semantic image
label, lane mark label, semantic instance segmentation for various
videos. Every frame of our videos is geo-tagged with high accurate
GPS/IMU device. ApolloScape is signiﬁcantly larger than existing
autonomous driving datasets, e.g., KITTI [2] and Cityscapes [3],
yielding more challenges for computer vision research ﬁeld. In
order to label such a large dataset, we developed an active 2D/3D
joint annotation pipeline, which effectively accelerate the labelling
process. Back on ApolloScape, we developed a joint localization
and segmentation algorithm with a 3D semantic map, which fuses
multi-sensors, is simple and runs efﬁciently, yielding strong results
in both tasks. We hope it may motivate researcher to develop
algorithms handling multiple tasks simultaneously by considering
their inner geometrical relationships. Finally, for each individual
task, we set up an online evaluation benchmark where different
algorithms can compete with a fair platform.

Last but not the least, ApolloScape is an evolving dataset,
not only in terms of data scale, but also in terms of various
driving conditions, tasks and acquisition devices. For example,
ﬁrstly, we plan to enlarge our dataset to contain more diversiﬁed
driving environments including snow, and foggy. Secondly, we
also released our labelled 3D cars [19], stereo images, 3D humans
and tracking [98] of objects in 3D recently. Thirdly, we plan to
mount a panoramic camera system, and Velodyne [4] in near future
to generate depth maps for objects and panoramic images.

ACKNOWLEDGMENTS

This work is supported by Baidu Inc.. We also thank the work of
Xibin Song, Binbin Cao, Jin Fang, He Jiang, Yu Zhang, Xiang
Gu, and Xiaofei Liu for their laborious efforts in organizing data,
helping writing label tools, checking labelled results and manage
the content of benchmark websites. We thank Alan L. Yuille,
Hongdong Li and Andreas Geiger for benchmark suggestions.

REFERENCES

“ApolloScape Website,” apolloscape.auto. 1

[1]
[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
The kitti dataset,” International Journal of Robotics Research (IJRR),
2013. 1, 2, 3, 4, 5, 11, 14

[3] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic
urban scene understanding,” in Proc. of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2016. 1, 3, 4, 5, 11, 12,
14

[4] Velodyne Lidar, “HDL-64E,” http://velodynelidar.com/, 2018, [Online;

accessed 01-March-2018]. 1, 4, 14

[5] A. Kar, C. H¨ane, and J. Malik, “Learning a multi-view stereo machine,”
in Advances in neural information processing systems, 2017, pp. 365–
376. 1

[6] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, “Deepmvs:
Learning multi-view stereopsis,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2018, pp. 2821–2830. 1

[7] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, “Mvsnet: Depth inference
for unstructured multi-view stereo,” in The European Conference on
Computer Vision (ECCV), September 2018. 1

[8] X. Cheng, P. Wang, and R. Yang, “Depth estimation via afﬁnity learned
with convolutional spatial propagation network,” European Conference
on Computer Vision, 2018. 1

[9] A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional
network for real-time 6-dof camera relocalization,” in Proceedings of the
IEEE international conference on computer vision, 2015, pp. 2938–2946.
1, 2, 3, 11

[10] J. L. Sch¨onberger, M. Pollefeys, A. Geiger, and T. Sattler, “Semantic
visual localization,” ISPRS Journal of Photogrammetry and Remote
Sensing (JPRS), 2018. 1, 3, 6

[11] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2015, pp. 3431–3440. 1, 4
[12] L. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking
atrous convolution for semantic image segmentation,” CoRR, vol.
abs/1706.05587, 2017. 1, 2, 4

[13] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” IEEE
transactions on pattern analysis and machine intelligence, 2018. 1, 7, 13
[14] L.-C. Chen, A. Hermans, G. Papandreou, F. Schroff, P. Wang, and
H. Adam, “Masklab: Instance segmentation by reﬁning object detection
with semantic and direction features,” arXiv preprint arXiv:1712.04837,
2017. 1

[15] Y. Xiang, R. Mottaghi, and S. Savarese, “Beyond pascal: A benchmark
for 3d object detection in the wild,” in Applications of Computer Vision
(WACV), 2014 IEEE Winter Conference on.
IEEE, 2014, pp. 75–82. 1
[16] A. Kar, S. Tulsiani, J. Carreira, and J. Malik, “Category-speciﬁc object
reconstruction from a single image,” in Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2015, pp. 1966–1974.
1

[17] F. Guney and A. Geiger, “Displets: Resolving stereo ambiguities using
object knowledge,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2015, pp. 4165–4175. 1

[18] A. Kundu, Y. Li, and J. M. Rehg, “3d-rcnn: Instance-level 3d object
reconstruction via render-and-compare,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
3559–3568. 1

[19] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su, H. Li, and
R. Yang, “Apollocar3d: A large 3d car instance understanding benchmark
for autonomous driving,” CVPR, 2019. 1, 14

[20] ApolloScape., “Semantic segmentation,” http://apolloscape.auto/scene.

html. 1, 4, 8, 12, 13
“Instance

[21] ——,

cvpr-2018-autonomous-driving. 1, 4, 12, 13

[22] ——,

“Lanemark
segmentation.html. 1, 5, 12, 14

segmentation,”

segmentation,”

https://www.kaggle.com/c/

http://apolloscape.auto/lane

[23] ——, “Localization,” http://apolloscape.auto/self localization.html. 1, 5,

[24] W. Peng et al., “ApolloScape API,” https://github.com/ApolloScapeAuto/

12, 14

dataset-api. 1

[25] A. Kundu, Y. Li, F. Dellaert, F. Li, and J. M. Rehg, “Joint semantic
segmentation and 3d reconstruction from monocular video,” in European
Conference on Computer Vision. Springer, 2014, pp. 703–718. 2, 4
[26] G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic object classes
in video: A high-deﬁnition ground truth database,” Pattern Recognition
Letters, vol. 30, no. 2, pp. 88–97, 2009. 2, 3

15

[27] S. Wang, M. Bai, G. Mattyus, H. Chu, W. Luo, B. Yang, J. Liang,
J. Cheverie, S. Fidler, and R. Urtasun, “Torontocity: Seeing the world
with a million eyes,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2017, pp. 3009–3017. 3

[28] G. Neuhold, T. Ollmann, S. R. Bulo, and P. Kontschieder, “The mapillary
vistas dataset for semantic understanding of street scenes,” in Proceed-
ings of the International Conference on Computer Vision (ICCV), Venice,
Italy, 2017, pp. 22–29. 3

[29] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,
“Bdd100k: A diverse driving video database with scalable annotation
tooling,” arXiv preprint arXiv:1805.04687, 2018. 3, 4

[30] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
synthia dataset: A large collection of synthetic images for semantic
segmentation of urban scenes,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 3234–3243. 3
[31] S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,” in

International Conference on Computer Vision (ICCV), 2017. 3

[32] D. Scharstein, H. Hirschm¨uller, Y. Kitajima, G. Krathwohl, N. Neˇsi´c,
X. Wang, and P. Westling, “High-resolution stereo datasets with subpixel-
accurate ground truth,” in German Conference on Pattern Recognition.
Springer, 2014, pp. 31–42. 2

[33] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
and support inference from rgbd images,” in European Conference on
Computer Vision. Springer, 2012, pp. 746–760. 2

[34] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
D. Safari, M. Okutomi, M. Pollefeys, J. Sivic et al., “Benchmarking 6dof
outdoor visual localization in changing conditions,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, vol. 1,
2018. 2

[35] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
“The pascal visual object classes (voc) challenge,” International journal
of computer vision, vol. 88, no. 2, pp. 303–338, 2010. 2

[36] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision. Springer, 2014,
pp. 740–755. 2, 12

[37] “Unity Development Platform,” https://unity3d.com/. 3
[38] J. Hoffman, D. Wang, F. Yu, and T. Darrell, “Fcns in the wild:
Pixel-level adversarial and constraint-based adaptation,” arXiv preprint
arXiv:1612.02649, 2016. 3

[39] Y. Zhang, P. David, and B. Gong, “Curriculum domain adaptation for
semantic segmentation of urban scenes,” in The IEEE International
Conference on Computer Vision (ICCV), vol. 2, no. 5, 2017, p. 6. 3
[40] Y. Chen, W. Li, and L. Van Gool, “Road: Reality oriented adaptation for
semantic segmentation of urban scenes,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
7892–7901. 3

[41] B. M. Haralick, C.-N. Lee, K. Ottenberg, and M. N¨olle, “Review and
analysis of solutions of the three point perspective pose estimation
problem,” IJCV, vol. 13, no. 3, pp. 331–356, 1994. 3

[42] L. Kneip, H. Li, and Y. Seo, “Upnp: An optimal o (n) solution to
the absolute pose problem with universal applicability,” in European
Conference on Computer Vision. Springer, 2014, pp. 127–142. 3
[43] P. David, D. Dementhon, R. Duraiswami, and H. Samet, “Softposit:
Simultaneous pose and correspondence determination,” IJCV, vol. 59,
no. 3, pp. 259–284, 2004. 3

[44] F. Moreno-Noguer, V. Lepetit, and P. Fua, “Pose priors for simultane-
ously solving alignment and correspondence,” European Conference on
Computer Vision, pp. 405–418, 2008. 3

[45] D. Campbell, L. Petersson, L. Kneip, and H. Li, “Globally-optimal
inlier set maximisation for simultaneous camera pose and feature corre-
spondence,” in The IEEE International Conference on Computer Vision
(ICCV), vol. 1, no. 3, 2017. 3

[46] T. Sattler, A. Torii, J. Sivic, M. Pollefeys, H. Taira, M. Okutomi, and
T. Pajdla, “Are large-scale 3d models really necessary for accurate visual
localization?” in 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

IEEE, 2017, pp. 6175–6184. 3

[47] J. Engel, T. Sch¨ops, and D. Cremers, “Lsd-slam: Large-scale di-
rect monocular slam,” in European Conference on Computer Vision.
Springer, 2014, pp. 834–849. 3

[48] A. Kendall, R. Cipolla et al., “Geometric loss functions for camera pose
regression with deep learning,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), vol. 3, 2017, p. 8.
3, 9, 10, 11

[49] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, and
D. Cremers, “Image-based localization using lstms for structured feature
correlation,” in Int. Conf. Comput. Vis.(ICCV), 2017, pp. 627–637. 3

[50] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen, “Vidloc:
A deep spatio-temporal model for 6-dof video-clip relocalization,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), vol. 3, 2017. 3

[51] H. Coskun, F. Achilles, R. DiPietro, N. Navab, and F. Tombari, “Long
short-term memory kalman ﬁlters: Recurrent neural estimators for pose
regularization,” in Proceedings of the International Conference on Com-
puter Vision (ICCV), 2017. 3

[52] K.-N. Lianos, J. L. Sch¨onberger, M. Pollefeys, and T. Sattler, “Vso:
Visual semantic odometry,” in Proceedings of the European Conference
on Computer Vision (ECCV), 2018, pp. 234–250. 3

[53] K. Vishal, C. Jawahar, and V. Chari, “Accurate localization by fusing
images and gps signals,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops, 2015, pp. 17–24.
4, 11

[54] Z. Laskar, I. Melekhov, S. Kalia, and J. Kannala, “Camera relocaliza-
tion by computing pairwise relative poses using convolutional neural
network,” Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2017. 4

[55] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy,
and T. Brox, “Demon: Depth and motion network for learning monocular
stereo,” in IEEE Conference on computer vision and pattern recognition
(CVPR), vol. 5, 2017, p. 6. 4, 9, 10

[56] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing

network,” CVPR, 2017. 4

[57] A. Arnab, S. Jayasumana, S. Zheng, and P. H. Torr, “Higher order condi-
tional random ﬁelds in deep neural networks,” in European Conference
on Computer Vision. Springer, 2016, pp. 524–540. 4

[58] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling with
lstm recurrent neural networks,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2015, pp. 3547–3555. 4

[59] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778. 4, 13

[60] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep
neural network architecture for real-time semantic segmentation,” CoRR,
vol. abs/1606.02147, 2016. 4

[61] H. Zhao, X. Qi, X. Shen, J. Shi, and J. Jia, “Icnet for real-time semantic
segmentation on high-resolution images,” CoRR, vol. abs/1704.08545,
2017. 4

[62] A. Kundu, V. Vineet, and V. Koltun, “Feature space optimization for
semantic video segmentation,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2016, pp. 3168–3175. 4

[63] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
P. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning
optical ﬂow with convolutional networks,” in Proceedings of the IEEE
International Conference on Computer Vision, 2015, pp. 2758–2766. 4

[64] R. Gadde, V. Jampani, and P. V. Gehler, “Semantic video cnns through
representation warping,” Proceedings of the International Conference on
Computer Vision (ICCV), 2017. 4

[65] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, “Deep feature ﬂow for
video recognition,” Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2017. 4

[66] C. Hane, C. Zach, A. Cohen, R. Angst, and M. Pollefeys, “Joint 3d
scene reconstruction and class segmentation,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2013, pp. 97–
104. 4

[67] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time
dense monocular slam with learned depth prediction,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), vol. 2, 2017. 4

[68] RIEGL, “VMX-1HA,” http://www.riegl.com/. 4
[69] TuSimple., “Lanemark segmentation,” http://benchmark.tusimple.ai/#/. 5
[70] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “Deepmatching:
Hierarchical deformable dense matching,” IJCV, vol. 120, no. 3, pp. 300–
323, 2016. 5

[71] C. Luo, Z. Yang, P. Wang, Y. Wang, W. Xu, R. Nevatia, and A. Yuille,
“Every pixel counts++: Joint learning of geometry and motion with 3d
holistic understanding,” arXiv preprint arXiv:1810.06125, 2018. 5
[72] J. Xie, M. Kiefel, M.-T. Sun, and A. Geiger, “Semantic instance anno-
tation of street scenes by 3d to 2d label transfer,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2016,
pp. 3688–3697. 6

[73] S. Christoph Stein, M. Schoeler, J. Papon, and F. Worgotter, “Object par-
titioning using local convexity,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2014, pp. 304–311. 6

[74] “Point Cloud Library,” pointclouds.org. 6

16

[75] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
feature learning on point sets in a metric space,” in Advances in Neural
Information Processing Systems, 2017, pp. 5105–5114. 6

[76] Z. Wu, C. Shen, and A. v. d. Hengel, “Wider or deeper: Revisiting the
resnet model for visual recognition,” arXiv preprint arXiv:1611.10080,
2016. 7, 11, 12, 13, 14

[77] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” in Advances in neural
information processing systems, 2015, pp. 91–99. 7

[78] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L. Yuille,
“Joint object and part segmentation using deep learned potentials,” in
Proceedings of the IEEE International Conference on Computer Vision,
2015, pp. 1573–1581. 7

[79] H. Su, J. Deng, and L. Fei-Fei, “Crowdsourcing annotations for visual
object detection,” in Workshops at the Twenty-Sixth AAAI Conference on
Artiﬁcial Intelligence, vol. 1, no. 2, 2012. 8

[80] A. Kovashka, O. Russakovsky, L. Fei-Fei, K. Grauman et al., “Crowd-
sourcing in computer vision,” Foundations and Trends R(cid:13) in Computer
Graphics and Vision, vol. 10, no. 3, pp. 177–243, 2016. 8

[81] G. Li, J. Wang, Y. Zheng, and M. J. Franklin, “Crowdsourced data
management: A survey,” IEEE Transactions on Knowledge and Data
Engineering, vol. 28, no. 9, pp. 2296–2319, 2016. 8

[82] P. Wang, R. Yang, B. Cao, W. Xu, and Y. Lin, “Dels-3d: Deep localization
and segmentation with a 3d semantic map,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
5860–5869. 9

[83] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,
M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., “Google’s neural ma-
chine translation system: Bridging the gap between human and machine
translation,” arXiv preprint arXiv:1609.08144, 2016. 9

[84] R. E. Kalman et al., “A new approach to linear ﬁltering and prediction
problems,” Journal of basic Engineering, vol. 82, no. 1, pp. 35–45, 1960.
9, 12

[85] T. Pohlen, A. Hermans, M. Mathias, and B. Leibe, “Full-resolution resid-
ual networks for semantic segmentation in street scenes,” Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
10

[86] D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic
labels with a common multi-scale convolutional architecture,” in Pro-
ceedings of the IEEE International Conference on Computer Vision,
2015, pp. 2650–2658. 10

[87] B.-H. Lee, J.-H. Song, J.-H. Im, S.-H. Im, M.-B. Heo, and G.-I. Jee,
“Gps/dr error estimation for autonomous vehicle localization,” Sensors,
vol. 15, no. 8, pp. 20 779–20 798, 2015. 11

[88] T. Dozat, “Incorporating nesterov momentum into adam,” 2016. 11
[89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,
C. Zhang, and Z. Zhang, “Mxnet: A ﬂexible and efﬁcient machine
learning library for heterogeneous distributed systems,” CoRR, vol.
abs/1512.01274, 2015. 11

[90] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a versatile
and accurate monocular slam system,” IEEE transactions on robotics,
vol. 31, no. 5, pp. 1147–1163, 2015. 11

[91] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik, “Simultaneous
detection and segmentation,” in European Conference on Computer
Vision. Springer, 2014, pp. 297–312. 12

[92] “Cityscapes

instance

https://www.
cityscapes-dataset.com/benchmarks/#instance-level-scene-labeling-task.
13

segmentation

benchmark,”

[93] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network
for instance segmentation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018, pp. 8759–8768. 13, 14
[94] Z. Yueqing, L. Zeming, and G. Yu, “Find tiny instance segmentation,”

http://www.skicyyu.org/WAD/wad ﬁnal.pdf, 2018. 13, 14

[95] Smart Vision SG,

place,”

2nd
Kaggle-CVPR-2018-WAD-Video-Segmentation-Challenge-Solution,
2018. 14

“Wad

segmentation
https://github.com/Computational-Camera/

instance

[96] SZU N606, “Wad instance segmentation 3rd place,” https://github.com/
2018.

wwoody827/cvpr-2018-autonomous-driving-autopilot-solution,
14

[97] L. Liu, H. Li, and Y. Dai, “Deep stochastic attraction and repulsion em-
bedding for image based localization,” arXiv preprint arXiv:1808.08779,
2018. 14

[98] Y. Ma, X. Zhu, S. Zhang, R. Yang, W. Wang, and D. Manocha,
“Trafﬁcpredict: Trajectory prediction for heterogeneous trafﬁc-agents,”
AAAI, 2019. 14

17

Ruigang Yang is the chief scientist for 3D vision
at Baidu. He is also a full professor at the Uni-
versity of Kentucky (on leave). His research in-
terests include 3D computer vision and 3D com-
puter graphics, in particular 3D modeling and 3D
data analysis. He has published over 100 papers
with an H-index of 48. He is an Associate Editor
for IEEE T-PAMI. He has been a program co-
chair for 3DIMPVT (now 3DV) 2011 and WACV
2014, and he has been area chairs for both ICCV
and CVPR multiple times.

Xinyu Huang is currently a senior research sci-
entist at Baidu Research and an associate pro-
fessor at the North Carolina Central University.
He obtained his Ph.D. degree from the University
of Kentucky and B.S. degree from Huazhong
University of Science and Technology. His re-
search areas include computer vision,
image
processing, pattern recognition, and machine
learning.

Peng Wang is a senior research scientist in
Baidu USA LLC. He obtained his Ph.D. degree
in University of California, Los Angeles, advised
by Prof. Alan Yuille. Before that, he received his
B.S. and M.S. from Peking University, China. His
research interest is image parsing and 3D under-
standing, and vision based autonomous driving
system. He has around 30 published papers in
ECCV/CVPR/ICCV/NIPS.

Xinjing Cheng is a research scientist with
Robotics and Autonomous Driving Lab, Baidu
Research, Baidu Inc., Beijing, China. Before
that, he was a research assistant with the In-
telligent Bionic Center, Shenzhen Institutes of
Advanced Technology (SIAT), Chinese Academy
of Sciences(CAS), Shenzhen, China. His current
research interests include computer vision, deep
learning, robotics and autonomous driving.

Dingfu Zhou a senior researcher at Robotics
and Autonomous Driving Laboratory (RAL) of
Baidu. Before joining in Baidu, he worked as
a PostDoc Researcher in the Research School
of Engineering at the Australian National Uni-
versity, Canberra, Australia. He obtained his
Ph.D degree in System and Control from Sor-
bonne Universit ´es, Universit ´e de Technologie de
Compi `egne, Compi `egne, France, in 2014. He
received the B.E. degree and M.E degree both
in signal and information processing from North-
western Polytechnical University, Xian, China. His research interests
include Simultaneous Localization and Mapping, Structure from Motion,
Classiﬁcation and their application in Autonomous Driving.

Qichuan Geng is a Ph.D. candidate, at State
Key Lab of Virtual Reality Technology and Sys-
tems, Beihang University, Beijing, China. He re-
ceived his B.S. degree from Beihang Univer-
sity in 2012. His main research interests in-
clude computer vision, semantic segmentation
and scene geometry recovery.

