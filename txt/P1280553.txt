8
1
0
2
 
g
u
A
 
3
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
6
9
7
0
.
8
0
8
1
:
v
i
X
r
a

Learning Human-Object Interactions by
Graph Parsing Neural Networks

Siyuan Qi∗1,2, Wenguan Wang∗1,3, Baoxiong Jia1,4,
Jianbing Shen†3,5, and Song-Chun Zhu1,2

1 University of California, Los Angeles
2 International Center for AI and Robot Autonomy (CARA)
3 Beijing Institute of Technology
4 Peking University
5 Inception Institute of Artiﬁcial Intelligence

syqi@cs.ucla.edu

wenguanwang.ai@gmail.com baoxiongjia@ucla.edu

shenjianbing@bit.edu.cn

sczhu@stat.ucla.edu

Abstract. This paper addresses the task of detecting and recognizing
human-object interactions (HOI) in images and videos. We introduce the
Graph Parsing Neural Network (GPNN), a framework that incorporates
structural knowledge while being diﬀerentiable end-to-end. For a given
scene, GPNN infers a parse graph that includes i) the HOI graph struc-
ture represented by an adjacency matrix, and ii) the node labels. Within
a message passing inference framework, GPNN iteratively computes the
adjacency matrices and node labels. We extensively evaluate our model
on three HOI detection benchmarks on images and videos: HICO-DET,
V-COCO, and CAD-120 datasets. Our approach signiﬁcantly outper-
forms state-of-art methods, verifying that GPNN is scalable to large
datasets and applies to spatial-temporal settings. The code is available
at https://github.com/SiyuanQi/gpnn.

Keywords: Human-Object Interaction · Message Passing · Graph Pars-
ing · Neural Networks

1 Introduction

The task of human-object interaction (HOI) understanding aims to infer the
relationships between human and objects, such as “riding a bike” or “washing a
bike”. Beyond traditional visual recognition of individual instances, e.g., human
pose estimation, action recognition, and object detection, recognizing HOIs re-
quires a deeper semantic understanding of image contents. Recently, deep neural
networks (DNNs) have shown impressive progress on above individual tasks of
instance recognition, while relatively few methods [1, 2, 14, 38] were proposed for
HOI recognition. This is mainly because it requires reasoning beyond perception,
by integrating information from human, objects, and their complex relationships.

∗ Equal contribution. † Corresponding author.

2

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Fig. 1. Illustration of the proposed GPNN for learning HOI. GPNN oﬀers a
generic HOI representation that applies to (a) HOI detection in images and (b) HOI
recognition in videos. With the integration of graphical model and neural network,
GPNN can iteratively learn/infer the graph structures (a.v) and message passing (a.vi).
The ﬁnal parse graph explains a given scene with the graph structure (e.g., the link
between the person and the knife) and the node labels (e.g., lick). A thicker edge
corresponds to stronger information ﬂow between nodes in the graph.

In this paper, we propose a novel model, Graph Parsing Neural Network
(GPNN), for HOI recognition. The proposed GPNN oﬀers a general framework
that explicitly represents HOI structures with graphs and automatically parses
the optimal graph structures in an end-to-end manner. In principle, it is an
generalization of Message Passing Neural Network (MPNN) [12]. An overview
of GPNN is shown in Fig. 1. The following two aspects motivate our design.

First, we seek a uniﬁed framework that utilizes the learning capability of neu-
ral networks and the power of graphical representations. Recent deep learning
based HOI models showed promising results, but few touched how to interpret
well and explicitly leverage spatial and temporal dependencies and human-object
relations in such structured task. Aiming for this, we introduce GPNN. It in-
herits the complementary strengths of neural networks and graphical models,
for forming a coherent HOI representation with strong learning ability. Specif-
ically, with the structured representation of an HOI graph, the rich relations
are explicitly utilized, and the information from individual elements can be ef-
ﬁciently integrated and broadcasted over the structures. The whole model and
message passing operations are well-deﬁned and fully diﬀerentiable. Thus it can
be eﬃciently learned from data in an end-to-end manner.

Second, based on our eﬃcient HOI representation and learning power, GPNN
applies to diverse HOI tasks in both static and dynamic scenes. Previous studies
for HOI achieved good performance in their speciﬁc domains (spatial [1, 14] or
temporal [20, 34, 35]). However, none of them addresses a generic framework for
representing and learning HOI in both images and videos. The key diﬃculty lies
in the diverse relations between components. Given a set of human and objects
candidates, there may exist an uncertain number of human-object interaction
pairs (see Fig. 1 (a.ii) as an example). The relations become more complex after

Graph Parsing Neural Networks (ECCV 2018)

3

taking temporal factors into consideration. Thus pre-ﬁxed graph structures, as
adopted by most previous graphical or structured DNN models [11, 20, 22, 43],
are not an optimal choice. Seeking a better generalization ability, GPNN incor-
porates an essential link function for addressing the problem of graph structure
learning. It learns to infer the adjacency matrix in an end-to-end manner and
thus can infer a parse graph that explicitly explains the HOI relations. With such
learnable graph structure, GPNN could also limit the information ﬂow from ir-
relevant nodes while encouraging message to propagate between related nodes,
thus improving graph parsing.

We extensively evaluate the proposed GPNN on three HOI datasets, namely
HICO-DET [1], V-COCO [17] and CAD-120 [22], for HOI detection from im-
ages (HICO-DET, V-COCO) and HOI recognition and anticipation in spatial-
temporal settings (CAD-120). The experimental results verify the generality
and scalability of our GPNN based HOI representation and show substantial
improvements over state-of-the-art approaches, including pure graphical models
and pure neural networks. We also demonstrate GPNN outperforms its variants
and other graph neural networks with pre-ﬁxed structures.

This paper makes three major contributions. First, we propose the GPNN
that incorporates structural knowledge and DNNs for learning and inference.
Second, with a set of well deﬁned modular functions, GPNN addresses the HOI
problem by jointly performing graph structure inference and message passing.
Third, we empirically show that GPNN oﬀers a scalable and generic HOI rep-
resentation that applies to both static and dynamic settings.

2 Related Work

Human-Object Interaction. Reasoning human actions with objects (like “play-
ing baseball”, “playing guitar”), rather than recognizing individual actions (“play-
ing”) or object instances (“baseball”, “guitar”), is essential for a more compre-
hensive understanding of what is happening in the scene. Early work in HOI
understanding studied Bayesian model [15, 16], utilized contextual relationship
between human and objects [47–49], learned structured representations with spa-
tial interaction and context [8], exploited compositional models [9], or referred
to a set of HOI exemplars [19]. They were mainly based on handcrafted features
(e.g., color, HOG, and SIFT) with object and human detectors. More recently,
inspired by the notable success of deep learning and the availability of large-scale
HOI datasets [1,2], several deep learning based HOI models were then proposed.
Speciﬁcally, Mallya et al. [29] modiﬁed Fast RCNN model [13] for HOI recogni-
tion, with the assistance of Visual Question Answering (VQA). In [38], zero-shot
learning was applied for addressing the long-tail problem in HOI recognition.
In [1], the human proposals, object regions, and their combinations were fed
into a multi-stream network for tackling the HOI detection problem. Gkioxari
et al. [14] estimated an action-type speciﬁc density map for identifying the in-
teracted object locations, with a modiﬁed Faster RCNN architecture [36].

4

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Although promising results were achieved by above deep HOI models, we
still observe two unsolved issues. First, they lack a powerful tool to represent
the structures in HOI tasks explicitly and encodes them into modern network ar-
chitectures eﬃciently. Second, despite the successes in speciﬁc tasks, a complete
and generic HOI representation is missing. These approaches can not be easily
extended to HOI recognition from videos. Aiming to address those issues, we in-
troduce GPNN for imposing high-level relations into DNN, leading to a powerful
HOI representation that is applicable in both static and dynamic settings.
Neural Networks with Graphs/Graphical Models. In the literature, some
approaches were proposed to combine graphical models and neural networks.
The most intuitive approach is to build graphical models upon DNN, where
the network that generates features is trained ﬁrst, and its output is used to
compute potential functions for the graphical predictor. Typical methods were
used in human pose estimation [42], human part parsing [33, 45], and semantic
image segmentation [3, 4]. These methods lack a deep integration in the sense
that the computation process of graphical models cannot be learned end-to-
end. Some attempts [7, 21, 31, 32, 37, 40, 44, 51] were made to generalize neural
network operations (e.g., convolutions) directly from regular grids (e.g., images)
to graphs. For the HOI problem, however, a structured representation is needed
to capture the high-level spatial-temporal relations between humans and objects.
Some other work integrated network architectures with graphical models [12,20]
and gained promising results on applications such as scene understanding [24,30,
46], object detection and parsing [27,50], and VQA [41]. However, these methods
only apply to problems that have pre-ﬁxed graph structures. Liang et al. [26]
merged graph nodes using Long Short-Term Memory (LSTM) for human parsing
problem, under the assumption that the nodes are mergeable.

Those methods achieved promising results in their speciﬁc tasks and well
demonstrated the beneﬁt in completing deep architectures with domain-speciﬁc
structures. However, most of them are based on pre-ﬁxed graph structures, and
they have not yet been studied in HOI recognition. In this work, we extend pre-
vious graphical neural networks with learnable graph structures, which well ad-
dresses the rich and high-level relations in HOI problems. The proposed GPNN
can automatically infer the graph structure and utilize that structure for en-
hancing information propagation and further inference. It oﬀers a generic HOI
representation for both spatial and spatial-temporal settings. To the best of our
knowledge, this is a ﬁrst attempt to integrate graph models with neural networks
in a uniﬁed framework to achieve state-of-art results in HOI recognition.

3 Graph Parsing Neural Network for HOI

3.1 Formulation

For HOI understanding, human and objects are represented by nodes, and their
relations are deﬁned as edges. Given a complete HOI graph that includes all the
possible relationships among human and objects, we want to automatically infer
a parse graph by keeping the meaningful edges and labeling the nodes.

Graph Parsing Neural Networks (ECCV 2018)

5

Fig. 2. Illustration of the forward pass of GPNN. GPNN takes node and edge
features as input, and outputs a parse graph in a message passing fasion. The struc-
ture of the parse graph is given by a soft adjacency matrix. It is computed by the link
function based on the features (or hidden node states). The darker the color in the
adjacency matrix, the stronger the connectivity is. Then message functions compute
incoming messages for each node as a weighted sum of the messages from other nodes.
Thicker edges indicate larger information ﬂows. The update functions update the hid-
den internal states of each node. Above process is repeated for several steps, iteratively
and jointly learning the computation of graph structures and message passing. Finally,
for each node, the readout functions output HOI action or object labels from the hidden
node states. See § 3 for more details.

Formally, let G = (V, E, Y) denote the complete HOI graph. Nodes v ∈ V take
unique values from {1, · · · , |V|}. Edges e ∈ E are two-tuples e = (v, w) ∈ V × V.
Each node v has a output state yv ∈ Y that takes a value from a set of labels
{1, · · · , Yv} (e.g., actions). A parse graph g = (Vg, Eg, Yg) is a sub-graph of G,
where Vg ⊆ V and Eg ⊆ E. Given the node features Γ V and edge features Γ E , we
want to infer the optimal parse graph g∗ that best explains the data according
to a probability distribution p:

g∗ = argmax

p(g|Γ, G) = argmax

p(Vg, Eg, Yg|Γ, G)

g

= argmax

p(Yg|Vg, Eg, Γ )p(Vg, Eg|Γ, G)

g

g

(1)

where Γ = {Γ V , Γ E }. Here p(Vg, Eg|Γ, G) evaluates the graph structure, and
p(Yg|Vg, Eg, Γ ) is the labeling probability for the nodes in the parse graph.

This formulation provides us a principled guideline for designing the GPNN.
We design the network to approximate the computations of argmaxg p(Vg, Eg|Γ, G)
and argmaxg p(Yg|Vg, Eg, Γ ). We introduce four types of functions as individual
modules in the forward pass of a GPNN: link functions, message functions, up-
date functions, and readout functions (as illustrated in Fig. 2). The link functions
L(·) estimate the graph structure, giving an approximation of p(Vg, Eg|Γ, G). The
message, update and readout functions together resemble the belief propagation
process and approximate argmaxYg p(Yg|Vg, Eg, Γ ).

6

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Speciﬁcally, the link function (

) takes edge features ( ) as input and infers
the connectivities between nodes. The soft adjacency matrix ( ) is thus con-
structed and used as weights for messages passing through edges between nodes.
),
The incoming messages for a node are summarized by the message function (
then the hidden embedding state of the node is updated based on the messages
by an update function ( ). Finally, readout functions ( ) compute the target
outputs for each nodes. Those four types of functions are deﬁned as follows:
Link Function. We ﬁrst infer an adjacency matrix that represents connectivities
(i.e., the graph structure) between nodes by a link function. A link function L(·)
takes the node features Γ V , and edge features Γ E as input and outputs an
adjacency matrix A ∈ [0, 1]|V|×|V|:

Avw = L(Γv, Γw, Γvw)

(2)

where Avw denotes the (v, w)-th entry of the matrix A. Here we overload the
notation and let Γv denote node features and Γvw denote edge features. In this
way, the structure of a parse graph g can be approximated by the adjacency
matrix. Then we start to propagate messages over the parse graph, where the
soft adjacency matrix controls the information to be passed through edges.
Message and Update Functions. Based on the learned graph structure, the
message passing algorithm is adopted for inference of node labels. During belief
propagation, the hidden states of the nodes are iteratively updated by communi-
cating with other nodes. Specially, message functions M (·) summarize messages
to nodes coming from other nodes, and update functions U (·) update the hidden
node states according to the incoming messages. At each iteration step s, the
two functions computes:

ms

v =

(cid:88)

w

AvwM (hs−1

v

, hs−1

w , Γvw)

v = U (hs−1
hs

v

, ms
v)

where ms
v is the summarized incoming message for node v at s-th iteration
and hs
v is the hidden state for node v. The node connectivity A encourages the
information ﬂow between nodes in the parse graph. The message passing phase
runs for S steps towards convergence. At the ﬁrst step, the node hidden states
h0
v are initialized by node features Γv.
Readout Function. Finally, for each node, hidden state is fed into a readout
function to output a label:

yv = R(hS

v ).

v (node embeddings).

Here the readout function R(·) computes output yv for node v by activating its
hidden state hS
Iterative Parsing. Based on the above four functions, the messages are passed
along the graph and weighted by the learned adjacency matrix A. We further
extend above process into a joint learning framework that iteratively infers the
graph structure and propagates the information to infer node labels. In particu-
lar, instead of learning A only at the beginning, we iteratively infer A with the

(3)

(4)

(5)

Graph Parsing Neural Networks (ECCV 2018)

7

updated node information and edge features at each step s:

As

vw = L(hs−1

v

, hs−1

w , ms−1

vw ).

Then the messages in Eq. 3 are redeﬁned as:

ms

v =

(cid:88)

w

As

vwM (hs−1

v

, hs−1

w , Γvw).

(6)

(7)

In this way, both the graph structure and the message update can be jointly and
iteratively learned in a uniﬁed framework. In practice, we ﬁnd such a strategy
would bring better performance (detailed in § 4.3).

In next section, we show that by implementing each function by neural net-
works, the entire system is diﬀerentiable end-to-end. Hence all the parameters
can be learned using gradient-based optimization.

3.2 Network Architecture

Link Function. Given the complete HOI graph G = (V, E, Y), we use dV and dE
to denote the dimension of the node features and the edge features, respectively.
In a message passing step s, we ﬁrst concatenate all the node features (hidden
vw ∈ RdE }v,w to
states) {hs
form a feature matrix F s ∈ R|V |×|V |×(2dV +dE ) (see
in Fig. 2). The link function
is deﬁned as a small neural network with one or several convolutional layer(s)
(with 1 × 1 × (2dV + dE) kernels) and a sigmoid activation. Then the adjacency
matrix As ∈ [0, 1]|V|×|V| can be computed as:

v ∈ RdV }v and all the edge features (messages) {ms

As = σ(WL ∗ F s),

(8)

where WL is the learnable parameters of the link function network L(·) and ∗
denotes conv operation. The sigmoid operation σ(·) is for normalizing the values
of the elements of As into [0, 1]. The essential eﬀect of multiple convolutional
layers with 1 × 1 kernels is similar to fully connected layers applied to each
individual edge features, except that the ﬁlter weights are shared by all the
edges. In practice, we ﬁnd such operation generates good enough results and
leads to a high computation eﬃciency.

For spatial-temporal problems where the adjacency matrices should account
for the previous states, we use convolutional LSTMs [39] for modeling L(·) in
temporal domain. At time t, the link function takes F s,t as input features and the
previous adjacency matrix As,t−1 as hidden state: As,t = convLST M (F s,t, As,t−1).
Again, the kernel size for the conv layer in convLSTM is 1 × 1 × (2dV + dE).
Message Function. In our implementation, the message function M (·) in Eq. 3
is computed by:

M (hv, hw, Γvw) = [WM

V hv, WM

V hw, WM

E Γvw],

(9)

where [., .] denotes concatenation. It concatenates the outputs of linear trans-
forms (i.e., fully connected layers parametrized by WM
E ) that takes
node hidden states hv or edge features Γvw as input.

V and WM

8

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Update Function. Recurrent neural networks [10, 18] are natural choices for
simulating the iterative update process, as done by previous works [12]. Here we
apply Gated Recurrent Unit (GRU) [5] as the update function, because of its
recurrent nature and smaller amount of parameters. Thus the update function
in Eq. 4 is implemented as:

v = U (hs−1
hs

v

, ms

v) = GRU (hs−1

v

, ms

v),

(10)

v is the hidden state and ms

where hs
in [25], the GRU is more eﬀective than vanilla recurrent neural networks.
Readout Function. A typical implementation of readout functions is com-
bining several fully connected layers (parameterized by WR) followed by an
activation function:

v is used as input features. As demonstrated

yv = R(hS

v ) = ϕ(WRhS

v ).

(11)

Here the activation function ϕ(·) can be used as softmax (one-class outputs) or
sigmoid (multi-class outputs) according to diﬀerent HOI tasks.

In this way, the entire GPNN is implemented to be fully diﬀerentiable and
end-to-end trainable. The loss for speciﬁc HOI task can be computed for the
outputs of readout functions, and the error can propagate back according to
chain rule. In next section, we will oﬀer more details for implementing GPNN
for HOI tasks on spatial and spatial-temporal settings and present qualitative
as well as quantitative results.

4 Experiments

To verify the eﬀectiveness and generic applicability of GPNN, we perform ex-
periments on two HOI problems: i) HOI detection in images [1, 17], and ii) HOI
recognition and anticipation from videos [22]. The ﬁrst experiment is performed
on HICO-DET [1] and V-COCO [17] datasets, showing that our approach is
scalable to large datasets (about 60K images in total) and achieves a good de-
tection accuracy over a large number of classes (more than 600 classes of HOIs).
The second experiment is reported on CAD-120 dataset [22], showing that our
method is well applicable to spatial-temporal domains.

4.1 Human-Object Interaction Detection in Images

For HOI detection in an image, the goal is to detect pairs of a human and an
object bounding box with an interaction class label connecting them.
Datasets. We use HICO-DET [1] and V-COCO [17] datasets for benchmarking
our GPNN model. HICO-DET provides more than 150K annotated instances
of human-object pairs in 47,051 images (37,536 training and 9,515 testing). It
has the same 80 object categories as MS-COCO [28] and 117 action categories.
V-COCO is a subset of MS-COCO [28]. It consists of a total of 10,346 images
with 16,199 people instances, where ∼2.5K images in the train set, ∼2.8K images
for validation and ∼4.9K images for testing. Each annotated person has binary

Graph Parsing Neural Networks (ECCV 2018)

9

Table 1. HOI detection results (mAP) on HICO-DET dataset [1]. Higher
values are better. The best scores are marked in bold.

Methods

Full (mAP %) ↑ Rare (mAP %) ↑ Non-rare (mAP %) ↑

Random
Fast-RCNN(union) [13]
Fast-RCNN(score) [13]
HO-RCNN [1]
HO-RCNN+IP [1]
HO-RCNN+IP+S [1]
Gupta et al. [17]
Shen et al. [38]
InteractNet [14]
GPNN
Performance Gain(%)

1.35 × 10−3
1.75
2.85
5.73
7.30
7.81
9.09
6.46
9.94
13.11
31.89

5.72 × 10−4
0.58
1.55
3.21
4.68
5.37
7.02
4.24
7.16
9.34
30.45

1.62 × 10−3
2.10
3.23
6.48
8.08
8.54
9.71
7.12
10.77
14.23
32.13

Fig. 3. HOI detection results on HICO-DET [1] test images. Human and ob-
jects are shown in red and green rectangles, respectively. Best viewed in color.

labels for 26 diﬀerent action classes. Note that three actions (i.e., cut, eat, and
hit) are annotated with two types of targets: instrument and direct object.
Implementation Details. Humans and objects are represented by nodes in
the graph, while human-object interactions are represented by edges. In this ex-
periment, we use a pre-trained deformable convolutional network [6] for object
detection and features extraction. Based on the detected bounding boxes, we ex-
tract node features (7 × 7 × 80) from the position-sensitive region of interest (PS
RoI) pooling layer from the deformable ConvNet. We extract the edge feature
from a combined bounding box, i.e., the smallest bounding box that contains
both two nodes’ bounding boxes. The functions of GPNN are implemented as
follows. We use a convolutional network (128-128-1)-Sigmoid(·) with 1×1 kernels
for the link function. The message functions are composed of a fully connected
layer, concatenation, and summation. For a node v, the neighboring node feature
Γw and edge feature Γvw are passed through a fully connected layer and concate-
nated. The ﬁnal incoming message is a weighted sum of messages from all neigh-
boring nodes. Speciﬁcally, the message for node v coming from node w through
edge e = (v, w) is the concatenation of output from FC(dV -dV ) and FC(dE-dE).
A GRU(dV ) is used for the update function. The propagation step number S

10

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 2. HOI detection results (mAP) on V-COCO [17] dataset. Legend: Set
1 indicates 18 HOI actions with one object, and Set 2 corresponds to 3 HOI actions
(i.e., cut, eat, hit) with two objects (instrument and object).

Method

Set 1 (mAP %) ↑

Set 2 (mAP %) ↑ Ave. (mAP %) ↑

Gupta et al. [17]
InteractNet [14]
GPNN
Performance Gain(%)

33.5
42.2
44.5
5.5

26.7
33.2
42.8
28.9

31.8
40.0
44.0
10.0

Fig. 4. HOI detection results on V-COCO [17] test images. Human and objects
are shown in red and green rectangles, respectively. Best viewed in color.

is set to be 3. For the readout function, we use a FC(dV -117)-Sigmoid(·) and
FC(dV -26)-Sigmoid(·) for HICO-DET and V-COCO, respectively.

The probability of an HOI label of a human-object pair is given by the
product of the ﬁnal output probabilities from the human node and the object
node. We employ an L1 loss for the adjacency matrix. For the node outputs, we
use a weighted multi-class multi-label hinge loss. The reasons are two-folds: the
training examples are not balanced, and it is essentially a multi-label problem
for each node (there might not even exist a meaningful human-object interaction
for detected humans and objects).

Our model is implemented using PyTorch and trained with a machine with
a single Nvidia Titan Xp GPU. We start with a learning rate of 1e-3, and the
rate decays every 5 epochs by 0.8. The training process takes about 20 epochs
(∼15 hours) to roughly converge with a batch size of 32.
Comparative Methods. We compare our method with eight baselines: (1)
Fast-RCNN (union) [13]: for each human-object proposal from detection results,
their attention windows are used as the region proposal for Fast-RCNN. (2) Fast-
RCNN (score) [13]: given human-object proposals, HOI is predicted by linearly
combining the human and object detection scores. (3) HO-RCNN [1]: a multi-
stream architecture with a ConvNet to classify human, object and human-object
proposals, respectively. The ﬁnal output is computed by combining the scores
from all the three streams. (4) HO-RCNN+IP [1] and (5) HO-RCNN+IP+S [1]:
HO-RCNN with additional components. Interaction Patterns (IP) acts as a at-
tention ﬁlter to images. S is an extra path with a single neuron that uses the

Graph Parsing Neural Networks (ECCV 2018)

11

raw object detection score to produce an oﬀset for the ﬁnal detection. More
detailed descriptions of above ﬁve baselines can be found in [1]. (6) Gupta et
al. [17]: trained based on Fast-RCNN [13]. We use the scores reported in [14].
(7) Shen et al. [38]: ﬁnal predictions are from two Faster RCNN [36] based net-
works which are trained for predicting verb and object classes, respectively. (8)
InteractNet [14]: a modiﬁed Faster RCNN [36] with an additional human-centric
branch that estimates an action-speciﬁc density map for locating objects.
Experiment Results. Following the standard settings in HICO-DET and V-
COCO benchmarks, we evaluate HOI detection using mean average precision
(mAP). An HOI detection is considered as a true positive when the human
detection, the object detection, and the interaction class are all correct. The
human and object bounding boxes are considered as true positives if they overlap
with a ground truth bounding boxes of the same class with an intersection over
union (IoU) greater than 0.5. For HICO-DET dataset, we report the mAP over
three diﬀerent HOI category sets: i) all 600 HOI categories in HICO (Full); ii)
138 HOI categories with less than 10 training instances (Rare); and iii) 462 HOI
categories with 10 or more training instances (Non-Rare). For V-COCO dataset,
since we concentrate on HOI detection, we report the mAP on three groups: i)
18 HOI action classes with one target object; ii) 3 HOI categories with two types
of objects; iii) all 24 (=18 + 3 × 2) HOI classes. Results are evaluated on the test
sets and reported in Table 1 and Table 2.

As shown in Table 1, the proposed GPNN substantially outperforms the
comparative methods, achieving 31.89%, 30.45%, and 32.13% improvement
over the second best methods on the three HOI category sets on the HICO-
DET dataset. The results on V-COCO dataset (in Table 2) also consistently
demonstrate the superior performance of the proposed GPNN. Two important
conclusions can be drawn from the results: i) our method is scalable to large
datasets; ii) and our method performs better than pure neural network. Some
visual results can be found in Fig. 3 and Fig. 4.

4.2 Human-Object Interaction Recognition in Videos

The goal of this experiment is to detect and predict the human sub-activity
labels and object aﬀordance labels as the human-object interaction progresses
in videos. The problem is challenging since it involves complex interactions that
humans make with multiple objects, and objects also interact with each other.
CAD-120 dataset [22]. It has 120 RGB-D videos of 4 subjects performing 10
activities, each of which is a sequence of sub-activities involving 10 actions (e.g.,
reaching, opening), and 12 object aﬀordances (e.g., reachable, openable) in total.
Implementation Details. The link function is implemented as: convLSTM(1024-
1024-1024-1)-Sigmoid(·) (i.e., a four-layer convLSTM). We use the same ar-
chitecture as the previous experiment for message functions and update func-
tions: [FC(dV -dV ), FC(dE-dE)] for message function and GRU(dV ) for update
function. The propagation step number S is set to be 3. We use a FC(dV -10)-
Softmax(·) and a FC(dV -12)-Softmax(·) for readout functions of sub-activity and
object aﬀordance detection/anticipation, respectively. We employ an L1 loss for

12

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 3. Human activity detection and future anticipation results on CAD-
120 [22] dataset, measured via F1-score.

Method

Detection (F1-score) ↑
Object
Sub
Aﬀordance(%)
-activity(%)

Anticipation (F1-score) ↑

Sub
-activity(%)

Object
Aﬀordance(%)

ATCRF [22]
S-RNN [20]
S-RNN (multi-task) [20]
GPNN
Performance Gain(%)

80.4
83.2
82.4
88.9
8.1

81.5
88.7
91.1
88.8
-

37.9
62.3
65.6
75.6
15.2

36.7
80.7
80.9
81.9
1.2

Fig. 5. Confusion matrices of HOI detection (a)(b) and anticipation (c)(d)
results on CAD-120 [22] dataset. Zoom in for more details.

the adjacency matrix and a cross entropy loss for the node outputs. We use the
publicly available node and edge features from [23].
Comparative Methods. We compare our method with two baselines: antici-
patory temporal CRF (ATCRF) [22] and structural RNN (S-RNN) [20]. ATCRF
is a top-performing graphical model approach for this problem, while S-RNN is
the state-of-art method using structured neural networks. ATCRF models the
human activities through a spatial-temporal conditional random ﬁeld. S-RNN
casts a pre-deﬁned spatial-temporal graph as an RNN mixture by representing
nodes and edges as LSTMs.
Experiment Results. In Table 3 we show the quantitative comparison of
our method with other competitors. It shows the F1-scores averaged over all
classes on detection and activity anticipation tasks. GPNN greatly improves
over ATCRF and S-RNN, especially on anticipation task. Our method outper-
forms the other two for the following reasons. i) Comparing to ATCRF limited
to the Markov assumption, our method allows arbitrary graph structures with
improved representation ability. ii) Our method enjoys the beneﬁt of deep in-
tegration of graphical models and neural networks and can be learned in an
end-to-end manner. iii) Rather than relying on a pre-ﬁxed graph structure as in
S-RNN, we infer the graph structure via learning an adjacency matrix and thus
be able to control the information ﬂow between nodes during massage passing.
Fig. 5 show the confusion matrices for detecting and predicting the sub-activities
and object aﬀordances, respectively. From above results we can draw two im-

Graph Parsing Neural Networks (ECCV 2018)

13

Fig. 6. HOI detection results on a “cleaning objects” activity on CAD-120 [22]
dataset. Human are shown in red rectangle. Two objects are shown in green and blue
rectangles, respectively. Detection and anticipation results are shown by diﬀerent bars.
For anticipation task, the label of the sub-activity at time t is anticipated at time t-1.

portant conclusions: i) our method is well applicable to the spatio-temporal
domain; and ii) our method outperforms pure graphical models (e.g., ATCRF)
and deep networks with pre-ﬁxed graph structures (e.g., S-RNN). Fig. 6 shows a
qualitative visualization of “cleaning objects”. We show one representative frame
for each sub-activity as well as the corresponding detections and anticipations.

4.3 Ablation Study

In this section, we analyze the contributions of diﬀerent model components to
the ﬁnal performance and examine the eﬀectiveness of our main assumptions.
Table 4 shows the detailed results on all three datasets.
Integration of DNN with Graphical Model. We ﬁrst examine the inﬂu-
ence of integrating DNN with a graphical model. We directly feed the features,
which are originally used for GPNN, into diﬀerent fully connected networks for
predicting HOI action or object classes. From Table 4, we can observe the per-
formance of w/o graph is signiﬁcantly worse than GPNN model over various
HOI datasets. This supports our view that modeling high-level structures and
leveraging learning capabilities of DNNs together is essential for HOI tasks.
GPNN with Fixed Graph Structures. In § 3, GPNN automatically infers
graph structures (i.e., parse graph) via learning a soft adjacency matrix. To
assess this strategy, we ﬁx all the entries in the soft adjacency matrices to be
constant 1. This way the graph structures are ﬁxed and the information ﬂow
between nodes are not weighted. For constant graph baseline, we see obvious
performance decrease, compared with the full GPNN model. This indicates that
inferring graph structures is critical to get reasonable performance.
GPNN without Supervision on Link Functions. We perform experiments
by turning oﬀ the L1 loss on adjacency matrices (w/o graph loss in Table 4). We
can observe that the intermediate L1 loss is eﬀective, further verifying our design
to learn the graph structure. Another interesting observation is that training

14

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 4. Ablation study of GPNN model. Higher values are better.

Aspect Method

Set 1 Set 2 Ave. Full Rare

V-COCO [17]
HOI Detection
mAP(%) ↑

HICO-DET [1]
HOI Detection
mAP(%) ↑

CAD-120 [22]

HOI Detec.
F1-score(%) ↑
Object
Sub-
Aﬀ.(%)
activity

HOI Antici.
F1-score(%) ↑
Object
Sub-
Aﬀ.(%)
activity

Non-
rare

44.5 42.8 44.0 13.11 9.34 14.23 88.9

88.8

75.6

81.9

GPNN
(3 iterations)

w/o graph
graph
constant graph
structure w/o graph loss

2.04
9.62
27.4 30.0 28.1
1.94 10.79
34.6 33.3 34.3
8.72
6.24
37.7 40.5 38.4
w/o joint parsing 43.6 39.4 42.5 10.17 5.81 11.47
42.0 40.7 41.7 11.38 7.27 12.61
44.1 42.2 43.6 12.37 9.01 13.38
43.6 40.9 42.9 12.39 8.95 13.41

7.88
8.75
8.15

iterative 1 iteration
learning 2 iterations
4 iterations

50.2
85.3
85.2
79.3
80.5
87.9
87.9

20.8
85.6
85.8
79.2
80.7
86.1
85.7

32.3
73.8
74.7
74.7
75.2
76.1
75.5

19.6
79.1
79.2
80.3
81.1
81.5
80.6

the model without this loss has a similar eﬀect to training with constant graph.
Hence supervision on the graph is fairly important.
Jointly Learning Parse Graph and Message Passing. We next study the
eﬀect of jointly learning graph structures and message passing. By isolating graph
parsing from message passing, we obtain w/o joint parsing, where the adjacency
matrices are directly computed by link functions from edge features at the be-
ginning. We observe a performance decrease in Table 4, showing that learning
graph structures and message passing together indeed boost the performance.
Iterative Learning Process. Next we examine the eﬀect of iterative message
passing, we report three baselines: 1 iteration, 2 iterations, and 4 iterations,
which correspond to the results from diﬀerent message passing iterations. The
baseline GPNN (ﬁrst row in Table 4) are the results after three iterations. From
the results we observe that the iterative learning process is able to gradually
improve the performance in general. We also observe that when the iteration
round is increased to a certain extent, the performance drops slightly.

5 Conclusion

In this paper, we propose Graph Parsing Neural Network (GPNN) for inferring
a parse graph in an end-to-end manner. The network can be decomposed into
four distinct functions, namely link functions, message functions, update func-
tions and readout functions, for iterative graph inference and message passing.
GPNN provides a generic HOI representation that is applicable in both spa-
tial and spatial-temporal domains. We demonstrate a substantial performance
gain on three HOI datasets, showing the eﬀectiveness of the proposed framework.

Acknowledgments. The authors thank Prof. Ying Nian Wu from UCLA Statis-
tics Department for helpful comments on this work. This research is supported by
DARPA XAI N66001-17-2-4029, ONR MURI N00014-16-1-2007, ARO W911NF1810296,
and N66001-17-2-3602.

Graph Parsing Neural Networks (ECCV 2018)

15

References

interactions (2018)

1. Chao, Y.W., Liu, Y., Liu, X., Zeng, H., Deng, J.: Learning to detect human-object

2. Chao, Y.W., Wang, Z., He, Y., Wang, J., Deng, J.: HICO: A benchmark for rec-

ognizing human-object interactions in images. In: ICCV (2015)

3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Se-
mantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected CRFs. PAMI (2016)

4. Chen, L.C., Schwing, A., Yuille, A., Urtasun, R.: Learning deep structured models.

In: ICML (2015)

5. Cho, K., Van Merri¨enboer, B., Bahdanau, D., Bengio, Y.: On the properties of
neural machine translation: Encoder–decoder approaches. Syntax, Semantics and
Structure in Statistical Translation p. 103 (2014)

6. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convo-

lutional networks. In: ICCV (2017)

7. Deﬀerrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on

graphs with fast localized spectral ﬁltering. In: NIPS (2016)

8. Delaitre, V., Sivic, J., Laptev, I.: Learning person-object interactions for action

recognition in still images. In: NIPS (2011)

9. Desai, C., Ramanan, D.: Detecting actions, poses, and objects with relational

phraselets. In: ECCV (2012)

10. Elman, J.L.: Finding structure in time. Cognitive science (1990)
11. Fang, H.S., Xu, Y., Wang, W., Zhu, S.C.: Learning pose grammar to encode human

body conﬁguration for 3d pose estimation. In: AAAI (2018)

12. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message

passing for quantum chemistry. In: ICML (2017)

13. Girshick, R.: Fast R-CNN. In: ICCV (2015)
14. Gkioxari, G., Girshick, R., Doll´ar, P., He, K.: Detecting and recognizing human-

object interactions. In: CVPR (2018)

15. Gupta, A., Davis, L.S.: Objects in action: An approach for combining action un-

derstanding and object perception. In: CVPR (2007)

16. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using

spatial and functional compatibility for recognition. PAMI (2009)

17. Gupta, S., Malik, J.: Visual semantic role labeling. arXiv preprint arXiv:1505.04474

18. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

(2015)

(1997)

19. Hu, J.F., Zheng, W.S., Lai, J., Gong, S., Xiang, T.: Recognising human-object

interaction via exemplar based modelling. In: ICCV (2013)

20. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: Deep learning on

spatio-temporal graphs. In: CVPR (2016)

21. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional

networks. In: ICLR (2017)

22. Koppula, H.S., Saxena, A.: Anticipating human activities using object aﬀordances

for reactive robotic response. PAMI (2016)

23. Koppula, H.S., Gupta, R., Saxena, A.: Learning human activities and object af-
fordances from RGB-D videos. The International Journal of Robotics Research
(2013)

16

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

24. Li, R., Tapaswi, M., Liao, R., Jia, J., Urtasun, R., Fidler, S.: Situation recognition

with graph neural networks. In: ICCV (2017)

25. Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R.: Gated graph sequence neural

26. Liang, X., Lin, L., Shen, X., Feng, J., Yan, S., Xing, E.P.: Interpretable structure-

27. Liang, X., Shen, X., Feng, J., Lin, L., Yan, S.: Semantic object parsing with graph

networks. In: ICLR (2016)

evolving lstm. In: ICCV (2017)

lstm. In: ECCV (2016)

28. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)
29. Mallya, A., Lazebnik, S.: Learning models for actions and person-object interac-

tions with transfer to question answering. In: ECCV (2016)

30. Marino, K., Salakhutdinov, R., Gupta, A.: The more you know: Using knowledge

graphs for image classiﬁcation. In: CVPR (2016)

31. Monti, F., Boscaini, D., Masci, J., Rodol`a, E., Svoboda, J., Bronstein, M.M.: Ge-
ometric deep learning on graphs and manifolds using mixture model cnns. CVPR
(2016)

32. Niepert, M., Ahmed, M., Kutzkov, K.: Learning convolutional neural networks for

graphs. In: ICML (2016)

33. Park, S., Nie, X., Zhu, S.C.: Attribute and-or grammar for joint parsing of human

pose, parts and attributes. PAMI (2017)

34. Qi, S., Huang, S., Wei, P., Zhu, S.C.: Predicting human activities using stochastic

grammar. In: ICCV (2017)

35. Qi, S., Jia, B., Zhu, S.C.: Generalized earley parser: Bridging symbolic grammars

and sequence data for future prediction. In: ICML (2018)

36. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

37. Seo, Y., Deﬀerrard, M., Vandergheynst, P., Bresson, X.: Structured sequence mod-
eling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659
(2016)

38. Shen, L., Yeung, S., Hoﬀman, J., Mori, G., Fei-Fei, L.: Scaling human-object in-

teraction recognition through zero-shot learning (2018)

39. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convolutional
LSTM network: A machine learning approach for precipitation nowcasting. In:
NIPS (2015)

40. Simonovsky, M., Komodakis, N.: Dynamic edge-conditioned ﬁlters in convolutional

neural networks on graphs. CVPR (2017)

41. Teney, D., Liu, L., Hengel, A.v.d.: Graph-structured representations for visual ques-

tion answering. In: CVPR (2017)

42. Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint training of a convolutional
network and a graphical model for human pose estimation. In: NIPS (2014)
43. Wang, W., Xu, Y., Shen, J., Zhu, S.C.: Attentive fashion grammar network for
fashion landmark detection and clothing category classiﬁcation. In: CVPR (2018)
44. Wu, Z., Lin, D., Tang, X.: Deep markov random ﬁeld for image modeling. In:

ECCV (2016)

45. Xia, F., Zhu, J., Wang, P., Yuille, A.L.: Pose-guided human parsing by an And/Or

graph using pose-context features. In: AAAI (2016)

46. Xu, D., Zhu, Y., Choy, C.B., Fei-Fei, L.: Scene graph generation by iterative mes-

sage passing. In: ICCV (2017)

47. Yao, B., Fei-Fei, L.: Grouplet: A structured image representation for recognizing

human and object interactions. In: CVPR (2010)

Graph Parsing Neural Networks (ECCV 2018)

17

48. Yao, B., Fei-Fei, L.: Modeling mutual context of object and human pose in human-

object interaction activities. In: CVPR (2010)

49. Yao, B., Jiang, X., Khosla, A., Lin, A.L., Guibas, L., Fei-Fei, L.: Human action
recognition by learning bases of action attributes and parts. In: ICCV (2011)
50. Yuan, Y., Liang, X., Wang, X., Yeung, D.Y., Gupta, A.: Temporal dynamic graph

LSTM for action-driven video object detection. In: ICCV (2017)

51. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: ICCV
(2015)

8
1
0
2
 
g
u
A
 
3
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
6
9
7
0
.
8
0
8
1
:
v
i
X
r
a

Learning Human-Object Interactions by
Graph Parsing Neural Networks

Siyuan Qi∗1,2, Wenguan Wang∗1,3, Baoxiong Jia1,4,
Jianbing Shen†3,5, and Song-Chun Zhu1,2

1 University of California, Los Angeles
2 International Center for AI and Robot Autonomy (CARA)
3 Beijing Institute of Technology
4 Peking University
5 Inception Institute of Artiﬁcial Intelligence

syqi@cs.ucla.edu

wenguanwang.ai@gmail.com baoxiongjia@ucla.edu

shenjianbing@bit.edu.cn

sczhu@stat.ucla.edu

Abstract. This paper addresses the task of detecting and recognizing
human-object interactions (HOI) in images and videos. We introduce the
Graph Parsing Neural Network (GPNN), a framework that incorporates
structural knowledge while being diﬀerentiable end-to-end. For a given
scene, GPNN infers a parse graph that includes i) the HOI graph struc-
ture represented by an adjacency matrix, and ii) the node labels. Within
a message passing inference framework, GPNN iteratively computes the
adjacency matrices and node labels. We extensively evaluate our model
on three HOI detection benchmarks on images and videos: HICO-DET,
V-COCO, and CAD-120 datasets. Our approach signiﬁcantly outper-
forms state-of-art methods, verifying that GPNN is scalable to large
datasets and applies to spatial-temporal settings. The code is available
at https://github.com/SiyuanQi/gpnn.

Keywords: Human-Object Interaction · Message Passing · Graph Pars-
ing · Neural Networks

1 Introduction

The task of human-object interaction (HOI) understanding aims to infer the
relationships between human and objects, such as “riding a bike” or “washing a
bike”. Beyond traditional visual recognition of individual instances, e.g., human
pose estimation, action recognition, and object detection, recognizing HOIs re-
quires a deeper semantic understanding of image contents. Recently, deep neural
networks (DNNs) have shown impressive progress on above individual tasks of
instance recognition, while relatively few methods [1, 2, 14, 38] were proposed for
HOI recognition. This is mainly because it requires reasoning beyond perception,
by integrating information from human, objects, and their complex relationships.

∗ Equal contribution. † Corresponding author.

2

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Fig. 1. Illustration of the proposed GPNN for learning HOI. GPNN oﬀers a
generic HOI representation that applies to (a) HOI detection in images and (b) HOI
recognition in videos. With the integration of graphical model and neural network,
GPNN can iteratively learn/infer the graph structures (a.v) and message passing (a.vi).
The ﬁnal parse graph explains a given scene with the graph structure (e.g., the link
between the person and the knife) and the node labels (e.g., lick). A thicker edge
corresponds to stronger information ﬂow between nodes in the graph.

In this paper, we propose a novel model, Graph Parsing Neural Network
(GPNN), for HOI recognition. The proposed GPNN oﬀers a general framework
that explicitly represents HOI structures with graphs and automatically parses
the optimal graph structures in an end-to-end manner. In principle, it is an
generalization of Message Passing Neural Network (MPNN) [12]. An overview
of GPNN is shown in Fig. 1. The following two aspects motivate our design.

First, we seek a uniﬁed framework that utilizes the learning capability of neu-
ral networks and the power of graphical representations. Recent deep learning
based HOI models showed promising results, but few touched how to interpret
well and explicitly leverage spatial and temporal dependencies and human-object
relations in such structured task. Aiming for this, we introduce GPNN. It in-
herits the complementary strengths of neural networks and graphical models,
for forming a coherent HOI representation with strong learning ability. Specif-
ically, with the structured representation of an HOI graph, the rich relations
are explicitly utilized, and the information from individual elements can be ef-
ﬁciently integrated and broadcasted over the structures. The whole model and
message passing operations are well-deﬁned and fully diﬀerentiable. Thus it can
be eﬃciently learned from data in an end-to-end manner.

Second, based on our eﬃcient HOI representation and learning power, GPNN
applies to diverse HOI tasks in both static and dynamic scenes. Previous studies
for HOI achieved good performance in their speciﬁc domains (spatial [1, 14] or
temporal [20, 34, 35]). However, none of them addresses a generic framework for
representing and learning HOI in both images and videos. The key diﬃculty lies
in the diverse relations between components. Given a set of human and objects
candidates, there may exist an uncertain number of human-object interaction
pairs (see Fig. 1 (a.ii) as an example). The relations become more complex after

Graph Parsing Neural Networks (ECCV 2018)

3

taking temporal factors into consideration. Thus pre-ﬁxed graph structures, as
adopted by most previous graphical or structured DNN models [11, 20, 22, 43],
are not an optimal choice. Seeking a better generalization ability, GPNN incor-
porates an essential link function for addressing the problem of graph structure
learning. It learns to infer the adjacency matrix in an end-to-end manner and
thus can infer a parse graph that explicitly explains the HOI relations. With such
learnable graph structure, GPNN could also limit the information ﬂow from ir-
relevant nodes while encouraging message to propagate between related nodes,
thus improving graph parsing.

We extensively evaluate the proposed GPNN on three HOI datasets, namely
HICO-DET [1], V-COCO [17] and CAD-120 [22], for HOI detection from im-
ages (HICO-DET, V-COCO) and HOI recognition and anticipation in spatial-
temporal settings (CAD-120). The experimental results verify the generality
and scalability of our GPNN based HOI representation and show substantial
improvements over state-of-the-art approaches, including pure graphical models
and pure neural networks. We also demonstrate GPNN outperforms its variants
and other graph neural networks with pre-ﬁxed structures.

This paper makes three major contributions. First, we propose the GPNN
that incorporates structural knowledge and DNNs for learning and inference.
Second, with a set of well deﬁned modular functions, GPNN addresses the HOI
problem by jointly performing graph structure inference and message passing.
Third, we empirically show that GPNN oﬀers a scalable and generic HOI rep-
resentation that applies to both static and dynamic settings.

2 Related Work

Human-Object Interaction. Reasoning human actions with objects (like “play-
ing baseball”, “playing guitar”), rather than recognizing individual actions (“play-
ing”) or object instances (“baseball”, “guitar”), is essential for a more compre-
hensive understanding of what is happening in the scene. Early work in HOI
understanding studied Bayesian model [15, 16], utilized contextual relationship
between human and objects [47–49], learned structured representations with spa-
tial interaction and context [8], exploited compositional models [9], or referred
to a set of HOI exemplars [19]. They were mainly based on handcrafted features
(e.g., color, HOG, and SIFT) with object and human detectors. More recently,
inspired by the notable success of deep learning and the availability of large-scale
HOI datasets [1,2], several deep learning based HOI models were then proposed.
Speciﬁcally, Mallya et al. [29] modiﬁed Fast RCNN model [13] for HOI recogni-
tion, with the assistance of Visual Question Answering (VQA). In [38], zero-shot
learning was applied for addressing the long-tail problem in HOI recognition.
In [1], the human proposals, object regions, and their combinations were fed
into a multi-stream network for tackling the HOI detection problem. Gkioxari
et al. [14] estimated an action-type speciﬁc density map for identifying the in-
teracted object locations, with a modiﬁed Faster RCNN architecture [36].

4

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Although promising results were achieved by above deep HOI models, we
still observe two unsolved issues. First, they lack a powerful tool to represent
the structures in HOI tasks explicitly and encodes them into modern network ar-
chitectures eﬃciently. Second, despite the successes in speciﬁc tasks, a complete
and generic HOI representation is missing. These approaches can not be easily
extended to HOI recognition from videos. Aiming to address those issues, we in-
troduce GPNN for imposing high-level relations into DNN, leading to a powerful
HOI representation that is applicable in both static and dynamic settings.
Neural Networks with Graphs/Graphical Models. In the literature, some
approaches were proposed to combine graphical models and neural networks.
The most intuitive approach is to build graphical models upon DNN, where
the network that generates features is trained ﬁrst, and its output is used to
compute potential functions for the graphical predictor. Typical methods were
used in human pose estimation [42], human part parsing [33, 45], and semantic
image segmentation [3, 4]. These methods lack a deep integration in the sense
that the computation process of graphical models cannot be learned end-to-
end. Some attempts [7, 21, 31, 32, 37, 40, 44, 51] were made to generalize neural
network operations (e.g., convolutions) directly from regular grids (e.g., images)
to graphs. For the HOI problem, however, a structured representation is needed
to capture the high-level spatial-temporal relations between humans and objects.
Some other work integrated network architectures with graphical models [12,20]
and gained promising results on applications such as scene understanding [24,30,
46], object detection and parsing [27,50], and VQA [41]. However, these methods
only apply to problems that have pre-ﬁxed graph structures. Liang et al. [26]
merged graph nodes using Long Short-Term Memory (LSTM) for human parsing
problem, under the assumption that the nodes are mergeable.

Those methods achieved promising results in their speciﬁc tasks and well
demonstrated the beneﬁt in completing deep architectures with domain-speciﬁc
structures. However, most of them are based on pre-ﬁxed graph structures, and
they have not yet been studied in HOI recognition. In this work, we extend pre-
vious graphical neural networks with learnable graph structures, which well ad-
dresses the rich and high-level relations in HOI problems. The proposed GPNN
can automatically infer the graph structure and utilize that structure for en-
hancing information propagation and further inference. It oﬀers a generic HOI
representation for both spatial and spatial-temporal settings. To the best of our
knowledge, this is a ﬁrst attempt to integrate graph models with neural networks
in a uniﬁed framework to achieve state-of-art results in HOI recognition.

3 Graph Parsing Neural Network for HOI

3.1 Formulation

For HOI understanding, human and objects are represented by nodes, and their
relations are deﬁned as edges. Given a complete HOI graph that includes all the
possible relationships among human and objects, we want to automatically infer
a parse graph by keeping the meaningful edges and labeling the nodes.

Graph Parsing Neural Networks (ECCV 2018)

5

Fig. 2. Illustration of the forward pass of GPNN. GPNN takes node and edge
features as input, and outputs a parse graph in a message passing fasion. The struc-
ture of the parse graph is given by a soft adjacency matrix. It is computed by the link
function based on the features (or hidden node states). The darker the color in the
adjacency matrix, the stronger the connectivity is. Then message functions compute
incoming messages for each node as a weighted sum of the messages from other nodes.
Thicker edges indicate larger information ﬂows. The update functions update the hid-
den internal states of each node. Above process is repeated for several steps, iteratively
and jointly learning the computation of graph structures and message passing. Finally,
for each node, the readout functions output HOI action or object labels from the hidden
node states. See § 3 for more details.

Formally, let G = (V, E, Y) denote the complete HOI graph. Nodes v ∈ V take
unique values from {1, · · · , |V|}. Edges e ∈ E are two-tuples e = (v, w) ∈ V × V.
Each node v has a output state yv ∈ Y that takes a value from a set of labels
{1, · · · , Yv} (e.g., actions). A parse graph g = (Vg, Eg, Yg) is a sub-graph of G,
where Vg ⊆ V and Eg ⊆ E. Given the node features Γ V and edge features Γ E , we
want to infer the optimal parse graph g∗ that best explains the data according
to a probability distribution p:

g∗ = argmax

p(g|Γ, G) = argmax

p(Vg, Eg, Yg|Γ, G)

g

= argmax

p(Yg|Vg, Eg, Γ )p(Vg, Eg|Γ, G)

g

g

(1)

where Γ = {Γ V , Γ E }. Here p(Vg, Eg|Γ, G) evaluates the graph structure, and
p(Yg|Vg, Eg, Γ ) is the labeling probability for the nodes in the parse graph.

This formulation provides us a principled guideline for designing the GPNN.
We design the network to approximate the computations of argmaxg p(Vg, Eg|Γ, G)
and argmaxg p(Yg|Vg, Eg, Γ ). We introduce four types of functions as individual
modules in the forward pass of a GPNN: link functions, message functions, up-
date functions, and readout functions (as illustrated in Fig. 2). The link functions
L(·) estimate the graph structure, giving an approximation of p(Vg, Eg|Γ, G). The
message, update and readout functions together resemble the belief propagation
process and approximate argmaxYg p(Yg|Vg, Eg, Γ ).

6

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Speciﬁcally, the link function (

) takes edge features ( ) as input and infers
the connectivities between nodes. The soft adjacency matrix ( ) is thus con-
structed and used as weights for messages passing through edges between nodes.
),
The incoming messages for a node are summarized by the message function (
then the hidden embedding state of the node is updated based on the messages
by an update function ( ). Finally, readout functions ( ) compute the target
outputs for each nodes. Those four types of functions are deﬁned as follows:
Link Function. We ﬁrst infer an adjacency matrix that represents connectivities
(i.e., the graph structure) between nodes by a link function. A link function L(·)
takes the node features Γ V , and edge features Γ E as input and outputs an
adjacency matrix A ∈ [0, 1]|V|×|V|:

Avw = L(Γv, Γw, Γvw)

(2)

where Avw denotes the (v, w)-th entry of the matrix A. Here we overload the
notation and let Γv denote node features and Γvw denote edge features. In this
way, the structure of a parse graph g can be approximated by the adjacency
matrix. Then we start to propagate messages over the parse graph, where the
soft adjacency matrix controls the information to be passed through edges.
Message and Update Functions. Based on the learned graph structure, the
message passing algorithm is adopted for inference of node labels. During belief
propagation, the hidden states of the nodes are iteratively updated by communi-
cating with other nodes. Specially, message functions M (·) summarize messages
to nodes coming from other nodes, and update functions U (·) update the hidden
node states according to the incoming messages. At each iteration step s, the
two functions computes:

ms

v =

(cid:88)

w

AvwM (hs−1

v

, hs−1

w , Γvw)

v = U (hs−1
hs

v

, ms
v)

where ms
v is the summarized incoming message for node v at s-th iteration
and hs
v is the hidden state for node v. The node connectivity A encourages the
information ﬂow between nodes in the parse graph. The message passing phase
runs for S steps towards convergence. At the ﬁrst step, the node hidden states
h0
v are initialized by node features Γv.
Readout Function. Finally, for each node, hidden state is fed into a readout
function to output a label:

yv = R(hS

v ).

v (node embeddings).

Here the readout function R(·) computes output yv for node v by activating its
hidden state hS
Iterative Parsing. Based on the above four functions, the messages are passed
along the graph and weighted by the learned adjacency matrix A. We further
extend above process into a joint learning framework that iteratively infers the
graph structure and propagates the information to infer node labels. In particu-
lar, instead of learning A only at the beginning, we iteratively infer A with the

(3)

(4)

(5)

Graph Parsing Neural Networks (ECCV 2018)

7

updated node information and edge features at each step s:

As

vw = L(hs−1

v

, hs−1

w , ms−1

vw ).

Then the messages in Eq. 3 are redeﬁned as:

ms

v =

(cid:88)

w

As

vwM (hs−1

v

, hs−1

w , Γvw).

(6)

(7)

In this way, both the graph structure and the message update can be jointly and
iteratively learned in a uniﬁed framework. In practice, we ﬁnd such a strategy
would bring better performance (detailed in § 4.3).

In next section, we show that by implementing each function by neural net-
works, the entire system is diﬀerentiable end-to-end. Hence all the parameters
can be learned using gradient-based optimization.

3.2 Network Architecture

Link Function. Given the complete HOI graph G = (V, E, Y), we use dV and dE
to denote the dimension of the node features and the edge features, respectively.
In a message passing step s, we ﬁrst concatenate all the node features (hidden
vw ∈ RdE }v,w to
states) {hs
form a feature matrix F s ∈ R|V |×|V |×(2dV +dE ) (see
in Fig. 2). The link function
is deﬁned as a small neural network with one or several convolutional layer(s)
(with 1 × 1 × (2dV + dE) kernels) and a sigmoid activation. Then the adjacency
matrix As ∈ [0, 1]|V|×|V| can be computed as:

v ∈ RdV }v and all the edge features (messages) {ms

As = σ(WL ∗ F s),

(8)

where WL is the learnable parameters of the link function network L(·) and ∗
denotes conv operation. The sigmoid operation σ(·) is for normalizing the values
of the elements of As into [0, 1]. The essential eﬀect of multiple convolutional
layers with 1 × 1 kernels is similar to fully connected layers applied to each
individual edge features, except that the ﬁlter weights are shared by all the
edges. In practice, we ﬁnd such operation generates good enough results and
leads to a high computation eﬃciency.

For spatial-temporal problems where the adjacency matrices should account
for the previous states, we use convolutional LSTMs [39] for modeling L(·) in
temporal domain. At time t, the link function takes F s,t as input features and the
previous adjacency matrix As,t−1 as hidden state: As,t = convLST M (F s,t, As,t−1).
Again, the kernel size for the conv layer in convLSTM is 1 × 1 × (2dV + dE).
Message Function. In our implementation, the message function M (·) in Eq. 3
is computed by:

M (hv, hw, Γvw) = [WM

V hv, WM

V hw, WM

E Γvw],

(9)

where [., .] denotes concatenation. It concatenates the outputs of linear trans-
forms (i.e., fully connected layers parametrized by WM
E ) that takes
node hidden states hv or edge features Γvw as input.

V and WM

8

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Update Function. Recurrent neural networks [10, 18] are natural choices for
simulating the iterative update process, as done by previous works [12]. Here we
apply Gated Recurrent Unit (GRU) [5] as the update function, because of its
recurrent nature and smaller amount of parameters. Thus the update function
in Eq. 4 is implemented as:

v = U (hs−1
hs

v

, ms

v) = GRU (hs−1

v

, ms

v),

(10)

v is the hidden state and ms

where hs
in [25], the GRU is more eﬀective than vanilla recurrent neural networks.
Readout Function. A typical implementation of readout functions is com-
bining several fully connected layers (parameterized by WR) followed by an
activation function:

v is used as input features. As demonstrated

yv = R(hS

v ) = ϕ(WRhS

v ).

(11)

Here the activation function ϕ(·) can be used as softmax (one-class outputs) or
sigmoid (multi-class outputs) according to diﬀerent HOI tasks.

In this way, the entire GPNN is implemented to be fully diﬀerentiable and
end-to-end trainable. The loss for speciﬁc HOI task can be computed for the
outputs of readout functions, and the error can propagate back according to
chain rule. In next section, we will oﬀer more details for implementing GPNN
for HOI tasks on spatial and spatial-temporal settings and present qualitative
as well as quantitative results.

4 Experiments

To verify the eﬀectiveness and generic applicability of GPNN, we perform ex-
periments on two HOI problems: i) HOI detection in images [1, 17], and ii) HOI
recognition and anticipation from videos [22]. The ﬁrst experiment is performed
on HICO-DET [1] and V-COCO [17] datasets, showing that our approach is
scalable to large datasets (about 60K images in total) and achieves a good de-
tection accuracy over a large number of classes (more than 600 classes of HOIs).
The second experiment is reported on CAD-120 dataset [22], showing that our
method is well applicable to spatial-temporal domains.

4.1 Human-Object Interaction Detection in Images

For HOI detection in an image, the goal is to detect pairs of a human and an
object bounding box with an interaction class label connecting them.
Datasets. We use HICO-DET [1] and V-COCO [17] datasets for benchmarking
our GPNN model. HICO-DET provides more than 150K annotated instances
of human-object pairs in 47,051 images (37,536 training and 9,515 testing). It
has the same 80 object categories as MS-COCO [28] and 117 action categories.
V-COCO is a subset of MS-COCO [28]. It consists of a total of 10,346 images
with 16,199 people instances, where ∼2.5K images in the train set, ∼2.8K images
for validation and ∼4.9K images for testing. Each annotated person has binary

Graph Parsing Neural Networks (ECCV 2018)

9

Table 1. HOI detection results (mAP) on HICO-DET dataset [1]. Higher
values are better. The best scores are marked in bold.

Methods

Full (mAP %) ↑ Rare (mAP %) ↑ Non-rare (mAP %) ↑

Random
Fast-RCNN(union) [13]
Fast-RCNN(score) [13]
HO-RCNN [1]
HO-RCNN+IP [1]
HO-RCNN+IP+S [1]
Gupta et al. [17]
Shen et al. [38]
InteractNet [14]
GPNN
Performance Gain(%)

1.35 × 10−3
1.75
2.85
5.73
7.30
7.81
9.09
6.46
9.94
13.11
31.89

5.72 × 10−4
0.58
1.55
3.21
4.68
5.37
7.02
4.24
7.16
9.34
30.45

1.62 × 10−3
2.10
3.23
6.48
8.08
8.54
9.71
7.12
10.77
14.23
32.13

Fig. 3. HOI detection results on HICO-DET [1] test images. Human and ob-
jects are shown in red and green rectangles, respectively. Best viewed in color.

labels for 26 diﬀerent action classes. Note that three actions (i.e., cut, eat, and
hit) are annotated with two types of targets: instrument and direct object.
Implementation Details. Humans and objects are represented by nodes in
the graph, while human-object interactions are represented by edges. In this ex-
periment, we use a pre-trained deformable convolutional network [6] for object
detection and features extraction. Based on the detected bounding boxes, we ex-
tract node features (7 × 7 × 80) from the position-sensitive region of interest (PS
RoI) pooling layer from the deformable ConvNet. We extract the edge feature
from a combined bounding box, i.e., the smallest bounding box that contains
both two nodes’ bounding boxes. The functions of GPNN are implemented as
follows. We use a convolutional network (128-128-1)-Sigmoid(·) with 1×1 kernels
for the link function. The message functions are composed of a fully connected
layer, concatenation, and summation. For a node v, the neighboring node feature
Γw and edge feature Γvw are passed through a fully connected layer and concate-
nated. The ﬁnal incoming message is a weighted sum of messages from all neigh-
boring nodes. Speciﬁcally, the message for node v coming from node w through
edge e = (v, w) is the concatenation of output from FC(dV -dV ) and FC(dE-dE).
A GRU(dV ) is used for the update function. The propagation step number S

10

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 2. HOI detection results (mAP) on V-COCO [17] dataset. Legend: Set
1 indicates 18 HOI actions with one object, and Set 2 corresponds to 3 HOI actions
(i.e., cut, eat, hit) with two objects (instrument and object).

Method

Set 1 (mAP %) ↑

Set 2 (mAP %) ↑ Ave. (mAP %) ↑

Gupta et al. [17]
InteractNet [14]
GPNN
Performance Gain(%)

33.5
42.2
44.5
5.5

26.7
33.2
42.8
28.9

31.8
40.0
44.0
10.0

Fig. 4. HOI detection results on V-COCO [17] test images. Human and objects
are shown in red and green rectangles, respectively. Best viewed in color.

is set to be 3. For the readout function, we use a FC(dV -117)-Sigmoid(·) and
FC(dV -26)-Sigmoid(·) for HICO-DET and V-COCO, respectively.

The probability of an HOI label of a human-object pair is given by the
product of the ﬁnal output probabilities from the human node and the object
node. We employ an L1 loss for the adjacency matrix. For the node outputs, we
use a weighted multi-class multi-label hinge loss. The reasons are two-folds: the
training examples are not balanced, and it is essentially a multi-label problem
for each node (there might not even exist a meaningful human-object interaction
for detected humans and objects).

Our model is implemented using PyTorch and trained with a machine with
a single Nvidia Titan Xp GPU. We start with a learning rate of 1e-3, and the
rate decays every 5 epochs by 0.8. The training process takes about 20 epochs
(∼15 hours) to roughly converge with a batch size of 32.
Comparative Methods. We compare our method with eight baselines: (1)
Fast-RCNN (union) [13]: for each human-object proposal from detection results,
their attention windows are used as the region proposal for Fast-RCNN. (2) Fast-
RCNN (score) [13]: given human-object proposals, HOI is predicted by linearly
combining the human and object detection scores. (3) HO-RCNN [1]: a multi-
stream architecture with a ConvNet to classify human, object and human-object
proposals, respectively. The ﬁnal output is computed by combining the scores
from all the three streams. (4) HO-RCNN+IP [1] and (5) HO-RCNN+IP+S [1]:
HO-RCNN with additional components. Interaction Patterns (IP) acts as a at-
tention ﬁlter to images. S is an extra path with a single neuron that uses the

Graph Parsing Neural Networks (ECCV 2018)

11

raw object detection score to produce an oﬀset for the ﬁnal detection. More
detailed descriptions of above ﬁve baselines can be found in [1]. (6) Gupta et
al. [17]: trained based on Fast-RCNN [13]. We use the scores reported in [14].
(7) Shen et al. [38]: ﬁnal predictions are from two Faster RCNN [36] based net-
works which are trained for predicting verb and object classes, respectively. (8)
InteractNet [14]: a modiﬁed Faster RCNN [36] with an additional human-centric
branch that estimates an action-speciﬁc density map for locating objects.
Experiment Results. Following the standard settings in HICO-DET and V-
COCO benchmarks, we evaluate HOI detection using mean average precision
(mAP). An HOI detection is considered as a true positive when the human
detection, the object detection, and the interaction class are all correct. The
human and object bounding boxes are considered as true positives if they overlap
with a ground truth bounding boxes of the same class with an intersection over
union (IoU) greater than 0.5. For HICO-DET dataset, we report the mAP over
three diﬀerent HOI category sets: i) all 600 HOI categories in HICO (Full); ii)
138 HOI categories with less than 10 training instances (Rare); and iii) 462 HOI
categories with 10 or more training instances (Non-Rare). For V-COCO dataset,
since we concentrate on HOI detection, we report the mAP on three groups: i)
18 HOI action classes with one target object; ii) 3 HOI categories with two types
of objects; iii) all 24 (=18 + 3 × 2) HOI classes. Results are evaluated on the test
sets and reported in Table 1 and Table 2.

As shown in Table 1, the proposed GPNN substantially outperforms the
comparative methods, achieving 31.89%, 30.45%, and 32.13% improvement
over the second best methods on the three HOI category sets on the HICO-
DET dataset. The results on V-COCO dataset (in Table 2) also consistently
demonstrate the superior performance of the proposed GPNN. Two important
conclusions can be drawn from the results: i) our method is scalable to large
datasets; ii) and our method performs better than pure neural network. Some
visual results can be found in Fig. 3 and Fig. 4.

4.2 Human-Object Interaction Recognition in Videos

The goal of this experiment is to detect and predict the human sub-activity
labels and object aﬀordance labels as the human-object interaction progresses
in videos. The problem is challenging since it involves complex interactions that
humans make with multiple objects, and objects also interact with each other.
CAD-120 dataset [22]. It has 120 RGB-D videos of 4 subjects performing 10
activities, each of which is a sequence of sub-activities involving 10 actions (e.g.,
reaching, opening), and 12 object aﬀordances (e.g., reachable, openable) in total.
Implementation Details. The link function is implemented as: convLSTM(1024-
1024-1024-1)-Sigmoid(·) (i.e., a four-layer convLSTM). We use the same ar-
chitecture as the previous experiment for message functions and update func-
tions: [FC(dV -dV ), FC(dE-dE)] for message function and GRU(dV ) for update
function. The propagation step number S is set to be 3. We use a FC(dV -10)-
Softmax(·) and a FC(dV -12)-Softmax(·) for readout functions of sub-activity and
object aﬀordance detection/anticipation, respectively. We employ an L1 loss for

12

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 3. Human activity detection and future anticipation results on CAD-
120 [22] dataset, measured via F1-score.

Method

Detection (F1-score) ↑
Object
Sub
Aﬀordance(%)
-activity(%)

Anticipation (F1-score) ↑

Sub
-activity(%)

Object
Aﬀordance(%)

ATCRF [22]
S-RNN [20]
S-RNN (multi-task) [20]
GPNN
Performance Gain(%)

80.4
83.2
82.4
88.9
8.1

81.5
88.7
91.1
88.8
-

37.9
62.3
65.6
75.6
15.2

36.7
80.7
80.9
81.9
1.2

Fig. 5. Confusion matrices of HOI detection (a)(b) and anticipation (c)(d)
results on CAD-120 [22] dataset. Zoom in for more details.

the adjacency matrix and a cross entropy loss for the node outputs. We use the
publicly available node and edge features from [23].
Comparative Methods. We compare our method with two baselines: antici-
patory temporal CRF (ATCRF) [22] and structural RNN (S-RNN) [20]. ATCRF
is a top-performing graphical model approach for this problem, while S-RNN is
the state-of-art method using structured neural networks. ATCRF models the
human activities through a spatial-temporal conditional random ﬁeld. S-RNN
casts a pre-deﬁned spatial-temporal graph as an RNN mixture by representing
nodes and edges as LSTMs.
Experiment Results. In Table 3 we show the quantitative comparison of
our method with other competitors. It shows the F1-scores averaged over all
classes on detection and activity anticipation tasks. GPNN greatly improves
over ATCRF and S-RNN, especially on anticipation task. Our method outper-
forms the other two for the following reasons. i) Comparing to ATCRF limited
to the Markov assumption, our method allows arbitrary graph structures with
improved representation ability. ii) Our method enjoys the beneﬁt of deep in-
tegration of graphical models and neural networks and can be learned in an
end-to-end manner. iii) Rather than relying on a pre-ﬁxed graph structure as in
S-RNN, we infer the graph structure via learning an adjacency matrix and thus
be able to control the information ﬂow between nodes during massage passing.
Fig. 5 show the confusion matrices for detecting and predicting the sub-activities
and object aﬀordances, respectively. From above results we can draw two im-

Graph Parsing Neural Networks (ECCV 2018)

13

Fig. 6. HOI detection results on a “cleaning objects” activity on CAD-120 [22]
dataset. Human are shown in red rectangle. Two objects are shown in green and blue
rectangles, respectively. Detection and anticipation results are shown by diﬀerent bars.
For anticipation task, the label of the sub-activity at time t is anticipated at time t-1.

portant conclusions: i) our method is well applicable to the spatio-temporal
domain; and ii) our method outperforms pure graphical models (e.g., ATCRF)
and deep networks with pre-ﬁxed graph structures (e.g., S-RNN). Fig. 6 shows a
qualitative visualization of “cleaning objects”. We show one representative frame
for each sub-activity as well as the corresponding detections and anticipations.

4.3 Ablation Study

In this section, we analyze the contributions of diﬀerent model components to
the ﬁnal performance and examine the eﬀectiveness of our main assumptions.
Table 4 shows the detailed results on all three datasets.
Integration of DNN with Graphical Model. We ﬁrst examine the inﬂu-
ence of integrating DNN with a graphical model. We directly feed the features,
which are originally used for GPNN, into diﬀerent fully connected networks for
predicting HOI action or object classes. From Table 4, we can observe the per-
formance of w/o graph is signiﬁcantly worse than GPNN model over various
HOI datasets. This supports our view that modeling high-level structures and
leveraging learning capabilities of DNNs together is essential for HOI tasks.
GPNN with Fixed Graph Structures. In § 3, GPNN automatically infers
graph structures (i.e., parse graph) via learning a soft adjacency matrix. To
assess this strategy, we ﬁx all the entries in the soft adjacency matrices to be
constant 1. This way the graph structures are ﬁxed and the information ﬂow
between nodes are not weighted. For constant graph baseline, we see obvious
performance decrease, compared with the full GPNN model. This indicates that
inferring graph structures is critical to get reasonable performance.
GPNN without Supervision on Link Functions. We perform experiments
by turning oﬀ the L1 loss on adjacency matrices (w/o graph loss in Table 4). We
can observe that the intermediate L1 loss is eﬀective, further verifying our design
to learn the graph structure. Another interesting observation is that training

14

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 4. Ablation study of GPNN model. Higher values are better.

Aspect Method

Set 1 Set 2 Ave. Full Rare

V-COCO [17]
HOI Detection
mAP(%) ↑

HICO-DET [1]
HOI Detection
mAP(%) ↑

CAD-120 [22]

HOI Detec.
F1-score(%) ↑
Object
Sub-
Aﬀ.(%)
activity

HOI Antici.
F1-score(%) ↑
Object
Sub-
Aﬀ.(%)
activity

Non-
rare

44.5 42.8 44.0 13.11 9.34 14.23 88.9

88.8

75.6

81.9

GPNN
(3 iterations)

w/o graph
graph
constant graph
structure w/o graph loss

2.04
9.62
27.4 30.0 28.1
1.94 10.79
34.6 33.3 34.3
8.72
6.24
37.7 40.5 38.4
w/o joint parsing 43.6 39.4 42.5 10.17 5.81 11.47
42.0 40.7 41.7 11.38 7.27 12.61
44.1 42.2 43.6 12.37 9.01 13.38
43.6 40.9 42.9 12.39 8.95 13.41

7.88
8.75
8.15

iterative 1 iteration
learning 2 iterations
4 iterations

50.2
85.3
85.2
79.3
80.5
87.9
87.9

20.8
85.6
85.8
79.2
80.7
86.1
85.7

32.3
73.8
74.7
74.7
75.2
76.1
75.5

19.6
79.1
79.2
80.3
81.1
81.5
80.6

the model without this loss has a similar eﬀect to training with constant graph.
Hence supervision on the graph is fairly important.
Jointly Learning Parse Graph and Message Passing. We next study the
eﬀect of jointly learning graph structures and message passing. By isolating graph
parsing from message passing, we obtain w/o joint parsing, where the adjacency
matrices are directly computed by link functions from edge features at the be-
ginning. We observe a performance decrease in Table 4, showing that learning
graph structures and message passing together indeed boost the performance.
Iterative Learning Process. Next we examine the eﬀect of iterative message
passing, we report three baselines: 1 iteration, 2 iterations, and 4 iterations,
which correspond to the results from diﬀerent message passing iterations. The
baseline GPNN (ﬁrst row in Table 4) are the results after three iterations. From
the results we observe that the iterative learning process is able to gradually
improve the performance in general. We also observe that when the iteration
round is increased to a certain extent, the performance drops slightly.

5 Conclusion

In this paper, we propose Graph Parsing Neural Network (GPNN) for inferring
a parse graph in an end-to-end manner. The network can be decomposed into
four distinct functions, namely link functions, message functions, update func-
tions and readout functions, for iterative graph inference and message passing.
GPNN provides a generic HOI representation that is applicable in both spa-
tial and spatial-temporal domains. We demonstrate a substantial performance
gain on three HOI datasets, showing the eﬀectiveness of the proposed framework.

Acknowledgments. The authors thank Prof. Ying Nian Wu from UCLA Statis-
tics Department for helpful comments on this work. This research is supported by
DARPA XAI N66001-17-2-4029, ONR MURI N00014-16-1-2007, ARO W911NF1810296,
and N66001-17-2-3602.

Graph Parsing Neural Networks (ECCV 2018)

15

References

interactions (2018)

1. Chao, Y.W., Liu, Y., Liu, X., Zeng, H., Deng, J.: Learning to detect human-object

2. Chao, Y.W., Wang, Z., He, Y., Wang, J., Deng, J.: HICO: A benchmark for rec-

ognizing human-object interactions in images. In: ICCV (2015)

3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Se-
mantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected CRFs. PAMI (2016)

4. Chen, L.C., Schwing, A., Yuille, A., Urtasun, R.: Learning deep structured models.

In: ICML (2015)

5. Cho, K., Van Merri¨enboer, B., Bahdanau, D., Bengio, Y.: On the properties of
neural machine translation: Encoder–decoder approaches. Syntax, Semantics and
Structure in Statistical Translation p. 103 (2014)

6. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convo-

lutional networks. In: ICCV (2017)

7. Deﬀerrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on

graphs with fast localized spectral ﬁltering. In: NIPS (2016)

8. Delaitre, V., Sivic, J., Laptev, I.: Learning person-object interactions for action

recognition in still images. In: NIPS (2011)

9. Desai, C., Ramanan, D.: Detecting actions, poses, and objects with relational

phraselets. In: ECCV (2012)

10. Elman, J.L.: Finding structure in time. Cognitive science (1990)
11. Fang, H.S., Xu, Y., Wang, W., Zhu, S.C.: Learning pose grammar to encode human

body conﬁguration for 3d pose estimation. In: AAAI (2018)

12. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message

passing for quantum chemistry. In: ICML (2017)

13. Girshick, R.: Fast R-CNN. In: ICCV (2015)
14. Gkioxari, G., Girshick, R., Doll´ar, P., He, K.: Detecting and recognizing human-

object interactions. In: CVPR (2018)

15. Gupta, A., Davis, L.S.: Objects in action: An approach for combining action un-

derstanding and object perception. In: CVPR (2007)

16. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using

spatial and functional compatibility for recognition. PAMI (2009)

17. Gupta, S., Malik, J.: Visual semantic role labeling. arXiv preprint arXiv:1505.04474

18. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

(2015)

(1997)

19. Hu, J.F., Zheng, W.S., Lai, J., Gong, S., Xiang, T.: Recognising human-object

interaction via exemplar based modelling. In: ICCV (2013)

20. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: Deep learning on

spatio-temporal graphs. In: CVPR (2016)

21. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional

networks. In: ICLR (2017)

22. Koppula, H.S., Saxena, A.: Anticipating human activities using object aﬀordances

for reactive robotic response. PAMI (2016)

23. Koppula, H.S., Gupta, R., Saxena, A.: Learning human activities and object af-
fordances from RGB-D videos. The International Journal of Robotics Research
(2013)

16

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

24. Li, R., Tapaswi, M., Liao, R., Jia, J., Urtasun, R., Fidler, S.: Situation recognition

with graph neural networks. In: ICCV (2017)

25. Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R.: Gated graph sequence neural

26. Liang, X., Lin, L., Shen, X., Feng, J., Yan, S., Xing, E.P.: Interpretable structure-

27. Liang, X., Shen, X., Feng, J., Lin, L., Yan, S.: Semantic object parsing with graph

networks. In: ICLR (2016)

evolving lstm. In: ICCV (2017)

lstm. In: ECCV (2016)

28. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)
29. Mallya, A., Lazebnik, S.: Learning models for actions and person-object interac-

tions with transfer to question answering. In: ECCV (2016)

30. Marino, K., Salakhutdinov, R., Gupta, A.: The more you know: Using knowledge

graphs for image classiﬁcation. In: CVPR (2016)

31. Monti, F., Boscaini, D., Masci, J., Rodol`a, E., Svoboda, J., Bronstein, M.M.: Ge-
ometric deep learning on graphs and manifolds using mixture model cnns. CVPR
(2016)

32. Niepert, M., Ahmed, M., Kutzkov, K.: Learning convolutional neural networks for

graphs. In: ICML (2016)

33. Park, S., Nie, X., Zhu, S.C.: Attribute and-or grammar for joint parsing of human

pose, parts and attributes. PAMI (2017)

34. Qi, S., Huang, S., Wei, P., Zhu, S.C.: Predicting human activities using stochastic

grammar. In: ICCV (2017)

35. Qi, S., Jia, B., Zhu, S.C.: Generalized earley parser: Bridging symbolic grammars

and sequence data for future prediction. In: ICML (2018)

36. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

37. Seo, Y., Deﬀerrard, M., Vandergheynst, P., Bresson, X.: Structured sequence mod-
eling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659
(2016)

38. Shen, L., Yeung, S., Hoﬀman, J., Mori, G., Fei-Fei, L.: Scaling human-object in-

teraction recognition through zero-shot learning (2018)

39. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convolutional
LSTM network: A machine learning approach for precipitation nowcasting. In:
NIPS (2015)

40. Simonovsky, M., Komodakis, N.: Dynamic edge-conditioned ﬁlters in convolutional

neural networks on graphs. CVPR (2017)

41. Teney, D., Liu, L., Hengel, A.v.d.: Graph-structured representations for visual ques-

tion answering. In: CVPR (2017)

42. Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint training of a convolutional
network and a graphical model for human pose estimation. In: NIPS (2014)
43. Wang, W., Xu, Y., Shen, J., Zhu, S.C.: Attentive fashion grammar network for
fashion landmark detection and clothing category classiﬁcation. In: CVPR (2018)
44. Wu, Z., Lin, D., Tang, X.: Deep markov random ﬁeld for image modeling. In:

ECCV (2016)

45. Xia, F., Zhu, J., Wang, P., Yuille, A.L.: Pose-guided human parsing by an And/Or

graph using pose-context features. In: AAAI (2016)

46. Xu, D., Zhu, Y., Choy, C.B., Fei-Fei, L.: Scene graph generation by iterative mes-

sage passing. In: ICCV (2017)

47. Yao, B., Fei-Fei, L.: Grouplet: A structured image representation for recognizing

human and object interactions. In: CVPR (2010)

Graph Parsing Neural Networks (ECCV 2018)

17

48. Yao, B., Fei-Fei, L.: Modeling mutual context of object and human pose in human-

object interaction activities. In: CVPR (2010)

49. Yao, B., Jiang, X., Khosla, A., Lin, A.L., Guibas, L., Fei-Fei, L.: Human action
recognition by learning bases of action attributes and parts. In: ICCV (2011)
50. Yuan, Y., Liang, X., Wang, X., Yeung, D.Y., Gupta, A.: Temporal dynamic graph

LSTM for action-driven video object detection. In: ICCV (2017)

51. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: ICCV
(2015)

8
1
0
2
 
g
u
A
 
3
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
6
9
7
0
.
8
0
8
1
:
v
i
X
r
a

Learning Human-Object Interactions by
Graph Parsing Neural Networks

Siyuan Qi∗1,2, Wenguan Wang∗1,3, Baoxiong Jia1,4,
Jianbing Shen†3,5, and Song-Chun Zhu1,2

1 University of California, Los Angeles
2 International Center for AI and Robot Autonomy (CARA)
3 Beijing Institute of Technology
4 Peking University
5 Inception Institute of Artiﬁcial Intelligence

syqi@cs.ucla.edu

wenguanwang.ai@gmail.com baoxiongjia@ucla.edu

shenjianbing@bit.edu.cn

sczhu@stat.ucla.edu

Abstract. This paper addresses the task of detecting and recognizing
human-object interactions (HOI) in images and videos. We introduce the
Graph Parsing Neural Network (GPNN), a framework that incorporates
structural knowledge while being diﬀerentiable end-to-end. For a given
scene, GPNN infers a parse graph that includes i) the HOI graph struc-
ture represented by an adjacency matrix, and ii) the node labels. Within
a message passing inference framework, GPNN iteratively computes the
adjacency matrices and node labels. We extensively evaluate our model
on three HOI detection benchmarks on images and videos: HICO-DET,
V-COCO, and CAD-120 datasets. Our approach signiﬁcantly outper-
forms state-of-art methods, verifying that GPNN is scalable to large
datasets and applies to spatial-temporal settings. The code is available
at https://github.com/SiyuanQi/gpnn.

Keywords: Human-Object Interaction · Message Passing · Graph Pars-
ing · Neural Networks

1 Introduction

The task of human-object interaction (HOI) understanding aims to infer the
relationships between human and objects, such as “riding a bike” or “washing a
bike”. Beyond traditional visual recognition of individual instances, e.g., human
pose estimation, action recognition, and object detection, recognizing HOIs re-
quires a deeper semantic understanding of image contents. Recently, deep neural
networks (DNNs) have shown impressive progress on above individual tasks of
instance recognition, while relatively few methods [1, 2, 14, 38] were proposed for
HOI recognition. This is mainly because it requires reasoning beyond perception,
by integrating information from human, objects, and their complex relationships.

∗ Equal contribution. † Corresponding author.

2

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Fig. 1. Illustration of the proposed GPNN for learning HOI. GPNN oﬀers a
generic HOI representation that applies to (a) HOI detection in images and (b) HOI
recognition in videos. With the integration of graphical model and neural network,
GPNN can iteratively learn/infer the graph structures (a.v) and message passing (a.vi).
The ﬁnal parse graph explains a given scene with the graph structure (e.g., the link
between the person and the knife) and the node labels (e.g., lick). A thicker edge
corresponds to stronger information ﬂow between nodes in the graph.

In this paper, we propose a novel model, Graph Parsing Neural Network
(GPNN), for HOI recognition. The proposed GPNN oﬀers a general framework
that explicitly represents HOI structures with graphs and automatically parses
the optimal graph structures in an end-to-end manner. In principle, it is an
generalization of Message Passing Neural Network (MPNN) [12]. An overview
of GPNN is shown in Fig. 1. The following two aspects motivate our design.

First, we seek a uniﬁed framework that utilizes the learning capability of neu-
ral networks and the power of graphical representations. Recent deep learning
based HOI models showed promising results, but few touched how to interpret
well and explicitly leverage spatial and temporal dependencies and human-object
relations in such structured task. Aiming for this, we introduce GPNN. It in-
herits the complementary strengths of neural networks and graphical models,
for forming a coherent HOI representation with strong learning ability. Specif-
ically, with the structured representation of an HOI graph, the rich relations
are explicitly utilized, and the information from individual elements can be ef-
ﬁciently integrated and broadcasted over the structures. The whole model and
message passing operations are well-deﬁned and fully diﬀerentiable. Thus it can
be eﬃciently learned from data in an end-to-end manner.

Second, based on our eﬃcient HOI representation and learning power, GPNN
applies to diverse HOI tasks in both static and dynamic scenes. Previous studies
for HOI achieved good performance in their speciﬁc domains (spatial [1, 14] or
temporal [20, 34, 35]). However, none of them addresses a generic framework for
representing and learning HOI in both images and videos. The key diﬃculty lies
in the diverse relations between components. Given a set of human and objects
candidates, there may exist an uncertain number of human-object interaction
pairs (see Fig. 1 (a.ii) as an example). The relations become more complex after

Graph Parsing Neural Networks (ECCV 2018)

3

taking temporal factors into consideration. Thus pre-ﬁxed graph structures, as
adopted by most previous graphical or structured DNN models [11, 20, 22, 43],
are not an optimal choice. Seeking a better generalization ability, GPNN incor-
porates an essential link function for addressing the problem of graph structure
learning. It learns to infer the adjacency matrix in an end-to-end manner and
thus can infer a parse graph that explicitly explains the HOI relations. With such
learnable graph structure, GPNN could also limit the information ﬂow from ir-
relevant nodes while encouraging message to propagate between related nodes,
thus improving graph parsing.

We extensively evaluate the proposed GPNN on three HOI datasets, namely
HICO-DET [1], V-COCO [17] and CAD-120 [22], for HOI detection from im-
ages (HICO-DET, V-COCO) and HOI recognition and anticipation in spatial-
temporal settings (CAD-120). The experimental results verify the generality
and scalability of our GPNN based HOI representation and show substantial
improvements over state-of-the-art approaches, including pure graphical models
and pure neural networks. We also demonstrate GPNN outperforms its variants
and other graph neural networks with pre-ﬁxed structures.

This paper makes three major contributions. First, we propose the GPNN
that incorporates structural knowledge and DNNs for learning and inference.
Second, with a set of well deﬁned modular functions, GPNN addresses the HOI
problem by jointly performing graph structure inference and message passing.
Third, we empirically show that GPNN oﬀers a scalable and generic HOI rep-
resentation that applies to both static and dynamic settings.

2 Related Work

Human-Object Interaction. Reasoning human actions with objects (like “play-
ing baseball”, “playing guitar”), rather than recognizing individual actions (“play-
ing”) or object instances (“baseball”, “guitar”), is essential for a more compre-
hensive understanding of what is happening in the scene. Early work in HOI
understanding studied Bayesian model [15, 16], utilized contextual relationship
between human and objects [47–49], learned structured representations with spa-
tial interaction and context [8], exploited compositional models [9], or referred
to a set of HOI exemplars [19]. They were mainly based on handcrafted features
(e.g., color, HOG, and SIFT) with object and human detectors. More recently,
inspired by the notable success of deep learning and the availability of large-scale
HOI datasets [1,2], several deep learning based HOI models were then proposed.
Speciﬁcally, Mallya et al. [29] modiﬁed Fast RCNN model [13] for HOI recogni-
tion, with the assistance of Visual Question Answering (VQA). In [38], zero-shot
learning was applied for addressing the long-tail problem in HOI recognition.
In [1], the human proposals, object regions, and their combinations were fed
into a multi-stream network for tackling the HOI detection problem. Gkioxari
et al. [14] estimated an action-type speciﬁc density map for identifying the in-
teracted object locations, with a modiﬁed Faster RCNN architecture [36].

4

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Although promising results were achieved by above deep HOI models, we
still observe two unsolved issues. First, they lack a powerful tool to represent
the structures in HOI tasks explicitly and encodes them into modern network ar-
chitectures eﬃciently. Second, despite the successes in speciﬁc tasks, a complete
and generic HOI representation is missing. These approaches can not be easily
extended to HOI recognition from videos. Aiming to address those issues, we in-
troduce GPNN for imposing high-level relations into DNN, leading to a powerful
HOI representation that is applicable in both static and dynamic settings.
Neural Networks with Graphs/Graphical Models. In the literature, some
approaches were proposed to combine graphical models and neural networks.
The most intuitive approach is to build graphical models upon DNN, where
the network that generates features is trained ﬁrst, and its output is used to
compute potential functions for the graphical predictor. Typical methods were
used in human pose estimation [42], human part parsing [33, 45], and semantic
image segmentation [3, 4]. These methods lack a deep integration in the sense
that the computation process of graphical models cannot be learned end-to-
end. Some attempts [7, 21, 31, 32, 37, 40, 44, 51] were made to generalize neural
network operations (e.g., convolutions) directly from regular grids (e.g., images)
to graphs. For the HOI problem, however, a structured representation is needed
to capture the high-level spatial-temporal relations between humans and objects.
Some other work integrated network architectures with graphical models [12,20]
and gained promising results on applications such as scene understanding [24,30,
46], object detection and parsing [27,50], and VQA [41]. However, these methods
only apply to problems that have pre-ﬁxed graph structures. Liang et al. [26]
merged graph nodes using Long Short-Term Memory (LSTM) for human parsing
problem, under the assumption that the nodes are mergeable.

Those methods achieved promising results in their speciﬁc tasks and well
demonstrated the beneﬁt in completing deep architectures with domain-speciﬁc
structures. However, most of them are based on pre-ﬁxed graph structures, and
they have not yet been studied in HOI recognition. In this work, we extend pre-
vious graphical neural networks with learnable graph structures, which well ad-
dresses the rich and high-level relations in HOI problems. The proposed GPNN
can automatically infer the graph structure and utilize that structure for en-
hancing information propagation and further inference. It oﬀers a generic HOI
representation for both spatial and spatial-temporal settings. To the best of our
knowledge, this is a ﬁrst attempt to integrate graph models with neural networks
in a uniﬁed framework to achieve state-of-art results in HOI recognition.

3 Graph Parsing Neural Network for HOI

3.1 Formulation

For HOI understanding, human and objects are represented by nodes, and their
relations are deﬁned as edges. Given a complete HOI graph that includes all the
possible relationships among human and objects, we want to automatically infer
a parse graph by keeping the meaningful edges and labeling the nodes.

Graph Parsing Neural Networks (ECCV 2018)

5

Fig. 2. Illustration of the forward pass of GPNN. GPNN takes node and edge
features as input, and outputs a parse graph in a message passing fasion. The struc-
ture of the parse graph is given by a soft adjacency matrix. It is computed by the link
function based on the features (or hidden node states). The darker the color in the
adjacency matrix, the stronger the connectivity is. Then message functions compute
incoming messages for each node as a weighted sum of the messages from other nodes.
Thicker edges indicate larger information ﬂows. The update functions update the hid-
den internal states of each node. Above process is repeated for several steps, iteratively
and jointly learning the computation of graph structures and message passing. Finally,
for each node, the readout functions output HOI action or object labels from the hidden
node states. See § 3 for more details.

Formally, let G = (V, E, Y) denote the complete HOI graph. Nodes v ∈ V take
unique values from {1, · · · , |V|}. Edges e ∈ E are two-tuples e = (v, w) ∈ V × V.
Each node v has a output state yv ∈ Y that takes a value from a set of labels
{1, · · · , Yv} (e.g., actions). A parse graph g = (Vg, Eg, Yg) is a sub-graph of G,
where Vg ⊆ V and Eg ⊆ E. Given the node features Γ V and edge features Γ E , we
want to infer the optimal parse graph g∗ that best explains the data according
to a probability distribution p:

g∗ = argmax

p(g|Γ, G) = argmax

p(Vg, Eg, Yg|Γ, G)

g

= argmax

p(Yg|Vg, Eg, Γ )p(Vg, Eg|Γ, G)

g

g

(1)

where Γ = {Γ V , Γ E }. Here p(Vg, Eg|Γ, G) evaluates the graph structure, and
p(Yg|Vg, Eg, Γ ) is the labeling probability for the nodes in the parse graph.

This formulation provides us a principled guideline for designing the GPNN.
We design the network to approximate the computations of argmaxg p(Vg, Eg|Γ, G)
and argmaxg p(Yg|Vg, Eg, Γ ). We introduce four types of functions as individual
modules in the forward pass of a GPNN: link functions, message functions, up-
date functions, and readout functions (as illustrated in Fig. 2). The link functions
L(·) estimate the graph structure, giving an approximation of p(Vg, Eg|Γ, G). The
message, update and readout functions together resemble the belief propagation
process and approximate argmaxYg p(Yg|Vg, Eg, Γ ).

6

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Speciﬁcally, the link function (

) takes edge features ( ) as input and infers
the connectivities between nodes. The soft adjacency matrix ( ) is thus con-
structed and used as weights for messages passing through edges between nodes.
),
The incoming messages for a node are summarized by the message function (
then the hidden embedding state of the node is updated based on the messages
by an update function ( ). Finally, readout functions ( ) compute the target
outputs for each nodes. Those four types of functions are deﬁned as follows:
Link Function. We ﬁrst infer an adjacency matrix that represents connectivities
(i.e., the graph structure) between nodes by a link function. A link function L(·)
takes the node features Γ V , and edge features Γ E as input and outputs an
adjacency matrix A ∈ [0, 1]|V|×|V|:

Avw = L(Γv, Γw, Γvw)

(2)

where Avw denotes the (v, w)-th entry of the matrix A. Here we overload the
notation and let Γv denote node features and Γvw denote edge features. In this
way, the structure of a parse graph g can be approximated by the adjacency
matrix. Then we start to propagate messages over the parse graph, where the
soft adjacency matrix controls the information to be passed through edges.
Message and Update Functions. Based on the learned graph structure, the
message passing algorithm is adopted for inference of node labels. During belief
propagation, the hidden states of the nodes are iteratively updated by communi-
cating with other nodes. Specially, message functions M (·) summarize messages
to nodes coming from other nodes, and update functions U (·) update the hidden
node states according to the incoming messages. At each iteration step s, the
two functions computes:

ms

v =

(cid:88)

w

AvwM (hs−1

v

, hs−1

w , Γvw)

v = U (hs−1
hs

v

, ms
v)

where ms
v is the summarized incoming message for node v at s-th iteration
and hs
v is the hidden state for node v. The node connectivity A encourages the
information ﬂow between nodes in the parse graph. The message passing phase
runs for S steps towards convergence. At the ﬁrst step, the node hidden states
h0
v are initialized by node features Γv.
Readout Function. Finally, for each node, hidden state is fed into a readout
function to output a label:

yv = R(hS

v ).

v (node embeddings).

Here the readout function R(·) computes output yv for node v by activating its
hidden state hS
Iterative Parsing. Based on the above four functions, the messages are passed
along the graph and weighted by the learned adjacency matrix A. We further
extend above process into a joint learning framework that iteratively infers the
graph structure and propagates the information to infer node labels. In particu-
lar, instead of learning A only at the beginning, we iteratively infer A with the

(3)

(4)

(5)

Graph Parsing Neural Networks (ECCV 2018)

7

updated node information and edge features at each step s:

As

vw = L(hs−1

v

, hs−1

w , ms−1

vw ).

Then the messages in Eq. 3 are redeﬁned as:

ms

v =

(cid:88)

w

As

vwM (hs−1

v

, hs−1

w , Γvw).

(6)

(7)

In this way, both the graph structure and the message update can be jointly and
iteratively learned in a uniﬁed framework. In practice, we ﬁnd such a strategy
would bring better performance (detailed in § 4.3).

In next section, we show that by implementing each function by neural net-
works, the entire system is diﬀerentiable end-to-end. Hence all the parameters
can be learned using gradient-based optimization.

3.2 Network Architecture

Link Function. Given the complete HOI graph G = (V, E, Y), we use dV and dE
to denote the dimension of the node features and the edge features, respectively.
In a message passing step s, we ﬁrst concatenate all the node features (hidden
vw ∈ RdE }v,w to
states) {hs
form a feature matrix F s ∈ R|V |×|V |×(2dV +dE ) (see
in Fig. 2). The link function
is deﬁned as a small neural network with one or several convolutional layer(s)
(with 1 × 1 × (2dV + dE) kernels) and a sigmoid activation. Then the adjacency
matrix As ∈ [0, 1]|V|×|V| can be computed as:

v ∈ RdV }v and all the edge features (messages) {ms

As = σ(WL ∗ F s),

(8)

where WL is the learnable parameters of the link function network L(·) and ∗
denotes conv operation. The sigmoid operation σ(·) is for normalizing the values
of the elements of As into [0, 1]. The essential eﬀect of multiple convolutional
layers with 1 × 1 kernels is similar to fully connected layers applied to each
individual edge features, except that the ﬁlter weights are shared by all the
edges. In practice, we ﬁnd such operation generates good enough results and
leads to a high computation eﬃciency.

For spatial-temporal problems where the adjacency matrices should account
for the previous states, we use convolutional LSTMs [39] for modeling L(·) in
temporal domain. At time t, the link function takes F s,t as input features and the
previous adjacency matrix As,t−1 as hidden state: As,t = convLST M (F s,t, As,t−1).
Again, the kernel size for the conv layer in convLSTM is 1 × 1 × (2dV + dE).
Message Function. In our implementation, the message function M (·) in Eq. 3
is computed by:

M (hv, hw, Γvw) = [WM

V hv, WM

V hw, WM

E Γvw],

(9)

where [., .] denotes concatenation. It concatenates the outputs of linear trans-
forms (i.e., fully connected layers parametrized by WM
E ) that takes
node hidden states hv or edge features Γvw as input.

V and WM

8

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Update Function. Recurrent neural networks [10, 18] are natural choices for
simulating the iterative update process, as done by previous works [12]. Here we
apply Gated Recurrent Unit (GRU) [5] as the update function, because of its
recurrent nature and smaller amount of parameters. Thus the update function
in Eq. 4 is implemented as:

v = U (hs−1
hs

v

, ms

v) = GRU (hs−1

v

, ms

v),

(10)

v is the hidden state and ms

where hs
in [25], the GRU is more eﬀective than vanilla recurrent neural networks.
Readout Function. A typical implementation of readout functions is com-
bining several fully connected layers (parameterized by WR) followed by an
activation function:

v is used as input features. As demonstrated

yv = R(hS

v ) = ϕ(WRhS

v ).

(11)

Here the activation function ϕ(·) can be used as softmax (one-class outputs) or
sigmoid (multi-class outputs) according to diﬀerent HOI tasks.

In this way, the entire GPNN is implemented to be fully diﬀerentiable and
end-to-end trainable. The loss for speciﬁc HOI task can be computed for the
outputs of readout functions, and the error can propagate back according to
chain rule. In next section, we will oﬀer more details for implementing GPNN
for HOI tasks on spatial and spatial-temporal settings and present qualitative
as well as quantitative results.

4 Experiments

To verify the eﬀectiveness and generic applicability of GPNN, we perform ex-
periments on two HOI problems: i) HOI detection in images [1, 17], and ii) HOI
recognition and anticipation from videos [22]. The ﬁrst experiment is performed
on HICO-DET [1] and V-COCO [17] datasets, showing that our approach is
scalable to large datasets (about 60K images in total) and achieves a good de-
tection accuracy over a large number of classes (more than 600 classes of HOIs).
The second experiment is reported on CAD-120 dataset [22], showing that our
method is well applicable to spatial-temporal domains.

4.1 Human-Object Interaction Detection in Images

For HOI detection in an image, the goal is to detect pairs of a human and an
object bounding box with an interaction class label connecting them.
Datasets. We use HICO-DET [1] and V-COCO [17] datasets for benchmarking
our GPNN model. HICO-DET provides more than 150K annotated instances
of human-object pairs in 47,051 images (37,536 training and 9,515 testing). It
has the same 80 object categories as MS-COCO [28] and 117 action categories.
V-COCO is a subset of MS-COCO [28]. It consists of a total of 10,346 images
with 16,199 people instances, where ∼2.5K images in the train set, ∼2.8K images
for validation and ∼4.9K images for testing. Each annotated person has binary

Graph Parsing Neural Networks (ECCV 2018)

9

Table 1. HOI detection results (mAP) on HICO-DET dataset [1]. Higher
values are better. The best scores are marked in bold.

Methods

Full (mAP %) ↑ Rare (mAP %) ↑ Non-rare (mAP %) ↑

Random
Fast-RCNN(union) [13]
Fast-RCNN(score) [13]
HO-RCNN [1]
HO-RCNN+IP [1]
HO-RCNN+IP+S [1]
Gupta et al. [17]
Shen et al. [38]
InteractNet [14]
GPNN
Performance Gain(%)

1.35 × 10−3
1.75
2.85
5.73
7.30
7.81
9.09
6.46
9.94
13.11
31.89

5.72 × 10−4
0.58
1.55
3.21
4.68
5.37
7.02
4.24
7.16
9.34
30.45

1.62 × 10−3
2.10
3.23
6.48
8.08
8.54
9.71
7.12
10.77
14.23
32.13

Fig. 3. HOI detection results on HICO-DET [1] test images. Human and ob-
jects are shown in red and green rectangles, respectively. Best viewed in color.

labels for 26 diﬀerent action classes. Note that three actions (i.e., cut, eat, and
hit) are annotated with two types of targets: instrument and direct object.
Implementation Details. Humans and objects are represented by nodes in
the graph, while human-object interactions are represented by edges. In this ex-
periment, we use a pre-trained deformable convolutional network [6] for object
detection and features extraction. Based on the detected bounding boxes, we ex-
tract node features (7 × 7 × 80) from the position-sensitive region of interest (PS
RoI) pooling layer from the deformable ConvNet. We extract the edge feature
from a combined bounding box, i.e., the smallest bounding box that contains
both two nodes’ bounding boxes. The functions of GPNN are implemented as
follows. We use a convolutional network (128-128-1)-Sigmoid(·) with 1×1 kernels
for the link function. The message functions are composed of a fully connected
layer, concatenation, and summation. For a node v, the neighboring node feature
Γw and edge feature Γvw are passed through a fully connected layer and concate-
nated. The ﬁnal incoming message is a weighted sum of messages from all neigh-
boring nodes. Speciﬁcally, the message for node v coming from node w through
edge e = (v, w) is the concatenation of output from FC(dV -dV ) and FC(dE-dE).
A GRU(dV ) is used for the update function. The propagation step number S

10

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 2. HOI detection results (mAP) on V-COCO [17] dataset. Legend: Set
1 indicates 18 HOI actions with one object, and Set 2 corresponds to 3 HOI actions
(i.e., cut, eat, hit) with two objects (instrument and object).

Method

Set 1 (mAP %) ↑

Set 2 (mAP %) ↑ Ave. (mAP %) ↑

Gupta et al. [17]
InteractNet [14]
GPNN
Performance Gain(%)

33.5
42.2
44.5
5.5

26.7
33.2
42.8
28.9

31.8
40.0
44.0
10.0

Fig. 4. HOI detection results on V-COCO [17] test images. Human and objects
are shown in red and green rectangles, respectively. Best viewed in color.

is set to be 3. For the readout function, we use a FC(dV -117)-Sigmoid(·) and
FC(dV -26)-Sigmoid(·) for HICO-DET and V-COCO, respectively.

The probability of an HOI label of a human-object pair is given by the
product of the ﬁnal output probabilities from the human node and the object
node. We employ an L1 loss for the adjacency matrix. For the node outputs, we
use a weighted multi-class multi-label hinge loss. The reasons are two-folds: the
training examples are not balanced, and it is essentially a multi-label problem
for each node (there might not even exist a meaningful human-object interaction
for detected humans and objects).

Our model is implemented using PyTorch and trained with a machine with
a single Nvidia Titan Xp GPU. We start with a learning rate of 1e-3, and the
rate decays every 5 epochs by 0.8. The training process takes about 20 epochs
(∼15 hours) to roughly converge with a batch size of 32.
Comparative Methods. We compare our method with eight baselines: (1)
Fast-RCNN (union) [13]: for each human-object proposal from detection results,
their attention windows are used as the region proposal for Fast-RCNN. (2) Fast-
RCNN (score) [13]: given human-object proposals, HOI is predicted by linearly
combining the human and object detection scores. (3) HO-RCNN [1]: a multi-
stream architecture with a ConvNet to classify human, object and human-object
proposals, respectively. The ﬁnal output is computed by combining the scores
from all the three streams. (4) HO-RCNN+IP [1] and (5) HO-RCNN+IP+S [1]:
HO-RCNN with additional components. Interaction Patterns (IP) acts as a at-
tention ﬁlter to images. S is an extra path with a single neuron that uses the

Graph Parsing Neural Networks (ECCV 2018)

11

raw object detection score to produce an oﬀset for the ﬁnal detection. More
detailed descriptions of above ﬁve baselines can be found in [1]. (6) Gupta et
al. [17]: trained based on Fast-RCNN [13]. We use the scores reported in [14].
(7) Shen et al. [38]: ﬁnal predictions are from two Faster RCNN [36] based net-
works which are trained for predicting verb and object classes, respectively. (8)
InteractNet [14]: a modiﬁed Faster RCNN [36] with an additional human-centric
branch that estimates an action-speciﬁc density map for locating objects.
Experiment Results. Following the standard settings in HICO-DET and V-
COCO benchmarks, we evaluate HOI detection using mean average precision
(mAP). An HOI detection is considered as a true positive when the human
detection, the object detection, and the interaction class are all correct. The
human and object bounding boxes are considered as true positives if they overlap
with a ground truth bounding boxes of the same class with an intersection over
union (IoU) greater than 0.5. For HICO-DET dataset, we report the mAP over
three diﬀerent HOI category sets: i) all 600 HOI categories in HICO (Full); ii)
138 HOI categories with less than 10 training instances (Rare); and iii) 462 HOI
categories with 10 or more training instances (Non-Rare). For V-COCO dataset,
since we concentrate on HOI detection, we report the mAP on three groups: i)
18 HOI action classes with one target object; ii) 3 HOI categories with two types
of objects; iii) all 24 (=18 + 3 × 2) HOI classes. Results are evaluated on the test
sets and reported in Table 1 and Table 2.

As shown in Table 1, the proposed GPNN substantially outperforms the
comparative methods, achieving 31.89%, 30.45%, and 32.13% improvement
over the second best methods on the three HOI category sets on the HICO-
DET dataset. The results on V-COCO dataset (in Table 2) also consistently
demonstrate the superior performance of the proposed GPNN. Two important
conclusions can be drawn from the results: i) our method is scalable to large
datasets; ii) and our method performs better than pure neural network. Some
visual results can be found in Fig. 3 and Fig. 4.

4.2 Human-Object Interaction Recognition in Videos

The goal of this experiment is to detect and predict the human sub-activity
labels and object aﬀordance labels as the human-object interaction progresses
in videos. The problem is challenging since it involves complex interactions that
humans make with multiple objects, and objects also interact with each other.
CAD-120 dataset [22]. It has 120 RGB-D videos of 4 subjects performing 10
activities, each of which is a sequence of sub-activities involving 10 actions (e.g.,
reaching, opening), and 12 object aﬀordances (e.g., reachable, openable) in total.
Implementation Details. The link function is implemented as: convLSTM(1024-
1024-1024-1)-Sigmoid(·) (i.e., a four-layer convLSTM). We use the same ar-
chitecture as the previous experiment for message functions and update func-
tions: [FC(dV -dV ), FC(dE-dE)] for message function and GRU(dV ) for update
function. The propagation step number S is set to be 3. We use a FC(dV -10)-
Softmax(·) and a FC(dV -12)-Softmax(·) for readout functions of sub-activity and
object aﬀordance detection/anticipation, respectively. We employ an L1 loss for

12

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 3. Human activity detection and future anticipation results on CAD-
120 [22] dataset, measured via F1-score.

Method

Detection (F1-score) ↑
Object
Sub
Aﬀordance(%)
-activity(%)

Anticipation (F1-score) ↑

Sub
-activity(%)

Object
Aﬀordance(%)

ATCRF [22]
S-RNN [20]
S-RNN (multi-task) [20]
GPNN
Performance Gain(%)

80.4
83.2
82.4
88.9
8.1

81.5
88.7
91.1
88.8
-

37.9
62.3
65.6
75.6
15.2

36.7
80.7
80.9
81.9
1.2

Fig. 5. Confusion matrices of HOI detection (a)(b) and anticipation (c)(d)
results on CAD-120 [22] dataset. Zoom in for more details.

the adjacency matrix and a cross entropy loss for the node outputs. We use the
publicly available node and edge features from [23].
Comparative Methods. We compare our method with two baselines: antici-
patory temporal CRF (ATCRF) [22] and structural RNN (S-RNN) [20]. ATCRF
is a top-performing graphical model approach for this problem, while S-RNN is
the state-of-art method using structured neural networks. ATCRF models the
human activities through a spatial-temporal conditional random ﬁeld. S-RNN
casts a pre-deﬁned spatial-temporal graph as an RNN mixture by representing
nodes and edges as LSTMs.
Experiment Results. In Table 3 we show the quantitative comparison of
our method with other competitors. It shows the F1-scores averaged over all
classes on detection and activity anticipation tasks. GPNN greatly improves
over ATCRF and S-RNN, especially on anticipation task. Our method outper-
forms the other two for the following reasons. i) Comparing to ATCRF limited
to the Markov assumption, our method allows arbitrary graph structures with
improved representation ability. ii) Our method enjoys the beneﬁt of deep in-
tegration of graphical models and neural networks and can be learned in an
end-to-end manner. iii) Rather than relying on a pre-ﬁxed graph structure as in
S-RNN, we infer the graph structure via learning an adjacency matrix and thus
be able to control the information ﬂow between nodes during massage passing.
Fig. 5 show the confusion matrices for detecting and predicting the sub-activities
and object aﬀordances, respectively. From above results we can draw two im-

Graph Parsing Neural Networks (ECCV 2018)

13

Fig. 6. HOI detection results on a “cleaning objects” activity on CAD-120 [22]
dataset. Human are shown in red rectangle. Two objects are shown in green and blue
rectangles, respectively. Detection and anticipation results are shown by diﬀerent bars.
For anticipation task, the label of the sub-activity at time t is anticipated at time t-1.

portant conclusions: i) our method is well applicable to the spatio-temporal
domain; and ii) our method outperforms pure graphical models (e.g., ATCRF)
and deep networks with pre-ﬁxed graph structures (e.g., S-RNN). Fig. 6 shows a
qualitative visualization of “cleaning objects”. We show one representative frame
for each sub-activity as well as the corresponding detections and anticipations.

4.3 Ablation Study

In this section, we analyze the contributions of diﬀerent model components to
the ﬁnal performance and examine the eﬀectiveness of our main assumptions.
Table 4 shows the detailed results on all three datasets.
Integration of DNN with Graphical Model. We ﬁrst examine the inﬂu-
ence of integrating DNN with a graphical model. We directly feed the features,
which are originally used for GPNN, into diﬀerent fully connected networks for
predicting HOI action or object classes. From Table 4, we can observe the per-
formance of w/o graph is signiﬁcantly worse than GPNN model over various
HOI datasets. This supports our view that modeling high-level structures and
leveraging learning capabilities of DNNs together is essential for HOI tasks.
GPNN with Fixed Graph Structures. In § 3, GPNN automatically infers
graph structures (i.e., parse graph) via learning a soft adjacency matrix. To
assess this strategy, we ﬁx all the entries in the soft adjacency matrices to be
constant 1. This way the graph structures are ﬁxed and the information ﬂow
between nodes are not weighted. For constant graph baseline, we see obvious
performance decrease, compared with the full GPNN model. This indicates that
inferring graph structures is critical to get reasonable performance.
GPNN without Supervision on Link Functions. We perform experiments
by turning oﬀ the L1 loss on adjacency matrices (w/o graph loss in Table 4). We
can observe that the intermediate L1 loss is eﬀective, further verifying our design
to learn the graph structure. Another interesting observation is that training

14

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 4. Ablation study of GPNN model. Higher values are better.

Aspect Method

Set 1 Set 2 Ave. Full Rare

V-COCO [17]
HOI Detection
mAP(%) ↑

HICO-DET [1]
HOI Detection
mAP(%) ↑

CAD-120 [22]

HOI Detec.
F1-score(%) ↑
Object
Sub-
Aﬀ.(%)
activity

HOI Antici.
F1-score(%) ↑
Object
Sub-
Aﬀ.(%)
activity

Non-
rare

44.5 42.8 44.0 13.11 9.34 14.23 88.9

88.8

75.6

81.9

GPNN
(3 iterations)

w/o graph
graph
constant graph
structure w/o graph loss

2.04
9.62
27.4 30.0 28.1
1.94 10.79
34.6 33.3 34.3
8.72
6.24
37.7 40.5 38.4
w/o joint parsing 43.6 39.4 42.5 10.17 5.81 11.47
42.0 40.7 41.7 11.38 7.27 12.61
44.1 42.2 43.6 12.37 9.01 13.38
43.6 40.9 42.9 12.39 8.95 13.41

7.88
8.75
8.15

iterative 1 iteration
learning 2 iterations
4 iterations

50.2
85.3
85.2
79.3
80.5
87.9
87.9

20.8
85.6
85.8
79.2
80.7
86.1
85.7

32.3
73.8
74.7
74.7
75.2
76.1
75.5

19.6
79.1
79.2
80.3
81.1
81.5
80.6

the model without this loss has a similar eﬀect to training with constant graph.
Hence supervision on the graph is fairly important.
Jointly Learning Parse Graph and Message Passing. We next study the
eﬀect of jointly learning graph structures and message passing. By isolating graph
parsing from message passing, we obtain w/o joint parsing, where the adjacency
matrices are directly computed by link functions from edge features at the be-
ginning. We observe a performance decrease in Table 4, showing that learning
graph structures and message passing together indeed boost the performance.
Iterative Learning Process. Next we examine the eﬀect of iterative message
passing, we report three baselines: 1 iteration, 2 iterations, and 4 iterations,
which correspond to the results from diﬀerent message passing iterations. The
baseline GPNN (ﬁrst row in Table 4) are the results after three iterations. From
the results we observe that the iterative learning process is able to gradually
improve the performance in general. We also observe that when the iteration
round is increased to a certain extent, the performance drops slightly.

5 Conclusion

In this paper, we propose Graph Parsing Neural Network (GPNN) for inferring
a parse graph in an end-to-end manner. The network can be decomposed into
four distinct functions, namely link functions, message functions, update func-
tions and readout functions, for iterative graph inference and message passing.
GPNN provides a generic HOI representation that is applicable in both spa-
tial and spatial-temporal domains. We demonstrate a substantial performance
gain on three HOI datasets, showing the eﬀectiveness of the proposed framework.

Acknowledgments. The authors thank Prof. Ying Nian Wu from UCLA Statis-
tics Department for helpful comments on this work. This research is supported by
DARPA XAI N66001-17-2-4029, ONR MURI N00014-16-1-2007, ARO W911NF1810296,
and N66001-17-2-3602.

Graph Parsing Neural Networks (ECCV 2018)

15

References

interactions (2018)

1. Chao, Y.W., Liu, Y., Liu, X., Zeng, H., Deng, J.: Learning to detect human-object

2. Chao, Y.W., Wang, Z., He, Y., Wang, J., Deng, J.: HICO: A benchmark for rec-

ognizing human-object interactions in images. In: ICCV (2015)

3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Se-
mantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected CRFs. PAMI (2016)

4. Chen, L.C., Schwing, A., Yuille, A., Urtasun, R.: Learning deep structured models.

In: ICML (2015)

5. Cho, K., Van Merri¨enboer, B., Bahdanau, D., Bengio, Y.: On the properties of
neural machine translation: Encoder–decoder approaches. Syntax, Semantics and
Structure in Statistical Translation p. 103 (2014)

6. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convo-

lutional networks. In: ICCV (2017)

7. Deﬀerrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on

graphs with fast localized spectral ﬁltering. In: NIPS (2016)

8. Delaitre, V., Sivic, J., Laptev, I.: Learning person-object interactions for action

recognition in still images. In: NIPS (2011)

9. Desai, C., Ramanan, D.: Detecting actions, poses, and objects with relational

phraselets. In: ECCV (2012)

10. Elman, J.L.: Finding structure in time. Cognitive science (1990)
11. Fang, H.S., Xu, Y., Wang, W., Zhu, S.C.: Learning pose grammar to encode human

body conﬁguration for 3d pose estimation. In: AAAI (2018)

12. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message

passing for quantum chemistry. In: ICML (2017)

13. Girshick, R.: Fast R-CNN. In: ICCV (2015)
14. Gkioxari, G., Girshick, R., Doll´ar, P., He, K.: Detecting and recognizing human-

object interactions. In: CVPR (2018)

15. Gupta, A., Davis, L.S.: Objects in action: An approach for combining action un-

derstanding and object perception. In: CVPR (2007)

16. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using

spatial and functional compatibility for recognition. PAMI (2009)

17. Gupta, S., Malik, J.: Visual semantic role labeling. arXiv preprint arXiv:1505.04474

18. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

(2015)

(1997)

19. Hu, J.F., Zheng, W.S., Lai, J., Gong, S., Xiang, T.: Recognising human-object

interaction via exemplar based modelling. In: ICCV (2013)

20. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: Deep learning on

spatio-temporal graphs. In: CVPR (2016)

21. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional

networks. In: ICLR (2017)

22. Koppula, H.S., Saxena, A.: Anticipating human activities using object aﬀordances

for reactive robotic response. PAMI (2016)

23. Koppula, H.S., Gupta, R., Saxena, A.: Learning human activities and object af-
fordances from RGB-D videos. The International Journal of Robotics Research
(2013)

16

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

24. Li, R., Tapaswi, M., Liao, R., Jia, J., Urtasun, R., Fidler, S.: Situation recognition

with graph neural networks. In: ICCV (2017)

25. Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R.: Gated graph sequence neural

26. Liang, X., Lin, L., Shen, X., Feng, J., Yan, S., Xing, E.P.: Interpretable structure-

27. Liang, X., Shen, X., Feng, J., Lin, L., Yan, S.: Semantic object parsing with graph

networks. In: ICLR (2016)

evolving lstm. In: ICCV (2017)

lstm. In: ECCV (2016)

28. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)
29. Mallya, A., Lazebnik, S.: Learning models for actions and person-object interac-

tions with transfer to question answering. In: ECCV (2016)

30. Marino, K., Salakhutdinov, R., Gupta, A.: The more you know: Using knowledge

graphs for image classiﬁcation. In: CVPR (2016)

31. Monti, F., Boscaini, D., Masci, J., Rodol`a, E., Svoboda, J., Bronstein, M.M.: Ge-
ometric deep learning on graphs and manifolds using mixture model cnns. CVPR
(2016)

32. Niepert, M., Ahmed, M., Kutzkov, K.: Learning convolutional neural networks for

graphs. In: ICML (2016)

33. Park, S., Nie, X., Zhu, S.C.: Attribute and-or grammar for joint parsing of human

pose, parts and attributes. PAMI (2017)

34. Qi, S., Huang, S., Wei, P., Zhu, S.C.: Predicting human activities using stochastic

grammar. In: ICCV (2017)

35. Qi, S., Jia, B., Zhu, S.C.: Generalized earley parser: Bridging symbolic grammars

and sequence data for future prediction. In: ICML (2018)

36. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

37. Seo, Y., Deﬀerrard, M., Vandergheynst, P., Bresson, X.: Structured sequence mod-
eling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659
(2016)

38. Shen, L., Yeung, S., Hoﬀman, J., Mori, G., Fei-Fei, L.: Scaling human-object in-

teraction recognition through zero-shot learning (2018)

39. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convolutional
LSTM network: A machine learning approach for precipitation nowcasting. In:
NIPS (2015)

40. Simonovsky, M., Komodakis, N.: Dynamic edge-conditioned ﬁlters in convolutional

neural networks on graphs. CVPR (2017)

41. Teney, D., Liu, L., Hengel, A.v.d.: Graph-structured representations for visual ques-

tion answering. In: CVPR (2017)

42. Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint training of a convolutional
network and a graphical model for human pose estimation. In: NIPS (2014)
43. Wang, W., Xu, Y., Shen, J., Zhu, S.C.: Attentive fashion grammar network for
fashion landmark detection and clothing category classiﬁcation. In: CVPR (2018)
44. Wu, Z., Lin, D., Tang, X.: Deep markov random ﬁeld for image modeling. In:

ECCV (2016)

45. Xia, F., Zhu, J., Wang, P., Yuille, A.L.: Pose-guided human parsing by an And/Or

graph using pose-context features. In: AAAI (2016)

46. Xu, D., Zhu, Y., Choy, C.B., Fei-Fei, L.: Scene graph generation by iterative mes-

sage passing. In: ICCV (2017)

47. Yao, B., Fei-Fei, L.: Grouplet: A structured image representation for recognizing

human and object interactions. In: CVPR (2010)

Graph Parsing Neural Networks (ECCV 2018)

17

48. Yao, B., Fei-Fei, L.: Modeling mutual context of object and human pose in human-

object interaction activities. In: CVPR (2010)

49. Yao, B., Jiang, X., Khosla, A., Lin, A.L., Guibas, L., Fei-Fei, L.: Human action
recognition by learning bases of action attributes and parts. In: ICCV (2011)
50. Yuan, Y., Liang, X., Wang, X., Yeung, D.Y., Gupta, A.: Temporal dynamic graph

LSTM for action-driven video object detection. In: ICCV (2017)

51. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: ICCV
(2015)

8
1
0
2
 
g
u
A
 
3
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
6
9
7
0
.
8
0
8
1
:
v
i
X
r
a

Learning Human-Object Interactions by
Graph Parsing Neural Networks

Siyuan Qi∗1,2, Wenguan Wang∗1,3, Baoxiong Jia1,4,
Jianbing Shen†3,5, and Song-Chun Zhu1,2

1 University of California, Los Angeles
2 International Center for AI and Robot Autonomy (CARA)
3 Beijing Institute of Technology
4 Peking University
5 Inception Institute of Artiﬁcial Intelligence

syqi@cs.ucla.edu

wenguanwang.ai@gmail.com baoxiongjia@ucla.edu

shenjianbing@bit.edu.cn

sczhu@stat.ucla.edu

Abstract. This paper addresses the task of detecting and recognizing
human-object interactions (HOI) in images and videos. We introduce the
Graph Parsing Neural Network (GPNN), a framework that incorporates
structural knowledge while being diﬀerentiable end-to-end. For a given
scene, GPNN infers a parse graph that includes i) the HOI graph struc-
ture represented by an adjacency matrix, and ii) the node labels. Within
a message passing inference framework, GPNN iteratively computes the
adjacency matrices and node labels. We extensively evaluate our model
on three HOI detection benchmarks on images and videos: HICO-DET,
V-COCO, and CAD-120 datasets. Our approach signiﬁcantly outper-
forms state-of-art methods, verifying that GPNN is scalable to large
datasets and applies to spatial-temporal settings. The code is available
at https://github.com/SiyuanQi/gpnn.

Keywords: Human-Object Interaction · Message Passing · Graph Pars-
ing · Neural Networks

1 Introduction

The task of human-object interaction (HOI) understanding aims to infer the
relationships between human and objects, such as “riding a bike” or “washing a
bike”. Beyond traditional visual recognition of individual instances, e.g., human
pose estimation, action recognition, and object detection, recognizing HOIs re-
quires a deeper semantic understanding of image contents. Recently, deep neural
networks (DNNs) have shown impressive progress on above individual tasks of
instance recognition, while relatively few methods [1, 2, 14, 38] were proposed for
HOI recognition. This is mainly because it requires reasoning beyond perception,
by integrating information from human, objects, and their complex relationships.

∗ Equal contribution. † Corresponding author.

2

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Fig. 1. Illustration of the proposed GPNN for learning HOI. GPNN oﬀers a
generic HOI representation that applies to (a) HOI detection in images and (b) HOI
recognition in videos. With the integration of graphical model and neural network,
GPNN can iteratively learn/infer the graph structures (a.v) and message passing (a.vi).
The ﬁnal parse graph explains a given scene with the graph structure (e.g., the link
between the person and the knife) and the node labels (e.g., lick). A thicker edge
corresponds to stronger information ﬂow between nodes in the graph.

In this paper, we propose a novel model, Graph Parsing Neural Network
(GPNN), for HOI recognition. The proposed GPNN oﬀers a general framework
that explicitly represents HOI structures with graphs and automatically parses
the optimal graph structures in an end-to-end manner. In principle, it is an
generalization of Message Passing Neural Network (MPNN) [12]. An overview
of GPNN is shown in Fig. 1. The following two aspects motivate our design.

First, we seek a uniﬁed framework that utilizes the learning capability of neu-
ral networks and the power of graphical representations. Recent deep learning
based HOI models showed promising results, but few touched how to interpret
well and explicitly leverage spatial and temporal dependencies and human-object
relations in such structured task. Aiming for this, we introduce GPNN. It in-
herits the complementary strengths of neural networks and graphical models,
for forming a coherent HOI representation with strong learning ability. Specif-
ically, with the structured representation of an HOI graph, the rich relations
are explicitly utilized, and the information from individual elements can be ef-
ﬁciently integrated and broadcasted over the structures. The whole model and
message passing operations are well-deﬁned and fully diﬀerentiable. Thus it can
be eﬃciently learned from data in an end-to-end manner.

Second, based on our eﬃcient HOI representation and learning power, GPNN
applies to diverse HOI tasks in both static and dynamic scenes. Previous studies
for HOI achieved good performance in their speciﬁc domains (spatial [1, 14] or
temporal [20, 34, 35]). However, none of them addresses a generic framework for
representing and learning HOI in both images and videos. The key diﬃculty lies
in the diverse relations between components. Given a set of human and objects
candidates, there may exist an uncertain number of human-object interaction
pairs (see Fig. 1 (a.ii) as an example). The relations become more complex after

Graph Parsing Neural Networks (ECCV 2018)

3

taking temporal factors into consideration. Thus pre-ﬁxed graph structures, as
adopted by most previous graphical or structured DNN models [11, 20, 22, 43],
are not an optimal choice. Seeking a better generalization ability, GPNN incor-
porates an essential link function for addressing the problem of graph structure
learning. It learns to infer the adjacency matrix in an end-to-end manner and
thus can infer a parse graph that explicitly explains the HOI relations. With such
learnable graph structure, GPNN could also limit the information ﬂow from ir-
relevant nodes while encouraging message to propagate between related nodes,
thus improving graph parsing.

We extensively evaluate the proposed GPNN on three HOI datasets, namely
HICO-DET [1], V-COCO [17] and CAD-120 [22], for HOI detection from im-
ages (HICO-DET, V-COCO) and HOI recognition and anticipation in spatial-
temporal settings (CAD-120). The experimental results verify the generality
and scalability of our GPNN based HOI representation and show substantial
improvements over state-of-the-art approaches, including pure graphical models
and pure neural networks. We also demonstrate GPNN outperforms its variants
and other graph neural networks with pre-ﬁxed structures.

This paper makes three major contributions. First, we propose the GPNN
that incorporates structural knowledge and DNNs for learning and inference.
Second, with a set of well deﬁned modular functions, GPNN addresses the HOI
problem by jointly performing graph structure inference and message passing.
Third, we empirically show that GPNN oﬀers a scalable and generic HOI rep-
resentation that applies to both static and dynamic settings.

2 Related Work

Human-Object Interaction. Reasoning human actions with objects (like “play-
ing baseball”, “playing guitar”), rather than recognizing individual actions (“play-
ing”) or object instances (“baseball”, “guitar”), is essential for a more compre-
hensive understanding of what is happening in the scene. Early work in HOI
understanding studied Bayesian model [15, 16], utilized contextual relationship
between human and objects [47–49], learned structured representations with spa-
tial interaction and context [8], exploited compositional models [9], or referred
to a set of HOI exemplars [19]. They were mainly based on handcrafted features
(e.g., color, HOG, and SIFT) with object and human detectors. More recently,
inspired by the notable success of deep learning and the availability of large-scale
HOI datasets [1,2], several deep learning based HOI models were then proposed.
Speciﬁcally, Mallya et al. [29] modiﬁed Fast RCNN model [13] for HOI recogni-
tion, with the assistance of Visual Question Answering (VQA). In [38], zero-shot
learning was applied for addressing the long-tail problem in HOI recognition.
In [1], the human proposals, object regions, and their combinations were fed
into a multi-stream network for tackling the HOI detection problem. Gkioxari
et al. [14] estimated an action-type speciﬁc density map for identifying the in-
teracted object locations, with a modiﬁed Faster RCNN architecture [36].

4

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Although promising results were achieved by above deep HOI models, we
still observe two unsolved issues. First, they lack a powerful tool to represent
the structures in HOI tasks explicitly and encodes them into modern network ar-
chitectures eﬃciently. Second, despite the successes in speciﬁc tasks, a complete
and generic HOI representation is missing. These approaches can not be easily
extended to HOI recognition from videos. Aiming to address those issues, we in-
troduce GPNN for imposing high-level relations into DNN, leading to a powerful
HOI representation that is applicable in both static and dynamic settings.
Neural Networks with Graphs/Graphical Models. In the literature, some
approaches were proposed to combine graphical models and neural networks.
The most intuitive approach is to build graphical models upon DNN, where
the network that generates features is trained ﬁrst, and its output is used to
compute potential functions for the graphical predictor. Typical methods were
used in human pose estimation [42], human part parsing [33, 45], and semantic
image segmentation [3, 4]. These methods lack a deep integration in the sense
that the computation process of graphical models cannot be learned end-to-
end. Some attempts [7, 21, 31, 32, 37, 40, 44, 51] were made to generalize neural
network operations (e.g., convolutions) directly from regular grids (e.g., images)
to graphs. For the HOI problem, however, a structured representation is needed
to capture the high-level spatial-temporal relations between humans and objects.
Some other work integrated network architectures with graphical models [12,20]
and gained promising results on applications such as scene understanding [24,30,
46], object detection and parsing [27,50], and VQA [41]. However, these methods
only apply to problems that have pre-ﬁxed graph structures. Liang et al. [26]
merged graph nodes using Long Short-Term Memory (LSTM) for human parsing
problem, under the assumption that the nodes are mergeable.

Those methods achieved promising results in their speciﬁc tasks and well
demonstrated the beneﬁt in completing deep architectures with domain-speciﬁc
structures. However, most of them are based on pre-ﬁxed graph structures, and
they have not yet been studied in HOI recognition. In this work, we extend pre-
vious graphical neural networks with learnable graph structures, which well ad-
dresses the rich and high-level relations in HOI problems. The proposed GPNN
can automatically infer the graph structure and utilize that structure for en-
hancing information propagation and further inference. It oﬀers a generic HOI
representation for both spatial and spatial-temporal settings. To the best of our
knowledge, this is a ﬁrst attempt to integrate graph models with neural networks
in a uniﬁed framework to achieve state-of-art results in HOI recognition.

3 Graph Parsing Neural Network for HOI

3.1 Formulation

For HOI understanding, human and objects are represented by nodes, and their
relations are deﬁned as edges. Given a complete HOI graph that includes all the
possible relationships among human and objects, we want to automatically infer
a parse graph by keeping the meaningful edges and labeling the nodes.

Graph Parsing Neural Networks (ECCV 2018)

5

Fig. 2. Illustration of the forward pass of GPNN. GPNN takes node and edge
features as input, and outputs a parse graph in a message passing fasion. The struc-
ture of the parse graph is given by a soft adjacency matrix. It is computed by the link
function based on the features (or hidden node states). The darker the color in the
adjacency matrix, the stronger the connectivity is. Then message functions compute
incoming messages for each node as a weighted sum of the messages from other nodes.
Thicker edges indicate larger information ﬂows. The update functions update the hid-
den internal states of each node. Above process is repeated for several steps, iteratively
and jointly learning the computation of graph structures and message passing. Finally,
for each node, the readout functions output HOI action or object labels from the hidden
node states. See § 3 for more details.

Formally, let G = (V, E, Y) denote the complete HOI graph. Nodes v ∈ V take
unique values from {1, · · · , |V|}. Edges e ∈ E are two-tuples e = (v, w) ∈ V × V.
Each node v has a output state yv ∈ Y that takes a value from a set of labels
{1, · · · , Yv} (e.g., actions). A parse graph g = (Vg, Eg, Yg) is a sub-graph of G,
where Vg ⊆ V and Eg ⊆ E. Given the node features Γ V and edge features Γ E , we
want to infer the optimal parse graph g∗ that best explains the data according
to a probability distribution p:

g∗ = argmax

p(g|Γ, G) = argmax

p(Vg, Eg, Yg|Γ, G)

g

= argmax

p(Yg|Vg, Eg, Γ )p(Vg, Eg|Γ, G)

g

g

(1)

where Γ = {Γ V , Γ E }. Here p(Vg, Eg|Γ, G) evaluates the graph structure, and
p(Yg|Vg, Eg, Γ ) is the labeling probability for the nodes in the parse graph.

This formulation provides us a principled guideline for designing the GPNN.
We design the network to approximate the computations of argmaxg p(Vg, Eg|Γ, G)
and argmaxg p(Yg|Vg, Eg, Γ ). We introduce four types of functions as individual
modules in the forward pass of a GPNN: link functions, message functions, up-
date functions, and readout functions (as illustrated in Fig. 2). The link functions
L(·) estimate the graph structure, giving an approximation of p(Vg, Eg|Γ, G). The
message, update and readout functions together resemble the belief propagation
process and approximate argmaxYg p(Yg|Vg, Eg, Γ ).

6

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Speciﬁcally, the link function (

) takes edge features ( ) as input and infers
the connectivities between nodes. The soft adjacency matrix ( ) is thus con-
structed and used as weights for messages passing through edges between nodes.
),
The incoming messages for a node are summarized by the message function (
then the hidden embedding state of the node is updated based on the messages
by an update function ( ). Finally, readout functions ( ) compute the target
outputs for each nodes. Those four types of functions are deﬁned as follows:
Link Function. We ﬁrst infer an adjacency matrix that represents connectivities
(i.e., the graph structure) between nodes by a link function. A link function L(·)
takes the node features Γ V , and edge features Γ E as input and outputs an
adjacency matrix A ∈ [0, 1]|V|×|V|:

Avw = L(Γv, Γw, Γvw)

(2)

where Avw denotes the (v, w)-th entry of the matrix A. Here we overload the
notation and let Γv denote node features and Γvw denote edge features. In this
way, the structure of a parse graph g can be approximated by the adjacency
matrix. Then we start to propagate messages over the parse graph, where the
soft adjacency matrix controls the information to be passed through edges.
Message and Update Functions. Based on the learned graph structure, the
message passing algorithm is adopted for inference of node labels. During belief
propagation, the hidden states of the nodes are iteratively updated by communi-
cating with other nodes. Specially, message functions M (·) summarize messages
to nodes coming from other nodes, and update functions U (·) update the hidden
node states according to the incoming messages. At each iteration step s, the
two functions computes:

ms

v =

(cid:88)

w

AvwM (hs−1

v

, hs−1

w , Γvw)

v = U (hs−1
hs

v

, ms
v)

where ms
v is the summarized incoming message for node v at s-th iteration
and hs
v is the hidden state for node v. The node connectivity A encourages the
information ﬂow between nodes in the parse graph. The message passing phase
runs for S steps towards convergence. At the ﬁrst step, the node hidden states
h0
v are initialized by node features Γv.
Readout Function. Finally, for each node, hidden state is fed into a readout
function to output a label:

yv = R(hS

v ).

v (node embeddings).

Here the readout function R(·) computes output yv for node v by activating its
hidden state hS
Iterative Parsing. Based on the above four functions, the messages are passed
along the graph and weighted by the learned adjacency matrix A. We further
extend above process into a joint learning framework that iteratively infers the
graph structure and propagates the information to infer node labels. In particu-
lar, instead of learning A only at the beginning, we iteratively infer A with the

(3)

(4)

(5)

Graph Parsing Neural Networks (ECCV 2018)

7

updated node information and edge features at each step s:

As

vw = L(hs−1

v

, hs−1

w , ms−1

vw ).

Then the messages in Eq. 3 are redeﬁned as:

ms

v =

(cid:88)

w

As

vwM (hs−1

v

, hs−1

w , Γvw).

(6)

(7)

In this way, both the graph structure and the message update can be jointly and
iteratively learned in a uniﬁed framework. In practice, we ﬁnd such a strategy
would bring better performance (detailed in § 4.3).

In next section, we show that by implementing each function by neural net-
works, the entire system is diﬀerentiable end-to-end. Hence all the parameters
can be learned using gradient-based optimization.

3.2 Network Architecture

Link Function. Given the complete HOI graph G = (V, E, Y), we use dV and dE
to denote the dimension of the node features and the edge features, respectively.
In a message passing step s, we ﬁrst concatenate all the node features (hidden
vw ∈ RdE }v,w to
states) {hs
form a feature matrix F s ∈ R|V |×|V |×(2dV +dE ) (see
in Fig. 2). The link function
is deﬁned as a small neural network with one or several convolutional layer(s)
(with 1 × 1 × (2dV + dE) kernels) and a sigmoid activation. Then the adjacency
matrix As ∈ [0, 1]|V|×|V| can be computed as:

v ∈ RdV }v and all the edge features (messages) {ms

As = σ(WL ∗ F s),

(8)

where WL is the learnable parameters of the link function network L(·) and ∗
denotes conv operation. The sigmoid operation σ(·) is for normalizing the values
of the elements of As into [0, 1]. The essential eﬀect of multiple convolutional
layers with 1 × 1 kernels is similar to fully connected layers applied to each
individual edge features, except that the ﬁlter weights are shared by all the
edges. In practice, we ﬁnd such operation generates good enough results and
leads to a high computation eﬃciency.

For spatial-temporal problems where the adjacency matrices should account
for the previous states, we use convolutional LSTMs [39] for modeling L(·) in
temporal domain. At time t, the link function takes F s,t as input features and the
previous adjacency matrix As,t−1 as hidden state: As,t = convLST M (F s,t, As,t−1).
Again, the kernel size for the conv layer in convLSTM is 1 × 1 × (2dV + dE).
Message Function. In our implementation, the message function M (·) in Eq. 3
is computed by:

M (hv, hw, Γvw) = [WM

V hv, WM

V hw, WM

E Γvw],

(9)

where [., .] denotes concatenation. It concatenates the outputs of linear trans-
forms (i.e., fully connected layers parametrized by WM
E ) that takes
node hidden states hv or edge features Γvw as input.

V and WM

8

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Update Function. Recurrent neural networks [10, 18] are natural choices for
simulating the iterative update process, as done by previous works [12]. Here we
apply Gated Recurrent Unit (GRU) [5] as the update function, because of its
recurrent nature and smaller amount of parameters. Thus the update function
in Eq. 4 is implemented as:

v = U (hs−1
hs

v

, ms

v) = GRU (hs−1

v

, ms

v),

(10)

v is the hidden state and ms

where hs
in [25], the GRU is more eﬀective than vanilla recurrent neural networks.
Readout Function. A typical implementation of readout functions is com-
bining several fully connected layers (parameterized by WR) followed by an
activation function:

v is used as input features. As demonstrated

yv = R(hS

v ) = ϕ(WRhS

v ).

(11)

Here the activation function ϕ(·) can be used as softmax (one-class outputs) or
sigmoid (multi-class outputs) according to diﬀerent HOI tasks.

In this way, the entire GPNN is implemented to be fully diﬀerentiable and
end-to-end trainable. The loss for speciﬁc HOI task can be computed for the
outputs of readout functions, and the error can propagate back according to
chain rule. In next section, we will oﬀer more details for implementing GPNN
for HOI tasks on spatial and spatial-temporal settings and present qualitative
as well as quantitative results.

4 Experiments

To verify the eﬀectiveness and generic applicability of GPNN, we perform ex-
periments on two HOI problems: i) HOI detection in images [1, 17], and ii) HOI
recognition and anticipation from videos [22]. The ﬁrst experiment is performed
on HICO-DET [1] and V-COCO [17] datasets, showing that our approach is
scalable to large datasets (about 60K images in total) and achieves a good de-
tection accuracy over a large number of classes (more than 600 classes of HOIs).
The second experiment is reported on CAD-120 dataset [22], showing that our
method is well applicable to spatial-temporal domains.

4.1 Human-Object Interaction Detection in Images

For HOI detection in an image, the goal is to detect pairs of a human and an
object bounding box with an interaction class label connecting them.
Datasets. We use HICO-DET [1] and V-COCO [17] datasets for benchmarking
our GPNN model. HICO-DET provides more than 150K annotated instances
of human-object pairs in 47,051 images (37,536 training and 9,515 testing). It
has the same 80 object categories as MS-COCO [28] and 117 action categories.
V-COCO is a subset of MS-COCO [28]. It consists of a total of 10,346 images
with 16,199 people instances, where ∼2.5K images in the train set, ∼2.8K images
for validation and ∼4.9K images for testing. Each annotated person has binary

Graph Parsing Neural Networks (ECCV 2018)

9

Table 1. HOI detection results (mAP) on HICO-DET dataset [1]. Higher
values are better. The best scores are marked in bold.

Methods

Full (mAP %) ↑ Rare (mAP %) ↑ Non-rare (mAP %) ↑

Random
Fast-RCNN(union) [13]
Fast-RCNN(score) [13]
HO-RCNN [1]
HO-RCNN+IP [1]
HO-RCNN+IP+S [1]
Gupta et al. [17]
Shen et al. [38]
InteractNet [14]
GPNN
Performance Gain(%)

1.35 × 10−3
1.75
2.85
5.73
7.30
7.81
9.09
6.46
9.94
13.11
31.89

5.72 × 10−4
0.58
1.55
3.21
4.68
5.37
7.02
4.24
7.16
9.34
30.45

1.62 × 10−3
2.10
3.23
6.48
8.08
8.54
9.71
7.12
10.77
14.23
32.13

Fig. 3. HOI detection results on HICO-DET [1] test images. Human and ob-
jects are shown in red and green rectangles, respectively. Best viewed in color.

labels for 26 diﬀerent action classes. Note that three actions (i.e., cut, eat, and
hit) are annotated with two types of targets: instrument and direct object.
Implementation Details. Humans and objects are represented by nodes in
the graph, while human-object interactions are represented by edges. In this ex-
periment, we use a pre-trained deformable convolutional network [6] for object
detection and features extraction. Based on the detected bounding boxes, we ex-
tract node features (7 × 7 × 80) from the position-sensitive region of interest (PS
RoI) pooling layer from the deformable ConvNet. We extract the edge feature
from a combined bounding box, i.e., the smallest bounding box that contains
both two nodes’ bounding boxes. The functions of GPNN are implemented as
follows. We use a convolutional network (128-128-1)-Sigmoid(·) with 1×1 kernels
for the link function. The message functions are composed of a fully connected
layer, concatenation, and summation. For a node v, the neighboring node feature
Γw and edge feature Γvw are passed through a fully connected layer and concate-
nated. The ﬁnal incoming message is a weighted sum of messages from all neigh-
boring nodes. Speciﬁcally, the message for node v coming from node w through
edge e = (v, w) is the concatenation of output from FC(dV -dV ) and FC(dE-dE).
A GRU(dV ) is used for the update function. The propagation step number S

10

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 2. HOI detection results (mAP) on V-COCO [17] dataset. Legend: Set
1 indicates 18 HOI actions with one object, and Set 2 corresponds to 3 HOI actions
(i.e., cut, eat, hit) with two objects (instrument and object).

Method

Set 1 (mAP %) ↑

Set 2 (mAP %) ↑ Ave. (mAP %) ↑

Gupta et al. [17]
InteractNet [14]
GPNN
Performance Gain(%)

33.5
42.2
44.5
5.5

26.7
33.2
42.8
28.9

31.8
40.0
44.0
10.0

Fig. 4. HOI detection results on V-COCO [17] test images. Human and objects
are shown in red and green rectangles, respectively. Best viewed in color.

is set to be 3. For the readout function, we use a FC(dV -117)-Sigmoid(·) and
FC(dV -26)-Sigmoid(·) for HICO-DET and V-COCO, respectively.

The probability of an HOI label of a human-object pair is given by the
product of the ﬁnal output probabilities from the human node and the object
node. We employ an L1 loss for the adjacency matrix. For the node outputs, we
use a weighted multi-class multi-label hinge loss. The reasons are two-folds: the
training examples are not balanced, and it is essentially a multi-label problem
for each node (there might not even exist a meaningful human-object interaction
for detected humans and objects).

Our model is implemented using PyTorch and trained with a machine with
a single Nvidia Titan Xp GPU. We start with a learning rate of 1e-3, and the
rate decays every 5 epochs by 0.8. The training process takes about 20 epochs
(∼15 hours) to roughly converge with a batch size of 32.
Comparative Methods. We compare our method with eight baselines: (1)
Fast-RCNN (union) [13]: for each human-object proposal from detection results,
their attention windows are used as the region proposal for Fast-RCNN. (2) Fast-
RCNN (score) [13]: given human-object proposals, HOI is predicted by linearly
combining the human and object detection scores. (3) HO-RCNN [1]: a multi-
stream architecture with a ConvNet to classify human, object and human-object
proposals, respectively. The ﬁnal output is computed by combining the scores
from all the three streams. (4) HO-RCNN+IP [1] and (5) HO-RCNN+IP+S [1]:
HO-RCNN with additional components. Interaction Patterns (IP) acts as a at-
tention ﬁlter to images. S is an extra path with a single neuron that uses the

Graph Parsing Neural Networks (ECCV 2018)

11

raw object detection score to produce an oﬀset for the ﬁnal detection. More
detailed descriptions of above ﬁve baselines can be found in [1]. (6) Gupta et
al. [17]: trained based on Fast-RCNN [13]. We use the scores reported in [14].
(7) Shen et al. [38]: ﬁnal predictions are from two Faster RCNN [36] based net-
works which are trained for predicting verb and object classes, respectively. (8)
InteractNet [14]: a modiﬁed Faster RCNN [36] with an additional human-centric
branch that estimates an action-speciﬁc density map for locating objects.
Experiment Results. Following the standard settings in HICO-DET and V-
COCO benchmarks, we evaluate HOI detection using mean average precision
(mAP). An HOI detection is considered as a true positive when the human
detection, the object detection, and the interaction class are all correct. The
human and object bounding boxes are considered as true positives if they overlap
with a ground truth bounding boxes of the same class with an intersection over
union (IoU) greater than 0.5. For HICO-DET dataset, we report the mAP over
three diﬀerent HOI category sets: i) all 600 HOI categories in HICO (Full); ii)
138 HOI categories with less than 10 training instances (Rare); and iii) 462 HOI
categories with 10 or more training instances (Non-Rare). For V-COCO dataset,
since we concentrate on HOI detection, we report the mAP on three groups: i)
18 HOI action classes with one target object; ii) 3 HOI categories with two types
of objects; iii) all 24 (=18 + 3 × 2) HOI classes. Results are evaluated on the test
sets and reported in Table 1 and Table 2.

As shown in Table 1, the proposed GPNN substantially outperforms the
comparative methods, achieving 31.89%, 30.45%, and 32.13% improvement
over the second best methods on the three HOI category sets on the HICO-
DET dataset. The results on V-COCO dataset (in Table 2) also consistently
demonstrate the superior performance of the proposed GPNN. Two important
conclusions can be drawn from the results: i) our method is scalable to large
datasets; ii) and our method performs better than pure neural network. Some
visual results can be found in Fig. 3 and Fig. 4.

4.2 Human-Object Interaction Recognition in Videos

The goal of this experiment is to detect and predict the human sub-activity
labels and object aﬀordance labels as the human-object interaction progresses
in videos. The problem is challenging since it involves complex interactions that
humans make with multiple objects, and objects also interact with each other.
CAD-120 dataset [22]. It has 120 RGB-D videos of 4 subjects performing 10
activities, each of which is a sequence of sub-activities involving 10 actions (e.g.,
reaching, opening), and 12 object aﬀordances (e.g., reachable, openable) in total.
Implementation Details. The link function is implemented as: convLSTM(1024-
1024-1024-1)-Sigmoid(·) (i.e., a four-layer convLSTM). We use the same ar-
chitecture as the previous experiment for message functions and update func-
tions: [FC(dV -dV ), FC(dE-dE)] for message function and GRU(dV ) for update
function. The propagation step number S is set to be 3. We use a FC(dV -10)-
Softmax(·) and a FC(dV -12)-Softmax(·) for readout functions of sub-activity and
object aﬀordance detection/anticipation, respectively. We employ an L1 loss for

12

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 3. Human activity detection and future anticipation results on CAD-
120 [22] dataset, measured via F1-score.

Method

Detection (F1-score) ↑
Object
Sub
Aﬀordance(%)
-activity(%)

Anticipation (F1-score) ↑

Sub
-activity(%)

Object
Aﬀordance(%)

ATCRF [22]
S-RNN [20]
S-RNN (multi-task) [20]
GPNN
Performance Gain(%)

80.4
83.2
82.4
88.9
8.1

81.5
88.7
91.1
88.8
-

37.9
62.3
65.6
75.6
15.2

36.7
80.7
80.9
81.9
1.2

Fig. 5. Confusion matrices of HOI detection (a)(b) and anticipation (c)(d)
results on CAD-120 [22] dataset. Zoom in for more details.

the adjacency matrix and a cross entropy loss for the node outputs. We use the
publicly available node and edge features from [23].
Comparative Methods. We compare our method with two baselines: antici-
patory temporal CRF (ATCRF) [22] and structural RNN (S-RNN) [20]. ATCRF
is a top-performing graphical model approach for this problem, while S-RNN is
the state-of-art method using structured neural networks. ATCRF models the
human activities through a spatial-temporal conditional random ﬁeld. S-RNN
casts a pre-deﬁned spatial-temporal graph as an RNN mixture by representing
nodes and edges as LSTMs.
Experiment Results. In Table 3 we show the quantitative comparison of
our method with other competitors. It shows the F1-scores averaged over all
classes on detection and activity anticipation tasks. GPNN greatly improves
over ATCRF and S-RNN, especially on anticipation task. Our method outper-
forms the other two for the following reasons. i) Comparing to ATCRF limited
to the Markov assumption, our method allows arbitrary graph structures with
improved representation ability. ii) Our method enjoys the beneﬁt of deep in-
tegration of graphical models and neural networks and can be learned in an
end-to-end manner. iii) Rather than relying on a pre-ﬁxed graph structure as in
S-RNN, we infer the graph structure via learning an adjacency matrix and thus
be able to control the information ﬂow between nodes during massage passing.
Fig. 5 show the confusion matrices for detecting and predicting the sub-activities
and object aﬀordances, respectively. From above results we can draw two im-

Graph Parsing Neural Networks (ECCV 2018)

13

Fig. 6. HOI detection results on a “cleaning objects” activity on CAD-120 [22]
dataset. Human are shown in red rectangle. Two objects are shown in green and blue
rectangles, respectively. Detection and anticipation results are shown by diﬀerent bars.
For anticipation task, the label of the sub-activity at time t is anticipated at time t-1.

portant conclusions: i) our method is well applicable to the spatio-temporal
domain; and ii) our method outperforms pure graphical models (e.g., ATCRF)
and deep networks with pre-ﬁxed graph structures (e.g., S-RNN). Fig. 6 shows a
qualitative visualization of “cleaning objects”. We show one representative frame
for each sub-activity as well as the corresponding detections and anticipations.

4.3 Ablation Study

In this section, we analyze the contributions of diﬀerent model components to
the ﬁnal performance and examine the eﬀectiveness of our main assumptions.
Table 4 shows the detailed results on all three datasets.
Integration of DNN with Graphical Model. We ﬁrst examine the inﬂu-
ence of integrating DNN with a graphical model. We directly feed the features,
which are originally used for GPNN, into diﬀerent fully connected networks for
predicting HOI action or object classes. From Table 4, we can observe the per-
formance of w/o graph is signiﬁcantly worse than GPNN model over various
HOI datasets. This supports our view that modeling high-level structures and
leveraging learning capabilities of DNNs together is essential for HOI tasks.
GPNN with Fixed Graph Structures. In § 3, GPNN automatically infers
graph structures (i.e., parse graph) via learning a soft adjacency matrix. To
assess this strategy, we ﬁx all the entries in the soft adjacency matrices to be
constant 1. This way the graph structures are ﬁxed and the information ﬂow
between nodes are not weighted. For constant graph baseline, we see obvious
performance decrease, compared with the full GPNN model. This indicates that
inferring graph structures is critical to get reasonable performance.
GPNN without Supervision on Link Functions. We perform experiments
by turning oﬀ the L1 loss on adjacency matrices (w/o graph loss in Table 4). We
can observe that the intermediate L1 loss is eﬀective, further verifying our design
to learn the graph structure. Another interesting observation is that training

14

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

Table 4. Ablation study of GPNN model. Higher values are better.

Aspect Method

Set 1 Set 2 Ave. Full Rare

V-COCO [17]
HOI Detection
mAP(%) ↑

HICO-DET [1]
HOI Detection
mAP(%) ↑

CAD-120 [22]

HOI Detec.
F1-score(%) ↑
Object
Sub-
Aﬀ.(%)
activity

HOI Antici.
F1-score(%) ↑
Object
Sub-
Aﬀ.(%)
activity

Non-
rare

44.5 42.8 44.0 13.11 9.34 14.23 88.9

88.8

75.6

81.9

GPNN
(3 iterations)

w/o graph
graph
constant graph
structure w/o graph loss

2.04
9.62
27.4 30.0 28.1
1.94 10.79
34.6 33.3 34.3
8.72
6.24
37.7 40.5 38.4
w/o joint parsing 43.6 39.4 42.5 10.17 5.81 11.47
42.0 40.7 41.7 11.38 7.27 12.61
44.1 42.2 43.6 12.37 9.01 13.38
43.6 40.9 42.9 12.39 8.95 13.41

7.88
8.75
8.15

iterative 1 iteration
learning 2 iterations
4 iterations

50.2
85.3
85.2
79.3
80.5
87.9
87.9

20.8
85.6
85.8
79.2
80.7
86.1
85.7

32.3
73.8
74.7
74.7
75.2
76.1
75.5

19.6
79.1
79.2
80.3
81.1
81.5
80.6

the model without this loss has a similar eﬀect to training with constant graph.
Hence supervision on the graph is fairly important.
Jointly Learning Parse Graph and Message Passing. We next study the
eﬀect of jointly learning graph structures and message passing. By isolating graph
parsing from message passing, we obtain w/o joint parsing, where the adjacency
matrices are directly computed by link functions from edge features at the be-
ginning. We observe a performance decrease in Table 4, showing that learning
graph structures and message passing together indeed boost the performance.
Iterative Learning Process. Next we examine the eﬀect of iterative message
passing, we report three baselines: 1 iteration, 2 iterations, and 4 iterations,
which correspond to the results from diﬀerent message passing iterations. The
baseline GPNN (ﬁrst row in Table 4) are the results after three iterations. From
the results we observe that the iterative learning process is able to gradually
improve the performance in general. We also observe that when the iteration
round is increased to a certain extent, the performance drops slightly.

5 Conclusion

In this paper, we propose Graph Parsing Neural Network (GPNN) for inferring
a parse graph in an end-to-end manner. The network can be decomposed into
four distinct functions, namely link functions, message functions, update func-
tions and readout functions, for iterative graph inference and message passing.
GPNN provides a generic HOI representation that is applicable in both spa-
tial and spatial-temporal domains. We demonstrate a substantial performance
gain on three HOI datasets, showing the eﬀectiveness of the proposed framework.

Acknowledgments. The authors thank Prof. Ying Nian Wu from UCLA Statis-
tics Department for helpful comments on this work. This research is supported by
DARPA XAI N66001-17-2-4029, ONR MURI N00014-16-1-2007, ARO W911NF1810296,
and N66001-17-2-3602.

Graph Parsing Neural Networks (ECCV 2018)

15

References

interactions (2018)

1. Chao, Y.W., Liu, Y., Liu, X., Zeng, H., Deng, J.: Learning to detect human-object

2. Chao, Y.W., Wang, Z., He, Y., Wang, J., Deng, J.: HICO: A benchmark for rec-

ognizing human-object interactions in images. In: ICCV (2015)

3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Se-
mantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected CRFs. PAMI (2016)

4. Chen, L.C., Schwing, A., Yuille, A., Urtasun, R.: Learning deep structured models.

In: ICML (2015)

5. Cho, K., Van Merri¨enboer, B., Bahdanau, D., Bengio, Y.: On the properties of
neural machine translation: Encoder–decoder approaches. Syntax, Semantics and
Structure in Statistical Translation p. 103 (2014)

6. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convo-

lutional networks. In: ICCV (2017)

7. Deﬀerrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on

graphs with fast localized spectral ﬁltering. In: NIPS (2016)

8. Delaitre, V., Sivic, J., Laptev, I.: Learning person-object interactions for action

recognition in still images. In: NIPS (2011)

9. Desai, C., Ramanan, D.: Detecting actions, poses, and objects with relational

phraselets. In: ECCV (2012)

10. Elman, J.L.: Finding structure in time. Cognitive science (1990)
11. Fang, H.S., Xu, Y., Wang, W., Zhu, S.C.: Learning pose grammar to encode human

body conﬁguration for 3d pose estimation. In: AAAI (2018)

12. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message

passing for quantum chemistry. In: ICML (2017)

13. Girshick, R.: Fast R-CNN. In: ICCV (2015)
14. Gkioxari, G., Girshick, R., Doll´ar, P., He, K.: Detecting and recognizing human-

object interactions. In: CVPR (2018)

15. Gupta, A., Davis, L.S.: Objects in action: An approach for combining action un-

derstanding and object perception. In: CVPR (2007)

16. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using

spatial and functional compatibility for recognition. PAMI (2009)

17. Gupta, S., Malik, J.: Visual semantic role labeling. arXiv preprint arXiv:1505.04474

18. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

(2015)

(1997)

19. Hu, J.F., Zheng, W.S., Lai, J., Gong, S., Xiang, T.: Recognising human-object

interaction via exemplar based modelling. In: ICCV (2013)

20. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: Deep learning on

spatio-temporal graphs. In: CVPR (2016)

21. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional

networks. In: ICLR (2017)

22. Koppula, H.S., Saxena, A.: Anticipating human activities using object aﬀordances

for reactive robotic response. PAMI (2016)

23. Koppula, H.S., Gupta, R., Saxena, A.: Learning human activities and object af-
fordances from RGB-D videos. The International Journal of Robotics Research
(2013)

16

S. Qi, W. Wang, B. Jia, J. Shen, S.-C. Zhu

24. Li, R., Tapaswi, M., Liao, R., Jia, J., Urtasun, R., Fidler, S.: Situation recognition

with graph neural networks. In: ICCV (2017)

25. Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R.: Gated graph sequence neural

26. Liang, X., Lin, L., Shen, X., Feng, J., Yan, S., Xing, E.P.: Interpretable structure-

27. Liang, X., Shen, X., Feng, J., Lin, L., Yan, S.: Semantic object parsing with graph

networks. In: ICLR (2016)

evolving lstm. In: ICCV (2017)

lstm. In: ECCV (2016)

28. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)
29. Mallya, A., Lazebnik, S.: Learning models for actions and person-object interac-

tions with transfer to question answering. In: ECCV (2016)

30. Marino, K., Salakhutdinov, R., Gupta, A.: The more you know: Using knowledge

graphs for image classiﬁcation. In: CVPR (2016)

31. Monti, F., Boscaini, D., Masci, J., Rodol`a, E., Svoboda, J., Bronstein, M.M.: Ge-
ometric deep learning on graphs and manifolds using mixture model cnns. CVPR
(2016)

32. Niepert, M., Ahmed, M., Kutzkov, K.: Learning convolutional neural networks for

graphs. In: ICML (2016)

33. Park, S., Nie, X., Zhu, S.C.: Attribute and-or grammar for joint parsing of human

pose, parts and attributes. PAMI (2017)

34. Qi, S., Huang, S., Wei, P., Zhu, S.C.: Predicting human activities using stochastic

grammar. In: ICCV (2017)

35. Qi, S., Jia, B., Zhu, S.C.: Generalized earley parser: Bridging symbolic grammars

and sequence data for future prediction. In: ICML (2018)

36. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

37. Seo, Y., Deﬀerrard, M., Vandergheynst, P., Bresson, X.: Structured sequence mod-
eling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659
(2016)

38. Shen, L., Yeung, S., Hoﬀman, J., Mori, G., Fei-Fei, L.: Scaling human-object in-

teraction recognition through zero-shot learning (2018)

39. Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convolutional
LSTM network: A machine learning approach for precipitation nowcasting. In:
NIPS (2015)

40. Simonovsky, M., Komodakis, N.: Dynamic edge-conditioned ﬁlters in convolutional

neural networks on graphs. CVPR (2017)

41. Teney, D., Liu, L., Hengel, A.v.d.: Graph-structured representations for visual ques-

tion answering. In: CVPR (2017)

42. Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint training of a convolutional
network and a graphical model for human pose estimation. In: NIPS (2014)
43. Wang, W., Xu, Y., Shen, J., Zhu, S.C.: Attentive fashion grammar network for
fashion landmark detection and clothing category classiﬁcation. In: CVPR (2018)
44. Wu, Z., Lin, D., Tang, X.: Deep markov random ﬁeld for image modeling. In:

ECCV (2016)

45. Xia, F., Zhu, J., Wang, P., Yuille, A.L.: Pose-guided human parsing by an And/Or

graph using pose-context features. In: AAAI (2016)

46. Xu, D., Zhu, Y., Choy, C.B., Fei-Fei, L.: Scene graph generation by iterative mes-

sage passing. In: ICCV (2017)

47. Yao, B., Fei-Fei, L.: Grouplet: A structured image representation for recognizing

human and object interactions. In: CVPR (2010)

Graph Parsing Neural Networks (ECCV 2018)

17

48. Yao, B., Fei-Fei, L.: Modeling mutual context of object and human pose in human-

object interaction activities. In: CVPR (2010)

49. Yao, B., Jiang, X., Khosla, A., Lin, A.L., Guibas, L., Fei-Fei, L.: Human action
recognition by learning bases of action attributes and parts. In: ICCV (2011)
50. Yuan, Y., Liang, X., Wang, X., Yeung, D.Y., Gupta, A.: Temporal dynamic graph

LSTM for action-driven video object detection. In: ICCV (2017)

51. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: ICCV
(2015)

