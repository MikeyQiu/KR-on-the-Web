8
1
0
2
 
t
c
O
 
9
1
 
 
]

V
C
.
s
c
[
 
 
2
v
2
3
9
7
0
.
5
0
8
1
:
v
i
X
r
a

Bilinear Attention Networks

Jin-Hwa Kim1∗, Jaehyun Jun2, Byoung-Tak Zhang2,3
1SK T-Brain, 2Seoul National University, 3Surromind Robotics
jnhwkim@sktbrain.com, {jhjun,btzhang}@bi.snu.ac.kr

Abstract

Attention networks in multimodal learning provide an efﬁcient way to utilize given
visual information selectively. However, the computational cost to learn attention
distributions for every pair of multimodal input channels is prohibitively expensive.
To solve this problem, co-attention builds two separate attention distributions for
each modality neglecting the interaction between multimodal inputs. In this paper,
we propose bilinear attention networks (BAN) that ﬁnd bilinear attention distri-
butions to utilize given vision-language information seamlessly. BAN considers
bilinear interactions among two groups of input channels, while low-rank bilinear
pooling extracts the joint representations for each pair of channels. Furthermore,
we propose a variant of multimodal residual networks to exploit eight-attention
maps of the BAN efﬁciently. We quantitatively and qualitatively evaluate our
model on visual question answering (VQA 2.0) and Flickr30k Entities datasets,
showing that BAN signiﬁcantly outperforms previous methods and achieves new
state-of-the-arts on both datasets.

1

Introduction

Machine learning for computer vision and natural language processing accelerates the advancement of
artiﬁcial intelligence. Since vision and natural language are the major modalities of human interaction,
understanding and reasoning of vision and natural language information become a key challenge. For
instance, visual question answering involves a vision-language cross-grounding problem. A machine
is expected to answer given questions like "who is wearing glasses?", "is the umbrella upside down?",
or "how many children are in the bed?" exploiting visually-grounded information.

For this reason, visual attention based models have succeeded in multimodal learning tasks, identifying
selective regions in a spatial map of an image deﬁned by the model. Also, textual attention can be
considered along with visual attention. The attention mechanism of co-attention networks [36, 18, 20,
39] concurrently infers visual and textual attention distributions for each modality. The co-attention
networks selectively attend to question words in addition to a part of image regions. However, the co-
attention neglects the interaction between words and visual regions to avoid increasing computational
complexity.

In this paper, we extend the idea of co-attention into bilinear attention which considers every pair
of multimodal channels, e.g., the pairs of question words and image regions. If the given question
involves multiple visual concepts represented by multiple words, the inference using visual attention
distributions for each word can exploit relevant information better than that using single compressed
attention distribution.

From this background, we propose bilinear attention networks (BAN) to use a bilinear attention
distribution, on top of low-rank bilinear pooling [15]. Notice that the BAN exploits bilinear inter-
actions between two groups of input channels, while low-rank bilinear pooling extracts the joint
representations for each pair of channels. Furthermore, we propose a variant of multimodal residual

∗This work was done while at Seoul National University.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: Overview of two-glimpse BAN. Two multi-channel inputs, φ-object detection features and
ρ-length GRU hidden vectors, are used to get bilinear attention maps and joint representations to be
used by a classiﬁer. For the deﬁnition of the BAN, see the text in Section 3.

networks (MRN) to efﬁciently utilize the multiple bilinear attention maps of the BAN, unlike the
previous works [6, 15] where multiple attention maps are used by concatenating the attended features.
Since the proposed residual learning method for BAN exploits residual summations instead of con-
catenation, which leads to parameter-efﬁciently and performance-effectively learn up to eight-glimpse
BAN. For the overview of two-glimpse BAN, please refer to Figure 1.

Our main contributions are:

• We propose the bilinear attention networks (BAN) to learn and use bilinear attention distributions,

on top of low-rank bilinear pooling technique.

• We propose a variant of multimodal residual networks (MRN) to efﬁciently utilize the multiple
bilinear attention maps generated by our model. Unlike previous works, our method successfully
utilizes up to 8 attention maps.

• Finally, we validate our proposed method on a large and highly-competitive dataset, VQA
2.0 [8]. Our model achieves a new state-of-the-art maintaining simplicity of model structure.
Moreover, we evaluate the visual grounding of bilinear attention map on Flickr30k Entities [23]
outperforming previous methods, along with 25.37% improvement of inference speed taking
advantage of the processing of multi-channel inputs.

2 Low-rank bilinear pooling

We ﬁrst review the low-rank bilinear pooling and its application to attention networks [15], which
uses single-channel input (question vector) to combine the other multi-channel input (image features)
as single-channel intermediate representation (attended feature).

Low-rank bilinear model. The previous works [35, 22] proposed a low-rank bilinear model to
reduce the rank of bilinear weight matrix Wi to give regularity. For this, Wi is replaced with the
i , where Ui ∈ RN ×d and Vi ∈ RM ×d. As a result,
multiplication of two smaller matrices UiVT
this replacement makes the rank of Wi to be at most d ≤ min(N, M ). For the scalar output fi (bias
terms are omitted without loss of generality):

fi = xT Wiy ≈ xT UiVT

i y = 1T (UT

i x ◦ VT

i y)

(1)

where 1 ∈ Rd is a vector of ones and ◦ denotes Hadamard product (element-wise multiplication).

Low-rank bilinear pooling. For a vector output f , a pooling matrix P is introduced:

f = PT (UT x ◦ VT y)
(2)
where P ∈ Rd×c, U ∈ RN ×d, and V ∈ RM ×d. It allows U and V to be two-dimensional tensors
by introducing P for a vector output f ∈ Rc, signiﬁcantly reducing the number of parameters.

Unitary attention networks. Attention provides an efﬁcient mechanism to reduce input channel
by selectively utilizing given information. Assuming that a multi-channel input Y consisting of
φ = |{yi}| column vectors, we want to get single channel ˆy from Y using the weights {αi}:

(3)

(cid:88)

ˆy =

αiyi

i

2

where α represents an attention distribution to selectively combine φ input channels. Using the
low-rank bilinear pooling, the α is deﬁned by the output of softmax function as:

α := softmax

(cid:16)

PT (cid:0)(UT x · 1T ) ◦ (VT Y)(cid:1)(cid:17)

(4)

where α ∈ RG×φ, P ∈ Rd×G, U ∈ RN ×d, x ∈ RN , 1 ∈ Rφ, V ∈ RM ×d, and Y ∈ RM ×φ. If
G > 1, multiple glimpses (a.k.a. attention heads) are used [13, 6, 15], then ˆy = (cid:102)
i αg,iyi, the
concatenation of attended outputs. Finally, two single channel inputs x and ˆy can be used to get the
joint representation using the other low-rank bilinear pooling for a classiﬁer.

G
g=1

(cid:80)

3 Bilinear attention networks

We generalize a bilinear model for two multi-channel inputs, X ∈ RN ×ρ and Y ∈ RM ×φ, where
ρ = |{xi}| and φ = |{yj}|, the numbers of two input channels, respectively. To reduce both input
channel simultaneously, we introduce bilinear attention map A ∈ Rρ×φ as follows:

k = (XT U(cid:48))T
f (cid:48)

k A(YT V(cid:48))k

(5)
where U(cid:48) ∈ RN ×K, V(cid:48) ∈ RM ×K, (XT U(cid:48))k ∈ Rρ, (YT V(cid:48))k ∈ Rφ, and f (cid:48)
k denotes the k-th
element of intermediate representation. The subscript k for the matrices indicates the index of column.
Notice that Equation 5 is a bilinear model for the two groups of input channels where A in the middle
is a bilinear weight matrix. Interestingly, Equation 5 can be rewritten as:

f (cid:48)
k =

ρ
(cid:88)

φ
(cid:88)

i=1

j=1

ρ
(cid:88)

φ
(cid:88)

i=1

j=1

Ai,j(XT

i U(cid:48)

k)(V(cid:48)T

k Yj) =

Ai,jXT

i (U(cid:48)

kV(cid:48)T

k )Yj

(6)

k and V(cid:48)

where Xi and Yj denotes the i-th channel (column) of input X and the j-th channel (channel) of
input Y, respectively, U(cid:48)
k denotes the k-th column of U(cid:48) and V(cid:48) matrices, respectively,
and Ai,j denotes an element in the i-th row and the j-th column of A. Notice that, for each pair of
channels, the 1-rank bilinear representation of two feature vectors is modeled in XT
k )Yj
of Equation 6 (eventually at most K-rank bilinear pooling for f (cid:48) ∈ RK). Then, the bilinear joint
representation is f = PT f (cid:48) where f ∈ RC and P ∈ RK×C. For the convenience, we deﬁne the
bilinear attention networks as a function of two multi-channel inputs parameterized by a bilinear
attention map as follows:

i (U(cid:48)

kV(cid:48)T

f = BAN(X, Y; A).

Bilinear attention map. Now, we want to get the attention map similarly to Equation 4. Using
Hadamard product and matrix-matrix multiplication, the attention map A is deﬁned as:

A := softmax

(cid:16)(cid:0)(1 · pT ) ◦ XT U(cid:1)VT Y

(cid:17)

where 1 ∈ Rρ, p ∈ RK(cid:48)
wisely. Notice that each logit Ai,j of the softmax is the output of low-rank bilinear pooling as:

, and remind that A ∈ Rρ×φ. The softmax function is applied element-

Ai,j = pT (cid:0)(UT Xi) ◦ (VT Yj)(cid:1).

The multiple bilinear attention maps can be extended as follows:

(cid:16)(cid:0)(1 · pT
where the parameters of U and V are shared, but not for pg where g denotes the index of glimpses.

g ) ◦ XT U(cid:1)VT Y

Ag := softmax

(10)

(cid:17)

Residual learning of attention. Inspired by multimodal residual networks (MRN) from Kim et al.
[14], we propose a variant of MRN to integrate the joint representations from the multiple bilinear
attention maps. The i + 1-th output is deﬁned as:

fi+1 = BANi(fi, Y; Ai) · 1T + fi
(11)
where f0 = X (if N = K) and 1 ∈ Rρ. Here, the size of fi is the same with the size of X as
successive attention maps are processed. To get the logits for a classiﬁer, e.g., two-layer MLP, we
sum over the channel dimension of the last output fG, where G is the number of glimpses.

Time complexity. When we assume that the number of input channels is smaller than feature
sizes, M ≥ N ≥ K (cid:29) φ ≥ ρ, the time complexity of the BAN is the same with the case of one
multi-channel input as O(KM φ) for single glimpse model. Since the BAN consists of matrix chain
multiplication and exploits the property of low-rank factorization in the low-rank bilinear pooling.

(7)

(8)

(9)

3

4 Related works

Multimodal factorized bilinear pooling. Yu et al. [39] extends low-rank bilinear pooling [15]
using the rank > 1. They remove a projection matrix P, instead, d in Equation 2 is replaced with
much smaller k while U and V are three-dimensional tensors. However, this generalization was
not effective for BAN, at least in our experimental setting. Please see BAN-1+MFB in Figure 2b
where the performance is not signiﬁcantly improved from that of BAN-1. Furthermore, the peak GPU
memory consumption is larger due to its model structure which hinders to use multiple-glimpse BAN.

Co-attention networks. Xu and Saenko [36] proposed the spatial memory network model estimating
the correlation among every image patches and tokens in a sentence. The estimated correlation C
is deﬁned as (UX)T Y in our notation. Unlike our method, they get an attention distribution
α = softmax(cid:0) maxi=1,...,ρ(Ci)(cid:1) ∈ Rρ where the logits to softmax is the maximum values in each
row vector of C. The attention distribution for the other input can be calculated similarly. There are
variants of co-attention networks [18, 20], especially, Lu et al. [18] sequentially get two attention
distributions conditioning on the other modality. Recently, Yu et al. [39] reduce the co-attention
method into two steps, self-attention for a question embedding and the question-conditioned attention
for a visual embedding. However, these co-attention approaches use separate attention distributions
for each modality, neglecting the interaction between the modalities what we consider and model.

5 Experiments

5.1 Datasets

Visual Question Answering (VQA). We evaluate on the VQA 2.0 dataset [1, 8], which is improved
from the previous version to emphasize visual understanding by reducing the answer bias in the
dataset. This improvement pushes the model to have more effective joint representation of question
and image, which ﬁts the motivation of our bilinear attention approach. The VQA evaluation metric
considers inter-human variability deﬁned as Accuracy(ans) = min(#humans that said ans/3, 1).
Note that reporting accuracies are averaged over all ten choose nine sets of ground-truths. The test
set is split into test-dev, test-standard, test-challenge, and test-reserve. The annotations for the test set
are unavailable except the remote evaluation servers.

Flickr30k Entities. For the evaluation of visual grounding by the bilinear attention maps, we use
Flickr30k Entities [23] consisting of 31,783 images [38] and 244,035 annotations that multiple
entities (phrases) in a sentence for an image are mapped to the boxes on the image to indicate the
correspondences between them. The task is to localize a corresponding box for each entity. In this
way, visual grounding of textual information is quantitatively measured. Following the evaluation
metric [23], if a predicted box has the intersection over union (IoU) of overlapping area with one
of the ground-truth boxes which are greater than or equal to 0.5, the prediction for a given entity is
correct. This metric is called Recall@1. If K predictions are permitted to ﬁnd at least one correction,
it is called Recall@K. We report Recall@1, 5, and 10 to compare state-of-the-arts (R@K in Table 4).
The upper bound of performance depends on the performance of object detection if the detector
proposes candidate boxes for the prediction.

5.2 Preprocessing

Question embedding. For VQA, we get a question embedding XT ∈ R14×N using GloVe word
embeddings [21] and the outputs of Gated Recurrent Unit (GRU) [5] for every time-steps up to the
ﬁrst 14 tokens following the previous work [29]. The questions shorter than 14 words are end-padded
with zero vectors. For Flickr30k Entities, we use a full length of sentences (82 is maximum) to get all
entities. We mark the token positions which are at the end of each annotated phrase. Then, we select
a subset of the output channels of GRU using these positions, which makes the number of channels is
the number of entities in a sentence. The word embeddings and GRU are ﬁne-tuned in training.

Image features. We use the image features extracted from bottom-up attention [2]. These features are
the output of Faster R-CNN [25], pre-trained using Visual Genome [17]. We set a threshold for object
detection to get φ = 10 to 100 objects per image. The features are represented as YT ∈ Rφ×2,048,
which is ﬁxed while training. To deal with variable-channel inputs, we mask the padding logits with
minus inﬁnite to get zero probability from softmax avoiding underﬂow.

4

5.3 Nonlinearity and classiﬁer

Nonlinearity. We use ReLU [19] to give nonlinearity to BAN:

where σ denotes ReLU(x) := max(x, 0). For the attention maps, the logits are deﬁned as:

k = σ(XT U(cid:48))T
f (cid:48)

k · A · σ(YT V(cid:48))k

A := (cid:0)(1 · pT ) ◦ σ(XT U)(cid:1) · σ(VT Y).

(12)

(13)

Classiﬁer. For VQA, we use a two-layer multi-layer perceptron as a classiﬁer for the ﬁnal joint
representation fG. The activation function is ReLU. The number of outputs is determined by the
minimum occurrence of the answer in unique questions as nine times in the dataset, which is 3,129.
Binary cross entropy is used for the loss function following the previous work [29]. For Flickr30k
Entities, we take the output of bilinear attention map, and binary cross entropy is used for this output.

5.4 Hyperparameters and regularization

Hyperparameters. The size of image features and question embeddings are M = 2, 048 and
N = 1, 024, respectively. The size of joint representation C is the same with the rank K in low-
rank bilinear pooling, C = K = 1, 024, but K (cid:48) = K × 3 is used in the bilinear attention maps
to increase a representational capacity for residual learning of attention. Every linear mapping is
regularized by Weight Normalization [27] and Dropout [28] (p = .2, except for the classiﬁer with
.5). Adamax optimizer [16], a variant of Adam based on inﬁnite norm, is used. The learning rate is
min(ie−3, 4e−3) where i is the number of epochs starting from 1, then after 10 epochs, the learning
rate is decayed by 1/4 for every 2 epochs up to 13 epochs (i.e. 1e−3 for 11-th and 2.5e−4 for 13-th
epoch). We clip the 2-norm of vectorized gradients to .25. The batch size is 512.

Regularization. For the test split of VQA, both train and validation splits are used for training. We
augment a subset of Visual Genome [17] dataset following the procedure of the previous works [29].
Accordingly, we adjust the model capacity by increasing all of N , C, and K to 1,280. And, G = 8
glimpses are used. For Flickr30k Entities, we use the same test split of the previous methods [23],
without additional hyperparameter tuning from VQA experiments.

6 VQA results and discussions

6.1 Quantitative results

Comparison with state-of-the-arts. The ﬁrst row in Table 1 shows 2017 VQA Challenge winner
architecture [2, 29]. BAN signiﬁcantly outperforms this baseline and successfully utilize up to eight
bilinear attention maps to improve its performance taking advantage of residual learning of attention.
As shown in Table 3, BAN outperforms the latest model [39] which uses the same bottom-up attention
feature [2] by a substantial margin. BAN-Glove uses the concatenation of 300-dimensional Glove
word embeddings and the semantically-closed mixture of these embeddings (see Appendix A.1).
Notice that similar approaches can be found in the competitive models [6, 39] in Table 3 with a
different initialization strategy for the same 600-dimensional word embedding. BAN-Glove-Counter
uses both the previous 600-dimensional word embeddings and counting module [41], which exploits
spatial information of detected object boxes from the feature extractor [2]. The learned representation
c ∈ Rφ+1 for the counting mechanism is linearly projected and added to joint representation after
applying ReLU (see Equation 15 in Appendix A.2). In Table 5 (Appendix), we compare with the
entries in the leaderboard of both VQA Challenge 2017 and 2018 achieving the 1st place at the time
of submission (our entry is not shown in the leaderboad since challenge entries are not visible).

Comparison with other attention methods. Unitary attention has a similar architecture with Kim
et al. [15] where a question embedding vector is used to calculate the attentional weights for multiple
image features of an image. Co-attention has the same mechanism of Yu et al. [39], similar to Lu et al.
[18], Xu and Saenko [36], where multiple question embeddings are combined as single embedding
vector using a self-attention mechanism, then unitary visual attention is applied. Table 2 conﬁrms that
bilinear attention is signiﬁcantly better than any other attention methods. The co-attention is slightly
better than simple unitary attention. In Figure 2a, co-attention suffers overﬁtting more severely
(green) than any other methods, while bilinear attention (blue) is more regularized compared with the

5

Table 1: Validation scores on VQA 2.0
dataset for the number of glimpses of the
BAN. The standard deviations are reported
after ± using three random initialization.

Table 2: Validation scores on VQA 2.0 dataset for
attention and integration mechanisms. The nParams
indicates the number of parameters. Note that the
hidden sizes of unitary attention and co-attention are
1,280, while 1,024 for the BAN.

Model

VQA Score

Bottom-Up [29]

63.37 ±0.21

BAN-1
BAN-2
BAN-4
BAN-8
BAN-12

65.36 ±0.14
65.61 ±0.10
65.81 ±0.09
66.00 ±0.11
66.04 ±0.08

Model

nParams

VQA Score

Unitary attention
Co-attention
Bilinear attention

BAN-4 (residual)
BAN-4 (sum)
BAN-4 (concat)

31.9M 64.59 ±0.04
32.5M 64.79 ±0.06
32.2M 65.36 ±0.14

44.8M 65.81 ±0.09
44.8M 64.78 ±0.08
51.1M 64.71 ±0.21

Figure 2: (a) learning curves. Bilinear attention (bi-att) is more robust to overﬁtting than unitary
attention (uni-att) and co-attention (co-att). (b) validation scores for the number of parameters. The
error bar indicates the standard deviation among three random initialized models, although it is too
small to be noticed for over-15M parameters. (c) ablation study for the ﬁrst-N-glimpses (x-axis) used
in evaluation. (d) the information entropy (y-axis) for each attention map in four-glimpse BAN. The
entropy of multiple attention maps is converged to certain levels.

others. In Figure 2b, BAN is the most parameter-efﬁcient among various attention methods. Notice
that four-glimpse BAN more parsimoniously utilizes its parameters than one-glimpse BAN does.

6.2 Residual learning of attention

Comparison with other approaches. In the second section of Table 2, the residual learning of
attention signiﬁcantly outperforms the other methods, sum, i.e., fG = (cid:80)
i BANi(X, Y; Ai), and
concatenation (concat), i.e., fG = (cid:107)iBANi(X, Y; Ai). Whereas, the difference between sum and
concat is not signiﬁcantly different. Notice that the number of parameters of concat is larger than the
others, since the input size of the classiﬁer is increased.

Ablation study. An interesting property of residual learning is robustness toward arbitrary abla-
tions [31]. To see the relative contributions, we observe the learning curve of validation scores
when incremental ablation is performed. First, we train {1,2,4,8,12}-glimpse models using training
split. Then, we evaluate the model on validation split using the ﬁrst N attention maps. Hence, the
intermediate representation fN is directly fed into the classiﬁer instead of fG. As shown in Figure 2c,
the accuracy gain of the ﬁrst glimpse is the highest, then the gain is smoothly decreased as the number
of used glimpses is increased.

Entropy of attention. We analyze the information entropy of attention distributions in a four-glimpse
BAN. As shown in Figure 2d, the mean entropy of each attention for validation split is converged
to a different level of values. This result is repeatably observed in the other number of glimpse
models. Our speculation is the multi-attention maps do not equally contribute similarly to voting
by committees, but the residual learning by the multi-step attention. We argue that this is a novel
observation where the residual learning [9] is used for stacked attention networks.

6

Figure 3: Visualization of the bilinear attention maps for two-glimpse BAN. The left and right
groups indicate the ﬁrst and second bilinear attention maps (right in each group, log-scaled) and the
visualized image (left in each group). The most salient six boxes (1-6 numbered in the images and
x-axis of the grids) in the ﬁrst attention map determined by marginalization are visualized on both
images to compare. The model gives the correct answer, brown.

Figure 4: Visualization examples from the test split of Flickr30k Entities are shown. Solid-lined
boxes indicate predicted phrase localizations and dashed-line boxes indicate the ground-truth. If there
are multiple ground-truth boxes, the closest box is shown to investigate. Each color of a phrase is
matched with the corresponding color of predicted and ground-truth boxes. Best view in color.

6.3 Qualitative analysis

The visualization for a two-glimpse BAN is shown in Figure 3. The question is “what color are the
pants of the guy skateboarding”. The question and content words, what, pants, guy, and skateboarding
and skateboarder’s pants in the image are attended. Notice that the box 2 (orange) captured the sitting
man’s pants in the bottom.

7 Flickr30k entities results and discussions

To examine the capability of bilinear attention map to capture vision-language interactions, we
conduct experiments on Flickr30k Entities [23]. Our experiments show that BAN outperforms the
previous state-of-the-art on the phrase localization task with a large margin of 4.48% at a high speed
of inference.

Performance. In Table 4, we compare with other previous approaches. Our bilinear attention map
to predict the boxes for the phrase entities in a sentence achieves new state-of-the-art with 69.69%
for Recall@1. This result is remarkable considering that BAN does not use any additional features
like box size, color, segmentation, or pose-estimation [23, 37]. Note that both Query-Adaptive
RCNN [10] and our off-the-shelf object detector [2] are based on Faster RCNN [25] and pre-trained
on Visual Genome [17]. Compared to Query-Adaptive RCNN, the parameters of our object detector
are ﬁxed and only used to extract 10-100 visual features and the corresponding box proposals.

Type. In Table 6 (included in Appendix), we report the results for each type of Flickr30k Entities.
Notice that clothing and body parts are signiﬁcantly improved to 74.95% and 47.23%, respectively.

Speed. The faster inference is achieved taking advantage of multi-channel inputs in our BAN. Unlike
previous methods, BAN ables to infer multiple entities in a sentence which can be prepared as a

7

Table 3: Test-dev and test-standard scores of single-model on VQA 2.0 dataset to compare state-of-
the-arts, trained on training and validation splits, and Visual Genome for feature extraction or data
augmentation. † This model can be found in https://github.com/yuzcccc/vqa-mfb, which is
not published in the paper.

Model

Overall Yes/no Number Other Test-std

Bottom-Up [2, 29]
MFH [39]
Counter [41]
MFH+Bottom-Up [39]†

BAN (ours)
BAN+Glove (ours)
BAN+Glove+Counter (ours)

65.32
66.12
68.09
68.76

69.52
69.66
70.04

81.82
-
83.14
84.27

85.31
85.46
85.42

44.21
-
51.62
49.56

50.93
50.66
54.04

56.05
-
58.97
59.89

60.26
60.50
60.52

65.67
-
68.41
-

-
-
70.35

Table 4: Test split results for Flickr30k Entities. We report the average performance of our three
randomly-initialized models (the standard deviation of R@1 is 0.17). Upper Bound of performance
asserted by object detector is shown. † box size and color information are used as additional features.
‡ semantic segmentation, object detection, and pose-estimation is used as additional features. Notice
that the detectors of Hinami and Satoh [10] and ours [2] are based on Faster RCNN [25], pre-trained
using Visual Genome dataset [17].

Model

Detector

R@1

R@5 R@10 Upper Bound

Zhang et al. [40]
Hu et al. [11]
Rohrbach et al. [26]
Wang et al. [33]
Wang et al. [32]
Rohrbach et al. [26]
Fukui et al. [6]
Plummer et al. [23]
Yeh et al. [37]
Hinami and Satoh [10] Query-Adaptive RCNN [10]

MCG [3]
Edge Boxes [42]
Fast RCNN [7]
Fast RCNN [7]
Fast RCNN [7]
Fast RCNN [7]
Fast RCNN [7]
Fast RCNN [7]†
YOLOv2 [24]‡

28.5
27.8
42.43
42.08
43.89
48.38
48.69
50.89
53.97
65.21

52.7
-
-
-
64.46
-
-
71.09
-
-

BAN (ours)

Bottom-Up [2]

69.69

84.22

61.3
62.9
-
-
68.66
-
-
75.73
-
-

86.35

-
76.9
77.90
76.91
76.91
77.90
-
85.12
-
-

87.45

multi-channel input. Therefore, the number of forwardings to infer is signiﬁcantly decreased. In our
experiment, BAN takes 0.67 ms/entity whereas the setting that single entity as an example takes 0.84
ms/entity, achieving 25.37% improvement. We emphasize that this property is a novel in our model
that considers every interaction among vision-language multi-channel inputs.

Visualization. Figure 4 shows three examples from the test split of Flickr30k Entities. The entities
which has visual properties, for instance, a yellow tennis suit and white tennis shoes in Figure 4a, and
a denim shirt in Figure 4b, are correct. However, relatively small object (e.g., a cigarette in Figure 4b)
and the entity that requires semantic inference (e.g., a male conductor in Figure 4c) are incorrect.

8 Conclusions

BAN gracefully extends unitary attention networks exploiting bilinear attention maps, where the joint
representations of multimodal multi-channel inputs are extracted using low-rank bilinear pooling.
Although BAN considers every pair of multimodal input channels, the computational cost remains in
the same magnitude, since BAN consists of matrix chain multiplication for efﬁcient computation. The
proposed residual learning of attention efﬁciently uses up to eight bilinear attention maps, keeping
the size of intermediate features constant. We believe our BAN gives a new opportunity to learn the
richer joint representation for multimodal multi-channel inputs, which appear in many real-world
problems.

8

Acknowledgments

We would like to thank Kyoung-Woon On, Bohyung Han, Hyeonwoo Noh, Sungeun Hong, Jaesun
Park, and Yongseok Choi for helpful comments and discussion. Jin-Hwa Kim was supported by 2017
Google Ph.D. Fellowship in Machine Learning and Ph.D. Completion Scholarship from College of
Humanities, Seoul National University. This work was funded by the Korea government (IITP-2017-
0-01772-VTT, IITP-R0126-16-1072-SW.StarLab, 2018-0-00622-RMI, KEIT-10060086-RISF). The
part of computing resources used in this study was generously shared by Standigm Inc.

References

[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C Lawrence Zitnick, Devi
Parikh, and Dhruv Batra. Vqa: Visual question answering. International Journal of Computer
Vision, 123(1):4–31, 2017.

[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould,
and Lei Zhang. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
Answering. arXiv preprint arXiv:1707.07998, 2017.

[3] Pablo Arbeláez, Jordi Pont-Tuset, Jonathan T Barron, Ferran Marques, and Jitendra Malik.
In IEEE conference on computer vision and pattern

Multiscale combinatorial grouping.
recognition, pages 328–335, 2014.

[4] Hedi Ben-younes, Rémi Cadene, Matthieu Cord, and Nicolas Thome. MUTAN: Multimodal
Tucker Fusion for Visual Question Answering. In IEEE International Conference on Computer
Vision, pages 2612–2620, 2017.

[5] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder-
Decoder for Statistical Machine Translation. In 2014 Conference on Empirical Methods in
Natural Language Processing, pages 1724–1734, 2014.

[6] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus
Rohrbach. Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
Grounding. arXiv preprint arXiv:1606.01847, 2016.

[7] Ross Girshick. Fast r-cnn. In IEEE International Conference on Computer Vision, pages

1440–1448, 2015.

[8] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V
in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In
IEEE Conference on Computer Vision and Pattern Recognition, 2017.

[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image

Recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.

[10] Ryota Hinami and Shin’ichi Satoh. Query-Adaptive R-CNN for Open-Vocabulary Object

Detection and Retrieval. arXiv preprint arXiv:1711.09509, 2017.

[11] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell.
Natural language object retrieval. In IEEE Computer Vision and Pattern Recognition, pages
4555–4564, 2016.

[12] Ilija Ilievski and Jiashi Feng. A Simple Loss Function for Improving the Convergence and

Accuracy of Visual Question Answering Models. 2017.

[13] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial Trans-
former Networks. In Advances in Neural Information Processing Systems 28, pages 2008–2016,
2015.

[14] Jin-Hwa Kim, Sang-Woo Lee, Donghyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha,
and Byoung-Tak Zhang. Multimodal Residual Learning for Visual QA. In Advances in Neural
Information Processing Systems 29, pages 361–369, 2016.

9

[15] Jin-Hwa Kim, Kyoung Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak
Zhang. Hadamard Product for Low-rank Bilinear Pooling. In The 5th International Conference
on Learning Representations, 2017.

[16] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization.

In

International Conference on Learning Representations, 2015.

[17] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual
genome: Connecting language and vision using crowdsourced dense image annotations. arXiv
preprint arXiv:1602.07332, 2016.

[18] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical Question-Image Co-

Attention for Visual Question Answering. arXiv preprint arXiv:1606.00061, 2016.

[19] Vinod Nair and Geoffrey E Hinton. Rectiﬁed Linear Units Improve Restricted Boltzmann
Machines. 27th International Conference on Machine Learning, pages 807–814, 2010.

[20] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual Attention Networks for Multimodal
Reasoning and Matching. In IEEE Conference on Computer Vision and Pattern Recognition,
2016.

[21] Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global Vectors for
Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing, 2014.

[22] Hamed Pirsiavash, Deva Ramanan, and Charless C. Fowlkes. Bilinear classiﬁers for visual
recognition. In Advances in Neural Information Processing Systems 22, pages 1482–1490,
2009.

[23] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier,
and Svetlana Lazebnik. Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
Richer Image-to-Sentence Models. International Journal of Computer Vision, 123:74–93, 2017.

[24] Joseph Redmon and Ali Farhadi. YOLO9000: Better, Faster, Stronger. In IEEE Computer

Vision and Pattern Recognition, 2017.

[25] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time
Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 39(6), 2017.

[26] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding
of textual phrases in images by reconstruction. In European Conference on Computer Vision,
pages 817–834, 2016.

[27] Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to
Accelerate Training of Deep Neural Networks. arXiv preprint arXiv:1602.07868, 2016.

[28] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. Dropout : A Simple Way to Prevent Neural Networks from Overﬁtting. Journal of
Machine Learning Research, 15(1):1929–1958, 2014.

[29] Damien Teney, Peter Anderson, Xiaodong He, and Anton van den Hengel. Tips and Tricks for Vi-
sual Question Answering: Learnings from the 2017 Challenge. arXiv preprint arXiv:1708.02711,
2017.

[30] Alexander Trott, Caiming Xiong, and Richard Socher.

Interpretable Counting for Visual

Question Answering. In International Conference on Learning Representations, 2018.

[31] Andreas Veit, Michael J Wilber, and Serge Belongie. Residual Networks are Exponential
Ensembles of Relatively Shallow Networks. In Advances in Neural Information Processing
Systems 29, pages 550–558, 2016.

10

[32] Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning Deep Structure-Preserving Image-
Text Embeddings. In IEEE Conference on Computer Vision and Pattern Recognition, pages
5005–5013, 2016.

[33] Mingzhe Wang, Mahmoud Azab, Noriyuki Kojima, Rada Mihalcea, and Jia Deng. Structured
Matching for Phrase Localization. In European Conference on Computer Vision, volume 9908,
pages 696–711, 2016.

[34] Peng Wang, Qi Wu, Chunhua Shen, and Anton van den Hengel. The VQA-Machine: Learning
How to Use Existing Vision Algorithms to Answer New Questions. In Computer Vision and
Pattern Recognition (CVPR), pages 1173–1182, 2017.

[35] Lior Wolf, Hueihan Jhuang, and Tamir Hazan. Modeling appearances with low-rank SVM.

IEEE Conference on Computer Vision and Pattern Recognition, 2007.

[36] Huijuan Xu and Kate Saenko. Ask, Attend and Answer: Exploring Question-Guided Spatial
Attention for Visual Question Answering. In European Conference on Computer Vision, 2016.

[37] Raymond A Yeh, Jinjun Xiong, Wen-Mei W Hwu, Minh N Do, and Alexander G Schwing.
Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts. In
Advances in Neural Information Processing Systems 30, 2017.

[38] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions
to visual denotations: New similarity metrics for semantic inference over event descriptions.
Transactions of the Association for Computational Linguistics, 2:67–78, 2014.

[39] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. Beyond Bilinear: Gen-
eralized Multi-modal Factorized High-order Pooling for Visual Question Answering. IEEE
Transactions on Neural Networks and Learning Systems, 2018.

[40] Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-Down Neural
Attention by Excitation Backprop. In European Conference on Computer Vision, volume 9908,
pages 543–559, 2016.

[41] Yan Zhang, Jonathon Hare, and Adam Prügel-Bennett. Learning to Count Objects in Natural Im-
ages for Visual Question Answering. In International Conference on Learning Representations,
2018.

[42] C Lawrence Zitnick and Piotr Dollár. Edge boxes: Locating object proposals from edges. In

European Conference on Computer Vision, pages 391–405, 2014.

11

Bilinear Attention Networks — Appendix

A Variants of BAN

A.1 Enhancing glove word embedding

We augment a computed 300-dimensional word embedding to each 300-dimensional Glove word
embedding. The computation is as follows: 1) we choose arbitrary two words wi and wj from each
question that can be found in VQA and Visual Genome datasets or each caption in MS COCO dataset.
2) we increase the value of Ai,j by one where A ∈ RV (cid:48)×V (cid:48)
is an association matrix initialized
with zeros. Notice that i and j can be the index out of vocabulary V and the size of vocabulary in
this computation is denoted by V (cid:48). 3) to penalize highly frequent words, each row of A is divided
by the number of sentences (question or caption) which contain the corresponding word . 4) each
row is normalized by the sum of all elements of each row. 5) we calculate W(cid:48) = A · W where
W ∈ RV (cid:48)×E is a Glove word embedding matrix and E is the size of word embedding, i.e., 300.
Therefore, W(cid:48) ∈ RV (cid:48)×E stands for the mixed word embeddings of semantically closed words. 6)
ﬁnally, we select V rows from W(cid:48) corresponding to the vocabulary in our model and augment these
rows to the previous word embeddings, which makes 600-dimensional word embeddings in total. The
input size of GRU is increased to 600 to match with these word embeddings. These word embeddings
are ﬁne-tuned.

As a result, this variant signiﬁcantly improves the performance to 66.03 (±0.12) compared with the
performance of 65.72 (± 0.11) which is done by augmenting the same 300-dimensional Glove word
embeddings (so the number of parameters is controlled). In this experiment, we use four-glimpse
BAN and evaluate on validation split. The standard deviation is calculated by three random initialized
models and the means are reported. The result on test-dev split can be found in Table 3 as BAN+Glove.

A.2

Integrating counting module

The counting module [41] is proposed to improve the performance related to counting tasks. This
module is a neural network component to get a dense representation from spatial information of
detected objects, i.e., the left-top and right-bottom positions of the φ proposed objects (rectangles)
denoted by S ∈ R4×φ. The interface of the counting module is deﬁned as:

c = Counter(s, ˜α)

(14)

where c ∈ Rφ+1 and ˜α ∈ Rφ is the logits of corresponding objects for sigmoid function inside
the counting module. We found that the ˜α deﬁned by maxj=1,...,φ(A·,j), i.e., the maximum values
in each column vector of A in Equation 9, was better than that of summation. Since the counting
module does not support variable-object inputs, we select 10-top objects for the input instead of φ
objects based on the values of ˜α.

The BAN integrated with the counting module is deﬁned as:

fi+1 = (cid:0)BANi(fi, Y; Ai) + gi(ci)(cid:1) · 1T + fi

(15)

where the function gi(·) is the i-th linear embedding followed by ReLU activation function and
ci = Counter(s, maxj=1,...,φ(A(i)
·,j )) where A(i) is the logit of Ai. Note that a dropout layer before
this linear embedding severely hurts performance, so we did not use it.

As a result, this variant signiﬁcantly improves the counting performance from 54.92 (±0.30) to
58.21 (±0.49), while overall performance is improved from 65.81 (±0.09) to 66.01 (±0.14) in a
controlled experiment using a vanilla four-glimpse BAN. The deﬁnition of a subset of counting
questions comes from the previous work [30]. The result on test-dev split can be found in Table 3 as
BAN+Glove+Counter, notice that, which is applied by the previous embedding variant, too.

12

A.3

Integrating multimodal factorized bilinear (MFB) pooling

Yu et al. [39] extend low-rank bilinear pooling [15] with the rank k > 1 and two factorized three-
dimensional matrices, which called as MFB. The implementation of MFB is effectively equivalent to
low-rank bilinear pooling with the rank d(cid:48) = d × k followed by sum pooling with the window size
of k and the stride of k, deﬁned by SumPool( ˜UT x ◦ ˜VT y, k). Notice that a pooling matrix P in
Equation 2 is not used. The variant of BAN inspired by MFB is deﬁned as:

zk(cid:48) = σ(XT ˜U)T
f (cid:48) = SumPool(z, k)

k(cid:48) · A · σ(YT ˜V)k(cid:48)

(16)

(17)

, ˜V ∈ RM ×K(cid:48)

where ˜U ∈ RN ×K(cid:48)
et al. [39]. Notice that K (cid:48) = K × k and k(cid:48) is the index for the elements in z ∈ RK(cid:48)
However, this generalization was not effective for BAN. In Figure 2b, the performance of BAN-
1+MFB is not signiﬁcantly different from that of BAN-1. Furthermore, the larger K (cid:48) increases the
peak consumption of GPU memory which hinders to use multiple-glimpses for the BAN.

, σ denotes ReLU activation function, and k = 5 following Yu
in our notation.

Table 5: Test-standard scores of ensemble-model on VQA 2.0 dataset to compare state-of-the-arts.
Excerpt from the VQA 2.0 Leaderboard at the time of writing. # denotes the number of models for
their ensemble methods.

Team Name

Overall Yes/no Number Other

vqateam_mcb_benchmark [6, 8]
vqa_hack3r
VQAMachine [34]
NWPU_VQA
yahia zakaria
ReasonNet_
JuneﬂowerIvaNlpr
UPMC-LIP6 [4]
Athena
Adelaide-Teney
LV_NUS [12]
vqahhi_drau
CFM-UESTC
VLC Southampton [41]
Tohoku CV
VQA-E
Adelaide-Teney ACRV MSR [29]
DeepSearch
HDU-USYD-UNCC [39]

BAN+Glove+Counter (ours)
BAN Ensemble (ours)
BAN Ensemble (ours)

#

1
-
-
-
-
-
-
-
-
-
-
-
-
1
-
-
30
-
8

1
8
15

62.27
62.89
62.97
63.00
63.57
64.61
65.70
65.71
66.67
66.73
66.77
66.85
67.02
68.41
68.91
69.44
70.34
70.40
70.92

70.35
71.72
71.84

78.82
79.88
79.82
80.38
79.77
78.86
81.09
82.07
82.88
83.71
81.89
83.35
83.69
83.56
85.54
85.74
86.60
86.21
86.65

85.82
87.02
87.22

38.28
38.95
40.91
40.32
40.53
41.98
41.56
41.06
43.17
43.77
46.29
44.37
45.17
51.39
49.00
48.18
48.64
48.82
51.13

53.71
54.41
54.37

53.36
53.58
53.35
53.07
54.75
57.39
57.83
57.12
57.95
57.20
58.30
57.63
57.52
59.11
58.99
60.12
61.15
61.58
61.75

60.69
62.37
62.45

Table 6: Recall@1 performance over types for Flickr30k Entities (%)

Model

Clothing

Body Parts

Animals

Vehicles

Instruments

Scene Other

Rohrbach et al. [26]
Plummer et al. [23]
Yeh et al. [37]
Hinami and Satoh [10]

BAN (ours)

# of Instances

People

60.24
64.73
68.71
78.17

79.90

5,656

39.16
46.88
46.83
61.99

74.95

2,306

64.48
65.83
70.07
74.41

81.85

518

67.50
68.75
73.75
76.16

76.92

400

38.27
37.65
39.50
56.69

43.00

162

59.17
51.39
60.38
68.07

30.56
31.77
32.45
47.42

68.69

51.33

1,619

3,374

14.34
17.21
19.50
35.25

47.23

523

13

