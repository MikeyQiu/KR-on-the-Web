9
1
0
2
 
n
u
J
 
4
1
 
 
]

G
L
.
s
c
[
 
 
2
v
1
5
4
5
0
.
6
0
8
1
:
v
i
X
r
a

The committee machine: Computational to statistical gaps
in learning a two-layers neural network

Benjamin Aubin(cid:63)†, Antoine Maillard†, Jean Barbier♦,
Florent Krzakala†, Nicolas Macris⊗ and Lenka Zdeborová(cid:63)

Abstract

Heuristic tools from statistical physics have been used in the past to locate the phase transitions and
compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural
networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers
neural network model called the committee machine, under a technical assumption. We also introduce a
version of the approximate message passing (AMP) algorithm for the committee machine that allows to
perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes
in which a low generalization error is information-theoretically achievable while the AMP algorithm fails
to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large
computational gap.

Contents

1 Introduction

2 Summary of contributions and related works

3 Main technical results
3.1 A general model .
.
3.2 Two auxiliary inference problems .
3.3 The free entropy .
.
3.4
3.5 Approximate message passing, and its state evolution .

.
.
.
.
Learning the teacher weights and optimal generalization error .
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

4 From two to more hidden neurons, and the specialization phase transition
.
.
.
.

4.1 Two neurons .
.
4.2 More is different .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

5 Structure of the proof of Theorem 3.1
Interpolating estimation problem .

5.1
.
5.2 Overlap concentration and fundamental sum rule .
.
5.3 A technical lemma and an assumption .
.
.
5.4 Matching bounds

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

3

3

5
5
5
6
7
8

10
10
11

12
12
14
15
16

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

(cid:63) Institut de Physique Théorique, CNRS & CEA & Université Paris-Saclay, Saclay, France.
† Laboratoire de Physique Statistique, CNRS & Sorbonnes Universités & École Normale Supérieure, PSL University, Paris, France.
⊗ Laboratoire de Théorie des Communications, École Polytechnique Fédérale de Lausanne, Suisse.
♦ International Center for Theoretical Physics, Trieste, Italy.

1

C.1 The generalization error at K = 2 .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

6 Discussion

A Proof details for Theorem 3.1

A.1 The Nishimori property in Bayes-optimal learning .
.
.
A.2 Setting in the Hamiltonian language .
.
A.3 Free entropy variation: Proof of Proposition 5.2 .
.
.
.
.
A.4 Technical lemmas .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

B Replica calculation

C Generalization error

D The large K limit in the committee symmetric setting
.
.
.
.

D.1 Large K limit for sign activation function .
.
D.2 The Gaussian prior
.
.
.
.
D.3 The fixed point equations .
.
D.4 The generalization error at large K .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

E Linear networks show no specialization

F Update functions and AMP derivation
F.1 Definition of the update functions .
.
F.2 Derivation of the Approximate Message Passing algorithm .

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

G State evolution equations from AMP
.

.

.

.

.
G.1 Messages distribution .
.
.
.
G.2 State evolution equations - Non Bayes optimal case .
G.3 State evolution equations - Bayes optimal case .
.
.
G.4 State evolution - Consistence between replicas and AMP - Bayes optimal case .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

H Parity machine for K = 2

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

18

23
23
23
24
26

29

31
32

32
33
36
36
38

38

39
39
39

44
45
46
47
47

49

2

1 Introduction

While the traditional approach to learning and generalization follows the Vapnik-Chervonenkis [1] and
Rademacher [2] worst-case type bounds, there has been a considerable body of theoretical work on calculating
the generalization ability of neural networks for data arising from a probabilistic model within the framework
of statistical mechanics [3, 4, 5, 6, 7]. In the wake of the need to understand the effectiveness of neural
networks and also the limitations of the classical approaches [8], it is of interest to revisit the results that have
emerged thanks to the physics perspective. This direction is currently experiencing a strong revival, see e.g.
[9, 10, 11, 12].

Of particular interest is the so-called teacher-student approach, where labels are generated by feeding
i.i.d. random samples to a neural network architecture (the teacher) and are then presented to another neural
network (the student) that is trained using these data. Early studies computed the information theoretic
limitations of the supervised learning abilities of the teacher weights by a student who is given m independent
n-dimensional examples with α ≡ m/n = Θ(1) and n → ∞ [3, 4, 7]. These works relied on non-rigorous
heuristic approaches, such as the replica and cavity methods [13, 14]. Additionally no provably efficient
algorithm was provided to achieve the predicted learning abilities, and it was thus difficult to test those
predictions, or to assess the computational difficulty.

Recent developments in statistical estimation and information theory —in particular of approximate
message passing algorithms (AMP) [15, 16, 17, 18], and a rigorous proof of the replica formula for the optimal
generalization error [11]— allowed to settle these two missing points for single-layer neural networks (i.e.
without any hidden variables). In the present paper, we leverage on these works, and provide rigorous
asymptotic predictions and corresponding message passing algorithm for a class of two-layers networks.

2 Summary of contributions and related works

While our results hold for a rather large class of non-linear activation functions, we illustrate our findings on a
case considered most commonly in the early literature: the committee machine. This is possibly the simplest
version of a two-layers neural network where all the weights in the second layer are fixed to unity, and we
illustrate it in Fig. 1. Denoting Yµ the label associated with a n-dimensional sample Xµ, and W ∗
il the weight
connecting the i-th coordinate of the input to the l-th node of the hidden layer, it is defined by:

K

n

Yµ = sign

sign

XµiW ∗
il

.

(cid:104)

l=1
(cid:88)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

(1)

We concentrate here on the teacher-student scenario: The teacher generates i.i.d. data samples with i.i.d.
standard Gaussian coordinates Xµi ∼ N (0, 1), then she/he generates the associated labels Yµ using a committee
machine as in (1), with i.i.d. weights W ∗
il unknown to the student (in the proof section we will consider the
l=1), but in practice we consider
more general case of a distribution for the weights of the form
the fully separable case). The student is then given the m input-output pairs (Xµ, Yµ)m
µ=1 and knows the
(cid:81)
distribution P0 used to generate W ∗
il. The goal of the student is to learn the weights W ∗
il from the available
examples (Xµ, Yµ)m
µ=1 in order to reach the smallest possible generalization error (i.e. to be able to predict the
label the teacher would generate for a new sample not present in the training set).

n
i=1 P0({W ∗

il}K

There have been several studies of this model within the non-rigorous statistical physics approach in
the limit where α ≡ m/n = Θ(1), K = Θ(1) and n → ∞ [19, 20, 21, 22, 6, 7]. A particularly interesting
result in the teacher-student setting is the specialization of hidden neurons (see sec. 12.6 of [7], or [23] in the
context of online learning): For α < αspec (where αspec is a certain critical value of the sample complexity),

3

n features

(Xµi)m,n
µ,i=1
samples

K hidden
units

f (1)

f (1)

output

f (2)

Yµ

W (2) ∈ RK

W ∗

il ∈ Rn×K

Figure 1: The committee machine is one of the simplest models belonging to the considered model class (2), and
on which we focus to illustrate our results. It is a two-layers neural network with activation sign functions
f (1), f (2) = sign and weights W (2) fixed to unity. It is represented for K = 2.

the permutational symmetry between hidden neurons remains conserved even after an optimal learning, and
the learned weights of each of the hidden neurons are identical. For α > αspec, however, this symmetry gets
broken as each of the hidden units correlates strongly with one of the hidden units of the teacher. Another
remarkable result is the calculation of the optimal generalization error as a function of α.

Our first contribution consists in a proof of the replica formula conjectured in the statistical physics
literature, using the adaptive interpolation method of [24, 11], that allows to put several of these results on a
rigorous basis. This proof uses a technical unproven assumption. Our second contribution is the design of
an AMP-type of algorithm that is able to achieve the optimal generalization error in the above limit of large
dimensions for a wide range of parameters. The study of AMP —that is widely believed to be optimal between
all polynomial algorithms in the above setting [25, 26, 27, 28]— unveils, in the case of the committee machine
with a large number of hidden neurons, the existence a large hard phase in which learning is information-
theoretically possible, leading to a good generalization error decaying asymptotically as 1.25K/α (in the
α = Θ(K) regime), but where AMP fails and provides only a poor generalization that does not go to zero
when increasing α. This strongly suggests that no efficient algorithm exists in this hard region and therefore
there is a computational gap in learning such neural networks. In other problems where a hard phase was
identified its study boosted the development of algorithms that are able to match the predicted thresholds and
we anticipate this will translate to the present model.

We also want to comment on a related line of work that studies the loss-function landscape of neural
networks. While a range of works show under various assumptions that spurious local minima are absent
in neural networks, others show under different conditions that they do exist, see e.g. [29]. The regime of
parameters that is hard for AMP must have spurious local minima, but the converse is not true in general. It
might be that there are spurious local minima, yet the AMP approach succeeds. Moreover, in all previously
studied models in the Bayes-optimal setting the (generalization) error obtained with the AMP is the best
known and other approaches, e.g. (noisy) gradient based, spectral algorithms or semidefinite programming,
are not better in generalizing even in cases where the “student” models are overparametrized. Of course in
order to be in the Bayes-optimal setting one needs to know the model used by the teacher which is not the
case in practice.

4

3 Main technical results

3.1 A general model

While in the illustration of our results we shall focus on the model (1), all our formulas are valid for a broader
class of models: Given m input samples (Xµi)m,n
il the teacher-weight connecting the i-th
input (i.e. visible unit) to the l-th node of the hidden layer. For a generic function ϕout : RK × R → R one can
formally write the output as

µ,i=1, we denote W ∗

Yµ = ϕout

n

1
√
n

(cid:16)(cid:110)

i=1
(cid:88)

XµiW ∗
il

K

l=1

, Aµ

(cid:111)

(cid:17)

or

Yµ ∼ Pout

·

1
√
n

n

i=1
(cid:88)

(cid:16)

(cid:12)
(cid:110)
(cid:12)
(cid:12)

K

XµiW ∗
il

,

(2)

l=1

(cid:111)

(cid:17)

where (Aµ)m
part of the model, generally accounting for noise.

µ=1 are i.i.d. real valued random variables with known distribution PA, that form the probabilistic

For deterministic models the second argument is simply absent (or is a Dirac mass). We can view altern-
atively (2) as a channel where the transition kernel Pout is directly related to ϕout. As discussed above, we
focus on the teacher-student scenario where the teacher generates Gaussian i.i.d. data Xµi ∼ N (0, 1), and i.i.d.
weights W ∗
µ=1 by computing marginal means of
the posterior probability distribution (5).

il ∼ P0. The student then learns W ∗ from the data (Xµ, Yµ)m

Different scenarii fit into this general framework. Among those, the committee machine is obtained when
K
l=1 sign(hl)) while another model considered previously is given by the parity
choosing ϕout(h) = sign(
K
l=1 sign(hl), see e.g. [7] and sec. H for the numerical results in the case K = 2.
machine, when ϕout(h) =
A number of layers beyond two has also been considered, see [22]. Other activation functions can be used, and
many more problems can be described, e.g. compressed pooling [30, 31] or multi-vector compressed sensing
[32].

(cid:80)
(cid:81)

3.2 Two auxiliary inference problems

Denote SK the finite dimensional vector space of K × K matrices, S +
K × K matrices, S ++
K s.t. N − M ∈ S +
S+

K for positive definite K × K matrices, and ∀ N ∈ S +
K}. Note that S +
K(N ) is convex and compact.

K the convex set of semi-definite positive
K(N ) ≡ {M ∈

K we set S+

Stating our results requires introducing two simpler auxiliary K-dimensional estimation problems:
• The first one consists in retrieving a K-dimensional input vector W0 ∼ P0 from the output of a Gaussian
vector channel with K-dimensional observations

Y0 = r1/2W0 + Z0 ,

Z0 ∼ N (0, IK×K) and the “channel gain” matrix r ∈ S +

K. The posterior distribution on w = (wl)K

l=1 is

P (w|Y0) =

P0(w)eY

(cid:124)
0 r1/2w− 1

2 w(cid:124)rw ,

1
ZP0

(3)

and the associated free entropy (or minus free energy) is given by the expectation over Y0 of the log-partition
function

ψP0(r) ≡ E ln ZP0

and involves K dimensional integrals.
• The second problem considers K-dimensional i.i.d. vectors V, U ∗ ∼ N (0, IK×K) where V is considered to

5

be known and one has to retrieve U ∗ from a scalar observation obtained as

Y0 ∼ Pout( · |q1/2V + (ρ − q)1/2U ∗)

where the second moment matrix ρ ≡ E[W0W
q is in S+

K(ρ). The associated posterior is

(cid:101)

(cid:124)
0 ] is in S +

K (where W0 ∼ P0) and the so-called “overlap matrix”

P (u|

Y0, V ) =

1
ZPout

2 u(cid:124)u
e− 1
(2π)K/2 Pout

Y0|q1/2V + (ρ − q)1/2u

,

(4)

and the free entropy reads this time

(cid:101)

(cid:0)

(cid:101)

(cid:1)

ΨPout(q; ρ) ≡ E ln ZPout

(with the expectation over

Y0 and V ) and also involves K dimensional integrals.

3.3 The free entropy

(cid:101)

The central object of study leading to the optimal learning and generalization errors in the present setting is
the posterior distribution of the weights:

P ({wil}n,K

i,l=1 | {Xµi, Yµ}m,n

µ,i=1) =

P0({wil}K

l=1)

Pout

Yµ

Xµiwil

,

(5)

1
Zn

n

i=1
(cid:89)

m

µ=1
(cid:89)

(cid:16)

1
√
n

n

i=1
(cid:88)

(cid:110)

(cid:12)
(cid:12)
(cid:12)

K

l=1

(cid:111)

(cid:17)

where the normalization factor is nothing else than a partition function, i.e. the integral of the numerator over
{wil}n,K

i,l=1. The expected1 free entropy is by definition

fn ≡

E ln Zn .

1
n

The replica formula gives an explicit (conjectural) expression of fn in the high-dimensional limit n, m → ∞
with α = m/n fixed. We show in sec. B how the heuristic replica method [13, 14] yields the formula. This
computation was first performed, to the best of our knowledge, by [19] in the case of the committee machine.
Our first contribution is a rigorous proof of the corresponding free entropy formula using an interpolation
method [33, 34, 24], under a technical Assumption 1.

In order to formulate our results, we add an (arbitrarily small) Gaussian regularization noise Zµ

∆ to the

√

first expression of the model (2), where ∆ > 0, Zµ ∼ N (0, 1), which thus becomes

Yµ = ϕout

n

1
√
n

(cid:16)(cid:110)

i=1
(cid:88)

XµiW ∗
il

, Aµ

+ Zµ

∆ ,

√

K

l=1

(cid:111)

(cid:17)

so that the channel kernel is (u ∈ RK)

Pout(y|u) =

√

dPA(a)e− 1

2∆ (y−ϕout(u,a))2

.

1
2π∆

R

(cid:90)
Let us define the replica symmetric (RS) potential as

fRS(q, r) = fRS(q, r; ρ) ≡ ψP0(r) + αΨPout(q; ρ) −

Tr(rq),

1
2

1The symbol E will generally denote an expectation over all random variables in the ensuing expression (here {Xµi, Yµ}).

Subscripts will be used only when we take partial expectations or if there is an ambiguity.

(6)

(7)

(8)

(9)

6

where α ≡ m/n, and ΨPout(q; ρ) and ψP0(r) are the free entropies of the two simpler K-dimensional
estimation problems (3) and (4).

All along this paper, we assume the following hypotheses for our rigorous statements:

(H1) The prior P0 has bounded support in RK.
(H2) The activation ϕout : RK × R → R is a bounded C2 function with bounded first and second derivatives

w.r.t. its first argument (in RK-space).

(H3) For all µ = 1, . . . , m and i = 1, . . . , n we have i.i.d. Xµi ∼ N (0, 1).

We finally rely on a technical hypothesis, stated as Assumption 1 in section 5.3.

Theorem 3.1 (Replica formula). Suppose (H1), (H2) and (H3), and Assumption 1. Then for the model (7) with
kernel (8) the limit of the free entropy is:

lim
n→∞

fn ≡ lim
n→∞

1
n

E ln Zn = sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

(10)

This theorem extends the recent progress for generalized linear models of [11], which includes the case
K = 1 of the present contribution, to the phenomenologically richer case of two-layers problems such as the
committee machine. The proof sketch based on an adaptive interpolation method recently developed in [24] is
outlined in sec. 5 and the details can be found in sec. A.

Remark 3.2 (Relaxing the hypotheses). Note that, following similar approximation arguments as in [11], the
hypothesis (H1) can be relaxed to the existence of the second moment of the prior; thus covering the Gaussian case,
(H2) can be dropped (and thus include model (1) and its sign(·) activation) and (H3) extended to data matrices X
with i.i.d. entries of zero mean, unit variance and finite third moment. Moreover, the case ∆ = 0 can be considered
when the outputs are discrete, as in the committee machine (1), see [11]. The channel kernel becomes in this case
dPA(a)1(y − ϕout(u, a)) and the replica formula is the limit ∆ → 0 of the one provided in
Pout(y|u) =
Theorem 3.1. In general this regularizing noise is needed for the free entropy limit to exist.

(cid:82)

3.4 Learning the teacher weights and optimal generalization error

A classical result in Bayesian estimation is that the estimator ˆW that minimizes the mean-square error with the
ground-truth W ∗ is given by the expected mean of the posterior distribution. Denoting q∗ the extremizer in
the replica formula (10), we expect from the replica method that in the limit n → ∞, m/n = α, and with high
probablity, ˆW (cid:124)W ∗/n → q∗. We refer to proposition 5.3 and to the proof in sec. A for the precise statement,
that remains rigorously valid only in the presence of an additional (possibly infinitesimal) side-information.
From the overlap matrix q∗, one can compute the Bayes-optimal generalization error when the student tries to
classify a new, yet unseen, sample Xnew. The estimator of the new label ˆYnew that minimizes the mean-square
error with the true label is given by computing the posterior mean of ϕout(Xneww) (Xnew is a row vector).
Given the new sample, the optimal generalization error is then

1
2

EX,W ∗

Ew|X,Y

ϕout(Xneww)

− ϕout(XnewW ∗)

2

−−−→
n→∞

(cid:15)g(q∗),

(11)

(cid:104)(cid:0)

(cid:2)

(cid:3)

(cid:105)

(cid:1)

where w is distributed according to the posterior measure (5) (note that this Bayes-optimal computation differs
from the so-called Gibbs estimator by a factor 2, see sec. C). In particular, when the data X is drawn from
the standard Gaussian distribution on Rm×n, and is thus rotationally invariant, it follows that this error only
depends on w(cid:124)W ∗/n, which converges to q∗. Then a direct algebraic computation gives a lengthy but explicit
formula for (cid:15)g(q∗), as shown in sec. C.

7

3.5 Approximate message passing, and its state evolution

Our next result is based on an adaptation of a popular algorithm to solve random instances of generalized linear
models, the Approximate Message Passing (AMP) algorithm [15, 16], for the case of the committee machine and
models described by (2).

The AMP algorithm can be obtained as a Taylor expansion of loopy belief-propagation (see sec. F) and
also originates in earlier statistical physics works [35, 36, 37, 38, 39, 26]. It is conjectured to perform the best
among all polynomial algorithms in the framework of these models. It thus gives us a tool to evaluate both the
intrinsic algorithmic hardness of the learning and the performance of existing algorithms with respect to the
optimal one in this model.

Algorithm 1 Approximate Message Passing for the committee machine

Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: gout,µ = 0, Σi = IK×K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
Initialize: ˆWi ∈ RK and ˆCi, ∂ωgout,µ ∈ S +
repeat

K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 1.

n

ωt

ˆW t

µ =

i −

Xµi√
n

Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
X 2
µi
n

i Σt−1
i

−1 ˆCt

gt−1
out,µ

Σt−1
i

i=1
(cid:80)
out,µ = gout(ωt
gt

Update of gout,µ ∈ RK and ∂ωgout,µ ∈ S +
K:
out,µ = ∂ωgout(ωt

µ, Yµ, V t
µ)
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt

(cid:1)
∂ωgt

Xµi√

i = Σt
T t
i

X 2
n ∂ωgt
µi

out,µ −

ˆW t
i

out,µ

n gt

m

(cid:1)

(cid:0)

(cid:0)

|

|

(cid:16)

µ=1
(cid:80)

Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = fw(Σt

i, T t
i )

|

ˆCt+1

i = fc(Σt

i, T t
i )

(cid:17)

µ, Yµ, V t
µ)

i = −
(cid:16)

m

µ=1
(cid:80)

V t
µ =

X 2
µi
n

ˆCt
i

n

i=1
(cid:80)

X 2
n ∂ωgt
µi

out,µ

−1

(cid:17)

ˆW t+1
t = t + 1

until Convergence on ˆW , ˆC.
Output: ˆW and ˆC.

The AMP algorithm is summarized by its pseudo-code in Algorithm 2, where the update functions gout,
∂ωgout, fw and fc are related, again, to the two auxiliary problems (3) and (4). The functions fw(Σ, T ) and
fc(Σ, T ) are respectively the mean and variance under the posterior distribution (3) when r → Σ−1 and
Y0 → Σ1/2T , while gout(ωµ, Yµ, Vµ) is given by the product of V
and the mean of u under the posterior
Y0 → Yµ, ρ − q → Vµ and q1/2V → ωµ (see sec. F for more details). After convergence, ˆW estimates
(4) using
the weights of the teacher-neural network. The label of a sample Xnew not seen in the training set is estimated
by the AMP algorithm as

−1/2
µ

(cid:101)

Y t
new =

dy

dzl

y Pout(y|{zl}K

l=1)N (z; ωt

new, V t

new) ,

(12)

K

(cid:90)

l=1
(cid:89)

(cid:0)

(cid:1)

new =

n
i=1 Xnew,i ˆW t

where ωt
is the K × K covariance matrix (see below for the definition of qt
(cid:80)
the algorithm on GitHub [40].

i is the mean of the normally distributed variable z ∈ RK, and V t

new = ρ−qt

AMP
AMP). We provide a demonstration code of

AMP is particularly interesting because its performance can be tracked rigorously, again in the asymptotic
limit when n → ∞, via a procedure known as state evolution (a rigorous version of the cavity method in
physics [14]), see [18]. State evolution tracks the value of the overlap between the hidden ground truth W ∗ and

8

Figure 2: Generalization error and order parameter for a committee machine with two hidden neurons (K = 2)
with Gaussian weights (left), binary/Rademacher weights (right). These are shown as a function of the ratio
α = m/n between the number of samples m and the dimensionality n. Lines are obtained from the state evolution
(SE) equations (dominating solution is shown in full line), data-points from the AMP algorithm averaged over 10
instances of the problem of size n = 104. q00 and q01 denote diagonal and off-diagonal overlaps, and their values
are given by the labels on the far-right of the figure.

the AMP estimate ˆW t, defined as qt

AMP ≡ limn→∞( ˆW t)(cid:124)W ∗/n, via the iteration of the following equations:

qt+1
AMP = 2∇ψP0(rt

AMP) ,

rt+1
AMP = 2α∇ΨPout(qt

AMP; ρ) .

(13)

See sec. G for more details and note that the fixed points of these equations correspond to the critical points of
the replica free entropy (10).

Let us comment further on the convergence of the algorithm. In the large n limit, and if the integrals
are performed without errors, then the algorithm is guaranteed to converge. This is a consequence of the
state evolution combined with the Bayes-optimal setting. In practice, of course, n is finite and integrals are
approximated. In that case convergence is not guaranteed, but is robustly achieved in all the cases presented
in this paper. We also expect (by experience with the single layer case) that if the input-data matrix is not
random (which is beyond our assumptions) then we will encounter convergence issues, which could be fixed
by moving to some variant of the algorithm such as VAMP [41].

9

Figure 3: (Left) Bayes optimal and AMP generalization errors and (right) diagonal and off-diagonal overlaps q00,
q01 for a committee machine with a large number of hidden neurons K and Gaussian weights, as a function of the
rescaled parameter ˜α = α/K. Solutions corresponding to global and local minima of the replica free entropy are
spinodal (cid:39) 7.17, ie the
respectively represented with full and dashed lines. The dotted line marks the spinodal at
apparition of a local minimum in the replica free entropy, associated to a solution with specialized hidden units.
spec (cid:39) 7.65, at which the specialized
The dotted-dashed line shows the first order specialization transition at
spec, AMP reaches the Bayes optimal generalization error
fixed point becomes the global minimum. For
and overlaps, corresponding to a non-specialized solution. However, for
spec, the AMP algorithm does not
follow the optimal specialized solution and is stuck in the non-specialized solution plateau, represented with
dashed lines. Hence it unveils a large computational gap (yellow area).

α >
(cid:101)

α <

αG

αG

αG

αG

(cid:101)

(cid:101)

(cid:101)

(cid:101)

(cid:101)

4 From two to more hidden neurons, and the specialization

phase transition

4.1 Two neurons

Let us now discuss how the above results can be used to study the optimal learning in the simplest non-trivial
case of a two-layers neural network with two hidden neurons, that is when model (1) is simply

n

n

Yµ = sign

sign

XµiW ∗
i1

+ sign

XµiW ∗
i2

,

(cid:104)

(cid:16)

i=1
(cid:88)

(cid:17)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

and is represented in Fig. 1, with the convention that sign(0) = 0. We remind that the input-data matrix X
has i.i.d. N (0, 1) entries, and the teacher-weights W ∗ used to generate the labels Y are taken i.i.d. from P0.
In Fig. 2 we plot the optimal generalization error as a function of the sample complexity α = m/n. In
the left panel the weights are Gaussian (for both the teacher and the student), while in the right panel they
are binary/Rademacher. The full line is obtained from the fixed point of the state evolution (SE) of the AMP
algorithm (13), corresponding to the extremizer of the replica free entropy (10). The points are results of
the AMP algorithm run till convergence averaged over 10 instances of size n = 104. In this case and with
random initial conditions the AMP algorithm did converge in all our trials. As expected we observe excellent

10

agreement between the SE and AMP.

In both left and right panels of Fig. 2 we observe the so-called specialization phase transition. Indeed
(13) has two types of fixed points: a non-specialized fixed point where every matrix element of the K × K
order parameter q is the same (so that both hidden neurons learn the same function) and a specialized fixed
point where the diagonal elements of the order parameter are different from the non-diagonal ones. We
checked for other types of fixed points for K = 2 (one where the two diagonal elements are not the same),
but have not found any. In terms of weight-learning, this means for the non-specialized fixed point that the
estimators for both W1 and W2 are the same, whereas in the specialized fixed point the estimators of the
weights corresponding to the two hidden neurons are different, and that the network “figured out” that the
data are better described by a model that is not linearly separable. The specialized fixed point is associated
with lower error than the non-specialized one (as one can see in Fig. 2). The existence of this phase transition
was discussed in statistical physics literature on the committee machine, see e.g. [20, 23].

For Gaussian weights (Fig. 2 left), the specialization phase transition arises continuously at αG

spec(K =
2) (cid:39) 2.04. This means that for α < αG
spec(K = 2) the number of samples is too small, and the student-neural
network is not able to learn that two different teacher-vectors W1 and W2 were used to generate the observed
labels. For α > αG
spec(K = 2), however, it is able to distinguish the two different weight-vectors and the
generalization error decreases fast to low values (see Fig. 2). For completeness we remind that in the case
of K = 1 corresponding to single-layer neural network no such specialization transition exists. We show
in sec. E that it is absent also in multi-layer neural networks as long as the activations remain linear. The
non-linearity of the activation function is therefore an essential ingredient in order to observe a specialization
phase transition.

The right part of Fig. 2 depicts the fixed point reached by the state evolution of AMP for the case of binary
weights. We observe two phase transitions in the performance of AMP in this case: (a) the specialization phase
spec(K = 2) (cid:39) 1.58, and for slightly larger sample complexity a transition towards perfect
transition at αB
generalization (beyond which the generalization error is asymptotically zero) at αB
perf (K = 2) (cid:39) 1.99. The
binary case with K = 2 differs from the Gaussian one in the fact that perfect generalization is achievable at finite
α. While the specialization transition is continuous here, the error has a discontinuity at the transition of perfect
generalization. This discontinuity is associated with the 1st order phase transition (in the physics nomenclature),
leading to a gap between algorithmic (AMP in our case) performance and information-theoretically optimal
performance reachable by exponential algorithms. To quantify the optimal performance we need to evaluate
the global extremum of the replica free entropy (not the local one reached by the state evolution). In doing
so that we get that information theoretically there is a single discontinuous phase transition towards perfect
generalization at αB

IT(K = 2) (cid:39) 1.54.

While the information-theoretic and specialization phase transitions were identified in the physics literature
on the committee machine [20, 21, 3, 4], the gap between the information-theoretic performance and the
performance of AMP —that is conjectured to be optimal among polynomial algorithms— was not yet discussed
in the context of this model. Indeed, even its understanding in simpler models than those discussed here, such
as the single layer case, is more recent [15, 26, 25].

4.2 More is different

It becomes more difficult to study the replica formula for larger values of K as it involves (at least) K-
dimensional integrals. Quite interestingly, it is possible to work out the solution of the replica formula in
the large K limit (thus taken after the large n limit, so that K/n vanishes). It is indeed natural to look for
(cid:124)
K, with the
solutions of the replica formula, as suggested in [19], of the form q = qdIK×K + (qa/K)1K1
unit vector 1K = (1)K
l=1. Since both q and ρ are assumed to be positive, this scaling implies that 0 ≤ qd ≤ 1
and 0 ≤ qa + qd ≤ 1, as it should, see sec. D. We also detail in this same section the corresponding large K

11

expansion of the free entropy for the teacher-student scenario with Gaussian weights. Only the information-
theoretically reachable generalization error was computed [19], thus we concentrated on the analysis of
performance of AMP by tracking the state evolution equations. In doing so, we unveil a large computational
gap.

(cid:101)

(cid:101)

αG

αG

αG

spec and then jumps discontinuously down to reach a decay aymptotically as 1.25/

In the right panel of Fig. 3 we show the fixed point values of the two overlaps q00 = qd + qa/K and
q01 = qa/K and the resulting generalization error, plotted in the left panel. As discussed in [19] it can be
written in a closed form as (cid:15)g = arccos [2 (qa + arcsin qd) /π] /π, represented in the left panel of Fig. 3. The
α ≡ α/K. The specialization is now a 1st order
specialization transition arises for α = Θ(K) so we define
spinodal (cid:39) 7.17 but the free
phase transition, meaning that the specialization fixed point first appears at
spec (cid:39) 7.65. This has
entropy global extremizer remains the one of the non-specialized fixed point until
interesting implications for the optimal generalization error that gets towards a plateau of value εplateau (cid:39) 0.28
for
α. See left panel
α <
of Fig. 3.
(cid:101)
AMP is conjectured to be optimal among all polynomial algorithms (in the considered limit) and thus
analyzing its state evolution sheds light on possible computational-to-statistical gaps that come hand in hand
with 1st order phase transitions. In the regime of α = Θ(K) for large K the non-specialized fixed point is
always stable implying that AMP will not be able to give a lower generalization error than εplateau. Analyzing
the replica formula for large K in more details, see sec. D, we concluded that AMP will not reach the optimal
generalization for any α < Θ(K2). This implies a rather sizable gap between the performance that can
be reached information-theoretically and the one reachable tractably (see yellow area in Fig. 3). Such large
computational gaps have been previously identified in a range of inference problems —most famously in the
planted clique problem [27]— but the committee machine is the first model of a multi-layer neural network
with realistic non-linearities (the parity machine is another example but use a very peculiar non-linearity) that
presents such large gap.

(cid:101)

(cid:101)

(cid:101)

5 Structure of the proof of Theorem 3.1

All along this section we assume (H1), (H2) and (H3), and all the rigorous statements are implicitely assuming
these hypotheses. We denote K-dimensional column vectors by underlined letters. In particular W ∗
i =
(W ∗
µ be K-dimensional vectors with i.i.d. N (0, 1) components.
Let sn ∈ (0, 1/2] a sequence that goes to 0 as n increases, and let M be the compact subset of matrices in
K with eigenvalues in the interval [1, 2]. For all M ∈ snM, 2snIK×K − M ∈ S +
S++
K.

l=1. For µ = 1, . . . m, let V µ, U ∗

l=1, wi = (wil)K

il)K

5.1 Interpolating estimation problem

Let (cid:15) = ((cid:15)1, (cid:15)2) ∈ (snM)2. Let q : [0, 1] → S +
will later on depend on (cid:15)), and

K(ρ) and r : [0, 1] → S +

K be two “interpolation functions” (that

R1(t) ≡ (cid:15)1 +

r(v)dv ,

R2(t) ≡ (cid:15)2 +

q(v)dv .

t

0
(cid:90)

t

0
(cid:90)

For t ∈ [0, 1], define the K-dimensional vector:

1 − t
n

(cid:114)

n

i=1
(cid:88)

St,µ ≡

XµiW ∗

i +

R2(t) V µ +

tρ − R2(t) + 2snIK×K U ∗
µ

(cid:112)
where matrix square-roots (that we denote equivalently A1/2 or
A) are well defined. We interpolate with
auxiliary problems related to those discussed in sec. 3; the interpolating estimation problem is given by the

(cid:112)

√

(14)

(15)

12

following observation model, with two types of t-dependent observations:

Yt,µ ∼ Pout( · | St,µ),
Y (cid:48)

R1(t) W ∗

t,i =

i + Z(cid:48)

1 ≤ µ ≤ m ,
i, 1 ≤ i ≤ n ,

(cid:40)

(16)

i is (for each i) a K-vector with i.i.d. N (0, 1) components, and Y (cid:48)

where Z(cid:48)
t,i is a K-vector as well. Recall that
in our notation the ∗-variables have to be retrieved, while the other random variables are assumed to be known
(except for the noise variables obviously). Define now st,µ by the expression of St,µ but with wi replacing W ∗
i
and uµ replacing U ∗

µ. We introduce the interpolating posterior:

(cid:112)

Pt,(cid:15)(w, u|Yt, Y (cid:48)

t , X, V ) =

1
Zn,(cid:15)(t)

n

i=1
(cid:89)

P0(wi)e− 1

2 (cid:107)Y (cid:48)

t,i−

R1(t)wi(cid:107)2
2

√

2 (cid:107)uµ(cid:107)2
2

e− 1
(2π)K/2 Pout(Yt,µ|st,µ)

(17)

m

µ=1
(cid:89)

where the normalization factor Zn,(cid:15)(t) equals the numerator integrated over all components of w and u. The
average free entropy at time t is by definition

fn,(cid:15)(t) ≡

E ln Zn,(cid:15)(t) =

E ln

Du

dP0(wi)

Pout(Yt,µ|st,µ)

1
n

1
n

(cid:90)

n

i=1
(cid:89)

m

µ=1
(cid:89)

√

e− 1

2 (cid:107)Y (cid:48)

t,i−

R1(t)wi(cid:107)2

2 ,

(18)

n

i=1
(cid:89)

where Du =

m
µ=1

K

l=1(2π)−1/2e−u2

µl/2.

interpolating model:

(cid:81)

(cid:81)

The presence of the small “pertubation” (cid:15) induces a proportional change in the free entropy of the

Lemma 5.1 (Perturbation of the free entropy). For all (cid:15) ∈ (snM)2 we have for t = 0 that |fn,(cid:15)(0) −
fn,(cid:15)=(0,0)(0)| ≤ C(cid:48)sn for some positive constant C(cid:48). Moreover, |fn − fn,(cid:15)=(0,0)(0)| ≤ Csn for some positive
constant C, so that

|fn − fn,(cid:15)=(0,0)(0)| = On(1) .

∇(cid:15)1fn,(cid:15)(0) = −

[ρ − E(cid:104)Q(cid:105)n,0,(cid:15)] ,

1
2

Proof. Let us compute (or directly obtain by the I-MMSE formula for vector channels [42, 43, 44])

(19)

(20)

where the K × K overlap matrix (Qll(cid:48)) is defined below by (23). Note that the r.h.s. of the above equation is
(up to a factor −1/2) the K × K MMSE matrix. Set uy(x) ≡ ln Pout(y|x). Now we compute (by calculations
very similar to the ones used in the proof of the following Proposition 5.2):

∇(cid:15)2fn,(cid:15)(0) =

1
2n

m

E

µ=1
(cid:88)

∇uYt,µ(St,µ)
(cid:104)

(cid:68)

∇uYt,µ(st,µ)

.

(cid:105)

n,0,(cid:15)

(cid:69)

Note that the r.h.s. of the above equation is symmetric by the Nishimori identity Proposition A.1. By
the mean value theorem we obtain then directly that |fn,(cid:15)(0) − fn,(cid:15)=(0,0)(0)| ≤ (cid:107)∇(cid:15)1fn,(cid:15)(0)(cid:107)F(cid:107)(cid:15)1(cid:107)F +
(cid:107)∇(cid:15)2fn,(cid:15)(0)(cid:107)F(cid:107)(cid:15)2(cid:107)F ≤ C maxi (cid:107)(cid:15)i(cid:107) ≤ C(cid:48)sn.

Using this lemma one verifies, using in particular continuity and boundedness properties of ψP0 and ΨPout

(see Lemma A.6 in sec. A for details; sec. A gathers the detailed proofs of all the propositions below):

fn,(cid:15)(0) = fn − K
2 + On(1) ,
1
0 r(t)dt) + αΨPout(
fn,(cid:15)(1) = ψP0(

(cid:40)

1

0 q(t)dt; ρ) − 1

2

1

0 Tr[ρ r(t)]dt − K

2 + On(1) .

(21)

(cid:82)

(cid:82)

13

(cid:82)

Here On(1) → 0 in the n, m → ∞ limit uniformly in t, q, r, (cid:15).

5.2 Overlap concentration and fundamental sum rule

Notice from (21) that at t = 1 the interpolating estimation problem constructs part of the RS potential (9),
while at t = 0 it is the free entropy (6) of the original model (7) (up to a constant). We thus now want to
compare these boundary values thanks to the identity

fn = fn,(cid:15)(0) +

+ On(1) = fn,(cid:15)(1) −

dt +

+ On(1) .

(22)

K
2

1

dfn,(cid:15)(t)
dt

0
(cid:90)

K
2

The next obvious step is therefore to compute the free entropy variation along the interpolation path, see

sec. A.3 for the proof:

Proposition 5.2 (Free entropy variation). Denote by (cid:104)−(cid:105)n,t,(cid:15) the (Gibbs) expectation w.r.t. the posterior Pt,(cid:15)
given by (17). Set uy(x) ≡ ln Pout(y|x). For all t ∈ [0, 1] we have

dfn,(cid:15)(t)
dt

= −

E

Tr

1
2

m

1
n

(cid:68)

(cid:104)(cid:16)

µ=1
(cid:88)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

Q − q(t)

+

Tr [r(t)(q(t) − ρ)] + On(1) ,

(cid:17)(cid:0)

n,t,(cid:15)

(cid:1)(cid:105)(cid:69)

1
2

where ∇ is the K-dimensional gradient w.r.t. the argument of uYt,µ(·), and On(1) → 0 in the n, m → ∞ limit
uniformly in t, q, r, (cid:15). Here, the K ×K overlap matrix Q is defined as

Qll(cid:48) ≡

W ∗

ilwil(cid:48) .

1
n

n

i=1
(cid:88)

(23)

We will plug this expression in identity (22), but in order to simplify it we need the following crucial
proposition, which says that the overlap concentrates. This property is what is generally refered to as a replica
symmetric behavior in statistical physics.

Proposition 5.3 (Overlap concentration). Assume that for any t ∈ (0, 1) the transformation (cid:15) ∈ (snM)2 (cid:55)→
(R1(t, (cid:15)), R2(t, (cid:15))) is a C1 diffeomorphism with a Jacobian determinant greater or equal to 1. Then one can find a
sequence sn going to 0 slowly enough such that there exists a constant C(ϕout, S, K, α) > 0 depending only on
the activation ϕout, the support S of the prior P0, the number of hidden neurons K and the sampling rate α, and
a constant γ > 0 such that ((cid:107) − (cid:107)F is the Frobenius norm):

1
Vol(snM)2

1

d(cid:15)

dt E

Q − E(cid:104)Q(cid:105)n,t,(cid:15)

(cid:90)(snM)2

0
(cid:90)

(cid:10)(cid:13)
(cid:13)

C(ϕout, S, K, α)
nγ

.

2
F

n,t,(cid:15) ≤
(cid:11)

(cid:13)
(cid:13)

The proof of this concentration result can be directly adapted from [45]. Using the results of [45] is
straightforward, under the assumption that (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism with a Jacobian determinant
greater or equal to 1. This Jacobian determinant can be computed from formula (30). To check that it is greater
than one we use Lemma 5.5 and need Assumption 1 stated in paragraph 5.3 below. With a Jacobian determinant
greater than one, we can “replace” (i.e., lower bound) the integrations over R1(t, (cid:15)), that naturally appear in
the proof of Proposition 5.3, by integrations over the perturbation matrix (cid:15). This is exactly what has been done
in the K = 1 version of the present model in [11] or in [46] i.e., in the scalar overlap case (see also [47] for a
setting with a matrix overlap as in the present case).

From there we can deduce the following fundamental sum rule which is at the core of the proof:

Proposition 5.4 (Fundamental sum rule). Assume that the interpolation functions r and q are such that the map
(cid:15) = ((cid:15)1, (cid:15)2) (cid:55)→ R(t, (cid:15)) = (R1(t, (cid:15)), R2(t, (cid:15))) given by (14) is a C1 diffeomorphism whose Jacobian determinant

14

Jn,(cid:15)(t) is greater or equal to 1. Assume that for all t ∈ [0, 1] and (cid:15) ∈ (snM)2 we have q(t) = q(t, (cid:15)) =
E(cid:104)Q(cid:105)n,t,(cid:15) ∈ S +

K(ρ). Then

fn =

1
Vol(snM)2

1

d(cid:15)

ψP0

r(t)dt

+ αΨPout

q(t, (cid:15))dt; ρ

(cid:90)(snM)2

(cid:110)

0
(cid:16) (cid:90)

(cid:17)

1

0
(cid:16) (cid:90)

−

1
2

0
(cid:90)

1

(cid:17)

Tr[q(t, (cid:15))r(t)]dt

+ On(1) .

(24)

(cid:111)

Proof. Let us denote Vn ≡ Vol(snM)2. The integral over (cid:15) is always over (snM)2. Consider the first term, i.e.
the Gibbs bracket, in the free entropy derivative given by Proposition 5.2. By the Cauchy-Schwarz inequality

E

Tr

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

Q − q(t)

(cid:16)

(cid:68)

d(cid:15)

1

(cid:104)(cid:16)
dt E

0
(cid:90)

≤

1
Vn (cid:90)

m

1
n

µ=1
(cid:88)
1
n

(cid:68)(cid:13)
(cid:13)
(cid:13)

m

µ=1
(cid:88)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:1)(cid:105)(cid:69)
×

n,t,(cid:15)

(cid:17)

1
Vn (cid:90)

1

0
(cid:90)

(cid:17)(cid:0)
(cid:124) − r(t)

2

F

n,t,(cid:15)

(cid:69)

(cid:13)
(cid:13)
(cid:13)

d(cid:15)

dt E

Q − q(t)

2
F

n,t,(cid:15) .

(cid:10)(cid:13)
(cid:13)

(cid:11)

(cid:13)
(cid:13)

The first term of this product is bounded by some constant C(ϕout, α) that only depend on ϕout and α, see
Lemma A.4 in sec. A.4. The second term is bounded by C(ϕout, S, K, α)n−γ by Proposition 5.3, since we
assumed that for all (cid:15) ∈ Bn and all t ∈ [0, 1] we have q(t) = q(t, (cid:15)) = E(cid:104)Q(cid:105)n,t,(cid:15). Therefore from Proposition
5.2 we obtain

1
Vn (cid:90)

d(cid:15)

0
(cid:90)

1

dfn,(cid:15)(t)
dt

dt =

1
2Vn (cid:90)

1

0
(cid:90)

d(cid:15)

Tr

q(t, (cid:15))r(t) − r(t)ρ

dt + On(1) + O(n−γ/2) .

(25)

(cid:2)
Here the small terms are both going to 0 uniformly w.r.t. to the choice of q and r. When replacing (25) in (22)
and combining it with (21) we reach the claimed identity.

(cid:3)

5.3 A technical lemma and an assumption

We give here a technical lemma used in the rest of the proof, and which allows us to detail the unproven
assumption on which we rely to prove Thm 3.1.

Lemma 5.5. The quantity E(cid:104)Q(cid:105)n,t,(cid:15) is a function of (n, t, R(t, (cid:15))). We define F
(2)
(2)
n ) is defined on the set:
n (t, R(t, (cid:15))) ≡ 2α∇ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)). Fn ≡ (F
F

(1)
n , F

(1)
n (t, R(t, (cid:15))) ≡ E(cid:104)Q(cid:105)n,t,(cid:15) and

Dn =

(t, r1, r2) ∈ [0, 1] × S +

K × S +

K

(cid:110)

(ρt − r2 + 2snIK) ∈ S +
K
(cid:12)
(cid:12)
(cid:12)

.

(cid:111)

Fn is a continuous function from Dn to S +
K × S +
and R2 on the interior of Dn. For every (t, R(t, (cid:15))) for which they are defined, they satisfy:

K(ρ). Moreover, Fn admits partial derivatives with respect to R1

(26)

(27)

We can now state the technical assumption on which we rely, and which essentially allows us to derive
that the map (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism with a Jacobian determinant greater or equal to 1 as it will
become clear in the next section:

(1)
∂(F
n )ll(cid:48)
∂(R1)ll(cid:48)

≥ 0.

K

l≤l(cid:48)
(cid:88)

15

Assumption 1. With the notations of Lemma 5.5,

(2)
∂(F
n )ll(cid:48)
∂(R2)ll(cid:48)

≥ 0.

K

l≤l(cid:48)
(cid:88)

Proof of Lemma 5.5. The fact that the image domain of Fn is S +
K(ρ) is known from Lemma A.2. The
continuity and differentiability of Fn follows from standard theorems of continuity and derivation under the
integral sign (recall that we are working at finite n). Indeed, the domination hypotheses are easily satisfied
since we work under (H1) and (H2).

K × S +

i≤j A(ij)(ij). Then one can write Tr[DR1F

Let us now prove (27). We write the formal differential of F

(1)
(1)
n with respect to R1 as DR1F
n , which
(1)
n ] ≥ 0, the trace of a 4-tensor over SK A(ij)(kl) being
is a 4-tensor, and our goal is to prove that Tr[DR1F
(1)
n ] = Tr[∇∇(cid:124)ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) × ∇R1
E(cid:104)Q(cid:105)n,t,(cid:15)]. We
Tr[A] =
know from Lemma A.2 and Lemma A.6 that ∇∇(cid:124)ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) is a positive symmetric matrix (when seen
E(cid:104)Q(cid:105)n,t,(cid:15) is also positive
as a linear operator over SK). Moreover, it is a known result that the derivative ∇R1
symmetric, since R1 is the matrix snr of a linear channel (see [42, 43, 44]). Since the product of two symmetric
positive matrices has always positive trace, this shows that Tr[DR1F

(1)
n ] ≥ 0.

(cid:80)

5.4 Matching bounds

Proposition 5.6 (Lower bound). Under Assumption 1, the free entropy of model (7) verifies

lim inf
n→∞

fn ≥ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

Proof. Choose first r(t) = r ∈ S +
the first order differential equation:

K a fixed matrix. Then R(t) = (R1(t), R2(t)) can be fixed as the solution to

d
dt

d
dt

R1(t) = r ,

R2(t) = E(cid:104)Q(cid:105)n,t,(cid:15) ,

and

R(0) = (cid:15) .

(28)

t
We denote this (unique) solution R(t, (cid:15)) = (rt + (cid:15)1,
0 q(v, (cid:15); r)dv + (cid:15)2). It is possible to check that this ODE
satisfies the hypotheses of the parametric Cauchy-Lipschitz theorem, and that by the Liouville formula the
determinant Jn,(cid:15)(t) of the Jacobian of (cid:15) (cid:55)→ R(t, (cid:15)) satisfies (see Lemma A.3 in sec. A)

(cid:82)

Jn,(cid:15)(t) = exp

(s, R(s, (cid:15))) ds

≥ 1 .

(29)

K

t

∂E(cid:104)Qll(cid:48)(cid:105)n,s,(cid:15)
∂(R2)ll(cid:48)

0
(cid:16) (cid:90)

l≥l(cid:48)
(cid:88)

Indeed, this sum of partial derivatives is always positive by Assumption 1. Moreover from (28), q(t, (cid:15); r) =
E(cid:104)Q(cid:105)n,t,(cid:15), which is in S +
K by Lemma A.2 in sec. A. The fact that the map (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism is
easily verified by its bijectivity (from the positivity of Jn,(cid:15)(t)) combined with the local inversion Theorem. All
the assumptions of Proposition 5.4 are veri.i.d. which then implies, recalling the potential expression (9),

fn =

1
Vol(snM)2

(cid:90)(snM)2

1

0
(cid:16) (cid:90)

d(cid:15) fRS

q(v, (cid:15); r)dv, r

+ On(1) .

This implies the lower bound as this equality is true for any r ∈ S +
K.

(cid:17)

(cid:17)

16

Proposition 5.7 (Upper bound). Under Assumption 1, the free entropy of model (7) verifies

lim sup
n→∞

fn ≤ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

Proof. We now fix R(t) = (R1(t), R2(t)) as the solution R(t, (cid:15)) = (
the following Cauchy problem:

t
0 r(v, (cid:15))dv + (cid:15)1,

t
0 q(v, (cid:15))dv + (cid:15)2) to

(cid:82)

(cid:82)

d
dt

R1(t) = 2α∇ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) ,

R2(t) = E(cid:104)Q(cid:105)n,t,(cid:15) ,

and

R(0) = (cid:15) .

d
dt

We denote this equation as ∂tR(t) = Fn(t, R(t)), R(0) = (cid:15). It is then possible to verify that Fn(R(t), t)
is a bounded C1 function of R(t), and thus a direct application of the Cauchy-Lipschitz theorem implies
that R(t, (cid:15)) is a C1 function of t and (cid:15). The Liouville formula for the Jacobian determinant of the map
(cid:15) ∈ (snM)2 (cid:55)→ R(t, (cid:15)) ∈ R(t, (snM)2) gives this time (see Lemma A.3 in sec. A)

Jn,(cid:15)(t) = exp

(s, R(s, (cid:15))) +

(s, R(s, (cid:15)))

ds

≥ 1 .

(30)

K

t

∂(Fn,1)ll(cid:48)
∂(R1)ll(cid:48)

0
(cid:16) (cid:90)

l≥l(cid:48) (cid:110)
(cid:88)

∂(Fn,2)ll(cid:48)
∂(R2)ll(cid:48)

(cid:111)

(cid:17)

The fact that this determinant is greater or equal to 1 for all t ∈ [0, 1] follows again from the positivity of this
sum of partials, see Lemma 5.5 and Assumption 1. Identity (30) implies the bijectivity of (cid:15) (cid:55)→ R(t, (cid:15)) which,
combined with the local inversion theorem, makes it a diffeomorphism. Since E(cid:104)Q(cid:105)n,t,(cid:15) and ρ − E(cid:104)Q(cid:105)n,t,(cid:15) are
positive matrices (see Lemma A.2 in sec. A) we also have that q(t, (cid:15)) ∈ S +
K(ρ) and since by the differential
equation we have r(t, (cid:15)) = 2α∇ΨPout(q(t, (cid:15))) and as ∇ΨPout(q) ∈ S +
K (see Lemma A.6 in sec. A), then
r(t, (cid:15)) ∈ S +

K too. We have everything needed for applying Proposition 5.4 again which gives in this case

fn =

1
Vol(snM)2

1

d(cid:15)

ψP0
(cid:110)

0
(cid:16) (cid:90)

(cid:90)

r(v, (cid:15))dv

+αΨPout

q(v, (cid:15))dv; ρ

−

Tr

q(v, (cid:15))r(v, (cid:15))dv

+On(1).

(cid:17)

(cid:17)

(cid:111)

1

0
(cid:16) (cid:90)

Then by convexity of ψP0 and ΨPout (see Lemma A.6),

fn ≤

=

1
Vol(snM)2
1
Vol(snM)2

d(cid:15)

d(cid:15)

1

0
(cid:90)

1

0
(cid:90)

(cid:90)

(cid:90)

We now remark that

dv

ψP0(r(v, (cid:15))dv) + αΨPout(q(v, (cid:15)); ρ) −
(cid:110)

dv fRS(q(v, (cid:15)), r(v, (cid:15))) + On(1) .

Tr[q(v, (cid:15))r(v, (cid:15))]

+ On(1)

(cid:111)

fRS(q(v, (cid:15)), r(v, (cid:15))) = inf
q∈S+

K (ρ)

fRS(q, r(v, (cid:15))) .

Indeed, for every r ∈ S +
K(ρ) (cid:55)→ fRS(q, r) ∈ R (recall (9)) is convex (by Lemma A.6),
and its q-derivative is ∇gr(q) = α∇ΨPout(q) − r/2. Since ∇gr(v,(cid:15))(q(v, (cid:15))) = 0 by definition of r(v, (cid:15)), and
S +
K(ρ) is convex, the minimum of gr(v,(cid:15))(q) is necessarily achieved at q = q(v, (cid:15)). Therefore

K, the function gr : q ∈ S +

fn ≤

1
Vol(snM)2

1

d(cid:15)

(cid:90)(snM)2

0
(cid:90)

dv inf
q∈S+

K (ρ)

fRS (q, r(v, (cid:15))) + On(1) ≤ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) + On(1),

which concludes the proof of Proposition 5.7.

Combining these two matching bounds ends the proof of Theorem 3.1.

1

0
(cid:90)

1
2

1
2

17

6 Discussion

One of the contributions of this paper is the design of an AMP-type algorithm that is able to achieve the
Bayes-optimal learning error in the limit of large dimensions for a range of parameters out of the so-called
hard phase. The hard phase is associated with first order phase transitions appearing in the solution of the
model. In the case of the committee machine with a large number of hidden neurons we identify a large hard
phase in which learning is possible information-theoretically but not efficiently. In other problems where
such a hard phase was identified, its study boosted the development of algorithms that are able to match the
predicted threshold. We anticipate this will also be the same for the present model. We should, however, note
that for larger K > 2 the present AMP algorithm includes higher-dimensional integrals that hamper the speed
of the algorithm. Our current strategy to tackle this is to combine the large-K expansion and use it in the
algorithm. Detailed account of the corresponding results are left for future work.

We studied the Bayes-optimal setting where the student-network is the same as the teacher-network, for
which the replica method can be readily applied. The method still applies when the number of hidden units in
the student and teacher are different, while our proof does not generalize easily to this case. It is an interesting
subject for future work to see how the hard phase evolves under over-parametrization and what is the interplay
between the simplicity of the loss-landscape and the achievable generalization error. We conjecture that in
the present model over-parametrization will not improve the generalization error achieved by AMP in the
Bayes-optimal case.

Even though we focused in this paper on a two-layers neural network, the analysis and algorithm can be
readily extended to a multi-layer setting, see [22], as long as the number of layers as well as the number of
hidden neurons in each layer is held constant, and as long as one learns only weights of the first layer, for which
the proof already applies. The numerical evaluation of the phase diagram would be more challenging than
the cases presented in this paper as multiple integrals would appear in the corresponding formulas. In future
works, we also plan to analyze the case where the weights of the second and subsequent layers (including the
biases of the activation functions) are also learned. This could be done for instance with a combination of EM
and AMP along the lines of [48, 49] where this is done for the simpler single layer case.

Concerning extensions of the present work, an important open case is the one where the number of
samples per dimension α = Θ(1) and also the size of the hidden layer per dimension K/n = Θ(1) as n → ∞,
while in this paper we treated the case K = Θ(1) and n → ∞. This other scaling where K/n = Θ(1) is
challenging even for the non-rigorous replica method.

Acknowledgments

This work has been supported by the ERC under the European Union’s FP7 Grant Agreement 307087-SPARCS
and the European Union’s Horizon 2020 Research and Innovation Program 714608-SMiLe, as well as by the
French Agence Nationale de la Recherche under grant ANR-17-CE23-0023-01 PAIL and the Swiss National
Foundation grant no 200021E-175541. Additional funding is acknowledged by A.M., F.K. and J.B. from “Chaire
de recherche sur les modèles et sciences des données”, Fondation CFM pour la Recherche-ENS. We also
acknowledge Léo Miolane for discussions.

References

[1] V. Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.

18

[2] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

[3] S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Physical

[4] T. L. Watkin, A. Rau, and M. Biehl. The statistical mechanics of learning a rule. Reviews of Modern Physics,

Review A, 45(8):6056, 1992.

65(2):499, 1993.

[5] R. Monasson and R. Zecchina. Learning and generalization theories of large committee-machines. Modern

Physics Letters B, 9(30):1887–1897, 1995.

[6] R. Monasson and R. Zecchina. Weight space structure and internal representations: a direct approach to
learning and generalization in multilayer neural networks. Physical review letters, 75(12):2432, 1995.

[7] A. Engel and C. P. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press, 2001.

[8] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking

generalization. arXiv preprint arXiv:1611.03530, 2016. in ICLR 2017.

[9] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. Chayes, L. Sagun, and
R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838,
2016. in ICLR 2017.

[10] C. H. Martin and M. W. Mahoney. Rethinking generalization requires revisiting old ideas: statistical

mechanics approaches and complex learning behavior. arXiv preprint arXiv:1710.09553, 2017.

[11] J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborová. Optimal errors and phase transitions in
high-dimensional generalized linear models. Proceedings of the National Academy of Sciences, 116(12):5451–
5460, 2019.

[12] M. Baity-Jesi, L. Sagun, M. Geiger, S. Spigler, G. Ben-Arous, C. Cammarota, Y. LeCun, M. Wyart, and G. Bir-
oli. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint arXiv:1803.06969,
2018.

[13] M. Mézard, G. Parisi, and M. Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method

and Its Applications, volume 9. World Scientific Publishing Company, 1987.

[14] M. Mézard and A. Montanari. Information, physics, and computation. Oxford University Press, 2009.

[15] D. L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing.

Proceedings of the National Academy of Sciences, 106(45):18914–18919, 2009.

[16] S. Rangan. Generalized approximate message passing for estimation with random linear mixing. In
Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pages 2168–2172. IEEE,
2011.

[17] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to

compressed sensing. IEEE Transactions on Information Theory, 57(2):764–785, 2011.

[18] A. Javanmard and A. Montanari. State evolution for general approximate message passing algorithms,
with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144, 2013.

19

[19] H. Schwarze. Learning a rule in a multilayer neural network. Journal of Physics A: Mathematical and

General, 26(21):5781, 1993.

20(4):375, 1992.

Letters), 21(7):785, 1993.

[20] H. Schwarze and J. Hertz. Generalization in a large committee machine. EPL (Europhysics Letters),

[21] H. Schwarze and J. Hertz. Generalization in fully connected committee machines. EPL (Europhysics

[22] G. Mato and N. Parga. Generalization properties of multilayered neural networks. Journal of Physics A:

Mathematical and General, 25(19):5047, 1992.

[23] D. Saad and S. A. Solla. On-line learning in soft committee machines. Physical Review E, 52(4):4225, 1995.

[24] J. Barbier and N. Macris. The adaptive interpolation method: a simple scheme to prove replica formulas

in bayesian inference. Probability Theory and Related Fields, pages 1–53, 2018.

[25] D. L. Donoho, I. Johnstone, and A. Montanari. Accurate prediction of phase transitions in compressed
sensing via a connection to minimax denoising. IEEE transactions on information theory, 59(6):3396–3433,
2013.

[26] L. Zdeborová and F. Krzakala. Statistical physics of inference: thresholds and algorithms. Advances in

Physics, 65(5):453–552, 2016.

[27] Y. Deshpande and A. Montanari. Finding hidden cliques of size \sqrt {N/e} n/e in nearly linear time.

Foundations of Computational Mathematics, 15(4):1069–1128, 2015.

[28] A. S. Bandeira, A. Perry, and A. S. Wein. Notes on computational-to-statistical gaps: predictions using

statistical physics. arXiv preprint arXiv:1803.11132, 2018.

[29] I. Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv

preprint arXiv:1712.08968, 2017.

[30] A. E. Alaoui, A. Ramdas, F. Krzakala, L. Zdeborová, and M. I. Jordan. Decoding from pooled data: Sharp

information-theoretic bounds. arXiv preprint arXiv:1611.09981, 2016.

[31] A. El Alaoui, A. Ramdas, F. Krzakala, L. Zdeborová, and M. I. Jordan. Decoding from pooled data: Phase
transitions of message passing. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages
2780–2784. IEEE, 2017.

[32] J. Zhu, D. Baron, and F. Krzakala. Performance limits for noisy multimeasurement vector problems. IEEE

Transactions on Signal Processing, 65(9):2444–2454, 2017.

[33] F. Guerra. Broken replica symmetry bounds in the mean field spin glass model. Communications in

mathematical physics, 233(1):1–12, 2003.

[34] M. Talagrand. Spin glasses: a challenge for mathematicians: cavity and mean field models, volume 46.

Springer Science & Business Media, 2003.

[35] D. J. Thouless, P. W. Anderson, and R. G. Palmer. Solution of’solvable model of a spin glass’. Philosophical

Magazine, 35(3):593–601, 1977.

[36] M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method.

Journal of Physics A: Mathematical and General, 22(12):2181–2190, 1989.

20

[37] M. Opper and O. Winther. Mean field approach to bayes learning in feed-forward neural networks.

Physical review letters, 76(11):1964, 1996.

[38] Y. Kabashima. Inference from correlated patterns: a unified theory for perceptron learning and linear

vector channels. Journal of Physics: Conference Series, 95(1):012001, 2008.

[39] C. Baldassi, A. Braunstein, N. Brunel, and R. Zecchina. Efficient supervised learning in networks with

binary synapses. Proceedings of the National Academy of Sciences, 104(26):11079–11084, 2007.

[40] B. Aubin, A. Maillard, J. Barbier, F. Krzakala, N. Macris, and L. Zdeborová. AMP implementation of the

committee machine. https://github.com/benjaminaubin/TheCommitteeMachine, 2018.

[41] P. Schniter, S. Rangan, and A. K. Fletcher. Vector approximate message passing for the generalized linear
model. In Signals, Systems and Computers, 2016 50th Asilomar Conference on, pages 1525–1529. IEEE, 2016.

[42] G. Reeves, H. D. Pfister, and A. Dytso. Mutual information as a function of matrix snr for linear gaussian
channels. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 1754–1758. IEEE,
2018.

[43] M. Payaró, M. Gregori, and D. Palomar. Yet another entropy power inequality with an application. In
Wireless Communications and Signal Processing (WCSP), 2011 International Conference on, pages 1–5. IEEE,
2011.

[44] M. Lamarca. Linear precoding for mutual information maximization in mimo systems.

In Wireless

Communication Systems, 2009. ISWCS 2009. 6th International Symposium on, pages 26–30. IEEE, 2009.

[45] J. Barbier. Overlap matrix concentration in optimal bayesian inference. arXiv preprint arXiv:1904.02808,

2019.

[46] J. Barbier and N. Macris. The adaptive interpolation method for proving replica formulas. applications to
the curie-weiss and wigner spike models. Journal of Physics A: Mathematical and Theoretical, 2019.

[47] J. Barbier, C. Luneau, and N. Macris. Mutual information for low-rank even-order symmetric tensor

factorization. arXiv preprint arXiv:1904.04565, 2019.

[48] F. Krzakala, M. Mézard, F. Sausset, Y. Sun, and L. Zdeborová. Probabilistic reconstruction in compressed
sensing: algorithms, phase diagrams, and threshold achieving matrices. Journal of Statistical Mechanics:
Theory and Experiment, 2012(08):P08009, 2012.

[49] U. Kamilov, S. Rangan, M. Unser, and A. K. Fletcher. Approximate message passing with consistent
parameter estimation and applications to sparse learning. In Advances in Neural Information Processing
Systems, pages 2438–2446, 2012.

[50] P. Hartman. Ordinary Differential Equations: Second Edition. Classics in Applied Mathematics. Society for
Industrial and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104), 1982.

[51] E. Gardner and B. Derrida. Optimal storage properties of neural network models. Journal of Physics A:

Mathematical and general, 21(1):271, 1988.

[52] J. Barbier, N. Macris, M. Dia, and F. Krzakala. Mutual information and optimality of approximate

message-passing in random linear estimation. arXiv preprint arXiv:1701.05823, 2017.

[53] M. Opper and W. Kinzel. Statistical mechanics of generalization. In Models of neural networks III, pages

151–209. Springer, 1996.

21

[54] J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse super-

position codes. IEEE Transactions on Information Theory, 63:4894–4927, 2017.

[55] M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

[56] M. Bayati, M. Lelarge, A. Montanari, et al. Universality in polytope phase transitions and message passing

algorithms. The Annals of Applied Probability, 25(2):753–822, 2015.

22

Supplementary material

A Proof details for Theorem 3.1

A.1 The Nishimori property in Bayes-optimal learning

We first state an important property of the Bayesian optimal setting (that is when all hyper-parameters of the
problem are assumed to be known), that is used several times, and is often refered to as the Nishimori identity.

Proposition A.1 (Nishimori identity). Let (X, Y ) ∈ Rn1 × Rn2 be a couple of random variables. Let k ≥ 1 and
let X (1), . . . , X (k) be k i.i.d. samples (given Y ) from the conditional distribution P (X = · |Y ), independently of
every other random variables. Let us denote (cid:104)−(cid:105) the expectation operator w.r.t. P (X = · |Y ) and E the expectation
w.r.t. (X, Y ). Then, for all continuous bounded function g we have

E(cid:104)g(Y, X (1), . . . , X (k))(cid:105) = E(cid:104)g(Y, X (1), . . . , X (k−1), X)(cid:105) .

(31)

Proof. This is a simple consequence of Bayes formula. It is equivalent to sample the couple (X, Y ) according
to its joint distribution or to sample first Y according to its marginal distribution and then to sample X
conditionally to Y from its conditional distribution P (X = · |Y ). Thus the (k + 1)-tuple (Y, X (1), . . . , X (k))
is equal in law to (Y, X (1), . . . , X (k−1), X). This proves the proposition.

As a first application of Proposition A.1 we prove the following Lemma which is used in the proof of the

upper bound Proposition 5.7.
Lemma A.2 (Positivity of some matrices). The matrices ρ, E(cid:104)Q(cid:105) and ρ − E(cid:104)Q(cid:105) are positive definite, i.e. in S +
K
In the application the Gibbs bracket is (cid:104)−(cid:105)n,t,(cid:15).
Proof. The statement for ρ follows from its definition (in Theorem 3.1). Note for further use that we also
have ρ = 1
ilwil(cid:48) in matrix notation we have Q =
n
n
i=1 W ∗
1
i w
n

i )(cid:124)]. Since by definition Qll(cid:48) ≡ 1
n
E[W ∗
i=1 W ∗
(cid:124)
i . An application of the Nishimori identity shows that

i (W ∗

n

.

(cid:80)

which is obviously in S +

E(cid:104)Q(cid:105) =

n

1
n

i=1
(cid:88)
K. Finally we note that

n

1
n

i=1 (cid:16)
(cid:88)

(cid:80)

1
n

n

i=1
(cid:88)

=

1
n

(cid:124)
i (cid:105)]
(cid:17)

n

i=1
(cid:88)

E[ρ − (cid:104)Q(cid:105)] =

E[W ∗

i (W ∗
i )

] − E[(cid:104)wi(cid:105)(cid:104)w

(cid:124)

E[(W ∗

i − (cid:104)wi(cid:105))((W ∗
i )

(cid:124) − (cid:104)w

(cid:124)
i (cid:105))]

E(cid:104)W ∗

i w

(cid:124)
i (cid:105) =

E[(cid:104)wi(cid:105)(cid:104)w

(cid:124)
i (cid:105)]

(32)

where the last equality is proved by an application of the Nishimori identity again. This last expression is
obviously in S +

K, i.e. E(cid:104)Q(cid:105) ∈ S +

K(ρ).

A.2 Setting in the Hamiltonian language

We set up some notations which will shortly be useful. Let uy(x) ≡ ln Pout(y|x). Here x ∈ RK and y ∈ R.
We will denote by ∇uy(x) the K-dimensional gradient w.r.t. x, and ∇∇(cid:124)uy(x) the K × K matrix of second
derivatives (the Hessian) w.r.t. x. Moreover ∇Pout(y|x) and ∇∇(cid:124)Pout(y|x) also denote the K-dimensional
gradient and Hessian w.r.t. x. We will also use the matrix identity

∇∇(cid:124)

uYµ(x) + ∇uYµ(x)∇(cid:124)

uYµ(x) =

∇∇(cid:124)Pout(Yµ|x)
Pout(Yµ|x)

.

(33)

23

t ∈ Rn×K, X ∈ Rm×n, V ∈ Rm×K,
Finally we will use the matrices w ∈ Rn×K, u ∈ Rm×K, Yt ∈ Rm, Y (cid:48)
W ∗ ∈ Rn×K and U ∗ ∈ Rm×K. Like in sec. 5 we adopt the convention that all underlined vectors are
K-dimensional, like e.g. uµ, U µ, V µ and Y (cid:48)

t,i.

It is convenient to reformulate the expression of the interpolating free entropy fn,(cid:15)(t) in the Hamiltonian

language. We introduce an interpolating Hamiltonian:

Ht(w, u; Yt, Y (cid:48)

t , X, V ) ≡ −

uYt,µ(st,µ) +

(cid:107)Y (cid:48)

t,i − R1(t)1/2 wi(cid:107)2
2

(34)

m

µ=1
(cid:88)

1
2

n

i=1
(cid:88)

where recall that

n

st,µ ≡

1 − t
n

(cid:114)

Xµiwi +

R2(t) V µ +

tρ − R2(t) + 2snIK×K uµ .

(35)

i=1
(cid:88)
The expression of Ht(W ∗, U ∗; Yt, Y (cid:48)
(35) replaced by St,µ given by (15). The average free entropy (18) at time t then reads

(cid:112)

(cid:112)

t , X, V ) is similar to (34), but with w replaced by W ∗ and st,µ given by

fn,(cid:15)(t) ≡

E ln

1
n

dP0(w)

Rn×K

(cid:90)

(cid:90)
µl/2 and dP0(w) =

Rm×K

Du e−Ht(w,u;Yt,Y (cid:48)

t ,X,V )

(36)

where Du =
in the simplest manner it is fruitful to represent the expectations over W ∗, U, Y, Y (cid:48) explicitly as integrals:

K
l=1 dwil. To develop the calculations

n
i=1 P0(wi)

l=1(2π)−1/2e−u2

m
µ=1

K

(cid:81)

(cid:81)

fn,(cid:15)(t) =

EX,V

dYtdY (cid:48)

t dP0(W ∗)DU ∗e−Ht(W ∗,U ;Yt,Y (cid:48)

t ,X,V ) ln

dP0(w)Du e−Ht(w,u;Yt,Y (cid:48)

t ,X,V ).

(37)

(cid:81)

(cid:81)

1
n

(cid:90)

(cid:90)

A.3 Free entropy variation: Proof of Proposition 5.2

The proof provided here follows very closely the one in [11] for the case K = 1, so we are more brief and
refer to this paper for more details. We first prove that for all t ∈ (0, 1)

dfn,(cid:15)(t)
dt

= −

Tr

E

1
2

(cid:68)

m

1
n

(cid:104)(cid:16)

+

1
2

µ=1
(cid:88)
Tr[r(t)(q(t) − ρ)] −

An
2

,

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

W ∗

i w

(cid:124)
i − q(t)

n

1
n

(cid:17)(cid:16)

i=1
(cid:88)

n,t,(cid:15)

(cid:17)(cid:69)

(38)

where

Tr

An = E
(cid:104)

(cid:104)

1
√
n

m

µ=1
(cid:88)

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

1
√
n

n

(cid:16)

i=1
(cid:88)

(W ∗

i (W ∗
i )

(cid:124) − ρ)

ln Zn,(cid:15)(t)

.

(39)

(cid:105)

Once this is done, we show that An goes to 0 as n → ∞ uniformly in t ∈ [0, 1] in order to conclude the proof.

The Hamiltonian (34) t-derivative evaluated at the ground-truth matrices is given by

dHt
dt

(W ∗, U ∗;Yt, Y (cid:48)

t , X, V ) = −

∇(cid:124)

uYt,µ(St,µ)

m

µ=1
(cid:88)
∇(cid:124)

m

= −

Tr

dSt,µ
dt

µ=1
(cid:88)

(cid:104)

uYt,µ(St,µ)
(cid:105)

n

dSt,µ
dt

−

dR1(t)1/2
dt

n

−

Tr

i=1
(cid:88)

(cid:104)(cid:16)

i=1 (cid:16)
(cid:88)
dR1(t)1/2
dt

(cid:124)

(cid:17)

W ∗
i

(Y (cid:48)

t,i − R1(t)1/2W ∗
i )

(Y (cid:48)

t,i − R1(t)1/2W ∗

i )W ∗(cid:124)

i

(40)

(cid:105)

1
n

(cid:17)(cid:105)

(cid:124)

(cid:17)

24

(where we used that R1(t) is symmetric). The t-derivative of fn,(cid:15)(t) thus reads, for 0 < t < 1,

(W ∗, U ∗; Yt, Y (cid:48)

t , X, V ) ln Zn,(cid:15)(t)

−

(w, u; Yt, Y (cid:48)

t , X, V )

(41)

T1

(cid:123)(cid:122)

E

1
n

dHt
dt

(cid:68)

(cid:105)

(cid:125)

(cid:124)

T2

(cid:123)(cid:122)

.

n,t,(cid:15)

(cid:69)

(cid:125)

First, we note that T2 = 0. This is a direct consequence of the Nishimori identity Proposition A.1:

(w, u; Yt, Y (cid:48)

t , X, V )

(W ∗, U ∗; Yt, Y (cid:48)

t , X, V ) = 0 .

(42)

=

1
n

E dHt
dt

n,t,(cid:15)

(cid:69)

We now compute T1. Starting from (40) and considering the first term only (recall also the expression (15)

dfn,(cid:15)(t)
dt

= −

E

1
n

dHt
dt

(cid:104)

(cid:124)

T2 =

E

1
n

dHt
dt

(cid:68)

for St,µ),

E

Tr

(cid:104)

dSt,µ
dt

∇(cid:124)

(cid:104)
+

d
dt

(cid:112)

uYt,µ(St,µ)

R2(t)V µ +

ln Zn,(cid:15)(t)

−

Tr

= E
2
(cid:80)
(cid:104)
tρ − R2(t) + 2snIK×K U ∗
(cid:112)
µ

(cid:104)(cid:110)

(cid:105)

(cid:105)
d
dt

(cid:112)

n
i=1 XµiW ∗
i
n(1 − t)
∇(cid:124)
(cid:111)

uYt,µ(St,µ)
(cid:105)

ln Zn,(cid:15)(t)

.

(43)

(cid:105)

We then compute the first line of the right-hand side of (43). By Gaussian integration by parts w.r.t. Xµi (recall
hypothesis (H3)), and using the identity (33), we find after some algebra

1

−

2

n(1 − t)

(cid:112)

= −

n

i=1
(cid:88)
1
n

(cid:104)

Tr

E

Tr

(cid:104)
E

(cid:104)

Tr

1
2

−

(cid:104)
1
E
2

n

i=1
(cid:88)
1
n

(cid:68)

(cid:104)

i=1
(cid:88)

XµiW ∗

i ∇(cid:124)

n

W ∗

i W

(cid:124)
i

ln Zn,(cid:15)(t)

uYt,µ(St,µ)
(cid:105)
∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

(cid:105)

W ∗

i w

(cid:124)

i ∇uYt,µ(St,µ)∇(cid:124)

uYt,µ(st,µ)

ln Zn,(cid:15)(t)

(cid:105)

(cid:105)

.

n,t,(cid:15)

(cid:105)(cid:69)

(44)

(45)

Similarly for the second line of the right hand side of (43), we use again Gaussian integrations by parts but
this time w.r.t. V µ, U ∗
µ which have i.i.d. N (0, 1) entries. This calculation has to be done carefully with the
help of the matrix identity

d
dt

M (t) =

M (t)

(cid:112)

d

M (t)
dt

d

+

M (t)
dt

(cid:112)

(cid:112)

M (t)

(cid:112)

for any M (t) ∈ S +
t
0 (ρ − q(s))ds, as well as the identity (33), we reach after some algebra

K, and the cyclicity and linearity of the trace. Applying (45) to M (t) equal to

t
0 q(s)ds and

(cid:82)

(cid:82)

E

Tr

(cid:104)(cid:16)
ρ

(cid:104)
= E
Tr
(cid:104)

(cid:104)

d
d
R2(t)V µ +
dt
dt
∇∇(cid:124)Pout(Yt,µ|Sµ,t)
(cid:112)
Pout(Yt,µ|Sµ,t)

(cid:112)

(cid:105)

tρ − R2(t) + 2snIK×K U ∗
µ

∇(cid:124)

ln Zn,(cid:15)(t)

uYµ(Sµ,t)
(cid:105)
uYt,µ(sµ,t)

(cid:105)

(cid:17)

q(t)∇uYt,µ(Sµ,t)∇(cid:124)

ln Zn,(cid:15)(t)

(cid:105)

Tr

+ E
(cid:104)
(cid:68)
R1(t))(cid:124)(Y (cid:48)

.

n,t,(cid:15)

(cid:105)(cid:69)

(46)

As seen from (40), (41) it remains to compute E[Tr[( d
dt
that Y (cid:48)
i = Z(cid:48)
t,i −
one obtains

R1(t)W ∗

(cid:112)

] ln Zn,(cid:15)(t)]. Recall
t,i −
i ∼ N (0, IK×K). Using Gaussian integration by parts as well as the identity (45)

i

(cid:112)

R1(t)W ∗

i )W ∗(cid:124)

(cid:112)
d
dt

(cid:124)

(cid:17)

E

Tr

R1(t)

(Y (cid:48)

t,i −

R1(t)W ∗

i )W ∗(cid:124)

i

ln Zn,(cid:15)(t)

= −Tr

R1(t)

ρ − E(cid:104)W ∗

j wj(cid:105)n,t,(cid:15)

.

(47)

(cid:104)

(cid:104)(cid:16)

(cid:112)

(cid:112)

(cid:105)

(cid:105)

(cid:104)(cid:112)

(cid:0)

(cid:1)(cid:105)

25

Finaly the term T1 is obtained by putting together (43), (44), (46) and (47).

It now remains to check that An → 0 as n → +∞ uniformly in t ∈ [0, 1]. The proof from [11] (Appendix
C.2) can easily be adapted so we give here just a few indications for the ease of the reader. First one notices that

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yµ|St,µ)

E

(cid:104)

(cid:12)
(cid:12)
(cid:12)

W ∗, {St,µ}m

µ=1

=

dYµ∇∇(cid:124)

Pout(Yt,µ|St,µ) = 0 ,

(cid:105)

(cid:90)

so that by the tower property of the conditional expectation one gets

E

Tr

1
√
n

m

µ=1
(cid:88)

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

1
√
n

n

i=1
(cid:88)

(W ∗

i (W ∗
i )

(cid:124) − ρ)

= 0 .

(cid:104)

(cid:104)

(cid:16)
Next, one shows by standard second moment methods that E[(ln Zn,(cid:15)(t)/n − fn,(cid:15)(t))2] → 0 as n → +∞
uniformly in t ∈ [0, 1] (see [11] for the proof at K = 1, that generalizes straightforwardly for any finite K).
Then, using this last fact together with (49), and under hypotheses (H1), (H2), (H3), an easy application of the
(cid:3)
Cauchy-Schwarz inequality implies An → 0 as n → +∞ uniformly in t ∈ [0, 1]. This ends the proof.

(cid:17)(cid:105)(cid:105)

(48)

(49)

A.4 Technical lemmas

Lemma A.3 (Cauchy-Lipschitz Theorem and Liouville Formula). Let

F :

[0, 1] × (0, +∞)d → [0, +∞)d
(cid:55)→ F (t, z)

(t, z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

be a continuous, bounded function. Assume that F admits continuous partial derivatives ∂F
∂zi
domain of definition. Then, for all (cid:15) ∈ (0, +∞)d, the Cauchy problem

(i = 1, . . . , d) on its

y(0) = (cid:15)

and

y(cid:48)(t) = F

t, y(t)

(50)

admits a unique solution t (cid:55)→ y(t, (cid:15)). For all t ∈ [0, 1], the mapping zt : (cid:15) (cid:55)→ y(t, (cid:15)) is a diffeomorphism of class
C1, from (0, +∞)d to zt

. Moreover the determinant J(zt)((cid:15)) of the Jacobian of zt at (cid:15) verifies

(0, +∞)d

(cid:0)

(cid:1)

(cid:0)

(cid:1)

J(zt)((cid:15)) = det

∂yi
∂(cid:15)j

d

t

∂Fi
∂zi

(cid:16)(cid:16)

d
i=1

∂Fi
∂zi

i,j

(cid:17)

0
(cid:16) (cid:90)
(cid:17)
≥ 0 then J(zt)((cid:15)) ≥ 1 for all (cid:15).

i=1
(cid:88)

(cid:0)

(cid:17)

(cid:1)

= exp

s, y(s, (cid:15))

ds

.

(51)

Thus, in particular, if in addition

Proof. The existence and uniqueness of the solution of (50) follows from the classical Cauchy-Lipschitz
Theorem. The solution is indeed defined on all the segment [0, 1] because F is bounded.

(cid:80)

Theorem 3.1 from Chapter 5 in [50] gives that y admits continuous partial derivatives ∂y
∂(cid:15)i

for i = 1, . . . , d,

and Corollary 3.1 from Chapter 5 in the same reference states the Liouville formula (51).

By the Cauchy-Lipschitz Theorem, two solutions of y(cid:48)(t) = F

that are equal at some t ∈ [0, 1]
are equal everywhere. This implies that the mapping zt : (cid:15) (cid:55)→ y(t, (cid:15)) is injective, for all t ∈ [0, 1]. Since y
admits continuous partial derivatives in (cid:15)i, i = 1, . . . , d, we obtain that zt is of class C1 on (0, +∞)d. Now,
the equation (51) gives that J(zt)((cid:15)) > 0 for all (cid:15) ∈ (0, +∞)d. The local inversion Theorem gives then that zt
is a C1 diffeomorphism.

t, y(t)

(cid:0)

(cid:1)

26

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

Lemma A.4 (Boundedness of an overlap fluctuation). Under hypothesis (H2) one can find a constant C(ϕ, K, ∆) <
+∞ (independent of n, t, (cid:15)) such that for any Rn ∈ S +
K

we have

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124) − Rn

≤ 2Tr(R2

n) + α2C(ϕ, K, ∆).

(52)

We note that the constant remains bounded as ∆ → 0 and diverges as K → +∞.

Proof. It is easy to see that for symmetric matrices A, B we have Tr(A − B)2 ≤ 2(TrA2 + TrB2). Therefore

F

n,t,(cid:15)

(cid:69)

(cid:13)
(cid:13)
(cid:13)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124) − Rn

F

n,t,(cid:15)

≤ 2Tr(R2

Tr

n) + 2E
(cid:68)

1
n

(cid:16)

µ=1
(cid:88)

m

(cid:69)

(cid:13)
(cid:13)
(cid:13)
(cid:124)
∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

.

n,t,(cid:15)

(cid:17)

(cid:69)

(53)

In the rest of the argument we bound the second term of the r.h.s. Using the triangle inequality and then
Cauchy-Schwarz we obtain

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

(cid:124)
∇uYt,µ(st,µ)∇uYt,µ(St,µ)

F

n,t,(cid:15)

2

(cid:69)

(cid:13)
(cid:13)
(cid:13)

≤ E

1
n2

(cid:68)

m

µ=1
(cid:16)
(cid:88)
2
(cid:124)(cid:107)2

(cid:107)∇uYt,µ(st,µ)(cid:107)2(cid:107)∇uYt,µ(St,µ)

m

≤ E

1
n2

(cid:68)

(cid:16)

µ=1
(cid:88)

.

n,t,(cid:15)

(cid:17)

(cid:69)

From the random representation of the transition kernel,

(cid:107)∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124)(cid:107)F

n,t,(cid:15)

(cid:17)

(cid:69)

and thus

uYt,µ(s) = ln Pout(Yt,µ|x) = ln

dPA(aµ)

√

1
2π∆

e− 1

2∆ (Yt,µ−ϕ(x,aµ))2

(cid:90)

∇uYt,µ(x) =

dPA(aµ)(Yt,µ − ϕ(x, aµ))∇ϕ(x, aµ)e− 1
dPA(aµ)e− 1
2∆ (Yt,µ−ϕ(x,aµ))2

(cid:82)

2∆ (Yt,µ−ϕ(x,aµ))2

where ∇ϕ is the K-dimensional gradient w.r.t. the first argument x ∈ RK. From the observation model we get
|Yt,µ| ≤ sup |ϕ| +
∆|Zµ|, where the supremum is taken over both arguments of ϕ, and thus we immediately
obtain for all s ∈ RK

√

(cid:82)

(cid:107)∇uYt,µ(x)(cid:107) ≤ (2 sup |ϕ| +

∆|Zµ|) sup (cid:107)∇ϕ(cid:107) .

√

From (57) and (54) we see that it suffices to check that

m2
n2

(cid:2)(cid:0)

E

(2 sup |ϕ| + |Zµ|)2(sup (cid:107)∇ϕ(cid:107))2

≤ C(ϕ, K, ∆)

2

(cid:1)

(cid:3)

where C(ϕ, K, ∆) < +∞ is a finite constant depending only on ϕ, K, and ∆. This is easily seen by expanding
all squares and using that m/n → α. This ends the proof of Lemma A.4.

Lemma A.5 (Properties of ψP0). ψP0

is defined as the free entropy of the first auxiliary channel (3). We have,

(54)

(55)

(56)

(57)

27

for any r ∈ S +
K

:

ψP0(r) ≡ E ln

dwP0(w)eY

(cid:124)
0 r1/2w− 1

2 w(cid:124)rw.

RK

(cid:90)

Then ψP0

is convex and differentiable on S +
K

, with ∇ψP0(r) ∈ S +
K

for any r ∈ S +
K

.

Proof. Note that ψP0 is related to the mutual information I(W0; Y0) via the relation I(W0; Y0) = −ψP0(r) +
K
2 + 1
2 Tr[rρ]. It is then a known result (see [42, 43, 44]) that the derivative ∇rI(W0; Y0) is given by the
matrix-MMSE, i.e. ∇rI(W0; Y0) = 1
]). Using
2
E [(w − (cid:104)w(cid:105))(w − (cid:104)w(cid:105))(cid:124)], which is clearly a
the Nishimori identity Prop.A.1, we can write it as ∇rψP0(r) = 1
2
positive matrix. It is also known (see for instance Lemma 4 of [42]), that I(W0; Y0) is a concave function of r,
which implies that ψP0 is convex, which ends the proof.

]. This implies that ∇rψP0(r) = 1

2 (ρ − E[(cid:104)w(cid:105) (cid:104)w(cid:105)

E [(cid:104)w(cid:105) (cid:104)w(cid:105)

(cid:124)

(cid:124)

Lemma A.6 (Properties of ΨPout). Recall that ΨPout
(4). More precisely, for q ∈ S +

K(ρ), we have:

is defined as the free entropy of the second auxiliary channel

ΨPout(q) ≡ E ln

Y0|q1/2V + (ρ − q)1/2w

.

dw

e− 1
2 (cid:107)w(cid:107)2
(2π)K/2 Pout

RK

(cid:90)

Then ΨPout

is continuous and convex on S +

K(ρ). Also, ∇ΨPout(q) ∈ S +
Proof. The continuity and differentiability of ΨPout is easy, and exactly similar to the first part of the proof of
Proposition 18 of [11]; it just follows from the hypothesis (H2) which allows to use continuity and differentiation
under the expectation, because all the domination hypotheses are easily verified.

K

.

(cid:0)
(cid:101)
K(ρ), and twice differentiable inside S +

(cid:1)

One can compute the gradient and Hessian matrix of ΨPout(q), for q inside S +

K(ρ), using Gaussian
integration by parts and the Nishimori identity. The calculation is tedious and essentially follows the steps of
Y0|x). We define the average (cid:104)−(cid:105)sc (where sc stands for
Proposition 11 of [11]. Recall that u
“scalar channel”) as

(x) ≡ ln Pout(

(cid:101)Y0

(cid:104)g(w)(cid:105)sc ≡

RK DwPout(

(cid:82)

RK DwPout(
(cid:101)

(cid:82)

(cid:101)
Y0|(ρ − q)1/2w + q1/2V )g(w)
Y0|(ρ − q)1/2w + q1/2V )

,

for any continuous bounded function g. One arrives at:
(cid:101)

∇ΨPout(q) =

(ρ − q)1/2W ∗ + q1/2V

(ρ − q)1/2w + q1/2V

E

1
2

∇u

(cid:101)Y0

(cid:68)

(cid:16)

(cid:124)

.

sc

(cid:17)

(cid:69)

Note that this gradient is actually a symmetric matrix of size K × K, as it is a gradient w.r.t. q, which is itself
a matrix of size K. The Hessian ∇∇(cid:124)ΨPout with respect to q is thus a 4-tensor. One can compute in the same
way:

∇∇(cid:124)

ΨPout(q) =

Y0|(ρ − q)1/2w + q1/2V )

Y0|(ρ − q)1/2w + q1/2V )

∇∇(cid:124)Pout(
Pout(

(cid:101)

E

1
2

(cid:104)(cid:16)(cid:68)
−

(cid:68)

(cid:16)

∇u

(cid:101)Y0

(ρ − q)1/2W ∗ + q1/2V
(cid:101)

∇u

(ρ − q)1/2w + q1/2V

(cid:124)

⊗2

.

sc

(cid:17)

(cid:69)

(cid:17)

(cid:105)

In this expression, ⊗2 means the “tensorized square” of a matrix, i.e. for any matrix M of size K × K, M ⊗2 is
a 4-tensor with indices M ⊗2
= Ml0l1Ml2l3. From this expression, it is clear that the Hessian of ΨPout is
always positive, when seen as a matrix with rows and columns in SK, and thus ΨPout is convex, which ends
the proof of Lemma A.6.

l0l1l2l3

∇u

(cid:101)Y0

(cid:17)

(cid:16)

sc

(cid:69)

(cid:101)Y0

(cid:16)

(cid:17)

(58)

(59)

(60)

28

B Replica calculation

Our goal here is to provide an heuristic derivation of the replica formula of Theorem 3.1 using the replica
method, a powerful non-rigorous tool from statistical physics of disordered systems [13, 14]. This computation
is necessary to properly “guess” the formula that we then prove using the adaptive interpolation method. The
reader interested in the replica approach to neural networks and the commitee machine is invited to look as
well to some of the classical papers [51, 36, 20, 21, 19, 5].

The replica trick makes use of the formula, for a random variable x ∈ Rn and a strictly positive function

fn : Rn → R that depends on n:

lim
n→∞

1
n

E ln fn = lim
p→0+

lim
n→∞

1
np

ln Ef p
n.

(61)

Note that the inversion of the two limits here is non-rigorous. Computing the moments Ef p can often
be done for integers p ∈ N, and one can conjecture from it its value for every p > 0, before taking the limit
p → 0+ in (61) by analytical continuation of the value for integer p.

In our calculation, we will use this formula to compute the free entropy of our system, f ≡ limn→∞ fn.

We will thus need the moments of the partition function, for integer p:

EZ p

n = E

dw

P0 ({ wil

K
l=1

= E

dwa

P0 ({ wa
il

K
l=1

n

i=1
(cid:89)

Rn×RK





(cid:90)
p

Rn×RK





a=1 (cid:90)
(cid:89)

n

i=1
(cid:89)

p

m

(cid:9)

µ=1
(cid:89)

(cid:1)

Pout

Yµ



m


Pout

(cid:9)

µ=1
(cid:89)

(cid:1)

K

p

,

(cid:41)

l=1








K

Xµiwa
il

Xµiwil

n

1
√
n

(cid:40)

(cid:12)
(cid:12)
(cid:12)





Yµ

(cid:40)

(cid:12)
(cid:12)
(cid:12)

i=1
(cid:88)

1
√
n

n

i=1
(cid:88)

(cid:41)

l=1

.









The outer expectation is done over Xµi ∼ N (0, 1), w(cid:63) and Y . Writing w(cid:63) as w0 we have:

EZ p

n = EX

dY

dwa

P0

{wa

il}K
l=1

Rm

(cid:90)

Rn×RK

a=0 (cid:34) (cid:90)
(cid:89)

m

n

i=1
(cid:89)

(cid:0)
1
√
n

n

(cid:1)
Xµiwa
il

K



(cid:41)

l=1

.
(cid:35)

i=1
(cid:88)

×

Pout

Yµ



µ=1
(cid:89)

(cid:40)

(cid:12)
(cid:12)
(cid:12)


To perform the average over X, we notice that, since it is an i.i.d. standard Gaussian matrix, then for every
il follows a Gaussian multivariate distribution, with zero mean. This naturally

n
i=1 Xµiwa
a, µ, l, Za
leads to introduce its covariance tensor, which is equal to:

µl ≡ n−1/2



(cid:80)

EZa

µlZb

νl(cid:48) = δµνΣ al
bl(cid:48)

= δµνQal
bl(cid:48),

Qal

bl(cid:48) ≡

wa

ilwb

il(cid:48).

1
n

n

i=1
(cid:88)

(62)

(63)

For every a, b, Qa
functions for fixing Q, we arrive at :

b ∈ RK×K is the overlap matrix, and Σ is of size size (p + 1)K × (p + 1)K. Introducing δ

E [Z p

n] =

dQar
ar

dQar
br(cid:48)

Iprior({Qar

br(cid:48)}) × Ichannel({Qar

br(cid:48)})

,

(64)

R
(cid:89)(a,r) (cid:90)

R
(cid:89){(a,r);(b,r(cid:48))} (cid:90)

(cid:2)

(cid:3)

29

with:

Iprior({Qar

br(cid:48)}) =

dwaP0(wa)

p

a=0 (cid:20)(cid:90)
(cid:89)

Rn×K

p

Ichannel({Qar

br(cid:48)}) =

dY

dZa

Rm×K

n

1
n

δ

Qal

bl(cid:48) −

wa

ilwb
il(cid:48)

,

(cid:21)

(cid:32)


(cid:89){(a,l);(b,l(cid:48))}
p

Pout(Y |Za)e− m

i=1
(cid:88)
2 ln det Σ− mK(p+1)

2

(cid:33)

ln 2π

Rm

(cid:90)

exp

−

1
2





a=0 (cid:90)
(cid:89)
m

a=0
(cid:89)
µl(cid:48)(Σ−1) al

µlZb
Za

µ=1
(cid:88)

a,b
(cid:88)

l,l(cid:48)
(cid:88)

.

bl(cid:48)



By Fourier expanding the delta functions in Iprior, and performing a saddle-point method, one obtains:

in which (recall α ≡ limn→∞ m/n) :

lim
n→∞

1
n

ln E [Z p

n] = extrQ, ˆQ

H(Q, ˆQ)
(cid:105)
(cid:104)

,

H(Q, ˆQ) ≡

p

1
2

a=0
(cid:88)

l,l(cid:48)
(cid:88)

Qal
al

ˆQal

al −

1
2

a(cid:54)=b
(cid:88)

l,l(cid:48)
(cid:88)

Qal

bl(cid:48) ˆQal

bl(cid:48) + ln I + α ln J,

in which we defined:

p

I ≡

RK

a=0 (cid:90)
(cid:89)

p

J ≡

dy

R

(cid:90)

RK

a=0 (cid:90)
(cid:89)

dwaP0(wa) exp

−

ˆQal

al(cid:48)wa

l wa

l(cid:48) +

p

1
2

a=0
(cid:88)

l,l(cid:48)
(cid:88)





dZa
(2π)K(p+1)/2

Pout(y|Za)
det Σ

√

exp

−

1
2

1
2

p

a(cid:54)=b
(cid:88)

l,l(cid:48)
(cid:88)
K

ˆQal

bl(cid:48)wa

l wb
l(cid:48)

,




l(cid:48)(Σ−1) al

l Zb
Za





a,b=0
(cid:88)

l,l(cid:48)=1
(cid:88)

.

bl(cid:48)



Our goal is to express H(Q, ˆQ) as an analytical function of p, in order to perform the replica trick. To do
so, we will assume that the extremum of H is attained at a point in Q, ˆQ space such that a replica symmetry
property is verified. More concretely, we assume:

∃Q0 ∈ RK×K s.t

∀a ∈ [|0, p|] ∀(l, l(cid:48)) ∈ [|1, K|]2 Qal
∀(a < b) ∈ [|0, p|]2 ∀(l, l(cid:48)) ∈ [|1, K|]2 Qal

al(cid:48) = Q0
ll(cid:48),
bl(cid:48) = qll(cid:48),

∃q ∈ RK×K s.t

and samely for ˆQ0 and ˆq. Note that Q0 is by definition a symmetric matrix, while q is also symmetric by our
assumption of replica symmetry. Under this ansatz, we obtain:

H(Q0, ˆQ0, q, ˆq) =

Tr[Q0 ˆQ0] −

Tr[q ˆq] + ln I + α ln J.

(73)

p + 1
2

p(p + 1)
2

Remains now to compute an expression for I and J that is analytical in p, in order to take the limit p → 0+.
This can be done easily, using the identity, for any symmetric positive matrix M ∈ RK×K and any vector
x ∈ RK: exp (x(cid:124)(M/2)x) =
, in which Dξ is the standard Gaussian measure on RK.

ξ(cid:124)M 1/2x

RK Dξ exp

(cid:82)

(cid:0)

(cid:1)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

30

We obtain:

I =

Dξ

dw P0(w) exp

−

(cid:124)

w

( ˆQ0 + ˆq)w + ξ

ˆq1/2w

(cid:124)

p+1

,

1
2

RK

(cid:90)

RK

(cid:20)(cid:90)

(cid:20)
y|(Q0 − q)1/2Z + q1/2ξ

(cid:21)(cid:21)

p+1

.

J =

dy

Dξ

dZPout

R

(cid:90)

RK

(cid:90)

RK

(cid:20)(cid:90)

(cid:110)

(cid:111)(cid:21)

Our assumptions must be consistent in the sense that extrQ, ˆQ
In the p → 0+ limit, one easily gets J = 1 and I =
optimal overlap parameters satisfy ˆQ0 = 0 and Q0
the free entropy:

(cid:82)

limp→0+ H(Q, ˆQ)
2 w(cid:124) ˆQ0w0
(cid:105)
− 1
. This implies that the
ll(cid:48) = EP0 [wlwl(cid:48)]. In the end, we obtain the final formula for

= 0 (because EZ 0

RK dw P0(w) exp

n = 1).

(cid:104)

(cid:104)

(cid:105)

lim
n→∞

fn = extrq,ˆq

−

Tr[q ˆq] + IP + αIC

,

1
2

IP ≡

(cid:26)
Dξ

RK

(cid:90)

RK

(cid:90)

(cid:27)
1
−
2

(cid:20)

dw0P0(w0) exp

(cid:124)
(w0)

ˆqw0 + ξ

(cid:124)

ˆq1/2w0

× ln

dwP0(w) exp

−

RK

(cid:20)(cid:90)
DZ0Pout

(cid:20)
y|(Q0 − q)1/2Z0 + q1/2ξ

(cid:21)
(cid:124)
ˆqw + ξ

(cid:124)

w

1
2

ˆq1/2w

,

(cid:21)(cid:21)

× ln

RK

(cid:20)(cid:90)

(cid:110)
DZPout

(cid:110)

y|(Q0 − q)1/2Z + q1/2ξ

.

(cid:111)

(cid:111)(cid:21)

IC ≡

dy

Dξ

R

(cid:90)

RK

(cid:90)

RK

(cid:90)

A known ambiguity of the replica method is that its result is given as an extremum, here over the set
S +
K(Q0) of positive symmetric matrices, such that (Q0 − q) is also a positive matrix. It is easy to show that
this form gives back the form given in Theorem 3.1, by assuming that this extremum is realized as a supˆq inf q.
Note that in the notations of Theorem 3.1, Q0 is denoted ρ and ˆq is denoted R.

C Generalization error

We detail here two different possible definitions of the generalization error, and how they are related in our
system. Recall that we wish to estimate W ∗ from the observation of ϕout(XW ∗). In the following, we denote
E for the average over the (quenched) W ∗ and the data X, and (cid:104)−(cid:105) for the Gibbs average over the posterior
distribution of W . One can naturally define the Gibbs generalization error as:

(cid:15)Gibbs
g

≡

EW ∗,X

[ϕout (XW ) − ϕout (XW ∗)]2

,

and define the Bayes-optimal generalization error as:

(cid:15)Bayes
g

≡

EW ∗,X

(cid:104)ϕout (XW )(cid:105) − ϕout (XW ∗)

1
2

1
2

(cid:10)

(cid:2)(cid:0)

(cid:11)

2

.

(cid:1)

(cid:3)

(74)

(75)

(76)

(77)

(78)

31

Using the Nishimori identity A.1, one can show that:

(cid:15)Bayes
g

=

EX,W ∗

ϕout (XW ∗)2

+

EX,W ∗

(cid:104)ϕout (XW )(cid:105)2

(cid:105)
− EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) ,

(cid:104)

(cid:105)

=

EX,W ∗

ϕout (XW ∗)2

−

EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) .

1
2

1
2

(cid:104)

(cid:104)

1
2

1
2

(cid:105)

Using again the Nishimori identity one can write:

(cid:15)Gibbs
g

= EX,W ∗

ϕout (XW ∗)2

− EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) ,

g

(cid:105)

= 2(cid:15)Bayes
g

(cid:104)
which shows that (cid:15)Gibbs
. Note finally that since the distribution of X is rotationally invariant, the
quantity EX [ϕout (XW ∗) ϕout (XW )] only depends on the overlap q ≡ W (cid:124)W ∗. As the overlap is shown to
concentrate under the Gibbs measure by Proposition 5.3, and as we expect that the value it concentrates on
is the optimum q∗ of the replica formula (such fact is proven, e.g., for random linear estimation problems in
[52]), the generalization error can itself be evaluated as a function of q∗. Examples where it is done include
[53, 3, 19, 11].

C.1 The generalization error at K = 2

In this subsection alone, we go back to the K = 2 case, instead of the K → ∞ limit. From the definition of
the generalization error (see sec. C), one can directly give an explicit expression of this error in the K = 2
case. Recall our committee-symmetric assumption on the overlap matrix, which here reads

q =

qd + qa
2
qa
2

(cid:32)

qa
2
qd + qa

2 (cid:33)

.

For concision, we denote here sign(x) = σ(x). One obtains from (78):

1
2

− 2(cid:15)Bayes,K=2

g

=

Dx σ [σ(x1) + σ(x2)]

× σ

σ

(

+ qd)x1 +

x2 + x3

1 −

qa
2

q2
a
2

− qaqd − q2
d

(cid:35)

(cid:114)

qa
2

(cid:40)

(cid:34)

R4

(cid:90)

qa
2

+σ

x1 + (

+ qd)x2 − x3

qa
2





qa(qd + qa
2 )
2 − qaqd − q2
d

a

1 − q2

+ x4

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:113)

(1 − q2

d)(1 − (qa + qd)2)
2 − qaqd − q2
d

a

1 − q2

.










Note that one could possibly simplify this expression by using an appropriate orthogonal transformation on x.
These integrals were then computed using Monte-Carlo methods to obtain the generalization error in the left
and middle plots of Fig. 2.

(79)

D The large K limit in the committee symmetric setting

We consider the large K limit2 for a sign activation function, and for different priors on the weights. Since the
output is a sign, the channel is simply a delta function. We assume a committee symmetric solution, i.e. the

2A similar limit has been derived in the context of coding with sparse superposition codes [54]. There the large input alphabet
limit of the mutual information is considered after the thermodynamic limit n → ∞ corresponding to the large codeword limit in this
coding context.

32

(cid:124)
matrices q and ˆq (q and R in the notations of Theorem 3.1) are of the type q = qd1K + qa
K, with the unit
K 1K1
vector 1K = (1)K
l=1, and similarly for ˆq. In the large K limit, this scaling of the order parameters is natural.
Indeed, assume that the covariance of the prior is Q0 = 1K (Q0 = ρ in the notations of Theorem 3.1). Since
both q and (Q0 − q) are assumed to be positive matrices, it is easily shown to imply that 0 ≤ qd ≤ 1 and
0 ≤ qa + qd ≤ 1.

D.1 Large K limit for sign activation function

In the following, we consider Q0 = σ21K. We are interested here in computing the leading order term in IC of
(76). Note that replacing σ2 by 1 in this equation only amounts to replacing q by q/σ2, so we can assume σ2 = 1
RK Dξ IC(y, ξ) log IC(y, ξ), with
without loss of generality. We (abusively) write IC in (76) as IC =
the definition

y=±1

(cid:80)

(cid:82)

IC(y, ξ) ≡

DZPout

y|(Q0 − q)1/2Z + q1/2ξ

.

(80)

RK

(cid:90)

(cid:110)

(cid:111)

the remarks above). Note that this implies that q1/2 =
√
√

Here, we assumed a sign activation function and no noise, as well as a particular form for Q0 and q (see
√
(cid:124)
qa+qd−
K and that (Q0 − q)1/2 =
K
(cid:124)
K. All together, this gives the following explicit expression for IC(y, ξ) :
1K1

1 − qd1K +

qd1K +

1−qa−qd−

1K1

1−qd

qd

√

√

√

K

IC(y, ξ) ≡

DZ

RK

(cid:90)

× δ

y − sign

(cid:40)

K

1
√
K

(cid:34)

sign

1 − qdZl +

1 − qa − qd −

1 − qd

l=1
(cid:88)
(cid:124)
K Z
Introducing a new variable w ≡ 1
√
K
another variable u being the argument of the outer sign function in the previous equations, one obtains:

and a Fourier-transform of the then-introduced delta function, as well as

(cid:16)(cid:112)

(cid:112)

(cid:112)

(cid:17)

(cid:20)

(cid:124)
1
KZ
K

+ (q1/2ξ)l

.

(cid:21)(cid:35)(cid:41)

IC(y, ξ) =

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

eiw ˆw+iuˆuδy,sign(u)

Dze

−i ˆw z√

K e

(cid:20)

(cid:20)(cid:114)

− iˆu√
K

sign

z+

(cid:21)

1−qa−qd
1−qd

−1

w√
K

+ 1√

1−qd

(q1/2ξ)l

(cid:21)

.

K

×

R

l=1 (cid:90)
(cid:89)

λl(w, ξ) ≡

1 − qa − qd
1 − qd

− 1
(cid:21)

(cid:20)(cid:114)

w
√
K

+

√

1
1 − qd

(q1/2ξ)l,

IC(y, ξ) =

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

eiw ˆw+iuˆuδy,sign(u)

Dze

−i ˆw z√

− iˆu√
K

K e

sign[z+λl(w,ξ)]

.

K

R

l=1 (cid:90)
(cid:89)

Denote

such that

33

(81)

(82)

(83)

(84)

(85)

For 1 ≤ l ≤ K, one can rewrite the factorized integral in the last expression of IC(y, ξ) as:

IC(y, ξ) =

eiw ˆw+iuˆuδy,sign(u)

J (λl(w, ξ), ˆw, ˆu) ,

J (λl(w, ξ), ˆw, ˆu) ≡ e

Dze

z(λl−i ˆw√
K

)

− iˆu√
e
K

sign[z]

.

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

λ2
l
2 +iλl

−

ˆw√
K

R

(cid:90)

K

l=1
(cid:89)

We abusively dropped the dependency of λl on (w, ξ). Note the following identity:

F (α, iβ) ≡

Dzeαz+iβ sign(z) = eα2/2

cos β + i sin β ˆH(α)

,

(cid:104)

(cid:105)

R

(cid:90)

√

with ˆH(x) = erf(x/

2). Using it in our previous expressions, we obtain:

J(λl, ˆw, ˆu) = e− 1

2K ˆw2

cos

− i sin

ˆH

λl − i

ˆu
√
K (cid:19)

ˆu
√
K (cid:19)

(cid:20)
Note that by our committee-symmetry assumption, we have λl(w, ξ) = λl,0(ξ) + 1√
K
λ1 typically of order 1 when K → ∞:

(cid:18)

(cid:18)

(cid:18)

ˆw
√
K (cid:19)(cid:21)

.

λ1(w, ξ) with λl,0 and

λl,0(ξ) ≡

λ1(w, ξ) ≡

ξl,

qd
1 − qd
1 − qa − qd
1 − qd

(cid:114)

(cid:20)(cid:114)

− 1

w +

(cid:21)

(cid:20)(cid:114)

qa + qd
1 − qd

−

qd
1 − qd (cid:21)

(cid:114)

(cid:124)
1
Kξ
√
K

.

Expanding J(λl, ˆw, ˆu) as K → ∞, we obtain using the known development of the error function:

J(λl, ˆw, ˆu) = e− 1

2K ˆw2

1 −

− i ˆH [λl,0(ξ)]

ˆu2
2K

(cid:34)

ˆu
√
K

− i

ˆu [λ1(w, ξ) − i ˆw]
K

2
π

(cid:114)

λl,0(ξ)2

e−

2 + O(K−3/2)

.

(cid:35)

This yields (putting back the (w, ξ) dependency):

K

l=1
(cid:89)

J [λl(w, ξ), ˆw, ˆu)] = e− 1

2 ˆw2

exp

−

− iˆuS1 − i

ˆu(λ1 − i ˆw)Γ0 +

ˆu2
2

(cid:34)

2
π

(cid:114)

1
2

ˆu2S2 + O(K−1/2)
(cid:35)

,

(86)

in which we defined the following quantities, that only depend on ξ (recall (84))

wξ(ξ) ≡

ξl,

Γ0(ξ) ≡

e− 1

2 λl,0(ξ)2

,

S1(ξ) ≡

ˆH(λl,0(ξ)),

S2(ξ) ≡

ˆH(λl,0(ξ))2.

1
√
K

1
√
K

K

l=1
(cid:88)
K

l=1
(cid:88)

1
K

1
K

K

l=1
(cid:88)
K

l=1
(cid:88)

A detailed calculation actually shows that the previous expansion of (86) is valid up to O(K−1), and not only
O(K−1/2). Recall also (81), in which one can now readily perform the integration over all variables w, ˆw, u, ˆu
to obtain (dropping the ξ dependency in wξ, Γ0, S1, S2):

IC(y, ξ) = H

−y

+ O(K−1),

(87)

S1 +

(cid:113)





√

qd

√

2
π wξΓ0
(cid:113)
1 − S2 − 2

√

qd+qa−
1−qd
qa
1−qd

π Γ2
0





34

∞

x Dz = 1

in which H(x) ≡
. Note that all quantities wξ, Γ0, S1, S2 only depend on ξ via
its empirical measure, which implies that the integration over ξ ∈ RK will be tractable. We compute it in the
following, using theoretical physics methods. We denote the quantity that appears in (87) as a function of
wξ, Γ0, S1, S2:

1 − erf(x/

2)

(cid:82)

(cid:3)

(cid:2)

2

√

G(y, wξ, Γ0, S1, S2) ≡ H

−y

S1 +

(cid:113)





√

qd

√

2
π wξΓ0
(cid:113)
1 − S2 − 2

√

qd+qa−
1−qd
qa
1−qd

π Γ2
0

.





Introducing once again delta functions and their Fourier transforms for wξ, Γ0, S1, S2, we write, starting from
(87):

IC =

DξIC(y, ξ) log IC(y, ξ)

RK

y=±1 (cid:90)
(cid:88)

=

y=±1 (cid:90)
(cid:88)

dwξd ˆwξ
2π

dΓ0dˆΓ0
2π

dS1d ˆS1
2π

dS2d ˆS2
2π

eiw ˆw+iΓ0 ˆΓ0+iS1 ˆS1+iS2 ˆS2 G(y, wξ, Γ0, S1, S2)

× log G(y, wξ, Γ0, S1, S2)

Dξe−i ˆwwξ(ξ)−iˆΓ0Γ0(ξ)−i ˆS1S1(ξ)−i ˆS2S2(ξ)

+ O(K−1).

(88)

RK

(cid:20)(cid:90)

(cid:21)

The integral over ξ in (88) can be computed in the limit K → ∞:

Λ ≡

Dξe−i ˆwwξ(ξ)−iˆΓ0Γ0(ξ)−i ˆS1S1(ξ)−i ˆS2S2(ξ)

RK

(cid:90)

= 
(cid:90)

R

Dξ exp 

−i



The large K expansion yields




ˆwξ
√
K

− i

−
ˆΓ0e

qd
2(1−qd) ξ2
K

− i

ˆS1 ˆH

qd
1−qd

ξ

ˆS2 ˆH

qd
1−qd

ξ

√
(cid:104)(cid:113)
K

− i

(cid:105)

(cid:104)(cid:113)
K

2

K

(cid:105)











Λ = exp

−

ˆw2 − iˆΓ

1 − qd − ˆS1 ˆwE

1
2

(cid:40)

ξ ˆH
(cid:20)

(cid:18)(cid:114)

qd
1 − qd

ξ

(cid:19)(cid:21)

2

qd
1 − qd

ξ

1 + i ˆS2
ˆS2

E

ˆH

+ O(K−1) .

(cid:34)

(cid:19)

(cid:18)(cid:114)

(cid:35) (cid:41)

(cid:19)

(cid:112)

−

1
2

(cid:18)

The expectations are taken with respect to a real variable ξ ∼ N (0, 1). These expectations are known by
properties of the error function:

One can now compute the integrals over the “hat” variables in (88). Denote Γf

0 ≡

2(1−qd)
π

, and Sf

2 ≡

(cid:113)

E

ˆH
(cid:34)

E

ξ ˆH

qd
1 − qd

ξ

2

(cid:35)

(cid:19)

(cid:18)(cid:114)

2
π

=

arcsin qd ,

qd
1 − qd

ξ

2qd
π

.

=

(cid:19)(cid:21)

(cid:114)

(cid:20)

(cid:18)(cid:114)

35

2
π arcsin qd. This yields:

IC =

DwDS1 G

R2

(cid:90)

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

2qd
π

, Sf
2

(cid:114)

log G

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

(cid:33)

2qd
π

(cid:114)

, Sf
2

.

(cid:33)

(89)

.





(90)

Note that

G

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

2qd
π

, Sf
2

(cid:33)

(cid:114)

= H

−y



2
π

(cid:114)

√

arcsin qd − qdS1 + w

qd + qa

√

1 − 2

π (qa + arcsin qd)

Making the change of variable Snew
reaches:

1 = S1 + w

√

√

qd+qa
arcsin qd−qd


in (89), and defining γ ≡ 2

(cid:113)

π (qa + arcsin qd), one

IC =

DxH

yx

log H

yx

γ
1 − γ

γ
1 − γ

+ O(K−1).

(cid:20)

(cid:114)

(cid:21)

(cid:20)

(cid:114)

(cid:21)

R

y=±1 (cid:90)
(cid:88)

The two values of y contribute in the same way, which finally yields:

IC = 2

DxH

x

log H

x

R

(cid:90)

(cid:20)

(cid:114)

(cid:21)

(cid:20)

(cid:114)

(cid:21)

γ
1 − γ

γ
1 − γ

+ O(K−1).

Note that the parameter γ is naturally bounded to the interval [0, 1] by the conditions 0 ≤ qd ≤ 1 and
0 ≤ qa + qd ≤ 1.

D.2 The Gaussian prior

The prior part IP of the free entropy of (76) is very easy to evaluate in the Gaussian prior setting. We consider
a prior with covariance matrix Q0 = IK (we can simply rescale q by q/σ2 in the final expression for a finite
variance Q0 = σ2IK as we already described). Performing the Gaussian integration in IP in (76) yields:

IP =

ˆqd +

ˆqa −

log(1 + ˆqd) −

log (1 + ˆqd + ˆqa) .

(91)

K
2

1
2

K − 1
2

1
2

D.3 The fixed point equations

From the definition of the free entropy (76) and the expansions for IP and IC obtained in (90) and (91), one
obtains the fixed point equations after having extremized over ˆqd and ˆqa (recall that α ≡ lim m

n ):

∂qa [IG(qd, qa) + αIC(qd, qa)] = 0,
∂qd [IG(qd, qa) + αIC(qd, qa)] = 0,

(92)

(93)

with IG(qd, qa) defined as:

IG(qd, qa) ≡

[qa + Kqd] −

1
2

IC(qd, qa) = 2

DxH

x

R

(cid:90)

(cid:20)

(cid:114)

K − 1
2
γ
1 − γ

(cid:21)

log

(cid:20)
log H

1
1 − qd (cid:21)
x

γ
1 − γ

,

(cid:21)

(cid:20)

(cid:114)

−

log

1
2

1
1 − qa − qd (cid:21)

,

(cid:20)

36

and recall that γ ≡ 2

π (qa + arcsin qd).

The fixed point equations (92), (93) have different behaviors depending on the scaling of α with the hidden

layer size K. We detail these different behaviors in the following paragraphs.

D.3.1 Regime α = oK→∞(K)

In this regime (which in particular contains the case in which α stays of order 1 when K → ∞), the fixed
point equations (92), (93) can be simplified as:

qd = 0,
qa = 2α(1 − qa) ∂IC
∂qa

.

(cid:40)

D.3.2 Regime α = ΘK→∞(K)

In this regime, we naturally define
solutions of the fixed point equations (92), (93) must satisfy the following scaling : qa + qd = 1 − χ
χ ≥ 0 a reaching a finite value when K → ∞. The fixed point equations in terms of χ and qd read:

α will remain of order 1. One can show that the
K , with

αK ≡ α/K, such that

(cid:101)

(cid:101)

qd = 2(1 − qd)

χ−1 = 2

α ∂IC
∂qa

.

1√

1−q2
d

− 1

˜α ∂IC
∂qa

,

(cid:19)

(cid:18)






Note that the State Evolution (SE) computation of Figure 2 was performed by solving the fixed point

(cid:101)

equations (94) and (95) (depending on the regime of α).

It is easy to show that (95) always admit what we call a non-specialized
The stability of the qd = 0 solution:
solution, i.e. a solution with qd = 0. This solution stops to be optimal in term of the free energy at a finite
αspec (cid:39) 7.65. However, one can show that this solution will remain linearly stable for every
α. Actually, it is
linearly stable in the much broader regime α = o(K2). Going back to the initial formulation of the fixed point
(cid:101)
equations (92),(93), and adding the correct time indices to iterate them, one obtains:

(cid:101)

with F and G defined as:

(cid:0)

(cid:1) (cid:0)

d, qt
F (qt
a)
1 + F (qt
d, qt
a)

,

qt+1
d =

qt+1
a =

1 + F (qt

d, qt
a)

d, qt

a)G(qt

d, qt
a)

d, qt
G(qt
a)
1 + F (qt

,

(cid:1)

F (qd, qa) ≡

[∂qdIC − ∂qaIC] ,

G(qd, qa) ≡

∂qaIC −
(cid:20)

1
K

∂qdIC

.

(cid:21)

2α
K − 1
2αK
K − 1

We focus on the behavior of (96) around qd = 0. Given our previous expansion of IC in the K → ∞
|qd=0 →K→∞ 0, which means the qd = 0 solution

limit, and (98), one easily sees that for α = oK→∞(K2), ∂F
∂qd
always remains linearly stable.

However, assume now that α = Θ(K2). Performing a similar calculation to the one shown in sec. D.1,

(94)

(95)

(96)

(97)

(98)

(99)

37

one can show the following expansion:

IC(qd, qa) = I

(0)
C (qd, qa) +

(1)
C (qd, qa) + O

I

1
K

1
K2

.

(cid:18)

(cid:19)

The term of ∂F
∂qd
seen from (98).

|qd=0 arising from I

(1)
C will thus have a possibly non-zero contribution in the K → ∞ limit, as

To summarize, the non-specialized solution always remains linearly stable in the large K limit at least for
α (cid:28) K2. This implies that in this regime, Approximate Message Passing can not escape the non-specialized
fixed point to find the specialized solution, as seen in Fig. 3. For α of order larger than K2, one would have to
(1)
C in order to check that ∂F
|qd=0 (cid:54)= 0 to show that the non-specialized solution is indeed
explicitly compute I
∂qd
linearly unstable. This tedious calculation is left for future work.

D.4 The generalization error at large K

Recall the definition of the generalization error in (78). From the remarks of section C, one can compute it at
large K by applying the same techniques used to compute the channel integral IC in sec. D.1. One obtains
after a tedious, yet straightforward, calculation:

(cid:15)Bayes
g

=

(cid:15)Gibbs
g

=

arccos

(qa + arcsin qd)

+ O(K−1).

(100)

1
2

1
π

2
π

(cid:20)

(cid:21)

This expression is the one used in the computation of the generalization error in the left panel of Fig. 3.

E Linear networks show no specialization

An easy yet interesting case is a linear network with identical weights in the second layer and a final output
function σ : R → R, i.e a network in which ϕout(h) = σ
. For clarity, in this section, we
decompose the channel as Pout(y|ϕout(Z)) for Z ∈ RK instead of Pout(y|Z). We will compute the channel
integral IC of the replica solution (76). For simplicity, we assume that Q0 = 1K the identity matrix (i.e w has
RK DξIC(y, ξ) log IC(y, ξ). One
identity covariance matrix under P0). Note that (76) gives IC as IC =
can easily derive:

K
l=1 hl

1√
K

R dy

(cid:80)

(cid:1)

(cid:0)

(cid:82)

(cid:82)

IC(y, ξ) = e− 1

2 ξ(cid:124)(1K −q)−1qξ

eiuˆuPout(y|σ(u))

dudˆu
2π

R2

(cid:90)

×

RK

(cid:90)

dZ
(2π)K det(1K − q)

e− 1

2 Z(cid:124)(1K −q)−1Z+Z(cid:124)X(ˆu,xi),

in which we denoted X(ˆu, xi) (cid:44) (1K − q)−1q1/2ξ − iˆu√
K
integration over Z can be done, as well as the integration over ˆu:

(cid:112)

1K, with the unit vector 1K = (1)K

l=1. The inner

IC(y, ξ) =

1
1 − 1

(cid:124)
R
Kq1K (cid:90)
K 1

du
√
2π

Pout(y|σ(u)) exp 

−

(cid:113)

(cid:124)
u − 1√
Rq1/2ξ
1
K
(cid:124)
1 − 1
Kq1K
K 1

(cid:16)
2

2

.






(cid:17)

(cid:1)




(cid:0)

So we can formally write the total dependency of IC(y, ξ) on ξ and on q as

IC(y, ξ) = IC

y,

1
√
K

(cid:124)
Kq1/2ξ,
1

(cid:124)
Kq1K
1

1
K

.

(cid:19)

(cid:18)

38

Note that we have the following identity, for any fixed vector x ∈ RK and smooth real function F :

DξF (x

ξ) =

√

(cid:124)

duF (u)e− u2

2x(cid:124)x .

1
2πx(cid:124)x

R

(cid:90)

RK

(cid:90)

In the end, if we denote Γ(q) (cid:44) 1

(cid:124)
Kq1K, we have:

K 1

IC =

dy

dve

2Γ(q) IC(v, y) log IC(v, y),

1

− v2

IC(v, y) ≡

du Pout(y|σ(u)) exp

−

1
2 (1 − Γ(q))

(u − v)2

.

(cid:21)

(cid:20)

R

(cid:90)

2πΓ(q)
1
(cid:112)
2π(1 − Γ(q))

R

(cid:90)

R

(cid:90)

(cid:112)

Note that by hypothesis, both q and 1K − q are positive matrices, so 0 ≤ Γ(q) ≤ 1. As these equations show,
IC only depends on Γ(q) = K−1
l,l(cid:48) qll(cid:48). From this one easily sees that extremizing over q implies that the
optimal ˆq satisfies ˆqll(cid:48) = ˆq/K for some real ˆq. Subsequently, all qll(cid:48) are also equal to a single value, that we
can denote q

K . This shows that this network never exhibits a specialized solution.

(cid:80)

(101)

(102)

(103)

F Update functions and AMP derivation

AMP can be seen as Taylor expansion of the loopy belief-propagation (BP) approach [13, 14, 55], similar to
the so-called Thouless-Anderson-Palmer equation in spin glass theory [35]. While the behaviour of AMP can
be rigorously studied [17, 18, 56], it is useful and instructive to see how the derivation can be performed in
the framework of belief-propagation and the cavity method, as was pioneered in [36, 38] for the single layer
problem. The derivation uses the Generalized AMP notations of [16] and follows closely the one of [26].

F.1 Definition of the update functions

Let’s consider the distributions probabilities Qout and Q0, closely related to the inference problems eq. (3) and
eq. (4):

Qout(z; ω, y, V ) ≡

e− 1

2 (z−ω)(cid:124)V −1(z−ω)Pout(y|z); Q0(W ; Σ, T ) ≡

P0(W )e− 1

2 W (cid:124)Σ−1W +T (cid:124)Σ−1W .

1
ZPout

1
ZP0

We define the update functions gout, ∂ωgout, fw and fc, which will be useful later in the algorithm:

gout(ω, y, V ) ≡ ∂ω log(ZPout) = V −1EQout [z − ω] ,

(cid:124)
∂ωgout(ω, y, V ) = V −1EQout [(z − ω)(z − ω)
fw(Σ, T ) ≡ ∂Σ−1T log ZP0 = EQ0[W ] ,
fc(Σ, T ) ≡ ∂Σ−1T fw = EQ0[W W

(cid:124)

] − fwf

(cid:124)
w .

] − V −1 − goutg

(cid:124)
out ,

Note that gout is the mean of V −1(z − ω) with respect tor Qout and fw the mean of Q0.

F.2 Derivation of the Approximate Message Passing algorithm

F.2.1 Relaxed BP equations

Lets consider a set of messages {mi→µ, ˜mµ→i}i=1..n,µ=1..m on the bipartite factor graph corresponding to
our problem Fig. 4. These messages correspond to the marginal probabilities of Wi if we remove the edges
i → µ or µ → i. The belief propagation (BP) equations (or sum-product equations) can be formulated as the
following [14, 55], where Wi = (wil)l=1..K ∈ RK:

39

Pout (Yµ|{XµWi}n
µ = 1...m

i=1)

˜mµ→i

Wi ∈ RK
i = 1...n

P0(Wi)
i = 1...n

mi→µ

Figure 4: Factor graph representation of the committee machine (for n = 4 and m = 3). The variable (circle)
Wi ∈ RK needs to satisfy a prior constraint (square) P0 and a constraint accounting for the fully connected layer,
that correlates all the variables together.

i→µ(Wi) =

P0(Wi)

˜mt

ν→i(Wi) ,

1
Zi→µ

m

k(cid:54)=µ
(cid:89)

µ→i(Wi) =

dWjPout

Yµ|

XµjWj

mt

j→µ(Wj) .

1
Zµ→i (cid:90)

n

j(cid:54)=i
(cid:89)

1
√
n

n

j=1
(cid:88)









The term inside Pout can be decouple using its K-dimensional Fourrier transform



mt+1



˜mt

Pout

Yµ|

XµjWj

=

1
√
n

n

j=1
(cid:88)





1
(2π)K/2

RK

(cid:90)

dξ exp

(cid:124)

iξ









1
√
n

n

j=1
(cid:88)





XµjWj

ˆPout(Yµ, ξ)

.









Injecting this representation in the BP equations, (104) becomes

˜mt

µ→i(Wi) =

dξ ˆPout(Yµ, ξ) exp

XµiWi

1
(2π)K/2Zµ→i (cid:90)
×

n

RK

RK

j(cid:54)=i (cid:90)
(cid:89)

(cid:124)

dWjmt

j→µ(Wj) exp

iξ

XµjWj)

,

iξ

(cid:124) 1
√
n

(cid:124) 1
√
n

(cid:18)

(cid:18)

≡Ij

(cid:123)(cid:122)

(cid:19)

(cid:19)

(cid:125)

and we define the mean and variance of the messages

ˆW t

j→µ ≡

dWjmt

j→µ(Wj)Wj ,

ˆCt

j→µ ≡

dWjmt

j→µ(Wj)WjW

(cid:124)
j − ˆW t

j→µ( ˆW t

(cid:124)
j→µ)

.

RK

(cid:90)

RK

(cid:90)






In the limit n → ∞ the term Ij can be easily expanded and expressed using ˆW and ˆC

(104)

(105)

Ij =

dWjmt

j→µ(Wj) exp

iξ

Wj)

(cid:39) exp

(cid:124) Xµj√
n

(cid:18)

(cid:19)

RK

(cid:90)

Xµj√
n

i
(cid:32)

(cid:124) ˆW t

ξ

j→µ −

(cid:124) ˆCt

ξ

j→µ , ξ

X 2
µj
n

1
2

,

(cid:33)

40

and finally using the inverse Fourier transform, we obtain

˜mt

µ→i(Wi) (cid:39)

dzPout(Yµ, z)

dξe−iξ(cid:124)zeiXµiξ(cid:124)Wi

1
(2π)KZµ→i (cid:90)
×

RK
n

RK

(cid:90)
(cid:124) ˆW t

ξ

j→µ −

X 2
µj
n

1
2

(cid:124) ˆCt

ξ

j→µξ

(cid:33)

exp

i

Xµj√
n

(cid:32)

j(cid:54)=i
(cid:89)

dzPout(Yµ, z)

dξe−iξ(cid:124)zeiXµiξ(cid:124)Wie

iξ(cid:124)

n
(cid:80)
j(cid:54)=i

Xµj√
n

ˆW t

j→µ

2 ξ(cid:124)

− 1
e

X2
µj
n

n
(cid:80)
j(cid:54)=i

ˆCt

j→µξ

=

=

1
(2π)KZµ→i (cid:90)
1
(2π)KZµ→i (cid:90)

RK

RK

dzPout(Yµ, z)

RK

(cid:90)

(2π)K
det(V t
iµ)

− 1
e
2

(cid:115)

(cid:16)

z−

Xµi√
n

Wi−ωt
iµ

(cid:17)(cid:124)

(V t

iµ)−1(cid:16)

z−

Xµi√
n

Wi−ωt
iµ

(cid:17)

,

(cid:125)

≡Hiµ

(cid:123)(cid:122)

1
n

n

j(cid:54)=i
(cid:88)

ωt
iµ ≡

Xµj ˆW t

j→µ ,

V t
iµ ≡

X 2
µj

ˆCt

j→µ .

(106)

1
√
n

n

j(cid:54)=i
(cid:88)

where we defined the mean and variance, depending on the node i

(cid:124)

Again, in the limit n → ∞, the term Hiµ can be expanded:

Hiµ (cid:39) e− 1

2 (z−ωt

(cid:124)
iµ)

(V t

iµ)−1(z−ωt

iµ)

Xµi√
n

1 +

(cid:32)

W

(cid:124)
i (V t

iµ)−1(z − ωt

iµ) −

W

(cid:124)
i (V t

iµ)−1Wi

X 2
µi
n

1
2

X 2
µi
n

1
2

+

W

(cid:124)
i (V t

iµ)−1(z − ωt

iµ)(z − ωt

(cid:124)
iµ)

(V t

iµ)−1Wi

.

(cid:33)

Gathering all pieces, the message ˜mµ→i can be expressed using definitions of gout and ∂ωgout

˜mt

µ→i(Wi) ∼

W

(cid:124)
i gout(ωt

iµ, Yµ, V t

iµ) +

W

(cid:124)
i goutg

(cid:124)
out(ωt

iµ, Yµ, V t

iµ)Wi+

1 +

Xµi√
n

X 2
µi
n

1
2

(cid:124)
i ∂ωgout(ωt

iµ, Yµ, V t

iµ)Wi

1 + W

(cid:124)
i Bt

µ→i +

W

(cid:124)
i Bt

µ→i(Bt

µ→i)

(cid:124)

(Wi) −

W

(cid:124)
i At

µ→iWi

1
2

(cid:27)

(cid:41)

1
2

µ→i)
(2π)K exp

(cid:115)

−

1
2

(cid:18)

(cid:0)

W

(cid:124)
i − (At

µ→i)−1Bt

µ→i

(cid:124)

At

µ→i

W

(cid:124)
i − (At

µ→i)−1Bt

µ→i

,

(cid:1)

(cid:0)

(cid:19)

(cid:1)

with the following definitions of Aµ→i and Bµ→i:

Bt

µ→i ≡

gout(ωt

iµ, Yµ, V t

iµ), At

µ→i ≡ −

∂ωgout(ωt

iµ, Yµ, V t
iµ)

(107)

Xµi√
n

X 2
µi
n

Using the set of BP equations (104), we can finaly close the set of equations only over {mi→µ}iµ:

mt+1

i→µ(Wi) =

1
Zi→µ

P0(Wi)

m

ν(cid:54)=µ (cid:115)
(cid:89)

det(At
ν→i)
(2π)K e− 1

2 (Wi−(At

ν→i)−1Bt

(cid:124)
ν→i)

At

ν→i(Wi−(At

ν→i)−1Bt

ν→i).

In the end, computing the mean and variance of the product of gaussians, the messages are updated using

1
Zµ→i (cid:40)
X 2
µi
n

W

1
Zµ→i (cid:26)
det(At

1
2

=

=

41

(108)

(109)

(110)

fw and fc:

ˆW t+1

i→µ = fw(Σt

µ→i, T t

µ→i) ,

ˆCt+1

i→µ = fc(Σt

µ→i, T t

µ→i) ,






m

ν(cid:54)=µ
(cid:80)

µ→i

(cid:32)



µ→i ≡

Σt

T t
µ→i ≡ Σt


At

ν→i

(cid:33)

m

(cid:32)

ν(cid:54)=µ
(cid:80)

−1

,

Bt

ν→i

.

(cid:33)

Summary of the Relaxed BP set of equations:

In the end, using eq .(105,106,107, 108), relaxed BP equations can be written as the following set of equations:

Xµj√
n

ˆW t

j→µ

X 2
µj
n

ˆCt

j→µ

=

=

n

j(cid:54)=i
(cid:80)
n

j(cid:54)=i
(cid:80)
Xµi√

Bt

µ→i =

At

µ→i = −

iµ, Yµ, V t
iµ)

n gout(ωt
X 2
n ∂ωgout(ωt
µi

iµ, Yµ, V t
iµ)

ωt
iµ

V t
iµ






Σt

µ→i =

−1

m

(cid:32)

ν(cid:54)=µ
(cid:80)

At

ν→i

(cid:33)

T t
µ→i = Σt

µ→i

ˆW t+1

i→µ = fw(Σt

m

Bt

ν→i

(cid:33)

(cid:32)

ν(cid:54)=µ
(cid:80)
µ→i, T t

µ→i)

ˆCt+1

i→µ = fc(Σt

µ→i, T t

µ→i)






F.2.2 Approximate Message Passing algorithm

The relaxed BP algorithm uses O(n2) messages. However all the messages depend weakly on the target node.
On a tree, the missing message is negligible, that allows us to expand the previous relaxed BP equations (109)
to make appear the Onsager term at a previous time step, and reduce the number of messages to O(n). We
define the following estimates and parameters based on the complete set of messages:

Xµj√
n

ˆW t

j→µ

X 2
µj
n

ˆCt

j→µ

n

j=1
(cid:80)
n

j=1
(cid:80)

ωt

µ ≡



V t
µ ≡


Σt

i ≡

−1

m

At

ν→i

ν=1

(cid:18)
(cid:80)

i ≡ Σt
T t
i


(cid:18)

m

ν=1
(cid:80)

(cid:19)

Bt

ν→i

(cid:19)

Let’s now expand the previous messages eq. (109), making appear these new target-independent messages:

• Σt

µ→i

• T t

µ→i

Σt

µ→i =

At

ν→i

=

m

ν(cid:54)=µ
(cid:88)









=

IK×K −

−1

m

−1

m

At

ν→i − At

µ→i

=

(cid:32)

ν=1
(cid:88)

−1

(cid:33)

m

−1

At

ν→i

(cid:33)

At

µ→i

At

ν→i

(cid:33)

(cid:32)

ν=1
(cid:88)





m

(cid:32)

ν=1
(cid:88)





ν=1
(cid:88)
−1

At

ν→i 

IK×K −


IK×K − Σt

=

(cid:32)

ν=1
(cid:88)

m

−1

−1

At

ν→i

(cid:33)

At

µ→i



iAt

µ→i

−1

Σt

i (cid:39) Σt


i + O


1
n

(cid:18)

(cid:19)

(cid:39)IK×K +Σt
(cid:0)

iAt

µ→i+O(n−1)

(cid:1)

(cid:124)

(cid:123)(cid:122)

(cid:125)

m

Bt

ν→i

=

Σt

i + O

1
n

m

Bt

ν→i − Bt

µ→i

(cid:33)

(cid:18)

(cid:19)(cid:19) (cid:32)

ν=1
(cid:88)

µ→i = Σt
T t

µ→i 

ν(cid:54)=µ
(cid:88)

iBt
i − Σt

= T t


µ→i + O

(cid:18)

1
n

(cid:18)

(cid:19)

42

• ˆW t+1
i→µ

• V t
µ

• ωt
µ

ˆW t+1

i→µ = fw(Σt

µ→i, T t

µ→i) = fw

Σt

i, T t

i − Σt

iBt

µ→i

+ O

1
n

(cid:18)

(cid:19)

(cid:0)

Σt

iBt

µ→i

(cid:1)

(cid:39) fw

Σt

i, T t
i

−

dfw
dT

= fw

(cid:0)

Σt

i, T t
i
= ˆW t+1
(cid:0)
i

(cid:1)

(cid:1)

−

Σt
i

(cid:0)

(cid:1)

(Σt
(cid:12)
(cid:12)
−1
(cid:12)
(cid:12)

i,T t
i )
Σt

fc

Σt
i

i, T t
i
= ˆCt+1
i

(cid:1)

(cid:0)

(cid:39)

Xµi√
n

(cid:123)(cid:122)
(cid:124)
= ˆW t+1
i −

(cid:125)
Xµi√
n

(cid:124)
−1 ˆCt+1

(cid:125)
(cid:123)(cid:122)
µ, Yµ, V t
igout(ωt
i Σt

Σt
i

Bt

µ→i

µ,Yµ,V t
µ)

gout(ωt
(cid:124) (cid:123)(cid:122) (cid:125)
µ) + O

1
n

(cid:18)

(cid:19)

(cid:0)

(cid:1)

• ˆCt+1
i→µ
Let’s denote for convenience, E =

Σt
i

−1 ˆCt+1

i Σt

igout(ωt

µ, Yµ, V t

µ). Then

ˆCt+1
i→µ = EQ0

ˆW t

i→µ( ˆW t

= EQ0

ˆW t

i −

(cid:0)

(cid:1)

(cid:124)
i→µ)
Xµi√
n

E

(cid:105)

− EQ0

ˆW t
(cid:104)
Xµi√
i −
n

ˆW t

i→µ

(cid:105)

E

i→µ

EQ0
(cid:124)

ˆW t
(cid:104)
− EQ0

(cid:124)

(cid:20)(cid:18)
ˆW t

(cid:124)
i ( ˆW t
i )

(cid:19) (cid:18)
− EQ0

= EQ0

ˆW t
i

EQ0

(cid:19)
(cid:21)
ˆW t
i

(cid:124)

+ O

(cid:105)

(cid:104)

(cid:105)

(cid:104)

(cid:105)

(cid:104)

(cid:104)

(cid:105)
ˆW t

i −

(cid:20)
1
√
n

(cid:18)

(cid:19)

Xµi√
n

E

EQ0

ˆW t
(cid:20)
i + O

(cid:21)
= ˆCt+1

1
√
n

(cid:18)

(cid:19)

i −

Xµi√
n

E

(cid:124)

(cid:21)

• gout(ωt

iµ, Yµ, V t
iµ)

gout(ωt

iµ, Yµ, V t

iµ) = gout

ωt
µ −

ˆW t

i→µ, Yµ, V t

µ −

= gout

µ, Yµ, V t
ωt
µ

−

Xµi√
n

∂gout
∂ω

µ, Yµ, V t
ωt
µ

= gout

µ, Yµ, V t
ωt
µ

−

µ, Yµ, V t
ωt
µ

ˆW t

Xµi√
n

∂gout
∂ω

X 2
µi
n

ˆCt

i→l

(cid:33)

+O

1
n

(cid:18)

(cid:19)

(cid:1)

= ˆW t

ˆW t

i→µ
(cid:16) 1√

i +O
(cid:124) (cid:123)(cid:122) (cid:125)
i + O

(cid:17)

n

1
n

(cid:18)

(cid:19)

(cid:1)

(cid:0)

(cid:0)

Xµi√
n

(cid:1)

(cid:1)

(cid:32)

(cid:0)

(cid:0)

V t
µ =

ˆCt

i→l =

ˆCt

i + O

X 2
µi
n

n

i=1
(cid:88)

X 2
µi
n

n

i=1
(cid:88)

1
n3/2

(cid:18)

(cid:19)

ωt
µ =

ˆW t

i→µ =

ˆW t

i − Xµi

Σt−1
i

−1 ˆCt

i Σt−1
i

gout(ωt−1

µ , Yµ, V t−1

µ

) + O

1
n

(cid:18)

(cid:19)(cid:19)

Σt−1
i

−1 ˆCt

(cid:0)
i Σt−1
i

(cid:1)
gout(ωt−1

µ , Yµ, V t−1

µ

) + O

1
n3/2

(cid:18)

(cid:19)

Xµi√
n

Xµi√
n

n

i=1
(cid:88)
n

i=1
(cid:88)

=

ˆW t

i −

n

Xµi√
n

(cid:18)

n

i=1
(cid:88)
X 2
µi
n

i=1
(cid:88)

(cid:0)

(cid:1)

43

−1

•

Σt
i

(cid:0)

(cid:1)
Σt
i

−1

=

(cid:0)

(cid:1)

• T t
i

m

µ=1
(cid:88)

m

µ=1
(cid:88)

At

µ→i = −

X 2

µi∂ωgout(ωt

iµ, Yµ, V t

iµ) = −

X 2

µi∂ωgout(ωt

µ, Yµ, V t

µ) + O

m

µ=1
(cid:88)

1
n3/2

(cid:18)

(cid:19)

Bt

µ→i

= Σt
i

m

Xµi√
n

µ=1
(cid:88)
µ, Yµ, V t
ωt
µ


gout

i = Σt
T t

m

i 

µ=1
(cid:88)
m


µ=1
(cid:88)
m

i 



µ=1
(cid:88)

Xµi√
n

(cid:18)

Xµi√
n

= Σt
i

= Σt

(cid:0)

(cid:0)

gout

µ, Yµ, V t
ωt
µ

−

gout(ωt

iµ, Yµ, V t
iµ)

−

Xµi√
n

∂gout
∂ω

X 2
µi
n

∂gout
∂ω

(cid:1)

(cid:1)

(cid:0)

(cid:0)

µ, Yµ, V t
ωt
µ

ˆW t

i + O

1
n

(cid:18)

(cid:19)(cid:19)

µ, Yµ, V t
ωt
µ

ˆW t

i 

+ O

1
n3/2

(cid:18)

(cid:19)

(cid:1)

(cid:1)



The AMP algorithm follows naturally the rBP updates (109) using the expanded estimates of the mean and

variance ωµ, Vµ, Ti and Σi, and finally reads in pseudo language:

Algorithm 2 Approximate Message Passing for the committee machine

Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: gout,µ = 0, Σi = IK×K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
Initialize: ˆWi ∈ RK and ˆCi, ∂ωgout,µ ∈ S +
repeat

K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 1.

n

ωt

ˆW t

µ =

i −

Xµi√
n

Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
X 2
µi
n

i Σt−1
i

−1 ˆCt

gt−1
out,µ

Σt−1
i

i=1
(cid:80)
out,µ = gout(ωt
gt

Update of gout,µ ∈ RK and ∂ωgout,µ ∈ S +
K:
out,µ = ∂ωgout(ωt

µ, Yµ, V t
µ)
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt

(cid:1)
∂ωgt

Xµi√

i = Σt
T t
i

X 2
n ∂ωgt
µi

out,µ −

ˆW t
i

out,µ

n gt

m

(cid:1)

(cid:0)

(cid:0)

|

|

(cid:16)

µ=1
(cid:80)

Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = fw(Σt

i, T t
i )

|

ˆCt+1

i = fc(Σt

i, T t
i )

(cid:17)

µ, Yµ, V t
µ)

i = −
(cid:16)

m

µ=1
(cid:80)

V t
µ =

X 2
µi
n

ˆCt
i

n

i=1
(cid:80)

X 2
n ∂ωgt
µi

out,µ

−1

(cid:17)

ˆW t+1
t = t + 1

until Convergence on ˆW , ˆC.
Output: ˆW and ˆC.

G State evolution equations from AMP

In this section, W (cid:63) denotes the ground truth weights of the teacher and we define the overlap parameters at
time t, mt, σt, qt, Q and that respectively measure the correlation of the AMP estimator with the ground truth,

44

its variance and the norms of student and teacher weights:



mt ≡ EW (cid:63) lim
n→∞

qt ≡ EW (cid:63) lim
n→∞


1
n

1
n

n

i=1
(cid:88)
n

i=1
(cid:88)

ˆW t

(cid:124)
i (W (cid:63)
i )

,

ˆW t

(cid:124)
i ( ˆW t
i )

,

and



σt ≡ EW (cid:63) lim
n→∞

Q ≡ EW (cid:63) lim
n→∞


n

ˆCt
i .

1
n

1
n

i=1
(cid:88)
n

i=1
(cid:88)

W (cid:63)

i (W (cid:63)
i )

(cid:124)

,

The aim is to derive the asymptotic behaviour of these overlap parameters, called state evolution. The idea is
to compute the overlap distributions starting with the relaxed BP equations eq. (109).

G.1 Messages distribution

In order to get the asymptotic behaviour of the overlap parameters, we need first to compute the distribution of
Σt
µ→i and T t
µ→i. Besides, we recall that in our model, the output has been generated by a teacher according to
n
1√
. We define zµ ≡ 1√
j(cid:54)=i XµjW (cid:63)
n W (cid:63)Xµ, A
Yµ = ϕ0
j .
And it useful to recall EX [Xµi] = 0 and EX [X 2
(cid:17)

n W (cid:63)Xµ = 1√
µi] = 1.

i and zµ→i ≡ 1√
n

n
i=1 XµiW (cid:63)

out

(cid:16)

n

(cid:80)

(cid:80)

µ→i

• ωt
Under belief propagation assumption messages are independent. ωt

µ→i is thus the sum of independent
variables and follows a gaussian distribution. Let’s compute the first two moments, using expansions of the
relaxed BP equations eq. (109):

EX

ωt
µ→i

=

EX [Xµj] ˆW t

j→µ = 0 ,

(cid:2)
µ→i(ωt
ωt

EX

(cid:124)
µ→i)

=

(cid:3)

(cid:3)

(cid:2)

• zµ

EX [XµjXµk] ˆW t

j→µ( ˆW t

(cid:124)
k→µ)

=

EX

X 2
µj

ˆWj→µ( ˆWj→µ)

(cid:124)

=

ˆW t

j→µ( ˆW t

j→µ)

(cid:124)

=

ˆW t

i ( ˆW t
i )

(cid:124)

+ O

1
n

n

i=1
(cid:88)

n

j(cid:54)=i
(cid:88)

(cid:3)

(cid:2)
1/n3/2

(cid:16)

(cid:17)

−→
n→∞

qt .

EX [zµ] =

1
√
n

EX [Xµi] W (cid:63)

i = 0 ,

EX,W (cid:63)

zµz

(cid:124)
µ

= EW (cid:63)

EX [XµjXµk] W (cid:63)

j (W (cid:63)
k )

(cid:124)

= EW (cid:63)

W (cid:63)

i (W (cid:63)
i )

(cid:124) −→
n→∞

Q .

1
√
n

n

j(cid:54)=i
(cid:88)
n

1
n

1
n

j(cid:54)=i,k(cid:54)=i
(cid:88)
n

j(cid:54)=i
(cid:88)

n

i=1
(cid:88)
1
n

n

j=1,k=1
(cid:88)

(cid:2)

(cid:3)

• zµ and ωt

µ→i

EX,W (cid:63)

ωt
µ→iz

(cid:124)
µ

= EW (cid:63)

EX [XµjXµk] ˆW t

j→µ(W (cid:63)
k )

(cid:124)

= EW (cid:63)

ˆW t

(cid:124)
j→µ(W (cid:63)
j )

1
n

1
n

n

j(cid:54)=i,k=1
(cid:88)
n

i=1
(cid:88)

(cid:2)

(cid:3)

= EW (cid:63)

ˆW t

(cid:124)
i (W (cid:63)
i )

+ O

1/n3/2

−→
n→∞

mt .

(cid:16)

(cid:17)

Hence asymptotically (zµ, ωt

µ→i) follow a Gaussian distribution with covariance matrix Qt =

Q mt
qt
mt
(cid:34)

.

(cid:35)

1
n

n

i=1
(cid:88)

1
n

n

j(cid:54)=i
(cid:88)

45

• Vµ→i

concentrates around its mean:

EX,W (cid:63)

V t
µ→i

= EW (cid:63)

EX

X 2
µj

ˆCt

j→µ = EW (cid:63)

ˆCt

j→µ = EW (cid:63)

ˆCt

i + O

1/n3/2

1
n

n

j(cid:54)=i
(cid:88)

1
n

n

i
(cid:88)

−→
n→∞

σt .

(cid:16)

(cid:17)

1
n

n

j(cid:54)=i
(cid:88)

(cid:3)

(cid:2)

(cid:3)
Let’s define other order parameters, that will appear in the following:

(cid:2)

ˆqt ≡ αEω,z,A
ˆmt ≡ αEω,z,A
ˆχt ≡ αEω,z,A

gout(ω, ϕ0
∂zgout(ω, ϕ0
(cid:2)
−∂ωgout(ω, ϕ0
(cid:2)

out(z, A), σt)

,
out(z, A), σt)
(cid:3)

out(z, A), σt)gout(ω, ϕ0

out(z, A), σt)(cid:124)

,

(cid:3)

.

(cid:3)





• T t

µ→i

can be expanded around zµ→i:

(cid:2)

Σt

µ→i

−1

T t
µ→i =

m





ν(cid:54)=µ
(cid:88)

Bt

ν→i

=



1
√
n

m

ν(cid:54)=µ
(cid:88)

Xνigout(ωt

ν→i, ϕ0

1
√
n

out 

XνjW (cid:63)

j + XνiW (cid:63)

i , A

, V t

ν→i)



n

j(cid:54)=i
(cid:88)

Xνigout(ωt

ν→i, ϕ0



out (zν→i, A) , V t

ν→i)

+


νi∂zgout(ωt

X 2


out (zν→i, A) , V t
ν→i)

ν→i, ϕ0

m

1
n









ν(cid:54)=µ
(cid:88)




W (cid:63)
i .





(cid:0)

=

(cid:1)
m





ν(cid:54)=µ
(cid:88)

1
√
n

• Σt

µ→i

At

ν→i = −

X 2

νi∂ωgout(ωt

ν→i, Yν, V t

ν→i)

m

Σt

µ→i

−1

=

(cid:0)

(cid:1)

m

1
n

ν(cid:54)=µ
(cid:88)

m

= −

ν(cid:54)=µ
(cid:88)
νi∂ωgout(ωt

X 2

1
n

ν(cid:54)=µ
(cid:88)

ν→i, ϕ0

out (zν→i, A) , V t

ν→i) + O

1/n3/2

.

Hence taking the average and the large size limit, the first moments of the variables Σt

(cid:16)

(cid:17)
µ→i and T t

µ→i read:

Eω,z,A,X

Eω,z,A,X

Eω,z,A,X

−1

(cid:17)

−1

(cid:17)

−1

Σt

µ→i

Σt

µ→i

Σt

µ→i

(cid:20)(cid:16)

(cid:20)(cid:16)

T t
µ→i

−→
n→∞

(cid:21)

ˆmtW (cid:63)
i ,
(cid:124)

T t
µ→i

T t
µ→i

Σt

µ→i

(cid:17)

(cid:16)

(cid:21)

(cid:17)

(cid:16)
−→
n→∞

ˆχt .

−1

−→
n→∞

ˆqt ,






And finally T t

µ→i ∼ ( ˆχt)−1

ˆmtW (cid:63)

with ξ ∼ N (0, 1) and

Σt

µ→i

−1

∼ ( ˆχt)−1 .

(cid:16)

(cid:17)

(cid:20)(cid:16)

(cid:21)

(cid:17)
i + (ˆqt)1/2ξ

(cid:0)

(cid:1)

G.2 State evolution equations - Non Bayes optimal case

Let’s define the following notations:

T t[W (cid:63), ξ] ≡ ( ˆχt)−1

ˆmtW (cid:63) + (ˆqt)1/2ξ

Σt ≡ ( ˆχt)−1

(cid:16)

(cid:17)

46

Gathering above results, the state evolution equations read:

ˆW t

i (W (cid:63)
i )

(cid:124)

= EW (cid:63),ξ

fw

Σt, T t[W (cid:63), ξ]

(cid:124)
(W (cid:63))

mt+1 = EW (cid:63) lim
n→∞

qt+1 = EW (cid:63) lim
n→∞

1
n

1
n

1
n

n

i=1
(cid:88)
n

i=1
(cid:88)
n

i=1
(cid:88)

ˆW t+1
i

( ˆW t+1
i

)

(cid:124)

= EW (cid:63),ξ

(cid:2)

(cid:0)
fw

(cid:1)

Σt, T t[W (cid:63), ξ]

fw

Σt, T t[W (cid:63), ξ]

(cid:3)

(cid:0)

(cid:124)

(cid:1)

(cid:3)

σt+1 = EW (cid:63) lim
n→∞

ˆCt+1
i = EW (cid:63),ξ

fc

Σt, T t[W (cid:63), ξ]

(cid:2)

(cid:0)

(cid:1)

(cid:2)

(cid:0)

(cid:1)(cid:3)

ˆqt = αEω,z,A

gout(ω, ϕ0

= α

dPA(A)
(cid:2)

out(z, A), σt)gout(ω, ϕ0
z, ω; 0, Qt

dzdωN

out(z, A), σt)(cid:124)

gout(ω, ϕ0

out(z, A), σt)gout(ω, ϕ0

out(z, A), σt)

(cid:3)

(cid:124)

= α

dPA(A)
(cid:2)

dzdωN

∂zgout(ω, ϕ0

out(z, A), σt)

ˆmt = αEω,z,A

∂zgout(ω, ϕ0

(cid:90)

(cid:90)

(cid:90)

(cid:90)

ˆχt = αEω,z,A

−∂ωgout(ω, ϕ0

= −α

dPA(A)

(cid:2)

dzdωN

(cid:90)

(cid:90)

(cid:0)

out(z, A), σt)
z, ω; 0, Qt
(cid:3)

(cid:1)

out(z, A), σt)
(cid:0)
(cid:1)
z, ω; 0, Qt
(cid:3)

(cid:0)

(cid:1)

∂ωgout(ω, ϕ0

out(z, A), σt)

G.3 State evolution equations - Bayes optimal case

and











In the bayes optimal case, the student knows all the parameters of the teacher and then P (cid:63)
mt = qt and ˆqt = ˆmt = ˆχt, σt = Q − qt and then, naturally

0 = P0, ϕ0

out = ϕout,

T t[W (cid:63), ξ] ≡ W (cid:63) + (ˆqt)−1/2ξ ,
Σt ≡ (ˆqt)−1 .

In the Bayes-optimal case, the set of state evolution equations reduces and simplifies to:

qt+1 = EW (cid:63),ξ

fw

Σt, T t[W (cid:63), ξ]

fw

Σt, T t[W (cid:63), ξ]

(cid:124)

,

ˆqt = αEω,z,A

(cid:2)
(cid:0)
gout(ω, ϕout(z, A), σt)gout(ω, ϕout(z, A), σt)(cid:124)

(cid:1)

(cid:1)

(cid:0)

(cid:3)

,

(111)

(cid:3)






(cid:0)

where (z, ω) ∼ Nz,ω

0, 0; Qt

with Qt =

(cid:2)

Q qt
qt
qt

.

(cid:35)

(cid:34)

(cid:1)

G.4 State evolution - Consistence between replicas and AMP - Bayes optimal case

State evolution - AMP

Using the change of variable ξ ← ξ +

1/2

ˆqt

W (cid:63), eq. (111) becomes:

qt+1 = Eξ

ZP0

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

(cid:0)

(cid:1)

(cid:104)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:124)

(cid:17)

(cid:105)

47

In addition in the Bayes-optimal case, as:

µ→i)(cid:124)

= mt − qt = 0

µ→i)(zµ − ωt

µ→i)(cid:124)

= Q − qt ,

µ→i)(cid:124)] = qt

(cid:105)



EX

ωt
µ→i(zµ − ωt
(cid:104)
EX [ωt

EX


µ→i(ωt
(cid:124)
µ − ωt

(z

(cid:104)

(cid:105)
0, 0; Qt

(cid:0)

(cid:1)

(cid:124)

the multivariate distribution can be written as a product: Nz,ω
using Pout(y|z) =

dPA(A)δ

y − ϕ0

, eq. (111) becomes:

out(z, A)

= Nω

0, qt

Nz

ω, Q − qt

. Hence,

(cid:0)

(cid:1)

(cid:0)

(cid:1)

ˆqt = αEω,z,A

(cid:2)

(cid:90)

(cid:82)
gout(ω, ϕ0
e− 1

(cid:0)

(cid:1)

out(z, A), Q − qt)

out(z, A), Q − qt)gout(ω, ϕ0
2 ω(cid:124)(qt)−1ω
(2π)K/2 det(qt)1/2
e− 1
(2π)K/2 det(Q − qt)1/2 gout((qt)1/2ξ, y, Q − qt)gout((qt)1/2ξ, y, Q − qt)

e− 1
(2π)K/2 det(Q − qt)1/2 gout(ω, y, Q − qt)gout(ω, y, Q − qt)

(cid:90)
2 (z−ω)(cid:124)(Q−qt)−1(z−ω)

2 (z−ω)(cid:124)(Q−qt)−1(z−ω)

dzPout(y|z)

dzPout(y|z)

(cid:3)

(cid:124)

(cid:124)

= α

dy

dω

(cid:90)

= α

dy

Dξ

(cid:90)
= αEy,ξ

(cid:90)
ZPout

(cid:90)
(qt)1/2ξ, y, Q − qt

gout

(qt)1/2ξ, y, Q − qt

gout

(qt)1/2ξ, y, Q − qt

(cid:16)
(cid:17)
Finally to summarize the state evolution equations can be written as:

(cid:16)

(cid:16)

(cid:17)

(cid:104)

(cid:124)

(cid:17)

(cid:105)

(cid:124)

qt+1 = Eξ

ZP0

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

(cid:104)
ˆqt = αEy,ξ

(cid:16)
ZPout

(qt)1/2ξ, y, Q − qt

gout

(cid:17)

(cid:16)

(qt)1/2ξ, y, Q − qt

(cid:17)

(cid:16)

gout

(cid:105)
(qt)1/2ξ, y, Q − qt

(cid:17)

(cid:124)

(112)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:105)

(cid:1)






State evolution - Replicas

(cid:104)

(cid:0)

Recall from sec. B, the free entropy eq. (76) reads

limn→∞ fn = extrq,ˆq

− 1

2 Tr[q ˆq] + IP + αIC

,

≡ Eξ

ZP0(ˆq1/2ξ, ˆq−1) log(ZP0(ˆq1/2ξ, ˆq−1))

(cid:9)

(cid:8)

,

(cid:2)
≡ Eξ,y

ZPout(q1/2ξ, y, Q − q) log(ZPout(q1/2ξ, y, Q − q))

(cid:3)

.

IP

IC






Taking the derivatives with respect to q and ˆq, using an integration by part and the following identities:

(cid:2)

(cid:3)

∂ZPout

∂q = − 1

2 q−1e

1

2 ξ(cid:124)ξ∂ξ

∂ZP0
∂ ˆq = − 1

2 ˆq−1e

1

2 ξ(cid:124)ξ∂ξ

e− 1
(cid:104)
e− 1

2 ξ(cid:124)ξ∂ξZPout
2 ξ(cid:124)ξ∂ξZP0

,

(cid:105)

,

(cid:104)

(cid:105)






the state evolution equations read:

q = 2 ∂IP
∂ ˆq




ˆq = 2α ∂IC
∂q

with

2

∂IP
∂ ˆq = 1
∂IC
∂q = 1

2




Eξ

(cid:2)
Ey,ξ

ZP0(ˆq1/2ξ, ˆq−1)fw(ˆq1/2ξ, ˆq)fw(ˆq1/2ξ, ˆq−1)(cid:124)
ZPout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)(cid:124)

(cid:3)

(cid:3)

that simplifies and allows to recover the state evolutions equations directly derived from AMP eq. (112), but



(cid:2)

48

without time indices

q = Eξ

ZP0(ˆq1/2ξ, ˆq−1)fw(ˆq1/2ξ, ˆq)fw(ˆq1/2ξ, ˆq−1)(cid:124)

,




(cid:2)
ˆq = αEy,ξ

ZPout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)(cid:124)

(cid:3)

.

(cid:3)

(cid:2)
H Parity machine for K = 2



Although we mainly focused on the committee machine, another classical two-layers neural network is the
parity machine [7] and our proof applies to this case as well. While learning is known to be computationally
hard for general K, the case K = 2 is special, and in fact can be reformulated as a committee machine, where
the sign activation function has been replaced by ϕ1(z) = 1(z (cid:54)= 0) − 1(z = 0):

K

n

K

n

Yµ = sign

sign

XµiW ∗
il

= ϕ1

sign

XµiW ∗
il

.

(113)

(cid:104)

l=1
(cid:89)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

(cid:104)

l=1
(cid:88)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

We have repeated our analysis for the K = 2 parity machine and the phase diagram is summarized in
Fig. 5 where we show the generalization error and the elements of the overlap matrix for Gaussian (left) and
binary weights (right), with the results of the AMP algorithm (points).

Below the specialization phase transition α < αspec, the symmetry of the output imposes the non-
specialized fixed point q00 = q01 = 0 to be the only solution, with αG
spec(K = 2) (cid:39)
2.49. Above the specialization transition αspec, the overlap becomes specialized with a non-trivial diagonal
term.

spec(K = 2) (cid:39) 2.48 and αB

Additionally, in the binary case, an information theoretical transition towards a perfect learning occurs
at αB
IT(K = 2) (cid:39) 2.00, meaning that the perfect generalization fixed point (q00 = 1, q01 = 0) becomes the
global optimizer of the free entropy. It leads to a first order phase transition of the AMP algorithm which
retrieves the perfect generalization phase only at αB
perf (K = 2) (cid:39) 3.03. This is similar to what happens in
single layer neural networks for the symmetric door activation function, see [11]. Again, these results for the
parity machine emphasize a gap between information-theoretical and computational performance.

49

Figure 5: Similar plot as in Fig. 2 but for the parity machine with two hidden neurons. Value of the order parameter
and the optimal generalization error for a parity machine with two hidden neurons with Gaussian weights (left)
and binary/Rademacher weights (right). SE and AMP overlaps are respectively represented in full line and points.

50

9
1
0
2
 
n
u
J
 
4
1
 
 
]

G
L
.
s
c
[
 
 
2
v
1
5
4
5
0
.
6
0
8
1
:
v
i
X
r
a

The committee machine: Computational to statistical gaps
in learning a two-layers neural network

Benjamin Aubin(cid:63)†, Antoine Maillard†, Jean Barbier♦,
Florent Krzakala†, Nicolas Macris⊗ and Lenka Zdeborová(cid:63)

Abstract

Heuristic tools from statistical physics have been used in the past to locate the phase transitions and
compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural
networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers
neural network model called the committee machine, under a technical assumption. We also introduce a
version of the approximate message passing (AMP) algorithm for the committee machine that allows to
perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes
in which a low generalization error is information-theoretically achievable while the AMP algorithm fails
to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large
computational gap.

Contents

1 Introduction

2 Summary of contributions and related works

3 Main technical results
3.1 A general model .
.
3.2 Two auxiliary inference problems .
3.3 The free entropy .
.
3.4
3.5 Approximate message passing, and its state evolution .

.
.
.
.
Learning the teacher weights and optimal generalization error .
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

4 From two to more hidden neurons, and the specialization phase transition
.
.
.
.

4.1 Two neurons .
.
4.2 More is different .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

5 Structure of the proof of Theorem 3.1
Interpolating estimation problem .

5.1
.
5.2 Overlap concentration and fundamental sum rule .
.
5.3 A technical lemma and an assumption .
.
.
5.4 Matching bounds

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

3

3

5
5
5
6
7
8

10
10
11

12
12
14
15
16

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

(cid:63) Institut de Physique Théorique, CNRS & CEA & Université Paris-Saclay, Saclay, France.
† Laboratoire de Physique Statistique, CNRS & Sorbonnes Universités & École Normale Supérieure, PSL University, Paris, France.
⊗ Laboratoire de Théorie des Communications, École Polytechnique Fédérale de Lausanne, Suisse.
♦ International Center for Theoretical Physics, Trieste, Italy.

1

C.1 The generalization error at K = 2 .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

6 Discussion

A Proof details for Theorem 3.1

A.1 The Nishimori property in Bayes-optimal learning .
.
.
A.2 Setting in the Hamiltonian language .
.
A.3 Free entropy variation: Proof of Proposition 5.2 .
.
.
.
.
A.4 Technical lemmas .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

B Replica calculation

C Generalization error

D The large K limit in the committee symmetric setting
.
.
.
.

D.1 Large K limit for sign activation function .
.
D.2 The Gaussian prior
.
.
.
.
D.3 The fixed point equations .
.
D.4 The generalization error at large K .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

E Linear networks show no specialization

F Update functions and AMP derivation
F.1 Definition of the update functions .
.
F.2 Derivation of the Approximate Message Passing algorithm .

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

G State evolution equations from AMP
.

.

.
G.1 Messages distribution .
.
.
.
G.2 State evolution equations - Non Bayes optimal case .
G.3 State evolution equations - Bayes optimal case .
.
.
G.4 State evolution - Consistence between replicas and AMP - Bayes optimal case .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

H Parity machine for K = 2

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

18

23
23
23
24
26

29

31
32

32
33
36
36
38

38

39
39
39

44
45
46
47
47

49

2

1 Introduction

While the traditional approach to learning and generalization follows the Vapnik-Chervonenkis [1] and
Rademacher [2] worst-case type bounds, there has been a considerable body of theoretical work on calculating
the generalization ability of neural networks for data arising from a probabilistic model within the framework
of statistical mechanics [3, 4, 5, 6, 7]. In the wake of the need to understand the effectiveness of neural
networks and also the limitations of the classical approaches [8], it is of interest to revisit the results that have
emerged thanks to the physics perspective. This direction is currently experiencing a strong revival, see e.g.
[9, 10, 11, 12].

Of particular interest is the so-called teacher-student approach, where labels are generated by feeding
i.i.d. random samples to a neural network architecture (the teacher) and are then presented to another neural
network (the student) that is trained using these data. Early studies computed the information theoretic
limitations of the supervised learning abilities of the teacher weights by a student who is given m independent
n-dimensional examples with α ≡ m/n = Θ(1) and n → ∞ [3, 4, 7]. These works relied on non-rigorous
heuristic approaches, such as the replica and cavity methods [13, 14]. Additionally no provably efficient
algorithm was provided to achieve the predicted learning abilities, and it was thus difficult to test those
predictions, or to assess the computational difficulty.

Recent developments in statistical estimation and information theory —in particular of approximate
message passing algorithms (AMP) [15, 16, 17, 18], and a rigorous proof of the replica formula for the optimal
generalization error [11]— allowed to settle these two missing points for single-layer neural networks (i.e.
without any hidden variables). In the present paper, we leverage on these works, and provide rigorous
asymptotic predictions and corresponding message passing algorithm for a class of two-layers networks.

2 Summary of contributions and related works

While our results hold for a rather large class of non-linear activation functions, we illustrate our findings on a
case considered most commonly in the early literature: the committee machine. This is possibly the simplest
version of a two-layers neural network where all the weights in the second layer are fixed to unity, and we
illustrate it in Fig. 1. Denoting Yµ the label associated with a n-dimensional sample Xµ, and W ∗
il the weight
connecting the i-th coordinate of the input to the l-th node of the hidden layer, it is defined by:

K

n

Yµ = sign

sign

XµiW ∗
il

.

(cid:104)

l=1
(cid:88)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

(1)

We concentrate here on the teacher-student scenario: The teacher generates i.i.d. data samples with i.i.d.
standard Gaussian coordinates Xµi ∼ N (0, 1), then she/he generates the associated labels Yµ using a committee
machine as in (1), with i.i.d. weights W ∗
il unknown to the student (in the proof section we will consider the
l=1), but in practice we consider
more general case of a distribution for the weights of the form
the fully separable case). The student is then given the m input-output pairs (Xµ, Yµ)m
µ=1 and knows the
(cid:81)
distribution P0 used to generate W ∗
il. The goal of the student is to learn the weights W ∗
il from the available
examples (Xµ, Yµ)m
µ=1 in order to reach the smallest possible generalization error (i.e. to be able to predict the
label the teacher would generate for a new sample not present in the training set).

n
i=1 P0({W ∗

il}K

There have been several studies of this model within the non-rigorous statistical physics approach in
the limit where α ≡ m/n = Θ(1), K = Θ(1) and n → ∞ [19, 20, 21, 22, 6, 7]. A particularly interesting
result in the teacher-student setting is the specialization of hidden neurons (see sec. 12.6 of [7], or [23] in the
context of online learning): For α < αspec (where αspec is a certain critical value of the sample complexity),

3

n features

(Xµi)m,n
µ,i=1
samples

K hidden
units

f (1)

f (1)

output

f (2)

Yµ

W (2) ∈ RK

W ∗

il ∈ Rn×K

Figure 1: The committee machine is one of the simplest models belonging to the considered model class (2), and
on which we focus to illustrate our results. It is a two-layers neural network with activation sign functions
f (1), f (2) = sign and weights W (2) fixed to unity. It is represented for K = 2.

the permutational symmetry between hidden neurons remains conserved even after an optimal learning, and
the learned weights of each of the hidden neurons are identical. For α > αspec, however, this symmetry gets
broken as each of the hidden units correlates strongly with one of the hidden units of the teacher. Another
remarkable result is the calculation of the optimal generalization error as a function of α.

Our first contribution consists in a proof of the replica formula conjectured in the statistical physics
literature, using the adaptive interpolation method of [24, 11], that allows to put several of these results on a
rigorous basis. This proof uses a technical unproven assumption. Our second contribution is the design of
an AMP-type of algorithm that is able to achieve the optimal generalization error in the above limit of large
dimensions for a wide range of parameters. The study of AMP —that is widely believed to be optimal between
all polynomial algorithms in the above setting [25, 26, 27, 28]— unveils, in the case of the committee machine
with a large number of hidden neurons, the existence a large hard phase in which learning is information-
theoretically possible, leading to a good generalization error decaying asymptotically as 1.25K/α (in the
α = Θ(K) regime), but where AMP fails and provides only a poor generalization that does not go to zero
when increasing α. This strongly suggests that no efficient algorithm exists in this hard region and therefore
there is a computational gap in learning such neural networks. In other problems where a hard phase was
identified its study boosted the development of algorithms that are able to match the predicted thresholds and
we anticipate this will translate to the present model.

We also want to comment on a related line of work that studies the loss-function landscape of neural
networks. While a range of works show under various assumptions that spurious local minima are absent
in neural networks, others show under different conditions that they do exist, see e.g. [29]. The regime of
parameters that is hard for AMP must have spurious local minima, but the converse is not true in general. It
might be that there are spurious local minima, yet the AMP approach succeeds. Moreover, in all previously
studied models in the Bayes-optimal setting the (generalization) error obtained with the AMP is the best
known and other approaches, e.g. (noisy) gradient based, spectral algorithms or semidefinite programming,
are not better in generalizing even in cases where the “student” models are overparametrized. Of course in
order to be in the Bayes-optimal setting one needs to know the model used by the teacher which is not the
case in practice.

4

3 Main technical results

3.1 A general model

While in the illustration of our results we shall focus on the model (1), all our formulas are valid for a broader
class of models: Given m input samples (Xµi)m,n
il the teacher-weight connecting the i-th
input (i.e. visible unit) to the l-th node of the hidden layer. For a generic function ϕout : RK × R → R one can
formally write the output as

µ,i=1, we denote W ∗

Yµ = ϕout

n

1
√
n

(cid:16)(cid:110)

i=1
(cid:88)

XµiW ∗
il

K

l=1

, Aµ

(cid:111)

(cid:17)

or

Yµ ∼ Pout

·

1
√
n

n

i=1
(cid:88)

(cid:16)

(cid:12)
(cid:110)
(cid:12)
(cid:12)

K

XµiW ∗
il

,

(2)

l=1

(cid:111)

(cid:17)

where (Aµ)m
part of the model, generally accounting for noise.

µ=1 are i.i.d. real valued random variables with known distribution PA, that form the probabilistic

For deterministic models the second argument is simply absent (or is a Dirac mass). We can view altern-
atively (2) as a channel where the transition kernel Pout is directly related to ϕout. As discussed above, we
focus on the teacher-student scenario where the teacher generates Gaussian i.i.d. data Xµi ∼ N (0, 1), and i.i.d.
weights W ∗
µ=1 by computing marginal means of
the posterior probability distribution (5).

il ∼ P0. The student then learns W ∗ from the data (Xµ, Yµ)m

Different scenarii fit into this general framework. Among those, the committee machine is obtained when
K
l=1 sign(hl)) while another model considered previously is given by the parity
choosing ϕout(h) = sign(
K
l=1 sign(hl), see e.g. [7] and sec. H for the numerical results in the case K = 2.
machine, when ϕout(h) =
A number of layers beyond two has also been considered, see [22]. Other activation functions can be used, and
many more problems can be described, e.g. compressed pooling [30, 31] or multi-vector compressed sensing
[32].

(cid:80)
(cid:81)

3.2 Two auxiliary inference problems

Denote SK the finite dimensional vector space of K × K matrices, S +
K × K matrices, S ++
K s.t. N − M ∈ S +
S+

K for positive definite K × K matrices, and ∀ N ∈ S +
K}. Note that S +
K(N ) is convex and compact.

K the convex set of semi-definite positive
K(N ) ≡ {M ∈

K we set S+

Stating our results requires introducing two simpler auxiliary K-dimensional estimation problems:
• The first one consists in retrieving a K-dimensional input vector W0 ∼ P0 from the output of a Gaussian
vector channel with K-dimensional observations

Y0 = r1/2W0 + Z0 ,

Z0 ∼ N (0, IK×K) and the “channel gain” matrix r ∈ S +

K. The posterior distribution on w = (wl)K

l=1 is

P (w|Y0) =

P0(w)eY

(cid:124)
0 r1/2w− 1

2 w(cid:124)rw ,

1
ZP0

(3)

and the associated free entropy (or minus free energy) is given by the expectation over Y0 of the log-partition
function

ψP0(r) ≡ E ln ZP0

and involves K dimensional integrals.
• The second problem considers K-dimensional i.i.d. vectors V, U ∗ ∼ N (0, IK×K) where V is considered to

5

be known and one has to retrieve U ∗ from a scalar observation obtained as

Y0 ∼ Pout( · |q1/2V + (ρ − q)1/2U ∗)

where the second moment matrix ρ ≡ E[W0W
q is in S+

K(ρ). The associated posterior is

(cid:101)

(cid:124)
0 ] is in S +

K (where W0 ∼ P0) and the so-called “overlap matrix”

P (u|

Y0, V ) =

1
ZPout

2 u(cid:124)u
e− 1
(2π)K/2 Pout

Y0|q1/2V + (ρ − q)1/2u

,

(4)

and the free entropy reads this time

(cid:101)

(cid:0)

(cid:101)

(cid:1)

ΨPout(q; ρ) ≡ E ln ZPout

(with the expectation over

Y0 and V ) and also involves K dimensional integrals.

3.3 The free entropy

(cid:101)

The central object of study leading to the optimal learning and generalization errors in the present setting is
the posterior distribution of the weights:

P ({wil}n,K

i,l=1 | {Xµi, Yµ}m,n

µ,i=1) =

P0({wil}K

l=1)

Pout

Yµ

Xµiwil

,

(5)

1
Zn

n

i=1
(cid:89)

m

µ=1
(cid:89)

(cid:16)

1
√
n

n

i=1
(cid:88)

(cid:110)

(cid:12)
(cid:12)
(cid:12)

K

l=1

(cid:111)

(cid:17)

where the normalization factor is nothing else than a partition function, i.e. the integral of the numerator over
{wil}n,K

i,l=1. The expected1 free entropy is by definition

fn ≡

E ln Zn .

1
n

The replica formula gives an explicit (conjectural) expression of fn in the high-dimensional limit n, m → ∞
with α = m/n fixed. We show in sec. B how the heuristic replica method [13, 14] yields the formula. This
computation was first performed, to the best of our knowledge, by [19] in the case of the committee machine.
Our first contribution is a rigorous proof of the corresponding free entropy formula using an interpolation
method [33, 34, 24], under a technical Assumption 1.

In order to formulate our results, we add an (arbitrarily small) Gaussian regularization noise Zµ

∆ to the

√

first expression of the model (2), where ∆ > 0, Zµ ∼ N (0, 1), which thus becomes

Yµ = ϕout

n

1
√
n

(cid:16)(cid:110)

i=1
(cid:88)

XµiW ∗
il

, Aµ

+ Zµ

∆ ,

√

K

l=1

(cid:111)

(cid:17)

so that the channel kernel is (u ∈ RK)

Pout(y|u) =

√

dPA(a)e− 1

2∆ (y−ϕout(u,a))2

.

1
2π∆

R

(cid:90)
Let us define the replica symmetric (RS) potential as

fRS(q, r) = fRS(q, r; ρ) ≡ ψP0(r) + αΨPout(q; ρ) −

Tr(rq),

1
2

1The symbol E will generally denote an expectation over all random variables in the ensuing expression (here {Xµi, Yµ}).

Subscripts will be used only when we take partial expectations or if there is an ambiguity.

(6)

(7)

(8)

(9)

6

where α ≡ m/n, and ΨPout(q; ρ) and ψP0(r) are the free entropies of the two simpler K-dimensional
estimation problems (3) and (4).

All along this paper, we assume the following hypotheses for our rigorous statements:

(H1) The prior P0 has bounded support in RK.
(H2) The activation ϕout : RK × R → R is a bounded C2 function with bounded first and second derivatives

w.r.t. its first argument (in RK-space).

(H3) For all µ = 1, . . . , m and i = 1, . . . , n we have i.i.d. Xµi ∼ N (0, 1).

We finally rely on a technical hypothesis, stated as Assumption 1 in section 5.3.

Theorem 3.1 (Replica formula). Suppose (H1), (H2) and (H3), and Assumption 1. Then for the model (7) with
kernel (8) the limit of the free entropy is:

lim
n→∞

fn ≡ lim
n→∞

1
n

E ln Zn = sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

(10)

This theorem extends the recent progress for generalized linear models of [11], which includes the case
K = 1 of the present contribution, to the phenomenologically richer case of two-layers problems such as the
committee machine. The proof sketch based on an adaptive interpolation method recently developed in [24] is
outlined in sec. 5 and the details can be found in sec. A.

Remark 3.2 (Relaxing the hypotheses). Note that, following similar approximation arguments as in [11], the
hypothesis (H1) can be relaxed to the existence of the second moment of the prior; thus covering the Gaussian case,
(H2) can be dropped (and thus include model (1) and its sign(·) activation) and (H3) extended to data matrices X
with i.i.d. entries of zero mean, unit variance and finite third moment. Moreover, the case ∆ = 0 can be considered
when the outputs are discrete, as in the committee machine (1), see [11]. The channel kernel becomes in this case
dPA(a)1(y − ϕout(u, a)) and the replica formula is the limit ∆ → 0 of the one provided in
Pout(y|u) =
Theorem 3.1. In general this regularizing noise is needed for the free entropy limit to exist.

(cid:82)

3.4 Learning the teacher weights and optimal generalization error

A classical result in Bayesian estimation is that the estimator ˆW that minimizes the mean-square error with the
ground-truth W ∗ is given by the expected mean of the posterior distribution. Denoting q∗ the extremizer in
the replica formula (10), we expect from the replica method that in the limit n → ∞, m/n = α, and with high
probablity, ˆW (cid:124)W ∗/n → q∗. We refer to proposition 5.3 and to the proof in sec. A for the precise statement,
that remains rigorously valid only in the presence of an additional (possibly infinitesimal) side-information.
From the overlap matrix q∗, one can compute the Bayes-optimal generalization error when the student tries to
classify a new, yet unseen, sample Xnew. The estimator of the new label ˆYnew that minimizes the mean-square
error with the true label is given by computing the posterior mean of ϕout(Xneww) (Xnew is a row vector).
Given the new sample, the optimal generalization error is then

1
2

EX,W ∗

Ew|X,Y

ϕout(Xneww)

− ϕout(XnewW ∗)

2

−−−→
n→∞

(cid:15)g(q∗),

(11)

(cid:104)(cid:0)

(cid:2)

(cid:3)

(cid:105)

(cid:1)

where w is distributed according to the posterior measure (5) (note that this Bayes-optimal computation differs
from the so-called Gibbs estimator by a factor 2, see sec. C). In particular, when the data X is drawn from
the standard Gaussian distribution on Rm×n, and is thus rotationally invariant, it follows that this error only
depends on w(cid:124)W ∗/n, which converges to q∗. Then a direct algebraic computation gives a lengthy but explicit
formula for (cid:15)g(q∗), as shown in sec. C.

7

3.5 Approximate message passing, and its state evolution

Our next result is based on an adaptation of a popular algorithm to solve random instances of generalized linear
models, the Approximate Message Passing (AMP) algorithm [15, 16], for the case of the committee machine and
models described by (2).

The AMP algorithm can be obtained as a Taylor expansion of loopy belief-propagation (see sec. F) and
also originates in earlier statistical physics works [35, 36, 37, 38, 39, 26]. It is conjectured to perform the best
among all polynomial algorithms in the framework of these models. It thus gives us a tool to evaluate both the
intrinsic algorithmic hardness of the learning and the performance of existing algorithms with respect to the
optimal one in this model.

Algorithm 1 Approximate Message Passing for the committee machine

Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: gout,µ = 0, Σi = IK×K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
Initialize: ˆWi ∈ RK and ˆCi, ∂ωgout,µ ∈ S +
repeat

K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 1.

n

ωt

ˆW t

µ =

i −

Xµi√
n

Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
X 2
µi
n

i Σt−1
i

−1 ˆCt

gt−1
out,µ

Σt−1
i

i=1
(cid:80)
out,µ = gout(ωt
gt

Update of gout,µ ∈ RK and ∂ωgout,µ ∈ S +
K:
out,µ = ∂ωgout(ωt

µ, Yµ, V t
µ)
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt

(cid:1)
∂ωgt

Xµi√

i = Σt
T t
i

X 2
n ∂ωgt
µi

out,µ −

ˆW t
i

out,µ

n gt

m

(cid:1)

(cid:0)

(cid:0)

|

|

(cid:16)

µ=1
(cid:80)

Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = fw(Σt

i, T t
i )

|

ˆCt+1

i = fc(Σt

i, T t
i )

(cid:17)

µ, Yµ, V t
µ)

i = −
(cid:16)

m

µ=1
(cid:80)

V t
µ =

X 2
µi
n

ˆCt
i

n

i=1
(cid:80)

X 2
n ∂ωgt
µi

out,µ

−1

(cid:17)

ˆW t+1
t = t + 1

until Convergence on ˆW , ˆC.
Output: ˆW and ˆC.

The AMP algorithm is summarized by its pseudo-code in Algorithm 2, where the update functions gout,
∂ωgout, fw and fc are related, again, to the two auxiliary problems (3) and (4). The functions fw(Σ, T ) and
fc(Σ, T ) are respectively the mean and variance under the posterior distribution (3) when r → Σ−1 and
Y0 → Σ1/2T , while gout(ωµ, Yµ, Vµ) is given by the product of V
and the mean of u under the posterior
Y0 → Yµ, ρ − q → Vµ and q1/2V → ωµ (see sec. F for more details). After convergence, ˆW estimates
(4) using
the weights of the teacher-neural network. The label of a sample Xnew not seen in the training set is estimated
by the AMP algorithm as

−1/2
µ

(cid:101)

Y t
new =

dy

dzl

y Pout(y|{zl}K

l=1)N (z; ωt

new, V t

new) ,

(12)

K

(cid:90)

l=1
(cid:89)

(cid:0)

(cid:1)

new =

n
i=1 Xnew,i ˆW t

where ωt
is the K × K covariance matrix (see below for the definition of qt
(cid:80)
the algorithm on GitHub [40].

i is the mean of the normally distributed variable z ∈ RK, and V t

new = ρ−qt

AMP
AMP). We provide a demonstration code of

AMP is particularly interesting because its performance can be tracked rigorously, again in the asymptotic
limit when n → ∞, via a procedure known as state evolution (a rigorous version of the cavity method in
physics [14]), see [18]. State evolution tracks the value of the overlap between the hidden ground truth W ∗ and

8

Figure 2: Generalization error and order parameter for a committee machine with two hidden neurons (K = 2)
with Gaussian weights (left), binary/Rademacher weights (right). These are shown as a function of the ratio
α = m/n between the number of samples m and the dimensionality n. Lines are obtained from the state evolution
(SE) equations (dominating solution is shown in full line), data-points from the AMP algorithm averaged over 10
instances of the problem of size n = 104. q00 and q01 denote diagonal and off-diagonal overlaps, and their values
are given by the labels on the far-right of the figure.

the AMP estimate ˆW t, defined as qt

AMP ≡ limn→∞( ˆW t)(cid:124)W ∗/n, via the iteration of the following equations:

qt+1
AMP = 2∇ψP0(rt

AMP) ,

rt+1
AMP = 2α∇ΨPout(qt

AMP; ρ) .

(13)

See sec. G for more details and note that the fixed points of these equations correspond to the critical points of
the replica free entropy (10).

Let us comment further on the convergence of the algorithm. In the large n limit, and if the integrals
are performed without errors, then the algorithm is guaranteed to converge. This is a consequence of the
state evolution combined with the Bayes-optimal setting. In practice, of course, n is finite and integrals are
approximated. In that case convergence is not guaranteed, but is robustly achieved in all the cases presented
in this paper. We also expect (by experience with the single layer case) that if the input-data matrix is not
random (which is beyond our assumptions) then we will encounter convergence issues, which could be fixed
by moving to some variant of the algorithm such as VAMP [41].

9

Figure 3: (Left) Bayes optimal and AMP generalization errors and (right) diagonal and off-diagonal overlaps q00,
q01 for a committee machine with a large number of hidden neurons K and Gaussian weights, as a function of the
rescaled parameter ˜α = α/K. Solutions corresponding to global and local minima of the replica free entropy are
spinodal (cid:39) 7.17, ie the
respectively represented with full and dashed lines. The dotted line marks the spinodal at
apparition of a local minimum in the replica free entropy, associated to a solution with specialized hidden units.
spec (cid:39) 7.65, at which the specialized
The dotted-dashed line shows the first order specialization transition at
spec, AMP reaches the Bayes optimal generalization error
fixed point becomes the global minimum. For
and overlaps, corresponding to a non-specialized solution. However, for
spec, the AMP algorithm does not
follow the optimal specialized solution and is stuck in the non-specialized solution plateau, represented with
dashed lines. Hence it unveils a large computational gap (yellow area).

α >
(cid:101)

α <

αG

αG

αG

αG

(cid:101)

(cid:101)

(cid:101)

(cid:101)

(cid:101)

4 From two to more hidden neurons, and the specialization

phase transition

4.1 Two neurons

Let us now discuss how the above results can be used to study the optimal learning in the simplest non-trivial
case of a two-layers neural network with two hidden neurons, that is when model (1) is simply

n

n

Yµ = sign

sign

XµiW ∗
i1

+ sign

XµiW ∗
i2

,

(cid:104)

(cid:16)

i=1
(cid:88)

(cid:17)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

and is represented in Fig. 1, with the convention that sign(0) = 0. We remind that the input-data matrix X
has i.i.d. N (0, 1) entries, and the teacher-weights W ∗ used to generate the labels Y are taken i.i.d. from P0.
In Fig. 2 we plot the optimal generalization error as a function of the sample complexity α = m/n. In
the left panel the weights are Gaussian (for both the teacher and the student), while in the right panel they
are binary/Rademacher. The full line is obtained from the fixed point of the state evolution (SE) of the AMP
algorithm (13), corresponding to the extremizer of the replica free entropy (10). The points are results of
the AMP algorithm run till convergence averaged over 10 instances of size n = 104. In this case and with
random initial conditions the AMP algorithm did converge in all our trials. As expected we observe excellent

10

agreement between the SE and AMP.

In both left and right panels of Fig. 2 we observe the so-called specialization phase transition. Indeed
(13) has two types of fixed points: a non-specialized fixed point where every matrix element of the K × K
order parameter q is the same (so that both hidden neurons learn the same function) and a specialized fixed
point where the diagonal elements of the order parameter are different from the non-diagonal ones. We
checked for other types of fixed points for K = 2 (one where the two diagonal elements are not the same),
but have not found any. In terms of weight-learning, this means for the non-specialized fixed point that the
estimators for both W1 and W2 are the same, whereas in the specialized fixed point the estimators of the
weights corresponding to the two hidden neurons are different, and that the network “figured out” that the
data are better described by a model that is not linearly separable. The specialized fixed point is associated
with lower error than the non-specialized one (as one can see in Fig. 2). The existence of this phase transition
was discussed in statistical physics literature on the committee machine, see e.g. [20, 23].

For Gaussian weights (Fig. 2 left), the specialization phase transition arises continuously at αG

spec(K =
2) (cid:39) 2.04. This means that for α < αG
spec(K = 2) the number of samples is too small, and the student-neural
network is not able to learn that two different teacher-vectors W1 and W2 were used to generate the observed
labels. For α > αG
spec(K = 2), however, it is able to distinguish the two different weight-vectors and the
generalization error decreases fast to low values (see Fig. 2). For completeness we remind that in the case
of K = 1 corresponding to single-layer neural network no such specialization transition exists. We show
in sec. E that it is absent also in multi-layer neural networks as long as the activations remain linear. The
non-linearity of the activation function is therefore an essential ingredient in order to observe a specialization
phase transition.

The right part of Fig. 2 depicts the fixed point reached by the state evolution of AMP for the case of binary
weights. We observe two phase transitions in the performance of AMP in this case: (a) the specialization phase
spec(K = 2) (cid:39) 1.58, and for slightly larger sample complexity a transition towards perfect
transition at αB
generalization (beyond which the generalization error is asymptotically zero) at αB
perf (K = 2) (cid:39) 1.99. The
binary case with K = 2 differs from the Gaussian one in the fact that perfect generalization is achievable at finite
α. While the specialization transition is continuous here, the error has a discontinuity at the transition of perfect
generalization. This discontinuity is associated with the 1st order phase transition (in the physics nomenclature),
leading to a gap between algorithmic (AMP in our case) performance and information-theoretically optimal
performance reachable by exponential algorithms. To quantify the optimal performance we need to evaluate
the global extremum of the replica free entropy (not the local one reached by the state evolution). In doing
so that we get that information theoretically there is a single discontinuous phase transition towards perfect
generalization at αB

IT(K = 2) (cid:39) 1.54.

While the information-theoretic and specialization phase transitions were identified in the physics literature
on the committee machine [20, 21, 3, 4], the gap between the information-theoretic performance and the
performance of AMP —that is conjectured to be optimal among polynomial algorithms— was not yet discussed
in the context of this model. Indeed, even its understanding in simpler models than those discussed here, such
as the single layer case, is more recent [15, 26, 25].

4.2 More is different

It becomes more difficult to study the replica formula for larger values of K as it involves (at least) K-
dimensional integrals. Quite interestingly, it is possible to work out the solution of the replica formula in
the large K limit (thus taken after the large n limit, so that K/n vanishes). It is indeed natural to look for
(cid:124)
K, with the
solutions of the replica formula, as suggested in [19], of the form q = qdIK×K + (qa/K)1K1
unit vector 1K = (1)K
l=1. Since both q and ρ are assumed to be positive, this scaling implies that 0 ≤ qd ≤ 1
and 0 ≤ qa + qd ≤ 1, as it should, see sec. D. We also detail in this same section the corresponding large K

11

expansion of the free entropy for the teacher-student scenario with Gaussian weights. Only the information-
theoretically reachable generalization error was computed [19], thus we concentrated on the analysis of
performance of AMP by tracking the state evolution equations. In doing so, we unveil a large computational
gap.

(cid:101)

(cid:101)

αG

αG

αG

spec and then jumps discontinuously down to reach a decay aymptotically as 1.25/

In the right panel of Fig. 3 we show the fixed point values of the two overlaps q00 = qd + qa/K and
q01 = qa/K and the resulting generalization error, plotted in the left panel. As discussed in [19] it can be
written in a closed form as (cid:15)g = arccos [2 (qa + arcsin qd) /π] /π, represented in the left panel of Fig. 3. The
α ≡ α/K. The specialization is now a 1st order
specialization transition arises for α = Θ(K) so we define
spinodal (cid:39) 7.17 but the free
phase transition, meaning that the specialization fixed point first appears at
spec (cid:39) 7.65. This has
entropy global extremizer remains the one of the non-specialized fixed point until
interesting implications for the optimal generalization error that gets towards a plateau of value εplateau (cid:39) 0.28
for
α. See left panel
α <
of Fig. 3.
(cid:101)
AMP is conjectured to be optimal among all polynomial algorithms (in the considered limit) and thus
analyzing its state evolution sheds light on possible computational-to-statistical gaps that come hand in hand
with 1st order phase transitions. In the regime of α = Θ(K) for large K the non-specialized fixed point is
always stable implying that AMP will not be able to give a lower generalization error than εplateau. Analyzing
the replica formula for large K in more details, see sec. D, we concluded that AMP will not reach the optimal
generalization for any α < Θ(K2). This implies a rather sizable gap between the performance that can
be reached information-theoretically and the one reachable tractably (see yellow area in Fig. 3). Such large
computational gaps have been previously identified in a range of inference problems —most famously in the
planted clique problem [27]— but the committee machine is the first model of a multi-layer neural network
with realistic non-linearities (the parity machine is another example but use a very peculiar non-linearity) that
presents such large gap.

(cid:101)

(cid:101)

(cid:101)

5 Structure of the proof of Theorem 3.1

All along this section we assume (H1), (H2) and (H3), and all the rigorous statements are implicitely assuming
these hypotheses. We denote K-dimensional column vectors by underlined letters. In particular W ∗
i =
(W ∗
µ be K-dimensional vectors with i.i.d. N (0, 1) components.
Let sn ∈ (0, 1/2] a sequence that goes to 0 as n increases, and let M be the compact subset of matrices in
K with eigenvalues in the interval [1, 2]. For all M ∈ snM, 2snIK×K − M ∈ S +
S++
K.

l=1. For µ = 1, . . . m, let V µ, U ∗

l=1, wi = (wil)K

il)K

5.1 Interpolating estimation problem

Let (cid:15) = ((cid:15)1, (cid:15)2) ∈ (snM)2. Let q : [0, 1] → S +
will later on depend on (cid:15)), and

K(ρ) and r : [0, 1] → S +

K be two “interpolation functions” (that

R1(t) ≡ (cid:15)1 +

r(v)dv ,

R2(t) ≡ (cid:15)2 +

q(v)dv .

t

0
(cid:90)

t

0
(cid:90)

For t ∈ [0, 1], define the K-dimensional vector:

1 − t
n

(cid:114)

n

i=1
(cid:88)

St,µ ≡

XµiW ∗

i +

R2(t) V µ +

tρ − R2(t) + 2snIK×K U ∗
µ

(cid:112)
where matrix square-roots (that we denote equivalently A1/2 or
A) are well defined. We interpolate with
auxiliary problems related to those discussed in sec. 3; the interpolating estimation problem is given by the

(cid:112)

√

(14)

(15)

12

following observation model, with two types of t-dependent observations:

Yt,µ ∼ Pout( · | St,µ),
Y (cid:48)

R1(t) W ∗

t,i =

i + Z(cid:48)

1 ≤ µ ≤ m ,
i, 1 ≤ i ≤ n ,

(cid:40)

(16)

i is (for each i) a K-vector with i.i.d. N (0, 1) components, and Y (cid:48)

where Z(cid:48)
t,i is a K-vector as well. Recall that
in our notation the ∗-variables have to be retrieved, while the other random variables are assumed to be known
(except for the noise variables obviously). Define now st,µ by the expression of St,µ but with wi replacing W ∗
i
and uµ replacing U ∗

µ. We introduce the interpolating posterior:

(cid:112)

Pt,(cid:15)(w, u|Yt, Y (cid:48)

t , X, V ) =

1
Zn,(cid:15)(t)

n

i=1
(cid:89)

P0(wi)e− 1

2 (cid:107)Y (cid:48)

t,i−

R1(t)wi(cid:107)2
2

√

2 (cid:107)uµ(cid:107)2
2

e− 1
(2π)K/2 Pout(Yt,µ|st,µ)

(17)

m

µ=1
(cid:89)

where the normalization factor Zn,(cid:15)(t) equals the numerator integrated over all components of w and u. The
average free entropy at time t is by definition

fn,(cid:15)(t) ≡

E ln Zn,(cid:15)(t) =

E ln

Du

dP0(wi)

Pout(Yt,µ|st,µ)

1
n

1
n

(cid:90)

n

i=1
(cid:89)

m

µ=1
(cid:89)

√

e− 1

2 (cid:107)Y (cid:48)

t,i−

R1(t)wi(cid:107)2

2 ,

(18)

n

i=1
(cid:89)

where Du =

m
µ=1

K

l=1(2π)−1/2e−u2

µl/2.

interpolating model:

(cid:81)

(cid:81)

The presence of the small “pertubation” (cid:15) induces a proportional change in the free entropy of the

Lemma 5.1 (Perturbation of the free entropy). For all (cid:15) ∈ (snM)2 we have for t = 0 that |fn,(cid:15)(0) −
fn,(cid:15)=(0,0)(0)| ≤ C(cid:48)sn for some positive constant C(cid:48). Moreover, |fn − fn,(cid:15)=(0,0)(0)| ≤ Csn for some positive
constant C, so that

|fn − fn,(cid:15)=(0,0)(0)| = On(1) .

∇(cid:15)1fn,(cid:15)(0) = −

[ρ − E(cid:104)Q(cid:105)n,0,(cid:15)] ,

1
2

Proof. Let us compute (or directly obtain by the I-MMSE formula for vector channels [42, 43, 44])

(19)

(20)

where the K × K overlap matrix (Qll(cid:48)) is defined below by (23). Note that the r.h.s. of the above equation is
(up to a factor −1/2) the K × K MMSE matrix. Set uy(x) ≡ ln Pout(y|x). Now we compute (by calculations
very similar to the ones used in the proof of the following Proposition 5.2):

∇(cid:15)2fn,(cid:15)(0) =

1
2n

m

E

µ=1
(cid:88)

∇uYt,µ(St,µ)
(cid:104)

(cid:68)

∇uYt,µ(st,µ)

.

(cid:105)

n,0,(cid:15)

(cid:69)

Note that the r.h.s. of the above equation is symmetric by the Nishimori identity Proposition A.1. By
the mean value theorem we obtain then directly that |fn,(cid:15)(0) − fn,(cid:15)=(0,0)(0)| ≤ (cid:107)∇(cid:15)1fn,(cid:15)(0)(cid:107)F(cid:107)(cid:15)1(cid:107)F +
(cid:107)∇(cid:15)2fn,(cid:15)(0)(cid:107)F(cid:107)(cid:15)2(cid:107)F ≤ C maxi (cid:107)(cid:15)i(cid:107) ≤ C(cid:48)sn.

Using this lemma one verifies, using in particular continuity and boundedness properties of ψP0 and ΨPout

(see Lemma A.6 in sec. A for details; sec. A gathers the detailed proofs of all the propositions below):

fn,(cid:15)(0) = fn − K
2 + On(1) ,
1
0 r(t)dt) + αΨPout(
fn,(cid:15)(1) = ψP0(

(cid:40)

1

0 q(t)dt; ρ) − 1

2

1

0 Tr[ρ r(t)]dt − K

2 + On(1) .

(21)

(cid:82)

(cid:82)

13

(cid:82)

Here On(1) → 0 in the n, m → ∞ limit uniformly in t, q, r, (cid:15).

5.2 Overlap concentration and fundamental sum rule

Notice from (21) that at t = 1 the interpolating estimation problem constructs part of the RS potential (9),
while at t = 0 it is the free entropy (6) of the original model (7) (up to a constant). We thus now want to
compare these boundary values thanks to the identity

fn = fn,(cid:15)(0) +

+ On(1) = fn,(cid:15)(1) −

dt +

+ On(1) .

(22)

K
2

1

dfn,(cid:15)(t)
dt

0
(cid:90)

K
2

The next obvious step is therefore to compute the free entropy variation along the interpolation path, see

sec. A.3 for the proof:

Proposition 5.2 (Free entropy variation). Denote by (cid:104)−(cid:105)n,t,(cid:15) the (Gibbs) expectation w.r.t. the posterior Pt,(cid:15)
given by (17). Set uy(x) ≡ ln Pout(y|x). For all t ∈ [0, 1] we have

dfn,(cid:15)(t)
dt

= −

E

Tr

1
2

m

1
n

(cid:68)

(cid:104)(cid:16)

µ=1
(cid:88)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

Q − q(t)

+

Tr [r(t)(q(t) − ρ)] + On(1) ,

(cid:17)(cid:0)

n,t,(cid:15)

(cid:1)(cid:105)(cid:69)

1
2

where ∇ is the K-dimensional gradient w.r.t. the argument of uYt,µ(·), and On(1) → 0 in the n, m → ∞ limit
uniformly in t, q, r, (cid:15). Here, the K ×K overlap matrix Q is defined as

Qll(cid:48) ≡

W ∗

ilwil(cid:48) .

1
n

n

i=1
(cid:88)

(23)

We will plug this expression in identity (22), but in order to simplify it we need the following crucial
proposition, which says that the overlap concentrates. This property is what is generally refered to as a replica
symmetric behavior in statistical physics.

Proposition 5.3 (Overlap concentration). Assume that for any t ∈ (0, 1) the transformation (cid:15) ∈ (snM)2 (cid:55)→
(R1(t, (cid:15)), R2(t, (cid:15))) is a C1 diffeomorphism with a Jacobian determinant greater or equal to 1. Then one can find a
sequence sn going to 0 slowly enough such that there exists a constant C(ϕout, S, K, α) > 0 depending only on
the activation ϕout, the support S of the prior P0, the number of hidden neurons K and the sampling rate α, and
a constant γ > 0 such that ((cid:107) − (cid:107)F is the Frobenius norm):

1
Vol(snM)2

1

d(cid:15)

dt E

Q − E(cid:104)Q(cid:105)n,t,(cid:15)

(cid:90)(snM)2

0
(cid:90)

(cid:10)(cid:13)
(cid:13)

C(ϕout, S, K, α)
nγ

.

2
F

n,t,(cid:15) ≤
(cid:11)

(cid:13)
(cid:13)

The proof of this concentration result can be directly adapted from [45]. Using the results of [45] is
straightforward, under the assumption that (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism with a Jacobian determinant
greater or equal to 1. This Jacobian determinant can be computed from formula (30). To check that it is greater
than one we use Lemma 5.5 and need Assumption 1 stated in paragraph 5.3 below. With a Jacobian determinant
greater than one, we can “replace” (i.e., lower bound) the integrations over R1(t, (cid:15)), that naturally appear in
the proof of Proposition 5.3, by integrations over the perturbation matrix (cid:15). This is exactly what has been done
in the K = 1 version of the present model in [11] or in [46] i.e., in the scalar overlap case (see also [47] for a
setting with a matrix overlap as in the present case).

From there we can deduce the following fundamental sum rule which is at the core of the proof:

Proposition 5.4 (Fundamental sum rule). Assume that the interpolation functions r and q are such that the map
(cid:15) = ((cid:15)1, (cid:15)2) (cid:55)→ R(t, (cid:15)) = (R1(t, (cid:15)), R2(t, (cid:15))) given by (14) is a C1 diffeomorphism whose Jacobian determinant

14

Jn,(cid:15)(t) is greater or equal to 1. Assume that for all t ∈ [0, 1] and (cid:15) ∈ (snM)2 we have q(t) = q(t, (cid:15)) =
E(cid:104)Q(cid:105)n,t,(cid:15) ∈ S +

K(ρ). Then

fn =

1
Vol(snM)2

1

d(cid:15)

ψP0

r(t)dt

+ αΨPout

q(t, (cid:15))dt; ρ

(cid:90)(snM)2

(cid:110)

0
(cid:16) (cid:90)

(cid:17)

1

0
(cid:16) (cid:90)

−

1
2

0
(cid:90)

1

(cid:17)

Tr[q(t, (cid:15))r(t)]dt

+ On(1) .

(24)

(cid:111)

Proof. Let us denote Vn ≡ Vol(snM)2. The integral over (cid:15) is always over (snM)2. Consider the first term, i.e.
the Gibbs bracket, in the free entropy derivative given by Proposition 5.2. By the Cauchy-Schwarz inequality

E

Tr

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

Q − q(t)

(cid:16)

(cid:68)

d(cid:15)

1

(cid:104)(cid:16)
dt E

0
(cid:90)

≤

1
Vn (cid:90)

m

1
n

µ=1
(cid:88)
1
n

(cid:68)(cid:13)
(cid:13)
(cid:13)

m

µ=1
(cid:88)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:1)(cid:105)(cid:69)
×

n,t,(cid:15)

(cid:17)

1
Vn (cid:90)

1

0
(cid:90)

(cid:17)(cid:0)
(cid:124) − r(t)

2

F

n,t,(cid:15)

(cid:69)

(cid:13)
(cid:13)
(cid:13)

d(cid:15)

dt E

Q − q(t)

2
F

n,t,(cid:15) .

(cid:10)(cid:13)
(cid:13)

(cid:11)

(cid:13)
(cid:13)

The first term of this product is bounded by some constant C(ϕout, α) that only depend on ϕout and α, see
Lemma A.4 in sec. A.4. The second term is bounded by C(ϕout, S, K, α)n−γ by Proposition 5.3, since we
assumed that for all (cid:15) ∈ Bn and all t ∈ [0, 1] we have q(t) = q(t, (cid:15)) = E(cid:104)Q(cid:105)n,t,(cid:15). Therefore from Proposition
5.2 we obtain

1
Vn (cid:90)

d(cid:15)

0
(cid:90)

1

dfn,(cid:15)(t)
dt

dt =

1
2Vn (cid:90)

1

0
(cid:90)

d(cid:15)

Tr

q(t, (cid:15))r(t) − r(t)ρ

dt + On(1) + O(n−γ/2) .

(25)

(cid:2)
Here the small terms are both going to 0 uniformly w.r.t. to the choice of q and r. When replacing (25) in (22)
and combining it with (21) we reach the claimed identity.

(cid:3)

5.3 A technical lemma and an assumption

We give here a technical lemma used in the rest of the proof, and which allows us to detail the unproven
assumption on which we rely to prove Thm 3.1.

Lemma 5.5. The quantity E(cid:104)Q(cid:105)n,t,(cid:15) is a function of (n, t, R(t, (cid:15))). We define F
(2)
(2)
n ) is defined on the set:
n (t, R(t, (cid:15))) ≡ 2α∇ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)). Fn ≡ (F
F

(1)
n , F

(1)
n (t, R(t, (cid:15))) ≡ E(cid:104)Q(cid:105)n,t,(cid:15) and

Dn =

(t, r1, r2) ∈ [0, 1] × S +

K × S +

K

(cid:110)

(ρt − r2 + 2snIK) ∈ S +
K
(cid:12)
(cid:12)
(cid:12)

.

(cid:111)

Fn is a continuous function from Dn to S +
K × S +
and R2 on the interior of Dn. For every (t, R(t, (cid:15))) for which they are defined, they satisfy:

K(ρ). Moreover, Fn admits partial derivatives with respect to R1

(26)

(27)

We can now state the technical assumption on which we rely, and which essentially allows us to derive
that the map (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism with a Jacobian determinant greater or equal to 1 as it will
become clear in the next section:

(1)
∂(F
n )ll(cid:48)
∂(R1)ll(cid:48)

≥ 0.

K

l≤l(cid:48)
(cid:88)

15

Assumption 1. With the notations of Lemma 5.5,

(2)
∂(F
n )ll(cid:48)
∂(R2)ll(cid:48)

≥ 0.

K

l≤l(cid:48)
(cid:88)

Proof of Lemma 5.5. The fact that the image domain of Fn is S +
K(ρ) is known from Lemma A.2. The
continuity and differentiability of Fn follows from standard theorems of continuity and derivation under the
integral sign (recall that we are working at finite n). Indeed, the domination hypotheses are easily satisfied
since we work under (H1) and (H2).

K × S +

i≤j A(ij)(ij). Then one can write Tr[DR1F

Let us now prove (27). We write the formal differential of F

(1)
(1)
n with respect to R1 as DR1F
n , which
(1)
n ] ≥ 0, the trace of a 4-tensor over SK A(ij)(kl) being
is a 4-tensor, and our goal is to prove that Tr[DR1F
(1)
n ] = Tr[∇∇(cid:124)ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) × ∇R1
E(cid:104)Q(cid:105)n,t,(cid:15)]. We
Tr[A] =
know from Lemma A.2 and Lemma A.6 that ∇∇(cid:124)ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) is a positive symmetric matrix (when seen
E(cid:104)Q(cid:105)n,t,(cid:15) is also positive
as a linear operator over SK). Moreover, it is a known result that the derivative ∇R1
symmetric, since R1 is the matrix snr of a linear channel (see [42, 43, 44]). Since the product of two symmetric
positive matrices has always positive trace, this shows that Tr[DR1F

(1)
n ] ≥ 0.

(cid:80)

5.4 Matching bounds

Proposition 5.6 (Lower bound). Under Assumption 1, the free entropy of model (7) verifies

lim inf
n→∞

fn ≥ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

Proof. Choose first r(t) = r ∈ S +
the first order differential equation:

K a fixed matrix. Then R(t) = (R1(t), R2(t)) can be fixed as the solution to

d
dt

d
dt

R1(t) = r ,

R2(t) = E(cid:104)Q(cid:105)n,t,(cid:15) ,

and

R(0) = (cid:15) .

(28)

t
We denote this (unique) solution R(t, (cid:15)) = (rt + (cid:15)1,
0 q(v, (cid:15); r)dv + (cid:15)2). It is possible to check that this ODE
satisfies the hypotheses of the parametric Cauchy-Lipschitz theorem, and that by the Liouville formula the
determinant Jn,(cid:15)(t) of the Jacobian of (cid:15) (cid:55)→ R(t, (cid:15)) satisfies (see Lemma A.3 in sec. A)

(cid:82)

Jn,(cid:15)(t) = exp

(s, R(s, (cid:15))) ds

≥ 1 .

(29)

K

t

∂E(cid:104)Qll(cid:48)(cid:105)n,s,(cid:15)
∂(R2)ll(cid:48)

0
(cid:16) (cid:90)

l≥l(cid:48)
(cid:88)

Indeed, this sum of partial derivatives is always positive by Assumption 1. Moreover from (28), q(t, (cid:15); r) =
E(cid:104)Q(cid:105)n,t,(cid:15), which is in S +
K by Lemma A.2 in sec. A. The fact that the map (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism is
easily verified by its bijectivity (from the positivity of Jn,(cid:15)(t)) combined with the local inversion Theorem. All
the assumptions of Proposition 5.4 are veri.i.d. which then implies, recalling the potential expression (9),

fn =

1
Vol(snM)2

(cid:90)(snM)2

1

0
(cid:16) (cid:90)

d(cid:15) fRS

q(v, (cid:15); r)dv, r

+ On(1) .

This implies the lower bound as this equality is true for any r ∈ S +
K.

(cid:17)

(cid:17)

16

Proposition 5.7 (Upper bound). Under Assumption 1, the free entropy of model (7) verifies

lim sup
n→∞

fn ≤ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

Proof. We now fix R(t) = (R1(t), R2(t)) as the solution R(t, (cid:15)) = (
the following Cauchy problem:

t
0 r(v, (cid:15))dv + (cid:15)1,

t
0 q(v, (cid:15))dv + (cid:15)2) to

(cid:82)

(cid:82)

d
dt

R1(t) = 2α∇ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) ,

R2(t) = E(cid:104)Q(cid:105)n,t,(cid:15) ,

and

R(0) = (cid:15) .

d
dt

We denote this equation as ∂tR(t) = Fn(t, R(t)), R(0) = (cid:15). It is then possible to verify that Fn(R(t), t)
is a bounded C1 function of R(t), and thus a direct application of the Cauchy-Lipschitz theorem implies
that R(t, (cid:15)) is a C1 function of t and (cid:15). The Liouville formula for the Jacobian determinant of the map
(cid:15) ∈ (snM)2 (cid:55)→ R(t, (cid:15)) ∈ R(t, (snM)2) gives this time (see Lemma A.3 in sec. A)

Jn,(cid:15)(t) = exp

(s, R(s, (cid:15))) +

(s, R(s, (cid:15)))

ds

≥ 1 .

(30)

K

t

∂(Fn,1)ll(cid:48)
∂(R1)ll(cid:48)

0
(cid:16) (cid:90)

l≥l(cid:48) (cid:110)
(cid:88)

∂(Fn,2)ll(cid:48)
∂(R2)ll(cid:48)

(cid:111)

(cid:17)

The fact that this determinant is greater or equal to 1 for all t ∈ [0, 1] follows again from the positivity of this
sum of partials, see Lemma 5.5 and Assumption 1. Identity (30) implies the bijectivity of (cid:15) (cid:55)→ R(t, (cid:15)) which,
combined with the local inversion theorem, makes it a diffeomorphism. Since E(cid:104)Q(cid:105)n,t,(cid:15) and ρ − E(cid:104)Q(cid:105)n,t,(cid:15) are
positive matrices (see Lemma A.2 in sec. A) we also have that q(t, (cid:15)) ∈ S +
K(ρ) and since by the differential
equation we have r(t, (cid:15)) = 2α∇ΨPout(q(t, (cid:15))) and as ∇ΨPout(q) ∈ S +
K (see Lemma A.6 in sec. A), then
r(t, (cid:15)) ∈ S +

K too. We have everything needed for applying Proposition 5.4 again which gives in this case

fn =

1
Vol(snM)2

1

d(cid:15)

ψP0
(cid:110)

0
(cid:16) (cid:90)

(cid:90)

r(v, (cid:15))dv

+αΨPout

q(v, (cid:15))dv; ρ

−

Tr

q(v, (cid:15))r(v, (cid:15))dv

+On(1).

(cid:17)

(cid:17)

(cid:111)

1

0
(cid:16) (cid:90)

Then by convexity of ψP0 and ΨPout (see Lemma A.6),

fn ≤

=

1
Vol(snM)2
1
Vol(snM)2

d(cid:15)

d(cid:15)

1

0
(cid:90)

1

0
(cid:90)

(cid:90)

(cid:90)

We now remark that

dv

ψP0(r(v, (cid:15))dv) + αΨPout(q(v, (cid:15)); ρ) −
(cid:110)

dv fRS(q(v, (cid:15)), r(v, (cid:15))) + On(1) .

Tr[q(v, (cid:15))r(v, (cid:15))]

+ On(1)

(cid:111)

fRS(q(v, (cid:15)), r(v, (cid:15))) = inf
q∈S+

K (ρ)

fRS(q, r(v, (cid:15))) .

Indeed, for every r ∈ S +
K(ρ) (cid:55)→ fRS(q, r) ∈ R (recall (9)) is convex (by Lemma A.6),
and its q-derivative is ∇gr(q) = α∇ΨPout(q) − r/2. Since ∇gr(v,(cid:15))(q(v, (cid:15))) = 0 by definition of r(v, (cid:15)), and
S +
K(ρ) is convex, the minimum of gr(v,(cid:15))(q) is necessarily achieved at q = q(v, (cid:15)). Therefore

K, the function gr : q ∈ S +

fn ≤

1
Vol(snM)2

1

d(cid:15)

(cid:90)(snM)2

0
(cid:90)

dv inf
q∈S+

K (ρ)

fRS (q, r(v, (cid:15))) + On(1) ≤ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) + On(1),

which concludes the proof of Proposition 5.7.

Combining these two matching bounds ends the proof of Theorem 3.1.

1

0
(cid:90)

1
2

1
2

17

6 Discussion

One of the contributions of this paper is the design of an AMP-type algorithm that is able to achieve the
Bayes-optimal learning error in the limit of large dimensions for a range of parameters out of the so-called
hard phase. The hard phase is associated with first order phase transitions appearing in the solution of the
model. In the case of the committee machine with a large number of hidden neurons we identify a large hard
phase in which learning is possible information-theoretically but not efficiently. In other problems where
such a hard phase was identified, its study boosted the development of algorithms that are able to match the
predicted threshold. We anticipate this will also be the same for the present model. We should, however, note
that for larger K > 2 the present AMP algorithm includes higher-dimensional integrals that hamper the speed
of the algorithm. Our current strategy to tackle this is to combine the large-K expansion and use it in the
algorithm. Detailed account of the corresponding results are left for future work.

We studied the Bayes-optimal setting where the student-network is the same as the teacher-network, for
which the replica method can be readily applied. The method still applies when the number of hidden units in
the student and teacher are different, while our proof does not generalize easily to this case. It is an interesting
subject for future work to see how the hard phase evolves under over-parametrization and what is the interplay
between the simplicity of the loss-landscape and the achievable generalization error. We conjecture that in
the present model over-parametrization will not improve the generalization error achieved by AMP in the
Bayes-optimal case.

Even though we focused in this paper on a two-layers neural network, the analysis and algorithm can be
readily extended to a multi-layer setting, see [22], as long as the number of layers as well as the number of
hidden neurons in each layer is held constant, and as long as one learns only weights of the first layer, for which
the proof already applies. The numerical evaluation of the phase diagram would be more challenging than
the cases presented in this paper as multiple integrals would appear in the corresponding formulas. In future
works, we also plan to analyze the case where the weights of the second and subsequent layers (including the
biases of the activation functions) are also learned. This could be done for instance with a combination of EM
and AMP along the lines of [48, 49] where this is done for the simpler single layer case.

Concerning extensions of the present work, an important open case is the one where the number of
samples per dimension α = Θ(1) and also the size of the hidden layer per dimension K/n = Θ(1) as n → ∞,
while in this paper we treated the case K = Θ(1) and n → ∞. This other scaling where K/n = Θ(1) is
challenging even for the non-rigorous replica method.

Acknowledgments

This work has been supported by the ERC under the European Union’s FP7 Grant Agreement 307087-SPARCS
and the European Union’s Horizon 2020 Research and Innovation Program 714608-SMiLe, as well as by the
French Agence Nationale de la Recherche under grant ANR-17-CE23-0023-01 PAIL and the Swiss National
Foundation grant no 200021E-175541. Additional funding is acknowledged by A.M., F.K. and J.B. from “Chaire
de recherche sur les modèles et sciences des données”, Fondation CFM pour la Recherche-ENS. We also
acknowledge Léo Miolane for discussions.

References

[1] V. Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.

18

[2] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

[3] S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Physical

[4] T. L. Watkin, A. Rau, and M. Biehl. The statistical mechanics of learning a rule. Reviews of Modern Physics,

Review A, 45(8):6056, 1992.

65(2):499, 1993.

[5] R. Monasson and R. Zecchina. Learning and generalization theories of large committee-machines. Modern

Physics Letters B, 9(30):1887–1897, 1995.

[6] R. Monasson and R. Zecchina. Weight space structure and internal representations: a direct approach to
learning and generalization in multilayer neural networks. Physical review letters, 75(12):2432, 1995.

[7] A. Engel and C. P. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press, 2001.

[8] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking

generalization. arXiv preprint arXiv:1611.03530, 2016. in ICLR 2017.

[9] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. Chayes, L. Sagun, and
R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838,
2016. in ICLR 2017.

[10] C. H. Martin and M. W. Mahoney. Rethinking generalization requires revisiting old ideas: statistical

mechanics approaches and complex learning behavior. arXiv preprint arXiv:1710.09553, 2017.

[11] J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborová. Optimal errors and phase transitions in
high-dimensional generalized linear models. Proceedings of the National Academy of Sciences, 116(12):5451–
5460, 2019.

[12] M. Baity-Jesi, L. Sagun, M. Geiger, S. Spigler, G. Ben-Arous, C. Cammarota, Y. LeCun, M. Wyart, and G. Bir-
oli. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint arXiv:1803.06969,
2018.

[13] M. Mézard, G. Parisi, and M. Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method

and Its Applications, volume 9. World Scientific Publishing Company, 1987.

[14] M. Mézard and A. Montanari. Information, physics, and computation. Oxford University Press, 2009.

[15] D. L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing.

Proceedings of the National Academy of Sciences, 106(45):18914–18919, 2009.

[16] S. Rangan. Generalized approximate message passing for estimation with random linear mixing. In
Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pages 2168–2172. IEEE,
2011.

[17] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to

compressed sensing. IEEE Transactions on Information Theory, 57(2):764–785, 2011.

[18] A. Javanmard and A. Montanari. State evolution for general approximate message passing algorithms,
with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144, 2013.

19

[19] H. Schwarze. Learning a rule in a multilayer neural network. Journal of Physics A: Mathematical and

General, 26(21):5781, 1993.

20(4):375, 1992.

Letters), 21(7):785, 1993.

[20] H. Schwarze and J. Hertz. Generalization in a large committee machine. EPL (Europhysics Letters),

[21] H. Schwarze and J. Hertz. Generalization in fully connected committee machines. EPL (Europhysics

[22] G. Mato and N. Parga. Generalization properties of multilayered neural networks. Journal of Physics A:

Mathematical and General, 25(19):5047, 1992.

[23] D. Saad and S. A. Solla. On-line learning in soft committee machines. Physical Review E, 52(4):4225, 1995.

[24] J. Barbier and N. Macris. The adaptive interpolation method: a simple scheme to prove replica formulas

in bayesian inference. Probability Theory and Related Fields, pages 1–53, 2018.

[25] D. L. Donoho, I. Johnstone, and A. Montanari. Accurate prediction of phase transitions in compressed
sensing via a connection to minimax denoising. IEEE transactions on information theory, 59(6):3396–3433,
2013.

[26] L. Zdeborová and F. Krzakala. Statistical physics of inference: thresholds and algorithms. Advances in

Physics, 65(5):453–552, 2016.

[27] Y. Deshpande and A. Montanari. Finding hidden cliques of size \sqrt {N/e} n/e in nearly linear time.

Foundations of Computational Mathematics, 15(4):1069–1128, 2015.

[28] A. S. Bandeira, A. Perry, and A. S. Wein. Notes on computational-to-statistical gaps: predictions using

statistical physics. arXiv preprint arXiv:1803.11132, 2018.

[29] I. Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv

preprint arXiv:1712.08968, 2017.

[30] A. E. Alaoui, A. Ramdas, F. Krzakala, L. Zdeborová, and M. I. Jordan. Decoding from pooled data: Sharp

information-theoretic bounds. arXiv preprint arXiv:1611.09981, 2016.

[31] A. El Alaoui, A. Ramdas, F. Krzakala, L. Zdeborová, and M. I. Jordan. Decoding from pooled data: Phase
transitions of message passing. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages
2780–2784. IEEE, 2017.

[32] J. Zhu, D. Baron, and F. Krzakala. Performance limits for noisy multimeasurement vector problems. IEEE

Transactions on Signal Processing, 65(9):2444–2454, 2017.

[33] F. Guerra. Broken replica symmetry bounds in the mean field spin glass model. Communications in

mathematical physics, 233(1):1–12, 2003.

[34] M. Talagrand. Spin glasses: a challenge for mathematicians: cavity and mean field models, volume 46.

Springer Science & Business Media, 2003.

[35] D. J. Thouless, P. W. Anderson, and R. G. Palmer. Solution of’solvable model of a spin glass’. Philosophical

Magazine, 35(3):593–601, 1977.

[36] M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method.

Journal of Physics A: Mathematical and General, 22(12):2181–2190, 1989.

20

[37] M. Opper and O. Winther. Mean field approach to bayes learning in feed-forward neural networks.

Physical review letters, 76(11):1964, 1996.

[38] Y. Kabashima. Inference from correlated patterns: a unified theory for perceptron learning and linear

vector channels. Journal of Physics: Conference Series, 95(1):012001, 2008.

[39] C. Baldassi, A. Braunstein, N. Brunel, and R. Zecchina. Efficient supervised learning in networks with

binary synapses. Proceedings of the National Academy of Sciences, 104(26):11079–11084, 2007.

[40] B. Aubin, A. Maillard, J. Barbier, F. Krzakala, N. Macris, and L. Zdeborová. AMP implementation of the

committee machine. https://github.com/benjaminaubin/TheCommitteeMachine, 2018.

[41] P. Schniter, S. Rangan, and A. K. Fletcher. Vector approximate message passing for the generalized linear
model. In Signals, Systems and Computers, 2016 50th Asilomar Conference on, pages 1525–1529. IEEE, 2016.

[42] G. Reeves, H. D. Pfister, and A. Dytso. Mutual information as a function of matrix snr for linear gaussian
channels. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 1754–1758. IEEE,
2018.

[43] M. Payaró, M. Gregori, and D. Palomar. Yet another entropy power inequality with an application. In
Wireless Communications and Signal Processing (WCSP), 2011 International Conference on, pages 1–5. IEEE,
2011.

[44] M. Lamarca. Linear precoding for mutual information maximization in mimo systems.

In Wireless

Communication Systems, 2009. ISWCS 2009. 6th International Symposium on, pages 26–30. IEEE, 2009.

[45] J. Barbier. Overlap matrix concentration in optimal bayesian inference. arXiv preprint arXiv:1904.02808,

2019.

[46] J. Barbier and N. Macris. The adaptive interpolation method for proving replica formulas. applications to
the curie-weiss and wigner spike models. Journal of Physics A: Mathematical and Theoretical, 2019.

[47] J. Barbier, C. Luneau, and N. Macris. Mutual information for low-rank even-order symmetric tensor

factorization. arXiv preprint arXiv:1904.04565, 2019.

[48] F. Krzakala, M. Mézard, F. Sausset, Y. Sun, and L. Zdeborová. Probabilistic reconstruction in compressed
sensing: algorithms, phase diagrams, and threshold achieving matrices. Journal of Statistical Mechanics:
Theory and Experiment, 2012(08):P08009, 2012.

[49] U. Kamilov, S. Rangan, M. Unser, and A. K. Fletcher. Approximate message passing with consistent
parameter estimation and applications to sparse learning. In Advances in Neural Information Processing
Systems, pages 2438–2446, 2012.

[50] P. Hartman. Ordinary Differential Equations: Second Edition. Classics in Applied Mathematics. Society for
Industrial and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104), 1982.

[51] E. Gardner and B. Derrida. Optimal storage properties of neural network models. Journal of Physics A:

Mathematical and general, 21(1):271, 1988.

[52] J. Barbier, N. Macris, M. Dia, and F. Krzakala. Mutual information and optimality of approximate

message-passing in random linear estimation. arXiv preprint arXiv:1701.05823, 2017.

[53] M. Opper and W. Kinzel. Statistical mechanics of generalization. In Models of neural networks III, pages

151–209. Springer, 1996.

21

[54] J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse super-

position codes. IEEE Transactions on Information Theory, 63:4894–4927, 2017.

[55] M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

[56] M. Bayati, M. Lelarge, A. Montanari, et al. Universality in polytope phase transitions and message passing

algorithms. The Annals of Applied Probability, 25(2):753–822, 2015.

22

Supplementary material

A Proof details for Theorem 3.1

A.1 The Nishimori property in Bayes-optimal learning

We first state an important property of the Bayesian optimal setting (that is when all hyper-parameters of the
problem are assumed to be known), that is used several times, and is often refered to as the Nishimori identity.

Proposition A.1 (Nishimori identity). Let (X, Y ) ∈ Rn1 × Rn2 be a couple of random variables. Let k ≥ 1 and
let X (1), . . . , X (k) be k i.i.d. samples (given Y ) from the conditional distribution P (X = · |Y ), independently of
every other random variables. Let us denote (cid:104)−(cid:105) the expectation operator w.r.t. P (X = · |Y ) and E the expectation
w.r.t. (X, Y ). Then, for all continuous bounded function g we have

E(cid:104)g(Y, X (1), . . . , X (k))(cid:105) = E(cid:104)g(Y, X (1), . . . , X (k−1), X)(cid:105) .

(31)

Proof. This is a simple consequence of Bayes formula. It is equivalent to sample the couple (X, Y ) according
to its joint distribution or to sample first Y according to its marginal distribution and then to sample X
conditionally to Y from its conditional distribution P (X = · |Y ). Thus the (k + 1)-tuple (Y, X (1), . . . , X (k))
is equal in law to (Y, X (1), . . . , X (k−1), X). This proves the proposition.

As a first application of Proposition A.1 we prove the following Lemma which is used in the proof of the

upper bound Proposition 5.7.
Lemma A.2 (Positivity of some matrices). The matrices ρ, E(cid:104)Q(cid:105) and ρ − E(cid:104)Q(cid:105) are positive definite, i.e. in S +
K
In the application the Gibbs bracket is (cid:104)−(cid:105)n,t,(cid:15).
Proof. The statement for ρ follows from its definition (in Theorem 3.1). Note for further use that we also
have ρ = 1
ilwil(cid:48) in matrix notation we have Q =
n
n
i=1 W ∗
1
i w
n

i )(cid:124)]. Since by definition Qll(cid:48) ≡ 1
n
E[W ∗
i=1 W ∗
(cid:124)
i . An application of the Nishimori identity shows that

i (W ∗

n

.

(cid:80)

which is obviously in S +

E(cid:104)Q(cid:105) =

n

1
n

i=1
(cid:88)
K. Finally we note that

n

1
n

i=1 (cid:16)
(cid:88)

(cid:80)

1
n

n

i=1
(cid:88)

=

1
n

(cid:124)
i (cid:105)]
(cid:17)

n

i=1
(cid:88)

E[ρ − (cid:104)Q(cid:105)] =

E[W ∗

i (W ∗
i )

] − E[(cid:104)wi(cid:105)(cid:104)w

(cid:124)

E[(W ∗

i − (cid:104)wi(cid:105))((W ∗
i )

(cid:124) − (cid:104)w

(cid:124)
i (cid:105))]

E(cid:104)W ∗

i w

(cid:124)
i (cid:105) =

E[(cid:104)wi(cid:105)(cid:104)w

(cid:124)
i (cid:105)]

(32)

where the last equality is proved by an application of the Nishimori identity again. This last expression is
obviously in S +

K, i.e. E(cid:104)Q(cid:105) ∈ S +

K(ρ).

A.2 Setting in the Hamiltonian language

We set up some notations which will shortly be useful. Let uy(x) ≡ ln Pout(y|x). Here x ∈ RK and y ∈ R.
We will denote by ∇uy(x) the K-dimensional gradient w.r.t. x, and ∇∇(cid:124)uy(x) the K × K matrix of second
derivatives (the Hessian) w.r.t. x. Moreover ∇Pout(y|x) and ∇∇(cid:124)Pout(y|x) also denote the K-dimensional
gradient and Hessian w.r.t. x. We will also use the matrix identity

∇∇(cid:124)

uYµ(x) + ∇uYµ(x)∇(cid:124)

uYµ(x) =

∇∇(cid:124)Pout(Yµ|x)
Pout(Yµ|x)

.

(33)

23

t ∈ Rn×K, X ∈ Rm×n, V ∈ Rm×K,
Finally we will use the matrices w ∈ Rn×K, u ∈ Rm×K, Yt ∈ Rm, Y (cid:48)
W ∗ ∈ Rn×K and U ∗ ∈ Rm×K. Like in sec. 5 we adopt the convention that all underlined vectors are
K-dimensional, like e.g. uµ, U µ, V µ and Y (cid:48)

t,i.

It is convenient to reformulate the expression of the interpolating free entropy fn,(cid:15)(t) in the Hamiltonian

language. We introduce an interpolating Hamiltonian:

Ht(w, u; Yt, Y (cid:48)

t , X, V ) ≡ −

uYt,µ(st,µ) +

(cid:107)Y (cid:48)

t,i − R1(t)1/2 wi(cid:107)2
2

(34)

m

µ=1
(cid:88)

1
2

n

i=1
(cid:88)

where recall that

n

st,µ ≡

1 − t
n

(cid:114)

Xµiwi +

R2(t) V µ +

tρ − R2(t) + 2snIK×K uµ .

(35)

i=1
(cid:88)
The expression of Ht(W ∗, U ∗; Yt, Y (cid:48)
(35) replaced by St,µ given by (15). The average free entropy (18) at time t then reads

(cid:112)

(cid:112)

t , X, V ) is similar to (34), but with w replaced by W ∗ and st,µ given by

fn,(cid:15)(t) ≡

E ln

1
n

dP0(w)

Rn×K

(cid:90)

(cid:90)
µl/2 and dP0(w) =

Rm×K

Du e−Ht(w,u;Yt,Y (cid:48)

t ,X,V )

(36)

where Du =
in the simplest manner it is fruitful to represent the expectations over W ∗, U, Y, Y (cid:48) explicitly as integrals:

K
l=1 dwil. To develop the calculations

n
i=1 P0(wi)

l=1(2π)−1/2e−u2

m
µ=1

K

(cid:81)

(cid:81)

fn,(cid:15)(t) =

EX,V

dYtdY (cid:48)

t dP0(W ∗)DU ∗e−Ht(W ∗,U ;Yt,Y (cid:48)

t ,X,V ) ln

dP0(w)Du e−Ht(w,u;Yt,Y (cid:48)

t ,X,V ).

(37)

(cid:81)

(cid:81)

1
n

(cid:90)

(cid:90)

A.3 Free entropy variation: Proof of Proposition 5.2

The proof provided here follows very closely the one in [11] for the case K = 1, so we are more brief and
refer to this paper for more details. We first prove that for all t ∈ (0, 1)

dfn,(cid:15)(t)
dt

= −

Tr

E

1
2

(cid:68)

m

1
n

(cid:104)(cid:16)

+

1
2

µ=1
(cid:88)
Tr[r(t)(q(t) − ρ)] −

An
2

,

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

W ∗

i w

(cid:124)
i − q(t)

n

1
n

(cid:17)(cid:16)

i=1
(cid:88)

n,t,(cid:15)

(cid:17)(cid:69)

(38)

where

Tr

An = E
(cid:104)

(cid:104)

1
√
n

m

µ=1
(cid:88)

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

1
√
n

n

(cid:16)

i=1
(cid:88)

(W ∗

i (W ∗
i )

(cid:124) − ρ)

ln Zn,(cid:15)(t)

.

(39)

(cid:105)

Once this is done, we show that An goes to 0 as n → ∞ uniformly in t ∈ [0, 1] in order to conclude the proof.

The Hamiltonian (34) t-derivative evaluated at the ground-truth matrices is given by

dHt
dt

(W ∗, U ∗;Yt, Y (cid:48)

t , X, V ) = −

∇(cid:124)

uYt,µ(St,µ)

m

µ=1
(cid:88)
∇(cid:124)

m

= −

Tr

dSt,µ
dt

µ=1
(cid:88)

(cid:104)

uYt,µ(St,µ)
(cid:105)

n

dSt,µ
dt

−

dR1(t)1/2
dt

n

−

Tr

i=1
(cid:88)

(cid:104)(cid:16)

i=1 (cid:16)
(cid:88)
dR1(t)1/2
dt

(cid:124)

(cid:17)

W ∗
i

(Y (cid:48)

t,i − R1(t)1/2W ∗
i )

(Y (cid:48)

t,i − R1(t)1/2W ∗

i )W ∗(cid:124)

i

(40)

(cid:105)

1
n

(cid:17)(cid:105)

(cid:124)

(cid:17)

24

(where we used that R1(t) is symmetric). The t-derivative of fn,(cid:15)(t) thus reads, for 0 < t < 1,

(W ∗, U ∗; Yt, Y (cid:48)

t , X, V ) ln Zn,(cid:15)(t)

−

(w, u; Yt, Y (cid:48)

t , X, V )

(41)

T1

(cid:123)(cid:122)

E

1
n

dHt
dt

(cid:68)

(cid:105)

(cid:125)

(cid:124)

T2

(cid:123)(cid:122)

.

n,t,(cid:15)

(cid:69)

(cid:125)

First, we note that T2 = 0. This is a direct consequence of the Nishimori identity Proposition A.1:

(w, u; Yt, Y (cid:48)

t , X, V )

(W ∗, U ∗; Yt, Y (cid:48)

t , X, V ) = 0 .

(42)

=

1
n

E dHt
dt

n,t,(cid:15)

(cid:69)

We now compute T1. Starting from (40) and considering the first term only (recall also the expression (15)

dfn,(cid:15)(t)
dt

= −

E

1
n

dHt
dt

(cid:104)

(cid:124)

T2 =

E

1
n

dHt
dt

(cid:68)

for St,µ),

E

Tr

(cid:104)

dSt,µ
dt

∇(cid:124)

(cid:104)
+

d
dt

(cid:112)

uYt,µ(St,µ)

R2(t)V µ +

ln Zn,(cid:15)(t)

−

Tr

= E
2
(cid:80)
(cid:104)
tρ − R2(t) + 2snIK×K U ∗
(cid:112)
µ

(cid:104)(cid:110)

(cid:105)

(cid:105)
d
dt

(cid:112)

n
i=1 XµiW ∗
i
n(1 − t)
∇(cid:124)
(cid:111)

uYt,µ(St,µ)
(cid:105)

ln Zn,(cid:15)(t)

.

(43)

(cid:105)

We then compute the first line of the right-hand side of (43). By Gaussian integration by parts w.r.t. Xµi (recall
hypothesis (H3)), and using the identity (33), we find after some algebra

1

−

2

n(1 − t)

(cid:112)

= −

n

i=1
(cid:88)
1
n

(cid:104)

Tr

E

Tr

(cid:104)
E

(cid:104)

Tr

1
2

−

(cid:104)
1
E
2

n

i=1
(cid:88)
1
n

(cid:68)

(cid:104)

i=1
(cid:88)

XµiW ∗

i ∇(cid:124)

n

W ∗

i W

(cid:124)
i

ln Zn,(cid:15)(t)

uYt,µ(St,µ)
(cid:105)
∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

(cid:105)

W ∗

i w

(cid:124)

i ∇uYt,µ(St,µ)∇(cid:124)

uYt,µ(st,µ)

ln Zn,(cid:15)(t)

(cid:105)

(cid:105)

.

n,t,(cid:15)

(cid:105)(cid:69)

(44)

(45)

Similarly for the second line of the right hand side of (43), we use again Gaussian integrations by parts but
this time w.r.t. V µ, U ∗
µ which have i.i.d. N (0, 1) entries. This calculation has to be done carefully with the
help of the matrix identity

d
dt

M (t) =

M (t)

(cid:112)

d

M (t)
dt

d

+

M (t)
dt

(cid:112)

(cid:112)

M (t)

(cid:112)

for any M (t) ∈ S +
t
0 (ρ − q(s))ds, as well as the identity (33), we reach after some algebra

K, and the cyclicity and linearity of the trace. Applying (45) to M (t) equal to

t
0 q(s)ds and

(cid:82)

(cid:82)

E

Tr

(cid:104)(cid:16)
ρ

(cid:104)
= E
Tr
(cid:104)

(cid:104)

d
d
R2(t)V µ +
dt
dt
∇∇(cid:124)Pout(Yt,µ|Sµ,t)
(cid:112)
Pout(Yt,µ|Sµ,t)

(cid:112)

(cid:105)

tρ − R2(t) + 2snIK×K U ∗
µ

∇(cid:124)

ln Zn,(cid:15)(t)

uYµ(Sµ,t)
(cid:105)
uYt,µ(sµ,t)

(cid:105)

(cid:17)

q(t)∇uYt,µ(Sµ,t)∇(cid:124)

ln Zn,(cid:15)(t)

(cid:105)

Tr

+ E
(cid:104)
(cid:68)
R1(t))(cid:124)(Y (cid:48)

.

n,t,(cid:15)

(cid:105)(cid:69)

(46)

As seen from (40), (41) it remains to compute E[Tr[( d
dt
that Y (cid:48)
i = Z(cid:48)
t,i −
one obtains

R1(t)W ∗

(cid:112)

] ln Zn,(cid:15)(t)]. Recall
t,i −
i ∼ N (0, IK×K). Using Gaussian integration by parts as well as the identity (45)

i

(cid:112)

R1(t)W ∗

i )W ∗(cid:124)

(cid:112)
d
dt

(cid:124)

(cid:17)

E

Tr

R1(t)

(Y (cid:48)

t,i −

R1(t)W ∗

i )W ∗(cid:124)

i

ln Zn,(cid:15)(t)

= −Tr

R1(t)

ρ − E(cid:104)W ∗

j wj(cid:105)n,t,(cid:15)

.

(47)

(cid:104)

(cid:104)(cid:16)

(cid:112)

(cid:112)

(cid:105)

(cid:105)

(cid:104)(cid:112)

(cid:0)

(cid:1)(cid:105)

25

Finaly the term T1 is obtained by putting together (43), (44), (46) and (47).

It now remains to check that An → 0 as n → +∞ uniformly in t ∈ [0, 1]. The proof from [11] (Appendix
C.2) can easily be adapted so we give here just a few indications for the ease of the reader. First one notices that

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yµ|St,µ)

E

(cid:104)

(cid:12)
(cid:12)
(cid:12)

W ∗, {St,µ}m

µ=1

=

dYµ∇∇(cid:124)

Pout(Yt,µ|St,µ) = 0 ,

(cid:105)

(cid:90)

so that by the tower property of the conditional expectation one gets

E

Tr

1
√
n

m

µ=1
(cid:88)

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

1
√
n

n

i=1
(cid:88)

(W ∗

i (W ∗
i )

(cid:124) − ρ)

= 0 .

(cid:104)

(cid:104)

(cid:16)
Next, one shows by standard second moment methods that E[(ln Zn,(cid:15)(t)/n − fn,(cid:15)(t))2] → 0 as n → +∞
uniformly in t ∈ [0, 1] (see [11] for the proof at K = 1, that generalizes straightforwardly for any finite K).
Then, using this last fact together with (49), and under hypotheses (H1), (H2), (H3), an easy application of the
(cid:3)
Cauchy-Schwarz inequality implies An → 0 as n → +∞ uniformly in t ∈ [0, 1]. This ends the proof.

(cid:17)(cid:105)(cid:105)

(48)

(49)

A.4 Technical lemmas

Lemma A.3 (Cauchy-Lipschitz Theorem and Liouville Formula). Let

F :

[0, 1] × (0, +∞)d → [0, +∞)d
(cid:55)→ F (t, z)

(t, z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

be a continuous, bounded function. Assume that F admits continuous partial derivatives ∂F
∂zi
domain of definition. Then, for all (cid:15) ∈ (0, +∞)d, the Cauchy problem

(i = 1, . . . , d) on its

y(0) = (cid:15)

and

y(cid:48)(t) = F

t, y(t)

(50)

admits a unique solution t (cid:55)→ y(t, (cid:15)). For all t ∈ [0, 1], the mapping zt : (cid:15) (cid:55)→ y(t, (cid:15)) is a diffeomorphism of class
C1, from (0, +∞)d to zt

. Moreover the determinant J(zt)((cid:15)) of the Jacobian of zt at (cid:15) verifies

(0, +∞)d

(cid:0)

(cid:1)

(cid:0)

(cid:1)

J(zt)((cid:15)) = det

∂yi
∂(cid:15)j

d

t

∂Fi
∂zi

(cid:16)(cid:16)

d
i=1

∂Fi
∂zi

i,j

(cid:17)

0
(cid:16) (cid:90)
(cid:17)
≥ 0 then J(zt)((cid:15)) ≥ 1 for all (cid:15).

i=1
(cid:88)

(cid:0)

(cid:17)

(cid:1)

= exp

s, y(s, (cid:15))

ds

.

(51)

Thus, in particular, if in addition

Proof. The existence and uniqueness of the solution of (50) follows from the classical Cauchy-Lipschitz
Theorem. The solution is indeed defined on all the segment [0, 1] because F is bounded.

(cid:80)

Theorem 3.1 from Chapter 5 in [50] gives that y admits continuous partial derivatives ∂y
∂(cid:15)i

for i = 1, . . . , d,

and Corollary 3.1 from Chapter 5 in the same reference states the Liouville formula (51).

By the Cauchy-Lipschitz Theorem, two solutions of y(cid:48)(t) = F

that are equal at some t ∈ [0, 1]
are equal everywhere. This implies that the mapping zt : (cid:15) (cid:55)→ y(t, (cid:15)) is injective, for all t ∈ [0, 1]. Since y
admits continuous partial derivatives in (cid:15)i, i = 1, . . . , d, we obtain that zt is of class C1 on (0, +∞)d. Now,
the equation (51) gives that J(zt)((cid:15)) > 0 for all (cid:15) ∈ (0, +∞)d. The local inversion Theorem gives then that zt
is a C1 diffeomorphism.

t, y(t)

(cid:0)

(cid:1)

26

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

Lemma A.4 (Boundedness of an overlap fluctuation). Under hypothesis (H2) one can find a constant C(ϕ, K, ∆) <
+∞ (independent of n, t, (cid:15)) such that for any Rn ∈ S +
K

we have

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124) − Rn

≤ 2Tr(R2

n) + α2C(ϕ, K, ∆).

(52)

We note that the constant remains bounded as ∆ → 0 and diverges as K → +∞.

Proof. It is easy to see that for symmetric matrices A, B we have Tr(A − B)2 ≤ 2(TrA2 + TrB2). Therefore

F

n,t,(cid:15)

(cid:69)

(cid:13)
(cid:13)
(cid:13)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124) − Rn

F

n,t,(cid:15)

≤ 2Tr(R2

Tr

n) + 2E
(cid:68)

1
n

(cid:16)

µ=1
(cid:88)

m

(cid:69)

(cid:13)
(cid:13)
(cid:13)
(cid:124)
∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

.

n,t,(cid:15)

(cid:17)

(cid:69)

(53)

In the rest of the argument we bound the second term of the r.h.s. Using the triangle inequality and then
Cauchy-Schwarz we obtain

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

(cid:124)
∇uYt,µ(st,µ)∇uYt,µ(St,µ)

F

n,t,(cid:15)

2

(cid:69)

(cid:13)
(cid:13)
(cid:13)

≤ E

1
n2

(cid:68)

m

µ=1
(cid:16)
(cid:88)
2
(cid:124)(cid:107)2

(cid:107)∇uYt,µ(st,µ)(cid:107)2(cid:107)∇uYt,µ(St,µ)

m

≤ E

1
n2

(cid:68)

(cid:16)

µ=1
(cid:88)

.

n,t,(cid:15)

(cid:17)

(cid:69)

From the random representation of the transition kernel,

(cid:107)∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124)(cid:107)F

n,t,(cid:15)

(cid:17)

(cid:69)

and thus

uYt,µ(s) = ln Pout(Yt,µ|x) = ln

dPA(aµ)

√

1
2π∆

e− 1

2∆ (Yt,µ−ϕ(x,aµ))2

(cid:90)

∇uYt,µ(x) =

dPA(aµ)(Yt,µ − ϕ(x, aµ))∇ϕ(x, aµ)e− 1
dPA(aµ)e− 1
2∆ (Yt,µ−ϕ(x,aµ))2

(cid:82)

2∆ (Yt,µ−ϕ(x,aµ))2

where ∇ϕ is the K-dimensional gradient w.r.t. the first argument x ∈ RK. From the observation model we get
|Yt,µ| ≤ sup |ϕ| +
∆|Zµ|, where the supremum is taken over both arguments of ϕ, and thus we immediately
obtain for all s ∈ RK

√

(cid:82)

(cid:107)∇uYt,µ(x)(cid:107) ≤ (2 sup |ϕ| +

∆|Zµ|) sup (cid:107)∇ϕ(cid:107) .

√

From (57) and (54) we see that it suffices to check that

m2
n2

(cid:2)(cid:0)

E

(2 sup |ϕ| + |Zµ|)2(sup (cid:107)∇ϕ(cid:107))2

≤ C(ϕ, K, ∆)

2

(cid:1)

(cid:3)

where C(ϕ, K, ∆) < +∞ is a finite constant depending only on ϕ, K, and ∆. This is easily seen by expanding
all squares and using that m/n → α. This ends the proof of Lemma A.4.

Lemma A.5 (Properties of ψP0). ψP0

is defined as the free entropy of the first auxiliary channel (3). We have,

(54)

(55)

(56)

(57)

27

for any r ∈ S +
K

:

ψP0(r) ≡ E ln

dwP0(w)eY

(cid:124)
0 r1/2w− 1

2 w(cid:124)rw.

RK

(cid:90)

Then ψP0

is convex and differentiable on S +
K

, with ∇ψP0(r) ∈ S +
K

for any r ∈ S +
K

.

Proof. Note that ψP0 is related to the mutual information I(W0; Y0) via the relation I(W0; Y0) = −ψP0(r) +
K
2 + 1
2 Tr[rρ]. It is then a known result (see [42, 43, 44]) that the derivative ∇rI(W0; Y0) is given by the
matrix-MMSE, i.e. ∇rI(W0; Y0) = 1
]). Using
2
E [(w − (cid:104)w(cid:105))(w − (cid:104)w(cid:105))(cid:124)], which is clearly a
the Nishimori identity Prop.A.1, we can write it as ∇rψP0(r) = 1
2
positive matrix. It is also known (see for instance Lemma 4 of [42]), that I(W0; Y0) is a concave function of r,
which implies that ψP0 is convex, which ends the proof.

]. This implies that ∇rψP0(r) = 1

2 (ρ − E[(cid:104)w(cid:105) (cid:104)w(cid:105)

E [(cid:104)w(cid:105) (cid:104)w(cid:105)

(cid:124)

(cid:124)

Lemma A.6 (Properties of ΨPout). Recall that ΨPout
(4). More precisely, for q ∈ S +

K(ρ), we have:

is defined as the free entropy of the second auxiliary channel

ΨPout(q) ≡ E ln

Y0|q1/2V + (ρ − q)1/2w

.

dw

e− 1
2 (cid:107)w(cid:107)2
(2π)K/2 Pout

RK

(cid:90)

Then ΨPout

is continuous and convex on S +

K(ρ). Also, ∇ΨPout(q) ∈ S +
Proof. The continuity and differentiability of ΨPout is easy, and exactly similar to the first part of the proof of
Proposition 18 of [11]; it just follows from the hypothesis (H2) which allows to use continuity and differentiation
under the expectation, because all the domination hypotheses are easily verified.

K

.

(cid:0)
(cid:101)
K(ρ), and twice differentiable inside S +

(cid:1)

One can compute the gradient and Hessian matrix of ΨPout(q), for q inside S +

K(ρ), using Gaussian
integration by parts and the Nishimori identity. The calculation is tedious and essentially follows the steps of
Y0|x). We define the average (cid:104)−(cid:105)sc (where sc stands for
Proposition 11 of [11]. Recall that u
“scalar channel”) as

(x) ≡ ln Pout(

(cid:101)Y0

(cid:104)g(w)(cid:105)sc ≡

RK DwPout(

(cid:82)

RK DwPout(
(cid:101)

(cid:82)

(cid:101)
Y0|(ρ − q)1/2w + q1/2V )g(w)
Y0|(ρ − q)1/2w + q1/2V )

,

for any continuous bounded function g. One arrives at:
(cid:101)

∇ΨPout(q) =

(ρ − q)1/2W ∗ + q1/2V

(ρ − q)1/2w + q1/2V

E

1
2

∇u

(cid:101)Y0

(cid:68)

(cid:16)

(cid:124)

.

sc

(cid:17)

(cid:69)

Note that this gradient is actually a symmetric matrix of size K × K, as it is a gradient w.r.t. q, which is itself
a matrix of size K. The Hessian ∇∇(cid:124)ΨPout with respect to q is thus a 4-tensor. One can compute in the same
way:

∇∇(cid:124)

ΨPout(q) =

Y0|(ρ − q)1/2w + q1/2V )

Y0|(ρ − q)1/2w + q1/2V )

∇∇(cid:124)Pout(
Pout(

(cid:101)

E

1
2

(cid:104)(cid:16)(cid:68)
−

(cid:68)

(cid:16)

∇u

(cid:101)Y0

(ρ − q)1/2W ∗ + q1/2V
(cid:101)

∇u

(ρ − q)1/2w + q1/2V

(cid:124)

⊗2

.

sc

(cid:17)

(cid:69)

(cid:17)

(cid:105)

In this expression, ⊗2 means the “tensorized square” of a matrix, i.e. for any matrix M of size K × K, M ⊗2 is
a 4-tensor with indices M ⊗2
= Ml0l1Ml2l3. From this expression, it is clear that the Hessian of ΨPout is
always positive, when seen as a matrix with rows and columns in SK, and thus ΨPout is convex, which ends
the proof of Lemma A.6.

l0l1l2l3

∇u

(cid:101)Y0

(cid:17)

(cid:16)

sc

(cid:69)

(cid:101)Y0

(cid:16)

(cid:17)

(58)

(59)

(60)

28

B Replica calculation

Our goal here is to provide an heuristic derivation of the replica formula of Theorem 3.1 using the replica
method, a powerful non-rigorous tool from statistical physics of disordered systems [13, 14]. This computation
is necessary to properly “guess” the formula that we then prove using the adaptive interpolation method. The
reader interested in the replica approach to neural networks and the commitee machine is invited to look as
well to some of the classical papers [51, 36, 20, 21, 19, 5].

The replica trick makes use of the formula, for a random variable x ∈ Rn and a strictly positive function

fn : Rn → R that depends on n:

lim
n→∞

1
n

E ln fn = lim
p→0+

lim
n→∞

1
np

ln Ef p
n.

(61)

Note that the inversion of the two limits here is non-rigorous. Computing the moments Ef p can often
be done for integers p ∈ N, and one can conjecture from it its value for every p > 0, before taking the limit
p → 0+ in (61) by analytical continuation of the value for integer p.

In our calculation, we will use this formula to compute the free entropy of our system, f ≡ limn→∞ fn.

We will thus need the moments of the partition function, for integer p:

EZ p

n = E

dw

P0 ({ wil

K
l=1

= E

dwa

P0 ({ wa
il

K
l=1

n

i=1
(cid:89)

Rn×RK





(cid:90)
p

Rn×RK





a=1 (cid:90)
(cid:89)

n

i=1
(cid:89)

p

m

(cid:9)

µ=1
(cid:89)

(cid:1)

Pout

Yµ



m


Pout

(cid:9)

µ=1
(cid:89)

(cid:1)

K

p

,

(cid:41)

l=1








K

Xµiwa
il

Xµiwil

n

1
√
n

(cid:40)

(cid:12)
(cid:12)
(cid:12)





Yµ

(cid:40)

(cid:12)
(cid:12)
(cid:12)

i=1
(cid:88)

1
√
n

n

i=1
(cid:88)

(cid:41)

l=1

.









The outer expectation is done over Xµi ∼ N (0, 1), w(cid:63) and Y . Writing w(cid:63) as w0 we have:

EZ p

n = EX

dY

dwa

P0

{wa

il}K
l=1

Rm

(cid:90)

Rn×RK

a=0 (cid:34) (cid:90)
(cid:89)

m

n

i=1
(cid:89)

(cid:0)
1
√
n

n

(cid:1)
Xµiwa
il

K



(cid:41)

l=1

.
(cid:35)

i=1
(cid:88)

×

Pout

Yµ



µ=1
(cid:89)

(cid:40)

(cid:12)
(cid:12)
(cid:12)


To perform the average over X, we notice that, since it is an i.i.d. standard Gaussian matrix, then for every
il follows a Gaussian multivariate distribution, with zero mean. This naturally

n
i=1 Xµiwa
a, µ, l, Za
leads to introduce its covariance tensor, which is equal to:

µl ≡ n−1/2



(cid:80)

EZa

µlZb

νl(cid:48) = δµνΣ al
bl(cid:48)

= δµνQal
bl(cid:48),

Qal

bl(cid:48) ≡

wa

ilwb

il(cid:48).

1
n

n

i=1
(cid:88)

(62)

(63)

For every a, b, Qa
functions for fixing Q, we arrive at :

b ∈ RK×K is the overlap matrix, and Σ is of size size (p + 1)K × (p + 1)K. Introducing δ

E [Z p

n] =

dQar
ar

dQar
br(cid:48)

Iprior({Qar

br(cid:48)}) × Ichannel({Qar

br(cid:48)})

,

(64)

R
(cid:89)(a,r) (cid:90)

R
(cid:89){(a,r);(b,r(cid:48))} (cid:90)

(cid:2)

(cid:3)

29

with:

Iprior({Qar

br(cid:48)}) =

dwaP0(wa)

p

a=0 (cid:20)(cid:90)
(cid:89)

Rn×K

p

Ichannel({Qar

br(cid:48)}) =

dY

dZa

Rm×K

n

1
n

δ

Qal

bl(cid:48) −

wa

ilwb
il(cid:48)

,

(cid:21)

(cid:32)


(cid:89){(a,l);(b,l(cid:48))}
p

Pout(Y |Za)e− m

i=1
(cid:88)
2 ln det Σ− mK(p+1)

2

(cid:33)

ln 2π

Rm

(cid:90)

exp

−

1
2





a=0 (cid:90)
(cid:89)
m

a=0
(cid:89)
µl(cid:48)(Σ−1) al

µlZb
Za

µ=1
(cid:88)

a,b
(cid:88)

l,l(cid:48)
(cid:88)

.

bl(cid:48)



By Fourier expanding the delta functions in Iprior, and performing a saddle-point method, one obtains:

in which (recall α ≡ limn→∞ m/n) :

lim
n→∞

1
n

ln E [Z p

n] = extrQ, ˆQ

H(Q, ˆQ)
(cid:105)
(cid:104)

,

H(Q, ˆQ) ≡

p

1
2

a=0
(cid:88)

l,l(cid:48)
(cid:88)

Qal
al

ˆQal

al −

1
2

a(cid:54)=b
(cid:88)

l,l(cid:48)
(cid:88)

Qal

bl(cid:48) ˆQal

bl(cid:48) + ln I + α ln J,

in which we defined:

p

I ≡

RK

a=0 (cid:90)
(cid:89)

p

J ≡

dy

R

(cid:90)

RK

a=0 (cid:90)
(cid:89)

dwaP0(wa) exp

−

ˆQal

al(cid:48)wa

l wa

l(cid:48) +

p

1
2

a=0
(cid:88)

l,l(cid:48)
(cid:88)





dZa
(2π)K(p+1)/2

Pout(y|Za)
det Σ

√

exp

−

1
2

1
2

p

a(cid:54)=b
(cid:88)

l,l(cid:48)
(cid:88)
K

ˆQal

bl(cid:48)wa

l wb
l(cid:48)

,




l(cid:48)(Σ−1) al

l Zb
Za





a,b=0
(cid:88)

l,l(cid:48)=1
(cid:88)

.

bl(cid:48)



Our goal is to express H(Q, ˆQ) as an analytical function of p, in order to perform the replica trick. To do
so, we will assume that the extremum of H is attained at a point in Q, ˆQ space such that a replica symmetry
property is verified. More concretely, we assume:

∃Q0 ∈ RK×K s.t

∀a ∈ [|0, p|] ∀(l, l(cid:48)) ∈ [|1, K|]2 Qal
∀(a < b) ∈ [|0, p|]2 ∀(l, l(cid:48)) ∈ [|1, K|]2 Qal

al(cid:48) = Q0
ll(cid:48),
bl(cid:48) = qll(cid:48),

∃q ∈ RK×K s.t

and samely for ˆQ0 and ˆq. Note that Q0 is by definition a symmetric matrix, while q is also symmetric by our
assumption of replica symmetry. Under this ansatz, we obtain:

H(Q0, ˆQ0, q, ˆq) =

Tr[Q0 ˆQ0] −

Tr[q ˆq] + ln I + α ln J.

(73)

p + 1
2

p(p + 1)
2

Remains now to compute an expression for I and J that is analytical in p, in order to take the limit p → 0+.
This can be done easily, using the identity, for any symmetric positive matrix M ∈ RK×K and any vector
x ∈ RK: exp (x(cid:124)(M/2)x) =
, in which Dξ is the standard Gaussian measure on RK.

ξ(cid:124)M 1/2x

RK Dξ exp

(cid:82)

(cid:0)

(cid:1)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

30

We obtain:

I =

Dξ

dw P0(w) exp

−

(cid:124)

w

( ˆQ0 + ˆq)w + ξ

ˆq1/2w

(cid:124)

p+1

,

1
2

RK

(cid:90)

RK

(cid:20)(cid:90)

(cid:20)
y|(Q0 − q)1/2Z + q1/2ξ

(cid:21)(cid:21)

p+1

.

J =

dy

Dξ

dZPout

R

(cid:90)

RK

(cid:90)

RK

(cid:20)(cid:90)

(cid:110)

(cid:111)(cid:21)

Our assumptions must be consistent in the sense that extrQ, ˆQ
In the p → 0+ limit, one easily gets J = 1 and I =
optimal overlap parameters satisfy ˆQ0 = 0 and Q0
the free entropy:

(cid:82)

limp→0+ H(Q, ˆQ)
2 w(cid:124) ˆQ0w0
(cid:105)
− 1
. This implies that the
ll(cid:48) = EP0 [wlwl(cid:48)]. In the end, we obtain the final formula for

= 0 (because EZ 0

RK dw P0(w) exp

n = 1).

(cid:104)

(cid:104)

(cid:105)

lim
n→∞

fn = extrq,ˆq

−

Tr[q ˆq] + IP + αIC

,

1
2

IP ≡

(cid:26)
Dξ

RK

(cid:90)

RK

(cid:90)

(cid:27)
1
−
2

(cid:20)

dw0P0(w0) exp

(cid:124)
(w0)

ˆqw0 + ξ

(cid:124)

ˆq1/2w0

× ln

dwP0(w) exp

−

RK

(cid:20)(cid:90)
DZ0Pout

(cid:20)
y|(Q0 − q)1/2Z0 + q1/2ξ

(cid:21)
(cid:124)
ˆqw + ξ

(cid:124)

w

1
2

ˆq1/2w

,

(cid:21)(cid:21)

× ln

RK

(cid:20)(cid:90)

(cid:110)
DZPout

(cid:110)

y|(Q0 − q)1/2Z + q1/2ξ

.

(cid:111)

(cid:111)(cid:21)

IC ≡

dy

Dξ

R

(cid:90)

RK

(cid:90)

RK

(cid:90)

A known ambiguity of the replica method is that its result is given as an extremum, here over the set
S +
K(Q0) of positive symmetric matrices, such that (Q0 − q) is also a positive matrix. It is easy to show that
this form gives back the form given in Theorem 3.1, by assuming that this extremum is realized as a supˆq inf q.
Note that in the notations of Theorem 3.1, Q0 is denoted ρ and ˆq is denoted R.

C Generalization error

We detail here two different possible definitions of the generalization error, and how they are related in our
system. Recall that we wish to estimate W ∗ from the observation of ϕout(XW ∗). In the following, we denote
E for the average over the (quenched) W ∗ and the data X, and (cid:104)−(cid:105) for the Gibbs average over the posterior
distribution of W . One can naturally define the Gibbs generalization error as:

(cid:15)Gibbs
g

≡

EW ∗,X

[ϕout (XW ) − ϕout (XW ∗)]2

,

and define the Bayes-optimal generalization error as:

(cid:15)Bayes
g

≡

EW ∗,X

(cid:104)ϕout (XW )(cid:105) − ϕout (XW ∗)

1
2

1
2

(cid:10)

(cid:2)(cid:0)

(cid:11)

2

.

(cid:1)

(cid:3)

(74)

(75)

(76)

(77)

(78)

31

Using the Nishimori identity A.1, one can show that:

(cid:15)Bayes
g

=

EX,W ∗

ϕout (XW ∗)2

+

EX,W ∗

(cid:104)ϕout (XW )(cid:105)2

(cid:105)
− EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) ,

(cid:104)

(cid:105)

=

EX,W ∗

ϕout (XW ∗)2

−

EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) .

1
2

1
2

(cid:104)

(cid:104)

1
2

1
2

(cid:105)

Using again the Nishimori identity one can write:

(cid:15)Gibbs
g

= EX,W ∗

ϕout (XW ∗)2

− EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) ,

g

(cid:105)

= 2(cid:15)Bayes
g

(cid:104)
which shows that (cid:15)Gibbs
. Note finally that since the distribution of X is rotationally invariant, the
quantity EX [ϕout (XW ∗) ϕout (XW )] only depends on the overlap q ≡ W (cid:124)W ∗. As the overlap is shown to
concentrate under the Gibbs measure by Proposition 5.3, and as we expect that the value it concentrates on
is the optimum q∗ of the replica formula (such fact is proven, e.g., for random linear estimation problems in
[52]), the generalization error can itself be evaluated as a function of q∗. Examples where it is done include
[53, 3, 19, 11].

C.1 The generalization error at K = 2

In this subsection alone, we go back to the K = 2 case, instead of the K → ∞ limit. From the definition of
the generalization error (see sec. C), one can directly give an explicit expression of this error in the K = 2
case. Recall our committee-symmetric assumption on the overlap matrix, which here reads

q =

qd + qa
2
qa
2

(cid:32)

qa
2
qd + qa

2 (cid:33)

.

For concision, we denote here sign(x) = σ(x). One obtains from (78):

1
2

− 2(cid:15)Bayes,K=2

g

=

Dx σ [σ(x1) + σ(x2)]

× σ

σ

(

+ qd)x1 +

x2 + x3

1 −

qa
2

q2
a
2

− qaqd − q2
d

(cid:35)

(cid:114)

qa
2

(cid:40)

(cid:34)

R4

(cid:90)

qa
2

+σ

x1 + (

+ qd)x2 − x3

qa
2





qa(qd + qa
2 )
2 − qaqd − q2
d

a

1 − q2

+ x4

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:113)

(1 − q2

d)(1 − (qa + qd)2)
2 − qaqd − q2
d

a

1 − q2

.










Note that one could possibly simplify this expression by using an appropriate orthogonal transformation on x.
These integrals were then computed using Monte-Carlo methods to obtain the generalization error in the left
and middle plots of Fig. 2.

(79)

D The large K limit in the committee symmetric setting

We consider the large K limit2 for a sign activation function, and for different priors on the weights. Since the
output is a sign, the channel is simply a delta function. We assume a committee symmetric solution, i.e. the

2A similar limit has been derived in the context of coding with sparse superposition codes [54]. There the large input alphabet
limit of the mutual information is considered after the thermodynamic limit n → ∞ corresponding to the large codeword limit in this
coding context.

32

(cid:124)
matrices q and ˆq (q and R in the notations of Theorem 3.1) are of the type q = qd1K + qa
K, with the unit
K 1K1
vector 1K = (1)K
l=1, and similarly for ˆq. In the large K limit, this scaling of the order parameters is natural.
Indeed, assume that the covariance of the prior is Q0 = 1K (Q0 = ρ in the notations of Theorem 3.1). Since
both q and (Q0 − q) are assumed to be positive matrices, it is easily shown to imply that 0 ≤ qd ≤ 1 and
0 ≤ qa + qd ≤ 1.

D.1 Large K limit for sign activation function

In the following, we consider Q0 = σ21K. We are interested here in computing the leading order term in IC of
(76). Note that replacing σ2 by 1 in this equation only amounts to replacing q by q/σ2, so we can assume σ2 = 1
RK Dξ IC(y, ξ) log IC(y, ξ), with
without loss of generality. We (abusively) write IC in (76) as IC =
the definition

y=±1

(cid:80)

(cid:82)

IC(y, ξ) ≡

DZPout

y|(Q0 − q)1/2Z + q1/2ξ

.

(80)

RK

(cid:90)

(cid:110)

(cid:111)

the remarks above). Note that this implies that q1/2 =
√
√

Here, we assumed a sign activation function and no noise, as well as a particular form for Q0 and q (see
√
(cid:124)
qa+qd−
K and that (Q0 − q)1/2 =
K
(cid:124)
K. All together, this gives the following explicit expression for IC(y, ξ) :
1K1

1 − qd1K +

qd1K +

1−qa−qd−

1K1

1−qd

qd

√

√

√

K

IC(y, ξ) ≡

DZ

RK

(cid:90)

× δ

y − sign

(cid:40)

K

1
√
K

(cid:34)

sign

1 − qdZl +

1 − qa − qd −

1 − qd

l=1
(cid:88)
(cid:124)
K Z
Introducing a new variable w ≡ 1
√
K
another variable u being the argument of the outer sign function in the previous equations, one obtains:

and a Fourier-transform of the then-introduced delta function, as well as

(cid:16)(cid:112)

(cid:112)

(cid:112)

(cid:17)

(cid:20)

(cid:124)
1
KZ
K

+ (q1/2ξ)l

.

(cid:21)(cid:35)(cid:41)

IC(y, ξ) =

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

eiw ˆw+iuˆuδy,sign(u)

Dze

−i ˆw z√

K e

(cid:20)

(cid:20)(cid:114)

− iˆu√
K

sign

z+

(cid:21)

1−qa−qd
1−qd

−1

w√
K

+ 1√

1−qd

(q1/2ξ)l

(cid:21)

.

K

×

R

l=1 (cid:90)
(cid:89)

λl(w, ξ) ≡

1 − qa − qd
1 − qd

− 1
(cid:21)

(cid:20)(cid:114)

w
√
K

+

√

1
1 − qd

(q1/2ξ)l,

IC(y, ξ) =

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

eiw ˆw+iuˆuδy,sign(u)

Dze

−i ˆw z√

− iˆu√
K

K e

sign[z+λl(w,ξ)]

.

K

R

l=1 (cid:90)
(cid:89)

Denote

such that

33

(81)

(82)

(83)

(84)

(85)

For 1 ≤ l ≤ K, one can rewrite the factorized integral in the last expression of IC(y, ξ) as:

IC(y, ξ) =

eiw ˆw+iuˆuδy,sign(u)

J (λl(w, ξ), ˆw, ˆu) ,

J (λl(w, ξ), ˆw, ˆu) ≡ e

Dze

z(λl−i ˆw√
K

)

− iˆu√
e
K

sign[z]

.

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

λ2
l
2 +iλl

−

ˆw√
K

R

(cid:90)

K

l=1
(cid:89)

We abusively dropped the dependency of λl on (w, ξ). Note the following identity:

F (α, iβ) ≡

Dzeαz+iβ sign(z) = eα2/2

cos β + i sin β ˆH(α)

,

(cid:104)

(cid:105)

R

(cid:90)

√

with ˆH(x) = erf(x/

2). Using it in our previous expressions, we obtain:

J(λl, ˆw, ˆu) = e− 1

2K ˆw2

cos

− i sin

ˆH

λl − i

ˆu
√
K (cid:19)

ˆu
√
K (cid:19)

(cid:20)
Note that by our committee-symmetry assumption, we have λl(w, ξ) = λl,0(ξ) + 1√
K
λ1 typically of order 1 when K → ∞:

(cid:18)

(cid:18)

(cid:18)

ˆw
√
K (cid:19)(cid:21)

.

λ1(w, ξ) with λl,0 and

λl,0(ξ) ≡

λ1(w, ξ) ≡

ξl,

qd
1 − qd
1 − qa − qd
1 − qd

(cid:114)

(cid:20)(cid:114)

− 1

w +

(cid:21)

(cid:20)(cid:114)

qa + qd
1 − qd

−

qd
1 − qd (cid:21)

(cid:114)

(cid:124)
1
Kξ
√
K

.

Expanding J(λl, ˆw, ˆu) as K → ∞, we obtain using the known development of the error function:

J(λl, ˆw, ˆu) = e− 1

2K ˆw2

1 −

− i ˆH [λl,0(ξ)]

ˆu2
2K

(cid:34)

ˆu
√
K

− i

ˆu [λ1(w, ξ) − i ˆw]
K

2
π

(cid:114)

λl,0(ξ)2

e−

2 + O(K−3/2)

.

(cid:35)

This yields (putting back the (w, ξ) dependency):

K

l=1
(cid:89)

J [λl(w, ξ), ˆw, ˆu)] = e− 1

2 ˆw2

exp

−

− iˆuS1 − i

ˆu(λ1 − i ˆw)Γ0 +

ˆu2
2

(cid:34)

2
π

(cid:114)

1
2

ˆu2S2 + O(K−1/2)
(cid:35)

,

(86)

in which we defined the following quantities, that only depend on ξ (recall (84))

wξ(ξ) ≡

ξl,

Γ0(ξ) ≡

e− 1

2 λl,0(ξ)2

,

S1(ξ) ≡

ˆH(λl,0(ξ)),

S2(ξ) ≡

ˆH(λl,0(ξ))2.

1
√
K

1
√
K

K

l=1
(cid:88)
K

l=1
(cid:88)

1
K

1
K

K

l=1
(cid:88)
K

l=1
(cid:88)

A detailed calculation actually shows that the previous expansion of (86) is valid up to O(K−1), and not only
O(K−1/2). Recall also (81), in which one can now readily perform the integration over all variables w, ˆw, u, ˆu
to obtain (dropping the ξ dependency in wξ, Γ0, S1, S2):

IC(y, ξ) = H

−y

+ O(K−1),

(87)

S1 +

(cid:113)





√

qd

√

2
π wξΓ0
(cid:113)
1 − S2 − 2

√

qd+qa−
1−qd
qa
1−qd

π Γ2
0





34

∞

x Dz = 1

in which H(x) ≡
. Note that all quantities wξ, Γ0, S1, S2 only depend on ξ via
its empirical measure, which implies that the integration over ξ ∈ RK will be tractable. We compute it in the
following, using theoretical physics methods. We denote the quantity that appears in (87) as a function of
wξ, Γ0, S1, S2:

1 − erf(x/

2)

(cid:82)

(cid:3)

(cid:2)

2

√

G(y, wξ, Γ0, S1, S2) ≡ H

−y

S1 +

(cid:113)





√

qd

√

2
π wξΓ0
(cid:113)
1 − S2 − 2

√

qd+qa−
1−qd
qa
1−qd

π Γ2
0

.





Introducing once again delta functions and their Fourier transforms for wξ, Γ0, S1, S2, we write, starting from
(87):

IC =

DξIC(y, ξ) log IC(y, ξ)

RK

y=±1 (cid:90)
(cid:88)

=

y=±1 (cid:90)
(cid:88)

dwξd ˆwξ
2π

dΓ0dˆΓ0
2π

dS1d ˆS1
2π

dS2d ˆS2
2π

eiw ˆw+iΓ0 ˆΓ0+iS1 ˆS1+iS2 ˆS2 G(y, wξ, Γ0, S1, S2)

× log G(y, wξ, Γ0, S1, S2)

Dξe−i ˆwwξ(ξ)−iˆΓ0Γ0(ξ)−i ˆS1S1(ξ)−i ˆS2S2(ξ)

+ O(K−1).

(88)

RK

(cid:20)(cid:90)

(cid:21)

The integral over ξ in (88) can be computed in the limit K → ∞:

Λ ≡

Dξe−i ˆwwξ(ξ)−iˆΓ0Γ0(ξ)−i ˆS1S1(ξ)−i ˆS2S2(ξ)

RK

(cid:90)

= 
(cid:90)

R

Dξ exp 

−i



The large K expansion yields




ˆwξ
√
K

− i

−
ˆΓ0e

qd
2(1−qd) ξ2
K

− i

ˆS1 ˆH

qd
1−qd

ξ

ˆS2 ˆH

qd
1−qd

ξ

√
(cid:104)(cid:113)
K

− i

(cid:105)

(cid:104)(cid:113)
K

2

K

(cid:105)











Λ = exp

−

ˆw2 − iˆΓ

1 − qd − ˆS1 ˆwE

1
2

(cid:40)

ξ ˆH
(cid:20)

(cid:18)(cid:114)

qd
1 − qd

ξ

(cid:19)(cid:21)

2

qd
1 − qd

ξ

1 + i ˆS2
ˆS2

E

ˆH

+ O(K−1) .

(cid:34)

(cid:19)

(cid:18)(cid:114)

(cid:35) (cid:41)

(cid:19)

(cid:112)

−

1
2

(cid:18)

The expectations are taken with respect to a real variable ξ ∼ N (0, 1). These expectations are known by
properties of the error function:

One can now compute the integrals over the “hat” variables in (88). Denote Γf

0 ≡

2(1−qd)
π

, and Sf

2 ≡

(cid:113)

E

ˆH
(cid:34)

E

ξ ˆH

qd
1 − qd

ξ

2

(cid:35)

(cid:19)

(cid:18)(cid:114)

2
π

=

arcsin qd ,

qd
1 − qd

ξ

2qd
π

.

=

(cid:19)(cid:21)

(cid:114)

(cid:20)

(cid:18)(cid:114)

35

2
π arcsin qd. This yields:

IC =

DwDS1 G

R2

(cid:90)

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

2qd
π

, Sf
2

(cid:114)

log G

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

(cid:33)

2qd
π

(cid:114)

, Sf
2

.

(cid:33)

(89)

.





(90)

Note that

G

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

2qd
π

, Sf
2

(cid:33)

(cid:114)

= H

−y



2
π

(cid:114)

√

arcsin qd − qdS1 + w

qd + qa

√

1 − 2

π (qa + arcsin qd)

Making the change of variable Snew
reaches:

1 = S1 + w

√

√

qd+qa
arcsin qd−qd


in (89), and defining γ ≡ 2

(cid:113)

π (qa + arcsin qd), one

IC =

DxH

yx

log H

yx

γ
1 − γ

γ
1 − γ

+ O(K−1).

(cid:20)

(cid:114)

(cid:21)

(cid:20)

(cid:114)

(cid:21)

R

y=±1 (cid:90)
(cid:88)

The two values of y contribute in the same way, which finally yields:

IC = 2

DxH

x

log H

x

R

(cid:90)

(cid:20)

(cid:114)

(cid:21)

(cid:20)

(cid:114)

(cid:21)

γ
1 − γ

γ
1 − γ

+ O(K−1).

Note that the parameter γ is naturally bounded to the interval [0, 1] by the conditions 0 ≤ qd ≤ 1 and
0 ≤ qa + qd ≤ 1.

D.2 The Gaussian prior

The prior part IP of the free entropy of (76) is very easy to evaluate in the Gaussian prior setting. We consider
a prior with covariance matrix Q0 = IK (we can simply rescale q by q/σ2 in the final expression for a finite
variance Q0 = σ2IK as we already described). Performing the Gaussian integration in IP in (76) yields:

IP =

ˆqd +

ˆqa −

log(1 + ˆqd) −

log (1 + ˆqd + ˆqa) .

(91)

K
2

1
2

K − 1
2

1
2

D.3 The fixed point equations

From the definition of the free entropy (76) and the expansions for IP and IC obtained in (90) and (91), one
obtains the fixed point equations after having extremized over ˆqd and ˆqa (recall that α ≡ lim m

n ):

∂qa [IG(qd, qa) + αIC(qd, qa)] = 0,
∂qd [IG(qd, qa) + αIC(qd, qa)] = 0,

(92)

(93)

with IG(qd, qa) defined as:

IG(qd, qa) ≡

[qa + Kqd] −

1
2

IC(qd, qa) = 2

DxH

x

R

(cid:90)

(cid:20)

(cid:114)

K − 1
2
γ
1 − γ

(cid:21)

log

(cid:20)
log H

1
1 − qd (cid:21)
x

γ
1 − γ

,

(cid:21)

(cid:20)

(cid:114)

−

log

1
2

1
1 − qa − qd (cid:21)

,

(cid:20)

36

and recall that γ ≡ 2

π (qa + arcsin qd).

The fixed point equations (92), (93) have different behaviors depending on the scaling of α with the hidden

layer size K. We detail these different behaviors in the following paragraphs.

D.3.1 Regime α = oK→∞(K)

In this regime (which in particular contains the case in which α stays of order 1 when K → ∞), the fixed
point equations (92), (93) can be simplified as:

qd = 0,
qa = 2α(1 − qa) ∂IC
∂qa

.

(cid:40)

D.3.2 Regime α = ΘK→∞(K)

In this regime, we naturally define
solutions of the fixed point equations (92), (93) must satisfy the following scaling : qa + qd = 1 − χ
χ ≥ 0 a reaching a finite value when K → ∞. The fixed point equations in terms of χ and qd read:

α will remain of order 1. One can show that the
K , with

αK ≡ α/K, such that

(cid:101)

(cid:101)

qd = 2(1 − qd)

χ−1 = 2

α ∂IC
∂qa

.

1√

1−q2
d

− 1

˜α ∂IC
∂qa

,

(cid:19)

(cid:18)






Note that the State Evolution (SE) computation of Figure 2 was performed by solving the fixed point

(cid:101)

equations (94) and (95) (depending on the regime of α).

It is easy to show that (95) always admit what we call a non-specialized
The stability of the qd = 0 solution:
solution, i.e. a solution with qd = 0. This solution stops to be optimal in term of the free energy at a finite
αspec (cid:39) 7.65. However, one can show that this solution will remain linearly stable for every
α. Actually, it is
linearly stable in the much broader regime α = o(K2). Going back to the initial formulation of the fixed point
(cid:101)
equations (92),(93), and adding the correct time indices to iterate them, one obtains:

(cid:101)

with F and G defined as:

(cid:0)

(cid:1) (cid:0)

d, qt
F (qt
a)
1 + F (qt
d, qt
a)

,

qt+1
d =

qt+1
a =

1 + F (qt

d, qt
a)

d, qt

a)G(qt

d, qt
a)

d, qt
G(qt
a)
1 + F (qt

,

(cid:1)

F (qd, qa) ≡

[∂qdIC − ∂qaIC] ,

G(qd, qa) ≡

∂qaIC −
(cid:20)

1
K

∂qdIC

.

(cid:21)

2α
K − 1
2αK
K − 1

We focus on the behavior of (96) around qd = 0. Given our previous expansion of IC in the K → ∞
|qd=0 →K→∞ 0, which means the qd = 0 solution

limit, and (98), one easily sees that for α = oK→∞(K2), ∂F
∂qd
always remains linearly stable.

However, assume now that α = Θ(K2). Performing a similar calculation to the one shown in sec. D.1,

(94)

(95)

(96)

(97)

(98)

(99)

37

one can show the following expansion:

IC(qd, qa) = I

(0)
C (qd, qa) +

(1)
C (qd, qa) + O

I

1
K

1
K2

.

(cid:18)

(cid:19)

The term of ∂F
∂qd
seen from (98).

|qd=0 arising from I

(1)
C will thus have a possibly non-zero contribution in the K → ∞ limit, as

To summarize, the non-specialized solution always remains linearly stable in the large K limit at least for
α (cid:28) K2. This implies that in this regime, Approximate Message Passing can not escape the non-specialized
fixed point to find the specialized solution, as seen in Fig. 3. For α of order larger than K2, one would have to
(1)
C in order to check that ∂F
|qd=0 (cid:54)= 0 to show that the non-specialized solution is indeed
explicitly compute I
∂qd
linearly unstable. This tedious calculation is left for future work.

D.4 The generalization error at large K

Recall the definition of the generalization error in (78). From the remarks of section C, one can compute it at
large K by applying the same techniques used to compute the channel integral IC in sec. D.1. One obtains
after a tedious, yet straightforward, calculation:

(cid:15)Bayes
g

=

(cid:15)Gibbs
g

=

arccos

(qa + arcsin qd)

+ O(K−1).

(100)

1
2

1
π

2
π

(cid:20)

(cid:21)

This expression is the one used in the computation of the generalization error in the left panel of Fig. 3.

E Linear networks show no specialization

An easy yet interesting case is a linear network with identical weights in the second layer and a final output
function σ : R → R, i.e a network in which ϕout(h) = σ
. For clarity, in this section, we
decompose the channel as Pout(y|ϕout(Z)) for Z ∈ RK instead of Pout(y|Z). We will compute the channel
integral IC of the replica solution (76). For simplicity, we assume that Q0 = 1K the identity matrix (i.e w has
RK DξIC(y, ξ) log IC(y, ξ). One
identity covariance matrix under P0). Note that (76) gives IC as IC =
can easily derive:

K
l=1 hl

1√
K

R dy

(cid:80)

(cid:1)

(cid:0)

(cid:82)

(cid:82)

IC(y, ξ) = e− 1

2 ξ(cid:124)(1K −q)−1qξ

eiuˆuPout(y|σ(u))

dudˆu
2π

R2

(cid:90)

×

RK

(cid:90)

dZ
(2π)K det(1K − q)

e− 1

2 Z(cid:124)(1K −q)−1Z+Z(cid:124)X(ˆu,xi),

in which we denoted X(ˆu, xi) (cid:44) (1K − q)−1q1/2ξ − iˆu√
K
integration over Z can be done, as well as the integration over ˆu:

(cid:112)

1K, with the unit vector 1K = (1)K

l=1. The inner

IC(y, ξ) =

1
1 − 1

(cid:124)
R
Kq1K (cid:90)
K 1

du
√
2π

Pout(y|σ(u)) exp 

−

(cid:113)

(cid:124)
u − 1√
Rq1/2ξ
1
K
(cid:124)
1 − 1
Kq1K
K 1

(cid:16)
2

2

.






(cid:17)

(cid:1)




(cid:0)

So we can formally write the total dependency of IC(y, ξ) on ξ and on q as

IC(y, ξ) = IC

y,

1
√
K

(cid:124)
Kq1/2ξ,
1

(cid:124)
Kq1K
1

1
K

.

(cid:19)

(cid:18)

38

Note that we have the following identity, for any fixed vector x ∈ RK and smooth real function F :

DξF (x

ξ) =

√

(cid:124)

duF (u)e− u2

2x(cid:124)x .

1
2πx(cid:124)x

R

(cid:90)

RK

(cid:90)

In the end, if we denote Γ(q) (cid:44) 1

(cid:124)
Kq1K, we have:

K 1

IC =

dy

dve

2Γ(q) IC(v, y) log IC(v, y),

1

− v2

IC(v, y) ≡

du Pout(y|σ(u)) exp

−

1
2 (1 − Γ(q))

(u − v)2

.

(cid:21)

(cid:20)

R

(cid:90)

2πΓ(q)
1
(cid:112)
2π(1 − Γ(q))

R

(cid:90)

R

(cid:90)

(cid:112)

Note that by hypothesis, both q and 1K − q are positive matrices, so 0 ≤ Γ(q) ≤ 1. As these equations show,
IC only depends on Γ(q) = K−1
l,l(cid:48) qll(cid:48). From this one easily sees that extremizing over q implies that the
optimal ˆq satisfies ˆqll(cid:48) = ˆq/K for some real ˆq. Subsequently, all qll(cid:48) are also equal to a single value, that we
can denote q

K . This shows that this network never exhibits a specialized solution.

(cid:80)

(101)

(102)

(103)

F Update functions and AMP derivation

AMP can be seen as Taylor expansion of the loopy belief-propagation (BP) approach [13, 14, 55], similar to
the so-called Thouless-Anderson-Palmer equation in spin glass theory [35]. While the behaviour of AMP can
be rigorously studied [17, 18, 56], it is useful and instructive to see how the derivation can be performed in
the framework of belief-propagation and the cavity method, as was pioneered in [36, 38] for the single layer
problem. The derivation uses the Generalized AMP notations of [16] and follows closely the one of [26].

F.1 Definition of the update functions

Let’s consider the distributions probabilities Qout and Q0, closely related to the inference problems eq. (3) and
eq. (4):

Qout(z; ω, y, V ) ≡

e− 1

2 (z−ω)(cid:124)V −1(z−ω)Pout(y|z); Q0(W ; Σ, T ) ≡

P0(W )e− 1

2 W (cid:124)Σ−1W +T (cid:124)Σ−1W .

1
ZPout

1
ZP0

We define the update functions gout, ∂ωgout, fw and fc, which will be useful later in the algorithm:

gout(ω, y, V ) ≡ ∂ω log(ZPout) = V −1EQout [z − ω] ,

(cid:124)
∂ωgout(ω, y, V ) = V −1EQout [(z − ω)(z − ω)
fw(Σ, T ) ≡ ∂Σ−1T log ZP0 = EQ0[W ] ,
fc(Σ, T ) ≡ ∂Σ−1T fw = EQ0[W W

(cid:124)

] − fwf

(cid:124)
w .

] − V −1 − goutg

(cid:124)
out ,

Note that gout is the mean of V −1(z − ω) with respect tor Qout and fw the mean of Q0.

F.2 Derivation of the Approximate Message Passing algorithm

F.2.1 Relaxed BP equations

Lets consider a set of messages {mi→µ, ˜mµ→i}i=1..n,µ=1..m on the bipartite factor graph corresponding to
our problem Fig. 4. These messages correspond to the marginal probabilities of Wi if we remove the edges
i → µ or µ → i. The belief propagation (BP) equations (or sum-product equations) can be formulated as the
following [14, 55], where Wi = (wil)l=1..K ∈ RK:

39

Pout (Yµ|{XµWi}n
µ = 1...m

i=1)

˜mµ→i

Wi ∈ RK
i = 1...n

P0(Wi)
i = 1...n

mi→µ

Figure 4: Factor graph representation of the committee machine (for n = 4 and m = 3). The variable (circle)
Wi ∈ RK needs to satisfy a prior constraint (square) P0 and a constraint accounting for the fully connected layer,
that correlates all the variables together.

i→µ(Wi) =

P0(Wi)

˜mt

ν→i(Wi) ,

1
Zi→µ

m

k(cid:54)=µ
(cid:89)

µ→i(Wi) =

dWjPout

Yµ|

XµjWj

mt

j→µ(Wj) .

1
Zµ→i (cid:90)

n

j(cid:54)=i
(cid:89)

1
√
n

n

j=1
(cid:88)









The term inside Pout can be decouple using its K-dimensional Fourrier transform



mt+1



˜mt

Pout

Yµ|

XµjWj

=

1
√
n

n

j=1
(cid:88)





1
(2π)K/2

RK

(cid:90)

dξ exp

(cid:124)

iξ









1
√
n

n

j=1
(cid:88)





XµjWj

ˆPout(Yµ, ξ)

.









Injecting this representation in the BP equations, (104) becomes

˜mt

µ→i(Wi) =

dξ ˆPout(Yµ, ξ) exp

XµiWi

1
(2π)K/2Zµ→i (cid:90)
×

n

RK

RK

j(cid:54)=i (cid:90)
(cid:89)

(cid:124)

dWjmt

j→µ(Wj) exp

iξ

XµjWj)

,

iξ

(cid:124) 1
√
n

(cid:124) 1
√
n

(cid:18)

(cid:18)

≡Ij

(cid:123)(cid:122)

(cid:19)

(cid:19)

(cid:125)

and we define the mean and variance of the messages

ˆW t

j→µ ≡

dWjmt

j→µ(Wj)Wj ,

ˆCt

j→µ ≡

dWjmt

j→µ(Wj)WjW

(cid:124)
j − ˆW t

j→µ( ˆW t

(cid:124)
j→µ)

.

RK

(cid:90)

RK

(cid:90)






In the limit n → ∞ the term Ij can be easily expanded and expressed using ˆW and ˆC

(104)

(105)

Ij =

dWjmt

j→µ(Wj) exp

iξ

Wj)

(cid:39) exp

(cid:124) Xµj√
n

(cid:18)

(cid:19)

RK

(cid:90)

Xµj√
n

i
(cid:32)

(cid:124) ˆW t

ξ

j→µ −

(cid:124) ˆCt

ξ

j→µ , ξ

X 2
µj
n

1
2

,

(cid:33)

40

and finally using the inverse Fourier transform, we obtain

˜mt

µ→i(Wi) (cid:39)

dzPout(Yµ, z)

dξe−iξ(cid:124)zeiXµiξ(cid:124)Wi

1
(2π)KZµ→i (cid:90)
×

RK
n

RK

(cid:90)
(cid:124) ˆW t

ξ

j→µ −

X 2
µj
n

1
2

(cid:124) ˆCt

ξ

j→µξ

(cid:33)

exp

i

Xµj√
n

(cid:32)

j(cid:54)=i
(cid:89)

dzPout(Yµ, z)

dξe−iξ(cid:124)zeiXµiξ(cid:124)Wie

iξ(cid:124)

n
(cid:80)
j(cid:54)=i

Xµj√
n

ˆW t

j→µ

2 ξ(cid:124)

− 1
e

X2
µj
n

n
(cid:80)
j(cid:54)=i

ˆCt

j→µξ

=

=

1
(2π)KZµ→i (cid:90)
1
(2π)KZµ→i (cid:90)

RK

RK

dzPout(Yµ, z)

RK

(cid:90)

(2π)K
det(V t
iµ)

− 1
e
2

(cid:115)

(cid:16)

z−

Xµi√
n

Wi−ωt
iµ

(cid:17)(cid:124)

(V t

iµ)−1(cid:16)

z−

Xµi√
n

Wi−ωt
iµ

(cid:17)

,

(cid:125)

≡Hiµ

(cid:123)(cid:122)

1
n

n

j(cid:54)=i
(cid:88)

ωt
iµ ≡

Xµj ˆW t

j→µ ,

V t
iµ ≡

X 2
µj

ˆCt

j→µ .

(106)

1
√
n

n

j(cid:54)=i
(cid:88)

where we defined the mean and variance, depending on the node i

(cid:124)

Again, in the limit n → ∞, the term Hiµ can be expanded:

Hiµ (cid:39) e− 1

2 (z−ωt

(cid:124)
iµ)

(V t

iµ)−1(z−ωt

iµ)

Xµi√
n

1 +

(cid:32)

W

(cid:124)
i (V t

iµ)−1(z − ωt

iµ) −

W

(cid:124)
i (V t

iµ)−1Wi

X 2
µi
n

1
2

X 2
µi
n

1
2

+

W

(cid:124)
i (V t

iµ)−1(z − ωt

iµ)(z − ωt

(cid:124)
iµ)

(V t

iµ)−1Wi

.

(cid:33)

Gathering all pieces, the message ˜mµ→i can be expressed using definitions of gout and ∂ωgout

˜mt

µ→i(Wi) ∼

W

(cid:124)
i gout(ωt

iµ, Yµ, V t

iµ) +

W

(cid:124)
i goutg

(cid:124)
out(ωt

iµ, Yµ, V t

iµ)Wi+

1 +

Xµi√
n

X 2
µi
n

1
2

(cid:124)
i ∂ωgout(ωt

iµ, Yµ, V t

iµ)Wi

1 + W

(cid:124)
i Bt

µ→i +

W

(cid:124)
i Bt

µ→i(Bt

µ→i)

(cid:124)

(Wi) −

W

(cid:124)
i At

µ→iWi

1
2

(cid:27)

(cid:41)

1
2

µ→i)
(2π)K exp

(cid:115)

−

1
2

(cid:18)

(cid:0)

W

(cid:124)
i − (At

µ→i)−1Bt

µ→i

(cid:124)

At

µ→i

W

(cid:124)
i − (At

µ→i)−1Bt

µ→i

,

(cid:1)

(cid:0)

(cid:19)

(cid:1)

with the following definitions of Aµ→i and Bµ→i:

Bt

µ→i ≡

gout(ωt

iµ, Yµ, V t

iµ), At

µ→i ≡ −

∂ωgout(ωt

iµ, Yµ, V t
iµ)

(107)

Xµi√
n

X 2
µi
n

Using the set of BP equations (104), we can finaly close the set of equations only over {mi→µ}iµ:

mt+1

i→µ(Wi) =

1
Zi→µ

P0(Wi)

m

ν(cid:54)=µ (cid:115)
(cid:89)

det(At
ν→i)
(2π)K e− 1

2 (Wi−(At

ν→i)−1Bt

(cid:124)
ν→i)

At

ν→i(Wi−(At

ν→i)−1Bt

ν→i).

In the end, computing the mean and variance of the product of gaussians, the messages are updated using

1
Zµ→i (cid:40)
X 2
µi
n

W

1
Zµ→i (cid:26)
det(At

1
2

=

=

41

(108)

(109)

(110)

fw and fc:

ˆW t+1

i→µ = fw(Σt

µ→i, T t

µ→i) ,

ˆCt+1

i→µ = fc(Σt

µ→i, T t

µ→i) ,






m

ν(cid:54)=µ
(cid:80)

µ→i

(cid:32)



µ→i ≡

Σt

T t
µ→i ≡ Σt


At

ν→i

(cid:33)

m

(cid:32)

ν(cid:54)=µ
(cid:80)

−1

,

Bt

ν→i

.

(cid:33)

Summary of the Relaxed BP set of equations:

In the end, using eq .(105,106,107, 108), relaxed BP equations can be written as the following set of equations:

Xµj√
n

ˆW t

j→µ

X 2
µj
n

ˆCt

j→µ

=

=

n

j(cid:54)=i
(cid:80)
n

j(cid:54)=i
(cid:80)
Xµi√

Bt

µ→i =

At

µ→i = −

iµ, Yµ, V t
iµ)

n gout(ωt
X 2
n ∂ωgout(ωt
µi

iµ, Yµ, V t
iµ)

ωt
iµ

V t
iµ






Σt

µ→i =

−1

m

(cid:32)

ν(cid:54)=µ
(cid:80)

At

ν→i

(cid:33)

T t
µ→i = Σt

µ→i

ˆW t+1

i→µ = fw(Σt

m

Bt

ν→i

(cid:33)

(cid:32)

ν(cid:54)=µ
(cid:80)
µ→i, T t

µ→i)

ˆCt+1

i→µ = fc(Σt

µ→i, T t

µ→i)






F.2.2 Approximate Message Passing algorithm

The relaxed BP algorithm uses O(n2) messages. However all the messages depend weakly on the target node.
On a tree, the missing message is negligible, that allows us to expand the previous relaxed BP equations (109)
to make appear the Onsager term at a previous time step, and reduce the number of messages to O(n). We
define the following estimates and parameters based on the complete set of messages:

Xµj√
n

ˆW t

j→µ

X 2
µj
n

ˆCt

j→µ

n

j=1
(cid:80)
n

j=1
(cid:80)

ωt

µ ≡



V t
µ ≡


Σt

i ≡

−1

m

At

ν→i

ν=1

(cid:18)
(cid:80)

i ≡ Σt
T t
i


(cid:18)

m

ν=1
(cid:80)

(cid:19)

Bt

ν→i

(cid:19)

Let’s now expand the previous messages eq. (109), making appear these new target-independent messages:

• Σt

µ→i

• T t

µ→i

Σt

µ→i =

At

ν→i

=

m

ν(cid:54)=µ
(cid:88)









=

IK×K −

−1

m

−1

m

At

ν→i − At

µ→i

=

(cid:32)

ν=1
(cid:88)

−1

(cid:33)

m

−1

At

ν→i

(cid:33)

At

µ→i

At

ν→i

(cid:33)

(cid:32)

ν=1
(cid:88)





m

(cid:32)

ν=1
(cid:88)





ν=1
(cid:88)
−1

At

ν→i 

IK×K −


IK×K − Σt

=

(cid:32)

ν=1
(cid:88)

m

−1

−1

At

ν→i

(cid:33)

At

µ→i



iAt

µ→i

−1

Σt

i (cid:39) Σt


i + O


1
n

(cid:18)

(cid:19)

(cid:39)IK×K +Σt
(cid:0)

iAt

µ→i+O(n−1)

(cid:1)

(cid:124)

(cid:123)(cid:122)

(cid:125)

m

Bt

ν→i

=

Σt

i + O

1
n

m

Bt

ν→i − Bt

µ→i

(cid:33)

(cid:18)

(cid:19)(cid:19) (cid:32)

ν=1
(cid:88)

µ→i = Σt
T t

µ→i 

ν(cid:54)=µ
(cid:88)

iBt
i − Σt

= T t


µ→i + O

(cid:18)

1
n

(cid:18)

(cid:19)

42

• ˆW t+1
i→µ

• V t
µ

• ωt
µ

ˆW t+1

i→µ = fw(Σt

µ→i, T t

µ→i) = fw

Σt

i, T t

i − Σt

iBt

µ→i

+ O

1
n

(cid:18)

(cid:19)

(cid:0)

Σt

iBt

µ→i

(cid:1)

(cid:39) fw

Σt

i, T t
i

−

dfw
dT

= fw

(cid:0)

Σt

i, T t
i
= ˆW t+1
(cid:0)
i

(cid:1)

(cid:1)

−

Σt
i

(cid:0)

(cid:1)

(Σt
(cid:12)
(cid:12)
−1
(cid:12)
(cid:12)

i,T t
i )
Σt

fc

Σt
i

i, T t
i
= ˆCt+1
i

(cid:1)

(cid:0)

(cid:39)

Xµi√
n

(cid:123)(cid:122)
(cid:124)
= ˆW t+1
i −

(cid:125)
Xµi√
n

(cid:124)
−1 ˆCt+1

(cid:125)
(cid:123)(cid:122)
µ, Yµ, V t
igout(ωt
i Σt

Σt
i

Bt

µ→i

µ,Yµ,V t
µ)

gout(ωt
(cid:124) (cid:123)(cid:122) (cid:125)
µ) + O

1
n

(cid:18)

(cid:19)

(cid:0)

(cid:1)

• ˆCt+1
i→µ
Let’s denote for convenience, E =

Σt
i

−1 ˆCt+1

i Σt

igout(ωt

µ, Yµ, V t

µ). Then

ˆCt+1
i→µ = EQ0

ˆW t

i→µ( ˆW t

= EQ0

ˆW t

i −

(cid:0)

(cid:1)

(cid:124)
i→µ)
Xµi√
n

E

(cid:105)

− EQ0

ˆW t
(cid:104)
Xµi√
i −
n

ˆW t

i→µ

(cid:105)

E

i→µ

EQ0
(cid:124)

ˆW t
(cid:104)
− EQ0

(cid:124)

(cid:20)(cid:18)
ˆW t

(cid:124)
i ( ˆW t
i )

(cid:19) (cid:18)
− EQ0

= EQ0

ˆW t
i

EQ0

(cid:19)
(cid:21)
ˆW t
i

(cid:124)

+ O

(cid:105)

(cid:104)

(cid:105)

(cid:104)

(cid:105)

(cid:104)

(cid:104)

(cid:105)
ˆW t

i −

(cid:20)
1
√
n

(cid:18)

(cid:19)

Xµi√
n

E

EQ0

ˆW t
(cid:20)
i + O

(cid:21)
= ˆCt+1

1
√
n

(cid:18)

(cid:19)

i −

Xµi√
n

E

(cid:124)

(cid:21)

• gout(ωt

iµ, Yµ, V t
iµ)

gout(ωt

iµ, Yµ, V t

iµ) = gout

ωt
µ −

ˆW t

i→µ, Yµ, V t

µ −

= gout

µ, Yµ, V t
ωt
µ

−

Xµi√
n

∂gout
∂ω

µ, Yµ, V t
ωt
µ

= gout

µ, Yµ, V t
ωt
µ

−

µ, Yµ, V t
ωt
µ

ˆW t

Xµi√
n

∂gout
∂ω

X 2
µi
n

ˆCt

i→l

(cid:33)

+O

1
n

(cid:18)

(cid:19)

(cid:1)

= ˆW t

ˆW t

i→µ
(cid:16) 1√

i +O
(cid:124) (cid:123)(cid:122) (cid:125)
i + O

(cid:17)

n

1
n

(cid:18)

(cid:19)

(cid:1)

(cid:0)

(cid:0)

Xµi√
n

(cid:1)

(cid:1)

(cid:32)

(cid:0)

(cid:0)

V t
µ =

ˆCt

i→l =

ˆCt

i + O

X 2
µi
n

n

i=1
(cid:88)

X 2
µi
n

n

i=1
(cid:88)

1
n3/2

(cid:18)

(cid:19)

ωt
µ =

ˆW t

i→µ =

ˆW t

i − Xµi

Σt−1
i

−1 ˆCt

i Σt−1
i

gout(ωt−1

µ , Yµ, V t−1

µ

) + O

1
n

(cid:18)

(cid:19)(cid:19)

Σt−1
i

−1 ˆCt

(cid:0)
i Σt−1
i

(cid:1)
gout(ωt−1

µ , Yµ, V t−1

µ

) + O

1
n3/2

(cid:18)

(cid:19)

Xµi√
n

Xµi√
n

n

i=1
(cid:88)
n

i=1
(cid:88)

=

ˆW t

i −

n

Xµi√
n

(cid:18)

n

i=1
(cid:88)
X 2
µi
n

i=1
(cid:88)

(cid:0)

(cid:1)

43

−1

•

Σt
i

(cid:0)

(cid:1)
Σt
i

−1

=

(cid:0)

(cid:1)

• T t
i

m

µ=1
(cid:88)

m

µ=1
(cid:88)

At

µ→i = −

X 2

µi∂ωgout(ωt

iµ, Yµ, V t

iµ) = −

X 2

µi∂ωgout(ωt

µ, Yµ, V t

µ) + O

m

µ=1
(cid:88)

1
n3/2

(cid:18)

(cid:19)

Bt

µ→i

= Σt
i

m

Xµi√
n

µ=1
(cid:88)
µ, Yµ, V t
ωt
µ


gout

i = Σt
T t

m

i 

µ=1
(cid:88)
m


µ=1
(cid:88)
m

i 



µ=1
(cid:88)

Xµi√
n

(cid:18)

Xµi√
n

= Σt
i

= Σt

(cid:0)

(cid:0)

gout

µ, Yµ, V t
ωt
µ

−

gout(ωt

iµ, Yµ, V t
iµ)

−

Xµi√
n

∂gout
∂ω

X 2
µi
n

∂gout
∂ω

(cid:1)

(cid:1)

(cid:0)

(cid:0)

µ, Yµ, V t
ωt
µ

ˆW t

i + O

1
n

(cid:18)

(cid:19)(cid:19)

µ, Yµ, V t
ωt
µ

ˆW t

i 

+ O

1
n3/2

(cid:18)

(cid:19)

(cid:1)

(cid:1)



The AMP algorithm follows naturally the rBP updates (109) using the expanded estimates of the mean and

variance ωµ, Vµ, Ti and Σi, and finally reads in pseudo language:

Algorithm 2 Approximate Message Passing for the committee machine

Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: gout,µ = 0, Σi = IK×K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
Initialize: ˆWi ∈ RK and ˆCi, ∂ωgout,µ ∈ S +
repeat

K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 1.

n

ωt

ˆW t

µ =

i −

Xµi√
n

Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
X 2
µi
n

i Σt−1
i

−1 ˆCt

gt−1
out,µ

Σt−1
i

i=1
(cid:80)
out,µ = gout(ωt
gt

Update of gout,µ ∈ RK and ∂ωgout,µ ∈ S +
K:
out,µ = ∂ωgout(ωt

µ, Yµ, V t
µ)
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt

(cid:1)
∂ωgt

Xµi√

i = Σt
T t
i

X 2
n ∂ωgt
µi

out,µ −

ˆW t
i

out,µ

n gt

m

(cid:1)

(cid:0)

(cid:0)

|

|

(cid:16)

µ=1
(cid:80)

Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = fw(Σt

i, T t
i )

|

ˆCt+1

i = fc(Σt

i, T t
i )

(cid:17)

µ, Yµ, V t
µ)

i = −
(cid:16)

m

µ=1
(cid:80)

V t
µ =

X 2
µi
n

ˆCt
i

n

i=1
(cid:80)

X 2
n ∂ωgt
µi

out,µ

−1

(cid:17)

ˆW t+1
t = t + 1

until Convergence on ˆW , ˆC.
Output: ˆW and ˆC.

G State evolution equations from AMP

In this section, W (cid:63) denotes the ground truth weights of the teacher and we define the overlap parameters at
time t, mt, σt, qt, Q and that respectively measure the correlation of the AMP estimator with the ground truth,

44

its variance and the norms of student and teacher weights:



mt ≡ EW (cid:63) lim
n→∞

qt ≡ EW (cid:63) lim
n→∞


1
n

1
n

n

i=1
(cid:88)
n

i=1
(cid:88)

ˆW t

(cid:124)
i (W (cid:63)
i )

,

ˆW t

(cid:124)
i ( ˆW t
i )

,

and



σt ≡ EW (cid:63) lim
n→∞

Q ≡ EW (cid:63) lim
n→∞


n

ˆCt
i .

1
n

1
n

i=1
(cid:88)
n

i=1
(cid:88)

W (cid:63)

i (W (cid:63)
i )

(cid:124)

,

The aim is to derive the asymptotic behaviour of these overlap parameters, called state evolution. The idea is
to compute the overlap distributions starting with the relaxed BP equations eq. (109).

G.1 Messages distribution

In order to get the asymptotic behaviour of the overlap parameters, we need first to compute the distribution of
Σt
µ→i and T t
µ→i. Besides, we recall that in our model, the output has been generated by a teacher according to
n
1√
. We define zµ ≡ 1√
j(cid:54)=i XµjW (cid:63)
n W (cid:63)Xµ, A
Yµ = ϕ0
j .
And it useful to recall EX [Xµi] = 0 and EX [X 2
(cid:17)

n W (cid:63)Xµ = 1√
µi] = 1.

i and zµ→i ≡ 1√
n

n
i=1 XµiW (cid:63)

out

(cid:16)

n

(cid:80)

(cid:80)

µ→i

• ωt
Under belief propagation assumption messages are independent. ωt

µ→i is thus the sum of independent
variables and follows a gaussian distribution. Let’s compute the first two moments, using expansions of the
relaxed BP equations eq. (109):

EX

ωt
µ→i

=

EX [Xµj] ˆW t

j→µ = 0 ,

(cid:2)
µ→i(ωt
ωt

EX

(cid:124)
µ→i)

=

(cid:3)

(cid:3)

(cid:2)

• zµ

EX [XµjXµk] ˆW t

j→µ( ˆW t

(cid:124)
k→µ)

=

EX

X 2
µj

ˆWj→µ( ˆWj→µ)

(cid:124)

=

ˆW t

j→µ( ˆW t

j→µ)

(cid:124)

=

ˆW t

i ( ˆW t
i )

(cid:124)

+ O

1
n

n

i=1
(cid:88)

n

j(cid:54)=i
(cid:88)

(cid:3)

(cid:2)
1/n3/2

(cid:16)

(cid:17)

−→
n→∞

qt .

EX [zµ] =

1
√
n

EX [Xµi] W (cid:63)

i = 0 ,

EX,W (cid:63)

zµz

(cid:124)
µ

= EW (cid:63)

EX [XµjXµk] W (cid:63)

j (W (cid:63)
k )

(cid:124)

= EW (cid:63)

W (cid:63)

i (W (cid:63)
i )

(cid:124) −→
n→∞

Q .

1
√
n

n

j(cid:54)=i
(cid:88)
n

1
n

1
n

j(cid:54)=i,k(cid:54)=i
(cid:88)
n

j(cid:54)=i
(cid:88)

n

i=1
(cid:88)
1
n

n

j=1,k=1
(cid:88)

(cid:2)

(cid:3)

• zµ and ωt

µ→i

EX,W (cid:63)

ωt
µ→iz

(cid:124)
µ

= EW (cid:63)

EX [XµjXµk] ˆW t

j→µ(W (cid:63)
k )

(cid:124)

= EW (cid:63)

ˆW t

(cid:124)
j→µ(W (cid:63)
j )

1
n

1
n

n

j(cid:54)=i,k=1
(cid:88)
n

i=1
(cid:88)

(cid:2)

(cid:3)

= EW (cid:63)

ˆW t

(cid:124)
i (W (cid:63)
i )

+ O

1/n3/2

−→
n→∞

mt .

(cid:16)

(cid:17)

Hence asymptotically (zµ, ωt

µ→i) follow a Gaussian distribution with covariance matrix Qt =

Q mt
qt
mt
(cid:34)

.

(cid:35)

1
n

n

i=1
(cid:88)

1
n

n

j(cid:54)=i
(cid:88)

45

• Vµ→i

concentrates around its mean:

EX,W (cid:63)

V t
µ→i

= EW (cid:63)

EX

X 2
µj

ˆCt

j→µ = EW (cid:63)

ˆCt

j→µ = EW (cid:63)

ˆCt

i + O

1/n3/2

1
n

n

j(cid:54)=i
(cid:88)

1
n

n

i
(cid:88)

−→
n→∞

σt .

(cid:16)

(cid:17)

1
n

n

j(cid:54)=i
(cid:88)

(cid:3)

(cid:2)

(cid:3)
Let’s define other order parameters, that will appear in the following:

(cid:2)

ˆqt ≡ αEω,z,A
ˆmt ≡ αEω,z,A
ˆχt ≡ αEω,z,A

gout(ω, ϕ0
∂zgout(ω, ϕ0
(cid:2)
−∂ωgout(ω, ϕ0
(cid:2)

out(z, A), σt)

,
out(z, A), σt)
(cid:3)

out(z, A), σt)gout(ω, ϕ0

out(z, A), σt)(cid:124)

,

(cid:3)

.

(cid:3)





• T t

µ→i

can be expanded around zµ→i:

(cid:2)

Σt

µ→i

−1

T t
µ→i =

m





ν(cid:54)=µ
(cid:88)

Bt

ν→i

=



1
√
n

m

ν(cid:54)=µ
(cid:88)

Xνigout(ωt

ν→i, ϕ0

1
√
n

out 

XνjW (cid:63)

j + XνiW (cid:63)

i , A

, V t

ν→i)



n

j(cid:54)=i
(cid:88)

Xνigout(ωt

ν→i, ϕ0



out (zν→i, A) , V t

ν→i)

+


νi∂zgout(ωt

X 2


out (zν→i, A) , V t
ν→i)

ν→i, ϕ0

m

1
n









ν(cid:54)=µ
(cid:88)




W (cid:63)
i .





(cid:0)

=

(cid:1)
m





ν(cid:54)=µ
(cid:88)

1
√
n

• Σt

µ→i

At

ν→i = −

X 2

νi∂ωgout(ωt

ν→i, Yν, V t

ν→i)

m

Σt

µ→i

−1

=

(cid:0)

(cid:1)

m

1
n

ν(cid:54)=µ
(cid:88)

m

= −

ν(cid:54)=µ
(cid:88)
νi∂ωgout(ωt

X 2

1
n

ν(cid:54)=µ
(cid:88)

ν→i, ϕ0

out (zν→i, A) , V t

ν→i) + O

1/n3/2

.

Hence taking the average and the large size limit, the first moments of the variables Σt

(cid:16)

(cid:17)
µ→i and T t

µ→i read:

Eω,z,A,X

Eω,z,A,X

Eω,z,A,X

−1

(cid:17)

−1

(cid:17)

−1

Σt

µ→i

Σt

µ→i

Σt

µ→i

(cid:20)(cid:16)

(cid:20)(cid:16)

T t
µ→i

−→
n→∞

(cid:21)

ˆmtW (cid:63)
i ,
(cid:124)

T t
µ→i

T t
µ→i

Σt

µ→i

(cid:17)

(cid:16)

(cid:21)

(cid:17)

(cid:16)
−→
n→∞

ˆχt .

−1

−→
n→∞

ˆqt ,






And finally T t

µ→i ∼ ( ˆχt)−1

ˆmtW (cid:63)

with ξ ∼ N (0, 1) and

Σt

µ→i

−1

∼ ( ˆχt)−1 .

(cid:16)

(cid:17)

(cid:20)(cid:16)

(cid:21)

(cid:17)
i + (ˆqt)1/2ξ

(cid:0)

(cid:1)

G.2 State evolution equations - Non Bayes optimal case

Let’s define the following notations:

T t[W (cid:63), ξ] ≡ ( ˆχt)−1

ˆmtW (cid:63) + (ˆqt)1/2ξ

Σt ≡ ( ˆχt)−1

(cid:16)

(cid:17)

46

Gathering above results, the state evolution equations read:

ˆW t

i (W (cid:63)
i )

(cid:124)

= EW (cid:63),ξ

fw

Σt, T t[W (cid:63), ξ]

(cid:124)
(W (cid:63))

mt+1 = EW (cid:63) lim
n→∞

qt+1 = EW (cid:63) lim
n→∞

1
n

1
n

1
n

n

i=1
(cid:88)
n

i=1
(cid:88)
n

i=1
(cid:88)

ˆW t+1
i

( ˆW t+1
i

)

(cid:124)

= EW (cid:63),ξ

(cid:2)

(cid:0)
fw

(cid:1)

Σt, T t[W (cid:63), ξ]

fw

Σt, T t[W (cid:63), ξ]

(cid:3)

(cid:0)

(cid:124)

(cid:1)

(cid:3)

σt+1 = EW (cid:63) lim
n→∞

ˆCt+1
i = EW (cid:63),ξ

fc

Σt, T t[W (cid:63), ξ]

(cid:2)

(cid:0)

(cid:1)

(cid:2)

(cid:0)

(cid:1)(cid:3)

ˆqt = αEω,z,A

gout(ω, ϕ0

= α

dPA(A)
(cid:2)

out(z, A), σt)gout(ω, ϕ0
z, ω; 0, Qt

dzdωN

out(z, A), σt)(cid:124)

gout(ω, ϕ0

out(z, A), σt)gout(ω, ϕ0

out(z, A), σt)

(cid:3)

(cid:124)

= α

dPA(A)
(cid:2)

dzdωN

∂zgout(ω, ϕ0

out(z, A), σt)

ˆmt = αEω,z,A

∂zgout(ω, ϕ0

(cid:90)

(cid:90)

(cid:90)

(cid:90)

ˆχt = αEω,z,A

−∂ωgout(ω, ϕ0

= −α

dPA(A)

(cid:2)

dzdωN

(cid:90)

(cid:90)

(cid:0)

out(z, A), σt)
z, ω; 0, Qt
(cid:3)

(cid:1)

out(z, A), σt)
(cid:0)
(cid:1)
z, ω; 0, Qt
(cid:3)

(cid:0)

(cid:1)

∂ωgout(ω, ϕ0

out(z, A), σt)

G.3 State evolution equations - Bayes optimal case

and











In the bayes optimal case, the student knows all the parameters of the teacher and then P (cid:63)
mt = qt and ˆqt = ˆmt = ˆχt, σt = Q − qt and then, naturally

0 = P0, ϕ0

out = ϕout,

T t[W (cid:63), ξ] ≡ W (cid:63) + (ˆqt)−1/2ξ ,
Σt ≡ (ˆqt)−1 .

In the Bayes-optimal case, the set of state evolution equations reduces and simplifies to:

qt+1 = EW (cid:63),ξ

fw

Σt, T t[W (cid:63), ξ]

fw

Σt, T t[W (cid:63), ξ]

(cid:124)

,

ˆqt = αEω,z,A

(cid:2)
(cid:0)
gout(ω, ϕout(z, A), σt)gout(ω, ϕout(z, A), σt)(cid:124)

(cid:1)

(cid:1)

(cid:0)

(cid:3)

,

(111)

(cid:3)






(cid:0)

where (z, ω) ∼ Nz,ω

0, 0; Qt

with Qt =

(cid:2)

Q qt
qt
qt

.

(cid:35)

(cid:34)

(cid:1)

G.4 State evolution - Consistence between replicas and AMP - Bayes optimal case

State evolution - AMP

Using the change of variable ξ ← ξ +

1/2

ˆqt

W (cid:63), eq. (111) becomes:

qt+1 = Eξ

ZP0

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

(cid:0)

(cid:1)

(cid:104)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:124)

(cid:17)

(cid:105)

47

In addition in the Bayes-optimal case, as:

µ→i)(cid:124)

= mt − qt = 0

µ→i)(zµ − ωt

µ→i)(cid:124)

= Q − qt ,

µ→i)(cid:124)] = qt

(cid:105)



EX

ωt
µ→i(zµ − ωt
(cid:104)
EX [ωt

EX


µ→i(ωt
(cid:124)
µ − ωt

(z

(cid:104)

(cid:105)
0, 0; Qt

(cid:0)

(cid:1)

(cid:124)

the multivariate distribution can be written as a product: Nz,ω
using Pout(y|z) =

dPA(A)δ

y − ϕ0

, eq. (111) becomes:

out(z, A)

= Nω

0, qt

Nz

ω, Q − qt

. Hence,

(cid:0)

(cid:1)

(cid:0)

(cid:1)

ˆqt = αEω,z,A

(cid:2)

(cid:90)

(cid:82)
gout(ω, ϕ0
e− 1

(cid:0)

(cid:1)

out(z, A), Q − qt)

out(z, A), Q − qt)gout(ω, ϕ0
2 ω(cid:124)(qt)−1ω
(2π)K/2 det(qt)1/2
e− 1
(2π)K/2 det(Q − qt)1/2 gout((qt)1/2ξ, y, Q − qt)gout((qt)1/2ξ, y, Q − qt)

e− 1
(2π)K/2 det(Q − qt)1/2 gout(ω, y, Q − qt)gout(ω, y, Q − qt)

(cid:90)
2 (z−ω)(cid:124)(Q−qt)−1(z−ω)

2 (z−ω)(cid:124)(Q−qt)−1(z−ω)

dzPout(y|z)

dzPout(y|z)

(cid:3)

(cid:124)

(cid:124)

= α

dy

dω

(cid:90)

= α

dy

Dξ

(cid:90)
= αEy,ξ

(cid:90)
ZPout

(cid:90)
(qt)1/2ξ, y, Q − qt

gout

(qt)1/2ξ, y, Q − qt

gout

(qt)1/2ξ, y, Q − qt

(cid:16)
(cid:17)
Finally to summarize the state evolution equations can be written as:

(cid:16)

(cid:17)

(cid:16)

(cid:104)

(cid:124)

(cid:17)

(cid:105)

(cid:124)

qt+1 = Eξ

ZP0

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

(cid:104)
ˆqt = αEy,ξ

(cid:16)
ZPout

(qt)1/2ξ, y, Q − qt

gout

(cid:17)

(cid:16)

(qt)1/2ξ, y, Q − qt

(cid:17)

(cid:16)

gout

(cid:105)
(qt)1/2ξ, y, Q − qt

(cid:17)

(cid:124)

(112)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:105)

(cid:1)






State evolution - Replicas

(cid:104)

(cid:0)

Recall from sec. B, the free entropy eq. (76) reads

limn→∞ fn = extrq,ˆq

− 1

2 Tr[q ˆq] + IP + αIC

,

≡ Eξ

ZP0(ˆq1/2ξ, ˆq−1) log(ZP0(ˆq1/2ξ, ˆq−1))

(cid:9)

(cid:8)

,

(cid:2)
≡ Eξ,y

ZPout(q1/2ξ, y, Q − q) log(ZPout(q1/2ξ, y, Q − q))

(cid:3)

.

IP

IC






Taking the derivatives with respect to q and ˆq, using an integration by part and the following identities:

(cid:3)

(cid:2)

∂ZPout

∂q = − 1

2 q−1e

1

2 ξ(cid:124)ξ∂ξ

∂ZP0
∂ ˆq = − 1

2 ˆq−1e

1

2 ξ(cid:124)ξ∂ξ

e− 1
(cid:104)
e− 1

2 ξ(cid:124)ξ∂ξZPout
2 ξ(cid:124)ξ∂ξZP0

,

(cid:105)

,

(cid:104)

(cid:105)






the state evolution equations read:

q = 2 ∂IP
∂ ˆq




ˆq = 2α ∂IC
∂q

with

2

∂IP
∂ ˆq = 1
∂IC
∂q = 1

2




Eξ

(cid:2)
Ey,ξ

ZP0(ˆq1/2ξ, ˆq−1)fw(ˆq1/2ξ, ˆq)fw(ˆq1/2ξ, ˆq−1)(cid:124)
ZPout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)(cid:124)

(cid:3)

(cid:3)

that simplifies and allows to recover the state evolutions equations directly derived from AMP eq. (112), but



(cid:2)

48

without time indices

q = Eξ

ZP0(ˆq1/2ξ, ˆq−1)fw(ˆq1/2ξ, ˆq)fw(ˆq1/2ξ, ˆq−1)(cid:124)

,




(cid:2)
ˆq = αEy,ξ

ZPout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)(cid:124)

(cid:3)

.

(cid:3)

(cid:2)
H Parity machine for K = 2



Although we mainly focused on the committee machine, another classical two-layers neural network is the
parity machine [7] and our proof applies to this case as well. While learning is known to be computationally
hard for general K, the case K = 2 is special, and in fact can be reformulated as a committee machine, where
the sign activation function has been replaced by ϕ1(z) = 1(z (cid:54)= 0) − 1(z = 0):

K

n

K

n

Yµ = sign

sign

XµiW ∗
il

= ϕ1

sign

XµiW ∗
il

.

(113)

(cid:104)

l=1
(cid:89)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

(cid:104)

l=1
(cid:88)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

We have repeated our analysis for the K = 2 parity machine and the phase diagram is summarized in
Fig. 5 where we show the generalization error and the elements of the overlap matrix for Gaussian (left) and
binary weights (right), with the results of the AMP algorithm (points).

Below the specialization phase transition α < αspec, the symmetry of the output imposes the non-
specialized fixed point q00 = q01 = 0 to be the only solution, with αG
spec(K = 2) (cid:39)
2.49. Above the specialization transition αspec, the overlap becomes specialized with a non-trivial diagonal
term.

spec(K = 2) (cid:39) 2.48 and αB

Additionally, in the binary case, an information theoretical transition towards a perfect learning occurs
at αB
IT(K = 2) (cid:39) 2.00, meaning that the perfect generalization fixed point (q00 = 1, q01 = 0) becomes the
global optimizer of the free entropy. It leads to a first order phase transition of the AMP algorithm which
retrieves the perfect generalization phase only at αB
perf (K = 2) (cid:39) 3.03. This is similar to what happens in
single layer neural networks for the symmetric door activation function, see [11]. Again, these results for the
parity machine emphasize a gap between information-theoretical and computational performance.

49

Figure 5: Similar plot as in Fig. 2 but for the parity machine with two hidden neurons. Value of the order parameter
and the optimal generalization error for a parity machine with two hidden neurons with Gaussian weights (left)
and binary/Rademacher weights (right). SE and AMP overlaps are respectively represented in full line and points.

50

9
1
0
2
 
n
u
J
 
4
1
 
 
]

G
L
.
s
c
[
 
 
2
v
1
5
4
5
0
.
6
0
8
1
:
v
i
X
r
a

The committee machine: Computational to statistical gaps
in learning a two-layers neural network

Benjamin Aubin(cid:63)†, Antoine Maillard†, Jean Barbier♦,
Florent Krzakala†, Nicolas Macris⊗ and Lenka Zdeborová(cid:63)

Abstract

Heuristic tools from statistical physics have been used in the past to locate the phase transitions and
compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural
networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers
neural network model called the committee machine, under a technical assumption. We also introduce a
version of the approximate message passing (AMP) algorithm for the committee machine that allows to
perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes
in which a low generalization error is information-theoretically achievable while the AMP algorithm fails
to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large
computational gap.

Contents

1 Introduction

2 Summary of contributions and related works

3 Main technical results
3.1 A general model .
.
3.2 Two auxiliary inference problems .
3.3 The free entropy .
.
3.4
3.5 Approximate message passing, and its state evolution .

.
.
.
.
Learning the teacher weights and optimal generalization error .
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

4 From two to more hidden neurons, and the specialization phase transition
.
.
.
.

4.1 Two neurons .
.
4.2 More is different .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

5 Structure of the proof of Theorem 3.1
Interpolating estimation problem .

5.1
.
5.2 Overlap concentration and fundamental sum rule .
.
5.3 A technical lemma and an assumption .
.
.
5.4 Matching bounds

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

3

3

5
5
5
6
7
8

10
10
11

12
12
14
15
16

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

(cid:63) Institut de Physique Théorique, CNRS & CEA & Université Paris-Saclay, Saclay, France.
† Laboratoire de Physique Statistique, CNRS & Sorbonnes Universités & École Normale Supérieure, PSL University, Paris, France.
⊗ Laboratoire de Théorie des Communications, École Polytechnique Fédérale de Lausanne, Suisse.
♦ International Center for Theoretical Physics, Trieste, Italy.

1

C.1 The generalization error at K = 2 .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

6 Discussion

A Proof details for Theorem 3.1

A.1 The Nishimori property in Bayes-optimal learning .
.
.
A.2 Setting in the Hamiltonian language .
.
A.3 Free entropy variation: Proof of Proposition 5.2 .
.
.
.
.
A.4 Technical lemmas .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

B Replica calculation

C Generalization error

D The large K limit in the committee symmetric setting
.
.
.
.

D.1 Large K limit for sign activation function .
.
D.2 The Gaussian prior
.
.
.
.
D.3 The fixed point equations .
.
D.4 The generalization error at large K .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

E Linear networks show no specialization

F Update functions and AMP derivation
F.1 Definition of the update functions .
.
F.2 Derivation of the Approximate Message Passing algorithm .

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

G State evolution equations from AMP
.

.
G.1 Messages distribution .
.
.
.
G.2 State evolution equations - Non Bayes optimal case .
G.3 State evolution equations - Bayes optimal case .
.
.
G.4 State evolution - Consistence between replicas and AMP - Bayes optimal case .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

H Parity machine for K = 2

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

18

23
23
23
24
26

29

31
32

32
33
36
36
38

38

39
39
39

44
45
46
47
47

49

2

1 Introduction

While the traditional approach to learning and generalization follows the Vapnik-Chervonenkis [1] and
Rademacher [2] worst-case type bounds, there has been a considerable body of theoretical work on calculating
the generalization ability of neural networks for data arising from a probabilistic model within the framework
of statistical mechanics [3, 4, 5, 6, 7]. In the wake of the need to understand the effectiveness of neural
networks and also the limitations of the classical approaches [8], it is of interest to revisit the results that have
emerged thanks to the physics perspective. This direction is currently experiencing a strong revival, see e.g.
[9, 10, 11, 12].

Of particular interest is the so-called teacher-student approach, where labels are generated by feeding
i.i.d. random samples to a neural network architecture (the teacher) and are then presented to another neural
network (the student) that is trained using these data. Early studies computed the information theoretic
limitations of the supervised learning abilities of the teacher weights by a student who is given m independent
n-dimensional examples with α ≡ m/n = Θ(1) and n → ∞ [3, 4, 7]. These works relied on non-rigorous
heuristic approaches, such as the replica and cavity methods [13, 14]. Additionally no provably efficient
algorithm was provided to achieve the predicted learning abilities, and it was thus difficult to test those
predictions, or to assess the computational difficulty.

Recent developments in statistical estimation and information theory —in particular of approximate
message passing algorithms (AMP) [15, 16, 17, 18], and a rigorous proof of the replica formula for the optimal
generalization error [11]— allowed to settle these two missing points for single-layer neural networks (i.e.
without any hidden variables). In the present paper, we leverage on these works, and provide rigorous
asymptotic predictions and corresponding message passing algorithm for a class of two-layers networks.

2 Summary of contributions and related works

While our results hold for a rather large class of non-linear activation functions, we illustrate our findings on a
case considered most commonly in the early literature: the committee machine. This is possibly the simplest
version of a two-layers neural network where all the weights in the second layer are fixed to unity, and we
illustrate it in Fig. 1. Denoting Yµ the label associated with a n-dimensional sample Xµ, and W ∗
il the weight
connecting the i-th coordinate of the input to the l-th node of the hidden layer, it is defined by:

K

n

Yµ = sign

sign

XµiW ∗
il

.

(cid:104)

l=1
(cid:88)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

(1)

We concentrate here on the teacher-student scenario: The teacher generates i.i.d. data samples with i.i.d.
standard Gaussian coordinates Xµi ∼ N (0, 1), then she/he generates the associated labels Yµ using a committee
machine as in (1), with i.i.d. weights W ∗
il unknown to the student (in the proof section we will consider the
l=1), but in practice we consider
more general case of a distribution for the weights of the form
the fully separable case). The student is then given the m input-output pairs (Xµ, Yµ)m
µ=1 and knows the
(cid:81)
distribution P0 used to generate W ∗
il. The goal of the student is to learn the weights W ∗
il from the available
examples (Xµ, Yµ)m
µ=1 in order to reach the smallest possible generalization error (i.e. to be able to predict the
label the teacher would generate for a new sample not present in the training set).

n
i=1 P0({W ∗

il}K

There have been several studies of this model within the non-rigorous statistical physics approach in
the limit where α ≡ m/n = Θ(1), K = Θ(1) and n → ∞ [19, 20, 21, 22, 6, 7]. A particularly interesting
result in the teacher-student setting is the specialization of hidden neurons (see sec. 12.6 of [7], or [23] in the
context of online learning): For α < αspec (where αspec is a certain critical value of the sample complexity),

3

n features

(Xµi)m,n
µ,i=1
samples

K hidden
units

f (1)

f (1)

output

f (2)

Yµ

W (2) ∈ RK

W ∗

il ∈ Rn×K

Figure 1: The committee machine is one of the simplest models belonging to the considered model class (2), and
on which we focus to illustrate our results. It is a two-layers neural network with activation sign functions
f (1), f (2) = sign and weights W (2) fixed to unity. It is represented for K = 2.

the permutational symmetry between hidden neurons remains conserved even after an optimal learning, and
the learned weights of each of the hidden neurons are identical. For α > αspec, however, this symmetry gets
broken as each of the hidden units correlates strongly with one of the hidden units of the teacher. Another
remarkable result is the calculation of the optimal generalization error as a function of α.

Our first contribution consists in a proof of the replica formula conjectured in the statistical physics
literature, using the adaptive interpolation method of [24, 11], that allows to put several of these results on a
rigorous basis. This proof uses a technical unproven assumption. Our second contribution is the design of
an AMP-type of algorithm that is able to achieve the optimal generalization error in the above limit of large
dimensions for a wide range of parameters. The study of AMP —that is widely believed to be optimal between
all polynomial algorithms in the above setting [25, 26, 27, 28]— unveils, in the case of the committee machine
with a large number of hidden neurons, the existence a large hard phase in which learning is information-
theoretically possible, leading to a good generalization error decaying asymptotically as 1.25K/α (in the
α = Θ(K) regime), but where AMP fails and provides only a poor generalization that does not go to zero
when increasing α. This strongly suggests that no efficient algorithm exists in this hard region and therefore
there is a computational gap in learning such neural networks. In other problems where a hard phase was
identified its study boosted the development of algorithms that are able to match the predicted thresholds and
we anticipate this will translate to the present model.

We also want to comment on a related line of work that studies the loss-function landscape of neural
networks. While a range of works show under various assumptions that spurious local minima are absent
in neural networks, others show under different conditions that they do exist, see e.g. [29]. The regime of
parameters that is hard for AMP must have spurious local minima, but the converse is not true in general. It
might be that there are spurious local minima, yet the AMP approach succeeds. Moreover, in all previously
studied models in the Bayes-optimal setting the (generalization) error obtained with the AMP is the best
known and other approaches, e.g. (noisy) gradient based, spectral algorithms or semidefinite programming,
are not better in generalizing even in cases where the “student” models are overparametrized. Of course in
order to be in the Bayes-optimal setting one needs to know the model used by the teacher which is not the
case in practice.

4

3 Main technical results

3.1 A general model

While in the illustration of our results we shall focus on the model (1), all our formulas are valid for a broader
class of models: Given m input samples (Xµi)m,n
il the teacher-weight connecting the i-th
input (i.e. visible unit) to the l-th node of the hidden layer. For a generic function ϕout : RK × R → R one can
formally write the output as

µ,i=1, we denote W ∗

Yµ = ϕout

n

1
√
n

(cid:16)(cid:110)

i=1
(cid:88)

XµiW ∗
il

K

l=1

, Aµ

(cid:111)

(cid:17)

or

Yµ ∼ Pout

·

1
√
n

n

i=1
(cid:88)

(cid:16)

(cid:12)
(cid:110)
(cid:12)
(cid:12)

K

XµiW ∗
il

,

(2)

l=1

(cid:111)

(cid:17)

where (Aµ)m
part of the model, generally accounting for noise.

µ=1 are i.i.d. real valued random variables with known distribution PA, that form the probabilistic

For deterministic models the second argument is simply absent (or is a Dirac mass). We can view altern-
atively (2) as a channel where the transition kernel Pout is directly related to ϕout. As discussed above, we
focus on the teacher-student scenario where the teacher generates Gaussian i.i.d. data Xµi ∼ N (0, 1), and i.i.d.
weights W ∗
µ=1 by computing marginal means of
the posterior probability distribution (5).

il ∼ P0. The student then learns W ∗ from the data (Xµ, Yµ)m

Different scenarii fit into this general framework. Among those, the committee machine is obtained when
K
l=1 sign(hl)) while another model considered previously is given by the parity
choosing ϕout(h) = sign(
K
l=1 sign(hl), see e.g. [7] and sec. H for the numerical results in the case K = 2.
machine, when ϕout(h) =
A number of layers beyond two has also been considered, see [22]. Other activation functions can be used, and
many more problems can be described, e.g. compressed pooling [30, 31] or multi-vector compressed sensing
[32].

(cid:80)
(cid:81)

3.2 Two auxiliary inference problems

Denote SK the finite dimensional vector space of K × K matrices, S +
K × K matrices, S ++
K s.t. N − M ∈ S +
S+

K for positive definite K × K matrices, and ∀ N ∈ S +
K}. Note that S +
K(N ) is convex and compact.

K the convex set of semi-definite positive
K(N ) ≡ {M ∈

K we set S+

Stating our results requires introducing two simpler auxiliary K-dimensional estimation problems:
• The first one consists in retrieving a K-dimensional input vector W0 ∼ P0 from the output of a Gaussian
vector channel with K-dimensional observations

Y0 = r1/2W0 + Z0 ,

Z0 ∼ N (0, IK×K) and the “channel gain” matrix r ∈ S +

K. The posterior distribution on w = (wl)K

l=1 is

P (w|Y0) =

P0(w)eY

(cid:124)
0 r1/2w− 1

2 w(cid:124)rw ,

1
ZP0

(3)

and the associated free entropy (or minus free energy) is given by the expectation over Y0 of the log-partition
function

ψP0(r) ≡ E ln ZP0

and involves K dimensional integrals.
• The second problem considers K-dimensional i.i.d. vectors V, U ∗ ∼ N (0, IK×K) where V is considered to

5

be known and one has to retrieve U ∗ from a scalar observation obtained as

Y0 ∼ Pout( · |q1/2V + (ρ − q)1/2U ∗)

where the second moment matrix ρ ≡ E[W0W
q is in S+

K(ρ). The associated posterior is

(cid:101)

(cid:124)
0 ] is in S +

K (where W0 ∼ P0) and the so-called “overlap matrix”

P (u|

Y0, V ) =

1
ZPout

2 u(cid:124)u
e− 1
(2π)K/2 Pout

Y0|q1/2V + (ρ − q)1/2u

,

(4)

and the free entropy reads this time

(cid:101)

(cid:0)

(cid:101)

(cid:1)

ΨPout(q; ρ) ≡ E ln ZPout

(with the expectation over

Y0 and V ) and also involves K dimensional integrals.

3.3 The free entropy

(cid:101)

The central object of study leading to the optimal learning and generalization errors in the present setting is
the posterior distribution of the weights:

P ({wil}n,K

i,l=1 | {Xµi, Yµ}m,n

µ,i=1) =

P0({wil}K

l=1)

Pout

Yµ

Xµiwil

,

(5)

1
Zn

n

i=1
(cid:89)

m

µ=1
(cid:89)

(cid:16)

1
√
n

n

i=1
(cid:88)

(cid:110)

(cid:12)
(cid:12)
(cid:12)

K

l=1

(cid:111)

(cid:17)

where the normalization factor is nothing else than a partition function, i.e. the integral of the numerator over
{wil}n,K

i,l=1. The expected1 free entropy is by definition

fn ≡

E ln Zn .

1
n

The replica formula gives an explicit (conjectural) expression of fn in the high-dimensional limit n, m → ∞
with α = m/n fixed. We show in sec. B how the heuristic replica method [13, 14] yields the formula. This
computation was first performed, to the best of our knowledge, by [19] in the case of the committee machine.
Our first contribution is a rigorous proof of the corresponding free entropy formula using an interpolation
method [33, 34, 24], under a technical Assumption 1.

In order to formulate our results, we add an (arbitrarily small) Gaussian regularization noise Zµ

∆ to the

√

first expression of the model (2), where ∆ > 0, Zµ ∼ N (0, 1), which thus becomes

Yµ = ϕout

n

1
√
n

(cid:16)(cid:110)

i=1
(cid:88)

XµiW ∗
il

, Aµ

+ Zµ

∆ ,

√

K

l=1

(cid:111)

(cid:17)

so that the channel kernel is (u ∈ RK)

Pout(y|u) =

√

dPA(a)e− 1

2∆ (y−ϕout(u,a))2

.

1
2π∆

R

(cid:90)
Let us define the replica symmetric (RS) potential as

fRS(q, r) = fRS(q, r; ρ) ≡ ψP0(r) + αΨPout(q; ρ) −

Tr(rq),

1
2

1The symbol E will generally denote an expectation over all random variables in the ensuing expression (here {Xµi, Yµ}).

Subscripts will be used only when we take partial expectations or if there is an ambiguity.

(6)

(7)

(8)

(9)

6

where α ≡ m/n, and ΨPout(q; ρ) and ψP0(r) are the free entropies of the two simpler K-dimensional
estimation problems (3) and (4).

All along this paper, we assume the following hypotheses for our rigorous statements:

(H1) The prior P0 has bounded support in RK.
(H2) The activation ϕout : RK × R → R is a bounded C2 function with bounded first and second derivatives

w.r.t. its first argument (in RK-space).

(H3) For all µ = 1, . . . , m and i = 1, . . . , n we have i.i.d. Xµi ∼ N (0, 1).

We finally rely on a technical hypothesis, stated as Assumption 1 in section 5.3.

Theorem 3.1 (Replica formula). Suppose (H1), (H2) and (H3), and Assumption 1. Then for the model (7) with
kernel (8) the limit of the free entropy is:

lim
n→∞

fn ≡ lim
n→∞

1
n

E ln Zn = sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

(10)

This theorem extends the recent progress for generalized linear models of [11], which includes the case
K = 1 of the present contribution, to the phenomenologically richer case of two-layers problems such as the
committee machine. The proof sketch based on an adaptive interpolation method recently developed in [24] is
outlined in sec. 5 and the details can be found in sec. A.

Remark 3.2 (Relaxing the hypotheses). Note that, following similar approximation arguments as in [11], the
hypothesis (H1) can be relaxed to the existence of the second moment of the prior; thus covering the Gaussian case,
(H2) can be dropped (and thus include model (1) and its sign(·) activation) and (H3) extended to data matrices X
with i.i.d. entries of zero mean, unit variance and finite third moment. Moreover, the case ∆ = 0 can be considered
when the outputs are discrete, as in the committee machine (1), see [11]. The channel kernel becomes in this case
dPA(a)1(y − ϕout(u, a)) and the replica formula is the limit ∆ → 0 of the one provided in
Pout(y|u) =
Theorem 3.1. In general this regularizing noise is needed for the free entropy limit to exist.

(cid:82)

3.4 Learning the teacher weights and optimal generalization error

A classical result in Bayesian estimation is that the estimator ˆW that minimizes the mean-square error with the
ground-truth W ∗ is given by the expected mean of the posterior distribution. Denoting q∗ the extremizer in
the replica formula (10), we expect from the replica method that in the limit n → ∞, m/n = α, and with high
probablity, ˆW (cid:124)W ∗/n → q∗. We refer to proposition 5.3 and to the proof in sec. A for the precise statement,
that remains rigorously valid only in the presence of an additional (possibly infinitesimal) side-information.
From the overlap matrix q∗, one can compute the Bayes-optimal generalization error when the student tries to
classify a new, yet unseen, sample Xnew. The estimator of the new label ˆYnew that minimizes the mean-square
error with the true label is given by computing the posterior mean of ϕout(Xneww) (Xnew is a row vector).
Given the new sample, the optimal generalization error is then

1
2

EX,W ∗

Ew|X,Y

ϕout(Xneww)

− ϕout(XnewW ∗)

2

−−−→
n→∞

(cid:15)g(q∗),

(11)

(cid:104)(cid:0)

(cid:2)

(cid:3)

(cid:105)

(cid:1)

where w is distributed according to the posterior measure (5) (note that this Bayes-optimal computation differs
from the so-called Gibbs estimator by a factor 2, see sec. C). In particular, when the data X is drawn from
the standard Gaussian distribution on Rm×n, and is thus rotationally invariant, it follows that this error only
depends on w(cid:124)W ∗/n, which converges to q∗. Then a direct algebraic computation gives a lengthy but explicit
formula for (cid:15)g(q∗), as shown in sec. C.

7

3.5 Approximate message passing, and its state evolution

Our next result is based on an adaptation of a popular algorithm to solve random instances of generalized linear
models, the Approximate Message Passing (AMP) algorithm [15, 16], for the case of the committee machine and
models described by (2).

The AMP algorithm can be obtained as a Taylor expansion of loopy belief-propagation (see sec. F) and
also originates in earlier statistical physics works [35, 36, 37, 38, 39, 26]. It is conjectured to perform the best
among all polynomial algorithms in the framework of these models. It thus gives us a tool to evaluate both the
intrinsic algorithmic hardness of the learning and the performance of existing algorithms with respect to the
optimal one in this model.

Algorithm 1 Approximate Message Passing for the committee machine

Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: gout,µ = 0, Σi = IK×K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
Initialize: ˆWi ∈ RK and ˆCi, ∂ωgout,µ ∈ S +
repeat

K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 1.

n

ωt

ˆW t

µ =

i −

Xµi√
n

Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
X 2
µi
n

i Σt−1
i

−1 ˆCt

gt−1
out,µ

Σt−1
i

i=1
(cid:80)
out,µ = gout(ωt
gt

Update of gout,µ ∈ RK and ∂ωgout,µ ∈ S +
K:
out,µ = ∂ωgout(ωt

µ, Yµ, V t
µ)
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt

(cid:1)
∂ωgt

Xµi√

i = Σt
T t
i

X 2
n ∂ωgt
µi

out,µ −

ˆW t
i

out,µ

n gt

m

(cid:1)

(cid:0)

(cid:0)

|

|

(cid:16)

µ=1
(cid:80)

Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = fw(Σt

i, T t
i )

|

ˆCt+1

i = fc(Σt

i, T t
i )

(cid:17)

µ, Yµ, V t
µ)

i = −
(cid:16)

m

µ=1
(cid:80)

V t
µ =

X 2
µi
n

ˆCt
i

n

i=1
(cid:80)

X 2
n ∂ωgt
µi

out,µ

−1

(cid:17)

ˆW t+1
t = t + 1

until Convergence on ˆW , ˆC.
Output: ˆW and ˆC.

The AMP algorithm is summarized by its pseudo-code in Algorithm 2, where the update functions gout,
∂ωgout, fw and fc are related, again, to the two auxiliary problems (3) and (4). The functions fw(Σ, T ) and
fc(Σ, T ) are respectively the mean and variance under the posterior distribution (3) when r → Σ−1 and
Y0 → Σ1/2T , while gout(ωµ, Yµ, Vµ) is given by the product of V
and the mean of u under the posterior
Y0 → Yµ, ρ − q → Vµ and q1/2V → ωµ (see sec. F for more details). After convergence, ˆW estimates
(4) using
the weights of the teacher-neural network. The label of a sample Xnew not seen in the training set is estimated
by the AMP algorithm as

−1/2
µ

(cid:101)

Y t
new =

dy

dzl

y Pout(y|{zl}K

l=1)N (z; ωt

new, V t

new) ,

(12)

K

(cid:90)

l=1
(cid:89)

(cid:0)

(cid:1)

new =

n
i=1 Xnew,i ˆW t

where ωt
is the K × K covariance matrix (see below for the definition of qt
(cid:80)
the algorithm on GitHub [40].

i is the mean of the normally distributed variable z ∈ RK, and V t

new = ρ−qt

AMP
AMP). We provide a demonstration code of

AMP is particularly interesting because its performance can be tracked rigorously, again in the asymptotic
limit when n → ∞, via a procedure known as state evolution (a rigorous version of the cavity method in
physics [14]), see [18]. State evolution tracks the value of the overlap between the hidden ground truth W ∗ and

8

Figure 2: Generalization error and order parameter for a committee machine with two hidden neurons (K = 2)
with Gaussian weights (left), binary/Rademacher weights (right). These are shown as a function of the ratio
α = m/n between the number of samples m and the dimensionality n. Lines are obtained from the state evolution
(SE) equations (dominating solution is shown in full line), data-points from the AMP algorithm averaged over 10
instances of the problem of size n = 104. q00 and q01 denote diagonal and off-diagonal overlaps, and their values
are given by the labels on the far-right of the figure.

the AMP estimate ˆW t, defined as qt

AMP ≡ limn→∞( ˆW t)(cid:124)W ∗/n, via the iteration of the following equations:

qt+1
AMP = 2∇ψP0(rt

AMP) ,

rt+1
AMP = 2α∇ΨPout(qt

AMP; ρ) .

(13)

See sec. G for more details and note that the fixed points of these equations correspond to the critical points of
the replica free entropy (10).

Let us comment further on the convergence of the algorithm. In the large n limit, and if the integrals
are performed without errors, then the algorithm is guaranteed to converge. This is a consequence of the
state evolution combined with the Bayes-optimal setting. In practice, of course, n is finite and integrals are
approximated. In that case convergence is not guaranteed, but is robustly achieved in all the cases presented
in this paper. We also expect (by experience with the single layer case) that if the input-data matrix is not
random (which is beyond our assumptions) then we will encounter convergence issues, which could be fixed
by moving to some variant of the algorithm such as VAMP [41].

9

Figure 3: (Left) Bayes optimal and AMP generalization errors and (right) diagonal and off-diagonal overlaps q00,
q01 for a committee machine with a large number of hidden neurons K and Gaussian weights, as a function of the
rescaled parameter ˜α = α/K. Solutions corresponding to global and local minima of the replica free entropy are
spinodal (cid:39) 7.17, ie the
respectively represented with full and dashed lines. The dotted line marks the spinodal at
apparition of a local minimum in the replica free entropy, associated to a solution with specialized hidden units.
spec (cid:39) 7.65, at which the specialized
The dotted-dashed line shows the first order specialization transition at
spec, AMP reaches the Bayes optimal generalization error
fixed point becomes the global minimum. For
and overlaps, corresponding to a non-specialized solution. However, for
spec, the AMP algorithm does not
follow the optimal specialized solution and is stuck in the non-specialized solution plateau, represented with
dashed lines. Hence it unveils a large computational gap (yellow area).

α >
(cid:101)

α <

αG

αG

αG

αG

(cid:101)

(cid:101)

(cid:101)

(cid:101)

(cid:101)

4 From two to more hidden neurons, and the specialization

phase transition

4.1 Two neurons

Let us now discuss how the above results can be used to study the optimal learning in the simplest non-trivial
case of a two-layers neural network with two hidden neurons, that is when model (1) is simply

n

n

Yµ = sign

sign

XµiW ∗
i1

+ sign

XµiW ∗
i2

,

(cid:104)

(cid:16)

i=1
(cid:88)

(cid:17)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

and is represented in Fig. 1, with the convention that sign(0) = 0. We remind that the input-data matrix X
has i.i.d. N (0, 1) entries, and the teacher-weights W ∗ used to generate the labels Y are taken i.i.d. from P0.
In Fig. 2 we plot the optimal generalization error as a function of the sample complexity α = m/n. In
the left panel the weights are Gaussian (for both the teacher and the student), while in the right panel they
are binary/Rademacher. The full line is obtained from the fixed point of the state evolution (SE) of the AMP
algorithm (13), corresponding to the extremizer of the replica free entropy (10). The points are results of
the AMP algorithm run till convergence averaged over 10 instances of size n = 104. In this case and with
random initial conditions the AMP algorithm did converge in all our trials. As expected we observe excellent

10

agreement between the SE and AMP.

In both left and right panels of Fig. 2 we observe the so-called specialization phase transition. Indeed
(13) has two types of fixed points: a non-specialized fixed point where every matrix element of the K × K
order parameter q is the same (so that both hidden neurons learn the same function) and a specialized fixed
point where the diagonal elements of the order parameter are different from the non-diagonal ones. We
checked for other types of fixed points for K = 2 (one where the two diagonal elements are not the same),
but have not found any. In terms of weight-learning, this means for the non-specialized fixed point that the
estimators for both W1 and W2 are the same, whereas in the specialized fixed point the estimators of the
weights corresponding to the two hidden neurons are different, and that the network “figured out” that the
data are better described by a model that is not linearly separable. The specialized fixed point is associated
with lower error than the non-specialized one (as one can see in Fig. 2). The existence of this phase transition
was discussed in statistical physics literature on the committee machine, see e.g. [20, 23].

For Gaussian weights (Fig. 2 left), the specialization phase transition arises continuously at αG

spec(K =
2) (cid:39) 2.04. This means that for α < αG
spec(K = 2) the number of samples is too small, and the student-neural
network is not able to learn that two different teacher-vectors W1 and W2 were used to generate the observed
labels. For α > αG
spec(K = 2), however, it is able to distinguish the two different weight-vectors and the
generalization error decreases fast to low values (see Fig. 2). For completeness we remind that in the case
of K = 1 corresponding to single-layer neural network no such specialization transition exists. We show
in sec. E that it is absent also in multi-layer neural networks as long as the activations remain linear. The
non-linearity of the activation function is therefore an essential ingredient in order to observe a specialization
phase transition.

The right part of Fig. 2 depicts the fixed point reached by the state evolution of AMP for the case of binary
weights. We observe two phase transitions in the performance of AMP in this case: (a) the specialization phase
spec(K = 2) (cid:39) 1.58, and for slightly larger sample complexity a transition towards perfect
transition at αB
generalization (beyond which the generalization error is asymptotically zero) at αB
perf (K = 2) (cid:39) 1.99. The
binary case with K = 2 differs from the Gaussian one in the fact that perfect generalization is achievable at finite
α. While the specialization transition is continuous here, the error has a discontinuity at the transition of perfect
generalization. This discontinuity is associated with the 1st order phase transition (in the physics nomenclature),
leading to a gap between algorithmic (AMP in our case) performance and information-theoretically optimal
performance reachable by exponential algorithms. To quantify the optimal performance we need to evaluate
the global extremum of the replica free entropy (not the local one reached by the state evolution). In doing
so that we get that information theoretically there is a single discontinuous phase transition towards perfect
generalization at αB

IT(K = 2) (cid:39) 1.54.

While the information-theoretic and specialization phase transitions were identified in the physics literature
on the committee machine [20, 21, 3, 4], the gap between the information-theoretic performance and the
performance of AMP —that is conjectured to be optimal among polynomial algorithms— was not yet discussed
in the context of this model. Indeed, even its understanding in simpler models than those discussed here, such
as the single layer case, is more recent [15, 26, 25].

4.2 More is different

It becomes more difficult to study the replica formula for larger values of K as it involves (at least) K-
dimensional integrals. Quite interestingly, it is possible to work out the solution of the replica formula in
the large K limit (thus taken after the large n limit, so that K/n vanishes). It is indeed natural to look for
(cid:124)
K, with the
solutions of the replica formula, as suggested in [19], of the form q = qdIK×K + (qa/K)1K1
unit vector 1K = (1)K
l=1. Since both q and ρ are assumed to be positive, this scaling implies that 0 ≤ qd ≤ 1
and 0 ≤ qa + qd ≤ 1, as it should, see sec. D. We also detail in this same section the corresponding large K

11

expansion of the free entropy for the teacher-student scenario with Gaussian weights. Only the information-
theoretically reachable generalization error was computed [19], thus we concentrated on the analysis of
performance of AMP by tracking the state evolution equations. In doing so, we unveil a large computational
gap.

(cid:101)

(cid:101)

αG

αG

αG

spec and then jumps discontinuously down to reach a decay aymptotically as 1.25/

In the right panel of Fig. 3 we show the fixed point values of the two overlaps q00 = qd + qa/K and
q01 = qa/K and the resulting generalization error, plotted in the left panel. As discussed in [19] it can be
written in a closed form as (cid:15)g = arccos [2 (qa + arcsin qd) /π] /π, represented in the left panel of Fig. 3. The
α ≡ α/K. The specialization is now a 1st order
specialization transition arises for α = Θ(K) so we define
spinodal (cid:39) 7.17 but the free
phase transition, meaning that the specialization fixed point first appears at
spec (cid:39) 7.65. This has
entropy global extremizer remains the one of the non-specialized fixed point until
interesting implications for the optimal generalization error that gets towards a plateau of value εplateau (cid:39) 0.28
for
α. See left panel
α <
of Fig. 3.
(cid:101)
AMP is conjectured to be optimal among all polynomial algorithms (in the considered limit) and thus
analyzing its state evolution sheds light on possible computational-to-statistical gaps that come hand in hand
with 1st order phase transitions. In the regime of α = Θ(K) for large K the non-specialized fixed point is
always stable implying that AMP will not be able to give a lower generalization error than εplateau. Analyzing
the replica formula for large K in more details, see sec. D, we concluded that AMP will not reach the optimal
generalization for any α < Θ(K2). This implies a rather sizable gap between the performance that can
be reached information-theoretically and the one reachable tractably (see yellow area in Fig. 3). Such large
computational gaps have been previously identified in a range of inference problems —most famously in the
planted clique problem [27]— but the committee machine is the first model of a multi-layer neural network
with realistic non-linearities (the parity machine is another example but use a very peculiar non-linearity) that
presents such large gap.

(cid:101)

(cid:101)

(cid:101)

5 Structure of the proof of Theorem 3.1

All along this section we assume (H1), (H2) and (H3), and all the rigorous statements are implicitely assuming
these hypotheses. We denote K-dimensional column vectors by underlined letters. In particular W ∗
i =
(W ∗
µ be K-dimensional vectors with i.i.d. N (0, 1) components.
Let sn ∈ (0, 1/2] a sequence that goes to 0 as n increases, and let M be the compact subset of matrices in
K with eigenvalues in the interval [1, 2]. For all M ∈ snM, 2snIK×K − M ∈ S +
S++
K.

l=1. For µ = 1, . . . m, let V µ, U ∗

l=1, wi = (wil)K

il)K

5.1 Interpolating estimation problem

Let (cid:15) = ((cid:15)1, (cid:15)2) ∈ (snM)2. Let q : [0, 1] → S +
will later on depend on (cid:15)), and

K(ρ) and r : [0, 1] → S +

K be two “interpolation functions” (that

R1(t) ≡ (cid:15)1 +

r(v)dv ,

R2(t) ≡ (cid:15)2 +

q(v)dv .

t

0
(cid:90)

t

0
(cid:90)

For t ∈ [0, 1], define the K-dimensional vector:

1 − t
n

(cid:114)

n

i=1
(cid:88)

St,µ ≡

XµiW ∗

i +

R2(t) V µ +

tρ − R2(t) + 2snIK×K U ∗
µ

(cid:112)
where matrix square-roots (that we denote equivalently A1/2 or
A) are well defined. We interpolate with
auxiliary problems related to those discussed in sec. 3; the interpolating estimation problem is given by the

(cid:112)

√

(14)

(15)

12

following observation model, with two types of t-dependent observations:

Yt,µ ∼ Pout( · | St,µ),
Y (cid:48)

R1(t) W ∗

t,i =

i + Z(cid:48)

1 ≤ µ ≤ m ,
i, 1 ≤ i ≤ n ,

(cid:40)

(16)

i is (for each i) a K-vector with i.i.d. N (0, 1) components, and Y (cid:48)

where Z(cid:48)
t,i is a K-vector as well. Recall that
in our notation the ∗-variables have to be retrieved, while the other random variables are assumed to be known
(except for the noise variables obviously). Define now st,µ by the expression of St,µ but with wi replacing W ∗
i
and uµ replacing U ∗

µ. We introduce the interpolating posterior:

(cid:112)

Pt,(cid:15)(w, u|Yt, Y (cid:48)

t , X, V ) =

1
Zn,(cid:15)(t)

n

i=1
(cid:89)

P0(wi)e− 1

2 (cid:107)Y (cid:48)

t,i−

R1(t)wi(cid:107)2
2

√

2 (cid:107)uµ(cid:107)2
2

e− 1
(2π)K/2 Pout(Yt,µ|st,µ)

(17)

m

µ=1
(cid:89)

where the normalization factor Zn,(cid:15)(t) equals the numerator integrated over all components of w and u. The
average free entropy at time t is by definition

fn,(cid:15)(t) ≡

E ln Zn,(cid:15)(t) =

E ln

Du

dP0(wi)

Pout(Yt,µ|st,µ)

1
n

1
n

(cid:90)

n

i=1
(cid:89)

m

µ=1
(cid:89)

√

e− 1

2 (cid:107)Y (cid:48)

t,i−

R1(t)wi(cid:107)2

2 ,

(18)

n

i=1
(cid:89)

where Du =

m
µ=1

K

l=1(2π)−1/2e−u2

µl/2.

interpolating model:

(cid:81)

(cid:81)

The presence of the small “pertubation” (cid:15) induces a proportional change in the free entropy of the

Lemma 5.1 (Perturbation of the free entropy). For all (cid:15) ∈ (snM)2 we have for t = 0 that |fn,(cid:15)(0) −
fn,(cid:15)=(0,0)(0)| ≤ C(cid:48)sn for some positive constant C(cid:48). Moreover, |fn − fn,(cid:15)=(0,0)(0)| ≤ Csn for some positive
constant C, so that

|fn − fn,(cid:15)=(0,0)(0)| = On(1) .

∇(cid:15)1fn,(cid:15)(0) = −

[ρ − E(cid:104)Q(cid:105)n,0,(cid:15)] ,

1
2

Proof. Let us compute (or directly obtain by the I-MMSE formula for vector channels [42, 43, 44])

(19)

(20)

where the K × K overlap matrix (Qll(cid:48)) is defined below by (23). Note that the r.h.s. of the above equation is
(up to a factor −1/2) the K × K MMSE matrix. Set uy(x) ≡ ln Pout(y|x). Now we compute (by calculations
very similar to the ones used in the proof of the following Proposition 5.2):

∇(cid:15)2fn,(cid:15)(0) =

1
2n

m

E

µ=1
(cid:88)

∇uYt,µ(St,µ)
(cid:104)

(cid:68)

∇uYt,µ(st,µ)

.

(cid:105)

n,0,(cid:15)

(cid:69)

Note that the r.h.s. of the above equation is symmetric by the Nishimori identity Proposition A.1. By
the mean value theorem we obtain then directly that |fn,(cid:15)(0) − fn,(cid:15)=(0,0)(0)| ≤ (cid:107)∇(cid:15)1fn,(cid:15)(0)(cid:107)F(cid:107)(cid:15)1(cid:107)F +
(cid:107)∇(cid:15)2fn,(cid:15)(0)(cid:107)F(cid:107)(cid:15)2(cid:107)F ≤ C maxi (cid:107)(cid:15)i(cid:107) ≤ C(cid:48)sn.

Using this lemma one verifies, using in particular continuity and boundedness properties of ψP0 and ΨPout

(see Lemma A.6 in sec. A for details; sec. A gathers the detailed proofs of all the propositions below):

fn,(cid:15)(0) = fn − K
2 + On(1) ,
1
0 r(t)dt) + αΨPout(
fn,(cid:15)(1) = ψP0(

(cid:40)

1

0 q(t)dt; ρ) − 1

2

1

0 Tr[ρ r(t)]dt − K

2 + On(1) .

(21)

(cid:82)

(cid:82)

13

(cid:82)

Here On(1) → 0 in the n, m → ∞ limit uniformly in t, q, r, (cid:15).

5.2 Overlap concentration and fundamental sum rule

Notice from (21) that at t = 1 the interpolating estimation problem constructs part of the RS potential (9),
while at t = 0 it is the free entropy (6) of the original model (7) (up to a constant). We thus now want to
compare these boundary values thanks to the identity

fn = fn,(cid:15)(0) +

+ On(1) = fn,(cid:15)(1) −

dt +

+ On(1) .

(22)

K
2

1

dfn,(cid:15)(t)
dt

0
(cid:90)

K
2

The next obvious step is therefore to compute the free entropy variation along the interpolation path, see

sec. A.3 for the proof:

Proposition 5.2 (Free entropy variation). Denote by (cid:104)−(cid:105)n,t,(cid:15) the (Gibbs) expectation w.r.t. the posterior Pt,(cid:15)
given by (17). Set uy(x) ≡ ln Pout(y|x). For all t ∈ [0, 1] we have

dfn,(cid:15)(t)
dt

= −

E

Tr

1
2

m

1
n

(cid:68)

(cid:104)(cid:16)

µ=1
(cid:88)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

Q − q(t)

+

Tr [r(t)(q(t) − ρ)] + On(1) ,

(cid:17)(cid:0)

n,t,(cid:15)

(cid:1)(cid:105)(cid:69)

1
2

where ∇ is the K-dimensional gradient w.r.t. the argument of uYt,µ(·), and On(1) → 0 in the n, m → ∞ limit
uniformly in t, q, r, (cid:15). Here, the K ×K overlap matrix Q is defined as

Qll(cid:48) ≡

W ∗

ilwil(cid:48) .

1
n

n

i=1
(cid:88)

(23)

We will plug this expression in identity (22), but in order to simplify it we need the following crucial
proposition, which says that the overlap concentrates. This property is what is generally refered to as a replica
symmetric behavior in statistical physics.

Proposition 5.3 (Overlap concentration). Assume that for any t ∈ (0, 1) the transformation (cid:15) ∈ (snM)2 (cid:55)→
(R1(t, (cid:15)), R2(t, (cid:15))) is a C1 diffeomorphism with a Jacobian determinant greater or equal to 1. Then one can find a
sequence sn going to 0 slowly enough such that there exists a constant C(ϕout, S, K, α) > 0 depending only on
the activation ϕout, the support S of the prior P0, the number of hidden neurons K and the sampling rate α, and
a constant γ > 0 such that ((cid:107) − (cid:107)F is the Frobenius norm):

1
Vol(snM)2

1

d(cid:15)

dt E

Q − E(cid:104)Q(cid:105)n,t,(cid:15)

(cid:90)(snM)2

0
(cid:90)

(cid:10)(cid:13)
(cid:13)

C(ϕout, S, K, α)
nγ

.

2
F

n,t,(cid:15) ≤
(cid:11)

(cid:13)
(cid:13)

The proof of this concentration result can be directly adapted from [45]. Using the results of [45] is
straightforward, under the assumption that (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism with a Jacobian determinant
greater or equal to 1. This Jacobian determinant can be computed from formula (30). To check that it is greater
than one we use Lemma 5.5 and need Assumption 1 stated in paragraph 5.3 below. With a Jacobian determinant
greater than one, we can “replace” (i.e., lower bound) the integrations over R1(t, (cid:15)), that naturally appear in
the proof of Proposition 5.3, by integrations over the perturbation matrix (cid:15). This is exactly what has been done
in the K = 1 version of the present model in [11] or in [46] i.e., in the scalar overlap case (see also [47] for a
setting with a matrix overlap as in the present case).

From there we can deduce the following fundamental sum rule which is at the core of the proof:

Proposition 5.4 (Fundamental sum rule). Assume that the interpolation functions r and q are such that the map
(cid:15) = ((cid:15)1, (cid:15)2) (cid:55)→ R(t, (cid:15)) = (R1(t, (cid:15)), R2(t, (cid:15))) given by (14) is a C1 diffeomorphism whose Jacobian determinant

14

Jn,(cid:15)(t) is greater or equal to 1. Assume that for all t ∈ [0, 1] and (cid:15) ∈ (snM)2 we have q(t) = q(t, (cid:15)) =
E(cid:104)Q(cid:105)n,t,(cid:15) ∈ S +

K(ρ). Then

fn =

1
Vol(snM)2

1

d(cid:15)

ψP0

r(t)dt

+ αΨPout

q(t, (cid:15))dt; ρ

(cid:90)(snM)2

(cid:110)

0
(cid:16) (cid:90)

(cid:17)

1

0
(cid:16) (cid:90)

−

1
2

0
(cid:90)

1

(cid:17)

Tr[q(t, (cid:15))r(t)]dt

+ On(1) .

(24)

(cid:111)

Proof. Let us denote Vn ≡ Vol(snM)2. The integral over (cid:15) is always over (snM)2. Consider the first term, i.e.
the Gibbs bracket, in the free entropy derivative given by Proposition 5.2. By the Cauchy-Schwarz inequality

E

Tr

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

Q − q(t)

(cid:16)

(cid:68)

d(cid:15)

1

(cid:104)(cid:16)
dt E

0
(cid:90)

≤

1
Vn (cid:90)

m

1
n

µ=1
(cid:88)
1
n

(cid:68)(cid:13)
(cid:13)
(cid:13)

m

µ=1
(cid:88)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:1)(cid:105)(cid:69)
×

n,t,(cid:15)

(cid:17)

1
Vn (cid:90)

1

0
(cid:90)

(cid:17)(cid:0)
(cid:124) − r(t)

2

F

n,t,(cid:15)

(cid:69)

(cid:13)
(cid:13)
(cid:13)

d(cid:15)

dt E

Q − q(t)

2
F

n,t,(cid:15) .

(cid:10)(cid:13)
(cid:13)

(cid:11)

(cid:13)
(cid:13)

The first term of this product is bounded by some constant C(ϕout, α) that only depend on ϕout and α, see
Lemma A.4 in sec. A.4. The second term is bounded by C(ϕout, S, K, α)n−γ by Proposition 5.3, since we
assumed that for all (cid:15) ∈ Bn and all t ∈ [0, 1] we have q(t) = q(t, (cid:15)) = E(cid:104)Q(cid:105)n,t,(cid:15). Therefore from Proposition
5.2 we obtain

1
Vn (cid:90)

d(cid:15)

0
(cid:90)

1

dfn,(cid:15)(t)
dt

dt =

1
2Vn (cid:90)

1

0
(cid:90)

d(cid:15)

Tr

q(t, (cid:15))r(t) − r(t)ρ

dt + On(1) + O(n−γ/2) .

(25)

(cid:2)
Here the small terms are both going to 0 uniformly w.r.t. to the choice of q and r. When replacing (25) in (22)
and combining it with (21) we reach the claimed identity.

(cid:3)

5.3 A technical lemma and an assumption

We give here a technical lemma used in the rest of the proof, and which allows us to detail the unproven
assumption on which we rely to prove Thm 3.1.

Lemma 5.5. The quantity E(cid:104)Q(cid:105)n,t,(cid:15) is a function of (n, t, R(t, (cid:15))). We define F
(2)
(2)
n ) is defined on the set:
n (t, R(t, (cid:15))) ≡ 2α∇ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)). Fn ≡ (F
F

(1)
n , F

(1)
n (t, R(t, (cid:15))) ≡ E(cid:104)Q(cid:105)n,t,(cid:15) and

Dn =

(t, r1, r2) ∈ [0, 1] × S +

K × S +

K

(cid:110)

(ρt − r2 + 2snIK) ∈ S +
K
(cid:12)
(cid:12)
(cid:12)

.

(cid:111)

Fn is a continuous function from Dn to S +
K × S +
and R2 on the interior of Dn. For every (t, R(t, (cid:15))) for which they are defined, they satisfy:

K(ρ). Moreover, Fn admits partial derivatives with respect to R1

(26)

(27)

We can now state the technical assumption on which we rely, and which essentially allows us to derive
that the map (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism with a Jacobian determinant greater or equal to 1 as it will
become clear in the next section:

(1)
∂(F
n )ll(cid:48)
∂(R1)ll(cid:48)

≥ 0.

K

l≤l(cid:48)
(cid:88)

15

Assumption 1. With the notations of Lemma 5.5,

(2)
∂(F
n )ll(cid:48)
∂(R2)ll(cid:48)

≥ 0.

K

l≤l(cid:48)
(cid:88)

Proof of Lemma 5.5. The fact that the image domain of Fn is S +
K(ρ) is known from Lemma A.2. The
continuity and differentiability of Fn follows from standard theorems of continuity and derivation under the
integral sign (recall that we are working at finite n). Indeed, the domination hypotheses are easily satisfied
since we work under (H1) and (H2).

K × S +

i≤j A(ij)(ij). Then one can write Tr[DR1F

Let us now prove (27). We write the formal differential of F

(1)
(1)
n with respect to R1 as DR1F
n , which
(1)
n ] ≥ 0, the trace of a 4-tensor over SK A(ij)(kl) being
is a 4-tensor, and our goal is to prove that Tr[DR1F
(1)
n ] = Tr[∇∇(cid:124)ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) × ∇R1
E(cid:104)Q(cid:105)n,t,(cid:15)]. We
Tr[A] =
know from Lemma A.2 and Lemma A.6 that ∇∇(cid:124)ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) is a positive symmetric matrix (when seen
E(cid:104)Q(cid:105)n,t,(cid:15) is also positive
as a linear operator over SK). Moreover, it is a known result that the derivative ∇R1
symmetric, since R1 is the matrix snr of a linear channel (see [42, 43, 44]). Since the product of two symmetric
positive matrices has always positive trace, this shows that Tr[DR1F

(1)
n ] ≥ 0.

(cid:80)

5.4 Matching bounds

Proposition 5.6 (Lower bound). Under Assumption 1, the free entropy of model (7) verifies

lim inf
n→∞

fn ≥ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

Proof. Choose first r(t) = r ∈ S +
the first order differential equation:

K a fixed matrix. Then R(t) = (R1(t), R2(t)) can be fixed as the solution to

d
dt

d
dt

R1(t) = r ,

R2(t) = E(cid:104)Q(cid:105)n,t,(cid:15) ,

and

R(0) = (cid:15) .

(28)

t
We denote this (unique) solution R(t, (cid:15)) = (rt + (cid:15)1,
0 q(v, (cid:15); r)dv + (cid:15)2). It is possible to check that this ODE
satisfies the hypotheses of the parametric Cauchy-Lipschitz theorem, and that by the Liouville formula the
determinant Jn,(cid:15)(t) of the Jacobian of (cid:15) (cid:55)→ R(t, (cid:15)) satisfies (see Lemma A.3 in sec. A)

(cid:82)

Jn,(cid:15)(t) = exp

(s, R(s, (cid:15))) ds

≥ 1 .

(29)

K

t

∂E(cid:104)Qll(cid:48)(cid:105)n,s,(cid:15)
∂(R2)ll(cid:48)

0
(cid:16) (cid:90)

l≥l(cid:48)
(cid:88)

Indeed, this sum of partial derivatives is always positive by Assumption 1. Moreover from (28), q(t, (cid:15); r) =
E(cid:104)Q(cid:105)n,t,(cid:15), which is in S +
K by Lemma A.2 in sec. A. The fact that the map (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism is
easily verified by its bijectivity (from the positivity of Jn,(cid:15)(t)) combined with the local inversion Theorem. All
the assumptions of Proposition 5.4 are veri.i.d. which then implies, recalling the potential expression (9),

fn =

1
Vol(snM)2

(cid:90)(snM)2

1

0
(cid:16) (cid:90)

d(cid:15) fRS

q(v, (cid:15); r)dv, r

+ On(1) .

This implies the lower bound as this equality is true for any r ∈ S +
K.

(cid:17)

(cid:17)

16

Proposition 5.7 (Upper bound). Under Assumption 1, the free entropy of model (7) verifies

lim sup
n→∞

fn ≤ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

Proof. We now fix R(t) = (R1(t), R2(t)) as the solution R(t, (cid:15)) = (
the following Cauchy problem:

t
0 r(v, (cid:15))dv + (cid:15)1,

t
0 q(v, (cid:15))dv + (cid:15)2) to

(cid:82)

(cid:82)

d
dt

R1(t) = 2α∇ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) ,

R2(t) = E(cid:104)Q(cid:105)n,t,(cid:15) ,

and

R(0) = (cid:15) .

d
dt

We denote this equation as ∂tR(t) = Fn(t, R(t)), R(0) = (cid:15). It is then possible to verify that Fn(R(t), t)
is a bounded C1 function of R(t), and thus a direct application of the Cauchy-Lipschitz theorem implies
that R(t, (cid:15)) is a C1 function of t and (cid:15). The Liouville formula for the Jacobian determinant of the map
(cid:15) ∈ (snM)2 (cid:55)→ R(t, (cid:15)) ∈ R(t, (snM)2) gives this time (see Lemma A.3 in sec. A)

Jn,(cid:15)(t) = exp

(s, R(s, (cid:15))) +

(s, R(s, (cid:15)))

ds

≥ 1 .

(30)

K

t

∂(Fn,1)ll(cid:48)
∂(R1)ll(cid:48)

0
(cid:16) (cid:90)

l≥l(cid:48) (cid:110)
(cid:88)

∂(Fn,2)ll(cid:48)
∂(R2)ll(cid:48)

(cid:111)

(cid:17)

The fact that this determinant is greater or equal to 1 for all t ∈ [0, 1] follows again from the positivity of this
sum of partials, see Lemma 5.5 and Assumption 1. Identity (30) implies the bijectivity of (cid:15) (cid:55)→ R(t, (cid:15)) which,
combined with the local inversion theorem, makes it a diffeomorphism. Since E(cid:104)Q(cid:105)n,t,(cid:15) and ρ − E(cid:104)Q(cid:105)n,t,(cid:15) are
positive matrices (see Lemma A.2 in sec. A) we also have that q(t, (cid:15)) ∈ S +
K(ρ) and since by the differential
equation we have r(t, (cid:15)) = 2α∇ΨPout(q(t, (cid:15))) and as ∇ΨPout(q) ∈ S +
K (see Lemma A.6 in sec. A), then
r(t, (cid:15)) ∈ S +

K too. We have everything needed for applying Proposition 5.4 again which gives in this case

fn =

1
Vol(snM)2

1

d(cid:15)

ψP0
(cid:110)

0
(cid:16) (cid:90)

(cid:90)

r(v, (cid:15))dv

+αΨPout

q(v, (cid:15))dv; ρ

−

Tr

q(v, (cid:15))r(v, (cid:15))dv

+On(1).

(cid:17)

(cid:17)

(cid:111)

1

0
(cid:16) (cid:90)

Then by convexity of ψP0 and ΨPout (see Lemma A.6),

fn ≤

=

1
Vol(snM)2
1
Vol(snM)2

d(cid:15)

d(cid:15)

1

0
(cid:90)

1

0
(cid:90)

(cid:90)

(cid:90)

We now remark that

dv

ψP0(r(v, (cid:15))dv) + αΨPout(q(v, (cid:15)); ρ) −
(cid:110)

dv fRS(q(v, (cid:15)), r(v, (cid:15))) + On(1) .

Tr[q(v, (cid:15))r(v, (cid:15))]

+ On(1)

(cid:111)

fRS(q(v, (cid:15)), r(v, (cid:15))) = inf
q∈S+

K (ρ)

fRS(q, r(v, (cid:15))) .

Indeed, for every r ∈ S +
K(ρ) (cid:55)→ fRS(q, r) ∈ R (recall (9)) is convex (by Lemma A.6),
and its q-derivative is ∇gr(q) = α∇ΨPout(q) − r/2. Since ∇gr(v,(cid:15))(q(v, (cid:15))) = 0 by definition of r(v, (cid:15)), and
S +
K(ρ) is convex, the minimum of gr(v,(cid:15))(q) is necessarily achieved at q = q(v, (cid:15)). Therefore

K, the function gr : q ∈ S +

fn ≤

1
Vol(snM)2

1

d(cid:15)

(cid:90)(snM)2

0
(cid:90)

dv inf
q∈S+

K (ρ)

fRS (q, r(v, (cid:15))) + On(1) ≤ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) + On(1),

which concludes the proof of Proposition 5.7.

Combining these two matching bounds ends the proof of Theorem 3.1.

1

0
(cid:90)

1
2

1
2

17

6 Discussion

One of the contributions of this paper is the design of an AMP-type algorithm that is able to achieve the
Bayes-optimal learning error in the limit of large dimensions for a range of parameters out of the so-called
hard phase. The hard phase is associated with first order phase transitions appearing in the solution of the
model. In the case of the committee machine with a large number of hidden neurons we identify a large hard
phase in which learning is possible information-theoretically but not efficiently. In other problems where
such a hard phase was identified, its study boosted the development of algorithms that are able to match the
predicted threshold. We anticipate this will also be the same for the present model. We should, however, note
that for larger K > 2 the present AMP algorithm includes higher-dimensional integrals that hamper the speed
of the algorithm. Our current strategy to tackle this is to combine the large-K expansion and use it in the
algorithm. Detailed account of the corresponding results are left for future work.

We studied the Bayes-optimal setting where the student-network is the same as the teacher-network, for
which the replica method can be readily applied. The method still applies when the number of hidden units in
the student and teacher are different, while our proof does not generalize easily to this case. It is an interesting
subject for future work to see how the hard phase evolves under over-parametrization and what is the interplay
between the simplicity of the loss-landscape and the achievable generalization error. We conjecture that in
the present model over-parametrization will not improve the generalization error achieved by AMP in the
Bayes-optimal case.

Even though we focused in this paper on a two-layers neural network, the analysis and algorithm can be
readily extended to a multi-layer setting, see [22], as long as the number of layers as well as the number of
hidden neurons in each layer is held constant, and as long as one learns only weights of the first layer, for which
the proof already applies. The numerical evaluation of the phase diagram would be more challenging than
the cases presented in this paper as multiple integrals would appear in the corresponding formulas. In future
works, we also plan to analyze the case where the weights of the second and subsequent layers (including the
biases of the activation functions) are also learned. This could be done for instance with a combination of EM
and AMP along the lines of [48, 49] where this is done for the simpler single layer case.

Concerning extensions of the present work, an important open case is the one where the number of
samples per dimension α = Θ(1) and also the size of the hidden layer per dimension K/n = Θ(1) as n → ∞,
while in this paper we treated the case K = Θ(1) and n → ∞. This other scaling where K/n = Θ(1) is
challenging even for the non-rigorous replica method.

Acknowledgments

This work has been supported by the ERC under the European Union’s FP7 Grant Agreement 307087-SPARCS
and the European Union’s Horizon 2020 Research and Innovation Program 714608-SMiLe, as well as by the
French Agence Nationale de la Recherche under grant ANR-17-CE23-0023-01 PAIL and the Swiss National
Foundation grant no 200021E-175541. Additional funding is acknowledged by A.M., F.K. and J.B. from “Chaire
de recherche sur les modèles et sciences des données”, Fondation CFM pour la Recherche-ENS. We also
acknowledge Léo Miolane for discussions.

References

[1] V. Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.

18

[2] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

[3] S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Physical

[4] T. L. Watkin, A. Rau, and M. Biehl. The statistical mechanics of learning a rule. Reviews of Modern Physics,

Review A, 45(8):6056, 1992.

65(2):499, 1993.

[5] R. Monasson and R. Zecchina. Learning and generalization theories of large committee-machines. Modern

Physics Letters B, 9(30):1887–1897, 1995.

[6] R. Monasson and R. Zecchina. Weight space structure and internal representations: a direct approach to
learning and generalization in multilayer neural networks. Physical review letters, 75(12):2432, 1995.

[7] A. Engel and C. P. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press, 2001.

[8] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking

generalization. arXiv preprint arXiv:1611.03530, 2016. in ICLR 2017.

[9] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. Chayes, L. Sagun, and
R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838,
2016. in ICLR 2017.

[10] C. H. Martin and M. W. Mahoney. Rethinking generalization requires revisiting old ideas: statistical

mechanics approaches and complex learning behavior. arXiv preprint arXiv:1710.09553, 2017.

[11] J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborová. Optimal errors and phase transitions in
high-dimensional generalized linear models. Proceedings of the National Academy of Sciences, 116(12):5451–
5460, 2019.

[12] M. Baity-Jesi, L. Sagun, M. Geiger, S. Spigler, G. Ben-Arous, C. Cammarota, Y. LeCun, M. Wyart, and G. Bir-
oli. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint arXiv:1803.06969,
2018.

[13] M. Mézard, G. Parisi, and M. Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method

and Its Applications, volume 9. World Scientific Publishing Company, 1987.

[14] M. Mézard and A. Montanari. Information, physics, and computation. Oxford University Press, 2009.

[15] D. L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing.

Proceedings of the National Academy of Sciences, 106(45):18914–18919, 2009.

[16] S. Rangan. Generalized approximate message passing for estimation with random linear mixing. In
Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pages 2168–2172. IEEE,
2011.

[17] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to

compressed sensing. IEEE Transactions on Information Theory, 57(2):764–785, 2011.

[18] A. Javanmard and A. Montanari. State evolution for general approximate message passing algorithms,
with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144, 2013.

19

[19] H. Schwarze. Learning a rule in a multilayer neural network. Journal of Physics A: Mathematical and

General, 26(21):5781, 1993.

20(4):375, 1992.

Letters), 21(7):785, 1993.

[20] H. Schwarze and J. Hertz. Generalization in a large committee machine. EPL (Europhysics Letters),

[21] H. Schwarze and J. Hertz. Generalization in fully connected committee machines. EPL (Europhysics

[22] G. Mato and N. Parga. Generalization properties of multilayered neural networks. Journal of Physics A:

Mathematical and General, 25(19):5047, 1992.

[23] D. Saad and S. A. Solla. On-line learning in soft committee machines. Physical Review E, 52(4):4225, 1995.

[24] J. Barbier and N. Macris. The adaptive interpolation method: a simple scheme to prove replica formulas

in bayesian inference. Probability Theory and Related Fields, pages 1–53, 2018.

[25] D. L. Donoho, I. Johnstone, and A. Montanari. Accurate prediction of phase transitions in compressed
sensing via a connection to minimax denoising. IEEE transactions on information theory, 59(6):3396–3433,
2013.

[26] L. Zdeborová and F. Krzakala. Statistical physics of inference: thresholds and algorithms. Advances in

Physics, 65(5):453–552, 2016.

[27] Y. Deshpande and A. Montanari. Finding hidden cliques of size \sqrt {N/e} n/e in nearly linear time.

Foundations of Computational Mathematics, 15(4):1069–1128, 2015.

[28] A. S. Bandeira, A. Perry, and A. S. Wein. Notes on computational-to-statistical gaps: predictions using

statistical physics. arXiv preprint arXiv:1803.11132, 2018.

[29] I. Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv

preprint arXiv:1712.08968, 2017.

[30] A. E. Alaoui, A. Ramdas, F. Krzakala, L. Zdeborová, and M. I. Jordan. Decoding from pooled data: Sharp

information-theoretic bounds. arXiv preprint arXiv:1611.09981, 2016.

[31] A. El Alaoui, A. Ramdas, F. Krzakala, L. Zdeborová, and M. I. Jordan. Decoding from pooled data: Phase
transitions of message passing. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages
2780–2784. IEEE, 2017.

[32] J. Zhu, D. Baron, and F. Krzakala. Performance limits for noisy multimeasurement vector problems. IEEE

Transactions on Signal Processing, 65(9):2444–2454, 2017.

[33] F. Guerra. Broken replica symmetry bounds in the mean field spin glass model. Communications in

mathematical physics, 233(1):1–12, 2003.

[34] M. Talagrand. Spin glasses: a challenge for mathematicians: cavity and mean field models, volume 46.

Springer Science & Business Media, 2003.

[35] D. J. Thouless, P. W. Anderson, and R. G. Palmer. Solution of’solvable model of a spin glass’. Philosophical

Magazine, 35(3):593–601, 1977.

[36] M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method.

Journal of Physics A: Mathematical and General, 22(12):2181–2190, 1989.

20

[37] M. Opper and O. Winther. Mean field approach to bayes learning in feed-forward neural networks.

Physical review letters, 76(11):1964, 1996.

[38] Y. Kabashima. Inference from correlated patterns: a unified theory for perceptron learning and linear

vector channels. Journal of Physics: Conference Series, 95(1):012001, 2008.

[39] C. Baldassi, A. Braunstein, N. Brunel, and R. Zecchina. Efficient supervised learning in networks with

binary synapses. Proceedings of the National Academy of Sciences, 104(26):11079–11084, 2007.

[40] B. Aubin, A. Maillard, J. Barbier, F. Krzakala, N. Macris, and L. Zdeborová. AMP implementation of the

committee machine. https://github.com/benjaminaubin/TheCommitteeMachine, 2018.

[41] P. Schniter, S. Rangan, and A. K. Fletcher. Vector approximate message passing for the generalized linear
model. In Signals, Systems and Computers, 2016 50th Asilomar Conference on, pages 1525–1529. IEEE, 2016.

[42] G. Reeves, H. D. Pfister, and A. Dytso. Mutual information as a function of matrix snr for linear gaussian
channels. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 1754–1758. IEEE,
2018.

[43] M. Payaró, M. Gregori, and D. Palomar. Yet another entropy power inequality with an application. In
Wireless Communications and Signal Processing (WCSP), 2011 International Conference on, pages 1–5. IEEE,
2011.

[44] M. Lamarca. Linear precoding for mutual information maximization in mimo systems.

In Wireless

Communication Systems, 2009. ISWCS 2009. 6th International Symposium on, pages 26–30. IEEE, 2009.

[45] J. Barbier. Overlap matrix concentration in optimal bayesian inference. arXiv preprint arXiv:1904.02808,

2019.

[46] J. Barbier and N. Macris. The adaptive interpolation method for proving replica formulas. applications to
the curie-weiss and wigner spike models. Journal of Physics A: Mathematical and Theoretical, 2019.

[47] J. Barbier, C. Luneau, and N. Macris. Mutual information for low-rank even-order symmetric tensor

factorization. arXiv preprint arXiv:1904.04565, 2019.

[48] F. Krzakala, M. Mézard, F. Sausset, Y. Sun, and L. Zdeborová. Probabilistic reconstruction in compressed
sensing: algorithms, phase diagrams, and threshold achieving matrices. Journal of Statistical Mechanics:
Theory and Experiment, 2012(08):P08009, 2012.

[49] U. Kamilov, S. Rangan, M. Unser, and A. K. Fletcher. Approximate message passing with consistent
parameter estimation and applications to sparse learning. In Advances in Neural Information Processing
Systems, pages 2438–2446, 2012.

[50] P. Hartman. Ordinary Differential Equations: Second Edition. Classics in Applied Mathematics. Society for
Industrial and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104), 1982.

[51] E. Gardner and B. Derrida. Optimal storage properties of neural network models. Journal of Physics A:

Mathematical and general, 21(1):271, 1988.

[52] J. Barbier, N. Macris, M. Dia, and F. Krzakala. Mutual information and optimality of approximate

message-passing in random linear estimation. arXiv preprint arXiv:1701.05823, 2017.

[53] M. Opper and W. Kinzel. Statistical mechanics of generalization. In Models of neural networks III, pages

151–209. Springer, 1996.

21

[54] J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse super-

position codes. IEEE Transactions on Information Theory, 63:4894–4927, 2017.

[55] M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

[56] M. Bayati, M. Lelarge, A. Montanari, et al. Universality in polytope phase transitions and message passing

algorithms. The Annals of Applied Probability, 25(2):753–822, 2015.

22

Supplementary material

A Proof details for Theorem 3.1

A.1 The Nishimori property in Bayes-optimal learning

We first state an important property of the Bayesian optimal setting (that is when all hyper-parameters of the
problem are assumed to be known), that is used several times, and is often refered to as the Nishimori identity.

Proposition A.1 (Nishimori identity). Let (X, Y ) ∈ Rn1 × Rn2 be a couple of random variables. Let k ≥ 1 and
let X (1), . . . , X (k) be k i.i.d. samples (given Y ) from the conditional distribution P (X = · |Y ), independently of
every other random variables. Let us denote (cid:104)−(cid:105) the expectation operator w.r.t. P (X = · |Y ) and E the expectation
w.r.t. (X, Y ). Then, for all continuous bounded function g we have

E(cid:104)g(Y, X (1), . . . , X (k))(cid:105) = E(cid:104)g(Y, X (1), . . . , X (k−1), X)(cid:105) .

(31)

Proof. This is a simple consequence of Bayes formula. It is equivalent to sample the couple (X, Y ) according
to its joint distribution or to sample first Y according to its marginal distribution and then to sample X
conditionally to Y from its conditional distribution P (X = · |Y ). Thus the (k + 1)-tuple (Y, X (1), . . . , X (k))
is equal in law to (Y, X (1), . . . , X (k−1), X). This proves the proposition.

As a first application of Proposition A.1 we prove the following Lemma which is used in the proof of the

upper bound Proposition 5.7.
Lemma A.2 (Positivity of some matrices). The matrices ρ, E(cid:104)Q(cid:105) and ρ − E(cid:104)Q(cid:105) are positive definite, i.e. in S +
K
In the application the Gibbs bracket is (cid:104)−(cid:105)n,t,(cid:15).
Proof. The statement for ρ follows from its definition (in Theorem 3.1). Note for further use that we also
have ρ = 1
ilwil(cid:48) in matrix notation we have Q =
n
n
i=1 W ∗
1
i w
n

i )(cid:124)]. Since by definition Qll(cid:48) ≡ 1
n
E[W ∗
i=1 W ∗
(cid:124)
i . An application of the Nishimori identity shows that

i (W ∗

n

.

(cid:80)

which is obviously in S +

E(cid:104)Q(cid:105) =

n

1
n

i=1
(cid:88)
K. Finally we note that

n

1
n

i=1 (cid:16)
(cid:88)

(cid:80)

1
n

n

i=1
(cid:88)

=

1
n

(cid:124)
i (cid:105)]
(cid:17)

n

i=1
(cid:88)

E[ρ − (cid:104)Q(cid:105)] =

E[W ∗

i (W ∗
i )

] − E[(cid:104)wi(cid:105)(cid:104)w

(cid:124)

E[(W ∗

i − (cid:104)wi(cid:105))((W ∗
i )

(cid:124) − (cid:104)w

(cid:124)
i (cid:105))]

E(cid:104)W ∗

i w

(cid:124)
i (cid:105) =

E[(cid:104)wi(cid:105)(cid:104)w

(cid:124)
i (cid:105)]

(32)

where the last equality is proved by an application of the Nishimori identity again. This last expression is
obviously in S +

K, i.e. E(cid:104)Q(cid:105) ∈ S +

K(ρ).

A.2 Setting in the Hamiltonian language

We set up some notations which will shortly be useful. Let uy(x) ≡ ln Pout(y|x). Here x ∈ RK and y ∈ R.
We will denote by ∇uy(x) the K-dimensional gradient w.r.t. x, and ∇∇(cid:124)uy(x) the K × K matrix of second
derivatives (the Hessian) w.r.t. x. Moreover ∇Pout(y|x) and ∇∇(cid:124)Pout(y|x) also denote the K-dimensional
gradient and Hessian w.r.t. x. We will also use the matrix identity

∇∇(cid:124)

uYµ(x) + ∇uYµ(x)∇(cid:124)

uYµ(x) =

∇∇(cid:124)Pout(Yµ|x)
Pout(Yµ|x)

.

(33)

23

t ∈ Rn×K, X ∈ Rm×n, V ∈ Rm×K,
Finally we will use the matrices w ∈ Rn×K, u ∈ Rm×K, Yt ∈ Rm, Y (cid:48)
W ∗ ∈ Rn×K and U ∗ ∈ Rm×K. Like in sec. 5 we adopt the convention that all underlined vectors are
K-dimensional, like e.g. uµ, U µ, V µ and Y (cid:48)

t,i.

It is convenient to reformulate the expression of the interpolating free entropy fn,(cid:15)(t) in the Hamiltonian

language. We introduce an interpolating Hamiltonian:

Ht(w, u; Yt, Y (cid:48)

t , X, V ) ≡ −

uYt,µ(st,µ) +

(cid:107)Y (cid:48)

t,i − R1(t)1/2 wi(cid:107)2
2

(34)

m

µ=1
(cid:88)

1
2

n

i=1
(cid:88)

where recall that

n

st,µ ≡

1 − t
n

(cid:114)

Xµiwi +

R2(t) V µ +

tρ − R2(t) + 2snIK×K uµ .

(35)

i=1
(cid:88)
The expression of Ht(W ∗, U ∗; Yt, Y (cid:48)
(35) replaced by St,µ given by (15). The average free entropy (18) at time t then reads

(cid:112)

(cid:112)

t , X, V ) is similar to (34), but with w replaced by W ∗ and st,µ given by

fn,(cid:15)(t) ≡

E ln

1
n

dP0(w)

Rn×K

(cid:90)

(cid:90)
µl/2 and dP0(w) =

Rm×K

Du e−Ht(w,u;Yt,Y (cid:48)

t ,X,V )

(36)

where Du =
in the simplest manner it is fruitful to represent the expectations over W ∗, U, Y, Y (cid:48) explicitly as integrals:

K
l=1 dwil. To develop the calculations

n
i=1 P0(wi)

l=1(2π)−1/2e−u2

m
µ=1

K

(cid:81)

(cid:81)

fn,(cid:15)(t) =

EX,V

dYtdY (cid:48)

t dP0(W ∗)DU ∗e−Ht(W ∗,U ;Yt,Y (cid:48)

t ,X,V ) ln

dP0(w)Du e−Ht(w,u;Yt,Y (cid:48)

t ,X,V ).

(37)

(cid:81)

(cid:81)

1
n

(cid:90)

(cid:90)

A.3 Free entropy variation: Proof of Proposition 5.2

The proof provided here follows very closely the one in [11] for the case K = 1, so we are more brief and
refer to this paper for more details. We first prove that for all t ∈ (0, 1)

dfn,(cid:15)(t)
dt

= −

Tr

E

1
2

(cid:68)

m

1
n

(cid:104)(cid:16)

+

1
2

µ=1
(cid:88)
Tr[r(t)(q(t) − ρ)] −

An
2

,

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

W ∗

i w

(cid:124)
i − q(t)

n

1
n

(cid:17)(cid:16)

i=1
(cid:88)

n,t,(cid:15)

(cid:17)(cid:69)

(38)

where

Tr

An = E
(cid:104)

(cid:104)

1
√
n

m

µ=1
(cid:88)

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

1
√
n

n

(cid:16)

i=1
(cid:88)

(W ∗

i (W ∗
i )

(cid:124) − ρ)

ln Zn,(cid:15)(t)

.

(39)

(cid:105)

Once this is done, we show that An goes to 0 as n → ∞ uniformly in t ∈ [0, 1] in order to conclude the proof.

The Hamiltonian (34) t-derivative evaluated at the ground-truth matrices is given by

dHt
dt

(W ∗, U ∗;Yt, Y (cid:48)

t , X, V ) = −

∇(cid:124)

uYt,µ(St,µ)

m

µ=1
(cid:88)
∇(cid:124)

m

= −

Tr

dSt,µ
dt

µ=1
(cid:88)

(cid:104)

uYt,µ(St,µ)
(cid:105)

n

dSt,µ
dt

−

dR1(t)1/2
dt

n

−

Tr

i=1
(cid:88)

(cid:104)(cid:16)

i=1 (cid:16)
(cid:88)
dR1(t)1/2
dt

(cid:124)

(cid:17)

W ∗
i

(Y (cid:48)

t,i − R1(t)1/2W ∗
i )

(Y (cid:48)

t,i − R1(t)1/2W ∗

i )W ∗(cid:124)

i

(40)

(cid:105)

1
n

(cid:17)(cid:105)

(cid:124)

(cid:17)

24

(where we used that R1(t) is symmetric). The t-derivative of fn,(cid:15)(t) thus reads, for 0 < t < 1,

(W ∗, U ∗; Yt, Y (cid:48)

t , X, V ) ln Zn,(cid:15)(t)

−

(w, u; Yt, Y (cid:48)

t , X, V )

(41)

T1

(cid:123)(cid:122)

E

1
n

dHt
dt

(cid:68)

(cid:105)

(cid:125)

(cid:124)

T2

(cid:123)(cid:122)

.

n,t,(cid:15)

(cid:69)

(cid:125)

First, we note that T2 = 0. This is a direct consequence of the Nishimori identity Proposition A.1:

(w, u; Yt, Y (cid:48)

t , X, V )

(W ∗, U ∗; Yt, Y (cid:48)

t , X, V ) = 0 .

(42)

=

1
n

E dHt
dt

n,t,(cid:15)

(cid:69)

We now compute T1. Starting from (40) and considering the first term only (recall also the expression (15)

dfn,(cid:15)(t)
dt

= −

E

1
n

dHt
dt

(cid:104)

(cid:124)

T2 =

E

1
n

dHt
dt

(cid:68)

for St,µ),

E

Tr

(cid:104)

dSt,µ
dt

∇(cid:124)

(cid:104)
+

d
dt

(cid:112)

uYt,µ(St,µ)

R2(t)V µ +

ln Zn,(cid:15)(t)

−

Tr

= E
2
(cid:80)
(cid:104)
tρ − R2(t) + 2snIK×K U ∗
(cid:112)
µ

(cid:104)(cid:110)

(cid:105)

(cid:105)
d
dt

(cid:112)

n
i=1 XµiW ∗
i
n(1 − t)
∇(cid:124)
(cid:111)

uYt,µ(St,µ)
(cid:105)

ln Zn,(cid:15)(t)

.

(43)

(cid:105)

We then compute the first line of the right-hand side of (43). By Gaussian integration by parts w.r.t. Xµi (recall
hypothesis (H3)), and using the identity (33), we find after some algebra

1

−

2

n(1 − t)

(cid:112)

= −

n

i=1
(cid:88)
1
n

(cid:104)

Tr

E

Tr

(cid:104)
E

(cid:104)

Tr

1
2

−

(cid:104)
1
E
2

n

i=1
(cid:88)
1
n

(cid:68)

(cid:104)

i=1
(cid:88)

XµiW ∗

i ∇(cid:124)

n

W ∗

i W

(cid:124)
i

ln Zn,(cid:15)(t)

uYt,µ(St,µ)
(cid:105)
∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

(cid:105)

W ∗

i w

(cid:124)

i ∇uYt,µ(St,µ)∇(cid:124)

uYt,µ(st,µ)

ln Zn,(cid:15)(t)

(cid:105)

(cid:105)

.

n,t,(cid:15)

(cid:105)(cid:69)

(44)

(45)

Similarly for the second line of the right hand side of (43), we use again Gaussian integrations by parts but
this time w.r.t. V µ, U ∗
µ which have i.i.d. N (0, 1) entries. This calculation has to be done carefully with the
help of the matrix identity

d
dt

M (t) =

M (t)

(cid:112)

d

M (t)
dt

d

+

M (t)
dt

(cid:112)

(cid:112)

M (t)

(cid:112)

for any M (t) ∈ S +
t
0 (ρ − q(s))ds, as well as the identity (33), we reach after some algebra

K, and the cyclicity and linearity of the trace. Applying (45) to M (t) equal to

t
0 q(s)ds and

(cid:82)

(cid:82)

E

Tr

(cid:104)(cid:16)
ρ

(cid:104)
= E
Tr
(cid:104)

(cid:104)

d
d
R2(t)V µ +
dt
dt
∇∇(cid:124)Pout(Yt,µ|Sµ,t)
(cid:112)
Pout(Yt,µ|Sµ,t)

(cid:112)

(cid:105)

tρ − R2(t) + 2snIK×K U ∗
µ

∇(cid:124)

ln Zn,(cid:15)(t)

uYµ(Sµ,t)
(cid:105)
uYt,µ(sµ,t)

(cid:105)

(cid:17)

q(t)∇uYt,µ(Sµ,t)∇(cid:124)

ln Zn,(cid:15)(t)

(cid:105)

Tr

+ E
(cid:104)
(cid:68)
R1(t))(cid:124)(Y (cid:48)

.

n,t,(cid:15)

(cid:105)(cid:69)

(46)

As seen from (40), (41) it remains to compute E[Tr[( d
dt
that Y (cid:48)
i = Z(cid:48)
t,i −
one obtains

R1(t)W ∗

(cid:112)

] ln Zn,(cid:15)(t)]. Recall
t,i −
i ∼ N (0, IK×K). Using Gaussian integration by parts as well as the identity (45)

i

(cid:112)

R1(t)W ∗

i )W ∗(cid:124)

(cid:112)
d
dt

(cid:124)

(cid:17)

E

Tr

R1(t)

(Y (cid:48)

t,i −

R1(t)W ∗

i )W ∗(cid:124)

i

ln Zn,(cid:15)(t)

= −Tr

R1(t)

ρ − E(cid:104)W ∗

j wj(cid:105)n,t,(cid:15)

.

(47)

(cid:104)

(cid:104)(cid:16)

(cid:112)

(cid:112)

(cid:105)

(cid:105)

(cid:104)(cid:112)

(cid:0)

(cid:1)(cid:105)

25

Finaly the term T1 is obtained by putting together (43), (44), (46) and (47).

It now remains to check that An → 0 as n → +∞ uniformly in t ∈ [0, 1]. The proof from [11] (Appendix
C.2) can easily be adapted so we give here just a few indications for the ease of the reader. First one notices that

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yµ|St,µ)

E

(cid:104)

(cid:12)
(cid:12)
(cid:12)

W ∗, {St,µ}m

µ=1

=

dYµ∇∇(cid:124)

Pout(Yt,µ|St,µ) = 0 ,

(cid:105)

(cid:90)

so that by the tower property of the conditional expectation one gets

E

Tr

1
√
n

m

µ=1
(cid:88)

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

1
√
n

n

i=1
(cid:88)

(W ∗

i (W ∗
i )

(cid:124) − ρ)

= 0 .

(cid:104)

(cid:104)

(cid:16)
Next, one shows by standard second moment methods that E[(ln Zn,(cid:15)(t)/n − fn,(cid:15)(t))2] → 0 as n → +∞
uniformly in t ∈ [0, 1] (see [11] for the proof at K = 1, that generalizes straightforwardly for any finite K).
Then, using this last fact together with (49), and under hypotheses (H1), (H2), (H3), an easy application of the
(cid:3)
Cauchy-Schwarz inequality implies An → 0 as n → +∞ uniformly in t ∈ [0, 1]. This ends the proof.

(cid:17)(cid:105)(cid:105)

(48)

(49)

A.4 Technical lemmas

Lemma A.3 (Cauchy-Lipschitz Theorem and Liouville Formula). Let

F :

[0, 1] × (0, +∞)d → [0, +∞)d
(cid:55)→ F (t, z)

(t, z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

be a continuous, bounded function. Assume that F admits continuous partial derivatives ∂F
∂zi
domain of definition. Then, for all (cid:15) ∈ (0, +∞)d, the Cauchy problem

(i = 1, . . . , d) on its

y(0) = (cid:15)

and

y(cid:48)(t) = F

t, y(t)

(50)

admits a unique solution t (cid:55)→ y(t, (cid:15)). For all t ∈ [0, 1], the mapping zt : (cid:15) (cid:55)→ y(t, (cid:15)) is a diffeomorphism of class
C1, from (0, +∞)d to zt

. Moreover the determinant J(zt)((cid:15)) of the Jacobian of zt at (cid:15) verifies

(0, +∞)d

(cid:0)

(cid:1)

(cid:0)

(cid:1)

J(zt)((cid:15)) = det

∂yi
∂(cid:15)j

d

t

∂Fi
∂zi

(cid:16)(cid:16)

d
i=1

∂Fi
∂zi

i,j

(cid:17)

0
(cid:16) (cid:90)
(cid:17)
≥ 0 then J(zt)((cid:15)) ≥ 1 for all (cid:15).

i=1
(cid:88)

(cid:0)

(cid:17)

(cid:1)

= exp

s, y(s, (cid:15))

ds

.

(51)

Thus, in particular, if in addition

Proof. The existence and uniqueness of the solution of (50) follows from the classical Cauchy-Lipschitz
Theorem. The solution is indeed defined on all the segment [0, 1] because F is bounded.

(cid:80)

Theorem 3.1 from Chapter 5 in [50] gives that y admits continuous partial derivatives ∂y
∂(cid:15)i

for i = 1, . . . , d,

and Corollary 3.1 from Chapter 5 in the same reference states the Liouville formula (51).

By the Cauchy-Lipschitz Theorem, two solutions of y(cid:48)(t) = F

that are equal at some t ∈ [0, 1]
are equal everywhere. This implies that the mapping zt : (cid:15) (cid:55)→ y(t, (cid:15)) is injective, for all t ∈ [0, 1]. Since y
admits continuous partial derivatives in (cid:15)i, i = 1, . . . , d, we obtain that zt is of class C1 on (0, +∞)d. Now,
the equation (51) gives that J(zt)((cid:15)) > 0 for all (cid:15) ∈ (0, +∞)d. The local inversion Theorem gives then that zt
is a C1 diffeomorphism.

t, y(t)

(cid:0)

(cid:1)

26

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

Lemma A.4 (Boundedness of an overlap fluctuation). Under hypothesis (H2) one can find a constant C(ϕ, K, ∆) <
+∞ (independent of n, t, (cid:15)) such that for any Rn ∈ S +
K

we have

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124) − Rn

≤ 2Tr(R2

n) + α2C(ϕ, K, ∆).

(52)

We note that the constant remains bounded as ∆ → 0 and diverges as K → +∞.

Proof. It is easy to see that for symmetric matrices A, B we have Tr(A − B)2 ≤ 2(TrA2 + TrB2). Therefore

F

n,t,(cid:15)

(cid:69)

(cid:13)
(cid:13)
(cid:13)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124) − Rn

F

n,t,(cid:15)

≤ 2Tr(R2

Tr

n) + 2E
(cid:68)

1
n

(cid:16)

µ=1
(cid:88)

m

(cid:69)

(cid:13)
(cid:13)
(cid:13)
(cid:124)
∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

.

n,t,(cid:15)

(cid:17)

(cid:69)

(53)

In the rest of the argument we bound the second term of the r.h.s. Using the triangle inequality and then
Cauchy-Schwarz we obtain

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

(cid:124)
∇uYt,µ(st,µ)∇uYt,µ(St,µ)

F

n,t,(cid:15)

2

(cid:69)

(cid:13)
(cid:13)
(cid:13)

≤ E

1
n2

(cid:68)

m

µ=1
(cid:16)
(cid:88)
2
(cid:124)(cid:107)2

(cid:107)∇uYt,µ(st,µ)(cid:107)2(cid:107)∇uYt,µ(St,µ)

m

≤ E

1
n2

(cid:68)

(cid:16)

µ=1
(cid:88)

.

n,t,(cid:15)

(cid:17)

(cid:69)

From the random representation of the transition kernel,

(cid:107)∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124)(cid:107)F

n,t,(cid:15)

(cid:17)

(cid:69)

and thus

uYt,µ(s) = ln Pout(Yt,µ|x) = ln

dPA(aµ)

√

1
2π∆

e− 1

2∆ (Yt,µ−ϕ(x,aµ))2

(cid:90)

∇uYt,µ(x) =

dPA(aµ)(Yt,µ − ϕ(x, aµ))∇ϕ(x, aµ)e− 1
dPA(aµ)e− 1
2∆ (Yt,µ−ϕ(x,aµ))2

(cid:82)

2∆ (Yt,µ−ϕ(x,aµ))2

where ∇ϕ is the K-dimensional gradient w.r.t. the first argument x ∈ RK. From the observation model we get
|Yt,µ| ≤ sup |ϕ| +
∆|Zµ|, where the supremum is taken over both arguments of ϕ, and thus we immediately
obtain for all s ∈ RK

√

(cid:82)

(cid:107)∇uYt,µ(x)(cid:107) ≤ (2 sup |ϕ| +

∆|Zµ|) sup (cid:107)∇ϕ(cid:107) .

√

From (57) and (54) we see that it suffices to check that

m2
n2

(cid:2)(cid:0)

E

(2 sup |ϕ| + |Zµ|)2(sup (cid:107)∇ϕ(cid:107))2

≤ C(ϕ, K, ∆)

2

(cid:1)

(cid:3)

where C(ϕ, K, ∆) < +∞ is a finite constant depending only on ϕ, K, and ∆. This is easily seen by expanding
all squares and using that m/n → α. This ends the proof of Lemma A.4.

Lemma A.5 (Properties of ψP0). ψP0

is defined as the free entropy of the first auxiliary channel (3). We have,

(54)

(55)

(56)

(57)

27

for any r ∈ S +
K

:

ψP0(r) ≡ E ln

dwP0(w)eY

(cid:124)
0 r1/2w− 1

2 w(cid:124)rw.

RK

(cid:90)

Then ψP0

is convex and differentiable on S +
K

, with ∇ψP0(r) ∈ S +
K

for any r ∈ S +
K

.

Proof. Note that ψP0 is related to the mutual information I(W0; Y0) via the relation I(W0; Y0) = −ψP0(r) +
K
2 + 1
2 Tr[rρ]. It is then a known result (see [42, 43, 44]) that the derivative ∇rI(W0; Y0) is given by the
matrix-MMSE, i.e. ∇rI(W0; Y0) = 1
]). Using
2
E [(w − (cid:104)w(cid:105))(w − (cid:104)w(cid:105))(cid:124)], which is clearly a
the Nishimori identity Prop.A.1, we can write it as ∇rψP0(r) = 1
2
positive matrix. It is also known (see for instance Lemma 4 of [42]), that I(W0; Y0) is a concave function of r,
which implies that ψP0 is convex, which ends the proof.

]. This implies that ∇rψP0(r) = 1

2 (ρ − E[(cid:104)w(cid:105) (cid:104)w(cid:105)

E [(cid:104)w(cid:105) (cid:104)w(cid:105)

(cid:124)

(cid:124)

Lemma A.6 (Properties of ΨPout). Recall that ΨPout
(4). More precisely, for q ∈ S +

K(ρ), we have:

is defined as the free entropy of the second auxiliary channel

ΨPout(q) ≡ E ln

Y0|q1/2V + (ρ − q)1/2w

.

dw

e− 1
2 (cid:107)w(cid:107)2
(2π)K/2 Pout

RK

(cid:90)

Then ΨPout

is continuous and convex on S +

K(ρ). Also, ∇ΨPout(q) ∈ S +
Proof. The continuity and differentiability of ΨPout is easy, and exactly similar to the first part of the proof of
Proposition 18 of [11]; it just follows from the hypothesis (H2) which allows to use continuity and differentiation
under the expectation, because all the domination hypotheses are easily verified.

K

.

(cid:0)
(cid:101)
K(ρ), and twice differentiable inside S +

(cid:1)

One can compute the gradient and Hessian matrix of ΨPout(q), for q inside S +

K(ρ), using Gaussian
integration by parts and the Nishimori identity. The calculation is tedious and essentially follows the steps of
Y0|x). We define the average (cid:104)−(cid:105)sc (where sc stands for
Proposition 11 of [11]. Recall that u
“scalar channel”) as

(x) ≡ ln Pout(

(cid:101)Y0

(cid:104)g(w)(cid:105)sc ≡

RK DwPout(

(cid:82)

RK DwPout(
(cid:101)

(cid:82)

(cid:101)
Y0|(ρ − q)1/2w + q1/2V )g(w)
Y0|(ρ − q)1/2w + q1/2V )

,

for any continuous bounded function g. One arrives at:
(cid:101)

∇ΨPout(q) =

(ρ − q)1/2W ∗ + q1/2V

(ρ − q)1/2w + q1/2V

E

1
2

∇u

(cid:101)Y0

(cid:68)

(cid:16)

(cid:124)

.

sc

(cid:17)

(cid:69)

Note that this gradient is actually a symmetric matrix of size K × K, as it is a gradient w.r.t. q, which is itself
a matrix of size K. The Hessian ∇∇(cid:124)ΨPout with respect to q is thus a 4-tensor. One can compute in the same
way:

∇∇(cid:124)

ΨPout(q) =

Y0|(ρ − q)1/2w + q1/2V )

Y0|(ρ − q)1/2w + q1/2V )

∇∇(cid:124)Pout(
Pout(

(cid:101)

E

1
2

(cid:104)(cid:16)(cid:68)
−

(cid:68)

(cid:16)

∇u

(cid:101)Y0

(ρ − q)1/2W ∗ + q1/2V
(cid:101)

∇u

(ρ − q)1/2w + q1/2V

(cid:124)

⊗2

.

sc

(cid:17)

(cid:69)

(cid:17)

(cid:105)

In this expression, ⊗2 means the “tensorized square” of a matrix, i.e. for any matrix M of size K × K, M ⊗2 is
a 4-tensor with indices M ⊗2
= Ml0l1Ml2l3. From this expression, it is clear that the Hessian of ΨPout is
always positive, when seen as a matrix with rows and columns in SK, and thus ΨPout is convex, which ends
the proof of Lemma A.6.

l0l1l2l3

∇u

(cid:101)Y0

(cid:17)

(cid:16)

sc

(cid:69)

(cid:101)Y0

(cid:16)

(cid:17)

(58)

(59)

(60)

28

B Replica calculation

Our goal here is to provide an heuristic derivation of the replica formula of Theorem 3.1 using the replica
method, a powerful non-rigorous tool from statistical physics of disordered systems [13, 14]. This computation
is necessary to properly “guess” the formula that we then prove using the adaptive interpolation method. The
reader interested in the replica approach to neural networks and the commitee machine is invited to look as
well to some of the classical papers [51, 36, 20, 21, 19, 5].

The replica trick makes use of the formula, for a random variable x ∈ Rn and a strictly positive function

fn : Rn → R that depends on n:

lim
n→∞

1
n

E ln fn = lim
p→0+

lim
n→∞

1
np

ln Ef p
n.

(61)

Note that the inversion of the two limits here is non-rigorous. Computing the moments Ef p can often
be done for integers p ∈ N, and one can conjecture from it its value for every p > 0, before taking the limit
p → 0+ in (61) by analytical continuation of the value for integer p.

In our calculation, we will use this formula to compute the free entropy of our system, f ≡ limn→∞ fn.

We will thus need the moments of the partition function, for integer p:

EZ p

n = E

dw

P0 ({ wil

K
l=1

= E

dwa

P0 ({ wa
il

K
l=1

n

i=1
(cid:89)

Rn×RK





(cid:90)
p

Rn×RK





a=1 (cid:90)
(cid:89)

n

i=1
(cid:89)

p

m

(cid:9)

µ=1
(cid:89)

(cid:1)

Pout

Yµ



m


Pout

(cid:9)

µ=1
(cid:89)

(cid:1)

K

p

,

(cid:41)

l=1








K

Xµiwa
il

Xµiwil

n

1
√
n

(cid:40)

(cid:12)
(cid:12)
(cid:12)





Yµ

(cid:40)

(cid:12)
(cid:12)
(cid:12)

i=1
(cid:88)

1
√
n

n

i=1
(cid:88)

(cid:41)

l=1

.









The outer expectation is done over Xµi ∼ N (0, 1), w(cid:63) and Y . Writing w(cid:63) as w0 we have:

EZ p

n = EX

dY

dwa

P0

{wa

il}K
l=1

Rm

(cid:90)

Rn×RK

a=0 (cid:34) (cid:90)
(cid:89)

m

n

i=1
(cid:89)

(cid:0)
1
√
n

n

(cid:1)
Xµiwa
il

K



(cid:41)

l=1

.
(cid:35)

i=1
(cid:88)

×

Pout

Yµ



µ=1
(cid:89)

(cid:40)

(cid:12)
(cid:12)
(cid:12)


To perform the average over X, we notice that, since it is an i.i.d. standard Gaussian matrix, then for every
il follows a Gaussian multivariate distribution, with zero mean. This naturally

n
i=1 Xµiwa
a, µ, l, Za
leads to introduce its covariance tensor, which is equal to:

µl ≡ n−1/2



(cid:80)

EZa

µlZb

νl(cid:48) = δµνΣ al
bl(cid:48)

= δµνQal
bl(cid:48),

Qal

bl(cid:48) ≡

wa

ilwb

il(cid:48).

1
n

n

i=1
(cid:88)

(62)

(63)

For every a, b, Qa
functions for fixing Q, we arrive at :

b ∈ RK×K is the overlap matrix, and Σ is of size size (p + 1)K × (p + 1)K. Introducing δ

E [Z p

n] =

dQar
ar

dQar
br(cid:48)

Iprior({Qar

br(cid:48)}) × Ichannel({Qar

br(cid:48)})

,

(64)

R
(cid:89)(a,r) (cid:90)

R
(cid:89){(a,r);(b,r(cid:48))} (cid:90)

(cid:2)

(cid:3)

29

with:

Iprior({Qar

br(cid:48)}) =

dwaP0(wa)

p

a=0 (cid:20)(cid:90)
(cid:89)

Rn×K

p

Ichannel({Qar

br(cid:48)}) =

dY

dZa

Rm×K

n

1
n

δ

Qal

bl(cid:48) −

wa

ilwb
il(cid:48)

,

(cid:21)

(cid:32)


(cid:89){(a,l);(b,l(cid:48))}
p

Pout(Y |Za)e− m

i=1
(cid:88)
2 ln det Σ− mK(p+1)

2

(cid:33)

ln 2π

Rm

(cid:90)

exp

−

1
2





a=0 (cid:90)
(cid:89)
m

a=0
(cid:89)
µl(cid:48)(Σ−1) al

µlZb
Za

µ=1
(cid:88)

a,b
(cid:88)

l,l(cid:48)
(cid:88)

.

bl(cid:48)



By Fourier expanding the delta functions in Iprior, and performing a saddle-point method, one obtains:

in which (recall α ≡ limn→∞ m/n) :

lim
n→∞

1
n

ln E [Z p

n] = extrQ, ˆQ

H(Q, ˆQ)
(cid:105)
(cid:104)

,

H(Q, ˆQ) ≡

p

1
2

a=0
(cid:88)

l,l(cid:48)
(cid:88)

Qal
al

ˆQal

al −

1
2

a(cid:54)=b
(cid:88)

l,l(cid:48)
(cid:88)

Qal

bl(cid:48) ˆQal

bl(cid:48) + ln I + α ln J,

in which we defined:

p

I ≡

RK

a=0 (cid:90)
(cid:89)

p

J ≡

dy

R

(cid:90)

RK

a=0 (cid:90)
(cid:89)

dwaP0(wa) exp

−

ˆQal

al(cid:48)wa

l wa

l(cid:48) +

p

1
2

a=0
(cid:88)

l,l(cid:48)
(cid:88)





dZa
(2π)K(p+1)/2

Pout(y|Za)
det Σ

√

exp

−

1
2

1
2

p

a(cid:54)=b
(cid:88)

l,l(cid:48)
(cid:88)
K

ˆQal

bl(cid:48)wa

l wb
l(cid:48)

,




l(cid:48)(Σ−1) al

l Zb
Za





a,b=0
(cid:88)

l,l(cid:48)=1
(cid:88)

.

bl(cid:48)



Our goal is to express H(Q, ˆQ) as an analytical function of p, in order to perform the replica trick. To do
so, we will assume that the extremum of H is attained at a point in Q, ˆQ space such that a replica symmetry
property is verified. More concretely, we assume:

∃Q0 ∈ RK×K s.t

∀a ∈ [|0, p|] ∀(l, l(cid:48)) ∈ [|1, K|]2 Qal
∀(a < b) ∈ [|0, p|]2 ∀(l, l(cid:48)) ∈ [|1, K|]2 Qal

al(cid:48) = Q0
ll(cid:48),
bl(cid:48) = qll(cid:48),

∃q ∈ RK×K s.t

and samely for ˆQ0 and ˆq. Note that Q0 is by definition a symmetric matrix, while q is also symmetric by our
assumption of replica symmetry. Under this ansatz, we obtain:

H(Q0, ˆQ0, q, ˆq) =

Tr[Q0 ˆQ0] −

Tr[q ˆq] + ln I + α ln J.

(73)

p + 1
2

p(p + 1)
2

Remains now to compute an expression for I and J that is analytical in p, in order to take the limit p → 0+.
This can be done easily, using the identity, for any symmetric positive matrix M ∈ RK×K and any vector
x ∈ RK: exp (x(cid:124)(M/2)x) =
, in which Dξ is the standard Gaussian measure on RK.

ξ(cid:124)M 1/2x

RK Dξ exp

(cid:82)

(cid:0)

(cid:1)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

30

We obtain:

I =

Dξ

dw P0(w) exp

−

(cid:124)

w

( ˆQ0 + ˆq)w + ξ

ˆq1/2w

(cid:124)

p+1

,

1
2

RK

(cid:90)

RK

(cid:20)(cid:90)

(cid:20)
y|(Q0 − q)1/2Z + q1/2ξ

(cid:21)(cid:21)

p+1

.

J =

dy

Dξ

dZPout

R

(cid:90)

RK

(cid:90)

RK

(cid:20)(cid:90)

(cid:110)

(cid:111)(cid:21)

Our assumptions must be consistent in the sense that extrQ, ˆQ
In the p → 0+ limit, one easily gets J = 1 and I =
optimal overlap parameters satisfy ˆQ0 = 0 and Q0
the free entropy:

(cid:82)

limp→0+ H(Q, ˆQ)
2 w(cid:124) ˆQ0w0
(cid:105)
− 1
. This implies that the
ll(cid:48) = EP0 [wlwl(cid:48)]. In the end, we obtain the final formula for

= 0 (because EZ 0

RK dw P0(w) exp

n = 1).

(cid:105)

(cid:104)

(cid:104)

lim
n→∞

fn = extrq,ˆq

−

Tr[q ˆq] + IP + αIC

,

1
2

IP ≡

(cid:26)
Dξ

RK

(cid:90)

RK

(cid:90)

(cid:27)
1
−
2

(cid:20)

dw0P0(w0) exp

(cid:124)
(w0)

ˆqw0 + ξ

(cid:124)

ˆq1/2w0

× ln

dwP0(w) exp

−

RK

(cid:20)(cid:90)
DZ0Pout

(cid:20)
y|(Q0 − q)1/2Z0 + q1/2ξ

(cid:21)
(cid:124)
ˆqw + ξ

(cid:124)

w

1
2

ˆq1/2w

,

(cid:21)(cid:21)

× ln

RK

(cid:20)(cid:90)

(cid:110)
DZPout

(cid:110)

y|(Q0 − q)1/2Z + q1/2ξ

.

(cid:111)

(cid:111)(cid:21)

IC ≡

dy

Dξ

R

(cid:90)

RK

(cid:90)

RK

(cid:90)

A known ambiguity of the replica method is that its result is given as an extremum, here over the set
S +
K(Q0) of positive symmetric matrices, such that (Q0 − q) is also a positive matrix. It is easy to show that
this form gives back the form given in Theorem 3.1, by assuming that this extremum is realized as a supˆq inf q.
Note that in the notations of Theorem 3.1, Q0 is denoted ρ and ˆq is denoted R.

C Generalization error

We detail here two different possible definitions of the generalization error, and how they are related in our
system. Recall that we wish to estimate W ∗ from the observation of ϕout(XW ∗). In the following, we denote
E for the average over the (quenched) W ∗ and the data X, and (cid:104)−(cid:105) for the Gibbs average over the posterior
distribution of W . One can naturally define the Gibbs generalization error as:

(cid:15)Gibbs
g

≡

EW ∗,X

[ϕout (XW ) − ϕout (XW ∗)]2

,

and define the Bayes-optimal generalization error as:

(cid:15)Bayes
g

≡

EW ∗,X

(cid:104)ϕout (XW )(cid:105) − ϕout (XW ∗)

1
2

1
2

(cid:10)

(cid:2)(cid:0)

(cid:11)

2

.

(cid:1)

(cid:3)

(74)

(75)

(76)

(77)

(78)

31

Using the Nishimori identity A.1, one can show that:

(cid:15)Bayes
g

=

EX,W ∗

ϕout (XW ∗)2

+

EX,W ∗

(cid:104)ϕout (XW )(cid:105)2

(cid:105)
− EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) ,

(cid:104)

(cid:105)

=

EX,W ∗

ϕout (XW ∗)2

−

EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) .

1
2

1
2

(cid:104)

(cid:104)

1
2

1
2

(cid:105)

Using again the Nishimori identity one can write:

(cid:15)Gibbs
g

= EX,W ∗

ϕout (XW ∗)2

− EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) ,

g

(cid:105)

= 2(cid:15)Bayes
g

(cid:104)
which shows that (cid:15)Gibbs
. Note finally that since the distribution of X is rotationally invariant, the
quantity EX [ϕout (XW ∗) ϕout (XW )] only depends on the overlap q ≡ W (cid:124)W ∗. As the overlap is shown to
concentrate under the Gibbs measure by Proposition 5.3, and as we expect that the value it concentrates on
is the optimum q∗ of the replica formula (such fact is proven, e.g., for random linear estimation problems in
[52]), the generalization error can itself be evaluated as a function of q∗. Examples where it is done include
[53, 3, 19, 11].

C.1 The generalization error at K = 2

In this subsection alone, we go back to the K = 2 case, instead of the K → ∞ limit. From the definition of
the generalization error (see sec. C), one can directly give an explicit expression of this error in the K = 2
case. Recall our committee-symmetric assumption on the overlap matrix, which here reads

q =

qd + qa
2
qa
2

(cid:32)

qa
2
qd + qa

2 (cid:33)

.

For concision, we denote here sign(x) = σ(x). One obtains from (78):

1
2

− 2(cid:15)Bayes,K=2

g

=

Dx σ [σ(x1) + σ(x2)]

× σ

σ

(

+ qd)x1 +

x2 + x3

1 −

qa
2

q2
a
2

− qaqd − q2
d

(cid:35)

(cid:114)

qa
2

(cid:40)

(cid:34)

R4

(cid:90)

qa
2

+σ

x1 + (

+ qd)x2 − x3

qa
2





qa(qd + qa
2 )
2 − qaqd − q2
d

a

1 − q2

+ x4

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:113)

(1 − q2

d)(1 − (qa + qd)2)
2 − qaqd − q2
d

a

1 − q2

.










Note that one could possibly simplify this expression by using an appropriate orthogonal transformation on x.
These integrals were then computed using Monte-Carlo methods to obtain the generalization error in the left
and middle plots of Fig. 2.

(79)

D The large K limit in the committee symmetric setting

We consider the large K limit2 for a sign activation function, and for different priors on the weights. Since the
output is a sign, the channel is simply a delta function. We assume a committee symmetric solution, i.e. the

2A similar limit has been derived in the context of coding with sparse superposition codes [54]. There the large input alphabet
limit of the mutual information is considered after the thermodynamic limit n → ∞ corresponding to the large codeword limit in this
coding context.

32

(cid:124)
matrices q and ˆq (q and R in the notations of Theorem 3.1) are of the type q = qd1K + qa
K, with the unit
K 1K1
vector 1K = (1)K
l=1, and similarly for ˆq. In the large K limit, this scaling of the order parameters is natural.
Indeed, assume that the covariance of the prior is Q0 = 1K (Q0 = ρ in the notations of Theorem 3.1). Since
both q and (Q0 − q) are assumed to be positive matrices, it is easily shown to imply that 0 ≤ qd ≤ 1 and
0 ≤ qa + qd ≤ 1.

D.1 Large K limit for sign activation function

In the following, we consider Q0 = σ21K. We are interested here in computing the leading order term in IC of
(76). Note that replacing σ2 by 1 in this equation only amounts to replacing q by q/σ2, so we can assume σ2 = 1
RK Dξ IC(y, ξ) log IC(y, ξ), with
without loss of generality. We (abusively) write IC in (76) as IC =
the definition

y=±1

(cid:80)

(cid:82)

IC(y, ξ) ≡

DZPout

y|(Q0 − q)1/2Z + q1/2ξ

.

(80)

RK

(cid:90)

(cid:110)

(cid:111)

the remarks above). Note that this implies that q1/2 =
√
√

Here, we assumed a sign activation function and no noise, as well as a particular form for Q0 and q (see
√
(cid:124)
qa+qd−
K and that (Q0 − q)1/2 =
K
(cid:124)
K. All together, this gives the following explicit expression for IC(y, ξ) :
1K1

1 − qd1K +

qd1K +

1−qa−qd−

1K1

1−qd

qd

√

√

√

K

IC(y, ξ) ≡

DZ

RK

(cid:90)

× δ

y − sign

(cid:40)

K

1
√
K

(cid:34)

sign

1 − qdZl +

1 − qa − qd −

1 − qd

l=1
(cid:88)
(cid:124)
K Z
Introducing a new variable w ≡ 1
√
K
another variable u being the argument of the outer sign function in the previous equations, one obtains:

and a Fourier-transform of the then-introduced delta function, as well as

(cid:16)(cid:112)

(cid:112)

(cid:112)

(cid:17)

(cid:20)

(cid:124)
1
KZ
K

+ (q1/2ξ)l

.

(cid:21)(cid:35)(cid:41)

IC(y, ξ) =

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

eiw ˆw+iuˆuδy,sign(u)

Dze

−i ˆw z√

K e

(cid:20)

(cid:20)(cid:114)

− iˆu√
K

sign

z+

(cid:21)

1−qa−qd
1−qd

−1

w√
K

+ 1√

1−qd

(q1/2ξ)l

(cid:21)

.

K

×

R

l=1 (cid:90)
(cid:89)

λl(w, ξ) ≡

1 − qa − qd
1 − qd

− 1
(cid:21)

(cid:20)(cid:114)

w
√
K

+

√

1
1 − qd

(q1/2ξ)l,

IC(y, ξ) =

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

eiw ˆw+iuˆuδy,sign(u)

Dze

−i ˆw z√

− iˆu√
K

K e

sign[z+λl(w,ξ)]

.

K

R

l=1 (cid:90)
(cid:89)

Denote

such that

33

(81)

(82)

(83)

(84)

(85)

For 1 ≤ l ≤ K, one can rewrite the factorized integral in the last expression of IC(y, ξ) as:

IC(y, ξ) =

eiw ˆw+iuˆuδy,sign(u)

J (λl(w, ξ), ˆw, ˆu) ,

J (λl(w, ξ), ˆw, ˆu) ≡ e

Dze

z(λl−i ˆw√
K

)

− iˆu√
e
K

sign[z]

.

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

λ2
l
2 +iλl

−

ˆw√
K

R

(cid:90)

K

l=1
(cid:89)

We abusively dropped the dependency of λl on (w, ξ). Note the following identity:

F (α, iβ) ≡

Dzeαz+iβ sign(z) = eα2/2

cos β + i sin β ˆH(α)

,

(cid:104)

(cid:105)

R

(cid:90)

√

with ˆH(x) = erf(x/

2). Using it in our previous expressions, we obtain:

J(λl, ˆw, ˆu) = e− 1

2K ˆw2

cos

− i sin

ˆH

λl − i

ˆu
√
K (cid:19)

ˆu
√
K (cid:19)

(cid:20)
Note that by our committee-symmetry assumption, we have λl(w, ξ) = λl,0(ξ) + 1√
K
λ1 typically of order 1 when K → ∞:

(cid:18)

(cid:18)

(cid:18)

ˆw
√
K (cid:19)(cid:21)

.

λ1(w, ξ) with λl,0 and

λl,0(ξ) ≡

λ1(w, ξ) ≡

ξl,

qd
1 − qd
1 − qa − qd
1 − qd

(cid:114)

(cid:20)(cid:114)

− 1

w +

(cid:21)

(cid:20)(cid:114)

qa + qd
1 − qd

−

qd
1 − qd (cid:21)

(cid:114)

(cid:124)
1
Kξ
√
K

.

Expanding J(λl, ˆw, ˆu) as K → ∞, we obtain using the known development of the error function:

J(λl, ˆw, ˆu) = e− 1

2K ˆw2

1 −

− i ˆH [λl,0(ξ)]

ˆu2
2K

(cid:34)

ˆu
√
K

− i

ˆu [λ1(w, ξ) − i ˆw]
K

2
π

(cid:114)

λl,0(ξ)2

e−

2 + O(K−3/2)

.

(cid:35)

This yields (putting back the (w, ξ) dependency):

K

l=1
(cid:89)

J [λl(w, ξ), ˆw, ˆu)] = e− 1

2 ˆw2

exp

−

− iˆuS1 − i

ˆu(λ1 − i ˆw)Γ0 +

ˆu2
2

(cid:34)

2
π

(cid:114)

1
2

ˆu2S2 + O(K−1/2)
(cid:35)

,

(86)

in which we defined the following quantities, that only depend on ξ (recall (84))

wξ(ξ) ≡

ξl,

Γ0(ξ) ≡

e− 1

2 λl,0(ξ)2

,

S1(ξ) ≡

ˆH(λl,0(ξ)),

S2(ξ) ≡

ˆH(λl,0(ξ))2.

1
√
K

1
√
K

K

l=1
(cid:88)
K

l=1
(cid:88)

1
K

1
K

K

l=1
(cid:88)
K

l=1
(cid:88)

A detailed calculation actually shows that the previous expansion of (86) is valid up to O(K−1), and not only
O(K−1/2). Recall also (81), in which one can now readily perform the integration over all variables w, ˆw, u, ˆu
to obtain (dropping the ξ dependency in wξ, Γ0, S1, S2):

IC(y, ξ) = H

−y

+ O(K−1),

(87)

S1 +

(cid:113)





√

qd

√

2
π wξΓ0
(cid:113)
1 − S2 − 2

√

qd+qa−
1−qd
qa
1−qd

π Γ2
0





34

∞

x Dz = 1

in which H(x) ≡
. Note that all quantities wξ, Γ0, S1, S2 only depend on ξ via
its empirical measure, which implies that the integration over ξ ∈ RK will be tractable. We compute it in the
following, using theoretical physics methods. We denote the quantity that appears in (87) as a function of
wξ, Γ0, S1, S2:

1 − erf(x/

2)

(cid:82)

(cid:3)

(cid:2)

2

√

G(y, wξ, Γ0, S1, S2) ≡ H

−y

S1 +

(cid:113)





√

qd

√

2
π wξΓ0
(cid:113)
1 − S2 − 2

√

qd+qa−
1−qd
qa
1−qd

π Γ2
0

.





Introducing once again delta functions and their Fourier transforms for wξ, Γ0, S1, S2, we write, starting from
(87):

IC =

DξIC(y, ξ) log IC(y, ξ)

RK

y=±1 (cid:90)
(cid:88)

=

y=±1 (cid:90)
(cid:88)

dwξd ˆwξ
2π

dΓ0dˆΓ0
2π

dS1d ˆS1
2π

dS2d ˆS2
2π

eiw ˆw+iΓ0 ˆΓ0+iS1 ˆS1+iS2 ˆS2 G(y, wξ, Γ0, S1, S2)

× log G(y, wξ, Γ0, S1, S2)

Dξe−i ˆwwξ(ξ)−iˆΓ0Γ0(ξ)−i ˆS1S1(ξ)−i ˆS2S2(ξ)

+ O(K−1).

(88)

RK

(cid:20)(cid:90)

(cid:21)

The integral over ξ in (88) can be computed in the limit K → ∞:

Λ ≡

Dξe−i ˆwwξ(ξ)−iˆΓ0Γ0(ξ)−i ˆS1S1(ξ)−i ˆS2S2(ξ)

RK

(cid:90)

= 
(cid:90)

R

Dξ exp 

−i



The large K expansion yields




ˆwξ
√
K

− i

−
ˆΓ0e

qd
2(1−qd) ξ2
K

− i

ˆS1 ˆH

qd
1−qd

ξ

ˆS2 ˆH

qd
1−qd

ξ

√
(cid:104)(cid:113)
K

− i

(cid:105)

(cid:104)(cid:113)
K

2

K

(cid:105)











Λ = exp

−

ˆw2 − iˆΓ

1 − qd − ˆS1 ˆwE

1
2

(cid:40)

ξ ˆH
(cid:20)

(cid:18)(cid:114)

qd
1 − qd

ξ

(cid:19)(cid:21)

2

qd
1 − qd

ξ

1 + i ˆS2
ˆS2

E

ˆH

+ O(K−1) .

(cid:34)

(cid:19)

(cid:18)(cid:114)

(cid:35) (cid:41)

(cid:19)

(cid:112)

−

1
2

(cid:18)

The expectations are taken with respect to a real variable ξ ∼ N (0, 1). These expectations are known by
properties of the error function:

One can now compute the integrals over the “hat” variables in (88). Denote Γf

0 ≡

2(1−qd)
π

, and Sf

2 ≡

(cid:113)

E

ˆH
(cid:34)

E

ξ ˆH

qd
1 − qd

ξ

2

(cid:35)

(cid:19)

(cid:18)(cid:114)

2
π

=

arcsin qd ,

qd
1 − qd

ξ

2qd
π

.

=

(cid:19)(cid:21)

(cid:114)

(cid:20)

(cid:18)(cid:114)

35

2
π arcsin qd. This yields:

IC =

DwDS1 G

R2

(cid:90)

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

2qd
π

, Sf
2

(cid:114)

log G

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

(cid:33)

2qd
π

(cid:114)

, Sf
2

.

(cid:33)

(89)

.





(90)

Note that

G

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

2qd
π

, Sf
2

(cid:33)

(cid:114)

= H

−y



2
π

(cid:114)

√

arcsin qd − qdS1 + w

qd + qa

√

1 − 2

π (qa + arcsin qd)

Making the change of variable Snew
reaches:

1 = S1 + w

√

√

qd+qa
arcsin qd−qd


in (89), and defining γ ≡ 2

(cid:113)

π (qa + arcsin qd), one

IC =

DxH

yx

log H

yx

γ
1 − γ

γ
1 − γ

+ O(K−1).

(cid:20)

(cid:114)

(cid:21)

(cid:20)

(cid:114)

(cid:21)

R

y=±1 (cid:90)
(cid:88)

The two values of y contribute in the same way, which finally yields:

IC = 2

DxH

x

log H

x

R

(cid:90)

(cid:20)

(cid:114)

(cid:21)

(cid:20)

(cid:114)

(cid:21)

γ
1 − γ

γ
1 − γ

+ O(K−1).

Note that the parameter γ is naturally bounded to the interval [0, 1] by the conditions 0 ≤ qd ≤ 1 and
0 ≤ qa + qd ≤ 1.

D.2 The Gaussian prior

The prior part IP of the free entropy of (76) is very easy to evaluate in the Gaussian prior setting. We consider
a prior with covariance matrix Q0 = IK (we can simply rescale q by q/σ2 in the final expression for a finite
variance Q0 = σ2IK as we already described). Performing the Gaussian integration in IP in (76) yields:

IP =

ˆqd +

ˆqa −

log(1 + ˆqd) −

log (1 + ˆqd + ˆqa) .

(91)

K
2

1
2

K − 1
2

1
2

D.3 The fixed point equations

From the definition of the free entropy (76) and the expansions for IP and IC obtained in (90) and (91), one
obtains the fixed point equations after having extremized over ˆqd and ˆqa (recall that α ≡ lim m

n ):

∂qa [IG(qd, qa) + αIC(qd, qa)] = 0,
∂qd [IG(qd, qa) + αIC(qd, qa)] = 0,

(92)

(93)

with IG(qd, qa) defined as:

IG(qd, qa) ≡

[qa + Kqd] −

1
2

IC(qd, qa) = 2

DxH

x

R

(cid:90)

(cid:20)

(cid:114)

K − 1
2
γ
1 − γ

(cid:21)

log

(cid:20)
log H

1
1 − qd (cid:21)
x

γ
1 − γ

,

(cid:21)

(cid:20)

(cid:114)

−

log

1
2

1
1 − qa − qd (cid:21)

,

(cid:20)

36

and recall that γ ≡ 2

π (qa + arcsin qd).

The fixed point equations (92), (93) have different behaviors depending on the scaling of α with the hidden

layer size K. We detail these different behaviors in the following paragraphs.

D.3.1 Regime α = oK→∞(K)

In this regime (which in particular contains the case in which α stays of order 1 when K → ∞), the fixed
point equations (92), (93) can be simplified as:

qd = 0,
qa = 2α(1 − qa) ∂IC
∂qa

.

(cid:40)

D.3.2 Regime α = ΘK→∞(K)

In this regime, we naturally define
solutions of the fixed point equations (92), (93) must satisfy the following scaling : qa + qd = 1 − χ
χ ≥ 0 a reaching a finite value when K → ∞. The fixed point equations in terms of χ and qd read:

α will remain of order 1. One can show that the
K , with

αK ≡ α/K, such that

(cid:101)

(cid:101)

qd = 2(1 − qd)

χ−1 = 2

α ∂IC
∂qa

.

1√

1−q2
d

− 1

˜α ∂IC
∂qa

,

(cid:19)

(cid:18)






Note that the State Evolution (SE) computation of Figure 2 was performed by solving the fixed point

(cid:101)

equations (94) and (95) (depending on the regime of α).

It is easy to show that (95) always admit what we call a non-specialized
The stability of the qd = 0 solution:
solution, i.e. a solution with qd = 0. This solution stops to be optimal in term of the free energy at a finite
αspec (cid:39) 7.65. However, one can show that this solution will remain linearly stable for every
α. Actually, it is
linearly stable in the much broader regime α = o(K2). Going back to the initial formulation of the fixed point
(cid:101)
equations (92),(93), and adding the correct time indices to iterate them, one obtains:

(cid:101)

with F and G defined as:

(cid:0)

(cid:1) (cid:0)

d, qt
F (qt
a)
1 + F (qt
d, qt
a)

,

qt+1
d =

qt+1
a =

1 + F (qt

d, qt
a)

d, qt

a)G(qt

d, qt
a)

d, qt
G(qt
a)
1 + F (qt

,

(cid:1)

F (qd, qa) ≡

[∂qdIC − ∂qaIC] ,

G(qd, qa) ≡

∂qaIC −
(cid:20)

1
K

∂qdIC

.

(cid:21)

2α
K − 1
2αK
K − 1

We focus on the behavior of (96) around qd = 0. Given our previous expansion of IC in the K → ∞
|qd=0 →K→∞ 0, which means the qd = 0 solution

limit, and (98), one easily sees that for α = oK→∞(K2), ∂F
∂qd
always remains linearly stable.

However, assume now that α = Θ(K2). Performing a similar calculation to the one shown in sec. D.1,

(94)

(95)

(96)

(97)

(98)

(99)

37

one can show the following expansion:

IC(qd, qa) = I

(0)
C (qd, qa) +

(1)
C (qd, qa) + O

I

1
K

1
K2

.

(cid:18)

(cid:19)

The term of ∂F
∂qd
seen from (98).

|qd=0 arising from I

(1)
C will thus have a possibly non-zero contribution in the K → ∞ limit, as

To summarize, the non-specialized solution always remains linearly stable in the large K limit at least for
α (cid:28) K2. This implies that in this regime, Approximate Message Passing can not escape the non-specialized
fixed point to find the specialized solution, as seen in Fig. 3. For α of order larger than K2, one would have to
(1)
C in order to check that ∂F
|qd=0 (cid:54)= 0 to show that the non-specialized solution is indeed
explicitly compute I
∂qd
linearly unstable. This tedious calculation is left for future work.

D.4 The generalization error at large K

Recall the definition of the generalization error in (78). From the remarks of section C, one can compute it at
large K by applying the same techniques used to compute the channel integral IC in sec. D.1. One obtains
after a tedious, yet straightforward, calculation:

(cid:15)Bayes
g

=

(cid:15)Gibbs
g

=

arccos

(qa + arcsin qd)

+ O(K−1).

(100)

1
2

1
π

2
π

(cid:20)

(cid:21)

This expression is the one used in the computation of the generalization error in the left panel of Fig. 3.

E Linear networks show no specialization

An easy yet interesting case is a linear network with identical weights in the second layer and a final output
function σ : R → R, i.e a network in which ϕout(h) = σ
. For clarity, in this section, we
decompose the channel as Pout(y|ϕout(Z)) for Z ∈ RK instead of Pout(y|Z). We will compute the channel
integral IC of the replica solution (76). For simplicity, we assume that Q0 = 1K the identity matrix (i.e w has
RK DξIC(y, ξ) log IC(y, ξ). One
identity covariance matrix under P0). Note that (76) gives IC as IC =
can easily derive:

K
l=1 hl

1√
K

R dy

(cid:80)

(cid:1)

(cid:0)

(cid:82)

(cid:82)

IC(y, ξ) = e− 1

2 ξ(cid:124)(1K −q)−1qξ

eiuˆuPout(y|σ(u))

dudˆu
2π

R2

(cid:90)

×

RK

(cid:90)

dZ
(2π)K det(1K − q)

e− 1

2 Z(cid:124)(1K −q)−1Z+Z(cid:124)X(ˆu,xi),

in which we denoted X(ˆu, xi) (cid:44) (1K − q)−1q1/2ξ − iˆu√
K
integration over Z can be done, as well as the integration over ˆu:

(cid:112)

1K, with the unit vector 1K = (1)K

l=1. The inner

IC(y, ξ) =

1
1 − 1

(cid:124)
R
Kq1K (cid:90)
K 1

du
√
2π

Pout(y|σ(u)) exp 

−

(cid:113)

(cid:124)
u − 1√
Rq1/2ξ
1
K
(cid:124)
1 − 1
Kq1K
K 1

(cid:16)
2

2

.






(cid:17)

(cid:1)




(cid:0)

So we can formally write the total dependency of IC(y, ξ) on ξ and on q as

IC(y, ξ) = IC

y,

1
√
K

(cid:124)
Kq1/2ξ,
1

(cid:124)
Kq1K
1

1
K

.

(cid:19)

(cid:18)

38

Note that we have the following identity, for any fixed vector x ∈ RK and smooth real function F :

DξF (x

ξ) =

√

(cid:124)

duF (u)e− u2

2x(cid:124)x .

1
2πx(cid:124)x

R

(cid:90)

RK

(cid:90)

In the end, if we denote Γ(q) (cid:44) 1

(cid:124)
Kq1K, we have:

K 1

IC =

dy

dve

2Γ(q) IC(v, y) log IC(v, y),

1

− v2

IC(v, y) ≡

du Pout(y|σ(u)) exp

−

1
2 (1 − Γ(q))

(u − v)2

.

(cid:21)

(cid:20)

R

(cid:90)

2πΓ(q)
1
(cid:112)
2π(1 − Γ(q))

R

(cid:90)

R

(cid:90)

(cid:112)

Note that by hypothesis, both q and 1K − q are positive matrices, so 0 ≤ Γ(q) ≤ 1. As these equations show,
IC only depends on Γ(q) = K−1
l,l(cid:48) qll(cid:48). From this one easily sees that extremizing over q implies that the
optimal ˆq satisfies ˆqll(cid:48) = ˆq/K for some real ˆq. Subsequently, all qll(cid:48) are also equal to a single value, that we
can denote q

K . This shows that this network never exhibits a specialized solution.

(cid:80)

(101)

(102)

(103)

F Update functions and AMP derivation

AMP can be seen as Taylor expansion of the loopy belief-propagation (BP) approach [13, 14, 55], similar to
the so-called Thouless-Anderson-Palmer equation in spin glass theory [35]. While the behaviour of AMP can
be rigorously studied [17, 18, 56], it is useful and instructive to see how the derivation can be performed in
the framework of belief-propagation and the cavity method, as was pioneered in [36, 38] for the single layer
problem. The derivation uses the Generalized AMP notations of [16] and follows closely the one of [26].

F.1 Definition of the update functions

Let’s consider the distributions probabilities Qout and Q0, closely related to the inference problems eq. (3) and
eq. (4):

Qout(z; ω, y, V ) ≡

e− 1

2 (z−ω)(cid:124)V −1(z−ω)Pout(y|z); Q0(W ; Σ, T ) ≡

P0(W )e− 1

2 W (cid:124)Σ−1W +T (cid:124)Σ−1W .

1
ZPout

1
ZP0

We define the update functions gout, ∂ωgout, fw and fc, which will be useful later in the algorithm:

gout(ω, y, V ) ≡ ∂ω log(ZPout) = V −1EQout [z − ω] ,

(cid:124)
∂ωgout(ω, y, V ) = V −1EQout [(z − ω)(z − ω)
fw(Σ, T ) ≡ ∂Σ−1T log ZP0 = EQ0[W ] ,
fc(Σ, T ) ≡ ∂Σ−1T fw = EQ0[W W

(cid:124)

] − fwf

(cid:124)
w .

] − V −1 − goutg

(cid:124)
out ,

Note that gout is the mean of V −1(z − ω) with respect tor Qout and fw the mean of Q0.

F.2 Derivation of the Approximate Message Passing algorithm

F.2.1 Relaxed BP equations

Lets consider a set of messages {mi→µ, ˜mµ→i}i=1..n,µ=1..m on the bipartite factor graph corresponding to
our problem Fig. 4. These messages correspond to the marginal probabilities of Wi if we remove the edges
i → µ or µ → i. The belief propagation (BP) equations (or sum-product equations) can be formulated as the
following [14, 55], where Wi = (wil)l=1..K ∈ RK:

39

Pout (Yµ|{XµWi}n
µ = 1...m

i=1)

˜mµ→i

Wi ∈ RK
i = 1...n

P0(Wi)
i = 1...n

mi→µ

Figure 4: Factor graph representation of the committee machine (for n = 4 and m = 3). The variable (circle)
Wi ∈ RK needs to satisfy a prior constraint (square) P0 and a constraint accounting for the fully connected layer,
that correlates all the variables together.

i→µ(Wi) =

P0(Wi)

˜mt

ν→i(Wi) ,

1
Zi→µ

m

k(cid:54)=µ
(cid:89)

µ→i(Wi) =

dWjPout

Yµ|

XµjWj

mt

j→µ(Wj) .

1
Zµ→i (cid:90)

n

j(cid:54)=i
(cid:89)

1
√
n

n

j=1
(cid:88)









The term inside Pout can be decouple using its K-dimensional Fourrier transform



mt+1



˜mt

Pout

Yµ|

XµjWj

=

1
√
n

n

j=1
(cid:88)





1
(2π)K/2

RK

(cid:90)

dξ exp

(cid:124)

iξ









1
√
n

n

j=1
(cid:88)





XµjWj

ˆPout(Yµ, ξ)

.









Injecting this representation in the BP equations, (104) becomes

˜mt

µ→i(Wi) =

dξ ˆPout(Yµ, ξ) exp

XµiWi

1
(2π)K/2Zµ→i (cid:90)
×

n

RK

RK

j(cid:54)=i (cid:90)
(cid:89)

(cid:124)

dWjmt

j→µ(Wj) exp

iξ

XµjWj)

,

iξ

(cid:124) 1
√
n

(cid:124) 1
√
n

(cid:18)

(cid:18)

≡Ij

(cid:123)(cid:122)

(cid:19)

(cid:19)

(cid:125)

and we define the mean and variance of the messages

ˆW t

j→µ ≡

dWjmt

j→µ(Wj)Wj ,

ˆCt

j→µ ≡

dWjmt

j→µ(Wj)WjW

(cid:124)
j − ˆW t

j→µ( ˆW t

(cid:124)
j→µ)

.

RK

(cid:90)

RK

(cid:90)






In the limit n → ∞ the term Ij can be easily expanded and expressed using ˆW and ˆC

(104)

(105)

Ij =

dWjmt

j→µ(Wj) exp

iξ

Wj)

(cid:39) exp

(cid:124) Xµj√
n

(cid:18)

(cid:19)

RK

(cid:90)

Xµj√
n

i
(cid:32)

(cid:124) ˆW t

ξ

j→µ −

(cid:124) ˆCt

ξ

j→µ , ξ

X 2
µj
n

1
2

,

(cid:33)

40

and finally using the inverse Fourier transform, we obtain

˜mt

µ→i(Wi) (cid:39)

dzPout(Yµ, z)

dξe−iξ(cid:124)zeiXµiξ(cid:124)Wi

1
(2π)KZµ→i (cid:90)
×

RK
n

RK

(cid:90)
(cid:124) ˆW t

ξ

j→µ −

X 2
µj
n

1
2

(cid:124) ˆCt

ξ

j→µξ

(cid:33)

exp

i

Xµj√
n

(cid:32)

j(cid:54)=i
(cid:89)

dzPout(Yµ, z)

dξe−iξ(cid:124)zeiXµiξ(cid:124)Wie

iξ(cid:124)

n
(cid:80)
j(cid:54)=i

Xµj√
n

ˆW t

j→µ

2 ξ(cid:124)

− 1
e

X2
µj
n

n
(cid:80)
j(cid:54)=i

ˆCt

j→µξ

=

=

1
(2π)KZµ→i (cid:90)
1
(2π)KZµ→i (cid:90)

RK

RK

dzPout(Yµ, z)

RK

(cid:90)

(2π)K
det(V t
iµ)

− 1
e
2

(cid:115)

(cid:16)

z−

Xµi√
n

Wi−ωt
iµ

(cid:17)(cid:124)

(V t

iµ)−1(cid:16)

z−

Xµi√
n

Wi−ωt
iµ

(cid:17)

,

(cid:125)

≡Hiµ

(cid:123)(cid:122)

1
n

n

j(cid:54)=i
(cid:88)

ωt
iµ ≡

Xµj ˆW t

j→µ ,

V t
iµ ≡

X 2
µj

ˆCt

j→µ .

(106)

1
√
n

n

j(cid:54)=i
(cid:88)

where we defined the mean and variance, depending on the node i

(cid:124)

Again, in the limit n → ∞, the term Hiµ can be expanded:

Hiµ (cid:39) e− 1

2 (z−ωt

(cid:124)
iµ)

(V t

iµ)−1(z−ωt

iµ)

Xµi√
n

1 +

(cid:32)

W

(cid:124)
i (V t

iµ)−1(z − ωt

iµ) −

W

(cid:124)
i (V t

iµ)−1Wi

X 2
µi
n

1
2

X 2
µi
n

1
2

+

W

(cid:124)
i (V t

iµ)−1(z − ωt

iµ)(z − ωt

(cid:124)
iµ)

(V t

iµ)−1Wi

.

(cid:33)

Gathering all pieces, the message ˜mµ→i can be expressed using definitions of gout and ∂ωgout

˜mt

µ→i(Wi) ∼

W

(cid:124)
i gout(ωt

iµ, Yµ, V t

iµ) +

W

(cid:124)
i goutg

(cid:124)
out(ωt

iµ, Yµ, V t

iµ)Wi+

1 +

Xµi√
n

X 2
µi
n

1
2

(cid:124)
i ∂ωgout(ωt

iµ, Yµ, V t

iµ)Wi

1 + W

(cid:124)
i Bt

µ→i +

W

(cid:124)
i Bt

µ→i(Bt

µ→i)

(cid:124)

(Wi) −

W

(cid:124)
i At

µ→iWi

1
2

(cid:27)

(cid:41)

1
2

µ→i)
(2π)K exp

(cid:115)

−

1
2

(cid:18)

(cid:0)

W

(cid:124)
i − (At

µ→i)−1Bt

µ→i

(cid:124)

At

µ→i

W

(cid:124)
i − (At

µ→i)−1Bt

µ→i

,

(cid:1)

(cid:0)

(cid:19)

(cid:1)

with the following definitions of Aµ→i and Bµ→i:

Bt

µ→i ≡

gout(ωt

iµ, Yµ, V t

iµ), At

µ→i ≡ −

∂ωgout(ωt

iµ, Yµ, V t
iµ)

(107)

Xµi√
n

X 2
µi
n

Using the set of BP equations (104), we can finaly close the set of equations only over {mi→µ}iµ:

mt+1

i→µ(Wi) =

1
Zi→µ

P0(Wi)

m

ν(cid:54)=µ (cid:115)
(cid:89)

det(At
ν→i)
(2π)K e− 1

2 (Wi−(At

ν→i)−1Bt

(cid:124)
ν→i)

At

ν→i(Wi−(At

ν→i)−1Bt

ν→i).

In the end, computing the mean and variance of the product of gaussians, the messages are updated using

1
Zµ→i (cid:40)
X 2
µi
n

W

1
Zµ→i (cid:26)
det(At

1
2

=

=

41

(108)

(109)

(110)

fw and fc:

ˆW t+1

i→µ = fw(Σt

µ→i, T t

µ→i) ,

ˆCt+1

i→µ = fc(Σt

µ→i, T t

µ→i) ,






m

ν(cid:54)=µ
(cid:80)

µ→i

(cid:32)



µ→i ≡

Σt

T t
µ→i ≡ Σt


At

ν→i

(cid:33)

m

(cid:32)

ν(cid:54)=µ
(cid:80)

−1

,

Bt

ν→i

.

(cid:33)

Summary of the Relaxed BP set of equations:

In the end, using eq .(105,106,107, 108), relaxed BP equations can be written as the following set of equations:

Xµj√
n

ˆW t

j→µ

X 2
µj
n

ˆCt

j→µ

=

=

n

j(cid:54)=i
(cid:80)
n

j(cid:54)=i
(cid:80)
Xµi√

Bt

µ→i =

At

µ→i = −

iµ, Yµ, V t
iµ)

n gout(ωt
X 2
n ∂ωgout(ωt
µi

iµ, Yµ, V t
iµ)

ωt
iµ

V t
iµ






Σt

µ→i =

−1

m

(cid:32)

ν(cid:54)=µ
(cid:80)

At

ν→i

(cid:33)

T t
µ→i = Σt

µ→i

ˆW t+1

i→µ = fw(Σt

m

Bt

ν→i

(cid:33)

(cid:32)

ν(cid:54)=µ
(cid:80)
µ→i, T t

µ→i)

ˆCt+1

i→µ = fc(Σt

µ→i, T t

µ→i)






F.2.2 Approximate Message Passing algorithm

The relaxed BP algorithm uses O(n2) messages. However all the messages depend weakly on the target node.
On a tree, the missing message is negligible, that allows us to expand the previous relaxed BP equations (109)
to make appear the Onsager term at a previous time step, and reduce the number of messages to O(n). We
define the following estimates and parameters based on the complete set of messages:

Xµj√
n

ˆW t

j→µ

X 2
µj
n

ˆCt

j→µ

n

j=1
(cid:80)
n

j=1
(cid:80)

ωt

µ ≡



V t
µ ≡


Σt

i ≡

−1

m

At

ν→i

ν=1

(cid:18)
(cid:80)

i ≡ Σt
T t
i


(cid:18)

m

ν=1
(cid:80)

(cid:19)

Bt

ν→i

(cid:19)

Let’s now expand the previous messages eq. (109), making appear these new target-independent messages:

• Σt

µ→i

• T t

µ→i

Σt

µ→i =

At

ν→i

=

m

ν(cid:54)=µ
(cid:88)









=

IK×K −

−1

m

−1

m

At

ν→i − At

µ→i

=

(cid:32)

ν=1
(cid:88)

−1

(cid:33)

m

−1

At

ν→i

(cid:33)

At

µ→i

At

ν→i

(cid:33)

(cid:32)

ν=1
(cid:88)





m

(cid:32)

ν=1
(cid:88)





ν=1
(cid:88)
−1

At

ν→i 

IK×K −


IK×K − Σt

=

(cid:32)

ν=1
(cid:88)

m

−1

−1

At

ν→i

(cid:33)

At

µ→i



iAt

µ→i

−1

Σt

i (cid:39) Σt


i + O


1
n

(cid:18)

(cid:19)

(cid:39)IK×K +Σt
(cid:0)

iAt

µ→i+O(n−1)

(cid:1)

(cid:124)

(cid:123)(cid:122)

(cid:125)

m

Bt

ν→i

=

Σt

i + O

1
n

m

Bt

ν→i − Bt

µ→i

(cid:33)

(cid:18)

(cid:19)(cid:19) (cid:32)

ν=1
(cid:88)

µ→i = Σt
T t

µ→i 

ν(cid:54)=µ
(cid:88)

iBt
i − Σt

= T t


µ→i + O

(cid:18)

1
n

(cid:18)

(cid:19)

42

• ˆW t+1
i→µ

• V t
µ

• ωt
µ

ˆW t+1

i→µ = fw(Σt

µ→i, T t

µ→i) = fw

Σt

i, T t

i − Σt

iBt

µ→i

+ O

1
n

(cid:18)

(cid:19)

(cid:0)

Σt

iBt

µ→i

(cid:1)

(cid:39) fw

Σt

i, T t
i

−

dfw
dT

= fw

(cid:0)

Σt

i, T t
i
= ˆW t+1
(cid:0)
i

(cid:1)

(cid:1)

−

Σt
i

(cid:0)

(cid:1)

(Σt
(cid:12)
(cid:12)
−1
(cid:12)
(cid:12)

i,T t
i )
Σt

fc

Σt
i

i, T t
i
= ˆCt+1
i

(cid:1)

(cid:0)

(cid:39)

Xµi√
n

(cid:123)(cid:122)
(cid:124)
= ˆW t+1
i −

(cid:125)
Xµi√
n

(cid:124)
−1 ˆCt+1

(cid:125)
(cid:123)(cid:122)
µ, Yµ, V t
igout(ωt
i Σt

Σt
i

Bt

µ→i

µ,Yµ,V t
µ)

gout(ωt
(cid:124) (cid:123)(cid:122) (cid:125)
µ) + O

1
n

(cid:18)

(cid:19)

(cid:0)

(cid:1)

• ˆCt+1
i→µ
Let’s denote for convenience, E =

Σt
i

−1 ˆCt+1

i Σt

igout(ωt

µ, Yµ, V t

µ). Then

ˆCt+1
i→µ = EQ0

ˆW t

i→µ( ˆW t

= EQ0

ˆW t

i −

(cid:0)

(cid:1)

(cid:124)
i→µ)
Xµi√
n

E

(cid:105)

− EQ0

ˆW t
(cid:104)
Xµi√
i −
n

ˆW t

i→µ

(cid:105)

E

i→µ

EQ0
(cid:124)

ˆW t
(cid:104)
− EQ0

(cid:124)

(cid:20)(cid:18)
ˆW t

(cid:124)
i ( ˆW t
i )

(cid:19) (cid:18)
− EQ0

= EQ0

ˆW t
i

EQ0

(cid:19)
(cid:21)
ˆW t
i

(cid:124)

+ O

(cid:105)

(cid:104)

(cid:105)

(cid:104)

(cid:105)

(cid:104)

(cid:104)

(cid:105)
ˆW t

i −

(cid:20)
1
√
n

(cid:18)

(cid:19)

Xµi√
n

E

EQ0

ˆW t
(cid:20)
i + O

(cid:21)
= ˆCt+1

1
√
n

(cid:18)

(cid:19)

i −

Xµi√
n

E

(cid:124)

(cid:21)

• gout(ωt

iµ, Yµ, V t
iµ)

gout(ωt

iµ, Yµ, V t

iµ) = gout

ωt
µ −

ˆW t

i→µ, Yµ, V t

µ −

= gout

µ, Yµ, V t
ωt
µ

−

Xµi√
n

∂gout
∂ω

µ, Yµ, V t
ωt
µ

= gout

µ, Yµ, V t
ωt
µ

−

µ, Yµ, V t
ωt
µ

ˆW t

Xµi√
n

∂gout
∂ω

X 2
µi
n

ˆCt

i→l

(cid:33)

+O

1
n

(cid:18)

(cid:19)

(cid:1)

= ˆW t

ˆW t

i→µ
(cid:16) 1√

i +O
(cid:124) (cid:123)(cid:122) (cid:125)
i + O

(cid:17)

n

1
n

(cid:18)

(cid:19)

(cid:1)

(cid:0)

(cid:0)

Xµi√
n

(cid:1)

(cid:1)

(cid:32)

(cid:0)

(cid:0)

V t
µ =

ˆCt

i→l =

ˆCt

i + O

X 2
µi
n

n

i=1
(cid:88)

X 2
µi
n

n

i=1
(cid:88)

1
n3/2

(cid:18)

(cid:19)

ωt
µ =

ˆW t

i→µ =

ˆW t

i − Xµi

Σt−1
i

−1 ˆCt

i Σt−1
i

gout(ωt−1

µ , Yµ, V t−1

µ

) + O

1
n

(cid:18)

(cid:19)(cid:19)

Σt−1
i

−1 ˆCt

(cid:0)
i Σt−1
i

(cid:1)
gout(ωt−1

µ , Yµ, V t−1

µ

) + O

1
n3/2

(cid:18)

(cid:19)

Xµi√
n

Xµi√
n

n

i=1
(cid:88)
n

i=1
(cid:88)

=

ˆW t

i −

n

Xµi√
n

(cid:18)

n

i=1
(cid:88)
X 2
µi
n

i=1
(cid:88)

(cid:0)

(cid:1)

43

−1

•

Σt
i

(cid:0)

(cid:1)
Σt
i

−1

=

(cid:0)

(cid:1)

• T t
i

m

µ=1
(cid:88)

m

µ=1
(cid:88)

At

µ→i = −

X 2

µi∂ωgout(ωt

iµ, Yµ, V t

iµ) = −

X 2

µi∂ωgout(ωt

µ, Yµ, V t

µ) + O

m

µ=1
(cid:88)

1
n3/2

(cid:18)

(cid:19)

Bt

µ→i

= Σt
i

m

Xµi√
n

µ=1
(cid:88)
µ, Yµ, V t
ωt
µ


gout

i = Σt
T t

m

i 

µ=1
(cid:88)
m


µ=1
(cid:88)
m

i 



µ=1
(cid:88)

Xµi√
n

(cid:18)

Xµi√
n

= Σt
i

= Σt

(cid:0)

(cid:0)

gout

µ, Yµ, V t
ωt
µ

−

gout(ωt

iµ, Yµ, V t
iµ)

−

Xµi√
n

∂gout
∂ω

X 2
µi
n

∂gout
∂ω

(cid:1)

(cid:1)

(cid:0)

(cid:0)

µ, Yµ, V t
ωt
µ

ˆW t

i + O

1
n

(cid:18)

(cid:19)(cid:19)

µ, Yµ, V t
ωt
µ

ˆW t

i 

+ O

1
n3/2

(cid:18)

(cid:19)

(cid:1)

(cid:1)



The AMP algorithm follows naturally the rBP updates (109) using the expanded estimates of the mean and

variance ωµ, Vµ, Ti and Σi, and finally reads in pseudo language:

Algorithm 2 Approximate Message Passing for the committee machine

Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: gout,µ = 0, Σi = IK×K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
Initialize: ˆWi ∈ RK and ˆCi, ∂ωgout,µ ∈ S +
repeat

K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 1.

n

ωt

ˆW t

µ =

i −

Xµi√
n

Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
X 2
µi
n

i Σt−1
i

−1 ˆCt

gt−1
out,µ

Σt−1
i

i=1
(cid:80)
out,µ = gout(ωt
gt

Update of gout,µ ∈ RK and ∂ωgout,µ ∈ S +
K:
out,µ = ∂ωgout(ωt

µ, Yµ, V t
µ)
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt

(cid:1)
∂ωgt

Xµi√

i = Σt
T t
i

X 2
n ∂ωgt
µi

out,µ −

ˆW t
i

out,µ

n gt

m

(cid:1)

(cid:0)

(cid:0)

|

|

(cid:16)

µ=1
(cid:80)

Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = fw(Σt

i, T t
i )

|

ˆCt+1

i = fc(Σt

i, T t
i )

(cid:17)

µ, Yµ, V t
µ)

i = −
(cid:16)

m

µ=1
(cid:80)

V t
µ =

X 2
µi
n

ˆCt
i

n

i=1
(cid:80)

X 2
n ∂ωgt
µi

out,µ

−1

(cid:17)

ˆW t+1
t = t + 1

until Convergence on ˆW , ˆC.
Output: ˆW and ˆC.

G State evolution equations from AMP

In this section, W (cid:63) denotes the ground truth weights of the teacher and we define the overlap parameters at
time t, mt, σt, qt, Q and that respectively measure the correlation of the AMP estimator with the ground truth,

44

its variance and the norms of student and teacher weights:



mt ≡ EW (cid:63) lim
n→∞

qt ≡ EW (cid:63) lim
n→∞


1
n

1
n

n

i=1
(cid:88)
n

i=1
(cid:88)

ˆW t

(cid:124)
i (W (cid:63)
i )

,

ˆW t

(cid:124)
i ( ˆW t
i )

,

and



σt ≡ EW (cid:63) lim
n→∞

Q ≡ EW (cid:63) lim
n→∞


n

ˆCt
i .

1
n

1
n

i=1
(cid:88)
n

i=1
(cid:88)

W (cid:63)

i (W (cid:63)
i )

(cid:124)

,

The aim is to derive the asymptotic behaviour of these overlap parameters, called state evolution. The idea is
to compute the overlap distributions starting with the relaxed BP equations eq. (109).

G.1 Messages distribution

In order to get the asymptotic behaviour of the overlap parameters, we need first to compute the distribution of
Σt
µ→i and T t
µ→i. Besides, we recall that in our model, the output has been generated by a teacher according to
n
1√
. We define zµ ≡ 1√
j(cid:54)=i XµjW (cid:63)
n W (cid:63)Xµ, A
Yµ = ϕ0
j .
And it useful to recall EX [Xµi] = 0 and EX [X 2
(cid:17)

n W (cid:63)Xµ = 1√
µi] = 1.

i and zµ→i ≡ 1√
n

n
i=1 XµiW (cid:63)

out

(cid:16)

n

(cid:80)

(cid:80)

µ→i

• ωt
Under belief propagation assumption messages are independent. ωt

µ→i is thus the sum of independent
variables and follows a gaussian distribution. Let’s compute the first two moments, using expansions of the
relaxed BP equations eq. (109):

EX

ωt
µ→i

=

EX [Xµj] ˆW t

j→µ = 0 ,

(cid:2)
µ→i(ωt
ωt

EX

(cid:124)
µ→i)

=

(cid:3)

(cid:3)

(cid:2)

• zµ

EX [XµjXµk] ˆW t

j→µ( ˆW t

(cid:124)
k→µ)

=

EX

X 2
µj

ˆWj→µ( ˆWj→µ)

(cid:124)

=

ˆW t

j→µ( ˆW t

j→µ)

(cid:124)

=

ˆW t

i ( ˆW t
i )

(cid:124)

+ O

1
n

n

i=1
(cid:88)

n

j(cid:54)=i
(cid:88)

(cid:3)

(cid:2)
1/n3/2

(cid:16)

(cid:17)

−→
n→∞

qt .

EX [zµ] =

1
√
n

EX [Xµi] W (cid:63)

i = 0 ,

EX,W (cid:63)

zµz

(cid:124)
µ

= EW (cid:63)

EX [XµjXµk] W (cid:63)

j (W (cid:63)
k )

(cid:124)

= EW (cid:63)

W (cid:63)

i (W (cid:63)
i )

(cid:124) −→
n→∞

Q .

1
√
n

n

j(cid:54)=i
(cid:88)
n

1
n

1
n

j(cid:54)=i,k(cid:54)=i
(cid:88)
n

j(cid:54)=i
(cid:88)

n

i=1
(cid:88)
1
n

n

j=1,k=1
(cid:88)

(cid:2)

(cid:3)

• zµ and ωt

µ→i

EX,W (cid:63)

ωt
µ→iz

(cid:124)
µ

= EW (cid:63)

EX [XµjXµk] ˆW t

j→µ(W (cid:63)
k )

(cid:124)

= EW (cid:63)

ˆW t

(cid:124)
j→µ(W (cid:63)
j )

1
n

1
n

n

j(cid:54)=i,k=1
(cid:88)
n

i=1
(cid:88)

(cid:2)

(cid:3)

= EW (cid:63)

ˆW t

(cid:124)
i (W (cid:63)
i )

+ O

1/n3/2

−→
n→∞

mt .

(cid:16)

(cid:17)

Hence asymptotically (zµ, ωt

µ→i) follow a Gaussian distribution with covariance matrix Qt =

Q mt
qt
mt
(cid:34)

.

(cid:35)

1
n

n

i=1
(cid:88)

1
n

n

j(cid:54)=i
(cid:88)

45

• Vµ→i

concentrates around its mean:

EX,W (cid:63)

V t
µ→i

= EW (cid:63)

EX

X 2
µj

ˆCt

j→µ = EW (cid:63)

ˆCt

j→µ = EW (cid:63)

ˆCt

i + O

1/n3/2

1
n

n

j(cid:54)=i
(cid:88)

1
n

n

i
(cid:88)

−→
n→∞

σt .

(cid:16)

(cid:17)

1
n

n

j(cid:54)=i
(cid:88)

(cid:3)

(cid:2)

(cid:3)
Let’s define other order parameters, that will appear in the following:

(cid:2)

ˆqt ≡ αEω,z,A
ˆmt ≡ αEω,z,A
ˆχt ≡ αEω,z,A

gout(ω, ϕ0
∂zgout(ω, ϕ0
(cid:2)
−∂ωgout(ω, ϕ0
(cid:2)

out(z, A), σt)

,
out(z, A), σt)
(cid:3)

out(z, A), σt)gout(ω, ϕ0

out(z, A), σt)(cid:124)

,

(cid:3)

.

(cid:3)





• T t

µ→i

can be expanded around zµ→i:

(cid:2)

Σt

µ→i

−1

T t
µ→i =

m





ν(cid:54)=µ
(cid:88)

Bt

ν→i

=



1
√
n

m

ν(cid:54)=µ
(cid:88)

Xνigout(ωt

ν→i, ϕ0

1
√
n

out 

XνjW (cid:63)

j + XνiW (cid:63)

i , A

, V t

ν→i)



n

j(cid:54)=i
(cid:88)

Xνigout(ωt

ν→i, ϕ0



out (zν→i, A) , V t

ν→i)

+


νi∂zgout(ωt

X 2


out (zν→i, A) , V t
ν→i)

ν→i, ϕ0

m

1
n









ν(cid:54)=µ
(cid:88)




W (cid:63)
i .





(cid:0)

=

(cid:1)
m





ν(cid:54)=µ
(cid:88)

1
√
n

• Σt

µ→i

At

ν→i = −

X 2

νi∂ωgout(ωt

ν→i, Yν, V t

ν→i)

m

Σt

µ→i

−1

=

(cid:0)

(cid:1)

m

1
n

ν(cid:54)=µ
(cid:88)

m

= −

ν(cid:54)=µ
(cid:88)
νi∂ωgout(ωt

X 2

1
n

ν(cid:54)=µ
(cid:88)

ν→i, ϕ0

out (zν→i, A) , V t

ν→i) + O

1/n3/2

.

Hence taking the average and the large size limit, the first moments of the variables Σt

(cid:16)

(cid:17)
µ→i and T t

µ→i read:

Eω,z,A,X

Eω,z,A,X

Eω,z,A,X

−1

(cid:17)

−1

(cid:17)

−1

Σt

µ→i

Σt

µ→i

Σt

µ→i

(cid:20)(cid:16)

(cid:20)(cid:16)

T t
µ→i

−→
n→∞

(cid:21)

ˆmtW (cid:63)
i ,
(cid:124)

T t
µ→i

T t
µ→i

Σt

µ→i

(cid:17)

(cid:16)

(cid:21)

(cid:17)

(cid:16)
−→
n→∞

ˆχt .

−1

−→
n→∞

ˆqt ,






And finally T t

µ→i ∼ ( ˆχt)−1

ˆmtW (cid:63)

with ξ ∼ N (0, 1) and

Σt

µ→i

−1

∼ ( ˆχt)−1 .

(cid:16)

(cid:17)

(cid:20)(cid:16)

(cid:21)

(cid:17)
i + (ˆqt)1/2ξ

(cid:0)

(cid:1)

G.2 State evolution equations - Non Bayes optimal case

Let’s define the following notations:

T t[W (cid:63), ξ] ≡ ( ˆχt)−1

ˆmtW (cid:63) + (ˆqt)1/2ξ

Σt ≡ ( ˆχt)−1

(cid:16)

(cid:17)

46

Gathering above results, the state evolution equations read:

ˆW t

i (W (cid:63)
i )

(cid:124)

= EW (cid:63),ξ

fw

Σt, T t[W (cid:63), ξ]

(cid:124)
(W (cid:63))

mt+1 = EW (cid:63) lim
n→∞

qt+1 = EW (cid:63) lim
n→∞

1
n

1
n

1
n

n

i=1
(cid:88)
n

i=1
(cid:88)
n

i=1
(cid:88)

ˆW t+1
i

( ˆW t+1
i

)

(cid:124)

= EW (cid:63),ξ

(cid:2)

(cid:0)
fw

(cid:1)

Σt, T t[W (cid:63), ξ]

fw

Σt, T t[W (cid:63), ξ]

(cid:3)

(cid:0)

(cid:124)

(cid:1)

(cid:3)

σt+1 = EW (cid:63) lim
n→∞

ˆCt+1
i = EW (cid:63),ξ

fc

Σt, T t[W (cid:63), ξ]

(cid:2)

(cid:0)

(cid:1)

(cid:2)

(cid:0)

(cid:1)(cid:3)

ˆqt = αEω,z,A

gout(ω, ϕ0

= α

dPA(A)
(cid:2)

out(z, A), σt)gout(ω, ϕ0
z, ω; 0, Qt

dzdωN

out(z, A), σt)(cid:124)

gout(ω, ϕ0

out(z, A), σt)gout(ω, ϕ0

out(z, A), σt)

(cid:3)

(cid:124)

= α

dPA(A)
(cid:2)

dzdωN

∂zgout(ω, ϕ0

out(z, A), σt)

ˆmt = αEω,z,A

∂zgout(ω, ϕ0

(cid:90)

(cid:90)

(cid:90)

(cid:90)

ˆχt = αEω,z,A

−∂ωgout(ω, ϕ0

= −α

dPA(A)

(cid:2)

dzdωN

(cid:90)

(cid:90)

(cid:0)

out(z, A), σt)
z, ω; 0, Qt
(cid:3)

(cid:1)

out(z, A), σt)
(cid:0)
(cid:1)
z, ω; 0, Qt
(cid:3)

(cid:0)

(cid:1)

∂ωgout(ω, ϕ0

out(z, A), σt)

G.3 State evolution equations - Bayes optimal case

and











In the bayes optimal case, the student knows all the parameters of the teacher and then P (cid:63)
mt = qt and ˆqt = ˆmt = ˆχt, σt = Q − qt and then, naturally

0 = P0, ϕ0

out = ϕout,

T t[W (cid:63), ξ] ≡ W (cid:63) + (ˆqt)−1/2ξ ,
Σt ≡ (ˆqt)−1 .

In the Bayes-optimal case, the set of state evolution equations reduces and simplifies to:

qt+1 = EW (cid:63),ξ

fw

Σt, T t[W (cid:63), ξ]

fw

Σt, T t[W (cid:63), ξ]

(cid:124)

,

ˆqt = αEω,z,A

(cid:2)
(cid:0)
gout(ω, ϕout(z, A), σt)gout(ω, ϕout(z, A), σt)(cid:124)

(cid:1)

(cid:1)

(cid:0)

(cid:3)

,

(111)

(cid:3)






(cid:0)

where (z, ω) ∼ Nz,ω

0, 0; Qt

with Qt =

(cid:2)

Q qt
qt
qt

.

(cid:35)

(cid:34)

(cid:1)

G.4 State evolution - Consistence between replicas and AMP - Bayes optimal case

State evolution - AMP

Using the change of variable ξ ← ξ +

1/2

ˆqt

W (cid:63), eq. (111) becomes:

qt+1 = Eξ

ZP0

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

(cid:0)

(cid:1)

(cid:104)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:124)

(cid:17)

(cid:105)

47

In addition in the Bayes-optimal case, as:

µ→i)(cid:124)

= mt − qt = 0

µ→i)(zµ − ωt

µ→i)(cid:124)

= Q − qt ,

µ→i)(cid:124)] = qt

(cid:105)



EX

ωt
µ→i(zµ − ωt
(cid:104)
EX [ωt

EX


µ→i(ωt
(cid:124)
µ − ωt

(z

(cid:104)

(cid:105)
0, 0; Qt

(cid:0)

(cid:1)

(cid:124)

the multivariate distribution can be written as a product: Nz,ω
using Pout(y|z) =

dPA(A)δ

y − ϕ0

, eq. (111) becomes:

out(z, A)

= Nω

0, qt

Nz

ω, Q − qt

. Hence,

(cid:0)

(cid:1)

(cid:0)

(cid:1)

ˆqt = αEω,z,A

(cid:2)

(cid:90)

(cid:82)
gout(ω, ϕ0
e− 1

(cid:0)

(cid:1)

out(z, A), Q − qt)

out(z, A), Q − qt)gout(ω, ϕ0
2 ω(cid:124)(qt)−1ω
(2π)K/2 det(qt)1/2
e− 1
(2π)K/2 det(Q − qt)1/2 gout((qt)1/2ξ, y, Q − qt)gout((qt)1/2ξ, y, Q − qt)

e− 1
(2π)K/2 det(Q − qt)1/2 gout(ω, y, Q − qt)gout(ω, y, Q − qt)

(cid:90)
2 (z−ω)(cid:124)(Q−qt)−1(z−ω)

2 (z−ω)(cid:124)(Q−qt)−1(z−ω)

dzPout(y|z)

dzPout(y|z)

(cid:3)

(cid:124)

(cid:124)

= α

dy

dω

(cid:90)

= α

dy

Dξ

(cid:90)
= αEy,ξ

(cid:90)
ZPout

(cid:90)
(qt)1/2ξ, y, Q − qt

gout

(qt)1/2ξ, y, Q − qt

gout

(qt)1/2ξ, y, Q − qt

(cid:16)
(cid:17)
Finally to summarize the state evolution equations can be written as:

(cid:16)

(cid:17)

(cid:16)

(cid:104)

(cid:124)

(cid:17)

(cid:105)

(cid:124)

qt+1 = Eξ

ZP0

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

(cid:104)
ˆqt = αEy,ξ

(cid:16)
ZPout

(qt)1/2ξ, y, Q − qt

gout

(cid:17)

(cid:16)

(qt)1/2ξ, y, Q − qt

(cid:17)

(cid:16)

gout

(cid:105)
(qt)1/2ξ, y, Q − qt

(cid:17)

(cid:124)

(112)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:105)

(cid:1)






State evolution - Replicas

(cid:104)

(cid:0)

Recall from sec. B, the free entropy eq. (76) reads

limn→∞ fn = extrq,ˆq

− 1

2 Tr[q ˆq] + IP + αIC

,

≡ Eξ

ZP0(ˆq1/2ξ, ˆq−1) log(ZP0(ˆq1/2ξ, ˆq−1))

(cid:9)

(cid:8)

,

(cid:2)
≡ Eξ,y

ZPout(q1/2ξ, y, Q − q) log(ZPout(q1/2ξ, y, Q − q))

(cid:3)

.

IP

IC






Taking the derivatives with respect to q and ˆq, using an integration by part and the following identities:

(cid:2)

(cid:3)

∂ZPout

∂q = − 1

2 q−1e

1

2 ξ(cid:124)ξ∂ξ

∂ZP0
∂ ˆq = − 1

2 ˆq−1e

1

2 ξ(cid:124)ξ∂ξ

e− 1
(cid:104)
e− 1

2 ξ(cid:124)ξ∂ξZPout
2 ξ(cid:124)ξ∂ξZP0

,

(cid:105)

,

(cid:104)

(cid:105)






the state evolution equations read:

q = 2 ∂IP
∂ ˆq




ˆq = 2α ∂IC
∂q

with

2

∂IP
∂ ˆq = 1
∂IC
∂q = 1

2




Eξ

(cid:2)
Ey,ξ

ZP0(ˆq1/2ξ, ˆq−1)fw(ˆq1/2ξ, ˆq)fw(ˆq1/2ξ, ˆq−1)(cid:124)
ZPout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)(cid:124)

(cid:3)

(cid:3)

that simplifies and allows to recover the state evolutions equations directly derived from AMP eq. (112), but



(cid:2)

48

without time indices

q = Eξ

ZP0(ˆq1/2ξ, ˆq−1)fw(ˆq1/2ξ, ˆq)fw(ˆq1/2ξ, ˆq−1)(cid:124)

,




(cid:2)
ˆq = αEy,ξ

ZPout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)(cid:124)

(cid:3)

.

(cid:3)

(cid:2)
H Parity machine for K = 2



Although we mainly focused on the committee machine, another classical two-layers neural network is the
parity machine [7] and our proof applies to this case as well. While learning is known to be computationally
hard for general K, the case K = 2 is special, and in fact can be reformulated as a committee machine, where
the sign activation function has been replaced by ϕ1(z) = 1(z (cid:54)= 0) − 1(z = 0):

K

n

K

n

Yµ = sign

sign

XµiW ∗
il

= ϕ1

sign

XµiW ∗
il

.

(113)

(cid:104)

l=1
(cid:89)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

(cid:104)

l=1
(cid:88)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

We have repeated our analysis for the K = 2 parity machine and the phase diagram is summarized in
Fig. 5 where we show the generalization error and the elements of the overlap matrix for Gaussian (left) and
binary weights (right), with the results of the AMP algorithm (points).

Below the specialization phase transition α < αspec, the symmetry of the output imposes the non-
specialized fixed point q00 = q01 = 0 to be the only solution, with αG
spec(K = 2) (cid:39)
2.49. Above the specialization transition αspec, the overlap becomes specialized with a non-trivial diagonal
term.

spec(K = 2) (cid:39) 2.48 and αB

Additionally, in the binary case, an information theoretical transition towards a perfect learning occurs
at αB
IT(K = 2) (cid:39) 2.00, meaning that the perfect generalization fixed point (q00 = 1, q01 = 0) becomes the
global optimizer of the free entropy. It leads to a first order phase transition of the AMP algorithm which
retrieves the perfect generalization phase only at αB
perf (K = 2) (cid:39) 3.03. This is similar to what happens in
single layer neural networks for the symmetric door activation function, see [11]. Again, these results for the
parity machine emphasize a gap between information-theoretical and computational performance.

49

Figure 5: Similar plot as in Fig. 2 but for the parity machine with two hidden neurons. Value of the order parameter
and the optimal generalization error for a parity machine with two hidden neurons with Gaussian weights (left)
and binary/Rademacher weights (right). SE and AMP overlaps are respectively represented in full line and points.

50

9
1
0
2
 
n
u
J
 
4
1
 
 
]

G
L
.
s
c
[
 
 
2
v
1
5
4
5
0
.
6
0
8
1
:
v
i
X
r
a

The committee machine: Computational to statistical gaps
in learning a two-layers neural network

Benjamin Aubin(cid:63)†, Antoine Maillard†, Jean Barbier♦,
Florent Krzakala†, Nicolas Macris⊗ and Lenka Zdeborová(cid:63)

Abstract

Heuristic tools from statistical physics have been used in the past to locate the phase transitions and
compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural
networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers
neural network model called the committee machine, under a technical assumption. We also introduce a
version of the approximate message passing (AMP) algorithm for the committee machine that allows to
perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes
in which a low generalization error is information-theoretically achievable while the AMP algorithm fails
to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large
computational gap.

Contents

1 Introduction

2 Summary of contributions and related works

3 Main technical results
3.1 A general model .
.
3.2 Two auxiliary inference problems .
3.3 The free entropy .
.
3.4
3.5 Approximate message passing, and its state evolution .

.
.
.
.
Learning the teacher weights and optimal generalization error .
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

4 From two to more hidden neurons, and the specialization phase transition
.
.
.
.

4.1 Two neurons .
.
4.2 More is different .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

5 Structure of the proof of Theorem 3.1
Interpolating estimation problem .

5.1
.
5.2 Overlap concentration and fundamental sum rule .
.
5.3 A technical lemma and an assumption .
.
.
5.4 Matching bounds

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

3

3

5
5
5
6
7
8

10
10
11

12
12
14
15
16

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

.
.
.
.
.

.
.

.
.
.
.

(cid:63) Institut de Physique Théorique, CNRS & CEA & Université Paris-Saclay, Saclay, France.
† Laboratoire de Physique Statistique, CNRS & Sorbonnes Universités & École Normale Supérieure, PSL University, Paris, France.
⊗ Laboratoire de Théorie des Communications, École Polytechnique Fédérale de Lausanne, Suisse.
♦ International Center for Theoretical Physics, Trieste, Italy.

1

C.1 The generalization error at K = 2 .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

6 Discussion

A Proof details for Theorem 3.1

A.1 The Nishimori property in Bayes-optimal learning .
.
.
A.2 Setting in the Hamiltonian language .
.
A.3 Free entropy variation: Proof of Proposition 5.2 .
.
.
.
.
A.4 Technical lemmas .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

B Replica calculation

C Generalization error

D The large K limit in the committee symmetric setting
.
.
.
.

D.1 Large K limit for sign activation function .
.
D.2 The Gaussian prior
.
.
.
.
D.3 The fixed point equations .
.
D.4 The generalization error at large K .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

E Linear networks show no specialization

F Update functions and AMP derivation
F.1 Definition of the update functions .
.
F.2 Derivation of the Approximate Message Passing algorithm .

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.

G State evolution equations from AMP
.

.
G.1 Messages distribution .
.
.
.
G.2 State evolution equations - Non Bayes optimal case .
G.3 State evolution equations - Bayes optimal case .
.
.
G.4 State evolution - Consistence between replicas and AMP - Bayes optimal case .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

H Parity machine for K = 2

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.
.
.

18

23
23
23
24
26

29

31
32

32
33
36
36
38

38

39
39
39

44
45
46
47
47

49

2

1 Introduction

While the traditional approach to learning and generalization follows the Vapnik-Chervonenkis [1] and
Rademacher [2] worst-case type bounds, there has been a considerable body of theoretical work on calculating
the generalization ability of neural networks for data arising from a probabilistic model within the framework
of statistical mechanics [3, 4, 5, 6, 7]. In the wake of the need to understand the effectiveness of neural
networks and also the limitations of the classical approaches [8], it is of interest to revisit the results that have
emerged thanks to the physics perspective. This direction is currently experiencing a strong revival, see e.g.
[9, 10, 11, 12].

Of particular interest is the so-called teacher-student approach, where labels are generated by feeding
i.i.d. random samples to a neural network architecture (the teacher) and are then presented to another neural
network (the student) that is trained using these data. Early studies computed the information theoretic
limitations of the supervised learning abilities of the teacher weights by a student who is given m independent
n-dimensional examples with α ≡ m/n = Θ(1) and n → ∞ [3, 4, 7]. These works relied on non-rigorous
heuristic approaches, such as the replica and cavity methods [13, 14]. Additionally no provably efficient
algorithm was provided to achieve the predicted learning abilities, and it was thus difficult to test those
predictions, or to assess the computational difficulty.

Recent developments in statistical estimation and information theory —in particular of approximate
message passing algorithms (AMP) [15, 16, 17, 18], and a rigorous proof of the replica formula for the optimal
generalization error [11]— allowed to settle these two missing points for single-layer neural networks (i.e.
without any hidden variables). In the present paper, we leverage on these works, and provide rigorous
asymptotic predictions and corresponding message passing algorithm for a class of two-layers networks.

2 Summary of contributions and related works

While our results hold for a rather large class of non-linear activation functions, we illustrate our findings on a
case considered most commonly in the early literature: the committee machine. This is possibly the simplest
version of a two-layers neural network where all the weights in the second layer are fixed to unity, and we
illustrate it in Fig. 1. Denoting Yµ the label associated with a n-dimensional sample Xµ, and W ∗
il the weight
connecting the i-th coordinate of the input to the l-th node of the hidden layer, it is defined by:

K

n

Yµ = sign

sign

XµiW ∗
il

.

(cid:104)

l=1
(cid:88)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

(1)

We concentrate here on the teacher-student scenario: The teacher generates i.i.d. data samples with i.i.d.
standard Gaussian coordinates Xµi ∼ N (0, 1), then she/he generates the associated labels Yµ using a committee
machine as in (1), with i.i.d. weights W ∗
il unknown to the student (in the proof section we will consider the
l=1), but in practice we consider
more general case of a distribution for the weights of the form
the fully separable case). The student is then given the m input-output pairs (Xµ, Yµ)m
µ=1 and knows the
(cid:81)
distribution P0 used to generate W ∗
il. The goal of the student is to learn the weights W ∗
il from the available
examples (Xµ, Yµ)m
µ=1 in order to reach the smallest possible generalization error (i.e. to be able to predict the
label the teacher would generate for a new sample not present in the training set).

n
i=1 P0({W ∗

il}K

There have been several studies of this model within the non-rigorous statistical physics approach in
the limit where α ≡ m/n = Θ(1), K = Θ(1) and n → ∞ [19, 20, 21, 22, 6, 7]. A particularly interesting
result in the teacher-student setting is the specialization of hidden neurons (see sec. 12.6 of [7], or [23] in the
context of online learning): For α < αspec (where αspec is a certain critical value of the sample complexity),

3

n features

(Xµi)m,n
µ,i=1
samples

K hidden
units

f (1)

f (1)

output

f (2)

Yµ

W (2) ∈ RK

W ∗

il ∈ Rn×K

Figure 1: The committee machine is one of the simplest models belonging to the considered model class (2), and
on which we focus to illustrate our results. It is a two-layers neural network with activation sign functions
f (1), f (2) = sign and weights W (2) fixed to unity. It is represented for K = 2.

the permutational symmetry between hidden neurons remains conserved even after an optimal learning, and
the learned weights of each of the hidden neurons are identical. For α > αspec, however, this symmetry gets
broken as each of the hidden units correlates strongly with one of the hidden units of the teacher. Another
remarkable result is the calculation of the optimal generalization error as a function of α.

Our first contribution consists in a proof of the replica formula conjectured in the statistical physics
literature, using the adaptive interpolation method of [24, 11], that allows to put several of these results on a
rigorous basis. This proof uses a technical unproven assumption. Our second contribution is the design of
an AMP-type of algorithm that is able to achieve the optimal generalization error in the above limit of large
dimensions for a wide range of parameters. The study of AMP —that is widely believed to be optimal between
all polynomial algorithms in the above setting [25, 26, 27, 28]— unveils, in the case of the committee machine
with a large number of hidden neurons, the existence a large hard phase in which learning is information-
theoretically possible, leading to a good generalization error decaying asymptotically as 1.25K/α (in the
α = Θ(K) regime), but where AMP fails and provides only a poor generalization that does not go to zero
when increasing α. This strongly suggests that no efficient algorithm exists in this hard region and therefore
there is a computational gap in learning such neural networks. In other problems where a hard phase was
identified its study boosted the development of algorithms that are able to match the predicted thresholds and
we anticipate this will translate to the present model.

We also want to comment on a related line of work that studies the loss-function landscape of neural
networks. While a range of works show under various assumptions that spurious local minima are absent
in neural networks, others show under different conditions that they do exist, see e.g. [29]. The regime of
parameters that is hard for AMP must have spurious local minima, but the converse is not true in general. It
might be that there are spurious local minima, yet the AMP approach succeeds. Moreover, in all previously
studied models in the Bayes-optimal setting the (generalization) error obtained with the AMP is the best
known and other approaches, e.g. (noisy) gradient based, spectral algorithms or semidefinite programming,
are not better in generalizing even in cases where the “student” models are overparametrized. Of course in
order to be in the Bayes-optimal setting one needs to know the model used by the teacher which is not the
case in practice.

4

3 Main technical results

3.1 A general model

While in the illustration of our results we shall focus on the model (1), all our formulas are valid for a broader
class of models: Given m input samples (Xµi)m,n
il the teacher-weight connecting the i-th
input (i.e. visible unit) to the l-th node of the hidden layer. For a generic function ϕout : RK × R → R one can
formally write the output as

µ,i=1, we denote W ∗

Yµ = ϕout

n

1
√
n

(cid:16)(cid:110)

i=1
(cid:88)

XµiW ∗
il

K

l=1

, Aµ

(cid:111)

(cid:17)

or

Yµ ∼ Pout

·

1
√
n

n

i=1
(cid:88)

(cid:16)

(cid:12)
(cid:110)
(cid:12)
(cid:12)

K

XµiW ∗
il

,

(2)

l=1

(cid:111)

(cid:17)

where (Aµ)m
part of the model, generally accounting for noise.

µ=1 are i.i.d. real valued random variables with known distribution PA, that form the probabilistic

For deterministic models the second argument is simply absent (or is a Dirac mass). We can view altern-
atively (2) as a channel where the transition kernel Pout is directly related to ϕout. As discussed above, we
focus on the teacher-student scenario where the teacher generates Gaussian i.i.d. data Xµi ∼ N (0, 1), and i.i.d.
weights W ∗
µ=1 by computing marginal means of
the posterior probability distribution (5).

il ∼ P0. The student then learns W ∗ from the data (Xµ, Yµ)m

Different scenarii fit into this general framework. Among those, the committee machine is obtained when
K
l=1 sign(hl)) while another model considered previously is given by the parity
choosing ϕout(h) = sign(
K
l=1 sign(hl), see e.g. [7] and sec. H for the numerical results in the case K = 2.
machine, when ϕout(h) =
A number of layers beyond two has also been considered, see [22]. Other activation functions can be used, and
many more problems can be described, e.g. compressed pooling [30, 31] or multi-vector compressed sensing
[32].

(cid:80)
(cid:81)

3.2 Two auxiliary inference problems

Denote SK the finite dimensional vector space of K × K matrices, S +
K × K matrices, S ++
K s.t. N − M ∈ S +
S+

K for positive definite K × K matrices, and ∀ N ∈ S +
K}. Note that S +
K(N ) is convex and compact.

K the convex set of semi-definite positive
K(N ) ≡ {M ∈

K we set S+

Stating our results requires introducing two simpler auxiliary K-dimensional estimation problems:
• The first one consists in retrieving a K-dimensional input vector W0 ∼ P0 from the output of a Gaussian
vector channel with K-dimensional observations

Y0 = r1/2W0 + Z0 ,

Z0 ∼ N (0, IK×K) and the “channel gain” matrix r ∈ S +

K. The posterior distribution on w = (wl)K

l=1 is

P (w|Y0) =

P0(w)eY

(cid:124)
0 r1/2w− 1

2 w(cid:124)rw ,

1
ZP0

(3)

and the associated free entropy (or minus free energy) is given by the expectation over Y0 of the log-partition
function

ψP0(r) ≡ E ln ZP0

and involves K dimensional integrals.
• The second problem considers K-dimensional i.i.d. vectors V, U ∗ ∼ N (0, IK×K) where V is considered to

5

be known and one has to retrieve U ∗ from a scalar observation obtained as

Y0 ∼ Pout( · |q1/2V + (ρ − q)1/2U ∗)

where the second moment matrix ρ ≡ E[W0W
q is in S+

K(ρ). The associated posterior is

(cid:101)

(cid:124)
0 ] is in S +

K (where W0 ∼ P0) and the so-called “overlap matrix”

P (u|

Y0, V ) =

1
ZPout

2 u(cid:124)u
e− 1
(2π)K/2 Pout

Y0|q1/2V + (ρ − q)1/2u

,

(4)

and the free entropy reads this time

(cid:101)

(cid:0)

(cid:101)

(cid:1)

ΨPout(q; ρ) ≡ E ln ZPout

(with the expectation over

Y0 and V ) and also involves K dimensional integrals.

3.3 The free entropy

(cid:101)

The central object of study leading to the optimal learning and generalization errors in the present setting is
the posterior distribution of the weights:

P ({wil}n,K

i,l=1 | {Xµi, Yµ}m,n

µ,i=1) =

P0({wil}K

l=1)

Pout

Yµ

Xµiwil

,

(5)

1
Zn

n

i=1
(cid:89)

m

µ=1
(cid:89)

(cid:16)

1
√
n

n

i=1
(cid:88)

(cid:110)

(cid:12)
(cid:12)
(cid:12)

K

l=1

(cid:111)

(cid:17)

where the normalization factor is nothing else than a partition function, i.e. the integral of the numerator over
{wil}n,K

i,l=1. The expected1 free entropy is by definition

fn ≡

E ln Zn .

1
n

The replica formula gives an explicit (conjectural) expression of fn in the high-dimensional limit n, m → ∞
with α = m/n fixed. We show in sec. B how the heuristic replica method [13, 14] yields the formula. This
computation was first performed, to the best of our knowledge, by [19] in the case of the committee machine.
Our first contribution is a rigorous proof of the corresponding free entropy formula using an interpolation
method [33, 34, 24], under a technical Assumption 1.

In order to formulate our results, we add an (arbitrarily small) Gaussian regularization noise Zµ

∆ to the

√

first expression of the model (2), where ∆ > 0, Zµ ∼ N (0, 1), which thus becomes

Yµ = ϕout

n

1
√
n

(cid:16)(cid:110)

i=1
(cid:88)

XµiW ∗
il

, Aµ

+ Zµ

∆ ,

√

K

l=1

(cid:111)

(cid:17)

so that the channel kernel is (u ∈ RK)

Pout(y|u) =

√

dPA(a)e− 1

2∆ (y−ϕout(u,a))2

.

1
2π∆

R

(cid:90)
Let us define the replica symmetric (RS) potential as

fRS(q, r) = fRS(q, r; ρ) ≡ ψP0(r) + αΨPout(q; ρ) −

Tr(rq),

1
2

1The symbol E will generally denote an expectation over all random variables in the ensuing expression (here {Xµi, Yµ}).

Subscripts will be used only when we take partial expectations or if there is an ambiguity.

(6)

(7)

(8)

(9)

6

where α ≡ m/n, and ΨPout(q; ρ) and ψP0(r) are the free entropies of the two simpler K-dimensional
estimation problems (3) and (4).

All along this paper, we assume the following hypotheses for our rigorous statements:

(H1) The prior P0 has bounded support in RK.
(H2) The activation ϕout : RK × R → R is a bounded C2 function with bounded first and second derivatives

w.r.t. its first argument (in RK-space).

(H3) For all µ = 1, . . . , m and i = 1, . . . , n we have i.i.d. Xµi ∼ N (0, 1).

We finally rely on a technical hypothesis, stated as Assumption 1 in section 5.3.

Theorem 3.1 (Replica formula). Suppose (H1), (H2) and (H3), and Assumption 1. Then for the model (7) with
kernel (8) the limit of the free entropy is:

lim
n→∞

fn ≡ lim
n→∞

1
n

E ln Zn = sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

(10)

This theorem extends the recent progress for generalized linear models of [11], which includes the case
K = 1 of the present contribution, to the phenomenologically richer case of two-layers problems such as the
committee machine. The proof sketch based on an adaptive interpolation method recently developed in [24] is
outlined in sec. 5 and the details can be found in sec. A.

Remark 3.2 (Relaxing the hypotheses). Note that, following similar approximation arguments as in [11], the
hypothesis (H1) can be relaxed to the existence of the second moment of the prior; thus covering the Gaussian case,
(H2) can be dropped (and thus include model (1) and its sign(·) activation) and (H3) extended to data matrices X
with i.i.d. entries of zero mean, unit variance and finite third moment. Moreover, the case ∆ = 0 can be considered
when the outputs are discrete, as in the committee machine (1), see [11]. The channel kernel becomes in this case
dPA(a)1(y − ϕout(u, a)) and the replica formula is the limit ∆ → 0 of the one provided in
Pout(y|u) =
Theorem 3.1. In general this regularizing noise is needed for the free entropy limit to exist.

(cid:82)

3.4 Learning the teacher weights and optimal generalization error

A classical result in Bayesian estimation is that the estimator ˆW that minimizes the mean-square error with the
ground-truth W ∗ is given by the expected mean of the posterior distribution. Denoting q∗ the extremizer in
the replica formula (10), we expect from the replica method that in the limit n → ∞, m/n = α, and with high
probablity, ˆW (cid:124)W ∗/n → q∗. We refer to proposition 5.3 and to the proof in sec. A for the precise statement,
that remains rigorously valid only in the presence of an additional (possibly infinitesimal) side-information.
From the overlap matrix q∗, one can compute the Bayes-optimal generalization error when the student tries to
classify a new, yet unseen, sample Xnew. The estimator of the new label ˆYnew that minimizes the mean-square
error with the true label is given by computing the posterior mean of ϕout(Xneww) (Xnew is a row vector).
Given the new sample, the optimal generalization error is then

1
2

EX,W ∗

Ew|X,Y

ϕout(Xneww)

− ϕout(XnewW ∗)

2

−−−→
n→∞

(cid:15)g(q∗),

(11)

(cid:104)(cid:0)

(cid:2)

(cid:3)

(cid:105)

(cid:1)

where w is distributed according to the posterior measure (5) (note that this Bayes-optimal computation differs
from the so-called Gibbs estimator by a factor 2, see sec. C). In particular, when the data X is drawn from
the standard Gaussian distribution on Rm×n, and is thus rotationally invariant, it follows that this error only
depends on w(cid:124)W ∗/n, which converges to q∗. Then a direct algebraic computation gives a lengthy but explicit
formula for (cid:15)g(q∗), as shown in sec. C.

7

3.5 Approximate message passing, and its state evolution

Our next result is based on an adaptation of a popular algorithm to solve random instances of generalized linear
models, the Approximate Message Passing (AMP) algorithm [15, 16], for the case of the committee machine and
models described by (2).

The AMP algorithm can be obtained as a Taylor expansion of loopy belief-propagation (see sec. F) and
also originates in earlier statistical physics works [35, 36, 37, 38, 39, 26]. It is conjectured to perform the best
among all polynomial algorithms in the framework of these models. It thus gives us a tool to evaluate both the
intrinsic algorithmic hardness of the learning and the performance of existing algorithms with respect to the
optimal one in this model.

Algorithm 1 Approximate Message Passing for the committee machine

Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: gout,µ = 0, Σi = IK×K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
Initialize: ˆWi ∈ RK and ˆCi, ∂ωgout,µ ∈ S +
repeat

K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 1.

n

ωt

ˆW t

µ =

i −

Xµi√
n

Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
X 2
µi
n

i Σt−1
i

−1 ˆCt

gt−1
out,µ

Σt−1
i

i=1
(cid:80)
out,µ = gout(ωt
gt

Update of gout,µ ∈ RK and ∂ωgout,µ ∈ S +
K:
out,µ = ∂ωgout(ωt

µ, Yµ, V t
µ)
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt

(cid:1)
∂ωgt

Xµi√

i = Σt
T t
i

X 2
n ∂ωgt
µi

out,µ −

ˆW t
i

out,µ

n gt

m

(cid:1)

(cid:0)

(cid:0)

|

|

(cid:16)

µ=1
(cid:80)

Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = fw(Σt

i, T t
i )

|

ˆCt+1

i = fc(Σt

i, T t
i )

(cid:17)

µ, Yµ, V t
µ)

i = −
(cid:16)

m

µ=1
(cid:80)

V t
µ =

X 2
µi
n

ˆCt
i

n

i=1
(cid:80)

X 2
n ∂ωgt
µi

out,µ

−1

(cid:17)

ˆW t+1
t = t + 1

until Convergence on ˆW , ˆC.
Output: ˆW and ˆC.

The AMP algorithm is summarized by its pseudo-code in Algorithm 2, where the update functions gout,
∂ωgout, fw and fc are related, again, to the two auxiliary problems (3) and (4). The functions fw(Σ, T ) and
fc(Σ, T ) are respectively the mean and variance under the posterior distribution (3) when r → Σ−1 and
Y0 → Σ1/2T , while gout(ωµ, Yµ, Vµ) is given by the product of V
and the mean of u under the posterior
Y0 → Yµ, ρ − q → Vµ and q1/2V → ωµ (see sec. F for more details). After convergence, ˆW estimates
(4) using
the weights of the teacher-neural network. The label of a sample Xnew not seen in the training set is estimated
by the AMP algorithm as

−1/2
µ

(cid:101)

Y t
new =

dy

dzl

y Pout(y|{zl}K

l=1)N (z; ωt

new, V t

new) ,

(12)

K

(cid:90)

l=1
(cid:89)

(cid:0)

(cid:1)

new =

n
i=1 Xnew,i ˆW t

where ωt
is the K × K covariance matrix (see below for the definition of qt
(cid:80)
the algorithm on GitHub [40].

i is the mean of the normally distributed variable z ∈ RK, and V t

new = ρ−qt

AMP
AMP). We provide a demonstration code of

AMP is particularly interesting because its performance can be tracked rigorously, again in the asymptotic
limit when n → ∞, via a procedure known as state evolution (a rigorous version of the cavity method in
physics [14]), see [18]. State evolution tracks the value of the overlap between the hidden ground truth W ∗ and

8

Figure 2: Generalization error and order parameter for a committee machine with two hidden neurons (K = 2)
with Gaussian weights (left), binary/Rademacher weights (right). These are shown as a function of the ratio
α = m/n between the number of samples m and the dimensionality n. Lines are obtained from the state evolution
(SE) equations (dominating solution is shown in full line), data-points from the AMP algorithm averaged over 10
instances of the problem of size n = 104. q00 and q01 denote diagonal and off-diagonal overlaps, and their values
are given by the labels on the far-right of the figure.

the AMP estimate ˆW t, defined as qt

AMP ≡ limn→∞( ˆW t)(cid:124)W ∗/n, via the iteration of the following equations:

qt+1
AMP = 2∇ψP0(rt

AMP) ,

rt+1
AMP = 2α∇ΨPout(qt

AMP; ρ) .

(13)

See sec. G for more details and note that the fixed points of these equations correspond to the critical points of
the replica free entropy (10).

Let us comment further on the convergence of the algorithm. In the large n limit, and if the integrals
are performed without errors, then the algorithm is guaranteed to converge. This is a consequence of the
state evolution combined with the Bayes-optimal setting. In practice, of course, n is finite and integrals are
approximated. In that case convergence is not guaranteed, but is robustly achieved in all the cases presented
in this paper. We also expect (by experience with the single layer case) that if the input-data matrix is not
random (which is beyond our assumptions) then we will encounter convergence issues, which could be fixed
by moving to some variant of the algorithm such as VAMP [41].

9

Figure 3: (Left) Bayes optimal and AMP generalization errors and (right) diagonal and off-diagonal overlaps q00,
q01 for a committee machine with a large number of hidden neurons K and Gaussian weights, as a function of the
rescaled parameter ˜α = α/K. Solutions corresponding to global and local minima of the replica free entropy are
spinodal (cid:39) 7.17, ie the
respectively represented with full and dashed lines. The dotted line marks the spinodal at
apparition of a local minimum in the replica free entropy, associated to a solution with specialized hidden units.
spec (cid:39) 7.65, at which the specialized
The dotted-dashed line shows the first order specialization transition at
spec, AMP reaches the Bayes optimal generalization error
fixed point becomes the global minimum. For
and overlaps, corresponding to a non-specialized solution. However, for
spec, the AMP algorithm does not
follow the optimal specialized solution and is stuck in the non-specialized solution plateau, represented with
dashed lines. Hence it unveils a large computational gap (yellow area).

α >
(cid:101)

α <

αG

αG

αG

αG

(cid:101)

(cid:101)

(cid:101)

(cid:101)

(cid:101)

4 From two to more hidden neurons, and the specialization

phase transition

4.1 Two neurons

Let us now discuss how the above results can be used to study the optimal learning in the simplest non-trivial
case of a two-layers neural network with two hidden neurons, that is when model (1) is simply

n

n

Yµ = sign

sign

XµiW ∗
i1

+ sign

XµiW ∗
i2

,

(cid:104)

(cid:16)

i=1
(cid:88)

(cid:17)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

and is represented in Fig. 1, with the convention that sign(0) = 0. We remind that the input-data matrix X
has i.i.d. N (0, 1) entries, and the teacher-weights W ∗ used to generate the labels Y are taken i.i.d. from P0.
In Fig. 2 we plot the optimal generalization error as a function of the sample complexity α = m/n. In
the left panel the weights are Gaussian (for both the teacher and the student), while in the right panel they
are binary/Rademacher. The full line is obtained from the fixed point of the state evolution (SE) of the AMP
algorithm (13), corresponding to the extremizer of the replica free entropy (10). The points are results of
the AMP algorithm run till convergence averaged over 10 instances of size n = 104. In this case and with
random initial conditions the AMP algorithm did converge in all our trials. As expected we observe excellent

10

agreement between the SE and AMP.

In both left and right panels of Fig. 2 we observe the so-called specialization phase transition. Indeed
(13) has two types of fixed points: a non-specialized fixed point where every matrix element of the K × K
order parameter q is the same (so that both hidden neurons learn the same function) and a specialized fixed
point where the diagonal elements of the order parameter are different from the non-diagonal ones. We
checked for other types of fixed points for K = 2 (one where the two diagonal elements are not the same),
but have not found any. In terms of weight-learning, this means for the non-specialized fixed point that the
estimators for both W1 and W2 are the same, whereas in the specialized fixed point the estimators of the
weights corresponding to the two hidden neurons are different, and that the network “figured out” that the
data are better described by a model that is not linearly separable. The specialized fixed point is associated
with lower error than the non-specialized one (as one can see in Fig. 2). The existence of this phase transition
was discussed in statistical physics literature on the committee machine, see e.g. [20, 23].

For Gaussian weights (Fig. 2 left), the specialization phase transition arises continuously at αG

spec(K =
2) (cid:39) 2.04. This means that for α < αG
spec(K = 2) the number of samples is too small, and the student-neural
network is not able to learn that two different teacher-vectors W1 and W2 were used to generate the observed
labels. For α > αG
spec(K = 2), however, it is able to distinguish the two different weight-vectors and the
generalization error decreases fast to low values (see Fig. 2). For completeness we remind that in the case
of K = 1 corresponding to single-layer neural network no such specialization transition exists. We show
in sec. E that it is absent also in multi-layer neural networks as long as the activations remain linear. The
non-linearity of the activation function is therefore an essential ingredient in order to observe a specialization
phase transition.

The right part of Fig. 2 depicts the fixed point reached by the state evolution of AMP for the case of binary
weights. We observe two phase transitions in the performance of AMP in this case: (a) the specialization phase
spec(K = 2) (cid:39) 1.58, and for slightly larger sample complexity a transition towards perfect
transition at αB
generalization (beyond which the generalization error is asymptotically zero) at αB
perf (K = 2) (cid:39) 1.99. The
binary case with K = 2 differs from the Gaussian one in the fact that perfect generalization is achievable at finite
α. While the specialization transition is continuous here, the error has a discontinuity at the transition of perfect
generalization. This discontinuity is associated with the 1st order phase transition (in the physics nomenclature),
leading to a gap between algorithmic (AMP in our case) performance and information-theoretically optimal
performance reachable by exponential algorithms. To quantify the optimal performance we need to evaluate
the global extremum of the replica free entropy (not the local one reached by the state evolution). In doing
so that we get that information theoretically there is a single discontinuous phase transition towards perfect
generalization at αB

IT(K = 2) (cid:39) 1.54.

While the information-theoretic and specialization phase transitions were identified in the physics literature
on the committee machine [20, 21, 3, 4], the gap between the information-theoretic performance and the
performance of AMP —that is conjectured to be optimal among polynomial algorithms— was not yet discussed
in the context of this model. Indeed, even its understanding in simpler models than those discussed here, such
as the single layer case, is more recent [15, 26, 25].

4.2 More is different

It becomes more difficult to study the replica formula for larger values of K as it involves (at least) K-
dimensional integrals. Quite interestingly, it is possible to work out the solution of the replica formula in
the large K limit (thus taken after the large n limit, so that K/n vanishes). It is indeed natural to look for
(cid:124)
K, with the
solutions of the replica formula, as suggested in [19], of the form q = qdIK×K + (qa/K)1K1
unit vector 1K = (1)K
l=1. Since both q and ρ are assumed to be positive, this scaling implies that 0 ≤ qd ≤ 1
and 0 ≤ qa + qd ≤ 1, as it should, see sec. D. We also detail in this same section the corresponding large K

11

expansion of the free entropy for the teacher-student scenario with Gaussian weights. Only the information-
theoretically reachable generalization error was computed [19], thus we concentrated on the analysis of
performance of AMP by tracking the state evolution equations. In doing so, we unveil a large computational
gap.

(cid:101)

(cid:101)

αG

αG

αG

spec and then jumps discontinuously down to reach a decay aymptotically as 1.25/

In the right panel of Fig. 3 we show the fixed point values of the two overlaps q00 = qd + qa/K and
q01 = qa/K and the resulting generalization error, plotted in the left panel. As discussed in [19] it can be
written in a closed form as (cid:15)g = arccos [2 (qa + arcsin qd) /π] /π, represented in the left panel of Fig. 3. The
α ≡ α/K. The specialization is now a 1st order
specialization transition arises for α = Θ(K) so we define
spinodal (cid:39) 7.17 but the free
phase transition, meaning that the specialization fixed point first appears at
spec (cid:39) 7.65. This has
entropy global extremizer remains the one of the non-specialized fixed point until
interesting implications for the optimal generalization error that gets towards a plateau of value εplateau (cid:39) 0.28
for
α. See left panel
α <
of Fig. 3.
(cid:101)
AMP is conjectured to be optimal among all polynomial algorithms (in the considered limit) and thus
analyzing its state evolution sheds light on possible computational-to-statistical gaps that come hand in hand
with 1st order phase transitions. In the regime of α = Θ(K) for large K the non-specialized fixed point is
always stable implying that AMP will not be able to give a lower generalization error than εplateau. Analyzing
the replica formula for large K in more details, see sec. D, we concluded that AMP will not reach the optimal
generalization for any α < Θ(K2). This implies a rather sizable gap between the performance that can
be reached information-theoretically and the one reachable tractably (see yellow area in Fig. 3). Such large
computational gaps have been previously identified in a range of inference problems —most famously in the
planted clique problem [27]— but the committee machine is the first model of a multi-layer neural network
with realistic non-linearities (the parity machine is another example but use a very peculiar non-linearity) that
presents such large gap.

(cid:101)

(cid:101)

(cid:101)

5 Structure of the proof of Theorem 3.1

All along this section we assume (H1), (H2) and (H3), and all the rigorous statements are implicitely assuming
these hypotheses. We denote K-dimensional column vectors by underlined letters. In particular W ∗
i =
(W ∗
µ be K-dimensional vectors with i.i.d. N (0, 1) components.
Let sn ∈ (0, 1/2] a sequence that goes to 0 as n increases, and let M be the compact subset of matrices in
K with eigenvalues in the interval [1, 2]. For all M ∈ snM, 2snIK×K − M ∈ S +
S++
K.

l=1. For µ = 1, . . . m, let V µ, U ∗

l=1, wi = (wil)K

il)K

5.1 Interpolating estimation problem

Let (cid:15) = ((cid:15)1, (cid:15)2) ∈ (snM)2. Let q : [0, 1] → S +
will later on depend on (cid:15)), and

K(ρ) and r : [0, 1] → S +

K be two “interpolation functions” (that

R1(t) ≡ (cid:15)1 +

r(v)dv ,

R2(t) ≡ (cid:15)2 +

q(v)dv .

t

0
(cid:90)

t

0
(cid:90)

For t ∈ [0, 1], define the K-dimensional vector:

1 − t
n

(cid:114)

n

i=1
(cid:88)

St,µ ≡

XµiW ∗

i +

R2(t) V µ +

tρ − R2(t) + 2snIK×K U ∗
µ

(cid:112)
where matrix square-roots (that we denote equivalently A1/2 or
A) are well defined. We interpolate with
auxiliary problems related to those discussed in sec. 3; the interpolating estimation problem is given by the

(cid:112)

√

(14)

(15)

12

following observation model, with two types of t-dependent observations:

Yt,µ ∼ Pout( · | St,µ),
Y (cid:48)

R1(t) W ∗

t,i =

i + Z(cid:48)

1 ≤ µ ≤ m ,
i, 1 ≤ i ≤ n ,

(cid:40)

(16)

i is (for each i) a K-vector with i.i.d. N (0, 1) components, and Y (cid:48)

where Z(cid:48)
t,i is a K-vector as well. Recall that
in our notation the ∗-variables have to be retrieved, while the other random variables are assumed to be known
(except for the noise variables obviously). Define now st,µ by the expression of St,µ but with wi replacing W ∗
i
and uµ replacing U ∗

µ. We introduce the interpolating posterior:

(cid:112)

Pt,(cid:15)(w, u|Yt, Y (cid:48)

t , X, V ) =

1
Zn,(cid:15)(t)

n

i=1
(cid:89)

P0(wi)e− 1

2 (cid:107)Y (cid:48)

t,i−

R1(t)wi(cid:107)2
2

√

2 (cid:107)uµ(cid:107)2
2

e− 1
(2π)K/2 Pout(Yt,µ|st,µ)

(17)

m

µ=1
(cid:89)

where the normalization factor Zn,(cid:15)(t) equals the numerator integrated over all components of w and u. The
average free entropy at time t is by definition

fn,(cid:15)(t) ≡

E ln Zn,(cid:15)(t) =

E ln

Du

dP0(wi)

Pout(Yt,µ|st,µ)

1
n

1
n

(cid:90)

n

i=1
(cid:89)

m

µ=1
(cid:89)

√

e− 1

2 (cid:107)Y (cid:48)

t,i−

R1(t)wi(cid:107)2

2 ,

(18)

n

i=1
(cid:89)

where Du =

m
µ=1

K

l=1(2π)−1/2e−u2

µl/2.

interpolating model:

(cid:81)

(cid:81)

The presence of the small “pertubation” (cid:15) induces a proportional change in the free entropy of the

Lemma 5.1 (Perturbation of the free entropy). For all (cid:15) ∈ (snM)2 we have for t = 0 that |fn,(cid:15)(0) −
fn,(cid:15)=(0,0)(0)| ≤ C(cid:48)sn for some positive constant C(cid:48). Moreover, |fn − fn,(cid:15)=(0,0)(0)| ≤ Csn for some positive
constant C, so that

|fn − fn,(cid:15)=(0,0)(0)| = On(1) .

∇(cid:15)1fn,(cid:15)(0) = −

[ρ − E(cid:104)Q(cid:105)n,0,(cid:15)] ,

1
2

Proof. Let us compute (or directly obtain by the I-MMSE formula for vector channels [42, 43, 44])

(19)

(20)

where the K × K overlap matrix (Qll(cid:48)) is defined below by (23). Note that the r.h.s. of the above equation is
(up to a factor −1/2) the K × K MMSE matrix. Set uy(x) ≡ ln Pout(y|x). Now we compute (by calculations
very similar to the ones used in the proof of the following Proposition 5.2):

∇(cid:15)2fn,(cid:15)(0) =

1
2n

m

E

µ=1
(cid:88)

∇uYt,µ(St,µ)
(cid:104)

(cid:68)

∇uYt,µ(st,µ)

.

(cid:105)

n,0,(cid:15)

(cid:69)

Note that the r.h.s. of the above equation is symmetric by the Nishimori identity Proposition A.1. By
the mean value theorem we obtain then directly that |fn,(cid:15)(0) − fn,(cid:15)=(0,0)(0)| ≤ (cid:107)∇(cid:15)1fn,(cid:15)(0)(cid:107)F(cid:107)(cid:15)1(cid:107)F +
(cid:107)∇(cid:15)2fn,(cid:15)(0)(cid:107)F(cid:107)(cid:15)2(cid:107)F ≤ C maxi (cid:107)(cid:15)i(cid:107) ≤ C(cid:48)sn.

Using this lemma one verifies, using in particular continuity and boundedness properties of ψP0 and ΨPout

(see Lemma A.6 in sec. A for details; sec. A gathers the detailed proofs of all the propositions below):

fn,(cid:15)(0) = fn − K
2 + On(1) ,
1
0 r(t)dt) + αΨPout(
fn,(cid:15)(1) = ψP0(

(cid:40)

1

0 q(t)dt; ρ) − 1

2

1

0 Tr[ρ r(t)]dt − K

2 + On(1) .

(21)

(cid:82)

(cid:82)

13

(cid:82)

Here On(1) → 0 in the n, m → ∞ limit uniformly in t, q, r, (cid:15).

5.2 Overlap concentration and fundamental sum rule

Notice from (21) that at t = 1 the interpolating estimation problem constructs part of the RS potential (9),
while at t = 0 it is the free entropy (6) of the original model (7) (up to a constant). We thus now want to
compare these boundary values thanks to the identity

fn = fn,(cid:15)(0) +

+ On(1) = fn,(cid:15)(1) −

dt +

+ On(1) .

(22)

K
2

1

dfn,(cid:15)(t)
dt

0
(cid:90)

K
2

The next obvious step is therefore to compute the free entropy variation along the interpolation path, see

sec. A.3 for the proof:

Proposition 5.2 (Free entropy variation). Denote by (cid:104)−(cid:105)n,t,(cid:15) the (Gibbs) expectation w.r.t. the posterior Pt,(cid:15)
given by (17). Set uy(x) ≡ ln Pout(y|x). For all t ∈ [0, 1] we have

dfn,(cid:15)(t)
dt

= −

E

Tr

1
2

m

1
n

(cid:68)

(cid:104)(cid:16)

µ=1
(cid:88)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

Q − q(t)

+

Tr [r(t)(q(t) − ρ)] + On(1) ,

(cid:17)(cid:0)

n,t,(cid:15)

(cid:1)(cid:105)(cid:69)

1
2

where ∇ is the K-dimensional gradient w.r.t. the argument of uYt,µ(·), and On(1) → 0 in the n, m → ∞ limit
uniformly in t, q, r, (cid:15). Here, the K ×K overlap matrix Q is defined as

Qll(cid:48) ≡

W ∗

ilwil(cid:48) .

1
n

n

i=1
(cid:88)

(23)

We will plug this expression in identity (22), but in order to simplify it we need the following crucial
proposition, which says that the overlap concentrates. This property is what is generally refered to as a replica
symmetric behavior in statistical physics.

Proposition 5.3 (Overlap concentration). Assume that for any t ∈ (0, 1) the transformation (cid:15) ∈ (snM)2 (cid:55)→
(R1(t, (cid:15)), R2(t, (cid:15))) is a C1 diffeomorphism with a Jacobian determinant greater or equal to 1. Then one can find a
sequence sn going to 0 slowly enough such that there exists a constant C(ϕout, S, K, α) > 0 depending only on
the activation ϕout, the support S of the prior P0, the number of hidden neurons K and the sampling rate α, and
a constant γ > 0 such that ((cid:107) − (cid:107)F is the Frobenius norm):

1
Vol(snM)2

1

d(cid:15)

dt E

Q − E(cid:104)Q(cid:105)n,t,(cid:15)

(cid:90)(snM)2

0
(cid:90)

(cid:10)(cid:13)
(cid:13)

C(ϕout, S, K, α)
nγ

.

2
F

n,t,(cid:15) ≤
(cid:11)

(cid:13)
(cid:13)

The proof of this concentration result can be directly adapted from [45]. Using the results of [45] is
straightforward, under the assumption that (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism with a Jacobian determinant
greater or equal to 1. This Jacobian determinant can be computed from formula (30). To check that it is greater
than one we use Lemma 5.5 and need Assumption 1 stated in paragraph 5.3 below. With a Jacobian determinant
greater than one, we can “replace” (i.e., lower bound) the integrations over R1(t, (cid:15)), that naturally appear in
the proof of Proposition 5.3, by integrations over the perturbation matrix (cid:15). This is exactly what has been done
in the K = 1 version of the present model in [11] or in [46] i.e., in the scalar overlap case (see also [47] for a
setting with a matrix overlap as in the present case).

From there we can deduce the following fundamental sum rule which is at the core of the proof:

Proposition 5.4 (Fundamental sum rule). Assume that the interpolation functions r and q are such that the map
(cid:15) = ((cid:15)1, (cid:15)2) (cid:55)→ R(t, (cid:15)) = (R1(t, (cid:15)), R2(t, (cid:15))) given by (14) is a C1 diffeomorphism whose Jacobian determinant

14

Jn,(cid:15)(t) is greater or equal to 1. Assume that for all t ∈ [0, 1] and (cid:15) ∈ (snM)2 we have q(t) = q(t, (cid:15)) =
E(cid:104)Q(cid:105)n,t,(cid:15) ∈ S +

K(ρ). Then

fn =

1
Vol(snM)2

1

d(cid:15)

ψP0

r(t)dt

+ αΨPout

q(t, (cid:15))dt; ρ

(cid:90)(snM)2

(cid:110)

0
(cid:16) (cid:90)

(cid:17)

1

0
(cid:16) (cid:90)

−

1
2

0
(cid:90)

1

(cid:17)

Tr[q(t, (cid:15))r(t)]dt

+ On(1) .

(24)

(cid:111)

Proof. Let us denote Vn ≡ Vol(snM)2. The integral over (cid:15) is always over (snM)2. Consider the first term, i.e.
the Gibbs bracket, in the free entropy derivative given by Proposition 5.2. By the Cauchy-Schwarz inequality

E

Tr

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

Q − q(t)

(cid:16)

(cid:68)

d(cid:15)

1

(cid:104)(cid:16)
dt E

0
(cid:90)

≤

1
Vn (cid:90)

m

1
n

µ=1
(cid:88)
1
n

(cid:68)(cid:13)
(cid:13)
(cid:13)

m

µ=1
(cid:88)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:1)(cid:105)(cid:69)
×

n,t,(cid:15)

(cid:17)

1
Vn (cid:90)

1

0
(cid:90)

(cid:17)(cid:0)
(cid:124) − r(t)

2

F

n,t,(cid:15)

(cid:69)

(cid:13)
(cid:13)
(cid:13)

d(cid:15)

dt E

Q − q(t)

2
F

n,t,(cid:15) .

(cid:10)(cid:13)
(cid:13)

(cid:11)

(cid:13)
(cid:13)

The first term of this product is bounded by some constant C(ϕout, α) that only depend on ϕout and α, see
Lemma A.4 in sec. A.4. The second term is bounded by C(ϕout, S, K, α)n−γ by Proposition 5.3, since we
assumed that for all (cid:15) ∈ Bn and all t ∈ [0, 1] we have q(t) = q(t, (cid:15)) = E(cid:104)Q(cid:105)n,t,(cid:15). Therefore from Proposition
5.2 we obtain

1
Vn (cid:90)

d(cid:15)

0
(cid:90)

1

dfn,(cid:15)(t)
dt

dt =

1
2Vn (cid:90)

1

0
(cid:90)

d(cid:15)

Tr

q(t, (cid:15))r(t) − r(t)ρ

dt + On(1) + O(n−γ/2) .

(25)

(cid:2)
Here the small terms are both going to 0 uniformly w.r.t. to the choice of q and r. When replacing (25) in (22)
and combining it with (21) we reach the claimed identity.

(cid:3)

5.3 A technical lemma and an assumption

We give here a technical lemma used in the rest of the proof, and which allows us to detail the unproven
assumption on which we rely to prove Thm 3.1.

Lemma 5.5. The quantity E(cid:104)Q(cid:105)n,t,(cid:15) is a function of (n, t, R(t, (cid:15))). We define F
(2)
(2)
n ) is defined on the set:
n (t, R(t, (cid:15))) ≡ 2α∇ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)). Fn ≡ (F
F

(1)
n , F

(1)
n (t, R(t, (cid:15))) ≡ E(cid:104)Q(cid:105)n,t,(cid:15) and

Dn =

(t, r1, r2) ∈ [0, 1] × S +

K × S +

K

(cid:110)

(ρt − r2 + 2snIK) ∈ S +
K
(cid:12)
(cid:12)
(cid:12)

.

(cid:111)

Fn is a continuous function from Dn to S +
K × S +
and R2 on the interior of Dn. For every (t, R(t, (cid:15))) for which they are defined, they satisfy:

K(ρ). Moreover, Fn admits partial derivatives with respect to R1

(26)

(27)

We can now state the technical assumption on which we rely, and which essentially allows us to derive
that the map (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism with a Jacobian determinant greater or equal to 1 as it will
become clear in the next section:

(1)
∂(F
n )ll(cid:48)
∂(R1)ll(cid:48)

≥ 0.

K

l≤l(cid:48)
(cid:88)

15

Assumption 1. With the notations of Lemma 5.5,

(2)
∂(F
n )ll(cid:48)
∂(R2)ll(cid:48)

≥ 0.

K

l≤l(cid:48)
(cid:88)

Proof of Lemma 5.5. The fact that the image domain of Fn is S +
K(ρ) is known from Lemma A.2. The
continuity and differentiability of Fn follows from standard theorems of continuity and derivation under the
integral sign (recall that we are working at finite n). Indeed, the domination hypotheses are easily satisfied
since we work under (H1) and (H2).

K × S +

i≤j A(ij)(ij). Then one can write Tr[DR1F

Let us now prove (27). We write the formal differential of F

(1)
(1)
n with respect to R1 as DR1F
n , which
(1)
n ] ≥ 0, the trace of a 4-tensor over SK A(ij)(kl) being
is a 4-tensor, and our goal is to prove that Tr[DR1F
(1)
n ] = Tr[∇∇(cid:124)ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) × ∇R1
E(cid:104)Q(cid:105)n,t,(cid:15)]. We
Tr[A] =
know from Lemma A.2 and Lemma A.6 that ∇∇(cid:124)ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) is a positive symmetric matrix (when seen
E(cid:104)Q(cid:105)n,t,(cid:15) is also positive
as a linear operator over SK). Moreover, it is a known result that the derivative ∇R1
symmetric, since R1 is the matrix snr of a linear channel (see [42, 43, 44]). Since the product of two symmetric
positive matrices has always positive trace, this shows that Tr[DR1F

(1)
n ] ≥ 0.

(cid:80)

5.4 Matching bounds

Proposition 5.6 (Lower bound). Under Assumption 1, the free entropy of model (7) verifies

lim inf
n→∞

fn ≥ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

Proof. Choose first r(t) = r ∈ S +
the first order differential equation:

K a fixed matrix. Then R(t) = (R1(t), R2(t)) can be fixed as the solution to

d
dt

d
dt

R1(t) = r ,

R2(t) = E(cid:104)Q(cid:105)n,t,(cid:15) ,

and

R(0) = (cid:15) .

(28)

t
We denote this (unique) solution R(t, (cid:15)) = (rt + (cid:15)1,
0 q(v, (cid:15); r)dv + (cid:15)2). It is possible to check that this ODE
satisfies the hypotheses of the parametric Cauchy-Lipschitz theorem, and that by the Liouville formula the
determinant Jn,(cid:15)(t) of the Jacobian of (cid:15) (cid:55)→ R(t, (cid:15)) satisfies (see Lemma A.3 in sec. A)

(cid:82)

Jn,(cid:15)(t) = exp

(s, R(s, (cid:15))) ds

≥ 1 .

(29)

K

t

∂E(cid:104)Qll(cid:48)(cid:105)n,s,(cid:15)
∂(R2)ll(cid:48)

0
(cid:16) (cid:90)

l≥l(cid:48)
(cid:88)

Indeed, this sum of partial derivatives is always positive by Assumption 1. Moreover from (28), q(t, (cid:15); r) =
E(cid:104)Q(cid:105)n,t,(cid:15), which is in S +
K by Lemma A.2 in sec. A. The fact that the map (cid:15) (cid:55)→ R(t, (cid:15)) is a C1 diffeomorphism is
easily verified by its bijectivity (from the positivity of Jn,(cid:15)(t)) combined with the local inversion Theorem. All
the assumptions of Proposition 5.4 are veri.i.d. which then implies, recalling the potential expression (9),

fn =

1
Vol(snM)2

(cid:90)(snM)2

1

0
(cid:16) (cid:90)

d(cid:15) fRS

q(v, (cid:15); r)dv, r

+ On(1) .

This implies the lower bound as this equality is true for any r ∈ S +
K.

(cid:17)

(cid:17)

16

Proposition 5.7 (Upper bound). Under Assumption 1, the free entropy of model (7) verifies

lim sup
n→∞

fn ≤ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) .

Proof. We now fix R(t) = (R1(t), R2(t)) as the solution R(t, (cid:15)) = (
the following Cauchy problem:

t
0 r(v, (cid:15))dv + (cid:15)1,

t
0 q(v, (cid:15))dv + (cid:15)2) to

(cid:82)

(cid:82)

d
dt

R1(t) = 2α∇ΨPout(E(cid:104)Q(cid:105)n,t,(cid:15)) ,

R2(t) = E(cid:104)Q(cid:105)n,t,(cid:15) ,

and

R(0) = (cid:15) .

d
dt

We denote this equation as ∂tR(t) = Fn(t, R(t)), R(0) = (cid:15). It is then possible to verify that Fn(R(t), t)
is a bounded C1 function of R(t), and thus a direct application of the Cauchy-Lipschitz theorem implies
that R(t, (cid:15)) is a C1 function of t and (cid:15). The Liouville formula for the Jacobian determinant of the map
(cid:15) ∈ (snM)2 (cid:55)→ R(t, (cid:15)) ∈ R(t, (snM)2) gives this time (see Lemma A.3 in sec. A)

Jn,(cid:15)(t) = exp

(s, R(s, (cid:15))) +

(s, R(s, (cid:15)))

ds

≥ 1 .

(30)

K

t

∂(Fn,1)ll(cid:48)
∂(R1)ll(cid:48)

0
(cid:16) (cid:90)

l≥l(cid:48) (cid:110)
(cid:88)

∂(Fn,2)ll(cid:48)
∂(R2)ll(cid:48)

(cid:111)

(cid:17)

The fact that this determinant is greater or equal to 1 for all t ∈ [0, 1] follows again from the positivity of this
sum of partials, see Lemma 5.5 and Assumption 1. Identity (30) implies the bijectivity of (cid:15) (cid:55)→ R(t, (cid:15)) which,
combined with the local inversion theorem, makes it a diffeomorphism. Since E(cid:104)Q(cid:105)n,t,(cid:15) and ρ − E(cid:104)Q(cid:105)n,t,(cid:15) are
positive matrices (see Lemma A.2 in sec. A) we also have that q(t, (cid:15)) ∈ S +
K(ρ) and since by the differential
equation we have r(t, (cid:15)) = 2α∇ΨPout(q(t, (cid:15))) and as ∇ΨPout(q) ∈ S +
K (see Lemma A.6 in sec. A), then
r(t, (cid:15)) ∈ S +

K too. We have everything needed for applying Proposition 5.4 again which gives in this case

fn =

1
Vol(snM)2

1

d(cid:15)

ψP0
(cid:110)

0
(cid:16) (cid:90)

(cid:90)

r(v, (cid:15))dv

+αΨPout

q(v, (cid:15))dv; ρ

−

Tr

q(v, (cid:15))r(v, (cid:15))dv

+On(1).

(cid:17)

(cid:17)

(cid:111)

1

0
(cid:16) (cid:90)

Then by convexity of ψP0 and ΨPout (see Lemma A.6),

fn ≤

=

1
Vol(snM)2
1
Vol(snM)2

d(cid:15)

d(cid:15)

1

0
(cid:90)

1

0
(cid:90)

(cid:90)

(cid:90)

We now remark that

dv

ψP0(r(v, (cid:15))dv) + αΨPout(q(v, (cid:15)); ρ) −
(cid:110)

dv fRS(q(v, (cid:15)), r(v, (cid:15))) + On(1) .

Tr[q(v, (cid:15))r(v, (cid:15))]

+ On(1)

(cid:111)

fRS(q(v, (cid:15)), r(v, (cid:15))) = inf
q∈S+

K (ρ)

fRS(q, r(v, (cid:15))) .

Indeed, for every r ∈ S +
K(ρ) (cid:55)→ fRS(q, r) ∈ R (recall (9)) is convex (by Lemma A.6),
and its q-derivative is ∇gr(q) = α∇ΨPout(q) − r/2. Since ∇gr(v,(cid:15))(q(v, (cid:15))) = 0 by definition of r(v, (cid:15)), and
S +
K(ρ) is convex, the minimum of gr(v,(cid:15))(q) is necessarily achieved at q = q(v, (cid:15)). Therefore

K, the function gr : q ∈ S +

fn ≤

1
Vol(snM)2

1

d(cid:15)

(cid:90)(snM)2

0
(cid:90)

dv inf
q∈S+

K (ρ)

fRS (q, r(v, (cid:15))) + On(1) ≤ sup
r∈S+
K

inf
q∈S+

K (ρ)

fRS(q, r) + On(1),

which concludes the proof of Proposition 5.7.

Combining these two matching bounds ends the proof of Theorem 3.1.

1

0
(cid:90)

1
2

1
2

17

6 Discussion

One of the contributions of this paper is the design of an AMP-type algorithm that is able to achieve the
Bayes-optimal learning error in the limit of large dimensions for a range of parameters out of the so-called
hard phase. The hard phase is associated with first order phase transitions appearing in the solution of the
model. In the case of the committee machine with a large number of hidden neurons we identify a large hard
phase in which learning is possible information-theoretically but not efficiently. In other problems where
such a hard phase was identified, its study boosted the development of algorithms that are able to match the
predicted threshold. We anticipate this will also be the same for the present model. We should, however, note
that for larger K > 2 the present AMP algorithm includes higher-dimensional integrals that hamper the speed
of the algorithm. Our current strategy to tackle this is to combine the large-K expansion and use it in the
algorithm. Detailed account of the corresponding results are left for future work.

We studied the Bayes-optimal setting where the student-network is the same as the teacher-network, for
which the replica method can be readily applied. The method still applies when the number of hidden units in
the student and teacher are different, while our proof does not generalize easily to this case. It is an interesting
subject for future work to see how the hard phase evolves under over-parametrization and what is the interplay
between the simplicity of the loss-landscape and the achievable generalization error. We conjecture that in
the present model over-parametrization will not improve the generalization error achieved by AMP in the
Bayes-optimal case.

Even though we focused in this paper on a two-layers neural network, the analysis and algorithm can be
readily extended to a multi-layer setting, see [22], as long as the number of layers as well as the number of
hidden neurons in each layer is held constant, and as long as one learns only weights of the first layer, for which
the proof already applies. The numerical evaluation of the phase diagram would be more challenging than
the cases presented in this paper as multiple integrals would appear in the corresponding formulas. In future
works, we also plan to analyze the case where the weights of the second and subsequent layers (including the
biases of the activation functions) are also learned. This could be done for instance with a combination of EM
and AMP along the lines of [48, 49] where this is done for the simpler single layer case.

Concerning extensions of the present work, an important open case is the one where the number of
samples per dimension α = Θ(1) and also the size of the hidden layer per dimension K/n = Θ(1) as n → ∞,
while in this paper we treated the case K = Θ(1) and n → ∞. This other scaling where K/n = Θ(1) is
challenging even for the non-rigorous replica method.

Acknowledgments

This work has been supported by the ERC under the European Union’s FP7 Grant Agreement 307087-SPARCS
and the European Union’s Horizon 2020 Research and Innovation Program 714608-SMiLe, as well as by the
French Agence Nationale de la Recherche under grant ANR-17-CE23-0023-01 PAIL and the Swiss National
Foundation grant no 200021E-175541. Additional funding is acknowledged by A.M., F.K. and J.B. from “Chaire
de recherche sur les modèles et sciences des données”, Fondation CFM pour la Recherche-ENS. We also
acknowledge Léo Miolane for discussions.

References

[1] V. Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.

18

[2] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

[3] S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Physical

[4] T. L. Watkin, A. Rau, and M. Biehl. The statistical mechanics of learning a rule. Reviews of Modern Physics,

Review A, 45(8):6056, 1992.

65(2):499, 1993.

[5] R. Monasson and R. Zecchina. Learning and generalization theories of large committee-machines. Modern

Physics Letters B, 9(30):1887–1897, 1995.

[6] R. Monasson and R. Zecchina. Weight space structure and internal representations: a direct approach to
learning and generalization in multilayer neural networks. Physical review letters, 75(12):2432, 1995.

[7] A. Engel and C. P. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press, 2001.

[8] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking

generalization. arXiv preprint arXiv:1611.03530, 2016. in ICLR 2017.

[9] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. Chayes, L. Sagun, and
R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838,
2016. in ICLR 2017.

[10] C. H. Martin and M. W. Mahoney. Rethinking generalization requires revisiting old ideas: statistical

mechanics approaches and complex learning behavior. arXiv preprint arXiv:1710.09553, 2017.

[11] J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborová. Optimal errors and phase transitions in
high-dimensional generalized linear models. Proceedings of the National Academy of Sciences, 116(12):5451–
5460, 2019.

[12] M. Baity-Jesi, L. Sagun, M. Geiger, S. Spigler, G. Ben-Arous, C. Cammarota, Y. LeCun, M. Wyart, and G. Bir-
oli. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint arXiv:1803.06969,
2018.

[13] M. Mézard, G. Parisi, and M. Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method

and Its Applications, volume 9. World Scientific Publishing Company, 1987.

[14] M. Mézard and A. Montanari. Information, physics, and computation. Oxford University Press, 2009.

[15] D. L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing.

Proceedings of the National Academy of Sciences, 106(45):18914–18919, 2009.

[16] S. Rangan. Generalized approximate message passing for estimation with random linear mixing. In
Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pages 2168–2172. IEEE,
2011.

[17] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to

compressed sensing. IEEE Transactions on Information Theory, 57(2):764–785, 2011.

[18] A. Javanmard and A. Montanari. State evolution for general approximate message passing algorithms,
with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144, 2013.

19

[19] H. Schwarze. Learning a rule in a multilayer neural network. Journal of Physics A: Mathematical and

General, 26(21):5781, 1993.

20(4):375, 1992.

Letters), 21(7):785, 1993.

[20] H. Schwarze and J. Hertz. Generalization in a large committee machine. EPL (Europhysics Letters),

[21] H. Schwarze and J. Hertz. Generalization in fully connected committee machines. EPL (Europhysics

[22] G. Mato and N. Parga. Generalization properties of multilayered neural networks. Journal of Physics A:

Mathematical and General, 25(19):5047, 1992.

[23] D. Saad and S. A. Solla. On-line learning in soft committee machines. Physical Review E, 52(4):4225, 1995.

[24] J. Barbier and N. Macris. The adaptive interpolation method: a simple scheme to prove replica formulas

in bayesian inference. Probability Theory and Related Fields, pages 1–53, 2018.

[25] D. L. Donoho, I. Johnstone, and A. Montanari. Accurate prediction of phase transitions in compressed
sensing via a connection to minimax denoising. IEEE transactions on information theory, 59(6):3396–3433,
2013.

[26] L. Zdeborová and F. Krzakala. Statistical physics of inference: thresholds and algorithms. Advances in

Physics, 65(5):453–552, 2016.

[27] Y. Deshpande and A. Montanari. Finding hidden cliques of size \sqrt {N/e} n/e in nearly linear time.

Foundations of Computational Mathematics, 15(4):1069–1128, 2015.

[28] A. S. Bandeira, A. Perry, and A. S. Wein. Notes on computational-to-statistical gaps: predictions using

statistical physics. arXiv preprint arXiv:1803.11132, 2018.

[29] I. Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv

preprint arXiv:1712.08968, 2017.

[30] A. E. Alaoui, A. Ramdas, F. Krzakala, L. Zdeborová, and M. I. Jordan. Decoding from pooled data: Sharp

information-theoretic bounds. arXiv preprint arXiv:1611.09981, 2016.

[31] A. El Alaoui, A. Ramdas, F. Krzakala, L. Zdeborová, and M. I. Jordan. Decoding from pooled data: Phase
transitions of message passing. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages
2780–2784. IEEE, 2017.

[32] J. Zhu, D. Baron, and F. Krzakala. Performance limits for noisy multimeasurement vector problems. IEEE

Transactions on Signal Processing, 65(9):2444–2454, 2017.

[33] F. Guerra. Broken replica symmetry bounds in the mean field spin glass model. Communications in

mathematical physics, 233(1):1–12, 2003.

[34] M. Talagrand. Spin glasses: a challenge for mathematicians: cavity and mean field models, volume 46.

Springer Science & Business Media, 2003.

[35] D. J. Thouless, P. W. Anderson, and R. G. Palmer. Solution of’solvable model of a spin glass’. Philosophical

Magazine, 35(3):593–601, 1977.

[36] M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method.

Journal of Physics A: Mathematical and General, 22(12):2181–2190, 1989.

20

[37] M. Opper and O. Winther. Mean field approach to bayes learning in feed-forward neural networks.

Physical review letters, 76(11):1964, 1996.

[38] Y. Kabashima. Inference from correlated patterns: a unified theory for perceptron learning and linear

vector channels. Journal of Physics: Conference Series, 95(1):012001, 2008.

[39] C. Baldassi, A. Braunstein, N. Brunel, and R. Zecchina. Efficient supervised learning in networks with

binary synapses. Proceedings of the National Academy of Sciences, 104(26):11079–11084, 2007.

[40] B. Aubin, A. Maillard, J. Barbier, F. Krzakala, N. Macris, and L. Zdeborová. AMP implementation of the

committee machine. https://github.com/benjaminaubin/TheCommitteeMachine, 2018.

[41] P. Schniter, S. Rangan, and A. K. Fletcher. Vector approximate message passing for the generalized linear
model. In Signals, Systems and Computers, 2016 50th Asilomar Conference on, pages 1525–1529. IEEE, 2016.

[42] G. Reeves, H. D. Pfister, and A. Dytso. Mutual information as a function of matrix snr for linear gaussian
channels. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 1754–1758. IEEE,
2018.

[43] M. Payaró, M. Gregori, and D. Palomar. Yet another entropy power inequality with an application. In
Wireless Communications and Signal Processing (WCSP), 2011 International Conference on, pages 1–5. IEEE,
2011.

[44] M. Lamarca. Linear precoding for mutual information maximization in mimo systems.

In Wireless

Communication Systems, 2009. ISWCS 2009. 6th International Symposium on, pages 26–30. IEEE, 2009.

[45] J. Barbier. Overlap matrix concentration in optimal bayesian inference. arXiv preprint arXiv:1904.02808,

2019.

[46] J. Barbier and N. Macris. The adaptive interpolation method for proving replica formulas. applications to
the curie-weiss and wigner spike models. Journal of Physics A: Mathematical and Theoretical, 2019.

[47] J. Barbier, C. Luneau, and N. Macris. Mutual information for low-rank even-order symmetric tensor

factorization. arXiv preprint arXiv:1904.04565, 2019.

[48] F. Krzakala, M. Mézard, F. Sausset, Y. Sun, and L. Zdeborová. Probabilistic reconstruction in compressed
sensing: algorithms, phase diagrams, and threshold achieving matrices. Journal of Statistical Mechanics:
Theory and Experiment, 2012(08):P08009, 2012.

[49] U. Kamilov, S. Rangan, M. Unser, and A. K. Fletcher. Approximate message passing with consistent
parameter estimation and applications to sparse learning. In Advances in Neural Information Processing
Systems, pages 2438–2446, 2012.

[50] P. Hartman. Ordinary Differential Equations: Second Edition. Classics in Applied Mathematics. Society for
Industrial and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104), 1982.

[51] E. Gardner and B. Derrida. Optimal storage properties of neural network models. Journal of Physics A:

Mathematical and general, 21(1):271, 1988.

[52] J. Barbier, N. Macris, M. Dia, and F. Krzakala. Mutual information and optimality of approximate

message-passing in random linear estimation. arXiv preprint arXiv:1701.05823, 2017.

[53] M. Opper and W. Kinzel. Statistical mechanics of generalization. In Models of neural networks III, pages

151–209. Springer, 1996.

21

[54] J. Barbier and F. Krzakala. Approximate message-passing decoder and capacity achieving sparse super-

position codes. IEEE Transactions on Information Theory, 63:4894–4927, 2017.

[55] M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

[56] M. Bayati, M. Lelarge, A. Montanari, et al. Universality in polytope phase transitions and message passing

algorithms. The Annals of Applied Probability, 25(2):753–822, 2015.

22

Supplementary material

A Proof details for Theorem 3.1

A.1 The Nishimori property in Bayes-optimal learning

We first state an important property of the Bayesian optimal setting (that is when all hyper-parameters of the
problem are assumed to be known), that is used several times, and is often refered to as the Nishimori identity.

Proposition A.1 (Nishimori identity). Let (X, Y ) ∈ Rn1 × Rn2 be a couple of random variables. Let k ≥ 1 and
let X (1), . . . , X (k) be k i.i.d. samples (given Y ) from the conditional distribution P (X = · |Y ), independently of
every other random variables. Let us denote (cid:104)−(cid:105) the expectation operator w.r.t. P (X = · |Y ) and E the expectation
w.r.t. (X, Y ). Then, for all continuous bounded function g we have

E(cid:104)g(Y, X (1), . . . , X (k))(cid:105) = E(cid:104)g(Y, X (1), . . . , X (k−1), X)(cid:105) .

(31)

Proof. This is a simple consequence of Bayes formula. It is equivalent to sample the couple (X, Y ) according
to its joint distribution or to sample first Y according to its marginal distribution and then to sample X
conditionally to Y from its conditional distribution P (X = · |Y ). Thus the (k + 1)-tuple (Y, X (1), . . . , X (k))
is equal in law to (Y, X (1), . . . , X (k−1), X). This proves the proposition.

As a first application of Proposition A.1 we prove the following Lemma which is used in the proof of the

upper bound Proposition 5.7.
Lemma A.2 (Positivity of some matrices). The matrices ρ, E(cid:104)Q(cid:105) and ρ − E(cid:104)Q(cid:105) are positive definite, i.e. in S +
K
In the application the Gibbs bracket is (cid:104)−(cid:105)n,t,(cid:15).
Proof. The statement for ρ follows from its definition (in Theorem 3.1). Note for further use that we also
have ρ = 1
ilwil(cid:48) in matrix notation we have Q =
n
n
i=1 W ∗
1
i w
n

i )(cid:124)]. Since by definition Qll(cid:48) ≡ 1
n
E[W ∗
i=1 W ∗
(cid:124)
i . An application of the Nishimori identity shows that

i (W ∗

n

.

(cid:80)

which is obviously in S +

E(cid:104)Q(cid:105) =

n

1
n

i=1
(cid:88)
K. Finally we note that

n

1
n

i=1 (cid:16)
(cid:88)

(cid:80)

1
n

n

i=1
(cid:88)

=

1
n

(cid:124)
i (cid:105)]
(cid:17)

n

i=1
(cid:88)

E[ρ − (cid:104)Q(cid:105)] =

E[W ∗

i (W ∗
i )

] − E[(cid:104)wi(cid:105)(cid:104)w

(cid:124)

E[(W ∗

i − (cid:104)wi(cid:105))((W ∗
i )

(cid:124) − (cid:104)w

(cid:124)
i (cid:105))]

E(cid:104)W ∗

i w

(cid:124)
i (cid:105) =

E[(cid:104)wi(cid:105)(cid:104)w

(cid:124)
i (cid:105)]

(32)

where the last equality is proved by an application of the Nishimori identity again. This last expression is
obviously in S +

K, i.e. E(cid:104)Q(cid:105) ∈ S +

K(ρ).

A.2 Setting in the Hamiltonian language

We set up some notations which will shortly be useful. Let uy(x) ≡ ln Pout(y|x). Here x ∈ RK and y ∈ R.
We will denote by ∇uy(x) the K-dimensional gradient w.r.t. x, and ∇∇(cid:124)uy(x) the K × K matrix of second
derivatives (the Hessian) w.r.t. x. Moreover ∇Pout(y|x) and ∇∇(cid:124)Pout(y|x) also denote the K-dimensional
gradient and Hessian w.r.t. x. We will also use the matrix identity

∇∇(cid:124)

uYµ(x) + ∇uYµ(x)∇(cid:124)

uYµ(x) =

∇∇(cid:124)Pout(Yµ|x)
Pout(Yµ|x)

.

(33)

23

t ∈ Rn×K, X ∈ Rm×n, V ∈ Rm×K,
Finally we will use the matrices w ∈ Rn×K, u ∈ Rm×K, Yt ∈ Rm, Y (cid:48)
W ∗ ∈ Rn×K and U ∗ ∈ Rm×K. Like in sec. 5 we adopt the convention that all underlined vectors are
K-dimensional, like e.g. uµ, U µ, V µ and Y (cid:48)

t,i.

It is convenient to reformulate the expression of the interpolating free entropy fn,(cid:15)(t) in the Hamiltonian

language. We introduce an interpolating Hamiltonian:

Ht(w, u; Yt, Y (cid:48)

t , X, V ) ≡ −

uYt,µ(st,µ) +

(cid:107)Y (cid:48)

t,i − R1(t)1/2 wi(cid:107)2
2

(34)

m

µ=1
(cid:88)

1
2

n

i=1
(cid:88)

where recall that

n

st,µ ≡

1 − t
n

(cid:114)

Xµiwi +

R2(t) V µ +

tρ − R2(t) + 2snIK×K uµ .

(35)

i=1
(cid:88)
The expression of Ht(W ∗, U ∗; Yt, Y (cid:48)
(35) replaced by St,µ given by (15). The average free entropy (18) at time t then reads

(cid:112)

(cid:112)

t , X, V ) is similar to (34), but with w replaced by W ∗ and st,µ given by

fn,(cid:15)(t) ≡

E ln

1
n

dP0(w)

Rn×K

(cid:90)

(cid:90)
µl/2 and dP0(w) =

Rm×K

Du e−Ht(w,u;Yt,Y (cid:48)

t ,X,V )

(36)

where Du =
in the simplest manner it is fruitful to represent the expectations over W ∗, U, Y, Y (cid:48) explicitly as integrals:

K
l=1 dwil. To develop the calculations

n
i=1 P0(wi)

l=1(2π)−1/2e−u2

m
µ=1

K

(cid:81)

(cid:81)

fn,(cid:15)(t) =

EX,V

dYtdY (cid:48)

t dP0(W ∗)DU ∗e−Ht(W ∗,U ;Yt,Y (cid:48)

t ,X,V ) ln

dP0(w)Du e−Ht(w,u;Yt,Y (cid:48)

t ,X,V ).

(37)

(cid:81)

(cid:81)

1
n

(cid:90)

(cid:90)

A.3 Free entropy variation: Proof of Proposition 5.2

The proof provided here follows very closely the one in [11] for the case K = 1, so we are more brief and
refer to this paper for more details. We first prove that for all t ∈ (0, 1)

dfn,(cid:15)(t)
dt

= −

Tr

E

1
2

(cid:68)

m

1
n

(cid:104)(cid:16)

+

1
2

µ=1
(cid:88)
Tr[r(t)(q(t) − ρ)] −

An
2

,

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

(cid:124) − r(t)

W ∗

i w

(cid:124)
i − q(t)

n

1
n

(cid:17)(cid:16)

i=1
(cid:88)

n,t,(cid:15)

(cid:17)(cid:69)

(38)

where

Tr

An = E
(cid:104)

(cid:104)

1
√
n

m

µ=1
(cid:88)

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

1
√
n

n

(cid:16)

i=1
(cid:88)

(W ∗

i (W ∗
i )

(cid:124) − ρ)

ln Zn,(cid:15)(t)

.

(39)

(cid:105)

Once this is done, we show that An goes to 0 as n → ∞ uniformly in t ∈ [0, 1] in order to conclude the proof.

The Hamiltonian (34) t-derivative evaluated at the ground-truth matrices is given by

dHt
dt

(W ∗, U ∗;Yt, Y (cid:48)

t , X, V ) = −

∇(cid:124)

uYt,µ(St,µ)

m

µ=1
(cid:88)
∇(cid:124)

m

= −

Tr

dSt,µ
dt

µ=1
(cid:88)

(cid:104)

uYt,µ(St,µ)
(cid:105)

n

dSt,µ
dt

−

dR1(t)1/2
dt

n

−

Tr

i=1
(cid:88)

(cid:104)(cid:16)

i=1 (cid:16)
(cid:88)
dR1(t)1/2
dt

(cid:124)

(cid:17)

W ∗
i

(Y (cid:48)

t,i − R1(t)1/2W ∗
i )

(Y (cid:48)

t,i − R1(t)1/2W ∗

i )W ∗(cid:124)

i

(40)

(cid:105)

1
n

(cid:17)(cid:105)

(cid:124)

(cid:17)

24

(where we used that R1(t) is symmetric). The t-derivative of fn,(cid:15)(t) thus reads, for 0 < t < 1,

(W ∗, U ∗; Yt, Y (cid:48)

t , X, V ) ln Zn,(cid:15)(t)

−

(w, u; Yt, Y (cid:48)

t , X, V )

(41)

T1

(cid:123)(cid:122)

E

1
n

dHt
dt

(cid:68)

(cid:105)

(cid:125)

(cid:124)

T2

(cid:123)(cid:122)

.

n,t,(cid:15)

(cid:69)

(cid:125)

First, we note that T2 = 0. This is a direct consequence of the Nishimori identity Proposition A.1:

(w, u; Yt, Y (cid:48)

t , X, V )

(W ∗, U ∗; Yt, Y (cid:48)

t , X, V ) = 0 .

(42)

=

1
n

E dHt
dt

n,t,(cid:15)

(cid:69)

We now compute T1. Starting from (40) and considering the first term only (recall also the expression (15)

dfn,(cid:15)(t)
dt

= −

E

1
n

dHt
dt

(cid:104)

(cid:124)

T2 =

E

1
n

dHt
dt

(cid:68)

for St,µ),

E

Tr

(cid:104)

dSt,µ
dt

∇(cid:124)

(cid:104)
+

d
dt

(cid:112)

uYt,µ(St,µ)

R2(t)V µ +

ln Zn,(cid:15)(t)

−

Tr

= E
2
(cid:80)
(cid:104)
tρ − R2(t) + 2snIK×K U ∗
(cid:112)
µ

(cid:104)(cid:110)

(cid:105)

(cid:105)
d
dt

(cid:112)

n
i=1 XµiW ∗
i
n(1 − t)
∇(cid:124)
(cid:111)

uYt,µ(St,µ)
(cid:105)

ln Zn,(cid:15)(t)

.

(43)

(cid:105)

We then compute the first line of the right-hand side of (43). By Gaussian integration by parts w.r.t. Xµi (recall
hypothesis (H3)), and using the identity (33), we find after some algebra

1

−

2

n(1 − t)

(cid:112)

= −

n

i=1
(cid:88)
1
n

(cid:104)

Tr

E

Tr

(cid:104)
E

(cid:104)

Tr

1
2

−

(cid:104)
1
E
2

n

i=1
(cid:88)
1
n

(cid:68)

(cid:104)

i=1
(cid:88)

XµiW ∗

i ∇(cid:124)

n

W ∗

i W

(cid:124)
i

ln Zn,(cid:15)(t)

uYt,µ(St,µ)
(cid:105)
∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

(cid:105)

W ∗

i w

(cid:124)

i ∇uYt,µ(St,µ)∇(cid:124)

uYt,µ(st,µ)

ln Zn,(cid:15)(t)

(cid:105)

(cid:105)

.

n,t,(cid:15)

(cid:105)(cid:69)

(44)

(45)

Similarly for the second line of the right hand side of (43), we use again Gaussian integrations by parts but
this time w.r.t. V µ, U ∗
µ which have i.i.d. N (0, 1) entries. This calculation has to be done carefully with the
help of the matrix identity

d
dt

M (t) =

M (t)

(cid:112)

d

M (t)
dt

d

+

M (t)
dt

(cid:112)

(cid:112)

M (t)

(cid:112)

for any M (t) ∈ S +
t
0 (ρ − q(s))ds, as well as the identity (33), we reach after some algebra

K, and the cyclicity and linearity of the trace. Applying (45) to M (t) equal to

t
0 q(s)ds and

(cid:82)

(cid:82)

E

Tr

(cid:104)(cid:16)
ρ

(cid:104)
= E
Tr
(cid:104)

(cid:104)

d
d
R2(t)V µ +
dt
dt
∇∇(cid:124)Pout(Yt,µ|Sµ,t)
(cid:112)
Pout(Yt,µ|Sµ,t)

(cid:112)

(cid:105)

tρ − R2(t) + 2snIK×K U ∗
µ

∇(cid:124)

ln Zn,(cid:15)(t)

uYµ(Sµ,t)
(cid:105)
uYt,µ(sµ,t)

(cid:105)

(cid:17)

q(t)∇uYt,µ(Sµ,t)∇(cid:124)

ln Zn,(cid:15)(t)

(cid:105)

Tr

+ E
(cid:104)
(cid:68)
R1(t))(cid:124)(Y (cid:48)

.

n,t,(cid:15)

(cid:105)(cid:69)

(46)

As seen from (40), (41) it remains to compute E[Tr[( d
dt
that Y (cid:48)
i = Z(cid:48)
t,i −
one obtains

R1(t)W ∗

(cid:112)

] ln Zn,(cid:15)(t)]. Recall
t,i −
i ∼ N (0, IK×K). Using Gaussian integration by parts as well as the identity (45)

i

(cid:112)

R1(t)W ∗

i )W ∗(cid:124)

(cid:112)
d
dt

(cid:124)

(cid:17)

E

Tr

R1(t)

(Y (cid:48)

t,i −

R1(t)W ∗

i )W ∗(cid:124)

i

ln Zn,(cid:15)(t)

= −Tr

R1(t)

ρ − E(cid:104)W ∗

j wj(cid:105)n,t,(cid:15)

.

(47)

(cid:104)

(cid:104)(cid:16)

(cid:112)

(cid:112)

(cid:105)

(cid:105)

(cid:104)(cid:112)

(cid:0)

(cid:1)(cid:105)

25

Finaly the term T1 is obtained by putting together (43), (44), (46) and (47).

It now remains to check that An → 0 as n → +∞ uniformly in t ∈ [0, 1]. The proof from [11] (Appendix
C.2) can easily be adapted so we give here just a few indications for the ease of the reader. First one notices that

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yµ|St,µ)

E

(cid:104)

(cid:12)
(cid:12)
(cid:12)

W ∗, {St,µ}m

µ=1

=

dYµ∇∇(cid:124)

Pout(Yt,µ|St,µ) = 0 ,

(cid:105)

(cid:90)

so that by the tower property of the conditional expectation one gets

E

Tr

1
√
n

m

µ=1
(cid:88)

∇∇(cid:124)Pout(Yt,µ|St,µ)
Pout(Yt,µ|St,µ)

1
√
n

n

i=1
(cid:88)

(W ∗

i (W ∗
i )

(cid:124) − ρ)

= 0 .

(cid:104)

(cid:104)

(cid:16)
Next, one shows by standard second moment methods that E[(ln Zn,(cid:15)(t)/n − fn,(cid:15)(t))2] → 0 as n → +∞
uniformly in t ∈ [0, 1] (see [11] for the proof at K = 1, that generalizes straightforwardly for any finite K).
Then, using this last fact together with (49), and under hypotheses (H1), (H2), (H3), an easy application of the
(cid:3)
Cauchy-Schwarz inequality implies An → 0 as n → +∞ uniformly in t ∈ [0, 1]. This ends the proof.

(cid:17)(cid:105)(cid:105)

(48)

(49)

A.4 Technical lemmas

Lemma A.3 (Cauchy-Lipschitz Theorem and Liouville Formula). Let

F :

[0, 1] × (0, +∞)d → [0, +∞)d
(cid:55)→ F (t, z)

(t, z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

be a continuous, bounded function. Assume that F admits continuous partial derivatives ∂F
∂zi
domain of definition. Then, for all (cid:15) ∈ (0, +∞)d, the Cauchy problem

(i = 1, . . . , d) on its

y(0) = (cid:15)

and

y(cid:48)(t) = F

t, y(t)

(50)

admits a unique solution t (cid:55)→ y(t, (cid:15)). For all t ∈ [0, 1], the mapping zt : (cid:15) (cid:55)→ y(t, (cid:15)) is a diffeomorphism of class
C1, from (0, +∞)d to zt

. Moreover the determinant J(zt)((cid:15)) of the Jacobian of zt at (cid:15) verifies

(0, +∞)d

(cid:0)

(cid:1)

(cid:0)

(cid:1)

J(zt)((cid:15)) = det

∂yi
∂(cid:15)j

d

t

∂Fi
∂zi

(cid:16)(cid:16)

d
i=1

∂Fi
∂zi

i,j

(cid:17)

0
(cid:16) (cid:90)
(cid:17)
≥ 0 then J(zt)((cid:15)) ≥ 1 for all (cid:15).

i=1
(cid:88)

(cid:0)

(cid:17)

(cid:1)

= exp

s, y(s, (cid:15))

ds

.

(51)

Thus, in particular, if in addition

Proof. The existence and uniqueness of the solution of (50) follows from the classical Cauchy-Lipschitz
Theorem. The solution is indeed defined on all the segment [0, 1] because F is bounded.

(cid:80)

Theorem 3.1 from Chapter 5 in [50] gives that y admits continuous partial derivatives ∂y
∂(cid:15)i

for i = 1, . . . , d,

and Corollary 3.1 from Chapter 5 in the same reference states the Liouville formula (51).

By the Cauchy-Lipschitz Theorem, two solutions of y(cid:48)(t) = F

that are equal at some t ∈ [0, 1]
are equal everywhere. This implies that the mapping zt : (cid:15) (cid:55)→ y(t, (cid:15)) is injective, for all t ∈ [0, 1]. Since y
admits continuous partial derivatives in (cid:15)i, i = 1, . . . , d, we obtain that zt is of class C1 on (0, +∞)d. Now,
the equation (51) gives that J(zt)((cid:15)) > 0 for all (cid:15) ∈ (0, +∞)d. The local inversion Theorem gives then that zt
is a C1 diffeomorphism.

t, y(t)

(cid:0)

(cid:1)

26

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

Lemma A.4 (Boundedness of an overlap fluctuation). Under hypothesis (H2) one can find a constant C(ϕ, K, ∆) <
+∞ (independent of n, t, (cid:15)) such that for any Rn ∈ S +
K

we have

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124) − Rn

≤ 2Tr(R2

n) + α2C(ϕ, K, ∆).

(52)

We note that the constant remains bounded as ∆ → 0 and diverges as K → +∞.

Proof. It is easy to see that for symmetric matrices A, B we have Tr(A − B)2 ≤ 2(TrA2 + TrB2). Therefore

F

n,t,(cid:15)

(cid:69)

(cid:13)
(cid:13)
(cid:13)

∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124) − Rn

F

n,t,(cid:15)

≤ 2Tr(R2

Tr

n) + 2E
(cid:68)

1
n

(cid:16)

µ=1
(cid:88)

m

(cid:69)

(cid:13)
(cid:13)
(cid:13)
(cid:124)
∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

.

n,t,(cid:15)

(cid:17)

(cid:69)

(53)

In the rest of the argument we bound the second term of the r.h.s. Using the triangle inequality and then
Cauchy-Schwarz we obtain

1
n

m

µ=1
(cid:88)

E

(cid:68)(cid:13)
(cid:13)
(cid:13)

(cid:124)
∇uYt,µ(st,µ)∇uYt,µ(St,µ)

F

n,t,(cid:15)

2

(cid:69)

(cid:13)
(cid:13)
(cid:13)

≤ E

1
n2

(cid:68)

m

µ=1
(cid:16)
(cid:88)
2
(cid:124)(cid:107)2

(cid:107)∇uYt,µ(st,µ)(cid:107)2(cid:107)∇uYt,µ(St,µ)

m

≤ E

1
n2

(cid:68)

(cid:16)

µ=1
(cid:88)

.

n,t,(cid:15)

(cid:17)

(cid:69)

From the random representation of the transition kernel,

(cid:107)∇uYt,µ(st,µ)∇uYt,µ(St,µ)

2

(cid:124)(cid:107)F

n,t,(cid:15)

(cid:17)

(cid:69)

and thus

uYt,µ(s) = ln Pout(Yt,µ|x) = ln

dPA(aµ)

√

1
2π∆

e− 1

2∆ (Yt,µ−ϕ(x,aµ))2

(cid:90)

∇uYt,µ(x) =

dPA(aµ)(Yt,µ − ϕ(x, aµ))∇ϕ(x, aµ)e− 1
dPA(aµ)e− 1
2∆ (Yt,µ−ϕ(x,aµ))2

(cid:82)

2∆ (Yt,µ−ϕ(x,aµ))2

where ∇ϕ is the K-dimensional gradient w.r.t. the first argument x ∈ RK. From the observation model we get
|Yt,µ| ≤ sup |ϕ| +
∆|Zµ|, where the supremum is taken over both arguments of ϕ, and thus we immediately
obtain for all s ∈ RK

√

(cid:82)

(cid:107)∇uYt,µ(x)(cid:107) ≤ (2 sup |ϕ| +

∆|Zµ|) sup (cid:107)∇ϕ(cid:107) .

√

From (57) and (54) we see that it suffices to check that

m2
n2

(cid:2)(cid:0)

E

(2 sup |ϕ| + |Zµ|)2(sup (cid:107)∇ϕ(cid:107))2

≤ C(ϕ, K, ∆)

2

(cid:1)

(cid:3)

where C(ϕ, K, ∆) < +∞ is a finite constant depending only on ϕ, K, and ∆. This is easily seen by expanding
all squares and using that m/n → α. This ends the proof of Lemma A.4.

Lemma A.5 (Properties of ψP0). ψP0

is defined as the free entropy of the first auxiliary channel (3). We have,

(54)

(55)

(56)

(57)

27

for any r ∈ S +
K

:

ψP0(r) ≡ E ln

dwP0(w)eY

(cid:124)
0 r1/2w− 1

2 w(cid:124)rw.

RK

(cid:90)

Then ψP0

is convex and differentiable on S +
K

, with ∇ψP0(r) ∈ S +
K

for any r ∈ S +
K

.

Proof. Note that ψP0 is related to the mutual information I(W0; Y0) via the relation I(W0; Y0) = −ψP0(r) +
K
2 + 1
2 Tr[rρ]. It is then a known result (see [42, 43, 44]) that the derivative ∇rI(W0; Y0) is given by the
matrix-MMSE, i.e. ∇rI(W0; Y0) = 1
]). Using
2
E [(w − (cid:104)w(cid:105))(w − (cid:104)w(cid:105))(cid:124)], which is clearly a
the Nishimori identity Prop.A.1, we can write it as ∇rψP0(r) = 1
2
positive matrix. It is also known (see for instance Lemma 4 of [42]), that I(W0; Y0) is a concave function of r,
which implies that ψP0 is convex, which ends the proof.

]. This implies that ∇rψP0(r) = 1

2 (ρ − E[(cid:104)w(cid:105) (cid:104)w(cid:105)

E [(cid:104)w(cid:105) (cid:104)w(cid:105)

(cid:124)

(cid:124)

Lemma A.6 (Properties of ΨPout). Recall that ΨPout
(4). More precisely, for q ∈ S +

K(ρ), we have:

is defined as the free entropy of the second auxiliary channel

ΨPout(q) ≡ E ln

Y0|q1/2V + (ρ − q)1/2w

.

dw

e− 1
2 (cid:107)w(cid:107)2
(2π)K/2 Pout

RK

(cid:90)

Then ΨPout

is continuous and convex on S +

K(ρ). Also, ∇ΨPout(q) ∈ S +
Proof. The continuity and differentiability of ΨPout is easy, and exactly similar to the first part of the proof of
Proposition 18 of [11]; it just follows from the hypothesis (H2) which allows to use continuity and differentiation
under the expectation, because all the domination hypotheses are easily verified.

K

.

(cid:0)
(cid:101)
K(ρ), and twice differentiable inside S +

(cid:1)

One can compute the gradient and Hessian matrix of ΨPout(q), for q inside S +

K(ρ), using Gaussian
integration by parts and the Nishimori identity. The calculation is tedious and essentially follows the steps of
Y0|x). We define the average (cid:104)−(cid:105)sc (where sc stands for
Proposition 11 of [11]. Recall that u
“scalar channel”) as

(x) ≡ ln Pout(

(cid:101)Y0

(cid:104)g(w)(cid:105)sc ≡

RK DwPout(

(cid:82)

RK DwPout(
(cid:101)

(cid:82)

(cid:101)
Y0|(ρ − q)1/2w + q1/2V )g(w)
Y0|(ρ − q)1/2w + q1/2V )

,

for any continuous bounded function g. One arrives at:
(cid:101)

∇ΨPout(q) =

(ρ − q)1/2W ∗ + q1/2V

(ρ − q)1/2w + q1/2V

E

1
2

∇u

(cid:101)Y0

(cid:68)

(cid:16)

(cid:124)

.

sc

(cid:17)

(cid:69)

Note that this gradient is actually a symmetric matrix of size K × K, as it is a gradient w.r.t. q, which is itself
a matrix of size K. The Hessian ∇∇(cid:124)ΨPout with respect to q is thus a 4-tensor. One can compute in the same
way:

∇∇(cid:124)

ΨPout(q) =

Y0|(ρ − q)1/2w + q1/2V )

Y0|(ρ − q)1/2w + q1/2V )

∇∇(cid:124)Pout(
Pout(

(cid:101)

E

1
2

(cid:104)(cid:16)(cid:68)
−

(cid:68)

(cid:16)

∇u

(cid:101)Y0

(ρ − q)1/2W ∗ + q1/2V
(cid:101)

∇u

(ρ − q)1/2w + q1/2V

(cid:124)

⊗2

.

sc

(cid:17)

(cid:69)

(cid:17)

(cid:105)

In this expression, ⊗2 means the “tensorized square” of a matrix, i.e. for any matrix M of size K × K, M ⊗2 is
a 4-tensor with indices M ⊗2
= Ml0l1Ml2l3. From this expression, it is clear that the Hessian of ΨPout is
always positive, when seen as a matrix with rows and columns in SK, and thus ΨPout is convex, which ends
the proof of Lemma A.6.

l0l1l2l3

∇u

(cid:101)Y0

(cid:17)

(cid:16)

sc

(cid:69)

(cid:101)Y0

(cid:16)

(cid:17)

(58)

(59)

(60)

28

B Replica calculation

Our goal here is to provide an heuristic derivation of the replica formula of Theorem 3.1 using the replica
method, a powerful non-rigorous tool from statistical physics of disordered systems [13, 14]. This computation
is necessary to properly “guess” the formula that we then prove using the adaptive interpolation method. The
reader interested in the replica approach to neural networks and the commitee machine is invited to look as
well to some of the classical papers [51, 36, 20, 21, 19, 5].

The replica trick makes use of the formula, for a random variable x ∈ Rn and a strictly positive function

fn : Rn → R that depends on n:

lim
n→∞

1
n

E ln fn = lim
p→0+

lim
n→∞

1
np

ln Ef p
n.

(61)

Note that the inversion of the two limits here is non-rigorous. Computing the moments Ef p can often
be done for integers p ∈ N, and one can conjecture from it its value for every p > 0, before taking the limit
p → 0+ in (61) by analytical continuation of the value for integer p.

In our calculation, we will use this formula to compute the free entropy of our system, f ≡ limn→∞ fn.

We will thus need the moments of the partition function, for integer p:

EZ p

n = E

dw

P0 ({ wil

K
l=1

= E

dwa

P0 ({ wa
il

K
l=1

n

i=1
(cid:89)

Rn×RK





(cid:90)
p

Rn×RK





a=1 (cid:90)
(cid:89)

n

i=1
(cid:89)

p

m

(cid:9)

µ=1
(cid:89)

(cid:1)

Pout

Yµ



m


Pout

(cid:9)

µ=1
(cid:89)

(cid:1)

K

p

,

(cid:41)

l=1








K

Xµiwa
il

Xµiwil

n

1
√
n

(cid:40)

(cid:12)
(cid:12)
(cid:12)





Yµ

(cid:40)

(cid:12)
(cid:12)
(cid:12)

i=1
(cid:88)

1
√
n

n

i=1
(cid:88)

(cid:41)

l=1

.









The outer expectation is done over Xµi ∼ N (0, 1), w(cid:63) and Y . Writing w(cid:63) as w0 we have:

EZ p

n = EX

dY

dwa

P0

{wa

il}K
l=1

Rm

(cid:90)

Rn×RK

a=0 (cid:34) (cid:90)
(cid:89)

m

n

i=1
(cid:89)

(cid:0)
1
√
n

n

(cid:1)
Xµiwa
il

K



(cid:41)

l=1

.
(cid:35)

i=1
(cid:88)

×

Pout

Yµ



µ=1
(cid:89)

(cid:40)

(cid:12)
(cid:12)
(cid:12)


To perform the average over X, we notice that, since it is an i.i.d. standard Gaussian matrix, then for every
il follows a Gaussian multivariate distribution, with zero mean. This naturally

n
i=1 Xµiwa
a, µ, l, Za
leads to introduce its covariance tensor, which is equal to:

µl ≡ n−1/2



(cid:80)

EZa

µlZb

νl(cid:48) = δµνΣ al
bl(cid:48)

= δµνQal
bl(cid:48),

Qal

bl(cid:48) ≡

wa

ilwb

il(cid:48).

1
n

n

i=1
(cid:88)

(62)

(63)

For every a, b, Qa
functions for fixing Q, we arrive at :

b ∈ RK×K is the overlap matrix, and Σ is of size size (p + 1)K × (p + 1)K. Introducing δ

E [Z p

n] =

dQar
ar

dQar
br(cid:48)

Iprior({Qar

br(cid:48)}) × Ichannel({Qar

br(cid:48)})

,

(64)

R
(cid:89)(a,r) (cid:90)

R
(cid:89){(a,r);(b,r(cid:48))} (cid:90)

(cid:2)

(cid:3)

29

with:

Iprior({Qar

br(cid:48)}) =

dwaP0(wa)

p

a=0 (cid:20)(cid:90)
(cid:89)

Rn×K

p

Ichannel({Qar

br(cid:48)}) =

dY

dZa

Rm×K

n

1
n

δ

Qal

bl(cid:48) −

wa

ilwb
il(cid:48)

,

(cid:21)

(cid:32)


(cid:89){(a,l);(b,l(cid:48))}
p

Pout(Y |Za)e− m

i=1
(cid:88)
2 ln det Σ− mK(p+1)

2

(cid:33)

ln 2π

Rm

(cid:90)

exp

−

1
2





a=0 (cid:90)
(cid:89)
m

a=0
(cid:89)
µl(cid:48)(Σ−1) al

µlZb
Za

µ=1
(cid:88)

a,b
(cid:88)

l,l(cid:48)
(cid:88)

.

bl(cid:48)



By Fourier expanding the delta functions in Iprior, and performing a saddle-point method, one obtains:

in which (recall α ≡ limn→∞ m/n) :

lim
n→∞

1
n

ln E [Z p

n] = extrQ, ˆQ

H(Q, ˆQ)
(cid:105)
(cid:104)

,

H(Q, ˆQ) ≡

p

1
2

a=0
(cid:88)

l,l(cid:48)
(cid:88)

Qal
al

ˆQal

al −

1
2

a(cid:54)=b
(cid:88)

l,l(cid:48)
(cid:88)

Qal

bl(cid:48) ˆQal

bl(cid:48) + ln I + α ln J,

in which we defined:

p

I ≡

RK

a=0 (cid:90)
(cid:89)

p

J ≡

dy

R

(cid:90)

RK

a=0 (cid:90)
(cid:89)

dwaP0(wa) exp

−

ˆQal

al(cid:48)wa

l wa

l(cid:48) +

p

1
2

a=0
(cid:88)

l,l(cid:48)
(cid:88)





dZa
(2π)K(p+1)/2

Pout(y|Za)
det Σ

√

exp

−

1
2

1
2

p

a(cid:54)=b
(cid:88)

l,l(cid:48)
(cid:88)
K

ˆQal

bl(cid:48)wa

l wb
l(cid:48)

,




l(cid:48)(Σ−1) al

l Zb
Za





a,b=0
(cid:88)

l,l(cid:48)=1
(cid:88)

.

bl(cid:48)



Our goal is to express H(Q, ˆQ) as an analytical function of p, in order to perform the replica trick. To do
so, we will assume that the extremum of H is attained at a point in Q, ˆQ space such that a replica symmetry
property is verified. More concretely, we assume:

∃Q0 ∈ RK×K s.t

∀a ∈ [|0, p|] ∀(l, l(cid:48)) ∈ [|1, K|]2 Qal
∀(a < b) ∈ [|0, p|]2 ∀(l, l(cid:48)) ∈ [|1, K|]2 Qal

al(cid:48) = Q0
ll(cid:48),
bl(cid:48) = qll(cid:48),

∃q ∈ RK×K s.t

and samely for ˆQ0 and ˆq. Note that Q0 is by definition a symmetric matrix, while q is also symmetric by our
assumption of replica symmetry. Under this ansatz, we obtain:

H(Q0, ˆQ0, q, ˆq) =

Tr[Q0 ˆQ0] −

Tr[q ˆq] + ln I + α ln J.

(73)

p + 1
2

p(p + 1)
2

Remains now to compute an expression for I and J that is analytical in p, in order to take the limit p → 0+.
This can be done easily, using the identity, for any symmetric positive matrix M ∈ RK×K and any vector
x ∈ RK: exp (x(cid:124)(M/2)x) =
, in which Dξ is the standard Gaussian measure on RK.

ξ(cid:124)M 1/2x

RK Dξ exp

(cid:82)

(cid:0)

(cid:1)

(65)

(66)

(67)

(68)

(69)

(70)

(71)

(72)

30

We obtain:

I =

Dξ

dw P0(w) exp

−

(cid:124)

w

( ˆQ0 + ˆq)w + ξ

ˆq1/2w

(cid:124)

p+1

,

1
2

RK

(cid:90)

RK

(cid:20)(cid:90)

(cid:20)
y|(Q0 − q)1/2Z + q1/2ξ

(cid:21)(cid:21)

p+1

.

J =

dy

Dξ

dZPout

R

(cid:90)

RK

(cid:90)

RK

(cid:20)(cid:90)

(cid:110)

(cid:111)(cid:21)

Our assumptions must be consistent in the sense that extrQ, ˆQ
In the p → 0+ limit, one easily gets J = 1 and I =
optimal overlap parameters satisfy ˆQ0 = 0 and Q0
the free entropy:

(cid:82)

limp→0+ H(Q, ˆQ)
2 w(cid:124) ˆQ0w0
(cid:105)
− 1
. This implies that the
ll(cid:48) = EP0 [wlwl(cid:48)]. In the end, we obtain the final formula for

= 0 (because EZ 0

RK dw P0(w) exp

n = 1).

(cid:104)

(cid:104)

(cid:105)

lim
n→∞

fn = extrq,ˆq

−

Tr[q ˆq] + IP + αIC

,

1
2

IP ≡

(cid:26)
Dξ

RK

(cid:90)

RK

(cid:90)

(cid:27)
1
−
2

(cid:20)

dw0P0(w0) exp

(cid:124)
(w0)

ˆqw0 + ξ

(cid:124)

ˆq1/2w0

× ln

dwP0(w) exp

−

RK

(cid:20)(cid:90)
DZ0Pout

(cid:20)
y|(Q0 − q)1/2Z0 + q1/2ξ

(cid:21)
(cid:124)
ˆqw + ξ

(cid:124)

w

1
2

ˆq1/2w

,

(cid:21)(cid:21)

× ln

RK

(cid:20)(cid:90)

(cid:110)
DZPout

(cid:110)

y|(Q0 − q)1/2Z + q1/2ξ

.

(cid:111)

(cid:111)(cid:21)

IC ≡

dy

Dξ

R

(cid:90)

RK

(cid:90)

RK

(cid:90)

A known ambiguity of the replica method is that its result is given as an extremum, here over the set
S +
K(Q0) of positive symmetric matrices, such that (Q0 − q) is also a positive matrix. It is easy to show that
this form gives back the form given in Theorem 3.1, by assuming that this extremum is realized as a supˆq inf q.
Note that in the notations of Theorem 3.1, Q0 is denoted ρ and ˆq is denoted R.

C Generalization error

We detail here two different possible definitions of the generalization error, and how they are related in our
system. Recall that we wish to estimate W ∗ from the observation of ϕout(XW ∗). In the following, we denote
E for the average over the (quenched) W ∗ and the data X, and (cid:104)−(cid:105) for the Gibbs average over the posterior
distribution of W . One can naturally define the Gibbs generalization error as:

(cid:15)Gibbs
g

≡

EW ∗,X

[ϕout (XW ) − ϕout (XW ∗)]2

,

and define the Bayes-optimal generalization error as:

(cid:15)Bayes
g

≡

EW ∗,X

(cid:104)ϕout (XW )(cid:105) − ϕout (XW ∗)

1
2

1
2

(cid:10)

(cid:2)(cid:0)

(cid:11)

2

.

(cid:1)

(cid:3)

(74)

(75)

(76)

(77)

(78)

31

Using the Nishimori identity A.1, one can show that:

(cid:15)Bayes
g

=

EX,W ∗

ϕout (XW ∗)2

+

EX,W ∗

(cid:104)ϕout (XW )(cid:105)2

(cid:105)
− EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) ,

(cid:104)

(cid:105)

=

EX,W ∗

ϕout (XW ∗)2

−

EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) .

1
2

1
2

(cid:104)

(cid:104)

1
2

1
2

(cid:105)

Using again the Nishimori identity one can write:

(cid:15)Gibbs
g

= EX,W ∗

ϕout (XW ∗)2

− EX,W ∗ (cid:104)ϕout (XW ∗) ϕout (XW )(cid:105) ,

g

(cid:105)

= 2(cid:15)Bayes
g

(cid:104)
which shows that (cid:15)Gibbs
. Note finally that since the distribution of X is rotationally invariant, the
quantity EX [ϕout (XW ∗) ϕout (XW )] only depends on the overlap q ≡ W (cid:124)W ∗. As the overlap is shown to
concentrate under the Gibbs measure by Proposition 5.3, and as we expect that the value it concentrates on
is the optimum q∗ of the replica formula (such fact is proven, e.g., for random linear estimation problems in
[52]), the generalization error can itself be evaluated as a function of q∗. Examples where it is done include
[53, 3, 19, 11].

C.1 The generalization error at K = 2

In this subsection alone, we go back to the K = 2 case, instead of the K → ∞ limit. From the definition of
the generalization error (see sec. C), one can directly give an explicit expression of this error in the K = 2
case. Recall our committee-symmetric assumption on the overlap matrix, which here reads

q =

qd + qa
2
qa
2

(cid:32)

qa
2
qd + qa

2 (cid:33)

.

For concision, we denote here sign(x) = σ(x). One obtains from (78):

1
2

− 2(cid:15)Bayes,K=2

g

=

Dx σ [σ(x1) + σ(x2)]

× σ

σ

(

+ qd)x1 +

x2 + x3

1 −

qa
2

q2
a
2

− qaqd − q2
d

(cid:35)

(cid:114)

qa
2

(cid:40)

(cid:34)

R4

(cid:90)

qa
2

+σ

x1 + (

+ qd)x2 − x3

qa
2





qa(qd + qa
2 )
2 − qaqd − q2
d

a

1 − q2

+ x4

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:113)

(1 − q2

d)(1 − (qa + qd)2)
2 − qaqd − q2
d

a

1 − q2

.










Note that one could possibly simplify this expression by using an appropriate orthogonal transformation on x.
These integrals were then computed using Monte-Carlo methods to obtain the generalization error in the left
and middle plots of Fig. 2.

(79)

D The large K limit in the committee symmetric setting

We consider the large K limit2 for a sign activation function, and for different priors on the weights. Since the
output is a sign, the channel is simply a delta function. We assume a committee symmetric solution, i.e. the

2A similar limit has been derived in the context of coding with sparse superposition codes [54]. There the large input alphabet
limit of the mutual information is considered after the thermodynamic limit n → ∞ corresponding to the large codeword limit in this
coding context.

32

(cid:124)
matrices q and ˆq (q and R in the notations of Theorem 3.1) are of the type q = qd1K + qa
K, with the unit
K 1K1
vector 1K = (1)K
l=1, and similarly for ˆq. In the large K limit, this scaling of the order parameters is natural.
Indeed, assume that the covariance of the prior is Q0 = 1K (Q0 = ρ in the notations of Theorem 3.1). Since
both q and (Q0 − q) are assumed to be positive matrices, it is easily shown to imply that 0 ≤ qd ≤ 1 and
0 ≤ qa + qd ≤ 1.

D.1 Large K limit for sign activation function

In the following, we consider Q0 = σ21K. We are interested here in computing the leading order term in IC of
(76). Note that replacing σ2 by 1 in this equation only amounts to replacing q by q/σ2, so we can assume σ2 = 1
RK Dξ IC(y, ξ) log IC(y, ξ), with
without loss of generality. We (abusively) write IC in (76) as IC =
the definition

y=±1

(cid:80)

(cid:82)

IC(y, ξ) ≡

DZPout

y|(Q0 − q)1/2Z + q1/2ξ

.

(80)

RK

(cid:90)

(cid:110)

(cid:111)

the remarks above). Note that this implies that q1/2 =
√
√

Here, we assumed a sign activation function and no noise, as well as a particular form for Q0 and q (see
√
(cid:124)
qa+qd−
K and that (Q0 − q)1/2 =
K
(cid:124)
K. All together, this gives the following explicit expression for IC(y, ξ) :
1K1

1 − qd1K +

qd1K +

1−qa−qd−

1K1

1−qd

qd

√

√

√

K

IC(y, ξ) ≡

DZ

RK

(cid:90)

× δ

y − sign

(cid:40)

K

1
√
K

(cid:34)

sign

1 − qdZl +

1 − qa − qd −

1 − qd

l=1
(cid:88)
(cid:124)
K Z
Introducing a new variable w ≡ 1
√
K
another variable u being the argument of the outer sign function in the previous equations, one obtains:

and a Fourier-transform of the then-introduced delta function, as well as

(cid:16)(cid:112)

(cid:112)

(cid:112)

(cid:17)

(cid:20)

(cid:124)
1
KZ
K

+ (q1/2ξ)l

.

(cid:21)(cid:35)(cid:41)

IC(y, ξ) =

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

eiw ˆw+iuˆuδy,sign(u)

Dze

−i ˆw z√

K e

(cid:20)

(cid:20)(cid:114)

− iˆu√
K

sign

z+

(cid:21)

1−qa−qd
1−qd

−1

w√
K

+ 1√

1−qd

(q1/2ξ)l

(cid:21)

.

K

×

R

l=1 (cid:90)
(cid:89)

λl(w, ξ) ≡

1 − qa − qd
1 − qd

− 1
(cid:21)

(cid:20)(cid:114)

w
√
K

+

√

1
1 − qd

(q1/2ξ)l,

IC(y, ξ) =

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

eiw ˆw+iuˆuδy,sign(u)

Dze

−i ˆw z√

− iˆu√
K

K e

sign[z+λl(w,ξ)]

.

K

R

l=1 (cid:90)
(cid:89)

Denote

such that

33

(81)

(82)

(83)

(84)

(85)

For 1 ≤ l ≤ K, one can rewrite the factorized integral in the last expression of IC(y, ξ) as:

IC(y, ξ) =

eiw ˆw+iuˆuδy,sign(u)

J (λl(w, ξ), ˆw, ˆu) ,

J (λl(w, ξ), ˆw, ˆu) ≡ e

Dze

z(λl−i ˆw√
K

)

− iˆu√
e
K

sign[z]

.

dwd ˆw
2π

dudˆu
2π

R

(cid:90)

λ2
l
2 +iλl

−

ˆw√
K

R

(cid:90)

K

l=1
(cid:89)

We abusively dropped the dependency of λl on (w, ξ). Note the following identity:

F (α, iβ) ≡

Dzeαz+iβ sign(z) = eα2/2

cos β + i sin β ˆH(α)

,

(cid:104)

(cid:105)

R

(cid:90)

√

with ˆH(x) = erf(x/

2). Using it in our previous expressions, we obtain:

J(λl, ˆw, ˆu) = e− 1

2K ˆw2

cos

− i sin

ˆH

λl − i

ˆu
√
K (cid:19)

ˆu
√
K (cid:19)

(cid:20)
Note that by our committee-symmetry assumption, we have λl(w, ξ) = λl,0(ξ) + 1√
K
λ1 typically of order 1 when K → ∞:

(cid:18)

(cid:18)

(cid:18)

ˆw
√
K (cid:19)(cid:21)

.

λ1(w, ξ) with λl,0 and

λl,0(ξ) ≡

λ1(w, ξ) ≡

ξl,

qd
1 − qd
1 − qa − qd
1 − qd

(cid:114)

(cid:20)(cid:114)

− 1

w +

(cid:21)

(cid:20)(cid:114)

qa + qd
1 − qd

−

qd
1 − qd (cid:21)

(cid:114)

(cid:124)
1
Kξ
√
K

.

Expanding J(λl, ˆw, ˆu) as K → ∞, we obtain using the known development of the error function:

J(λl, ˆw, ˆu) = e− 1

2K ˆw2

1 −

− i ˆH [λl,0(ξ)]

ˆu2
2K

(cid:34)

ˆu
√
K

− i

ˆu [λ1(w, ξ) − i ˆw]
K

2
π

(cid:114)

λl,0(ξ)2

e−

2 + O(K−3/2)

.

(cid:35)

This yields (putting back the (w, ξ) dependency):

K

l=1
(cid:89)

J [λl(w, ξ), ˆw, ˆu)] = e− 1

2 ˆw2

exp

−

− iˆuS1 − i

ˆu(λ1 − i ˆw)Γ0 +

ˆu2
2

(cid:34)

2
π

(cid:114)

1
2

ˆu2S2 + O(K−1/2)
(cid:35)

,

(86)

in which we defined the following quantities, that only depend on ξ (recall (84))

wξ(ξ) ≡

ξl,

Γ0(ξ) ≡

e− 1

2 λl,0(ξ)2

,

S1(ξ) ≡

ˆH(λl,0(ξ)),

S2(ξ) ≡

ˆH(λl,0(ξ))2.

1
√
K

1
√
K

K

l=1
(cid:88)
K

l=1
(cid:88)

1
K

1
K

K

l=1
(cid:88)
K

l=1
(cid:88)

A detailed calculation actually shows that the previous expansion of (86) is valid up to O(K−1), and not only
O(K−1/2). Recall also (81), in which one can now readily perform the integration over all variables w, ˆw, u, ˆu
to obtain (dropping the ξ dependency in wξ, Γ0, S1, S2):

IC(y, ξ) = H

−y

+ O(K−1),

(87)

S1 +

(cid:113)





√

qd

√

2
π wξΓ0
(cid:113)
1 − S2 − 2

√

qd+qa−
1−qd
qa
1−qd

π Γ2
0





34

∞

x Dz = 1

in which H(x) ≡
. Note that all quantities wξ, Γ0, S1, S2 only depend on ξ via
its empirical measure, which implies that the integration over ξ ∈ RK will be tractable. We compute it in the
following, using theoretical physics methods. We denote the quantity that appears in (87) as a function of
wξ, Γ0, S1, S2:

1 − erf(x/

2)

(cid:82)

(cid:3)

(cid:2)

2

√

G(y, wξ, Γ0, S1, S2) ≡ H

−y

S1 +

(cid:113)





√

qd

√

2
π wξΓ0
(cid:113)
1 − S2 − 2

√

qd+qa−
1−qd
qa
1−qd

π Γ2
0

.





Introducing once again delta functions and their Fourier transforms for wξ, Γ0, S1, S2, we write, starting from
(87):

IC =

DξIC(y, ξ) log IC(y, ξ)

RK

y=±1 (cid:90)
(cid:88)

=

y=±1 (cid:90)
(cid:88)

dwξd ˆwξ
2π

dΓ0dˆΓ0
2π

dS1d ˆS1
2π

dS2d ˆS2
2π

eiw ˆw+iΓ0 ˆΓ0+iS1 ˆS1+iS2 ˆS2 G(y, wξ, Γ0, S1, S2)

× log G(y, wξ, Γ0, S1, S2)

Dξe−i ˆwwξ(ξ)−iˆΓ0Γ0(ξ)−i ˆS1S1(ξ)−i ˆS2S2(ξ)

+ O(K−1).

(88)

RK

(cid:20)(cid:90)

(cid:21)

The integral over ξ in (88) can be computed in the limit K → ∞:

Λ ≡

Dξe−i ˆwwξ(ξ)−iˆΓ0Γ0(ξ)−i ˆS1S1(ξ)−i ˆS2S2(ξ)

RK

(cid:90)

= 
(cid:90)

R

Dξ exp 

−i



The large K expansion yields




ˆwξ
√
K

− i

−
ˆΓ0e

qd
2(1−qd) ξ2
K

− i

ˆS1 ˆH

qd
1−qd

ξ

ˆS2 ˆH

qd
1−qd

ξ

√
(cid:104)(cid:113)
K

− i

(cid:105)

(cid:104)(cid:113)
K

2

K

(cid:105)











Λ = exp

−

ˆw2 − iˆΓ

1 − qd − ˆS1 ˆwE

1
2

(cid:40)

ξ ˆH
(cid:20)

(cid:18)(cid:114)

qd
1 − qd

ξ

(cid:19)(cid:21)

2

qd
1 − qd

ξ

1 + i ˆS2
ˆS2

E

ˆH

+ O(K−1) .

(cid:34)

(cid:19)

(cid:18)(cid:114)

(cid:35) (cid:41)

(cid:19)

(cid:112)

−

1
2

(cid:18)

The expectations are taken with respect to a real variable ξ ∼ N (0, 1). These expectations are known by
properties of the error function:

One can now compute the integrals over the “hat” variables in (88). Denote Γf

0 ≡

2(1−qd)
π

, and Sf

2 ≡

(cid:113)

E

ˆH
(cid:34)

E

ξ ˆH

qd
1 − qd

ξ

2

(cid:35)

(cid:19)

(cid:18)(cid:114)

2
π

=

arcsin qd ,

qd
1 − qd

ξ

2qd
π

.

=

(cid:19)(cid:21)

(cid:114)

(cid:20)

(cid:18)(cid:114)

35

2
π arcsin qd. This yields:

IC =

DwDS1 G

R2

(cid:90)

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

2qd
π

, Sf
2

(cid:114)

log G

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

(cid:33)

2qd
π

(cid:114)

, Sf
2

.

(cid:33)

(89)

.





(90)

Note that

G

y, w, Γf
0 ,

(cid:32)

(cid:114)

2(arcsin qd − qd)
π

S1 + w

2qd
π

, Sf
2

(cid:33)

(cid:114)

= H

−y



2
π

(cid:114)

√

arcsin qd − qdS1 + w

qd + qa

√

1 − 2

π (qa + arcsin qd)

Making the change of variable Snew
reaches:

1 = S1 + w

√

√

qd+qa
arcsin qd−qd


in (89), and defining γ ≡ 2

(cid:113)

π (qa + arcsin qd), one

IC =

DxH

yx

log H

yx

γ
1 − γ

γ
1 − γ

+ O(K−1).

(cid:20)

(cid:114)

(cid:21)

(cid:20)

(cid:114)

(cid:21)

R

y=±1 (cid:90)
(cid:88)

The two values of y contribute in the same way, which finally yields:

IC = 2

DxH

x

log H

x

R

(cid:90)

(cid:20)

(cid:114)

(cid:21)

(cid:20)

(cid:114)

(cid:21)

γ
1 − γ

γ
1 − γ

+ O(K−1).

Note that the parameter γ is naturally bounded to the interval [0, 1] by the conditions 0 ≤ qd ≤ 1 and
0 ≤ qa + qd ≤ 1.

D.2 The Gaussian prior

The prior part IP of the free entropy of (76) is very easy to evaluate in the Gaussian prior setting. We consider
a prior with covariance matrix Q0 = IK (we can simply rescale q by q/σ2 in the final expression for a finite
variance Q0 = σ2IK as we already described). Performing the Gaussian integration in IP in (76) yields:

IP =

ˆqd +

ˆqa −

log(1 + ˆqd) −

log (1 + ˆqd + ˆqa) .

(91)

K
2

1
2

K − 1
2

1
2

D.3 The fixed point equations

From the definition of the free entropy (76) and the expansions for IP and IC obtained in (90) and (91), one
obtains the fixed point equations after having extremized over ˆqd and ˆqa (recall that α ≡ lim m

n ):

∂qa [IG(qd, qa) + αIC(qd, qa)] = 0,
∂qd [IG(qd, qa) + αIC(qd, qa)] = 0,

(92)

(93)

with IG(qd, qa) defined as:

IG(qd, qa) ≡

[qa + Kqd] −

1
2

IC(qd, qa) = 2

DxH

x

R

(cid:90)

(cid:20)

(cid:114)

K − 1
2
γ
1 − γ

(cid:21)

log

(cid:20)
log H

1
1 − qd (cid:21)
x

γ
1 − γ

,

(cid:21)

(cid:20)

(cid:114)

−

log

1
2

1
1 − qa − qd (cid:21)

,

(cid:20)

36

and recall that γ ≡ 2

π (qa + arcsin qd).

The fixed point equations (92), (93) have different behaviors depending on the scaling of α with the hidden

layer size K. We detail these different behaviors in the following paragraphs.

D.3.1 Regime α = oK→∞(K)

In this regime (which in particular contains the case in which α stays of order 1 when K → ∞), the fixed
point equations (92), (93) can be simplified as:

qd = 0,
qa = 2α(1 − qa) ∂IC
∂qa

.

(cid:40)

D.3.2 Regime α = ΘK→∞(K)

In this regime, we naturally define
solutions of the fixed point equations (92), (93) must satisfy the following scaling : qa + qd = 1 − χ
χ ≥ 0 a reaching a finite value when K → ∞. The fixed point equations in terms of χ and qd read:

α will remain of order 1. One can show that the
K , with

αK ≡ α/K, such that

(cid:101)

(cid:101)

qd = 2(1 − qd)

χ−1 = 2

α ∂IC
∂qa

.

1√

1−q2
d

− 1

˜α ∂IC
∂qa

,

(cid:19)

(cid:18)






Note that the State Evolution (SE) computation of Figure 2 was performed by solving the fixed point

(cid:101)

equations (94) and (95) (depending on the regime of α).

It is easy to show that (95) always admit what we call a non-specialized
The stability of the qd = 0 solution:
solution, i.e. a solution with qd = 0. This solution stops to be optimal in term of the free energy at a finite
αspec (cid:39) 7.65. However, one can show that this solution will remain linearly stable for every
α. Actually, it is
linearly stable in the much broader regime α = o(K2). Going back to the initial formulation of the fixed point
(cid:101)
equations (92),(93), and adding the correct time indices to iterate them, one obtains:

(cid:101)

with F and G defined as:

(cid:0)

(cid:1) (cid:0)

d, qt
F (qt
a)
1 + F (qt
d, qt
a)

,

qt+1
d =

qt+1
a =

1 + F (qt

d, qt
a)

d, qt

a)G(qt

d, qt
a)

d, qt
G(qt
a)
1 + F (qt

,

(cid:1)

F (qd, qa) ≡

[∂qdIC − ∂qaIC] ,

G(qd, qa) ≡

∂qaIC −
(cid:20)

1
K

∂qdIC

.

(cid:21)

2α
K − 1
2αK
K − 1

We focus on the behavior of (96) around qd = 0. Given our previous expansion of IC in the K → ∞
|qd=0 →K→∞ 0, which means the qd = 0 solution

limit, and (98), one easily sees that for α = oK→∞(K2), ∂F
∂qd
always remains linearly stable.

However, assume now that α = Θ(K2). Performing a similar calculation to the one shown in sec. D.1,

(94)

(95)

(96)

(97)

(98)

(99)

37

one can show the following expansion:

IC(qd, qa) = I

(0)
C (qd, qa) +

(1)
C (qd, qa) + O

I

1
K

1
K2

.

(cid:18)

(cid:19)

The term of ∂F
∂qd
seen from (98).

|qd=0 arising from I

(1)
C will thus have a possibly non-zero contribution in the K → ∞ limit, as

To summarize, the non-specialized solution always remains linearly stable in the large K limit at least for
α (cid:28) K2. This implies that in this regime, Approximate Message Passing can not escape the non-specialized
fixed point to find the specialized solution, as seen in Fig. 3. For α of order larger than K2, one would have to
(1)
C in order to check that ∂F
|qd=0 (cid:54)= 0 to show that the non-specialized solution is indeed
explicitly compute I
∂qd
linearly unstable. This tedious calculation is left for future work.

D.4 The generalization error at large K

Recall the definition of the generalization error in (78). From the remarks of section C, one can compute it at
large K by applying the same techniques used to compute the channel integral IC in sec. D.1. One obtains
after a tedious, yet straightforward, calculation:

(cid:15)Bayes
g

=

(cid:15)Gibbs
g

=

arccos

(qa + arcsin qd)

+ O(K−1).

(100)

1
2

1
π

2
π

(cid:20)

(cid:21)

This expression is the one used in the computation of the generalization error in the left panel of Fig. 3.

E Linear networks show no specialization

An easy yet interesting case is a linear network with identical weights in the second layer and a final output
function σ : R → R, i.e a network in which ϕout(h) = σ
. For clarity, in this section, we
decompose the channel as Pout(y|ϕout(Z)) for Z ∈ RK instead of Pout(y|Z). We will compute the channel
integral IC of the replica solution (76). For simplicity, we assume that Q0 = 1K the identity matrix (i.e w has
RK DξIC(y, ξ) log IC(y, ξ). One
identity covariance matrix under P0). Note that (76) gives IC as IC =
can easily derive:

K
l=1 hl

1√
K

R dy

(cid:80)

(cid:1)

(cid:0)

(cid:82)

(cid:82)

IC(y, ξ) = e− 1

2 ξ(cid:124)(1K −q)−1qξ

eiuˆuPout(y|σ(u))

dudˆu
2π

R2

(cid:90)

×

RK

(cid:90)

dZ
(2π)K det(1K − q)

e− 1

2 Z(cid:124)(1K −q)−1Z+Z(cid:124)X(ˆu,xi),

in which we denoted X(ˆu, xi) (cid:44) (1K − q)−1q1/2ξ − iˆu√
K
integration over Z can be done, as well as the integration over ˆu:

(cid:112)

1K, with the unit vector 1K = (1)K

l=1. The inner

IC(y, ξ) =

1
1 − 1

(cid:124)
R
Kq1K (cid:90)
K 1

du
√
2π

Pout(y|σ(u)) exp 

−

(cid:113)

(cid:124)
u − 1√
Rq1/2ξ
1
K
(cid:124)
1 − 1
Kq1K
K 1

(cid:16)
2

2

.






(cid:17)

(cid:1)




(cid:0)

So we can formally write the total dependency of IC(y, ξ) on ξ and on q as

IC(y, ξ) = IC

y,

1
√
K

(cid:124)
Kq1/2ξ,
1

(cid:124)
Kq1K
1

1
K

.

(cid:19)

(cid:18)

38

Note that we have the following identity, for any fixed vector x ∈ RK and smooth real function F :

DξF (x

ξ) =

√

(cid:124)

duF (u)e− u2

2x(cid:124)x .

1
2πx(cid:124)x

R

(cid:90)

RK

(cid:90)

In the end, if we denote Γ(q) (cid:44) 1

(cid:124)
Kq1K, we have:

K 1

IC =

dy

dve

2Γ(q) IC(v, y) log IC(v, y),

1

− v2

IC(v, y) ≡

du Pout(y|σ(u)) exp

−

1
2 (1 − Γ(q))

(u − v)2

.

(cid:21)

(cid:20)

R

(cid:90)

2πΓ(q)
1
(cid:112)
2π(1 − Γ(q))

R

(cid:90)

R

(cid:90)

(cid:112)

Note that by hypothesis, both q and 1K − q are positive matrices, so 0 ≤ Γ(q) ≤ 1. As these equations show,
IC only depends on Γ(q) = K−1
l,l(cid:48) qll(cid:48). From this one easily sees that extremizing over q implies that the
optimal ˆq satisfies ˆqll(cid:48) = ˆq/K for some real ˆq. Subsequently, all qll(cid:48) are also equal to a single value, that we
can denote q

K . This shows that this network never exhibits a specialized solution.

(cid:80)

(101)

(102)

(103)

F Update functions and AMP derivation

AMP can be seen as Taylor expansion of the loopy belief-propagation (BP) approach [13, 14, 55], similar to
the so-called Thouless-Anderson-Palmer equation in spin glass theory [35]. While the behaviour of AMP can
be rigorously studied [17, 18, 56], it is useful and instructive to see how the derivation can be performed in
the framework of belief-propagation and the cavity method, as was pioneered in [36, 38] for the single layer
problem. The derivation uses the Generalized AMP notations of [16] and follows closely the one of [26].

F.1 Definition of the update functions

Let’s consider the distributions probabilities Qout and Q0, closely related to the inference problems eq. (3) and
eq. (4):

Qout(z; ω, y, V ) ≡

e− 1

2 (z−ω)(cid:124)V −1(z−ω)Pout(y|z); Q0(W ; Σ, T ) ≡

P0(W )e− 1

2 W (cid:124)Σ−1W +T (cid:124)Σ−1W .

1
ZPout

1
ZP0

We define the update functions gout, ∂ωgout, fw and fc, which will be useful later in the algorithm:

gout(ω, y, V ) ≡ ∂ω log(ZPout) = V −1EQout [z − ω] ,

(cid:124)
∂ωgout(ω, y, V ) = V −1EQout [(z − ω)(z − ω)
fw(Σ, T ) ≡ ∂Σ−1T log ZP0 = EQ0[W ] ,
fc(Σ, T ) ≡ ∂Σ−1T fw = EQ0[W W

(cid:124)

] − fwf

(cid:124)
w .

] − V −1 − goutg

(cid:124)
out ,

Note that gout is the mean of V −1(z − ω) with respect tor Qout and fw the mean of Q0.

F.2 Derivation of the Approximate Message Passing algorithm

F.2.1 Relaxed BP equations

Lets consider a set of messages {mi→µ, ˜mµ→i}i=1..n,µ=1..m on the bipartite factor graph corresponding to
our problem Fig. 4. These messages correspond to the marginal probabilities of Wi if we remove the edges
i → µ or µ → i. The belief propagation (BP) equations (or sum-product equations) can be formulated as the
following [14, 55], where Wi = (wil)l=1..K ∈ RK:

39

Pout (Yµ|{XµWi}n
µ = 1...m

i=1)

˜mµ→i

Wi ∈ RK
i = 1...n

P0(Wi)
i = 1...n

mi→µ

Figure 4: Factor graph representation of the committee machine (for n = 4 and m = 3). The variable (circle)
Wi ∈ RK needs to satisfy a prior constraint (square) P0 and a constraint accounting for the fully connected layer,
that correlates all the variables together.

i→µ(Wi) =

P0(Wi)

˜mt

ν→i(Wi) ,

1
Zi→µ

m

k(cid:54)=µ
(cid:89)

µ→i(Wi) =

dWjPout

Yµ|

XµjWj

mt

j→µ(Wj) .

1
Zµ→i (cid:90)

n

j(cid:54)=i
(cid:89)

1
√
n

n

j=1
(cid:88)









The term inside Pout can be decouple using its K-dimensional Fourrier transform



mt+1



˜mt

Pout

Yµ|

XµjWj

=

1
√
n

n

j=1
(cid:88)





1
(2π)K/2

RK

(cid:90)

dξ exp

(cid:124)

iξ









1
√
n

n

j=1
(cid:88)





XµjWj

ˆPout(Yµ, ξ)

.









Injecting this representation in the BP equations, (104) becomes

˜mt

µ→i(Wi) =

dξ ˆPout(Yµ, ξ) exp

XµiWi

1
(2π)K/2Zµ→i (cid:90)
×

n

RK

RK

j(cid:54)=i (cid:90)
(cid:89)

(cid:124)

dWjmt

j→µ(Wj) exp

iξ

XµjWj)

,

iξ

(cid:124) 1
√
n

(cid:124) 1
√
n

(cid:18)

(cid:18)

≡Ij

(cid:123)(cid:122)

(cid:19)

(cid:19)

(cid:125)

and we define the mean and variance of the messages

ˆW t

j→µ ≡

dWjmt

j→µ(Wj)Wj ,

ˆCt

j→µ ≡

dWjmt

j→µ(Wj)WjW

(cid:124)
j − ˆW t

j→µ( ˆW t

(cid:124)
j→µ)

.

RK

(cid:90)

RK

(cid:90)






In the limit n → ∞ the term Ij can be easily expanded and expressed using ˆW and ˆC

(104)

(105)

Ij =

dWjmt

j→µ(Wj) exp

iξ

Wj)

(cid:39) exp

(cid:124) Xµj√
n

(cid:18)

(cid:19)

RK

(cid:90)

Xµj√
n

i
(cid:32)

(cid:124) ˆW t

ξ

j→µ −

(cid:124) ˆCt

ξ

j→µ , ξ

X 2
µj
n

1
2

,

(cid:33)

40

and finally using the inverse Fourier transform, we obtain

˜mt

µ→i(Wi) (cid:39)

dzPout(Yµ, z)

dξe−iξ(cid:124)zeiXµiξ(cid:124)Wi

1
(2π)KZµ→i (cid:90)
×

RK
n

RK

(cid:90)
(cid:124) ˆW t

ξ

j→µ −

X 2
µj
n

1
2

(cid:124) ˆCt

ξ

j→µξ

(cid:33)

exp

i

Xµj√
n

(cid:32)

j(cid:54)=i
(cid:89)

dzPout(Yµ, z)

dξe−iξ(cid:124)zeiXµiξ(cid:124)Wie

iξ(cid:124)

n
(cid:80)
j(cid:54)=i

Xµj√
n

ˆW t

j→µ

2 ξ(cid:124)

− 1
e

X2
µj
n

n
(cid:80)
j(cid:54)=i

ˆCt

j→µξ

=

=

1
(2π)KZµ→i (cid:90)
1
(2π)KZµ→i (cid:90)

RK

RK

dzPout(Yµ, z)

RK

(cid:90)

(2π)K
det(V t
iµ)

− 1
e
2

(cid:115)

(cid:16)

z−

Xµi√
n

Wi−ωt
iµ

(cid:17)(cid:124)

(V t

iµ)−1(cid:16)

z−

Xµi√
n

Wi−ωt
iµ

(cid:17)

,

(cid:125)

≡Hiµ

(cid:123)(cid:122)

1
n

n

j(cid:54)=i
(cid:88)

ωt
iµ ≡

Xµj ˆW t

j→µ ,

V t
iµ ≡

X 2
µj

ˆCt

j→µ .

(106)

1
√
n

n

j(cid:54)=i
(cid:88)

where we defined the mean and variance, depending on the node i

(cid:124)

Again, in the limit n → ∞, the term Hiµ can be expanded:

Hiµ (cid:39) e− 1

2 (z−ωt

(cid:124)
iµ)

(V t

iµ)−1(z−ωt

iµ)

Xµi√
n

1 +

(cid:32)

W

(cid:124)
i (V t

iµ)−1(z − ωt

iµ) −

W

(cid:124)
i (V t

iµ)−1Wi

X 2
µi
n

1
2

X 2
µi
n

1
2

+

W

(cid:124)
i (V t

iµ)−1(z − ωt

iµ)(z − ωt

(cid:124)
iµ)

(V t

iµ)−1Wi

.

(cid:33)

Gathering all pieces, the message ˜mµ→i can be expressed using definitions of gout and ∂ωgout

˜mt

µ→i(Wi) ∼

W

(cid:124)
i gout(ωt

iµ, Yµ, V t

iµ) +

W

(cid:124)
i goutg

(cid:124)
out(ωt

iµ, Yµ, V t

iµ)Wi+

1 +

Xµi√
n

X 2
µi
n

1
2

(cid:124)
i ∂ωgout(ωt

iµ, Yµ, V t

iµ)Wi

1 + W

(cid:124)
i Bt

µ→i +

W

(cid:124)
i Bt

µ→i(Bt

µ→i)

(cid:124)

(Wi) −

W

(cid:124)
i At

µ→iWi

1
2

(cid:27)

(cid:41)

1
2

µ→i)
(2π)K exp

(cid:115)

−

1
2

(cid:18)

(cid:0)

W

(cid:124)
i − (At

µ→i)−1Bt

µ→i

(cid:124)

At

µ→i

W

(cid:124)
i − (At

µ→i)−1Bt

µ→i

,

(cid:1)

(cid:0)

(cid:19)

(cid:1)

with the following definitions of Aµ→i and Bµ→i:

Bt

µ→i ≡

gout(ωt

iµ, Yµ, V t

iµ), At

µ→i ≡ −

∂ωgout(ωt

iµ, Yµ, V t
iµ)

(107)

Xµi√
n

X 2
µi
n

Using the set of BP equations (104), we can finaly close the set of equations only over {mi→µ}iµ:

mt+1

i→µ(Wi) =

1
Zi→µ

P0(Wi)

m

ν(cid:54)=µ (cid:115)
(cid:89)

det(At
ν→i)
(2π)K e− 1

2 (Wi−(At

ν→i)−1Bt

(cid:124)
ν→i)

At

ν→i(Wi−(At

ν→i)−1Bt

ν→i).

In the end, computing the mean and variance of the product of gaussians, the messages are updated using

1
Zµ→i (cid:40)
X 2
µi
n

W

1
Zµ→i (cid:26)
det(At

1
2

=

=

41

(108)

(109)

(110)

fw and fc:

ˆW t+1

i→µ = fw(Σt

µ→i, T t

µ→i) ,

ˆCt+1

i→µ = fc(Σt

µ→i, T t

µ→i) ,






m

ν(cid:54)=µ
(cid:80)

µ→i

(cid:32)



µ→i ≡

Σt

T t
µ→i ≡ Σt


At

ν→i

(cid:33)

m

(cid:32)

ν(cid:54)=µ
(cid:80)

−1

,

Bt

ν→i

.

(cid:33)

Summary of the Relaxed BP set of equations:

In the end, using eq .(105,106,107, 108), relaxed BP equations can be written as the following set of equations:

Xµj√
n

ˆW t

j→µ

X 2
µj
n

ˆCt

j→µ

=

=

n

j(cid:54)=i
(cid:80)
n

j(cid:54)=i
(cid:80)
Xµi√

Bt

µ→i =

At

µ→i = −

iµ, Yµ, V t
iµ)

n gout(ωt
X 2
n ∂ωgout(ωt
µi

iµ, Yµ, V t
iµ)

ωt
iµ

V t
iµ






Σt

µ→i =

−1

m

(cid:32)

ν(cid:54)=µ
(cid:80)

At

ν→i

(cid:33)

T t
µ→i = Σt

µ→i

ˆW t+1

i→µ = fw(Σt

m

Bt

ν→i

(cid:33)

(cid:32)

ν(cid:54)=µ
(cid:80)
µ→i, T t

µ→i)

ˆCt+1

i→µ = fc(Σt

µ→i, T t

µ→i)






F.2.2 Approximate Message Passing algorithm

The relaxed BP algorithm uses O(n2) messages. However all the messages depend weakly on the target node.
On a tree, the missing message is negligible, that allows us to expand the previous relaxed BP equations (109)
to make appear the Onsager term at a previous time step, and reduce the number of messages to O(n). We
define the following estimates and parameters based on the complete set of messages:

Xµj√
n

ˆW t

j→µ

X 2
µj
n

ˆCt

j→µ

n

j=1
(cid:80)
n

j=1
(cid:80)

ωt

µ ≡



V t
µ ≡


Σt

i ≡

−1

m

At

ν→i

ν=1

(cid:18)
(cid:80)

i ≡ Σt
T t
i


(cid:18)

m

ν=1
(cid:80)

(cid:19)

Bt

ν→i

(cid:19)

Let’s now expand the previous messages eq. (109), making appear these new target-independent messages:

• Σt

µ→i

• T t

µ→i

Σt

µ→i =

At

ν→i

=

m

ν(cid:54)=µ
(cid:88)









=

IK×K −

−1

m

−1

m

At

ν→i − At

µ→i

=

(cid:32)

ν=1
(cid:88)

−1

(cid:33)

m

−1

At

ν→i

(cid:33)

At

µ→i

At

ν→i

(cid:33)

(cid:32)

ν=1
(cid:88)





m

(cid:32)

ν=1
(cid:88)





ν=1
(cid:88)
−1

At

ν→i 

IK×K −


IK×K − Σt

=

(cid:32)

ν=1
(cid:88)

m

−1

−1

At

ν→i

(cid:33)

At

µ→i



iAt

µ→i

−1

Σt

i (cid:39) Σt


i + O


1
n

(cid:18)

(cid:19)

(cid:39)IK×K +Σt
(cid:0)

iAt

µ→i+O(n−1)

(cid:1)

(cid:124)

(cid:123)(cid:122)

(cid:125)

m

Bt

ν→i

=

Σt

i + O

1
n

m

Bt

ν→i − Bt

µ→i

(cid:33)

(cid:18)

(cid:19)(cid:19) (cid:32)

ν=1
(cid:88)

µ→i = Σt
T t

µ→i 

ν(cid:54)=µ
(cid:88)

iBt
i − Σt

= T t


µ→i + O

(cid:18)

1
n

(cid:18)

(cid:19)

42

• ˆW t+1
i→µ

• V t
µ

• ωt
µ

ˆW t+1

i→µ = fw(Σt

µ→i, T t

µ→i) = fw

Σt

i, T t

i − Σt

iBt

µ→i

+ O

1
n

(cid:18)

(cid:19)

(cid:0)

Σt

iBt

µ→i

(cid:1)

(cid:39) fw

Σt

i, T t
i

−

dfw
dT

= fw

(cid:0)

Σt

i, T t
i
= ˆW t+1
(cid:0)
i

(cid:1)

(cid:1)

−

Σt
i

(cid:0)

(cid:1)

(Σt
(cid:12)
(cid:12)
−1
(cid:12)
(cid:12)

i,T t
i )
Σt

fc

Σt
i

i, T t
i
= ˆCt+1
i

(cid:1)

(cid:0)

(cid:39)

Xµi√
n

(cid:123)(cid:122)
(cid:124)
= ˆW t+1
i −

(cid:125)
Xµi√
n

(cid:124)
−1 ˆCt+1

(cid:125)
(cid:123)(cid:122)
µ, Yµ, V t
igout(ωt
i Σt

Σt
i

Bt

µ→i

µ,Yµ,V t
µ)

gout(ωt
(cid:124) (cid:123)(cid:122) (cid:125)
µ) + O

1
n

(cid:18)

(cid:19)

(cid:0)

(cid:1)

• ˆCt+1
i→µ
Let’s denote for convenience, E =

Σt
i

−1 ˆCt+1

i Σt

igout(ωt

µ, Yµ, V t

µ). Then

ˆCt+1
i→µ = EQ0

ˆW t

i→µ( ˆW t

= EQ0

ˆW t

i −

(cid:0)

(cid:1)

(cid:124)
i→µ)
Xµi√
n

E

(cid:105)

− EQ0

ˆW t
(cid:104)
Xµi√
i −
n

ˆW t

i→µ

(cid:105)

E

i→µ

EQ0
(cid:124)

ˆW t
(cid:104)
− EQ0

(cid:124)

(cid:20)(cid:18)
ˆW t

(cid:124)
i ( ˆW t
i )

(cid:19) (cid:18)
− EQ0

= EQ0

ˆW t
i

EQ0

(cid:19)
(cid:21)
ˆW t
i

(cid:124)

+ O

(cid:105)

(cid:104)

(cid:105)

(cid:104)

(cid:105)

(cid:104)

(cid:104)

(cid:105)
ˆW t

i −

(cid:20)
1
√
n

(cid:18)

(cid:19)

Xµi√
n

E

EQ0

ˆW t
(cid:20)
i + O

(cid:21)
= ˆCt+1

1
√
n

(cid:18)

(cid:19)

i −

Xµi√
n

E

(cid:124)

(cid:21)

• gout(ωt

iµ, Yµ, V t
iµ)

gout(ωt

iµ, Yµ, V t

iµ) = gout

ωt
µ −

ˆW t

i→µ, Yµ, V t

µ −

= gout

µ, Yµ, V t
ωt
µ

−

Xµi√
n

∂gout
∂ω

µ, Yµ, V t
ωt
µ

= gout

µ, Yµ, V t
ωt
µ

−

µ, Yµ, V t
ωt
µ

ˆW t

Xµi√
n

∂gout
∂ω

X 2
µi
n

ˆCt

i→l

(cid:33)

+O

1
n

(cid:18)

(cid:19)

(cid:1)

= ˆW t

ˆW t

i→µ
(cid:16) 1√

i +O
(cid:124) (cid:123)(cid:122) (cid:125)
i + O

(cid:17)

n

1
n

(cid:18)

(cid:19)

(cid:1)

(cid:0)

(cid:0)

Xµi√
n

(cid:1)

(cid:1)

(cid:32)

(cid:0)

(cid:0)

V t
µ =

ˆCt

i→l =

ˆCt

i + O

X 2
µi
n

n

i=1
(cid:88)

X 2
µi
n

n

i=1
(cid:88)

1
n3/2

(cid:18)

(cid:19)

ωt
µ =

ˆW t

i→µ =

ˆW t

i − Xµi

Σt−1
i

−1 ˆCt

i Σt−1
i

gout(ωt−1

µ , Yµ, V t−1

µ

) + O

1
n

(cid:18)

(cid:19)(cid:19)

Σt−1
i

−1 ˆCt

(cid:0)
i Σt−1
i

(cid:1)
gout(ωt−1

µ , Yµ, V t−1

µ

) + O

1
n3/2

(cid:18)

(cid:19)

Xµi√
n

Xµi√
n

n

i=1
(cid:88)
n

i=1
(cid:88)

=

ˆW t

i −

n

Xµi√
n

(cid:18)

n

i=1
(cid:88)
X 2
µi
n

i=1
(cid:88)

(cid:0)

(cid:1)

43

−1

•

Σt
i

(cid:0)

(cid:1)
Σt
i

−1

=

(cid:0)

(cid:1)

• T t
i

m

µ=1
(cid:88)

m

µ=1
(cid:88)

At

µ→i = −

X 2

µi∂ωgout(ωt

iµ, Yµ, V t

iµ) = −

X 2

µi∂ωgout(ωt

µ, Yµ, V t

µ) + O

m

µ=1
(cid:88)

1
n3/2

(cid:18)

(cid:19)

Bt

µ→i

= Σt
i

m

Xµi√
n

µ=1
(cid:88)
µ, Yµ, V t
ωt
µ


gout

i = Σt
T t

m

i 

µ=1
(cid:88)
m


µ=1
(cid:88)
m

i 



µ=1
(cid:88)

Xµi√
n

(cid:18)

Xµi√
n

= Σt
i

= Σt

(cid:0)

(cid:0)

gout

µ, Yµ, V t
ωt
µ

−

gout(ωt

iµ, Yµ, V t
iµ)

−

Xµi√
n

∂gout
∂ω

X 2
µi
n

∂gout
∂ω

(cid:1)

(cid:1)

(cid:0)

(cid:0)

µ, Yµ, V t
ωt
µ

ˆW t

i + O

1
n

(cid:18)

(cid:19)(cid:19)

µ, Yµ, V t
ωt
µ

ˆW t

i 

+ O

1
n3/2

(cid:18)

(cid:19)

(cid:1)

(cid:1)



The AMP algorithm follows naturally the rBP updates (109) using the expanded estimates of the mean and

variance ωµ, Vµ, Ti and Σi, and finally reads in pseudo language:

Algorithm 2 Approximate Message Passing for the committee machine

Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: gout,µ = 0, Σi = IK×K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
Initialize: ˆWi ∈ RK and ˆCi, ∂ωgout,µ ∈ S +
repeat

K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 1.

n

ωt

ˆW t

µ =

i −

Xµi√
n

Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
X 2
µi
n

i Σt−1
i

−1 ˆCt

gt−1
out,µ

Σt−1
i

i=1
(cid:80)
out,µ = gout(ωt
gt

Update of gout,µ ∈ RK and ∂ωgout,µ ∈ S +
K:
out,µ = ∂ωgout(ωt

µ, Yµ, V t
µ)
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt

(cid:1)
∂ωgt

Xµi√

i = Σt
T t
i

X 2
n ∂ωgt
µi

out,µ −

ˆW t
i

out,µ

n gt

m

(cid:1)

(cid:0)

(cid:0)

|

|

(cid:16)

µ=1
(cid:80)

Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = fw(Σt

i, T t
i )

|

ˆCt+1

i = fc(Σt

i, T t
i )

(cid:17)

µ, Yµ, V t
µ)

i = −
(cid:16)

m

µ=1
(cid:80)

V t
µ =

X 2
µi
n

ˆCt
i

n

i=1
(cid:80)

X 2
n ∂ωgt
µi

out,µ

−1

(cid:17)

ˆW t+1
t = t + 1

until Convergence on ˆW , ˆC.
Output: ˆW and ˆC.

G State evolution equations from AMP

In this section, W (cid:63) denotes the ground truth weights of the teacher and we define the overlap parameters at
time t, mt, σt, qt, Q and that respectively measure the correlation of the AMP estimator with the ground truth,

44

its variance and the norms of student and teacher weights:



mt ≡ EW (cid:63) lim
n→∞

qt ≡ EW (cid:63) lim
n→∞


1
n

1
n

n

i=1
(cid:88)
n

i=1
(cid:88)

ˆW t

(cid:124)
i (W (cid:63)
i )

,

ˆW t

(cid:124)
i ( ˆW t
i )

,

and



σt ≡ EW (cid:63) lim
n→∞

Q ≡ EW (cid:63) lim
n→∞


n

ˆCt
i .

1
n

1
n

i=1
(cid:88)
n

i=1
(cid:88)

W (cid:63)

i (W (cid:63)
i )

(cid:124)

,

The aim is to derive the asymptotic behaviour of these overlap parameters, called state evolution. The idea is
to compute the overlap distributions starting with the relaxed BP equations eq. (109).

G.1 Messages distribution

In order to get the asymptotic behaviour of the overlap parameters, we need first to compute the distribution of
Σt
µ→i and T t
µ→i. Besides, we recall that in our model, the output has been generated by a teacher according to
n
1√
. We define zµ ≡ 1√
j(cid:54)=i XµjW (cid:63)
n W (cid:63)Xµ, A
Yµ = ϕ0
j .
And it useful to recall EX [Xµi] = 0 and EX [X 2
(cid:17)

n W (cid:63)Xµ = 1√
µi] = 1.

i and zµ→i ≡ 1√
n

n
i=1 XµiW (cid:63)

out

(cid:16)

n

(cid:80)

(cid:80)

µ→i

• ωt
Under belief propagation assumption messages are independent. ωt

µ→i is thus the sum of independent
variables and follows a gaussian distribution. Let’s compute the first two moments, using expansions of the
relaxed BP equations eq. (109):

EX

ωt
µ→i

=

EX [Xµj] ˆW t

j→µ = 0 ,

(cid:2)
µ→i(ωt
ωt

EX

(cid:124)
µ→i)

=

(cid:3)

(cid:3)

(cid:2)

• zµ

EX [XµjXµk] ˆW t

j→µ( ˆW t

(cid:124)
k→µ)

=

EX

X 2
µj

ˆWj→µ( ˆWj→µ)

(cid:124)

=

ˆW t

j→µ( ˆW t

j→µ)

(cid:124)

=

ˆW t

i ( ˆW t
i )

(cid:124)

+ O

1
n

n

i=1
(cid:88)

n

j(cid:54)=i
(cid:88)

(cid:3)

(cid:2)
1/n3/2

(cid:16)

(cid:17)

−→
n→∞

qt .

EX [zµ] =

1
√
n

EX [Xµi] W (cid:63)

i = 0 ,

EX,W (cid:63)

zµz

(cid:124)
µ

= EW (cid:63)

EX [XµjXµk] W (cid:63)

j (W (cid:63)
k )

(cid:124)

= EW (cid:63)

W (cid:63)

i (W (cid:63)
i )

(cid:124) −→
n→∞

Q .

1
√
n

n

j(cid:54)=i
(cid:88)
n

1
n

1
n

j(cid:54)=i,k(cid:54)=i
(cid:88)
n

j(cid:54)=i
(cid:88)

n

i=1
(cid:88)
1
n

n

j=1,k=1
(cid:88)

(cid:2)

(cid:3)

• zµ and ωt

µ→i

EX,W (cid:63)

ωt
µ→iz

(cid:124)
µ

= EW (cid:63)

EX [XµjXµk] ˆW t

j→µ(W (cid:63)
k )

(cid:124)

= EW (cid:63)

ˆW t

(cid:124)
j→µ(W (cid:63)
j )

1
n

1
n

n

j(cid:54)=i,k=1
(cid:88)
n

i=1
(cid:88)

(cid:2)

(cid:3)

= EW (cid:63)

ˆW t

(cid:124)
i (W (cid:63)
i )

+ O

1/n3/2

−→
n→∞

mt .

(cid:16)

(cid:17)

Hence asymptotically (zµ, ωt

µ→i) follow a Gaussian distribution with covariance matrix Qt =

Q mt
qt
mt
(cid:34)

.

(cid:35)

1
n

n

i=1
(cid:88)

1
n

n

j(cid:54)=i
(cid:88)

45

• Vµ→i

concentrates around its mean:

EX,W (cid:63)

V t
µ→i

= EW (cid:63)

EX

X 2
µj

ˆCt

j→µ = EW (cid:63)

ˆCt

j→µ = EW (cid:63)

ˆCt

i + O

1/n3/2

1
n

n

j(cid:54)=i
(cid:88)

1
n

n

i
(cid:88)

−→
n→∞

σt .

(cid:16)

(cid:17)

1
n

n

j(cid:54)=i
(cid:88)

(cid:3)

(cid:2)

(cid:3)
Let’s define other order parameters, that will appear in the following:

(cid:2)

ˆqt ≡ αEω,z,A
ˆmt ≡ αEω,z,A
ˆχt ≡ αEω,z,A

gout(ω, ϕ0
∂zgout(ω, ϕ0
(cid:2)
−∂ωgout(ω, ϕ0
(cid:2)

out(z, A), σt)

,
out(z, A), σt)
(cid:3)

out(z, A), σt)gout(ω, ϕ0

out(z, A), σt)(cid:124)

,

(cid:3)

.

(cid:3)





• T t

µ→i

can be expanded around zµ→i:

(cid:2)

Σt

µ→i

−1

T t
µ→i =

m





ν(cid:54)=µ
(cid:88)

Bt

ν→i

=



1
√
n

m

ν(cid:54)=µ
(cid:88)

Xνigout(ωt

ν→i, ϕ0

1
√
n

out 

XνjW (cid:63)

j + XνiW (cid:63)

i , A

, V t

ν→i)



n

j(cid:54)=i
(cid:88)

Xνigout(ωt

ν→i, ϕ0



out (zν→i, A) , V t

ν→i)

+


νi∂zgout(ωt

X 2


out (zν→i, A) , V t
ν→i)

ν→i, ϕ0

m

1
n









ν(cid:54)=µ
(cid:88)




W (cid:63)
i .





(cid:0)

=

(cid:1)
m





ν(cid:54)=µ
(cid:88)

1
√
n

• Σt

µ→i

At

ν→i = −

X 2

νi∂ωgout(ωt

ν→i, Yν, V t

ν→i)

m

Σt

µ→i

−1

=

(cid:0)

(cid:1)

m

1
n

ν(cid:54)=µ
(cid:88)

m

= −

ν(cid:54)=µ
(cid:88)
νi∂ωgout(ωt

X 2

1
n

ν(cid:54)=µ
(cid:88)

ν→i, ϕ0

out (zν→i, A) , V t

ν→i) + O

1/n3/2

.

Hence taking the average and the large size limit, the first moments of the variables Σt

(cid:16)

(cid:17)
µ→i and T t

µ→i read:

Eω,z,A,X

Eω,z,A,X

Eω,z,A,X

−1

(cid:17)

−1

(cid:17)

−1

Σt

µ→i

Σt

µ→i

Σt

µ→i

(cid:20)(cid:16)

(cid:20)(cid:16)

T t
µ→i

−→
n→∞

(cid:21)

ˆmtW (cid:63)
i ,
(cid:124)

T t
µ→i

T t
µ→i

Σt

µ→i

(cid:17)

(cid:16)

(cid:21)

(cid:17)

(cid:16)
−→
n→∞

ˆχt .

−1

−→
n→∞

ˆqt ,






And finally T t

µ→i ∼ ( ˆχt)−1

ˆmtW (cid:63)

with ξ ∼ N (0, 1) and

Σt

µ→i

−1

∼ ( ˆχt)−1 .

(cid:16)

(cid:17)

(cid:20)(cid:16)

(cid:21)

(cid:17)
i + (ˆqt)1/2ξ

(cid:0)

(cid:1)

G.2 State evolution equations - Non Bayes optimal case

Let’s define the following notations:

T t[W (cid:63), ξ] ≡ ( ˆχt)−1

ˆmtW (cid:63) + (ˆqt)1/2ξ

Σt ≡ ( ˆχt)−1

(cid:16)

(cid:17)

46

Gathering above results, the state evolution equations read:

ˆW t

i (W (cid:63)
i )

(cid:124)

= EW (cid:63),ξ

fw

Σt, T t[W (cid:63), ξ]

(cid:124)
(W (cid:63))

mt+1 = EW (cid:63) lim
n→∞

qt+1 = EW (cid:63) lim
n→∞

1
n

1
n

1
n

n

i=1
(cid:88)
n

i=1
(cid:88)
n

i=1
(cid:88)

ˆW t+1
i

( ˆW t+1
i

)

(cid:124)

= EW (cid:63),ξ

(cid:2)

(cid:0)
fw

(cid:1)

Σt, T t[W (cid:63), ξ]

fw

Σt, T t[W (cid:63), ξ]

(cid:3)

(cid:0)

(cid:124)

(cid:1)

(cid:3)

σt+1 = EW (cid:63) lim
n→∞

ˆCt+1
i = EW (cid:63),ξ

fc

Σt, T t[W (cid:63), ξ]

(cid:2)

(cid:0)

(cid:1)

(cid:2)

(cid:0)

(cid:1)(cid:3)

ˆqt = αEω,z,A

gout(ω, ϕ0

= α

dPA(A)
(cid:2)

out(z, A), σt)gout(ω, ϕ0
z, ω; 0, Qt

dzdωN

out(z, A), σt)(cid:124)

gout(ω, ϕ0

out(z, A), σt)gout(ω, ϕ0

out(z, A), σt)

(cid:3)

(cid:124)

= α

dPA(A)
(cid:2)

dzdωN

∂zgout(ω, ϕ0

out(z, A), σt)

ˆmt = αEω,z,A

∂zgout(ω, ϕ0

(cid:90)

(cid:90)

(cid:90)

(cid:90)

ˆχt = αEω,z,A

−∂ωgout(ω, ϕ0

= −α

dPA(A)

(cid:2)

dzdωN

(cid:90)

(cid:90)

(cid:0)

out(z, A), σt)
z, ω; 0, Qt
(cid:3)

(cid:1)

out(z, A), σt)
(cid:0)
(cid:1)
z, ω; 0, Qt
(cid:3)

(cid:0)

(cid:1)

∂ωgout(ω, ϕ0

out(z, A), σt)

G.3 State evolution equations - Bayes optimal case

and











In the bayes optimal case, the student knows all the parameters of the teacher and then P (cid:63)
mt = qt and ˆqt = ˆmt = ˆχt, σt = Q − qt and then, naturally

0 = P0, ϕ0

out = ϕout,

T t[W (cid:63), ξ] ≡ W (cid:63) + (ˆqt)−1/2ξ ,
Σt ≡ (ˆqt)−1 .

In the Bayes-optimal case, the set of state evolution equations reduces and simplifies to:

qt+1 = EW (cid:63),ξ

fw

Σt, T t[W (cid:63), ξ]

fw

Σt, T t[W (cid:63), ξ]

(cid:124)

,

ˆqt = αEω,z,A

(cid:2)
(cid:0)
gout(ω, ϕout(z, A), σt)gout(ω, ϕout(z, A), σt)(cid:124)

(cid:1)

(cid:1)

(cid:0)

(cid:3)

,

(111)

(cid:3)






(cid:0)

where (z, ω) ∼ Nz,ω

0, 0; Qt

with Qt =

(cid:2)

Q qt
qt
qt

.

(cid:35)

(cid:34)

(cid:1)

G.4 State evolution - Consistence between replicas and AMP - Bayes optimal case

State evolution - AMP

Using the change of variable ξ ← ξ +

1/2

ˆqt

W (cid:63), eq. (111) becomes:

qt+1 = Eξ

ZP0

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

(cid:0)

(cid:1)

(cid:104)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:124)

(cid:17)

(cid:105)

47

In addition in the Bayes-optimal case, as:

µ→i)(cid:124)

= mt − qt = 0

µ→i)(zµ − ωt

µ→i)(cid:124)

= Q − qt ,

µ→i)(cid:124)] = qt

(cid:105)



EX

ωt
µ→i(zµ − ωt
(cid:104)
EX [ωt

EX


µ→i(ωt
(cid:124)
µ − ωt

(z

(cid:104)

(cid:105)
0, 0; Qt

(cid:0)

(cid:1)

(cid:124)

the multivariate distribution can be written as a product: Nz,ω
using Pout(y|z) =

dPA(A)δ

y − ϕ0

, eq. (111) becomes:

out(z, A)

= Nω

0, qt

Nz

ω, Q − qt

. Hence,

(cid:0)

(cid:1)

(cid:0)

(cid:1)

ˆqt = αEω,z,A

(cid:2)

(cid:90)

(cid:82)
gout(ω, ϕ0
e− 1

(cid:0)

(cid:1)

out(z, A), Q − qt)

out(z, A), Q − qt)gout(ω, ϕ0
2 ω(cid:124)(qt)−1ω
(2π)K/2 det(qt)1/2
e− 1
(2π)K/2 det(Q − qt)1/2 gout((qt)1/2ξ, y, Q − qt)gout((qt)1/2ξ, y, Q − qt)

e− 1
(2π)K/2 det(Q − qt)1/2 gout(ω, y, Q − qt)gout(ω, y, Q − qt)

(cid:90)
2 (z−ω)(cid:124)(Q−qt)−1(z−ω)

2 (z−ω)(cid:124)(Q−qt)−1(z−ω)

dzPout(y|z)

dzPout(y|z)

(cid:3)

(cid:124)

(cid:124)

= α

dy

dω

(cid:90)

= α

dy

Dξ

(cid:90)
= αEy,ξ

(cid:90)
ZPout

(cid:90)
(qt)1/2ξ, y, Q − qt

gout

(qt)1/2ξ, y, Q − qt

gout

(qt)1/2ξ, y, Q − qt

(cid:16)
(cid:17)
Finally to summarize the state evolution equations can be written as:

(cid:16)

(cid:16)

(cid:17)

(cid:104)

(cid:124)

(cid:17)

(cid:105)

(cid:124)

qt+1 = Eξ

ZP0

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

fw

(ˆqt)1/2ξ, (ˆqt)−1

(cid:104)
ˆqt = αEy,ξ

(cid:16)
ZPout

(qt)1/2ξ, y, Q − qt

gout

(cid:17)

(cid:16)

(qt)1/2ξ, y, Q − qt

(cid:17)

(cid:16)

gout

(cid:105)
(qt)1/2ξ, y, Q − qt

(cid:17)

(cid:124)

(112)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:105)

(cid:1)






State evolution - Replicas

(cid:104)

(cid:0)

Recall from sec. B, the free entropy eq. (76) reads

limn→∞ fn = extrq,ˆq

− 1

2 Tr[q ˆq] + IP + αIC

,

≡ Eξ

ZP0(ˆq1/2ξ, ˆq−1) log(ZP0(ˆq1/2ξ, ˆq−1))

(cid:9)

(cid:8)

,

(cid:2)
≡ Eξ,y

ZPout(q1/2ξ, y, Q − q) log(ZPout(q1/2ξ, y, Q − q))

(cid:3)

.

IP

IC






Taking the derivatives with respect to q and ˆq, using an integration by part and the following identities:

(cid:2)

(cid:3)

∂ZPout

∂q = − 1

2 q−1e

1

2 ξ(cid:124)ξ∂ξ

∂ZP0
∂ ˆq = − 1

2 ˆq−1e

1

2 ξ(cid:124)ξ∂ξ

e− 1
(cid:104)
e− 1

2 ξ(cid:124)ξ∂ξZPout
2 ξ(cid:124)ξ∂ξZP0

,

(cid:105)

,

(cid:104)

(cid:105)






the state evolution equations read:

q = 2 ∂IP
∂ ˆq




ˆq = 2α ∂IC
∂q

with

2

∂IP
∂ ˆq = 1
∂IC
∂q = 1

2




Eξ

(cid:2)
Ey,ξ

ZP0(ˆq1/2ξ, ˆq−1)fw(ˆq1/2ξ, ˆq)fw(ˆq1/2ξ, ˆq−1)(cid:124)
ZPout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)(cid:124)

(cid:3)

(cid:3)

that simplifies and allows to recover the state evolutions equations directly derived from AMP eq. (112), but



(cid:2)

48

without time indices

q = Eξ

ZP0(ˆq1/2ξ, ˆq−1)fw(ˆq1/2ξ, ˆq)fw(ˆq1/2ξ, ˆq−1)(cid:124)

,




(cid:2)
ˆq = αEy,ξ

ZPout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)gout(q1/2ξ, y, Q − q)(cid:124)

(cid:3)

.

(cid:3)

(cid:2)
H Parity machine for K = 2



Although we mainly focused on the committee machine, another classical two-layers neural network is the
parity machine [7] and our proof applies to this case as well. While learning is known to be computationally
hard for general K, the case K = 2 is special, and in fact can be reformulated as a committee machine, where
the sign activation function has been replaced by ϕ1(z) = 1(z (cid:54)= 0) − 1(z = 0):

K

n

K

n

Yµ = sign

sign

XµiW ∗
il

= ϕ1

sign

XµiW ∗
il

.

(113)

(cid:104)

l=1
(cid:89)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

(cid:104)

l=1
(cid:88)

(cid:16)

i=1
(cid:88)

(cid:17)(cid:105)

We have repeated our analysis for the K = 2 parity machine and the phase diagram is summarized in
Fig. 5 where we show the generalization error and the elements of the overlap matrix for Gaussian (left) and
binary weights (right), with the results of the AMP algorithm (points).

Below the specialization phase transition α < αspec, the symmetry of the output imposes the non-
specialized fixed point q00 = q01 = 0 to be the only solution, with αG
spec(K = 2) (cid:39)
2.49. Above the specialization transition αspec, the overlap becomes specialized with a non-trivial diagonal
term.

spec(K = 2) (cid:39) 2.48 and αB

Additionally, in the binary case, an information theoretical transition towards a perfect learning occurs
at αB
IT(K = 2) (cid:39) 2.00, meaning that the perfect generalization fixed point (q00 = 1, q01 = 0) becomes the
global optimizer of the free entropy. It leads to a first order phase transition of the AMP algorithm which
retrieves the perfect generalization phase only at αB
perf (K = 2) (cid:39) 3.03. This is similar to what happens in
single layer neural networks for the symmetric door activation function, see [11]. Again, these results for the
parity machine emphasize a gap between information-theoretical and computational performance.

49

Figure 5: Similar plot as in Fig. 2 but for the parity machine with two hidden neurons. Value of the order parameter
and the optimal generalization error for a parity machine with two hidden neurons with Gaussian weights (left)
and binary/Rademacher weights (right). SE and AMP overlaps are respectively represented in full line and points.

50

