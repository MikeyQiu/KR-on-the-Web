9
1
0
2
 
r
p
A
 
3
2
 
 
]

G
L
.
s
c
[
 
 
3
v
5
1
9
6
0
.
4
0
9
1
:
v
i
X
r
a

Published as a workshop paper at ICLR 2019

GRAPHTSNE: A VISUALIZATION TECHNIQUE FOR
GRAPH-STRUCTURED DATA

Yao Yang Leow
School of Computer Science and Engineering
Nanyang Technological University, Singapore
yleow002@e.ntu.edu.sg

Thomas Laurent
Department of Mathematics
Loyola Marymount University
tlaurent@lmu.edu

Xavier Bresson
School of Computer Science and Engineering
Nanyang Technological University, Singapore
xbresson@ntu.edu.sg

ABSTRACT

We present GraphTSNE, a novel visualization technique for graph-structured data
based on t-SNE. The growing interest in graph-structured data increases the im-
portance of gaining human insight into such datasets by means of visualization.
Among the most popular visualization techniques, classical t-SNE is not suitable
on such datasets because it has no mechanism to make use of information from
the graph structure. On the other hand, visualization techniques which operate on
graphs, such as Laplacian Eigenmaps and tsNET, have no mechanism to make use
of information from node features. Our proposed method GraphTSNE produces
visualizations which account for both graph structure and node features. It is based
on scalable and unsupervised training of a graph convolutional network on a mod-
iﬁed t-SNE loss. By assembling a suite of evaluation metrics, we demonstrate that
our method produces desirable visualizations on three benchmark datasets.1

1

INTRODUCTION

Visualization of high-dimensional data has become common practice following the success of di-
mensionality reduction techniques such as Laplacian Eigenmaps (Belkin & Niyogi, 2001), t-SNE
(van der Maaten & Hinton, 2008), tsNET (Kruiger et al., 2017), and UMAP (McInnes et al., 2018).
In contrast to the more general problem of dimensionality reduction, visualizations can be particu-
larly useful as a tool to explore and hypothesize about given data.

However, to the best of our knowledge, such techniques have not been extended to handle high-
dimensional data lying within an explicit graph structure. This limitation results in poor performance
on graph-structured datasets as depicted in Figure 1. Our proposed method GraphTSNE is based on
extending the highly successful method of t-SNE to produce visualizations which account for both
graph structure and node features.
In particular, we shall consider datasets with two sources of
information: graph connectivity between nodes and node features. Examples of graph-structured
datasets include social networks, functional brain networks and gene-regulatory networks.

=

= (

N
i=1, and a set
Formally, we consider a graph
vi}
{
of edges
In addition, each
V
E
node is associated with a n-dimensional feature vector, given by a set of high-dimensional points
X =
N
xi ∈
to a set of low-
i=1. The goal of visualization is to map the set of nodes
V
}
{
dimensional points Y =
.
2, 3
}

), consisting of a set of nodes
V
V
that maps relations between nodes in

(vi, vj)
{
Rn

n and typically m

N
i=1, where m

} ⊆ V × V

yi ∈
{

=
.

∈ {

Rm

(cid:28)

G

E

}

,

1Our implementation is available at: https://github.com/leowyy/GraphTSNE

1

Published as a workshop paper at ICLR 2019

Figure 1: Three different visualizations of the CORA citation network. Compared to t-SNE (left)
and Laplacian Eigenmaps (right), our proposed method GraphTSNE (middle) is able to produce
visualizations which account for both graph structure and node features.

2 RELATED WORK

t-SNE. t-SNE (van der Maaten & Hinton, 2008) operates by deﬁning pairwise joint probabilities
pij of picking a point-pair in high-dimensional space and probabilities qij of picking a point-pair
in the low-dimensional map. The probabilities pij of picking the pair (xi, xj) are parametrized
by a normalized Gaussian distribution with mean given by the corresponding entry Di,j of a pair-
wise distance matrix D. The pairwise distances Di,j are computed by a distance measure in high-
2
dimensional space, typically the squared Euclidean distance, i.e. Di,j =
2. Whereas the
probabilities qij of picking the pair (yi, yj) are parametrized by a normalized Student’s t-distribution
2
yj||
with mean given by the squared Euclidean distance between low-dimensional data points
2.
The objective of t-SNE is to ﬁnd a low-dimensional data representation Y that minimizes the mis-
match between the two probability distributions pij and qij, given by the Kullback-Leiber (KL)
divergence CKL = (cid:80)

. The t-SNE loss is minimized via gradient descent on ∂CKL
∂yi

j pij log pij

xi −
||

yi−
||

xj||

(cid:80)

qij

.

i

Application of t-SNE on graphs. In the domain of graph layouts, force-directed graph drawing
(Fruchterman & Reingold, 1991) considers a node placement such that the Euclidean distance be-
tween any pair of nodes in the layout is proportional to their pairwise graph-theoretic distances.
Theoretical links between dimensionality reduction and graph layouts have been proposed by Yang
et al. (2014). Based on this observation, Kruiger et al. (2017) have proposed a graph layout algo-
rithm tsNET which operates similar to t-SNE, except the input distance matrix D is computed using
the pairwise graph-theoretic shortest-path distances between nodes, i.e. Di,j = δ
(i, j). However,
similar to other graph layout algorithms (von Landesberger et al., 2011), tsNET only takes the graph
structure

as input and therefore, cannot incorporate information provided by node features X.

G

G

3 METHOD

Our method relies on two modiﬁcations to a parametric version of t-SNE proposed by van der
Maaten (2009). First, we use a graph convolutional network (GCN) (Sukhbaatar et al., 2016; Kipf
& Welling, 2016; Hamilton et al., 2017) as the parametric model for the non-linear mapping between
the high-dimensional data space and the low-dimensional embedding space, i.e. Y = fGCN(
, X).
In our present work, we use a two-layer residual gated GCN of Bresson & Laurent (2018) with the
following layer-wise propagation model:

G

hl+1

i = ReLU

(cid:16)

U lhl

i + 1
n(i)
|

|

(cid:17)

V lhl
j

+ hl
i ,

(cid:88)

j

i

→

ηij (cid:12)

(1)

i denotes the latent representation of node vi at layer l, with h0

j) denotes edge gates between the node pair (vi, vj), with σ(
·

where hl
i +
Blhl
n(i)
|
denotes the indegree of node vi. The learnable parameters of the model are given by Al, Bl, U l, V l.
Second, we train our GCN using a modiﬁed t-SNE loss composed of two sub-losses: a graph clus-
and a feature clustering loss CX . Both sub-losses follow the same formulation of
tering loss C

i = xi. ηij = σ(Alhl

) as the sigmoid function.

|

G

2

Published as a workshop paper at ICLR 2019

the classical t-SNE loss, but differ in the choice of distance metric used to compute the input pair-
wise distance matrix D between point-pairs. First, the graph clustering loss C
takes as input the
G
pairwise graph-theoretic shortest-path distances, i.e. DGi,j = δ
(i, j). Second, the feature clustering
loss CX takes as input a suitable distance measure between pairs of node features, e.g. the squared
2
Euclidean distance DX
2. Lastly, we explore the tradeoff between the two sub-losses
i,j =
by taking a weighted combination: CT = αC
α)CX . The composite loss CT relies on
G
two hyperparameters: the weight of the graph clustering loss α and a perplexity parameter asso-
ciated with the classical t-SNE loss which measures the effective number of neighbors of a point
(Wattenberg et al., 2016). The perplexity parameter is set to a default value of 30 in all experiments.

xi −
||

xj||

+ (1

−

G

4 EXPERIMENTS

We evaluate our method by producing visualizations of three benchmark citation networks - CORA,
CITESEER and PUBMED (Sen et al., 2008) - where nodes are documents, each associated with a
high-dimensional word occurrence feature vector, and edges are citation links. Summary statistics
for the benchmark datasets are presented in Table 1. Full details of our experimental setup are
provided in Appendix A.

Table 1: Datasets statistics

Type
Dataset
CORA
Citation
CITESEER Citation
Citation
PUBMED

Nodes
2,708
3,337
19,717

Edges Classes Features
1,433
5,429
3,703
4,732
500
44,328

7
6
3

4.1 EVALUATION METRICS

Following the literature on dimensionality reduction, we adopt metrics based on how well local
neighborhoods are preserved in the low-dimensional map. We proceed with these deﬁnitions:

Graph neighborhood: S
(vi, r) =
G
Feature neighborhood: SX (vi, k) =
Embedding neighborhood: SY (vi, k) =

r

;

vj ∈ V |
{
vj ∈ V |
{
vj ∈ V |
{

δ
(i, j)
G
xj ∈
yj ∈

}

≤
k nearest neighbors of xi}
k nearest neighbors of yi}

;
.

(2)
(3)
(4)

(r), adapted
As a graph neighborhood preservation metric, we adopt the graph trustworthiness T
from Martins et al. (2015), which expresses the extent to which graph neighborhoods S
G
are retained
in the embedding neighborhoods SY . As a feature neighborhood preservation metric, we adopt the
G
feature trustworthiness TX (k), proposed by Venna & Kaski (2006), which expresses the extent to
which feature neighborhoods SX are retained in the embedding neighborhoods SY . Complete details
of the trustworthiness measures are provided in Appendix B.

Next, we introduce two distance-based evaluation metrics speciﬁc to the goal of visualization. To
as the k-nearest neighbors graph computed by pairwise distances in the feature
begin, denote
space for a chosen value of k. In our visualizations, a point-pair (yi, yj) would ideally be placed
close together in the low-dimensional map if they are either connected by an edge in the graph, i.e.
. Hence, a reasonable objective in visualization is
(i, j)
∈ K
to minimize the graph-based distance PG and the feature-based distance PX deﬁned as follows:

, or have similar features, i.e. (i, j)

∈ E

K

=

P

G

1

(cid:88)

|E|

(i,j)

∈E

yi −
||

yj||

2
2 ;

1

(cid:88)

PX =

|K|

(i,j)

∈K

yi −
||

yj||

2
2 .

(5, 6)

Lastly, we also report the generalization accuracy of 1-nearest neighbor (1-NN) classiﬁers which are
trained on the set of low-dimensional points Y obtained from the visualization and the underlying
class labels, following the method used by van der Maaten (2009).

3

Published as a workshop paper at ICLR 2019

4.2 RESULTS

Quantitative assessment. In Figure 2, we vary the weight of the graph clustering loss α
[0, 1]
and report the performance of the resulting visualizations. As α varies, GraphTSNE tradeoffs be-
tween graph and feature trustworthiness. Based only on the trustworthiness measures, it is hard to
determine the optimal value of α, which we denote as α∗, since the measures have incomparable
scales. Instead, we suggest setting α∗ to the value of α with the best performance on the combined
distance metric (P
+ PX ) or 1-NN generalization accuracy (if applicable). Since we do not assume
the presence of class labels during training, we use the former strategy to select α∗.

∈

G

By incorporating both graph connectivity and node features, the visualizations produced with in-
termediate values of α achieve better separation between classes and therefore, higher 1-NN gen-
eralization accuracy. This phenomenon has been well-studied in the context of semi-supervised
classiﬁcation (Yang et al., 2016; Kipf & Welling, 2016; Veliˇckovi´c et al., 2018) and unsupervised
representation learning (Hamilton et al., 2017; Veliˇckovi´c et al., 2019).

Figure 2: Performance evaluation of different benchmarks on varying the weight of the graph clus-
[0, 1], denoted by the x axis. The red vertical line denotes α∗, the value of α that
tering loss α
+ PX ) on each dataset.
minimizes the combined distance metric (P

∈

G

Qualitative assessment. In Figure 3, we provide a visual comparison of the citation networks under
three settings of α. At α = 0, our method reverts to pure feature clustering as in classical t-SNE.
This results in long edges that crowd the layout and poorly reﬂect the overall graph structure. At
α = 1, our method performs pure graph clustering, similar to tsNET (Kruiger et al., 2017). This
creates many tight clusters representing graph cliques, while arbitrarily placing disconnected nodes
in the low-dimensional map. At the proposed value of α∗, GraphTSNE visualizations are able to
accurately reﬂect the overall graph structure and achieve better separation between classes.

O

(N 2) time-complexity due to the compu-
Training time. Similar to t-SNE, GraphTSNE runs in
tation of the input distance matrices DX and DG. To lower the cost of this preprocessing step,
we use the neighbor subsampling (NS) approach of Hamilton et al. (2017) to train our GCNs using
stochastic mini-batch gradient descent. After selecting a random mini-batch of nodes
VB, NS iter-
atively expands the mini-batch by randomly choosing d(l) neighbors for each node at layer l. For
our two-layer GCNs, we set d = [10, 15], yielding a receptive ﬁeld size of 150 per node. After the
preprocessing step, a potential speed-up, which is not in our current implementation, would be to
compute the gradients in
(N log N ) time-complexity using tree-based approximation algorithms
(van der Maaten, 2014). In future work, we will explore GraphTSNE as an inductive visualization
technique that will enable its use in larger and time-evolving graph datasets.

O

4

Published as a workshop paper at ICLR 2019

Figure 3: Comparison of visualization techniques on benchmark datasets. Colors denote document
class which are not provided during training. GraphTSNE visualizations are produced with α = α∗.

ACKNOWLEDGMENTS

Xavier Bresson is supported by NRF Fellowship NRFF2017-10.

5

Published as a workshop paper at ICLR 2019

REFERENCES

Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and

clustering. In Advances in neural information processing systems (NIPS), 2001.

Xavier Bresson and Thomas Laurent. An experimental study of neural networks for variable graphs.

In International Conference on Learning Representations (ICLR), 2018.

Thomas M. J. Fruchterman and Edward M. Reingold. Graph drawing by force-directed placement.

Software–Practice and Experience, 21:1129–1164, 1991.

Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In AISTATS, volume 9, pp. 249–256, 2010.

William L. Hamilton, Rex Ying, and Jure Leskovec.

Inductive representation learning on large

graphs. In Advances in neural information processing systems (NIPS), 2017.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015.

Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Interna-

tional Conference on Learning Representations (ICLR), 2015.

Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-

works. In International Conference on Learning Representations (ICLR), 2016.

Johannes Kruiger, Paulo E. Rauber, Rafael M. Martins, Andreas Kerren, Stephen Kobourov, and
Alexandru C. Telea. Graph layouts by t-sne. Eurographics Conference on Visualization (EuroVis),
2017.

Rafael M. Martins, Rosane Minghim, and Alexandru C. Telea. Explaining neighborhood preserva-
tion for multidimensional projections. In Proceedings of Computer Graphics and Visual Comput-
ing (CGVC), 2015.

Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger. Umap: Uniform manifold

approximation and projection. The Journal of Open Source Software, 3(29):861, 2018.

Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.

Collective classiﬁcation in network data. AI magazine, 29(3):93, 2008.

Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with

backpropagation. In Advances in neural information processing systems (NIPS), 2016.

Laurens van der Maaten. Learning a parametric embedding by preserving local structure. Journal

of Machine Learning Research (JMLR), 5:348–391, 2009.

Laurens van der Maaten. Accelerating t-sne using tree-based algorithms. Journal of Machine Learn-

ing Research (JMLR), 15:3221–3245, 2014.

Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine

Learning Research (JMLR), 9:2579–2605, 2008.

Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
In International Conference on Learning Representations

Bengio. Graph attention networks.
(ICLR), 2018.

Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In International Conference on Learning Representations (ICLR),
2019.

Jarkko Venna and Samuel Kaski. Visualizing gene interaction graphs with local multidimensional
In Proceedings of the 14th European Symposium on Artiﬁcial Neural Networks, pp.

scaling.
557–562, 2006.

6

Published as a workshop paper at ICLR 2019

Tatiana von Landesberger, Arjan Kuijper, Tobias Schreck, J¨orn Kohlhammer, Jarke van Wijk, Jean-
Daniel Fekete, and Dieter Fellner. Visual analysis of large graphs: State-of-the-art and future
research challenges. Computer Graphics Forum, 30(6):1719–1749, 2011.

Martin Wattenberg, Fernanda Vigas, and Ian Johnson. How to use t-sne effectively. Distill, 2016.

doi: 10.23915/distill.00002. URL http://distill.pub/2016/misread-tsne.

Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning

with graph embeddings. In International Conference on Machine Learning (ICML), 2016.

Zhirong Yang, Jaakko Peltonen, and Samuel Kaski. Optimization equivalence of divergences im-
proves neighbor embedding. In International Conference on Machine Learning (ICML), 2014.

A TRAINING HYPERPARAMETERS

V

| ≤

10000) and larger datasets (

Our method uses a residual gated graph convolutional network (GCN) (Bresson & Laurent, 2018)
with two graph convolutional layers. We consider different sets of hyperparameters for small
> 10000). For small datasets, we train our nets
datasets (
|
|
with 128 hidden units per layer for 360 epochs using full-batch gradient descent. For larger datasets,
we train our nets with 256 hidden units per layer for 5 epochs using mini-batch gradient descent. At
each epoch, we randomly partition the dataset into 1000 batches which are expanded with neighbor
subsampling. We initialize network weights with Xavier initialisation (Glorot & Bengio, 2010) and
use batch normalization (Ioffe & Szegedy, 2015). We train using Adam (Kingma & Ba, 2015) with
a learning rate of 0.00075. The learning rate scheduler has a decay factor of 1.25. Finally, we set
the perplexity associated with the t-SNE loss to a default value of 30 for all experiments.

V

|

B DETAILS OF TRUSTWORTHINESS MEASURES

Recall the following local neighborhood deﬁnitions from Section 4.1:

Graph neighborhood: S
(vi, r) =
G
Feature neighborhood: SX (vi, k) =
Embedding neighborhood: SY (vi, k) =

r

;

vj ∈ V |
{
vj ∈ V |
{
vj ∈ V |
{

δ
(i, j)
G
xj ∈
yj ∈

}

≤
k nearest neighbors of xi}
k nearest neighbors of yi}

;
.

The feature trustworthiness (Venna & Kaski, 2006) expresses the extent to which feature neighbor-
hoods SX are retained in the embedding neighborhoods SY . The measure is deﬁned as:

TX (k) = 1

−

N k(2N

3k

1)

−

i=1

j

U(vi,k)

∈

N
(cid:88)

(cid:88)

(r(i, j)

k) ,

−

2

−

where r(i, j) denotes the rank of the low-dimensional point yj according to the pairwise distances
from a given reference point yi. U(vi, k) denotes the set of points in the embedding neighborhood
of vi but not in its feature neighborhood, i.e. U(vi, k) = SY (vi, k)
The graph trustworthiness, adapted from Martins et al. (2015), computes the Jaccard similarity be-
tween graph neighborhoods S
and embedding neighborhoods SY . Given a node vi and a ﬁxed
G
value of r, the graph neighborhood is deﬁned by the r-hop neighborhood of node vi. For each node,
set k =

SX (vi, k).
\

. The graph neighborhood measure is deﬁned as:
|

SG(vi, r)

|

(2)
(3)
(4)

(7)

(8)

(r) =

T
G

1

N
(cid:88)

|V|

i=1

S
|
G
S
|
G

(vi, r)
(vi, r)

SY (vi, k)
|
SY (vi, k)
|

∩
∪

.

7

