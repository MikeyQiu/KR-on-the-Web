8
1
0
2
 
l
u
J
 
2
 
 
]

G
L
.
s
c
[
 
 
4
v
7
9
1
9
0
.
6
0
6
1
:
v
i
X
r
a

Model-Free Trajectory-based Policy Optimization

Model-Free Trajectory-based Policy Optimization with
Monotonic Improvement

Riad Akrour1
riad@robot-learning.de
Abbas Abdolmaleki2
abbas.a@ua.pt
Hany Abdulsamad1
hany@robot-learning.de
Jan Peters1,3
jan@robot-learning.de
Gerhard Neumann1,4
geri@robot-learning.de
1CLAS/IAS, Technische Universit¨ate Darmstadt, Hochschulstr. 10, D-64289 Darmstadt, Germany
2DeepMind, London N1C 4AG, UK
3Max Planck Institute for Intelligent Systems, Max-Planck-Ring 4, T¨ubingen, Germany
4L-CAS, University of Lincoln, Lincoln LN6 7TS, UK

Editor: -

Abstract

Many of the recent trajectory optimization algorithms alternate between linear approxi-
mation of the system dynamics around the mean trajectory and conservative policy up-
date. One way of constraining the policy change is by bounding the Kullback-Leibler (KL)
divergence between successive policies. These approaches already demonstrated great ex-
perimental success in challenging problems such as end-to-end control of physical systems.
However, the linear approximation of the system dynamics can introduce a bias in the
policy update and prevent convergence to the optimal policy. In this article, we propose
a new model-free trajectory-based policy optimization algorithm with guaranteed mono-
tonic improvement. The algorithm backpropagates a local, quadratic and time-dependent
Q-Function learned from trajectory data instead of a model of the system dynamics. Our
policy update ensures exact KL-constraint satisfaction without simplifying assumptions on
the system dynamics. We experimentally demonstrate on highly non-linear control tasks
the improvement in performance of our algorithm in comparison to approaches linearizing
the system dynamics. In order to show the monotonic improvement of our algorithm, we
additionally conduct a theoretical analysis of our policy update scheme to derive a lower
bound of the change in policy return between successive iterations.

Reinforcement Learning, Policy Optimization, Trajectory Optimization,

Keywords:
Robotics

1. Introduction

Trajectory Optimization methods based on stochastic optimal control (Todorov, 2006;
Theodorou et al., 2009; Todorov and Tassa, 2009) have been very successful in learning
high dimensional controls in complex settings such as end-to-end control of physical sys-
tems (Levine and Abbeel, 2014). These methods are based on a time-dependent lineariza-
tion of the dynamics model around the mean trajectory in order to obtain a closed form
update of the policy as a Linear-Quadratic Regulator (LQR). This linearization is then
repeated locally for the new policy at every iteration. However, this iterative process does

1

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

not oﬀer convergence guarantees as the linearization of the dynamics might introduce a
bias and impede the algorithm from converging to the optimal policy. To circumvent this
limitation, we propose in this paper a novel model-free trajectory-based policy optimiza-
tion algorithm (MOTO) couched in the approximate policy iteration framework. At each
iteration, a Q-Function is estimated locally around the current trajectory distribution us-
ing a time-dependent quadratic function. Afterwards, the policy is updated according to a
new information-theoretic trust region that bounds the KL-divergence between successive
policies in closed form.

MOTO is well suited for high dimensional continuous state and action spaces control
problems. The policy is represented by a time-dependent stochastic linear-feedback con-
troller which is updated by a Q-Function propagated backward in time. We extend the
work of (Abdolmaleki et al., 2015), which was proposed in the domain of stochastic search
(having no notion of state space nor that of sequential decisions), to that of sequential deci-
sion making and show that our policy class can be updated under a KL-constraint in closed
form, when the learned Q-Function is a quadratic function of the state and action space.
In order to maximize sample eﬃciency, we rely on importance sampling to reuse transi-
tion samples from policies of all time-steps and all previous iterations in a principled way.
MOTO is able to solve complex control problems despite the simplicity of the Q-Function
thanks to two key properties: i) the learned Q-Function is ﬁtted to samples of the current
policy, which ensures that the function is valid locally and ii) the closed form update of
the policy ensures that the KL-constraint is satisﬁed exactly irrespective of the number of
samples or the non-linearity of the dynamics, which ensures that the Q-Function is used
locally.

The experimental section demonstrates that on tasks with highly non-linear dynamics
MOTO outperforms similar methods that rely on a linearization of these dynamics. Addi-
tionally, it is shown on a simulated Robot Table Tennis Task that MOTO is able to scale
to high dimensional tasks while keeping the sample complexity relatively low; amenable to
a direct application to a physical system.

In addition to the experimental validation previously reported in Akrour et al. (2016),
we conduct a theoretical analysis of the policy update in Sec. 5 and lower bound the increase
in policy return between successive iterations of the algorithm. The resulting lower bound
validates the use of an expected KL-constraint (Sec. 3.1) in a trajectory-based policy
optimization setting for ensuring a monotonic improvement of the policy return. Prior
theoretical studies reported similar results when the maximum (over the state space) KL
is upper bounded which is hard to enforce in practice (Schulman et al., 2015). Leveraging
standard trajectory optimization assumptions, we are able to extend the results when only
the expected KL under the state distribution of the previous policy is bounded.

2. Notation

Consider an undiscounted ﬁnite-horizon Markov Decision Process (MDP) of horizon T with
state space S = Rds and action space A = Rda. The transition function p(st+1|st, at),
which gives the probability (density) of transitioning to state st+1 upon the execution of
action at in st, is assumed to be time-independent; while there are T time-dependent
reward functions rt : S × A (cid:55)→ R. A policy π is deﬁned by a set of time-dependent density

2

Model-Free Trajectory-based Policy Optimization

functions πt, where πt(a|s) is the probability of executing action a in state s at time-
step t. The goal is to ﬁnd the optimal policy π∗ = {π∗
T } maximizing the policy
(cid:105)
return J(π) = IEs1,a1,...
, where the expectation is taken w.r.t. all the
random variables st and at such that s1 ∼ ρ1 follows the distribution of the initial state,
at ∼ πt(.|st) and st+1 ∼ p(st+1|st, at).

t=1 rt(st, at)

1, . . . , π∗

(cid:104)(cid:80)T

As is common in Policy Search (Deisenroth et al., 2013), our algorithm operates on a
restricted class of parameterized policies πθ, θ ∈ Rdθ and is an iterative algorithm comprising
two main steps, policy evaluation and policy update. Throughout this article, we will assume
that each time-dependent policy is parameterized by θt = {Kt, kt, Σt} such that πθt is of
linear-Gaussian form πθt(a|s) = N (Kts + kt, Σt), where the gain matrix Kt is a da × ds
matrix, the bias term kt is a da dimensional column vector and the covariance matrix Σt,
which controls the exploration of the policy, is of dimension da × da; yielding a total number
of parameters across all time-steps of dθ = T (dads + 1

2 da(da + 3)).

The policy at iteration i of the algorithm is denoted by πi and following standard deﬁni-
(cid:105)
tions, the Q-Function of πi at time-step t is given by Qi
t(cid:48)=t rt(cid:48)(st(cid:48), at(cid:48))
t(s, a) = IEst,at,...
with (st, at) = (s, a) and at(cid:48) ∼ πi
t(cid:48)(.|st(cid:48)), ∀t(cid:48) > t. While the V-Function is given by
V i
t(s, a) − V i
t (s, a)] and the Advantage Function by Ai
t (s) = IEa∼πt(.|s) [Qπ
t (s).
Furthermore the state distribution at time-step t, related to policy πi, is denoted by ρi
t(s).
In order to keep the notations uncluttered, the time-step or the iteration number is occa-
sionally dropped when a deﬁnition applies similarly for all time-steps or iteration number.

t(s, a) = Qi

(cid:104)(cid:80)T

3. Model-free Policy Update for Trajectory-based Policy Optimization

MOTO alternates between policy evaluation and policy update. At each iteration i, the
policy evaluation step generates a set of M rollouts1 from the policy πi in order to estimate
a (quadratic) Q-Function ˜Qi (Sec. 4.1) and a (Gaussian) state distribution ˜ρi (Sec. 4.3).
Using these quantities, an information-theoretic policy update is derived at each time-step
that uses a KL-bound as a trust region to obtain the policy πi+1 of the next iteration.

3.1 Optimization Problem

The goal of the policy update is to return a new policy πi+1 that maximizes the Q-Function
˜Qi in expectation under the state distribution ˜pi of the previous policy πi. In order to limit
policy oscillation between iterations (Wagner, 2011), the KL w.r.t. πi is upper bounded.
The use of the KL divergence to deﬁne the step-size of the policy update has already been
successfully applied in prior work (Peters et al., 2010; Levine and Abbeel, 2014; Schulman
et al., 2015). Additionally, we lower bound the entropy of πi+1 in order to better control

1. A rollout is a Monte Carlo simulation of a trajectory according to ρ1, π and p or the execution of π on

a physical system.

3

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

the reduction of exploration yielding the following non-linear program:

maximize
π

subject to

(cid:90) (cid:90)

t(s)π(a|s) ˜Qi
˜ρi
(cid:2)KL(π(.|s) (cid:107) πi

t(s)
t(s) [H (π(.|s))] ≥ β.

IEs∼˜ρi
IEs∼˜ρi

t(s, a)dads,

t(.|s))(cid:3) ≤ (cid:15),

(1)

(2)

(3)

The KL between two distributions p and q is given by KL(p (cid:107) q) = (cid:82) p(x) log p(x)
q(x) dx while
the entropy H is given by H = − (cid:82) p(x) log p(x)dx. The step-size (cid:15) is a hyper-parameter of
the algorithm kept constant throughout the iterations while β is set according to the entropy
t(.|s)(cid:1)(cid:3) − β0 and β0 is the entropy reduction
of the current policy πi
t, β = IEs∼˜ρi
hyper-parameter kept constant throughout the iterations.

(cid:2)H (cid:0)πi

t(s)

t maximizes ˜Qi

Eq. (1) indicates that πi+1
t in expectation under its own action distribu-
tion and the state distribution of πi
t. Eq. (2) bounds the average change in the policy to
the step-size (cid:15) while Eq. (3) controls the exploration-exploitation trade-oﬀ and ensures that
the exploration in the action space (which is directly linked to the entropy of the policy) is
not reduced too quickly. A similar constraint was introduced in the stochastic search do-
main by (Abdolmaleki et al., 2015), and was shown to avoid premature convergence. This
constraint is even more crucial in our setting because of the inherent non-stationarity of the
objective function being optimized at each iteration. The cause for the non-stationarity of
the objective optimized at time-step t in the policy update is twofold: i) updates of policies
πt(cid:48) with time-step t(cid:48) > t will modify in the next iteration of the algorithm ˜Qt as a function
of s and a and hence the optimization landscape as a function of the policy parameters,
ii) updates of policies with time-step t(cid:48) < t will induce a change in the state distribution
ρt. If the policy had unlimited expressiveness, the optimal solution of Eq. (1) would be
to choose arg maxa ˜Qt irrespective of ρt. However, due to the restricted class of the policy,
any change in ρt will likely change the optimization landscape including the position of
the optimal policy parameter. Hence, Eq. (3) ensures that exploration in action space is
maintained as the optimization landscape evolves and avoids premature convergence.

3.2 Closed Form Update

Using the method of Lagrange multipliers, the solution of the optimization problem in
section 3.1 is given by

t(a|s) ∝ πt(a|s)η∗/(η∗+ω∗) exp
π(cid:48)

(cid:32) ˜Qt(s, a)
η∗ + ω∗

(cid:33)

,

with η∗ and ω∗ being the optimal Lagrange multipliers related to the KL and entropy
constraints respectively. Assuming that ˜Qt(s, a) is of quadratic form in a and s

(4)

(5)

˜Qt(s, a) =

aT Qaaa + aT Qass + aT qa + q(s),

1
2

4

Model-Free Trajectory-based Policy Optimization

with q(s) grouping all terms of ˜Qt(s, a) that do not depend2 on a, then π(cid:48)
linear-Gaussian form

t(a|s) is again of

t(a|s) = N (a|F Ls + F f , F (η∗ + ω∗)),
π(cid:48)

such that the gain matrix, bias and covariance matrix of π(cid:48)
L and vector f where

t are function of matrices F and

F = (η∗Σ−1

t − Qaa)−1,

L = η∗Σ−1

t Kt + Qas,

f = η∗Σ−1

t kt + qa.

Note that ηΣ−1
t − Qaa needs to be invertible and positive semi-deﬁnite as it deﬁnes the new
covariance matrix of the linear-Gaussian policy. For this to hold, either Qt(s, .) needs to
be concave in a (i.e. Qaa is negative semi-deﬁnite), or η needs to be large enough (and
for any Qaa such η always exists). A too large η is not desirable as it would barely yield
a change to the current policy (too small KL divergence) and could negatively impact
the convergence speed. Gradient based algorithms for learning model parameters with a
speciﬁc semi-deﬁnite shape are available (Bhojanapalli et al., 2015) and could be used for
learning a concave Qt. However, we found in practice that the resulting η was always small
enough (resulting in a maximally tolerated KL divergence of (cid:15) between successive policies)
while F remains well deﬁned, without requiring additional constraints on the nature of Qaa.

3.3 Dual Minimization

The Lagrangian multipliers η and ω are obtained by minimizing the convex dual function

gt(η, ω) = η(cid:15) − ωβ + (η + ω)

˜ρt(s) log

πt(a|s)η/(η+ω) exp

(cid:90)

(cid:18)(cid:90)

(cid:16) ˜Qt(s, a)/(η + ω)

(cid:17)

(cid:19)

da

ds.

Exploiting the structure of the quadratic Q-Function ˜Qt and the linear-Gaussian policy
πt(a|s), the inner integral over the action space can be evaluated in closed form and the
dual simpliﬁes to

g(η, ω) = η(cid:15)t − ωβt +

ρt(s)(sT M s + sT m + m0)ds.

(cid:90)

The dual function further simpliﬁes, by additionally assuming normality of the state distri-
bution ˜ρt(s) = N (s|µs, Σs), to the function

gt(η, ω) = η(cid:15) − ωβ + µT

s M µs + tr(ΣsM ) + µT

s m + m0,

which can be eﬃciently optimized by gradient descent to obtain η∗ and ω∗. The full ex-
pression of the dual function, including the deﬁnition of M , m and m0 in addition to the
partial derivatives ∂gt(η,ω)

are given in Appendix A.

and ∂gt(η,ω)

∂η

∂ω

2. Constant terms and terms depending on s but not a won’t appear in the policy update. As such, and al-
beit we only refer in this article to Qt(s, a), the Advantage Function At(s, a) can be used interchangeably
in lieu of Qt(s, a) for updating the policy.

5

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

4. Sample Eﬃcient Policy Evaluation

The KL constraint introduced in the policy update gives rise to a non-linear optimization
problem. This problem can still be solved in closed form for the class of linear-Gaussian
policies, if the learned function ˜Qi
t is quadratic in s and a. The ﬁrst subsection introduces
the main supervised learning problem solved during the policy evaluation for learning ˜Qi
t
while the remaining subsections discuss how to improve its sample eﬃciency.

4.1 The Q-Function Supervised Learning Problem

In the remainder of the section, we will be interested in ﬁnding the parameter w of a linear
model ˜Qi
t = (cid:104)w, φ(s, a)(cid:105), where the feature function φ contains a bias and all the linear
and quadratic terms of s and a, yielding 1 + (da + ds)(da + ds + 3)/2 parameters. ˜Qi
t can
subsequently be written as in Eq. (5) by extracting Qaa, Qas and qa from w.

t is learned only from samples Di

At each iteration i, M rollouts are performed following πi. Let us initially assume
, a[k]
that ˜Qi
t+1; k = 1..M } gathered from the
t
execution of the M rollouts. The parameter w of ˜Qi
t is learned by regularized linear least
square regression

t = {s[k]

, s[k]

t

w = arg min

w

1
M

M
(cid:88)

k=1

(cid:0)(cid:104)w, φ(s[k]
t

, a[k]

t )(cid:105) − ˆQi

t(s[k]

t

, a[k]

t )(cid:1)2 + λwT w,

(6)

where the target value ˆQi
will distinguish two cases for obtaining the estimate ˆQi

t(s[k], a[k]) is a noisy estimate of the true value Qi

, a[k]

t(s[k]

t

t ).

t(s[k]

t

, a[k]

t ). We

4.1.1 Monte-Carlo Estimate

t

, a[k]

t(s[k]

t ) = (cid:80)T

This estimate is obtained by summing the future rewards for each trajectory k, yielding
ˆQi
t(cid:48) ). This estimator is known to have no bias but high vari-
ance. The variance can be reduced by averaging over multiple rollouts, assuming we can
reset to states s[k]
. However, such an assumption would severely limit the applicability of
t
the algorithm on physical systems.

t(cid:48)=t rt(cid:48)(s[k]

t(cid:48) , a[k]

4.1.2 Dynamic Programming

In order to reduce the variance, this estimate exploits the V-Function to reduce the noise
of the expected rewards of time-steps t(cid:48) > t through the following identity

ˆQi

t(s[k]

t

, a[k]

t ) = rt(s[k]

t

, a[k]

t ) + ˆV i

t+1(s[k]

t+1),

(7)

t+1 is. However, we will use for ˆV i

which is unbiased if ˆV i
t+1 an approximate V-Function
˜V i
t+1 learned recursively in time. This approximation might introduce a bias which will
accumulate as t goes to 1. Fortunately, ˜V is not restricted by our algorithm to be of
a particular class as it does not appear in the policy update. Hence, the bias can be
reduced by increasing the complexity of the function approximator class. Nonetheless, in
this article, a quadratic function will also be used for the V-Function which worked well in
our experiments.

6

Model-Free Trajectory-based Policy Optimization

The V-Function is learned by ﬁrst assuming that ˜V i

quently and recursively in time, the function ˜V i
to ﬁt the parametric function ˜V i

t+1 and the transition samples in Di
(cid:16) ˆQi
t(s[k]
t ) − ˜V i
, a[k]
t ), the choice of learning
a V-Function is further justiﬁed by the increased possibility of reusing sample transitions
from all time-steps and previous iterations.

t by minimizing the loss (cid:80)M
t(s[k]

In addition to reducing the variance of the estimate ˆQi

T +1 is the zero function.3 Subse-
t are used
t (s[k]
.
t )

, a[k]

k=1

(cid:17)2

t

t

4.2 Sample Reuse

In order to improve the sample eﬃciency of our approach, we will reuse samples from
diﬀerent time-steps and iterations using importance sampling. Let the expected loss which
˜Qi

t minimizes under the assumption of an inﬁnite number of samples be

w = arg min

IE[(cid:96)i

t(s, a, s(cid:48); w)],

w

t is the inner term within the sum in Eq. (6); the estimate ˆQi

where the loss (cid:96)i
taken as in Eq. (7) and the expectation is with respect to the current state s ∼ ρi
action a ∼ πi

t(.|s) and the next state s(cid:48) ∼ p(.|s, a).

t(s[k]

t

, a[k]

t ) is
t, the

4.2.1 Reusing samples from different time-steps

To use transition samples from all time-steps when learning ˜Qi
t, we rely on importance
sampling, where the importance weight (IW) is given by the ratio between the state-action
probability of the current time-step zi
t(s)πi
t(a|s) divided by the time-independent
(cid:80)T
state-action probability of πi given by zi(s, a) = 1
t(s, a). The expected loss mini-
T
mized by ˜Qi

t(s, a) = ρi

t=1 zi

t becomes

min
w

IE

(cid:20) zi
t(s, a)
zi(s, a)

(cid:21)
t(s, a, s(cid:48); w) | (s, a) ∼ zi(s, a)
(cid:96)i

.

(8)

Since the transition probabilities are not time-dependent they cancel out from the IW.
Upon the computation of the IW, weighted least square regression is used to minimize an
empirical estimate of (8) for the data set Di = ∪T
t. Note that the (numerator of the) IW
needs to be recomputed at every time-step for all samples (s, a) ∈ Di. Additionally, if the
t(s[k]
rewards are time-dependent, the estimate ˆQi
t ) in Eq. (7) needs to be recomputed
with the current time-dependent reward, assuming the reward function is known.

t=1Di

, a[k]

t

4.2.2 Reusing samples from previous iterations

Following a similar reasoning, at a given time-step t, samples from previous iterations can
be reused for learning ˜Qi
t. In this case, we have access to the samples of the state-action
distribution z1:i
requires the storage of all
previous policies and state distributions. Thus, we will in practice limit ourselves to the K
last iterations.

t (s, a). The computation of z1:i
t

t (s, a) ∝ (cid:80)i

j=1 zj

3. Alternatively one could assume the presence of a ﬁnal reward RT +1(st+1), as is usually formulated in

control tasks (Bertsekas, 1995), to which V i

T +1 could be initialized to.

7

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

Finally, both forms of sample reuse will be combined for learning ˜Qi

data set up to iteration i, D1:i = ∪i
IW are given by zi

t(s, a)/z1:i(s, a) with z1:i(s, a) ∝ (cid:80)T

t under the complete
j=1Dj using weighted least square regression where the
t (s, a).

t=1 z1:i

4.3 Estimating the State Distribution

To compute the IW, the state distribution at every time-step ρi
t needs to be estimated.
Since M rollouts are sampled for every policy πi only M state samples are available for
the estimation of ρi
t, necessitating again the reuse of previous samples to cope with higher
dimensional control tasks.

4.3.1 Forward propagation of the state distribution

t forward in time. Starting from ˜ρi

The ﬁrst investigated solution for the estimation of the state distribution is the propagation
of the estimate ˜ρi
1 which is identical for all iterations,
importance sampling is used to learn ˜ρi
t by
weighted maximum-likelihood; where each sample st+1 is weighted by zi
t (st, at).
And the computation of this IW only depends on the previously estimated state distribution
t. In practice however, the estimate ˜ρi
˜ρi
t might entail errors despite the use of all samples
from past iterations, which are propagated forward leading to a degeneracy of the number
of eﬀective samples in latter time-steps.

t+1 with t > 1 from samples (st, at, st+1) ∈ D1:i

t(st, at)/z1:i

4.3.2 State distribution of a mixture policy

The second considered solution for the estimation of ˜ρi
t is heuristic but behaved better in
practice. It is based on the intuition that the KL constraint of the policy update will yield
state distributions that are close to each other (see Sec. 5 for a theoretical justiﬁcation of the
closeness in state distributions) and state samples from previous iterations can be reused
in a simpler manner. Speciﬁcally, ˜ρi
t will be learned from samples of the mixture policy
π1:i ∝ (cid:80)i
j=1 γi−jπj which selects a policy from previous iterations with an exponentially
decaying (w.r.t. the iteration number) probability and executes it for a whole rollout. In
practice, the decay factor γ is selected according to the dimensionality of the problem, the
number of samples per iterations M and the KL upper bound (cid:15) (intuitively, a smaller (cid:15) yields
closer policies and henceforth more reusable samples). The estimated state distribution ˜ρi
t
is learned as a Gaussian distribution by weighted maximum likelihood from samples of D1:i
where a sample of iteration j is weighted by γi−j.

t

4.4 The MOTO Algorithm

MOTO is summarized in Alg. 1. The innermost loop is split between policy evaluation
(Sec. 4) and policy update (Sec. 3). For every time-step t, once the state distribution
˜ρi
t is estimated, the IWs of all the transition samples are computed and used to learn the
Q-Function (and the V-Function using the same IWs, if dynamic programming is used
when estimating the Q-Function), concluding the policy evaluation part. Subsequently, the
components of the quadratic model ˜Qi
t that depend on the action are extracted and used
to ﬁnd the optimal dual parameters η∗ and ω∗ that are respectively related to the KL and
the entropy constraint, by minimizing the convex dual function gi
t using gradient descent.

8

Model-Free Trajectory-based Policy Optimization

Algorithm 1 Model-Free Trajectory-based Policy Optimization (MOTO)

Input: Initial policy π0, number of trajectories per iteration M, step-size (cid:15) and entropy
reduction rate β0
Output: Policy πN
for i = 0 to N − 1 do

Sample M trajectories from πi
for t = T to 1 do

Estimate state distribution ˜ρi
t
Compute IW for all (s, a, s(cid:48)) ∈ D1:i
Estimate the Q-Function ˜Qi
t
Optimize: (η∗, ω∗) = arg min gi
Update πi+1

using η∗, ω∗,˜ρi

t(η, ω)
t and ˜Qi
t

t

end for

end for

(Sec. 4.3)
(Sec. 4.2)
(Sec. 4.1)
(Sec. 3.3)
(Sec. 3.2)

The policy update then uses η∗ and ω∗ to return the new policy πt+1 and the process is
iterated.

In addition to the simpliﬁcation of the policy update, the rationale behind the use of a
t is twofold: i) since Qi
local quadratic approximation for Qi
t is only optimized locally (because
of the KL constraint), a quadratic model would potentially contain as much information
as a Hessian matrix in a second order gradient descent setting ii) If ˜Qt in Eq. (4) is an
arbitrarily complex model then it is common that π(cid:48)
t, of linear-Gaussian form, is ﬁt by
weighted maximum-likelihood (Deisenroth et al., 2013); it is clear though from Eq. (4) that
however complex ˜Qt(s, a) is, if both πt and π(cid:48)
t are of linear-Gaussian form then there exist
a quadratic model that would result in the same policy update. Additionally, note that
˜Qt is not used when learning ˜Qt−1 (sec. 4.1) and hence the bias introduced by ˜Qt will not
propagate back. For these reasons, we think that choosing a more complex class for ˜Qt than
that of quadratic functions might not necessarily lead to an improvement of the resulting
policy, for the class of linear-Gaussian policies.

5. Monotonic Improvement of the Policy Update

We analyze in this section the properties of the constrained optimization problem solved
during our policy update. Kakade and Langford (2002) showed that in the approximate
policy iteration setting, a monotonic improvement of the policy return can be obtained if
the successive policies are close enough. While in our algorithm the optimization problem
deﬁned in Sec. 3.1 bounds the expected policy KL under the state distribution of the current
iteration i, it does not tell us how similar the policies are under the new state distribution
and a more careful analysis needs to be conducted.

The analysis we present builds on the results of Kakade and Langford (2002) to lower-
bound the change in policy return J(πi+1) − J(πi) between the new policy πi+1 (solution
of the optimization problem deﬁned in Sec. 3.1) and the current policy πi. Unlike Kakade
and Langford (2002), we enforce closeness between successive policies with a KL constraint
instead of by mixing πi+1 and πi. Related results were obtained when a KL constraint is

9

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

used in Schulman et al. (2015). Our main contribution is to extend these results to the
trajectory optimization setting with continuous states and actions and where the expected
KL between the policies is bounded instead of the maximum KL over the state space (which
is hard to achieve in practice).

In what follows, p and q denote the next policy πi+1 and the current policy πi respec-
tively. We will denote the state distribution and policy at time-step t by pt and pt(.|s)
respectively (and similarly for q). First, we start by writing the diﬀerence between policy
returns in term of advantage functions.

Lemma 1 For any two policies p and q, and where Aq
time-step t of policy q, the diﬀerence in policy return is given by

t denotes the advantage function at

J(p) − J(q) =

IEs∼pt,a∼pt(.|s) [Aq

t (s, a)] .

T
(cid:88)

t=1

The proof of Lemma 1 is given by the proof of Lemma 5.2.1 in (Kakade, 2003). Note
that Lemma 1 expresses the change in policy return in term of expected advantage under
the current state distribution while we optimize the advantage function under the state
distribution of policy q, which is made apparent in Lemma 2.

Lemma 2 Let (cid:15)t = KL(pt (cid:107) qt) be the KL divergence between state distributions pt(.) and
qt(.) and let δt = maxs |IEa∼pt(.|s)[Aq

t (s, a)]|, then for any two policies p and q we have

J(p) − J(q) ≥

IEs∼qt,a∼pt(.|s) [Aq

t (s, a)] − 2

T
(cid:88)

t=1

(cid:114) (cid:15)t
2

.

δt

Proof

IEs∼pt,a∼pt(.|s) [Aq

t (st, at)] =

pt(s)

pt(at|st)Aq

t (st, at),

T
(cid:88)

t=1

(cid:90)

(cid:90)

=

(cid:90)

(cid:90)

qt(s)
(cid:90)

pt(at|st)Aq
t (st, at)
(cid:90)

+

(pt(s) − qt(s))

≥ IEs∼qt,a∼pt(.|s) [Aq

t (s, a)] − δt

≥ IEs∼qt,a∼pt(.|s) [Aq

t (s, a)] − 2δt

pt(at|st)Aq
(cid:90)

t (st, at),

(pt(s) − qt(s)),
(cid:90)

|pt(s) − qt(s)|,

≥ IEs∼qt,a∼pt(.|s) [Aq

t (s, a)] − 2δt

KL(pt (cid:107) qt).

1
2
(cid:114) 1
2

(Pinsker’s inequality)

Summing over the time-steps and using Lemma 1 completes the proof.

Lemma 2 lower-bounds the change in policy return by the advantage term optimized
during the policy update and a negative change that quantiﬁes the change in state distribu-
tions between successive policies. The core of our contribution is given by Lemma 3 which

10

Model-Free Trajectory-based Policy Optimization

relates the change in state distribution to the expected KL constraint between policies of
our policy update.

Lemma 3 If for every time-step, the state distributions pt and qt are Gaussian and the
policies pt(.|st) and qt(.|st) are linear-Gaussian and if IEs∼qt [KL(pt(.|s) (cid:107) qt(.|s))] ≤ (cid:15) for
every time-step then KL(pt (cid:107) qt) = O((cid:15)) as (cid:15) → 0 for every time-step.

Proof We will demonstrate the lemma by induction noting that for t = 1 the state
distributions are identical and hence their KL is zero. Assuming (cid:15)t = KL(pt (cid:107) qt) = O((cid:15))
as (cid:15) → 0, let us compute the KL between state distributions for t + 1

KL(pt+1 (cid:107) qt+1) =

pt+1(s(cid:48)) log

pt+1(s(cid:48))
qt+1(s(cid:48))

,

(cid:90) (cid:90) (cid:90)

(cid:90)

(cid:90)

≤

=

pt(s, a)p(s(cid:48)|a, s) log

pt(s, a)p(s(cid:48)|a, s)
qt(s, a)p(s(cid:48)|a, s)

,

(cid:90)

pt(s(cid:48))

pt(a|s(cid:48)) log

pt(s(cid:48))pt(a|s(cid:48))
qt(s(cid:48))qt(a|s(cid:48))

,

(log sum inequality)

= (cid:15)t + IEs∼pt[KL(pt(.|s) (cid:107) qt(.|s))].

(9)

Hence we have bounded the KL between state distributions at t + 1 by the KL between
state distributions and the expected KL between policies of the previous time-step t.
Now we will express the KL between policies under the new state distributions, given
by IEs∼pt[KL(pt(.|s) (cid:107) qt(.|s))], in terms of KL between policies under the previous state
distribution, IEs∼qt [KL(pt(.|s) (cid:107) qt(.|s))] which is bounded during policy update by (cid:15), and
KL(pt (cid:107) qt). To do so, we will use the assumption that the state distribution and the policy
are Gaussian and linear-Gaussian. The complete demonstration is given in Appendix B,
and we only report the following result

IEs∼pt[KL(pt(.|s) (cid:107) qt(.|s))] ≤ 2(cid:15) (3(cid:15)t + ds + 1) .

(10)

It is now easy to see that the combination of (9) and (10) together with the induction
hypothesis yields KL(pt+1 (cid:107) qt+1) = O((cid:15)) as (cid:15) → 0.

Finally, the combination of Lemma 2 and Lemma 3 results in the following theorem,

lower-bounding the change in policy return.

Theorem 4 If for every time-step the state distributions pt and qt are Gaussian and the
policies pt(.|st) and qt(.|st) are linear-Gaussian and if IEs∼qt [KL(pt(.|s) (cid:107) qt(.|s))] ≤ (cid:15) for
every time-step then

J(p) − J(q) ≥

IEs∼qt,a∼pt(.|s) [Aq

t (s, a)] −

T
(cid:88)

t=1

√

δtO(

(cid:15)).

T
(cid:88)

t=1

Theorem 4 shows that we are able to obtain similar bounds than those derived in
(Schulman et al., 2015) for our continuous state-action trajectory optimization setting with
a bounded KL policy update in expectation under the previous state distribution. While, it

11

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

is not easy to apply Theorem 4 in practice to choose an appropriate step-size (cid:15) since Aq
t (s, a)
is generally only known approximately, Theorem 4 still shows that our constrained policy
update will result in small changes in the overall behavior of the policy between successive
iterations which is crucial in the approximate RL setting.

6. Related Work

In the Approximate Policy Iteration scheme (Szepesvari, 2010), policy updates can poten-
tially decrease the expected reward, leading to policy oscillations (Wagner, 2011), unless
the updated policy is ’close’ enough to the previous one (Kakade and Langford, 2002).
Bounding the change between πi and πi+1 during the policy update step is thus a well
studied idea in the Approximate Policy Iteration literature. Already in 2002, Kakade and
Langford proposed the Conservative Policy Iteration (CPI) algorithm where the new policy
πi+1 is obtained as a mixture of πi and the greedy policy w.r.t. Qi. The mixture pa-
rameter is chosen such that a lower bound of J(πi+1) − J(πi) is positive and improvement
is guaranteed. However, convergence was only asymptotic and in practice a single policy
update would require as many samples as other algorithms would need to ﬁnd the optimal
solution (Pirotta et al., 2013b). Pirotta et al. (2013b) reﬁned the lower bound of CPI by
adding an additional term capturing the closeness between policies (deﬁned as the matrix
norm of the diﬀerence between the two policies), resulting in a more aggressive updates
and better experimental results. However, both approaches only considered discrete action
spaces. Pirotta et al. (2013a) provide an extension to continuous domains but only for single
dimensional actions.

When the action space is continuous, which is typical in e.g.

robotic applications,
using a stochastic policy and updating it under a KL constraint to ensure ’closeness’ of
successive policies has shown several empirical successes (Daniel et al., 2012; Levine and
Koltun, 2014; Schulman et al., 2015). However, only an empirical sample estimate of the
objective function is generally optimized (Peters et al., 2010; Schulman et al., 2015), which
typically requires a high number of samples and precludes it from a direct application to
physical systems. The sample complexity can be reduced when a model of the dynamics
is available (Levine and Koltun, 2014) or learned (Levine and Abbeel, 2014). In the latter
work, empirical evidence suggests that good policies can be learned on high dimensional
continuous state-action spaces with only a few hundred episodes. The counter part being
that time-dependent dynamics are assumed to be linear, which is a simplifying assumption
in many cases. Learning more sophisticated models using for example Gaussian Processes
was experimented by Deisenroth and Rasmussen (2011) and Pan and Theodorou (2014) in
the Policy Search and Trajectory Optimization context, but it is still considered to be a
challenging task, see Deisenroth et al. (2013), chapter 3.

The policy update in Eq. (4) resembles that of (Peters et al., 2010; Daniel et al., 2012)
with three main diﬀerences. First, without the assumption of a quadratic Q-Function, an
additional weighted maximum likelihood step is required for ﬁtting πi+1 to weighted samples
as in the r.h.s of Eq. (4), since this policy might not be of the same policy class. As a
result, the KL between πi and πi+1 is no longer respected. Secondly, we added an entropy
constraint in order to cope with the inherent non-stationary objective function maximized
by the policy (Eq. 1) and to ensure that exploration is sustained, resulting in better quality

12

Model-Free Trajectory-based Policy Optimization

policies. Thirdly, their sample based optimization algorithm requires the introduction of a
number of dual variables typically scaling at least linearly with the dimension of the state
space, while we only have to optimize over two dual variables irrespective of the state space.
Most trajectory optimization methods are based on stochastic optimal control. These
methods linearize the system dynamics and update the policy in closed form as a LQR.
Instances of such algorithms are for example iLQG (Todorov, 2006), DDP (Theodorou
et al., 2010), AICO (Toussaint, 2009) and its more robust variant (R¨uckert et al., 2014)
and the trajectory optimization algorithm used in the GPS algorithm (Levine and Abbeel,
2014). These methods share the same assumptions as MOTO for ρi
t respectively
considered to be of Gaussian and linear-Gaussian form. These methods face issues in
maintaining the stability of the policy update and, similarly to MOTO, introduce additional
constraints and regularizers to their update step. DDP, iLQG and AICO regularize the
update by introducing a damping term in the matrix inversion step, while GPS uses a KL
bound on successive trajectory distributions. However, as demonstrated in Sec. 7, the
quadratic approximation of the Q-Function performed by MOTO seems to be empirically
less detrimental to the quality of the policy update than the linearization of the system
dynamics around the mean trajectory performed by related approaches.

t and πi

7. Experimental Validation

MOTO is experimentally validated on a set of multi-link swing-up tasks and on a robot
table tennis task. The experimental section aims at analyzing the proposed algorithm from
four diﬀerent angles: i) the quality of the returned policy comparatively to state-of-the-art
trajectory optimization algorithms, ii) the eﬀectiveness of the proposed variance reduction
and sample reuse schemes, iii) the contribution of the added entropy constraint during
policy updates in ﬁnding better local optima and iv) the ability of the algorithm to scale
to higher dimensional problems. The experimental section concludes with a comparison
to TRPO (Schulman et al., 2015), a state-of-the-art reinforcement learning algorithm that
bounds the KL between successive policies; showcasing settings in which the time-dependent
linear-Gaussian policies used by MOTO are a suitable alternative to neural networks.

7.1 Multi-link Swing-up Tasks

A set of swing-up tasks involving a multi-link pole with respectively two and four joints
(Fig. 1.a and 2.a) is considered in this section. The set of tasks includes several variants
with diﬀerent torque and joint limits, introducing additional non-linearities in the dynamics
and resulting in more challenging control problems for trajectory optimization algorithms
based on linearizing the dynamics. The state space consists of the joint positions and joint
velocities while the control actions are the motor torques. In all the tasks, the reward func-
tion is split between an action cost and a state cost. The action cost is constant throughout
the time-steps while the state cost is time-dependent and is equal to zero for all but the
20 last time-steps. During this period, a quadratic cost penalizes the state for not being
the null vector, i.e. having zero velocity and reaching the upright position. Examples of
successful swing-ups learned with MOTO are depicted in Fig. 1.a and 2.a.

13

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

(a)

(b)

(c)

Figure 1: a) Double link swing-up policy found by MOTO. b) Comparison between GPS and
MOTO on the double link swing-up task (diﬀerent torque limits and state costs are
applied compared to c) and f). c) MOTO and its variants on the double link swing-up
task: MOTO without the entropy constraint (EC), importance sampling (IS ) or dynamic
programming (DP). All plots are averaged over 15 runs.

(a)

(b)

(c)

Figure 2: a) Quad link swing-up policy found by MOTO. b) Comparison between GPS and MOTO
on the quad link swing-up task with restricted joint limits and two diﬀerent torque limits.
c) MOTO on the double link swing-up task for varying number of rollouts per episode
and step-sizes. All plots are averaged over 15 runs.

MOTO is compared to the trajectory optimization algorithm proposed in Levine and
Abbeel (2014), that we will refer to as GPS.4 We chose to compare MOTO and GPS as
both use a KL constraint to bound the change in policy. As such, the choice of approxi-
mating the Q-Function with time-dependent quadratic models (as done in MOTO) in order
to solve the policy update instead of linearizing the system dynamics around the mean
trajectory (as done in most trajectory optimization algorithms) is better isolated. GPS
and MOTO both use a time-dependent linear-Gaussian policy. In order to learn the linear

4. This is a slight abuse of notation as the GPS algorithm of (Levine and Abbeel, 2014) additionally feeds
the optimized trajectory to an upper level policy. However, in this article, we are only interested in the
trajectory optimization part.

14

Model-Free Trajectory-based Policy Optimization

model of the system dynamics, GPS reuses samples from diﬀerent time-steps by learning a
Gaussian mixture model on all the samples and uses this model as a prior to learn a joint
Gaussian distribution p(st, at, st+1) for every time-step. To single out the choice of lineariz-
ing the dynamics model or lack thereof from the diﬀerent approaches to sample reuse, we
give to both algorithm a high number of samples (200 and 400 rollouts per iteration for the
double and quad link respectively) and bypass any form of sample reuse for both algorithms.

Fig. 1.b compares GPS to two conﬁgurations of MOTO on the double-link swing up
task. The same initial policy and step-size (cid:15) are used by both algorithm. However, we found
that GPS performs better with a smaller initial variance, as otherwise actions quickly hit
the torque limits making the dynamics modeling harder. Fig. 1.b shows that even if the
dynamics of the system are not linear, GPS manages to improve the policy return, and even-
tually ﬁnds a swing-up policy. The two conﬁgurations of MOTO have an entropy reduction
constant β0 of .1 and .5. The eﬀect of the entropy constraint is similar to the one observed
in the stochastic search domain by (Abdolmaleki et al., 2015). Speciﬁcally, a smaller en-
tropy reduction constant β0 results in an initially slower convergence but ultimately leads
to higher quality policies. In this particular task, MOTO with β0 = .1 manages to slightly
outperform GPS.

Next, GPS and MOTO are compared on the quad link swing-up task. We found this
task to be signiﬁcantly more challenging than the double link and to increase the diﬃ-
culty further, soft joint limits are introduced on the three last joints in the following way:
whenever a joint angle exceeds in absolute value the threshold 2
3 π, the desired torque of
the policy is ignored in favor of a linear-feedback controller that aims at pushing back
the joint angle within the constrained range. As a result, Fig. 2.b shows that GPS can
barely improve its average return (with the torque limits set to 25, as in the double link
task.) while MOTO performs signiﬁcantly better. Finally, the torque limits are reduced
even further but MOTO still manages to ﬁnd a swing-up policy as demonstrated by Fig. 2.a.

In the last set of comparisons, the importance of each of the components of MOTO is
assessed on the double link experiment. The number of rollouts per iteration is reduced
to M = 20. Fig. 1.c shows that:
i) the entropy constraint provides an improvement on
the quality of the policy in the last iterations in exchange of a slower initial progress, ii)
importance sampling greatly helps in speeding-up the convergence and iii) the Monte-Carlo
estimate of ˆQt
i is not adequate for the smaller number of rollouts per iterations, which is
further exacerbated by the fact that sample reuse of transitions from diﬀerent time-steps is
not possible with the Monte-Carlo estimate.

Finally, we explore on the double-link swing-up task several values of M , trying to ﬁnd
the balance between performing a small number of rollouts per iterations with a small step-
size (cid:15) versus having a large number of rollouts for the policy evaluation that would allow
to take larger update steps. To do so, we start with an initial M = 20 and successively
divide this number by two until M = 5. In each case, the entropy reduction constant is
set such that, for a similar number of rollouts, the entropy is reduced by the same amount,
while we choose γ(cid:48), the discount of the state sample weights as γ(cid:48) = γM/M (cid:48) to yield again

15

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

(a)

(b)

(c)

Figure 3: a) Comparison on the robot table tennis task with no noise on the initial velocity of the
ball. b) Comparison on the robot table tennis task with Gaussian noise during the ball
bounce on the table. c) Comparison on the robot table tennis task with initial velocity
sampled uniformly in a 15cm range.

a similar sample decay after the same number of rollouts have been performed. Tuning
(cid:15) was, however, more complicated and we tested several values on non-overlapping ranges
for each M and selected the best one. Fig. 2.c shows that, on the double link swing-up
task, a better sample eﬃciency is achieved with a smaller M . However, the improvement
becomes negligible from M = 10 to M = 5. We also noticed a sharp decrease in the
number of eﬀective samples when M tends to 1. In this limit case, the complexity of the
mixture policy z1:i in the denominator of the importance ratio increases with the decrease
of M and might become a poor representation of the data set. Fitting a simpler state-action
distribution that is more representative of the data can be the subject of future work in order
to further improve the sample eﬃciency of the algorithm, which is crucial for applications
on physical systems.

7.2 Robot Table Tennis

The considered robot table tennis task consists of a simulated robotic arm mounted on
a ﬂoating base, having a racket on the end eﬀector. The task of the robot is to return
incoming balls using a forehand strike to the opposite side of the table (Fig. 4). The arm
has 9 degrees of freedom comprising the six joints of the arm and the three linear joints
of the base allowing (small) 3D movement. Together with the joint velocities and the 3D
position of the incoming ball, the resulting state space is of dimension ds = 21 and the
action space is of dimension da = 9 and consists of direct torque commands.

We use the analytical player of M¨ulling et al. (2011) to generate a single forehand stroke,
which is subsequently used to learn from demonstration the initial policy π1. The analytica
player comprises a waiting phase (keeping the arm still), a preparation phase, a hitting
phase and a return phase, which resets the arm to the waiting position of the arm. Only
the preparation and the hitting phase are replaced by a learned policy. The total control
time for the two learned phases is of 300 time-steps at 500hz, although for the MOTO

16

Model-Free Trajectory-based Policy Optimization

Figure 4: Robot table tennis setting and a forehand stroke learned by MOTO upon a spinning ball.

algorithm we subsequently divide the control frequency by a factor of 10 resulting in a
time-dependent policy of 30 linear-Gaussian controllers.

The learning from demonstration step is straightforward and only consists in averaging
the torque commands of every 10 time-steps and using these quantities as the initial bias
for each of the 30 controllers. Although this captures the basic template of the forehand
strike, no correlation between the action and the state (e.g. the ball position) is learned
from demonstration as the initial gain matrix K for all the time-steps is set to the null
matrix. Similarly, the exploration in action space is uninformed and initially set to the
identity matrix.

Three settings of the task are considered, a noiseless case where the ball is launched
with the same initial velocity, a varying context setting where the initial velocity is sampled
uniformly within a ﬁxed range and the noisy bounce setting where a Gaussian noise is added
to both the x and y velocities of the ball upon bouncing on the table, to simulate the eﬀect
of a spin.

We compare MOTO to the policy search algorithm REPS (Kupcsik et al., 2013) and
the stochastic search algorithm MORE (Abdolmaleki et al., 2015) that shares a related
information-theoretic update. Both algorithms will optimize the parameters of a Dynamical
Movement Primitive (DMP) (Ijspeert and Schaal, 2003). A DMP is a non-linear attractor
system commonly used in robotics. The DMP is initialized from the same single trajectory
and the two algorithm will optimize the goal joint positions and velocities of the attractor
system. Note that the DMP generates a trajectory of states, which will be tracked by a
linear controller using the inverse dynamics. While MOTO will directly output the torque
commands and does not rely on this model.

Fig. 3.a and 3.c show that our algorithm converges faster than REPS and to a smaller
extent than MORE in both the noiseless and the varying context setting. This is somewhat
surprising since MOTO with its time-dependent linear policy have a much higher number
of parameters to optimize than the 18 parameters of the DMP’s attractor. However, the
resulting policy in both cases is slightly less good than that of MORE and REPS. Note
that for the varying context setting, we used a contextual variant of REPS that learns a
mapping from the initial ball velocity to the DMP’s parameters. MORE, on the other hand
couldn’t be compared in this setting. Finally, Fig. 3.b shows that our policy is successfully

17

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

capable of adapting to noise at ball bounce, while the other methods fail to do so since the
trajectory of the DMP is not updated once generated.

7.3 Comparison to Neural Network Policies

Recent advances in reinforcement learning using neural network policies and supported by
the ability of generating and processing large amounts of data allowed impressive achieve-
ments such as playing Atari at human level (Mnih et al., 2015) or mastering the game
of Go (Silver et al., 2016). On continuous control tasks, success was found by combining
trajectory optimization and supervised learning of a neural network policy (Levine and
Abbeel, 2014), or by directly optimizing the policy’s neural network using reinforcement
learning (Lillicrap et al., 2015; Schulman et al., 2015). The latter work, side-stepping tra-
jectory optimization to directly optimize a neural network policy raises the question as to
whether the linear-Gaussian policies used in MOTO and related algorithms provide any
beneﬁt compared to neural network policies.

To this end, we propose to compare on the multi-link swing-up tasks of Sec. 7.1, MOTO
learning a time-dependent linear-Gaussian policy to TRPO (Schulman et al., 2015) learning
a neural network policy. We chose TRPO as our reinforcement learning baseline for its state-
of-the-art performance and because of its similar policy update than that of MOTO (both
bound the KL between successive policies). Three variants of TRPO are considered while
for MOTO, we refrain from using importance sampling (Sec. 4.2) since similar techniques
such as oﬀ-policy policy evaluation can be used for TRPO.

First, MOTO is compared to a default version of TRPO using OpenAI’s baselines im-
plementation (Dhariwal et al., 2017) where TRPO optimizes a neural network for both
learning the policy and the V-Function. Default parameters are used except for the KL
divergence constraint where we set (cid:15) = .1 for TRPO to match MOTO’s setting. Note that
because the rewards are time-dependent (distance to the upright position penalized only for
the last 20 steps, see Sec. 7.1) we add time as an additional entry to the state description.
Time entry is in the interval [0, 1] (current time-step divided by horizon T ) and is fed to
both the policy and V-Function neural networks. This ﬁrst variant of TRPO answers the
question:
is there any beneﬁt for using MOTO with its time-dependent linear-Gaussian
policy instead of a state-of-the-art deep RL implementation with a neural network policy.
The second considered baseline uses the same base TRPO algorithm but replaces the
policy evaluation using a neural network V-Function with the same policy evaluation used
by MOTO (Sec. 4), back-propagating a quadratic V-Function. In this variant of TRPO the
time-entry is dropped for the V-Function. This second baseline better isolates the policy
update, which is the core of both algorithms, from the learning of the V-Function which
could be interchanged.

Finally, we consider a third variant of TRPO that uses both the quadratic V-Function
and a time-dependent linear-Gaussian policy with diagonal covariance matrix (standard for-
mulation and implementation of TRPO does not support full covariance exploration noise).
The time entry is dropped for both the V-Function and the policy in this third baseline.
While both algorithms bound the KL divergence between successive policies, there are still a
few diﬀerentiating factors between this third baseline and MOTO. First, TRPO bounds the
KL of the whole policy while MOTO solves a policy update for each time-step independently

18

Model-Free Trajectory-based Policy Optimization

(but still results in a well-founded approximate policy iteration algorithm as discussed in
Sec. 5). In practice the KL divergence upon update for every time-step for MOTO is often
equal to (cid:15) and hence both MOTO and TRPO result in the same KL divergence of the overall
policy (in expectation of the state distribution) while the KL divergence of the sub-policies
(w.r.t. the time-step) may vary. Secondly, MOTO performs a quadratic approximation
of the Q-Function and solves the policy update exactly while TRPO performs a quadratic
approximation of the KL constraint and solves the policy update using conjugate gradient
descent. TRPO does not solve the policy update in closed form because it would require a
matrix inversion and the matrix to invert has the dimensionality of the number of policy
parameters. In contrast, MOTO can aﬀord the closed form solution because the matrix
to invert has the dimensionality of the action space which is generally signiﬁcantly smaller
than the number of policy parameters.

Fig. 5 shows the learning performance of MOTO and three TRPO variants on the double
link and quadruple link swing-up tasks (Sec. 7.1). In both tasks MOTO outperforms all
three TRPO variants albeit when TRPO is combined with the quadratic V-Function (second
variant), it initially outperforms MOTO on the double link swing-up task. The quadratic
V-Function beﬁts these two tasks in particular and the quadratic regulation setting more
generally because the reward is a quadratic function of the state-action pair (here the
negative squared distance to the upright position and a quadratic action cost). However,
MOTO makes better use of the task’s nature and largely outperforms the third variant
of TRPO despite having a similar policy evaluation step and using the same policy class.
In conclusion, while neural networks can be a general purpose policy class demonstrating
success on a wide variety of tasks, on speciﬁc settings such as on quadratic regulator tasks,
trajectory-based policy optimization is able to outperform deep RL algorithms. MOTO
in particular, which does not rely on a linearization of the dynamics around the mean
trajectory is able to handle quadratic reward problems with highly non-linear dynamics such
as the quadruple link swing-up task and outperform state-of-the-art trajectory optimization
algorithms (Sec. 7.1) as a result.

8. Conclusion

We proposed in this article MOTO, a new trajectory-based policy optimization algorithm
that does not rely on a linearization of the dynamics. Yet, an eﬃcient policy update
could be derived in closed form by locally ﬁtting a quadratic Q-Function. We additionally
conducted a theoretical analysis of the constrained optimization problem solved during the
policy update. We showed that the upper bound on the expected KL between successive
policies leads to only a small drift in successive state distributions which is a key property
in the approximate policy iteration scheme.

The use of a KL constraint is widely spread including in other trajectory optimization
algorithms. The experiments demonstrate however that our algorithm has an increased ro-
bustness towards non-linearities of the system dynamics when compared to a closely related
trajectory optimization algorithm. It appears as such that the simpliﬁcation resulting from
considering a local linear approximation of the dynamics is more detrimental to the overall
convergence of the algorithm than a local quadratic approximation of the Q-Function.

19

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

Figure 5: Comparisons on multi-link swing-up tasks between MOTO and TRPO. TRPO uses a
neural network policy and V-Function (default) or a quadratic V-Function and a time-
dependent linear-Gaussian policy as in MOTO. Quadratic V-Function is a good ﬁt for
such tasks and allows MOTO to outperform neural network policies on the double and
quadruple link swing-up tasks. Rewards of the original task divided by 1e4 to accomodate
with the neural network V-Function. Plots averaged over 11 independent runs.

On simulated robotics tasks, we demonstrated the merits of our approach compared to
direct policy search algorithms that optimize commonly used low dimensional parameterized
policies. The main strength of our approach is its ability to learn reactive policies capable
of adapting to external perturbations in a sample eﬃcient way. However, the exploration
scheme of our algorithm based on adding Gaussian noise at every time-step is less structured
than that of low dimensional parameterized policies and can be harmful to the robot. One
of the main addition that would ease the transition from simulation to physical systems is
thus to consider the safety of the exploration scheme of the algorithm. On a more technical
note, and as the V-Function can be of any shape in our setting, the use of a more complex
function approximator such as a deep network can be considered in future extensions to
allow for a more reﬁned bias-variance trade-oﬀ.

Acknowledgments

This work was supported by the DFG Project LearnRobotS under the SPP 1527 Au-
tonomous Learning.

20

Model-Free Trajectory-based Policy Optimization

Appendix A. Dual Function Derivations
Recall the quadratic form of the Q-Function ˜Qt(s, a) in the action a and state s

˜Qt(s, a) =

aT Qaaa + aT Qass + aT qa + q(s).

1
2

The new policy π(cid:48)
Gaussian form and given by

t(a|s) solution of the constrained maximization problem is again of linear-

t(a|s) = N (a|F Ls + F f , F (η∗ + ω∗)),
π(cid:48)

such that the gain matrix, bias and covariance matrix of π(cid:48)
L and vector f where

t are function of matrices F and

F = (η∗Σ−1
f = η∗Σ−1

t − Qaa)−1,
t kt + qa,

L = η∗Σ−1

t Kt + Qas,

with η∗ and ω∗ being the optimal Lagrange multipliers related to the KL and entropy
constraints, obtained by minimizing the dual function

gt(η, ω) = η(cid:15) − ωβ + (η + ω)

˜ρt(s) log

π(a|s)η/(η+ω) exp

(cid:90)

(cid:18)(cid:90)

(cid:16) ˜Qt(s, a)/(η + ω)

(cid:17)(cid:19)

.

From the quadratic form of ˜Qt(s, a) and by additionally assuming that the state distri-

bution is approximated by ˜ρt(s) = N (s|µs, Σs), the dual function simpliﬁes to

gt(η, ω) = η(cid:15) − ωβ + µT

s M µs + tr(ΣsM ) + µT

s m + m0,

where M , m and m0 are deﬁned by

M =

(cid:0)LT F L − ηKT

t Kt

(cid:1) , m = LT F f − ηKT

t Σ−1

t kt,

1
2

t Σ−1
1
2

m0 =

(f T F f − ηkT

t Σ−1

t kt − η log |2πΣt| + (η + ω) log |2π(η + ω)F |).

The convex dual function gt can be eﬃciently minimized by gradient descent and the policy
update is performed upon the computation of η∗ and ω∗. The gradient w.r.t. η and ω is
given by5

∂gt(η, ω)
∂η

= cst + lin + quad

cst = (cid:15) −

(kt − F f )T Σ−1

t

(kt − F f ) −

[log |2πΣt| − log |2π(η + ω)F |

1
2

1
2

+ (η + ω)tr(Σ−1
lin = ((Kt − F L)µs)T Σ−1

t F ) − da].

t (F f − kt).

∂gt(η, ω)
∂ω

1
2

= − β +

(da + log |2π(η + ω)F |).

quad = µT

s (Kt + F L)T Σ−1

t (Kt + F L)µs + tr(Σs(Kt + F L)T Σ−1

t (Kt + F L))

5. cst, lin, quad, F , L and f all depend on η and ω. We dropped the dependency from the notations for

compactness. da is the dimensionality of the action.

21

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

Appendix B. Bounding the Expected Policy KL Under the Current

State Distribution

Let the state distributions and policies be parameterized as following: pt(s) = N (s|µp, Σp),
qt(s) = N (s|µq, Σq), pt(a|s) = N (a|Ks + b, Σ) and qt(a|s) = N (a|K(cid:48)s + b(cid:48), Σ(cid:48)). The
change of the state distribution in the expected KL constraint of our policy update, given
by IEs∼qt[KL(pt(.|s) (cid:107) qt(.|s))] from state distribution qt to pt will only aﬀect the part of
the KL that depends on the state.

We give as a reminder the general formula for the KL between two Gaussian distributions

l = N (µ, Σ) and l(cid:48) = N (µ(cid:48), Σ(cid:48))

KL(l (cid:107) l(cid:48)) =

tr(Σ

(cid:48)−1Σ) + (µ − µ(cid:48))T Σ−(cid:48)1(µ − µ(cid:48)) − dim + log

(cid:18)

1
2

(cid:19)

.

|Σ(cid:48)|
|Σ|

For the linear-Gaussian policies, and since only the mean of the policies depend on the

state, the change of state distribution in the expected KL will only aﬀect the term

(Ks − K(cid:48)s)T Σ(cid:48)(Ks − K(cid:48)s) = sT M s,

with p.s.d. matrix M = (K − K(cid:48))T Σ(cid:48)(K − K(cid:48)). Thus it suﬃces to bound the expectation
(cid:82) pt(s)sT M s since the rest of the KL terms are already bounded by (cid:15), yielding

IEs∼pt[KL(pt(.|s) (cid:107) qt(.|s))] ≤ (cid:15) +

pt(s)sT M s,

(cid:90)

1
2
1
2

= (cid:15) +

(cid:0)µpM µp + tr(M Σp)(cid:1) ,

where we exploited the Gaussian nature of pt in the second line of the equation. We will
now bound both µpM µp and tr(M Σp). First, note that for any two p.d. matrices Σ and
Σ(cid:48) we have

tr(Σ

(cid:48)−1Σ) + −ds + log

|Σ(cid:48)|
|Σ|

≥ 0.

This immediately follows from the non-negativity of the KL. Since, if for some Σ and Σ(cid:48), eq.
(11) is negative then the KL for two Gaussian distributions having Σ and Σ(cid:48) as covariance
matrices and sharing the same mean would be negative which is not possible. Hence it also
follows that

from the bounded KL induction hypothesis between pt and qt.

(µq − µp)T Σ−1

q (µq − µp) ≤ 2(cid:15)t,

For the expected policy KL, since the part that does not depend on s is positive as in

eq. (11), it can thus be dropped out yielding

IEs∼qt[KL(pt(.|s) (cid:107) qt(.|s))] ≤ (cid:15) ⇒

qt(s)sT M s ≤ 2(cid:15),

(cid:90)

(11)

(12)

⇒ µqM µq + tr(M Σq) ≤ 2(cid:15).

(13)

22

Model-Free Trajectory-based Policy Optimization

Also note that for any p.s.d. matrices A and B, tr(AB) ≥ 0. Letting x = µp − µq, we

have

Third line is due to Cauchy-Schwarz inequality and positiveness of traces while the last one
is from eq. (12) and (13). Finally, from the reverse triangular inequality, we have

xT M x = tr(xxT M ),

= tr(Σ−1
≤ tr(Σ−1

q xxT M Σq),
q xxT )tr(M Σq),

≤ 4(cid:15)t(cid:15).

µpM µp ≤ xT M x + µqM µq,
≤ 2(cid:15)(1 + 2(cid:15)t),

tr(M Σp) = tr(M ΣqΣ−1

q Σp),
≤ tr(M Σq)tr(Σ−1

q Σp).

Which concludes the bounding of µpM µp.

To bound tr(M Σp) we can write

We know how to bound tr(M Σq) from Eq. (13). While tr(Σ−1
q Σp) appears in the bounded
q Σp) is equivalent to solving max (cid:80) λi
KL between state distributions. Bounding tr(Σ−1
under constraint (cid:80) λi − ds − (cid:80) log λi ≤ 2(cid:15)t, where the {λi} are the eigenvalues of Σ−1
q Σp.
For any solution {λi}, we can keep the same optimization objective using equal {λ(cid:48)
i} where
i = ¯λ = (cid:80) λi/ds is the average lambda. This transformation will at the same
for each i, λ(cid:48)
time reduce the value of the constraint since −ds log ¯λ ≤ − (cid:80) log λi from Jensen’s inequality.
Hence the optimum is reached when all the λi are equal, and the constraint is active (i.e.
ds¯λ − ds − ds log ¯λ = 2(cid:15)t). Finally, the constraint is at a minimum for ¯λ = 1, hence ¯λ > 1.
The maximum is reached at

The equation in the second line has a unique solution (f (λ) = λ−log λ is a strictly increasing
function for λ > 1) for which no closed form expression exists. We thus lower bound f by
g(λ) = e−1
e λ and solve the equation for g which yields an upper bound of the original
equation that is further simpliﬁed in the last inequality.
Eq. (13) and (14) yield tr(M Σp) ≤ 2(cid:15)(4(cid:15)t + 2ds) and grouping all the results yields

IEs∼pt[KL(pt(.|s) (cid:107) qt(.|s))] ≤ 2(cid:15) (3(cid:15)t + ds + 1)

(14)

⇔¯λ − log ¯λ =

ds¯λ − ds − ds log ¯λ = 2(cid:15)t
2(cid:15)t
+ 1
ds
(cid:19) e

⇒¯λ ≤

(cid:18) 2(cid:15)t
ds

+ 1

e − 1

⇒tr(Σ−1

q Σp) ≤ 4(cid:15)t + 2ds

23

Akrour, Abdolmaleki, Abdulsamad, Peters, and Neumann

References

A. Abdolmaleki, R. Lioutikov, J. Peters, N. Lau, L. Pualo Reis, and G. Neumann. Model-
based relative entropy stochastic search. In Advances in Neural Information Processing
Systems (NIPS). 2015.

R. Akrour, A. Abdolmaleki, H. Abdulsamad, and G. Neumann. Model-free trajectory op-
timization for reinforcement learning. In International Conference on Machine Learning
(ICML), 2016.

D. P. Bertsekas. Dynamic programming and optimal control. Athena Scientiﬁc, 1995.

S. Bhojanapalli, A. T. Kyrillidis, and S. Sanghavi. Dropping convexity for faster semi-

deﬁnite optimization. CoRR, 2015.

C. Daniel, G. Neumann, and J. Peters. Hierarchical Relative Entropy Policy Search. In

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2012.

M. Deisenroth and C. Rasmussen. PILCO: A Model-Based and Data-Eﬃcient Approach to

Policy Search. In International Conference on Machine Learning (ICML), 2011.

M. P. Deisenroth, G. Neumann, and J. Peters. A Survey on Policy Search for Robotics.

Foundations and Trends in Robotics, 2013.

P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman,
S. Sidor, and Y. Wu. Openai baselines. https://github.com/openai/baselines, 2017.

A. Ijspeert and S. Schaal. Learning attractor landscapes for learning motor primitives. In

Advances in Neural Information Processing Systems (NIPS). 2003.

S. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University

College London, 2003.

S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning.

In International Conference on Machine Learning (ICML), 2002.

A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-eﬃcient generalization
of robot skills with contextual policy search. In The Conference on Artiﬁcial Intelligence
(AAAI), 2013.

S. Levine and P. Abbeel. Learning neural network policies with guided policy search under
unknown dynamics. In Advances in Neural Information Processing Systems (NIPS). 2014.

S. Levine and V. Koltun. Learning complex neural network policies with trajectory opti-

mization. In International Conference on Machine Learning (ICML), 2014.

T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.

Continuous control with deep reinforcement learning. CoRR, 2015.

24

Model-Free Trajectory-based Policy Optimization

V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik,
I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level
control through deep reinforcement learning. Nature, 2015.

K. M¨ulling, J. Kober, and J. Peters. A biomimetic approach to robot table tennis. Adaptive

Behavior Journal, 2011.

Y. Pan and E. Theodorou. Probabilistic diﬀerential dynamic programming. In Advances in

Neural Information Processing Systems (NIPS). 2014.

J. Peters, K. M¨ulling, and Y. Altun. Relative entropy policy search.

In Conference on

Artiﬁcial Intelligence (AAAI), 2010.

M. Pirotta, M. Restelli, and L. Bascetta. Adaptive step-size for policy gradient methods.

In Advances in Neural Information Processing Systems (NIPS). 2013a.

M. Pirotta, M. Restelli, A. Pecorino, and D. Calandriello. Safe policy iteration. In Inter-

national Conference on Machine Learning (ICML), 2013b.

E. A. R¨uckert, M. Mindt, J. Peters, and G. Neumann. Robust policy updates for stochastic
optimal control. In International Conference on Humanoid Robots (Humanoids), 2014.

J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy opti-

mization. In International Conference on Machine Learning (ICML), 2015.

D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrit-
twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham,
N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and
D. Hassabis. Mastering the game of Go with deep neural networks and tree search.
Nature, 2016.

C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool, 2010.

E. Theodorou, J. Buchli, and S. Schaal. Path integral stochastic optimal control for rigid
body dynamics. In International Symposium on Approximate Dynamic Programming and
Reinforcement Learning (ADPRL), 2009.

E. Theodorou, Y. Tassa, and E. Todorov. Stochastic diﬀerential dynamic programming. In

American Control Conference (ACC), 2010.

E. Todorov. Optimal control theory. Bayesian Brain, 2006.

E. Todorov and Y. Tassa. Iterative local dynamic programming. In International Symposium

on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2009.

M. Toussaint. Robot trajectory optimization using approximate inference. In International

Conference on Machine Learning (ICML), 2009.

P. Wagner. A reinterpretation of the policy oscillation phenomenon in approximate policy

iteration. In Advances in Neural Information Processing Systems (NIPS). 2011.

25

