7
1
0
2
 
v
o
N
 
9
2
 
 
]

V
C
.
s
c
[
 
 
3
v
7
4
2
1
0
.
5
0
7
1
:
v
i
X
r
a

Unsupervised Part-based Weighting Aggregation of Deep Convolutional Features
for Image Retrieval

Jian Xu, Cunzhao Shi, Chengzuo Qi, Chunheng Wang∗, Baihua Xiao
State Key Laboratory of Management and Control for Complex Systems,
Institute of Automation, Chinese Academy of Sciences(CASIA)
University of Chinese Academy of Sciences
{xujian2015, cunzhao.shi, qichengzuo2013, chunheng.wang, baihua.xiao}@ia.ac.cn

Abstract

In this paper, we propose a simple but effective semantic
part-based weighting aggregation (PWA) for image retrieval.
The proposed PWA utilizes the discriminative ﬁlters of deep
convolutional layers as part detectors. Moreover, we propose
the effective unsupervised strategy to select some part de-
tectors to generate the “probabilistic proposals”, which high-
light certain discriminative parts of objects and suppress the
noise of background. The ﬁnal global PWA representation
could then be acquired by aggregating the regional repre-
sentations weighted by the selected ”probabilistic proposals”
corresponding to various semantic content. We conduct com-
prehensive experiments on four standard datasets and show
that our unsupervised PWA outperforms the state-of-the-art
unsupervised and supervised aggregation methods. Code is
available at https://github.com/XJhaoren/PWA.

Introduction
Over the past decades, image retrieval has received sustained
attention. The general retrieval framework (Zhou, Li, and
Tian 2017) consists of some pivotal modules, i.e., image rep-
resentation (Husain and Bober 2016; Tolias, Sicre, and Jgou
2016), database indexing (Babenko and Lempitsky 2015b),
image scoring (Xie et al. 2015; Zhong, Zhu, and Hoi 2015)
and search reranking (Arandjelovic and Zisserman 2012).
Image representations derived by aggregating features such
as Scale-Invariant Feature Transform (SIFT) (Lowe 2004)
and Convolutional Neural Network (CNN) (LeCun et al.
1989) are shown to be effective for image retrieval (Sivic
and Zisserman 2003; Jegou et al. 2012; Perronnin and
Dance 2007; Gou and Zisserman 2014; Do, Tran, and Che-
ung 2015; Husain and Bober 2016; Babenko et al. 2014;
Razavian et al. 2016; Babenko and Lempitsky 2015a; To-
lias, Sicre, and Jgou 2016; Kalantidis, Mellina, and Osindero
2016; Xie et al. 2016; Wei et al. 2017).

Recently, the performance of CNN-based features aggre-
gation methods (Babenko et al. 2014; Razavian et al. 2016;
Babenko and Lempitsky 2015a; Tolias, Sicre, and Jgou
2016; Kalantidis, Mellina, and Osindero 2016) rapidly out-
performs that of SIFT-based features aggregation meth-
ods (Sivic and Zisserman 2003; Jegou et al. 2012; Perronnin

∗Corresponding author

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

and Dance 2007; Perronnin, Nchez, and Mensink 2010;
Gou and Zisserman 2014; Do, Tran, and Cheung 2015;
Husain and Bober 2016). Some methods (Sharif Razavian
et al. 2014; Gong et al. 2014; Babenko et al. 2014) generate
the global representation based on fully connected layer fea-
tures for image retrieval. After that, convolutional features
are aggregated to obtain the global representation (Razavian
et al. 2016; Babenko and Lempitsky 2015a; Tolias, Sicre,
and Jgou 2016; Kalantidis, Mellina, and Osindero 2016;
Xie et al. 2016) and achieve better performance. Many re-
cent methods (Arandjelovic et al. 2016; Radenovic, Tolias,
and Chum 2016; Gordo et al. 2016a; 2016b) re-train the im-
age representations end-to-end for image retrieval task by
collected landmark buildings datasets. The ﬁne-tuning pro-
cess signiﬁcantly improves the adaptation ability for the spe-
ciﬁc task. However, these methods (Arandjelovic et al. 2016;
Radenovic, Tolias, and Chum 2016; Gordo et al. 2016a;
2016b) need to collect the labeled training datasets and the
performance heavily relies on the collected datasets. The
discrepant retrieval objects need different training datasets,
for example, the ﬁne-tuned model based on landmarks is not
suitable for logo retrieval.

Previous aggregation methods ignore the discriminative
information from the object parts. The part-based informa-
tion is utilized for ﬁne-gained categorization (Zhang et al.
2016a; Xiao et al. 2015; Simon and Rodner 2015; Zhang
et al. 2016b; He and Peng 2017) and the part-based repre-
sentation provides the state-of-the-art performance. Zhang
et al. (Zhang et al. 2016a) pick some distinctive ﬁlters which
respond to speciﬁc patterns signiﬁcantly and consistently to
learn a set of part detectors. Then, they conditionally en-
code the deep ﬁlter responses into the ﬁnal representation
based on Fisher vector (Perronnin and Dance 2007). In re-
cent work (Zhang et al. 2016b), the part-based image repre-
sentation is generated by aggregating selected parts on sev-
eral different scales. The recent work (He and Peng 2017)
applies spatial constraints to select part proposals which are
generated by selective search (Uijlings et al. 2013). Differ-
ent with these methods, the selected parts proposals (“prob-
abilistic proposals”) in our algorithm are not constrained to
rectangular box but erose shape.

Some recent works (He et al. 2014; Zhang et al. 2016a;
Zeiler and Fergus 2014) analyze the meaning of feature
maps of CNN. Zeiler et al. (Zeiler and Fergus 2014) show

background is suppressed (cold). For example, the 220th
feature map (Fig. 1 (b)) of pool5 layers from VGG16 (Si-
monyan and Zisserman 2015) is most activated by the sharp
shape; the 478th feature map (Fig. 1 (c)) is most activated
by the arc shape; the 483th feature map (Fig. 1 (d)) is most
activated by the bottom of buildings; the 360th feature map
(Fig. 1 (e)) is most activated by the body of buildings. We
can see that different ﬁlters of deep convolutional layers are
sensitive to different shapes or semantic, and they highlight
different parts and patterns of objects. Some special parts
of object are discriminative, for example, the 220th feature
maps highlight the spire of buildings. Therefore, ﬁlters of
deep convolutional layers can work as part detectors to pick
special patterns corresponding to ﬁxed semantic content. We
select the discriminative ﬁlters of deep convolutional layers
as the part detectors to generate erose “probabilistic propos-
als, which are related to different semantic content.

Inspired by the characteristics of feature maps, in this pa-
per we propose a novel and simple way of creating powerful
image representation via part-based aggregation. Our unsu-
pervised part-based weighting aggregation (PWA) method
signiﬁcantly outperforms the state-of-the-art unsupervised
aggregation methods (Razavian et al. 2016; Babenko and
Lempitsky 2015a; Tolias, Sicre, and Jgou 2016; Kalan-
tidis, Mellina, and Osindero 2016) and supervised meth-
ods (Radenovic, Tolias, and Chum 2016; Gordo et al. 2016a;
2016b) on four standard retrieval datasets.

The main contributions of this paper can be summarized

as follows:

“Probabilistic proposal” We select some discriminative
part detectors by succinct unsupervised strategy to gener-
ate the “probabilistic proposals” corresponding to special
semantic content. Different with previous methods, the se-
lected “probabilistic proposals” are not constrained to rect-
angular box and represent the conﬁdence degree of ﬁxed se-
mantic. To the best of our knowledge, this paper is the ﬁrst
work to select the erose “probabilistic proposals” for image
retrieval, and the selected “probabilistic proposals” corre-
sponding to special semantic content are tactfully employed
to generate high-dimensional representation which contains
discriminative semantic information.

Part-based weighting aggregation We aggregate the
convolutional features weighted by selected “probabilistic
proposals” and concatenate the regional representations as
global PWA representation. Because selected “probabilistic
proposals” corresponds to ﬁxed semantic but not ﬁxed po-
sition, the selected regional representations can be concate-
nated as the global PWA representation. Concatenation as
the global representation preserves more discrimination than
summing regional representations.

Aggregation based on “probabilistic
proposals”
The diagram of the proposed method is shown in Fig. 6.
Based on the dataset, we pick the discriminative part de-
tectors to generate the “probabilistic proposals” by the un-
supervised strategy in the off-line stage. Each “probabilis-

Figure 1: Visualization of “probabilistic proposals”. (a)
Some images in Oxford5K (Philbin et al. 2007). (b)-(e)
The various channels of feature maps in pool5 layer from
pre-trained VGG16 (Simonyan and Zisserman 2015). Each
channel of feature maps is activated (warm) by different
parts or patterns of objects and some discriminative chan-
nels can work as “probabilistic proposals”.

that some input patterns stimulate the special channels of
feature maps of the latter convolutional layers. He et al. vi-
sualize the feature maps generated by some ﬁlters of the
conv5 layer from SPP-net (He et al. 2014) and show that
the ﬁlters of deep convolutional layers are activated by spe-
ciﬁc semantic content and some distinctive ﬁlters can work
as part detectors. The various channels of convolutional fea-
ture maps can represent the pixel-level label mask of differ-
ent categories in Fully Convolutional Network (FCN) (Long,
Shelhamer, and Darrell 2015). Instance-aware semantic seg-
mentation (Dai, He, and Sun 2016; Li et al. 2017) employs
the different channels of shared convolutional layers to de-
tect and segment the various object instance jointly. Mask
R-CNN (He et al. 2017) demonstrates that the erose pro-
posals perform better than the rectangular regions on object
detection task. Inspired by above works, we employ some
selected discriminative ﬁlters of deep convolutional layers
as the part detectors to generate erose “probabilistic propos-
als”, which correspond to ﬁxed semantic content implicitly.
In this paper, we deﬁne the special channel of normal-
ized feature maps as “probabilistic proposal”. The “prob-
abilistic proposal” encodes the spatial layout of input ob-
ject’s parts corresponding to various semantic content, and
represents the probability of pixels belonging to ﬁxed se-
mantic. To further understand the meanings and characteris-
tics of the “probabilistic proposals”, we visualize some im-
ages and corresponding typical “probabilistic proposals” in
Fig. 1. We select some images in Oxford5K (Philbin et al.
2007) as shown in Fig. 1 (a). In Fig. 1 (b)-(e), we visualize
some discriminative channels of feature maps which work as
the “probabilistic proposals” for the selected images. Each
channel of feature maps is activated (warm) by special parts
or patterns corresponding to ﬁxed semantic content and the

Figure 2: Flow chart of our part-based weighting aggregation (PWA) method. We pick the discriminative part detectors to
generate the “probabilistic proposals” by the unsupervised strategy in the ﬁrst off-line stage. Each “probabilistic proposal”
corresponds to ﬁxed semantic content implicitly, such as pinnacles, arcs and bottom of buildings. In the aggregation stage, we
employ the selected N “probabilistic proposals” to weight and aggregate the feature maps as C-dimensional regional represen-
tations, and concatenate N regional representations as the ﬁnal global PWA representation.

tic proposal” corresponds to ﬁxed semantic content implic-
itly, such as pinnacles, arcs and bottom of buildings. In the
aggregation stage, we employ the selected N “probabilistic
proposals” to weight and aggregate the feature maps as C-
dimensional regional representations. Finally, we concate-
nate N regional representations corresponding to special se-
matic content as the ﬁnal global PWA representation.

In this section, we analyse the characteristics of the ﬁl-
ters of deep convolutional layers which can be interpreted
as part detectors. We propose the unsupervised strategy to
select discriminative part detectors to generate “probabilis-
tic proposals”. Based on the selected “probabilistic propos-
als” corresponding to special semantic content, we propose
a novel and effective PWA aggregation method for image
retrieval.

We extract features f from deep convolutional layers by
passing an image I through a pre-trained or ﬁne-tuned deep
network, which consist of C channels feature maps each
with height H and width W . Finally, the input image I is
represented by the aggregated N × C-dimensional vector
that are weighted by the N selected part detectors.

“Probabilistic proposals”
Selection of part detectors Because the responses with
large variances are signiﬁcantly different among the various
objects, the channels of feature maps with large variances
are more discriminative. Therefore, we select part detectors
according to variances based on dataset.

We ﬁrst calculate the C-channels variances V =
{v1, v2, ..., vc, ..., vC} of the C-dimensional vectors gi (i =
1, 2, ..., D) computed by sum pooling the C × W × H-
dimensional deep convolutional features fi of image i.

V =

(gi − ¯g)2

1
D

D
(cid:88)

i=1

(1)

where D is the number of database images. g = 1
D

D
(cid:80)
i=1
the average vector of feature vectors gi (i = 1, 2, ..., D).

gi is

gi =

fi(x, y)

(2)

W
(cid:88)

H
(cid:88)

x=1

y=1

Then we sort the variances {v1, v2, ..., vC} of C channels.
We select the discriminative deep convolutional layers ﬁl-
ters corresponding to large variances as the part detectors.
We also observe the ﬁlters with large variances to be more
discriminative by the following experiment. We performed
retrieval by PWA but we select (1) 30% random part detec-
tors (2) 30% part detectors with the largest variance. The
mAP score for the Oxford5k dataset (Philbin et al. 2007) for
(1) is only 0.775±0.006, which is much small than mAP for
(2), 0.790. This veriﬁes that feature maps with large vari-
ances are much more discriminative than random feature
maps. Moreover, our simple unsupervised selection method
not only boosts the performance but also reduces the com-
putational complexity of PWA representation.

Effects of “probabilistic proposals” The special chan-
nels of feature maps generated by selected part detectors can
work as the “probabilistic proposals” corresponding to ﬁxed
semantic content. To investigate the effects of “probabilistic
proposals” in detail, we compare the 512-dimensional repre-
sentation computed by sum pooling with the representation
weighted by the discriminative “probabilistic proposals” in
Fig. 3. As shown in Fig. 3, the selected “probabilistic pro-
posal” generated by 220th part detector suppresses the noise
of background and activates the sharp shape. Weighted by
the selected “probabilistic proposal”, the values of feature
maps that are activated by background (such as (a) 507th
and (b) 155th) are smaller. However, the values of the rep-
resentation corresponding to similar semantic content to the

Figure 3: The comparison of the 512-dimensional representations computed by PWA and sum pooling. Weighted by the selected
“probabilistic proposal”, values of the feature map’s channels activated by background (such as (a) 507th and (b) 155th) are
reduced. However, values of the representation corresponding to similar patterns to the selected “probabilistic proposal” (such
as (c) 53th) still keep large. The selected “probabilistic proposal” suppresses (cold) the noise of background and highlights
(warm) the special semantic content.

selected “probabilistic proposal” (such as (c) 53th) still keep
large. As a result, the representations weighted by the dis-
criminative “probabilistic proposals” are more discrimina-
tive and robust.

Overall, the discriminative ﬁlters of latter convolutional
layers are interpreted as part detectors to generate the “prob-
abilistic proposals”. The selected “probabilistic proposals”
suppress the noise of background and highlight the discrim-
inative parts and patterns of objects. We make use of the
selected “probabilistic proposals” to weight the activations
of convolutional layers and generate the regional represen-
tations. Because each ﬁlter of deep convolutional layers ac-
tivates special pattern, the various selected part detectors can
be employed to generate the erose proposals corresponding
to special semantic content. Each proposal corresponds to
a ﬁxed semantic pattern implicitly. The erose “probabilistic
proposals” maintain the explicit W × H object spatial lay-
out which can be addressed naturally by the pixel-to-pixel
correspondence provided by convolutions. Different with R-
MAC (Tolias, Sicre, and Jgou 2016), the “probabilistic pro-
posals” corresponds to ﬁxed semantic content rather than
ﬁxed position. Our “probabilistic proposals” are not con-
strained to box and represent the probability of pixels be-
long to ﬁxed semantic content. Although the “probabilistic
proposals” corresponding to the part detectors selected by
unsupervised strategy do not explicitly describe the seman-
tic, they implicitly represent discriminative semantic con-
tent, such as pinnacles, arcs and bottom of buildings. There-
fore we can concatenate the selected regional representa-
tions weighted by special semantic “probabilistic propos-
als” as the ﬁnal global representation. CroW (Kalantidis,
Mellina, and Osindero 2016), InterActive (Xie et al. 2016)
and PWA can be interpreted as spatial-weighted represen-
tations. InterActive (Xie et al. 2016) is much more gener-
alized in the aspect of spatial-weighted, which integrates
high-level visual context with low-level neuron responses by
back-propagation. Compared to CroW (Kalantidis, Mellina,
and Osindero 2016) and InterActive (Xie et al. 2016) that
sum the spatial-weighted representations, we independently
employ the selected part detectors to extract the regional
representations corresponding to special semantic content

and concatenate them as ﬁnal PWA representation. The con-
catenation of regional representations preserves more dis-
criminative information than summation in R-MAC (Tolias,
Sicre, and Jgou 2016), CroW (Kalantidis, Mellina, and Osin-
dero 2016) and InterActive (Xie et al. 2016).

PWA design
In this section, we describe the PWA method in detail. We
aggregate the feature maps weighted by the selected “prob-
abilistic proposals” and concatenate the regional representa-
tions as global PWA representation. We reduce the dimen-
sionality of high-dimensional PWA representation by unsu-
pervised method (PCA) in post-processing.

Weighted by selected “probabilistic proposals” The
construction of the PWA representation starts with the
weighted sum pooling of the C × W × H-dimensional deep
convolutional features f of image I with height H and width
W :

ψn(I) =

wn(x, y)f (x, y)

(3)

W
(cid:88)

H
(cid:88)

x=1

y=1

The coefﬁcients wn are the normalized weights as fol-
lows, which depend on the activation values vn(x, y) in posi-
tion (x, y) of the selected “probabilistic proposal” generated
by part detector n:

wn(x, y) =









vn(x, y)

W
(cid:80)
(
x=1

H
(cid:80)
y=1

vn(x, y)α)



1/β







1/α

(4)

where α and β are parameters of power normalization and
power-scaling respectively.

Concatenation N selected C-dimensional regional repre-
sentations ψn(I) are obtained from weighted sum pooling
process. We get the global N × C-dimensional representa-
tion vector ψ(I) by concatenating selected regional repre-
sentations:

ψ(I) = [ψ1, ψ2, · · · ψN ]

(5)

where we select the N part detectors depending on the dis-
crimination of them. The selection based on the values of the
variances of different C channels of feature maps both pro-
vides boost in performance and enhances the computation
efﬁciency.

Post-processing We perform l2-normalization, PCA com-
pression and whitening on the global representation ψ(I)
subsequently and obtain the ﬁnal M-dimensional represen-
tation ψP W A(I) :

ψP W A(I) = diag(σ1, σ2, · · · , σM )−1V

ψ(I)
(cid:107)ψ(I) (cid:107)2

(6)

where V is the M × N PCA-matrix, M is the number of the
retained dimensionality, and σ1, σ2, · · · , σM are the associ-
ated singular values.

Experiments

Datasets
We evaluate the performance of PWA and other aggregation
algorithms on four standard datasets (Oxford5k, Paris6k,
Oxford105k and Paris106k) for image retrieval.

Oxford5k (Philbin et al. 2007) and Paris6k (Philbin et al.
2008) datasets contain photographs collected from Flickr
associated with Oxford and Paris landmarks respectively.
The performance is measured using mean average precision
(mAP) over the 55 queries annotated manually. Oxford105k
and Paris106k contain the additional 10,000 distractor im-
ages from Flicker (Philbin et al. 2007).

Implementation details
We extract deep convolutional features using the pre-trained
VGG16 (Simonyan and Zisserman 2015) and ﬁne-tuned
ResNet101 from the work (Gordo et al. 2016b). In the exper-
iments, Caffe (Jia et al. 2014) package for CNNs is used. For
VGG16 model, we extract convolutional feature maps from
the pool5 layer and the number of channels is C=512. For
ResNet-101 model, we extract convolutional feature maps
from the res5c−relu layer and the number of channels is
C=2048. Regarding image size, we keep the original size of
the images except for the very large images which are re-
sized to the half size. The parameters for power normaliza-
tion and power-scaling are set as α = 2 and β = 2, through-
out our experiments.

We evaluate the mean average precision (mAP) over the
cropped query. For fair comparison with the related retrieval
methods, we learn the PCA and whitening parameters on
Oxford5k when testing on Paris6k and vice versa.

Impact of the parameters
The main parameters are the numbers of the selected part
detectors and the dimensionality of ﬁnal representations
ψP W A(I).

Select part detectors We employ the discriminative ﬁl-
ters of deep convolutional layers as part detectors to generate
“probabilistic proposals”. The discriminative part detectors
are selected according to the variances of C channels of fea-
ture maps. We also aggregate the responses of convolutional

Table 1: Performance of different number of selected part
detectors (N). We aggregate the responses of convolutional
layers by all the C=512 part detectors as the baseline. Note,
the ﬁnal representation ψP W A(I) is reduced into 4096 di-
mensionality by PCA.

Datasets

Oxford5k

Paris6k

78.5
78.7
79.0
78.7
79.0
78.2
79.1
77.7

85.4
85.7
85.9
86.0
85.4
86.1
86.1
83.8

N

512
450
350
250
150
50
25
10

layers based on all the C part detectors as the baseline. We
show the results of selecting the ﬁrst N part detectors with
the largest variance in Table 1. In this experiment, the ﬁnal
representation ψP W A(I) is reduced into 4096 dimensional-
ity by PCA.

The results show that our PWA representation is not heav-
ily relied on the number of selected part detectors. Select-
ing a small number of part detectors (e.g., N=25), we still
achieve good performance. The selection strategy boosts
above 0.6% mAP than baseline and reduces the computa-
tional cost to 1/20 of the baseline. The results demonstrate
that our straightforward unsupervised selection strategy is
effective.

Table 2: Performance of varying dimensionality (M), into
which the ﬁnal representation is reduced. The representation
is reduced by PCA and whitening. Note, we select 25 part
detectors to aggregate the convolutional features.

Datasets

M Oxford5k

Paris6k

128
256
512
1024
2048
4096

64.5
68.7
72.0
75.3
78.2
79.1

76.9
79.6
82.3
84.2
85.4
86.1

Dimensionality reduction In order to get shorter repre-
sentations, we compress the N × C-dimensional aggregated
representation ψ(I) by PCA and whitening process. Table 2
reports the performance of representations with varying di-
mensionality, M=128 to 4096. We do not reduce the ﬁnal
representation into higher dimensionality because of the lim-
ited number of images in Oxford5k and Paris6k datasets.
We select N=25 part detectors to aggregate the convolutional
features in this experiment.

Table 3: Accuracy comparison with the state-of-the-art unsupervised methods. We compare our PWA+QE with other methods
followed by query expansion at the bottom of table. Part-based weighting aggregation (PWA) consistently outperforms the
state-of-the-art unsupervised aggregation methods.

Dimensionality Oxford5k

Paris6k Oxford105k

Datasets

Method
Tri-embedding (Gou and Zisserman 2014)
FAemb (Do, Tran, and Cheung 2015)
RVD-W (Husain and Bober 2016)
Razavian et al. (Razavian et al. 2016)
Neural Codes (Babenko et al. 2014)
SPoC (Babenko and Lempitsky 2015a)
InterActive (Xie et al. 2016)
R-MAC (Tolias, Sicre, and Jgou 2016)
CroW (Kalantidis, Mellina, and Osindero 2016)
Previous state-of-the-art
PWA
PWA
PWA
PWA
CroW+QE (Kalantidis, Mellina, and Osindero 2016)
R-MAC+AML+QE (Tolias, Sicre, and Jgou 2016)
DSM (Zhong, Zhu, and Hoi 2015)
PWA+QE
PWA+QE
PWA+QE
PWA+QE

8k
16k
16k
512
512
256
512
512
512

512
1024
2048
4096
512
512
—
512
1024
2048
4096

67.6
70.9
68.9
46.2
43.5
53.1
65.6
66.9
70.8
70.8
72.0
75.3
78.2
79.1
74.9
77.3
95.0
74.8
77.9
80.7
81.7

—
—
—
67.4
—
—
79.2
83.0
79.7
83.0
82.3
84.2
85.4
86.1
84.8
86.5
91.5
86.0
87.8
88.7
89.2

61.1
—
66.0
—
39.2
50.1
—
61.6
65.3
65.3
66.2
69.3
71.1
73.6
70.6
73.2
93.2
72.5
76.7
79.3
80.6

Paris106k
—
—
—
—
—
—
—
75.7
72.2
75.7
75.8
78.2
79.7
80.4
79.4
79.8
—
80.7
82.8
83.9
84.7

The results show that the performance boosts gradually
with the increase of dimensionality and the best performance
is achieved at 4096 dimensionality. We get the consistent
conclusion with other methods, the compression leads to the
loss of discriminative information and performance degra-
dation. The previous works (Babenko and Lempitsky 2015a;
Tolias, Sicre, and Jgou 2016; Kalantidis, Mellina, and Osin-
dero 2016) aggregate convolutional features as compressed
representations with dimensionality under 512, but our PWA
representation has more choice of dimensionality. Compared
with (Babenko and Lempitsky 2015a; Tolias, Sicre, and Jgou
2016; Kalantidis, Mellina, and Osindero 2016), our PWA
methods can generate representations with both low and
high dimensionality and achieve better performance on most
datasets. The dimensionality of PWA representation can be
chosen according to the tradeoff between performance and
efﬁciency on different tasks.

Comparison with the state-of-the-art
Unsupervised methods
In the ﬁrst part of Table 3, we
compare our PWA method using pre-trained VGG16 (Si-
monyan and Zisserman 2015) with the state-of-the-art un-
supervised methods, which employ global representations
of images. Our PWA representation signiﬁcantly outper-
form them on all four standard retrieval datasets. In par-
ticular, the gain is more than 8.3% in mAP on Oxford5k
and Oxford105k datasets. The results demonstrate that our
PWA representation weighted by the selected “probabilis-

tic proposals” is effective and discriminative for image re-
trieval. Our 512-dimensional PWA representation is compa-
rable with the previous state-of-the-art, and its results are
only lower than R-MAC (Tolias, Sicre, and Jgou 2016) on
Paris6k. The PWA representation with higher dimensional-
ity (such as 1024, 2048 and 4096) consistently outperform
all of them on all datasets.

We compare other methods that contain query expansion
(QE) and spatial veriﬁcation stages with our approach in
the second part of Table 3. In the experiments, we use av-
erage query expansion (QE) (Chum et al. 2007) computed
by the top 10 query results. Our PWA+QE method performs
better than the related works (Tolias, Sicre, and Jgou 2016;
Kalantidis, Mellina, and Osindero 2016) on all datasets. Al-
though the approximate max pooling localization (AML)
process in R-MAC (Tolias, Sicre, and Jgou 2016) requires
a costly veriﬁcation stage and the extra memory storage,
our PWA+QE still achieves better performance than R-
MAC+AML+QE. DSM (Zhong, Zhu, and Hoi 2015) uses
handcrafted features(SIFT) and achieves better performance
by employing additional time-consuming reranking pro-
cesses ,i.e., spatial veriﬁcation in tf-idf and k-NN reranking.

Supervised methods with end-to-end training We also
compare our method with the current state-of-the-art su-
pervised methods containing end-to-end training process
(Arandjelovic et al. 2016; Radenovic, Tolias, and Chum
2016; Gordo et al. 2016b)) in Table 4. In order to compare

Table 4: Accuracy comparison with the state-of-the-art supervised methods. Employing the convolutional layer features of ﬁne-
tuned network (Gordo et al. 2016b), we achieve the comparable performance with the state-of-the-art methods with end-to-end
supervised training.

Method
NetVLAD (Arandjelovic et al. 2016)
CNNBoW (Radenovic, Tolias, and Chum 2016)
DeepRepresentation (Gordo et al. 2016b)

Previous state-of-the-art

PWA

Dimensionality Oxford5k

Paris6k Oxford105k

Datasets

4096
512
2048

2048

71.6
79.7
86.1

86.1

87.8

79.7
83.8
94.5

94.5

94.9

—
73.9
82.8

82.8

82.8

Paris106k
—
76.4
90.6

90.6

91.0

with them, we employ convolutional layers features of ﬁne-
tuned ResNet101 from the work (Gordo et al. 2016b). Be-
cause these methods (Radenovic, Tolias, and Chum 2016;
Gordo et al. 2016b) map the ﬁnal representation by the su-
pervised methods for similarity evaluation, we also map the
PWA representations for comparison purposes. In order to
keep consistently unsupervised, we utilize the unsupervised
IME layer (Xu et al. 2017) to map our PWA representations
for similarity evaluation.

The results show that our unsupervised PWA repre-
sentation outperforms the state-of-the-art supervised meth-
ods (Arandjelovic et al. 2016; Radenovic, Tolias, and Chum
2016; Gordo et al. 2016b) on all datasets. Furthermore, the
effectiveness of the supervised methods is heavily relied on
the collected training set. However, our unsupervised PWA
method can make better use of the convolutional features
extracted from both pre-trained and ﬁne-tuned CNN model
to represent the images and does not need the further su-
pervised re-training. Considering the fact that the annotated
training dataset is difﬁcult to collect, it is impractical to ﬁne-
tune the model for each discrepant task respectively. Our un-
supervised PWA method is very suitable for this condition.
Our PWA method retains more discriminative information
of the retrieval object parts and signiﬁcantly suppress the
noise of background, and better utilizes the convolutional
features extracted from both pre-trained and ﬁne-tuned CNN
models.

Conclusion
In this paper, we propose a novel PWA method for image
retrieval. The key characteristic of our method is that it em-
ploys discriminative part detectors selected by unsupervised
strategy to generate “probabilistic proposals”. Based on the
selected “probabilistic proposals” corresponding to special
semantic content implicitly, we weight and aggregate the
deep convolutional features extracted from pre-trained or
ﬁne-tuned CNN models. Due to the selected “probabilistic
proposals” corresponding to ﬁxed semantic content but not
ﬁxed position, we concatenate the regional representations
as global PWA representation. The results show that our
PWA representation suppress the noise of background and
highlight the discriminative parts and patterns of retrieval
objects.

Experiments on four standard retrieval datasets demon-
strate that our unsupervised approach outperforms the pre-
vious state-of-the-art unsupervised and supervised aggrega-
tion methods. It is worth noting that our unsupervised PWA
method is very suitable and effective for the situation where
the annotated training dataset is difﬁcult to collect.

Acknowledgments
This work was supported by the National Natural Sci-
ence Foundation of China under Grant 61531019, Grant
61601462, and Grant 71621002.

References
Arandjelovic, R., and Zisserman, A. 2012. Three things everyone
should know to improve object retrieval. In IEEE Conference on
Computer Vision and Pattern Recognition, 2911–2918. IEEE.
Arandjelovic, R.; Gronat, P.; Torii, A.; Pajdla, T.; and Sivic, J.
2016. Netvlad: Cnn architecture for weakly supervised place
recognition. In IEEE Conference on Computer Vision and Pattern
Recognition, 5297–5307.
Babenko, A., and Lempitsky, V. 2015a. Aggregating local deep
features for image retrieval. In IEEE international conference on
computer vision, 1269–1277.
Babenko, A., and Lempitsky, V. 2015b. The inverted multi-index.
IEEE Transactions on Pattern Analysis and Machine Intelligence
37(6):1247–1260.
Babenko, A.; Slesarev, A.; Chigorin, A.; and Lempitsky, V. 2014.
Neural codes for image retrieval. In European conference on com-
puter vision, 584–599. Springer.
Chum, O.; Philbin, J.; Sivic, J.; Isard, M.; and Zisserman, A. 2007.
Total recall: Automatic query expansion with a generative feature
In IEEE International Conference on
model for object retrieval.
Computer Vision, 1–8. IEEE.
Dai, J.; He, K.; and Sun, J. 2016. Instance-aware semantic seg-
mentation via multi-task network cascades. In IEEE Conference
on Computer Vision and Pattern Recognition, 3150–3158.
Do, T.-T.; Tran, Q. D.; and Cheung, N.-M. 2015. Faemb: a func-
tion approximation-based embedding method for image retrieval.
In IEEE Conference on Computer Vision and Pattern Recognition,
3556–3564.
Gong, Y.; Wang, L.; Guo, R.; and Lazebnik, S. 2014. Multi-scale
orderless pooling of deep convolutional activation features. In Eu-
ropean conference on computer vision, 392–407. Springer.

Gordo, A.; Almazan, J.; Revaud, J.; and Larlus, D. 2016a. Deep
image retrieval: Learning global representations for image search.
In European Conference on Computer Vision, 241–257. Springer.
Gordo, A.; Almazan, J.; Revaud, J.; and Larlus, D. 2016b. End-
to-end learning of deep visual representations for image retrieval.
International Journal of Computer Vision 1–18.
Gou, H., and Zisserman, A. 2014. Triangulation embedding and
democratic aggregation for image search. In Computer Vision and
Pattern Recognition, 3310–3317.
He, X., and Peng, Y. 2017. Weakly supervised learning of part se-
lection model with spatial constraints for ﬁne-grained image clas-
siﬁcation. In AAAI, 4075–4081.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2014. Spatial pyramid
pooling in deep convolutional networks for visual recognition. In
European Conference on Computer Vision, 346–361. Springer.
He, K.; Gkioxari, G.; Dollar, P.; and Girshick, R. 2017. Mask r-cnn.
IEEE International Conference on Computer Vision.
Husain, S. S., and Bober, M. 2016.
Improving large-scale im-
age retrieval through robust aggregation of local descriptors. IEEE
Transactions on Pattern Analysis and Machine Intelligence.
Jegou, H.; Perronnin, F.; Douze, M.; Sanchez, J.; Perez, P.; and
Schmid, C. 2012. Aggregating local image descriptors into com-
IEEE transactions on pattern analysis and machine
pact codes.
intelligence 34(9):1704–1716.
Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.; Girshick,
R.; Guadarrama, S.; and Darrell, T. 2014. Caffe: Convolutional
architecture for fast feature embedding. In ACM international con-
ference on Multimedia, 675–678. ACM.
Kalantidis, Y.; Mellina, C.; and Osindero, S.
2016. Cross-
dimensional weighting for aggregated deep convolutional features.
In European Conference on Computer Vision, 685–701. Springer.
LeCun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.;
Hubbard, W.; and Jackel, L. D. 1989. Backpropagation applied to
handwritten zip code recognition. Neural computation 1(4):541–
551.
Li, Y.; Qi, H.; Dai, J.; Ji, X.; and Wei, Y. 2017. Fully convolutional
instance-aware semantic segmentation.
Long, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convolutional
networks for semantic segmentation. In IEEE Conference on Com-
puter Vision and Pattern Recognition, 3431–3440.
Lowe, D. G. 2004. Distinctive image features from scale-invariant
International Journal of Computer Vision 60(60):91–
keypoints.
110.
Perronnin, F., and Dance, C. 2007. Fisher kernels on visual vocab-
ularies for image categorization. In IEEE Conference on Computer
Vision and Pattern Recognition, 1–8.
Perronnin, F.; Nchez, J.; and Mensink, T. 2010.
Improving the
ﬁsher kernel for large-scale image classiﬁcation. In European Con-
ference on Computer Vision, 143–156.
Philbin, J.; Chum, O.; Isard, M.; Sivic, J.; and Zisserman, A. 2007.
Object retrieval with large vocabularies and fast spatial matching.
In IEEE Conference on Computer Vision and Pattern Recognition,
1–8. IEEE.
Philbin, J.; Chum, O.; Isard, M.; Sivic, J.; and Zisserman, A. 2008.
Lost in quantization: Improving particular object retrieval in large
scale image databases. In IEEE Conference on Computer Vision
and Pattern Recognition, 1–8. IEEE.
Radenovic, F.; Tolias, G.; and Chum, O. 2016. Cnn image retrieval
learns from bow: Unsupervised ﬁne-tuning with hard examples. In
European Conference on Computer Vision, 3–20. Springer.

Razavian, A. S.; Sullivan, J.; Carlsson, S.; and Maki, A. 2016.
ITE
Visual instance retrieval with deep convolutional networks.
Transactions on Media Technology and Applications 4(3):251–
258.
Sharif Razavian, A.; Azizpour, H.; Sullivan, J.; and Carlsson, S.
2014. Cnn features off-the-shelf: an astounding baseline for recog-
In IEEE Conference on Computer Vision and Pattern
nition.
Recognition Workshops, 806–813.
Simon, M., and Rodner, E. 2015. Neural activation constella-
tions: Unsupervised part model discovery with convolutional net-
In IEEE International Conference on Computer Vision,
works.
1143–1151.
Simonyan, K., and Zisserman, A. 2015. Very deep convolutional
networks for large-scale image recognition. ICLR.
Sivic, J., and Zisserman, A. 2003. Video google: A text retrieval
approach to object matching in videos. In IEEE International Con-
ference on Computer Vision, 1470.
Tolias, G.; Sicre, R.; and Jgou, H. 2016. Particular object retrieval
with integral max-pooling of cnn activations. ICLR.
Uijlings, J. R.; Van De Sande, K. E.; Gevers, T.; and Smeulders,
A. W. 2013. Selective search for object recognition. International
journal of computer vision 104(2):154–171.
Wei, X.-S.; Luo, J.-H.; Wu, J.; and Zhou, Z.-H. 2017. Selec-
tive convolutional descriptor aggregation for ﬁne-grained image re-
trieval. IEEE Transactions on Image Processing 26(6):2868–2881.
Xiao, T.; Xu, Y.; Yang, K.; Zhang, J.; Peng, Y.; and Zhang, Z. 2015.
The application of two-level attention models in deep convolutional
neural network for ﬁne-grained image classiﬁcation. In IEEE Con-
ference on Computer Vision and Pattern Recognition, 842–850.
Xie, L.; Hong, R.; Zhang, B.; and Tian, Q. 2015. Image classiﬁca-
tion and retrieval are one. In ACM on International Conference on
Multimedia Retrieval, 3–10. ACM.
Xie, L.; Zheng, L.; Wang, J.; Yuille, A. L.; and Tian, Q. 2016.
Interactive: Inter-layer activeness propagation. In IEEE Conference
on Computer Vision and Pattern Recognition, 270–279.
Xu, J.; Wang, C.; Qi, C.; Shi, C.; and Xiao, B. 2017.
Iterative
manifold embedding layer learned by incomplete data for large-
scale image retrieval. arXiv preprint arXiv:1707.09862.
Zeiler, M. D., and Fergus, R. 2014. Visualizing and understand-
ing convolutional networks. In European conference on computer
vision, 818–833. Springer.
Zhang, X.; Xiong, H.; Zhou, W.; Lin, W.; and Tian, Q. 2016a.
Picking deep ﬁlter responses for ﬁne-grained image recognition.
In IEEE Conference on Computer Vision and Pattern Recognition,
1134–1142.
Zhang, Y.; Wei, X.-S.; Wu, J.; Cai, J.; Lu, J.; Nguyen, V.-A.; and
Do, M. N. 2016b. Weakly supervised ﬁne-grained categorization
with part-based image representation. IEEE Transactions on Image
Processing 25(4):1713–1725.
Zhong, Z.; Zhu, J.; and Hoi, S. C. 2015. Fast object retrieval
using direct spatial matching. IEEE Transactions on Multimedia
17(8):1391–1397.
Zhou, W.; Li, H.; and Tian, Q.
2017. Recent advance in
content-based image retrieval: A literature survey. arXiv preprint
arXiv:1706.06064.

