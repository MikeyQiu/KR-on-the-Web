Recent Advances in Efﬁcient Computation of
Deep Convolutional Neural Networks

8
1
0
2
 
b
e
F
 
1
1
 
 
]

V
C
.
s
c
[
 
 
2
v
9
3
9
0
0
.
2
0
8
1
:
v
i
X
r
a

Jian Cheng∗, Peisong Wang, Gang Li, Qinghao Hu, Hanqing Lu
National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences
University of Chinese Academy of Sciences
jcheng@nlpr.ia.ac.cn

Abstract

Deep neural networks have evolved remarkably over the
past few years and they are currently the fundamental tools
of many intelligent systems. At the same time, the compu-
tational complexity and resource consumption of these net-
works also continue to increase. This will pose a signiﬁ-
cant challenge to the deployment of such networks, espe-
cially in real-time applications or on resource-limited de-
vices. Thus, network acceleration has become a hot topic
within the deep learning community. As for hardware im-
plementation of deep neural networks, a batch of acceler-
ators based on FPGA/ASIC have been proposed in recent
years.
In this paper, we provide a comprehensive survey
of recent advances in network acceleration, compression
and accelerator design from both algorithm and hardware
points of view. Speciﬁcally, we provide a thorough analysis
of each of the following topics: network pruning, low-rank
approximation, network quantization, teacher-student net-
works, compact network design and hardware accelerators.
Finally, we will introduce and discuss a few possible future
directions.

1. Introduction

In recent years, deep neural networks (DNNs) have
achieved remarkable performance across a wide range of
applications, including but not limited to computer vision,
natural language processing, speech recognition, etc. These
breakthroughs are closely related to the increased amount
of training data and more powerful computing resources
now available. For example, one breakthrough in the nat-
ural image recognition ﬁeld was achieved by AlexNet [44],
which was trained using multiple graphics processing units
(GPUs) on about 1.2M images. Since then, the performance
of DNNs has continued to improve. For many tasks, DNNs
are reported to be able to outperform humans. The problem,

∗The corresponding author.

however, is that the computational complexity as well as
the storage requirements of these DNNs has also increased
drastically as shown in Table 1. Speciﬁcally, the widely
used VGG-16 model [72] involves more than 500MB of
storage and over 15B FLOPs to classify a single 224 × 224
image.

Thanks to the recent crop of powerful GPUs and CPU
clusters equipped with more abundant memory resources
and computational units, these more powerful DNNs can be
trained within a relatively reasonable time period. However,
when it is time for the inference phase, such a long execu-
tion time is impractical for real-time applications. Recent
years have witnessed great progress in embedded and mo-
bile devices including unmanned drones, smart phones, in-
telligent glasses, etc. The demand for deployment of DNN
models on these devices has become more intense. How-
ever, the resources of these devices, for example, the storage
and computational units as well as the battery power remain
very limited, and this poses a real challenge in accelerating
modern DNNs in low-cost settings.

Therefore, a critical problem currently is how to equip
speciﬁc hardware with efﬁcient deep networks without sig-
niﬁcantly lowering the performance. To deal with this issue,
many great ideas and methods from the algorithm side have
been investigated over the past few years. Some of these
works focused on model compression while others focused
on acceleration or lowering power consumption. As for the
hardware side, a wide variety of FPGA/ASIC-based accel-
erators have been proposed for embedded and mobile appli-
cations. In this paper, we present a comprehensive survey of
several advanced approaches in network compression, ac-
celeration and accelerator design. We will present the cen-
tral ideas behind each approach and explore the similarities
and differences between the different methods. Finally, we
will present some future directions in the ﬁeld.

The rest of this paper is organized as follows. In Sec-
tion 2, we give some background on network acceleration
and compression. From Section 3 to Section 7, we sys-
tematically describe a series of hardware-efﬁcient DNN al-

gorithms, including network pruning, low-rank approxima-
tion, network quantization, teacher-student networks and
compact network design. In Section 8, we introduce the de-
sign and implementation of hardware accelerators based on
FPGA/ASIC technologies. In Section 9, we discuss some
future directions in the ﬁeld, and Section 10 concludes the
paper.

2. Background

Recently, deep convolutional neural networks (CNNs)
have become quite popular due to their powerful represen-
tational capacity. With the huge success of CNNs, the de-
mand for deployment of deep networks in real world ap-
plications has continued to increase. However, the large
storage consumption and computational complexity remain
two key problems for deployment of these networks. For
the CNN training phase, the computational complexity is
not a critical problem thanks to the high performance GPUs
or CPU clouds. The large storage consumption also has less
effect on the training phase because modern computers have
very large disk and memory storage capacities. However,
things are quite different for the inference phase in CNNs,
especially with regard to embedded and mobile devices.

The enormous computational complexity introduces two
problems in the deployment of CNNs in real-world appli-
cations. One is that the CNN inference phase slows down
as the computational complexity grows larger. This makes
it difﬁcult to deploy CNNs in real-time applications. The
other problem is that the dense computation inherent to
CNNs will consume substantial battery power, which is lim-
ited on mobile devices.

The large number of parameters of CNNs consumes con-
siderable storage and run-time memory, which are quite
limited on embedded devices. In addition, it becomes more
difﬁcult to download new models online on mobile devices.
To solve these problems, network compression and ac-
celeration methods have been proposed.
In general, the
computational complexity of CNNs is dominated by the
convolutional layers, while the number of parameters is
mainly related to the fully connected layers as shown in
Table 1. Thus, most network acceleration methods focus
on decreasing the computational complexity of the con-
volutional layers, while the network compression methods
mainly try to compress the fully connected layers.

3. Network Pruning

Pruning methods were proposed before deep learning be-
came popular, and they have been widely studied in recent
years [47, 24, 22, 23]. Based on the assumption that many
parameters in deep networks are unimportant or unneces-
sary, pruning methods are used to remove the unimportant
parameters. In this way, pruning methods can expand the

sparsity of the parameters signiﬁcantly. The high sparsity
of the parameters after pruning introduces two beneﬁts for
deep neural networks. On the one hand, the sparse param-
eters after pruning require less disk storage since the pa-
rameters can be stored in the compressed sparse row format
(CSR) or compressed sparse column (CSC) format. On the
other hand, computations involving those pruned parame-
ters are omitted; thus, the computational complexity of deep
networks can be reduced. According to the granularity of
the pruning, pruning methods can be categorized into ﬁve
groups: ﬁne-grained pruning, vector-level pruning, kernel-
level pruning, group-level pruning and ﬁlter-level pruning.
Figure. 1 shows the pruning methods with their different
granularities. In the following subsections, we describe the
different pruning methods in detail.

3.1. Fine-grained Pruning

Fine-grained pruning methods or vanilla pruning meth-
ods remove parameters in an unstructured way, i.e., any
unimportant parameters in the convolutional kernels can
be pruned, as shown in Figure. 1. Since there are no ex-
tra constraints on the pruning patterns, the parameters can
be pruned with a high sparsity. Early works on pruning
[47, 24] used the approximate second-order derivativeses
of the loss function w.r.t. the parameters to determine the
saliency of the parameters, and then pruned those param-
eters with low saliency. Yet, deep networks can ill afford
to compute the second order derivativeses due to the huge
computational complexity. Recently [22] proposed a deep
compression framework to compress deep neural networks
in three steps: pruning, quantization, and Huffman encod-
ing. By using this method, AlexNet could be compressed by
35× without drops in accuracy. After pruning, the pruned
parameters in [22] remain unchanged, incorrectly pruned
parameters could cause accuracy drops. To solve this prob-
lem, [17] proposed a dynamic network surgery framework,
which consists of two operations: pruning and splicing. The
pruning operation aims to prune those unimportant param-
eters while the splicing operation aims to recover the in-
correctly pruned connections. Their method requires fewer
training epochs and achieves a better compression ratio than
[22].

3.2. Vector-level and Kernel-level Pruning

Vector-level pruning methods prune vectors in the con-
volutional kernels, and kernel-level pruning methods prune
2-D convolutional kernels in the ﬁlters. Since most pruning
methods focus on ﬁne-grained pruning or ﬁlter-level prun-
ing, there are few works on vector-level and kernel-level
pruning. [3] ﬁrst explored the kernel-level pruning, and then
proposed an intra-kernel strided pruning method, which
prunes a sub-vector in a ﬁxed stride. [58] explored differ-
ent granularity levels in pruning, and found that vector-level

Table 1. The computation and parameters for state-of-art convolution neural networks

Parameters
Size(M) Conv(%)

Computation
FLOPS(G) Conv(%)

Method

AlexNet
VGG-S
VGG16
NIN
GoogLeNet
ResNet-18
ResNet-50
ResNet-101

61
103
138
7.6
6.9
5.6
12.2
21.2

3.8
6.3
10.6
100
85.1
100
100
100

Fc(%)
96.2
93.7
89.4
0
14.9
0
0
0

0.72
2.6
15.5
1.1
1.6
1.8
3.8
7.6

91.9
96.3
99.2
100
99.9
100
100
100

Fc(%)
8.1
3.7
0.8
0
0.1
0
0
0

implemented by thinned dense matrices multiplication. As
a result, the Basic Linear Algebra Subprograms (BLAS)
can be utilized to achieve a higher speed-up. [46] proposed
the group-wise brain damage approach, which prunes the
weight matrix in a group-wise fashion. By using group-
sparsity regularization, deep networks can be trained easily
with group-sparsiﬁed parameters. Since group-level prun-
ing can utilize the BLAS library, the practical speed-up is
almost linear at the sparsity level. By using this method,
they achieved a 3.2× speed-up for all convolutional layers
in AlexNet. Concurrent with [46], [85] proposed using the
group Lasso to prune groups of parameters. In contrast, [85]
explored different levels of structured sparsity in terms of
ﬁlters, channels, ﬁlter shapes, and depth. Their method can
be regarded as a more general group-regularized pruning
method. For AlexNet’s convolutional layers, their method
achieves about 5.1× and 3.1× speed-ups on a CPU and
GPU respectively.

3.4. Filter-level Pruning

Filter-level pruning methods prune the convolutional ﬁl-
ters or channels which make the deep networks thinner. Af-
ter the ﬁlter pruning for one layer, the number of input chan-
nels of the next layer is also reduced. Thus, ﬁlter-level prun-
ing is more efﬁcient for accelerating deep networks. [54]
proposed a ﬁlter-level pruning method named ThiNet. They
used the next layer’s feature map to guide the ﬁlter prun-
ing in the current layer. By minimizing the feature map’s
reconstruction errors, they select the channels in a greedy
way. Similar to [54], [26] proposed an iterative two-step
algorithm to prune ﬁlters by minimizing the feature map
errors. Speciﬁcally, they introduced a selection weight βi
for each ﬁlter Wi, then added sparse constraints on βi.
Then the channel selection problem can be casted into a
LASSO regression problem. To minimize the feature map
errors, they iteratively updated β and W . Moreover, their
method achieved a 5× speed-up on VGG-16 network with
little drop in accuracy. Instead of using additional selection
weight β, [53] proposed to leverage the scaling factor of the
batch normalization layer for to evaluate the importance of

Figure 1. Different pruning methods for a convolutional layer
which has 3 convolutional ﬁlters of size 3 × 3 × 3.

Figure 2. Group-level Pruning.

pruning takes up less storage than ﬁne-grained pruning be-
cause vector-level pruning requires fewer indices to indicate
the pruned parameters. Nevertheless, vector-level, kernel-
level, and ﬁlter-level pruning techniques are friendlier in
hardware implementations since they are the more struc-
tured pruning methods.

3.3. Group-level Pruning

Group-level pruning methods prune the parameters ac-
cording to the same sparse pattern on the ﬁlters. As shown
in Figure 2, each ﬁlter has the same sparsity pattern, thus the
convolutional ﬁlters can be represented as a thinned dense
matrix. By using group-level pruning, convolutions can be

the ﬁlters. By pruning the channels with near-zero scaling
factors, they can prune ﬁlters without introducing overhead
into the networks.

4. Low-rank Approximation

The convolutional kernel of a convolutional layer W ∈
Rw×h×c×n is a 4-D tensor. These four dimensions corre-
spond to the kernel width, kernel height and the number of
input and output channels respectively. Note that by merg-
ing some of the dimensions, the 4-D tensor can be trans-
formed into a t-D (t = 1, · · · 4) tensor. The motivation
behind low-rank decomposition is to ﬁnd an approximate
tensor ˆW that is close to W but facilitates more efﬁcient
computation. Many low-rank based methods have been pro-
posed by the community; two key differences are in how
to rearrange the four dimensions, and on which dimension
the low-rank constraint is imposed. Here we roughly divide
the low-rank based methods into three categories, accord-
ing to how many components the ﬁlters are decomposed
into: two-component decomposition, three-component de-
composition and four-component decomposition.

4.1. Two-component Decomposition

For two-component decomposition, the weight tensor is
divided into two parts and the convolutional layer is re-
placed by two successive layers. [35] decomposed the spa-
tial dimension w∗h into w∗1 and 1∗h ﬁlters. They achieved
a 4.5× speedup for a CNN trained on a text character recog-
nition dataset, with a 1% accuracy drop.

SVD is a popular

low-rank matrix decomposition
method. By merging the dimensions w, h and c, the ker-
nel becomes a 2-D matrix of size (w ∗ h ∗ c) × n, on which
the SVD decomposition method can be conducted. In [11],
the authors utilized SVD to reduce the network redundancy.
SVD decomposition was also investigated in [97], in which
the ﬁlters were replaced by two ﬁlter banks: one consisting
of d ﬁlters of shape w × h × c and the other composed of
n ﬁlters of shape 1 × 1 × d. Here, d represents the rank
of the decomposition, i.e., the n ﬁlters are linear combina-
tions of the ﬁrst d ﬁlters. They also proposed the non-linear
response reconstruction method based on the low-rank de-
composition. On the challenging VGG-16 model for the
ImageNet classiﬁcation task, this two-component SVD de-
composition method achieved a 3× theoretical speedup at a
cost of about 1.66% increased top-5 error.

Similarly, another SVD decomposition method can be
used by exploring the low-rank property along the input
channel dimension c. In this way, we reshape the weight
tensor into a matrix of size c × (w ∗ h ∗ n). By selecting
the rank to d, the convolution can be decomposed ﬁrst by a
1 × 1 × c × d convolution and then by a w × h × d × n
convolution. These two decomposition are symmetric.

4.2. Three-component Decomposition

Based on the analysis of two-component decomposi-
tion methods, one straightforward three-component decom-
position method can be obtained by two successive two-
component decompositions. Note that in the SVD decom-
position, two weight tensors are introduced. The ﬁrst is a
w × h × c × d tensor and the other is a d × n tensor (ma-
trix). The ﬁrst convolution is also very time consuming due
to the large size of the ﬁrst tensor. We can also conduct
a two-component decomposition on the ﬁrst weight ten-
sor after the SVD decomposition, which turns into a three-
component decomposition method. This strategy was stud-
ied by [97], whereby after the SVD decomposition, they
utilized the decomposition method proposed by [35] for the
ﬁrst decomposed tensor. Thus, the ﬁnal three components
were convolutions with a spatial size of w × 1, 1 × h, and
1 × 1, respectively. By utilizing this three-component de-
composition, only a 0.3% increased top-5 error was pro-
duced in [97] for a 4× theoretical speedup.

If we use the SVD decomposition along the input chan-
nel dimension for the ﬁrst tensor after the two-component
decomposition, we can get the Tucker decomposition for-
mat as proposed by [41]. These three components are
convolutions of a spatial size 1 × 1, w × h and another
1 × 1 convolution. Note that instead of using the two-
step SVD decomposition, [41] utilized the Tucker decom-
position method directly to obtain these three components.
Their method achieved a 4.93× theoretical speedup at a cost
of 0.5% increased top-5 accuracy.

To further reduce complexity, [80] proposed a Block-
Term Decomposition (BTD) method based on low-rank and
group sparse decomposition. Note that in the Tucker de-
composition, the second component corresponding to the
w × h convolution also requires a large number of com-
putations. Because the second tensor is already low rank
along both the input and output channel dimensions, the de-
composition methods discussed above cannot be used any
longer. [80] proposed to approximate the original weight
tensor by the sum of some smaller subtensors, each of
which is in the Tucker decomposition format. By rearrang-
ing these subtensors, the BTD can be seen as a Tucker de-
composition where the second decomposed tensor is a block
diagonal tensor. By using this decomposition, they achieved
a 7.4% actual speedup for the VGG-16 model, at a cost
of a 1.3% increased in the top-5 error. Their method also
achieved high speedup for object detection and image re-
trieval tasks as reported in [82].

4.3. Four-component Decomposition

By exploring the low-rank property along the in-
put/output channel dimension as well as the spatial dimen-
sion, a four-component decomposition can be obtained.
This is corresponds to the CP-decomposition acceleration

method proposed in [45]. In this way, the four components
are convolutions of size 1 × 1, w × 1, 1 × h and 1 × 1.
The CP-decomposition can achieve a very high speedup ra-
tio, however, due to the approximate error, only the second
layer of AlexNet was processed in [45]. They achieved a
4.5× speedup for the second layer of AlexNet at a cost of
about a 1% accuracy drop.

5. Network Quantization

Quantization is an approach for many compression and
acceleration applications. It has wide applications in image
compression, information retrieval, etc. Many quantization
methods have also been investigated for network acceler-
ation and compression. We can categorize these methods
into two main groups: (1) scalar and vector quantization,
which may need a codebook for quantization, and (2) ﬁxed-
point quantization.

5.1. Scalar and Vector Quantization

Scalar and vector quantization techniques have a long
history, and they were originally used for data compression.
By using scalar or vector quantization, the original data
can be represented by a codebook and a set of quantization
codes. The codebook contains a set of quantization centers,
and the quantization codes are used to indicate the assign-
ment of the quantization centers. In general, the number of
quantization centers is far smaller than the amount of orig-
inal data. In addition, quantization codes can be encoded
through a lossless encoding method, e.g., Huffman coding,
or just represented as low-bit ﬁxed points. Thus, scalar or
vector quantization can achieve a high compression ratio.
[15] explored scalar and vector quantization techniques for
compressing deep networks. For scalar quantization, they
used the well-known K-means algorithm to compress the
parameters. In addition, the product quantization algorithm
(PQ) [36], a special case of vector quantization, was lever-
aged to compress the fully connected layers. By partitioning
the feature space into several disjoint subspaces and then
conducting K-means in each subspace, the PQ algorithm
can compress the fully connected layers with little loss. As
[15] only compressed the fully connected layers, in [86] and
[8], the authors proposed to utilize the PQ algorithm to si-
multaneously accelerate and compress convolutional neural
networks. They proposed to quantize the convolutional ﬁl-
ters layer by layer by minimizing the feature map’s recon-
struction loss. During the inference phase, a look-up table
is built by pre-computing the inner product between feature
map patches and codebooks, then the output feature map
can be calculated by simply accessing the look-up table. By
using this method, they can achieve 4 ∼ 6× speedup and 15
∼ 20× compression ratio with little accuracy loss.

5.2. Fixed-point Quantization

Fixed-point quantization is an effective approach for
lowering the resource consumption of a network. Based on
which part is quantized, two main categories can be clas-
siﬁed, i.e., weight quantization and activation quantization.
There are some other works that try to also quantize gradi-
ents, which can result in acceleration at the network train-
ing stage. Here, we mainly review weight quantization and
activation quantization methods, which accelerate the test-
phase computation. Table 2 summarizes these methods ac-
cording to which part is quantized and whether the training
and testing stages can be accelerated.

5.2.1 Fixed-point Quantization of Weights

Fixed-point weight quantization is a fairly mature topic in
[19] proposed a
network acceleration and compression.
VLSI architecture for network acceleration using 8-bit in-
put and output, and 16-bit internal representation. In [28],
the authors provided a theoretical analysis of error caused
by low-bit quantization to determine the bitwidth for a mul-
tilayer perceptron. They showed that 8∼16 bit quantization
was sufﬁcient for training small neural networks. These
early works mainly focused on simple multilayer percep-
trons. A more recent work [6] showed that it is necessary
to use 32-bit ﬁxed-point for the convergence of a convolu-
tional neural network trained on MNIST dataset. By using
stochastic rounding, the work by [18] found that it is sufﬁ-
cient to use 16-bit ﬁxed-point numbers to train a convolu-
tional neural network on MNIST. In addition, 8-bit ﬁxed-
point quantization was also investigated in [12] to speed up
the convergence of deep networks in parallel training. Log-
arithmic data representation was also investigated in [59].

Recently, much lower bit quantization or even binary
and ternary quantization methods have been investigated.
Expectation Backpropagation (EBP) was introduced in [9],
which utilized the variational Bayes method to binarize
the network. The BinaryConnect method proposed in [10]
constrained all weights to be either +1 or -1. By train-
ing from scratch, the BinaryConnect can even outperform
the ﬂoating-point counterpart on the CIFAR-10 [43] image
classiﬁcation dataset. Using binary quantization, the net-
work can be compressed about 32 times compared to 32-bit
ﬂoating-point networks. Most of the ﬂoating-point multi-
plication can also be eliminated [51]. In [65], the authors
proposed the Binary Weight Network (BWN), which was
among the earliest works that achieved good results on the
large ImageNet [68] dataset. Loss-aware binarization was
proposed in ([30]), which can directly minimize the clas-
siﬁcation loss with respect to the binarized weights.
In
the work of [32], the authors proposed a novel approach
called BWNH to train Binary Weight Networks via Hash-
ing, which outperformed other weight binarization methods

Table 2. Comparison of ﬁxed-point quantization methods according to which part is quantized and whether the training and testing stages
can be accelerated.

Method

BinaryConnect [10]
BWN [65]
BWNH [32]
TWN [48]
FFN [81]
INQ [99]
BNN [65]
XNOR [65]
HWGQ [4]
DoReFa-Net [100]

Weight
Binary
Binary
Binary
Binary
Ternary
Ternary-5bit
Binary
Binary
Binary
Binary

Quantization
Activation
Full
Full
Full
Full
Full
Full
Binary
Binary
2bit
1-4bit

Gradient
Full
Full
Full
Full
Full
Full
Full
Full
Full
6bit, 8bit, Full

Acceleration

Training
No
No
No
No
No
No
No
No
No
Yes

Testing
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

by a large margin. Ternary quantization was also utilized in
[33]. In [48], the authors proposed the Ternary Weight Net-
work (TWN), which was similar to BWN, but constrained
all weights to be ternary values among {-1, 0, +1}. The
TWN outperformed BWN by a large margin on deep mod-
els like ResNet. Trained Ternary Quantization proposed in
[101] learned both ternary values and ternary assignments at
the same time using back-propagation. They achieved com-
parable results on the AlexNet model. Different from previ-
ous quantization methods, the Incremental Network Quan-
tization (INQ) method proposed in [99] gradually turned
all weights into a logarithmic format in a multi-step man-
ner. This incremental quantization strategy can lower the
quantization error during each stage, and thus can make
the quantization problem much easier. All these low-bit
quantization methods discussed above directly quantize the
full-precision weight into a ﬁxed-point format. In [81], the
authors proposed a very different quantization strategy. In
stead of direct quantization, they proposed using a ﬁxed-
point factorized network (FFN) to quantize all weights into
ternary values. This ﬁxed-point decomposition method can
signiﬁcantly lower the quantization error. The FFN method
achieved comparable results on commonly used deep mod-
els such as AlexNet, VGG-16 and ResNet.

5.2.2 Fixed-point Quantization of Activations

Given only weight quantization, there is also a need for the
time-consuming ﬂoating-point operations. If the activations
were also quantized into ﬁxed-point values, the network
can be efﬁciently executed by only ﬁxed-point operations.
Many activation quantization methods were also proposed
by the deep learning community. The bitwise neural net-
work was proposed in [40]. Binarized Neural Networks
(BNN) were among the ﬁrst works that quantized both
weights and activations into either -1 or +1. BNN achieved
a comparable accuracy with the full-precision baseline on
the CIFAR-10 dataset. To extend the BNN for the Ima-
geNet classiﬁcation task, the authors in [75] improved the

training strategies of the BNN. Much higher accuracy was
reported using these strategies. Based on the BWN, the
authors in [65] further quantize all activations into binary
values, making the network into a XNOR-Net. Compared
with BNN, the XNOR-Net can achieve much higher accu-
racy on the ImageNet dataset. To further understand the
effect of bit-width on the training of deep neural networks,
the DoReFa-Net was proposed in [100]. It investigated the
effect of different bit-widths for weights and activations as
well as gradients. By making use of batch normalization,
the work by [4] presented the Half-wave Gaussian Quanti-
zation (HWGQ) method to quantize both weights and ac-
tivations. A high performance was achieved on commonly
used CNN models using the HWGQ methond, with 2-bit
activations and binary weights.

6. Teacher-student Network

The teacher-student network is different from the net-
work compression or acceleration methods since it trains
a student network using a teacher network and the student
network can be designed with a different network architec-
ture. Generally speaking, a teacher network is a large neural
network or the ensemble of neural networks while a student
network is a compact and efﬁcient neural network. By uti-
lizing the dark knowledge transferred from the teacher net-
work, the student network can achieve higher accuracy than
training merely through the class labels. [27] proposed the
knowledge distillation (KD) method which trains a student
network by the softmax layer’s output of the teacher net-
work. Following this line of thinking, [67] proposed the Fit-
Nets to train a deeper and thinner student network. Since the
depth of neural networks is more important than the width
of them, a deeper student network would have higher ac-
curacy. Besides, they utilized both intermediate layers’ fea-
turemaps and soft outputs of the teacher network to train the
student network. Rather than mimicking the intermediate
layers’ feature maps, [91] proposed to train a student net-
work by imitating the attention maps of a teacher network.

Their experiments showed that the attention maps are more
important than the layers’ activations and their method can
achieve higher accuracy than FitNets.

7. Compact Network Design

The objective of network acceleration and compression
is to optimize the execution and storage framework for a
given deep neural network. One property is that the network
architecture is not changed. Another parallel line of inquiry
for network acceleration and compression is to design more
efﬁcient but low-cost network architecture itself.

In [50], the authors proposed Network-In-Network ar-
chitecture, where a 1×1 convolution was utilized to increase
the network capacity while keeping the overall computa-
tional complexity small. To reduce the storage requirement
of the CNN models, they also proposed to remove the fully
connected layer and make use of a global average pooling.
These strategies are also used by many state-of-the-art CNN
models like GoogLeNet [74] and ResNet [25].

Branching (multiple group convolution) is another com-
monly used strategy for lowering network complexity,
which was explored in the work of GoogLeNet [74]. By
largely making use of 1 × 1 convolution and the branching
strategy, the SqueezeNet proposed in [34] achieved about
50× compression over AlexNet, with comparable accu-
racy. By branching, the work of ResNeXt work of [89]
can achieve much higher accuracy than the ResNet [25]
at the same computational budget. The depth-wise con-
volution proposed in MobileNet [31] takes the branching
strategy to the extreme, i.e., the number of branches equals
the number of input/output channels. The resulting Mo-
bileNet can be 32× smaller and 27× faster than the VGG-
16 model, with comparable image classiﬁcation accuracy
on ImageNet. When using depth-wise convolution and 1×1
convolution as in MobileNet, most of the computation and
parameters reside in the 1 × 1 convolutions. One strat-
egy to further lower the complexity of the 1 × 1 convolu-
tion is to use multiple groups. The ShufﬂeNet proposed in
[96] introduced the channel shufﬂe operation to increase the
information change within the multiple groups, which can
prominently increase the representational power of the net-
works. Their method achieved about 13× actual speedup
over AlexNet with comparable accuracy.

8. Hardware Accelerator

8.1. Background

Deep neural networks provide impressive performance
for various tasks while suffering from degrees of computa-
tional complexity. Traditionally, algorithms based on deep
neural networks should be executed on general purpose
platforms such as CPUs and GPUs, but this works at the
expense of unexpected power consumption and oversized

Figure 3. General architecture of an accelerator on dedicated hard-
ware.

resource utilization for both computing and storage. In re-
cent years, there are an increasing number of applications
that are based on embedded systems, including autonomous
vehicles, unmanned drones, security cameras, etc. Con-
sidering the demands for high performance, light weight
and low power consumption on these devices, CPU/GPU-
based solutions are no longer suitable.
In this scenario,
FPGA/ASIC-based hardware accelerators are gaining pop-
ularity as efﬁcient alternatives.

8.2. General Architecture

The deployment of a DNN on a real-world application
training and inference. Network
consists of two phases:
training is known to be expensive in terms of speed and
memory, thus it is usually carried out on GPUs off-line.
During the inference phase, the pre-trained network param-
eters can be loaded either from the cloud or from dedicated
off-chip memory. Most recently, hardware accelerators for
training have received widespread attention [42, 90, 79], but
in this section we mainly focus on the inference phase in
embedded settings.

Typically, an accelerator is composed of ﬁve parts: data
buffers, parameter buffers, processing elements, global con-
troller and off-chip transfer manager, as shown in Figure. 3.
The data buffers are used to caching input images, interme-
diate data and output predictions, while the weight buffers
are mainly used to cache convolutional ﬁlters. Processing
elements are a collection of basic computing units that ex-
ecute multiply-adds, non-linearity and any other functions
such as normalization, quantization, etc. The global con-
troller is used to orchestrate the computing ﬂow on-chip,
while off-chip transfers of data and instructions are con-
ducted through a manager. This basic architecture can be
found in existing accelerators designed for both speciﬁc and
general tasks.

Heterogeneous computing is widely adopted in hard-

ware acceleration. For computing-intensive operations such
as multiply-adds, it is efﬁcient to ﬁt them on hardware
for high throughout, otherwise, data pre-processing, soft-
max and any other graphic operations can be placed on the
CPU/GPU for low latency processing.

8.3. Processing Elements

Among all of the accelerators, the biggest differences ex-
ist in the processing elements as they are designed for the
majority of computing tasks in deep networks, such as mas-
sive multiply-add operations, normalization (batch normal-
ization or local response normalization), and non-linearities
(ReLU, sigmoid and tanh). Typically, the computing engine
of an accelerator is composed of many small basic process-
ing elements, as shown in Figure. 3, and this architecture is
mainly designed for fully investing in data reuse and paral-
lelism. However, there are many accelerators that operate
with only one processing element in consideration of lower
data movement and resource conservation [92, 57].

8.4. Optimizing for High Throughput

Since the majority of the computations in a network
are matrix-matrix/matrix-vector multiplication, it is criti-
cal to deal with the massive nested loops to achieve high
throughput. Loop optimization is one of the most frequently
adopted techniques in accelerator design [92, 56, 73, 2, 88,
49], including loop tiling, loop unrolling, loop interchange,
etc. Loop tiling is used to divide all of the data into mul-
tiple small blocks in order to alleviate the pressure of on-
chip storage [56, 2, 64], while loop unrolling attempts to
improve the parallelism of the computing engine for high
speed [56, 64]. Loop interchange determines the sequen-
tial computation order of the nested loops because different
computation orders can result in signiﬁcant differences in
performance. The well-known systolic array can be seen
as a combination of the loop optimization methods listed
above, which leverage the nature of data locality and weight
sharing in the network to achieve high throughput [37, 84].
SIMD-based computation is another way for high
throughput. [60] presented a method for packing two low-
bit multiplications into a single DSP block to double the
computation, and [63] also proposed a SIMD-based archi-
tecture for speech recognition.

8.5. Optimizing for Low Energy Consumption

Existing works attempt to reduce the energy consump-
tion of a hardware accelerator from both computing and
I/O perspectives.
[29] systematically illustrated the en-
ergy cost in terms of arithmetic operations and memory
accesses. He demonstrated that operations based on inte-
gers are much more cheaper than their ﬂoat-point counter-
parts, and lower bit integers are better. Therefore, most ex-
isting accelerators adopt low-bit or even binary data repre-

sentation [98, 77, 61] to preserve energy efﬁciency. Most
recently, logarithmic computation that transfers multiplica-
tions into bit shift operations has also shown its promise in
energy savings [13, 16, 76].

Sparsity is gaining an increased popularity in accelera-
tor design based on the observation that a great number of
arithmetic operations can be discarded to obtain energy efﬁ-
ciency. [21], [20] and [62] designed architectures for image
or speech recognition based on network pruning, while [1]
and [95] proposed to eliminate ineffectual operations based
on the inherent sparsity in networks.

Off-chip data transfers happen inordinately in hardware
accelerators due to the fact that both network parameters
and intermediate data are too large to ﬁt on chip. [29] sug-
gested that power consumption caused by DRAM access is
several orders of magnitude of the SRAM, and therefore re-
ducing off-chip transfers is a critical issue. [70] designed a
ﬂexible data bufﬁng scheme to reduce bandwidth require-
ments, and [2] and [88] proposed a fusion-based method
to reduce off-chip trafﬁc. Most recently, [49] presented
a block-based convolution that can completely avoid off-
chip transfers of intermediate data in VGG-16 with high
throughput.

Many other approaches have been proposed to reduce
power consumption. [94] used a pipelined FPGA cluster to
realize acceleration, [7] presented an energy-efﬁcient row
stationary scheme to reduce data movements, and [102] at-
tempted to reduce power consumption via low-rank approx-
imation.

8.6. Design Automation

Recently, design automation frameworks that automati-
cally map deep neural networks onto hardware are receiving
wider attention. [83], [69], [78] and [84] proposed frame-
works that automatically generate synthesizable accelerator
for a given network. [55] presented an RTL compiler for
FPGA implementation of diverse networks. [52] proposed
an instruction set for hardware implementation, while [93]
proposed a uniformed convolutional matrix multiplication
representation for CNNs.

8.7. Emerging Techniques

In the past few years, there have been many new tech-
niques from both the algorithm side and the circuit side that
have been adopted to implement fast and energy-efﬁcient
accelerators. Stochastic computing representing continuous
values through streams of random bits have been investi-
gated for hardware acceleration of deep neural networks
[66, 71, 39]. On the hardware side, RRAM-based accel-
erators [5, 87] and the usage of 3-D DRAM [38, 14] have
received greater attention.

9. Future Trends and Discussion

In this section, we discuss some possible future direc-

tions in this ﬁeld.

Non-ﬁne-tuning or Unsupervised Compression. Most
of the existing methods, including network pruning, low-
rank compression and quantization, need labeled data to re-
train the network for accuracy retention. The problems are
twofold. First, labeled data is sometimes unavailable, as in
medical images. Another problem is that retraining requires
considerable human effort as well as professional knowl-
edge. These two problems raise the need for unsupervised
compression or even ﬁne-tuning-free compression methods.

Scalable (Self-adaptive) Compression. Current com-
pression methods have many hyperparameters that need to
be determined ahead of time. For example, the sparsity
of the network pruning, the rank of the decomposition-
based methods or the bitwidth of ﬁxed-point quantization
methods. The selection of these hyperparameters is tedious
work, which also requires professional experience. Thus,
the investigation of methods that do not rely on human-
designed hyperparameters is a promising research topic.
One direction may be to use annealing methods, or rein-
forcement learning.

Network Acceleration for Object Detection. Most of
the model acceleration methods are optimized for image
classiﬁcation, yet very little effort has been devoted to the
acceleration of other computer vision tasks such object de-
tection. It seems that model acceleration methods for im-
age classiﬁcation can be directly used for detection. How-
ever, the deep neural networks for object detection or im-
age segmentation are more sensitive to model acceleration
methods, i.e., using the same model acceleration methods
for object detection would suffer from a greater number of
accuracy drops than with image classiﬁcation. One reason
for this phenomenon may be that object detection requires
more complex feature representation than image classiﬁca-
tion. The design of model acceleration methods for object
detection represents a challenge.

Hardware-software Co-design. To accelerate deep
learning algorithms on dedicated hardware, a straight-
forward method is to pick up a model and design a
corresponding architecture. However, the gap between al-
gorithm modeling and hardware implementation will make
it difﬁcult to put this into practice. Recent advances in deep
learning algorithms and hardware accelerators demonstrate
that
is highly desirable to design hardware-efﬁcient
algorithms according to the low-level features of speciﬁc

it

hardware platforms. This co-design methodology will be a
trend in future work.

10. Conclusion

Deep neural networks provide impressive performance
while suffering from huge computational complexity and
high energy expenditure. In this paper, we provide a sur-
vey of recent advances in efﬁcient processing of deep neu-
ral networks from both the algorithm and hardware points
of view. In addition, we point out a few topics that deserve
further investigation in the future.

References

[1] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E.
Jerger, and A. Moshovos. Cnvlutin: Ineffectual-neuron-free
deep neural network computing. In International Sympo-
sium on Computer Architecture, pages 1–13, 2016.

[2] M. Alwani, H. Chen, M. Ferdman, and P. A. Milder. Fused-

layer cnn accelerators. In MICRO, 2016.

[3] S. Anwar, K. Hwang, and W. Sung. Structured prun-
ing of deep convolutional neural networks. ACM Journal
on Emerging Technologies in Computing Systems (JETC),
13(3):32, 2017.

[4] Z. Cai, X. He, J. Sun, and N. Vasconcelos. Deep learning
with low precision by half-wave gaussian quantization. July
2017.

[5] L. Chen, J. Li, Y. Chen, Q. Deng, J. Shen, X. Liang,
and L. Jiang. Accelerator-friendly neural-network training:
Learning variations and defects in rram crossbar. In Design,
Automation and Test in Europe Conference and Exhibition,
pages 19–24, 2017.

[6] Y. Chen, N. Sun, O. Temam, T. Luo, S. Liu, S. Zhang,
L. He, J. Wang, L. Li, and T. Chen. Dadiannao: A machine-
learning supercomputer. In Ieee/acm International Sympo-
sium on Microarchitecture, pages 609–622, 2014.

[7] Y. H. Chen, T. Krishna, J. S. Emer, and V. Sze. Eyeriss: An
energy-efﬁcient reconﬁgurable accelerator for deep convo-
lutional neural networks. IEEE Journal of Solid-State Cir-
cuits, 52(1):127–138, 2017.

[8] J. Cheng, J. Wu, C. Leng, Y. Wang, and Q. Hu. Quantized
cnn: A uniﬁed approach to accelerate and compress convo-
lutional networks. IEEE Transactions on Neural Networks
and Learning Systems (TNNLS), PP:1–14.

[9] Z. Cheng, D. Soudry, Z. Mao, and Z. Lan. Training
binary multilayer neural networks for image classiﬁca-
arXiv preprint
tion using expectation backpropagation.
arXiv:1503.03562, 2015.

[10] M. Courbariaux, Y. Bengio, and J.-P. David. Binarycon-
nect: Training deep neural networks with binary weights
In Advances in Neural Information
during propagations.
Processing Systems, pages 3123–3131, 2015.

[11] M. Denil, B. Shakibi, L. Dinh, N. de Freitas, et al. Pre-
dicting parameters in deep learning. In Advances in Neural
Information Processing Systems, pages 2148–2156, 2013.

[12] T. Dettmers. 8-bit approximations for parallelism in deep

learning. arXiv preprint arXiv:1511.04561, 2015.

[13] Edward. Lognet: Energy-efﬁcient neural networks us-
2017 IEEE International
ing logarithmic computation.
Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 5900–5904, 2017.

[14] M. Gao, J. Pu, X. Yang, M. Horowitz, and C. Kozyrakis.
Tetris: Scalable and efﬁcient neural network acceleration
with 3d memory. In International Conference on Architec-
tural Support for Programming Languages and Operating
Systems, pages 751–764, 2017.

[15] Y. Gong, L. Liu, M. Yang, and L. Bourdev. Compress-
ing deep convolutional networks using vector quantization.
arXiv preprint arXiv:1412.6115, 2014.

[16] D. Gudovskiy and L. Rigazio. ShiftCNN: Generalized low-
precision architecture for inference of convolutional neural
networks. arXiv preprint arXiv:1706.02393, 2017.

[17] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for
efﬁcient dnns. In Advances In Neural Information Process-
ing Systems, pages 1379–1387, 2016.

[18] S. Gupta, A. Agrawal, K. Gopalakrishnan,

and
P. Narayanan. Deep learning with limited numerical
the 32nd International
precision.
Conference on Machine Learning (ICML-15), pages
1737–1746, 2015.

In Proceedings of

[19] D. Hammerstrom.

A vlsi architecture for high-
performance, low-cost, on-chip learning. In IJCNN Inter-
national Joint Conference on Neural Networks, pages 537–
544 vol.2, 2012.

[20] S. Han, J. Kang, H. Mao, Y. Hu, X. Li, Y. Li, D. Xie,
H. Luo, S. Yao, and Y. Wang. Ese: Efﬁcient speech recog-
nition engine with sparse lstm on fpga. 2017.

[21] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz,
and W. J. Dally. Eie: Efﬁcient inference engine on com-
In ACM/IEEE Interna-
pressed deep neural network.
tional Symposium on Computer Architecture, pages 243–
254, 2016.

[22] S. Han, H. Mao, and W. J. Dally.

Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015.

[23] S. Han, J. Pool, J. Tran, and W. Dally. Learning both
weights and connections for efﬁcient neural network.
In
Advances in Neural Information Processing Systems, pages
1135–1143, 2015.

[24] B. Hassibi and D. G. Stork. Second order derivatives for
network pruning: Optimal brain surgeon. Morgan Kauf-
mann, 1993.

[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016.

[26] Y. He, X. Zhang, and J. Sun. Channel pruning for ac-
arXiv preprint

celerating very deep neural networks.
arXiv:1707.06168, 2017.

[27] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowl-
edge in a neural network. arXiv preprint arXiv:1503.02531,
2015.

[28] J. L. Holi and J. N. Hwang. Finite precision error analysis
of neural network hardware implementations. In Ijcnn-91-
Seattle International Joint Conference on Neural Networks,
pages 519–525 vol.1, 1993.

[29] M. Horowitz. 1.1 computing’s energy problem (and what
we can do about it). In Solid-State Circuits Conference Di-
gest of Technical Papers, pages 10–14, 2014.

[30] L. Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization
of deep networks. arXiv preprint arXiv:1611.01600, 2016.
[31] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko,
W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mo-
bilenets: Efﬁcient convolutional neural networks for mo-
bile vision applications. arXiv preprint arXiv:1704.04861,
2017.

[32] Q. Hu, P. Wang, and J. Cheng. From hashing to cnns: Train-
ing binary weight networks via hashing. In AAAI, February
2018.

[33] K. Hwang and W. Sung. Fixed-point feedforward deep
In
neural network design using weights+ 1, 0, and- 1.
2014 IEEE Workshop on Signal Processing Systems (SiPS),
pages 1–6. IEEE, 2014.

[34] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. arXiv
preprint arXiv:1602.07360, 2016.

[35] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions.
arXiv preprint arXiv:1405.3866, 2014.

[36] H. Jegou, M. Douze, and C. Schmid. Product quantization
for nearest neighbor search. IEEE transactions on pattern
analysis and machine intelligence, 33(1):117–128, 2011.

[37] N. P. Jouppi. In-datacenter performance analysis of a tensor
processing unit. In Proceedings of the 44th Annual Inter-
national Symposium on Computer Architecture, ISCA ’17,
2017.
[38] D. Kim,

J. Kung, S. Chai, S. Yalamanchili,

and
S. Mukhopadhyay. Neurocube: A programmable digi-
tal neuromorphic architecture with high-density 3d mem-
ory. In International Symposium on Computer Architecture,
pages 380–392, 2016.

[39] K. Kim, J. Kim, J. Yu, J. Seo, J. Lee, and K. Choi. Dy-
namic energy-accuracy trade-off using stochastic comput-
ing in deep neural networks. In Design Automation Confer-
ence, page 124, 2016.

[40] M. Kim and P. Smaragdis. Bitwise neural networks. arXiv

preprint arXiv:1601.06071, 2016.

[41] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin.
Compression of deep convolutional neural networks for
fast and low power mobile applications. arXiv preprint
arXiv:1511.06530, 2015.

[42] J. H. Ko, B. Mudassar, T. Na, and S. Mukhopadhyay. De-
sign of an energy-efﬁcient accelerator for training of convo-
lutional neural networks using frequency-domain computa-
tion. In Design Automation Conference, page 59, 2017.
[43] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009.

[44] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. In
Advances in neural information processing systems, pages
1097–1105, 2012.

[45] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and
Speeding-up convolutional neural net-
V. Lempitsky.
works using ﬁne-tuned cp-decomposition. arXiv preprint
arXiv:1412.6553, 2014.

[46] V. Lebedev and V. Lempitsky. Fast convnets using group-
wise brain damage. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2554–
2564, 2016.

[47] Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and
L. D. Jackel. Optimal brain damage. In Advances in Neural
Information Processing Systems, volume 89, 1989.

[48] F. Li, B. Zhang, and B. Liu. Ternary weight networks. arXiv

preprint arXiv:1605.04711, 2016.

[49] G. Li, F. Li, T. Zhao, and J. Cheng. Block convolution:
Towards memory-efﬁceint inference of large-scale cnns on
fpga. In Design Automation and Test in Europe, 2018.
[50] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv

preprint arXiv:1312.4400, 2013.

[51] Z. Lin, M. Courbariaux, R. Memisevic, and Y. Bengio.
Neural networks with few multiplications. arXiv preprint
arXiv:1510.03009, 2015.

[52] S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y. Xie, Y. Chen, and
T. Chen. Cambricon: An instruction set architecture for
neural networks. SIGARCH Comput. Archit. News, 44(3),
June 2016.

[53] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang.
Learning efﬁcient convolutional networks through network
slimming. arxiv preprint, 1708, 2017.

[54] J.-H. Luo, J. Wu, and W. Lin. Thinet: A ﬁlter level prun-
ing method for deep neural network compression. arXiv
preprint arXiv:1707.06342, 2017.

[55] Y. Ma, Y. Cao, S. Vrudhula, and J. S. Seo. An automatic
rtl compiler for high-throughput fpga implementation of
In Interna-
diverse deep convolutional neural networks.
tional Conference on Field Programmable Logic and Ap-
plications, pages 1–8, 2017.

[56] Y. Ma, Y. Cao, S. Vrudhula, and J.-s. Seo. Optimiz-
ing loop operation and dataﬂow in fpga acceleration of
In Proceedings of
deep convolutional neural networks.
the 2017 ACM/SIGDA International Symposium on Field-
Programmable Gate Arrays, FPGA ’17, 2017.

[57] Y. Ma, M. Kim, Y. Cao, S. Vrudhula, J. S. Seo, Y. Ma,
M. Kim, Y. Cao, S. Vrudhula, and J. S. Seo. End-to-end
scalable fpga accelerator for deep residual networks.
In
IEEE International Symposium on Circuits and Systems,
pages 1–4, 2017.

[58] H. Mao, S. Han, J. Pool, W. Li, X. Liu, Y. Wang, and W. J.
Dally. Exploring the regularity of sparse structure in convo-
lutional neural networks. arXiv preprint arXiv:1705.08922,
2017.

[59] D. Miyashita, E. H. Lee, and B. Murmann. Convolu-
tional neural networks using logarithmic data representa-
tion. arXiv preprint arXiv:1603.01025, 2016.

[60] D. Nguyen, D. Kim, and J. Lee. Double MAC: doubling the
performance of convolutional neural networks on modern
fpgas. In Design, Automation and Test in Europe Confer-
ence and Exhibition, DATE 2017, Lausanne, Switzerland,
March 27-31, 2017, pages 890–893, 2017.

[61] Nurvitadhi. Can fpgas beat gpus in accelerating next-
In Proceedings of
generation deep neural networks?
the 2017 ACM/SIGDA International Symposium on Field-
Programmable Gate Arrays, FPGA ’17, 2017.

[62] A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkate-
san, B. Khailany, J. Emer, S. W. Keckler, and W. J. Dally.
Scnn: An accelerator for compressed-sparse convolutional
neural networks. pages 27–40, 2017.

[63] M. Price, J. Glass, and A. P. Chandrakasan. 14.4 a scalable
speech recognizer with deep-neural-network acoustic mod-
els and voice-activated power gating. In Solid-State Circuits
Conference, pages 244–245, 2017.

[64] Qiu. Going deeper with embedded fpga platform for
the
convolutional neural network.
2016 ACM/SIGDA International Symposium on Field-
Programmable Gate Arrays, FPGA ’16, 2016.

In Proceedings of

[65] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi.
Xnor-net:
Imagenet classiﬁcation using binary convolu-
tional neural networks. In ECCV (4), volume 9908, pages
525–542. Springer, 2016.

[66] A. Ren, Z. Li, C. Ding, Q. Qiu, Y. Wang, J. Li, X. Qian,
and B. Yuan. Sc-dcnn: Highly-scalable deep convolutional
neural network using stochastic computing. Acm Sigops
Operating Systems Review, 51(2):405–418, 2017.

[67] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv
preprint arXiv:1412.6550, 2014.

[68] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015.

[69] H. Sharma, J. Park, D. Mahajan, E. Amaro, J. K. Kim,
C. Shao, A. Mishra, and H. Esmaeilzadeh. From high-
In Ieee/acm Interna-
level deep neural models to fpgas.
tional Symposium on Microarchitecture, pages 1–12, 2016.
[70] Y. Shen, M. Ferdman, and P. Milder. Escher: A cnn accel-
erator with ﬂexible buffering to minimize off-chip transfer.
In IEEE International Symposium on Field-Programmable
Custom Computing Machines, 2017.

[71] H. Sim and J. Lee. A new stochastic computing multiplier
with application to deep convolutional neural networks. In
Design Automation Conference, page 29, 2017.

[72] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[73] Suda. Throughput-optimized opencl-based fpga accelerator
for large-scale convolutional neural networks. In Proceed-
ings of the 2016 ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays, FPGA ’16, 2016.
[74] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.

Going deeper with convolutions. In Computer Vision and
Pattern Recognition, pages 1–9, 2015.

[75] W. Tang, G. Hua, and L. Wang. How to train a compact
binary neural network with high accuracy? In AAAI, pages
2625–2631, 2017.

[76] H. Tann, S. Hashemi, I. Bahar, and S. Reda. Hardware-
software codesign of accurate, multiplier-free deep neural
networks. CoRR, abs/1705.04288, 2017.

[77] Umuroglu.

Finn: A framework for fast, scalable bi-
In Proceedings of
narized neural network inference.
the 2017 ACM/SIGDA International Symposium on Field-
Programmable Gate Arrays, FPGA ’17, 2017.

[78] S. I. Venieris and C. S. Bouganis. fpgaconvnet: A frame-
work for mapping convolutional neural networks on fpgas.
In IEEE International Symposium on Field-Programmable
Custom Computing Machines, pages 40–47, 2016.

[79] S. Venkataramani, A. Ranjan, S. Banerjee, D. Das, S. Avan-
cha, A. Jagannathan, A. Durg, D. Nagaraj, B. Kaul,
P. Dubey, and A. Raghunathan. Scaledeep: A scalable com-
pute architecture for learning and evaluating deep networks.
SIGARCH Comput. Archit. News, 45(2):13–26, June 2017.
[80] P. Wang and J. Cheng. Accelerating convolutional neu-
In Proceedings of
ral networks for mobile applications.
the 2016 ACM on Multimedia Conference, pages 541–545.
ACM, 2016.

[81] P. Wang and J. Cheng. Fixed-point factorized networks.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017.

[82] P. Wang, Q. Hu, Z. Fang, C. Zhao, and J. Cheng.
Deepsearch: A fast image search framework for mobile de-
vices. ACM Transactions on Multimedia Computing, Com-
munications, and Applications (TOMM), 14, 2018.

[83] Y. Wang, J. Xu, Y. Han, H. Li, and X. Li. Deepburning: au-
tomatic generation of fpga-based learning accelerators for
the neural network family. In Design Automation Confer-
ence, page 110, 2016.

[84] Wei. Automated systolic array architecture synthesis for
high throughput cnn inference on fpgas. In Proceedings of
the 54th Annual Design Automation Conference 2017, DAC
’17, 2017.

[85] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning
structured sparsity in deep neural networks. In Advances in
Neural Information Processing Systems, pages 2074–2082,
2016.

[86] J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng. Quantized
IEEE
convolutional neural networks for mobile devices.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.

[87] L. Xia, T. Tang, W. Huangfu, M. Cheng, X. Yin, B. Li,
Y. Wang, and H. Yang. Switched by input: Power efﬁcient
structure for rram-based convolutional neural network. In
Design Automation Conference, page 125, 2016.

[88] Xiao. Exploring heterogeneous algorithms for accelerat-
ing deep convolutional neural networks on fpgas. In Pro-
ceedings of the 54th Annual Design Automation Conference
2017, DAC ’17, 2017.

[89] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In The
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), July 2017.

[90] H. Yang. Time: A training-in-memory architecture for
memristor-based deep neural networks. In Design Automa-
tion Conference, page 26, 2017.
[91] S. Zagoruyko and N. Komodakis.

Paying more atten-
tion to attention: Improving the performance of convolu-
tional neural networks via attention transfer. arXiv preprint
arXiv:1612.03928, 2016.

[92] Zhang. Optimizing fpga-based accelerator design for
In Proceedings of
deep convolutional neural networks.
the 2015 ACM/SIGDA International Symposium on Field-
Programmable Gate Arrays, FPGA ’15, 2015.

[93] C. Zhang, Z. Fang, P. Pan, P. Pan, and J. Cong. Caffeine:
towards uniformed representation and acceleration for deep
convolutional neural networks. In International Conference
on Computer-Aided Design, page 12, 2016.

[94] C. Zhang, D. Wu, J. Sun, G. Sun, G. Luo, and J. Cong.
Energy-efﬁcient cnn implementation on a deeply pipelined
fpga cluster. In Proceedings of the 2016 International Sym-
posium on Low Power Electronics and Design, ISLPED
’16, 2016.

[95] S. Zhang, Z. Du, L. Zhang, H. Lan, S. Liu, L. Li, Q. Guo,
T. Chen, and Y. Chen. Cambricon-x: An accelerator for
sparse neural networks. In Ieee/acm International Sympo-
sium on Microarchitecture, pages 1–12, 2016.

[96] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An
extremely efﬁcient convolutional neural network for mobile
devices. arXiv preprint arXiv:1707.01083, 2017.

[97] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very
deep convolutional networks for classiﬁcation and detec-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI), 2015.

[98] Zhao. Accelerating binarized convolutional neural net-
In Proceed-
works with software-programmable fpgas.
ings of the 2017 ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays, FPGA ’17, 2017.
[99] A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen. Incremen-
tal network quantization: Towards lossless cnns with low-
precision weights. arXiv preprint arXiv:1702.03044, 2017.
[100] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou.
Dorefa-net: Training low bitwidth convolutional neural
arXiv preprint
networks with low bitwidth gradients.
arXiv:1606.06160, 2016.

[101] C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary
quantization. arXiv preprint arXiv:1612.01064, 2016.
[102] J. Zhu, Z. Qian, and C. Y. Tsui. Lradnn: High-throughput
and energy-efﬁcient deep neural network accelerator using
low rank approximation. In Asia and South Paciﬁc Design
Automation Conference, pages 581–586, 2016.

