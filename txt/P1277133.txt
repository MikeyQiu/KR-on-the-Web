RACE: Large-scale ReAding Comprehension Dataset From Examinations

Guokun Lai∗ and Qizhe Xie∗ and Hanxiao Liu and Yiming Yang and Eduard Hovy
{guokun, qzxie, hanxiaol, yiming, hovy}@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract

We present RACE, a new dataset for
benchmark evaluation of methods in the
reading comprehension task. Collected
from the English exams for middle and
high school Chinese students in the age
range between 12 to 18, RACE con-
sists of near 28,000 passages and near
100,000 questions generated by human
experts (English instructors), and cov-
ers a variety of topics which are care-
fully designed for evaluating the students’
ability in understanding and reasoning.
In particular, the proportion of questions
that requires reasoning is much larger
in RACE than that in other benchmark
datasets for reading comprehension, and
there is a signiﬁcant gap between the
performance of the state-of-the-art mod-
els (43%) and the ceiling human perfor-
mance (95%). We hope this new dataset
can serve as a valuable resource for re-
search and evaluation in machine com-
prehension. The dataset is freely avail-
able at http://www.cs.cmu.edu/
˜glai1/data/race/ and the code is
available at https://github.com/
qizhex/RACE_AR_baselines

1

Introduction

Constructing an intelligence agent capable of un-
derstanding text as people is the major challenge
of NLP research. With recent advances in deep
learning techniques, it seems possible to achieve
human-level performance in certain language un-
derstanding tasks, and a surge of effort has been
devoted to the machine comprehension task where
people aim to construct a system with the ability to

∗* indicates equal contribution

answer questions related to a document that it has
to comprehend (Chen et al., 2016; Kadlec et al.,
2016; Dhingra et al., 2016; Yang et al., 2017).

Towards this goal, several large-scale datasets
(Rajpurkar et al., 2016; Onishi et al., 2016; Hill
et al., 2015; Trischler et al., 2016; Hermann
et al., 2015) have been proposed, which allow re-
searchers to train deep learning systems and ob-
tain results comparable to the human performance.
While having a suitable dataset is crucial for eval-
uating the system’s true ability in reading compre-
hension, the existing datasets suffer several critical
limitations. Firstly, in all datasets, the candidate
options are directly extracted from the context (as
a single entity or a text span), which leads to the
fact that lots of questions can be solved trivially
via word-based search and context-matching with-
out deeper reasoning; this constrains the types of
questions as well. Secondly, answers and ques-
tions of most datasets are either crowd-sourced
or automatically-generated, bringing a signiﬁcant
amount of noises in the datasets and limits the ceil-
ing performance by domain experts, such as 82%
for Childrens Book Test and 84% for Who-did-
What. Yet another issue in existing datasets is that
the topic coverages are often biased due to the spe-
ciﬁc ways that the data were initially collected,
making it hard to evaluate the ability of systems in
text comprehension over a broader range of topics.
To address the aforementioned limitations, we
constructed a new dataset by collecting a large
set of questions, answers and associated pas-
sages in the English exams for middle-school and
high-school Chinese students within the 12–18
age range. Those exams were designed by do-
main experts (instructors) for evaluating the read-
ing comprehension ability of students, with en-
Fur-
sured quality and broad topic coverage.
thermore,
the answers by machines or by hu-
mans can be objectively graded for evaluation

7
1
0
2
 
c
e
D
 
5
 
 
]
L
C
.
s
c
[
 
 
5
v
3
8
6
4
0
.
4
0
7
1
:
v
i
X
r
a

and comparison using the same evaluation met-
rics. Although efforts have been made with a sim-
ilar motivation, including the MCTest dataset cre-
ated by (Richardson et al., 2013) (containing 500
passages and 2000 questions) and several others
(Pe˜nas et al., 2014; Rodrigo et al., 2015; Khashabi
et al., 2016; Shibuki et al., 2014), the usefulness
of those datasets is signiﬁcantly restricted due to
their small sizes, especially not suitable for train-
ing powerful deep neural networks whose success
relies on the availability of relatively large training
sets.

Our new dataset, namely RACE, consists of
27,933 passages and 97,687 questions. After read-
ing each passage, each student is asked to answer
several questions where each question is provided
with four candidate answers – only one of them is
correct . Unlike existing datasets, both the ques-
tions and candidate answers in RACE are not re-
stricted to be the text spans in the original passage;
instead, they can be described in any words. A
sample from our dataset is presented in Table 1.

Our latter analysis shows that correctly answer-
ing a large portion of questions in RACE requires
the ability of reasoning, the most important fea-
ture as a machine comprehension dataset (Chen
et al., 2016). RACE also offers two important sub-
divisions of the reasoning types in its questions,
namely passage summarization and attitude anal-
ysis, which have not been introduced by the any of
the existing large-scale datasets to our knowledge.
In addition, compared to other existing datasets
where passages are either domain-speciﬁc or of a
single ﬁxed style (namely news stories for CNN/-
Daily Mail, NEWSQA and Who-did-What, ﬁction
stories for Children’s Book Test and Book Test,
and Wikipedia articles for SQUAD), passages in
RACE almost cover all types of human articles,
such as news, stories, ads, biography, philosophy,
etc., in a variety of styles. This comprehensiveness
of topic/style coverage makes RACE a desirable
resource for evaluating the reading comprehension
ability of machine learning systems in general.

The advantages of our proposed dataset over ex-
isting large datasets in machine reading compre-
hension can be summarized as follows:

text comprehension ability of machine learn-
ing systems under human judge.

• The questions are substantially more difﬁcult
than those in existing datasets, in terms of the
large portion of questions involving reason-
ing. At the meantime, it is also sufﬁciently
large to support the training of deep learning
models.

• Unlike existing large-scale datasets, candi-
date options in RACE are human generated
sentences which may not appear in the origi-
nal passage. This makes the task more chal-
lenging and allows a rich type of questions
such as passage summarization and attitude
analysis.

• Broad coverage in various domains and writ-
ing styles: a desirable property for evaluating
generic (in contrast to domain/style-speciﬁc)
comprehension ability of learning models.

2 Related Work

In this section, we brieﬂy outline existing datasets
for the machine reading comprehension task, in-
cluding their strengths and weaknesses.

2.1 MCTest

MCTest (Richardson et al., 2013) is a popular
dataset for question answering in the same for-
mat as RACE, where each question is associated
with four candidate answers with a single cor-
rect answer. Although questions in MCTest are
of high-quality ensured by careful examinations
through crowdsourcing, it contains only 500 stores
and 2000 questions, which substantially restricts
its usage in training advanced machine compre-
hension models. Moreover, while MCTest is de-
signed for 7 years old children, RACE is con-
structed for middle and high school students at
12–18 years old hence is more complicated and
requires stronger reasoning skills. In other words,
RACE can be viewed as a larger and more difﬁcult
version of the MCTest dataset.

2.2 Cloze-style datasets

• All questions and candidate options are gen-
erated by human experts, which are intention-
ally designed to test human agent’s ability in
reading comprehension. This makes RACE a
relatively accurate indicator for reﬂecting the

The past few years have witnessed several large-
scale cloze-style datasets (Hermann et al., 2015;
Hill et al., 2015; Bajgar et al., 2016; Onishi et al.,
2016), whose questions are formulated by obliter-
ating a word or an entity in a sentence.

Passage:
In a small village in England about 150 years ago, a mail coach was standing on the street. It didn’t come to that village often.
People had to pay a lot to get a letter. The person who sent the letter didn’t have to pay the postage, while the receiver had to.
“Here’s a letter for Miss Alice Brown,” said the mailman.
“ I’m Alice Brown,” a girl of about 18 said in a low voice.
Alice looked at the envelope for a minute, and then handed it back to the mailman.
“I’m sorry I can’t take it, I don’t have enough money to pay it”, she said.
A gentleman standing around were very sorry for her. Then he came up and paid the postage for her.
When the gentleman gave the letter to her, she said with a smile, “ Thank you very much, This letter is from Tom. I’m going
to marry him. He went to London to look for work. I’ve waited a long time for this letter, but now I don’t need it, there is
nothing in it.”
“Really? How do you know that?” the gentleman said in surprise.
“He told me that he would put some signs on the envelope. Look, sir, this cross in the corner means that he is well and this
circle means he has found work. That’s good news.”
The gentleman was Sir Rowland Hill. He didn’t forgot Alice and her letter.
“The postage to be paid by the receiver has to be changed,” he said to himself and had a good plan.
“The postage has to be much lower, what about a penny? And the person who sends the letter pays the postage. He has to buy
a stamp and put it on the envelope.” he said . The government accepted his plan. Then the ﬁrst stamp was put out in 1840. It
was called the “Penny Black”. It had a picture of the Queen on it.

Questions:

1): The ﬁrst postage stamp was made .
A. in England B. in America C. by Alice D. in 1910

2): The girl handed the letter back to the mailman because
.
A. she didn’t know whose letter it was
B. she had no money to pay the postage
C. she received the letter but she didn’t want to open it
D. she had already known what was written in the letter

3): We can know from Alice’s words that
A. Tom had told her what the signs meant before leaving
B. Alice was clever and could guess the meaning of the signs
C. Alice had put the signs on the envelope herself
D. Tom had put the signs as Alice had told him to

.

4): The idea of using stamps was thought of by .
A. the government
B. Sir Rowland Hill
C. Alice Brown
D. Tom

5): From the passage we know the high postage made .
A. people never send each other letters
B. lovers almost lose every touch with each other
C. people try their best to avoid paying it
D. receivers refuse to pay the coming letters

Answer: ADABC

Table 1: Sample reading comprehension problems from our dataset.

CNN/Daily Mail (Hermann et al., 2015) are
the largest machine comprehension datasets with
1.4M questions. However, both require limited
reasoning ability (Chen et al., 2016). In fact, the
best machine performance obtained by researchers
(Chen et al., 2016; Dhingra et al., 2016) is close to
human’s performance on CNN/Daily Mail.

Childrens Book Test (CBT) (Hill et al., 2015)
and Book Test (BT) (Bajgar et al., 2016) are con-
structed in a similar manner. Each passage in CBT
consist of 20 contiguous sentences extracted from
children’s books and the next (21st) sentence is
used to make the question. The main difference
between the two datasets is the size of BT being
60 times larger. Machine comprehension models
have also matched human performance on CBT
(Bajgar et al., 2016).

Who Did What (WDW) (Onishi et al., 2016)
is yet another cloze-style dataset constructed from
the LDC English Gigaword newswire corpus. The
authors generate passages and questions by pick-
ing two news articles describing the same event,

using one as the passage and the other as the ques-
tion.

High noise is inevitable in cloze-style datasets
due to their automatic generation process, which
is reﬂected in the human performance on these
datasets: 82% for CBT and 84% for WDW.

2.3 Datasets with Span-based Answers

In datasets such as SQUAD (Rajpurkar et al.,
2016), NEWSQA (Trischler et al., 2016) MS
MARCO (Nguyen et al., 2016) and recently pro-
posed TriviaQA (Joshi et al., 2017). the answer to
each question is in the form of a text span in the
article. Articles of SQUAD, NEWSQA and MS
MARCO come from Wikipedia, CNN news and
the Bing search engine respectively. The answer to
a certain question may not be unique and could be
multiple spans. Instead of evaluating the accuracy,
researchers need to use F1 score, BLEU (Papineni
et al., 2002) or ROUGE (Lin and Hovy, 2003)
as metrics, which measure the overlap between
the prediction and ground truth answers since the

questions come without candidate spans.

Datasets with span-based answers are challeng-
ing as the space of possible spans is usually large.
However, restricting answers to be text spans in
the context passage may be unrealistic and more
importantly, may not be intuitive even for humans,
indicated by the suffered human performance of
80.3% on SQUAD (or 65% claimed by Trischler
et al. (2016)) and 46.5% on NEWSQA. In other
words, the format of span-based answers may not
necessarily be a good examination of reading com-
prehension of machines whose aim is to approach
the comprehension ability of humans.

2.4 Datasets from Examinations

There have been several datasets extracted from
examinations, aiming at evaluating systems un-
der the same conditions as how humans are evalu-
ated in schools. E.g., the AI2 Elementary School
Science Questions dataset (Khashabi et al., 2016)
contains 1080 questions for students in elementary
schools; NTCIR QA Lab (Shibuki et al., 2014)
evaluates systems by the task of solving real-world
university entrance exam questions; The Entrance
Exams task at CLEF QA Track (Pe˜nas et al., 2014;
Rodrigo et al., 2015) evaluates the system’s read-
ing comprehension ability. However, data pro-
vided in these existing tasks are far from sufﬁcient
for the training of advanced data-driven machine
reading models, partially due to the expensive data
generation process by human experts.

To the best of our knowledge, RACE is the ﬁrst
large-scale dataset of this type, where questions
are created based on exams designed to evaluate
human performance in reading comprehension.

3 Data Analysis

In this section, we study the nature of questions
covered in RACE at a detailed level. Speciﬁcally,
we present the dataset statistics in Section 3.1, and
then analyze different reasoning/question types in
RACE in the remaining subsections.

3.1 Dataset Statistics

nations. We split 5% data as the development set
and 5% as the test set for RACE-M and RACE-H
respectively. The number of samples in each set is
shown in Table 2. The statistics for RACE-M and
RACE-H is summarized in Table 3. We can ﬁnd
that the length of the passages and the vocabulary
size in the RACE-H are much larger than that of
the RACE-M, an evidence of the higher difﬁculty
of high school examinations.

However, notice that since the articles and ques-
tions are selected and designed to test Chinese
students learning English as a foreign language,
the vocabulary size and the complexity of the lan-
guage constructs are simpler than news articles
and Wikipedia articles in other QA datasets.

3.2 Reasoning Types of the Questions

To get a comprehensive picture about the reason-
ing difﬁculty requirement of RACE, we conduct
human annotations of questions types. Following
Chen et al. (2016); Trischler et al. (2016), we strat-
ify the questions into ﬁve classes as follows with
ascending order of difﬁculty:

• Word matching:

The question exactly
matches a span in the article. The answer is
self-evident.

• Paraphrasing: The question is entailed or
paraphrased by exactly one sentence in the
passage. The answer can be extracted within
the sentence.

• Single-sentence reasoning: The answer could
be inferred from a single sentence of the arti-
cle by recognizing incomplete information or
conceptual overlap.

• Multi-sentence reasoning: The answer must
be inferred from synthesizing information
distributed across multiple sentences.

• Insufﬁcient/Ambiguous: The question has no
answer or the answer is not unique based on
the given passage.

As mentioned in section 1, RACE is collected
from English examinations designed for 12–15
year-old middle school students, and 15–18 year-
old high school students in China. To distin-
guish the two subgroups with drastic difﬁculty
gap, RACE-M denotes the middle school exami-
nations and RACE-H denotes high school exami-

We refer readers to (Chen et al., 2016; Trischler

et al., 2016) for examples of each category.

To obtain the proportion of different question
types, we sample 100 passages from RACE (50
from RACE-M and 50 from RACE-H), all of
which have 5 questions hence there are 500 ques-
tions in total. We put the passages on Amazon Me-

Dataset
Subset
# passages
# questions

RACE-M
Dev
368
1,436

Train
6,409
25,421

Test
362
1,436

Train
18,728
62,445

RACE-H
Dev
1,021
3,451

RACE

Test
1,045
3,498

Train
25,137
87,866

Dev
1,389
4,887

Test
1,407
4,934

All
27,933
97,687

Table 2: The separation of the training, development and test sets of RACE-M,RACE-H and RACE

Dataset
Passage Len
Question Len
Option Len
Vocab size

RACE-M RACE-H RACE
321.9
353.1
10.0
10.4
5.3
5.8
136,629
125,120

231.1
9.0
3.9
32,811

Table 3: Statistics of RACE where Len denotes
length and Vocab denotes Vocabulary.

chanical Turk1, and a Hit is generated by a passage
with 5 questions. Each question is labeled by two
crowdworkers. We require the turkers to both an-
swer the questions and label the reasoning type.
We pay $0.70 and $1.00 per passage in RACE-M
and RACE-H respectively, and restrict the access
to master turkers only. Finally, we get 1000 labels
for the 500 questions.

The statistics about the reasoning type is sum-
marized in Table 4. The higher difﬁculty level
of RACE is justiﬁed by its higher ratio of rea-
soning questions in comparison to CNN, SQUAD
and NEWSQA. Speciﬁcally, 59.2% questions of
RACE are either in the category of single-sentence
reasoning or in the category of multi-sentence
reasoning, while the ratio is 21%, 20.5% and
33.9% for CNN, SQUAD and NEWSQA respec-
tively. Also notice that the ratio of word match-
ing questions on RACE is only 15.8%, the lowest
among several categories. In addition, questions
in RACE-H are more complex than questions in
RACE-M since RACE-M has more word match-
ing questions and fewer reasoning questions.

3.3 Subdividing Reasoning Types

To better understand our dataset and facilitate fu-
ture research, we list the subdivisions of ques-
tions under the reasoning category. We ﬁnd the
most frequent reasoning subdivisions include: de-
tail reasoning, whole-picture understanding, pas-
sage summarization, attitude analysis and world
knowledge. One question may fall into multiple
divisions. Deﬁnition of these subdivisions and
their associated examples are as follows:

1https://www.mturk.com/mturk/welcome

1. Detail reasoning: to answer the question, the
agent should be clear about the details of the pas-
sage. The answer appears in the passage but it can-
not be found by simply matching the question with
the passage. For example, Question 1 in the sam-
ple passage falls into this category.

2. Whole-picture reasoning: the agent needs to
understand the whole picture of the story to ob-
tain the correct answer. For example, to answer
the Question 2 in the sample passage, the agent is
required to comprehend the entire story.

3. Passage summarization: The question re-
quires the agent to select the best summarization
of the passage among four candidate summariza-
tions. A typical question of this type is “The main
idea of this passage is
.”. An example question
can be found in Appendix A.1.

4. Attitude analysis: The question asks about
the opinions/attitudes of the author or a character
in the story towards somebody or something, e.g.,

• Evidence:
“. . . Many people optimistically
thought
industry awards for better equipment
the production of quieter
would stimulate
appliances. It was even suggested that noise from
building sites could be alleviated . . . ”

• Question: What was the author’s attitude towards

the industry awards for quieter?

• Options:

A.suspicious

B.positive

C.enthusiastic D.indifferent

5. World knowledge: Certain external knowl-
edge is needed. Most frequent questions under this
category involve simple arithmetic.

• Evidence: “The park is open from 8 am to 5 pm.”

• Question: The park is open for

hours a day.

• Options: A.eight B.nine C.ten D.eleven

To the best of our knowledge, questions like
passage summarization and attitude analysis have
not been introduced by any of the existing large-
scale machine comprehension datasets. Both are

Dataset
Word Matching
Paraphrasing
Single-Sentence Reasoning
Multi-Sentence Reasoning
Ambiguous/Insufﬁcient

RACE-M RACE-H RACE

29.4%
14.8%
31.3%
22.6%
1.8%

11.3%
20.6%
34.1%
26.9%
7.1%

CNN
15.8% 13.0%†
19.2% 41.0%†
33.4% 19.0%†
25.8% 2.0%†
5.8% 25.0%†

SQUAD NEWSQA
39.8%*
34.3%*
8.6%*
11.9%*
5.4%*

32.7%*
27.0%*
13.2%*
20.7%*
6.4%*

Table 4: Statistic information about Reasoning type in different datasets. * denotes the numbers coming
from (Trischler et al., 2016) based on 1000 samples per dataset, and numbers with † come from (Chen
et al., 2016).

crucial components in evaluating humans’ reading
comprehension abilities.

5 Experiments

4 Collection Methodology

We collected the raw data from three large free
public websites in China2, where the reading com-
prehension problems are extracted from English
examinations designed by teachers in China. The
data before cleaning contains 137,918 passages
and 519,878 questions in total, where there are
38,159 passages with 156,782 questions in the
middle school group, and 99,759 passages with
363,096 questions in the high school group.

The following ﬁltering steps are conducted to
clean the raw data. Firstly, we remove all prob-
lems and questions that do not have the same for-
mat as our problem setting, e.g., a question would
be removed if the number of its options is not four.
Secondly, we ﬁlter all articles and questions that
are not self-contained based on the text informa-
tion, i.e. we remove the articles and questions con-
taining images or tables. We also remove all ques-
tions containing keywords “underlined” or “para-
graph”, since it is difﬁcult to reproduce the effect
of underlines and the paragraph segment informa-
tion. Thirdly, we remove all duplicated articles.

On one of the websites (xkw.com), the answers
are stored as images. We used two standard OCR
programs tesseract 3 and ABBYY FineReader 4 to
process the images. We remove all the answers
that two software disagree. The OCR task is easy
since we only need to recognize printed alphabet
A, B, C, D with a standard font. Finally, we get
the cleaned dataset RACE, with 27,933 passages
and 97,687 questions.

In this section, we compare the performance
of several state-of-the-art reading comprehension
models with human performance. We use accu-
racy as the metric to evaluate different models.

5.1 Methods for Comparison

Sliding Window Algorithm Firstly, we build
the rule-based baseline introduced by Richardson
et al. (2013).
It chooses the answer having the
highest matching score. Speciﬁcally, it ﬁrst con-
catenates the question and the answer and then cal-
culates the TF-IDF style matching score between
the concatenated sentence with every window (a
span of text) of the article. The window size is
decided by the model performance in the training
and dev sets.

Stanford Attentive Reader Stanford Attentive
Reader (Stanford AR) (Chen et al., 2016) is a
strong model that achieves state-of-the-art results
on CNN/Daily Mail. Moreover, the authors claim
that their model has nearly reached the ceiling per-
formance on these two datasets.

Suppose that the triple of passage, question and
options is denoted by (p, q, o1,··· ,4). We ﬁrst em-
ploy bidirectional GRUs to encode p and q respec-
tively into hp
n and hq. Then we sum-
marize the most relevant part of the passage into
sp with an attention model. Following Chen et al.
(2016), we adopt a bilinear attention form. Specif-
ically,

2, . . . , hp

1, hp

αi = Softmaxi((hp
sp =

(cid:88)

αihp
i

i )T W1hq)

i

(1)

2We checked that our dataset does not include exam-
ple questions of exams with copyright, such as SSAT, SAT,
TOEFL and GRE.

3https://github.com/tesseract-ocr
4https://www.abbyy.com/FineReader

Similarly, we use bidirectional GRUs to encode
option oi into a vector hoi. Finally, we com-
pute the matching score between the i-th option
(i = 1, · · · , 4) and the summarized passage using

Random
Sliding Window
Stanford AR
GA
Turkers
Ceiling Performance

RACE-M RACE-H RACE MCTest CNN DM CBT-N CBT-C WDW
32.0†
10.2
19.6† 48.0†
64.0†
67.3† 71.2†

–

–

24.8
51.5†
–
–
–
–

10.6
0.06 0.06
24.8 30.8 16.8†
73.6† 76.6†
77.9† 80.9† 70.1†
–
–

–
81.6†

–
–

25.0
30.4
43.0
44.2
69.4
94.2

24.6
37.3
44.2
43.7
85.1
95.4

24.9
32.2
43.3
44.1
73.3
94.5

–
81.6†

–
84†

Table 5: Accuracy of models and human on the each dataset, where † denotes the results coming from
previous publications. DM denotes Daily Mail and WDW denotes Who-Did-What .

(a) RACE-M

(b) RACE-H

Figure 1: Test accuracy of different baselines on each question type category introduced in Section 3.2,
where Word-Match, Single-Reason, Multi-Reason and Ambiguous are the abbreviations for Word match-
ing, Single-sentence Reasoning, Multi-sentence Reasoning and Insufﬁcient/Ambiguous respectively.

a bilinear attention. We pass the scores through
softmax to get a probability distribution. Specif-
ically, the probability of option i being the right
answer is calculated as

pi = Softmaxi(hoiW2sd)

(2)

Gated-Attention Reader Gated AR (Dhingra
et al., 2016) is the state-of-the-art model on mul-
tiple datasets. To build query-speciﬁc represen-
tations of tokens in the document, it employs an
attention mechanism to model multiplicative in-
teractions between the query embedding and the
document representation. With a multi-hop ar-
chitecture, GA also enables a model to scan the
document and the question iteratively for multi-
ple passes.
In other words, the multi-hop struc-
ture makes it possible for the reader to reﬁne token
representations iteratively and the attention mech-
anism ﬁnd the most relevant part of the document.
We refer readers to (Dhingra et al., 2016) for more
details.

After obtaining a query speciﬁc document rep-
resentation sd, we use the same method as bilinear
operation listed in Equation 2 to get the output.

Note that our implementation slightly differs
from the original GA reader. Speciﬁcally, the At-
tention Sum layer is not applied at the ﬁnal layer
and no character-level embeddings are used.

Implementation Details We follow Chen et al.
(2016) in our experiment settings. The vocabulary
size is set to 50k. We choose word embedding
size d = 100 and use the 100-dimensional Glove
word embedding (Pennington et al., 2014) as em-
bedding initialization. GRU weights are initial-
ized from Gaussian distribution N (0, 0.1). Other
parameters are initialized from a uniform distri-
bution on (−0.01, 0.01). The hidden dimension-
ality is set to 128 and the number of layers is
set to one for both Stanford AR and GA. We use
vanilla stochastic gradient descent (SGD) to train
our models. We apply dropout on word embed-
dings and the gradient is clipped when the norm

of the gradient is larger than 10. We use a grid
search on validation set to choose the learning
rate within {0.05, 0.1, 0.3, 0.5} and dropout rate
within {0.2, 0.5, 0.7}. The highest accuracy on
validation set is obtained by setting learning rate to
0.1 for Stanford AR and 0.3 for GA and dropout
rate to 0.5. The data of RACE-M and RACE-H
is used together to train our model and testing is
performed separately.

5.2 Human Evaluation

As described in section 3.2, a randomly sam-
pled subset of test set has been labeled by Ama-
zon Turkers, which contains 500 questions with
half from RACE-H and with the other half from
RACE-M. The turkers’ performance is 85% for
RACE-M and 70% for RACE-H. However, it is
hard to guarantee that every turker performs the
survey carefully, given the difﬁcult and long pas-
sages of high school problems. Therefore, to ob-
tain the ceiling human performance on RACE,
we manually labeled the proportion of valid ques-
tions. A question is valid if it is unambiguous and
has a correct answer. We found that 94.5% of the
data is valid, which sets the ceiling human per-
formance. Similarly, the ceiling performance on
RACE-M and RACE-H is 95.4% and 94.2% re-
spectively.

5.3 Main Results

We compare models’ and human ceiling perfor-
mance on datasets which have the same evalua-
tion metric with RACE. The compared datasets
include RACE, MCTest, CNN/Daily Mail (CNN
and DM), CBT and WDW. On CBT, we report per-
formance on two subsets where the missing token
is either a common noun (CBT-C) or name entity
(CBT-N) since the language models have already
reached human-level performance on other types
(Hill et al., 2015). The comparison is shown in
Table 5.

Performance of Sliding Window We ﬁrst com-
pare MCTest with RACE using Sliding Window,
where it is unable to train Stanford AR and Gated
Slid-
AR on MCTest’s limited training data.
ing Window achieves an accuracy of 51.5% on
MCTest while only 37.3% on RACE, meaning that
to answer the questions of RACE requires more
reasoning than MCTest.

The performance of sliding window on RACE
is not directly comparable with CBT and WDW

since CBT has ten candidate answers for each
question and WDW has an average of three. In-
stead, we evaluate the performance improvement
of sliding window on the random baseline. Larger
improvement indicates more questions solvable by
simple matching. On RACE, Sliding Window is
28.6% better than the random baseline, while the
improvement is 58.5%, 92.2% and 50% for CBT-
N, CBT-C and WDW.

The accuracy on RACE-M (37.3%) and RACE-
H (30.4%) indicates that the middle school ques-
tions are simpler based on the matching algorithm.

Performance of Neural Models We further
compare the difﬁculty of different datasets by
state-of-the-art neural models’ performance. A
lower performance means that more problems are
unsolvable by machines. The Stanford AR and
Gated AR achieve an accuracy of only 43.3% and
44.1% on RACE while their accuracy is much
higher on CNN/Daily Mail, Childrens Book
Test and Who-Did-What. It justiﬁes the fact that,
among current large-scale machine comprehen-
sion datasets, RACE is the most challenging one.

Human Ceiling Performance The human per-
formance is 94.5% which shows our data is quite
clean compared to other large-scale machine com-
prehension datasets. Since we cannot enforce ev-
ery turker do the test cautiously, the result shows
a gap between turkers’ performance and human
performance. Reasonably, problems in the high
school group with longer passages and more com-
plex questions lead to more signiﬁcant divergence.
Nevertheless, the start-of-the-art models still have
a large room to be improved to reach turkers’ per-
formance. The performance gap is 41% for the
middle school problems and 25% for the high
school problems. What’s more, The performance
of Stanford AR and GA is only less than a half
of the ceiling human performance, which indicates
that to match the humans’ reading comprehension
ability, we still have a long way to go.

5.4 Reason Types Analysis

We evaluate human and models on different types
of questions, shown in Figure 1. Turkers do the
best on word matching problems while doing the
worst on reasoning problems. Sliding window
performs better on word matching than problems
needing reasoning or paraphrasing. Surprisingly,
Stanford AR does not have a stronger performance

on the word matching category than reasoning cat-
egories. A possible reason is that the proportion
of data in reasoning categories is larger than that
of data. Also, the candidate answers of simple
matching questions may share similar word em-
beddings. For example, if the question is about
color, it is difﬁcult to distinguish candidate an-
swers, “green”, “red”, “blue” and “yellow”, in the
embedding vector space. The similar performance
on different categories also explains the reason
that the performance of the neural models is close
in the middle and high school groups in Table 5.

6 Conclusion

We introduce a large, high-quality dataset for read-
ing comprehension that is carefully designed to
examine human ability on this task. Some desir-
able properties of RACE include the broad cover-
age of domains/styles and the richness in the ques-
tion format. Most importantly, it requires substan-
tially more reasoning to do well on RACE than
on other datasets, as there is a signiﬁcant gap be-
tween the performance of state-of-the-art machine
comprehension models and that of the human. We
hope this dataset will stimulate the development of
more advanced machine comprehension models.

Acknowledgement

We would like to thank Graham Neubig for sug-
gestions on the draft and Diyi Yang’s help on ob-
taining the crowdsourced labels.

This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program.

References

Ondrej Bajgar, Rudolf Kadlec, and Jan Kleindi-
enst. 2016. Embracing data abundance: Booktest
dataset for reading comprehension. arXiv preprint
arXiv:1610.00956 .

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A thorough examination of the cn-
arXiv
n/daily mail reading comprehension task.
preprint arXiv:1606.02858 .

Bhuwan Dhingra, Hanxiao Liu, William W Cohen,
and Ruslan Salakhutdinov. 2016. Gated-attention
arXiv preprint
readers for text comprehension.
arXiv:1606.01549 .

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-

chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301 .

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. ACL .

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv:1603.01547 .

Daniel Khashabi, Tushar Khot, Ashish Sabhar-
wal, Peter Clark, Oren Etzioni, and Dan Roth.
2016. Question answering via integer programming
arXiv preprint
over semi-structured knowledge.
arXiv:1604.06076 .

Chin-Yew Lin and Eduard Hovy. 2003.

Auto-
matic evaluation of summaries using n-gram co-
In Proceedings of the 2003
occurrence statistics.
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1. Association
for Computational Linguistics, pages 71–78.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
arXiv preprint
reading comprehension dataset.
arXiv:1611.09268 .

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. 2016. Who did what:
A large-scale person-centered cloze dataset. arXiv
preprint arXiv:1608.05457 .

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Anselmo Pe˜nas, Yusuke Miyao, ´Alvaro Rodrigo, Ed-
uard H Hovy, and Noriko Kando. 2014. Overview of
clef qa entrance exams task 2014. In CLEF (Work-
ing Notes). pages 1194–1200.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
EMNLP. volume 3, page 4.

´Alvaro Rodrigo, Anselmo Pe˜nas, Yusuke Miyao, Ed-
uard H Hovy, and Noriko Kando. 2015. Overview of
clef qa entrance exams task 2015. In CLEF (Work-
ing Notes).

Hideyuki Shibuki, Kotaro Sakamoto, Yoshinobu Kano,
Teruko Mitamura, Madoka Ishioroshi, Kelly Y
Itakura, Di Wang, Tatsunori Mori, and Noriko
Kando. 2014. Overview of the ntcir-11 qa-lab task.
In NTCIR.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830 .

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W Cohen. 2017. Semi-supervised qa with
arXiv preprint
generative domain-adaptive nets.
arXiv:1702.02206 .

A Appendix

A.1 Example Question of Passage

Summarization

Passage: Do you love holidays but hate gaining
weight? You are not alone. Holidays are times for
celebrating. Many people are worried about their
weight. With proper planning, though, it is pos-
sible to keep normal weight during the holidays.
The idea is to enjoy the holidays but not to eat too
much. You don’t have to turn away from the foods
that you enjoy.

Here are some tips for preventing weight gain

and maintaining physical ﬁtness:

Don’t skip meals. Before you leave home, have
a small, low-fat meal or snack. This may help to
avoid getting too excited before delicious foods.

Control the amount of food. Use a small plate
that may encourage you to ”load up”. You should
be most comfortable eating an amount of food
about the size of your ﬁst.

Begin with soup and fruit or vegetables. Fill up
beforehand on water-based soup and raw fruit or
vegetables, or drink a large glass of water before
you eat to help you to feel full.

Avoid high-fat foods. Dishes that look oily or
creamy may have large amount of fat. Choose lean
meat . Fill your plate with salad and green vegeta-
bles. Use lemon juice instead of creamy food.

Stick to physical activity. Don’t let exercise take
a break during the holidays. A 20-minute walk
helps to burn off extra calories.

Questions:
What is the best title of the passage?
Options:
A. How to avoid holiday feasting
B. Do’s and don’ts for keeping slim and ﬁt.
C. How to avoid weight gain over holidays.
D. Wonderful holidays, boring experiences.

RACE: Large-scale ReAding Comprehension Dataset From Examinations

Guokun Lai∗ and Qizhe Xie∗ and Hanxiao Liu and Yiming Yang and Eduard Hovy
{guokun, qzxie, hanxiaol, yiming, hovy}@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract

We present RACE, a new dataset for
benchmark evaluation of methods in the
reading comprehension task. Collected
from the English exams for middle and
high school Chinese students in the age
range between 12 to 18, RACE con-
sists of near 28,000 passages and near
100,000 questions generated by human
experts (English instructors), and cov-
ers a variety of topics which are care-
fully designed for evaluating the students’
ability in understanding and reasoning.
In particular, the proportion of questions
that requires reasoning is much larger
in RACE than that in other benchmark
datasets for reading comprehension, and
there is a signiﬁcant gap between the
performance of the state-of-the-art mod-
els (43%) and the ceiling human perfor-
mance (95%). We hope this new dataset
can serve as a valuable resource for re-
search and evaluation in machine com-
prehension. The dataset is freely avail-
able at http://www.cs.cmu.edu/
˜glai1/data/race/ and the code is
available at https://github.com/
qizhex/RACE_AR_baselines

1

Introduction

Constructing an intelligence agent capable of un-
derstanding text as people is the major challenge
of NLP research. With recent advances in deep
learning techniques, it seems possible to achieve
human-level performance in certain language un-
derstanding tasks, and a surge of effort has been
devoted to the machine comprehension task where
people aim to construct a system with the ability to

∗* indicates equal contribution

answer questions related to a document that it has
to comprehend (Chen et al., 2016; Kadlec et al.,
2016; Dhingra et al., 2016; Yang et al., 2017).

Towards this goal, several large-scale datasets
(Rajpurkar et al., 2016; Onishi et al., 2016; Hill
et al., 2015; Trischler et al., 2016; Hermann
et al., 2015) have been proposed, which allow re-
searchers to train deep learning systems and ob-
tain results comparable to the human performance.
While having a suitable dataset is crucial for eval-
uating the system’s true ability in reading compre-
hension, the existing datasets suffer several critical
limitations. Firstly, in all datasets, the candidate
options are directly extracted from the context (as
a single entity or a text span), which leads to the
fact that lots of questions can be solved trivially
via word-based search and context-matching with-
out deeper reasoning; this constrains the types of
questions as well. Secondly, answers and ques-
tions of most datasets are either crowd-sourced
or automatically-generated, bringing a signiﬁcant
amount of noises in the datasets and limits the ceil-
ing performance by domain experts, such as 82%
for Childrens Book Test and 84% for Who-did-
What. Yet another issue in existing datasets is that
the topic coverages are often biased due to the spe-
ciﬁc ways that the data were initially collected,
making it hard to evaluate the ability of systems in
text comprehension over a broader range of topics.
To address the aforementioned limitations, we
constructed a new dataset by collecting a large
set of questions, answers and associated pas-
sages in the English exams for middle-school and
high-school Chinese students within the 12–18
age range. Those exams were designed by do-
main experts (instructors) for evaluating the read-
ing comprehension ability of students, with en-
Fur-
sured quality and broad topic coverage.
thermore,
the answers by machines or by hu-
mans can be objectively graded for evaluation

7
1
0
2
 
c
e
D
 
5
 
 
]
L
C
.
s
c
[
 
 
5
v
3
8
6
4
0
.
4
0
7
1
:
v
i
X
r
a

and comparison using the same evaluation met-
rics. Although efforts have been made with a sim-
ilar motivation, including the MCTest dataset cre-
ated by (Richardson et al., 2013) (containing 500
passages and 2000 questions) and several others
(Pe˜nas et al., 2014; Rodrigo et al., 2015; Khashabi
et al., 2016; Shibuki et al., 2014), the usefulness
of those datasets is signiﬁcantly restricted due to
their small sizes, especially not suitable for train-
ing powerful deep neural networks whose success
relies on the availability of relatively large training
sets.

Our new dataset, namely RACE, consists of
27,933 passages and 97,687 questions. After read-
ing each passage, each student is asked to answer
several questions where each question is provided
with four candidate answers – only one of them is
correct . Unlike existing datasets, both the ques-
tions and candidate answers in RACE are not re-
stricted to be the text spans in the original passage;
instead, they can be described in any words. A
sample from our dataset is presented in Table 1.

Our latter analysis shows that correctly answer-
ing a large portion of questions in RACE requires
the ability of reasoning, the most important fea-
ture as a machine comprehension dataset (Chen
et al., 2016). RACE also offers two important sub-
divisions of the reasoning types in its questions,
namely passage summarization and attitude anal-
ysis, which have not been introduced by the any of
the existing large-scale datasets to our knowledge.
In addition, compared to other existing datasets
where passages are either domain-speciﬁc or of a
single ﬁxed style (namely news stories for CNN/-
Daily Mail, NEWSQA and Who-did-What, ﬁction
stories for Children’s Book Test and Book Test,
and Wikipedia articles for SQUAD), passages in
RACE almost cover all types of human articles,
such as news, stories, ads, biography, philosophy,
etc., in a variety of styles. This comprehensiveness
of topic/style coverage makes RACE a desirable
resource for evaluating the reading comprehension
ability of machine learning systems in general.

The advantages of our proposed dataset over ex-
isting large datasets in machine reading compre-
hension can be summarized as follows:

text comprehension ability of machine learn-
ing systems under human judge.

• The questions are substantially more difﬁcult
than those in existing datasets, in terms of the
large portion of questions involving reason-
ing. At the meantime, it is also sufﬁciently
large to support the training of deep learning
models.

• Unlike existing large-scale datasets, candi-
date options in RACE are human generated
sentences which may not appear in the origi-
nal passage. This makes the task more chal-
lenging and allows a rich type of questions
such as passage summarization and attitude
analysis.

• Broad coverage in various domains and writ-
ing styles: a desirable property for evaluating
generic (in contrast to domain/style-speciﬁc)
comprehension ability of learning models.

2 Related Work

In this section, we brieﬂy outline existing datasets
for the machine reading comprehension task, in-
cluding their strengths and weaknesses.

2.1 MCTest

MCTest (Richardson et al., 2013) is a popular
dataset for question answering in the same for-
mat as RACE, where each question is associated
with four candidate answers with a single cor-
rect answer. Although questions in MCTest are
of high-quality ensured by careful examinations
through crowdsourcing, it contains only 500 stores
and 2000 questions, which substantially restricts
its usage in training advanced machine compre-
hension models. Moreover, while MCTest is de-
signed for 7 years old children, RACE is con-
structed for middle and high school students at
12–18 years old hence is more complicated and
requires stronger reasoning skills. In other words,
RACE can be viewed as a larger and more difﬁcult
version of the MCTest dataset.

2.2 Cloze-style datasets

• All questions and candidate options are gen-
erated by human experts, which are intention-
ally designed to test human agent’s ability in
reading comprehension. This makes RACE a
relatively accurate indicator for reﬂecting the

The past few years have witnessed several large-
scale cloze-style datasets (Hermann et al., 2015;
Hill et al., 2015; Bajgar et al., 2016; Onishi et al.,
2016), whose questions are formulated by obliter-
ating a word or an entity in a sentence.

Passage:
In a small village in England about 150 years ago, a mail coach was standing on the street. It didn’t come to that village often.
People had to pay a lot to get a letter. The person who sent the letter didn’t have to pay the postage, while the receiver had to.
“Here’s a letter for Miss Alice Brown,” said the mailman.
“ I’m Alice Brown,” a girl of about 18 said in a low voice.
Alice looked at the envelope for a minute, and then handed it back to the mailman.
“I’m sorry I can’t take it, I don’t have enough money to pay it”, she said.
A gentleman standing around were very sorry for her. Then he came up and paid the postage for her.
When the gentleman gave the letter to her, she said with a smile, “ Thank you very much, This letter is from Tom. I’m going
to marry him. He went to London to look for work. I’ve waited a long time for this letter, but now I don’t need it, there is
nothing in it.”
“Really? How do you know that?” the gentleman said in surprise.
“He told me that he would put some signs on the envelope. Look, sir, this cross in the corner means that he is well and this
circle means he has found work. That’s good news.”
The gentleman was Sir Rowland Hill. He didn’t forgot Alice and her letter.
“The postage to be paid by the receiver has to be changed,” he said to himself and had a good plan.
“The postage has to be much lower, what about a penny? And the person who sends the letter pays the postage. He has to buy
a stamp and put it on the envelope.” he said . The government accepted his plan. Then the ﬁrst stamp was put out in 1840. It
was called the “Penny Black”. It had a picture of the Queen on it.

Questions:

1): The ﬁrst postage stamp was made .
A. in England B. in America C. by Alice D. in 1910

2): The girl handed the letter back to the mailman because
.
A. she didn’t know whose letter it was
B. she had no money to pay the postage
C. she received the letter but she didn’t want to open it
D. she had already known what was written in the letter

3): We can know from Alice’s words that
A. Tom had told her what the signs meant before leaving
B. Alice was clever and could guess the meaning of the signs
C. Alice had put the signs on the envelope herself
D. Tom had put the signs as Alice had told him to

.

4): The idea of using stamps was thought of by .
A. the government
B. Sir Rowland Hill
C. Alice Brown
D. Tom

5): From the passage we know the high postage made .
A. people never send each other letters
B. lovers almost lose every touch with each other
C. people try their best to avoid paying it
D. receivers refuse to pay the coming letters

Answer: ADABC

Table 1: Sample reading comprehension problems from our dataset.

CNN/Daily Mail (Hermann et al., 2015) are
the largest machine comprehension datasets with
1.4M questions. However, both require limited
reasoning ability (Chen et al., 2016). In fact, the
best machine performance obtained by researchers
(Chen et al., 2016; Dhingra et al., 2016) is close to
human’s performance on CNN/Daily Mail.

Childrens Book Test (CBT) (Hill et al., 2015)
and Book Test (BT) (Bajgar et al., 2016) are con-
structed in a similar manner. Each passage in CBT
consist of 20 contiguous sentences extracted from
children’s books and the next (21st) sentence is
used to make the question. The main difference
between the two datasets is the size of BT being
60 times larger. Machine comprehension models
have also matched human performance on CBT
(Bajgar et al., 2016).

Who Did What (WDW) (Onishi et al., 2016)
is yet another cloze-style dataset constructed from
the LDC English Gigaword newswire corpus. The
authors generate passages and questions by pick-
ing two news articles describing the same event,

using one as the passage and the other as the ques-
tion.

High noise is inevitable in cloze-style datasets
due to their automatic generation process, which
is reﬂected in the human performance on these
datasets: 82% for CBT and 84% for WDW.

2.3 Datasets with Span-based Answers

In datasets such as SQUAD (Rajpurkar et al.,
2016), NEWSQA (Trischler et al., 2016) MS
MARCO (Nguyen et al., 2016) and recently pro-
posed TriviaQA (Joshi et al., 2017). the answer to
each question is in the form of a text span in the
article. Articles of SQUAD, NEWSQA and MS
MARCO come from Wikipedia, CNN news and
the Bing search engine respectively. The answer to
a certain question may not be unique and could be
multiple spans. Instead of evaluating the accuracy,
researchers need to use F1 score, BLEU (Papineni
et al., 2002) or ROUGE (Lin and Hovy, 2003)
as metrics, which measure the overlap between
the prediction and ground truth answers since the

questions come without candidate spans.

Datasets with span-based answers are challeng-
ing as the space of possible spans is usually large.
However, restricting answers to be text spans in
the context passage may be unrealistic and more
importantly, may not be intuitive even for humans,
indicated by the suffered human performance of
80.3% on SQUAD (or 65% claimed by Trischler
et al. (2016)) and 46.5% on NEWSQA. In other
words, the format of span-based answers may not
necessarily be a good examination of reading com-
prehension of machines whose aim is to approach
the comprehension ability of humans.

2.4 Datasets from Examinations

There have been several datasets extracted from
examinations, aiming at evaluating systems un-
der the same conditions as how humans are evalu-
ated in schools. E.g., the AI2 Elementary School
Science Questions dataset (Khashabi et al., 2016)
contains 1080 questions for students in elementary
schools; NTCIR QA Lab (Shibuki et al., 2014)
evaluates systems by the task of solving real-world
university entrance exam questions; The Entrance
Exams task at CLEF QA Track (Pe˜nas et al., 2014;
Rodrigo et al., 2015) evaluates the system’s read-
ing comprehension ability. However, data pro-
vided in these existing tasks are far from sufﬁcient
for the training of advanced data-driven machine
reading models, partially due to the expensive data
generation process by human experts.

To the best of our knowledge, RACE is the ﬁrst
large-scale dataset of this type, where questions
are created based on exams designed to evaluate
human performance in reading comprehension.

3 Data Analysis

In this section, we study the nature of questions
covered in RACE at a detailed level. Speciﬁcally,
we present the dataset statistics in Section 3.1, and
then analyze different reasoning/question types in
RACE in the remaining subsections.

3.1 Dataset Statistics

nations. We split 5% data as the development set
and 5% as the test set for RACE-M and RACE-H
respectively. The number of samples in each set is
shown in Table 2. The statistics for RACE-M and
RACE-H is summarized in Table 3. We can ﬁnd
that the length of the passages and the vocabulary
size in the RACE-H are much larger than that of
the RACE-M, an evidence of the higher difﬁculty
of high school examinations.

However, notice that since the articles and ques-
tions are selected and designed to test Chinese
students learning English as a foreign language,
the vocabulary size and the complexity of the lan-
guage constructs are simpler than news articles
and Wikipedia articles in other QA datasets.

3.2 Reasoning Types of the Questions

To get a comprehensive picture about the reason-
ing difﬁculty requirement of RACE, we conduct
human annotations of questions types. Following
Chen et al. (2016); Trischler et al. (2016), we strat-
ify the questions into ﬁve classes as follows with
ascending order of difﬁculty:

• Word matching:

The question exactly
matches a span in the article. The answer is
self-evident.

• Paraphrasing: The question is entailed or
paraphrased by exactly one sentence in the
passage. The answer can be extracted within
the sentence.

• Single-sentence reasoning: The answer could
be inferred from a single sentence of the arti-
cle by recognizing incomplete information or
conceptual overlap.

• Multi-sentence reasoning: The answer must
be inferred from synthesizing information
distributed across multiple sentences.

• Insufﬁcient/Ambiguous: The question has no
answer or the answer is not unique based on
the given passage.

As mentioned in section 1, RACE is collected
from English examinations designed for 12–15
year-old middle school students, and 15–18 year-
old high school students in China. To distin-
guish the two subgroups with drastic difﬁculty
gap, RACE-M denotes the middle school exami-
nations and RACE-H denotes high school exami-

We refer readers to (Chen et al., 2016; Trischler

et al., 2016) for examples of each category.

To obtain the proportion of different question
types, we sample 100 passages from RACE (50
from RACE-M and 50 from RACE-H), all of
which have 5 questions hence there are 500 ques-
tions in total. We put the passages on Amazon Me-

Dataset
Subset
# passages
# questions

RACE-M
Dev
368
1,436

Train
6,409
25,421

Test
362
1,436

Train
18,728
62,445

RACE-H
Dev
1,021
3,451

RACE

Test
1,045
3,498

Train
25,137
87,866

Dev
1,389
4,887

Test
1,407
4,934

All
27,933
97,687

Table 2: The separation of the training, development and test sets of RACE-M,RACE-H and RACE

Dataset
Passage Len
Question Len
Option Len
Vocab size

RACE-M RACE-H RACE
321.9
353.1
10.0
10.4
5.3
5.8
136,629
125,120

231.1
9.0
3.9
32,811

Table 3: Statistics of RACE where Len denotes
length and Vocab denotes Vocabulary.

chanical Turk1, and a Hit is generated by a passage
with 5 questions. Each question is labeled by two
crowdworkers. We require the turkers to both an-
swer the questions and label the reasoning type.
We pay $0.70 and $1.00 per passage in RACE-M
and RACE-H respectively, and restrict the access
to master turkers only. Finally, we get 1000 labels
for the 500 questions.

The statistics about the reasoning type is sum-
marized in Table 4. The higher difﬁculty level
of RACE is justiﬁed by its higher ratio of rea-
soning questions in comparison to CNN, SQUAD
and NEWSQA. Speciﬁcally, 59.2% questions of
RACE are either in the category of single-sentence
reasoning or in the category of multi-sentence
reasoning, while the ratio is 21%, 20.5% and
33.9% for CNN, SQUAD and NEWSQA respec-
tively. Also notice that the ratio of word match-
ing questions on RACE is only 15.8%, the lowest
among several categories. In addition, questions
in RACE-H are more complex than questions in
RACE-M since RACE-M has more word match-
ing questions and fewer reasoning questions.

3.3 Subdividing Reasoning Types

To better understand our dataset and facilitate fu-
ture research, we list the subdivisions of ques-
tions under the reasoning category. We ﬁnd the
most frequent reasoning subdivisions include: de-
tail reasoning, whole-picture understanding, pas-
sage summarization, attitude analysis and world
knowledge. One question may fall into multiple
divisions. Deﬁnition of these subdivisions and
their associated examples are as follows:

1https://www.mturk.com/mturk/welcome

1. Detail reasoning: to answer the question, the
agent should be clear about the details of the pas-
sage. The answer appears in the passage but it can-
not be found by simply matching the question with
the passage. For example, Question 1 in the sam-
ple passage falls into this category.

2. Whole-picture reasoning: the agent needs to
understand the whole picture of the story to ob-
tain the correct answer. For example, to answer
the Question 2 in the sample passage, the agent is
required to comprehend the entire story.

3. Passage summarization: The question re-
quires the agent to select the best summarization
of the passage among four candidate summariza-
tions. A typical question of this type is “The main
idea of this passage is
.”. An example question
can be found in Appendix A.1.

4. Attitude analysis: The question asks about
the opinions/attitudes of the author or a character
in the story towards somebody or something, e.g.,

• Evidence:
“. . . Many people optimistically
thought
industry awards for better equipment
the production of quieter
would stimulate
appliances. It was even suggested that noise from
building sites could be alleviated . . . ”

• Question: What was the author’s attitude towards

the industry awards for quieter?

• Options:

A.suspicious

B.positive

C.enthusiastic D.indifferent

5. World knowledge: Certain external knowl-
edge is needed. Most frequent questions under this
category involve simple arithmetic.

• Evidence: “The park is open from 8 am to 5 pm.”

• Question: The park is open for

hours a day.

• Options: A.eight B.nine C.ten D.eleven

To the best of our knowledge, questions like
passage summarization and attitude analysis have
not been introduced by any of the existing large-
scale machine comprehension datasets. Both are

Dataset
Word Matching
Paraphrasing
Single-Sentence Reasoning
Multi-Sentence Reasoning
Ambiguous/Insufﬁcient

RACE-M RACE-H RACE

29.4%
14.8%
31.3%
22.6%
1.8%

11.3%
20.6%
34.1%
26.9%
7.1%

CNN
15.8% 13.0%†
19.2% 41.0%†
33.4% 19.0%†
25.8% 2.0%†
5.8% 25.0%†

SQUAD NEWSQA
39.8%*
34.3%*
8.6%*
11.9%*
5.4%*

32.7%*
27.0%*
13.2%*
20.7%*
6.4%*

Table 4: Statistic information about Reasoning type in different datasets. * denotes the numbers coming
from (Trischler et al., 2016) based on 1000 samples per dataset, and numbers with † come from (Chen
et al., 2016).

crucial components in evaluating humans’ reading
comprehension abilities.

5 Experiments

4 Collection Methodology

We collected the raw data from three large free
public websites in China2, where the reading com-
prehension problems are extracted from English
examinations designed by teachers in China. The
data before cleaning contains 137,918 passages
and 519,878 questions in total, where there are
38,159 passages with 156,782 questions in the
middle school group, and 99,759 passages with
363,096 questions in the high school group.

The following ﬁltering steps are conducted to
clean the raw data. Firstly, we remove all prob-
lems and questions that do not have the same for-
mat as our problem setting, e.g., a question would
be removed if the number of its options is not four.
Secondly, we ﬁlter all articles and questions that
are not self-contained based on the text informa-
tion, i.e. we remove the articles and questions con-
taining images or tables. We also remove all ques-
tions containing keywords “underlined” or “para-
graph”, since it is difﬁcult to reproduce the effect
of underlines and the paragraph segment informa-
tion. Thirdly, we remove all duplicated articles.

On one of the websites (xkw.com), the answers
are stored as images. We used two standard OCR
programs tesseract 3 and ABBYY FineReader 4 to
process the images. We remove all the answers
that two software disagree. The OCR task is easy
since we only need to recognize printed alphabet
A, B, C, D with a standard font. Finally, we get
the cleaned dataset RACE, with 27,933 passages
and 97,687 questions.

In this section, we compare the performance
of several state-of-the-art reading comprehension
models with human performance. We use accu-
racy as the metric to evaluate different models.

5.1 Methods for Comparison

Sliding Window Algorithm Firstly, we build
the rule-based baseline introduced by Richardson
et al. (2013).
It chooses the answer having the
highest matching score. Speciﬁcally, it ﬁrst con-
catenates the question and the answer and then cal-
culates the TF-IDF style matching score between
the concatenated sentence with every window (a
span of text) of the article. The window size is
decided by the model performance in the training
and dev sets.

Stanford Attentive Reader Stanford Attentive
Reader (Stanford AR) (Chen et al., 2016) is a
strong model that achieves state-of-the-art results
on CNN/Daily Mail. Moreover, the authors claim
that their model has nearly reached the ceiling per-
formance on these two datasets.

Suppose that the triple of passage, question and
options is denoted by (p, q, o1,··· ,4). We ﬁrst em-
ploy bidirectional GRUs to encode p and q respec-
tively into hp
n and hq. Then we sum-
marize the most relevant part of the passage into
sp with an attention model. Following Chen et al.
(2016), we adopt a bilinear attention form. Specif-
ically,

2, . . . , hp

1, hp

αi = Softmaxi((hp
sp =

(cid:88)

αihp
i

i )T W1hq)

i

(1)

2We checked that our dataset does not include exam-
ple questions of exams with copyright, such as SSAT, SAT,
TOEFL and GRE.

3https://github.com/tesseract-ocr
4https://www.abbyy.com/FineReader

Similarly, we use bidirectional GRUs to encode
option oi into a vector hoi. Finally, we com-
pute the matching score between the i-th option
(i = 1, · · · , 4) and the summarized passage using

Random
Sliding Window
Stanford AR
GA
Turkers
Ceiling Performance

RACE-M RACE-H RACE MCTest CNN DM CBT-N CBT-C WDW
32.0†
10.2
19.6† 48.0†
64.0†
67.3† 71.2†

–

24.8
51.5†
–
–
–
–

10.6
0.06 0.06
24.8 30.8 16.8†
73.6† 76.6†
77.9† 80.9† 70.1†
–
–

–
81.6†

–
–

–

25.0
30.4
43.0
44.2
69.4
94.2

24.6
37.3
44.2
43.7
85.1
95.4

24.9
32.2
43.3
44.1
73.3
94.5

–
81.6†

–
84†

Table 5: Accuracy of models and human on the each dataset, where † denotes the results coming from
previous publications. DM denotes Daily Mail and WDW denotes Who-Did-What .

(a) RACE-M

(b) RACE-H

Figure 1: Test accuracy of different baselines on each question type category introduced in Section 3.2,
where Word-Match, Single-Reason, Multi-Reason and Ambiguous are the abbreviations for Word match-
ing, Single-sentence Reasoning, Multi-sentence Reasoning and Insufﬁcient/Ambiguous respectively.

a bilinear attention. We pass the scores through
softmax to get a probability distribution. Specif-
ically, the probability of option i being the right
answer is calculated as

pi = Softmaxi(hoiW2sd)

(2)

Gated-Attention Reader Gated AR (Dhingra
et al., 2016) is the state-of-the-art model on mul-
tiple datasets. To build query-speciﬁc represen-
tations of tokens in the document, it employs an
attention mechanism to model multiplicative in-
teractions between the query embedding and the
document representation. With a multi-hop ar-
chitecture, GA also enables a model to scan the
document and the question iteratively for multi-
ple passes.
In other words, the multi-hop struc-
ture makes it possible for the reader to reﬁne token
representations iteratively and the attention mech-
anism ﬁnd the most relevant part of the document.
We refer readers to (Dhingra et al., 2016) for more
details.

After obtaining a query speciﬁc document rep-
resentation sd, we use the same method as bilinear
operation listed in Equation 2 to get the output.

Note that our implementation slightly differs
from the original GA reader. Speciﬁcally, the At-
tention Sum layer is not applied at the ﬁnal layer
and no character-level embeddings are used.

Implementation Details We follow Chen et al.
(2016) in our experiment settings. The vocabulary
size is set to 50k. We choose word embedding
size d = 100 and use the 100-dimensional Glove
word embedding (Pennington et al., 2014) as em-
bedding initialization. GRU weights are initial-
ized from Gaussian distribution N (0, 0.1). Other
parameters are initialized from a uniform distri-
bution on (−0.01, 0.01). The hidden dimension-
ality is set to 128 and the number of layers is
set to one for both Stanford AR and GA. We use
vanilla stochastic gradient descent (SGD) to train
our models. We apply dropout on word embed-
dings and the gradient is clipped when the norm

of the gradient is larger than 10. We use a grid
search on validation set to choose the learning
rate within {0.05, 0.1, 0.3, 0.5} and dropout rate
within {0.2, 0.5, 0.7}. The highest accuracy on
validation set is obtained by setting learning rate to
0.1 for Stanford AR and 0.3 for GA and dropout
rate to 0.5. The data of RACE-M and RACE-H
is used together to train our model and testing is
performed separately.

5.2 Human Evaluation

As described in section 3.2, a randomly sam-
pled subset of test set has been labeled by Ama-
zon Turkers, which contains 500 questions with
half from RACE-H and with the other half from
RACE-M. The turkers’ performance is 85% for
RACE-M and 70% for RACE-H. However, it is
hard to guarantee that every turker performs the
survey carefully, given the difﬁcult and long pas-
sages of high school problems. Therefore, to ob-
tain the ceiling human performance on RACE,
we manually labeled the proportion of valid ques-
tions. A question is valid if it is unambiguous and
has a correct answer. We found that 94.5% of the
data is valid, which sets the ceiling human per-
formance. Similarly, the ceiling performance on
RACE-M and RACE-H is 95.4% and 94.2% re-
spectively.

5.3 Main Results

We compare models’ and human ceiling perfor-
mance on datasets which have the same evalua-
tion metric with RACE. The compared datasets
include RACE, MCTest, CNN/Daily Mail (CNN
and DM), CBT and WDW. On CBT, we report per-
formance on two subsets where the missing token
is either a common noun (CBT-C) or name entity
(CBT-N) since the language models have already
reached human-level performance on other types
(Hill et al., 2015). The comparison is shown in
Table 5.

Performance of Sliding Window We ﬁrst com-
pare MCTest with RACE using Sliding Window,
where it is unable to train Stanford AR and Gated
Slid-
AR on MCTest’s limited training data.
ing Window achieves an accuracy of 51.5% on
MCTest while only 37.3% on RACE, meaning that
to answer the questions of RACE requires more
reasoning than MCTest.

The performance of sliding window on RACE
is not directly comparable with CBT and WDW

since CBT has ten candidate answers for each
question and WDW has an average of three. In-
stead, we evaluate the performance improvement
of sliding window on the random baseline. Larger
improvement indicates more questions solvable by
simple matching. On RACE, Sliding Window is
28.6% better than the random baseline, while the
improvement is 58.5%, 92.2% and 50% for CBT-
N, CBT-C and WDW.

The accuracy on RACE-M (37.3%) and RACE-
H (30.4%) indicates that the middle school ques-
tions are simpler based on the matching algorithm.

Performance of Neural Models We further
compare the difﬁculty of different datasets by
state-of-the-art neural models’ performance. A
lower performance means that more problems are
unsolvable by machines. The Stanford AR and
Gated AR achieve an accuracy of only 43.3% and
44.1% on RACE while their accuracy is much
higher on CNN/Daily Mail, Childrens Book
Test and Who-Did-What. It justiﬁes the fact that,
among current large-scale machine comprehen-
sion datasets, RACE is the most challenging one.

Human Ceiling Performance The human per-
formance is 94.5% which shows our data is quite
clean compared to other large-scale machine com-
prehension datasets. Since we cannot enforce ev-
ery turker do the test cautiously, the result shows
a gap between turkers’ performance and human
performance. Reasonably, problems in the high
school group with longer passages and more com-
plex questions lead to more signiﬁcant divergence.
Nevertheless, the start-of-the-art models still have
a large room to be improved to reach turkers’ per-
formance. The performance gap is 41% for the
middle school problems and 25% for the high
school problems. What’s more, The performance
of Stanford AR and GA is only less than a half
of the ceiling human performance, which indicates
that to match the humans’ reading comprehension
ability, we still have a long way to go.

5.4 Reason Types Analysis

We evaluate human and models on different types
of questions, shown in Figure 1. Turkers do the
best on word matching problems while doing the
worst on reasoning problems. Sliding window
performs better on word matching than problems
needing reasoning or paraphrasing. Surprisingly,
Stanford AR does not have a stronger performance

on the word matching category than reasoning cat-
egories. A possible reason is that the proportion
of data in reasoning categories is larger than that
of data. Also, the candidate answers of simple
matching questions may share similar word em-
beddings. For example, if the question is about
color, it is difﬁcult to distinguish candidate an-
swers, “green”, “red”, “blue” and “yellow”, in the
embedding vector space. The similar performance
on different categories also explains the reason
that the performance of the neural models is close
in the middle and high school groups in Table 5.

6 Conclusion

We introduce a large, high-quality dataset for read-
ing comprehension that is carefully designed to
examine human ability on this task. Some desir-
able properties of RACE include the broad cover-
age of domains/styles and the richness in the ques-
tion format. Most importantly, it requires substan-
tially more reasoning to do well on RACE than
on other datasets, as there is a signiﬁcant gap be-
tween the performance of state-of-the-art machine
comprehension models and that of the human. We
hope this dataset will stimulate the development of
more advanced machine comprehension models.

Acknowledgement

We would like to thank Graham Neubig for sug-
gestions on the draft and Diyi Yang’s help on ob-
taining the crowdsourced labels.

This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program.

References

Ondrej Bajgar, Rudolf Kadlec, and Jan Kleindi-
enst. 2016. Embracing data abundance: Booktest
dataset for reading comprehension. arXiv preprint
arXiv:1610.00956 .

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A thorough examination of the cn-
arXiv
n/daily mail reading comprehension task.
preprint arXiv:1606.02858 .

Bhuwan Dhingra, Hanxiao Liu, William W Cohen,
and Ruslan Salakhutdinov. 2016. Gated-attention
arXiv preprint
readers for text comprehension.
arXiv:1606.01549 .

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-

chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301 .

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. ACL .

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv:1603.01547 .

Daniel Khashabi, Tushar Khot, Ashish Sabhar-
wal, Peter Clark, Oren Etzioni, and Dan Roth.
2016. Question answering via integer programming
arXiv preprint
over semi-structured knowledge.
arXiv:1604.06076 .

Chin-Yew Lin and Eduard Hovy. 2003.

Auto-
matic evaluation of summaries using n-gram co-
In Proceedings of the 2003
occurrence statistics.
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1. Association
for Computational Linguistics, pages 71–78.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
arXiv preprint
reading comprehension dataset.
arXiv:1611.09268 .

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. 2016. Who did what:
A large-scale person-centered cloze dataset. arXiv
preprint arXiv:1608.05457 .

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Anselmo Pe˜nas, Yusuke Miyao, ´Alvaro Rodrigo, Ed-
uard H Hovy, and Noriko Kando. 2014. Overview of
clef qa entrance exams task 2014. In CLEF (Work-
ing Notes). pages 1194–1200.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
EMNLP. volume 3, page 4.

´Alvaro Rodrigo, Anselmo Pe˜nas, Yusuke Miyao, Ed-
uard H Hovy, and Noriko Kando. 2015. Overview of
clef qa entrance exams task 2015. In CLEF (Work-
ing Notes).

Hideyuki Shibuki, Kotaro Sakamoto, Yoshinobu Kano,
Teruko Mitamura, Madoka Ishioroshi, Kelly Y
Itakura, Di Wang, Tatsunori Mori, and Noriko
Kando. 2014. Overview of the ntcir-11 qa-lab task.
In NTCIR.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830 .

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W Cohen. 2017. Semi-supervised qa with
arXiv preprint
generative domain-adaptive nets.
arXiv:1702.02206 .

A Appendix

A.1 Example Question of Passage

Summarization

Passage: Do you love holidays but hate gaining
weight? You are not alone. Holidays are times for
celebrating. Many people are worried about their
weight. With proper planning, though, it is pos-
sible to keep normal weight during the holidays.
The idea is to enjoy the holidays but not to eat too
much. You don’t have to turn away from the foods
that you enjoy.

Here are some tips for preventing weight gain

and maintaining physical ﬁtness:

Don’t skip meals. Before you leave home, have
a small, low-fat meal or snack. This may help to
avoid getting too excited before delicious foods.

Control the amount of food. Use a small plate
that may encourage you to ”load up”. You should
be most comfortable eating an amount of food
about the size of your ﬁst.

Begin with soup and fruit or vegetables. Fill up
beforehand on water-based soup and raw fruit or
vegetables, or drink a large glass of water before
you eat to help you to feel full.

Avoid high-fat foods. Dishes that look oily or
creamy may have large amount of fat. Choose lean
meat . Fill your plate with salad and green vegeta-
bles. Use lemon juice instead of creamy food.

Stick to physical activity. Don’t let exercise take
a break during the holidays. A 20-minute walk
helps to burn off extra calories.

Questions:
What is the best title of the passage?
Options:
A. How to avoid holiday feasting
B. Do’s and don’ts for keeping slim and ﬁt.
C. How to avoid weight gain over holidays.
D. Wonderful holidays, boring experiences.

RACE: Large-scale ReAding Comprehension Dataset From Examinations

Guokun Lai∗ and Qizhe Xie∗ and Hanxiao Liu and Yiming Yang and Eduard Hovy
{guokun, qzxie, hanxiaol, yiming, hovy}@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract

We present RACE, a new dataset for
benchmark evaluation of methods in the
reading comprehension task. Collected
from the English exams for middle and
high school Chinese students in the age
range between 12 to 18, RACE con-
sists of near 28,000 passages and near
100,000 questions generated by human
experts (English instructors), and cov-
ers a variety of topics which are care-
fully designed for evaluating the students’
ability in understanding and reasoning.
In particular, the proportion of questions
that requires reasoning is much larger
in RACE than that in other benchmark
datasets for reading comprehension, and
there is a signiﬁcant gap between the
performance of the state-of-the-art mod-
els (43%) and the ceiling human perfor-
mance (95%). We hope this new dataset
can serve as a valuable resource for re-
search and evaluation in machine com-
prehension. The dataset is freely avail-
able at http://www.cs.cmu.edu/
˜glai1/data/race/ and the code is
available at https://github.com/
qizhex/RACE_AR_baselines

1

Introduction

Constructing an intelligence agent capable of un-
derstanding text as people is the major challenge
of NLP research. With recent advances in deep
learning techniques, it seems possible to achieve
human-level performance in certain language un-
derstanding tasks, and a surge of effort has been
devoted to the machine comprehension task where
people aim to construct a system with the ability to

∗* indicates equal contribution

answer questions related to a document that it has
to comprehend (Chen et al., 2016; Kadlec et al.,
2016; Dhingra et al., 2016; Yang et al., 2017).

Towards this goal, several large-scale datasets
(Rajpurkar et al., 2016; Onishi et al., 2016; Hill
et al., 2015; Trischler et al., 2016; Hermann
et al., 2015) have been proposed, which allow re-
searchers to train deep learning systems and ob-
tain results comparable to the human performance.
While having a suitable dataset is crucial for eval-
uating the system’s true ability in reading compre-
hension, the existing datasets suffer several critical
limitations. Firstly, in all datasets, the candidate
options are directly extracted from the context (as
a single entity or a text span), which leads to the
fact that lots of questions can be solved trivially
via word-based search and context-matching with-
out deeper reasoning; this constrains the types of
questions as well. Secondly, answers and ques-
tions of most datasets are either crowd-sourced
or automatically-generated, bringing a signiﬁcant
amount of noises in the datasets and limits the ceil-
ing performance by domain experts, such as 82%
for Childrens Book Test and 84% for Who-did-
What. Yet another issue in existing datasets is that
the topic coverages are often biased due to the spe-
ciﬁc ways that the data were initially collected,
making it hard to evaluate the ability of systems in
text comprehension over a broader range of topics.
To address the aforementioned limitations, we
constructed a new dataset by collecting a large
set of questions, answers and associated pas-
sages in the English exams for middle-school and
high-school Chinese students within the 12–18
age range. Those exams were designed by do-
main experts (instructors) for evaluating the read-
ing comprehension ability of students, with en-
Fur-
sured quality and broad topic coverage.
thermore,
the answers by machines or by hu-
mans can be objectively graded for evaluation

7
1
0
2
 
c
e
D
 
5
 
 
]
L
C
.
s
c
[
 
 
5
v
3
8
6
4
0
.
4
0
7
1
:
v
i
X
r
a

and comparison using the same evaluation met-
rics. Although efforts have been made with a sim-
ilar motivation, including the MCTest dataset cre-
ated by (Richardson et al., 2013) (containing 500
passages and 2000 questions) and several others
(Pe˜nas et al., 2014; Rodrigo et al., 2015; Khashabi
et al., 2016; Shibuki et al., 2014), the usefulness
of those datasets is signiﬁcantly restricted due to
their small sizes, especially not suitable for train-
ing powerful deep neural networks whose success
relies on the availability of relatively large training
sets.

Our new dataset, namely RACE, consists of
27,933 passages and 97,687 questions. After read-
ing each passage, each student is asked to answer
several questions where each question is provided
with four candidate answers – only one of them is
correct . Unlike existing datasets, both the ques-
tions and candidate answers in RACE are not re-
stricted to be the text spans in the original passage;
instead, they can be described in any words. A
sample from our dataset is presented in Table 1.

Our latter analysis shows that correctly answer-
ing a large portion of questions in RACE requires
the ability of reasoning, the most important fea-
ture as a machine comprehension dataset (Chen
et al., 2016). RACE also offers two important sub-
divisions of the reasoning types in its questions,
namely passage summarization and attitude anal-
ysis, which have not been introduced by the any of
the existing large-scale datasets to our knowledge.
In addition, compared to other existing datasets
where passages are either domain-speciﬁc or of a
single ﬁxed style (namely news stories for CNN/-
Daily Mail, NEWSQA and Who-did-What, ﬁction
stories for Children’s Book Test and Book Test,
and Wikipedia articles for SQUAD), passages in
RACE almost cover all types of human articles,
such as news, stories, ads, biography, philosophy,
etc., in a variety of styles. This comprehensiveness
of topic/style coverage makes RACE a desirable
resource for evaluating the reading comprehension
ability of machine learning systems in general.

The advantages of our proposed dataset over ex-
isting large datasets in machine reading compre-
hension can be summarized as follows:

text comprehension ability of machine learn-
ing systems under human judge.

• The questions are substantially more difﬁcult
than those in existing datasets, in terms of the
large portion of questions involving reason-
ing. At the meantime, it is also sufﬁciently
large to support the training of deep learning
models.

• Unlike existing large-scale datasets, candi-
date options in RACE are human generated
sentences which may not appear in the origi-
nal passage. This makes the task more chal-
lenging and allows a rich type of questions
such as passage summarization and attitude
analysis.

• Broad coverage in various domains and writ-
ing styles: a desirable property for evaluating
generic (in contrast to domain/style-speciﬁc)
comprehension ability of learning models.

2 Related Work

In this section, we brieﬂy outline existing datasets
for the machine reading comprehension task, in-
cluding their strengths and weaknesses.

2.1 MCTest

MCTest (Richardson et al., 2013) is a popular
dataset for question answering in the same for-
mat as RACE, where each question is associated
with four candidate answers with a single cor-
rect answer. Although questions in MCTest are
of high-quality ensured by careful examinations
through crowdsourcing, it contains only 500 stores
and 2000 questions, which substantially restricts
its usage in training advanced machine compre-
hension models. Moreover, while MCTest is de-
signed for 7 years old children, RACE is con-
structed for middle and high school students at
12–18 years old hence is more complicated and
requires stronger reasoning skills. In other words,
RACE can be viewed as a larger and more difﬁcult
version of the MCTest dataset.

2.2 Cloze-style datasets

• All questions and candidate options are gen-
erated by human experts, which are intention-
ally designed to test human agent’s ability in
reading comprehension. This makes RACE a
relatively accurate indicator for reﬂecting the

The past few years have witnessed several large-
scale cloze-style datasets (Hermann et al., 2015;
Hill et al., 2015; Bajgar et al., 2016; Onishi et al.,
2016), whose questions are formulated by obliter-
ating a word or an entity in a sentence.

Passage:
In a small village in England about 150 years ago, a mail coach was standing on the street. It didn’t come to that village often.
People had to pay a lot to get a letter. The person who sent the letter didn’t have to pay the postage, while the receiver had to.
“Here’s a letter for Miss Alice Brown,” said the mailman.
“ I’m Alice Brown,” a girl of about 18 said in a low voice.
Alice looked at the envelope for a minute, and then handed it back to the mailman.
“I’m sorry I can’t take it, I don’t have enough money to pay it”, she said.
A gentleman standing around were very sorry for her. Then he came up and paid the postage for her.
When the gentleman gave the letter to her, she said with a smile, “ Thank you very much, This letter is from Tom. I’m going
to marry him. He went to London to look for work. I’ve waited a long time for this letter, but now I don’t need it, there is
nothing in it.”
“Really? How do you know that?” the gentleman said in surprise.
“He told me that he would put some signs on the envelope. Look, sir, this cross in the corner means that he is well and this
circle means he has found work. That’s good news.”
The gentleman was Sir Rowland Hill. He didn’t forgot Alice and her letter.
“The postage to be paid by the receiver has to be changed,” he said to himself and had a good plan.
“The postage has to be much lower, what about a penny? And the person who sends the letter pays the postage. He has to buy
a stamp and put it on the envelope.” he said . The government accepted his plan. Then the ﬁrst stamp was put out in 1840. It
was called the “Penny Black”. It had a picture of the Queen on it.

Questions:

1): The ﬁrst postage stamp was made .
A. in England B. in America C. by Alice D. in 1910

2): The girl handed the letter back to the mailman because
.
A. she didn’t know whose letter it was
B. she had no money to pay the postage
C. she received the letter but she didn’t want to open it
D. she had already known what was written in the letter

3): We can know from Alice’s words that
A. Tom had told her what the signs meant before leaving
B. Alice was clever and could guess the meaning of the signs
C. Alice had put the signs on the envelope herself
D. Tom had put the signs as Alice had told him to

.

4): The idea of using stamps was thought of by .
A. the government
B. Sir Rowland Hill
C. Alice Brown
D. Tom

5): From the passage we know the high postage made .
A. people never send each other letters
B. lovers almost lose every touch with each other
C. people try their best to avoid paying it
D. receivers refuse to pay the coming letters

Answer: ADABC

Table 1: Sample reading comprehension problems from our dataset.

CNN/Daily Mail (Hermann et al., 2015) are
the largest machine comprehension datasets with
1.4M questions. However, both require limited
reasoning ability (Chen et al., 2016). In fact, the
best machine performance obtained by researchers
(Chen et al., 2016; Dhingra et al., 2016) is close to
human’s performance on CNN/Daily Mail.

Childrens Book Test (CBT) (Hill et al., 2015)
and Book Test (BT) (Bajgar et al., 2016) are con-
structed in a similar manner. Each passage in CBT
consist of 20 contiguous sentences extracted from
children’s books and the next (21st) sentence is
used to make the question. The main difference
between the two datasets is the size of BT being
60 times larger. Machine comprehension models
have also matched human performance on CBT
(Bajgar et al., 2016).

Who Did What (WDW) (Onishi et al., 2016)
is yet another cloze-style dataset constructed from
the LDC English Gigaword newswire corpus. The
authors generate passages and questions by pick-
ing two news articles describing the same event,

using one as the passage and the other as the ques-
tion.

High noise is inevitable in cloze-style datasets
due to their automatic generation process, which
is reﬂected in the human performance on these
datasets: 82% for CBT and 84% for WDW.

2.3 Datasets with Span-based Answers

In datasets such as SQUAD (Rajpurkar et al.,
2016), NEWSQA (Trischler et al., 2016) MS
MARCO (Nguyen et al., 2016) and recently pro-
posed TriviaQA (Joshi et al., 2017). the answer to
each question is in the form of a text span in the
article. Articles of SQUAD, NEWSQA and MS
MARCO come from Wikipedia, CNN news and
the Bing search engine respectively. The answer to
a certain question may not be unique and could be
multiple spans. Instead of evaluating the accuracy,
researchers need to use F1 score, BLEU (Papineni
et al., 2002) or ROUGE (Lin and Hovy, 2003)
as metrics, which measure the overlap between
the prediction and ground truth answers since the

questions come without candidate spans.

Datasets with span-based answers are challeng-
ing as the space of possible spans is usually large.
However, restricting answers to be text spans in
the context passage may be unrealistic and more
importantly, may not be intuitive even for humans,
indicated by the suffered human performance of
80.3% on SQUAD (or 65% claimed by Trischler
et al. (2016)) and 46.5% on NEWSQA. In other
words, the format of span-based answers may not
necessarily be a good examination of reading com-
prehension of machines whose aim is to approach
the comprehension ability of humans.

2.4 Datasets from Examinations

There have been several datasets extracted from
examinations, aiming at evaluating systems un-
der the same conditions as how humans are evalu-
ated in schools. E.g., the AI2 Elementary School
Science Questions dataset (Khashabi et al., 2016)
contains 1080 questions for students in elementary
schools; NTCIR QA Lab (Shibuki et al., 2014)
evaluates systems by the task of solving real-world
university entrance exam questions; The Entrance
Exams task at CLEF QA Track (Pe˜nas et al., 2014;
Rodrigo et al., 2015) evaluates the system’s read-
ing comprehension ability. However, data pro-
vided in these existing tasks are far from sufﬁcient
for the training of advanced data-driven machine
reading models, partially due to the expensive data
generation process by human experts.

To the best of our knowledge, RACE is the ﬁrst
large-scale dataset of this type, where questions
are created based on exams designed to evaluate
human performance in reading comprehension.

3 Data Analysis

In this section, we study the nature of questions
covered in RACE at a detailed level. Speciﬁcally,
we present the dataset statistics in Section 3.1, and
then analyze different reasoning/question types in
RACE in the remaining subsections.

3.1 Dataset Statistics

nations. We split 5% data as the development set
and 5% as the test set for RACE-M and RACE-H
respectively. The number of samples in each set is
shown in Table 2. The statistics for RACE-M and
RACE-H is summarized in Table 3. We can ﬁnd
that the length of the passages and the vocabulary
size in the RACE-H are much larger than that of
the RACE-M, an evidence of the higher difﬁculty
of high school examinations.

However, notice that since the articles and ques-
tions are selected and designed to test Chinese
students learning English as a foreign language,
the vocabulary size and the complexity of the lan-
guage constructs are simpler than news articles
and Wikipedia articles in other QA datasets.

3.2 Reasoning Types of the Questions

To get a comprehensive picture about the reason-
ing difﬁculty requirement of RACE, we conduct
human annotations of questions types. Following
Chen et al. (2016); Trischler et al. (2016), we strat-
ify the questions into ﬁve classes as follows with
ascending order of difﬁculty:

• Word matching:

The question exactly
matches a span in the article. The answer is
self-evident.

• Paraphrasing: The question is entailed or
paraphrased by exactly one sentence in the
passage. The answer can be extracted within
the sentence.

• Single-sentence reasoning: The answer could
be inferred from a single sentence of the arti-
cle by recognizing incomplete information or
conceptual overlap.

• Multi-sentence reasoning: The answer must
be inferred from synthesizing information
distributed across multiple sentences.

• Insufﬁcient/Ambiguous: The question has no
answer or the answer is not unique based on
the given passage.

As mentioned in section 1, RACE is collected
from English examinations designed for 12–15
year-old middle school students, and 15–18 year-
old high school students in China. To distin-
guish the two subgroups with drastic difﬁculty
gap, RACE-M denotes the middle school exami-
nations and RACE-H denotes high school exami-

We refer readers to (Chen et al., 2016; Trischler

et al., 2016) for examples of each category.

To obtain the proportion of different question
types, we sample 100 passages from RACE (50
from RACE-M and 50 from RACE-H), all of
which have 5 questions hence there are 500 ques-
tions in total. We put the passages on Amazon Me-

Dataset
Subset
# passages
# questions

RACE-M
Dev
368
1,436

Train
6,409
25,421

Test
362
1,436

Train
18,728
62,445

RACE-H
Dev
1,021
3,451

RACE

Test
1,045
3,498

Train
25,137
87,866

Dev
1,389
4,887

Test
1,407
4,934

All
27,933
97,687

Table 2: The separation of the training, development and test sets of RACE-M,RACE-H and RACE

Dataset
Passage Len
Question Len
Option Len
Vocab size

RACE-M RACE-H RACE
321.9
353.1
10.0
10.4
5.3
5.8
136,629
125,120

231.1
9.0
3.9
32,811

Table 3: Statistics of RACE where Len denotes
length and Vocab denotes Vocabulary.

chanical Turk1, and a Hit is generated by a passage
with 5 questions. Each question is labeled by two
crowdworkers. We require the turkers to both an-
swer the questions and label the reasoning type.
We pay $0.70 and $1.00 per passage in RACE-M
and RACE-H respectively, and restrict the access
to master turkers only. Finally, we get 1000 labels
for the 500 questions.

The statistics about the reasoning type is sum-
marized in Table 4. The higher difﬁculty level
of RACE is justiﬁed by its higher ratio of rea-
soning questions in comparison to CNN, SQUAD
and NEWSQA. Speciﬁcally, 59.2% questions of
RACE are either in the category of single-sentence
reasoning or in the category of multi-sentence
reasoning, while the ratio is 21%, 20.5% and
33.9% for CNN, SQUAD and NEWSQA respec-
tively. Also notice that the ratio of word match-
ing questions on RACE is only 15.8%, the lowest
among several categories. In addition, questions
in RACE-H are more complex than questions in
RACE-M since RACE-M has more word match-
ing questions and fewer reasoning questions.

3.3 Subdividing Reasoning Types

To better understand our dataset and facilitate fu-
ture research, we list the subdivisions of ques-
tions under the reasoning category. We ﬁnd the
most frequent reasoning subdivisions include: de-
tail reasoning, whole-picture understanding, pas-
sage summarization, attitude analysis and world
knowledge. One question may fall into multiple
divisions. Deﬁnition of these subdivisions and
their associated examples are as follows:

1https://www.mturk.com/mturk/welcome

1. Detail reasoning: to answer the question, the
agent should be clear about the details of the pas-
sage. The answer appears in the passage but it can-
not be found by simply matching the question with
the passage. For example, Question 1 in the sam-
ple passage falls into this category.

2. Whole-picture reasoning: the agent needs to
understand the whole picture of the story to ob-
tain the correct answer. For example, to answer
the Question 2 in the sample passage, the agent is
required to comprehend the entire story.

3. Passage summarization: The question re-
quires the agent to select the best summarization
of the passage among four candidate summariza-
tions. A typical question of this type is “The main
idea of this passage is
.”. An example question
can be found in Appendix A.1.

4. Attitude analysis: The question asks about
the opinions/attitudes of the author or a character
in the story towards somebody or something, e.g.,

• Evidence:
“. . . Many people optimistically
thought
industry awards for better equipment
the production of quieter
would stimulate
appliances. It was even suggested that noise from
building sites could be alleviated . . . ”

• Question: What was the author’s attitude towards

the industry awards for quieter?

• Options:

A.suspicious

B.positive

C.enthusiastic D.indifferent

5. World knowledge: Certain external knowl-
edge is needed. Most frequent questions under this
category involve simple arithmetic.

• Evidence: “The park is open from 8 am to 5 pm.”

• Question: The park is open for

hours a day.

• Options: A.eight B.nine C.ten D.eleven

To the best of our knowledge, questions like
passage summarization and attitude analysis have
not been introduced by any of the existing large-
scale machine comprehension datasets. Both are

Dataset
Word Matching
Paraphrasing
Single-Sentence Reasoning
Multi-Sentence Reasoning
Ambiguous/Insufﬁcient

RACE-M RACE-H RACE

29.4%
14.8%
31.3%
22.6%
1.8%

11.3%
20.6%
34.1%
26.9%
7.1%

CNN
15.8% 13.0%†
19.2% 41.0%†
33.4% 19.0%†
25.8% 2.0%†
5.8% 25.0%†

SQUAD NEWSQA
39.8%*
34.3%*
8.6%*
11.9%*
5.4%*

32.7%*
27.0%*
13.2%*
20.7%*
6.4%*

Table 4: Statistic information about Reasoning type in different datasets. * denotes the numbers coming
from (Trischler et al., 2016) based on 1000 samples per dataset, and numbers with † come from (Chen
et al., 2016).

crucial components in evaluating humans’ reading
comprehension abilities.

5 Experiments

4 Collection Methodology

We collected the raw data from three large free
public websites in China2, where the reading com-
prehension problems are extracted from English
examinations designed by teachers in China. The
data before cleaning contains 137,918 passages
and 519,878 questions in total, where there are
38,159 passages with 156,782 questions in the
middle school group, and 99,759 passages with
363,096 questions in the high school group.

The following ﬁltering steps are conducted to
clean the raw data. Firstly, we remove all prob-
lems and questions that do not have the same for-
mat as our problem setting, e.g., a question would
be removed if the number of its options is not four.
Secondly, we ﬁlter all articles and questions that
are not self-contained based on the text informa-
tion, i.e. we remove the articles and questions con-
taining images or tables. We also remove all ques-
tions containing keywords “underlined” or “para-
graph”, since it is difﬁcult to reproduce the effect
of underlines and the paragraph segment informa-
tion. Thirdly, we remove all duplicated articles.

On one of the websites (xkw.com), the answers
are stored as images. We used two standard OCR
programs tesseract 3 and ABBYY FineReader 4 to
process the images. We remove all the answers
that two software disagree. The OCR task is easy
since we only need to recognize printed alphabet
A, B, C, D with a standard font. Finally, we get
the cleaned dataset RACE, with 27,933 passages
and 97,687 questions.

In this section, we compare the performance
of several state-of-the-art reading comprehension
models with human performance. We use accu-
racy as the metric to evaluate different models.

5.1 Methods for Comparison

Sliding Window Algorithm Firstly, we build
the rule-based baseline introduced by Richardson
et al. (2013).
It chooses the answer having the
highest matching score. Speciﬁcally, it ﬁrst con-
catenates the question and the answer and then cal-
culates the TF-IDF style matching score between
the concatenated sentence with every window (a
span of text) of the article. The window size is
decided by the model performance in the training
and dev sets.

Stanford Attentive Reader Stanford Attentive
Reader (Stanford AR) (Chen et al., 2016) is a
strong model that achieves state-of-the-art results
on CNN/Daily Mail. Moreover, the authors claim
that their model has nearly reached the ceiling per-
formance on these two datasets.

Suppose that the triple of passage, question and
options is denoted by (p, q, o1,··· ,4). We ﬁrst em-
ploy bidirectional GRUs to encode p and q respec-
tively into hp
n and hq. Then we sum-
marize the most relevant part of the passage into
sp with an attention model. Following Chen et al.
(2016), we adopt a bilinear attention form. Specif-
ically,

2, . . . , hp

1, hp

αi = Softmaxi((hp
sp =

(cid:88)

αihp
i

i )T W1hq)

i

(1)

2We checked that our dataset does not include exam-
ple questions of exams with copyright, such as SSAT, SAT,
TOEFL and GRE.

3https://github.com/tesseract-ocr
4https://www.abbyy.com/FineReader

Similarly, we use bidirectional GRUs to encode
option oi into a vector hoi. Finally, we com-
pute the matching score between the i-th option
(i = 1, · · · , 4) and the summarized passage using

Random
Sliding Window
Stanford AR
GA
Turkers
Ceiling Performance

RACE-M RACE-H RACE MCTest CNN DM CBT-N CBT-C WDW
32.0†
10.2
19.6† 48.0†
64.0†
67.3† 71.2†

–

–

24.8
51.5†
–
–
–
–

10.6
0.06 0.06
24.8 30.8 16.8†
73.6† 76.6†
77.9† 80.9† 70.1†
–
–

–
81.6†

–
–

25.0
30.4
43.0
44.2
69.4
94.2

24.6
37.3
44.2
43.7
85.1
95.4

24.9
32.2
43.3
44.1
73.3
94.5

–
81.6†

–
84†

Table 5: Accuracy of models and human on the each dataset, where † denotes the results coming from
previous publications. DM denotes Daily Mail and WDW denotes Who-Did-What .

(a) RACE-M

(b) RACE-H

Figure 1: Test accuracy of different baselines on each question type category introduced in Section 3.2,
where Word-Match, Single-Reason, Multi-Reason and Ambiguous are the abbreviations for Word match-
ing, Single-sentence Reasoning, Multi-sentence Reasoning and Insufﬁcient/Ambiguous respectively.

a bilinear attention. We pass the scores through
softmax to get a probability distribution. Specif-
ically, the probability of option i being the right
answer is calculated as

pi = Softmaxi(hoiW2sd)

(2)

Gated-Attention Reader Gated AR (Dhingra
et al., 2016) is the state-of-the-art model on mul-
tiple datasets. To build query-speciﬁc represen-
tations of tokens in the document, it employs an
attention mechanism to model multiplicative in-
teractions between the query embedding and the
document representation. With a multi-hop ar-
chitecture, GA also enables a model to scan the
document and the question iteratively for multi-
ple passes.
In other words, the multi-hop struc-
ture makes it possible for the reader to reﬁne token
representations iteratively and the attention mech-
anism ﬁnd the most relevant part of the document.
We refer readers to (Dhingra et al., 2016) for more
details.

After obtaining a query speciﬁc document rep-
resentation sd, we use the same method as bilinear
operation listed in Equation 2 to get the output.

Note that our implementation slightly differs
from the original GA reader. Speciﬁcally, the At-
tention Sum layer is not applied at the ﬁnal layer
and no character-level embeddings are used.

Implementation Details We follow Chen et al.
(2016) in our experiment settings. The vocabulary
size is set to 50k. We choose word embedding
size d = 100 and use the 100-dimensional Glove
word embedding (Pennington et al., 2014) as em-
bedding initialization. GRU weights are initial-
ized from Gaussian distribution N (0, 0.1). Other
parameters are initialized from a uniform distri-
bution on (−0.01, 0.01). The hidden dimension-
ality is set to 128 and the number of layers is
set to one for both Stanford AR and GA. We use
vanilla stochastic gradient descent (SGD) to train
our models. We apply dropout on word embed-
dings and the gradient is clipped when the norm

of the gradient is larger than 10. We use a grid
search on validation set to choose the learning
rate within {0.05, 0.1, 0.3, 0.5} and dropout rate
within {0.2, 0.5, 0.7}. The highest accuracy on
validation set is obtained by setting learning rate to
0.1 for Stanford AR and 0.3 for GA and dropout
rate to 0.5. The data of RACE-M and RACE-H
is used together to train our model and testing is
performed separately.

5.2 Human Evaluation

As described in section 3.2, a randomly sam-
pled subset of test set has been labeled by Ama-
zon Turkers, which contains 500 questions with
half from RACE-H and with the other half from
RACE-M. The turkers’ performance is 85% for
RACE-M and 70% for RACE-H. However, it is
hard to guarantee that every turker performs the
survey carefully, given the difﬁcult and long pas-
sages of high school problems. Therefore, to ob-
tain the ceiling human performance on RACE,
we manually labeled the proportion of valid ques-
tions. A question is valid if it is unambiguous and
has a correct answer. We found that 94.5% of the
data is valid, which sets the ceiling human per-
formance. Similarly, the ceiling performance on
RACE-M and RACE-H is 95.4% and 94.2% re-
spectively.

5.3 Main Results

We compare models’ and human ceiling perfor-
mance on datasets which have the same evalua-
tion metric with RACE. The compared datasets
include RACE, MCTest, CNN/Daily Mail (CNN
and DM), CBT and WDW. On CBT, we report per-
formance on two subsets where the missing token
is either a common noun (CBT-C) or name entity
(CBT-N) since the language models have already
reached human-level performance on other types
(Hill et al., 2015). The comparison is shown in
Table 5.

Performance of Sliding Window We ﬁrst com-
pare MCTest with RACE using Sliding Window,
where it is unable to train Stanford AR and Gated
Slid-
AR on MCTest’s limited training data.
ing Window achieves an accuracy of 51.5% on
MCTest while only 37.3% on RACE, meaning that
to answer the questions of RACE requires more
reasoning than MCTest.

The performance of sliding window on RACE
is not directly comparable with CBT and WDW

since CBT has ten candidate answers for each
question and WDW has an average of three. In-
stead, we evaluate the performance improvement
of sliding window on the random baseline. Larger
improvement indicates more questions solvable by
simple matching. On RACE, Sliding Window is
28.6% better than the random baseline, while the
improvement is 58.5%, 92.2% and 50% for CBT-
N, CBT-C and WDW.

The accuracy on RACE-M (37.3%) and RACE-
H (30.4%) indicates that the middle school ques-
tions are simpler based on the matching algorithm.

Performance of Neural Models We further
compare the difﬁculty of different datasets by
state-of-the-art neural models’ performance. A
lower performance means that more problems are
unsolvable by machines. The Stanford AR and
Gated AR achieve an accuracy of only 43.3% and
44.1% on RACE while their accuracy is much
higher on CNN/Daily Mail, Childrens Book
Test and Who-Did-What. It justiﬁes the fact that,
among current large-scale machine comprehen-
sion datasets, RACE is the most challenging one.

Human Ceiling Performance The human per-
formance is 94.5% which shows our data is quite
clean compared to other large-scale machine com-
prehension datasets. Since we cannot enforce ev-
ery turker do the test cautiously, the result shows
a gap between turkers’ performance and human
performance. Reasonably, problems in the high
school group with longer passages and more com-
plex questions lead to more signiﬁcant divergence.
Nevertheless, the start-of-the-art models still have
a large room to be improved to reach turkers’ per-
formance. The performance gap is 41% for the
middle school problems and 25% for the high
school problems. What’s more, The performance
of Stanford AR and GA is only less than a half
of the ceiling human performance, which indicates
that to match the humans’ reading comprehension
ability, we still have a long way to go.

5.4 Reason Types Analysis

We evaluate human and models on different types
of questions, shown in Figure 1. Turkers do the
best on word matching problems while doing the
worst on reasoning problems. Sliding window
performs better on word matching than problems
needing reasoning or paraphrasing. Surprisingly,
Stanford AR does not have a stronger performance

on the word matching category than reasoning cat-
egories. A possible reason is that the proportion
of data in reasoning categories is larger than that
of data. Also, the candidate answers of simple
matching questions may share similar word em-
beddings. For example, if the question is about
color, it is difﬁcult to distinguish candidate an-
swers, “green”, “red”, “blue” and “yellow”, in the
embedding vector space. The similar performance
on different categories also explains the reason
that the performance of the neural models is close
in the middle and high school groups in Table 5.

6 Conclusion

We introduce a large, high-quality dataset for read-
ing comprehension that is carefully designed to
examine human ability on this task. Some desir-
able properties of RACE include the broad cover-
age of domains/styles and the richness in the ques-
tion format. Most importantly, it requires substan-
tially more reasoning to do well on RACE than
on other datasets, as there is a signiﬁcant gap be-
tween the performance of state-of-the-art machine
comprehension models and that of the human. We
hope this dataset will stimulate the development of
more advanced machine comprehension models.

Acknowledgement

We would like to thank Graham Neubig for sug-
gestions on the draft and Diyi Yang’s help on ob-
taining the crowdsourced labels.

This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program.

References

Ondrej Bajgar, Rudolf Kadlec, and Jan Kleindi-
enst. 2016. Embracing data abundance: Booktest
dataset for reading comprehension. arXiv preprint
arXiv:1610.00956 .

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A thorough examination of the cn-
arXiv
n/daily mail reading comprehension task.
preprint arXiv:1606.02858 .

Bhuwan Dhingra, Hanxiao Liu, William W Cohen,
and Ruslan Salakhutdinov. 2016. Gated-attention
arXiv preprint
readers for text comprehension.
arXiv:1606.01549 .

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-

chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301 .

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. ACL .

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv:1603.01547 .

Daniel Khashabi, Tushar Khot, Ashish Sabhar-
wal, Peter Clark, Oren Etzioni, and Dan Roth.
2016. Question answering via integer programming
arXiv preprint
over semi-structured knowledge.
arXiv:1604.06076 .

Chin-Yew Lin and Eduard Hovy. 2003.

Auto-
matic evaluation of summaries using n-gram co-
In Proceedings of the 2003
occurrence statistics.
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1. Association
for Computational Linguistics, pages 71–78.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
arXiv preprint
reading comprehension dataset.
arXiv:1611.09268 .

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. 2016. Who did what:
A large-scale person-centered cloze dataset. arXiv
preprint arXiv:1608.05457 .

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Anselmo Pe˜nas, Yusuke Miyao, ´Alvaro Rodrigo, Ed-
uard H Hovy, and Noriko Kando. 2014. Overview of
clef qa entrance exams task 2014. In CLEF (Work-
ing Notes). pages 1194–1200.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
EMNLP. volume 3, page 4.

´Alvaro Rodrigo, Anselmo Pe˜nas, Yusuke Miyao, Ed-
uard H Hovy, and Noriko Kando. 2015. Overview of
clef qa entrance exams task 2015. In CLEF (Work-
ing Notes).

Hideyuki Shibuki, Kotaro Sakamoto, Yoshinobu Kano,
Teruko Mitamura, Madoka Ishioroshi, Kelly Y
Itakura, Di Wang, Tatsunori Mori, and Noriko
Kando. 2014. Overview of the ntcir-11 qa-lab task.
In NTCIR.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830 .

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W Cohen. 2017. Semi-supervised qa with
arXiv preprint
generative domain-adaptive nets.
arXiv:1702.02206 .

A Appendix

A.1 Example Question of Passage

Summarization

Passage: Do you love holidays but hate gaining
weight? You are not alone. Holidays are times for
celebrating. Many people are worried about their
weight. With proper planning, though, it is pos-
sible to keep normal weight during the holidays.
The idea is to enjoy the holidays but not to eat too
much. You don’t have to turn away from the foods
that you enjoy.

Here are some tips for preventing weight gain

and maintaining physical ﬁtness:

Don’t skip meals. Before you leave home, have
a small, low-fat meal or snack. This may help to
avoid getting too excited before delicious foods.

Control the amount of food. Use a small plate
that may encourage you to ”load up”. You should
be most comfortable eating an amount of food
about the size of your ﬁst.

Begin with soup and fruit or vegetables. Fill up
beforehand on water-based soup and raw fruit or
vegetables, or drink a large glass of water before
you eat to help you to feel full.

Avoid high-fat foods. Dishes that look oily or
creamy may have large amount of fat. Choose lean
meat . Fill your plate with salad and green vegeta-
bles. Use lemon juice instead of creamy food.

Stick to physical activity. Don’t let exercise take
a break during the holidays. A 20-minute walk
helps to burn off extra calories.

Questions:
What is the best title of the passage?
Options:
A. How to avoid holiday feasting
B. Do’s and don’ts for keeping slim and ﬁt.
C. How to avoid weight gain over holidays.
D. Wonderful holidays, boring experiences.

RACE: Large-scale ReAding Comprehension Dataset From Examinations

Guokun Lai∗ and Qizhe Xie∗ and Hanxiao Liu and Yiming Yang and Eduard Hovy
{guokun, qzxie, hanxiaol, yiming, hovy}@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract

We present RACE, a new dataset for
benchmark evaluation of methods in the
reading comprehension task. Collected
from the English exams for middle and
high school Chinese students in the age
range between 12 to 18, RACE con-
sists of near 28,000 passages and near
100,000 questions generated by human
experts (English instructors), and cov-
ers a variety of topics which are care-
fully designed for evaluating the students’
ability in understanding and reasoning.
In particular, the proportion of questions
that requires reasoning is much larger
in RACE than that in other benchmark
datasets for reading comprehension, and
there is a signiﬁcant gap between the
performance of the state-of-the-art mod-
els (43%) and the ceiling human perfor-
mance (95%). We hope this new dataset
can serve as a valuable resource for re-
search and evaluation in machine com-
prehension. The dataset is freely avail-
able at http://www.cs.cmu.edu/
˜glai1/data/race/ and the code is
available at https://github.com/
qizhex/RACE_AR_baselines

1

Introduction

Constructing an intelligence agent capable of un-
derstanding text as people is the major challenge
of NLP research. With recent advances in deep
learning techniques, it seems possible to achieve
human-level performance in certain language un-
derstanding tasks, and a surge of effort has been
devoted to the machine comprehension task where
people aim to construct a system with the ability to

∗* indicates equal contribution

answer questions related to a document that it has
to comprehend (Chen et al., 2016; Kadlec et al.,
2016; Dhingra et al., 2016; Yang et al., 2017).

Towards this goal, several large-scale datasets
(Rajpurkar et al., 2016; Onishi et al., 2016; Hill
et al., 2015; Trischler et al., 2016; Hermann
et al., 2015) have been proposed, which allow re-
searchers to train deep learning systems and ob-
tain results comparable to the human performance.
While having a suitable dataset is crucial for eval-
uating the system’s true ability in reading compre-
hension, the existing datasets suffer several critical
limitations. Firstly, in all datasets, the candidate
options are directly extracted from the context (as
a single entity or a text span), which leads to the
fact that lots of questions can be solved trivially
via word-based search and context-matching with-
out deeper reasoning; this constrains the types of
questions as well. Secondly, answers and ques-
tions of most datasets are either crowd-sourced
or automatically-generated, bringing a signiﬁcant
amount of noises in the datasets and limits the ceil-
ing performance by domain experts, such as 82%
for Childrens Book Test and 84% for Who-did-
What. Yet another issue in existing datasets is that
the topic coverages are often biased due to the spe-
ciﬁc ways that the data were initially collected,
making it hard to evaluate the ability of systems in
text comprehension over a broader range of topics.
To address the aforementioned limitations, we
constructed a new dataset by collecting a large
set of questions, answers and associated pas-
sages in the English exams for middle-school and
high-school Chinese students within the 12–18
age range. Those exams were designed by do-
main experts (instructors) for evaluating the read-
ing comprehension ability of students, with en-
Fur-
sured quality and broad topic coverage.
thermore,
the answers by machines or by hu-
mans can be objectively graded for evaluation

7
1
0
2
 
c
e
D
 
5
 
 
]
L
C
.
s
c
[
 
 
5
v
3
8
6
4
0
.
4
0
7
1
:
v
i
X
r
a

and comparison using the same evaluation met-
rics. Although efforts have been made with a sim-
ilar motivation, including the MCTest dataset cre-
ated by (Richardson et al., 2013) (containing 500
passages and 2000 questions) and several others
(Pe˜nas et al., 2014; Rodrigo et al., 2015; Khashabi
et al., 2016; Shibuki et al., 2014), the usefulness
of those datasets is signiﬁcantly restricted due to
their small sizes, especially not suitable for train-
ing powerful deep neural networks whose success
relies on the availability of relatively large training
sets.

Our new dataset, namely RACE, consists of
27,933 passages and 97,687 questions. After read-
ing each passage, each student is asked to answer
several questions where each question is provided
with four candidate answers – only one of them is
correct . Unlike existing datasets, both the ques-
tions and candidate answers in RACE are not re-
stricted to be the text spans in the original passage;
instead, they can be described in any words. A
sample from our dataset is presented in Table 1.

Our latter analysis shows that correctly answer-
ing a large portion of questions in RACE requires
the ability of reasoning, the most important fea-
ture as a machine comprehension dataset (Chen
et al., 2016). RACE also offers two important sub-
divisions of the reasoning types in its questions,
namely passage summarization and attitude anal-
ysis, which have not been introduced by the any of
the existing large-scale datasets to our knowledge.
In addition, compared to other existing datasets
where passages are either domain-speciﬁc or of a
single ﬁxed style (namely news stories for CNN/-
Daily Mail, NEWSQA and Who-did-What, ﬁction
stories for Children’s Book Test and Book Test,
and Wikipedia articles for SQUAD), passages in
RACE almost cover all types of human articles,
such as news, stories, ads, biography, philosophy,
etc., in a variety of styles. This comprehensiveness
of topic/style coverage makes RACE a desirable
resource for evaluating the reading comprehension
ability of machine learning systems in general.

The advantages of our proposed dataset over ex-
isting large datasets in machine reading compre-
hension can be summarized as follows:

text comprehension ability of machine learn-
ing systems under human judge.

• The questions are substantially more difﬁcult
than those in existing datasets, in terms of the
large portion of questions involving reason-
ing. At the meantime, it is also sufﬁciently
large to support the training of deep learning
models.

• Unlike existing large-scale datasets, candi-
date options in RACE are human generated
sentences which may not appear in the origi-
nal passage. This makes the task more chal-
lenging and allows a rich type of questions
such as passage summarization and attitude
analysis.

• Broad coverage in various domains and writ-
ing styles: a desirable property for evaluating
generic (in contrast to domain/style-speciﬁc)
comprehension ability of learning models.

2 Related Work

In this section, we brieﬂy outline existing datasets
for the machine reading comprehension task, in-
cluding their strengths and weaknesses.

2.1 MCTest

MCTest (Richardson et al., 2013) is a popular
dataset for question answering in the same for-
mat as RACE, where each question is associated
with four candidate answers with a single cor-
rect answer. Although questions in MCTest are
of high-quality ensured by careful examinations
through crowdsourcing, it contains only 500 stores
and 2000 questions, which substantially restricts
its usage in training advanced machine compre-
hension models. Moreover, while MCTest is de-
signed for 7 years old children, RACE is con-
structed for middle and high school students at
12–18 years old hence is more complicated and
requires stronger reasoning skills. In other words,
RACE can be viewed as a larger and more difﬁcult
version of the MCTest dataset.

2.2 Cloze-style datasets

• All questions and candidate options are gen-
erated by human experts, which are intention-
ally designed to test human agent’s ability in
reading comprehension. This makes RACE a
relatively accurate indicator for reﬂecting the

The past few years have witnessed several large-
scale cloze-style datasets (Hermann et al., 2015;
Hill et al., 2015; Bajgar et al., 2016; Onishi et al.,
2016), whose questions are formulated by obliter-
ating a word or an entity in a sentence.

Passage:
In a small village in England about 150 years ago, a mail coach was standing on the street. It didn’t come to that village often.
People had to pay a lot to get a letter. The person who sent the letter didn’t have to pay the postage, while the receiver had to.
“Here’s a letter for Miss Alice Brown,” said the mailman.
“ I’m Alice Brown,” a girl of about 18 said in a low voice.
Alice looked at the envelope for a minute, and then handed it back to the mailman.
“I’m sorry I can’t take it, I don’t have enough money to pay it”, she said.
A gentleman standing around were very sorry for her. Then he came up and paid the postage for her.
When the gentleman gave the letter to her, she said with a smile, “ Thank you very much, This letter is from Tom. I’m going
to marry him. He went to London to look for work. I’ve waited a long time for this letter, but now I don’t need it, there is
nothing in it.”
“Really? How do you know that?” the gentleman said in surprise.
“He told me that he would put some signs on the envelope. Look, sir, this cross in the corner means that he is well and this
circle means he has found work. That’s good news.”
The gentleman was Sir Rowland Hill. He didn’t forgot Alice and her letter.
“The postage to be paid by the receiver has to be changed,” he said to himself and had a good plan.
“The postage has to be much lower, what about a penny? And the person who sends the letter pays the postage. He has to buy
a stamp and put it on the envelope.” he said . The government accepted his plan. Then the ﬁrst stamp was put out in 1840. It
was called the “Penny Black”. It had a picture of the Queen on it.

Questions:

1): The ﬁrst postage stamp was made .
A. in England B. in America C. by Alice D. in 1910

2): The girl handed the letter back to the mailman because
.
A. she didn’t know whose letter it was
B. she had no money to pay the postage
C. she received the letter but she didn’t want to open it
D. she had already known what was written in the letter

3): We can know from Alice’s words that
A. Tom had told her what the signs meant before leaving
B. Alice was clever and could guess the meaning of the signs
C. Alice had put the signs on the envelope herself
D. Tom had put the signs as Alice had told him to

.

4): The idea of using stamps was thought of by .
A. the government
B. Sir Rowland Hill
C. Alice Brown
D. Tom

5): From the passage we know the high postage made .
A. people never send each other letters
B. lovers almost lose every touch with each other
C. people try their best to avoid paying it
D. receivers refuse to pay the coming letters

Answer: ADABC

Table 1: Sample reading comprehension problems from our dataset.

CNN/Daily Mail (Hermann et al., 2015) are
the largest machine comprehension datasets with
1.4M questions. However, both require limited
reasoning ability (Chen et al., 2016). In fact, the
best machine performance obtained by researchers
(Chen et al., 2016; Dhingra et al., 2016) is close to
human’s performance on CNN/Daily Mail.

Childrens Book Test (CBT) (Hill et al., 2015)
and Book Test (BT) (Bajgar et al., 2016) are con-
structed in a similar manner. Each passage in CBT
consist of 20 contiguous sentences extracted from
children’s books and the next (21st) sentence is
used to make the question. The main difference
between the two datasets is the size of BT being
60 times larger. Machine comprehension models
have also matched human performance on CBT
(Bajgar et al., 2016).

Who Did What (WDW) (Onishi et al., 2016)
is yet another cloze-style dataset constructed from
the LDC English Gigaword newswire corpus. The
authors generate passages and questions by pick-
ing two news articles describing the same event,

using one as the passage and the other as the ques-
tion.

High noise is inevitable in cloze-style datasets
due to their automatic generation process, which
is reﬂected in the human performance on these
datasets: 82% for CBT and 84% for WDW.

2.3 Datasets with Span-based Answers

In datasets such as SQUAD (Rajpurkar et al.,
2016), NEWSQA (Trischler et al., 2016) MS
MARCO (Nguyen et al., 2016) and recently pro-
posed TriviaQA (Joshi et al., 2017). the answer to
each question is in the form of a text span in the
article. Articles of SQUAD, NEWSQA and MS
MARCO come from Wikipedia, CNN news and
the Bing search engine respectively. The answer to
a certain question may not be unique and could be
multiple spans. Instead of evaluating the accuracy,
researchers need to use F1 score, BLEU (Papineni
et al., 2002) or ROUGE (Lin and Hovy, 2003)
as metrics, which measure the overlap between
the prediction and ground truth answers since the

questions come without candidate spans.

Datasets with span-based answers are challeng-
ing as the space of possible spans is usually large.
However, restricting answers to be text spans in
the context passage may be unrealistic and more
importantly, may not be intuitive even for humans,
indicated by the suffered human performance of
80.3% on SQUAD (or 65% claimed by Trischler
et al. (2016)) and 46.5% on NEWSQA. In other
words, the format of span-based answers may not
necessarily be a good examination of reading com-
prehension of machines whose aim is to approach
the comprehension ability of humans.

2.4 Datasets from Examinations

There have been several datasets extracted from
examinations, aiming at evaluating systems un-
der the same conditions as how humans are evalu-
ated in schools. E.g., the AI2 Elementary School
Science Questions dataset (Khashabi et al., 2016)
contains 1080 questions for students in elementary
schools; NTCIR QA Lab (Shibuki et al., 2014)
evaluates systems by the task of solving real-world
university entrance exam questions; The Entrance
Exams task at CLEF QA Track (Pe˜nas et al., 2014;
Rodrigo et al., 2015) evaluates the system’s read-
ing comprehension ability. However, data pro-
vided in these existing tasks are far from sufﬁcient
for the training of advanced data-driven machine
reading models, partially due to the expensive data
generation process by human experts.

To the best of our knowledge, RACE is the ﬁrst
large-scale dataset of this type, where questions
are created based on exams designed to evaluate
human performance in reading comprehension.

3 Data Analysis

In this section, we study the nature of questions
covered in RACE at a detailed level. Speciﬁcally,
we present the dataset statistics in Section 3.1, and
then analyze different reasoning/question types in
RACE in the remaining subsections.

3.1 Dataset Statistics

nations. We split 5% data as the development set
and 5% as the test set for RACE-M and RACE-H
respectively. The number of samples in each set is
shown in Table 2. The statistics for RACE-M and
RACE-H is summarized in Table 3. We can ﬁnd
that the length of the passages and the vocabulary
size in the RACE-H are much larger than that of
the RACE-M, an evidence of the higher difﬁculty
of high school examinations.

However, notice that since the articles and ques-
tions are selected and designed to test Chinese
students learning English as a foreign language,
the vocabulary size and the complexity of the lan-
guage constructs are simpler than news articles
and Wikipedia articles in other QA datasets.

3.2 Reasoning Types of the Questions

To get a comprehensive picture about the reason-
ing difﬁculty requirement of RACE, we conduct
human annotations of questions types. Following
Chen et al. (2016); Trischler et al. (2016), we strat-
ify the questions into ﬁve classes as follows with
ascending order of difﬁculty:

• Word matching:

The question exactly
matches a span in the article. The answer is
self-evident.

• Paraphrasing: The question is entailed or
paraphrased by exactly one sentence in the
passage. The answer can be extracted within
the sentence.

• Single-sentence reasoning: The answer could
be inferred from a single sentence of the arti-
cle by recognizing incomplete information or
conceptual overlap.

• Multi-sentence reasoning: The answer must
be inferred from synthesizing information
distributed across multiple sentences.

• Insufﬁcient/Ambiguous: The question has no
answer or the answer is not unique based on
the given passage.

As mentioned in section 1, RACE is collected
from English examinations designed for 12–15
year-old middle school students, and 15–18 year-
old high school students in China. To distin-
guish the two subgroups with drastic difﬁculty
gap, RACE-M denotes the middle school exami-
nations and RACE-H denotes high school exami-

We refer readers to (Chen et al., 2016; Trischler

et al., 2016) for examples of each category.

To obtain the proportion of different question
types, we sample 100 passages from RACE (50
from RACE-M and 50 from RACE-H), all of
which have 5 questions hence there are 500 ques-
tions in total. We put the passages on Amazon Me-

Dataset
Subset
# passages
# questions

RACE-M
Dev
368
1,436

Train
6,409
25,421

Test
362
1,436

Train
18,728
62,445

RACE-H
Dev
1,021
3,451

RACE

Test
1,045
3,498

Train
25,137
87,866

Dev
1,389
4,887

Test
1,407
4,934

All
27,933
97,687

Table 2: The separation of the training, development and test sets of RACE-M,RACE-H and RACE

Dataset
Passage Len
Question Len
Option Len
Vocab size

RACE-M RACE-H RACE
321.9
353.1
10.0
10.4
5.3
5.8
136,629
125,120

231.1
9.0
3.9
32,811

Table 3: Statistics of RACE where Len denotes
length and Vocab denotes Vocabulary.

chanical Turk1, and a Hit is generated by a passage
with 5 questions. Each question is labeled by two
crowdworkers. We require the turkers to both an-
swer the questions and label the reasoning type.
We pay $0.70 and $1.00 per passage in RACE-M
and RACE-H respectively, and restrict the access
to master turkers only. Finally, we get 1000 labels
for the 500 questions.

The statistics about the reasoning type is sum-
marized in Table 4. The higher difﬁculty level
of RACE is justiﬁed by its higher ratio of rea-
soning questions in comparison to CNN, SQUAD
and NEWSQA. Speciﬁcally, 59.2% questions of
RACE are either in the category of single-sentence
reasoning or in the category of multi-sentence
reasoning, while the ratio is 21%, 20.5% and
33.9% for CNN, SQUAD and NEWSQA respec-
tively. Also notice that the ratio of word match-
ing questions on RACE is only 15.8%, the lowest
among several categories. In addition, questions
in RACE-H are more complex than questions in
RACE-M since RACE-M has more word match-
ing questions and fewer reasoning questions.

3.3 Subdividing Reasoning Types

To better understand our dataset and facilitate fu-
ture research, we list the subdivisions of ques-
tions under the reasoning category. We ﬁnd the
most frequent reasoning subdivisions include: de-
tail reasoning, whole-picture understanding, pas-
sage summarization, attitude analysis and world
knowledge. One question may fall into multiple
divisions. Deﬁnition of these subdivisions and
their associated examples are as follows:

1https://www.mturk.com/mturk/welcome

1. Detail reasoning: to answer the question, the
agent should be clear about the details of the pas-
sage. The answer appears in the passage but it can-
not be found by simply matching the question with
the passage. For example, Question 1 in the sam-
ple passage falls into this category.

2. Whole-picture reasoning: the agent needs to
understand the whole picture of the story to ob-
tain the correct answer. For example, to answer
the Question 2 in the sample passage, the agent is
required to comprehend the entire story.

3. Passage summarization: The question re-
quires the agent to select the best summarization
of the passage among four candidate summariza-
tions. A typical question of this type is “The main
idea of this passage is
.”. An example question
can be found in Appendix A.1.

4. Attitude analysis: The question asks about
the opinions/attitudes of the author or a character
in the story towards somebody or something, e.g.,

• Evidence:
“. . . Many people optimistically
thought
industry awards for better equipment
the production of quieter
would stimulate
appliances. It was even suggested that noise from
building sites could be alleviated . . . ”

• Question: What was the author’s attitude towards

the industry awards for quieter?

• Options:

A.suspicious

B.positive

C.enthusiastic D.indifferent

5. World knowledge: Certain external knowl-
edge is needed. Most frequent questions under this
category involve simple arithmetic.

• Evidence: “The park is open from 8 am to 5 pm.”

• Question: The park is open for

hours a day.

• Options: A.eight B.nine C.ten D.eleven

To the best of our knowledge, questions like
passage summarization and attitude analysis have
not been introduced by any of the existing large-
scale machine comprehension datasets. Both are

Dataset
Word Matching
Paraphrasing
Single-Sentence Reasoning
Multi-Sentence Reasoning
Ambiguous/Insufﬁcient

RACE-M RACE-H RACE

29.4%
14.8%
31.3%
22.6%
1.8%

11.3%
20.6%
34.1%
26.9%
7.1%

CNN
15.8% 13.0%†
19.2% 41.0%†
33.4% 19.0%†
25.8% 2.0%†
5.8% 25.0%†

SQUAD NEWSQA
39.8%*
34.3%*
8.6%*
11.9%*
5.4%*

32.7%*
27.0%*
13.2%*
20.7%*
6.4%*

Table 4: Statistic information about Reasoning type in different datasets. * denotes the numbers coming
from (Trischler et al., 2016) based on 1000 samples per dataset, and numbers with † come from (Chen
et al., 2016).

crucial components in evaluating humans’ reading
comprehension abilities.

5 Experiments

4 Collection Methodology

We collected the raw data from three large free
public websites in China2, where the reading com-
prehension problems are extracted from English
examinations designed by teachers in China. The
data before cleaning contains 137,918 passages
and 519,878 questions in total, where there are
38,159 passages with 156,782 questions in the
middle school group, and 99,759 passages with
363,096 questions in the high school group.

The following ﬁltering steps are conducted to
clean the raw data. Firstly, we remove all prob-
lems and questions that do not have the same for-
mat as our problem setting, e.g., a question would
be removed if the number of its options is not four.
Secondly, we ﬁlter all articles and questions that
are not self-contained based on the text informa-
tion, i.e. we remove the articles and questions con-
taining images or tables. We also remove all ques-
tions containing keywords “underlined” or “para-
graph”, since it is difﬁcult to reproduce the effect
of underlines and the paragraph segment informa-
tion. Thirdly, we remove all duplicated articles.

On one of the websites (xkw.com), the answers
are stored as images. We used two standard OCR
programs tesseract 3 and ABBYY FineReader 4 to
process the images. We remove all the answers
that two software disagree. The OCR task is easy
since we only need to recognize printed alphabet
A, B, C, D with a standard font. Finally, we get
the cleaned dataset RACE, with 27,933 passages
and 97,687 questions.

In this section, we compare the performance
of several state-of-the-art reading comprehension
models with human performance. We use accu-
racy as the metric to evaluate different models.

5.1 Methods for Comparison

Sliding Window Algorithm Firstly, we build
the rule-based baseline introduced by Richardson
et al. (2013).
It chooses the answer having the
highest matching score. Speciﬁcally, it ﬁrst con-
catenates the question and the answer and then cal-
culates the TF-IDF style matching score between
the concatenated sentence with every window (a
span of text) of the article. The window size is
decided by the model performance in the training
and dev sets.

Stanford Attentive Reader Stanford Attentive
Reader (Stanford AR) (Chen et al., 2016) is a
strong model that achieves state-of-the-art results
on CNN/Daily Mail. Moreover, the authors claim
that their model has nearly reached the ceiling per-
formance on these two datasets.

Suppose that the triple of passage, question and
options is denoted by (p, q, o1,··· ,4). We ﬁrst em-
ploy bidirectional GRUs to encode p and q respec-
tively into hp
n and hq. Then we sum-
marize the most relevant part of the passage into
sp with an attention model. Following Chen et al.
(2016), we adopt a bilinear attention form. Specif-
ically,

2, . . . , hp

1, hp

αi = Softmaxi((hp
sp =

(cid:88)

αihp
i

i )T W1hq)

i

(1)

2We checked that our dataset does not include exam-
ple questions of exams with copyright, such as SSAT, SAT,
TOEFL and GRE.

3https://github.com/tesseract-ocr
4https://www.abbyy.com/FineReader

Similarly, we use bidirectional GRUs to encode
option oi into a vector hoi. Finally, we com-
pute the matching score between the i-th option
(i = 1, · · · , 4) and the summarized passage using

Random
Sliding Window
Stanford AR
GA
Turkers
Ceiling Performance

RACE-M RACE-H RACE MCTest CNN DM CBT-N CBT-C WDW
32.0†
10.2
19.6† 48.0†
64.0†
67.3† 71.2†

–

24.8
51.5†
–
–
–
–

10.6
0.06 0.06
24.8 30.8 16.8†
73.6† 76.6†
77.9† 80.9† 70.1†
–
–

–
81.6†

–
–

–

25.0
30.4
43.0
44.2
69.4
94.2

24.6
37.3
44.2
43.7
85.1
95.4

24.9
32.2
43.3
44.1
73.3
94.5

–
81.6†

–
84†

Table 5: Accuracy of models and human on the each dataset, where † denotes the results coming from
previous publications. DM denotes Daily Mail and WDW denotes Who-Did-What .

(a) RACE-M

(b) RACE-H

Figure 1: Test accuracy of different baselines on each question type category introduced in Section 3.2,
where Word-Match, Single-Reason, Multi-Reason and Ambiguous are the abbreviations for Word match-
ing, Single-sentence Reasoning, Multi-sentence Reasoning and Insufﬁcient/Ambiguous respectively.

a bilinear attention. We pass the scores through
softmax to get a probability distribution. Specif-
ically, the probability of option i being the right
answer is calculated as

pi = Softmaxi(hoiW2sd)

(2)

Gated-Attention Reader Gated AR (Dhingra
et al., 2016) is the state-of-the-art model on mul-
tiple datasets. To build query-speciﬁc represen-
tations of tokens in the document, it employs an
attention mechanism to model multiplicative in-
teractions between the query embedding and the
document representation. With a multi-hop ar-
chitecture, GA also enables a model to scan the
document and the question iteratively for multi-
ple passes.
In other words, the multi-hop struc-
ture makes it possible for the reader to reﬁne token
representations iteratively and the attention mech-
anism ﬁnd the most relevant part of the document.
We refer readers to (Dhingra et al., 2016) for more
details.

After obtaining a query speciﬁc document rep-
resentation sd, we use the same method as bilinear
operation listed in Equation 2 to get the output.

Note that our implementation slightly differs
from the original GA reader. Speciﬁcally, the At-
tention Sum layer is not applied at the ﬁnal layer
and no character-level embeddings are used.

Implementation Details We follow Chen et al.
(2016) in our experiment settings. The vocabulary
size is set to 50k. We choose word embedding
size d = 100 and use the 100-dimensional Glove
word embedding (Pennington et al., 2014) as em-
bedding initialization. GRU weights are initial-
ized from Gaussian distribution N (0, 0.1). Other
parameters are initialized from a uniform distri-
bution on (−0.01, 0.01). The hidden dimension-
ality is set to 128 and the number of layers is
set to one for both Stanford AR and GA. We use
vanilla stochastic gradient descent (SGD) to train
our models. We apply dropout on word embed-
dings and the gradient is clipped when the norm

of the gradient is larger than 10. We use a grid
search on validation set to choose the learning
rate within {0.05, 0.1, 0.3, 0.5} and dropout rate
within {0.2, 0.5, 0.7}. The highest accuracy on
validation set is obtained by setting learning rate to
0.1 for Stanford AR and 0.3 for GA and dropout
rate to 0.5. The data of RACE-M and RACE-H
is used together to train our model and testing is
performed separately.

5.2 Human Evaluation

As described in section 3.2, a randomly sam-
pled subset of test set has been labeled by Ama-
zon Turkers, which contains 500 questions with
half from RACE-H and with the other half from
RACE-M. The turkers’ performance is 85% for
RACE-M and 70% for RACE-H. However, it is
hard to guarantee that every turker performs the
survey carefully, given the difﬁcult and long pas-
sages of high school problems. Therefore, to ob-
tain the ceiling human performance on RACE,
we manually labeled the proportion of valid ques-
tions. A question is valid if it is unambiguous and
has a correct answer. We found that 94.5% of the
data is valid, which sets the ceiling human per-
formance. Similarly, the ceiling performance on
RACE-M and RACE-H is 95.4% and 94.2% re-
spectively.

5.3 Main Results

We compare models’ and human ceiling perfor-
mance on datasets which have the same evalua-
tion metric with RACE. The compared datasets
include RACE, MCTest, CNN/Daily Mail (CNN
and DM), CBT and WDW. On CBT, we report per-
formance on two subsets where the missing token
is either a common noun (CBT-C) or name entity
(CBT-N) since the language models have already
reached human-level performance on other types
(Hill et al., 2015). The comparison is shown in
Table 5.

Performance of Sliding Window We ﬁrst com-
pare MCTest with RACE using Sliding Window,
where it is unable to train Stanford AR and Gated
Slid-
AR on MCTest’s limited training data.
ing Window achieves an accuracy of 51.5% on
MCTest while only 37.3% on RACE, meaning that
to answer the questions of RACE requires more
reasoning than MCTest.

The performance of sliding window on RACE
is not directly comparable with CBT and WDW

since CBT has ten candidate answers for each
question and WDW has an average of three. In-
stead, we evaluate the performance improvement
of sliding window on the random baseline. Larger
improvement indicates more questions solvable by
simple matching. On RACE, Sliding Window is
28.6% better than the random baseline, while the
improvement is 58.5%, 92.2% and 50% for CBT-
N, CBT-C and WDW.

The accuracy on RACE-M (37.3%) and RACE-
H (30.4%) indicates that the middle school ques-
tions are simpler based on the matching algorithm.

Performance of Neural Models We further
compare the difﬁculty of different datasets by
state-of-the-art neural models’ performance. A
lower performance means that more problems are
unsolvable by machines. The Stanford AR and
Gated AR achieve an accuracy of only 43.3% and
44.1% on RACE while their accuracy is much
higher on CNN/Daily Mail, Childrens Book
Test and Who-Did-What. It justiﬁes the fact that,
among current large-scale machine comprehen-
sion datasets, RACE is the most challenging one.

Human Ceiling Performance The human per-
formance is 94.5% which shows our data is quite
clean compared to other large-scale machine com-
prehension datasets. Since we cannot enforce ev-
ery turker do the test cautiously, the result shows
a gap between turkers’ performance and human
performance. Reasonably, problems in the high
school group with longer passages and more com-
plex questions lead to more signiﬁcant divergence.
Nevertheless, the start-of-the-art models still have
a large room to be improved to reach turkers’ per-
formance. The performance gap is 41% for the
middle school problems and 25% for the high
school problems. What’s more, The performance
of Stanford AR and GA is only less than a half
of the ceiling human performance, which indicates
that to match the humans’ reading comprehension
ability, we still have a long way to go.

5.4 Reason Types Analysis

We evaluate human and models on different types
of questions, shown in Figure 1. Turkers do the
best on word matching problems while doing the
worst on reasoning problems. Sliding window
performs better on word matching than problems
needing reasoning or paraphrasing. Surprisingly,
Stanford AR does not have a stronger performance

on the word matching category than reasoning cat-
egories. A possible reason is that the proportion
of data in reasoning categories is larger than that
of data. Also, the candidate answers of simple
matching questions may share similar word em-
beddings. For example, if the question is about
color, it is difﬁcult to distinguish candidate an-
swers, “green”, “red”, “blue” and “yellow”, in the
embedding vector space. The similar performance
on different categories also explains the reason
that the performance of the neural models is close
in the middle and high school groups in Table 5.

6 Conclusion

We introduce a large, high-quality dataset for read-
ing comprehension that is carefully designed to
examine human ability on this task. Some desir-
able properties of RACE include the broad cover-
age of domains/styles and the richness in the ques-
tion format. Most importantly, it requires substan-
tially more reasoning to do well on RACE than
on other datasets, as there is a signiﬁcant gap be-
tween the performance of state-of-the-art machine
comprehension models and that of the human. We
hope this dataset will stimulate the development of
more advanced machine comprehension models.

Acknowledgement

We would like to thank Graham Neubig for sug-
gestions on the draft and Diyi Yang’s help on ob-
taining the crowdsourced labels.

This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program.

References

Ondrej Bajgar, Rudolf Kadlec, and Jan Kleindi-
enst. 2016. Embracing data abundance: Booktest
dataset for reading comprehension. arXiv preprint
arXiv:1610.00956 .

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A thorough examination of the cn-
arXiv
n/daily mail reading comprehension task.
preprint arXiv:1606.02858 .

Bhuwan Dhingra, Hanxiao Liu, William W Cohen,
and Ruslan Salakhutdinov. 2016. Gated-attention
arXiv preprint
readers for text comprehension.
arXiv:1606.01549 .

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-

chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301 .

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. ACL .

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv:1603.01547 .

Daniel Khashabi, Tushar Khot, Ashish Sabhar-
wal, Peter Clark, Oren Etzioni, and Dan Roth.
2016. Question answering via integer programming
arXiv preprint
over semi-structured knowledge.
arXiv:1604.06076 .

Chin-Yew Lin and Eduard Hovy. 2003.

Auto-
matic evaluation of summaries using n-gram co-
In Proceedings of the 2003
occurrence statistics.
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1. Association
for Computational Linguistics, pages 71–78.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
arXiv preprint
reading comprehension dataset.
arXiv:1611.09268 .

Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim-
pel, and David McAllester. 2016. Who did what:
A large-scale person-centered cloze dataset. arXiv
preprint arXiv:1608.05457 .

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Anselmo Pe˜nas, Yusuke Miyao, ´Alvaro Rodrigo, Ed-
uard H Hovy, and Noriko Kando. 2014. Overview of
clef qa entrance exams task 2014. In CLEF (Work-
ing Notes). pages 1194–1200.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
EMNLP. volume 3, page 4.

´Alvaro Rodrigo, Anselmo Pe˜nas, Yusuke Miyao, Ed-
uard H Hovy, and Noriko Kando. 2015. Overview of
clef qa entrance exams task 2015. In CLEF (Work-
ing Notes).

Hideyuki Shibuki, Kotaro Sakamoto, Yoshinobu Kano,
Teruko Mitamura, Madoka Ishioroshi, Kelly Y
Itakura, Di Wang, Tatsunori Mori, and Noriko
Kando. 2014. Overview of the ntcir-11 qa-lab task.
In NTCIR.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830 .

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W Cohen. 2017. Semi-supervised qa with
arXiv preprint
generative domain-adaptive nets.
arXiv:1702.02206 .

A Appendix

A.1 Example Question of Passage

Summarization

Passage: Do you love holidays but hate gaining
weight? You are not alone. Holidays are times for
celebrating. Many people are worried about their
weight. With proper planning, though, it is pos-
sible to keep normal weight during the holidays.
The idea is to enjoy the holidays but not to eat too
much. You don’t have to turn away from the foods
that you enjoy.

Here are some tips for preventing weight gain

and maintaining physical ﬁtness:

Don’t skip meals. Before you leave home, have
a small, low-fat meal or snack. This may help to
avoid getting too excited before delicious foods.

Control the amount of food. Use a small plate
that may encourage you to ”load up”. You should
be most comfortable eating an amount of food
about the size of your ﬁst.

Begin with soup and fruit or vegetables. Fill up
beforehand on water-based soup and raw fruit or
vegetables, or drink a large glass of water before
you eat to help you to feel full.

Avoid high-fat foods. Dishes that look oily or
creamy may have large amount of fat. Choose lean
meat . Fill your plate with salad and green vegeta-
bles. Use lemon juice instead of creamy food.

Stick to physical activity. Don’t let exercise take
a break during the holidays. A 20-minute walk
helps to burn off extra calories.

Questions:
What is the best title of the passage?
Options:
A. How to avoid holiday feasting
B. Do’s and don’ts for keeping slim and ﬁt.
C. How to avoid weight gain over holidays.
D. Wonderful holidays, boring experiences.

