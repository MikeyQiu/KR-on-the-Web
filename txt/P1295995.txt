TweetCaT: a tool for building Twitter corpora of smaller languages

Nikola Ljubeˇsi´c,∗ Darja Fiˇser,† Tomaˇz Erjavec‡

* Dept. of Information and Communication Sciences, University of Zagreb
Ivana Luˇci´ca 3, HR-10000 Zagreb, Croatia
nikola.ljubesic@ffzg.hr

† Dept. of Translation, University of Ljubljana
Aˇskerˇceva 2, SI-1000 Ljubljana, Slovenia
darja.ﬁser@ff.uni-lj.si

‡ Dept. of Knowledge Technologies, Joˇzef Stefan Institute
Jamova cesta 39, SI-1000 Ljubljana, Slovenia
tomaz.erjavec@ijs.si

Abstract
This paper presents TweetCaT, an open-source Python tool for building Twitter corpora that was designed for smaller languages. Using
the Twitter search API and a set of seed terms, the tool identiﬁes users tweeting in the language of interest together with their friends
and followers. By running the tool for 235 days we tested it on the task of collecting two monitor corpora, one for Croatian and Serbian
and the other for Slovene, thus also creating new and valuable resources for these languages. A post-processing step on the collected
corpus is also described, which ﬁlters out users that tweet predominantly in a foreign language thus further cleans the collected corpora.
Finally, an experiment on discriminating between Croatian and Serbian Twitter users is reported.

Keywords: Twitter corpora, open source, less-resourced languages, Croatian, Serbian, Slovene

1.

Introduction

detail by Kumar et al. (2013).

Twitter is a microblogging platform that was created in
2006 and offers the users the ability to interact with other
members in the community in real time over the Internet or
on their mobile phones. The users send short, 140-character
messages, called “tweets” to their “followers” (other users
who subscribe to those messages). Today, there are more
than 550 million Twitter users with over 100,000 new users
joining every day. On average, almost 60 million tweets are
published every day (Ponzetto and Zielinski, 2013). Tweets
are becoming an important data source for natural language
processing and corpus linguistics and with its growing pop-
ularity Twitter an increasing number of tweets are written in
languages other than English (Jacobs, 2011), which is why
it is becoming increasingly important to be able to process
tweets in other languages as well.
One of the ﬁrst and best known Twitter corpora is the Ed-
inburgh Twitter Corpus (Petrovi´c et al., 2010) that contains
almost 100 million tweets which were collected over a pe-
riod of two months using the Twitter streaming API. An-
other well-known Twitter collection is the Stanford Twit-
ter Corpus (Yang and Leskovec, 2011), which contains 467
million posts from 20 million users covering a 7 month pe-
riod in 2009. However, due to Twitter’s terms of service,1
they are no longer available as a dataset.
McCreadie et al. (2012) have overcome this problem by
developing Tweets2011, a methodology to distribute a set
of tweet identiﬁers and a separate tweet crawling tool for
downloading the identiﬁed tweets. Using the Twitter API
to collect tweets has become the standard way of compiling
Twitter corpora and has been described and discussed in

However, these approaches assume English as the language
of interest and cannot be directly used for other languages,
especially smaller ones, such as Croatian (4 million speak-
ers), Serbian (7 million) and Slovene (2 million), for which
standard techniques would return results of which only a
small fraction would be useful as a source of data for the
language in questions.

We present an alternative method that uses seed terms and
a simple language identiﬁcation module to ﬁnd new users
as well as new tweets from already known users that tweet
in the target language. The tool is named TweetCat, as the
basic idea follows the well known BootCat2 (Baroni and
Bernardini, 2004) tool, which collects URLs of Web pages
from seed terms in order to build Web-based corpora of par-
ticular languages and domains. We build two Twitter cor-
pora, one for Croatian and Serbian, and one for Slovene.
In a ﬁnal experiment we attempt to discriminate between
Croatian and Serbian Twitter users with partial success, but
obtain interesting insights in the problem and Twitter pop-
ularity among the speakers of the two languages. The col-
lection tool as well as the compiled corpora are available
under permissive licenses. The tool can easily be adapted
for other languages.

The rest of this paper is structured as follows: Section
2 presents the TweetCat tool, Section 3 gives an analysis
of the collection procedure, Section 4 details several post-
processing steps for corpus clean-up and Section 5 gives
conclusions and directions for future work.

1https://twitter.com/tos

2http://bootcat.sslmit.unibo.it/

2279

2. Description of TweetCaT
Since only a small fraction of the streamed tweets are in the
desired language, we use the Twitter searching API to iden-
tify the tweets containing user-speciﬁed seed terms speciﬁc
for the language. The seed terms are manually selected to
be fairly high-frequency content words that are, however,
not in the vocabulary of other languages. The number of
such terms can be quite small: we have deﬁned 40 seed
terms for Croatian and Serbian and 20 for Slovene.
The tool is written in Python and iterates a user-speciﬁed
number of times over the two basic steps:

1. querying the search API with the goal of identifying

new users and

2. retrieving new tweets of already known users

In the ﬁrst step we query the search API with every seed
term and check if the retrieved users are already in our user
index. If they are not, we retrieve their timeline of recent
tweets and perform basic language identiﬁcation over this
collection. We determine the language of the tweets by
comparing their vocabulary against a user-deﬁned list of
very frequent words. Again, this list can be quite small,
for Croatian and Serbian we used 87 words and 40 words
for Slovene. We temporarily discard users with timelines of
less than 100 tweets because this turns out to be a minimum
for a reliable language estimate.
Once a user tweeting in the language is identiﬁed, (s)he
is added to the user index and checked for followers and
friends (users (s)he follows) assuming that they will tweet
in the same language. Each of the followers and friends is
subjected to the same language identiﬁcation procedure as
the initial user.
The second step consists of retrieving new tweets from
the timelines of all known users. No additional language
identiﬁcation is performed once a user has passed the ini-
tial check since this would signiﬁcantly complicate our
straight-forward procedure which main goal is to produce
high-recall data collections that can be ﬁltered later on,
when the maximum amount of information on users is
available.
The whole procedure presented in pseudocode is as fol-
lows:

function lang_id(author):

timeline=tokenize(timeline_api(author))
return coverage(timeline,function_words)>=threshold

for seed_term in seed_terms:

for hit in search_api(seed_term):

if hit.author not in user_index:

if lang_id(hit.author):

user_index.add(hit.author)
for follower in follower_api(hit.author):

if follower not in user_index:

if lang_id(follower):

user_index.add(follower)
for friend in friend_api(hit.author):

if friend not in user_index:

if lang_id(friend):

user_index.add(friend)

for user in user_index:

for tweet in new_tweets_api(user):

output(tweet)

The tool outputs the tweets in a simple XML format, where
each tweet is given its metadata along with the name of the

author and the text of the tweet, as shown below:

<tweet id="429333550584721413"

created_at="2014-01-31T19:21:09"
retrieved_at="2014-02-01T17:29:55.98763"
favorite_count="19" retweet_count="1">

<screen_name>dfiser3</screen_name>
<text>Kurc pa petkov veˇcer, v katerem je

beseda izpiti samostalnik,
ne glagol.</text>

</tweet>

The information about the number of users that favorited
or retweeted the tweet is likely to change during time. The
fact that this collection will be distributed as a script for
downloading the collection via the API has the beneﬁt of
keeping these data as up-to-date as possible.

3. Analysis of the collection procedure
While the tool can be run for a speciﬁed period of time or
to gather a speciﬁed amount of text, it is our goal to run
the tool continuously in order to compile monitor corpora
for all three languages, especially because historical Twitter
data is not available through the API. To gain insight into
the dynamics of collecting the tweetosphere for smaller lan-
guages we present a timeline of identifying new tweets and
users in Figure 1 for the ﬁrst 235 days of running the tool.
The ﬁgure shows that both curves drop rapidly in the ﬁrst
ten days and stabilize from that point on. For Croatian and
Serbian an average 320k tweets are collected daily in the
ﬁrst ten days, which drops by half in the next 10 days. Af-
ter that period the number of new tweets stays constant on
around 120k tweets daily. The number of new users be-
haves in a similar fashion although with much smaller num-
bers, identifying on average 2.3k users daily in the ﬁrst ten
days, going down to constant 110 users a day for the re-
mainder of the timeframe.
For Slovene we observe a similar phenomenon with a day
average of 59k tweets in the ﬁrst ten days and constant 20k
new tweets in the remainder of the timeframe. In the ﬁrst
ten days we identify just below 400 users daily which drops
off to average 13 new users a day.
As can be seen, this approach yields a reasonably sized
Twitter collection in a matter of days by collecting 2.6 mil-
lion of Croatian and Serbian tweets and half a million of
Slovene tweets in the ﬁrst week. Running the tool for a
longer period produces a smaller, but constant and still sig-
niﬁcant stream of new data.

4. Post-processing
The amount of data collected in the ﬁrst 235 days
(hrsrTweets and slTweets) is given in the ﬁrst half of Ta-
ble 1. We decided to post-process this data collection with
the idea of producing Twitter corpora suitable for a wide
range of applications.
We focused on further tuning language identiﬁcation by
removing users who tweet more in languages similar to
the languages of interest and users who tweet signiﬁcantly
more in any other language than in the languages of inter-
est. It is important to notice that we were not able to make

2280

Figure 1: The number of new tweets and users obtained during time on both corpora

a decision of this quality during the data collection process
because of a smaller amount of information available. It
could be possible to extend the TweetCaT tool with an ad-
ditional ﬁlter that would be applied after enough data was
collected and we consider this to be one of the most likely
improvements for the next version of the tool.
We ﬁrst used the Python module langid.py (Lui and Bald-
win, 2012) with off-the-shelf models to identify the lan-
guage of each tweet with mentions, retweets, URLs and
hashtags removed.
In both corpora langid.py identiﬁed
tweets written in 97 different languages (all the languages
langid.py is trained on) showing that per-tweet language
identiﬁcation is a very difﬁcult task. In each corpus around
50% of the tweets are identiﬁed as being in the sought
language(s), followed by English with around 15-20% of
tweets, similar languages having around 10%, with each of
all other languages being identiﬁed in less than 1% of the
tweets.
We encoded the results of this procedure on the level of
each tweet yielding the following ﬁnal format of each
tweet:

<tweet id="429333550584721413"

created_at="2014-01-31T19:21:09"
retrieved_at="2014-02-01T17:29:55.98763"
favorite_count="19" retweet_count="1"
lang="sl" prob="0.999999999423"
norm_length="72">

<screen_name>dfiser3</screen_name>
<text>Kurc pa petkov veˇcer, v katerem je

beseda izpiti samostalnik,
ne glagol.</text>

</tweet>

removing thereby errors in our simple language identiﬁca-
tion procedure. With the second criterion we remove users
that tweet twice as often in any other language than the lan-
guage(s) of interest removing thereby users that tweet prob-
ably mostly in English.
The ﬁrst criterion ﬁlters out 16% of users in the Croatian
and Serbian collection and 20% of users in the Slovene col-
lection. The second criterion ﬁlters out a further 20% of
users in the Croatian and Serbian collection and 25% of
users in the Slovene one. The intersection between the sets
of users that were ﬁltered out with each criterion is quite
high, 85%, while12% of users satisfy the second criterion
only and 2% of users only the ﬁrst criterion. This points to
the direction that using the second criterion, i.e. removing
users who tweet in any other language twice as often, could
be sufﬁcient for this task.
After performing post-processing, the size of the current
235 day corpora (called hrsrTwitterCorpus and slTwitter-
Corpus) by the number of users, number of tweets and num-
ber of words is given in the second part of Table 1.
Finally, we applied a recently developed language identi-
ﬁer (Ljubeˇsi´c, 2014) that showed to be very efﬁcient in
discriminating between very closely related languages, in
particular Croatian and Serbian web data. It is based on un-
igram language models trained on 1.9 billion tokens from
the hrWaC Croatian top-level-domain web corpus3 and 900
million tokens of from the srWaC Serbian top-level-domain
web corpus4 with a simple maximum-a-posteriori (MAP)
decision rule. On the task of discriminating between those
languages on web text, the approach has shown to cut the
error of the very efﬁcient Blacklist classiﬁer (Tiedemann
and Ljubeˇsi´c, 2012) four times. It is important to note that

After annotating each tweet with the language attribute, we
applied a user-level ﬁlter based on two criteria. With the
ﬁrst criterion we remove users that tweet more in a deﬁned
set of similar languages than in the language(s) of interest

hrwac/

srwac/

3http://nlp.ffzg.hr/resources/corpora/

4http://nlp.ffzg.hr/resources/corpora/

2281

hrsrTweets
slTweets
hrsrTwitterCorpus
hrTwitterCorpus
srTwitterCorpus
slTwitterCorpus

users
51,381
7,284
41,807
4,465
26,869
5,483

tweets
26,047,874
4,504,745
21,360,940
2,070,381
14,072,777
3,035,304

words
290,557,841
55,492,816
235,952,967
23,410,410
157,531,327
38,465,311

Table 1: The number of users, tweets and words in the ini-
tial data collections and the ﬁnal corpora

the Blacklist classiﬁer was already performing at the 100%
accuracy level on newspaper texts, but has shown to strug-
gle with everything the Web has to offer.
The language identiﬁer was run on the collection of each
user’s tweets with URL-s, hashtags and mentions removed.
Beside the MAP decision we also calculated a normalized
log-probability for each language so that the sum of those
normalized probabilities sums to -1.
Manual inspection of the results revealed that Serbian users
are correctly classiﬁed, but that users identiﬁed as bor-
derline Croatian in many cases are actually Montenegrin,
who use the ijekavian yat reﬂex, but partially use Serbian-
speciﬁc lexis and Serbian-speciﬁc syntactic patterns.
By plotting the distribution of the normalized log proba-
bility for each user regarding the Croatian model, which is
depicted in Figure 2, we realized that the Montenegrin (and
possibly Bosnian?) tweet distribution is actually visible on
the right side of the left, bigger, Serbian user distribution.
This was also our ﬁrst realization that the Twitter service is
obviously very popular in a country of only 600 thousand
inhabitants. Additionally, we were surprised by the dras-
tically larger number of users tweeting in Serbian than in
Croatian.
Having in mind our current limitations regarding the capa-
bility to discriminate between Croatian and Serbian Twitter
users, we deﬁned for now two subcorpora, hrTwitterCorpus
and srTwitterCorpus, where the srTwitterCorpus contains
users that were classiﬁed by MAP as Serbian (dotted line
in Figure 2) while for the hrTwitterCorpus the normalized
log-probability criterion was set to -0.49 (full line in Fig-
ure 2). The ﬁnal sizes of those corpora are given in Table
1. The normalized log-probabilities for each user are dis-
tributed as part of the corpora enabling each user to redeﬁne
the language criterion.
It is important to stress that the produced corpora do not
contain tweets in the speciﬁc language only, but all tweets
of users that mostly tweet in that language. This design cri-
terion was motivated by the fact that cleaning up a Twitter
collection on the level of tweets runs the danger of remov-
ing also mixed-language tweets, which are very frequent
esp. with English, and whether such tweets should actually
be removed heavily depends on the intended usage of the
collection.

5. Conclusion

We have presented TweetCaT, a freely available tool for
collecting Twitter corpora of smaller languages and the

twitter/

2282

Figure 2: Distribution of the normalized log probability
for Croatian regarding the discrimination between Croatian
and Serbian on the user level

Twitter corpora of Croatian, Serbian and Slovene we have
built with the tool. The procedure aims to collect the largest
corpus with the least strain on the available Twitter APIs.
The tool is published on github5.
In our further work we plan to make the corpora available
via a concordancer. We are in the process of making the
corpora available for download following the Twitter Terms
of Service, as a set of tweet identiﬁers, their annotations
and a tweet crawling tool for downloading and recreating
the corpus6. We are also working on way to normalize the
language of Croatian and Slovene tweets, which is often
colloquial and informal, in order to be able to process it
with tools tranined on standard language; we have currently
taken the ﬁrst steps in this direction, by using character-
based statistical machine translation on a test collection of
Slovene tweets (Ljubeˇsi´c et al., 2014). Finaly, it would be
interesting to further investigate language identiﬁcation for
very closely related languages, in order to be able to reli-
ably distinguish not only Croatian and Serbian tweets, but
also Montenegrin and Bosnian ones.

6. Acknowledgement
The research leading to these results has received fund-
ing from the European Union Seventh Framework Pro-
gramme FP7/2007-2013 under grant agreement no. PIAP-
GA-2012-324414 (project Abu-MaTran) and the Research
programme of the Slovenian Research Agency P2–0103
”Knowledge Technologies”.

7. References
Marco Baroni and Silvia Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In In Pro-
ceedings of LREC 2004, pages 1313–1316.

5https://github.com/nljubesi/tweetcat
6http://nlp.ffzg.hr/resources/corpora/

Frank Jacobs. 2011. 539 - Vive le tweet! a map of Twitter’s

languages.

Shamanth Kumar, Fred Morstatter, and Huan Liu. 2013.
Twitter Data Analytics. Springer, New York, NY, USA.
Nikola Ljubeˇsi´c. 2014. {bs,hr,sr}WaC: Web corpora of
Bosnian, Croatian and Serbian. In Proceedings of the
WAC-9 Workshop.

Nikola Ljubeˇsi´c, Tomaˇz Erjavec, and Darja Fiˇsser. 2014.
Standardizing tweets with character-level machine trans-
lation. In Proceedings of the 15th International Confer-
ence, CICLing 2014, Lecture Notes in Computer Sci-
ence. Springer.

Marco Lui and Timothy Baldwin. 2012. langid.py: An off-
the-shelf language identiﬁcation tool. In ACL (System
Demonstrations), pages 25–30.

Richard McCreadie, Ian Soboroff, Jimmy Lin, Craig Mac-
donald, Iadh Ounis, and Dean McCullough. 2012. On
building a reusable twitter corpus. In Proceedings of the
35th international ACM SIGIR conference on Research
and development in information retrieval, SIGIR ’12,
pages 1113–1114, New York, NY, USA. ACM.

Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010.

In Proceedings of the
The edinburgh twitter corpus.
NAACL HLT 2010 Workshop on Computational Linguis-
tics in a World of Social Media, WSA ’10, pages 25–26,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Simone Paolo Ponzetto and Andrea Zielinski. 2013. Ex-
ploiting social media for natural language processing:
Bridging the gap between language-centric and real-
world applications. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Tutorials), pages 5–6, Soﬁa, Bulgaria, August. As-
sociation for Computational Linguistics.

J¨org Tiedemann and Nikola Ljubeˇsi´c. 2012. Efﬁcient dis-
crimination between closely related languages. In Pro-
ceedings of COLING 2012, pages 2619–2634, Mumbai,
India.

Jaewon Yang and Jure Leskovec. 2011. Patterns of tem-
poral variation in online media. In Proceedings of the
fourth ACM international conference on Web search and
data mining, WSDM ’11, pages 177–186, New York,
NY, USA. ACM.

2283

TweetCaT: a tool for building Twitter corpora of smaller languages

Nikola Ljubeˇsi´c,∗ Darja Fiˇser,† Tomaˇz Erjavec‡

* Dept. of Information and Communication Sciences, University of Zagreb
Ivana Luˇci´ca 3, HR-10000 Zagreb, Croatia
nikola.ljubesic@ffzg.hr

† Dept. of Translation, University of Ljubljana
Aˇskerˇceva 2, SI-1000 Ljubljana, Slovenia
darja.ﬁser@ff.uni-lj.si

‡ Dept. of Knowledge Technologies, Joˇzef Stefan Institute
Jamova cesta 39, SI-1000 Ljubljana, Slovenia
tomaz.erjavec@ijs.si

Abstract
This paper presents TweetCaT, an open-source Python tool for building Twitter corpora that was designed for smaller languages. Using
the Twitter search API and a set of seed terms, the tool identiﬁes users tweeting in the language of interest together with their friends
and followers. By running the tool for 235 days we tested it on the task of collecting two monitor corpora, one for Croatian and Serbian
and the other for Slovene, thus also creating new and valuable resources for these languages. A post-processing step on the collected
corpus is also described, which ﬁlters out users that tweet predominantly in a foreign language thus further cleans the collected corpora.
Finally, an experiment on discriminating between Croatian and Serbian Twitter users is reported.

Keywords: Twitter corpora, open source, less-resourced languages, Croatian, Serbian, Slovene

1.

Introduction

detail by Kumar et al. (2013).

Twitter is a microblogging platform that was created in
2006 and offers the users the ability to interact with other
members in the community in real time over the Internet or
on their mobile phones. The users send short, 140-character
messages, called “tweets” to their “followers” (other users
who subscribe to those messages). Today, there are more
than 550 million Twitter users with over 100,000 new users
joining every day. On average, almost 60 million tweets are
published every day (Ponzetto and Zielinski, 2013). Tweets
are becoming an important data source for natural language
processing and corpus linguistics and with its growing pop-
ularity Twitter an increasing number of tweets are written in
languages other than English (Jacobs, 2011), which is why
it is becoming increasingly important to be able to process
tweets in other languages as well.
One of the ﬁrst and best known Twitter corpora is the Ed-
inburgh Twitter Corpus (Petrovi´c et al., 2010) that contains
almost 100 million tweets which were collected over a pe-
riod of two months using the Twitter streaming API. An-
other well-known Twitter collection is the Stanford Twit-
ter Corpus (Yang and Leskovec, 2011), which contains 467
million posts from 20 million users covering a 7 month pe-
riod in 2009. However, due to Twitter’s terms of service,1
they are no longer available as a dataset.
McCreadie et al. (2012) have overcome this problem by
developing Tweets2011, a methodology to distribute a set
of tweet identiﬁers and a separate tweet crawling tool for
downloading the identiﬁed tweets. Using the Twitter API
to collect tweets has become the standard way of compiling
Twitter corpora and has been described and discussed in

However, these approaches assume English as the language
of interest and cannot be directly used for other languages,
especially smaller ones, such as Croatian (4 million speak-
ers), Serbian (7 million) and Slovene (2 million), for which
standard techniques would return results of which only a
small fraction would be useful as a source of data for the
language in questions.

We present an alternative method that uses seed terms and
a simple language identiﬁcation module to ﬁnd new users
as well as new tweets from already known users that tweet
in the target language. The tool is named TweetCat, as the
basic idea follows the well known BootCat2 (Baroni and
Bernardini, 2004) tool, which collects URLs of Web pages
from seed terms in order to build Web-based corpora of par-
ticular languages and domains. We build two Twitter cor-
pora, one for Croatian and Serbian, and one for Slovene.
In a ﬁnal experiment we attempt to discriminate between
Croatian and Serbian Twitter users with partial success, but
obtain interesting insights in the problem and Twitter pop-
ularity among the speakers of the two languages. The col-
lection tool as well as the compiled corpora are available
under permissive licenses. The tool can easily be adapted
for other languages.

The rest of this paper is structured as follows: Section
2 presents the TweetCat tool, Section 3 gives an analysis
of the collection procedure, Section 4 details several post-
processing steps for corpus clean-up and Section 5 gives
conclusions and directions for future work.

1https://twitter.com/tos

2http://bootcat.sslmit.unibo.it/

2279

2. Description of TweetCaT
Since only a small fraction of the streamed tweets are in the
desired language, we use the Twitter searching API to iden-
tify the tweets containing user-speciﬁed seed terms speciﬁc
for the language. The seed terms are manually selected to
be fairly high-frequency content words that are, however,
not in the vocabulary of other languages. The number of
such terms can be quite small: we have deﬁned 40 seed
terms for Croatian and Serbian and 20 for Slovene.
The tool is written in Python and iterates a user-speciﬁed
number of times over the two basic steps:

1. querying the search API with the goal of identifying

new users and

2. retrieving new tweets of already known users

In the ﬁrst step we query the search API with every seed
term and check if the retrieved users are already in our user
index. If they are not, we retrieve their timeline of recent
tweets and perform basic language identiﬁcation over this
collection. We determine the language of the tweets by
comparing their vocabulary against a user-deﬁned list of
very frequent words. Again, this list can be quite small,
for Croatian and Serbian we used 87 words and 40 words
for Slovene. We temporarily discard users with timelines of
less than 100 tweets because this turns out to be a minimum
for a reliable language estimate.
Once a user tweeting in the language is identiﬁed, (s)he
is added to the user index and checked for followers and
friends (users (s)he follows) assuming that they will tweet
in the same language. Each of the followers and friends is
subjected to the same language identiﬁcation procedure as
the initial user.
The second step consists of retrieving new tweets from
the timelines of all known users. No additional language
identiﬁcation is performed once a user has passed the ini-
tial check since this would signiﬁcantly complicate our
straight-forward procedure which main goal is to produce
high-recall data collections that can be ﬁltered later on,
when the maximum amount of information on users is
available.
The whole procedure presented in pseudocode is as fol-
lows:

function lang_id(author):

timeline=tokenize(timeline_api(author))
return coverage(timeline,function_words)>=threshold

for seed_term in seed_terms:

for hit in search_api(seed_term):

if hit.author not in user_index:

if lang_id(hit.author):

user_index.add(hit.author)
for follower in follower_api(hit.author):

if follower not in user_index:

if lang_id(follower):

user_index.add(follower)
for friend in friend_api(hit.author):

if friend not in user_index:

if lang_id(friend):

user_index.add(friend)

for user in user_index:

for tweet in new_tweets_api(user):

output(tweet)

The tool outputs the tweets in a simple XML format, where
each tweet is given its metadata along with the name of the

author and the text of the tweet, as shown below:

<tweet id="429333550584721413"

created_at="2014-01-31T19:21:09"
retrieved_at="2014-02-01T17:29:55.98763"
favorite_count="19" retweet_count="1">

<screen_name>dfiser3</screen_name>
<text>Kurc pa petkov veˇcer, v katerem je

beseda izpiti samostalnik,
ne glagol.</text>

</tweet>

The information about the number of users that favorited
or retweeted the tweet is likely to change during time. The
fact that this collection will be distributed as a script for
downloading the collection via the API has the beneﬁt of
keeping these data as up-to-date as possible.

3. Analysis of the collection procedure
While the tool can be run for a speciﬁed period of time or
to gather a speciﬁed amount of text, it is our goal to run
the tool continuously in order to compile monitor corpora
for all three languages, especially because historical Twitter
data is not available through the API. To gain insight into
the dynamics of collecting the tweetosphere for smaller lan-
guages we present a timeline of identifying new tweets and
users in Figure 1 for the ﬁrst 235 days of running the tool.
The ﬁgure shows that both curves drop rapidly in the ﬁrst
ten days and stabilize from that point on. For Croatian and
Serbian an average 320k tweets are collected daily in the
ﬁrst ten days, which drops by half in the next 10 days. Af-
ter that period the number of new tweets stays constant on
around 120k tweets daily. The number of new users be-
haves in a similar fashion although with much smaller num-
bers, identifying on average 2.3k users daily in the ﬁrst ten
days, going down to constant 110 users a day for the re-
mainder of the timeframe.
For Slovene we observe a similar phenomenon with a day
average of 59k tweets in the ﬁrst ten days and constant 20k
new tweets in the remainder of the timeframe. In the ﬁrst
ten days we identify just below 400 users daily which drops
off to average 13 new users a day.
As can be seen, this approach yields a reasonably sized
Twitter collection in a matter of days by collecting 2.6 mil-
lion of Croatian and Serbian tweets and half a million of
Slovene tweets in the ﬁrst week. Running the tool for a
longer period produces a smaller, but constant and still sig-
niﬁcant stream of new data.

4. Post-processing
The amount of data collected in the ﬁrst 235 days
(hrsrTweets and slTweets) is given in the ﬁrst half of Ta-
ble 1. We decided to post-process this data collection with
the idea of producing Twitter corpora suitable for a wide
range of applications.
We focused on further tuning language identiﬁcation by
removing users who tweet more in languages similar to
the languages of interest and users who tweet signiﬁcantly
more in any other language than in the languages of inter-
est. It is important to notice that we were not able to make

2280

Figure 1: The number of new tweets and users obtained during time on both corpora

a decision of this quality during the data collection process
because of a smaller amount of information available. It
could be possible to extend the TweetCaT tool with an ad-
ditional ﬁlter that would be applied after enough data was
collected and we consider this to be one of the most likely
improvements for the next version of the tool.
We ﬁrst used the Python module langid.py (Lui and Bald-
win, 2012) with off-the-shelf models to identify the lan-
guage of each tweet with mentions, retweets, URLs and
hashtags removed.
In both corpora langid.py identiﬁed
tweets written in 97 different languages (all the languages
langid.py is trained on) showing that per-tweet language
identiﬁcation is a very difﬁcult task. In each corpus around
50% of the tweets are identiﬁed as being in the sought
language(s), followed by English with around 15-20% of
tweets, similar languages having around 10%, with each of
all other languages being identiﬁed in less than 1% of the
tweets.
We encoded the results of this procedure on the level of
each tweet yielding the following ﬁnal format of each
tweet:

<tweet id="429333550584721413"

created_at="2014-01-31T19:21:09"
retrieved_at="2014-02-01T17:29:55.98763"
favorite_count="19" retweet_count="1"
lang="sl" prob="0.999999999423"
norm_length="72">

<screen_name>dfiser3</screen_name>
<text>Kurc pa petkov veˇcer, v katerem je

beseda izpiti samostalnik,
ne glagol.</text>

</tweet>

removing thereby errors in our simple language identiﬁca-
tion procedure. With the second criterion we remove users
that tweet twice as often in any other language than the lan-
guage(s) of interest removing thereby users that tweet prob-
ably mostly in English.
The ﬁrst criterion ﬁlters out 16% of users in the Croatian
and Serbian collection and 20% of users in the Slovene col-
lection. The second criterion ﬁlters out a further 20% of
users in the Croatian and Serbian collection and 25% of
users in the Slovene one. The intersection between the sets
of users that were ﬁltered out with each criterion is quite
high, 85%, while12% of users satisfy the second criterion
only and 2% of users only the ﬁrst criterion. This points to
the direction that using the second criterion, i.e. removing
users who tweet in any other language twice as often, could
be sufﬁcient for this task.
After performing post-processing, the size of the current
235 day corpora (called hrsrTwitterCorpus and slTwitter-
Corpus) by the number of users, number of tweets and num-
ber of words is given in the second part of Table 1.
Finally, we applied a recently developed language identi-
ﬁer (Ljubeˇsi´c, 2014) that showed to be very efﬁcient in
discriminating between very closely related languages, in
particular Croatian and Serbian web data. It is based on un-
igram language models trained on 1.9 billion tokens from
the hrWaC Croatian top-level-domain web corpus3 and 900
million tokens of from the srWaC Serbian top-level-domain
web corpus4 with a simple maximum-a-posteriori (MAP)
decision rule. On the task of discriminating between those
languages on web text, the approach has shown to cut the
error of the very efﬁcient Blacklist classiﬁer (Tiedemann
and Ljubeˇsi´c, 2012) four times. It is important to note that

After annotating each tweet with the language attribute, we
applied a user-level ﬁlter based on two criteria. With the
ﬁrst criterion we remove users that tweet more in a deﬁned
set of similar languages than in the language(s) of interest

hrwac/

srwac/

3http://nlp.ffzg.hr/resources/corpora/

4http://nlp.ffzg.hr/resources/corpora/

2281

hrsrTweets
slTweets
hrsrTwitterCorpus
hrTwitterCorpus
srTwitterCorpus
slTwitterCorpus

users
51,381
7,284
41,807
4,465
26,869
5,483

tweets
26,047,874
4,504,745
21,360,940
2,070,381
14,072,777
3,035,304

words
290,557,841
55,492,816
235,952,967
23,410,410
157,531,327
38,465,311

Table 1: The number of users, tweets and words in the ini-
tial data collections and the ﬁnal corpora

the Blacklist classiﬁer was already performing at the 100%
accuracy level on newspaper texts, but has shown to strug-
gle with everything the Web has to offer.
The language identiﬁer was run on the collection of each
user’s tweets with URL-s, hashtags and mentions removed.
Beside the MAP decision we also calculated a normalized
log-probability for each language so that the sum of those
normalized probabilities sums to -1.
Manual inspection of the results revealed that Serbian users
are correctly classiﬁed, but that users identiﬁed as bor-
derline Croatian in many cases are actually Montenegrin,
who use the ijekavian yat reﬂex, but partially use Serbian-
speciﬁc lexis and Serbian-speciﬁc syntactic patterns.
By plotting the distribution of the normalized log proba-
bility for each user regarding the Croatian model, which is
depicted in Figure 2, we realized that the Montenegrin (and
possibly Bosnian?) tweet distribution is actually visible on
the right side of the left, bigger, Serbian user distribution.
This was also our ﬁrst realization that the Twitter service is
obviously very popular in a country of only 600 thousand
inhabitants. Additionally, we were surprised by the dras-
tically larger number of users tweeting in Serbian than in
Croatian.
Having in mind our current limitations regarding the capa-
bility to discriminate between Croatian and Serbian Twitter
users, we deﬁned for now two subcorpora, hrTwitterCorpus
and srTwitterCorpus, where the srTwitterCorpus contains
users that were classiﬁed by MAP as Serbian (dotted line
in Figure 2) while for the hrTwitterCorpus the normalized
log-probability criterion was set to -0.49 (full line in Fig-
ure 2). The ﬁnal sizes of those corpora are given in Table
1. The normalized log-probabilities for each user are dis-
tributed as part of the corpora enabling each user to redeﬁne
the language criterion.
It is important to stress that the produced corpora do not
contain tweets in the speciﬁc language only, but all tweets
of users that mostly tweet in that language. This design cri-
terion was motivated by the fact that cleaning up a Twitter
collection on the level of tweets runs the danger of remov-
ing also mixed-language tweets, which are very frequent
esp. with English, and whether such tweets should actually
be removed heavily depends on the intended usage of the
collection.

5. Conclusion

We have presented TweetCaT, a freely available tool for
collecting Twitter corpora of smaller languages and the

twitter/

2282

Figure 2: Distribution of the normalized log probability
for Croatian regarding the discrimination between Croatian
and Serbian on the user level

Twitter corpora of Croatian, Serbian and Slovene we have
built with the tool. The procedure aims to collect the largest
corpus with the least strain on the available Twitter APIs.
The tool is published on github5.
In our further work we plan to make the corpora available
via a concordancer. We are in the process of making the
corpora available for download following the Twitter Terms
of Service, as a set of tweet identiﬁers, their annotations
and a tweet crawling tool for downloading and recreating
the corpus6. We are also working on way to normalize the
language of Croatian and Slovene tweets, which is often
colloquial and informal, in order to be able to process it
with tools tranined on standard language; we have currently
taken the ﬁrst steps in this direction, by using character-
based statistical machine translation on a test collection of
Slovene tweets (Ljubeˇsi´c et al., 2014). Finaly, it would be
interesting to further investigate language identiﬁcation for
very closely related languages, in order to be able to reli-
ably distinguish not only Croatian and Serbian tweets, but
also Montenegrin and Bosnian ones.

6. Acknowledgement
The research leading to these results has received fund-
ing from the European Union Seventh Framework Pro-
gramme FP7/2007-2013 under grant agreement no. PIAP-
GA-2012-324414 (project Abu-MaTran) and the Research
programme of the Slovenian Research Agency P2–0103
”Knowledge Technologies”.

7. References
Marco Baroni and Silvia Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In In Pro-
ceedings of LREC 2004, pages 1313–1316.

5https://github.com/nljubesi/tweetcat
6http://nlp.ffzg.hr/resources/corpora/

Frank Jacobs. 2011. 539 - Vive le tweet! a map of Twitter’s

languages.

Shamanth Kumar, Fred Morstatter, and Huan Liu. 2013.
Twitter Data Analytics. Springer, New York, NY, USA.
Nikola Ljubeˇsi´c. 2014. {bs,hr,sr}WaC: Web corpora of
Bosnian, Croatian and Serbian. In Proceedings of the
WAC-9 Workshop.

Nikola Ljubeˇsi´c, Tomaˇz Erjavec, and Darja Fiˇsser. 2014.
Standardizing tweets with character-level machine trans-
lation. In Proceedings of the 15th International Confer-
ence, CICLing 2014, Lecture Notes in Computer Sci-
ence. Springer.

Marco Lui and Timothy Baldwin. 2012. langid.py: An off-
the-shelf language identiﬁcation tool. In ACL (System
Demonstrations), pages 25–30.

Richard McCreadie, Ian Soboroff, Jimmy Lin, Craig Mac-
donald, Iadh Ounis, and Dean McCullough. 2012. On
building a reusable twitter corpus. In Proceedings of the
35th international ACM SIGIR conference on Research
and development in information retrieval, SIGIR ’12,
pages 1113–1114, New York, NY, USA. ACM.

Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010.

In Proceedings of the
The edinburgh twitter corpus.
NAACL HLT 2010 Workshop on Computational Linguis-
tics in a World of Social Media, WSA ’10, pages 25–26,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Simone Paolo Ponzetto and Andrea Zielinski. 2013. Ex-
ploiting social media for natural language processing:
Bridging the gap between language-centric and real-
world applications. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Tutorials), pages 5–6, Soﬁa, Bulgaria, August. As-
sociation for Computational Linguistics.

J¨org Tiedemann and Nikola Ljubeˇsi´c. 2012. Efﬁcient dis-
crimination between closely related languages. In Pro-
ceedings of COLING 2012, pages 2619–2634, Mumbai,
India.

Jaewon Yang and Jure Leskovec. 2011. Patterns of tem-
poral variation in online media. In Proceedings of the
fourth ACM international conference on Web search and
data mining, WSDM ’11, pages 177–186, New York,
NY, USA. ACM.

2283

TweetCaT: a tool for building Twitter corpora of smaller languages

Nikola Ljubeˇsi´c,∗ Darja Fiˇser,† Tomaˇz Erjavec‡

* Dept. of Information and Communication Sciences, University of Zagreb
Ivana Luˇci´ca 3, HR-10000 Zagreb, Croatia
nikola.ljubesic@ffzg.hr

† Dept. of Translation, University of Ljubljana
Aˇskerˇceva 2, SI-1000 Ljubljana, Slovenia
darja.ﬁser@ff.uni-lj.si

‡ Dept. of Knowledge Technologies, Joˇzef Stefan Institute
Jamova cesta 39, SI-1000 Ljubljana, Slovenia
tomaz.erjavec@ijs.si

Abstract
This paper presents TweetCaT, an open-source Python tool for building Twitter corpora that was designed for smaller languages. Using
the Twitter search API and a set of seed terms, the tool identiﬁes users tweeting in the language of interest together with their friends
and followers. By running the tool for 235 days we tested it on the task of collecting two monitor corpora, one for Croatian and Serbian
and the other for Slovene, thus also creating new and valuable resources for these languages. A post-processing step on the collected
corpus is also described, which ﬁlters out users that tweet predominantly in a foreign language thus further cleans the collected corpora.
Finally, an experiment on discriminating between Croatian and Serbian Twitter users is reported.

Keywords: Twitter corpora, open source, less-resourced languages, Croatian, Serbian, Slovene

1.

Introduction

detail by Kumar et al. (2013).

Twitter is a microblogging platform that was created in
2006 and offers the users the ability to interact with other
members in the community in real time over the Internet or
on their mobile phones. The users send short, 140-character
messages, called “tweets” to their “followers” (other users
who subscribe to those messages). Today, there are more
than 550 million Twitter users with over 100,000 new users
joining every day. On average, almost 60 million tweets are
published every day (Ponzetto and Zielinski, 2013). Tweets
are becoming an important data source for natural language
processing and corpus linguistics and with its growing pop-
ularity Twitter an increasing number of tweets are written in
languages other than English (Jacobs, 2011), which is why
it is becoming increasingly important to be able to process
tweets in other languages as well.
One of the ﬁrst and best known Twitter corpora is the Ed-
inburgh Twitter Corpus (Petrovi´c et al., 2010) that contains
almost 100 million tweets which were collected over a pe-
riod of two months using the Twitter streaming API. An-
other well-known Twitter collection is the Stanford Twit-
ter Corpus (Yang and Leskovec, 2011), which contains 467
million posts from 20 million users covering a 7 month pe-
riod in 2009. However, due to Twitter’s terms of service,1
they are no longer available as a dataset.
McCreadie et al. (2012) have overcome this problem by
developing Tweets2011, a methodology to distribute a set
of tweet identiﬁers and a separate tweet crawling tool for
downloading the identiﬁed tweets. Using the Twitter API
to collect tweets has become the standard way of compiling
Twitter corpora and has been described and discussed in

However, these approaches assume English as the language
of interest and cannot be directly used for other languages,
especially smaller ones, such as Croatian (4 million speak-
ers), Serbian (7 million) and Slovene (2 million), for which
standard techniques would return results of which only a
small fraction would be useful as a source of data for the
language in questions.

We present an alternative method that uses seed terms and
a simple language identiﬁcation module to ﬁnd new users
as well as new tweets from already known users that tweet
in the target language. The tool is named TweetCat, as the
basic idea follows the well known BootCat2 (Baroni and
Bernardini, 2004) tool, which collects URLs of Web pages
from seed terms in order to build Web-based corpora of par-
ticular languages and domains. We build two Twitter cor-
pora, one for Croatian and Serbian, and one for Slovene.
In a ﬁnal experiment we attempt to discriminate between
Croatian and Serbian Twitter users with partial success, but
obtain interesting insights in the problem and Twitter pop-
ularity among the speakers of the two languages. The col-
lection tool as well as the compiled corpora are available
under permissive licenses. The tool can easily be adapted
for other languages.

The rest of this paper is structured as follows: Section
2 presents the TweetCat tool, Section 3 gives an analysis
of the collection procedure, Section 4 details several post-
processing steps for corpus clean-up and Section 5 gives
conclusions and directions for future work.

1https://twitter.com/tos

2http://bootcat.sslmit.unibo.it/

2279

2. Description of TweetCaT
Since only a small fraction of the streamed tweets are in the
desired language, we use the Twitter searching API to iden-
tify the tweets containing user-speciﬁed seed terms speciﬁc
for the language. The seed terms are manually selected to
be fairly high-frequency content words that are, however,
not in the vocabulary of other languages. The number of
such terms can be quite small: we have deﬁned 40 seed
terms for Croatian and Serbian and 20 for Slovene.
The tool is written in Python and iterates a user-speciﬁed
number of times over the two basic steps:

1. querying the search API with the goal of identifying

new users and

2. retrieving new tweets of already known users

In the ﬁrst step we query the search API with every seed
term and check if the retrieved users are already in our user
index. If they are not, we retrieve their timeline of recent
tweets and perform basic language identiﬁcation over this
collection. We determine the language of the tweets by
comparing their vocabulary against a user-deﬁned list of
very frequent words. Again, this list can be quite small,
for Croatian and Serbian we used 87 words and 40 words
for Slovene. We temporarily discard users with timelines of
less than 100 tweets because this turns out to be a minimum
for a reliable language estimate.
Once a user tweeting in the language is identiﬁed, (s)he
is added to the user index and checked for followers and
friends (users (s)he follows) assuming that they will tweet
in the same language. Each of the followers and friends is
subjected to the same language identiﬁcation procedure as
the initial user.
The second step consists of retrieving new tweets from
the timelines of all known users. No additional language
identiﬁcation is performed once a user has passed the ini-
tial check since this would signiﬁcantly complicate our
straight-forward procedure which main goal is to produce
high-recall data collections that can be ﬁltered later on,
when the maximum amount of information on users is
available.
The whole procedure presented in pseudocode is as fol-
lows:

function lang_id(author):

timeline=tokenize(timeline_api(author))
return coverage(timeline,function_words)>=threshold

for seed_term in seed_terms:

for hit in search_api(seed_term):

if hit.author not in user_index:

if lang_id(hit.author):

user_index.add(hit.author)
for follower in follower_api(hit.author):

if follower not in user_index:

if lang_id(follower):

user_index.add(follower)
for friend in friend_api(hit.author):

if friend not in user_index:

if lang_id(friend):

user_index.add(friend)

for user in user_index:

for tweet in new_tweets_api(user):

output(tweet)

The tool outputs the tweets in a simple XML format, where
each tweet is given its metadata along with the name of the

author and the text of the tweet, as shown below:

<tweet id="429333550584721413"

created_at="2014-01-31T19:21:09"
retrieved_at="2014-02-01T17:29:55.98763"
favorite_count="19" retweet_count="1">

<screen_name>dfiser3</screen_name>
<text>Kurc pa petkov veˇcer, v katerem je

beseda izpiti samostalnik,
ne glagol.</text>

</tweet>

The information about the number of users that favorited
or retweeted the tweet is likely to change during time. The
fact that this collection will be distributed as a script for
downloading the collection via the API has the beneﬁt of
keeping these data as up-to-date as possible.

3. Analysis of the collection procedure
While the tool can be run for a speciﬁed period of time or
to gather a speciﬁed amount of text, it is our goal to run
the tool continuously in order to compile monitor corpora
for all three languages, especially because historical Twitter
data is not available through the API. To gain insight into
the dynamics of collecting the tweetosphere for smaller lan-
guages we present a timeline of identifying new tweets and
users in Figure 1 for the ﬁrst 235 days of running the tool.
The ﬁgure shows that both curves drop rapidly in the ﬁrst
ten days and stabilize from that point on. For Croatian and
Serbian an average 320k tweets are collected daily in the
ﬁrst ten days, which drops by half in the next 10 days. Af-
ter that period the number of new tweets stays constant on
around 120k tweets daily. The number of new users be-
haves in a similar fashion although with much smaller num-
bers, identifying on average 2.3k users daily in the ﬁrst ten
days, going down to constant 110 users a day for the re-
mainder of the timeframe.
For Slovene we observe a similar phenomenon with a day
average of 59k tweets in the ﬁrst ten days and constant 20k
new tweets in the remainder of the timeframe. In the ﬁrst
ten days we identify just below 400 users daily which drops
off to average 13 new users a day.
As can be seen, this approach yields a reasonably sized
Twitter collection in a matter of days by collecting 2.6 mil-
lion of Croatian and Serbian tweets and half a million of
Slovene tweets in the ﬁrst week. Running the tool for a
longer period produces a smaller, but constant and still sig-
niﬁcant stream of new data.

4. Post-processing
The amount of data collected in the ﬁrst 235 days
(hrsrTweets and slTweets) is given in the ﬁrst half of Ta-
ble 1. We decided to post-process this data collection with
the idea of producing Twitter corpora suitable for a wide
range of applications.
We focused on further tuning language identiﬁcation by
removing users who tweet more in languages similar to
the languages of interest and users who tweet signiﬁcantly
more in any other language than in the languages of inter-
est. It is important to notice that we were not able to make

2280

Figure 1: The number of new tweets and users obtained during time on both corpora

a decision of this quality during the data collection process
because of a smaller amount of information available. It
could be possible to extend the TweetCaT tool with an ad-
ditional ﬁlter that would be applied after enough data was
collected and we consider this to be one of the most likely
improvements for the next version of the tool.
We ﬁrst used the Python module langid.py (Lui and Bald-
win, 2012) with off-the-shelf models to identify the lan-
guage of each tweet with mentions, retweets, URLs and
hashtags removed.
In both corpora langid.py identiﬁed
tweets written in 97 different languages (all the languages
langid.py is trained on) showing that per-tweet language
identiﬁcation is a very difﬁcult task. In each corpus around
50% of the tweets are identiﬁed as being in the sought
language(s), followed by English with around 15-20% of
tweets, similar languages having around 10%, with each of
all other languages being identiﬁed in less than 1% of the
tweets.
We encoded the results of this procedure on the level of
each tweet yielding the following ﬁnal format of each
tweet:

<tweet id="429333550584721413"

created_at="2014-01-31T19:21:09"
retrieved_at="2014-02-01T17:29:55.98763"
favorite_count="19" retweet_count="1"
lang="sl" prob="0.999999999423"
norm_length="72">

<screen_name>dfiser3</screen_name>
<text>Kurc pa petkov veˇcer, v katerem je

beseda izpiti samostalnik,
ne glagol.</text>

</tweet>

removing thereby errors in our simple language identiﬁca-
tion procedure. With the second criterion we remove users
that tweet twice as often in any other language than the lan-
guage(s) of interest removing thereby users that tweet prob-
ably mostly in English.
The ﬁrst criterion ﬁlters out 16% of users in the Croatian
and Serbian collection and 20% of users in the Slovene col-
lection. The second criterion ﬁlters out a further 20% of
users in the Croatian and Serbian collection and 25% of
users in the Slovene one. The intersection between the sets
of users that were ﬁltered out with each criterion is quite
high, 85%, while12% of users satisfy the second criterion
only and 2% of users only the ﬁrst criterion. This points to
the direction that using the second criterion, i.e. removing
users who tweet in any other language twice as often, could
be sufﬁcient for this task.
After performing post-processing, the size of the current
235 day corpora (called hrsrTwitterCorpus and slTwitter-
Corpus) by the number of users, number of tweets and num-
ber of words is given in the second part of Table 1.
Finally, we applied a recently developed language identi-
ﬁer (Ljubeˇsi´c, 2014) that showed to be very efﬁcient in
discriminating between very closely related languages, in
particular Croatian and Serbian web data. It is based on un-
igram language models trained on 1.9 billion tokens from
the hrWaC Croatian top-level-domain web corpus3 and 900
million tokens of from the srWaC Serbian top-level-domain
web corpus4 with a simple maximum-a-posteriori (MAP)
decision rule. On the task of discriminating between those
languages on web text, the approach has shown to cut the
error of the very efﬁcient Blacklist classiﬁer (Tiedemann
and Ljubeˇsi´c, 2012) four times. It is important to note that

After annotating each tweet with the language attribute, we
applied a user-level ﬁlter based on two criteria. With the
ﬁrst criterion we remove users that tweet more in a deﬁned
set of similar languages than in the language(s) of interest

hrwac/

srwac/

3http://nlp.ffzg.hr/resources/corpora/

4http://nlp.ffzg.hr/resources/corpora/

2281

hrsrTweets
slTweets
hrsrTwitterCorpus
hrTwitterCorpus
srTwitterCorpus
slTwitterCorpus

users
51,381
7,284
41,807
4,465
26,869
5,483

tweets
26,047,874
4,504,745
21,360,940
2,070,381
14,072,777
3,035,304

words
290,557,841
55,492,816
235,952,967
23,410,410
157,531,327
38,465,311

Table 1: The number of users, tweets and words in the ini-
tial data collections and the ﬁnal corpora

the Blacklist classiﬁer was already performing at the 100%
accuracy level on newspaper texts, but has shown to strug-
gle with everything the Web has to offer.
The language identiﬁer was run on the collection of each
user’s tweets with URL-s, hashtags and mentions removed.
Beside the MAP decision we also calculated a normalized
log-probability for each language so that the sum of those
normalized probabilities sums to -1.
Manual inspection of the results revealed that Serbian users
are correctly classiﬁed, but that users identiﬁed as bor-
derline Croatian in many cases are actually Montenegrin,
who use the ijekavian yat reﬂex, but partially use Serbian-
speciﬁc lexis and Serbian-speciﬁc syntactic patterns.
By plotting the distribution of the normalized log proba-
bility for each user regarding the Croatian model, which is
depicted in Figure 2, we realized that the Montenegrin (and
possibly Bosnian?) tweet distribution is actually visible on
the right side of the left, bigger, Serbian user distribution.
This was also our ﬁrst realization that the Twitter service is
obviously very popular in a country of only 600 thousand
inhabitants. Additionally, we were surprised by the dras-
tically larger number of users tweeting in Serbian than in
Croatian.
Having in mind our current limitations regarding the capa-
bility to discriminate between Croatian and Serbian Twitter
users, we deﬁned for now two subcorpora, hrTwitterCorpus
and srTwitterCorpus, where the srTwitterCorpus contains
users that were classiﬁed by MAP as Serbian (dotted line
in Figure 2) while for the hrTwitterCorpus the normalized
log-probability criterion was set to -0.49 (full line in Fig-
ure 2). The ﬁnal sizes of those corpora are given in Table
1. The normalized log-probabilities for each user are dis-
tributed as part of the corpora enabling each user to redeﬁne
the language criterion.
It is important to stress that the produced corpora do not
contain tweets in the speciﬁc language only, but all tweets
of users that mostly tweet in that language. This design cri-
terion was motivated by the fact that cleaning up a Twitter
collection on the level of tweets runs the danger of remov-
ing also mixed-language tweets, which are very frequent
esp. with English, and whether such tweets should actually
be removed heavily depends on the intended usage of the
collection.

5. Conclusion

We have presented TweetCaT, a freely available tool for
collecting Twitter corpora of smaller languages and the

twitter/

2282

Figure 2: Distribution of the normalized log probability
for Croatian regarding the discrimination between Croatian
and Serbian on the user level

Twitter corpora of Croatian, Serbian and Slovene we have
built with the tool. The procedure aims to collect the largest
corpus with the least strain on the available Twitter APIs.
The tool is published on github5.
In our further work we plan to make the corpora available
via a concordancer. We are in the process of making the
corpora available for download following the Twitter Terms
of Service, as a set of tweet identiﬁers, their annotations
and a tweet crawling tool for downloading and recreating
the corpus6. We are also working on way to normalize the
language of Croatian and Slovene tweets, which is often
colloquial and informal, in order to be able to process it
with tools tranined on standard language; we have currently
taken the ﬁrst steps in this direction, by using character-
based statistical machine translation on a test collection of
Slovene tweets (Ljubeˇsi´c et al., 2014). Finaly, it would be
interesting to further investigate language identiﬁcation for
very closely related languages, in order to be able to reli-
ably distinguish not only Croatian and Serbian tweets, but
also Montenegrin and Bosnian ones.

6. Acknowledgement
The research leading to these results has received fund-
ing from the European Union Seventh Framework Pro-
gramme FP7/2007-2013 under grant agreement no. PIAP-
GA-2012-324414 (project Abu-MaTran) and the Research
programme of the Slovenian Research Agency P2–0103
”Knowledge Technologies”.

7. References
Marco Baroni and Silvia Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In In Pro-
ceedings of LREC 2004, pages 1313–1316.

5https://github.com/nljubesi/tweetcat
6http://nlp.ffzg.hr/resources/corpora/

Frank Jacobs. 2011. 539 - Vive le tweet! a map of Twitter’s

languages.

Shamanth Kumar, Fred Morstatter, and Huan Liu. 2013.
Twitter Data Analytics. Springer, New York, NY, USA.
Nikola Ljubeˇsi´c. 2014. {bs,hr,sr}WaC: Web corpora of
Bosnian, Croatian and Serbian. In Proceedings of the
WAC-9 Workshop.

Nikola Ljubeˇsi´c, Tomaˇz Erjavec, and Darja Fiˇsser. 2014.
Standardizing tweets with character-level machine trans-
lation. In Proceedings of the 15th International Confer-
ence, CICLing 2014, Lecture Notes in Computer Sci-
ence. Springer.

Marco Lui and Timothy Baldwin. 2012. langid.py: An off-
the-shelf language identiﬁcation tool. In ACL (System
Demonstrations), pages 25–30.

Richard McCreadie, Ian Soboroff, Jimmy Lin, Craig Mac-
donald, Iadh Ounis, and Dean McCullough. 2012. On
building a reusable twitter corpus. In Proceedings of the
35th international ACM SIGIR conference on Research
and development in information retrieval, SIGIR ’12,
pages 1113–1114, New York, NY, USA. ACM.

Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010.

In Proceedings of the
The edinburgh twitter corpus.
NAACL HLT 2010 Workshop on Computational Linguis-
tics in a World of Social Media, WSA ’10, pages 25–26,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Simone Paolo Ponzetto and Andrea Zielinski. 2013. Ex-
ploiting social media for natural language processing:
Bridging the gap between language-centric and real-
world applications. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Tutorials), pages 5–6, Soﬁa, Bulgaria, August. As-
sociation for Computational Linguistics.

J¨org Tiedemann and Nikola Ljubeˇsi´c. 2012. Efﬁcient dis-
crimination between closely related languages. In Pro-
ceedings of COLING 2012, pages 2619–2634, Mumbai,
India.

Jaewon Yang and Jure Leskovec. 2011. Patterns of tem-
poral variation in online media. In Proceedings of the
fourth ACM international conference on Web search and
data mining, WSDM ’11, pages 177–186, New York,
NY, USA. ACM.

2283

TweetCaT: a tool for building Twitter corpora of smaller languages

Nikola Ljubeˇsi´c,∗ Darja Fiˇser,† Tomaˇz Erjavec‡

* Dept. of Information and Communication Sciences, University of Zagreb
Ivana Luˇci´ca 3, HR-10000 Zagreb, Croatia
nikola.ljubesic@ffzg.hr

† Dept. of Translation, University of Ljubljana
Aˇskerˇceva 2, SI-1000 Ljubljana, Slovenia
darja.ﬁser@ff.uni-lj.si

‡ Dept. of Knowledge Technologies, Joˇzef Stefan Institute
Jamova cesta 39, SI-1000 Ljubljana, Slovenia
tomaz.erjavec@ijs.si

Abstract
This paper presents TweetCaT, an open-source Python tool for building Twitter corpora that was designed for smaller languages. Using
the Twitter search API and a set of seed terms, the tool identiﬁes users tweeting in the language of interest together with their friends
and followers. By running the tool for 235 days we tested it on the task of collecting two monitor corpora, one for Croatian and Serbian
and the other for Slovene, thus also creating new and valuable resources for these languages. A post-processing step on the collected
corpus is also described, which ﬁlters out users that tweet predominantly in a foreign language thus further cleans the collected corpora.
Finally, an experiment on discriminating between Croatian and Serbian Twitter users is reported.

Keywords: Twitter corpora, open source, less-resourced languages, Croatian, Serbian, Slovene

1.

Introduction

detail by Kumar et al. (2013).

Twitter is a microblogging platform that was created in
2006 and offers the users the ability to interact with other
members in the community in real time over the Internet or
on their mobile phones. The users send short, 140-character
messages, called “tweets” to their “followers” (other users
who subscribe to those messages). Today, there are more
than 550 million Twitter users with over 100,000 new users
joining every day. On average, almost 60 million tweets are
published every day (Ponzetto and Zielinski, 2013). Tweets
are becoming an important data source for natural language
processing and corpus linguistics and with its growing pop-
ularity Twitter an increasing number of tweets are written in
languages other than English (Jacobs, 2011), which is why
it is becoming increasingly important to be able to process
tweets in other languages as well.
One of the ﬁrst and best known Twitter corpora is the Ed-
inburgh Twitter Corpus (Petrovi´c et al., 2010) that contains
almost 100 million tweets which were collected over a pe-
riod of two months using the Twitter streaming API. An-
other well-known Twitter collection is the Stanford Twit-
ter Corpus (Yang and Leskovec, 2011), which contains 467
million posts from 20 million users covering a 7 month pe-
riod in 2009. However, due to Twitter’s terms of service,1
they are no longer available as a dataset.
McCreadie et al. (2012) have overcome this problem by
developing Tweets2011, a methodology to distribute a set
of tweet identiﬁers and a separate tweet crawling tool for
downloading the identiﬁed tweets. Using the Twitter API
to collect tweets has become the standard way of compiling
Twitter corpora and has been described and discussed in

However, these approaches assume English as the language
of interest and cannot be directly used for other languages,
especially smaller ones, such as Croatian (4 million speak-
ers), Serbian (7 million) and Slovene (2 million), for which
standard techniques would return results of which only a
small fraction would be useful as a source of data for the
language in questions.

We present an alternative method that uses seed terms and
a simple language identiﬁcation module to ﬁnd new users
as well as new tweets from already known users that tweet
in the target language. The tool is named TweetCat, as the
basic idea follows the well known BootCat2 (Baroni and
Bernardini, 2004) tool, which collects URLs of Web pages
from seed terms in order to build Web-based corpora of par-
ticular languages and domains. We build two Twitter cor-
pora, one for Croatian and Serbian, and one for Slovene.
In a ﬁnal experiment we attempt to discriminate between
Croatian and Serbian Twitter users with partial success, but
obtain interesting insights in the problem and Twitter pop-
ularity among the speakers of the two languages. The col-
lection tool as well as the compiled corpora are available
under permissive licenses. The tool can easily be adapted
for other languages.

The rest of this paper is structured as follows: Section
2 presents the TweetCat tool, Section 3 gives an analysis
of the collection procedure, Section 4 details several post-
processing steps for corpus clean-up and Section 5 gives
conclusions and directions for future work.

1https://twitter.com/tos

2http://bootcat.sslmit.unibo.it/

2279

2. Description of TweetCaT
Since only a small fraction of the streamed tweets are in the
desired language, we use the Twitter searching API to iden-
tify the tweets containing user-speciﬁed seed terms speciﬁc
for the language. The seed terms are manually selected to
be fairly high-frequency content words that are, however,
not in the vocabulary of other languages. The number of
such terms can be quite small: we have deﬁned 40 seed
terms for Croatian and Serbian and 20 for Slovene.
The tool is written in Python and iterates a user-speciﬁed
number of times over the two basic steps:

1. querying the search API with the goal of identifying

new users and

2. retrieving new tweets of already known users

In the ﬁrst step we query the search API with every seed
term and check if the retrieved users are already in our user
index. If they are not, we retrieve their timeline of recent
tweets and perform basic language identiﬁcation over this
collection. We determine the language of the tweets by
comparing their vocabulary against a user-deﬁned list of
very frequent words. Again, this list can be quite small,
for Croatian and Serbian we used 87 words and 40 words
for Slovene. We temporarily discard users with timelines of
less than 100 tweets because this turns out to be a minimum
for a reliable language estimate.
Once a user tweeting in the language is identiﬁed, (s)he
is added to the user index and checked for followers and
friends (users (s)he follows) assuming that they will tweet
in the same language. Each of the followers and friends is
subjected to the same language identiﬁcation procedure as
the initial user.
The second step consists of retrieving new tweets from
the timelines of all known users. No additional language
identiﬁcation is performed once a user has passed the ini-
tial check since this would signiﬁcantly complicate our
straight-forward procedure which main goal is to produce
high-recall data collections that can be ﬁltered later on,
when the maximum amount of information on users is
available.
The whole procedure presented in pseudocode is as fol-
lows:

function lang_id(author):

timeline=tokenize(timeline_api(author))
return coverage(timeline,function_words)>=threshold

for seed_term in seed_terms:

for hit in search_api(seed_term):

if hit.author not in user_index:

if lang_id(hit.author):

user_index.add(hit.author)
for follower in follower_api(hit.author):

if follower not in user_index:

if lang_id(follower):

user_index.add(follower)
for friend in friend_api(hit.author):

if friend not in user_index:

if lang_id(friend):

user_index.add(friend)

for user in user_index:

for tweet in new_tweets_api(user):

output(tweet)

The tool outputs the tweets in a simple XML format, where
each tweet is given its metadata along with the name of the

author and the text of the tweet, as shown below:

<tweet id="429333550584721413"

created_at="2014-01-31T19:21:09"
retrieved_at="2014-02-01T17:29:55.98763"
favorite_count="19" retweet_count="1">

<screen_name>dfiser3</screen_name>
<text>Kurc pa petkov veˇcer, v katerem je

beseda izpiti samostalnik,
ne glagol.</text>

</tweet>

The information about the number of users that favorited
or retweeted the tweet is likely to change during time. The
fact that this collection will be distributed as a script for
downloading the collection via the API has the beneﬁt of
keeping these data as up-to-date as possible.

3. Analysis of the collection procedure
While the tool can be run for a speciﬁed period of time or
to gather a speciﬁed amount of text, it is our goal to run
the tool continuously in order to compile monitor corpora
for all three languages, especially because historical Twitter
data is not available through the API. To gain insight into
the dynamics of collecting the tweetosphere for smaller lan-
guages we present a timeline of identifying new tweets and
users in Figure 1 for the ﬁrst 235 days of running the tool.
The ﬁgure shows that both curves drop rapidly in the ﬁrst
ten days and stabilize from that point on. For Croatian and
Serbian an average 320k tweets are collected daily in the
ﬁrst ten days, which drops by half in the next 10 days. Af-
ter that period the number of new tweets stays constant on
around 120k tweets daily. The number of new users be-
haves in a similar fashion although with much smaller num-
bers, identifying on average 2.3k users daily in the ﬁrst ten
days, going down to constant 110 users a day for the re-
mainder of the timeframe.
For Slovene we observe a similar phenomenon with a day
average of 59k tweets in the ﬁrst ten days and constant 20k
new tweets in the remainder of the timeframe. In the ﬁrst
ten days we identify just below 400 users daily which drops
off to average 13 new users a day.
As can be seen, this approach yields a reasonably sized
Twitter collection in a matter of days by collecting 2.6 mil-
lion of Croatian and Serbian tweets and half a million of
Slovene tweets in the ﬁrst week. Running the tool for a
longer period produces a smaller, but constant and still sig-
niﬁcant stream of new data.

4. Post-processing
The amount of data collected in the ﬁrst 235 days
(hrsrTweets and slTweets) is given in the ﬁrst half of Ta-
ble 1. We decided to post-process this data collection with
the idea of producing Twitter corpora suitable for a wide
range of applications.
We focused on further tuning language identiﬁcation by
removing users who tweet more in languages similar to
the languages of interest and users who tweet signiﬁcantly
more in any other language than in the languages of inter-
est. It is important to notice that we were not able to make

2280

Figure 1: The number of new tweets and users obtained during time on both corpora

a decision of this quality during the data collection process
because of a smaller amount of information available. It
could be possible to extend the TweetCaT tool with an ad-
ditional ﬁlter that would be applied after enough data was
collected and we consider this to be one of the most likely
improvements for the next version of the tool.
We ﬁrst used the Python module langid.py (Lui and Bald-
win, 2012) with off-the-shelf models to identify the lan-
guage of each tweet with mentions, retweets, URLs and
hashtags removed.
In both corpora langid.py identiﬁed
tweets written in 97 different languages (all the languages
langid.py is trained on) showing that per-tweet language
identiﬁcation is a very difﬁcult task. In each corpus around
50% of the tweets are identiﬁed as being in the sought
language(s), followed by English with around 15-20% of
tweets, similar languages having around 10%, with each of
all other languages being identiﬁed in less than 1% of the
tweets.
We encoded the results of this procedure on the level of
each tweet yielding the following ﬁnal format of each
tweet:

<tweet id="429333550584721413"

created_at="2014-01-31T19:21:09"
retrieved_at="2014-02-01T17:29:55.98763"
favorite_count="19" retweet_count="1"
lang="sl" prob="0.999999999423"
norm_length="72">

<screen_name>dfiser3</screen_name>
<text>Kurc pa petkov veˇcer, v katerem je

beseda izpiti samostalnik,
ne glagol.</text>

</tweet>

removing thereby errors in our simple language identiﬁca-
tion procedure. With the second criterion we remove users
that tweet twice as often in any other language than the lan-
guage(s) of interest removing thereby users that tweet prob-
ably mostly in English.
The ﬁrst criterion ﬁlters out 16% of users in the Croatian
and Serbian collection and 20% of users in the Slovene col-
lection. The second criterion ﬁlters out a further 20% of
users in the Croatian and Serbian collection and 25% of
users in the Slovene one. The intersection between the sets
of users that were ﬁltered out with each criterion is quite
high, 85%, while12% of users satisfy the second criterion
only and 2% of users only the ﬁrst criterion. This points to
the direction that using the second criterion, i.e. removing
users who tweet in any other language twice as often, could
be sufﬁcient for this task.
After performing post-processing, the size of the current
235 day corpora (called hrsrTwitterCorpus and slTwitter-
Corpus) by the number of users, number of tweets and num-
ber of words is given in the second part of Table 1.
Finally, we applied a recently developed language identi-
ﬁer (Ljubeˇsi´c, 2014) that showed to be very efﬁcient in
discriminating between very closely related languages, in
particular Croatian and Serbian web data. It is based on un-
igram language models trained on 1.9 billion tokens from
the hrWaC Croatian top-level-domain web corpus3 and 900
million tokens of from the srWaC Serbian top-level-domain
web corpus4 with a simple maximum-a-posteriori (MAP)
decision rule. On the task of discriminating between those
languages on web text, the approach has shown to cut the
error of the very efﬁcient Blacklist classiﬁer (Tiedemann
and Ljubeˇsi´c, 2012) four times. It is important to note that

After annotating each tweet with the language attribute, we
applied a user-level ﬁlter based on two criteria. With the
ﬁrst criterion we remove users that tweet more in a deﬁned
set of similar languages than in the language(s) of interest

hrwac/

srwac/

3http://nlp.ffzg.hr/resources/corpora/

4http://nlp.ffzg.hr/resources/corpora/

2281

hrsrTweets
slTweets
hrsrTwitterCorpus
hrTwitterCorpus
srTwitterCorpus
slTwitterCorpus

users
51,381
7,284
41,807
4,465
26,869
5,483

tweets
26,047,874
4,504,745
21,360,940
2,070,381
14,072,777
3,035,304

words
290,557,841
55,492,816
235,952,967
23,410,410
157,531,327
38,465,311

Table 1: The number of users, tweets and words in the ini-
tial data collections and the ﬁnal corpora

the Blacklist classiﬁer was already performing at the 100%
accuracy level on newspaper texts, but has shown to strug-
gle with everything the Web has to offer.
The language identiﬁer was run on the collection of each
user’s tweets with URL-s, hashtags and mentions removed.
Beside the MAP decision we also calculated a normalized
log-probability for each language so that the sum of those
normalized probabilities sums to -1.
Manual inspection of the results revealed that Serbian users
are correctly classiﬁed, but that users identiﬁed as bor-
derline Croatian in many cases are actually Montenegrin,
who use the ijekavian yat reﬂex, but partially use Serbian-
speciﬁc lexis and Serbian-speciﬁc syntactic patterns.
By plotting the distribution of the normalized log proba-
bility for each user regarding the Croatian model, which is
depicted in Figure 2, we realized that the Montenegrin (and
possibly Bosnian?) tweet distribution is actually visible on
the right side of the left, bigger, Serbian user distribution.
This was also our ﬁrst realization that the Twitter service is
obviously very popular in a country of only 600 thousand
inhabitants. Additionally, we were surprised by the dras-
tically larger number of users tweeting in Serbian than in
Croatian.
Having in mind our current limitations regarding the capa-
bility to discriminate between Croatian and Serbian Twitter
users, we deﬁned for now two subcorpora, hrTwitterCorpus
and srTwitterCorpus, where the srTwitterCorpus contains
users that were classiﬁed by MAP as Serbian (dotted line
in Figure 2) while for the hrTwitterCorpus the normalized
log-probability criterion was set to -0.49 (full line in Fig-
ure 2). The ﬁnal sizes of those corpora are given in Table
1. The normalized log-probabilities for each user are dis-
tributed as part of the corpora enabling each user to redeﬁne
the language criterion.
It is important to stress that the produced corpora do not
contain tweets in the speciﬁc language only, but all tweets
of users that mostly tweet in that language. This design cri-
terion was motivated by the fact that cleaning up a Twitter
collection on the level of tweets runs the danger of remov-
ing also mixed-language tweets, which are very frequent
esp. with English, and whether such tweets should actually
be removed heavily depends on the intended usage of the
collection.

5. Conclusion

We have presented TweetCaT, a freely available tool for
collecting Twitter corpora of smaller languages and the

twitter/

2282

Figure 2: Distribution of the normalized log probability
for Croatian regarding the discrimination between Croatian
and Serbian on the user level

Twitter corpora of Croatian, Serbian and Slovene we have
built with the tool. The procedure aims to collect the largest
corpus with the least strain on the available Twitter APIs.
The tool is published on github5.
In our further work we plan to make the corpora available
via a concordancer. We are in the process of making the
corpora available for download following the Twitter Terms
of Service, as a set of tweet identiﬁers, their annotations
and a tweet crawling tool for downloading and recreating
the corpus6. We are also working on way to normalize the
language of Croatian and Slovene tweets, which is often
colloquial and informal, in order to be able to process it
with tools tranined on standard language; we have currently
taken the ﬁrst steps in this direction, by using character-
based statistical machine translation on a test collection of
Slovene tweets (Ljubeˇsi´c et al., 2014). Finaly, it would be
interesting to further investigate language identiﬁcation for
very closely related languages, in order to be able to reli-
ably distinguish not only Croatian and Serbian tweets, but
also Montenegrin and Bosnian ones.

6. Acknowledgement
The research leading to these results has received fund-
ing from the European Union Seventh Framework Pro-
gramme FP7/2007-2013 under grant agreement no. PIAP-
GA-2012-324414 (project Abu-MaTran) and the Research
programme of the Slovenian Research Agency P2–0103
”Knowledge Technologies”.

7. References
Marco Baroni and Silvia Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In In Pro-
ceedings of LREC 2004, pages 1313–1316.

5https://github.com/nljubesi/tweetcat
6http://nlp.ffzg.hr/resources/corpora/

Frank Jacobs. 2011. 539 - Vive le tweet! a map of Twitter’s

languages.

Shamanth Kumar, Fred Morstatter, and Huan Liu. 2013.
Twitter Data Analytics. Springer, New York, NY, USA.
Nikola Ljubeˇsi´c. 2014. {bs,hr,sr}WaC: Web corpora of
Bosnian, Croatian and Serbian. In Proceedings of the
WAC-9 Workshop.

Nikola Ljubeˇsi´c, Tomaˇz Erjavec, and Darja Fiˇsser. 2014.
Standardizing tweets with character-level machine trans-
lation. In Proceedings of the 15th International Confer-
ence, CICLing 2014, Lecture Notes in Computer Sci-
ence. Springer.

Marco Lui and Timothy Baldwin. 2012. langid.py: An off-
the-shelf language identiﬁcation tool. In ACL (System
Demonstrations), pages 25–30.

Richard McCreadie, Ian Soboroff, Jimmy Lin, Craig Mac-
donald, Iadh Ounis, and Dean McCullough. 2012. On
building a reusable twitter corpus. In Proceedings of the
35th international ACM SIGIR conference on Research
and development in information retrieval, SIGIR ’12,
pages 1113–1114, New York, NY, USA. ACM.

Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010.

In Proceedings of the
The edinburgh twitter corpus.
NAACL HLT 2010 Workshop on Computational Linguis-
tics in a World of Social Media, WSA ’10, pages 25–26,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Simone Paolo Ponzetto and Andrea Zielinski. 2013. Ex-
ploiting social media for natural language processing:
Bridging the gap between language-centric and real-
world applications. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Tutorials), pages 5–6, Soﬁa, Bulgaria, August. As-
sociation for Computational Linguistics.

J¨org Tiedemann and Nikola Ljubeˇsi´c. 2012. Efﬁcient dis-
crimination between closely related languages. In Pro-
ceedings of COLING 2012, pages 2619–2634, Mumbai,
India.

Jaewon Yang and Jure Leskovec. 2011. Patterns of tem-
poral variation in online media. In Proceedings of the
fourth ACM international conference on Web search and
data mining, WSDM ’11, pages 177–186, New York,
NY, USA. ACM.

2283

