8
1
0
2
 
y
a
M
 
0
1
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
9
3
0
.
5
0
8
1
:
v
i
X
r
a

Ensemble Soft-Margin Softmax Loss for Image Classiﬁcation

Xiaobo Wang1,2,∗, Shifeng Zhang1,2,∗, Zhen Lei1,2,†, Si Liu3, Xiaojie Guo4, Stan Z. Li5,1,2
1 CBSR&NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China
2 University of Chinese Academy of Sciences, Beijing, China.
3 Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University.
4 School of Computer Software, Tianjin University, Tianjin, China.
5 Faculty of Information Technology, Macau University of Science and Technology, Macau, China.
{xiaobo.wang, shifeng.zhang, zlei, szli}@nlpr.ia.ac.cn, ﬁfthzombiesi@gmail.com, xj.max.guo@gmail.com

Abstract

Softmax loss is arguably one of the most popular
losses to train CNN models for image classiﬁca-
tion. However, recent works have exposed its limi-
tation on feature discriminability. This paper casts a
new viewpoint on the weakness of softmax loss. On
the one hand, the CNN features learned using the
softmax loss are often inadequately discriminative.
We hence introduce a soft-margin softmax function
to explicitly encourage the discrimination between
different classes. On the other hand, the learned
classiﬁer of softmax loss is weak. We propose to
assemble multiple these weak classiﬁers to a strong
one, inspired by the recognition that the diversity
among weak classiﬁers is critical to a good ensem-
ble. To achieve the diversity, we adopt the Hilbert-
Schmidt Independence Criterion (HSIC). Consid-
ering these two aspects in one framework, we de-
sign a novel loss, named as Ensemble soft-Margin
Softmax (EM-Softmax). Extensive experiments on
benchmark datasets are conducted to show the su-
periority of our design over the baseline softmax
loss and several state-of-the-art alternatives.

1 Introduction

Image classiﬁcation is a fundamental yet still challenging task
in machine learning and computer vision. Over the past years,
deep Convolutional Neural Networks (CNNs) have greatly
boosted the performance of a series of image classiﬁcation
tasks, like object classiﬁcation [Krizhevsky et al., 2012; He et
al., 2016; Liu et al., 2016], face veriﬁcation [Wen et al., 2016;
Zhang et al., 2016; Liu et al., 2017a; Wang et al., 2017a]
and hand-written digit recognition [Goodfellow et al., 2013;
Lin et al., 2013], etc. Deep networks naturally integrate
low/mid/high-level features and classiﬁers in an end-to-end
multi-layer fashion. Wherein each layer mainly consists of
convolution, pooling and non-linear activation, leading CNNs
to the strong visual representation ability as well as their cur-
rent signiﬁcant positions.

∗These authors contributed equally to this work.
†Corresponding author

To train a deep model, the loss functions, such as (square)
hinge loss, contrastive loss, triplet loss and softmax loss, etc.,
are usually equipped. Among them, the softmax loss is ar-
guably the most popular one [Liu et al., 2016], which con-
sists of three components, including the last fully connected
layer, the softmax function, and the cross-entropy loss1 . It
is widely adopted by many CNNs [Krizhevsky et al., 2012;
Simonyan and Andrew, 2014; He et al., 2016] due to its sim-
plicity and clear probabilistic interpretation. However, the
works [Liu et al., 2016; Wen et al., 2016; Zhang et al., 2016]
have shown that the original softmax loss is inadequate due to
the lack of encouraging the discriminability of CNN features.
Recently, a renewed trend is to design more effective losses
to enhance the performance. But this is non-trivial because a
new designed loss usually should be easily optimized by the
Stochastic Gradient Descent (SGD) [LeCun et al., 1998a].

To improve the softmax loss, existing works can be mainly
categorized into two groups. One group tries to reﬁne the
[Sun et al.,
cross-entropy loss of softmax loss. Sun et al.
2014] trained the CNNs with the combination of softmax loss
and contrastive loss, but the pairs of training samples are dif-
[Schroff et al., 2015] used
ﬁcult to select. Schroff et al.
the triplet loss to minimize the distance between an anchor
sample and a positive sample (of the same identity), as well
as maximize the distance between the anchor and a negative
sample (of different identities). However, requiring a mul-
tiple of three training samples as input makes it inefﬁcient.
Tang et al. [Tang, 2013] replaced the cross-entropy loss with
the hinge loss, while Liu et al. [Liu et al., 2017b] employed a
congenerous cosine loss to enlarge the inter-class distinction
as well as alleviate the inner-class variance. Its drawback is
that these two losses are frequently unstable. Recently, Wen
[Wen et al., 2016] introduced a center loss together
et al.
with the softmax loss. Zhang et al. [Zhang et al., 2016] pro-
posed a range loss to handle the case of the long tail distri-
bution of data. Both of them have achieved promising re-
sults on face veriﬁcation task. However, the objective of the
open-set face veriﬁcation (i.e., mainly to learn discriminative
features) is different from that of the closed-set image classi-
ﬁcation (i.e., simultaneously to learn discriminative features
and a strong classiﬁer). The other group is to reformulate the
softmax function of softmax loss.Liu et al. [Liu et al., 2016;

1The details of each component will be described in section 2.1.

Liu et al., 2017a] enlarged the margin of the softmax func-
tion to encourage the discriminability of features and further
extended it to the face veriﬁcation task. Wang et al. [Wang et
al., 2017a] developed a normalized softmax function to learn
discriminative features. However, for the last fully connected
layer2of softmax loss, few works have considered. The fully
convolutional networks [Li et al., 2016] and the global aver-
age pooling [Lin et al., 2013; Zhou et al., 2016] aim to modify
the fully connected layers of DNNs, they are not applicable to
the softmax classiﬁer. In fact, for deep image classiﬁcation,
the softmax classiﬁer is of utmost importance.

Since feature extracting and classiﬁer learning in CNNs are
in an end-to-end framework, in this paper, we argue that the
weakness of softmax loss mainly comes from two aspects.
One is that the extracted features are not discriminative. The
other one is that the learned classiﬁer is not strong. To address
the above issues, we introduce a simple yet effective soft-
margin softmax function to explicitly emphasize the feature
discriminability, and adopt a novel ensemble strategy to learn
a strong softmax classiﬁer. For clarity, our main contributions
are summarized as follows:

• We cast a new viewpoint on the weakness of the original
softmax loss. i.e., the extracted CNN features are insuf-
ﬁciently discriminative and the learned classiﬁer is weak
for deep image classiﬁcation.

• We design a soft-margin softmax function to encour-
age the feature discriminability and attempt to assemble
the weak classiﬁers of softmax loss by employing the
Hilbert-Schmidt Independence Criterion (HSIC).

• We conduct experiments on the datasets of MNIST, CI-
FAR10/CIFAR10+, CIFAR100/CIFAR100+, and Ima-
geNet32 [Chrabaszcz et al., 2017], which reveal the ef-
fectiveness of the proposed method.

2 Preliminary Knowledge

2.1 Softmax Loss
Assume that the output of a single image through deep con-
volution neural networks is x (i.e., CNN features), where x ∈
Rd, d is the feature dimension. Given a mini-batch of labeled
images, their outputs are {x1, x2, . . . , xn}. The correspond-
ing labels are {y1, y2, . . . , yn}, where yi ∈ {1, 2, . . . , K} is
the class indicator, and K is the number of classes. Similar
to the work [Liu et al., 2016], we deﬁne the complete soft-
max loss as the pipeline combination of the last fully con-
nected layer, the softmax function and the cross-entropy loss.
The last fully connected layer transforms the feature x into
a primary score z = [z1, z2, . . . , zK]T ∈ RK through mul-
tiple parameters W = [w1, w2, . . . , wK] ∈ Rd×K, which
is formulated as: zk = wT
k x = xT wk. Generally speak-
ing, the parameter wk can be regarded as the linear classi-
ﬁer of class k. Then, the softmax function is applied to
transform the primary score zk into a new predicted class
ezk
score as: sk =
t(cid:54)=k ezt . Finally, the cross-entropy loss
ezk +(cid:80)K
LS = − log(sy) is employed.

2For convenience, we denote it as softmax classiﬁer.

2.2 Hilbert-Schmidt Independence Criterion
The Hilbert-Schmidt Independence Criterion (HSIC) was
proposed in [Gretton et al., 2005]
to measure the
(in)dependence of two random variables X and Y.
Deﬁnition 1 (HSIC) Consider n independent observations
drawn from pxy, Z := {(x1, y1), . . . , (xn, yn)} ⊆ X × Y,
an empirical estimator of HSIC(Z, F, G), is given by:
HSIC(Z, F, G) = (n − 1)−2 tr(K1HK2H),

(1)

where K1 and K2 are the Gram matrices with k1,ij =
k1(xi, xj), k2,ij = k2(yi, yj). k1(xi, xj) and k2(yi, yj) are
the kernel functions deﬁned in space F and G, respectively.
H = I−n−111T centers the Gram matrix to have zero mean.
Note that, according to Eq.
(1), to maximize the indepen-
dence between two random variables X and Y, the empirical
estimate of HSIC, i.e., tr(K1HK2H) should be minimized.

3 Problem Formulation
Inspired by the recent works [Liu et al., 2016; Wen et al.,
2016; Zhang et al., 2016], which argue that the original soft-
max loss is inadequate due to its non-discriminative features.
They either reformulate the softmax function into a new de-
sired one (e.g., L-softmax [Liu et al., 2016] etc.) or add ad-
ditional constraints to reﬁne the original softmax loss (e.g.,
contrastive loss [Sun et al., 2014] and center loss [Wen et al.,
2016] etc.). Here, we follow this argument but cast a new
viewpoint on the weakness, say the extracted features are not
discriminative meanwhile the learned classiﬁer is not strong.

3.1 Soft-Margin Softmax Function
To enhance the discriminability of CNN features, we design
a new soft-margin softmax function to enlarge the margin
between different classes. We ﬁrst give a simple example
to describe our intuition. Consider the binary classiﬁcation
and we have a sample x from class 1. The original softmax
loss is to enforce wT
2 x ( i.e., (cid:107)w1(cid:107)(cid:107)x(cid:107) cos(θ1) >
(cid:107)w2(cid:107)(cid:107)x(cid:107) cos(θ2)) to classify x correctly. To make this ob-
jective more rigorous, the work L-Softmax [Liu et al., 2016]
introduced an angular margin:

1 x > wT

(cid:107)w1(cid:107)(cid:107)x(cid:107) cos(θ1) ≥ (cid:107)w1(cid:107)(cid:107)x(cid:107) cos(mθ1) > (cid:107)w2(cid:107)(cid:107)x(cid:107) cos(θ2),

(2)
and used the intermediate value (cid:107)w1(cid:107)(cid:107)x(cid:107) cos(mθ1) to re-
place the original (cid:107)w1(cid:107)(cid:107)x(cid:107) cos(θ1) during training. In that
way, the discrmination between class 1 and class 2 is ex-
plicitly emphasized. However, to make cos(mθ1) derivable,
m should be a positive integer.
In other words, the angu-
lar margin cannot go through all possible angles and is a
hard one. Moreover, the forward and backward computa-
tion are complex due to the angular margin involved. To ad-
dress these issues, inspired by the works [Sun et al., 2014;
Liang et al., 2017; Bell and Bala., 2015], we here introduce a
soft distance margin and simply let

wT

1 x ≥ wT

1 x − m > wT

2 x,

(3)

where m is a non-negative real number and is a distance
margin. In training, we employ wT
1 x,

1 x − m to replace wT

thus our multi-class soft-margin softmax function can be de-

ﬁned as: si =

y xi−m

ewT
y xi−m+(cid:80)K

ewT

k(cid:54)=y ewT

k

xi

. Consequently, the soft-

Margin Softmax (M-Softmax) loss is formulated as:

For simplicity, we adopt the inner product kernel for the pro-
posed HSIC, say K = WT W for both Wv and Wu. Con-
sidering the multiple ensemble settings and ignoring the scal-
ing factor (K − 1)−2 of HSIC for notational convenience,
leads to the following equation:

LM = − log

ewT
y xi−m
y xi−m + (cid:80)K

ewT

k(cid:54)=y ewT

k xi

.

(4)

V
(cid:88)

HSIC(Wv, Wu) =

tr(KvHKuH)

V
(cid:88)

u=1;u(cid:54)=v

Obviously, when m is set to zero, the designed M-Softmax
loss Eq. (4) becomes identical to the original softmax loss.

3.2 Diversity Regularized Ensemble Strategy

Though learning discriminative features may result in bet-
ter classiﬁer as these two components highly depend on each
other, the classiﬁer may not be strong enough without explic-
itly encouragement. To learn a strong one, as indicted in [Guo
et al., 2017], a combination of various classiﬁers can improve
predictions. Thus we adopt the ensemble strategy. Prior to
formulating our ensemble strategy, we revisit that the most
popular way to train an ensemble in deep learning is arguably
the dropout [Hinton et al., 2012]. The idea behind dropout is
to train an ensemble of DNNs by randomly dropping the acti-
vations and average the results of the whole ensemble instead
of training a single DNN. However, in the last fully connected
layer of softmax loss, dropout is usually not permitted be-
cause it will lose the useful label information, especially with
the limited training samples. Therefore, we need to design a
new manner to assemble weak classiﬁers.

1, wv

1 , wu

2, . . . , wv

2 , . . . , wu

Without loss of generality, we take two weak softmax clas-
K] ∈ Rd×K and Wu =
siﬁers Wv = [wv
K] ∈ Rd×K as an example to illustrate the
[wu
main idea. Speciﬁcally, it has been well-recognized that the
diversity of weak classiﬁers is of utmost importance to a
good ensemble [Guo et al., 2017; Li et al., 2012]. Here,
we exploit the diverse/complementary information across dif-
ferent weak classiﬁers by enforcing them to be indepen-
dent. High independence of two weak classiﬁers Wv and
Wu means high diversity of them. Classical independence
criteria like the Spearmans rho and Kendalls tau [Fredricks
and Nelsen., 2007], can only exploit linear dependence.
The recent exclusivity regularized term [Guo et al., 2017;
Wang et al., 2017b] and ensemble pruning [Li et al., 2012]
may be good candidates for classiﬁer ensemble, but both of
them are difﬁcult to differentiate. Therefore, these methods
are not suitable for assembling the weak softmax classiﬁers.
In this paper, we employ the Hilbert-Schmidt Indepen-
dence Criterion (HSIC) to measure the independence (i.e.,
diversity) of weak classiﬁers, mainly for two reasons. One
is that the HSIC measures the dependence by mapping vari-
ables into a Reproducing Kernel Hilbert Space (RKHS), such
that the nonlinear dependence can be addressed. The other
one is that the HSIC is computational efﬁcient. The empir-
ical HSIC in Eq. (1) turns out to be the trace of product of
weak classiﬁers, which can be easily optimized by the typical
SGD. Based on the above analysis, we naturally minimize the
following constraint according to Eq. (1):

HSIC(Wv, Wu) = (K − 1)−2 tr(KvHKuH).

(5)

u=1;u(cid:54)=v

V
(cid:88)

=

u=1;u(cid:54)=v

tr(WvHWT

u WuHWT

v ) = tr(WvKvWT

v ),

u=1;u(cid:54)=v HWT

(6)
where Kv = (cid:80)V
u WuH, H is the centered ma-
trix deﬁned in section 2.2, and HT = H. However, accord-
ing to the formulation Eq. (6), we can see that the HSIC con-
straint is value-aware, the diversity is determined by the value
of weak classiﬁers. If the magnitude of different weak classi-
ﬁers is quite large, the diversity may not be well handled. To
avoid the scale issue, we use the normalized weak classiﬁers
to compute the diversity. In other words, if not speciﬁc, the
weak classiﬁers Wv, where v ∈ {1, 2, . . . , V } are normal-
ized in Eq. (6). Merging the diversity constraint into softmax
loss, leads to Ensemble Softmax (E-Softmax) loss as:
exT
i wv
k=1 exT

+ λ tr(WvKvWT
v )

LE =

− log

V
(cid:88)

(cid:80)K

i wv
k

(cid:105)
,

v=1

= ∂LS
∂Wv

(7)
where λ is a hyperparameter to balance the importance of di-
versity. The backward propagation of Wv is computed as
∂LE
+ λWvKv. Clearly, the update of the weak
∂Wv
classiﬁer Wv is co-determined by the initializations and other
weak classiﬁers (i.e., Kv is computed based on other classi-
ﬁers). This means that the diversity of different weak classi-
ﬁers will be explicitly enhanced.

(cid:104)

y

Since feature extracting and classiﬁer learning is an end-
to-end framework, we prefer to simultaneously harness them.
Now, putting all concerns, say Eqs.
(4) and (6), together
results in our ﬁnal Ensemble soft-Margin Softmax (EM-
Softmax) loss LEM as follows:
exT
i wv
y −m
y −m + (cid:80)K

(cid:105)
+ λ tr(WvKvWT
v )

− log

V
(cid:88)

exT

i wv
k

i wv

(cid:104)

k(cid:54)=y exT

v=1

.

3.3 Optimization
In this part, we show that the proposed EM-Softmax loss is
trainable and can be easily optimized by the typical SGD.
Speciﬁcally, we implement the CNNs using the well-known
Caffe [Jia et al., 2014] library and use the chain rule to com-
pute the partial derivative of each Wv and the feature x as:
∂sv
∂zv

∂LEM
∂Wv

+ λWvKv,

∂zv
∂Wv

=

∂LS
∂sv
V
(cid:88)

v=1

∂LEM
∂x

=

∂LS
∂sv

∂sv
∂zv

∂zv
∂x

,

where the computation forms of ∂LS
∂sv
same as the original softmax loss.

, ∂sv
∂zv

∂zv
∂x , ∂zv
∂Wv

are the

(8)

(9)

4 Experiments
4.1 Dataset Description
MNIST [LeCun et al., 1998b]: The MNIST is a dataset of
handwritten digits (from 0 to 9) composed of 28 × 28 pixel
gray scale images. There are 60, 000 training images and 10,
000 test images. We scaled the pixel values to the [0, 1] range
before inputting to our neural network.
CIFAR10/CIFAR10+ [Krizhevsky and Hinton, 2009]: The
CIFAR10 contains 10 classes, each with 5, 000 training sam-
ples and 1, 000 test samples. We ﬁrst compare EM-Softmax
loss with others under no data augmentation setup. For the
data augmentation, we follow the standard technique in [Lee
et al., 2015; Liu et al., 2016] for training, that is, 4 pixels are
padded on each side, and a 32 × 32 crop is randomly sam-
pled from the padded image or its horizontal ﬂip. In testing,
we only evaluate the single view of the original 32 × 32 im-
age. In addition, before inputting the images to the network,
we subtract the per-pixel mean computed over the training set
from each image.
CIFAR100/CIFAR100+ [Krizhevsky and Hinton, 2009]: We
also evaluate the performance of the proposed EM-Softmax
loss on CIFAR100 dataset. The CIFAR100 dataset is the
same size and format as the CIFAR10 dataset, except it has
100 classes containing 600 images each. There are 500 train-
ing images and 100 testing images per class. For the data
augmentation set CIFAR100+, similarly, we follow the same
technique provided in [Lee et al., 2015; Liu et al., 2016].
ImageNet32 [Chrabaszcz et al., 2017]: The ImageNet32
is a downsampled version of the ImageNet 2012 challenge
dataset, which contains exactly the same number of images
as the original ImageNet, i.e., 1281, 167 training images and
50, 000 validation images for 1, 000 classes. All images are
downsampled to 32 × 32. Similarly, we subtract the per-pixel
mean computed over the downsampled training set from each
image before feeding them into the network.

4.2 Compared Methods
We compare our EM-Softmax loss with recently proposed
state-of-the-art alternatives, including the baseline softmax
loss (Softmax),
the margin-based hinge loss (HingeLoss
[Tang, 2013]), the combination of softmax loss and center
loss (CenterLoss [Wen et al., 2016]), the large-margin soft-
max loss (L-Softmax [Liu et al., 2016]), the angular margin
softmax loss (A-Softmax [Liu et al., 2017a]) and the normal-
ized features softmax loss (N-Softmax [Wang et al., 2017a]).
The source codes of Softmax and HingeLoss are provided in
Caffe community. For other compared methods, their source
codes can be downloaded from the github or from authors’
webpages. For fair comparison, the experimental results are
cropped from the paper [Liu et al., 2016] (indicated as *) or
obtained by trying our best to tune their corresponding hyper-
parameters. Moreover, to verify the gain of our soft margin
and ensemble strategy, we also report the results of the M-
Softmax loss Eq. (4) and the E-Softmax loss Eq. (7).

Implementation Details

4.3
In this section, we give the major implementation details on
the baseline works and training/testing settings. The impor-
tant statistics are provided as follows:

Figure 1: From Left to Right: The effects parameter λ of EM-
Softmax loss on CIFAR10 and CIFAR100, respectively. The results
are obtained by tuning different λ with V = 2.

Baseline works. To verify the universality of EM-Softmax,
we choose the work [Liu et al., 2016] as the baseline. We
strictly follow all experimental settings in [Liu et al., 2016],
including the CNN architectures (LiuNet3), the datasets, the
pre-processing methods and the evaluation criteria.
Training. The proposed EM-Softmax loss is appended af-
ter the feature layer, i.e., the second last inner-product layer.
We start with a learning rate of 0.1, use a weight decay
of 0.0005 and momentum of 0.9. For MNIST, the learn-
ing rate is divided by 10 at 8k and 14k iterations. For CI-
FAR10/CIFAR10+, the learning rate is also divided by 10 at
8k and 14k iterations. For CIFAR100/CIFAR100+, the learn-
ing rate is divided by 10 at 12k and 15k iterations. For all
these three datasets, the training eventually terminates at 20k
iterations. For ImageNet32, the learning rate is divided by 10
at 15k, 25k and 35k iterations, and the maximal iteration is
40k. The accuracy on validation set is reported.
Testing. At testing stage, we simply construct the ﬁnal en-
semble classiﬁer by averaging weak classiﬁers as: W =
1
v=1 Wv. Finally, W is the learned strong classiﬁer and
V
we use it with the discriminative feature x to predict labels.

(cid:80)V

4.4 Accuracy vs HyperParameter
The soft-margin softmax function (4) involves one parameter
m. Inspired by [Bell and Bala., 2015], we try a few differ-
ent m ∈ {0.1, 0.5, 1, 5, 10} and select the one that performs
best. Regarding the diversity regularization (6), it involves
the trade-off parameter λ and the ensemble number V . In this
part, we mainly report the sensitiveness of these two vari-
ables on CIFAR10 and CIFAR100. The subﬁgures of Figure
1 displays the testing accuracy rate vs. parameter λ plot of
EM-Softmax loss. We set V = 2 and vary λ from 0 to 30
to learn different models. From the curves, we can observe
that, as λ grows, the accuracy grows gradually at the very be-
ginning and changes slightly in a relatively large range. The
experimental results are insensitive to λ ∈ [0.01, 1.0]. Too
large λ may hinder the performance because it will degener-
ate the focus of classiﬁcation part in Eq. (8). Moreover, it
also reveals the effectiveness of the diversity regularization
(λ (cid:54)= 0 vs. λ = 0). The subﬁgures of Figure 2 is the testing
accuracy rate vs. ensemble number V plot of EM-Softmax
loss. We set λ = 0.1 and vary V from 1 to 10. From the
curves, we can see that a single classiﬁer (V = 1) is weak for

3The detailed CNNs for each dataset can be found at https:

//github.com/wy1iu/LargeMargin_Softmax_Loss.

Method
HingeLoss [Tang, 2013]
CenterLoss [Wen et al., 2016]
A-Softmax [Liu et al., 2017a]
N-Softmax [Wang et al., 2017a]
L-Softmax [Liu et al., 2016]
Softmax
M-Softmax
E-Softmax
EM-Softmax

MNIST
99.53*
99.41
99.66
99.48
99.69*
99.60*
99.70
99.69
99.73

CIFAR10
90.09*
91.65
91.72
91.46
92.42*
90.95*
92.50
92.38
93.31

CIFAR10+
93.31*
93.82
93.98
93.90
94.08*
93.50*
94.27
93.92
95.02

CIFAR100
67.10*
69.23
70.87
70.49
70.47*
67.26*
70.72
70.34
72.21

CIFAR100+
68.48
70.97
72.23
71.85
71.96
69.15
72.54
71.33
75.69

Compared

Baseline

Our

Table 1: Recognition accuracy rate (%) on MNIST, CIFAR10/CIFAR10+ and CIFAR100/CIFAR100+ datasets.

Method
HingeLoss [Tang, 2013]
CenterLoss [Wen et al., 2016]
A-Softmax [Liu et al., 2017a]
N-Softmax [Wang et al., 2017a]
L-Softmax [Liu et al., 2016]
Softmax
M-Softmax
E-Softmax
EM-Softmax

Top-1 Top-5
71.56
46.52
71.98
47.43
72.51
48.12
72.06
47.52
72.63
47.85
71.94
46.89
72.77
48.21
72.99
48.16
74.22
49.22

Compared

Baseline

Our

Table 2: The top-1 and top-5 recognition accuracy rate (%) on
ImageNet32 validation set.

all possible desired ones. It is much better than Softmax be-
cause of the learned discriminative features. To the ensemble
softmax E-Softmax, it is about 1% higher than the baseline
Softmax because of the assembled strong classiﬁer. Our EM-
Softmax absorbs the complementary merits from these two
aspects (i.e., discriminative features and strong classiﬁer).

On CIFAR100/CIFAR100+ dataset, it can be observed that
most of competitors achieve relatively low performance. The
major reason is that the large variation of subjects, color and
textures and the ﬁne-grained category involve in this dataset.
Even so, our EM-Softmax still reaches signiﬁcant improve-
ments, at least 5% higher than the baseline Softmax. Com-
pared with the recent L-Softmax [Liu et al., 2016] and A-
Softmax [Liu et al., 2017a], our EM-Softmax can achieve
about 3% improvement. Moreover, we can see a similar trend
as that shown on CIFAR10/CIFAR10+ dataset, that is, the
EM-Softmax loss is generally better than its degenerations
M-Softmax and E-Softmax.

4.6 Classiﬁcation Results on ImageNet

We report the top-1 and top-5 validation accuracy rates on Im-
ageNet32 in Table 2. From the numbers, we can see that the
results exhibit the same phenomena that emerged on CIFAR
dataset. In particular, the proposed EM-Softmax achieves a
higher top-1 accuracy by 2.4% and top-5 accuracy by 2.2%
in comparison with the baseline Softmax. The improvements
are signiﬁcant as the imagenet is very large and difﬁcult for
image classiﬁcation, especially with such a smaller down-
sampled size (32×32). Compared with other competitors, our
EM-Softmax can achieve at least 1% improvement. The re-

Figure 2: From left to right: The effects of ensemble number V on
CIFAR10 and CIFAR100, respectively. The results are obtained by
tuning different ensemble number V with λ = 0.1.

classiﬁcation. Our EM-Softmax (V ≥ 2) beneﬁts from the
ensemble number of weak classiﬁers. But the improvement
is slight when the ensemble number V is big enough. The
reason behind this may come from two aspects. One is that
too many classiﬁers ensemble will bring too much redundant
information, thus the improvement is ﬁnite. The other one is
that the discriminative features help to promote weak classi-
ﬁers, without needing assemble too many classiﬁers. Based
on the above observations, we empirically suggest V = 2 in
practice to avoid the parameter explosion of weak classiﬁers.

4.5 Classiﬁcation Results on MNIST and CIFAR
Table 1 provides the quantitative comparison among all the
competitors on MNIST and CIFAR datasets. The bold num-
ber in each column represents the best performance.

On MNIST dataset, it is well-known that this dataset is typ-
ical and easy for deep image classiﬁcation, and all the com-
petitors can achieve over 99% accuracy rate. So the improve-
ment of our EM-Softmax is not quite large. From the experi-
mental results, we observe that A-Softmax [Liu et al., 2017a],
L-Softmax [Liu et al., 2016], the proposed EM-Softmax and
its degenerations M-Softmax and E-Softmax outperform the
other compared methods. Moreover, we have achieved a high
accuracy rate 99.73% on this dataset.

On CIFAR10/CIFAR10+ dataset, we can see that our EM-
Softmax signiﬁcantly boosts the performance, achieving at
least 2% improvement over the baseline Softmax. Consider-
ing all the competitors can achieve over 90% accuracy rate
on this dataset, the improvement is signiﬁcant. To the soft
distance margin M-Softmax, it is slightly better than the hard
angular margin L-Softmax [Liu et al., 2016] and A-Softmax
[Liu et al., 2017a] because the soft margin can go through

Method
LiuNet
Full
AlexNet
LiuNet+Full+Alex
LiuNet(48)
LiuNet(64)
LiuNet(96)
LiuNet(48+64+96)
EM-Softmax

CIFAR10

15.1
0.35
113.9
129.35
9.3
15.1
30.9
55.4
15.2

90.95
83.09
88.76
90.97
90.63
90.95
91.16
91.99
93.31

CIFAR100
15.7
0.71
115.3
131.71
9.4
15.3
31.0
55.7
31.1

67.26
63.32
64.56
68.05
66.08
66.70
67.26
70.01
72.74

Table 3: The comparison of model size (MB) | recognition accuracy
rate (%) of different ensemble strategies.

sults presented in Table 2 also reveal that our EM-Softmax
can beneﬁts from the discriminative features (M-Softmax)
and the strong classiﬁer (E-Softmax).

4.7 EM-Softmax vs. Model Averaging
To validate the superiority of our weak classiﬁers ensem-
ble strategy (i.e., EM-Softmax) over the simple model av-
eraging, we conduct two kinds of model averaging experi-
ments on both CIFAR10 and CIFAR100 datasets. One is
the model averaging of the same architecture but with dif-
ferent numbers of ﬁlters (i.e., 48/48/96/192, 64/64/128/256
and 96/96/192/382)4. For convenience, we use CNN(48),
CNN(64) and CNN(96) to denote them, respectively. The
other one is the model averaging of different CNN architec-
tures. We use AlexNet [Krizhevsky et al., 2012] (much larger
than LiuNet [Liu et al., 2016]) and CIFAR10 Full (much
smaller than LiuNet [Liu et al., 2016]) architectures as an
example, which have been provided in the standard Caffe
[Jia et al., 2014] library5. For comparison, all the architec-
tures of these two kinds of model averaging strategies are
equipped with the original softmax loss. Table 3 provides the
experimental results of model averaging on CIFAR10 and CI-
FAR100 datasets, from which, we can see that the strategy of
model averaging is beneﬁcial to boost the classiﬁcation per-
formance. However, the training is time-consuming and the
model size is large. Compared our weak classiﬁers ensemble
(EM-Softmax) with these two kinds of model averaging, we
can summarize that the accuracy of our EM-Softmax is gen-
eral higher and our model size is much lower than the simple
model averaging.

4.8 EM-Softmax vs. Dropout
Dropout is a popular way to train an ensemble and has been
widely adopted in many works. The idea behind it is to
train an ensemble of DNNs by randomly dropping the ac-
tivations6 and averaging the results of the whole ensemble.
The adopted architecture LiuNet [Liu et al., 2016] contains
the second last FC layer and is without the dropout. To val-
idate the gain of our weak classiﬁers ensemble, we add the

464/64/128/256 denotes the number of ﬁlters in conv0.x,

conv1.x, conv2.x and conv3.x, respectively.

5https://github.com/BVLC/caffe.
6Thus it cannot be applied to softmax classiﬁer.

Method
Softmax
Softmax+Dropout
EM-Softmax
EM-Softmax+Dropout

CIFAR10
90.95
91.06
93.31
93.49

CIFAR100
67.26
68.01
72.74
72.85

Table 4: Recognition accuracy rate (%) vs. Dropout.

Method

Softmax
Softmax+Dropout
LiuNet+Full+AlexNet
LiuNet(48+64+96)
EM-Softmax

Training
(GPU)
0.99h
0.99h
4.82h
3.02h
1.01h

Test
(CPU)
2.5m
2.5m
8.1m
10m
3.1m

Accuracy

90.90
90.96
90.97
91.99
93.31

Table 5: Recognition accuracy rate (%) vs. Running time (h-hours,
m-minutes) on CIFAR10 dataset.

dropout technique to the second last fully-connected layer
and conduct the experiments of Softmax, Softmax+Dropout
and EM-Softmax+Dropout. The proportion of dropout is
tuned in {0.3, 0.5, 0.7} and the diversity hyperparameter λ of
our EM-Softmax is 0.1. Table 4 gives the experimental results
of dropout on CIAFR10 and CIFAR100 datasets. From the
numbers, we can see that the accuracy of our EM-Softmax is
much higher than the Softmax+Dropout, which has shown the
superiority of our weak classiﬁer ensemble over the dropout
strategy. Moreover, we emperically ﬁnd that the improve-
ment of dropout on both Softmax and EM-Softmax losses is
not big with the adopted CNN architecture. To sum up, our
weak classiﬁers ensemble is superior to the simple dropout
strategy and can seamlessly incorporate with it.

4.9 Running Time
We give the time cost vs. accuracy of EM-Softmax, Softmax
and two kinds of model averaging on CIFAR10. From Table
5, the training time on 2 Titan X GPU is about 1.01h, 0.99h,
4.82h and 3.02h, respectively. The testing time on CPU (In-
tel Xeon E5-2660v0@2.20Ghz) is about 3.1m, 2.5m, 8.1m
and 10m, respectively. While the corresponding accuracy is
93.31%, 90.90%, 90.97% and 91.99%, respectively. Consid-
ering time cost, model size and accuracy together, our weak
classiﬁers ensemble EM-Softmax is a good candidate.

5 Conclusion
This paper has proposed a novel ensemble soft-margin soft-
max loss (i.e., EM-Softmax) for deep image classiﬁcation.
The proposed EM-Softmax loss beneﬁts from two aspects.
One is the designed soft-margin softmax function to make the
learned CNN features to be discriminative. The other one is
the ensemble weak classiﬁers to learn a strong classiﬁer. Both
of them can boost the performance. Extensive experiments
on several benchmark datasets have demonstrated the advan-
tages of our EM-Softmax loss over the baseline softmax loss
and the state-of-the-art alternatives. The experiments have
also shown that the proposed weak classiﬁers ensemble is

generally better than model ensemble strategies (e.g., model
averaging and dropout).

[Li et al., 2012] N. Li, Y. Yu, and Z. Zhou. Diversity regu-

larized ensemble pruning. In ECML, 2012.

Acknowledgments
This work was supported by National Key Research and De-
velopment Plan (Grant No.2016YFC0801002), the Chinese
National Natural Science Foundation Projects #61473291,
#61572501, #61502491, #61572536,
the Science and
Technology Development Fund of Macau (No.151/2017/A,
152/2017/A), and AuthenMetric R&D Funds.

References
[Bell and Bala., 2015] S. Bell and K. Bala. Learning vi-
sual similarity for product design with convolutional neu-
ral networks. In TOG, 2015.

[Chrabaszcz et al., 2017] P. Chrabaszcz, L.

and
A downsampled variant of imagenet as
arXiv preprint

H. Frank.
an alternative to the cifar datasets.
arXiv:1707.08819, 2017.

Ilya,

[Fredricks and Nelsen., 2007] G. Fredricks and R. Nelsen.
On the relationship between spearman’s rho and kendall’s
tau for pairs of continuous random variables. Journal of
statistical planning and inference., 2007.

[Goodfellow et al., 2013] I. Goodfellow, D. Warde-Farley,

and M. Mirza. Maxout networks. In ICML, 2013.
[Gretton et al., 2005] A. Gretton, O. Bousquet,

and
Measuring statistical dependence with

A. Smola.
hilbert-schmidt norms. In ICAL, 2005.

[Guo et al., 2017] X. Guo, X. Wang, and H. Ling. Exclusiv-

ity regularized machine. In IJCAI, 2017.

[He et al., 2016] K. He, X. Zhang, and S. Ren. Deep residual

learning for image recognition. In CVPR, 2016.

[Hinton et al., 2012] G. Hinton, N.

and
A. Krizhevsky.
Improving neural networks by pre-
venting co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580., 2012.

Srivastava,

Jia, E. Shelhamer,

[Jia et al., 2014] Y.
J. Donahue,
S. Karayev,
J. Long, R. Girshick, S. Guadarrama,
and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. arXiv preprint arXiv:1408.5093,
2014.

[Krizhevsky and Hinton, 2009] A. Krizhevsky and G. Hin-
ton. Learning multiple layers of features from tiny images.
Technical report, 2009.

[Krizhevsky et al., 2012] A. Krizhevsky, I. Sutskever, and
Imagenet classiﬁcation with deep convolu-

G. Hinton.
tional neural networks. In NIPS, 2012.

[LeCun et al., 1998a] Y. LeCun, L. Bottou, and Y. Bengio.
Gradient-based learning applied to document recognition.
In Proceedings of the IEEE, 1998.

[LeCun et al., 1998b] Y. LeCun, C. Cortes, and C. Burges.

The mnist database of handwritten digits. 1998.

[Lee et al., 2015] Y. Lee, S. Xie, and P. Gallagher. Deeply-

supervised nets. In AISTATS, 2015.

[Li et al., 2016] Y. Li, K. He, and J. Sun. R-fcn: Object de-
tection via region-based fully convolutional networks. In
NIPS, 2016.

[Liang et al., 2017] X. Liang, X. Wang, Z. Lei, S. Liao, and
Stan. Li. Soft-margin softmax for deep classiﬁcation. In
ICONIP, 2017.

[Lin et al., 2013] M. Lin, Q. Chen, and Y. Yan. Network in

network. In ICLR, 2013.

[Liu et al., 2016] W. Liu, Y. Wen, and Z. Yu. Large-margin
softmax loss for convolutional neural networks. In ICML,
2016.

[Liu et al., 2017a] W. Liu, Y. Wen, Z. Yu, M. Li, and
L. Song. Sphereface: Deep hypersphere embedding for
face recognition. In CVPR, 2017.

[Liu et al., 2017b] Y. Liu, H. Li, and X. Wang. Learn-
ing deep features via congenerous cosine loss for person
recognition. arXiv preprint arXiv:1702.06890., 2017.
[Martins and Astudillo., 2016] A. Martins and R. Astudillo.
From softmax to sparsemax: A sparse model of attention
and multi-label classiﬁcation. In ICML, 2016.

[Schroff et al., 2015] F. Schroff, D. Kalenichenko,

and
J. Philbin. Facenet: A uniﬁed embedding for face recog-
nition and clustering. In CVPR, 2015.

[Simonyan and Andrew, 2014] K. Simonyan and Z. Andrew.
Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.

[Sun et al., 2014] Y. Sun, Y. Chen, and X. Wang. Deep learn-
ing face representation by joint identiﬁcation-veriﬁcation.
In NIPS, 2014.

[Tang, 2013] Y. Tang. Deep learning using linear support
vector machines. arXiv preprint arXiv:1306.0239, 2013.
[Wang et al., 2015] X. Wang, X. Guo, and S. Li. Adap-
tively uniﬁed semi-supervised dictionary learning with ac-
tive points. In CVPR, 2015.

[Wang et al., 2017a] F. Wang, X. Xiang, J. Chen, and
A. Yuille. Normface: l2 hypersphere embedding for face
veriﬁcation.. In ACM MM, 2017.

[Wang et al., 2017b] X. Wang, X. Guo, Z. Lei, C. Zhang, and
S. Li. Exclusivity-consistency regularized multi-view sub-
space clustering. In CVPR, 2017.

[Wen et al., 2016] Y. Wen, K. Zhang, and Z. Li. A discrimi-
native feature learning approach for deep face recognition.
In ECCV, 2016.

[Zhang et al., 2016] X. Zhang, Z. Fang, and Y. Wen. Range
arXiv

loss for deep face recognition with long-tail.
preprint arXiv:1611.08976., 2016.

[Zhou et al., 2016] B. Zhou, A. Khosla, and A. Lapedriza.
Learning deep features for discriminative localization. In
CVPR, 2016.

