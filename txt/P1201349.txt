A Study of Enhancement, Augmentation, and Autoencoder Methods
for Domain Adaptation in Distant Speech Recognition

Hao Tang, Wei-Ning Hsu, Franc¸ois Grondin, James Glass

Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{haotang,wnhsu,fgrondin,glass}@mit.edu

8
1
0
2
 
n
u
J
 
3
1
 
 
]
L
C
.
s
c
[
 
 
1
v
1
4
8
4
0
.
6
0
8
1
:
v
i
X
r
a

Abstract

Speech recognizers trained on close-talking speech do not gen-
eralize to distant speech and the word error rate degradation
can be as large as 40% absolute. Most studies focus on tack-
ling distant speech recognition as a separate problem, leaving
little effort to adapting close-talking speech recognizers to dis-
tant speech. In this work, we review several approaches from
a domain adaptation perspective. These approaches, including
speech enhancement, multi-condition training, data augmenta-
tion, and autoencoders, all involve a transformation of the data
between domains. We conduct experiments on the AMI data
set, where these approaches can be realized under the same con-
trolled setting. These approaches lead to different amounts of
improvement under their respective assumptions. The purpose
of this paper is to quantify and characterize the performance
gap between the two domains, setting up the basis for studying
adaptation of speech recognizers from close-talking speech to
distant speech. Our results also have implications for improv-
ing distant speech recognition.
Index Terms: distant speech recognition, speech enhancement,
multi-condition training, data augmentation, variational autoen-
coders

1. Introduction

Domain adaptation refers to the task of adapting models trained
on one domain to other domains. In the general setting, models
are trained in the source domain and tested on the target do-
main. The source domain may or may not have overlaps with
the target domain. The mismatch between the training and the
test conditions causes the task performance to deteriorate, be-
cause generalization guarantees rely on the assumption that the
training and test samples come from the same underlying dis-
tribution. Domain adaption in the most general case is possible
under some assumptions, but deemed challenging [1, 2].

Domain adaptation for speech recognition is particularly
difﬁcult considering the mismatch in speakers, speaking styles,
noise types, and room acoustics etc. There has been signiﬁcant
success in dealing with speaker mismatch, for example, adapt-
ing a speaker-independent model to a known speaker or even
adapting to an unknown speaker [3, 4]. Developing speech rec-
ognizers that are robust to many noise types is more challeng-
ing, and in theory it is impossible to have a model that is ro-
bust to any adversarial noise [5]. It is, however, still possible to
design speech recognizers that are robust to natural noise types
that occur in our daily lives. Signiﬁcant progress has been made
in this direction, especially when the noise types are known
at training time, for example, with speech enhancement tech-
niques or multi-condition training [6, 7, 8].

This paper focuses on the task of adapting speech recog-
nizers trained on close-talking speech to distant speech. Distant
speech recognition is itself a difﬁcult task [9]. The difﬁculty
is often attributed to reverberation, i.e., weaker copies of the
original speech signals. Early reverberation is considered easy
to handle, because convolving shifted impulses in the time do-
main is nothing but a constant scaling function on the power
spectrum. Late reverberation, on the other hand, is not limited
to single short-term spectra and cannot be approximated well
with shifted impulses. As a result, the speech is corrupted with
a type of noise that is highly correlated with the speech from the
past. Important effort has been devoted to training models di-
rectly on distant speech [10]. Other solutions for distant speech
recognition include using multiple microphones [11, 12, 13],
speech enhancement techniques [14, 8], and data augmentation
[15, 16]. It is also unclear if the degradation in performance
is really due to reverberation and not due to other causes, such
as the difference in gain levels. We investigate this by training
models on data augmented with simulated reverberation.

There has been some work in adapting speech recognizers
to distant speech [10, 17, 18, 19]. However, different studies
use different settings, for example, whether it is allowable to
use parallel data to train models, or whether we have access to
labels in the other domain. In this paper, we consider various
settings, their requirements, and the performance of speech rec-
ognizers of a ﬁxed architecture. The purpose of this paper is
to quantify and characterize the gap of these settings, and set
up the basis for studying domain adaptation for distant speech
recognition. Note that we do not consider the online adaptive
setting, a common scenario for speaker adaptation [3, 4, 20, 21],
where we have a small amount of labeled data to adapt to the
target domain.

2. Domain Adaptation

In this section, we summarize the approaches and their re-
quirements for adapting speech recognizers from close-talking
speech to distant speech.
In the general setting, let the input
space be X and the output space be Y. For speech recognition,
X is the set of sequences of log Mel ﬁlterbank feature vectors,
and Y is the set of word sequences. We have two unknown data
distributions D1 and D2 over X × Y representing the source
and the target domain. In the following discussion, we refer to
close-talking speech as the source domain and distant speech as
the target domain.

2.1. Speech enhancement

To reduce the mismatch between domains, a simple approach
is to transform data from the target domain to the source do-

main where the recognizer is trained. We assume there is an un-
known distortion function C : X → X such that C(x) ∼ D2
for x ∼ D1. The goal is to ﬁnd a function T : X → X such
that T (C(x)) ≈ x for x ∼ D1. For speech processing, trans-
forming noisy speech to clean speech is referred to as speech
enhancement.

In general, speech enhancement has a broader goal: trans-
forming signals so that the speech stands out and becomes more
audible. This typically involves removing noise (though some-
times adding noise can improve intelligibility [22]). We focus
on the limited sense of enhancement, making speech closer to
its clean counterpart while ignoring intelligibility. We assume
we have access to a parallel data set {(x1, ˜x1), . . . , (xn, ˜xn)}
where ˜xi = C(xi) for i = 1, . . . , n. The objective is to approx-
imate the clean speech xi given the noisy speech ˜xi by minimiz-
ing the Euclidean distance kxi − T (˜xi)k2
2 for i = 1, . . . , n.
Minimizing this objective for speech enhancement was ﬁrst
proposed in [23] and is explored in the context of neural net-
works in [24]. Deep neural networks are particularly suitable
for speech enhancement without posing any assumptions on the
noise types. Modern treatments with deep networks are studied
in [25, 6, 26, 8].

Once a model for speech enhancement is trained, we en-
hance the speech signal prior to doing speech recognition, i.e.,
using T (˜x) instead of ˜x as the input to the speech recognizer.
Training speech enhancement models requires parallel data in
both domains, which makes data collection costly. However,
this approach does not need transcripts for the parallel data.
Speech recognizers trained on the source domain can also be
reused without additional training.

2.2. Multi-condition training

1 and S2 ∼ Dm

Another simple approach to reduce the mismatch between do-
mains is to use the data from the target domain during training.
Suppose we have S1 ∼ Dn
2 where n and m are
the numbers of samples for the two data sets. Models are simply
on the data set S1 ∪ S2. If the performance on the target domain
is the only concern, we can always discard S1 and train models
only on S2. For noise-robust speech recognition, training mod-
els on different noise conditions is referred to as multi-condition
training or multi-style training. Multi-condition training can be
traced back to [27], and has been shown to reduce mismatch
for different noise conditions [28]. Deep neural networks work
particularly well with multi-condition training due to the large
model capacity [7, 10].

Multi-condition training requires labeled data in both do-
mains, so data collection can be costly. Additional training,
either from scratch or from a pre-trained model, is required.
When the model capacity is large enough, a single model is able
to cover multiple domains. However, the training time scales
linearly with the amount of data.

2.3. Data augmentation

As a special case of multi-condition training, data augmenta-
tion transforms data from the source domain to the target do-
main (i.e., the opposite of speech enhancement). This typically
involves corrupting the clean data with different noise types or
transforming the clean data with simulators, such as convolving
the clean speech with room impulse responses. Formally, we
assume we have a generator distribution G(ˆx|x). Let G(S) =
{(ˆx1, y1), . . . , (ˆxn, yn)} where ˆxi ∼ G(ˆx|xi) for i = 1, . . . , n
and some data set S = {(x1, y1), . . . , (xn, yn)} ∼ Dn
1 . We
train models on the data set S ∪ G(S). This approach is ex-

pected to work well if the generator is able to the match the tar-
get domain, i.e., for x ∼ D1 and ˆx ∼ G(ˆx|x), either ˆx ∼ D2
or ˆx ≈ C(x) for an unknown distortion function C such that
C(x) ∼ D2. Data augmentation was originally designed as
a regularization technique for learning transformation invariant
features, and has been successful in image classiﬁcation tasks
with convolutional neural networks [29, 30, 31]. Data augmen-
tation has been applied to speech recognition in [32, 16, 15].

Data augmentation is suitable when the simulation of noise
or other factors is simple, for example, perturbing vocal tract
lengths [32], perturbing speed [16], and simulating reverbera-
tion [15]. Data from the target domain is not required. However,
the training time scales linearly with the amount of generated
data.

2.4. Unsupervised domain adaptation with autoencoders

Finding similarities between the target and the source domains
is yet another way to tackle domain mismatch. For example,
we assume a common distribution for linguistic content, such
as English utterances. The source and the target domain can
still have their own nuisance factors depending on speakers and
channels. Each domain can be modeled as a generative pro-
cess where an utterance is ﬁrst sampled from the shared dis-
tribution and is transformed according to the nuisance factors.
Since the two domains are symmetric, we describe the process
in one domain α; the other domain follows the same generative
story. For example, we can have α ∈ {0, 1} where 0 denotes
the source domain and 1 denotes the target domain. Suppose
an utterance from domain α has K segments s1, . . . , sK . Each
segment sk is generated by a domain-independent vector z1
k and
a domain-dependent vector z2
k,α. The domain-independent vec-
tor z1
k ∼ D encodes the linguistic content where D is the shared
distribution for all domains, while the domain-independent vec-
tor z2
k,α ∼ Dα encodes the nuisance factors, such as speakers
and channels, speciﬁc to domain α. The segment sk is then
generated from a function that depends on z1

k and z2

k,α

We use factorized hierarchical variational autoencoders
(FHVAE) [33] to model the above generative process with two
inference networks q(z2|x), q(z1|x, z2). Without any further
constraints, z1 and z2 are fully exchangeable. To make sure
z2 captures the nuisance factors, we constrain the z2’s from the
same utterance to be similar while leaving z1 unconstrained,
because the nuisance factors largely remain unchanged within
the same utterance. In addition, there is a loss enforcing z2 to
be predictive of the utterance identity.

After training the FHVAE on all data combined, we use
the inference network to obtain the vectors that encode the lin-
guistic contents and discard the vectors for the nuisance factors.
Speech recognizers are trained on these new set of features.
This approach does not require parallel data from both domains,
and the data does not need to be labeled. However, tuning FH-
VAEs might be difﬁcult. If the model has too many parameters
for reconstruction, we might obtain a trivial identity function.
If the weight between reconstruction and the KL-divergence is
tuned, we do not have a ﬁxed objective to compare different
FHVAEs.

3. Experiments

In order to have a fair comparison for all the settings, we con-
duct experiments on the AMI data set, where parallel record-
ings and labels are available for both the close-talking and the
distant speech domains. The AMI data set is a meeting cor-

pus with around 100 hours of conversational non-native English
speech. The meetings are recorded in a controlled environment
with independent headset microphones (IHM) on each speaker
and multiple distant microphones. The audio streams from dif-
ferent microphones are aligned with beamforming. We take the
aligned recordings from the IHMs and one speciﬁc distant mi-
crophone, referred to as the single distant microphone (SDM),
for our experiments. To avoid excessively querying the standard
test set, we do not report numbers on the standard test set. In-
stead, we use 90% of the training set for training, leave 10% for
development, such as step size tuning and early stopping, and
only report word error rates (WERs) on the standard develop-
ment set.

Following [34], we use 80-dimensional

log Mel ﬁl-
terbank features, and train two speaker-adaptive hidden
Markov models (HMM), one for IHM and one for SDM.
We obtain forced alignments of the tied HMM states (also
known as pdf-ids) for both IHM and SDM recordings with
the corresponding systems, and use the pdf-ids as targets
time-
for acoustic model
delay neural networks (TDNNs) with 1000 hidden hidden
units per layer as our acoustic models.
Following [35],
the context sizes of the TDNNs from layer one to seven are
[−1, 0, 1], [−1, 0, 1], [−1, 0, 1], [−3, 0, 3], [−3, 0, 3], [−3, 0, 3],
where [i, 0, k] indicates the summation of hidden vectors at
time t + i, t, and t + k. Formally, to compute the hidden layer
hℓ from hℓ−1 with context [i, 0, k], we have

training. We use eight-layer

˜ht = Wℓhℓ−1,t + bℓ
hℓ,t = ReLU(˜ht+i + ˜ht + ˜ht+k)

(1)

(2)

Note that in contrast to the standard recipe, we only use the 80-
dimensional log Mel features as input without appending the
i-vectors.

3.1. Baseline and multi-condition training

Since we are interested in adapting models trained on close-
talking speech to distant speech, we train two TDNNs, one on
IHM and one on SDM, and test them on utterances from both
IHM and SDM. We use stochastic gradient descent (SGD) with
a ﬁxed step size 0.025 and a mini-batch size of 1 utterance to
optimize the cross entropy for 20 epochs. Gradients are clipped
to norm 5. We choose the best performing model from the 20
epochs based on the frame error rates, and train it for another
5 epochs with step size 0.025 × 0.75 decayed by 0.75 after
each epoch. Results are shown in Table 1. The WER increases
from 27.4% to 70.3% when using a close-talking model on dis-
tant speech. Note that in the SDM setting, the WER of IHM is
lower than the that of SDM, consistent with the results reported
in [17, 36]. We also conﬁrm the improvement reported in [36],
training models on SDM data while using IHM alignments. For
consistency, we use IHM alignments for the rest of the experi-
ments.

For multi-condition training, we train and tune TDNNs on
both IHM and SDM combined. The training procedure remains
the same except that we use a smaller initial step size 0.01. Re-
sults are shown in Table 1. The TDNN is able to match the
results on both domains.

Table 1: WERs (%) for models trained and tested on various
domains.

train
IHM
IHM
SDM
SDM
SDM (IHM alignments)
SDM (IHM alignments)
IHM + SDM
IHM + SDM

target
IHM 27.4
SDM 70.3
IHM 41.8
SDM 49.7
IHM 39.2
SDM 46.6
IHM 27.2
SDM 45.3

Table 2: WERs (%) for models trained with data augmentation
and tested on various domains, where IHM-r denote the domain
with data corrupted with simulated reverberation.

train
IHM
IHM + IHM-r
IHM
IHM + IHM-r
IHM
IHM + IHM-r

target
27.4
IHM
28.7
IHM
59.3
IHM-r
IHM-r
43.7
SDM 70.3
SDM 63.3

ferent rectangular room sizes, speaker positions, and micro-
phone positions, as proposed in [15]. Three sets of rooms (S1,
S2, and S3) are generated by uniformly sampling the width Lx,
length Ly and height Lz (in meters) in set-wise ranges (where
U(a, b) stands for a uniform distribution between a and b):

S1 : Lx ∼ U(1, 10), Ly ∼ U(1, 10), Lz ∼ U(2, 5)
S2 : Lx ∼ U(10, 30), Ly ∼ U(10, 30), Lz ∼ U(2, 5)
S3 : Lx ∼ U(30, 50), Ly ∼ U(30, 50), Lz ∼ U(2, 5)

For each room, the speed of sound is set to 343 m/sec, and the
wall, ceiling and ﬂoor reﬂection coefﬁcient is sampled from a
uniform distribution between 0.2 and 0.8. For each set, 200
rooms are sampled, 100 RIRs are obtained for both the source
and microphone positioned randomly in the room, leading to a
total of 60,000 simulated RIRs for all sets. For each utterance
in the dataset, one of these RIRs is selected randomly and con-
volved with the clean speech.

We train TDNNs on IHM and IHM corrupted with sim-
ulated reverberation the same way we train multi-conditioned
models in the previous section. We test the TDNNs on the IHM
data corrupted with reverberation and SDM. Results are shown
in Table 2. The degradation due to reverberation is not as severe
compared to that of SDM. Training TDNNs with the additional
data does help generalize to the SDM domain. However, the
improvement is far from closing the gap, suggesting that re-
verberation might not be the major cause of the performance
degradation.

3.3. Speech enhancement

3.2. Data augmentation with simulated reverberation

To investigate the impact of reverberation on distant speech
recognition, we use the image method described in [37] to cre-
ate a set of simulated room impulse responses (RIRs) with dif-

For speech enhancement, we use the same TDNN architecture
without the ﬁnal softmax. TDNNs are trained to predict the
features of IHM utterances given the corresponding features of
SDM utterances, while being an identity function given features
of IHM utterances. The training set in this case is the IHM and

Table 3: WERs (%) for models trained on IHM and tested on
various domains, where IHM-e and SDM-e denote the domains
with enhanced data and IHM-dr, IHM-r-dr, and SDM-dr denote
the domains with dereveberated data.

Table 4: WERs (%) for models trained on hidden vectors pro-
duced by an FHVAE, where the rows with µ1 use the mean of
shared distribution and the rows with log σ1 use the log vari-
ance of the shared distribution as features.

target
train
IHM IHM
IHM IHM-e
IHM SDM
IHM SDM-e
IHM IHM-dr
IHM IHM-r-dr
IHM SDM-dr

27.4
27.8
70.3
54.2
27.6
53.1
70.0

SDM combined. The mean squared error is minimized using the
same training procedure with an initial step size of 0.01. After
training the enhancement model, we take the baseline model
trained on IHM and test it on the enhanced data. Results are
shown in Table 3. The WER on the enhanced SDM (SDM-e) is
signiﬁcantly reduced from 70.3% to 54.2%, while maintaining
the WER on the IHM domain.

Again, to investigate how reverberation plays a role in dis-
tant speech recognition, we train a dereveberation TDNN on the
IHM data corrupted with reverberation while being an identity
function on the clean IHM data. We then evaluate the baseline
TDNN trained on IHM with the dereveberated data. Results are
shown in Table 3. We see some amount of improvement from
59.3% (in Table 2) to 53.1%, suggesting that the TDNN is able
to perform blind dereverberation. However, the improvement is
not as large as the multi-condition TDNN, suggesting that blind
dereverberation is in itself a challenging task. We also evaluate
the dereverberation model on SDM, and ﬁnd no improvement
over the baseline. This again suggests that the domain mismatch
between IHM and SDM might not be due to reverberation but
some other types of mismatch.

3.4. Unsupervised domain adaptation with FHVAEs

For unsupervised domain adaptation, we train a FHVAE
by minimizing the discriminative segmental variational lower
bound [33] with a factor α = 10 for the utterance discrimi-
native loss. The FHVAE consists of two encoders and one de-
coder. One encoder is for the shared distribution (representing
linguistic content) and the other is for the domain-speciﬁc dis-
tribution (representing nuisance factors). The decoder takes the
output vectors from both encoders and reconstructs the input
features. Inputs to an FHVAE are 20 frames of 80-dimensional
log Mel features. Both encoders are LSTM networks [38] with
256 memory cells that process one frame at each step, followed
by an afﬁne transform layer that takes the LSTM output from
the last step and predicts the posterior mean and log variance of
the corresponding latent variables. We use an LSTM decoder
with 256 memory cells, where the LSTM output from each step
is passed to an afﬁne transform layer to predict the mean and
variance of a frame. The Adam [39] optimizer is used with
−8, and initial learning rate of
β1 = 0.95, β2 = 0.999, ǫ = 10
−3. Early stopping is done by monitoring the evidence lower
10
bound on the development set.

After the FAVAE is trained, we use the encoder for the
shared distribution to produce features. A feature vector is gen-
erated at each time point by taking a 20-frame segment cen-
tered at the current time point and feeding it forward into the

train
IHM
IHM
IHM-µ1
IHM-µ1
IHM-(µ1, log σ1)
IHM-(µ1, log σ1)

target
IHM
SDM
IHM-µ1
SDM-µ1
IHM-(µ1, log σ1)
SDM-(µ1, log σ1)

27.4
70.3
31.8
61.8
30.3
72.9

encoder. Following [40], since the generated feature sequence
is 19 frame shorter, we repeat the ﬁrst and the last feature vec-
tor at each end to match the original length. The hidden vectors
are then normalized by subtracting the mean and dividing by
the standard deviation computed over the training set. TDNNs
are trained on the produced feature vectors with the same train-
ing procedure as in previous sections. Since the distribution is
modeled as a Gaussian, we use the Gaussian mean vectors and
have the option to include the log-variance vectors as features.
Results are shown in Table 4. While there is a small amount of
degradation in the IHM domain, we see an improvement from
70.3% to 61.8% in the SDM domain. This suggests that the
SDM features produced by the FHVAE are closer to IHM in
the latent space. The improvement is even larger than data aug-
mentation with simulated reverberation. However, we ﬁnd that
including the log-variance as features might not help adapting
to the target domain. This needs further investigation.

4. Conclusion

In this work, we review several approaches, including speech
enhancement, data augmentation, and autoencoders, to bridge
the gap from close-talking speech recognition to distant speech
recognition from a domain adaptation perspective. We ﬁnd that
all approaches are able to produce models that are more robust
than the baseline. Multi-condition training gives the best re-
sults among all approaches, but it also has the most stringent
requirement, requiring labeled data in all domains. Speech en-
hancement comes second but also has a stringent requirement,
requiring parallel unlabeled data. Data augmentation has the
potential to match the performance of multi-condition training.
However, it requires the data generation process to cover the
condition of the target domain. Unsupervised domain adap-
tation with autoencoders is promising, achieving better results
than data augmentation with simulated reverberation while only
requiring independent unlabeled data from both domains. Fi-
nally, the results suggest that the mismatch between IHM and
SDM in the AMI data set is probably less about reverberation
and has some other factors, such as cross talking [36], that need
to be studied further.

5. References
[1] S. Den-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and
J. W. Vaughan, “A theory of learning from different domains,”
Machine Learning, vol. 79, 2010.

[2] Y. Ganin, E. Ustinova, H. Ajakan, P. Germanin, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky, “Domain-
training of neural networks,” Journal of Machine
adversarial
Learning, vol. 17, 2016.

[3] J.-L. Gauvain and C.-H. Lee, “Maximum a posteriori estima-
tion for multivariate Gaussian mixture observations of Markov
chains,” IEEE Transactions on Acoustics, Speech, and Signal Pro-
cessing, 1994.

[4] C. J. Leggetter and P. C. Woodland, “Maximum likelihood lin-
ear regression for speaker adaptation of continuous density hidden
Markov models,” Computer Speech & Language, vol. 9, 1995.

[5] S. Ben-David, T. Luu, T. Lu, and D. P´al, “Impossibility theorems
for domain adaptation,” in International Conference on Artiﬁcial
Intelligence and Statistics, 2010.

[6] M. L. Seltzer, D. Yu, and Y. Wang, “An investigation of deep neu-
ral networks for noise robust speech recognition,” in International
Conference on Acoustics, Speech and Signal Processing, 2013.

[7] T. Yoshioka, A. Sehr, M. Delcroix, K. Kinoshita, R. Maas,
T. Nakatani, and W. Kellermann, “Making machines understand
us in reverberant rooms,” IEEE Signal Processing Letter, 2012.

[8] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. L. Roux,
J. R. Hershey, and B. Schuller, “Speech enhancement with LSTM
recurrent neural networks and its application to noise-robust
ASR,” in International Conference on Latent Variable Analysis
and Single Separation, 2015.

[9] M. W¨olfel and J. McDonough, Distant speech recognition.

John

Wiley & Sons, 2009.

[10] P. Swietojanski, A. Ghoshal, and S. Renals, “Hybrid acous-
tic models for distant and multichannel large vocabulary speech
recognition,” in IEEE Workshop on Automatic Speech Recogni-
tion and Understanding, 2013.

[11] J. Du, Y.-H. Tu, L. Sun, F. Ma, H.-K. Wang, J. Pan, C. Liu, J.-D.
Chen, and C.-H. Lee, “The USTC-iFlytek system for CHiME-4
challenge,” The 4th CHiME Speech Separation and Recognition
Challenge Workshop, pp. 36–38, 2016.

[12] L. D. Jahn Heymann and R. Haeb-Umbach, “Wide residual
BLSTM network with discriminative speaker adaptation for ro-
bust speech recognition,” in The 4th CHiME Speech Separation
and Recognition Challenge Workshop, 2016.

[13] H. Erdogan, T. Hayashi, J. R. Hershey, T. Hori, C. Hori, W.-N.
Hsu, S. Kim, J. L. Roux, Z. Meng, and S. Watanabe, “Multi-
channel speech recognition: LSTMs all the way through,” in The
4th CHiME Speech Separation and Recognition Challenge Work-
shop, 2016.

[14] J. Du, Q. Wang, T. Gao, Y. Xu, L. Dai, and C.-H. Lee, “Robust
speech recognition with speech enhanced deep neural networks,”
in Interspeech, 2014.

[15] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in International Conference on Acoustics,
Speech and Signal Processing, 2017.

[16] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, “Audio augmen-

tation for speech recogntion,” in Interspeech, 2015.

[17] I. Himawan, P. Motlicek, D. Imseng, B. Potard, N. Kim, and
J. Lee, “Learning feature mapping using deep neural network bot-
tleneck features for distant large vocabulary speech recognition,”
in International Conference on Acoustics, Speech and Signal Pro-
cessing, 2015.

[18] Y. Qian, T. Tan, and D. Yu, “An investigation into using parallel
data for far-ﬁeld speech recognition,” in International Conference
on Acoustics, Speech and Signal Processing, 2016.

[19] Y. Qian, T. Tan, D. Yu, and Y. Zhang, “Integrated adaptation with
multi-factor joint-learning for far-ﬁeld speech recognition,” in In-
ternational Conference on Acoustics, Speech and Signal Process-
ing, 2016.

[20] J. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes,
S. Renals, and T. Robinson, “Speaker-adaptation for hybrid
HMM-ANN continuous speech recognition system,” in Euro-
pean Conference on Speech Communication and Technology (EU-
ROSPEECH), 1995.

[21] B. Li and K. C. Sim, “Comparison of discriminative input
and output transformations for speaker adaptation in the hybrid
NN/HMM systems,” in Interspeech, 2010.

[22] R. M. Warren, K. R. Hainsworth, B. S. Brubaker, J. A. Bashford,
and E. W. Healy, “Spectral restoration of speech: intelligibility is
increased by inserting noise in spectral gaps,” Perception & Psy-
chophysics, vol. 59, 1997.

[23] Y. Ephraim and D. Malah, “Speech enhancement using a mini-
mum mean-square error short-time spectral amplitude estimator,”
IEEE Transactions on Acoustics, Speech, and Signal Processing,
vol. 32, 1984.

[24] E. A. Wan and A. T. Nelson, “Networks for speech enhancement,”
in Handbook of Neural Networks for Speech Processing. Artech
House, 1998.

[25] A. L. Maas, Q. V. Le, T. M. O’Neil, O. Vinyals, P. Nguyen, and
A. Y. Ng, “Recurrent neural networks for noise reduction in robust
ASR,” in Interspeech, 2012.

[26] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, “An experimental study
on speech enhancement based on deep neural networks,” IEEE
Signal Processing Letter, vol. 21, 2014.

[27] B. B. Paul and E. A. Martin, “Speaker stress-resistant continu-
ous speech recognition,” in International Conference on Acous-
tics, Speech and Signal Processing, 1988.

[28] H. Hermansky, D. P. Ellis, and S. Sharma, “Tandem connectionist
feature extraction for conventional HMM systems,” in Interna-
tional Conference on Acoustics, Speech and Signal Processing,
2000.

[29] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the
IEEE, 1998.

[30] P. Y. Simard, D. Steinkraus, and J. C. Platt, “Best practices for
convolutional neural networks applied to visual document anal-
ysis,” in International Conference on Document Analysis and
Recognition, 2003.

[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-
ﬁcation with deep convolutional neural networks,” in Advances in
Neural Information Processing Systems, 2012.

[32] N. Jaitly and G. E. Hinton, “Vocal tract

length perturbation
(VTLP) improves speech recognition,” in International Confer-
ence on Machine Learning, 2013.

[33] W.-N. Hsu, Y. Zhang, and J. Glass, “Unsupervised learning of dis-
entangled and interpretable representations from sequential data,”
in Advances in Neural Information Processing Systems, 2017.

[34] Y. Zhang, G. Chen, D. Yu, K. Yao, S. Khudanpur, and J. Glass,
“Highway long short-term memory RNNs for distant speech
recognition,” in International Conference on Acoustics, Speech
and Signal Processing, 2016.

[35] V. Peddinti, Y. Wang, D. Povey, and S. Khudanpur, “Low latency
acoustic modeling using temporal convolution and LSTMs,” IEEE
Signal Processing Letters, vol. 25, 2018.

[36] V. Peddinti, V. Manohar, Y. Wang, D. Povey, and S. Khudanpur,
“Far-ﬁeld ASR without parallel data,” in Interspeech, 2016.

[37] J. B. Allen and D. A. Berkley, “Image method for efﬁciently sim-
ulating small-room acoustics,” The Journal of the Acoustical So-
ciety of America, vol. 65, no. 4, pp. 943–950, 1979.

[38] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”

Neural computation, 1997.

[39] D. P. Kingma and J. Ba, “Adam: A method for stochastic op-
timization,” in Proceedings of the International Conference on
Learning Representations.

[40] W.-N. Hsu and J. Glass, “Extracting domain invariant features by
unsupervised learning for robust automatic speech recognition,”
in International Conference on Acoustics, Speech and Signal Pro-
cessing, 2018.

