Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling

Alexander Richard, Hilde Kuehne, Juergen Gall
University of Bonn, Germany
{richard,kuehne,gall}@iai.uni-bonn.de

7
1
0
2
 
t
c
O
 
9
 
 
]

V
C
.
s
c
[
 
 
3
v
2
3
1
8
0
.
3
0
7
1
:
v
i
X
r
a

Abstract

We present an approach for weakly supervised learning
of human actions. Given a set of videos and an ordered list
of the occurring actions, the goal is to infer start and end
frames of the related action classes within the video and
to train the respective action classiﬁers without any need
for hand labeled frame boundaries. To address this task,
we propose a combination of a discriminative representa-
tion of subactions, modeled by a recurrent neural network,
and a coarse probabilistic model to allow for a temporal
alignment and inference over long sequences. While this
system alone already generates good results, we show that
the performance can be further improved by approximating
the number of subactions to the characteristics of the dif-
ferent action classes. To this end, we adapt the number of
subaction classes by iterating realignment and reestimation
during training. The proposed system is evaluated on two
benchmark datasets, the Breakfast and the Hollywood ex-
tended dataset, showing a competitive performance on var-
ious weak learning tasks such as temporal action segmen-
tation and action alignment.

1. Introduction

Given the large amount of available video data, e.g. on
Youtube, from movies or even in the context of surveillance,
methods to automatically ﬁnd and classify human actions
within these videos gained an increased interest within the
last years [30, 12, 26, 23, 34].

While there are several successful methods to classify
trimmed video clips [30, 26], temporal localization and
classiﬁcation of human actions in untrimmed, long video
sequences are still a huge challenge. Most existing ap-
proaches in this ﬁeld rely on fully annotated video data, i.e.
the exact start and end time of each action in the training set
needs to be provided [24, 23, 34]. For real world applica-
tions, this requires an enormous effort of creating training
data and can be too expensive to realize. Therefore, weakly
supervised methods are of particular interest. Such methods
usually assume that only an ordered list of actions occurring

Figure 1. Overview of the proposed weak learning system. Given
a list of ordered actions for each video, an initial segmentation is
generated by uniform segmentation. Based on this input informa-
tion we iteratively train an RNN-based ﬁne-to-coarse system to
align the frames to the respective action.

in the video is annotated instead of exact framewise start
and end points [7, 3, 9]. This information is much easier
to generate for human annotators, or can even be automati-
cally derived from scripts [17, 20] or subtitles [1]. The idea
that all those approaches share is that - given a set of videos
and a respective list of the actions that occur in the video -
it is possible to learn the characteristics of the related action
classes, to infer their start and end frames within the video,
and to build the corresponding action models without any
need for hand labeled frame boundaries (see Figure 1).

In this work, we address the task of weak learning of
human actions by a ﬁne-to-coarse model. On the ﬁne
grained level, we use a discriminative representation of sub-
actions, modeled by a recurrent neural network as e.g. used
by [6, 35, 27, 33].
In our case, the RNN is used as ba-
sic recognition model as it provides robust classiﬁcation of
small temporal chunks. This allows to capture local tem-
poral information. The RNN is supplemented by a coarse
probabilistic model to allow for temporal alignment and in-
ference over long sequences.

Further, to bypass the difﬁculty of modeling long and
complex action classes, we divide all actions into smaller
building blocks. Those subactions are eventually modeled
within the RNN and later combined by the inference pro-
cess. The usage of subactions allows to distribute hetero-

1

geneous information of one action class over many sub-
classes and to capture characteristics such as the length of
the overall action class. Additionally, we show that auto-
matically learning the number of subactions for each action
class leads to a notably improved performance.

Our model is trained with an iterative procedure. Given
the weakly supervised training data, an initial segmentation
is generated by uniformly distributing all actions among the
video. For each obtained action segment, all subactions are
then also uniformly distributed among the part of the video
belonging to the corresponding action. This way, an initial
alignment between video frames and subactions is deﬁned.
In an iterative phase, the RNN is then trained on this align-
ment and used in combination with the coarse model to in-
fer new action segment boundaries. From those boundaries,
we recompute the number of subactions needed for each ac-
tion class, distribute them again among the frames aligned
to the respective action, and repeat the training process until
convergence.

We evaluate our approach on two common benchmark
datasets, the Breakfast dataset [14] and the Hollywood ex-
tended dataset [3], regarding two different tasks. The ﬁrst
task is temporal action segmentation, which refers to a com-
bined segmentation and classiﬁcation, where the test video
is given without any further annotation. The second task is
aligning a test video to a given order of actions, as proposed
by Bojanowski et al. [3]. Our approach is able to outper-
form current state-of-the-art methods on both tasks.

2. Related Work

For the case of fully supervised learning of actions, well-
studied deep learning and temporal modeling approaches
exist. While the authors of
[34] focus on a purely neu-
ral network based approach, Tang et al. [29] propose to
learn the latent temporal structure of videos with a hid-
den Markov model. Combining deep learning and tempo-
ral modeling, the authors of [18] use a segmental CNN and
a semi-Markov model to represent temporal transitions be-
tween actions. However, these methods are not applicable
in a weakly supervised setting.

Addressing the problem of weakly supervised learning
of actions, a variety of different approaches have been ex-
plored. First works, proposed by Laptev et al. [17] and
Marszalek et al. [20], focus on mining training samples
from movie scripts. They extract class samples based on
the respective text passages and use those snippets for train-
ing without applying a dedicated temporal alignment of the
action within the extracted clips. First attempts for learn-
ing action classes including temporal alignment on weakly
annotated data are made by Duchenne et al. [7]. Here, it
is assumed that all snippets contain only one class and the
task is to temporally segment frames containing the relevant
action from the background activities. The temporal align-

ment is thus interpreted as a binary clustering problem, sep-
arating temporal snippets containing the action class from
the background segments. The clustering problem is for-
mulated as a minimization of a discriminative cost func-
tion. This problem formulation is extended by Bojanowski
et al. [3] also introducing the Hollywood extended dataset.
Here, the weak learning is formulated as a temporal assign-
ment problem. Given a set of videos and the action order of
each video, the task is to assign the respective class to each
frame, thus to infer the respective action boundaries. The
authors propose a discriminative clustering model using the
temporal ordering constraints to combine classiﬁcation of
each action and their temporal localization in each video
clip. They propose the usage of the Frank-Wolfe algorithm
to solve the convex minimization problem. This method has
been adopted by Alayrac et al. [1] for unsupervised learn-
ing of task and story lines from instructional video. An-
other approach for weakly supervised learning from tempo-
rally ordered action lists is introduced by Huang et al. [9].
They feature extended connectionist temporal classiﬁcation
and propose the induction of visual similarity measures to
prevent the CTC framework from degeneration and to en-
force visually consistent paths. On the other hand, Kuehne
et al. [16] borrow on the concept of ﬂat models in speech
recognition. They model actions by hidden Markov models
(HMMs) and aim to maximize the probability of training
sequences being generated by the HMMs by iteratively in-
ferring the segmentation boundaries for each video and us-
ing the new segmentation to reestimate the model. The last
two approaches were both evaluated on the Hollywood ex-
tended as well as on the Breakfast dataset, thus, these two
datasets are also used for the evaluation of the here proposed
framework.

Beside the approaches focusing on weak learning of hu-
man actions based on temporally ordered labels, also other
weak learning scenarios have been explored. A closely re-
lated approach comes from the ﬁeld of sign language recog-
nition. Here, Koller et al. [13] integrate CNNs with hidden
Markov models to learn sign language hand shapes based
on a single frame CNN model from weakly annotated data.
They evaluate their approach on various large scale sign
language corpora, e.g. for Danish and New Zealand sign
language. Gan et al. [8] show an approach to learn action
classes from web images and videos retrieved by speciﬁc
search queries. They feature a pairwise match of images
and video frames and combine this with a regularization
over the selected video frames to balance the matching pro-
cedure. The approach is evaluated on standard action classi-
ﬁcation datasets such a UCF101 and Trecvid. Also learning
from web videos and images is the approach of [28]. Weak
video labels and noisy image labels are taken as input, and
localized action frames are generated as output. The lo-
calized action frames are used to train action recognition

models with long short-term memory networks. Results are
reported, among others, for temporal detection on the THU-
MOS 2014 dataset. Another idea is proposed by Misra et
al. [21], aiming to learn a temporal order veriﬁcation for hu-
man actions in an unsupervised way by training a CNN with
correct vs. shufﬂed video snippets and thus capturing tem-
poral information. The system can be used for pre-training
feature extractors on small datasets as well as in combina-
tion with other supervised methods. A more speech related
task is also proposed by Malmaud et al. [19], trying to align
recipe steps to automatically generated speech transcripts
from cooking videos. They use an hybrid HMM model
in combination with a CNN based visual food detector to
align a sequence of instructions, e.g. from textual recipes,
to a video of someone carrying out a task. Finally, [32]
propose an unsupervised technique to derive action classes
from RGB-D videos, respectively human skeleton represen-
tations, also considering an activity as a sequence of short-
term action clips. They propose Gibbs sampling for learn-
ing and inference of long activities from basic action words
and evaluate their approach on an RGB-D activity video
dataset.

3. Technical Details

In the following, we describe the proposed framework in
detail, starting with a short deﬁnition of the weak learning
task and the related training data. We then deﬁne our model
and describe the overall training procedure as well as how
it can be used for inference.

In contrast to fully supervised action detection or seg-
mentation approaches, where frame based ground truth data
is available, in weakly supervised learning only an ordered
list of the actions occurring in the video is provided for
training. A video of making tea, for instance, might con-
sist of taking a cup, putting the teabag in it, and pouring
water into the cup. While fully supervised tasks would pro-
vide a temporal annotation of each action start and end time,
in our weakly supervised setup, all given information is the
ordered action sequence

take cup, add teabag, pour water.

1 ), where xT

More formally, we assume the training data is a set of tu-
1 , aN
pels (xT
1 are framewise features of a video
with T frames and aN
1 is an ordered sequence (a1, . . . , aN )
of actions occurring in the video. The segmentation of the
video is deﬁned by the mapping

n(t) :

1, . . . , T
{

} (cid:55)→ {

1, . . . , N

}

(1)

that assigns an action segment index to each frame. Since
our model iteratively optimizes the action segmentation, ini-
tially, this can simply be a linear segmentation of the pro-
vided actions, see Figure 4a. The likelihood of the video
frames xT
1 is then deﬁned as

1 given the action transcripts aN

p(xT
1 |

aN
1 ) :=

p

xt|

an(t)

,

(2)

(cid:0)
an(t)) is the probability of frame xt being gen-

(cid:1)

T

t=1
(cid:89)

where p(xt|
erated by the action an(t).

1 , . . . , s(a)

The action classes given for training usually describe
longer, task-oriented procedures that naturally consist of
more than one signiﬁcant motion, e.g. take cup can involve
moving a hand towards a cupboard, opening the cupboard,
grabbing the cup and placing it on the countertop. This
makes it difﬁcult to train long, heterogeneous actions as a
whole. To efﬁciently capture those characteristics, we pro-
pose to model each action as a sequential combination of
subactions. Therefore, for each action class a, a set of sub-
actions s(a)
is deﬁned. The number Ka is initially
estimated by a heuristic and reﬁned during the optimization
process. Practically, this means that we subdivide the orig-
inal long action classes into a set of smaller subactions. As
subactions are obviously not deﬁned by the given ordered
action sequences, we treat them as latent variables that need
to be learned by the model.
In the following system de-
scription, we assume that the subaction frame boundaries
are known, e.g. from previous iterations or from an initial
uniform segmentation (see Figure 4b), and discuss the in-
ference of more accurate boundaries in Section 3.4.

Ka

In order to combine the ﬁne grained subactions to action
Ha for each action a is
sequences, a hidden Markov model
deﬁned. The HMM ensures that subactions only occur in
the correct ordering, i.e. that s(a)
s(a)
j. More
j
precisely, let

i ≺

for i

≤

(3)

s(a1)
1

, . . . , s(aN )
KaN }

s(t) :

1, . . . , T
{

} (cid:55)→ {
be the known mapping from video frames to the subactions
of the ordered action sequence aN
1 . This is basically the
same mapping as the one in Equation (1) but on subac-
tion level rather than on action level. When going from
one frame to the next, we only allow to assign either the
same subaction or the next subaction, so if at frame t, the
assigned subaction is s(t) = s(a)
, then at frame t + 1, either
s(t + 1) = s(a)
i+1. The likelihood of the
video frames xT

1 given the action transcripts aN

or s(t + 1) = s(a)

1 is then

i

i

p(xT
1 |

aN
1 ) :=

p

xt|

s(t)

p

·

s(t
s(t)
|

−

1)

,

(4)

(cid:1)

(cid:0)

(cid:1)

T

t=1
(cid:89)

(cid:0)

3.1. Weakly Supervised Learning from Action Se-

quences

3.2. Coarse Action Model

Figure 3. The extractor function A computes the unique action
sequence induced by the frame-to-subaction alignment s(t).

−

create a chunk over x[t
20, t] and forward it through the
RNN. While this practically increases the amount of data
that needs to be processed by a factor of 20, only short se-
quences need to be forwarded at once and we beneﬁt from a
high parallelization degree and comparable large minibatch
size.

Additionally, one has to note that even LSTMs and
GRUs can only capture a limited amount of temporal con-
text. For instance, studies from machine translation suggest
that 20 frames is a range that can be well captured by these
architectures [4]. This ﬁnding is conﬁrmed for video data
in [27]. Also, humans usually do not need much more con-
text to accurately classify a part of an action. Hence, storing
the information of e.g. frame 10 while computing the output
of frame 500 is not necessary. Thus, it can be appropriate to
limit the temporal scope in favor of a faster, more feasible
training.

Based on the observation probabilities of the ﬁne-
grained subaction model and the coarse model for overall
actions, we will now discuss the combined inference of both
models on video level.
Given a video xT

1 , the most likely action sequence

ˆaN
1 = arg max

p(xT
1 |
{

aN
1 )

·

p(aN
1 )

}

aN
1

(6)

G

and p(aN

and the factor p(aN

and the corresponding frame alignment is to be found. In
order to limit the amount of action sequences to optimize
is created from the training
over, a context-free grammar
set as in [16]. We set p(aN
1 ) = 1 if aN
1 is generated by
1 ) = 0 otherwise. Thus, in Equation (6), the
G
arg max only needs to be taken over action sequences gen-
1 ) can be omitted. Instead of
erated by
ﬁnding the optimal action sequence directly, the inference
can equivalently be performed over all possible frame-to-
subaction alignments s(t) that are consistent with
. Con-
sistent means that the unique action sequence deﬁned by
. Formally, we deﬁne an extractor
s(t) is generated by
G
aN
1 that maps the frame-to-subaction
function
alignment s(t) to its action sequence, see Figure 3 for an
illustration. Equation (6) can then be rewritten as

: s(t)

(cid:55)→

A

G

G

ˆaN
1 =

arg max
s(t):A(s(t))∈L(G)

T

p

xt|

s(t)

p

·

s(t
s(t)
|

−

1)

(cid:110)

t=1
(cid:89)

(cid:0)

(cid:1)

(cid:0)

,
(cid:1)(cid:111)
(7)

3.3. Fine-grained Subaction Model

3.4. Inference

Figure 2. RNN using gated recurrent units with framewise video
features as input. At each frame, the network outputs a probability
for each possible subaction while considering the context of the
video.

s) are probabilities computed by the ﬁne-
where p(xt|
grained model, see Section 3.3. The transition probabili-
s(cid:48)) from subaction s(cid:48) to subaction s are relative fre-
ties p(s
|
quencies of how often the transition s(cid:48)
s occurs in the
s(t)-mappings of all training videos.

→

For the classiﬁcation of ﬁne-grained subactions, we use
an RNN with a single hidden layer of gated recurrent units
(GRUs) [4]. It is a simpliﬁed version of LSTMs that shows
comparable performance [11, 5] also in case of video clas-
siﬁcation [2]. The network is shown in Figure 2.

For each frame, it predicts a probability distribution over
all subactions, while the recurrent structure of the net-
work allows to incorporate local temporal context. Since
xt) but our
the RNN generates a posterior distribution p(s
|
coarse model deals with subaction-conditional probabili-
ties, we use Bayes’ rule to transform the network output
to

s) = const

p(xt|

xt)

p(s
|
p(s)

.

·

(5)

Solving Efﬁciency Issues. Recurrent neural networks are
usually trained using backpropagation through time (BPTT)
[31], which requires to process the whole sequence in a for-
ward and backward pass. As videos can be very long and
may easily exceed 10, 000 frames, the computation time per
minibatch can be extremely high. Even worse, long videos
may not ﬁt the memory of high-end GPUs, since during
training the output of all network layers needs to be stored
for each frame of the video in order to compute the gradient.
We tackle this problem by using small chunks around
each video frame that can be processed efﬁciently and with
a reasonably large minibatch size in order to enable efﬁ-
cient RNN training on long videos. For each frame t, we

L

) is the set of all possible action sequences that
where
(
G
can be generated by
. Note that Equation (7) can be
solved efﬁciently using a Viterbi algorithm if the grammar
is context-free, see e.g. [10].

G

For training, as well as for the task of aligning videos to
a given ordered action sequence aN
1 , the best frame align-
ment to a single sequence needs to be inferred. By deﬁning
a grammar that generates only the given action sequence
aN
1 , this alignment task can be solved using Equation (7).
For the task of temporal action segmentation, i.e. when no
action sequence is provided for inference, the context-free
grammar can be derived from the ordered action sequences
given in the training samples.

3.5. Training

Training of the model is an iterative process, altering
between both, the recurrent neural network and the HMM
training, and the alignment of frames to subaction units via
the HMM. The whole process is illustrated in Figure 4.
Initialization. The video is divided into N segments of
equal size, where N is the number of action instances in the
transcript (Figure 4a). Each action segment is further sub-
divided equally across the subactions (Figure 4b). Note that
this deﬁnes the mapping s(t) from frames to subactions.
Each subaction should cover m frames of an action on aver-
age. Thus, the initial number of subactions for each action
is

number of frames

number of action instances

,

m

·

(8)

where we usually choose m = 10 as proposed in [15, 16].
Hence, initially each action is modeled with the same num-
ber of subactions. This can change during the iterative opti-
mization.
Iterative Training. The ﬁne-grained RNN is trained with
the current mapping s(t) as ground truth. Then, the RNN
and HMM are applied to the training videos and a new
alignment of frames to subactions (Figure 4d) is inferred
s) from the
given the new ﬁne-grained probabilities p(xt|
RNN. The new alignment is obtained by ﬁnding the subac-
tion mapping s(t) that best explains the data:

ˆs(t) = arg max

s(t)

p(xT
1 |

aN
1 )

(cid:110)

T

(cid:111)

(9)

= arg max

s(t)

p

xt|

s(t)

p

·

s(t
s(t)
|

−

1)

(cid:110)

t=1
(cid:89)

(cid:0)

(cid:1)

(cid:0)

.
(cid:1)(cid:111)

(10)

Note that Equation (10) can be efﬁciently computed using a
Viterbi algorithm. Once the realignment is computed for all
training videos, the average length of each action is reesti-
mated as

len(a) =

number of frames aligned to a
number of a-instances

(11)

and the number of subactions is reestimated based on the
updated average action lengths. Particularly, for action a,
there are now len(a)/m subactions, which are again uni-
formly distributed among the frames assigned to the corre-
sponding action, cf . Figure 4e. These steps are iterated until
convergence.
Stop Criterion. As the system iteratively approximates the
optimal action segmentation on the training data, we de-
ﬁne a stop criterion based on the overall amount of action
boundaries shifted from one iteration to the succeeding one.
In iteration i, let change(i) denote the percentage of frames
that is labeled differently compared to iteration i
1. We
stop the optimization if the frame change rate between two
iterations is less than a threshold,

−

change(i)
|

−

change(i

1)

< ϑ

−

|

⇒

stop.

(12)

4. Experiments

In this section, we provide a detailed analysis of our

method. Code and models are available online1.

4.1. Setup

Datasets. We evaluate the proposed approach on two het-
erogeneous datasets. The Breakfast dataset is a large scale
dataset with 1, 712 clips and an overall duration of 66.7
hours. The dataset comprises various kitchen tasks such as
making tea but also complex activities such as the prepara-
tion of fried egg or pancake. It features 48 action classes
with a mean of 4.9 instances per video. We follow the eval-
uation protocol as proposed by the authors in [14].

The Hollywood extended [3] dataset is an extension of
the well known Hollywood dataset, featuring 937 clips from
different Hollywood movies. The clips are annotated with
two or more action labels resulting in 16 different action
classes overall and a mean of 2.5 action instances per clip.
Features. For both datasets we follow the feature computa-
tion as described in [15] using improved dense trajectories
(IDT) and Fisher vectors (FVs). To compute the FV rep-
resentation, we ﬁrst reduce the dimensionality of the IDT
features from 426 to 64 by PCA and sample 150, 000 ran-
domly selected features to build a GMM with 64 Gaussians.
The Fisher vector representation [25] for each frame is com-
puted over a sliding window of 20 frames. Following [22],
we apply power and l2 normalization to the resulting FV
representation. Additionally, we reduce the ﬁnal FV rep-
resentation from 8, 192 to 64 dimensions via PCA to keep
the overall video representation manageable and easier to
process.
Stop Criterion. For the stop criterion, we ﬁx ϑ = 0.02,
i.e. if the difference of the frame change between two iter-
ations is less than two percent, we stop iterating. Figure 5

1https://github.com/alexanderrichard/weakly-sup-action-learning

Figure 4. Training process of our model. Initially, each action is modeled with the same number of subactions and the video is linearly
aligned to these subactions. Based on this alignment, the RNN is trained and used in combination with the HMMs to realign the video
frames to the subactions. Eventually, the number of subactions per action is reestimated and the process is iterated until convergence.

Breakfast

GRU no subactions
GRU w/o reestimation
GRU + reestimation

GRU + GT length

Accuracy (Mof)

22.4
28.8
33.3

51.3

Table 1. Results for temporal segmentation on the Breakfast
dataset comparing accuracy of the proposed system (GRU + rees-
timation) to the accuracy of the same architecture without subac-
tions (GRU no subactions) and to the architecture with subclasses
but without reestimation.

illustrates the criterion for two example experiments. The
blue curve is the frame accuracy, the red curve is the dif-
ference of frame changes between two iterations. It can be
seen that after a few iterations, the frame accuracy does not
increase anymore but begins to oscillate, see also Table 2.
Comparing the frame change rate of the train data is a good
indicator of when to stop iterating. In all experiments, we
compute results based on the alignment of the last iteration
before the threshold ϑ is crossed.

4.2. Analysis of the Coarse Model

For the following evaluation of our system, we report
performance for the task of temporal action segmentation,
i.e. the combined video segmentation and classiﬁcation.

Given a video without any further information, the task is
to classify all frames according to their related action. This
includes to infer which actions occur in the video, in which
order they occur, and their respective start and end frames.
We evaluate on the test set of the Breakfast dataset and re-
port results as mean accuracy over frames (Mof) (see [14]).
We iterate the system until the stop criterion as described in
Section 3.5 is reached.

First, we regard the properties of the coarse action mod-
eling. We therefore compare the proposed system to the
results of the same setting, but without further subdividing
actions into subactions (GRU no subactions, Table 1). Ad-
ditionally, we regard results of the system without reesti-
mation of subactions during optimization (GRU w/o rees-
timation, Table 1). For the system without reestimation,
we follow the initial steps as shown in Figure 4, thus, we
linearly segment the videos according to the number of ac-
tions, generate an initial subaction alignment, train the re-
spective subaction classes, and realign the sequence based
on the RNN output. But, opposed to the setup with reesti-
mation, we omit the step of reestimating the number of sub-
classes and the following alignment. Instead we just use the
output of the realignment to retrain the classiﬁer and iterate
the process of training, alignment, and re-training. Thus,

Breakfast

Iter 1

Iter 2

Iter 3

Iter 4

Iter 5

GMM w/o reest.
MLP w/o reest.
GRU w/o reest.

GRU w/o HMM

15.3
22.4
25.5

21.3

23.3
24.0
29.1

20.1

26.3
23.7
28.6

23.8

27.0
23.1
29.3

21.8

26.5
20.3
28.8

22.4

Table 2. Results for low level recognition with an MLP compared
to GRUs over ﬁve iterations. The MLP quickly starts to overﬁt,
whereas the GRU oscillates at a constant level. Last row: A GRU
without HMM, showing that short term dependencies are well cap-
tured by the ﬁne-grained recurrent network.

same system, just with different numbers of subactions, in-
creases by almost 20%. Qualitative results on two example
videos are shown in Figure 6.

4.3. Analysis of the Fine-Grained Model

In order to analyze the capability of capturing temporal
context with the recurrent network, we compare it to a sys-
tem where a Gaussian mixture model (GMM) and a multi-
layer perceptron (MLP) are used instead. Both only operate
on frame level and do not capture ﬁne-grained information
between the frames. In order to provide a fair comparison
to the recurrent model, we equip the MLP with a single hid-
den layer of rectiﬁed units such that it has the same number
of parameters as the recurrent network. We use a simpli-
ﬁed version of the system without subaction reestimation to
achieve comparable results after each iteration.

Results for the ﬁrst ﬁve iterations on the Breakfast
dataset are shown in Table 2. We ﬁnd that GRUs clearly
outperform both, GMMs and MLPs, starting with 25.5%
for the initial recognition, and reaching up to 29.3% after
the fourth iteration. Particularly, the MLP baseline stays
continuously below this performance. Thus, it can be as-
sumed that the the additional information gained by recur-
rent connections in this context supports classiﬁcation. One
can further see that the MLP reaches its best performance
after the second iteration and then continuously decreases,
whereas the GRU begins to oscillate around 29%, hinting
that the MLP also starts to overﬁt at an earlier stage com-
pared to the GRU. In the last row of Table 2, the coarse
model, i.e. the HMM, is removed from the system. Thus,
there is no modeling of subactions anymore and the GRU
directly learns the original action classes. The performance
is clearly worse than with the coarse model, but still the
system is remarkably good, being on par with the MLP that
uses the coarse model (second row of Table 2).

4.4. Comparison to State-of-the-Art

Temporal Action Segmentation. We compare our sys-
tem to three different approaches published for the task:
The ﬁrst is the Ordered Constrained Discriminative Cluster-
ing (OCDC) proposed by Bojanowski et al. [3], which has
been introduced on the Hollywood extended dataset. Sec-

Figure 5. Stop criterion for two experiments, one using the ﬁne-
grained model and no subaction reestimation, one using it with
subaction reestimation. The blue curve shows the frame accuracy
over the iterations, the red curve shows the frame change rate be-
tween the current and the preceding iteration. The dashed line
represents the threshold ϑ = 0.02.

the number of subclasses is constant and the coarse model
is not adapted to the overall estimated length of the action
class. Finally, we compare against a system in which we use
the ground truth boundaries to compute the mean length of
an action class and set the number of subactions based on
the mean ground truth length (GRU + GT length, Table 1).
Here, all action classes are still uniformly initialized, but
longer action classes are divided into more subactions than
shorter ones. We include this artiﬁcial scenario as it models
the performance in case that the optimal number of subac-
tion classes would be found.

One can see in Table 1 that recognition performance
without subactions is signiﬁcantly below all other conﬁgu-
rations, supporting the idea that subaction modeling in gen-
eral helps recognition in this scenario. The model with sub-
actions, but without reestimation, improves over the single
class model, but is still below the system with subaction
reestimation. Compared to that, the model with subaction
reestimation performs 5% better. We ascribe the perfor-
mance increase of the reestimated model to the fact that
a good performance is highly related to the correct num-
ber of subactions, thus to a good length representation of
the single actions. By reestimating the overall number of
subactions after each iteration, this ground truth length is
approximated. The impact of the number of subactions be-
comes clear, when considering the results when the ground
truth action lengths are used. Here the performance of the

Figure 6. Example segmentation results for two samples from the Breakfast dataset showing the segmentation result for “preparing cereals”
and “preparing friedegg”. Although the actions are not always correctly detected, there is still reasonable alignment of detected actions and
ground truth boundaries.

Model

OCDC [3]*
HTK [16]
ECTC [9]

GRU w/o reestimation
GRU + reestimation

Breakfast Hollywood Ext.

Accuracy (Mof)

Jacc. (IoU)

Model

Breakfast Hollywood Ext.

Jacc. (IoD)

Jacc. (IoD)

8.9
25.9
27.7

28.8
33.3

-
8.6
-

11.2
11.9

OCDC [3]
HTK [16]**
ECTC [9]**

GRU w/o reestimation
GRU + reestimation

23.4
40.6
-

41.5
47.3

43.9
42.4
41.0

45.6
46.3

Table 3. Comparison of temporal action segmentation perfor-
mance for GRU based weak learning with other approaches. For
the Breakfast dataset, we report performance as mean over frames
(Mof), for Hollywood extended, we measure the Jaccard index as
intersection over union for this task (*from [9]).

ond, we compare against the HTK system used by Kuehne
et al. [16], and third against the Extended Connectionist
Temporal Classiﬁcation (ECTC) by Huang et al. [9]. For
the Breakfast dataset, we follow the evaluation script of
[14, 9] and report results as mean accuracy over frames over
four splits. For the Hollywood Extended dataset, we follow
the evaluation script of [16] and report the Jaccard index
(Jacc.) as intersection over union (IoU) over 10 splits. Re-
sults are shown in Table 3.

One can see that the proposed subaction based GRU sys-
tems show a good performance, and that both subaction
based systems outperform current approaches on the two
evaluated datasets. It also shows that the GRU based sys-
tem without reestimation shows comparable performance
to other RNN based systems, such as ECTC by [9], which
uses an LSTM model with comparable size. The signiﬁ-
cant increase in accuracy of our method can be attributed
to the reestimation. We also observe that the performance
boost of the system with reestimation is more prominent
on the Breakfast than on the Hollywood extended dataset.
We attribute this to the fact that in case of Hollywood ex-
tended, all action classes usually have a consistent mean
frame length, whereas in case of Breakfast, the mean length
of action classes signiﬁcantly varies. Thus, the beneﬁt of
adapting to action class lengths increases with the hetero-
geneity of the target action classes.

Action Alignment. We also address the task of action
alignment. Here, we assume that given a video and a se-
quence of temporally ordered actions, the task is to infer the

Table 4. Results for action alignment on the test set of the Break-
fast and the Hollywood extended dataset reported as Jaccard index
of intersection over detection (IoD) (**results obtained from the
authors).

respective boundaries for the given action order. We report
results for the test set of Breakfast as well as for the Hol-
lywood extended dataset based on the Jaccard index (Jacc.)
computed as intersection over detection (IoD) as proposed
by [3]. The results are shown in Table 4.2 Here, the GRU
system without reestimation performs on par with other sys-
tems for the alignment task on the Breakfast dataset, but
the GRU system with reestimation again shows a clear im-
provement over current systems.

5. Conclusion

We presented an approach for weakly supervised learn-
ing of human actions based on a combination of a discrim-
inative representation of subactions, modeled by a recur-
rent neural network, and a coarse probabilistic model to al-
low for a temporal alignment and inference over long se-
quences. Although the system itself already shows good re-
sults, the performance is signiﬁcantly improved by approx-
imating the number of subactions for the different action
classes. Accordingly, we propose to adapt the number of
subaction classes by iterating realignment and reestimation
during training. The resulting models outperform state-of-
the-art on various weak learning tasks such as temporal ac-
tion segmentation and action alignment.

Acknowledgments. The work has been ﬁnancially sup-
ported by the DFG projects KU 3396/2-1 (Hierarchical

2Errata: The results for Hollywood Extended have been corrected. 45.6
and 46.3 are the correct numbers, the results 50.1 and 51.1 from the orig-
inal CVPR paper are incorrect.

Models for Action Recognition and Analysis in Video Data)
and GA 1927/4-1 (DFG Research Unit FOR 2535 An-
ticipating Human Behavior) and the ERC Starting Grant
ARCA (677650).

References

[1] J.-B. Alayrac, P. Bojanowski, N. Agrawal, I. Laptev, J. Sivic,
and S. Lacoste-Julien. Unsupervised learning from narrated
instruction videos. In IEEE Conf. on Computer Vision and
Pattern Recognition, 2016. 1, 2

[2] N. Ballas, L. Yao, P. Chris, and A. Courville. Delving deeper
into convolutional networks for learning video representa-
tions. In Int. Conf. on Learning Representations, 2016. 4
[3] P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce,
C. Schmid, and J. Sivic. Weakly supervised action label-
ing in videos under ordering constraints. In European Conf.
on Computer Vision, 2014. 1, 2, 5, 7, 8

[4] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio.
On the properties of neural machine translation: Encoder-
In Proceedings of SSST@EMNLP,
decoder approaches.
Eighth Workshop on Syntax, Semantics, and Structure in Sta-
tistical Translation, pages 103–111, 2014. 4

[5] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical
evaluation of gated recurrent neural networks on sequence
modeling. arXiv preprint arXiv:1412.3555, 2014. 4

[6] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term recur-
rent convolutional networks for visual recognition and de-
In IEEE Conf. on Computer Vision and Pattern
scription.
Recognition, 2015. 1

[7] O. Duchenne, I. Laptev, J. Sivic, F. Bach, and J. Ponce. Au-
tomatic annotation of human actions in video. In Int. Conf.
on Computer Vision, 2009. 1, 2

[8] C. Gan, C. Sun, L. Duan, and B. Gong. Webly-supervised
video recognition by mutually voting for relevant web im-
ages and web video frames. In European Conf. on Computer
Vision, 2016. 2

[9] D.-A. Huang, L. Fei-Fei, and J. C. Niebles. Connectionist
temporal modeling for weakly supervised action labeling. In
European Conf. on Computer Vision, 2016. 1, 2, 8

[10] D. Jurafsky, C. Wooters, J. Segal, A. Stolcke, E. Fosler,
G. Tajchaman, and N. Morgan. Using a stochastic context-
free grammar as a language model for speech recognition. In
IEEE Int. Conf. on Acoustics, Speech and Signal Processing,
volume 1, pages 189–192, 1995. 5

[11] R. Jzefowicz, W. Zaremba, and I. Sutskever. An empirical
exploration of recurrent network architectures. In Int. Conf.
on Machine Learning, 2015. 4

[12] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In IEEE Conf. on Computer Vision
and Pattern Recognition, pages 1725–1732, 2014. 1

[13] O. Koller, H. Ney, and R. Bowden. Deep hand: How to train
a cnn on 1 million hand images when your data is continuous
and weakly labelled. In IEEE Conf. on Computer Vision and
Pattern Recognition, 2016. 2

[14] H. Kuehne, A. B. Arslan, and T. Serre. The language of ac-
tions: Recovering the syntax and semantics of goal-directed
In IEEE Conf. on Computer Vision and
human activities.
Pattern Recognition, 2014. 2, 5, 6, 8

[15] H. Kuehne, J. Gall, and T. Serre. An end-to-end generative
framework for video segmentation and recognition. In IEEE
Winter Conf. on Applications of Computer Vision, 2016. 5

[16] H. Kuehne, A. Richard, and J. Gall. Weakly super-
vised learning of actions from transcripts. arXiv preprint
arXiv:1610.02237, 2016. 2, 4, 5, 8

[17] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld.
In IEEE
Learning realistic human actions from movies.
Conf. on Computer Vision and Pattern Recognition, 2008.
1, 2

[18] C. Lea, A. Reiter, R. Vidal, and G. D. Hager. Segmental
spatiotemporal cnns for ﬁne-grained action segmentation. In
European Conf. on Computer Vision, pages 36–52, 2016. 2
[19] J. Malmaud, J. Huang, V. Rathod, N. Johnston, A. Rabi-
novich, and K. Murphy. Whats cookin? interpreting cooking
videos using text, speech and vision. In Conf. of the North
American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, 2015. 3

[20] M. Marszalek, I. Laptev, and C. Schmid. Actions in context.
In IEEE Conf. on Computer Vision and Pattern Recognition,
2009. 1, 2

[21] I. Misra, C. L. Zitnick, and M. Hebert. Shufﬂe and learn:
Unsupervised learning using temporal order veriﬁcation. In
European Conf. on Computer Vision, 2016. 3

[22] F. Perronnin, J. S´anchez, and T. Mensink.

ﬁsher kernel for large-scale image classiﬁcation.
pean Conf. on Computer Vision, 2010. 5

Improving the
In Euro-

[23] A. Richard and J. Gall. Temporal action detection using a
statistical language model. In IEEE Conf. on Computer Vi-
sion and Pattern Recognition, 2016. 1

[24] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A
database for ﬁne grained activity detection of cooking activ-
ities. In IEEE Conf. on Computer Vision and Pattern Recog-
nition, pages 1194–1201, 2012. 1

[25] J. Sanchez, F. Perronnin, T. Mensink, and J. Verbeek. Image
Classiﬁcation with the Fisher Vector: Theory and Practice.
International Journal on Computer Vision, 105(3):222–245,
Dec. 2013. 5

[26] K. Simonyan and A. Zisserman. Two-stream convolutional
In Advances
networks for action recognition in videos.
in Neural Information Processing Systems, pages 568–576,
2014. 1

[27] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao. A
multi-stream bi-directional recurrent neural network for ﬁne-
grained action detection. In IEEE Conf. on Computer Vision
and Pattern Recognition, 2016. 1, 4

[28] C. Sun, S. Shetty, R. Sukthankar, and R. Nevatia. Tempo-
ral localization of ﬁne-grained actions in videos by domain
In ACM Conf. on Multimedia,
transfer from web images.
2015. 2

[29] K. Tang, L. Fei-Fei, and D. Koller. Learning latent tempo-
In IEEE Conf.
ral structure for complex event detection.
on Computer Vision and Pattern Recognition, pages 1250–
1257, 2012. 2

[30] H. Wang and C. Schmid. Action recognition with improved
trajectories. In Int. Conf. on Computer Vision, pages 3551–
3558, 2013. 1

[31] P. J. Werbos. Backpropagation through time: what it does
and how to do it. Proceedings of the IEEE, 78:1550–1560,
1990. 4

[32] C. Wu, J. Zhang, S. Savarese, and A. Saxena. Watch-n-
patch: Unsupervised understanding of actions and relations.
In IEEE Conf. on Computer Vision and Pattern Recognition,
2015. 3

[33] Z. Wu, X. Wang, Y.-G. Jiang, H. Ye, and X. Xue. Modeling
spatial-temporal clues in a hybrid deep learning framework
for video classiﬁcation. In ACM Conf. on Multimedia, 2015.
1

[34] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
In IEEE Conf. on Computer Vision and Pattern
videos.
Recognition, 2016. 1, 2

[35] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan,
O. Vinyals, R. Monga, and G. Toderici. Beyond short snip-
pets: Deep networks for video classiﬁcation. In IEEE Conf.
on Computer Vision and Pattern Recognition, 2015. 1

