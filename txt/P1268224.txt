Mach Learn (2016) 102:209–245
DOI 10.1007/s10994-015-5517-9

Propagation kernels: efﬁcient graph kernels from
propagated information

Marion Neumann1 · Roman Garnett2 ·
Christian Bauckhage3 · Kristian Kersting4

Received: 17 April 2014 / Accepted: 18 June 2015 / Published online: 17 July 2015
© The Author(s) 2015

Abstract We introduce propagation kernels, a general graph-kernel framework for efﬁ-
ciently measuring the similarity of structured data. Propagation kernels are based on
monitoring how information spreads through a set of given graphs. They leverage early-
stage distributions from propagation schemes such as random walks to capture structural
information encoded in node labels, attributes, and edge information. This has two beneﬁts.
First, off-the-shelf propagation schemes can be used to naturally construct kernels for many
graph types, including labeled, partially labeled, unlabeled, directed, and attributed graphs.
Second, by leveraging existing efﬁcient and informative propagation schemes, propagation
kernels can be considerably faster than state-of-the-art approaches without sacriﬁcing pre-
dictive performance. We will also show that if the graphs at hand have a regular structure,
for instance when modeling image or video data, one can exploit this regularity to scale the
kernel computation to large databases of graphs with thousands of nodes. We support our
contributions by exhaustive experiments on a number of real-world graphs from a variety of
application domains.

Editor: Karsten Borgwardt.

B Marion Neumann

marion.neumann@uni-bonn.de

Roman Garnett
garnett@wustl.edu

Christian Bauckhage
christian.bauckhage@iais.fraunhofer.de

Kristian Kersting
kristian.kersting@cs.tu-dortmund.de

BIT, University of Bonn, Bonn, Germany

1

2

3

4

CSE, Washington University, St. Louis, MO, USA

Fraunhofer IAIS, Sankt Augustin, Germany

CS, Technical University of Dortmund, Dortmund, Germany

123

210

Mach Learn (2016) 102:209–245

Keywords Learning with graphs · Graph kernels · Random walks ·
Locality sensitive hashing · Convolutions

1 Introduction

Learning from structured data is an active area of research. As domains of interest become
increasingly diverse and complex, it is important to design ﬂexible and powerful methods
for analysis and learning. By structured data we refer to situations where objects of interest
are structured and hence can naturally be represented using graphs. Real-world examples are
molecules or proteins, images annotated with semantic information, text documents reﬂecting
complex content dependencies, and manifold data modeling objects and scenes in robotics.
The goal of learning with graphs is to exploit the rich information contained in graphs rep-
resenting structured data. The main challenge is to efﬁciently exploit the graph structure for
machine-learning tasks such as classiﬁcation or retrieval. A popular approach to learning from
structured data is to design graph kernels measuring the similarity between graphs. For clas-
siﬁcation or regression problems, the graph kernel can then be plugged into a kernel machine,
such as a support vector machine or a Gaussian process, for efﬁcient learning and prediction.
Several graph kernels have been proposed in the literature, but they often make strong
assumptions as to the nature and availability of information related to the graphs at hand.
The most simple of these proposals assume that graphs are unlabeled and have no structure
beyond that encoded by their edges. However, graphs encountered in real-world applications
often come with rich additional information attached to their nodes and edges. This naturally
implies many challenges for representation and learning such as:

– missing information leading to partially labeled graphs,
– uncertain information arising from aggregating information from multiple sources, and
– continuous information derived from complex and possibly noisy sensor measurements.

Images, for instance, often have metadata and semantic annotations which are likely to be
only partially available due to the high cost of collecting training data. Point clouds captured
by laser range sensors consist of continuous 3d coordinates and curvature information; in
addition, part detectors can provide possibly noisy semantic annotations. Entities in text
documents can be backed by entire Wikipedia articles providing huge amounts of structured
information, themselves forming another network.

Surprisingly, existing work on graph kernels does not broadly account for these challenges.
Most of the existing graph kernels (Shervashidze et al. 2009, 2011; Hido and Kashima 2009;
Kashima et al. 2003; Gärtner et al. 2003) are designed for unlabeled graphs or graphs with
a complete set of discrete node labels. Kernels for graphs with continuous node attributes
have only recently gained greater interest (Borgwardt and Kriegel 2005; Kriege and Mutzel
2012; Feragen et al. 2013). These graph kernels have two major drawbacks: they can only
handle graphs with complete label or attribute information in a principled manner and they are
either efﬁcient, but limited to speciﬁc graph types, or they are ﬂexible, but their computation
is memory and/or time consuming. To overcome these problems, we introduce propagation
kernels. Their design is motivated by the observation that iterative information propagation
schemes originally developed for within-network relational and semi-supervised learning
have two desirable properties: they capture structural information and they can often adapt
to the aforementioned issues of real-world data. In particular, propagation schemes such as
diffusion or label propagation can be computed efﬁciently and they can be initialized with
uncertain and partial information.

123

Mach Learn (2016) 102:209–245

211

A high-level overview of the propagation kernel algorithm is as follows. We begin by
initializing label and/or attribute distributions for every node in the graphs at hand. We then
iteratively propagate this information along edges using an appropriate propagation scheme.
By maintaining entire distributions of labels and attributes, we can accommodate uncertain
information in a natural way. After each iteration, we compare the similarity of the induced
node distributions between each pair of graphs. Structural similarities between graphs will
tend to induce similar local distributions during the propagation process, and our kernel will
be based on approximate counts of the number of induced similar distributions throughout
the information propagation.

To achieve competitive running times and to avoid having to compare the distributions
of all pairs of nodes between two given graphs, we will exploit locality sensitive hashing
(lsh) to bin the label/attribute distributions into efﬁciently computable graph feature vectors
in time linear in the total number of nodes. These new graph features will then be fed into a
base kernel, a common scheme for constructing graph kernels. Whereas lsh is usually used
to preserve the (cid:2)1 or (cid:2)2 distance, we are able to show that the hash values can preserve both the
total variation and the Hellinger probability metrics. Exploiting explicit feature computation
and efﬁcient information propagation, propagation kernels allow for using graph kernels to
tackle novel applications beyond the classical benchmark problems on datasets of chemical
compounds and small- to medium-sized image or point-cloud graphs.

The present paper is a signiﬁcant extension of a previously published conference paper
(Neumann et al. 2012) and presents and extends a novel graph-kernel application already
published as a workshop contribution (Neumann et al. 2013). Propagation kernels were
originally deﬁned and applied for graphs with discrete node labels (Neumann et al. 2012,
2013); here we extend their deﬁnition to a more general and ﬂexible framework that is able to
handle continuous node attributes. In addition to this expanded view of propagation kernels,
we also introduce and discuss efﬁcient propagation schemes for numerous classes of graphs.
A central message of this paper is:

A suitable propagation scheme is the key to designing fast and powerful propagation
kernels.

In particular, we will discuss propagation schemes applicable to huge graphs with regular
structure, for example grid graphs representing images or videos. Thus, implemented with
care, propagation kernels can easily scale to large image databases. The design of kernels for
grids allows us to perform graph-based image analysis not only on the scene level (Neumann
et al. 2012; Harchaoui and Bach 2007) but also on the pixel level opening up novel application
domains for graph kernels.

We proceed as follows. We begin by touching upon related work on kernels and graphs.
After introducing information propagation on graphs via random walks, we introduce the fam-
ily of propagation kernels (Sect. 4). The following two sections discuss speciﬁc examples
of the two main components of propagation kernels: node kernels for comparing propagated
information (Sect. 5) and propagation schemes for various kinds of information (Sect. 6).
We will then analyze the sensitivity of propagation kernels with respect to noisy and missing
information, as well as with respect to the choice of their parameters. Finally, to demonstrate
the feasibility and power of propagation kernels for large real-world graph databases, we
provide experimental results on several challenging classiﬁcation problems, including com-
monly used bioinformatics benchmark problems, as well as real-world applications such as
image-based plant-disease classiﬁcation and 3d object category prediction in the context of
robotic grasping.

123

212

Mach Learn (2016) 102:209–245

2 Kernels and graphs

Propagation kernels are related to three lines of research on kernels: kernels between graphs
(graph kernels) developed within the graph mining community, kernels between nodes (ker-
nels on graphs) established in the within-network relational learning and semi-supervised
learning communities, and kernels between probability distributions.

2.1 Graph kernels

Propagation kernels are deeply connected to several graph kernels developed within the
graph-mining community. Categorizing graph kernels with respect to how the graph struc-
ture is captured, we can distinguish four classes: kernels based on walks (Gärtner et al.
2003; Kashima et al. 2003; Vishwanathan et al. 2010; Harchaoui and Bach 2007) and paths
(Borgwardt and Kriegel 2005; Feragen et al. 2013), kernels based on limited-size subgraphs
(Horváth et al. 2004; Shervashidze et al. 2009; Kriege and Mutzel 2012), kernels based on
subtree patterns (Mahé and Vert 2009; Ramon and Gärtner 2003), and kernels based on struc-
ture propagation (Shervashidze et al. 2011). However, there are two major problems with
most existing graph kernels: they are often slow or overly specialized. There are efﬁcient
graph kernels speciﬁcally designed for unlabeled and fully labeled graphs (Shervashidze
et al. 2009, 2011), attributed graphs (Feragen et al. 2013), or planar labeled graphs (Har-
chaoui and Bach 2007), but these are constrained by design. There are also more ﬂexible but
slower graph kernels such as the shortest path kernel (Borgwardt and Kriegel 2005) or the
common subgraph matching kernel (Kriege and Mutzel 2012).

The Weisfeiler–Lehman (wl) subtree kernel, one instance of the recently introduced fam-
ily of wl-kernels (Shervashidze et al. 2011), computes count features for each graph based
on signatures arising from iterative multi-set label determination and compression steps. In
each kernel iteration, these features are then fed into a base kernel. The wl-kernel is ﬁnally
the sum of those base kernels over the iterations.

Although wl-kernels are usually competitive in terms of performance and runtime, they
are designed for fully labeled graphs. The challenge of comparing large, partially labeled
graphs—which can easily be considered by propagation kernels introduced in the present
paper—remains to a large extent unsolved. A straightforward way to compute graph kernels
between partially labeled graphs is to mark unlabeled nodes with a unique symbol or their
degree as suggested in Shervashidze et al. (2011) for the case of unlabeled graphs. However,
both solutions neglect any notion of uncertainty in the labels. Another option is to propagate
labels across the graph and then run a graph kernel on the imputed labels (Neumann et al.
2012). Unfortunately, this also ignores the uncertainty induced by the inference procedure,
as hard labels have to be assigned after convergence. A key observation motivating propaga-
tion kernels is that intermediate label distributions induced will, before convergence, carry
information about the structure of the graph. Propagation kernels interleave label inference
and kernel computation steps, avoiding the requirement of running inference to termination
prior to the kernel computation.

2.2 Kernels on graphs and within-network relational learning

Measuring the structural similarity of local node neighborhoods has recently become popular
for inference in networked data (Kondor and Lafferty 2002; Desrosiers and Karypis 2009;
Neumann et al. 2013) where this idea has been used for designing kernels on graphs (ker-
nels between the nodes of a graph) and for within-network relational learning approaches.

123

Mach Learn (2016) 102:209–245

213

An example of the former are coinciding walk kernels (Neumann et al. 2013) which are
deﬁned in terms of the probability that the labels encountered during parallel random walks
starting from the respective nodes of a graph coincide. Desrosiers and Karypis (2009) use a
similarity measure based on parallel random walks with constant termination probability in
a relaxation-labeling algorithm. Another approach exploiting random walks and the struc-
ture of subnetworks for node-label prediction is heterogeneous label propagation (Hwang
and Kuang 2010). Random walks with restart are used as proximity weights for so-called
“ghost edges” in Gallagher et al. (2008), but then the features considered by a bag of logis-
tic regression classiﬁers are only based on a one-step neighborhood. These approaches, as
well as propagation kernels, use random walks to measure structure similarity. Therefore,
propagation kernels establish an important connection of graph-based machine learning for
inference about node- and graph-level properties.

2.3 Kernels between probability distributions and kernels between sets

Finally, propagation kernels mark another connection, namely between graph kernels and
kernels between probability distributions (Jaakkola and Haussler 1998; Lafferty and Lebanon
2002; Moreno et al. 2003; Jebara et al. 2004) and between sets (Kondor and Jebara 2003; Shi
et al. 2009). However, whereas the former essentially build kernels based on the outcome
of probabilistic inference after convergence, propagation kernels intuitively count common
sub-distributions induced after each iteration of running inference in two graphs.

Kernels between sets and more speciﬁcally between structured sets, also called hash
kernels (Shi et al. 2009), have been successfully applied to strings, data streams, and unlabeled
graphs. While propagation kernels hash probability distributions and derive count features
from them, hash kernels directly approximate the kernel values k(x, x (cid:2)), where x and x (cid:2) are
(structured) sets. Propagation kernels iteratively approximate node kernels k(u, v) comparing
nodes u in graph G(i) with nodes v in graph G( j). Counts summarizing these approximations
are then fed into a base kernel that is computed exactly. Before we give a detailed deﬁnition
of propagation kernels, we introduce the basic concept of information propagation on graphs,
and exemplify important propagation schemes and concepts when utilizing random walks
for learning with graphs.

3 Information propagation on graphs

Information propagation or diffusion on a graph is most commonly modeled via Markov ran-
dom walks (rws). Propagation kernels measure the similarity of two graphs by comparing
node label or attribute distributions after each step of an appropriate random walk. In the
following, we review label diffusion and label propagation via rws—two techniques com-
monly used for learning on the node level (Zhu et al. 2003; Szummer and Jaakkola 2001).
Based on these ideas, we will then develop propagation kernels in the subsequent sections.

3.1 Basic notation

Throughout, we consider graphs whose nodes are endowed with (possibly partially observed)
label and/or attribute information. That is, a graph G = (V, E, (cid:2), a) is represented by a set
of |V | = n vertices, a set of edges E speciﬁed by a weighted adjacency matrix A ∈ Rn×n, a
label function (cid:2) : V → [k], where k is the number of available node labels, and an attribute
function with a : V → RD, where D is the dimension of the continuous attributes. Given

123

214

Mach Learn (2016) 102:209–245

V = {v1, v2, ..., vn}, node labels (cid:2)(vi ) are represented by nominal values and attributes
a(vi ) = xi ∈ RD are represented by continuous vectors.

3.2 Markov random walks

Consider a graph G = (V, E). A random walk on G is a Markov process X = {Xt : t ≥ 0}
with a given initial state X0 = vi . We will also write Xt|i to indicate that the walk began
at vi . The transition probability Ti j = P(Xt+1 = v j
| Xt = vi ) only depends on the
current state Xt = vi and the one-step transition probabilities for all nodes in V can be
easily represented by the row-normalized adjacency or transition matrix T = D−1 A, where
D = diag(

(cid:2)

j Ai j ).

3.3 Information propagation via random walks

(cid:2)

For now, we consider (partially) labeled graphs without attributes, where V = VL ∪ VU is the
union of labeled and unlabeled nodes and (cid:2) : V → [k] is a label function with known values
for the nodes in VL . A common mechanism for providing labels for the nodes of an unlabeled
graph is to deﬁne the label function by (cid:2)(vi ) =
j Ai j = degree(vi ). Hence for fully labeled
and unlabeled graphs we have VU = ∅. We will monitor the distribution of labels encountered
by random walks leaving each node in the graph to endow each node with a k-dimensional
feature vector. Let the matrix P0 ∈ Rn×k give the prior label distributions of all nodes in V,
where the ith row (P0)i = p0,vi corresponds to node vi . If node vi ∈ VL is observed with
label (cid:2)(vi ), then p0,vi can be conveniently set to a Kronecker delta distribution concentrating
at (cid:2)(vi ); i.e., p0,vi
= δ(cid:2)(vi ). Thus, on graphs with VU = ∅ the simplest rw-based information
propagation is the label diffusion process or simply diffusion process

Pt+1 ← T Pt ,

where pt,vi gives the distribution over (cid:2)(Xt|i ) at iteration t.

Let S ⊆ V be a set of nodes in G. Given T and S, we deﬁne an absorbing random walk

to have the modiﬁed transition probabilities ˆT , deﬁned by:

ˆTi j =

⎧
⎨

⎩

0
1
Ti j

if i ∈ S and i (cid:10)= j;
if i ∈ S and i = j;
otherwise.

(1)

(2)

Nodes in S are “absorbing” in that a walk never leaves a node in S after it is encountered. The
ith row of P0 now gives the probability distribution for the ﬁrst label encountered, (cid:2)(X0|i ),
for an absorbing rw starting at vi . It is easy to see by induction that by iterating the map
Pt+1 ← ˆT Pt ,

(3)

pt,vi similarly gives the distribution over (cid:2)(Xt|i ).

In the case of partially labeled graphs we can now initialize the label distributions for
the unlabeled nodes VU with some prior, for example a uniform distribution.1 If we deﬁne
the absorbing states to be the labeled nodes, S = VL , then the label propagation algorithm
introduced in Zhu et al. (2003) can be cast in terms of simulating label-absorbing rws with
transition probabilities given in Eq. (2) until convergence, then assigning the most probable
absorbing label to the nodes in VU .

The schemes discussed so far are two extreme cases of absorbing rws: one with no
absorbing states, the diffusion process, and one which absorbs at all labeled nodes, label

1 This prior could also be the output of an external classiﬁer built on available node attributes.

123

Mach Learn (2016) 102:209–245

215

propagation. One useful extension of absorbing rws is to soften the deﬁnition of absorbing
states. This can be naturally achieved by employing partially absorbing random walks (Wu
et al. 2012). As the propagation kernel framework does not require a speciﬁc propagation
scheme, we are free to choose any rw-based information propagation scheme suitable for
the given graph types. Based on the basic techniques introduced in this section, we will sug-
gest speciﬁc propagation schemes for (un-)labeled, partially labeled, directed, and attributed
graphs as well as for graphs with regular structure in Sect. 6.

3.4 Steady-state distributions versus early stopping

Assuming non-bipartite graphs, all rws, absorbing or not, converge to a steady-state distrib-
ution P∞ (Lovász 1996; Wu et al. 2012). Most existing rw-based approaches only analyze
the walks’ steady-state distributions to make node-label predictions (Kondor and Lafferty
2002; Zhu et al. 2003; Wu et al. 2012). However, rws without absorbing states converge
to a constant steady-state distribution, which is clearly uninteresting. To address this, the
idea of early stopping was successfully introduced into power-iteration methods for node
clustering (Lin and Cohen 2010), node-label prediction (Szummer and Jaakkola 2001), as
well as for the construction of a kernel for node-label prediction (Neumann et al. 2013). The
insight here is that the intermediate distributions obtained by the rws during the convergence
process provide useful insights about their structure. In this paper, we adopt this idea for the
construction of graph kernels. That is, we use the entire evolution of distributions encoun-
tered during rws up to a given length to represent graph structure. This is accomplished by
summing contributions computed from the intermediate distributions of each iteration, rather
then only using the limiting distribution.

4 Propagation kernel framework

In this section, we introduce the general family of propagation kernels (pks).

4.1 General deﬁnition

Here we will deﬁne a kernel K : X × X → R among graph instances G(i) ∈ X . The input
space X comprises graphs G(i) = (V (i), E (i), (cid:2), a), where V (i) is the set of |V (i)| = ni
nodes and E (i) is the set of edges in graph G(i). Edge weights are represented by weighted
adjacency matrices A(i) ∈ Rni ×ni and the label and attribute functions (cid:2) and a endow nodes
with label and attribute information2 as deﬁned in the previous section.

A simple way to compare two graphs G(i) and G( j) is to compare all pairs of nodes in the

two graphs:

K (G(i), G( j)) =

k(u, v),

(cid:6)

(cid:6)

u∈G( j)

v∈G(i)
where k(u, v) is an arbitrary node kernel determined by node labels and, if present, node
attributes. This simple graph kernel, however, does not account for graph structure given by
the arrangement of node labels and attributes in the graphs. Hence, we consider a sequence of
(i)
graphs G
t with evolving node information based on information propagation, as introduced
for node labels in the previous section. We deﬁne the kernel contribution of iteration t by

2 Note that not both label and attribute information have to be present and both could also be partially observed.

123

216

Mach Learn (2016) 102:209–245

Algorithm 1 The general propagation kernel computation.
given: graph database {G(i)}i , # iterations tmax, propagation scheme(s), base kernel (cid:12)·, ·(cid:13)
(i)
initialization: K ← 0, initialize distributions P
0
for t ← 0 . . . tmax do

for all graphs G(i) do

for all nodes u ∈ G(i) do

quantize pt,u ,
(i)
where pt,u is the row in P
t

end for
compute Φi· = φ(G

(i)
t

)

end for
K ← K + (cid:12)Φ, Φ(cid:13)
for all graphs G(i) do
(i)
← P
t

(i)
P
t+1
end for

end for

that corresponds to node u

(cid:14) bin node information

(cid:14) count bin strengths

(cid:14) compute and add kernel contribution

(cid:14) propagate node information

K (G

(i)
t

, G

( j)
t

) =

(cid:6)

(cid:6)

v∈G

(i)
t

u∈G

( j)
t

k(u, v).

An important feature of propagation kernels is that the node kernel k(u, v) is deﬁned in
terms of the nodes’ corresponding probability distributions pt,u and pt,v, which we update
and maintain throughout the process of information propagation. For propagation kernels
between labeled and attributed graphs we deﬁne

k(u, v) = k(cid:2)(u, v) · ka(u, v),
where k(cid:2)(u, v) is a kernel corresponding to label information and ka(u, v) is a kernel corre-
sponding to attribute information. If no attributes are present, then k(u, v) = k(cid:2)(u, v). k(cid:2) and
ka will be deﬁned in more detail later. For now assume they are given, then the tmax-iteration
propagation kernel is given by

(5)

(cid:7)

(cid:8)

tmax(cid:6)

(cid:7)

Ktmax

G(i), G( j)

=

K

(i)
t

G

, G

( j)
t

(cid:8)

.

t=1

Lemma 1 Given that k(cid:2)(u, v) and ka(u, v) are positive semideﬁnite node kernels, the prop-
agation kernel Ktmax is a positive semideﬁnite kernel.

Proof As k(cid:2)(u, v) and ka(u, v) are assumed to be valid node kernels, k(u, v) is a valid
node kernel as the product of positive semideﬁnite kernels is again positive semideﬁnite. As
for a given graph G(i) the number of nodes is ﬁnite, K (G
) is a convolution kernel
(Haussler 1999). As sums of positive semideﬁnite matrices are again positive semideﬁnite,
(cid:15)(cid:16)
the propagation kernel as deﬁned in Eq. (6) is positive semideﬁnite.

( j)
t

, G

(i)
t

Let |V (i)| = ni and |V ( j)| = n j . Assuming that all node information is given, the
complexity of computing each contribution between two graphs, Eq. (4), is O(ni n j ). Even
for medium-sized graphs this can be prohibitively expensive considering that the computation
has to be performed for every pair of graphs in a possibly large graph database. However, if
we have a node kernel of the form

(cid:9)

k(u, v) =

1 if condition
0 otherwise,

(4)

(6)

(7)

123

Mach Learn (2016) 102:209–245

217

G(i)
1

G(j)
1

0

G(i)
0

0

G(j)
0

1

0

1

1

0

φ(G(i)
0

) = [2, 1, 3]

φ(G(j)
0

) = [2, 2, 2]

φ(G(i)
1

) = [1, 1, 3, 1]

φ(G(j)
1

) = [3, 1, 2, 0]

φ(G(i)
0

), φ(G(j)
0

) = 12

φ(G(i)
1

), φ(G(j)
1

) = 10

(a)

(b)

and G( j)

Fig. 1 Propagation kernel computation. Distributions, bins, count features, and kernel contributions for two
graphs G(i)
with binary node labels and one iteration of label propagation, cf. Eq. (3), as the
propagation scheme. Node-label distributions are decoded by color: white means p0,u = [1, 0], dark red
stands for p0,u = [0, 1], and the initial distributions for unlabeled nodes (light red) are p0,u = [1/2, 1/2]. a
Initial label distributions (t = 0), b updated label distributions (t = 1) (Color ﬁgure online)

where condition is an equality condition on the information of nodes u and v, we can compute
K efﬁciently by binning the node information, counting the respective bin strengths for all
graphs, and computing a base kernel among these counts. That is, we compute count features
φ(G

) for each graph and plug them into a base kernel (cid:12)·, ·(cid:13):

(i)
t

(cid:7)

K

(i)
t

G

, G

( j)
t

(cid:8)

(cid:10)

(cid:7)

(cid:8)

(cid:7)

=

φ

(i)
t

G

, φ

( j)
t

G

(cid:8)(cid:11)

.

(8)

, where the ith row of Φt , (Φt )i · = φ(G

In the simple case of a linear base kernel, the last step is just an outer product of count
vectors Φt Φ(cid:17)
). Now, for two graphs, binning
t
and counting can be done in O(ni + n j ) and the computation of the linear base kernel value
is O(|bins|). This is one of the main insights for efﬁcient graph-kernel computation and it
has already been exploited for labeled graphs in previous work (Shervashidze et al. 2011;
Neumann et al. 2012).

(i)
t

Figure 1 illustrates the propagation kernel computation for t = 0 and t = 1 for two
example graphs and Algorithm 1 summarizes the kernel computation for a graph database
{G(i)}i . From this general algorithm and Eqs. (4) and (6), we see that the two main components
to design a propagation kernel are
– the node kernel k(u, v) comparing propagated information, and
– the propagation scheme P

propagating the information within the graphs.

(i)
t+1

(i)
← P
t

The propagation scheme depends on the input graphs and we will give speciﬁc suggestions for
different graph types in Sect. 6. Before deﬁning the node kernels depending on the available
node information in Sect. 5, we brieﬂy discuss the general runtime complexity of propagation
kernels.

4.2 Complexity analysis

The total runtime complexity of propagation kernels for a set of n graphs with a total number
(cid:13)
, where n(cid:6) := maxi (ni ). For a pair
of N nodes and M edges is O
of graphs the runtime complexity of computing the count features, that is, binning the node
information and counting the bin strengths is O(ni + n j ). Computing and adding the kernel

(tmax − 1)M + tmax n2 n(cid:6)

(cid:12)

123

218

Mach Learn (2016) 102:209–245

contribution is O(|bins|), where |bins| is bounded by ni + n j . So, one iteration of the kernel
computation for all graphs is O(n2 n(cid:6)). Note that in practice |bins| (cid:18) 2n(cid:6) as we aim to bin
together similar nodes to derive a meaningful feature representation.

Feature computation basically depends on propagating node information along the edges
of all graphs. This operation depends on the number of edges and the information propagated,
so it is O((k + D)M) = O(M), where k is the number of node labels and D is the attribute
dimensionality. This operation has to be performed tmax − 1 times. Note that the number of
edges is usually much lower than N 2.

5 Propagation kernel component 1: node kernel

In this section, we deﬁne node kernels comparing propagated information appropriate for
the use in propagation kernels. Moreover, we introduce locality sensitive hashing, which is
used to discretize the distributions arsing from rw-based information propagation as well as
the continuous attributes directly.

5.1 Deﬁnitions

Above, we saw that one way to allow for efﬁcient computation of propagation kernels is to
restrict the range of the node kernels to {0, 1}. Let us now deﬁne the two components of the
node kernel [Eq. (5)] in this form. The label kernel can be represented as
(cid:9)

k(cid:2)(u, v) =

1
0

if h(cid:2)( pt,u) = h(cid:2)( pt,v);
otherwise,

where pt,u is the node-label distribution of node u at iteration t and h(cid:2)(·) is a quantization
function (Gersho and Gray 1991), more precisely a locality sensitive hash (lsh) function
(Datar and Indyk 2004), which will be introduced in more detail in the next section. Note
(i)
that pt,u denotes a row in the label distribution matrix P
, namely the row corresponding
t
to node u of graph G(i).

Propagation kernels can be computed for various kinds of attribute kernels as long as they

have the form of Eq. (7). The most rudimentary attribute kernel is

ka(u, v) =

(cid:9)

1
0

if ha(xu) = ha(xv);
otherwise,

(9)

(10)

where xu is the one-dimensional continuous attribute of node u and ha(·) is again an lsh
function. Figure 2 contrasts this simple attribute kernel for a one-dimensional attribute to
a thresholded Gaussian function and the Gaussian kernel commonly used to compare node
attributes in graph kernels. To deal with higher-dimensional attributes, we can choose the
attribute kernel to be the product of kernels on each attribute dimension:

ka(u, v) =

kad

(u, v), where

D(cid:14)

d=1
(cid:9)
1
0

kad

(u, v) =

(xu,d ) = had

(xv,d );

if had
otherwise,

(11)

where xu,d is the respective dimension of the attribute xu of node u. Note that each dimension
(·). However, analogous to the label kernel, we can also
now has its own lsh function had

123

219

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

(12)

Mach Learn (2016) 102:209–245

−2

−1.5

−1

−0.5

0.5

0

1

1.5

−2

−1.5

−1

−0.5

0.5

0

1

1.5

(b)

u
x

u
x

u
x

−2

−1.5

−1

−0.5

0.5

0

1

1.5

2
−2 −1.5 −1 −0.5

1

1.5

2

2
−2 −1.5 −1 −0.5

0.5

1

1.5

2

2
−2 −1.5 −1 −0.5

0.5

1

1.5

2

0.5

0
xv
ka(u, v)

(a)

0
xv

0
xv

(c)

Fig. 2 Attribute kernels. Three different attribute functions among nodes with continuous attributes xu and
xv ∈ [−2, 2]. a An attribute kernel suitable for the efﬁcient computation in propagation kernels (ka (u, v)), b
a thresholded Gaussian, and c a Gaussian kernel

deﬁne an attribute kernel based on propagated attribute distributions

ka(u, v) =

(cid:9)

1
0

if ha(qt,u) = ha(qt,v);
otherwise,

where qt,u is the attribute distribution of node u at iteration t. Next we explain the locality
sensitive hashing approach used to discretize distributions and continuous attributes directly.
In Sect. 6.3, we will then derive an efﬁcient way to propagate and hash continuous attribute
distributions.

5.2 Locality sensitive hashing

We now describe our quantization approach for implementing propagation kernels for graphs
with node-label distributions and continuous attributes. The idea is inspired by locality sen-
sitive hashing (Datar and Indyk 2004) which seeks quantization functions on metric spaces
where points “close enough” to each other in that space are “probably” assigned to the same
bin. In the case of distributions, we will consider each node-label vector as being an element
of the space of discrete probability distributions on k items equipped with an appropriate
probability metric. If we want to hash attributes directly, we simply consider metrics for
continuous values.

Deﬁnition 1 (Locality Sensitive Hash (lsh)) Let X be a metric space with metric d : X ×
X → R, and let Y = {1, 2, . . . , k(cid:2)}. Let θ > 0 be a threshold, c > 1 be an approximation
factor, and p1, p2 ∈ (0, 1) be the given success probabilities. A set of functions H from
X to Y is called a (θ, cθ, p1, p2)-locality sensitive hash if for any function h ∈ H chosen
uniformly at random, and for any two points x, x (cid:2) ∈ X , it holds that
– if d(x, x (cid:2)) < θ, then Pr(h(x) = h(x (cid:2))) > p1, and
– if d(x, x (cid:2)) > cθ , then Pr(h(x) = h(x (cid:2))) < p2.

It is known that we can construct lsh families for (cid:2) p spaces with p ∈ (0, 2] (Datar and
Indyk 2004). Let V be a real-valued random variable. V is called p-stable if for any
{x1, x2, . . . , xd }, xi ∈ R and independently sampled v1, v2, . . . , vd , we have
xi vi ∼
(cid:20)x(cid:20) p V .

(cid:2)

Explicit p-stable distributions are known for some p; for example, the standard Cauchy
distribution is 1-stable, and the standard normal distribution is 2-stable. Given the ability to
sample from a p-stable distribution V , we may deﬁne an lsh H on Rd with the (cid:2) p metric

123

220

Mach Learn (2016) 102:209–245

Algorithm 2 calculate- lsh

given: matrix X ∈ RN ×D, bin width w, metric m
if m = h then
X ←
X

√

end if
if m = h or m = l2 then
v ← rand- norm(D)
else if m = tv or m = l1 then

end if
b = w ∗ rand- unif()
h(X ) = ﬂoor((X ∗ v + b)/w)

v ← rand- norm(D)/rand- norm(D)

(cid:14) square root transformation

(cid:14) generate random projection vector
(cid:14) sample from N (0, 1)

(cid:14) sample from Cauchy(0, 1)

(cid:14) random offset b ∼ U [0, w]
(cid:14) compute hashes

(Datar and Indyk 2004). An element h of H is speciﬁed by three parameters: a width w ∈ R+,
a d-dimensional vector v whose entries are independent samples of V , and b drawn from
U[0, w]. Given these, h is then deﬁned as

h(x; w, v, b) =

(cid:15)

(cid:16)

.

v(cid:17)x + b
w

(13)

We may now consider h(·) to be a function mapping our distributions or attribute values to
integer-valued bins, where similar distributions end up in the same bin. Hence, we obtain
node kernels as deﬁned in Eqs. (9) and (12) in the case of distributions, as well as simple
attribute kernels as deﬁned in Eqs. (10) and (11). To decrease the probability of collision,
it is common to choose more than one random vector v. For propagation kernels, however,
we only use one hyperplane, as we effectively have tmax hyperplanes for the whole kernel
computation and the probability of a hash conﬂict is reduced over the iterations.

The intuition behind the expression in Eq. (13) is that p-stability implies that two vectors
that are close under the (cid:2) p norm will be close after taking the dot product with v; speciﬁcally,
(v(cid:17)x − v(cid:17)x(cid:2)) is distributed as (cid:20)x − x(cid:2)(cid:20) p V . So, in the case where we want to construct a
hashing for D-dimensional continuous node attributes to preserve (cid:2)1 (l1) or (cid:2)2 (l2) distance

dl1(xu, xv) =

|xu,d − xu,d |,

dl2(xu, xv) =

xu,d − xv,d

(cid:17)

D(cid:6)

(cid:12)

d=1

(cid:18)1/2
,

(cid:13)

2

we directly apply Eq. (13). In the case of distributions, we are concerned with the space of
discrete probability distributions on k elements, endowed with a probability metric d. Here
we speciﬁcally consider the total variation (tv) and Hellinger (h) distances:

dtv( pu, pv) = 1/2

| pu,i − pv,i |, dh( pu, pv) =

1/2

(cid:17)

k(cid:6)

(cid:12)√

√

pu,i −

pv,i

(cid:18)1/2
.

(cid:13)

2

i=1

D(cid:6)

d=1

k(cid:6)

i=1

The total variation distance is simply half the (cid:2)1 metric, and the Hellinger distance is a
scaled version of the (cid:2)2 metric after applying the map p (cid:23)→
p. We may therefore create a
locality-sensitive hash family for dtv by direct application of Eq. (13) and create a locality-
sensitive hash family for dh by using Eq. (13) after applying the square root map to our
label distributions. The lsh computation for a matrix X ∈ RN ×D, where xu is the row in X
corresponding to node u, is summarized in Algorithm 2.
123

√

Mach Learn (2016) 102:209–245

221

Algorithm 3 Propagation kernel for fully labeled graphs (speciﬁc parts compared to the
general computation (Algorithm 1) are marked in green (input) and blue (computation)–
Color version online)
given: graph database {G(i)}i , # iterations tmax, transition matrix T , bin width w, metric m, base kernel
(cid:12)·, ·(cid:13)
initialization: K ← 0, P0 ← δ(cid:2)(V )
for t ← 0 . . . tmax do

calculate- lsh(Pt , w, m)
for all graphs G(i) do

compute Φi· = φ(G

(i)
t

)

end for
Pt+1 ← T Pt
K ← K + (cid:12)Φ, Φ(cid:13)

end for

(cid:14) bin node information

(cid:14) count bin strengths

(cid:14) label diffusion
(cid:14) compute and add kernel contribution

6 Propagation kernel component 2: propagation scheme

As pointed out in the introduction, the input graphs for graph kernels may vary considerably.
One key to design efﬁcient and powerful propagation kernels is the choice of a propagation
scheme appropriate for the graph dataset at hand. By utilizing random walks (rws) we are able
to use efﬁcient off-the-shelf algorithms, such as label diffusion or label propagation (Szummer
and Jaakkola 2001; Zhu et al. 2003; Wu et al. 2012), to implement information propagation
within the input graphs. In this section, we explicitly deﬁne propagation kernels for fully
labeled, unlabeled, partially labeled, directed, and attributed graphs as well as for graphs
with a regular grid structure using appropriate rws. In each particular algorithm, the speciﬁc
parts changing compared to the general propagation kernel computation (Algorithm 1) will
be marked in color.

6.1 Labeled and unlabeled graphs

(cid:19)

(cid:2)

For fully labeled graphs we suggest the use of the label diffusion process from Eq. (1) as
the propagation scheme. Given a database of fully labeled graphs {G(i)}i=1,...,n with a total
number of N =
i ni nodes, label diffusion on all graphs can be efﬁciently implemented
by multiplying a sparse block-diagonal transition matrix T ∈ RN ×N , where the blocks
are the transition matrices T (i) of the respective graphs, with the label distribution matrix
∈ RN ×k. This can be done efﬁciently due to the sparsity of T .
Pt =
The propagation kernel computation for labeled graphs is summarized in Algorithm 3. The
speciﬁc parts compared to the general propagation kernel computation (Algorithm 1) for fully
labeled graphs are marked in green (input) and blue (computation). For unlabeled graphs we
suggest to set the label function to be the node degree (cid:2)(u) = degree(u) and then apply the
same pk computation as for fully labeled graphs.

(n)
, . . . , P
t

(1)
P
t

(cid:20)(cid:17)

6.2 Partially labeled and directed graphs

For partially labeled graphs, where some of the node labels are unknown, we sug-
gest label propagation as an appropriate propagation scheme. Label propagation differs
from label diffusion in the fact that before each iteration of the information propaga-
tion, the labels of the originally labeled nodes are pushed back (Zhu et al. 2003). Let

123

222

(cid:21)

P0,[labeled], P0,[unlabeled]

P0 =
represent the prior label distributions for the nodes of
all graphs in the graph database, where the distributions in P0,[labeled] represent observed
labels and P0,[unlabeled] are initialized uniformly. Then label propagation is deﬁned by

(cid:22)(cid:17)

Mach Learn (2016) 102:209–245

Pt,[labeled] ← P0,[labeled];
Pt+1 ← T Pt .

(14)

Note that this propagation scheme is equivalent to the one deﬁned in Eq. (3) using fully
absorbing rws. Other similar update schemes, such as “label spreading” (Zhou et al. 2003),
could be used in a propagation kernel as well. Thus, the propagation kernel computation for
partially labeled graphs is essentially the same as Algorithm 3, where the initialization for the
unlabeled nodes has to be adapted, and the (partial) label push back has to be added before
the node information is propagated. The relevant parts are the ones marked in blue. Note that
for graphs with large fractions of labeled nodes it might be preferable to use label diffusion
even though they are partially labeled.

To implement propagation kernels between directed graphs, we can proceed as above after
simply deriving transition matrices computed from the potentially non-symmetric adjacency
matrices. That is, for the propagation kernel computation only the input changes (marked in
green in Algorithm 3). The same idea allows weighted edges to be accommodated; again,
only the transition matrix has to be adapted. Obviously, we can also combine partially labeled
graphs with directed or weighted edges by changing both the blue and green marked parts
accordingly.

6.3 Graphs with continuous node attributes

Nowadays, learning tasks often involve graphs whose nodes are attributed with continuous
information. Chemical compounds can be annotated with the length of the secondary struc-
ture elements (the nodes) or measurements for various properties, such as hydrophobicity
or polarity. 3d point clouds can be enriched with curvature information, and images are
inherently composed of 3-channel color information. All this information can be modeled by
continuous node attributes. In Eq. (10) we introduced a simple way to deal with attributes.
The resulting propagation kernel essentially counts similar label arrangements only if the
corresponding node attributes are similar as well. Note that for higher-dimensional attributes
it can be advantageous to compute separate lshs per dimension, leading to the node kernel
introduced in Eq. (11). This has the advantage that if we standardize the attributes, we can
use the same bin-width parameter wa for all dimensions. In all our experiments we normalize
each attribute to have unit standard deviation and will set wa = 1. The disadvantage of this
method, however, is that the arrangement of attributes in the graphs is ignored.

In the following, we derive p2k, a variant of propagation kernels for attributed graphs
based on the idea of propagating both attributes and labels. That is, we model graph similar-
ity by comparing the arrangement of labels and the arrangement of attributes in the graph.
The attribute kernel for p2k is deﬁned as in Eq. (12); now the question is how to efﬁciently
propagate the continuous attributes and how to efﬁciently model and hash the distributions
of (multivariate) continuous variables. Let X ∈ RN ×D be the design matrix, where a row xu
represents the attribute vector of node u. We will associate with each node of each graph a
probability distribution deﬁned on the attribute space, qu, and will update these as attribute
information is propagated across graph edges as before. One challenge in doing so is ensuring
that these distributions can be represented with a ﬁnite description. The discrete label distri-
butions from before have naturally a ﬁnite number of dimensions and could be compactly

123

Mach Learn (2016) 102:209–245

223

Algorithm 4 Propagation kernel (p2k) for attributed graphs (speciﬁc parts compared to the
general computation (Algorithm 1) are marked in green (input) and blue (computation)–Color
version online).
given: graph database {G(i)}i , # iterations tmax, transition matrix T , bin widths wl , wa , metrics ml , ma ,
base kernel (cid:12)·, ·(cid:13)
initialization: K ← 0, P0 ← δ(cid:2)(V )
µ = X, Σ = cov(X ), W0 = I
y ← rand(num-samples)
for t ← 0 . . . tmax do

(cid:14) gm initialization
(cid:14) sample points for gm evaluations

hl ← calculate- lsh(Pt , wl , ml )
Qt ← evaluate- pdfs(µ, Σ, Wt , y)
ha ← calculate- lsh(Qt , wa , ma )
h ← hl ∧ ha
for all graphs G(i) do

compute Φi· = φ(G

(i)
t

)

end for
Pt+1 ← T Pt
Wt+1 ← T Wt
K ← K + (cid:12)Φ, Φ(cid:13)

end for

(cid:14) bin label distributions
(cid:14) evaluate gms at y
(cid:14) bin attribute distributions
(cid:14) combine label and attribute bins

(cid:14) count bin strengths

(cid:14) propagate label information
(cid:14) propagate attribute information
(cid:14) compute and add kernel contribution

represented and updated via the Pt matrices. We seek a similar representation for attributes.
Our proposal is to deﬁne the node-attribute distributions to be mixtures of D-dimensional
multivariate Gaussians, one centered on each attribute vector in X :

qu =

Wuv N (xv, Σ),

(cid:6)

v

where the sum ranges over all nodes v, Wu,· is a vector of mixture weights, and Σ is a
shared D × D covariance matrix for each component of the mixture. In particular, here
we set Σ to be the sample covariance matrix calculated from the N vectors in X . Now
the N × N row-normalized W matrix can be used to compactly represent the entire set of
attribute distributions. As before, we will use the graph structure to iteratively spread attribute
information, updating these W matrices, deriving a sequence of attribute distributions for each
node to use as inputs to node attribute kernels in a propagation kernel scheme.

We begin by deﬁning the initial weight matrix W0 to be the identity matrix; this is equivalent
to beginning with each node attribute distribution being a single Gaussian centered on the
corresponding attribute vector:

Now, in each propagation step the attribute distributions are updated by the distribution of
their neighboring nodes Qt+1 ← Qt . We accomplish this by propagating the mixture weights
W across the edges of the graph according to a row-normalized transition matrix T , derived
as in Sect. 3:

W0 = I ;
q0,u = N (xu, Σ).

Wt+1 ← T Wt = T t ;
(cid:6)
qt+1,u =

(Wt )uv N (xv, Σ).

v

We have described how attribute distributions are associated with each node and how they
are updated via propagating their weights across the edges of the graph. However, the weight
vectors contained in W are not themselves directly suitable for comparing in an attribute

(15)

123

224

Mach Learn (2016) 102:209–245

kernel ka, because any information about the similarity of the mean vectors is ignored. For
example, imagine that two nodes u and v had exactly the same attribute vector, xu = xv. Then
mass on the u component of the Gaussian mixture is interchangeable with mass on the v com-
ponent; a simple kernel among weight vectors cannot capture this. For this reason, we use a
vector more appropriate for kernel comparison. Namely, we select a ﬁxed set of sample points
in attribute space (in our case, chosen uniformly from the node attribute vectors in X ), evaluate
the pdfs of the Gaussian mixtures associated with each node at these points, and use this vector
to summarize the node information. This handles the exchangeability issue from above and
also allows a more compact representation for hash inputs; in our experiments, we used 100
sample points and achieved good performance. As before, these vectors can then be hashed
jointly or individually for each sample point. Note that the bin width wa has to be adapted
accordingly. In our experiments, we will use the latter option and set wa = 1 for all datasets.
The computational details of p2k are given in Algorithm 4, where the additional parts
compared to Algorithm 1 are marked in blue (computation) and green (input). An extension
to Algorithm 4 would be to reﬁt the gms after a couple of propagation iterations. We did not
consider reﬁtting in our experiments as the number of kernel iterations tmax was set to 10
or 15 for all datasets—following the descriptions in existing work on iterative graph kernels
(Shervashidze et al. 2011; Neumann et al. 2012).

6.4 Grid graphs

One of our goals in this paper is to compute propagation kernels for pixel grid graphs. A
graph kernel between grid graphs can be deﬁned such that two grids should have a high
kernel value if they have similarly arranged node information. This can be naturally captured
by propagation kernels as they monitor information spread on the grids. Naïvely, one could
think that we can simply apply Algorithm 3 to achieve this goal. However, given that the
space complexity of this algorithm scales with the number of edges and even medium sized
images such as texture patches will easily contain thousands of nodes, this is not feasible. For
example considering 100 × 100-pixel image patches with an 8-neighborhood graph struc-
ture, the space complexity required would be 2.4 million units3 (ﬂoating point numbers) per
graph. Fortunately, we can exploit the ﬂexibility of propagation kernels by exchanging the
propagation scheme. Rather than label diffusion as used earlier, we employ discrete convo-
lution; this idea was introduced for efﬁcient clustering on discrete lattices (Bauckhage and
Kersting 2013). In fact, isotropic diffusion for denoising or sharpening is a highly developed
technique in image processing (Jähne 2005). In each iteration, the diffused image is derived
as the convolution of the previous image and an isotropic (linear and space-invariant) ﬁlter.
In the following, we derive a space- and time-efﬁcient way of computing propagation kernels
for grid graphs by means of convolutions.

6.4.1 Basic Deﬁnitions

Given that the neighborhood of a node is the subgraph induced by all its adjacent vertices,
we deﬁne a d-dimensional grid graph as a lattice graph whose node embedding in Rd forms
a regular square tiling and the neighborhoods N of each non-border node are isomorphic
(ignoring the node information). Figure 3 illustrates a regular square tiling and several iso-
morphic neighborhoods of a 2-dimensional grid graph. If we ignore boundary nodes, a grid

3 Using a coordinate list sparse representation, the memory usage per pixel grid graph for Algorithm 3 is
O(3m1m2 p), where m1 × m2 are the grid dimensions and p is the size of the pixel neighborhood.

123

Mach Learn (2016) 102:209–245

225

(a)

(b)

(c)

(d)

Fig. 3 Grid graph. Regular square tiling (a) and three example neighborhoods (b–d) for a 2-dimensional
grid graph derived from line graphs L7 and L6. a Square tiling, b 4-neighborhood, c 8-neighborhood, d
non-symmetric neighborhood

graph is a regular graph; i.e., each non-border node has the same degree. Note that the size
of the border depends on the radius of the neighborhood. In order to be able to neglect the
special treatment for border nodes, it is common to view the actual grid graph as a ﬁnite
section of an actually inﬁnite graph.

A grid graph whose node embedding in Rd forms a regular square tiling can be derived

from the graph Cartesian product of line graphs. So, a two-dimensional grid is deﬁned as

G(i) = L mi,1

× L mi,2

,

where L mi,1 is a line graph with mi,1 nodes. G(i) consists of ni = mi,1 mi,2 nodes, where
non-border nodes have the same number of neighbors. Note that the grid graph G(i) only
speciﬁes the node layout in the graph but not the edge structure. The edges are given by
the neighborhood N which can be deﬁned by any arbitrary matrix B encoding the weighted
adjacency of its center node. The nodes, being for instance image pixels, can carry discrete
or continuous vector-valued information. Thus, in the most-general setting the database of
grid graphs is given by G = {G(i)}i=1,...,n with G(i) = (V (i), N , (cid:2)), where (cid:2): V (i) → L
with L = ([k], RD). Commonly used neighborhoods N are the 4-neighborhood and the
8-neighborhood illustrated in Fig. 3b, c.

6.4.2 Discrete Convolution

The general convolution operation on two functions f and g is deﬁned as

f (x) ∗ g(x) = ( f ∗ g)(x) =

f (τ ) g(x − τ ) dτ.

(cid:23) ∞

−∞

That is, the convolution operation produces a modiﬁed, or ﬁltered, version of the original
function f . The function g is called a ﬁlter. For two-dimensional grid graphs interpreted as
discrete functions of two variables x and y, e.g., the pixel location, we consider the discrete
spatial convolution deﬁned by:

f (x, y) ∗ g(x, y) = ( f ∗ g)(x, y) =

f (i, j) g(x − i, y − j),

∞(cid:6)

∞(cid:6)

i=−∞

j=−∞

where the computation is in fact done for ﬁnite intervals. As convolution is a well-studied
operation in low-level signal processing and discrete convolution is a standard operation in
digital image processing, we can resort to highly developed algorithms for its computation;
see for example Chapter 2 in Jähne (2005). Convolutions can be computed efﬁciently via the
fast Fourier transformation in O(ni log ni ) per graph.

123

226

Mach Learn (2016) 102:209–245

Algorithm 5 Propagation kernel for grid graphs (speciﬁc parts compared to the general
computation (Algorithm 1) are marked in green (input) and blue (computation)–Color version
online).
given: graph database {G(i)}i , # iterations tmax, ﬁlter matrix B, bin width w, metric m, base kernel (cid:12)·, ·(cid:13)
(i)
initialization: K ← 0, P
0
for t ← 0 . . . tmax do

← δ(cid:2)(Vi ) ∀i

(i)
calculate- lsh({P
t
for all graphs G(i) do

}i , w, m)

compute Φi· = φ(G

(i)
t

)

end for
for all graphs G(i)
(i, j)
← P
t

(i, j)
P
t+1
end for
K ← K + (cid:12)Φ, Φ(cid:13)

end for

and labels j do
∗ B

(cid:14) bin node information

(cid:14) count bin strengths

(cid:14) discrete convolution

(cid:14) compute and add kernel contribution

6.4.3 Efﬁcient Propagation Kernel Computation

Now let G = {G(i)}i be a database of grid graphs. To simplify notation, however without
loss of generality, we assume two-dimensional grids G(i) = L mi,1
× L mi,2 . Unlike in the
case of general graphs, each graph now has a natural two-dimensional structure, so we will
update our notation to reﬂect this structure. Instead of representing the label probability
distributions of each node as rows in a two-dimensional matrix, we now represent them
(i)
∈ Rmi,1×mi,2×k. Modifying the
in the third dimension of a three-dimensional tensor P
t
structure makes both the exposition more clear and also enables efﬁcient computation. Now,
we can simply consider discrete convolution on k matrices of label probabilities P (i, j) per
grid graph G(i), where P (i, j) ∈ Rmi,1×mi,2 contains the probabilities of all nodes in G(i)
(i)
of being label j and j ∈ {1, . . . , k}. For observed labels, P
is again initialized with a
0
Kronecker delta distribution across the third dimension and, in each propagation step, we
perform a discrete convolution of each matrix P (i, j) per graph. Thus, we can create various
propagation schemes efﬁciently by applying appropriate ﬁlters, which are represented by
matrices B in our discrete case. We use circular symmetric neighbor sets Nr, p as introduced
in Ojala et al. (2002), where each pixel has p neighbors which are equally spaced pixels on
a circle of radius r . We use the following approximated ﬁlter matrices in our experiments:

⎤

⎦ , N1,8 =

N1,4 =

N2,16 =

⎡

⎣

⎡

⎢
⎢
⎢
⎢
⎣

0
0.25
0
0.01
0.06
0.09
0.06
0.01

0.25
0
0.25
0.06
0.04
0
0.04
0.06

0
0.25
0
0.09
0
0
0
0.09

0.06
0.04
0
0.04
0.06

⎡

⎣

0.06
0.17
0.06
⎤

⎥
⎥
⎥
⎥
⎦

.

0.01
0.06
0.09
0.06
0.01

0.17
0.05
0.17

⎤

0.06
0.17
0.06

⎦ , and

(16)

The propagation kernel computation for grid graphs is summarized in Algorithm 5, where
the speciﬁc parts compared to the general propagation kernel computation (Algorithm 1)
are highlighted in green (input) and blue (computation). Using fast Fourier transformation,
the time complexity of Algorithm 5 is O
. Note that for
the purpose of efﬁcient computation, calculate- lsh has to be adapted to take the label

(tmax − 1)N log N + tmax n2 n(cid:6)

(cid:13)

(cid:12)

123

Mach Learn (2016) 102:209–245

227

(i)
distributions {P
}i as a set of 3-dimensional tensors. By virtue of the invariance of the
t
convolutions used, propagation kernels for grid graphs are translation invariant, and when
using the circular symmetric neighbor sets they are also 90-degree rotation invariant. These
properties make them attractive for image-based texture classiﬁcation. The use of other ﬁlters
implementing for instance anisotropic diffusion depending on the local node information is
a straightforward extension.

7 Experimental evaluation

Our intent here is to investigate the power of propagation kernels (pks) for graph classiﬁcation.
Speciﬁcally, we ask:
(Q1) How sensitive are propagation kernels with respect to their parameters, and how should

propagation kernels be used for graph classiﬁcation?

(Q2) How sensitive are propagation kernels to missing and noisy information?
(Q3) Are propagation kernels more ﬂexible than state-of-the-art graph kernels?
(Q4) Can propagation kernels be computed faster than state-of-the-art graph kernels while

achieving comparable classiﬁcation performance?

Towards answering these questions, we consider several evaluation scenarios on diverse
graph datasets including chemical compounds, semantic image scenes, pixel texture images,
and 3d point clouds to illustrate the ﬂexibility of pks.

7.1 Datasets

The datasets used for evaluating propagation kernels come from a variety of different domains
and thus have diverse properties. We distinguish graph databases of labeled and attributed
graphs, where attributed graphs usually also have label information on the nodes. Also, we
separate image datasets where we use the pixel grid graphs from general graphs, which
have varying node degrees. Table 1 summarizes the properties of all datasets used in our
experiments.4

7.1.1 Labeled Graphs

For labeled graphs, we consider the following benchmark datasets from bioinformatics:
mutag, nci1, nci109, and d&d. mutag contains 188 sets of mutagenic aromatic and
heteroaromatic nitro compounds, and the label refers to their mutagenic effect on the Gram-
negative bacterium Salmonella typhimurium (Debnath et al. 1991). nci1 and nci109 are
anti-cancer screens, in particular for cell lung cancer and ovarian cancer cell lines, respec-
tively (Wale and Karypis 2006). d&d consists of 1178 protein structures (Dobson and Doig
2003), where the nodes in each graph represent amino acids and two nodes are connected
by an edge if they are less than 6 Ångstroms apart. The graph classes are enzymes and
non-enzymes.

7.1.2 Partially Labeled Graphs

The two real-world image datasets msrc 9-class and msrc 21-class5 are state-of-the-art
datasets in semantic image processing originally introduced in Winn et al. (2005). Each

4 All datasets are publicly available for download from http://tiny.cc/PK_MLJ_data.
5 http://research.microsoft.com/en-us/projects/objectclassrecognition/.

123

228

Mach Learn (2016) 102:209–245

# Graphs Median #

Max #
nodes

Total #
nodes

# Node
labels

# Graph
labels

Attr. dim.

Table 1 Dataset statistics and properties

Dataset

Properties

mutag

nci1

nci109

d&d

msrc9

msrc21

db

synthetic

enzymes

proteins
pro-full
bzr

cox2

dhfr

brodatz

plants

188

4110

4127

1178

221

563

41

300

600

1113

1113

405

467

756

2048

2957

nodes

17.5
27

26

241

40

76

964

100

32

26

26

35

41

42

4096

4725

28

111

111

5748

55

141

5037

100

126

620

620

57

56

71

4096

5625

3371
122, 747
122, 494
334, 925
8968
43, 644
56, 468
30, 000
19, 580
43, 471
43, 471
14, 479
19, 252
32, 075
8, 388, 608
13, 587, 375

7

37

38

82

10

24

5
−

10

3

3

3

8

9

3

5

20

11

2

2

2

2

8

2

6

2

2

2

2

2

32

6

−

−

−

−

−

−

1

1

18

1

29

3

3

3
−

−

image is represented by a conditional Markov random ﬁeld graph, as illustrated in Fig. 4a, b.
The nodes of each graph are derived by oversegmenting the images using the quick shift algo-
rithm,6 resulting in one graph among the superpixels of each image. Nodes are connected if
the superpixels are adjacent, and each node can further be annotated with a semantic label.
Imagining an image retrieval system, where users provide images with semantic information,
it is realistic to assume that this information is only available for parts of the images, as it
is easier for a human annotator to label a small number of image regions rather than the
full image. As the images in the msrc datasets are fully annotated, we can derive semantic
(ground-truth) node labels by taking the mode ground-truth label of all pixels in the corre-
sponding superpixel. Semantic labels are, for example, building, grass, tree, cow, sky, sheep,
boat, face, car, bicycle, and a label void to handle objects that do not fall into one of these
classes. We removed images consisting of solely one semantic label, leading to a classiﬁcation
task among eight classes for msrc9 and 20 classes for msrc21.

7.1.3 Attributed Graphs

To evaluate the ability of pks to incorporate continuous node attributes, we consider the
attributed graphs used in Feragen et al. (2013), Kriege and Mutzel (2012). Apart from one
synthetic dataset (synthetic), the graphs are all chemical compounds (enzymes, proteins,
pro-full, bzr, cox2, and dhfr). synthetic comprises 300 graphs with 100 nodes, each
endowed with a one-dimensional normally distributed attribute and 196 edges each. Each
graph class, A and B, has 150 examples, where in A, 10 node attributes were ﬂipped randomly

6 http://www.vlfeat.org/overview/quickshift.html.

123

Mach Learn (2016) 102:209–245

229

(a)

(b)

(c)

Fig. 4 Semantic scene and point cloud graphs. The rgb image in (a) is represented by a graph of superpixels
(b) with semantic labels b = building, c = car, v = void, and ? = unlabeled. c Point clouds of household
objects represented by labeled 4-nn graphs with part labels top (yellow), middle (blue), bottom (red), usable-
area (cyan), and handle (green). Edge colors are derived from the adjacent nodes. a rgb image, b superpixel
graph, c point cloud graphs (Color ﬁgure online)

and in B, 5 were ﬂipped randomly. Further, noise drawn from N (0, 0.452) was added to the
attributes in B. proteins is a dataset of chemical compounds with two classes (enzyme and
non-enzyme) introduced in Dobson and Doig (2003). enzymes is a dataset of protein tertiary
structures belonging to 600 enzymes from the brenda database (Schomburg et al. 2004). The
graph classes are their ec (enzyme commission) numbers which are based on the chemical
reactions they catalyze. In both datasets, nodes are secondary structure elements (sse), which
are connected whenever they are neighbors either in the amino acid sequence or in 3d space.
Node attributes contain physical and chemical measurements including length of the sse in
Ångstrom, its hydrophobicity, its van der Waals volume, its polarity, and its polarizability. For
bzr, cox2, and dhfr—originally used in Mahé and Vert (2009)—we use the 3d coordinates
of the structures as attributes.

7.1.4 Point Cloud Graphs

In addition, we consider the object database db,7 introduced in Neumann et al. (2013). db is
a collection of 41 simulated 3d point clouds of household objects. Each object is represented
by a labeled graph where nodes represent points, labels are semantic parts (top, middle,
bottom, handle, and usable-area), and the graph structure is given by a k-nearest neighbor
(k-nn) graph w.r.t. Euclidean distance of the points in 3d space, cf. Fig. 4c. We further
endowed each node with a continuous curvature attribute approximated by its derivative, that
is, by the tangent plane orientations of its incident nodes. The attribute of node u is given by
xu =
v∈N (u) 1 − |nu · nv|, where nu is the normal of point u and N (u) are the neighbors
of node u. The classiﬁcation task here is to predict the category of each object. Examples of
the 11 categories are glass, cup, pot, pan, bottle, knife, hammer, and screwdriver.

(cid:2)

7.1.5 Grid Graphs

We consider a classical benchmark dataset for texture classiﬁcation (brodatz) and a dataset
for plant disease classiﬁcation (plants). All graphs in these datasets are grid graphs derived
from pixel images. That is, the nodes are image pixels connected according to circular sym-
metric neighbor sets Nr, p as exempliﬁed in Eq. (16). Node labels are computed from the rgb
color values by quantization.

7 http://www.ﬁrst-mm.eu/data.html.

123

230

Mach Learn (2016) 102:209–245

Fig. 5 Example images from brodatz (a, b) and plants (c, d) and the corresponding quantized versions
with three colors (e, f) and ﬁve colors (g, h). a bark, b grass, c phoma, d cercospora, e bark-3, f grass-3, g
phoma-5, h cercospora-5 (Color ﬁgure online)

brodatz,8 introduced in Valkealahti and Oja (1998), covers 32 textures from the Brodatz
album with 64 images per class comprising the following subsets of images: 16 “original”
images (o), 16 rotated versions (r), 16 scaled versions (s), and 16 rotated and scaled versions
(rs) of the “original” images. Figure 5a, b show example images with their corresponding
quantized versions (e) and (f). For parameter learning, we used a random subset of 20 % of
the original images and their rotated versions, and for evaluation we use test suites similar
to the ones provided with the dataset.9 All train/test splits are created such that whenever an
original image (o) occurs in one split, their modiﬁed versions (r,s,rs) are also included in
the same split.

The images in plants, introduced in Neumann et al. (2014), are regions showing disease
symptoms extracted from a database of 495 rgb images of beet leaves. The dataset has six
classes: ﬁve disease symptoms cercospora, ramularia, pseudomonas, rust, and phoma, and
one class for extracted regions not showing a disease symptom. Figure 5c, d illustrates two
regions and their quantized versions (g) and (h). We follow the experimental protocol in
Neumann et al. (2014) and use 10 % of the full data covering a balanced number of classes
(296 regions) for parameter learning and the full dataset for evaluation. Note that this dataset
is highly imbalanced, with two infrequent classes accounting for only 2 % of the examples
and two frequent classes covering 35 % of the examples.

7.2 Experimental protocol

We implemented propagation kernels in Matlab10 and classiﬁcation performance on all
datasets except for db is evaluated by running c- svm classiﬁcations using libSVM.11

8 http://www.ee.oulu.ﬁ/research/imag/texture/image_data/Brodatz32.html.
9 The test suites provided with the data are incorrect; we use a corrected version.
10 https://github.com/marionmari/propagation_kernels.
11 http://www.csie.ntu.edu.tw/~cjlin/libsvm/.

123

Mach Learn (2016) 102:209–245

231

For the parameter analysis (Sect. 7.3), the cost parameter c was learned on the full dataset
(c ∈ {10−3, 10−1, 101, 103} for normalized kernels and c ∈ {10−3, 10−2, 10−1, 100} for
unnormalized kernels), for the sensitivity analysis (Sect. 7.4), it was set to its default value
of 1 for all datasets, and for the experimental comparison with existing graph kernels
(Sect. 7.5), we learned it via 5-fold cross-validation on the training set for all methods
(c ∈ {10−7, 10−5, . . . , 105, 107} for normalized kernels and c ∈ {10−7, 10−5, 10−3, 10−1}
for unnormalized kernels). The number of kernel iterations tmax was learned on the training
splits (tmax ∈ {0, 1, . . . , 10} unless stated otherwise). Reported accuracies are an average of
10 reruns of a stratiﬁed 10-fold cross-validation.

For db, we follow the protocol introduced in Neumann et al. (2013). We perform a leave-
one-out (loo) cross validation on the 41 objects in db, where the kernel parameter tmax is
learned on each training set again via loo. We further enhanced the nodes by a standardized
continuous curvature attribute, which was only encoded in the edge weights in previous work
(Neumann et al. 2013).

For all pks, the lsh bin-width parameters were set to wl = 10−5 for labels and to wa = 1
for the normalized attributes, and as lsh metrics we chose ml = tv and ma = l1 in all
experiments. Before we evaluate classiﬁcation performance and runtimes of the proposed
propagation kernels, we analyze their sensitivity towards the choice of kernel parameters and
with respect to missing and noisy observations.

7.3 Parameter analysis

To analyze parameter sensitivity with respect to the kernel parameters w (lsh bin width)
and tmax (number of kernel iterations), we computed average accuracies over 10 randomly
generated test sets for all combinations of w and tmax, where w ∈ {10−8, 10−7, . . . , 10−1}
and tmax ∈ {0, 1, . . . , 14} on mutag, enzymes, nci1, and db. The propagation kernel
computation is as described in Algorithm 3, that is, we used the label information on the nodes
and the label diffusion process as propagation scheme. To assess classiﬁcation performance,
we performed a 10-fold cross validation (cv). Further, we repeated each of these experiments
with the normalized kernel, where normalization means dividing each kernel value by the
square root of the product of the respective diagonal entries. Note that for normalized kernels
we test for larger svm cost values. Figure 6 shows heatmaps of the results.

In general, we see that the pk performance is relatively smooth, especially if w < 10−3
and tmax > 4. Speciﬁcally, the number of iterations leading to the best results are in the
range from {4, . . . , 10} meaning that we do not have to use a larger number of iterations in
the pk computations, helping to keep a low computation time. This is especially important
for parameter learning. Comparing the heatmaps of the normalized pk to the unnormalized
pk leads to the conclusion that normalizing the kernel matrix can actually hurt performance.
This seems to be the case for the molecular datasets mutag and nci1. For mutag, Fig. 6a, b,
the performance drops from 88.2 to 82.9 %, indicating that for this dataset the size of the
graphs, or more speciﬁcally the amount of labels from the different kind of node classes, are a
strong class indicator for the graph label. Nevertheless, incorporating the graph structure, i.e.,
comparing tmax = 0 to tmax = 10, can still improve classiﬁcation performance by 1.5 %. For
other prediction scenarios such as the object category prediction on the db dataset, Fig. 6g, h,
we actually want to normalize the kernel matrix to make the prediction independent of the
object scale. That is, a cup scanned from a larger distance being represented by a smaller
graph is still a cup and should be similar to a larger cup scanned from a closer view. So, for
our experiments on object category prediction we will use normalized graph kernels whereas
for the chemical compounds we will use unnormalized kernels unless stated otherwise.

123

232

x
a
m
t

x
a
m
t

0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

x
a
m
t

x
a
m
t

0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

Mach Learn (2016) 102:209–245

90%

85%

80%

75%

70%

85%

80%

75%

70%

65%

0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

x
a
m
t

x
a
m
t

x
a
m
t

x
a
m
t

0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

50%

45%

40%

35%

30%

25%

80%

75%

70%

65%

60%

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

(a)

(b)

(c)

(d)

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

(e)

(f)

(g)

(h)

Fig. 6 Parameter sensitivity of pk. The plots show heatmaps of average accuracies (tenfold cv) of pk (labels
only) w.r.t. the bin widths parameter w and the number of kernel iterations tmax for four datasets mutag,
enzymes, nci1, and db. In panels (a, c, e, g) we used the kernel matrix directly, in panels (b, d, f, h) we
normalized the kernel matrix. The svm cost parameter is learned for each combination of w and tmax on
the full dataset. × marks the highest accuracy. a mutag; b mutag, normalized; c enzymes; d enzymes,
normalized; e nci1; f nci1, normalized; g db and h db, normalized

Recall that our propagation kernel schemes are randomized algorithms, as there is ran-
domization inherent in the choice of hyperplanes used during the lsh computation. We ran
a simple experiment to test the sensitivity of the resulting graph kernels with respect to the
hyperplane used. We computed the pk between all graphs in the datasets mutag, enzymes,
msrc9, and msrc21 with tmax = 10 100 times, differing only in the random selection of the
lsh hyperplanes. To make comparisons easier, we normalized each of these kernel matri-
ces. We then measured the standard deviation of each kernel entry across these repetitions
to gain insight into the stability of the pk to changes in the lsh hyperplanes. The median
standard deviations were: mutag: 5.5 × 10−5, enzymes: 1.1 × 10−3, msrc9: 2.2 × 10−4,
and msrc21: 1.1 × 10−4. The maximum standard deviations over all pairs of graphs were:
mutag: 6.7 × 10−3, enzymes: 1.4 × 10−2, msrc9: 1.4 × 10−2, and msrc21: 1.1 × 10−2.
Clearly the pk values are not overly sensitive to random variation due to differing random
lsh hyperplanes.

In summary, we can answer (Q1) by concluding that pks are not overly sensitive to the
random selection of the hyperplane as well as to the choice of parameters and we propose
to learn tmax ∈ {0, 1, . . . , 10} and ﬁx w ≤ 10−3. Further, we recommend to decide on using
the normalized version of pks only when graph size invariance is deemed important for the
classiﬁcation task.

7.4 Sensitivity to missing and noisy information

This section analyzes the performance of propagation kernels in the presence of missing and
noisy information.

To asses how sensitive propagation kernels are to missing information, we randomly
selected x % of the nodes in all graphs of db and removed their labels (labels) or attributes
(attr), where x ∈ {0, 10, . . . , 90, 95, 98, 99, 99.5, 100}. To study the performance when

123

Mach Learn (2016) 102:209–245

233

both label and attribute information is missing, we selected (independently) x % of the nodes
to remove their label information and x% of the nodes to remove their attribute information
(labels & attr). Figure 7 shows the average accuracy of 10 reruns. While we see that
the accuracy decreases with more missing information, the performance remains stable in
the case when attribute information is missing. This suggests that the label information is
more important for the problem of object category prediction. Further, the standard error
is increasing with more missing information, which corresponds to the intuition that fewer
available information results in a higher variance in the predictions.

We also compare the predictive performance of propagation kernels when only some
graphs, as for instance graphs at prediction time, have missing labels. Therefore, we divided
the graphs of the following datasets, mutag, enzymes, msrc9, and msrc21, into two groups.
For one group (fully labeled) we consider all nodes to be labeled, and for the other group (miss-
ing labels) we remove x % of the labels at random, where x ∈ {10, 20, . . . , 90, 91, . . . , 99}.
Figure 8 shows average accuracies over 10 reruns for each dataset. Whereas for mutag we
do not observe a signiﬁcant difference of the two groups, for enzymes the graphs with miss-
ing labels could only be predicted with lower accuracy, even when only 20 % of the labels
were missing. For both msrc datasets, we observe that we can still predict the graphs with
full label information quite accurately; however, the classiﬁcation accuracy for the graphs
with missing information decreases signiﬁcantly with the amount of missing labels. For all
datasets removing even 99 % of the labels still leads to better classiﬁcation results than a ran-
dom predictor. This result may indicate that the size of the graphs itself bears some predictive
information. This observation conﬁrms the results from Sect. 7.3.

The next experiment analyzes the performance of propagation kernels when label infor-
mation is encoded as attributes in a one-hot encoding. We also examine how sensitive they
are in the presence of label noise. We corrupted the label encoding by an increasing amount
of noise. A noisy label distribution vector nu was generated by sampling nu,i ∼ U(0, 1) and
nu,i = 1. Given a noise level α, we used the following values encoded
normalizing so that
as attributes

(cid:2)

xu ← (1 − α) xu + α nu.
Figure 9 shows average accuracies over 10 reruns for msrc9, msrc21, mutag, and enzymes.
First, we see that using the attribute encoding of the label information in a p2k variant only
propagating attributes achieves similar performances to propagating the labels directly in
pk. This conﬁrms that the Gaussian mixture approximation of the attribute distributions is
a reasonable choice. Moreover, we can observe that the performance on msrc9 and mutag
is stable across the tested noise levels. For msrc21 the performance drops for noise levels
larger than 0.3. Whereas the same happens for enzymes, adding a small amount of noise
(α = 0.1) actually increases performance. This could be due to a regularization effect caused
by the noise and should be investigated in future work.

Finally, we performed an experiment to test the sensitivity of pks with respect to noise in
edge weights. For this experiment, we used the datasets bzr, cox2, and dhfr, and deﬁned
edge weights between connected nodes according to the distance between the corresponding
structure elements in 3d space. Namely, the edge weight (before row normalization) was
taken to be the inverse Euclidean distance between the incident nodes. Given a noise-level
σ , we corrupted each edge weight by multiplying by random log-normally distributed noise:
wi j ← exp(log(wi j ) + ε),
where ε ∼ N (0, σ 2). Figure 10 shows the average test accuracy across ten repetitions of 10-
fold cross-validation for this experiment. The bzr and cox2 datasets tolerated a large amount

123

234

Mach Learn (2016) 102:209–245

)

%

(

c
c
a

.
g
v
a

90

75

60

45

30

15

0

fully labeled
overall
missing labels
random

fully labeled
overall
missing labels
random

)

%

(

c
c
a

.
g
v
a

)

%

(

c
c
a

.
g
v
a

90

85

80

75

70

65

60

55

50

90

80

70

60

50

40

30

20

10

123

attr

labels

labels & attr

random

)

%

(

c
c
a

.
g
v
a

)

%

(

c
c
a

.
g
v
a

40

35

30

25

20

15

10

100

80

60

40

20

0

fully labeled
overall
missing labels
random

fully labeled
overall
missing labels
random

0

10 20 30 40 50 60 70 80 90 95 98 99 99.5 100

missing information (%)

Fig. 7 Missing information versus accuracy on db. We plot missing information (%) versus avg. accuracy
(%) ± standard error of p2k on db. For labels only label information is missing, for attr only attribute
information is missing and for labels & attr both is missing. The reported accuracies are averaged over ten
reruns. random indicates the result of a random predictor

10 20 30 40 50 60 70 80 90 91 92 93 94 95 96 97 98 99
missing information (%)
(a)

10 20 30 40 50 60 70 80 90 91 92 93 94 95 96 97 98 99
missing information (%)

(b)

10 20 30 40 50 60 70 80 90 91 92 93 94 95 96 97 98 99
missing information (%)
(c)

10 20 30 40 50 60 70 80 90 91 92 93 94 95 96 97 98 99

missing information (%)

(d)

Fig. 8 Missing node labels. We plot missing labels (%) versus avg. accuracy (%) ± standard error for mutag,
enzymes, msrc9, and msrc21. We divided the graphs randomly in two equally sized groups (fully labeled
and missing labels). The reported accuracies are averaged over 10 reruns. random indicates the result of a
predictor randomly choosing the class label. a mutag, b enzymes, c msrc9, d msrc21

Mach Learn (2016) 102:209–245

235

)

%

(

c
c
a

.
g
v
a

95

90

85

80

75

70

)

%

(

c
c
a

.
g
v
a

50

45

40

35

30

25

msrc9
msrc21
mutag

0.2
noise level α

0

0.1

0.3

0.4

0

0.1

0.3

0.4

Fig. 9 Noisy node labels. Noise level α is plotted versus avg. accuracy ± standard error of 10 reruns on
msrc9, msrc21, mutag, and enzymes when encoding the labels as k-dimensional attributes using a one-hot
representation and attribute propagation as in Algorithm 4. The dashed lines indicate the performance when
using the usual label encoding without noise and Algorithm 3

enzymes

0.2
noise level α

)

%

(

c
c
a

.
g
v
a

90

85

80

75

70

65

60

bzr
cox2
dhfr

0

10−4

10−2

10−1

10−3
noise level σ

Fig. 10 Noisy edge weights. Noise level σ is plotted versus avg. accuracy ± standard deviation of 10 reruns
on bzr, cox2, and dhfr

of edge-weight noise without a large effect on predictive performance, whereas dhfr was
somewhat more sensitive to larger noise levels.

Summing up these experimental results we answer (Q2) by concluding that propagation

kernels behave well in the presence of missing and noisy information.

7.5 Comparison to existing graph kernels

We compare classiﬁcation accuracy and runtime of propagation kernels (pk) with the follow-
ing state-of-the-art graph kernels: the Weisfeiler–Lehman subtree kernel (wl) (Shervashidze
et al. 2011), the shortest path kernel (sp) (Borgwardt and Kriegel 2005), the graph hop-
per kernel (gh) (Feragen et al. 2013), and the common subgraph matching kernel (csm)
(Kriege and Mutzel 2012). Table 2 lists all graph kernels and the types of information they

123

Mach Learn (2016) 102:209–245

236

pk

wl

sp

gh

csm

Table 2 Graph kernels and their intended use

Kernel

Information type

Node labels

Partial labels

Edge weights

Edge labels Node attributes Huge grids

Yes

Yes

Yes

Yes

Yes

Yes

–

–

–

–

Yes

–

Yes

Yes

Yes

–

–

–

Yes

Yes

Yes

–

Yes

Yes

Yes

(fast scaling)

Yes

–

–

–

–

are intended for. For all wl computations, we used the fast implementation12 introduced in
(Kersting et al. 2014). In sp, gh, and csm, we used a Dirac kernel to compare node labels and
a Gaussian kernel ka(u, v) = exp(−γ (cid:20)xu − xv(cid:20)2) with γ = 1/D for attribute information, if
feasible. csm for the bigger datasets (enzymes, proteins, synthetic) was computed using
a Gaussian truncated for inputs with (cid:20)xu − xv(cid:20) > 1. We made this decision to encourage
sparsity in the generated (node) kernel matrices, reducing the size of the induced product
graphs and speeding up computation. Note that this is technically not a valid kernel between
nodes; nonetheless, the resulting graph kernels were always positive deﬁnite. For pk and wl
the number of kernel iterations (tmax or hmax) and for csm the maximum size of subgraphs
(k) was learned on the training splits via 10-fold cross validation. For all runtime experiments
all kernels were computed for the largest value of tmax, hmax, or k, respectively. We used
a linear base kernel for all kernels involving count features, and attributes, if present, were
standardized. Further, we considered two baselines that do not take the graph structure into
account. labels, corresponding to a pk with tmax = 0, only compares the label proportions
in the graphs and a takes the mean of a Gaussian node kernel among all pairs of nodes in the
respective graphs.

7.5.1 Graph classiﬁcation on benchmark data

In this section, we consider graph classiﬁcation for fully labeled, partially labeled, and
attributed graphs.
Fully labeled graphs The experimental results for labeled graphs are shown in Table 3. On
mutag, the baseline using label information only (labels) already gives the best perfor-
mance indicating that for this dataset the actual graph structure is not adding any predictive
information. On nci1 and nci109, wl performs best; however, propagation kernels come in
second while being computed over one minute faster. Although sp can be computed quickly, it
performs signiﬁcantly worse than pk and wl. This is also the case for gh, whose computation
time is signiﬁcantly higher. In general, the results on labeled graphs show that propagation
kernels can be computed faster than state-of-the-art graph kernels but achieve comparable
classiﬁcation performance, thus question (Q4) can be answered afﬁrmatively.
Partially labeled graphs To assess the predictive performance of propagation kernels on
partially labeled graphs, we ran the following experiments 10 times. We randomly removed
20–80 % of the node labels in all graphs in msrc9 and msrc21 and computed cross-validation
accuracies and standard errors. Because the wl-subtree kernel was not designed for partially

12 https://github.com/rmgarnett/fast_wl.

123

Mach Learn (2016) 102:209–245

237

Table 3 Labeled graphs

Method

Dataset

mutag

nci1

nci109

dd

pk

wl

sp

gh

labels

84.5 ± 0.6 (0.2
84.0 ± 0.4 (0.2
85.8 ± 0.2 (0.2
85.4 ± 0.5 (1.0
85.8 ± 0.2 (0.0

(cid:2)(cid:2))
(cid:2)(cid:2))
(cid:2)(cid:2))
(cid:2))
(cid:2)(cid:2))

(cid:2))
84.5 ± 0.1 (4.5
(cid:2))
85.9 ± 0.1 (5.6
(cid:2)(cid:2))
74.4 ± 0.1 (21.3
73.2 ± 0.1 (13.0h)
(cid:2)(cid:2))
64.6 ± 0.0 (0.8

(cid:2))
83.5 ± 0.1 (4.4
(cid:2))
85.9 ± 0.1 (7.4
(cid:2)(cid:2))
73.7 ± 0.0 (19.3
72.6 ± 0.1 (22.1h)
(cid:2)(cid:2))
63.6 ± 0.0 (0.7

(cid:2))
(cid:2))

78.8 ± 0.2 (3.6
79.0 ± 0.2 (6.7
out of time
68.9 ± 0.2 (69.1h)
(cid:2)(cid:2))
78.4 ± 0.1 (0.3

Average accuracies ± standard error of 10-fold cross validation (10 runs). Average runtimes in sec (x (cid:2)(cid:2)
), min
(x (cid:2)
), or hours (xh) are given in parentheses. All kernels are unnormalized. The kernel parameters tmax for pk
and hmax for wl were learned on the training splits (tmax, hmax ∈ {0, 1, . . . , 10}). labels corresponds to pk
with tmax = 0. out of time indicates that the kernel computation did not ﬁnish within 24 h. Bold indicates
that the method performs signiﬁcantly better than the second best method under a paired t test ( p < 0.05)

Table 4 Partially labeled graphs

Dataset

Method

Labels missing

20 %

40 %

60 %

80 %

msrc9

msrc21

lp + wl

lp + wl

pk

wl

pk

wl

90.0 ± 0.4
90.0 ± 0.2
89.2 ± 0.5
86.9 ± 0.3
85.8 ± 0.2
85.4 ± 0.4

88.7 ± 0.3
87.9 ± 0.6
88.1 ± 0.5
84.7 ± 0.3
81.5 ± 0.3
81.9 ± 0.4

86.6 ± 0.4
83.2 ± 0.6
85.7 ± 0.6
79.5 ± 0.3
74.5 ± 0.3
76.0 ± 0.3

80.4 ± 0.6
77.9 ± 1.0
78.5 ± 0.9
69.3 ± 0.3
64.0 ± 0.4
63.7 ± 0.4

Average accuracies (and standard errors) on 10 different sets of partially labeled images for pk and wl with
unlabeled nodes treated as additional label (wl) and with hard labels derived from converged label propagation
(lp + wl). Bold indicates that the method performs signiﬁcantly better than the second best method under a
paired t test ( p < 0.05)

labeled graphs, we compare pk to two variants: one where we treat unlabeled nodes as an
additional label “u” (wl) and another where we use hard labels derived from running label
propagation (lp) until convergence (lp + wl). For this experiment we did not learn the
number of kernel iterations, but selected the best performing tmax resp. hmax.

The results are shown in Table 4. For larger fractions of missing labels, pk obviously outper-
forms the baseline methods, and, surprisingly, running label propagation until convergence
and then computing wl gives slightly worse results than wl. However, label propagation
might be beneﬁcial for larger amounts of missing labels. The runtimes of the different meth-
ods on msrc21 are shown in Fig. 12 in the “Appendix 1”. wl computed via the string-based
implementation suggested in (Shervashidze et al. 2011) is over 36 times slower than pk. These
results again conﬁrm that propagation kernels have attractive scalability properties for large
datasets. The lp + wl approach wastes computation time while running lp to convergence
before it can even begin calculating the kernel. The intermediate label distributions obtained
during the convergence process are already extremely powerful for classiﬁcation. These
results clearly show that propagation kernels can successfully deal with partially labeled
graphs and suggest an afﬁrmative answer to questions (Q3) and (Q4).

123

238

Mach Learn (2016) 102:209–245

p2k

wl

sp

gh

csm

labels

attr

labels & attr

pk

100

90

80

70

60

50

40

90

85

80

)

%

(

y
c
a
r
u
c
c
a

)

%

(

y
c
a
r
u
c
c
a

70

60

50

40

75

70

65

60

)

%

(

y
c
a
r
u
c
c
a

)

%

(

y
c
a
r
u
c
c
a

1”

1’
101
runtime (s)

103 1h 104 10h 48h

101

1’ 102
runtime (s)

103

1 h

104 5 h

30

1”

(a)

(b)

1”

101

1’102

103

1 h 104

10 h

1’ 102

103

1 h

104 5 h

55

101

runtime (s)

(c)

runtime (s)

(d)

Fig. 11 Attributed graphs: log runtime versus accuracy. The plots show log(runtimes) in seconds plotted
against average accuracy (± standard deviation). Methods are encoded by color and the used information
(labels, attributes or both) is encoded by shape. Note that p2k also uses both, labels and attributes, however
in contrast to pk both are propagated. For pro-full the csm kernel computation exceeded 32 GB of memory.
On synthetic csm using the attribute information only could not be computed within 72 h. a synthetic, b
enzymes, c bzr, d pro-full (Color ﬁgure online)

Attributed graphs The experimental results for various datasets with attributed graphs are
illustrated in Fig. 11. The plots show runtime versus average accuracy, where the error
bars reﬂect standard deviation of the accuracies. As we are interested in good predictive
performance while achieving fast kernel computation, methods in the upper-left corners
provide the best performance with respect to both quality and speed. For pk, sp, gh, and csm
we compare three variants: one where we use the labels only, one where we use the attribute
information only, and one where both labels and attributes are used. wl is computed with label
information only. For synthetic, cf. Fig. 11a, we used the node degree as label information.
Further, we compare the performance of p2k, which propagates labels and attributes as
described in Sect. 6.3. Detailed results on synthetic and all bioinformatics datasets are
provided in Table 8 (average accuracies) and Table 7 (runtimes) in the “Appendix 2”. From
Fig. 11 we clearly see that propagation kernels tend to appear in the upper-left corner, that
is, they are achieving good predictive performance while being fast, leading to a positive
answer of question (Q4). Note that the runtimes are shown on a log scale. We can also
see that p2k, propagating both labels and attributes, (blue star) usually outperforms the the
simple pk implementation not considering attribute arrangements (blue diamond). However,
this comes at the cost of being slower. So, we can use the ﬂexibility of propagation kernels

123

239

attr

a

Mach Learn (2016) 102:209–245

Table 5 Point cloud graphs

labels

pk

75.6 ± 0.6
0.2

(cid:2)(cid:2)

labels & attr

wl

pk

p2k

acc±stderr
runtime

70.7 ± 0.0
0.4

(cid:2)(cid:2)

76.8 ± 1.3
0.3

(cid:2)(cid:2)

82.9 ± 0.0
(cid:2)(cid:2)
34.8

36.4 ± 0.0
(cid:2)(cid:2)
40.0

Average accuracies of loo cross validation on db. The reported standard errors refer to 10 kernel recomputa-
tions. Runtimes are given in sec (x (cid:2)(cid:2)
). All kernels are normalized and the kernel parameters, tmax for all pks
and hmax for wl, were learned on the training splits (tmax, hmax ∈ {0, . . . 15}). sp, gh, and csm were either
out of memory or the computation did not ﬁnish within 24h for all settings
Bold indicates that the method performs signiﬁcantly better than the second best method under a paired t-test
( p < 0.05)

to trade predictive quality against speed or vice versa according to the requirements of the
application at hand. This supports a positive answer to question (Q3).

7.5.2 Graph classiﬁcation on novel applications

The ﬂexibility of propagation kernels arising from easily interchangeable propagation
schemes and their efﬁcient computation via lsh allows us to apply graph kernels to novel
domains. First, we are able to compare larger graphs with reasonable time expended, opening
up the use of graph kernels for object category prediction of 3d point clouds in the context of
robotic grasping (Neumann et al. 2013). Depending on their size and the perception distance,
point clouds of household objects can easily consist of several thousands of nodes. Tradi-
tional graph kernels suffer from enormous computation times or memory problems even on
datasets like db, which can still be regarded medium sized. These issues aggravate even more
when considering image data. So far, graph kernels have been used for image classiﬁcation
on the scene level where the nodes comprise segments of similar pixels and one image is
then represented by less than 100 so-called superpixels. Utilizing off-the-shelf techniques for
efﬁcient diffusion on grid graphs allows the use of propagation kernels to analyze images on
the pixel level and thus opens up a whole area of interesting problems in the intersection of
graph-based machine learning and computer vision. As a ﬁrst step, we apply graph kernels,
more precisely propagation kernels, to texture classiﬁcation, where we consider datasets with
thousands of graphs containing a total of several millions of nodes.
3d object category prediction In this set of experiments, we follow the protocol introduced in
Neumann et al. (2013), where the graph kernel values are used to derive a prior distribution
on the object category for a given query object. The experimental results for the 3d-object
classiﬁcation are summarized in Table 5. We observe that propagation kernels easily deal
with the point-cloud graphs. From the set of baseline graph kernels considered, only wl was
feasible to compute, however with poor performance. Propagation kernels clearly beneﬁt
form their ﬂexibility as we can improve the classiﬁcation accuracy from 75.4 to 80.7 % when
considering the object curvature attribute. These results are extremely promising given that
we tackle a classiﬁcation problem with 11 classes having only 40 training examples for each
query object.
Grid graphs For brodatz and plants we follow the experimental protocol in Neumann et al.
(2014). The pk parameter tmax was learned on a training subset of the full dataset (tmax ∈
{0, 3, 5, 8, 10, 15, 20}). For plants this training dataset consists of 10 % of the full data; for
brodatz we used 20 % of the brodatz-o-r data as the number of classes in this dataset is

123

240

Table 6 Grid graphs

Method

pk

labels
glcm-gray
glcm-quant

Dataset

brodatz-o-r

(cid:2))
89.6 ± 0.0 (3.5
(cid:2))
5.0 ± 0.0 (1.1
87.2 ± 0.0 (29.5
78.6 ± 0.0 (24.9

(cid:2)(cid:2))
(cid:2)(cid:2))

Mach Learn (2016) 102:209–245

brodatz-o-r-s-rs

plants

(cid:2))
85.7 ± 0.0 (7.1
(cid:2))
4.9 ± 0.0 (2.2
79.4 ± 0.0 (44.8
68.6 ± 0.0 (44.8

(cid:2)(cid:2))
(cid:2)(cid:2))

(cid:2))
82.5 ± 0.1 (3.0
(cid:2)(cid:2))
59.5 ± 0.0 (11.5
(cid:2))
76.6 ± 0.0 (1.4
(cid:2))
37.5 ± 0.0 (1.1

) or min (x (cid:2)

Average accuracies ± standard errors of 10-fold cv (10 runs). The pk parameter tmax as well as color
quantization and pixel neighborhood was learned on a training subset of the full dataset. Average runtimes in
sec (x (cid:2)(cid:2)
) given in parentheses refer to the learned parameter settings. All kernels are unnormalized
and for glcm-quant and labels the same color quantization as for pk was applied. labels corresponds to
pk with tmax = 0. brodatz-o-r is using the original images and their 90
rotated versions and brodatz-o-
r-s-rs additionally includes their scaled, and scaled and rotated versions
Bold indicates that the method performs signiﬁcantly better than the second best method under a paired t-test
( p < 0.05)

◦

much larger (32 textures). We also learned the quantization values (col ∈ {3, 5, 8, 10, 15}) and
neighborhoods (B ∈ {N1,4, N1,8, N2,16}, cf. Eq. (16)). For brodatz the best performance
on the training data was achieved with 3 colors and a 8-neighborhood, whereas for plants 5
colors and the 4-neighborhood was learned. We compare pk to the simple baseline labels
using label counts only and to a powerful second-order statistical feature based on the gray-
level co-occurrence matrix (Haralick et al. 1973) comparing intensities (glcm-gray) resp.
quantized labels (glcm-quant) of neighboring pixels. The experimental results for grid
graphs are shown in Table 6. While not outperforming sophisticated and complex state-of-
the-art computer vision approaches to texture classiﬁcation, we ﬁnd that it is feasible to
compute pks on huge image datasets achieving respectable performance out of the box. This
is—compared to the immense tuning of features and methods commonly done in computer
vision—a great success. On plants pk achieves an average accuracy of 82.5 %, where the
best reported result so far is 83.7 %, which was only achieved after tailoring a complex
feature ensemble (Neumann et al. 2014). In conclusion, propagation kernels are an extremely
promising approach in the intersection of machine learning, graph mining, and computer
vision.

Summarizing all experimental results, the capabilities claimed in Table 2 are supported.
Propagation kernels have proven extremely ﬂexible and efﬁcient and thus question (Q3) can
ultimately be answered afﬁrmatively.

Random walk-based models provide a principled way of spreading information and even
handling missing and uncertain information within graphs. Known labels are, for example,
propagated through the graph in order to label all unlabeled nodes. In this paper, we showed
how to use random walks to discover structural similarities shared between graphs for the
construction of a graph kernel, namely the propagation kernel. Intuitively, propagation kernels
count common sub-distributions induced in each iteration of running inference in two graphs
leading to the insight that graph kernels are much closer to graph-based learning than assumed
before.

8 Conclusion

123

Mach Learn (2016) 102:209–245

241

As our experimental results demonstrate, propagation kernels are competitive in terms of
accuracy with state-of-the-art kernels on several classiﬁcation benchmark datasets of labeled
and attributed graphs. In terms of runtime, propagation kernels outperform all recently devel-
oped efﬁcient and scalable graph kernels. Moreover, being tied to the propagation scheme,
propagation kernels can be easily adapted to novel applications not having been tractable for
graph kernels before.

Propagation kernels provide several interesting avenues for future work. For handling con-
tinuous attributes using mixture of different covariance matrices could improve performance.
Also, the effect that adding noise to the label encoding actually improves the predictive per-
formance should be investigated in more details. While we have used classiﬁcation to guide
the development of propagation kernels, the results are directly applicable to regression,
clustering, and ranking, among other tasks. Employing message-based probabilistic infer-
ence schemes such as (loopy) belief propagation directly paves the way to deriving novel
graph kernels from graphical models. Exploiting that graph kernels and graph-based learning
(learning on the node level) are closely related, hence, a natural extension to this work is the
derivation of a unifying propagation-based framework for structure representation indepen-
dent of the learning task being on the graph or node level.

Acknowledgments Part of this work grew out of the dfg Collaborative Research Center sfb 876 “Providing
Information by Resource-Constrained Analysis” and discussions with Petra Mutzel and Christian Sohler. We
thank Nino Shervashidze, Nils Kriege, Aasa Feragen, and Karsten Borgwardt for our discussions and for
sharing their kernel implementations and datasets. Part of this work was supported by the German Science
Foundation (dfg) under the reference number ‘GA 1615/1-1’.

Appendix 1: Runtimes for Partially Labeled Graphs

See Fig. 12.

lp + wl
wl
pk

)
s
(

e
m

i
t

l
a
t
o
t

2.5

1.5

3

2

1

0

0.5

2

4

6

8

10

iteration

Fig. 12 Runtime for partially labeled msrc21. Avg. time in seconds over 10 instances of the msrc21 dataset
with 50 % labeled nodes for kernel iterations from 0 to 10. We compare pk to wl with unlabeled nodes treated
as additional label and with hard labels derived from converged label propagation (lp + wl)

123

Mach Learn (2016) 102:209–245

Table 7 Attributed graphs: runtimes

Method

Dataset

synthetic

enzymes

proteins

pro_full

bzr

cox2

dhfr

242

labels

pk

wl

sp

gh

csm

attr

pk

sp

gh

csm

(cid:2)(cid:2)

0.1(cid:2)(cid:2)
(cid:2)(cid:2)
3.4
5.1
19.8
54.8h

(cid:2)

0.3(cid:2)(cid:2)
11.5h
(cid:2)
16.2

OUT OF TIME

pk

p2k

labels & attr
0.3(cid:2)(cid:2)
17.2
13.7h
(cid:2)
16.9
56.8h

csm

gh

sp

(cid:2)(cid:2)

attr

a

(cid:2)

1.4

3.6(cid:2)(cid:2)
(cid:2)(cid:2)
4.1
(cid:2)(cid:2)
1.3
17.8
5.2h

(cid:2)

1.4(cid:2)
55.4
13.2
(cid:2)
7.6

(cid:2)

(cid:2)

1.2(cid:2)
(cid:2)
6.7
1.0h
(cid:2)
9.8
21.8

(cid:2)

(cid:2)

1.3

17.8(cid:2)(cid:2)
(cid:2)(cid:2)
25.9
38.7
2.2h
–

(cid:2)(cid:2)

23.6(cid:2)(cid:2)
5.9h
1.9h
–

20.0(cid:2)(cid:2)
(cid:2)
31.4
6.8h
1.2h
–

(cid:2)

6.4

18.7(cid:2)(cid:2)
(cid:2)(cid:2)
22.2
40.3
1.4h
–

(cid:2)(cid:2)

10.7(cid:2)
6.0h
72.4
–

(cid:2)

(cid:2)

9.0(cid:2)
30.6
7.0h
1.2h
–

(cid:2)

4.6

(cid:2)(cid:2)
0.9
0.8(cid:2)(cid:2)
(cid:2)(cid:2)
1.0
7.4
46.4

(cid:2)

(cid:2)

(cid:2)

2.6(cid:2)(cid:2)
29.5
(cid:2)
4.5
16.2h

3.0(cid:2)(cid:2)
(cid:2)
1.3
20.5
(cid:2)
7.8
48.8

(cid:2)

(cid:2)

(cid:2)(cid:2)

(cid:2)(cid:2)

(cid:2)(cid:2)

1.1
1.0
1.2
10.8
1.6h

(cid:2)

(cid:2)

2.8(cid:2)(cid:2)
25.9
(cid:2)
6.6
31.5h

3.5(cid:2)(cid:2)
(cid:2)
1.8
32.9
(cid:2)
11.7
1.6h

(cid:2)

(cid:2)(cid:2)

3.4
(cid:2)(cid:2)
4.2
2.3(cid:2)(cid:2)
29.5
3.7h

(cid:2)

14.8(cid:2)(cid:2)
1.3h
15.8
97.5h

(cid:2)

15.1(cid:2)(cid:2)
(cid:2)
5.9
1.6h
31.2
3.7h

(cid:2)

(cid:2)(cid:2)

38.3

(cid:2)

1.1

(cid:2)

3.2

Kernel computation times (cputime) are given in sec (x (cid:2)(cid:2)
), or hours (xh). For all pks, tmax = 10; for
wl, hmax = 10; and for csm, k = 7. All computations are performed on machines with 3.4 GHz Intel core
i7 processors. Note that csm is implemented in Java, so comparing computation times is only possible to a
limited extent. out of time means that the computation did not ﬁnish within 72h
Bold indicates the method fastest method

), min (x (cid:2)

Appendix 2: Detailed Results on Attributed Graphs

See Tables 7 and 8.

Table 8 Attributed graphs: accuracies

Method Dataset

synthetic

enzymes

proteins

pro-full

bzr

cox2

dhfr

50.0 ± 0.0
50.0 ± 0.0
50.0 ± 0.0
50.0 ± 0.0
50.0 ± 0.0

∗

∗

∗

∗

∗

46.0 ± 0.3
∗
46.3 ± 0.2
40.4 ± 0.3
∗
34.8 ± 0.4
60.8 ± 0.2

∗

∗

∗

∗

∗

87.0 ± 0.4
75.6 ± 0.1
75.6 ± 0.1
88.2 ± 0.2
75.5 ± 0.1
75.5 ± 0.1
85.6 ± 0.3
76.2 ± 0.1
76.2 ± 0.1
72.9 ± 0.2
87.0 ± 0.3
72.9 ± 0.2
∗
OUT OF MEMORY OUT OF MEMORY 87.5 ± 0.2

∗

∗

∗

∗

∗

81.0 ± 0.2
83.2 ± 0.2
81.0 ± 0.4
∗
81.4 ± 0.3
80.7 ± 0.3

∗

83.5 ± 0.2
84.1 ± 0.2
82.0 ± 0.3
79.5 ± 0.4
82.6 ± 0.2

∗

∗

∗

labels

pk

wl

sp

gh

csm

123

Mach Learn (2016) 102:209–245

243

Table 8 continued

Method Dataset

csm
labels & attr
pk

attr

pk

sp

gh

p2k

sp

gh

csm

attr

a

synthetic

enzymes

proteins

pro-full

bzr

cox2

dhfr

∗

∗

99.2 ± 0.1
83.9 ± 0.2
85.3 ± 0.4
–

56.4 ± 0.6
∗
63.9 ± 0.3
68.8 ± 0.2
68.9 ± 0.2

∗

∗

86.8 ± 0.2
74.1 ± 0.2
72.7 ± 0.2
∗
85.0 ± 0.3
59.9 ± 0.0
74.3 ± 0.1
∗
72.6 ± 0.1
84.1 ± 0.3
61.2 ± 0.0
∗
OUT OF MEMORY OUT OF MEMORY 85.1 ± 0.3

∗

∗

∗

∗

78.2 ± 0.4
78.2 ± 0.0
79.5 ± 0.2
77.6 ± 0.3

∗

84.3 ± 0.1
∗
78.9 ± 0.3
79.0 ± 0.2
79.5 ± 0.2

∗

∗

99.2 ± 0.1
98.7 ± 0.1
99.0 ± 0.1
50.0 ± 0.0
99.0 ± 0.1

65.9 ± 0.4
68.1 ± 0.5
64.3 ± 0.3
71.2 ± 0.2
72.8 ± 0.4

∗

∗

∗

88.1 ± 0.2
75.7 ± 0.4
76.3 ± 0.2
88.8 ± 0.2
76.9 ± 0.2
75.9 ± 0.2
∗
85.2 ± 0.2
59.9 ± 0.0
73.2 ± 0.2
73.0 ± 0.1
84.8 ± 0.4
60.9 ± 0.0
OUT OF MEMORY OUT OF MEMORY 87.0 ± 0.2

∗

∗

∗

79.4 ± 0.6
80.9 ± 0.4
78.5 ± 0.1
79.5 ± 0.2
79.2 ± 0.4

∗

∗

∗

84.1 ± 0.3
∗
83.5 ± 0.3
79.7 ± 0.2
80.0 ± 0.2
∗
80.1 ± 0.3

∗

∗

∗

∗

∗

∗

∗

∗

∗

∗

∗

∗

∗

47.8 ± 0.9

∗

68.5 ± 0.2

72.8 ± 0.2

∗

59.8 ± 0.0

∗

83.9 ± 0.3

78.2 ± 0.0

74.8 ± 0.2

∗

Average accuracies ± standard error of 10-fold cross validation (10 runs). All pks were also recomputed for
each run as there is randomization in the lsh computation. The kernel parameters tmax for all pks and hmax for
wl were learned on the training splits (tmax, hmax ∈ {0, 1, . . . 10}). For all pks we applied standardization to
the attributes and one hash per attribute dimension is computed. Whenever the normalized version of a kernel
performed better than the unnormalized version we report these results and mark the method with ∗. csm is
implemented in Java and computations were performed on a machine with 32gb of memory. out of memory
indicates a Java outOfMemeoryError. Bold indicates that the method performs signiﬁcantly better than
the second best method under a paired t-test ( p < 0.05). The svm cost parameter is learned on the training
−1}
splits. We choose c ∈ {10
for unnormalized kernels

−5, . . . , 105, 107} for normalized kernels and c ∈ {10

−3, 10

−5, 10

−7, 10

−7, 10

References

arxiv:1310.7114.

Bauckhage, C., & Kersting, K. (2013). Efﬁcient information theoretic clustering on discrete lattices (2013).

Borgwardt, K., & Kriegel, H. P. (2005). Shortest-path kernels on graphs. In Proceedings of international

conference on data mining (ICDM-05), pp. 74–81.

Datar, M., & Indyk, P. (2004). Locality-sensitive hashing scheme based on p-stable distributions. In Proceed-

ings of the 20th annual symposium on computational geometry (SCG-2004), pp. 253–262.

Debnath, A., de Compadre, R. L., Debnath, G., Schusterman, A., & Hansch, C. (1991). Structure-activity
relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular
orbital energies and hydrophobicity. Journal of Medicinal Chemistry, 34(2), 786–797.

Desrosiers, C., & Karypis, G. (2009). Within-network classiﬁcation using local structure similarity. In Pro-
ceedings of the European conference on machine learning and knowledge discovery in databases
(ECML/PKDD-09), pp. 260–275.

Dobson, P. D., & Doig, A. J. (2003). Distinguishing enzyme structures from non-enzymes without alignments.

Journal of Molecular Biology, 330(4), 771–783.

Feragen, A., Kasenburg, N., Petersen, J., de Bruijne, M., & Borgwardt, K. M. (2013). Scalable kernels for
graphs with continuous attributes. Advances in Neural Information Processing Systems, 26(NIPS–13),
216–224.

Gallagher, B., Tong, H., Eliassi-Rad, T., & Faloutsos, C. (2008). Using ghost edges for classiﬁcation in sparsely
labeled networks. In Proceedings of the 14th ACM SIGKDD international conference on knowledge
discovery and data mining (KDD-08), pp. 256–264.

Gärtner, T., Flach, P. A., & Wrobel, S. (2003). On graph kernels: Hardness results and efﬁcient alternatives.
In Proceedings of computational learning theory and kernel machines (COLT-03), pp. 129–143.

123

244

Mach Learn (2016) 102:209–245

Gersho, A., & Gray, R. (1991). Vector quantization and signal compression. Norwell, MA: Kluwer Academic

Publishers.

Haralick, R. M., Shanmugam, K., & Dinstein, I. H. (1973). Textural features for image classiﬁcation. IEEE

Transactions on Systems, Man and Cybernetics, SMC–3(6), 610–621.

Harchaoui, Z., & Bach, F. (2007). Image classiﬁcation with segmentation graph kernels. In CVPR. IEEE

Haussler, D. (1999). Convolution kernels on discrete structures. Tech. rep., Department of Computer Science,

Computer Society.

University of California, Santa Cruz.

Hido, S., & Kashima, H. (2009). A linear-time graph kernel. In Proceedings of the 9th IEEE international

conference on data mining (ICDM-09), pp. 179–188.

Horváth, T., Gärtner, T., & Wrobel, S. (2004). Cyclic pattern kernels for predictive graph mining. In Proceedings

of knowledge discovery in databases (KDD-04), pp. 158–167.

Hwang, T., & Kuang, R. (2010). A heterogeneous label propagation algorithm for disease gene discovery. In

Proceedings of the SIAM international conference on data mining (SDM-10), pp. 583–594.

Jaakkola, T., & Haussler, D. (1998). Exploiting generative models in discriminative classiﬁers. Advances in

Neural Information Processing Systems, 11(NIPS–98), 487–493.
Jähne, B. (2005). Digital Image Processing (6th ed.). Berlin: Springer.
Jebara, T., Kondor, R., & Howard, A. (2004). Probability product kernels. Journal of Machine Learning

Research, 5, 819–844.

Kashima, H., Tsuda, K., & Inokuchi, A. (2003). Marginalized kernels between labeled graphs. In Proceedings

of the 20th international conference on machine learning (ICML-03), pp. 321–328.

Kersting, K., Mladenov, M., Garnett, R., & Grohe, M. (2014). Power iterated color reﬁnement. In Proceedings

of the 28th AAAI conference on artiﬁcial intelligence (AAAI-14), pp. 1904–1910.

Kondor, R., & Jebara, T. (2003). A kernel between sets of vectors. In Proceedings of the twentieth international

conference on machine learning (ICML-03), pp. 361–368.

Kondor, R. I., & Lafferty, J. D. (2002). Diffusion kernels on graphs and other discrete input spaces. In Pro-
ceedings of the nineteenth international conference on machine learning (ICML-02), pp. 315–322.
Kriege, N., & Mutzel, P. (2012). Subgraph matching kernels for attributed graphs. In Proceedings of the 29th

international conference on machine learning (ICML-12).

Lafferty, J., & Lebanon, G. (2002). Information diffusion kernels. Advances in Neural Information Processing

Lin, F., & Cohen, W. W. (2010). Power iteration clustering. In Proceedings of the 27th international conference

Systems, 22(NIPS–02), 375–382.

on machine learning (ICML-10), pp. 655–662.

Lovász, L. (1996). Random walks on graphs: A survey. In D. Miklós, V. T. Sós, & T. Sz˝onyi (Eds.), Combi-
natorics, Paul Erd˝os is Eighty (Vol. 2, pp. 353–398). Budapest: János Bolyai Mathematical Society.
Mahé, P., & Vert, J. P. (2009). Graph kernels based on tree patterns for molecules. Machine Learning, 75(1),

3–35.

Moreno, P., Ho, P., & Vasconcelos, N. (2003). A Kullback–Leibler divergence based kernel for SVM classiﬁ-
cation in multimedia applications. Advances in Neural Information Processing Systems, it 23(NIPS-03),
1385–1392.

Neumann, M., Garnett, R., & Kersting, K. (2013). Coinciding walk kernels: Parallel absorbing random walks
for learning with graphs and few labels. In Asian conference on machine learning (ACML-13), pp.
357–372.

Neumann, M., Hallau, L., Klatt, B., Kersting, K., & Bauckhage, C. (2014). Erosion band features for cell
phone image based plant disease classiﬁcation. In Proceedings of the 22nd international conference on
pattern recognition (ICPR-14), pp. 3315–3320.

Neumann, M., Moreno, P., Antanas, L., Garnett, R., & Kersting, K. (2013). Graph kernels for object category
prediction in task-dependent robot grasping. In Eleventh workshop on mining and learning with graphs
(MLG-13), Chicago, Illinois.

Neumann, M., Patricia, N., Garnett, R., & Kersting, K. (2012). Efﬁcient graph kernels by randomization. In
European conference on machine learning and knowledge discovery in databases (ECML/PKDD-12),
pp. 378–393.

Ojala, T., Pietikäinen, M., & Mäenpää, T. (2002). Multiresolution gray-scale and rotation invariant texture
classiﬁcation with local binary patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence,
24(7), 971–987.

Ramon, J., & Gärtner, T. (2003). Expressivity versus efﬁciency of graph kernels. In Proceedings of the 1st

international workshop on mining graphs, trees and sequences, pp. 65–74.

Schomburg, I., Chang, A., Ebeling, C., Gremse, M., Heldt, C., Huhn, G., et al. (2004). Brenda, the enzyme
database: Updates and major new developments. Nucleic Acids Research, 32(Database–Issue), 431–433.

123

Mach Learn (2016) 102:209–245

245

Shervashidze, N., Schweitzer, P., van Leeuwen, E., Mehlhorn, K., & Borgwardt, K. (2011). Weisfeiler–Lehman

graph kernels. Journal of Machine Learning Research, 12, 2539–2561.

Shervashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K., & Borgwardt, K. (2009). Efﬁcient graphlet kernels

for large graph comparison. Journal of Machine Learning Research—Proceedings Track, 5, 488–495.

Shi, Q., Petterson, J., Dror, G., Langford, J., Smola, A. J., & Vishwanathan, S. V. N. (2009). Hash kernels for

structured data. Journal of Machine Learning Research, 10, 2615–2637.

Szummer, M., & Jaakkola, T. (2001). Partially labeled classiﬁcation with Markov random walks. Advances in

Neural Information Processing Systems, 15(NIPS–01), 945–952.

Valkealahti, K., & Oja, E. (1998). Reduced multidimensional co-occurrence histograms in texture classiﬁca-

tion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1), 90–94.

Vishwanathan, S., Schraudolph, N., Kondor, R., & Borgwardt, K. (2010). Graph kernels. Journal of Machine

Learning Research, 11, 1201–1242.

Wale, N., & Karypis, G. (2006). Comparison of descriptor spaces for chemical compound retrieval and classi-
ﬁcation. In Proceedings of the international conference on data mining (ICDM-06) (pp. 678–689). Silver
Spring, MD: IEEE Computer Society.

Winn, J. M., Criminisi, A., & Minka, T. P. (2005). Object categorization by learned universal visual dictionary.

In 10th IEEE international conference on computer vision (ICCV-05), pp. 1800–1807.

Wu, X. M., Li, Z., So, A. M. C., Wright, J., & Chang, S. F. (2012). Learning with partially absorbing random

walks. Advances in Neural Information Processing Systems, 26(NIPS–12), 3086–3094.

Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Schölkopf, B. (2003). Learning with local and global

consistency. Advances in Neural Information Processing Systems, 17(NIPS–03), 321–328.

Zhu, X., Ghahramani, Z., & Lafferty, J. D. (2003). Semi-supervised learning using Gaussian ﬁelds and harmonic
functions. In Proceedings of the twentieth international conference on machine learning (ICML-03), pp.
912–919.

123

Mach Learn (2016) 102:209–245
DOI 10.1007/s10994-015-5517-9

Propagation kernels: efﬁcient graph kernels from
propagated information

Marion Neumann1 · Roman Garnett2 ·
Christian Bauckhage3 · Kristian Kersting4

Received: 17 April 2014 / Accepted: 18 June 2015 / Published online: 17 July 2015
© The Author(s) 2015

Abstract We introduce propagation kernels, a general graph-kernel framework for efﬁ-
ciently measuring the similarity of structured data. Propagation kernels are based on
monitoring how information spreads through a set of given graphs. They leverage early-
stage distributions from propagation schemes such as random walks to capture structural
information encoded in node labels, attributes, and edge information. This has two beneﬁts.
First, off-the-shelf propagation schemes can be used to naturally construct kernels for many
graph types, including labeled, partially labeled, unlabeled, directed, and attributed graphs.
Second, by leveraging existing efﬁcient and informative propagation schemes, propagation
kernels can be considerably faster than state-of-the-art approaches without sacriﬁcing pre-
dictive performance. We will also show that if the graphs at hand have a regular structure,
for instance when modeling image or video data, one can exploit this regularity to scale the
kernel computation to large databases of graphs with thousands of nodes. We support our
contributions by exhaustive experiments on a number of real-world graphs from a variety of
application domains.

Editor: Karsten Borgwardt.

B Marion Neumann

marion.neumann@uni-bonn.de

Roman Garnett
garnett@wustl.edu

Christian Bauckhage
christian.bauckhage@iais.fraunhofer.de

Kristian Kersting
kristian.kersting@cs.tu-dortmund.de

BIT, University of Bonn, Bonn, Germany

1

2

3

4

CSE, Washington University, St. Louis, MO, USA

Fraunhofer IAIS, Sankt Augustin, Germany

CS, Technical University of Dortmund, Dortmund, Germany

123

210

Mach Learn (2016) 102:209–245

Keywords Learning with graphs · Graph kernels · Random walks ·
Locality sensitive hashing · Convolutions

1 Introduction

Learning from structured data is an active area of research. As domains of interest become
increasingly diverse and complex, it is important to design ﬂexible and powerful methods
for analysis and learning. By structured data we refer to situations where objects of interest
are structured and hence can naturally be represented using graphs. Real-world examples are
molecules or proteins, images annotated with semantic information, text documents reﬂecting
complex content dependencies, and manifold data modeling objects and scenes in robotics.
The goal of learning with graphs is to exploit the rich information contained in graphs rep-
resenting structured data. The main challenge is to efﬁciently exploit the graph structure for
machine-learning tasks such as classiﬁcation or retrieval. A popular approach to learning from
structured data is to design graph kernels measuring the similarity between graphs. For clas-
siﬁcation or regression problems, the graph kernel can then be plugged into a kernel machine,
such as a support vector machine or a Gaussian process, for efﬁcient learning and prediction.
Several graph kernels have been proposed in the literature, but they often make strong
assumptions as to the nature and availability of information related to the graphs at hand.
The most simple of these proposals assume that graphs are unlabeled and have no structure
beyond that encoded by their edges. However, graphs encountered in real-world applications
often come with rich additional information attached to their nodes and edges. This naturally
implies many challenges for representation and learning such as:

– missing information leading to partially labeled graphs,
– uncertain information arising from aggregating information from multiple sources, and
– continuous information derived from complex and possibly noisy sensor measurements.

Images, for instance, often have metadata and semantic annotations which are likely to be
only partially available due to the high cost of collecting training data. Point clouds captured
by laser range sensors consist of continuous 3d coordinates and curvature information; in
addition, part detectors can provide possibly noisy semantic annotations. Entities in text
documents can be backed by entire Wikipedia articles providing huge amounts of structured
information, themselves forming another network.

Surprisingly, existing work on graph kernels does not broadly account for these challenges.
Most of the existing graph kernels (Shervashidze et al. 2009, 2011; Hido and Kashima 2009;
Kashima et al. 2003; Gärtner et al. 2003) are designed for unlabeled graphs or graphs with
a complete set of discrete node labels. Kernels for graphs with continuous node attributes
have only recently gained greater interest (Borgwardt and Kriegel 2005; Kriege and Mutzel
2012; Feragen et al. 2013). These graph kernels have two major drawbacks: they can only
handle graphs with complete label or attribute information in a principled manner and they are
either efﬁcient, but limited to speciﬁc graph types, or they are ﬂexible, but their computation
is memory and/or time consuming. To overcome these problems, we introduce propagation
kernels. Their design is motivated by the observation that iterative information propagation
schemes originally developed for within-network relational and semi-supervised learning
have two desirable properties: they capture structural information and they can often adapt
to the aforementioned issues of real-world data. In particular, propagation schemes such as
diffusion or label propagation can be computed efﬁciently and they can be initialized with
uncertain and partial information.

123

Mach Learn (2016) 102:209–245

211

A high-level overview of the propagation kernel algorithm is as follows. We begin by
initializing label and/or attribute distributions for every node in the graphs at hand. We then
iteratively propagate this information along edges using an appropriate propagation scheme.
By maintaining entire distributions of labels and attributes, we can accommodate uncertain
information in a natural way. After each iteration, we compare the similarity of the induced
node distributions between each pair of graphs. Structural similarities between graphs will
tend to induce similar local distributions during the propagation process, and our kernel will
be based on approximate counts of the number of induced similar distributions throughout
the information propagation.

To achieve competitive running times and to avoid having to compare the distributions
of all pairs of nodes between two given graphs, we will exploit locality sensitive hashing
(lsh) to bin the label/attribute distributions into efﬁciently computable graph feature vectors
in time linear in the total number of nodes. These new graph features will then be fed into a
base kernel, a common scheme for constructing graph kernels. Whereas lsh is usually used
to preserve the (cid:2)1 or (cid:2)2 distance, we are able to show that the hash values can preserve both the
total variation and the Hellinger probability metrics. Exploiting explicit feature computation
and efﬁcient information propagation, propagation kernels allow for using graph kernels to
tackle novel applications beyond the classical benchmark problems on datasets of chemical
compounds and small- to medium-sized image or point-cloud graphs.

The present paper is a signiﬁcant extension of a previously published conference paper
(Neumann et al. 2012) and presents and extends a novel graph-kernel application already
published as a workshop contribution (Neumann et al. 2013). Propagation kernels were
originally deﬁned and applied for graphs with discrete node labels (Neumann et al. 2012,
2013); here we extend their deﬁnition to a more general and ﬂexible framework that is able to
handle continuous node attributes. In addition to this expanded view of propagation kernels,
we also introduce and discuss efﬁcient propagation schemes for numerous classes of graphs.
A central message of this paper is:

A suitable propagation scheme is the key to designing fast and powerful propagation
kernels.

In particular, we will discuss propagation schemes applicable to huge graphs with regular
structure, for example grid graphs representing images or videos. Thus, implemented with
care, propagation kernels can easily scale to large image databases. The design of kernels for
grids allows us to perform graph-based image analysis not only on the scene level (Neumann
et al. 2012; Harchaoui and Bach 2007) but also on the pixel level opening up novel application
domains for graph kernels.

We proceed as follows. We begin by touching upon related work on kernels and graphs.
After introducing information propagation on graphs via random walks, we introduce the fam-
ily of propagation kernels (Sect. 4). The following two sections discuss speciﬁc examples
of the two main components of propagation kernels: node kernels for comparing propagated
information (Sect. 5) and propagation schemes for various kinds of information (Sect. 6).
We will then analyze the sensitivity of propagation kernels with respect to noisy and missing
information, as well as with respect to the choice of their parameters. Finally, to demonstrate
the feasibility and power of propagation kernels for large real-world graph databases, we
provide experimental results on several challenging classiﬁcation problems, including com-
monly used bioinformatics benchmark problems, as well as real-world applications such as
image-based plant-disease classiﬁcation and 3d object category prediction in the context of
robotic grasping.

123

212

Mach Learn (2016) 102:209–245

2 Kernels and graphs

Propagation kernels are related to three lines of research on kernels: kernels between graphs
(graph kernels) developed within the graph mining community, kernels between nodes (ker-
nels on graphs) established in the within-network relational learning and semi-supervised
learning communities, and kernels between probability distributions.

2.1 Graph kernels

Propagation kernels are deeply connected to several graph kernels developed within the
graph-mining community. Categorizing graph kernels with respect to how the graph struc-
ture is captured, we can distinguish four classes: kernels based on walks (Gärtner et al.
2003; Kashima et al. 2003; Vishwanathan et al. 2010; Harchaoui and Bach 2007) and paths
(Borgwardt and Kriegel 2005; Feragen et al. 2013), kernels based on limited-size subgraphs
(Horváth et al. 2004; Shervashidze et al. 2009; Kriege and Mutzel 2012), kernels based on
subtree patterns (Mahé and Vert 2009; Ramon and Gärtner 2003), and kernels based on struc-
ture propagation (Shervashidze et al. 2011). However, there are two major problems with
most existing graph kernels: they are often slow or overly specialized. There are efﬁcient
graph kernels speciﬁcally designed for unlabeled and fully labeled graphs (Shervashidze
et al. 2009, 2011), attributed graphs (Feragen et al. 2013), or planar labeled graphs (Har-
chaoui and Bach 2007), but these are constrained by design. There are also more ﬂexible but
slower graph kernels such as the shortest path kernel (Borgwardt and Kriegel 2005) or the
common subgraph matching kernel (Kriege and Mutzel 2012).

The Weisfeiler–Lehman (wl) subtree kernel, one instance of the recently introduced fam-
ily of wl-kernels (Shervashidze et al. 2011), computes count features for each graph based
on signatures arising from iterative multi-set label determination and compression steps. In
each kernel iteration, these features are then fed into a base kernel. The wl-kernel is ﬁnally
the sum of those base kernels over the iterations.

Although wl-kernels are usually competitive in terms of performance and runtime, they
are designed for fully labeled graphs. The challenge of comparing large, partially labeled
graphs—which can easily be considered by propagation kernels introduced in the present
paper—remains to a large extent unsolved. A straightforward way to compute graph kernels
between partially labeled graphs is to mark unlabeled nodes with a unique symbol or their
degree as suggested in Shervashidze et al. (2011) for the case of unlabeled graphs. However,
both solutions neglect any notion of uncertainty in the labels. Another option is to propagate
labels across the graph and then run a graph kernel on the imputed labels (Neumann et al.
2012). Unfortunately, this also ignores the uncertainty induced by the inference procedure,
as hard labels have to be assigned after convergence. A key observation motivating propaga-
tion kernels is that intermediate label distributions induced will, before convergence, carry
information about the structure of the graph. Propagation kernels interleave label inference
and kernel computation steps, avoiding the requirement of running inference to termination
prior to the kernel computation.

2.2 Kernels on graphs and within-network relational learning

Measuring the structural similarity of local node neighborhoods has recently become popular
for inference in networked data (Kondor and Lafferty 2002; Desrosiers and Karypis 2009;
Neumann et al. 2013) where this idea has been used for designing kernels on graphs (ker-
nels between the nodes of a graph) and for within-network relational learning approaches.

123

Mach Learn (2016) 102:209–245

213

An example of the former are coinciding walk kernels (Neumann et al. 2013) which are
deﬁned in terms of the probability that the labels encountered during parallel random walks
starting from the respective nodes of a graph coincide. Desrosiers and Karypis (2009) use a
similarity measure based on parallel random walks with constant termination probability in
a relaxation-labeling algorithm. Another approach exploiting random walks and the struc-
ture of subnetworks for node-label prediction is heterogeneous label propagation (Hwang
and Kuang 2010). Random walks with restart are used as proximity weights for so-called
“ghost edges” in Gallagher et al. (2008), but then the features considered by a bag of logis-
tic regression classiﬁers are only based on a one-step neighborhood. These approaches, as
well as propagation kernels, use random walks to measure structure similarity. Therefore,
propagation kernels establish an important connection of graph-based machine learning for
inference about node- and graph-level properties.

2.3 Kernels between probability distributions and kernels between sets

Finally, propagation kernels mark another connection, namely between graph kernels and
kernels between probability distributions (Jaakkola and Haussler 1998; Lafferty and Lebanon
2002; Moreno et al. 2003; Jebara et al. 2004) and between sets (Kondor and Jebara 2003; Shi
et al. 2009). However, whereas the former essentially build kernels based on the outcome
of probabilistic inference after convergence, propagation kernels intuitively count common
sub-distributions induced after each iteration of running inference in two graphs.

Kernels between sets and more speciﬁcally between structured sets, also called hash
kernels (Shi et al. 2009), have been successfully applied to strings, data streams, and unlabeled
graphs. While propagation kernels hash probability distributions and derive count features
from them, hash kernels directly approximate the kernel values k(x, x (cid:2)), where x and x (cid:2) are
(structured) sets. Propagation kernels iteratively approximate node kernels k(u, v) comparing
nodes u in graph G(i) with nodes v in graph G( j). Counts summarizing these approximations
are then fed into a base kernel that is computed exactly. Before we give a detailed deﬁnition
of propagation kernels, we introduce the basic concept of information propagation on graphs,
and exemplify important propagation schemes and concepts when utilizing random walks
for learning with graphs.

3 Information propagation on graphs

Information propagation or diffusion on a graph is most commonly modeled via Markov ran-
dom walks (rws). Propagation kernels measure the similarity of two graphs by comparing
node label or attribute distributions after each step of an appropriate random walk. In the
following, we review label diffusion and label propagation via rws—two techniques com-
monly used for learning on the node level (Zhu et al. 2003; Szummer and Jaakkola 2001).
Based on these ideas, we will then develop propagation kernels in the subsequent sections.

3.1 Basic notation

Throughout, we consider graphs whose nodes are endowed with (possibly partially observed)
label and/or attribute information. That is, a graph G = (V, E, (cid:2), a) is represented by a set
of |V | = n vertices, a set of edges E speciﬁed by a weighted adjacency matrix A ∈ Rn×n, a
label function (cid:2) : V → [k], where k is the number of available node labels, and an attribute
function with a : V → RD, where D is the dimension of the continuous attributes. Given

123

214

Mach Learn (2016) 102:209–245

V = {v1, v2, ..., vn}, node labels (cid:2)(vi ) are represented by nominal values and attributes
a(vi ) = xi ∈ RD are represented by continuous vectors.

3.2 Markov random walks

Consider a graph G = (V, E). A random walk on G is a Markov process X = {Xt : t ≥ 0}
with a given initial state X0 = vi . We will also write Xt|i to indicate that the walk began
at vi . The transition probability Ti j = P(Xt+1 = v j
| Xt = vi ) only depends on the
current state Xt = vi and the one-step transition probabilities for all nodes in V can be
easily represented by the row-normalized adjacency or transition matrix T = D−1 A, where
D = diag(

(cid:2)

j Ai j ).

3.3 Information propagation via random walks

(cid:2)

For now, we consider (partially) labeled graphs without attributes, where V = VL ∪ VU is the
union of labeled and unlabeled nodes and (cid:2) : V → [k] is a label function with known values
for the nodes in VL . A common mechanism for providing labels for the nodes of an unlabeled
graph is to deﬁne the label function by (cid:2)(vi ) =
j Ai j = degree(vi ). Hence for fully labeled
and unlabeled graphs we have VU = ∅. We will monitor the distribution of labels encountered
by random walks leaving each node in the graph to endow each node with a k-dimensional
feature vector. Let the matrix P0 ∈ Rn×k give the prior label distributions of all nodes in V,
where the ith row (P0)i = p0,vi corresponds to node vi . If node vi ∈ VL is observed with
label (cid:2)(vi ), then p0,vi can be conveniently set to a Kronecker delta distribution concentrating
at (cid:2)(vi ); i.e., p0,vi
= δ(cid:2)(vi ). Thus, on graphs with VU = ∅ the simplest rw-based information
propagation is the label diffusion process or simply diffusion process

Pt+1 ← T Pt ,

where pt,vi gives the distribution over (cid:2)(Xt|i ) at iteration t.

Let S ⊆ V be a set of nodes in G. Given T and S, we deﬁne an absorbing random walk

to have the modiﬁed transition probabilities ˆT , deﬁned by:

ˆTi j =

⎧
⎨

⎩

0
1
Ti j

if i ∈ S and i (cid:10)= j;
if i ∈ S and i = j;
otherwise.

(1)

(2)

Nodes in S are “absorbing” in that a walk never leaves a node in S after it is encountered. The
ith row of P0 now gives the probability distribution for the ﬁrst label encountered, (cid:2)(X0|i ),
for an absorbing rw starting at vi . It is easy to see by induction that by iterating the map
Pt+1 ← ˆT Pt ,

(3)

pt,vi similarly gives the distribution over (cid:2)(Xt|i ).

In the case of partially labeled graphs we can now initialize the label distributions for
the unlabeled nodes VU with some prior, for example a uniform distribution.1 If we deﬁne
the absorbing states to be the labeled nodes, S = VL , then the label propagation algorithm
introduced in Zhu et al. (2003) can be cast in terms of simulating label-absorbing rws with
transition probabilities given in Eq. (2) until convergence, then assigning the most probable
absorbing label to the nodes in VU .

The schemes discussed so far are two extreme cases of absorbing rws: one with no
absorbing states, the diffusion process, and one which absorbs at all labeled nodes, label

1 This prior could also be the output of an external classiﬁer built on available node attributes.

123

Mach Learn (2016) 102:209–245

215

propagation. One useful extension of absorbing rws is to soften the deﬁnition of absorbing
states. This can be naturally achieved by employing partially absorbing random walks (Wu
et al. 2012). As the propagation kernel framework does not require a speciﬁc propagation
scheme, we are free to choose any rw-based information propagation scheme suitable for
the given graph types. Based on the basic techniques introduced in this section, we will sug-
gest speciﬁc propagation schemes for (un-)labeled, partially labeled, directed, and attributed
graphs as well as for graphs with regular structure in Sect. 6.

3.4 Steady-state distributions versus early stopping

Assuming non-bipartite graphs, all rws, absorbing or not, converge to a steady-state distrib-
ution P∞ (Lovász 1996; Wu et al. 2012). Most existing rw-based approaches only analyze
the walks’ steady-state distributions to make node-label predictions (Kondor and Lafferty
2002; Zhu et al. 2003; Wu et al. 2012). However, rws without absorbing states converge
to a constant steady-state distribution, which is clearly uninteresting. To address this, the
idea of early stopping was successfully introduced into power-iteration methods for node
clustering (Lin and Cohen 2010), node-label prediction (Szummer and Jaakkola 2001), as
well as for the construction of a kernel for node-label prediction (Neumann et al. 2013). The
insight here is that the intermediate distributions obtained by the rws during the convergence
process provide useful insights about their structure. In this paper, we adopt this idea for the
construction of graph kernels. That is, we use the entire evolution of distributions encoun-
tered during rws up to a given length to represent graph structure. This is accomplished by
summing contributions computed from the intermediate distributions of each iteration, rather
then only using the limiting distribution.

4 Propagation kernel framework

In this section, we introduce the general family of propagation kernels (pks).

4.1 General deﬁnition

Here we will deﬁne a kernel K : X × X → R among graph instances G(i) ∈ X . The input
space X comprises graphs G(i) = (V (i), E (i), (cid:2), a), where V (i) is the set of |V (i)| = ni
nodes and E (i) is the set of edges in graph G(i). Edge weights are represented by weighted
adjacency matrices A(i) ∈ Rni ×ni and the label and attribute functions (cid:2) and a endow nodes
with label and attribute information2 as deﬁned in the previous section.

A simple way to compare two graphs G(i) and G( j) is to compare all pairs of nodes in the

two graphs:

K (G(i), G( j)) =

k(u, v),

(cid:6)

(cid:6)

u∈G( j)

v∈G(i)
where k(u, v) is an arbitrary node kernel determined by node labels and, if present, node
attributes. This simple graph kernel, however, does not account for graph structure given by
the arrangement of node labels and attributes in the graphs. Hence, we consider a sequence of
(i)
graphs G
t with evolving node information based on information propagation, as introduced
for node labels in the previous section. We deﬁne the kernel contribution of iteration t by

2 Note that not both label and attribute information have to be present and both could also be partially observed.

123

216

Mach Learn (2016) 102:209–245

Algorithm 1 The general propagation kernel computation.
given: graph database {G(i)}i , # iterations tmax, propagation scheme(s), base kernel (cid:12)·, ·(cid:13)
(i)
initialization: K ← 0, initialize distributions P
0
for t ← 0 . . . tmax do

for all graphs G(i) do

for all nodes u ∈ G(i) do

quantize pt,u ,
(i)
where pt,u is the row in P
t

end for
compute Φi· = φ(G

(i)
t

)

end for
K ← K + (cid:12)Φ, Φ(cid:13)
for all graphs G(i) do
(i)
← P
t

(i)
P
t+1
end for

end for

that corresponds to node u

(cid:14) bin node information

(cid:14) count bin strengths

(cid:14) compute and add kernel contribution

(cid:14) propagate node information

K (G

(i)
t

, G

( j)
t

) =

(cid:6)

(cid:6)

v∈G

(i)
t

u∈G

( j)
t

k(u, v).

An important feature of propagation kernels is that the node kernel k(u, v) is deﬁned in
terms of the nodes’ corresponding probability distributions pt,u and pt,v, which we update
and maintain throughout the process of information propagation. For propagation kernels
between labeled and attributed graphs we deﬁne

k(u, v) = k(cid:2)(u, v) · ka(u, v),
where k(cid:2)(u, v) is a kernel corresponding to label information and ka(u, v) is a kernel corre-
sponding to attribute information. If no attributes are present, then k(u, v) = k(cid:2)(u, v). k(cid:2) and
ka will be deﬁned in more detail later. For now assume they are given, then the tmax-iteration
propagation kernel is given by

(5)

(cid:7)

(cid:8)

tmax(cid:6)

(cid:7)

Ktmax

G(i), G( j)

=

K

(i)
t

G

, G

( j)
t

(cid:8)

.

t=1

Lemma 1 Given that k(cid:2)(u, v) and ka(u, v) are positive semideﬁnite node kernels, the prop-
agation kernel Ktmax is a positive semideﬁnite kernel.

Proof As k(cid:2)(u, v) and ka(u, v) are assumed to be valid node kernels, k(u, v) is a valid
node kernel as the product of positive semideﬁnite kernels is again positive semideﬁnite. As
for a given graph G(i) the number of nodes is ﬁnite, K (G
) is a convolution kernel
(Haussler 1999). As sums of positive semideﬁnite matrices are again positive semideﬁnite,
(cid:15)(cid:16)
the propagation kernel as deﬁned in Eq. (6) is positive semideﬁnite.

( j)
t

, G

(i)
t

Let |V (i)| = ni and |V ( j)| = n j . Assuming that all node information is given, the
complexity of computing each contribution between two graphs, Eq. (4), is O(ni n j ). Even
for medium-sized graphs this can be prohibitively expensive considering that the computation
has to be performed for every pair of graphs in a possibly large graph database. However, if
we have a node kernel of the form

(cid:9)

k(u, v) =

1 if condition
0 otherwise,

(4)

(6)

(7)

123

Mach Learn (2016) 102:209–245

217

G(i)
1

G(j)
1

0

G(i)
0

0

G(j)
0

1

0

1

1

0

φ(G(i)
0

) = [2, 1, 3]

φ(G(j)
0

) = [2, 2, 2]

φ(G(i)
1

) = [1, 1, 3, 1]

φ(G(j)
1

) = [3, 1, 2, 0]

φ(G(i)
0

), φ(G(j)
0

) = 12

φ(G(i)
1

), φ(G(j)
1

) = 10

(a)

(b)

and G( j)

Fig. 1 Propagation kernel computation. Distributions, bins, count features, and kernel contributions for two
graphs G(i)
with binary node labels and one iteration of label propagation, cf. Eq. (3), as the
propagation scheme. Node-label distributions are decoded by color: white means p0,u = [1, 0], dark red
stands for p0,u = [0, 1], and the initial distributions for unlabeled nodes (light red) are p0,u = [1/2, 1/2]. a
Initial label distributions (t = 0), b updated label distributions (t = 1) (Color ﬁgure online)

where condition is an equality condition on the information of nodes u and v, we can compute
K efﬁciently by binning the node information, counting the respective bin strengths for all
graphs, and computing a base kernel among these counts. That is, we compute count features
φ(G

) for each graph and plug them into a base kernel (cid:12)·, ·(cid:13):

(i)
t

(cid:7)

K

(i)
t

G

, G

( j)
t

(cid:8)

(cid:10)

(cid:7)

(cid:8)

(cid:7)

=

φ

(i)
t

G

, φ

( j)
t

G

(cid:8)(cid:11)

.

(8)

, where the ith row of Φt , (Φt )i · = φ(G

In the simple case of a linear base kernel, the last step is just an outer product of count
vectors Φt Φ(cid:17)
). Now, for two graphs, binning
t
and counting can be done in O(ni + n j ) and the computation of the linear base kernel value
is O(|bins|). This is one of the main insights for efﬁcient graph-kernel computation and it
has already been exploited for labeled graphs in previous work (Shervashidze et al. 2011;
Neumann et al. 2012).

(i)
t

Figure 1 illustrates the propagation kernel computation for t = 0 and t = 1 for two
example graphs and Algorithm 1 summarizes the kernel computation for a graph database
{G(i)}i . From this general algorithm and Eqs. (4) and (6), we see that the two main components
to design a propagation kernel are
– the node kernel k(u, v) comparing propagated information, and
– the propagation scheme P

propagating the information within the graphs.

(i)
t+1

(i)
← P
t

The propagation scheme depends on the input graphs and we will give speciﬁc suggestions for
different graph types in Sect. 6. Before deﬁning the node kernels depending on the available
node information in Sect. 5, we brieﬂy discuss the general runtime complexity of propagation
kernels.

4.2 Complexity analysis

The total runtime complexity of propagation kernels for a set of n graphs with a total number
(cid:13)
, where n(cid:6) := maxi (ni ). For a pair
of N nodes and M edges is O
of graphs the runtime complexity of computing the count features, that is, binning the node
information and counting the bin strengths is O(ni + n j ). Computing and adding the kernel

(tmax − 1)M + tmax n2 n(cid:6)

(cid:12)

123

218

Mach Learn (2016) 102:209–245

contribution is O(|bins|), where |bins| is bounded by ni + n j . So, one iteration of the kernel
computation for all graphs is O(n2 n(cid:6)). Note that in practice |bins| (cid:18) 2n(cid:6) as we aim to bin
together similar nodes to derive a meaningful feature representation.

Feature computation basically depends on propagating node information along the edges
of all graphs. This operation depends on the number of edges and the information propagated,
so it is O((k + D)M) = O(M), where k is the number of node labels and D is the attribute
dimensionality. This operation has to be performed tmax − 1 times. Note that the number of
edges is usually much lower than N 2.

5 Propagation kernel component 1: node kernel

In this section, we deﬁne node kernels comparing propagated information appropriate for
the use in propagation kernels. Moreover, we introduce locality sensitive hashing, which is
used to discretize the distributions arsing from rw-based information propagation as well as
the continuous attributes directly.

5.1 Deﬁnitions

Above, we saw that one way to allow for efﬁcient computation of propagation kernels is to
restrict the range of the node kernels to {0, 1}. Let us now deﬁne the two components of the
node kernel [Eq. (5)] in this form. The label kernel can be represented as
(cid:9)

k(cid:2)(u, v) =

1
0

if h(cid:2)( pt,u) = h(cid:2)( pt,v);
otherwise,

where pt,u is the node-label distribution of node u at iteration t and h(cid:2)(·) is a quantization
function (Gersho and Gray 1991), more precisely a locality sensitive hash (lsh) function
(Datar and Indyk 2004), which will be introduced in more detail in the next section. Note
(i)
that pt,u denotes a row in the label distribution matrix P
, namely the row corresponding
t
to node u of graph G(i).

Propagation kernels can be computed for various kinds of attribute kernels as long as they

have the form of Eq. (7). The most rudimentary attribute kernel is

ka(u, v) =

(cid:9)

1
0

if ha(xu) = ha(xv);
otherwise,

(9)

(10)

where xu is the one-dimensional continuous attribute of node u and ha(·) is again an lsh
function. Figure 2 contrasts this simple attribute kernel for a one-dimensional attribute to
a thresholded Gaussian function and the Gaussian kernel commonly used to compare node
attributes in graph kernels. To deal with higher-dimensional attributes, we can choose the
attribute kernel to be the product of kernels on each attribute dimension:

ka(u, v) =

kad

(u, v), where

D(cid:14)

d=1
(cid:9)
1
0

kad

(u, v) =

(xu,d ) = had

(xv,d );

if had
otherwise,

(11)

where xu,d is the respective dimension of the attribute xu of node u. Note that each dimension
(·). However, analogous to the label kernel, we can also
now has its own lsh function had

123

219

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

(12)

Mach Learn (2016) 102:209–245

−2

−1.5

−1

−0.5

0.5

0

1

1.5

−2

−1.5

−1

−0.5

0.5

0

1

1.5

(b)

u
x

u
x

u
x

−2

−1.5

−1

−0.5

0.5

0

1

1.5

2
−2 −1.5 −1 −0.5

1

1.5

2

2
−2 −1.5 −1 −0.5

0.5

1

1.5

2

2
−2 −1.5 −1 −0.5

0.5

1

1.5

2

0.5

0
xv
ka(u, v)

(a)

0
xv

0
xv

(c)

Fig. 2 Attribute kernels. Three different attribute functions among nodes with continuous attributes xu and
xv ∈ [−2, 2]. a An attribute kernel suitable for the efﬁcient computation in propagation kernels (ka (u, v)), b
a thresholded Gaussian, and c a Gaussian kernel

deﬁne an attribute kernel based on propagated attribute distributions

ka(u, v) =

(cid:9)

1
0

if ha(qt,u) = ha(qt,v);
otherwise,

where qt,u is the attribute distribution of node u at iteration t. Next we explain the locality
sensitive hashing approach used to discretize distributions and continuous attributes directly.
In Sect. 6.3, we will then derive an efﬁcient way to propagate and hash continuous attribute
distributions.

5.2 Locality sensitive hashing

We now describe our quantization approach for implementing propagation kernels for graphs
with node-label distributions and continuous attributes. The idea is inspired by locality sen-
sitive hashing (Datar and Indyk 2004) which seeks quantization functions on metric spaces
where points “close enough” to each other in that space are “probably” assigned to the same
bin. In the case of distributions, we will consider each node-label vector as being an element
of the space of discrete probability distributions on k items equipped with an appropriate
probability metric. If we want to hash attributes directly, we simply consider metrics for
continuous values.

Deﬁnition 1 (Locality Sensitive Hash (lsh)) Let X be a metric space with metric d : X ×
X → R, and let Y = {1, 2, . . . , k(cid:2)}. Let θ > 0 be a threshold, c > 1 be an approximation
factor, and p1, p2 ∈ (0, 1) be the given success probabilities. A set of functions H from
X to Y is called a (θ, cθ, p1, p2)-locality sensitive hash if for any function h ∈ H chosen
uniformly at random, and for any two points x, x (cid:2) ∈ X , it holds that
– if d(x, x (cid:2)) < θ, then Pr(h(x) = h(x (cid:2))) > p1, and
– if d(x, x (cid:2)) > cθ , then Pr(h(x) = h(x (cid:2))) < p2.

It is known that we can construct lsh families for (cid:2) p spaces with p ∈ (0, 2] (Datar and
Indyk 2004). Let V be a real-valued random variable. V is called p-stable if for any
{x1, x2, . . . , xd }, xi ∈ R and independently sampled v1, v2, . . . , vd , we have
xi vi ∼
(cid:20)x(cid:20) p V .

(cid:2)

Explicit p-stable distributions are known for some p; for example, the standard Cauchy
distribution is 1-stable, and the standard normal distribution is 2-stable. Given the ability to
sample from a p-stable distribution V , we may deﬁne an lsh H on Rd with the (cid:2) p metric

123

220

Mach Learn (2016) 102:209–245

Algorithm 2 calculate- lsh

given: matrix X ∈ RN ×D, bin width w, metric m
if m = h then
X ←
X

√

end if
if m = h or m = l2 then
v ← rand- norm(D)
else if m = tv or m = l1 then

end if
b = w ∗ rand- unif()
h(X ) = ﬂoor((X ∗ v + b)/w)

v ← rand- norm(D)/rand- norm(D)

(cid:14) square root transformation

(cid:14) generate random projection vector
(cid:14) sample from N (0, 1)

(cid:14) sample from Cauchy(0, 1)

(cid:14) random offset b ∼ U [0, w]
(cid:14) compute hashes

(Datar and Indyk 2004). An element h of H is speciﬁed by three parameters: a width w ∈ R+,
a d-dimensional vector v whose entries are independent samples of V , and b drawn from
U[0, w]. Given these, h is then deﬁned as

h(x; w, v, b) =

(cid:15)

(cid:16)

.

v(cid:17)x + b
w

(13)

We may now consider h(·) to be a function mapping our distributions or attribute values to
integer-valued bins, where similar distributions end up in the same bin. Hence, we obtain
node kernels as deﬁned in Eqs. (9) and (12) in the case of distributions, as well as simple
attribute kernels as deﬁned in Eqs. (10) and (11). To decrease the probability of collision,
it is common to choose more than one random vector v. For propagation kernels, however,
we only use one hyperplane, as we effectively have tmax hyperplanes for the whole kernel
computation and the probability of a hash conﬂict is reduced over the iterations.

The intuition behind the expression in Eq. (13) is that p-stability implies that two vectors
that are close under the (cid:2) p norm will be close after taking the dot product with v; speciﬁcally,
(v(cid:17)x − v(cid:17)x(cid:2)) is distributed as (cid:20)x − x(cid:2)(cid:20) p V . So, in the case where we want to construct a
hashing for D-dimensional continuous node attributes to preserve (cid:2)1 (l1) or (cid:2)2 (l2) distance

dl1(xu, xv) =

|xu,d − xu,d |,

dl2(xu, xv) =

xu,d − xv,d

(cid:17)

D(cid:6)

(cid:12)

d=1

(cid:18)1/2
,

(cid:13)

2

we directly apply Eq. (13). In the case of distributions, we are concerned with the space of
discrete probability distributions on k elements, endowed with a probability metric d. Here
we speciﬁcally consider the total variation (tv) and Hellinger (h) distances:

dtv( pu, pv) = 1/2

| pu,i − pv,i |, dh( pu, pv) =

1/2

(cid:17)

k(cid:6)

(cid:12)√

√

pu,i −

pv,i

(cid:18)1/2
.

(cid:13)

2

i=1

D(cid:6)

d=1

k(cid:6)

i=1

The total variation distance is simply half the (cid:2)1 metric, and the Hellinger distance is a
scaled version of the (cid:2)2 metric after applying the map p (cid:23)→
p. We may therefore create a
locality-sensitive hash family for dtv by direct application of Eq. (13) and create a locality-
sensitive hash family for dh by using Eq. (13) after applying the square root map to our
label distributions. The lsh computation for a matrix X ∈ RN ×D, where xu is the row in X
corresponding to node u, is summarized in Algorithm 2.
123

√

Mach Learn (2016) 102:209–245

221

Algorithm 3 Propagation kernel for fully labeled graphs (speciﬁc parts compared to the
general computation (Algorithm 1) are marked in green (input) and blue (computation)–
Color version online)
given: graph database {G(i)}i , # iterations tmax, transition matrix T , bin width w, metric m, base kernel
(cid:12)·, ·(cid:13)
initialization: K ← 0, P0 ← δ(cid:2)(V )
for t ← 0 . . . tmax do

calculate- lsh(Pt , w, m)
for all graphs G(i) do

compute Φi· = φ(G

(i)
t

)

end for
Pt+1 ← T Pt
K ← K + (cid:12)Φ, Φ(cid:13)

end for

(cid:14) bin node information

(cid:14) count bin strengths

(cid:14) label diffusion
(cid:14) compute and add kernel contribution

6 Propagation kernel component 2: propagation scheme

As pointed out in the introduction, the input graphs for graph kernels may vary considerably.
One key to design efﬁcient and powerful propagation kernels is the choice of a propagation
scheme appropriate for the graph dataset at hand. By utilizing random walks (rws) we are able
to use efﬁcient off-the-shelf algorithms, such as label diffusion or label propagation (Szummer
and Jaakkola 2001; Zhu et al. 2003; Wu et al. 2012), to implement information propagation
within the input graphs. In this section, we explicitly deﬁne propagation kernels for fully
labeled, unlabeled, partially labeled, directed, and attributed graphs as well as for graphs
with a regular grid structure using appropriate rws. In each particular algorithm, the speciﬁc
parts changing compared to the general propagation kernel computation (Algorithm 1) will
be marked in color.

6.1 Labeled and unlabeled graphs

(cid:19)

(cid:2)

For fully labeled graphs we suggest the use of the label diffusion process from Eq. (1) as
the propagation scheme. Given a database of fully labeled graphs {G(i)}i=1,...,n with a total
number of N =
i ni nodes, label diffusion on all graphs can be efﬁciently implemented
by multiplying a sparse block-diagonal transition matrix T ∈ RN ×N , where the blocks
are the transition matrices T (i) of the respective graphs, with the label distribution matrix
∈ RN ×k. This can be done efﬁciently due to the sparsity of T .
Pt =
The propagation kernel computation for labeled graphs is summarized in Algorithm 3. The
speciﬁc parts compared to the general propagation kernel computation (Algorithm 1) for fully
labeled graphs are marked in green (input) and blue (computation). For unlabeled graphs we
suggest to set the label function to be the node degree (cid:2)(u) = degree(u) and then apply the
same pk computation as for fully labeled graphs.

(n)
, . . . , P
t

(1)
P
t

(cid:20)(cid:17)

6.2 Partially labeled and directed graphs

For partially labeled graphs, where some of the node labels are unknown, we sug-
gest label propagation as an appropriate propagation scheme. Label propagation differs
from label diffusion in the fact that before each iteration of the information propaga-
tion, the labels of the originally labeled nodes are pushed back (Zhu et al. 2003). Let

123

222

(cid:21)

P0,[labeled], P0,[unlabeled]

P0 =
represent the prior label distributions for the nodes of
all graphs in the graph database, where the distributions in P0,[labeled] represent observed
labels and P0,[unlabeled] are initialized uniformly. Then label propagation is deﬁned by

(cid:22)(cid:17)

Mach Learn (2016) 102:209–245

Pt,[labeled] ← P0,[labeled];
Pt+1 ← T Pt .

(14)

Note that this propagation scheme is equivalent to the one deﬁned in Eq. (3) using fully
absorbing rws. Other similar update schemes, such as “label spreading” (Zhou et al. 2003),
could be used in a propagation kernel as well. Thus, the propagation kernel computation for
partially labeled graphs is essentially the same as Algorithm 3, where the initialization for the
unlabeled nodes has to be adapted, and the (partial) label push back has to be added before
the node information is propagated. The relevant parts are the ones marked in blue. Note that
for graphs with large fractions of labeled nodes it might be preferable to use label diffusion
even though they are partially labeled.

To implement propagation kernels between directed graphs, we can proceed as above after
simply deriving transition matrices computed from the potentially non-symmetric adjacency
matrices. That is, for the propagation kernel computation only the input changes (marked in
green in Algorithm 3). The same idea allows weighted edges to be accommodated; again,
only the transition matrix has to be adapted. Obviously, we can also combine partially labeled
graphs with directed or weighted edges by changing both the blue and green marked parts
accordingly.

6.3 Graphs with continuous node attributes

Nowadays, learning tasks often involve graphs whose nodes are attributed with continuous
information. Chemical compounds can be annotated with the length of the secondary struc-
ture elements (the nodes) or measurements for various properties, such as hydrophobicity
or polarity. 3d point clouds can be enriched with curvature information, and images are
inherently composed of 3-channel color information. All this information can be modeled by
continuous node attributes. In Eq. (10) we introduced a simple way to deal with attributes.
The resulting propagation kernel essentially counts similar label arrangements only if the
corresponding node attributes are similar as well. Note that for higher-dimensional attributes
it can be advantageous to compute separate lshs per dimension, leading to the node kernel
introduced in Eq. (11). This has the advantage that if we standardize the attributes, we can
use the same bin-width parameter wa for all dimensions. In all our experiments we normalize
each attribute to have unit standard deviation and will set wa = 1. The disadvantage of this
method, however, is that the arrangement of attributes in the graphs is ignored.

In the following, we derive p2k, a variant of propagation kernels for attributed graphs
based on the idea of propagating both attributes and labels. That is, we model graph similar-
ity by comparing the arrangement of labels and the arrangement of attributes in the graph.
The attribute kernel for p2k is deﬁned as in Eq. (12); now the question is how to efﬁciently
propagate the continuous attributes and how to efﬁciently model and hash the distributions
of (multivariate) continuous variables. Let X ∈ RN ×D be the design matrix, where a row xu
represents the attribute vector of node u. We will associate with each node of each graph a
probability distribution deﬁned on the attribute space, qu, and will update these as attribute
information is propagated across graph edges as before. One challenge in doing so is ensuring
that these distributions can be represented with a ﬁnite description. The discrete label distri-
butions from before have naturally a ﬁnite number of dimensions and could be compactly

123

Mach Learn (2016) 102:209–245

223

Algorithm 4 Propagation kernel (p2k) for attributed graphs (speciﬁc parts compared to the
general computation (Algorithm 1) are marked in green (input) and blue (computation)–Color
version online).
given: graph database {G(i)}i , # iterations tmax, transition matrix T , bin widths wl , wa , metrics ml , ma ,
base kernel (cid:12)·, ·(cid:13)
initialization: K ← 0, P0 ← δ(cid:2)(V )
µ = X, Σ = cov(X ), W0 = I
y ← rand(num-samples)
for t ← 0 . . . tmax do

(cid:14) gm initialization
(cid:14) sample points for gm evaluations

hl ← calculate- lsh(Pt , wl , ml )
Qt ← evaluate- pdfs(µ, Σ, Wt , y)
ha ← calculate- lsh(Qt , wa , ma )
h ← hl ∧ ha
for all graphs G(i) do

compute Φi· = φ(G

(i)
t

)

end for
Pt+1 ← T Pt
Wt+1 ← T Wt
K ← K + (cid:12)Φ, Φ(cid:13)

end for

(cid:14) bin label distributions
(cid:14) evaluate gms at y
(cid:14) bin attribute distributions
(cid:14) combine label and attribute bins

(cid:14) count bin strengths

(cid:14) propagate label information
(cid:14) propagate attribute information
(cid:14) compute and add kernel contribution

represented and updated via the Pt matrices. We seek a similar representation for attributes.
Our proposal is to deﬁne the node-attribute distributions to be mixtures of D-dimensional
multivariate Gaussians, one centered on each attribute vector in X :

qu =

Wuv N (xv, Σ),

(cid:6)

v

where the sum ranges over all nodes v, Wu,· is a vector of mixture weights, and Σ is a
shared D × D covariance matrix for each component of the mixture. In particular, here
we set Σ to be the sample covariance matrix calculated from the N vectors in X . Now
the N × N row-normalized W matrix can be used to compactly represent the entire set of
attribute distributions. As before, we will use the graph structure to iteratively spread attribute
information, updating these W matrices, deriving a sequence of attribute distributions for each
node to use as inputs to node attribute kernels in a propagation kernel scheme.

We begin by deﬁning the initial weight matrix W0 to be the identity matrix; this is equivalent
to beginning with each node attribute distribution being a single Gaussian centered on the
corresponding attribute vector:

Now, in each propagation step the attribute distributions are updated by the distribution of
their neighboring nodes Qt+1 ← Qt . We accomplish this by propagating the mixture weights
W across the edges of the graph according to a row-normalized transition matrix T , derived
as in Sect. 3:

W0 = I ;
q0,u = N (xu, Σ).

Wt+1 ← T Wt = T t ;
(cid:6)
qt+1,u =

(Wt )uv N (xv, Σ).

v

We have described how attribute distributions are associated with each node and how they
are updated via propagating their weights across the edges of the graph. However, the weight
vectors contained in W are not themselves directly suitable for comparing in an attribute

(15)

123

224

Mach Learn (2016) 102:209–245

kernel ka, because any information about the similarity of the mean vectors is ignored. For
example, imagine that two nodes u and v had exactly the same attribute vector, xu = xv. Then
mass on the u component of the Gaussian mixture is interchangeable with mass on the v com-
ponent; a simple kernel among weight vectors cannot capture this. For this reason, we use a
vector more appropriate for kernel comparison. Namely, we select a ﬁxed set of sample points
in attribute space (in our case, chosen uniformly from the node attribute vectors in X ), evaluate
the pdfs of the Gaussian mixtures associated with each node at these points, and use this vector
to summarize the node information. This handles the exchangeability issue from above and
also allows a more compact representation for hash inputs; in our experiments, we used 100
sample points and achieved good performance. As before, these vectors can then be hashed
jointly or individually for each sample point. Note that the bin width wa has to be adapted
accordingly. In our experiments, we will use the latter option and set wa = 1 for all datasets.
The computational details of p2k are given in Algorithm 4, where the additional parts
compared to Algorithm 1 are marked in blue (computation) and green (input). An extension
to Algorithm 4 would be to reﬁt the gms after a couple of propagation iterations. We did not
consider reﬁtting in our experiments as the number of kernel iterations tmax was set to 10
or 15 for all datasets—following the descriptions in existing work on iterative graph kernels
(Shervashidze et al. 2011; Neumann et al. 2012).

6.4 Grid graphs

One of our goals in this paper is to compute propagation kernels for pixel grid graphs. A
graph kernel between grid graphs can be deﬁned such that two grids should have a high
kernel value if they have similarly arranged node information. This can be naturally captured
by propagation kernels as they monitor information spread on the grids. Naïvely, one could
think that we can simply apply Algorithm 3 to achieve this goal. However, given that the
space complexity of this algorithm scales with the number of edges and even medium sized
images such as texture patches will easily contain thousands of nodes, this is not feasible. For
example considering 100 × 100-pixel image patches with an 8-neighborhood graph struc-
ture, the space complexity required would be 2.4 million units3 (ﬂoating point numbers) per
graph. Fortunately, we can exploit the ﬂexibility of propagation kernels by exchanging the
propagation scheme. Rather than label diffusion as used earlier, we employ discrete convo-
lution; this idea was introduced for efﬁcient clustering on discrete lattices (Bauckhage and
Kersting 2013). In fact, isotropic diffusion for denoising or sharpening is a highly developed
technique in image processing (Jähne 2005). In each iteration, the diffused image is derived
as the convolution of the previous image and an isotropic (linear and space-invariant) ﬁlter.
In the following, we derive a space- and time-efﬁcient way of computing propagation kernels
for grid graphs by means of convolutions.

6.4.1 Basic Deﬁnitions

Given that the neighborhood of a node is the subgraph induced by all its adjacent vertices,
we deﬁne a d-dimensional grid graph as a lattice graph whose node embedding in Rd forms
a regular square tiling and the neighborhoods N of each non-border node are isomorphic
(ignoring the node information). Figure 3 illustrates a regular square tiling and several iso-
morphic neighborhoods of a 2-dimensional grid graph. If we ignore boundary nodes, a grid

3 Using a coordinate list sparse representation, the memory usage per pixel grid graph for Algorithm 3 is
O(3m1m2 p), where m1 × m2 are the grid dimensions and p is the size of the pixel neighborhood.

123

Mach Learn (2016) 102:209–245

225

(a)

(b)

(c)

(d)

Fig. 3 Grid graph. Regular square tiling (a) and three example neighborhoods (b–d) for a 2-dimensional
grid graph derived from line graphs L7 and L6. a Square tiling, b 4-neighborhood, c 8-neighborhood, d
non-symmetric neighborhood

graph is a regular graph; i.e., each non-border node has the same degree. Note that the size
of the border depends on the radius of the neighborhood. In order to be able to neglect the
special treatment for border nodes, it is common to view the actual grid graph as a ﬁnite
section of an actually inﬁnite graph.

A grid graph whose node embedding in Rd forms a regular square tiling can be derived

from the graph Cartesian product of line graphs. So, a two-dimensional grid is deﬁned as

G(i) = L mi,1

× L mi,2

,

where L mi,1 is a line graph with mi,1 nodes. G(i) consists of ni = mi,1 mi,2 nodes, where
non-border nodes have the same number of neighbors. Note that the grid graph G(i) only
speciﬁes the node layout in the graph but not the edge structure. The edges are given by
the neighborhood N which can be deﬁned by any arbitrary matrix B encoding the weighted
adjacency of its center node. The nodes, being for instance image pixels, can carry discrete
or continuous vector-valued information. Thus, in the most-general setting the database of
grid graphs is given by G = {G(i)}i=1,...,n with G(i) = (V (i), N , (cid:2)), where (cid:2): V (i) → L
with L = ([k], RD). Commonly used neighborhoods N are the 4-neighborhood and the
8-neighborhood illustrated in Fig. 3b, c.

6.4.2 Discrete Convolution

The general convolution operation on two functions f and g is deﬁned as

f (x) ∗ g(x) = ( f ∗ g)(x) =

f (τ ) g(x − τ ) dτ.

(cid:23) ∞

−∞

That is, the convolution operation produces a modiﬁed, or ﬁltered, version of the original
function f . The function g is called a ﬁlter. For two-dimensional grid graphs interpreted as
discrete functions of two variables x and y, e.g., the pixel location, we consider the discrete
spatial convolution deﬁned by:

f (x, y) ∗ g(x, y) = ( f ∗ g)(x, y) =

f (i, j) g(x − i, y − j),

∞(cid:6)

∞(cid:6)

i=−∞

j=−∞

where the computation is in fact done for ﬁnite intervals. As convolution is a well-studied
operation in low-level signal processing and discrete convolution is a standard operation in
digital image processing, we can resort to highly developed algorithms for its computation;
see for example Chapter 2 in Jähne (2005). Convolutions can be computed efﬁciently via the
fast Fourier transformation in O(ni log ni ) per graph.

123

226

Mach Learn (2016) 102:209–245

Algorithm 5 Propagation kernel for grid graphs (speciﬁc parts compared to the general
computation (Algorithm 1) are marked in green (input) and blue (computation)–Color version
online).
given: graph database {G(i)}i , # iterations tmax, ﬁlter matrix B, bin width w, metric m, base kernel (cid:12)·, ·(cid:13)
(i)
initialization: K ← 0, P
0
for t ← 0 . . . tmax do

← δ(cid:2)(Vi ) ∀i

(i)
calculate- lsh({P
t
for all graphs G(i) do

}i , w, m)

compute Φi· = φ(G

(i)
t

)

end for
for all graphs G(i)
(i, j)
← P
t

(i, j)
P
t+1
end for
K ← K + (cid:12)Φ, Φ(cid:13)

end for

and labels j do
∗ B

(cid:14) bin node information

(cid:14) count bin strengths

(cid:14) discrete convolution

(cid:14) compute and add kernel contribution

6.4.3 Efﬁcient Propagation Kernel Computation

Now let G = {G(i)}i be a database of grid graphs. To simplify notation, however without
loss of generality, we assume two-dimensional grids G(i) = L mi,1
× L mi,2 . Unlike in the
case of general graphs, each graph now has a natural two-dimensional structure, so we will
update our notation to reﬂect this structure. Instead of representing the label probability
distributions of each node as rows in a two-dimensional matrix, we now represent them
(i)
∈ Rmi,1×mi,2×k. Modifying the
in the third dimension of a three-dimensional tensor P
t
structure makes both the exposition more clear and also enables efﬁcient computation. Now,
we can simply consider discrete convolution on k matrices of label probabilities P (i, j) per
grid graph G(i), where P (i, j) ∈ Rmi,1×mi,2 contains the probabilities of all nodes in G(i)
(i)
of being label j and j ∈ {1, . . . , k}. For observed labels, P
is again initialized with a
0
Kronecker delta distribution across the third dimension and, in each propagation step, we
perform a discrete convolution of each matrix P (i, j) per graph. Thus, we can create various
propagation schemes efﬁciently by applying appropriate ﬁlters, which are represented by
matrices B in our discrete case. We use circular symmetric neighbor sets Nr, p as introduced
in Ojala et al. (2002), where each pixel has p neighbors which are equally spaced pixels on
a circle of radius r . We use the following approximated ﬁlter matrices in our experiments:

⎤

⎦ , N1,8 =

N1,4 =

N2,16 =

⎡

⎣

⎡

⎢
⎢
⎢
⎢
⎣

0
0.25
0
0.01
0.06
0.09
0.06
0.01

0.25
0
0.25
0.06
0.04
0
0.04
0.06

0
0.25
0
0.09
0
0
0
0.09

0.06
0.04
0
0.04
0.06

⎡

⎣

0.06
0.17
0.06
⎤

⎥
⎥
⎥
⎥
⎦

.

0.01
0.06
0.09
0.06
0.01

0.17
0.05
0.17

⎤

0.06
0.17
0.06

⎦ , and

(16)

The propagation kernel computation for grid graphs is summarized in Algorithm 5, where
the speciﬁc parts compared to the general propagation kernel computation (Algorithm 1)
are highlighted in green (input) and blue (computation). Using fast Fourier transformation,
the time complexity of Algorithm 5 is O
. Note that for
the purpose of efﬁcient computation, calculate- lsh has to be adapted to take the label

(tmax − 1)N log N + tmax n2 n(cid:6)

(cid:13)

(cid:12)

123

Mach Learn (2016) 102:209–245

227

(i)
distributions {P
}i as a set of 3-dimensional tensors. By virtue of the invariance of the
t
convolutions used, propagation kernels for grid graphs are translation invariant, and when
using the circular symmetric neighbor sets they are also 90-degree rotation invariant. These
properties make them attractive for image-based texture classiﬁcation. The use of other ﬁlters
implementing for instance anisotropic diffusion depending on the local node information is
a straightforward extension.

7 Experimental evaluation

Our intent here is to investigate the power of propagation kernels (pks) for graph classiﬁcation.
Speciﬁcally, we ask:
(Q1) How sensitive are propagation kernels with respect to their parameters, and how should

propagation kernels be used for graph classiﬁcation?

(Q2) How sensitive are propagation kernels to missing and noisy information?
(Q3) Are propagation kernels more ﬂexible than state-of-the-art graph kernels?
(Q4) Can propagation kernels be computed faster than state-of-the-art graph kernels while

achieving comparable classiﬁcation performance?

Towards answering these questions, we consider several evaluation scenarios on diverse
graph datasets including chemical compounds, semantic image scenes, pixel texture images,
and 3d point clouds to illustrate the ﬂexibility of pks.

7.1 Datasets

The datasets used for evaluating propagation kernels come from a variety of different domains
and thus have diverse properties. We distinguish graph databases of labeled and attributed
graphs, where attributed graphs usually also have label information on the nodes. Also, we
separate image datasets where we use the pixel grid graphs from general graphs, which
have varying node degrees. Table 1 summarizes the properties of all datasets used in our
experiments.4

7.1.1 Labeled Graphs

For labeled graphs, we consider the following benchmark datasets from bioinformatics:
mutag, nci1, nci109, and d&d. mutag contains 188 sets of mutagenic aromatic and
heteroaromatic nitro compounds, and the label refers to their mutagenic effect on the Gram-
negative bacterium Salmonella typhimurium (Debnath et al. 1991). nci1 and nci109 are
anti-cancer screens, in particular for cell lung cancer and ovarian cancer cell lines, respec-
tively (Wale and Karypis 2006). d&d consists of 1178 protein structures (Dobson and Doig
2003), where the nodes in each graph represent amino acids and two nodes are connected
by an edge if they are less than 6 Ångstroms apart. The graph classes are enzymes and
non-enzymes.

7.1.2 Partially Labeled Graphs

The two real-world image datasets msrc 9-class and msrc 21-class5 are state-of-the-art
datasets in semantic image processing originally introduced in Winn et al. (2005). Each

4 All datasets are publicly available for download from http://tiny.cc/PK_MLJ_data.
5 http://research.microsoft.com/en-us/projects/objectclassrecognition/.

123

228

Mach Learn (2016) 102:209–245

# Graphs Median #

Max #
nodes

Total #
nodes

# Node
labels

# Graph
labels

Attr. dim.

Table 1 Dataset statistics and properties

Dataset

Properties

mutag

nci1

nci109

d&d

msrc9

msrc21

db

synthetic

enzymes

proteins
pro-full
bzr

cox2

dhfr

brodatz

plants

188

4110

4127

1178

221

563

41

300

600

1113

1113

405

467

756

2048

2957

nodes

17.5
27

26

241

40

76

964

100

32

26

26

35

41

42

4096

4725

28

111

111

5748

55

141

5037

100

126

620

620

57

56

71

4096

5625

3371
122, 747
122, 494
334, 925
8968
43, 644
56, 468
30, 000
19, 580
43, 471
43, 471
14, 479
19, 252
32, 075
8, 388, 608
13, 587, 375

7

37

38

82

10

24

5
−

10

3

3

3

8

9

3

5

20

11

2

2

2

2

8

2

6

2

2

2

2

2

32

6

−

−

−

−

−

−

1

1

18

1

29

3

3

3
−

−

image is represented by a conditional Markov random ﬁeld graph, as illustrated in Fig. 4a, b.
The nodes of each graph are derived by oversegmenting the images using the quick shift algo-
rithm,6 resulting in one graph among the superpixels of each image. Nodes are connected if
the superpixels are adjacent, and each node can further be annotated with a semantic label.
Imagining an image retrieval system, where users provide images with semantic information,
it is realistic to assume that this information is only available for parts of the images, as it
is easier for a human annotator to label a small number of image regions rather than the
full image. As the images in the msrc datasets are fully annotated, we can derive semantic
(ground-truth) node labels by taking the mode ground-truth label of all pixels in the corre-
sponding superpixel. Semantic labels are, for example, building, grass, tree, cow, sky, sheep,
boat, face, car, bicycle, and a label void to handle objects that do not fall into one of these
classes. We removed images consisting of solely one semantic label, leading to a classiﬁcation
task among eight classes for msrc9 and 20 classes for msrc21.

7.1.3 Attributed Graphs

To evaluate the ability of pks to incorporate continuous node attributes, we consider the
attributed graphs used in Feragen et al. (2013), Kriege and Mutzel (2012). Apart from one
synthetic dataset (synthetic), the graphs are all chemical compounds (enzymes, proteins,
pro-full, bzr, cox2, and dhfr). synthetic comprises 300 graphs with 100 nodes, each
endowed with a one-dimensional normally distributed attribute and 196 edges each. Each
graph class, A and B, has 150 examples, where in A, 10 node attributes were ﬂipped randomly

6 http://www.vlfeat.org/overview/quickshift.html.

123

Mach Learn (2016) 102:209–245

229

(a)

(b)

(c)

Fig. 4 Semantic scene and point cloud graphs. The rgb image in (a) is represented by a graph of superpixels
(b) with semantic labels b = building, c = car, v = void, and ? = unlabeled. c Point clouds of household
objects represented by labeled 4-nn graphs with part labels top (yellow), middle (blue), bottom (red), usable-
area (cyan), and handle (green). Edge colors are derived from the adjacent nodes. a rgb image, b superpixel
graph, c point cloud graphs (Color ﬁgure online)

and in B, 5 were ﬂipped randomly. Further, noise drawn from N (0, 0.452) was added to the
attributes in B. proteins is a dataset of chemical compounds with two classes (enzyme and
non-enzyme) introduced in Dobson and Doig (2003). enzymes is a dataset of protein tertiary
structures belonging to 600 enzymes from the brenda database (Schomburg et al. 2004). The
graph classes are their ec (enzyme commission) numbers which are based on the chemical
reactions they catalyze. In both datasets, nodes are secondary structure elements (sse), which
are connected whenever they are neighbors either in the amino acid sequence or in 3d space.
Node attributes contain physical and chemical measurements including length of the sse in
Ångstrom, its hydrophobicity, its van der Waals volume, its polarity, and its polarizability. For
bzr, cox2, and dhfr—originally used in Mahé and Vert (2009)—we use the 3d coordinates
of the structures as attributes.

7.1.4 Point Cloud Graphs

In addition, we consider the object database db,7 introduced in Neumann et al. (2013). db is
a collection of 41 simulated 3d point clouds of household objects. Each object is represented
by a labeled graph where nodes represent points, labels are semantic parts (top, middle,
bottom, handle, and usable-area), and the graph structure is given by a k-nearest neighbor
(k-nn) graph w.r.t. Euclidean distance of the points in 3d space, cf. Fig. 4c. We further
endowed each node with a continuous curvature attribute approximated by its derivative, that
is, by the tangent plane orientations of its incident nodes. The attribute of node u is given by
xu =
v∈N (u) 1 − |nu · nv|, where nu is the normal of point u and N (u) are the neighbors
of node u. The classiﬁcation task here is to predict the category of each object. Examples of
the 11 categories are glass, cup, pot, pan, bottle, knife, hammer, and screwdriver.

(cid:2)

7.1.5 Grid Graphs

We consider a classical benchmark dataset for texture classiﬁcation (brodatz) and a dataset
for plant disease classiﬁcation (plants). All graphs in these datasets are grid graphs derived
from pixel images. That is, the nodes are image pixels connected according to circular sym-
metric neighbor sets Nr, p as exempliﬁed in Eq. (16). Node labels are computed from the rgb
color values by quantization.

7 http://www.ﬁrst-mm.eu/data.html.

123

230

Mach Learn (2016) 102:209–245

Fig. 5 Example images from brodatz (a, b) and plants (c, d) and the corresponding quantized versions
with three colors (e, f) and ﬁve colors (g, h). a bark, b grass, c phoma, d cercospora, e bark-3, f grass-3, g
phoma-5, h cercospora-5 (Color ﬁgure online)

brodatz,8 introduced in Valkealahti and Oja (1998), covers 32 textures from the Brodatz
album with 64 images per class comprising the following subsets of images: 16 “original”
images (o), 16 rotated versions (r), 16 scaled versions (s), and 16 rotated and scaled versions
(rs) of the “original” images. Figure 5a, b show example images with their corresponding
quantized versions (e) and (f). For parameter learning, we used a random subset of 20 % of
the original images and their rotated versions, and for evaluation we use test suites similar
to the ones provided with the dataset.9 All train/test splits are created such that whenever an
original image (o) occurs in one split, their modiﬁed versions (r,s,rs) are also included in
the same split.

The images in plants, introduced in Neumann et al. (2014), are regions showing disease
symptoms extracted from a database of 495 rgb images of beet leaves. The dataset has six
classes: ﬁve disease symptoms cercospora, ramularia, pseudomonas, rust, and phoma, and
one class for extracted regions not showing a disease symptom. Figure 5c, d illustrates two
regions and their quantized versions (g) and (h). We follow the experimental protocol in
Neumann et al. (2014) and use 10 % of the full data covering a balanced number of classes
(296 regions) for parameter learning and the full dataset for evaluation. Note that this dataset
is highly imbalanced, with two infrequent classes accounting for only 2 % of the examples
and two frequent classes covering 35 % of the examples.

7.2 Experimental protocol

We implemented propagation kernels in Matlab10 and classiﬁcation performance on all
datasets except for db is evaluated by running c- svm classiﬁcations using libSVM.11

8 http://www.ee.oulu.ﬁ/research/imag/texture/image_data/Brodatz32.html.
9 The test suites provided with the data are incorrect; we use a corrected version.
10 https://github.com/marionmari/propagation_kernels.
11 http://www.csie.ntu.edu.tw/~cjlin/libsvm/.

123

Mach Learn (2016) 102:209–245

231

For the parameter analysis (Sect. 7.3), the cost parameter c was learned on the full dataset
(c ∈ {10−3, 10−1, 101, 103} for normalized kernels and c ∈ {10−3, 10−2, 10−1, 100} for
unnormalized kernels), for the sensitivity analysis (Sect. 7.4), it was set to its default value
of 1 for all datasets, and for the experimental comparison with existing graph kernels
(Sect. 7.5), we learned it via 5-fold cross-validation on the training set for all methods
(c ∈ {10−7, 10−5, . . . , 105, 107} for normalized kernels and c ∈ {10−7, 10−5, 10−3, 10−1}
for unnormalized kernels). The number of kernel iterations tmax was learned on the training
splits (tmax ∈ {0, 1, . . . , 10} unless stated otherwise). Reported accuracies are an average of
10 reruns of a stratiﬁed 10-fold cross-validation.

For db, we follow the protocol introduced in Neumann et al. (2013). We perform a leave-
one-out (loo) cross validation on the 41 objects in db, where the kernel parameter tmax is
learned on each training set again via loo. We further enhanced the nodes by a standardized
continuous curvature attribute, which was only encoded in the edge weights in previous work
(Neumann et al. 2013).

For all pks, the lsh bin-width parameters were set to wl = 10−5 for labels and to wa = 1
for the normalized attributes, and as lsh metrics we chose ml = tv and ma = l1 in all
experiments. Before we evaluate classiﬁcation performance and runtimes of the proposed
propagation kernels, we analyze their sensitivity towards the choice of kernel parameters and
with respect to missing and noisy observations.

7.3 Parameter analysis

To analyze parameter sensitivity with respect to the kernel parameters w (lsh bin width)
and tmax (number of kernel iterations), we computed average accuracies over 10 randomly
generated test sets for all combinations of w and tmax, where w ∈ {10−8, 10−7, . . . , 10−1}
and tmax ∈ {0, 1, . . . , 14} on mutag, enzymes, nci1, and db. The propagation kernel
computation is as described in Algorithm 3, that is, we used the label information on the nodes
and the label diffusion process as propagation scheme. To assess classiﬁcation performance,
we performed a 10-fold cross validation (cv). Further, we repeated each of these experiments
with the normalized kernel, where normalization means dividing each kernel value by the
square root of the product of the respective diagonal entries. Note that for normalized kernels
we test for larger svm cost values. Figure 6 shows heatmaps of the results.

In general, we see that the pk performance is relatively smooth, especially if w < 10−3
and tmax > 4. Speciﬁcally, the number of iterations leading to the best results are in the
range from {4, . . . , 10} meaning that we do not have to use a larger number of iterations in
the pk computations, helping to keep a low computation time. This is especially important
for parameter learning. Comparing the heatmaps of the normalized pk to the unnormalized
pk leads to the conclusion that normalizing the kernel matrix can actually hurt performance.
This seems to be the case for the molecular datasets mutag and nci1. For mutag, Fig. 6a, b,
the performance drops from 88.2 to 82.9 %, indicating that for this dataset the size of the
graphs, or more speciﬁcally the amount of labels from the different kind of node classes, are a
strong class indicator for the graph label. Nevertheless, incorporating the graph structure, i.e.,
comparing tmax = 0 to tmax = 10, can still improve classiﬁcation performance by 1.5 %. For
other prediction scenarios such as the object category prediction on the db dataset, Fig. 6g, h,
we actually want to normalize the kernel matrix to make the prediction independent of the
object scale. That is, a cup scanned from a larger distance being represented by a smaller
graph is still a cup and should be similar to a larger cup scanned from a closer view. So, for
our experiments on object category prediction we will use normalized graph kernels whereas
for the chemical compounds we will use unnormalized kernels unless stated otherwise.

123

232

x
a
m
t

x
a
m
t

0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

x
a
m
t

x
a
m
t

0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

Mach Learn (2016) 102:209–245

90%

85%

80%

75%

70%

85%

80%

75%

70%

65%

0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

x
a
m
t

x
a
m
t

x
a
m
t

x
a
m
t

0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

50%

45%

40%

35%

30%

25%

80%

75%

70%

65%

60%

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

(a)

(b)

(c)

(d)

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1
w

(e)

(f)

(g)

(h)

Fig. 6 Parameter sensitivity of pk. The plots show heatmaps of average accuracies (tenfold cv) of pk (labels
only) w.r.t. the bin widths parameter w and the number of kernel iterations tmax for four datasets mutag,
enzymes, nci1, and db. In panels (a, c, e, g) we used the kernel matrix directly, in panels (b, d, f, h) we
normalized the kernel matrix. The svm cost parameter is learned for each combination of w and tmax on
the full dataset. × marks the highest accuracy. a mutag; b mutag, normalized; c enzymes; d enzymes,
normalized; e nci1; f nci1, normalized; g db and h db, normalized

Recall that our propagation kernel schemes are randomized algorithms, as there is ran-
domization inherent in the choice of hyperplanes used during the lsh computation. We ran
a simple experiment to test the sensitivity of the resulting graph kernels with respect to the
hyperplane used. We computed the pk between all graphs in the datasets mutag, enzymes,
msrc9, and msrc21 with tmax = 10 100 times, differing only in the random selection of the
lsh hyperplanes. To make comparisons easier, we normalized each of these kernel matri-
ces. We then measured the standard deviation of each kernel entry across these repetitions
to gain insight into the stability of the pk to changes in the lsh hyperplanes. The median
standard deviations were: mutag: 5.5 × 10−5, enzymes: 1.1 × 10−3, msrc9: 2.2 × 10−4,
and msrc21: 1.1 × 10−4. The maximum standard deviations over all pairs of graphs were:
mutag: 6.7 × 10−3, enzymes: 1.4 × 10−2, msrc9: 1.4 × 10−2, and msrc21: 1.1 × 10−2.
Clearly the pk values are not overly sensitive to random variation due to differing random
lsh hyperplanes.

In summary, we can answer (Q1) by concluding that pks are not overly sensitive to the
random selection of the hyperplane as well as to the choice of parameters and we propose
to learn tmax ∈ {0, 1, . . . , 10} and ﬁx w ≤ 10−3. Further, we recommend to decide on using
the normalized version of pks only when graph size invariance is deemed important for the
classiﬁcation task.

7.4 Sensitivity to missing and noisy information

This section analyzes the performance of propagation kernels in the presence of missing and
noisy information.

To asses how sensitive propagation kernels are to missing information, we randomly
selected x % of the nodes in all graphs of db and removed their labels (labels) or attributes
(attr), where x ∈ {0, 10, . . . , 90, 95, 98, 99, 99.5, 100}. To study the performance when

123

Mach Learn (2016) 102:209–245

233

both label and attribute information is missing, we selected (independently) x % of the nodes
to remove their label information and x% of the nodes to remove their attribute information
(labels & attr). Figure 7 shows the average accuracy of 10 reruns. While we see that
the accuracy decreases with more missing information, the performance remains stable in
the case when attribute information is missing. This suggests that the label information is
more important for the problem of object category prediction. Further, the standard error
is increasing with more missing information, which corresponds to the intuition that fewer
available information results in a higher variance in the predictions.

We also compare the predictive performance of propagation kernels when only some
graphs, as for instance graphs at prediction time, have missing labels. Therefore, we divided
the graphs of the following datasets, mutag, enzymes, msrc9, and msrc21, into two groups.
For one group (fully labeled) we consider all nodes to be labeled, and for the other group (miss-
ing labels) we remove x % of the labels at random, where x ∈ {10, 20, . . . , 90, 91, . . . , 99}.
Figure 8 shows average accuracies over 10 reruns for each dataset. Whereas for mutag we
do not observe a signiﬁcant difference of the two groups, for enzymes the graphs with miss-
ing labels could only be predicted with lower accuracy, even when only 20 % of the labels
were missing. For both msrc datasets, we observe that we can still predict the graphs with
full label information quite accurately; however, the classiﬁcation accuracy for the graphs
with missing information decreases signiﬁcantly with the amount of missing labels. For all
datasets removing even 99 % of the labels still leads to better classiﬁcation results than a ran-
dom predictor. This result may indicate that the size of the graphs itself bears some predictive
information. This observation conﬁrms the results from Sect. 7.3.

The next experiment analyzes the performance of propagation kernels when label infor-
mation is encoded as attributes in a one-hot encoding. We also examine how sensitive they
are in the presence of label noise. We corrupted the label encoding by an increasing amount
of noise. A noisy label distribution vector nu was generated by sampling nu,i ∼ U(0, 1) and
nu,i = 1. Given a noise level α, we used the following values encoded
normalizing so that
as attributes

(cid:2)

xu ← (1 − α) xu + α nu.
Figure 9 shows average accuracies over 10 reruns for msrc9, msrc21, mutag, and enzymes.
First, we see that using the attribute encoding of the label information in a p2k variant only
propagating attributes achieves similar performances to propagating the labels directly in
pk. This conﬁrms that the Gaussian mixture approximation of the attribute distributions is
a reasonable choice. Moreover, we can observe that the performance on msrc9 and mutag
is stable across the tested noise levels. For msrc21 the performance drops for noise levels
larger than 0.3. Whereas the same happens for enzymes, adding a small amount of noise
(α = 0.1) actually increases performance. This could be due to a regularization effect caused
by the noise and should be investigated in future work.

Finally, we performed an experiment to test the sensitivity of pks with respect to noise in
edge weights. For this experiment, we used the datasets bzr, cox2, and dhfr, and deﬁned
edge weights between connected nodes according to the distance between the corresponding
structure elements in 3d space. Namely, the edge weight (before row normalization) was
taken to be the inverse Euclidean distance between the incident nodes. Given a noise-level
σ , we corrupted each edge weight by multiplying by random log-normally distributed noise:
wi j ← exp(log(wi j ) + ε),
where ε ∼ N (0, σ 2). Figure 10 shows the average test accuracy across ten repetitions of 10-
fold cross-validation for this experiment. The bzr and cox2 datasets tolerated a large amount

123

234

Mach Learn (2016) 102:209–245

)

%

(

c
c
a

.
g
v
a

90

75

60

45

30

15

0

fully labeled
overall
missing labels
random

fully labeled
overall
missing labels
random

)

%

(

c
c
a

.
g
v
a

)

%

(

c
c
a

.
g
v
a

90

85

80

75

70

65

60

55

50

90

80

70

60

50

40

30

20

10

123

attr

labels

labels & attr

random

)

%

(

c
c
a

.
g
v
a

)

%

(

c
c
a

.
g
v
a

40

35

30

25

20

15

10

100

80

60

40

20

0

fully labeled
overall
missing labels
random

fully labeled
overall
missing labels
random

0

10 20 30 40 50 60 70 80 90 95 98 99 99.5 100

missing information (%)

Fig. 7 Missing information versus accuracy on db. We plot missing information (%) versus avg. accuracy
(%) ± standard error of p2k on db. For labels only label information is missing, for attr only attribute
information is missing and for labels & attr both is missing. The reported accuracies are averaged over ten
reruns. random indicates the result of a random predictor

10 20 30 40 50 60 70 80 90 91 92 93 94 95 96 97 98 99
missing information (%)
(a)

10 20 30 40 50 60 70 80 90 91 92 93 94 95 96 97 98 99
missing information (%)

(b)

10 20 30 40 50 60 70 80 90 91 92 93 94 95 96 97 98 99
missing information (%)
(c)

10 20 30 40 50 60 70 80 90 91 92 93 94 95 96 97 98 99

missing information (%)

(d)

Fig. 8 Missing node labels. We plot missing labels (%) versus avg. accuracy (%) ± standard error for mutag,
enzymes, msrc9, and msrc21. We divided the graphs randomly in two equally sized groups (fully labeled
and missing labels). The reported accuracies are averaged over 10 reruns. random indicates the result of a
predictor randomly choosing the class label. a mutag, b enzymes, c msrc9, d msrc21

Mach Learn (2016) 102:209–245

235

)

%

(

c
c
a

.
g
v
a

95

90

85

80

75

70

)

%

(

c
c
a

.
g
v
a

50

45

40

35

30

25

msrc9
msrc21
mutag

0.2
noise level α

0

0.1

0.3

0.4

0

0.1

0.3

0.4

Fig. 9 Noisy node labels. Noise level α is plotted versus avg. accuracy ± standard error of 10 reruns on
msrc9, msrc21, mutag, and enzymes when encoding the labels as k-dimensional attributes using a one-hot
representation and attribute propagation as in Algorithm 4. The dashed lines indicate the performance when
using the usual label encoding without noise and Algorithm 3

enzymes

0.2
noise level α

)

%

(

c
c
a

.
g
v
a

90

85

80

75

70

65

60

bzr
cox2
dhfr

0

10−4

10−2

10−1

10−3
noise level σ

Fig. 10 Noisy edge weights. Noise level σ is plotted versus avg. accuracy ± standard deviation of 10 reruns
on bzr, cox2, and dhfr

of edge-weight noise without a large effect on predictive performance, whereas dhfr was
somewhat more sensitive to larger noise levels.

Summing up these experimental results we answer (Q2) by concluding that propagation

kernels behave well in the presence of missing and noisy information.

7.5 Comparison to existing graph kernels

We compare classiﬁcation accuracy and runtime of propagation kernels (pk) with the follow-
ing state-of-the-art graph kernels: the Weisfeiler–Lehman subtree kernel (wl) (Shervashidze
et al. 2011), the shortest path kernel (sp) (Borgwardt and Kriegel 2005), the graph hop-
per kernel (gh) (Feragen et al. 2013), and the common subgraph matching kernel (csm)
(Kriege and Mutzel 2012). Table 2 lists all graph kernels and the types of information they

123

Mach Learn (2016) 102:209–245

236

pk

wl

sp

gh

csm

Table 2 Graph kernels and their intended use

Kernel

Information type

Node labels

Partial labels

Edge weights

Edge labels Node attributes Huge grids

Yes

Yes

Yes

Yes

Yes

Yes

–

–

–

–

Yes

–

Yes

Yes

Yes

–

–

–

Yes

Yes

Yes

–

Yes

Yes

Yes

(fast scaling)

Yes

–

–

–

–

are intended for. For all wl computations, we used the fast implementation12 introduced in
(Kersting et al. 2014). In sp, gh, and csm, we used a Dirac kernel to compare node labels and
a Gaussian kernel ka(u, v) = exp(−γ (cid:20)xu − xv(cid:20)2) with γ = 1/D for attribute information, if
feasible. csm for the bigger datasets (enzymes, proteins, synthetic) was computed using
a Gaussian truncated for inputs with (cid:20)xu − xv(cid:20) > 1. We made this decision to encourage
sparsity in the generated (node) kernel matrices, reducing the size of the induced product
graphs and speeding up computation. Note that this is technically not a valid kernel between
nodes; nonetheless, the resulting graph kernels were always positive deﬁnite. For pk and wl
the number of kernel iterations (tmax or hmax) and for csm the maximum size of subgraphs
(k) was learned on the training splits via 10-fold cross validation. For all runtime experiments
all kernels were computed for the largest value of tmax, hmax, or k, respectively. We used
a linear base kernel for all kernels involving count features, and attributes, if present, were
standardized. Further, we considered two baselines that do not take the graph structure into
account. labels, corresponding to a pk with tmax = 0, only compares the label proportions
in the graphs and a takes the mean of a Gaussian node kernel among all pairs of nodes in the
respective graphs.

7.5.1 Graph classiﬁcation on benchmark data

In this section, we consider graph classiﬁcation for fully labeled, partially labeled, and
attributed graphs.
Fully labeled graphs The experimental results for labeled graphs are shown in Table 3. On
mutag, the baseline using label information only (labels) already gives the best perfor-
mance indicating that for this dataset the actual graph structure is not adding any predictive
information. On nci1 and nci109, wl performs best; however, propagation kernels come in
second while being computed over one minute faster. Although sp can be computed quickly, it
performs signiﬁcantly worse than pk and wl. This is also the case for gh, whose computation
time is signiﬁcantly higher. In general, the results on labeled graphs show that propagation
kernels can be computed faster than state-of-the-art graph kernels but achieve comparable
classiﬁcation performance, thus question (Q4) can be answered afﬁrmatively.
Partially labeled graphs To assess the predictive performance of propagation kernels on
partially labeled graphs, we ran the following experiments 10 times. We randomly removed
20–80 % of the node labels in all graphs in msrc9 and msrc21 and computed cross-validation
accuracies and standard errors. Because the wl-subtree kernel was not designed for partially

12 https://github.com/rmgarnett/fast_wl.

123

Mach Learn (2016) 102:209–245

237

Table 3 Labeled graphs

Method

Dataset

mutag

nci1

nci109

dd

pk

wl

sp

gh

labels

84.5 ± 0.6 (0.2
84.0 ± 0.4 (0.2
85.8 ± 0.2 (0.2
85.4 ± 0.5 (1.0
85.8 ± 0.2 (0.0

(cid:2)(cid:2))
(cid:2)(cid:2))
(cid:2)(cid:2))
(cid:2))
(cid:2)(cid:2))

(cid:2))
84.5 ± 0.1 (4.5
(cid:2))
85.9 ± 0.1 (5.6
(cid:2)(cid:2))
74.4 ± 0.1 (21.3
73.2 ± 0.1 (13.0h)
(cid:2)(cid:2))
64.6 ± 0.0 (0.8

(cid:2))
83.5 ± 0.1 (4.4
(cid:2))
85.9 ± 0.1 (7.4
(cid:2)(cid:2))
73.7 ± 0.0 (19.3
72.6 ± 0.1 (22.1h)
(cid:2)(cid:2))
63.6 ± 0.0 (0.7

(cid:2))
(cid:2))

78.8 ± 0.2 (3.6
79.0 ± 0.2 (6.7
out of time
68.9 ± 0.2 (69.1h)
(cid:2)(cid:2))
78.4 ± 0.1 (0.3

Average accuracies ± standard error of 10-fold cross validation (10 runs). Average runtimes in sec (x (cid:2)(cid:2)
), min
(x (cid:2)
), or hours (xh) are given in parentheses. All kernels are unnormalized. The kernel parameters tmax for pk
and hmax for wl were learned on the training splits (tmax, hmax ∈ {0, 1, . . . , 10}). labels corresponds to pk
with tmax = 0. out of time indicates that the kernel computation did not ﬁnish within 24 h. Bold indicates
that the method performs signiﬁcantly better than the second best method under a paired t test ( p < 0.05)

Table 4 Partially labeled graphs

Dataset

Method

Labels missing

20 %

40 %

60 %

80 %

msrc9

msrc21

lp + wl

lp + wl

pk

wl

pk

wl

90.0 ± 0.4
90.0 ± 0.2
89.2 ± 0.5
86.9 ± 0.3
85.8 ± 0.2
85.4 ± 0.4

88.7 ± 0.3
87.9 ± 0.6
88.1 ± 0.5
84.7 ± 0.3
81.5 ± 0.3
81.9 ± 0.4

86.6 ± 0.4
83.2 ± 0.6
85.7 ± 0.6
79.5 ± 0.3
74.5 ± 0.3
76.0 ± 0.3

80.4 ± 0.6
77.9 ± 1.0
78.5 ± 0.9
69.3 ± 0.3
64.0 ± 0.4
63.7 ± 0.4

Average accuracies (and standard errors) on 10 different sets of partially labeled images for pk and wl with
unlabeled nodes treated as additional label (wl) and with hard labels derived from converged label propagation
(lp + wl). Bold indicates that the method performs signiﬁcantly better than the second best method under a
paired t test ( p < 0.05)

labeled graphs, we compare pk to two variants: one where we treat unlabeled nodes as an
additional label “u” (wl) and another where we use hard labels derived from running label
propagation (lp) until convergence (lp + wl). For this experiment we did not learn the
number of kernel iterations, but selected the best performing tmax resp. hmax.

The results are shown in Table 4. For larger fractions of missing labels, pk obviously outper-
forms the baseline methods, and, surprisingly, running label propagation until convergence
and then computing wl gives slightly worse results than wl. However, label propagation
might be beneﬁcial for larger amounts of missing labels. The runtimes of the different meth-
ods on msrc21 are shown in Fig. 12 in the “Appendix 1”. wl computed via the string-based
implementation suggested in (Shervashidze et al. 2011) is over 36 times slower than pk. These
results again conﬁrm that propagation kernels have attractive scalability properties for large
datasets. The lp + wl approach wastes computation time while running lp to convergence
before it can even begin calculating the kernel. The intermediate label distributions obtained
during the convergence process are already extremely powerful for classiﬁcation. These
results clearly show that propagation kernels can successfully deal with partially labeled
graphs and suggest an afﬁrmative answer to questions (Q3) and (Q4).

123

238

Mach Learn (2016) 102:209–245

p2k

wl

sp

gh

csm

labels

attr

labels & attr

pk

100

90

80

70

60

50

40

90

85

80

)

%

(

y
c
a
r
u
c
c
a

)

%

(

y
c
a
r
u
c
c
a

70

60

50

40

75

70

65

60

)

%

(

y
c
a
r
u
c
c
a

)

%

(

y
c
a
r
u
c
c
a

1”

1’
101
runtime (s)

103 1h 104 10h 48h

101

1’ 102
runtime (s)

103

1 h

104 5 h

30

1”

(a)

(b)

1”

101

1’102

103

1 h 104

10 h

1’ 102

103

1 h

104 5 h

55

101

runtime (s)

(c)

runtime (s)

(d)

Fig. 11 Attributed graphs: log runtime versus accuracy. The plots show log(runtimes) in seconds plotted
against average accuracy (± standard deviation). Methods are encoded by color and the used information
(labels, attributes or both) is encoded by shape. Note that p2k also uses both, labels and attributes, however
in contrast to pk both are propagated. For pro-full the csm kernel computation exceeded 32 GB of memory.
On synthetic csm using the attribute information only could not be computed within 72 h. a synthetic, b
enzymes, c bzr, d pro-full (Color ﬁgure online)

Attributed graphs The experimental results for various datasets with attributed graphs are
illustrated in Fig. 11. The plots show runtime versus average accuracy, where the error
bars reﬂect standard deviation of the accuracies. As we are interested in good predictive
performance while achieving fast kernel computation, methods in the upper-left corners
provide the best performance with respect to both quality and speed. For pk, sp, gh, and csm
we compare three variants: one where we use the labels only, one where we use the attribute
information only, and one where both labels and attributes are used. wl is computed with label
information only. For synthetic, cf. Fig. 11a, we used the node degree as label information.
Further, we compare the performance of p2k, which propagates labels and attributes as
described in Sect. 6.3. Detailed results on synthetic and all bioinformatics datasets are
provided in Table 8 (average accuracies) and Table 7 (runtimes) in the “Appendix 2”. From
Fig. 11 we clearly see that propagation kernels tend to appear in the upper-left corner, that
is, they are achieving good predictive performance while being fast, leading to a positive
answer of question (Q4). Note that the runtimes are shown on a log scale. We can also
see that p2k, propagating both labels and attributes, (blue star) usually outperforms the the
simple pk implementation not considering attribute arrangements (blue diamond). However,
this comes at the cost of being slower. So, we can use the ﬂexibility of propagation kernels

123

239

attr

a

Mach Learn (2016) 102:209–245

Table 5 Point cloud graphs

labels

pk

75.6 ± 0.6
0.2

(cid:2)(cid:2)

labels & attr

wl

pk

p2k

acc±stderr
runtime

70.7 ± 0.0
0.4

(cid:2)(cid:2)

76.8 ± 1.3
0.3

(cid:2)(cid:2)

82.9 ± 0.0
(cid:2)(cid:2)
34.8

36.4 ± 0.0
(cid:2)(cid:2)
40.0

Average accuracies of loo cross validation on db. The reported standard errors refer to 10 kernel recomputa-
tions. Runtimes are given in sec (x (cid:2)(cid:2)
). All kernels are normalized and the kernel parameters, tmax for all pks
and hmax for wl, were learned on the training splits (tmax, hmax ∈ {0, . . . 15}). sp, gh, and csm were either
out of memory or the computation did not ﬁnish within 24h for all settings
Bold indicates that the method performs signiﬁcantly better than the second best method under a paired t-test
( p < 0.05)

to trade predictive quality against speed or vice versa according to the requirements of the
application at hand. This supports a positive answer to question (Q3).

7.5.2 Graph classiﬁcation on novel applications

The ﬂexibility of propagation kernels arising from easily interchangeable propagation
schemes and their efﬁcient computation via lsh allows us to apply graph kernels to novel
domains. First, we are able to compare larger graphs with reasonable time expended, opening
up the use of graph kernels for object category prediction of 3d point clouds in the context of
robotic grasping (Neumann et al. 2013). Depending on their size and the perception distance,
point clouds of household objects can easily consist of several thousands of nodes. Tradi-
tional graph kernels suffer from enormous computation times or memory problems even on
datasets like db, which can still be regarded medium sized. These issues aggravate even more
when considering image data. So far, graph kernels have been used for image classiﬁcation
on the scene level where the nodes comprise segments of similar pixels and one image is
then represented by less than 100 so-called superpixels. Utilizing off-the-shelf techniques for
efﬁcient diffusion on grid graphs allows the use of propagation kernels to analyze images on
the pixel level and thus opens up a whole area of interesting problems in the intersection of
graph-based machine learning and computer vision. As a ﬁrst step, we apply graph kernels,
more precisely propagation kernels, to texture classiﬁcation, where we consider datasets with
thousands of graphs containing a total of several millions of nodes.
3d object category prediction In this set of experiments, we follow the protocol introduced in
Neumann et al. (2013), where the graph kernel values are used to derive a prior distribution
on the object category for a given query object. The experimental results for the 3d-object
classiﬁcation are summarized in Table 5. We observe that propagation kernels easily deal
with the point-cloud graphs. From the set of baseline graph kernels considered, only wl was
feasible to compute, however with poor performance. Propagation kernels clearly beneﬁt
form their ﬂexibility as we can improve the classiﬁcation accuracy from 75.4 to 80.7 % when
considering the object curvature attribute. These results are extremely promising given that
we tackle a classiﬁcation problem with 11 classes having only 40 training examples for each
query object.
Grid graphs For brodatz and plants we follow the experimental protocol in Neumann et al.
(2014). The pk parameter tmax was learned on a training subset of the full dataset (tmax ∈
{0, 3, 5, 8, 10, 15, 20}). For plants this training dataset consists of 10 % of the full data; for
brodatz we used 20 % of the brodatz-o-r data as the number of classes in this dataset is

123

240

Table 6 Grid graphs

Method

pk

labels
glcm-gray
glcm-quant

Dataset

brodatz-o-r

(cid:2))
89.6 ± 0.0 (3.5
(cid:2))
5.0 ± 0.0 (1.1
87.2 ± 0.0 (29.5
78.6 ± 0.0 (24.9

(cid:2)(cid:2))
(cid:2)(cid:2))

Mach Learn (2016) 102:209–245

brodatz-o-r-s-rs

plants

(cid:2))
85.7 ± 0.0 (7.1
(cid:2))
4.9 ± 0.0 (2.2
79.4 ± 0.0 (44.8
68.6 ± 0.0 (44.8

(cid:2)(cid:2))
(cid:2)(cid:2))

(cid:2))
82.5 ± 0.1 (3.0
(cid:2)(cid:2))
59.5 ± 0.0 (11.5
(cid:2))
76.6 ± 0.0 (1.4
(cid:2))
37.5 ± 0.0 (1.1

) or min (x (cid:2)

Average accuracies ± standard errors of 10-fold cv (10 runs). The pk parameter tmax as well as color
quantization and pixel neighborhood was learned on a training subset of the full dataset. Average runtimes in
sec (x (cid:2)(cid:2)
) given in parentheses refer to the learned parameter settings. All kernels are unnormalized
and for glcm-quant and labels the same color quantization as for pk was applied. labels corresponds to
pk with tmax = 0. brodatz-o-r is using the original images and their 90
rotated versions and brodatz-o-
r-s-rs additionally includes their scaled, and scaled and rotated versions
Bold indicates that the method performs signiﬁcantly better than the second best method under a paired t-test
( p < 0.05)

◦

much larger (32 textures). We also learned the quantization values (col ∈ {3, 5, 8, 10, 15}) and
neighborhoods (B ∈ {N1,4, N1,8, N2,16}, cf. Eq. (16)). For brodatz the best performance
on the training data was achieved with 3 colors and a 8-neighborhood, whereas for plants 5
colors and the 4-neighborhood was learned. We compare pk to the simple baseline labels
using label counts only and to a powerful second-order statistical feature based on the gray-
level co-occurrence matrix (Haralick et al. 1973) comparing intensities (glcm-gray) resp.
quantized labels (glcm-quant) of neighboring pixels. The experimental results for grid
graphs are shown in Table 6. While not outperforming sophisticated and complex state-of-
the-art computer vision approaches to texture classiﬁcation, we ﬁnd that it is feasible to
compute pks on huge image datasets achieving respectable performance out of the box. This
is—compared to the immense tuning of features and methods commonly done in computer
vision—a great success. On plants pk achieves an average accuracy of 82.5 %, where the
best reported result so far is 83.7 %, which was only achieved after tailoring a complex
feature ensemble (Neumann et al. 2014). In conclusion, propagation kernels are an extremely
promising approach in the intersection of machine learning, graph mining, and computer
vision.

Summarizing all experimental results, the capabilities claimed in Table 2 are supported.
Propagation kernels have proven extremely ﬂexible and efﬁcient and thus question (Q3) can
ultimately be answered afﬁrmatively.

Random walk-based models provide a principled way of spreading information and even
handling missing and uncertain information within graphs. Known labels are, for example,
propagated through the graph in order to label all unlabeled nodes. In this paper, we showed
how to use random walks to discover structural similarities shared between graphs for the
construction of a graph kernel, namely the propagation kernel. Intuitively, propagation kernels
count common sub-distributions induced in each iteration of running inference in two graphs
leading to the insight that graph kernels are much closer to graph-based learning than assumed
before.

8 Conclusion

123

Mach Learn (2016) 102:209–245

241

As our experimental results demonstrate, propagation kernels are competitive in terms of
accuracy with state-of-the-art kernels on several classiﬁcation benchmark datasets of labeled
and attributed graphs. In terms of runtime, propagation kernels outperform all recently devel-
oped efﬁcient and scalable graph kernels. Moreover, being tied to the propagation scheme,
propagation kernels can be easily adapted to novel applications not having been tractable for
graph kernels before.

Propagation kernels provide several interesting avenues for future work. For handling con-
tinuous attributes using mixture of different covariance matrices could improve performance.
Also, the effect that adding noise to the label encoding actually improves the predictive per-
formance should be investigated in more details. While we have used classiﬁcation to guide
the development of propagation kernels, the results are directly applicable to regression,
clustering, and ranking, among other tasks. Employing message-based probabilistic infer-
ence schemes such as (loopy) belief propagation directly paves the way to deriving novel
graph kernels from graphical models. Exploiting that graph kernels and graph-based learning
(learning on the node level) are closely related, hence, a natural extension to this work is the
derivation of a unifying propagation-based framework for structure representation indepen-
dent of the learning task being on the graph or node level.

Acknowledgments Part of this work grew out of the dfg Collaborative Research Center sfb 876 “Providing
Information by Resource-Constrained Analysis” and discussions with Petra Mutzel and Christian Sohler. We
thank Nino Shervashidze, Nils Kriege, Aasa Feragen, and Karsten Borgwardt for our discussions and for
sharing their kernel implementations and datasets. Part of this work was supported by the German Science
Foundation (dfg) under the reference number ‘GA 1615/1-1’.

Appendix 1: Runtimes for Partially Labeled Graphs

See Fig. 12.

lp + wl
wl
pk

)
s
(

e
m

i
t

l
a
t
o
t

2.5

1.5

3

2

1

0

0.5

2

4

6

8

10

iteration

Fig. 12 Runtime for partially labeled msrc21. Avg. time in seconds over 10 instances of the msrc21 dataset
with 50 % labeled nodes for kernel iterations from 0 to 10. We compare pk to wl with unlabeled nodes treated
as additional label and with hard labels derived from converged label propagation (lp + wl)

123

Mach Learn (2016) 102:209–245

Table 7 Attributed graphs: runtimes

Method

Dataset

synthetic

enzymes

proteins

pro_full

bzr

cox2

dhfr

242

labels

pk

wl

sp

gh

csm

attr

pk

sp

gh

csm

(cid:2)(cid:2)

0.1(cid:2)(cid:2)
(cid:2)(cid:2)
3.4
5.1
19.8
54.8h

(cid:2)

0.3(cid:2)(cid:2)
11.5h
(cid:2)
16.2

OUT OF TIME

pk

p2k

labels & attr
0.3(cid:2)(cid:2)
17.2
13.7h
(cid:2)
16.9
56.8h

csm

gh

sp

(cid:2)(cid:2)

attr

a

(cid:2)

1.4

3.6(cid:2)(cid:2)
(cid:2)(cid:2)
4.1
(cid:2)(cid:2)
1.3
17.8
5.2h

(cid:2)

1.4(cid:2)
55.4
13.2
(cid:2)
7.6

(cid:2)

(cid:2)

1.2(cid:2)
(cid:2)
6.7
1.0h
(cid:2)
9.8
21.8

(cid:2)

(cid:2)

1.3

17.8(cid:2)(cid:2)
(cid:2)(cid:2)
25.9
38.7
2.2h
–

(cid:2)(cid:2)

23.6(cid:2)(cid:2)
5.9h
1.9h
–

20.0(cid:2)(cid:2)
(cid:2)
31.4
6.8h
1.2h
–

(cid:2)

6.4

18.7(cid:2)(cid:2)
(cid:2)(cid:2)
22.2
40.3
1.4h
–

(cid:2)(cid:2)

10.7(cid:2)
6.0h
72.4
–

(cid:2)

(cid:2)

9.0(cid:2)
30.6
7.0h
1.2h
–

(cid:2)

4.6

(cid:2)(cid:2)
0.9
0.8(cid:2)(cid:2)
(cid:2)(cid:2)
1.0
7.4
46.4

(cid:2)

(cid:2)

(cid:2)

2.6(cid:2)(cid:2)
29.5
(cid:2)
4.5
16.2h

3.0(cid:2)(cid:2)
(cid:2)
1.3
20.5
(cid:2)
7.8
48.8

(cid:2)

(cid:2)

(cid:2)(cid:2)

(cid:2)(cid:2)

(cid:2)(cid:2)

1.1
1.0
1.2
10.8
1.6h

(cid:2)

(cid:2)

2.8(cid:2)(cid:2)
25.9
(cid:2)
6.6
31.5h

3.5(cid:2)(cid:2)
(cid:2)
1.8
32.9
(cid:2)
11.7
1.6h

(cid:2)

(cid:2)(cid:2)

3.4
(cid:2)(cid:2)
4.2
2.3(cid:2)(cid:2)
29.5
3.7h

(cid:2)

14.8(cid:2)(cid:2)
1.3h
15.8
97.5h

(cid:2)

15.1(cid:2)(cid:2)
(cid:2)
5.9
1.6h
31.2
3.7h

(cid:2)

(cid:2)(cid:2)

38.3

(cid:2)

1.1

(cid:2)

3.2

Kernel computation times (cputime) are given in sec (x (cid:2)(cid:2)
), or hours (xh). For all pks, tmax = 10; for
wl, hmax = 10; and for csm, k = 7. All computations are performed on machines with 3.4 GHz Intel core
i7 processors. Note that csm is implemented in Java, so comparing computation times is only possible to a
limited extent. out of time means that the computation did not ﬁnish within 72h
Bold indicates the method fastest method

), min (x (cid:2)

Appendix 2: Detailed Results on Attributed Graphs

See Tables 7 and 8.

Table 8 Attributed graphs: accuracies

Method Dataset

synthetic

enzymes

proteins

pro-full

bzr

cox2

dhfr

50.0 ± 0.0
50.0 ± 0.0
50.0 ± 0.0
50.0 ± 0.0
50.0 ± 0.0

∗

∗

∗

∗

∗

46.0 ± 0.3
∗
46.3 ± 0.2
40.4 ± 0.3
∗
34.8 ± 0.4
60.8 ± 0.2

∗

∗

∗

∗

∗

87.0 ± 0.4
75.6 ± 0.1
75.6 ± 0.1
88.2 ± 0.2
75.5 ± 0.1
75.5 ± 0.1
85.6 ± 0.3
76.2 ± 0.1
76.2 ± 0.1
72.9 ± 0.2
87.0 ± 0.3
72.9 ± 0.2
∗
OUT OF MEMORY OUT OF MEMORY 87.5 ± 0.2

∗

∗

∗

∗

∗

81.0 ± 0.2
83.2 ± 0.2
81.0 ± 0.4
∗
81.4 ± 0.3
80.7 ± 0.3

∗

83.5 ± 0.2
84.1 ± 0.2
82.0 ± 0.3
79.5 ± 0.4
82.6 ± 0.2

∗

∗

∗

labels

pk

wl

sp

gh

csm

123

Mach Learn (2016) 102:209–245

243

Table 8 continued

Method Dataset

csm
labels & attr
pk

attr

pk

sp

gh

p2k

sp

gh

csm

attr

a

synthetic

enzymes

proteins

pro-full

bzr

cox2

dhfr

∗

∗

99.2 ± 0.1
83.9 ± 0.2
85.3 ± 0.4
–

56.4 ± 0.6
∗
63.9 ± 0.3
68.8 ± 0.2
68.9 ± 0.2

∗

∗

86.8 ± 0.2
74.1 ± 0.2
72.7 ± 0.2
∗
85.0 ± 0.3
59.9 ± 0.0
74.3 ± 0.1
∗
72.6 ± 0.1
84.1 ± 0.3
61.2 ± 0.0
∗
OUT OF MEMORY OUT OF MEMORY 85.1 ± 0.3

∗

∗

∗

∗

78.2 ± 0.4
78.2 ± 0.0
79.5 ± 0.2
77.6 ± 0.3

∗

84.3 ± 0.1
∗
78.9 ± 0.3
79.0 ± 0.2
79.5 ± 0.2

∗

∗

99.2 ± 0.1
98.7 ± 0.1
99.0 ± 0.1
50.0 ± 0.0
99.0 ± 0.1

65.9 ± 0.4
68.1 ± 0.5
64.3 ± 0.3
71.2 ± 0.2
72.8 ± 0.4

∗

∗

∗

88.1 ± 0.2
75.7 ± 0.4
76.3 ± 0.2
88.8 ± 0.2
76.9 ± 0.2
75.9 ± 0.2
∗
85.2 ± 0.2
59.9 ± 0.0
73.2 ± 0.2
73.0 ± 0.1
84.8 ± 0.4
60.9 ± 0.0
OUT OF MEMORY OUT OF MEMORY 87.0 ± 0.2

∗

∗

∗

79.4 ± 0.6
80.9 ± 0.4
78.5 ± 0.1
79.5 ± 0.2
79.2 ± 0.4

∗

∗

∗

84.1 ± 0.3
∗
83.5 ± 0.3
79.7 ± 0.2
80.0 ± 0.2
∗
80.1 ± 0.3

∗

∗

∗

∗

∗

∗

∗

∗

∗

∗

∗

∗

∗

47.8 ± 0.9

∗

68.5 ± 0.2

72.8 ± 0.2

∗

59.8 ± 0.0

∗

83.9 ± 0.3

78.2 ± 0.0

74.8 ± 0.2

∗

Average accuracies ± standard error of 10-fold cross validation (10 runs). All pks were also recomputed for
each run as there is randomization in the lsh computation. The kernel parameters tmax for all pks and hmax for
wl were learned on the training splits (tmax, hmax ∈ {0, 1, . . . 10}). For all pks we applied standardization to
the attributes and one hash per attribute dimension is computed. Whenever the normalized version of a kernel
performed better than the unnormalized version we report these results and mark the method with ∗. csm is
implemented in Java and computations were performed on a machine with 32gb of memory. out of memory
indicates a Java outOfMemeoryError. Bold indicates that the method performs signiﬁcantly better than
the second best method under a paired t-test ( p < 0.05). The svm cost parameter is learned on the training
−1}
splits. We choose c ∈ {10
for unnormalized kernels

−5, . . . , 105, 107} for normalized kernels and c ∈ {10

−3, 10

−5, 10

−7, 10

−7, 10

References

arxiv:1310.7114.

Bauckhage, C., & Kersting, K. (2013). Efﬁcient information theoretic clustering on discrete lattices (2013).

Borgwardt, K., & Kriegel, H. P. (2005). Shortest-path kernels on graphs. In Proceedings of international

conference on data mining (ICDM-05), pp. 74–81.

Datar, M., & Indyk, P. (2004). Locality-sensitive hashing scheme based on p-stable distributions. In Proceed-

ings of the 20th annual symposium on computational geometry (SCG-2004), pp. 253–262.

Debnath, A., de Compadre, R. L., Debnath, G., Schusterman, A., & Hansch, C. (1991). Structure-activity
relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular
orbital energies and hydrophobicity. Journal of Medicinal Chemistry, 34(2), 786–797.

Desrosiers, C., & Karypis, G. (2009). Within-network classiﬁcation using local structure similarity. In Pro-
ceedings of the European conference on machine learning and knowledge discovery in databases
(ECML/PKDD-09), pp. 260–275.

Dobson, P. D., & Doig, A. J. (2003). Distinguishing enzyme structures from non-enzymes without alignments.

Journal of Molecular Biology, 330(4), 771–783.

Feragen, A., Kasenburg, N., Petersen, J., de Bruijne, M., & Borgwardt, K. M. (2013). Scalable kernels for
graphs with continuous attributes. Advances in Neural Information Processing Systems, 26(NIPS–13),
216–224.

Gallagher, B., Tong, H., Eliassi-Rad, T., & Faloutsos, C. (2008). Using ghost edges for classiﬁcation in sparsely
labeled networks. In Proceedings of the 14th ACM SIGKDD international conference on knowledge
discovery and data mining (KDD-08), pp. 256–264.

Gärtner, T., Flach, P. A., & Wrobel, S. (2003). On graph kernels: Hardness results and efﬁcient alternatives.
In Proceedings of computational learning theory and kernel machines (COLT-03), pp. 129–143.

123

244

Mach Learn (2016) 102:209–245

Gersho, A., & Gray, R. (1991). Vector quantization and signal compression. Norwell, MA: Kluwer Academic

Publishers.

Haralick, R. M., Shanmugam, K., & Dinstein, I. H. (1973). Textural features for image classiﬁcation. IEEE

Transactions on Systems, Man and Cybernetics, SMC–3(6), 610–621.

Harchaoui, Z., & Bach, F. (2007). Image classiﬁcation with segmentation graph kernels. In CVPR. IEEE

Haussler, D. (1999). Convolution kernels on discrete structures. Tech. rep., Department of Computer Science,

Computer Society.

University of California, Santa Cruz.

Hido, S., & Kashima, H. (2009). A linear-time graph kernel. In Proceedings of the 9th IEEE international

conference on data mining (ICDM-09), pp. 179–188.

Horváth, T., Gärtner, T., & Wrobel, S. (2004). Cyclic pattern kernels for predictive graph mining. In Proceedings

of knowledge discovery in databases (KDD-04), pp. 158–167.

Hwang, T., & Kuang, R. (2010). A heterogeneous label propagation algorithm for disease gene discovery. In

Proceedings of the SIAM international conference on data mining (SDM-10), pp. 583–594.

Jaakkola, T., & Haussler, D. (1998). Exploiting generative models in discriminative classiﬁers. Advances in

Neural Information Processing Systems, 11(NIPS–98), 487–493.
Jähne, B. (2005). Digital Image Processing (6th ed.). Berlin: Springer.
Jebara, T., Kondor, R., & Howard, A. (2004). Probability product kernels. Journal of Machine Learning

Research, 5, 819–844.

Kashima, H., Tsuda, K., & Inokuchi, A. (2003). Marginalized kernels between labeled graphs. In Proceedings

of the 20th international conference on machine learning (ICML-03), pp. 321–328.

Kersting, K., Mladenov, M., Garnett, R., & Grohe, M. (2014). Power iterated color reﬁnement. In Proceedings

of the 28th AAAI conference on artiﬁcial intelligence (AAAI-14), pp. 1904–1910.

Kondor, R., & Jebara, T. (2003). A kernel between sets of vectors. In Proceedings of the twentieth international

conference on machine learning (ICML-03), pp. 361–368.

Kondor, R. I., & Lafferty, J. D. (2002). Diffusion kernels on graphs and other discrete input spaces. In Pro-
ceedings of the nineteenth international conference on machine learning (ICML-02), pp. 315–322.
Kriege, N., & Mutzel, P. (2012). Subgraph matching kernels for attributed graphs. In Proceedings of the 29th

international conference on machine learning (ICML-12).

Lafferty, J., & Lebanon, G. (2002). Information diffusion kernels. Advances in Neural Information Processing

Lin, F., & Cohen, W. W. (2010). Power iteration clustering. In Proceedings of the 27th international conference

Systems, 22(NIPS–02), 375–382.

on machine learning (ICML-10), pp. 655–662.

Lovász, L. (1996). Random walks on graphs: A survey. In D. Miklós, V. T. Sós, & T. Sz˝onyi (Eds.), Combi-
natorics, Paul Erd˝os is Eighty (Vol. 2, pp. 353–398). Budapest: János Bolyai Mathematical Society.
Mahé, P., & Vert, J. P. (2009). Graph kernels based on tree patterns for molecules. Machine Learning, 75(1),

3–35.

Moreno, P., Ho, P., & Vasconcelos, N. (2003). A Kullback–Leibler divergence based kernel for SVM classiﬁ-
cation in multimedia applications. Advances in Neural Information Processing Systems, it 23(NIPS-03),
1385–1392.

Neumann, M., Garnett, R., & Kersting, K. (2013). Coinciding walk kernels: Parallel absorbing random walks
for learning with graphs and few labels. In Asian conference on machine learning (ACML-13), pp.
357–372.

Neumann, M., Hallau, L., Klatt, B., Kersting, K., & Bauckhage, C. (2014). Erosion band features for cell
phone image based plant disease classiﬁcation. In Proceedings of the 22nd international conference on
pattern recognition (ICPR-14), pp. 3315–3320.

Neumann, M., Moreno, P., Antanas, L., Garnett, R., & Kersting, K. (2013). Graph kernels for object category
prediction in task-dependent robot grasping. In Eleventh workshop on mining and learning with graphs
(MLG-13), Chicago, Illinois.

Neumann, M., Patricia, N., Garnett, R., & Kersting, K. (2012). Efﬁcient graph kernels by randomization. In
European conference on machine learning and knowledge discovery in databases (ECML/PKDD-12),
pp. 378–393.

Ojala, T., Pietikäinen, M., & Mäenpää, T. (2002). Multiresolution gray-scale and rotation invariant texture
classiﬁcation with local binary patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence,
24(7), 971–987.

Ramon, J., & Gärtner, T. (2003). Expressivity versus efﬁciency of graph kernels. In Proceedings of the 1st

international workshop on mining graphs, trees and sequences, pp. 65–74.

Schomburg, I., Chang, A., Ebeling, C., Gremse, M., Heldt, C., Huhn, G., et al. (2004). Brenda, the enzyme
database: Updates and major new developments. Nucleic Acids Research, 32(Database–Issue), 431–433.

123

Mach Learn (2016) 102:209–245

245

Shervashidze, N., Schweitzer, P., van Leeuwen, E., Mehlhorn, K., & Borgwardt, K. (2011). Weisfeiler–Lehman

graph kernels. Journal of Machine Learning Research, 12, 2539–2561.

Shervashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K., & Borgwardt, K. (2009). Efﬁcient graphlet kernels

for large graph comparison. Journal of Machine Learning Research—Proceedings Track, 5, 488–495.

Shi, Q., Petterson, J., Dror, G., Langford, J., Smola, A. J., & Vishwanathan, S. V. N. (2009). Hash kernels for

structured data. Journal of Machine Learning Research, 10, 2615–2637.

Szummer, M., & Jaakkola, T. (2001). Partially labeled classiﬁcation with Markov random walks. Advances in

Neural Information Processing Systems, 15(NIPS–01), 945–952.

Valkealahti, K., & Oja, E. (1998). Reduced multidimensional co-occurrence histograms in texture classiﬁca-

tion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1), 90–94.

Vishwanathan, S., Schraudolph, N., Kondor, R., & Borgwardt, K. (2010). Graph kernels. Journal of Machine

Learning Research, 11, 1201–1242.

Wale, N., & Karypis, G. (2006). Comparison of descriptor spaces for chemical compound retrieval and classi-
ﬁcation. In Proceedings of the international conference on data mining (ICDM-06) (pp. 678–689). Silver
Spring, MD: IEEE Computer Society.

Winn, J. M., Criminisi, A., & Minka, T. P. (2005). Object categorization by learned universal visual dictionary.

In 10th IEEE international conference on computer vision (ICCV-05), pp. 1800–1807.

Wu, X. M., Li, Z., So, A. M. C., Wright, J., & Chang, S. F. (2012). Learning with partially absorbing random

walks. Advances in Neural Information Processing Systems, 26(NIPS–12), 3086–3094.

Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Schölkopf, B. (2003). Learning with local and global

consistency. Advances in Neural Information Processing Systems, 17(NIPS–03), 321–328.

Zhu, X., Ghahramani, Z., & Lafferty, J. D. (2003). Semi-supervised learning using Gaussian ﬁelds and harmonic
functions. In Proceedings of the twentieth international conference on machine learning (ICML-03), pp.
912–919.

123

