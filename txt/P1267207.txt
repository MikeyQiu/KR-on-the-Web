Dynamic Computational Time for Visual Attention

Zhichao Li,Yi Yang,Xiao Liu,Feng Zhou,Shilei Wen,Wei Xu
Baidu Research
{lizhichao01,yangyi05,liuxiao12,zhoufeng09,wenshilei,xuwei06}@baidu.com

7
1
0
2
 
p
e
S
 
7
 
 
]

V
C
.
s
c
[
 
 
3
v
2
3
3
0
1
.
3
0
7
1
:
v
i
X
r
a

Abstract

We propose a dynamic computational time model to ac-
celerate the average processing time for recurrent visual
attention (RAM). Rather than attention with a ﬁxed num-
ber of steps for each input image, the model learns to de-
cide when to stop on the ﬂy. To achieve this, we add an
additional continue/stop action per time step to RAM and
use reinforcement learning to learn both the optimal atten-
tion policy and stopping policy. The modiﬁcation is sim-
ple but could dramatically save the average computational
time while keeping the same recognition performance as
RAM. Experimental results on CUB-200-2011 and Stan-
ford Cars dataset demonstrate the dynamic computational
model can work effectively for ﬁne-grained image recogni-
tion. The source code of this paper can be obtained from
https://github.com/baidu-research/DT-RAM.

1. Introduction

Human have the remarkable ability of selective visual at-
tention [27, 19]. Cognitive science explains this as the “Bi-
ased Competition Theory” [3, 18] that human visual cortex
is enhanced by top-down guidance during feedback loops.
The feedback signals suppress non-relevant stimuli present
in the visual ﬁeld, helping human searching for ”goals”.
With visual attention, both human recognition and detec-
tion performances increase signiﬁcantly, especially on im-
ages with cluttered background [13].

Inspired by human attention, the Recurrent Visual Atten-
tion Model (RAM) is proposed for image recognition [47].
RAM is a deep recurrent neural architecture with iterative
attention selection mechanism, which mimics the human vi-
sual system to suppress non-relevant image regions and ex-
tract discriminative features in a complicated environment.
This signiﬁcantly improves the recognition accuracy [1], es-
pecially for ﬁne-grained object recognition [52, 45]. RAM
also allows the network to process a high resolution im-
age with only limited computational resources. By itera-
tively attending to different sub-regions (with a ﬁxed reso-
lution), RAM could efﬁciently process images with various

(a) Easy

(b) Moderate

(c) Hard

Figure 1. We show the recognition process of 3 tawny owl images
with increasing level of difﬁculty for recognition. When recogniz-
ing the same object in different images, human may spend differ-
ent length of time.

resolutions and aspect ratios in a constant computational
time [47, 1].

Besides attention, human also tend to dynamically allo-
cate different computational time when processing different
images [13, 16]. The length of the processing time often de-
pends on the task and the content of the input images (e.g.
background clutter, occlusion, object scale). For example,
during the recognition of a ﬁne-grained bird category, if the
bird appears in a large proportion with clean background
(Figure 1a), human can immediately recognize the image
without hesitation. However, when the bird is under cam-
ouﬂage (Figure 1b) or hiding in the scene with background
clutter and pose variation (Figure 1c), people may spend
much more time on locating the bird and extracting discrim-
inative parts to produce a conﬁdent prediction.

Inspired by this, we propose an extension to RAM named
as Dynamic Time Recurrent Attention Model (DT-RAM),
by adding an extra binary (continue/stop) action at every
time step. During each step, DT-RAM will not only up-
date the next attention, but produce a decision whether stop
the computation and output the classiﬁcation score. The
model is a simple extension to RAM, but can be viewed as
a ﬁrst step towards dynamic model during inference [26],
where the model structure can vary based on each input
instance. This could bring DT-RAM more ﬂexibility and

1

reduce redundant computation to further save computation,
especially when the input examples are “easy” to recognize.
Although DT-RAM is an end-to-end recurrent neural ar-
chitecture, we ﬁnd it is hard to directly train the model
parameters from scratch, particularly for challenging tasks
like ﬁne-grained recognition. When the total number of
steps increases, the delayed reward issue becomes more se-
vere and the variance of gradients becomes larger. This
makes policy gradient training algorithms such as REIN-
FORCE [57] harder to optimize. We address this problem
with curriculum learning [4]. During the training of RAM,
we gradually increase the training difﬁculty by gradually
increasing the total number of time steps. We then initial-
ize the parameters in DT-RAM with the pre-trained RAM
and ﬁne-tune it with REINFORCE. This strategy helps the
model to converge to a better local optimum than training
from scratch. We also ﬁnd intermediate supervision is cru-
cial to the performance, particularly when training longer
sequences.

We demonstrate the effectiveness of our model on pub-
lic benchmark datasets including MNIST [42] as well as
two ﬁne-grained datasets, CUB-200-2011 [60] and Stanford
Cars [39]. We also conduct an extensive study to understand
how dynamic time works in these datasets. Experimental
results suggest that DT-RAM can achieve state-of-the-art
performance on ﬁne-grained image recognition. Compared
to RAM, the model also uses less average computational
time, better ﬁtting devices with computational limitations.

2. Related Work

2.1. Visual Attention Models

Visual attention is a long-standing topic in computer vi-
sion [32, 31, 59]. With the recent success of deep neural
networks [41, 54, 58, 28], Mnih et al. [47] develop the Re-
current Visual Attention Model (RAM) for image recogni-
tion, where the attention is modeled with neural networks to
capture local regions in the image. Ba et al. [1] follow the
same framework and apply RAM to recognize multiple ob-
jects in images. Sermanet et al. [52] further extend RAM to
ﬁne-grained image recognition, since ﬁne-grained problems
usually require the comparison between local parts. Be-
sides ﬁne-grained recognition, attention models also work
for various machine learning problems including machine
translation [2], image captioning [64], image question an-
swering [63, 12, 21, 65] and video activity recognition [66].
Based on the differentiable property of attention models,
most of the existing work can be divided into two groups:
soft attention and hard attention [64]. The soft attention
models deﬁne attention as a set of continuous variables
representing the relative importance of spatial or temporal
cues. The model is differentiable hence can be trained with
backpropogation. The hard attention models deﬁne atten-

tion as actions and model the whole problem as a Partially
Observed Markov Decision Process (POMDP) [56]. Such
models are usually nondifferentiable to the reward function
hence use policy gradient such as REINFORCE [57] to op-
timize the model parameters. Our model belongs to the hard
attention since its stopping action is discrete.

2.2. Feedback Neural Networks

The visual attention models can be also viewed as a
special type of feedback neural networks [67, 55, 9, 61].
A feedback neural network is a special recurrent architec-
ture that uses previously computed high level features to
back reﬁne low level features. It uses both top-down and
bottom-up information to compute the intermediate lay-
ers. Besides attention models, feedback neural networks
also have other variants. For example, Carreira et al. [10]
perform human pose estimation with iterative error feed-
back. Newell et al. [49] build a stacked hourglass network
for human pose estimation. Hu and Ramanan [29] show
that network feedbacks can help better locating human face
landmarks. All these models demonstrate top-down infor-
mation could potentially improve the model discriminative
ability [67]. However, these models either ﬁx the number of
recurrent steps or use simple rules to decide early stopping.

2.3. Dynamic Computational Time

Graves [26] recently introduce adaptive computational
time in recurrent neural networks. The model augments the
network with a sigmoidal halting unit at each time step,
whose activation determines the probability whether the
computation should stop. Figurnov et al. [20] extend [26]
to spatially adaptive computational time for residual net-
works. Their approach is similar but deﬁne the halting units
over spatial positions. Neumann et al. [48] extend the simi-
lar idea to temporally dependent reasoning. They achieve a
small performance beneﬁt on top of a similar model without
an adaptive component. Jernite et al. [34] learn a scheduler
to determine what portion of the hidden state to compute
based on the current hidden and input vectors. All these
models can vary the computation time during inference, but
the stopping policy is based on the cumulative probability
of halting units, which can be viewed as a ﬁxed policy.

As far as we know, Odena et al. [51] is the ﬁrst attempt
that learns to change model behavior at test time with re-
inforcement learning. Their model adaptively constructs
computational graphs from sub-modules on a per-input ba-
sis. However, they only verify on small dataset such as
MNIST [42] and CIFAR-10 [40]. Ba et al. [1] augment
RAM with the ”end-of-sequence” symbol to deal with vari-
able number of objects in an image, which inspires our work
on DT-RAM. However, they still ﬁx the number of atten-
tions for each target. There is also a lack of diagnostic ex-
periments on understanding how ”end-of-sequence” symbol

affects the dynamics. In this work, we conduct extensive ex-
perimental comparisons on larger scale natural images from
ﬁne-grained recognition.

2.4. Fine-Grained Recognition

Fine-grained image recognition has been extensively
studied in recent years [7, 6, 15, 30, 37, 39, 35, 44, 50].
Based on the research focus, ﬁne-grained recognition ap-
proaches can be divided into representation learning, part
alignment models or emphasis on data. The ﬁrst group at-
tempts to build implicitly powerful feature representations
such as bilinear pooling or compact bilinear pooling [22, 36,
43], which turn to be very effective for ﬁne-grained prob-
lems. The second group attempts to localize discriminative
parts to effectively deal with large intra-class variation as
well as subtle inter-class variation [5, 8, 23, 30, 45]. The
third group studies the importance of the scale of training
data [38]. They achieve signiﬁcantly better performance on
multiple ﬁne-grained dataset by using an extra large set of
training images.

With the fast development of deep models such as Bilin-
ear CNN [43] and Spatial Transformer Networks [33], it is
unclear whether attention models are still effective for ﬁne-
grained recognition. In this paper, we show that the visual
attention model, if trained carefully, can still achieve com-
parable performance as state-of-the-art methods.

3. Model

Figure 2. An illustration of the architecture of recurrent attention
model. By iteratively attending to more discriminative area lt, the
model could output more conﬁdent predictions yt.

smaller loss more probable. The second term is the standard
gradient for neural nets with a ﬁxed structure.

During experiments, it is difﬁcult to directly compute the
gradient of the L over θ because it requires to evaluate expo-
nentially many possible structures during training. Hence to
train the model, we ﬁrst sample a set of structures, then ap-
proximate the gradient with Monte Carlo Simulation [57]:

∂L
∂θ

≈

1
M

M
(cid:88)

i=1

(cid:18) ∂ log P (Si|x, θ)
∂θ

LSi(x, θ) +

(cid:19)

∂LSi(x, θ)
∂θ

(2)

where M is the number of samples.

3.1. Learning with Dynamic Structure

3.2. Recurrent Attention Model (RAM)

The difference between a dynamic structure model and
a ﬁxed structure model is that during inference the model
structure S depends on both the input x and parameter θ.

Given an input x, the probability of choosing a compu-
tational structure S is P (S|x, θ). Given the model space of
S, this probability can be modeled with a neural network.
Suppose the loss during training is deﬁned as LS (x, θ). The
overall expected loss for an input x is

L = ES [LS (x, θ)] =

P (S|x, θ)LS (x, θ)

(1)

(cid:88)

S

The gradient of L with respect to parameter θ can be calcu-
lated as:

∂L
∂θ

(cid:88)

(cid:18) ∂P (S)
∂θ

LS + P (S)

(cid:19)

∂LS
∂θ

=

=

S
(cid:88)

(cid:18)

S

P (S)

∂ log P (S)
∂θ

LS + P (S)

(cid:19)

∂LS
∂θ

= ES

(cid:20) ∂ log P (S|x, θ)
∂θ

LS (x, θ) +

(cid:21)

∂LS (x, θ)
∂θ

The recurrent attention model is formulated as a Par-
tially Observed Markov Decision Process (POMDP). At
each time step, the model works as an agent that executes an
action based on the observation and receives a reward. The
agent actively controls how to act, and it may affect the state
of the environment. In RAM, the action corresponds to the
localization of the attention region. The observation is a lo-
cal (partially observed) region cropped from the image. The
reward measures the quality of the prediction using all the
cropped regions and can be delayed. The target of learning
is to ﬁnd the optimal decision policy to generate attentions
from observations that maximizes the expected cumulative
reward across all time steps.

More formally, RAM deﬁnes the input image as x and
the total number of attentions as T . At each time step
t ∈ {1, . . . , T }, the model crops a local region φ(x, lt−1)
around location lt−1 which is computed from the previous
time step. It then updates the internal state ht with a recur-
rent neural network

ht = fh(ht−1, φ(x, lt−1), θh)

(3)

The ﬁrst term in the above expectation is the same as REIN-
FORCE algorithm [57], it makes the learning leading to a

which is parameterized by θh. The model then computes
two branches. One is the localization network fl(ht, θl)

Figure 3. An illustration of the architecture of dynamic time recur-
rent attention model. An extra binary stopping action at is added
to each time step. at = 0 represents ”continue” (green solid circle)
and at = 1 represents ”stop” (red solid circle).

which models the attention policy, parameterized by θl. The
other is the classiﬁcation network fc(ht, θc) which com-
putes the classiﬁcation score, parameterized by θc. During
inference, it samples the attention location based on the pol-
icy π(lt|fl(ht, θl)). Figure 2 illustrates the inference proce-
dure.

3.3. Dynamic Computational Time for Recurrent

Attention (DT-RAM)

To introduce dynamic structure to RAM, we simply aug-
ment it with an additional set of actions {at} that decides
when it will stop taking further attention and output results.
at ∈ {0, 1} is a binary variable with 0 representing “con-
tinue” and 1 indicating “stop”. Its sampling policy is mod-
eled via a stopping network fa(ht, θa). During inference,
we sample both the attention lt and stopping at with each
policy independently.

lt ∼ π(lt|fl(ht, θl)), at ∼ π(at|fa(ht, θa))

(4)

Figure 3 shows how the model works. Compared to Fig-
ure 2, the change is mainly an addition of at onto each time
step.

Figure 4 illustrates how DT-RAM adapts its model struc-
ture and computational time to different input images for
image recognition. When the input image is “easy” to rec-
ognize (Figure 4 left), we expect DT-RAM stop at the ﬁrst
few steps. When the input image is “hard” (Figure 4 right),
we expect the model learn to continue searching for infor-
mative regions.

3.4. Training

Figure 4. An illustration of how DT-RAM adapts its model struc-
ture and computational time to different input images.

by computing the following gradient:

∂L
∂θ

≈

(cid:88)

(cid:88)

(cid:18)

n

S

−

∂ log P (S|xn, θ)
∂θ

Rn +

∂LS (xn, yn, θ)
∂θ

(cid:19)

(5)
where θ = {θf , θl, θa, θc} are the parameters of the recur-
rent network, the attention network, the stopping network
and the classiﬁcation network respectively.

Compared to Equation 2, Equation 5 is an approximation
where we use a negative of reward function R to replace the
loss of a given structure LS in the ﬁrst term. This training
loss is similar to [47, 1]. Although the loss in Equation 2
can be optimized directly, using R can reduce the variance
in the estimator [1]. In addition,

P (S|xn, θ) =

π(lt|fl(ht, θl))π(at|fa(ht, θa))

(6)

T (n)
(cid:89)

t=1

is the sampling policy for structure S.

Rn =

γtrnt

T (n)
(cid:88)

t=1

(7)

is the cumulative discounted reward over T (n) time steps
for the n-th training example. The discount factor γ
controls the trade-off between making correct classiﬁca-
tion and taking more attentions. rnt is the reward at t-
th step. During experiments, we use a delayed reward.
We set rnt = 0 if t (cid:54)= T (n) and rnT = 1 only if
y = arg maxy P (y|fc(hT , θc)).
Intermediate Supervision:

the original
RAM [47], DT-RAM has intermediate supervision for
the classiﬁcation network at every time step, since its
underlying dynamic structure could require the model to
output classiﬁcation scores at any time step. The loss of

Unlike

LS (xn, yn, θ) =

Lt(xn, yn, θh, θc)

(8)

T (n)
(cid:88)

t=1

Given a set of training images with ground truth labels
(xn, yn)n=1···N , we jointly optimize the model parameters

is the average cross-entropy classiﬁcation loss over N train-
ing samples and T (n) time steps. Note that T depends on

Dataset

#Classes #Train #Test BBox

MNIST [42]
CUB-200-2011 [60]
Stanford Cars [39]

10
200
196

60000 10000
5794
5994
8041
8144

-
√
√

Table 1. Statistics of the three dataset. CUB-200-2011 and Stan-
ford Cars are both benchmark datasets in ﬁne-grained recognition.

n, indicating that each instance may have different stopping
times. During experiments, we ﬁnd intermediate supervi-
sion is also effective for the baseline RAM.

Curriculum Learning: During experiments, we adopt a
gradual training approach for the sake of accuracy. First, we
start with a base convolutional network (e.g. Residual Net-
works [28]) pre-trained on ImageNet [17]. We then ﬁne-
tune the base network on the ﬁne-grained dataset. This
gives us a very high baseline. Second, we train the RAM
model by gradually increase the total number of time steps.
Finally, we initialize DT-RAM with the trained RAM and
further ﬁne-tune the whole network with REINFORCE al-
gorithm.

4. Experiments

4.1. Dataset

We conduct experiments on three popular benchmark
datasets: MNIST [42], CUB-200-2011 [60] and Stanford
Cars [39]. Table 1 summarizes the details of each dataset.
MNIST contains 70,000 images with 10 digital numbers.
This is the dataset where the original visual attention model
tests its performance. However, images in MNIST dataset
are often too simple to generate conclusions to natural im-
ages. Therefore, we also compare on two challenging ﬁne-
grained recognition dataset. CUB-200-2011 [60] consists of
11,778 images with 200 bird categories. Stanford Cars [39]
includes 16,185 images of 196 car classes. Both datasets
contain a bounding box annotation for each image. CUB-
200-2011 also contains part annotation, which we do not
use in our algorithm. Most of the images in these two
datasets have cluttered background, hence visual attention
could be effective for them. All models are trained and
tested without ground truth bounding box annotations.

4.2. Implementation Details

MNIST: We use the original digital images with 28×28
pixel resolution. The digits are generally centered in the
image. We ﬁrst train multiple RAM models with up to 7
steps. At each time step, we crop a 8×8 patch from the
image based on the sampled attention location. The 8×8
patch only captures a part of a digit, hence the model usually
requires multiple steps to produce an accurate prediction.

The attention network, classiﬁcation network and stop-
ping network all output actions at every time step. The out-

put dimensions of the three networks are 2, 10 and 1 re-
spectively. All three networks are linear layers on top of
the recurrent network. The classiﬁcation network and stop-
ping network contain softmax layers to compute the discrete
probability. The attention network deﬁnes a Gaussian pol-
icy with a ﬁxed variance, representing the continuous dis-
tribution of the two location variables. The recurrent state
vector has 256 dimensions. All methods are trained using
stochastic gradient descent (SGD) with batch size of 20 and
momentum of 0.9. The reward at the last time step is 1 if
the agent classiﬁes correctly and 0 otherwise. The rewards
for all other time steps are 0. One can refer to [47] for more
training details.

CUB-200-2011 and Stanford Cars: We use the same
setting for both dataset. All images are ﬁrst normalized by
resizing to 512×512. We then crop a 224×224 image patch
at every time step except for the ﬁrst step, which is a key dif-
ference from MNIST. At the ﬁrst step, we use a 448×448
crop. This guarantees the 1-step RAM and 1-step DT-RAM
to have the same performance as the baseline convolutional
network. We use Residual Networks [28] pre-trained on Im-
ageNet [17] as the baseline network. We use the “pool-5”
feature as input to the recurrent hidden layer. The recurrent
layer is a fully connected layer with ReLU activations. The
attention network, classiﬁcation network and stopping net-
work are all linear layers on top of the recurrent network.
The dimensionality of the hidden layer is set to 512.

All models are trained using SGD with momentum of
0.9 for 90 epochs. The learning rate is set to 0.001 at the
beginning and multiplied by 0.1 every 30 epochs. The batch
size is 28 which is the maximum we can use for 512×512
resolution.
(For diagnostic experiments with smaller im-
ages, we use batch size of 96.) The reward strategy is the
same as MNIST. During testing, the actions are chosen to
be the maximal probability output from each network. Note
that although bounding boxs or part-level annotations are
available with these datasets, we do not utilize any of them
throughout the experiments.

Computational Time: Our implementation is based on
Torch [14]. The computational time heavily depends on
the resolution of the input image and the baseline network
structure. We run all our experiments on a single Tesla K-
40 GPU. The average running time for a ResNet-50 on a
512×512 resolution image is 42ms. A 3-step RAM is 77ms
since it runs ResNet-50 on 2 extra 224×224 images.

4.3. Comparison with State-of-the-Art

MNIST: We train two DT-RAM models with different
discount factors. We train DT-RAM-1 with a smaller dis-
count factor (0.98) and DT-RAM-2 with a larger discount
factor (0.99). The smaller discount factor will encourage
the model to stop early in order to obtain a large reward,
hence one can expect DT-RAM-1 stops with less number of

MNIST

# Steps Error(%)

CUB-200-2011

Accuracy(%) Acc w. Box(%)

FC, 2 layers (256 hiddens each)
Convolutional, 2 layers
RAM 2 steps
RAM 4 steps
RAM 5 steps
RAM 7 steps

DT-RAM-1 3.6 steps
DT-RAM-2 5.2 steps

-
-
2
4
5
7

3.6
5.2

1.69
1.21
3.79
1.54
1.34
1.07

1.46
1.12

Table 2. Comparison to related work on MNIST. All the RAM
results are from [47].

Zhang et al. [68]
Branson et al. [8]
Simon et al. [53]
Krause et al. [37]
Lin et al. [43]
Jaderberg et al. [33]
Kong et al. [36]
Liu et al. [46]
Liu et al. [45]

ResNet-50 [28]
RAM 3 steps
DT-RAM 1.9 steps

73.9
75.7
81.0
82.0
84.1
84.1
84.2
84.3
85.4

84.5
86.0
86.0

76.4
85.4∗
-
82.8
85.1
-
-
84.7
85.5

-
-
-

Table 3. Comparison to related work on CUB-200-2011 dataset. ∗
Testing with both ground truth box and parts.

of the existing works. Adding further recurrent visual atten-
tion (RAM) reaches 86.0%, improving the baseline Resid-
ual Net by 1.5%, leading to a new state-of-the-art on CUB-
200-2011. DT-RAM further improves RAM, by achieving
the same state-of-the-art performance, with less number of
computational time on average (1.9 steps v.s. 3 steps).

Stanford Cars: We also compare extensively on Stan-
ford Cars dataset. Table 4 shows the results. Surprisingly, a
ﬁne-tuned 50-layer Residual Network again achieves 92.3%
accuracy on the testing set, surpassing most of the existing
work. This suggests that a single deep network (without any
further modiﬁcation or extra bounding box supervision) can
be the ﬁrst choice for ﬁne-grained recognition.

A 3-step RAM on top of the Residual Net further im-
proves to 93.1% accuracy, which is by far the new state-
of-the-art performance on Stanford Cars dataset. Compared
to [46], this is achieved without using bounding box an-
notation during testing. DT-RAM again achieves the same
accuracy as RAM but using 1.9 steps on average. We also
observe that the relative improvement of RAM to the base-
line model is no longer large (0.8%).

4.4. Ablation Study

We conduct a set of ablation studies to understand how
each component affects RAM and DT-RAM on ﬁne-grained
recognition. We focus on CUB-200-2011 dataset since its
images are real and challenging. However, due to the large
resolution of the images, we are not able to run many steps
with a very deep Residual Net. Therefore, instead of using
ResNet-50 with 512×512 image resolution which produces
state-of-the-art results, we use ResNet-34 with 256×256 as
the baseline model (79.9%). This allows us to train more
steps on the bird images to deeply understand the model.

Figure 5. The distribution of number of steps from two different
DT-RAM models on MNIST dataset. A model with longer average
steps tends to have a better accuracy.

steps than DT-RAM-2.

Table 2 summarizes the performance of different mod-
els on MNIST. Comparing to RAM with similar number of
steps, DT-RAM achieves a better error rate. For example,
DT-RAM-1 gets 1.46% recognition error with an average of
3.6 steps while RAM with 4 steps gets 1.54% error. Sim-
ilarly, DT-RAM-2 gets 1.12% error with an average of 5.2
steps while RAM with 5 steps has 1.34% error.

Figure 5 shows the distribution of number of steps for
the two DT-RAM models across all testing examples. We
ﬁnd a clear trade-off between efﬁciency and accuracy. DT-
RAM-1 which has less computational time, achieves higher
error than DT-RAM-2.

CUB-200-2011: We compare our model with all pre-
viously published methods. Table 3 summarizes the re-
sults. We observe that methods including Bilinear CNN [43,
36] (84.1% - 84.2%), Spatial Transformer Networks [33]
(84.1%) and Fully Convolutional Attention Networks [46]
(84.3%) all achieve similar performances. [45] further im-
prove the testing accuracy to 85.4% by utilizing attribute
labels annotated in the dataset.

Surprisingly, we ﬁnd that a carefully ﬁne-tuned Residual
Network with 50 layers already hits 84.5%, surpassing most

Input Resolution and Network Depth: Table 5 com-
pares the effect on different image resolutions with differ-

Stanford Cars

Accuracy(%) Acc w. Box(%)

Model

# Steps Accuracy(%)

ResNet-34
RAM 2 steps
RAM 3 steps
RAM 4 steps
RAM 5 steps
RAM 6 steps

1
2
3
4
5
6

79.9
80.7
81.1
81.5
81.8
81.8

81.8

DT-RAM (6 max steps)

3.6

Table 6. Comparison to RAM on CUB-200-2011. Note that the
1-step RAM is the same as the ResNet.

Chai et al. [11]
Gosselin et al. [25]
Girshick et al. [24]
Lin et al. [43]
Wang et al. [62]
Liu et al. [46]
Krause et al. [37]

ResNet-50 [28]
RAM 3 steps
DT-RAM 1.9 steps

78.0
82.7
88.4
91.3
-
91.5
92.6

92.3
93.1
93.1

-
87.9
-
-
92.5
93.1
92.8

-
-
-

Table 4. Comparison to related work on Stanford Cars dataset.

Resolution ResNet-34 RAM-34 ResNet-50 RAM-50

224×224
448×448

79.9
-

81.8
-

81.5
84.5

82.8
86.0

Table 5. The effect of input resolution and network depth on
ResNet and its RAM extension.

ent network depths. In general, a higher image resolution
signiﬁcantly helps ﬁne-grained recognition. For example,
given the same ResNet-50 model, 512×512 resolution with
448×448 crops gets 84.5% accuracy, outperforming 81.5%
from 256×256 with 224×224 crops. This is probably be-
cause higher resolution images contain more detailed in-
formation for ﬁne-grained recognition. A deeper Resid-
ual Network also signiﬁcantly improves performance. For
example, ResNet-50 obtains 81.5% accuracy compared to
ResNet-34 with only 79.9%. During experiments, we also
train a ResNet-101 on 224×224 crops and get 82.8% recog-
nition accuracy.

RAM v.s. DT-RAM: Table 6 shows how the number
of steps affects RAM on CUB-200-2011 dataset. Starting
from ResNet-34 which is also the 1 step RAM, the model
gradually increases its accuracy with more steps (79.9% →
81.8%). After 5 steps, RAM no longer improves. DT-RAM
also reaches the same accuracy. However, it only uses 3.5
steps on average than 6 steps, which is a promising trade-off
between computation and performance.

Figure 6 plots the distribution of number of steps from
the DT-RAM on the testing images. Surprisingly different
from MNIST, the model prefers to stop either at the begin-
ning of the time (1-2 steps), or in the end of the time (5-6
steps). There are very few images that the model choose to
stop at 3-4 steps.

Learned Policy v.s. Fixed Policy: One may suspect that
instead of learning the optimal stopping policy, whether we
can just use a ﬁxed stopping policy on RAM to determine
when to stop the recurrent iteration. For example, one can
simply learn a RAM model with intermediate supervision

Figure 6. The distribution of number of steps from a DT-RAM
model on CUB-200-2011 dataset. Unlike MNIST, the distribution
suggests the model prefer either stop at the beginning or in the end.

Threshold

# Steps Accuracy(%)

0
0.4
0.5
0.6
0.9
1.0

1
1.4
1.6
1.9
3.6
6

3.6

79.9
80.7
81.0
81.2
81.3
81.8

81.8

DT-RAM (6 max steps)

Table 7. Comparison to a ﬁxed stopping policy on CUB-200-2011.
The ﬁxed stopping policy runs on RAM (6 steps) such that the
recurrent attention stops if one of the class softmax probabilities is
above the threshold.

at every time step and use a threshold over the classiﬁ-
cation network fc(ht, θc) to determine when to stop. We
compare the results between the described ﬁxed policy and
DT-RAM. Table 7 shows the comparison. We ﬁnd that al-
though the ﬁxed stopping policy gives reasonably good re-
sults (i.e. 3.6 steps with 81.3% accuracy), DT-RAM still
works slightly better (i.e. 3.6 steps with 81.8% accuracy).

Curriculum Learning: Table 8 compares the results
on whether using curriculum learning to train RAM. If we
learn the model parameters completely from scratch with-
out curriculum, the performance start to decrease with more

(a) 1 step

(b) 2 steps

(c) 3 steps

(d) 4 steps

(e) 5 steps

(f) 6 steps

Figure 7. Qualitative results of DT-RAM on CUB-200-2011 testing set. We show images with different ending steps from 1 to 6. Each
bounding box indicates an attention region. Bounding box colors are displayed in order. The ﬁrst step uses the full image as input hence
there is no bounding box. From step 1 to step 6, we observe a gradual increase of background clutter and recognition difﬁculty, matching
our hypothesis for using dynamic computation time for different types of images.

(b) 2 steps
Figure 8. Qualitative results of DT-RAM on Stanford Car testing set. We only manage to train a 3-step model with 512×512 resolution.

(c) 3 steps

(a) 1 step

# Steps

1

2

3

4

5

6

# Steps

1

2

3

4

5

6

w.o C.L. 79.9 80.7 80.5 80.9 80.3 80.0
79.9 80.7 81.1 81.5 81.8 81.8
w. C.L.

w.o I.S. 79.9 78.8 76.1 74.8 74.9 74.7
79.9 80.7 81.1 81.5 81.8 81.8
w. I.S.

Table 8. The effect of Curriculum Learning on RAM.

Table 9. The effect of Intermediate Supervision on RAM.

time steps (79.9%→80.9%→80.0%). This is because sim-
ple policy gradient method becomes harder to train with
longer sequences. Curriculum learning makes training more
stable, since it guarantees the accuracy on adding more
steps will not hurt the performance. The testing perfor-
mance hence gradually increases over number of steps,
from 79.9% to 81.8%.

Intermediate Supervision: Table 9 compares the test-
ing results for using intermediate supervision. Note that
the original RAM model [47] only computes output at the
last time step. Although this works for small dataset like
MNIST. when the input images become more challenging
and time step increases, the RAM model learned without
intermediate supervision starts to get worse. On contrary,
adding an intermediate loss at each step makes RAM mod-
els with more steps steadily improve the ﬁnal performance.
Qualitative Results: We visualize the qualitative results

of DT-RAM on CUB-200-2011 and Stanford Cars testing
set in Figure 7 and Figure 8 respectively. From step 1 to
step 6, we observe a gradual increase of background clutter
and recognition difﬁculty, matching our hypothesis of using
dynamic computation time for different types of images.

5. Conclusion and Future Work

In this work we present a novel method for learning
to dynamically adjust computational time during inference
with reinforcement learning. We apply it on the recurrent
visual attention model and show its effectiveness for ﬁne-
grained recognition. We believe that such methods will be
important for developing dynamic reasoning in deep learn-
ing and computer vision. Future work on developing more
sophisticated dynamic models and apply it to more complex
tasks such as visual question answering will be conducted.

References

[1] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recog-
nition with visual attention. arXiv preprint arXiv:1412.7755,
2014. 1, 2, 4

[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473, 2014. 2

[3] D. M. Beck and S. Kastner. Top-down and bottom-up mech-
anisms in biasing competition in the human brain. Vision
research, 49(10):1154–1165, 2009. 1

[4] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-
riculum learning. In Proceedings of the 26th annual interna-
tional conference on machine learning, pages 41–48. ACM,
2009. 2

[5] T. Berg and P. Belhumeur. Poof: Part-based one-vs.-one fea-
tures for ﬁne-grained categorization, face veriﬁcation, and
attribute estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 955–
962, 2013. 3

[6] T. Berg, J. Liu, S. Woo Lee, M. L. Alexander, D. W. Jacobs,
and P. N. Belhumeur. Birdsnap: Large-scale ﬁne-grained vi-
sual categorization of birds. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2011–2018, 2014. 3

[7] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101–
mining discriminative components with random forests. In
European Conference on Computer Vision, pages 446–461.
Springer, 2014. 3

[8] S. Branson, G. Van Horn, S. Belongie, and P. Perona. Bird
species categorization using pose normalized deep convolu-
tional nets. arXiv preprint arXiv:1406.2952, 2014. 3, 6
[9] C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang,
L. Wang, C. Huang, W. Xu, et al. Look and think twice: Cap-
turing top-down visual attention with feedback convolutional
neural networks. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2956–2964, 2015. 2
[10] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Hu-
man pose estimation with iterative error feedback. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4733–4742, 2016. 2

[11] Y. Chai, V. Lempitsky, and A. Zisserman. Symbiotic seg-
mentation and part localization for ﬁne-grained categoriza-
tion. In Proceedings of the IEEE International Conference
on Computer Vision, pages 321–328, 2013. 7

[12] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Neva-
tia. Abc-cnn: An attention based convolutional neural
arXiv preprint
network for visual question answering.
arXiv:1511.05960, 2015. 2

[13] R. M. Cichy, D. Pantazis, and A. Oliva. Resolving human
object recognition in space and time. Nature neuroscience,
17(3):455–462, 2014. 1

[14] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In BigLearn,
NIPS Workshop, number EPFL-CONF-192376, 2011. 5
[15] Y. Cui, F. Zhou, Y. Lin, and S. Belongie. Fine-grained cate-
gorization and dataset bootstrapping using deep metric learn-
In Proceedings of the IEEE
ing with humans in the loop.

Conference on Computer Vision and Pattern Recognition,
pages 1153–1162, 2016. 3

[16] G. Deco and E. T. Rolls. A neurodynamical cortical model
of visual attention and invariant object recognition. Vision
research, 44(6):621–642, 2004. 1

[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. IEEE, 2009. 5

[18] R. Desimone. Visual attention mediated by biased competi-
tion in extrastriate visual cortex. Philosophical Transactions
of the Royal Society B: Biological Sciences, 353(1373):1245,
1998. 1

[19] R. Desimone and J. Duncan. Neural mechanisms of selective
visual attention. Annual review of neuroscience, 18(1):193–
222, 1995. 1

[20] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang,
Spatially adaptive
arXiv preprint

D. Vetrov, and R. Salakhutdinov.
computation time for residual networks.
arXiv:1612.02297, 2016. 2

[21] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016. 2

[22] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact
bilinear pooling. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 317–326,
2016. 3

[23] E. Gavves, B. Fernando, C. G. Snoek, A. W. Smeulders, and
T. Tuytelaars. Fine-grained categorization by alignments. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1713–1720, 2013. 3

[24] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
In Proceedings of the IEEE conference on
segmentation.
computer vision and pattern recognition, pages 580–587,
2014. 7

[25] P.-H. Gosselin, N. Murray, H. J´egou, and F. Perronnin. Re-
visiting the ﬁsher vector for ﬁne-grained classiﬁcation. Pat-
tern Recognition Letters, 49:92–98, 2014. 7

[26] A. Graves. Adaptive computation time for recurrent neural
networks. arXiv preprint arXiv:1603.08983, 2016. 1, 2
[27] M. Hayhoe and D. Ballard. Eye movements in natural be-
havior. Trends in cognitive sciences, 9(4):188–194, 2005. 1
[28] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
770–778, 2016. 2, 5, 6, 7

[29] P. Hu and D. Ramanan. Bottom-up and top-down reasoning
with hierarchical rectiﬁed gaussians. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5600–5609, 2016. 2

[30] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn
for ﬁne-grained visual categorization. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1173–1182, 2016. 3

[31] L. Itti and C. Koch. Computational modelling of visual at-
tention. Nature reviews neuroscience, 2(3):194–203, 2001.
2

[32] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
visual attention for rapid scene analysis. IEEE Transactions
on pattern analysis and machine intelligence, 20(11):1254–
1259, 1998. 2

[33] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in Neural Information

transformer networks.
Processing Systems, pages 2017–2025, 2015. 3, 6

[34] Y. Jernite, E. Grave, A. Joulin, and T. Mikolov. Variable
computation in recurrent neural networks. arXiv preprint
arXiv:1611.06188, 2016. 2

[35] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In Proc. CVPR Workshop on Fine-Grained Visual Catego-
rization (FGVC), volume 2, 2011. 3

[36] S. Kong and C. Fowlkes. Low-rank bilinear pooling for
ﬁne-grained classiﬁcation. arXiv preprint arXiv:1611.05109,
2016. 3, 6

[37] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5546–5555, 2015. 3, 6, 7

[38] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev,
T. Duerig, J. Philbin, and L. Fei-Fei. The unreasonable ef-
fectiveness of noisy data for ﬁne-grained recognition.
In
European Conference on Computer Vision, pages 301–320.
Springer, 2016. 3

[39] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In Proceedings
of the IEEE International Conference on Computer Vision
Workshops, pages 554–561, 2013. 2, 3, 5

[40] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 2

[41] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, pages
1097–1105, 2012. 2

[42] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 2, 5

[43] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In Proceedings of the
IEEE International Conference on Computer Vision, pages
1449–1457, 2015. 3, 6, 7

[44] J. Liu, A. Kanazawa, D. Jacobs, and P. Belhumeur. Dog
In European
breed classiﬁcation using part localization.
Conference on Computer Vision, pages 172–185. Springer,
2012. 3

[45] X. Liu, J. Wang, S. Wen, E. Ding, and Y. Lin. Localizing by
describing: Attribute-guided attention localization for ﬁne-
grained recognition. arXiv preprint arXiv:1605.06217, 2016.
1, 3, 6

[46] X. Liu, T. Xia, J. Wang, Y. Yang, F. Zhou, and Y. Lin. Fine-
grained recognition with automatic and efﬁcient part atten-
tion. arXiv preprint arXiv:1603.06765, 2016. 6, 7

[47] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of vi-
sual attention. In Advances in neural information processing
systems, pages 2204–2212, 2014. 1, 2, 4, 5, 6, 8
[48] M. Neumann, P. Stenetorp, and S. Riedel.
to reason with adaptive computation.
arXiv:1610.07647, 2016. 2

Learning
arXiv preprint

[49] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In European Conference
on Computer Vision, pages 483–499. Springer, 2016. 2
[50] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Computer Vision,
Graphics & Image Processing, 2008. ICVGIP’08. Sixth In-
dian Conference on, pages 722–729. IEEE, 2008. 3

[51] A. Odena, D. Lawson, and C. Olah. Changing model behav-
ior at test-time using reinforcement learning. arXiv preprint
arXiv:1702.07780, 2017. 2

[52] P. Sermanet, A. Frome, and E. Real. Attention for ﬁne-
arXiv preprint arXiv:1412.7054,

grained categorization.
2014. 1, 2

[53] M. Simon and E. Rodner. Neural activation constellations:
Unsupervised part model discovery with convolutional net-
works. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1143–1151, 2015. 6

[54] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 2

[55] M. F. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber.
Deep networks with internal selective attention through feed-
back connections. In Advances in Neural Information Pro-
cessing Systems, pages 3545–3553, 2014. 2

[56] R. S. Sutton and A. G. Barto. Reinforcement learning: An
introduction, volume 1. MIT press Cambridge, 1998. 2
[57] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour,
et al. Policy gradient methods for reinforcement learning
In NIPS, volume 99, pages
with function approximation.
1057–1063, 1999. 2, 3

[58] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1–9, 2015. 2

[59] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. Lai, N. Davis,
and F. Nuﬂo. Modeling visual attention via selective tuning.
Artiﬁcial intelligence, 78(1-2):507–545, 1995. 2

[60] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. 2011. 2, 5
[61] Q. Wang, J. Zhang, S. Song, and Z. Zhang. Attentional neu-
ral network: Feature selection using cognitive feedback. In
Advances in Neural Information Processing Systems, pages
2033–2041, 2014. 2

[62] Y. Wang, J. Choi, V. Morariu, and L. S. Davis. Mining dis-
criminative triplets of patches for ﬁne-grained classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1163–1172, 2016. 7

[63] H. Xu and K. Saenko. Ask, attend and answer: Exploring
question-guided spatial attention for visual question answer-
In European Conference on Computer Vision, pages
ing.
451–466. Springer, 2016. 2

[64] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhut-
dinov, R. S. Zemel, and Y. Bengio. Show, attend and tell:
Neural image caption generation with visual attention.
In
ICML, volume 14, pages 77–81, 2015. 2

[65] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked
In Pro-
attention networks for image question answering.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 21–29, 2016. 2

[66] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2678–2687, 2016. 2

[67] A. R. Zamir, T.-L. Wu, L. Sun, W. Shen, J. Malik,
arXiv preprint

Feedback networks.

and S. Savarese.
arXiv:1612.09508, 2016. 2

[68] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-
In Eu-
based r-cnns for ﬁne-grained category detection.
ropean conference on computer vision, pages 834–849.
Springer, 2014. 6

Dynamic Computational Time for Visual Attention

Zhichao Li,Yi Yang,Xiao Liu,Feng Zhou,Shilei Wen,Wei Xu
Baidu Research
{lizhichao01,yangyi05,liuxiao12,zhoufeng09,wenshilei,xuwei06}@baidu.com

7
1
0
2
 
p
e
S
 
7
 
 
]

V
C
.
s
c
[
 
 
3
v
2
3
3
0
1
.
3
0
7
1
:
v
i
X
r
a

Abstract

We propose a dynamic computational time model to ac-
celerate the average processing time for recurrent visual
attention (RAM). Rather than attention with a ﬁxed num-
ber of steps for each input image, the model learns to de-
cide when to stop on the ﬂy. To achieve this, we add an
additional continue/stop action per time step to RAM and
use reinforcement learning to learn both the optimal atten-
tion policy and stopping policy. The modiﬁcation is sim-
ple but could dramatically save the average computational
time while keeping the same recognition performance as
RAM. Experimental results on CUB-200-2011 and Stan-
ford Cars dataset demonstrate the dynamic computational
model can work effectively for ﬁne-grained image recogni-
tion. The source code of this paper can be obtained from
https://github.com/baidu-research/DT-RAM.

1. Introduction

Human have the remarkable ability of selective visual at-
tention [27, 19]. Cognitive science explains this as the “Bi-
ased Competition Theory” [3, 18] that human visual cortex
is enhanced by top-down guidance during feedback loops.
The feedback signals suppress non-relevant stimuli present
in the visual ﬁeld, helping human searching for ”goals”.
With visual attention, both human recognition and detec-
tion performances increase signiﬁcantly, especially on im-
ages with cluttered background [13].

Inspired by human attention, the Recurrent Visual Atten-
tion Model (RAM) is proposed for image recognition [47].
RAM is a deep recurrent neural architecture with iterative
attention selection mechanism, which mimics the human vi-
sual system to suppress non-relevant image regions and ex-
tract discriminative features in a complicated environment.
This signiﬁcantly improves the recognition accuracy [1], es-
pecially for ﬁne-grained object recognition [52, 45]. RAM
also allows the network to process a high resolution im-
age with only limited computational resources. By itera-
tively attending to different sub-regions (with a ﬁxed reso-
lution), RAM could efﬁciently process images with various

(a) Easy

(b) Moderate

(c) Hard

Figure 1. We show the recognition process of 3 tawny owl images
with increasing level of difﬁculty for recognition. When recogniz-
ing the same object in different images, human may spend differ-
ent length of time.

resolutions and aspect ratios in a constant computational
time [47, 1].

Besides attention, human also tend to dynamically allo-
cate different computational time when processing different
images [13, 16]. The length of the processing time often de-
pends on the task and the content of the input images (e.g.
background clutter, occlusion, object scale). For example,
during the recognition of a ﬁne-grained bird category, if the
bird appears in a large proportion with clean background
(Figure 1a), human can immediately recognize the image
without hesitation. However, when the bird is under cam-
ouﬂage (Figure 1b) or hiding in the scene with background
clutter and pose variation (Figure 1c), people may spend
much more time on locating the bird and extracting discrim-
inative parts to produce a conﬁdent prediction.

Inspired by this, we propose an extension to RAM named
as Dynamic Time Recurrent Attention Model (DT-RAM),
by adding an extra binary (continue/stop) action at every
time step. During each step, DT-RAM will not only up-
date the next attention, but produce a decision whether stop
the computation and output the classiﬁcation score. The
model is a simple extension to RAM, but can be viewed as
a ﬁrst step towards dynamic model during inference [26],
where the model structure can vary based on each input
instance. This could bring DT-RAM more ﬂexibility and

1

reduce redundant computation to further save computation,
especially when the input examples are “easy” to recognize.
Although DT-RAM is an end-to-end recurrent neural ar-
chitecture, we ﬁnd it is hard to directly train the model
parameters from scratch, particularly for challenging tasks
like ﬁne-grained recognition. When the total number of
steps increases, the delayed reward issue becomes more se-
vere and the variance of gradients becomes larger. This
makes policy gradient training algorithms such as REIN-
FORCE [57] harder to optimize. We address this problem
with curriculum learning [4]. During the training of RAM,
we gradually increase the training difﬁculty by gradually
increasing the total number of time steps. We then initial-
ize the parameters in DT-RAM with the pre-trained RAM
and ﬁne-tune it with REINFORCE. This strategy helps the
model to converge to a better local optimum than training
from scratch. We also ﬁnd intermediate supervision is cru-
cial to the performance, particularly when training longer
sequences.

We demonstrate the effectiveness of our model on pub-
lic benchmark datasets including MNIST [42] as well as
two ﬁne-grained datasets, CUB-200-2011 [60] and Stanford
Cars [39]. We also conduct an extensive study to understand
how dynamic time works in these datasets. Experimental
results suggest that DT-RAM can achieve state-of-the-art
performance on ﬁne-grained image recognition. Compared
to RAM, the model also uses less average computational
time, better ﬁtting devices with computational limitations.

2. Related Work

2.1. Visual Attention Models

Visual attention is a long-standing topic in computer vi-
sion [32, 31, 59]. With the recent success of deep neural
networks [41, 54, 58, 28], Mnih et al. [47] develop the Re-
current Visual Attention Model (RAM) for image recogni-
tion, where the attention is modeled with neural networks to
capture local regions in the image. Ba et al. [1] follow the
same framework and apply RAM to recognize multiple ob-
jects in images. Sermanet et al. [52] further extend RAM to
ﬁne-grained image recognition, since ﬁne-grained problems
usually require the comparison between local parts. Be-
sides ﬁne-grained recognition, attention models also work
for various machine learning problems including machine
translation [2], image captioning [64], image question an-
swering [63, 12, 21, 65] and video activity recognition [66].
Based on the differentiable property of attention models,
most of the existing work can be divided into two groups:
soft attention and hard attention [64]. The soft attention
models deﬁne attention as a set of continuous variables
representing the relative importance of spatial or temporal
cues. The model is differentiable hence can be trained with
backpropogation. The hard attention models deﬁne atten-

tion as actions and model the whole problem as a Partially
Observed Markov Decision Process (POMDP) [56]. Such
models are usually nondifferentiable to the reward function
hence use policy gradient such as REINFORCE [57] to op-
timize the model parameters. Our model belongs to the hard
attention since its stopping action is discrete.

2.2. Feedback Neural Networks

The visual attention models can be also viewed as a
special type of feedback neural networks [67, 55, 9, 61].
A feedback neural network is a special recurrent architec-
ture that uses previously computed high level features to
back reﬁne low level features. It uses both top-down and
bottom-up information to compute the intermediate lay-
ers. Besides attention models, feedback neural networks
also have other variants. For example, Carreira et al. [10]
perform human pose estimation with iterative error feed-
back. Newell et al. [49] build a stacked hourglass network
for human pose estimation. Hu and Ramanan [29] show
that network feedbacks can help better locating human face
landmarks. All these models demonstrate top-down infor-
mation could potentially improve the model discriminative
ability [67]. However, these models either ﬁx the number of
recurrent steps or use simple rules to decide early stopping.

2.3. Dynamic Computational Time

Graves [26] recently introduce adaptive computational
time in recurrent neural networks. The model augments the
network with a sigmoidal halting unit at each time step,
whose activation determines the probability whether the
computation should stop. Figurnov et al. [20] extend [26]
to spatially adaptive computational time for residual net-
works. Their approach is similar but deﬁne the halting units
over spatial positions. Neumann et al. [48] extend the simi-
lar idea to temporally dependent reasoning. They achieve a
small performance beneﬁt on top of a similar model without
an adaptive component. Jernite et al. [34] learn a scheduler
to determine what portion of the hidden state to compute
based on the current hidden and input vectors. All these
models can vary the computation time during inference, but
the stopping policy is based on the cumulative probability
of halting units, which can be viewed as a ﬁxed policy.

As far as we know, Odena et al. [51] is the ﬁrst attempt
that learns to change model behavior at test time with re-
inforcement learning. Their model adaptively constructs
computational graphs from sub-modules on a per-input ba-
sis. However, they only verify on small dataset such as
MNIST [42] and CIFAR-10 [40]. Ba et al. [1] augment
RAM with the ”end-of-sequence” symbol to deal with vari-
able number of objects in an image, which inspires our work
on DT-RAM. However, they still ﬁx the number of atten-
tions for each target. There is also a lack of diagnostic ex-
periments on understanding how ”end-of-sequence” symbol

affects the dynamics. In this work, we conduct extensive ex-
perimental comparisons on larger scale natural images from
ﬁne-grained recognition.

2.4. Fine-Grained Recognition

Fine-grained image recognition has been extensively
studied in recent years [7, 6, 15, 30, 37, 39, 35, 44, 50].
Based on the research focus, ﬁne-grained recognition ap-
proaches can be divided into representation learning, part
alignment models or emphasis on data. The ﬁrst group at-
tempts to build implicitly powerful feature representations
such as bilinear pooling or compact bilinear pooling [22, 36,
43], which turn to be very effective for ﬁne-grained prob-
lems. The second group attempts to localize discriminative
parts to effectively deal with large intra-class variation as
well as subtle inter-class variation [5, 8, 23, 30, 45]. The
third group studies the importance of the scale of training
data [38]. They achieve signiﬁcantly better performance on
multiple ﬁne-grained dataset by using an extra large set of
training images.

With the fast development of deep models such as Bilin-
ear CNN [43] and Spatial Transformer Networks [33], it is
unclear whether attention models are still effective for ﬁne-
grained recognition. In this paper, we show that the visual
attention model, if trained carefully, can still achieve com-
parable performance as state-of-the-art methods.

3. Model

Figure 2. An illustration of the architecture of recurrent attention
model. By iteratively attending to more discriminative area lt, the
model could output more conﬁdent predictions yt.

smaller loss more probable. The second term is the standard
gradient for neural nets with a ﬁxed structure.

During experiments, it is difﬁcult to directly compute the
gradient of the L over θ because it requires to evaluate expo-
nentially many possible structures during training. Hence to
train the model, we ﬁrst sample a set of structures, then ap-
proximate the gradient with Monte Carlo Simulation [57]:

∂L
∂θ

≈

1
M

M
(cid:88)

i=1

(cid:18) ∂ log P (Si|x, θ)
∂θ

LSi(x, θ) +

(cid:19)

∂LSi(x, θ)
∂θ

(2)

where M is the number of samples.

3.1. Learning with Dynamic Structure

3.2. Recurrent Attention Model (RAM)

The difference between a dynamic structure model and
a ﬁxed structure model is that during inference the model
structure S depends on both the input x and parameter θ.

Given an input x, the probability of choosing a compu-
tational structure S is P (S|x, θ). Given the model space of
S, this probability can be modeled with a neural network.
Suppose the loss during training is deﬁned as LS (x, θ). The
overall expected loss for an input x is

L = ES [LS (x, θ)] =

P (S|x, θ)LS (x, θ)

(1)

(cid:88)

S

The gradient of L with respect to parameter θ can be calcu-
lated as:

∂L
∂θ

(cid:88)

(cid:18) ∂P (S)
∂θ

LS + P (S)

(cid:19)

∂LS
∂θ

=

=

S
(cid:88)

(cid:18)

S

P (S)

∂ log P (S)
∂θ

LS + P (S)

(cid:19)

∂LS
∂θ

= ES

(cid:20) ∂ log P (S|x, θ)
∂θ

LS (x, θ) +

(cid:21)

∂LS (x, θ)
∂θ

The recurrent attention model is formulated as a Par-
tially Observed Markov Decision Process (POMDP). At
each time step, the model works as an agent that executes an
action based on the observation and receives a reward. The
agent actively controls how to act, and it may affect the state
of the environment. In RAM, the action corresponds to the
localization of the attention region. The observation is a lo-
cal (partially observed) region cropped from the image. The
reward measures the quality of the prediction using all the
cropped regions and can be delayed. The target of learning
is to ﬁnd the optimal decision policy to generate attentions
from observations that maximizes the expected cumulative
reward across all time steps.

More formally, RAM deﬁnes the input image as x and
the total number of attentions as T . At each time step
t ∈ {1, . . . , T }, the model crops a local region φ(x, lt−1)
around location lt−1 which is computed from the previous
time step. It then updates the internal state ht with a recur-
rent neural network

ht = fh(ht−1, φ(x, lt−1), θh)

(3)

The ﬁrst term in the above expectation is the same as REIN-
FORCE algorithm [57], it makes the learning leading to a

which is parameterized by θh. The model then computes
two branches. One is the localization network fl(ht, θl)

Figure 3. An illustration of the architecture of dynamic time recur-
rent attention model. An extra binary stopping action at is added
to each time step. at = 0 represents ”continue” (green solid circle)
and at = 1 represents ”stop” (red solid circle).

which models the attention policy, parameterized by θl. The
other is the classiﬁcation network fc(ht, θc) which com-
putes the classiﬁcation score, parameterized by θc. During
inference, it samples the attention location based on the pol-
icy π(lt|fl(ht, θl)). Figure 2 illustrates the inference proce-
dure.

3.3. Dynamic Computational Time for Recurrent

Attention (DT-RAM)

To introduce dynamic structure to RAM, we simply aug-
ment it with an additional set of actions {at} that decides
when it will stop taking further attention and output results.
at ∈ {0, 1} is a binary variable with 0 representing “con-
tinue” and 1 indicating “stop”. Its sampling policy is mod-
eled via a stopping network fa(ht, θa). During inference,
we sample both the attention lt and stopping at with each
policy independently.

lt ∼ π(lt|fl(ht, θl)), at ∼ π(at|fa(ht, θa))

(4)

Figure 3 shows how the model works. Compared to Fig-
ure 2, the change is mainly an addition of at onto each time
step.

Figure 4 illustrates how DT-RAM adapts its model struc-
ture and computational time to different input images for
image recognition. When the input image is “easy” to rec-
ognize (Figure 4 left), we expect DT-RAM stop at the ﬁrst
few steps. When the input image is “hard” (Figure 4 right),
we expect the model learn to continue searching for infor-
mative regions.

3.4. Training

Figure 4. An illustration of how DT-RAM adapts its model struc-
ture and computational time to different input images.

by computing the following gradient:

∂L
∂θ

≈

(cid:88)

(cid:88)

(cid:18)

n

S

−

∂ log P (S|xn, θ)
∂θ

Rn +

∂LS (xn, yn, θ)
∂θ

(cid:19)

(5)
where θ = {θf , θl, θa, θc} are the parameters of the recur-
rent network, the attention network, the stopping network
and the classiﬁcation network respectively.

Compared to Equation 2, Equation 5 is an approximation
where we use a negative of reward function R to replace the
loss of a given structure LS in the ﬁrst term. This training
loss is similar to [47, 1]. Although the loss in Equation 2
can be optimized directly, using R can reduce the variance
in the estimator [1]. In addition,

P (S|xn, θ) =

π(lt|fl(ht, θl))π(at|fa(ht, θa))

(6)

T (n)
(cid:89)

t=1

is the sampling policy for structure S.

Rn =

γtrnt

T (n)
(cid:88)

t=1

(7)

is the cumulative discounted reward over T (n) time steps
for the n-th training example. The discount factor γ
controls the trade-off between making correct classiﬁca-
tion and taking more attentions. rnt is the reward at t-
th step. During experiments, we use a delayed reward.
We set rnt = 0 if t (cid:54)= T (n) and rnT = 1 only if
y = arg maxy P (y|fc(hT , θc)).
Intermediate Supervision:

the original
RAM [47], DT-RAM has intermediate supervision for
the classiﬁcation network at every time step, since its
underlying dynamic structure could require the model to
output classiﬁcation scores at any time step. The loss of

Unlike

LS (xn, yn, θ) =

Lt(xn, yn, θh, θc)

(8)

T (n)
(cid:88)

t=1

Given a set of training images with ground truth labels
(xn, yn)n=1···N , we jointly optimize the model parameters

is the average cross-entropy classiﬁcation loss over N train-
ing samples and T (n) time steps. Note that T depends on

Dataset

#Classes #Train #Test BBox

MNIST [42]
CUB-200-2011 [60]
Stanford Cars [39]

10
200
196

60000 10000
5794
5994
8041
8144

-
√
√

Table 1. Statistics of the three dataset. CUB-200-2011 and Stan-
ford Cars are both benchmark datasets in ﬁne-grained recognition.

n, indicating that each instance may have different stopping
times. During experiments, we ﬁnd intermediate supervi-
sion is also effective for the baseline RAM.

Curriculum Learning: During experiments, we adopt a
gradual training approach for the sake of accuracy. First, we
start with a base convolutional network (e.g. Residual Net-
works [28]) pre-trained on ImageNet [17]. We then ﬁne-
tune the base network on the ﬁne-grained dataset. This
gives us a very high baseline. Second, we train the RAM
model by gradually increase the total number of time steps.
Finally, we initialize DT-RAM with the trained RAM and
further ﬁne-tune the whole network with REINFORCE al-
gorithm.

4. Experiments

4.1. Dataset

We conduct experiments on three popular benchmark
datasets: MNIST [42], CUB-200-2011 [60] and Stanford
Cars [39]. Table 1 summarizes the details of each dataset.
MNIST contains 70,000 images with 10 digital numbers.
This is the dataset where the original visual attention model
tests its performance. However, images in MNIST dataset
are often too simple to generate conclusions to natural im-
ages. Therefore, we also compare on two challenging ﬁne-
grained recognition dataset. CUB-200-2011 [60] consists of
11,778 images with 200 bird categories. Stanford Cars [39]
includes 16,185 images of 196 car classes. Both datasets
contain a bounding box annotation for each image. CUB-
200-2011 also contains part annotation, which we do not
use in our algorithm. Most of the images in these two
datasets have cluttered background, hence visual attention
could be effective for them. All models are trained and
tested without ground truth bounding box annotations.

4.2. Implementation Details

MNIST: We use the original digital images with 28×28
pixel resolution. The digits are generally centered in the
image. We ﬁrst train multiple RAM models with up to 7
steps. At each time step, we crop a 8×8 patch from the
image based on the sampled attention location. The 8×8
patch only captures a part of a digit, hence the model usually
requires multiple steps to produce an accurate prediction.

The attention network, classiﬁcation network and stop-
ping network all output actions at every time step. The out-

put dimensions of the three networks are 2, 10 and 1 re-
spectively. All three networks are linear layers on top of
the recurrent network. The classiﬁcation network and stop-
ping network contain softmax layers to compute the discrete
probability. The attention network deﬁnes a Gaussian pol-
icy with a ﬁxed variance, representing the continuous dis-
tribution of the two location variables. The recurrent state
vector has 256 dimensions. All methods are trained using
stochastic gradient descent (SGD) with batch size of 20 and
momentum of 0.9. The reward at the last time step is 1 if
the agent classiﬁes correctly and 0 otherwise. The rewards
for all other time steps are 0. One can refer to [47] for more
training details.

CUB-200-2011 and Stanford Cars: We use the same
setting for both dataset. All images are ﬁrst normalized by
resizing to 512×512. We then crop a 224×224 image patch
at every time step except for the ﬁrst step, which is a key dif-
ference from MNIST. At the ﬁrst step, we use a 448×448
crop. This guarantees the 1-step RAM and 1-step DT-RAM
to have the same performance as the baseline convolutional
network. We use Residual Networks [28] pre-trained on Im-
ageNet [17] as the baseline network. We use the “pool-5”
feature as input to the recurrent hidden layer. The recurrent
layer is a fully connected layer with ReLU activations. The
attention network, classiﬁcation network and stopping net-
work are all linear layers on top of the recurrent network.
The dimensionality of the hidden layer is set to 512.

All models are trained using SGD with momentum of
0.9 for 90 epochs. The learning rate is set to 0.001 at the
beginning and multiplied by 0.1 every 30 epochs. The batch
size is 28 which is the maximum we can use for 512×512
resolution.
(For diagnostic experiments with smaller im-
ages, we use batch size of 96.) The reward strategy is the
same as MNIST. During testing, the actions are chosen to
be the maximal probability output from each network. Note
that although bounding boxs or part-level annotations are
available with these datasets, we do not utilize any of them
throughout the experiments.

Computational Time: Our implementation is based on
Torch [14]. The computational time heavily depends on
the resolution of the input image and the baseline network
structure. We run all our experiments on a single Tesla K-
40 GPU. The average running time for a ResNet-50 on a
512×512 resolution image is 42ms. A 3-step RAM is 77ms
since it runs ResNet-50 on 2 extra 224×224 images.

4.3. Comparison with State-of-the-Art

MNIST: We train two DT-RAM models with different
discount factors. We train DT-RAM-1 with a smaller dis-
count factor (0.98) and DT-RAM-2 with a larger discount
factor (0.99). The smaller discount factor will encourage
the model to stop early in order to obtain a large reward,
hence one can expect DT-RAM-1 stops with less number of

MNIST

# Steps Error(%)

CUB-200-2011

Accuracy(%) Acc w. Box(%)

FC, 2 layers (256 hiddens each)
Convolutional, 2 layers
RAM 2 steps
RAM 4 steps
RAM 5 steps
RAM 7 steps

DT-RAM-1 3.6 steps
DT-RAM-2 5.2 steps

-
-
2
4
5
7

3.6
5.2

1.69
1.21
3.79
1.54
1.34
1.07

1.46
1.12

Table 2. Comparison to related work on MNIST. All the RAM
results are from [47].

Zhang et al. [68]
Branson et al. [8]
Simon et al. [53]
Krause et al. [37]
Lin et al. [43]
Jaderberg et al. [33]
Kong et al. [36]
Liu et al. [46]
Liu et al. [45]

ResNet-50 [28]
RAM 3 steps
DT-RAM 1.9 steps

73.9
75.7
81.0
82.0
84.1
84.1
84.2
84.3
85.4

84.5
86.0
86.0

76.4
85.4∗
-
82.8
85.1
-
-
84.7
85.5

-
-
-

Table 3. Comparison to related work on CUB-200-2011 dataset. ∗
Testing with both ground truth box and parts.

of the existing works. Adding further recurrent visual atten-
tion (RAM) reaches 86.0%, improving the baseline Resid-
ual Net by 1.5%, leading to a new state-of-the-art on CUB-
200-2011. DT-RAM further improves RAM, by achieving
the same state-of-the-art performance, with less number of
computational time on average (1.9 steps v.s. 3 steps).

Stanford Cars: We also compare extensively on Stan-
ford Cars dataset. Table 4 shows the results. Surprisingly, a
ﬁne-tuned 50-layer Residual Network again achieves 92.3%
accuracy on the testing set, surpassing most of the existing
work. This suggests that a single deep network (without any
further modiﬁcation or extra bounding box supervision) can
be the ﬁrst choice for ﬁne-grained recognition.

A 3-step RAM on top of the Residual Net further im-
proves to 93.1% accuracy, which is by far the new state-
of-the-art performance on Stanford Cars dataset. Compared
to [46], this is achieved without using bounding box an-
notation during testing. DT-RAM again achieves the same
accuracy as RAM but using 1.9 steps on average. We also
observe that the relative improvement of RAM to the base-
line model is no longer large (0.8%).

4.4. Ablation Study

We conduct a set of ablation studies to understand how
each component affects RAM and DT-RAM on ﬁne-grained
recognition. We focus on CUB-200-2011 dataset since its
images are real and challenging. However, due to the large
resolution of the images, we are not able to run many steps
with a very deep Residual Net. Therefore, instead of using
ResNet-50 with 512×512 image resolution which produces
state-of-the-art results, we use ResNet-34 with 256×256 as
the baseline model (79.9%). This allows us to train more
steps on the bird images to deeply understand the model.

Figure 5. The distribution of number of steps from two different
DT-RAM models on MNIST dataset. A model with longer average
steps tends to have a better accuracy.

steps than DT-RAM-2.

Table 2 summarizes the performance of different mod-
els on MNIST. Comparing to RAM with similar number of
steps, DT-RAM achieves a better error rate. For example,
DT-RAM-1 gets 1.46% recognition error with an average of
3.6 steps while RAM with 4 steps gets 1.54% error. Sim-
ilarly, DT-RAM-2 gets 1.12% error with an average of 5.2
steps while RAM with 5 steps has 1.34% error.

Figure 5 shows the distribution of number of steps for
the two DT-RAM models across all testing examples. We
ﬁnd a clear trade-off between efﬁciency and accuracy. DT-
RAM-1 which has less computational time, achieves higher
error than DT-RAM-2.

CUB-200-2011: We compare our model with all pre-
viously published methods. Table 3 summarizes the re-
sults. We observe that methods including Bilinear CNN [43,
36] (84.1% - 84.2%), Spatial Transformer Networks [33]
(84.1%) and Fully Convolutional Attention Networks [46]
(84.3%) all achieve similar performances. [45] further im-
prove the testing accuracy to 85.4% by utilizing attribute
labels annotated in the dataset.

Surprisingly, we ﬁnd that a carefully ﬁne-tuned Residual
Network with 50 layers already hits 84.5%, surpassing most

Input Resolution and Network Depth: Table 5 com-
pares the effect on different image resolutions with differ-

Stanford Cars

Accuracy(%) Acc w. Box(%)

Model

# Steps Accuracy(%)

ResNet-34
RAM 2 steps
RAM 3 steps
RAM 4 steps
RAM 5 steps
RAM 6 steps

1
2
3
4
5
6

79.9
80.7
81.1
81.5
81.8
81.8

81.8

DT-RAM (6 max steps)

3.6

Table 6. Comparison to RAM on CUB-200-2011. Note that the
1-step RAM is the same as the ResNet.

Chai et al. [11]
Gosselin et al. [25]
Girshick et al. [24]
Lin et al. [43]
Wang et al. [62]
Liu et al. [46]
Krause et al. [37]

ResNet-50 [28]
RAM 3 steps
DT-RAM 1.9 steps

78.0
82.7
88.4
91.3
-
91.5
92.6

92.3
93.1
93.1

-
87.9
-
-
92.5
93.1
92.8

-
-
-

Table 4. Comparison to related work on Stanford Cars dataset.

Resolution ResNet-34 RAM-34 ResNet-50 RAM-50

224×224
448×448

79.9
-

81.8
-

81.5
84.5

82.8
86.0

Table 5. The effect of input resolution and network depth on
ResNet and its RAM extension.

ent network depths. In general, a higher image resolution
signiﬁcantly helps ﬁne-grained recognition. For example,
given the same ResNet-50 model, 512×512 resolution with
448×448 crops gets 84.5% accuracy, outperforming 81.5%
from 256×256 with 224×224 crops. This is probably be-
cause higher resolution images contain more detailed in-
formation for ﬁne-grained recognition. A deeper Resid-
ual Network also signiﬁcantly improves performance. For
example, ResNet-50 obtains 81.5% accuracy compared to
ResNet-34 with only 79.9%. During experiments, we also
train a ResNet-101 on 224×224 crops and get 82.8% recog-
nition accuracy.

RAM v.s. DT-RAM: Table 6 shows how the number
of steps affects RAM on CUB-200-2011 dataset. Starting
from ResNet-34 which is also the 1 step RAM, the model
gradually increases its accuracy with more steps (79.9% →
81.8%). After 5 steps, RAM no longer improves. DT-RAM
also reaches the same accuracy. However, it only uses 3.5
steps on average than 6 steps, which is a promising trade-off
between computation and performance.

Figure 6 plots the distribution of number of steps from
the DT-RAM on the testing images. Surprisingly different
from MNIST, the model prefers to stop either at the begin-
ning of the time (1-2 steps), or in the end of the time (5-6
steps). There are very few images that the model choose to
stop at 3-4 steps.

Learned Policy v.s. Fixed Policy: One may suspect that
instead of learning the optimal stopping policy, whether we
can just use a ﬁxed stopping policy on RAM to determine
when to stop the recurrent iteration. For example, one can
simply learn a RAM model with intermediate supervision

Figure 6. The distribution of number of steps from a DT-RAM
model on CUB-200-2011 dataset. Unlike MNIST, the distribution
suggests the model prefer either stop at the beginning or in the end.

Threshold

# Steps Accuracy(%)

0
0.4
0.5
0.6
0.9
1.0

1
1.4
1.6
1.9
3.6
6

3.6

79.9
80.7
81.0
81.2
81.3
81.8

81.8

DT-RAM (6 max steps)

Table 7. Comparison to a ﬁxed stopping policy on CUB-200-2011.
The ﬁxed stopping policy runs on RAM (6 steps) such that the
recurrent attention stops if one of the class softmax probabilities is
above the threshold.

at every time step and use a threshold over the classiﬁ-
cation network fc(ht, θc) to determine when to stop. We
compare the results between the described ﬁxed policy and
DT-RAM. Table 7 shows the comparison. We ﬁnd that al-
though the ﬁxed stopping policy gives reasonably good re-
sults (i.e. 3.6 steps with 81.3% accuracy), DT-RAM still
works slightly better (i.e. 3.6 steps with 81.8% accuracy).

Curriculum Learning: Table 8 compares the results
on whether using curriculum learning to train RAM. If we
learn the model parameters completely from scratch with-
out curriculum, the performance start to decrease with more

(a) 1 step

(b) 2 steps

(c) 3 steps

(d) 4 steps

(e) 5 steps

(f) 6 steps

Figure 7. Qualitative results of DT-RAM on CUB-200-2011 testing set. We show images with different ending steps from 1 to 6. Each
bounding box indicates an attention region. Bounding box colors are displayed in order. The ﬁrst step uses the full image as input hence
there is no bounding box. From step 1 to step 6, we observe a gradual increase of background clutter and recognition difﬁculty, matching
our hypothesis for using dynamic computation time for different types of images.

(b) 2 steps
Figure 8. Qualitative results of DT-RAM on Stanford Car testing set. We only manage to train a 3-step model with 512×512 resolution.

(c) 3 steps

(a) 1 step

# Steps

1

2

3

4

5

6

# Steps

1

2

3

4

5

6

w.o C.L. 79.9 80.7 80.5 80.9 80.3 80.0
79.9 80.7 81.1 81.5 81.8 81.8
w. C.L.

w.o I.S. 79.9 78.8 76.1 74.8 74.9 74.7
79.9 80.7 81.1 81.5 81.8 81.8
w. I.S.

Table 8. The effect of Curriculum Learning on RAM.

Table 9. The effect of Intermediate Supervision on RAM.

time steps (79.9%→80.9%→80.0%). This is because sim-
ple policy gradient method becomes harder to train with
longer sequences. Curriculum learning makes training more
stable, since it guarantees the accuracy on adding more
steps will not hurt the performance. The testing perfor-
mance hence gradually increases over number of steps,
from 79.9% to 81.8%.

Intermediate Supervision: Table 9 compares the test-
ing results for using intermediate supervision. Note that
the original RAM model [47] only computes output at the
last time step. Although this works for small dataset like
MNIST. when the input images become more challenging
and time step increases, the RAM model learned without
intermediate supervision starts to get worse. On contrary,
adding an intermediate loss at each step makes RAM mod-
els with more steps steadily improve the ﬁnal performance.
Qualitative Results: We visualize the qualitative results

of DT-RAM on CUB-200-2011 and Stanford Cars testing
set in Figure 7 and Figure 8 respectively. From step 1 to
step 6, we observe a gradual increase of background clutter
and recognition difﬁculty, matching our hypothesis of using
dynamic computation time for different types of images.

5. Conclusion and Future Work

In this work we present a novel method for learning
to dynamically adjust computational time during inference
with reinforcement learning. We apply it on the recurrent
visual attention model and show its effectiveness for ﬁne-
grained recognition. We believe that such methods will be
important for developing dynamic reasoning in deep learn-
ing and computer vision. Future work on developing more
sophisticated dynamic models and apply it to more complex
tasks such as visual question answering will be conducted.

References

[1] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recog-
nition with visual attention. arXiv preprint arXiv:1412.7755,
2014. 1, 2, 4

[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473, 2014. 2

[3] D. M. Beck and S. Kastner. Top-down and bottom-up mech-
anisms in biasing competition in the human brain. Vision
research, 49(10):1154–1165, 2009. 1

[4] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-
riculum learning. In Proceedings of the 26th annual interna-
tional conference on machine learning, pages 41–48. ACM,
2009. 2

[5] T. Berg and P. Belhumeur. Poof: Part-based one-vs.-one fea-
tures for ﬁne-grained categorization, face veriﬁcation, and
attribute estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 955–
962, 2013. 3

[6] T. Berg, J. Liu, S. Woo Lee, M. L. Alexander, D. W. Jacobs,
and P. N. Belhumeur. Birdsnap: Large-scale ﬁne-grained vi-
sual categorization of birds. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2011–2018, 2014. 3

[7] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101–
mining discriminative components with random forests. In
European Conference on Computer Vision, pages 446–461.
Springer, 2014. 3

[8] S. Branson, G. Van Horn, S. Belongie, and P. Perona. Bird
species categorization using pose normalized deep convolu-
tional nets. arXiv preprint arXiv:1406.2952, 2014. 3, 6
[9] C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang,
L. Wang, C. Huang, W. Xu, et al. Look and think twice: Cap-
turing top-down visual attention with feedback convolutional
neural networks. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2956–2964, 2015. 2
[10] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Hu-
man pose estimation with iterative error feedback. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4733–4742, 2016. 2

[11] Y. Chai, V. Lempitsky, and A. Zisserman. Symbiotic seg-
mentation and part localization for ﬁne-grained categoriza-
tion. In Proceedings of the IEEE International Conference
on Computer Vision, pages 321–328, 2013. 7

[12] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Neva-
tia. Abc-cnn: An attention based convolutional neural
arXiv preprint
network for visual question answering.
arXiv:1511.05960, 2015. 2

[13] R. M. Cichy, D. Pantazis, and A. Oliva. Resolving human
object recognition in space and time. Nature neuroscience,
17(3):455–462, 2014. 1

[14] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In BigLearn,
NIPS Workshop, number EPFL-CONF-192376, 2011. 5
[15] Y. Cui, F. Zhou, Y. Lin, and S. Belongie. Fine-grained cate-
gorization and dataset bootstrapping using deep metric learn-
In Proceedings of the IEEE
ing with humans in the loop.

Conference on Computer Vision and Pattern Recognition,
pages 1153–1162, 2016. 3

[16] G. Deco and E. T. Rolls. A neurodynamical cortical model
of visual attention and invariant object recognition. Vision
research, 44(6):621–642, 2004. 1

[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. IEEE, 2009. 5

[18] R. Desimone. Visual attention mediated by biased competi-
tion in extrastriate visual cortex. Philosophical Transactions
of the Royal Society B: Biological Sciences, 353(1373):1245,
1998. 1

[19] R. Desimone and J. Duncan. Neural mechanisms of selective
visual attention. Annual review of neuroscience, 18(1):193–
222, 1995. 1

[20] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang,
Spatially adaptive
arXiv preprint

D. Vetrov, and R. Salakhutdinov.
computation time for residual networks.
arXiv:1612.02297, 2016. 2

[21] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016. 2

[22] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact
bilinear pooling. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 317–326,
2016. 3

[23] E. Gavves, B. Fernando, C. G. Snoek, A. W. Smeulders, and
T. Tuytelaars. Fine-grained categorization by alignments. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1713–1720, 2013. 3

[24] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
In Proceedings of the IEEE conference on
segmentation.
computer vision and pattern recognition, pages 580–587,
2014. 7

[25] P.-H. Gosselin, N. Murray, H. J´egou, and F. Perronnin. Re-
visiting the ﬁsher vector for ﬁne-grained classiﬁcation. Pat-
tern Recognition Letters, 49:92–98, 2014. 7

[26] A. Graves. Adaptive computation time for recurrent neural
networks. arXiv preprint arXiv:1603.08983, 2016. 1, 2
[27] M. Hayhoe and D. Ballard. Eye movements in natural be-
havior. Trends in cognitive sciences, 9(4):188–194, 2005. 1
[28] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
770–778, 2016. 2, 5, 6, 7

[29] P. Hu and D. Ramanan. Bottom-up and top-down reasoning
with hierarchical rectiﬁed gaussians. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5600–5609, 2016. 2

[30] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn
for ﬁne-grained visual categorization. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1173–1182, 2016. 3

[31] L. Itti and C. Koch. Computational modelling of visual at-
tention. Nature reviews neuroscience, 2(3):194–203, 2001.
2

[32] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
visual attention for rapid scene analysis. IEEE Transactions
on pattern analysis and machine intelligence, 20(11):1254–
1259, 1998. 2

[33] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in Neural Information

transformer networks.
Processing Systems, pages 2017–2025, 2015. 3, 6

[34] Y. Jernite, E. Grave, A. Joulin, and T. Mikolov. Variable
computation in recurrent neural networks. arXiv preprint
arXiv:1611.06188, 2016. 2

[35] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In Proc. CVPR Workshop on Fine-Grained Visual Catego-
rization (FGVC), volume 2, 2011. 3

[36] S. Kong and C. Fowlkes. Low-rank bilinear pooling for
ﬁne-grained classiﬁcation. arXiv preprint arXiv:1611.05109,
2016. 3, 6

[37] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5546–5555, 2015. 3, 6, 7

[38] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev,
T. Duerig, J. Philbin, and L. Fei-Fei. The unreasonable ef-
fectiveness of noisy data for ﬁne-grained recognition.
In
European Conference on Computer Vision, pages 301–320.
Springer, 2016. 3

[39] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In Proceedings
of the IEEE International Conference on Computer Vision
Workshops, pages 554–561, 2013. 2, 3, 5

[40] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 2

[41] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, pages
1097–1105, 2012. 2

[42] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 2, 5

[43] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In Proceedings of the
IEEE International Conference on Computer Vision, pages
1449–1457, 2015. 3, 6, 7

[44] J. Liu, A. Kanazawa, D. Jacobs, and P. Belhumeur. Dog
In European
breed classiﬁcation using part localization.
Conference on Computer Vision, pages 172–185. Springer,
2012. 3

[45] X. Liu, J. Wang, S. Wen, E. Ding, and Y. Lin. Localizing by
describing: Attribute-guided attention localization for ﬁne-
grained recognition. arXiv preprint arXiv:1605.06217, 2016.
1, 3, 6

[46] X. Liu, T. Xia, J. Wang, Y. Yang, F. Zhou, and Y. Lin. Fine-
grained recognition with automatic and efﬁcient part atten-
tion. arXiv preprint arXiv:1603.06765, 2016. 6, 7

[47] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of vi-
sual attention. In Advances in neural information processing
systems, pages 2204–2212, 2014. 1, 2, 4, 5, 6, 8
[48] M. Neumann, P. Stenetorp, and S. Riedel.
to reason with adaptive computation.
arXiv:1610.07647, 2016. 2

Learning
arXiv preprint

[49] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In European Conference
on Computer Vision, pages 483–499. Springer, 2016. 2
[50] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Computer Vision,
Graphics & Image Processing, 2008. ICVGIP’08. Sixth In-
dian Conference on, pages 722–729. IEEE, 2008. 3

[51] A. Odena, D. Lawson, and C. Olah. Changing model behav-
ior at test-time using reinforcement learning. arXiv preprint
arXiv:1702.07780, 2017. 2

[52] P. Sermanet, A. Frome, and E. Real. Attention for ﬁne-
arXiv preprint arXiv:1412.7054,

grained categorization.
2014. 1, 2

[53] M. Simon and E. Rodner. Neural activation constellations:
Unsupervised part model discovery with convolutional net-
works. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1143–1151, 2015. 6

[54] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 2

[55] M. F. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber.
Deep networks with internal selective attention through feed-
back connections. In Advances in Neural Information Pro-
cessing Systems, pages 3545–3553, 2014. 2

[56] R. S. Sutton and A. G. Barto. Reinforcement learning: An
introduction, volume 1. MIT press Cambridge, 1998. 2
[57] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour,
et al. Policy gradient methods for reinforcement learning
In NIPS, volume 99, pages
with function approximation.
1057–1063, 1999. 2, 3

[58] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1–9, 2015. 2

[59] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. Lai, N. Davis,
and F. Nuﬂo. Modeling visual attention via selective tuning.
Artiﬁcial intelligence, 78(1-2):507–545, 1995. 2

[60] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. 2011. 2, 5
[61] Q. Wang, J. Zhang, S. Song, and Z. Zhang. Attentional neu-
ral network: Feature selection using cognitive feedback. In
Advances in Neural Information Processing Systems, pages
2033–2041, 2014. 2

[62] Y. Wang, J. Choi, V. Morariu, and L. S. Davis. Mining dis-
criminative triplets of patches for ﬁne-grained classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1163–1172, 2016. 7

[63] H. Xu and K. Saenko. Ask, attend and answer: Exploring
question-guided spatial attention for visual question answer-
In European Conference on Computer Vision, pages
ing.
451–466. Springer, 2016. 2

[64] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhut-
dinov, R. S. Zemel, and Y. Bengio. Show, attend and tell:
Neural image caption generation with visual attention.
In
ICML, volume 14, pages 77–81, 2015. 2

[65] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked
In Pro-
attention networks for image question answering.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 21–29, 2016. 2

[66] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2678–2687, 2016. 2

[67] A. R. Zamir, T.-L. Wu, L. Sun, W. Shen, J. Malik,
arXiv preprint

Feedback networks.

and S. Savarese.
arXiv:1612.09508, 2016. 2

[68] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-
In Eu-
based r-cnns for ﬁne-grained category detection.
ropean conference on computer vision, pages 834–849.
Springer, 2014. 6

Dynamic Computational Time for Visual Attention

Zhichao Li,Yi Yang,Xiao Liu,Feng Zhou,Shilei Wen,Wei Xu
Baidu Research
{lizhichao01,yangyi05,liuxiao12,zhoufeng09,wenshilei,xuwei06}@baidu.com

7
1
0
2
 
p
e
S
 
7
 
 
]

V
C
.
s
c
[
 
 
3
v
2
3
3
0
1
.
3
0
7
1
:
v
i
X
r
a

Abstract

We propose a dynamic computational time model to ac-
celerate the average processing time for recurrent visual
attention (RAM). Rather than attention with a ﬁxed num-
ber of steps for each input image, the model learns to de-
cide when to stop on the ﬂy. To achieve this, we add an
additional continue/stop action per time step to RAM and
use reinforcement learning to learn both the optimal atten-
tion policy and stopping policy. The modiﬁcation is sim-
ple but could dramatically save the average computational
time while keeping the same recognition performance as
RAM. Experimental results on CUB-200-2011 and Stan-
ford Cars dataset demonstrate the dynamic computational
model can work effectively for ﬁne-grained image recogni-
tion. The source code of this paper can be obtained from
https://github.com/baidu-research/DT-RAM.

1. Introduction

Human have the remarkable ability of selective visual at-
tention [27, 19]. Cognitive science explains this as the “Bi-
ased Competition Theory” [3, 18] that human visual cortex
is enhanced by top-down guidance during feedback loops.
The feedback signals suppress non-relevant stimuli present
in the visual ﬁeld, helping human searching for ”goals”.
With visual attention, both human recognition and detec-
tion performances increase signiﬁcantly, especially on im-
ages with cluttered background [13].

Inspired by human attention, the Recurrent Visual Atten-
tion Model (RAM) is proposed for image recognition [47].
RAM is a deep recurrent neural architecture with iterative
attention selection mechanism, which mimics the human vi-
sual system to suppress non-relevant image regions and ex-
tract discriminative features in a complicated environment.
This signiﬁcantly improves the recognition accuracy [1], es-
pecially for ﬁne-grained object recognition [52, 45]. RAM
also allows the network to process a high resolution im-
age with only limited computational resources. By itera-
tively attending to different sub-regions (with a ﬁxed reso-
lution), RAM could efﬁciently process images with various

(a) Easy

(b) Moderate

(c) Hard

Figure 1. We show the recognition process of 3 tawny owl images
with increasing level of difﬁculty for recognition. When recogniz-
ing the same object in different images, human may spend differ-
ent length of time.

resolutions and aspect ratios in a constant computational
time [47, 1].

Besides attention, human also tend to dynamically allo-
cate different computational time when processing different
images [13, 16]. The length of the processing time often de-
pends on the task and the content of the input images (e.g.
background clutter, occlusion, object scale). For example,
during the recognition of a ﬁne-grained bird category, if the
bird appears in a large proportion with clean background
(Figure 1a), human can immediately recognize the image
without hesitation. However, when the bird is under cam-
ouﬂage (Figure 1b) or hiding in the scene with background
clutter and pose variation (Figure 1c), people may spend
much more time on locating the bird and extracting discrim-
inative parts to produce a conﬁdent prediction.

Inspired by this, we propose an extension to RAM named
as Dynamic Time Recurrent Attention Model (DT-RAM),
by adding an extra binary (continue/stop) action at every
time step. During each step, DT-RAM will not only up-
date the next attention, but produce a decision whether stop
the computation and output the classiﬁcation score. The
model is a simple extension to RAM, but can be viewed as
a ﬁrst step towards dynamic model during inference [26],
where the model structure can vary based on each input
instance. This could bring DT-RAM more ﬂexibility and

1

reduce redundant computation to further save computation,
especially when the input examples are “easy” to recognize.
Although DT-RAM is an end-to-end recurrent neural ar-
chitecture, we ﬁnd it is hard to directly train the model
parameters from scratch, particularly for challenging tasks
like ﬁne-grained recognition. When the total number of
steps increases, the delayed reward issue becomes more se-
vere and the variance of gradients becomes larger. This
makes policy gradient training algorithms such as REIN-
FORCE [57] harder to optimize. We address this problem
with curriculum learning [4]. During the training of RAM,
we gradually increase the training difﬁculty by gradually
increasing the total number of time steps. We then initial-
ize the parameters in DT-RAM with the pre-trained RAM
and ﬁne-tune it with REINFORCE. This strategy helps the
model to converge to a better local optimum than training
from scratch. We also ﬁnd intermediate supervision is cru-
cial to the performance, particularly when training longer
sequences.

We demonstrate the effectiveness of our model on pub-
lic benchmark datasets including MNIST [42] as well as
two ﬁne-grained datasets, CUB-200-2011 [60] and Stanford
Cars [39]. We also conduct an extensive study to understand
how dynamic time works in these datasets. Experimental
results suggest that DT-RAM can achieve state-of-the-art
performance on ﬁne-grained image recognition. Compared
to RAM, the model also uses less average computational
time, better ﬁtting devices with computational limitations.

2. Related Work

2.1. Visual Attention Models

Visual attention is a long-standing topic in computer vi-
sion [32, 31, 59]. With the recent success of deep neural
networks [41, 54, 58, 28], Mnih et al. [47] develop the Re-
current Visual Attention Model (RAM) for image recogni-
tion, where the attention is modeled with neural networks to
capture local regions in the image. Ba et al. [1] follow the
same framework and apply RAM to recognize multiple ob-
jects in images. Sermanet et al. [52] further extend RAM to
ﬁne-grained image recognition, since ﬁne-grained problems
usually require the comparison between local parts. Be-
sides ﬁne-grained recognition, attention models also work
for various machine learning problems including machine
translation [2], image captioning [64], image question an-
swering [63, 12, 21, 65] and video activity recognition [66].
Based on the differentiable property of attention models,
most of the existing work can be divided into two groups:
soft attention and hard attention [64]. The soft attention
models deﬁne attention as a set of continuous variables
representing the relative importance of spatial or temporal
cues. The model is differentiable hence can be trained with
backpropogation. The hard attention models deﬁne atten-

tion as actions and model the whole problem as a Partially
Observed Markov Decision Process (POMDP) [56]. Such
models are usually nondifferentiable to the reward function
hence use policy gradient such as REINFORCE [57] to op-
timize the model parameters. Our model belongs to the hard
attention since its stopping action is discrete.

2.2. Feedback Neural Networks

The visual attention models can be also viewed as a
special type of feedback neural networks [67, 55, 9, 61].
A feedback neural network is a special recurrent architec-
ture that uses previously computed high level features to
back reﬁne low level features. It uses both top-down and
bottom-up information to compute the intermediate lay-
ers. Besides attention models, feedback neural networks
also have other variants. For example, Carreira et al. [10]
perform human pose estimation with iterative error feed-
back. Newell et al. [49] build a stacked hourglass network
for human pose estimation. Hu and Ramanan [29] show
that network feedbacks can help better locating human face
landmarks. All these models demonstrate top-down infor-
mation could potentially improve the model discriminative
ability [67]. However, these models either ﬁx the number of
recurrent steps or use simple rules to decide early stopping.

2.3. Dynamic Computational Time

Graves [26] recently introduce adaptive computational
time in recurrent neural networks. The model augments the
network with a sigmoidal halting unit at each time step,
whose activation determines the probability whether the
computation should stop. Figurnov et al. [20] extend [26]
to spatially adaptive computational time for residual net-
works. Their approach is similar but deﬁne the halting units
over spatial positions. Neumann et al. [48] extend the simi-
lar idea to temporally dependent reasoning. They achieve a
small performance beneﬁt on top of a similar model without
an adaptive component. Jernite et al. [34] learn a scheduler
to determine what portion of the hidden state to compute
based on the current hidden and input vectors. All these
models can vary the computation time during inference, but
the stopping policy is based on the cumulative probability
of halting units, which can be viewed as a ﬁxed policy.

As far as we know, Odena et al. [51] is the ﬁrst attempt
that learns to change model behavior at test time with re-
inforcement learning. Their model adaptively constructs
computational graphs from sub-modules on a per-input ba-
sis. However, they only verify on small dataset such as
MNIST [42] and CIFAR-10 [40]. Ba et al. [1] augment
RAM with the ”end-of-sequence” symbol to deal with vari-
able number of objects in an image, which inspires our work
on DT-RAM. However, they still ﬁx the number of atten-
tions for each target. There is also a lack of diagnostic ex-
periments on understanding how ”end-of-sequence” symbol

affects the dynamics. In this work, we conduct extensive ex-
perimental comparisons on larger scale natural images from
ﬁne-grained recognition.

2.4. Fine-Grained Recognition

Fine-grained image recognition has been extensively
studied in recent years [7, 6, 15, 30, 37, 39, 35, 44, 50].
Based on the research focus, ﬁne-grained recognition ap-
proaches can be divided into representation learning, part
alignment models or emphasis on data. The ﬁrst group at-
tempts to build implicitly powerful feature representations
such as bilinear pooling or compact bilinear pooling [22, 36,
43], which turn to be very effective for ﬁne-grained prob-
lems. The second group attempts to localize discriminative
parts to effectively deal with large intra-class variation as
well as subtle inter-class variation [5, 8, 23, 30, 45]. The
third group studies the importance of the scale of training
data [38]. They achieve signiﬁcantly better performance on
multiple ﬁne-grained dataset by using an extra large set of
training images.

With the fast development of deep models such as Bilin-
ear CNN [43] and Spatial Transformer Networks [33], it is
unclear whether attention models are still effective for ﬁne-
grained recognition. In this paper, we show that the visual
attention model, if trained carefully, can still achieve com-
parable performance as state-of-the-art methods.

3. Model

Figure 2. An illustration of the architecture of recurrent attention
model. By iteratively attending to more discriminative area lt, the
model could output more conﬁdent predictions yt.

smaller loss more probable. The second term is the standard
gradient for neural nets with a ﬁxed structure.

During experiments, it is difﬁcult to directly compute the
gradient of the L over θ because it requires to evaluate expo-
nentially many possible structures during training. Hence to
train the model, we ﬁrst sample a set of structures, then ap-
proximate the gradient with Monte Carlo Simulation [57]:

∂L
∂θ

≈

1
M

M
(cid:88)

i=1

(cid:18) ∂ log P (Si|x, θ)
∂θ

LSi(x, θ) +

(cid:19)

∂LSi(x, θ)
∂θ

(2)

where M is the number of samples.

3.1. Learning with Dynamic Structure

3.2. Recurrent Attention Model (RAM)

The difference between a dynamic structure model and
a ﬁxed structure model is that during inference the model
structure S depends on both the input x and parameter θ.

Given an input x, the probability of choosing a compu-
tational structure S is P (S|x, θ). Given the model space of
S, this probability can be modeled with a neural network.
Suppose the loss during training is deﬁned as LS (x, θ). The
overall expected loss for an input x is

L = ES [LS (x, θ)] =

P (S|x, θ)LS (x, θ)

(1)

(cid:88)

S

The gradient of L with respect to parameter θ can be calcu-
lated as:

∂L
∂θ

(cid:88)

(cid:18) ∂P (S)
∂θ

LS + P (S)

(cid:19)

∂LS
∂θ

=

=

S
(cid:88)

(cid:18)

S

P (S)

∂ log P (S)
∂θ

LS + P (S)

(cid:19)

∂LS
∂θ

= ES

(cid:20) ∂ log P (S|x, θ)
∂θ

LS (x, θ) +

(cid:21)

∂LS (x, θ)
∂θ

The recurrent attention model is formulated as a Par-
tially Observed Markov Decision Process (POMDP). At
each time step, the model works as an agent that executes an
action based on the observation and receives a reward. The
agent actively controls how to act, and it may affect the state
of the environment. In RAM, the action corresponds to the
localization of the attention region. The observation is a lo-
cal (partially observed) region cropped from the image. The
reward measures the quality of the prediction using all the
cropped regions and can be delayed. The target of learning
is to ﬁnd the optimal decision policy to generate attentions
from observations that maximizes the expected cumulative
reward across all time steps.

More formally, RAM deﬁnes the input image as x and
the total number of attentions as T . At each time step
t ∈ {1, . . . , T }, the model crops a local region φ(x, lt−1)
around location lt−1 which is computed from the previous
time step. It then updates the internal state ht with a recur-
rent neural network

ht = fh(ht−1, φ(x, lt−1), θh)

(3)

The ﬁrst term in the above expectation is the same as REIN-
FORCE algorithm [57], it makes the learning leading to a

which is parameterized by θh. The model then computes
two branches. One is the localization network fl(ht, θl)

Figure 3. An illustration of the architecture of dynamic time recur-
rent attention model. An extra binary stopping action at is added
to each time step. at = 0 represents ”continue” (green solid circle)
and at = 1 represents ”stop” (red solid circle).

which models the attention policy, parameterized by θl. The
other is the classiﬁcation network fc(ht, θc) which com-
putes the classiﬁcation score, parameterized by θc. During
inference, it samples the attention location based on the pol-
icy π(lt|fl(ht, θl)). Figure 2 illustrates the inference proce-
dure.

3.3. Dynamic Computational Time for Recurrent

Attention (DT-RAM)

To introduce dynamic structure to RAM, we simply aug-
ment it with an additional set of actions {at} that decides
when it will stop taking further attention and output results.
at ∈ {0, 1} is a binary variable with 0 representing “con-
tinue” and 1 indicating “stop”. Its sampling policy is mod-
eled via a stopping network fa(ht, θa). During inference,
we sample both the attention lt and stopping at with each
policy independently.

lt ∼ π(lt|fl(ht, θl)), at ∼ π(at|fa(ht, θa))

(4)

Figure 3 shows how the model works. Compared to Fig-
ure 2, the change is mainly an addition of at onto each time
step.

Figure 4 illustrates how DT-RAM adapts its model struc-
ture and computational time to different input images for
image recognition. When the input image is “easy” to rec-
ognize (Figure 4 left), we expect DT-RAM stop at the ﬁrst
few steps. When the input image is “hard” (Figure 4 right),
we expect the model learn to continue searching for infor-
mative regions.

3.4. Training

Figure 4. An illustration of how DT-RAM adapts its model struc-
ture and computational time to different input images.

by computing the following gradient:

∂L
∂θ

≈

(cid:88)

(cid:88)

(cid:18)

n

S

−

∂ log P (S|xn, θ)
∂θ

Rn +

∂LS (xn, yn, θ)
∂θ

(cid:19)

(5)
where θ = {θf , θl, θa, θc} are the parameters of the recur-
rent network, the attention network, the stopping network
and the classiﬁcation network respectively.

Compared to Equation 2, Equation 5 is an approximation
where we use a negative of reward function R to replace the
loss of a given structure LS in the ﬁrst term. This training
loss is similar to [47, 1]. Although the loss in Equation 2
can be optimized directly, using R can reduce the variance
in the estimator [1]. In addition,

P (S|xn, θ) =

π(lt|fl(ht, θl))π(at|fa(ht, θa))

(6)

T (n)
(cid:89)

t=1

is the sampling policy for structure S.

Rn =

γtrnt

T (n)
(cid:88)

t=1

(7)

is the cumulative discounted reward over T (n) time steps
for the n-th training example. The discount factor γ
controls the trade-off between making correct classiﬁca-
tion and taking more attentions. rnt is the reward at t-
th step. During experiments, we use a delayed reward.
We set rnt = 0 if t (cid:54)= T (n) and rnT = 1 only if
y = arg maxy P (y|fc(hT , θc)).
Intermediate Supervision:

the original
RAM [47], DT-RAM has intermediate supervision for
the classiﬁcation network at every time step, since its
underlying dynamic structure could require the model to
output classiﬁcation scores at any time step. The loss of

Unlike

LS (xn, yn, θ) =

Lt(xn, yn, θh, θc)

(8)

T (n)
(cid:88)

t=1

Given a set of training images with ground truth labels
(xn, yn)n=1···N , we jointly optimize the model parameters

is the average cross-entropy classiﬁcation loss over N train-
ing samples and T (n) time steps. Note that T depends on

Dataset

#Classes #Train #Test BBox

MNIST [42]
CUB-200-2011 [60]
Stanford Cars [39]

10
200
196

60000 10000
5794
5994
8041
8144

-
√
√

Table 1. Statistics of the three dataset. CUB-200-2011 and Stan-
ford Cars are both benchmark datasets in ﬁne-grained recognition.

n, indicating that each instance may have different stopping
times. During experiments, we ﬁnd intermediate supervi-
sion is also effective for the baseline RAM.

Curriculum Learning: During experiments, we adopt a
gradual training approach for the sake of accuracy. First, we
start with a base convolutional network (e.g. Residual Net-
works [28]) pre-trained on ImageNet [17]. We then ﬁne-
tune the base network on the ﬁne-grained dataset. This
gives us a very high baseline. Second, we train the RAM
model by gradually increase the total number of time steps.
Finally, we initialize DT-RAM with the trained RAM and
further ﬁne-tune the whole network with REINFORCE al-
gorithm.

4. Experiments

4.1. Dataset

We conduct experiments on three popular benchmark
datasets: MNIST [42], CUB-200-2011 [60] and Stanford
Cars [39]. Table 1 summarizes the details of each dataset.
MNIST contains 70,000 images with 10 digital numbers.
This is the dataset where the original visual attention model
tests its performance. However, images in MNIST dataset
are often too simple to generate conclusions to natural im-
ages. Therefore, we also compare on two challenging ﬁne-
grained recognition dataset. CUB-200-2011 [60] consists of
11,778 images with 200 bird categories. Stanford Cars [39]
includes 16,185 images of 196 car classes. Both datasets
contain a bounding box annotation for each image. CUB-
200-2011 also contains part annotation, which we do not
use in our algorithm. Most of the images in these two
datasets have cluttered background, hence visual attention
could be effective for them. All models are trained and
tested without ground truth bounding box annotations.

4.2. Implementation Details

MNIST: We use the original digital images with 28×28
pixel resolution. The digits are generally centered in the
image. We ﬁrst train multiple RAM models with up to 7
steps. At each time step, we crop a 8×8 patch from the
image based on the sampled attention location. The 8×8
patch only captures a part of a digit, hence the model usually
requires multiple steps to produce an accurate prediction.

The attention network, classiﬁcation network and stop-
ping network all output actions at every time step. The out-

put dimensions of the three networks are 2, 10 and 1 re-
spectively. All three networks are linear layers on top of
the recurrent network. The classiﬁcation network and stop-
ping network contain softmax layers to compute the discrete
probability. The attention network deﬁnes a Gaussian pol-
icy with a ﬁxed variance, representing the continuous dis-
tribution of the two location variables. The recurrent state
vector has 256 dimensions. All methods are trained using
stochastic gradient descent (SGD) with batch size of 20 and
momentum of 0.9. The reward at the last time step is 1 if
the agent classiﬁes correctly and 0 otherwise. The rewards
for all other time steps are 0. One can refer to [47] for more
training details.

CUB-200-2011 and Stanford Cars: We use the same
setting for both dataset. All images are ﬁrst normalized by
resizing to 512×512. We then crop a 224×224 image patch
at every time step except for the ﬁrst step, which is a key dif-
ference from MNIST. At the ﬁrst step, we use a 448×448
crop. This guarantees the 1-step RAM and 1-step DT-RAM
to have the same performance as the baseline convolutional
network. We use Residual Networks [28] pre-trained on Im-
ageNet [17] as the baseline network. We use the “pool-5”
feature as input to the recurrent hidden layer. The recurrent
layer is a fully connected layer with ReLU activations. The
attention network, classiﬁcation network and stopping net-
work are all linear layers on top of the recurrent network.
The dimensionality of the hidden layer is set to 512.

All models are trained using SGD with momentum of
0.9 for 90 epochs. The learning rate is set to 0.001 at the
beginning and multiplied by 0.1 every 30 epochs. The batch
size is 28 which is the maximum we can use for 512×512
resolution.
(For diagnostic experiments with smaller im-
ages, we use batch size of 96.) The reward strategy is the
same as MNIST. During testing, the actions are chosen to
be the maximal probability output from each network. Note
that although bounding boxs or part-level annotations are
available with these datasets, we do not utilize any of them
throughout the experiments.

Computational Time: Our implementation is based on
Torch [14]. The computational time heavily depends on
the resolution of the input image and the baseline network
structure. We run all our experiments on a single Tesla K-
40 GPU. The average running time for a ResNet-50 on a
512×512 resolution image is 42ms. A 3-step RAM is 77ms
since it runs ResNet-50 on 2 extra 224×224 images.

4.3. Comparison with State-of-the-Art

MNIST: We train two DT-RAM models with different
discount factors. We train DT-RAM-1 with a smaller dis-
count factor (0.98) and DT-RAM-2 with a larger discount
factor (0.99). The smaller discount factor will encourage
the model to stop early in order to obtain a large reward,
hence one can expect DT-RAM-1 stops with less number of

MNIST

# Steps Error(%)

CUB-200-2011

Accuracy(%) Acc w. Box(%)

FC, 2 layers (256 hiddens each)
Convolutional, 2 layers
RAM 2 steps
RAM 4 steps
RAM 5 steps
RAM 7 steps

DT-RAM-1 3.6 steps
DT-RAM-2 5.2 steps

-
-
2
4
5
7

3.6
5.2

1.69
1.21
3.79
1.54
1.34
1.07

1.46
1.12

Table 2. Comparison to related work on MNIST. All the RAM
results are from [47].

Zhang et al. [68]
Branson et al. [8]
Simon et al. [53]
Krause et al. [37]
Lin et al. [43]
Jaderberg et al. [33]
Kong et al. [36]
Liu et al. [46]
Liu et al. [45]

ResNet-50 [28]
RAM 3 steps
DT-RAM 1.9 steps

73.9
75.7
81.0
82.0
84.1
84.1
84.2
84.3
85.4

84.5
86.0
86.0

76.4
85.4∗
-
82.8
85.1
-
-
84.7
85.5

-
-
-

Table 3. Comparison to related work on CUB-200-2011 dataset. ∗
Testing with both ground truth box and parts.

of the existing works. Adding further recurrent visual atten-
tion (RAM) reaches 86.0%, improving the baseline Resid-
ual Net by 1.5%, leading to a new state-of-the-art on CUB-
200-2011. DT-RAM further improves RAM, by achieving
the same state-of-the-art performance, with less number of
computational time on average (1.9 steps v.s. 3 steps).

Stanford Cars: We also compare extensively on Stan-
ford Cars dataset. Table 4 shows the results. Surprisingly, a
ﬁne-tuned 50-layer Residual Network again achieves 92.3%
accuracy on the testing set, surpassing most of the existing
work. This suggests that a single deep network (without any
further modiﬁcation or extra bounding box supervision) can
be the ﬁrst choice for ﬁne-grained recognition.

A 3-step RAM on top of the Residual Net further im-
proves to 93.1% accuracy, which is by far the new state-
of-the-art performance on Stanford Cars dataset. Compared
to [46], this is achieved without using bounding box an-
notation during testing. DT-RAM again achieves the same
accuracy as RAM but using 1.9 steps on average. We also
observe that the relative improvement of RAM to the base-
line model is no longer large (0.8%).

4.4. Ablation Study

We conduct a set of ablation studies to understand how
each component affects RAM and DT-RAM on ﬁne-grained
recognition. We focus on CUB-200-2011 dataset since its
images are real and challenging. However, due to the large
resolution of the images, we are not able to run many steps
with a very deep Residual Net. Therefore, instead of using
ResNet-50 with 512×512 image resolution which produces
state-of-the-art results, we use ResNet-34 with 256×256 as
the baseline model (79.9%). This allows us to train more
steps on the bird images to deeply understand the model.

Figure 5. The distribution of number of steps from two different
DT-RAM models on MNIST dataset. A model with longer average
steps tends to have a better accuracy.

steps than DT-RAM-2.

Table 2 summarizes the performance of different mod-
els on MNIST. Comparing to RAM with similar number of
steps, DT-RAM achieves a better error rate. For example,
DT-RAM-1 gets 1.46% recognition error with an average of
3.6 steps while RAM with 4 steps gets 1.54% error. Sim-
ilarly, DT-RAM-2 gets 1.12% error with an average of 5.2
steps while RAM with 5 steps has 1.34% error.

Figure 5 shows the distribution of number of steps for
the two DT-RAM models across all testing examples. We
ﬁnd a clear trade-off between efﬁciency and accuracy. DT-
RAM-1 which has less computational time, achieves higher
error than DT-RAM-2.

CUB-200-2011: We compare our model with all pre-
viously published methods. Table 3 summarizes the re-
sults. We observe that methods including Bilinear CNN [43,
36] (84.1% - 84.2%), Spatial Transformer Networks [33]
(84.1%) and Fully Convolutional Attention Networks [46]
(84.3%) all achieve similar performances. [45] further im-
prove the testing accuracy to 85.4% by utilizing attribute
labels annotated in the dataset.

Surprisingly, we ﬁnd that a carefully ﬁne-tuned Residual
Network with 50 layers already hits 84.5%, surpassing most

Input Resolution and Network Depth: Table 5 com-
pares the effect on different image resolutions with differ-

Stanford Cars

Accuracy(%) Acc w. Box(%)

Model

# Steps Accuracy(%)

ResNet-34
RAM 2 steps
RAM 3 steps
RAM 4 steps
RAM 5 steps
RAM 6 steps

1
2
3
4
5
6

79.9
80.7
81.1
81.5
81.8
81.8

81.8

DT-RAM (6 max steps)

3.6

Table 6. Comparison to RAM on CUB-200-2011. Note that the
1-step RAM is the same as the ResNet.

Chai et al. [11]
Gosselin et al. [25]
Girshick et al. [24]
Lin et al. [43]
Wang et al. [62]
Liu et al. [46]
Krause et al. [37]

ResNet-50 [28]
RAM 3 steps
DT-RAM 1.9 steps

78.0
82.7
88.4
91.3
-
91.5
92.6

92.3
93.1
93.1

-
87.9
-
-
92.5
93.1
92.8

-
-
-

Table 4. Comparison to related work on Stanford Cars dataset.

Resolution ResNet-34 RAM-34 ResNet-50 RAM-50

224×224
448×448

79.9
-

81.8
-

81.5
84.5

82.8
86.0

Table 5. The effect of input resolution and network depth on
ResNet and its RAM extension.

ent network depths. In general, a higher image resolution
signiﬁcantly helps ﬁne-grained recognition. For example,
given the same ResNet-50 model, 512×512 resolution with
448×448 crops gets 84.5% accuracy, outperforming 81.5%
from 256×256 with 224×224 crops. This is probably be-
cause higher resolution images contain more detailed in-
formation for ﬁne-grained recognition. A deeper Resid-
ual Network also signiﬁcantly improves performance. For
example, ResNet-50 obtains 81.5% accuracy compared to
ResNet-34 with only 79.9%. During experiments, we also
train a ResNet-101 on 224×224 crops and get 82.8% recog-
nition accuracy.

RAM v.s. DT-RAM: Table 6 shows how the number
of steps affects RAM on CUB-200-2011 dataset. Starting
from ResNet-34 which is also the 1 step RAM, the model
gradually increases its accuracy with more steps (79.9% →
81.8%). After 5 steps, RAM no longer improves. DT-RAM
also reaches the same accuracy. However, it only uses 3.5
steps on average than 6 steps, which is a promising trade-off
between computation and performance.

Figure 6 plots the distribution of number of steps from
the DT-RAM on the testing images. Surprisingly different
from MNIST, the model prefers to stop either at the begin-
ning of the time (1-2 steps), or in the end of the time (5-6
steps). There are very few images that the model choose to
stop at 3-4 steps.

Learned Policy v.s. Fixed Policy: One may suspect that
instead of learning the optimal stopping policy, whether we
can just use a ﬁxed stopping policy on RAM to determine
when to stop the recurrent iteration. For example, one can
simply learn a RAM model with intermediate supervision

Figure 6. The distribution of number of steps from a DT-RAM
model on CUB-200-2011 dataset. Unlike MNIST, the distribution
suggests the model prefer either stop at the beginning or in the end.

Threshold

# Steps Accuracy(%)

0
0.4
0.5
0.6
0.9
1.0

1
1.4
1.6
1.9
3.6
6

3.6

79.9
80.7
81.0
81.2
81.3
81.8

81.8

DT-RAM (6 max steps)

Table 7. Comparison to a ﬁxed stopping policy on CUB-200-2011.
The ﬁxed stopping policy runs on RAM (6 steps) such that the
recurrent attention stops if one of the class softmax probabilities is
above the threshold.

at every time step and use a threshold over the classiﬁ-
cation network fc(ht, θc) to determine when to stop. We
compare the results between the described ﬁxed policy and
DT-RAM. Table 7 shows the comparison. We ﬁnd that al-
though the ﬁxed stopping policy gives reasonably good re-
sults (i.e. 3.6 steps with 81.3% accuracy), DT-RAM still
works slightly better (i.e. 3.6 steps with 81.8% accuracy).

Curriculum Learning: Table 8 compares the results
on whether using curriculum learning to train RAM. If we
learn the model parameters completely from scratch with-
out curriculum, the performance start to decrease with more

(a) 1 step

(b) 2 steps

(c) 3 steps

(d) 4 steps

(e) 5 steps

(f) 6 steps

Figure 7. Qualitative results of DT-RAM on CUB-200-2011 testing set. We show images with different ending steps from 1 to 6. Each
bounding box indicates an attention region. Bounding box colors are displayed in order. The ﬁrst step uses the full image as input hence
there is no bounding box. From step 1 to step 6, we observe a gradual increase of background clutter and recognition difﬁculty, matching
our hypothesis for using dynamic computation time for different types of images.

(b) 2 steps
Figure 8. Qualitative results of DT-RAM on Stanford Car testing set. We only manage to train a 3-step model with 512×512 resolution.

(c) 3 steps

(a) 1 step

# Steps

1

2

3

4

5

6

# Steps

1

2

3

4

5

6

w.o C.L. 79.9 80.7 80.5 80.9 80.3 80.0
79.9 80.7 81.1 81.5 81.8 81.8
w. C.L.

w.o I.S. 79.9 78.8 76.1 74.8 74.9 74.7
79.9 80.7 81.1 81.5 81.8 81.8
w. I.S.

Table 8. The effect of Curriculum Learning on RAM.

Table 9. The effect of Intermediate Supervision on RAM.

time steps (79.9%→80.9%→80.0%). This is because sim-
ple policy gradient method becomes harder to train with
longer sequences. Curriculum learning makes training more
stable, since it guarantees the accuracy on adding more
steps will not hurt the performance. The testing perfor-
mance hence gradually increases over number of steps,
from 79.9% to 81.8%.

Intermediate Supervision: Table 9 compares the test-
ing results for using intermediate supervision. Note that
the original RAM model [47] only computes output at the
last time step. Although this works for small dataset like
MNIST. when the input images become more challenging
and time step increases, the RAM model learned without
intermediate supervision starts to get worse. On contrary,
adding an intermediate loss at each step makes RAM mod-
els with more steps steadily improve the ﬁnal performance.
Qualitative Results: We visualize the qualitative results

of DT-RAM on CUB-200-2011 and Stanford Cars testing
set in Figure 7 and Figure 8 respectively. From step 1 to
step 6, we observe a gradual increase of background clutter
and recognition difﬁculty, matching our hypothesis of using
dynamic computation time for different types of images.

5. Conclusion and Future Work

In this work we present a novel method for learning
to dynamically adjust computational time during inference
with reinforcement learning. We apply it on the recurrent
visual attention model and show its effectiveness for ﬁne-
grained recognition. We believe that such methods will be
important for developing dynamic reasoning in deep learn-
ing and computer vision. Future work on developing more
sophisticated dynamic models and apply it to more complex
tasks such as visual question answering will be conducted.

References

[1] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recog-
nition with visual attention. arXiv preprint arXiv:1412.7755,
2014. 1, 2, 4

[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473, 2014. 2

[3] D. M. Beck and S. Kastner. Top-down and bottom-up mech-
anisms in biasing competition in the human brain. Vision
research, 49(10):1154–1165, 2009. 1

[4] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-
riculum learning. In Proceedings of the 26th annual interna-
tional conference on machine learning, pages 41–48. ACM,
2009. 2

[5] T. Berg and P. Belhumeur. Poof: Part-based one-vs.-one fea-
tures for ﬁne-grained categorization, face veriﬁcation, and
attribute estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 955–
962, 2013. 3

[6] T. Berg, J. Liu, S. Woo Lee, M. L. Alexander, D. W. Jacobs,
and P. N. Belhumeur. Birdsnap: Large-scale ﬁne-grained vi-
sual categorization of birds. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2011–2018, 2014. 3

[7] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101–
mining discriminative components with random forests. In
European Conference on Computer Vision, pages 446–461.
Springer, 2014. 3

[8] S. Branson, G. Van Horn, S. Belongie, and P. Perona. Bird
species categorization using pose normalized deep convolu-
tional nets. arXiv preprint arXiv:1406.2952, 2014. 3, 6
[9] C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang,
L. Wang, C. Huang, W. Xu, et al. Look and think twice: Cap-
turing top-down visual attention with feedback convolutional
neural networks. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2956–2964, 2015. 2
[10] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Hu-
man pose estimation with iterative error feedback. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4733–4742, 2016. 2

[11] Y. Chai, V. Lempitsky, and A. Zisserman. Symbiotic seg-
mentation and part localization for ﬁne-grained categoriza-
tion. In Proceedings of the IEEE International Conference
on Computer Vision, pages 321–328, 2013. 7

[12] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Neva-
tia. Abc-cnn: An attention based convolutional neural
arXiv preprint
network for visual question answering.
arXiv:1511.05960, 2015. 2

[13] R. M. Cichy, D. Pantazis, and A. Oliva. Resolving human
object recognition in space and time. Nature neuroscience,
17(3):455–462, 2014. 1

[14] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In BigLearn,
NIPS Workshop, number EPFL-CONF-192376, 2011. 5
[15] Y. Cui, F. Zhou, Y. Lin, and S. Belongie. Fine-grained cate-
gorization and dataset bootstrapping using deep metric learn-
In Proceedings of the IEEE
ing with humans in the loop.

Conference on Computer Vision and Pattern Recognition,
pages 1153–1162, 2016. 3

[16] G. Deco and E. T. Rolls. A neurodynamical cortical model
of visual attention and invariant object recognition. Vision
research, 44(6):621–642, 2004. 1

[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. IEEE, 2009. 5

[18] R. Desimone. Visual attention mediated by biased competi-
tion in extrastriate visual cortex. Philosophical Transactions
of the Royal Society B: Biological Sciences, 353(1373):1245,
1998. 1

[19] R. Desimone and J. Duncan. Neural mechanisms of selective
visual attention. Annual review of neuroscience, 18(1):193–
222, 1995. 1

[20] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang,
Spatially adaptive
arXiv preprint

D. Vetrov, and R. Salakhutdinov.
computation time for residual networks.
arXiv:1612.02297, 2016. 2

[21] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016. 2

[22] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact
bilinear pooling. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 317–326,
2016. 3

[23] E. Gavves, B. Fernando, C. G. Snoek, A. W. Smeulders, and
T. Tuytelaars. Fine-grained categorization by alignments. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1713–1720, 2013. 3

[24] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
In Proceedings of the IEEE conference on
segmentation.
computer vision and pattern recognition, pages 580–587,
2014. 7

[25] P.-H. Gosselin, N. Murray, H. J´egou, and F. Perronnin. Re-
visiting the ﬁsher vector for ﬁne-grained classiﬁcation. Pat-
tern Recognition Letters, 49:92–98, 2014. 7

[26] A. Graves. Adaptive computation time for recurrent neural
networks. arXiv preprint arXiv:1603.08983, 2016. 1, 2
[27] M. Hayhoe and D. Ballard. Eye movements in natural be-
havior. Trends in cognitive sciences, 9(4):188–194, 2005. 1
[28] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
770–778, 2016. 2, 5, 6, 7

[29] P. Hu and D. Ramanan. Bottom-up and top-down reasoning
with hierarchical rectiﬁed gaussians. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5600–5609, 2016. 2

[30] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn
for ﬁne-grained visual categorization. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1173–1182, 2016. 3

[31] L. Itti and C. Koch. Computational modelling of visual at-
tention. Nature reviews neuroscience, 2(3):194–203, 2001.
2

[32] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
visual attention for rapid scene analysis. IEEE Transactions
on pattern analysis and machine intelligence, 20(11):1254–
1259, 1998. 2

[33] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in Neural Information

transformer networks.
Processing Systems, pages 2017–2025, 2015. 3, 6

[34] Y. Jernite, E. Grave, A. Joulin, and T. Mikolov. Variable
computation in recurrent neural networks. arXiv preprint
arXiv:1611.06188, 2016. 2

[35] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In Proc. CVPR Workshop on Fine-Grained Visual Catego-
rization (FGVC), volume 2, 2011. 3

[36] S. Kong and C. Fowlkes. Low-rank bilinear pooling for
ﬁne-grained classiﬁcation. arXiv preprint arXiv:1611.05109,
2016. 3, 6

[37] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5546–5555, 2015. 3, 6, 7

[38] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev,
T. Duerig, J. Philbin, and L. Fei-Fei. The unreasonable ef-
fectiveness of noisy data for ﬁne-grained recognition.
In
European Conference on Computer Vision, pages 301–320.
Springer, 2016. 3

[39] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In Proceedings
of the IEEE International Conference on Computer Vision
Workshops, pages 554–561, 2013. 2, 3, 5

[40] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 2

[41] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, pages
1097–1105, 2012. 2

[42] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 2, 5

[43] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In Proceedings of the
IEEE International Conference on Computer Vision, pages
1449–1457, 2015. 3, 6, 7

[44] J. Liu, A. Kanazawa, D. Jacobs, and P. Belhumeur. Dog
In European
breed classiﬁcation using part localization.
Conference on Computer Vision, pages 172–185. Springer,
2012. 3

[45] X. Liu, J. Wang, S. Wen, E. Ding, and Y. Lin. Localizing by
describing: Attribute-guided attention localization for ﬁne-
grained recognition. arXiv preprint arXiv:1605.06217, 2016.
1, 3, 6

[46] X. Liu, T. Xia, J. Wang, Y. Yang, F. Zhou, and Y. Lin. Fine-
grained recognition with automatic and efﬁcient part atten-
tion. arXiv preprint arXiv:1603.06765, 2016. 6, 7

[47] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of vi-
sual attention. In Advances in neural information processing
systems, pages 2204–2212, 2014. 1, 2, 4, 5, 6, 8
[48] M. Neumann, P. Stenetorp, and S. Riedel.
to reason with adaptive computation.
arXiv:1610.07647, 2016. 2

Learning
arXiv preprint

[49] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In European Conference
on Computer Vision, pages 483–499. Springer, 2016. 2
[50] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Computer Vision,
Graphics & Image Processing, 2008. ICVGIP’08. Sixth In-
dian Conference on, pages 722–729. IEEE, 2008. 3

[51] A. Odena, D. Lawson, and C. Olah. Changing model behav-
ior at test-time using reinforcement learning. arXiv preprint
arXiv:1702.07780, 2017. 2

[52] P. Sermanet, A. Frome, and E. Real. Attention for ﬁne-
arXiv preprint arXiv:1412.7054,

grained categorization.
2014. 1, 2

[53] M. Simon and E. Rodner. Neural activation constellations:
Unsupervised part model discovery with convolutional net-
works. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1143–1151, 2015. 6

[54] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 2

[55] M. F. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber.
Deep networks with internal selective attention through feed-
back connections. In Advances in Neural Information Pro-
cessing Systems, pages 3545–3553, 2014. 2

[56] R. S. Sutton and A. G. Barto. Reinforcement learning: An
introduction, volume 1. MIT press Cambridge, 1998. 2
[57] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour,
et al. Policy gradient methods for reinforcement learning
In NIPS, volume 99, pages
with function approximation.
1057–1063, 1999. 2, 3

[58] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1–9, 2015. 2

[59] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. Lai, N. Davis,
and F. Nuﬂo. Modeling visual attention via selective tuning.
Artiﬁcial intelligence, 78(1-2):507–545, 1995. 2

[60] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. 2011. 2, 5
[61] Q. Wang, J. Zhang, S. Song, and Z. Zhang. Attentional neu-
ral network: Feature selection using cognitive feedback. In
Advances in Neural Information Processing Systems, pages
2033–2041, 2014. 2

[62] Y. Wang, J. Choi, V. Morariu, and L. S. Davis. Mining dis-
criminative triplets of patches for ﬁne-grained classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1163–1172, 2016. 7

[63] H. Xu and K. Saenko. Ask, attend and answer: Exploring
question-guided spatial attention for visual question answer-
In European Conference on Computer Vision, pages
ing.
451–466. Springer, 2016. 2

[64] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhut-
dinov, R. S. Zemel, and Y. Bengio. Show, attend and tell:
Neural image caption generation with visual attention.
In
ICML, volume 14, pages 77–81, 2015. 2

[65] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked
In Pro-
attention networks for image question answering.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 21–29, 2016. 2

[66] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2678–2687, 2016. 2

[67] A. R. Zamir, T.-L. Wu, L. Sun, W. Shen, J. Malik,
arXiv preprint

Feedback networks.

and S. Savarese.
arXiv:1612.09508, 2016. 2

[68] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-
In Eu-
based r-cnns for ﬁne-grained category detection.
ropean conference on computer vision, pages 834–849.
Springer, 2014. 6

Dynamic Computational Time for Visual Attention

Zhichao Li,Yi Yang,Xiao Liu,Feng Zhou,Shilei Wen,Wei Xu
Baidu Research
{lizhichao01,yangyi05,liuxiao12,zhoufeng09,wenshilei,xuwei06}@baidu.com

7
1
0
2
 
p
e
S
 
7
 
 
]

V
C
.
s
c
[
 
 
3
v
2
3
3
0
1
.
3
0
7
1
:
v
i
X
r
a

Abstract

We propose a dynamic computational time model to ac-
celerate the average processing time for recurrent visual
attention (RAM). Rather than attention with a ﬁxed num-
ber of steps for each input image, the model learns to de-
cide when to stop on the ﬂy. To achieve this, we add an
additional continue/stop action per time step to RAM and
use reinforcement learning to learn both the optimal atten-
tion policy and stopping policy. The modiﬁcation is sim-
ple but could dramatically save the average computational
time while keeping the same recognition performance as
RAM. Experimental results on CUB-200-2011 and Stan-
ford Cars dataset demonstrate the dynamic computational
model can work effectively for ﬁne-grained image recogni-
tion. The source code of this paper can be obtained from
https://github.com/baidu-research/DT-RAM.

1. Introduction

Human have the remarkable ability of selective visual at-
tention [27, 19]. Cognitive science explains this as the “Bi-
ased Competition Theory” [3, 18] that human visual cortex
is enhanced by top-down guidance during feedback loops.
The feedback signals suppress non-relevant stimuli present
in the visual ﬁeld, helping human searching for ”goals”.
With visual attention, both human recognition and detec-
tion performances increase signiﬁcantly, especially on im-
ages with cluttered background [13].

Inspired by human attention, the Recurrent Visual Atten-
tion Model (RAM) is proposed for image recognition [47].
RAM is a deep recurrent neural architecture with iterative
attention selection mechanism, which mimics the human vi-
sual system to suppress non-relevant image regions and ex-
tract discriminative features in a complicated environment.
This signiﬁcantly improves the recognition accuracy [1], es-
pecially for ﬁne-grained object recognition [52, 45]. RAM
also allows the network to process a high resolution im-
age with only limited computational resources. By itera-
tively attending to different sub-regions (with a ﬁxed reso-
lution), RAM could efﬁciently process images with various

(a) Easy

(b) Moderate

(c) Hard

Figure 1. We show the recognition process of 3 tawny owl images
with increasing level of difﬁculty for recognition. When recogniz-
ing the same object in different images, human may spend differ-
ent length of time.

resolutions and aspect ratios in a constant computational
time [47, 1].

Besides attention, human also tend to dynamically allo-
cate different computational time when processing different
images [13, 16]. The length of the processing time often de-
pends on the task and the content of the input images (e.g.
background clutter, occlusion, object scale). For example,
during the recognition of a ﬁne-grained bird category, if the
bird appears in a large proportion with clean background
(Figure 1a), human can immediately recognize the image
without hesitation. However, when the bird is under cam-
ouﬂage (Figure 1b) or hiding in the scene with background
clutter and pose variation (Figure 1c), people may spend
much more time on locating the bird and extracting discrim-
inative parts to produce a conﬁdent prediction.

Inspired by this, we propose an extension to RAM named
as Dynamic Time Recurrent Attention Model (DT-RAM),
by adding an extra binary (continue/stop) action at every
time step. During each step, DT-RAM will not only up-
date the next attention, but produce a decision whether stop
the computation and output the classiﬁcation score. The
model is a simple extension to RAM, but can be viewed as
a ﬁrst step towards dynamic model during inference [26],
where the model structure can vary based on each input
instance. This could bring DT-RAM more ﬂexibility and

1

reduce redundant computation to further save computation,
especially when the input examples are “easy” to recognize.
Although DT-RAM is an end-to-end recurrent neural ar-
chitecture, we ﬁnd it is hard to directly train the model
parameters from scratch, particularly for challenging tasks
like ﬁne-grained recognition. When the total number of
steps increases, the delayed reward issue becomes more se-
vere and the variance of gradients becomes larger. This
makes policy gradient training algorithms such as REIN-
FORCE [57] harder to optimize. We address this problem
with curriculum learning [4]. During the training of RAM,
we gradually increase the training difﬁculty by gradually
increasing the total number of time steps. We then initial-
ize the parameters in DT-RAM with the pre-trained RAM
and ﬁne-tune it with REINFORCE. This strategy helps the
model to converge to a better local optimum than training
from scratch. We also ﬁnd intermediate supervision is cru-
cial to the performance, particularly when training longer
sequences.

We demonstrate the effectiveness of our model on pub-
lic benchmark datasets including MNIST [42] as well as
two ﬁne-grained datasets, CUB-200-2011 [60] and Stanford
Cars [39]. We also conduct an extensive study to understand
how dynamic time works in these datasets. Experimental
results suggest that DT-RAM can achieve state-of-the-art
performance on ﬁne-grained image recognition. Compared
to RAM, the model also uses less average computational
time, better ﬁtting devices with computational limitations.

2. Related Work

2.1. Visual Attention Models

Visual attention is a long-standing topic in computer vi-
sion [32, 31, 59]. With the recent success of deep neural
networks [41, 54, 58, 28], Mnih et al. [47] develop the Re-
current Visual Attention Model (RAM) for image recogni-
tion, where the attention is modeled with neural networks to
capture local regions in the image. Ba et al. [1] follow the
same framework and apply RAM to recognize multiple ob-
jects in images. Sermanet et al. [52] further extend RAM to
ﬁne-grained image recognition, since ﬁne-grained problems
usually require the comparison between local parts. Be-
sides ﬁne-grained recognition, attention models also work
for various machine learning problems including machine
translation [2], image captioning [64], image question an-
swering [63, 12, 21, 65] and video activity recognition [66].
Based on the differentiable property of attention models,
most of the existing work can be divided into two groups:
soft attention and hard attention [64]. The soft attention
models deﬁne attention as a set of continuous variables
representing the relative importance of spatial or temporal
cues. The model is differentiable hence can be trained with
backpropogation. The hard attention models deﬁne atten-

tion as actions and model the whole problem as a Partially
Observed Markov Decision Process (POMDP) [56]. Such
models are usually nondifferentiable to the reward function
hence use policy gradient such as REINFORCE [57] to op-
timize the model parameters. Our model belongs to the hard
attention since its stopping action is discrete.

2.2. Feedback Neural Networks

The visual attention models can be also viewed as a
special type of feedback neural networks [67, 55, 9, 61].
A feedback neural network is a special recurrent architec-
ture that uses previously computed high level features to
back reﬁne low level features. It uses both top-down and
bottom-up information to compute the intermediate lay-
ers. Besides attention models, feedback neural networks
also have other variants. For example, Carreira et al. [10]
perform human pose estimation with iterative error feed-
back. Newell et al. [49] build a stacked hourglass network
for human pose estimation. Hu and Ramanan [29] show
that network feedbacks can help better locating human face
landmarks. All these models demonstrate top-down infor-
mation could potentially improve the model discriminative
ability [67]. However, these models either ﬁx the number of
recurrent steps or use simple rules to decide early stopping.

2.3. Dynamic Computational Time

Graves [26] recently introduce adaptive computational
time in recurrent neural networks. The model augments the
network with a sigmoidal halting unit at each time step,
whose activation determines the probability whether the
computation should stop. Figurnov et al. [20] extend [26]
to spatially adaptive computational time for residual net-
works. Their approach is similar but deﬁne the halting units
over spatial positions. Neumann et al. [48] extend the simi-
lar idea to temporally dependent reasoning. They achieve a
small performance beneﬁt on top of a similar model without
an adaptive component. Jernite et al. [34] learn a scheduler
to determine what portion of the hidden state to compute
based on the current hidden and input vectors. All these
models can vary the computation time during inference, but
the stopping policy is based on the cumulative probability
of halting units, which can be viewed as a ﬁxed policy.

As far as we know, Odena et al. [51] is the ﬁrst attempt
that learns to change model behavior at test time with re-
inforcement learning. Their model adaptively constructs
computational graphs from sub-modules on a per-input ba-
sis. However, they only verify on small dataset such as
MNIST [42] and CIFAR-10 [40]. Ba et al. [1] augment
RAM with the ”end-of-sequence” symbol to deal with vari-
able number of objects in an image, which inspires our work
on DT-RAM. However, they still ﬁx the number of atten-
tions for each target. There is also a lack of diagnostic ex-
periments on understanding how ”end-of-sequence” symbol

affects the dynamics. In this work, we conduct extensive ex-
perimental comparisons on larger scale natural images from
ﬁne-grained recognition.

2.4. Fine-Grained Recognition

Fine-grained image recognition has been extensively
studied in recent years [7, 6, 15, 30, 37, 39, 35, 44, 50].
Based on the research focus, ﬁne-grained recognition ap-
proaches can be divided into representation learning, part
alignment models or emphasis on data. The ﬁrst group at-
tempts to build implicitly powerful feature representations
such as bilinear pooling or compact bilinear pooling [22, 36,
43], which turn to be very effective for ﬁne-grained prob-
lems. The second group attempts to localize discriminative
parts to effectively deal with large intra-class variation as
well as subtle inter-class variation [5, 8, 23, 30, 45]. The
third group studies the importance of the scale of training
data [38]. They achieve signiﬁcantly better performance on
multiple ﬁne-grained dataset by using an extra large set of
training images.

With the fast development of deep models such as Bilin-
ear CNN [43] and Spatial Transformer Networks [33], it is
unclear whether attention models are still effective for ﬁne-
grained recognition. In this paper, we show that the visual
attention model, if trained carefully, can still achieve com-
parable performance as state-of-the-art methods.

3. Model

Figure 2. An illustration of the architecture of recurrent attention
model. By iteratively attending to more discriminative area lt, the
model could output more conﬁdent predictions yt.

smaller loss more probable. The second term is the standard
gradient for neural nets with a ﬁxed structure.

During experiments, it is difﬁcult to directly compute the
gradient of the L over θ because it requires to evaluate expo-
nentially many possible structures during training. Hence to
train the model, we ﬁrst sample a set of structures, then ap-
proximate the gradient with Monte Carlo Simulation [57]:

∂L
∂θ

≈

1
M

M
(cid:88)

i=1

(cid:18) ∂ log P (Si|x, θ)
∂θ

LSi(x, θ) +

(cid:19)

∂LSi(x, θ)
∂θ

(2)

where M is the number of samples.

3.1. Learning with Dynamic Structure

3.2. Recurrent Attention Model (RAM)

The difference between a dynamic structure model and
a ﬁxed structure model is that during inference the model
structure S depends on both the input x and parameter θ.

Given an input x, the probability of choosing a compu-
tational structure S is P (S|x, θ). Given the model space of
S, this probability can be modeled with a neural network.
Suppose the loss during training is deﬁned as LS (x, θ). The
overall expected loss for an input x is

L = ES [LS (x, θ)] =

P (S|x, θ)LS (x, θ)

(1)

(cid:88)

S

The gradient of L with respect to parameter θ can be calcu-
lated as:

∂L
∂θ

(cid:88)

(cid:18) ∂P (S)
∂θ

LS + P (S)

(cid:19)

∂LS
∂θ

=

=

S
(cid:88)

(cid:18)

S

P (S)

∂ log P (S)
∂θ

LS + P (S)

(cid:19)

∂LS
∂θ

= ES

(cid:20) ∂ log P (S|x, θ)
∂θ

LS (x, θ) +

(cid:21)

∂LS (x, θ)
∂θ

The recurrent attention model is formulated as a Par-
tially Observed Markov Decision Process (POMDP). At
each time step, the model works as an agent that executes an
action based on the observation and receives a reward. The
agent actively controls how to act, and it may affect the state
of the environment. In RAM, the action corresponds to the
localization of the attention region. The observation is a lo-
cal (partially observed) region cropped from the image. The
reward measures the quality of the prediction using all the
cropped regions and can be delayed. The target of learning
is to ﬁnd the optimal decision policy to generate attentions
from observations that maximizes the expected cumulative
reward across all time steps.

More formally, RAM deﬁnes the input image as x and
the total number of attentions as T . At each time step
t ∈ {1, . . . , T }, the model crops a local region φ(x, lt−1)
around location lt−1 which is computed from the previous
time step. It then updates the internal state ht with a recur-
rent neural network

ht = fh(ht−1, φ(x, lt−1), θh)

(3)

The ﬁrst term in the above expectation is the same as REIN-
FORCE algorithm [57], it makes the learning leading to a

which is parameterized by θh. The model then computes
two branches. One is the localization network fl(ht, θl)

Figure 3. An illustration of the architecture of dynamic time recur-
rent attention model. An extra binary stopping action at is added
to each time step. at = 0 represents ”continue” (green solid circle)
and at = 1 represents ”stop” (red solid circle).

which models the attention policy, parameterized by θl. The
other is the classiﬁcation network fc(ht, θc) which com-
putes the classiﬁcation score, parameterized by θc. During
inference, it samples the attention location based on the pol-
icy π(lt|fl(ht, θl)). Figure 2 illustrates the inference proce-
dure.

3.3. Dynamic Computational Time for Recurrent

Attention (DT-RAM)

To introduce dynamic structure to RAM, we simply aug-
ment it with an additional set of actions {at} that decides
when it will stop taking further attention and output results.
at ∈ {0, 1} is a binary variable with 0 representing “con-
tinue” and 1 indicating “stop”. Its sampling policy is mod-
eled via a stopping network fa(ht, θa). During inference,
we sample both the attention lt and stopping at with each
policy independently.

lt ∼ π(lt|fl(ht, θl)), at ∼ π(at|fa(ht, θa))

(4)

Figure 3 shows how the model works. Compared to Fig-
ure 2, the change is mainly an addition of at onto each time
step.

Figure 4 illustrates how DT-RAM adapts its model struc-
ture and computational time to different input images for
image recognition. When the input image is “easy” to rec-
ognize (Figure 4 left), we expect DT-RAM stop at the ﬁrst
few steps. When the input image is “hard” (Figure 4 right),
we expect the model learn to continue searching for infor-
mative regions.

3.4. Training

Figure 4. An illustration of how DT-RAM adapts its model struc-
ture and computational time to different input images.

by computing the following gradient:

∂L
∂θ

≈

(cid:88)

(cid:88)

(cid:18)

n

S

−

∂ log P (S|xn, θ)
∂θ

Rn +

∂LS (xn, yn, θ)
∂θ

(cid:19)

(5)
where θ = {θf , θl, θa, θc} are the parameters of the recur-
rent network, the attention network, the stopping network
and the classiﬁcation network respectively.

Compared to Equation 2, Equation 5 is an approximation
where we use a negative of reward function R to replace the
loss of a given structure LS in the ﬁrst term. This training
loss is similar to [47, 1]. Although the loss in Equation 2
can be optimized directly, using R can reduce the variance
in the estimator [1]. In addition,

P (S|xn, θ) =

π(lt|fl(ht, θl))π(at|fa(ht, θa))

(6)

T (n)
(cid:89)

t=1

is the sampling policy for structure S.

Rn =

γtrnt

T (n)
(cid:88)

t=1

(7)

is the cumulative discounted reward over T (n) time steps
for the n-th training example. The discount factor γ
controls the trade-off between making correct classiﬁca-
tion and taking more attentions. rnt is the reward at t-
th step. During experiments, we use a delayed reward.
We set rnt = 0 if t (cid:54)= T (n) and rnT = 1 only if
y = arg maxy P (y|fc(hT , θc)).
Intermediate Supervision:

the original
RAM [47], DT-RAM has intermediate supervision for
the classiﬁcation network at every time step, since its
underlying dynamic structure could require the model to
output classiﬁcation scores at any time step. The loss of

Unlike

LS (xn, yn, θ) =

Lt(xn, yn, θh, θc)

(8)

T (n)
(cid:88)

t=1

Given a set of training images with ground truth labels
(xn, yn)n=1···N , we jointly optimize the model parameters

is the average cross-entropy classiﬁcation loss over N train-
ing samples and T (n) time steps. Note that T depends on

Dataset

#Classes #Train #Test BBox

MNIST [42]
CUB-200-2011 [60]
Stanford Cars [39]

10
200
196

60000 10000
5794
5994
8041
8144

-
√
√

Table 1. Statistics of the three dataset. CUB-200-2011 and Stan-
ford Cars are both benchmark datasets in ﬁne-grained recognition.

n, indicating that each instance may have different stopping
times. During experiments, we ﬁnd intermediate supervi-
sion is also effective for the baseline RAM.

Curriculum Learning: During experiments, we adopt a
gradual training approach for the sake of accuracy. First, we
start with a base convolutional network (e.g. Residual Net-
works [28]) pre-trained on ImageNet [17]. We then ﬁne-
tune the base network on the ﬁne-grained dataset. This
gives us a very high baseline. Second, we train the RAM
model by gradually increase the total number of time steps.
Finally, we initialize DT-RAM with the trained RAM and
further ﬁne-tune the whole network with REINFORCE al-
gorithm.

4. Experiments

4.1. Dataset

We conduct experiments on three popular benchmark
datasets: MNIST [42], CUB-200-2011 [60] and Stanford
Cars [39]. Table 1 summarizes the details of each dataset.
MNIST contains 70,000 images with 10 digital numbers.
This is the dataset where the original visual attention model
tests its performance. However, images in MNIST dataset
are often too simple to generate conclusions to natural im-
ages. Therefore, we also compare on two challenging ﬁne-
grained recognition dataset. CUB-200-2011 [60] consists of
11,778 images with 200 bird categories. Stanford Cars [39]
includes 16,185 images of 196 car classes. Both datasets
contain a bounding box annotation for each image. CUB-
200-2011 also contains part annotation, which we do not
use in our algorithm. Most of the images in these two
datasets have cluttered background, hence visual attention
could be effective for them. All models are trained and
tested without ground truth bounding box annotations.

4.2. Implementation Details

MNIST: We use the original digital images with 28×28
pixel resolution. The digits are generally centered in the
image. We ﬁrst train multiple RAM models with up to 7
steps. At each time step, we crop a 8×8 patch from the
image based on the sampled attention location. The 8×8
patch only captures a part of a digit, hence the model usually
requires multiple steps to produce an accurate prediction.

The attention network, classiﬁcation network and stop-
ping network all output actions at every time step. The out-

put dimensions of the three networks are 2, 10 and 1 re-
spectively. All three networks are linear layers on top of
the recurrent network. The classiﬁcation network and stop-
ping network contain softmax layers to compute the discrete
probability. The attention network deﬁnes a Gaussian pol-
icy with a ﬁxed variance, representing the continuous dis-
tribution of the two location variables. The recurrent state
vector has 256 dimensions. All methods are trained using
stochastic gradient descent (SGD) with batch size of 20 and
momentum of 0.9. The reward at the last time step is 1 if
the agent classiﬁes correctly and 0 otherwise. The rewards
for all other time steps are 0. One can refer to [47] for more
training details.

CUB-200-2011 and Stanford Cars: We use the same
setting for both dataset. All images are ﬁrst normalized by
resizing to 512×512. We then crop a 224×224 image patch
at every time step except for the ﬁrst step, which is a key dif-
ference from MNIST. At the ﬁrst step, we use a 448×448
crop. This guarantees the 1-step RAM and 1-step DT-RAM
to have the same performance as the baseline convolutional
network. We use Residual Networks [28] pre-trained on Im-
ageNet [17] as the baseline network. We use the “pool-5”
feature as input to the recurrent hidden layer. The recurrent
layer is a fully connected layer with ReLU activations. The
attention network, classiﬁcation network and stopping net-
work are all linear layers on top of the recurrent network.
The dimensionality of the hidden layer is set to 512.

All models are trained using SGD with momentum of
0.9 for 90 epochs. The learning rate is set to 0.001 at the
beginning and multiplied by 0.1 every 30 epochs. The batch
size is 28 which is the maximum we can use for 512×512
resolution.
(For diagnostic experiments with smaller im-
ages, we use batch size of 96.) The reward strategy is the
same as MNIST. During testing, the actions are chosen to
be the maximal probability output from each network. Note
that although bounding boxs or part-level annotations are
available with these datasets, we do not utilize any of them
throughout the experiments.

Computational Time: Our implementation is based on
Torch [14]. The computational time heavily depends on
the resolution of the input image and the baseline network
structure. We run all our experiments on a single Tesla K-
40 GPU. The average running time for a ResNet-50 on a
512×512 resolution image is 42ms. A 3-step RAM is 77ms
since it runs ResNet-50 on 2 extra 224×224 images.

4.3. Comparison with State-of-the-Art

MNIST: We train two DT-RAM models with different
discount factors. We train DT-RAM-1 with a smaller dis-
count factor (0.98) and DT-RAM-2 with a larger discount
factor (0.99). The smaller discount factor will encourage
the model to stop early in order to obtain a large reward,
hence one can expect DT-RAM-1 stops with less number of

MNIST

# Steps Error(%)

CUB-200-2011

Accuracy(%) Acc w. Box(%)

FC, 2 layers (256 hiddens each)
Convolutional, 2 layers
RAM 2 steps
RAM 4 steps
RAM 5 steps
RAM 7 steps

DT-RAM-1 3.6 steps
DT-RAM-2 5.2 steps

-
-
2
4
5
7

3.6
5.2

1.69
1.21
3.79
1.54
1.34
1.07

1.46
1.12

Table 2. Comparison to related work on MNIST. All the RAM
results are from [47].

Zhang et al. [68]
Branson et al. [8]
Simon et al. [53]
Krause et al. [37]
Lin et al. [43]
Jaderberg et al. [33]
Kong et al. [36]
Liu et al. [46]
Liu et al. [45]

ResNet-50 [28]
RAM 3 steps
DT-RAM 1.9 steps

73.9
75.7
81.0
82.0
84.1
84.1
84.2
84.3
85.4

84.5
86.0
86.0

76.4
85.4∗
-
82.8
85.1
-
-
84.7
85.5

-
-
-

Table 3. Comparison to related work on CUB-200-2011 dataset. ∗
Testing with both ground truth box and parts.

of the existing works. Adding further recurrent visual atten-
tion (RAM) reaches 86.0%, improving the baseline Resid-
ual Net by 1.5%, leading to a new state-of-the-art on CUB-
200-2011. DT-RAM further improves RAM, by achieving
the same state-of-the-art performance, with less number of
computational time on average (1.9 steps v.s. 3 steps).

Stanford Cars: We also compare extensively on Stan-
ford Cars dataset. Table 4 shows the results. Surprisingly, a
ﬁne-tuned 50-layer Residual Network again achieves 92.3%
accuracy on the testing set, surpassing most of the existing
work. This suggests that a single deep network (without any
further modiﬁcation or extra bounding box supervision) can
be the ﬁrst choice for ﬁne-grained recognition.

A 3-step RAM on top of the Residual Net further im-
proves to 93.1% accuracy, which is by far the new state-
of-the-art performance on Stanford Cars dataset. Compared
to [46], this is achieved without using bounding box an-
notation during testing. DT-RAM again achieves the same
accuracy as RAM but using 1.9 steps on average. We also
observe that the relative improvement of RAM to the base-
line model is no longer large (0.8%).

4.4. Ablation Study

We conduct a set of ablation studies to understand how
each component affects RAM and DT-RAM on ﬁne-grained
recognition. We focus on CUB-200-2011 dataset since its
images are real and challenging. However, due to the large
resolution of the images, we are not able to run many steps
with a very deep Residual Net. Therefore, instead of using
ResNet-50 with 512×512 image resolution which produces
state-of-the-art results, we use ResNet-34 with 256×256 as
the baseline model (79.9%). This allows us to train more
steps on the bird images to deeply understand the model.

Figure 5. The distribution of number of steps from two different
DT-RAM models on MNIST dataset. A model with longer average
steps tends to have a better accuracy.

steps than DT-RAM-2.

Table 2 summarizes the performance of different mod-
els on MNIST. Comparing to RAM with similar number of
steps, DT-RAM achieves a better error rate. For example,
DT-RAM-1 gets 1.46% recognition error with an average of
3.6 steps while RAM with 4 steps gets 1.54% error. Sim-
ilarly, DT-RAM-2 gets 1.12% error with an average of 5.2
steps while RAM with 5 steps has 1.34% error.

Figure 5 shows the distribution of number of steps for
the two DT-RAM models across all testing examples. We
ﬁnd a clear trade-off between efﬁciency and accuracy. DT-
RAM-1 which has less computational time, achieves higher
error than DT-RAM-2.

CUB-200-2011: We compare our model with all pre-
viously published methods. Table 3 summarizes the re-
sults. We observe that methods including Bilinear CNN [43,
36] (84.1% - 84.2%), Spatial Transformer Networks [33]
(84.1%) and Fully Convolutional Attention Networks [46]
(84.3%) all achieve similar performances. [45] further im-
prove the testing accuracy to 85.4% by utilizing attribute
labels annotated in the dataset.

Surprisingly, we ﬁnd that a carefully ﬁne-tuned Residual
Network with 50 layers already hits 84.5%, surpassing most

Input Resolution and Network Depth: Table 5 com-
pares the effect on different image resolutions with differ-

Stanford Cars

Accuracy(%) Acc w. Box(%)

Model

# Steps Accuracy(%)

ResNet-34
RAM 2 steps
RAM 3 steps
RAM 4 steps
RAM 5 steps
RAM 6 steps

1
2
3
4
5
6

79.9
80.7
81.1
81.5
81.8
81.8

81.8

DT-RAM (6 max steps)

3.6

Table 6. Comparison to RAM on CUB-200-2011. Note that the
1-step RAM is the same as the ResNet.

Chai et al. [11]
Gosselin et al. [25]
Girshick et al. [24]
Lin et al. [43]
Wang et al. [62]
Liu et al. [46]
Krause et al. [37]

ResNet-50 [28]
RAM 3 steps
DT-RAM 1.9 steps

78.0
82.7
88.4
91.3
-
91.5
92.6

92.3
93.1
93.1

-
87.9
-
-
92.5
93.1
92.8

-
-
-

Table 4. Comparison to related work on Stanford Cars dataset.

Resolution ResNet-34 RAM-34 ResNet-50 RAM-50

224×224
448×448

79.9
-

81.8
-

81.5
84.5

82.8
86.0

Table 5. The effect of input resolution and network depth on
ResNet and its RAM extension.

ent network depths. In general, a higher image resolution
signiﬁcantly helps ﬁne-grained recognition. For example,
given the same ResNet-50 model, 512×512 resolution with
448×448 crops gets 84.5% accuracy, outperforming 81.5%
from 256×256 with 224×224 crops. This is probably be-
cause higher resolution images contain more detailed in-
formation for ﬁne-grained recognition. A deeper Resid-
ual Network also signiﬁcantly improves performance. For
example, ResNet-50 obtains 81.5% accuracy compared to
ResNet-34 with only 79.9%. During experiments, we also
train a ResNet-101 on 224×224 crops and get 82.8% recog-
nition accuracy.

RAM v.s. DT-RAM: Table 6 shows how the number
of steps affects RAM on CUB-200-2011 dataset. Starting
from ResNet-34 which is also the 1 step RAM, the model
gradually increases its accuracy with more steps (79.9% →
81.8%). After 5 steps, RAM no longer improves. DT-RAM
also reaches the same accuracy. However, it only uses 3.5
steps on average than 6 steps, which is a promising trade-off
between computation and performance.

Figure 6 plots the distribution of number of steps from
the DT-RAM on the testing images. Surprisingly different
from MNIST, the model prefers to stop either at the begin-
ning of the time (1-2 steps), or in the end of the time (5-6
steps). There are very few images that the model choose to
stop at 3-4 steps.

Learned Policy v.s. Fixed Policy: One may suspect that
instead of learning the optimal stopping policy, whether we
can just use a ﬁxed stopping policy on RAM to determine
when to stop the recurrent iteration. For example, one can
simply learn a RAM model with intermediate supervision

Figure 6. The distribution of number of steps from a DT-RAM
model on CUB-200-2011 dataset. Unlike MNIST, the distribution
suggests the model prefer either stop at the beginning or in the end.

Threshold

# Steps Accuracy(%)

0
0.4
0.5
0.6
0.9
1.0

1
1.4
1.6
1.9
3.6
6

3.6

79.9
80.7
81.0
81.2
81.3
81.8

81.8

DT-RAM (6 max steps)

Table 7. Comparison to a ﬁxed stopping policy on CUB-200-2011.
The ﬁxed stopping policy runs on RAM (6 steps) such that the
recurrent attention stops if one of the class softmax probabilities is
above the threshold.

at every time step and use a threshold over the classiﬁ-
cation network fc(ht, θc) to determine when to stop. We
compare the results between the described ﬁxed policy and
DT-RAM. Table 7 shows the comparison. We ﬁnd that al-
though the ﬁxed stopping policy gives reasonably good re-
sults (i.e. 3.6 steps with 81.3% accuracy), DT-RAM still
works slightly better (i.e. 3.6 steps with 81.8% accuracy).

Curriculum Learning: Table 8 compares the results
on whether using curriculum learning to train RAM. If we
learn the model parameters completely from scratch with-
out curriculum, the performance start to decrease with more

(a) 1 step

(b) 2 steps

(c) 3 steps

(d) 4 steps

(e) 5 steps

(f) 6 steps

Figure 7. Qualitative results of DT-RAM on CUB-200-2011 testing set. We show images with different ending steps from 1 to 6. Each
bounding box indicates an attention region. Bounding box colors are displayed in order. The ﬁrst step uses the full image as input hence
there is no bounding box. From step 1 to step 6, we observe a gradual increase of background clutter and recognition difﬁculty, matching
our hypothesis for using dynamic computation time for different types of images.

(b) 2 steps
Figure 8. Qualitative results of DT-RAM on Stanford Car testing set. We only manage to train a 3-step model with 512×512 resolution.

(c) 3 steps

(a) 1 step

# Steps

1

2

3

4

5

6

# Steps

1

2

3

4

5

6

w.o C.L. 79.9 80.7 80.5 80.9 80.3 80.0
79.9 80.7 81.1 81.5 81.8 81.8
w. C.L.

w.o I.S. 79.9 78.8 76.1 74.8 74.9 74.7
79.9 80.7 81.1 81.5 81.8 81.8
w. I.S.

Table 8. The effect of Curriculum Learning on RAM.

Table 9. The effect of Intermediate Supervision on RAM.

time steps (79.9%→80.9%→80.0%). This is because sim-
ple policy gradient method becomes harder to train with
longer sequences. Curriculum learning makes training more
stable, since it guarantees the accuracy on adding more
steps will not hurt the performance. The testing perfor-
mance hence gradually increases over number of steps,
from 79.9% to 81.8%.

Intermediate Supervision: Table 9 compares the test-
ing results for using intermediate supervision. Note that
the original RAM model [47] only computes output at the
last time step. Although this works for small dataset like
MNIST. when the input images become more challenging
and time step increases, the RAM model learned without
intermediate supervision starts to get worse. On contrary,
adding an intermediate loss at each step makes RAM mod-
els with more steps steadily improve the ﬁnal performance.
Qualitative Results: We visualize the qualitative results

of DT-RAM on CUB-200-2011 and Stanford Cars testing
set in Figure 7 and Figure 8 respectively. From step 1 to
step 6, we observe a gradual increase of background clutter
and recognition difﬁculty, matching our hypothesis of using
dynamic computation time for different types of images.

5. Conclusion and Future Work

In this work we present a novel method for learning
to dynamically adjust computational time during inference
with reinforcement learning. We apply it on the recurrent
visual attention model and show its effectiveness for ﬁne-
grained recognition. We believe that such methods will be
important for developing dynamic reasoning in deep learn-
ing and computer vision. Future work on developing more
sophisticated dynamic models and apply it to more complex
tasks such as visual question answering will be conducted.

References

[1] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recog-
nition with visual attention. arXiv preprint arXiv:1412.7755,
2014. 1, 2, 4

[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473, 2014. 2

[3] D. M. Beck and S. Kastner. Top-down and bottom-up mech-
anisms in biasing competition in the human brain. Vision
research, 49(10):1154–1165, 2009. 1

[4] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur-
riculum learning. In Proceedings of the 26th annual interna-
tional conference on machine learning, pages 41–48. ACM,
2009. 2

[5] T. Berg and P. Belhumeur. Poof: Part-based one-vs.-one fea-
tures for ﬁne-grained categorization, face veriﬁcation, and
attribute estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 955–
962, 2013. 3

[6] T. Berg, J. Liu, S. Woo Lee, M. L. Alexander, D. W. Jacobs,
and P. N. Belhumeur. Birdsnap: Large-scale ﬁne-grained vi-
sual categorization of birds. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
2011–2018, 2014. 3

[7] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101–
mining discriminative components with random forests. In
European Conference on Computer Vision, pages 446–461.
Springer, 2014. 3

[8] S. Branson, G. Van Horn, S. Belongie, and P. Perona. Bird
species categorization using pose normalized deep convolu-
tional nets. arXiv preprint arXiv:1406.2952, 2014. 3, 6
[9] C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang,
L. Wang, C. Huang, W. Xu, et al. Look and think twice: Cap-
turing top-down visual attention with feedback convolutional
neural networks. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2956–2964, 2015. 2
[10] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Hu-
man pose estimation with iterative error feedback. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 4733–4742, 2016. 2

[11] Y. Chai, V. Lempitsky, and A. Zisserman. Symbiotic seg-
mentation and part localization for ﬁne-grained categoriza-
tion. In Proceedings of the IEEE International Conference
on Computer Vision, pages 321–328, 2013. 7

[12] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Neva-
tia. Abc-cnn: An attention based convolutional neural
arXiv preprint
network for visual question answering.
arXiv:1511.05960, 2015. 2

[13] R. M. Cichy, D. Pantazis, and A. Oliva. Resolving human
object recognition in space and time. Nature neuroscience,
17(3):455–462, 2014. 1

[14] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In BigLearn,
NIPS Workshop, number EPFL-CONF-192376, 2011. 5
[15] Y. Cui, F. Zhou, Y. Lin, and S. Belongie. Fine-grained cate-
gorization and dataset bootstrapping using deep metric learn-
In Proceedings of the IEEE
ing with humans in the loop.

Conference on Computer Vision and Pattern Recognition,
pages 1153–1162, 2016. 3

[16] G. Deco and E. T. Rolls. A neurodynamical cortical model
of visual attention and invariant object recognition. Vision
research, 44(6):621–642, 2004. 1

[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 248–255. IEEE, 2009. 5

[18] R. Desimone. Visual attention mediated by biased competi-
tion in extrastriate visual cortex. Philosophical Transactions
of the Royal Society B: Biological Sciences, 353(1373):1245,
1998. 1

[19] R. Desimone and J. Duncan. Neural mechanisms of selective
visual attention. Annual review of neuroscience, 18(1):193–
222, 1995. 1

[20] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang,
Spatially adaptive
arXiv preprint

D. Vetrov, and R. Salakhutdinov.
computation time for residual networks.
arXiv:1612.02297, 2016. 2

[21] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016. 2

[22] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact
bilinear pooling. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 317–326,
2016. 3

[23] E. Gavves, B. Fernando, C. G. Snoek, A. W. Smeulders, and
T. Tuytelaars. Fine-grained categorization by alignments. In
Proceedings of the IEEE International Conference on Com-
puter Vision, pages 1713–1720, 2013. 3

[24] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
In Proceedings of the IEEE conference on
segmentation.
computer vision and pattern recognition, pages 580–587,
2014. 7

[25] P.-H. Gosselin, N. Murray, H. J´egou, and F. Perronnin. Re-
visiting the ﬁsher vector for ﬁne-grained classiﬁcation. Pat-
tern Recognition Letters, 49:92–98, 2014. 7

[26] A. Graves. Adaptive computation time for recurrent neural
networks. arXiv preprint arXiv:1603.08983, 2016. 1, 2
[27] M. Hayhoe and D. Ballard. Eye movements in natural be-
havior. Trends in cognitive sciences, 9(4):188–194, 2005. 1
[28] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
770–778, 2016. 2, 5, 6, 7

[29] P. Hu and D. Ramanan. Bottom-up and top-down reasoning
with hierarchical rectiﬁed gaussians. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5600–5609, 2016. 2

[30] S. Huang, Z. Xu, D. Tao, and Y. Zhang. Part-stacked cnn
for ﬁne-grained visual categorization. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1173–1182, 2016. 3

[31] L. Itti and C. Koch. Computational modelling of visual at-
tention. Nature reviews neuroscience, 2(3):194–203, 2001.
2

[32] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
visual attention for rapid scene analysis. IEEE Transactions
on pattern analysis and machine intelligence, 20(11):1254–
1259, 1998. 2

[33] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
In Advances in Neural Information

transformer networks.
Processing Systems, pages 2017–2025, 2015. 3, 6

[34] Y. Jernite, E. Grave, A. Joulin, and T. Mikolov. Variable
computation in recurrent neural networks. arXiv preprint
arXiv:1611.06188, 2016. 2

[35] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In Proc. CVPR Workshop on Fine-Grained Visual Catego-
rization (FGVC), volume 2, 2011. 3

[36] S. Kong and C. Fowlkes. Low-rank bilinear pooling for
ﬁne-grained classiﬁcation. arXiv preprint arXiv:1611.05109,
2016. 3, 6

[37] J. Krause, H. Jin, J. Yang, and L. Fei-Fei. Fine-grained
recognition without part annotations. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 5546–5555, 2015. 3, 6, 7

[38] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev,
T. Duerig, J. Philbin, and L. Fei-Fei. The unreasonable ef-
fectiveness of noisy data for ﬁne-grained recognition.
In
European Conference on Computer Vision, pages 301–320.
Springer, 2016. 3

[39] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object rep-
resentations for ﬁne-grained categorization. In Proceedings
of the IEEE International Conference on Computer Vision
Workshops, pages 554–561, 2013. 2, 3, 5

[40] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009. 2

[41] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, pages
1097–1105, 2012. 2

[42] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998. 2, 5

[43] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn mod-
els for ﬁne-grained visual recognition. In Proceedings of the
IEEE International Conference on Computer Vision, pages
1449–1457, 2015. 3, 6, 7

[44] J. Liu, A. Kanazawa, D. Jacobs, and P. Belhumeur. Dog
In European
breed classiﬁcation using part localization.
Conference on Computer Vision, pages 172–185. Springer,
2012. 3

[45] X. Liu, J. Wang, S. Wen, E. Ding, and Y. Lin. Localizing by
describing: Attribute-guided attention localization for ﬁne-
grained recognition. arXiv preprint arXiv:1605.06217, 2016.
1, 3, 6

[46] X. Liu, T. Xia, J. Wang, Y. Yang, F. Zhou, and Y. Lin. Fine-
grained recognition with automatic and efﬁcient part atten-
tion. arXiv preprint arXiv:1603.06765, 2016. 6, 7

[47] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of vi-
sual attention. In Advances in neural information processing
systems, pages 2204–2212, 2014. 1, 2, 4, 5, 6, 8
[48] M. Neumann, P. Stenetorp, and S. Riedel.
to reason with adaptive computation.
arXiv:1610.07647, 2016. 2

Learning
arXiv preprint

[49] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In European Conference
on Computer Vision, pages 483–499. Springer, 2016. 2
[50] M.-E. Nilsback and A. Zisserman. Automated ﬂower classi-
ﬁcation over a large number of classes. In Computer Vision,
Graphics & Image Processing, 2008. ICVGIP’08. Sixth In-
dian Conference on, pages 722–729. IEEE, 2008. 3

[51] A. Odena, D. Lawson, and C. Olah. Changing model behav-
ior at test-time using reinforcement learning. arXiv preprint
arXiv:1702.07780, 2017. 2

[52] P. Sermanet, A. Frome, and E. Real. Attention for ﬁne-
arXiv preprint arXiv:1412.7054,

grained categorization.
2014. 1, 2

[53] M. Simon and E. Rodner. Neural activation constellations:
Unsupervised part model discovery with convolutional net-
works. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1143–1151, 2015. 6

[54] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 2

[55] M. F. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber.
Deep networks with internal selective attention through feed-
back connections. In Advances in Neural Information Pro-
cessing Systems, pages 3545–3553, 2014. 2

[56] R. S. Sutton and A. G. Barto. Reinforcement learning: An
introduction, volume 1. MIT press Cambridge, 1998. 2
[57] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour,
et al. Policy gradient methods for reinforcement learning
In NIPS, volume 99, pages
with function approximation.
1057–1063, 1999. 2, 3

[58] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1–9, 2015. 2

[59] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. Lai, N. Davis,
and F. Nuﬂo. Modeling visual attention via selective tuning.
Artiﬁcial intelligence, 78(1-2):507–545, 1995. 2

[60] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. 2011. 2, 5
[61] Q. Wang, J. Zhang, S. Song, and Z. Zhang. Attentional neu-
ral network: Feature selection using cognitive feedback. In
Advances in Neural Information Processing Systems, pages
2033–2041, 2014. 2

[62] Y. Wang, J. Choi, V. Morariu, and L. S. Davis. Mining dis-
criminative triplets of patches for ﬁne-grained classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1163–1172, 2016. 7

[63] H. Xu and K. Saenko. Ask, attend and answer: Exploring
question-guided spatial attention for visual question answer-
In European Conference on Computer Vision, pages
ing.
451–466. Springer, 2016. 2

[64] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhut-
dinov, R. S. Zemel, and Y. Bengio. Show, attend and tell:
Neural image caption generation with visual attention.
In
ICML, volume 14, pages 77–81, 2015. 2

[65] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked
In Pro-
attention networks for image question answering.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 21–29, 2016. 2

[66] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2678–2687, 2016. 2

[67] A. R. Zamir, T.-L. Wu, L. Sun, W. Shen, J. Malik,
arXiv preprint

Feedback networks.

and S. Savarese.
arXiv:1612.09508, 2016. 2

[68] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-
In Eu-
based r-cnns for ﬁne-grained category detection.
ropean conference on computer vision, pages 834–849.
Springer, 2014. 6

