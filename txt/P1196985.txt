0
2
0
2
 
r
p
A
 
0
1
 
 
]

G
L
.
s
c
[
 
 
1
v
8
9
8
4
0
.
4
0
0
2
:
v
i
X
r
a

Secret Sharing based Secure Regressions with
Applications

Chaochao Chen1, Liang Li1, Wenjing Fang1, Jun Zhou1, Li Wang1,
Lei Wang1, Shuang Yang1, Alex Liu1, and Hao Wang2
{chaochao.ccc,liangli.ll,bean.fwj,jun.zhoujun,raymond.wangl}@antﬁn.com
{shensi.wl,shuang.yang,alexliu}@antﬁn.com, wanghao@sdnu.edu.cn

1Ant Financial Services Group, 2Shandong Normal University

Abstract. Nowadays, the utilization of the ever expanding amount of
data has made a huge impact on web technologies while also causing var-
ious types of security concerns. On one hand, potential gains are highly
anticipated if diﬀerent organizations could somehow collaboratively share
their data for technological improvements. On the other hand, data se-
curity concerns may arise for both data holders and data providers due
to commercial or sociological concerns. To make a balance between tech-
nical improvements and security limitations, we implement secure and
scalable protocols for multiple data holders to train linear regression and
logistic regression models. We build our protocols based on the secret
sharing scheme, which is scalable and eﬃcient in applications. Moreover,
our proposed paradigm can be generalized to any secure multiparty train-
ing scenarios where only matrix summation and matrix multiplications
are used. We demonstrate our approach by experiments which shows the
scalability and eﬃciency of our proposed protocols, and ﬁnally present
its real-world applications.

Keywords: Linear regression, logistic regression, shared machine learning, se-
cret sharing

1

Introduction

With the ever expanding collection of data, there have been increasing concerns
on the security and privacy issues when utilizing big data to facilitate technolog-
ical improvement. On the customer side, privacy concerns arise when individual
information is collected with potential risk of leakage [4], while on the collector
side, there are security concerns since the collectors are always willing to protect
their own resources including data.

Meanwhile, there is increasingly potential gains in terms of analytical power
if diﬀerent organizations could collaboratively combine their data assets for data
mining or information retrieval. For example, health data from diﬀerent hospitals
can be used together to facilitate more accurate diagnosis [17], while ﬁnancial
companies can collaborate to train more eﬀective fraud-detection engines [28].

2

Chaochao Chen et al.

The major task is to combine data from multiple entities so as to improve
model training performance while still protecting data securities for individual
holders from any possibility of information disclosure, i.e., shared machine learn-
ing in literature [25,19,28,3,5]. The security and privacy concern, together with
the desire of data combination poses an important challenge for both academia
and industry.

To derive an application-oriented solution to the above challenges, we design
secure multiparty protocols based on the classical secret sharing scheme, which
is eﬃcient in both computational and communicational costs. We restrict our
attention to the simple yet widely used regression models in industry, i.e., lin-
ear regression and logistic regression, and consider two scenarios (vertical and
horizontal) of data partitioning among data holders.

1.1 Our Contribution

We summarize the main contributions as follows.

– We make a computational reduction from the multiparty training of regression
models to secure multiparty matrix summation and secure two party matrix
multiplications. This paradigm is quite diﬀerent from previous work in this
area [20] since for the gradient descent step in Equation (3), although the
calculation of subtraction can be implemented easily via secret sharing, the
matrix multiplications can not be easily generalized to arbitrary number of
parties and so their training protocols are not applicable to multiparty cases.
Our paradigm takes the secure two-party matrix multiplication protocol as a
complete ‘black-box’, which means any secure matrix product protocols can
be “embedded” into our paradigm to train regression models. By reducing the
matrix multiplications for multiple data holders into matrix multiplications for
two parties, we can even generalize our paradigm to any training process other
than linear regression and logistic regression. As long as the training process
involves only summation (subtraction) and multiplication, our method can be
applied. We also remark that for some special functions like sigmoid functions,
we can apply polynomial approximations to transform the special functions
into summation and multiplication.

– We conduct empirical studies on our proposed method and experimentally
verify that our protocols achieve exactly the same performance while the ad-
ditional overhead for the computation and communication costs scale linearly
with data size. This demonstrates that our proposed method can be applied
into large scale datasets.

1.2 Related Work

Secure Multi-Party Computation (MPC) was initiated in [26], which aims to
generate methods (or protocals) for multi-parties to jointly compute a function
(e.g., vector multiplication) over their inputs (e.g., vector for each party) while

Secret Sharing based Secure Regressions with Applications

3

keeping those inputs private. Diﬀerent schemes can be used to design MPC pro-
tocols, such as garbled circuits [27] and secret sharing [24]. MPC makes the
secure collaboration between diﬀerent data holders possible, and has been ap-
plied to implement many secure machine learning algorithms, such as decision
tree [18], linear regression [22,11,20,15,13], logistic regression [8,20,16], neural
networks [20,28], and recommender system [3,5].

The most similar work to ours is SecureML [20], however, we are diﬀer-
ent from them in mainly three aspects. First, since the SGD update step in
[20] is fully secret-shared for each matrix operation in Equation (3), secure ma-
trix product cannot be easily generalized to multiple parties. Second, SecureML
adopts the techniques of arithmatic sharing, boolean sharing, and Yao sharing
for secure machine learning. Although it provides fruitful protocols for academic
researchers to build any machine learning models, it also increases the diﬃculty
for industrial practitioners deploy customized models. Thirdly, our protocol un-
der vertically data partition setting has less communication complexity, since we
only need to secretly share models and labels instead of features. These are the
main diﬀerences between the training protocol in [20] and ours.

2 Preliminaries

2.1 Linear Regression and Logistic Regression

Linear Regression. Linear gression is popularly used in industry due to its
simply yet robust ability. Given m data samples xi ∈ Rd each with d dimensional
input features and output labels yi ∈ R, regression is a statistical method to
learn a ﬁtting function f : Rd → R such that f (xi) ≈ yi for all i ∈ [m]. Note
that, without loss of generality, we use italic text (e.g., yi) for variables, lower-
case bold letters (e.g., w) for vectors, and upper-case bold letters (e.g., X) for
matrices. In linear regression, we assume that f can be represented as a linear
combination of the input features, i.e., f (xi) = (cid:80)d
j=1 xijwj = xi · w, where xij
is the jth feature of sample xi and wj is the jth coeﬃcient of w. The value
ˆyi = f (xi) is called the predictive value on xi. To learn the coeﬃcient vector w,
square loss is commonly used:

Lli(w) =

(ˆyi − yi)2,

1
2

m
(cid:88)

i=1

(1)

which measures the total diﬀerence between the predictive values and the true
labels. The objective is to minimize this loss function, which is also known as
the least squares estimation.
Logistic Regression. Logistic regression is also widely used for binary clas-
siﬁcation tasks in practice, in which each data sample xi ∈ Rd has an output
label y ∈ {0, 1}. The ﬁtting function f , also called the activation function in this
setting, can be represented as the logistic function of the weighted combination
of the input features: f (xi) =

1
1+e−xi·w .

4

Chaochao Chen et al.

To train the coeﬃcients w of the model, an information-theoretic loss func-

tion called cross entropy is used:

Llo(w) = −

(yi log ˆyi + (1 − yi) log(1 − ˆyi)) ,

(2)

m
(cid:88)

i=1

where ˆyi = f (xi) is the predictive value of data sample xi. The cross entropy
measures the total distributional diﬀerence between the predictive values and
the true labels over all data samples and the objective of logistic regression is to
minimize it. Note that we omit the regularization terms for conciseness.

2.2 Mini-Batch Stochastic Gradient Descent

Mini-batch stochastic gradient descent (SGD) is an eﬃcient iterative algorithm
for optimizing a global function. In linear regression and logistic regression, the
goal is to ﬁnd a best-ﬁt coeﬃcient vector w which minimizes of the loss function.
Let XB (resp. yB) be the |B| × d (resp. |B| × 1) submatrices of X (resp. y),
where the row indices are all from batch B. For linear regression and logistic
regression, the update equation for batch SGD is as follows:

w := w −

XT

B × ( ˆyB − yB),

α
|B|

(3)

where α is the learning rate which controls the moving magnitude.

2.3 Secure Multiparty Computation

Suppose n parties individually hold private data x1, x2, · · · , xn and g is a mul-
tivariate function with n variables over arbitrary domain (e.g. boolean, inte-
ger, or real). Secure multiparty computation aims at designing protocols for
the n parties to collaboratively compute g(x1, x2, · · · , xn) such that at the end
of the protocol, each party gets the value g(x1, x2, · · · , xn) without being able
to explorer any information of other party’s private data. Speciﬁcally, secure
multi-party computation is a multi-party random process. Take two parties
for example, it maps pairs of inputs (one from each party) to pairs of out-
puts (one for each party), while preserving several security properties, such as
correctness, privacy, and independence of inputs [14]. This random process is
called functionality. Formally, denote a two-output functionality f = (f1, f2) as
f : {0, 1}∗ × {0, 1}∗ → {0, 1}∗ × {0, 1}∗. For a pair of inputs (x, y), where x is
from party P1 and y is from party P2, the output pair (f1(x, y), f2(x, y)) is a
random variable. f1(x, y) is the output for P1, and f2(x, y) is for P2. During this
process, neither party should learn anything more than its prescribed output.

There are fruitful methods such as garbled circuit and secret sharing for
designing secure protocols, among which the secret sharing scheme is the most
eﬃcient one in practice. The very basic idea of secret sharing is that each data
holder divide its own data into completely random shares and distribute the

Secret Sharing based Secure Regressions with Applications

5

shares among diﬀerent parities. In running the protocol, each party do local
computations based on its shares coming from other parties and the ﬁnal result
can be recovered via global communication. In this paper, we use (cid:104)x(cid:105)i to denote
the i-th share of a secret x, which can be generalized to vectors and matrices.
One should note that secret sharing only works in ﬁnite ﬁeld, since random
shares should be generated in uniform distribution to guarantee security. We
use ﬁx-point approximation following the existing research [20].
Security Model. In this paper, we consider the standard semi-honest model,
where a probabilistic polynomial-time adversary with semi-honest behaviors is
considered. Take two parties for example, in this security model, the adversary
may corrupt and control one party (referred as to the corrupted party), and try to
obtain information about the input of the other party (referred as to the honest
party). During the protocol execution with the honest party, the adversary will
follow the protocol speciﬁcation, but may attempt to obtain additional infor-
mation about the honest party’s input by analyzing the corrupted party’s view,
i.e., the transcripts it receives during the protocol execution. In order to ensure
correctness and privacy, the following formal security deﬁnition is proposed [12].

Deﬁnition 1 (Security in semi-honest model).

Let f = (f1, f2) be a deterministic functionality and π be a two-party protocol
for computing f . Given the security parameter κ, and a pair of inputs (x, y)
(where x is from P1 and y is from P2), the view of Pi (i = 1, 2) in the protocol
π is denoted as viewπ
i), where w ∈ {x, y}, ri is the
randomness used by Pi, and mj
i is the j-th message received by Pi; the output
of Pi is denoted as outputπ
i (x, y, κ), and the joint output of the two parties is
outputπ(x, y, κ) = (outputπ
1 (x, y, κ)), outputπ
2 (x, y, κ)). We say that π securely
computes f in semi-honest model if

i (x, y, κ) = (w, ri, m1

i , · · · , mt

– There exist probabilistic polynomial-time simulators S1 and S2, such that

{S1(1κ, x, f1(x, y))}x,y,κ

∼= {viewπ

1 (x, y, κ)}x,y,κ,

{S2(1κ, y, f2(x, y))}x,y,κ

∼= {viewπ

2 (x, y, κ)}x,y,κ.

– The joint output and the functionality output satisfy

{outputπ(x, y, κ)}x,y,κ

∼= {f (x, y)}x,y,κ,

where x, y ∈ {0, 1}∗, and ∼= denotes computationally indistinguishablity.

3 Secret Sharing based Secure Multiparty Regressions

3.1 Scenarios

We mainly consider two scenarios of data partitioning, as is shown in Figure . In
the ﬁrst scenario, data are partitioned horizontally among multiple parties, which
means each party holds a subset of data samples and the data in diﬀerent parities

6

Chaochao Chen et al.

(a) Data partitioned horizontally

(b) Data partitioned vertically

Fig. 1. Data partition scenarios.

have the same feature dimensions. In the second scenario, data are partitioned
vertically, where each party holds a subset of features over all samples and the
sample indices of diﬀerent parties have already been aligned.

For notational convenience in describing the protocol, we use Ai to denote
the i-th party involved in the protocol, where i ∈ [n] and n is the total number of
all parties. Using secret sharing, we use {(cid:104)X(cid:105)i}i∈[n] to denote the set of shares of
X such that X = (cid:80)n
i=1 (cid:104)X(cid:105)i. Here X could be the original data, or secret-shares,
or intermediate data during the algorithm procedure. (cid:104)X(cid:105)i is called the i-share
of data X, which is the fraction of data X distributed to Ai.

To securely train linear regression and logistic regression models, the crucial
point is to design a protocol for multiple parties such that the SGD step in
Equation (3) can be conducted securely among diﬀerent parties. We will describe
our secure protocols for both scenarios in the following sections.
Threat Model. Simialr as the existing researches [20], we use the semi-honest
(passive) adversary model, where the participants stricly follow the protocol,
but may try to infer additional information from the middle messages during
the protocol execution. Comparing with malicious (active) adversary model,
semi-honest adversary model enables the development of highly eﬃcient secure
computation protocols and has been widely used to develop secure machine
learning applications [8].

3.2 Protocols For Horizontally Partitioned Data

For secure linear regression and logistic regression models, the general idea is to
locally sample a batch from one party in each iteration and do the computation
in Equation (3) via secret sharing. The crucial point for calculations in Equa-
tion (3) is that every arithmetical operation (including matrix subtraction and
multiplication) has to be secret-shared so that none of the other parties can get
any information from the current party who generates the batch.
Secure Linear Regression Protocol. We ﬁrst present the secure protocol for
linear regresion under horizontally partitioned data in Algorithm 1. As the data
are horizontally partitioned among parties, we have

XT =

yT =

(cid:16)(cid:0)X1(cid:1)T
(cid:16)(cid:0)y1(cid:1)T

, (cid:0)X2(cid:1)T

, (cid:0)y2(cid:1)T

, ..., (Xn)T (cid:17)
, ..., (yn)T (cid:17)

,

,

Secret Sharing based Secure Regressions with Applications

7

Algorithm 1: Linear regression protocol for horizontally partitioned data

Input: Feature matrices Xi and label vectors yi for all party Ai(i ∈ [n]),

learning rate (α), and maximum iterations (T )

Output: Coeﬃcient vectors wi such that w = (cid:80)n
coeﬃcient vector for the regression model

i=1 wi where w is the

Ai initializes wi

1 for i = 1 to n in parallel do
2
3 end
4 for t = 1 to T do
5

n parties randomly select i ∈ [n] via protocol
Ai locally samples batches Xi
B and yi
B
(cid:111)
Ai locally generates
and
(cid:110)(cid:10)Xi

(cid:110)(cid:10)yi
(cid:111)

(cid:110)(cid:10)Xi
(cid:111)
(cid:11)
j

(cid:11)
j

j∈[n]

B

B

and

(cid:110)(cid:10)yi
× wi as i-share

(cid:11)
j

B

j(cid:54)=i

Ai distributes
Ai locally calculates (cid:10)Xi
for j (cid:54)= i do

B

j(cid:54)=i
(cid:11)

B

i

Aj locally calculates (cid:10)Xi
Ai and Aj calculate i-share (cid:10)(cid:10)Xi
via SMM protocol

(cid:11)

B

j

Ai and Aj calculate i-share
(cid:68)(cid:10)Xi

× wi

(cid:69)

(cid:11)

B

j

j

via SMM protocol

(cid:68)(cid:10)Xi

B

(cid:11)

j

(cid:11)
i

(cid:69)

i

end
for j = 1 to n in parallel do

(cid:111)

(cid:11)

j

j∈[n]

to other parties

× wj as j-share

(cid:11)

i

B

× wj

and j-share (cid:10)(cid:10)Xi

(cid:11)
i

B

× wj

(cid:11)

j

× wi

and j-share

B × w(cid:11)

Aj locally calculates the summation of all j-shares, denoted as
(cid:10)Xi
Aj locally calculates errj = (cid:10)Xi
Aj clears its j-shares to zero value

B × w(cid:11)

− (cid:10)yi

(cid:11)
j

B

j

j

end
for j = 1 to n in paralle do
(cid:11)T
Aj locally calculates (cid:10)Xi
j
for k = 1 to n and k (cid:54)= j do

B

× errj as j-share

Aj and Ak calculates j-share
(cid:68)(cid:10)Xi

× errj

(cid:69)

B

(cid:11)T
k

k

via SMM protocol

(cid:68)(cid:10)Xi

B

(cid:11)T
k

× errj

and k-share

(cid:69)

j

end

end
for j = 1 to n in parallel do

Aj locally calculates the summation of all j-shares, denoted as gradj
Aj locally updates wj by wj ← wj − α

|B| · gradj

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

end

29
30 end

8

Chaochao Chen et al.

where XT denotes the transpose of matrix X, each sample in the feature matrix
X is a row vector and the label vector y is a column vector.

The for loops with ‘in parallel’ means that the n parties should do the
steps within the loop in parallel. SMM is short for Secure Matrix Multiplication.
Notice that when we call a SMM protocol, we assume that the parties each
has one matrix whose dimensions are aligned to calculate the matrix product.
Existing SMM protocols can be used as a black-box procedure in Line 12, 13 and
23. Line 5 is a protocol that determines whose data are selected to update model
in the current batch, e.g., this can be simply sequential protocol that indicates
all the participants’ data are used to train the regression model sequentially.
SMM protocols. SMM protocol makes sure that each of the parties (1) holds
a secret share matrix such that the summation of the shares equals the matrix
product, and (2) only gets the output, i.e., the secret share matrix, but not other
middle information. To date, diﬀerent SMM protocols have been proposed, they
can be divided into two types, i.e., SMM with trusted initializer [7] and SMM
without trusted initializer [29]. The main diﬀerence is that the former needs
a trusted third-party to generate Beaver triples for the participates before the
protocols starts, while the later one bypass the trusted initializer by sacriﬁcing
some security guarantee.
Secure Logistic Regression Protocol. One can slightly modify Algorithm
1 to change it from linear regression to logistic regression. The only diﬀerence
1+e−xi·w . For logistic
is the calculation of predictive values, i.e., from xi · w to
regression, to compute the logistic function using secret sharing, we approximate
it by a k-order polynomial

1

1
1 + e−z ≈

k
(cid:88)

j=0

qjzj.

(4)

Existing researches proposed diﬀerent coeﬃcients (qj) for logistic function
[1,6]. Here, we follow the 3-order polynomial in [1], i.e., q0 = 0.5, q1 = 0.197,
q2 = 0, and q3 = 0.004. With this in mind, the logistic function can be easily
approximated by using SMM protocol. We omit the details for conciseness.

3.3 Protocol For Vertically Partitioned Data

Secure Linear Regression Protocol. We ﬁrst summarize the secure protocol
for linear regression in Algorithm 2. Since the data matrix is vertically parti-
tioned, we only need to secretly share models (Line 4) and labels (Line 7) among
participants. Each participant calculates shares of the prediction by using SMM
protocol (Lines 8-12), and During model training, each participant gets a share
of the prediction (Line 15), error (Line 16), and gradients (Line 26), updates
the shared models (Line 27), and ﬁnally reconstruct their corresponding models
(Line 35). Similar as the protocol in horizontally partitioned data in Algorithm
1, Algorithm 2 also works for any number of participants.

Secret Sharing based Secure Regressions with Applications

9

Algorithm 2: Linear regression protocol for vertically partitioned data
Input: Feature matrices Xi for party Ai(i ∈ [n]), label vector y located in
party Ak, learning rate (α), and maximum iterations (T )

Output: Coeﬃcient vectors wi such that wT =

(cid:16)

(w1)T , (w2)T , · · · , (wn)T (cid:17)

where w is the coeﬃcient vector for the regression model

1 All n parties agreed on a sequence of batches B1, B2, · · · , BT
2 for i = 1 to n do
3

Ai initializes wi
Ai generates shares {(cid:104)wi(cid:105)j}j∈[n] and distributes

(cid:104)wi(cid:105)j

(cid:110)

4

(cid:111)

to others

j(cid:54)=i

5 end
6 for t = 1 to T do

Ak generates shares

and distributes

(cid:110)(cid:10)yBt

(cid:11)

j

(cid:111)

j∈[n]

(cid:110)(cid:10)yBt

(cid:11)

j

(cid:111)

j(cid:54)=k

to others

for i = 1 to n do

Ai locally calculates Xi
for j = 1 to n and j (cid:54)= i do

Bt × (cid:104)wi(cid:105)i as i-share

Ai and Aj calculate i-share

(cid:68)

Xi

Bt × (cid:104)wi(cid:105)j

(cid:69)

j

via SMM protocol

(cid:68)

Xi

Bt × (cid:104)wi(cid:105)j

(cid:69)

i

and j-share

end

end
for j = 1 to n in parallel do

end
for i = 1 to n do

Aj locally calculates the sum of all j-shares, denoted as (cid:104)XBt × w(cid:105)j
Aj locally calculates (cid:104)err(cid:105)j = (cid:104)XBt × w(cid:105)j − (cid:10)yBt

(cid:11)

j

Ai locally calculates (cid:104)err(cid:105)T
for j = 1 to n and j (cid:54)= i do

i × Xi

Bt as i-share of gradi

Ai and Aj calculate i-share (cid:10)(cid:104)err(cid:105)t
(cid:10)(cid:104)err(cid:105)t

i × Xj
Bt

(cid:11)
j

of gradj via SMM protocol

i × Xj
Bt

(cid:11)

i

and j-share

end

end
for i = 1 to n in parallel do

for j = 1 to n do

Ai locally calculates the summation of all j-shares of gradi,
denoted as (cid:104)gradi(cid:105)j
Ai locally updates (cid:104)wi(cid:105)j by (cid:104)wi(cid:105)j ← (cid:104)wi(cid:105)j − α

|B| · (cid:104)gradi(cid:105)j

end

end

29
30 end
31 for i = 1 to n do
32

for j = 1 to n and j (cid:54)= i do
Aj sends (cid:104)wi(cid:105)j to Ai

end
Ai locally calculates the summation of {(cid:104)wi(cid:105)j}j∈[n], denoted as wi

35
36 end

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

33

34

10

Chaochao Chen et al.

Secure Logistic Regression Protocol. Similar as the secure logistic regres-
sion protocol under horizontally partitioned data, one can use a k-order polyno-
mial to approximate the logistic function. After it, the polynomial can be easily
calculated using SMM protocol.
Security Proof.

Theorem 1. Algorithm 2 is secure against semi-honest adversaries, as in Def-
inition 1.

Proof. We skip the correctness proof of Algorithm 2 considering it is obvious.
To proof its security, for concise purpose, we use two parties as examples, i.e.,
A1 and A2, A1 has X1 and B has X2 and y. We construct two simulators SA1
and SA2, such that

{SA1 (1κ, X1, w1)}X1,X2,y,κ
{SB(1κ, X2, y, w2)}X1,X2,y,κ

∼= {viewA1 (X1, X2, y, κ)}X1,X2,y,κ,
∼= {viewB(X1, X2, y, κ)}X1,X2,y,κ,

(5)

(6)

where viewA1 and viewA2 denotes the views of A1 and A2, respectively. We
prove the above equations for a corrupted A1 and a corrupted A2, respectively.

Corrupted A1. In this case, we construct a probabilistic polynomial-time simula-
tor SA1 that, when given the security parameter κ, A1’s input X1 and output w1,
can simulate the view of A1 in the protocol execution. To this end, we ﬁrst ana-
lyze A1’s view viewA1(X1, X2, y, κ) in Algorithm 2. The messages obtained by A1
are consisted of three parts. The ﬁrst part is the messages sent before the train-
ing process, i.e., (cid:104)y1(cid:105), (cid:104)w2(cid:105)1; the second part is the messages sent in the training
process, which are from SMM protocols, i.e., (cid:10)X2
2,
Bt
(cid:68)
(cid:68)
(cid:104)err(cid:105)t
; the third part is the message sent af-
2
ter the training process, i.e., (cid:104)w1(cid:105)2. Therefore, viewA1 (X1, X2, y, κ) consists
of A1’s input X1, the shares (cid:104)y(cid:105)1, (cid:104)w2(cid:105)1, (cid:10)X2
2,
Bt
(cid:68)
(cid:104)err(cid:105)t
, and (cid:104)w1(cid:105)2.
Given κ, X1, and w1, SA1 generates a simulation of viewA1(X1, X2, y, κ) as

2, (cid:10)X1
(cid:11)

2 × X1
Bt

1 × X2
Bt

2 × X1
Bt

1 × X2
Bt

2, (cid:10)X1

× (cid:104)w1(cid:105)2

× (cid:104)w2(cid:105)1

× (cid:104)w1(cid:105)2

× (cid:104)w2(cid:105)1

(cid:104)err(cid:105)t

(cid:104)err(cid:105)t

, and

Bt

Bt

(cid:69)

(cid:69)

(cid:69)

(cid:68)

(cid:69)

(cid:11)

(cid:11)

(cid:11)

2

2

2

,

the following steps.

– SA1 randomly selects shares (cid:104)y(cid:48)(cid:105)1 and (cid:104)w(cid:48)
– SA1 simulates the SMM protocol, randomly generates (cid:10)X(cid:48)2

2(cid:105)1.

2, (cid:10)X(cid:48)1
(cid:11)
Bt
Bt
, and takes them as the output for A1

× (cid:104)w(cid:48)

2(cid:105)1

× (cid:104)w(cid:48)

1(cid:105)2

(cid:11)

2,

(cid:69)

(cid:68)
(cid:104)err(cid:48)(cid:105)t

2 × X(cid:48)1
Bt

(cid:69)

2

(cid:68)

1 × X(cid:48)2
Bt

(cid:104)err(cid:48)(cid:105)t
,
2
in SMM protocol.
– SA1 computes (cid:104)w(cid:48)

results in previous steps.

1(cid:105)2 following Line 27 in Algorithm 2 using the simulated

– SA1 generates a simulation of viewA1(X1, X2, y, κ) by outputting (X1, (cid:104)y(cid:48)(cid:105)1,
(cid:69)
2 × X(cid:48)1
1(cid:105)2
Bt

(cid:68)
(cid:104)err(cid:48)(cid:105)t

(cid:68)
(cid:104)err(cid:48)(cid:105)t

1 × X(cid:48)2
Bt

× (cid:104)w(cid:48)

× (cid:104)w(cid:48)

2, (cid:10)X(cid:48)1

2(cid:105)1

2,

Bt

Bt

(cid:69)

(cid:11)

(cid:11)

,

2

,

2

(cid:104)w(cid:48)
(cid:104)w(cid:48)

2(cid:105)1, (cid:10)X(cid:48)2
1(cid:105)2).

Secret Sharing based Secure Regressions with Applications

11

Table 1. Dataset statistics.

Dataset

Number of feature
Number of sample

News

61
39,797

Blog

Bank APS

180
52,397

17

171

45,211 60,000

Therefore, we have the following two equations:

(cid:69)

viewA1(X1, X2, y, κ)
= (X1, (cid:104)y(cid:105)1, (cid:104)w2(cid:105)1, (cid:10)X2
Bt
(cid:68)
(cid:104)err(cid:105)t

2 × X1
Bt
2
SA1(X1, X2, y, κ)
2(cid:105)1, (cid:10)X(cid:48)2
= (X1, (cid:104)y(cid:48)(cid:105)1, (cid:104)w(cid:48)
(cid:69)
(cid:68)
(cid:104)err(cid:48)(cid:105)t

, (cid:104)w(cid:48)

2 × X(cid:48)1
Bt

, (cid:104)w1(cid:105)2),

1(cid:105)2).

2

× (cid:104)w2(cid:105)1

(cid:11)

2 , (cid:10)X1

Bt

× (cid:104)w1(cid:105)2

(cid:68)

(cid:11)

2 ,

(cid:104)err(cid:105)t

1 × X2
Bt

(cid:69)

,

2

× (cid:104)w(cid:48)

2(cid:105)1

Bt

(cid:11)

2 , (cid:10)X(cid:48)1

Bt

× (cid:104)w(cid:48)

1(cid:105)2

(cid:11)

2 ,

(cid:104)err(cid:48)(cid:105)t

1 × X(cid:48)2
Bt

(cid:68)

(cid:69)

,

2

We note that the probability distributions of A1’s view and SA1 ’s output
are computationally indistinguishable. This completes the proof in the case of
corrupted A1.

Corrupted A2. In this case, we construct a probabilistic polynomial-time simula-
tor SA2 that, when given the security parameter κ, A2’s input X2, y and output
w2, can simulate the view of A2 in the protocol execution. Following the proof
of Corrupted A1, one can simply proof that the probability distributions of A2’s
view and SA2’s output are computationally indistinguishable. This completes
the proof in the case of corrupted A2.

The above proof can be can be extended to multiple parties. Similarly, one

can also prove that Algorithm 1 is secure against semi-honest adversaries.
Discussion. Before applying our proposed Algorithm 2 in practice, the batches
in each SGD iteration must be aligned for the parties. This is why we need the
step in Line 1. The alignment operation can be done eﬃciently by using private
set intersection [23], which can match the samples in diﬀerent datasets and keep
the secure of these data at the same time.

4 Experiments and Applications

4.1 Dataset and Data Split Description

Dataset description. We use four public dataset to perform experiments, i.e.,
online news popularity dataset (News for short) [10], BlogFeedback dataset (Blog
for short) [2], Bank Marketing dataset (Bank for short) [21], and APS Failure
dataset (APS for short) [9]. The ﬁrst two datasets are for linear regression task

12

Chaochao Chen et al.

Table 2. RMSE and running time of (secure) linear regressions on News dataset.

Model LiRe Sec-LiRe-TI-V Sec-LiRe-OTI-V Sec-LiRe-TI-H Sec-LiRe-OTI-H

RMSE 0.0096
Time 32.41

0.0096
70.28

0.0096
246.31

0.0096
78.52

0.0096
274.11

and the last two datasets are for logistic regression task. We summarize their
statistics in Table 1. Note that for all the datasets, we normalize all the features
and labels so that they are robust to regression tasks.
Data split. For simpliﬁcation, we only assume there are two parties. For Hor-
izontally data split setting, we assume the two parties have the same number
of samples. For vertically data split setting, we assume the two parties have the
same number of features. Note that, our protocols are suitable for any number
parties in practice.

4.2 Experimental Settings

Evaluation metric. We use Root Mean Square Error (RSME) to evaluate the
performance of (secure) linear regression models, and choose Area Under the
ROC Curve (AUC) to evaluate the performance of (secure) logistic regression
models.
Comparison methods. We propose secure linear regression and logistic re-
gression protocols for both Horizontally (H) and Vertically (V) partitioned data.
Therefore, we compare with plaintext linear regression (LiRe) and logistic re-
gression (LoRe) to study (1) whether they have the same accuracy, and (2)
what is the diﬀerence of their running time. Besides, we apply two SMM proto-
cols, i.e., with Trusted Initializer (TI) and withOut TI (OTI), for our proposed
secure regressions. Thus, for ablation study, we use Sec-LiRe-TI-H/Sec-LiRe-
OTI-H to denote secure linear regression model with/without trusted initializer
under horizontally partitioned data, and use Sec-LiRe-TI-V/Sec-LiRe-OTI-V to
denote secure linear regression model with/without trusted initializer under ver-
tically partitioned data. Similarly, we have Sec-LoRe-TI-H, Sec-LoRe-OTI-H,
Sec-LoRe-TI-V, and Sec-LoRe-OTI-V for logistic regression.
Parameter setting. For all the (secret sharing based) models, we set the batch
size to B = 5 and the number of iteration to 100, which means that we use
mini-batch gradient descent to train the model. We also search the learning rate
α in {0.001, 0.01, 0.1} to ﬁnd its best values.

4.3 Comparison Results

We use ﬁve-fold cross validation during comparison, and report the average re-
sults. We summarize the comparison results, including RMSE/AUC and running
time (in seconds) in Tables 2-5. Note that we omit the oﬄine Beaver triple gener-
ation time for the trusted-initializer based methods and use local area network.
From them, we observe that:

Secret Sharing based Secure Regressions with Applications

13

Table 3. RMSE and running time of (secure) linear regressions on Blog dataset.

Model LiRe Sec-LiRe-TI-V Sec-LiRe-OTI-V Sec-LiRe-TI-H Sec-LiRe-OTI-H

RMSE 0.0125
Time 126.27

0.0125
240.82

0.0125
628.98

0.0125
264.21

0.0125
692.72

Table 4. AUC and running time of (secure) logistic regressions on Bank dataset.

Model LoRe Sec-LoRe-TI-V Sec-LoRe-OTI-V Sec-LoRe-TI-H Sec-LoRe-OTI-H

AUC 0.7849
Time 47.99

0.7792
170.89

0.7792
318.99

0.7792
206.12

0.7792
387.83

– Our proposed secure linear regression protocols have exactly the same perfor-
mance with plaintext ones, and secure logistic regression protocols also have
comparable performance with plaintext ones. This is because we use a 3-order
polynomial to approximate the logistic function.

– The computation time of our proposed secure linear regression and logistic
regression models are slower than plaintext ones, especially for the secure
logistic regression models. This is because it needs more rounds of SMM pro-
tocols for secure logistic regression to calculate the 3-order polynomial. Take
Sec-LoRe-OTI-H on News dataset for example, Sec-LoRe-OTI-H takes 9.06
times longer than plaintext LoRe, which is acceptable considering its ability
of protecting data privacy.

4.4 Time Complexity Analysis

We now study the time complexity of our proposed secure regression protocols,
and report the results in Figure 2, where we use the same setting as in compari-
son. From it, we can see that with the increase of data size, the running time of
our protocols scale linearly. This results demonstrate that our proposed model
can be applied into large scale dataset.

4.5 Applications

Our proposed secure regression models have been successfully deployed in vari-
ous tasks inside and outside Ant Financial, including intelligent marketing, risk
control, and intelligent lending. For example, CDFinance1, a bank in China,
together with Ant Financial deployed secure logistic regression models, which
not only signiﬁcantly improved its risk control ability, but also transformed the
traditional oﬄine lending mode into an online automatic lending mode.

5 Conclusion and Future Work

In this paper, we ﬁrst made a computational reduction from the multiparty train-
ing of regression models to secure multiparty matrix summation multiplication.

1 http://www.cdﬁnance.com.cn/en/index

14

Chaochao Chen et al.

Table 5. AUC and running time of (secure) logistic regressions on APS dataset.

Model LoRe Sec-LoRe-TI-V Sec-LoRe-OTI-V Sec-LoRe-TI-H Sec-LoRe-OTI-H

AUC 0.9807
Time 151.72

0.9749
607.09

0.9749
1138.82

0.9749
722.16

0.9749
1374.45

Fig. 2. Running time (in seconds) with diﬀerent data size.

Based on secret sharing schemes, we then proposed two secure regression algo-
rithms for horizontally and vertically partitioned data respectively. We ﬁnally
demonstrated the eﬀectiveness and eﬃciency of our approach by experiments,
and presented the real-world applications. In future, we would like to apply our
proposed protocols into more machine learning algorithms and deploy them into
more applications.

References

1. Aono, Y., Hayashi, T., Trieu Phong, L., Wang, L.: Scalable and secure logistic
regression via homomorphic encryption. In: Proceedings of the Sixth ACM Con-
ference on Data and Application Security and Privacy. pp. 142–144. ACM (2016)
2. Buza, K.: Feedback prediction for blogs. In: Data analysis, machine learning and

knowledge discovery, pp. 145–152. Springer (2014)

3. Chen, C., Li, L., Wu, B., Hong, C., Wang, L., Zhou, J.: Secure social recommen-

dation based on secret sharing. arXiv preprint arXiv:2002.02088 (2020)

4. Chen, C., Liu, Z., Zhao, P., Zhou, J., Li, X.: Privacy preserving point-of-interest
recommendation using decentralized matrix factorization. In: Thirty-Second AAAI
Conference on Artiﬁcial Intelligence (2018)

5. Chen, C., Wu, B., Fang, W., Zhou, J., Wang, L., Qi, Y., Zheng, X.: Practical
privacy preserving poi recommendation. arXiv preprint arXiv:2003.02834 (2020)
6. Chen, H., Gilad-Bachrach, R., Han, K., Huang, Z., Jalali, A., Laine, K., Lauter,
K.: Logistic regression over encrypted data from fully homomorphic encryption.
BMC medical genomics 11(4), 81 (2018)

7. De Cock, M., Dowsley, R., Horst, C., Katti, R., Nascimento, A., Poon, W.S., Truex,
S.: Eﬃcient and private scoring of decision trees, support vector machines and
logistic regression models based on pre-computation. TDSC (2017)

8. Demmler, D., Schneider, T., Zohner, M.: Aby-a framework for eﬃcient mixed-

protocol secure two-party computation. In: NDSS (2015)

Secret Sharing based Secure Regressions with Applications

15

9. Dua, D., Graﬀ, C.: UCI machine learning repository (2017), http://archive.ics.

uci.edu/ml

10. Fernandes, K., Vinagre, P., Cortez, P.: A proactive intelligent decision support
system for predicting the popularity of online news. In: Portuguese Conference on
Artiﬁcial Intelligence. pp. 535–546. Springer (2015)

11. Gasc´on, A., Schoppmann, P., Balle, B., Raykova, M., Doerner, J., Zahur, S.,
Evans, D.: Privacy-preserving distributed linear regression on high-dimensional
data. PoPETs 2017(4), 345–364 (2017)

12. Goldreich, O.: Foundations of cryptography: volume 2, basic applications. Cam-

bridge university press (2009)

13. Hall, R., Fienberg, S.E., Nardi, Y.: Secure multiple linear regression based on

homomorphic encryption. Journal of Oﬃcial Statistics 27(4), 669 (2011)

14. Hazay, C., Lindell, Y.: Eﬃcient secure two-party protocols: Techniques and con-

structions. Springer Science & Business Media (2010)

15. Karr, A.F.: Secure statistical analysis of distributed databases, emphasizing what
we don’t know. Journal of Privacy and Conﬁdentiality 1(2), 197–211 (2010)
16. Kim, M., Song, Y., Wang, S., Xia, Y., Jiang, X.: Secure logistic regression based on
homomorphic encryption: Design and evaluation. JMIR medical informatics 6(2)
(2018)

17. Li, T., Sahu, A.K., Talwalkar, A., Smith, V.: Federated learning: Challenges, meth-

ods, and future directions. arXiv preprint arXiv:1908.07873 (2019)

18. Lindell, Y.: Secure multiparty computation for privacy preserving data mining. In:
Encyclopedia of Data Warehousing and Mining, pp. 1005–1009. IGI Global (2005)
19. Liu, Y., Chen, C., Zheng, L., Wang, L., Zhou, J., Liu, G.: Privacy preserving pca

for multiparty modeling. arXiv preprint arXiv:2002.02091 (2020)

20. Mohassel, P., Zhang, Y.: Secureml: A system for scalable privacy-preserving ma-

chine learning. In: IEEE S&P. pp. 19–38 (2017)

21. Moro, S., Cortez, P., Rita, P.: A data-driven approach to predict the success of

bank telemarketing. Decision Support Systems 62, 22–31 (2014)

22. Nikolaenko, V., Weinsberg, U., Ioannidis, S., Joye, M., Boneh, D., Taft, N.: Privacy-
preserving ridge regression on hundreds of millions of records. In: IEEE S&P. pp.
334–348 (2013)

23. Pinkas, B., Schneider, T., Zohner, M.: Faster private set intersection based on

{OT} extension. In: {USENIX} Security. pp. 797–812 (2014)

24. Shamir, A.: How to share a secret. Communications of the ACM 22(11), 612–613

25. Wu, B., Zhao, S., Chen, C., Xu, H., Wang, L., Zhang, X., Sun, G., Zhou, J.:
Generalization in generative adversarial networks: A novel perspective from privacy
protection. In: Advances in Neural Information Processing Systems. pp. 306–316
(2019)

26. Yao, A.C.: Protocols for secure computations. In: FOCS. pp. 160–164. IEEE (1982)
27. Yao, A.C.C.: How to generate and exchange secrets. In: FOCS. pp. 162–167. IEEE

(1979)

(1986)

28. Zheng, L., Chen, C., Liu, Y., Wu, B., Wu, X., Wang, L., Wang, L., Zhou, J.,
Yang, S.: Industrial scale privacy preserving deep neural network. arXiv preprint
arXiv:2003.05198 (2020)

29. Zhu, Y., Takagi, T.: Eﬃcient scalar product protocol and its privacy-preserving

application. IJESDF 7(1), 1–19 (2015)

