0
2
0
2
 
r
p
A
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
4
8
6
2
0
.
4
0
0
2
:
v
i
X
r
a

Attribute Mix: Semantic Data Augmentation for
Fine-grained Recognition

Hao Li1(cid:63), Xiaopeng Zhang2, Hongkai Xiong1, and Qi Tian2

1 Shanghai Jiaotong University
2 Huawei Noah’s Ark Lab

Abstract. Collecting ﬁne-grained labels usually requires expert-level
domain knowledge and is prohibitive to scale up. In this paper, we pro-
pose Attribute Mix, a data augmentation strategy at attribute level to
expand the ﬁne-grained samples. The principle lies in that attribute fea-
tures are shared among ﬁne-grained sub-categories, and can be seam-
lessly transferred among images. Toward this goal, we propose an auto-
matic attribute mining approach to discover attributes that belong to the
same super-category, and Attribute Mix is operated by mixing seman-
tically meaningful attribute features from two images. Attribute Mix is
a simple but eﬀective data augmentation strategy that can signiﬁcantly
improve the recognition performance without increasing the inference
budgets. Furthermore, since attributes can be shared among images from
the same super-category, we further enrich the training samples with at-
tribute level labels using images from the generic domain. Experiments
on widely used ﬁne-grained benchmarks demonstrate the eﬀectiveness
of our proposed method. Speciﬁcally, without any bells and whistles, we
achieve accuracies of 90.2%, 93.1% and 94.9% on CUB-200-2011, FGVC-
Aircraft and Standford Cars, respectively.

Keywords: Fine-grained Recognition, Attribute Augmentation, Semi-
supervised Learning

1 Introduction

Fine-grained recognition aims at discriminating sub-categories that belong to the
same general category, i.e., recognizing diﬀerent kinds of birds [38,2], dogs [19],
and cars [22] etc.. Diﬀerent from general category recognition, ﬁne-grained sub-
categories often share the same parts (e.g., all birds should have wings, legs,
heads, etc.), and usually can only be distinguished by the subtle diﬀerences in
texture and color properties of these parts (e.g., only the breast color counts
when discriminating some similar birds). Although the advances of Convolu-
tional Neural Networks (CNNs) have fueled remarkable progress for general im-
age recognition [18,32,17,39], ﬁne-grained recognition still remains to be chal-
lenging where discriminative details are too subtle to discern.

Existing deep learning based ﬁne-grained recognition approaches usually fo-
cus on developing better models for part localization and representation. Typ-
ical strategies include: 1) part based methods that ﬁrst localize parts, crop

(cid:63) This work was done when the ﬁrst author was an intern at Huawei Noah’s Ark Lab.

2

Hao Li et al.

(a) Image xa

(b) Image xb

(c) Ma,1 (cid:12) xa

(d) Mb,1 (cid:12) xb

(e) Mixup

(f) CutMix

(g) A-M

(h) A-M

Fig. 1. Data augmentation via Mixup, CutMix and Attribute Mix. (a)(b) are samples
xa and xb drawn at random from CUB-200-2011. (c)(d) are two mined discriminative
attribute regions from xa and xb, respectively. (e)(f) are virtual samples generated by
Mixup and CutMix. (g)(h) are two virtual samples generated by Attribute Mix.

and amplify the attended parts, and concatenate part features for recogni-
tion [24,43,4,44]; 2) attention based methods that use visual attention mechanism
to ﬁnd the most discriminative regions of the ﬁne-grained images [40,13,47]; 3)
feature based methods such as bilinear pooling [25] or trilinear pooling [49] for
better representation. However, we argue that for ﬁne-grained recognition, the
most critical challenge arises from the limited training samples, since collect-
ing labels for ﬁne-grained samples often requires expert-level domain knowledge,
which is diﬃcult to extend to large scale. As a result, existing deep models are
easy to overﬁt to the small scale training data, and this is especially true when
deploying with more complex modules.

In this paper, we promote ﬁne-grained recognition via enriching the train-
ing samples at low cost, and propose a simple but eﬀective data augmenta-
tion method to alleviate the overﬁtting and improve model generalization. Our
method, termed as Attribute Mix, aims at enlarging the training data substan-
tially via mixing semantically meaningful attribute features from two images.
The motivation is that attribute level features are the key success factors to dis-
criminate diﬀerent sub-categories, and are transferable since all sub-categories
share the same attributes. Toward this goal, we propose an automatic attribute
learning approach to discover attribute features. This is achieved by training a
multi-hot attribute level deep classiﬁcation network through iteratively masking
out the most discriminative parts, hoping that the network can focus on diverse
parts of an object. The new generated images, accompanied with attribute labels
mixed proportionally to the extent that two attributes fuse, can greatly enrich
the training samples while maintaining the discriminative semantic meanings,
and thus greatly alleviate the overﬁtting issue and are beneﬁcial to improve
model generalization.

Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition

3

Attribute Mix shares similarity with MixUp [42] and CutMix [41], which all
mix two samples by interpolating both images and labels. The diﬀerence is that
both MixUp and CutMix randomly mix images or patches from two images,
without considering their semantic meanings. As a result, it is usual for these
two mixing operations to fuse images with non-discriminative regions, which in
turn introduces noise and makes the model unstable for training. An example is
shown in Fig.1. In comparison, Attribute Mix intentionally fuses two images at
the attribute level, which results in more semantically meaningful images, and
is helpful for improving model generalization.

Beneﬁting from the discovered attributes, we are able to introduce more
training samples at attribute level with only generic labels. Here we denote the
generic labels as single bit supervision indicating whether an object is from
a general category, e.g., whether an object is a bird or not. We claim that
the attribute features can be seamlessly transferred from the generic domain to
the ﬁne-grained domain without knowing the object’s ﬁne-grained sub-category
labels. This is achieved by a standard semi-supervised learning strategy that
mines samples at the attribute levels. The mined samples, intrinsically with
mixed attribute labels, and we term this proposed method which mines attribute
from the general domain as Attribute Mix+. Note that although ﬁne-grained
labels are diﬃcult to obtain, it is much easier to obtain a general label of an
object. In this way, we are able to conveniently scale the ﬁne-grained training
samples via mining attributes from the generic domain for better performance
investigation.

Our proposed data augmentation strategy is a general framework at low
cost, and can be combined with most state-of-the-art ﬁne-grained recognition
methods to further improve the performance. Experiments conducted on several
widely used ﬁne-grained benchmarks have demonstrated the eﬀectiveness of our
proposed method. In particular, without any bells and whistles, using ResNet
101 [17] as backbone, we achieve accuracies of 90.2%, 93.1% and 94.9% on CUB-
200-2011, FGVC-Aircraft and Standford Cars, respectively, which surpass the
corresponding baselines by a large margin, while not sacriﬁcing the speed com-
paring with the baseline. We hope that our research on attribute-based data
augmentation could oﬀer useful guidelines for ﬁne-grained recognition.

To sum up, this paper makes the following contributions:

• We propose Attribute Mix, a data augmentation strategy to alleviate the

overﬁtting for ﬁne-grained recognition.

• We propose Attribute Mix+, which mines ﬁne-grained samples at attribute
level, and does not need to know the speciﬁc sub-category labels of the mined
samples. Attribute Mix+ is able to scale up the ﬁne-grained training for better
performance investigation conveniently.

• We evaluate our methods on three challenging datasets (CUB-200-2011,
FGVC-Aircraft, Standford Cars), and achieve superior performance over the
state-of-the-art methods.

4

Hao Li et al.

2 Related Work

We brieﬂy review some works for ﬁne-grained recognition, as well as some recent
technologies in data augmentation, which are most related with our work.

2.1 Fine-grained Recognition

Fine-grained recognition has been studied for several years. Early works on ﬁne-
grained recognition focus on leveraging extra parts and bounding box annota-
tions to localize the discriminative regions of an object [4,43,44,24]. Later, some
[1,33,20,29,50,31,45] are proposed to
weakly supervised localization methods
localize objects with only image level annotations. In [50], Zhou et al. proposed
to localize the objects by picking out the class-speciﬁc feature maps. In [45],
Zhang et al. proposed to use adversarial training to locate the integral object
and achieved superior localization results.

On the other hand, powerful features have been provided by better CNN
networks. Lin et al. [25] proposed a bilinear structure to compute the pairwise
feature interactions by two independent CNNs. And it turns out that higher-
order features interaction can make the features highly discriminative [8]. To
model the subtle diﬀerences between two ﬁne-grained sub-categories, attention
mechanism [40,47,13] and metric learning [30,7] are often used. Besides, Zhang
et al. [46] proposed to unify CNN with spatially weighted representation by
Fisher Vectors, which achieves high performances on CUB-200-2011. Although
promising performance has been achieved, these methods are all at the expense
of higher computational cost, and are prohibitive to deploy on low-end devices.
Few works rely on external data to help facilitate recognition. Cui et al. [6]
proposed to use Earth Mover’s Distance to estimate the domain similarity, and
transfer the knowledge from the source domain which is similar to the target
domain. In [21], Krause et al. collected millions of images with tags from Web
and utilized the Web data by transfer learning. However, both methods make
use of a large amount of class-speciﬁc labels for transfer learning. In this paper,
we demonstrate that for ﬁne-grained recognition, transferring attribute features
is a powerful proxy to improve the recognition accuracy at low cost, without
knowing the class-speciﬁc labels of the source domain.

2.2 Data Augmentation

Data augmentation can greatly alleviate overﬁtting in training deep networks.
Simple transformations such as horizontal ﬂipping, color space augmentations,
and random cropping are widely used in recognition tasks to improve generaliza-
tion. Recently, automatic data augmentation techniques, e.g., AutoAugment [5],
are proposed to search for a better augmentation strategy among a large pool
of candidates. Diﬀerently, Mixup [42] combined two samples linearly in pixel
level, where the target of the synthetic image was the linear combination of
one-hot label. Though not meaningful for human perception, Mixup has been
demonstrated surprisingly eﬀective for the recognition task. Following Mixup,

Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition

5

Fig. 2. The framework of the proposed Attribute Mix data augmentation. First, an
automatic attribute learning approach is proposed to mine attribute features from a
small scale ﬁne-grained samples, implemented by training a multi-hot attribute level
deep classiﬁcation network through iteratively masking out the most discriminative
parts. These attributes information can be used to generate new samples by Attribute
Mix and shared between images from the same generic domain at attribute level.

there are a few variants [37,16] as well as a recent eﬀort named Cutmix [41],
which combined Mixup and Cutout [9] by cutting and pasting patches. How-
ever, all these methods would inevitably introduce unreasonable noise due to
augmentation operations on random patches of an image, without considering
their semantic meanings. Our method is similar to these methods in mixing two
samples for data augmentation. Diﬀerently, the mixing operation is only per-
formed around those semantic meaningful regions, which enables the model for
more stable training and is beneﬁcial for generalization.

3 Approach

In this section, we describe our proposed attribute-based data augmentation
strategy in detail. As shown in Fig. 2, the core ingredients of the proposed
method consist of three modules: 1) Automatic attribute mining, which aims
at discovering attributes with only image level labels. 2) Attribute Mix data
augmentation, which mixes attribute features from any two images for new im-
age generation. 3) Attribute Mix+, which enriches training samples via mining
images from the same generic domain at attribute level.

3.1 Attribute Mining

The attribute level features are the core of the following data augmentation op-
eration. We ﬁrst elaborate how to obtain attribute level features with only image
level labels. Denote {x, y}, y ∈ {0, 1}C as a training image and its corresponding
one-hot label with yc = 1, where C is the number of ﬁne-grained sub-categories.

6

Hao Li et al.

Fig. 3. All ﬁne-grained sub-categories from the same generic domain often share the
same parts. The one-hot label is extended to multi-hot label from C dimension to k ∗ C
dimension, where each sub-category is endowed with k attributes. For simplicity, we
order the attributes belonging to the same sub-category at adjacent locations.

i=1 yA

Without loss of generality, assuming that all ﬁne-grained sub-categories share k
attributes, we simply convert the C class level labels to more detailed, k × C
attribute level labels for attribute discovery, as shown in Fig. 3. Speciﬁcally, the
one-hot label y of image x is extended to multi-hot label yA, while (cid:80)C
i=1 yi = 1
and (cid:80)kC
i = k, with each none zero hot regarded as one attribute corre-
sponding to a speciﬁc sub-category. As shown in Fig. 2, for a typical CNN, we
simply remove all the fully connected layers, and add a 1 × 1 convolutional layer
to produce feature maps f ∈ Rh×w×kC with kC channels, here every adjacent
k channels correspond to k attributes for a certain sub-category. These feature
maps are fed into a GAP (Global Average Pooling) layer to aggregate attribute
level features for classiﬁcation. The multiple attribute mining is proceeded as
follows:

1. Training multi-hot attribute level classiﬁcation network with original images

and attribute level multi-hot labels yA

ck:(c+1)k−1 = 1.

2. For an image with label yA

c , picking out the corresponding feature map at
the kcth channel f (:, :, kc), generating the attention map according to the
activations and upsampling to the original image size, thus we obtain the
most discriminative region mask Mc,1 of an image.

3. Mc,1 is used to erase the original image x to get erased image xMc,1, and the

corresponding multi-hot label, yA

ck changes from 1 to 0.

4. Using xMc,1 as a new training sample, and do the above three steps to obtain

masks Mc,i, i=2,...,k for all remained attributes.

Following the above procedures, we are able to train an attribute level clas-
siﬁcation network automatically and obtain a series of masks Mc, which cor-
respond to diﬀerent attributes of an object. Fig. 4 shows an example of what
the Attribute Mining procedure learns. It can be shown that these attributes
coarsely correspond to diﬀerent parts of an object, e.g., the birds’ heads, wings,
and tails.

Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition

7

(a) Raw image

(b) Part-1

(c) Part-2

(d) Part-3

(e) Raw image

(f) Part-1

(g) Part-2

(h) Part-3

Fig. 4. Illustration of multiple attribute mining. Example images show the mined at-
tributes when k = 3. These attributes approximately correspond to birds’ heads, wings
and tails, respectively.

3.2 Attribute Mix

After obtaining the attribute regions, we introduce a simple data augmenta-
tion strategy on attribute level to facilitate ﬁne-grained training. The proposed
Attribute Mix operation constructs synthetic examples by intentionally mixing
the corresponding attribute regions from any two images. Speciﬁcally, given two
b ) with xa, xb ∈ RW ×H×3, and random picked at-
samples (xa, yA
tribute masks Ma,k, Mb,k ∈ {0, 1}W ×H , the generated training sample ((cid:101)x, (cid:101)y) is
obtained by:

a ) and (xb, yA

(cid:101)x = (1 − (cid:102)Mb,k) (cid:12) xa + λ (cid:102)Mb,k (cid:12) xa + (1 − λ) (cid:102)Mb,k (cid:12) xb
(cid:101)y = λyA

a + (1 − λ)yA
b ,

(1)

where (cid:102)Mb,k is the transformed binary mask from xb to xa, with the mask
center-aligned with Ma,k, and denotes the region that needs to be mixed. 1 is
a binary mask ﬁlled with ones, and (cid:12) is the element-wise multiplication opera-
tion. Like Mixup [42], the combination ratio λ between two regions is sampled
from the beta distribution Beta(α, α). In all our experiments, we set α = 1,
meaning that λ is sampled from the uniform distribution (0, 1). An illustration
of the Attribute Mix operation is shown in Fig.1 , as well as some comparison
results that generated by Mixup and CutMix operations. Compared with Mixup
and CutMix which inevitably introduce some meaningless samples (e.g. ran-
dom patches, background noise), Attribute Mix only focuses on the foreground
attribute regions and is more suitable for ﬁne-grained categorization.

Though more semantically meaningful our Attribute Mix operation is, the
generated virtual samples still suﬀer large domain gap with the original, natural

8

Hao Li et al.

images. As a result, memorizing these samples would deteriorates the model
generalization. To address this issue, we introduce a time decay learning strategy
to limit the probability of applying Attribute Mix operation. This is achieved
by controlling the mixing ratio λ in Eq. (1), i.e., we introduce a variable γ(t),
which increases from 0 to 1 as the training proceeds, and limit γ(t) ≤ λ ≤ 1. In
this way, λ is sampled from Beta(α, α) distribution, and only when λ is larger
than γ(t), the generated samples are used for training. As the training process
goes on, the mixed operations between two images decay, and ﬁnally degenerate
to using the original images. In the experimental section, we will validate its
eﬀectiveness in improving model generalization.

3.3 Attribute Mix+

In principle, the attribute features are shared among images from the same
super-category, regardless of their speciﬁc sub-category labels. This conveniently
enables us to expand the training samples at attribute level to images from the
generic domain. In this section, we enrich the training samples in another view,
i.e., transfer attributes from images with only generic labels. This is achieved
by generating soft, attribute level labels via a standard semi-supervised learning
strategy over a large amount of images with low cost, general labels. Since at-
tributes are shared among all the sub-categories, it is not necessary for images
from the generic domain that belong to the same sub-categories with the target
domain.

Using the model trained with Attribute Mix strategy, we conduct inference
over the images from the generic domain, and produce attribute level probabili-
ties by using a softmax layer. Speciﬁcally, denoting the model output as z ∈ RkC,
we reshape the output to (cid:101)z ∈ Rk×C, where each row corresponds to the same
attribute among diﬀerent sub-categories. The softmax operation is conducted
over the row dimension to obtain the probability p ∈ Rk×C:

pi,j =

exp((cid:101)zi,j/T ))
c=1 exp((cid:101)zi,c/T ))

,

(cid:80)C

(2)

where T is a temperature parameter that controls the smooth degree of the

probability.

Entropy ranking. In many semi-supervised learning methods [15,3,23,27],
it is a common underlying assumption that classiﬁers’ decision boundary should
not pass through high-density regions of the marginal data distribution. Simi-
larly, in our experiment, the collected data with generic labels inevitably contain
noise that would probably hurt the performance, and some of them are with too
smooth soft-labels that can not provide any valuable attribute information. To
address this issue, we propose an entropy ranking strategy to select images adap-
tively. Speciﬁcally, given an image xg from the generic domain with soft attribute
label p, its entropy is calculated as follows:

H(xg) = −

pi,clog2pi,c,

(3)

k
(cid:88)

C
(cid:88)

i=1

c=1

Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition

9

where pi,c denotes the probability that image xg contains attribute i of ﬁne-
grained sub-category c. H(xg) is large if the attribute distribution is smooth,
and reaches its maximum when all attributes obtain the same probability, which
we think carry no valuable information for ﬁne-grained training. Based on this
property, we set the criteria that samples with entropy H(xg) that higher than
threshold τ to be ﬁltered out. In the ablation study, we will validate its eﬀec-
tiveness for achieving stable results, especially when noisy images exist.

4 Experiments

4.1 Datasets and Implementation Details

Datasets. The empirical evaluation is performed on three widely used ﬁne-
grained benchmarks: Caltech-USCD Birds-200-2011 [38], Standford Cars [22],
and FGVC-Aircraft [26], which belong to three generic domains: birds, cars and
aircraft, respectively. Each dataset is endowed with speciﬁc statistic proper-
ties, which are crucial for recognition performance. CUB-200-2011 is the most
widely used ﬁne-grained dataset, which contains 11,788 images spanning 200
sub-species. Standford Cars consists of 16,185 images with 196 classes, which
are produced by diﬀerent manufacturers, while FGVC-Aircraft consists of 10,000
images with 100 species. In current view, these are all small scale datasets. We
use the default training/test split for experiments, which gives us around 30
training examples per class for birds, 40 training examples per category for cars,
and approximately 66 training examples per category for aircraft.

Implementation details. In our implementation, all experiments are based
on ResNet-101 [17] backbone. We ﬁrst construct our baseline model over three
datasets for following comparisons. During network training, the input images
are randomly cropped to 448 × 448 pixels after being resized to 512 × 512 pixels
and randomly ﬂipped. For Standford Cars, color jittering is used for extra data
augmentation. We train the models for 80 epochs, using Stochastic Gradient
Descent (SGD) with the momentum of 0.9, weight decay of 0.0001. The learning
rate is set to 0.001, which decays by a factor of 10 every 30 epochs. During
inference, the original image is center cropped and resized to 448 × 448. Bene-
ﬁting from the powerful features, our baseline models have achieved considerable
pleasing performance over these datasets. In the following, we will validate the
eﬀectiveness of the proposed method, even over such high baselines.

For multi-hot attribute level classiﬁcation, a standard binary cross-entropy
loss is used. We increase the training epochs to 300 after introducing Attribute
Mixed samples, since it needs more rounds to converge for these augmented data.
When mining attributes from the generic domain, we use the model pretrained
with Attribute Mix to inference over data from the generic domains to produce
soft attribute level labels. During inference, the multi-hot attribute level classi-
ﬁcation network outputs the predicted scores of k ∗ C attributes, and we simply
combine the predicted scores for k attributes of each sub-category.

10

Hao Li et al.

Fig. 5. The left plot shows the performances with diﬀerent choices of α, and the right
plot presents the performances of diﬀerent threshold τ for entropy ranking.

4.2 Collecting Data From Generic Domain

To mine the attribute information from the generic domain, we need to col-
lect extra data with generic labels. Corresponding to the target ﬁne-grained
datasets, we collect data from three generic domains, i.e., birds, aircraft, and
cars. We search the corresponding category data via some keywords from Flickr
1. Speciﬁcally, for birds, we simply choose keywords from 200 sub-categories of
CUB-200-2011 and 555 sub-categories of Nabirds [35], and make use of the 755
keywords to crawl images from the Web. For simplicity, we do not do keywords
or image ﬁltering procedure as our method is robust to the label noise to some
extent. As a result, we obtain about 131K images from 755 sub-categories. For
aircraft, we assemble a list of 400 species from Wikipedia, which contain ”Airbus
A-320”, ”Beriev a 40”, ”bede bd 5” etc., and obtain around 88K images. For
cars, we combine the 196 species from Standford Cars and 1, 716 species from
CompCars together to get a list of 1, 912 species as keywords for data collection,
resulting in around 17K images. We only retain the generic labels such as ”bird”,
”car” and ”aircraft”, and discard those speciﬁc labels used for crawling for the
following experiments. Note that these images are directly crawled from the Web
and we do not conduct any ﬁltering procedure, which inevitably contain noisy
images that do not belong to these three generic domains.

Cross-dataset redundancy. One concern when training using auxiliary
datasets is that there might be redundancy between the crawled data and the
test set. Even though we do not have the ﬁne-grained labels on the auxiliary
dataset, we still conduct a duplicate check that quantiﬁes the extent to which
the test images are contained in our crawled images. Following [14], we choose
GIST [28] descriptor matching, which has been shown to have excellent perfor-
mance at near-duplicate image detection in large image collections. We remove
those images that are probably used for test, and the remained number of images
for birds, cars and aircraft are 131, 662, 171, 616 and 88, 523, respectively.

1 Flickr : https://www.ﬂickr.com

Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition

11

Table 1. Performances comparison for diﬀerent k on Attributes Mining.

Dataset

CUB-200-2011

Number of attributes k
k = 1
k = 2
k = 3
k = 4

Acc.
85.8
86.6
87.2
86.3

Table 2. Impact of adaptive Attribute Mix on CUB-200-2011 top-1 Acc.

Dataset

CUB-200-2011

Adaptive
(cid:88)

Acc.
88.5
88.0

4.3 Ablation Study

In this section, we investigate some parameters which are important for recog-
nition performance, as well as some in-depth analysis over the robustness of the
image mining at attribute level. Unless otherwise speciﬁed, all experiments are
conducted on CUB-200-2011.

Eﬀects of number of attributes k. Here we inspect the recognition per-
formance w.r.t. the number of attributes k during Attribute Mining. The per-
formances for diﬀerent choices of k are shown in Table 1. If we set k too small,
the model cannot mine adequate attributes of an object and the improvement is
marginal. Speciﬁcally, k = 1 denotes the baseline without multiple attribute min-
ing. While for larger k, the model is at the risk of including background clutters,
and making the optimization diﬃcult. We achieve the best performance when
k = 3. In the following, we keep this parameter ﬁxed for all the experiments for
simplicity.

Impact of hyperparameter α. The hyperparameter α in Eq. (1) plays
an important role during mixing, which controls the strength of interpolation
between attributes from two training samples. Here we try diﬀerent choices with
α ∈ {0.25, 0.5, 1, 2, 4}. The performances of diﬀerent α are shown in the left plot
of Fig. 5, and the best performance can be achieved when α is set to 1. For
simplicity, we keep this parameter ﬁxed for all the following experiments when
needed.

Eﬀects of adaptive Attribute Mix. We introduce the time decay strat-
egy to alleviate the overﬁtting over the augmented samples. The probability of
applying Attribute Mix decays from 1 to 0 as the cosine curve. In order to
validate the eﬀectiveness of this strategy, we keep all hyperparameters constant
and train the model using the Attribute Mix without time decay strategy. The
comparison on CUB-200-2011 is shown in Table 2. It shows that Attribute Mix
with the time decay learning strategy achieves higher accuracy of 88.5%, which
surpasses Attribute Mix without that strategy by 0.5 points.

12

Hao Li et al.

Table 3. Comparison between attribute level and image level sample mining.

Dataset

CUB-200-2011

Methods
image level
attribute level

Acc.
88.3
89.6

Comparison with image level image mining. In Section 3.3, the images
from the generic domain are used to mine attribute level features. In order to
validate the advantages of attribute level features, we compare with the tradi-
tional, semi-supervised learning using only image level labels. Speciﬁcally, the
image level pseudo labels directly leverage the information from unlabeled data
DU L without considering the attribute information. For fair comparison, both
methods use ResNet-101 model and the Attribute Mix is not applied during
training. We use the model’s prediction on attribute level and image level sep-
arately to generate the soft-labels over those images from the generic domain.
As shown in Table 3, our proposed attributes level features achieve much higher
accuracy of 89.6%, which surpasses the image level result 88.3% by 1.3 points.
It is not a small gain considering the high baseline we used and the diﬃculty of
CUB-200-2011 dataset.

Eﬀects of τ for entropy ranking. When leverage the data from generic
domain, we introduce the entropy ranking to select samples that share attributes
contributing most for the ﬁne-grained recognition. The entropy ranking mech-
anism investigates the correlations between the mined images from generic do-
main and ﬁne-grained domain, and is robust to noisy images that probably do
not belong to the same generic domain. In order to inspect the eﬀectiveness of
entropy ranking at length, we intentionally introduce some extra images with
labels diﬀerent from the generic labels, and test the performance of our method
under such a situation. Speciﬁcally, we choose external dataset PASCAL VOC
2007 [12] and 2012 [11] as noisy images, which both contain 20 object classes.
This is a dataset with multi-label images, and the number of samples that in-
clude birds is 1,377, only a small ratio (around 6.4%) over the whole dataset.
Overall, this dataset can be treated as adding noisy images (around 16.3%) to
the generic domain dataset. We evaluate our method using these noisy data, and
the results with diﬀerent thresholds τ are shown in the right plot of Fig. 5. It
can be shown that with entropy ranking, images without valuable information
will be ﬁltered out, and the best result 89.6% is comparable with the best result
89.6% without intentionally adding noisy labels. When the threshold τ is set to
1.5, entropy ranking mechanism can ﬁlter out most of the noisy samples in VOC
07 + 12, only 1, 833 samples reserved. We claim that the advantages of ranking
mechanism are obvious.

4.4 Comparisons with State-of-the-arts

We now move on to compare our proposed method with state-of-the-art works
on above mentioned ﬁne-grained datasets. In Table 4, we show the compari-

Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition

13

Table 4. Comparisions with state-of-the-art methods on CUB-200-2011.

Methods
Baseline
MAMC [34]
PC-DenseNet [10]
DBTNet [48]
Transfer Learning ∗ [6]
Transfer Learning ∗ [6]
Mixup [42]
CutMix [41]
Attribute Mix
Attribute Mix+ ∗

Backbone
ResNet-101
ResNet-101
DenseNet-161
ResNet-101
Inception-v3
Inception-ResNet-v2 SE
ResNet-101
ResNet-101
ResNet-101
ResNet-101

Acc.
85.8
86.5
86.9
88.1
89.6
89.3
86.5
86.7
88.5
90.2

son results on CUB-200-2011. Noticed that * denotes methods using external
data. For fair comparison, we choose recent works which use similar backbone
with us. MAMC [34] introduces complex attention modules to model the subtle
visual diﬀerences. In [10], Pairwise Confusion procedure is designed to reduce
the overﬁtting by intentionally introduced confusion in the activations. And bi-
linear interaction with high computational complexity is used in [48] to learn
ﬁne-grained image representations. As for our method, our proposed Attribute
Mix achieves a superior accuracy of 88.5% on CUB-200-2011 without any com-
plicated modules or external data. Compared with the other data-augmentation
methods, Attribute Mix outperforms the Mixup and CutMix at least 1.8 points,
which further demonstrates that Attribute Mix is more suitable for ﬁne-grained
categorization.

We also compare our proposed Attribute Mix+ with the work using the ex-
ternal data. Transfer learning [6] introduces external labeled ﬁne-grained dataset
iNat [36] and leverages this large scale dataset by transfer learning. As for our
proposed Attribute Mix+, we only use the data with low cost, generic labels. The
result of Attribute Mix+ surpasses transfer learning [6] using labeled ﬁne-grained
dataset by 0.6 points.

Our method also exhibits good performances on Standford Cars [22] and Air-
craft [26]. The comparison results on these two ﬁne-grained dataset are shown
in Table 5 and Table 6, respectively. Our proposed Attribute Mix achieves accu-
racies of 94.6% and 91.6% on these two datasets without external data, which
bring about improvements of 2.0 and 2.1 points over the high baseline. When
using the data with generic labels, the performances of Attribute Mix+ can be
boosted to 94.9% and 93.1%. As far as we know, these accuracies are the best
results over the two datasets. It can be seen that Attribute Mix+ achieves the
state-of-art performances on all these ﬁne-grained datasets. Most importantly,
our proposed method does not increase inference budgets comparing with the
baseline model.

14

Hao Li et al.

Table 5. Comparisions with state-of-the-art methods on Standford Cars.

Methods
Baseline
MAMC [34]
PC-DenseNet-161 [10]
DBTNet-101 [48]
Transfer Learning ∗ [6]
Transfer Learning ∗ [6]
Mixup [42]
CutMix [41]
Attribute Mix
Attribute Mix+ ∗

Backbone
ResNet-101
ResNet-101
DenseNet-161
ResNet-101
Inception-v3
Inception-ResNet-v2 SE
ResNet-101
ResNet-101
ResNet-101
ResNet-101

Methods
Baseline
MAMC [34]
PC-DenseNet-161 [10]
DBTNet-101 [48]
Transfer Learning ∗ [6]
Transfer Learning ∗ [6]
Mixup [42]
CutMix [41]
Attribute Mix
Attribute Mix+ ∗

Backbone
ResNet-101
ResNet-101
DenseNet-161
ResNet-101
Inception-v3
Inception-ResNet-v2 SE
ResNet-101
ResNet-101
ResNet-101
ResNet-101

Acc.
92.6
93.0
92.9
94.5
93.1
93.5
93.8
94.0
94.6
94.9

Acc.
89.5
-
89.2
91.6
89.6
90.7
90.0
89.7
91.6
93.1

Table 6. Comparisions with state-of-the-art methods on FGVC-Aircraft.

5 Conclusion

This paper presented a general data augmentation framework for ﬁne-grained
recognition. Our proposed method, named Attribute Mix, conducts data aug-
mentation via mixing two images at attribute level, and can greatly improve
the performance without increasing the inference budgets. Furthermore, based
on the principle that the attribute level features can be seamlessly transferred
from the generic domain to the ﬁne-grained domain regardless of their speciﬁc
labels, we enrich the training samples with attribute level labels using images
from the generic domain with low labelling cost, and further boost the perfor-
mance. Our proposed method is a general framework for data augmentation at
low cost, and can be combined with most state-of-the-art ﬁne-grained recognition
methods to further improve the performance. Experiments conducted on several
widely used ﬁne-grained benchmarks have demonstrated the eﬀectiveness of our
proposed method.

Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition

15

References

1. Bency, A.J., Kwon, H., Lee, H., Karthikeyan, S., Manjunath, B.: Weakly super-
vised localization using deep feature maps. In: European Conference on Computer
Vision. pp. 714–731. Springer (2016)

2. Berg, T., Liu, J., Lee, S.W., Alexander, M.L., Jacobs, D.W., Belhumeur, P.N.:
Birdsnap: Large-scale ﬁne-grained visual categorization of birds. 2014 IEEE Con-
ference on Computer Vision and Pattern Recognition pp. 2019–2026 (2014)

3. Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., Raﬀel, C.A.:
Mixmatch: A holistic approach to semi-supervised learning. In: Advances in Neural
Information Processing Systems. pp. 5050–5060 (2019)

4. Branson, S., Van Horn, G., Belongie, S., Perona, P.: Bird species categorization
using pose normalized deep convolutional nets. arXiv preprint arXiv:1406.2952
(2014)

5. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning
augmentation strategies from data. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 113–123 (2019)

6. Cui, Y., Song, Y., Sun, C., Howard, A., Belongie, S.: Large scale ﬁne-grained
categorization and domain-speciﬁc transfer learning. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 4109–4118 (2018)
7. Cui, Y., Zhou, F., Lin, Y., Belongie, S.: Fine-grained categorization and dataset
bootstrapping using deep metric learning with humans in the loop. In: Proceedings
of the IEEE conference on computer vision and pattern recognition. pp. 1153–1162
(2016)

8. Cui, Y., Zhou, F., Wang, J., Liu, X., Lin, Y., Belongie, S.: Kernel pooling for
convolutional neural networks. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 2921–2930 (2017)

9. DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net-

works with cutout. arXiv preprint arXiv:1708.04552 (2017)

10. Dubey, A., Gupta, O., Guo, P., Raskar, R., Farrell, R., Naik, N.: Pairwise confusion
for ﬁne-grained visual classiﬁcation. In: Proceedings of the European Conference
on Computer Vision (ECCV). pp. 70–86 (2018)

11. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.:
The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.
http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html

12. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal

visual object classes challenge 2007 (voc2007) results (2007)

13. Fu, J., Zheng, H., Mei, T.: Look closer to see better: Recurrent attention convo-
lutional neural network for ﬁne-grained image recognition. In: Proceedings of the
IEEE conference on computer vision and pattern recognition. pp. 4438–4446 (2017)
14. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac-
curate object detection and semantic segmentation. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 580–587 (2014)

15. Grandvalet, Y., Bengio, Y.: Semi-supervised learning by entropy minimization. In:

Advances in neural information processing systems. pp. 529–536 (2005)

16. Guo, H., Mao, Y., Zhang, R.: Mixup as locally linear out-of-manifold regularization.
In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. vol. 33, pp. 3714–
3722 (2019)

17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770–778 (2016)

16

Hao Li et al.

18. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the
IEEE conference on computer vision and pattern recognition. pp. 7132–7141 (2018)
19. Khosla, A., Jayadevaprakash, N., Yao, B., Fei-Fei, L.: Novel dataset for ﬁne-grained
image categorization. In: First Workshop on Fine-Grained Visual Categorization,
IEEE Conference on Computer Vision and Pattern Recognition. Colorado Springs,
CO (June 2011)

20. Kim, D., Cho, D., Yoo, D., So Kweon, I.: Two-phase learning for weakly super-
vised object localization. In: Proceedings of the IEEE International Conference on
Computer Vision. pp. 3534–3543 (2017)

21. Krause, J., Sapp, B., Howard, A., Zhou, H., Toshev, A., Duerig, T., Philbin, J., Fei-
Fei, L.: The unreasonable eﬀectiveness of noisy data for ﬁne-grained recognition.
In: European Conference on Computer Vision. pp. 301–320. Springer (2016)
22. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for ﬁne-
grained categorization. In: 4th International IEEE Workshop on 3D Representation
and Recognition (3dRR-13). Sydney, Australia (2013)

23. Lee, D.H.: Pseudo-label: The simple and eﬃcient semi-supervised learning method
for deep neural networks. In: Workshop on challenges in representation learning,
ICML. vol. 3, p. 2 (2013)

24. Lin, D., Shen, X., Lu, C., Jia, J.: Deep lac: Deep localization, alignment and
classiﬁcation for ﬁne-grained recognition. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 1666–1674 (2015)

25. Lin, T.Y., RoyChowdhury, A., Maji, S.: Bilinear cnn models for ﬁne-grained visual
recognition. In: Proceedings of the IEEE international conference on computer
vision. pp. 1449–1457 (2015)

26. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual

classiﬁcation of aircraft. arXiv preprint arXiv:1306.5151 (2013)

27. Miyato, T., Maeda, S.i., Koyama, M., Ishii, S.: Virtual adversarial training: a regu-
larization method for supervised and semi-supervised learning. IEEE transactions
on pattern analysis and machine intelligence 41(8), 1979–1993 (2018)

28. Oliva, A., Torralba, A.: Modeling the shape of the scene: A holistic representation
of the spatial envelope. International journal of computer vision 42(3), 145–175
(2001)

29. Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Is object localization for free?-weakly-
supervised learning with convolutional neural networks. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. pp. 685–694 (2015)
30. Schroﬀ, F., Kalenichenko, D., Philbin, J.: Facenet: A uniﬁed embedding for face
recognition and clustering. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 815–823 (2015)

31. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-
cam: Visual explanations from deep networks via gradient-based localization. In:
Proceedings of the IEEE International Conference on Computer Vision. pp. 618–
626 (2017)

32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556 (2014)

33. Singh, K.K., Lee, Y.J.: Hide-and-seek: Forcing a network to be meticulous for
weakly-supervised object and action localization. In: 2017 IEEE International Con-
ference on Computer Vision (ICCV). pp. 3544–3553. IEEE (2017)

34. Sun, M., Yuan, Y., Zhou, F., Ding, E.: Multi-attention multi-class constraint for
ﬁne-grained image recognition. In: Proceedings of the European Conference on
Computer Vision (ECCV). pp. 805–821 (2018)

Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition

17

35. Van Horn, G., Branson, S., Farrell, R., Haber, S., Barry, J., Ipeirotis, P., Perona,
P., Belongie, S.: Building a bird recognition app and large scale dataset with citizen
scientists: The ﬁne print in ﬁne-grained dataset collection. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. pp. 595–604 (2015)
36. Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam,
H., Perona, P., Belongie, S.: The inaturalist species classiﬁcation and detection
dataset. In: Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 8769–8778 (2018)

37. Verma, V., Lamb, A., Beckham, C., Najaﬁ, A., Mitliagkas, I., Courville, A., Lopez-
Paz, D., Bengio, Y.: Manifold mixup: Better representations by interpolating hid-
den states. arXiv preprint arXiv:1806.05236 (2018)

38. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd

birds-200-2011 dataset (2011)

39. Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X.:
Residual attention network for image classiﬁcation. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. pp. 3156–3164 (2017)

40. Xiao, T., Xu, Y., Yang, K., Zhang, J., Peng, Y., Zhang, Z.: The application of
two-level attention models in deep convolutional neural network for ﬁne-grained
image classiﬁcation. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 842–850 (2015)

41. Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization
strategy to train strong classiﬁers with localizable features. In: Proceedings of the
IEEE International Conference on Computer Vision. pp. 6023–6032 (2019)

42. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk

minimization. arXiv preprint arXiv:1710.09412 (2017)

43. Zhang, N., Donahue, J., Girshick, R., Darrell, T.: Part-based r-cnns for ﬁne-grained
category detection. In: European conference on computer vision. pp. 834–849.
Springer (2014)

44. Zhang, N., Shelhamer, E., Gao, Y., Darrell, T.: Fine-grained pose prediction, nor-

malization, and recognition. arXiv preprint arXiv:1511.07063 (2015)

45. Zhang, X., Wei, Y., Feng, J., Yang, Y., Huang, T.S.: Adversarial complementary
learning for weakly supervised object localization. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. pp. 1325–1334 (2018)

46. Zhang, X., Xiong, H., Zhou, W., Lin, W., Tian, Q.: Picking deep ﬁlter responses
for ﬁne-grained image recognition. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 1134–1142 (2016)

47. Zheng, H., Fu, J., Mei, T., Luo, J.: Learning multi-attention convolutional neural
network for ﬁne-grained image recognition. In: Proceedings of the IEEE interna-
tional conference on computer vision. pp. 5209–5217 (2017)

48. Zheng, H., Fu, J., Zha, Z.J., Luo, J.: Learning deep bilinear transformation for
ﬁne-grained image representation. In: Advances in Neural Information Processing
Systems. pp. 4279–4288 (2019)

49. Zheng, H., Fu, J., Zha, Z.J., Luo, J.: Looking for the devil in the details: Learning
trilinear attention sampling network for ﬁne-grained image recognition. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 5012–5021 (2019)

50. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features
for discriminative localization. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 2921–2929 (2016)

