3
1
0
2
 
b
e
F
 
5
 
 
]

V
C
.
s
c
[
 
 
3
v
5
0
0
1
.
3
0
2
1
:
v
i
X
r
a

1

Sparse Subspace Clustering:
Algorithm, Theory, and Applications

Ehsan Elhamifar, Student Member, IEEE, and Ren ´e Vidal, Senior Member, IEEE

Abstract—Many real-world problems deal with collections of high-dimensional data, such as images, videos, text and web documents, DNA
microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or
categories to which the data belong. In this paper, we propose and study an algorithm, called Sparse Subspace Clustering (SSC), to cluster
data points that lie in a union of low-dimensional subspaces. The key idea is that, among the inﬁnitely many possible representations of a data
point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a
sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since
solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions
on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired
sparse representations. The proposed algorithm is efﬁcient and can handle data points near the intersections of subspaces. Another key
advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise,
sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the
effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation
and face clustering.

Index Terms—High-dimensional data, intrinsic low-dimensionality, subspaces, clustering, sparse representation, (cid:96)1-minimization, convex
programming, spectral clustering, principal angles, motion segmentation, face clustering.

(cid:70)

1 INTRODUCTION

H IGH-DIMENSIONAL data are ubiquitous in many areas of

machine learning, signal and image processing, computer
vision, pattern recognition, bioinformatics, etc. For instance,
images consist of billions of pixels, videos can have millions of
frames, text and web documents are associated with hundreds of
thousands of features, etc. The high-dimensionality of the data not
only increases the computational time and memory requirements
of algorithms, but also adversely affects their performance due to
the noise effect and insufﬁcient number of samples with respect
to the ambient space dimension, commonly referred to as the
“curse of dimensionality” [1]. However, high-dimensional data
often lie in low-dimensional structures instead of being uniformly
distributed across the ambient space. Recovering low-dimensional
structures in the data helps to not only reduce the computational
cost and memory requirements of algorithms, but also reduce
the effect of high-dimensional noise in the data and improve the
performance of inference, learning, and recognition tasks.

In fact, in many problems, data in a class or category can
be well represented by a low-dimensional subspace of the high-
dimensional ambient space. For example, feature trajectories of
a rigidly moving object in a video [2], face images of a subject
under varying illumination [3], and multiple instances of a hand-
written digit with different rotations, translations, and thicknesses
[4] lie in a low-dimensional subspace of the ambient space. As a
result, the collection of data from multiple classes or categories
lie in a union of low-dimensional subspaces. Subspace clustering

• E. Elhamifar

is with the Department of Electrical Engineering and
Computer Science, University of California, Berkeley, USA. E-mail:
ehsan@eecs.berkeley.edu.

• R. Vidal

is with the Center for Imaging Science and the Department of
Biomedical Engineering, The Johns Hopkins University, USA. E-mail: rvi-
dal@cis.jhu.edu.

(see [5] and references therein) refers to the problem of separating
data according to their underlying subspaces and ﬁnds numerous
applications in image processing (e.g., image representation and
compression [6]) and computer vision (e.g., image segmentation
[7], motion segmentation [8], [9], and temporal video segmen-
tation [10]), as illustrated in Figures 1 and 2. Since data in
a subspace are often distributed arbitrarily and not around a
centroid, standard clustering methods [11] that take advantage
of the spatial proximity of the data in each cluster are not in
general applicable to subspace clustering. Therefore, there is a
need for having clustering algorithms that take into account the
multi-subspace structure of the data.

1.1 Prior Work on Subspace Clustering

Existing algorithms can be divided into four main categories: iter-
ative, algebraic, statistical, and spectral clustering-based methods.

Iterative methods.
Iterative approaches, such as K-subspaces
[12], [13] and median K-ﬂats [14] alternate between assigning
points to subspaces and ﬁtting a subspace to each cluster. The
main drawbacks of such approaches are that they generally require
to know the number and dimensions of the subspaces, and that
they are sensitive to initialization.

Algebraic approaches. Factorization-based algebraic approaches
such as [8], [9], [15] ﬁnd an initial segmentation by thresholding
the entries of a similarity matrix built from the factorization
of the data matrix. These methods are provably correct when
the subspaces are independent, but fail when this assumption
is violated. In addition, they are sensitive to noise and outliers
in the data. Algebraic-geometric approaches such as Generalized
Principal Component Analysis (GPCA) [10], [16], ﬁt the data
with a polynomial whose gradient at a point gives the normal
vector to the subspace containing that point. While GPCA can

2

Fig. 1. Motion segmentation: given feature points on multiple rigidly moving objects tracked in multiple frames of a video (top), the goal
is to separate the feature trajectories according to the moving objects (bottom).

Fig. 2. Face clustering: given face images of multiple subjects (top), the goal is to ﬁnd images that belong to the same subject (bottom).

deal with subspaces of different dimensions, it is sensitive to noise
and outliers, and its complexity increases exponentially in terms
of the number and dimensions of subspaces.

Statistical methods. Iterative statistical approaches, such as Mix-
tures of Probabilistic PCA (MPPCA) [17], Multi-Stage Learning
(MSL) [18], or [19], assume that the distribution of the data inside
each subspace is Gaussian and alternate between data clustering
and subspace estimation by applying Expectation Maximization
(EM). The main drawbacks of these methods are that they gener-
ally need to know the number and dimensions of the subspaces,
and that
they are sensitive to initialization. Robust statistical
approaches, such as Random Sample Consensus (RANSAC) [20],
ﬁt a subspace of dimension d to randomly chosen subsets of d
points until the number of inliers is large enough. The inliers
are then removed, and the process is repeated to ﬁnd a second
subspace, and so on. RANSAC can deal with noise and outliers,
and does not need to know the number of subspaces. However,
the dimensions of the subspaces must be known and equal. In
addition, the complexity of the algorithm increases exponentially
in the dimension of the subspaces. Information-theoretic statistical
approaches, such as Agglomerative Lossy Compression (ALC)
[21], look for the segmentation of the data that minimizes the
coding length needed to ﬁt the points with a mixture of degenerate
Gaussians up to a given distortion. As this minimization problem
is NP-hard, a suboptimal solution is found by ﬁrst assuming that
each point forms its own group, and then iteratively merging pairs
of groups to reduce the coding length. ALC can handle noise
and outliers in the data. While, in principle, it does not need to
know the number and dimensions of the subspaces, the number
of subspaces found by the algorithms is dependent on the choice
of a distortion parameter. In addition, there is no theoretical proof
for the optimality of the agglomerative algorithm.

Spectral clustering-based methods. Local spectral clustering-
based approaches such as Local Subspace Afﬁnity (LSA) [22],
Locally Linear Manifold Clustering (LLMC) [23], Spectral Local
Best-ﬁt Flats (SLBF) [24], and [25] use local information around
each point to build a similarity between pairs of points. The
segmentation of the data is then obtained by applying spectral

clustering [26], [27] to the similarity matrix. These methods
have difﬁculties in dealing with points near the intersection of
two subspaces, because the neighborhood of a point can contain
points from different subspaces. In addition, they are sensitive to
the right choice of the neighborhood size to compute the local
information at each point.

Global spectral clustering-based approaches try to resolve these
issues by building better similarities between data points using
global information. Spectral Curvature Clustering (SCC) [28] uses
multi-way similarities that capture the curvature of a collection of
points within an afﬁne subspace. SCC can deal with noisy data
but requires to know the number and dimensions of subspaces and
assumes that subspaces have the same dimension. In addition, the
complexity of building the multi-way similarity grows exponen-
tially with the dimensions of the subspaces, hence, in practice,
a sampling strategy is employed to reduce the computational
cost. Using advances in sparse [29], [30], [31] and low-rank
[32], [33], [34] recovery algorithms, Sparse Subspace Clustering
(SSC) [35], [36], [37], Low-Rank Recovery (LRR) [38], [39],
[40], and Low-Rank Subspace Clustering (LRSC) [41] algorithms
pose the clustering problem as one of ﬁnding a sparse or low-rank
representation of the data in the dictionary of the data itself. The
solution of the corresponding global optimization algorithm is
then used to build a similarity graph from which the segmentation
of the data is obtained. The advantages of these methods with
respect to most state-of-the-art algorithms are that they can handle
noise and outliers in data, and that they do not need to know the
dimensions and, in principle, the number of subspaces a priori.

1.2 Paper Contributions
In this paper, we propose and study an algorithm based on
sparse representation techniques, called Sparse Subspace Cluster-
ing (SSC), to cluster a collection of data points lying in a union
of low-dimensional subspaces. The underlying idea behind the
algorithm is what we call the self-expressiveness property of the
data, which states that each data point in a union of subspaces can
be efﬁciently represented as a linear or afﬁne combination of other
points. Such a representation is not unique in general because
there are inﬁnitely many ways in which a data point can be

expressed as a combination of other points. The key observation
is that a sparse representation of a data point ideally corresponds
to a combination of a few points from its own subspace. This
motivates solving a global sparse optimization program whose
solution is used in a spectral clustering framework to infer the
clustering of data. As a result, we can overcome the problems
of local spectral clustering-based algorithms, such as choosing
the right neighborhood size and dealing with points near the
intersection of subspaces, since, for a given data point, the sparse
optimization program automatically picks a few other points that
are not necessarily close to it but belong to the same subspace.
Since solving the sparse optimization program is in general
NP-hard, we consider its (cid:96)1 relaxation. We show that, under mild
conditions on the arrangement of subspaces and data distribution,
the proposed (cid:96)1-minimization program recovers the desired solu-
tion, guaranteeing the success of the algorithm. Our theoretical
analysis extends the sparse representation theory to the multi-
subspace setting where the number of points in a subspace is
arbitrary, possibly much larger than its dimension. Unlike block-
sparse recovery problems [42], [43], [44], [45], [46], [47] where
the bases for the subspaces are known and given, we do not have
the bases for subspaces nor do we know which data points belong
to which subspace, making our case more challenging. We only
have the sparsifying dictionary for the union of subspaces given
by the matrix of data points.

The proposed (cid:96)1-minimization program can be solved efﬁ-
ciently using convex programming tools [48], [49], [50] and does
not require initialization. Our algorithm can directly deal with
noise, sparse outlying entries, and missing entries in the data as
well as the more general class of afﬁne subspaces by incorporating
the data corruption or subspace model into the sparse optimization
program. Finally,
through experimental results, we show that
our algorithm outperforms state-of-the-art subspace clustering
methods on the two real-world problems of motion segmentation
(Fig. 1) and face clustering (Fig. 2).

Paper Organization.
In Section 2, we motivate and introduce
the SSC algorithm for clustering data points in a union of linear
subspaces. In Section 3, we generalize the algorithm to deal with
noise, sparse outlying entries, and missing entries in the data as
well as the more general class of afﬁne subspaces. In Section
4, we investigate theoretical conditions under which the (cid:96)1-
minimization program recovers the desired sparse representations
of data points. In Section 5, we discuss the connectivity of the
similarity graph and propose a regularization term to increase the
connectivity of points in each subspace. In Section 6, we verify
our theoretical analysis through experiments on synthetic data. In
Section 7, we compare the performance of SSC with the state of
the art on the two real-world problems of motion segmentation
and face clustering. Finally, Section 8 concludes the paper.

2 SPARSE SUBSPACE CLUSTERING
In this section, we introduce the sparse subspace clustering (SSC)
algorithm for clustering a collection of multi-subspace data using
sparse representation techniques. We motivate and formulate the
algorithm for data points that perfectly lie in a union of linear
subspaces. In the next section, we will generalize the algorithm
to deal with data nuisances such as noise, sparse outlying entries,
and missing entries as well as the more general class of afﬁne
subspaces.

3

Let

n
{S(cid:96)}
of dimensions
d(cid:96)}
{
free data points
yi}
{
Denote the matrix containing all the data points as

(cid:96)=1 be an arrangement of n linear subspaces of RD
n
(cid:96)=1. Consider a given collection of N noise-
N
i=1 that lie in the union of the n subspaces.

(cid:2)

(cid:3)

=

Γ,

(1)

RN

Y 1

. . . Y n

. . . yN

Y (cid:44)
RD
(cid:2)
×
S(cid:96) and Γ

y1
N(cid:96) is a rank-d(cid:96) matrix of the N(cid:96) > d(cid:96) points
where Y (cid:96) ∈
N is an unknown permutation matrix.
that lie in
We assume that we do not know a priori the bases of the subspaces
nor do we know which data points belong to which subspace. The
subspace clustering problem refers to the problem of ﬁnding the
number of subspaces, their dimensions, a basis for each subspace,
and the segmentation of the data from Y .

∈

×

(cid:3)

To address the subspace clustering problem, we propose an
algorithm that consists of two steps. In the ﬁrst step, for each data
point, we ﬁnd a few other points that belong to the same subspace.
To do so, we propose a global sparse optimization program
whose solution encodes information about the memberships of
data points to the underlying subspace of each point. In the second
step, we use these information in a spectral clustering framework
to infer the clustering of the data.

2.1 Sparse Optimization Program

Our proposed algorithm takes advantage of what we refer to as
the self-expressiveness property of the data, i.e.,

each data point in a union of subspaces can be efﬁciently re-
constructed by a combination of other points in the dataset.

More precisely, each data point for data point yi ∈ ∪
be written as
yi = Y ci,

cii = 0,

n

(cid:96)=1S(cid:96) can
(2)

(cid:2)

(cid:3)

. . .

ci2

ci1

where ci (cid:44)
(cid:62) and the constraint cii = 0
ciN
eliminates the trivial solution of writing a point as a linear
combination of itself. In other words, the matrix of data points
Y is a self-expressive dictionary in which each point can be
written as a linear combination of other points. However, the
representation of yi in the dictionary Y is not unique in general.
This comes from the fact that the number of data points in a
subspace is often greater than its dimension, i.e., N(cid:96) > d(cid:96). As a
result, each Y (cid:96), and consequently Y , has a non-trivial nullspace
giving rise to inﬁnitely many representations of each data point.
The key observation in our proposed algorithm is that among

all solutions of (2),

there exists a sparse solution, ci, whose nonzero entries
correspond to data points from the same subspace as yi. We
refer to such a solution as a subspace-sparse representation.

More speciﬁcally, a data point yi that lies in the d(cid:96)-dimensional
S(cid:96) can be written as a linear combination of d(cid:96) other
subspace
points in general directions from
S(cid:96). As a result, ideally, a sparse
representation of a data point ﬁnds points from the same subspace
where the number of the nonzero elements corresponds to the
dimension of the underlying subspace.

For a system of equations such as (2) with inﬁnitely many
solutions, one can restrict the set of solutions by minimizing an
objective function such as the (cid:96)q-norm of the solution1 as

min

ci(cid:107)q

(cid:107)

s. t. yi = Y ci, cii = 0.

(3)

1. The (cid:96)q-norm of ci ∈ RN is deﬁned as (cid:107)ci(cid:107)q (cid:44) ((cid:80)N

j=1 |cij |q)

1
q .

4

Fig. 3. Three subspaces in R3 with 10 data points in each subspace, ordered such that the ﬁst and the last 10 points belong to S1 and
S3, respectively. The solution of the (cid:96)q-minimization program in (3) for yi lying in S1 for q = 1, 2, ∞ is shown. Note that as the value of q
decreases, the sparsity of the solution increases. For q = 1, the solution corresponds to choosing two other points lying in S1.

Different choices of q have different effects in the obtained
solution. Typically, by decreasing the value of q from inﬁnity
toward zero, the sparsity of the solution increases, as shown in
Figure 3. The extreme case of q = 0 corresponds to the general
NP-hard problem [51] of ﬁnding the sparsest representation of
the given point, as the (cid:96)0-norm counts the number of nonzero
elements of the solution. Since we are interested in efﬁciently
ﬁnding a non-trivial sparse representation of yi in the dictionary
Y , we consider minimizing the tightest convex relaxation of the
(cid:96)0-norm, i.e.,

min

ci(cid:107)1
(cid:107)

s. t. yi = Y ci, cii = 0,

(4)

which can be solved efﬁciently using convex programming tools
[48], [49], [50] and is known to prefer sparse solutions [29], [30],
[31].

We can also rewrite the sparse optimization program (4) for all

data points i = 1, . . . , N in matrix form as

min

C
(cid:107)1
(cid:107)
c1 c2

s. t. Y = Y C, diag(C) = 0,

(5)

×

∈

RN

(cid:2)
∈

. . . cN

where C (cid:44)
N is the matrix whose
i-th column corresponds to the sparse representation of yi, ci,
(cid:3)
RN is the vector of the diagonal elements of C.
and diag(C)
the solution of (5) corresponds to subspace-sparse
Ideally,
representations of the data points, which we use next to infer the
clustering of the data. In Section 4, we study conditions under
which the convex optimization program in (5) is guaranteed to
recover a subspace-sparse representation of each data point.

2.2 Clustering using Sparse Coefﬁcients

E

V

, W ), where

After solving the proposed optimization program in (5), we
obtain a sparse representation for each data point whose nonzero
elements ideally correspond to points from the same subspace.
The next step of the algorithm is to infer the segmentation of the
data into different subspaces using the sparse coefﬁcients.
To address this problem, we build a weighted graph
,

=
denotes the set of N nodes of the graph
(
V
denotes the set of
corresponding to N data points and
N is a symmetric non-negative
edges between nodes. W
×
similarity matrix representing the weights of the edges, i.e., node
i is connected to node j by an edge whose weight is equal to wij.
An ideal similarity matrix W , hence an ideal similarity graph
,
G
is one in which nodes that correspond to points from the same
subspace are connected to each other and there are no edges
between nodes that correspond to points in different subspaces.
Note that the sparse optimization program ideally recovers to a
subspace-sparse representation of each point, i.e., a representation

E ⊆ V × V

RN

∈

G

+

C

whose nonzero elements correspond to points from the same
subspace of the given data point. This provides an immediate
(cid:62). In other
choice of the similarity matrix as W =
|
words, each node i connects itself to a node j by an edge whose
weight is equal to
. The reason for the symmetrization
is that, in general, a data point yi ∈ S(cid:96) can write itself as a
linear combination of some points including yj ∈ S(cid:96). However,
yj may not necessarily choose yi in its sparse representation. By
this particular choice of the weight, we make sure that nodes i
and j get connected to each other if either yi or yj is in the
sparse representation of the other.2

cij|
|

cji|

C
|

+

|

|

|

The similarity graph built this way has ideally n connected

components corresponding to the n subspaces, i.e.,

W = 

W 1 · · ·
...
. . .
0

0
...
W n

Γ,



(6)




· · ·



where W (cid:96) is the similarity matrix of data points in
S(cid:96). Clustering
of data into subspaces follows then by applying spectral clustering
. More speciﬁcally, we obtain the clustering
[26] to the graph
of data by applying the Kmeans algorithm [11] to the normalized
rows of a matrix whose columns are the n bottom eigenvectors
of the symmetric normalized Laplacian matrix of the graph.

G

ci/
(cid:107)

Remark 1: An optional step prior to building the similarity
.
graph is to normalize the sparse coefﬁcients as ci ←
ci(cid:107)∞
This helps to better deal with different norms of data points.
More speciﬁcally, if a data point with a large Euclidean norm
selects a few points with small Euclidean norms, then the values
of the nonzero coefﬁcients will generally be large. On the other
hand, if a data point with a small Euclidean norm selects a few
points with large Euclidean norms, then the values of the nonzero
coefﬁcients will generally be small. Since spectral clustering puts
more emphasis on keeping the stronger connections in the graph,
by the normalization step we make sure that the largest edge
weights for all the nodes are of the same scale.

Algorithm 1 summarizes the SSC algorithm. Note that an
advantage of spectral clustering, which will be shown in the
experimental results, is that it provides robustness with respect
to a few errors in the sparse representations of the data points.
In other words, as long as edges between points in different
subspaces are weak, spectral clustering can ﬁnd the correct
segmentation.

2. To obtain a symmetric similarity matrix, one can directly impose the
constraint of C = C(cid:62) in the optimization program. However, this results in
increasing the complexity of the optimization program and, in practice, does not
perform better than the post-symmetrization of C, as described above. See also
[52] for other processing approaches of the similarity matrix.

Algorithm 1 : Sparse Subspace Clustering (SSC)
Input: A set of points
subspaces
{Si}

yi}
{

n
i=1.

N
i=1 lying in a union of n linear

1: Solve the sparse optimization program (5) in the case of

uncorrupted data or (13) in the case of corrupted data.

2: Normalize the columns of C as ci ←
3: Form a similarity graph with N nodes representing the data
points. Set the weights on the edges between the nodes by
W =

C

+

∞

(cid:107)

(cid:107)

.

ci
ci

C
|

|

(cid:62).
|

|

4: Apply spectral clustering [26] to the similarity graph.

Output: Segmentation of the data: Y 1, Y 2, . . . , Y n.

Remark 2: In principle, SSC does not need to know the
number of subspaces. More speciﬁcally, under the conditions of
the theoretical results in Section 4, in the similarity graph there
will be no connections between points in different subspaces.
Thus, one can determine the number of subspaces by ﬁnding the
number of graph components, which can be obtained by analyzing
[27]. However,
the eigenspectrum of the Laplacian matrix of
when there are connections between points in different subspaces,
other model selection techniques should be employed [53].

G

3 PRACTICAL EXTENSIONS
In real-world problems, data are often corrupted by noise and
sparse outlying entries due to measurement/process noise and ad-
hoc data collection techniques. In such cases, the data do not lie
perfectly in a union of subspaces. For instance, in the motion
segmentation problem, because of the malfunctioning of the
tracker, feature trajectories can be corrupted by noise or can have
entries with large errors [21]. Similarly, in clustering of human
faces, images can be corrupted by errors due to specularities, cast
shadows, and occlusions [54]. On the other hand, data points may
have missing entries, e.g., when the tracker loses track of some
feature points in a video due to occlusions [55]. Finally, data may
lie in a union of afﬁne subspaces, a more general model which
includes linear subspaces as a particular case.

In this section, we generalize the SSC algorithm for clustering
data lying perfectly in a union of linear subspaces, to deal with
the aforementioned challenges. Unlike state-of-the-art methods,
which require to run a separate algorithm ﬁrst to correct the errors
in the data [21], [55], we deal with these problems in a uniﬁed
framework by incorporating a model for the corruption into the
sparse optimization program. Thus, the sparse coefﬁcients again
encode information about memberships of data to subspaces,
which are used in a spectral clustering framework, as before.

3.1 Noise and Sparse Outlying Entries

In this section, we consider clustering of data points that are
contaminated with sparse outlying entries and noise. Let

5

(9)

(10)

(11)

using the self-expressiveness property, we can reconstruct y0
S(cid:96) in terms of other error-free points as
cijy0
j .

i =

y0

i ∈

(8)

=i
(cid:88)j

Note that the above equation has a sparse solution since y0
i can
be expressed as a linear combination of at most d(cid:96) other points
i using (7) in terms of the corrupted point
from
yi, the sparse outlying entries vector e0
i , and the noise vector z0
i
and substituting it into (8), we obtain

S(cid:96). Rewriting y0

where the vectors ei ∈

RD are deﬁned as

yi =

cijyj + ei + zi,

(cid:88)j
=i
RD and zi ∈
i −
i −

j

j

ei (cid:44) e0
zi (cid:44) z0

=i cije0
j ,
=i cijz0
j .

(cid:80)
Since (8) has a sparse solution ci, ei and zi also correspond to
(cid:80)
vectors of sparse outlying entries and noise, respectively. More
precisely, when a few cij are nonzero, ei is a vector of sparse
outlying entries since it is a linear combination of a few vectors
of outlying entries in (10). Similarly, when a few cij are nonzero
and do not have signiﬁcantly large magnitudes3, zi is a vector of
noise since it is linear combination of a few noise vectors in (11).
Collecting ei and zi as columns of the matrices E and Z,

respectively, we can rewrite (9) in matrix form as

Y = Y C + E + Z,

diag(C) = 0.

(12)

Our objective is then to ﬁnd a solution (C, E, Z) for (12), where
C corresponds to a sparse coefﬁcient matrix, E corresponds to
a matrix of sparse outlying entries, and Z is a noise matrix. To
do so, we propose to solve the following optimization program

min

C
(cid:107)

(cid:107)1 +
s. t. Y = Y C + E + Z, diag(C) = 0,

(cid:107)1 + λe(cid:107)

E

Z

(cid:107)

2
F

λz
2 (cid:107)

(13)

where the (cid:96)1-norm promotes sparsity of the columns of C and
E, while the Frobenius norm promotes having small entries in
the columns of Z. The two parameters λe > 0 and λz > 0
balance the three terms in the objective function. Note that
the optimization program in (13) is convex with respect to the
optimization variables (C, E, Z), hence, can be solved efﬁciently
using convex programming tools.

When data are corrupted only by noise, we can eliminate E
from the optimization program in (13). On the other hand, when
the data are corrupted only by sparse outlying entries, we can
eliminate Z in (13). In practice, however, E can also deal with
small errors due to noise. The following proposition suggests
setting λz = αz/µz and λe = αe/µe, where αz, αe > 1 and

µz (cid:44) min

i

max
=i |
j

,
y(cid:62)i yj|

µe (cid:44) min

i

max
j

=i (cid:107)

yj(cid:107)1.

(14)

yi = y0

i + e0

i + z0
i

(7)

The proofs of all theoretical results in the paper are provided in
the supplementary material.

be the i-th data point that is obtained by corrupting an error-
free point y0
i , which perfectly lies in a subspace, with a vector of
RD that has only a few large nonzero
sparse outlying entries e0
i ∈
e0
k for some integer k, and with a noise
elements, i.e.,
i (cid:107)0 ≤
(cid:107)
RD whose norm is bounded as
z0
z0
ζ for some ζ > 0.
i (cid:107)2 ≤
i ∈
(cid:107)
Since error-free data points perfectly lie in a union of subspaces,

Proposition 1: Consider the optimization program (13). With-
least
1/µe,

then there exists at

the term Z,

out

if λe ≤

3. One can show that, under broad conditions, sum of |cij | is bounded above
by the square root of the dimension of the underlying subspace of yi. Theoretical
guarantees of the proposed optimization program in the case of corrupted data is
the subject of the current research.

one data point y(cid:96) for which in the optimal solution we have
(c(cid:96), e(cid:96)) = (0, y(cid:96)). Also, without the term E, if λz ≤
1/µz, then
there exists at least one data point y(cid:96) for which (c(cid:96), z(cid:96)) = (0, y(cid:96)).
After solving the proposed optimization programs, we use C
to build a similarity graph and infer the clustering of data using
spectral clustering. Thus, by incorporating the corruption model
of data into the sparse optimization program, we can deal with
clustering of corrupted data, as before, without explicitly running
a separate algorithm to correct the errors in the data [21], [55].

3.2 Missing Entries

We consider now the clustering of incomplete data, where some
of the entries of a subset of the data points are missing. Note that
when only a small fraction of the entries of each data point is
missing, clustering of incomplete data can be cast as clustering of
data with sparse outlying entries. More precisely, one can ﬁll in
the missing entries of each data point with random values, hence
obtain data points with sparse outlying entries. Then clustering of
the data follows by solving (13) and applying spectral clustering
to the graph built using the obtained sparse coefﬁcients. However,
the drawback of this approach is that it disregards the fact that
we know the locations of the missing entries in the data matrix.
It is possible, in some cases, to cast the clustering of data
with missing entries as clustering of complete data. To see this,
i=1 in RD. Let Ji ⊂
consider a collection of data points
yi}
{
denote indices of the known entries of yi and deﬁne
1, . . . , D
{
N
J (cid:44)
i=1 Ji. Thus, for every index in J, all data points have
, is not small
known entries. When the size of J, denoted by
|
relative to the ambient space dimension, D, we can project the
data, hence, the original subspaces, into a subspace spanned by
the columns of the identity matrix indexed by J and apply the
SSC algorithm to the obtained complete data. In other words,
we can only keep the rows of Y indexed by J, obtain a new
data matrix of complete data ¯Y
N , and solve the sparse
optimization program (13). We can then infer the clustering of the
data by applying spectral clustering to the graph built using the
sparse coefﬁcient matrix. Note that the approach described above
is based on the assumption that J is nonempty. Addressing the
problem of subspace clustering with missing entries when J is
empty or has a small size is the subject of the future research.

R|
J

J
|

(cid:84)

∈

|×

}

N

3.3 Afﬁne Subspaces

In some real-world problems, the data lie in a union of afﬁne
rather than linear subspaces. For instance, the motion segmen-
tation problem involves clustering of data that lie in a union of
3-dimensional afﬁne subspaces [2], [55]. A naive way to deal
with this case is to ignore the afﬁne structure of the data and
perform clustering as in the case of linear subspaces. This comes
S(cid:96) can be
from the fact that a d(cid:96)-dimensional afﬁne subspace
considered as a subset of a (d(cid:96) + 1)-dimensional linear subspace
that includes
S(cid:96) and the origin. However, this has the drawback
of possibly increasing the dimension of the intersection of two
subspaces, which in some cases can result in indistinguishability
of subspaces from each other. For example, two different lines
1 and x = +1 in the x-y plane form the same 2-
x =
dimensional
linear subspace after including the origin, hence
become indistinguishable.

−

To directly deal with afﬁne subspaces, we use the fact that any
S(cid:96) of dimension d(cid:96) can be

data point yi in an afﬁne subspace

6

Fig. 4. Left: the three 1-dimensional subspaces are independent
as they span the 3-dimensional space and the sum of their dimen-
sions is also 3. Right: the three 1-dimensional are disjoint as any
two subspaces intersect at the origin.

written as an afﬁne combination of d(cid:96) + 1 other points from
In other words, a sparse solution of

S(cid:96).

yi = Y ci,

1(cid:62)ci = 1, cii = 0,

(15)

corresponds to d(cid:96) + 1 other points that belong to
S(cid:96) containing
yi. Thus, to cluster data points lying close to a union of afﬁne
subspaces, we propose to solve the sparse optimization program

C

min

λz
2 (cid:107)
s. t. Y = Y C + E + Z, 1(cid:62)C = 1(cid:62), diag(C) = 0,

E
(cid:107)1 + λe(cid:107)

(cid:107)1 +

2
F
(cid:107)

Z

(cid:107)

(16)

which, in comparison to (13) for the case of linear subspaces,
includes additional linear equality constraints. Note that (16) can
deal with linear subspaces as well since a linear subspace is also
an afﬁne subspace.

4 SUBSPACE-SPARSE RECOVERY THEORY
The underlying assumption for the success of the SSC algorithm
is that the proposed optimization program recovers a subspace-
sparse representation of each data point, i.e., a representation
whose nonzero elements correspond to the subspace of the given
point. In this section, we investigate conditions under which, for
data points that lie in a union of linear subspaces, the sparse
optimization program in (4) recovers subspace-sparse represen-
tations of data points. We investigate recovery conditions for
two classes of subspace arrangements: independent and disjoint
subspace models [36].

Deﬁnition 1: A collection of subspaces
n
n
i=1 dim(

independent if dim(
the direct sum operator.

i=1Si) =

⊕

n
i=1 is said to be
{Si}
denotes
Si), where

⊕

the three 1-dimensional subspaces shown in
As an example,
Figure 4 (left) are independent since they span a 3-dimensional
space and the sum of their dimensions is also 3. On the other
hand, the subspaces shown in Figure 4 (right) are not independent
since they span a 2-dimensional space while the sum of their
dimensions is 3.

Deﬁnition 2: A collection of subspaces

n
i=1 is said to be
{Si}
disjoint if every pair of subspaces intersect only at the origin. In
other words, for every pair of subspaces we have dim(
Si ⊕Sj) =
dim(

Si) + dim(

Sj).

As an example, both subspace arrangements shown in Figure 4 are
disjoint since each pair of subspaces intersect at the origin. Note
that, based on the above deﬁnitions, the notion of disjointness
is weaker than independence as an independent subspace model
is always disjoint while the converse is not necessarily true. An

(cid:80)

7

important notion that can be used to characterize two disjoint
subspaces is the smallest principal angle, deﬁned as follows.

Si with
smaller than the (cid:96)1-norm of the solution of (20), i.e.,

=iSj, the (cid:96)1-norm of the solution of (19) is strictly

⊕j

Deﬁnition 3: The smallest principal angle between two sub-

spaces

Si and

Sj, denoted by θij, is deﬁned as
v(cid:62)i vj
cos(θij) (cid:44)
vi(cid:107)2(cid:107)

max
i,vj

j
∈S

∈S

vi

(cid:107)

vj(cid:107)2

.

Note that two disjoint subspaces intersect at the origin, hence their
[0, 1).
smallest principal angle is greater than zero and cos(θij)

∈

4.1 Independent Subspace Model

In this section, we consider data points that lie in a union of
independent subspaces, which is the underlying model of many
subspace clustering algorithms. We show that the (cid:96)1-minimization
program in (4) and more generally the (cid:96)q-minimization in (3) for
always recover subspace-sparse representations of the data
q <
points. More speciﬁcally, we show the following result.

∞

n
i=1 of dimensions

Theorem 1: Consider a collection of data points drawn from
n
i=1. Let Y i
i
−
Si

n independent subspaces
denote Ni data points in
denote data points in all subspaces except
and every nonzero y in

di}
{Si}
{
Si, where rank(Y i) = di, and let Y
Si. Then, for every
Si, the (cid:96)q-minimization program
c
s. t. y = [Y i Y
c

= argmin

c
c

(18)

i]

−

,

(cid:20)

−(cid:21)

, recovers a subspace-sparse representation, i.e., c∗
= 0.

=

(cid:20)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

q
−(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

c∗
c∗
(cid:20)
−(cid:21)
for q <
∞
0 and c∗
−

Note that the subspace-sparse recovery holds without any as-
sumption on the distribution of the data points in each subspace,
other than rank(Y i) = di. This comes at the price of having a
more restrictive model for the subspace arrangements. Next, we
will show that for the more general class of disjoint subspaces,
under appropriate conditions on the relative conﬁguration of
the subspaces as well as the distribution of the data in each
subspace, the (cid:96)1-minimization in (4) recovers subspace-sparse
representations of the data points.

4.2 Disjoint Subspace Model

We consider now the more general class of disjoint subspaces and
investigate conditions under which the optimization program in
(4) recovers a subspace-sparse representation of each data point.
To that end, we consider a vector x in the intersection of
Si with
=iSj and let the optimal solution of the (cid:96)1-minimization when
⊕j
we restrict the dictionary to data points from

Si be
s. t. x = Y i a.

(19)

ai = argmin

a
(cid:107)1

(cid:107)

We also let the optimal solution of the (cid:96)1-minimization when we
Si be
restrict the dictionary to points from all subspaces except
(20)

s. t. x = Y

i = argmin

i a.4

a

−

a
(cid:107)1
(cid:107)

−

We show in the supplementary material that the SSC algorithm
succeeds in recovering subspace-sparse representations of data
Si, if for every nonzero x in the intersection of
points in each
4. In fact, ai and a−i depend on x, Y i, and Y −i. Since this dependence is
clear from the context, we drop the arguments in ai(x, Y i) and a−i(x, Y −i).

x

ai(cid:107)1 <
=iSj), x
More precisely, we show the following result.

∈ Si ∩

= 0 =

⇒ (cid:107)

⊕j

∀

(

a

i(cid:107)1.

−

(cid:107)

(21)

(17)

Theorem 2: Consider a collection of data points drawn from
n
i=1. Let Y i
i

n
i=1 of dimensions
{Si}
Si, where rank(Y i) = di, and let Y

n disjoint subspaces
denote Ni data points in
denote data points in all subspaces except

di}
{

−

Si. The (cid:96)1-minimization
c
(22)
i]
c

−

,

(cid:20)

−(cid:21)

= argmin

s. t. y = [Y i Y

c∗
c∗
−(cid:21)

(cid:20)

c
c

(cid:20)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
−(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

recovers a subspace-sparse representation of every nonzero y in
Si, i.e., c∗

= 0, if and only if (21) holds.

= 0 and c∗
−

While the necessary and sufﬁcient condition in (21) guarantees
a successful subspace-sparse recovery via the (cid:96)1-minimization
program, it does not explicitly show the relationship between the
subspace arrangements and the data distribution for the success of
the (cid:96)1-minimization program. To establish such a relationship, we
βi, where βi depends on the singular values
show that
ai(cid:107)1 ≤
(cid:107)
i depends on
of data points in
i(cid:107)1, where β
Si, and β
a
i ≤ (cid:107)
−
the subspace angles between
Si and other subspaces. Then, the
sufﬁcient condition βi < β
i establishes the relationship between
the subspace angles and the data distribution under which the (cid:96)1-
minimization is successful in subspace-sparse recovery, since it
implies that

−

−

−

a
i ≤ (cid:107)
i.e., the condition of Theorem 2 holds.

ai(cid:107)1 ≤

βi < β

(cid:107)

−

i(cid:107)1,

−

(23)

Theorem 3: Consider a collection of data points drawn from
i=1. Let Wi be
n
n
n disjoint subspaces
i=1 of dimensions
{Si}
the set of all full-rank submatrices ˜Y i ∈
di of Y i, where
rank(Y i) = di. If the condition

di}

{
RD

×

σdi( ˜Y i) >

j

∈

−

=i

(24)

(cid:112)

cos(θij)

max
˜Y i
Wi

i(cid:107)1,2 max

Si, the (cid:96)1-minimization in (22)
= 0.5
= 0 and c∗
−

Y
di (cid:107)
holds, then for every nonzero y in
recovers a subspace-sparse solution, i.e., c∗
Loosely speaking, the sufﬁcient condition in Theorem 3 states
that if the smallest principal angle between each
Si and any other
subspace is larger than a certain value that depends on the data
distribution in
Si, then the subspace-sparse recovery holds. This
bound can be rather high when the norms of the data points are
oddly distributed, e.g., when the maximum norm of data points
Si is much smaller than the maximum norm of data points in
in
all other subspaces. Since the segmentation of the data does not
change when data points are scaled, we can apply SSC to linear
subspaces after normalizing the data points to have unit Euclidean
norms. In this case, the sufﬁcient condition in (24) reduces to

max
˜Y i
Wi

∈

σdi ( ˜Y i) >

di max
=i
j

cos(θij).

(25)

(cid:112)
Remark 3: For independent subspaces, the intersection of a
subspace with the direct sum of other subspaces is the origin,
hence, the condition in (21) always holds. As a result, from
Theorem 2, the (cid:96)1-minimization always recovers subspace-sparse
representations of data points in independent subspaces.

5. The induced norm (cid:107)Y −i(cid:107)1,2 denotes the maximum (cid:96)2-norm of the columns

of Y −i.

8

Fig. 5. Left: for any nonzero x in the intersection of S1 and S2 ⊕ S3, the polytope αP1 reaches x for a smaller α than αP−1, hence,
subspace-sparse recovery holds. Middle: when the subspace angle decreases, the polytope αP−1 reaches x for a smaller α than αP1.
Right: when the distribution of the data in S1 becomes nearly degenerate, in this case close to a line, the polytope αP−1 reaches x for a
smaller α than αP1. In both cases, in the middle and right, the subspace-sparse recovery does not hold for points at the intersecion.

Remark 4: The condition in (21) is closely related to the
nullspace property in the sparse recovery literature [56], [57],
[45], [58]. The key difference, however, is that we only require
the inequality in (21) to hold for the optimal solutions of (19) and
(20) instead of any feasible solution. Thus, while the inequality
can be violated for many feasible solutions, it can still hold for
the optimal solutions, guaranteeing successful subspace-sparse
recovery from Theorem 2. Thus, our result can be thought of as
a generalization of the nullspace property to the multi-subspace
setting where the number of points in each subspace is arbitrary.

4.3 Geometric interpretation

In this section, we provide a geometric interpretation of the
subspace-sparse recovery conditions in (21) and (24). To do so,
it is necessary to recall the relationship between the (cid:96)1-norm of
the optimal solution of

min

a
(cid:107)1

(cid:107)

s. t. x = Ba,

(26)

and the symmetrized convex polytope of the columns of B [59].
More precisely, if we denote the columns of B by bi and deﬁne
the symmetrized convex hull of the columns of B by

(cid:44) conv(

b1,

b2,

),

(27)

P

±

· · ·

±
then the (cid:96)1-norm of the optimal solution of (26) corresponds to
the smallest α > 0 such that the scaled polytope α
reaches x
[59]. Let us denote the symmetrized convex polytopes of Y i and
i, respectively. Then the condition in (21) has
Y
the following geometric interpretation:

Pi and

i by

P−

P

−

the subspace-sparse recovery in
any nonzero x in the intersection of
reaches x before α

Si and
i, i.e., for a smaller α.

Si holds if and only if for
Pi
⊕j

=iSj, α

P−

P1 reaches x before α

S2 ⊕ S3, the polytope α

As shown in the left plot of Figure 5, for x in the intersection
of
1,
S1 and
P−
hence the subspace-sparse recovery condition holds. On the other
S1 and other subspaces
hand, when the principal angles between
decrease, as shown in the middle plot of Figure 5, the subspace-
sparse recovery condition does not hold since the polytope α
P−
reaches x before α
P1. Also, as shown in the right plot of Figure 5,
S1 becomes nearly degenerate,
when the distribution of the data in
in this case close to a 1-dimensional subspace orthogonal to
the direction of x, then the subspace-sparse recovery condition
does not hold since α
P1. Note that the
sufﬁcient condition in (24) translates the relationship between the
polytopes, mentioned above, explicitly in terms of a relationship
between the subspace angles and the singular values of the data.

1 reaches x before α

P−

1

Fig. 6. Coefﬁcient matrix obtained from the solution of (30) for data
points in two subspaces. Left: λr = 0. Right: λr = 10. Increasing
λr results in concentration of the nonzero elements in a few rows of
the coefﬁcient matrix, hence choosing a few common data points.

5 GRAPH CONNECTIVITY

In the previous section, we studied conditions under which
the proposed (cid:96)1-minimization program recovers subspace-sparse
representations of data points. As a result, in the similarity graph,
the points that lie in different subspaces do not get connected to
each other. On the other hand, our extensive experimental results
on synthetic and real data show that data points in the same
subspace always form a connected component of the graph, hence,
for n subspaces the similarity graph has n connected components.
[60] has theoretically veriﬁed the connectivity of points in the
same subspace for 2 and 3 dimensional subspaces. However, it
has shown that, for subspaces of dimensions greater than or equal
to 4, under odd distribution of the data, it is possible that points
in the same subspace form multiple components of the graph.

In this section, we consider a regularization term in the sparse
optimization program that promotes connectivity of the points
within each subspace.6 We use the idea that if data points in each
subspace choose a few common points from the same subspace in
their sparse representations, then they form a single component
of the similarity graph. Thus, we add to the sparse optimization
program the regularization term

C
(cid:107)

(cid:107)r,0 (cid:44)

I(

ci

(cid:107)

(cid:107)2 > 0),

(28)

N

i=1
(cid:88)

·

) denotes the indicator function and ci denotes the i-th
where I(
row of C. Hence, minimizing (28) corresponds to minimizing the
number of nonzero rows of C [61], [62], [63], i.e., choosing a few
common data points in the sparse representation of each point.

6. Another approach to deal with the connectivity issue is to analyze the sub-
spaces corresponding to the components of the graph and merge the components
whose associated subspaces have a small distance from each other, i.e., have a
small principal angle. However, the result can be sensitive to the choice of the
dimension of the subspaces to ﬁt to each component as well as the threshold value
on the principal angles to merge the subspaces.

9

Fig. 7. Left: three 1-dimensional subspaces in R2 with normalized data points. Middle: C 1 corresponds to the solution of (30) for λr = 0.
The similarity graph of C 1 has three components corresponding to the three subspaces. Right: C 2 corresponds to the solution of (30) for
λr → +∞ and θ ∈ (0, 4π

10 ). The similarity graph of C 2 has only one connected component.

Since a minimization problem that involves (28) is in general
NP-hard, we consider its convex relaxation as

C
(cid:107)

(cid:107)r,1 (cid:44)

ci

(cid:107)2.

(cid:107)

N

i=1
(cid:88)

(29)

Thus, to increase the connectivity of data points from the same
subspace in the similarity graph, we propose to solve

min

C
(cid:107)

C
(cid:107)1 + λr(cid:107)

(cid:107)r,1

s. t. Y = Y C, diag(C) = 0,

(30)

where λr > 0 sets the trade-off between the sparsity of the
solution and the connectivity of the graph. Figure 6 shows how
adding this regularization term promotes selecting common points
in sparse representations. The following example demonstrates the
reason for using the row-sparsity term as a regularizer but not as
an objective function instead of the (cid:96)1-norm.

(0, π

Example 1: Consider the three 1-dimensional subspaces in
R2, shown in Figure 7, where the data points have unit Euclidean
S1 and
S1 and
norms and the angle between
S2 as well as between
S3 is equal to θ. Note that in this example, the sufﬁcient condition
2 ). As a result, the solution
in (24) holds for all values of θ
∈
of (30) with λr = 0 recovers a subspace-sparse representation for
each data point, which in this example is uniquely given by C1
shown in Figure 7. Hence, the similarity graph has exactly 3
connected components corresponding to the data points in each
subspace. Another feasible solution of (30) is given by C2, shown
in Figure 7, where the points in
S3
in their representations. Hence, the similarity graph has only one
connected component. Note that for a large range of subspace
angles θ

S1 choose points from

S2 and

(0, 4π

10 ) we have

16 + 2/ cos2(θ) <

C1(cid:107)r,1 = 6.
(cid:107)

(31)

(cid:112)

As a result, for large values of λr, i.e., when we only minimize the
second term of the objective function in (30), we cannot recover
subspace-sparse representations of the data points. This suggests
using the row-sparsity regularizer with a small value of λr.

∈
C2(cid:107)r,1 =

(cid:107)

6 EXPERIMENTS WITH SYNTHETIC DATA
In Section 4, we showed that the success of the (cid:96)1-minimization
for subspace-sparse recovery depends on the principal angles
between subspaces and the distribution of the data in each
subspace. In this section, we verify this relationship through
experiments on synthetic data.

We consider three disjoint subspaces

3
i=1 of the same
{Si}
dimension d embedded in the D-dimensional ambient space. To
make the problem hard enough so that every data point in a sub-
space can also be reconstructed as a linear combination of points
3
in other subspaces, we generate subspace bases
i=1
}
such that each subspace lies in the direct sum of the other two

U i ∈
{

RD

×

d

Fig. 8.
Subspace-sparse recovery error (left) and subspace
clustering error (right) for three disjoint subspaces. Increasing the
number of points or smallest principal angle decreases the errors.

(cid:3)

(cid:2)

i.e., rank(

U 1 U 2 U 3

subspaces,
) = 2d. In addition, we
generate the subspaces such that the smallest principal angles
θ12 and θ23 are equal to θ. Thus, we can verify the effect of
the smallest principal angle in the subspace-sparse recovery by
changing the value of θ. To investigate the effect of the data
distribution in the subspace-sparse recovery, we generate the same
number of data points, Ng, in each subspace at random and
change the value of Ng. Typically, as the number of data points
in a subspace increases, the probability of the data being close to
a degenerate subspace decreases.7

After generating three d-dimensional subspaces associated to
(θ, Ng), we solve the (cid:96)1-minimization program in (4) for each
data point and measure two different errors. First, denoting the
,
sparse representation of yi ∈ Ski by c(cid:62)i
c(cid:62)i1
c(cid:62)i3
Sj, we measure the subspace-
with cij corresponding to points in
(cid:2)
(cid:3)
sparse recovery error by

c(cid:62)i2

(cid:44)

ssr error =

1
3Ng

3Ng

i=1
(cid:88)

(1

−

ciki (cid:107)1
(cid:107)
ci(cid:107)1
(cid:107)

)

[0, 1],

∈

(32)

where each term inside the summation indicates the fraction of
the (cid:96)1-norm of ci that comes from points in other subspaces. The
error being zero corresponds to yi choosing points only in its
own subspace, while the error being equal to one corresponds to
yi choosing points from other subspaces. Second, after building
the similarity graph using the sparse coefﬁcients and applying
spectral clustering, we measure the subspace clustering error by

subspace clustering error =

# of misclassiﬁed points
total # of points

.

(33)

In our experiments, we set the dimension of the ambient space
to D = 50. We change the smallest principal angle between
[6, 60] degrees and change the number of points
subspaces as θ
[d + 1, 32d]. For each pair (θ, Ng) we
in each subspace as Ng ∈

∈

7. To remove the effect of different scalings of data points, i.e., to consider
only the effect of the principal angle and number of points, we normalize the
data points.

10

Multipliers (ADMM) framework [50], [64] whose derivation is
provided in the supplementary material. For the motion segmen-
tation experiments, we use the noisy variation of the optimization
program (13), i.e., without the term E, with the afﬁne constraint,
and choose λz = 800/µz in all the experiments (µz is deﬁned
in (14)). For the face clustering experiments, we use the sparse
outlying entries variation of the optimization program (13), i.e.,
without the term Z, and choose λe = 20/µe in all the experiments
(µe is deﬁned in (14)). It is also worth mentioning that SSC
performs better with the ADMM approach than with general
interior point solvers [49], which typically return many small
nonzero coefﬁcients, degrading the spectral clustering result.

For the state-of-the-art algorithms, we use the codes provided
by their authors. For LSA, we use K = 8 nearest neighbors and
dimension d = 4, to ﬁt local subspaces, for motion segmentation
and use K = 7 nearest neighbors and dimension d = 5 for
face clustering. For SCC, we use dimension d = 3, for the
subspaces, for motion segmentation and d = 9 for face clustering.
For LRR, we use λ = 4 for motion segmentation and λ = 0.18
for face clustering. Note that the LRR algorithm according to
[38], similar to SSC, applies spectral clustering to a similarity
graph built directly from the solution of its proposed optimization
program. However, the code of the algorithm applies a heuristic
post-processing step, similar to [65], to the low-rank solution
prior to building the similarity graph [40]. Thus, to compare
the effectiveness of sparse versus low-rank objective function
and to investigate the effect of the post-processing step of LRR,
we report the results for both cases of without (LRR) and with
(LRR-H) the heuristic post-processing step.8 For LRSC, we use
the method in [41, Lemma 1] with parameter τ = 420 for
motion segmentation, and an ALM variant of the method in [41,
(1.25/σ1(Y ))2,
Section 4.2] with parameters α = 3τ = 0.5
γ = 0.008 and ρ = 1.5 for face clustering. Finally, as LSA and
SCC need to know the number of subspaces a priori and the
estimation of the number of subspaces from the eigenspectrum
of the graph Laplacian in the noisy setting is often unreliable,
in order to have a fair comparison, we provide the number of
subspaces as an input to all the algorithms.

∗

Datasets and some statistics. For the motion segmentation prob-
lem, we consider the Hopkins 155 dataset [66], which consists
of 155 video sequences of 2 or 3 motions corresponding to 2
or 3 low-dimensional subspaces in each video [2], [67]. For the
face clustering problem, we consider the Extended Yale B dataset
[68], which consists of face images of 38 human subjects, where
images of each subject lie in a low-dimensional subspace [3].

Before describing each problem in detail and presenting the
experimental results, we present some statistics on the two
datasets that help to better understand the challenges of subspace
clustering and the performance of different algorithms. First, we
compute the smallest principal angle for each pair of subspaces,
which in the motion segmentation problem corresponds to a pair
of motions in a video and in the face clustering problem corre-
sponds to a pair of subjects. Then, we compute the percentage
of the subspace pairs whose smallest principal angle is below
a certain value, which ranges from 0 to 90 degrees. Figure 9
(left) shows the corresponding graphs for the two datasets. As

8. The original published code of LRR contains the function “compacc.m”
for computing the misclassiﬁcation rate, which is erroneous. We have used the
correct code for computing the misclassiﬁcation rate and as a result, the reported
performance for LRR-H is different from the published results in [38] and [40].

Fig. 9. Left: percentage of pairs of subspaces whose smallest prin-
cipal angle is smaller than a given value. Right: average percentage
of data points in pairs of subspaces that have one or more of their
K-nearest neighbors in the other subspace.

Fig. 10.
Left: singular values of several motions in the Hop-
kins 155 dataset. Each motion corresponds to a subspace of
dimension at most 4. Right: singular values of several faces in the
Extended Yale B dataset. Each subject corresponds to a subspace
of dimension around 9.

compute the average of the errors in (32) and (33) over 100 trials
(randomly generated subspaces and data points). The results for
d = 4 are shown in Figure 8. Note that when either θ or Ng is
small, both the subspace-sparse recovery error and the clustering
error are large, as predicted by our theoretical analysis. On the
other hand, when θ or Ng increases, the errors decrease, and for
(θ, Ng) sufﬁciently large we obtain zero errors. The results also
verify that the success of the clustering relies on the success of the
(cid:96)1-minimization in recovering subspace-sparse representations of
data points. Note that for small θ as we increase Ng, the subspace-
sparse recovery error is large and slightly decreases, while the
clustering error increases. This is due to the fact that increasing
the number of points, the number of undesirable edges between
different subspaces in the similarity graph increases, making the
spectral clustering more difﬁcult. Note also that, for the values
of (θ, Ng) where the subspace-sparse recovery error is zero, i.e.,
points in different subspaces are not connected to each other in the
similarity graph, the clustering error is also zero. This implies that,
in such cases, the similarity graph has exactly three connected
components, i.e., data points in the same subspace form a single
component of the graph.

7 EXPERIMENTS WITH REAL DATA

In this section, we evaluate the performance of the SSC algorithm
in dealing with two real-world problems: segmenting multiple
motions in videos (Fig. 1) and clustering images of human faces
(Fig. 2). We compare the performance of SSC with the best state-
of-the-art subspace clustering algorithms: LSA [22], SCC [28],
LRR [38], and LRSC [41].

Implementation details. We implement the SSC optimization
algorithm in (13) using an Alternating Direction Method of

11

TABLE 1
Clustering error (%) of different algorithms on the Hopkins 155 dataset with
the 2F -dimensional data points.

TABLE 2
Clustering error (%) of different algorithms on the Hopkins 155 dataset with
the 4n-dimensional data points obtained by applying PCA.

Algorithms LSA SCC LRR LRR-H LRSC
2 Motions
Mean
Median
3 Motions
Mean
Median

4.23 2.89 4.10
0.22
0.56 0.00

7.02 8.25 9.89
6.22
1.45 0.24

2.13
0.00

3.69
0.29

7.69
3.80

4.03
1.43

SSC

1.52 (2.07)
0.00 (0.00)

4.40 (5.27)
0.56 (0.40)

All

Algorithms LSA SCC LRR LRR-H LRSC
2 Motions
Mean
Median
3 Motions
Mean
Median

3.61 3.04 4.83
0.26
0.51 0.00

7.65 7.91 9.89
1.27 1.14 6.22

3.41
0.00

3.87
0.26

7.72
3.80

4.86
1.47

SSC

1.83 (2.14)
0.00 (0.00)

4.40 (5.29)
0.56 (0.40)

All

Mean
Median

4.86 4.10 5.41
0.53
0.89 0.00

2.56
0.00

4.59
0.60

2.18 (2.79)
0.00 (0.00)

Mean
Median

4.52 4.14 5.98
0.59
0.57 0.00

3.74
0.00

4.74
0.58

2.41 (2.85)
0.00 (0.00)

shown, subspaces in both datasets have relatively small principal
angles. In the Hopkins-155 dataset, principal angles between
subspaces are always smaller than 10 degrees, while in the
Extended Yale B dataset, principal angles between subspaces are
between 10 and 20 degrees. Second, for each pair of subspaces,
we compute the percentage of data points that have one or more of
their K-nearest neighbors in the other subspace. Figure 9 (right)
shows the average percentages over all possible pairs of subspaces
in each dataset. As shown, in the Hopkins-155 dataset for almost
all data points, their few nearest neighbors belong to the same
subspace. On the other hand, for the Extended Yale B dataset,
there is a relatively large number of data points whose nearest
neighbors come from the other subspace. This percentage rapidly
increases as the number of nearest neighbors increases. As a
result, from the two plots in Figure 9, we can conclude that in the
Hopkins 155 dataset the challenge is that subspaces have small
principal angles, while in the Extended Yale B dataset, beside the
principal angles between subspaces being small, the challenge is
that data points in a subspace are very close to other subspaces.

7.1 Motion Segmentation

Motion segmentation refers to the problem of segmenting a
video sequence of multiple rigidly moving objects into multiple
spatiotemporal regions that correspond to different motions in
the scene (Fig. 1). This problem is often solved by extracting and
N
tracking a set of N feature points
i=1 through the
}
frames f = 1, . . . , F of the video. Each data point yi, which is
also called a feature trajectory, corresponds to a 2F -dimensional
vector obtained by stacking the feature points xf i in the video as

xf i ∈
{

R2

(34)

(cid:44)

yi

x(cid:62)1i x(cid:62)2i

x(cid:62)F i

(cid:62)

R2F .

(cid:3)

(cid:2)

∈

· · ·
Motion segmentation refers to the problem of separating these
feature trajectories according to their underlying motions. Under
the afﬁne projection model, all feature trajectories associated with
a single rigid motion lie in an afﬁne subspace of R2F of dimen-
sion at most 3, or equivalently lie in a linear subspace of R2F of
dimension at most 4 [2], [67]. Therefore, feature trajectories of
n rigid motions lie in a union of n low-dimensional subspaces of
R2F . Hence, motion segmentation reduces to clustering of data
points in a union of subspaces.

In this section, we evaluate the performance of the SSC
algorithm as well as that of state-of-the-art subspace clustering
methods for the problem of motion segmentation. To do so, we
consider the Hopkins 155 dataset [66] that consists of 155 video
sequences, where 120 of the videos have two motions and 35 of
the videos have three motions. On average, in the dataset, each

sequence of 2 motions has N = 266 feature trajectories and
F = 30 frames, while each sequence of 3 motions has N = 398
feature trajectories and F = 29 frames. The left plot of Figure
10 shows the singular values of several motions in the dataset.
Note that the ﬁrst four singular values are nonzero and the rest
of the singular values are very close to zero, corroborating the 4-
dimensionality of the underlying linear subspace of each motion.9
In addition, it shows that the feature trajectories of each video can
be well modeled as data points that almost perfectly lie in a union
of linear subspaces of dimension at most 4.

The results of applying subspace clustering algorithms to
the dataset when we use the original 2F -dimensional feature
trajectories and when we project the data into a 4n-dimensional
subspace (n is the number of subspaces) using PCA are shown
in Table 1 and Table 2, respectively. From the results, we make
the following conclusions:

– In both cases, SSC obtains a small clustering error outper-
forming the other algorithms. This suggests that the separation of
different motion subspaces in terms of their principal angles and
the distribution of the feature trajectories in each motion subspace
are sufﬁcient for the success of the sparse optimization program,
hence clustering. The numbers inside parentheses show the clus-
tering errors of SSC without normalizing the similarity matrix,
i.e., without step 2 in Algorithm 1. Notice that, as explained in
Remark 1, the normalization step helps to improve the clustering
results. However, this improvement is small (about 0.5%), i.e.,
SSC performs well with or without the post-processing of C.

– Without post-processing of its coefﬁcient matrix, LRR has
higher errors than other algorithms. On the other hand, post-
processing of the low-rank coefﬁcient matrix signiﬁcantly im-
proves the clustering performance (LRR-H).

– LRSC tries to ﬁnd a noise-free dictionary for data while ﬁnding
their low-rank representation. This helps to improve over LRR.
Also, note that the errors of LRSC are higher than the reported
ones in [41]. This comes from the fact that [41] has used the
erroneous compacc.m function from [32] to compute the errors.

– The clustering performances of different algorithms when using
the 2F -dimensional feature trajectories or the 4n-dimensional
PCA projections are close. This comes from the fact that the
feature trajectories of n motions in a video almost perfectly
lie in a 4n-dimensional linear subspace of the 2F -dimensional
ambient space. Thus, projection using PCA onto a 4n-dimensional
subspace preserves the structure of the subspaces and the data,

9. If we subtract the mean of the data points in each motion from them, the
singular values drop at 3, showing the 3-dimensionality of the afﬁne subspaces.

12

To study the effect of the number of subjects in the clustering
performance of different algorithms, we devise the following
experimental setting: we divide the 38 subjects into 4 groups,
where the ﬁrst three groups correspond to subjects 1 to 10, 11 to
20, 21 to 30, and the fourth group corresponds to subjects 31 to
38. For each of the ﬁrst three groups we consider all choices of
subjects and for the last group we consider
n
2, 3, 5, 8, 10
.10 Finally, we apply clustering
all choices of n
2, 3, 5, 8
}
algorithms for each trial, i.e., each set of n subjects.

}
∈ {

∈ {

7.2.1 Applying RPCA separately on each subject

As shown by the SVD plot of the face data in Figure 10 (right),
the face images do not perfectly lie in a linear subspace as they
are corrupted by errors. In fact, the errors correspond to the
cast shadows and specularities in the face images and can be
modeled as sparse outlying entries. As a result, it is important
for a subspace clustering algorithm to effectively deal with data
with sparse corruptions.

In order to validate the fact that corruption of faces is due
to sparse outlying errors and show the importance of dealing
with corruptions while clustering, we start by the following
experiment. We apply the Robust Principal Component Analysis
(RPCA) algorithm [32] to remove the sparse outlying entries
of the face data in each subject. Note that in practice, we do
not know the clustering of the data beforehand, hence cannot
apply the RPCA to the faces of each subject. However, as we
will show, this experiment illustrates some of the challenges of
the face clustering and validates several conclusions about the
performances of different algorithms.

Table 3 shows the clustering error of different algorithms after
applying RPCA to the data points in each subject and removing
the sparse outlying entries, i.e., after bringing the data points back
to their low-dimensional subspaces. From the results, we make
the following conclusions:

– The clustering error of SSC is very close to zero for different
number of subjects suggesting that SSC can deal well with face
clustering if the face images are corruption free. In other words,
while the data in different subspaces are very close to each other,
as shown in Figure 9 (right), the performance of the SSC is
more dependent on the principal angles between subspaces which,
while small, are large enough for the success of SSC.

– The LRR and LRSC algorithms have also low clustering errors
(LRSC obtains zero errors) showing the effectiveness of removing
sparse outliers in the clustering performance. On the other hand,
while LRR-H has a low clustering error for 2, 3, and 5 subjects, it
has a relatively large error for 8 and 10 subjects, showing that the
post processing step on the obtained low-rank coefﬁcient matrix
not always improves the result of LRR.

– For LSA and SCC, the clustering error is relatively large and the
error increases as the number of subjects increases. This comes
from the fact that, as shown in Figure 9 (right), for face images,
the neighborhood of each data point contains points that belong
to other subjects and, in addition, the number of neighbors from
other subjects increases as we increase the number of subjects.

10. Note that choosing n out of 38 leads to extremely large number of trials.
Thus, we have devised the above setting in order to have a repeatable experiment
with a reasonably large number of trials for each n.

Fig. 11. Clustering error (%) of SSC as a function of αz in the
regularization parameter λz = αz/µz for the two cases of clustering
of 2F -dimensional data and 4n-dimensional data obtained by PCA.

hence, for each algorithm, the clustering error in Table 1 is close
to the error in Table 2.

In Figure 11 we show the effect of the regularization parameter
λz = αz/µz in the clustering performance of SSC over the entire
Hopkins 155 dataset. Note that the clustering errors of SSC as
a function of αz follow a similar pattern using both the 2F -
dimensional data and the 4n-dimensional data. Moreover, in both
cases the clustering error is less than 2.5% in both cases for a
large range of values of αz.

Finally, notice that the results of SSC in Tables 1-2 do not
coincide with those reported in [35]. This is mainly due to the fact
in [35] we used random projections for dimensionality reduction,
while here we use PCA or the original 2F -dimensional data. In
addition, in [35] we used a CVX solver to compute a subspace-
sparse representation, while here we use an ADMM solver. Also,
notice that we have improved the overall clustering error of LSA,
for the the case of 4n-dimensional data, from 4.94%, reported in
[66], [35], to 4.52%. This is due to using K = 8 nearest neighbors
here instead of K = 5 in [66].

7.2 Face Clustering

Given face images of multiple subjects, acquired with a ﬁxed pose
and varying illumination, we consider the problem of clustering
images according to their subjects (Fig. 2). It has been shown
that, under the Lambertian assumption, images of a subject with
a ﬁxed pose and varying illumination lie close to a linear subspace
of dimension 9 [3]. Thus, the collection of face images of multiple
subjects lie close to a union of 9-dimensional subspaces.

×

In this section, we evaluate the clustering performance of SSC
as well as the state-of-the-art methods on the Extended Yale B
dataset [68]. The dataset consists of 192
168 pixel cropped face
images of n = 38 individuals, where there are Ni = 64 frontal
face images for each subject acquired under various lighting
conditions. To reduce the computational cost and the memory
requirements of all algorithms, we downsample the images to
42 pixels and treat each 2, 016-dimensional vectorized image
48
as a data point, hence, D = 2, 016. The right plot in Figure 10
shows the singular values of data points of several subjects in the
dataset. Note that the singular value curve has a knee around 9,
corroborating the approximate 9-dimensionality of the face data
in each subject. In addition, the singular values gradually decay
to zero, showing that the data are corrupted by errors. Thus, the
face images of n subjects can be modeled as corrupted data points
lying close to a union of 9-dimensional subspaces.

×

13

TABLE 3
Clustering error (%) of different algorithms on the Extended Yale B dataset
after applying RPCA separately to the data points in each subject.

TABLE 5
Clustering error (%) of different algorithms on the Extended Yale B dataset
without pre-processing the data.

0.06
0.00

0.00
0.00

0.05
0.00

0.09
0.00

1.29
0.00

6.15
0.00

11.67 19.33 0.12
8.59
2.60
0.00

Algorithm LSA SCC LRR LRR-H LRSC SSC
2 Subjects
Mean
Median
3 Subjects
Mean
Median
5 Subjects
Mean
Median
8 Subjects
Mean
Median
10 Subjects
Mean
Median

21.08 47.53 0.16
19.21 47.19 0.00

35.31 63.80 0.15
30.16 64.84 0.00

30.04 64.20 4.50
29.00 63.77 0.20

11.57
15.43

13.02
13.13

0.10
0.00

0.15
0.00

0.00
0.00

0.00
0.00

0.00
0.00

0.00
0.00

0.08
0.00

0.07
0.00

0.06
0.00

0.89
0.31

9.52
5.47

32.80 16.62
7.82
47.66

Algorithm LSA SCC
2 Subjects
Mean
Median
3 Subjects
Mean
Median
5 Subjects
Mean
Median
8 Subjects
Mean
Median
10 Subjects
Mean
Median

52.29 38.16 19.52
50.00 39.06 14.58

58.02 58.90 34.16
56.87 59.38 35.00

LRR LRR-H LRSC SSC

2.54
0.78

4.21
2.60

6.90
5.63

5.32
4.69

8.47
7.81

12.24
11.25

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

59.19 66.11 41.19
58.59 64.65 43.75

14.34
10.06

23.72
28.03

60.42 73.02 38.85
57.50 75.78 41.09

22.92
23.59

30.36 10.94
28.75
5.63

TABLE 4
Clustering error (%) of different algorithms on the Extended Yale B dataset
after applying RPCA simultaneously to all the data in each trial.

9.29
7.03

32.53
47.66

Algorithm LSA SCC
2 Subjects
Mean
Median
3 Subjects
Mean
Median
5 Subjects
Mean
Median
8 Subjects
Mean
Median
10 Subjects
Mean
Median

LRR LRR-H LRSC SSC

7.27
6.25

5.72
3.91

53.02 32.00 12.29
51.04 37.50 11.98

10.01
9.38

58.76 53.05 19.92
56.87 51.25 19.38

15.33
15.94

10.99
10.94

5.67
4.69

8.72
8.33

2.09
0.78

3.77
2.60

6.79
5.31

62.32 66.27 31.39
62.50 64.84 33.30

28.67
31.05

16.14 10.28
14.65
9.57

62.40 63.07 35.89
62.50 60.31 34.06

32.55
30.00

21.82 11.46
25.00 11.09

7.2.2 Applying RPCA simultaneously on all subjects
In practice, we cannot apply RPCA separately to the data in each
subject because the clustering is unknown. In this section, we deal
with sparse outlying entries in the data by applying the RPCA
algorithm to the collection of all data points for each trial prior
to clustering. The results are shown in Table 4 from which we
make the following conclusions:

– The clustering error for SSC is low for all different number
of subjects. Speciﬁcally, SSC obtains 2.09% and 11.46% for
clustering of data points in 2 and 10 subjects, respectively.
– Applying RPCA to all data points simultaneously may not be
as effective as applying RPCA to data points in each subject
separately. This comes from the fact that RPCA tends to bring the
data points into a common low-rank subspace which can result in
decreasing the principal angles between subspaces and decreasing
the distances between data points in different subjects. This
can explain the increase in the clustering error of all clustering
algorithms with respect to the results in Table 3.

7.2.3 Using original data points
Finally, we apply the clustering algorithms to the original data
points without pre-processing the data. The results are shown in
Table 5 from which we make the following conclusions:

– The SSC algorithm obtains a low clustering error for all
numbers of subjects, obtaining 1.86% and 10.94% clustering error
for 2 and 10 subjects, respectively. In fact, the error is smaller than
when applying RPCA to all data points. This is due to the fact
that SSC directly incorporates the corruption model of the data
by sparse outlying entries into the sparse optimization program,
giving it the ability to perform clustering on the corrupted data.
– While LRR also has a regularization term to deal with the
corrupted data, the clustering error is relatively large especially
as the number of subjects increases. This can be due to the fact
that there is not a clear relationship between corruption of each
data point and the LRR regularization term in general [38]. On
the other hand, the post processing step of LRR-H on the low-
rank coefﬁcient matrix helps to signiﬁcantly reduce the clustering
error, although it is larger than the SSC error.
– As LRSC tries to recover error-free data points while ﬁnding
their low-rank representation, it obtains smaller errors than LRR.
– LSA and SCC do not have an explicit way to deal with corrupted
data. This together with the fact that the face images of each
subject have relatively a large number of neighbors in other
subjects, as shown in Figure 9 (right), result in low performances
of these algorithms.

7.2.4 Computational time comparison
The average computational time of each algorithm as a function
of the number of subjects (or equivalently the number of data
points) is shown in Figure 12. Note that the computational time
of SCC is drastically higher than other algorithms. This comes
from the fact that the complexity of SCC increases exponentially
in the dimension of the subspaces, which in this case is d = 9.
On the other hand, SSC, LRR and LRSC use fast and efﬁcient
convex optimization techniques which keeps their computational
time lower than other algorithms. The exact computational times
are provided in the supplementary materials.

8 CONCLUSIONS AND FUTURE WORK
We studied the problem of clustering a collection of data points
that lie in or close to a union of low-dimensional subspaces.
We proposed a subspace clustering algorithm based on sparse
representation techniques, called SSC, that ﬁnds a sparse rep-
resentation of each point in the dictionary of the other points,

14

[10] R. Vidal, Y. Ma, and S. Sastry, “Generalized Principal Component Analysis
(GPCA),” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 27, no. 12, pp. 1–15, 2005.

[11] R. Duda, P. Hart, and D. Stork, Pattern Classiﬁcation. Wiley-Interscience,

October 2004.

[12] P. Tseng, “Nearest q-ﬂat to m points,” Journal of Optimization Theory and

Applications, vol. 105, no. 1, pp. 249–252, 2000.

[13] J. Ho, M. H. Yang, J. Lim, K. Lee, and D. Kriegman, “Clustering appear-
ances of objects under varying illumination conditions.” in IEEE Conf. on
Computer Vision and Pattern Recognition, 2003.

[14] T. Zhang, A. Szlam, and G. Lerman, “Median k-ﬂats for hybrid linear

modeling with many outliers,” in Workshop on Subspace Methods, 2009.

[15] C. W. Gear, “Multibody grouping from motion images,” Int. Journal of

Computer Vision, vol. 29, no. 2, pp. 133–150, 1998.

[16] Y. Ma, A. Yang, H. Derksen, and R. Fossum, “Estimation of subspace
arrangements with applications in modeling and segmenting mixed data,”
SIAM Review, 2008.

[17] M. Tipping and C. Bishop, “Mixtures of probabilistic principal component
analyzers,” Neural Computation, vol. 11, no. 2, pp. 443–482, 1999.
[18] Y. Sugaya and K. Kanatani, “Geometric structure of degeneracy for multi-
body motion segmentation,” in Workshop on Statistical Methods in Video
Processing, 2004.

[19] A. Gruber and Y. Weiss, “Multibody factorization with uncertainty and
missing data using the EM algorithm,” in IEEE Conf. on Computer Vision
and Pattern Recognition, vol. I, 2004, pp. 707–714.

[20] M. A. Fischler and R. C. Bolles, “RANSAC random sample consensus: A
paradigm for model ﬁtting with applications to image analysis and automated
cartography,” Communications of the ACM, vol. 26, pp. 381–395, 1981.

[21] S. Rao, R. Tron, R. Vidal, and Y. Ma, “Motion segmentation in the presence
of outlying, incomplete, or corrupted trajectories,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 32, no. 10, 2010.

[22] J. Yan and M. Pollefeys, “A general framework for motion segmentation:
Independent, articulated, rigid, non-rigid, degenerate and non-degenerate,”
in European Conf. on Computer Vision, 2006, pp. 94–106.

[23] A. Goh and R. Vidal, “Segmenting motions of different types by unsuper-
vised manifold clustering,” in IEEE Conference on Computer Vision and
Pattern Recognition, 2007.

[24] T. Zhang, A. Szlam, Y. Wang, and G. Lerman, “Hybrid linear modeling via
local best-ﬁt ﬂats,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2010, pp. 1927–1934.

[25] L. Zelnik-Manor and M. Irani, “Degeneracies, dependencies and their
implications in multi-body and multi-sequence factorization,” in IEEE Conf.
on Computer Vision and Pattern Recognition, vol. 2, 2003, pp. 287–293.

[26] A. Ng, Y. Weiss, and M. Jordan, “On spectral clustering: analysis and an
algorithm,” in Neural Information Processing Systems, 2001, pp. 849–856.
[27] U. von Luxburg, “A tutorial on spectral clustering,” Statistics and Comput-

ing, vol. 17, 2007.

[28] G. Chen and G. Lerman, “Spectral curvature clustering (SCC),” International

Journal of Computer Vision, vol. 81, no. 3, pp. 317–330, 2009.

[29] D. L. Donoho, “For most large underdetermined systems of linear equations
the minimal (cid:96)1-norm solution is also the sparsest solution,” Communications
on Pure and Applied Mathematics, vol. 59, no. 6, pp. 797–829, 2006.
[30] E. Cand`es and T. Tao, “Decoding by linear programming,” IEEE Trans. on

Information Theory, vol. 51, no. 12, pp. 4203–4215, 2005.

[31] R. Tibshirani, “Regression shrinkage and selection via the LASSO,” Journal
of the Royal Statistical Society B, vol. 58, no. 1, pp. 267–288, 1996.
[32] E. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal component

analysis,” Journal of the ACM, vol. 58, 2011.

[33] E. Cand`es and B. Recht, “Exact matrix completion via convex optimization,”
Foundations of Computational Mathematics, vol. 9, pp. 717–772, 2008.
[34] B. Recht, M. Fazel, and P. Parrilo, “Guaranteed minimum-rank solutions
of linear matrix equations via nuclear norm minimization,” SIAM Review,
vol. 52, no. 3, pp. 471–501, 2010.

[35] E. Elhamifar and R. Vidal, “Sparse subspace clustering,” in IEEE Conference

on Computer Vision and Pattern Recognition, 2009.

[36] ——, “Clustering disjoint subspaces via sparse representation,” in IEEE
International Conference on Acoustics, Speech, and Signal Processing, 2010.
[37] M. Soltanolkotabi and E. J. Candes, “A geometric analysis of subspace

clustering with outliers,” Annals of Statistics, 2012.

[38] G. Liu, Z. Lin, and Y. Yu, “Robust subspace segmentation by low-rank

representation,” in International Conference on Machine Learning, 2010.

[39] G. Liu and S. Yan, “Latent low-rank representation for subspace segmenta-
tion and feature extraction,” International Conference on Computer Vision,
2011.

[40] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery
of subspace structures by low-rank representation,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2012.

Fig. 12. Average computational time (sec.) of the algorithms on the
Extended Yale B dataset as a function of the number of subjects.

builds a similarity graph using the sparse coefﬁcients, and ob-
tains the segmentation of the data using spectral clustering. We
showed that, under appropriate conditions on the arrangement of
subspaces and the distribution of data, the algorithm succeeds
in recovering the desired sparse representations of data points.
A key advantage of the algorithm is its ability to directly deal
with data nuisances, such as noise, sparse outlying entries, and
missing entries as well as the more general class of afﬁne
subspaces by incorporating the corresponding models into the
sparse optimization program. Experiments on real data such as
face images and motions in videos showed the effectiveness of
our algorithm and its superiority over the state of the art.

Interesting avenues of research, which we are currently investi-
gating, include theoretical analysis of the subspace-sparse recov-
ery in the presence of noise, sparse outlying entries, and missing
entries in the data. As our extensive experiments on synthetic and
real data show, the points in each subspace, in general, form a
single component of the similarity graph. Theoretical analysis of
the connectivity of the similarity graph for points in the same
subspace in a probabilistic framework would provide a better
understanding for this observation. Finally, making the two steps
of solving a sparse optimization program and spectral clustering
applicable to very large datasets is an interesting and a practical
subject for the future work.

ACKNOWLEDGMENT
The authors would like to thank the ﬁnancial support of grants
nsf-iss 0447739 and nsf-csn 0931805.

REFERENCES
[1] R. E. Bellman, Dynamic programming. Princeton University Press, 1957.
[2] C. Tomasi and T. Kanade, “Shape and motion from image streams under
orthography,” International Journal of Computer Vision, vol. 9, no. 2, pp.
137–154, 1992.

[3] R. Basri and D. Jacobs, “Lambertian reﬂection and linear subspaces,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 3,
pp. 218–233, 2003.

[4] T. Hastie and P. Simard, “Metrics and models for handwritten character

recognition,” Statistical Science, vol. 13, no. 1, pp. 54–65, 1998.

[5] R. Vidal, “Subspace clustering,” Signal Processing Magazine, vol. 28, no. 2,

pp. 52–68, 2011.

[6] W. Hong, J. Wright, K. Huang, and Y. Ma, “Multi-scale hybrid linear models
for lossy image representation,” IEEE Trans. on Image Processing, vol. 15,
no. 12, pp. 3655–3671, 2006.

[7] A. Yang, J. Wright, Y. Ma, and S. Sastry, “Unsupervised segmentation of
natural images via lossy data compression,” Computer Vision and Image
Understanding, vol. 110, no. 2, pp. 212–225, 2008.
J. Costeira and T. Kanade, “A multibody factorization method for indepen-
dently moving objects.” Int. Journal of Computer Vision, vol. 29, no. 3,
1998.

[8]

[9] K. Kanatani, “Motion segmentation by subspace separation and model
selection,” in IEEE Int. Conf. on Computer Vision, vol. 2, 2001, pp. 586–591.

[41] P. Favaro, R. Vidal, and A. Ravichandran, “A closed form solution to robust
subspace estimation and clustering,” in IEEE Conference on Computer
Vision and Pattern Recognition, 2011.

[42] E. Elhamifar and R. Vidal, “Robust classiﬁcation using structured sparse
representation,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2011.

[43] ——, “Block-sparse recovery via convex optimization,” IEEE Transactions

on Signal Processing, 2012.

[44] F. Parvaresh, H. Vikalo, S. Misra, and B. Hassibi, “Recovering sparse signals
using sparse measurement matrices in compressed dna microarrays,” IEEE
Journal of Selected Topics in Signal Processing, vol. 2, no. 3, pp. 275–285,
Jun. 2008.

[45] M. Stojnic, F. Parvaresh, and B. Hassibi, “On the reconstruction of block-
sparse signals with and optimal number of measurements,” IEEE Trans.
Signal Processing, vol. 57, no. 8, pp. 3075–3085, Aug. 2009.

[46] Y. C. Eldar and M. Mishali, “Robust recovery of signals from a structured
union of subspaces,” IEEE Trans. Inform. Theory, vol. 55, no. 11, pp. 5302–
5316, 2009.

[47] Y. C. Eldar, P. Kuppinger, and H. Bolcskei, “Compressed sensing of block-
sparse signals: Uncertainty relations and efﬁcient recovery,” IEEE Trans.
Signal Processing, vol. 58, no. 6, pp. 3042–3054, June 2010.

[48] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University

Press, 2004.

[49] S. J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky, “An interior-
point method for large-scale l1-regularized least squares,” IEEE Journal on
Selected Topics in Signal Processing, vol. 1, no. 4, pp. 606–617, 2007.
[50] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction method of
multipliers,” Foundations and Trends in Machine Learning, vol. 3, no. 1,
pp. 1–122, 2010.

[51] E. Amaldi and V. Kann, “On the approximability of minimizing nonzero
variables or unsatisﬁed relations in linear systems,” Theoretical Computer
Science, vol. 209, pp. 237–260, 1998.

[52] R. Zass and A. Shashua, “Doubly stochastic normalization for spectral

clustering,” Neural Information Processing Systems, 2006.

[53] T. Brox and J. Malik, “Object segmentation by long term analysis of point

trajectories,” European Conference on Computer Vision, 2010.

[54] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma, “Robust face recogni-
tion via sparse representation,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 31, no. 2, pp. 210–227, Feb. 2009.

[55] R. Vidal and R. Hartley, “Motion segmentation with missing data by Pow-
erFactorization and Generalized PCA,” in IEEE Conference on Computer
Vision and Pattern Recognition, vol. II, 2004, pp. 310–316.

[56] D. L. Donoho and M. Elad, “Optimally sparse representation in general
(nonorthogonal) dictionaries via (cid:96)1 minimization,” PNAS, vol. 100, no. 5,
pp. 2197–2202, 2003.

[57] R. Gribonval and M. Nielsen, “Sparse representations in unions of bases,”
IEEE Trans. Information Theory, vol. 49, no. 12, pp. 3320–3325, Dec. 2003.
[58] E. van den Berg and M. Friedlander, “Theoretical and empirical results for
recovery from multiple measurements,” IEEE Trans. Information Theory,
vol. 56, no. 5, pp. 2516–2527, 2010.

[59] D. L. Donoho, “Neighborly polytopes and sparse solution of underdeter-
mined linear equations,” Technical Report, Stanford University, 2005.
[60] B. Nasihatkon and R. Hartley, “Graph connectivity in sparse subspace clus-
tering,” in IEEE Conference on Computer Vision and Pattern Recognition,
2011.

[61] E. Elhamifar, G. Sapiro, and R. Vidal, “See all by looking at a few:
Sparse modeling for ﬁnding representative objects,” in IEEE Conference
on Computer Vision and Pattern Recognition, 2012.

[62] R. Jenatton, J. Y. Audibert, and F. Bach, “Structured variable selection with
sparsity-inducing norms,” Journal of Machine Learning Research, vol. 12,
pp. 2777–2824, 2011.

[63] J. A. Tropp., “Algorithms for simultaneous sparse approximation. part ii:
Convex relaxation,” Signal Processing, special issue ”Sparse approximations
in signal and image processing”, vol. 86, pp. 589–602, 2006.

[64] D. Gabay and B. Mercier, “A dual algorithm for the solution of nonlinear
variational problems via ﬁnite-element approximations,” Comp. Math. Appl.,
vol. 2, pp. 17–40, 1976.

[65] F. Lauer and C. Schn¨orr, “Spectral clustering of linear subspaces for motion
segmentation,” in IEEE International Conference on Computer Vision, 2009.
[66] R. Tron and R. Vidal, “A benchmark for the comparison of 3-D motion
segmentation algorithms,” in IEEE Conference on Computer Vision and
Pattern Recognition, 2007.

[67] T. Boult and L. Brown, “Factorization-based segmentation of motions,” in

IEEE Workshop on Motion Understanding, 1991, pp. 179–186.

15

(35)

(36)

(37)

(38)

APPENDIX
PROOF OF PROPOSITION 1
In this section, we prove the result of Proposition 1 in the paper
regarding the optimization program

C

min

(cid:107)1 +
s. t. Y = Y C + E + Z, diag(C) = 0.

E
(cid:107)1 + λe(cid:107)

Z

(cid:107)

(cid:107)

2
F

λz
2 (cid:107)

The result of the proposition suggests to set the regularization
parameters as

λe = αe/µe,

λz = αz/µz,

where αe, αz > 1 and µe and µz are deﬁned as

µe (cid:44) min

i

max
j

=i (cid:107)

yj(cid:107)1,

µz (cid:44) min

i

max
=i |
j

y(cid:62)i yj|

.

We use the following Lemma in the theoretical proof of the
proposition. Proof of this Lemma can be found in [49].

Lemma 1: Consider the optimization program

min

c

(cid:107)1 +

(cid:107)

λ
2 (cid:107)

y

Ac

2
2.
(cid:107)

−

For λ <

A(cid:62)y

, we have c = 0.

(cid:107)

(cid:107)∞

the term Z,

Proposition 1: Consider the optimization program (35). With-
out
least
1/µe,
one data point y(cid:96) for which in the optimal solution we have
(c(cid:96), e(cid:96)) = (0, y(cid:96)). Also, without the term E, if λz ≤
1/µz, then
there exists at least one data point y(cid:96) for which (c(cid:96), z(cid:96)) = (0, y(cid:96)).

then there exists at

if λe ≤

Proof: Note that solving the optimization program (35) is

equivalent to solving N optimization programs as

min

ci(cid:107)1 + λe(cid:107)

ei(cid:107)1 +

(cid:107)

λz
2 (cid:107)

2
2

zi(cid:107)

s. t. yi = Y ci + ei + zi, cii = 0,

(39)

where ci, ei, and zi are the i-th columns of C, E, and Z,
repsectively.
(a) Consider the optimization program (39) without the term zi
and denote the objective function value by

cost(ci, ei) (cid:44)

ci(cid:107)1 + λe(cid:107)
(cid:107)
Note that a feasible solution of (39) is given by (0, ei) for which
the value of the objective function is equal to

ei(cid:107)1.

(40)

cost(0, ei) = λe(cid:107)

yi(cid:107)1.

(41)

On the other hand, using matrix norm properties, for any feasible
solution (ci, ei) of (39) we have

yi(cid:107)1 =

(cid:107)

Y ci + ei(cid:107)1 ≤
(cid:107)

(max
j

=i (cid:107)

yj(cid:107)1)

ci(cid:107)1 +
(cid:107)

ei(cid:107)1,
(cid:107)

(42)

where we used the fact that cii = 0. Multiplying both sides of
the above inequality by λe we obtain

Y
cost(0, yi) = λe(cid:107)

(cid:107)1 ≤

(λe max
j

=i (cid:107)

yj(cid:107)1)

ci(cid:107)1 + λe(cid:107)
(cid:107)

ei(cid:107)1,

(43)
, then from the above equation we

cost(0, yi)

cost(ci, ei).

≤

(44)

[68] K.-C. Lee, J. Ho, and D. Kriegman, “Acquiring linear subspaces for face
recognition under variable lighting,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 27, no. 5, pp. 684–698, 2005.

Note that if λe <
have

1
maxj(cid:54)=i

yj (cid:107)

(cid:107)

1

1

(cid:107)

1,

yj (cid:107)

1
maxj(cid:54)=i

In other words, (ci = 0, ei = yi) achieve the minimum
if λe <
cost among all feasible solutions of (39). Hence,
such that in
, then there exists (cid:96)
maxi
the solution of the optimization program (35) we have (c(cid:96), e(cid:96)) =
(0, y(cid:96)).
(b) Consider the optimization program (39) without the term ei,
which, using zi = yi −
λz
2 (cid:107)

Y ci, can be rewritten as

ci(cid:107)1 +

Y ci(cid:107)

cii = 0.

yi −

min

(45)

s. t.

∈ {

, N

· · ·

2
2

(cid:107)

}

From Lemma 1 we have that, for λz <
, the
solution of (45) is equal to ci = 0, or equivalently, the so-
lution of (39) is given by (ci, zi) = (0, yi). As a result, if
such
λz < maxi
that in the solution of the optimization program (35) we have
(c(cid:96), z(cid:96)) = (0, y(cid:96)).

, then there exists (cid:96)

1
maxj(cid:54)=i

y(cid:62)
j yi|
|

yj (cid:62)
|

maxj(cid:54)=i

∈ {

yi|

, N

· · ·

1,

}

1

PROOF OF THEOREM 1

In this section, we prove Theorem 1 in the paper, where we
showed that for data points in a union of independent subspaces,
the solution of the (cid:96)q-minimization recovers subspace-sparse
representations of data points.

n
i=1 of dimensions

Theorem 1: Consider a collection of data points drawn from
n
i=1. Let Y i
i
−
Si

n independent subspaces
denote Ni data points in
denote data points in all subspaces except
and every nonzero y in

di}
{Si}
{
Si, where rank(Y i) = di, and let Y
Si. Then, for every
Si, the (cid:96)q-minimization program
c
c

s. t. y = [Y i Y

= argmin

c
c

(46)

i]

−

,

(cid:20)

−(cid:21)

, recovers a subspace-sparse representation, i.e., c∗
= 0.

=

Proof: We prove the result using contradiction. Assume

c∗
c∗
(cid:20)
−(cid:21)
for q <
∞
0 and c∗
−

(cid:20)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

q
−(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

= 0. Then we can write

c∗
− (cid:54)

Since y is a data point in subspace
y = Y ic. Substituting this into (47) we get

Si, there exists a c such that

y = Y ic∗ + Y

.

ic∗
−

−

Y i(c

c∗) = Y

−

.

ic∗
−

−

(47)

(48)

Note that the left hand side of equation (48) corresponds to a point
Si while the right hand side of (48) corresponds to
in the subspace
a point in the subspace
=iSj. By the independence assumption,
=iSj are also independent hence
the two subspaces
disjoint and intersect only at the origin. Thus, from (48) we must
= 0 and from (47) we obtain y = Y ic∗. In other
have Y
words,
(cid:62) is a feasible solution of the optimization
= 0, we have
problem (46). Finally, from the assumption of c∗
− (cid:54)

ic∗
−
−
c∗(cid:62) 0(cid:62)

⊕j
Si and

⊕j

(cid:3)

(cid:2)

<

c∗
0

c∗
c∗
q
−(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)
c∗(cid:62) c∗
−

(cid:20)

(cid:20)

q
(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
= 0, obtaining the desired result.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:3)

that contradicts the optimality of
have c∗

= 0 and c∗
−

(cid:2)

(cid:62). Thus, we must

16

PROOF OF THEOREM 2
In this section, we prove Theorem 2 in the paper, where we
provide a necessary and sufﬁcient condition for subspace-sparse
recovery in a union of disjoint subspaces. To do so, we consider a
vector x in the intersection of
=iSj and let the optimal
Si with
solution of the (cid:96)1-minimization when we restrict the dictionary
to the points from

⊕j

Si be
ai = argmin

a
(cid:107)1
(cid:107)

s. t. x = Y i a.

(50)

We also let the optimal solution of the (cid:96)1 minimization when we
restrict the dictionary to the points from all subspaces except
Si
be

a

i = argmin

−

a
(cid:107)1
(cid:107)

s. t. x = Y

i a.

−

(51)

Si with
=iSj, the (cid:96)1-norm of the solution of (50) is strictly smaller than

We show that if for every nonzero x in the intersection of
⊕j
the (cid:96)1-norm of the solution of (51), i.e.,

x

∀

∈ Si ∩

(

⊕j

=iSj), x

= 0 =

ai(cid:107)1 <

a

i(cid:107)1,

−

(cid:107)

⇒ (cid:107)

(52)

then the SSC algorithm succeeds in recovering subspace-sparse
representations of all the data points in

Theorem 2: Consider a collection of data points drawn from n
n
n
i=1. Let Y i denote
i=1 of dimensions
i denote

{Si}
Si, where rank(Y i) = di, and let Y

disjoint subspaces
Ni data points in
data points in all subspaces except

di}

{

−

Si.

Si. The (cid:96)1-minimization
,

i]

s. t. y = [Y i Y

c
c

(cid:20)

−(cid:21)

−

(53)

c∗
c∗
−(cid:21)

(cid:20)

= argmin

c
c

(cid:20)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
−(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

recovers a subspace-sparse representation of every nonzero y in
Si, i.e., c∗

= 0, if and only if (52) holds.

= 0 and c∗
−

Proof: (

=) We prove the result using contradiction. As-

sume c∗
− (cid:54)

⇐

= 0 and deﬁne

x (cid:44) y

Y ic∗ = Y

.

ic∗
−

−

−

(54)

Si and Y ic∗ is a linear combination of points in
Since y lies in
Si, from the ﬁrst equality in (54) we have that x is a vector in
Si. Let ai be the solution of (50) for x. We have

x = y

Y ic∗ = Y iai ⇒

−

y = Y i(c∗ + ai).

(55)

On the other hand, since Y
in all subspaces except
have that x is a vector in
(51) for x. We have

−

ic∗
−

is a linear combination of points
Si, from the second equality in (54) we
i be the solution of

=iSj. Let a

⊕j

−

x = Y

ic∗
−

−

= Y

ia

−

i ⇒
−

y = Y ic∗ + Y

ia

i.

−

−

(56)

Note that the left hand side of (56) together with the fact that
a

i is the optimal solution of (51) imply that

−

From (55) and (56) we have that

a

(cid:107)

i(cid:107)1 ≤ (cid:107)
−

−(cid:107)1.
c∗
c∗ + ai
0

(cid:20)

(cid:21)

and

c∗
a

−

(cid:20)

i(cid:21)

(57)

are

feasible solutions of the original optimization program in (53).
Thus, we have

(49)

(cid:20)

c∗

c∗ + ai
0

c∗
,
c∗
1
−(cid:21)(cid:13)
(cid:13)
(58)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
where the ﬁrst inequality follows from triangle inequality, the
(cid:13)
(cid:13)
second strict inequality follows from the sufﬁcient condition in

1 ≤ (cid:107)
(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

ai(cid:107)1 <

i(cid:107)1 ≤
−

(cid:107)1 +

(cid:107)1 +

c∗
(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

a

(cid:20)

(cid:107)

(cid:107)

(cid:2)

(cid:3)

(52), and the last inequality follows from (57). This contradicts the
(cid:62) for the original optimization program
optimality of
c∗(cid:62) c∗(cid:62)
−
in (53), hence proving the desired result.

=) We prove the result using contradiction. Assume the
(
⇐
i.e.,
condition in (52) does not hold,
there exists a nonzero
x in the intersection of
=iSj for which we have
Si and
⊕j
ai(cid:107)1. As a result, for y = x, a solution of the (cid:96)1-
a
(cid:107)
minimization program (53) corresponds to selecting points from
all subspaces except
Si, which contradicts the subspace-sparse
recovery assumption.

i(cid:107)1 ≤ (cid:107)

−

PROOF OF THEOREM 3

{Si}

di}
{
RD

Theorem 3: Consider a collection of data points drawn from
i=1. Let Wi be
n
n
i=1 of dimensions
n disjoint subspaces
the set of all full-rank submatrices ˜Y i ∈
di of Y i, where
rank(Y i) = di. If the condition
Y
di (cid:107)
holds, then for every nonzero y in
recovers a subspace-sparse solution, i.e., c∗

Si, the (cid:96)1-minimization in (53)
= 0.11
= 0 and c∗
−

i(cid:107)1,2 max

σdi ( ˜Y i) >

max
˜Y i
Wi

cos(θij)

(59)

(cid:112)

=i

−

×

∈

j

Proof: We prove the result in two steps. In step 1, we show
i(cid:107)1. Then,
a
ai(cid:107)1 ≤
i ≤ (cid:107)
−
i establishes the result of the

βi. In step 2, we show that β

that
the sufﬁcient condition βi < β
theorem, since it implies

(cid:107)

−

−

βi < β

ai(cid:107)1 ≤
(cid:107)

a
i ≤ (cid:107)
−
i.e., the condition of Theorem 2 holds.
Step 1: Upper bound on the (cid:96)1-norm of (50) Let Wi be the set
of all submatrices ˜Y i ∈
di of Y i that are full column rank.
We can write the vector x

i(cid:107)1,

RD

(60)

−

x = ˜Y i ˜a =

⇒

1 ˜Y (cid:62)

i x.

(61)

Using vector and matrix norm properties, we have

×
∈ Si ∩

(
⊕j
˜a = ( ˜Y (cid:62)
i

=iSj)
˜Y i)−

˜a
(cid:107)1 ≤
(cid:107)

˜a
di(cid:107)
(cid:107)2 =
(cid:112)
(cid:112)
˜Y i)−
( ˜Y (cid:62)
di (cid:107)
i

di (cid:107)
1 ˜Y (cid:62)

≤

(cid:112)

( ˜Y (cid:62)
i

˜Y i)−

1 ˜Y (cid:62)

i x

x
i (cid:107)2,2(cid:107)

(cid:107)2 =

(cid:107)2
√di
x
σdi( ˜Y i) (cid:107)

(cid:107)2,

(62)

where σdi( ˜Y i) denotes the di-th largest singular value of ˜Y i.
Thus, for the solution of the optimization problem in (50), we
have

ai(cid:107)1 ≤
(cid:107)

min
˜Y i

Wi (cid:107)

˜a
(cid:107)1 ≤

min
˜Y i
Wi

√di
x
σdi( ˜Y i) (cid:107)

(cid:107)2 (cid:44) βi,

(63)

∈

∈
which established the upper bound on the (cid:96)1-norm of the solution
of the optimization program in (50).
Step 2: Lower bound on the (cid:96)1-norm of (51) For the solution
of (51) we have x = Y
i. If we multiply both sides of this
equation from left by x(cid:62), we get

ia

−

−

x
(cid:107)
−
Applying the Holder’s inequality (
u(cid:62)v
|
above equation, we obtain

2
2 = x(cid:62)x = x(cid:62)Y
(cid:107)

ia

i.

−

u

| ≤ (cid:107)

(cid:107)∞(cid:107)

(64)
(cid:107)1) to the

v

x
(cid:107)

2
2 ≤ (cid:107)
(cid:107)

Y (cid:62)
−

ix

a

i(cid:107)1.

−

(cid:107)∞(cid:107)

(65)

17

By recalling the deﬁnition of the smallest principal angle between
two subspaces, we can write

2
2 ≤

(cid:107)

x
(cid:107)

max
=i
j

cos(θij)

i(cid:107)1,
Si and
i(cid:107)1,2 is the maximum (cid:96)2-norm of the columns of Y

where θij is the ﬁrst principal angle between
Y
(cid:107)
data points in all subspaces except

x
i(cid:107)1,2 (cid:107)

a
(cid:107)2 (cid:107)

Y
(cid:107)

−

−

−

(66)

Sj and
i, i.e.,

−

Si. We can rewrite (66) as

i (cid:44)

β

−

x
(cid:107)2
(cid:107)
=i cos(θij)

maxj

a
i(cid:107)1,2 ≤ (cid:107)

−

Y
(cid:107)

i(cid:107)1

−

(67)

which establishes the lower bound on the (cid:96)1 norm of the solution
of the optimization program in (51).

SOLVING THE SPARSE OPTIMIZATION PROGRAM

Note that the proposed convex programs can be solved using
generic convex solvers such as CVX12. However, generic solvers
typically have high computational costs and do not scale well
with the dimension and the number of data points.

In this section, we study efﬁcient implementations of the pro-
posed sparse optimizations using an Alternating Direction Method
of Multipliers (ADMM) method [50], [64]. We ﬁst consider the
most general optimization program

C

min
(C,E,Z) (cid:107)
s. t. Y = Y C + E + Z, C(cid:62)1 = 1, diag(C) = 0,

E
(cid:107)1 + λe(cid:107)

(cid:107)1 +

Z

(cid:107)

2
F

λz
2 (cid:107)

(68)

and present an ADMM algorithm to solve it.

First, note that using the equality constraint in (68), we can
eliminate Z from the optimization program and equivalently solve

C

(cid:107)1 + λe(cid:107)

min
(C,E) (cid:107)
s. t. C(cid:62)1 = 1, diag(C) = 0.

(cid:107)1 +

E

Y

λz
2 (cid:107)

Y C

E

2
F
(cid:107)

−

−

(69)

The overall procedure of the ADMM algorithm is to introduce
appropriate auxiliary variables into the optimization program,
augment the constraints into the objective function, and iteratively
minimize the Lagrangian with respect to the primal variables and
maximize it with respect to the Lagrange multipliers. With an
abuse of notation, throughout this section, we denote by diag(C)
both a vector whose elements are the diagonal entries of C and a
diagonal matrix whose diagonal elements are the diagonal entries
of C.

To start, we introduce an auxiliary matrix A

consider the optimization program

RN

N and

×

∈

C

(cid:107)1 +
min
(C,E,A) (cid:107)
s. t. A(cid:62)1 = 1, A = C

E
(cid:107)1 + λe(cid:107)

Y

λz
2 (cid:107)
−
diag(C).

Y A

E

2
F
(cid:107)

−

(70)

−
whose solution for (C, E) coincides with the solution of (69).
As we will see shortly, introducing A helps to obtain efﬁcient
updates on the optimization variables. Next, using a parameter
ρ > 0, we add to the objective function of (70) two penalty terms

11. (cid:107)Y −i(cid:107)1,2 denotes the maximum (cid:96)2-norm of the columns of Y −i.

downloaded from http://cvxr.com.

12. CVX is a Matlab-based software for convex programming and can be

Algorithm 2 : Solving (68) via an ADMM Algorithm
Initialization: Set maxIter = 104, k = 0, and Terminate

1: while (Terminate == False) do
2:

update A(k+1) by solving the following system of linear equations

False. Initialize C(0), A(0), E(0), δ(0), and ∆(0) to zero.

←

(λzY (cid:62)Y + ρI + ρ11(cid:62))A(k+1) = λzY (cid:62)(Y

E(k)) + ρ(11(cid:62) + C(k))

1δ(k)

(cid:62)

∆(k),

−

−

−

1
ρ

T

(A(k+1) + ∆(k)/ρ),

18

3:

4:

5:

6:
7:
8:

diag(J ), where J (cid:44)
Y A(k+1)),
(Y

−
λe
λz

update C(k+1) as C(k+1) = J
update E(k+1) as E(k+1) =
update δ(k+1) as δ(k+1) = δ(k) + ρ (A(k+1)
(cid:62)1
update ∆(k+1) as ∆(k+1) = ∆(k) + ρ (A(k+1)
k + 1,
k
←
A(k)
if (
(cid:62)1
(cid:107)

(cid:107)∞ ≤

C(k)

A(k)

(cid:15) and

−

−

−

1

T

(cid:107)

(cid:107)∞ ≤

(cid:15) and

−
−

1),
C(k+1)),

then

True

Terminate

9:
end if
10:
11: end while
Output: Optimal sparse coefﬁcient matrix C∗ = C(k).

←

A(k)
(cid:107)

−

A(k

−

1)

(cid:107)∞ ≤

(cid:15) and

E(k)

E(k

−

1)

(cid:107)

−

(cid:107)∞ ≤

(cid:15) or (k

maxIter)

≥

corresponding to the constraints A(cid:62)1 = 1 and A = C
and consider the following optimization program

−

diag(C)

min
(C,E,A) (cid:107)

Y

C

E
(cid:107)1 + λe(cid:107)
ρ
A(cid:62)1
2 (cid:107)

λz
2 (cid:107)
ρ
2 (cid:107)
s. t. A(cid:62)1 = 1, A = C

(cid:107)1 +
1
(cid:107)

2
2 +

−

+

−

A

Y A

E

2
F
(cid:107)

−

(C

−

−
diag(C).

−

diag(C))
(cid:107)

2
F

(71)

Note that adding the penalty terms to (70) do not change its
optimal solution, i.e., both (70) and (71) have the same solutions,
since for any feasible solution of (71) that satisﬁes the constraints,
the penalty terms vanish. However, adding the penalty terms
makes the objective function strictly convex in terms of the
optimization variables (C, E, A), which allows using the ADMM
approach.

Introducing a vector δ

N of
Lagrange multipliers for the two equality constraints in (71), we
can write the Lagrangian function of (71) as

RN and and a matrix ∆

RN

∈

∈

×

(C, A, E, δ, ∆) =

L

+

A(cid:62)1

ρ
2 (cid:107)

+ δ(cid:62)(A(cid:62)1

−

−

C

E
(cid:107)1 + λe(cid:107)
(cid:107)
ρ
2
A
2 +
2 (cid:107)
(cid:107)
1) + tr(∆(cid:62)(A

−

1

(cid:107)1 +
(C

−

Y A

E

2
F
(cid:107)

−

Y

λz
2 (cid:107)
−
diag(C))

2
F
(cid:107)

−
C + diag(C))),

(72)

where tr(
) denotes the trace operator of a given matrix. The
·
ADMM approach then consists of an iterative procedure as
follows: Denote by (C(k), E(k), A(k)) the optimization variables
at iteration k, and by (δ(k), ∆(k)) the Lagrange multipliers at
iteration k and

•

Obtain A(k+1) by minimizing
with respect to A, while
(C(k), E(k), δ(k), ∆(k)) are ﬁxed. Note that computing the
derivative of
with respect to A and setting it to zero, we
obtain

L

L

(λzY (cid:62)Y + ρI + ρ11(cid:62))A(k+1) = λzY (cid:62)(Y
+ ρ(11(cid:62) + C(k))

1δ(k)

(cid:62)

−
∆(k).

E(k))

(73)

−

−

In other words, A(k+1) is obtained by solving an N
N
system of linear equations. When N is not very large, one
can simply matrix inversion to obtain A(k+1) from (73).

×

For large values of N , conjugate gradient methods should
be employed to solve for A(k+1).
Obtain C(k+1) by minimizing
with respect to C, while
(A(k), E(k), δ(k), ∆(k)) are ﬁxed. Note that the update on
C also has a closed-form solution given by

L

•

C(k+1) = J

diag(J ),
−
(A(k+1) + ∆(k)/ρ),

J (cid:44)

1
ρ

T

(74)

(75)

where
each element of the given matrix, and is deﬁned as

) is the shrinkage-thresholding operator acting on
·

Tη(

v

| −

η)+ sgn(v).

Tη(v) = (
|
The operator (
)+ returns its argument if it is non-negative
·
and returns zero otherwise.
Obtain E(k+1) by minimizing
with respect to E, while
(C(k+1), A(k+1), δ(k), ∆(k)) are ﬁxed. The update on E can
also be computed in closed-form as

(76)

L

E(k+1) =

(Y A(k+1)

Y ),

(77)

λe
λz

T

−

Having (C(k+1), A(k+1), E(k+1) ﬁxed, perform a gradient
ascent update with the step size of ρ on the Lagrange
multipliers as

•

•

δ(k+1) = δ(k) + ρ (A(k+1)
∆(k+1) = ∆(k) + ρ (A(k+1)

(cid:62)1

1),
−
C(k+1)).

(78)

(79)

−

C(k)

These three steps are repeated until convergence is achieved or
the number of iterations exceeds a maximum iteration number.
Convergence is achieved when we have
(cid:15),
1)
A(k)
−
(cid:107)∞ ≤
−
(cid:107)
E(k
1)
(cid:15), where (cid:15) denotes the error tolerance for the primal
−
4 works well
and dual residuals. In practice, the choice of (cid:15) = 10−
in real experiments. In summary, Algorithm 2 shows the updates
for the ADMM implementation of the optimization program (68).

A(k)
(cid:107)
(cid:107)∞ ≤

1
(cid:62)1
−
(cid:15) and

−
(cid:107)∞ ≤

(cid:107)∞ ≤
E(k)
(cid:107)

A(k)
(cid:107)

A(k

−

(cid:15),

COMPUTATIONAL TIME COMPARISON
Table 6 shows the computational time of different algorithms on
the Extended Yale B dataset as a function of the number of

19

TABLE 6
Average computational time (sec.) of the algorithms on the
Extended Yale B dataset as a function of the number of subjects.

LRR LRSC SSC
1.8
1.1
1.6
2 Subjects
3.29
1.9
2.2
3 Subjects
11.4
5.7
7.6
5 Subjects
42.6
16.3
22.1
8 Subjects
160.3
96.9
10 Subjects 405.3 1439.8 255.0

SCC
LSA
262.8
5.2
451.5
13.4
62.7
630.3
180.2 1020.5

subjects. Note that these computational times are based on the
codes of the algorithms used by their authors. It is important
to mention that LRR and SSC can be implemented using faster
optimization solvers. More speciﬁcally, LRR can be made faster
using LADMAP method proposed in: “Z. Lin and R. Liu and
Z. Su, Linearized Alternating Direction Method with Adaptive
Penalty for Low-Rank Representation, NIPS 2011.” Also, SSC
can be made faster using LADM method proposed in “J. Yang
and Y. Zhang. Alternating direction algorithms for (cid:96)1 problems
in compressive sensing. SIAM J. Scientiﬁc Computing, 2010.”

