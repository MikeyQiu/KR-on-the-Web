Data Mining and Knowledge Discovery manuscript No.
(will be inserted by the editor)

SkOPUS
Mining top-k sequential patterns under leverage

Fran¸cois Petitjean · Tao Li · Nikolaj Tatti ·
Geoﬀrey I. Webb

8
1
0
2
 
b
e
F
 
5
 
 
]
I

A
.
s
c
[
 
 
4
v
9
0
0
8
0
.
6
0
5
1
:
v
i
X
r
a

Received: 14 December 2015 / Accepted: 23 May 2016 / Published online: 14 June 2016

Abstract This paper presents a framework for exact discovery of the top-k se-
quential patterns under Leverage. It combines (1) a novel deﬁnition of the expected
support for a sequential pattern — a concept on which most interestingness mea-
sures directly rely — with (2) SkOPUS: a new branch-and-bound algorithm for
the exact discovery of top-k sequential patterns under a given measure of interest.
Our interestingness measure employs the partition approach. A pattern is inter-
esting to the extent that it is more frequent than can be explained by assuming
independence between any of the pairs of patterns from which it can be com-
posed. The larger the support compared to the expectation under independence,
the more interesting is the pattern. We build on these two elements to exactly ex-
tract the k sequential patterns with highest leverage, consistent with our deﬁnition
of expected support.

We conduct experiments on both synthetic data with known patterns and
real-world datasets; both experiments conﬁrm the consistency and relevance of
our approach with regard to the state of the art.

This article was published in Data Mining and Knowledge Discovery and is

accessible at http://dx.doi.org/10.1007/s10618-016-0467-9.

Keywords data mining · pattern mining · sequential data · exact discovery ·
interestingness measures

Francois Petitjean
Faculty of Information Technology, Monash University
E-mail: francois.petitjean@monash.edu

Tao Li
School of Electronic and Information Engineering, Nanjing University of Information Science
and Technology, E-mail: litao@nuist.edu.cn

Nikolaj Tatti
Department of Information and Computer Science, Aalto University
E-mail: nikolaj.tatti@aalto.ﬁ

Geoﬀrey I. Webb
Faculty of Information Technology, Monash University
E-mail: geoﬀ.webb@monash.edu

2

1 Introduction

Fran¸cois Petitjean et al.

Extracting interesting patterns from data is a core data mining task. This pa-
per introduces a method to eﬃciently and exactly identify the top-k patterns
in a sequential database, using as the measure of interest leverage — the diﬀer-
ence between a pattern’s observed and expected frequency. To deﬁne the expected
frequency, we use a maximum-likelihood estimate under an assumption of inde-
pendence between any pair of patters from which it can be composed.

The notion of interestingness is at the core of this paper. In early work on
pattern mining, patterns were considered interesting if they appeared frequently
in data [5]. The underlying idea was that the fact that something happens often is
useful information for the data practitioner. However, as a mature body of research
has now shown [30, 43, 16, 44, 8, 15, 34, 39, 22, 47, 36], frequency is often a poor proxy
for interestingness. One reason for this is that many patterns should be expected
to be frequent in real data. For instance, in the traditional market basket use
case, if 90% of people buy apples and 90% of people buy pears, then the pattern
{apples, pears} will be frequent even if the two events are completely independent.
When handling large databases, this phenomenon creates a deluge of frequent but
uninteresting patterns. This is especially problematic for real-world applications,
because the most frequent patterns will often be well-known; it is less frequent
interactions within the data that are most likely to provide novel insights.

When tackling sequential databases, this issue becomes even more critical;
patterns such as “buying apples and subsequently buying pears” will be extremely
common.

This is a signiﬁcant problem in real-world data, even with relatively short
sequences. As a simple demonstration of this, we report in Table 1 the ﬁve most
frequent sequential patterns in the book “The Adventures of Tom Sawyer”, with
every sentence constituting a record in the database (the rest of the paper will
make it clear how we have performed this extraction).

Table 1 The ﬁve most frequent sequential patterns in the Adventures of Tom Sawyer.

pattern

support

(cid:104)and, and(cid:105)
(cid:104)and, to(cid:105)
(cid:104)to, and(cid:105)
(cid:104)of, and(cid:105)
(cid:104)and, of (cid:105)

13%
9.8%
9.1%
8.6%
8.0%

We can observe that most frequent sequential patterns are not interesting, and
simply correspond to the permutations of the most frequent word ‘and’ and each
of the two next most frequent words ‘to’ and ‘of’. This has motivated several ap-
proaches to mining interesting sequential patterns, which we detail in Section 2.
The above example illustrates two main points about the incompatibility of fre-
quency as a proxy for interestingness in sequential pattern discovery:

1. The main reason for the high frequency of all these patterns, is the frequency of
the individual words. In English, it has been shown that one word in four comes

SkOPUS

3

from {the, be, to, of, and, a, in, that, have, I} [1]. It is then not surprising that a
sentence would contain several occurrences of those. If the sequences are long
enough, any sequence of independent events (which we believe will not often
be of interest) will become frequent. In fact, for any pattern with probability
p > 0 of occurring at any time in the data, the probability of its repetition
tends to 100% as the length of the sequences in the database increases (this
is even true for singletons).1 This clearly demonstrates that frequency is not a
good proxy for interestingness for sequential databases.

2. Let us consider the second and third most frequent sequential patterns (cid:104)and, to(cid:105)
and (cid:104)to, and(cid:105). These two patterns have similar frequency. This directly ques-
tions the relevance of frequency as an interestingness measure. For sequential
patterns the order of items should be key to determining whether the pat-
tern is interesting. If all the orderings of the terms in a sequential pattern are
equally frequent then an unordered pattern, such as an itemset, captures all
of the information about the potentially interesting regularity in the data. We
argue that the sequential pattern should be reserved for regularities that can-
not be fully captured by unordered patterns. In this case, we can see that with
such simple patterns of length 2, frequency ranks the two possible orderings
of {and, to} as close to equivalent. The question is then: ”Should a pattern
(cid:104)i1, i2(cid:105) be considered sequentially interesting if it appears as often as (cid:104)i2, i1(cid:105) in
the database?” We argue below that this contradicts the natural deﬁnition of
interestingness for sequential patterns.

In this paper, we make the two following contributions:

1. Scoring: Expected support is the core element of standard pattern mining
measures of interest, such as leverage or lift [46], because most measures of
interest involve a comparison between the observed support of a pattern in
the data and its expected support [28]. We introduce a new deﬁnition for
the expected support of a sequential pattern and present an algorithm for its
computation. Following our motivation above, our approach tries to ﬁnd a
model that is local to the tested pattern by considering its re-orderings.

2. Search: We introduce SkOPUS: a sequential extension of the branch-and-bound
OPUS algorithm [47]. SkOPUS can extract, exactly and eﬃciently, the k se-
quential patterns with highest leverage, i.e., the k patterns with the highest
diﬀerence between their observed and expected support. Note that we will show
that our algorithm is not limited to our deﬁnition of the expected support, and
can be directly used to extract the top-k patterns under any deﬁnition of the
expected support.

Our paper is divided into ﬁve main sections. We present the related research in
Section 2. In Section 3, we detail the proposed framework for the discovery of the
top-k patterns with highest leverage. In Section 4, we present the results of exper-
iments conducted on both synthetic data for which we control what patterns are
actually present, and on real-world datasets. We conclude the paper and present
some future work in Section 5.

1 If p(“buying apples”) > 0 and p(“buying pears”) > 0 and these events are independent,
then the probability of observing their sequence in data follows: liml→∞ p((cid:104)apples, pears(cid:105)) = 1

4

2 Related work

Fran¸cois Petitjean et al.

We structure the related work around the two main elements that this paper
addresses: discovery of sequential and interesting patterns.

2.1 Mining interesting non-sequential patterns

There is a very mature body of research about non-sequential frequent pattern
mining [16]. As raised in the introduction, it has now long been identiﬁed that the
major issue is not whether we can derive the complete set of frequent patterns
under certain constraints, but whether we can derive a compact but high-quality
set of patterns that will be useful for most applications [16, 8, 34].

While the major focus has been on eﬃcient discovery of frequent patterns,
there is a growing body of research into identifying interesting patterns. Several
methods aim at ﬁnding the set of patterns that will best describe the dataset,
using a variety of methods for scoring the set such as entropy [22] or information
theoretic frameworks [30, 39]. Other approaches rather try to deﬁne measures of
interest for patterns [36, 18], and then perform the extraction of the most interest-
ing patterns for diﬀerent measures [7, 42, 11, 15, 47]. We refer the reader to [4], for
a more complete survey on frequent pattern mining, and discovering interesting
patterns.

2.2 Mining frequent sequential patterns

Sequential pattern mining extends frequent pattern mining to sequential databases
[21, 26]. Real-world applications are numerous and include analysis of the purchase
behaviour of customers over time, analysis of Web clickstreams, and study of bi-
ological sequences. Algorithms for mining frequent sequential patterns from se-
quential databases were ﬁrst proposed in the 1990s [6, 23, 24]. After these seminal
papers, researchers quickly moved to the development of algorithms for the extrac-
tion of frequent sequential patterns of higher complexity (sequences of itemsets)
[25, 27, 17, 48, 29, 38, 10].

Note that diﬀerent researchers have used diﬀerent deﬁnitions of support and
that these directly inﬂuence the patterns that they extract. Some methods, like
ours, consider the support as being the number of sequences that have the sequen-
tial pattern as a subsequence, while others consider the number of times that the
pattern occurs in all the sequences of the dataset, multiple counting a sequence
that embeds the subsequence multiple times. Some application domains and prob-
lems will beneﬁt from one approach while others will from the other approach.
Our techniques extend directly to the second deﬁnition, should such an approach
be desirable, the primary change simply being in the counting of the support of a
pattern and its sub-patterns.

2.3 Mining interesting sequential patterns

It is only in the last decade that extracting interesting sequential patterns has
emerged as an important research topic.

SkOPUS

5

We distinguish two main families of methods:

1. methods using information theoretic approaches, such as the Minimum De-
scription Length, to score a set of patterns that best explain the dataset.
2. methods deﬁning an expected support for the patterns under a null model (or
hypothesis), and using it to score and rank the patterns by comparison to the
actual observed support of the pattern

Minimum Description Length methods scoring a set of patterns In [35, 19], the
authors propose a Minimum Description Length (MDL) approach for scoring a set
of patterns relative to a dataset as well as a heuristic search method to construct
the set. Here the idea is to score a pattern set instead of a just individual patterns;
these patterns should explain the data well but at the same time be non-redundant.
These goals are quantiﬁed, with scores derived from MDL principles.

Methods based on expected support Deriving an expected support under some null
hypothesis is particularly complex for sequential patterns. Several approaches have
made independence assumptions between the elements composing the sequential
patterns. That is, they use a null hypothesis that the data are generated by a
0-order Markov model or stationary and independent stochastic process [13, 31,
14, 20].

Approaches to deriving expected support from Markov chains have been pro-
posed in [12, 9]; in particular, the statistical signiﬁcance of the extracted patterns
is studied in [9]. More complex Markov models have been studied in [2], with
statistical signiﬁcance studied in [3].

Tatti [32] takes a diﬀerent approach to interestingness and posits that for some
applications, interesting patterns will be the ones that occur in a short window.
He then builds a model of the expected length of a sequence for a pattern to occur,
and compares it to the actual one.

Finally, in very recent work Tatti [33] introduces an approach that is the most
related to ours. This aproach takes inspiration from Webb’s approach to ﬁnding
interesting (non-sequential) itemsets [45], and builds the expected support by look-
ing at diﬀerent partitions of the sequential pattern. It then uses the derived score
to rank the strict episodes. The key diﬀerence between our work in this paper and
that in Tatti [33] is the information from which the expectation is derived: Tatti
[33] uses item probabilities as well as the number of gaps in a subpattern to derive
the expectation whereas, in this work, we compare the pattern against all alter-
native orderings of the same items. Among other technical diﬀerences, Tatti [33]
deﬁnes its measure for strict episodes whereas we focus on sequential patterns,
that is, serial episodes. Finally, the way we measure the diﬀerence between the
observed support and expected support allows us to use a monotonic bound, and
essentially mine top-k episodes without ﬁrst generating a candidate set, whereas
the previously discussed methods require that a candidate set be ﬁrst generated,
typically a set of frequent patterns mined with a low threshold.

3 Extracting the Top-K interesting sequential patterns

We introduce our method in this section. We ﬁrst introduce some notation. We
then detail our model for the expected frequency as well as how to derive our

6

Fran¸cois Petitjean et al.

interestingness measure from it – leverage. Finally, we introduce our eﬃcient search
algorithm to extract the most interesting sequential patterns under these measures.
Note that we have made all code freely available to allow independent conﬁr-
mation and extensions of our work at https://github.com/fpetitjean/Skopus.

3.1 Deﬁnitions

A sequence S over a ﬁnite set of items I is an ordered list (cid:104)s1, . . . , s(cid:96)(cid:105), with
si ∈ I, 1 ≤ i ≤ (cid:96), where (cid:96) is the length of sequence S, denoted by |S| = (cid:96). A
sequential database D = {S1, S2, . . . , Sn} of size n is a multiset of n sequences
over I, where each Si is a sequence.
Deﬁnition 1 (Sub-sequence) A sequence S(cid:48) = (cid:10)t1, t2, . . . , t(cid:96)
(cid:11) is a sub-sequence
of the sequence S = (cid:10)s1, s2, . . . , sk
(cid:11), if there exists an index sequence 1 ≤ r1 <
r2 < · · · < r(cid:96) ≤ k, such that tj = srj for all 1 ≤ j ≤ (cid:96). In such a case, we write
s(cid:48) ≺ s.

Deﬁnition 2 (Head and Tail) The head of a sequence is its ﬁrst element and
the tail is its remaining elements. Given an item a and a sequence T , we write
(cid:104)a | T (cid:105) to mean a sequence starting with the head a followed by the tail T . For
example, (cid:104)a | (cid:104)b, c(cid:105)(cid:105) = (cid:104)a, b, c(cid:105) and (cid:104)a | (cid:104)(cid:105)(cid:105) = (cid:104)a(cid:105).

Deﬁnition 3 (Cover) The cover of a sequence S in a sequential database D is
the set of records of which S is a sub-sequence,

cover (S, D) = {Si : Si ∈ D, S ≺ Si}.

Such a sequence is often called sequential pattern.

Deﬁnition 4 (Count) The count of a sequence S in a sequential dataset D is
number of records of which S is a subsequence,

Deﬁnition 5 (Support) The support of a pattern S is the proportion of the n
records in database D of which S is a subsequence,

#(S, D) = |cover (S, D)|.

sup(S, D) = |cover (S, D)|/n.

For notational convenience, and when it is clear in the context, we will omit D.

Note that this paper focuses on sequential patterns and as such does not aim at
extracting patterns of the type “A, then either B or C, and then D”. As mentioned
in related work, such patterns refer to the more general class of episodes (see for
example [33]). Nor does it seek patterns of the form “sequences that contain A are
more likely to contain B, irrespective of their order.” Such patterns are already
addressed by the substantial literature on itemset discovery.

3.2 When is a sequential pattern interesting?

We now introduce our model for the expected frequency of a given sequential
pattern, as well as the interestingness measures that we derive from it. We start by
providing an intuition for our framework and then introduce its formal deﬁnition.

SkOPUS

7

3.2.1 From interestingness to expected support

“When is a sequential pattern interesting?” is the main question that we tackle in
this paper. We have motivated in the introduction that, for example, it is unlikely
that a pattern (cid:104)and, of (cid:105) would be interesting if the pattern (cid:104)of, and(cid:105) is also. It is
unclear when both these patterns could be interesting except in a context where
the non-sequential pattern {and, of } holds, when the presence of and increases
the frequency of of and vice versa, irrespective of order. Our philosophy is that
such patterns are more succinctly captured as non-sequential patterns.

The starting point of our approach to assessing interestingness is that a pattern
should be interesting if is not possible to explain its frequency by the frequency
of its sub-patterns [40]. (cid:104)a, b(cid:105) should only be interesting if its frequency is greater
than can be explained just by the frequency of (cid:104)a(cid:105) and (cid:104)b(cid:105). (cid:104)a, b, c, d(cid:105) should not
be interesting if its frequency can be explained by the frequency of any of its con-
stituent sequential sub-patterns such as (cid:104)a, c(cid:105), (cid:104)b, d(cid:105), and (cid:104)a, c, d(cid:105). For example,
if buying shoes is often followed by buying socks, and buying jeans is often fol-
lowed by buying a belt, then the pattern (cid:104)shoes, jeans, socks, belt(cid:105) should only be
interesting if it is more frequent than should be expected given the frequency of
(cid:104)shoes, socks(cid:105) and (cid:104)jeans, belt(cid:105) (as well as any other sub-sequences of this 4-element
sequence).

Following the standard approach for deﬁning interestingness measures, we as-
sess interestingness in terms of deviation between observed support and expected
support.

An intuitive ﬁrst approach to deriving the expected support might be to simply
multiply the supports of the two subsequences. If (cid:104)a(cid:105) and (cid:104)b(cid:105) both occur in 50%
of sequences then perhaps (cid:104)a, b(cid:105) should be expected in 25%. However, this ignores
that they may also occur in the order (cid:104)b, a(cid:105), suggesting that there is a need to
adjust for the number of permutations that could occur. But there is a further
complication — (cid:104)a(cid:105) and (cid:104)b(cid:105) can both occur in sequences of length 1 and (cid:104)a, b(cid:105)
cannot. The length of the sequences in which the pattern appears must be taken
into account, but exactly how this will aﬀect the expectation is greatly dependent
upon the type of distribution from which the data are drawn. It is far from clear
what simple models might take account of all these factors without making very
strong assumptions about the form of the distribution. We seek to develop a simple
null hypothesis which can be tested without making any such assumptions.

3.2.2 Our proposed deﬁnition of expected support for sequential patterns

Before developing the general formulas for patterns of any length, it is instructive
to consider patterns of limited length to gain intuition about our general deﬁni-
tions.

Pattern of length 2 We seek sequential patterns, that is, patterns in which the
order of the items is signiﬁcant. That is, we want a pattern (cid:104)a, b(cid:105) to indicate
that the presence of an a in a sequence increases the probability of a b appearing
subsequently. We want to distinguish such a pattern from a co-occurrence pattern,
{a, b} which indicates that a sequence that contains an ‘a’ is more likely to contain
a ‘b’, irrespective of the order of their appearance. One simply hypothesis to test,
which makes no assumptions about the form of the distribution from which the

8

Fran¸cois Petitjean et al.

data are drawn, is that of all sequences in which a occurs, there are more in which
b occurs after a than there are sequences in which b occurs before a. One way of
stating this hypothesis is that sup(cid:104)a, b(cid:105) > sup(cid:104)b, a(cid:105). If a statistical hypothesis test
were developed, the null hypothesis would be that sup(cid:104)a, b(cid:105) ≤ sup(cid:104)b, a(cid:105).

Patterns of length 3 or more The considerations become more complicated when
we consider patterns of length three. One possibility would be that we want
sup(cid:104)a, b, c(cid:105) to be greater than the support of every other permutation of a, b and
c. However, that would disallow the possibility of both (cid:104)a, b, c(cid:105) and (cid:104)a, c, b(cid:105) being
interesting patterns. While we want to disallow all permutations of a set of items
being interesting sequential patterns, there is no reason to mandate that only one
permutation should be interesting.

It is well understood in the context of non-sequential patterns, that unless
speciﬁc steps are taken to prevent patterns from being accepted that are the com-
position of multiple interesting sub-patterns, or the inclusion of frequent singletons
into an interesting pattern, the results of a pattern discovery system will often be
swamped by myriads of spurious byproducts of the core patterns [40]. One eﬀective
approach to prevent this is to test a pattern X against every partition Y, Z such
that Y (cid:40) X and Z = X\Y . The counterpart of such a test for sequential patterns
would test against every pair of subsequences from which it can be composed.
Thus, we want to test (cid:104)a, b, c(cid:105) against (cid:104)a, b(cid:105), (cid:104)c(cid:105); (cid:104)a, c(cid:105), (cid:104)b(cid:105); and (cid:104)b, c(cid:105), (cid:104)a(cid:105). For the
‘partition’ (cid:104)a, b(cid:105), (cid:104)c(cid:105) we can compose three sequential patterns by interweaving the
two patterns, (cid:104)c, a, b(cid:105); (cid:104)a, c, b(cid:105); and (cid:104)a, b, c(cid:105). There are two obvious criteria that
might be imposed to determine whether sup(cid:104)a, b, c(cid:105) can be explained by sup(cid:104)a, b(cid:105)
and sup(cid:104)c(cid:105). One is that sup(cid:104)a, b, c(cid:105) must be greater than at least one of the sup-
ports of the other compositions of the generator. The other is that sup(cid:104)a, b, c(cid:105)
must be greater than the mean of sup(cid:104)c, a, b(cid:105); sup(cid:104)a, c, b(cid:105); and sup(cid:104)a, b, c(cid:105). Both
approaches are credible. The latter is a stronger constraint than the former, but
still allows two of the three patterns that can be composed from a two-element
and one-element sequential pattern to be found to be ‘interesting’. In the current
work we choose this stronger approach, but our software supports both. We thus
assess (cid:104)a, b, c(cid:105) to be ‘interesting’ if its support is greater than mean of the supports
of the recompositions of every one of its sequential partitions.

We now turn to the general formula for the expected support of a sequential
pattern, for which we ﬁrst introduce the notions of binary sequential partition and
of composition of two sequences.

Deﬁnition 6 (Sequential compositions) The sequential compositions of se-
quences S and T , denoted comp(S, T ), is the set of all sequences that contain all
and only the elements of S and T , respecting their order. More formally, we deﬁne
the composition recursively

comp(S, (cid:104)(cid:105)) = {S}

comp((cid:104)(cid:105), T ) = {T }

comp((cid:104)s1 | S∗(cid:105), (cid:104)t1 | T ∗(cid:105)) = {(cid:104)s1 | U (cid:105) | U ∈ comp(S∗, (cid:104)t1 | T ∗(cid:105))}

∪ {(cid:104)t1 | V (cid:105) | V ∈ comp((cid:104)s1 | S∗(cid:105), T ∗)}.

Deﬁnition 7 (Binary sequential partition) Let S, S1 and S2 be three se-
quences. We call {S1, S2} a binary sequential partition of S, if |S1| > 0, |S2| > 0

SkOPUS

9

and S ∈ comp(S1, S2). We denote as BSP (S) the set of all binary sequential
partitions of S,

BSP (S) = {{S1, S2} | S ∈ comp(S1, S2)} .

For example, the binary sequential partitions of

BSP ((cid:104)a, b, c, d(cid:105)) = {{(cid:104)a(cid:105), (cid:104)b, c, d(cid:105)}, {(cid:104)a, b(cid:105), (cid:104)c, d(cid:105)}, {(cid:104)a, b, c(cid:105), (cid:104)d(cid:105)},

{(cid:104)a, b, d(cid:105), (cid:104)c(cid:105)}, {(cid:104)a, c(cid:105), (cid:104)b, d(cid:105)}, {(cid:104)a, c, d(cid:105), (cid:104)b(cid:105)}, {(cid:104)a, d(cid:105), (cid:104)b, c(cid:105)}} .

We can now introduce our general deﬁnition for the expected support of a

sequential pattern.

Deﬁnition 8 (Expected support for a sequential pattern) Let S be a se-
quential pattern, we deﬁne the expected support of S as

ExpSupport(S) =

max
(S1,S2)∈BSP(S)

{

mean
S(cid:48)∈comp(S1,S2)

{sup(S(cid:48))}}.

Let us give an intuition about this deﬁnition. We consider all the possible pairs
of subsequences that respect the order in the target sequence (all the binary se-
quential partitions). Each of these partitions corresponds to a potential generator
of the targeted sequence. For example, to determine the interestingness of the
sequence S = (cid:104)a, b, c, d(cid:105), we look at how much more frequent it is than the max-
imum likelihood expectation if the data was generated from, say, U = (cid:104)a, d(cid:105) and
V = (cid:104)b, c(cid:105) assuming that all possible ways to interweave U and V are equiprobable.
We then take the maximum expected support over all potential binary sequential
partitions (i.e. over all possible generators). This is because ﬁnding one pair (U, V )
that ‘explains’ the support S should be enough to establish that pattern S is not
interesting. It is important to note that we do not need to consider the expected
support of compositions of more than two subsequences, as the relevant pairwise
partitions will subsume compositions of more than two subsequences. The aim of
this paper is to extract the top-k sequential patterns under leverage, i.e. that will
have maximum diﬀerence between their observed and expected support:

leverage = sup(S) − ExpSupport(S).

A pattern will be considered interesting if its support cannot be explained by any
of its sub-patterns.

It is ﬁnally important to note that if a pattern is considered interesting under
our framework, then some of its sub-patterns might also be considered interesting.
This will for example be the case with singular patterns that have low expected
support. An example is the pattern (cid:104)if, you, can(cid:105) present in the poem “If—” by R.
Kipling: it is present in more than 1/3 of the verses while none of its reorderings
are, and also none of the re-orderings of its sub-patterns. For example the pattern
(cid:104)can, you(cid:105), which is a reordering of (cid:104)you, can(cid:105) ≺ (cid:104)if, you, can(cid:105), never occurs either,
thus making the pattern (cid:104)you, can(cid:105) interesting as well. It is however here interesting
to note that our approach will rank the full pattern (cid:104)if, you, can(cid:105) higher than
(cid:104)you, can(cid:105), because there are more compositions for patterns of length 3 than
there are for patterns of length 2, and thus the mean over all those compositions
is lower.

10

Fran¸cois Petitjean et al.

3.2.3 Our framework’s null hypothesis

Having completely deﬁned our proposed deﬁnition of expected support, it is im-
portant to take a step back and look at what it is achieving. Its null hypothesis
is that the frequency of a pattern can be explained by the frequency of
its sub-patterns (or generators). We have imbricated two elements:

1. We deﬁne the expected frequency for each pair of sub-patterns (what we call
a binary sequential partition). Our null hypothesis there (and null model from
which we get the expectancy) is that all possible ways to “interweave” the
patterns – such that their order is respected – are equally likely. The expected
frequency of a pair of generators is then the average frequency observed over
all possible ways to interweave these patterns.

2. Our null hypothesis being that the frequency of a pattern can be explained by a
pair of generators, the expected frequency for our framework simply considers
the partition that produces the greatest expectation.

Note on the diﬃculty of assessing our framework’s statistical signiﬁcance We will
demonstrate in Section 4 that our framework is very eﬀective at extracting interest-
ing (with all its subjectivity) patterns. It is important here to note that extracting
a measure of the statistical signiﬁcance of patterns under our model is a diﬃcult
task, because it requires to be able to assess the probability of a pattern given
our null model. Although we can easily obtain its expected frequency, computing
its probability requires more assumptions about the form of the distribution from
which the data are drawn. Each sequence indeed only has a certain length, and
it is unclear what model would best match our framework. We however feel that
this is a consequence of the strength of our framework; we do not assume a sim-
plistic model of the underlying distribution for all patterns. In some sense, our
partitioning approach allows us to ﬁnd and ﬁt a dedicated model independently
for each sequence which makes the extraction powerful, but at the cost of failing
to support a simple test for assessing statistical signiﬁcance.

3.2.4 Algorithmic notes

We detail here the algorithmic considerations regarding the generation of Binary
Sequential Partitions (BSPs) and of Compositions.

Templates. It is ﬁrst interesting to note that in either case, the results only de-
pend on the length of the considered pattern and are independent of the actual
letters themselves. All partitions and compositions are thus generated as templates
depending as a function of the length.

Indexing templates. Our ﬁrst optimization directly follows from this observation;
every time we create a template for a BSP of a particular length, we index it
with its length so that it is only computed once for each length. We employ a
similar indexing for Compositions, with the diﬀerence that they have two associ-
ated lengths; we thus index them using a matrix. Note that the template for a
Composition of length l1 and l2 is identical, we only use the upper triangle in the
matrix.

SkOPUS

11

Generating BSPs. Generating all Binary Sequential Partitions for a template of
length l is an enumeration exercise. The ﬁrst element to note is that knowing
the pattern and its left partition set (S1 in Deﬁnition 7) completely determines
its right partition set (S2 in Deﬁnition 7), and conversely. This means that, to
construct the template for all BSPs of a particular length, we only need to ﬁnd
what is the left partition set, i.e., the sequence of positions of elements in the
original pattern. Furthermore, for a pattern of length l, we only need to consider
S1 of lengths l1 from 1 to (cid:98) l
2 (cid:99), because of symmetry.

To generate all possible templates for S1, we enumerate all the l1-combinations
(cid:1) such combinations. To this end, we

of the set of l positions in S; there are (cid:0) l
l1
use the standard enumeration algorithm for combinations as described in [37].

Generating Combinations. Generating combinations is simpler than BSPs. The
same symmetry observations hold here, i.e., the template for all combinations of
lengths (l1, l2) is the same as the one for all combinations of lengths (l2, l1). We
then generate all the possible l1-combinations of the set of l1 + l2 positions; here
again we use the standard algorithm described in [37].

3.3 Exploring the space of all sequential patterns

Our algorithm to explore the space of sequential patterns and exactly extract
the top-k sequential patterns with highest leverage2 is based on the OPUS Miner
algorithm [41, 47], ﬁrst introduced for mining non-sequential databases.

3.3.1 Sequential top-k OPUS Miner (SkOPUS)

OPUS Miner ﬁnds the top-k itemsets under given interestingness measures from a
non-sequential database, with regard to a given measure of interest. OPUS Miner
is a depth-ﬁrst branch-and-bound algorithm, that exploits the monotonicity of
itemsets (sup(I ∪ i) (cid:54) sup(I)). It performs an exhaustive depth-ﬁrst search of the
space of sequential patterns except in so far as it can prune parts of the search
space that cannot contain itemsets in the top-k with respect to the measure of
interest. For example, if the minimum (i.e. worse) value in the top-k is α, and we
know that any specialization of an itemset I will have an score that is lower than
α, then the sub-tree for which the root is I does not need to be explored.

OPUS Miner’s depth-ﬁrst search has the advantage that the number of open
nodes at any one time is minimized. This allows extensive data to be maintained
for every open node and ensures that only one such record will be stored at any
given time of each level of depth that is explored. It also promotes locality of
memory access, as most new nodes that are opened are minor variants of the most
recently explored node.

Algorithms 1 and 2 describe SkOPUS.The main elements that diﬀer from the

original OPUS Miner algorithm are as follows:

2 In the remainder of this paper, we will use the term ’leverage’ as a proxy for ’leverage

under our deﬁnition of expected support’.

12

Fran¸cois Petitjean et al.

1. When a sequential pattern is specialized, the specializing item is always placed
at the end of the sequential pattern (as a suﬃx – appending item i to sequence
S is noted S.i hereafter). This ensures that sub-spaces that are pruned will not
be reconsidered later in the search.

2. When retrieving an item from the queue of available items to specialize a
pattern with, the item is left in the queue (as opposed to OPUS Miner where
the item is removed from the queue). This is necessary to handle repetitions of
items in the patterns such as (cid:104)a, a, b(cid:105). Thus, we use the variant of OPUS [41]
that allows multiple applications of a single operator (the addition of a given
item).

3. Upper bounds for any sequential extension of a pattern S have to be adapted
for our measure of interest (leverage) and our deﬁnition of expected support;
we detail this element in Section 3.3.2.

Algorithm 1: SkOPUS Miner

Input: Sequential database D, a measure of interest M , integer k
Output: The top-k sequential patterns with regard to M
q ← a queue of all items i ∈ D in descending order on sup((cid:104)i(cid:105));
topK ← an empty queue to contain the top-k sequential patterns;
return ExpandSequence((cid:104)(cid:105), q, topK, M );

Algorithm 2: ExpandSequence

Input: A sequence S, a queue q, the current topK and a measure of interest M
Output: The top-k sequential patterns with regard to M in the search space explored

to date

initialize q(cid:48) to be an empty queue of items;
for all i ∈ q do
T ← (cid:104)S | i(cid:105);
score ← M (T );
if score > topK.min then
topK.add(T, score);

end
if U pper bound(M, T ) > topK.min then

q(cid:48) ← append(q(cid:48), i);

end

end
for all i ∈ q(cid:48) in descending order w.r.t. M do
topK ←ExpandSequence(T , q(cid:48), topK, M );

end
return topK;

Algorithm 1 simply initializes the necessary data structures to then call Algo-
rithm 2 that systematically explores the entire space of sequential patterns other
than those branches of the search space of which it can be determined no top-k
can be contained therein. The correctness of SkOPUS follows from the correctness
of OPUS. As SkOPUS investigates each pattern it maintains a list topK of the
top-k patterns found so far. Thus, on termination, as the entire search space has

SkOPUS

13

been explored other than those branches that cannot contain patterns in the top-k,
topK contains the top-k patterns.

3.3.2 Upper bounding

We now detail how to upper bound the score of any extension of a sequential
pattern S with regard to leverage (i.e., how to compute U pper bound(M, S(cid:63)) in
Algorithm 2). As presented earlier, we measure the interestingness of a pattern
with a leverage,

leverage = sup(S) − ExpSupport(S).

In order to prune the search space we need an upper-bound on leverage for
any extension pattern, say T , of a pattern S. Since ExpSupport(S) ≥ 0 and
sup(T ) ≤ sup(S), we immediately obtain leverage(T ) ≤ sup(S), allowing us to
use sup(S) as an upper bound. Note that, possibly surprisingly, the lower bound
on ExpSupport(S) is tight. Consider the example of a database where all sequences
start with the same pattern P of length (cid:96), and then do not use any of the elements
composing the pattern in the rest of the sequence. We will then have all sequential
compositions of all possible binary sequential partitions with 0 frequency, except
for the one with the actual pattern. The highest frequency will then be for the one
with the lowest number of compositions, which is (cid:96).

ExpSupport(P ) (cid:62) sup(P )

(cid:96)

Given that we want to bound all possible extensions of P , (cid:96) is unbounded, which
makes ExpSupport(P (cid:63)) > 0.

In the deﬁnitions above, we exploit the fact that support and expected support
are positive. It is interesting to note that, so long as the deﬁnition of expected
support does not allow negative values, these elements will remain valid. It follows
that our SkOPUS algorithm can be directly used with any sensible deﬁnition of
expected support.

3.3.3 Bootstrap — How to quickly get a good top-k?

In practice, the time requirements of SkOPUS depend on the eﬃciency of the prun-
ing mechanisms, and can vary greatly from dataset to dataset3. SkOPUS traverses
the search space maintaining the set of top-k sequential patterns discovered in the
search space explored so far. The search space can only be pruned of branches that
cannot contain a pattern with higher leverage than the kth best leverage found
so far. Thus, eﬃcient pruning relies critically on our ability to quickly ﬁll topK
with high-scoring patterns so that as much of the search space can be pruned as
possible.

We use two main strategies. First, we consider the addition (as suﬃx) of items
with highest support ﬁrst; this is apparent in the order in which we go through
the diﬀerent queues in Algorithms 1 and 2.

Algorithm 2 performs a depth-ﬁrst search. The primary reason for doing so is
that it limits the number of nodes in the search space that must be simultaneously

3 Note that if, for instance, most of the sequential patterns in a dataset are non-interesting,

then the algorithm will have to explore a large part of the space.

14

Fran¸cois Petitjean et al.

open. To eﬃciently compute the cover of a new node it is important to store the
cover of the parent and only update it with respect to the item appended to the
sequence. Minimizing the number of open nodes minimizes the number of covers
that must be stored. However, we store the cover of every item, and so this issue
does not aﬀect two-item sequences. Our second strategy uses a hybrid search.
First a breadth-ﬁrst search is performed over all two-item sequences. Then the
regular depth-ﬁrst search is employed for sequences of length three and greater.
We present the two-item breadth-ﬁrst bootstrap in Algorithm 3. The algorithm
remains correct as it still systematically searches all of the search space that might
contain top-k patterns, all that changes is the order in which they are explored.

Algorithm 3: SkOPUS Bootstrapping

Input: Sequential database D and a measure of interest M
Output: A preﬁlled topK
Let topK be an empty ordered queue
foreach i1, i2 ∈ I do
S ← (cid:104)i1, i2(cid:105)
Es ← ExpSupport(S)
score ← M (S, Es)
if score > topK.min then

topK ← topK.add(S, score)

end

end
return topK

4 Experiments

In this section, we present the results of SkOPUS for the exact extraction of
the k sequential patterns with highest leverage. In the synthetic experiments we
compare the performance of SkOPUS using leverage as the measure of interest
against SkOPUS using support as the measure of interest (included to provide
a contrast against frequent pattern discovery), state-of-the-art sequential pattern
discovery algorithm rprt [33] and baseline rind [13]. As the synthetic experiments
clearly show the baseline is less eﬀective than either SkOPUS using leverage or
rprt , we do not include it in the real-world experiments.

4.1 Datasets

We shall start this section by making a general observation about the availability
of interpretable sequential databases. Applications of sequential pattern mining
methods are numerous and include:

1. sequences of the diﬀerent webpages browsed by users on a website, where each

visit represent a new transaction in the database;

2. locations of series of events, such as the neighborhoods that a taxi drives
through for each client, or the restaurants visited by registered customers of
say Yelp R(cid:13) or Urbanspoon R(cid:13);

SkOPUS

15

3. the performance of diﬀerent industries on the share market over diﬀerent days

4. the sequence of actions performed by a surgeon over the course of diﬀerent

5. the tests performed on (and action taken about) patients from admission to

of trading;

surgeries;

discharge.

The datasets associated to all these applications are however extremely valuable
and hence only rarely made available freely to the scientiﬁc community. In ad-
dition, assessing the quality of technologies for pattern discovery requires having
knowledge about the patterns that are actually present in data. Therefore we start
by evaluating the discovery with sequences that are sampled from known distri-
butions with speciﬁc patterns. We can then compare the discovered interactions
to the true structure from which the data was sampled. This is the ﬁrst set of ex-
periments we present in Section 4.2. Then, in Section 4.3, we assess the relevance
and scalability of our approach on public domain literary works. We have selected
several books which we believe have characteristics that are quite representative
of many other types datasets.

It is important to note that the main objective of this paper is not to prove
that extracting patterns from sequential databases is an important topic; this has
in fact been largely motivated by the data mining community. Rather, we aim at
demonstrating 1. that interestingness-based sequential pattern extraction is critical
and 2. the relevance of our approach.

All the datasets used in this paper (as well as their “ground truths”, when avail-
able) are provided for reviewing purposes at https://github.com/fpetitjean/
Skopus.

4.2 Experiments on data with known patterns

We ﬁrst assess our method on datasets that embed a known set of sequential
patterns.

Details of the data generation process are as follows: we start by generating
a dataset with 10,000 random sequences over a vocabulary I with 10 tokens;
the probability of the tokens follows a ﬂat Dirichlet distribution: I ∼ Dir(1). We
generate each random sequence in the dataset by choosing its length ls distributed
accordingly to a shifted4 Poisson distribution: ls ∼ Pois(9.0) + 1. This makes the
sequences to have an average length of 10 with standard deviation 3 (E(lp) = 10,
Var(lp) = 9).

We then generate a set of k sequential patterns, each with an associated prob-
ability of occurrence in a sequence drawn uniformly in [0.05, 0.2] and length lp
following lp ∼ Pois(1.0) + 2, i.e. with average length 3 and standard deviation 1
(E(lp) = 10, Var(lp) = 1). Then, each pattern is embedded sequentially accord-
ing to its associated probability. Embedding is performed by uniformly at random
selecting insertion points in the sequence.

We then extracted the top-20 sequential patterns according to leverage and
compare the results with a support-based extraction5, rprt [33] and rind , a baseline

4 Shifting ensures that the sequences have at least 2 elements.
5 Note that we “help” the support-based extraction by not allowing it to test single items,

otherwise it would only return those.

16

Fran¸cois Petitjean et al.

Table 2 Results on data with known embedded patterns

#patterns Recall

1
2
3
4
5
6
7
8
9
10

Support

Leverage

rprt [33]

rind [13]

0%
0%
0%
25%
0%
33%
14%
0%
0%
0%

100%
100%
100%
100%
100%
67%
86%
25%
78%
40%

100%
100%
66%
100%
40%
67%
43%
12%
44%
20%

100%
50%
66%
75%
40%
33%
43%
12%
22%
10%

used by [33] using the indepdence model proposed by [13]. Both baselines require
a set of candidate patterns: here we used frequent patterns with a threshold of
500. Note that this threshold potentially “helps” these two methods, because none
of the injected patterns have a support lower than 500; such a threshold has thus
potential to prune a signiﬁcant number of patterns that could have been ranked
in the top-10. We report recall rates in Table 2, that is, proportions of all patterns
that were embedded that are included in the top-20 patterns returned by each
approach. As expected, the support-based method does not perform well. In fact,
it only ever extracted patterns of length 2; which mainly correspond to sequential
patterns that are frequent because their composing items are as well. This is
a similar behaviour to the one that we have explained with and, to and of
in
the introduction: patterns composed with frequent items will appear frequently
by chance, without being interesting. This means that top-k extraction based on
support has diﬃculty extracting patterns of length (cid:62) 3.

To further highlight this point, we shall mention that for the experiment with
a single embedded pattern (#patterns = 1 in Table 2), (cid:104)c, b, c, a(cid:105) embedded in
19% of the sequences, this pattern is ranked 92nd under support while it is the
top pattern under our leverage, rprt and rind . More generally, Table 2 shows that
all approaches outperform support-based approaches, by recovering a much larger
number of embedded patterns.

We can also observe that our method — SkOPUS with leverage — outperforms
rprt and rind by obtaining a signiﬁcantly higher recall of the embedded patterns.
It is also interesting to note that these experiments conﬁrm the ones in [33] by
showing the poorer performance of rind compared to rprt .

It can be observed that, as the number of patterns embedded in the data

increases, the recall decreases. This is due to three main factors:

1. because we keep the number of patterns extracted constant (k = 20), it is
normal that recall diminishes as the number of actual patterns increases;
2. our approach considers the subsequences of an embedded pattern to also be
interesting patterns: this means even if we were to extract an actual pattern
(cid:104)d, m, k, d(cid:105), we are also prone to extract subsequences of this patterns, such as

SkOPUS

17

Table 3 Details on the datasets embedding 7 patterns.

Injected patterns

Support

Leverage

(cid:104)i, g, d(cid:105)
(cid:104)d, e(cid:105)
(cid:104)a, e, j(cid:105)
(cid:104)i, b(cid:105)
(cid:104)j, e, j, g(cid:105)
(cid:104)j, d, c, i, a(cid:105)
(cid:104)j, h, i(cid:105)

17.1%
15.9%
14.6%
14.2%
10.5%
7.8%
7.1%

3.36%
0.85%
2.95%
4.26%
2.11%
3.58%
2.34%

(cid:104)d, m, k(cid:105) or (cid:104)d, m, d(cid:105). There is no counterpart of independent productivity from
self suﬃcient itemset mining whereby subpatterns can be discarded [40].

3. some patterns can overlap; for example if we have two (actual) patterns
(cid:104)a, e, e, i(cid:105) and (cid:104)e, f, i, d(cid:105) that are independently embedded in the data, then,
the pattern (cid:104)e, i(cid:105) is going to be even more frequent and can be extracted; even
though it is not one of the directly embedded patterns.

Note that the two last points actually complicate the extraction of patterns
within the top-20 as well. These three elements are best exempliﬁed with the top-
20 sequential patterns corresponding to the dataset containing 7 patterns, which
we illustrate in Table 3 and Table 4, where we report the exact matches and
subpatterns that are discovered, adopting the elements appropriate to sequential
patterns from the approach pioneered by Zimmermann [49] for non-sequential
patterns.

First, this table further illustrates the consistency of our method. We have
depicted the extracted patterns that are actual injected patterns in with a • next
to them. While our method can extract 6 out the 7 actual patterns within the
top-20, we can see that the support-based method only extracts one pattern (cor-
responding to a pair), and the two other methods only 3 actual patterns. It is also
interesting to note that the similarity between the 4 ﬁrst patterns extracted by
our method and by rprt .

It is informative to explain why the pattern (cid:104)d, e(cid:105) is not part of our top-20 while
it is part of the top-20 on the support: it it is quite a frequent pattern (support
of 63%), but this is actually mostly due to the support of d and e. This pattern
was introduced with a 16% probability; the diﬀerence is explained by the fact that
both tokens d and e were sampled with a relatively high probability (cid:62) 15%. This
means that the support method only extracts it in the top-20 because d and e
are frequent to start with; had they been infrequent tokens, the support wouldn’t
have ranked the pattern that high. This is exactly what happens with pattern
(cid:104)i, b(cid:105), for which token b has much lower probability (p((cid:104)b(cid:105)) = 0.02). Because it
does not appear by chance frequently, the support-based approach ranks the
pattern very low. It is also interesting to see that under support, pattern (cid:104)e, d(cid:105)
is only slightly less interesting, which clearly shows its inconsistency. Conversely,
we can see the consistent behaviour of the leverage-based approach, which ranks
(cid:104)i, b(cid:105) with much higher value than (cid:104)d, e(cid:105). Before this pattern was embedded into
the data it already appeared in about 61% of the sequences. Observing it in 63%
is thus not a very strong eﬀect and our method ranks it accordingly. On the other
hand, before introduction of (cid:104)i, b(cid:105), the pattern only appeared in about 13% of

18

Fran¸cois Petitjean et al.

Table 4 Top 20 results on the dataset embedding 7 patterns. Exact matches are represented
in blue with a • next to it; subsequences of such exact matches are depicted in boldface.

Leverage (value)

Support

rprt [33]

rind [13]

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Top-k

(cid:104)i, g(cid:105) (4.3%)
•(cid:104)i, b(cid:105) (4.3%)
•(cid:104)j, d, c, i, a(cid:105) (3.6%)
•(cid:104)i, g, d(cid:105) (3.4%)
(cid:104)d, c, i, a(cid:105) (3.2%)
(cid:104)j, c, i, a(cid:105) (3.2%)
•(cid:104)a, e, j(cid:105) (2.9%)
(cid:104)i, g, g(cid:105) (2.8%)
(cid:104)i, e(cid:105) (2.7%)
(cid:104)j, d, c, a(cid:105) (2.6%)
(cid:104)j, d, c, i(cid:105) (2.6%)
•(cid:104)j, h, i(cid:105) (2.3%)
(cid:104)a, e, e, j(cid:105) (2.3%)
(cid:104)a, e, e(cid:105) (2.3%)
(cid:104)j, d, i, a(cid:105) (2.2%)
•(cid:104)j, e, j, g(cid:105) (2.1%)
(cid:104)j, e, g(cid:105) (2.1%)
(cid:104)a, e, g(cid:105) (2.1%)
(cid:104)i, g, g, d(cid:105) (2.1%)
(cid:104)i, e, e(cid:105) (2.0%)

(cid:104)j, d(cid:105)
(cid:104)j, e(cid:105)
(cid:104)d, d(cid:105)
(cid:104)j, j(cid:105)
•(cid:104)d, e(cid:105)
(cid:104)d, j(cid:105)
(cid:104)e, j(cid:105)
(cid:104)i, d(cid:105)
(cid:104)e, e(cid:105)
(cid:104)j, g(cid:105)
(cid:104)i, d(cid:105)
(cid:104)g, d(cid:105)
(cid:104)d, g(cid:105)
(cid:104)i, j(cid:105)
(cid:104)i, e(cid:105)
(cid:104)e, g(cid:105)
(cid:104)g, j(cid:105)
(cid:104)j, i(cid:105)
(cid:104)d, i(cid:105)
(cid:104)g, e(cid:105)

•(cid:104)j, d, c, i, a(cid:105)
•(cid:104)i, b(cid:105)
•(cid:104)i, g, d(cid:105)
(cid:104)i, g(cid:105)
(cid:104)j, c, i, a(cid:105)
(cid:104)j, j, c, i, a(cid:105)
(cid:104)d, d, c, i, a(cid:105)
(cid:104)i, g, g(cid:105)
(cid:104)d, c, i, a(cid:105)
(cid:104)i, g, g, d(cid:105)
(cid:104)j, e, c, i, a(cid:105)
(cid:104)j, h, i(cid:105)
(cid:104)j, d, c, a(cid:105)
(cid:104)d, j, c, i, a(cid:105)
(cid:104)j, j, e, j, g(cid:105)
(cid:104)j, e, e, j, g(cid:105)
(cid:104)i, e, d(cid:105)
(cid:104)i, g, d, d(cid:105)
(cid:104)i, c, i, a(cid:105)
(cid:104)j, d, c, e, a(cid:105)

•(cid:104)j, d, c, i, a(cid:105)
(cid:104)d, d, c, i, a(cid:105)
(cid:104)j, j, c, i, a(cid:105)
(cid:104)j, c, i, a(cid:105)
(cid:104)d, c, i, a(cid:105)
(cid:104)j, e, c, i, a(cid:105)
(cid:104)j, e, e, j, g(cid:105)
(cid:104)d, j, c, i, a(cid:105)
(cid:104)j, d, c, e, a(cid:105)
(cid:104)i, i, b(cid:105)
(cid:104)j, d, c, a(cid:105)
•(cid:104)i, g, d(cid:105)
(cid:104)j, d, c, i(cid:105)
•(cid:104)j, e, j, g(cid:105)
(cid:104)i, g, g, d(cid:105)
(cid:104)j, d, e, j, g(cid:105)
(cid:104)j, e, j, j, g(cid:105)
(cid:104)j, d, c, i, g(cid:105)
(cid:104)j, d, c, j, a(cid:105)
(cid:104)j, d, d, i, a(cid:105)

the sequences, observing it 22% is thus more signiﬁcant and our method correctly
ranks it accordingly.

Moreover, we have also depicted in boldface (with no •) the patterns extracted
by our approach that correspond to subsequences of actual patterns that we have
also extracted. This mainly illustrates the two ﬁrst elements that we have noted, by
having 8 slots in our top-20 “consumed” by sub-patterns of high leverage. Although
this falls out of the scope of this ﬁrst attempt at extracting the top-k sequential
patterns with leverage, this is naturally echoing the work that has been done on
ﬁltered-top-k association discovery [40, 46]. More generally, these results call for a
reﬂection on the evaluation of sequential pattern mining procedures, similarly to
the work that has been performed in this area for non-sequential pattern mining
[49].

4.3 Experiments on literary works

We compared the top patterns from several works of literature and the JMLR
abstract dataset. For brevity, we only discuss the patterns obtained from JMLR.
For more information about other results, see our section Supplementary material
at the end of the manuscript.

We study in detail the results of the diﬀerent methods on the JMLR dataset
[35], which represents the abstracts of the papers published in the Journal of
Machine Learning Research. This dataset holds 788 abstracts (hence sequences),
which use 3, 844 words (items). The average length of abstracts is 96 words with

SkOPUS

19

Table 5 Top 10 results on abstracts of JMLR papers

Top-k patterns

# Support

Leverage

rprt [33]

1
2
3
4
5
6
7
8
9
10

(cid:104)algorithm, algorithm(cid:105)
(cid:104)learn, learn(cid:105)
(cid:104)learn, algorithm(cid:105)
(cid:104)algorithm, learn(cid:105)
(cid:104)data, data(cid:105)
(cid:104)learn,data(cid:105)
(cid:104)model, model(cid:105)
(cid:104)problem, problem(cid:105)
(cid:104)learn, result(cid:105)
(cid:104)problem, algorithm(cid:105)

(cid:104)paper, show (cid:105)
(cid:104)paper, result(cid:105)
(cid:104)support, vector, machin(cid:105)
(cid:104)paper, algorithm(cid:105)
(cid:104)support, vector (cid:105)
(cid:104)base, result(cid:105)
(cid:104)paper, method(cid:105)
(cid:104)learn,result(cid:105)
(cid:104)paper, propos(cid:105)
(cid:104)vector,machin(cid:105)

(cid:104)support, vector (cid:105)
(cid:104)support, vector, machin(cid:105)
(cid:104)support, machin(cid:105)
(cid:104)real, world(cid:105)
(cid:104)vector, machin(cid:105)
(cid:104)state, art(cid:105)
(cid:104)reproduc, hilbert(cid:105)
(cid:104)high, dimension(cid:105)
(cid:104)ﬁrst, second(cid:105)
(cid:104)larg, scale(cid:105)

a maximum length of 231. Since rprt requires a candidate set, we use frequent
patterns with a threshold of 10. For the other literature works we used a threshold
of 5.

We ﬁrst present the results and detail the computation times in the next sub-
section. The top-10 patterns are presented in Table 5 for our method (leverage),
support for reference, and the results from rprt ; having shown in the previous sec-
tion that our method and rprt outperform rind , we focus on other methods and
keep top-support patterns for reference.

The ﬁrst critical observation echoes the ones we made in the introduction about
the inconsistency of support-based extraction: when using the support, most of
the patterns correspond in repetitions of very frequent words, such as algorithm,
learn, data, model and problem. Moreover we can see that when using the support
as the measure of interest, the pattern (cid:104)learn, algorithm(cid:105) and its reversed version
(cid:104)algorithm, learn(cid:105) appear with very similar scores (resp. supports 36% and 29%).
In contrast, we can observe that our method and rprt present patterns that seem
of higher general interest than support. Interestingly, the patterns extracted seem
to diﬀer signiﬁcantly between the methods. Beyond highlighting the importance of
synthetic data experiments, this also re-conﬁrms the subjectivity of interestingness.
We ﬁnd then interesting to examine two elements:

1. The overlap of the diﬀerent methods.
2. The patterns that crystallize the diﬀerences between the methods.

Overlap between SkOPUS and rprt For each method, we examine what percentage
of its top-k is found in the top-k of the other. Figure 1 presents this overlap analysis
for SkOPUS and rprt : in (a) their top-100 and (b) their top-100. For example, a
point at coordinates (x, y) for the top-100 tells us that y percent of the top-x of
the ﬁrst method was found in the top-100 of the other. Two similar results would
then have a slowly decreasing rate. In this case we observe quite the opposite:
there is only little overlap between the two methods with less than 20% overlap
within the top-100 and top-1000. Interestingly, SkOPUS seem to ﬁnd more of the
top patterns ranked by rprt than the opposite, which supports the relevance of our
method.

20

Fran¸cois Petitjean et al.

(a) Top-100

(b) Top-1000

Fig. 1 Overlap between the top-k given by SkOPUS and rprt

Analysis of representative diﬀerences Signiﬁcant diﬀerences exist even in the very
top patterns, which clearly appears in Figure 1(a), which shows that patterns as
early as in SkOPUS’ third position are not part of the top-100 of the patterns
ranked by rprt . We detail below a few such example patterns that are highly
contrasting the diﬀerences between SkOPUS and rprt :

– Patterns SkOPUS ranks high but not rprt : (cid:104)paper, algorithm(cid:105) is extracted
as the 4th pattern with highest leverage while it is ranked 7946 by rprt . This
pattern is extracted by SkOPUS for its leverage because this succession appears
174 times in the dataset, while its reversed pattern (cid:104)algorithm, paper(cid:105) appears
only 80 times. It seems reasonable to be highly rank a pattern that occurs with
more than twice the frequency of its reverse. rprt ranks this pattern much lower
because paper and algorithm are both probable individually (resp. support
of 43% and 58%). A similar phenomenon is seen for (cid:104)base, result(cid:105) — which
SkOPUS ranks 6 vs. 650 for rprt — and for (cid:104)learn, result(cid:105)— which SkOPUS
ranks 8 vs. 25, 810 for rprt .

– Patterns rprt ranks high but not SkOPUS: (cid:104)reproduc, hilbert(cid:105) is extracted as
the 7th pattern by rprt while it is ranked 696 by SkOPUS. This pattern is ex-
tracted by rprt because it consists of two relatively rare items, reproduc occurs
32 times and hilbert occurs 36 times. On the other hand, SkOPUS ranks this
pattern relatively low because it appears in only 28 abstracts. While under our
measure of expected support this pattern has high lift, it has low leverage, the
latter being a measure that favors patterns that appear the greatest number of
times in excess of the expected. It is also interesting to note that SkOPUS ranks
the pattern (cid:104)reproduc, hilbert, spac(cid:105) much higher than (cid:104)reproduc, hilbert(cid:105) (390
vs. 696). It seems natural that if all abstracts that include (cid:104)reproduc, hilbert(cid:105)
are followed by space, then the pattern of length 3 should be more interesting.
SkOPUS’ rank respects this ordering while rprt does not in this case, having
the pattern (cid:104)reproduc, hilbert, spac(cid:105) ranked 42nd. Note also that the pattern
(cid:104)hilbert, spac(cid:105) occurs less frequently than (cid:104)reproduc, hilbert(cid:105), as some papers
mention “reproduc[ing] the Hilbert norm”, but not space (see e.g. the paper
by F. Bach at http://www.jmlr.org/papers/v9/bach08b.html).

SkOPUS

4.4 Execution time

21

We ﬁnish the experiments by showing the running time of our approach; all ex-
periments are performed using a standard desktop computer.

Results are reported in Table 6. Not surprisingly, extracting the top-20 under
support with SkOPUS is extremely fast, because the most frequent elements are
encountered very early in the exploration of the search space;6 no top-20 were
actually presenting patterns of more than length 2 regardless of the experiments.
For other methods, it can be observed than synthetic data are extremely chal-
lenging, mostly because we included only a few patterns with reasonable proba-
bility. Results are obtained in a few hours for SkOPUS under leverage and less
than a minute for rprt . The main reason why rprt is faster is due to a high min-
ing threshold of 500 without which extraction cannot be performed, because if
we lower this threshold then the mining step will take a considerate time due to
a frequent pattern explosion. On the other hand, SkOPUS does not require any
threshold.

Finally, our approach exhibits extremely competitive running time compared
to support on real-world datasets where interesting patterns are more present. It
only takes SkOPUS 37s to extract the top-20 patterns in the JMLR dataset; only
slightly more than twice the time taken for support-extraction, and signiﬁcantly
faster than rprt with minimum support set to 10. On other literary works, SkOPUS
ﬁnishes in less than a few minutes, while rprt is generally ﬁnishes in seconds but
it required to set a minimum support threshold of 5.

It is ﬁnally interesting to note that the algorithmic complexity of SkOPUS and
rprt varies with diﬀerent elements. For SkOPUS, it is a function of how interesting
the patterns in the data are: data that holds patterns of high interest will prune
signiﬁcantly large parts of the search space (and the earlier they are found, the
quicker the process). For rprt , the interests of the patterns that the data holds
is almost neutral to the complexity, which will mostly vary as a function of the
number of frequent closed patterns.

5 Conclusion

This paper introduced a new general deﬁnition for the expected support of se-
quential patterns, which speciﬁcally focuses on the order within the pattern.

We have described the intuition behind our deﬁnitions, as well as eﬃcient algo-
rithms for both the computation of the expected support and the exact exploration
of the search space. Put together, these contributions allowed us to introduce SkO-
PUS which constitutes, to the best of our knowledge, the ﬁrst framework for the
exact mining of the k sequential patterns with highest leverage from data.

Experiments on controlled data have validated the consistency and relevance

of our framework, relevance that we have then conﬁrmed on literary work.

This work naturally raised a number of questions and issues that, we anticipate,

will be of great interest to the community in the future. These include:

6 Although support is often not a good proxy for interestingness, it remains that SkOPUS

can be very eﬃciently and exactly extract the most frequent sequential patterns.

22

Fran¸cois Petitjean et al.

Table 6 Running time on the diﬀerent datasets for extracting the top-20.

Runtimes

Name

Support

Leverage

rprt [33] (mining + ranking)

Synth-1
Synth-2
Synth-3
Synth-4
Synth-5
Synth-6
Synth-7
Synth-8
Synth-9
Synth-10
JMLR
Lawyers
Finn
Sawyer
NewPhysics
TwoCities
Animals

< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
16s
26s
6s
7s
5s
14s
8s

1h41m57s
1h44m28s
53m19s
1h24m40s
1h34m41s
3h55m33s
6h5m31s
1h25m25s
2h28m30s
8h58m53s
37s
3m51s
1m8s
1m23s
48s
2m16s
1m46s

5s + 4s
5s + 5s
9s + 9s
6s + 6s
7s + 7s
10s + 11s
18s + 19s
16s + 14s
21s + 24s
8m15s + 6m16s
6m47s + 1m3s
13s + 2s
5s + 2s
2s + 1s
3s + 1s
6s + 2s
1s + 1s

1. ﬁnding heuristics to quickly ﬁll the temporary top-k with patterns of high-

interest, possibly by integrating some background knowledge;

2. reﬁning upper bounds for the most common interestingness measures;
3. ﬁltering the results to remove trivial subpatterns from those that are discovered

(patterns highlighted in boldface in Table 3);

4. assessing the statistical signiﬁcance of the extracted patterns, so that only
patterns that have high-likelihood to be observed in future data would be
extracted (as initiated by [9, 3]);

5. using background knowledge to deﬁne a model of the joint distribution, and
then extract the patterns that diﬀer the most from it; such an approach has
been investigated by S. Jaroszewicz on non-sequential patterns, and models
the joint distribution with a Bayesian Network [18];

6. many domains hold patterns of the type “admission, then biopsy or blood-test,
and then surgery”. Our work focuses on the extraction of chains of events (serial
episodes); we believe that extension to the more general classes of episodes will
be of major interest.

Acknowledgements We are grateful to the anonymous reviewers and to Matteo Riondato
for valuable and constructive comments on this manuscript.

Supplementary material

For completeness, we make available the top-100 patterns extracted from six lit-
erary works (see Table 7) and for all the methods discussed at https://github.
com/fpetitjean/Skopus/.

SkOPUS

23

Table 7 Characteristics of the literary datasets

Name

|I|

|D|

Avg. length Max. length

The Industries of Animals
A Book About Lawyers
Adventures of Huckleberry Finn
A Tale of Two Cities
The Adventures of Tom Sawyers
The New Physics and Its Evolution

5547
8386
4504
6392
5073
3832

4931
4787
6402
8584
5259
3081

12.8
21.9
13.7
12.4
10.7
19.4

107
211
213
150
119
81

Compliance with Ethical Standards

This material is based upon work supported by the Air Force Oﬃce of Scien-
tiﬁc Research, Asian Oﬃce of Aerospace Research and Development (AOARD)
under award number FA2386-15-1-4007. This research was also supported by
China Special Fund for Meteorological Research in the Public Interest under grant
GYHY201306070, Jiangsu Government Scholarship for Overseas Studies; and by
the Australian Research Council under grant DP140100087.

References

1. The Oxford English Corpus: Facts about Language.

In: Oxford Dictionar-
http://www.oxforddictionaries.com/words/

ies. Oxford University Press (2015).
the-oec-facts-about-the-language

2. Achar, A., Laxman, S., Viswanathan, R., Sastry, P.: Discovering injective episodes with
general partial orders. Data Mining and Knowledge Discovery 25(1), 67–108 (2012)
3. Achar, A., Sastry, P.: Statistical signiﬁcance of episodes with general partial orders. In-

formation Sciences 296(0), 175–200 (2015)

4. Aggarwal, C.C., Han, J. (eds.): Frequent pattern mining. Springer (2014)
5. Agrawal, R., Imielinski, T., Swami, A.: Mining association rules between sets of items in
large databases. In: Proceedings of the 1993 ACM-SIGMOD International Conference on
Management of Data, pp. 207–216. Washington, DC (1993)

6. Agrawal, R., Srikant, R.: Mining sequential patterns.

In: Proceedings of the Eleventh
International Conference on Data Engineering, March 6-10, 1995, Taipei, Taiwan, pp.
3–14. IEEE Computer Society (1995)

7. Bayardo Jr, R.J., Agrawal, R.: Mining the most interesting rules. In: Proceedings of the
ﬁfth ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 145–154. ACM (1999)

8. Boley, M., Horv´ath, T., Wrobel, S.: Eﬁcient discovery of interesting patterns based on

strong closedness. Statistical Analysis and Data Mining 2(5-6), 346–360 (2009)

9. Castro, N.C., Azevedo, P.J.: Signiﬁcant motifs in time series. Statistical Analysis and

Data Mining: The ASA Data Science Journal 5(1), 35–53 (2012)

10. Fournier-Viger, P., Gomariz, A., Gueniche, T., Mwamikazi, E., Thomas, R.: TKS: eﬃcient
mining of top-k sequential patterns.
In: Advanced Data Mining and Applications, 9th
International Conference, ADMA 2013, Hangzhou, China, December 14-16, 2013, Pro-
ceedings, Part I, Lecture Notes in Computer Science, vol. 8346, pp. 109–120. Springer
(2013)

11. Geng, L., Hamilton, H.J.: Choosing the right lens: Finding what is interesting in data

mining. In: Quality Measures in Data Mining, pp. 3–24. Springer (2007)

12. Gwadera, R., Atallah, M.J., Szpankowski, W.: Markov models for identiﬁcation of signiﬁ-
cant episodes. In: SIAM International Conference on Data Mining, pp. 404–414 (2005)
13. Gwadera, R., Atallah, M.J., Szpankowski, W.: Reliable detection of episodes in event

sequences. Knowledge and Information Systems 7(4), 415–437 (2005)

24

Fran¸cois Petitjean et al.

14. Gwadera, R., Crestani, F.: Ranking sequential patterns with respect to signiﬁcance. In: Ad-
vances in Knowledge Discovery and Data Mining, 14th Paciﬁc-Asia Conference, PAKDD
2010, Hyderabad, India, June 21-24, 2010. Proceedings. Part I, Lecture Notes in Computer
Science, vol. 6118, pp. 286–299. Springer (2010)

15. H¨am¨al¨ainen, W.: Eﬃcient discovery of the top-k optimal dependency rules with Fisher’s
exact test of signiﬁcance. In: ICDM 2010, The 10th IEEE International Conference on Data
Mining, Sydney, Australia, 14-17 December 2010, pp. 196–205. IEEE Computer Society
(2010)

16. Han, J., Cheng, H., Xin, D., Yan, X.: Frequent pattern mining: current status and future

directions. Data Mining and Knowledge Discovery 15(1), 55–86 (2007)

17. Han, J., Pei, J., Mortazavi-Asl, B., Chen, Q., Dayal, U., Hsu, M.: Freespan: frequent
pattern-projected sequential pattern mining. In: Proceedings of the sixth ACM SIGKDD
international conference on Knowledge discovery and data mining, Boston, MA, USA,
August 20-23, 2000, pp. 355–359. ACM (2000)

18. Jaroszewicz, S., Simovici, D.A.: Interestingness of frequent itemsets using bayesian net-
works as background knowledge. In: Proceedings of the Tenth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 178–196. ACM (2004)

19. Lam, H.T., Moerchen, F., Fradkin, D., Calders, T.: Mining compressing sequential pat-
terns. Statistical Analysis and Data Mining: The ASA Data Science Journal 7(1), 34–52
(2014)

20. Low-Kam, C., Ra¨ıssi, C., Kaytoue, M., Pei, J.: Mining statistically signiﬁcant sequential
patterns. In: 2013 IEEE 13th International Conference on Data Mining, Dallas, TX, USA,
December 7-10, 2013, pp. 488–497. IEEE Computer Society (2013)

21. Mabroukeh, N.R., Ezeife, C.I.: A taxonomy of sequential pattern mining algorithms. ACM

Computing Surveys (CSUR) 43(1), 3 (2010)

22. Mampaey, M., Vreeken, J., Tatti, N.: Summarizing data succinctly with the most infor-
mative itemsets. ACM Transactions on Knowledge Discovery from Data 6(4), 16 (2012)
23. Mannila, H., Toivonen, H., Verkamo, A.I.: Discovering frequent episodes in sequences.
In: Proceedings of the First International Conference on Knowledge Discovery and Data
Mining (KDD-95), Montreal, Canada, August 20-21, 1995, pp. 210–215. AAAI Press (1995)
24. Mannila, H., Toivonen, H., Verkamo, A.I.: Discovery of frequent episodes in event se-

quences. Data Mining and Knowledge Discovery 1(3), 259–289 (1997)

25. Masseglia, F., Cathala, F., Poncelet, P.: The PSP approach for mining sequential patterns.
In: Principles of Data Mining and Knowledge Discovery, Second European Symposium,
PKDD ’98, Nantes, France, September 23-26, 1998, Proceedings, Lecture Notes in Com-
puter Science, vol. 1510, pp. 176–184. Springer (1998)

26. Mooney, C.H., Roddick, J.F.: Sequential pattern mining–approaches and algorithms. ACM

Computing Surveys (CSUR) 45(2), 19 (2013)

27. Pei, J., Han, J., Mortazavi-Asl, B., Pinto, H., Chen, Q., Dayal, U., Hsu, M.: Preﬁxspan:
Mining sequential patterns by preﬁx-projected growth. In: Proceedings of the 17th In-
ternational Conference on Data Engineering, April 2-6, 2001, Heidelberg, Germany, pp.
215–224. IEEE Computer Society (2001)

28. Piatetsky-Shapiro, G.: Discovery, analysis, and presentation of strong rules.

In:
G. Piatetsky-Shapiro, J. Frawley (eds.) Knowledge Discovery in Databases, pp. 229–248.
AAAI/MIT Press, Menlo Park, CA. (1991)

29. Ra¨ıssi, C., Calders, T., Poncelet, P.: Mining conjunctive sequential patterns. Data Mining

and Knowledge Discovery 17(1), 77–93 (2008)

30. Siebes, A., Vreeken, J., van Leeuwen, M.: Item sets that compress. In: Proceedings of the
Sixth SIAM International Conference on Data Mining, April 20-22, 2006, Bethesda, MD,
USA, pp. 395–406. SIAM (2006)

31. Tatti, N.: Signiﬁcance of episodes based on minimal windows.

In: IEEE International

Conference on Data Mining, pp. 513–522 (2009)

32. Tatti, N.: Discovering episodes with compact minimal windows. Data Mining and Knowl-

edge Discovery 28(4), 1046–1077 (2014)

33. Tatti, N.: Ranking episodes using a partition model. Data Mining and Knowledge Discov-

ery pp. 1–31 (2015)

34. Tatti, N., Mampaey, M.: Using background knowledge to rank itemsets. Data Mining and

Knowledge Discovery 21(2), 293–309 (2010)

35. Tatti, N., Vreeken, J.: The long and the short of it: Summarising event sequences with
serial episodes. In: Proceedings of the Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 462–470 (2012)

SkOPUS

25

36. Tew, C.V., Giraud-Carrier, C.G., Tanner, K.W., Burton, S.H.: Behavior-based clustering
and analysis of interestingness measures for association rule mining. Data Mining and
Knowledge Discovery 28(4), 1004–1045 (2014)

37. Tucker, A.: Applied Combinatorics. John Wiley & Sons, Inc., New York, NY, USA (2006)
38. Tzvetkov, P., Yan, X., Han, J.: TSP: mining top-k closed sequential patterns. In: Pro-
ceedings of the 3rd IEEE International Conference on Data Mining (ICDM 2003), 19-22
December 2003, Melbourne, Florida, USA, pp. 347–354. IEEE Computer Society (2003)
39. Vreeken, J., van Leeuwen, M., Siebes, A.: Krimp: mining itemsets that compress. Data

Mining and Knowledge Discovery 23(1), 169–214 (2011)

40. Webb, G.: Self-suﬃcient itemsets: An approach to screening potentially interesting as-
sociations between items. Transactions on Knowledge Discovery from Data 4, 3:1–3:20
(2010)

41. Webb, G.I.: OPUS: an eﬃcient admissible algorithm for unordered search. Journal of

Artiﬁcial Intelligence Research (JAIR) 3, 431–465 (1995)

42. Webb, G.I.: Eﬃcient search for association rules.

In: Proceedings of the Sixth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 99–
107. ACM (2000)

43. Webb, G.I.: Discovering signiﬁcant patterns. Machine Learning 68(1), 1–33 (2007)
44. Webb, G.I.: Layered critical values: A powerful direct-adjustment approach to discovering

signiﬁcant patterns. Machine Learning 71(2-3), 307–323 (2008)

45. Webb, G.I.: Self-suﬃcient itemsets: An approach to screening potentially interesting asso-
ciations between items. ACM Transactions on Knowledge Discovery from Data (TKDD)
4(1), 3 (2010)

46. Webb, G.I.: Filtered-top-k association discovery. Wiley Interdisc. Rev.: Data Mining and

Knowledge Discovery 1(3), 183–192 (2011)

47. Webb, G.I., Vreeken, J.: Eﬃcient discovery of the most interesting associations. ACM

Transactions on Knowledge Discovery from Data 8(3), 1–31 (2014)

48. Yan, X., Han, J., Afshar, R.: Clospan: Mining closed sequential patterns in large databases.
In: Proceedings of the Third SIAM International Conference on Data Mining, San Fran-
cisco, CA, USA, May 1-3, 2003, pp. 166–177. SIAM (2003)

49. Zimmermann, A.: Objectively evaluating interestingness measures for frequent itemset
In: J. Li, L. Cao, C. Wang, K. Tan, B. Liu, J. Pei, V. Tseng (eds.) Trends
mining.
and Applications in Knowledge Discovery and Data Mining, Lecture Notes in Computer
Science, vol. 7867, pp. 354–366. Springer Berlin Heidelberg (2013)

Data Mining and Knowledge Discovery manuscript No.
(will be inserted by the editor)

SkOPUS
Mining top-k sequential patterns under leverage

Fran¸cois Petitjean · Tao Li · Nikolaj Tatti ·
Geoﬀrey I. Webb

8
1
0
2
 
b
e
F
 
5
 
 
]
I

A
.
s
c
[
 
 
4
v
9
0
0
8
0
.
6
0
5
1
:
v
i
X
r
a

Received: 14 December 2015 / Accepted: 23 May 2016 / Published online: 14 June 2016

Abstract This paper presents a framework for exact discovery of the top-k se-
quential patterns under Leverage. It combines (1) a novel deﬁnition of the expected
support for a sequential pattern — a concept on which most interestingness mea-
sures directly rely — with (2) SkOPUS: a new branch-and-bound algorithm for
the exact discovery of top-k sequential patterns under a given measure of interest.
Our interestingness measure employs the partition approach. A pattern is inter-
esting to the extent that it is more frequent than can be explained by assuming
independence between any of the pairs of patterns from which it can be com-
posed. The larger the support compared to the expectation under independence,
the more interesting is the pattern. We build on these two elements to exactly ex-
tract the k sequential patterns with highest leverage, consistent with our deﬁnition
of expected support.

We conduct experiments on both synthetic data with known patterns and
real-world datasets; both experiments conﬁrm the consistency and relevance of
our approach with regard to the state of the art.

This article was published in Data Mining and Knowledge Discovery and is

accessible at http://dx.doi.org/10.1007/s10618-016-0467-9.

Keywords data mining · pattern mining · sequential data · exact discovery ·
interestingness measures

Francois Petitjean
Faculty of Information Technology, Monash University
E-mail: francois.petitjean@monash.edu

Tao Li
School of Electronic and Information Engineering, Nanjing University of Information Science
and Technology, E-mail: litao@nuist.edu.cn

Nikolaj Tatti
Department of Information and Computer Science, Aalto University
E-mail: nikolaj.tatti@aalto.ﬁ

Geoﬀrey I. Webb
Faculty of Information Technology, Monash University
E-mail: geoﬀ.webb@monash.edu

2

1 Introduction

Fran¸cois Petitjean et al.

Extracting interesting patterns from data is a core data mining task. This pa-
per introduces a method to eﬃciently and exactly identify the top-k patterns
in a sequential database, using as the measure of interest leverage — the diﬀer-
ence between a pattern’s observed and expected frequency. To deﬁne the expected
frequency, we use a maximum-likelihood estimate under an assumption of inde-
pendence between any pair of patters from which it can be composed.

The notion of interestingness is at the core of this paper. In early work on
pattern mining, patterns were considered interesting if they appeared frequently
in data [5]. The underlying idea was that the fact that something happens often is
useful information for the data practitioner. However, as a mature body of research
has now shown [30, 43, 16, 44, 8, 15, 34, 39, 22, 47, 36], frequency is often a poor proxy
for interestingness. One reason for this is that many patterns should be expected
to be frequent in real data. For instance, in the traditional market basket use
case, if 90% of people buy apples and 90% of people buy pears, then the pattern
{apples, pears} will be frequent even if the two events are completely independent.
When handling large databases, this phenomenon creates a deluge of frequent but
uninteresting patterns. This is especially problematic for real-world applications,
because the most frequent patterns will often be well-known; it is less frequent
interactions within the data that are most likely to provide novel insights.

When tackling sequential databases, this issue becomes even more critical;
patterns such as “buying apples and subsequently buying pears” will be extremely
common.

This is a signiﬁcant problem in real-world data, even with relatively short
sequences. As a simple demonstration of this, we report in Table 1 the ﬁve most
frequent sequential patterns in the book “The Adventures of Tom Sawyer”, with
every sentence constituting a record in the database (the rest of the paper will
make it clear how we have performed this extraction).

Table 1 The ﬁve most frequent sequential patterns in the Adventures of Tom Sawyer.

pattern

support

(cid:104)and, and(cid:105)
(cid:104)and, to(cid:105)
(cid:104)to, and(cid:105)
(cid:104)of, and(cid:105)
(cid:104)and, of (cid:105)

13%
9.8%
9.1%
8.6%
8.0%

We can observe that most frequent sequential patterns are not interesting, and
simply correspond to the permutations of the most frequent word ‘and’ and each
of the two next most frequent words ‘to’ and ‘of’. This has motivated several ap-
proaches to mining interesting sequential patterns, which we detail in Section 2.
The above example illustrates two main points about the incompatibility of fre-
quency as a proxy for interestingness in sequential pattern discovery:

1. The main reason for the high frequency of all these patterns, is the frequency of
the individual words. In English, it has been shown that one word in four comes

SkOPUS

3

from {the, be, to, of, and, a, in, that, have, I} [1]. It is then not surprising that a
sentence would contain several occurrences of those. If the sequences are long
enough, any sequence of independent events (which we believe will not often
be of interest) will become frequent. In fact, for any pattern with probability
p > 0 of occurring at any time in the data, the probability of its repetition
tends to 100% as the length of the sequences in the database increases (this
is even true for singletons).1 This clearly demonstrates that frequency is not a
good proxy for interestingness for sequential databases.

2. Let us consider the second and third most frequent sequential patterns (cid:104)and, to(cid:105)
and (cid:104)to, and(cid:105). These two patterns have similar frequency. This directly ques-
tions the relevance of frequency as an interestingness measure. For sequential
patterns the order of items should be key to determining whether the pat-
tern is interesting. If all the orderings of the terms in a sequential pattern are
equally frequent then an unordered pattern, such as an itemset, captures all
of the information about the potentially interesting regularity in the data. We
argue that the sequential pattern should be reserved for regularities that can-
not be fully captured by unordered patterns. In this case, we can see that with
such simple patterns of length 2, frequency ranks the two possible orderings
of {and, to} as close to equivalent. The question is then: ”Should a pattern
(cid:104)i1, i2(cid:105) be considered sequentially interesting if it appears as often as (cid:104)i2, i1(cid:105) in
the database?” We argue below that this contradicts the natural deﬁnition of
interestingness for sequential patterns.

In this paper, we make the two following contributions:

1. Scoring: Expected support is the core element of standard pattern mining
measures of interest, such as leverage or lift [46], because most measures of
interest involve a comparison between the observed support of a pattern in
the data and its expected support [28]. We introduce a new deﬁnition for
the expected support of a sequential pattern and present an algorithm for its
computation. Following our motivation above, our approach tries to ﬁnd a
model that is local to the tested pattern by considering its re-orderings.

2. Search: We introduce SkOPUS: a sequential extension of the branch-and-bound
OPUS algorithm [47]. SkOPUS can extract, exactly and eﬃciently, the k se-
quential patterns with highest leverage, i.e., the k patterns with the highest
diﬀerence between their observed and expected support. Note that we will show
that our algorithm is not limited to our deﬁnition of the expected support, and
can be directly used to extract the top-k patterns under any deﬁnition of the
expected support.

Our paper is divided into ﬁve main sections. We present the related research in
Section 2. In Section 3, we detail the proposed framework for the discovery of the
top-k patterns with highest leverage. In Section 4, we present the results of exper-
iments conducted on both synthetic data for which we control what patterns are
actually present, and on real-world datasets. We conclude the paper and present
some future work in Section 5.

1 If p(“buying apples”) > 0 and p(“buying pears”) > 0 and these events are independent,
then the probability of observing their sequence in data follows: liml→∞ p((cid:104)apples, pears(cid:105)) = 1

4

2 Related work

Fran¸cois Petitjean et al.

We structure the related work around the two main elements that this paper
addresses: discovery of sequential and interesting patterns.

2.1 Mining interesting non-sequential patterns

There is a very mature body of research about non-sequential frequent pattern
mining [16]. As raised in the introduction, it has now long been identiﬁed that the
major issue is not whether we can derive the complete set of frequent patterns
under certain constraints, but whether we can derive a compact but high-quality
set of patterns that will be useful for most applications [16, 8, 34].

While the major focus has been on eﬃcient discovery of frequent patterns,
there is a growing body of research into identifying interesting patterns. Several
methods aim at ﬁnding the set of patterns that will best describe the dataset,
using a variety of methods for scoring the set such as entropy [22] or information
theoretic frameworks [30, 39]. Other approaches rather try to deﬁne measures of
interest for patterns [36, 18], and then perform the extraction of the most interest-
ing patterns for diﬀerent measures [7, 42, 11, 15, 47]. We refer the reader to [4], for
a more complete survey on frequent pattern mining, and discovering interesting
patterns.

2.2 Mining frequent sequential patterns

Sequential pattern mining extends frequent pattern mining to sequential databases
[21, 26]. Real-world applications are numerous and include analysis of the purchase
behaviour of customers over time, analysis of Web clickstreams, and study of bi-
ological sequences. Algorithms for mining frequent sequential patterns from se-
quential databases were ﬁrst proposed in the 1990s [6, 23, 24]. After these seminal
papers, researchers quickly moved to the development of algorithms for the extrac-
tion of frequent sequential patterns of higher complexity (sequences of itemsets)
[25, 27, 17, 48, 29, 38, 10].

Note that diﬀerent researchers have used diﬀerent deﬁnitions of support and
that these directly inﬂuence the patterns that they extract. Some methods, like
ours, consider the support as being the number of sequences that have the sequen-
tial pattern as a subsequence, while others consider the number of times that the
pattern occurs in all the sequences of the dataset, multiple counting a sequence
that embeds the subsequence multiple times. Some application domains and prob-
lems will beneﬁt from one approach while others will from the other approach.
Our techniques extend directly to the second deﬁnition, should such an approach
be desirable, the primary change simply being in the counting of the support of a
pattern and its sub-patterns.

2.3 Mining interesting sequential patterns

It is only in the last decade that extracting interesting sequential patterns has
emerged as an important research topic.

SkOPUS

5

We distinguish two main families of methods:

1. methods using information theoretic approaches, such as the Minimum De-
scription Length, to score a set of patterns that best explain the dataset.
2. methods deﬁning an expected support for the patterns under a null model (or
hypothesis), and using it to score and rank the patterns by comparison to the
actual observed support of the pattern

Minimum Description Length methods scoring a set of patterns In [35, 19], the
authors propose a Minimum Description Length (MDL) approach for scoring a set
of patterns relative to a dataset as well as a heuristic search method to construct
the set. Here the idea is to score a pattern set instead of a just individual patterns;
these patterns should explain the data well but at the same time be non-redundant.
These goals are quantiﬁed, with scores derived from MDL principles.

Methods based on expected support Deriving an expected support under some null
hypothesis is particularly complex for sequential patterns. Several approaches have
made independence assumptions between the elements composing the sequential
patterns. That is, they use a null hypothesis that the data are generated by a
0-order Markov model or stationary and independent stochastic process [13, 31,
14, 20].

Approaches to deriving expected support from Markov chains have been pro-
posed in [12, 9]; in particular, the statistical signiﬁcance of the extracted patterns
is studied in [9]. More complex Markov models have been studied in [2], with
statistical signiﬁcance studied in [3].

Tatti [32] takes a diﬀerent approach to interestingness and posits that for some
applications, interesting patterns will be the ones that occur in a short window.
He then builds a model of the expected length of a sequence for a pattern to occur,
and compares it to the actual one.

Finally, in very recent work Tatti [33] introduces an approach that is the most
related to ours. This aproach takes inspiration from Webb’s approach to ﬁnding
interesting (non-sequential) itemsets [45], and builds the expected support by look-
ing at diﬀerent partitions of the sequential pattern. It then uses the derived score
to rank the strict episodes. The key diﬀerence between our work in this paper and
that in Tatti [33] is the information from which the expectation is derived: Tatti
[33] uses item probabilities as well as the number of gaps in a subpattern to derive
the expectation whereas, in this work, we compare the pattern against all alter-
native orderings of the same items. Among other technical diﬀerences, Tatti [33]
deﬁnes its measure for strict episodes whereas we focus on sequential patterns,
that is, serial episodes. Finally, the way we measure the diﬀerence between the
observed support and expected support allows us to use a monotonic bound, and
essentially mine top-k episodes without ﬁrst generating a candidate set, whereas
the previously discussed methods require that a candidate set be ﬁrst generated,
typically a set of frequent patterns mined with a low threshold.

3 Extracting the Top-K interesting sequential patterns

We introduce our method in this section. We ﬁrst introduce some notation. We
then detail our model for the expected frequency as well as how to derive our

6

Fran¸cois Petitjean et al.

interestingness measure from it – leverage. Finally, we introduce our eﬃcient search
algorithm to extract the most interesting sequential patterns under these measures.
Note that we have made all code freely available to allow independent conﬁr-
mation and extensions of our work at https://github.com/fpetitjean/Skopus.

3.1 Deﬁnitions

A sequence S over a ﬁnite set of items I is an ordered list (cid:104)s1, . . . , s(cid:96)(cid:105), with
si ∈ I, 1 ≤ i ≤ (cid:96), where (cid:96) is the length of sequence S, denoted by |S| = (cid:96). A
sequential database D = {S1, S2, . . . , Sn} of size n is a multiset of n sequences
over I, where each Si is a sequence.
Deﬁnition 1 (Sub-sequence) A sequence S(cid:48) = (cid:10)t1, t2, . . . , t(cid:96)
(cid:11) is a sub-sequence
of the sequence S = (cid:10)s1, s2, . . . , sk
(cid:11), if there exists an index sequence 1 ≤ r1 <
r2 < · · · < r(cid:96) ≤ k, such that tj = srj for all 1 ≤ j ≤ (cid:96). In such a case, we write
s(cid:48) ≺ s.

Deﬁnition 2 (Head and Tail) The head of a sequence is its ﬁrst element and
the tail is its remaining elements. Given an item a and a sequence T , we write
(cid:104)a | T (cid:105) to mean a sequence starting with the head a followed by the tail T . For
example, (cid:104)a | (cid:104)b, c(cid:105)(cid:105) = (cid:104)a, b, c(cid:105) and (cid:104)a | (cid:104)(cid:105)(cid:105) = (cid:104)a(cid:105).

Deﬁnition 3 (Cover) The cover of a sequence S in a sequential database D is
the set of records of which S is a sub-sequence,

cover (S, D) = {Si : Si ∈ D, S ≺ Si}.

Such a sequence is often called sequential pattern.

Deﬁnition 4 (Count) The count of a sequence S in a sequential dataset D is
number of records of which S is a subsequence,

Deﬁnition 5 (Support) The support of a pattern S is the proportion of the n
records in database D of which S is a subsequence,

#(S, D) = |cover (S, D)|.

sup(S, D) = |cover (S, D)|/n.

For notational convenience, and when it is clear in the context, we will omit D.

Note that this paper focuses on sequential patterns and as such does not aim at
extracting patterns of the type “A, then either B or C, and then D”. As mentioned
in related work, such patterns refer to the more general class of episodes (see for
example [33]). Nor does it seek patterns of the form “sequences that contain A are
more likely to contain B, irrespective of their order.” Such patterns are already
addressed by the substantial literature on itemset discovery.

3.2 When is a sequential pattern interesting?

We now introduce our model for the expected frequency of a given sequential
pattern, as well as the interestingness measures that we derive from it. We start by
providing an intuition for our framework and then introduce its formal deﬁnition.

SkOPUS

7

3.2.1 From interestingness to expected support

“When is a sequential pattern interesting?” is the main question that we tackle in
this paper. We have motivated in the introduction that, for example, it is unlikely
that a pattern (cid:104)and, of (cid:105) would be interesting if the pattern (cid:104)of, and(cid:105) is also. It is
unclear when both these patterns could be interesting except in a context where
the non-sequential pattern {and, of } holds, when the presence of and increases
the frequency of of and vice versa, irrespective of order. Our philosophy is that
such patterns are more succinctly captured as non-sequential patterns.

The starting point of our approach to assessing interestingness is that a pattern
should be interesting if is not possible to explain its frequency by the frequency
of its sub-patterns [40]. (cid:104)a, b(cid:105) should only be interesting if its frequency is greater
than can be explained just by the frequency of (cid:104)a(cid:105) and (cid:104)b(cid:105). (cid:104)a, b, c, d(cid:105) should not
be interesting if its frequency can be explained by the frequency of any of its con-
stituent sequential sub-patterns such as (cid:104)a, c(cid:105), (cid:104)b, d(cid:105), and (cid:104)a, c, d(cid:105). For example,
if buying shoes is often followed by buying socks, and buying jeans is often fol-
lowed by buying a belt, then the pattern (cid:104)shoes, jeans, socks, belt(cid:105) should only be
interesting if it is more frequent than should be expected given the frequency of
(cid:104)shoes, socks(cid:105) and (cid:104)jeans, belt(cid:105) (as well as any other sub-sequences of this 4-element
sequence).

Following the standard approach for deﬁning interestingness measures, we as-
sess interestingness in terms of deviation between observed support and expected
support.

An intuitive ﬁrst approach to deriving the expected support might be to simply
multiply the supports of the two subsequences. If (cid:104)a(cid:105) and (cid:104)b(cid:105) both occur in 50%
of sequences then perhaps (cid:104)a, b(cid:105) should be expected in 25%. However, this ignores
that they may also occur in the order (cid:104)b, a(cid:105), suggesting that there is a need to
adjust for the number of permutations that could occur. But there is a further
complication — (cid:104)a(cid:105) and (cid:104)b(cid:105) can both occur in sequences of length 1 and (cid:104)a, b(cid:105)
cannot. The length of the sequences in which the pattern appears must be taken
into account, but exactly how this will aﬀect the expectation is greatly dependent
upon the type of distribution from which the data are drawn. It is far from clear
what simple models might take account of all these factors without making very
strong assumptions about the form of the distribution. We seek to develop a simple
null hypothesis which can be tested without making any such assumptions.

3.2.2 Our proposed deﬁnition of expected support for sequential patterns

Before developing the general formulas for patterns of any length, it is instructive
to consider patterns of limited length to gain intuition about our general deﬁni-
tions.

Pattern of length 2 We seek sequential patterns, that is, patterns in which the
order of the items is signiﬁcant. That is, we want a pattern (cid:104)a, b(cid:105) to indicate
that the presence of an a in a sequence increases the probability of a b appearing
subsequently. We want to distinguish such a pattern from a co-occurrence pattern,
{a, b} which indicates that a sequence that contains an ‘a’ is more likely to contain
a ‘b’, irrespective of the order of their appearance. One simply hypothesis to test,
which makes no assumptions about the form of the distribution from which the

8

Fran¸cois Petitjean et al.

data are drawn, is that of all sequences in which a occurs, there are more in which
b occurs after a than there are sequences in which b occurs before a. One way of
stating this hypothesis is that sup(cid:104)a, b(cid:105) > sup(cid:104)b, a(cid:105). If a statistical hypothesis test
were developed, the null hypothesis would be that sup(cid:104)a, b(cid:105) ≤ sup(cid:104)b, a(cid:105).

Patterns of length 3 or more The considerations become more complicated when
we consider patterns of length three. One possibility would be that we want
sup(cid:104)a, b, c(cid:105) to be greater than the support of every other permutation of a, b and
c. However, that would disallow the possibility of both (cid:104)a, b, c(cid:105) and (cid:104)a, c, b(cid:105) being
interesting patterns. While we want to disallow all permutations of a set of items
being interesting sequential patterns, there is no reason to mandate that only one
permutation should be interesting.

It is well understood in the context of non-sequential patterns, that unless
speciﬁc steps are taken to prevent patterns from being accepted that are the com-
position of multiple interesting sub-patterns, or the inclusion of frequent singletons
into an interesting pattern, the results of a pattern discovery system will often be
swamped by myriads of spurious byproducts of the core patterns [40]. One eﬀective
approach to prevent this is to test a pattern X against every partition Y, Z such
that Y (cid:40) X and Z = X\Y . The counterpart of such a test for sequential patterns
would test against every pair of subsequences from which it can be composed.
Thus, we want to test (cid:104)a, b, c(cid:105) against (cid:104)a, b(cid:105), (cid:104)c(cid:105); (cid:104)a, c(cid:105), (cid:104)b(cid:105); and (cid:104)b, c(cid:105), (cid:104)a(cid:105). For the
‘partition’ (cid:104)a, b(cid:105), (cid:104)c(cid:105) we can compose three sequential patterns by interweaving the
two patterns, (cid:104)c, a, b(cid:105); (cid:104)a, c, b(cid:105); and (cid:104)a, b, c(cid:105). There are two obvious criteria that
might be imposed to determine whether sup(cid:104)a, b, c(cid:105) can be explained by sup(cid:104)a, b(cid:105)
and sup(cid:104)c(cid:105). One is that sup(cid:104)a, b, c(cid:105) must be greater than at least one of the sup-
ports of the other compositions of the generator. The other is that sup(cid:104)a, b, c(cid:105)
must be greater than the mean of sup(cid:104)c, a, b(cid:105); sup(cid:104)a, c, b(cid:105); and sup(cid:104)a, b, c(cid:105). Both
approaches are credible. The latter is a stronger constraint than the former, but
still allows two of the three patterns that can be composed from a two-element
and one-element sequential pattern to be found to be ‘interesting’. In the current
work we choose this stronger approach, but our software supports both. We thus
assess (cid:104)a, b, c(cid:105) to be ‘interesting’ if its support is greater than mean of the supports
of the recompositions of every one of its sequential partitions.

We now turn to the general formula for the expected support of a sequential
pattern, for which we ﬁrst introduce the notions of binary sequential partition and
of composition of two sequences.

Deﬁnition 6 (Sequential compositions) The sequential compositions of se-
quences S and T , denoted comp(S, T ), is the set of all sequences that contain all
and only the elements of S and T , respecting their order. More formally, we deﬁne
the composition recursively

comp(S, (cid:104)(cid:105)) = {S}

comp((cid:104)(cid:105), T ) = {T }

comp((cid:104)s1 | S∗(cid:105), (cid:104)t1 | T ∗(cid:105)) = {(cid:104)s1 | U (cid:105) | U ∈ comp(S∗, (cid:104)t1 | T ∗(cid:105))}

∪ {(cid:104)t1 | V (cid:105) | V ∈ comp((cid:104)s1 | S∗(cid:105), T ∗)}.

Deﬁnition 7 (Binary sequential partition) Let S, S1 and S2 be three se-
quences. We call {S1, S2} a binary sequential partition of S, if |S1| > 0, |S2| > 0

SkOPUS

9

and S ∈ comp(S1, S2). We denote as BSP (S) the set of all binary sequential
partitions of S,

BSP (S) = {{S1, S2} | S ∈ comp(S1, S2)} .

For example, the binary sequential partitions of

BSP ((cid:104)a, b, c, d(cid:105)) = {{(cid:104)a(cid:105), (cid:104)b, c, d(cid:105)}, {(cid:104)a, b(cid:105), (cid:104)c, d(cid:105)}, {(cid:104)a, b, c(cid:105), (cid:104)d(cid:105)},

{(cid:104)a, b, d(cid:105), (cid:104)c(cid:105)}, {(cid:104)a, c(cid:105), (cid:104)b, d(cid:105)}, {(cid:104)a, c, d(cid:105), (cid:104)b(cid:105)}, {(cid:104)a, d(cid:105), (cid:104)b, c(cid:105)}} .

We can now introduce our general deﬁnition for the expected support of a

sequential pattern.

Deﬁnition 8 (Expected support for a sequential pattern) Let S be a se-
quential pattern, we deﬁne the expected support of S as

ExpSupport(S) =

max
(S1,S2)∈BSP(S)

{

mean
S(cid:48)∈comp(S1,S2)

{sup(S(cid:48))}}.

Let us give an intuition about this deﬁnition. We consider all the possible pairs
of subsequences that respect the order in the target sequence (all the binary se-
quential partitions). Each of these partitions corresponds to a potential generator
of the targeted sequence. For example, to determine the interestingness of the
sequence S = (cid:104)a, b, c, d(cid:105), we look at how much more frequent it is than the max-
imum likelihood expectation if the data was generated from, say, U = (cid:104)a, d(cid:105) and
V = (cid:104)b, c(cid:105) assuming that all possible ways to interweave U and V are equiprobable.
We then take the maximum expected support over all potential binary sequential
partitions (i.e. over all possible generators). This is because ﬁnding one pair (U, V )
that ‘explains’ the support S should be enough to establish that pattern S is not
interesting. It is important to note that we do not need to consider the expected
support of compositions of more than two subsequences, as the relevant pairwise
partitions will subsume compositions of more than two subsequences. The aim of
this paper is to extract the top-k sequential patterns under leverage, i.e. that will
have maximum diﬀerence between their observed and expected support:

leverage = sup(S) − ExpSupport(S).

A pattern will be considered interesting if its support cannot be explained by any
of its sub-patterns.

It is ﬁnally important to note that if a pattern is considered interesting under
our framework, then some of its sub-patterns might also be considered interesting.
This will for example be the case with singular patterns that have low expected
support. An example is the pattern (cid:104)if, you, can(cid:105) present in the poem “If—” by R.
Kipling: it is present in more than 1/3 of the verses while none of its reorderings
are, and also none of the re-orderings of its sub-patterns. For example the pattern
(cid:104)can, you(cid:105), which is a reordering of (cid:104)you, can(cid:105) ≺ (cid:104)if, you, can(cid:105), never occurs either,
thus making the pattern (cid:104)you, can(cid:105) interesting as well. It is however here interesting
to note that our approach will rank the full pattern (cid:104)if, you, can(cid:105) higher than
(cid:104)you, can(cid:105), because there are more compositions for patterns of length 3 than
there are for patterns of length 2, and thus the mean over all those compositions
is lower.

10

Fran¸cois Petitjean et al.

3.2.3 Our framework’s null hypothesis

Having completely deﬁned our proposed deﬁnition of expected support, it is im-
portant to take a step back and look at what it is achieving. Its null hypothesis
is that the frequency of a pattern can be explained by the frequency of
its sub-patterns (or generators). We have imbricated two elements:

1. We deﬁne the expected frequency for each pair of sub-patterns (what we call
a binary sequential partition). Our null hypothesis there (and null model from
which we get the expectancy) is that all possible ways to “interweave” the
patterns – such that their order is respected – are equally likely. The expected
frequency of a pair of generators is then the average frequency observed over
all possible ways to interweave these patterns.

2. Our null hypothesis being that the frequency of a pattern can be explained by a
pair of generators, the expected frequency for our framework simply considers
the partition that produces the greatest expectation.

Note on the diﬃculty of assessing our framework’s statistical signiﬁcance We will
demonstrate in Section 4 that our framework is very eﬀective at extracting interest-
ing (with all its subjectivity) patterns. It is important here to note that extracting
a measure of the statistical signiﬁcance of patterns under our model is a diﬃcult
task, because it requires to be able to assess the probability of a pattern given
our null model. Although we can easily obtain its expected frequency, computing
its probability requires more assumptions about the form of the distribution from
which the data are drawn. Each sequence indeed only has a certain length, and
it is unclear what model would best match our framework. We however feel that
this is a consequence of the strength of our framework; we do not assume a sim-
plistic model of the underlying distribution for all patterns. In some sense, our
partitioning approach allows us to ﬁnd and ﬁt a dedicated model independently
for each sequence which makes the extraction powerful, but at the cost of failing
to support a simple test for assessing statistical signiﬁcance.

3.2.4 Algorithmic notes

We detail here the algorithmic considerations regarding the generation of Binary
Sequential Partitions (BSPs) and of Compositions.

Templates. It is ﬁrst interesting to note that in either case, the results only de-
pend on the length of the considered pattern and are independent of the actual
letters themselves. All partitions and compositions are thus generated as templates
depending as a function of the length.

Indexing templates. Our ﬁrst optimization directly follows from this observation;
every time we create a template for a BSP of a particular length, we index it
with its length so that it is only computed once for each length. We employ a
similar indexing for Compositions, with the diﬀerence that they have two associ-
ated lengths; we thus index them using a matrix. Note that the template for a
Composition of length l1 and l2 is identical, we only use the upper triangle in the
matrix.

SkOPUS

11

Generating BSPs. Generating all Binary Sequential Partitions for a template of
length l is an enumeration exercise. The ﬁrst element to note is that knowing
the pattern and its left partition set (S1 in Deﬁnition 7) completely determines
its right partition set (S2 in Deﬁnition 7), and conversely. This means that, to
construct the template for all BSPs of a particular length, we only need to ﬁnd
what is the left partition set, i.e., the sequence of positions of elements in the
original pattern. Furthermore, for a pattern of length l, we only need to consider
S1 of lengths l1 from 1 to (cid:98) l
2 (cid:99), because of symmetry.

To generate all possible templates for S1, we enumerate all the l1-combinations
(cid:1) such combinations. To this end, we

of the set of l positions in S; there are (cid:0) l
l1
use the standard enumeration algorithm for combinations as described in [37].

Generating Combinations. Generating combinations is simpler than BSPs. The
same symmetry observations hold here, i.e., the template for all combinations of
lengths (l1, l2) is the same as the one for all combinations of lengths (l2, l1). We
then generate all the possible l1-combinations of the set of l1 + l2 positions; here
again we use the standard algorithm described in [37].

3.3 Exploring the space of all sequential patterns

Our algorithm to explore the space of sequential patterns and exactly extract
the top-k sequential patterns with highest leverage2 is based on the OPUS Miner
algorithm [41, 47], ﬁrst introduced for mining non-sequential databases.

3.3.1 Sequential top-k OPUS Miner (SkOPUS)

OPUS Miner ﬁnds the top-k itemsets under given interestingness measures from a
non-sequential database, with regard to a given measure of interest. OPUS Miner
is a depth-ﬁrst branch-and-bound algorithm, that exploits the monotonicity of
itemsets (sup(I ∪ i) (cid:54) sup(I)). It performs an exhaustive depth-ﬁrst search of the
space of sequential patterns except in so far as it can prune parts of the search
space that cannot contain itemsets in the top-k with respect to the measure of
interest. For example, if the minimum (i.e. worse) value in the top-k is α, and we
know that any specialization of an itemset I will have an score that is lower than
α, then the sub-tree for which the root is I does not need to be explored.

OPUS Miner’s depth-ﬁrst search has the advantage that the number of open
nodes at any one time is minimized. This allows extensive data to be maintained
for every open node and ensures that only one such record will be stored at any
given time of each level of depth that is explored. It also promotes locality of
memory access, as most new nodes that are opened are minor variants of the most
recently explored node.

Algorithms 1 and 2 describe SkOPUS.The main elements that diﬀer from the

original OPUS Miner algorithm are as follows:

2 In the remainder of this paper, we will use the term ’leverage’ as a proxy for ’leverage

under our deﬁnition of expected support’.

12

Fran¸cois Petitjean et al.

1. When a sequential pattern is specialized, the specializing item is always placed
at the end of the sequential pattern (as a suﬃx – appending item i to sequence
S is noted S.i hereafter). This ensures that sub-spaces that are pruned will not
be reconsidered later in the search.

2. When retrieving an item from the queue of available items to specialize a
pattern with, the item is left in the queue (as opposed to OPUS Miner where
the item is removed from the queue). This is necessary to handle repetitions of
items in the patterns such as (cid:104)a, a, b(cid:105). Thus, we use the variant of OPUS [41]
that allows multiple applications of a single operator (the addition of a given
item).

3. Upper bounds for any sequential extension of a pattern S have to be adapted
for our measure of interest (leverage) and our deﬁnition of expected support;
we detail this element in Section 3.3.2.

Algorithm 1: SkOPUS Miner

Input: Sequential database D, a measure of interest M , integer k
Output: The top-k sequential patterns with regard to M
q ← a queue of all items i ∈ D in descending order on sup((cid:104)i(cid:105));
topK ← an empty queue to contain the top-k sequential patterns;
return ExpandSequence((cid:104)(cid:105), q, topK, M );

Algorithm 2: ExpandSequence

Input: A sequence S, a queue q, the current topK and a measure of interest M
Output: The top-k sequential patterns with regard to M in the search space explored

to date

initialize q(cid:48) to be an empty queue of items;
for all i ∈ q do
T ← (cid:104)S | i(cid:105);
score ← M (T );
if score > topK.min then
topK.add(T, score);

end
if U pper bound(M, T ) > topK.min then

q(cid:48) ← append(q(cid:48), i);

end

end
for all i ∈ q(cid:48) in descending order w.r.t. M do
topK ←ExpandSequence(T , q(cid:48), topK, M );

end
return topK;

Algorithm 1 simply initializes the necessary data structures to then call Algo-
rithm 2 that systematically explores the entire space of sequential patterns other
than those branches of the search space of which it can be determined no top-k
can be contained therein. The correctness of SkOPUS follows from the correctness
of OPUS. As SkOPUS investigates each pattern it maintains a list topK of the
top-k patterns found so far. Thus, on termination, as the entire search space has

SkOPUS

13

been explored other than those branches that cannot contain patterns in the top-k,
topK contains the top-k patterns.

3.3.2 Upper bounding

We now detail how to upper bound the score of any extension of a sequential
pattern S with regard to leverage (i.e., how to compute U pper bound(M, S(cid:63)) in
Algorithm 2). As presented earlier, we measure the interestingness of a pattern
with a leverage,

leverage = sup(S) − ExpSupport(S).

In order to prune the search space we need an upper-bound on leverage for
any extension pattern, say T , of a pattern S. Since ExpSupport(S) ≥ 0 and
sup(T ) ≤ sup(S), we immediately obtain leverage(T ) ≤ sup(S), allowing us to
use sup(S) as an upper bound. Note that, possibly surprisingly, the lower bound
on ExpSupport(S) is tight. Consider the example of a database where all sequences
start with the same pattern P of length (cid:96), and then do not use any of the elements
composing the pattern in the rest of the sequence. We will then have all sequential
compositions of all possible binary sequential partitions with 0 frequency, except
for the one with the actual pattern. The highest frequency will then be for the one
with the lowest number of compositions, which is (cid:96).

ExpSupport(P ) (cid:62) sup(P )

(cid:96)

Given that we want to bound all possible extensions of P , (cid:96) is unbounded, which
makes ExpSupport(P (cid:63)) > 0.

In the deﬁnitions above, we exploit the fact that support and expected support
are positive. It is interesting to note that, so long as the deﬁnition of expected
support does not allow negative values, these elements will remain valid. It follows
that our SkOPUS algorithm can be directly used with any sensible deﬁnition of
expected support.

3.3.3 Bootstrap — How to quickly get a good top-k?

In practice, the time requirements of SkOPUS depend on the eﬃciency of the prun-
ing mechanisms, and can vary greatly from dataset to dataset3. SkOPUS traverses
the search space maintaining the set of top-k sequential patterns discovered in the
search space explored so far. The search space can only be pruned of branches that
cannot contain a pattern with higher leverage than the kth best leverage found
so far. Thus, eﬃcient pruning relies critically on our ability to quickly ﬁll topK
with high-scoring patterns so that as much of the search space can be pruned as
possible.

We use two main strategies. First, we consider the addition (as suﬃx) of items
with highest support ﬁrst; this is apparent in the order in which we go through
the diﬀerent queues in Algorithms 1 and 2.

Algorithm 2 performs a depth-ﬁrst search. The primary reason for doing so is
that it limits the number of nodes in the search space that must be simultaneously

3 Note that if, for instance, most of the sequential patterns in a dataset are non-interesting,

then the algorithm will have to explore a large part of the space.

14

Fran¸cois Petitjean et al.

open. To eﬃciently compute the cover of a new node it is important to store the
cover of the parent and only update it with respect to the item appended to the
sequence. Minimizing the number of open nodes minimizes the number of covers
that must be stored. However, we store the cover of every item, and so this issue
does not aﬀect two-item sequences. Our second strategy uses a hybrid search.
First a breadth-ﬁrst search is performed over all two-item sequences. Then the
regular depth-ﬁrst search is employed for sequences of length three and greater.
We present the two-item breadth-ﬁrst bootstrap in Algorithm 3. The algorithm
remains correct as it still systematically searches all of the search space that might
contain top-k patterns, all that changes is the order in which they are explored.

Algorithm 3: SkOPUS Bootstrapping

Input: Sequential database D and a measure of interest M
Output: A preﬁlled topK
Let topK be an empty ordered queue
foreach i1, i2 ∈ I do
S ← (cid:104)i1, i2(cid:105)
Es ← ExpSupport(S)
score ← M (S, Es)
if score > topK.min then

topK ← topK.add(S, score)

end

end
return topK

4 Experiments

In this section, we present the results of SkOPUS for the exact extraction of
the k sequential patterns with highest leverage. In the synthetic experiments we
compare the performance of SkOPUS using leverage as the measure of interest
against SkOPUS using support as the measure of interest (included to provide
a contrast against frequent pattern discovery), state-of-the-art sequential pattern
discovery algorithm rprt [33] and baseline rind [13]. As the synthetic experiments
clearly show the baseline is less eﬀective than either SkOPUS using leverage or
rprt , we do not include it in the real-world experiments.

4.1 Datasets

We shall start this section by making a general observation about the availability
of interpretable sequential databases. Applications of sequential pattern mining
methods are numerous and include:

1. sequences of the diﬀerent webpages browsed by users on a website, where each

visit represent a new transaction in the database;

2. locations of series of events, such as the neighborhoods that a taxi drives
through for each client, or the restaurants visited by registered customers of
say Yelp R(cid:13) or Urbanspoon R(cid:13);

SkOPUS

15

3. the performance of diﬀerent industries on the share market over diﬀerent days

4. the sequence of actions performed by a surgeon over the course of diﬀerent

5. the tests performed on (and action taken about) patients from admission to

of trading;

surgeries;

discharge.

The datasets associated to all these applications are however extremely valuable
and hence only rarely made available freely to the scientiﬁc community. In ad-
dition, assessing the quality of technologies for pattern discovery requires having
knowledge about the patterns that are actually present in data. Therefore we start
by evaluating the discovery with sequences that are sampled from known distri-
butions with speciﬁc patterns. We can then compare the discovered interactions
to the true structure from which the data was sampled. This is the ﬁrst set of ex-
periments we present in Section 4.2. Then, in Section 4.3, we assess the relevance
and scalability of our approach on public domain literary works. We have selected
several books which we believe have characteristics that are quite representative
of many other types datasets.

It is important to note that the main objective of this paper is not to prove
that extracting patterns from sequential databases is an important topic; this has
in fact been largely motivated by the data mining community. Rather, we aim at
demonstrating 1. that interestingness-based sequential pattern extraction is critical
and 2. the relevance of our approach.

All the datasets used in this paper (as well as their “ground truths”, when avail-
able) are provided for reviewing purposes at https://github.com/fpetitjean/
Skopus.

4.2 Experiments on data with known patterns

We ﬁrst assess our method on datasets that embed a known set of sequential
patterns.

Details of the data generation process are as follows: we start by generating
a dataset with 10,000 random sequences over a vocabulary I with 10 tokens;
the probability of the tokens follows a ﬂat Dirichlet distribution: I ∼ Dir(1). We
generate each random sequence in the dataset by choosing its length ls distributed
accordingly to a shifted4 Poisson distribution: ls ∼ Pois(9.0) + 1. This makes the
sequences to have an average length of 10 with standard deviation 3 (E(lp) = 10,
Var(lp) = 9).

We then generate a set of k sequential patterns, each with an associated prob-
ability of occurrence in a sequence drawn uniformly in [0.05, 0.2] and length lp
following lp ∼ Pois(1.0) + 2, i.e. with average length 3 and standard deviation 1
(E(lp) = 10, Var(lp) = 1). Then, each pattern is embedded sequentially accord-
ing to its associated probability. Embedding is performed by uniformly at random
selecting insertion points in the sequence.

We then extracted the top-20 sequential patterns according to leverage and
compare the results with a support-based extraction5, rprt [33] and rind , a baseline

4 Shifting ensures that the sequences have at least 2 elements.
5 Note that we “help” the support-based extraction by not allowing it to test single items,

otherwise it would only return those.

16

Fran¸cois Petitjean et al.

Table 2 Results on data with known embedded patterns

#patterns Recall

1
2
3
4
5
6
7
8
9
10

Support

Leverage

rprt [33]

rind [13]

0%
0%
0%
25%
0%
33%
14%
0%
0%
0%

100%
100%
100%
100%
100%
67%
86%
25%
78%
40%

100%
100%
66%
100%
40%
67%
43%
12%
44%
20%

100%
50%
66%
75%
40%
33%
43%
12%
22%
10%

used by [33] using the indepdence model proposed by [13]. Both baselines require
a set of candidate patterns: here we used frequent patterns with a threshold of
500. Note that this threshold potentially “helps” these two methods, because none
of the injected patterns have a support lower than 500; such a threshold has thus
potential to prune a signiﬁcant number of patterns that could have been ranked
in the top-10. We report recall rates in Table 2, that is, proportions of all patterns
that were embedded that are included in the top-20 patterns returned by each
approach. As expected, the support-based method does not perform well. In fact,
it only ever extracted patterns of length 2; which mainly correspond to sequential
patterns that are frequent because their composing items are as well. This is
a similar behaviour to the one that we have explained with and, to and of
in
the introduction: patterns composed with frequent items will appear frequently
by chance, without being interesting. This means that top-k extraction based on
support has diﬃculty extracting patterns of length (cid:62) 3.

To further highlight this point, we shall mention that for the experiment with
a single embedded pattern (#patterns = 1 in Table 2), (cid:104)c, b, c, a(cid:105) embedded in
19% of the sequences, this pattern is ranked 92nd under support while it is the
top pattern under our leverage, rprt and rind . More generally, Table 2 shows that
all approaches outperform support-based approaches, by recovering a much larger
number of embedded patterns.

We can also observe that our method — SkOPUS with leverage — outperforms
rprt and rind by obtaining a signiﬁcantly higher recall of the embedded patterns.
It is also interesting to note that these experiments conﬁrm the ones in [33] by
showing the poorer performance of rind compared to rprt .

It can be observed that, as the number of patterns embedded in the data

increases, the recall decreases. This is due to three main factors:

1. because we keep the number of patterns extracted constant (k = 20), it is
normal that recall diminishes as the number of actual patterns increases;
2. our approach considers the subsequences of an embedded pattern to also be
interesting patterns: this means even if we were to extract an actual pattern
(cid:104)d, m, k, d(cid:105), we are also prone to extract subsequences of this patterns, such as

SkOPUS

17

Table 3 Details on the datasets embedding 7 patterns.

Injected patterns

Support

Leverage

(cid:104)i, g, d(cid:105)
(cid:104)d, e(cid:105)
(cid:104)a, e, j(cid:105)
(cid:104)i, b(cid:105)
(cid:104)j, e, j, g(cid:105)
(cid:104)j, d, c, i, a(cid:105)
(cid:104)j, h, i(cid:105)

17.1%
15.9%
14.6%
14.2%
10.5%
7.8%
7.1%

3.36%
0.85%
2.95%
4.26%
2.11%
3.58%
2.34%

(cid:104)d, m, k(cid:105) or (cid:104)d, m, d(cid:105). There is no counterpart of independent productivity from
self suﬃcient itemset mining whereby subpatterns can be discarded [40].

3. some patterns can overlap; for example if we have two (actual) patterns
(cid:104)a, e, e, i(cid:105) and (cid:104)e, f, i, d(cid:105) that are independently embedded in the data, then,
the pattern (cid:104)e, i(cid:105) is going to be even more frequent and can be extracted; even
though it is not one of the directly embedded patterns.

Note that the two last points actually complicate the extraction of patterns
within the top-20 as well. These three elements are best exempliﬁed with the top-
20 sequential patterns corresponding to the dataset containing 7 patterns, which
we illustrate in Table 3 and Table 4, where we report the exact matches and
subpatterns that are discovered, adopting the elements appropriate to sequential
patterns from the approach pioneered by Zimmermann [49] for non-sequential
patterns.

First, this table further illustrates the consistency of our method. We have
depicted the extracted patterns that are actual injected patterns in with a • next
to them. While our method can extract 6 out the 7 actual patterns within the
top-20, we can see that the support-based method only extracts one pattern (cor-
responding to a pair), and the two other methods only 3 actual patterns. It is also
interesting to note that the similarity between the 4 ﬁrst patterns extracted by
our method and by rprt .

It is informative to explain why the pattern (cid:104)d, e(cid:105) is not part of our top-20 while
it is part of the top-20 on the support: it it is quite a frequent pattern (support
of 63%), but this is actually mostly due to the support of d and e. This pattern
was introduced with a 16% probability; the diﬀerence is explained by the fact that
both tokens d and e were sampled with a relatively high probability (cid:62) 15%. This
means that the support method only extracts it in the top-20 because d and e
are frequent to start with; had they been infrequent tokens, the support wouldn’t
have ranked the pattern that high. This is exactly what happens with pattern
(cid:104)i, b(cid:105), for which token b has much lower probability (p((cid:104)b(cid:105)) = 0.02). Because it
does not appear by chance frequently, the support-based approach ranks the
pattern very low. It is also interesting to see that under support, pattern (cid:104)e, d(cid:105)
is only slightly less interesting, which clearly shows its inconsistency. Conversely,
we can see the consistent behaviour of the leverage-based approach, which ranks
(cid:104)i, b(cid:105) with much higher value than (cid:104)d, e(cid:105). Before this pattern was embedded into
the data it already appeared in about 61% of the sequences. Observing it in 63%
is thus not a very strong eﬀect and our method ranks it accordingly. On the other
hand, before introduction of (cid:104)i, b(cid:105), the pattern only appeared in about 13% of

18

Fran¸cois Petitjean et al.

Table 4 Top 20 results on the dataset embedding 7 patterns. Exact matches are represented
in blue with a • next to it; subsequences of such exact matches are depicted in boldface.

Leverage (value)

Support

rprt [33]

rind [13]

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Top-k

(cid:104)i, g(cid:105) (4.3%)
•(cid:104)i, b(cid:105) (4.3%)
•(cid:104)j, d, c, i, a(cid:105) (3.6%)
•(cid:104)i, g, d(cid:105) (3.4%)
(cid:104)d, c, i, a(cid:105) (3.2%)
(cid:104)j, c, i, a(cid:105) (3.2%)
•(cid:104)a, e, j(cid:105) (2.9%)
(cid:104)i, g, g(cid:105) (2.8%)
(cid:104)i, e(cid:105) (2.7%)
(cid:104)j, d, c, a(cid:105) (2.6%)
(cid:104)j, d, c, i(cid:105) (2.6%)
•(cid:104)j, h, i(cid:105) (2.3%)
(cid:104)a, e, e, j(cid:105) (2.3%)
(cid:104)a, e, e(cid:105) (2.3%)
(cid:104)j, d, i, a(cid:105) (2.2%)
•(cid:104)j, e, j, g(cid:105) (2.1%)
(cid:104)j, e, g(cid:105) (2.1%)
(cid:104)a, e, g(cid:105) (2.1%)
(cid:104)i, g, g, d(cid:105) (2.1%)
(cid:104)i, e, e(cid:105) (2.0%)

(cid:104)j, d(cid:105)
(cid:104)j, e(cid:105)
(cid:104)d, d(cid:105)
(cid:104)j, j(cid:105)
•(cid:104)d, e(cid:105)
(cid:104)d, j(cid:105)
(cid:104)e, j(cid:105)
(cid:104)i, d(cid:105)
(cid:104)e, e(cid:105)
(cid:104)j, g(cid:105)
(cid:104)i, d(cid:105)
(cid:104)g, d(cid:105)
(cid:104)d, g(cid:105)
(cid:104)i, j(cid:105)
(cid:104)i, e(cid:105)
(cid:104)e, g(cid:105)
(cid:104)g, j(cid:105)
(cid:104)j, i(cid:105)
(cid:104)d, i(cid:105)
(cid:104)g, e(cid:105)

•(cid:104)j, d, c, i, a(cid:105)
•(cid:104)i, b(cid:105)
•(cid:104)i, g, d(cid:105)
(cid:104)i, g(cid:105)
(cid:104)j, c, i, a(cid:105)
(cid:104)j, j, c, i, a(cid:105)
(cid:104)d, d, c, i, a(cid:105)
(cid:104)i, g, g(cid:105)
(cid:104)d, c, i, a(cid:105)
(cid:104)i, g, g, d(cid:105)
(cid:104)j, e, c, i, a(cid:105)
(cid:104)j, h, i(cid:105)
(cid:104)j, d, c, a(cid:105)
(cid:104)d, j, c, i, a(cid:105)
(cid:104)j, j, e, j, g(cid:105)
(cid:104)j, e, e, j, g(cid:105)
(cid:104)i, e, d(cid:105)
(cid:104)i, g, d, d(cid:105)
(cid:104)i, c, i, a(cid:105)
(cid:104)j, d, c, e, a(cid:105)

•(cid:104)j, d, c, i, a(cid:105)
(cid:104)d, d, c, i, a(cid:105)
(cid:104)j, j, c, i, a(cid:105)
(cid:104)j, c, i, a(cid:105)
(cid:104)d, c, i, a(cid:105)
(cid:104)j, e, c, i, a(cid:105)
(cid:104)j, e, e, j, g(cid:105)
(cid:104)d, j, c, i, a(cid:105)
(cid:104)j, d, c, e, a(cid:105)
(cid:104)i, i, b(cid:105)
(cid:104)j, d, c, a(cid:105)
•(cid:104)i, g, d(cid:105)
(cid:104)j, d, c, i(cid:105)
•(cid:104)j, e, j, g(cid:105)
(cid:104)i, g, g, d(cid:105)
(cid:104)j, d, e, j, g(cid:105)
(cid:104)j, e, j, j, g(cid:105)
(cid:104)j, d, c, i, g(cid:105)
(cid:104)j, d, c, j, a(cid:105)
(cid:104)j, d, d, i, a(cid:105)

the sequences, observing it 22% is thus more signiﬁcant and our method correctly
ranks it accordingly.

Moreover, we have also depicted in boldface (with no •) the patterns extracted
by our approach that correspond to subsequences of actual patterns that we have
also extracted. This mainly illustrates the two ﬁrst elements that we have noted, by
having 8 slots in our top-20 “consumed” by sub-patterns of high leverage. Although
this falls out of the scope of this ﬁrst attempt at extracting the top-k sequential
patterns with leverage, this is naturally echoing the work that has been done on
ﬁltered-top-k association discovery [40, 46]. More generally, these results call for a
reﬂection on the evaluation of sequential pattern mining procedures, similarly to
the work that has been performed in this area for non-sequential pattern mining
[49].

4.3 Experiments on literary works

We compared the top patterns from several works of literature and the JMLR
abstract dataset. For brevity, we only discuss the patterns obtained from JMLR.
For more information about other results, see our section Supplementary material
at the end of the manuscript.

We study in detail the results of the diﬀerent methods on the JMLR dataset
[35], which represents the abstracts of the papers published in the Journal of
Machine Learning Research. This dataset holds 788 abstracts (hence sequences),
which use 3, 844 words (items). The average length of abstracts is 96 words with

SkOPUS

19

Table 5 Top 10 results on abstracts of JMLR papers

Top-k patterns

# Support

Leverage

rprt [33]

1
2
3
4
5
6
7
8
9
10

(cid:104)algorithm, algorithm(cid:105)
(cid:104)learn, learn(cid:105)
(cid:104)learn, algorithm(cid:105)
(cid:104)algorithm, learn(cid:105)
(cid:104)data, data(cid:105)
(cid:104)learn,data(cid:105)
(cid:104)model, model(cid:105)
(cid:104)problem, problem(cid:105)
(cid:104)learn, result(cid:105)
(cid:104)problem, algorithm(cid:105)

(cid:104)paper, show (cid:105)
(cid:104)paper, result(cid:105)
(cid:104)support, vector, machin(cid:105)
(cid:104)paper, algorithm(cid:105)
(cid:104)support, vector (cid:105)
(cid:104)base, result(cid:105)
(cid:104)paper, method(cid:105)
(cid:104)learn,result(cid:105)
(cid:104)paper, propos(cid:105)
(cid:104)vector,machin(cid:105)

(cid:104)support, vector (cid:105)
(cid:104)support, vector, machin(cid:105)
(cid:104)support, machin(cid:105)
(cid:104)real, world(cid:105)
(cid:104)vector, machin(cid:105)
(cid:104)state, art(cid:105)
(cid:104)reproduc, hilbert(cid:105)
(cid:104)high, dimension(cid:105)
(cid:104)ﬁrst, second(cid:105)
(cid:104)larg, scale(cid:105)

a maximum length of 231. Since rprt requires a candidate set, we use frequent
patterns with a threshold of 10. For the other literature works we used a threshold
of 5.

We ﬁrst present the results and detail the computation times in the next sub-
section. The top-10 patterns are presented in Table 5 for our method (leverage),
support for reference, and the results from rprt ; having shown in the previous sec-
tion that our method and rprt outperform rind , we focus on other methods and
keep top-support patterns for reference.

The ﬁrst critical observation echoes the ones we made in the introduction about
the inconsistency of support-based extraction: when using the support, most of
the patterns correspond in repetitions of very frequent words, such as algorithm,
learn, data, model and problem. Moreover we can see that when using the support
as the measure of interest, the pattern (cid:104)learn, algorithm(cid:105) and its reversed version
(cid:104)algorithm, learn(cid:105) appear with very similar scores (resp. supports 36% and 29%).
In contrast, we can observe that our method and rprt present patterns that seem
of higher general interest than support. Interestingly, the patterns extracted seem
to diﬀer signiﬁcantly between the methods. Beyond highlighting the importance of
synthetic data experiments, this also re-conﬁrms the subjectivity of interestingness.
We ﬁnd then interesting to examine two elements:

1. The overlap of the diﬀerent methods.
2. The patterns that crystallize the diﬀerences between the methods.

Overlap between SkOPUS and rprt For each method, we examine what percentage
of its top-k is found in the top-k of the other. Figure 1 presents this overlap analysis
for SkOPUS and rprt : in (a) their top-100 and (b) their top-100. For example, a
point at coordinates (x, y) for the top-100 tells us that y percent of the top-x of
the ﬁrst method was found in the top-100 of the other. Two similar results would
then have a slowly decreasing rate. In this case we observe quite the opposite:
there is only little overlap between the two methods with less than 20% overlap
within the top-100 and top-1000. Interestingly, SkOPUS seem to ﬁnd more of the
top patterns ranked by rprt than the opposite, which supports the relevance of our
method.

20

Fran¸cois Petitjean et al.

(a) Top-100

(b) Top-1000

Fig. 1 Overlap between the top-k given by SkOPUS and rprt

Analysis of representative diﬀerences Signiﬁcant diﬀerences exist even in the very
top patterns, which clearly appears in Figure 1(a), which shows that patterns as
early as in SkOPUS’ third position are not part of the top-100 of the patterns
ranked by rprt . We detail below a few such example patterns that are highly
contrasting the diﬀerences between SkOPUS and rprt :

– Patterns SkOPUS ranks high but not rprt : (cid:104)paper, algorithm(cid:105) is extracted
as the 4th pattern with highest leverage while it is ranked 7946 by rprt . This
pattern is extracted by SkOPUS for its leverage because this succession appears
174 times in the dataset, while its reversed pattern (cid:104)algorithm, paper(cid:105) appears
only 80 times. It seems reasonable to be highly rank a pattern that occurs with
more than twice the frequency of its reverse. rprt ranks this pattern much lower
because paper and algorithm are both probable individually (resp. support
of 43% and 58%). A similar phenomenon is seen for (cid:104)base, result(cid:105) — which
SkOPUS ranks 6 vs. 650 for rprt — and for (cid:104)learn, result(cid:105)— which SkOPUS
ranks 8 vs. 25, 810 for rprt .

– Patterns rprt ranks high but not SkOPUS: (cid:104)reproduc, hilbert(cid:105) is extracted as
the 7th pattern by rprt while it is ranked 696 by SkOPUS. This pattern is ex-
tracted by rprt because it consists of two relatively rare items, reproduc occurs
32 times and hilbert occurs 36 times. On the other hand, SkOPUS ranks this
pattern relatively low because it appears in only 28 abstracts. While under our
measure of expected support this pattern has high lift, it has low leverage, the
latter being a measure that favors patterns that appear the greatest number of
times in excess of the expected. It is also interesting to note that SkOPUS ranks
the pattern (cid:104)reproduc, hilbert, spac(cid:105) much higher than (cid:104)reproduc, hilbert(cid:105) (390
vs. 696). It seems natural that if all abstracts that include (cid:104)reproduc, hilbert(cid:105)
are followed by space, then the pattern of length 3 should be more interesting.
SkOPUS’ rank respects this ordering while rprt does not in this case, having
the pattern (cid:104)reproduc, hilbert, spac(cid:105) ranked 42nd. Note also that the pattern
(cid:104)hilbert, spac(cid:105) occurs less frequently than (cid:104)reproduc, hilbert(cid:105), as some papers
mention “reproduc[ing] the Hilbert norm”, but not space (see e.g. the paper
by F. Bach at http://www.jmlr.org/papers/v9/bach08b.html).

SkOPUS

4.4 Execution time

21

We ﬁnish the experiments by showing the running time of our approach; all ex-
periments are performed using a standard desktop computer.

Results are reported in Table 6. Not surprisingly, extracting the top-20 under
support with SkOPUS is extremely fast, because the most frequent elements are
encountered very early in the exploration of the search space;6 no top-20 were
actually presenting patterns of more than length 2 regardless of the experiments.
For other methods, it can be observed than synthetic data are extremely chal-
lenging, mostly because we included only a few patterns with reasonable proba-
bility. Results are obtained in a few hours for SkOPUS under leverage and less
than a minute for rprt . The main reason why rprt is faster is due to a high min-
ing threshold of 500 without which extraction cannot be performed, because if
we lower this threshold then the mining step will take a considerate time due to
a frequent pattern explosion. On the other hand, SkOPUS does not require any
threshold.

Finally, our approach exhibits extremely competitive running time compared
to support on real-world datasets where interesting patterns are more present. It
only takes SkOPUS 37s to extract the top-20 patterns in the JMLR dataset; only
slightly more than twice the time taken for support-extraction, and signiﬁcantly
faster than rprt with minimum support set to 10. On other literary works, SkOPUS
ﬁnishes in less than a few minutes, while rprt is generally ﬁnishes in seconds but
it required to set a minimum support threshold of 5.

It is ﬁnally interesting to note that the algorithmic complexity of SkOPUS and
rprt varies with diﬀerent elements. For SkOPUS, it is a function of how interesting
the patterns in the data are: data that holds patterns of high interest will prune
signiﬁcantly large parts of the search space (and the earlier they are found, the
quicker the process). For rprt , the interests of the patterns that the data holds
is almost neutral to the complexity, which will mostly vary as a function of the
number of frequent closed patterns.

5 Conclusion

This paper introduced a new general deﬁnition for the expected support of se-
quential patterns, which speciﬁcally focuses on the order within the pattern.

We have described the intuition behind our deﬁnitions, as well as eﬃcient algo-
rithms for both the computation of the expected support and the exact exploration
of the search space. Put together, these contributions allowed us to introduce SkO-
PUS which constitutes, to the best of our knowledge, the ﬁrst framework for the
exact mining of the k sequential patterns with highest leverage from data.

Experiments on controlled data have validated the consistency and relevance

of our framework, relevance that we have then conﬁrmed on literary work.

This work naturally raised a number of questions and issues that, we anticipate,

will be of great interest to the community in the future. These include:

6 Although support is often not a good proxy for interestingness, it remains that SkOPUS

can be very eﬃciently and exactly extract the most frequent sequential patterns.

22

Fran¸cois Petitjean et al.

Table 6 Running time on the diﬀerent datasets for extracting the top-20.

Runtimes

Name

Support

Leverage

rprt [33] (mining + ranking)

Synth-1
Synth-2
Synth-3
Synth-4
Synth-5
Synth-6
Synth-7
Synth-8
Synth-9
Synth-10
JMLR
Lawyers
Finn
Sawyer
NewPhysics
TwoCities
Animals

< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
< 1s
16s
26s
6s
7s
5s
14s
8s

1h41m57s
1h44m28s
53m19s
1h24m40s
1h34m41s
3h55m33s
6h5m31s
1h25m25s
2h28m30s
8h58m53s
37s
3m51s
1m8s
1m23s
48s
2m16s
1m46s

5s + 4s
5s + 5s
9s + 9s
6s + 6s
7s + 7s
10s + 11s
18s + 19s
16s + 14s
21s + 24s
8m15s + 6m16s
6m47s + 1m3s
13s + 2s
5s + 2s
2s + 1s
3s + 1s
6s + 2s
1s + 1s

1. ﬁnding heuristics to quickly ﬁll the temporary top-k with patterns of high-

interest, possibly by integrating some background knowledge;

2. reﬁning upper bounds for the most common interestingness measures;
3. ﬁltering the results to remove trivial subpatterns from those that are discovered

(patterns highlighted in boldface in Table 3);

4. assessing the statistical signiﬁcance of the extracted patterns, so that only
patterns that have high-likelihood to be observed in future data would be
extracted (as initiated by [9, 3]);

5. using background knowledge to deﬁne a model of the joint distribution, and
then extract the patterns that diﬀer the most from it; such an approach has
been investigated by S. Jaroszewicz on non-sequential patterns, and models
the joint distribution with a Bayesian Network [18];

6. many domains hold patterns of the type “admission, then biopsy or blood-test,
and then surgery”. Our work focuses on the extraction of chains of events (serial
episodes); we believe that extension to the more general classes of episodes will
be of major interest.

Acknowledgements We are grateful to the anonymous reviewers and to Matteo Riondato
for valuable and constructive comments on this manuscript.

Supplementary material

For completeness, we make available the top-100 patterns extracted from six lit-
erary works (see Table 7) and for all the methods discussed at https://github.
com/fpetitjean/Skopus/.

SkOPUS

23

Table 7 Characteristics of the literary datasets

Name

|I|

|D|

Avg. length Max. length

The Industries of Animals
A Book About Lawyers
Adventures of Huckleberry Finn
A Tale of Two Cities
The Adventures of Tom Sawyers
The New Physics and Its Evolution

5547
8386
4504
6392
5073
3832

4931
4787
6402
8584
5259
3081

12.8
21.9
13.7
12.4
10.7
19.4

107
211
213
150
119
81

Compliance with Ethical Standards

This material is based upon work supported by the Air Force Oﬃce of Scien-
tiﬁc Research, Asian Oﬃce of Aerospace Research and Development (AOARD)
under award number FA2386-15-1-4007. This research was also supported by
China Special Fund for Meteorological Research in the Public Interest under grant
GYHY201306070, Jiangsu Government Scholarship for Overseas Studies; and by
the Australian Research Council under grant DP140100087.

References

1. The Oxford English Corpus: Facts about Language.

In: Oxford Dictionar-
http://www.oxforddictionaries.com/words/

ies. Oxford University Press (2015).
the-oec-facts-about-the-language

2. Achar, A., Laxman, S., Viswanathan, R., Sastry, P.: Discovering injective episodes with
general partial orders. Data Mining and Knowledge Discovery 25(1), 67–108 (2012)
3. Achar, A., Sastry, P.: Statistical signiﬁcance of episodes with general partial orders. In-

formation Sciences 296(0), 175–200 (2015)

4. Aggarwal, C.C., Han, J. (eds.): Frequent pattern mining. Springer (2014)
5. Agrawal, R., Imielinski, T., Swami, A.: Mining association rules between sets of items in
large databases. In: Proceedings of the 1993 ACM-SIGMOD International Conference on
Management of Data, pp. 207–216. Washington, DC (1993)

6. Agrawal, R., Srikant, R.: Mining sequential patterns.

In: Proceedings of the Eleventh
International Conference on Data Engineering, March 6-10, 1995, Taipei, Taiwan, pp.
3–14. IEEE Computer Society (1995)

7. Bayardo Jr, R.J., Agrawal, R.: Mining the most interesting rules. In: Proceedings of the
ﬁfth ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 145–154. ACM (1999)

8. Boley, M., Horv´ath, T., Wrobel, S.: Eﬁcient discovery of interesting patterns based on

strong closedness. Statistical Analysis and Data Mining 2(5-6), 346–360 (2009)

9. Castro, N.C., Azevedo, P.J.: Signiﬁcant motifs in time series. Statistical Analysis and

Data Mining: The ASA Data Science Journal 5(1), 35–53 (2012)

10. Fournier-Viger, P., Gomariz, A., Gueniche, T., Mwamikazi, E., Thomas, R.: TKS: eﬃcient
mining of top-k sequential patterns.
In: Advanced Data Mining and Applications, 9th
International Conference, ADMA 2013, Hangzhou, China, December 14-16, 2013, Pro-
ceedings, Part I, Lecture Notes in Computer Science, vol. 8346, pp. 109–120. Springer
(2013)

11. Geng, L., Hamilton, H.J.: Choosing the right lens: Finding what is interesting in data

mining. In: Quality Measures in Data Mining, pp. 3–24. Springer (2007)

12. Gwadera, R., Atallah, M.J., Szpankowski, W.: Markov models for identiﬁcation of signiﬁ-
cant episodes. In: SIAM International Conference on Data Mining, pp. 404–414 (2005)
13. Gwadera, R., Atallah, M.J., Szpankowski, W.: Reliable detection of episodes in event

sequences. Knowledge and Information Systems 7(4), 415–437 (2005)

24

Fran¸cois Petitjean et al.

14. Gwadera, R., Crestani, F.: Ranking sequential patterns with respect to signiﬁcance. In: Ad-
vances in Knowledge Discovery and Data Mining, 14th Paciﬁc-Asia Conference, PAKDD
2010, Hyderabad, India, June 21-24, 2010. Proceedings. Part I, Lecture Notes in Computer
Science, vol. 6118, pp. 286–299. Springer (2010)

15. H¨am¨al¨ainen, W.: Eﬃcient discovery of the top-k optimal dependency rules with Fisher’s
exact test of signiﬁcance. In: ICDM 2010, The 10th IEEE International Conference on Data
Mining, Sydney, Australia, 14-17 December 2010, pp. 196–205. IEEE Computer Society
(2010)

16. Han, J., Cheng, H., Xin, D., Yan, X.: Frequent pattern mining: current status and future

directions. Data Mining and Knowledge Discovery 15(1), 55–86 (2007)

17. Han, J., Pei, J., Mortazavi-Asl, B., Chen, Q., Dayal, U., Hsu, M.: Freespan: frequent
pattern-projected sequential pattern mining. In: Proceedings of the sixth ACM SIGKDD
international conference on Knowledge discovery and data mining, Boston, MA, USA,
August 20-23, 2000, pp. 355–359. ACM (2000)

18. Jaroszewicz, S., Simovici, D.A.: Interestingness of frequent itemsets using bayesian net-
works as background knowledge. In: Proceedings of the Tenth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 178–196. ACM (2004)

19. Lam, H.T., Moerchen, F., Fradkin, D., Calders, T.: Mining compressing sequential pat-
terns. Statistical Analysis and Data Mining: The ASA Data Science Journal 7(1), 34–52
(2014)

20. Low-Kam, C., Ra¨ıssi, C., Kaytoue, M., Pei, J.: Mining statistically signiﬁcant sequential
patterns. In: 2013 IEEE 13th International Conference on Data Mining, Dallas, TX, USA,
December 7-10, 2013, pp. 488–497. IEEE Computer Society (2013)

21. Mabroukeh, N.R., Ezeife, C.I.: A taxonomy of sequential pattern mining algorithms. ACM

Computing Surveys (CSUR) 43(1), 3 (2010)

22. Mampaey, M., Vreeken, J., Tatti, N.: Summarizing data succinctly with the most infor-
mative itemsets. ACM Transactions on Knowledge Discovery from Data 6(4), 16 (2012)
23. Mannila, H., Toivonen, H., Verkamo, A.I.: Discovering frequent episodes in sequences.
In: Proceedings of the First International Conference on Knowledge Discovery and Data
Mining (KDD-95), Montreal, Canada, August 20-21, 1995, pp. 210–215. AAAI Press (1995)
24. Mannila, H., Toivonen, H., Verkamo, A.I.: Discovery of frequent episodes in event se-

quences. Data Mining and Knowledge Discovery 1(3), 259–289 (1997)

25. Masseglia, F., Cathala, F., Poncelet, P.: The PSP approach for mining sequential patterns.
In: Principles of Data Mining and Knowledge Discovery, Second European Symposium,
PKDD ’98, Nantes, France, September 23-26, 1998, Proceedings, Lecture Notes in Com-
puter Science, vol. 1510, pp. 176–184. Springer (1998)

26. Mooney, C.H., Roddick, J.F.: Sequential pattern mining–approaches and algorithms. ACM

Computing Surveys (CSUR) 45(2), 19 (2013)

27. Pei, J., Han, J., Mortazavi-Asl, B., Pinto, H., Chen, Q., Dayal, U., Hsu, M.: Preﬁxspan:
Mining sequential patterns by preﬁx-projected growth. In: Proceedings of the 17th In-
ternational Conference on Data Engineering, April 2-6, 2001, Heidelberg, Germany, pp.
215–224. IEEE Computer Society (2001)

28. Piatetsky-Shapiro, G.: Discovery, analysis, and presentation of strong rules.

In:
G. Piatetsky-Shapiro, J. Frawley (eds.) Knowledge Discovery in Databases, pp. 229–248.
AAAI/MIT Press, Menlo Park, CA. (1991)

29. Ra¨ıssi, C., Calders, T., Poncelet, P.: Mining conjunctive sequential patterns. Data Mining

and Knowledge Discovery 17(1), 77–93 (2008)

30. Siebes, A., Vreeken, J., van Leeuwen, M.: Item sets that compress. In: Proceedings of the
Sixth SIAM International Conference on Data Mining, April 20-22, 2006, Bethesda, MD,
USA, pp. 395–406. SIAM (2006)

31. Tatti, N.: Signiﬁcance of episodes based on minimal windows.

In: IEEE International

Conference on Data Mining, pp. 513–522 (2009)

32. Tatti, N.: Discovering episodes with compact minimal windows. Data Mining and Knowl-

edge Discovery 28(4), 1046–1077 (2014)

33. Tatti, N.: Ranking episodes using a partition model. Data Mining and Knowledge Discov-

ery pp. 1–31 (2015)

34. Tatti, N., Mampaey, M.: Using background knowledge to rank itemsets. Data Mining and

Knowledge Discovery 21(2), 293–309 (2010)

35. Tatti, N., Vreeken, J.: The long and the short of it: Summarising event sequences with
serial episodes. In: Proceedings of the Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 462–470 (2012)

SkOPUS

25

36. Tew, C.V., Giraud-Carrier, C.G., Tanner, K.W., Burton, S.H.: Behavior-based clustering
and analysis of interestingness measures for association rule mining. Data Mining and
Knowledge Discovery 28(4), 1004–1045 (2014)

37. Tucker, A.: Applied Combinatorics. John Wiley & Sons, Inc., New York, NY, USA (2006)
38. Tzvetkov, P., Yan, X., Han, J.: TSP: mining top-k closed sequential patterns. In: Pro-
ceedings of the 3rd IEEE International Conference on Data Mining (ICDM 2003), 19-22
December 2003, Melbourne, Florida, USA, pp. 347–354. IEEE Computer Society (2003)
39. Vreeken, J., van Leeuwen, M., Siebes, A.: Krimp: mining itemsets that compress. Data

Mining and Knowledge Discovery 23(1), 169–214 (2011)

40. Webb, G.: Self-suﬃcient itemsets: An approach to screening potentially interesting as-
sociations between items. Transactions on Knowledge Discovery from Data 4, 3:1–3:20
(2010)

41. Webb, G.I.: OPUS: an eﬃcient admissible algorithm for unordered search. Journal of

Artiﬁcial Intelligence Research (JAIR) 3, 431–465 (1995)

42. Webb, G.I.: Eﬃcient search for association rules.

In: Proceedings of the Sixth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 99–
107. ACM (2000)

43. Webb, G.I.: Discovering signiﬁcant patterns. Machine Learning 68(1), 1–33 (2007)
44. Webb, G.I.: Layered critical values: A powerful direct-adjustment approach to discovering

signiﬁcant patterns. Machine Learning 71(2-3), 307–323 (2008)

45. Webb, G.I.: Self-suﬃcient itemsets: An approach to screening potentially interesting asso-
ciations between items. ACM Transactions on Knowledge Discovery from Data (TKDD)
4(1), 3 (2010)

46. Webb, G.I.: Filtered-top-k association discovery. Wiley Interdisc. Rev.: Data Mining and

Knowledge Discovery 1(3), 183–192 (2011)

47. Webb, G.I., Vreeken, J.: Eﬃcient discovery of the most interesting associations. ACM

Transactions on Knowledge Discovery from Data 8(3), 1–31 (2014)

48. Yan, X., Han, J., Afshar, R.: Clospan: Mining closed sequential patterns in large databases.
In: Proceedings of the Third SIAM International Conference on Data Mining, San Fran-
cisco, CA, USA, May 1-3, 2003, pp. 166–177. SIAM (2003)

49. Zimmermann, A.: Objectively evaluating interestingness measures for frequent itemset
In: J. Li, L. Cao, C. Wang, K. Tan, B. Liu, J. Pei, V. Tseng (eds.) Trends
mining.
and Applications in Knowledge Discovery and Data Mining, Lecture Notes in Computer
Science, vol. 7867, pp. 354–366. Springer Berlin Heidelberg (2013)

