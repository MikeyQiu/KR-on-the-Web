Bridging the Gap between Stochastic Gradient MCMC
and Stochastic Optimization

6
1
0
2
 
g
u
A
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
6
9
7
0
.
2
1
5
1
:
v
i
X
r
a

Changyou Chen†

David Carlson‡

Zhe Gan†
†Department of Electrical and Computer Engineering, Duke University
‡Department of Statistics and Grossman Center for Statistics of Mind, Columbia University

Chunyuan Li†

Lawrence Carin†

Abstract

Stochastic gradient Markov chain Monte
Carlo (SG-MCMC) methods are Bayesian
analogs to popular stochastic optimization
methods; however, this connection is not
well studied. We explore this relationship
by applying simulated annealing to an SG-
MCMC algorithm. Furthermore, we extend
recent SG-MCMC methods with two key
components: i) adaptive preconditioners (as
in ADAgrad or RMSprop), and ii) adaptive
element-wise momentum weights. The zero-
temperature limit gives a novel stochastic
optimization method with adaptive element-
wise momentum weights, while conventional
optimization methods only have a shared,
static momentum weight. Under certain as-
sumptions, our theoretical analysis suggests
the proposed simulated annealing approach
converges close to the global optima. Experi-
ments on several deep neural network models
show state-of-the-art results compared to re-
lated stochastic optimization algorithms.

1 Introduction

Machine learning has made signiﬁcant recent strides
due to large-scale learning applied to “big data”.
typically performed with
Large-scale learning is
stochastic optimization,
common
and the most
method is stochastic gradient descent (SGD) (Bottou,
2010). Stochastic optimization methods are devoted
to obtaining a (local) optima of an objective function.
Alternatively, Bayesian methods aim to compute the
expectation of a test function over the posterior dis-

Appearing in Proceedings of the 19th International Con-
ference on Artiﬁcial Intelligence and Statistics (AISTATS)
2016, Cadiz, Spain. JMLR: W&CP volume 41. Copyright
2016 by the authors.

tribution. At ﬁrst glance, these methods appear to be
distinct, independent approaches to learning. How-
ever, even the celebrated Gibbs sampler was ﬁrst in-
troduced to statistics as a simulated annealing method
for maximum a posteriori estimation (i.e., ﬁnding an
optima) (Geman and Geman, 1984).

Recent work on large-scale Bayesian learning has fo-
cused on incorporating the speed and low-memory
costs from stochastic optimization. These approaches
are referred to as stochastic gradient Markov chain
Monte Carlo (SG-MCMC) methods. Well-known SG-
MCMC methods include stochastic gradient Langevin
dynamics (SGLD) (Welling and Teh, 2011), stochastic
gradient Hamiltonian Monte Carlo (SGHMC) (Chen
et al., 2014), and stochastic gradient thermostats
(SGNHT) (Ding et al., 2014). SG-MCMC has become
increasingly popular in the literature due to practi-
cal successes, ease of implementation, and theoretical
convergence properties (Teh et al., 2014; Vollmer et al.,
2015; Chen et al., 2015).

There are obvious structural similarities between SG-
MCMC algorithms and stochastic optimization meth-
ods. For example, SGLD resembles SGD with additive
Gaussian noise. SGHMC resembles SGD with momen-
tum (Rumelhart et al., 1986), adding additive Gaus-
sian noise when updating the momentum terms (Chen
et al., 2014). These similarities are detailed in Section
2. Despite these structural similarities, the theory is
unclear on how additive Gaussian noise diﬀerentiates
a Bayesian algorithm from its optimization analog.

Just as classical sampling methods were originally used
for optimization (Geman and Geman, 1984), we di-
rectly address using SG-MCMC algorithms for opti-
mization. A major beneﬁt of adapting these schemes
is that Bayesian learning is (in theory) able to fully ex-
plore the parameter space. Thus it may ﬁnd a better
local optima, if not the global optima, for a non-convex
objective function.

Speciﬁcally, in this work we ﬁrst extend the recently
proposed multivariate stochastic gradient thermostat

Bridging the Gap between SG-MCMC and Stochastic Optimization

algorithm (Gan et al., 2015) with Riemannian infor-
mation geometry, which results in an adaptive pre-
conditioning and momentum scheme with analogs to
Adam (Kingma and Ba, 2015) and RMSprop (Tiele-
man and Hinton, 2012). We propose an annealing
scheme on the system temperature to move from a
Bayesian method to a stochastic optimization method.
We call the proposed algorithm Stochastic AnNeal-
ing Thermostats with Adaptive momentum (Santa).
We show that in the temperature limit, Santa recov-
ers the SGD with momentum algorithm except that: i)
adaptive preconditioners are used when updating both
model and momentum parameters; ii) each parame-
ter has an individual, learned momentum parameter.
Adaptive preconditioners and momentums are desir-
able in practice because of their ability to deal with
uneven, dynamic curvature (Dauphin et al., 2015). For
completeness, we ﬁrst review related algorithms in Sec-
tion 2, and present our novel algorithm in Section 3.

We develop theory to analyze convergence properties
of our algorithm, suggesting that Santa is able to ﬁnd
a solution for an (non-convex) objective function close
to its global optima, shown in Section 4. The theory is
based on the analysis from stochastic diﬀerential equa-
tions (Teh et al., 2014; Chen et al., 2015), and presents
results on bias and variance of the annealed Markov
chain. This is a fundamentally diﬀerent approach from
the traditional convergence explored in stochastic opti-
mization, or the regret bounds used in online optimiza-
tion. We note we can adapt the regret bound of Adam
(Kingma and Ba, 2015) for our zero-temperature al-
gorithm (with a few trivial modiﬁcations) for a con-
vex problem, as shown in Supplementary Section F.
However, this neither addresses non-convexity nor the
annealing scheme that our analysis does.

In addition to theory, we demonstrate eﬀective empir-
ical performance on a variety of deep neural networks
(DNNs), achieving the best performance compared to
all competing algorithms for the same model size. This
is shown in Section 5. The code is publicly available
at https://github.com/cchangyou/Santa.

2 Preliminaries

Throughout this paper, we denote vectors as bold,
lower-case letters, and matrices as bold, upper-case
letters. We use (cid:12) for element-wise multiplication, and
· denotes the element-
(cid:11) as element-wise division;
wise square root when applied to vectors or matricies.
We reserve (·)1/2 for the standard matrix square root.
Ip is the p × p identity matrix, 1 is an all-ones vector.

√

The goal of an optimization algorithm is to min-
imize an objective function U (θ) that corresponds
to a (non-convex) model of interest.
In a Bayesian
model, this corresponds to the potential energy de-

ﬁned as the negative log-posterior, U (θ) (cid:44) − log p(θ)−
(cid:80)N
n=1 log p(xn |θ). Here θ ∈ Rp are the model param-
eters, and {xn}n=1,...,N are the d-dimensional observed
data; p(θ) corresponds to the prior and p(xn |θ) is a
likelihood term for the nth observation. In optimiza-
tion, − (cid:80)N
n=1 log p(xn |θ) is typically referred to as the
loss function, and − log p(θ) as a regularizer.

In large-scale learning, N is prohibitively large.
This motivates the use of stochastic approximations.
We denote the stochastic approximation ˜Ut(θ) (cid:44)
− log p(θ) − N
j=1 log p(xij |θ), where (i1, · · · , im)
m
is a random subset of the set {1, 2, · · · , N }. The gra-
dient on this minibatch is denoted as ˜f t(θ) = ∇ ˜Ut(θ),
which is an unbiased estimate of the true gradient.

(cid:80)m

A standard approach to learning is SGD, where param-
˜f t−1(θ) with
eter updates are given by θt = θt−1 − ηt
ηt the learning rate. This is guaranteed to converge to
a local minima under mild conditions (Bottou, 2010).
The SG-MCMC analog to this is SGLD, with updates
2ηtζt. The additional term
θt = θt−1 − ηt
is a standard normal random vector, ζt ∼ N (0, Ip)
(Welling and Teh, 2011). The SGLD method draws
approximate posterior samples instead of obtaining a
local minima.

˜f t−1(θ) +

√

Using momentum in stochastic optimization is impor-
tant in learning deep models (Sutskever et al., 2013).
This motivates SG-MCMC algorithms with momen-
tum. The standard SGD with momentum (SGD-M)
approach introduces an auxiliary variable ut ∈ Rp
to represent the momentum. Given a momentum
weight α, the updates are θt = θt−1 + ηtut and
ut = (1 − α)ut−1 − ˜f t−1(θ). A Bayesian analog is
SGHMC (Chen et al., 2014) or multivariate SGNHT
(mSGNHT) (Gan et al., 2015).
In mSGNHT, each
parameter has a unique momentum weight αt ∈ Rp
that is learned during the sampling sequence. The
momentum weights are updated to maintain the sys-
tem temperature 1/β. An inverse temperature of
β = 1 corresponds to the posterior. This algorithm
has updates θt = θt−1 + ηtut, ut = (1 − ηtαt−1) (cid:12)
˜f t−1(θ) + (cid:112)2ηt/βζt. The main diﬀerence
ut−1 − ηt
is the additive Gaussian noise and step-size depen-
dent momentum update. The weights have updates
αt = αt−1 + ηt((ut (cid:12) ut) − 1/β), which matches the
kinetic energy to the system temperature.

A recent idea in stochastic optimization is to use
an adaptive preconditioner, also known as a variable
metric, to improve convergence rates. Both ADA-
grad (Duchi et al., 2011) and Adam (Kingma and Ba,
2015) adapt to the local geometry with a regret bound
N ). Adam adds momentum as well through
of O(
moment smoothing. RMSprop (Tieleman and Hin-
ton, 2012), Adadelta (Zeiler, 2012), and RMSspectral

√

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Algorithm 1: Santa with the Euler scheme
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t (cid:44) ∇θ ˜U (θt−1) on the tth mini-batch;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
if t < burnin then
/* exploration
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt);
(cid:1) (cid:11) ut−1 +
ut = η
βt

(cid:0)1 − gt−1 (cid:11) gt

(cid:113) 2η
βt

gt−1 (cid:12) ζt

*/

else

/* refinement
αt = αt−1; ut = 0;

end
ut = ut + (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t;
θt = θt−1 + gt (cid:12) ut;

end

*/

(Carlson et al., 2015) are similar methods with pre-
conditioners. Our method introduces adaptive mo-
mentum and preconditioners to the SG-MCMC. This
diﬀers from stochastic optimization in implementation
and theory, and is novel in SG-MCMC.
Simulated annealing (Kirkpatrick et al., 1983; ˇCern´y,
1985) is well-established as a way of acquiring a local
mode by moving from a high-temperature, ﬂat surface
to a low-temperature, peaky surface. It has been ex-
plored in the context of MCMC, including reversible
jump MCMC (Andrieu et al., 2000), annealed impor-
tant sampling (Neal, 2001) and parallel tempering (Li
et al., 2009). Traditional algorithms are based on
Metropolis–Hastings sampling, which require compu-
tationally expensive accept-reject steps. Recent work
has applied simulated annealing to large-scale learn-
ing through mini-batch based annealing (van de Meent
et al., 2014; Obermeyer et al., 2014). Our approach in-
corporates annealing into SG-MCMC with its inherent
speed and mini-batch nature.

3 The Santa Algorithm

Santa extends the mSGNHT algorithm with precon-
ditioners and a simulated annealing scheme. A simple
pseudocode is shown in Algorithm 1, or a more com-
plex, but higher accuracy version, is shown in Algo-
rithm 2, and we detail the steps below.

The ﬁrst extension we consider is the use of adaptive
preconditioners. Preconditioning has been proven crit-
ical for fast convergence in both stochastic optimiza-

tion (Dauphin et al., 2015) and SG-MCMC algorithms
(Patterson and Teh, 2013). In the MCMC literature,
preconditioning is alternatively referred to as Rieman-
nian information geometry (Patterson and Teh, 2013).
We denote the preconditioner as {Gt ∈ Rp×p}. A pop-
ular choice in SG-MCMC is the Fisher information ma-
trix (Girolami and Calderhead, 2011). Unfortunately,
this approach is computationally prohibitive for many
models of interest. To avoid this problem, we adopt
the preconditioner from RMSprop and Adam, which
uses a vector {gt ∈ Rp} to approximate the diagonal
of the Fisher information matrixes (Li et al., 2016a).
The construction sequentially updates the precondi-
tioner based on current and historical gradients with
a smoothing parameter σ, and is shown as part of Al-
gorithm 1. While this approach will not capture the
Riemannian geometry as eﬀectively as the Fisher in-
formation matrix, it is computationally eﬃcient.

Santa also introduces an annealing scheme on sys-
tem temperatures. As discussed in Section 2, mS-
GNHT naturally accounts for a varying temperature
by matching the particle momentum to the system
temperature. We introduce β = {β1, β2, · · · }, a se-
quence of inverse temperature variables with βi < βj
for i < j and limi→∞ βi = ∞. The inﬁnite case
corresponds to the zero-temperature limit, where SG-
MCMCs become deterministic optimization methods.

The annealing scheme leads to two stages:
the ex-
ploration and the reﬁnement stages. The exploration
stage updates all parameters based on an annealed se-
quence of stochastic dynamic systems (see Section 4
for more details). This stage is able to explore the
parameter space eﬃciently, escape poor local modes,
and ﬁnally converge close to the global mode. The re-
ﬁnement stage corresponds to the temperature limit,
i.e., βn → ∞. In the temperature limit, the momen-
tum weight updates vanish and it becomes a stochastic
optimization algorithm.

We propose two update schemes to solve the corre-
sponding stochastic diﬀerential equations: the Euler
scheme and the symmetric splitting scheme (SSS). The
Euler scheme has simpler updates, as detailed in Algo-
rithm 1; while SSS endows increased accuracy (Chen
et al., 2015) with a slight increase in overhead compu-
tation, as shown in Algorithm 2. Section 4.1 elaborates
on the details of these two schemes. We recommend
the use of SSS, but the Euler scheme is simpler to im-
plement and compare to known algorithms.

Practical considerations According to Section 4,
the exploration stage helps the algorithm traverse the
parameter space following the posterior curve as ac-
curate as possible. For optimization, slightly biased
samples do not aﬀect the ﬁnal solution. As a result,

Bridging the Gap between SG-MCMC and Stochastic Optimization

Algorithm 2: Santa with SSS
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t (cid:44) ∇θ ˜U (θt−1) on the tth mini-batch;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
θt = θt−1 + gt (cid:12) ut−1 /2;
if t < burnin then
/* exploration
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt) /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη + (cid:112)2 gt−1 η/βt (cid:12) ζt

*/

+ η/βt

(cid:0)1 − gt−1 (cid:11) gt
ut = exp (−αt/2) (cid:12) ut;
αt = αt + (ut (cid:12) ut −η/βt) /2;

(cid:1) (cid:11) ut−1;

else

/* refinement
αt = αt−1;
ut = ut − gt (cid:12)˜f tη; ut = exp (−αt/2) (cid:12) ut;

ut = exp (−αt/2) (cid:12) ut−1;

*/

end
θt = θt + gt (cid:12) ut /2;

end

the term consisting of (1 − gt−1 (cid:11) gt) in the algorithm
(which is an approximation term, see Section 4.1) is
ignored. We found no decreasing performance in our
experiments. Furthermore, the term gt−1 associated
with the Gaussian noise could be replaced with a ﬁxed
constant without aﬀecting the algorithm.

4 Theoretical Foundation

In this section we present the stochastic diﬀerential
equations (SDEs) that correspond to the Santa algo-
rithm. We ﬁrst introduce the general SDE framework,
then describe the exploration stage in Section 4.1 and
the reﬁnement stage in Section 4.2. We give the con-
vergence properties of the numerical scheme in Section
4.3. This theory uses tools from the SDE literature and
extends the mSGNHT theory (Ding et al., 2014; Gan
et al., 2015).

The SDEs are presented with re-parameterized p =
u /η1/2, Ξ = diag(α)/η1/2, as in Ding et al. (2014).
The SDEs describe the motion of a particle in a system
where θ is the location and p is the momentum.

In mSGNHT, the particle is driven by a force
−∇θ ˜Ut(θ) at time t. The stationary distribution of θ
corresponds to the model posterior (Gan et al., 2015).
Our critical extension is the use of Riemannian infor-

mation geometry, important for fast convergence (Pat-
terson and Teh, 2013). Given an inverse temperature
β, the system is described by the following SDEs1:






dθ = G1(θ) p dt
(cid:16)
d p =

−G1(θ)∇θU (θ) − Ξ p + 1

β ∇θG1(θ)

+G1(θ)(Ξ − G2(θ))∇θG2(θ)) dt + ( 2
dt ,

dΞ =

(cid:17)

(cid:16)

Q − 1
β I

β G2(θ))

1
2 dw

(1)

where Q = diag(p (cid:12) p), w is standard Brownian mo-
tion, G1(θ) encodes geometric information of the po-
tential energy U (θ), and G2(θ) characterizes the man-
ifold geometry of the Brownian motion. Note G2(θ)
may be the same as G1(θ) for the same Riemannian
manifold. We call G1(θ) and G2(θ) Riemannian met-
rics, which are commonly deﬁned by the Fisher infor-
mation matrix (Girolami and Calderhead, 2011). We
use the RMSprop preconditioner (with updates from
Algorithm 1) for computational feasibility. Using the
Fokker-Plank equation (Risken, 1989), we show that
the marginal stationary distribution of (1) corresponds
to the posterior distribution.
AT B
Lemma 1. Denote A : B (cid:44) tr
ary distribution of (1) is: pβ(θ, p, Ξ) ∝

.The station-

(cid:110)

(cid:111)

e−βU (θ)− β

2 pT p − β

2 (Ξ−G2(θ)):(Ξ−G2(θ)) .

(2)

An inverse temperature β = 1 corresponds to the stan-
dard Bayesian posterior.

We note that p in (1) has additional dependencies on
G1 and G2 compared to Gan et al. (2015) that must be
accounted for. Ξ p introduces friction into the system
so that the particle does not move too far away by
the random force; the terms ∇θG1(θ) and ∇θG2(θ)
penalize the inﬂuences of the Riemannian metrics so
that the stationary distribution remains invariant.

4.1 Exploration

The ﬁrst stage of Santa, exploration, explores the pa-
rameter space to obtain parameters near the global
mode of an objective function2. This approach applies
ideas from simulated annealing (Kirkpatrick et al.,
1983). Speciﬁcally, the inverse temperature β is slowly
annealed to temperature zero to freeze the particles at
the global mode.

Minimizing U (θ) is equivalent to sampling from the
zero-temperature limit pβ(θ) (cid:44) 1
e−βU (θ) (propor-
Zβ

1We abuse notation for conciseness. Here, ∇θ G(θ) is

a vector with the i-th element being (cid:80)

j ∇θj Gij(θ).

2This requires an ergodic algorithm. While ergodicity is
not straightforward to check, we follow most MCMC work
and assume it holds in our algorithm.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

tional to (2)), with Zβ being the normalization con-
stant such that pβ(θ) is a valid distribution. We
construct a Markov chain that sequentially transits
from high temperatures to low temperatures. At the
state equilibrium, the chain reaches the temperature
limit with marginal stationary distribution ρ0(θ) (cid:44)
limβ→∞ e−βU (θ), a point mass3 located at the global
mode of U (θ). Speciﬁcally, we ﬁrst deﬁne a sequence
of inverse temperatures, (β1, β2, · · · , βL), such that βL
is large enough4. For each time t, we generate a sam-
ple according to the SDE system (1) with temperature
1
, conditioned on the sample from the previous tem-
βt
. We call this procedure annealing ther-
perature,
mostats to denote the analog to simulated annealing.

1
βt−1

Generating approximate samples Generating
exact samples from (1) is infeasible for general mod-
els. One well-known numerical approach is the Euler
scheme in Algorithm 1. The Euler scheme is a 1st-
order method with relatively high approximation error
(Chen et al., 2015). We increase accuracy by imple-
menting the symmetric splitting scheme (SSS) (Chen
et al., 2015; Li et al., 2016b). The idea of SSS is to split
an infeasible SDE into several sub-SDEs, where each
sub-SDE is analytically solvable; approximate samples
are generated by sequentially evolving parameters via
these sub-SDEs. Speciﬁcally, in Santa, we split (1)
into the following three sub-SDEs:

A :

O :









dθ = G1(θ) p dt
d p = 0
(cid:16)
dΞ =

Q − 1
β I

(cid:17)

dt

, B :






dθ = 0
d p = −Ξ p dt
dΞ = 0

,

dθ = 0
(cid:16)
d p =
+G1(θ)(Ξ − G2(θ))∇θG2(θ)) dt + ( 2
dΞ = 0

−G1(θ)∇θU (θ) + 1

β ∇θG1(θ)

β G2(θ))

1
2 dw

We then update the sub-SDEs in order A-B-O-B-A
to generative approximate samples (Chen et al., 2015).
This uses half-steps h/2 on the A and B updates5,
and full steps h in the O update. This is analogous to
the leapfrog steps in Hamiltonian Monte Carlo (Neal,
2011). Update equations are given in the Supplemen-
tary Section A. The resulting parameters then serve
as an approximate sample from the posterior distribu-
tion with the inverse temperature of β. Replacing G1
and G2 with the RMSprop preconditioners gives Al-
gorithm 2. These updates require approximations to
∇θG1(θ) and ∇θG2(θ), addressed below.

3The sampler samples a uniform distribution over global
modes, or a point mass if the mode is unique. We assume
uniqueness and say point mass for clarity henceforth.

4Due to numerical issues, it is impossible to set βL to
inﬁnity; we thus assign a large enough value for it and
handle the inﬁnity case in the reﬁnement stage.
5As in Ding et al. (2014), we deﬁne h =
η.

√

Approximate calculation for ∇θ G1(θ) We pro-
pose a computationally eﬃcient approximation for
calculating the derivative vector ∇θ G1(θ) based
on the deﬁnition.
for the i-th ele-
ment of ∇θ G1(θ) at the t-th iteration, denoted as
(∇θ Gt

1(θ))i, it is approximated as:

Speciﬁcally,

(∇θ Gt

1(θ))i

A1≈

(cid:88)

(Gt

1(θ))ij − (Gt−1
θtj − θ(t−1)j

1

(θ))ij

j
(∆ Gt
1)ij
1(θ) pt−1)jh

(cid:88)

A2=

(Gt

j

(cid:88)

=

j

(∆ Gt
1)ij
1(θ) ut−1)j

(Gt

(cid:44) Gt

1 − Gt−1

where ∆ Gt
. Step A1 follows by the
1
deﬁnition of a derivative, and A2 by using the update
equation for θt, i.e., θt = θt−1+Gt
1(θ) pt−1 h. Accord-
ing to Taylor’s theory, the approximation error for the
∇θ G1(θ) is O(h), e.g.,

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

i

j

(∆ Gt
1)ij
1(θ) ut−1)j

(Gt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− (∇θ Gt

1(θ))i

≤ Bth ,

(3)

for some positive constant Bt. The approximation error
is negligible in term of convergence behaviors because
it can be absorbed into the stochastic gradients error.
Formal theoretical analysis on convergence behaviors
with this approximation is given in later sections. Us-
ing similar methods, ∇θGt
2(θ) is also approximately
calculated.

4.2 Reﬁnement

stage

reﬁnement

corresponds

zero-
The
temperature limit of the exploration stage, where Ξ
is learned. We show that in the limit Santa gives sig-
niﬁcantly simpliﬁed updates, leading to an stochastic
optimization algorithm similar to Adam or SGD-M.

to the

We assume that the Markov chain has reached its
equilibrium after the exploration stage.
In the zero-
temperature limit, some terms in the SDE (1) vanish.
First, as β → ∞, the term 1
β ∇θ G1(θ) and the vari-
ance term for the Brownian motion approach 0. As
well, the thermostat variable Ξ approaches G2(θ), so
the term G1(θ)(Ξ − G2(θ))∇θ G2(θ) vanishes. The
stationary distribution in (2) implies E Qii
i →
0, which makes the SDE for Ξ in (1) vanish. As a re-
sult, in the reﬁnement stage, only θ and p need to be
updated. The Euler scheme for this is shown in Algo-
rithm 1, and the symmetric splitting scheme is shown
in Algorithm 2.

(cid:44) E p2

Relation to stochastic optimization algorithms
In the reﬁnement stage Santa is a stochastic optimiza-
tion algorithm. This relation is easier seen with the
Euler scheme in Algorithm 1. Compared with SGD-M
(Rumelhart et al., 1986), Santa has both adaptive gra-
dient and adaptive momentum updates. Unlike Ada-
grad (Duchi et al., 2011) and RMSprop (Tieleman and

Bridging the Gap between SG-MCMC and Stochastic Optimization

Hinton, 2012), reﬁnement Santa is a momentum based
algorithm.

The recently proposed Adam algorithm (Kingma and
Ba, 2015) incorporates momentum and precondition-
ing in what is denoted as “adaptive moments.” We
show in Supplementary Section F that a constant step
size combined with a change of variables nearly recov-
ers the Adam algorithm with element-wise momentum
weights. For these reasons, Santa serves as a more gen-
eral stochastic optimization algorithm that extends all
current algorithms. As well, for a convex problem and
a few trivial algorithmic changes, the regret bound of
T ),
Adam holds for reﬁnement Santa, which is O(
as detailed in Supplementary Section F. However, our
analysis is focused on non-convex problems that do not
ﬁt in the regret bound formulation.

√

4.3 Convergence properties

Our convergence properties are based on the frame-
work of Chen et al. (2015). The proofs for all the-
orems are given in the Supplementary Material. We
focus on the exploration stage of the algorithm. Us-
ing the Monotone Convergence argument (Schechter,
1997), the reﬁnement stage convergence is obtained by
taking the temperature limit from the results of the ex-
ploration stage. We emphasize that our approach dif-
fers from conventional stochastic optimization or on-
line optimization approaches. Our convergence rate
is weaker than many stochastic optimization methods,
including SGD; however, our analysis applies to non-
convex problems, whereas traditionally convergence
rates only apply to convex problems.

The goal of Santa is to obtain θ∗ such that θ∗ =
argminθ U (θ). Let {θ1, · · · , θL} be a sequence of pa-
rameters collected from the algorithm. Deﬁne ˆU (cid:44)
t=1 U (θt) as the sample average, ¯U (cid:44) U (θ∗) the
1
L
global optima of U (θ).

(cid:80)L

As in Chen et al. (2015), we require certain assump-
tions on the potential energy U . To show these as-
sumptions, we ﬁrst deﬁne a functional ψt for each t
that solves the following Poisson equation:

Ltψt(θt) = U (θt) − ¯U ,

(4)

Lt is the generator of the SDE system (1) in the t-th
E[f (xt+h)]−f (xt)
iteration, deﬁned Ltf (xt) (cid:44) limh→0+
h
where xt (cid:44) (θt, pt, Ξt), f : R3p → R is a compactly
supported twice diﬀerentiable function. The solution
functional ψt(θt) characterizes the diﬀerence between
U (θt) and the global optima ¯U for every θt. As shown
in Mattingly et al. (2010), (4) typically possesses a
unique solution, which is at least as smooth as U under
the elliptic or hypoelliptic settings. We assume ψt is
bounded and smooth, as described below.

Assumption 1. ψt and its up to 3rd-order deriva-
tives, Dkψt, are bounded by a function V(θ, p, Ξ),
i.e., (cid:107)Dkψ(cid:107) ≤ CkV rk for k = (0, 1, 2, 3), Ck, rk >
0. Furthermore, the expectation of V is bounded:
EV r(θ, p, Ξ) < ∞, and V is smooth such that
supt
sups∈(0,1) V r (sx + (1 − s) y) ≤ C (V r (x) + V r (y)),
∀x ∈ R3p, y ∈ R3p, r ≤ max{2rk} for some C > 0.

(cid:17)

Let ∆U (θ) (cid:44) U (θ) − U (θ∗). Further deﬁne an opera-
(cid:16)
G1(θ)(∇θ ˜Ut − ∇θU ) + Bt
· ∇p for each
tor ∆Vt =
βt
t, where Bt is from (3). Theorem 2 depicts the close-
ness of ˆU to the global optima ¯U in term of bias and
mean square error (MSE) deﬁned below.
Theorem 2. Let (cid:107)·(cid:107) be the operator norm. Under
Assumption 1, the bias and MSE of the exploration
stage in Santa with respect to the global optima for L
steps with stepsize h is bounded, for some constants
C > 0 and D > 0, with:

Bias:

E ˆU − ¯U

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ Ce−U (θ∗)
(cid:12)
(cid:18) 1
Lh

+ D

L
(cid:88)

(cid:90)

1
L
(cid:80)

t=1

(cid:32)

+

MSE: E

(cid:16) ˆU − ¯U

(cid:17)2

≤ C 2e−2U (θ∗)

(cid:33)

e−βt∆U (θ)dθ

t (cid:107)E∆Vt(cid:107)
L

(cid:19)

+ h2

.

L
(cid:88)

(cid:90)

(cid:32)

1
L

t=1
E (cid:107)∆Vt(cid:107)2
L

(cid:33)2

e−βt∆U (θ)dθ

(cid:33)

+ h4

.

+

1
Lh

(cid:32) 1
L

(cid:80)
t

+ D2

Both bounds for the bias and MSE have two parts.
The ﬁrst part contains integration terms, which char-
acterizes the distance between the global optima,
e−U (θ∗), and the unnormalized annealing distribu-
tions, e−βtU (θ), decreasing to zero exponentially fast
with increasing β; the remaining part characterizes the
distance between the sample average and the anneal-
ing posterior average. This shares a similar form as in
general SG-MCMC algorithms (Chen et al., 2015), and
can be controlled to converge. Furthermore, the term
(cid:80)
in the bias vanishes as long as the sum of
the annealing sequence {βt} is ﬁnite6, indicating that
the gradient approximation for ∇θG1(θ) in Section 4.1
does not aﬀect the bias of the algorithm. Similar ar-
guments apply for the MSE bound.

t(cid:107)E∆Vt(cid:107)
L

(cid:80)L+m−1
l=m

To get convergence results right before the reﬁnement
stage, let a sequence of functions {gm} be deﬁned as
e−βl ˆU (θ); it is easy to see that {gm}
gm (cid:44) − 1
L
satisﬁes gm1 < gm2 for m1 < m2, and limm→∞ gm =
0. According to the Monotone Convergence Theorem
(Schechter, 1997), the bias and MSE in the limit exists,
leading to Corollary 3.
Corollary 3. Under Assumptions 1, the bias and
MSE of the reﬁnement stage in Santa with respect

6In practice we might not need to care about this con-
straint because a small bias in the exploration stage does
not aﬀect convergence of the reﬁnement stage.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

to the global optima for L steps with stepsize h are
bounded, for some constants D1 > 0, D2 > 0, as
(cid:18) 1
Lh
(cid:32) 1
L

(cid:12)
(cid:12)
(cid:12) ≤ D1

E ˆU − ¯U

+ h2

Bias:

(cid:80)
t

(cid:80)

(cid:17)2

+

(cid:19)

(cid:12)
(cid:12)
(cid:12)

MSE: E

(cid:16) ˆU − ¯U

t (cid:107)E∆Vt(cid:107)
L
E (cid:107)∆Vt(cid:107)2
L

+

1
Lh

≤ D2

+ h4

(cid:33)

Corollary 3 implies that in the reﬁnement stage, the
discrepancy between annealing distributions and the
global optima vanishes, leaving only errors from dis-
cretized simulations of the SDEs, similar to the result
of general SG-MCMC (Chen et al., 2015). We note
that after exploration, Santa becomes a pure stochas-
tic optimization algorithm, thus convergence results
in term of regret bounds can also be derived; refer to
Supplementary Section F for more details.

5 Experiments

5.1

Illustration

In order to demonstrate that Santa is able to achieve
the global mode of an objective function, we consider
the double-well potential (Ding et al., 2014),

U (θ) = (θ + 4)(θ + 1)(θ − 1)(θ − 3)/14 + 0.5 .

As shown in Figure 1 (left), the double-well potential
has two modes, located at θ = −3 and θ = 2, with the
global optima at θ = −3. We use a decreasing learning
rate ht = t−0.3/10, and the annealing sequence is set to
βt = t2. To make the optimization more challenging,
we initialize the parameter at θ0 = 4, close to the local
mode. The evolution of θ with respect to iterations is
shown in Figure 1(right). As can be seen, θ ﬁrst moves
to the local mode but quickly jumps out and moves
to the global mode in the exploration stage (ﬁrst half
iterations); in the reﬁnement stage, θ quickly converges
to the global mode and sticks to it afterwards.
In
contrast, RMSprop is trapped on the local optima, and
convergences slower than Santa at the beginning.

Figure 1: (Left) Double-well potential. (Right) The
evolution of θ using Santa and RMSprop algorithms.

Figure 2: Learning curves of diﬀerent algorithms on
MNIST. (Left) FNN with size of 400. (Right) CNN.

5.2 Feedforward neural networks

We ﬁrst test Santa on the Feedforward Neural Net-
work (FNN) with rectiﬁed linear units (ReLU). We
test two-layer models with network sizes 784-X-X-10,
where X is the number of hidden units for each layer;
100 epochs are used. For variants of Santa, we de-
note Santa-E as Santa with a Euler scheme illustrated
in Algorithm 1, Santa-r as Santa running only on the
reﬁnement stage, but with updates on α as in the ex-
ploration stage. We compare Santa with SGD, SGD-
M, RMSprop, Adam, SGD with dropout, SGLD and
Bayes by Backprop (Blundell et al., 2015). We use
a grid search to obtain good learning rates for each
algorithm, resulting in 4 × 10−6 for Santa, 5 × 10−4
for RMSprop, 10−3 for Adam, and 5 × 10−1 for SGD,
SGD-M and SGLD. We choose an annealing schedule
of βt = Atγ with A = 1 and γ selected from 0.1 to 1
with an interval of 0.1. For simplicity, the exploration
is set to take half of total iterations.

We test the algorithms on the standard MNIST
dataset, which contains 28 × 28 handwritten digital
images from 10 classes with 60, 000 training samples
and 10, 000 test samples. The network size (X-X) is
set to 400-400 and 800-800, and test classiﬁcation er-
rors are shown in Table 1. Santa show improved state-
of-the-art performance amongst all algorithms. The
Euler scheme shows a slight decrease in performance,
due to the integration error when solving the SDE.
Santa without exploration (i.e., Santa-r) still performs
relatively well. Learning curves are plotted in Fig-
ure 2, showing that Santa converges as fast as other
algorithms but to a better local optima7.

5.3 Convolution neural networks

We next test Santa on the Convolution Neural Net-
work (CNN). Following Jarrett et al. (2009), a stan-
dard network conﬁguration with 2 convolutional layers
followed by 2 fully-connected layers is adopted. Both
convolutional layers use 5 × 5 ﬁlter size with 32 and 64

7Learning curves of FNN with size of 800 are provided

in Supplementary Section G.

Bridging the Gap between SG-MCMC and Stochastic Optimization

Algorithms
Santa
Santa-E
Santa-r
Adam
RMSprop
SGD-M
SGD
SGLD
BPB(cid:5)
SGD, Dropout(cid:5)
Stoc. Pooling(cid:46)
NIN, Dropout◦
Maxout, Dropout(cid:63)

FNN-400 FNN-800
1.21%
1.41%
1.45%
1.53%
1.59%
1.66%
1.72%
1.64%
1.32%
1.51%
−
−
−

CNN
1.16% 0.47%
0.58%
1.27%
0.49%
1.40%
0.59%
1.47%
0.64%
1.43%
0.77%
1.72%
0.81%
1.47%
0.71%
1.41%
−
1.34%
−
1.33%
0.47%
−
0.47%
−
0.45%
−

Table 1: Test error on MNIST classiﬁcation using FNN
and CNN. ((cid:5)) taken from Blundell et al. (2015). ((cid:46)) taken
from Zeiler and Fergus (2013). (◦) taken from Lin et al.
(2014). ((cid:63)) taken from Goodfellow et al. (2013).

channels, respectively; 2 × 2 max pooling is used af-
ter each convolutional layer. The fully-connected lay-
ers have 200-200 hidden nodes with ReLU activation.
The same parameter setting and dataset as in the FNN
are used. The test errors are shown in Table 1, and
the corresponding learning curves are shown in Fig-
ure 2. Similar trends as in FNN are obtained. Santa
signiﬁcantly outperforms other algorithms with an er-
ror of 0.45%. This result is comparable or even better
than some recent state-of-the-art CNN-based systems,
which have much more complex architectures.

5.4 Recurrent neural networks

We test Santa on the Recurrent Neural Network
is
(RNN) for sequence modeling, where a model
trained to minimize the negative log-likelihood of
training sequences:

min
θ

1
N

N
(cid:88)

Tn(cid:88)

n=1

t=1

− log p(xn

t | xn

1 , . . . , xn

t−1; θ)

(5)

where θ is a set of model parameters, {xn
t } is the
observed data. The conditional distributions in (5)
are modeled by the RNN. The hidden units are set to
gated recurrent units (Cho et al., 2014).

We consider the task of sequence modeling on four
diﬀerent polyphonic music sequences of piano, i.e.,
Piano-midi.de (Piano), Nottingham (Nott), MuseData
(Muse) and JSB chorales (JSB). Each of these datasets
are represented as a collection of 88-dimensional bi-
nary sequences, that span the whole range of piano
from A0 to C8.

The number of hidden units is set to 200. Each model
is trained for at most 100 epochs. According to the
experiments and their results on the validation set, we

Figure 3: Learning curves of diﬀerent algorithms on Piano
using RNN. (Left) training set. (Right) validation set.

Algorithms Piano. Nott. Muse.
7.20
7.56
7.22
7.69
10.08
7.19
8.13

Santa
Adam
RMSprop
SGD-M
SGD
HF(cid:5)
SGD-M(cid:5)

7.60
8.00
7.70
8.32
11.13
7.66
8.37

3.39
3.70
3.48
3.60
5.26
3.89
4.46

JSB.
8.46
8.51
8.52
8.59
10.81
8.58
8.71

Table 2: Test negative log-likelihood results on poly-
phonic music datasets using RNN. ((cid:5)) taken from
Boulanger-Lewandowski et al. (2012).

use a learning rate of 0.001 for all the algorithms. For
Santa, we consider an additional experiment using a
learning rate of 0.0002, denoted Santa-s. The anneal-
ing coeﬃcient γ is set to 0.5. Gradients are clipped
if the norm of the parameter vector exceeds 5. We
do not perform any dataset-speciﬁc tuning other than
early stopping on validation sets. Each update is done
using a minibatch of one sequence.

The best log-likelihood results on the test set are
achieved by using Santa, shown in Table 2. Learn-
ing curves on the Piano dataset are plotted in Fig-
ure 3. We observe that Santa achieves fast conver-
gence, but is overﬁtting. This is straightforwardly ad-
dressed through early stopping. The learning curves
for all the other datasets are provided in Supplemen-
tary Section G.

6 Conclusions

We propose Santa, an annealed SG-MCMC method
for stochastic optimization. Santa is able to explore
the parameter space eﬃciently and locate close to the
global optima by annealing. At the zero-temperature
limit, Santa gives a novel stochastic optimization algo-
rithm where both model parameters and momentum
are updated element-wise and adaptively. We provide
theory on the convergence of Santa to the global op-
tima for an (non-convex) objective function. Experi-
ments show best results on several deep models com-
pared to related stochastic optimization algorithms.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Acknowledgements

This research was supported in part by ARO, DARPA,
DOE, NGA, ONR and NSF.

References

C. Andrieu, N. de Freitas, and A. Doucet. Reversible
jump mcmc simulated annealing for neural net-
works. In UAI, 2000.

C. Blundell, J. Cornebise, K. Kavukcuoglu, and
D. Wierstra. Weight uncertainty in neural networks.
In ICML, 2015.

L. Bottou. Large-scale machine learning with stochas-
tic gradient descent. In Proc. COMPSTAT, 2010.

N. Boulanger-Lewandowski, Y. Bengio, and P. Vin-
cent. Modeling temporal dependencies in high-
dimensional sequences: Application to polyphonic
music generation and transcription. In ICML, 2012.

D. E. Carlson, E. Collins, Y.-P. Hsieh, L. Carin, and
V. Cevher. Preconditioned spectral descent for deep
learning. In Advances in Neural Information Pro-
cessing Systems, pages 2953–2961, 2015.

C. Chen, N. Ding, and L. Carin. On the convergence
of stochastic gradient mcmc algorithms with high-
order integrators. In NIPS, 2015.

T. Chen, E. B. Fox, and C. Guestrin. Stochastic gra-
dient Hamiltonian Monte Carlo. In ICML, 2014.

K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Ben-
gio.
Learning phrase representations using rnn
encoder-decoder for statistical machine translation.
In arXiv:1406.1078, 2014.

Y. N. Dauphin, H. de Vries, and Y. Bengio. Equili-
brated adaptive learning rates for non-convex opti-
mization. In NIPS, 2015.

N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel,
and H. Neven. Bayesian sampling using stochastic
gradient thermostats. In NIPS, 2014.

J. Duchi, E. Hazan, and Y. Singer. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In JMLR, 2011.

Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin.
Scalable deep Poisson factor analysis for topic mod-
eling. In ICML, 2015.

I. Goodfellow, D. Warde-farley, M. Mirza,
A. Courville, and Y. Bengio. Maxout networks. In
ICML, 2013.

K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. Le-
Cun. What is the best multi-stage architecture for
object recognition? In ICCV, 2009.

D. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015.

S. Kirkpatrick, C. D. G. Jr, and M. P. Vecchi. Opti-
mization by simulated annealing. In Science, 1983.

C. Li, C. Chen, D. Carlson, and L. Carin. Precon-
ditioned stochastic gradient Langevin dynamics for
deep neural networks. In AAAI, 2016a.

C. Li, C. Chen, K. Fan, and L. Carin. High-order
stochastic gradient thermostats for Bayesian learn-
ing of deep models. In AAAI, 2016b.

Y. Li, V. A. Protopopescu, N. Arnold, X. Zhang,
and A. Gorin. Hybrid parallel tempering and sim-
ulated annealing method. In Applied Mathematics
and Computation, 2009.

M. Lin, Q. Chen, and S. Yan. Network in network. In

ICLR, 2014.

J. C. Mattingly, A. M. Stuart, and M. V. Tretyakov.
Construction of numerical time-average and station-
ary measures via Poisson equations.
In SIAM J.
NUMER. ANAL., 2010.

R. M. Neal. Annealed importance sampling. In Statis-

tics and Computing, 2001.

R. M. Neal. Mcmc using hamiltonian dynamics.
Handbook of Markov Chain Monte Carlo, 2011.

In

F. Obermeyer, J. Glidden, and E. Jonas. Scaling
nonparametric bayesian inference via subsample-
annealing. In AISTATS, 2014.

S. Patterson and Y. W. Teh. Stochastic gradient Rie-
mannian Langevin dynamics on the probability sim-
plex. In NIPS, 2013.

H. Risken. The Fokker-Planck equation. Springer-

Verlag, New York, 1989.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
Learning representations by back-propagating er-
rors. In Nature, 1986.

E. Schechter. Handbook of Analysis and Its Founda-

tions. Elsevier, 1997.

S. Geman and D. Geman. Stochastic relaxation, gibbs
distributions, and the bayesian restoration of im-
ages. In PAMI, 1984.

I. Sutskever, J. Martens, G. Dahl, and G. E. Hinton.
On the importance of initialization and momentum
in deep learning. In ICML, 2013.

M. Girolami and B. Calderhead. Riemann manifold
Langevin and Hamiltonian Monte Carlo methods.
In JRSS, 2011.

Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Con-
sistency and ﬂuctuations for stochastic gradient
Langevin dynamics. In arXiv:1409.0578, 2014.

Bridging the Gap between SG-MCMC and Stochastic Optimization

T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop:
Divide the gradient by a running average of its re-
cent magnitude. In Coursera: Neural Networks for
Machine Learning, 2012.

J. W. van de Meent, B. Paige, and F. Wood. Temper-

ing by subsampling. In arXiv:1401.7145, 2014.
V. ˇCern´y. Thermodynamical approach to the travel-
ing salesman problem: An eﬃcient simulation algo-
rithm. In J. Optimization Theory and Applications,
1985.

S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh.
(Non-)asymptotic properties of stochastic gradient
Langevin dynamics. In arXiv:1501.00438, 2015.

M. Welling and Y. W. Teh. Bayesian learning via
In ICML,

stochastic gradient Langevin dynamics.
2011.

M. Zeiler and R. Fergus. Stochastic pooling for regu-
larization of deep convolutional neural networks. In
ICLR, 2013.

M. D. Zeiler. Adadelta: An adaptive learning rate

method. In arXiv:1212.5701, 2012.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Bridging the Gap between Stochastic Gradient MCMC and
Stochastic Optimization: Supplementary Material

Changyou Chen†

David Carlson‡

Zhe Gan†
†Department of Electrical and Computer Engineering, Duke University
‡Department of Statistics and Grossman Center for Statistics of Mind, Columbia University

Chunyuan Li†

Lawrence Carin†

A Solutions for the sub-SDEs

We provide analytic solutions for the split sub-SDEs
in Section 4.1. For stepsize h, the solutions are given
in (6).



θt = θt−1 + G1(θ) p h
pt = pt−1
Ξt = Ξt−1 +

Q − 1
β I

(cid:16)

(cid:17)

(6)

,

h

,

θt = θt−1
pt = exp (−Ξh) pt−1
Ξt = Ξt−1
θt = θt−1
pt = pt−1 +

(cid:16)

−G1(θ)∇θU (θ) + 1

β ∇θG1(θ)

+G1(θ)(Ξ − G2(θ))∇θG2(θ)) h
+ ( 2
2 (cid:12) ζt

β G2(θ)) 1

A :

B :

O :











Ξt = Ξt−1

B Proof of Lemma 1

For a general stochastic diﬀerential equation of the
form

d x = F (x)dt +

2D1/2(x)d w ,

(7)

√

where x ∈ RN , F : RN → RN , D : RM → RN ×P
are measurable functions with P , and w is standard
P -dimensional Brownian motion. (1) is a special case
of the general form (7) with

x = (θ, p, Ξ)


F (x) =

D(x) =









G1(θ) p

− G1(θ)∇θU (θ) − Ξ p + 1

β ∇θ G1(θ)
+ G1(θ)(Ξ − G2(θ))∇θ G2(θ)
Q − 1
β I


0
0
0

0

0
1
β G2(θ) 0
0

0



(8)






We write the joint distribution of x as
exp {−H(x)} (cid:44) 1
Z

ρ(x) =

1
Z

exp {−U (θ) − E(θ, p, Ξ)} .

A reformulation of the main theorem in Ding et al.
(2014) gives the following lemma, which is used to
prove Lemma 1 in the main text.

Lemma 4. The stochastic process of (cid:126)θ generated by
the stochastic diﬀerential equation (7) has the target
distribution pθ(θ) = 1
Z exp{−U (θ)} as its stationary
distribution, if ρ(x) satisﬁes the following marginaliza-
tion condition:

exp{−U (θ)} ∝

exp{−U (θ) − E(θ, p, Ξ)}d p dΞ ,

(cid:90)

(9)

and if the following condition is also satisﬁed:

∇ · (ρF ) = ∇∇(cid:62) : (ρD) ,

(10)

where ∇ (cid:44) (∂/∂θ, ∂/∂ p, ∂/Ξ), “·” represents the vec-
tor inner product operator, “:” represents a matrix
double dot product, i.e., X : Y (cid:44) tr(X(cid:62) Y).

Proof of Lemma 1. We ﬁrst have reformulated (1) us-
ing the general SDE form of (7), resulting in (8).
Lemma 1 states the joint distribution of (θ, p, Ξ) is

ρ(x) =

exp

−

p(cid:62) p −U (θ)

(cid:18)

1
2

(cid:110)
(Ξ − G2(θ))(cid:62) (Ξ − G2(θ))

tr

−

(cid:111)(cid:19)

,

(11)

1
Z
1
2

1

=

H(x)

with
(cid:110)
(Ξ − G2(θ))(cid:62) (Ξ − G2(θ))
1
2 tr
tion condition (9) is trivially satisﬁed, we are left to
verify condition (10). Substituting ρ(x) and F into
(10), we have the left-hand side

2 p(cid:62) p +U (θ)
. The marginaliza-

+

(cid:111)

Bridging the Gap between SG-MCMC and Stochastic Optimization

LHS =

(cid:88)

∂
∂ xi

(ρFi)

i

∂ρ
∂ xi
(cid:18) ∂Fi
∂ xi

Fi +

∂Fi
∂ xi

ρ

(cid:19)

Fi

ρ

−

∂H
∂ xi

(cid:88)

i
(cid:88)

i
(cid:32)

=

=

=

∇θi(G1)i: p −

diag(Ξ)

(cid:88)

i

(cid:88)

j



(cid:88)

i

i

(cid:88)

−

(cid:18)

− βpT

(cid:88)

−β

i

=

1
β

β

∇θi U −

(Ξij − (G2)ij)∇θi(G2)ij

 (G1 p)i

− G1 ∇θU − Ξ p +

∇θ G1 + G1(Ξ − G2)∇θ G2

1
β

(cid:18)

(Ξii − (G2)ii)

Qii −

(cid:19)(cid:33)

ρ

1
β

tr (cid:8)G2(p pT −I)(cid:9) ρ .

It is easy to see for the right-hand side

RHS =

(cid:88)

(cid:88)

1
β

j

(G2)ij

∂2
∂ xi ∂ xj

ρ

(cid:88)

(cid:88)

(G2)ij

∂
∂ pj

(cid:19)

(cid:18)

−

∂H
∂ pi

ρ

j

i
(cid:88)

(G2)ii

(cid:0)p2

i −1(cid:1) ρ

i
1
β

1
β

=

=

i
≡ LHS .

According to Lemma 4, the joint distribution (11) is
the equilibrium distribution of (1).

C Proof of Theorem 2

We start by proving the bias result of Theorem 2.

Proof of the bias. For our 2nd-order integrator, ac-
cording to the deﬁnition, we have:

E[ψ(Xt)] = ˜P l
(cid:17)

(cid:16)

I + h˜Lt

=

hψ(Xt−1) = eh ˜Ltψ(Xt−1) + O(h3)

ψ(Xt−1) +

2
˜L
t ψ(Xt−1) + O(h3) ,

h2
2

where Lt is the generator of the SDE for the t-th it-
eration, i.e., using stochastic gradient instead of the
full gradient, I is the identity map. Compared to the
prove of Chen et al. (2015), we need to consider the
approximation error for ∇θG1(θ). As a result, (12)
needs to be rewritten as:

(13)

E[ψ(Xt)]
(cid:16)

where Bt is from (3). Sum over t = 1, · · · , L in (13),
take expectation on both sides, and use the relation
˜Lt + Bt = Lβt +∆Vt to expand the ﬁrst order term.
We obtain



E[Lβtψ(Xt−1)] + h

E[∆Vtψ(Xt−1)]

E[ψ(Xt)] = ψ(X0) +

E[ψ(Xt)]

L−1
(cid:88)

t=1

L
(cid:88)

t=1

L
(cid:88)

t=1

+ h

L
(cid:88)

t=1

+

h2
2

L
(cid:88)

t=1

(cid:19)

2
E[˜L
t ψ(Xt−1)] + O(Lh3).

We divide both sides by Lh, use the Poisson equation
(4), and reorganize terms. We have:

(cid:0)φ(Xt) − ¯φβt

(cid:1)] =

E[Lβtψ(Xt−1)]

(E[ψ(Xt)] − ψ(X0)) −

E[∆Vtψ(Xt−1)]

1
L

1
L

L
(cid:88)

t=1
(cid:88)

t

2
E[˜L
t ψ(Xt−1)] + O(h2)

(14)

E[

1
L

(cid:88)

t

=

1
Lh

−

h
2L

L
(cid:88)

t=1

2
Now we try to bound ˜L
t . Based on ideas from Mat-
tingly et al. (2010), we apply the following procedure.
First replace ψ with ˜Ltψ from (13) to (14), and apply
the same logic for ˜Ltψ as for ψ in the above deriva-
tions, but this time expand in (13) up to the order
of O(h2), instead of the previous order O(h3). After
simpliﬁcation, we obtain:

E[˜L

2
t ψ(Xt−1)] = O

(cid:88)

t

(cid:19)

+ Lh

(cid:18) 1
h

(15)

(12)

Substituting (15) into (14), after simpliﬁcation, we
have: E (cid:0) 1

(cid:0)φ(Xt) − ¯φβt

(cid:1)(cid:1)

(cid:80)
t

L

=

1
Lh

(E[ψ(Xt)] − ψ(X0))
(cid:123)(cid:122)
(cid:125)
(cid:124)
C1

−

1
L

(cid:88)

t

− O

(cid:18) h
Lh

(cid:19)

+ h2

+ C3h2 ,

E[∆Vtψ(Xt−1)]

=

I + h(˜Lt + Bt)

ψ(Xt−1) +

(cid:17)

˜L

2
t ψ(Xt−1) + O(h3) ,

h2
2

for some C3 ≥ 0. According to the assumption, the
term C1 is bounded. As a result, collecting low order

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

terms, the bias can be expressed as:

lated terms, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
E ˆφ − ¯φ
(cid:12)
(cid:12)
(cid:32)

=

E

(cid:32)

≤

E

1
L

1
L

(cid:88)

t

(cid:88)

t
(cid:32)

≤Cφ(θ∗)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

C1
Lh

−

(cid:32)

≤Cφ(θ∗)

+

(cid:80)
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤Cφ(θ∗)

(cid:33)

(cid:88)

+

1
L

(cid:12)
(cid:12)
¯φβt − ¯φ
(cid:12)
(cid:12)
(cid:12)

(cid:0)φ(Xt) − ¯φβt

(cid:1)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)φ(Xt) − ¯φβt

(cid:1)

t

(cid:88)

t

(cid:33)

¯φβt − ¯φ

+

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
(cid:12)

(cid:32)

1
L

e−βt ˆU (θ)dθ

L
(cid:88)

(cid:90)

1
L
(cid:80)
t

t=1

θ(cid:54)=θ∗

E∆Vtψ(Xt−1)

+ C3h2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

L

L
(cid:88)

(cid:90)

1
L

θ(cid:54)=θ∗
t=1
E∆Vtψ(Xt−1)

L
L
(cid:88)

(cid:90)

(cid:32)

1
L

e−βt ˆU (θ)dθ

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

C1
Lh

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:12)

(cid:12)C3h2(cid:12)
(cid:12)

e−βt ˆU (θ)dθ

(cid:18) 1
Lh

+

t=1
(cid:80)

θ(cid:54)=θ∗
t (cid:107)E∆Vt(cid:107)
L

+ D

(cid:19)

+ h2

,

where the last equation follows from the ﬁniteness as-
sumption of ψ, (cid:107) · (cid:107) denotes the operator norm and
is bounded in the space of ψ due to the assumptions.
This completes the proof.

We will now prove the MSE result .

Proof of the MSE bound. Similar to the proof of The-
orem 2, for our 2nd–order integrator we have:

E (ψβt(Xt)) = (I + h(Lβt + ∆Vt)) ψβt−1 (Xt−1)

+

h2
2

˜L2

t ψβt−1(Xt−1) + O(h3) .

Sum over t from 1 to L + 1 and simplify, we have:

L
(cid:88)

t=1

E (ψβt(Xt)) =

ψβt−1(Xt−1)

L
(cid:88)

t=1

+ h

Lβtψβt−1(Xt−1) + h

∆Vtψβt−1 (Xt−1)

L
(cid:88)

t=1

˜L2

t ψβt−1 (Xt−1) + O(Lh3) .

L
(cid:88)

t=1

+

h2
2

L
(cid:88)

t=1

Substitute the Poisson equation (4) into the above
equation, divide both sides by Lh and rearrange re-

1
L

L
(cid:88)

t=1

−

−

1
Lh

L
(cid:88)

t=1

1
L

L
(cid:88)

t=1

(cid:0)φ(Xt) − ¯φβt

(cid:1) =

(EψβL(XLh) − ψβ0 (X0))

1
Lh

(cid:0)Eψβt−1 (Xt−1) − ψβt−1(Xt−1)(cid:1)

∆Vtψβt−1(Xt−1) −

˜L2

t ψβt−1(Xt−1) + O(h2)

h
2L

L
(cid:88)

t=1

Taking the square of both sides, it is then easy to see
there exists some positive constant C, such that

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

(cid:32)

L
(cid:88)

t=1

1
L


≤C




(cid:124)

(EψβL(XLh) − ψβ0(X0))2
L2h2
(cid:123)(cid:122)
A1

(cid:125)

(16)

L
(cid:88)

t=1

+

1
L2h2
(cid:124)

+

1
L2

L
(cid:88)

t=1

(cid:32) L
(cid:88)

t=1

+

h2
2L2
(cid:124)

(cid:0)Eψβt−1 (Xt−1) − ψβt−1(Xt−1)(cid:1)2

(cid:123)(cid:122)
A2

(cid:125)

∆V 2

t ψβt−1 (Xt−1)









(cid:33)2

(cid:125)

˜L2

t ψβt−1 (Xt−1)

+h4

(cid:123)(cid:122)
A3

A1 is easily bounded by the assumption that (cid:107)ψ(cid:107) ≤
V r0 < ∞. A2 is bounded because it can be shown that
E (ψβt(Xt)) − ψβt(Xt) ≤ C1
h + O(h) for C1 ≥ 0.
Intuitively this is true because the only diﬀerence be-
tween E (ψβt(Xt)) and ψβt(Xt) lies in the additional
Gaussian noise with variance h. A formal proof is
given in Chen et al. (2015). Furthermore, A3 is

√

Bridging the Gap between SG-MCMC and Stochastic Optimization

bounded by the following arguments:

D Proof of Corollary 3

t ψβt−1(Xt−1) − E ˜L2

t ψβt−1(Xt−1)

(cid:104) ˜L2

E

t ψβt−1 (Xt−1)

A3 =

(cid:32) L
(cid:88)

t=1

h2
2L2
(cid:124)

(cid:32) L
(cid:88)

E

(cid:16) ˜L2

t=1

+

h2
2L2
(cid:124)

(cid:123)(cid:122)
B1

(cid:123)(cid:122)
B2

(cid:46) B1 +

˜L2

t ψβt−1 (Xt−1)

(cid:32)

h2
Lh

L
(cid:88)

t=1

E ˜L2

t ψβt−1(Xt−1)

(cid:33)2
(cid:105)

(cid:125)

(cid:33)2

(cid:33)2

(cid:17)

(cid:33)2

(cid:17)

(cid:125)

Proof. The reﬁnement stage corresponds to β → ∞.
We can prove that in this case, the integration terms
in the bias and MSE in Theorem 2 converge to 0.

To show this, deﬁne a sequence of functions {gm} as:

gm (cid:44) −

1
L

L+m−1
(cid:88)

l=m

e−βl ˆU (θ) .

(18)

it is easy to see the sequence {gm} satisﬁes gm1 < gm2
for m1 < m2, and limm→∞ gm = 0. According to the
monotone convergence theorem, we have

(cid:19)

+

1
Lh

(cid:32)

h2
L

L
(cid:88)

t=1

(cid:33)

( ˜L2

t ψ(Xt−1))2

(cid:90)

lim
m→∞

gm (cid:44) lim
m→∞

(cid:90)

−

1
L

L+m−1
(cid:88)

l=m

e−βl ˆU (θ)dθ

(cid:90)

=

lim
m→∞

gm = 0 .

(cid:32)

+

h2
Lh

L
(cid:88)

(cid:16)

t=1

≤ O

+ O

= O

(cid:18) 1
2L2 + L2h2
(cid:19)

(cid:18) 1
L2h2 + h4
(cid:19)
(cid:18) 1
Lh

+ L4

As a result, the integration terms in the bounds for the
bias and MSE vanish, leaving only the terms stated in
Corollary 3. This completes the proof.

E Reformulation of the Santa

Algorithm

E (cid:107)∆Vt(cid:107)2
L

+

1
Lh

(cid:33)

+ h4

.

(17)

In this section we give a version of the Santa algorithm
that matches better than our actual implementation,
shown in Algorithm 3–7.

Collecting low order terms we have:

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

E

(cid:32)

1
L
(cid:32) 1
L

L
(cid:88)

t=1
(cid:80)
t

=O

Finally, we have:

L
(cid:88)

t=1
(cid:32)

1
L

(cid:16) ˆφ − ¯φ

(cid:17)2

E

< E

(cid:32)

1
L

(cid:88)

t

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

+ E

(cid:32)

1
L

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

≤Cφ(θ∗)2

(cid:32) 1
L

(cid:80)
t

+ O

L
(cid:88)

(cid:90)

θ(cid:54)=θ∗
t=1
E (cid:107)∆Vt(cid:107)2
L

≤Cφ(θ∗)2

L
(cid:88)

(cid:90)

(cid:32)

1
L

(cid:33)2

(cid:33)

(cid:33)2

e−βt ˆU (θ)dθ

+

1
Lh

+ h4

e−βt ˆU (θ)dθ

(cid:32) 1
L

(cid:80)
t

+ D

θ(cid:54)=θ∗
t=1
E (cid:107)∆Vt(cid:107)2
L

(cid:33)

+ h4

.

+

1
Lh

Algorithm 3: Santa
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t = ∇θ ˜Ut(θt−1) on the t-th minibatch ;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
if t < burnin then
/* exploration
*/
(θt, ut, αt) = Exploration S(θt−1, ut−1, αt−1)
or
(θt, ut, αt) = Exploration E(θt−1, ut−1, αt−1)

/* refinement
(θt, ut, αt) = Reﬁnement S(θt−1, ut−1, αt−1)
or
(θt, ut, αt) = Reﬁnement E(θt−1, ut−1, αt−1)

*/

else

end

end

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Algorithm 4: Exploration S (θt−1, ut−1, αt−1)
θt = θt−1 + gt (cid:12) ut−1 /2;
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt) /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη +
ut = exp (−αt/2) (cid:12) ut;
αt = αt + (ut (cid:12) ut −η/βt) /2;
θt = θt + gt (cid:12) ut /2;
Return (θt, ut, αt)

2 gt−1 η3/2/βt (cid:12) ζt;

(cid:113)

Algorithm 5: Reﬁnement S (θt−1, ut−1, αt−1)
αt = αt−1;
θt = θt−1 + gt (cid:12) ut−1 /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη;
ut = exp (−αt/2) (cid:12) ut;
θt = θt + gt (cid:12) ut /2;
Return (θt, ut, αt)

F Relationship of reﬁnement Santa to

Adam

In the Adam algorithm (see Algorithm 1 of Kingma
and Ba (2015)), the key steps are:

˜f t (cid:44) ∇θ ˜U (θt−1)
vt = σ vt−1 +(1 − σ)˜f t (cid:12) ˜f t

(cid:113)

√

vt

λ +

gt = 1 (cid:11)
˜ut = (1 − b1) (cid:12) ˜ut−1 + b1 (cid:12)˜f t
θt = θt + η(gt (cid:12) gt) (cid:12) ˜ut

Here, we maintain the square root form of gt, so
the square is equivalent to the preconditioner used in
Adam. As well, in Adam, the vector b1 is set to the
same constant between 0 and 1 for all entries. An
equivalent formulation of this is:
˜f t (cid:44) ∇θ ˜U (θt−1)
vt = σ vt−1 +(1 − σ)˜f t (cid:12) ˜f t

(cid:113)

√

λ +

gt = 1 (cid:11)
vt
ut = (1 − b1) (cid:12) ut−1 − η(gt (cid:12) b1 (cid:12)˜f t)
θt = θt − gt (cid:12)ut

The only diﬀerences between these steps and the Eu-
ler integrator we present in our Algorithm 1 are that
our b1 has a separate constant for each entry, and the
second term in u does not include the b1 in our for-
mulation. If we modify our algorithm to multiply the
gradient by b1, then our algorithm, under the same as-
sumptions as Adam, will have a similar regret bound
of O(

T ) for a convex problem.

√

Algorithm 6: Exploration E (θt−1, ut−1, αt−1)
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt);
(cid:113)
ut = (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t +
θt = θt + gt (cid:12) ut;
Return (θt, ut, αt)

2 gt−1 η3/2/βt (cid:12) ζt;

Algorithm 7: Reﬁnement E (θt−1, ut−1, αt−1)
αt = αt−1;
ut = (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t;
θt = θt + gt (cid:12) ut;
Return (θt, ut, αt)

Because the focus of this paper is not on the regret
bound, we only brieﬂy discuss the changes in the the-
ory. We note that Lemma 10.4 from Kingma and Ba
(2015) will hold with element-wise b1.
Lemma 5. Let γi (cid:44) b2
1,i√
σ . For b1,i, σ ∈ [0, 1) that
satisfy β2
< 1 and bounded ˜ft, || ˜ft||2 ≤ G, || ˜ft||∞ ≤
1√
β2
G∞, the following inequality holds

T
(cid:88)

t=1

u2
i
(cid:112)tg2

i

≤

2
1 − γi

|| ˜f1:T,i||2

which contains an element-dependent γi compared to
Adam.

Theorem 10.5 of Kingma and Ba (2015) will hold with
the same modiﬁcations and assumptions for a b with
distinct entries; the proof in Kingma and Ba (2015)
is already element-wise, so it suﬃces to replace their
global parameter γ with distinct γi (cid:44) b2
1,i√
σ . This will
√
give a regret of O(

T ), the same as Adam.

G Additional Results

Figure 4: MNIST using FNN with size of 800.

Bridging the Gap between SG-MCMC and Stochastic Optimization

Learning curves of diﬀerent algorithms on MNIST us-
ing FNN with size of 800 are plotted in Figure 4.
Learning curves of diﬀerent algorithms on four poly-
phonic music datasets using RNN are shown in Fig-
ure 6.

We additionally test Santa on the ImageNet dataset.
We use the GoogleNet architecture, which is a 22
layer deep model. We use the default setting de-
ﬁned in the Caﬀe package8. We were not able to
make other stochastic optimization algorithms except
SGD with momentum and the proposed Santa work
on this dataset. Figure 5 shows the comparison on
this dataset. We did not tune the parameter setting,
note the default setting is favourable by SGD with
momentum. Nevertheless, Santa still signiﬁcantly out-
performs SGD with momentum in term of convergence
speed.

Figure 5: Santa vs. SGD with momentum on Ima-
geNet. We used ImageNet11 for training.

8https : //github.com/cchangyou/Santa/tree/master/caf f e/models/bvlc googlenet

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Figure 6: Learning curves of diﬀerent algorithms on four polyphonic music datasets using RNN.

Bridging the Gap between Stochastic Gradient MCMC
and Stochastic Optimization

6
1
0
2
 
g
u
A
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
6
9
7
0
.
2
1
5
1
:
v
i
X
r
a

Changyou Chen†

David Carlson‡

Zhe Gan†
†Department of Electrical and Computer Engineering, Duke University
‡Department of Statistics and Grossman Center for Statistics of Mind, Columbia University

Chunyuan Li†

Lawrence Carin†

Abstract

Stochastic gradient Markov chain Monte
Carlo (SG-MCMC) methods are Bayesian
analogs to popular stochastic optimization
methods; however, this connection is not
well studied. We explore this relationship
by applying simulated annealing to an SG-
MCMC algorithm. Furthermore, we extend
recent SG-MCMC methods with two key
components: i) adaptive preconditioners (as
in ADAgrad or RMSprop), and ii) adaptive
element-wise momentum weights. The zero-
temperature limit gives a novel stochastic
optimization method with adaptive element-
wise momentum weights, while conventional
optimization methods only have a shared,
static momentum weight. Under certain as-
sumptions, our theoretical analysis suggests
the proposed simulated annealing approach
converges close to the global optima. Experi-
ments on several deep neural network models
show state-of-the-art results compared to re-
lated stochastic optimization algorithms.

1 Introduction

Machine learning has made signiﬁcant recent strides
due to large-scale learning applied to “big data”.
typically performed with
Large-scale learning is
stochastic optimization,
common
and the most
method is stochastic gradient descent (SGD) (Bottou,
2010). Stochastic optimization methods are devoted
to obtaining a (local) optima of an objective function.
Alternatively, Bayesian methods aim to compute the
expectation of a test function over the posterior dis-

Appearing in Proceedings of the 19th International Con-
ference on Artiﬁcial Intelligence and Statistics (AISTATS)
2016, Cadiz, Spain. JMLR: W&CP volume 41. Copyright
2016 by the authors.

tribution. At ﬁrst glance, these methods appear to be
distinct, independent approaches to learning. How-
ever, even the celebrated Gibbs sampler was ﬁrst in-
troduced to statistics as a simulated annealing method
for maximum a posteriori estimation (i.e., ﬁnding an
optima) (Geman and Geman, 1984).

Recent work on large-scale Bayesian learning has fo-
cused on incorporating the speed and low-memory
costs from stochastic optimization. These approaches
are referred to as stochastic gradient Markov chain
Monte Carlo (SG-MCMC) methods. Well-known SG-
MCMC methods include stochastic gradient Langevin
dynamics (SGLD) (Welling and Teh, 2011), stochastic
gradient Hamiltonian Monte Carlo (SGHMC) (Chen
et al., 2014), and stochastic gradient thermostats
(SGNHT) (Ding et al., 2014). SG-MCMC has become
increasingly popular in the literature due to practi-
cal successes, ease of implementation, and theoretical
convergence properties (Teh et al., 2014; Vollmer et al.,
2015; Chen et al., 2015).

There are obvious structural similarities between SG-
MCMC algorithms and stochastic optimization meth-
ods. For example, SGLD resembles SGD with additive
Gaussian noise. SGHMC resembles SGD with momen-
tum (Rumelhart et al., 1986), adding additive Gaus-
sian noise when updating the momentum terms (Chen
et al., 2014). These similarities are detailed in Section
2. Despite these structural similarities, the theory is
unclear on how additive Gaussian noise diﬀerentiates
a Bayesian algorithm from its optimization analog.

Just as classical sampling methods were originally used
for optimization (Geman and Geman, 1984), we di-
rectly address using SG-MCMC algorithms for opti-
mization. A major beneﬁt of adapting these schemes
is that Bayesian learning is (in theory) able to fully ex-
plore the parameter space. Thus it may ﬁnd a better
local optima, if not the global optima, for a non-convex
objective function.

Speciﬁcally, in this work we ﬁrst extend the recently
proposed multivariate stochastic gradient thermostat

Bridging the Gap between SG-MCMC and Stochastic Optimization

algorithm (Gan et al., 2015) with Riemannian infor-
mation geometry, which results in an adaptive pre-
conditioning and momentum scheme with analogs to
Adam (Kingma and Ba, 2015) and RMSprop (Tiele-
man and Hinton, 2012). We propose an annealing
scheme on the system temperature to move from a
Bayesian method to a stochastic optimization method.
We call the proposed algorithm Stochastic AnNeal-
ing Thermostats with Adaptive momentum (Santa).
We show that in the temperature limit, Santa recov-
ers the SGD with momentum algorithm except that: i)
adaptive preconditioners are used when updating both
model and momentum parameters; ii) each parame-
ter has an individual, learned momentum parameter.
Adaptive preconditioners and momentums are desir-
able in practice because of their ability to deal with
uneven, dynamic curvature (Dauphin et al., 2015). For
completeness, we ﬁrst review related algorithms in Sec-
tion 2, and present our novel algorithm in Section 3.

We develop theory to analyze convergence properties
of our algorithm, suggesting that Santa is able to ﬁnd
a solution for an (non-convex) objective function close
to its global optima, shown in Section 4. The theory is
based on the analysis from stochastic diﬀerential equa-
tions (Teh et al., 2014; Chen et al., 2015), and presents
results on bias and variance of the annealed Markov
chain. This is a fundamentally diﬀerent approach from
the traditional convergence explored in stochastic opti-
mization, or the regret bounds used in online optimiza-
tion. We note we can adapt the regret bound of Adam
(Kingma and Ba, 2015) for our zero-temperature al-
gorithm (with a few trivial modiﬁcations) for a con-
vex problem, as shown in Supplementary Section F.
However, this neither addresses non-convexity nor the
annealing scheme that our analysis does.

In addition to theory, we demonstrate eﬀective empir-
ical performance on a variety of deep neural networks
(DNNs), achieving the best performance compared to
all competing algorithms for the same model size. This
is shown in Section 5. The code is publicly available
at https://github.com/cchangyou/Santa.

2 Preliminaries

Throughout this paper, we denote vectors as bold,
lower-case letters, and matrices as bold, upper-case
letters. We use (cid:12) for element-wise multiplication, and
· denotes the element-
(cid:11) as element-wise division;
wise square root when applied to vectors or matricies.
We reserve (·)1/2 for the standard matrix square root.
Ip is the p × p identity matrix, 1 is an all-ones vector.

√

The goal of an optimization algorithm is to min-
imize an objective function U (θ) that corresponds
to a (non-convex) model of interest.
In a Bayesian
model, this corresponds to the potential energy de-

ﬁned as the negative log-posterior, U (θ) (cid:44) − log p(θ)−
(cid:80)N
n=1 log p(xn |θ). Here θ ∈ Rp are the model param-
eters, and {xn}n=1,...,N are the d-dimensional observed
data; p(θ) corresponds to the prior and p(xn |θ) is a
likelihood term for the nth observation. In optimiza-
tion, − (cid:80)N
n=1 log p(xn |θ) is typically referred to as the
loss function, and − log p(θ) as a regularizer.

In large-scale learning, N is prohibitively large.
This motivates the use of stochastic approximations.
We denote the stochastic approximation ˜Ut(θ) (cid:44)
− log p(θ) − N
j=1 log p(xij |θ), where (i1, · · · , im)
m
is a random subset of the set {1, 2, · · · , N }. The gra-
dient on this minibatch is denoted as ˜f t(θ) = ∇ ˜Ut(θ),
which is an unbiased estimate of the true gradient.

(cid:80)m

A standard approach to learning is SGD, where param-
˜f t−1(θ) with
eter updates are given by θt = θt−1 − ηt
ηt the learning rate. This is guaranteed to converge to
a local minima under mild conditions (Bottou, 2010).
The SG-MCMC analog to this is SGLD, with updates
2ηtζt. The additional term
θt = θt−1 − ηt
is a standard normal random vector, ζt ∼ N (0, Ip)
(Welling and Teh, 2011). The SGLD method draws
approximate posterior samples instead of obtaining a
local minima.

˜f t−1(θ) +

√

Using momentum in stochastic optimization is impor-
tant in learning deep models (Sutskever et al., 2013).
This motivates SG-MCMC algorithms with momen-
tum. The standard SGD with momentum (SGD-M)
approach introduces an auxiliary variable ut ∈ Rp
to represent the momentum. Given a momentum
weight α, the updates are θt = θt−1 + ηtut and
ut = (1 − α)ut−1 − ˜f t−1(θ). A Bayesian analog is
SGHMC (Chen et al., 2014) or multivariate SGNHT
(mSGNHT) (Gan et al., 2015).
In mSGNHT, each
parameter has a unique momentum weight αt ∈ Rp
that is learned during the sampling sequence. The
momentum weights are updated to maintain the sys-
tem temperature 1/β. An inverse temperature of
β = 1 corresponds to the posterior. This algorithm
has updates θt = θt−1 + ηtut, ut = (1 − ηtαt−1) (cid:12)
˜f t−1(θ) + (cid:112)2ηt/βζt. The main diﬀerence
ut−1 − ηt
is the additive Gaussian noise and step-size depen-
dent momentum update. The weights have updates
αt = αt−1 + ηt((ut (cid:12) ut) − 1/β), which matches the
kinetic energy to the system temperature.

A recent idea in stochastic optimization is to use
an adaptive preconditioner, also known as a variable
metric, to improve convergence rates. Both ADA-
grad (Duchi et al., 2011) and Adam (Kingma and Ba,
2015) adapt to the local geometry with a regret bound
N ). Adam adds momentum as well through
of O(
moment smoothing. RMSprop (Tieleman and Hin-
ton, 2012), Adadelta (Zeiler, 2012), and RMSspectral

√

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Algorithm 1: Santa with the Euler scheme
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t (cid:44) ∇θ ˜U (θt−1) on the tth mini-batch;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
if t < burnin then
/* exploration
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt);
(cid:1) (cid:11) ut−1 +
ut = η
βt

(cid:0)1 − gt−1 (cid:11) gt

(cid:113) 2η
βt

gt−1 (cid:12) ζt

*/

else

/* refinement
αt = αt−1; ut = 0;

end
ut = ut + (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t;
θt = θt−1 + gt (cid:12) ut;

end

*/

(Carlson et al., 2015) are similar methods with pre-
conditioners. Our method introduces adaptive mo-
mentum and preconditioners to the SG-MCMC. This
diﬀers from stochastic optimization in implementation
and theory, and is novel in SG-MCMC.
Simulated annealing (Kirkpatrick et al., 1983; ˇCern´y,
1985) is well-established as a way of acquiring a local
mode by moving from a high-temperature, ﬂat surface
to a low-temperature, peaky surface. It has been ex-
plored in the context of MCMC, including reversible
jump MCMC (Andrieu et al., 2000), annealed impor-
tant sampling (Neal, 2001) and parallel tempering (Li
et al., 2009). Traditional algorithms are based on
Metropolis–Hastings sampling, which require compu-
tationally expensive accept-reject steps. Recent work
has applied simulated annealing to large-scale learn-
ing through mini-batch based annealing (van de Meent
et al., 2014; Obermeyer et al., 2014). Our approach in-
corporates annealing into SG-MCMC with its inherent
speed and mini-batch nature.

3 The Santa Algorithm

Santa extends the mSGNHT algorithm with precon-
ditioners and a simulated annealing scheme. A simple
pseudocode is shown in Algorithm 1, or a more com-
plex, but higher accuracy version, is shown in Algo-
rithm 2, and we detail the steps below.

The ﬁrst extension we consider is the use of adaptive
preconditioners. Preconditioning has been proven crit-
ical for fast convergence in both stochastic optimiza-

tion (Dauphin et al., 2015) and SG-MCMC algorithms
(Patterson and Teh, 2013). In the MCMC literature,
preconditioning is alternatively referred to as Rieman-
nian information geometry (Patterson and Teh, 2013).
We denote the preconditioner as {Gt ∈ Rp×p}. A pop-
ular choice in SG-MCMC is the Fisher information ma-
trix (Girolami and Calderhead, 2011). Unfortunately,
this approach is computationally prohibitive for many
models of interest. To avoid this problem, we adopt
the preconditioner from RMSprop and Adam, which
uses a vector {gt ∈ Rp} to approximate the diagonal
of the Fisher information matrixes (Li et al., 2016a).
The construction sequentially updates the precondi-
tioner based on current and historical gradients with
a smoothing parameter σ, and is shown as part of Al-
gorithm 1. While this approach will not capture the
Riemannian geometry as eﬀectively as the Fisher in-
formation matrix, it is computationally eﬃcient.

Santa also introduces an annealing scheme on sys-
tem temperatures. As discussed in Section 2, mS-
GNHT naturally accounts for a varying temperature
by matching the particle momentum to the system
temperature. We introduce β = {β1, β2, · · · }, a se-
quence of inverse temperature variables with βi < βj
for i < j and limi→∞ βi = ∞. The inﬁnite case
corresponds to the zero-temperature limit, where SG-
MCMCs become deterministic optimization methods.

The annealing scheme leads to two stages:
the ex-
ploration and the reﬁnement stages. The exploration
stage updates all parameters based on an annealed se-
quence of stochastic dynamic systems (see Section 4
for more details). This stage is able to explore the
parameter space eﬃciently, escape poor local modes,
and ﬁnally converge close to the global mode. The re-
ﬁnement stage corresponds to the temperature limit,
i.e., βn → ∞. In the temperature limit, the momen-
tum weight updates vanish and it becomes a stochastic
optimization algorithm.

We propose two update schemes to solve the corre-
sponding stochastic diﬀerential equations: the Euler
scheme and the symmetric splitting scheme (SSS). The
Euler scheme has simpler updates, as detailed in Algo-
rithm 1; while SSS endows increased accuracy (Chen
et al., 2015) with a slight increase in overhead compu-
tation, as shown in Algorithm 2. Section 4.1 elaborates
on the details of these two schemes. We recommend
the use of SSS, but the Euler scheme is simpler to im-
plement and compare to known algorithms.

Practical considerations According to Section 4,
the exploration stage helps the algorithm traverse the
parameter space following the posterior curve as ac-
curate as possible. For optimization, slightly biased
samples do not aﬀect the ﬁnal solution. As a result,

Bridging the Gap between SG-MCMC and Stochastic Optimization

Algorithm 2: Santa with SSS
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t (cid:44) ∇θ ˜U (θt−1) on the tth mini-batch;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
θt = θt−1 + gt (cid:12) ut−1 /2;
if t < burnin then
/* exploration
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt) /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη + (cid:112)2 gt−1 η/βt (cid:12) ζt

*/

+ η/βt

(cid:0)1 − gt−1 (cid:11) gt
ut = exp (−αt/2) (cid:12) ut;
αt = αt + (ut (cid:12) ut −η/βt) /2;

(cid:1) (cid:11) ut−1;

else

/* refinement
αt = αt−1;
ut = ut − gt (cid:12)˜f tη; ut = exp (−αt/2) (cid:12) ut;

ut = exp (−αt/2) (cid:12) ut−1;

*/

end
θt = θt + gt (cid:12) ut /2;

end

the term consisting of (1 − gt−1 (cid:11) gt) in the algorithm
(which is an approximation term, see Section 4.1) is
ignored. We found no decreasing performance in our
experiments. Furthermore, the term gt−1 associated
with the Gaussian noise could be replaced with a ﬁxed
constant without aﬀecting the algorithm.

4 Theoretical Foundation

In this section we present the stochastic diﬀerential
equations (SDEs) that correspond to the Santa algo-
rithm. We ﬁrst introduce the general SDE framework,
then describe the exploration stage in Section 4.1 and
the reﬁnement stage in Section 4.2. We give the con-
vergence properties of the numerical scheme in Section
4.3. This theory uses tools from the SDE literature and
extends the mSGNHT theory (Ding et al., 2014; Gan
et al., 2015).

The SDEs are presented with re-parameterized p =
u /η1/2, Ξ = diag(α)/η1/2, as in Ding et al. (2014).
The SDEs describe the motion of a particle in a system
where θ is the location and p is the momentum.

In mSGNHT, the particle is driven by a force
−∇θ ˜Ut(θ) at time t. The stationary distribution of θ
corresponds to the model posterior (Gan et al., 2015).
Our critical extension is the use of Riemannian infor-

mation geometry, important for fast convergence (Pat-
terson and Teh, 2013). Given an inverse temperature
β, the system is described by the following SDEs1:






dθ = G1(θ) p dt
(cid:16)
d p =

−G1(θ)∇θU (θ) − Ξ p + 1

β ∇θG1(θ)

+G1(θ)(Ξ − G2(θ))∇θG2(θ)) dt + ( 2
dt ,

dΞ =

(cid:17)

(cid:16)

Q − 1
β I

β G2(θ))

1
2 dw

(1)

where Q = diag(p (cid:12) p), w is standard Brownian mo-
tion, G1(θ) encodes geometric information of the po-
tential energy U (θ), and G2(θ) characterizes the man-
ifold geometry of the Brownian motion. Note G2(θ)
may be the same as G1(θ) for the same Riemannian
manifold. We call G1(θ) and G2(θ) Riemannian met-
rics, which are commonly deﬁned by the Fisher infor-
mation matrix (Girolami and Calderhead, 2011). We
use the RMSprop preconditioner (with updates from
Algorithm 1) for computational feasibility. Using the
Fokker-Plank equation (Risken, 1989), we show that
the marginal stationary distribution of (1) corresponds
to the posterior distribution.
AT B
Lemma 1. Denote A : B (cid:44) tr
ary distribution of (1) is: pβ(θ, p, Ξ) ∝

.The station-

(cid:110)

(cid:111)

e−βU (θ)− β

2 pT p − β

2 (Ξ−G2(θ)):(Ξ−G2(θ)) .

(2)

An inverse temperature β = 1 corresponds to the stan-
dard Bayesian posterior.

We note that p in (1) has additional dependencies on
G1 and G2 compared to Gan et al. (2015) that must be
accounted for. Ξ p introduces friction into the system
so that the particle does not move too far away by
the random force; the terms ∇θG1(θ) and ∇θG2(θ)
penalize the inﬂuences of the Riemannian metrics so
that the stationary distribution remains invariant.

4.1 Exploration

The ﬁrst stage of Santa, exploration, explores the pa-
rameter space to obtain parameters near the global
mode of an objective function2. This approach applies
ideas from simulated annealing (Kirkpatrick et al.,
1983). Speciﬁcally, the inverse temperature β is slowly
annealed to temperature zero to freeze the particles at
the global mode.

Minimizing U (θ) is equivalent to sampling from the
zero-temperature limit pβ(θ) (cid:44) 1
e−βU (θ) (propor-
Zβ

1We abuse notation for conciseness. Here, ∇θ G(θ) is

a vector with the i-th element being (cid:80)

j ∇θj Gij(θ).

2This requires an ergodic algorithm. While ergodicity is
not straightforward to check, we follow most MCMC work
and assume it holds in our algorithm.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

tional to (2)), with Zβ being the normalization con-
stant such that pβ(θ) is a valid distribution. We
construct a Markov chain that sequentially transits
from high temperatures to low temperatures. At the
state equilibrium, the chain reaches the temperature
limit with marginal stationary distribution ρ0(θ) (cid:44)
limβ→∞ e−βU (θ), a point mass3 located at the global
mode of U (θ). Speciﬁcally, we ﬁrst deﬁne a sequence
of inverse temperatures, (β1, β2, · · · , βL), such that βL
is large enough4. For each time t, we generate a sam-
ple according to the SDE system (1) with temperature
1
, conditioned on the sample from the previous tem-
βt
. We call this procedure annealing ther-
perature,
mostats to denote the analog to simulated annealing.

1
βt−1

Generating approximate samples Generating
exact samples from (1) is infeasible for general mod-
els. One well-known numerical approach is the Euler
scheme in Algorithm 1. The Euler scheme is a 1st-
order method with relatively high approximation error
(Chen et al., 2015). We increase accuracy by imple-
menting the symmetric splitting scheme (SSS) (Chen
et al., 2015; Li et al., 2016b). The idea of SSS is to split
an infeasible SDE into several sub-SDEs, where each
sub-SDE is analytically solvable; approximate samples
are generated by sequentially evolving parameters via
these sub-SDEs. Speciﬁcally, in Santa, we split (1)
into the following three sub-SDEs:

A :

O :









dθ = G1(θ) p dt
d p = 0
(cid:16)
dΞ =

Q − 1
β I

(cid:17)

dt

, B :






dθ = 0
d p = −Ξ p dt
dΞ = 0

,

dθ = 0
(cid:16)
d p =
+G1(θ)(Ξ − G2(θ))∇θG2(θ)) dt + ( 2
dΞ = 0

−G1(θ)∇θU (θ) + 1

β ∇θG1(θ)

β G2(θ))

1
2 dw

We then update the sub-SDEs in order A-B-O-B-A
to generative approximate samples (Chen et al., 2015).
This uses half-steps h/2 on the A and B updates5,
and full steps h in the O update. This is analogous to
the leapfrog steps in Hamiltonian Monte Carlo (Neal,
2011). Update equations are given in the Supplemen-
tary Section A. The resulting parameters then serve
as an approximate sample from the posterior distribu-
tion with the inverse temperature of β. Replacing G1
and G2 with the RMSprop preconditioners gives Al-
gorithm 2. These updates require approximations to
∇θG1(θ) and ∇θG2(θ), addressed below.

3The sampler samples a uniform distribution over global
modes, or a point mass if the mode is unique. We assume
uniqueness and say point mass for clarity henceforth.

4Due to numerical issues, it is impossible to set βL to
inﬁnity; we thus assign a large enough value for it and
handle the inﬁnity case in the reﬁnement stage.
5As in Ding et al. (2014), we deﬁne h =
η.

√

Approximate calculation for ∇θ G1(θ) We pro-
pose a computationally eﬃcient approximation for
calculating the derivative vector ∇θ G1(θ) based
on the deﬁnition.
for the i-th ele-
ment of ∇θ G1(θ) at the t-th iteration, denoted as
(∇θ Gt

1(θ))i, it is approximated as:

Speciﬁcally,

(∇θ Gt

1(θ))i

A1≈

(cid:88)

(Gt

1(θ))ij − (Gt−1
θtj − θ(t−1)j

1

(θ))ij

j
(∆ Gt
1)ij
1(θ) pt−1)jh

(cid:88)

A2=

(Gt

j

(cid:88)

=

j

(∆ Gt
1)ij
1(θ) ut−1)j

(Gt

(cid:44) Gt

1 − Gt−1

where ∆ Gt
. Step A1 follows by the
1
deﬁnition of a derivative, and A2 by using the update
equation for θt, i.e., θt = θt−1+Gt
1(θ) pt−1 h. Accord-
ing to Taylor’s theory, the approximation error for the
∇θ G1(θ) is O(h), e.g.,

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

i

j

(∆ Gt
1)ij
1(θ) ut−1)j

(Gt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− (∇θ Gt

1(θ))i

≤ Bth ,

(3)

for some positive constant Bt. The approximation error
is negligible in term of convergence behaviors because
it can be absorbed into the stochastic gradients error.
Formal theoretical analysis on convergence behaviors
with this approximation is given in later sections. Us-
ing similar methods, ∇θGt
2(θ) is also approximately
calculated.

4.2 Reﬁnement

stage

reﬁnement

corresponds

zero-
The
temperature limit of the exploration stage, where Ξ
is learned. We show that in the limit Santa gives sig-
niﬁcantly simpliﬁed updates, leading to an stochastic
optimization algorithm similar to Adam or SGD-M.

to the

We assume that the Markov chain has reached its
equilibrium after the exploration stage.
In the zero-
temperature limit, some terms in the SDE (1) vanish.
First, as β → ∞, the term 1
β ∇θ G1(θ) and the vari-
ance term for the Brownian motion approach 0. As
well, the thermostat variable Ξ approaches G2(θ), so
the term G1(θ)(Ξ − G2(θ))∇θ G2(θ) vanishes. The
stationary distribution in (2) implies E Qii
i →
0, which makes the SDE for Ξ in (1) vanish. As a re-
sult, in the reﬁnement stage, only θ and p need to be
updated. The Euler scheme for this is shown in Algo-
rithm 1, and the symmetric splitting scheme is shown
in Algorithm 2.

(cid:44) E p2

Relation to stochastic optimization algorithms
In the reﬁnement stage Santa is a stochastic optimiza-
tion algorithm. This relation is easier seen with the
Euler scheme in Algorithm 1. Compared with SGD-M
(Rumelhart et al., 1986), Santa has both adaptive gra-
dient and adaptive momentum updates. Unlike Ada-
grad (Duchi et al., 2011) and RMSprop (Tieleman and

Bridging the Gap between SG-MCMC and Stochastic Optimization

Hinton, 2012), reﬁnement Santa is a momentum based
algorithm.

The recently proposed Adam algorithm (Kingma and
Ba, 2015) incorporates momentum and precondition-
ing in what is denoted as “adaptive moments.” We
show in Supplementary Section F that a constant step
size combined with a change of variables nearly recov-
ers the Adam algorithm with element-wise momentum
weights. For these reasons, Santa serves as a more gen-
eral stochastic optimization algorithm that extends all
current algorithms. As well, for a convex problem and
a few trivial algorithmic changes, the regret bound of
T ),
Adam holds for reﬁnement Santa, which is O(
as detailed in Supplementary Section F. However, our
analysis is focused on non-convex problems that do not
ﬁt in the regret bound formulation.

√

4.3 Convergence properties

Our convergence properties are based on the frame-
work of Chen et al. (2015). The proofs for all the-
orems are given in the Supplementary Material. We
focus on the exploration stage of the algorithm. Us-
ing the Monotone Convergence argument (Schechter,
1997), the reﬁnement stage convergence is obtained by
taking the temperature limit from the results of the ex-
ploration stage. We emphasize that our approach dif-
fers from conventional stochastic optimization or on-
line optimization approaches. Our convergence rate
is weaker than many stochastic optimization methods,
including SGD; however, our analysis applies to non-
convex problems, whereas traditionally convergence
rates only apply to convex problems.

The goal of Santa is to obtain θ∗ such that θ∗ =
argminθ U (θ). Let {θ1, · · · , θL} be a sequence of pa-
rameters collected from the algorithm. Deﬁne ˆU (cid:44)
t=1 U (θt) as the sample average, ¯U (cid:44) U (θ∗) the
1
L
global optima of U (θ).

(cid:80)L

As in Chen et al. (2015), we require certain assump-
tions on the potential energy U . To show these as-
sumptions, we ﬁrst deﬁne a functional ψt for each t
that solves the following Poisson equation:

Ltψt(θt) = U (θt) − ¯U ,

(4)

Lt is the generator of the SDE system (1) in the t-th
E[f (xt+h)]−f (xt)
iteration, deﬁned Ltf (xt) (cid:44) limh→0+
h
where xt (cid:44) (θt, pt, Ξt), f : R3p → R is a compactly
supported twice diﬀerentiable function. The solution
functional ψt(θt) characterizes the diﬀerence between
U (θt) and the global optima ¯U for every θt. As shown
in Mattingly et al. (2010), (4) typically possesses a
unique solution, which is at least as smooth as U under
the elliptic or hypoelliptic settings. We assume ψt is
bounded and smooth, as described below.

Assumption 1. ψt and its up to 3rd-order deriva-
tives, Dkψt, are bounded by a function V(θ, p, Ξ),
i.e., (cid:107)Dkψ(cid:107) ≤ CkV rk for k = (0, 1, 2, 3), Ck, rk >
0. Furthermore, the expectation of V is bounded:
EV r(θ, p, Ξ) < ∞, and V is smooth such that
supt
sups∈(0,1) V r (sx + (1 − s) y) ≤ C (V r (x) + V r (y)),
∀x ∈ R3p, y ∈ R3p, r ≤ max{2rk} for some C > 0.

(cid:17)

Let ∆U (θ) (cid:44) U (θ) − U (θ∗). Further deﬁne an opera-
(cid:16)
G1(θ)(∇θ ˜Ut − ∇θU ) + Bt
· ∇p for each
tor ∆Vt =
βt
t, where Bt is from (3). Theorem 2 depicts the close-
ness of ˆU to the global optima ¯U in term of bias and
mean square error (MSE) deﬁned below.
Theorem 2. Let (cid:107)·(cid:107) be the operator norm. Under
Assumption 1, the bias and MSE of the exploration
stage in Santa with respect to the global optima for L
steps with stepsize h is bounded, for some constants
C > 0 and D > 0, with:

Bias:

E ˆU − ¯U

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ Ce−U (θ∗)
(cid:12)
(cid:18) 1
Lh

+ D

L
(cid:88)

(cid:90)

1
L
(cid:80)

t=1

(cid:32)

+

MSE: E

(cid:16) ˆU − ¯U

(cid:17)2

≤ C 2e−2U (θ∗)

(cid:33)

e−βt∆U (θ)dθ

t (cid:107)E∆Vt(cid:107)
L

(cid:19)

+ h2

.

L
(cid:88)

(cid:90)

(cid:32)

1
L

t=1
E (cid:107)∆Vt(cid:107)2
L

(cid:33)2

e−βt∆U (θ)dθ

(cid:33)

+ h4

.

+

1
Lh

(cid:32) 1
L

(cid:80)
t

+ D2

Both bounds for the bias and MSE have two parts.
The ﬁrst part contains integration terms, which char-
acterizes the distance between the global optima,
e−U (θ∗), and the unnormalized annealing distribu-
tions, e−βtU (θ), decreasing to zero exponentially fast
with increasing β; the remaining part characterizes the
distance between the sample average and the anneal-
ing posterior average. This shares a similar form as in
general SG-MCMC algorithms (Chen et al., 2015), and
can be controlled to converge. Furthermore, the term
(cid:80)
in the bias vanishes as long as the sum of
the annealing sequence {βt} is ﬁnite6, indicating that
the gradient approximation for ∇θG1(θ) in Section 4.1
does not aﬀect the bias of the algorithm. Similar ar-
guments apply for the MSE bound.

t(cid:107)E∆Vt(cid:107)
L

(cid:80)L+m−1
l=m

To get convergence results right before the reﬁnement
stage, let a sequence of functions {gm} be deﬁned as
e−βl ˆU (θ); it is easy to see that {gm}
gm (cid:44) − 1
L
satisﬁes gm1 < gm2 for m1 < m2, and limm→∞ gm =
0. According to the Monotone Convergence Theorem
(Schechter, 1997), the bias and MSE in the limit exists,
leading to Corollary 3.
Corollary 3. Under Assumptions 1, the bias and
MSE of the reﬁnement stage in Santa with respect

6In practice we might not need to care about this con-
straint because a small bias in the exploration stage does
not aﬀect convergence of the reﬁnement stage.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

to the global optima for L steps with stepsize h are
bounded, for some constants D1 > 0, D2 > 0, as
(cid:18) 1
Lh
(cid:32) 1
L

(cid:12)
(cid:12)
(cid:12) ≤ D1

E ˆU − ¯U

+ h2

Bias:

(cid:80)
t

(cid:80)

(cid:17)2

+

(cid:19)

(cid:12)
(cid:12)
(cid:12)

MSE: E

(cid:16) ˆU − ¯U

t (cid:107)E∆Vt(cid:107)
L
E (cid:107)∆Vt(cid:107)2
L

+

1
Lh

≤ D2

+ h4

(cid:33)

Corollary 3 implies that in the reﬁnement stage, the
discrepancy between annealing distributions and the
global optima vanishes, leaving only errors from dis-
cretized simulations of the SDEs, similar to the result
of general SG-MCMC (Chen et al., 2015). We note
that after exploration, Santa becomes a pure stochas-
tic optimization algorithm, thus convergence results
in term of regret bounds can also be derived; refer to
Supplementary Section F for more details.

5 Experiments

5.1

Illustration

In order to demonstrate that Santa is able to achieve
the global mode of an objective function, we consider
the double-well potential (Ding et al., 2014),

U (θ) = (θ + 4)(θ + 1)(θ − 1)(θ − 3)/14 + 0.5 .

As shown in Figure 1 (left), the double-well potential
has two modes, located at θ = −3 and θ = 2, with the
global optima at θ = −3. We use a decreasing learning
rate ht = t−0.3/10, and the annealing sequence is set to
βt = t2. To make the optimization more challenging,
we initialize the parameter at θ0 = 4, close to the local
mode. The evolution of θ with respect to iterations is
shown in Figure 1(right). As can be seen, θ ﬁrst moves
to the local mode but quickly jumps out and moves
to the global mode in the exploration stage (ﬁrst half
iterations); in the reﬁnement stage, θ quickly converges
to the global mode and sticks to it afterwards.
In
contrast, RMSprop is trapped on the local optima, and
convergences slower than Santa at the beginning.

Figure 1: (Left) Double-well potential. (Right) The
evolution of θ using Santa and RMSprop algorithms.

Figure 2: Learning curves of diﬀerent algorithms on
MNIST. (Left) FNN with size of 400. (Right) CNN.

5.2 Feedforward neural networks

We ﬁrst test Santa on the Feedforward Neural Net-
work (FNN) with rectiﬁed linear units (ReLU). We
test two-layer models with network sizes 784-X-X-10,
where X is the number of hidden units for each layer;
100 epochs are used. For variants of Santa, we de-
note Santa-E as Santa with a Euler scheme illustrated
in Algorithm 1, Santa-r as Santa running only on the
reﬁnement stage, but with updates on α as in the ex-
ploration stage. We compare Santa with SGD, SGD-
M, RMSprop, Adam, SGD with dropout, SGLD and
Bayes by Backprop (Blundell et al., 2015). We use
a grid search to obtain good learning rates for each
algorithm, resulting in 4 × 10−6 for Santa, 5 × 10−4
for RMSprop, 10−3 for Adam, and 5 × 10−1 for SGD,
SGD-M and SGLD. We choose an annealing schedule
of βt = Atγ with A = 1 and γ selected from 0.1 to 1
with an interval of 0.1. For simplicity, the exploration
is set to take half of total iterations.

We test the algorithms on the standard MNIST
dataset, which contains 28 × 28 handwritten digital
images from 10 classes with 60, 000 training samples
and 10, 000 test samples. The network size (X-X) is
set to 400-400 and 800-800, and test classiﬁcation er-
rors are shown in Table 1. Santa show improved state-
of-the-art performance amongst all algorithms. The
Euler scheme shows a slight decrease in performance,
due to the integration error when solving the SDE.
Santa without exploration (i.e., Santa-r) still performs
relatively well. Learning curves are plotted in Fig-
ure 2, showing that Santa converges as fast as other
algorithms but to a better local optima7.

5.3 Convolution neural networks

We next test Santa on the Convolution Neural Net-
work (CNN). Following Jarrett et al. (2009), a stan-
dard network conﬁguration with 2 convolutional layers
followed by 2 fully-connected layers is adopted. Both
convolutional layers use 5 × 5 ﬁlter size with 32 and 64

7Learning curves of FNN with size of 800 are provided

in Supplementary Section G.

Bridging the Gap between SG-MCMC and Stochastic Optimization

Algorithms
Santa
Santa-E
Santa-r
Adam
RMSprop
SGD-M
SGD
SGLD
BPB(cid:5)
SGD, Dropout(cid:5)
Stoc. Pooling(cid:46)
NIN, Dropout◦
Maxout, Dropout(cid:63)

FNN-400 FNN-800
1.21%
1.41%
1.45%
1.53%
1.59%
1.66%
1.72%
1.64%
1.32%
1.51%
−
−
−

CNN
1.16% 0.47%
0.58%
1.27%
0.49%
1.40%
0.59%
1.47%
0.64%
1.43%
0.77%
1.72%
0.81%
1.47%
0.71%
1.41%
−
1.34%
−
1.33%
0.47%
−
0.47%
−
0.45%
−

Table 1: Test error on MNIST classiﬁcation using FNN
and CNN. ((cid:5)) taken from Blundell et al. (2015). ((cid:46)) taken
from Zeiler and Fergus (2013). (◦) taken from Lin et al.
(2014). ((cid:63)) taken from Goodfellow et al. (2013).

channels, respectively; 2 × 2 max pooling is used af-
ter each convolutional layer. The fully-connected lay-
ers have 200-200 hidden nodes with ReLU activation.
The same parameter setting and dataset as in the FNN
are used. The test errors are shown in Table 1, and
the corresponding learning curves are shown in Fig-
ure 2. Similar trends as in FNN are obtained. Santa
signiﬁcantly outperforms other algorithms with an er-
ror of 0.45%. This result is comparable or even better
than some recent state-of-the-art CNN-based systems,
which have much more complex architectures.

5.4 Recurrent neural networks

We test Santa on the Recurrent Neural Network
is
(RNN) for sequence modeling, where a model
trained to minimize the negative log-likelihood of
training sequences:

min
θ

1
N

N
(cid:88)

Tn(cid:88)

n=1

t=1

− log p(xn

t | xn

1 , . . . , xn

t−1; θ)

(5)

where θ is a set of model parameters, {xn
t } is the
observed data. The conditional distributions in (5)
are modeled by the RNN. The hidden units are set to
gated recurrent units (Cho et al., 2014).

We consider the task of sequence modeling on four
diﬀerent polyphonic music sequences of piano, i.e.,
Piano-midi.de (Piano), Nottingham (Nott), MuseData
(Muse) and JSB chorales (JSB). Each of these datasets
are represented as a collection of 88-dimensional bi-
nary sequences, that span the whole range of piano
from A0 to C8.

The number of hidden units is set to 200. Each model
is trained for at most 100 epochs. According to the
experiments and their results on the validation set, we

Figure 3: Learning curves of diﬀerent algorithms on Piano
using RNN. (Left) training set. (Right) validation set.

Algorithms Piano. Nott. Muse.
7.20
7.56
7.22
7.69
10.08
7.19
8.13

Santa
Adam
RMSprop
SGD-M
SGD
HF(cid:5)
SGD-M(cid:5)

7.60
8.00
7.70
8.32
11.13
7.66
8.37

3.39
3.70
3.48
3.60
5.26
3.89
4.46

JSB.
8.46
8.51
8.52
8.59
10.81
8.58
8.71

Table 2: Test negative log-likelihood results on poly-
phonic music datasets using RNN. ((cid:5)) taken from
Boulanger-Lewandowski et al. (2012).

use a learning rate of 0.001 for all the algorithms. For
Santa, we consider an additional experiment using a
learning rate of 0.0002, denoted Santa-s. The anneal-
ing coeﬃcient γ is set to 0.5. Gradients are clipped
if the norm of the parameter vector exceeds 5. We
do not perform any dataset-speciﬁc tuning other than
early stopping on validation sets. Each update is done
using a minibatch of one sequence.

The best log-likelihood results on the test set are
achieved by using Santa, shown in Table 2. Learn-
ing curves on the Piano dataset are plotted in Fig-
ure 3. We observe that Santa achieves fast conver-
gence, but is overﬁtting. This is straightforwardly ad-
dressed through early stopping. The learning curves
for all the other datasets are provided in Supplemen-
tary Section G.

6 Conclusions

We propose Santa, an annealed SG-MCMC method
for stochastic optimization. Santa is able to explore
the parameter space eﬃciently and locate close to the
global optima by annealing. At the zero-temperature
limit, Santa gives a novel stochastic optimization algo-
rithm where both model parameters and momentum
are updated element-wise and adaptively. We provide
theory on the convergence of Santa to the global op-
tima for an (non-convex) objective function. Experi-
ments show best results on several deep models com-
pared to related stochastic optimization algorithms.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Acknowledgements

This research was supported in part by ARO, DARPA,
DOE, NGA, ONR and NSF.

References

C. Andrieu, N. de Freitas, and A. Doucet. Reversible
jump mcmc simulated annealing for neural net-
works. In UAI, 2000.

C. Blundell, J. Cornebise, K. Kavukcuoglu, and
D. Wierstra. Weight uncertainty in neural networks.
In ICML, 2015.

L. Bottou. Large-scale machine learning with stochas-
tic gradient descent. In Proc. COMPSTAT, 2010.

N. Boulanger-Lewandowski, Y. Bengio, and P. Vin-
cent. Modeling temporal dependencies in high-
dimensional sequences: Application to polyphonic
music generation and transcription. In ICML, 2012.

D. E. Carlson, E. Collins, Y.-P. Hsieh, L. Carin, and
V. Cevher. Preconditioned spectral descent for deep
learning. In Advances in Neural Information Pro-
cessing Systems, pages 2953–2961, 2015.

C. Chen, N. Ding, and L. Carin. On the convergence
of stochastic gradient mcmc algorithms with high-
order integrators. In NIPS, 2015.

T. Chen, E. B. Fox, and C. Guestrin. Stochastic gra-
dient Hamiltonian Monte Carlo. In ICML, 2014.

K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Ben-
gio.
Learning phrase representations using rnn
encoder-decoder for statistical machine translation.
In arXiv:1406.1078, 2014.

Y. N. Dauphin, H. de Vries, and Y. Bengio. Equili-
brated adaptive learning rates for non-convex opti-
mization. In NIPS, 2015.

N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel,
and H. Neven. Bayesian sampling using stochastic
gradient thermostats. In NIPS, 2014.

J. Duchi, E. Hazan, and Y. Singer. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In JMLR, 2011.

Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin.
Scalable deep Poisson factor analysis for topic mod-
eling. In ICML, 2015.

I. Goodfellow, D. Warde-farley, M. Mirza,
A. Courville, and Y. Bengio. Maxout networks. In
ICML, 2013.

K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. Le-
Cun. What is the best multi-stage architecture for
object recognition? In ICCV, 2009.

D. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015.

S. Kirkpatrick, C. D. G. Jr, and M. P. Vecchi. Opti-
mization by simulated annealing. In Science, 1983.

C. Li, C. Chen, D. Carlson, and L. Carin. Precon-
ditioned stochastic gradient Langevin dynamics for
deep neural networks. In AAAI, 2016a.

C. Li, C. Chen, K. Fan, and L. Carin. High-order
stochastic gradient thermostats for Bayesian learn-
ing of deep models. In AAAI, 2016b.

Y. Li, V. A. Protopopescu, N. Arnold, X. Zhang,
and A. Gorin. Hybrid parallel tempering and sim-
ulated annealing method. In Applied Mathematics
and Computation, 2009.

M. Lin, Q. Chen, and S. Yan. Network in network. In

ICLR, 2014.

J. C. Mattingly, A. M. Stuart, and M. V. Tretyakov.
Construction of numerical time-average and station-
ary measures via Poisson equations.
In SIAM J.
NUMER. ANAL., 2010.

R. M. Neal. Annealed importance sampling. In Statis-

tics and Computing, 2001.

R. M. Neal. Mcmc using hamiltonian dynamics.
Handbook of Markov Chain Monte Carlo, 2011.

In

F. Obermeyer, J. Glidden, and E. Jonas. Scaling
nonparametric bayesian inference via subsample-
annealing. In AISTATS, 2014.

S. Patterson and Y. W. Teh. Stochastic gradient Rie-
mannian Langevin dynamics on the probability sim-
plex. In NIPS, 2013.

H. Risken. The Fokker-Planck equation. Springer-

Verlag, New York, 1989.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
Learning representations by back-propagating er-
rors. In Nature, 1986.

E. Schechter. Handbook of Analysis and Its Founda-

tions. Elsevier, 1997.

S. Geman and D. Geman. Stochastic relaxation, gibbs
distributions, and the bayesian restoration of im-
ages. In PAMI, 1984.

I. Sutskever, J. Martens, G. Dahl, and G. E. Hinton.
On the importance of initialization and momentum
in deep learning. In ICML, 2013.

M. Girolami and B. Calderhead. Riemann manifold
Langevin and Hamiltonian Monte Carlo methods.
In JRSS, 2011.

Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Con-
sistency and ﬂuctuations for stochastic gradient
Langevin dynamics. In arXiv:1409.0578, 2014.

Bridging the Gap between SG-MCMC and Stochastic Optimization

T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop:
Divide the gradient by a running average of its re-
cent magnitude. In Coursera: Neural Networks for
Machine Learning, 2012.

J. W. van de Meent, B. Paige, and F. Wood. Temper-

ing by subsampling. In arXiv:1401.7145, 2014.
V. ˇCern´y. Thermodynamical approach to the travel-
ing salesman problem: An eﬃcient simulation algo-
rithm. In J. Optimization Theory and Applications,
1985.

S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh.
(Non-)asymptotic properties of stochastic gradient
Langevin dynamics. In arXiv:1501.00438, 2015.

M. Welling and Y. W. Teh. Bayesian learning via
In ICML,

stochastic gradient Langevin dynamics.
2011.

M. Zeiler and R. Fergus. Stochastic pooling for regu-
larization of deep convolutional neural networks. In
ICLR, 2013.

M. D. Zeiler. Adadelta: An adaptive learning rate

method. In arXiv:1212.5701, 2012.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Bridging the Gap between Stochastic Gradient MCMC and
Stochastic Optimization: Supplementary Material

Changyou Chen†

David Carlson‡

Zhe Gan†
†Department of Electrical and Computer Engineering, Duke University
‡Department of Statistics and Grossman Center for Statistics of Mind, Columbia University

Chunyuan Li†

Lawrence Carin†

A Solutions for the sub-SDEs

We provide analytic solutions for the split sub-SDEs
in Section 4.1. For stepsize h, the solutions are given
in (6).



θt = θt−1 + G1(θ) p h
pt = pt−1
Ξt = Ξt−1 +

Q − 1
β I

(cid:16)

(cid:17)

(6)

,

h

,

θt = θt−1
pt = exp (−Ξh) pt−1
Ξt = Ξt−1
θt = θt−1
pt = pt−1 +

(cid:16)

−G1(θ)∇θU (θ) + 1

β ∇θG1(θ)

+G1(θ)(Ξ − G2(θ))∇θG2(θ)) h
+ ( 2
2 (cid:12) ζt

β G2(θ)) 1

A :

B :

O :











Ξt = Ξt−1

B Proof of Lemma 1

For a general stochastic diﬀerential equation of the
form

d x = F (x)dt +

2D1/2(x)d w ,

(7)

√

where x ∈ RN , F : RN → RN , D : RM → RN ×P
are measurable functions with P , and w is standard
P -dimensional Brownian motion. (1) is a special case
of the general form (7) with

x = (θ, p, Ξ)


F (x) =

D(x) =









G1(θ) p

− G1(θ)∇θU (θ) − Ξ p + 1

β ∇θ G1(θ)
+ G1(θ)(Ξ − G2(θ))∇θ G2(θ)
Q − 1
β I


0
0
0

0

0
1
β G2(θ) 0
0

0



(8)






We write the joint distribution of x as
exp {−H(x)} (cid:44) 1
Z

ρ(x) =

1
Z

exp {−U (θ) − E(θ, p, Ξ)} .

A reformulation of the main theorem in Ding et al.
(2014) gives the following lemma, which is used to
prove Lemma 1 in the main text.

Lemma 4. The stochastic process of (cid:126)θ generated by
the stochastic diﬀerential equation (7) has the target
distribution pθ(θ) = 1
Z exp{−U (θ)} as its stationary
distribution, if ρ(x) satisﬁes the following marginaliza-
tion condition:

exp{−U (θ)} ∝

exp{−U (θ) − E(θ, p, Ξ)}d p dΞ ,

(cid:90)

(9)

and if the following condition is also satisﬁed:

∇ · (ρF ) = ∇∇(cid:62) : (ρD) ,

(10)

where ∇ (cid:44) (∂/∂θ, ∂/∂ p, ∂/Ξ), “·” represents the vec-
tor inner product operator, “:” represents a matrix
double dot product, i.e., X : Y (cid:44) tr(X(cid:62) Y).

Proof of Lemma 1. We ﬁrst have reformulated (1) us-
ing the general SDE form of (7), resulting in (8).
Lemma 1 states the joint distribution of (θ, p, Ξ) is

ρ(x) =

exp

−

p(cid:62) p −U (θ)

(cid:18)

1
2

(cid:110)
(Ξ − G2(θ))(cid:62) (Ξ − G2(θ))

tr

−

(cid:111)(cid:19)

,

(11)

1
Z
1
2

1

=

H(x)

with
(cid:110)
(Ξ − G2(θ))(cid:62) (Ξ − G2(θ))
1
2 tr
tion condition (9) is trivially satisﬁed, we are left to
verify condition (10). Substituting ρ(x) and F into
(10), we have the left-hand side

2 p(cid:62) p +U (θ)
. The marginaliza-

+

(cid:111)

Bridging the Gap between SG-MCMC and Stochastic Optimization

LHS =

(cid:88)

∂
∂ xi

(ρFi)

i

∂ρ
∂ xi
(cid:18) ∂Fi
∂ xi

Fi +

∂Fi
∂ xi

ρ

(cid:19)

Fi

ρ

−

∂H
∂ xi

(cid:88)

i
(cid:88)

i
(cid:32)

=

=

=

∇θi(G1)i: p −

diag(Ξ)

(cid:88)

i

(cid:88)

j



(cid:88)

i

i

(cid:88)

−

(cid:18)

− βpT

(cid:88)

−β

i

=

1
β

β

∇θi U −

(Ξij − (G2)ij)∇θi(G2)ij

 (G1 p)i

− G1 ∇θU − Ξ p +

∇θ G1 + G1(Ξ − G2)∇θ G2

1
β

(cid:18)

(Ξii − (G2)ii)

Qii −

(cid:19)(cid:33)

ρ

1
β

tr (cid:8)G2(p pT −I)(cid:9) ρ .

It is easy to see for the right-hand side

RHS =

(cid:88)

(cid:88)

1
β

j

(G2)ij

∂2
∂ xi ∂ xj

ρ

(cid:88)

(cid:88)

(G2)ij

∂
∂ pj

(cid:19)

(cid:18)

−

∂H
∂ pi

ρ

j

i
(cid:88)

(G2)ii

(cid:0)p2

i −1(cid:1) ρ

i
1
β

1
β

=

=

i
≡ LHS .

According to Lemma 4, the joint distribution (11) is
the equilibrium distribution of (1).

C Proof of Theorem 2

We start by proving the bias result of Theorem 2.

Proof of the bias. For our 2nd-order integrator, ac-
cording to the deﬁnition, we have:

E[ψ(Xt)] = ˜P l
(cid:17)

(cid:16)

I + h˜Lt

=

hψ(Xt−1) = eh ˜Ltψ(Xt−1) + O(h3)

ψ(Xt−1) +

2
˜L
t ψ(Xt−1) + O(h3) ,

h2
2

where Lt is the generator of the SDE for the t-th it-
eration, i.e., using stochastic gradient instead of the
full gradient, I is the identity map. Compared to the
prove of Chen et al. (2015), we need to consider the
approximation error for ∇θG1(θ). As a result, (12)
needs to be rewritten as:

(13)

E[ψ(Xt)]
(cid:16)

where Bt is from (3). Sum over t = 1, · · · , L in (13),
take expectation on both sides, and use the relation
˜Lt + Bt = Lβt +∆Vt to expand the ﬁrst order term.
We obtain



E[Lβtψ(Xt−1)] + h

E[∆Vtψ(Xt−1)]

E[ψ(Xt)] = ψ(X0) +

E[ψ(Xt)]

L−1
(cid:88)

t=1

L
(cid:88)

t=1

L
(cid:88)

t=1

+ h

L
(cid:88)

t=1

+

h2
2

L
(cid:88)

t=1

(cid:19)

2
E[˜L
t ψ(Xt−1)] + O(Lh3).

We divide both sides by Lh, use the Poisson equation
(4), and reorganize terms. We have:

(cid:0)φ(Xt) − ¯φβt

(cid:1)] =

E[Lβtψ(Xt−1)]

(E[ψ(Xt)] − ψ(X0)) −

E[∆Vtψ(Xt−1)]

1
L

1
L

L
(cid:88)

t=1
(cid:88)

t

2
E[˜L
t ψ(Xt−1)] + O(h2)

(14)

E[

1
L

(cid:88)

t

=

1
Lh

−

h
2L

L
(cid:88)

t=1

2
Now we try to bound ˜L
t . Based on ideas from Mat-
tingly et al. (2010), we apply the following procedure.
First replace ψ with ˜Ltψ from (13) to (14), and apply
the same logic for ˜Ltψ as for ψ in the above deriva-
tions, but this time expand in (13) up to the order
of O(h2), instead of the previous order O(h3). After
simpliﬁcation, we obtain:

E[˜L

2
t ψ(Xt−1)] = O

(cid:88)

t

(cid:19)

+ Lh

(cid:18) 1
h

(15)

(12)

Substituting (15) into (14), after simpliﬁcation, we
have: E (cid:0) 1

(cid:0)φ(Xt) − ¯φβt

(cid:1)(cid:1)

(cid:80)
t

L

=

1
Lh

(E[ψ(Xt)] − ψ(X0))
(cid:123)(cid:122)
(cid:125)
(cid:124)
C1

−

1
L

(cid:88)

t

− O

(cid:18) h
Lh

(cid:19)

+ h2

+ C3h2 ,

E[∆Vtψ(Xt−1)]

=

I + h(˜Lt + Bt)

ψ(Xt−1) +

(cid:17)

˜L

2
t ψ(Xt−1) + O(h3) ,

h2
2

for some C3 ≥ 0. According to the assumption, the
term C1 is bounded. As a result, collecting low order

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

terms, the bias can be expressed as:

lated terms, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
E ˆφ − ¯φ
(cid:12)
(cid:12)
(cid:32)

=

E

(cid:32)

≤

E

1
L

1
L

(cid:88)

t

(cid:88)

t
(cid:32)

≤Cφ(θ∗)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

C1
Lh

−

(cid:32)

≤Cφ(θ∗)

+

(cid:80)
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤Cφ(θ∗)

(cid:33)

(cid:88)

+

1
L

(cid:12)
(cid:12)
¯φβt − ¯φ
(cid:12)
(cid:12)
(cid:12)

(cid:0)φ(Xt) − ¯φβt

(cid:1)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)φ(Xt) − ¯φβt

(cid:1)

t

(cid:88)

t

(cid:33)

¯φβt − ¯φ

+

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
(cid:12)

(cid:32)

1
L

e−βt ˆU (θ)dθ

L
(cid:88)

(cid:90)

1
L
(cid:80)
t

t=1

θ(cid:54)=θ∗

E∆Vtψ(Xt−1)

+ C3h2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

L

L
(cid:88)

(cid:90)

1
L

θ(cid:54)=θ∗
t=1
E∆Vtψ(Xt−1)

L
L
(cid:88)

(cid:90)

(cid:32)

1
L

e−βt ˆU (θ)dθ

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

C1
Lh

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:12)

(cid:12)C3h2(cid:12)
(cid:12)

e−βt ˆU (θ)dθ

(cid:18) 1
Lh

+

t=1
(cid:80)

θ(cid:54)=θ∗
t (cid:107)E∆Vt(cid:107)
L

+ D

(cid:19)

+ h2

,

where the last equation follows from the ﬁniteness as-
sumption of ψ, (cid:107) · (cid:107) denotes the operator norm and
is bounded in the space of ψ due to the assumptions.
This completes the proof.

We will now prove the MSE result .

Proof of the MSE bound. Similar to the proof of The-
orem 2, for our 2nd–order integrator we have:

E (ψβt(Xt)) = (I + h(Lβt + ∆Vt)) ψβt−1 (Xt−1)

+

h2
2

˜L2

t ψβt−1(Xt−1) + O(h3) .

Sum over t from 1 to L + 1 and simplify, we have:

L
(cid:88)

t=1

E (ψβt(Xt)) =

ψβt−1(Xt−1)

L
(cid:88)

t=1

+ h

Lβtψβt−1(Xt−1) + h

∆Vtψβt−1 (Xt−1)

L
(cid:88)

t=1

˜L2

t ψβt−1 (Xt−1) + O(Lh3) .

L
(cid:88)

t=1

+

h2
2

L
(cid:88)

t=1

Substitute the Poisson equation (4) into the above
equation, divide both sides by Lh and rearrange re-

1
L

L
(cid:88)

t=1

−

−

1
Lh

L
(cid:88)

t=1

1
L

L
(cid:88)

t=1

(cid:0)φ(Xt) − ¯φβt

(cid:1) =

(EψβL(XLh) − ψβ0 (X0))

1
Lh

(cid:0)Eψβt−1 (Xt−1) − ψβt−1(Xt−1)(cid:1)

∆Vtψβt−1(Xt−1) −

˜L2

t ψβt−1(Xt−1) + O(h2)

h
2L

L
(cid:88)

t=1

Taking the square of both sides, it is then easy to see
there exists some positive constant C, such that

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

(cid:32)

L
(cid:88)

t=1

1
L


≤C




(cid:124)

(EψβL(XLh) − ψβ0(X0))2
L2h2
(cid:123)(cid:122)
A1

(cid:125)

(16)

L
(cid:88)

t=1

+

1
L2h2
(cid:124)

+

1
L2

L
(cid:88)

t=1

(cid:32) L
(cid:88)

t=1

+

h2
2L2
(cid:124)

(cid:0)Eψβt−1 (Xt−1) − ψβt−1(Xt−1)(cid:1)2

(cid:123)(cid:122)
A2

(cid:125)

∆V 2

t ψβt−1 (Xt−1)









(cid:33)2

(cid:125)

˜L2

t ψβt−1 (Xt−1)

+h4

(cid:123)(cid:122)
A3

A1 is easily bounded by the assumption that (cid:107)ψ(cid:107) ≤
V r0 < ∞. A2 is bounded because it can be shown that
E (ψβt(Xt)) − ψβt(Xt) ≤ C1
h + O(h) for C1 ≥ 0.
Intuitively this is true because the only diﬀerence be-
tween E (ψβt(Xt)) and ψβt(Xt) lies in the additional
Gaussian noise with variance h. A formal proof is
given in Chen et al. (2015). Furthermore, A3 is

√

Bridging the Gap between SG-MCMC and Stochastic Optimization

bounded by the following arguments:

D Proof of Corollary 3

t ψβt−1(Xt−1) − E ˜L2

t ψβt−1(Xt−1)

(cid:104) ˜L2

E

t ψβt−1 (Xt−1)

A3 =

(cid:32) L
(cid:88)

t=1

h2
2L2
(cid:124)

(cid:32) L
(cid:88)

E

(cid:16) ˜L2

t=1

+

h2
2L2
(cid:124)

(cid:123)(cid:122)
B1

(cid:123)(cid:122)
B2

(cid:46) B1 +

˜L2

t ψβt−1 (Xt−1)

(cid:32)

h2
Lh

L
(cid:88)

t=1

E ˜L2

t ψβt−1(Xt−1)

(cid:33)2
(cid:105)

(cid:125)

(cid:33)2

(cid:33)2

(cid:17)

(cid:33)2

(cid:17)

(cid:125)

Proof. The reﬁnement stage corresponds to β → ∞.
We can prove that in this case, the integration terms
in the bias and MSE in Theorem 2 converge to 0.

To show this, deﬁne a sequence of functions {gm} as:

gm (cid:44) −

1
L

L+m−1
(cid:88)

l=m

e−βl ˆU (θ) .

(18)

it is easy to see the sequence {gm} satisﬁes gm1 < gm2
for m1 < m2, and limm→∞ gm = 0. According to the
monotone convergence theorem, we have

(cid:19)

+

1
Lh

(cid:32)

h2
L

L
(cid:88)

t=1

(cid:33)

( ˜L2

t ψ(Xt−1))2

(cid:90)

lim
m→∞

gm (cid:44) lim
m→∞

(cid:90)

−

1
L

L+m−1
(cid:88)

l=m

e−βl ˆU (θ)dθ

(cid:90)

=

lim
m→∞

gm = 0 .

(cid:32)

+

h2
Lh

L
(cid:88)

(cid:16)

t=1

≤ O

+ O

= O

(cid:18) 1
2L2 + L2h2
(cid:19)

(cid:18) 1
L2h2 + h4
(cid:19)
(cid:18) 1
Lh

+ L4

As a result, the integration terms in the bounds for the
bias and MSE vanish, leaving only the terms stated in
Corollary 3. This completes the proof.

E Reformulation of the Santa

Algorithm

E (cid:107)∆Vt(cid:107)2
L

+

1
Lh

(cid:33)

+ h4

.

(17)

In this section we give a version of the Santa algorithm
that matches better than our actual implementation,
shown in Algorithm 3–7.

Collecting low order terms we have:

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

E

(cid:32)

1
L
(cid:32) 1
L

L
(cid:88)

t=1
(cid:80)
t

=O

Finally, we have:

L
(cid:88)

t=1
(cid:32)

1
L

(cid:16) ˆφ − ¯φ

(cid:17)2

E

< E

(cid:32)

1
L

(cid:88)

t

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

+ E

(cid:32)

1
L

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

≤Cφ(θ∗)2

(cid:32) 1
L

(cid:80)
t

+ O

L
(cid:88)

(cid:90)

θ(cid:54)=θ∗
t=1
E (cid:107)∆Vt(cid:107)2
L

≤Cφ(θ∗)2

L
(cid:88)

(cid:90)

(cid:32)

1
L

(cid:33)2

(cid:33)

(cid:33)2

e−βt ˆU (θ)dθ

+

1
Lh

+ h4

e−βt ˆU (θ)dθ

(cid:32) 1
L

(cid:80)
t

+ D

θ(cid:54)=θ∗
t=1
E (cid:107)∆Vt(cid:107)2
L

(cid:33)

+ h4

.

+

1
Lh

Algorithm 3: Santa
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t = ∇θ ˜Ut(θt−1) on the t-th minibatch ;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
if t < burnin then
/* exploration
*/
(θt, ut, αt) = Exploration S(θt−1, ut−1, αt−1)
or
(θt, ut, αt) = Exploration E(θt−1, ut−1, αt−1)

/* refinement
(θt, ut, αt) = Reﬁnement S(θt−1, ut−1, αt−1)
or
(θt, ut, αt) = Reﬁnement E(θt−1, ut−1, αt−1)

*/

else

end

end

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Algorithm 4: Exploration S (θt−1, ut−1, αt−1)
θt = θt−1 + gt (cid:12) ut−1 /2;
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt) /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη +
ut = exp (−αt/2) (cid:12) ut;
αt = αt + (ut (cid:12) ut −η/βt) /2;
θt = θt + gt (cid:12) ut /2;
Return (θt, ut, αt)

2 gt−1 η3/2/βt (cid:12) ζt;

(cid:113)

Algorithm 5: Reﬁnement S (θt−1, ut−1, αt−1)
αt = αt−1;
θt = θt−1 + gt (cid:12) ut−1 /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη;
ut = exp (−αt/2) (cid:12) ut;
θt = θt + gt (cid:12) ut /2;
Return (θt, ut, αt)

F Relationship of reﬁnement Santa to

Adam

In the Adam algorithm (see Algorithm 1 of Kingma
and Ba (2015)), the key steps are:

˜f t (cid:44) ∇θ ˜U (θt−1)
vt = σ vt−1 +(1 − σ)˜f t (cid:12) ˜f t

(cid:113)

√

vt

λ +

gt = 1 (cid:11)
˜ut = (1 − b1) (cid:12) ˜ut−1 + b1 (cid:12)˜f t
θt = θt + η(gt (cid:12) gt) (cid:12) ˜ut

Here, we maintain the square root form of gt, so
the square is equivalent to the preconditioner used in
Adam. As well, in Adam, the vector b1 is set to the
same constant between 0 and 1 for all entries. An
equivalent formulation of this is:
˜f t (cid:44) ∇θ ˜U (θt−1)
vt = σ vt−1 +(1 − σ)˜f t (cid:12) ˜f t

(cid:113)

√

λ +

gt = 1 (cid:11)
vt
ut = (1 − b1) (cid:12) ut−1 − η(gt (cid:12) b1 (cid:12)˜f t)
θt = θt − gt (cid:12)ut

The only diﬀerences between these steps and the Eu-
ler integrator we present in our Algorithm 1 are that
our b1 has a separate constant for each entry, and the
second term in u does not include the b1 in our for-
mulation. If we modify our algorithm to multiply the
gradient by b1, then our algorithm, under the same as-
sumptions as Adam, will have a similar regret bound
of O(

T ) for a convex problem.

√

Algorithm 6: Exploration E (θt−1, ut−1, αt−1)
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt);
(cid:113)
ut = (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t +
θt = θt + gt (cid:12) ut;
Return (θt, ut, αt)

2 gt−1 η3/2/βt (cid:12) ζt;

Algorithm 7: Reﬁnement E (θt−1, ut−1, αt−1)
αt = αt−1;
ut = (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t;
θt = θt + gt (cid:12) ut;
Return (θt, ut, αt)

Because the focus of this paper is not on the regret
bound, we only brieﬂy discuss the changes in the the-
ory. We note that Lemma 10.4 from Kingma and Ba
(2015) will hold with element-wise b1.
Lemma 5. Let γi (cid:44) b2
1,i√
σ . For b1,i, σ ∈ [0, 1) that
satisfy β2
< 1 and bounded ˜ft, || ˜ft||2 ≤ G, || ˜ft||∞ ≤
1√
β2
G∞, the following inequality holds

T
(cid:88)

t=1

u2
i
(cid:112)tg2

i

≤

2
1 − γi

|| ˜f1:T,i||2

which contains an element-dependent γi compared to
Adam.

Theorem 10.5 of Kingma and Ba (2015) will hold with
the same modiﬁcations and assumptions for a b with
distinct entries; the proof in Kingma and Ba (2015)
is already element-wise, so it suﬃces to replace their
global parameter γ with distinct γi (cid:44) b2
1,i√
σ . This will
√
give a regret of O(

T ), the same as Adam.

G Additional Results

Figure 4: MNIST using FNN with size of 800.

Bridging the Gap between SG-MCMC and Stochastic Optimization

Learning curves of diﬀerent algorithms on MNIST us-
ing FNN with size of 800 are plotted in Figure 4.
Learning curves of diﬀerent algorithms on four poly-
phonic music datasets using RNN are shown in Fig-
ure 6.

We additionally test Santa on the ImageNet dataset.
We use the GoogleNet architecture, which is a 22
layer deep model. We use the default setting de-
ﬁned in the Caﬀe package8. We were not able to
make other stochastic optimization algorithms except
SGD with momentum and the proposed Santa work
on this dataset. Figure 5 shows the comparison on
this dataset. We did not tune the parameter setting,
note the default setting is favourable by SGD with
momentum. Nevertheless, Santa still signiﬁcantly out-
performs SGD with momentum in term of convergence
speed.

Figure 5: Santa vs. SGD with momentum on Ima-
geNet. We used ImageNet11 for training.

8https : //github.com/cchangyou/Santa/tree/master/caf f e/models/bvlc googlenet

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Figure 6: Learning curves of diﬀerent algorithms on four polyphonic music datasets using RNN.

Bridging the Gap between Stochastic Gradient MCMC
and Stochastic Optimization

6
1
0
2
 
g
u
A
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
6
9
7
0
.
2
1
5
1
:
v
i
X
r
a

Changyou Chen†

David Carlson‡

Zhe Gan†
†Department of Electrical and Computer Engineering, Duke University
‡Department of Statistics and Grossman Center for Statistics of Mind, Columbia University

Chunyuan Li†

Lawrence Carin†

Abstract

Stochastic gradient Markov chain Monte
Carlo (SG-MCMC) methods are Bayesian
analogs to popular stochastic optimization
methods; however, this connection is not
well studied. We explore this relationship
by applying simulated annealing to an SG-
MCMC algorithm. Furthermore, we extend
recent SG-MCMC methods with two key
components: i) adaptive preconditioners (as
in ADAgrad or RMSprop), and ii) adaptive
element-wise momentum weights. The zero-
temperature limit gives a novel stochastic
optimization method with adaptive element-
wise momentum weights, while conventional
optimization methods only have a shared,
static momentum weight. Under certain as-
sumptions, our theoretical analysis suggests
the proposed simulated annealing approach
converges close to the global optima. Experi-
ments on several deep neural network models
show state-of-the-art results compared to re-
lated stochastic optimization algorithms.

1 Introduction

Machine learning has made signiﬁcant recent strides
due to large-scale learning applied to “big data”.
typically performed with
Large-scale learning is
stochastic optimization,
common
and the most
method is stochastic gradient descent (SGD) (Bottou,
2010). Stochastic optimization methods are devoted
to obtaining a (local) optima of an objective function.
Alternatively, Bayesian methods aim to compute the
expectation of a test function over the posterior dis-

Appearing in Proceedings of the 19th International Con-
ference on Artiﬁcial Intelligence and Statistics (AISTATS)
2016, Cadiz, Spain. JMLR: W&CP volume 41. Copyright
2016 by the authors.

tribution. At ﬁrst glance, these methods appear to be
distinct, independent approaches to learning. How-
ever, even the celebrated Gibbs sampler was ﬁrst in-
troduced to statistics as a simulated annealing method
for maximum a posteriori estimation (i.e., ﬁnding an
optima) (Geman and Geman, 1984).

Recent work on large-scale Bayesian learning has fo-
cused on incorporating the speed and low-memory
costs from stochastic optimization. These approaches
are referred to as stochastic gradient Markov chain
Monte Carlo (SG-MCMC) methods. Well-known SG-
MCMC methods include stochastic gradient Langevin
dynamics (SGLD) (Welling and Teh, 2011), stochastic
gradient Hamiltonian Monte Carlo (SGHMC) (Chen
et al., 2014), and stochastic gradient thermostats
(SGNHT) (Ding et al., 2014). SG-MCMC has become
increasingly popular in the literature due to practi-
cal successes, ease of implementation, and theoretical
convergence properties (Teh et al., 2014; Vollmer et al.,
2015; Chen et al., 2015).

There are obvious structural similarities between SG-
MCMC algorithms and stochastic optimization meth-
ods. For example, SGLD resembles SGD with additive
Gaussian noise. SGHMC resembles SGD with momen-
tum (Rumelhart et al., 1986), adding additive Gaus-
sian noise when updating the momentum terms (Chen
et al., 2014). These similarities are detailed in Section
2. Despite these structural similarities, the theory is
unclear on how additive Gaussian noise diﬀerentiates
a Bayesian algorithm from its optimization analog.

Just as classical sampling methods were originally used
for optimization (Geman and Geman, 1984), we di-
rectly address using SG-MCMC algorithms for opti-
mization. A major beneﬁt of adapting these schemes
is that Bayesian learning is (in theory) able to fully ex-
plore the parameter space. Thus it may ﬁnd a better
local optima, if not the global optima, for a non-convex
objective function.

Speciﬁcally, in this work we ﬁrst extend the recently
proposed multivariate stochastic gradient thermostat

Bridging the Gap between SG-MCMC and Stochastic Optimization

algorithm (Gan et al., 2015) with Riemannian infor-
mation geometry, which results in an adaptive pre-
conditioning and momentum scheme with analogs to
Adam (Kingma and Ba, 2015) and RMSprop (Tiele-
man and Hinton, 2012). We propose an annealing
scheme on the system temperature to move from a
Bayesian method to a stochastic optimization method.
We call the proposed algorithm Stochastic AnNeal-
ing Thermostats with Adaptive momentum (Santa).
We show that in the temperature limit, Santa recov-
ers the SGD with momentum algorithm except that: i)
adaptive preconditioners are used when updating both
model and momentum parameters; ii) each parame-
ter has an individual, learned momentum parameter.
Adaptive preconditioners and momentums are desir-
able in practice because of their ability to deal with
uneven, dynamic curvature (Dauphin et al., 2015). For
completeness, we ﬁrst review related algorithms in Sec-
tion 2, and present our novel algorithm in Section 3.

We develop theory to analyze convergence properties
of our algorithm, suggesting that Santa is able to ﬁnd
a solution for an (non-convex) objective function close
to its global optima, shown in Section 4. The theory is
based on the analysis from stochastic diﬀerential equa-
tions (Teh et al., 2014; Chen et al., 2015), and presents
results on bias and variance of the annealed Markov
chain. This is a fundamentally diﬀerent approach from
the traditional convergence explored in stochastic opti-
mization, or the regret bounds used in online optimiza-
tion. We note we can adapt the regret bound of Adam
(Kingma and Ba, 2015) for our zero-temperature al-
gorithm (with a few trivial modiﬁcations) for a con-
vex problem, as shown in Supplementary Section F.
However, this neither addresses non-convexity nor the
annealing scheme that our analysis does.

In addition to theory, we demonstrate eﬀective empir-
ical performance on a variety of deep neural networks
(DNNs), achieving the best performance compared to
all competing algorithms for the same model size. This
is shown in Section 5. The code is publicly available
at https://github.com/cchangyou/Santa.

2 Preliminaries

Throughout this paper, we denote vectors as bold,
lower-case letters, and matrices as bold, upper-case
letters. We use (cid:12) for element-wise multiplication, and
· denotes the element-
(cid:11) as element-wise division;
wise square root when applied to vectors or matricies.
We reserve (·)1/2 for the standard matrix square root.
Ip is the p × p identity matrix, 1 is an all-ones vector.

√

The goal of an optimization algorithm is to min-
imize an objective function U (θ) that corresponds
to a (non-convex) model of interest.
In a Bayesian
model, this corresponds to the potential energy de-

ﬁned as the negative log-posterior, U (θ) (cid:44) − log p(θ)−
(cid:80)N
n=1 log p(xn |θ). Here θ ∈ Rp are the model param-
eters, and {xn}n=1,...,N are the d-dimensional observed
data; p(θ) corresponds to the prior and p(xn |θ) is a
likelihood term for the nth observation. In optimiza-
tion, − (cid:80)N
n=1 log p(xn |θ) is typically referred to as the
loss function, and − log p(θ) as a regularizer.

In large-scale learning, N is prohibitively large.
This motivates the use of stochastic approximations.
We denote the stochastic approximation ˜Ut(θ) (cid:44)
− log p(θ) − N
j=1 log p(xij |θ), where (i1, · · · , im)
m
is a random subset of the set {1, 2, · · · , N }. The gra-
dient on this minibatch is denoted as ˜f t(θ) = ∇ ˜Ut(θ),
which is an unbiased estimate of the true gradient.

(cid:80)m

A standard approach to learning is SGD, where param-
˜f t−1(θ) with
eter updates are given by θt = θt−1 − ηt
ηt the learning rate. This is guaranteed to converge to
a local minima under mild conditions (Bottou, 2010).
The SG-MCMC analog to this is SGLD, with updates
2ηtζt. The additional term
θt = θt−1 − ηt
is a standard normal random vector, ζt ∼ N (0, Ip)
(Welling and Teh, 2011). The SGLD method draws
approximate posterior samples instead of obtaining a
local minima.

˜f t−1(θ) +

√

Using momentum in stochastic optimization is impor-
tant in learning deep models (Sutskever et al., 2013).
This motivates SG-MCMC algorithms with momen-
tum. The standard SGD with momentum (SGD-M)
approach introduces an auxiliary variable ut ∈ Rp
to represent the momentum. Given a momentum
weight α, the updates are θt = θt−1 + ηtut and
ut = (1 − α)ut−1 − ˜f t−1(θ). A Bayesian analog is
SGHMC (Chen et al., 2014) or multivariate SGNHT
(mSGNHT) (Gan et al., 2015).
In mSGNHT, each
parameter has a unique momentum weight αt ∈ Rp
that is learned during the sampling sequence. The
momentum weights are updated to maintain the sys-
tem temperature 1/β. An inverse temperature of
β = 1 corresponds to the posterior. This algorithm
has updates θt = θt−1 + ηtut, ut = (1 − ηtαt−1) (cid:12)
˜f t−1(θ) + (cid:112)2ηt/βζt. The main diﬀerence
ut−1 − ηt
is the additive Gaussian noise and step-size depen-
dent momentum update. The weights have updates
αt = αt−1 + ηt((ut (cid:12) ut) − 1/β), which matches the
kinetic energy to the system temperature.

A recent idea in stochastic optimization is to use
an adaptive preconditioner, also known as a variable
metric, to improve convergence rates. Both ADA-
grad (Duchi et al., 2011) and Adam (Kingma and Ba,
2015) adapt to the local geometry with a regret bound
N ). Adam adds momentum as well through
of O(
moment smoothing. RMSprop (Tieleman and Hin-
ton, 2012), Adadelta (Zeiler, 2012), and RMSspectral

√

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Algorithm 1: Santa with the Euler scheme
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t (cid:44) ∇θ ˜U (θt−1) on the tth mini-batch;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
if t < burnin then
/* exploration
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt);
(cid:1) (cid:11) ut−1 +
ut = η
βt

(cid:0)1 − gt−1 (cid:11) gt

(cid:113) 2η
βt

gt−1 (cid:12) ζt

*/

else

/* refinement
αt = αt−1; ut = 0;

end
ut = ut + (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t;
θt = θt−1 + gt (cid:12) ut;

end

*/

(Carlson et al., 2015) are similar methods with pre-
conditioners. Our method introduces adaptive mo-
mentum and preconditioners to the SG-MCMC. This
diﬀers from stochastic optimization in implementation
and theory, and is novel in SG-MCMC.
Simulated annealing (Kirkpatrick et al., 1983; ˇCern´y,
1985) is well-established as a way of acquiring a local
mode by moving from a high-temperature, ﬂat surface
to a low-temperature, peaky surface. It has been ex-
plored in the context of MCMC, including reversible
jump MCMC (Andrieu et al., 2000), annealed impor-
tant sampling (Neal, 2001) and parallel tempering (Li
et al., 2009). Traditional algorithms are based on
Metropolis–Hastings sampling, which require compu-
tationally expensive accept-reject steps. Recent work
has applied simulated annealing to large-scale learn-
ing through mini-batch based annealing (van de Meent
et al., 2014; Obermeyer et al., 2014). Our approach in-
corporates annealing into SG-MCMC with its inherent
speed and mini-batch nature.

3 The Santa Algorithm

Santa extends the mSGNHT algorithm with precon-
ditioners and a simulated annealing scheme. A simple
pseudocode is shown in Algorithm 1, or a more com-
plex, but higher accuracy version, is shown in Algo-
rithm 2, and we detail the steps below.

The ﬁrst extension we consider is the use of adaptive
preconditioners. Preconditioning has been proven crit-
ical for fast convergence in both stochastic optimiza-

tion (Dauphin et al., 2015) and SG-MCMC algorithms
(Patterson and Teh, 2013). In the MCMC literature,
preconditioning is alternatively referred to as Rieman-
nian information geometry (Patterson and Teh, 2013).
We denote the preconditioner as {Gt ∈ Rp×p}. A pop-
ular choice in SG-MCMC is the Fisher information ma-
trix (Girolami and Calderhead, 2011). Unfortunately,
this approach is computationally prohibitive for many
models of interest. To avoid this problem, we adopt
the preconditioner from RMSprop and Adam, which
uses a vector {gt ∈ Rp} to approximate the diagonal
of the Fisher information matrixes (Li et al., 2016a).
The construction sequentially updates the precondi-
tioner based on current and historical gradients with
a smoothing parameter σ, and is shown as part of Al-
gorithm 1. While this approach will not capture the
Riemannian geometry as eﬀectively as the Fisher in-
formation matrix, it is computationally eﬃcient.

Santa also introduces an annealing scheme on sys-
tem temperatures. As discussed in Section 2, mS-
GNHT naturally accounts for a varying temperature
by matching the particle momentum to the system
temperature. We introduce β = {β1, β2, · · · }, a se-
quence of inverse temperature variables with βi < βj
for i < j and limi→∞ βi = ∞. The inﬁnite case
corresponds to the zero-temperature limit, where SG-
MCMCs become deterministic optimization methods.

The annealing scheme leads to two stages:
the ex-
ploration and the reﬁnement stages. The exploration
stage updates all parameters based on an annealed se-
quence of stochastic dynamic systems (see Section 4
for more details). This stage is able to explore the
parameter space eﬃciently, escape poor local modes,
and ﬁnally converge close to the global mode. The re-
ﬁnement stage corresponds to the temperature limit,
i.e., βn → ∞. In the temperature limit, the momen-
tum weight updates vanish and it becomes a stochastic
optimization algorithm.

We propose two update schemes to solve the corre-
sponding stochastic diﬀerential equations: the Euler
scheme and the symmetric splitting scheme (SSS). The
Euler scheme has simpler updates, as detailed in Algo-
rithm 1; while SSS endows increased accuracy (Chen
et al., 2015) with a slight increase in overhead compu-
tation, as shown in Algorithm 2. Section 4.1 elaborates
on the details of these two schemes. We recommend
the use of SSS, but the Euler scheme is simpler to im-
plement and compare to known algorithms.

Practical considerations According to Section 4,
the exploration stage helps the algorithm traverse the
parameter space following the posterior curve as ac-
curate as possible. For optimization, slightly biased
samples do not aﬀect the ﬁnal solution. As a result,

Bridging the Gap between SG-MCMC and Stochastic Optimization

Algorithm 2: Santa with SSS
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t (cid:44) ∇θ ˜U (θt−1) on the tth mini-batch;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
θt = θt−1 + gt (cid:12) ut−1 /2;
if t < burnin then
/* exploration
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt) /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη + (cid:112)2 gt−1 η/βt (cid:12) ζt

*/

+ η/βt

(cid:0)1 − gt−1 (cid:11) gt
ut = exp (−αt/2) (cid:12) ut;
αt = αt + (ut (cid:12) ut −η/βt) /2;

(cid:1) (cid:11) ut−1;

else

/* refinement
αt = αt−1;
ut = ut − gt (cid:12)˜f tη; ut = exp (−αt/2) (cid:12) ut;

ut = exp (−αt/2) (cid:12) ut−1;

*/

end
θt = θt + gt (cid:12) ut /2;

end

the term consisting of (1 − gt−1 (cid:11) gt) in the algorithm
(which is an approximation term, see Section 4.1) is
ignored. We found no decreasing performance in our
experiments. Furthermore, the term gt−1 associated
with the Gaussian noise could be replaced with a ﬁxed
constant without aﬀecting the algorithm.

4 Theoretical Foundation

In this section we present the stochastic diﬀerential
equations (SDEs) that correspond to the Santa algo-
rithm. We ﬁrst introduce the general SDE framework,
then describe the exploration stage in Section 4.1 and
the reﬁnement stage in Section 4.2. We give the con-
vergence properties of the numerical scheme in Section
4.3. This theory uses tools from the SDE literature and
extends the mSGNHT theory (Ding et al., 2014; Gan
et al., 2015).

The SDEs are presented with re-parameterized p =
u /η1/2, Ξ = diag(α)/η1/2, as in Ding et al. (2014).
The SDEs describe the motion of a particle in a system
where θ is the location and p is the momentum.

In mSGNHT, the particle is driven by a force
−∇θ ˜Ut(θ) at time t. The stationary distribution of θ
corresponds to the model posterior (Gan et al., 2015).
Our critical extension is the use of Riemannian infor-

mation geometry, important for fast convergence (Pat-
terson and Teh, 2013). Given an inverse temperature
β, the system is described by the following SDEs1:






dθ = G1(θ) p dt
(cid:16)
d p =

−G1(θ)∇θU (θ) − Ξ p + 1

β ∇θG1(θ)

+G1(θ)(Ξ − G2(θ))∇θG2(θ)) dt + ( 2
dt ,

dΞ =

(cid:17)

(cid:16)

Q − 1
β I

β G2(θ))

1
2 dw

(1)

where Q = diag(p (cid:12) p), w is standard Brownian mo-
tion, G1(θ) encodes geometric information of the po-
tential energy U (θ), and G2(θ) characterizes the man-
ifold geometry of the Brownian motion. Note G2(θ)
may be the same as G1(θ) for the same Riemannian
manifold. We call G1(θ) and G2(θ) Riemannian met-
rics, which are commonly deﬁned by the Fisher infor-
mation matrix (Girolami and Calderhead, 2011). We
use the RMSprop preconditioner (with updates from
Algorithm 1) for computational feasibility. Using the
Fokker-Plank equation (Risken, 1989), we show that
the marginal stationary distribution of (1) corresponds
to the posterior distribution.
AT B
Lemma 1. Denote A : B (cid:44) tr
ary distribution of (1) is: pβ(θ, p, Ξ) ∝

.The station-

(cid:110)

(cid:111)

e−βU (θ)− β

2 pT p − β

2 (Ξ−G2(θ)):(Ξ−G2(θ)) .

(2)

An inverse temperature β = 1 corresponds to the stan-
dard Bayesian posterior.

We note that p in (1) has additional dependencies on
G1 and G2 compared to Gan et al. (2015) that must be
accounted for. Ξ p introduces friction into the system
so that the particle does not move too far away by
the random force; the terms ∇θG1(θ) and ∇θG2(θ)
penalize the inﬂuences of the Riemannian metrics so
that the stationary distribution remains invariant.

4.1 Exploration

The ﬁrst stage of Santa, exploration, explores the pa-
rameter space to obtain parameters near the global
mode of an objective function2. This approach applies
ideas from simulated annealing (Kirkpatrick et al.,
1983). Speciﬁcally, the inverse temperature β is slowly
annealed to temperature zero to freeze the particles at
the global mode.

Minimizing U (θ) is equivalent to sampling from the
zero-temperature limit pβ(θ) (cid:44) 1
e−βU (θ) (propor-
Zβ

1We abuse notation for conciseness. Here, ∇θ G(θ) is

a vector with the i-th element being (cid:80)

j ∇θj Gij(θ).

2This requires an ergodic algorithm. While ergodicity is
not straightforward to check, we follow most MCMC work
and assume it holds in our algorithm.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

tional to (2)), with Zβ being the normalization con-
stant such that pβ(θ) is a valid distribution. We
construct a Markov chain that sequentially transits
from high temperatures to low temperatures. At the
state equilibrium, the chain reaches the temperature
limit with marginal stationary distribution ρ0(θ) (cid:44)
limβ→∞ e−βU (θ), a point mass3 located at the global
mode of U (θ). Speciﬁcally, we ﬁrst deﬁne a sequence
of inverse temperatures, (β1, β2, · · · , βL), such that βL
is large enough4. For each time t, we generate a sam-
ple according to the SDE system (1) with temperature
1
, conditioned on the sample from the previous tem-
βt
. We call this procedure annealing ther-
perature,
mostats to denote the analog to simulated annealing.

1
βt−1

Generating approximate samples Generating
exact samples from (1) is infeasible for general mod-
els. One well-known numerical approach is the Euler
scheme in Algorithm 1. The Euler scheme is a 1st-
order method with relatively high approximation error
(Chen et al., 2015). We increase accuracy by imple-
menting the symmetric splitting scheme (SSS) (Chen
et al., 2015; Li et al., 2016b). The idea of SSS is to split
an infeasible SDE into several sub-SDEs, where each
sub-SDE is analytically solvable; approximate samples
are generated by sequentially evolving parameters via
these sub-SDEs. Speciﬁcally, in Santa, we split (1)
into the following three sub-SDEs:

A :

O :









dθ = G1(θ) p dt
d p = 0
(cid:16)
dΞ =

Q − 1
β I

(cid:17)

dt

, B :






dθ = 0
d p = −Ξ p dt
dΞ = 0

,

dθ = 0
(cid:16)
d p =
+G1(θ)(Ξ − G2(θ))∇θG2(θ)) dt + ( 2
dΞ = 0

−G1(θ)∇θU (θ) + 1

β ∇θG1(θ)

β G2(θ))

1
2 dw

We then update the sub-SDEs in order A-B-O-B-A
to generative approximate samples (Chen et al., 2015).
This uses half-steps h/2 on the A and B updates5,
and full steps h in the O update. This is analogous to
the leapfrog steps in Hamiltonian Monte Carlo (Neal,
2011). Update equations are given in the Supplemen-
tary Section A. The resulting parameters then serve
as an approximate sample from the posterior distribu-
tion with the inverse temperature of β. Replacing G1
and G2 with the RMSprop preconditioners gives Al-
gorithm 2. These updates require approximations to
∇θG1(θ) and ∇θG2(θ), addressed below.

3The sampler samples a uniform distribution over global
modes, or a point mass if the mode is unique. We assume
uniqueness and say point mass for clarity henceforth.

4Due to numerical issues, it is impossible to set βL to
inﬁnity; we thus assign a large enough value for it and
handle the inﬁnity case in the reﬁnement stage.
5As in Ding et al. (2014), we deﬁne h =
η.

√

Approximate calculation for ∇θ G1(θ) We pro-
pose a computationally eﬃcient approximation for
calculating the derivative vector ∇θ G1(θ) based
on the deﬁnition.
for the i-th ele-
ment of ∇θ G1(θ) at the t-th iteration, denoted as
(∇θ Gt

1(θ))i, it is approximated as:

Speciﬁcally,

(∇θ Gt

1(θ))i

A1≈

(cid:88)

(Gt

1(θ))ij − (Gt−1
θtj − θ(t−1)j

1

(θ))ij

j
(∆ Gt
1)ij
1(θ) pt−1)jh

(cid:88)

A2=

(Gt

j

(cid:88)

=

j

(∆ Gt
1)ij
1(θ) ut−1)j

(Gt

(cid:44) Gt

1 − Gt−1

where ∆ Gt
. Step A1 follows by the
1
deﬁnition of a derivative, and A2 by using the update
equation for θt, i.e., θt = θt−1+Gt
1(θ) pt−1 h. Accord-
ing to Taylor’s theory, the approximation error for the
∇θ G1(θ) is O(h), e.g.,

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

i

j

(∆ Gt
1)ij
1(θ) ut−1)j

(Gt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− (∇θ Gt

1(θ))i

≤ Bth ,

(3)

for some positive constant Bt. The approximation error
is negligible in term of convergence behaviors because
it can be absorbed into the stochastic gradients error.
Formal theoretical analysis on convergence behaviors
with this approximation is given in later sections. Us-
ing similar methods, ∇θGt
2(θ) is also approximately
calculated.

4.2 Reﬁnement

stage

reﬁnement

corresponds

zero-
The
temperature limit of the exploration stage, where Ξ
is learned. We show that in the limit Santa gives sig-
niﬁcantly simpliﬁed updates, leading to an stochastic
optimization algorithm similar to Adam or SGD-M.

to the

We assume that the Markov chain has reached its
equilibrium after the exploration stage.
In the zero-
temperature limit, some terms in the SDE (1) vanish.
First, as β → ∞, the term 1
β ∇θ G1(θ) and the vari-
ance term for the Brownian motion approach 0. As
well, the thermostat variable Ξ approaches G2(θ), so
the term G1(θ)(Ξ − G2(θ))∇θ G2(θ) vanishes. The
stationary distribution in (2) implies E Qii
i →
0, which makes the SDE for Ξ in (1) vanish. As a re-
sult, in the reﬁnement stage, only θ and p need to be
updated. The Euler scheme for this is shown in Algo-
rithm 1, and the symmetric splitting scheme is shown
in Algorithm 2.

(cid:44) E p2

Relation to stochastic optimization algorithms
In the reﬁnement stage Santa is a stochastic optimiza-
tion algorithm. This relation is easier seen with the
Euler scheme in Algorithm 1. Compared with SGD-M
(Rumelhart et al., 1986), Santa has both adaptive gra-
dient and adaptive momentum updates. Unlike Ada-
grad (Duchi et al., 2011) and RMSprop (Tieleman and

Bridging the Gap between SG-MCMC and Stochastic Optimization

Hinton, 2012), reﬁnement Santa is a momentum based
algorithm.

The recently proposed Adam algorithm (Kingma and
Ba, 2015) incorporates momentum and precondition-
ing in what is denoted as “adaptive moments.” We
show in Supplementary Section F that a constant step
size combined with a change of variables nearly recov-
ers the Adam algorithm with element-wise momentum
weights. For these reasons, Santa serves as a more gen-
eral stochastic optimization algorithm that extends all
current algorithms. As well, for a convex problem and
a few trivial algorithmic changes, the regret bound of
T ),
Adam holds for reﬁnement Santa, which is O(
as detailed in Supplementary Section F. However, our
analysis is focused on non-convex problems that do not
ﬁt in the regret bound formulation.

√

4.3 Convergence properties

Our convergence properties are based on the frame-
work of Chen et al. (2015). The proofs for all the-
orems are given in the Supplementary Material. We
focus on the exploration stage of the algorithm. Us-
ing the Monotone Convergence argument (Schechter,
1997), the reﬁnement stage convergence is obtained by
taking the temperature limit from the results of the ex-
ploration stage. We emphasize that our approach dif-
fers from conventional stochastic optimization or on-
line optimization approaches. Our convergence rate
is weaker than many stochastic optimization methods,
including SGD; however, our analysis applies to non-
convex problems, whereas traditionally convergence
rates only apply to convex problems.

The goal of Santa is to obtain θ∗ such that θ∗ =
argminθ U (θ). Let {θ1, · · · , θL} be a sequence of pa-
rameters collected from the algorithm. Deﬁne ˆU (cid:44)
t=1 U (θt) as the sample average, ¯U (cid:44) U (θ∗) the
1
L
global optima of U (θ).

(cid:80)L

As in Chen et al. (2015), we require certain assump-
tions on the potential energy U . To show these as-
sumptions, we ﬁrst deﬁne a functional ψt for each t
that solves the following Poisson equation:

Ltψt(θt) = U (θt) − ¯U ,

(4)

Lt is the generator of the SDE system (1) in the t-th
E[f (xt+h)]−f (xt)
iteration, deﬁned Ltf (xt) (cid:44) limh→0+
h
where xt (cid:44) (θt, pt, Ξt), f : R3p → R is a compactly
supported twice diﬀerentiable function. The solution
functional ψt(θt) characterizes the diﬀerence between
U (θt) and the global optima ¯U for every θt. As shown
in Mattingly et al. (2010), (4) typically possesses a
unique solution, which is at least as smooth as U under
the elliptic or hypoelliptic settings. We assume ψt is
bounded and smooth, as described below.

Assumption 1. ψt and its up to 3rd-order deriva-
tives, Dkψt, are bounded by a function V(θ, p, Ξ),
i.e., (cid:107)Dkψ(cid:107) ≤ CkV rk for k = (0, 1, 2, 3), Ck, rk >
0. Furthermore, the expectation of V is bounded:
EV r(θ, p, Ξ) < ∞, and V is smooth such that
supt
sups∈(0,1) V r (sx + (1 − s) y) ≤ C (V r (x) + V r (y)),
∀x ∈ R3p, y ∈ R3p, r ≤ max{2rk} for some C > 0.

(cid:17)

Let ∆U (θ) (cid:44) U (θ) − U (θ∗). Further deﬁne an opera-
(cid:16)
G1(θ)(∇θ ˜Ut − ∇θU ) + Bt
· ∇p for each
tor ∆Vt =
βt
t, where Bt is from (3). Theorem 2 depicts the close-
ness of ˆU to the global optima ¯U in term of bias and
mean square error (MSE) deﬁned below.
Theorem 2. Let (cid:107)·(cid:107) be the operator norm. Under
Assumption 1, the bias and MSE of the exploration
stage in Santa with respect to the global optima for L
steps with stepsize h is bounded, for some constants
C > 0 and D > 0, with:

Bias:

E ˆU − ¯U

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ Ce−U (θ∗)
(cid:12)
(cid:18) 1
Lh

+ D

L
(cid:88)

(cid:90)

1
L
(cid:80)

t=1

(cid:32)

+

MSE: E

(cid:16) ˆU − ¯U

(cid:17)2

≤ C 2e−2U (θ∗)

(cid:33)

e−βt∆U (θ)dθ

t (cid:107)E∆Vt(cid:107)
L

(cid:19)

+ h2

.

L
(cid:88)

(cid:90)

(cid:32)

1
L

t=1
E (cid:107)∆Vt(cid:107)2
L

(cid:33)2

e−βt∆U (θ)dθ

(cid:33)

+ h4

.

+

1
Lh

(cid:32) 1
L

(cid:80)
t

+ D2

Both bounds for the bias and MSE have two parts.
The ﬁrst part contains integration terms, which char-
acterizes the distance between the global optima,
e−U (θ∗), and the unnormalized annealing distribu-
tions, e−βtU (θ), decreasing to zero exponentially fast
with increasing β; the remaining part characterizes the
distance between the sample average and the anneal-
ing posterior average. This shares a similar form as in
general SG-MCMC algorithms (Chen et al., 2015), and
can be controlled to converge. Furthermore, the term
(cid:80)
in the bias vanishes as long as the sum of
the annealing sequence {βt} is ﬁnite6, indicating that
the gradient approximation for ∇θG1(θ) in Section 4.1
does not aﬀect the bias of the algorithm. Similar ar-
guments apply for the MSE bound.

t(cid:107)E∆Vt(cid:107)
L

(cid:80)L+m−1
l=m

To get convergence results right before the reﬁnement
stage, let a sequence of functions {gm} be deﬁned as
e−βl ˆU (θ); it is easy to see that {gm}
gm (cid:44) − 1
L
satisﬁes gm1 < gm2 for m1 < m2, and limm→∞ gm =
0. According to the Monotone Convergence Theorem
(Schechter, 1997), the bias and MSE in the limit exists,
leading to Corollary 3.
Corollary 3. Under Assumptions 1, the bias and
MSE of the reﬁnement stage in Santa with respect

6In practice we might not need to care about this con-
straint because a small bias in the exploration stage does
not aﬀect convergence of the reﬁnement stage.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

to the global optima for L steps with stepsize h are
bounded, for some constants D1 > 0, D2 > 0, as
(cid:18) 1
Lh
(cid:32) 1
L

(cid:12)
(cid:12)
(cid:12) ≤ D1

E ˆU − ¯U

+ h2

Bias:

(cid:80)
t

(cid:80)

(cid:17)2

+

(cid:19)

(cid:12)
(cid:12)
(cid:12)

MSE: E

(cid:16) ˆU − ¯U

t (cid:107)E∆Vt(cid:107)
L
E (cid:107)∆Vt(cid:107)2
L

+

1
Lh

≤ D2

+ h4

(cid:33)

Corollary 3 implies that in the reﬁnement stage, the
discrepancy between annealing distributions and the
global optima vanishes, leaving only errors from dis-
cretized simulations of the SDEs, similar to the result
of general SG-MCMC (Chen et al., 2015). We note
that after exploration, Santa becomes a pure stochas-
tic optimization algorithm, thus convergence results
in term of regret bounds can also be derived; refer to
Supplementary Section F for more details.

5 Experiments

5.1

Illustration

In order to demonstrate that Santa is able to achieve
the global mode of an objective function, we consider
the double-well potential (Ding et al., 2014),

U (θ) = (θ + 4)(θ + 1)(θ − 1)(θ − 3)/14 + 0.5 .

As shown in Figure 1 (left), the double-well potential
has two modes, located at θ = −3 and θ = 2, with the
global optima at θ = −3. We use a decreasing learning
rate ht = t−0.3/10, and the annealing sequence is set to
βt = t2. To make the optimization more challenging,
we initialize the parameter at θ0 = 4, close to the local
mode. The evolution of θ with respect to iterations is
shown in Figure 1(right). As can be seen, θ ﬁrst moves
to the local mode but quickly jumps out and moves
to the global mode in the exploration stage (ﬁrst half
iterations); in the reﬁnement stage, θ quickly converges
to the global mode and sticks to it afterwards.
In
contrast, RMSprop is trapped on the local optima, and
convergences slower than Santa at the beginning.

Figure 1: (Left) Double-well potential. (Right) The
evolution of θ using Santa and RMSprop algorithms.

Figure 2: Learning curves of diﬀerent algorithms on
MNIST. (Left) FNN with size of 400. (Right) CNN.

5.2 Feedforward neural networks

We ﬁrst test Santa on the Feedforward Neural Net-
work (FNN) with rectiﬁed linear units (ReLU). We
test two-layer models with network sizes 784-X-X-10,
where X is the number of hidden units for each layer;
100 epochs are used. For variants of Santa, we de-
note Santa-E as Santa with a Euler scheme illustrated
in Algorithm 1, Santa-r as Santa running only on the
reﬁnement stage, but with updates on α as in the ex-
ploration stage. We compare Santa with SGD, SGD-
M, RMSprop, Adam, SGD with dropout, SGLD and
Bayes by Backprop (Blundell et al., 2015). We use
a grid search to obtain good learning rates for each
algorithm, resulting in 4 × 10−6 for Santa, 5 × 10−4
for RMSprop, 10−3 for Adam, and 5 × 10−1 for SGD,
SGD-M and SGLD. We choose an annealing schedule
of βt = Atγ with A = 1 and γ selected from 0.1 to 1
with an interval of 0.1. For simplicity, the exploration
is set to take half of total iterations.

We test the algorithms on the standard MNIST
dataset, which contains 28 × 28 handwritten digital
images from 10 classes with 60, 000 training samples
and 10, 000 test samples. The network size (X-X) is
set to 400-400 and 800-800, and test classiﬁcation er-
rors are shown in Table 1. Santa show improved state-
of-the-art performance amongst all algorithms. The
Euler scheme shows a slight decrease in performance,
due to the integration error when solving the SDE.
Santa without exploration (i.e., Santa-r) still performs
relatively well. Learning curves are plotted in Fig-
ure 2, showing that Santa converges as fast as other
algorithms but to a better local optima7.

5.3 Convolution neural networks

We next test Santa on the Convolution Neural Net-
work (CNN). Following Jarrett et al. (2009), a stan-
dard network conﬁguration with 2 convolutional layers
followed by 2 fully-connected layers is adopted. Both
convolutional layers use 5 × 5 ﬁlter size with 32 and 64

7Learning curves of FNN with size of 800 are provided

in Supplementary Section G.

Bridging the Gap between SG-MCMC and Stochastic Optimization

Algorithms
Santa
Santa-E
Santa-r
Adam
RMSprop
SGD-M
SGD
SGLD
BPB(cid:5)
SGD, Dropout(cid:5)
Stoc. Pooling(cid:46)
NIN, Dropout◦
Maxout, Dropout(cid:63)

FNN-400 FNN-800
1.21%
1.41%
1.45%
1.53%
1.59%
1.66%
1.72%
1.64%
1.32%
1.51%
−
−
−

CNN
1.16% 0.47%
0.58%
1.27%
0.49%
1.40%
0.59%
1.47%
0.64%
1.43%
0.77%
1.72%
0.81%
1.47%
0.71%
1.41%
−
1.34%
−
1.33%
0.47%
−
0.47%
−
0.45%
−

Table 1: Test error on MNIST classiﬁcation using FNN
and CNN. ((cid:5)) taken from Blundell et al. (2015). ((cid:46)) taken
from Zeiler and Fergus (2013). (◦) taken from Lin et al.
(2014). ((cid:63)) taken from Goodfellow et al. (2013).

channels, respectively; 2 × 2 max pooling is used af-
ter each convolutional layer. The fully-connected lay-
ers have 200-200 hidden nodes with ReLU activation.
The same parameter setting and dataset as in the FNN
are used. The test errors are shown in Table 1, and
the corresponding learning curves are shown in Fig-
ure 2. Similar trends as in FNN are obtained. Santa
signiﬁcantly outperforms other algorithms with an er-
ror of 0.45%. This result is comparable or even better
than some recent state-of-the-art CNN-based systems,
which have much more complex architectures.

5.4 Recurrent neural networks

We test Santa on the Recurrent Neural Network
is
(RNN) for sequence modeling, where a model
trained to minimize the negative log-likelihood of
training sequences:

min
θ

1
N

N
(cid:88)

Tn(cid:88)

n=1

t=1

− log p(xn

t | xn

1 , . . . , xn

t−1; θ)

(5)

where θ is a set of model parameters, {xn
t } is the
observed data. The conditional distributions in (5)
are modeled by the RNN. The hidden units are set to
gated recurrent units (Cho et al., 2014).

We consider the task of sequence modeling on four
diﬀerent polyphonic music sequences of piano, i.e.,
Piano-midi.de (Piano), Nottingham (Nott), MuseData
(Muse) and JSB chorales (JSB). Each of these datasets
are represented as a collection of 88-dimensional bi-
nary sequences, that span the whole range of piano
from A0 to C8.

The number of hidden units is set to 200. Each model
is trained for at most 100 epochs. According to the
experiments and their results on the validation set, we

Figure 3: Learning curves of diﬀerent algorithms on Piano
using RNN. (Left) training set. (Right) validation set.

Algorithms Piano. Nott. Muse.
7.20
7.56
7.22
7.69
10.08
7.19
8.13

Santa
Adam
RMSprop
SGD-M
SGD
HF(cid:5)
SGD-M(cid:5)

7.60
8.00
7.70
8.32
11.13
7.66
8.37

3.39
3.70
3.48
3.60
5.26
3.89
4.46

JSB.
8.46
8.51
8.52
8.59
10.81
8.58
8.71

Table 2: Test negative log-likelihood results on poly-
phonic music datasets using RNN. ((cid:5)) taken from
Boulanger-Lewandowski et al. (2012).

use a learning rate of 0.001 for all the algorithms. For
Santa, we consider an additional experiment using a
learning rate of 0.0002, denoted Santa-s. The anneal-
ing coeﬃcient γ is set to 0.5. Gradients are clipped
if the norm of the parameter vector exceeds 5. We
do not perform any dataset-speciﬁc tuning other than
early stopping on validation sets. Each update is done
using a minibatch of one sequence.

The best log-likelihood results on the test set are
achieved by using Santa, shown in Table 2. Learn-
ing curves on the Piano dataset are plotted in Fig-
ure 3. We observe that Santa achieves fast conver-
gence, but is overﬁtting. This is straightforwardly ad-
dressed through early stopping. The learning curves
for all the other datasets are provided in Supplemen-
tary Section G.

6 Conclusions

We propose Santa, an annealed SG-MCMC method
for stochastic optimization. Santa is able to explore
the parameter space eﬃciently and locate close to the
global optima by annealing. At the zero-temperature
limit, Santa gives a novel stochastic optimization algo-
rithm where both model parameters and momentum
are updated element-wise and adaptively. We provide
theory on the convergence of Santa to the global op-
tima for an (non-convex) objective function. Experi-
ments show best results on several deep models com-
pared to related stochastic optimization algorithms.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Acknowledgements

This research was supported in part by ARO, DARPA,
DOE, NGA, ONR and NSF.

References

C. Andrieu, N. de Freitas, and A. Doucet. Reversible
jump mcmc simulated annealing for neural net-
works. In UAI, 2000.

C. Blundell, J. Cornebise, K. Kavukcuoglu, and
D. Wierstra. Weight uncertainty in neural networks.
In ICML, 2015.

L. Bottou. Large-scale machine learning with stochas-
tic gradient descent. In Proc. COMPSTAT, 2010.

N. Boulanger-Lewandowski, Y. Bengio, and P. Vin-
cent. Modeling temporal dependencies in high-
dimensional sequences: Application to polyphonic
music generation and transcription. In ICML, 2012.

D. E. Carlson, E. Collins, Y.-P. Hsieh, L. Carin, and
V. Cevher. Preconditioned spectral descent for deep
learning. In Advances in Neural Information Pro-
cessing Systems, pages 2953–2961, 2015.

C. Chen, N. Ding, and L. Carin. On the convergence
of stochastic gradient mcmc algorithms with high-
order integrators. In NIPS, 2015.

T. Chen, E. B. Fox, and C. Guestrin. Stochastic gra-
dient Hamiltonian Monte Carlo. In ICML, 2014.

K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Ben-
gio.
Learning phrase representations using rnn
encoder-decoder for statistical machine translation.
In arXiv:1406.1078, 2014.

Y. N. Dauphin, H. de Vries, and Y. Bengio. Equili-
brated adaptive learning rates for non-convex opti-
mization. In NIPS, 2015.

N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel,
and H. Neven. Bayesian sampling using stochastic
gradient thermostats. In NIPS, 2014.

J. Duchi, E. Hazan, and Y. Singer. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In JMLR, 2011.

Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin.
Scalable deep Poisson factor analysis for topic mod-
eling. In ICML, 2015.

I. Goodfellow, D. Warde-farley, M. Mirza,
A. Courville, and Y. Bengio. Maxout networks. In
ICML, 2013.

K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. Le-
Cun. What is the best multi-stage architecture for
object recognition? In ICCV, 2009.

D. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015.

S. Kirkpatrick, C. D. G. Jr, and M. P. Vecchi. Opti-
mization by simulated annealing. In Science, 1983.

C. Li, C. Chen, D. Carlson, and L. Carin. Precon-
ditioned stochastic gradient Langevin dynamics for
deep neural networks. In AAAI, 2016a.

C. Li, C. Chen, K. Fan, and L. Carin. High-order
stochastic gradient thermostats for Bayesian learn-
ing of deep models. In AAAI, 2016b.

Y. Li, V. A. Protopopescu, N. Arnold, X. Zhang,
and A. Gorin. Hybrid parallel tempering and sim-
ulated annealing method. In Applied Mathematics
and Computation, 2009.

M. Lin, Q. Chen, and S. Yan. Network in network. In

ICLR, 2014.

J. C. Mattingly, A. M. Stuart, and M. V. Tretyakov.
Construction of numerical time-average and station-
ary measures via Poisson equations.
In SIAM J.
NUMER. ANAL., 2010.

R. M. Neal. Annealed importance sampling. In Statis-

tics and Computing, 2001.

R. M. Neal. Mcmc using hamiltonian dynamics.
Handbook of Markov Chain Monte Carlo, 2011.

In

F. Obermeyer, J. Glidden, and E. Jonas. Scaling
nonparametric bayesian inference via subsample-
annealing. In AISTATS, 2014.

S. Patterson and Y. W. Teh. Stochastic gradient Rie-
mannian Langevin dynamics on the probability sim-
plex. In NIPS, 2013.

H. Risken. The Fokker-Planck equation. Springer-

Verlag, New York, 1989.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
Learning representations by back-propagating er-
rors. In Nature, 1986.

E. Schechter. Handbook of Analysis and Its Founda-

tions. Elsevier, 1997.

S. Geman and D. Geman. Stochastic relaxation, gibbs
distributions, and the bayesian restoration of im-
ages. In PAMI, 1984.

I. Sutskever, J. Martens, G. Dahl, and G. E. Hinton.
On the importance of initialization and momentum
in deep learning. In ICML, 2013.

M. Girolami and B. Calderhead. Riemann manifold
Langevin and Hamiltonian Monte Carlo methods.
In JRSS, 2011.

Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Con-
sistency and ﬂuctuations for stochastic gradient
Langevin dynamics. In arXiv:1409.0578, 2014.

Bridging the Gap between SG-MCMC and Stochastic Optimization

T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop:
Divide the gradient by a running average of its re-
cent magnitude. In Coursera: Neural Networks for
Machine Learning, 2012.

J. W. van de Meent, B. Paige, and F. Wood. Temper-

ing by subsampling. In arXiv:1401.7145, 2014.
V. ˇCern´y. Thermodynamical approach to the travel-
ing salesman problem: An eﬃcient simulation algo-
rithm. In J. Optimization Theory and Applications,
1985.

S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh.
(Non-)asymptotic properties of stochastic gradient
Langevin dynamics. In arXiv:1501.00438, 2015.

M. Welling and Y. W. Teh. Bayesian learning via
In ICML,

stochastic gradient Langevin dynamics.
2011.

M. Zeiler and R. Fergus. Stochastic pooling for regu-
larization of deep convolutional neural networks. In
ICLR, 2013.

M. D. Zeiler. Adadelta: An adaptive learning rate

method. In arXiv:1212.5701, 2012.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Bridging the Gap between Stochastic Gradient MCMC and
Stochastic Optimization: Supplementary Material

Changyou Chen†

David Carlson‡

Zhe Gan†
†Department of Electrical and Computer Engineering, Duke University
‡Department of Statistics and Grossman Center for Statistics of Mind, Columbia University

Chunyuan Li†

Lawrence Carin†

A Solutions for the sub-SDEs

We provide analytic solutions for the split sub-SDEs
in Section 4.1. For stepsize h, the solutions are given
in (6).



θt = θt−1 + G1(θ) p h
pt = pt−1
Ξt = Ξt−1 +

Q − 1
β I

(cid:16)

(cid:17)

(6)

,

h

,

θt = θt−1
pt = exp (−Ξh) pt−1
Ξt = Ξt−1
θt = θt−1
pt = pt−1 +

(cid:16)

−G1(θ)∇θU (θ) + 1

β ∇θG1(θ)

+G1(θ)(Ξ − G2(θ))∇θG2(θ)) h
+ ( 2
2 (cid:12) ζt

β G2(θ)) 1

A :

B :

O :











Ξt = Ξt−1

B Proof of Lemma 1

For a general stochastic diﬀerential equation of the
form

d x = F (x)dt +

2D1/2(x)d w ,

(7)

√

where x ∈ RN , F : RN → RN , D : RM → RN ×P
are measurable functions with P , and w is standard
P -dimensional Brownian motion. (1) is a special case
of the general form (7) with

x = (θ, p, Ξ)


F (x) =

D(x) =









G1(θ) p

− G1(θ)∇θU (θ) − Ξ p + 1

β ∇θ G1(θ)
+ G1(θ)(Ξ − G2(θ))∇θ G2(θ)
Q − 1
β I


0
0
0

0

0
1
β G2(θ) 0
0

0



(8)






We write the joint distribution of x as
exp {−H(x)} (cid:44) 1
Z

ρ(x) =

1
Z

exp {−U (θ) − E(θ, p, Ξ)} .

A reformulation of the main theorem in Ding et al.
(2014) gives the following lemma, which is used to
prove Lemma 1 in the main text.

Lemma 4. The stochastic process of (cid:126)θ generated by
the stochastic diﬀerential equation (7) has the target
distribution pθ(θ) = 1
Z exp{−U (θ)} as its stationary
distribution, if ρ(x) satisﬁes the following marginaliza-
tion condition:

exp{−U (θ)} ∝

exp{−U (θ) − E(θ, p, Ξ)}d p dΞ ,

(cid:90)

(9)

and if the following condition is also satisﬁed:

∇ · (ρF ) = ∇∇(cid:62) : (ρD) ,

(10)

where ∇ (cid:44) (∂/∂θ, ∂/∂ p, ∂/Ξ), “·” represents the vec-
tor inner product operator, “:” represents a matrix
double dot product, i.e., X : Y (cid:44) tr(X(cid:62) Y).

Proof of Lemma 1. We ﬁrst have reformulated (1) us-
ing the general SDE form of (7), resulting in (8).
Lemma 1 states the joint distribution of (θ, p, Ξ) is

ρ(x) =

exp

−

p(cid:62) p −U (θ)

(cid:18)

1
2

(cid:110)
(Ξ − G2(θ))(cid:62) (Ξ − G2(θ))

tr

−

(cid:111)(cid:19)

,

(11)

1
Z
1
2

1

=

H(x)

with
(cid:110)
(Ξ − G2(θ))(cid:62) (Ξ − G2(θ))
1
2 tr
tion condition (9) is trivially satisﬁed, we are left to
verify condition (10). Substituting ρ(x) and F into
(10), we have the left-hand side

2 p(cid:62) p +U (θ)
. The marginaliza-

+

(cid:111)

Bridging the Gap between SG-MCMC and Stochastic Optimization

LHS =

(cid:88)

∂
∂ xi

(ρFi)

i

∂ρ
∂ xi
(cid:18) ∂Fi
∂ xi

Fi +

∂Fi
∂ xi

ρ

(cid:19)

Fi

ρ

−

∂H
∂ xi

(cid:88)

i
(cid:88)

i
(cid:32)

=

=

=

∇θi(G1)i: p −

diag(Ξ)

(cid:88)

i

(cid:88)

j



(cid:88)

i

i

(cid:88)

−

(cid:18)

− βpT

(cid:88)

−β

i

=

1
β

β

∇θi U −

(Ξij − (G2)ij)∇θi(G2)ij

 (G1 p)i

− G1 ∇θU − Ξ p +

∇θ G1 + G1(Ξ − G2)∇θ G2

1
β

(cid:18)

(Ξii − (G2)ii)

Qii −

(cid:19)(cid:33)

ρ

1
β

tr (cid:8)G2(p pT −I)(cid:9) ρ .

It is easy to see for the right-hand side

RHS =

(cid:88)

(cid:88)

1
β

j

(G2)ij

∂2
∂ xi ∂ xj

ρ

(cid:88)

(cid:88)

(G2)ij

∂
∂ pj

(cid:19)

(cid:18)

−

∂H
∂ pi

ρ

j

i
(cid:88)

(G2)ii

(cid:0)p2

i −1(cid:1) ρ

i
1
β

1
β

=

=

i
≡ LHS .

According to Lemma 4, the joint distribution (11) is
the equilibrium distribution of (1).

C Proof of Theorem 2

We start by proving the bias result of Theorem 2.

Proof of the bias. For our 2nd-order integrator, ac-
cording to the deﬁnition, we have:

E[ψ(Xt)] = ˜P l
(cid:17)

(cid:16)

I + h˜Lt

=

hψ(Xt−1) = eh ˜Ltψ(Xt−1) + O(h3)

ψ(Xt−1) +

2
˜L
t ψ(Xt−1) + O(h3) ,

h2
2

where Lt is the generator of the SDE for the t-th it-
eration, i.e., using stochastic gradient instead of the
full gradient, I is the identity map. Compared to the
prove of Chen et al. (2015), we need to consider the
approximation error for ∇θG1(θ). As a result, (12)
needs to be rewritten as:

(13)

E[ψ(Xt)]
(cid:16)

where Bt is from (3). Sum over t = 1, · · · , L in (13),
take expectation on both sides, and use the relation
˜Lt + Bt = Lβt +∆Vt to expand the ﬁrst order term.
We obtain



E[Lβtψ(Xt−1)] + h

E[∆Vtψ(Xt−1)]

E[ψ(Xt)] = ψ(X0) +

E[ψ(Xt)]

L−1
(cid:88)

t=1

L
(cid:88)

t=1

L
(cid:88)

t=1

+ h

L
(cid:88)

t=1

+

h2
2

L
(cid:88)

t=1

(cid:19)

2
E[˜L
t ψ(Xt−1)] + O(Lh3).

We divide both sides by Lh, use the Poisson equation
(4), and reorganize terms. We have:

(cid:0)φ(Xt) − ¯φβt

(cid:1)] =

E[Lβtψ(Xt−1)]

(E[ψ(Xt)] − ψ(X0)) −

E[∆Vtψ(Xt−1)]

1
L

1
L

L
(cid:88)

t=1
(cid:88)

t

2
E[˜L
t ψ(Xt−1)] + O(h2)

(14)

E[

1
L

(cid:88)

t

=

1
Lh

−

h
2L

L
(cid:88)

t=1

2
Now we try to bound ˜L
t . Based on ideas from Mat-
tingly et al. (2010), we apply the following procedure.
First replace ψ with ˜Ltψ from (13) to (14), and apply
the same logic for ˜Ltψ as for ψ in the above deriva-
tions, but this time expand in (13) up to the order
of O(h2), instead of the previous order O(h3). After
simpliﬁcation, we obtain:

E[˜L

2
t ψ(Xt−1)] = O

(cid:88)

t

(cid:19)

+ Lh

(cid:18) 1
h

(15)

(12)

Substituting (15) into (14), after simpliﬁcation, we
have: E (cid:0) 1

(cid:0)φ(Xt) − ¯φβt

(cid:1)(cid:1)

(cid:80)
t

L

=

1
Lh

(E[ψ(Xt)] − ψ(X0))
(cid:123)(cid:122)
(cid:125)
(cid:124)
C1

−

1
L

(cid:88)

t

− O

(cid:18) h
Lh

(cid:19)

+ h2

+ C3h2 ,

E[∆Vtψ(Xt−1)]

=

I + h(˜Lt + Bt)

ψ(Xt−1) +

(cid:17)

˜L

2
t ψ(Xt−1) + O(h3) ,

h2
2

for some C3 ≥ 0. According to the assumption, the
term C1 is bounded. As a result, collecting low order

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

terms, the bias can be expressed as:

lated terms, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
E ˆφ − ¯φ
(cid:12)
(cid:12)
(cid:32)

=

E

(cid:32)

≤

E

1
L

1
L

(cid:88)

t

(cid:88)

t
(cid:32)

≤Cφ(θ∗)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

C1
Lh

−

(cid:32)

≤Cφ(θ∗)

+

(cid:80)
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤Cφ(θ∗)

(cid:33)

(cid:88)

+

1
L

(cid:12)
(cid:12)
¯φβt − ¯φ
(cid:12)
(cid:12)
(cid:12)

(cid:0)φ(Xt) − ¯φβt

(cid:1)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)φ(Xt) − ¯φβt

(cid:1)

t

(cid:88)

t

(cid:33)

¯φβt − ¯φ

+

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
(cid:12)

(cid:32)

1
L

e−βt ˆU (θ)dθ

L
(cid:88)

(cid:90)

1
L
(cid:80)
t

t=1

θ(cid:54)=θ∗

E∆Vtψ(Xt−1)

+ C3h2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

L

L
(cid:88)

(cid:90)

1
L

θ(cid:54)=θ∗
t=1
E∆Vtψ(Xt−1)

L
L
(cid:88)

(cid:90)

(cid:32)

1
L

e−βt ˆU (θ)dθ

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

C1
Lh

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:12)

(cid:12)C3h2(cid:12)
(cid:12)

e−βt ˆU (θ)dθ

(cid:18) 1
Lh

+

t=1
(cid:80)

θ(cid:54)=θ∗
t (cid:107)E∆Vt(cid:107)
L

+ D

(cid:19)

+ h2

,

where the last equation follows from the ﬁniteness as-
sumption of ψ, (cid:107) · (cid:107) denotes the operator norm and
is bounded in the space of ψ due to the assumptions.
This completes the proof.

We will now prove the MSE result .

Proof of the MSE bound. Similar to the proof of The-
orem 2, for our 2nd–order integrator we have:

E (ψβt(Xt)) = (I + h(Lβt + ∆Vt)) ψβt−1 (Xt−1)

+

h2
2

˜L2

t ψβt−1(Xt−1) + O(h3) .

Sum over t from 1 to L + 1 and simplify, we have:

L
(cid:88)

t=1

E (ψβt(Xt)) =

ψβt−1(Xt−1)

L
(cid:88)

t=1

+ h

Lβtψβt−1(Xt−1) + h

∆Vtψβt−1 (Xt−1)

L
(cid:88)

t=1

˜L2

t ψβt−1 (Xt−1) + O(Lh3) .

L
(cid:88)

t=1

+

h2
2

L
(cid:88)

t=1

Substitute the Poisson equation (4) into the above
equation, divide both sides by Lh and rearrange re-

1
L

L
(cid:88)

t=1

−

−

1
Lh

L
(cid:88)

t=1

1
L

L
(cid:88)

t=1

(cid:0)φ(Xt) − ¯φβt

(cid:1) =

(EψβL(XLh) − ψβ0 (X0))

1
Lh

(cid:0)Eψβt−1 (Xt−1) − ψβt−1(Xt−1)(cid:1)

∆Vtψβt−1(Xt−1) −

˜L2

t ψβt−1(Xt−1) + O(h2)

h
2L

L
(cid:88)

t=1

Taking the square of both sides, it is then easy to see
there exists some positive constant C, such that

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

(cid:32)

L
(cid:88)

t=1

1
L


≤C




(cid:124)

(EψβL(XLh) − ψβ0(X0))2
L2h2
(cid:123)(cid:122)
A1

(cid:125)

(16)

L
(cid:88)

t=1

+

1
L2h2
(cid:124)

+

1
L2

L
(cid:88)

t=1

(cid:32) L
(cid:88)

t=1

+

h2
2L2
(cid:124)

(cid:0)Eψβt−1 (Xt−1) − ψβt−1(Xt−1)(cid:1)2

(cid:123)(cid:122)
A2

(cid:125)

∆V 2

t ψβt−1 (Xt−1)









(cid:33)2

(cid:125)

˜L2

t ψβt−1 (Xt−1)

+h4

(cid:123)(cid:122)
A3

A1 is easily bounded by the assumption that (cid:107)ψ(cid:107) ≤
V r0 < ∞. A2 is bounded because it can be shown that
E (ψβt(Xt)) − ψβt(Xt) ≤ C1
h + O(h) for C1 ≥ 0.
Intuitively this is true because the only diﬀerence be-
tween E (ψβt(Xt)) and ψβt(Xt) lies in the additional
Gaussian noise with variance h. A formal proof is
given in Chen et al. (2015). Furthermore, A3 is

√

Bridging the Gap between SG-MCMC and Stochastic Optimization

bounded by the following arguments:

D Proof of Corollary 3

t ψβt−1(Xt−1) − E ˜L2

t ψβt−1(Xt−1)

(cid:104) ˜L2

E

t ψβt−1 (Xt−1)

A3 =

(cid:32) L
(cid:88)

t=1

h2
2L2
(cid:124)

(cid:32) L
(cid:88)

E

(cid:16) ˜L2

t=1

+

h2
2L2
(cid:124)

(cid:123)(cid:122)
B1

(cid:123)(cid:122)
B2

(cid:46) B1 +

˜L2

t ψβt−1 (Xt−1)

(cid:32)

h2
Lh

L
(cid:88)

t=1

E ˜L2

t ψβt−1(Xt−1)

(cid:33)2
(cid:105)

(cid:125)

(cid:33)2

(cid:33)2

(cid:17)

(cid:33)2

(cid:17)

(cid:125)

Proof. The reﬁnement stage corresponds to β → ∞.
We can prove that in this case, the integration terms
in the bias and MSE in Theorem 2 converge to 0.

To show this, deﬁne a sequence of functions {gm} as:

gm (cid:44) −

1
L

L+m−1
(cid:88)

l=m

e−βl ˆU (θ) .

(18)

it is easy to see the sequence {gm} satisﬁes gm1 < gm2
for m1 < m2, and limm→∞ gm = 0. According to the
monotone convergence theorem, we have

(cid:19)

+

1
Lh

(cid:32)

h2
L

L
(cid:88)

t=1

(cid:33)

( ˜L2

t ψ(Xt−1))2

(cid:90)

lim
m→∞

gm (cid:44) lim
m→∞

(cid:90)

−

1
L

L+m−1
(cid:88)

l=m

e−βl ˆU (θ)dθ

(cid:90)

=

lim
m→∞

gm = 0 .

(cid:32)

+

h2
Lh

L
(cid:88)

(cid:16)

t=1

≤ O

+ O

= O

(cid:18) 1
2L2 + L2h2
(cid:19)

(cid:18) 1
L2h2 + h4
(cid:19)
(cid:18) 1
Lh

+ L4

As a result, the integration terms in the bounds for the
bias and MSE vanish, leaving only the terms stated in
Corollary 3. This completes the proof.

E Reformulation of the Santa

Algorithm

E (cid:107)∆Vt(cid:107)2
L

+

1
Lh

(cid:33)

+ h4

.

(17)

In this section we give a version of the Santa algorithm
that matches better than our actual implementation,
shown in Algorithm 3–7.

Collecting low order terms we have:

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

E

(cid:32)

1
L
(cid:32) 1
L

L
(cid:88)

t=1
(cid:80)
t

=O

Finally, we have:

L
(cid:88)

t=1
(cid:32)

1
L

(cid:16) ˆφ − ¯φ

(cid:17)2

E

< E

(cid:32)

1
L

(cid:88)

t

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

+ E

(cid:32)

1
L

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

≤Cφ(θ∗)2

(cid:32) 1
L

(cid:80)
t

+ O

L
(cid:88)

(cid:90)

θ(cid:54)=θ∗
t=1
E (cid:107)∆Vt(cid:107)2
L

≤Cφ(θ∗)2

L
(cid:88)

(cid:90)

(cid:32)

1
L

(cid:33)2

(cid:33)

(cid:33)2

e−βt ˆU (θ)dθ

+

1
Lh

+ h4

e−βt ˆU (θ)dθ

(cid:32) 1
L

(cid:80)
t

+ D

θ(cid:54)=θ∗
t=1
E (cid:107)∆Vt(cid:107)2
L

(cid:33)

+ h4

.

+

1
Lh

Algorithm 3: Santa
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t = ∇θ ˜Ut(θt−1) on the t-th minibatch ;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
if t < burnin then
/* exploration
*/
(θt, ut, αt) = Exploration S(θt−1, ut−1, αt−1)
or
(θt, ut, αt) = Exploration E(θt−1, ut−1, αt−1)

/* refinement
(θt, ut, αt) = Reﬁnement S(θt−1, ut−1, αt−1)
or
(θt, ut, αt) = Reﬁnement E(θt−1, ut−1, αt−1)

*/

else

end

end

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Algorithm 4: Exploration S (θt−1, ut−1, αt−1)
θt = θt−1 + gt (cid:12) ut−1 /2;
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt) /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη +
ut = exp (−αt/2) (cid:12) ut;
αt = αt + (ut (cid:12) ut −η/βt) /2;
θt = θt + gt (cid:12) ut /2;
Return (θt, ut, αt)

2 gt−1 η3/2/βt (cid:12) ζt;

(cid:113)

Algorithm 5: Reﬁnement S (θt−1, ut−1, αt−1)
αt = αt−1;
θt = θt−1 + gt (cid:12) ut−1 /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη;
ut = exp (−αt/2) (cid:12) ut;
θt = θt + gt (cid:12) ut /2;
Return (θt, ut, αt)

F Relationship of reﬁnement Santa to

Adam

In the Adam algorithm (see Algorithm 1 of Kingma
and Ba (2015)), the key steps are:

˜f t (cid:44) ∇θ ˜U (θt−1)
vt = σ vt−1 +(1 − σ)˜f t (cid:12) ˜f t

(cid:113)

√

vt

λ +

gt = 1 (cid:11)
˜ut = (1 − b1) (cid:12) ˜ut−1 + b1 (cid:12)˜f t
θt = θt + η(gt (cid:12) gt) (cid:12) ˜ut

Here, we maintain the square root form of gt, so
the square is equivalent to the preconditioner used in
Adam. As well, in Adam, the vector b1 is set to the
same constant between 0 and 1 for all entries. An
equivalent formulation of this is:
˜f t (cid:44) ∇θ ˜U (θt−1)
vt = σ vt−1 +(1 − σ)˜f t (cid:12) ˜f t

(cid:113)

√

λ +

gt = 1 (cid:11)
vt
ut = (1 − b1) (cid:12) ut−1 − η(gt (cid:12) b1 (cid:12)˜f t)
θt = θt − gt (cid:12)ut

The only diﬀerences between these steps and the Eu-
ler integrator we present in our Algorithm 1 are that
our b1 has a separate constant for each entry, and the
second term in u does not include the b1 in our for-
mulation. If we modify our algorithm to multiply the
gradient by b1, then our algorithm, under the same as-
sumptions as Adam, will have a similar regret bound
of O(

T ) for a convex problem.

√

Algorithm 6: Exploration E (θt−1, ut−1, αt−1)
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt);
(cid:113)
ut = (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t +
θt = θt + gt (cid:12) ut;
Return (θt, ut, αt)

2 gt−1 η3/2/βt (cid:12) ζt;

Algorithm 7: Reﬁnement E (θt−1, ut−1, αt−1)
αt = αt−1;
ut = (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t;
θt = θt + gt (cid:12) ut;
Return (θt, ut, αt)

Because the focus of this paper is not on the regret
bound, we only brieﬂy discuss the changes in the the-
ory. We note that Lemma 10.4 from Kingma and Ba
(2015) will hold with element-wise b1.
Lemma 5. Let γi (cid:44) b2
1,i√
σ . For b1,i, σ ∈ [0, 1) that
satisfy β2
< 1 and bounded ˜ft, || ˜ft||2 ≤ G, || ˜ft||∞ ≤
1√
β2
G∞, the following inequality holds

T
(cid:88)

t=1

u2
i
(cid:112)tg2

i

≤

2
1 − γi

|| ˜f1:T,i||2

which contains an element-dependent γi compared to
Adam.

Theorem 10.5 of Kingma and Ba (2015) will hold with
the same modiﬁcations and assumptions for a b with
distinct entries; the proof in Kingma and Ba (2015)
is already element-wise, so it suﬃces to replace their
global parameter γ with distinct γi (cid:44) b2
1,i√
σ . This will
√
give a regret of O(

T ), the same as Adam.

G Additional Results

Figure 4: MNIST using FNN with size of 800.

Bridging the Gap between SG-MCMC and Stochastic Optimization

Learning curves of diﬀerent algorithms on MNIST us-
ing FNN with size of 800 are plotted in Figure 4.
Learning curves of diﬀerent algorithms on four poly-
phonic music datasets using RNN are shown in Fig-
ure 6.

We additionally test Santa on the ImageNet dataset.
We use the GoogleNet architecture, which is a 22
layer deep model. We use the default setting de-
ﬁned in the Caﬀe package8. We were not able to
make other stochastic optimization algorithms except
SGD with momentum and the proposed Santa work
on this dataset. Figure 5 shows the comparison on
this dataset. We did not tune the parameter setting,
note the default setting is favourable by SGD with
momentum. Nevertheless, Santa still signiﬁcantly out-
performs SGD with momentum in term of convergence
speed.

Figure 5: Santa vs. SGD with momentum on Ima-
geNet. We used ImageNet11 for training.

8https : //github.com/cchangyou/Santa/tree/master/caf f e/models/bvlc googlenet

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Figure 6: Learning curves of diﬀerent algorithms on four polyphonic music datasets using RNN.

Bridging the Gap between Stochastic Gradient MCMC
and Stochastic Optimization

6
1
0
2
 
g
u
A
 
5
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
6
9
7
0
.
2
1
5
1
:
v
i
X
r
a

Changyou Chen†

David Carlson‡

Zhe Gan†
†Department of Electrical and Computer Engineering, Duke University
‡Department of Statistics and Grossman Center for Statistics of Mind, Columbia University

Chunyuan Li†

Lawrence Carin†

Abstract

Stochastic gradient Markov chain Monte
Carlo (SG-MCMC) methods are Bayesian
analogs to popular stochastic optimization
methods; however, this connection is not
well studied. We explore this relationship
by applying simulated annealing to an SG-
MCMC algorithm. Furthermore, we extend
recent SG-MCMC methods with two key
components: i) adaptive preconditioners (as
in ADAgrad or RMSprop), and ii) adaptive
element-wise momentum weights. The zero-
temperature limit gives a novel stochastic
optimization method with adaptive element-
wise momentum weights, while conventional
optimization methods only have a shared,
static momentum weight. Under certain as-
sumptions, our theoretical analysis suggests
the proposed simulated annealing approach
converges close to the global optima. Experi-
ments on several deep neural network models
show state-of-the-art results compared to re-
lated stochastic optimization algorithms.

1 Introduction

Machine learning has made signiﬁcant recent strides
due to large-scale learning applied to “big data”.
typically performed with
Large-scale learning is
stochastic optimization,
common
and the most
method is stochastic gradient descent (SGD) (Bottou,
2010). Stochastic optimization methods are devoted
to obtaining a (local) optima of an objective function.
Alternatively, Bayesian methods aim to compute the
expectation of a test function over the posterior dis-

Appearing in Proceedings of the 19th International Con-
ference on Artiﬁcial Intelligence and Statistics (AISTATS)
2016, Cadiz, Spain. JMLR: W&CP volume 41. Copyright
2016 by the authors.

tribution. At ﬁrst glance, these methods appear to be
distinct, independent approaches to learning. How-
ever, even the celebrated Gibbs sampler was ﬁrst in-
troduced to statistics as a simulated annealing method
for maximum a posteriori estimation (i.e., ﬁnding an
optima) (Geman and Geman, 1984).

Recent work on large-scale Bayesian learning has fo-
cused on incorporating the speed and low-memory
costs from stochastic optimization. These approaches
are referred to as stochastic gradient Markov chain
Monte Carlo (SG-MCMC) methods. Well-known SG-
MCMC methods include stochastic gradient Langevin
dynamics (SGLD) (Welling and Teh, 2011), stochastic
gradient Hamiltonian Monte Carlo (SGHMC) (Chen
et al., 2014), and stochastic gradient thermostats
(SGNHT) (Ding et al., 2014). SG-MCMC has become
increasingly popular in the literature due to practi-
cal successes, ease of implementation, and theoretical
convergence properties (Teh et al., 2014; Vollmer et al.,
2015; Chen et al., 2015).

There are obvious structural similarities between SG-
MCMC algorithms and stochastic optimization meth-
ods. For example, SGLD resembles SGD with additive
Gaussian noise. SGHMC resembles SGD with momen-
tum (Rumelhart et al., 1986), adding additive Gaus-
sian noise when updating the momentum terms (Chen
et al., 2014). These similarities are detailed in Section
2. Despite these structural similarities, the theory is
unclear on how additive Gaussian noise diﬀerentiates
a Bayesian algorithm from its optimization analog.

Just as classical sampling methods were originally used
for optimization (Geman and Geman, 1984), we di-
rectly address using SG-MCMC algorithms for opti-
mization. A major beneﬁt of adapting these schemes
is that Bayesian learning is (in theory) able to fully ex-
plore the parameter space. Thus it may ﬁnd a better
local optima, if not the global optima, for a non-convex
objective function.

Speciﬁcally, in this work we ﬁrst extend the recently
proposed multivariate stochastic gradient thermostat

Bridging the Gap between SG-MCMC and Stochastic Optimization

algorithm (Gan et al., 2015) with Riemannian infor-
mation geometry, which results in an adaptive pre-
conditioning and momentum scheme with analogs to
Adam (Kingma and Ba, 2015) and RMSprop (Tiele-
man and Hinton, 2012). We propose an annealing
scheme on the system temperature to move from a
Bayesian method to a stochastic optimization method.
We call the proposed algorithm Stochastic AnNeal-
ing Thermostats with Adaptive momentum (Santa).
We show that in the temperature limit, Santa recov-
ers the SGD with momentum algorithm except that: i)
adaptive preconditioners are used when updating both
model and momentum parameters; ii) each parame-
ter has an individual, learned momentum parameter.
Adaptive preconditioners and momentums are desir-
able in practice because of their ability to deal with
uneven, dynamic curvature (Dauphin et al., 2015). For
completeness, we ﬁrst review related algorithms in Sec-
tion 2, and present our novel algorithm in Section 3.

We develop theory to analyze convergence properties
of our algorithm, suggesting that Santa is able to ﬁnd
a solution for an (non-convex) objective function close
to its global optima, shown in Section 4. The theory is
based on the analysis from stochastic diﬀerential equa-
tions (Teh et al., 2014; Chen et al., 2015), and presents
results on bias and variance of the annealed Markov
chain. This is a fundamentally diﬀerent approach from
the traditional convergence explored in stochastic opti-
mization, or the regret bounds used in online optimiza-
tion. We note we can adapt the regret bound of Adam
(Kingma and Ba, 2015) for our zero-temperature al-
gorithm (with a few trivial modiﬁcations) for a con-
vex problem, as shown in Supplementary Section F.
However, this neither addresses non-convexity nor the
annealing scheme that our analysis does.

In addition to theory, we demonstrate eﬀective empir-
ical performance on a variety of deep neural networks
(DNNs), achieving the best performance compared to
all competing algorithms for the same model size. This
is shown in Section 5. The code is publicly available
at https://github.com/cchangyou/Santa.

2 Preliminaries

Throughout this paper, we denote vectors as bold,
lower-case letters, and matrices as bold, upper-case
letters. We use (cid:12) for element-wise multiplication, and
· denotes the element-
(cid:11) as element-wise division;
wise square root when applied to vectors or matricies.
We reserve (·)1/2 for the standard matrix square root.
Ip is the p × p identity matrix, 1 is an all-ones vector.

√

The goal of an optimization algorithm is to min-
imize an objective function U (θ) that corresponds
to a (non-convex) model of interest.
In a Bayesian
model, this corresponds to the potential energy de-

ﬁned as the negative log-posterior, U (θ) (cid:44) − log p(θ)−
(cid:80)N
n=1 log p(xn |θ). Here θ ∈ Rp are the model param-
eters, and {xn}n=1,...,N are the d-dimensional observed
data; p(θ) corresponds to the prior and p(xn |θ) is a
likelihood term for the nth observation. In optimiza-
tion, − (cid:80)N
n=1 log p(xn |θ) is typically referred to as the
loss function, and − log p(θ) as a regularizer.

In large-scale learning, N is prohibitively large.
This motivates the use of stochastic approximations.
We denote the stochastic approximation ˜Ut(θ) (cid:44)
− log p(θ) − N
j=1 log p(xij |θ), where (i1, · · · , im)
m
is a random subset of the set {1, 2, · · · , N }. The gra-
dient on this minibatch is denoted as ˜f t(θ) = ∇ ˜Ut(θ),
which is an unbiased estimate of the true gradient.

(cid:80)m

A standard approach to learning is SGD, where param-
˜f t−1(θ) with
eter updates are given by θt = θt−1 − ηt
ηt the learning rate. This is guaranteed to converge to
a local minima under mild conditions (Bottou, 2010).
The SG-MCMC analog to this is SGLD, with updates
2ηtζt. The additional term
θt = θt−1 − ηt
is a standard normal random vector, ζt ∼ N (0, Ip)
(Welling and Teh, 2011). The SGLD method draws
approximate posterior samples instead of obtaining a
local minima.

˜f t−1(θ) +

√

Using momentum in stochastic optimization is impor-
tant in learning deep models (Sutskever et al., 2013).
This motivates SG-MCMC algorithms with momen-
tum. The standard SGD with momentum (SGD-M)
approach introduces an auxiliary variable ut ∈ Rp
to represent the momentum. Given a momentum
weight α, the updates are θt = θt−1 + ηtut and
ut = (1 − α)ut−1 − ˜f t−1(θ). A Bayesian analog is
SGHMC (Chen et al., 2014) or multivariate SGNHT
(mSGNHT) (Gan et al., 2015).
In mSGNHT, each
parameter has a unique momentum weight αt ∈ Rp
that is learned during the sampling sequence. The
momentum weights are updated to maintain the sys-
tem temperature 1/β. An inverse temperature of
β = 1 corresponds to the posterior. This algorithm
has updates θt = θt−1 + ηtut, ut = (1 − ηtαt−1) (cid:12)
˜f t−1(θ) + (cid:112)2ηt/βζt. The main diﬀerence
ut−1 − ηt
is the additive Gaussian noise and step-size depen-
dent momentum update. The weights have updates
αt = αt−1 + ηt((ut (cid:12) ut) − 1/β), which matches the
kinetic energy to the system temperature.

A recent idea in stochastic optimization is to use
an adaptive preconditioner, also known as a variable
metric, to improve convergence rates. Both ADA-
grad (Duchi et al., 2011) and Adam (Kingma and Ba,
2015) adapt to the local geometry with a regret bound
N ). Adam adds momentum as well through
of O(
moment smoothing. RMSprop (Tieleman and Hin-
ton, 2012), Adadelta (Zeiler, 2012), and RMSspectral

√

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Algorithm 1: Santa with the Euler scheme
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t (cid:44) ∇θ ˜U (θt−1) on the tth mini-batch;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
if t < burnin then
/* exploration
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt);
(cid:1) (cid:11) ut−1 +
ut = η
βt

(cid:0)1 − gt−1 (cid:11) gt

(cid:113) 2η
βt

gt−1 (cid:12) ζt

*/

else

/* refinement
αt = αt−1; ut = 0;

end
ut = ut + (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t;
θt = θt−1 + gt (cid:12) ut;

end

*/

(Carlson et al., 2015) are similar methods with pre-
conditioners. Our method introduces adaptive mo-
mentum and preconditioners to the SG-MCMC. This
diﬀers from stochastic optimization in implementation
and theory, and is novel in SG-MCMC.
Simulated annealing (Kirkpatrick et al., 1983; ˇCern´y,
1985) is well-established as a way of acquiring a local
mode by moving from a high-temperature, ﬂat surface
to a low-temperature, peaky surface. It has been ex-
plored in the context of MCMC, including reversible
jump MCMC (Andrieu et al., 2000), annealed impor-
tant sampling (Neal, 2001) and parallel tempering (Li
et al., 2009). Traditional algorithms are based on
Metropolis–Hastings sampling, which require compu-
tationally expensive accept-reject steps. Recent work
has applied simulated annealing to large-scale learn-
ing through mini-batch based annealing (van de Meent
et al., 2014; Obermeyer et al., 2014). Our approach in-
corporates annealing into SG-MCMC with its inherent
speed and mini-batch nature.

3 The Santa Algorithm

Santa extends the mSGNHT algorithm with precon-
ditioners and a simulated annealing scheme. A simple
pseudocode is shown in Algorithm 1, or a more com-
plex, but higher accuracy version, is shown in Algo-
rithm 2, and we detail the steps below.

The ﬁrst extension we consider is the use of adaptive
preconditioners. Preconditioning has been proven crit-
ical for fast convergence in both stochastic optimiza-

tion (Dauphin et al., 2015) and SG-MCMC algorithms
(Patterson and Teh, 2013). In the MCMC literature,
preconditioning is alternatively referred to as Rieman-
nian information geometry (Patterson and Teh, 2013).
We denote the preconditioner as {Gt ∈ Rp×p}. A pop-
ular choice in SG-MCMC is the Fisher information ma-
trix (Girolami and Calderhead, 2011). Unfortunately,
this approach is computationally prohibitive for many
models of interest. To avoid this problem, we adopt
the preconditioner from RMSprop and Adam, which
uses a vector {gt ∈ Rp} to approximate the diagonal
of the Fisher information matrixes (Li et al., 2016a).
The construction sequentially updates the precondi-
tioner based on current and historical gradients with
a smoothing parameter σ, and is shown as part of Al-
gorithm 1. While this approach will not capture the
Riemannian geometry as eﬀectively as the Fisher in-
formation matrix, it is computationally eﬃcient.

Santa also introduces an annealing scheme on sys-
tem temperatures. As discussed in Section 2, mS-
GNHT naturally accounts for a varying temperature
by matching the particle momentum to the system
temperature. We introduce β = {β1, β2, · · · }, a se-
quence of inverse temperature variables with βi < βj
for i < j and limi→∞ βi = ∞. The inﬁnite case
corresponds to the zero-temperature limit, where SG-
MCMCs become deterministic optimization methods.

The annealing scheme leads to two stages:
the ex-
ploration and the reﬁnement stages. The exploration
stage updates all parameters based on an annealed se-
quence of stochastic dynamic systems (see Section 4
for more details). This stage is able to explore the
parameter space eﬃciently, escape poor local modes,
and ﬁnally converge close to the global mode. The re-
ﬁnement stage corresponds to the temperature limit,
i.e., βn → ∞. In the temperature limit, the momen-
tum weight updates vanish and it becomes a stochastic
optimization algorithm.

We propose two update schemes to solve the corre-
sponding stochastic diﬀerential equations: the Euler
scheme and the symmetric splitting scheme (SSS). The
Euler scheme has simpler updates, as detailed in Algo-
rithm 1; while SSS endows increased accuracy (Chen
et al., 2015) with a slight increase in overhead compu-
tation, as shown in Algorithm 2. Section 4.1 elaborates
on the details of these two schemes. We recommend
the use of SSS, but the Euler scheme is simpler to im-
plement and compare to known algorithms.

Practical considerations According to Section 4,
the exploration stage helps the algorithm traverse the
parameter space following the posterior curve as ac-
curate as possible. For optimization, slightly biased
samples do not aﬀect the ﬁnal solution. As a result,

Bridging the Gap between SG-MCMC and Stochastic Optimization

Algorithm 2: Santa with SSS
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t (cid:44) ∇θ ˜U (θt−1) on the tth mini-batch;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
θt = θt−1 + gt (cid:12) ut−1 /2;
if t < burnin then
/* exploration
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt) /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη + (cid:112)2 gt−1 η/βt (cid:12) ζt

*/

+ η/βt

(cid:0)1 − gt−1 (cid:11) gt
ut = exp (−αt/2) (cid:12) ut;
αt = αt + (ut (cid:12) ut −η/βt) /2;

(cid:1) (cid:11) ut−1;

else

/* refinement
αt = αt−1;
ut = ut − gt (cid:12)˜f tη; ut = exp (−αt/2) (cid:12) ut;

ut = exp (−αt/2) (cid:12) ut−1;

*/

end
θt = θt + gt (cid:12) ut /2;

end

the term consisting of (1 − gt−1 (cid:11) gt) in the algorithm
(which is an approximation term, see Section 4.1) is
ignored. We found no decreasing performance in our
experiments. Furthermore, the term gt−1 associated
with the Gaussian noise could be replaced with a ﬁxed
constant without aﬀecting the algorithm.

4 Theoretical Foundation

In this section we present the stochastic diﬀerential
equations (SDEs) that correspond to the Santa algo-
rithm. We ﬁrst introduce the general SDE framework,
then describe the exploration stage in Section 4.1 and
the reﬁnement stage in Section 4.2. We give the con-
vergence properties of the numerical scheme in Section
4.3. This theory uses tools from the SDE literature and
extends the mSGNHT theory (Ding et al., 2014; Gan
et al., 2015).

The SDEs are presented with re-parameterized p =
u /η1/2, Ξ = diag(α)/η1/2, as in Ding et al. (2014).
The SDEs describe the motion of a particle in a system
where θ is the location and p is the momentum.

In mSGNHT, the particle is driven by a force
−∇θ ˜Ut(θ) at time t. The stationary distribution of θ
corresponds to the model posterior (Gan et al., 2015).
Our critical extension is the use of Riemannian infor-

mation geometry, important for fast convergence (Pat-
terson and Teh, 2013). Given an inverse temperature
β, the system is described by the following SDEs1:






dθ = G1(θ) p dt
(cid:16)
d p =

−G1(θ)∇θU (θ) − Ξ p + 1

β ∇θG1(θ)

+G1(θ)(Ξ − G2(θ))∇θG2(θ)) dt + ( 2
dt ,

dΞ =

(cid:17)

(cid:16)

Q − 1
β I

β G2(θ))

1
2 dw

(1)

where Q = diag(p (cid:12) p), w is standard Brownian mo-
tion, G1(θ) encodes geometric information of the po-
tential energy U (θ), and G2(θ) characterizes the man-
ifold geometry of the Brownian motion. Note G2(θ)
may be the same as G1(θ) for the same Riemannian
manifold. We call G1(θ) and G2(θ) Riemannian met-
rics, which are commonly deﬁned by the Fisher infor-
mation matrix (Girolami and Calderhead, 2011). We
use the RMSprop preconditioner (with updates from
Algorithm 1) for computational feasibility. Using the
Fokker-Plank equation (Risken, 1989), we show that
the marginal stationary distribution of (1) corresponds
to the posterior distribution.
AT B
Lemma 1. Denote A : B (cid:44) tr
ary distribution of (1) is: pβ(θ, p, Ξ) ∝

.The station-

(cid:110)

(cid:111)

e−βU (θ)− β

2 pT p − β

2 (Ξ−G2(θ)):(Ξ−G2(θ)) .

(2)

An inverse temperature β = 1 corresponds to the stan-
dard Bayesian posterior.

We note that p in (1) has additional dependencies on
G1 and G2 compared to Gan et al. (2015) that must be
accounted for. Ξ p introduces friction into the system
so that the particle does not move too far away by
the random force; the terms ∇θG1(θ) and ∇θG2(θ)
penalize the inﬂuences of the Riemannian metrics so
that the stationary distribution remains invariant.

4.1 Exploration

The ﬁrst stage of Santa, exploration, explores the pa-
rameter space to obtain parameters near the global
mode of an objective function2. This approach applies
ideas from simulated annealing (Kirkpatrick et al.,
1983). Speciﬁcally, the inverse temperature β is slowly
annealed to temperature zero to freeze the particles at
the global mode.

Minimizing U (θ) is equivalent to sampling from the
zero-temperature limit pβ(θ) (cid:44) 1
e−βU (θ) (propor-
Zβ

1We abuse notation for conciseness. Here, ∇θ G(θ) is

a vector with the i-th element being (cid:80)

j ∇θj Gij(θ).

2This requires an ergodic algorithm. While ergodicity is
not straightforward to check, we follow most MCMC work
and assume it holds in our algorithm.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

tional to (2)), with Zβ being the normalization con-
stant such that pβ(θ) is a valid distribution. We
construct a Markov chain that sequentially transits
from high temperatures to low temperatures. At the
state equilibrium, the chain reaches the temperature
limit with marginal stationary distribution ρ0(θ) (cid:44)
limβ→∞ e−βU (θ), a point mass3 located at the global
mode of U (θ). Speciﬁcally, we ﬁrst deﬁne a sequence
of inverse temperatures, (β1, β2, · · · , βL), such that βL
is large enough4. For each time t, we generate a sam-
ple according to the SDE system (1) with temperature
1
, conditioned on the sample from the previous tem-
βt
. We call this procedure annealing ther-
perature,
mostats to denote the analog to simulated annealing.

1
βt−1

Generating approximate samples Generating
exact samples from (1) is infeasible for general mod-
els. One well-known numerical approach is the Euler
scheme in Algorithm 1. The Euler scheme is a 1st-
order method with relatively high approximation error
(Chen et al., 2015). We increase accuracy by imple-
menting the symmetric splitting scheme (SSS) (Chen
et al., 2015; Li et al., 2016b). The idea of SSS is to split
an infeasible SDE into several sub-SDEs, where each
sub-SDE is analytically solvable; approximate samples
are generated by sequentially evolving parameters via
these sub-SDEs. Speciﬁcally, in Santa, we split (1)
into the following three sub-SDEs:

A :

O :









dθ = G1(θ) p dt
d p = 0
(cid:16)
dΞ =

Q − 1
β I

(cid:17)

dt

, B :






dθ = 0
d p = −Ξ p dt
dΞ = 0

,

dθ = 0
(cid:16)
d p =
+G1(θ)(Ξ − G2(θ))∇θG2(θ)) dt + ( 2
dΞ = 0

−G1(θ)∇θU (θ) + 1

β ∇θG1(θ)

β G2(θ))

1
2 dw

We then update the sub-SDEs in order A-B-O-B-A
to generative approximate samples (Chen et al., 2015).
This uses half-steps h/2 on the A and B updates5,
and full steps h in the O update. This is analogous to
the leapfrog steps in Hamiltonian Monte Carlo (Neal,
2011). Update equations are given in the Supplemen-
tary Section A. The resulting parameters then serve
as an approximate sample from the posterior distribu-
tion with the inverse temperature of β. Replacing G1
and G2 with the RMSprop preconditioners gives Al-
gorithm 2. These updates require approximations to
∇θG1(θ) and ∇θG2(θ), addressed below.

3The sampler samples a uniform distribution over global
modes, or a point mass if the mode is unique. We assume
uniqueness and say point mass for clarity henceforth.

4Due to numerical issues, it is impossible to set βL to
inﬁnity; we thus assign a large enough value for it and
handle the inﬁnity case in the reﬁnement stage.
5As in Ding et al. (2014), we deﬁne h =
η.

√

Approximate calculation for ∇θ G1(θ) We pro-
pose a computationally eﬃcient approximation for
calculating the derivative vector ∇θ G1(θ) based
on the deﬁnition.
for the i-th ele-
ment of ∇θ G1(θ) at the t-th iteration, denoted as
(∇θ Gt

1(θ))i, it is approximated as:

Speciﬁcally,

(∇θ Gt

1(θ))i

A1≈

(cid:88)

(Gt

1(θ))ij − (Gt−1
θtj − θ(t−1)j

1

(θ))ij

j
(∆ Gt
1)ij
1(θ) pt−1)jh

(cid:88)

A2=

(Gt

j

(cid:88)

=

j

(∆ Gt
1)ij
1(θ) ut−1)j

(Gt

(cid:44) Gt

1 − Gt−1

where ∆ Gt
. Step A1 follows by the
1
deﬁnition of a derivative, and A2 by using the update
equation for θt, i.e., θt = θt−1+Gt
1(θ) pt−1 h. Accord-
ing to Taylor’s theory, the approximation error for the
∇θ G1(θ) is O(h), e.g.,

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

i

j

(∆ Gt
1)ij
1(θ) ut−1)j

(Gt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− (∇θ Gt

1(θ))i

≤ Bth ,

(3)

for some positive constant Bt. The approximation error
is negligible in term of convergence behaviors because
it can be absorbed into the stochastic gradients error.
Formal theoretical analysis on convergence behaviors
with this approximation is given in later sections. Us-
ing similar methods, ∇θGt
2(θ) is also approximately
calculated.

4.2 Reﬁnement

stage

reﬁnement

corresponds

zero-
The
temperature limit of the exploration stage, where Ξ
is learned. We show that in the limit Santa gives sig-
niﬁcantly simpliﬁed updates, leading to an stochastic
optimization algorithm similar to Adam or SGD-M.

to the

We assume that the Markov chain has reached its
equilibrium after the exploration stage.
In the zero-
temperature limit, some terms in the SDE (1) vanish.
First, as β → ∞, the term 1
β ∇θ G1(θ) and the vari-
ance term for the Brownian motion approach 0. As
well, the thermostat variable Ξ approaches G2(θ), so
the term G1(θ)(Ξ − G2(θ))∇θ G2(θ) vanishes. The
stationary distribution in (2) implies E Qii
i →
0, which makes the SDE for Ξ in (1) vanish. As a re-
sult, in the reﬁnement stage, only θ and p need to be
updated. The Euler scheme for this is shown in Algo-
rithm 1, and the symmetric splitting scheme is shown
in Algorithm 2.

(cid:44) E p2

Relation to stochastic optimization algorithms
In the reﬁnement stage Santa is a stochastic optimiza-
tion algorithm. This relation is easier seen with the
Euler scheme in Algorithm 1. Compared with SGD-M
(Rumelhart et al., 1986), Santa has both adaptive gra-
dient and adaptive momentum updates. Unlike Ada-
grad (Duchi et al., 2011) and RMSprop (Tieleman and

Bridging the Gap between SG-MCMC and Stochastic Optimization

Hinton, 2012), reﬁnement Santa is a momentum based
algorithm.

The recently proposed Adam algorithm (Kingma and
Ba, 2015) incorporates momentum and precondition-
ing in what is denoted as “adaptive moments.” We
show in Supplementary Section F that a constant step
size combined with a change of variables nearly recov-
ers the Adam algorithm with element-wise momentum
weights. For these reasons, Santa serves as a more gen-
eral stochastic optimization algorithm that extends all
current algorithms. As well, for a convex problem and
a few trivial algorithmic changes, the regret bound of
T ),
Adam holds for reﬁnement Santa, which is O(
as detailed in Supplementary Section F. However, our
analysis is focused on non-convex problems that do not
ﬁt in the regret bound formulation.

√

4.3 Convergence properties

Our convergence properties are based on the frame-
work of Chen et al. (2015). The proofs for all the-
orems are given in the Supplementary Material. We
focus on the exploration stage of the algorithm. Us-
ing the Monotone Convergence argument (Schechter,
1997), the reﬁnement stage convergence is obtained by
taking the temperature limit from the results of the ex-
ploration stage. We emphasize that our approach dif-
fers from conventional stochastic optimization or on-
line optimization approaches. Our convergence rate
is weaker than many stochastic optimization methods,
including SGD; however, our analysis applies to non-
convex problems, whereas traditionally convergence
rates only apply to convex problems.

The goal of Santa is to obtain θ∗ such that θ∗ =
argminθ U (θ). Let {θ1, · · · , θL} be a sequence of pa-
rameters collected from the algorithm. Deﬁne ˆU (cid:44)
t=1 U (θt) as the sample average, ¯U (cid:44) U (θ∗) the
1
L
global optima of U (θ).

(cid:80)L

As in Chen et al. (2015), we require certain assump-
tions on the potential energy U . To show these as-
sumptions, we ﬁrst deﬁne a functional ψt for each t
that solves the following Poisson equation:

Ltψt(θt) = U (θt) − ¯U ,

(4)

Lt is the generator of the SDE system (1) in the t-th
E[f (xt+h)]−f (xt)
iteration, deﬁned Ltf (xt) (cid:44) limh→0+
h
where xt (cid:44) (θt, pt, Ξt), f : R3p → R is a compactly
supported twice diﬀerentiable function. The solution
functional ψt(θt) characterizes the diﬀerence between
U (θt) and the global optima ¯U for every θt. As shown
in Mattingly et al. (2010), (4) typically possesses a
unique solution, which is at least as smooth as U under
the elliptic or hypoelliptic settings. We assume ψt is
bounded and smooth, as described below.

Assumption 1. ψt and its up to 3rd-order deriva-
tives, Dkψt, are bounded by a function V(θ, p, Ξ),
i.e., (cid:107)Dkψ(cid:107) ≤ CkV rk for k = (0, 1, 2, 3), Ck, rk >
0. Furthermore, the expectation of V is bounded:
EV r(θ, p, Ξ) < ∞, and V is smooth such that
supt
sups∈(0,1) V r (sx + (1 − s) y) ≤ C (V r (x) + V r (y)),
∀x ∈ R3p, y ∈ R3p, r ≤ max{2rk} for some C > 0.

(cid:17)

Let ∆U (θ) (cid:44) U (θ) − U (θ∗). Further deﬁne an opera-
(cid:16)
G1(θ)(∇θ ˜Ut − ∇θU ) + Bt
· ∇p for each
tor ∆Vt =
βt
t, where Bt is from (3). Theorem 2 depicts the close-
ness of ˆU to the global optima ¯U in term of bias and
mean square error (MSE) deﬁned below.
Theorem 2. Let (cid:107)·(cid:107) be the operator norm. Under
Assumption 1, the bias and MSE of the exploration
stage in Santa with respect to the global optima for L
steps with stepsize h is bounded, for some constants
C > 0 and D > 0, with:

Bias:

E ˆU − ¯U

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ Ce−U (θ∗)
(cid:12)
(cid:18) 1
Lh

+ D

L
(cid:88)

(cid:90)

1
L
(cid:80)

t=1

(cid:32)

+

MSE: E

(cid:16) ˆU − ¯U

(cid:17)2

≤ C 2e−2U (θ∗)

(cid:33)

e−βt∆U (θ)dθ

t (cid:107)E∆Vt(cid:107)
L

(cid:19)

+ h2

.

L
(cid:88)

(cid:90)

(cid:32)

1
L

t=1
E (cid:107)∆Vt(cid:107)2
L

(cid:33)2

e−βt∆U (θ)dθ

(cid:33)

+ h4

.

+

1
Lh

(cid:32) 1
L

(cid:80)
t

+ D2

Both bounds for the bias and MSE have two parts.
The ﬁrst part contains integration terms, which char-
acterizes the distance between the global optima,
e−U (θ∗), and the unnormalized annealing distribu-
tions, e−βtU (θ), decreasing to zero exponentially fast
with increasing β; the remaining part characterizes the
distance between the sample average and the anneal-
ing posterior average. This shares a similar form as in
general SG-MCMC algorithms (Chen et al., 2015), and
can be controlled to converge. Furthermore, the term
(cid:80)
in the bias vanishes as long as the sum of
the annealing sequence {βt} is ﬁnite6, indicating that
the gradient approximation for ∇θG1(θ) in Section 4.1
does not aﬀect the bias of the algorithm. Similar ar-
guments apply for the MSE bound.

t(cid:107)E∆Vt(cid:107)
L

(cid:80)L+m−1
l=m

To get convergence results right before the reﬁnement
stage, let a sequence of functions {gm} be deﬁned as
e−βl ˆU (θ); it is easy to see that {gm}
gm (cid:44) − 1
L
satisﬁes gm1 < gm2 for m1 < m2, and limm→∞ gm =
0. According to the Monotone Convergence Theorem
(Schechter, 1997), the bias and MSE in the limit exists,
leading to Corollary 3.
Corollary 3. Under Assumptions 1, the bias and
MSE of the reﬁnement stage in Santa with respect

6In practice we might not need to care about this con-
straint because a small bias in the exploration stage does
not aﬀect convergence of the reﬁnement stage.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

to the global optima for L steps with stepsize h are
bounded, for some constants D1 > 0, D2 > 0, as
(cid:18) 1
Lh
(cid:32) 1
L

(cid:12)
(cid:12)
(cid:12) ≤ D1

E ˆU − ¯U

+ h2

Bias:

(cid:80)
t

(cid:80)

(cid:17)2

+

(cid:19)

(cid:12)
(cid:12)
(cid:12)

MSE: E

(cid:16) ˆU − ¯U

t (cid:107)E∆Vt(cid:107)
L
E (cid:107)∆Vt(cid:107)2
L

+

1
Lh

≤ D2

+ h4

(cid:33)

Corollary 3 implies that in the reﬁnement stage, the
discrepancy between annealing distributions and the
global optima vanishes, leaving only errors from dis-
cretized simulations of the SDEs, similar to the result
of general SG-MCMC (Chen et al., 2015). We note
that after exploration, Santa becomes a pure stochas-
tic optimization algorithm, thus convergence results
in term of regret bounds can also be derived; refer to
Supplementary Section F for more details.

5 Experiments

5.1

Illustration

In order to demonstrate that Santa is able to achieve
the global mode of an objective function, we consider
the double-well potential (Ding et al., 2014),

U (θ) = (θ + 4)(θ + 1)(θ − 1)(θ − 3)/14 + 0.5 .

As shown in Figure 1 (left), the double-well potential
has two modes, located at θ = −3 and θ = 2, with the
global optima at θ = −3. We use a decreasing learning
rate ht = t−0.3/10, and the annealing sequence is set to
βt = t2. To make the optimization more challenging,
we initialize the parameter at θ0 = 4, close to the local
mode. The evolution of θ with respect to iterations is
shown in Figure 1(right). As can be seen, θ ﬁrst moves
to the local mode but quickly jumps out and moves
to the global mode in the exploration stage (ﬁrst half
iterations); in the reﬁnement stage, θ quickly converges
to the global mode and sticks to it afterwards.
In
contrast, RMSprop is trapped on the local optima, and
convergences slower than Santa at the beginning.

Figure 1: (Left) Double-well potential. (Right) The
evolution of θ using Santa and RMSprop algorithms.

Figure 2: Learning curves of diﬀerent algorithms on
MNIST. (Left) FNN with size of 400. (Right) CNN.

5.2 Feedforward neural networks

We ﬁrst test Santa on the Feedforward Neural Net-
work (FNN) with rectiﬁed linear units (ReLU). We
test two-layer models with network sizes 784-X-X-10,
where X is the number of hidden units for each layer;
100 epochs are used. For variants of Santa, we de-
note Santa-E as Santa with a Euler scheme illustrated
in Algorithm 1, Santa-r as Santa running only on the
reﬁnement stage, but with updates on α as in the ex-
ploration stage. We compare Santa with SGD, SGD-
M, RMSprop, Adam, SGD with dropout, SGLD and
Bayes by Backprop (Blundell et al., 2015). We use
a grid search to obtain good learning rates for each
algorithm, resulting in 4 × 10−6 for Santa, 5 × 10−4
for RMSprop, 10−3 for Adam, and 5 × 10−1 for SGD,
SGD-M and SGLD. We choose an annealing schedule
of βt = Atγ with A = 1 and γ selected from 0.1 to 1
with an interval of 0.1. For simplicity, the exploration
is set to take half of total iterations.

We test the algorithms on the standard MNIST
dataset, which contains 28 × 28 handwritten digital
images from 10 classes with 60, 000 training samples
and 10, 000 test samples. The network size (X-X) is
set to 400-400 and 800-800, and test classiﬁcation er-
rors are shown in Table 1. Santa show improved state-
of-the-art performance amongst all algorithms. The
Euler scheme shows a slight decrease in performance,
due to the integration error when solving the SDE.
Santa without exploration (i.e., Santa-r) still performs
relatively well. Learning curves are plotted in Fig-
ure 2, showing that Santa converges as fast as other
algorithms but to a better local optima7.

5.3 Convolution neural networks

We next test Santa on the Convolution Neural Net-
work (CNN). Following Jarrett et al. (2009), a stan-
dard network conﬁguration with 2 convolutional layers
followed by 2 fully-connected layers is adopted. Both
convolutional layers use 5 × 5 ﬁlter size with 32 and 64

7Learning curves of FNN with size of 800 are provided

in Supplementary Section G.

Bridging the Gap between SG-MCMC and Stochastic Optimization

Algorithms
Santa
Santa-E
Santa-r
Adam
RMSprop
SGD-M
SGD
SGLD
BPB(cid:5)
SGD, Dropout(cid:5)
Stoc. Pooling(cid:46)
NIN, Dropout◦
Maxout, Dropout(cid:63)

FNN-400 FNN-800
1.21%
1.41%
1.45%
1.53%
1.59%
1.66%
1.72%
1.64%
1.32%
1.51%
−
−
−

CNN
1.16% 0.47%
0.58%
1.27%
0.49%
1.40%
0.59%
1.47%
0.64%
1.43%
0.77%
1.72%
0.81%
1.47%
0.71%
1.41%
−
1.34%
−
1.33%
0.47%
−
0.47%
−
0.45%
−

Table 1: Test error on MNIST classiﬁcation using FNN
and CNN. ((cid:5)) taken from Blundell et al. (2015). ((cid:46)) taken
from Zeiler and Fergus (2013). (◦) taken from Lin et al.
(2014). ((cid:63)) taken from Goodfellow et al. (2013).

channels, respectively; 2 × 2 max pooling is used af-
ter each convolutional layer. The fully-connected lay-
ers have 200-200 hidden nodes with ReLU activation.
The same parameter setting and dataset as in the FNN
are used. The test errors are shown in Table 1, and
the corresponding learning curves are shown in Fig-
ure 2. Similar trends as in FNN are obtained. Santa
signiﬁcantly outperforms other algorithms with an er-
ror of 0.45%. This result is comparable or even better
than some recent state-of-the-art CNN-based systems,
which have much more complex architectures.

5.4 Recurrent neural networks

We test Santa on the Recurrent Neural Network
is
(RNN) for sequence modeling, where a model
trained to minimize the negative log-likelihood of
training sequences:

min
θ

1
N

N
(cid:88)

Tn(cid:88)

n=1

t=1

− log p(xn

t | xn

1 , . . . , xn

t−1; θ)

(5)

where θ is a set of model parameters, {xn
t } is the
observed data. The conditional distributions in (5)
are modeled by the RNN. The hidden units are set to
gated recurrent units (Cho et al., 2014).

We consider the task of sequence modeling on four
diﬀerent polyphonic music sequences of piano, i.e.,
Piano-midi.de (Piano), Nottingham (Nott), MuseData
(Muse) and JSB chorales (JSB). Each of these datasets
are represented as a collection of 88-dimensional bi-
nary sequences, that span the whole range of piano
from A0 to C8.

The number of hidden units is set to 200. Each model
is trained for at most 100 epochs. According to the
experiments and their results on the validation set, we

Figure 3: Learning curves of diﬀerent algorithms on Piano
using RNN. (Left) training set. (Right) validation set.

Algorithms Piano. Nott. Muse.
7.20
7.56
7.22
7.69
10.08
7.19
8.13

Santa
Adam
RMSprop
SGD-M
SGD
HF(cid:5)
SGD-M(cid:5)

7.60
8.00
7.70
8.32
11.13
7.66
8.37

3.39
3.70
3.48
3.60
5.26
3.89
4.46

JSB.
8.46
8.51
8.52
8.59
10.81
8.58
8.71

Table 2: Test negative log-likelihood results on poly-
phonic music datasets using RNN. ((cid:5)) taken from
Boulanger-Lewandowski et al. (2012).

use a learning rate of 0.001 for all the algorithms. For
Santa, we consider an additional experiment using a
learning rate of 0.0002, denoted Santa-s. The anneal-
ing coeﬃcient γ is set to 0.5. Gradients are clipped
if the norm of the parameter vector exceeds 5. We
do not perform any dataset-speciﬁc tuning other than
early stopping on validation sets. Each update is done
using a minibatch of one sequence.

The best log-likelihood results on the test set are
achieved by using Santa, shown in Table 2. Learn-
ing curves on the Piano dataset are plotted in Fig-
ure 3. We observe that Santa achieves fast conver-
gence, but is overﬁtting. This is straightforwardly ad-
dressed through early stopping. The learning curves
for all the other datasets are provided in Supplemen-
tary Section G.

6 Conclusions

We propose Santa, an annealed SG-MCMC method
for stochastic optimization. Santa is able to explore
the parameter space eﬃciently and locate close to the
global optima by annealing. At the zero-temperature
limit, Santa gives a novel stochastic optimization algo-
rithm where both model parameters and momentum
are updated element-wise and adaptively. We provide
theory on the convergence of Santa to the global op-
tima for an (non-convex) objective function. Experi-
ments show best results on several deep models com-
pared to related stochastic optimization algorithms.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Acknowledgements

This research was supported in part by ARO, DARPA,
DOE, NGA, ONR and NSF.

References

C. Andrieu, N. de Freitas, and A. Doucet. Reversible
jump mcmc simulated annealing for neural net-
works. In UAI, 2000.

C. Blundell, J. Cornebise, K. Kavukcuoglu, and
D. Wierstra. Weight uncertainty in neural networks.
In ICML, 2015.

L. Bottou. Large-scale machine learning with stochas-
tic gradient descent. In Proc. COMPSTAT, 2010.

N. Boulanger-Lewandowski, Y. Bengio, and P. Vin-
cent. Modeling temporal dependencies in high-
dimensional sequences: Application to polyphonic
music generation and transcription. In ICML, 2012.

D. E. Carlson, E. Collins, Y.-P. Hsieh, L. Carin, and
V. Cevher. Preconditioned spectral descent for deep
learning. In Advances in Neural Information Pro-
cessing Systems, pages 2953–2961, 2015.

C. Chen, N. Ding, and L. Carin. On the convergence
of stochastic gradient mcmc algorithms with high-
order integrators. In NIPS, 2015.

T. Chen, E. B. Fox, and C. Guestrin. Stochastic gra-
dient Hamiltonian Monte Carlo. In ICML, 2014.

K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Ben-
gio.
Learning phrase representations using rnn
encoder-decoder for statistical machine translation.
In arXiv:1406.1078, 2014.

Y. N. Dauphin, H. de Vries, and Y. Bengio. Equili-
brated adaptive learning rates for non-convex opti-
mization. In NIPS, 2015.

N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel,
and H. Neven. Bayesian sampling using stochastic
gradient thermostats. In NIPS, 2014.

J. Duchi, E. Hazan, and Y. Singer. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In JMLR, 2011.

Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin.
Scalable deep Poisson factor analysis for topic mod-
eling. In ICML, 2015.

I. Goodfellow, D. Warde-farley, M. Mirza,
A. Courville, and Y. Bengio. Maxout networks. In
ICML, 2013.

K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. Le-
Cun. What is the best multi-stage architecture for
object recognition? In ICCV, 2009.

D. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015.

S. Kirkpatrick, C. D. G. Jr, and M. P. Vecchi. Opti-
mization by simulated annealing. In Science, 1983.

C. Li, C. Chen, D. Carlson, and L. Carin. Precon-
ditioned stochastic gradient Langevin dynamics for
deep neural networks. In AAAI, 2016a.

C. Li, C. Chen, K. Fan, and L. Carin. High-order
stochastic gradient thermostats for Bayesian learn-
ing of deep models. In AAAI, 2016b.

Y. Li, V. A. Protopopescu, N. Arnold, X. Zhang,
and A. Gorin. Hybrid parallel tempering and sim-
ulated annealing method. In Applied Mathematics
and Computation, 2009.

M. Lin, Q. Chen, and S. Yan. Network in network. In

ICLR, 2014.

J. C. Mattingly, A. M. Stuart, and M. V. Tretyakov.
Construction of numerical time-average and station-
ary measures via Poisson equations.
In SIAM J.
NUMER. ANAL., 2010.

R. M. Neal. Annealed importance sampling. In Statis-

tics and Computing, 2001.

R. M. Neal. Mcmc using hamiltonian dynamics.
Handbook of Markov Chain Monte Carlo, 2011.

In

F. Obermeyer, J. Glidden, and E. Jonas. Scaling
nonparametric bayesian inference via subsample-
annealing. In AISTATS, 2014.

S. Patterson and Y. W. Teh. Stochastic gradient Rie-
mannian Langevin dynamics on the probability sim-
plex. In NIPS, 2013.

H. Risken. The Fokker-Planck equation. Springer-

Verlag, New York, 1989.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
Learning representations by back-propagating er-
rors. In Nature, 1986.

E. Schechter. Handbook of Analysis and Its Founda-

tions. Elsevier, 1997.

S. Geman and D. Geman. Stochastic relaxation, gibbs
distributions, and the bayesian restoration of im-
ages. In PAMI, 1984.

I. Sutskever, J. Martens, G. Dahl, and G. E. Hinton.
On the importance of initialization and momentum
in deep learning. In ICML, 2013.

M. Girolami and B. Calderhead. Riemann manifold
Langevin and Hamiltonian Monte Carlo methods.
In JRSS, 2011.

Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Con-
sistency and ﬂuctuations for stochastic gradient
Langevin dynamics. In arXiv:1409.0578, 2014.

Bridging the Gap between SG-MCMC and Stochastic Optimization

T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop:
Divide the gradient by a running average of its re-
cent magnitude. In Coursera: Neural Networks for
Machine Learning, 2012.

J. W. van de Meent, B. Paige, and F. Wood. Temper-

ing by subsampling. In arXiv:1401.7145, 2014.
V. ˇCern´y. Thermodynamical approach to the travel-
ing salesman problem: An eﬃcient simulation algo-
rithm. In J. Optimization Theory and Applications,
1985.

S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh.
(Non-)asymptotic properties of stochastic gradient
Langevin dynamics. In arXiv:1501.00438, 2015.

M. Welling and Y. W. Teh. Bayesian learning via
In ICML,

stochastic gradient Langevin dynamics.
2011.

M. Zeiler and R. Fergus. Stochastic pooling for regu-
larization of deep convolutional neural networks. In
ICLR, 2013.

M. D. Zeiler. Adadelta: An adaptive learning rate

method. In arXiv:1212.5701, 2012.

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Bridging the Gap between Stochastic Gradient MCMC and
Stochastic Optimization: Supplementary Material

Changyou Chen†

David Carlson‡

Zhe Gan†
†Department of Electrical and Computer Engineering, Duke University
‡Department of Statistics and Grossman Center for Statistics of Mind, Columbia University

Chunyuan Li†

Lawrence Carin†

A Solutions for the sub-SDEs

We provide analytic solutions for the split sub-SDEs
in Section 4.1. For stepsize h, the solutions are given
in (6).



θt = θt−1 + G1(θ) p h
pt = pt−1
Ξt = Ξt−1 +

Q − 1
β I

(cid:16)

(cid:17)

(6)

,

h

,

θt = θt−1
pt = exp (−Ξh) pt−1
Ξt = Ξt−1
θt = θt−1
pt = pt−1 +

(cid:16)

−G1(θ)∇θU (θ) + 1

β ∇θG1(θ)

+G1(θ)(Ξ − G2(θ))∇θG2(θ)) h
+ ( 2
2 (cid:12) ζt

β G2(θ)) 1

A :

B :

O :











Ξt = Ξt−1

B Proof of Lemma 1

For a general stochastic diﬀerential equation of the
form

d x = F (x)dt +

2D1/2(x)d w ,

(7)

√

where x ∈ RN , F : RN → RN , D : RM → RN ×P
are measurable functions with P , and w is standard
P -dimensional Brownian motion. (1) is a special case
of the general form (7) with

x = (θ, p, Ξ)


F (x) =

D(x) =









G1(θ) p

− G1(θ)∇θU (θ) − Ξ p + 1

β ∇θ G1(θ)
+ G1(θ)(Ξ − G2(θ))∇θ G2(θ)
Q − 1
β I


0
0
0

0

0
1
β G2(θ) 0
0

0



(8)






We write the joint distribution of x as
exp {−H(x)} (cid:44) 1
Z

ρ(x) =

1
Z

exp {−U (θ) − E(θ, p, Ξ)} .

A reformulation of the main theorem in Ding et al.
(2014) gives the following lemma, which is used to
prove Lemma 1 in the main text.

Lemma 4. The stochastic process of (cid:126)θ generated by
the stochastic diﬀerential equation (7) has the target
distribution pθ(θ) = 1
Z exp{−U (θ)} as its stationary
distribution, if ρ(x) satisﬁes the following marginaliza-
tion condition:

exp{−U (θ)} ∝

exp{−U (θ) − E(θ, p, Ξ)}d p dΞ ,

(cid:90)

(9)

and if the following condition is also satisﬁed:

∇ · (ρF ) = ∇∇(cid:62) : (ρD) ,

(10)

where ∇ (cid:44) (∂/∂θ, ∂/∂ p, ∂/Ξ), “·” represents the vec-
tor inner product operator, “:” represents a matrix
double dot product, i.e., X : Y (cid:44) tr(X(cid:62) Y).

Proof of Lemma 1. We ﬁrst have reformulated (1) us-
ing the general SDE form of (7), resulting in (8).
Lemma 1 states the joint distribution of (θ, p, Ξ) is

ρ(x) =

exp

−

p(cid:62) p −U (θ)

(cid:18)

1
2

(cid:110)
(Ξ − G2(θ))(cid:62) (Ξ − G2(θ))

tr

−

(cid:111)(cid:19)

,

(11)

1
Z
1
2

1

=

H(x)

with
(cid:110)
(Ξ − G2(θ))(cid:62) (Ξ − G2(θ))
1
2 tr
tion condition (9) is trivially satisﬁed, we are left to
verify condition (10). Substituting ρ(x) and F into
(10), we have the left-hand side

2 p(cid:62) p +U (θ)
. The marginaliza-

+

(cid:111)

Bridging the Gap between SG-MCMC and Stochastic Optimization

LHS =

(cid:88)

∂
∂ xi

(ρFi)

i

∂ρ
∂ xi
(cid:18) ∂Fi
∂ xi

Fi +

∂Fi
∂ xi

ρ

(cid:19)

Fi

ρ

−

∂H
∂ xi

(cid:88)

i
(cid:88)

i
(cid:32)

=

=

=

∇θi(G1)i: p −

diag(Ξ)

(cid:88)

i

(cid:88)

j



(cid:88)

i

i

(cid:88)

−

(cid:18)

− βpT

(cid:88)

−β

i

=

1
β

β

∇θi U −

(Ξij − (G2)ij)∇θi(G2)ij

 (G1 p)i

− G1 ∇θU − Ξ p +

∇θ G1 + G1(Ξ − G2)∇θ G2

1
β

(cid:18)

(Ξii − (G2)ii)

Qii −

(cid:19)(cid:33)

ρ

1
β

tr (cid:8)G2(p pT −I)(cid:9) ρ .

It is easy to see for the right-hand side

RHS =

(cid:88)

(cid:88)

1
β

j

(G2)ij

∂2
∂ xi ∂ xj

ρ

(cid:88)

(cid:88)

(G2)ij

∂
∂ pj

(cid:19)

(cid:18)

−

∂H
∂ pi

ρ

j

i
(cid:88)

(G2)ii

(cid:0)p2

i −1(cid:1) ρ

i
1
β

1
β

=

=

i
≡ LHS .

According to Lemma 4, the joint distribution (11) is
the equilibrium distribution of (1).

C Proof of Theorem 2

We start by proving the bias result of Theorem 2.

Proof of the bias. For our 2nd-order integrator, ac-
cording to the deﬁnition, we have:

E[ψ(Xt)] = ˜P l
(cid:17)

(cid:16)

I + h˜Lt

=

hψ(Xt−1) = eh ˜Ltψ(Xt−1) + O(h3)

ψ(Xt−1) +

2
˜L
t ψ(Xt−1) + O(h3) ,

h2
2

where Lt is the generator of the SDE for the t-th it-
eration, i.e., using stochastic gradient instead of the
full gradient, I is the identity map. Compared to the
prove of Chen et al. (2015), we need to consider the
approximation error for ∇θG1(θ). As a result, (12)
needs to be rewritten as:

(13)

E[ψ(Xt)]
(cid:16)

where Bt is from (3). Sum over t = 1, · · · , L in (13),
take expectation on both sides, and use the relation
˜Lt + Bt = Lβt +∆Vt to expand the ﬁrst order term.
We obtain



E[Lβtψ(Xt−1)] + h

E[∆Vtψ(Xt−1)]

E[ψ(Xt)] = ψ(X0) +

E[ψ(Xt)]

L−1
(cid:88)

t=1

L
(cid:88)

t=1

L
(cid:88)

t=1

+ h

L
(cid:88)

t=1

+

h2
2

L
(cid:88)

t=1

(cid:19)

2
E[˜L
t ψ(Xt−1)] + O(Lh3).

We divide both sides by Lh, use the Poisson equation
(4), and reorganize terms. We have:

(cid:0)φ(Xt) − ¯φβt

(cid:1)] =

E[Lβtψ(Xt−1)]

(E[ψ(Xt)] − ψ(X0)) −

E[∆Vtψ(Xt−1)]

1
L

1
L

L
(cid:88)

t=1
(cid:88)

t

2
E[˜L
t ψ(Xt−1)] + O(h2)

(14)

E[

1
L

(cid:88)

t

=

1
Lh

−

h
2L

L
(cid:88)

t=1

2
Now we try to bound ˜L
t . Based on ideas from Mat-
tingly et al. (2010), we apply the following procedure.
First replace ψ with ˜Ltψ from (13) to (14), and apply
the same logic for ˜Ltψ as for ψ in the above deriva-
tions, but this time expand in (13) up to the order
of O(h2), instead of the previous order O(h3). After
simpliﬁcation, we obtain:

E[˜L

2
t ψ(Xt−1)] = O

(cid:88)

t

(cid:19)

+ Lh

(cid:18) 1
h

(15)

(12)

Substituting (15) into (14), after simpliﬁcation, we
have: E (cid:0) 1

(cid:0)φ(Xt) − ¯φβt

(cid:1)(cid:1)

(cid:80)
t

L

=

1
Lh

(E[ψ(Xt)] − ψ(X0))
(cid:123)(cid:122)
(cid:125)
(cid:124)
C1

−

1
L

(cid:88)

t

− O

(cid:18) h
Lh

(cid:19)

+ h2

+ C3h2 ,

E[∆Vtψ(Xt−1)]

=

I + h(˜Lt + Bt)

ψ(Xt−1) +

(cid:17)

˜L

2
t ψ(Xt−1) + O(h3) ,

h2
2

for some C3 ≥ 0. According to the assumption, the
term C1 is bounded. As a result, collecting low order

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

terms, the bias can be expressed as:

lated terms, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
E ˆφ − ¯φ
(cid:12)
(cid:12)
(cid:32)

=

E

(cid:32)

≤

E

1
L

1
L

(cid:88)

t

(cid:88)

t
(cid:32)

≤Cφ(θ∗)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

C1
Lh

−

(cid:32)

≤Cφ(θ∗)

+

(cid:80)
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤Cφ(θ∗)

(cid:33)

(cid:88)

+

1
L

(cid:12)
(cid:12)
¯φβt − ¯φ
(cid:12)
(cid:12)
(cid:12)

(cid:0)φ(Xt) − ¯φβt

(cid:1)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)φ(Xt) − ¯φβt

(cid:1)

t

(cid:88)

t

(cid:33)

¯φβt − ¯φ

+

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
(cid:12)

(cid:32)

1
L

e−βt ˆU (θ)dθ

L
(cid:88)

(cid:90)

1
L
(cid:80)
t

t=1

θ(cid:54)=θ∗

E∆Vtψ(Xt−1)

+ C3h2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

L

L
(cid:88)

(cid:90)

1
L

θ(cid:54)=θ∗
t=1
E∆Vtψ(Xt−1)

L
L
(cid:88)

(cid:90)

(cid:32)

1
L

e−βt ˆU (θ)dθ

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

C1
Lh

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:12)

(cid:12)C3h2(cid:12)
(cid:12)

e−βt ˆU (θ)dθ

(cid:18) 1
Lh

+

t=1
(cid:80)

θ(cid:54)=θ∗
t (cid:107)E∆Vt(cid:107)
L

+ D

(cid:19)

+ h2

,

where the last equation follows from the ﬁniteness as-
sumption of ψ, (cid:107) · (cid:107) denotes the operator norm and
is bounded in the space of ψ due to the assumptions.
This completes the proof.

We will now prove the MSE result .

Proof of the MSE bound. Similar to the proof of The-
orem 2, for our 2nd–order integrator we have:

E (ψβt(Xt)) = (I + h(Lβt + ∆Vt)) ψβt−1 (Xt−1)

+

h2
2

˜L2

t ψβt−1(Xt−1) + O(h3) .

Sum over t from 1 to L + 1 and simplify, we have:

L
(cid:88)

t=1

E (ψβt(Xt)) =

ψβt−1(Xt−1)

L
(cid:88)

t=1

+ h

Lβtψβt−1(Xt−1) + h

∆Vtψβt−1 (Xt−1)

L
(cid:88)

t=1

˜L2

t ψβt−1 (Xt−1) + O(Lh3) .

L
(cid:88)

t=1

+

h2
2

L
(cid:88)

t=1

Substitute the Poisson equation (4) into the above
equation, divide both sides by Lh and rearrange re-

1
L

L
(cid:88)

t=1

−

−

1
Lh

L
(cid:88)

t=1

1
L

L
(cid:88)

t=1

(cid:0)φ(Xt) − ¯φβt

(cid:1) =

(EψβL(XLh) − ψβ0 (X0))

1
Lh

(cid:0)Eψβt−1 (Xt−1) − ψβt−1(Xt−1)(cid:1)

∆Vtψβt−1(Xt−1) −

˜L2

t ψβt−1(Xt−1) + O(h2)

h
2L

L
(cid:88)

t=1

Taking the square of both sides, it is then easy to see
there exists some positive constant C, such that

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

(cid:32)

L
(cid:88)

t=1

1
L


≤C




(cid:124)

(EψβL(XLh) − ψβ0(X0))2
L2h2
(cid:123)(cid:122)
A1

(cid:125)

(16)

L
(cid:88)

t=1

+

1
L2h2
(cid:124)

+

1
L2

L
(cid:88)

t=1

(cid:32) L
(cid:88)

t=1

+

h2
2L2
(cid:124)

(cid:0)Eψβt−1 (Xt−1) − ψβt−1(Xt−1)(cid:1)2

(cid:123)(cid:122)
A2

(cid:125)

∆V 2

t ψβt−1 (Xt−1)









(cid:33)2

(cid:125)

˜L2

t ψβt−1 (Xt−1)

+h4

(cid:123)(cid:122)
A3

A1 is easily bounded by the assumption that (cid:107)ψ(cid:107) ≤
V r0 < ∞. A2 is bounded because it can be shown that
E (ψβt(Xt)) − ψβt(Xt) ≤ C1
h + O(h) for C1 ≥ 0.
Intuitively this is true because the only diﬀerence be-
tween E (ψβt(Xt)) and ψβt(Xt) lies in the additional
Gaussian noise with variance h. A formal proof is
given in Chen et al. (2015). Furthermore, A3 is

√

Bridging the Gap between SG-MCMC and Stochastic Optimization

bounded by the following arguments:

D Proof of Corollary 3

t ψβt−1(Xt−1) − E ˜L2

t ψβt−1(Xt−1)

(cid:104) ˜L2

E

t ψβt−1 (Xt−1)

A3 =

(cid:32) L
(cid:88)

t=1

h2
2L2
(cid:124)

(cid:32) L
(cid:88)

E

(cid:16) ˜L2

t=1

+

h2
2L2
(cid:124)

(cid:123)(cid:122)
B1

(cid:123)(cid:122)
B2

(cid:46) B1 +

˜L2

t ψβt−1 (Xt−1)

(cid:32)

h2
Lh

L
(cid:88)

t=1

E ˜L2

t ψβt−1(Xt−1)

(cid:33)2
(cid:105)

(cid:125)

(cid:33)2

(cid:33)2

(cid:17)

(cid:33)2

(cid:17)

(cid:125)

Proof. The reﬁnement stage corresponds to β → ∞.
We can prove that in this case, the integration terms
in the bias and MSE in Theorem 2 converge to 0.

To show this, deﬁne a sequence of functions {gm} as:

gm (cid:44) −

1
L

L+m−1
(cid:88)

l=m

e−βl ˆU (θ) .

(18)

it is easy to see the sequence {gm} satisﬁes gm1 < gm2
for m1 < m2, and limm→∞ gm = 0. According to the
monotone convergence theorem, we have

(cid:19)

+

1
Lh

(cid:32)

h2
L

L
(cid:88)

t=1

(cid:33)

( ˜L2

t ψ(Xt−1))2

(cid:90)

lim
m→∞

gm (cid:44) lim
m→∞

(cid:90)

−

1
L

L+m−1
(cid:88)

l=m

e−βl ˆU (θ)dθ

(cid:90)

=

lim
m→∞

gm = 0 .

(cid:32)

+

h2
Lh

L
(cid:88)

(cid:16)

t=1

≤ O

+ O

= O

(cid:18) 1
2L2 + L2h2
(cid:19)

(cid:18) 1
L2h2 + h4
(cid:19)
(cid:18) 1
Lh

+ L4

As a result, the integration terms in the bounds for the
bias and MSE vanish, leaving only the terms stated in
Corollary 3. This completes the proof.

E Reformulation of the Santa

Algorithm

E (cid:107)∆Vt(cid:107)2
L

+

1
Lh

(cid:33)

+ h4

.

(17)

In this section we give a version of the Santa algorithm
that matches better than our actual implementation,
shown in Algorithm 3–7.

Collecting low order terms we have:

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

E

(cid:32)

1
L
(cid:32) 1
L

L
(cid:88)

t=1
(cid:80)
t

=O

Finally, we have:

L
(cid:88)

t=1
(cid:32)

1
L

(cid:16) ˆφ − ¯φ

(cid:17)2

E

< E

(cid:32)

1
L

(cid:88)

t

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

+ E

(cid:32)

1
L

(cid:33)2

(cid:0)φ(Xt) − ¯φβt

(cid:1)

≤Cφ(θ∗)2

(cid:32) 1
L

(cid:80)
t

+ O

L
(cid:88)

(cid:90)

θ(cid:54)=θ∗
t=1
E (cid:107)∆Vt(cid:107)2
L

≤Cφ(θ∗)2

L
(cid:88)

(cid:90)

(cid:32)

1
L

(cid:33)2

(cid:33)

(cid:33)2

e−βt ˆU (θ)dθ

+

1
Lh

+ h4

e−βt ˆU (θ)dθ

(cid:32) 1
L

(cid:80)
t

+ D

θ(cid:54)=θ∗
t=1
E (cid:107)∆Vt(cid:107)2
L

(cid:33)

+ h4

.

+

1
Lh

Algorithm 3: Santa
Input: ηt (learning rate), σ, λ, burnin,

β = {β1, β2, · · · } → ∞, {ζt ∈ Rp} ∼ N (0, Ip).
ηC, v0 = 0 ;
η × N (0, I), α0 =

√

√

Initialize θ0, u0 =
for t = 1, 2, . . . do

˜f t (cid:12) ˜f t ;
vt ;

Evaluate ˜f t = ∇θ ˜Ut(θt−1) on the t-th minibatch ;
vt = σ vt−1 + 1−σ
N 2
√
gt = 1 (cid:11) (cid:112)λ +
if t < burnin then
/* exploration
*/
(θt, ut, αt) = Exploration S(θt−1, ut−1, αt−1)
or
(θt, ut, αt) = Exploration E(θt−1, ut−1, αt−1)

/* refinement
(θt, ut, αt) = Reﬁnement S(θt−1, ut−1, αt−1)
or
(θt, ut, αt) = Reﬁnement E(θt−1, ut−1, αt−1)

*/

else

end

end

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Algorithm 4: Exploration S (θt−1, ut−1, αt−1)
θt = θt−1 + gt (cid:12) ut−1 /2;
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt) /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη +
ut = exp (−αt/2) (cid:12) ut;
αt = αt + (ut (cid:12) ut −η/βt) /2;
θt = θt + gt (cid:12) ut /2;
Return (θt, ut, αt)

2 gt−1 η3/2/βt (cid:12) ζt;

(cid:113)

Algorithm 5: Reﬁnement S (θt−1, ut−1, αt−1)
αt = αt−1;
θt = θt−1 + gt (cid:12) ut−1 /2;
ut = exp (−αt/2) (cid:12) ut−1;
ut = ut − gt (cid:12)˜f tη;
ut = exp (−αt/2) (cid:12) ut;
θt = θt + gt (cid:12) ut /2;
Return (θt, ut, αt)

F Relationship of reﬁnement Santa to

Adam

In the Adam algorithm (see Algorithm 1 of Kingma
and Ba (2015)), the key steps are:

˜f t (cid:44) ∇θ ˜U (θt−1)
vt = σ vt−1 +(1 − σ)˜f t (cid:12) ˜f t

(cid:113)

√

vt

λ +

gt = 1 (cid:11)
˜ut = (1 − b1) (cid:12) ˜ut−1 + b1 (cid:12)˜f t
θt = θt + η(gt (cid:12) gt) (cid:12) ˜ut

Here, we maintain the square root form of gt, so
the square is equivalent to the preconditioner used in
Adam. As well, in Adam, the vector b1 is set to the
same constant between 0 and 1 for all entries. An
equivalent formulation of this is:
˜f t (cid:44) ∇θ ˜U (θt−1)
vt = σ vt−1 +(1 − σ)˜f t (cid:12) ˜f t

(cid:113)

√

λ +

gt = 1 (cid:11)
vt
ut = (1 − b1) (cid:12) ut−1 − η(gt (cid:12) b1 (cid:12)˜f t)
θt = θt − gt (cid:12)ut

The only diﬀerences between these steps and the Eu-
ler integrator we present in our Algorithm 1 are that
our b1 has a separate constant for each entry, and the
second term in u does not include the b1 in our for-
mulation. If we modify our algorithm to multiply the
gradient by b1, then our algorithm, under the same as-
sumptions as Adam, will have a similar regret bound
of O(

T ) for a convex problem.

√

Algorithm 6: Exploration E (θt−1, ut−1, αt−1)
αt = αt−1 + (ut−1 (cid:12) ut−1 −η/βt);
(cid:113)
ut = (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t +
θt = θt + gt (cid:12) ut;
Return (θt, ut, αt)

2 gt−1 η3/2/βt (cid:12) ζt;

Algorithm 7: Reﬁnement E (θt−1, ut−1, αt−1)
αt = αt−1;
ut = (1 − αt) (cid:12) ut−1 −η gt (cid:12)˜f t;
θt = θt + gt (cid:12) ut;
Return (θt, ut, αt)

Because the focus of this paper is not on the regret
bound, we only brieﬂy discuss the changes in the the-
ory. We note that Lemma 10.4 from Kingma and Ba
(2015) will hold with element-wise b1.
Lemma 5. Let γi (cid:44) b2
1,i√
σ . For b1,i, σ ∈ [0, 1) that
satisfy β2
< 1 and bounded ˜ft, || ˜ft||2 ≤ G, || ˜ft||∞ ≤
1√
β2
G∞, the following inequality holds

T
(cid:88)

t=1

u2
i
(cid:112)tg2

i

≤

2
1 − γi

|| ˜f1:T,i||2

which contains an element-dependent γi compared to
Adam.

Theorem 10.5 of Kingma and Ba (2015) will hold with
the same modiﬁcations and assumptions for a b with
distinct entries; the proof in Kingma and Ba (2015)
is already element-wise, so it suﬃces to replace their
global parameter γ with distinct γi (cid:44) b2
1,i√
σ . This will
√
give a regret of O(

T ), the same as Adam.

G Additional Results

Figure 4: MNIST using FNN with size of 800.

Bridging the Gap between SG-MCMC and Stochastic Optimization

Learning curves of diﬀerent algorithms on MNIST us-
ing FNN with size of 800 are plotted in Figure 4.
Learning curves of diﬀerent algorithms on four poly-
phonic music datasets using RNN are shown in Fig-
ure 6.

We additionally test Santa on the ImageNet dataset.
We use the GoogleNet architecture, which is a 22
layer deep model. We use the default setting de-
ﬁned in the Caﬀe package8. We were not able to
make other stochastic optimization algorithms except
SGD with momentum and the proposed Santa work
on this dataset. Figure 5 shows the comparison on
this dataset. We did not tune the parameter setting,
note the default setting is favourable by SGD with
momentum. Nevertheless, Santa still signiﬁcantly out-
performs SGD with momentum in term of convergence
speed.

Figure 5: Santa vs. SGD with momentum on Ima-
geNet. We used ImageNet11 for training.

8https : //github.com/cchangyou/Santa/tree/master/caf f e/models/bvlc googlenet

C. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin

Figure 6: Learning curves of diﬀerent algorithms on four polyphonic music datasets using RNN.

