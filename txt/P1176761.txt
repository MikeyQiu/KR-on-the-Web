Reinforced Mnemonic Reader for Machine Reading Comprehension

Minghao Hu†∗, Yuxing Peng†, Zhen Huang†, Xipeng Qiu‡, Furu Wei§, Ming Zhou§
† College of Computer, National University of Defense Technology, Changsha, China
‡ School of Computer Science, Fudan University, Shanghai, China
§ Microsoft Research, Beijing, China
{huminghao09,pengyuhang,huangzhen}@nudt.edu.cn
xpqiu@fudan.edu.cn, {fuwei,mingzhou}@microsoft.com

8
1
0
2
 
n
u
J
 
6
 
 
]
L
C
.
s
c
[
 
 
6
v
8
9
7
2
0
.
5
0
7
1
:
v
i
X
r
a

Abstract

In this paper, we introduce the Reinforced
Mnemonic Reader for machine reading compre-
hension tasks, which enhances previous attentive
readers in two aspects. First, a reattention mech-
anism is proposed to reﬁne current attentions by di-
rectly accessing to past attentions that are tempo-
rally memorized in a multi-round alignment archi-
tecture, so as to avoid the problems of attention re-
dundancy and attention deﬁciency. Second, a new
optimization approach, called dynamic-critical re-
inforcement learning, is introduced to extend the
standard supervised method. It always encourages
to predict a more acceptable answer so as to address
the convergence suppression problem occurred in
traditional reinforcement learning algorithms. Ex-
tensive experiments on the Stanford Question An-
swering Dataset (SQuAD) show that our model
achieves state-of-the-art results. Meanwhile, our
model outperforms previous systems by over 6% in
terms of both Exact Match and F1 metrics on two
adversarial SQuAD datasets.

1 Introduction
Teaching machines to comprehend a given context paragraph
and answer corresponding questions is one of the long-term
goals of natural language processing and artiﬁcial intelli-
gence. Figure 1 gives an example of the machine reading
comprehension (MRC) task. Beneﬁting from the rapid devel-
opment of deep learning techniques [Goodfellow et al., 2016]
and large-scale benchmark datasets [Hermann et al., 2015;
Hill et al., 2016; Rajpurkar et al., 2016], end-to-end neu-
ral networks have achieved promising results on this task
[Wang et al., 2017; Seo et al., 2017; Xiong et al., 2017a;
Huang et al., 2017].

Despite of the advancements, we argue that there still exists

two limitations:

1. To capture complex interactions between the context and
the question, a variety of neural attention [Dzmitry Bah-
danau, 2015], such as bi-attention [Seo et al., 2017],

∗Contribution during internship at Fudan University and Mi-

crosoft Research.

coattention [Xiong et al., 2017b], are proposed in a
In order to fully
single-round alignment architecture.
compose complete information of the inputs, multi-
round alignment architectures that compute attentions
repeatedly have been proposed [Huang et al., 2017;
Xiong et al., 2017a]. However, in these approaches, the
current attention is unaware of which parts of the con-
text and question have been focused in earlier attentions,
which results in two distinct but related issues, where
multiple attentions 1) focuses on same texts, leading to
attention redundancy and 2) fail to focus on some salient
parts of the input, causing attention deﬁciency.

2. To train the model,

standard maximum-likelihood
method is used for predicting exactly-matched (EM) an-
swer spans [Wang and Jiang, 2017]. Recently, rein-
forcement learning algorithm, which measures the re-
ward as word overlap between the predicted answer and
the groung truth, is introduced to optimize towards the
F1 metric instead of EM metric [Xiong et al., 2017a].
Speciﬁcally, an estimated baseline is utilized to normal-
ize the reward and reduce variances. However, the con-
vergence can be suppressed when the baseline is bet-
ter than the reward. This is harmful if the inferior re-
ward is partially overlapped with the ground truth, as
the normalized objective will discourage the prediction
of ground truth positions. We refer to this case as the
convergence suppression problem.

To address the ﬁrst problem, we present a reattention mech-
anism that temporally memorizes past attentions and uses
them to reﬁne current attentions in a multi-round alignment
architecture. The computation is based on the fact that two
words should share similar semantics if their attentions about
same texts are highly overlapped, and be less similar vice
versa. Therefore, the reattention can be more concentrated
if past attentions focus on same parts of the input, or be rel-
atively more distracted so as to focus on new regions if past
attentions are not overlapped at all.

As for the second problem, we extend the traditional train-
ing method with a novel approach called dynamic-critical re-
inforcement learning. Unlike the traditional reinforcement
learning algorithm where the reward and baseline are stat-
ically sampled, our approach dynamically decides the re-
ward and the baseline according to two sampling strategies,

Aligning Rounds Attention
Interactive Self

Model

Match-LSTM1
Rnet2
BiDAF3
FastQAExt4
DCN+5
FusionNet6
Our Model

1
1
1
1
2
3
3

Type
Serial
Serial
Parallel
Parallel
Parallel
Parallel
Parallel

-
1
-
1
2
1
3

Table 1: Comparison of alignment architectures of competing mod-
els: Wang & Jiang[2017]1, Wang et al.[2017]2, Seo et al.[2017]3,
Weissenborn et al.[2017]4, Xiong et al.[2017a]5 and Huang et
al.[2017]6.

and U = {uj}m
tively, a similarity matrix E ∈ Rn×m is computed as

j=1, representing question and context respec-

Eij = f (vi, uj)

(1)

where Eij indicates the similarity between i-th question word
and j-th context word, and f is a scalar function. Different
methods are proposed to normalize the matrix, resulting in
variants of attention such as bi-attention[Seo et al., 2017] and
coattention [Xiong et al., 2017b]. The attention is then used
to attend the question and form a question-aware context rep-
resentation H = {hj}m
Later, Wang et al.

j=1.
[2017] propose a serial self aligning
method to align the context aginst itself for capturing long-
term dependencies among context words. Weissenborn et
al. [Weissenborn et al., 2017] apply the self alignment in
a similar way of Eq. 1, yielding another similarity matrix
B ∈ Rm×m as

Bij = 1{i(cid:54)=j}f (hi, hj)

(2)
where 1{·} is an indicator function ensuring that the context
word is not aligned with itself. Finally, the attentive informa-
tion can be integrated to form a self-aware context represen-
tation Z = {zj}m

j=1, which is used to predict the answer.

We refer to the above process as a single-round alignment
architecture. Such architecture, however, is limited in its ca-
pability to capture complex interactions among question and
context. Therefore, recent works build multi-round align-
ment architectures by stacking several identical aligning lay-
ers [Huang et al., 2017; Xiong et al., 2017a]. More speciﬁ-
cally, let V t = {vt
i=1 and U t = {ut
j=1 denote the hid-
den representations of question and context in t-th layer, and
H t = {ht
j=1 is the corresponding question-aware context
representation. Then the two similarity matrices can be com-
puted as

j}m

j}m

i }n

Et

ij = f (vt

i , ut

j),

Bt

ij = 1{i(cid:54)=j}f (ht

i, ht
j)

(3)

However, one problem is that each alignment is not directly
aware of previous alignments in such architecture. The atten-
tive information can only ﬂow to the subsequent layer through
the hidden representation. This can cause two problems: 1)
the attention redundancy, where multiple attention distribu-
tions are highly similar. Let softmax(x) denote the softmax

Figure 1: An example from the SQuAD dataset. Evidences needed
for the answer are marked as green.

namely random inference and greedy inference. The result
with higher score is always set to be the reward while the
other is the baseline. In this way, the normalized reward is
ensured to be always positive so that no convergence suppres-
sion will be made.

All of the above innovations are integrated into a new
end-to-end neural architecture called Reinforced Mnemonic
Reader in Figure 3. We conducted extensive experiments on
both the SQuAD [Rajpurkar et al., 2016] dataset and two
adversarial SQuAD datasets [Jia and Liang, 2017] to eval-
uate the proposed model. On SQuAD, our single model ob-
tains an exact match (EM) score of 79.5% and F1 score of
86.6%, while our ensemble model further boosts the result
to 82.3% and 88.5% respectively. On adversarial SQuAD,
our model surpasses existing approahces by more than 6% on
both AddSent and AddOneSent datasets.

2 MRC with Reattention

2.1 Task Description

For the MRC tasks, a question Q and a context C are given,
our goal is to predict an answer A, which has different forms
In the SQuAD dataset [Ra-
according to the speciﬁc task.
jpurkar et al., 2016], the answer A is constrained as a seg-
ment of text in the context C, nerual networks are designed
to model the probability distribution p(A|C, Q).

2.2 Alignment Architecture for MRC

Among all state-of-the-art works for MRC, one of the key
factors is the alignment architecture. That is, given the hid-
den representations of question and context, we align each
context word with the entire question using attention mecha-
nisms, and enhance the context representation with the atten-
tive question information. A detailed comparison of different
alignment architectures is shown in Table 1.

Early work for MRC, such as Match-LSTM [Wang and
Jiang, 2017], utilizes the attention mechanism stemmed from
neural machine translation [Dzmitry Bahdanau, 2015] seri-
ally, where the attention is computed inside the cell of recur-
rent neural networks. A more popular approach is to com-
pute attentions in parallel, resulting in a similarity matrix.
Concretely, given two sets of hidden vectors, V = {vi}n

i=1

:j)(cid:107)softmax(Ek

function over a vector x. Then this problem can be formulized
as D(softmax(Et
:j)) < σ(t (cid:54)= k), where σ
is a small bound and D is a function measuring the distri-
bution distance. 2) the attention deﬁciency, which means
that the attention fails to focus on salient parts of the input:
D(softmax(Et
:j)) > δ, where δ is another
:j
∗) is the “ground truth” attention dis-
bound and softmax(Et
:j
tribution.

∗)(cid:107)softmax(Et

2.3 Reattention Mechanism
To address these problems, we propose to temporally memo-
rize past attentions and explicitly use them to reﬁne current at-
tentions. The intuition is that two words should be correlated
if their attentions about same texts are highly overlapped, and
be less related vice versa. For example, in Figure 2, sup-
pose that we have access to previous attentions, and then we
can compute their dot product to obtain a “similarity of atten-
tion”. In this case, the similarity of word pair (team, Broncos)
is higher than (team, Panthers).

i:

i:

)

:j

:j

ij

(4)

) · softmax(Bt−1

Therefore, we deﬁne the computation of reattention as fol-
lows. Let Et−1 and Bt−1 denote the past similarity matrices
that are temporally memorized. The reﬁned similarity matrix
Et (t > 1) is computed as
˜Et
Et

ij =softmax(Et−1
j) + γ ˜Et
i , ut
ij =f (vt
where γ is a trainable parameter. Here, softmax(Et−1
) is the
past context attention distribution for the i-th question word,
and softmax(Bt−1
) is the self attention distribution for the j-
th context word. In the extreme case, when there is no overlap
between two distributions, the dot product will be 0. On the
other hand, if the two distributions are identical and focus on
one single word, it will have a maximum value of 1. There-
fore, the similarity of two words can be explicitly measured
using their past attentions. Since the dot product is relatively
small than the original similarity, we initialize the γ with a
tunable hyper-parameter and keep it trainable. The reﬁned
similarity matrix can then be normalized for attending the
question. Similarly, we can compute the reﬁned matrix Bt
to get the unnormalized self reattention as
ij =softmax(Bt−1
i:
ij =1(i(cid:54)=j)
i, ht
f (ht

) · softmax(Bt−1
j) + γ ˜Bt

˜Bt

Bt

(5)

(cid:16)

(cid:17)

ij

:j

)

3 Dynamic-critical Reinforcement Learning
the model distribution
In the extractive MRC task,
p(A|C, Q; θ) can be divided into two steps: ﬁrst predicting
the start position i and then the end position j as

p(A|C, Q; θ) = p1(i|C, Q; θ)p2(j|i, C, Q; θ)

(6)

where θ represents all trainable parameters.

The standard maximum-likelihood (ML) training method
is to maximize the log probabilities of the ground truth an-
swer positions [Wang and Jiang, 2017]

LM L(θ) = −

log p1(y1

k) + log p2(y2

k|y1
k)

(7)

(cid:88)

k

Figure 2: Illustrations of reattention for the example in Figure 1.

k and y2

where y1
k are the answer span for the k-th example,
and we denote p1(i|C, Q; θ) and p2(j|i, C, Q; θ) as p1(i) and
p2(j|i) respectively for abbreviation.

Recently, reinforcement learning (RL), with the task re-
ward measured as word overlap between predicted answer
and groung truth, is introduced to MRC [Xiong et al., 2017a].
A baseline b, which is obtained by running greedy inference
with the current model, is used to normalize the reward and
reduce variances. Such approach is known as the self-critical
sequence training (SCST) [Rennie et al., 2016], which is ﬁrst
used in image caption. More speciﬁcally, let R(As, A∗) de-
note the F1 score between a sampled answer As and the
ground truth A∗. The training objective is to minimize the
negative expected reward by

LSCST (θ) = −EAs∼pθ(A)[R(As) − R( ˆA)]

(8)

where we abbreviate the model distribution p(A|C, Q; θ) as
pθ(A), and the reward function R(As, A∗) as R(As). ˆA is
obtained by greedily maximizing the model distribution:

ˆA = arg max

p(A|C, Q; θ)

A

The expected gradient ∇θLSCST (θ) can be computed ac-
cording to the REINFORCE algorithm [Sutton and Barto,
1998] as
∇θLSCST (θ) = −EAs∼pθ(A)[(R(As) − b) ∇θ log pθ(As)]
(cid:17)
R(As) − R( ˆA)

∇θ log pθ(As)

≈ −

(9)

(cid:16)

where the gradient can be approxiamated using a single
Monte-Carlo sample As derived from pθ.

However, a sampled answer is discouraged by the objec-
tive when it is worse than the baseline. This is harmful if
the answer is partially overlapped with ground truth, since
the normalized objective would discourage the prediction of
ground truth positions. For example, in Figure 1, suppose that
As is champion Denver Broncos and ˆA is Denver Broncos.
Although the former is an acceptable answer, the normalized
reward would be negative and the prediction for end position
would be suppressed, thus hindering the convergence. We
refer to this case as the convergence suppression problem.

Here, we consider both random inference and greedy in-
ference as two different sampling strategies: the ﬁrst one en-
courages exploration while the latter one is for exploitation1.
Therefore, we approximate the expected gradient by dynam-
ically set the reward and baseline based on the F1 scores of
both As and ˆA. The one with higher score is set as reward,
while the other is baseline. We call this approach as dynamic-
critical reinforcement learning (DCRL)
∇θLDCRL(θ) = −EAs∼pθ(A)[(R(As) − b) ∇θ log pθ(As)]

≈ −1

{R(As)≥R( ˆA)}
(cid:16)

{R( ˆA)>R(As)}

− 1

(cid:16)

(cid:17)
R(As) − R( ˆA)
(cid:17)

R( ˆA) − R(As)

∇θ log pθ(As)

∇θ log pθ( ˆA) (10)

Notice that the normalized reward is constantly positive so
that superior answers are always encouraged. Besides, when
the score of random inference is higher than the greedy one,
DCRL is equivalent to SCST. Thus, Eq. 9 is a special case of
Eq. 10.

Following [Xiong et al., 2017a] and [Kendall et al., 2017],
we combine ML and DCRL objectives using homoscedastic
uncertainty as task-dependent weightings so as to stabilize the
RL training as

L =

LM L +

LDCRL + log σ2

a + log σ2
b

(11)

1
2σ2
a

1
2σ2
b

where σa and σb are trainable parameters.

4 End-to-end Architecture
Based on previous innovations, we introduce an end-to-end
architecture called Reinforced Mnemonic Reader, which is
shown in Figure 3. It consists of three main components: 1)
an encoder builds contextual representations for question and
context jointly; 2) an iterative aligner performs multi-round
alignments between question and context with the reattention
mechanism; 3) an answer pointer predicts the answer span se-
quentially. Beblow we give more details of each component.
Encoder. Let W Q = {wq
j=1 denote
the word sequences of the question and context respectively.
The encoder ﬁrstly converts each word to an input vector.
We utilize the 100-dim GloVe embedding [Pennington et al.,
2014] and 1024-dim ELMo embedding [Peters et al., 2018].

i=1 and W C = {wc

j }m

i }n

1In practice we found that a better approximation can be made
by considering a top-K answer list, where ˆA is the best result and
As is sampled from the rest of the list.

Figure 3: The architecture overview of Reinforced Mnemonic
Reader. The subﬁgures to the right show detailed demonstrations
of the reattention mechanism: 1) reﬁned Et to attend the query; 2)
reﬁned Bt to attend the context.

Besides, a character-level embedding is obtained by encoding
the character sequence with a bi-directional long short-term
memory network (BiLSTM) [Hochreiter and Schmidhuber,
1997], where two last hidden states are concatenated to form
the embedding. In addition, we use binary feature of exact
match, POS embedding and NER embedding for both ques-
tion and context, as suggested in [Chen et al., 2017]. Together
the inputs X Q = {xq
i=1 and X C = {xc
j=1 are obtained.
To model each word with its contextual information, a

j}m

i }n

weight-shared BiLSTM is utilized to perform the encoding

vi = BiLSTM(xq

i ),

uj = BiLSTM(xc
j)

(12)

Thus, the contextual representations for both question and
context words can be obtained, denoted as two matrices:
V = [v1, ..., vn] ∈ R2d×n and U = [u1, ..., um] ∈ R2d×m.
Iterative Aligner. The iterative aligner contains a stack of
three aligning blocks. Each block consists of three modules:
1) an interactive alignment to attend the question into the con-
text; 2) a self alignment to attend the context against itself;
3) an evidence collection to model the context representation
with a BiLSTM. The reattention mechanism is utilized be-
tween two blocks, where past attentions are temporally mem-
orizes to help modulating current attentions. Below we ﬁrst

R1, Z 1, E1, B1 = align1(U, V )
R2, Z 2, E2, B2 = align2(R1, V, E1, B1)

R3, Z 3, E3, B3 = align3(R2, V, E2, B2, Z 1, Z 2)

(14)

where alignt denote the t-th block. In the t-th block (t > 1),
we ﬁx the hidden representation of question as V , and set
the hidden representation of context as previous fully-aware
context vectors Rt−1. Then we compute the unnormalized
reattention Et and Bt with Eq. 4 and Eq. 5 respectively. In
addition, we utilize a residual connection [He et al., 2016] in
the last BiLSTM to form the ﬁnal fully-aware context vectors
j = BiLSTM (cid:0)[z1
R3 = [r3
Answer Pointer. We apply a variant of pointer net-
works [Vinyals et al., 2015] as the answer pointer to make the
predictions. First, the question representation V is summa-
rized into a ﬁxed-size summary vector s as: s = (cid:80)n
i=1 αivi,
where αi ∝ exp(wTvi). Then we compute the start probabil-
ity p1(i) by heuristically attending the context representation
R3 with the question summary s as

1, ..., r3

m]: r3

j ; z2

j ; z3

j ](cid:1).

p1(i) ∝ exp (cid:0)wT

1 tanh(W1[r3

i ; s; r3

i ◦ s; r3

i − s])(cid:1)

(15)

Next, a new question summary ˜s is updated by fusing con-
text information of the start position, which is computed as
l = R3 · p1, into the old question summary: ˜s = fusion(s, l).
Finally the end probability p2(j|i) is computed as

p2(j|i) ∝ exp (cid:0)wT

2 tanh(W2[r3

j ; ˜s; r3

j ◦ ˜s; r3

j − ˜s])(cid:1)

(16)

Figure 4: The detailed overview of a single aligning block. Different
colors in E and B represent different degrees of similarity.

describe a single block in details, which is shown in Figure 4,
and then introduce the entire architecture.
Single Aligning Block. First, the similarity matrix E ∈
Rn×m is computed using Eq. 1, where the multiplicative
product with nonlinearity is applied as attention function:
f (u, v) = relu(Wuu)Trelu(Wvv). The question attention
for the j-th context word is then: softmax(E:j), which is
used to compute an attended question vector ˜vj = V ·
softmax(E:j).

To efﬁciently fuse the attentive information into the
context, an heuristic fusion function, denoted as o =
fusion(x, y), is proposed as

˜x = relu (Wr[x; y; x ◦ y; x − y])
g = σ (Wg[x; y; x ◦ y; x − y])
o = g ◦ ˜x + (1 − g) ◦ x

5 Experiments

(13)

where σ denotes the sigmoid activation function, ◦ denotes
element-wise multiplication, and the bias term is omitted.
The computation is similar to the highway networks [Srivas-
tava et al., 2015], where the output vector o is a linear inter-
polation of the input x and the intermediate vector ˜x. A gate
g is used to control the composition degree to which the inter-
mediate vector is exposed. With this function, the question-
aware context vectors H = [h1, ..., hm] can be obtained as:
hj = fusion(uj, ˜vj).

Similar to the above computation, a self alignment is ap-
plied to capture the long-term dependencies among context
words. Again, we compute a similarity matrix B ∈ Rm×m
using Eq. 2. The attended context vector is then computed
as: ˜hj = H · softmax(B:j), where softmax(B:j) is the self
attention for the j-th context word. Using the same fusion
function as zj = fusion(hj, ˜hj), we can obtain self-aware
context vectors Z = [z1, ..., zm].

Finally, a BiLSTM is used to perform the evidence col-
lection, which outputs the fully-aware context vectors R =
[r1, ..., rm] with Z as its inputs.
Multi-round Alignments with Reattention. To enhance the
ability of capturing complex interactions among inputs, we
stack two more aligning blocks with the reattention mecha-
nism as follows

Implementation Details

5.1
We mainly focus on the SQuAD dataset [Rajpurkar et al.,
2016] to train and evaluate our model. SQuAD is a machine
comprehension dataset, totally containing more than 100, 000
questions manually annotated by crowdsourcing workers on
a set of 536 Wikipedia articles. In addition, we also test our
model on two adversarial SQuAD datasets [Jia and Liang,
2017], namely AddSent and AddOneSent. In both adversar-
ial datasets, a confusing sentence with a wrong answer is ap-
pended at the end of the context in order to fool the model.

We evaluate the Reinforced Mnemonic Reader (R.M-
Reader) by running the following setting. We ﬁrst train the
model until convergence by optimizing Eq. 7. We then ﬁne-
tune this model with Eq. 11, until the F1 score on the devel-
opment set no longer improves.

We use the Adam optimizer [Kingma and Ba, 2014] for
both ML and DCRL training. The initial learning rates are
0.0008 and 0.0001 respectively, and are halved whenever
meeting a bad iteration. The batch size is 48 and a dropout
rate [Srivastava et al., 2014] of 0.3 is used to prevent overﬁt-
ting. Word embeddings remain ﬁxed during training. For out
of vocabulary words, we set the embeddings from Gaussian
distributions and keep them trainable. The size of character
embedding and corresponding LSTMs is 50, the main hidden
size is 100, and the hyperparameter γ is 3.

Single Model

LR Baseline1
DCN+2
FusionNet3
SAN4
AttentionReader+†
BSE5
R-net+†
SLQA+†
Hybrid AoA Reader+†
R.M-Reader
Ensemble Model
DCN+2
FusionNet3
SAN4
BSE5
AttentionReader+†
R-net+†
SLQA+†
Hybrid AoA Reader+†
R.M-Reader
Human1

Dev
EM F1
40.0
74.5
75.3
76.2
-
77.9
-
-
-
78.9

51.0
83.1
83.6
84.1
-
85.6
-
-
-
86.3

Test
EM F1
40.4
75.1
76.0
76.8
77.3
78.6
79.9
80.4
80.0
79.5

51.0
83.1
83.9
84.4
84.9
85.8
86.5
87.0
87.3
86.6

-
78.5
78.6
79.6
-
-
-
-
81.2
80.3

-
85.8
85.8
86.6
-
-
-
-
87.9
90.5

78.8
79.0
79.6
81.0
81.8
82.6
82.4
82.5
82.3
82.3

86.0
86.0
86.5
87.4
88.2
88.5
88.6
89.3
88.5
91.2

Table 2: The performance of Reinforced Mnemonic Reader and
other competing approaches on the SQuAD dataset. The results
of test set are extracted on Feb 2, 2018: Rajpurkar et al.[2016]1,
Xiong et al.[2017a]2, Huang et al.[2017]3, Liu et al.[2017b]4 and
Peters[2018]5. † indicates unpublished works. BSE refers to BiDAF
+ Self Attention + ELMo.

5.2 Overall Results
We submitted our model on the hidden test set of SQuAD for
evaluation. Two evaluation metrics are used: Exact Match
(EM), which measures whether the predicted answer are ex-
actly matched with the ground truth, and F1 score, which
measures the degree of word overlap at token level.

As shown in Table 2, R.M-Reader achieves an EM score
of 79.5% and F1 score of 86.6%. Since SQuAD is a com-
petitve MRC benchmark, we also build an ensemble model
that consists of 12 single models with the same architecture
but initialized with different parameters. Our ensemble model
improves the metrics to 82.3% and 88.5% respectively2.

Table 3 shows the performance comparison on two adver-
sarial datasets, AddSent and AddOneSent. All models are
trained on the original train set of SQuAD, and are tested on
the two datasets. As we can see, R.M-Reader comfortably
outperforms all previous models by more than 6% in both
EM and F1 scores, indicating that our model is more robust
against adversarial attacks.

5.3 Ablation Study
The contributions of each component of our model are shown
in Table 4. Firstly, ablation (1-4) explores the utility of

2The results are on https://worksheets.codalab.org/worksheets/

0xe6c23cbae5e440b8942f86641f49fd80.

Model

LR Baseline
Match-LSTM1∗
BiDAF2∗
SEDT3∗
ReasoNet4∗
FusionNet5∗
R.M-Reader

AddSent
EM F1
17.0
24.3
29.6
30.0
34.6
46.2
53.0

23.2
34.2
34.2
35.0
39.4
51.4
58.5

AddOneSent
EM F1
22.3
34.8
40.7
40.0
43.6
54.7
60.9

41.8
41.8
46.9
46.5
49.8
60.7
67.0

Table 3:
Performance comparison on two adversarial SQuAD
datasets. Wang & Jiang[2017]1, Seo et al.[2017]2, Liu et
al.[2017a]3, Shen et al.[2016]4 and Huang et al.[2017]5. ∗ indicates
ensemble models.

∆EM ∆F1

EM F1
Conﬁguration
78.9
R.M-Reader
78.1
(1) - Reattention
(2) - DCRL
78.2
(3) - Reattention, DCRL 77.1
78.5
(4) - DCRL, + SCST
78.2
(5) Attention: Dot
78.1
(6) - Heuristic Sub
78.3
(7) - Heuristic Mul
77.9
(8) Fusion: Gate
77.2
(9) Fusion: MLP
78.7
(10) Num of Blocks: 2
78.8
(11) Num of Blocks: 4
77.5
(12) Num of Blocks: 5

86.3 −
85.8
85.4
84.8
85.8
85.9
85.7
86.0
85.6
85.2
86.1
86.3
85.2

-0.8
-0.7
-1.8
-0.4
-0.7
-0.8
-0.6
-1.0
-1.7
-0.2
-0.1
-1.4

−
-0.5
-0.9
-1.5
-0.5
-0.4
-0.6
-0.3
-0.7
-1.1
-0.2
0
-1.1

Table 4: Ablation study on SQuAD dev set.

reattention mechanism and DCRL training method. We no-
tice that reattention has more inﬂuences on EM score while
DCRL contributes more to F1 metric, and removing both
of them results in huge drops on both metrics. Replacing
DCRL with SCST also causes a marginal decline of perfor-
mance on both metrics. Next, we relace the default atten-
tion function with the dot product: f (u, v) = u · v (5), and
both metrics suffer from degradations. (6-7) shows the effec-
tiveness of heuristics used in the fusion function. Removing
any of the two heuristics leads to some performance declines,
and heuristic subtraction is more effective than multiplica-
tion. Ablation (8-9) further explores different forms of fu-
sion, where gate refers to o = g ◦ ˜x and MLP denotes o = ˜x
in Eq. 4, respectively. In both cases the highway-like function
has outperformed its simpler variants. Finally, we study the
effect of different numbers of aligning blocks in (10-12). We
notice that using 2 blocks causes a slight performance drop,
while increasing to 4 blocks barely affects the SoTA result.
Interestingly, a very deep alignment with 5 blocks results in
a signiﬁcant performance decline. We argue that this is be-
cause the model encounters the degradation problem existed
in deep networks [He et al., 2016].

5.4 Effectiveness of Reattention
We further present experiments to demonstrate the effec-
tiveness of reattention mechanism. For the attention redun-

KL divergence
Redundancy
E1 to E2
E2 to E3
B1 to B2
B2 to B3
Deﬁciency
E2 to E2∗
E3 to E3∗

- Reattention

+ Reattention

0.695 ± 0.086
0.404 ± 0.067
0.976 ± 0.092
1.179 ± 0.118

0.866 ± 0.074
0.450 ± 0.052
1.207 ± 0.121
1.193 ± 0.097

0.650 ± 0.044
0.536 ± 0.047

0.568 ± 0.059
0.482 ± 0.035

Table 5: Comparison of KL diverfence on different attention distri-
butions on SQuAD dev set.

dancy problem, we measure the distance of attention distri-
butions in two adjacent aligning blocks, e.g., softmax(E1
:j)
and softmax(E2
:j). Higher distance means less attention re-
dundancy. For the attention deﬁciency problem, we take the
arithmetic mean of multiple attention distributions from the
ensemble model as the “ground truth” attention distribution
∗), and compute the distance of individual at-
softmax(Et
:j
tention softmax(Et
:j) with it. Lower distance refers to less
attention deﬁciency. We use Kullback–Leibler divergence as
the distance function D, and we report the averaged value
over all examples.

Table 5 shows the results. We ﬁrst see that the reattention
indeed help in alleviating the attention redundancy: the diver-
gence between any two adjacent blocks has been successfully
enlarged with reattention. However, we ﬁnd that the improve-
ment between the ﬁrst two blocks is larger than the one of last
two blocks. We conjecture that the ﬁrst reattention is more ac-
curate at measuring the similarity of word pairs by using the
original encoded word representation, while the latter reatten-
tion is distracted by highly nonlinear word representations. In
addition, we notice that the attention deﬁciency has also been
moderated: the divergence betwen normalized Et and Et∗ is
reduced.

5.5 Prediction Analysis

Figure 5 compares predictions made either with dynamic-
critical reinforcement learning or with self-critical sequence
training. We ﬁrst ﬁnd that both approaches are able to ob-
tain answers that match the query-sensitive category. For ex-
ample, the ﬁrst example shows that both four and two are
retrieved when the questions asks for how many. Neverthe-
less, we observe that DCRL constantly makes more accu-
rate prediction on answer spans, especially when SCST al-
ready points a rough boundary. In the second example, SCST
takes the whole phrase after Dyrrachium as its location. The
third example shows a similar phenomenon, where the SCST
retrieves the phrase constantly servicing and replacing me-
chanical brushes as its answer. We demonstrates that this is
because SCST encounters the convergence suppression prob-
lem, which impedes the prediction of ground truth answer
boundaries. DCRL, however, successfully avoids such prob-
lem and thus ﬁnds the exactly correct entity.

Figure 5: Predictions with DCRL (red) and with SCST (blue) on
SQuAD dev set.

6 Conclusion
We propose the Reinforced Mnemonic Reader, an enhanced
attention reader with two main contributions. First, a reat-
tention mechanism is introduced to alleviate the problems
of attention redundancy and deﬁciency in multi-round align-
ment architectures. Second, a dynamic-critical reinforce-
ment learning approach is presented to address the conver-
gence suppression problem existed in traditional reinforce-
ment learning methods. Our model achieves the state-of-
the-art results on the SQuAD dataset, outperforming sev-
eral strong competing systems. Besides, our model outper-
forms existing approaches by more than 6% on two adver-
sarial SQuAD datasets. We believe that both reattention and
DCRL are general approaches, and can be applied to other
NLP task such as natural language inference. Our future work
is to study the compatibility of our proposed methods.

Acknowledgments
This research work is supported by National Basic Research
Program of China under Grant No. 2014CB340303. In addi-
tion, we thank Pranav Rajpurkar for help in SQuAD submis-
sions.

References
[Chen et al., 2017] Danqi Chen, Adam Fisch, Jason We-
ston, and Antoine Bordes. Reading wikipedia to answer
open-domain questions. arXiv preprint arXiv:1704.00051,
2017.

[Dzmitry Bahdanau, 2015] Yoshua Bengio Dzmitry Bah-
danau, Kyunghyun Cho. Neural machine translation by
jointly learning to align and translate. In Proceedings of
ICLR, 2015.

[Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali
Farhadi, and Hananneh Hajishirzi. Bidirectional attention
ﬂow for machine comprehension. In Proceedings of ICLR,
2017.

[Shen et al., 2016] Yelong Shen, Po-Sen Huang, Jianfeng
Gao, and Weizhu Chen. Reasonet: Learning to stop
arXiv preprint
reading in machine comprehension.
arXiv:1609.05284, 2016.

[Srivastava et al., 2014] Nitish Srivastava, Geoffrey Hinton,
Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning Re-
search, pages 1929–1958, 2014.

[Srivastava et al., 2015] RupeshKumar Srivastava, Klaus Gr-
eff, and Jurgen Schmidhuber. Highway networks. arXiv
preprint arXiv:1505.00387, 2015.

[Sutton and Barto, 1998] Richard S. Sutton and Andrew G.
Barto. Reinforcement learning: An introduction. MIT
Press, 1998.

[Vinyals et al., 2015] Oriol Vinyals, Meire Fortunato, and
Navdeep Jaitly. Pointer networks. In Proceedings of NIPS,
2015.

[Wang and Jiang, 2017] Shuohang Wang and Jing Jiang.
Machine comprehension using match-lstm and answer
pointer. In Proceedings of ICLR, 2017.

[Wang et al., 2017] Wenhui Wang, Nan Yang, Furu Wei,
Baobao Chang, and Ming Zhou. Gated self-matching net-
works for reading comprehension and question answering.
In Proceedings of ACL, 2017.

[Weissenborn et al., 2017] Dirk Weissenborn, Georg Wiese,
and Laura Seiffe. Making neural qa as simple as possible
but not simpler. In Proceedings of CoNLL, pages 271–280,
2017.

[Xiong et al., 2017a] Caiming Xiong, Victor Zhong, and
Richard Socher. Dcn+: Mixed objective and deep resid-
ual coattention for question answering. arXiv preprint
arXiv:1711.00106, 2017.

[Xiong et al., 2017b] Caiming Xiong, Victor Zhong, and
Richard Socher. Dynamic coattention networks for ques-
tion answering. In Proceedings of ICLR, 2017.

[Goodfellow et al., 2016] Ian Goodfellow, Yoshua Bengio,
and Aaron Courville. Deep learning. MIT Press, 2016.
[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of CVPR, pages 770–778, 2016.
[Hermann et al., 2015] Karl Moritz Hermann, Tomas Ko-
cisky, Edward Grefenstette, Lasse Espeholt, Will Kay,
Mustafa Suleyman, , and Phil Blunsom. Teaching ma-
chines to read and comprehend. In Proceedings of NIPS,
2015.

[Hill et al., 2016] Felix Hill, Antoine Bordes, Sumit Chopra,
and Jason Weston. The goldilocks principle: Reading chil-
dren’s books with explicit memory representations. In Pro-
ceedings of ICLR, 2016.

[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and
J¨urgen Schmidhuber. Long short-term memory. Neural
computation, 9(8):1735—-1780, 1997.

[Huang et al., 2017] Hsin-Yuan Huang, Chenguang Zhu, Ye-
long Shen, and Weizhu Chen. Fusionnet: Fusing via fully-
aware attention with application to machine comprehen-
sion. arXiv preprint arXiv:1711.07341, 2017.

[Jia and Liang, 2017] Robin Jia and Percy Liang. Adversar-
ial examples for evaluating reading comprehension sys-
tems. In Proceedings of EMNLP, 2017.

[Kendall et al., 2017] Alex Kendall, Yarin Gal, and Roberto
Cipolla. Multi-task learning using uncertainty to weigh
losses for scene geometry and semantics. arXiv preprint
arXiv:1705.07115, 2017.

[Kingma and Ba, 2014] Diederik P. Kingma and Lei Jimmy
Ba. Adam: A method for stochastic optimization.
In
CoRR, abs/1412.6980, 2014.

[Liu et al., 2017a] Rui Liu, Junjie Hu, Wei Wei, Zi Yang,
Structural embedding of syntac-
arXiv preprint

and Eric Nyberg.
tic trees for machine comprehension.
arXiv:1703.00572, 2017.

[Liu et al., 2017b] Xiaodong Liu, Yelong Shen, Kevin
Stochastic answer networks
arXiv preprint

Duh, and Jianfeng Gao.
for machine reading comprehension.
arXiv:1712.03556, 2017.
[Pennington et al., 2014] Jeffrey

Richard
Socher, and Christopher D. Manning. Glove: Global
In Proceedings of
vectors for word representation.
EMNLP, 2014.

Pennington,

[Peters et al., 2018] Matthew E. Peters, Mark Neumann,
Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton
Lee, and Luke Zettlemoyer. Deep contextualized word
prepresentations. In Proceedings of NAACL, 2018.

[Rajpurkar et al., 2016] Pranav Rajpurkar, Jian Zhang, Kon-
stantin Lopyrev, and Percy Liang. Squad: 100,000+ ques-
tions for machine comprehension of text. In Proceedings
of EMNLP, 2016.

[Rennie et al., 2016] Steven J Rennie, Etienne Marcheret,
Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-
arXiv
critical sequence training for image captioning.
preprint arXiv:1612.00563, 2016.

