7
1
0
2
 
n
u
J
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
5
1
5
0
.
6
0
7
1
:
v
i
X
r
a

The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge

He-Da Wang
whd.thu@gmail.com

Teng Zhang
zhangteng1887@gmail.com

Ji Wu
wuji ee@mail.tsinghua.edu.cn
Multimedia Signal and Intelligent Information Processing Laboratory
Department of Electronic Engineering
Tsinghua University, Beijing, China

Abstract

This article describes the ﬁnal solution 1 of team mon-
keytyping, who ﬁnished in second place in the YouTube-8M
video understanding challenge. The dataset used in this
challenge is a large-scale benchmark for multi-label video
classiﬁcation. We extend the work in [1] and propose sev-
eral improvements for frame sequence modeling. We pro-
pose a network structure called Chaining that can better
capture the interactions between labels. Also, we report our
approaches in dealing with multi-scale information and at-
tention pooling. In addition, We ﬁnd that using the output of
model ensemble as a side target in training can boost single
model performance. We report our experiments in bagging,
boosting, cascade, and stacking, and propose a stacking al-
gorithm called attention weighted stacking. Our ﬁnal sub-
mission is an ensemble that consists of 74 sub models, all of
which are listed in the appendix.

1. Introduction

Videos have been a very important type of content on
Internet. Understanding video from its audio-visual con-
tent is key to various applications such as recommendation,
searching, and question answering. The research on video
analysis is also an important step for the computer to un-
derstand the real world. Datasets such as Sports-1M[8] and
ActivityNet[4] encourage the research on video classiﬁca-
tion of sports and human activities. YouTube-8M[1] is a
large-scale video dataset that consists of about 7.0 million
YouTube videos that was annotated with a vocabulary of
4716 tags from 24 diverse categories. The average number
of tags per video is 3.4.

The YouTube-8M video understanding challenge is an

1visit https://github.com/wangheda/youtube-8m for the source code.

arena for video classiﬁcation researchers and practitioners.
In the competition, the dataset is divided into three parts.
The training set contains 4.9 million samples. The validate
set contains 1.4 million samples. The test set contains 0.7
million samples. The ground truth labels annotated to the
samples in the training set and the validate set are available
to the participants. The test set is divided into two half,
the open test set and the blind one. In the progress of the
competition, participants can submit their predictions to the
test set. The scoring server on Kaggle would evaluate them
and return the scores on the open test set (the public leader-
board). After the competition, the winners are decided by
the scores of their submissions on the blind test set (the pri-
vate leaderboard).

In the competition, submissions are evaluated using
Global Average Precision (GAP) at 20. The metrics is cal-
culated as follows. For each video, the most conﬁdent 20
label predictions are selected along with the conﬁdence val-
ues. The tuples of the form {video, label, conf idence}
from all the videos are then put into a long list sorted by
conﬁdence values. This list of predictions are then evalu-
ated with the Average Precision (Eq. 1), in which p(i) is the
precision and r(i) is the recall given the ﬁrst i predictions.

AP =

p(i)∆r(i)

(1)

N
(cid:88)

i=1

We divide the dataset into ﬁve parts: train1, validate1,
train2, validate2, and test. The division is done base on ﬁle
name pattern (Table 1). The set train1 is used for single
model training, in which the set validate1 served as a hold-
out test set for early-stopping. Ensemble models are trained
on the set train2, which have no intersection with the train-
ing set of single models. The set validate2 is a hold-out
test set for early-stopping in the training of ensemble mod-
els. The inference procedure uses the test set to generate
submissions. The whole system structure is shown in Fig.

1

Figure 1: The structure of our system.

Part
train1
validate1
train2
validate2
test

File glob
train??.tfrecord
validate[a]?.tfrecord
validate[ˆa0-9]?.tfrecord
validate[0-9]?.tfrecord
test??.tfrecord

#samples
4,906,660
21,918
1,270,412
109,498
700,640

Table 1: Dataset division scheme in our system.

1. This data division strategy may not be optimal, since it
limit the amount of data used in single model training to the
4.9 millions training set examples, which may considerably
affect the performance of single models.

During the training of all our models, We did not use
any data augmentation techniques. Adam [10] optimization
algorithm is used throughout the training process. For mod-
els that use frame level feature rgb, audio, we use a learn-
ing rate of 0.001 and a batch size of 128. For models that
use video level feature mean rgb, mean audio, the default
learning rate and batch size is 0.01 and 1024. Note that
these hyper parameters may not be optimal since we did not
perform any parameter search. Every of our models can ﬁt
in the graphics memory of either an Nvidia GTX 1080 (8G)
or an GTX 1080Ti (11G).

In the rest of this report, we summarize our contribu-
tions in our solution. We ﬁrst introduce our improvements
over the baseline Long-Short Term Memory (LSTM) model
(Section 2). We then introduce a deep network structure
that we use in many models to capture label correlations
(Section 3). Then, we introduce our best performing sin-
gle model that utilize multiscale information via Convolu-
tional Neural Network (CNN) and LSTM (Section 4). Also,
we explore the use of attention pooling in video sequence
classiﬁcation (Section 5). Ensemble methods such as bag-
ging, boosting (Section 6), cascade (Section 7), and stack-
ing (Section 8) are also explored in this report. We also
found that using the predictions of ensemble models as a

soft target can greatly boost single model performance (Sec-
tion 9). In the end, we summarize our road-map and discuss
the contribution of our solution(Section 10).

2. Baseline models

In this section, we list the performances of some of the
baseline models in the technical report [1] and some alter-
native architectures evaluated in our system. The perfor-
mances reported here are different from the ones reported
in the technical report, since the dataset used in the report is
a little larger (8.3 million examples) than the one provided
in this competition. Also, we have limited the amount of
data used for single model training to 4.9 million examples.
The GAP scores we reported in this paper are evaluated with
the ”validate2” set if not otherwise speciﬁed.

We re-implemented four baseline approaches: Logistic
Regression (LR), Mixture-of-Expert (MoE), Deep Bag-of-
Frames (DBoF), and Long-Short Term Memory (LSTM).
The GAP scores are listed in Table 2. We choose a mixture
of 16 for the MoE model for the best performance. For the
LSTM model, we choose the number of layers, the number
of cells per layer, and the number of Moe mixtures to be 2,
1024 and 8 respectively.

Input Feature
Video Level, µ
Video Level, µ
Frame Level, {xv
Frame Level, {xv

Model
Logistic Regression
Mixture-of-Experts

} Deep Bag-of-Frames
} LSTM

1:Fv

1:Fv

GAP
0.7577
0.7957
0.7845
0.8131

Table 2: The performance of baseline approaches in our
system.

In the conventional LSTM [7], the memory cells that
share the same input gate and the same output gate form
a memory block. Putting more than one cells into a block
makes the training of LSTM more efﬁcient. We create an ar-

2

chitecture in which all the cells share a single input gate and
a single forget gate and call it ”LSTM-S”. In this architec-
ture, the cells use individual output gates, since we ﬁnd that
sharing the output gate can be harmful to the performance.
Another network structure we create by adding an input
accumulator to the architecture of LSTM model is what we
call the ”LSTM-A”. In this structure, we add a new set of
memory, input gate and forget gate to the cell to directly
”remember” the input. Eq. 2 gives the architecture of this
model. In the equation, xt, ct, ht are the input, the memory,
and the hidden state, ot, it, ft are the output gate, the input
gate, and the forget gate, c(cid:48)
t, dt are the added memory and
the hidden state for input accumulation, i(cid:48)
t, f (cid:48)
t are the added
input gate and forget gate, σ is the sigmoid function, g is
the activation function which we choose to be tanh, and n
is the L2-normalization function.

content or audio content. Therefore, the visual or audio ac-
tivities do not have to happen spontaneously to be meaning-
ful. We adopt a parallel way that model visual and audio
content with separate LSTM models. The ﬁnal states of the
two LSTM models are then concatenated and go through an
MoE unit. The performance of the parallel model is shown
in Table 4. The number of layers is 2. The number of cells
c and the number of mixtures m in MoE model are shown
in the table.

Model
vanilla LSTM, c = 1024, m = 8
parallel LSTM, cv = 1024, ca = 128, m = 8

GAP
0.8131
0.8161

Table 4: Parallel modeling of visual and audio content is
better than simple feature concatenation.





















ot
mt
it
ft
i(cid:48)
t
f (cid:48)
t
ct
ht
c(cid:48)
t
dt

= TN +2M,6N





ht−1
xt
dt−1





= σ(ft) · ct−1 + σ(it) · g(mt)
= σ(ot) · g(ct)
t) · c(cid:48)
= σ(f (cid:48)
= n(c(cid:48)
t)

t−1 + σ(i(cid:48)

t) · xt

The performance of the two modiﬁed LSTM models are
listed in Table 3. The number of cells per layer are 1024.
The number of mixtures in MoE is set to 8. The number of
layers l is shown in the table. The two models performs bet-
ter than the original model and contribute to the ensemble
because their varieties in model structure.

Model
vanilla LSTM, l = 1
vanilla LSTM, l = 2
LSTM-S, l = 1
LSTM-A, l = 1

GAP
0.8091
0.8131
0.8123
0.8131

Table 3: The performances of the single-block LSTM
(LSTM-S) and the input-accumulator LSTM (LSTM-A),
compared to the original LSTM model.

In the dataset, an example has both visual feature and
audio feature. The naive way of modeling two different
features is to concatenate them into a single feature. We
ﬁnd this to be questionable for two reasons. First, in many
videos such as music videos and family albums, the audio
and the visual content are independent of each other. Also,
people often can make sense of a video only by its visual

3

There is a potential weakness in modeling visual and au-
dio features independently. It might be preferable to allow
the visual network and the audio network to interact at cer-
tain points, since a visual / audio event may be related to a
former audio / visual event in the same video. We leave this
issue to future works.

(2)

3. Label Correlation

In multi-label classiﬁcation settings, an example may be
annotated many labels. Some labels tend to appear in the
same example at the same time, some tend not to. Such in-
formation can be used to improve the performance of multi-
label classiﬁcation models.

Classiﬁer chain [12] create a chain of classiﬁers. One
classiﬁer on the chain predict one label by using not only the
input feature but also the predictions of other labels from the
previous models on the chain. The training of one chain of
classiﬁer involves training L models in which L is the size
of the vocabulary of all labels. However, in real world prob-
lems where the direction of dependency is unknown, an en-
semble of classiﬁer chains is usually used, which makes the
computation complexity even higher and intractable if the
vocabulary of labels is large. A neural network structure
that mimic the classiﬁer chain [11] is proposed to address
the multi-label problem. However, such network structure
has a depth of L that makes it hard to optimize if the vocab-
ulary of labels is large.

We propose a novel end-to-end deep learning structure
that we called ”Chaining” to better utilize the correlation be-
tween labels. In a Chaining model (Figure 2), several repre-
sentations are joined by a chain of MoE model. The predic-
tions are projected to features of lower dimension and used
in the following stages. The representations can be gener-
ated by homogeneous models or heterogeneous ones. We
constantly apply auxiliary cross-entropy loss on the inter-

(a) A unit in Chaining accept one fea-
ture and several model predictions as the
input. The predictions are projected to
lower dimension for efﬁciency.

(b) The whole architecture of Chaining consists of several stages. A Chaining unit
in one stage accepts a feature vector, either from input directly or from a represen-
tation of LSTM or CNN, and all the predictions from earlier stages.

Figure 2: Chaining: a deep learning architecture for multi-label classiﬁcation.

mediate predictions to accelerate the training progress. The
ﬁnal loss function is a weighted average over the loss on
the ﬁnal prediction and the auxiliary losses. We typically
allocate only 10% ∼ 20% of the weights to the auxiliary
losses, since the loss function at the ﬁnal stage is the most
important.

We performed experiments using three basic models,
Mixture-of-Expert, LSTM and CNN. The CNN model we
use in our system is the same as the benchmark method of
sentence classiﬁcation [9], where the length of the ﬁlter is
ﬁxed to the size of feature vector per frame and the resulting
feature map goes through a max-over-time pooling. The ﬁ-
nal state of LSTM and the max-pooled feature map are used
as feature representation in Chaining models and go through
an MoE model for label prediction in the original models.
The performance with and without using Chaining is shown
in Table 5. For the original MoE model, the number of mix-
tures is 16. For the Chaining MoE model, the number of
mixture is 2, the number of stages is 8, and the predictions
are projected into a 128 dimensions vector. For the original
LSTM model, the number of mixtures is 8. For the Chain-
ing LSTM model, the number of mixtures is 4, the number
of stages is 2, and the dimension of projection is 200. For
the original CNN model, the width of ﬁlter and the corre-
sponding numbers of channel are 1×512, 2×512, 3×1024.
For the Chaining CNN model, the corresponding parame-
ters are 1 × 128, 2 × 128, 3 × 256, and the number of stages
is 4. The parameters are chosen to make the original models
have almost equal number of parameters with their Chain-
ing counterparts.

4. Temporal Multi-Scale Information

Input Feature
Video-level, µ
Frame Level, {xv
Frame Level, {xv

1:Fv

1:Fv

Model Original Chaining
0.8106
MoE
0.8172
0.8179

0.7965
} LSTM 0.8131
} CNN
0.7904

Table 5: The performance (GAP) of Mixture-of-Experts,
LSTM and CNN models with and without using Chaining
structure.

age would change with its distance to the observer. How-
ever, temporal information does not have the same rescaling
effect. Therefore it may seems unnatural to model videos
from different temporal scales.

However, we argue that temporal scales matters in video
analysis. A task can be divided into several actions, and
each action involves the interaction between certain objects.
The label annotated to a video can be related to a concept at
different temporal scale: an object, an action, or a task.

One important observation is that video can be seg-
mented into several clips. Each of these clips may contain
content information of a certain aspect. We can make pre-
dictions based on each of these clips and then join them
together to make more accurate predictions. The segmenta-
tion of video is done by either clustering the adjacent frames
[14] or splitting the video into clips of equal durations [15].
We propose a temporal-segment LSTM model (Fig. 3a),
in which the video is split into equal-sized clips, each of
which is modeled by an LSTM model. The models for dif-
ferent clips share the same parameters. The ﬁnal state of
each sequence are treated as another high-level sequence
and modeled by another LSTM model.

Using information from different spatial scales have
been discussed in many image analysis literatures. The
main reason to do so is that the size of an object in an im-

Temporal-pooling LSTM model (Fig. 3a) is a multi-
layer LSTM model in which we inserted temporal k-pooling
layer between LSTM layers. It is similar to the temporal-

4

ition that the representation of low-level sequences contain
more reﬁned features, while the high-level representation is
close to an averaged view of the whole video.

We propose a novel temporal CNN-LSTM model (Fig.
3c) that utilize multi-scale information. This model shares
the previous intuition that adjacent frames can be aggre-
gated to generate features at different temporal scale. In-
stead of direct operating on the original feature, we use
convolution layers to detect patterns in the low-level fea-
tures and combine adjacent ﬁlter outputs by max-pooling
along the time dimension. The ﬁlters used in a convolution
layer are of the same length with the dimension of the fea-
ture, and their widths and channels can be varied. We use
different LSTM models and Moe classiﬁers for the repre-
sentation of the feature maps of different temporal scales.
The predictions generated from features of different scales
are combine using a consensus function. We ﬁnd that av-
eraging is a good consensus function, and maximum would
lead to difﬁculties in convergence.

GAP
Model
0.8131
vanilla LSTM
temporal-pooling LSTM 0.8085
temporal-segment LSTM 0.8122
multi-resolution LSTM
0.8148
multi-scale CNN-LSTM 0.8204

Table 6: Performance of multi-scale models.

Table 6 lists the performances of the models discussed
in this section. The temporal-pooling LSTM model uses 2-
pooling and has 4 LSTM layers (#cells=1024) and 4 MoE
mixtures. The temporal-segment LSTM model uses a dura-
tion of 10 frames for the clips and has 8 MoE mixtures and
2 LSTM layers, each of the layers has 1024 memory unit.
In the multi-resolution LSTM model, we use 2-pooling to
get shorter input sequences; the number of stages in Chain-
ing is 4; the number of mixtures in MoE is 4; the projection
dimension is 256; the LSTM models in it are 2-layer par-
allel LSTM (#cells is 512 for video and 64 for audio). In
the multi-scale CNN-LSTM model, the number of layers in
CNN model is 4; the number of mixtures in MoE is 4; the
LSTM models in it are 1-layer LSTM (#cells=1024); in ev-
ery layer of the CNN model, the width of ﬁlters and the cor-
responding number of channels are 1×256, 2×256, 3×512;
the pooling layers in it are 2-pooling.

5. Identifying Salient Frames with Attention

Not all the frames in a video are equally informative.
There are many reasons that a frame might contribute little
in video classiﬁcation. Images that are too dark, too bright,
or too blurry are hard to analysis for the image classiﬁcation

(a) Temporal-pooling (with the dashes) and temporal-segment
(without the dashes) LSTM model.

(b) Multi-resolution LSTM model.

(c) Multi-scale CNN-LSTM model.

Figure 3: Models that utilize temporal multi-scale informa-
tion.

segment LSTM model. The difference is whether to use the
ﬁnal state of the one clip as the initial state of the next clip
in the ﬁrst layer of the LSTM model.

Instead of directly segmenting videos into clips, we
can aggregate the adjacent frames and gradually construct
features containing long-range information.
In multi-
resolution LSTM model (Fig. 3b), the original features
are average pooled along the time dimension to get shorter
sequences in which each frame covers longer time range.
For each sequence generated in this way, a separate LSTM
model is used to generate a sequence representation. The
representations are then joined with a Chaining model.
The representation from the highest-level goes into the ﬁrst
stage, while the one from the original sequence goes into the
ﬁnal stage of the Chaining model. This is due to the intu-

5

(b) An attention network generates multiple attention
weights for a frame based on the input feature and the
LSTM output at that frame.

(a) The multiple attention pooling scheme. The outputs
of LSTM are pooled with multiple attention weights.

Figure 4: Multiple attention pooling model.

network. Therefore, such frames may not be able to provide
useful semantic information. Also, sometimes a video con-
tains title screens, credits or text. They might be useful if
the frames are processed using optical character recognition
(OCR). However, in this competition, the frames are pre-
processed using an image classiﬁcation network pre-trained
with ImageNet dataset, which means the frame-level fea-
tures may not contain the semantic information in these text.
In addition, there are always frames irrelevant to the theme
of the video. In talk show videos the content of frames will
always be people talking while the topic of the talking is
what really important. In documentaries there are often a
lot of driving scenes which do not reﬂect the theme of the
video.

We propose a multiple attention pooling scheme (multi-
AP) for selecting salient frames in the video. As Fig. 4
shows, an LSTM model is used to deal with the frame-level
features. The outputs of the LSTM model are then aggre-
gated by pooling over the time dimension. The aggregation
is achieved by taking a weighted average over the outputs
of the LSTM model, in which the weights are generated
using an attention network, for which we use a fully con-
nected layer. The aggregated feature is fed into an MoE
model for label predictions. The attention pooling is re-
peated for multiple times and the predictions are combined
using a classiﬁer consensus function. For the choice of com-
bining method, we ﬁnd that maximum performs better than
averaging as a consensus function.

the input and output of the LSTM at a certain frame as the
input. The attention network output K groups of different
attention weights.

Each group of the weights are used to generate a group of
aggregated features and prediction results. The results from
all the groups are then combined by choosing the highest
conﬁdence predicted from all the models for each label l.
The MoE models in Fig. 4a share parameters with each
other.

eik = Wk[xi; yi]
aik = exp(eik)
(cid:80)Fv

i=1 exp(eik)

zk = (cid:80)Fv
i=1 aikyi
pk = M oE(zk)
(pk,l)
pl = max

k

(3)

(4)

We present a few examples outside of the training set to
show what kind of frames is highlighted by the attention
network in the multi-AP model (Fig. 5). For each video,
the frames with the lowest attention weights (left) and the
ones with the highest attention weights (right) are presented.
Since there are many groups of attention weight, the ﬁrst
group is used for this presentation. We have not observed
any noticeable patterns in the inter-group differences of the
weights.

We use a fully connected layer with the softmax acti-
vation function as the attention network (Eq. 3). It takes

In analysis of these examples, we ﬁnd that there are some
patterns about the attention weights. Title screens and the

6

Figure 5: Visualization of the attention weights in the multiple attention pooling model. The frames in each row comes from
the same video. The left four in each row are the frames with the lowest weights, while the right four are the ones with the
highest weights.

frames that is very dark tend to have low attention weights.
Also, if the object of interest is too small or partly blocked,
the weight of that frame tend to be low.

Our work is different from a previous work on attention
pooling [5] in two ways. First, our model uses multiple
groups of attention on the same output sequence and multi-
ple classiﬁers for prediction. The multiple attention scheme
may contribute to generating more stable classiﬁcation re-
sult. Second, our model do an attention pooling over the
outputs of LSTM model which is a global representation
of the sequence while the previous work do a pooling over
local CNN outputs.

We also adopt a local attention pooling scheme that ap-
plies attention pooling over the input features (Eq. 5). The
representation of the LSTM model is also used in the MoE
model for prediction.

The LSTM model in the four models share the same pa-
rameter (#layers=2, #cells=1024). The MoE models have 8
mixtures in the two multi-AP models, while the one in the
local-AP model has 4 mixtures. The two multi-AP models
both have 8 groups of attention weights. And the dimen-
sion of the positional embedding in the positional multi-AP
model is 32.

GAP
Model
0.8131
vanilla LSTM
0.8133
local-AP LSTM
multi-AP LSTM
0.8157
positional multi-AP LSTM 0.8169

Table 7: Performance (GAP) of attention models.

z = (cid:80)Fv

i=1 aixi

p = M oE([yFv ; z])

(5)

6. Bagging and Boosting

We compare the two attention pooling scheme with the
baseline in Table 7. The ”local-AP LSTM” refers to the
scheme using attention pooling over the input feature. The
”multi-AP LSTM” refers to the multiple attention pooling
scheme. The ”positional multi-AP LSTM” model add an
embedding for every frame position to the attention network
on the basis of the multi-AP LSTM model.

The parameters for the models in Table 7 is as follows.

Bootstrap aggregating [2], also called Bagging, is an en-
semble method that creates many versions of a model and
combines their results together. To create one version of a
model, one applies sampling with replacement to get a sub-
set of the original data, in which some original examples
may not present or present more than once[16]. Training on
different subset sampled from the original data would re-
sults in different models. The bagging algorithm is known
for its ability to reduce the variance of a model. We apply

7

Bagging to some of our models, and ﬁnd that Bagging can
generally boost the GAP performance by 0.6% ∼ 1.2%.
The results are shown in Table 8.

weights to 5 to ensure that the algorithm would not place all
the weights on a few formerly misclassiﬁed examples, since
they may not be visually classiﬁable.

Original Bagging
Input Feature Model
0.8225
0.8106
Video-level
0.8216
0.8160
Frame Level
0.8258
Frame Level
0.8179
0.8244
Frame Level multi-AP LSTM 0.8157

Chaining
parallel LSTM
Chaining CNN

Model
Chaining (Video)
parallel LSTM
Chaining CNN
multi-AP LSTM

Original Boosting
0.8218
0.8106
0.8218
0.8160
0.8242
0.8179
0.8246
0.8157

Table 8: Performance (GAP) of bagging models.

Table 9: Performance (GAP) of boosting models.

Boosting [3] is another way to create different versions
of a model. Compared to Bagging which can be run in
parallel, Boosting is a sequential ensemble method. The
(k + 1)th classiﬁer is constructed considering the previous
k classiﬁers by re-sampling a distribution that highlights the
misclassiﬁed examples.

The re-sampling is often implemented by using weighted
examples. If there are N training examples and L labels,
the number of total classiﬁcation results is N × L. Most
works on Boosting in multi-label classiﬁcation use a weight
W N ×L over N samples and L labels, which is computa-
tionally intractable when the vocabulary of label is large, as
is the case in this competition.

We adopt a per-example weighting scheme that assign
a weight W N over the training samples. The weights are
updated with Eq. 6. In the equations, Wk,n is the weight
assigned to the nth example in the training of the kth clas-
siﬁer. Errk,n is the error rate of the nth example by eval-
uating the kth classiﬁer. Errk is the average error rate of
the kth classiﬁer. And Zk is a coefﬁcient that scales the av-
erage value of Wk,n to 1. α is a parameter controlling the
highlighting effect on the misclassiﬁed examples which we
constantly set to 1 in our system.

Wk,n exp(αrkErrk,n)

W0,n = 1.0
Wk+1,n = N
Zk
in which,
rk = log( 1.0−Errk
Errk = 1
N
Errk,n ∈ [0, 1]
Zk = (cid:80)N

Errk
(cid:80)N

)

n=1 Errk,n

n=1 Wk,n exp(αrkErrk,n)

In this algorithm, the weights of the misclassiﬁed ex-
amples are increased in the following classiﬁers. However,
in multi-label classiﬁcation, misclassiﬁcation is hard to de-
ﬁne. We choose the Precision Equal Recall Rate (PERR)
as the implementation of error rate, since it is both per-
example evaluated and in coordinate with the GAP score in
most models (from empirical observation). We also clip the

The comparison between the single models and their
Boosting counterparts are shown in Table 9. The Boosting
algorithm generates a performance boost similar as Bagging
does.

7. Cascade Classiﬁer Training

Adding models into the ensemble would usually make
the performance better. However, with the number of mod-
els in the ensemble model increasing, the gain from newly
added models tends to diminish, especially if one add mod-
els that are similar to the existing models. We address this
problem by using cascade training in which the predictions
of other models as a part of its input of the model during
training.

(6)

Figure 6: Cascade layer as a replacement for MoE in cas-
cade classiﬁer training.

The structure of cascade layer is shown in Fig. 6. The
predictions from the other models are averaged to generate
an averaged prediction, which is then projected to a feature
of low dimension. All the models that have MoE as their
last layer can be modiﬁed into a cascade model by having
their MoE layer replaced by the cascade layer.

8. Stacking Methods

Stacking is an ensemble method that uses a machine
learning model to combine the predictions of many mod-
els into one prediction. We use the ”train2” set (Section 1)

8

as the training set for stacking. The trivial case of stacking
is to simply average all the predictions. If there are M mod-
els and L labels in total, simple averaging the predictions of
all models can be written as Eq. 7.

pl =

1
M

M
(cid:88)

m=1

pm,l

(7)

Linear weighted averaging (Eq.

8) is also a simple
scheme. This method has M weights, one for each model
in the ensemble.

M
(cid:88)

m=1

M
(cid:88)

m=1

pl =

wmpm,l, s.t.

wm = 1, wm > 0

(8)

1
M

M
(cid:88)

m=1

For multi-label classiﬁcation, there are L predictions for
every example. Some models might perform better than the
others on a subset of the labels. Therefore, we could extend
linear weighted averaging to every class. We denote this
method class-wise weighted averaging (Eq. 9).

1
M

M
(cid:88)

m=1

pl =

wm,lpm,l, s.t.

wm,l = 1, wm,l > 0

(9)
While the ensemble methods often deal with the predic-
tions of individual models, the original input may also help
to decide which models to trust. The winning team of Net-
ﬂix Prize proposes the Feature-Weighted Linear Stacking
[13] to utilize the information in meta-features. Although
there are no useful meta-feature in the Youtube-8M dataset,
we consider the averaged frame input and the averaged pre-
dictions from individual models as useful indicators. We
propose the attention weighted stacking to utilized these in-
formation.

The attention weights α (Eq. 10) are generated with a
fully connected layer with softmax as the activation func-
tion and averaged input ¯x and prediction ¯p as the input of
the layer.

The predictions are then weighted averaged as shown in
Eq. 11. The weight is a mixture of K low-rank matrices
(rank D) in which the matrices are selected by the output
of the attention network (Eq. 10). In the attention weighted
stacking, V, A, B, a, b, c are trainable parameters.

We perform a comparison among the stacking methods
on an ensemble of 74 models (model #1 - #74 in Appendix
A). The number of attention weights is 16 and the rank
of the matrix components is 4 in the attention weighted
stacking method. The comparison shows that the attention
weighted stacking performs better than the other methods
(Table 10).

Stacking method
Simple Averaging
Linear Weighted Averaging
Class-wise Weighted Averaging
Attention Weighted Stacking

GAP
0.8436
0.8449
0.8453
0.8458

Table 10: Performance (GAP) of different stacking models
on the entire set of models (74 models). The GAP scores
in this table is evaluated with the blind test set (the private
leaderboard score in the competition).

9. Distilling Knowledge from Ensemble

Ensemble models might perform much better than even
the best single model. However, in real world systems, the
memory and computation resources are often too limited
for ensemble models to deploy. Distillation [6] is a training
scheme that aims to compress the knowledge in ensemble
model to a single model. Another motivation of using dis-
tillation is the observation that the labels are often wrong
or incomplete, especially when the vocabulary of label is
large.

We adopt a training scheme that uses the predictions of
ensemble ˆp as a soft target along with the real target l during
training. The new loss function is a weighted average of two
losses, one for each target (Eq. 12).

α = σ(V [¯x; ¯p])

¯x = 1
Fv

(cid:80)Fv

i=1 xi

¯p = 1
M

(cid:80)M

m=1 pm

pl

= 1
M

(cid:80)M

m=1 wm,lpm,l

wm,l =

(cid:80)M

e

= (cid:80)K

exp(em,l)
m=1 exp(em,l)
k=1 αk(AT

s.t.

Ak ∈ RD×M , Bk ∈ RD×L
ak ∈ RM ×1, bk ∈ RL×1
ck ∈ R

(10)

ˆL = (1 − λ)ce(p, l) + λce(p, ˆp)

(12)

We use the predictions of model #75 (Appendix A.9) as
the soft target in training and train several models with the
same parameters as the original models. The comparison is
listed in Table 11. The comparison shows that distillation
can greatly boost single model performance (often better
than Bagging).

In the previous sections, we review our system, present
our intuition, and show how we address the problems in

9

k Bk + akbT

k + ck)

(11)

10. Summary

Original Distillation
Model
0.8106
Chaining (Video)
0.8160
parallel LSTM
0.8179
Chaining CNN
0.8172
Chaining LSTM
Multi-scale CNN-LSTM 0.8204

0.8169
0.8237
0.8266
0.8291
0.8258

Table 11: Performance (GAP) of single models trained us-
ing distillation.

multi-label video classiﬁcation. We ﬁnd that attention pool-
ing and temporal multi-scale information is very important
for video sequence modeling. We also propose a network
structure for large-vocabulary multi-label classiﬁcation. We
review our work in ensemble methods such as Bagging,
Boosting, Cascade, Distillation and Stacking. We propose a
stacking network that uses attention to weight models. Our
system road-map is shown in Table 12.

Changes
27 single models
+ 11 bagging & boosting models
+ 8 distillation models
+ 11 cascade models
+ 17 more cascade models
Attention Weighted Stacking

GAP
0.8425
0.8435
0.8437
0.8451
0.8453
0.8458

Table 12: Performance (GAP) of ensemble models. The
stacking method used for the ﬁrst 5 results is class-wise
weighted model. The ensemble in each row include all the
changes in the rows above it. The results reported in this
table are evaluated on the blind test set (the private leader-
board score in the competition).

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m:
CoRR,
A large-scale video classiﬁcation benchmark.
abs/1609.08675, 2016.

[2] L. Breiman.

Bagging predictors. Machine Learning,

24(2):123–140, 1996.

[3] L. Breiman. Arcing classiﬁer (with discussion and a rejoin-
der by the author). Ann. Statist., 26(3):801–849, 06 1998.
[4] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 961–970, 2015.

[5] M. J. Er, Y. Zhang, N. Wang, and M. Pratama. Atten-
tion pooling-based convolutional neural network for sen-
tence modelling. Information Sciences, 373:388 – 403, 2016.
[6] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, pages
1725–1732, 2014.

[9] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014.

[11] J. Read and J. Hollm´en. A deep interpretation of classi-
ﬁer chains. In International Symposium on Intelligent Data
Analysis, pages 251–262. Springer, 2014.

[12] J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classiﬁer
chains for multi-label classiﬁcation. Machine Learning and
Knowledge Discovery in Databases, pages 254–269, 2009.

[13] J. Sill, G. Tak´acs, L. Mackey, and D. Lin. Feature-weighted
linear stacking. arXiv preprint arXiv:0911.0460, 2009.
[14] E. H. Spriggs, F. De La Torre, and M. Hebert. Temporal seg-
mentation and activity classiﬁcation from ﬁrst-person sens-
ing. In Computer Vision and Pattern Recognition Workshops,
2009. CVPR Workshops 2009. IEEE Computer Society Con-
ference On, pages 17–24. IEEE, 2009.

[15] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
towards good
L. Van Gool. Temporal segment networks:
practices for deep action recognition. In European Confer-
ence on Computer Vision, pages 20–36. Springer, 2016.

[16] Z.-H. Zhou. Ensemble methods:

foundations and algo-

rithms. CRC press, 2012.

10

Appendix A. Final Submission

In this section we list all the components included in our
ﬁnal submission. We apply a stacking method called the At-
tention Weighted Average (Section 8) to merge the result of
these models instead of carrying on careful model selection.
Therefore it might be possible that a subset of these models
could reach equal or better performance. The parameters
are for a quick grasp of the structure of the models. Re-
fer to our code 2 if you are interested in the implementation
details. The GAP scores reported in the appendix are eval-
uated with the ”validate2” set if not otherwise speciﬁed.

A.1. Video-level Models

1. Chaining model (Section 3). During training, if the
loss of the current batch is less than 10, use the most conﬁ-
dent tag predicted as a soft target for every example in the
batch. #mixture of MoE is 4. #stage of Chaining is 3. pro-
jection dimension in Chaining is 100. GAP = 0.8067.

2. Chaining model. #mixture of MoE is 2. #stage of
Chaining is 8. projection dimension in Chaining is 128.
GAP = 0.8106.

A.2. Baseline Models

If not otherwise speciﬁed, the memory cells of the ﬁnal
state are used for the input of the MoE model in the baseline
models.

3. LSTM model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8131.
4. LSTM model. #cell of LSTM is 2048. #layer of

LSTM is 2. #mixture of MoE is 4. GAP = 0.8152.

5. LSTM model of 2 layers. The ﬁrst layer is bi-
directional. The second layer is uni-directional. #cell of
LSTM is 1024,. #mixture of MoE is 4. GAP = 0.8105.

6. LSTM-S model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8123.
7. LSTM-A model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.
8. LSTM-A model of 2 layers. The ﬁrst layer is a for-
ward LSTM-A model. The second layer is a backward
LSTM-A model that takes the original input and the out-
put of the ﬁrst layer as input. #cell of LSTM is 1024. #layer
of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.

9. Parallel LSTM model (Section 2). #cell of LSTM is
1024. #layer of LSTM is 2. #mixture of MoE is 8. GAP =
0.8161.

10. Parallel LSTM model. The gated outputs of the ﬁ-
nal state are used for the input of the MoE model. #cell of
LSTM is 1024. #layer of LSTM is 2. #mixture of MoE is
8. GAP = 0.8160.

11. LSTM model with data augmentation by random
sampling 50% of the frames. #cell of LSTM is 1024. #layer

2https://github.com/wangheda/youtube-8m

of LSTM is 2. #mixture of MoE is 8. GAP = 0.8137.

12. CNN-LSTM model. The CNN model is described
in section 3. #layer of CNN is 1. The widths of ﬁlter are
1, 2 and 3. The corresponding #channels of ﬁlters are 1024,
1024, and 1024. #cell of LSTM is 1024. #layer of LSTM is
2. #mixture of MoE is 8. GAP = 0.8103.

A.3. Temporal Multi-Scale Models

13. Temporal-segment LSTM model (Section 4). The
duration of each clip is 10 frames. #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8122.
14. Temporal-pooling LSTM model (Section 4). The
model has 2-pooling between LSTM layers. #cell of LSTM
is 1024. #layer of LSTM is 4, #mixture of MoE is 8. GAP
= 0.8085.

15. Multi-resolution LSTM model (Section 4). The
model has parallel LSTM for sequence modeling. The
model has 2-pooling to get shorter representation of input.
#cell of LSTM is 512 for video and 64 for audio. #layer
of LSTM is 2. #stage of Chaining is 4. The dimension of
projection in Chaining is 256. #mixture of MoE is 4. GAP
= 0.8149.

16. Multi-scale CNN-LSTM model (Section 4). The
widths of ﬁlter are 1, 2 and 3. The corresponding #chan-
nels of ﬁlters are 256, 256, and 512. #layer in CNN is 4.
#layer of LSTM is 1. #cell of LSTM is 1024. #mixture of
MoE is 4. GAP = 0.8204.

17. Multi-scale CNN-LSTM model. All the parameters
are the same as model #16, except that the type of the LSTM
model is LSTM-S (as in model #6). GAP = 0.8147.

A.4. Chaining Models

18. Chaining CNN model (Section 3). The widths of ﬁl-
ter are 1, 2 and 3. The corresponding #channels of ﬁlters are
128, 128, and 256. The dimension of projection in Chaining
is 256. #mixture of MoE is 4. #stage of Chaining is 4. GAP
= 0.8179.

19. Chaining Deep CNN model. In this model a 3-layer
CNN model is used. The averaged input feature and the
3 max-pooled feature maps from the 3 layers of CNN are
combined using a 4-stage Chaining model. The widths of
ﬁlter are 1, 2 and 3. The corresponding #channels of ﬁl-
ters are 128, 128, and 256. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. GAP = 0.8155.

20. Chaining LSTM-CNN model. This model uses a
cascade of parallel LSTM and CNN model as the sub-model
in Chaining.#layer of LSTM is 1. #cell of LSTM is 1024 for
video and 128 for audio. The widths of ﬁlter are 1, 2 and
3. The corresponding #channels of ﬁlters are 128, 128, and
256. The dimension of projection in Chaining is 128. #stage
in Chaining is 3. #mixture of MoE is 4. GAP = 0.8122.

21. Chaining LSTM model (Section 3). #layer of LSTM
is 2. #cell of LSTM is 1024. The dimension of projection in

11

Chaining is 200. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8172.

22. Chaining LSTM model. The different stages in
Chaining model uses a shared input which is the cell mem-
ory of the ﬁnal state of an LSTM model. #layer of LSTM is
2. #cell of LSTM is 1024. The dimension of projection in
Chaining is 256. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8162.

A.5. Attention Pooling Models

23. Local attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. GAP =
0.8133.

24. Attention pooling LSTM model that has one LSTM
model to generate the attention weights for pooling over the
output of another LSTM model. #layer of LSTM is 1. #cell
of LSTM is 1024. #mixture of MoE is 8. GAP = 0.8088.

25. Multiple attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. #mixture of
MoE is 4. #group of attention weights is 8. GAP = 0.8157.
26. Multiple attention pooling LSTM model that has one
LSTM model to generate the attention weights for pooling
over the output of another LSTM model. #layer of LSTM
is 2. #cell of LSTM is 1024. #mixture of MoE is 4. #group
of attention weights is 8. GAP = 0.8081.

27. Multiple attention pooling LSTM model with posi-
tional embedding (Section 5). #layer of LSTM is 2. #cell
of LSTM is 1024. #mixture of MoE is 4. The dimension of
positional embedding is 32. #group of attention weights is
8. GAP = 0.8169.

A.6. Bagging and Boosting Models

GAP = 0.8218.

33. Bagging model of 8 versions of model #18, com-
bined with class-wise weighted averaging. GAP = 0.8258.
34. Boosting model of 8 versions of model #18, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8246.

35. Bagging model of 8 versions of model #25, com-
bined with class-wise weighted averaging. GAP = 0.8244.
36. Boosting model of 8 versions of model #25, com-
bined with attention weighted stacking, with weight clip-
ping. GAP = 0.8242.

37. Bagging model of 8 versions of model #10, com-

bined with simple averaging. GAP = 0.8216.

38. Boosting model of 8 versions of model #10, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8218.

A.7. Distillation Models

The models in this section is trained using distillation
(Section 9). During training, the predictions from model
#75 are used as soft targets.

39. Model #2 with distillation training. GAP = 0.8169.
40. Model #18 with distillation training. GAP = 0.8266.
41. Model #20 with distillation training. GAP = 0.8259.
42. Model #10 with distillation training. GAP = 0.8237.
43. Model #21 with distillation training. GAP = 0.8291.
44. Model #16 with distillation training. GAP = 0.8258.
45. Bagging model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8249.

46. Boosting model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8254.

28. Bagging model of 8 versions of model #2, combined

with simple averaging. GAP = 0.8225.

A.8. Cascade Models

29. Ensemble of 4 Chaining models. The ﬁrst is the
normal Chaining model with video-level input. The second
is a Chaining model with weighted cross entropy loss. The
third is a Chaining model that predict label and top-level
verticals at the same time. The fourth is a Chaining model
that predict the infrequent labels with softmax function. All
4 models share the same parameters. #mixture of MoE is
4. #stage in Chaining is 3. The dimension of projection in
Chaining is 100. GAP = 0.8216.

30. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging. Remove the ex-
amples with too high weights from training set, since it may
be hopeless to predict these examples correctly. GAP =
0.8213.

31. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging, without weight
clipping. GAP = 0.8198.

32. Boosting model of 8 versions of model #2, combined
with class-wise weighted averaging, with weight clipping.

The models in this section are models using the predic-

tion of other models as part of the input (Section 7).

47. Model #2 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8231.

48. Model #6 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8245.

49. Model #18 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8268.

50. Model #25 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

51. Chaining CNN model with cascade training using
the prediction of model #75 as part of the input. The widths
of ﬁlter are 1, 2 and 3. The corresponding #channels of
ﬁlters are 256, 256, and 512. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. #stage of Chaining
is 2. GAP = 0.8214.

52. Model #3 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

12

74. Model #2 with cascade training using the L1-
normalized prediction of model #76 as part of the input.
GAP = 0.8227.

A.9. Ensemble Models

In this section we lists the performance of some of the
ensemble models. Alongside the GAP score on ”validate2”
set, we also report the GAP score on the blind test set (Pri-
vate LeaderBoard in the competition).

75. Ensemble of 4 single models. Model #2, #10, #18,
and #25 with a class-wise weighted stacking gets a GAP of
0.8373. Private LB GAP = 0.83703.

76. Ensemble of 8 single models. Model #2, #10, #18,
#25, #47, #48, #49, and #50 with a class-wise weighted
stacking gets a GAP of 0.8397. Private LB GAP = 0.83941.
77. Ensemble of 27 single models. Model #1 - #27 with a
class-wise weighted stacking gets a GAP of 0.8427. Private
LB GAP = 0.84250.

78. Ensemble of 57 models. Model #1 - #57 with an
attention weighted stacking (#rank=3, #attention=16) gets a
GAP of 0.8459. Private LB GAP = 0.84561.

79. Ensemble of 74 models. Model #1 - #74 with an
attention weighted stacking (#rank=4, #attention=16) gets a
GAP of 0.8462. Private LB GAP = 0.84583.

80. Ensemble of model #78 and #79 by getting the most
conﬁdent 20 tags from each model and averaging the con-
ﬁdence scores. This is done directly on the test set without
training. Private LB GAP = 0.84590. This is our ﬁnal sub-
mission.

Appendix B. Source Code

The source code of our solution is made public on
GitHub. Visit https://github.com/wangheda/youtube-8m for
the implementation details of our models.

53. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 2.
GAP = 0.8214.

54. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 4.
GAP = 0.8266.

55. Model #20 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8265.

56. Model #6 with cascade training using the prediction
of model #75 as part of the input. The prediction of model
#6 for infrequent labels and the prediction of the cascade
model for frequent ones are joined together as the ﬁnal pre-
diction. GAP = 0.8228.

57. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8202.

58. Model #25 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8254.

59. Model #20 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8250.

60. Model #6 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8245.

61. Model #7 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8247.

62. Model #21 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8251.

63. Model #16 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8258.

64. Model #10 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8248.

65. Model #10 with cascade training using the prediction
of model #76 as part of the input. The prediction of model
#76 is also used to up-sample the misclassiﬁed samples as
in the Boosting model. GAP = 0.8218.

66. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8181.

67. Model #2 with cascade training using the predictions

of model #76 and #66 as part of the input. GAP = 0.7989.

68. Model #2 with cascade training using the predictions
of model #76, #66, and #67 as part of the input. GAP =
0.7849.

69. Model #2 with cascade training using the predictions
of model #76, #66, #67, and #68 as part of the input. GAP
= 0.7753.

70. Model #2 with cascade training using the predictions
of model #76, #66, #67, #68, and #69 as part of the input.
GAP = 0.7833.

71. Model #2 with cascade training using the predictions

of model #76 as part of the input. GAP = 0.8183.

72. Model #2 with cascade training using the predictions

of model #76 and #71 as part of the input. GAP = 0.8085.

73. Model #2 with cascade training using the predictions
of model #76, #71, and #72 as part of the input. GAP =
0.8111.

13

7
1
0
2
 
n
u
J
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
5
1
5
0
.
6
0
7
1
:
v
i
X
r
a

The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge

He-Da Wang
whd.thu@gmail.com

Teng Zhang
zhangteng1887@gmail.com

Ji Wu
wuji ee@mail.tsinghua.edu.cn
Multimedia Signal and Intelligent Information Processing Laboratory
Department of Electronic Engineering
Tsinghua University, Beijing, China

Abstract

This article describes the ﬁnal solution 1 of team mon-
keytyping, who ﬁnished in second place in the YouTube-8M
video understanding challenge. The dataset used in this
challenge is a large-scale benchmark for multi-label video
classiﬁcation. We extend the work in [1] and propose sev-
eral improvements for frame sequence modeling. We pro-
pose a network structure called Chaining that can better
capture the interactions between labels. Also, we report our
approaches in dealing with multi-scale information and at-
tention pooling. In addition, We ﬁnd that using the output of
model ensemble as a side target in training can boost single
model performance. We report our experiments in bagging,
boosting, cascade, and stacking, and propose a stacking al-
gorithm called attention weighted stacking. Our ﬁnal sub-
mission is an ensemble that consists of 74 sub models, all of
which are listed in the appendix.

1. Introduction

Videos have been a very important type of content on
Internet. Understanding video from its audio-visual con-
tent is key to various applications such as recommendation,
searching, and question answering. The research on video
analysis is also an important step for the computer to un-
derstand the real world. Datasets such as Sports-1M[8] and
ActivityNet[4] encourage the research on video classiﬁca-
tion of sports and human activities. YouTube-8M[1] is a
large-scale video dataset that consists of about 7.0 million
YouTube videos that was annotated with a vocabulary of
4716 tags from 24 diverse categories. The average number
of tags per video is 3.4.

The YouTube-8M video understanding challenge is an

1visit https://github.com/wangheda/youtube-8m for the source code.

arena for video classiﬁcation researchers and practitioners.
In the competition, the dataset is divided into three parts.
The training set contains 4.9 million samples. The validate
set contains 1.4 million samples. The test set contains 0.7
million samples. The ground truth labels annotated to the
samples in the training set and the validate set are available
to the participants. The test set is divided into two half,
the open test set and the blind one. In the progress of the
competition, participants can submit their predictions to the
test set. The scoring server on Kaggle would evaluate them
and return the scores on the open test set (the public leader-
board). After the competition, the winners are decided by
the scores of their submissions on the blind test set (the pri-
vate leaderboard).

In the competition, submissions are evaluated using
Global Average Precision (GAP) at 20. The metrics is cal-
culated as follows. For each video, the most conﬁdent 20
label predictions are selected along with the conﬁdence val-
ues. The tuples of the form {video, label, conf idence}
from all the videos are then put into a long list sorted by
conﬁdence values. This list of predictions are then evalu-
ated with the Average Precision (Eq. 1), in which p(i) is the
precision and r(i) is the recall given the ﬁrst i predictions.

AP =

p(i)∆r(i)

(1)

N
(cid:88)

i=1

We divide the dataset into ﬁve parts: train1, validate1,
train2, validate2, and test. The division is done base on ﬁle
name pattern (Table 1). The set train1 is used for single
model training, in which the set validate1 served as a hold-
out test set for early-stopping. Ensemble models are trained
on the set train2, which have no intersection with the train-
ing set of single models. The set validate2 is a hold-out
test set for early-stopping in the training of ensemble mod-
els. The inference procedure uses the test set to generate
submissions. The whole system structure is shown in Fig.

1

Figure 1: The structure of our system.

Part
train1
validate1
train2
validate2
test

File glob
train??.tfrecord
validate[a]?.tfrecord
validate[ˆa0-9]?.tfrecord
validate[0-9]?.tfrecord
test??.tfrecord

#samples
4,906,660
21,918
1,270,412
109,498
700,640

Table 1: Dataset division scheme in our system.

1. This data division strategy may not be optimal, since it
limit the amount of data used in single model training to the
4.9 millions training set examples, which may considerably
affect the performance of single models.

During the training of all our models, We did not use
any data augmentation techniques. Adam [10] optimization
algorithm is used throughout the training process. For mod-
els that use frame level feature rgb, audio, we use a learn-
ing rate of 0.001 and a batch size of 128. For models that
use video level feature mean rgb, mean audio, the default
learning rate and batch size is 0.01 and 1024. Note that
these hyper parameters may not be optimal since we did not
perform any parameter search. Every of our models can ﬁt
in the graphics memory of either an Nvidia GTX 1080 (8G)
or an GTX 1080Ti (11G).

In the rest of this report, we summarize our contribu-
tions in our solution. We ﬁrst introduce our improvements
over the baseline Long-Short Term Memory (LSTM) model
(Section 2). We then introduce a deep network structure
that we use in many models to capture label correlations
(Section 3). Then, we introduce our best performing sin-
gle model that utilize multiscale information via Convolu-
tional Neural Network (CNN) and LSTM (Section 4). Also,
we explore the use of attention pooling in video sequence
classiﬁcation (Section 5). Ensemble methods such as bag-
ging, boosting (Section 6), cascade (Section 7), and stack-
ing (Section 8) are also explored in this report. We also
found that using the predictions of ensemble models as a

soft target can greatly boost single model performance (Sec-
tion 9). In the end, we summarize our road-map and discuss
the contribution of our solution(Section 10).

2. Baseline models

In this section, we list the performances of some of the
baseline models in the technical report [1] and some alter-
native architectures evaluated in our system. The perfor-
mances reported here are different from the ones reported
in the technical report, since the dataset used in the report is
a little larger (8.3 million examples) than the one provided
in this competition. Also, we have limited the amount of
data used for single model training to 4.9 million examples.
The GAP scores we reported in this paper are evaluated with
the ”validate2” set if not otherwise speciﬁed.

We re-implemented four baseline approaches: Logistic
Regression (LR), Mixture-of-Expert (MoE), Deep Bag-of-
Frames (DBoF), and Long-Short Term Memory (LSTM).
The GAP scores are listed in Table 2. We choose a mixture
of 16 for the MoE model for the best performance. For the
LSTM model, we choose the number of layers, the number
of cells per layer, and the number of Moe mixtures to be 2,
1024 and 8 respectively.

Input Feature
Video Level, µ
Video Level, µ
Frame Level, {xv
Frame Level, {xv

Model
Logistic Regression
Mixture-of-Experts

} Deep Bag-of-Frames
} LSTM

1:Fv

1:Fv

GAP
0.7577
0.7957
0.7845
0.8131

Table 2: The performance of baseline approaches in our
system.

In the conventional LSTM [7], the memory cells that
share the same input gate and the same output gate form
a memory block. Putting more than one cells into a block
makes the training of LSTM more efﬁcient. We create an ar-

2

chitecture in which all the cells share a single input gate and
a single forget gate and call it ”LSTM-S”. In this architec-
ture, the cells use individual output gates, since we ﬁnd that
sharing the output gate can be harmful to the performance.
Another network structure we create by adding an input
accumulator to the architecture of LSTM model is what we
call the ”LSTM-A”. In this structure, we add a new set of
memory, input gate and forget gate to the cell to directly
”remember” the input. Eq. 2 gives the architecture of this
model. In the equation, xt, ct, ht are the input, the memory,
and the hidden state, ot, it, ft are the output gate, the input
gate, and the forget gate, c(cid:48)
t, dt are the added memory and
the hidden state for input accumulation, i(cid:48)
t, f (cid:48)
t are the added
input gate and forget gate, σ is the sigmoid function, g is
the activation function which we choose to be tanh, and n
is the L2-normalization function.

content or audio content. Therefore, the visual or audio ac-
tivities do not have to happen spontaneously to be meaning-
ful. We adopt a parallel way that model visual and audio
content with separate LSTM models. The ﬁnal states of the
two LSTM models are then concatenated and go through an
MoE unit. The performance of the parallel model is shown
in Table 4. The number of layers is 2. The number of cells
c and the number of mixtures m in MoE model are shown
in the table.

Model
vanilla LSTM, c = 1024, m = 8
parallel LSTM, cv = 1024, ca = 128, m = 8

GAP
0.8131
0.8161

Table 4: Parallel modeling of visual and audio content is
better than simple feature concatenation.





















ot
mt
it
ft
i(cid:48)
t
f (cid:48)
t
ct
ht
c(cid:48)
t
dt

= TN +2M,6N





ht−1
xt
dt−1





= σ(ft) · ct−1 + σ(it) · g(mt)
= σ(ot) · g(ct)
t) · c(cid:48)
= σ(f (cid:48)
= n(c(cid:48)
t)

t−1 + σ(i(cid:48)

t) · xt

The performance of the two modiﬁed LSTM models are
listed in Table 3. The number of cells per layer are 1024.
The number of mixtures in MoE is set to 8. The number of
layers l is shown in the table. The two models performs bet-
ter than the original model and contribute to the ensemble
because their varieties in model structure.

Model
vanilla LSTM, l = 1
vanilla LSTM, l = 2
LSTM-S, l = 1
LSTM-A, l = 1

GAP
0.8091
0.8131
0.8123
0.8131

Table 3: The performances of the single-block LSTM
(LSTM-S) and the input-accumulator LSTM (LSTM-A),
compared to the original LSTM model.

In the dataset, an example has both visual feature and
audio feature. The naive way of modeling two different
features is to concatenate them into a single feature. We
ﬁnd this to be questionable for two reasons. First, in many
videos such as music videos and family albums, the audio
and the visual content are independent of each other. Also,
people often can make sense of a video only by its visual

3

There is a potential weakness in modeling visual and au-
dio features independently. It might be preferable to allow
the visual network and the audio network to interact at cer-
tain points, since a visual / audio event may be related to a
former audio / visual event in the same video. We leave this
issue to future works.

(2)

3. Label Correlation

In multi-label classiﬁcation settings, an example may be
annotated many labels. Some labels tend to appear in the
same example at the same time, some tend not to. Such in-
formation can be used to improve the performance of multi-
label classiﬁcation models.

Classiﬁer chain [12] create a chain of classiﬁers. One
classiﬁer on the chain predict one label by using not only the
input feature but also the predictions of other labels from the
previous models on the chain. The training of one chain of
classiﬁer involves training L models in which L is the size
of the vocabulary of all labels. However, in real world prob-
lems where the direction of dependency is unknown, an en-
semble of classiﬁer chains is usually used, which makes the
computation complexity even higher and intractable if the
vocabulary of labels is large. A neural network structure
that mimic the classiﬁer chain [11] is proposed to address
the multi-label problem. However, such network structure
has a depth of L that makes it hard to optimize if the vocab-
ulary of labels is large.

We propose a novel end-to-end deep learning structure
that we called ”Chaining” to better utilize the correlation be-
tween labels. In a Chaining model (Figure 2), several repre-
sentations are joined by a chain of MoE model. The predic-
tions are projected to features of lower dimension and used
in the following stages. The representations can be gener-
ated by homogeneous models or heterogeneous ones. We
constantly apply auxiliary cross-entropy loss on the inter-

(a) A unit in Chaining accept one fea-
ture and several model predictions as the
input. The predictions are projected to
lower dimension for efﬁciency.

(b) The whole architecture of Chaining consists of several stages. A Chaining unit
in one stage accepts a feature vector, either from input directly or from a represen-
tation of LSTM or CNN, and all the predictions from earlier stages.

Figure 2: Chaining: a deep learning architecture for multi-label classiﬁcation.

mediate predictions to accelerate the training progress. The
ﬁnal loss function is a weighted average over the loss on
the ﬁnal prediction and the auxiliary losses. We typically
allocate only 10% ∼ 20% of the weights to the auxiliary
losses, since the loss function at the ﬁnal stage is the most
important.

We performed experiments using three basic models,
Mixture-of-Expert, LSTM and CNN. The CNN model we
use in our system is the same as the benchmark method of
sentence classiﬁcation [9], where the length of the ﬁlter is
ﬁxed to the size of feature vector per frame and the resulting
feature map goes through a max-over-time pooling. The ﬁ-
nal state of LSTM and the max-pooled feature map are used
as feature representation in Chaining models and go through
an MoE model for label prediction in the original models.
The performance with and without using Chaining is shown
in Table 5. For the original MoE model, the number of mix-
tures is 16. For the Chaining MoE model, the number of
mixture is 2, the number of stages is 8, and the predictions
are projected into a 128 dimensions vector. For the original
LSTM model, the number of mixtures is 8. For the Chain-
ing LSTM model, the number of mixtures is 4, the number
of stages is 2, and the dimension of projection is 200. For
the original CNN model, the width of ﬁlter and the corre-
sponding numbers of channel are 1×512, 2×512, 3×1024.
For the Chaining CNN model, the corresponding parame-
ters are 1 × 128, 2 × 128, 3 × 256, and the number of stages
is 4. The parameters are chosen to make the original models
have almost equal number of parameters with their Chain-
ing counterparts.

4. Temporal Multi-Scale Information

Input Feature
Video-level, µ
Frame Level, {xv
Frame Level, {xv

1:Fv

1:Fv

Model Original Chaining
0.8106
MoE
0.8172
0.8179

0.7965
} LSTM 0.8131
} CNN
0.7904

Table 5: The performance (GAP) of Mixture-of-Experts,
LSTM and CNN models with and without using Chaining
structure.

age would change with its distance to the observer. How-
ever, temporal information does not have the same rescaling
effect. Therefore it may seems unnatural to model videos
from different temporal scales.

However, we argue that temporal scales matters in video
analysis. A task can be divided into several actions, and
each action involves the interaction between certain objects.
The label annotated to a video can be related to a concept at
different temporal scale: an object, an action, or a task.

One important observation is that video can be seg-
mented into several clips. Each of these clips may contain
content information of a certain aspect. We can make pre-
dictions based on each of these clips and then join them
together to make more accurate predictions. The segmenta-
tion of video is done by either clustering the adjacent frames
[14] or splitting the video into clips of equal durations [15].
We propose a temporal-segment LSTM model (Fig. 3a),
in which the video is split into equal-sized clips, each of
which is modeled by an LSTM model. The models for dif-
ferent clips share the same parameters. The ﬁnal state of
each sequence are treated as another high-level sequence
and modeled by another LSTM model.

Using information from different spatial scales have
been discussed in many image analysis literatures. The
main reason to do so is that the size of an object in an im-

Temporal-pooling LSTM model (Fig. 3a) is a multi-
layer LSTM model in which we inserted temporal k-pooling
layer between LSTM layers. It is similar to the temporal-

4

ition that the representation of low-level sequences contain
more reﬁned features, while the high-level representation is
close to an averaged view of the whole video.

We propose a novel temporal CNN-LSTM model (Fig.
3c) that utilize multi-scale information. This model shares
the previous intuition that adjacent frames can be aggre-
gated to generate features at different temporal scale. In-
stead of direct operating on the original feature, we use
convolution layers to detect patterns in the low-level fea-
tures and combine adjacent ﬁlter outputs by max-pooling
along the time dimension. The ﬁlters used in a convolution
layer are of the same length with the dimension of the fea-
ture, and their widths and channels can be varied. We use
different LSTM models and Moe classiﬁers for the repre-
sentation of the feature maps of different temporal scales.
The predictions generated from features of different scales
are combine using a consensus function. We ﬁnd that av-
eraging is a good consensus function, and maximum would
lead to difﬁculties in convergence.

GAP
Model
0.8131
vanilla LSTM
temporal-pooling LSTM 0.8085
temporal-segment LSTM 0.8122
multi-resolution LSTM
0.8148
multi-scale CNN-LSTM 0.8204

Table 6: Performance of multi-scale models.

Table 6 lists the performances of the models discussed
in this section. The temporal-pooling LSTM model uses 2-
pooling and has 4 LSTM layers (#cells=1024) and 4 MoE
mixtures. The temporal-segment LSTM model uses a dura-
tion of 10 frames for the clips and has 8 MoE mixtures and
2 LSTM layers, each of the layers has 1024 memory unit.
In the multi-resolution LSTM model, we use 2-pooling to
get shorter input sequences; the number of stages in Chain-
ing is 4; the number of mixtures in MoE is 4; the projection
dimension is 256; the LSTM models in it are 2-layer par-
allel LSTM (#cells is 512 for video and 64 for audio). In
the multi-scale CNN-LSTM model, the number of layers in
CNN model is 4; the number of mixtures in MoE is 4; the
LSTM models in it are 1-layer LSTM (#cells=1024); in ev-
ery layer of the CNN model, the width of ﬁlters and the cor-
responding number of channels are 1×256, 2×256, 3×512;
the pooling layers in it are 2-pooling.

5. Identifying Salient Frames with Attention

Not all the frames in a video are equally informative.
There are many reasons that a frame might contribute little
in video classiﬁcation. Images that are too dark, too bright,
or too blurry are hard to analysis for the image classiﬁcation

(a) Temporal-pooling (with the dashes) and temporal-segment
(without the dashes) LSTM model.

(b) Multi-resolution LSTM model.

(c) Multi-scale CNN-LSTM model.

Figure 3: Models that utilize temporal multi-scale informa-
tion.

segment LSTM model. The difference is whether to use the
ﬁnal state of the one clip as the initial state of the next clip
in the ﬁrst layer of the LSTM model.

Instead of directly segmenting videos into clips, we
can aggregate the adjacent frames and gradually construct
features containing long-range information.
In multi-
resolution LSTM model (Fig. 3b), the original features
are average pooled along the time dimension to get shorter
sequences in which each frame covers longer time range.
For each sequence generated in this way, a separate LSTM
model is used to generate a sequence representation. The
representations are then joined with a Chaining model.
The representation from the highest-level goes into the ﬁrst
stage, while the one from the original sequence goes into the
ﬁnal stage of the Chaining model. This is due to the intu-

5

(b) An attention network generates multiple attention
weights for a frame based on the input feature and the
LSTM output at that frame.

(a) The multiple attention pooling scheme. The outputs
of LSTM are pooled with multiple attention weights.

Figure 4: Multiple attention pooling model.

network. Therefore, such frames may not be able to provide
useful semantic information. Also, sometimes a video con-
tains title screens, credits or text. They might be useful if
the frames are processed using optical character recognition
(OCR). However, in this competition, the frames are pre-
processed using an image classiﬁcation network pre-trained
with ImageNet dataset, which means the frame-level fea-
tures may not contain the semantic information in these text.
In addition, there are always frames irrelevant to the theme
of the video. In talk show videos the content of frames will
always be people talking while the topic of the talking is
what really important. In documentaries there are often a
lot of driving scenes which do not reﬂect the theme of the
video.

We propose a multiple attention pooling scheme (multi-
AP) for selecting salient frames in the video. As Fig. 4
shows, an LSTM model is used to deal with the frame-level
features. The outputs of the LSTM model are then aggre-
gated by pooling over the time dimension. The aggregation
is achieved by taking a weighted average over the outputs
of the LSTM model, in which the weights are generated
using an attention network, for which we use a fully con-
nected layer. The aggregated feature is fed into an MoE
model for label predictions. The attention pooling is re-
peated for multiple times and the predictions are combined
using a classiﬁer consensus function. For the choice of com-
bining method, we ﬁnd that maximum performs better than
averaging as a consensus function.

the input and output of the LSTM at a certain frame as the
input. The attention network output K groups of different
attention weights.

Each group of the weights are used to generate a group of
aggregated features and prediction results. The results from
all the groups are then combined by choosing the highest
conﬁdence predicted from all the models for each label l.
The MoE models in Fig. 4a share parameters with each
other.

eik = Wk[xi; yi]
aik = exp(eik)
(cid:80)Fv

i=1 exp(eik)

zk = (cid:80)Fv
i=1 aikyi
pk = M oE(zk)
(pk,l)
pl = max

k

(3)

(4)

We present a few examples outside of the training set to
show what kind of frames is highlighted by the attention
network in the multi-AP model (Fig. 5). For each video,
the frames with the lowest attention weights (left) and the
ones with the highest attention weights (right) are presented.
Since there are many groups of attention weight, the ﬁrst
group is used for this presentation. We have not observed
any noticeable patterns in the inter-group differences of the
weights.

We use a fully connected layer with the softmax acti-
vation function as the attention network (Eq. 3). It takes

In analysis of these examples, we ﬁnd that there are some
patterns about the attention weights. Title screens and the

6

Figure 5: Visualization of the attention weights in the multiple attention pooling model. The frames in each row comes from
the same video. The left four in each row are the frames with the lowest weights, while the right four are the ones with the
highest weights.

frames that is very dark tend to have low attention weights.
Also, if the object of interest is too small or partly blocked,
the weight of that frame tend to be low.

Our work is different from a previous work on attention
pooling [5] in two ways. First, our model uses multiple
groups of attention on the same output sequence and multi-
ple classiﬁers for prediction. The multiple attention scheme
may contribute to generating more stable classiﬁcation re-
sult. Second, our model do an attention pooling over the
outputs of LSTM model which is a global representation
of the sequence while the previous work do a pooling over
local CNN outputs.

We also adopt a local attention pooling scheme that ap-
plies attention pooling over the input features (Eq. 5). The
representation of the LSTM model is also used in the MoE
model for prediction.

The LSTM model in the four models share the same pa-
rameter (#layers=2, #cells=1024). The MoE models have 8
mixtures in the two multi-AP models, while the one in the
local-AP model has 4 mixtures. The two multi-AP models
both have 8 groups of attention weights. And the dimen-
sion of the positional embedding in the positional multi-AP
model is 32.

GAP
Model
0.8131
vanilla LSTM
0.8133
local-AP LSTM
multi-AP LSTM
0.8157
positional multi-AP LSTM 0.8169

Table 7: Performance (GAP) of attention models.

z = (cid:80)Fv

i=1 aixi

p = M oE([yFv ; z])

(5)

6. Bagging and Boosting

We compare the two attention pooling scheme with the
baseline in Table 7. The ”local-AP LSTM” refers to the
scheme using attention pooling over the input feature. The
”multi-AP LSTM” refers to the multiple attention pooling
scheme. The ”positional multi-AP LSTM” model add an
embedding for every frame position to the attention network
on the basis of the multi-AP LSTM model.

The parameters for the models in Table 7 is as follows.

Bootstrap aggregating [2], also called Bagging, is an en-
semble method that creates many versions of a model and
combines their results together. To create one version of a
model, one applies sampling with replacement to get a sub-
set of the original data, in which some original examples
may not present or present more than once[16]. Training on
different subset sampled from the original data would re-
sults in different models. The bagging algorithm is known
for its ability to reduce the variance of a model. We apply

7

Bagging to some of our models, and ﬁnd that Bagging can
generally boost the GAP performance by 0.6% ∼ 1.2%.
The results are shown in Table 8.

weights to 5 to ensure that the algorithm would not place all
the weights on a few formerly misclassiﬁed examples, since
they may not be visually classiﬁable.

Original Bagging
Input Feature Model
0.8225
0.8106
Video-level
0.8216
0.8160
Frame Level
0.8258
Frame Level
0.8179
0.8244
Frame Level multi-AP LSTM 0.8157

Chaining
parallel LSTM
Chaining CNN

Model
Chaining (Video)
parallel LSTM
Chaining CNN
multi-AP LSTM

Original Boosting
0.8218
0.8106
0.8218
0.8160
0.8242
0.8179
0.8246
0.8157

Table 8: Performance (GAP) of bagging models.

Table 9: Performance (GAP) of boosting models.

Boosting [3] is another way to create different versions
of a model. Compared to Bagging which can be run in
parallel, Boosting is a sequential ensemble method. The
(k + 1)th classiﬁer is constructed considering the previous
k classiﬁers by re-sampling a distribution that highlights the
misclassiﬁed examples.

The re-sampling is often implemented by using weighted
examples. If there are N training examples and L labels,
the number of total classiﬁcation results is N × L. Most
works on Boosting in multi-label classiﬁcation use a weight
W N ×L over N samples and L labels, which is computa-
tionally intractable when the vocabulary of label is large, as
is the case in this competition.

We adopt a per-example weighting scheme that assign
a weight W N over the training samples. The weights are
updated with Eq. 6. In the equations, Wk,n is the weight
assigned to the nth example in the training of the kth clas-
siﬁer. Errk,n is the error rate of the nth example by eval-
uating the kth classiﬁer. Errk is the average error rate of
the kth classiﬁer. And Zk is a coefﬁcient that scales the av-
erage value of Wk,n to 1. α is a parameter controlling the
highlighting effect on the misclassiﬁed examples which we
constantly set to 1 in our system.

Wk,n exp(αrkErrk,n)

W0,n = 1.0
Wk+1,n = N
Zk
in which,
rk = log( 1.0−Errk
Errk = 1
N
Errk,n ∈ [0, 1]
Zk = (cid:80)N

Errk
(cid:80)N

)

n=1 Errk,n

n=1 Wk,n exp(αrkErrk,n)

In this algorithm, the weights of the misclassiﬁed ex-
amples are increased in the following classiﬁers. However,
in multi-label classiﬁcation, misclassiﬁcation is hard to de-
ﬁne. We choose the Precision Equal Recall Rate (PERR)
as the implementation of error rate, since it is both per-
example evaluated and in coordinate with the GAP score in
most models (from empirical observation). We also clip the

The comparison between the single models and their
Boosting counterparts are shown in Table 9. The Boosting
algorithm generates a performance boost similar as Bagging
does.

7. Cascade Classiﬁer Training

Adding models into the ensemble would usually make
the performance better. However, with the number of mod-
els in the ensemble model increasing, the gain from newly
added models tends to diminish, especially if one add mod-
els that are similar to the existing models. We address this
problem by using cascade training in which the predictions
of other models as a part of its input of the model during
training.

(6)

Figure 6: Cascade layer as a replacement for MoE in cas-
cade classiﬁer training.

The structure of cascade layer is shown in Fig. 6. The
predictions from the other models are averaged to generate
an averaged prediction, which is then projected to a feature
of low dimension. All the models that have MoE as their
last layer can be modiﬁed into a cascade model by having
their MoE layer replaced by the cascade layer.

8. Stacking Methods

Stacking is an ensemble method that uses a machine
learning model to combine the predictions of many mod-
els into one prediction. We use the ”train2” set (Section 1)

8

as the training set for stacking. The trivial case of stacking
is to simply average all the predictions. If there are M mod-
els and L labels in total, simple averaging the predictions of
all models can be written as Eq. 7.

pl =

1
M

M
(cid:88)

m=1

pm,l

(7)

Linear weighted averaging (Eq.

8) is also a simple
scheme. This method has M weights, one for each model
in the ensemble.

M
(cid:88)

m=1

M
(cid:88)

m=1

pl =

wmpm,l, s.t.

wm = 1, wm > 0

(8)

1
M

M
(cid:88)

m=1

For multi-label classiﬁcation, there are L predictions for
every example. Some models might perform better than the
others on a subset of the labels. Therefore, we could extend
linear weighted averaging to every class. We denote this
method class-wise weighted averaging (Eq. 9).

1
M

M
(cid:88)

m=1

pl =

wm,lpm,l, s.t.

wm,l = 1, wm,l > 0

(9)
While the ensemble methods often deal with the predic-
tions of individual models, the original input may also help
to decide which models to trust. The winning team of Net-
ﬂix Prize proposes the Feature-Weighted Linear Stacking
[13] to utilize the information in meta-features. Although
there are no useful meta-feature in the Youtube-8M dataset,
we consider the averaged frame input and the averaged pre-
dictions from individual models as useful indicators. We
propose the attention weighted stacking to utilized these in-
formation.

The attention weights α (Eq. 10) are generated with a
fully connected layer with softmax as the activation func-
tion and averaged input ¯x and prediction ¯p as the input of
the layer.

The predictions are then weighted averaged as shown in
Eq. 11. The weight is a mixture of K low-rank matrices
(rank D) in which the matrices are selected by the output
of the attention network (Eq. 10). In the attention weighted
stacking, V, A, B, a, b, c are trainable parameters.

We perform a comparison among the stacking methods
on an ensemble of 74 models (model #1 - #74 in Appendix
A). The number of attention weights is 16 and the rank
of the matrix components is 4 in the attention weighted
stacking method. The comparison shows that the attention
weighted stacking performs better than the other methods
(Table 10).

Stacking method
Simple Averaging
Linear Weighted Averaging
Class-wise Weighted Averaging
Attention Weighted Stacking

GAP
0.8436
0.8449
0.8453
0.8458

Table 10: Performance (GAP) of different stacking models
on the entire set of models (74 models). The GAP scores
in this table is evaluated with the blind test set (the private
leaderboard score in the competition).

9. Distilling Knowledge from Ensemble

Ensemble models might perform much better than even
the best single model. However, in real world systems, the
memory and computation resources are often too limited
for ensemble models to deploy. Distillation [6] is a training
scheme that aims to compress the knowledge in ensemble
model to a single model. Another motivation of using dis-
tillation is the observation that the labels are often wrong
or incomplete, especially when the vocabulary of label is
large.

We adopt a training scheme that uses the predictions of
ensemble ˆp as a soft target along with the real target l during
training. The new loss function is a weighted average of two
losses, one for each target (Eq. 12).

α = σ(V [¯x; ¯p])

¯x = 1
Fv

(cid:80)Fv

i=1 xi

¯p = 1
M

(cid:80)M

m=1 pm

pl

= 1
M

(cid:80)M

m=1 wm,lpm,l

wm,l =

(cid:80)M

e

= (cid:80)K

exp(em,l)
m=1 exp(em,l)
k=1 αk(AT

s.t.

Ak ∈ RD×M , Bk ∈ RD×L
ak ∈ RM ×1, bk ∈ RL×1
ck ∈ R

(10)

ˆL = (1 − λ)ce(p, l) + λce(p, ˆp)

(12)

We use the predictions of model #75 (Appendix A.9) as
the soft target in training and train several models with the
same parameters as the original models. The comparison is
listed in Table 11. The comparison shows that distillation
can greatly boost single model performance (often better
than Bagging).

In the previous sections, we review our system, present
our intuition, and show how we address the problems in

9

k Bk + akbT

k + ck)

(11)

10. Summary

Original Distillation
Model
0.8106
Chaining (Video)
0.8160
parallel LSTM
0.8179
Chaining CNN
0.8172
Chaining LSTM
Multi-scale CNN-LSTM 0.8204

0.8169
0.8237
0.8266
0.8291
0.8258

Table 11: Performance (GAP) of single models trained us-
ing distillation.

multi-label video classiﬁcation. We ﬁnd that attention pool-
ing and temporal multi-scale information is very important
for video sequence modeling. We also propose a network
structure for large-vocabulary multi-label classiﬁcation. We
review our work in ensemble methods such as Bagging,
Boosting, Cascade, Distillation and Stacking. We propose a
stacking network that uses attention to weight models. Our
system road-map is shown in Table 12.

Changes
27 single models
+ 11 bagging & boosting models
+ 8 distillation models
+ 11 cascade models
+ 17 more cascade models
Attention Weighted Stacking

GAP
0.8425
0.8435
0.8437
0.8451
0.8453
0.8458

Table 12: Performance (GAP) of ensemble models. The
stacking method used for the ﬁrst 5 results is class-wise
weighted model. The ensemble in each row include all the
changes in the rows above it. The results reported in this
table are evaluated on the blind test set (the private leader-
board score in the competition).

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m:
CoRR,
A large-scale video classiﬁcation benchmark.
abs/1609.08675, 2016.

[2] L. Breiman.

Bagging predictors. Machine Learning,

24(2):123–140, 1996.

[3] L. Breiman. Arcing classiﬁer (with discussion and a rejoin-
der by the author). Ann. Statist., 26(3):801–849, 06 1998.
[4] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 961–970, 2015.

[5] M. J. Er, Y. Zhang, N. Wang, and M. Pratama. Atten-
tion pooling-based convolutional neural network for sen-
tence modelling. Information Sciences, 373:388 – 403, 2016.
[6] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, pages
1725–1732, 2014.

[9] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014.

[11] J. Read and J. Hollm´en. A deep interpretation of classi-
ﬁer chains. In International Symposium on Intelligent Data
Analysis, pages 251–262. Springer, 2014.

[12] J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classiﬁer
chains for multi-label classiﬁcation. Machine Learning and
Knowledge Discovery in Databases, pages 254–269, 2009.

[13] J. Sill, G. Tak´acs, L. Mackey, and D. Lin. Feature-weighted
linear stacking. arXiv preprint arXiv:0911.0460, 2009.
[14] E. H. Spriggs, F. De La Torre, and M. Hebert. Temporal seg-
mentation and activity classiﬁcation from ﬁrst-person sens-
ing. In Computer Vision and Pattern Recognition Workshops,
2009. CVPR Workshops 2009. IEEE Computer Society Con-
ference On, pages 17–24. IEEE, 2009.

[15] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
towards good
L. Van Gool. Temporal segment networks:
practices for deep action recognition. In European Confer-
ence on Computer Vision, pages 20–36. Springer, 2016.

[16] Z.-H. Zhou. Ensemble methods:

foundations and algo-

rithms. CRC press, 2012.

10

Appendix A. Final Submission

In this section we list all the components included in our
ﬁnal submission. We apply a stacking method called the At-
tention Weighted Average (Section 8) to merge the result of
these models instead of carrying on careful model selection.
Therefore it might be possible that a subset of these models
could reach equal or better performance. The parameters
are for a quick grasp of the structure of the models. Re-
fer to our code 2 if you are interested in the implementation
details. The GAP scores reported in the appendix are eval-
uated with the ”validate2” set if not otherwise speciﬁed.

A.1. Video-level Models

1. Chaining model (Section 3). During training, if the
loss of the current batch is less than 10, use the most conﬁ-
dent tag predicted as a soft target for every example in the
batch. #mixture of MoE is 4. #stage of Chaining is 3. pro-
jection dimension in Chaining is 100. GAP = 0.8067.

2. Chaining model. #mixture of MoE is 2. #stage of
Chaining is 8. projection dimension in Chaining is 128.
GAP = 0.8106.

A.2. Baseline Models

If not otherwise speciﬁed, the memory cells of the ﬁnal
state are used for the input of the MoE model in the baseline
models.

3. LSTM model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8131.
4. LSTM model. #cell of LSTM is 2048. #layer of

LSTM is 2. #mixture of MoE is 4. GAP = 0.8152.

5. LSTM model of 2 layers. The ﬁrst layer is bi-
directional. The second layer is uni-directional. #cell of
LSTM is 1024,. #mixture of MoE is 4. GAP = 0.8105.

6. LSTM-S model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8123.
7. LSTM-A model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.
8. LSTM-A model of 2 layers. The ﬁrst layer is a for-
ward LSTM-A model. The second layer is a backward
LSTM-A model that takes the original input and the out-
put of the ﬁrst layer as input. #cell of LSTM is 1024. #layer
of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.

9. Parallel LSTM model (Section 2). #cell of LSTM is
1024. #layer of LSTM is 2. #mixture of MoE is 8. GAP =
0.8161.

10. Parallel LSTM model. The gated outputs of the ﬁ-
nal state are used for the input of the MoE model. #cell of
LSTM is 1024. #layer of LSTM is 2. #mixture of MoE is
8. GAP = 0.8160.

11. LSTM model with data augmentation by random
sampling 50% of the frames. #cell of LSTM is 1024. #layer

2https://github.com/wangheda/youtube-8m

of LSTM is 2. #mixture of MoE is 8. GAP = 0.8137.

12. CNN-LSTM model. The CNN model is described
in section 3. #layer of CNN is 1. The widths of ﬁlter are
1, 2 and 3. The corresponding #channels of ﬁlters are 1024,
1024, and 1024. #cell of LSTM is 1024. #layer of LSTM is
2. #mixture of MoE is 8. GAP = 0.8103.

A.3. Temporal Multi-Scale Models

13. Temporal-segment LSTM model (Section 4). The
duration of each clip is 10 frames. #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8122.
14. Temporal-pooling LSTM model (Section 4). The
model has 2-pooling between LSTM layers. #cell of LSTM
is 1024. #layer of LSTM is 4, #mixture of MoE is 8. GAP
= 0.8085.

15. Multi-resolution LSTM model (Section 4). The
model has parallel LSTM for sequence modeling. The
model has 2-pooling to get shorter representation of input.
#cell of LSTM is 512 for video and 64 for audio. #layer
of LSTM is 2. #stage of Chaining is 4. The dimension of
projection in Chaining is 256. #mixture of MoE is 4. GAP
= 0.8149.

16. Multi-scale CNN-LSTM model (Section 4). The
widths of ﬁlter are 1, 2 and 3. The corresponding #chan-
nels of ﬁlters are 256, 256, and 512. #layer in CNN is 4.
#layer of LSTM is 1. #cell of LSTM is 1024. #mixture of
MoE is 4. GAP = 0.8204.

17. Multi-scale CNN-LSTM model. All the parameters
are the same as model #16, except that the type of the LSTM
model is LSTM-S (as in model #6). GAP = 0.8147.

A.4. Chaining Models

18. Chaining CNN model (Section 3). The widths of ﬁl-
ter are 1, 2 and 3. The corresponding #channels of ﬁlters are
128, 128, and 256. The dimension of projection in Chaining
is 256. #mixture of MoE is 4. #stage of Chaining is 4. GAP
= 0.8179.

19. Chaining Deep CNN model. In this model a 3-layer
CNN model is used. The averaged input feature and the
3 max-pooled feature maps from the 3 layers of CNN are
combined using a 4-stage Chaining model. The widths of
ﬁlter are 1, 2 and 3. The corresponding #channels of ﬁl-
ters are 128, 128, and 256. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. GAP = 0.8155.

20. Chaining LSTM-CNN model. This model uses a
cascade of parallel LSTM and CNN model as the sub-model
in Chaining.#layer of LSTM is 1. #cell of LSTM is 1024 for
video and 128 for audio. The widths of ﬁlter are 1, 2 and
3. The corresponding #channels of ﬁlters are 128, 128, and
256. The dimension of projection in Chaining is 128. #stage
in Chaining is 3. #mixture of MoE is 4. GAP = 0.8122.

21. Chaining LSTM model (Section 3). #layer of LSTM
is 2. #cell of LSTM is 1024. The dimension of projection in

11

Chaining is 200. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8172.

22. Chaining LSTM model. The different stages in
Chaining model uses a shared input which is the cell mem-
ory of the ﬁnal state of an LSTM model. #layer of LSTM is
2. #cell of LSTM is 1024. The dimension of projection in
Chaining is 256. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8162.

A.5. Attention Pooling Models

23. Local attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. GAP =
0.8133.

24. Attention pooling LSTM model that has one LSTM
model to generate the attention weights for pooling over the
output of another LSTM model. #layer of LSTM is 1. #cell
of LSTM is 1024. #mixture of MoE is 8. GAP = 0.8088.

25. Multiple attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. #mixture of
MoE is 4. #group of attention weights is 8. GAP = 0.8157.
26. Multiple attention pooling LSTM model that has one
LSTM model to generate the attention weights for pooling
over the output of another LSTM model. #layer of LSTM
is 2. #cell of LSTM is 1024. #mixture of MoE is 4. #group
of attention weights is 8. GAP = 0.8081.

27. Multiple attention pooling LSTM model with posi-
tional embedding (Section 5). #layer of LSTM is 2. #cell
of LSTM is 1024. #mixture of MoE is 4. The dimension of
positional embedding is 32. #group of attention weights is
8. GAP = 0.8169.

A.6. Bagging and Boosting Models

GAP = 0.8218.

33. Bagging model of 8 versions of model #18, com-
bined with class-wise weighted averaging. GAP = 0.8258.
34. Boosting model of 8 versions of model #18, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8246.

35. Bagging model of 8 versions of model #25, com-
bined with class-wise weighted averaging. GAP = 0.8244.
36. Boosting model of 8 versions of model #25, com-
bined with attention weighted stacking, with weight clip-
ping. GAP = 0.8242.

37. Bagging model of 8 versions of model #10, com-

bined with simple averaging. GAP = 0.8216.

38. Boosting model of 8 versions of model #10, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8218.

A.7. Distillation Models

The models in this section is trained using distillation
(Section 9). During training, the predictions from model
#75 are used as soft targets.

39. Model #2 with distillation training. GAP = 0.8169.
40. Model #18 with distillation training. GAP = 0.8266.
41. Model #20 with distillation training. GAP = 0.8259.
42. Model #10 with distillation training. GAP = 0.8237.
43. Model #21 with distillation training. GAP = 0.8291.
44. Model #16 with distillation training. GAP = 0.8258.
45. Bagging model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8249.

46. Boosting model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8254.

28. Bagging model of 8 versions of model #2, combined

with simple averaging. GAP = 0.8225.

A.8. Cascade Models

29. Ensemble of 4 Chaining models. The ﬁrst is the
normal Chaining model with video-level input. The second
is a Chaining model with weighted cross entropy loss. The
third is a Chaining model that predict label and top-level
verticals at the same time. The fourth is a Chaining model
that predict the infrequent labels with softmax function. All
4 models share the same parameters. #mixture of MoE is
4. #stage in Chaining is 3. The dimension of projection in
Chaining is 100. GAP = 0.8216.

30. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging. Remove the ex-
amples with too high weights from training set, since it may
be hopeless to predict these examples correctly. GAP =
0.8213.

31. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging, without weight
clipping. GAP = 0.8198.

32. Boosting model of 8 versions of model #2, combined
with class-wise weighted averaging, with weight clipping.

The models in this section are models using the predic-

tion of other models as part of the input (Section 7).

47. Model #2 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8231.

48. Model #6 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8245.

49. Model #18 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8268.

50. Model #25 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

51. Chaining CNN model with cascade training using
the prediction of model #75 as part of the input. The widths
of ﬁlter are 1, 2 and 3. The corresponding #channels of
ﬁlters are 256, 256, and 512. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. #stage of Chaining
is 2. GAP = 0.8214.

52. Model #3 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

12

74. Model #2 with cascade training using the L1-
normalized prediction of model #76 as part of the input.
GAP = 0.8227.

A.9. Ensemble Models

In this section we lists the performance of some of the
ensemble models. Alongside the GAP score on ”validate2”
set, we also report the GAP score on the blind test set (Pri-
vate LeaderBoard in the competition).

75. Ensemble of 4 single models. Model #2, #10, #18,
and #25 with a class-wise weighted stacking gets a GAP of
0.8373. Private LB GAP = 0.83703.

76. Ensemble of 8 single models. Model #2, #10, #18,
#25, #47, #48, #49, and #50 with a class-wise weighted
stacking gets a GAP of 0.8397. Private LB GAP = 0.83941.
77. Ensemble of 27 single models. Model #1 - #27 with a
class-wise weighted stacking gets a GAP of 0.8427. Private
LB GAP = 0.84250.

78. Ensemble of 57 models. Model #1 - #57 with an
attention weighted stacking (#rank=3, #attention=16) gets a
GAP of 0.8459. Private LB GAP = 0.84561.

79. Ensemble of 74 models. Model #1 - #74 with an
attention weighted stacking (#rank=4, #attention=16) gets a
GAP of 0.8462. Private LB GAP = 0.84583.

80. Ensemble of model #78 and #79 by getting the most
conﬁdent 20 tags from each model and averaging the con-
ﬁdence scores. This is done directly on the test set without
training. Private LB GAP = 0.84590. This is our ﬁnal sub-
mission.

Appendix B. Source Code

The source code of our solution is made public on
GitHub. Visit https://github.com/wangheda/youtube-8m for
the implementation details of our models.

53. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 2.
GAP = 0.8214.

54. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 4.
GAP = 0.8266.

55. Model #20 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8265.

56. Model #6 with cascade training using the prediction
of model #75 as part of the input. The prediction of model
#6 for infrequent labels and the prediction of the cascade
model for frequent ones are joined together as the ﬁnal pre-
diction. GAP = 0.8228.

57. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8202.

58. Model #25 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8254.

59. Model #20 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8250.

60. Model #6 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8245.

61. Model #7 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8247.

62. Model #21 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8251.

63. Model #16 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8258.

64. Model #10 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8248.

65. Model #10 with cascade training using the prediction
of model #76 as part of the input. The prediction of model
#76 is also used to up-sample the misclassiﬁed samples as
in the Boosting model. GAP = 0.8218.

66. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8181.

67. Model #2 with cascade training using the predictions

of model #76 and #66 as part of the input. GAP = 0.7989.

68. Model #2 with cascade training using the predictions
of model #76, #66, and #67 as part of the input. GAP =
0.7849.

69. Model #2 with cascade training using the predictions
of model #76, #66, #67, and #68 as part of the input. GAP
= 0.7753.

70. Model #2 with cascade training using the predictions
of model #76, #66, #67, #68, and #69 as part of the input.
GAP = 0.7833.

71. Model #2 with cascade training using the predictions

of model #76 as part of the input. GAP = 0.8183.

72. Model #2 with cascade training using the predictions

of model #76 and #71 as part of the input. GAP = 0.8085.

73. Model #2 with cascade training using the predictions
of model #76, #71, and #72 as part of the input. GAP =
0.8111.

13

7
1
0
2
 
n
u
J
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
5
1
5
0
.
6
0
7
1
:
v
i
X
r
a

The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge

He-Da Wang
whd.thu@gmail.com

Teng Zhang
zhangteng1887@gmail.com

Ji Wu
wuji ee@mail.tsinghua.edu.cn
Multimedia Signal and Intelligent Information Processing Laboratory
Department of Electronic Engineering
Tsinghua University, Beijing, China

Abstract

This article describes the ﬁnal solution 1 of team mon-
keytyping, who ﬁnished in second place in the YouTube-8M
video understanding challenge. The dataset used in this
challenge is a large-scale benchmark for multi-label video
classiﬁcation. We extend the work in [1] and propose sev-
eral improvements for frame sequence modeling. We pro-
pose a network structure called Chaining that can better
capture the interactions between labels. Also, we report our
approaches in dealing with multi-scale information and at-
tention pooling. In addition, We ﬁnd that using the output of
model ensemble as a side target in training can boost single
model performance. We report our experiments in bagging,
boosting, cascade, and stacking, and propose a stacking al-
gorithm called attention weighted stacking. Our ﬁnal sub-
mission is an ensemble that consists of 74 sub models, all of
which are listed in the appendix.

1. Introduction

Videos have been a very important type of content on
Internet. Understanding video from its audio-visual con-
tent is key to various applications such as recommendation,
searching, and question answering. The research on video
analysis is also an important step for the computer to un-
derstand the real world. Datasets such as Sports-1M[8] and
ActivityNet[4] encourage the research on video classiﬁca-
tion of sports and human activities. YouTube-8M[1] is a
large-scale video dataset that consists of about 7.0 million
YouTube videos that was annotated with a vocabulary of
4716 tags from 24 diverse categories. The average number
of tags per video is 3.4.

The YouTube-8M video understanding challenge is an

1visit https://github.com/wangheda/youtube-8m for the source code.

arena for video classiﬁcation researchers and practitioners.
In the competition, the dataset is divided into three parts.
The training set contains 4.9 million samples. The validate
set contains 1.4 million samples. The test set contains 0.7
million samples. The ground truth labels annotated to the
samples in the training set and the validate set are available
to the participants. The test set is divided into two half,
the open test set and the blind one. In the progress of the
competition, participants can submit their predictions to the
test set. The scoring server on Kaggle would evaluate them
and return the scores on the open test set (the public leader-
board). After the competition, the winners are decided by
the scores of their submissions on the blind test set (the pri-
vate leaderboard).

In the competition, submissions are evaluated using
Global Average Precision (GAP) at 20. The metrics is cal-
culated as follows. For each video, the most conﬁdent 20
label predictions are selected along with the conﬁdence val-
ues. The tuples of the form {video, label, conf idence}
from all the videos are then put into a long list sorted by
conﬁdence values. This list of predictions are then evalu-
ated with the Average Precision (Eq. 1), in which p(i) is the
precision and r(i) is the recall given the ﬁrst i predictions.

AP =

p(i)∆r(i)

(1)

N
(cid:88)

i=1

We divide the dataset into ﬁve parts: train1, validate1,
train2, validate2, and test. The division is done base on ﬁle
name pattern (Table 1). The set train1 is used for single
model training, in which the set validate1 served as a hold-
out test set for early-stopping. Ensemble models are trained
on the set train2, which have no intersection with the train-
ing set of single models. The set validate2 is a hold-out
test set for early-stopping in the training of ensemble mod-
els. The inference procedure uses the test set to generate
submissions. The whole system structure is shown in Fig.

1

Figure 1: The structure of our system.

Part
train1
validate1
train2
validate2
test

File glob
train??.tfrecord
validate[a]?.tfrecord
validate[ˆa0-9]?.tfrecord
validate[0-9]?.tfrecord
test??.tfrecord

#samples
4,906,660
21,918
1,270,412
109,498
700,640

Table 1: Dataset division scheme in our system.

1. This data division strategy may not be optimal, since it
limit the amount of data used in single model training to the
4.9 millions training set examples, which may considerably
affect the performance of single models.

During the training of all our models, We did not use
any data augmentation techniques. Adam [10] optimization
algorithm is used throughout the training process. For mod-
els that use frame level feature rgb, audio, we use a learn-
ing rate of 0.001 and a batch size of 128. For models that
use video level feature mean rgb, mean audio, the default
learning rate and batch size is 0.01 and 1024. Note that
these hyper parameters may not be optimal since we did not
perform any parameter search. Every of our models can ﬁt
in the graphics memory of either an Nvidia GTX 1080 (8G)
or an GTX 1080Ti (11G).

In the rest of this report, we summarize our contribu-
tions in our solution. We ﬁrst introduce our improvements
over the baseline Long-Short Term Memory (LSTM) model
(Section 2). We then introduce a deep network structure
that we use in many models to capture label correlations
(Section 3). Then, we introduce our best performing sin-
gle model that utilize multiscale information via Convolu-
tional Neural Network (CNN) and LSTM (Section 4). Also,
we explore the use of attention pooling in video sequence
classiﬁcation (Section 5). Ensemble methods such as bag-
ging, boosting (Section 6), cascade (Section 7), and stack-
ing (Section 8) are also explored in this report. We also
found that using the predictions of ensemble models as a

soft target can greatly boost single model performance (Sec-
tion 9). In the end, we summarize our road-map and discuss
the contribution of our solution(Section 10).

2. Baseline models

In this section, we list the performances of some of the
baseline models in the technical report [1] and some alter-
native architectures evaluated in our system. The perfor-
mances reported here are different from the ones reported
in the technical report, since the dataset used in the report is
a little larger (8.3 million examples) than the one provided
in this competition. Also, we have limited the amount of
data used for single model training to 4.9 million examples.
The GAP scores we reported in this paper are evaluated with
the ”validate2” set if not otherwise speciﬁed.

We re-implemented four baseline approaches: Logistic
Regression (LR), Mixture-of-Expert (MoE), Deep Bag-of-
Frames (DBoF), and Long-Short Term Memory (LSTM).
The GAP scores are listed in Table 2. We choose a mixture
of 16 for the MoE model for the best performance. For the
LSTM model, we choose the number of layers, the number
of cells per layer, and the number of Moe mixtures to be 2,
1024 and 8 respectively.

Input Feature
Video Level, µ
Video Level, µ
Frame Level, {xv
Frame Level, {xv

Model
Logistic Regression
Mixture-of-Experts

} Deep Bag-of-Frames
} LSTM

1:Fv

1:Fv

GAP
0.7577
0.7957
0.7845
0.8131

Table 2: The performance of baseline approaches in our
system.

In the conventional LSTM [7], the memory cells that
share the same input gate and the same output gate form
a memory block. Putting more than one cells into a block
makes the training of LSTM more efﬁcient. We create an ar-

2

chitecture in which all the cells share a single input gate and
a single forget gate and call it ”LSTM-S”. In this architec-
ture, the cells use individual output gates, since we ﬁnd that
sharing the output gate can be harmful to the performance.
Another network structure we create by adding an input
accumulator to the architecture of LSTM model is what we
call the ”LSTM-A”. In this structure, we add a new set of
memory, input gate and forget gate to the cell to directly
”remember” the input. Eq. 2 gives the architecture of this
model. In the equation, xt, ct, ht are the input, the memory,
and the hidden state, ot, it, ft are the output gate, the input
gate, and the forget gate, c(cid:48)
t, dt are the added memory and
the hidden state for input accumulation, i(cid:48)
t, f (cid:48)
t are the added
input gate and forget gate, σ is the sigmoid function, g is
the activation function which we choose to be tanh, and n
is the L2-normalization function.

content or audio content. Therefore, the visual or audio ac-
tivities do not have to happen spontaneously to be meaning-
ful. We adopt a parallel way that model visual and audio
content with separate LSTM models. The ﬁnal states of the
two LSTM models are then concatenated and go through an
MoE unit. The performance of the parallel model is shown
in Table 4. The number of layers is 2. The number of cells
c and the number of mixtures m in MoE model are shown
in the table.

Model
vanilla LSTM, c = 1024, m = 8
parallel LSTM, cv = 1024, ca = 128, m = 8

GAP
0.8131
0.8161

Table 4: Parallel modeling of visual and audio content is
better than simple feature concatenation.





















ot
mt
it
ft
i(cid:48)
t
f (cid:48)
t
ct
ht
c(cid:48)
t
dt

= TN +2M,6N





ht−1
xt
dt−1





= σ(ft) · ct−1 + σ(it) · g(mt)
= σ(ot) · g(ct)
t) · c(cid:48)
= σ(f (cid:48)
= n(c(cid:48)
t)

t−1 + σ(i(cid:48)

t) · xt

The performance of the two modiﬁed LSTM models are
listed in Table 3. The number of cells per layer are 1024.
The number of mixtures in MoE is set to 8. The number of
layers l is shown in the table. The two models performs bet-
ter than the original model and contribute to the ensemble
because their varieties in model structure.

Model
vanilla LSTM, l = 1
vanilla LSTM, l = 2
LSTM-S, l = 1
LSTM-A, l = 1

GAP
0.8091
0.8131
0.8123
0.8131

Table 3: The performances of the single-block LSTM
(LSTM-S) and the input-accumulator LSTM (LSTM-A),
compared to the original LSTM model.

In the dataset, an example has both visual feature and
audio feature. The naive way of modeling two different
features is to concatenate them into a single feature. We
ﬁnd this to be questionable for two reasons. First, in many
videos such as music videos and family albums, the audio
and the visual content are independent of each other. Also,
people often can make sense of a video only by its visual

3

There is a potential weakness in modeling visual and au-
dio features independently. It might be preferable to allow
the visual network and the audio network to interact at cer-
tain points, since a visual / audio event may be related to a
former audio / visual event in the same video. We leave this
issue to future works.

(2)

3. Label Correlation

In multi-label classiﬁcation settings, an example may be
annotated many labels. Some labels tend to appear in the
same example at the same time, some tend not to. Such in-
formation can be used to improve the performance of multi-
label classiﬁcation models.

Classiﬁer chain [12] create a chain of classiﬁers. One
classiﬁer on the chain predict one label by using not only the
input feature but also the predictions of other labels from the
previous models on the chain. The training of one chain of
classiﬁer involves training L models in which L is the size
of the vocabulary of all labels. However, in real world prob-
lems where the direction of dependency is unknown, an en-
semble of classiﬁer chains is usually used, which makes the
computation complexity even higher and intractable if the
vocabulary of labels is large. A neural network structure
that mimic the classiﬁer chain [11] is proposed to address
the multi-label problem. However, such network structure
has a depth of L that makes it hard to optimize if the vocab-
ulary of labels is large.

We propose a novel end-to-end deep learning structure
that we called ”Chaining” to better utilize the correlation be-
tween labels. In a Chaining model (Figure 2), several repre-
sentations are joined by a chain of MoE model. The predic-
tions are projected to features of lower dimension and used
in the following stages. The representations can be gener-
ated by homogeneous models or heterogeneous ones. We
constantly apply auxiliary cross-entropy loss on the inter-

(a) A unit in Chaining accept one fea-
ture and several model predictions as the
input. The predictions are projected to
lower dimension for efﬁciency.

(b) The whole architecture of Chaining consists of several stages. A Chaining unit
in one stage accepts a feature vector, either from input directly or from a represen-
tation of LSTM or CNN, and all the predictions from earlier stages.

Figure 2: Chaining: a deep learning architecture for multi-label classiﬁcation.

mediate predictions to accelerate the training progress. The
ﬁnal loss function is a weighted average over the loss on
the ﬁnal prediction and the auxiliary losses. We typically
allocate only 10% ∼ 20% of the weights to the auxiliary
losses, since the loss function at the ﬁnal stage is the most
important.

We performed experiments using three basic models,
Mixture-of-Expert, LSTM and CNN. The CNN model we
use in our system is the same as the benchmark method of
sentence classiﬁcation [9], where the length of the ﬁlter is
ﬁxed to the size of feature vector per frame and the resulting
feature map goes through a max-over-time pooling. The ﬁ-
nal state of LSTM and the max-pooled feature map are used
as feature representation in Chaining models and go through
an MoE model for label prediction in the original models.
The performance with and without using Chaining is shown
in Table 5. For the original MoE model, the number of mix-
tures is 16. For the Chaining MoE model, the number of
mixture is 2, the number of stages is 8, and the predictions
are projected into a 128 dimensions vector. For the original
LSTM model, the number of mixtures is 8. For the Chain-
ing LSTM model, the number of mixtures is 4, the number
of stages is 2, and the dimension of projection is 200. For
the original CNN model, the width of ﬁlter and the corre-
sponding numbers of channel are 1×512, 2×512, 3×1024.
For the Chaining CNN model, the corresponding parame-
ters are 1 × 128, 2 × 128, 3 × 256, and the number of stages
is 4. The parameters are chosen to make the original models
have almost equal number of parameters with their Chain-
ing counterparts.

4. Temporal Multi-Scale Information

Input Feature
Video-level, µ
Frame Level, {xv
Frame Level, {xv

1:Fv

1:Fv

Model Original Chaining
0.8106
MoE
0.8172
0.8179

0.7965
} LSTM 0.8131
} CNN
0.7904

Table 5: The performance (GAP) of Mixture-of-Experts,
LSTM and CNN models with and without using Chaining
structure.

age would change with its distance to the observer. How-
ever, temporal information does not have the same rescaling
effect. Therefore it may seems unnatural to model videos
from different temporal scales.

However, we argue that temporal scales matters in video
analysis. A task can be divided into several actions, and
each action involves the interaction between certain objects.
The label annotated to a video can be related to a concept at
different temporal scale: an object, an action, or a task.

One important observation is that video can be seg-
mented into several clips. Each of these clips may contain
content information of a certain aspect. We can make pre-
dictions based on each of these clips and then join them
together to make more accurate predictions. The segmenta-
tion of video is done by either clustering the adjacent frames
[14] or splitting the video into clips of equal durations [15].
We propose a temporal-segment LSTM model (Fig. 3a),
in which the video is split into equal-sized clips, each of
which is modeled by an LSTM model. The models for dif-
ferent clips share the same parameters. The ﬁnal state of
each sequence are treated as another high-level sequence
and modeled by another LSTM model.

Using information from different spatial scales have
been discussed in many image analysis literatures. The
main reason to do so is that the size of an object in an im-

Temporal-pooling LSTM model (Fig. 3a) is a multi-
layer LSTM model in which we inserted temporal k-pooling
layer between LSTM layers. It is similar to the temporal-

4

ition that the representation of low-level sequences contain
more reﬁned features, while the high-level representation is
close to an averaged view of the whole video.

We propose a novel temporal CNN-LSTM model (Fig.
3c) that utilize multi-scale information. This model shares
the previous intuition that adjacent frames can be aggre-
gated to generate features at different temporal scale. In-
stead of direct operating on the original feature, we use
convolution layers to detect patterns in the low-level fea-
tures and combine adjacent ﬁlter outputs by max-pooling
along the time dimension. The ﬁlters used in a convolution
layer are of the same length with the dimension of the fea-
ture, and their widths and channels can be varied. We use
different LSTM models and Moe classiﬁers for the repre-
sentation of the feature maps of different temporal scales.
The predictions generated from features of different scales
are combine using a consensus function. We ﬁnd that av-
eraging is a good consensus function, and maximum would
lead to difﬁculties in convergence.

GAP
Model
0.8131
vanilla LSTM
temporal-pooling LSTM 0.8085
temporal-segment LSTM 0.8122
multi-resolution LSTM
0.8148
multi-scale CNN-LSTM 0.8204

Table 6: Performance of multi-scale models.

Table 6 lists the performances of the models discussed
in this section. The temporal-pooling LSTM model uses 2-
pooling and has 4 LSTM layers (#cells=1024) and 4 MoE
mixtures. The temporal-segment LSTM model uses a dura-
tion of 10 frames for the clips and has 8 MoE mixtures and
2 LSTM layers, each of the layers has 1024 memory unit.
In the multi-resolution LSTM model, we use 2-pooling to
get shorter input sequences; the number of stages in Chain-
ing is 4; the number of mixtures in MoE is 4; the projection
dimension is 256; the LSTM models in it are 2-layer par-
allel LSTM (#cells is 512 for video and 64 for audio). In
the multi-scale CNN-LSTM model, the number of layers in
CNN model is 4; the number of mixtures in MoE is 4; the
LSTM models in it are 1-layer LSTM (#cells=1024); in ev-
ery layer of the CNN model, the width of ﬁlters and the cor-
responding number of channels are 1×256, 2×256, 3×512;
the pooling layers in it are 2-pooling.

5. Identifying Salient Frames with Attention

Not all the frames in a video are equally informative.
There are many reasons that a frame might contribute little
in video classiﬁcation. Images that are too dark, too bright,
or too blurry are hard to analysis for the image classiﬁcation

(a) Temporal-pooling (with the dashes) and temporal-segment
(without the dashes) LSTM model.

(b) Multi-resolution LSTM model.

(c) Multi-scale CNN-LSTM model.

Figure 3: Models that utilize temporal multi-scale informa-
tion.

segment LSTM model. The difference is whether to use the
ﬁnal state of the one clip as the initial state of the next clip
in the ﬁrst layer of the LSTM model.

Instead of directly segmenting videos into clips, we
can aggregate the adjacent frames and gradually construct
features containing long-range information.
In multi-
resolution LSTM model (Fig. 3b), the original features
are average pooled along the time dimension to get shorter
sequences in which each frame covers longer time range.
For each sequence generated in this way, a separate LSTM
model is used to generate a sequence representation. The
representations are then joined with a Chaining model.
The representation from the highest-level goes into the ﬁrst
stage, while the one from the original sequence goes into the
ﬁnal stage of the Chaining model. This is due to the intu-

5

(b) An attention network generates multiple attention
weights for a frame based on the input feature and the
LSTM output at that frame.

(a) The multiple attention pooling scheme. The outputs
of LSTM are pooled with multiple attention weights.

Figure 4: Multiple attention pooling model.

network. Therefore, such frames may not be able to provide
useful semantic information. Also, sometimes a video con-
tains title screens, credits or text. They might be useful if
the frames are processed using optical character recognition
(OCR). However, in this competition, the frames are pre-
processed using an image classiﬁcation network pre-trained
with ImageNet dataset, which means the frame-level fea-
tures may not contain the semantic information in these text.
In addition, there are always frames irrelevant to the theme
of the video. In talk show videos the content of frames will
always be people talking while the topic of the talking is
what really important. In documentaries there are often a
lot of driving scenes which do not reﬂect the theme of the
video.

We propose a multiple attention pooling scheme (multi-
AP) for selecting salient frames in the video. As Fig. 4
shows, an LSTM model is used to deal with the frame-level
features. The outputs of the LSTM model are then aggre-
gated by pooling over the time dimension. The aggregation
is achieved by taking a weighted average over the outputs
of the LSTM model, in which the weights are generated
using an attention network, for which we use a fully con-
nected layer. The aggregated feature is fed into an MoE
model for label predictions. The attention pooling is re-
peated for multiple times and the predictions are combined
using a classiﬁer consensus function. For the choice of com-
bining method, we ﬁnd that maximum performs better than
averaging as a consensus function.

the input and output of the LSTM at a certain frame as the
input. The attention network output K groups of different
attention weights.

Each group of the weights are used to generate a group of
aggregated features and prediction results. The results from
all the groups are then combined by choosing the highest
conﬁdence predicted from all the models for each label l.
The MoE models in Fig. 4a share parameters with each
other.

eik = Wk[xi; yi]
aik = exp(eik)
(cid:80)Fv

i=1 exp(eik)

zk = (cid:80)Fv
i=1 aikyi
pk = M oE(zk)
(pk,l)
pl = max

k

(3)

(4)

We present a few examples outside of the training set to
show what kind of frames is highlighted by the attention
network in the multi-AP model (Fig. 5). For each video,
the frames with the lowest attention weights (left) and the
ones with the highest attention weights (right) are presented.
Since there are many groups of attention weight, the ﬁrst
group is used for this presentation. We have not observed
any noticeable patterns in the inter-group differences of the
weights.

We use a fully connected layer with the softmax acti-
vation function as the attention network (Eq. 3). It takes

In analysis of these examples, we ﬁnd that there are some
patterns about the attention weights. Title screens and the

6

Figure 5: Visualization of the attention weights in the multiple attention pooling model. The frames in each row comes from
the same video. The left four in each row are the frames with the lowest weights, while the right four are the ones with the
highest weights.

frames that is very dark tend to have low attention weights.
Also, if the object of interest is too small or partly blocked,
the weight of that frame tend to be low.

Our work is different from a previous work on attention
pooling [5] in two ways. First, our model uses multiple
groups of attention on the same output sequence and multi-
ple classiﬁers for prediction. The multiple attention scheme
may contribute to generating more stable classiﬁcation re-
sult. Second, our model do an attention pooling over the
outputs of LSTM model which is a global representation
of the sequence while the previous work do a pooling over
local CNN outputs.

We also adopt a local attention pooling scheme that ap-
plies attention pooling over the input features (Eq. 5). The
representation of the LSTM model is also used in the MoE
model for prediction.

The LSTM model in the four models share the same pa-
rameter (#layers=2, #cells=1024). The MoE models have 8
mixtures in the two multi-AP models, while the one in the
local-AP model has 4 mixtures. The two multi-AP models
both have 8 groups of attention weights. And the dimen-
sion of the positional embedding in the positional multi-AP
model is 32.

GAP
Model
0.8131
vanilla LSTM
0.8133
local-AP LSTM
multi-AP LSTM
0.8157
positional multi-AP LSTM 0.8169

Table 7: Performance (GAP) of attention models.

z = (cid:80)Fv

i=1 aixi

p = M oE([yFv ; z])

(5)

6. Bagging and Boosting

We compare the two attention pooling scheme with the
baseline in Table 7. The ”local-AP LSTM” refers to the
scheme using attention pooling over the input feature. The
”multi-AP LSTM” refers to the multiple attention pooling
scheme. The ”positional multi-AP LSTM” model add an
embedding for every frame position to the attention network
on the basis of the multi-AP LSTM model.

The parameters for the models in Table 7 is as follows.

Bootstrap aggregating [2], also called Bagging, is an en-
semble method that creates many versions of a model and
combines their results together. To create one version of a
model, one applies sampling with replacement to get a sub-
set of the original data, in which some original examples
may not present or present more than once[16]. Training on
different subset sampled from the original data would re-
sults in different models. The bagging algorithm is known
for its ability to reduce the variance of a model. We apply

7

Bagging to some of our models, and ﬁnd that Bagging can
generally boost the GAP performance by 0.6% ∼ 1.2%.
The results are shown in Table 8.

weights to 5 to ensure that the algorithm would not place all
the weights on a few formerly misclassiﬁed examples, since
they may not be visually classiﬁable.

Original Bagging
Input Feature Model
0.8225
0.8106
Video-level
0.8216
0.8160
Frame Level
0.8258
Frame Level
0.8179
0.8244
Frame Level multi-AP LSTM 0.8157

Chaining
parallel LSTM
Chaining CNN

Model
Chaining (Video)
parallel LSTM
Chaining CNN
multi-AP LSTM

Original Boosting
0.8218
0.8106
0.8218
0.8160
0.8242
0.8179
0.8246
0.8157

Table 8: Performance (GAP) of bagging models.

Table 9: Performance (GAP) of boosting models.

Boosting [3] is another way to create different versions
of a model. Compared to Bagging which can be run in
parallel, Boosting is a sequential ensemble method. The
(k + 1)th classiﬁer is constructed considering the previous
k classiﬁers by re-sampling a distribution that highlights the
misclassiﬁed examples.

The re-sampling is often implemented by using weighted
examples. If there are N training examples and L labels,
the number of total classiﬁcation results is N × L. Most
works on Boosting in multi-label classiﬁcation use a weight
W N ×L over N samples and L labels, which is computa-
tionally intractable when the vocabulary of label is large, as
is the case in this competition.

We adopt a per-example weighting scheme that assign
a weight W N over the training samples. The weights are
updated with Eq. 6. In the equations, Wk,n is the weight
assigned to the nth example in the training of the kth clas-
siﬁer. Errk,n is the error rate of the nth example by eval-
uating the kth classiﬁer. Errk is the average error rate of
the kth classiﬁer. And Zk is a coefﬁcient that scales the av-
erage value of Wk,n to 1. α is a parameter controlling the
highlighting effect on the misclassiﬁed examples which we
constantly set to 1 in our system.

Wk,n exp(αrkErrk,n)

W0,n = 1.0
Wk+1,n = N
Zk
in which,
rk = log( 1.0−Errk
Errk = 1
N
Errk,n ∈ [0, 1]
Zk = (cid:80)N

Errk
(cid:80)N

)

n=1 Errk,n

n=1 Wk,n exp(αrkErrk,n)

In this algorithm, the weights of the misclassiﬁed ex-
amples are increased in the following classiﬁers. However,
in multi-label classiﬁcation, misclassiﬁcation is hard to de-
ﬁne. We choose the Precision Equal Recall Rate (PERR)
as the implementation of error rate, since it is both per-
example evaluated and in coordinate with the GAP score in
most models (from empirical observation). We also clip the

The comparison between the single models and their
Boosting counterparts are shown in Table 9. The Boosting
algorithm generates a performance boost similar as Bagging
does.

7. Cascade Classiﬁer Training

Adding models into the ensemble would usually make
the performance better. However, with the number of mod-
els in the ensemble model increasing, the gain from newly
added models tends to diminish, especially if one add mod-
els that are similar to the existing models. We address this
problem by using cascade training in which the predictions
of other models as a part of its input of the model during
training.

(6)

Figure 6: Cascade layer as a replacement for MoE in cas-
cade classiﬁer training.

The structure of cascade layer is shown in Fig. 6. The
predictions from the other models are averaged to generate
an averaged prediction, which is then projected to a feature
of low dimension. All the models that have MoE as their
last layer can be modiﬁed into a cascade model by having
their MoE layer replaced by the cascade layer.

8. Stacking Methods

Stacking is an ensemble method that uses a machine
learning model to combine the predictions of many mod-
els into one prediction. We use the ”train2” set (Section 1)

8

as the training set for stacking. The trivial case of stacking
is to simply average all the predictions. If there are M mod-
els and L labels in total, simple averaging the predictions of
all models can be written as Eq. 7.

pl =

1
M

M
(cid:88)

m=1

pm,l

(7)

Linear weighted averaging (Eq.

8) is also a simple
scheme. This method has M weights, one for each model
in the ensemble.

M
(cid:88)

m=1

M
(cid:88)

m=1

pl =

wmpm,l, s.t.

wm = 1, wm > 0

(8)

1
M

M
(cid:88)

m=1

For multi-label classiﬁcation, there are L predictions for
every example. Some models might perform better than the
others on a subset of the labels. Therefore, we could extend
linear weighted averaging to every class. We denote this
method class-wise weighted averaging (Eq. 9).

1
M

M
(cid:88)

m=1

pl =

wm,lpm,l, s.t.

wm,l = 1, wm,l > 0

(9)
While the ensemble methods often deal with the predic-
tions of individual models, the original input may also help
to decide which models to trust. The winning team of Net-
ﬂix Prize proposes the Feature-Weighted Linear Stacking
[13] to utilize the information in meta-features. Although
there are no useful meta-feature in the Youtube-8M dataset,
we consider the averaged frame input and the averaged pre-
dictions from individual models as useful indicators. We
propose the attention weighted stacking to utilized these in-
formation.

The attention weights α (Eq. 10) are generated with a
fully connected layer with softmax as the activation func-
tion and averaged input ¯x and prediction ¯p as the input of
the layer.

The predictions are then weighted averaged as shown in
Eq. 11. The weight is a mixture of K low-rank matrices
(rank D) in which the matrices are selected by the output
of the attention network (Eq. 10). In the attention weighted
stacking, V, A, B, a, b, c are trainable parameters.

We perform a comparison among the stacking methods
on an ensemble of 74 models (model #1 - #74 in Appendix
A). The number of attention weights is 16 and the rank
of the matrix components is 4 in the attention weighted
stacking method. The comparison shows that the attention
weighted stacking performs better than the other methods
(Table 10).

Stacking method
Simple Averaging
Linear Weighted Averaging
Class-wise Weighted Averaging
Attention Weighted Stacking

GAP
0.8436
0.8449
0.8453
0.8458

Table 10: Performance (GAP) of different stacking models
on the entire set of models (74 models). The GAP scores
in this table is evaluated with the blind test set (the private
leaderboard score in the competition).

9. Distilling Knowledge from Ensemble

Ensemble models might perform much better than even
the best single model. However, in real world systems, the
memory and computation resources are often too limited
for ensemble models to deploy. Distillation [6] is a training
scheme that aims to compress the knowledge in ensemble
model to a single model. Another motivation of using dis-
tillation is the observation that the labels are often wrong
or incomplete, especially when the vocabulary of label is
large.

We adopt a training scheme that uses the predictions of
ensemble ˆp as a soft target along with the real target l during
training. The new loss function is a weighted average of two
losses, one for each target (Eq. 12).

α = σ(V [¯x; ¯p])

¯x = 1
Fv

(cid:80)Fv

i=1 xi

¯p = 1
M

(cid:80)M

m=1 pm

pl

= 1
M

(cid:80)M

m=1 wm,lpm,l

wm,l =

(cid:80)M

e

= (cid:80)K

exp(em,l)
m=1 exp(em,l)
k=1 αk(AT

s.t.

Ak ∈ RD×M , Bk ∈ RD×L
ak ∈ RM ×1, bk ∈ RL×1
ck ∈ R

(10)

ˆL = (1 − λ)ce(p, l) + λce(p, ˆp)

(12)

We use the predictions of model #75 (Appendix A.9) as
the soft target in training and train several models with the
same parameters as the original models. The comparison is
listed in Table 11. The comparison shows that distillation
can greatly boost single model performance (often better
than Bagging).

In the previous sections, we review our system, present
our intuition, and show how we address the problems in

9

k Bk + akbT

k + ck)

(11)

10. Summary

Original Distillation
Model
0.8106
Chaining (Video)
0.8160
parallel LSTM
0.8179
Chaining CNN
0.8172
Chaining LSTM
Multi-scale CNN-LSTM 0.8204

0.8169
0.8237
0.8266
0.8291
0.8258

Table 11: Performance (GAP) of single models trained us-
ing distillation.

multi-label video classiﬁcation. We ﬁnd that attention pool-
ing and temporal multi-scale information is very important
for video sequence modeling. We also propose a network
structure for large-vocabulary multi-label classiﬁcation. We
review our work in ensemble methods such as Bagging,
Boosting, Cascade, Distillation and Stacking. We propose a
stacking network that uses attention to weight models. Our
system road-map is shown in Table 12.

Changes
27 single models
+ 11 bagging & boosting models
+ 8 distillation models
+ 11 cascade models
+ 17 more cascade models
Attention Weighted Stacking

GAP
0.8425
0.8435
0.8437
0.8451
0.8453
0.8458

Table 12: Performance (GAP) of ensemble models. The
stacking method used for the ﬁrst 5 results is class-wise
weighted model. The ensemble in each row include all the
changes in the rows above it. The results reported in this
table are evaluated on the blind test set (the private leader-
board score in the competition).

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m:
CoRR,
A large-scale video classiﬁcation benchmark.
abs/1609.08675, 2016.

[2] L. Breiman.

Bagging predictors. Machine Learning,

24(2):123–140, 1996.

[3] L. Breiman. Arcing classiﬁer (with discussion and a rejoin-
der by the author). Ann. Statist., 26(3):801–849, 06 1998.
[4] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 961–970, 2015.

[5] M. J. Er, Y. Zhang, N. Wang, and M. Pratama. Atten-
tion pooling-based convolutional neural network for sen-
tence modelling. Information Sciences, 373:388 – 403, 2016.
[6] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, pages
1725–1732, 2014.

[9] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014.

[11] J. Read and J. Hollm´en. A deep interpretation of classi-
ﬁer chains. In International Symposium on Intelligent Data
Analysis, pages 251–262. Springer, 2014.

[12] J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classiﬁer
chains for multi-label classiﬁcation. Machine Learning and
Knowledge Discovery in Databases, pages 254–269, 2009.

[13] J. Sill, G. Tak´acs, L. Mackey, and D. Lin. Feature-weighted
linear stacking. arXiv preprint arXiv:0911.0460, 2009.
[14] E. H. Spriggs, F. De La Torre, and M. Hebert. Temporal seg-
mentation and activity classiﬁcation from ﬁrst-person sens-
ing. In Computer Vision and Pattern Recognition Workshops,
2009. CVPR Workshops 2009. IEEE Computer Society Con-
ference On, pages 17–24. IEEE, 2009.

[15] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
towards good
L. Van Gool. Temporal segment networks:
practices for deep action recognition. In European Confer-
ence on Computer Vision, pages 20–36. Springer, 2016.

[16] Z.-H. Zhou. Ensemble methods:

foundations and algo-

rithms. CRC press, 2012.

10

Appendix A. Final Submission

In this section we list all the components included in our
ﬁnal submission. We apply a stacking method called the At-
tention Weighted Average (Section 8) to merge the result of
these models instead of carrying on careful model selection.
Therefore it might be possible that a subset of these models
could reach equal or better performance. The parameters
are for a quick grasp of the structure of the models. Re-
fer to our code 2 if you are interested in the implementation
details. The GAP scores reported in the appendix are eval-
uated with the ”validate2” set if not otherwise speciﬁed.

A.1. Video-level Models

1. Chaining model (Section 3). During training, if the
loss of the current batch is less than 10, use the most conﬁ-
dent tag predicted as a soft target for every example in the
batch. #mixture of MoE is 4. #stage of Chaining is 3. pro-
jection dimension in Chaining is 100. GAP = 0.8067.

2. Chaining model. #mixture of MoE is 2. #stage of
Chaining is 8. projection dimension in Chaining is 128.
GAP = 0.8106.

A.2. Baseline Models

If not otherwise speciﬁed, the memory cells of the ﬁnal
state are used for the input of the MoE model in the baseline
models.

3. LSTM model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8131.
4. LSTM model. #cell of LSTM is 2048. #layer of

LSTM is 2. #mixture of MoE is 4. GAP = 0.8152.

5. LSTM model of 2 layers. The ﬁrst layer is bi-
directional. The second layer is uni-directional. #cell of
LSTM is 1024,. #mixture of MoE is 4. GAP = 0.8105.

6. LSTM-S model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8123.
7. LSTM-A model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.
8. LSTM-A model of 2 layers. The ﬁrst layer is a for-
ward LSTM-A model. The second layer is a backward
LSTM-A model that takes the original input and the out-
put of the ﬁrst layer as input. #cell of LSTM is 1024. #layer
of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.

9. Parallel LSTM model (Section 2). #cell of LSTM is
1024. #layer of LSTM is 2. #mixture of MoE is 8. GAP =
0.8161.

10. Parallel LSTM model. The gated outputs of the ﬁ-
nal state are used for the input of the MoE model. #cell of
LSTM is 1024. #layer of LSTM is 2. #mixture of MoE is
8. GAP = 0.8160.

11. LSTM model with data augmentation by random
sampling 50% of the frames. #cell of LSTM is 1024. #layer

2https://github.com/wangheda/youtube-8m

of LSTM is 2. #mixture of MoE is 8. GAP = 0.8137.

12. CNN-LSTM model. The CNN model is described
in section 3. #layer of CNN is 1. The widths of ﬁlter are
1, 2 and 3. The corresponding #channels of ﬁlters are 1024,
1024, and 1024. #cell of LSTM is 1024. #layer of LSTM is
2. #mixture of MoE is 8. GAP = 0.8103.

A.3. Temporal Multi-Scale Models

13. Temporal-segment LSTM model (Section 4). The
duration of each clip is 10 frames. #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8122.
14. Temporal-pooling LSTM model (Section 4). The
model has 2-pooling between LSTM layers. #cell of LSTM
is 1024. #layer of LSTM is 4, #mixture of MoE is 8. GAP
= 0.8085.

15. Multi-resolution LSTM model (Section 4). The
model has parallel LSTM for sequence modeling. The
model has 2-pooling to get shorter representation of input.
#cell of LSTM is 512 for video and 64 for audio. #layer
of LSTM is 2. #stage of Chaining is 4. The dimension of
projection in Chaining is 256. #mixture of MoE is 4. GAP
= 0.8149.

16. Multi-scale CNN-LSTM model (Section 4). The
widths of ﬁlter are 1, 2 and 3. The corresponding #chan-
nels of ﬁlters are 256, 256, and 512. #layer in CNN is 4.
#layer of LSTM is 1. #cell of LSTM is 1024. #mixture of
MoE is 4. GAP = 0.8204.

17. Multi-scale CNN-LSTM model. All the parameters
are the same as model #16, except that the type of the LSTM
model is LSTM-S (as in model #6). GAP = 0.8147.

A.4. Chaining Models

18. Chaining CNN model (Section 3). The widths of ﬁl-
ter are 1, 2 and 3. The corresponding #channels of ﬁlters are
128, 128, and 256. The dimension of projection in Chaining
is 256. #mixture of MoE is 4. #stage of Chaining is 4. GAP
= 0.8179.

19. Chaining Deep CNN model. In this model a 3-layer
CNN model is used. The averaged input feature and the
3 max-pooled feature maps from the 3 layers of CNN are
combined using a 4-stage Chaining model. The widths of
ﬁlter are 1, 2 and 3. The corresponding #channels of ﬁl-
ters are 128, 128, and 256. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. GAP = 0.8155.

20. Chaining LSTM-CNN model. This model uses a
cascade of parallel LSTM and CNN model as the sub-model
in Chaining.#layer of LSTM is 1. #cell of LSTM is 1024 for
video and 128 for audio. The widths of ﬁlter are 1, 2 and
3. The corresponding #channels of ﬁlters are 128, 128, and
256. The dimension of projection in Chaining is 128. #stage
in Chaining is 3. #mixture of MoE is 4. GAP = 0.8122.

21. Chaining LSTM model (Section 3). #layer of LSTM
is 2. #cell of LSTM is 1024. The dimension of projection in

11

Chaining is 200. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8172.

22. Chaining LSTM model. The different stages in
Chaining model uses a shared input which is the cell mem-
ory of the ﬁnal state of an LSTM model. #layer of LSTM is
2. #cell of LSTM is 1024. The dimension of projection in
Chaining is 256. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8162.

A.5. Attention Pooling Models

23. Local attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. GAP =
0.8133.

24. Attention pooling LSTM model that has one LSTM
model to generate the attention weights for pooling over the
output of another LSTM model. #layer of LSTM is 1. #cell
of LSTM is 1024. #mixture of MoE is 8. GAP = 0.8088.

25. Multiple attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. #mixture of
MoE is 4. #group of attention weights is 8. GAP = 0.8157.
26. Multiple attention pooling LSTM model that has one
LSTM model to generate the attention weights for pooling
over the output of another LSTM model. #layer of LSTM
is 2. #cell of LSTM is 1024. #mixture of MoE is 4. #group
of attention weights is 8. GAP = 0.8081.

27. Multiple attention pooling LSTM model with posi-
tional embedding (Section 5). #layer of LSTM is 2. #cell
of LSTM is 1024. #mixture of MoE is 4. The dimension of
positional embedding is 32. #group of attention weights is
8. GAP = 0.8169.

A.6. Bagging and Boosting Models

GAP = 0.8218.

33. Bagging model of 8 versions of model #18, com-
bined with class-wise weighted averaging. GAP = 0.8258.
34. Boosting model of 8 versions of model #18, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8246.

35. Bagging model of 8 versions of model #25, com-
bined with class-wise weighted averaging. GAP = 0.8244.
36. Boosting model of 8 versions of model #25, com-
bined with attention weighted stacking, with weight clip-
ping. GAP = 0.8242.

37. Bagging model of 8 versions of model #10, com-

bined with simple averaging. GAP = 0.8216.

38. Boosting model of 8 versions of model #10, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8218.

A.7. Distillation Models

The models in this section is trained using distillation
(Section 9). During training, the predictions from model
#75 are used as soft targets.

39. Model #2 with distillation training. GAP = 0.8169.
40. Model #18 with distillation training. GAP = 0.8266.
41. Model #20 with distillation training. GAP = 0.8259.
42. Model #10 with distillation training. GAP = 0.8237.
43. Model #21 with distillation training. GAP = 0.8291.
44. Model #16 with distillation training. GAP = 0.8258.
45. Bagging model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8249.

46. Boosting model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8254.

28. Bagging model of 8 versions of model #2, combined

with simple averaging. GAP = 0.8225.

A.8. Cascade Models

29. Ensemble of 4 Chaining models. The ﬁrst is the
normal Chaining model with video-level input. The second
is a Chaining model with weighted cross entropy loss. The
third is a Chaining model that predict label and top-level
verticals at the same time. The fourth is a Chaining model
that predict the infrequent labels with softmax function. All
4 models share the same parameters. #mixture of MoE is
4. #stage in Chaining is 3. The dimension of projection in
Chaining is 100. GAP = 0.8216.

30. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging. Remove the ex-
amples with too high weights from training set, since it may
be hopeless to predict these examples correctly. GAP =
0.8213.

31. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging, without weight
clipping. GAP = 0.8198.

32. Boosting model of 8 versions of model #2, combined
with class-wise weighted averaging, with weight clipping.

The models in this section are models using the predic-

tion of other models as part of the input (Section 7).

47. Model #2 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8231.

48. Model #6 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8245.

49. Model #18 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8268.

50. Model #25 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

51. Chaining CNN model with cascade training using
the prediction of model #75 as part of the input. The widths
of ﬁlter are 1, 2 and 3. The corresponding #channels of
ﬁlters are 256, 256, and 512. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. #stage of Chaining
is 2. GAP = 0.8214.

52. Model #3 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

12

74. Model #2 with cascade training using the L1-
normalized prediction of model #76 as part of the input.
GAP = 0.8227.

A.9. Ensemble Models

In this section we lists the performance of some of the
ensemble models. Alongside the GAP score on ”validate2”
set, we also report the GAP score on the blind test set (Pri-
vate LeaderBoard in the competition).

75. Ensemble of 4 single models. Model #2, #10, #18,
and #25 with a class-wise weighted stacking gets a GAP of
0.8373. Private LB GAP = 0.83703.

76. Ensemble of 8 single models. Model #2, #10, #18,
#25, #47, #48, #49, and #50 with a class-wise weighted
stacking gets a GAP of 0.8397. Private LB GAP = 0.83941.
77. Ensemble of 27 single models. Model #1 - #27 with a
class-wise weighted stacking gets a GAP of 0.8427. Private
LB GAP = 0.84250.

78. Ensemble of 57 models. Model #1 - #57 with an
attention weighted stacking (#rank=3, #attention=16) gets a
GAP of 0.8459. Private LB GAP = 0.84561.

79. Ensemble of 74 models. Model #1 - #74 with an
attention weighted stacking (#rank=4, #attention=16) gets a
GAP of 0.8462. Private LB GAP = 0.84583.

80. Ensemble of model #78 and #79 by getting the most
conﬁdent 20 tags from each model and averaging the con-
ﬁdence scores. This is done directly on the test set without
training. Private LB GAP = 0.84590. This is our ﬁnal sub-
mission.

Appendix B. Source Code

The source code of our solution is made public on
GitHub. Visit https://github.com/wangheda/youtube-8m for
the implementation details of our models.

53. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 2.
GAP = 0.8214.

54. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 4.
GAP = 0.8266.

55. Model #20 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8265.

56. Model #6 with cascade training using the prediction
of model #75 as part of the input. The prediction of model
#6 for infrequent labels and the prediction of the cascade
model for frequent ones are joined together as the ﬁnal pre-
diction. GAP = 0.8228.

57. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8202.

58. Model #25 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8254.

59. Model #20 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8250.

60. Model #6 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8245.

61. Model #7 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8247.

62. Model #21 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8251.

63. Model #16 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8258.

64. Model #10 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8248.

65. Model #10 with cascade training using the prediction
of model #76 as part of the input. The prediction of model
#76 is also used to up-sample the misclassiﬁed samples as
in the Boosting model. GAP = 0.8218.

66. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8181.

67. Model #2 with cascade training using the predictions

of model #76 and #66 as part of the input. GAP = 0.7989.

68. Model #2 with cascade training using the predictions
of model #76, #66, and #67 as part of the input. GAP =
0.7849.

69. Model #2 with cascade training using the predictions
of model #76, #66, #67, and #68 as part of the input. GAP
= 0.7753.

70. Model #2 with cascade training using the predictions
of model #76, #66, #67, #68, and #69 as part of the input.
GAP = 0.7833.

71. Model #2 with cascade training using the predictions

of model #76 as part of the input. GAP = 0.8183.

72. Model #2 with cascade training using the predictions

of model #76 and #71 as part of the input. GAP = 0.8085.

73. Model #2 with cascade training using the predictions
of model #76, #71, and #72 as part of the input. GAP =
0.8111.

13

7
1
0
2
 
n
u
J
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
5
1
5
0
.
6
0
7
1
:
v
i
X
r
a

The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge

He-Da Wang
whd.thu@gmail.com

Teng Zhang
zhangteng1887@gmail.com

Ji Wu
wuji ee@mail.tsinghua.edu.cn
Multimedia Signal and Intelligent Information Processing Laboratory
Department of Electronic Engineering
Tsinghua University, Beijing, China

Abstract

This article describes the ﬁnal solution 1 of team mon-
keytyping, who ﬁnished in second place in the YouTube-8M
video understanding challenge. The dataset used in this
challenge is a large-scale benchmark for multi-label video
classiﬁcation. We extend the work in [1] and propose sev-
eral improvements for frame sequence modeling. We pro-
pose a network structure called Chaining that can better
capture the interactions between labels. Also, we report our
approaches in dealing with multi-scale information and at-
tention pooling. In addition, We ﬁnd that using the output of
model ensemble as a side target in training can boost single
model performance. We report our experiments in bagging,
boosting, cascade, and stacking, and propose a stacking al-
gorithm called attention weighted stacking. Our ﬁnal sub-
mission is an ensemble that consists of 74 sub models, all of
which are listed in the appendix.

1. Introduction

Videos have been a very important type of content on
Internet. Understanding video from its audio-visual con-
tent is key to various applications such as recommendation,
searching, and question answering. The research on video
analysis is also an important step for the computer to un-
derstand the real world. Datasets such as Sports-1M[8] and
ActivityNet[4] encourage the research on video classiﬁca-
tion of sports and human activities. YouTube-8M[1] is a
large-scale video dataset that consists of about 7.0 million
YouTube videos that was annotated with a vocabulary of
4716 tags from 24 diverse categories. The average number
of tags per video is 3.4.

The YouTube-8M video understanding challenge is an

1visit https://github.com/wangheda/youtube-8m for the source code.

arena for video classiﬁcation researchers and practitioners.
In the competition, the dataset is divided into three parts.
The training set contains 4.9 million samples. The validate
set contains 1.4 million samples. The test set contains 0.7
million samples. The ground truth labels annotated to the
samples in the training set and the validate set are available
to the participants. The test set is divided into two half,
the open test set and the blind one. In the progress of the
competition, participants can submit their predictions to the
test set. The scoring server on Kaggle would evaluate them
and return the scores on the open test set (the public leader-
board). After the competition, the winners are decided by
the scores of their submissions on the blind test set (the pri-
vate leaderboard).

In the competition, submissions are evaluated using
Global Average Precision (GAP) at 20. The metrics is cal-
culated as follows. For each video, the most conﬁdent 20
label predictions are selected along with the conﬁdence val-
ues. The tuples of the form {video, label, conf idence}
from all the videos are then put into a long list sorted by
conﬁdence values. This list of predictions are then evalu-
ated with the Average Precision (Eq. 1), in which p(i) is the
precision and r(i) is the recall given the ﬁrst i predictions.

AP =

p(i)∆r(i)

(1)

N
(cid:88)

i=1

We divide the dataset into ﬁve parts: train1, validate1,
train2, validate2, and test. The division is done base on ﬁle
name pattern (Table 1). The set train1 is used for single
model training, in which the set validate1 served as a hold-
out test set for early-stopping. Ensemble models are trained
on the set train2, which have no intersection with the train-
ing set of single models. The set validate2 is a hold-out
test set for early-stopping in the training of ensemble mod-
els. The inference procedure uses the test set to generate
submissions. The whole system structure is shown in Fig.

1

Figure 1: The structure of our system.

Part
train1
validate1
train2
validate2
test

File glob
train??.tfrecord
validate[a]?.tfrecord
validate[ˆa0-9]?.tfrecord
validate[0-9]?.tfrecord
test??.tfrecord

#samples
4,906,660
21,918
1,270,412
109,498
700,640

Table 1: Dataset division scheme in our system.

1. This data division strategy may not be optimal, since it
limit the amount of data used in single model training to the
4.9 millions training set examples, which may considerably
affect the performance of single models.

During the training of all our models, We did not use
any data augmentation techniques. Adam [10] optimization
algorithm is used throughout the training process. For mod-
els that use frame level feature rgb, audio, we use a learn-
ing rate of 0.001 and a batch size of 128. For models that
use video level feature mean rgb, mean audio, the default
learning rate and batch size is 0.01 and 1024. Note that
these hyper parameters may not be optimal since we did not
perform any parameter search. Every of our models can ﬁt
in the graphics memory of either an Nvidia GTX 1080 (8G)
or an GTX 1080Ti (11G).

In the rest of this report, we summarize our contribu-
tions in our solution. We ﬁrst introduce our improvements
over the baseline Long-Short Term Memory (LSTM) model
(Section 2). We then introduce a deep network structure
that we use in many models to capture label correlations
(Section 3). Then, we introduce our best performing sin-
gle model that utilize multiscale information via Convolu-
tional Neural Network (CNN) and LSTM (Section 4). Also,
we explore the use of attention pooling in video sequence
classiﬁcation (Section 5). Ensemble methods such as bag-
ging, boosting (Section 6), cascade (Section 7), and stack-
ing (Section 8) are also explored in this report. We also
found that using the predictions of ensemble models as a

soft target can greatly boost single model performance (Sec-
tion 9). In the end, we summarize our road-map and discuss
the contribution of our solution(Section 10).

2. Baseline models

In this section, we list the performances of some of the
baseline models in the technical report [1] and some alter-
native architectures evaluated in our system. The perfor-
mances reported here are different from the ones reported
in the technical report, since the dataset used in the report is
a little larger (8.3 million examples) than the one provided
in this competition. Also, we have limited the amount of
data used for single model training to 4.9 million examples.
The GAP scores we reported in this paper are evaluated with
the ”validate2” set if not otherwise speciﬁed.

We re-implemented four baseline approaches: Logistic
Regression (LR), Mixture-of-Expert (MoE), Deep Bag-of-
Frames (DBoF), and Long-Short Term Memory (LSTM).
The GAP scores are listed in Table 2. We choose a mixture
of 16 for the MoE model for the best performance. For the
LSTM model, we choose the number of layers, the number
of cells per layer, and the number of Moe mixtures to be 2,
1024 and 8 respectively.

Input Feature
Video Level, µ
Video Level, µ
Frame Level, {xv
Frame Level, {xv

Model
Logistic Regression
Mixture-of-Experts

} Deep Bag-of-Frames
} LSTM

1:Fv

1:Fv

GAP
0.7577
0.7957
0.7845
0.8131

Table 2: The performance of baseline approaches in our
system.

In the conventional LSTM [7], the memory cells that
share the same input gate and the same output gate form
a memory block. Putting more than one cells into a block
makes the training of LSTM more efﬁcient. We create an ar-

2

chitecture in which all the cells share a single input gate and
a single forget gate and call it ”LSTM-S”. In this architec-
ture, the cells use individual output gates, since we ﬁnd that
sharing the output gate can be harmful to the performance.
Another network structure we create by adding an input
accumulator to the architecture of LSTM model is what we
call the ”LSTM-A”. In this structure, we add a new set of
memory, input gate and forget gate to the cell to directly
”remember” the input. Eq. 2 gives the architecture of this
model. In the equation, xt, ct, ht are the input, the memory,
and the hidden state, ot, it, ft are the output gate, the input
gate, and the forget gate, c(cid:48)
t, dt are the added memory and
the hidden state for input accumulation, i(cid:48)
t, f (cid:48)
t are the added
input gate and forget gate, σ is the sigmoid function, g is
the activation function which we choose to be tanh, and n
is the L2-normalization function.

content or audio content. Therefore, the visual or audio ac-
tivities do not have to happen spontaneously to be meaning-
ful. We adopt a parallel way that model visual and audio
content with separate LSTM models. The ﬁnal states of the
two LSTM models are then concatenated and go through an
MoE unit. The performance of the parallel model is shown
in Table 4. The number of layers is 2. The number of cells
c and the number of mixtures m in MoE model are shown
in the table.

Model
vanilla LSTM, c = 1024, m = 8
parallel LSTM, cv = 1024, ca = 128, m = 8

GAP
0.8131
0.8161

Table 4: Parallel modeling of visual and audio content is
better than simple feature concatenation.





















ot
mt
it
ft
i(cid:48)
t
f (cid:48)
t
ct
ht
c(cid:48)
t
dt

= TN +2M,6N





ht−1
xt
dt−1





= σ(ft) · ct−1 + σ(it) · g(mt)
= σ(ot) · g(ct)
t) · c(cid:48)
= σ(f (cid:48)
= n(c(cid:48)
t)

t−1 + σ(i(cid:48)

t) · xt

The performance of the two modiﬁed LSTM models are
listed in Table 3. The number of cells per layer are 1024.
The number of mixtures in MoE is set to 8. The number of
layers l is shown in the table. The two models performs bet-
ter than the original model and contribute to the ensemble
because their varieties in model structure.

Model
vanilla LSTM, l = 1
vanilla LSTM, l = 2
LSTM-S, l = 1
LSTM-A, l = 1

GAP
0.8091
0.8131
0.8123
0.8131

Table 3: The performances of the single-block LSTM
(LSTM-S) and the input-accumulator LSTM (LSTM-A),
compared to the original LSTM model.

In the dataset, an example has both visual feature and
audio feature. The naive way of modeling two different
features is to concatenate them into a single feature. We
ﬁnd this to be questionable for two reasons. First, in many
videos such as music videos and family albums, the audio
and the visual content are independent of each other. Also,
people often can make sense of a video only by its visual

3

There is a potential weakness in modeling visual and au-
dio features independently. It might be preferable to allow
the visual network and the audio network to interact at cer-
tain points, since a visual / audio event may be related to a
former audio / visual event in the same video. We leave this
issue to future works.

(2)

3. Label Correlation

In multi-label classiﬁcation settings, an example may be
annotated many labels. Some labels tend to appear in the
same example at the same time, some tend not to. Such in-
formation can be used to improve the performance of multi-
label classiﬁcation models.

Classiﬁer chain [12] create a chain of classiﬁers. One
classiﬁer on the chain predict one label by using not only the
input feature but also the predictions of other labels from the
previous models on the chain. The training of one chain of
classiﬁer involves training L models in which L is the size
of the vocabulary of all labels. However, in real world prob-
lems where the direction of dependency is unknown, an en-
semble of classiﬁer chains is usually used, which makes the
computation complexity even higher and intractable if the
vocabulary of labels is large. A neural network structure
that mimic the classiﬁer chain [11] is proposed to address
the multi-label problem. However, such network structure
has a depth of L that makes it hard to optimize if the vocab-
ulary of labels is large.

We propose a novel end-to-end deep learning structure
that we called ”Chaining” to better utilize the correlation be-
tween labels. In a Chaining model (Figure 2), several repre-
sentations are joined by a chain of MoE model. The predic-
tions are projected to features of lower dimension and used
in the following stages. The representations can be gener-
ated by homogeneous models or heterogeneous ones. We
constantly apply auxiliary cross-entropy loss on the inter-

(a) A unit in Chaining accept one fea-
ture and several model predictions as the
input. The predictions are projected to
lower dimension for efﬁciency.

(b) The whole architecture of Chaining consists of several stages. A Chaining unit
in one stage accepts a feature vector, either from input directly or from a represen-
tation of LSTM or CNN, and all the predictions from earlier stages.

Figure 2: Chaining: a deep learning architecture for multi-label classiﬁcation.

mediate predictions to accelerate the training progress. The
ﬁnal loss function is a weighted average over the loss on
the ﬁnal prediction and the auxiliary losses. We typically
allocate only 10% ∼ 20% of the weights to the auxiliary
losses, since the loss function at the ﬁnal stage is the most
important.

We performed experiments using three basic models,
Mixture-of-Expert, LSTM and CNN. The CNN model we
use in our system is the same as the benchmark method of
sentence classiﬁcation [9], where the length of the ﬁlter is
ﬁxed to the size of feature vector per frame and the resulting
feature map goes through a max-over-time pooling. The ﬁ-
nal state of LSTM and the max-pooled feature map are used
as feature representation in Chaining models and go through
an MoE model for label prediction in the original models.
The performance with and without using Chaining is shown
in Table 5. For the original MoE model, the number of mix-
tures is 16. For the Chaining MoE model, the number of
mixture is 2, the number of stages is 8, and the predictions
are projected into a 128 dimensions vector. For the original
LSTM model, the number of mixtures is 8. For the Chain-
ing LSTM model, the number of mixtures is 4, the number
of stages is 2, and the dimension of projection is 200. For
the original CNN model, the width of ﬁlter and the corre-
sponding numbers of channel are 1×512, 2×512, 3×1024.
For the Chaining CNN model, the corresponding parame-
ters are 1 × 128, 2 × 128, 3 × 256, and the number of stages
is 4. The parameters are chosen to make the original models
have almost equal number of parameters with their Chain-
ing counterparts.

4. Temporal Multi-Scale Information

Input Feature
Video-level, µ
Frame Level, {xv
Frame Level, {xv

1:Fv

1:Fv

Model Original Chaining
0.8106
MoE
0.8172
0.8179

0.7965
} LSTM 0.8131
} CNN
0.7904

Table 5: The performance (GAP) of Mixture-of-Experts,
LSTM and CNN models with and without using Chaining
structure.

age would change with its distance to the observer. How-
ever, temporal information does not have the same rescaling
effect. Therefore it may seems unnatural to model videos
from different temporal scales.

However, we argue that temporal scales matters in video
analysis. A task can be divided into several actions, and
each action involves the interaction between certain objects.
The label annotated to a video can be related to a concept at
different temporal scale: an object, an action, or a task.

One important observation is that video can be seg-
mented into several clips. Each of these clips may contain
content information of a certain aspect. We can make pre-
dictions based on each of these clips and then join them
together to make more accurate predictions. The segmenta-
tion of video is done by either clustering the adjacent frames
[14] or splitting the video into clips of equal durations [15].
We propose a temporal-segment LSTM model (Fig. 3a),
in which the video is split into equal-sized clips, each of
which is modeled by an LSTM model. The models for dif-
ferent clips share the same parameters. The ﬁnal state of
each sequence are treated as another high-level sequence
and modeled by another LSTM model.

Using information from different spatial scales have
been discussed in many image analysis literatures. The
main reason to do so is that the size of an object in an im-

Temporal-pooling LSTM model (Fig. 3a) is a multi-
layer LSTM model in which we inserted temporal k-pooling
layer between LSTM layers. It is similar to the temporal-

4

ition that the representation of low-level sequences contain
more reﬁned features, while the high-level representation is
close to an averaged view of the whole video.

We propose a novel temporal CNN-LSTM model (Fig.
3c) that utilize multi-scale information. This model shares
the previous intuition that adjacent frames can be aggre-
gated to generate features at different temporal scale. In-
stead of direct operating on the original feature, we use
convolution layers to detect patterns in the low-level fea-
tures and combine adjacent ﬁlter outputs by max-pooling
along the time dimension. The ﬁlters used in a convolution
layer are of the same length with the dimension of the fea-
ture, and their widths and channels can be varied. We use
different LSTM models and Moe classiﬁers for the repre-
sentation of the feature maps of different temporal scales.
The predictions generated from features of different scales
are combine using a consensus function. We ﬁnd that av-
eraging is a good consensus function, and maximum would
lead to difﬁculties in convergence.

GAP
Model
0.8131
vanilla LSTM
temporal-pooling LSTM 0.8085
temporal-segment LSTM 0.8122
multi-resolution LSTM
0.8148
multi-scale CNN-LSTM 0.8204

Table 6: Performance of multi-scale models.

Table 6 lists the performances of the models discussed
in this section. The temporal-pooling LSTM model uses 2-
pooling and has 4 LSTM layers (#cells=1024) and 4 MoE
mixtures. The temporal-segment LSTM model uses a dura-
tion of 10 frames for the clips and has 8 MoE mixtures and
2 LSTM layers, each of the layers has 1024 memory unit.
In the multi-resolution LSTM model, we use 2-pooling to
get shorter input sequences; the number of stages in Chain-
ing is 4; the number of mixtures in MoE is 4; the projection
dimension is 256; the LSTM models in it are 2-layer par-
allel LSTM (#cells is 512 for video and 64 for audio). In
the multi-scale CNN-LSTM model, the number of layers in
CNN model is 4; the number of mixtures in MoE is 4; the
LSTM models in it are 1-layer LSTM (#cells=1024); in ev-
ery layer of the CNN model, the width of ﬁlters and the cor-
responding number of channels are 1×256, 2×256, 3×512;
the pooling layers in it are 2-pooling.

5. Identifying Salient Frames with Attention

Not all the frames in a video are equally informative.
There are many reasons that a frame might contribute little
in video classiﬁcation. Images that are too dark, too bright,
or too blurry are hard to analysis for the image classiﬁcation

(a) Temporal-pooling (with the dashes) and temporal-segment
(without the dashes) LSTM model.

(b) Multi-resolution LSTM model.

(c) Multi-scale CNN-LSTM model.

Figure 3: Models that utilize temporal multi-scale informa-
tion.

segment LSTM model. The difference is whether to use the
ﬁnal state of the one clip as the initial state of the next clip
in the ﬁrst layer of the LSTM model.

Instead of directly segmenting videos into clips, we
can aggregate the adjacent frames and gradually construct
features containing long-range information.
In multi-
resolution LSTM model (Fig. 3b), the original features
are average pooled along the time dimension to get shorter
sequences in which each frame covers longer time range.
For each sequence generated in this way, a separate LSTM
model is used to generate a sequence representation. The
representations are then joined with a Chaining model.
The representation from the highest-level goes into the ﬁrst
stage, while the one from the original sequence goes into the
ﬁnal stage of the Chaining model. This is due to the intu-

5

(b) An attention network generates multiple attention
weights for a frame based on the input feature and the
LSTM output at that frame.

(a) The multiple attention pooling scheme. The outputs
of LSTM are pooled with multiple attention weights.

Figure 4: Multiple attention pooling model.

network. Therefore, such frames may not be able to provide
useful semantic information. Also, sometimes a video con-
tains title screens, credits or text. They might be useful if
the frames are processed using optical character recognition
(OCR). However, in this competition, the frames are pre-
processed using an image classiﬁcation network pre-trained
with ImageNet dataset, which means the frame-level fea-
tures may not contain the semantic information in these text.
In addition, there are always frames irrelevant to the theme
of the video. In talk show videos the content of frames will
always be people talking while the topic of the talking is
what really important. In documentaries there are often a
lot of driving scenes which do not reﬂect the theme of the
video.

We propose a multiple attention pooling scheme (multi-
AP) for selecting salient frames in the video. As Fig. 4
shows, an LSTM model is used to deal with the frame-level
features. The outputs of the LSTM model are then aggre-
gated by pooling over the time dimension. The aggregation
is achieved by taking a weighted average over the outputs
of the LSTM model, in which the weights are generated
using an attention network, for which we use a fully con-
nected layer. The aggregated feature is fed into an MoE
model for label predictions. The attention pooling is re-
peated for multiple times and the predictions are combined
using a classiﬁer consensus function. For the choice of com-
bining method, we ﬁnd that maximum performs better than
averaging as a consensus function.

the input and output of the LSTM at a certain frame as the
input. The attention network output K groups of different
attention weights.

Each group of the weights are used to generate a group of
aggregated features and prediction results. The results from
all the groups are then combined by choosing the highest
conﬁdence predicted from all the models for each label l.
The MoE models in Fig. 4a share parameters with each
other.

eik = Wk[xi; yi]
aik = exp(eik)
(cid:80)Fv

i=1 exp(eik)

zk = (cid:80)Fv
i=1 aikyi
pk = M oE(zk)
(pk,l)
pl = max

k

(3)

(4)

We present a few examples outside of the training set to
show what kind of frames is highlighted by the attention
network in the multi-AP model (Fig. 5). For each video,
the frames with the lowest attention weights (left) and the
ones with the highest attention weights (right) are presented.
Since there are many groups of attention weight, the ﬁrst
group is used for this presentation. We have not observed
any noticeable patterns in the inter-group differences of the
weights.

We use a fully connected layer with the softmax acti-
vation function as the attention network (Eq. 3). It takes

In analysis of these examples, we ﬁnd that there are some
patterns about the attention weights. Title screens and the

6

Figure 5: Visualization of the attention weights in the multiple attention pooling model. The frames in each row comes from
the same video. The left four in each row are the frames with the lowest weights, while the right four are the ones with the
highest weights.

frames that is very dark tend to have low attention weights.
Also, if the object of interest is too small or partly blocked,
the weight of that frame tend to be low.

Our work is different from a previous work on attention
pooling [5] in two ways. First, our model uses multiple
groups of attention on the same output sequence and multi-
ple classiﬁers for prediction. The multiple attention scheme
may contribute to generating more stable classiﬁcation re-
sult. Second, our model do an attention pooling over the
outputs of LSTM model which is a global representation
of the sequence while the previous work do a pooling over
local CNN outputs.

We also adopt a local attention pooling scheme that ap-
plies attention pooling over the input features (Eq. 5). The
representation of the LSTM model is also used in the MoE
model for prediction.

The LSTM model in the four models share the same pa-
rameter (#layers=2, #cells=1024). The MoE models have 8
mixtures in the two multi-AP models, while the one in the
local-AP model has 4 mixtures. The two multi-AP models
both have 8 groups of attention weights. And the dimen-
sion of the positional embedding in the positional multi-AP
model is 32.

GAP
Model
0.8131
vanilla LSTM
0.8133
local-AP LSTM
multi-AP LSTM
0.8157
positional multi-AP LSTM 0.8169

Table 7: Performance (GAP) of attention models.

z = (cid:80)Fv

i=1 aixi

p = M oE([yFv ; z])

(5)

6. Bagging and Boosting

We compare the two attention pooling scheme with the
baseline in Table 7. The ”local-AP LSTM” refers to the
scheme using attention pooling over the input feature. The
”multi-AP LSTM” refers to the multiple attention pooling
scheme. The ”positional multi-AP LSTM” model add an
embedding for every frame position to the attention network
on the basis of the multi-AP LSTM model.

The parameters for the models in Table 7 is as follows.

Bootstrap aggregating [2], also called Bagging, is an en-
semble method that creates many versions of a model and
combines their results together. To create one version of a
model, one applies sampling with replacement to get a sub-
set of the original data, in which some original examples
may not present or present more than once[16]. Training on
different subset sampled from the original data would re-
sults in different models. The bagging algorithm is known
for its ability to reduce the variance of a model. We apply

7

Bagging to some of our models, and ﬁnd that Bagging can
generally boost the GAP performance by 0.6% ∼ 1.2%.
The results are shown in Table 8.

weights to 5 to ensure that the algorithm would not place all
the weights on a few formerly misclassiﬁed examples, since
they may not be visually classiﬁable.

Original Bagging
Input Feature Model
0.8225
0.8106
Video-level
0.8216
0.8160
Frame Level
0.8258
Frame Level
0.8179
0.8244
Frame Level multi-AP LSTM 0.8157

Chaining
parallel LSTM
Chaining CNN

Model
Chaining (Video)
parallel LSTM
Chaining CNN
multi-AP LSTM

Original Boosting
0.8218
0.8106
0.8218
0.8160
0.8242
0.8179
0.8246
0.8157

Table 8: Performance (GAP) of bagging models.

Table 9: Performance (GAP) of boosting models.

Boosting [3] is another way to create different versions
of a model. Compared to Bagging which can be run in
parallel, Boosting is a sequential ensemble method. The
(k + 1)th classiﬁer is constructed considering the previous
k classiﬁers by re-sampling a distribution that highlights the
misclassiﬁed examples.

The re-sampling is often implemented by using weighted
examples. If there are N training examples and L labels,
the number of total classiﬁcation results is N × L. Most
works on Boosting in multi-label classiﬁcation use a weight
W N ×L over N samples and L labels, which is computa-
tionally intractable when the vocabulary of label is large, as
is the case in this competition.

We adopt a per-example weighting scheme that assign
a weight W N over the training samples. The weights are
updated with Eq. 6. In the equations, Wk,n is the weight
assigned to the nth example in the training of the kth clas-
siﬁer. Errk,n is the error rate of the nth example by eval-
uating the kth classiﬁer. Errk is the average error rate of
the kth classiﬁer. And Zk is a coefﬁcient that scales the av-
erage value of Wk,n to 1. α is a parameter controlling the
highlighting effect on the misclassiﬁed examples which we
constantly set to 1 in our system.

Wk,n exp(αrkErrk,n)

W0,n = 1.0
Wk+1,n = N
Zk
in which,
rk = log( 1.0−Errk
Errk = 1
N
Errk,n ∈ [0, 1]
Zk = (cid:80)N

Errk
(cid:80)N

)

n=1 Errk,n

n=1 Wk,n exp(αrkErrk,n)

In this algorithm, the weights of the misclassiﬁed ex-
amples are increased in the following classiﬁers. However,
in multi-label classiﬁcation, misclassiﬁcation is hard to de-
ﬁne. We choose the Precision Equal Recall Rate (PERR)
as the implementation of error rate, since it is both per-
example evaluated and in coordinate with the GAP score in
most models (from empirical observation). We also clip the

The comparison between the single models and their
Boosting counterparts are shown in Table 9. The Boosting
algorithm generates a performance boost similar as Bagging
does.

7. Cascade Classiﬁer Training

Adding models into the ensemble would usually make
the performance better. However, with the number of mod-
els in the ensemble model increasing, the gain from newly
added models tends to diminish, especially if one add mod-
els that are similar to the existing models. We address this
problem by using cascade training in which the predictions
of other models as a part of its input of the model during
training.

(6)

Figure 6: Cascade layer as a replacement for MoE in cas-
cade classiﬁer training.

The structure of cascade layer is shown in Fig. 6. The
predictions from the other models are averaged to generate
an averaged prediction, which is then projected to a feature
of low dimension. All the models that have MoE as their
last layer can be modiﬁed into a cascade model by having
their MoE layer replaced by the cascade layer.

8. Stacking Methods

Stacking is an ensemble method that uses a machine
learning model to combine the predictions of many mod-
els into one prediction. We use the ”train2” set (Section 1)

8

as the training set for stacking. The trivial case of stacking
is to simply average all the predictions. If there are M mod-
els and L labels in total, simple averaging the predictions of
all models can be written as Eq. 7.

pl =

1
M

M
(cid:88)

m=1

pm,l

(7)

Linear weighted averaging (Eq.

8) is also a simple
scheme. This method has M weights, one for each model
in the ensemble.

M
(cid:88)

m=1

M
(cid:88)

m=1

pl =

wmpm,l, s.t.

wm = 1, wm > 0

(8)

1
M

M
(cid:88)

m=1

For multi-label classiﬁcation, there are L predictions for
every example. Some models might perform better than the
others on a subset of the labels. Therefore, we could extend
linear weighted averaging to every class. We denote this
method class-wise weighted averaging (Eq. 9).

1
M

M
(cid:88)

m=1

pl =

wm,lpm,l, s.t.

wm,l = 1, wm,l > 0

(9)
While the ensemble methods often deal with the predic-
tions of individual models, the original input may also help
to decide which models to trust. The winning team of Net-
ﬂix Prize proposes the Feature-Weighted Linear Stacking
[13] to utilize the information in meta-features. Although
there are no useful meta-feature in the Youtube-8M dataset,
we consider the averaged frame input and the averaged pre-
dictions from individual models as useful indicators. We
propose the attention weighted stacking to utilized these in-
formation.

The attention weights α (Eq. 10) are generated with a
fully connected layer with softmax as the activation func-
tion and averaged input ¯x and prediction ¯p as the input of
the layer.

The predictions are then weighted averaged as shown in
Eq. 11. The weight is a mixture of K low-rank matrices
(rank D) in which the matrices are selected by the output
of the attention network (Eq. 10). In the attention weighted
stacking, V, A, B, a, b, c are trainable parameters.

We perform a comparison among the stacking methods
on an ensemble of 74 models (model #1 - #74 in Appendix
A). The number of attention weights is 16 and the rank
of the matrix components is 4 in the attention weighted
stacking method. The comparison shows that the attention
weighted stacking performs better than the other methods
(Table 10).

Stacking method
Simple Averaging
Linear Weighted Averaging
Class-wise Weighted Averaging
Attention Weighted Stacking

GAP
0.8436
0.8449
0.8453
0.8458

Table 10: Performance (GAP) of different stacking models
on the entire set of models (74 models). The GAP scores
in this table is evaluated with the blind test set (the private
leaderboard score in the competition).

9. Distilling Knowledge from Ensemble

Ensemble models might perform much better than even
the best single model. However, in real world systems, the
memory and computation resources are often too limited
for ensemble models to deploy. Distillation [6] is a training
scheme that aims to compress the knowledge in ensemble
model to a single model. Another motivation of using dis-
tillation is the observation that the labels are often wrong
or incomplete, especially when the vocabulary of label is
large.

We adopt a training scheme that uses the predictions of
ensemble ˆp as a soft target along with the real target l during
training. The new loss function is a weighted average of two
losses, one for each target (Eq. 12).

α = σ(V [¯x; ¯p])

¯x = 1
Fv

(cid:80)Fv

i=1 xi

¯p = 1
M

(cid:80)M

m=1 pm

pl

= 1
M

(cid:80)M

m=1 wm,lpm,l

wm,l =

(cid:80)M

e

= (cid:80)K

exp(em,l)
m=1 exp(em,l)
k=1 αk(AT

s.t.

Ak ∈ RD×M , Bk ∈ RD×L
ak ∈ RM ×1, bk ∈ RL×1
ck ∈ R

(10)

ˆL = (1 − λ)ce(p, l) + λce(p, ˆp)

(12)

We use the predictions of model #75 (Appendix A.9) as
the soft target in training and train several models with the
same parameters as the original models. The comparison is
listed in Table 11. The comparison shows that distillation
can greatly boost single model performance (often better
than Bagging).

In the previous sections, we review our system, present
our intuition, and show how we address the problems in

9

k Bk + akbT

k + ck)

(11)

10. Summary

Original Distillation
Model
0.8106
Chaining (Video)
0.8160
parallel LSTM
0.8179
Chaining CNN
0.8172
Chaining LSTM
Multi-scale CNN-LSTM 0.8204

0.8169
0.8237
0.8266
0.8291
0.8258

Table 11: Performance (GAP) of single models trained us-
ing distillation.

multi-label video classiﬁcation. We ﬁnd that attention pool-
ing and temporal multi-scale information is very important
for video sequence modeling. We also propose a network
structure for large-vocabulary multi-label classiﬁcation. We
review our work in ensemble methods such as Bagging,
Boosting, Cascade, Distillation and Stacking. We propose a
stacking network that uses attention to weight models. Our
system road-map is shown in Table 12.

Changes
27 single models
+ 11 bagging & boosting models
+ 8 distillation models
+ 11 cascade models
+ 17 more cascade models
Attention Weighted Stacking

GAP
0.8425
0.8435
0.8437
0.8451
0.8453
0.8458

Table 12: Performance (GAP) of ensemble models. The
stacking method used for the ﬁrst 5 results is class-wise
weighted model. The ensemble in each row include all the
changes in the rows above it. The results reported in this
table are evaluated on the blind test set (the private leader-
board score in the competition).

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m:
CoRR,
A large-scale video classiﬁcation benchmark.
abs/1609.08675, 2016.

[2] L. Breiman.

Bagging predictors. Machine Learning,

24(2):123–140, 1996.

[3] L. Breiman. Arcing classiﬁer (with discussion and a rejoin-
der by the author). Ann. Statist., 26(3):801–849, 06 1998.
[4] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 961–970, 2015.

[5] M. J. Er, Y. Zhang, N. Wang, and M. Pratama. Atten-
tion pooling-based convolutional neural network for sen-
tence modelling. Information Sciences, 373:388 – 403, 2016.
[6] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, pages
1725–1732, 2014.

[9] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014.

[11] J. Read and J. Hollm´en. A deep interpretation of classi-
ﬁer chains. In International Symposium on Intelligent Data
Analysis, pages 251–262. Springer, 2014.

[12] J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classiﬁer
chains for multi-label classiﬁcation. Machine Learning and
Knowledge Discovery in Databases, pages 254–269, 2009.

[13] J. Sill, G. Tak´acs, L. Mackey, and D. Lin. Feature-weighted
linear stacking. arXiv preprint arXiv:0911.0460, 2009.
[14] E. H. Spriggs, F. De La Torre, and M. Hebert. Temporal seg-
mentation and activity classiﬁcation from ﬁrst-person sens-
ing. In Computer Vision and Pattern Recognition Workshops,
2009. CVPR Workshops 2009. IEEE Computer Society Con-
ference On, pages 17–24. IEEE, 2009.

[15] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
towards good
L. Van Gool. Temporal segment networks:
practices for deep action recognition. In European Confer-
ence on Computer Vision, pages 20–36. Springer, 2016.

[16] Z.-H. Zhou. Ensemble methods:

foundations and algo-

rithms. CRC press, 2012.

10

Appendix A. Final Submission

In this section we list all the components included in our
ﬁnal submission. We apply a stacking method called the At-
tention Weighted Average (Section 8) to merge the result of
these models instead of carrying on careful model selection.
Therefore it might be possible that a subset of these models
could reach equal or better performance. The parameters
are for a quick grasp of the structure of the models. Re-
fer to our code 2 if you are interested in the implementation
details. The GAP scores reported in the appendix are eval-
uated with the ”validate2” set if not otherwise speciﬁed.

A.1. Video-level Models

1. Chaining model (Section 3). During training, if the
loss of the current batch is less than 10, use the most conﬁ-
dent tag predicted as a soft target for every example in the
batch. #mixture of MoE is 4. #stage of Chaining is 3. pro-
jection dimension in Chaining is 100. GAP = 0.8067.

2. Chaining model. #mixture of MoE is 2. #stage of
Chaining is 8. projection dimension in Chaining is 128.
GAP = 0.8106.

A.2. Baseline Models

If not otherwise speciﬁed, the memory cells of the ﬁnal
state are used for the input of the MoE model in the baseline
models.

3. LSTM model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8131.
4. LSTM model. #cell of LSTM is 2048. #layer of

LSTM is 2. #mixture of MoE is 4. GAP = 0.8152.

5. LSTM model of 2 layers. The ﬁrst layer is bi-
directional. The second layer is uni-directional. #cell of
LSTM is 1024,. #mixture of MoE is 4. GAP = 0.8105.

6. LSTM-S model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8123.
7. LSTM-A model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.
8. LSTM-A model of 2 layers. The ﬁrst layer is a for-
ward LSTM-A model. The second layer is a backward
LSTM-A model that takes the original input and the out-
put of the ﬁrst layer as input. #cell of LSTM is 1024. #layer
of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.

9. Parallel LSTM model (Section 2). #cell of LSTM is
1024. #layer of LSTM is 2. #mixture of MoE is 8. GAP =
0.8161.

10. Parallel LSTM model. The gated outputs of the ﬁ-
nal state are used for the input of the MoE model. #cell of
LSTM is 1024. #layer of LSTM is 2. #mixture of MoE is
8. GAP = 0.8160.

11. LSTM model with data augmentation by random
sampling 50% of the frames. #cell of LSTM is 1024. #layer

2https://github.com/wangheda/youtube-8m

of LSTM is 2. #mixture of MoE is 8. GAP = 0.8137.

12. CNN-LSTM model. The CNN model is described
in section 3. #layer of CNN is 1. The widths of ﬁlter are
1, 2 and 3. The corresponding #channels of ﬁlters are 1024,
1024, and 1024. #cell of LSTM is 1024. #layer of LSTM is
2. #mixture of MoE is 8. GAP = 0.8103.

A.3. Temporal Multi-Scale Models

13. Temporal-segment LSTM model (Section 4). The
duration of each clip is 10 frames. #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8122.
14. Temporal-pooling LSTM model (Section 4). The
model has 2-pooling between LSTM layers. #cell of LSTM
is 1024. #layer of LSTM is 4, #mixture of MoE is 8. GAP
= 0.8085.

15. Multi-resolution LSTM model (Section 4). The
model has parallel LSTM for sequence modeling. The
model has 2-pooling to get shorter representation of input.
#cell of LSTM is 512 for video and 64 for audio. #layer
of LSTM is 2. #stage of Chaining is 4. The dimension of
projection in Chaining is 256. #mixture of MoE is 4. GAP
= 0.8149.

16. Multi-scale CNN-LSTM model (Section 4). The
widths of ﬁlter are 1, 2 and 3. The corresponding #chan-
nels of ﬁlters are 256, 256, and 512. #layer in CNN is 4.
#layer of LSTM is 1. #cell of LSTM is 1024. #mixture of
MoE is 4. GAP = 0.8204.

17. Multi-scale CNN-LSTM model. All the parameters
are the same as model #16, except that the type of the LSTM
model is LSTM-S (as in model #6). GAP = 0.8147.

A.4. Chaining Models

18. Chaining CNN model (Section 3). The widths of ﬁl-
ter are 1, 2 and 3. The corresponding #channels of ﬁlters are
128, 128, and 256. The dimension of projection in Chaining
is 256. #mixture of MoE is 4. #stage of Chaining is 4. GAP
= 0.8179.

19. Chaining Deep CNN model. In this model a 3-layer
CNN model is used. The averaged input feature and the
3 max-pooled feature maps from the 3 layers of CNN are
combined using a 4-stage Chaining model. The widths of
ﬁlter are 1, 2 and 3. The corresponding #channels of ﬁl-
ters are 128, 128, and 256. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. GAP = 0.8155.

20. Chaining LSTM-CNN model. This model uses a
cascade of parallel LSTM and CNN model as the sub-model
in Chaining.#layer of LSTM is 1. #cell of LSTM is 1024 for
video and 128 for audio. The widths of ﬁlter are 1, 2 and
3. The corresponding #channels of ﬁlters are 128, 128, and
256. The dimension of projection in Chaining is 128. #stage
in Chaining is 3. #mixture of MoE is 4. GAP = 0.8122.

21. Chaining LSTM model (Section 3). #layer of LSTM
is 2. #cell of LSTM is 1024. The dimension of projection in

11

Chaining is 200. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8172.

22. Chaining LSTM model. The different stages in
Chaining model uses a shared input which is the cell mem-
ory of the ﬁnal state of an LSTM model. #layer of LSTM is
2. #cell of LSTM is 1024. The dimension of projection in
Chaining is 256. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8162.

A.5. Attention Pooling Models

23. Local attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. GAP =
0.8133.

24. Attention pooling LSTM model that has one LSTM
model to generate the attention weights for pooling over the
output of another LSTM model. #layer of LSTM is 1. #cell
of LSTM is 1024. #mixture of MoE is 8. GAP = 0.8088.

25. Multiple attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. #mixture of
MoE is 4. #group of attention weights is 8. GAP = 0.8157.
26. Multiple attention pooling LSTM model that has one
LSTM model to generate the attention weights for pooling
over the output of another LSTM model. #layer of LSTM
is 2. #cell of LSTM is 1024. #mixture of MoE is 4. #group
of attention weights is 8. GAP = 0.8081.

27. Multiple attention pooling LSTM model with posi-
tional embedding (Section 5). #layer of LSTM is 2. #cell
of LSTM is 1024. #mixture of MoE is 4. The dimension of
positional embedding is 32. #group of attention weights is
8. GAP = 0.8169.

A.6. Bagging and Boosting Models

GAP = 0.8218.

33. Bagging model of 8 versions of model #18, com-
bined with class-wise weighted averaging. GAP = 0.8258.
34. Boosting model of 8 versions of model #18, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8246.

35. Bagging model of 8 versions of model #25, com-
bined with class-wise weighted averaging. GAP = 0.8244.
36. Boosting model of 8 versions of model #25, com-
bined with attention weighted stacking, with weight clip-
ping. GAP = 0.8242.

37. Bagging model of 8 versions of model #10, com-

bined with simple averaging. GAP = 0.8216.

38. Boosting model of 8 versions of model #10, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8218.

A.7. Distillation Models

The models in this section is trained using distillation
(Section 9). During training, the predictions from model
#75 are used as soft targets.

39. Model #2 with distillation training. GAP = 0.8169.
40. Model #18 with distillation training. GAP = 0.8266.
41. Model #20 with distillation training. GAP = 0.8259.
42. Model #10 with distillation training. GAP = 0.8237.
43. Model #21 with distillation training. GAP = 0.8291.
44. Model #16 with distillation training. GAP = 0.8258.
45. Bagging model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8249.

46. Boosting model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8254.

28. Bagging model of 8 versions of model #2, combined

with simple averaging. GAP = 0.8225.

A.8. Cascade Models

29. Ensemble of 4 Chaining models. The ﬁrst is the
normal Chaining model with video-level input. The second
is a Chaining model with weighted cross entropy loss. The
third is a Chaining model that predict label and top-level
verticals at the same time. The fourth is a Chaining model
that predict the infrequent labels with softmax function. All
4 models share the same parameters. #mixture of MoE is
4. #stage in Chaining is 3. The dimension of projection in
Chaining is 100. GAP = 0.8216.

30. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging. Remove the ex-
amples with too high weights from training set, since it may
be hopeless to predict these examples correctly. GAP =
0.8213.

31. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging, without weight
clipping. GAP = 0.8198.

32. Boosting model of 8 versions of model #2, combined
with class-wise weighted averaging, with weight clipping.

The models in this section are models using the predic-

tion of other models as part of the input (Section 7).

47. Model #2 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8231.

48. Model #6 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8245.

49. Model #18 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8268.

50. Model #25 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

51. Chaining CNN model with cascade training using
the prediction of model #75 as part of the input. The widths
of ﬁlter are 1, 2 and 3. The corresponding #channels of
ﬁlters are 256, 256, and 512. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. #stage of Chaining
is 2. GAP = 0.8214.

52. Model #3 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

12

74. Model #2 with cascade training using the L1-
normalized prediction of model #76 as part of the input.
GAP = 0.8227.

A.9. Ensemble Models

In this section we lists the performance of some of the
ensemble models. Alongside the GAP score on ”validate2”
set, we also report the GAP score on the blind test set (Pri-
vate LeaderBoard in the competition).

75. Ensemble of 4 single models. Model #2, #10, #18,
and #25 with a class-wise weighted stacking gets a GAP of
0.8373. Private LB GAP = 0.83703.

76. Ensemble of 8 single models. Model #2, #10, #18,
#25, #47, #48, #49, and #50 with a class-wise weighted
stacking gets a GAP of 0.8397. Private LB GAP = 0.83941.
77. Ensemble of 27 single models. Model #1 - #27 with a
class-wise weighted stacking gets a GAP of 0.8427. Private
LB GAP = 0.84250.

78. Ensemble of 57 models. Model #1 - #57 with an
attention weighted stacking (#rank=3, #attention=16) gets a
GAP of 0.8459. Private LB GAP = 0.84561.

79. Ensemble of 74 models. Model #1 - #74 with an
attention weighted stacking (#rank=4, #attention=16) gets a
GAP of 0.8462. Private LB GAP = 0.84583.

80. Ensemble of model #78 and #79 by getting the most
conﬁdent 20 tags from each model and averaging the con-
ﬁdence scores. This is done directly on the test set without
training. Private LB GAP = 0.84590. This is our ﬁnal sub-
mission.

Appendix B. Source Code

The source code of our solution is made public on
GitHub. Visit https://github.com/wangheda/youtube-8m for
the implementation details of our models.

53. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 2.
GAP = 0.8214.

54. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 4.
GAP = 0.8266.

55. Model #20 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8265.

56. Model #6 with cascade training using the prediction
of model #75 as part of the input. The prediction of model
#6 for infrequent labels and the prediction of the cascade
model for frequent ones are joined together as the ﬁnal pre-
diction. GAP = 0.8228.

57. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8202.

58. Model #25 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8254.

59. Model #20 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8250.

60. Model #6 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8245.

61. Model #7 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8247.

62. Model #21 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8251.

63. Model #16 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8258.

64. Model #10 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8248.

65. Model #10 with cascade training using the prediction
of model #76 as part of the input. The prediction of model
#76 is also used to up-sample the misclassiﬁed samples as
in the Boosting model. GAP = 0.8218.

66. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8181.

67. Model #2 with cascade training using the predictions

of model #76 and #66 as part of the input. GAP = 0.7989.

68. Model #2 with cascade training using the predictions
of model #76, #66, and #67 as part of the input. GAP =
0.7849.

69. Model #2 with cascade training using the predictions
of model #76, #66, #67, and #68 as part of the input. GAP
= 0.7753.

70. Model #2 with cascade training using the predictions
of model #76, #66, #67, #68, and #69 as part of the input.
GAP = 0.7833.

71. Model #2 with cascade training using the predictions

of model #76 as part of the input. GAP = 0.8183.

72. Model #2 with cascade training using the predictions

of model #76 and #71 as part of the input. GAP = 0.8085.

73. Model #2 with cascade training using the predictions
of model #76, #71, and #72 as part of the input. GAP =
0.8111.

13

7
1
0
2
 
n
u
J
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
5
1
5
0
.
6
0
7
1
:
v
i
X
r
a

The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge

He-Da Wang
whd.thu@gmail.com

Teng Zhang
zhangteng1887@gmail.com

Ji Wu
wuji ee@mail.tsinghua.edu.cn
Multimedia Signal and Intelligent Information Processing Laboratory
Department of Electronic Engineering
Tsinghua University, Beijing, China

Abstract

This article describes the ﬁnal solution 1 of team mon-
keytyping, who ﬁnished in second place in the YouTube-8M
video understanding challenge. The dataset used in this
challenge is a large-scale benchmark for multi-label video
classiﬁcation. We extend the work in [1] and propose sev-
eral improvements for frame sequence modeling. We pro-
pose a network structure called Chaining that can better
capture the interactions between labels. Also, we report our
approaches in dealing with multi-scale information and at-
tention pooling. In addition, We ﬁnd that using the output of
model ensemble as a side target in training can boost single
model performance. We report our experiments in bagging,
boosting, cascade, and stacking, and propose a stacking al-
gorithm called attention weighted stacking. Our ﬁnal sub-
mission is an ensemble that consists of 74 sub models, all of
which are listed in the appendix.

1. Introduction

Videos have been a very important type of content on
Internet. Understanding video from its audio-visual con-
tent is key to various applications such as recommendation,
searching, and question answering. The research on video
analysis is also an important step for the computer to un-
derstand the real world. Datasets such as Sports-1M[8] and
ActivityNet[4] encourage the research on video classiﬁca-
tion of sports and human activities. YouTube-8M[1] is a
large-scale video dataset that consists of about 7.0 million
YouTube videos that was annotated with a vocabulary of
4716 tags from 24 diverse categories. The average number
of tags per video is 3.4.

The YouTube-8M video understanding challenge is an

1visit https://github.com/wangheda/youtube-8m for the source code.

arena for video classiﬁcation researchers and practitioners.
In the competition, the dataset is divided into three parts.
The training set contains 4.9 million samples. The validate
set contains 1.4 million samples. The test set contains 0.7
million samples. The ground truth labels annotated to the
samples in the training set and the validate set are available
to the participants. The test set is divided into two half,
the open test set and the blind one. In the progress of the
competition, participants can submit their predictions to the
test set. The scoring server on Kaggle would evaluate them
and return the scores on the open test set (the public leader-
board). After the competition, the winners are decided by
the scores of their submissions on the blind test set (the pri-
vate leaderboard).

In the competition, submissions are evaluated using
Global Average Precision (GAP) at 20. The metrics is cal-
culated as follows. For each video, the most conﬁdent 20
label predictions are selected along with the conﬁdence val-
ues. The tuples of the form {video, label, conf idence}
from all the videos are then put into a long list sorted by
conﬁdence values. This list of predictions are then evalu-
ated with the Average Precision (Eq. 1), in which p(i) is the
precision and r(i) is the recall given the ﬁrst i predictions.

AP =

p(i)∆r(i)

(1)

N
(cid:88)

i=1

We divide the dataset into ﬁve parts: train1, validate1,
train2, validate2, and test. The division is done base on ﬁle
name pattern (Table 1). The set train1 is used for single
model training, in which the set validate1 served as a hold-
out test set for early-stopping. Ensemble models are trained
on the set train2, which have no intersection with the train-
ing set of single models. The set validate2 is a hold-out
test set for early-stopping in the training of ensemble mod-
els. The inference procedure uses the test set to generate
submissions. The whole system structure is shown in Fig.

1

Figure 1: The structure of our system.

Part
train1
validate1
train2
validate2
test

File glob
train??.tfrecord
validate[a]?.tfrecord
validate[ˆa0-9]?.tfrecord
validate[0-9]?.tfrecord
test??.tfrecord

#samples
4,906,660
21,918
1,270,412
109,498
700,640

Table 1: Dataset division scheme in our system.

1. This data division strategy may not be optimal, since it
limit the amount of data used in single model training to the
4.9 millions training set examples, which may considerably
affect the performance of single models.

During the training of all our models, We did not use
any data augmentation techniques. Adam [10] optimization
algorithm is used throughout the training process. For mod-
els that use frame level feature rgb, audio, we use a learn-
ing rate of 0.001 and a batch size of 128. For models that
use video level feature mean rgb, mean audio, the default
learning rate and batch size is 0.01 and 1024. Note that
these hyper parameters may not be optimal since we did not
perform any parameter search. Every of our models can ﬁt
in the graphics memory of either an Nvidia GTX 1080 (8G)
or an GTX 1080Ti (11G).

In the rest of this report, we summarize our contribu-
tions in our solution. We ﬁrst introduce our improvements
over the baseline Long-Short Term Memory (LSTM) model
(Section 2). We then introduce a deep network structure
that we use in many models to capture label correlations
(Section 3). Then, we introduce our best performing sin-
gle model that utilize multiscale information via Convolu-
tional Neural Network (CNN) and LSTM (Section 4). Also,
we explore the use of attention pooling in video sequence
classiﬁcation (Section 5). Ensemble methods such as bag-
ging, boosting (Section 6), cascade (Section 7), and stack-
ing (Section 8) are also explored in this report. We also
found that using the predictions of ensemble models as a

soft target can greatly boost single model performance (Sec-
tion 9). In the end, we summarize our road-map and discuss
the contribution of our solution(Section 10).

2. Baseline models

In this section, we list the performances of some of the
baseline models in the technical report [1] and some alter-
native architectures evaluated in our system. The perfor-
mances reported here are different from the ones reported
in the technical report, since the dataset used in the report is
a little larger (8.3 million examples) than the one provided
in this competition. Also, we have limited the amount of
data used for single model training to 4.9 million examples.
The GAP scores we reported in this paper are evaluated with
the ”validate2” set if not otherwise speciﬁed.

We re-implemented four baseline approaches: Logistic
Regression (LR), Mixture-of-Expert (MoE), Deep Bag-of-
Frames (DBoF), and Long-Short Term Memory (LSTM).
The GAP scores are listed in Table 2. We choose a mixture
of 16 for the MoE model for the best performance. For the
LSTM model, we choose the number of layers, the number
of cells per layer, and the number of Moe mixtures to be 2,
1024 and 8 respectively.

Input Feature
Video Level, µ
Video Level, µ
Frame Level, {xv
Frame Level, {xv

Model
Logistic Regression
Mixture-of-Experts

} Deep Bag-of-Frames
} LSTM

1:Fv

1:Fv

GAP
0.7577
0.7957
0.7845
0.8131

Table 2: The performance of baseline approaches in our
system.

In the conventional LSTM [7], the memory cells that
share the same input gate and the same output gate form
a memory block. Putting more than one cells into a block
makes the training of LSTM more efﬁcient. We create an ar-

2

chitecture in which all the cells share a single input gate and
a single forget gate and call it ”LSTM-S”. In this architec-
ture, the cells use individual output gates, since we ﬁnd that
sharing the output gate can be harmful to the performance.
Another network structure we create by adding an input
accumulator to the architecture of LSTM model is what we
call the ”LSTM-A”. In this structure, we add a new set of
memory, input gate and forget gate to the cell to directly
”remember” the input. Eq. 2 gives the architecture of this
model. In the equation, xt, ct, ht are the input, the memory,
and the hidden state, ot, it, ft are the output gate, the input
gate, and the forget gate, c(cid:48)
t, dt are the added memory and
the hidden state for input accumulation, i(cid:48)
t, f (cid:48)
t are the added
input gate and forget gate, σ is the sigmoid function, g is
the activation function which we choose to be tanh, and n
is the L2-normalization function.

content or audio content. Therefore, the visual or audio ac-
tivities do not have to happen spontaneously to be meaning-
ful. We adopt a parallel way that model visual and audio
content with separate LSTM models. The ﬁnal states of the
two LSTM models are then concatenated and go through an
MoE unit. The performance of the parallel model is shown
in Table 4. The number of layers is 2. The number of cells
c and the number of mixtures m in MoE model are shown
in the table.

Model
vanilla LSTM, c = 1024, m = 8
parallel LSTM, cv = 1024, ca = 128, m = 8

GAP
0.8131
0.8161

Table 4: Parallel modeling of visual and audio content is
better than simple feature concatenation.





















ot
mt
it
ft
i(cid:48)
t
f (cid:48)
t
ct
ht
c(cid:48)
t
dt

= TN +2M,6N





ht−1
xt
dt−1





= σ(ft) · ct−1 + σ(it) · g(mt)
= σ(ot) · g(ct)
t) · c(cid:48)
= σ(f (cid:48)
= n(c(cid:48)
t)

t−1 + σ(i(cid:48)

t) · xt

The performance of the two modiﬁed LSTM models are
listed in Table 3. The number of cells per layer are 1024.
The number of mixtures in MoE is set to 8. The number of
layers l is shown in the table. The two models performs bet-
ter than the original model and contribute to the ensemble
because their varieties in model structure.

Model
vanilla LSTM, l = 1
vanilla LSTM, l = 2
LSTM-S, l = 1
LSTM-A, l = 1

GAP
0.8091
0.8131
0.8123
0.8131

Table 3: The performances of the single-block LSTM
(LSTM-S) and the input-accumulator LSTM (LSTM-A),
compared to the original LSTM model.

In the dataset, an example has both visual feature and
audio feature. The naive way of modeling two different
features is to concatenate them into a single feature. We
ﬁnd this to be questionable for two reasons. First, in many
videos such as music videos and family albums, the audio
and the visual content are independent of each other. Also,
people often can make sense of a video only by its visual

3

There is a potential weakness in modeling visual and au-
dio features independently. It might be preferable to allow
the visual network and the audio network to interact at cer-
tain points, since a visual / audio event may be related to a
former audio / visual event in the same video. We leave this
issue to future works.

(2)

3. Label Correlation

In multi-label classiﬁcation settings, an example may be
annotated many labels. Some labels tend to appear in the
same example at the same time, some tend not to. Such in-
formation can be used to improve the performance of multi-
label classiﬁcation models.

Classiﬁer chain [12] create a chain of classiﬁers. One
classiﬁer on the chain predict one label by using not only the
input feature but also the predictions of other labels from the
previous models on the chain. The training of one chain of
classiﬁer involves training L models in which L is the size
of the vocabulary of all labels. However, in real world prob-
lems where the direction of dependency is unknown, an en-
semble of classiﬁer chains is usually used, which makes the
computation complexity even higher and intractable if the
vocabulary of labels is large. A neural network structure
that mimic the classiﬁer chain [11] is proposed to address
the multi-label problem. However, such network structure
has a depth of L that makes it hard to optimize if the vocab-
ulary of labels is large.

We propose a novel end-to-end deep learning structure
that we called ”Chaining” to better utilize the correlation be-
tween labels. In a Chaining model (Figure 2), several repre-
sentations are joined by a chain of MoE model. The predic-
tions are projected to features of lower dimension and used
in the following stages. The representations can be gener-
ated by homogeneous models or heterogeneous ones. We
constantly apply auxiliary cross-entropy loss on the inter-

(a) A unit in Chaining accept one fea-
ture and several model predictions as the
input. The predictions are projected to
lower dimension for efﬁciency.

(b) The whole architecture of Chaining consists of several stages. A Chaining unit
in one stage accepts a feature vector, either from input directly or from a represen-
tation of LSTM or CNN, and all the predictions from earlier stages.

Figure 2: Chaining: a deep learning architecture for multi-label classiﬁcation.

mediate predictions to accelerate the training progress. The
ﬁnal loss function is a weighted average over the loss on
the ﬁnal prediction and the auxiliary losses. We typically
allocate only 10% ∼ 20% of the weights to the auxiliary
losses, since the loss function at the ﬁnal stage is the most
important.

We performed experiments using three basic models,
Mixture-of-Expert, LSTM and CNN. The CNN model we
use in our system is the same as the benchmark method of
sentence classiﬁcation [9], where the length of the ﬁlter is
ﬁxed to the size of feature vector per frame and the resulting
feature map goes through a max-over-time pooling. The ﬁ-
nal state of LSTM and the max-pooled feature map are used
as feature representation in Chaining models and go through
an MoE model for label prediction in the original models.
The performance with and without using Chaining is shown
in Table 5. For the original MoE model, the number of mix-
tures is 16. For the Chaining MoE model, the number of
mixture is 2, the number of stages is 8, and the predictions
are projected into a 128 dimensions vector. For the original
LSTM model, the number of mixtures is 8. For the Chain-
ing LSTM model, the number of mixtures is 4, the number
of stages is 2, and the dimension of projection is 200. For
the original CNN model, the width of ﬁlter and the corre-
sponding numbers of channel are 1×512, 2×512, 3×1024.
For the Chaining CNN model, the corresponding parame-
ters are 1 × 128, 2 × 128, 3 × 256, and the number of stages
is 4. The parameters are chosen to make the original models
have almost equal number of parameters with their Chain-
ing counterparts.

4. Temporal Multi-Scale Information

Input Feature
Video-level, µ
Frame Level, {xv
Frame Level, {xv

1:Fv

1:Fv

Model Original Chaining
0.8106
MoE
0.8172
0.8179

0.7965
} LSTM 0.8131
} CNN
0.7904

Table 5: The performance (GAP) of Mixture-of-Experts,
LSTM and CNN models with and without using Chaining
structure.

age would change with its distance to the observer. How-
ever, temporal information does not have the same rescaling
effect. Therefore it may seems unnatural to model videos
from different temporal scales.

However, we argue that temporal scales matters in video
analysis. A task can be divided into several actions, and
each action involves the interaction between certain objects.
The label annotated to a video can be related to a concept at
different temporal scale: an object, an action, or a task.

One important observation is that video can be seg-
mented into several clips. Each of these clips may contain
content information of a certain aspect. We can make pre-
dictions based on each of these clips and then join them
together to make more accurate predictions. The segmenta-
tion of video is done by either clustering the adjacent frames
[14] or splitting the video into clips of equal durations [15].
We propose a temporal-segment LSTM model (Fig. 3a),
in which the video is split into equal-sized clips, each of
which is modeled by an LSTM model. The models for dif-
ferent clips share the same parameters. The ﬁnal state of
each sequence are treated as another high-level sequence
and modeled by another LSTM model.

Using information from different spatial scales have
been discussed in many image analysis literatures. The
main reason to do so is that the size of an object in an im-

Temporal-pooling LSTM model (Fig. 3a) is a multi-
layer LSTM model in which we inserted temporal k-pooling
layer between LSTM layers. It is similar to the temporal-

4

ition that the representation of low-level sequences contain
more reﬁned features, while the high-level representation is
close to an averaged view of the whole video.

We propose a novel temporal CNN-LSTM model (Fig.
3c) that utilize multi-scale information. This model shares
the previous intuition that adjacent frames can be aggre-
gated to generate features at different temporal scale. In-
stead of direct operating on the original feature, we use
convolution layers to detect patterns in the low-level fea-
tures and combine adjacent ﬁlter outputs by max-pooling
along the time dimension. The ﬁlters used in a convolution
layer are of the same length with the dimension of the fea-
ture, and their widths and channels can be varied. We use
different LSTM models and Moe classiﬁers for the repre-
sentation of the feature maps of different temporal scales.
The predictions generated from features of different scales
are combine using a consensus function. We ﬁnd that av-
eraging is a good consensus function, and maximum would
lead to difﬁculties in convergence.

GAP
Model
0.8131
vanilla LSTM
temporal-pooling LSTM 0.8085
temporal-segment LSTM 0.8122
multi-resolution LSTM
0.8148
multi-scale CNN-LSTM 0.8204

Table 6: Performance of multi-scale models.

Table 6 lists the performances of the models discussed
in this section. The temporal-pooling LSTM model uses 2-
pooling and has 4 LSTM layers (#cells=1024) and 4 MoE
mixtures. The temporal-segment LSTM model uses a dura-
tion of 10 frames for the clips and has 8 MoE mixtures and
2 LSTM layers, each of the layers has 1024 memory unit.
In the multi-resolution LSTM model, we use 2-pooling to
get shorter input sequences; the number of stages in Chain-
ing is 4; the number of mixtures in MoE is 4; the projection
dimension is 256; the LSTM models in it are 2-layer par-
allel LSTM (#cells is 512 for video and 64 for audio). In
the multi-scale CNN-LSTM model, the number of layers in
CNN model is 4; the number of mixtures in MoE is 4; the
LSTM models in it are 1-layer LSTM (#cells=1024); in ev-
ery layer of the CNN model, the width of ﬁlters and the cor-
responding number of channels are 1×256, 2×256, 3×512;
the pooling layers in it are 2-pooling.

5. Identifying Salient Frames with Attention

Not all the frames in a video are equally informative.
There are many reasons that a frame might contribute little
in video classiﬁcation. Images that are too dark, too bright,
or too blurry are hard to analysis for the image classiﬁcation

(a) Temporal-pooling (with the dashes) and temporal-segment
(without the dashes) LSTM model.

(b) Multi-resolution LSTM model.

(c) Multi-scale CNN-LSTM model.

Figure 3: Models that utilize temporal multi-scale informa-
tion.

segment LSTM model. The difference is whether to use the
ﬁnal state of the one clip as the initial state of the next clip
in the ﬁrst layer of the LSTM model.

Instead of directly segmenting videos into clips, we
can aggregate the adjacent frames and gradually construct
features containing long-range information.
In multi-
resolution LSTM model (Fig. 3b), the original features
are average pooled along the time dimension to get shorter
sequences in which each frame covers longer time range.
For each sequence generated in this way, a separate LSTM
model is used to generate a sequence representation. The
representations are then joined with a Chaining model.
The representation from the highest-level goes into the ﬁrst
stage, while the one from the original sequence goes into the
ﬁnal stage of the Chaining model. This is due to the intu-

5

(b) An attention network generates multiple attention
weights for a frame based on the input feature and the
LSTM output at that frame.

(a) The multiple attention pooling scheme. The outputs
of LSTM are pooled with multiple attention weights.

Figure 4: Multiple attention pooling model.

network. Therefore, such frames may not be able to provide
useful semantic information. Also, sometimes a video con-
tains title screens, credits or text. They might be useful if
the frames are processed using optical character recognition
(OCR). However, in this competition, the frames are pre-
processed using an image classiﬁcation network pre-trained
with ImageNet dataset, which means the frame-level fea-
tures may not contain the semantic information in these text.
In addition, there are always frames irrelevant to the theme
of the video. In talk show videos the content of frames will
always be people talking while the topic of the talking is
what really important. In documentaries there are often a
lot of driving scenes which do not reﬂect the theme of the
video.

We propose a multiple attention pooling scheme (multi-
AP) for selecting salient frames in the video. As Fig. 4
shows, an LSTM model is used to deal with the frame-level
features. The outputs of the LSTM model are then aggre-
gated by pooling over the time dimension. The aggregation
is achieved by taking a weighted average over the outputs
of the LSTM model, in which the weights are generated
using an attention network, for which we use a fully con-
nected layer. The aggregated feature is fed into an MoE
model for label predictions. The attention pooling is re-
peated for multiple times and the predictions are combined
using a classiﬁer consensus function. For the choice of com-
bining method, we ﬁnd that maximum performs better than
averaging as a consensus function.

the input and output of the LSTM at a certain frame as the
input. The attention network output K groups of different
attention weights.

Each group of the weights are used to generate a group of
aggregated features and prediction results. The results from
all the groups are then combined by choosing the highest
conﬁdence predicted from all the models for each label l.
The MoE models in Fig. 4a share parameters with each
other.

eik = Wk[xi; yi]
aik = exp(eik)
(cid:80)Fv

i=1 exp(eik)

zk = (cid:80)Fv
i=1 aikyi
pk = M oE(zk)
(pk,l)
pl = max

k

(3)

(4)

We present a few examples outside of the training set to
show what kind of frames is highlighted by the attention
network in the multi-AP model (Fig. 5). For each video,
the frames with the lowest attention weights (left) and the
ones with the highest attention weights (right) are presented.
Since there are many groups of attention weight, the ﬁrst
group is used for this presentation. We have not observed
any noticeable patterns in the inter-group differences of the
weights.

We use a fully connected layer with the softmax acti-
vation function as the attention network (Eq. 3). It takes

In analysis of these examples, we ﬁnd that there are some
patterns about the attention weights. Title screens and the

6

Figure 5: Visualization of the attention weights in the multiple attention pooling model. The frames in each row comes from
the same video. The left four in each row are the frames with the lowest weights, while the right four are the ones with the
highest weights.

frames that is very dark tend to have low attention weights.
Also, if the object of interest is too small or partly blocked,
the weight of that frame tend to be low.

Our work is different from a previous work on attention
pooling [5] in two ways. First, our model uses multiple
groups of attention on the same output sequence and multi-
ple classiﬁers for prediction. The multiple attention scheme
may contribute to generating more stable classiﬁcation re-
sult. Second, our model do an attention pooling over the
outputs of LSTM model which is a global representation
of the sequence while the previous work do a pooling over
local CNN outputs.

We also adopt a local attention pooling scheme that ap-
plies attention pooling over the input features (Eq. 5). The
representation of the LSTM model is also used in the MoE
model for prediction.

The LSTM model in the four models share the same pa-
rameter (#layers=2, #cells=1024). The MoE models have 8
mixtures in the two multi-AP models, while the one in the
local-AP model has 4 mixtures. The two multi-AP models
both have 8 groups of attention weights. And the dimen-
sion of the positional embedding in the positional multi-AP
model is 32.

GAP
Model
0.8131
vanilla LSTM
0.8133
local-AP LSTM
multi-AP LSTM
0.8157
positional multi-AP LSTM 0.8169

Table 7: Performance (GAP) of attention models.

z = (cid:80)Fv

i=1 aixi

p = M oE([yFv ; z])

(5)

6. Bagging and Boosting

We compare the two attention pooling scheme with the
baseline in Table 7. The ”local-AP LSTM” refers to the
scheme using attention pooling over the input feature. The
”multi-AP LSTM” refers to the multiple attention pooling
scheme. The ”positional multi-AP LSTM” model add an
embedding for every frame position to the attention network
on the basis of the multi-AP LSTM model.

The parameters for the models in Table 7 is as follows.

Bootstrap aggregating [2], also called Bagging, is an en-
semble method that creates many versions of a model and
combines their results together. To create one version of a
model, one applies sampling with replacement to get a sub-
set of the original data, in which some original examples
may not present or present more than once[16]. Training on
different subset sampled from the original data would re-
sults in different models. The bagging algorithm is known
for its ability to reduce the variance of a model. We apply

7

Bagging to some of our models, and ﬁnd that Bagging can
generally boost the GAP performance by 0.6% ∼ 1.2%.
The results are shown in Table 8.

weights to 5 to ensure that the algorithm would not place all
the weights on a few formerly misclassiﬁed examples, since
they may not be visually classiﬁable.

Original Bagging
Input Feature Model
0.8225
0.8106
Video-level
0.8216
0.8160
Frame Level
0.8258
Frame Level
0.8179
0.8244
Frame Level multi-AP LSTM 0.8157

Chaining
parallel LSTM
Chaining CNN

Model
Chaining (Video)
parallel LSTM
Chaining CNN
multi-AP LSTM

Original Boosting
0.8218
0.8106
0.8218
0.8160
0.8242
0.8179
0.8246
0.8157

Table 8: Performance (GAP) of bagging models.

Table 9: Performance (GAP) of boosting models.

Boosting [3] is another way to create different versions
of a model. Compared to Bagging which can be run in
parallel, Boosting is a sequential ensemble method. The
(k + 1)th classiﬁer is constructed considering the previous
k classiﬁers by re-sampling a distribution that highlights the
misclassiﬁed examples.

The re-sampling is often implemented by using weighted
examples. If there are N training examples and L labels,
the number of total classiﬁcation results is N × L. Most
works on Boosting in multi-label classiﬁcation use a weight
W N ×L over N samples and L labels, which is computa-
tionally intractable when the vocabulary of label is large, as
is the case in this competition.

We adopt a per-example weighting scheme that assign
a weight W N over the training samples. The weights are
updated with Eq. 6. In the equations, Wk,n is the weight
assigned to the nth example in the training of the kth clas-
siﬁer. Errk,n is the error rate of the nth example by eval-
uating the kth classiﬁer. Errk is the average error rate of
the kth classiﬁer. And Zk is a coefﬁcient that scales the av-
erage value of Wk,n to 1. α is a parameter controlling the
highlighting effect on the misclassiﬁed examples which we
constantly set to 1 in our system.

Wk,n exp(αrkErrk,n)

W0,n = 1.0
Wk+1,n = N
Zk
in which,
rk = log( 1.0−Errk
Errk = 1
N
Errk,n ∈ [0, 1]
Zk = (cid:80)N

Errk
(cid:80)N

)

n=1 Errk,n

n=1 Wk,n exp(αrkErrk,n)

In this algorithm, the weights of the misclassiﬁed ex-
amples are increased in the following classiﬁers. However,
in multi-label classiﬁcation, misclassiﬁcation is hard to de-
ﬁne. We choose the Precision Equal Recall Rate (PERR)
as the implementation of error rate, since it is both per-
example evaluated and in coordinate with the GAP score in
most models (from empirical observation). We also clip the

The comparison between the single models and their
Boosting counterparts are shown in Table 9. The Boosting
algorithm generates a performance boost similar as Bagging
does.

7. Cascade Classiﬁer Training

Adding models into the ensemble would usually make
the performance better. However, with the number of mod-
els in the ensemble model increasing, the gain from newly
added models tends to diminish, especially if one add mod-
els that are similar to the existing models. We address this
problem by using cascade training in which the predictions
of other models as a part of its input of the model during
training.

(6)

Figure 6: Cascade layer as a replacement for MoE in cas-
cade classiﬁer training.

The structure of cascade layer is shown in Fig. 6. The
predictions from the other models are averaged to generate
an averaged prediction, which is then projected to a feature
of low dimension. All the models that have MoE as their
last layer can be modiﬁed into a cascade model by having
their MoE layer replaced by the cascade layer.

8. Stacking Methods

Stacking is an ensemble method that uses a machine
learning model to combine the predictions of many mod-
els into one prediction. We use the ”train2” set (Section 1)

8

as the training set for stacking. The trivial case of stacking
is to simply average all the predictions. If there are M mod-
els and L labels in total, simple averaging the predictions of
all models can be written as Eq. 7.

pl =

1
M

M
(cid:88)

m=1

pm,l

(7)

Linear weighted averaging (Eq.

8) is also a simple
scheme. This method has M weights, one for each model
in the ensemble.

M
(cid:88)

m=1

M
(cid:88)

m=1

pl =

wmpm,l, s.t.

wm = 1, wm > 0

(8)

1
M

M
(cid:88)

m=1

For multi-label classiﬁcation, there are L predictions for
every example. Some models might perform better than the
others on a subset of the labels. Therefore, we could extend
linear weighted averaging to every class. We denote this
method class-wise weighted averaging (Eq. 9).

1
M

M
(cid:88)

m=1

pl =

wm,lpm,l, s.t.

wm,l = 1, wm,l > 0

(9)
While the ensemble methods often deal with the predic-
tions of individual models, the original input may also help
to decide which models to trust. The winning team of Net-
ﬂix Prize proposes the Feature-Weighted Linear Stacking
[13] to utilize the information in meta-features. Although
there are no useful meta-feature in the Youtube-8M dataset,
we consider the averaged frame input and the averaged pre-
dictions from individual models as useful indicators. We
propose the attention weighted stacking to utilized these in-
formation.

The attention weights α (Eq. 10) are generated with a
fully connected layer with softmax as the activation func-
tion and averaged input ¯x and prediction ¯p as the input of
the layer.

The predictions are then weighted averaged as shown in
Eq. 11. The weight is a mixture of K low-rank matrices
(rank D) in which the matrices are selected by the output
of the attention network (Eq. 10). In the attention weighted
stacking, V, A, B, a, b, c are trainable parameters.

We perform a comparison among the stacking methods
on an ensemble of 74 models (model #1 - #74 in Appendix
A). The number of attention weights is 16 and the rank
of the matrix components is 4 in the attention weighted
stacking method. The comparison shows that the attention
weighted stacking performs better than the other methods
(Table 10).

Stacking method
Simple Averaging
Linear Weighted Averaging
Class-wise Weighted Averaging
Attention Weighted Stacking

GAP
0.8436
0.8449
0.8453
0.8458

Table 10: Performance (GAP) of different stacking models
on the entire set of models (74 models). The GAP scores
in this table is evaluated with the blind test set (the private
leaderboard score in the competition).

9. Distilling Knowledge from Ensemble

Ensemble models might perform much better than even
the best single model. However, in real world systems, the
memory and computation resources are often too limited
for ensemble models to deploy. Distillation [6] is a training
scheme that aims to compress the knowledge in ensemble
model to a single model. Another motivation of using dis-
tillation is the observation that the labels are often wrong
or incomplete, especially when the vocabulary of label is
large.

We adopt a training scheme that uses the predictions of
ensemble ˆp as a soft target along with the real target l during
training. The new loss function is a weighted average of two
losses, one for each target (Eq. 12).

α = σ(V [¯x; ¯p])

¯x = 1
Fv

(cid:80)Fv

i=1 xi

¯p = 1
M

(cid:80)M

m=1 pm

pl

= 1
M

(cid:80)M

m=1 wm,lpm,l

wm,l =

(cid:80)M

e

= (cid:80)K

exp(em,l)
m=1 exp(em,l)
k=1 αk(AT

s.t.

Ak ∈ RD×M , Bk ∈ RD×L
ak ∈ RM ×1, bk ∈ RL×1
ck ∈ R

(10)

ˆL = (1 − λ)ce(p, l) + λce(p, ˆp)

(12)

We use the predictions of model #75 (Appendix A.9) as
the soft target in training and train several models with the
same parameters as the original models. The comparison is
listed in Table 11. The comparison shows that distillation
can greatly boost single model performance (often better
than Bagging).

In the previous sections, we review our system, present
our intuition, and show how we address the problems in

9

k Bk + akbT

k + ck)

(11)

10. Summary

Original Distillation
Model
0.8106
Chaining (Video)
0.8160
parallel LSTM
0.8179
Chaining CNN
0.8172
Chaining LSTM
Multi-scale CNN-LSTM 0.8204

0.8169
0.8237
0.8266
0.8291
0.8258

Table 11: Performance (GAP) of single models trained us-
ing distillation.

multi-label video classiﬁcation. We ﬁnd that attention pool-
ing and temporal multi-scale information is very important
for video sequence modeling. We also propose a network
structure for large-vocabulary multi-label classiﬁcation. We
review our work in ensemble methods such as Bagging,
Boosting, Cascade, Distillation and Stacking. We propose a
stacking network that uses attention to weight models. Our
system road-map is shown in Table 12.

Changes
27 single models
+ 11 bagging & boosting models
+ 8 distillation models
+ 11 cascade models
+ 17 more cascade models
Attention Weighted Stacking

GAP
0.8425
0.8435
0.8437
0.8451
0.8453
0.8458

Table 12: Performance (GAP) of ensemble models. The
stacking method used for the ﬁrst 5 results is class-wise
weighted model. The ensemble in each row include all the
changes in the rows above it. The results reported in this
table are evaluated on the blind test set (the private leader-
board score in the competition).

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m:
CoRR,
A large-scale video classiﬁcation benchmark.
abs/1609.08675, 2016.

[2] L. Breiman.

Bagging predictors. Machine Learning,

24(2):123–140, 1996.

[3] L. Breiman. Arcing classiﬁer (with discussion and a rejoin-
der by the author). Ann. Statist., 26(3):801–849, 06 1998.
[4] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 961–970, 2015.

[5] M. J. Er, Y. Zhang, N. Wang, and M. Pratama. Atten-
tion pooling-based convolutional neural network for sen-
tence modelling. Information Sciences, 373:388 – 403, 2016.
[6] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, pages
1725–1732, 2014.

[9] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014.

[11] J. Read and J. Hollm´en. A deep interpretation of classi-
ﬁer chains. In International Symposium on Intelligent Data
Analysis, pages 251–262. Springer, 2014.

[12] J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classiﬁer
chains for multi-label classiﬁcation. Machine Learning and
Knowledge Discovery in Databases, pages 254–269, 2009.

[13] J. Sill, G. Tak´acs, L. Mackey, and D. Lin. Feature-weighted
linear stacking. arXiv preprint arXiv:0911.0460, 2009.
[14] E. H. Spriggs, F. De La Torre, and M. Hebert. Temporal seg-
mentation and activity classiﬁcation from ﬁrst-person sens-
ing. In Computer Vision and Pattern Recognition Workshops,
2009. CVPR Workshops 2009. IEEE Computer Society Con-
ference On, pages 17–24. IEEE, 2009.

[15] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
towards good
L. Van Gool. Temporal segment networks:
practices for deep action recognition. In European Confer-
ence on Computer Vision, pages 20–36. Springer, 2016.

[16] Z.-H. Zhou. Ensemble methods:

foundations and algo-

rithms. CRC press, 2012.

10

Appendix A. Final Submission

In this section we list all the components included in our
ﬁnal submission. We apply a stacking method called the At-
tention Weighted Average (Section 8) to merge the result of
these models instead of carrying on careful model selection.
Therefore it might be possible that a subset of these models
could reach equal or better performance. The parameters
are for a quick grasp of the structure of the models. Re-
fer to our code 2 if you are interested in the implementation
details. The GAP scores reported in the appendix are eval-
uated with the ”validate2” set if not otherwise speciﬁed.

A.1. Video-level Models

1. Chaining model (Section 3). During training, if the
loss of the current batch is less than 10, use the most conﬁ-
dent tag predicted as a soft target for every example in the
batch. #mixture of MoE is 4. #stage of Chaining is 3. pro-
jection dimension in Chaining is 100. GAP = 0.8067.

2. Chaining model. #mixture of MoE is 2. #stage of
Chaining is 8. projection dimension in Chaining is 128.
GAP = 0.8106.

A.2. Baseline Models

If not otherwise speciﬁed, the memory cells of the ﬁnal
state are used for the input of the MoE model in the baseline
models.

3. LSTM model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8131.
4. LSTM model. #cell of LSTM is 2048. #layer of

LSTM is 2. #mixture of MoE is 4. GAP = 0.8152.

5. LSTM model of 2 layers. The ﬁrst layer is bi-
directional. The second layer is uni-directional. #cell of
LSTM is 1024,. #mixture of MoE is 4. GAP = 0.8105.

6. LSTM-S model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8123.
7. LSTM-A model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.
8. LSTM-A model of 2 layers. The ﬁrst layer is a for-
ward LSTM-A model. The second layer is a backward
LSTM-A model that takes the original input and the out-
put of the ﬁrst layer as input. #cell of LSTM is 1024. #layer
of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.

9. Parallel LSTM model (Section 2). #cell of LSTM is
1024. #layer of LSTM is 2. #mixture of MoE is 8. GAP =
0.8161.

10. Parallel LSTM model. The gated outputs of the ﬁ-
nal state are used for the input of the MoE model. #cell of
LSTM is 1024. #layer of LSTM is 2. #mixture of MoE is
8. GAP = 0.8160.

11. LSTM model with data augmentation by random
sampling 50% of the frames. #cell of LSTM is 1024. #layer

2https://github.com/wangheda/youtube-8m

of LSTM is 2. #mixture of MoE is 8. GAP = 0.8137.

12. CNN-LSTM model. The CNN model is described
in section 3. #layer of CNN is 1. The widths of ﬁlter are
1, 2 and 3. The corresponding #channels of ﬁlters are 1024,
1024, and 1024. #cell of LSTM is 1024. #layer of LSTM is
2. #mixture of MoE is 8. GAP = 0.8103.

A.3. Temporal Multi-Scale Models

13. Temporal-segment LSTM model (Section 4). The
duration of each clip is 10 frames. #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8122.
14. Temporal-pooling LSTM model (Section 4). The
model has 2-pooling between LSTM layers. #cell of LSTM
is 1024. #layer of LSTM is 4, #mixture of MoE is 8. GAP
= 0.8085.

15. Multi-resolution LSTM model (Section 4). The
model has parallel LSTM for sequence modeling. The
model has 2-pooling to get shorter representation of input.
#cell of LSTM is 512 for video and 64 for audio. #layer
of LSTM is 2. #stage of Chaining is 4. The dimension of
projection in Chaining is 256. #mixture of MoE is 4. GAP
= 0.8149.

16. Multi-scale CNN-LSTM model (Section 4). The
widths of ﬁlter are 1, 2 and 3. The corresponding #chan-
nels of ﬁlters are 256, 256, and 512. #layer in CNN is 4.
#layer of LSTM is 1. #cell of LSTM is 1024. #mixture of
MoE is 4. GAP = 0.8204.

17. Multi-scale CNN-LSTM model. All the parameters
are the same as model #16, except that the type of the LSTM
model is LSTM-S (as in model #6). GAP = 0.8147.

A.4. Chaining Models

18. Chaining CNN model (Section 3). The widths of ﬁl-
ter are 1, 2 and 3. The corresponding #channels of ﬁlters are
128, 128, and 256. The dimension of projection in Chaining
is 256. #mixture of MoE is 4. #stage of Chaining is 4. GAP
= 0.8179.

19. Chaining Deep CNN model. In this model a 3-layer
CNN model is used. The averaged input feature and the
3 max-pooled feature maps from the 3 layers of CNN are
combined using a 4-stage Chaining model. The widths of
ﬁlter are 1, 2 and 3. The corresponding #channels of ﬁl-
ters are 128, 128, and 256. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. GAP = 0.8155.

20. Chaining LSTM-CNN model. This model uses a
cascade of parallel LSTM and CNN model as the sub-model
in Chaining.#layer of LSTM is 1. #cell of LSTM is 1024 for
video and 128 for audio. The widths of ﬁlter are 1, 2 and
3. The corresponding #channels of ﬁlters are 128, 128, and
256. The dimension of projection in Chaining is 128. #stage
in Chaining is 3. #mixture of MoE is 4. GAP = 0.8122.

21. Chaining LSTM model (Section 3). #layer of LSTM
is 2. #cell of LSTM is 1024. The dimension of projection in

11

Chaining is 200. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8172.

22. Chaining LSTM model. The different stages in
Chaining model uses a shared input which is the cell mem-
ory of the ﬁnal state of an LSTM model. #layer of LSTM is
2. #cell of LSTM is 1024. The dimension of projection in
Chaining is 256. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8162.

A.5. Attention Pooling Models

23. Local attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. GAP =
0.8133.

24. Attention pooling LSTM model that has one LSTM
model to generate the attention weights for pooling over the
output of another LSTM model. #layer of LSTM is 1. #cell
of LSTM is 1024. #mixture of MoE is 8. GAP = 0.8088.

25. Multiple attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. #mixture of
MoE is 4. #group of attention weights is 8. GAP = 0.8157.
26. Multiple attention pooling LSTM model that has one
LSTM model to generate the attention weights for pooling
over the output of another LSTM model. #layer of LSTM
is 2. #cell of LSTM is 1024. #mixture of MoE is 4. #group
of attention weights is 8. GAP = 0.8081.

27. Multiple attention pooling LSTM model with posi-
tional embedding (Section 5). #layer of LSTM is 2. #cell
of LSTM is 1024. #mixture of MoE is 4. The dimension of
positional embedding is 32. #group of attention weights is
8. GAP = 0.8169.

A.6. Bagging and Boosting Models

GAP = 0.8218.

33. Bagging model of 8 versions of model #18, com-
bined with class-wise weighted averaging. GAP = 0.8258.
34. Boosting model of 8 versions of model #18, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8246.

35. Bagging model of 8 versions of model #25, com-
bined with class-wise weighted averaging. GAP = 0.8244.
36. Boosting model of 8 versions of model #25, com-
bined with attention weighted stacking, with weight clip-
ping. GAP = 0.8242.

37. Bagging model of 8 versions of model #10, com-

bined with simple averaging. GAP = 0.8216.

38. Boosting model of 8 versions of model #10, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8218.

A.7. Distillation Models

The models in this section is trained using distillation
(Section 9). During training, the predictions from model
#75 are used as soft targets.

39. Model #2 with distillation training. GAP = 0.8169.
40. Model #18 with distillation training. GAP = 0.8266.
41. Model #20 with distillation training. GAP = 0.8259.
42. Model #10 with distillation training. GAP = 0.8237.
43. Model #21 with distillation training. GAP = 0.8291.
44. Model #16 with distillation training. GAP = 0.8258.
45. Bagging model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8249.

46. Boosting model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8254.

28. Bagging model of 8 versions of model #2, combined

with simple averaging. GAP = 0.8225.

A.8. Cascade Models

29. Ensemble of 4 Chaining models. The ﬁrst is the
normal Chaining model with video-level input. The second
is a Chaining model with weighted cross entropy loss. The
third is a Chaining model that predict label and top-level
verticals at the same time. The fourth is a Chaining model
that predict the infrequent labels with softmax function. All
4 models share the same parameters. #mixture of MoE is
4. #stage in Chaining is 3. The dimension of projection in
Chaining is 100. GAP = 0.8216.

30. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging. Remove the ex-
amples with too high weights from training set, since it may
be hopeless to predict these examples correctly. GAP =
0.8213.

31. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging, without weight
clipping. GAP = 0.8198.

32. Boosting model of 8 versions of model #2, combined
with class-wise weighted averaging, with weight clipping.

The models in this section are models using the predic-

tion of other models as part of the input (Section 7).

47. Model #2 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8231.

48. Model #6 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8245.

49. Model #18 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8268.

50. Model #25 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

51. Chaining CNN model with cascade training using
the prediction of model #75 as part of the input. The widths
of ﬁlter are 1, 2 and 3. The corresponding #channels of
ﬁlters are 256, 256, and 512. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. #stage of Chaining
is 2. GAP = 0.8214.

52. Model #3 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

12

74. Model #2 with cascade training using the L1-
normalized prediction of model #76 as part of the input.
GAP = 0.8227.

A.9. Ensemble Models

In this section we lists the performance of some of the
ensemble models. Alongside the GAP score on ”validate2”
set, we also report the GAP score on the blind test set (Pri-
vate LeaderBoard in the competition).

75. Ensemble of 4 single models. Model #2, #10, #18,
and #25 with a class-wise weighted stacking gets a GAP of
0.8373. Private LB GAP = 0.83703.

76. Ensemble of 8 single models. Model #2, #10, #18,
#25, #47, #48, #49, and #50 with a class-wise weighted
stacking gets a GAP of 0.8397. Private LB GAP = 0.83941.
77. Ensemble of 27 single models. Model #1 - #27 with a
class-wise weighted stacking gets a GAP of 0.8427. Private
LB GAP = 0.84250.

78. Ensemble of 57 models. Model #1 - #57 with an
attention weighted stacking (#rank=3, #attention=16) gets a
GAP of 0.8459. Private LB GAP = 0.84561.

79. Ensemble of 74 models. Model #1 - #74 with an
attention weighted stacking (#rank=4, #attention=16) gets a
GAP of 0.8462. Private LB GAP = 0.84583.

80. Ensemble of model #78 and #79 by getting the most
conﬁdent 20 tags from each model and averaging the con-
ﬁdence scores. This is done directly on the test set without
training. Private LB GAP = 0.84590. This is our ﬁnal sub-
mission.

Appendix B. Source Code

The source code of our solution is made public on
GitHub. Visit https://github.com/wangheda/youtube-8m for
the implementation details of our models.

53. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 2.
GAP = 0.8214.

54. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 4.
GAP = 0.8266.

55. Model #20 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8265.

56. Model #6 with cascade training using the prediction
of model #75 as part of the input. The prediction of model
#6 for infrequent labels and the prediction of the cascade
model for frequent ones are joined together as the ﬁnal pre-
diction. GAP = 0.8228.

57. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8202.

58. Model #25 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8254.

59. Model #20 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8250.

60. Model #6 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8245.

61. Model #7 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8247.

62. Model #21 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8251.

63. Model #16 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8258.

64. Model #10 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8248.

65. Model #10 with cascade training using the prediction
of model #76 as part of the input. The prediction of model
#76 is also used to up-sample the misclassiﬁed samples as
in the Boosting model. GAP = 0.8218.

66. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8181.

67. Model #2 with cascade training using the predictions

of model #76 and #66 as part of the input. GAP = 0.7989.

68. Model #2 with cascade training using the predictions
of model #76, #66, and #67 as part of the input. GAP =
0.7849.

69. Model #2 with cascade training using the predictions
of model #76, #66, #67, and #68 as part of the input. GAP
= 0.7753.

70. Model #2 with cascade training using the predictions
of model #76, #66, #67, #68, and #69 as part of the input.
GAP = 0.7833.

71. Model #2 with cascade training using the predictions

of model #76 as part of the input. GAP = 0.8183.

72. Model #2 with cascade training using the predictions

of model #76 and #71 as part of the input. GAP = 0.8085.

73. Model #2 with cascade training using the predictions
of model #76, #71, and #72 as part of the input. GAP =
0.8111.

13

7
1
0
2
 
n
u
J
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
5
1
5
0
.
6
0
7
1
:
v
i
X
r
a

The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge

He-Da Wang
whd.thu@gmail.com

Teng Zhang
zhangteng1887@gmail.com

Ji Wu
wuji ee@mail.tsinghua.edu.cn
Multimedia Signal and Intelligent Information Processing Laboratory
Department of Electronic Engineering
Tsinghua University, Beijing, China

Abstract

This article describes the ﬁnal solution 1 of team mon-
keytyping, who ﬁnished in second place in the YouTube-8M
video understanding challenge. The dataset used in this
challenge is a large-scale benchmark for multi-label video
classiﬁcation. We extend the work in [1] and propose sev-
eral improvements for frame sequence modeling. We pro-
pose a network structure called Chaining that can better
capture the interactions between labels. Also, we report our
approaches in dealing with multi-scale information and at-
tention pooling. In addition, We ﬁnd that using the output of
model ensemble as a side target in training can boost single
model performance. We report our experiments in bagging,
boosting, cascade, and stacking, and propose a stacking al-
gorithm called attention weighted stacking. Our ﬁnal sub-
mission is an ensemble that consists of 74 sub models, all of
which are listed in the appendix.

1. Introduction

Videos have been a very important type of content on
Internet. Understanding video from its audio-visual con-
tent is key to various applications such as recommendation,
searching, and question answering. The research on video
analysis is also an important step for the computer to un-
derstand the real world. Datasets such as Sports-1M[8] and
ActivityNet[4] encourage the research on video classiﬁca-
tion of sports and human activities. YouTube-8M[1] is a
large-scale video dataset that consists of about 7.0 million
YouTube videos that was annotated with a vocabulary of
4716 tags from 24 diverse categories. The average number
of tags per video is 3.4.

The YouTube-8M video understanding challenge is an

1visit https://github.com/wangheda/youtube-8m for the source code.

arena for video classiﬁcation researchers and practitioners.
In the competition, the dataset is divided into three parts.
The training set contains 4.9 million samples. The validate
set contains 1.4 million samples. The test set contains 0.7
million samples. The ground truth labels annotated to the
samples in the training set and the validate set are available
to the participants. The test set is divided into two half,
the open test set and the blind one. In the progress of the
competition, participants can submit their predictions to the
test set. The scoring server on Kaggle would evaluate them
and return the scores on the open test set (the public leader-
board). After the competition, the winners are decided by
the scores of their submissions on the blind test set (the pri-
vate leaderboard).

In the competition, submissions are evaluated using
Global Average Precision (GAP) at 20. The metrics is cal-
culated as follows. For each video, the most conﬁdent 20
label predictions are selected along with the conﬁdence val-
ues. The tuples of the form {video, label, conf idence}
from all the videos are then put into a long list sorted by
conﬁdence values. This list of predictions are then evalu-
ated with the Average Precision (Eq. 1), in which p(i) is the
precision and r(i) is the recall given the ﬁrst i predictions.

AP =

p(i)∆r(i)

(1)

N
(cid:88)

i=1

We divide the dataset into ﬁve parts: train1, validate1,
train2, validate2, and test. The division is done base on ﬁle
name pattern (Table 1). The set train1 is used for single
model training, in which the set validate1 served as a hold-
out test set for early-stopping. Ensemble models are trained
on the set train2, which have no intersection with the train-
ing set of single models. The set validate2 is a hold-out
test set for early-stopping in the training of ensemble mod-
els. The inference procedure uses the test set to generate
submissions. The whole system structure is shown in Fig.

1

Figure 1: The structure of our system.

Part
train1
validate1
train2
validate2
test

File glob
train??.tfrecord
validate[a]?.tfrecord
validate[ˆa0-9]?.tfrecord
validate[0-9]?.tfrecord
test??.tfrecord

#samples
4,906,660
21,918
1,270,412
109,498
700,640

Table 1: Dataset division scheme in our system.

1. This data division strategy may not be optimal, since it
limit the amount of data used in single model training to the
4.9 millions training set examples, which may considerably
affect the performance of single models.

During the training of all our models, We did not use
any data augmentation techniques. Adam [10] optimization
algorithm is used throughout the training process. For mod-
els that use frame level feature rgb, audio, we use a learn-
ing rate of 0.001 and a batch size of 128. For models that
use video level feature mean rgb, mean audio, the default
learning rate and batch size is 0.01 and 1024. Note that
these hyper parameters may not be optimal since we did not
perform any parameter search. Every of our models can ﬁt
in the graphics memory of either an Nvidia GTX 1080 (8G)
or an GTX 1080Ti (11G).

In the rest of this report, we summarize our contribu-
tions in our solution. We ﬁrst introduce our improvements
over the baseline Long-Short Term Memory (LSTM) model
(Section 2). We then introduce a deep network structure
that we use in many models to capture label correlations
(Section 3). Then, we introduce our best performing sin-
gle model that utilize multiscale information via Convolu-
tional Neural Network (CNN) and LSTM (Section 4). Also,
we explore the use of attention pooling in video sequence
classiﬁcation (Section 5). Ensemble methods such as bag-
ging, boosting (Section 6), cascade (Section 7), and stack-
ing (Section 8) are also explored in this report. We also
found that using the predictions of ensemble models as a

soft target can greatly boost single model performance (Sec-
tion 9). In the end, we summarize our road-map and discuss
the contribution of our solution(Section 10).

2. Baseline models

In this section, we list the performances of some of the
baseline models in the technical report [1] and some alter-
native architectures evaluated in our system. The perfor-
mances reported here are different from the ones reported
in the technical report, since the dataset used in the report is
a little larger (8.3 million examples) than the one provided
in this competition. Also, we have limited the amount of
data used for single model training to 4.9 million examples.
The GAP scores we reported in this paper are evaluated with
the ”validate2” set if not otherwise speciﬁed.

We re-implemented four baseline approaches: Logistic
Regression (LR), Mixture-of-Expert (MoE), Deep Bag-of-
Frames (DBoF), and Long-Short Term Memory (LSTM).
The GAP scores are listed in Table 2. We choose a mixture
of 16 for the MoE model for the best performance. For the
LSTM model, we choose the number of layers, the number
of cells per layer, and the number of Moe mixtures to be 2,
1024 and 8 respectively.

Input Feature
Video Level, µ
Video Level, µ
Frame Level, {xv
Frame Level, {xv

Model
Logistic Regression
Mixture-of-Experts

} Deep Bag-of-Frames
} LSTM

1:Fv

1:Fv

GAP
0.7577
0.7957
0.7845
0.8131

Table 2: The performance of baseline approaches in our
system.

In the conventional LSTM [7], the memory cells that
share the same input gate and the same output gate form
a memory block. Putting more than one cells into a block
makes the training of LSTM more efﬁcient. We create an ar-

2

chitecture in which all the cells share a single input gate and
a single forget gate and call it ”LSTM-S”. In this architec-
ture, the cells use individual output gates, since we ﬁnd that
sharing the output gate can be harmful to the performance.
Another network structure we create by adding an input
accumulator to the architecture of LSTM model is what we
call the ”LSTM-A”. In this structure, we add a new set of
memory, input gate and forget gate to the cell to directly
”remember” the input. Eq. 2 gives the architecture of this
model. In the equation, xt, ct, ht are the input, the memory,
and the hidden state, ot, it, ft are the output gate, the input
gate, and the forget gate, c(cid:48)
t, dt are the added memory and
the hidden state for input accumulation, i(cid:48)
t, f (cid:48)
t are the added
input gate and forget gate, σ is the sigmoid function, g is
the activation function which we choose to be tanh, and n
is the L2-normalization function.

content or audio content. Therefore, the visual or audio ac-
tivities do not have to happen spontaneously to be meaning-
ful. We adopt a parallel way that model visual and audio
content with separate LSTM models. The ﬁnal states of the
two LSTM models are then concatenated and go through an
MoE unit. The performance of the parallel model is shown
in Table 4. The number of layers is 2. The number of cells
c and the number of mixtures m in MoE model are shown
in the table.

Model
vanilla LSTM, c = 1024, m = 8
parallel LSTM, cv = 1024, ca = 128, m = 8

GAP
0.8131
0.8161

Table 4: Parallel modeling of visual and audio content is
better than simple feature concatenation.





















ot
mt
it
ft
i(cid:48)
t
f (cid:48)
t
ct
ht
c(cid:48)
t
dt

= TN +2M,6N





ht−1
xt
dt−1





= σ(ft) · ct−1 + σ(it) · g(mt)
= σ(ot) · g(ct)
t) · c(cid:48)
= σ(f (cid:48)
= n(c(cid:48)
t)

t−1 + σ(i(cid:48)

t) · xt

The performance of the two modiﬁed LSTM models are
listed in Table 3. The number of cells per layer are 1024.
The number of mixtures in MoE is set to 8. The number of
layers l is shown in the table. The two models performs bet-
ter than the original model and contribute to the ensemble
because their varieties in model structure.

Model
vanilla LSTM, l = 1
vanilla LSTM, l = 2
LSTM-S, l = 1
LSTM-A, l = 1

GAP
0.8091
0.8131
0.8123
0.8131

Table 3: The performances of the single-block LSTM
(LSTM-S) and the input-accumulator LSTM (LSTM-A),
compared to the original LSTM model.

In the dataset, an example has both visual feature and
audio feature. The naive way of modeling two different
features is to concatenate them into a single feature. We
ﬁnd this to be questionable for two reasons. First, in many
videos such as music videos and family albums, the audio
and the visual content are independent of each other. Also,
people often can make sense of a video only by its visual

3

There is a potential weakness in modeling visual and au-
dio features independently. It might be preferable to allow
the visual network and the audio network to interact at cer-
tain points, since a visual / audio event may be related to a
former audio / visual event in the same video. We leave this
issue to future works.

(2)

3. Label Correlation

In multi-label classiﬁcation settings, an example may be
annotated many labels. Some labels tend to appear in the
same example at the same time, some tend not to. Such in-
formation can be used to improve the performance of multi-
label classiﬁcation models.

Classiﬁer chain [12] create a chain of classiﬁers. One
classiﬁer on the chain predict one label by using not only the
input feature but also the predictions of other labels from the
previous models on the chain. The training of one chain of
classiﬁer involves training L models in which L is the size
of the vocabulary of all labels. However, in real world prob-
lems where the direction of dependency is unknown, an en-
semble of classiﬁer chains is usually used, which makes the
computation complexity even higher and intractable if the
vocabulary of labels is large. A neural network structure
that mimic the classiﬁer chain [11] is proposed to address
the multi-label problem. However, such network structure
has a depth of L that makes it hard to optimize if the vocab-
ulary of labels is large.

We propose a novel end-to-end deep learning structure
that we called ”Chaining” to better utilize the correlation be-
tween labels. In a Chaining model (Figure 2), several repre-
sentations are joined by a chain of MoE model. The predic-
tions are projected to features of lower dimension and used
in the following stages. The representations can be gener-
ated by homogeneous models or heterogeneous ones. We
constantly apply auxiliary cross-entropy loss on the inter-

(a) A unit in Chaining accept one fea-
ture and several model predictions as the
input. The predictions are projected to
lower dimension for efﬁciency.

(b) The whole architecture of Chaining consists of several stages. A Chaining unit
in one stage accepts a feature vector, either from input directly or from a represen-
tation of LSTM or CNN, and all the predictions from earlier stages.

Figure 2: Chaining: a deep learning architecture for multi-label classiﬁcation.

mediate predictions to accelerate the training progress. The
ﬁnal loss function is a weighted average over the loss on
the ﬁnal prediction and the auxiliary losses. We typically
allocate only 10% ∼ 20% of the weights to the auxiliary
losses, since the loss function at the ﬁnal stage is the most
important.

We performed experiments using three basic models,
Mixture-of-Expert, LSTM and CNN. The CNN model we
use in our system is the same as the benchmark method of
sentence classiﬁcation [9], where the length of the ﬁlter is
ﬁxed to the size of feature vector per frame and the resulting
feature map goes through a max-over-time pooling. The ﬁ-
nal state of LSTM and the max-pooled feature map are used
as feature representation in Chaining models and go through
an MoE model for label prediction in the original models.
The performance with and without using Chaining is shown
in Table 5. For the original MoE model, the number of mix-
tures is 16. For the Chaining MoE model, the number of
mixture is 2, the number of stages is 8, and the predictions
are projected into a 128 dimensions vector. For the original
LSTM model, the number of mixtures is 8. For the Chain-
ing LSTM model, the number of mixtures is 4, the number
of stages is 2, and the dimension of projection is 200. For
the original CNN model, the width of ﬁlter and the corre-
sponding numbers of channel are 1×512, 2×512, 3×1024.
For the Chaining CNN model, the corresponding parame-
ters are 1 × 128, 2 × 128, 3 × 256, and the number of stages
is 4. The parameters are chosen to make the original models
have almost equal number of parameters with their Chain-
ing counterparts.

4. Temporal Multi-Scale Information

Input Feature
Video-level, µ
Frame Level, {xv
Frame Level, {xv

1:Fv

1:Fv

Model Original Chaining
0.8106
MoE
0.8172
0.8179

0.7965
} LSTM 0.8131
} CNN
0.7904

Table 5: The performance (GAP) of Mixture-of-Experts,
LSTM and CNN models with and without using Chaining
structure.

age would change with its distance to the observer. How-
ever, temporal information does not have the same rescaling
effect. Therefore it may seems unnatural to model videos
from different temporal scales.

However, we argue that temporal scales matters in video
analysis. A task can be divided into several actions, and
each action involves the interaction between certain objects.
The label annotated to a video can be related to a concept at
different temporal scale: an object, an action, or a task.

One important observation is that video can be seg-
mented into several clips. Each of these clips may contain
content information of a certain aspect. We can make pre-
dictions based on each of these clips and then join them
together to make more accurate predictions. The segmenta-
tion of video is done by either clustering the adjacent frames
[14] or splitting the video into clips of equal durations [15].
We propose a temporal-segment LSTM model (Fig. 3a),
in which the video is split into equal-sized clips, each of
which is modeled by an LSTM model. The models for dif-
ferent clips share the same parameters. The ﬁnal state of
each sequence are treated as another high-level sequence
and modeled by another LSTM model.

Using information from different spatial scales have
been discussed in many image analysis literatures. The
main reason to do so is that the size of an object in an im-

Temporal-pooling LSTM model (Fig. 3a) is a multi-
layer LSTM model in which we inserted temporal k-pooling
layer between LSTM layers. It is similar to the temporal-

4

ition that the representation of low-level sequences contain
more reﬁned features, while the high-level representation is
close to an averaged view of the whole video.

We propose a novel temporal CNN-LSTM model (Fig.
3c) that utilize multi-scale information. This model shares
the previous intuition that adjacent frames can be aggre-
gated to generate features at different temporal scale. In-
stead of direct operating on the original feature, we use
convolution layers to detect patterns in the low-level fea-
tures and combine adjacent ﬁlter outputs by max-pooling
along the time dimension. The ﬁlters used in a convolution
layer are of the same length with the dimension of the fea-
ture, and their widths and channels can be varied. We use
different LSTM models and Moe classiﬁers for the repre-
sentation of the feature maps of different temporal scales.
The predictions generated from features of different scales
are combine using a consensus function. We ﬁnd that av-
eraging is a good consensus function, and maximum would
lead to difﬁculties in convergence.

GAP
Model
0.8131
vanilla LSTM
temporal-pooling LSTM 0.8085
temporal-segment LSTM 0.8122
multi-resolution LSTM
0.8148
multi-scale CNN-LSTM 0.8204

Table 6: Performance of multi-scale models.

Table 6 lists the performances of the models discussed
in this section. The temporal-pooling LSTM model uses 2-
pooling and has 4 LSTM layers (#cells=1024) and 4 MoE
mixtures. The temporal-segment LSTM model uses a dura-
tion of 10 frames for the clips and has 8 MoE mixtures and
2 LSTM layers, each of the layers has 1024 memory unit.
In the multi-resolution LSTM model, we use 2-pooling to
get shorter input sequences; the number of stages in Chain-
ing is 4; the number of mixtures in MoE is 4; the projection
dimension is 256; the LSTM models in it are 2-layer par-
allel LSTM (#cells is 512 for video and 64 for audio). In
the multi-scale CNN-LSTM model, the number of layers in
CNN model is 4; the number of mixtures in MoE is 4; the
LSTM models in it are 1-layer LSTM (#cells=1024); in ev-
ery layer of the CNN model, the width of ﬁlters and the cor-
responding number of channels are 1×256, 2×256, 3×512;
the pooling layers in it are 2-pooling.

5. Identifying Salient Frames with Attention

Not all the frames in a video are equally informative.
There are many reasons that a frame might contribute little
in video classiﬁcation. Images that are too dark, too bright,
or too blurry are hard to analysis for the image classiﬁcation

(a) Temporal-pooling (with the dashes) and temporal-segment
(without the dashes) LSTM model.

(b) Multi-resolution LSTM model.

(c) Multi-scale CNN-LSTM model.

Figure 3: Models that utilize temporal multi-scale informa-
tion.

segment LSTM model. The difference is whether to use the
ﬁnal state of the one clip as the initial state of the next clip
in the ﬁrst layer of the LSTM model.

Instead of directly segmenting videos into clips, we
can aggregate the adjacent frames and gradually construct
features containing long-range information.
In multi-
resolution LSTM model (Fig. 3b), the original features
are average pooled along the time dimension to get shorter
sequences in which each frame covers longer time range.
For each sequence generated in this way, a separate LSTM
model is used to generate a sequence representation. The
representations are then joined with a Chaining model.
The representation from the highest-level goes into the ﬁrst
stage, while the one from the original sequence goes into the
ﬁnal stage of the Chaining model. This is due to the intu-

5

(b) An attention network generates multiple attention
weights for a frame based on the input feature and the
LSTM output at that frame.

(a) The multiple attention pooling scheme. The outputs
of LSTM are pooled with multiple attention weights.

Figure 4: Multiple attention pooling model.

network. Therefore, such frames may not be able to provide
useful semantic information. Also, sometimes a video con-
tains title screens, credits or text. They might be useful if
the frames are processed using optical character recognition
(OCR). However, in this competition, the frames are pre-
processed using an image classiﬁcation network pre-trained
with ImageNet dataset, which means the frame-level fea-
tures may not contain the semantic information in these text.
In addition, there are always frames irrelevant to the theme
of the video. In talk show videos the content of frames will
always be people talking while the topic of the talking is
what really important. In documentaries there are often a
lot of driving scenes which do not reﬂect the theme of the
video.

We propose a multiple attention pooling scheme (multi-
AP) for selecting salient frames in the video. As Fig. 4
shows, an LSTM model is used to deal with the frame-level
features. The outputs of the LSTM model are then aggre-
gated by pooling over the time dimension. The aggregation
is achieved by taking a weighted average over the outputs
of the LSTM model, in which the weights are generated
using an attention network, for which we use a fully con-
nected layer. The aggregated feature is fed into an MoE
model for label predictions. The attention pooling is re-
peated for multiple times and the predictions are combined
using a classiﬁer consensus function. For the choice of com-
bining method, we ﬁnd that maximum performs better than
averaging as a consensus function.

the input and output of the LSTM at a certain frame as the
input. The attention network output K groups of different
attention weights.

Each group of the weights are used to generate a group of
aggregated features and prediction results. The results from
all the groups are then combined by choosing the highest
conﬁdence predicted from all the models for each label l.
The MoE models in Fig. 4a share parameters with each
other.

eik = Wk[xi; yi]
aik = exp(eik)
(cid:80)Fv

i=1 exp(eik)

zk = (cid:80)Fv
i=1 aikyi
pk = M oE(zk)
(pk,l)
pl = max

k

(3)

(4)

We present a few examples outside of the training set to
show what kind of frames is highlighted by the attention
network in the multi-AP model (Fig. 5). For each video,
the frames with the lowest attention weights (left) and the
ones with the highest attention weights (right) are presented.
Since there are many groups of attention weight, the ﬁrst
group is used for this presentation. We have not observed
any noticeable patterns in the inter-group differences of the
weights.

We use a fully connected layer with the softmax acti-
vation function as the attention network (Eq. 3). It takes

In analysis of these examples, we ﬁnd that there are some
patterns about the attention weights. Title screens and the

6

Figure 5: Visualization of the attention weights in the multiple attention pooling model. The frames in each row comes from
the same video. The left four in each row are the frames with the lowest weights, while the right four are the ones with the
highest weights.

frames that is very dark tend to have low attention weights.
Also, if the object of interest is too small or partly blocked,
the weight of that frame tend to be low.

Our work is different from a previous work on attention
pooling [5] in two ways. First, our model uses multiple
groups of attention on the same output sequence and multi-
ple classiﬁers for prediction. The multiple attention scheme
may contribute to generating more stable classiﬁcation re-
sult. Second, our model do an attention pooling over the
outputs of LSTM model which is a global representation
of the sequence while the previous work do a pooling over
local CNN outputs.

We also adopt a local attention pooling scheme that ap-
plies attention pooling over the input features (Eq. 5). The
representation of the LSTM model is also used in the MoE
model for prediction.

The LSTM model in the four models share the same pa-
rameter (#layers=2, #cells=1024). The MoE models have 8
mixtures in the two multi-AP models, while the one in the
local-AP model has 4 mixtures. The two multi-AP models
both have 8 groups of attention weights. And the dimen-
sion of the positional embedding in the positional multi-AP
model is 32.

GAP
Model
0.8131
vanilla LSTM
0.8133
local-AP LSTM
multi-AP LSTM
0.8157
positional multi-AP LSTM 0.8169

Table 7: Performance (GAP) of attention models.

z = (cid:80)Fv

i=1 aixi

p = M oE([yFv ; z])

(5)

6. Bagging and Boosting

We compare the two attention pooling scheme with the
baseline in Table 7. The ”local-AP LSTM” refers to the
scheme using attention pooling over the input feature. The
”multi-AP LSTM” refers to the multiple attention pooling
scheme. The ”positional multi-AP LSTM” model add an
embedding for every frame position to the attention network
on the basis of the multi-AP LSTM model.

The parameters for the models in Table 7 is as follows.

Bootstrap aggregating [2], also called Bagging, is an en-
semble method that creates many versions of a model and
combines their results together. To create one version of a
model, one applies sampling with replacement to get a sub-
set of the original data, in which some original examples
may not present or present more than once[16]. Training on
different subset sampled from the original data would re-
sults in different models. The bagging algorithm is known
for its ability to reduce the variance of a model. We apply

7

Bagging to some of our models, and ﬁnd that Bagging can
generally boost the GAP performance by 0.6% ∼ 1.2%.
The results are shown in Table 8.

weights to 5 to ensure that the algorithm would not place all
the weights on a few formerly misclassiﬁed examples, since
they may not be visually classiﬁable.

Original Bagging
Input Feature Model
0.8225
0.8106
Video-level
0.8216
0.8160
Frame Level
0.8258
Frame Level
0.8179
0.8244
Frame Level multi-AP LSTM 0.8157

Chaining
parallel LSTM
Chaining CNN

Model
Chaining (Video)
parallel LSTM
Chaining CNN
multi-AP LSTM

Original Boosting
0.8218
0.8106
0.8218
0.8160
0.8242
0.8179
0.8246
0.8157

Table 8: Performance (GAP) of bagging models.

Table 9: Performance (GAP) of boosting models.

Boosting [3] is another way to create different versions
of a model. Compared to Bagging which can be run in
parallel, Boosting is a sequential ensemble method. The
(k + 1)th classiﬁer is constructed considering the previous
k classiﬁers by re-sampling a distribution that highlights the
misclassiﬁed examples.

The re-sampling is often implemented by using weighted
examples. If there are N training examples and L labels,
the number of total classiﬁcation results is N × L. Most
works on Boosting in multi-label classiﬁcation use a weight
W N ×L over N samples and L labels, which is computa-
tionally intractable when the vocabulary of label is large, as
is the case in this competition.

We adopt a per-example weighting scheme that assign
a weight W N over the training samples. The weights are
updated with Eq. 6. In the equations, Wk,n is the weight
assigned to the nth example in the training of the kth clas-
siﬁer. Errk,n is the error rate of the nth example by eval-
uating the kth classiﬁer. Errk is the average error rate of
the kth classiﬁer. And Zk is a coefﬁcient that scales the av-
erage value of Wk,n to 1. α is a parameter controlling the
highlighting effect on the misclassiﬁed examples which we
constantly set to 1 in our system.

Wk,n exp(αrkErrk,n)

W0,n = 1.0
Wk+1,n = N
Zk
in which,
rk = log( 1.0−Errk
Errk = 1
N
Errk,n ∈ [0, 1]
Zk = (cid:80)N

Errk
(cid:80)N

)

n=1 Errk,n

n=1 Wk,n exp(αrkErrk,n)

In this algorithm, the weights of the misclassiﬁed ex-
amples are increased in the following classiﬁers. However,
in multi-label classiﬁcation, misclassiﬁcation is hard to de-
ﬁne. We choose the Precision Equal Recall Rate (PERR)
as the implementation of error rate, since it is both per-
example evaluated and in coordinate with the GAP score in
most models (from empirical observation). We also clip the

The comparison between the single models and their
Boosting counterparts are shown in Table 9. The Boosting
algorithm generates a performance boost similar as Bagging
does.

7. Cascade Classiﬁer Training

Adding models into the ensemble would usually make
the performance better. However, with the number of mod-
els in the ensemble model increasing, the gain from newly
added models tends to diminish, especially if one add mod-
els that are similar to the existing models. We address this
problem by using cascade training in which the predictions
of other models as a part of its input of the model during
training.

(6)

Figure 6: Cascade layer as a replacement for MoE in cas-
cade classiﬁer training.

The structure of cascade layer is shown in Fig. 6. The
predictions from the other models are averaged to generate
an averaged prediction, which is then projected to a feature
of low dimension. All the models that have MoE as their
last layer can be modiﬁed into a cascade model by having
their MoE layer replaced by the cascade layer.

8. Stacking Methods

Stacking is an ensemble method that uses a machine
learning model to combine the predictions of many mod-
els into one prediction. We use the ”train2” set (Section 1)

8

as the training set for stacking. The trivial case of stacking
is to simply average all the predictions. If there are M mod-
els and L labels in total, simple averaging the predictions of
all models can be written as Eq. 7.

pl =

1
M

M
(cid:88)

m=1

pm,l

(7)

Linear weighted averaging (Eq.

8) is also a simple
scheme. This method has M weights, one for each model
in the ensemble.

M
(cid:88)

m=1

M
(cid:88)

m=1

pl =

wmpm,l, s.t.

wm = 1, wm > 0

(8)

1
M

M
(cid:88)

m=1

For multi-label classiﬁcation, there are L predictions for
every example. Some models might perform better than the
others on a subset of the labels. Therefore, we could extend
linear weighted averaging to every class. We denote this
method class-wise weighted averaging (Eq. 9).

1
M

M
(cid:88)

m=1

pl =

wm,lpm,l, s.t.

wm,l = 1, wm,l > 0

(9)
While the ensemble methods often deal with the predic-
tions of individual models, the original input may also help
to decide which models to trust. The winning team of Net-
ﬂix Prize proposes the Feature-Weighted Linear Stacking
[13] to utilize the information in meta-features. Although
there are no useful meta-feature in the Youtube-8M dataset,
we consider the averaged frame input and the averaged pre-
dictions from individual models as useful indicators. We
propose the attention weighted stacking to utilized these in-
formation.

The attention weights α (Eq. 10) are generated with a
fully connected layer with softmax as the activation func-
tion and averaged input ¯x and prediction ¯p as the input of
the layer.

The predictions are then weighted averaged as shown in
Eq. 11. The weight is a mixture of K low-rank matrices
(rank D) in which the matrices are selected by the output
of the attention network (Eq. 10). In the attention weighted
stacking, V, A, B, a, b, c are trainable parameters.

We perform a comparison among the stacking methods
on an ensemble of 74 models (model #1 - #74 in Appendix
A). The number of attention weights is 16 and the rank
of the matrix components is 4 in the attention weighted
stacking method. The comparison shows that the attention
weighted stacking performs better than the other methods
(Table 10).

Stacking method
Simple Averaging
Linear Weighted Averaging
Class-wise Weighted Averaging
Attention Weighted Stacking

GAP
0.8436
0.8449
0.8453
0.8458

Table 10: Performance (GAP) of different stacking models
on the entire set of models (74 models). The GAP scores
in this table is evaluated with the blind test set (the private
leaderboard score in the competition).

9. Distilling Knowledge from Ensemble

Ensemble models might perform much better than even
the best single model. However, in real world systems, the
memory and computation resources are often too limited
for ensemble models to deploy. Distillation [6] is a training
scheme that aims to compress the knowledge in ensemble
model to a single model. Another motivation of using dis-
tillation is the observation that the labels are often wrong
or incomplete, especially when the vocabulary of label is
large.

We adopt a training scheme that uses the predictions of
ensemble ˆp as a soft target along with the real target l during
training. The new loss function is a weighted average of two
losses, one for each target (Eq. 12).

α = σ(V [¯x; ¯p])

¯x = 1
Fv

(cid:80)Fv

i=1 xi

¯p = 1
M

(cid:80)M

m=1 pm

pl

= 1
M

(cid:80)M

m=1 wm,lpm,l

wm,l =

(cid:80)M

e

= (cid:80)K

exp(em,l)
m=1 exp(em,l)
k=1 αk(AT

s.t.

Ak ∈ RD×M , Bk ∈ RD×L
ak ∈ RM ×1, bk ∈ RL×1
ck ∈ R

(10)

ˆL = (1 − λ)ce(p, l) + λce(p, ˆp)

(12)

We use the predictions of model #75 (Appendix A.9) as
the soft target in training and train several models with the
same parameters as the original models. The comparison is
listed in Table 11. The comparison shows that distillation
can greatly boost single model performance (often better
than Bagging).

In the previous sections, we review our system, present
our intuition, and show how we address the problems in

9

k Bk + akbT

k + ck)

(11)

10. Summary

Original Distillation
Model
0.8106
Chaining (Video)
0.8160
parallel LSTM
0.8179
Chaining CNN
0.8172
Chaining LSTM
Multi-scale CNN-LSTM 0.8204

0.8169
0.8237
0.8266
0.8291
0.8258

Table 11: Performance (GAP) of single models trained us-
ing distillation.

multi-label video classiﬁcation. We ﬁnd that attention pool-
ing and temporal multi-scale information is very important
for video sequence modeling. We also propose a network
structure for large-vocabulary multi-label classiﬁcation. We
review our work in ensemble methods such as Bagging,
Boosting, Cascade, Distillation and Stacking. We propose a
stacking network that uses attention to weight models. Our
system road-map is shown in Table 12.

Changes
27 single models
+ 11 bagging & boosting models
+ 8 distillation models
+ 11 cascade models
+ 17 more cascade models
Attention Weighted Stacking

GAP
0.8425
0.8435
0.8437
0.8451
0.8453
0.8458

Table 12: Performance (GAP) of ensemble models. The
stacking method used for the ﬁrst 5 results is class-wise
weighted model. The ensemble in each row include all the
changes in the rows above it. The results reported in this
table are evaluated on the blind test set (the private leader-
board score in the competition).

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m:
CoRR,
A large-scale video classiﬁcation benchmark.
abs/1609.08675, 2016.

[2] L. Breiman.

Bagging predictors. Machine Learning,

24(2):123–140, 1996.

[3] L. Breiman. Arcing classiﬁer (with discussion and a rejoin-
der by the author). Ann. Statist., 26(3):801–849, 06 1998.
[4] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 961–970, 2015.

[5] M. J. Er, Y. Zhang, N. Wang, and M. Pratama. Atten-
tion pooling-based convolutional neural network for sen-
tence modelling. Information Sciences, 373:388 – 403, 2016.
[6] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, pages
1725–1732, 2014.

[9] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014.

[11] J. Read and J. Hollm´en. A deep interpretation of classi-
ﬁer chains. In International Symposium on Intelligent Data
Analysis, pages 251–262. Springer, 2014.

[12] J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classiﬁer
chains for multi-label classiﬁcation. Machine Learning and
Knowledge Discovery in Databases, pages 254–269, 2009.

[13] J. Sill, G. Tak´acs, L. Mackey, and D. Lin. Feature-weighted
linear stacking. arXiv preprint arXiv:0911.0460, 2009.
[14] E. H. Spriggs, F. De La Torre, and M. Hebert. Temporal seg-
mentation and activity classiﬁcation from ﬁrst-person sens-
ing. In Computer Vision and Pattern Recognition Workshops,
2009. CVPR Workshops 2009. IEEE Computer Society Con-
ference On, pages 17–24. IEEE, 2009.

[15] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
towards good
L. Van Gool. Temporal segment networks:
practices for deep action recognition. In European Confer-
ence on Computer Vision, pages 20–36. Springer, 2016.

[16] Z.-H. Zhou. Ensemble methods:

foundations and algo-

rithms. CRC press, 2012.

10

Appendix A. Final Submission

In this section we list all the components included in our
ﬁnal submission. We apply a stacking method called the At-
tention Weighted Average (Section 8) to merge the result of
these models instead of carrying on careful model selection.
Therefore it might be possible that a subset of these models
could reach equal or better performance. The parameters
are for a quick grasp of the structure of the models. Re-
fer to our code 2 if you are interested in the implementation
details. The GAP scores reported in the appendix are eval-
uated with the ”validate2” set if not otherwise speciﬁed.

A.1. Video-level Models

1. Chaining model (Section 3). During training, if the
loss of the current batch is less than 10, use the most conﬁ-
dent tag predicted as a soft target for every example in the
batch. #mixture of MoE is 4. #stage of Chaining is 3. pro-
jection dimension in Chaining is 100. GAP = 0.8067.

2. Chaining model. #mixture of MoE is 2. #stage of
Chaining is 8. projection dimension in Chaining is 128.
GAP = 0.8106.

A.2. Baseline Models

If not otherwise speciﬁed, the memory cells of the ﬁnal
state are used for the input of the MoE model in the baseline
models.

3. LSTM model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8131.
4. LSTM model. #cell of LSTM is 2048. #layer of

LSTM is 2. #mixture of MoE is 4. GAP = 0.8152.

5. LSTM model of 2 layers. The ﬁrst layer is bi-
directional. The second layer is uni-directional. #cell of
LSTM is 1024,. #mixture of MoE is 4. GAP = 0.8105.

6. LSTM-S model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8123.
7. LSTM-A model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.
8. LSTM-A model of 2 layers. The ﬁrst layer is a for-
ward LSTM-A model. The second layer is a backward
LSTM-A model that takes the original input and the out-
put of the ﬁrst layer as input. #cell of LSTM is 1024. #layer
of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.

9. Parallel LSTM model (Section 2). #cell of LSTM is
1024. #layer of LSTM is 2. #mixture of MoE is 8. GAP =
0.8161.

10. Parallel LSTM model. The gated outputs of the ﬁ-
nal state are used for the input of the MoE model. #cell of
LSTM is 1024. #layer of LSTM is 2. #mixture of MoE is
8. GAP = 0.8160.

11. LSTM model with data augmentation by random
sampling 50% of the frames. #cell of LSTM is 1024. #layer

2https://github.com/wangheda/youtube-8m

of LSTM is 2. #mixture of MoE is 8. GAP = 0.8137.

12. CNN-LSTM model. The CNN model is described
in section 3. #layer of CNN is 1. The widths of ﬁlter are
1, 2 and 3. The corresponding #channels of ﬁlters are 1024,
1024, and 1024. #cell of LSTM is 1024. #layer of LSTM is
2. #mixture of MoE is 8. GAP = 0.8103.

A.3. Temporal Multi-Scale Models

13. Temporal-segment LSTM model (Section 4). The
duration of each clip is 10 frames. #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8122.
14. Temporal-pooling LSTM model (Section 4). The
model has 2-pooling between LSTM layers. #cell of LSTM
is 1024. #layer of LSTM is 4, #mixture of MoE is 8. GAP
= 0.8085.

15. Multi-resolution LSTM model (Section 4). The
model has parallel LSTM for sequence modeling. The
model has 2-pooling to get shorter representation of input.
#cell of LSTM is 512 for video and 64 for audio. #layer
of LSTM is 2. #stage of Chaining is 4. The dimension of
projection in Chaining is 256. #mixture of MoE is 4. GAP
= 0.8149.

16. Multi-scale CNN-LSTM model (Section 4). The
widths of ﬁlter are 1, 2 and 3. The corresponding #chan-
nels of ﬁlters are 256, 256, and 512. #layer in CNN is 4.
#layer of LSTM is 1. #cell of LSTM is 1024. #mixture of
MoE is 4. GAP = 0.8204.

17. Multi-scale CNN-LSTM model. All the parameters
are the same as model #16, except that the type of the LSTM
model is LSTM-S (as in model #6). GAP = 0.8147.

A.4. Chaining Models

18. Chaining CNN model (Section 3). The widths of ﬁl-
ter are 1, 2 and 3. The corresponding #channels of ﬁlters are
128, 128, and 256. The dimension of projection in Chaining
is 256. #mixture of MoE is 4. #stage of Chaining is 4. GAP
= 0.8179.

19. Chaining Deep CNN model. In this model a 3-layer
CNN model is used. The averaged input feature and the
3 max-pooled feature maps from the 3 layers of CNN are
combined using a 4-stage Chaining model. The widths of
ﬁlter are 1, 2 and 3. The corresponding #channels of ﬁl-
ters are 128, 128, and 256. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. GAP = 0.8155.

20. Chaining LSTM-CNN model. This model uses a
cascade of parallel LSTM and CNN model as the sub-model
in Chaining.#layer of LSTM is 1. #cell of LSTM is 1024 for
video and 128 for audio. The widths of ﬁlter are 1, 2 and
3. The corresponding #channels of ﬁlters are 128, 128, and
256. The dimension of projection in Chaining is 128. #stage
in Chaining is 3. #mixture of MoE is 4. GAP = 0.8122.

21. Chaining LSTM model (Section 3). #layer of LSTM
is 2. #cell of LSTM is 1024. The dimension of projection in

11

Chaining is 200. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8172.

22. Chaining LSTM model. The different stages in
Chaining model uses a shared input which is the cell mem-
ory of the ﬁnal state of an LSTM model. #layer of LSTM is
2. #cell of LSTM is 1024. The dimension of projection in
Chaining is 256. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8162.

A.5. Attention Pooling Models

23. Local attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. GAP =
0.8133.

24. Attention pooling LSTM model that has one LSTM
model to generate the attention weights for pooling over the
output of another LSTM model. #layer of LSTM is 1. #cell
of LSTM is 1024. #mixture of MoE is 8. GAP = 0.8088.

25. Multiple attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. #mixture of
MoE is 4. #group of attention weights is 8. GAP = 0.8157.
26. Multiple attention pooling LSTM model that has one
LSTM model to generate the attention weights for pooling
over the output of another LSTM model. #layer of LSTM
is 2. #cell of LSTM is 1024. #mixture of MoE is 4. #group
of attention weights is 8. GAP = 0.8081.

27. Multiple attention pooling LSTM model with posi-
tional embedding (Section 5). #layer of LSTM is 2. #cell
of LSTM is 1024. #mixture of MoE is 4. The dimension of
positional embedding is 32. #group of attention weights is
8. GAP = 0.8169.

A.6. Bagging and Boosting Models

GAP = 0.8218.

33. Bagging model of 8 versions of model #18, com-
bined with class-wise weighted averaging. GAP = 0.8258.
34. Boosting model of 8 versions of model #18, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8246.

35. Bagging model of 8 versions of model #25, com-
bined with class-wise weighted averaging. GAP = 0.8244.
36. Boosting model of 8 versions of model #25, com-
bined with attention weighted stacking, with weight clip-
ping. GAP = 0.8242.

37. Bagging model of 8 versions of model #10, com-

bined with simple averaging. GAP = 0.8216.

38. Boosting model of 8 versions of model #10, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8218.

A.7. Distillation Models

The models in this section is trained using distillation
(Section 9). During training, the predictions from model
#75 are used as soft targets.

39. Model #2 with distillation training. GAP = 0.8169.
40. Model #18 with distillation training. GAP = 0.8266.
41. Model #20 with distillation training. GAP = 0.8259.
42. Model #10 with distillation training. GAP = 0.8237.
43. Model #21 with distillation training. GAP = 0.8291.
44. Model #16 with distillation training. GAP = 0.8258.
45. Bagging model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8249.

46. Boosting model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8254.

28. Bagging model of 8 versions of model #2, combined

with simple averaging. GAP = 0.8225.

A.8. Cascade Models

29. Ensemble of 4 Chaining models. The ﬁrst is the
normal Chaining model with video-level input. The second
is a Chaining model with weighted cross entropy loss. The
third is a Chaining model that predict label and top-level
verticals at the same time. The fourth is a Chaining model
that predict the infrequent labels with softmax function. All
4 models share the same parameters. #mixture of MoE is
4. #stage in Chaining is 3. The dimension of projection in
Chaining is 100. GAP = 0.8216.

30. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging. Remove the ex-
amples with too high weights from training set, since it may
be hopeless to predict these examples correctly. GAP =
0.8213.

31. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging, without weight
clipping. GAP = 0.8198.

32. Boosting model of 8 versions of model #2, combined
with class-wise weighted averaging, with weight clipping.

The models in this section are models using the predic-

tion of other models as part of the input (Section 7).

47. Model #2 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8231.

48. Model #6 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8245.

49. Model #18 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8268.

50. Model #25 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

51. Chaining CNN model with cascade training using
the prediction of model #75 as part of the input. The widths
of ﬁlter are 1, 2 and 3. The corresponding #channels of
ﬁlters are 256, 256, and 512. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. #stage of Chaining
is 2. GAP = 0.8214.

52. Model #3 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

12

74. Model #2 with cascade training using the L1-
normalized prediction of model #76 as part of the input.
GAP = 0.8227.

A.9. Ensemble Models

In this section we lists the performance of some of the
ensemble models. Alongside the GAP score on ”validate2”
set, we also report the GAP score on the blind test set (Pri-
vate LeaderBoard in the competition).

75. Ensemble of 4 single models. Model #2, #10, #18,
and #25 with a class-wise weighted stacking gets a GAP of
0.8373. Private LB GAP = 0.83703.

76. Ensemble of 8 single models. Model #2, #10, #18,
#25, #47, #48, #49, and #50 with a class-wise weighted
stacking gets a GAP of 0.8397. Private LB GAP = 0.83941.
77. Ensemble of 27 single models. Model #1 - #27 with a
class-wise weighted stacking gets a GAP of 0.8427. Private
LB GAP = 0.84250.

78. Ensemble of 57 models. Model #1 - #57 with an
attention weighted stacking (#rank=3, #attention=16) gets a
GAP of 0.8459. Private LB GAP = 0.84561.

79. Ensemble of 74 models. Model #1 - #74 with an
attention weighted stacking (#rank=4, #attention=16) gets a
GAP of 0.8462. Private LB GAP = 0.84583.

80. Ensemble of model #78 and #79 by getting the most
conﬁdent 20 tags from each model and averaging the con-
ﬁdence scores. This is done directly on the test set without
training. Private LB GAP = 0.84590. This is our ﬁnal sub-
mission.

Appendix B. Source Code

The source code of our solution is made public on
GitHub. Visit https://github.com/wangheda/youtube-8m for
the implementation details of our models.

53. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 2.
GAP = 0.8214.

54. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 4.
GAP = 0.8266.

55. Model #20 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8265.

56. Model #6 with cascade training using the prediction
of model #75 as part of the input. The prediction of model
#6 for infrequent labels and the prediction of the cascade
model for frequent ones are joined together as the ﬁnal pre-
diction. GAP = 0.8228.

57. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8202.

58. Model #25 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8254.

59. Model #20 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8250.

60. Model #6 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8245.

61. Model #7 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8247.

62. Model #21 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8251.

63. Model #16 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8258.

64. Model #10 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8248.

65. Model #10 with cascade training using the prediction
of model #76 as part of the input. The prediction of model
#76 is also used to up-sample the misclassiﬁed samples as
in the Boosting model. GAP = 0.8218.

66. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8181.

67. Model #2 with cascade training using the predictions

of model #76 and #66 as part of the input. GAP = 0.7989.

68. Model #2 with cascade training using the predictions
of model #76, #66, and #67 as part of the input. GAP =
0.7849.

69. Model #2 with cascade training using the predictions
of model #76, #66, #67, and #68 as part of the input. GAP
= 0.7753.

70. Model #2 with cascade training using the predictions
of model #76, #66, #67, #68, and #69 as part of the input.
GAP = 0.7833.

71. Model #2 with cascade training using the predictions

of model #76 as part of the input. GAP = 0.8183.

72. Model #2 with cascade training using the predictions

of model #76 and #71 as part of the input. GAP = 0.8085.

73. Model #2 with cascade training using the predictions
of model #76, #71, and #72 as part of the input. GAP =
0.8111.

13

7
1
0
2
 
n
u
J
 
6
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
5
1
5
0
.
6
0
7
1
:
v
i
X
r
a

The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge

He-Da Wang
whd.thu@gmail.com

Teng Zhang
zhangteng1887@gmail.com

Ji Wu
wuji ee@mail.tsinghua.edu.cn
Multimedia Signal and Intelligent Information Processing Laboratory
Department of Electronic Engineering
Tsinghua University, Beijing, China

Abstract

This article describes the ﬁnal solution 1 of team mon-
keytyping, who ﬁnished in second place in the YouTube-8M
video understanding challenge. The dataset used in this
challenge is a large-scale benchmark for multi-label video
classiﬁcation. We extend the work in [1] and propose sev-
eral improvements for frame sequence modeling. We pro-
pose a network structure called Chaining that can better
capture the interactions between labels. Also, we report our
approaches in dealing with multi-scale information and at-
tention pooling. In addition, We ﬁnd that using the output of
model ensemble as a side target in training can boost single
model performance. We report our experiments in bagging,
boosting, cascade, and stacking, and propose a stacking al-
gorithm called attention weighted stacking. Our ﬁnal sub-
mission is an ensemble that consists of 74 sub models, all of
which are listed in the appendix.

1. Introduction

Videos have been a very important type of content on
Internet. Understanding video from its audio-visual con-
tent is key to various applications such as recommendation,
searching, and question answering. The research on video
analysis is also an important step for the computer to un-
derstand the real world. Datasets such as Sports-1M[8] and
ActivityNet[4] encourage the research on video classiﬁca-
tion of sports and human activities. YouTube-8M[1] is a
large-scale video dataset that consists of about 7.0 million
YouTube videos that was annotated with a vocabulary of
4716 tags from 24 diverse categories. The average number
of tags per video is 3.4.

The YouTube-8M video understanding challenge is an

1visit https://github.com/wangheda/youtube-8m for the source code.

arena for video classiﬁcation researchers and practitioners.
In the competition, the dataset is divided into three parts.
The training set contains 4.9 million samples. The validate
set contains 1.4 million samples. The test set contains 0.7
million samples. The ground truth labels annotated to the
samples in the training set and the validate set are available
to the participants. The test set is divided into two half,
the open test set and the blind one. In the progress of the
competition, participants can submit their predictions to the
test set. The scoring server on Kaggle would evaluate them
and return the scores on the open test set (the public leader-
board). After the competition, the winners are decided by
the scores of their submissions on the blind test set (the pri-
vate leaderboard).

In the competition, submissions are evaluated using
Global Average Precision (GAP) at 20. The metrics is cal-
culated as follows. For each video, the most conﬁdent 20
label predictions are selected along with the conﬁdence val-
ues. The tuples of the form {video, label, conf idence}
from all the videos are then put into a long list sorted by
conﬁdence values. This list of predictions are then evalu-
ated with the Average Precision (Eq. 1), in which p(i) is the
precision and r(i) is the recall given the ﬁrst i predictions.

AP =

p(i)∆r(i)

(1)

N
(cid:88)

i=1

We divide the dataset into ﬁve parts: train1, validate1,
train2, validate2, and test. The division is done base on ﬁle
name pattern (Table 1). The set train1 is used for single
model training, in which the set validate1 served as a hold-
out test set for early-stopping. Ensemble models are trained
on the set train2, which have no intersection with the train-
ing set of single models. The set validate2 is a hold-out
test set for early-stopping in the training of ensemble mod-
els. The inference procedure uses the test set to generate
submissions. The whole system structure is shown in Fig.

1

Figure 1: The structure of our system.

Part
train1
validate1
train2
validate2
test

File glob
train??.tfrecord
validate[a]?.tfrecord
validate[ˆa0-9]?.tfrecord
validate[0-9]?.tfrecord
test??.tfrecord

#samples
4,906,660
21,918
1,270,412
109,498
700,640

Table 1: Dataset division scheme in our system.

1. This data division strategy may not be optimal, since it
limit the amount of data used in single model training to the
4.9 millions training set examples, which may considerably
affect the performance of single models.

During the training of all our models, We did not use
any data augmentation techniques. Adam [10] optimization
algorithm is used throughout the training process. For mod-
els that use frame level feature rgb, audio, we use a learn-
ing rate of 0.001 and a batch size of 128. For models that
use video level feature mean rgb, mean audio, the default
learning rate and batch size is 0.01 and 1024. Note that
these hyper parameters may not be optimal since we did not
perform any parameter search. Every of our models can ﬁt
in the graphics memory of either an Nvidia GTX 1080 (8G)
or an GTX 1080Ti (11G).

In the rest of this report, we summarize our contribu-
tions in our solution. We ﬁrst introduce our improvements
over the baseline Long-Short Term Memory (LSTM) model
(Section 2). We then introduce a deep network structure
that we use in many models to capture label correlations
(Section 3). Then, we introduce our best performing sin-
gle model that utilize multiscale information via Convolu-
tional Neural Network (CNN) and LSTM (Section 4). Also,
we explore the use of attention pooling in video sequence
classiﬁcation (Section 5). Ensemble methods such as bag-
ging, boosting (Section 6), cascade (Section 7), and stack-
ing (Section 8) are also explored in this report. We also
found that using the predictions of ensemble models as a

soft target can greatly boost single model performance (Sec-
tion 9). In the end, we summarize our road-map and discuss
the contribution of our solution(Section 10).

2. Baseline models

In this section, we list the performances of some of the
baseline models in the technical report [1] and some alter-
native architectures evaluated in our system. The perfor-
mances reported here are different from the ones reported
in the technical report, since the dataset used in the report is
a little larger (8.3 million examples) than the one provided
in this competition. Also, we have limited the amount of
data used for single model training to 4.9 million examples.
The GAP scores we reported in this paper are evaluated with
the ”validate2” set if not otherwise speciﬁed.

We re-implemented four baseline approaches: Logistic
Regression (LR), Mixture-of-Expert (MoE), Deep Bag-of-
Frames (DBoF), and Long-Short Term Memory (LSTM).
The GAP scores are listed in Table 2. We choose a mixture
of 16 for the MoE model for the best performance. For the
LSTM model, we choose the number of layers, the number
of cells per layer, and the number of Moe mixtures to be 2,
1024 and 8 respectively.

Input Feature
Video Level, µ
Video Level, µ
Frame Level, {xv
Frame Level, {xv

Model
Logistic Regression
Mixture-of-Experts

} Deep Bag-of-Frames
} LSTM

1:Fv

1:Fv

GAP
0.7577
0.7957
0.7845
0.8131

Table 2: The performance of baseline approaches in our
system.

In the conventional LSTM [7], the memory cells that
share the same input gate and the same output gate form
a memory block. Putting more than one cells into a block
makes the training of LSTM more efﬁcient. We create an ar-

2

chitecture in which all the cells share a single input gate and
a single forget gate and call it ”LSTM-S”. In this architec-
ture, the cells use individual output gates, since we ﬁnd that
sharing the output gate can be harmful to the performance.
Another network structure we create by adding an input
accumulator to the architecture of LSTM model is what we
call the ”LSTM-A”. In this structure, we add a new set of
memory, input gate and forget gate to the cell to directly
”remember” the input. Eq. 2 gives the architecture of this
model. In the equation, xt, ct, ht are the input, the memory,
and the hidden state, ot, it, ft are the output gate, the input
gate, and the forget gate, c(cid:48)
t, dt are the added memory and
the hidden state for input accumulation, i(cid:48)
t, f (cid:48)
t are the added
input gate and forget gate, σ is the sigmoid function, g is
the activation function which we choose to be tanh, and n
is the L2-normalization function.

content or audio content. Therefore, the visual or audio ac-
tivities do not have to happen spontaneously to be meaning-
ful. We adopt a parallel way that model visual and audio
content with separate LSTM models. The ﬁnal states of the
two LSTM models are then concatenated and go through an
MoE unit. The performance of the parallel model is shown
in Table 4. The number of layers is 2. The number of cells
c and the number of mixtures m in MoE model are shown
in the table.

Model
vanilla LSTM, c = 1024, m = 8
parallel LSTM, cv = 1024, ca = 128, m = 8

GAP
0.8131
0.8161

Table 4: Parallel modeling of visual and audio content is
better than simple feature concatenation.





















ot
mt
it
ft
i(cid:48)
t
f (cid:48)
t
ct
ht
c(cid:48)
t
dt

= TN +2M,6N





ht−1
xt
dt−1





= σ(ft) · ct−1 + σ(it) · g(mt)
= σ(ot) · g(ct)
t) · c(cid:48)
= σ(f (cid:48)
= n(c(cid:48)
t)

t−1 + σ(i(cid:48)

t) · xt

The performance of the two modiﬁed LSTM models are
listed in Table 3. The number of cells per layer are 1024.
The number of mixtures in MoE is set to 8. The number of
layers l is shown in the table. The two models performs bet-
ter than the original model and contribute to the ensemble
because their varieties in model structure.

Model
vanilla LSTM, l = 1
vanilla LSTM, l = 2
LSTM-S, l = 1
LSTM-A, l = 1

GAP
0.8091
0.8131
0.8123
0.8131

Table 3: The performances of the single-block LSTM
(LSTM-S) and the input-accumulator LSTM (LSTM-A),
compared to the original LSTM model.

In the dataset, an example has both visual feature and
audio feature. The naive way of modeling two different
features is to concatenate them into a single feature. We
ﬁnd this to be questionable for two reasons. First, in many
videos such as music videos and family albums, the audio
and the visual content are independent of each other. Also,
people often can make sense of a video only by its visual

3

There is a potential weakness in modeling visual and au-
dio features independently. It might be preferable to allow
the visual network and the audio network to interact at cer-
tain points, since a visual / audio event may be related to a
former audio / visual event in the same video. We leave this
issue to future works.

(2)

3. Label Correlation

In multi-label classiﬁcation settings, an example may be
annotated many labels. Some labels tend to appear in the
same example at the same time, some tend not to. Such in-
formation can be used to improve the performance of multi-
label classiﬁcation models.

Classiﬁer chain [12] create a chain of classiﬁers. One
classiﬁer on the chain predict one label by using not only the
input feature but also the predictions of other labels from the
previous models on the chain. The training of one chain of
classiﬁer involves training L models in which L is the size
of the vocabulary of all labels. However, in real world prob-
lems where the direction of dependency is unknown, an en-
semble of classiﬁer chains is usually used, which makes the
computation complexity even higher and intractable if the
vocabulary of labels is large. A neural network structure
that mimic the classiﬁer chain [11] is proposed to address
the multi-label problem. However, such network structure
has a depth of L that makes it hard to optimize if the vocab-
ulary of labels is large.

We propose a novel end-to-end deep learning structure
that we called ”Chaining” to better utilize the correlation be-
tween labels. In a Chaining model (Figure 2), several repre-
sentations are joined by a chain of MoE model. The predic-
tions are projected to features of lower dimension and used
in the following stages. The representations can be gener-
ated by homogeneous models or heterogeneous ones. We
constantly apply auxiliary cross-entropy loss on the inter-

(a) A unit in Chaining accept one fea-
ture and several model predictions as the
input. The predictions are projected to
lower dimension for efﬁciency.

(b) The whole architecture of Chaining consists of several stages. A Chaining unit
in one stage accepts a feature vector, either from input directly or from a represen-
tation of LSTM or CNN, and all the predictions from earlier stages.

Figure 2: Chaining: a deep learning architecture for multi-label classiﬁcation.

mediate predictions to accelerate the training progress. The
ﬁnal loss function is a weighted average over the loss on
the ﬁnal prediction and the auxiliary losses. We typically
allocate only 10% ∼ 20% of the weights to the auxiliary
losses, since the loss function at the ﬁnal stage is the most
important.

We performed experiments using three basic models,
Mixture-of-Expert, LSTM and CNN. The CNN model we
use in our system is the same as the benchmark method of
sentence classiﬁcation [9], where the length of the ﬁlter is
ﬁxed to the size of feature vector per frame and the resulting
feature map goes through a max-over-time pooling. The ﬁ-
nal state of LSTM and the max-pooled feature map are used
as feature representation in Chaining models and go through
an MoE model for label prediction in the original models.
The performance with and without using Chaining is shown
in Table 5. For the original MoE model, the number of mix-
tures is 16. For the Chaining MoE model, the number of
mixture is 2, the number of stages is 8, and the predictions
are projected into a 128 dimensions vector. For the original
LSTM model, the number of mixtures is 8. For the Chain-
ing LSTM model, the number of mixtures is 4, the number
of stages is 2, and the dimension of projection is 200. For
the original CNN model, the width of ﬁlter and the corre-
sponding numbers of channel are 1×512, 2×512, 3×1024.
For the Chaining CNN model, the corresponding parame-
ters are 1 × 128, 2 × 128, 3 × 256, and the number of stages
is 4. The parameters are chosen to make the original models
have almost equal number of parameters with their Chain-
ing counterparts.

4. Temporal Multi-Scale Information

Input Feature
Video-level, µ
Frame Level, {xv
Frame Level, {xv

1:Fv

1:Fv

Model Original Chaining
0.8106
MoE
0.8172
0.8179

0.7965
} LSTM 0.8131
} CNN
0.7904

Table 5: The performance (GAP) of Mixture-of-Experts,
LSTM and CNN models with and without using Chaining
structure.

age would change with its distance to the observer. How-
ever, temporal information does not have the same rescaling
effect. Therefore it may seems unnatural to model videos
from different temporal scales.

However, we argue that temporal scales matters in video
analysis. A task can be divided into several actions, and
each action involves the interaction between certain objects.
The label annotated to a video can be related to a concept at
different temporal scale: an object, an action, or a task.

One important observation is that video can be seg-
mented into several clips. Each of these clips may contain
content information of a certain aspect. We can make pre-
dictions based on each of these clips and then join them
together to make more accurate predictions. The segmenta-
tion of video is done by either clustering the adjacent frames
[14] or splitting the video into clips of equal durations [15].
We propose a temporal-segment LSTM model (Fig. 3a),
in which the video is split into equal-sized clips, each of
which is modeled by an LSTM model. The models for dif-
ferent clips share the same parameters. The ﬁnal state of
each sequence are treated as another high-level sequence
and modeled by another LSTM model.

Using information from different spatial scales have
been discussed in many image analysis literatures. The
main reason to do so is that the size of an object in an im-

Temporal-pooling LSTM model (Fig. 3a) is a multi-
layer LSTM model in which we inserted temporal k-pooling
layer between LSTM layers. It is similar to the temporal-

4

ition that the representation of low-level sequences contain
more reﬁned features, while the high-level representation is
close to an averaged view of the whole video.

We propose a novel temporal CNN-LSTM model (Fig.
3c) that utilize multi-scale information. This model shares
the previous intuition that adjacent frames can be aggre-
gated to generate features at different temporal scale. In-
stead of direct operating on the original feature, we use
convolution layers to detect patterns in the low-level fea-
tures and combine adjacent ﬁlter outputs by max-pooling
along the time dimension. The ﬁlters used in a convolution
layer are of the same length with the dimension of the fea-
ture, and their widths and channels can be varied. We use
different LSTM models and Moe classiﬁers for the repre-
sentation of the feature maps of different temporal scales.
The predictions generated from features of different scales
are combine using a consensus function. We ﬁnd that av-
eraging is a good consensus function, and maximum would
lead to difﬁculties in convergence.

GAP
Model
0.8131
vanilla LSTM
temporal-pooling LSTM 0.8085
temporal-segment LSTM 0.8122
multi-resolution LSTM
0.8148
multi-scale CNN-LSTM 0.8204

Table 6: Performance of multi-scale models.

Table 6 lists the performances of the models discussed
in this section. The temporal-pooling LSTM model uses 2-
pooling and has 4 LSTM layers (#cells=1024) and 4 MoE
mixtures. The temporal-segment LSTM model uses a dura-
tion of 10 frames for the clips and has 8 MoE mixtures and
2 LSTM layers, each of the layers has 1024 memory unit.
In the multi-resolution LSTM model, we use 2-pooling to
get shorter input sequences; the number of stages in Chain-
ing is 4; the number of mixtures in MoE is 4; the projection
dimension is 256; the LSTM models in it are 2-layer par-
allel LSTM (#cells is 512 for video and 64 for audio). In
the multi-scale CNN-LSTM model, the number of layers in
CNN model is 4; the number of mixtures in MoE is 4; the
LSTM models in it are 1-layer LSTM (#cells=1024); in ev-
ery layer of the CNN model, the width of ﬁlters and the cor-
responding number of channels are 1×256, 2×256, 3×512;
the pooling layers in it are 2-pooling.

5. Identifying Salient Frames with Attention

Not all the frames in a video are equally informative.
There are many reasons that a frame might contribute little
in video classiﬁcation. Images that are too dark, too bright,
or too blurry are hard to analysis for the image classiﬁcation

(a) Temporal-pooling (with the dashes) and temporal-segment
(without the dashes) LSTM model.

(b) Multi-resolution LSTM model.

(c) Multi-scale CNN-LSTM model.

Figure 3: Models that utilize temporal multi-scale informa-
tion.

segment LSTM model. The difference is whether to use the
ﬁnal state of the one clip as the initial state of the next clip
in the ﬁrst layer of the LSTM model.

Instead of directly segmenting videos into clips, we
can aggregate the adjacent frames and gradually construct
features containing long-range information.
In multi-
resolution LSTM model (Fig. 3b), the original features
are average pooled along the time dimension to get shorter
sequences in which each frame covers longer time range.
For each sequence generated in this way, a separate LSTM
model is used to generate a sequence representation. The
representations are then joined with a Chaining model.
The representation from the highest-level goes into the ﬁrst
stage, while the one from the original sequence goes into the
ﬁnal stage of the Chaining model. This is due to the intu-

5

(b) An attention network generates multiple attention
weights for a frame based on the input feature and the
LSTM output at that frame.

(a) The multiple attention pooling scheme. The outputs
of LSTM are pooled with multiple attention weights.

Figure 4: Multiple attention pooling model.

network. Therefore, such frames may not be able to provide
useful semantic information. Also, sometimes a video con-
tains title screens, credits or text. They might be useful if
the frames are processed using optical character recognition
(OCR). However, in this competition, the frames are pre-
processed using an image classiﬁcation network pre-trained
with ImageNet dataset, which means the frame-level fea-
tures may not contain the semantic information in these text.
In addition, there are always frames irrelevant to the theme
of the video. In talk show videos the content of frames will
always be people talking while the topic of the talking is
what really important. In documentaries there are often a
lot of driving scenes which do not reﬂect the theme of the
video.

We propose a multiple attention pooling scheme (multi-
AP) for selecting salient frames in the video. As Fig. 4
shows, an LSTM model is used to deal with the frame-level
features. The outputs of the LSTM model are then aggre-
gated by pooling over the time dimension. The aggregation
is achieved by taking a weighted average over the outputs
of the LSTM model, in which the weights are generated
using an attention network, for which we use a fully con-
nected layer. The aggregated feature is fed into an MoE
model for label predictions. The attention pooling is re-
peated for multiple times and the predictions are combined
using a classiﬁer consensus function. For the choice of com-
bining method, we ﬁnd that maximum performs better than
averaging as a consensus function.

the input and output of the LSTM at a certain frame as the
input. The attention network output K groups of different
attention weights.

Each group of the weights are used to generate a group of
aggregated features and prediction results. The results from
all the groups are then combined by choosing the highest
conﬁdence predicted from all the models for each label l.
The MoE models in Fig. 4a share parameters with each
other.

eik = Wk[xi; yi]
aik = exp(eik)
(cid:80)Fv

i=1 exp(eik)

zk = (cid:80)Fv
i=1 aikyi
pk = M oE(zk)
(pk,l)
pl = max

k

(3)

(4)

We present a few examples outside of the training set to
show what kind of frames is highlighted by the attention
network in the multi-AP model (Fig. 5). For each video,
the frames with the lowest attention weights (left) and the
ones with the highest attention weights (right) are presented.
Since there are many groups of attention weight, the ﬁrst
group is used for this presentation. We have not observed
any noticeable patterns in the inter-group differences of the
weights.

We use a fully connected layer with the softmax acti-
vation function as the attention network (Eq. 3). It takes

In analysis of these examples, we ﬁnd that there are some
patterns about the attention weights. Title screens and the

6

Figure 5: Visualization of the attention weights in the multiple attention pooling model. The frames in each row comes from
the same video. The left four in each row are the frames with the lowest weights, while the right four are the ones with the
highest weights.

frames that is very dark tend to have low attention weights.
Also, if the object of interest is too small or partly blocked,
the weight of that frame tend to be low.

Our work is different from a previous work on attention
pooling [5] in two ways. First, our model uses multiple
groups of attention on the same output sequence and multi-
ple classiﬁers for prediction. The multiple attention scheme
may contribute to generating more stable classiﬁcation re-
sult. Second, our model do an attention pooling over the
outputs of LSTM model which is a global representation
of the sequence while the previous work do a pooling over
local CNN outputs.

We also adopt a local attention pooling scheme that ap-
plies attention pooling over the input features (Eq. 5). The
representation of the LSTM model is also used in the MoE
model for prediction.

The LSTM model in the four models share the same pa-
rameter (#layers=2, #cells=1024). The MoE models have 8
mixtures in the two multi-AP models, while the one in the
local-AP model has 4 mixtures. The two multi-AP models
both have 8 groups of attention weights. And the dimen-
sion of the positional embedding in the positional multi-AP
model is 32.

GAP
Model
0.8131
vanilla LSTM
0.8133
local-AP LSTM
multi-AP LSTM
0.8157
positional multi-AP LSTM 0.8169

Table 7: Performance (GAP) of attention models.

z = (cid:80)Fv

i=1 aixi

p = M oE([yFv ; z])

(5)

6. Bagging and Boosting

We compare the two attention pooling scheme with the
baseline in Table 7. The ”local-AP LSTM” refers to the
scheme using attention pooling over the input feature. The
”multi-AP LSTM” refers to the multiple attention pooling
scheme. The ”positional multi-AP LSTM” model add an
embedding for every frame position to the attention network
on the basis of the multi-AP LSTM model.

The parameters for the models in Table 7 is as follows.

Bootstrap aggregating [2], also called Bagging, is an en-
semble method that creates many versions of a model and
combines their results together. To create one version of a
model, one applies sampling with replacement to get a sub-
set of the original data, in which some original examples
may not present or present more than once[16]. Training on
different subset sampled from the original data would re-
sults in different models. The bagging algorithm is known
for its ability to reduce the variance of a model. We apply

7

Bagging to some of our models, and ﬁnd that Bagging can
generally boost the GAP performance by 0.6% ∼ 1.2%.
The results are shown in Table 8.

weights to 5 to ensure that the algorithm would not place all
the weights on a few formerly misclassiﬁed examples, since
they may not be visually classiﬁable.

Original Bagging
Input Feature Model
0.8225
0.8106
Video-level
0.8216
0.8160
Frame Level
0.8258
Frame Level
0.8179
0.8244
Frame Level multi-AP LSTM 0.8157

Chaining
parallel LSTM
Chaining CNN

Model
Chaining (Video)
parallel LSTM
Chaining CNN
multi-AP LSTM

Original Boosting
0.8218
0.8106
0.8218
0.8160
0.8242
0.8179
0.8246
0.8157

Table 8: Performance (GAP) of bagging models.

Table 9: Performance (GAP) of boosting models.

Boosting [3] is another way to create different versions
of a model. Compared to Bagging which can be run in
parallel, Boosting is a sequential ensemble method. The
(k + 1)th classiﬁer is constructed considering the previous
k classiﬁers by re-sampling a distribution that highlights the
misclassiﬁed examples.

The re-sampling is often implemented by using weighted
examples. If there are N training examples and L labels,
the number of total classiﬁcation results is N × L. Most
works on Boosting in multi-label classiﬁcation use a weight
W N ×L over N samples and L labels, which is computa-
tionally intractable when the vocabulary of label is large, as
is the case in this competition.

We adopt a per-example weighting scheme that assign
a weight W N over the training samples. The weights are
updated with Eq. 6. In the equations, Wk,n is the weight
assigned to the nth example in the training of the kth clas-
siﬁer. Errk,n is the error rate of the nth example by eval-
uating the kth classiﬁer. Errk is the average error rate of
the kth classiﬁer. And Zk is a coefﬁcient that scales the av-
erage value of Wk,n to 1. α is a parameter controlling the
highlighting effect on the misclassiﬁed examples which we
constantly set to 1 in our system.

Wk,n exp(αrkErrk,n)

W0,n = 1.0
Wk+1,n = N
Zk
in which,
rk = log( 1.0−Errk
Errk = 1
N
Errk,n ∈ [0, 1]
Zk = (cid:80)N

Errk
(cid:80)N

)

n=1 Errk,n

n=1 Wk,n exp(αrkErrk,n)

In this algorithm, the weights of the misclassiﬁed ex-
amples are increased in the following classiﬁers. However,
in multi-label classiﬁcation, misclassiﬁcation is hard to de-
ﬁne. We choose the Precision Equal Recall Rate (PERR)
as the implementation of error rate, since it is both per-
example evaluated and in coordinate with the GAP score in
most models (from empirical observation). We also clip the

The comparison between the single models and their
Boosting counterparts are shown in Table 9. The Boosting
algorithm generates a performance boost similar as Bagging
does.

7. Cascade Classiﬁer Training

Adding models into the ensemble would usually make
the performance better. However, with the number of mod-
els in the ensemble model increasing, the gain from newly
added models tends to diminish, especially if one add mod-
els that are similar to the existing models. We address this
problem by using cascade training in which the predictions
of other models as a part of its input of the model during
training.

(6)

Figure 6: Cascade layer as a replacement for MoE in cas-
cade classiﬁer training.

The structure of cascade layer is shown in Fig. 6. The
predictions from the other models are averaged to generate
an averaged prediction, which is then projected to a feature
of low dimension. All the models that have MoE as their
last layer can be modiﬁed into a cascade model by having
their MoE layer replaced by the cascade layer.

8. Stacking Methods

Stacking is an ensemble method that uses a machine
learning model to combine the predictions of many mod-
els into one prediction. We use the ”train2” set (Section 1)

8

as the training set for stacking. The trivial case of stacking
is to simply average all the predictions. If there are M mod-
els and L labels in total, simple averaging the predictions of
all models can be written as Eq. 7.

pl =

1
M

M
(cid:88)

m=1

pm,l

(7)

Linear weighted averaging (Eq.

8) is also a simple
scheme. This method has M weights, one for each model
in the ensemble.

M
(cid:88)

m=1

M
(cid:88)

m=1

pl =

wmpm,l, s.t.

wm = 1, wm > 0

(8)

1
M

M
(cid:88)

m=1

For multi-label classiﬁcation, there are L predictions for
every example. Some models might perform better than the
others on a subset of the labels. Therefore, we could extend
linear weighted averaging to every class. We denote this
method class-wise weighted averaging (Eq. 9).

1
M

M
(cid:88)

m=1

pl =

wm,lpm,l, s.t.

wm,l = 1, wm,l > 0

(9)
While the ensemble methods often deal with the predic-
tions of individual models, the original input may also help
to decide which models to trust. The winning team of Net-
ﬂix Prize proposes the Feature-Weighted Linear Stacking
[13] to utilize the information in meta-features. Although
there are no useful meta-feature in the Youtube-8M dataset,
we consider the averaged frame input and the averaged pre-
dictions from individual models as useful indicators. We
propose the attention weighted stacking to utilized these in-
formation.

The attention weights α (Eq. 10) are generated with a
fully connected layer with softmax as the activation func-
tion and averaged input ¯x and prediction ¯p as the input of
the layer.

The predictions are then weighted averaged as shown in
Eq. 11. The weight is a mixture of K low-rank matrices
(rank D) in which the matrices are selected by the output
of the attention network (Eq. 10). In the attention weighted
stacking, V, A, B, a, b, c are trainable parameters.

We perform a comparison among the stacking methods
on an ensemble of 74 models (model #1 - #74 in Appendix
A). The number of attention weights is 16 and the rank
of the matrix components is 4 in the attention weighted
stacking method. The comparison shows that the attention
weighted stacking performs better than the other methods
(Table 10).

Stacking method
Simple Averaging
Linear Weighted Averaging
Class-wise Weighted Averaging
Attention Weighted Stacking

GAP
0.8436
0.8449
0.8453
0.8458

Table 10: Performance (GAP) of different stacking models
on the entire set of models (74 models). The GAP scores
in this table is evaluated with the blind test set (the private
leaderboard score in the competition).

9. Distilling Knowledge from Ensemble

Ensemble models might perform much better than even
the best single model. However, in real world systems, the
memory and computation resources are often too limited
for ensemble models to deploy. Distillation [6] is a training
scheme that aims to compress the knowledge in ensemble
model to a single model. Another motivation of using dis-
tillation is the observation that the labels are often wrong
or incomplete, especially when the vocabulary of label is
large.

We adopt a training scheme that uses the predictions of
ensemble ˆp as a soft target along with the real target l during
training. The new loss function is a weighted average of two
losses, one for each target (Eq. 12).

α = σ(V [¯x; ¯p])

¯x = 1
Fv

(cid:80)Fv

i=1 xi

¯p = 1
M

(cid:80)M

m=1 pm

pl

= 1
M

(cid:80)M

m=1 wm,lpm,l

wm,l =

(cid:80)M

e

= (cid:80)K

exp(em,l)
m=1 exp(em,l)
k=1 αk(AT

s.t.

Ak ∈ RD×M , Bk ∈ RD×L
ak ∈ RM ×1, bk ∈ RL×1
ck ∈ R

(10)

ˆL = (1 − λ)ce(p, l) + λce(p, ˆp)

(12)

We use the predictions of model #75 (Appendix A.9) as
the soft target in training and train several models with the
same parameters as the original models. The comparison is
listed in Table 11. The comparison shows that distillation
can greatly boost single model performance (often better
than Bagging).

In the previous sections, we review our system, present
our intuition, and show how we address the problems in

9

k Bk + akbT

k + ck)

(11)

10. Summary

Original Distillation
Model
0.8106
Chaining (Video)
0.8160
parallel LSTM
0.8179
Chaining CNN
0.8172
Chaining LSTM
Multi-scale CNN-LSTM 0.8204

0.8169
0.8237
0.8266
0.8291
0.8258

Table 11: Performance (GAP) of single models trained us-
ing distillation.

multi-label video classiﬁcation. We ﬁnd that attention pool-
ing and temporal multi-scale information is very important
for video sequence modeling. We also propose a network
structure for large-vocabulary multi-label classiﬁcation. We
review our work in ensemble methods such as Bagging,
Boosting, Cascade, Distillation and Stacking. We propose a
stacking network that uses attention to weight models. Our
system road-map is shown in Table 12.

Changes
27 single models
+ 11 bagging & boosting models
+ 8 distillation models
+ 11 cascade models
+ 17 more cascade models
Attention Weighted Stacking

GAP
0.8425
0.8435
0.8437
0.8451
0.8453
0.8458

Table 12: Performance (GAP) of ensemble models. The
stacking method used for the ﬁrst 5 results is class-wise
weighted model. The ensemble in each row include all the
changes in the rows above it. The results reported in this
table are evaluated on the blind test set (the private leader-
board score in the competition).

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m:
CoRR,
A large-scale video classiﬁcation benchmark.
abs/1609.08675, 2016.

[2] L. Breiman.

Bagging predictors. Machine Learning,

24(2):123–140, 1996.

[3] L. Breiman. Arcing classiﬁer (with discussion and a rejoin-
der by the author). Ann. Statist., 26(3):801–849, 06 1998.
[4] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Car-
los Niebles. Activitynet: A large-scale video benchmark for
human activity understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 961–970, 2015.

[5] M. J. Er, Y. Zhang, N. Wang, and M. Pratama. Atten-
tion pooling-based convolutional neural network for sen-
tence modelling. Information Sciences, 373:388 – 403, 2016.
[6] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[8] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, pages
1725–1732, 2014.

[9] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[10] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. CoRR, abs/1412.6980, 2014.

[11] J. Read and J. Hollm´en. A deep interpretation of classi-
ﬁer chains. In International Symposium on Intelligent Data
Analysis, pages 251–262. Springer, 2014.

[12] J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classiﬁer
chains for multi-label classiﬁcation. Machine Learning and
Knowledge Discovery in Databases, pages 254–269, 2009.

[13] J. Sill, G. Tak´acs, L. Mackey, and D. Lin. Feature-weighted
linear stacking. arXiv preprint arXiv:0911.0460, 2009.
[14] E. H. Spriggs, F. De La Torre, and M. Hebert. Temporal seg-
mentation and activity classiﬁcation from ﬁrst-person sens-
ing. In Computer Vision and Pattern Recognition Workshops,
2009. CVPR Workshops 2009. IEEE Computer Society Con-
ference On, pages 17–24. IEEE, 2009.

[15] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and
towards good
L. Van Gool. Temporal segment networks:
practices for deep action recognition. In European Confer-
ence on Computer Vision, pages 20–36. Springer, 2016.

[16] Z.-H. Zhou. Ensemble methods:

foundations and algo-

rithms. CRC press, 2012.

10

Appendix A. Final Submission

In this section we list all the components included in our
ﬁnal submission. We apply a stacking method called the At-
tention Weighted Average (Section 8) to merge the result of
these models instead of carrying on careful model selection.
Therefore it might be possible that a subset of these models
could reach equal or better performance. The parameters
are for a quick grasp of the structure of the models. Re-
fer to our code 2 if you are interested in the implementation
details. The GAP scores reported in the appendix are eval-
uated with the ”validate2” set if not otherwise speciﬁed.

A.1. Video-level Models

1. Chaining model (Section 3). During training, if the
loss of the current batch is less than 10, use the most conﬁ-
dent tag predicted as a soft target for every example in the
batch. #mixture of MoE is 4. #stage of Chaining is 3. pro-
jection dimension in Chaining is 100. GAP = 0.8067.

2. Chaining model. #mixture of MoE is 2. #stage of
Chaining is 8. projection dimension in Chaining is 128.
GAP = 0.8106.

A.2. Baseline Models

If not otherwise speciﬁed, the memory cells of the ﬁnal
state are used for the input of the MoE model in the baseline
models.

3. LSTM model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8131.
4. LSTM model. #cell of LSTM is 2048. #layer of

LSTM is 2. #mixture of MoE is 4. GAP = 0.8152.

5. LSTM model of 2 layers. The ﬁrst layer is bi-
directional. The second layer is uni-directional. #cell of
LSTM is 1024,. #mixture of MoE is 4. GAP = 0.8105.

6. LSTM-S model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8123.
7. LSTM-A model (Section 2). #cell of LSTM is 1024.
#layer of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.
8. LSTM-A model of 2 layers. The ﬁrst layer is a for-
ward LSTM-A model. The second layer is a backward
LSTM-A model that takes the original input and the out-
put of the ﬁrst layer as input. #cell of LSTM is 1024. #layer
of LSTM is 1. #mixture of MoE is 8. GAP = 0.8131.

9. Parallel LSTM model (Section 2). #cell of LSTM is
1024. #layer of LSTM is 2. #mixture of MoE is 8. GAP =
0.8161.

10. Parallel LSTM model. The gated outputs of the ﬁ-
nal state are used for the input of the MoE model. #cell of
LSTM is 1024. #layer of LSTM is 2. #mixture of MoE is
8. GAP = 0.8160.

11. LSTM model with data augmentation by random
sampling 50% of the frames. #cell of LSTM is 1024. #layer

2https://github.com/wangheda/youtube-8m

of LSTM is 2. #mixture of MoE is 8. GAP = 0.8137.

12. CNN-LSTM model. The CNN model is described
in section 3. #layer of CNN is 1. The widths of ﬁlter are
1, 2 and 3. The corresponding #channels of ﬁlters are 1024,
1024, and 1024. #cell of LSTM is 1024. #layer of LSTM is
2. #mixture of MoE is 8. GAP = 0.8103.

A.3. Temporal Multi-Scale Models

13. Temporal-segment LSTM model (Section 4). The
duration of each clip is 10 frames. #cell of LSTM is 1024.
#layer of LSTM is 2. #mixture of MoE is 8. GAP = 0.8122.
14. Temporal-pooling LSTM model (Section 4). The
model has 2-pooling between LSTM layers. #cell of LSTM
is 1024. #layer of LSTM is 4, #mixture of MoE is 8. GAP
= 0.8085.

15. Multi-resolution LSTM model (Section 4). The
model has parallel LSTM for sequence modeling. The
model has 2-pooling to get shorter representation of input.
#cell of LSTM is 512 for video and 64 for audio. #layer
of LSTM is 2. #stage of Chaining is 4. The dimension of
projection in Chaining is 256. #mixture of MoE is 4. GAP
= 0.8149.

16. Multi-scale CNN-LSTM model (Section 4). The
widths of ﬁlter are 1, 2 and 3. The corresponding #chan-
nels of ﬁlters are 256, 256, and 512. #layer in CNN is 4.
#layer of LSTM is 1. #cell of LSTM is 1024. #mixture of
MoE is 4. GAP = 0.8204.

17. Multi-scale CNN-LSTM model. All the parameters
are the same as model #16, except that the type of the LSTM
model is LSTM-S (as in model #6). GAP = 0.8147.

A.4. Chaining Models

18. Chaining CNN model (Section 3). The widths of ﬁl-
ter are 1, 2 and 3. The corresponding #channels of ﬁlters are
128, 128, and 256. The dimension of projection in Chaining
is 256. #mixture of MoE is 4. #stage of Chaining is 4. GAP
= 0.8179.

19. Chaining Deep CNN model. In this model a 3-layer
CNN model is used. The averaged input feature and the
3 max-pooled feature maps from the 3 layers of CNN are
combined using a 4-stage Chaining model. The widths of
ﬁlter are 1, 2 and 3. The corresponding #channels of ﬁl-
ters are 128, 128, and 256. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. GAP = 0.8155.

20. Chaining LSTM-CNN model. This model uses a
cascade of parallel LSTM and CNN model as the sub-model
in Chaining.#layer of LSTM is 1. #cell of LSTM is 1024 for
video and 128 for audio. The widths of ﬁlter are 1, 2 and
3. The corresponding #channels of ﬁlters are 128, 128, and
256. The dimension of projection in Chaining is 128. #stage
in Chaining is 3. #mixture of MoE is 4. GAP = 0.8122.

21. Chaining LSTM model (Section 3). #layer of LSTM
is 2. #cell of LSTM is 1024. The dimension of projection in

11

Chaining is 200. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8172.

22. Chaining LSTM model. The different stages in
Chaining model uses a shared input which is the cell mem-
ory of the ﬁnal state of an LSTM model. #layer of LSTM is
2. #cell of LSTM is 1024. The dimension of projection in
Chaining is 256. #stage in Chaining is 2. #mixture of MoE
is 4. GAP = 0.8162.

A.5. Attention Pooling Models

23. Local attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. GAP =
0.8133.

24. Attention pooling LSTM model that has one LSTM
model to generate the attention weights for pooling over the
output of another LSTM model. #layer of LSTM is 1. #cell
of LSTM is 1024. #mixture of MoE is 8. GAP = 0.8088.

25. Multiple attention pooling LSTM model (Section 5).
#layer of LSTM is 2. #cell of LSTM is 1024. #mixture of
MoE is 4. #group of attention weights is 8. GAP = 0.8157.
26. Multiple attention pooling LSTM model that has one
LSTM model to generate the attention weights for pooling
over the output of another LSTM model. #layer of LSTM
is 2. #cell of LSTM is 1024. #mixture of MoE is 4. #group
of attention weights is 8. GAP = 0.8081.

27. Multiple attention pooling LSTM model with posi-
tional embedding (Section 5). #layer of LSTM is 2. #cell
of LSTM is 1024. #mixture of MoE is 4. The dimension of
positional embedding is 32. #group of attention weights is
8. GAP = 0.8169.

A.6. Bagging and Boosting Models

GAP = 0.8218.

33. Bagging model of 8 versions of model #18, com-
bined with class-wise weighted averaging. GAP = 0.8258.
34. Boosting model of 8 versions of model #18, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8246.

35. Bagging model of 8 versions of model #25, com-
bined with class-wise weighted averaging. GAP = 0.8244.
36. Boosting model of 8 versions of model #25, com-
bined with attention weighted stacking, with weight clip-
ping. GAP = 0.8242.

37. Bagging model of 8 versions of model #10, com-

bined with simple averaging. GAP = 0.8216.

38. Boosting model of 8 versions of model #10, com-
bined with class-wise weighted averaging, with weight clip-
ping. GAP = 0.8218.

A.7. Distillation Models

The models in this section is trained using distillation
(Section 9). During training, the predictions from model
#75 are used as soft targets.

39. Model #2 with distillation training. GAP = 0.8169.
40. Model #18 with distillation training. GAP = 0.8266.
41. Model #20 with distillation training. GAP = 0.8259.
42. Model #10 with distillation training. GAP = 0.8237.
43. Model #21 with distillation training. GAP = 0.8291.
44. Model #16 with distillation training. GAP = 0.8258.
45. Bagging model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8249.

46. Boosting model of 4 versions of model #2 with dis-

tillation training. GAP = 0.8254.

28. Bagging model of 8 versions of model #2, combined

with simple averaging. GAP = 0.8225.

A.8. Cascade Models

29. Ensemble of 4 Chaining models. The ﬁrst is the
normal Chaining model with video-level input. The second
is a Chaining model with weighted cross entropy loss. The
third is a Chaining model that predict label and top-level
verticals at the same time. The fourth is a Chaining model
that predict the infrequent labels with softmax function. All
4 models share the same parameters. #mixture of MoE is
4. #stage in Chaining is 3. The dimension of projection in
Chaining is 100. GAP = 0.8216.

30. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging. Remove the ex-
amples with too high weights from training set, since it may
be hopeless to predict these examples correctly. GAP =
0.8213.

31. Boosting model of 8 versions of model #2, com-
bined with class-wise weighted averaging, without weight
clipping. GAP = 0.8198.

32. Boosting model of 8 versions of model #2, combined
with class-wise weighted averaging, with weight clipping.

The models in this section are models using the predic-

tion of other models as part of the input (Section 7).

47. Model #2 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8231.

48. Model #6 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8245.

49. Model #18 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8268.

50. Model #25 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

51. Chaining CNN model with cascade training using
the prediction of model #75 as part of the input. The widths
of ﬁlter are 1, 2 and 3. The corresponding #channels of
ﬁlters are 256, 256, and 512. The dimension of projection in
Chaining is 256. #mixture of MoE is 4. #stage of Chaining
is 2. GAP = 0.8214.

52. Model #3 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8267.

12

74. Model #2 with cascade training using the L1-
normalized prediction of model #76 as part of the input.
GAP = 0.8227.

A.9. Ensemble Models

In this section we lists the performance of some of the
ensemble models. Alongside the GAP score on ”validate2”
set, we also report the GAP score on the blind test set (Pri-
vate LeaderBoard in the competition).

75. Ensemble of 4 single models. Model #2, #10, #18,
and #25 with a class-wise weighted stacking gets a GAP of
0.8373. Private LB GAP = 0.83703.

76. Ensemble of 8 single models. Model #2, #10, #18,
#25, #47, #48, #49, and #50 with a class-wise weighted
stacking gets a GAP of 0.8397. Private LB GAP = 0.83941.
77. Ensemble of 27 single models. Model #1 - #27 with a
class-wise weighted stacking gets a GAP of 0.8427. Private
LB GAP = 0.84250.

78. Ensemble of 57 models. Model #1 - #57 with an
attention weighted stacking (#rank=3, #attention=16) gets a
GAP of 0.8459. Private LB GAP = 0.84561.

79. Ensemble of 74 models. Model #1 - #74 with an
attention weighted stacking (#rank=4, #attention=16) gets a
GAP of 0.8462. Private LB GAP = 0.84583.

80. Ensemble of model #78 and #79 by getting the most
conﬁdent 20 tags from each model and averaging the con-
ﬁdence scores. This is done directly on the test set without
training. Private LB GAP = 0.84590. This is our ﬁnal sub-
mission.

Appendix B. Source Code

The source code of our solution is made public on
GitHub. Visit https://github.com/wangheda/youtube-8m for
the implementation details of our models.

53. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 2.
GAP = 0.8214.

54. Model #16 with cascade training using the prediction
of model #75 as part of the input. #stage of Chaining is 4.
GAP = 0.8266.

55. Model #20 with cascade training using the prediction

of model #75 as part of the input. GAP = 0.8265.

56. Model #6 with cascade training using the prediction
of model #75 as part of the input. The prediction of model
#6 for infrequent labels and the prediction of the cascade
model for frequent ones are joined together as the ﬁnal pre-
diction. GAP = 0.8228.

57. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8202.

58. Model #25 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8254.

59. Model #20 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8250.

60. Model #6 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8245.

61. Model #7 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8247.

62. Model #21 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8251.

63. Model #16 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8258.

64. Model #10 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8248.

65. Model #10 with cascade training using the prediction
of model #76 as part of the input. The prediction of model
#76 is also used to up-sample the misclassiﬁed samples as
in the Boosting model. GAP = 0.8218.

66. Model #2 with cascade training using the prediction

of model #76 as part of the input. GAP = 0.8181.

67. Model #2 with cascade training using the predictions

of model #76 and #66 as part of the input. GAP = 0.7989.

68. Model #2 with cascade training using the predictions
of model #76, #66, and #67 as part of the input. GAP =
0.7849.

69. Model #2 with cascade training using the predictions
of model #76, #66, #67, and #68 as part of the input. GAP
= 0.7753.

70. Model #2 with cascade training using the predictions
of model #76, #66, #67, #68, and #69 as part of the input.
GAP = 0.7833.

71. Model #2 with cascade training using the predictions

of model #76 as part of the input. GAP = 0.8183.

72. Model #2 with cascade training using the predictions

of model #76 and #71 as part of the input. GAP = 0.8085.

73. Model #2 with cascade training using the predictions
of model #76, #71, and #72 as part of the input. GAP =
0.8111.

13

