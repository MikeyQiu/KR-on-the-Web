8
1
0
2
 
b
e
F
 
2
2
 
 
]

G
L
.
s
c
[
 
 
3
v
6
2
9
1
0
.
7
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

DIFFUSION CONVOLUTIONAL RECURRENT NEURAL
NETWORK: DATA-DRIVEN TRAFFIC FORECASTING

Yaguang Li†, Rose Yu‡, Cyrus Shahabi†, Yan Liu†
† University of Southern California, ‡ California Institute of Technology
† {yaguang, shahabi, yanliu.cs}@usc.edu, ‡ rose@caltech.edu

ABSTRACT
Spatiotemporal forecasting has various applications in neuroscience, climate and
transportation domain. Trafﬁc forecasting is one canonical example of such learn-
ing task. The task is challenging due to (1) complex spatial dependency on road
networks, (2) non-linear temporal dynamics with changing road conditions and
(3) inherent difﬁculty of long-term forecasting. To address these challenges, we
propose to model the trafﬁc ﬂow as a diffusion process on a directed graph and
introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep
learning framework for trafﬁc forecasting that incorporates both spatial and tem-
poral dependency in the trafﬁc ﬂow. Speciﬁcally, DCRNN captures the spatial
dependency using bidirectional random walks on the graph, and the temporal de-
pendency using the encoder-decoder architecture with scheduled sampling. We
evaluate the framework on two real-world large scale road network trafﬁc datasets
and observe consistent improvement of 12% - 15% over state-of-the-art baselines.

INTRODUCTION

1
Spatiotemporal forecasting is a crucial task for a learning system that operates in a dynamic environ-
ment. It has a wide range of applications from autonomous vehicles operations, to energy and smart
grid optimization, to logistics and supply chain management. In this paper, we study one important
task: trafﬁc forecasting on road networks, the core component of the intelligent transportation systems.
The goal of trafﬁc forecasting is to predict the future trafﬁc speeds of a sensor network given historic
trafﬁc speeds and the underlying road networks.

This task is challenging mainly due to the com-
plex spatiotemporal dependencies and inher-
ent difﬁculty in the long term forecasting. On
the one hand, trafﬁc time series demonstrate
strong temporal dynamics. Recurring incidents
such as rush hours or accidents can cause non-
stationarity, making it difﬁcult to forecast long-
term. On the other hand, sensors on the road
network contain complex yet unique spatial cor-
relations. Figure 1 illustrates an example. Road
1 and road 2 are correlated, while road 1 and
road 3 are not. Although road 1 and road 3
are close in the Euclidean space, they demon-
strate very different behaviors. Moreover, the
future trafﬁc speed is inﬂuenced more by the
downstream trafﬁc than the upstream one. This
means that the spatial structure in trafﬁc is non-
Euclidean and directional.

Figure 1: Spatial correlation is dominated by road
network structure. (1) Trafﬁc speed in road 1 are
similar to road 2 as they locate in the same highway.
(2) Road 1 and road 3 locate in the opposite direc-
tions of the highway. Though close to each other
in the Euclidean space, their road network distance
is large, and their trafﬁc speeds differ signiﬁcantly.

Trafﬁc forecasting has been studied for decades,
falling into two main categories: knowledge-
driven approach and data-driven approach. In transportation and operational research, knowledge-
driven methods usually apply queuing theory and simulate user behaviors in trafﬁc (Cascetta, 2013).
In time series community, data-driven methods such as Auto-Regressive Integrated Moving Average
(ARIMA) model and Kalman ﬁltering remain popular (Liu et al., 2011; Lippi et al., 2013). However,
simple time series models usually rely on the stationarity assumption, which is often violated by

1

Published as a conference paper at ICLR 2018

the trafﬁc data. Most recently, deep learning models for trafﬁc forecasting have been developed
in Lv et al. (2015); Yu et al. (2017b), but without considering the spatial structure. Wu & Tan (2016)
and Ma et al. (2017) model the spatial correlation with Convolutional Neural Networks (CNN), but
the spatial structure is in the Euclidean space (e.g., 2D images). Bruna et al. (2014), Defferrard et al.
(2016) studied graph convolution, but only for undirected graphs.

In this work, we represent the pair-wise spatial correlations between trafﬁc sensors using a directed
graph whose nodes are sensors and edge weights denote proximity between the sensor pairs measured
by the road network distance. We model the dynamics of the trafﬁc ﬂow as a diffusion process and
propose the diffusion convolution operation to capture the spatial dependency. We further propose
Diffusion Convolutional Recurrent Neural Network (DCRNN) that integrates diffusion convolution,
the sequence to sequence architecture and the scheduled sampling technique. When evaluated on real-
world trafﬁc datasets, DCRNN consistently outperforms state-of-the-art trafﬁc forecasting baselines
by a large margin. In summary:

• We study the trafﬁc forecasting problem and model the spatial dependency of trafﬁc as
a diffusion process on a directed graph. We propose diffusion convolution, which has an
intuitive interpretation and can be computed efﬁciently.

• We propose Diffusion Convolutional Recurrent Neural Network (DCRNN), a holistic ap-
proach that captures both spatial and temporal dependencies among time series using
diffusion convolution and the sequence to sequence learning framework together with sched-
uled sampling. DCRNN is not limited to transportation and is readily applicable to other
spatiotemporal forecasting tasks.

• We conducted extensive experiments on two large-scale real-world datasets, and the proposed

approach obtains signiﬁcant improvement over state-of-the-art baseline methods.

2 METHODOLOGY
We formalize the learning problem of spatiotemporal trafﬁc forecasting and describe how to model
the dependency structures using diffusion convolutional recurrent neural network.

2.1 TRAFFIC FORECASTING PROBLEM
The goal of trafﬁc forecasting is to predict the future trafﬁc speed given previously observed trafﬁc
ﬂow from N correlated sensors on the road network. We can represent the sensor network as a
weighted directed graph G = (V, E, W ), where V is a set of nodes |V| = N , E is a set of edges and
W ∈ RN ×N is a weighted adjacency matrix representing the nodes proximity (e.g., a function of
their road network distance). Denote the trafﬁc ﬂow observed on G as a graph signal X ∈ RN ×P ,
where P is the number of features of each node (e.g., velocity, volume). Let X (t) represent the graph
signal observed at time t, the trafﬁc forecasting problem aims to learn a function h(·) that maps T (cid:48)
historical graph signals to future T graph signals, given a graph G:

[X (t−T (cid:48)+1), · · · , X (t); G]

h(·)
−−→ [X (t+1), · · · , X (t+T )]

2.2 SPATIAL DEPENDENCY MODELING

We model the spatial dependency by relating trafﬁc ﬂow to a diffusion process, which explicitly
captures the stochastic nature of trafﬁc dynamics. This diffusion process is characterized by a
random walk on G with restart probability α ∈ [0, 1], and a state transition matrix D−1
O W . Here
DO = diag(W 1) is the out-degree diagonal matrix, and 1 ∈ RN denotes the all one vector. After
many time steps, such Markov process converges to a stationary distribution P ∈ RN ×N whose ith
row P i,: ∈ RN represents the likelihood of diffusion from node vi ∈ V, hence the proximity w.r.t.
the node vi. The following Lemma provides a closed form solution for the stationary distribution.
Lemma 2.1. (Teng et al., 2016) The stationary distribution of the diffusion process can be represented
as a weighted combination of inﬁnite random walks on the graph, and be calculated in closed form:

P =

α(1 − α)k (cid:0)D−1

O W (cid:1)k

∞
(cid:88)

k=0

(1)

where k is the diffusion step. In practice, we use a ﬁnite K-step truncation of the diffusion process
and assign a trainable weight to each step. We also include the reversed direction diffusion process,

2

Published as a conference paper at ICLR 2018

such that the bidirectional diffusion offers the model more ﬂexibility to capture the inﬂuence from
both the upstream and the downstream trafﬁc.

Diffusion Convolution The resulted diffusion convolution operation over a graph signal X ∈
RN ×P and a ﬁlter fθ is deﬁned as:

X:,p (cid:63)G fθ =

θk,1

(cid:0)D−1

O W (cid:1)k

+ θk,2

(cid:0)D−1

I W (cid:124)(cid:1)k(cid:17)

X:,p

for p ∈ {1, · · · , P }

(2)

K−1
(cid:88)

(cid:16)

k=0

I W (cid:124) represent the transition
where θ ∈ RK×2 are the parameters for the ﬁlter and D−1
O W , D−1
matrices of the diffusion process and the reverse one, respectively.
In general, computing the
convolution can be expensive. However, if G is sparse, Equation 2 can be calculated efﬁciently using
O(K) recursive sparse-dense matrix multiplication with total time complexity O(K|E|) (cid:28) O(N 2).
See Appendix B for more detail.

Diffusion Convolutional Layer With the convolution operation deﬁned in Equation 2, we can
build a diffusion convolutional layer that maps P -dimensional features to Q-dimensional outputs.
Denote the parameter tensor as Θ ∈ RQ×P ×K×2 = [θ]q,p, where Θq,p,:,: ∈ RK×2 parameterizes
the convolutional ﬁlter for the pth input and the qth output. The diffusion convolutional layer is thus:

H:,q = a

X:,p (cid:63)G fΘq,p,:,:

for q ∈ {1, · · · , Q}

(3)

(cid:32) P

(cid:88)

p=1

(cid:33)

where X ∈ RN ×P is the input, H ∈ RN ×Q is the output, {fΘq,p,,: } are the ﬁlters and a is the
activation function (e.g., ReLU, Sigmoid). Diffusion convolutional layer learns the representations
for graph structured data and we can train it using stochastic gradient based method.

Relation with Spectral Graph Convolution Diffusion convolution is deﬁned on both directed and
undirected graphs. When applied to undirected graphs, we show that many existing graph structured
convolutional operations including the popular spectral graph convolution, i.e., ChebNet (Defferrard
et al., 2016), can be considered as a special case of diffusion convolution (up to a similarity transfor-
mation). Let D denote the degree matrix, and L = D− 1
2 be the normalized graph
Laplacian, the following Proposition demonstrates the connection.
Proposition 2.2. The spectral graph convolution deﬁned as

2 (D − W )D− 1

X:,p (cid:63)G fθ = Φ F (θ) Φ(cid:124)X:,p

with eigenvalue decomposition L = ΦΛΦ(cid:124) and F (θ) = (cid:80)K−1
diffusion convolution up to a similarity transformation, when the graph G is undirected.

θkΛk, is equivalent to graph

0

Proof. See Appendix C.

2.3 TEMPORAL DYNAMICS MODELING
We leverage the recurrent neural networks (RNNs) to model the temporal dependency. In particular,
we use Gated Recurrent Units (GRU) (Chung et al., 2014), which is a simple yet powerful variant of
RNNs. We replace the matrix multiplications in GRU with the diffusion convolution, which leads to
our proposed Diffusion Convolutional Gated Recurrent Unit (DCGRU).

r(t) =
C(t) = tanh(ΘC (cid:63)G

σ(Θr (cid:63)G [X (t), H (t−1)] + br)

u(t) = σ(Θu (cid:63)G [X (t), H (t−1)] + bu)

(cid:2)X (t), (r(t) (cid:12) H (t−1))(cid:3) + bc) H (t) = u(t) (cid:12) H (t−1) + (1 − u(t)) (cid:12) C(t)

where X (t), H (t) denote the input and output of at time t, r(t), u(t) are reset gate and update gate at
time t, respectively. (cid:63)G denotes the diffusion convolution deﬁned in Equation 2 and Θr, Θu, ΘC are
parameters for the corresponding ﬁlters. Similar to GRU, DCGRU can be used to build recurrent
neural network layers and be trained using backpropagation through time.

In multiple step ahead forecasting, we employ the Sequence to Sequence architecture (Sutskever
et al., 2014). Both the encoder and the decoder are recurrent neural networks with DCGRU. During
training, we feed the historical time series into the encoder and use its ﬁnal states to initialize the

3

Published as a conference paper at ICLR 2018

Figure 2: System architecture for the Diffusion Convolutional Recurrent Neural Network designed
for spatiotemporal trafﬁc forecasting. The historical time series are fed into an encoder whose ﬁnal
states are used to initialize the decoder. The decoder makes predictions based on either previous
ground truth or the model output.

decoder. The decoder generates predictions given previous ground truth observations. At testing time,
ground truth observations are replaced by predictions generated by the model itself. The discrepancy
between the input distributions of training and testing can cause degraded performance. To mitigate
this issue, we integrate scheduled sampling (Bengio et al., 2015) into the model, where we feed the
model with either the ground truth observation with probability (cid:15)i or the prediction by the model with
probability 1 − (cid:15)i at the ith iteration. During the training process, (cid:15)i gradually decreases to 0 to allow
the model to learn the testing distribution.

With both spatial and temporal modeling, we build a Diffusion Convolutional Recurrent Neural
Network (DCRNN). The model architecture of DCRNN is shown in Figure 2. The entire network is
trained by maximizing the likelihood of generating the target future time series using backpropagation
through time. DCRNN is able to capture spatiotemporal dependencies among time series and can be
applied to various spatiotemporal forecasting problems.

3 RELATED WORK

Trafﬁc forecasting is a classic problem in transportation and operational research which are primarily
based on queuing theory and simulations (Drew, 1968). Data-driven approaches for trafﬁc forecasting
have received considerable attention, and more details can be found in a recent survey paper (Vla-
hogianni et al., 2014) and the references therein. However, existing machine learning models either
impose strong stationary assumptions on the data (e.g., auto-regressive model) or fail to account for
highly non-linear temporal dependency (e.g., latent space model Yu et al. (2016); Deng et al. (2016)).
Deep learning models deliver new promise for time series forecasting problem. For example, in Yu
et al. (2017b); Laptev et al. (2017), the authors study time series forecasting using deep Recurrent
Neural Networks (RNN). Convolutional Neural Networks (CNN) have also been applied to trafﬁc
forecasting. Zhang et al. (2016; 2017) convert the road network to a regular 2-D grid and apply
traditional CNN to predict crowd ﬂow. Cheng et al. (2017) propose DeepTransport which models the
spatial dependency by explicitly collecting upstream and downstream neighborhood roads for each
individual road and then conduct convolution on these neighborhoods respectively.

Recently, CNN has been generalized to arbitrary graphs based on the spectral graph theory. Graph
convolutional neural networks (GCN) are ﬁrst introduced in Bruna et al. (2014), which bridges the
spectral graph theory and deep neural networks. Defferrard et al. (2016) propose ChebNet which
improves GCN with fast localized convolutions ﬁlters. Kipf & Welling (2017) simplify ChebNet
and achieve state-of-the-art performance in semi-supervised classiﬁcation tasks. Seo et al. (2016)
combine ChebNet with Recurrent Neural Networks (RNN) for structured sequence modeling. Yu
et al. (2017a) model the sensor network as a undirected graph and applied ChebNet and convolutional
sequence model (Gehring et al., 2017) to do forecasting. One limitation of the mentioned spectral
based convolutions is that they generally require the graph to be undirected to calculate meaningful

4

Published as a conference paper at ICLR 2018

Table 1: Performance comparison of different approaches for trafﬁc speed forecasting. DCRNN
achieves the best performance with all three metrics for all forecasting horizons, and the advantage
becomes more evident with the increase of the forecasting horizon.

T

Metric

HA

ARIMAKal

VAR

SVR

FNN

FC-LSTM DCRNN

A
L
-
R
T
E
M

Y
A
B
-
S
M
E
P

15 min

30 min

1 hour

15 min

30 min

1 hour

4.16
MAE
RMSE
7.80
MAPE 13.0%
4.16
MAE
RMSE
7.80
MAPE 13.0%
4.16
MAE
RMSE
7.80
MAPE 13.0%

MAE
RMSE
MAPE
MAE
RMSE
MAPE
MAE
RMSE
MAPE

2.88
5.59
6.8%
2.88
5.59
6.8%
2.88
5.59
6.8%

3.99
8.21
9.6%
5.15
10.45
12.7%
6.90
13.23
17.4%

1.62
3.30
3.5%
2.33
4.76
5.4%
3.38
6.50
8.3%

3.99
3.99
4.42
7.94
7.89
8.45
9.9%
10.2% 9.3%
4.23
5.05
5.41
9.13
8.17
10.87
12.7% 12.1% 12.9%
4.49
6.72
6.52
10.11
8.69
13.76
15.8% 16.7% 14.0%

1.74
3.16
3.6%
2.32
4.25
5.0%
2.93
5.44
6.5%

2.20
1.85
3.59
4.42
3.8% 5.19%
2.30
2.48
5.18
4.63
5.5% 5.43%
2.46
3.28
7.08
4.98
8.0% 5.89%

3.44
6.30
9.6%
3.77
7.23
10.9%
4.37
8.69
13.2%

2.05
4.19
4.8%
2.20
4.55
5.2%
2.37
4.96
5.7%

2.77
5.38
7.3%
3.15
6.45
8.8%
3.60
7.59
10.5%

1.38
2.95
2.9%
1.74
3.97
3.9%
2.07
4.74
4.9%

spectral decomposition. Going from spectral domain to vertex domain, Atwood & Towsley (2016)
propose diffusion-convolutional neural network (DCNN) which deﬁnes convolution as a diffusion
process across each node in a graph-structured input. Hechtlinger et al. (2017) propose GraphCNN to
generalize convolution to graph by convolving every node with its p nearest neighbors. However,
both these methods do not consider the temporal dynamics and mainly deal with static graph settings.

Our approach is different from all those methods due to both the problem settings and the formulation
of the convolution on the graph. We model the sensor network as a weighted directed graph which
is more realistic than grid or undirected graph. Besides, the proposed convolution is deﬁned using
bidirectional graph random walk and is further integrated with the sequence to sequence learning
framework as well as the scheduled sampling to model the long-term temporal dependency.

4 EXPERIMENTS

We conduct experiments on two real-world large-scale datasets: (1) METR-LA This trafﬁc dataset
contains trafﬁc information collected from loop detectors in the highway of Los Angeles County (Ja-
gadish et al., 2014). We select 207 sensors and collect 4 months of data ranging from Mar 1st 2012
to Jun 30th 2012 for the experiment. (2) PEMS-BAY This trafﬁc dataset is collected by California
Transportation Agencies (CalTrans) Performance Measurement System (PeMS). We select 325
sensors in the Bay Area and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for
the experiment. The sensor distributions of both datasets are visualized in Figure 8 in the Appendix.

(cid:16)

In both of those datasets, we aggregate trafﬁc speed readings into 5 minutes windows, and apply
Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining
10% for validation. To construct the sensor graph, we compute the pairwise road network distances
between sensors and build the adjacency matrix using thresholded Gaussian kernel (Shuman et al.,
2013). Wij = exp
if dist(vi, vj) ≤ κ, otherwise 0, where Wij represents the edge
weight between sensor vi and sensor vj, dist(vi, vj) denotes the road network distance from sensor
vi to sensor vj. σ is the standard deviation of distances and κ is the threshold.
4.1 EXPERIMENTAL SETTINGS
Baselines We compare DCRNN1 with widely used time series regression models, including (1)
HA: Historical Average, which models the trafﬁc ﬂow as a seasonal process, and uses weighted

− dist(vi,vj )2
σ2

(cid:17)

1The source code is available at https://github.com/liyaguang/DCRNN.

5

Published as a conference paper at ICLR 2018

Figure 3: Learning curve for DCRNN and
DCRNN without diffusion convolution. Remov-
ing diffusion convolution results in much higher
validation error. Moreover, DCRNN with bi-
directional random walk achieves the lowest val-
idation error.

Figure 4: Effects of K and the number of units
in each layer of DCRNN. K corresponds to the
reception ﬁeld width of the ﬁlter, and the number
of units corresponds to the number of ﬁlters.

average of previous seasons as the prediction; (2) ARIMAkal: Auto-Regressive Integrated Moving
Average model with Kalman ﬁlter which is widely used in time series prediction; (3) VAR: Vector
Auto-Regression (Hamilton, 1994). (4) SVR: Support Vector Regression which uses linear support
vector machine for the regression task; The following deep neural network based approaches are also
included: (5) Feed forward Neural network (FNN): Feed forward neural network with two hidden
layers and L2 regularization. (6) Recurrent Neural Network with fully connected LSTM hidden units
(FC-LSTM) (Sutskever et al., 2014).

All neural network based approaches are implemented using Tensorﬂow (Abadi et al., 2016), and
trained using the Adam optimizer with learning rate annealing. The best hyperparameters are chosen
using the Tree-structured Parzen Estimator (TPE) (Bergstra et al., 2011) on the validation dataset.
Detailed parameter settings for DCRNN as well as baselines are available in Appendix E.

4.2 TRAFFIC FORECASTING PERFORMANCE COMPARISON

Table 1 shows the comparison of different approaches for 15 minutes, 30 minutes and 1 hour ahead
forecasting on both datasets. These methods are evaluated based on three commonly used metrics in
trafﬁc forecasting, including (1) Mean Absolute Error (MAE), (2) Mean Absolute Percentage Error
(MAPE), and (3) Root Mean Squared Error (RMSE). Missing values are excluded in calculating
these metrics. Detailed formulations of these metrics are provided in Appendix E.2. We observe the
following phenomenon in both of these datasets. (1) RNN-based methods, including FC-LSTM and
DCRNN, generally outperform other baselines which emphasizes the importance of modeling the
temporal dependency. (2) DCRNN achieves the best performance regarding all the metrics for all
forecasting horizons, which suggests the effectiveness of spatiotemporal dependency modeling. (3)
Deep neural network based methods including FNN, FC-LSTM and DCRNN, tend to have better
performance than linear baselines for long-term forecasting, e.g., 1 hour ahead. This is because the
temporal dependency becomes increasingly non-linear with the growth of the horizon. Besides, as
the historical average method does not depend on short-term data, its performance is invariant to the
small increases in the forecasting horizon.

Note that, trafﬁc forecasting on the METR-LA (Los Angeles, which is known for its complicated
trafﬁc conditions) dataset is more challenging than that in the PEMS-BAY (Bay Area) dataset. Thus
we use METR-LA as the default dataset for following experiments.

4.3 EFFECT OF SPATIAL DEPENDENCY MODELING
To further investigate the effect of spatial dependency modeling, we compare DCRNN with the
following variants: (1) DCRNN-NoConv, which ignores spatial dependency by replacing the transition
matrices in the diffusion convolution (Equation 2) with identity matrices. This essentially means the
forecasting of a sensor can be only be inferred from its own historical readings; (2) DCRNN-UniConv,

6

Published as a conference paper at ICLR 2018

Table 2: Performance comparison for DCRNN and GCRNN on the METRA-LA dataset.

15 min

30 min

1 hour

DCRNN 2.77
GCRNN 2.80

MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
10.5%
6.45
10.9%
6.74

8.8% 3.60
3.81
9.0%

7.3% 3.15
3.24
7.5%

7.60
8.16

5.38
5.51

Figure 5: Performance comparison for dif-
ferent DCRNN variants. DCRNN, with the
sequence to sequence framework and sched-
uled sampling, achieves the lowest MAE on
the validation dataset. The advantage be-
comes more clear with the increase of the
forecasting horizon.

Figure 6: Trafﬁc time series forecasting visualization.
DCRNN generates smooth prediction and is usually
better at predict the start and end of peak hours.

which only uses the forward random walk transition matrix for diffusion convolution; Figure 3 shows
the learning curves of these three models with roughly the same number of parameters. Without
diffusion convolution, DCRNN-NoConv has much higher validation error. Moreover, DCRNN
achieves the lowest validation error which shows the effectiveness of using bidirectional random
walk. The intuition is that the bidirectional random walk gives the model the ability and ﬂexibility to
capture the inﬂuence from both the upstream and the downstream trafﬁc.

To investigate the effect of graph construction, we construct a undirected graph by setting (cid:99)Wij =
(cid:99)Wji = max(Wij, Wji), where (cid:99)W is the new symmetric weight matrix. Then we develop a variant
of DCRNN denotes GCRNN, which uses the sequence to sequence learning with ChebNet graph
convolution (Equation 5) with roughly the same amount of parameters. Table 2 shows the comparison
between DCRNN and GCRNN in the METR-LA dataset. DCRNN consistently outperforms GCRNN.
The intuition is that directed graph better captures the asymmetric correlation between trafﬁc sensors.
Figure 4 shows the effects of different parameters. K roughly corresponds to the size of ﬁlters’
reception ﬁelds while the number of units corresponds to the number of ﬁlters. Larger K enables
the model to capture broader spatial dependency at the cost of increasing learning complexity. We
observe that with the increase of K, the error on the validation dataset ﬁrst quickly decrease, and
then slightly increase. Similar behavior is observed for varying the number of units.

4.4 EFFECT OF TEMPORAL DEPENDENCY MODELING

To evaluate the effect of temporal modeling including the sequence to sequence framework as well
as the scheduled sampling mechanism, we further design three variants of DCRNN: (1) DCNN: in
which we concatenate the historical observations as a ﬁxed length vector and feed it into stacked
diffusion convolutional layers to predict the future time series. We train a single model for one step
ahead prediction, and feed the previous prediction into the model as input to perform multiple steps
ahead prediction. (2) DCRNN-SEQ: which uses the encoder-decoder sequence to sequence learning
framework to perform multiple steps ahead forecasting. (3) DCRNN: similar to DCRNN-SEQ except
for adding scheduled sampling.

7

Published as a conference paper at ICLR 2018

Figure 7: Visualization of learned localized ﬁlters centered at different nodes with K = 3 on the
METR-LA dataset. The star denotes the center, and the colors represent the weights. We observe that
weights are localized around the center, and diffuse alongside the road network.

Figure 5 shows the comparison of those four methods with regards to MAE for different forecasting
horizons. We observe that: (1) DCRNN-SEQ outperforms DCNN by a large margin which conforms
the importance of modeling temporal dependency. (2) DCRNN achieves the best result, and its
superiority becomes more evident with the increase of the forecasting horizon. This is mainly because
the model is trained to deal with its mistakes during multiple steps ahead prediction and thus suffers
less from the problem of error propagation. We also train a model that always been fed its output as
input for multiple steps ahead prediction. However, its performance is much worse than all the three
variants which emphasizes the importance of scheduled sampling.

4.5 MODEL INTERPRETATION

To better understand the model, we visualize forecasting results as well as learned ﬁlters. Figure 6
shows the visualization of 1 hour ahead forecasting. We have the following observations: (1)
DCRNN generates smooth prediction of the mean when small oscillation exists in the trafﬁc speeds
(Figure 6(a)). This reﬂects the robustness of the model. (2) DCRNN is more likely to accurately
predict abrupt changes in the trafﬁc speed than baseline methods (e.g., FC-LSTM). As shown in
Figure 6(b), DCRNN predicts the start and the end of the peak hours. This is because DCRNN
captures the spatial dependency, and is able to utilize the speed changes in neighborhood sensors
for more accurate forecasting. Figure 7 visualizes examples of learned ﬁlters centered at different
nodes. The star denotes the center, and colors denote the weights. We can observe that (1) weights
are well localized around the center, and (2) the weights diffuse based on road network distance.
More visualizations are provided in Appendix F.

5 CONCLUSION

In this paper, we formulated the trafﬁc prediction on road network as a spatiotemporal forecasting
problem, and proposed the diffusion convolutional recurrent neural network that captures the spa-
tiotemporal dependencies. Speciﬁcally, we use bidirectional graph random walk to model spatial
dependency and recurrent neural network to capture the temporal dynamics. We further integrated the
encoder-decoder architecture and the scheduled sampling technique to improve the performance for
long-term forecasting. When evaluated on two large-scale real-world trafﬁc datasets, our approach
obtained signiﬁcantly better prediction than baselines. For future work, we will investigate the
following two aspects (1) applying the proposed model to other spatial-temporal forecasting tasks;
(2) modeling the spatiotemporal dependency when the underlying graph structure is evolving, e.g.,
the K nearest neighbor graph for moving objects.

ACKNOWLEDGMENTS

This research has been funded in part by NSF grants CNS-1461963, IIS-1254206, IIS-1539608,
Caltrans-65A0533, the USC Integrated Media Systems Center (IMSC), and the USC METRANS
Transportation Center. Any opinions, ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily reﬂect the views of any of the sponsors
such as NSF. Also, the authors would like to thank Shang-Hua Teng, Dehua Cheng and Siyang Li for
helpful discussions and comments.

8

Published as a conference paper at ICLR 2018

REFERENCES

Mart´ın Abadi et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.

arXiv preprint arXiv:1603.04467, 2016.

James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural

Information Processing Systems, pp. 1993–2001, 2016.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence

prediction with recurrent neural networks. In NIPS, pp. 1171–1179, 2015.

James S Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter
optimization. In Advances in Neural Information Processing Systems, pp. 2546–2554, 2011.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally

connected networks on graphs. In ICLR, 2014.

Pinlong Cai, Yunpeng Wang, Guangquan Lu, Peng Chen, Chuan Ding, and Jianping Sun. A
spatiotemporal correlative k-nearest neighbor model for short-term trafﬁc multistep forecasting.
Transportation Research Part C: Emerging Technologies, 62:21–34, 2016.

Ennio Cascetta. Transportation systems engineering: theory and methods, volume 49. Springer

Science & Business Media, 2013.

Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. Efﬁcient sampling for
gaussian graphical models via spectral sparsiﬁcation. In Conference on Learning Theory, pp.
364–390, 2015.

Xingyi Cheng, Ruiqing Zhang, Jie Zhou, and Wei Xu. Deeptransport: Learning spatial-temporal

dependency for trafﬁc condition forecasting. arXiv preprint arXiv:1709.09585, 2017.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on

graphs with fast localized spectral ﬁltering. In NIPS, pp. 3837–3845, 2016.

Dingxiong Deng, Cyrus Shahabi, Ugur Demiryurek, Linhong Zhu, Rose Yu, and Yan Liu. Latent
space model for road networks to predict time-varying trafﬁc. In SIGKDD, pp. 1525–1534, 2016.

Donald R Drew. Trafﬁc ﬂow theory and control. Technical report, 1968.

Gaetano Fusco, Chiara Colombaroni, and Natalia Isaenko. Short-term speed predictions exploiting
big data on large urban road networks. Transportation Research Part C: Emerging Technologies,
73:183–201, 2016.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional

sequence to sequence learning. In ICML, 2017.

Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
855–864. ACM, 2016.

James Douglas Hamilton. Time series analysis, volume 2. Princeton university press Princeton, 1994.

Yotam Hechtlinger, Purvasha Chakravarti, and Jining Qin. A generalization of convolutional neural

networks to graph-structured data. arXiv preprint arXiv:1704.08165, 2017.

H. V. Jagadish, Johannes Gehrke, Alexandros Labrinidis, Yannis Papakonstantinou, Jignesh M. Patel,
Raghu Ramakrishnan, and Cyrus Shahabi. Big data and its technical challenges. Commun. ACM,
57(7):86–94, July 2014.

Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.

In International Conference on Learning Representations (ICLR), 2017.

9

Published as a conference paper at ICLR 2018

Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event forecasting
with neural networks at Uber. In Int. Conf. on Machine Learning Time Series Workshop, 2017.

Marco Lippi, Marco Bertini, and Paolo Frasconi. Short-term trafﬁc ﬂow forecasting: An experimental
comparison of time-series analysis and supervised learning. ITS, IEEE Transactions on, 14(2):
871–882, 2013.

Wei Liu, Yu Zheng, Sanjay Chawla, Jing Yuan, and Xie Xing. Discovering spatio-temporal causal

interactions in trafﬁc data streams. In SIGKDD, pp. 1010–1018. ACM, 2011.

Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. Trafﬁc ﬂow prediction

with big data: A deep learning approach. ITS, IEEE Transactions on, 16(2):865–873, 2015.

Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang, and Yunpeng Wang. Learning
trafﬁc as images: a deep convolutional neural network for large-scale transportation network speed
prediction. Sensors, 17(4):818, 2017.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining, pp. 701–710. ACM, 2014.

Youngjoo Seo, Micha¨el Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence
modeling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659, 2016.

David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst.
The emerging ﬁeld of signal processing on graphs: Extending high-dimensional data analysis to
networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83–98, 2013.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

In NIPS, pp. 3104–3112, 2014.

Shang-Hua Teng et al. Scalable algorithms for data and network analysis. Foundations and Trends R(cid:13)

in Theoretical Computer Science, 12(1–2):1–274, 2016.

Eleni I Vlahogianni, Matthew G Karlaftis, and John C Golias. Short-term trafﬁc forecasting: Where
we are and where were going. Transportation Research Part C: Emerging Technologies, 43:3–19,
2014.

Yuankai Wu and Huachun Tan. Short-term trafﬁc ﬂow forecasting with spatial-temporal correlation

in a hybrid deep learning framework. arXiv preprint arXiv:1612.01022, 2016.

Yuanchang Xie, Kaiguang Zhao, Ying Sun, and Dawei Chen. Gaussian processes for short-term trafﬁc
volume forecasting. Transportation Research Record: Journal of the Transportation Research
Board, (2165):69–78, 2010.

Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional neural network: A

deep learning framework for trafﬁc forecasting. arXiv preprint arXiv:1709.04875, 2017a.

Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for
high-dimensional time series prediction. In Advances in Neural Information Processing Systems,
pp. 847–855, 2016.

Rose Yu, Yaguang Li, Cyrus Shahabi, Ugur Demiryurek, and Yan Liu. Deep learning: A generic
approach for extreme condition trafﬁc forecasting. In SIAM International Conference on Data
Mining (SDM), 2017b.

Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, and Xiuwen Yi. Dnn-based prediction model for
spatio-temporal data. In Proceedings of the 24th ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems, pp. 92. ACM, 2016.

Junbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide crowd

ﬂows prediction. In AAAI, pp. 1655–1661, 2017.

10

Published as a conference paper at ICLR 2018

APPENDIX

A NOTATION

Name
G
V, vi
E
W , Wij,
D, DI , DO
L
Φ, Λ
X, ˆX ∈ RN ×P
X (t) ∈ RN ×P
H ∈ RN ×Q
fθ, θ
fΘ, Θ

Table 3: Notation

a graph
nodes of a graph, |V| = N and the i-th node.
edges of a graph
weight matrix of a graph and its entries
undirected degree matrix, In-degree/out-degree matrix
normalized graph Laplacian
eigen-vector matrix and eigen-value matrix of L
a graph signal, and the predicted graph signal.
a graph signal at time t.
output of the diffusion convolutional layer.
convolutional ﬁlter and its parameters.
convolutional layer and its parameters.

Table 3 summarizes the main notations used in the paper.

B EFFICIENT CALCULATION OF EQUATION 2

O W and the other part with D−1

Equation 2 can be decomposed into two parts with the same time complexity, i.e., one part with
I W (cid:124). Thus we will only show the time complexity of the ﬁrst
D−1
part.
Let Tk(x) = (cid:0)D−1

x, The ﬁrst part of Equation 2 can be rewritten as

O W (cid:1)k

K−1
(cid:88)

k=0

θkTk(X:,p)

(4)

O W Tk(x) and D−1

As Tk+1(x) = D−1
O W is sparse, it is easy to see that Equation 4 can be
calculated using O(K) recursive sparse-dense matrix multiplication each with time complexity
O(|E|). Consequently, the time complexities of both Equation 2 and Equation 4 are O(K|E|). For
dense graph, we may use spectral sparsiﬁcation (Cheng et al., 2015) to make it sparse.

C RELATION WITH SPECTRAL GRAPH CONVOLUTION

Proof. The spectral graph convolution utilizes the concept of normalized graph Laplacian L =
2 = ΦΛΦ(cid:124). ChebNet parametrizes fθ to be a K order polynomial of Λ, and
D− 1
calculates it using stable Chebyshev polynomial basis.

2 (D − W )D− 1

X:,p (cid:63)G fθ = Φ

θkΛk

Φ(cid:124)X:,p =

θkLkX:,p =

˜θkTk( ˜L)X:,p

(5)

(cid:33)

(cid:32)K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

where T0(x) = 1, T1(x) = x, Tk(x) = xTk−1(x) − Tk−2(x) are the basis of the Cheyshev polyno-
mial. Let λmax denote the largest eigenvalue of L, and ˜L = 2
L − I represents a rescaling of the
graph Laplacian that maps the eigenvalues from [0, λmax] to [−1, 1] since Chebyshev polynomial
forms an orthogonal basis in [−1, 1]. Equation 5 can be considered as a polynomial of ˜L and we will
show that the output of ChebNet Convolution is similar to the output of diffusion convolution up to
constant scaling factor. Assume λmax = 2 and DI = DO = D for undirected graph.

λmax

˜L = D− 1

2 − I = −D− 1
(6)
˜L is similar to the negative random walk transition matrix, thus the output of Equation 5 is also
similar to the output of Equation 2 up to constant scaling factor.

2 (D − W )D− 1

2 ∼ −D−1W

2 W D− 1

11

Published as a conference paper at ICLR 2018

Figure 8: Sensor distribution of the METR-LA and PEMS-BAY dataset.

D MORE RELATED WORK AND DISCUSSION

Xie et al. (2010) introduce a Gaussian processes (GPs) based method. GPs are hard to scale to the
large dataset and are generally not suitable for relatively long-term trafﬁc prediction like 1 hour
(i.e.,12 steps ahead), as the variance can be accumulated and becomes extremely large.

Cai et al. (2016) propose to use spatiotemporal nearest neighbor for trafﬁc forecasting (ST-KNN).
Though ST-KNN considers both the spatial and the temporal dependencies, it has the following
drawbacks. As shown in Fusco et al. (2016), ST-KNN performs independent forecasting for each
individual road. The prediction of a road is a weighted combination of its own historical trafﬁc speeds.
This makes it hard for ST-KNN to fully utilize information from neighbors. Besides, ST-KNN is a
non-parametric approach and each road is modeled and calculated separately (Cai et al., 2016), which
makes it hard to generalize to unseen situations and to scale to large datasets. Finally, in ST-KNN, all
the similarities are calculated using hand-designed metrics with few learnable parameters, and this
may limit its representational power.

Cheng et al. (2017) propose DeepTransport which models the spatial dependency by explicitly
collecting certain number of upstream and downstream roads for each individual road and then
conduct convolution on these roads respectively. Comparing with Cheng et al. (2017), DCRNN
models the spatial dependency in a more systematic way, i.e., generalizing convolution to the trafﬁc
sensor graph based on the diffusion nature of trafﬁc. Besides, we derive DCRNN from the property of
random walk and show that the popular spectral convolution ChebNet is a special case of our method.

The proposed approach is also related to graph embedding techniques, e.g., Deepwalk (Perozzi et al.,
2014), node2vec (Grover & Leskovec, 2016) which learn a low dimension representation for each
node in the graph. DCRNN also learns a representation for each node. The learned representations
capture both the spatial and the temporal dependency and at the same time are optimized with
regarding to the objective, e.g., future trafﬁc speeds.

E DETAILED EXPERIMENTAL SETTINGS

HA Historical Average, which models the trafﬁc ﬂow as a seasonal process, and uses weighted
average of previous seasons as the prediction. The period used is 1 week, and the prediction is based
on aggregated data from previous weeks. For example, the prediction for this Wednesday is the
averaged trafﬁc speeds from last four Wednesdays. As the historical average method does not depend
on short-term data, its performance is invariant to the small increases in the forecasting horizon

ARIMAkal
are (3, 0, 1), and the model is implemented using the statsmodel python package.

: Auto-Regressive Integrated Moving Average model with Kalman ﬁlter. The orders

12

Published as a conference paper at ICLR 2018

VAR Vector Auto-regressive model (Hamilton, 1994). The number of lags is set to 3, and the model
is implemented using the statsmodel python package.

SVR Linear Support Vector Regression, the penalty term C = 0.1, the number of historical
observation is 5.

The following deep neural network based approaches are also included.

FNN Feed forward neural network with two hidden layers, each layer contains 256 units. The initial
learning rate is 1e−3, and reduces to 1
10 every 20 epochs starting at the 50th epochs. In addition, for
all hidden layers, dropout with ratio 0.5 and L2 weight decay 1e−2 is used. The model is trained with
batch size 64 and MAE as the loss function. Early stop is performed by monitoring the validation
error.

FC-LSTM The Encoder-decoder framework using LSTM with peephole (Sutskever et al., 2014).
Both the encoder and the decoder contain two recurrent layers. In each recurrent layer, there are 256
LSTM units, L1 weight decay is 2e−5, L2 weight decay 5e−4. The model is trained with batch size
64 and loss function MAE. The initial learning rate is 1e-4 and reduces to 1
10 every 10 epochs starting
from the 20th epochs. Early stop is performed by monitoring the validation error.

DCRNN : Diffusion Convolutional Recurrent Neural Network. Both encoder and decoder contain
two recurrent layers. In each recurrent layer, there are 64 units, the initial learning rate is 1e−2, and
reduces to 1
10 every 10 epochs starting at the 20th epoch and early stopping on the validation dataset
is used. Besides, the maximum steps of random walks, i.e., K, is set to 3. For scheduled sampling,
the thresholded inverse sigmoid function is used as the probability decay:

(cid:15)i =

τ
τ + exp (i/τ )

where i is the number of iterations while τ are parameters to control the speed of convergence. τ
is set to 3,000 in the experiments. The implementation is available in https://github.com/
liyaguang/DCRNN.

E.1 DATASET

We conduct experiments on two real-world large-scale datasets:

• METR-LA This trafﬁc dataset contains trafﬁc information collected from loop detectors
in the highway of Los Angeles County (Jagadish et al., 2014). We select 207 sensors and
collect 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 for the experiment.
The total number of observed trafﬁc data points is 6,519,002.

• PEMS-BAY This trafﬁc dataset is collected by California Transportation Agencies (Cal-
Trans) Performance Measurement System (PeMS). We select 325 sensors in the Bay Area
and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for the experiment.
The total number of observed trafﬁc data points is 16,937,179.

The sensor distributions of both datasets are visualized in Figure 8.

In both of those datasets, we aggregate trafﬁc speed readings into 5 minutes windows, and apply
Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining
10% for validation. To construct the sensor graph, we compute the pairwise road network distances
between sensors and build the adjacency matrix using thresholded Gaussian kernel (Shuman et al.,
2013).

Wij = exp

−

(cid:18)

(cid:19)

dist(vi, vj)2
σ2

if dist(vi, vj) ≤ κ, otherwise 0

where Wij represents the edge weight between sensor vi and sensor vj, dist(vi, vj) denotes the road
network distance from sensor vi to sensor vj. σ is the standard deviation of distances and κ is the
threshold.

13

Published as a conference paper at ICLR 2018

E.2 METRICS

Suppose x = x1, · · · , xn represents the ground truth, ˆx = ˆx1, · · · , ˆxn represents the predicted
values, and Ω denotes the indices of observed samples, the metrics are deﬁned as follows.

Root Mean Square Error (RMSE)

RMSE(x, ˆx) =

(xi − ˆxi)2

(cid:115) 1
|Ω|

(cid:88)

i∈Ω

MAPE(x, ˆx) =

1
|Ω|

(cid:88)

i∈Ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

xi − ˆxi
xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

MAE(x, ˆx) =

|xi − ˆxi|

1
|Ω|

(cid:88)

i∈Ω

Mean Absolute Percentage Error (MAPE)

Mean Absolute Error (MAE)

F MODEL VISUALIZATION

Figure 9: Sensor correlations between the center sensor and its neighborhoods for different forecasting
horizons. The correlations are estimated using regularized VAR. We observe that the correlations are
localized and closer neighborhoods usually have larger relevance, and the magnitude of correlation
quickly decay with the increase of distance which is consistent with the diffusion process on the
graph.

14

Published as a conference paper at ICLR 2018

15
Figure 10: Trafﬁc time series forecasting visualization.

Published as a conference paper at ICLR 2018

16
Figure 11: Trafﬁc time series forecasting visualization.

8
1
0
2
 
b
e
F
 
2
2
 
 
]

G
L
.
s
c
[
 
 
3
v
6
2
9
1
0
.
7
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

DIFFUSION CONVOLUTIONAL RECURRENT NEURAL
NETWORK: DATA-DRIVEN TRAFFIC FORECASTING

Yaguang Li†, Rose Yu‡, Cyrus Shahabi†, Yan Liu†
† University of Southern California, ‡ California Institute of Technology
† {yaguang, shahabi, yanliu.cs}@usc.edu, ‡ rose@caltech.edu

ABSTRACT
Spatiotemporal forecasting has various applications in neuroscience, climate and
transportation domain. Trafﬁc forecasting is one canonical example of such learn-
ing task. The task is challenging due to (1) complex spatial dependency on road
networks, (2) non-linear temporal dynamics with changing road conditions and
(3) inherent difﬁculty of long-term forecasting. To address these challenges, we
propose to model the trafﬁc ﬂow as a diffusion process on a directed graph and
introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep
learning framework for trafﬁc forecasting that incorporates both spatial and tem-
poral dependency in the trafﬁc ﬂow. Speciﬁcally, DCRNN captures the spatial
dependency using bidirectional random walks on the graph, and the temporal de-
pendency using the encoder-decoder architecture with scheduled sampling. We
evaluate the framework on two real-world large scale road network trafﬁc datasets
and observe consistent improvement of 12% - 15% over state-of-the-art baselines.

INTRODUCTION

1
Spatiotemporal forecasting is a crucial task for a learning system that operates in a dynamic environ-
ment. It has a wide range of applications from autonomous vehicles operations, to energy and smart
grid optimization, to logistics and supply chain management. In this paper, we study one important
task: trafﬁc forecasting on road networks, the core component of the intelligent transportation systems.
The goal of trafﬁc forecasting is to predict the future trafﬁc speeds of a sensor network given historic
trafﬁc speeds and the underlying road networks.

This task is challenging mainly due to the com-
plex spatiotemporal dependencies and inher-
ent difﬁculty in the long term forecasting. On
the one hand, trafﬁc time series demonstrate
strong temporal dynamics. Recurring incidents
such as rush hours or accidents can cause non-
stationarity, making it difﬁcult to forecast long-
term. On the other hand, sensors on the road
network contain complex yet unique spatial cor-
relations. Figure 1 illustrates an example. Road
1 and road 2 are correlated, while road 1 and
road 3 are not. Although road 1 and road 3
are close in the Euclidean space, they demon-
strate very different behaviors. Moreover, the
future trafﬁc speed is inﬂuenced more by the
downstream trafﬁc than the upstream one. This
means that the spatial structure in trafﬁc is non-
Euclidean and directional.

Figure 1: Spatial correlation is dominated by road
network structure. (1) Trafﬁc speed in road 1 are
similar to road 2 as they locate in the same highway.
(2) Road 1 and road 3 locate in the opposite direc-
tions of the highway. Though close to each other
in the Euclidean space, their road network distance
is large, and their trafﬁc speeds differ signiﬁcantly.

Trafﬁc forecasting has been studied for decades,
falling into two main categories: knowledge-
driven approach and data-driven approach. In transportation and operational research, knowledge-
driven methods usually apply queuing theory and simulate user behaviors in trafﬁc (Cascetta, 2013).
In time series community, data-driven methods such as Auto-Regressive Integrated Moving Average
(ARIMA) model and Kalman ﬁltering remain popular (Liu et al., 2011; Lippi et al., 2013). However,
simple time series models usually rely on the stationarity assumption, which is often violated by

1

Published as a conference paper at ICLR 2018

the trafﬁc data. Most recently, deep learning models for trafﬁc forecasting have been developed
in Lv et al. (2015); Yu et al. (2017b), but without considering the spatial structure. Wu & Tan (2016)
and Ma et al. (2017) model the spatial correlation with Convolutional Neural Networks (CNN), but
the spatial structure is in the Euclidean space (e.g., 2D images). Bruna et al. (2014), Defferrard et al.
(2016) studied graph convolution, but only for undirected graphs.

In this work, we represent the pair-wise spatial correlations between trafﬁc sensors using a directed
graph whose nodes are sensors and edge weights denote proximity between the sensor pairs measured
by the road network distance. We model the dynamics of the trafﬁc ﬂow as a diffusion process and
propose the diffusion convolution operation to capture the spatial dependency. We further propose
Diffusion Convolutional Recurrent Neural Network (DCRNN) that integrates diffusion convolution,
the sequence to sequence architecture and the scheduled sampling technique. When evaluated on real-
world trafﬁc datasets, DCRNN consistently outperforms state-of-the-art trafﬁc forecasting baselines
by a large margin. In summary:

• We study the trafﬁc forecasting problem and model the spatial dependency of trafﬁc as
a diffusion process on a directed graph. We propose diffusion convolution, which has an
intuitive interpretation and can be computed efﬁciently.

• We propose Diffusion Convolutional Recurrent Neural Network (DCRNN), a holistic ap-
proach that captures both spatial and temporal dependencies among time series using
diffusion convolution and the sequence to sequence learning framework together with sched-
uled sampling. DCRNN is not limited to transportation and is readily applicable to other
spatiotemporal forecasting tasks.

• We conducted extensive experiments on two large-scale real-world datasets, and the proposed

approach obtains signiﬁcant improvement over state-of-the-art baseline methods.

2 METHODOLOGY
We formalize the learning problem of spatiotemporal trafﬁc forecasting and describe how to model
the dependency structures using diffusion convolutional recurrent neural network.

2.1 TRAFFIC FORECASTING PROBLEM
The goal of trafﬁc forecasting is to predict the future trafﬁc speed given previously observed trafﬁc
ﬂow from N correlated sensors on the road network. We can represent the sensor network as a
weighted directed graph G = (V, E, W ), where V is a set of nodes |V| = N , E is a set of edges and
W ∈ RN ×N is a weighted adjacency matrix representing the nodes proximity (e.g., a function of
their road network distance). Denote the trafﬁc ﬂow observed on G as a graph signal X ∈ RN ×P ,
where P is the number of features of each node (e.g., velocity, volume). Let X (t) represent the graph
signal observed at time t, the trafﬁc forecasting problem aims to learn a function h(·) that maps T (cid:48)
historical graph signals to future T graph signals, given a graph G:

[X (t−T (cid:48)+1), · · · , X (t); G]

h(·)
−−→ [X (t+1), · · · , X (t+T )]

2.2 SPATIAL DEPENDENCY MODELING

We model the spatial dependency by relating trafﬁc ﬂow to a diffusion process, which explicitly
captures the stochastic nature of trafﬁc dynamics. This diffusion process is characterized by a
random walk on G with restart probability α ∈ [0, 1], and a state transition matrix D−1
O W . Here
DO = diag(W 1) is the out-degree diagonal matrix, and 1 ∈ RN denotes the all one vector. After
many time steps, such Markov process converges to a stationary distribution P ∈ RN ×N whose ith
row P i,: ∈ RN represents the likelihood of diffusion from node vi ∈ V, hence the proximity w.r.t.
the node vi. The following Lemma provides a closed form solution for the stationary distribution.
Lemma 2.1. (Teng et al., 2016) The stationary distribution of the diffusion process can be represented
as a weighted combination of inﬁnite random walks on the graph, and be calculated in closed form:

P =

α(1 − α)k (cid:0)D−1

O W (cid:1)k

∞
(cid:88)

k=0

(1)

where k is the diffusion step. In practice, we use a ﬁnite K-step truncation of the diffusion process
and assign a trainable weight to each step. We also include the reversed direction diffusion process,

2

Published as a conference paper at ICLR 2018

such that the bidirectional diffusion offers the model more ﬂexibility to capture the inﬂuence from
both the upstream and the downstream trafﬁc.

Diffusion Convolution The resulted diffusion convolution operation over a graph signal X ∈
RN ×P and a ﬁlter fθ is deﬁned as:

X:,p (cid:63)G fθ =

θk,1

(cid:0)D−1

O W (cid:1)k

+ θk,2

(cid:0)D−1

I W (cid:124)(cid:1)k(cid:17)

X:,p

for p ∈ {1, · · · , P }

(2)

K−1
(cid:88)

(cid:16)

k=0

I W (cid:124) represent the transition
where θ ∈ RK×2 are the parameters for the ﬁlter and D−1
O W , D−1
matrices of the diffusion process and the reverse one, respectively.
In general, computing the
convolution can be expensive. However, if G is sparse, Equation 2 can be calculated efﬁciently using
O(K) recursive sparse-dense matrix multiplication with total time complexity O(K|E|) (cid:28) O(N 2).
See Appendix B for more detail.

Diffusion Convolutional Layer With the convolution operation deﬁned in Equation 2, we can
build a diffusion convolutional layer that maps P -dimensional features to Q-dimensional outputs.
Denote the parameter tensor as Θ ∈ RQ×P ×K×2 = [θ]q,p, where Θq,p,:,: ∈ RK×2 parameterizes
the convolutional ﬁlter for the pth input and the qth output. The diffusion convolutional layer is thus:

H:,q = a

X:,p (cid:63)G fΘq,p,:,:

for q ∈ {1, · · · , Q}

(3)

(cid:32) P

(cid:88)

p=1

(cid:33)

where X ∈ RN ×P is the input, H ∈ RN ×Q is the output, {fΘq,p,,: } are the ﬁlters and a is the
activation function (e.g., ReLU, Sigmoid). Diffusion convolutional layer learns the representations
for graph structured data and we can train it using stochastic gradient based method.

Relation with Spectral Graph Convolution Diffusion convolution is deﬁned on both directed and
undirected graphs. When applied to undirected graphs, we show that many existing graph structured
convolutional operations including the popular spectral graph convolution, i.e., ChebNet (Defferrard
et al., 2016), can be considered as a special case of diffusion convolution (up to a similarity transfor-
mation). Let D denote the degree matrix, and L = D− 1
2 be the normalized graph
Laplacian, the following Proposition demonstrates the connection.
Proposition 2.2. The spectral graph convolution deﬁned as

2 (D − W )D− 1

X:,p (cid:63)G fθ = Φ F (θ) Φ(cid:124)X:,p

with eigenvalue decomposition L = ΦΛΦ(cid:124) and F (θ) = (cid:80)K−1
diffusion convolution up to a similarity transformation, when the graph G is undirected.

θkΛk, is equivalent to graph

0

Proof. See Appendix C.

2.3 TEMPORAL DYNAMICS MODELING
We leverage the recurrent neural networks (RNNs) to model the temporal dependency. In particular,
we use Gated Recurrent Units (GRU) (Chung et al., 2014), which is a simple yet powerful variant of
RNNs. We replace the matrix multiplications in GRU with the diffusion convolution, which leads to
our proposed Diffusion Convolutional Gated Recurrent Unit (DCGRU).

r(t) =
C(t) = tanh(ΘC (cid:63)G

σ(Θr (cid:63)G [X (t), H (t−1)] + br)

u(t) = σ(Θu (cid:63)G [X (t), H (t−1)] + bu)

(cid:2)X (t), (r(t) (cid:12) H (t−1))(cid:3) + bc) H (t) = u(t) (cid:12) H (t−1) + (1 − u(t)) (cid:12) C(t)

where X (t), H (t) denote the input and output of at time t, r(t), u(t) are reset gate and update gate at
time t, respectively. (cid:63)G denotes the diffusion convolution deﬁned in Equation 2 and Θr, Θu, ΘC are
parameters for the corresponding ﬁlters. Similar to GRU, DCGRU can be used to build recurrent
neural network layers and be trained using backpropagation through time.

In multiple step ahead forecasting, we employ the Sequence to Sequence architecture (Sutskever
et al., 2014). Both the encoder and the decoder are recurrent neural networks with DCGRU. During
training, we feed the historical time series into the encoder and use its ﬁnal states to initialize the

3

Published as a conference paper at ICLR 2018

Figure 2: System architecture for the Diffusion Convolutional Recurrent Neural Network designed
for spatiotemporal trafﬁc forecasting. The historical time series are fed into an encoder whose ﬁnal
states are used to initialize the decoder. The decoder makes predictions based on either previous
ground truth or the model output.

decoder. The decoder generates predictions given previous ground truth observations. At testing time,
ground truth observations are replaced by predictions generated by the model itself. The discrepancy
between the input distributions of training and testing can cause degraded performance. To mitigate
this issue, we integrate scheduled sampling (Bengio et al., 2015) into the model, where we feed the
model with either the ground truth observation with probability (cid:15)i or the prediction by the model with
probability 1 − (cid:15)i at the ith iteration. During the training process, (cid:15)i gradually decreases to 0 to allow
the model to learn the testing distribution.

With both spatial and temporal modeling, we build a Diffusion Convolutional Recurrent Neural
Network (DCRNN). The model architecture of DCRNN is shown in Figure 2. The entire network is
trained by maximizing the likelihood of generating the target future time series using backpropagation
through time. DCRNN is able to capture spatiotemporal dependencies among time series and can be
applied to various spatiotemporal forecasting problems.

3 RELATED WORK

Trafﬁc forecasting is a classic problem in transportation and operational research which are primarily
based on queuing theory and simulations (Drew, 1968). Data-driven approaches for trafﬁc forecasting
have received considerable attention, and more details can be found in a recent survey paper (Vla-
hogianni et al., 2014) and the references therein. However, existing machine learning models either
impose strong stationary assumptions on the data (e.g., auto-regressive model) or fail to account for
highly non-linear temporal dependency (e.g., latent space model Yu et al. (2016); Deng et al. (2016)).
Deep learning models deliver new promise for time series forecasting problem. For example, in Yu
et al. (2017b); Laptev et al. (2017), the authors study time series forecasting using deep Recurrent
Neural Networks (RNN). Convolutional Neural Networks (CNN) have also been applied to trafﬁc
forecasting. Zhang et al. (2016; 2017) convert the road network to a regular 2-D grid and apply
traditional CNN to predict crowd ﬂow. Cheng et al. (2017) propose DeepTransport which models the
spatial dependency by explicitly collecting upstream and downstream neighborhood roads for each
individual road and then conduct convolution on these neighborhoods respectively.

Recently, CNN has been generalized to arbitrary graphs based on the spectral graph theory. Graph
convolutional neural networks (GCN) are ﬁrst introduced in Bruna et al. (2014), which bridges the
spectral graph theory and deep neural networks. Defferrard et al. (2016) propose ChebNet which
improves GCN with fast localized convolutions ﬁlters. Kipf & Welling (2017) simplify ChebNet
and achieve state-of-the-art performance in semi-supervised classiﬁcation tasks. Seo et al. (2016)
combine ChebNet with Recurrent Neural Networks (RNN) for structured sequence modeling. Yu
et al. (2017a) model the sensor network as a undirected graph and applied ChebNet and convolutional
sequence model (Gehring et al., 2017) to do forecasting. One limitation of the mentioned spectral
based convolutions is that they generally require the graph to be undirected to calculate meaningful

4

Published as a conference paper at ICLR 2018

Table 1: Performance comparison of different approaches for trafﬁc speed forecasting. DCRNN
achieves the best performance with all three metrics for all forecasting horizons, and the advantage
becomes more evident with the increase of the forecasting horizon.

T

Metric

HA

ARIMAKal

VAR

SVR

FNN

FC-LSTM DCRNN

A
L
-
R
T
E
M

Y
A
B
-
S
M
E
P

15 min

30 min

1 hour

15 min

30 min

1 hour

4.16
MAE
RMSE
7.80
MAPE 13.0%
4.16
MAE
RMSE
7.80
MAPE 13.0%
4.16
MAE
RMSE
7.80
MAPE 13.0%

MAE
RMSE
MAPE
MAE
RMSE
MAPE
MAE
RMSE
MAPE

2.88
5.59
6.8%
2.88
5.59
6.8%
2.88
5.59
6.8%

3.99
8.21
9.6%
5.15
10.45
12.7%
6.90
13.23
17.4%

1.62
3.30
3.5%
2.33
4.76
5.4%
3.38
6.50
8.3%

3.99
3.99
4.42
7.94
7.89
8.45
9.9%
10.2% 9.3%
4.23
5.05
5.41
9.13
8.17
10.87
12.7% 12.1% 12.9%
4.49
6.72
6.52
10.11
8.69
13.76
15.8% 16.7% 14.0%

1.74
3.16
3.6%
2.32
4.25
5.0%
2.93
5.44
6.5%

2.20
1.85
3.59
4.42
3.8% 5.19%
2.30
2.48
5.18
4.63
5.5% 5.43%
2.46
3.28
7.08
4.98
8.0% 5.89%

3.44
6.30
9.6%
3.77
7.23
10.9%
4.37
8.69
13.2%

2.05
4.19
4.8%
2.20
4.55
5.2%
2.37
4.96
5.7%

2.77
5.38
7.3%
3.15
6.45
8.8%
3.60
7.59
10.5%

1.38
2.95
2.9%
1.74
3.97
3.9%
2.07
4.74
4.9%

spectral decomposition. Going from spectral domain to vertex domain, Atwood & Towsley (2016)
propose diffusion-convolutional neural network (DCNN) which deﬁnes convolution as a diffusion
process across each node in a graph-structured input. Hechtlinger et al. (2017) propose GraphCNN to
generalize convolution to graph by convolving every node with its p nearest neighbors. However,
both these methods do not consider the temporal dynamics and mainly deal with static graph settings.

Our approach is different from all those methods due to both the problem settings and the formulation
of the convolution on the graph. We model the sensor network as a weighted directed graph which
is more realistic than grid or undirected graph. Besides, the proposed convolution is deﬁned using
bidirectional graph random walk and is further integrated with the sequence to sequence learning
framework as well as the scheduled sampling to model the long-term temporal dependency.

4 EXPERIMENTS

We conduct experiments on two real-world large-scale datasets: (1) METR-LA This trafﬁc dataset
contains trafﬁc information collected from loop detectors in the highway of Los Angeles County (Ja-
gadish et al., 2014). We select 207 sensors and collect 4 months of data ranging from Mar 1st 2012
to Jun 30th 2012 for the experiment. (2) PEMS-BAY This trafﬁc dataset is collected by California
Transportation Agencies (CalTrans) Performance Measurement System (PeMS). We select 325
sensors in the Bay Area and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for
the experiment. The sensor distributions of both datasets are visualized in Figure 8 in the Appendix.

(cid:16)

In both of those datasets, we aggregate trafﬁc speed readings into 5 minutes windows, and apply
Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining
10% for validation. To construct the sensor graph, we compute the pairwise road network distances
between sensors and build the adjacency matrix using thresholded Gaussian kernel (Shuman et al.,
2013). Wij = exp
if dist(vi, vj) ≤ κ, otherwise 0, where Wij represents the edge
weight between sensor vi and sensor vj, dist(vi, vj) denotes the road network distance from sensor
vi to sensor vj. σ is the standard deviation of distances and κ is the threshold.
4.1 EXPERIMENTAL SETTINGS
Baselines We compare DCRNN1 with widely used time series regression models, including (1)
HA: Historical Average, which models the trafﬁc ﬂow as a seasonal process, and uses weighted

− dist(vi,vj )2
σ2

(cid:17)

1The source code is available at https://github.com/liyaguang/DCRNN.

5

Published as a conference paper at ICLR 2018

Figure 3: Learning curve for DCRNN and
DCRNN without diffusion convolution. Remov-
ing diffusion convolution results in much higher
validation error. Moreover, DCRNN with bi-
directional random walk achieves the lowest val-
idation error.

Figure 4: Effects of K and the number of units
in each layer of DCRNN. K corresponds to the
reception ﬁeld width of the ﬁlter, and the number
of units corresponds to the number of ﬁlters.

average of previous seasons as the prediction; (2) ARIMAkal: Auto-Regressive Integrated Moving
Average model with Kalman ﬁlter which is widely used in time series prediction; (3) VAR: Vector
Auto-Regression (Hamilton, 1994). (4) SVR: Support Vector Regression which uses linear support
vector machine for the regression task; The following deep neural network based approaches are also
included: (5) Feed forward Neural network (FNN): Feed forward neural network with two hidden
layers and L2 regularization. (6) Recurrent Neural Network with fully connected LSTM hidden units
(FC-LSTM) (Sutskever et al., 2014).

All neural network based approaches are implemented using Tensorﬂow (Abadi et al., 2016), and
trained using the Adam optimizer with learning rate annealing. The best hyperparameters are chosen
using the Tree-structured Parzen Estimator (TPE) (Bergstra et al., 2011) on the validation dataset.
Detailed parameter settings for DCRNN as well as baselines are available in Appendix E.

4.2 TRAFFIC FORECASTING PERFORMANCE COMPARISON

Table 1 shows the comparison of different approaches for 15 minutes, 30 minutes and 1 hour ahead
forecasting on both datasets. These methods are evaluated based on three commonly used metrics in
trafﬁc forecasting, including (1) Mean Absolute Error (MAE), (2) Mean Absolute Percentage Error
(MAPE), and (3) Root Mean Squared Error (RMSE). Missing values are excluded in calculating
these metrics. Detailed formulations of these metrics are provided in Appendix E.2. We observe the
following phenomenon in both of these datasets. (1) RNN-based methods, including FC-LSTM and
DCRNN, generally outperform other baselines which emphasizes the importance of modeling the
temporal dependency. (2) DCRNN achieves the best performance regarding all the metrics for all
forecasting horizons, which suggests the effectiveness of spatiotemporal dependency modeling. (3)
Deep neural network based methods including FNN, FC-LSTM and DCRNN, tend to have better
performance than linear baselines for long-term forecasting, e.g., 1 hour ahead. This is because the
temporal dependency becomes increasingly non-linear with the growth of the horizon. Besides, as
the historical average method does not depend on short-term data, its performance is invariant to the
small increases in the forecasting horizon.

Note that, trafﬁc forecasting on the METR-LA (Los Angeles, which is known for its complicated
trafﬁc conditions) dataset is more challenging than that in the PEMS-BAY (Bay Area) dataset. Thus
we use METR-LA as the default dataset for following experiments.

4.3 EFFECT OF SPATIAL DEPENDENCY MODELING
To further investigate the effect of spatial dependency modeling, we compare DCRNN with the
following variants: (1) DCRNN-NoConv, which ignores spatial dependency by replacing the transition
matrices in the diffusion convolution (Equation 2) with identity matrices. This essentially means the
forecasting of a sensor can be only be inferred from its own historical readings; (2) DCRNN-UniConv,

6

Published as a conference paper at ICLR 2018

Table 2: Performance comparison for DCRNN and GCRNN on the METRA-LA dataset.

15 min

30 min

1 hour

DCRNN 2.77
GCRNN 2.80

MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
10.5%
6.45
10.9%
6.74

8.8% 3.60
3.81
9.0%

7.3% 3.15
3.24
7.5%

7.60
8.16

5.38
5.51

Figure 5: Performance comparison for dif-
ferent DCRNN variants. DCRNN, with the
sequence to sequence framework and sched-
uled sampling, achieves the lowest MAE on
the validation dataset. The advantage be-
comes more clear with the increase of the
forecasting horizon.

Figure 6: Trafﬁc time series forecasting visualization.
DCRNN generates smooth prediction and is usually
better at predict the start and end of peak hours.

which only uses the forward random walk transition matrix for diffusion convolution; Figure 3 shows
the learning curves of these three models with roughly the same number of parameters. Without
diffusion convolution, DCRNN-NoConv has much higher validation error. Moreover, DCRNN
achieves the lowest validation error which shows the effectiveness of using bidirectional random
walk. The intuition is that the bidirectional random walk gives the model the ability and ﬂexibility to
capture the inﬂuence from both the upstream and the downstream trafﬁc.

To investigate the effect of graph construction, we construct a undirected graph by setting (cid:99)Wij =
(cid:99)Wji = max(Wij, Wji), where (cid:99)W is the new symmetric weight matrix. Then we develop a variant
of DCRNN denotes GCRNN, which uses the sequence to sequence learning with ChebNet graph
convolution (Equation 5) with roughly the same amount of parameters. Table 2 shows the comparison
between DCRNN and GCRNN in the METR-LA dataset. DCRNN consistently outperforms GCRNN.
The intuition is that directed graph better captures the asymmetric correlation between trafﬁc sensors.
Figure 4 shows the effects of different parameters. K roughly corresponds to the size of ﬁlters’
reception ﬁelds while the number of units corresponds to the number of ﬁlters. Larger K enables
the model to capture broader spatial dependency at the cost of increasing learning complexity. We
observe that with the increase of K, the error on the validation dataset ﬁrst quickly decrease, and
then slightly increase. Similar behavior is observed for varying the number of units.

4.4 EFFECT OF TEMPORAL DEPENDENCY MODELING

To evaluate the effect of temporal modeling including the sequence to sequence framework as well
as the scheduled sampling mechanism, we further design three variants of DCRNN: (1) DCNN: in
which we concatenate the historical observations as a ﬁxed length vector and feed it into stacked
diffusion convolutional layers to predict the future time series. We train a single model for one step
ahead prediction, and feed the previous prediction into the model as input to perform multiple steps
ahead prediction. (2) DCRNN-SEQ: which uses the encoder-decoder sequence to sequence learning
framework to perform multiple steps ahead forecasting. (3) DCRNN: similar to DCRNN-SEQ except
for adding scheduled sampling.

7

Published as a conference paper at ICLR 2018

Figure 7: Visualization of learned localized ﬁlters centered at different nodes with K = 3 on the
METR-LA dataset. The star denotes the center, and the colors represent the weights. We observe that
weights are localized around the center, and diffuse alongside the road network.

Figure 5 shows the comparison of those four methods with regards to MAE for different forecasting
horizons. We observe that: (1) DCRNN-SEQ outperforms DCNN by a large margin which conforms
the importance of modeling temporal dependency. (2) DCRNN achieves the best result, and its
superiority becomes more evident with the increase of the forecasting horizon. This is mainly because
the model is trained to deal with its mistakes during multiple steps ahead prediction and thus suffers
less from the problem of error propagation. We also train a model that always been fed its output as
input for multiple steps ahead prediction. However, its performance is much worse than all the three
variants which emphasizes the importance of scheduled sampling.

4.5 MODEL INTERPRETATION

To better understand the model, we visualize forecasting results as well as learned ﬁlters. Figure 6
shows the visualization of 1 hour ahead forecasting. We have the following observations: (1)
DCRNN generates smooth prediction of the mean when small oscillation exists in the trafﬁc speeds
(Figure 6(a)). This reﬂects the robustness of the model. (2) DCRNN is more likely to accurately
predict abrupt changes in the trafﬁc speed than baseline methods (e.g., FC-LSTM). As shown in
Figure 6(b), DCRNN predicts the start and the end of the peak hours. This is because DCRNN
captures the spatial dependency, and is able to utilize the speed changes in neighborhood sensors
for more accurate forecasting. Figure 7 visualizes examples of learned ﬁlters centered at different
nodes. The star denotes the center, and colors denote the weights. We can observe that (1) weights
are well localized around the center, and (2) the weights diffuse based on road network distance.
More visualizations are provided in Appendix F.

5 CONCLUSION

In this paper, we formulated the trafﬁc prediction on road network as a spatiotemporal forecasting
problem, and proposed the diffusion convolutional recurrent neural network that captures the spa-
tiotemporal dependencies. Speciﬁcally, we use bidirectional graph random walk to model spatial
dependency and recurrent neural network to capture the temporal dynamics. We further integrated the
encoder-decoder architecture and the scheduled sampling technique to improve the performance for
long-term forecasting. When evaluated on two large-scale real-world trafﬁc datasets, our approach
obtained signiﬁcantly better prediction than baselines. For future work, we will investigate the
following two aspects (1) applying the proposed model to other spatial-temporal forecasting tasks;
(2) modeling the spatiotemporal dependency when the underlying graph structure is evolving, e.g.,
the K nearest neighbor graph for moving objects.

ACKNOWLEDGMENTS

This research has been funded in part by NSF grants CNS-1461963, IIS-1254206, IIS-1539608,
Caltrans-65A0533, the USC Integrated Media Systems Center (IMSC), and the USC METRANS
Transportation Center. Any opinions, ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily reﬂect the views of any of the sponsors
such as NSF. Also, the authors would like to thank Shang-Hua Teng, Dehua Cheng and Siyang Li for
helpful discussions and comments.

8

Published as a conference paper at ICLR 2018

REFERENCES

Mart´ın Abadi et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.

arXiv preprint arXiv:1603.04467, 2016.

James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural

Information Processing Systems, pp. 1993–2001, 2016.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence

prediction with recurrent neural networks. In NIPS, pp. 1171–1179, 2015.

James S Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter
optimization. In Advances in Neural Information Processing Systems, pp. 2546–2554, 2011.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally

connected networks on graphs. In ICLR, 2014.

Pinlong Cai, Yunpeng Wang, Guangquan Lu, Peng Chen, Chuan Ding, and Jianping Sun. A
spatiotemporal correlative k-nearest neighbor model for short-term trafﬁc multistep forecasting.
Transportation Research Part C: Emerging Technologies, 62:21–34, 2016.

Ennio Cascetta. Transportation systems engineering: theory and methods, volume 49. Springer

Science & Business Media, 2013.

Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. Efﬁcient sampling for
gaussian graphical models via spectral sparsiﬁcation. In Conference on Learning Theory, pp.
364–390, 2015.

Xingyi Cheng, Ruiqing Zhang, Jie Zhou, and Wei Xu. Deeptransport: Learning spatial-temporal

dependency for trafﬁc condition forecasting. arXiv preprint arXiv:1709.09585, 2017.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on

graphs with fast localized spectral ﬁltering. In NIPS, pp. 3837–3845, 2016.

Dingxiong Deng, Cyrus Shahabi, Ugur Demiryurek, Linhong Zhu, Rose Yu, and Yan Liu. Latent
space model for road networks to predict time-varying trafﬁc. In SIGKDD, pp. 1525–1534, 2016.

Donald R Drew. Trafﬁc ﬂow theory and control. Technical report, 1968.

Gaetano Fusco, Chiara Colombaroni, and Natalia Isaenko. Short-term speed predictions exploiting
big data on large urban road networks. Transportation Research Part C: Emerging Technologies,
73:183–201, 2016.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional

sequence to sequence learning. In ICML, 2017.

Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
855–864. ACM, 2016.

James Douglas Hamilton. Time series analysis, volume 2. Princeton university press Princeton, 1994.

Yotam Hechtlinger, Purvasha Chakravarti, and Jining Qin. A generalization of convolutional neural

networks to graph-structured data. arXiv preprint arXiv:1704.08165, 2017.

H. V. Jagadish, Johannes Gehrke, Alexandros Labrinidis, Yannis Papakonstantinou, Jignesh M. Patel,
Raghu Ramakrishnan, and Cyrus Shahabi. Big data and its technical challenges. Commun. ACM,
57(7):86–94, July 2014.

Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.

In International Conference on Learning Representations (ICLR), 2017.

9

Published as a conference paper at ICLR 2018

Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event forecasting
with neural networks at Uber. In Int. Conf. on Machine Learning Time Series Workshop, 2017.

Marco Lippi, Marco Bertini, and Paolo Frasconi. Short-term trafﬁc ﬂow forecasting: An experimental
comparison of time-series analysis and supervised learning. ITS, IEEE Transactions on, 14(2):
871–882, 2013.

Wei Liu, Yu Zheng, Sanjay Chawla, Jing Yuan, and Xie Xing. Discovering spatio-temporal causal

interactions in trafﬁc data streams. In SIGKDD, pp. 1010–1018. ACM, 2011.

Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. Trafﬁc ﬂow prediction

with big data: A deep learning approach. ITS, IEEE Transactions on, 16(2):865–873, 2015.

Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang, and Yunpeng Wang. Learning
trafﬁc as images: a deep convolutional neural network for large-scale transportation network speed
prediction. Sensors, 17(4):818, 2017.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining, pp. 701–710. ACM, 2014.

Youngjoo Seo, Micha¨el Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence
modeling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659, 2016.

David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst.
The emerging ﬁeld of signal processing on graphs: Extending high-dimensional data analysis to
networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83–98, 2013.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

In NIPS, pp. 3104–3112, 2014.

Shang-Hua Teng et al. Scalable algorithms for data and network analysis. Foundations and Trends R(cid:13)

in Theoretical Computer Science, 12(1–2):1–274, 2016.

Eleni I Vlahogianni, Matthew G Karlaftis, and John C Golias. Short-term trafﬁc forecasting: Where
we are and where were going. Transportation Research Part C: Emerging Technologies, 43:3–19,
2014.

Yuankai Wu and Huachun Tan. Short-term trafﬁc ﬂow forecasting with spatial-temporal correlation

in a hybrid deep learning framework. arXiv preprint arXiv:1612.01022, 2016.

Yuanchang Xie, Kaiguang Zhao, Ying Sun, and Dawei Chen. Gaussian processes for short-term trafﬁc
volume forecasting. Transportation Research Record: Journal of the Transportation Research
Board, (2165):69–78, 2010.

Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional neural network: A

deep learning framework for trafﬁc forecasting. arXiv preprint arXiv:1709.04875, 2017a.

Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for
high-dimensional time series prediction. In Advances in Neural Information Processing Systems,
pp. 847–855, 2016.

Rose Yu, Yaguang Li, Cyrus Shahabi, Ugur Demiryurek, and Yan Liu. Deep learning: A generic
approach for extreme condition trafﬁc forecasting. In SIAM International Conference on Data
Mining (SDM), 2017b.

Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, and Xiuwen Yi. Dnn-based prediction model for
spatio-temporal data. In Proceedings of the 24th ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems, pp. 92. ACM, 2016.

Junbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide crowd

ﬂows prediction. In AAAI, pp. 1655–1661, 2017.

10

Published as a conference paper at ICLR 2018

APPENDIX

A NOTATION

Name
G
V, vi
E
W , Wij,
D, DI , DO
L
Φ, Λ
X, ˆX ∈ RN ×P
X (t) ∈ RN ×P
H ∈ RN ×Q
fθ, θ
fΘ, Θ

Table 3: Notation

a graph
nodes of a graph, |V| = N and the i-th node.
edges of a graph
weight matrix of a graph and its entries
undirected degree matrix, In-degree/out-degree matrix
normalized graph Laplacian
eigen-vector matrix and eigen-value matrix of L
a graph signal, and the predicted graph signal.
a graph signal at time t.
output of the diffusion convolutional layer.
convolutional ﬁlter and its parameters.
convolutional layer and its parameters.

Table 3 summarizes the main notations used in the paper.

B EFFICIENT CALCULATION OF EQUATION 2

O W and the other part with D−1

Equation 2 can be decomposed into two parts with the same time complexity, i.e., one part with
I W (cid:124). Thus we will only show the time complexity of the ﬁrst
D−1
part.
Let Tk(x) = (cid:0)D−1

x, The ﬁrst part of Equation 2 can be rewritten as

O W (cid:1)k

K−1
(cid:88)

k=0

θkTk(X:,p)

(4)

O W Tk(x) and D−1

As Tk+1(x) = D−1
O W is sparse, it is easy to see that Equation 4 can be
calculated using O(K) recursive sparse-dense matrix multiplication each with time complexity
O(|E|). Consequently, the time complexities of both Equation 2 and Equation 4 are O(K|E|). For
dense graph, we may use spectral sparsiﬁcation (Cheng et al., 2015) to make it sparse.

C RELATION WITH SPECTRAL GRAPH CONVOLUTION

Proof. The spectral graph convolution utilizes the concept of normalized graph Laplacian L =
2 = ΦΛΦ(cid:124). ChebNet parametrizes fθ to be a K order polynomial of Λ, and
D− 1
calculates it using stable Chebyshev polynomial basis.

2 (D − W )D− 1

X:,p (cid:63)G fθ = Φ

θkΛk

Φ(cid:124)X:,p =

θkLkX:,p =

˜θkTk( ˜L)X:,p

(5)

(cid:33)

(cid:32)K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

where T0(x) = 1, T1(x) = x, Tk(x) = xTk−1(x) − Tk−2(x) are the basis of the Cheyshev polyno-
mial. Let λmax denote the largest eigenvalue of L, and ˜L = 2
L − I represents a rescaling of the
graph Laplacian that maps the eigenvalues from [0, λmax] to [−1, 1] since Chebyshev polynomial
forms an orthogonal basis in [−1, 1]. Equation 5 can be considered as a polynomial of ˜L and we will
show that the output of ChebNet Convolution is similar to the output of diffusion convolution up to
constant scaling factor. Assume λmax = 2 and DI = DO = D for undirected graph.

λmax

˜L = D− 1

2 − I = −D− 1
(6)
˜L is similar to the negative random walk transition matrix, thus the output of Equation 5 is also
similar to the output of Equation 2 up to constant scaling factor.

2 (D − W )D− 1

2 ∼ −D−1W

2 W D− 1

11

Published as a conference paper at ICLR 2018

Figure 8: Sensor distribution of the METR-LA and PEMS-BAY dataset.

D MORE RELATED WORK AND DISCUSSION

Xie et al. (2010) introduce a Gaussian processes (GPs) based method. GPs are hard to scale to the
large dataset and are generally not suitable for relatively long-term trafﬁc prediction like 1 hour
(i.e.,12 steps ahead), as the variance can be accumulated and becomes extremely large.

Cai et al. (2016) propose to use spatiotemporal nearest neighbor for trafﬁc forecasting (ST-KNN).
Though ST-KNN considers both the spatial and the temporal dependencies, it has the following
drawbacks. As shown in Fusco et al. (2016), ST-KNN performs independent forecasting for each
individual road. The prediction of a road is a weighted combination of its own historical trafﬁc speeds.
This makes it hard for ST-KNN to fully utilize information from neighbors. Besides, ST-KNN is a
non-parametric approach and each road is modeled and calculated separately (Cai et al., 2016), which
makes it hard to generalize to unseen situations and to scale to large datasets. Finally, in ST-KNN, all
the similarities are calculated using hand-designed metrics with few learnable parameters, and this
may limit its representational power.

Cheng et al. (2017) propose DeepTransport which models the spatial dependency by explicitly
collecting certain number of upstream and downstream roads for each individual road and then
conduct convolution on these roads respectively. Comparing with Cheng et al. (2017), DCRNN
models the spatial dependency in a more systematic way, i.e., generalizing convolution to the trafﬁc
sensor graph based on the diffusion nature of trafﬁc. Besides, we derive DCRNN from the property of
random walk and show that the popular spectral convolution ChebNet is a special case of our method.

The proposed approach is also related to graph embedding techniques, e.g., Deepwalk (Perozzi et al.,
2014), node2vec (Grover & Leskovec, 2016) which learn a low dimension representation for each
node in the graph. DCRNN also learns a representation for each node. The learned representations
capture both the spatial and the temporal dependency and at the same time are optimized with
regarding to the objective, e.g., future trafﬁc speeds.

E DETAILED EXPERIMENTAL SETTINGS

HA Historical Average, which models the trafﬁc ﬂow as a seasonal process, and uses weighted
average of previous seasons as the prediction. The period used is 1 week, and the prediction is based
on aggregated data from previous weeks. For example, the prediction for this Wednesday is the
averaged trafﬁc speeds from last four Wednesdays. As the historical average method does not depend
on short-term data, its performance is invariant to the small increases in the forecasting horizon

ARIMAkal
are (3, 0, 1), and the model is implemented using the statsmodel python package.

: Auto-Regressive Integrated Moving Average model with Kalman ﬁlter. The orders

12

Published as a conference paper at ICLR 2018

VAR Vector Auto-regressive model (Hamilton, 1994). The number of lags is set to 3, and the model
is implemented using the statsmodel python package.

SVR Linear Support Vector Regression, the penalty term C = 0.1, the number of historical
observation is 5.

The following deep neural network based approaches are also included.

FNN Feed forward neural network with two hidden layers, each layer contains 256 units. The initial
learning rate is 1e−3, and reduces to 1
10 every 20 epochs starting at the 50th epochs. In addition, for
all hidden layers, dropout with ratio 0.5 and L2 weight decay 1e−2 is used. The model is trained with
batch size 64 and MAE as the loss function. Early stop is performed by monitoring the validation
error.

FC-LSTM The Encoder-decoder framework using LSTM with peephole (Sutskever et al., 2014).
Both the encoder and the decoder contain two recurrent layers. In each recurrent layer, there are 256
LSTM units, L1 weight decay is 2e−5, L2 weight decay 5e−4. The model is trained with batch size
64 and loss function MAE. The initial learning rate is 1e-4 and reduces to 1
10 every 10 epochs starting
from the 20th epochs. Early stop is performed by monitoring the validation error.

DCRNN : Diffusion Convolutional Recurrent Neural Network. Both encoder and decoder contain
two recurrent layers. In each recurrent layer, there are 64 units, the initial learning rate is 1e−2, and
reduces to 1
10 every 10 epochs starting at the 20th epoch and early stopping on the validation dataset
is used. Besides, the maximum steps of random walks, i.e., K, is set to 3. For scheduled sampling,
the thresholded inverse sigmoid function is used as the probability decay:

(cid:15)i =

τ
τ + exp (i/τ )

where i is the number of iterations while τ are parameters to control the speed of convergence. τ
is set to 3,000 in the experiments. The implementation is available in https://github.com/
liyaguang/DCRNN.

E.1 DATASET

We conduct experiments on two real-world large-scale datasets:

• METR-LA This trafﬁc dataset contains trafﬁc information collected from loop detectors
in the highway of Los Angeles County (Jagadish et al., 2014). We select 207 sensors and
collect 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 for the experiment.
The total number of observed trafﬁc data points is 6,519,002.

• PEMS-BAY This trafﬁc dataset is collected by California Transportation Agencies (Cal-
Trans) Performance Measurement System (PeMS). We select 325 sensors in the Bay Area
and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for the experiment.
The total number of observed trafﬁc data points is 16,937,179.

The sensor distributions of both datasets are visualized in Figure 8.

In both of those datasets, we aggregate trafﬁc speed readings into 5 minutes windows, and apply
Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining
10% for validation. To construct the sensor graph, we compute the pairwise road network distances
between sensors and build the adjacency matrix using thresholded Gaussian kernel (Shuman et al.,
2013).

Wij = exp

−

(cid:18)

(cid:19)

dist(vi, vj)2
σ2

if dist(vi, vj) ≤ κ, otherwise 0

where Wij represents the edge weight between sensor vi and sensor vj, dist(vi, vj) denotes the road
network distance from sensor vi to sensor vj. σ is the standard deviation of distances and κ is the
threshold.

13

Published as a conference paper at ICLR 2018

E.2 METRICS

Suppose x = x1, · · · , xn represents the ground truth, ˆx = ˆx1, · · · , ˆxn represents the predicted
values, and Ω denotes the indices of observed samples, the metrics are deﬁned as follows.

Root Mean Square Error (RMSE)

RMSE(x, ˆx) =

(xi − ˆxi)2

(cid:115) 1
|Ω|

(cid:88)

i∈Ω

MAPE(x, ˆx) =

1
|Ω|

(cid:88)

i∈Ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

xi − ˆxi
xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

MAE(x, ˆx) =

|xi − ˆxi|

1
|Ω|

(cid:88)

i∈Ω

Mean Absolute Percentage Error (MAPE)

Mean Absolute Error (MAE)

F MODEL VISUALIZATION

Figure 9: Sensor correlations between the center sensor and its neighborhoods for different forecasting
horizons. The correlations are estimated using regularized VAR. We observe that the correlations are
localized and closer neighborhoods usually have larger relevance, and the magnitude of correlation
quickly decay with the increase of distance which is consistent with the diffusion process on the
graph.

14

Published as a conference paper at ICLR 2018

15
Figure 10: Trafﬁc time series forecasting visualization.

Published as a conference paper at ICLR 2018

16
Figure 11: Trafﬁc time series forecasting visualization.

8
1
0
2
 
b
e
F
 
2
2
 
 
]

G
L
.
s
c
[
 
 
3
v
6
2
9
1
0
.
7
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

DIFFUSION CONVOLUTIONAL RECURRENT NEURAL
NETWORK: DATA-DRIVEN TRAFFIC FORECASTING

Yaguang Li†, Rose Yu‡, Cyrus Shahabi†, Yan Liu†
† University of Southern California, ‡ California Institute of Technology
† {yaguang, shahabi, yanliu.cs}@usc.edu, ‡ rose@caltech.edu

ABSTRACT
Spatiotemporal forecasting has various applications in neuroscience, climate and
transportation domain. Trafﬁc forecasting is one canonical example of such learn-
ing task. The task is challenging due to (1) complex spatial dependency on road
networks, (2) non-linear temporal dynamics with changing road conditions and
(3) inherent difﬁculty of long-term forecasting. To address these challenges, we
propose to model the trafﬁc ﬂow as a diffusion process on a directed graph and
introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep
learning framework for trafﬁc forecasting that incorporates both spatial and tem-
poral dependency in the trafﬁc ﬂow. Speciﬁcally, DCRNN captures the spatial
dependency using bidirectional random walks on the graph, and the temporal de-
pendency using the encoder-decoder architecture with scheduled sampling. We
evaluate the framework on two real-world large scale road network trafﬁc datasets
and observe consistent improvement of 12% - 15% over state-of-the-art baselines.

INTRODUCTION

1
Spatiotemporal forecasting is a crucial task for a learning system that operates in a dynamic environ-
ment. It has a wide range of applications from autonomous vehicles operations, to energy and smart
grid optimization, to logistics and supply chain management. In this paper, we study one important
task: trafﬁc forecasting on road networks, the core component of the intelligent transportation systems.
The goal of trafﬁc forecasting is to predict the future trafﬁc speeds of a sensor network given historic
trafﬁc speeds and the underlying road networks.

This task is challenging mainly due to the com-
plex spatiotemporal dependencies and inher-
ent difﬁculty in the long term forecasting. On
the one hand, trafﬁc time series demonstrate
strong temporal dynamics. Recurring incidents
such as rush hours or accidents can cause non-
stationarity, making it difﬁcult to forecast long-
term. On the other hand, sensors on the road
network contain complex yet unique spatial cor-
relations. Figure 1 illustrates an example. Road
1 and road 2 are correlated, while road 1 and
road 3 are not. Although road 1 and road 3
are close in the Euclidean space, they demon-
strate very different behaviors. Moreover, the
future trafﬁc speed is inﬂuenced more by the
downstream trafﬁc than the upstream one. This
means that the spatial structure in trafﬁc is non-
Euclidean and directional.

Figure 1: Spatial correlation is dominated by road
network structure. (1) Trafﬁc speed in road 1 are
similar to road 2 as they locate in the same highway.
(2) Road 1 and road 3 locate in the opposite direc-
tions of the highway. Though close to each other
in the Euclidean space, their road network distance
is large, and their trafﬁc speeds differ signiﬁcantly.

Trafﬁc forecasting has been studied for decades,
falling into two main categories: knowledge-
driven approach and data-driven approach. In transportation and operational research, knowledge-
driven methods usually apply queuing theory and simulate user behaviors in trafﬁc (Cascetta, 2013).
In time series community, data-driven methods such as Auto-Regressive Integrated Moving Average
(ARIMA) model and Kalman ﬁltering remain popular (Liu et al., 2011; Lippi et al., 2013). However,
simple time series models usually rely on the stationarity assumption, which is often violated by

1

Published as a conference paper at ICLR 2018

the trafﬁc data. Most recently, deep learning models for trafﬁc forecasting have been developed
in Lv et al. (2015); Yu et al. (2017b), but without considering the spatial structure. Wu & Tan (2016)
and Ma et al. (2017) model the spatial correlation with Convolutional Neural Networks (CNN), but
the spatial structure is in the Euclidean space (e.g., 2D images). Bruna et al. (2014), Defferrard et al.
(2016) studied graph convolution, but only for undirected graphs.

In this work, we represent the pair-wise spatial correlations between trafﬁc sensors using a directed
graph whose nodes are sensors and edge weights denote proximity between the sensor pairs measured
by the road network distance. We model the dynamics of the trafﬁc ﬂow as a diffusion process and
propose the diffusion convolution operation to capture the spatial dependency. We further propose
Diffusion Convolutional Recurrent Neural Network (DCRNN) that integrates diffusion convolution,
the sequence to sequence architecture and the scheduled sampling technique. When evaluated on real-
world trafﬁc datasets, DCRNN consistently outperforms state-of-the-art trafﬁc forecasting baselines
by a large margin. In summary:

• We study the trafﬁc forecasting problem and model the spatial dependency of trafﬁc as
a diffusion process on a directed graph. We propose diffusion convolution, which has an
intuitive interpretation and can be computed efﬁciently.

• We propose Diffusion Convolutional Recurrent Neural Network (DCRNN), a holistic ap-
proach that captures both spatial and temporal dependencies among time series using
diffusion convolution and the sequence to sequence learning framework together with sched-
uled sampling. DCRNN is not limited to transportation and is readily applicable to other
spatiotemporal forecasting tasks.

• We conducted extensive experiments on two large-scale real-world datasets, and the proposed

approach obtains signiﬁcant improvement over state-of-the-art baseline methods.

2 METHODOLOGY
We formalize the learning problem of spatiotemporal trafﬁc forecasting and describe how to model
the dependency structures using diffusion convolutional recurrent neural network.

2.1 TRAFFIC FORECASTING PROBLEM
The goal of trafﬁc forecasting is to predict the future trafﬁc speed given previously observed trafﬁc
ﬂow from N correlated sensors on the road network. We can represent the sensor network as a
weighted directed graph G = (V, E, W ), where V is a set of nodes |V| = N , E is a set of edges and
W ∈ RN ×N is a weighted adjacency matrix representing the nodes proximity (e.g., a function of
their road network distance). Denote the trafﬁc ﬂow observed on G as a graph signal X ∈ RN ×P ,
where P is the number of features of each node (e.g., velocity, volume). Let X (t) represent the graph
signal observed at time t, the trafﬁc forecasting problem aims to learn a function h(·) that maps T (cid:48)
historical graph signals to future T graph signals, given a graph G:

[X (t−T (cid:48)+1), · · · , X (t); G]

h(·)
−−→ [X (t+1), · · · , X (t+T )]

2.2 SPATIAL DEPENDENCY MODELING

We model the spatial dependency by relating trafﬁc ﬂow to a diffusion process, which explicitly
captures the stochastic nature of trafﬁc dynamics. This diffusion process is characterized by a
random walk on G with restart probability α ∈ [0, 1], and a state transition matrix D−1
O W . Here
DO = diag(W 1) is the out-degree diagonal matrix, and 1 ∈ RN denotes the all one vector. After
many time steps, such Markov process converges to a stationary distribution P ∈ RN ×N whose ith
row P i,: ∈ RN represents the likelihood of diffusion from node vi ∈ V, hence the proximity w.r.t.
the node vi. The following Lemma provides a closed form solution for the stationary distribution.
Lemma 2.1. (Teng et al., 2016) The stationary distribution of the diffusion process can be represented
as a weighted combination of inﬁnite random walks on the graph, and be calculated in closed form:

P =

α(1 − α)k (cid:0)D−1

O W (cid:1)k

∞
(cid:88)

k=0

(1)

where k is the diffusion step. In practice, we use a ﬁnite K-step truncation of the diffusion process
and assign a trainable weight to each step. We also include the reversed direction diffusion process,

2

Published as a conference paper at ICLR 2018

such that the bidirectional diffusion offers the model more ﬂexibility to capture the inﬂuence from
both the upstream and the downstream trafﬁc.

Diffusion Convolution The resulted diffusion convolution operation over a graph signal X ∈
RN ×P and a ﬁlter fθ is deﬁned as:

X:,p (cid:63)G fθ =

θk,1

(cid:0)D−1

O W (cid:1)k

+ θk,2

(cid:0)D−1

I W (cid:124)(cid:1)k(cid:17)

X:,p

for p ∈ {1, · · · , P }

(2)

K−1
(cid:88)

(cid:16)

k=0

I W (cid:124) represent the transition
where θ ∈ RK×2 are the parameters for the ﬁlter and D−1
O W , D−1
matrices of the diffusion process and the reverse one, respectively.
In general, computing the
convolution can be expensive. However, if G is sparse, Equation 2 can be calculated efﬁciently using
O(K) recursive sparse-dense matrix multiplication with total time complexity O(K|E|) (cid:28) O(N 2).
See Appendix B for more detail.

Diffusion Convolutional Layer With the convolution operation deﬁned in Equation 2, we can
build a diffusion convolutional layer that maps P -dimensional features to Q-dimensional outputs.
Denote the parameter tensor as Θ ∈ RQ×P ×K×2 = [θ]q,p, where Θq,p,:,: ∈ RK×2 parameterizes
the convolutional ﬁlter for the pth input and the qth output. The diffusion convolutional layer is thus:

H:,q = a

X:,p (cid:63)G fΘq,p,:,:

for q ∈ {1, · · · , Q}

(3)

(cid:32) P

(cid:88)

p=1

(cid:33)

where X ∈ RN ×P is the input, H ∈ RN ×Q is the output, {fΘq,p,,: } are the ﬁlters and a is the
activation function (e.g., ReLU, Sigmoid). Diffusion convolutional layer learns the representations
for graph structured data and we can train it using stochastic gradient based method.

Relation with Spectral Graph Convolution Diffusion convolution is deﬁned on both directed and
undirected graphs. When applied to undirected graphs, we show that many existing graph structured
convolutional operations including the popular spectral graph convolution, i.e., ChebNet (Defferrard
et al., 2016), can be considered as a special case of diffusion convolution (up to a similarity transfor-
mation). Let D denote the degree matrix, and L = D− 1
2 be the normalized graph
Laplacian, the following Proposition demonstrates the connection.
Proposition 2.2. The spectral graph convolution deﬁned as

2 (D − W )D− 1

X:,p (cid:63)G fθ = Φ F (θ) Φ(cid:124)X:,p

with eigenvalue decomposition L = ΦΛΦ(cid:124) and F (θ) = (cid:80)K−1
diffusion convolution up to a similarity transformation, when the graph G is undirected.

θkΛk, is equivalent to graph

0

Proof. See Appendix C.

2.3 TEMPORAL DYNAMICS MODELING
We leverage the recurrent neural networks (RNNs) to model the temporal dependency. In particular,
we use Gated Recurrent Units (GRU) (Chung et al., 2014), which is a simple yet powerful variant of
RNNs. We replace the matrix multiplications in GRU with the diffusion convolution, which leads to
our proposed Diffusion Convolutional Gated Recurrent Unit (DCGRU).

r(t) =
C(t) = tanh(ΘC (cid:63)G

σ(Θr (cid:63)G [X (t), H (t−1)] + br)

u(t) = σ(Θu (cid:63)G [X (t), H (t−1)] + bu)

(cid:2)X (t), (r(t) (cid:12) H (t−1))(cid:3) + bc) H (t) = u(t) (cid:12) H (t−1) + (1 − u(t)) (cid:12) C(t)

where X (t), H (t) denote the input and output of at time t, r(t), u(t) are reset gate and update gate at
time t, respectively. (cid:63)G denotes the diffusion convolution deﬁned in Equation 2 and Θr, Θu, ΘC are
parameters for the corresponding ﬁlters. Similar to GRU, DCGRU can be used to build recurrent
neural network layers and be trained using backpropagation through time.

In multiple step ahead forecasting, we employ the Sequence to Sequence architecture (Sutskever
et al., 2014). Both the encoder and the decoder are recurrent neural networks with DCGRU. During
training, we feed the historical time series into the encoder and use its ﬁnal states to initialize the

3

Published as a conference paper at ICLR 2018

Figure 2: System architecture for the Diffusion Convolutional Recurrent Neural Network designed
for spatiotemporal trafﬁc forecasting. The historical time series are fed into an encoder whose ﬁnal
states are used to initialize the decoder. The decoder makes predictions based on either previous
ground truth or the model output.

decoder. The decoder generates predictions given previous ground truth observations. At testing time,
ground truth observations are replaced by predictions generated by the model itself. The discrepancy
between the input distributions of training and testing can cause degraded performance. To mitigate
this issue, we integrate scheduled sampling (Bengio et al., 2015) into the model, where we feed the
model with either the ground truth observation with probability (cid:15)i or the prediction by the model with
probability 1 − (cid:15)i at the ith iteration. During the training process, (cid:15)i gradually decreases to 0 to allow
the model to learn the testing distribution.

With both spatial and temporal modeling, we build a Diffusion Convolutional Recurrent Neural
Network (DCRNN). The model architecture of DCRNN is shown in Figure 2. The entire network is
trained by maximizing the likelihood of generating the target future time series using backpropagation
through time. DCRNN is able to capture spatiotemporal dependencies among time series and can be
applied to various spatiotemporal forecasting problems.

3 RELATED WORK

Trafﬁc forecasting is a classic problem in transportation and operational research which are primarily
based on queuing theory and simulations (Drew, 1968). Data-driven approaches for trafﬁc forecasting
have received considerable attention, and more details can be found in a recent survey paper (Vla-
hogianni et al., 2014) and the references therein. However, existing machine learning models either
impose strong stationary assumptions on the data (e.g., auto-regressive model) or fail to account for
highly non-linear temporal dependency (e.g., latent space model Yu et al. (2016); Deng et al. (2016)).
Deep learning models deliver new promise for time series forecasting problem. For example, in Yu
et al. (2017b); Laptev et al. (2017), the authors study time series forecasting using deep Recurrent
Neural Networks (RNN). Convolutional Neural Networks (CNN) have also been applied to trafﬁc
forecasting. Zhang et al. (2016; 2017) convert the road network to a regular 2-D grid and apply
traditional CNN to predict crowd ﬂow. Cheng et al. (2017) propose DeepTransport which models the
spatial dependency by explicitly collecting upstream and downstream neighborhood roads for each
individual road and then conduct convolution on these neighborhoods respectively.

Recently, CNN has been generalized to arbitrary graphs based on the spectral graph theory. Graph
convolutional neural networks (GCN) are ﬁrst introduced in Bruna et al. (2014), which bridges the
spectral graph theory and deep neural networks. Defferrard et al. (2016) propose ChebNet which
improves GCN with fast localized convolutions ﬁlters. Kipf & Welling (2017) simplify ChebNet
and achieve state-of-the-art performance in semi-supervised classiﬁcation tasks. Seo et al. (2016)
combine ChebNet with Recurrent Neural Networks (RNN) for structured sequence modeling. Yu
et al. (2017a) model the sensor network as a undirected graph and applied ChebNet and convolutional
sequence model (Gehring et al., 2017) to do forecasting. One limitation of the mentioned spectral
based convolutions is that they generally require the graph to be undirected to calculate meaningful

4

Published as a conference paper at ICLR 2018

Table 1: Performance comparison of different approaches for trafﬁc speed forecasting. DCRNN
achieves the best performance with all three metrics for all forecasting horizons, and the advantage
becomes more evident with the increase of the forecasting horizon.

T

Metric

HA

ARIMAKal

VAR

SVR

FNN

FC-LSTM DCRNN

A
L
-
R
T
E
M

Y
A
B
-
S
M
E
P

15 min

30 min

1 hour

15 min

30 min

1 hour

4.16
MAE
RMSE
7.80
MAPE 13.0%
4.16
MAE
RMSE
7.80
MAPE 13.0%
4.16
MAE
RMSE
7.80
MAPE 13.0%

MAE
RMSE
MAPE
MAE
RMSE
MAPE
MAE
RMSE
MAPE

2.88
5.59
6.8%
2.88
5.59
6.8%
2.88
5.59
6.8%

3.99
8.21
9.6%
5.15
10.45
12.7%
6.90
13.23
17.4%

1.62
3.30
3.5%
2.33
4.76
5.4%
3.38
6.50
8.3%

3.99
3.99
4.42
7.94
7.89
8.45
9.9%
10.2% 9.3%
4.23
5.05
5.41
9.13
8.17
10.87
12.7% 12.1% 12.9%
4.49
6.72
6.52
10.11
8.69
13.76
15.8% 16.7% 14.0%

1.74
3.16
3.6%
2.32
4.25
5.0%
2.93
5.44
6.5%

2.20
1.85
3.59
4.42
3.8% 5.19%
2.30
2.48
5.18
4.63
5.5% 5.43%
2.46
3.28
7.08
4.98
8.0% 5.89%

3.44
6.30
9.6%
3.77
7.23
10.9%
4.37
8.69
13.2%

2.05
4.19
4.8%
2.20
4.55
5.2%
2.37
4.96
5.7%

2.77
5.38
7.3%
3.15
6.45
8.8%
3.60
7.59
10.5%

1.38
2.95
2.9%
1.74
3.97
3.9%
2.07
4.74
4.9%

spectral decomposition. Going from spectral domain to vertex domain, Atwood & Towsley (2016)
propose diffusion-convolutional neural network (DCNN) which deﬁnes convolution as a diffusion
process across each node in a graph-structured input. Hechtlinger et al. (2017) propose GraphCNN to
generalize convolution to graph by convolving every node with its p nearest neighbors. However,
both these methods do not consider the temporal dynamics and mainly deal with static graph settings.

Our approach is different from all those methods due to both the problem settings and the formulation
of the convolution on the graph. We model the sensor network as a weighted directed graph which
is more realistic than grid or undirected graph. Besides, the proposed convolution is deﬁned using
bidirectional graph random walk and is further integrated with the sequence to sequence learning
framework as well as the scheduled sampling to model the long-term temporal dependency.

4 EXPERIMENTS

We conduct experiments on two real-world large-scale datasets: (1) METR-LA This trafﬁc dataset
contains trafﬁc information collected from loop detectors in the highway of Los Angeles County (Ja-
gadish et al., 2014). We select 207 sensors and collect 4 months of data ranging from Mar 1st 2012
to Jun 30th 2012 for the experiment. (2) PEMS-BAY This trafﬁc dataset is collected by California
Transportation Agencies (CalTrans) Performance Measurement System (PeMS). We select 325
sensors in the Bay Area and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for
the experiment. The sensor distributions of both datasets are visualized in Figure 8 in the Appendix.

(cid:16)

In both of those datasets, we aggregate trafﬁc speed readings into 5 minutes windows, and apply
Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining
10% for validation. To construct the sensor graph, we compute the pairwise road network distances
between sensors and build the adjacency matrix using thresholded Gaussian kernel (Shuman et al.,
2013). Wij = exp
if dist(vi, vj) ≤ κ, otherwise 0, where Wij represents the edge
weight between sensor vi and sensor vj, dist(vi, vj) denotes the road network distance from sensor
vi to sensor vj. σ is the standard deviation of distances and κ is the threshold.
4.1 EXPERIMENTAL SETTINGS
Baselines We compare DCRNN1 with widely used time series regression models, including (1)
HA: Historical Average, which models the trafﬁc ﬂow as a seasonal process, and uses weighted

− dist(vi,vj )2
σ2

(cid:17)

1The source code is available at https://github.com/liyaguang/DCRNN.

5

Published as a conference paper at ICLR 2018

Figure 3: Learning curve for DCRNN and
DCRNN without diffusion convolution. Remov-
ing diffusion convolution results in much higher
validation error. Moreover, DCRNN with bi-
directional random walk achieves the lowest val-
idation error.

Figure 4: Effects of K and the number of units
in each layer of DCRNN. K corresponds to the
reception ﬁeld width of the ﬁlter, and the number
of units corresponds to the number of ﬁlters.

average of previous seasons as the prediction; (2) ARIMAkal: Auto-Regressive Integrated Moving
Average model with Kalman ﬁlter which is widely used in time series prediction; (3) VAR: Vector
Auto-Regression (Hamilton, 1994). (4) SVR: Support Vector Regression which uses linear support
vector machine for the regression task; The following deep neural network based approaches are also
included: (5) Feed forward Neural network (FNN): Feed forward neural network with two hidden
layers and L2 regularization. (6) Recurrent Neural Network with fully connected LSTM hidden units
(FC-LSTM) (Sutskever et al., 2014).

All neural network based approaches are implemented using Tensorﬂow (Abadi et al., 2016), and
trained using the Adam optimizer with learning rate annealing. The best hyperparameters are chosen
using the Tree-structured Parzen Estimator (TPE) (Bergstra et al., 2011) on the validation dataset.
Detailed parameter settings for DCRNN as well as baselines are available in Appendix E.

4.2 TRAFFIC FORECASTING PERFORMANCE COMPARISON

Table 1 shows the comparison of different approaches for 15 minutes, 30 minutes and 1 hour ahead
forecasting on both datasets. These methods are evaluated based on three commonly used metrics in
trafﬁc forecasting, including (1) Mean Absolute Error (MAE), (2) Mean Absolute Percentage Error
(MAPE), and (3) Root Mean Squared Error (RMSE). Missing values are excluded in calculating
these metrics. Detailed formulations of these metrics are provided in Appendix E.2. We observe the
following phenomenon in both of these datasets. (1) RNN-based methods, including FC-LSTM and
DCRNN, generally outperform other baselines which emphasizes the importance of modeling the
temporal dependency. (2) DCRNN achieves the best performance regarding all the metrics for all
forecasting horizons, which suggests the effectiveness of spatiotemporal dependency modeling. (3)
Deep neural network based methods including FNN, FC-LSTM and DCRNN, tend to have better
performance than linear baselines for long-term forecasting, e.g., 1 hour ahead. This is because the
temporal dependency becomes increasingly non-linear with the growth of the horizon. Besides, as
the historical average method does not depend on short-term data, its performance is invariant to the
small increases in the forecasting horizon.

Note that, trafﬁc forecasting on the METR-LA (Los Angeles, which is known for its complicated
trafﬁc conditions) dataset is more challenging than that in the PEMS-BAY (Bay Area) dataset. Thus
we use METR-LA as the default dataset for following experiments.

4.3 EFFECT OF SPATIAL DEPENDENCY MODELING
To further investigate the effect of spatial dependency modeling, we compare DCRNN with the
following variants: (1) DCRNN-NoConv, which ignores spatial dependency by replacing the transition
matrices in the diffusion convolution (Equation 2) with identity matrices. This essentially means the
forecasting of a sensor can be only be inferred from its own historical readings; (2) DCRNN-UniConv,

6

Published as a conference paper at ICLR 2018

Table 2: Performance comparison for DCRNN and GCRNN on the METRA-LA dataset.

15 min

30 min

1 hour

DCRNN 2.77
GCRNN 2.80

MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
10.5%
6.45
10.9%
6.74

8.8% 3.60
3.81
9.0%

7.3% 3.15
3.24
7.5%

7.60
8.16

5.38
5.51

Figure 5: Performance comparison for dif-
ferent DCRNN variants. DCRNN, with the
sequence to sequence framework and sched-
uled sampling, achieves the lowest MAE on
the validation dataset. The advantage be-
comes more clear with the increase of the
forecasting horizon.

Figure 6: Trafﬁc time series forecasting visualization.
DCRNN generates smooth prediction and is usually
better at predict the start and end of peak hours.

which only uses the forward random walk transition matrix for diffusion convolution; Figure 3 shows
the learning curves of these three models with roughly the same number of parameters. Without
diffusion convolution, DCRNN-NoConv has much higher validation error. Moreover, DCRNN
achieves the lowest validation error which shows the effectiveness of using bidirectional random
walk. The intuition is that the bidirectional random walk gives the model the ability and ﬂexibility to
capture the inﬂuence from both the upstream and the downstream trafﬁc.

To investigate the effect of graph construction, we construct a undirected graph by setting (cid:99)Wij =
(cid:99)Wji = max(Wij, Wji), where (cid:99)W is the new symmetric weight matrix. Then we develop a variant
of DCRNN denotes GCRNN, which uses the sequence to sequence learning with ChebNet graph
convolution (Equation 5) with roughly the same amount of parameters. Table 2 shows the comparison
between DCRNN and GCRNN in the METR-LA dataset. DCRNN consistently outperforms GCRNN.
The intuition is that directed graph better captures the asymmetric correlation between trafﬁc sensors.
Figure 4 shows the effects of different parameters. K roughly corresponds to the size of ﬁlters’
reception ﬁelds while the number of units corresponds to the number of ﬁlters. Larger K enables
the model to capture broader spatial dependency at the cost of increasing learning complexity. We
observe that with the increase of K, the error on the validation dataset ﬁrst quickly decrease, and
then slightly increase. Similar behavior is observed for varying the number of units.

4.4 EFFECT OF TEMPORAL DEPENDENCY MODELING

To evaluate the effect of temporal modeling including the sequence to sequence framework as well
as the scheduled sampling mechanism, we further design three variants of DCRNN: (1) DCNN: in
which we concatenate the historical observations as a ﬁxed length vector and feed it into stacked
diffusion convolutional layers to predict the future time series. We train a single model for one step
ahead prediction, and feed the previous prediction into the model as input to perform multiple steps
ahead prediction. (2) DCRNN-SEQ: which uses the encoder-decoder sequence to sequence learning
framework to perform multiple steps ahead forecasting. (3) DCRNN: similar to DCRNN-SEQ except
for adding scheduled sampling.

7

Published as a conference paper at ICLR 2018

Figure 7: Visualization of learned localized ﬁlters centered at different nodes with K = 3 on the
METR-LA dataset. The star denotes the center, and the colors represent the weights. We observe that
weights are localized around the center, and diffuse alongside the road network.

Figure 5 shows the comparison of those four methods with regards to MAE for different forecasting
horizons. We observe that: (1) DCRNN-SEQ outperforms DCNN by a large margin which conforms
the importance of modeling temporal dependency. (2) DCRNN achieves the best result, and its
superiority becomes more evident with the increase of the forecasting horizon. This is mainly because
the model is trained to deal with its mistakes during multiple steps ahead prediction and thus suffers
less from the problem of error propagation. We also train a model that always been fed its output as
input for multiple steps ahead prediction. However, its performance is much worse than all the three
variants which emphasizes the importance of scheduled sampling.

4.5 MODEL INTERPRETATION

To better understand the model, we visualize forecasting results as well as learned ﬁlters. Figure 6
shows the visualization of 1 hour ahead forecasting. We have the following observations: (1)
DCRNN generates smooth prediction of the mean when small oscillation exists in the trafﬁc speeds
(Figure 6(a)). This reﬂects the robustness of the model. (2) DCRNN is more likely to accurately
predict abrupt changes in the trafﬁc speed than baseline methods (e.g., FC-LSTM). As shown in
Figure 6(b), DCRNN predicts the start and the end of the peak hours. This is because DCRNN
captures the spatial dependency, and is able to utilize the speed changes in neighborhood sensors
for more accurate forecasting. Figure 7 visualizes examples of learned ﬁlters centered at different
nodes. The star denotes the center, and colors denote the weights. We can observe that (1) weights
are well localized around the center, and (2) the weights diffuse based on road network distance.
More visualizations are provided in Appendix F.

5 CONCLUSION

In this paper, we formulated the trafﬁc prediction on road network as a spatiotemporal forecasting
problem, and proposed the diffusion convolutional recurrent neural network that captures the spa-
tiotemporal dependencies. Speciﬁcally, we use bidirectional graph random walk to model spatial
dependency and recurrent neural network to capture the temporal dynamics. We further integrated the
encoder-decoder architecture and the scheduled sampling technique to improve the performance for
long-term forecasting. When evaluated on two large-scale real-world trafﬁc datasets, our approach
obtained signiﬁcantly better prediction than baselines. For future work, we will investigate the
following two aspects (1) applying the proposed model to other spatial-temporal forecasting tasks;
(2) modeling the spatiotemporal dependency when the underlying graph structure is evolving, e.g.,
the K nearest neighbor graph for moving objects.

ACKNOWLEDGMENTS

This research has been funded in part by NSF grants CNS-1461963, IIS-1254206, IIS-1539608,
Caltrans-65A0533, the USC Integrated Media Systems Center (IMSC), and the USC METRANS
Transportation Center. Any opinions, ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily reﬂect the views of any of the sponsors
such as NSF. Also, the authors would like to thank Shang-Hua Teng, Dehua Cheng and Siyang Li for
helpful discussions and comments.

8

Published as a conference paper at ICLR 2018

REFERENCES

Mart´ın Abadi et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.

arXiv preprint arXiv:1603.04467, 2016.

James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural

Information Processing Systems, pp. 1993–2001, 2016.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence

prediction with recurrent neural networks. In NIPS, pp. 1171–1179, 2015.

James S Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter
optimization. In Advances in Neural Information Processing Systems, pp. 2546–2554, 2011.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally

connected networks on graphs. In ICLR, 2014.

Pinlong Cai, Yunpeng Wang, Guangquan Lu, Peng Chen, Chuan Ding, and Jianping Sun. A
spatiotemporal correlative k-nearest neighbor model for short-term trafﬁc multistep forecasting.
Transportation Research Part C: Emerging Technologies, 62:21–34, 2016.

Ennio Cascetta. Transportation systems engineering: theory and methods, volume 49. Springer

Science & Business Media, 2013.

Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. Efﬁcient sampling for
gaussian graphical models via spectral sparsiﬁcation. In Conference on Learning Theory, pp.
364–390, 2015.

Xingyi Cheng, Ruiqing Zhang, Jie Zhou, and Wei Xu. Deeptransport: Learning spatial-temporal

dependency for trafﬁc condition forecasting. arXiv preprint arXiv:1709.09585, 2017.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on

graphs with fast localized spectral ﬁltering. In NIPS, pp. 3837–3845, 2016.

Dingxiong Deng, Cyrus Shahabi, Ugur Demiryurek, Linhong Zhu, Rose Yu, and Yan Liu. Latent
space model for road networks to predict time-varying trafﬁc. In SIGKDD, pp. 1525–1534, 2016.

Donald R Drew. Trafﬁc ﬂow theory and control. Technical report, 1968.

Gaetano Fusco, Chiara Colombaroni, and Natalia Isaenko. Short-term speed predictions exploiting
big data on large urban road networks. Transportation Research Part C: Emerging Technologies,
73:183–201, 2016.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional

sequence to sequence learning. In ICML, 2017.

Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
855–864. ACM, 2016.

James Douglas Hamilton. Time series analysis, volume 2. Princeton university press Princeton, 1994.

Yotam Hechtlinger, Purvasha Chakravarti, and Jining Qin. A generalization of convolutional neural

networks to graph-structured data. arXiv preprint arXiv:1704.08165, 2017.

H. V. Jagadish, Johannes Gehrke, Alexandros Labrinidis, Yannis Papakonstantinou, Jignesh M. Patel,
Raghu Ramakrishnan, and Cyrus Shahabi. Big data and its technical challenges. Commun. ACM,
57(7):86–94, July 2014.

Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.

In International Conference on Learning Representations (ICLR), 2017.

9

Published as a conference paper at ICLR 2018

Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event forecasting
with neural networks at Uber. In Int. Conf. on Machine Learning Time Series Workshop, 2017.

Marco Lippi, Marco Bertini, and Paolo Frasconi. Short-term trafﬁc ﬂow forecasting: An experimental
comparison of time-series analysis and supervised learning. ITS, IEEE Transactions on, 14(2):
871–882, 2013.

Wei Liu, Yu Zheng, Sanjay Chawla, Jing Yuan, and Xie Xing. Discovering spatio-temporal causal

interactions in trafﬁc data streams. In SIGKDD, pp. 1010–1018. ACM, 2011.

Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. Trafﬁc ﬂow prediction

with big data: A deep learning approach. ITS, IEEE Transactions on, 16(2):865–873, 2015.

Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang, and Yunpeng Wang. Learning
trafﬁc as images: a deep convolutional neural network for large-scale transportation network speed
prediction. Sensors, 17(4):818, 2017.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining, pp. 701–710. ACM, 2014.

Youngjoo Seo, Micha¨el Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence
modeling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659, 2016.

David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst.
The emerging ﬁeld of signal processing on graphs: Extending high-dimensional data analysis to
networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83–98, 2013.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

In NIPS, pp. 3104–3112, 2014.

Shang-Hua Teng et al. Scalable algorithms for data and network analysis. Foundations and Trends R(cid:13)

in Theoretical Computer Science, 12(1–2):1–274, 2016.

Eleni I Vlahogianni, Matthew G Karlaftis, and John C Golias. Short-term trafﬁc forecasting: Where
we are and where were going. Transportation Research Part C: Emerging Technologies, 43:3–19,
2014.

Yuankai Wu and Huachun Tan. Short-term trafﬁc ﬂow forecasting with spatial-temporal correlation

in a hybrid deep learning framework. arXiv preprint arXiv:1612.01022, 2016.

Yuanchang Xie, Kaiguang Zhao, Ying Sun, and Dawei Chen. Gaussian processes for short-term trafﬁc
volume forecasting. Transportation Research Record: Journal of the Transportation Research
Board, (2165):69–78, 2010.

Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional neural network: A

deep learning framework for trafﬁc forecasting. arXiv preprint arXiv:1709.04875, 2017a.

Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for
high-dimensional time series prediction. In Advances in Neural Information Processing Systems,
pp. 847–855, 2016.

Rose Yu, Yaguang Li, Cyrus Shahabi, Ugur Demiryurek, and Yan Liu. Deep learning: A generic
approach for extreme condition trafﬁc forecasting. In SIAM International Conference on Data
Mining (SDM), 2017b.

Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, and Xiuwen Yi. Dnn-based prediction model for
spatio-temporal data. In Proceedings of the 24th ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems, pp. 92. ACM, 2016.

Junbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide crowd

ﬂows prediction. In AAAI, pp. 1655–1661, 2017.

10

Published as a conference paper at ICLR 2018

APPENDIX

A NOTATION

Name
G
V, vi
E
W , Wij,
D, DI , DO
L
Φ, Λ
X, ˆX ∈ RN ×P
X (t) ∈ RN ×P
H ∈ RN ×Q
fθ, θ
fΘ, Θ

Table 3: Notation

a graph
nodes of a graph, |V| = N and the i-th node.
edges of a graph
weight matrix of a graph and its entries
undirected degree matrix, In-degree/out-degree matrix
normalized graph Laplacian
eigen-vector matrix and eigen-value matrix of L
a graph signal, and the predicted graph signal.
a graph signal at time t.
output of the diffusion convolutional layer.
convolutional ﬁlter and its parameters.
convolutional layer and its parameters.

Table 3 summarizes the main notations used in the paper.

B EFFICIENT CALCULATION OF EQUATION 2

O W and the other part with D−1

Equation 2 can be decomposed into two parts with the same time complexity, i.e., one part with
I W (cid:124). Thus we will only show the time complexity of the ﬁrst
D−1
part.
Let Tk(x) = (cid:0)D−1

x, The ﬁrst part of Equation 2 can be rewritten as

O W (cid:1)k

K−1
(cid:88)

k=0

θkTk(X:,p)

(4)

O W Tk(x) and D−1

As Tk+1(x) = D−1
O W is sparse, it is easy to see that Equation 4 can be
calculated using O(K) recursive sparse-dense matrix multiplication each with time complexity
O(|E|). Consequently, the time complexities of both Equation 2 and Equation 4 are O(K|E|). For
dense graph, we may use spectral sparsiﬁcation (Cheng et al., 2015) to make it sparse.

C RELATION WITH SPECTRAL GRAPH CONVOLUTION

Proof. The spectral graph convolution utilizes the concept of normalized graph Laplacian L =
2 = ΦΛΦ(cid:124). ChebNet parametrizes fθ to be a K order polynomial of Λ, and
D− 1
calculates it using stable Chebyshev polynomial basis.

2 (D − W )D− 1

X:,p (cid:63)G fθ = Φ

θkΛk

Φ(cid:124)X:,p =

θkLkX:,p =

˜θkTk( ˜L)X:,p

(5)

(cid:33)

(cid:32)K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

where T0(x) = 1, T1(x) = x, Tk(x) = xTk−1(x) − Tk−2(x) are the basis of the Cheyshev polyno-
mial. Let λmax denote the largest eigenvalue of L, and ˜L = 2
L − I represents a rescaling of the
graph Laplacian that maps the eigenvalues from [0, λmax] to [−1, 1] since Chebyshev polynomial
forms an orthogonal basis in [−1, 1]. Equation 5 can be considered as a polynomial of ˜L and we will
show that the output of ChebNet Convolution is similar to the output of diffusion convolution up to
constant scaling factor. Assume λmax = 2 and DI = DO = D for undirected graph.

λmax

˜L = D− 1

2 − I = −D− 1
(6)
˜L is similar to the negative random walk transition matrix, thus the output of Equation 5 is also
similar to the output of Equation 2 up to constant scaling factor.

2 (D − W )D− 1

2 ∼ −D−1W

2 W D− 1

11

Published as a conference paper at ICLR 2018

Figure 8: Sensor distribution of the METR-LA and PEMS-BAY dataset.

D MORE RELATED WORK AND DISCUSSION

Xie et al. (2010) introduce a Gaussian processes (GPs) based method. GPs are hard to scale to the
large dataset and are generally not suitable for relatively long-term trafﬁc prediction like 1 hour
(i.e.,12 steps ahead), as the variance can be accumulated and becomes extremely large.

Cai et al. (2016) propose to use spatiotemporal nearest neighbor for trafﬁc forecasting (ST-KNN).
Though ST-KNN considers both the spatial and the temporal dependencies, it has the following
drawbacks. As shown in Fusco et al. (2016), ST-KNN performs independent forecasting for each
individual road. The prediction of a road is a weighted combination of its own historical trafﬁc speeds.
This makes it hard for ST-KNN to fully utilize information from neighbors. Besides, ST-KNN is a
non-parametric approach and each road is modeled and calculated separately (Cai et al., 2016), which
makes it hard to generalize to unseen situations and to scale to large datasets. Finally, in ST-KNN, all
the similarities are calculated using hand-designed metrics with few learnable parameters, and this
may limit its representational power.

Cheng et al. (2017) propose DeepTransport which models the spatial dependency by explicitly
collecting certain number of upstream and downstream roads for each individual road and then
conduct convolution on these roads respectively. Comparing with Cheng et al. (2017), DCRNN
models the spatial dependency in a more systematic way, i.e., generalizing convolution to the trafﬁc
sensor graph based on the diffusion nature of trafﬁc. Besides, we derive DCRNN from the property of
random walk and show that the popular spectral convolution ChebNet is a special case of our method.

The proposed approach is also related to graph embedding techniques, e.g., Deepwalk (Perozzi et al.,
2014), node2vec (Grover & Leskovec, 2016) which learn a low dimension representation for each
node in the graph. DCRNN also learns a representation for each node. The learned representations
capture both the spatial and the temporal dependency and at the same time are optimized with
regarding to the objective, e.g., future trafﬁc speeds.

E DETAILED EXPERIMENTAL SETTINGS

HA Historical Average, which models the trafﬁc ﬂow as a seasonal process, and uses weighted
average of previous seasons as the prediction. The period used is 1 week, and the prediction is based
on aggregated data from previous weeks. For example, the prediction for this Wednesday is the
averaged trafﬁc speeds from last four Wednesdays. As the historical average method does not depend
on short-term data, its performance is invariant to the small increases in the forecasting horizon

ARIMAkal
are (3, 0, 1), and the model is implemented using the statsmodel python package.

: Auto-Regressive Integrated Moving Average model with Kalman ﬁlter. The orders

12

Published as a conference paper at ICLR 2018

VAR Vector Auto-regressive model (Hamilton, 1994). The number of lags is set to 3, and the model
is implemented using the statsmodel python package.

SVR Linear Support Vector Regression, the penalty term C = 0.1, the number of historical
observation is 5.

The following deep neural network based approaches are also included.

FNN Feed forward neural network with two hidden layers, each layer contains 256 units. The initial
learning rate is 1e−3, and reduces to 1
10 every 20 epochs starting at the 50th epochs. In addition, for
all hidden layers, dropout with ratio 0.5 and L2 weight decay 1e−2 is used. The model is trained with
batch size 64 and MAE as the loss function. Early stop is performed by monitoring the validation
error.

FC-LSTM The Encoder-decoder framework using LSTM with peephole (Sutskever et al., 2014).
Both the encoder and the decoder contain two recurrent layers. In each recurrent layer, there are 256
LSTM units, L1 weight decay is 2e−5, L2 weight decay 5e−4. The model is trained with batch size
64 and loss function MAE. The initial learning rate is 1e-4 and reduces to 1
10 every 10 epochs starting
from the 20th epochs. Early stop is performed by monitoring the validation error.

DCRNN : Diffusion Convolutional Recurrent Neural Network. Both encoder and decoder contain
two recurrent layers. In each recurrent layer, there are 64 units, the initial learning rate is 1e−2, and
reduces to 1
10 every 10 epochs starting at the 20th epoch and early stopping on the validation dataset
is used. Besides, the maximum steps of random walks, i.e., K, is set to 3. For scheduled sampling,
the thresholded inverse sigmoid function is used as the probability decay:

(cid:15)i =

τ
τ + exp (i/τ )

where i is the number of iterations while τ are parameters to control the speed of convergence. τ
is set to 3,000 in the experiments. The implementation is available in https://github.com/
liyaguang/DCRNN.

E.1 DATASET

We conduct experiments on two real-world large-scale datasets:

• METR-LA This trafﬁc dataset contains trafﬁc information collected from loop detectors
in the highway of Los Angeles County (Jagadish et al., 2014). We select 207 sensors and
collect 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 for the experiment.
The total number of observed trafﬁc data points is 6,519,002.

• PEMS-BAY This trafﬁc dataset is collected by California Transportation Agencies (Cal-
Trans) Performance Measurement System (PeMS). We select 325 sensors in the Bay Area
and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for the experiment.
The total number of observed trafﬁc data points is 16,937,179.

The sensor distributions of both datasets are visualized in Figure 8.

In both of those datasets, we aggregate trafﬁc speed readings into 5 minutes windows, and apply
Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining
10% for validation. To construct the sensor graph, we compute the pairwise road network distances
between sensors and build the adjacency matrix using thresholded Gaussian kernel (Shuman et al.,
2013).

Wij = exp

−

(cid:18)

(cid:19)

dist(vi, vj)2
σ2

if dist(vi, vj) ≤ κ, otherwise 0

where Wij represents the edge weight between sensor vi and sensor vj, dist(vi, vj) denotes the road
network distance from sensor vi to sensor vj. σ is the standard deviation of distances and κ is the
threshold.

13

Published as a conference paper at ICLR 2018

E.2 METRICS

Suppose x = x1, · · · , xn represents the ground truth, ˆx = ˆx1, · · · , ˆxn represents the predicted
values, and Ω denotes the indices of observed samples, the metrics are deﬁned as follows.

Root Mean Square Error (RMSE)

RMSE(x, ˆx) =

(xi − ˆxi)2

(cid:115) 1
|Ω|

(cid:88)

i∈Ω

MAPE(x, ˆx) =

1
|Ω|

(cid:88)

i∈Ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

xi − ˆxi
xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

MAE(x, ˆx) =

|xi − ˆxi|

1
|Ω|

(cid:88)

i∈Ω

Mean Absolute Percentage Error (MAPE)

Mean Absolute Error (MAE)

F MODEL VISUALIZATION

Figure 9: Sensor correlations between the center sensor and its neighborhoods for different forecasting
horizons. The correlations are estimated using regularized VAR. We observe that the correlations are
localized and closer neighborhoods usually have larger relevance, and the magnitude of correlation
quickly decay with the increase of distance which is consistent with the diffusion process on the
graph.

14

Published as a conference paper at ICLR 2018

15
Figure 10: Trafﬁc time series forecasting visualization.

Published as a conference paper at ICLR 2018

16
Figure 11: Trafﬁc time series forecasting visualization.

8
1
0
2
 
b
e
F
 
2
2
 
 
]

G
L
.
s
c
[
 
 
3
v
6
2
9
1
0
.
7
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

DIFFUSION CONVOLUTIONAL RECURRENT NEURAL
NETWORK: DATA-DRIVEN TRAFFIC FORECASTING

Yaguang Li†, Rose Yu‡, Cyrus Shahabi†, Yan Liu†
† University of Southern California, ‡ California Institute of Technology
† {yaguang, shahabi, yanliu.cs}@usc.edu, ‡ rose@caltech.edu

ABSTRACT
Spatiotemporal forecasting has various applications in neuroscience, climate and
transportation domain. Trafﬁc forecasting is one canonical example of such learn-
ing task. The task is challenging due to (1) complex spatial dependency on road
networks, (2) non-linear temporal dynamics with changing road conditions and
(3) inherent difﬁculty of long-term forecasting. To address these challenges, we
propose to model the trafﬁc ﬂow as a diffusion process on a directed graph and
introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep
learning framework for trafﬁc forecasting that incorporates both spatial and tem-
poral dependency in the trafﬁc ﬂow. Speciﬁcally, DCRNN captures the spatial
dependency using bidirectional random walks on the graph, and the temporal de-
pendency using the encoder-decoder architecture with scheduled sampling. We
evaluate the framework on two real-world large scale road network trafﬁc datasets
and observe consistent improvement of 12% - 15% over state-of-the-art baselines.

INTRODUCTION

1
Spatiotemporal forecasting is a crucial task for a learning system that operates in a dynamic environ-
ment. It has a wide range of applications from autonomous vehicles operations, to energy and smart
grid optimization, to logistics and supply chain management. In this paper, we study one important
task: trafﬁc forecasting on road networks, the core component of the intelligent transportation systems.
The goal of trafﬁc forecasting is to predict the future trafﬁc speeds of a sensor network given historic
trafﬁc speeds and the underlying road networks.

This task is challenging mainly due to the com-
plex spatiotemporal dependencies and inher-
ent difﬁculty in the long term forecasting. On
the one hand, trafﬁc time series demonstrate
strong temporal dynamics. Recurring incidents
such as rush hours or accidents can cause non-
stationarity, making it difﬁcult to forecast long-
term. On the other hand, sensors on the road
network contain complex yet unique spatial cor-
relations. Figure 1 illustrates an example. Road
1 and road 2 are correlated, while road 1 and
road 3 are not. Although road 1 and road 3
are close in the Euclidean space, they demon-
strate very different behaviors. Moreover, the
future trafﬁc speed is inﬂuenced more by the
downstream trafﬁc than the upstream one. This
means that the spatial structure in trafﬁc is non-
Euclidean and directional.

Figure 1: Spatial correlation is dominated by road
network structure. (1) Trafﬁc speed in road 1 are
similar to road 2 as they locate in the same highway.
(2) Road 1 and road 3 locate in the opposite direc-
tions of the highway. Though close to each other
in the Euclidean space, their road network distance
is large, and their trafﬁc speeds differ signiﬁcantly.

Trafﬁc forecasting has been studied for decades,
falling into two main categories: knowledge-
driven approach and data-driven approach. In transportation and operational research, knowledge-
driven methods usually apply queuing theory and simulate user behaviors in trafﬁc (Cascetta, 2013).
In time series community, data-driven methods such as Auto-Regressive Integrated Moving Average
(ARIMA) model and Kalman ﬁltering remain popular (Liu et al., 2011; Lippi et al., 2013). However,
simple time series models usually rely on the stationarity assumption, which is often violated by

1

Published as a conference paper at ICLR 2018

the trafﬁc data. Most recently, deep learning models for trafﬁc forecasting have been developed
in Lv et al. (2015); Yu et al. (2017b), but without considering the spatial structure. Wu & Tan (2016)
and Ma et al. (2017) model the spatial correlation with Convolutional Neural Networks (CNN), but
the spatial structure is in the Euclidean space (e.g., 2D images). Bruna et al. (2014), Defferrard et al.
(2016) studied graph convolution, but only for undirected graphs.

In this work, we represent the pair-wise spatial correlations between trafﬁc sensors using a directed
graph whose nodes are sensors and edge weights denote proximity between the sensor pairs measured
by the road network distance. We model the dynamics of the trafﬁc ﬂow as a diffusion process and
propose the diffusion convolution operation to capture the spatial dependency. We further propose
Diffusion Convolutional Recurrent Neural Network (DCRNN) that integrates diffusion convolution,
the sequence to sequence architecture and the scheduled sampling technique. When evaluated on real-
world trafﬁc datasets, DCRNN consistently outperforms state-of-the-art trafﬁc forecasting baselines
by a large margin. In summary:

• We study the trafﬁc forecasting problem and model the spatial dependency of trafﬁc as
a diffusion process on a directed graph. We propose diffusion convolution, which has an
intuitive interpretation and can be computed efﬁciently.

• We propose Diffusion Convolutional Recurrent Neural Network (DCRNN), a holistic ap-
proach that captures both spatial and temporal dependencies among time series using
diffusion convolution and the sequence to sequence learning framework together with sched-
uled sampling. DCRNN is not limited to transportation and is readily applicable to other
spatiotemporal forecasting tasks.

• We conducted extensive experiments on two large-scale real-world datasets, and the proposed

approach obtains signiﬁcant improvement over state-of-the-art baseline methods.

2 METHODOLOGY
We formalize the learning problem of spatiotemporal trafﬁc forecasting and describe how to model
the dependency structures using diffusion convolutional recurrent neural network.

2.1 TRAFFIC FORECASTING PROBLEM
The goal of trafﬁc forecasting is to predict the future trafﬁc speed given previously observed trafﬁc
ﬂow from N correlated sensors on the road network. We can represent the sensor network as a
weighted directed graph G = (V, E, W ), where V is a set of nodes |V| = N , E is a set of edges and
W ∈ RN ×N is a weighted adjacency matrix representing the nodes proximity (e.g., a function of
their road network distance). Denote the trafﬁc ﬂow observed on G as a graph signal X ∈ RN ×P ,
where P is the number of features of each node (e.g., velocity, volume). Let X (t) represent the graph
signal observed at time t, the trafﬁc forecasting problem aims to learn a function h(·) that maps T (cid:48)
historical graph signals to future T graph signals, given a graph G:

[X (t−T (cid:48)+1), · · · , X (t); G]

h(·)
−−→ [X (t+1), · · · , X (t+T )]

2.2 SPATIAL DEPENDENCY MODELING

We model the spatial dependency by relating trafﬁc ﬂow to a diffusion process, which explicitly
captures the stochastic nature of trafﬁc dynamics. This diffusion process is characterized by a
random walk on G with restart probability α ∈ [0, 1], and a state transition matrix D−1
O W . Here
DO = diag(W 1) is the out-degree diagonal matrix, and 1 ∈ RN denotes the all one vector. After
many time steps, such Markov process converges to a stationary distribution P ∈ RN ×N whose ith
row P i,: ∈ RN represents the likelihood of diffusion from node vi ∈ V, hence the proximity w.r.t.
the node vi. The following Lemma provides a closed form solution for the stationary distribution.
Lemma 2.1. (Teng et al., 2016) The stationary distribution of the diffusion process can be represented
as a weighted combination of inﬁnite random walks on the graph, and be calculated in closed form:

P =

α(1 − α)k (cid:0)D−1

O W (cid:1)k

∞
(cid:88)

k=0

(1)

where k is the diffusion step. In practice, we use a ﬁnite K-step truncation of the diffusion process
and assign a trainable weight to each step. We also include the reversed direction diffusion process,

2

Published as a conference paper at ICLR 2018

such that the bidirectional diffusion offers the model more ﬂexibility to capture the inﬂuence from
both the upstream and the downstream trafﬁc.

Diffusion Convolution The resulted diffusion convolution operation over a graph signal X ∈
RN ×P and a ﬁlter fθ is deﬁned as:

X:,p (cid:63)G fθ =

θk,1

(cid:0)D−1

O W (cid:1)k

+ θk,2

(cid:0)D−1

I W (cid:124)(cid:1)k(cid:17)

X:,p

for p ∈ {1, · · · , P }

(2)

K−1
(cid:88)

(cid:16)

k=0

I W (cid:124) represent the transition
where θ ∈ RK×2 are the parameters for the ﬁlter and D−1
O W , D−1
matrices of the diffusion process and the reverse one, respectively.
In general, computing the
convolution can be expensive. However, if G is sparse, Equation 2 can be calculated efﬁciently using
O(K) recursive sparse-dense matrix multiplication with total time complexity O(K|E|) (cid:28) O(N 2).
See Appendix B for more detail.

Diffusion Convolutional Layer With the convolution operation deﬁned in Equation 2, we can
build a diffusion convolutional layer that maps P -dimensional features to Q-dimensional outputs.
Denote the parameter tensor as Θ ∈ RQ×P ×K×2 = [θ]q,p, where Θq,p,:,: ∈ RK×2 parameterizes
the convolutional ﬁlter for the pth input and the qth output. The diffusion convolutional layer is thus:

H:,q = a

X:,p (cid:63)G fΘq,p,:,:

for q ∈ {1, · · · , Q}

(3)

(cid:32) P

(cid:88)

p=1

(cid:33)

where X ∈ RN ×P is the input, H ∈ RN ×Q is the output, {fΘq,p,,: } are the ﬁlters and a is the
activation function (e.g., ReLU, Sigmoid). Diffusion convolutional layer learns the representations
for graph structured data and we can train it using stochastic gradient based method.

Relation with Spectral Graph Convolution Diffusion convolution is deﬁned on both directed and
undirected graphs. When applied to undirected graphs, we show that many existing graph structured
convolutional operations including the popular spectral graph convolution, i.e., ChebNet (Defferrard
et al., 2016), can be considered as a special case of diffusion convolution (up to a similarity transfor-
mation). Let D denote the degree matrix, and L = D− 1
2 be the normalized graph
Laplacian, the following Proposition demonstrates the connection.
Proposition 2.2. The spectral graph convolution deﬁned as

2 (D − W )D− 1

X:,p (cid:63)G fθ = Φ F (θ) Φ(cid:124)X:,p

with eigenvalue decomposition L = ΦΛΦ(cid:124) and F (θ) = (cid:80)K−1
diffusion convolution up to a similarity transformation, when the graph G is undirected.

θkΛk, is equivalent to graph

0

Proof. See Appendix C.

2.3 TEMPORAL DYNAMICS MODELING
We leverage the recurrent neural networks (RNNs) to model the temporal dependency. In particular,
we use Gated Recurrent Units (GRU) (Chung et al., 2014), which is a simple yet powerful variant of
RNNs. We replace the matrix multiplications in GRU with the diffusion convolution, which leads to
our proposed Diffusion Convolutional Gated Recurrent Unit (DCGRU).

r(t) =
C(t) = tanh(ΘC (cid:63)G

σ(Θr (cid:63)G [X (t), H (t−1)] + br)

u(t) = σ(Θu (cid:63)G [X (t), H (t−1)] + bu)

(cid:2)X (t), (r(t) (cid:12) H (t−1))(cid:3) + bc) H (t) = u(t) (cid:12) H (t−1) + (1 − u(t)) (cid:12) C(t)

where X (t), H (t) denote the input and output of at time t, r(t), u(t) are reset gate and update gate at
time t, respectively. (cid:63)G denotes the diffusion convolution deﬁned in Equation 2 and Θr, Θu, ΘC are
parameters for the corresponding ﬁlters. Similar to GRU, DCGRU can be used to build recurrent
neural network layers and be trained using backpropagation through time.

In multiple step ahead forecasting, we employ the Sequence to Sequence architecture (Sutskever
et al., 2014). Both the encoder and the decoder are recurrent neural networks with DCGRU. During
training, we feed the historical time series into the encoder and use its ﬁnal states to initialize the

3

Published as a conference paper at ICLR 2018

Figure 2: System architecture for the Diffusion Convolutional Recurrent Neural Network designed
for spatiotemporal trafﬁc forecasting. The historical time series are fed into an encoder whose ﬁnal
states are used to initialize the decoder. The decoder makes predictions based on either previous
ground truth or the model output.

decoder. The decoder generates predictions given previous ground truth observations. At testing time,
ground truth observations are replaced by predictions generated by the model itself. The discrepancy
between the input distributions of training and testing can cause degraded performance. To mitigate
this issue, we integrate scheduled sampling (Bengio et al., 2015) into the model, where we feed the
model with either the ground truth observation with probability (cid:15)i or the prediction by the model with
probability 1 − (cid:15)i at the ith iteration. During the training process, (cid:15)i gradually decreases to 0 to allow
the model to learn the testing distribution.

With both spatial and temporal modeling, we build a Diffusion Convolutional Recurrent Neural
Network (DCRNN). The model architecture of DCRNN is shown in Figure 2. The entire network is
trained by maximizing the likelihood of generating the target future time series using backpropagation
through time. DCRNN is able to capture spatiotemporal dependencies among time series and can be
applied to various spatiotemporal forecasting problems.

3 RELATED WORK

Trafﬁc forecasting is a classic problem in transportation and operational research which are primarily
based on queuing theory and simulations (Drew, 1968). Data-driven approaches for trafﬁc forecasting
have received considerable attention, and more details can be found in a recent survey paper (Vla-
hogianni et al., 2014) and the references therein. However, existing machine learning models either
impose strong stationary assumptions on the data (e.g., auto-regressive model) or fail to account for
highly non-linear temporal dependency (e.g., latent space model Yu et al. (2016); Deng et al. (2016)).
Deep learning models deliver new promise for time series forecasting problem. For example, in Yu
et al. (2017b); Laptev et al. (2017), the authors study time series forecasting using deep Recurrent
Neural Networks (RNN). Convolutional Neural Networks (CNN) have also been applied to trafﬁc
forecasting. Zhang et al. (2016; 2017) convert the road network to a regular 2-D grid and apply
traditional CNN to predict crowd ﬂow. Cheng et al. (2017) propose DeepTransport which models the
spatial dependency by explicitly collecting upstream and downstream neighborhood roads for each
individual road and then conduct convolution on these neighborhoods respectively.

Recently, CNN has been generalized to arbitrary graphs based on the spectral graph theory. Graph
convolutional neural networks (GCN) are ﬁrst introduced in Bruna et al. (2014), which bridges the
spectral graph theory and deep neural networks. Defferrard et al. (2016) propose ChebNet which
improves GCN with fast localized convolutions ﬁlters. Kipf & Welling (2017) simplify ChebNet
and achieve state-of-the-art performance in semi-supervised classiﬁcation tasks. Seo et al. (2016)
combine ChebNet with Recurrent Neural Networks (RNN) for structured sequence modeling. Yu
et al. (2017a) model the sensor network as a undirected graph and applied ChebNet and convolutional
sequence model (Gehring et al., 2017) to do forecasting. One limitation of the mentioned spectral
based convolutions is that they generally require the graph to be undirected to calculate meaningful

4

Published as a conference paper at ICLR 2018

Table 1: Performance comparison of different approaches for trafﬁc speed forecasting. DCRNN
achieves the best performance with all three metrics for all forecasting horizons, and the advantage
becomes more evident with the increase of the forecasting horizon.

T

Metric

HA

ARIMAKal

VAR

SVR

FNN

FC-LSTM DCRNN

A
L
-
R
T
E
M

Y
A
B
-
S
M
E
P

15 min

30 min

1 hour

15 min

30 min

1 hour

4.16
MAE
RMSE
7.80
MAPE 13.0%
4.16
MAE
RMSE
7.80
MAPE 13.0%
4.16
MAE
RMSE
7.80
MAPE 13.0%

MAE
RMSE
MAPE
MAE
RMSE
MAPE
MAE
RMSE
MAPE

2.88
5.59
6.8%
2.88
5.59
6.8%
2.88
5.59
6.8%

3.99
8.21
9.6%
5.15
10.45
12.7%
6.90
13.23
17.4%

1.62
3.30
3.5%
2.33
4.76
5.4%
3.38
6.50
8.3%

3.99
3.99
4.42
7.94
7.89
8.45
9.9%
10.2% 9.3%
4.23
5.05
5.41
9.13
8.17
10.87
12.7% 12.1% 12.9%
4.49
6.72
6.52
10.11
8.69
13.76
15.8% 16.7% 14.0%

1.74
3.16
3.6%
2.32
4.25
5.0%
2.93
5.44
6.5%

2.20
1.85
3.59
4.42
3.8% 5.19%
2.30
2.48
5.18
4.63
5.5% 5.43%
2.46
3.28
7.08
4.98
8.0% 5.89%

3.44
6.30
9.6%
3.77
7.23
10.9%
4.37
8.69
13.2%

2.05
4.19
4.8%
2.20
4.55
5.2%
2.37
4.96
5.7%

2.77
5.38
7.3%
3.15
6.45
8.8%
3.60
7.59
10.5%

1.38
2.95
2.9%
1.74
3.97
3.9%
2.07
4.74
4.9%

spectral decomposition. Going from spectral domain to vertex domain, Atwood & Towsley (2016)
propose diffusion-convolutional neural network (DCNN) which deﬁnes convolution as a diffusion
process across each node in a graph-structured input. Hechtlinger et al. (2017) propose GraphCNN to
generalize convolution to graph by convolving every node with its p nearest neighbors. However,
both these methods do not consider the temporal dynamics and mainly deal with static graph settings.

Our approach is different from all those methods due to both the problem settings and the formulation
of the convolution on the graph. We model the sensor network as a weighted directed graph which
is more realistic than grid or undirected graph. Besides, the proposed convolution is deﬁned using
bidirectional graph random walk and is further integrated with the sequence to sequence learning
framework as well as the scheduled sampling to model the long-term temporal dependency.

4 EXPERIMENTS

We conduct experiments on two real-world large-scale datasets: (1) METR-LA This trafﬁc dataset
contains trafﬁc information collected from loop detectors in the highway of Los Angeles County (Ja-
gadish et al., 2014). We select 207 sensors and collect 4 months of data ranging from Mar 1st 2012
to Jun 30th 2012 for the experiment. (2) PEMS-BAY This trafﬁc dataset is collected by California
Transportation Agencies (CalTrans) Performance Measurement System (PeMS). We select 325
sensors in the Bay Area and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for
the experiment. The sensor distributions of both datasets are visualized in Figure 8 in the Appendix.

(cid:16)

In both of those datasets, we aggregate trafﬁc speed readings into 5 minutes windows, and apply
Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining
10% for validation. To construct the sensor graph, we compute the pairwise road network distances
between sensors and build the adjacency matrix using thresholded Gaussian kernel (Shuman et al.,
2013). Wij = exp
if dist(vi, vj) ≤ κ, otherwise 0, where Wij represents the edge
weight between sensor vi and sensor vj, dist(vi, vj) denotes the road network distance from sensor
vi to sensor vj. σ is the standard deviation of distances and κ is the threshold.
4.1 EXPERIMENTAL SETTINGS
Baselines We compare DCRNN1 with widely used time series regression models, including (1)
HA: Historical Average, which models the trafﬁc ﬂow as a seasonal process, and uses weighted

− dist(vi,vj )2
σ2

(cid:17)

1The source code is available at https://github.com/liyaguang/DCRNN.

5

Published as a conference paper at ICLR 2018

Figure 3: Learning curve for DCRNN and
DCRNN without diffusion convolution. Remov-
ing diffusion convolution results in much higher
validation error. Moreover, DCRNN with bi-
directional random walk achieves the lowest val-
idation error.

Figure 4: Effects of K and the number of units
in each layer of DCRNN. K corresponds to the
reception ﬁeld width of the ﬁlter, and the number
of units corresponds to the number of ﬁlters.

average of previous seasons as the prediction; (2) ARIMAkal: Auto-Regressive Integrated Moving
Average model with Kalman ﬁlter which is widely used in time series prediction; (3) VAR: Vector
Auto-Regression (Hamilton, 1994). (4) SVR: Support Vector Regression which uses linear support
vector machine for the regression task; The following deep neural network based approaches are also
included: (5) Feed forward Neural network (FNN): Feed forward neural network with two hidden
layers and L2 regularization. (6) Recurrent Neural Network with fully connected LSTM hidden units
(FC-LSTM) (Sutskever et al., 2014).

All neural network based approaches are implemented using Tensorﬂow (Abadi et al., 2016), and
trained using the Adam optimizer with learning rate annealing. The best hyperparameters are chosen
using the Tree-structured Parzen Estimator (TPE) (Bergstra et al., 2011) on the validation dataset.
Detailed parameter settings for DCRNN as well as baselines are available in Appendix E.

4.2 TRAFFIC FORECASTING PERFORMANCE COMPARISON

Table 1 shows the comparison of different approaches for 15 minutes, 30 minutes and 1 hour ahead
forecasting on both datasets. These methods are evaluated based on three commonly used metrics in
trafﬁc forecasting, including (1) Mean Absolute Error (MAE), (2) Mean Absolute Percentage Error
(MAPE), and (3) Root Mean Squared Error (RMSE). Missing values are excluded in calculating
these metrics. Detailed formulations of these metrics are provided in Appendix E.2. We observe the
following phenomenon in both of these datasets. (1) RNN-based methods, including FC-LSTM and
DCRNN, generally outperform other baselines which emphasizes the importance of modeling the
temporal dependency. (2) DCRNN achieves the best performance regarding all the metrics for all
forecasting horizons, which suggests the effectiveness of spatiotemporal dependency modeling. (3)
Deep neural network based methods including FNN, FC-LSTM and DCRNN, tend to have better
performance than linear baselines for long-term forecasting, e.g., 1 hour ahead. This is because the
temporal dependency becomes increasingly non-linear with the growth of the horizon. Besides, as
the historical average method does not depend on short-term data, its performance is invariant to the
small increases in the forecasting horizon.

Note that, trafﬁc forecasting on the METR-LA (Los Angeles, which is known for its complicated
trafﬁc conditions) dataset is more challenging than that in the PEMS-BAY (Bay Area) dataset. Thus
we use METR-LA as the default dataset for following experiments.

4.3 EFFECT OF SPATIAL DEPENDENCY MODELING
To further investigate the effect of spatial dependency modeling, we compare DCRNN with the
following variants: (1) DCRNN-NoConv, which ignores spatial dependency by replacing the transition
matrices in the diffusion convolution (Equation 2) with identity matrices. This essentially means the
forecasting of a sensor can be only be inferred from its own historical readings; (2) DCRNN-UniConv,

6

Published as a conference paper at ICLR 2018

Table 2: Performance comparison for DCRNN and GCRNN on the METRA-LA dataset.

15 min

30 min

1 hour

DCRNN 2.77
GCRNN 2.80

MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
10.5%
6.45
10.9%
6.74

8.8% 3.60
3.81
9.0%

7.3% 3.15
3.24
7.5%

7.60
8.16

5.38
5.51

Figure 5: Performance comparison for dif-
ferent DCRNN variants. DCRNN, with the
sequence to sequence framework and sched-
uled sampling, achieves the lowest MAE on
the validation dataset. The advantage be-
comes more clear with the increase of the
forecasting horizon.

Figure 6: Trafﬁc time series forecasting visualization.
DCRNN generates smooth prediction and is usually
better at predict the start and end of peak hours.

which only uses the forward random walk transition matrix for diffusion convolution; Figure 3 shows
the learning curves of these three models with roughly the same number of parameters. Without
diffusion convolution, DCRNN-NoConv has much higher validation error. Moreover, DCRNN
achieves the lowest validation error which shows the effectiveness of using bidirectional random
walk. The intuition is that the bidirectional random walk gives the model the ability and ﬂexibility to
capture the inﬂuence from both the upstream and the downstream trafﬁc.

To investigate the effect of graph construction, we construct a undirected graph by setting (cid:99)Wij =
(cid:99)Wji = max(Wij, Wji), where (cid:99)W is the new symmetric weight matrix. Then we develop a variant
of DCRNN denotes GCRNN, which uses the sequence to sequence learning with ChebNet graph
convolution (Equation 5) with roughly the same amount of parameters. Table 2 shows the comparison
between DCRNN and GCRNN in the METR-LA dataset. DCRNN consistently outperforms GCRNN.
The intuition is that directed graph better captures the asymmetric correlation between trafﬁc sensors.
Figure 4 shows the effects of different parameters. K roughly corresponds to the size of ﬁlters’
reception ﬁelds while the number of units corresponds to the number of ﬁlters. Larger K enables
the model to capture broader spatial dependency at the cost of increasing learning complexity. We
observe that with the increase of K, the error on the validation dataset ﬁrst quickly decrease, and
then slightly increase. Similar behavior is observed for varying the number of units.

4.4 EFFECT OF TEMPORAL DEPENDENCY MODELING

To evaluate the effect of temporal modeling including the sequence to sequence framework as well
as the scheduled sampling mechanism, we further design three variants of DCRNN: (1) DCNN: in
which we concatenate the historical observations as a ﬁxed length vector and feed it into stacked
diffusion convolutional layers to predict the future time series. We train a single model for one step
ahead prediction, and feed the previous prediction into the model as input to perform multiple steps
ahead prediction. (2) DCRNN-SEQ: which uses the encoder-decoder sequence to sequence learning
framework to perform multiple steps ahead forecasting. (3) DCRNN: similar to DCRNN-SEQ except
for adding scheduled sampling.

7

Published as a conference paper at ICLR 2018

Figure 7: Visualization of learned localized ﬁlters centered at different nodes with K = 3 on the
METR-LA dataset. The star denotes the center, and the colors represent the weights. We observe that
weights are localized around the center, and diffuse alongside the road network.

Figure 5 shows the comparison of those four methods with regards to MAE for different forecasting
horizons. We observe that: (1) DCRNN-SEQ outperforms DCNN by a large margin which conforms
the importance of modeling temporal dependency. (2) DCRNN achieves the best result, and its
superiority becomes more evident with the increase of the forecasting horizon. This is mainly because
the model is trained to deal with its mistakes during multiple steps ahead prediction and thus suffers
less from the problem of error propagation. We also train a model that always been fed its output as
input for multiple steps ahead prediction. However, its performance is much worse than all the three
variants which emphasizes the importance of scheduled sampling.

4.5 MODEL INTERPRETATION

To better understand the model, we visualize forecasting results as well as learned ﬁlters. Figure 6
shows the visualization of 1 hour ahead forecasting. We have the following observations: (1)
DCRNN generates smooth prediction of the mean when small oscillation exists in the trafﬁc speeds
(Figure 6(a)). This reﬂects the robustness of the model. (2) DCRNN is more likely to accurately
predict abrupt changes in the trafﬁc speed than baseline methods (e.g., FC-LSTM). As shown in
Figure 6(b), DCRNN predicts the start and the end of the peak hours. This is because DCRNN
captures the spatial dependency, and is able to utilize the speed changes in neighborhood sensors
for more accurate forecasting. Figure 7 visualizes examples of learned ﬁlters centered at different
nodes. The star denotes the center, and colors denote the weights. We can observe that (1) weights
are well localized around the center, and (2) the weights diffuse based on road network distance.
More visualizations are provided in Appendix F.

5 CONCLUSION

In this paper, we formulated the trafﬁc prediction on road network as a spatiotemporal forecasting
problem, and proposed the diffusion convolutional recurrent neural network that captures the spa-
tiotemporal dependencies. Speciﬁcally, we use bidirectional graph random walk to model spatial
dependency and recurrent neural network to capture the temporal dynamics. We further integrated the
encoder-decoder architecture and the scheduled sampling technique to improve the performance for
long-term forecasting. When evaluated on two large-scale real-world trafﬁc datasets, our approach
obtained signiﬁcantly better prediction than baselines. For future work, we will investigate the
following two aspects (1) applying the proposed model to other spatial-temporal forecasting tasks;
(2) modeling the spatiotemporal dependency when the underlying graph structure is evolving, e.g.,
the K nearest neighbor graph for moving objects.

ACKNOWLEDGMENTS

This research has been funded in part by NSF grants CNS-1461963, IIS-1254206, IIS-1539608,
Caltrans-65A0533, the USC Integrated Media Systems Center (IMSC), and the USC METRANS
Transportation Center. Any opinions, ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily reﬂect the views of any of the sponsors
such as NSF. Also, the authors would like to thank Shang-Hua Teng, Dehua Cheng and Siyang Li for
helpful discussions and comments.

8

Published as a conference paper at ICLR 2018

REFERENCES

Mart´ın Abadi et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.

arXiv preprint arXiv:1603.04467, 2016.

James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural

Information Processing Systems, pp. 1993–2001, 2016.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence

prediction with recurrent neural networks. In NIPS, pp. 1171–1179, 2015.

James S Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter
optimization. In Advances in Neural Information Processing Systems, pp. 2546–2554, 2011.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally

connected networks on graphs. In ICLR, 2014.

Pinlong Cai, Yunpeng Wang, Guangquan Lu, Peng Chen, Chuan Ding, and Jianping Sun. A
spatiotemporal correlative k-nearest neighbor model for short-term trafﬁc multistep forecasting.
Transportation Research Part C: Emerging Technologies, 62:21–34, 2016.

Ennio Cascetta. Transportation systems engineering: theory and methods, volume 49. Springer

Science & Business Media, 2013.

Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. Efﬁcient sampling for
gaussian graphical models via spectral sparsiﬁcation. In Conference on Learning Theory, pp.
364–390, 2015.

Xingyi Cheng, Ruiqing Zhang, Jie Zhou, and Wei Xu. Deeptransport: Learning spatial-temporal

dependency for trafﬁc condition forecasting. arXiv preprint arXiv:1709.09585, 2017.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on

graphs with fast localized spectral ﬁltering. In NIPS, pp. 3837–3845, 2016.

Dingxiong Deng, Cyrus Shahabi, Ugur Demiryurek, Linhong Zhu, Rose Yu, and Yan Liu. Latent
space model for road networks to predict time-varying trafﬁc. In SIGKDD, pp. 1525–1534, 2016.

Donald R Drew. Trafﬁc ﬂow theory and control. Technical report, 1968.

Gaetano Fusco, Chiara Colombaroni, and Natalia Isaenko. Short-term speed predictions exploiting
big data on large urban road networks. Transportation Research Part C: Emerging Technologies,
73:183–201, 2016.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional

sequence to sequence learning. In ICML, 2017.

Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
855–864. ACM, 2016.

James Douglas Hamilton. Time series analysis, volume 2. Princeton university press Princeton, 1994.

Yotam Hechtlinger, Purvasha Chakravarti, and Jining Qin. A generalization of convolutional neural

networks to graph-structured data. arXiv preprint arXiv:1704.08165, 2017.

H. V. Jagadish, Johannes Gehrke, Alexandros Labrinidis, Yannis Papakonstantinou, Jignesh M. Patel,
Raghu Ramakrishnan, and Cyrus Shahabi. Big data and its technical challenges. Commun. ACM,
57(7):86–94, July 2014.

Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.

In International Conference on Learning Representations (ICLR), 2017.

9

Published as a conference paper at ICLR 2018

Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event forecasting
with neural networks at Uber. In Int. Conf. on Machine Learning Time Series Workshop, 2017.

Marco Lippi, Marco Bertini, and Paolo Frasconi. Short-term trafﬁc ﬂow forecasting: An experimental
comparison of time-series analysis and supervised learning. ITS, IEEE Transactions on, 14(2):
871–882, 2013.

Wei Liu, Yu Zheng, Sanjay Chawla, Jing Yuan, and Xie Xing. Discovering spatio-temporal causal

interactions in trafﬁc data streams. In SIGKDD, pp. 1010–1018. ACM, 2011.

Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. Trafﬁc ﬂow prediction

with big data: A deep learning approach. ITS, IEEE Transactions on, 16(2):865–873, 2015.

Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang, and Yunpeng Wang. Learning
trafﬁc as images: a deep convolutional neural network for large-scale transportation network speed
prediction. Sensors, 17(4):818, 2017.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining, pp. 701–710. ACM, 2014.

Youngjoo Seo, Micha¨el Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence
modeling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659, 2016.

David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst.
The emerging ﬁeld of signal processing on graphs: Extending high-dimensional data analysis to
networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83–98, 2013.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

In NIPS, pp. 3104–3112, 2014.

Shang-Hua Teng et al. Scalable algorithms for data and network analysis. Foundations and Trends R(cid:13)

in Theoretical Computer Science, 12(1–2):1–274, 2016.

Eleni I Vlahogianni, Matthew G Karlaftis, and John C Golias. Short-term trafﬁc forecasting: Where
we are and where were going. Transportation Research Part C: Emerging Technologies, 43:3–19,
2014.

Yuankai Wu and Huachun Tan. Short-term trafﬁc ﬂow forecasting with spatial-temporal correlation

in a hybrid deep learning framework. arXiv preprint arXiv:1612.01022, 2016.

Yuanchang Xie, Kaiguang Zhao, Ying Sun, and Dawei Chen. Gaussian processes for short-term trafﬁc
volume forecasting. Transportation Research Record: Journal of the Transportation Research
Board, (2165):69–78, 2010.

Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional neural network: A

deep learning framework for trafﬁc forecasting. arXiv preprint arXiv:1709.04875, 2017a.

Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for
high-dimensional time series prediction. In Advances in Neural Information Processing Systems,
pp. 847–855, 2016.

Rose Yu, Yaguang Li, Cyrus Shahabi, Ugur Demiryurek, and Yan Liu. Deep learning: A generic
approach for extreme condition trafﬁc forecasting. In SIAM International Conference on Data
Mining (SDM), 2017b.

Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, and Xiuwen Yi. Dnn-based prediction model for
spatio-temporal data. In Proceedings of the 24th ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems, pp. 92. ACM, 2016.

Junbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide crowd

ﬂows prediction. In AAAI, pp. 1655–1661, 2017.

10

Published as a conference paper at ICLR 2018

APPENDIX

A NOTATION

Name
G
V, vi
E
W , Wij,
D, DI , DO
L
Φ, Λ
X, ˆX ∈ RN ×P
X (t) ∈ RN ×P
H ∈ RN ×Q
fθ, θ
fΘ, Θ

Table 3: Notation

a graph
nodes of a graph, |V| = N and the i-th node.
edges of a graph
weight matrix of a graph and its entries
undirected degree matrix, In-degree/out-degree matrix
normalized graph Laplacian
eigen-vector matrix and eigen-value matrix of L
a graph signal, and the predicted graph signal.
a graph signal at time t.
output of the diffusion convolutional layer.
convolutional ﬁlter and its parameters.
convolutional layer and its parameters.

Table 3 summarizes the main notations used in the paper.

B EFFICIENT CALCULATION OF EQUATION 2

O W and the other part with D−1

Equation 2 can be decomposed into two parts with the same time complexity, i.e., one part with
I W (cid:124). Thus we will only show the time complexity of the ﬁrst
D−1
part.
Let Tk(x) = (cid:0)D−1

x, The ﬁrst part of Equation 2 can be rewritten as

O W (cid:1)k

K−1
(cid:88)

k=0

θkTk(X:,p)

(4)

O W Tk(x) and D−1

As Tk+1(x) = D−1
O W is sparse, it is easy to see that Equation 4 can be
calculated using O(K) recursive sparse-dense matrix multiplication each with time complexity
O(|E|). Consequently, the time complexities of both Equation 2 and Equation 4 are O(K|E|). For
dense graph, we may use spectral sparsiﬁcation (Cheng et al., 2015) to make it sparse.

C RELATION WITH SPECTRAL GRAPH CONVOLUTION

Proof. The spectral graph convolution utilizes the concept of normalized graph Laplacian L =
2 = ΦΛΦ(cid:124). ChebNet parametrizes fθ to be a K order polynomial of Λ, and
D− 1
calculates it using stable Chebyshev polynomial basis.

2 (D − W )D− 1

X:,p (cid:63)G fθ = Φ

θkΛk

Φ(cid:124)X:,p =

θkLkX:,p =

˜θkTk( ˜L)X:,p

(5)

(cid:33)

(cid:32)K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

where T0(x) = 1, T1(x) = x, Tk(x) = xTk−1(x) − Tk−2(x) are the basis of the Cheyshev polyno-
mial. Let λmax denote the largest eigenvalue of L, and ˜L = 2
L − I represents a rescaling of the
graph Laplacian that maps the eigenvalues from [0, λmax] to [−1, 1] since Chebyshev polynomial
forms an orthogonal basis in [−1, 1]. Equation 5 can be considered as a polynomial of ˜L and we will
show that the output of ChebNet Convolution is similar to the output of diffusion convolution up to
constant scaling factor. Assume λmax = 2 and DI = DO = D for undirected graph.

λmax

˜L = D− 1

2 − I = −D− 1
(6)
˜L is similar to the negative random walk transition matrix, thus the output of Equation 5 is also
similar to the output of Equation 2 up to constant scaling factor.

2 (D − W )D− 1

2 ∼ −D−1W

2 W D− 1

11

Published as a conference paper at ICLR 2018

Figure 8: Sensor distribution of the METR-LA and PEMS-BAY dataset.

D MORE RELATED WORK AND DISCUSSION

Xie et al. (2010) introduce a Gaussian processes (GPs) based method. GPs are hard to scale to the
large dataset and are generally not suitable for relatively long-term trafﬁc prediction like 1 hour
(i.e.,12 steps ahead), as the variance can be accumulated and becomes extremely large.

Cai et al. (2016) propose to use spatiotemporal nearest neighbor for trafﬁc forecasting (ST-KNN).
Though ST-KNN considers both the spatial and the temporal dependencies, it has the following
drawbacks. As shown in Fusco et al. (2016), ST-KNN performs independent forecasting for each
individual road. The prediction of a road is a weighted combination of its own historical trafﬁc speeds.
This makes it hard for ST-KNN to fully utilize information from neighbors. Besides, ST-KNN is a
non-parametric approach and each road is modeled and calculated separately (Cai et al., 2016), which
makes it hard to generalize to unseen situations and to scale to large datasets. Finally, in ST-KNN, all
the similarities are calculated using hand-designed metrics with few learnable parameters, and this
may limit its representational power.

Cheng et al. (2017) propose DeepTransport which models the spatial dependency by explicitly
collecting certain number of upstream and downstream roads for each individual road and then
conduct convolution on these roads respectively. Comparing with Cheng et al. (2017), DCRNN
models the spatial dependency in a more systematic way, i.e., generalizing convolution to the trafﬁc
sensor graph based on the diffusion nature of trafﬁc. Besides, we derive DCRNN from the property of
random walk and show that the popular spectral convolution ChebNet is a special case of our method.

The proposed approach is also related to graph embedding techniques, e.g., Deepwalk (Perozzi et al.,
2014), node2vec (Grover & Leskovec, 2016) which learn a low dimension representation for each
node in the graph. DCRNN also learns a representation for each node. The learned representations
capture both the spatial and the temporal dependency and at the same time are optimized with
regarding to the objective, e.g., future trafﬁc speeds.

E DETAILED EXPERIMENTAL SETTINGS

HA Historical Average, which models the trafﬁc ﬂow as a seasonal process, and uses weighted
average of previous seasons as the prediction. The period used is 1 week, and the prediction is based
on aggregated data from previous weeks. For example, the prediction for this Wednesday is the
averaged trafﬁc speeds from last four Wednesdays. As the historical average method does not depend
on short-term data, its performance is invariant to the small increases in the forecasting horizon

ARIMAkal
are (3, 0, 1), and the model is implemented using the statsmodel python package.

: Auto-Regressive Integrated Moving Average model with Kalman ﬁlter. The orders

12

Published as a conference paper at ICLR 2018

VAR Vector Auto-regressive model (Hamilton, 1994). The number of lags is set to 3, and the model
is implemented using the statsmodel python package.

SVR Linear Support Vector Regression, the penalty term C = 0.1, the number of historical
observation is 5.

The following deep neural network based approaches are also included.

FNN Feed forward neural network with two hidden layers, each layer contains 256 units. The initial
learning rate is 1e−3, and reduces to 1
10 every 20 epochs starting at the 50th epochs. In addition, for
all hidden layers, dropout with ratio 0.5 and L2 weight decay 1e−2 is used. The model is trained with
batch size 64 and MAE as the loss function. Early stop is performed by monitoring the validation
error.

FC-LSTM The Encoder-decoder framework using LSTM with peephole (Sutskever et al., 2014).
Both the encoder and the decoder contain two recurrent layers. In each recurrent layer, there are 256
LSTM units, L1 weight decay is 2e−5, L2 weight decay 5e−4. The model is trained with batch size
64 and loss function MAE. The initial learning rate is 1e-4 and reduces to 1
10 every 10 epochs starting
from the 20th epochs. Early stop is performed by monitoring the validation error.

DCRNN : Diffusion Convolutional Recurrent Neural Network. Both encoder and decoder contain
two recurrent layers. In each recurrent layer, there are 64 units, the initial learning rate is 1e−2, and
reduces to 1
10 every 10 epochs starting at the 20th epoch and early stopping on the validation dataset
is used. Besides, the maximum steps of random walks, i.e., K, is set to 3. For scheduled sampling,
the thresholded inverse sigmoid function is used as the probability decay:

(cid:15)i =

τ
τ + exp (i/τ )

where i is the number of iterations while τ are parameters to control the speed of convergence. τ
is set to 3,000 in the experiments. The implementation is available in https://github.com/
liyaguang/DCRNN.

E.1 DATASET

We conduct experiments on two real-world large-scale datasets:

• METR-LA This trafﬁc dataset contains trafﬁc information collected from loop detectors
in the highway of Los Angeles County (Jagadish et al., 2014). We select 207 sensors and
collect 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 for the experiment.
The total number of observed trafﬁc data points is 6,519,002.

• PEMS-BAY This trafﬁc dataset is collected by California Transportation Agencies (Cal-
Trans) Performance Measurement System (PeMS). We select 325 sensors in the Bay Area
and collect 6 months of data ranging from Jan 1st 2017 to May 31th 2017 for the experiment.
The total number of observed trafﬁc data points is 16,937,179.

The sensor distributions of both datasets are visualized in Figure 8.

In both of those datasets, we aggregate trafﬁc speed readings into 5 minutes windows, and apply
Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining
10% for validation. To construct the sensor graph, we compute the pairwise road network distances
between sensors and build the adjacency matrix using thresholded Gaussian kernel (Shuman et al.,
2013).

Wij = exp

−

(cid:18)

(cid:19)

dist(vi, vj)2
σ2

if dist(vi, vj) ≤ κ, otherwise 0

where Wij represents the edge weight between sensor vi and sensor vj, dist(vi, vj) denotes the road
network distance from sensor vi to sensor vj. σ is the standard deviation of distances and κ is the
threshold.

13

Published as a conference paper at ICLR 2018

E.2 METRICS

Suppose x = x1, · · · , xn represents the ground truth, ˆx = ˆx1, · · · , ˆxn represents the predicted
values, and Ω denotes the indices of observed samples, the metrics are deﬁned as follows.

Root Mean Square Error (RMSE)

RMSE(x, ˆx) =

(xi − ˆxi)2

(cid:115) 1
|Ω|

(cid:88)

i∈Ω

MAPE(x, ˆx) =

1
|Ω|

(cid:88)

i∈Ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

xi − ˆxi
xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

MAE(x, ˆx) =

|xi − ˆxi|

1
|Ω|

(cid:88)

i∈Ω

Mean Absolute Percentage Error (MAPE)

Mean Absolute Error (MAE)

F MODEL VISUALIZATION

Figure 9: Sensor correlations between the center sensor and its neighborhoods for different forecasting
horizons. The correlations are estimated using regularized VAR. We observe that the correlations are
localized and closer neighborhoods usually have larger relevance, and the magnitude of correlation
quickly decay with the increase of distance which is consistent with the diffusion process on the
graph.

14

Published as a conference paper at ICLR 2018

15
Figure 10: Trafﬁc time series forecasting visualization.

Published as a conference paper at ICLR 2018

16
Figure 11: Trafﬁc time series forecasting visualization.

