LIUM-CVC Submissions for WMT18 Multimodal Translation Task

Ozan Caglayan, Adrien Bardet,
Fethi Bougares, Lo¨ıc Barrault
LIUM, Le Mans University
FirstName.LastName@univ-lemans.fr

Kai Wang, Marc Masana, Luis Herranz and Joost van de Weijer
CVC, Universitat Autonoma de Barcelona
{kwang,mmasana,lherranz,joost}@cvc.uab.es

Abstract

This paper describes the multimodal Neural
Machine Translation systems developed by
LIUM and CVC for WMT18 Shared Task
on Multimodal Translation. This year we
propose several modiﬁcations to our previ-
ous multimodal attention architecture in or-
der to better integrate convolutional features
and reﬁne them using encoder-side infor-
mation. Our ﬁnal constrained submissions
ranked ﬁrst for English→French and second
for English→German language pairs among
the constrained submissions according to the
automatic evaluation metric METEOR.

1

Introduction

In this paper, we present the neural machine trans-
lation (NMT) and multimodal NMT (MMT) sys-
tems developed by LIUM and CVC for the third
edition of the shared task. Several lines of work
have been conducted since the introduction of the
shared task on MMT in 2016 (Specia et al., 2016).
The majority of last year submissions including
ours (Caglayan et al., 2017a) were based on the
integration of global visual features into various
parts of the NMT architecture (Elliott et al., 2017).
Apart from these, hierarchical multimodal atten-
tion (Helcl and Libovick´y, 2017) and multi-task
learning (Elliott and K´ad´ar, 2017) were also ex-
plored by the participants.

This year we decided to revisit the multimodal
attention (Caglayan et al., 2016) since our previ-
ous observations about qualitative analysis of the
visual attention was not satisfying. In order to im-
prove the multimodal attention both qualitatively
and quantitatively, we experiment with several re-
ﬁnements to it: ﬁrst, we try to use different in-
put image sizes prior to feature extraction and sec-
ond we normalize the ﬁnal convolutional feature
maps to assess its impact on the ﬁnal MMT per-
formance. In terms of architecture, we propose to

reﬁne the visual features by learning an encoder-
guided early spatial attention. In overall, we ﬁnd
that normalizing feature maps is crucial for the
multimodal attention to obtain a comparable per-
formance to monomodal NMT while the impact of
the input image size remains unclear. Finally, with
the help of the reﬁned attention, we obtain modest
improvements in terms of BLEU (Papineni et al.,
2002) and METEOR (Lavie and Agarwal, 2007).
The paper is organized as follows: data prepro-
cessing, model details and training hyperparame-
ters are detailed respectively in section 2 and sec-
tion 3. The results based on automatic evaluation
metrics are reported in section 4. Finally the paper
ends with a conclusion in section 5.

2 Data

We use Multi30k (Elliott et al., 2016) dataset pro-
vided by the organizers which contains 29000,
1014, 1000 and 1000 English→{German,French}
sentence pairs respectively for train, dev,
test2016 and test2017. A new training split
of 30014 pairs is formed by concatenating the
train and dev splits. Early-stopping is per-
formed based on METEOR computed over the
test2016 set and the ﬁnal model selection is
done over test2017.

Punctuation normalization, lowercasing and ag-
gressive hyphen splitting were applied to all sen-
tences prior to training. A Byte Pair Encoding
(BPE) model (Sennrich et al., 2016) with 10K
merge operations is jointly learned on English-
German and English-French resulting in vocabu-
laries of 5189-7090 and 5830-6608 subwords re-
spectively.

2.1 Visual Features

Since Multi30k images involve much more com-
plex region-level relationships and scene composi-
tions compared to ImageNet (Russakovsky et al.,

8
1
0
2
 
p
e
S
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
1
5
1
0
0
.
9
0
8
1
:
v
i
X
r
a

Figure 1: Filtered attention (FA): the convolutional feature maps are dynamically masked using an attention con-
ditioned on the source sentence representation.

2015) object classiﬁcation task, we explore dif-
ferent input image sizes to quantify its impact in
the context of MMT since rescaling the input im-
age has a direct effect on the size of the recep-
tive ﬁelds of the CNN. After normalizing the im-
ages using ImageNet mean and standard devia-
tion, we resize and crop the images to 224x224
and 448x448. Features are then extracted from the
ﬁnal convolutional layer (res5c relu) of a pre-
trained ResNet50 (He et al., 2016) CNN.1 This led
to feature maps V ∈ R2048×w×w where the spatial
dimensionality w is 7 or 14.

2.1.1 Feature Normalization

We conjecture that transferring ReLU features
from a CNN into a model that only makes use of
bounded non-linearities like sigmoid and tanh,
can saturate the non-linear neurons in the very
early stages of training if their weights are not
carefully initialized. Instead of tuning the initial-
ization, we experiment with L2 normalization over
the channel dimension so that each feature vector
(∈ R2048) has an L2 norm of 1.

3 Models

In this section we will describe our baseline
NMT and multimodal NMT systems. All mod-
els use 128 dimensional embeddings and GRU
(Cho et al., 2014) layers with 256 hidden states.
Dropout (Srivastava et al., 2014) is applied over
source embeddings xs, encoder states Henc and
pre-softmax activations ot. We also apply L2 reg-
ularization with a factor of 1e−5 on all parame-
ters except biases. The parameters are initialized
using the method proposed by He et al. (2015)
and optimized with Adam (Kingma and Ba, 2014).
The total gradient norm is clipped to 1 (Pascanu
et al., 2013). We use batches of size 64 and an
initial learning rate of 4e−4. All systems are im-

plemented using the PyTorch version of nmtpy2
(Caglayan et al., 2017b).

3.1 Baseline NMT

Let us denote the length of the source sentence
{x1, . . . , xS} and the target sentence {y1, . . . , yT }
by S and T respectively. The source sentence is
ﬁrst encoded with a 2-layer bidirectional GRU to
obtain the set of hidden states:

Henc ← Enc({x1, . . . , xS}), Henc ∈ RS×512

The decoder is a 2-layer conditional GRU
(CGRU) (Sennrich et al., 2017) with tied embed-
dings (Press and Wolf, 2016). CGRU is a stacked
2-layer recurrence block with the attention mech-
anism in the middle. We use feed-forward atten-
tion (Bahdanau et al., 2014) which encapsulates a
learnable layer. The ﬁrst decoder (which is ini-
tialized with a zero vector) receives the previous
target embeddings as inputs (equation 1). At each
timestep of the decoding stage, the attention mech-
anisms produces a context vector ctxt
(equation 2)
that becomes the input to the second GRU (equa-
tion 3). Finally, the probability over the target vo-
cabulary is conditioned over a transformation of
the ﬁnal hidden state hdec2

(equation 4, 5).

t

t

t = DEC1(yt−1, hdec2
hdec1
t−1 )
t = ATTtxt(Henc, hdec1
ctxt
hdec2
t = DEC2(ctxt
)

t
, hdec1
t
ot = tanh(Wohdec2
P (yt) = sof tmax(Wvot)

t

t + bo)

)

(1)

(2)

(3)

(4)

(5)

3.2 Multimodal Attention (MA)

Our baseline multimodal attention (MA) system
(Caglayan et al., 2016) applies a spatial attention
mechanism (Xu et al., 2015) over the visual fea-
tures. At each timestep t of the decoding stage,

1We use torchvision for feature extraction.

2github.com/lium-lst/nmtpytorch

a multimodal context vector ct is computed and
given as input to the second decoder (equation 3):

(cid:2)ctxt
; Wviscvis
ct = Wf
t
t
t = ATTvis(V, hdec1
cvis
)

t

(cid:3)

(6)

(7)

Previous analysis showed that the attention over
the visual features is inconsistent and weak. We
argue that this is because of the diluted relevant
visual information, and the competition with the
far more relevant source text information.

3.3 Filtered Attention (FA)

In order to enhance the visual attention, we pro-
pose an extension to the multimodal attention
where the objective is to ﬁlter the convolutional
feature maps using the last hidden state of the
source language encoder (Figure 1). We conjec-
ture that a learnable masking operation over the
convolutional feature maps can help the decoder-
side visual attention mechanism by ﬁltering out
regions irrelevant to translation and focus on the
most important part of the visual input. The ﬁl-
tered convolutional feature map (cid:101)V is computed as
follows:

βpre = ConvAtt((cid:2)T ile(henc

S ); V (cid:3))
(cid:101)V = βpre (cid:12) V, βpre ∈ R1×w×w

(8)

(9)

ConvAtt block is inspired from previous works
in visual question answering (VQA) (Yang et al.,
2016; Kazemi and Elqursh, 2017).
It basi-
cally computes a spatial attention distribution βpre
which we further use to mask the actual convolu-
tional features V. The ﬁltered (cid:101)V replaces V in the
equation 7 instead of being pooled into a single
visual embedding in contrast to VQA models.

EN→DE test2017

BLEU

METEOR

Baseline NMT

31.0 ± 0.3

52.1 ± 0.4

MA448
MA448 + L2-norm

28.6 ± 0.8
30.8 ± 0.5

50.1 ± 0.3
52.0 ± 0.2

Table 1:
mance of multimodal attention.

Impact of L2 normalization on the perfor-

4 Results

We train each model 4 times using different seeds
and report mean and standard deviation for the ﬁ-
nal results using multeval (Clark et al., 2011)

Feature Normalization We can see from Ta-
ble 1 that without L2 normalization, multimodal
attention is not able to reach the performance of
baseline NMT. Applying the normalization con-
sistently improves the results for all input sizes by
around ∼2 points in BLEU and METEOR. From
now on, we only present systems trained with nor-
malized features.

EN→DE test2017

BLEU

METEOR

MA224
MA448

FA224
FA448

30.6 ± 0.4
30.8 ± 0.5

51.8 ± 0.2
52.0 ± 0.2

31.5 ± 0.5
31.6 ± 0.5

52.2 ± 0.5
52.5 ± 0.4

Table 2:
mance of multimodal attention variants.

Impact of input image width on the perfor-

Image Size Although the impact of doubling
the image width and height at the input seems
marginal (Table 2), we switch to 448x448 images
to beneﬁt from the slight gains which are consis-
tent across both attention variants.

4.1 Monomodal vs Multimodal Comparison

We ﬁrst present the mean and standard deviation
of BLEU and METEOR over 4 runs on the inter-
nal test set test2017 (Table 3). With the help of
L2 normalization, MA system almost reaches the
monomodal system but fails to improve over it. On
the contrary, the ﬁltered attention (FA) mechanism
improves over the baseline and produces hypothe-
ses that are statistically different than the baseline
with p ≤ 0.02.

The improvements obtained for EN→DE lan-
guage pair are not reﬂected on the EN→FR perfor-
mance. One should note that the hyperparameters
from EN→DE task were transferred to EN→FR
without any other tuning.

The automatic evaluation of our ﬁnal submis-
sions (which are ensembles of 4 runs) on the of-
ﬁcial test set test2018 is presented in Table 5.
In addition to our submissions, we also provide
the best constrained and unconstrained systems3
in terms of METEOR. However, it should be noted
that the submitted systems will be primarily eval-
uated using human direct assessment.

On EN→DE, our constrained FA system is
comparable to the constrained UMONS submis-
sion. On EN→FR, our submission obtained the

3www.statmt.org/wmt18/multimodal-task.html

English→German

# Params

test2017 (µ ± σ)
METEOR

TER

BLEU

Baseline NMT

4.6M

31.0 ± 0.3

52.1 ± 0.4

51.2 ± 0.5

Multimodal Attention (MA)
Filtered Attention (FA)

10.0M
11.3M

30.8 ± 0.5
31.6 ± 0.5

52.0 ± 0.2
52.5 ± 0.4

51.1 ± 0.7
50.5 ± 0.5

Table 3: EN→DE results: Filtered attention is statistically different than the NMT (p ≤ 0.02).

English→French

# Params

test2017 (µ ± σ)
METEOR

TER

BLEU

Baseline NMT

4.6M

53.1 ± 0.3

69.9 ± 0.2

31.9 ± 0.8

Multimodal Attention (MA)
Filtered Attention (FA)

10.0M
11.3M

52.6 ± 0.3
52.8 ± 0.2

69.6 ± 0.3
69.6 ± 0.1

31.9 ± 0.4
31.9 ± 0.1

Table 4: EN→FR results: multimodal systems are not able to improve over NMT in terms of automatic metrics.

EN→DE

BLEU METEOR TER

the spatial attention and try to improve it further.

MeMAD†
UMONS(cid:63)
LIUMCVC-FA(cid:63)
LIUMCVC-NMT(cid:63)

EN→FR

CUNI†
LIUMCVC-FA(cid:63)
LIUMCVC-NMT(cid:63)

38.5
31.1
31.4
31.1

40.4
39.5
39.1

56.6
51.6
51.4
51.5

60.7
59.9
59.8

44.5
53.4
52.1
52.6

40.7
41.7
41.9

Acknowledgments

This work was supported by the French National
Research Agency (ANR) through the CHIST-
ERA M2CR project4, under the contract num-
ber ANR-15-CHR2-0006-01 and by MINECO
through APCIN 2015 under the contract number
PCIN-2015-251.

Table 5: Ofﬁcial test2018 results (†: Unconstrained,
(cid:63): Constrained.)

References

highest automatic evaluation scores among the
constrained submissions and is slightly worse than
the unconstrained CUNI system.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR,

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473.

5 Conclusion

MMT task consists of translating a source sen-
tence into a target language with the help of an
image representing the source sentence. The dif-
ferent level of relevance of both input modalities
makes it a difﬁcult task where the image should be
used with parsimony. With the aim of improving
the attention over visual input, we introduced a ﬁl-
tering technique to allow the network to ignore ir-
relevant parts of the image that should not be con-
sidered during decoding. This is done by using an
attention-like mechanism between the source sen-
tence and the convolutional feature maps. Results
show that this mechanism signiﬁcantly improves
the results for English→German on one of the test
sets. In the future, we plan to qualitatively analyze

Ozan Caglayan, Walid Aransa, Adrien Bardet, Mer-
cedes Garc´ıa-Mart´ınez, Fethi Bougares, Lo¨ıc Bar-
rault, Marc Masana, Luis Herranz, and Joost van de
Weijer. 2017a. Lium-cvc submissions for wmt17
multimodal translation task. In Proceedings of the
Second Conference on Machine Translation, Vol-
ume 2: Shared Task Papers, pages 432–439, Copen-
hagen, Denmark. Association for Computational
Linguistics.

Ozan Caglayan, Lo¨ıc Barrault, and Fethi Bougares.
2016. Multimodal attention for neural machine
translation. CoRR, abs/1609.03976.

Ozan Caglayan, Mercedes Garc´ıa-Mart´ınez, Adrien
Bardet, Walid Aransa, Fethi Bougares, and Lo¨ıc
Barrault. 2017b. Nmtpy: A ﬂexible toolkit for ad-
vanced neural machine translation systems. Prague
Bull. Math. Linguistics, 109:15–28.

4http://m2cr.univ-lemans.fr

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, HLT ’11, pages 176–181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Desmond Elliott, Stella Frank, Lo¨ıc Barrault, Fethi
Bougares, and Lucia Specia. 2017. Findings of
the Second Shared Task on Multimodal Machine
Translation and Multilingual Image Description. In
Proceedings of the Second Conference on Machine
Translation, Copenhagen, Denmark.

Desmond Elliott, Stella Frank, Khalil Sima’an, and Lu-
cia Specia. 2016. Multi30k: Multilingual english-
In Proceedings of the
german image descriptions.
5th Workshop on Vision and Language, pages 70–
74, Berlin, Germany. Association for Computational
Linguistics.

Desmond Elliott and ´Akos K´ad´ar. 2017.

Imagi-
nation improves multimodal translation. CoRR,
abs/1705.04350.

K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep resid-
In 2016 IEEE
ual learning for image recognition.
Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 770–778.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectiﬁers: Surpass-
ing human-level performance on imagenet classiﬁ-
In The IEEE International Conference on
cation.
Computer Vision (ICCV).

Jindˇrich Helcl and Jindˇrich Libovick´y. 2017. Cuni sys-
tem for the wmt17 multimodal translation task. In
Proceedings of the Second Conference on Machine
Translation, Volume 2: Shared Task Papers, pages
450–457, Copenhagen, Denmark. Association for
Computational Linguistics.

Vahid Kazemi and Ali Elqursh. 2017. Show, ask, at-
tend, and answer: A strong baseline for visual ques-
tion answering.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ’07, pages 228–231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difﬁculty of training recurrent neu-
In Proceedings of the 30th Interna-
ral networks.
tional Conference on International Conference on
Machine Learning - Volume 28, ICML’13, pages III–
1310–III–1318. JMLR.org.

Oﬁr Press and Lior Wolf. 2016. Using the output
arXiv

embedding to improve language models.
preprint arXiv:1608.05859.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2015.
Ima-
geNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV),
115(3):211–252.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch-Mayne, Barry Haddow, Julian Hitschler,
Marcin Junczys-Dowmunt, Samuel Lubli, Antonio
Miceli Barone, Jozef Mokry, and Maria Nadejde.
2017. Nematus: a toolkit for neural machine trans-
lation. In Proceedings of the EACL 2017 Software
Demonstrations, pages 65–68. Association for Com-
putational Linguistics (ACL).

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Lucia Specia, Stella Frank, Khalil Sima’an, and
Desmond Elliott. 2016. A shared task on multi-
modal machine translation and crosslingual image
description. In Proceedings of the First Conference
on Machine Translation, pages 543–553, Berlin,
Germany. Association for Computational Linguis-
tics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. J. Mach. Learn. Res., 15(1):1929–
1958.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
In Proceedings of the 32nd International
tion.
Conference on Machine Learning (ICML-15), pages
2048–2057. JMLR Workshop and Conference Pro-
ceedings.

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alexander J. Smola. 2016. Stacked attention
networks for image question answering. In CVPR,
pages 21–29. IEEE Computer Society.

