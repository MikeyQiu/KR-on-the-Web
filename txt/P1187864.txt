Reinforcement Learning via Recurrent
Convolutional Neural Networks

Tanmay Shankar
Department of Mech. Engg.
IIT Guwahati
tanmay.shankar@gmail.com

Santosha K Dwivedy
Department of Mech. Engg.
IIT Guwahati
dwivedy@iitg.ac.in

Prithwijit Guha
Department of EEE
IIT Guwahati
pguha@iitg.ac.in

7
1
0
2
 
n
a
J
 
9
 
 
]

G
L
.
s
c
[
 
 
1
v
2
9
3
2
0
.
1
0
7
1
:
v
i
X
r
a

Abstract—Deep Reinforcement Learning has enabled the learn-
ing of policies for complex tasks in partially observable envi-
ronments, without explicitly learning the underlying model of
the tasks. While such model-free methods achieve considerable
performance, they often ignore the structure of task. We present a
natural representation of to Reinforcement Learning (RL) prob-
lems using Recurrent Convolutional Neural Networks (RCNNs),
to better exploit this inherent structure. We deﬁne 3 such RCNNs,
whose forward passes execute an efﬁcient Value Iteration, prop-
agate beliefs of state in partially observable environments, and
choose optimal actions respectively. Backpropagating gradients
through these RCNNs allows the system to explicitly learn the
Transition Model and Reward Function associated with the
underlying MDP, serving as an elegant alternative to classical
model-based RL. We evaluate the proposed algorithms in sim-
ulation, considering a robot planning problem. We demonstrate
the capability of our framework to reduce the cost of re-planning,
learn accurate MDP models, and ﬁnally re-plan with learnt
models to achieve near-optimal policies.

I. INTRODUCTION

techniques,

Deep Reinforcement Learning (DRL) algorithms exploit
model-free Reinforcement Learning (RL)
to
achieve high levels of performance on a variety of tasks, often
on par with human experts in the same domain [1]. These DRL
methods use deep networks to either approximate the action-
value functions as in Deep Q Networks [1]–[3], or directly
parametrizing the policy, as in policy gradient methods [4].
While DRL methods adopt model-free approaches in order
to generalize performance across various tasks, it is difﬁcult
to intuitively understand the reasoning of DRL approaches in
making a particular choice of action, since they often ignore
the underlying structure of the tasks.

In contrast, model-based methods [5]–[8] exploit this inher-
ent structure to make decisions, based on domain knowledge.
The estimates of the transition model and reward function
associated with the underlying Markov Decision Process
(MDP) are transferable across environments and agents [9],
and provide insight into the system’s choice of actions. A sig-
niﬁcant deterrent from model-based RL is the indirect nature
of learning with the estimated model; subsequent planning is
required to obtain the optimal policy [10].

To jointly exploit this inherent structure and overcome this
indirect nature, we present a novel fusion of Reinforcement
and Deep Learning, by representing classical solutions to RL

problems within the framework of Deep Learning architec-
tures; in particular Recurrent Convolutional Neural Networks
(RCNNs). By explicitly connecting the steps of solving MDPs
with architectural elements of RCNNs, our representation
inherits properties of such networks; allowing the use of
backpropagation on the deﬁned RCNNs as an elegant solution
to model-based RL problems. The representation also exploits
the inherent structure of the MDP to reduce the cost of
replanning, further incentivizing model-based approaches.

The contributions of this paper are hence three-fold. We
deﬁne a Value Iteration RCNN (VI RCNN), whose forward
passes carry out Value Iteration to efﬁciently obtain the
optimal policy. Second, we deﬁne a Belief Propagation RCNN
(BP RCNN), to update beliefs of state via the Bayes Filter.
Backpropagation through this network learns the underlying
transition model of the agent in partially observable envi-
ronments. Finally, we deﬁne a QMDP RCNN by combining
the VI RCNN and the BP RCNN. The QMDP RCNN com-
putes optimal choices of actions for a given belief of state.
Backpropagation through the QMDP RCNN learns the reward
function of an expert agent, in a Learning from Demonstration
via Inverse Reinforcement Learning (LfD-IRL) setting [11].
The learnt reward function and transition model may then be
used to re-plan via the VI RCNN, and the QMDP RCNN to
make optimal action choices for new beliefs and the learnt
Q-values. 1

We note that [12] follows an approach mathematically
similar to the VI RCNN; however it differs in being a model-
free approach. The model-based approach followed here learns
MDP models transferable across environments and agents.
Further, the QMDP RCNN approach proposed here follows
more naturally than the fully connected layer used in [12].
The gradient updates of the QMDP RCNN adopt an intuitive
form, that further contributes to an intuitive understanding of
the action choices of the QMDP RCNN, as compared to [12].
We evaluate each of the RCNNs proposed in simulation,
where the given task is a 2-D robot planning problem. We
demonstrate the capacity of the VI RCNN to reduce the cost
of re-planning by signiﬁcantly reducing the execution time
as compared to classical Value Iteration. We evaluate the BP

1Please ﬁnd our code and supplementary material at

https://

github.com/tanmayshankar/RCNN_MDP.

Fig. 1. Schematic representation of the Value Iteration Recurrent Convolution Network. Notice the 4 stages present- convolution, Fixed-bias,
Max-pooling and recurrence. The VI RCNN facilitates a natural representation of the Bellman update as a single RCNN layer.

RCNN based on the accuracy of the learnt transition models
against ground truth models, and show that it appreciably out-
performs naive model-based methods for partially observable
environments. Our experiments ﬁnally demonstrate that the
QMDP RCNN is able to generate policies via re-planning
with learnt MDP models, that accurately represent policies
generated from ground truth MDP models.

II. THE VALUE ITERATION RCNN

In this section, we formulate Value Iteration as a recurrent
convolution, and hence present the Value Iteration RCNN (VI
RCNN) to carry out Value Iteration. We consider a standard
Markov Decision Process, consisting of a 2-dimensional state
space S of size Nd × Nd, state s ∈ S, action space A
of size Na, actions a ∈ A, a transition model T (s, a, s(cid:48)),
reward function R(s, a), and discount factor γ. Value Iteration,
which is typically used in solving for optimal policies in an
MDP, invokes the Bellman Update equation as Vk+1(s) =
maxa R(s, a) + γ (cid:80)
s(cid:48) T (s, a, s(cid:48))Vk(s(cid:48)).
Value Iteration as a Convolution: In our 2-D state space, S,
states s and s(cid:48) are deﬁned by 2 pairs of indices (i, j) and (p, q).
The summation over all states s(cid:48) may hence be equivalently
represented as (cid:80)
(cid:80)Nd
q=1. The transition model is
hence of size N 2
d . We may express Vk+1(s) as
maxa R(s, a) + γ (cid:80)Nd

s(cid:48) = (cid:80)Nd
p=1
d × Na × N 2
(cid:80)Nd
p=1

q=1 T (s, a, s(cid:48))p,qVk(s(cid:48))p,q.

is impossible for the agent

Actions correspond to transitions in the agent’s state, dic-
tated by the internal dynamics of the agent, or in the case
of physical robots, restricted by the robot’s conﬁguration
to
and actuator capacities. It
immediately move from its current state to a far away state.
This allows us to deﬁne a w neighborhood centred around
the state s(cid:48), W (s(cid:48)), so the agent only has a ﬁnite probability
of transitioning to other states within W . Mathematically,
T (s, a, s(cid:48)) (cid:54)= 0 if |i − p| ≤ w and |j − q| ≤ w or if s ∈ W (s(cid:48)),
and T (s, a, s(cid:48)) = 0 for s (cid:54)∈ W (s(cid:48)).
the transition model

to the
location of the occurring transition. For example,
spatial
a differential drive robot’s dynamics are independent of
the robot’s position. We assume the transition model
is
stationary over
incorporating appropriate
state boundary conditions. We may now visualize this w
neighborhood restricted transition model
itself to be cen-
tred around s(cid:48). By deﬁning a ﬂipped transition model as:
T (s, a, s(cid:48))m,n = T (s, a, s(cid:48))Nt−m,Nt−n, and indexing it

is often invariant

the state space,

Further,

by (u, v), we may express Vk+1(s) as maxa R(s, a) +
γ (cid:80)w
v=−w T (s, a, s(cid:48))u,vVk(s(cid:48))i−u,j−v. We may rep-
resent this as a convolution:

(cid:80)w

u=−w

Vk+1(s) = max

R(s, a) + γ T (s, a, s(cid:48)) ∗ Vk(s(cid:48))

(1)

a

The Value Iteration RCNN: We deﬁne a Value Iteration
RCNN (VI RCNN) to represent classical Value Iteration, based
on the correlation between the Bellman update equation, and
the architectural elements of an RCNN. Equation (1) can be
thought of as a single layer of a recurrent convolutional neural
network consisting of the following 4 stages:
1) Convolutional Stage: The convolution of T (s, a, s(cid:48))∗Vk(s(cid:48))

represents the convolutional stage.

2) Max-Pooling Stage: The maximum at every state s taken
over all actions a, maxa is analogous to a max-pooling
stage along the action ‘channel’.

3) Fixed Bias Stage: The introduction of the reward term

R(s, a) is treated as addition of a ﬁxed bias.

4) Recurrence Stage: As k is incremented in successive iter-
ations, Vk+1(s) is fed back as an ‘input’ to the network,
introducing a recurrence relation into the network.
We may think of Vk(s(cid:48)) as a single-channel Nd ×Nd image.
T (s, a, s(cid:48)) then corresponds to a series of Na transition ﬁlters,
each of Nt × Nt size (Nt = 2w + 1), each to be convolved
with the image. The values of the transition ﬁlters correspond
directly to transition probabilities between states s and s(cid:48),
upon taking action a. Note that these convolutional transition
ﬁlters naturally capture the spatial invariance of transitions by
virtue of lateral parameter sharing inherent to convolutional
networks. Further, in this representation, each transition ﬁlter
corresponds directly to a particular action. This unique one-
to-one mapping between actions and ﬁlters proves to be very
useful in learning MDP or RL models, as we demonstrate in
section III. Finally, the VI RCNN is completely differentiable,
and can be trained by applying backpropagation through it.

III. THE BELIEF PROPAGATION RCNN
In this section, we present the Belief Propagation RCNN
(BP RCNN), to represent the Bayes ﬁlter belief update within
the architecture of an RCNN, and hence learn MDP transition
models. For an agent to make optimal choices of actions
in partially observable environments, an accurate belief of
state b(s) is required. Upon taking an action a and receiving
observation z, we may use the Bayes ﬁlter to propagate the

The Belief Propagation RCNN: We deﬁne a Belief Propaga-
tion RCNN (BP RCNN) to represent the Bayes Filter belief
update, analogous to the representation of Value Iteration as
the VI RCNN. A single layer of the BP RCNN represents
equation (3), consisting of the following stages:
1) Convolution Stage: The convolution T (s, a, s(cid:48)) ∗ b(s) rep-

resents the convolution stage of the BP RCNN.

2) Element-wise Product Stage: The element-wise multiplica-
tion O(sz) (cid:12) b(s(cid:48)), can be considered as an element-wise
product (or Hadamard product) layer in 2D.

3) Recurrence Stage: The network output, b(s(cid:48)), is fed as
the input b(s) at the next time step, forming an output
recurrence network.
Forward passes of the BP RCNN propagate beliefs of state,
given a choice of action at and a received observation zt.
The belief of state at any given instant, b(s), is treated as a
single channel Nd × Nd image, and is convolved with a single
transition ﬁlter Tat, corresponding to the action executed, at.
A key difference between the VI RCNN and the BP RCNN is
that only a single transition ﬁlter is activated during the belief
propagation, since the agent may only execute a single action
at any given time instant. The BP RCNN is also completely
differentiable.
Training and Loss: Learning the weights of the BP RCNN via
backpropagation learns the transition model of the agent. Our
objective is thus to learn a transition model T (cid:48)(s, a, s(cid:48)) such
that network output b(s(cid:48)) is as close to the target belief (cid:100)b(s(cid:48))
as possible, at every time step. Formally, we minimize a loss
function deﬁned as the least square error between both beliefs
over all states: Lt = (cid:80)Nd
i,j, with
conditions on the transition model, 0 ≤ T (cid:48)(s, a, s(cid:48))m,n ≤ 1,
and (cid:80)w
n=−w T (cid:48)(s, a, s(cid:48))m,n = 1 ∀ a ∈ A, being
incorporated using appropriate penalties.

t) − b(s(cid:48)

(cid:80)Nd
j=1

(cid:0)(cid:91)b(s(cid:48)

t)(cid:1)2

m=−w

(cid:80)w

i=1

The BP RCNN is trained in a typical RL setting, with
an agent interacting with an environment by taking random
action choices (at, at+1...), and receiving corresponding ob-
servations, (zt, zt+1...). The target beliefs, (cid:100)b(s(cid:48)), are gener-
ated as one-hot representations of the observed state. While
training the BP RCNN,
the randomly initialized transition
model magniﬁes the initial uncertainty in the belief as it is
propagated forward in time, leading to instability in learning
the transition model. Teacher forcing [13] is adopted to handle
this uncertainty; thus the target belief, rather than the network
output, is propagated as the input for the next time step. Since
the target belief is independent of the initial uncertainty of the
transition model, the network is able to learn a meaningful
set of ﬁlter values. Teacher forcing, or such target recurrence,
also decouples consecutive time steps, reducing the backprop-
agation through time to backpropagation over data points that
are only generated sequentially.

IV. THE QMDP RCNN

We ﬁnally present the QMDP RCNN as a combination of
the VI RCNN and the BP RCNN, to retrieve optimal choices
of actions, and learn the underlying reward function of the

Fig. 2. Schematic representation of the Belief Propagation Recurrent
Convolution Network. Forward passes represented by black arrows,
while backpropagation is symbolized by blue arrows.

belief b(s(cid:48)) in terms of an observation model O(s(cid:48), a, z),
associated with probability p(z|s(cid:48), a). The discrete Bayes ﬁlter
is depicted in (2). Note the denominator is equivalent
to
p(z|a, b), and can be implemented as a normalization factor
η.

b(s(cid:48)) =

O(s(cid:48), a, z) (cid:80)
s(cid:48)∈S O(s(cid:48), a, z) (cid:80)

(cid:80)

s∈S T (s, a, s(cid:48))b(s)

s∈S T (s, a, s(cid:48))b(s)

(2)

i=1

(cid:80)w

s as (cid:80)Nd

Bayes Filter as a Convolution and an Element-wise Product:
We may represent the motion update of the traditional Bayes
ﬁlter as a convolution, analogous to the representation of
(cid:80)
s(cid:48) T (s, a, s(cid:48))Vk(s(cid:48)) step of Value Iteration as T (s, a, s(cid:48)) ∗
Vk(s(cid:48)) in equation (1). Transitions in a Bayes ﬁlter occur
from s to s(cid:48), in contrast to the ‘backward’ passes executed
in Value Iteration, from s(cid:48) to s. This suggests our transition
model is centred around s rather than s(cid:48), and we may represent
(cid:80)
(cid:80)Nd
j=1. We write the intermediate belief b(s(cid:48))p,q
as (cid:80)w
v=−w T (s, a, s(cid:48))u,v b(s)p−u,q−v, which can be
expressed as a convolution b(s(cid:48)) = T (s, a, s(cid:48)) ∗ b(s). For
the Bayes Filter correction update, we consider a simple
observation model O(z, s(cid:48)) for observation z in state s(cid:48).
The agent observes its own state as sz
x,y, indexed by x and
y, given that the actual state of the robot is s(cid:48)
i,j, with a
probability p(sz
i,j). Analogous to the w neighborhood for
the transition model, we introduce a h neighborhood for the
observation model, centred around the observed state sz
x,y.
Thus O(sz
i,j within the h neighborhood
H(sz

x,y, s(cid:48)) (cid:54)= 0 for states s(cid:48)

x,y, s(cid:48)) = 0 for all s(cid:48)

x,y), and O(sz

i,j (cid:54)∈ H(sz

x,y|s(cid:48)

x,y).

u=−w

The correction update in the Bayes ﬁlter can be represented
as a masking operation on the intermediate belief b(s(cid:48)), or an
element-wise (Hadamard) product ((cid:12)). Since the observation
O(sz) is also centred around sz, we may express b(s(cid:48))i,j as
η O(sz)i−x,j−y b(s(cid:48))i,j ∀ i, j, which corresponds to a nor-
malized element-wise product b(s(cid:48)) = η O(sz) (cid:12) b(s(cid:48)). Upon
combining the motion update convolution and the correction
update element-wise product, we may represent the Bayes
Filter as a convolutional stage followed by an element-wise
product. We have:

b(s(cid:48)) = η O(sz) (cid:12) T (s, a, s(cid:48)) ∗ b(s)

(3)

Fig. 3. Schematic representation of the QMDP RCNN, as a combination of the Value Iteration RCNN and the Belief Propagation RCNN.
Notice the following stages: inner product, sum-pooling, and the softmax stage providing the output actions. The gradients propagated to
the reward function may alternately be propagated to the Q-value estimate. Forward passes of the QMDP RCNN are used for planning in
partially observable domains, and backpropagation learns the reward function of the MDP. The BP RCNN component can be trained in
parallel to the QMDP RCNN.

POMDP (in addition to the transition model). Treating the
outputs of the VI RCNN and the BP RCNN as the action-value
function, Q(s, a), and the belief of state, b(s) respectively
at every time step, we may fuse these outputs to compute
belief space Q-values Q(b(s), a) using the QMDP approxi-
mation. We have Q(b(s), a) = (cid:80)
s∈S Q(s, a)M DP b(s), which
may alternately be represented as a Frobenius Inner product
(cid:104)b(s), Q(s, a)(cid:105)F . Optimal action choices ya
t , corresponding to
the highest belief space Q-values, and can be retrieved from
the softmax of Q(b(s), a):

ya

t =

eQ(b(s),a)
a eQ(b(s),a)

(cid:80)

(4)

The QMDP RCNN: In addition to the stages of the VI RCNN
and BP RCNN, the QMDP RCNN is constructed from the
following stages:
1) Frobenius Inner Product Stage: The QMDP approximation
step Q(b(s), a) = (cid:104)b(s), Q(s, a)(cid:105)F . represents an Frobe-
nius inner product stage. This can be implemented as a
‘valid’ convolutional layer with zero stride.

2) Softmax Stage: ya

t = sof tmax Q(b(s), a) represents the

softmax stage of the network.
The resultant QMDP RCNN, as depicted in Figure 3,
combines planning and learning in a single network. Forward
passes of the network provide action choices ya
t as an output,
and backpropagating through the network learns the reward
function, and updates Q-values and optimal action choices via
planning through the VI RCNN component.
Training and Loss: The QMDP RCNN is trained in a Learning
from Demonstration - Inverse Reinforcement Learning setting.

An expert agent demonstrates a set of tasks, recorded as a
series of trajectories {(at, zt), (at+1, zt+1)...}, which serve
as inputs to the network. The actions executed in the expert
trajectories are represented in a one-hot encoding, serving as
the network targets (cid:98)ya
t . The objective is to hence learn a
reward function such that the action choices output by the
network match the target actions at every time step. The
loss function chosen is the cross-entropy loss between the
output actions and the target actions at a given time step,
deﬁned as Ct = − (cid:80)
t lnya
t . The QMDP RCNN learns
the reward function by backpropagating the gradients retrieved
from this loss function through the network, to update the
reward function. The updated reward estimate is then fed back
to the VI RCNN component to update the Q-values, and hence
the action choices of the network. Experience replay [14] is
used to randomize over the expert trajectories and transitions
while training the QMDP RCNN.

a (cid:98)ya

t − (cid:98)ya

t )b(s). The (ya

The closed form of the gradient for reward updates is of the
form of R(s, a) αt←− −(ya
t ) term thus
dictates the extent to which actions are positively or negatively
reinforced, while the belief term b(s) acts in a manner similar
to an attention mechanism, which directs the reward function
updates to speciﬁc regions of the state space where the agent
is believed to be.

t − (cid:98)ya

We emphasize that

the QMDP RCNN differs from tra-
ditional DRL approaches in that the QMDP RCNN is not
provided with samples of the reward function itself via its
interaction with the environment, rather, it is provided with
action choices made by the expert. While the LfD-IRL ap-
proach employed for the QMDP RCNN is on-policy, the in-

TABLE I
Transition Model Error and Replanning Policy Accuracy for BP RCNN.
Values in Bold represent best-in-class performance. Parameter values marked * are maintained during variation of alternate parameters, and are used during ﬁnal experiments.

Belief Propagation RCNN

Weighted Counting

Naive Counting

Parameter
Environment
Observability

Teacher Forcing

Learning Rate
Adaptation

Algorithm:
Type
Fully Observable
Partially Observable*
Output Recurrence
Target Recurrence*
RMSProp
Linear Decay
Filter-wise Decay*

Transition Error
0.0210
0.1133
3.5614
0.1133
2.8811
0.2418
0.1133

Replanning Accuracy
96.882 %
95.620 %
21.113 %
95.620 %
40.315 %
93.596 %
95.620 %

Transition Error
0.0249
1.0637
3.0831
1.0637
2.5703
1.3236
1.0637

Replanning Accuracy
96.564 %
75.571 %
27.239 %
75.571 %
44.092 %
52.451 %
75.571 %

Transition Error
0.0249
0.1840

Replanning Accuracy
96.564 %
83.590 %

No use of Recurrence.

No use of Learning Rate.

compared to the original policy. A comparison is provided
against the models learnt using a counting style algorithm
analogous to that used in [15], as well as a weighted-counting
style algorithm that updates model-estimates by counting over
belief values. We experiment with the performance of these
algorithms under both fully and partially observable settings,
and whether teacher forcing is used (via target recurrence) or
not (as in output recurrence). Different methods of adapting the
learning rate are explored, including RMSProp, linear decay,
and maintaining individual learning rates for each transition
ﬁlter, or ﬁlter-wise decay, as presented in Table I.

The BP RCNN’s loss function is deﬁned in terms of the
non-stationary beliefs at any given time instant. An online
training approach is hence rather than batch mode. We observe
the ﬁlter-wise decay outperforms linear decay when different
actions are chosen with varying frequencies. RMSProp suffers
from the oscillations in the loss function that arise due to this
dynamic nature, and hence performs poorly. The use of teacher
forcing mitigates this dynamic nature over time, increasing
replanning accuracy from 21.11% to 95.62%. The 95.62%
replanning accuracy attained under partial-observability ap-
proaches the 96.88% accuracy of the algorithm under fully
observable settings.
QMDP RCNN: For the QMDP RCNN, the objective is learn
a reward function that results in a similar policy and level
of performance as the original reward. Since learning such
a reward such that the demonstrated trajectories are optimal
does not have a unique solution, quantifying the accuracy of
the reward estimate is meaningless. Rather, we present the
replanning accuracy (as used in the BP RCNN) for both learnt
and known transition models and rewards. We also run policy
evaluation on the generated policy using the original rewards,
and present the increase in expected reward for the learnt
models; deﬁned as 1
. The results are
N 2
d
presented in Table II.

V (s)learnt−V (s)orig
V (s)orig

(cid:80)
s

As is the case in the BP RCNN, RMSProp (and adaptive
learning rates in general) counter the magnitude of reward
updates dictated by the QMDP RCNN, and hence adversely
affect performance. Experience Replay marginally increases
the replanning accuracy, but has a signiﬁcant effect on the
increase in expected reward. Similarly, using delayed feedback
(after passes through an entire trajectory) also boosts the
increase in expected reward. On learning both rewards and
transition models, the QMDP RCNN achieves an appreciable
65.120% policy error and a minimal −10.688% change in

Fig. 4. Plot of the time per iteration of Value Iteration versus size
of the State Space. The time taken by the VI RCNN is orders of
magnitude lesser than regular Value Iteration, for all transition space
sizes.

built planning (via the VI RCNN) causes reward updates to
permeate the entire state space. The dependence of the LfD-
IRL approach in the QMDP RCNN on expert-optimal actions
for training differs from the BP RCNN, which uses arbitrary
(and often suboptimal) choices of actions.

V. EXPERIMENTAL RESULTS AND DISCUSSIONS

In this section, experimental results are individually pre-

sented with respect to each of the 3 RCNNs deﬁned.
VI RCNN: Here, our contribution is a representation that
enables a more efﬁcient computation of Value Iteration. We
thus present the per-iteration run-time of Value Iteration via
the VI RCNN, versus a standard implementation of Value
Iteration. Both algorithms are implemented in Python, run on
an Intel Core i7 machine, 2.13 GHz, with 4-cores. The inherent
parallelization of the convolution facilitates the speed-up of
VI by several orders of magnitude, as depicted in Figure 4; at
best, the VI RCNN completes a single iteration 5106.42 times
faster, and at worst, it is 1704.43 times faster than the regular
implementation.
BP RCNN: The primary objective while training the BP
RCNN is to determine an accurate estimate of the transition
model. We evaluate performance using the least square error
between the transition model estimate and the ground truth
u,v)2. Since
model, deﬁned as C (cid:48)
a
the ﬁnal objective is to use these learnt models to generate
new policies by replanning via the VI RCNN, we also present
the replanning accuracy of each model; deﬁned as the per-
centage of actions chosen correctly by the network’s policy,

t = (cid:80)

u,a − T (cid:48)a

v (T a

(cid:80)
u

(cid:80)

TABLE II
Replanning Accuracy and Increase in Expected Reward for Inverse Reinforcement Learning from Demonstrations via the QMDP RCNN.
Values in Bold represent best-in-class performance. Parameter values marked * are maintained during variation of alternate parameters, and are used during ﬁnal experiments.

Algorithm:
Parameter:

Learning Rate

QMDP RCNN
Experience Replay

Environment:

Parameter Value:

Linear Decay*

None

Learnt Reward
Learnt Transition

Learnt Reward
Known Transition

Known Reward
Learnt Transition

Replanning Accuracy
Expected Reward Increase

Replanning Accuracy
Expected Reward Increase

Replanning Accuracy
Expected Reward Increase

RMSProp

45.113 %
-75.030 %

43.932 %
-79.601 %

65.120 %
-10.688 %

63.476 %
-11.429 %

62.211 %
-16.358 %

60.852 %
-16.106 %

Used*

65.120 %
-10.688 %

63.476 %
-11.429 %

Feedback Recurrence

Immediate

65.434 %
-16.063 %

63.900 %
-17.949 %

Delayed*

65.120 %
-10.688 %

63.476 %
-11.429 %

95.620 % (Known Reward used for Replanning)
+0.709 % (Known Reward used for Replanning)

Weighted
Counting

45.102 %
-74.030 %

49.192 %
-64.294 %

75.571 %
-0.617 %

Naive
Counting

50.192 %
-43.166 %

53.427 %
-31.812 %

83.590 %
-0.806 %

expected reward. We emphasize that given access solely to
observations and action choices of the agent, and without
assuming any feature engineering of the reward, the QMDP
RCNN is able to achieve near-optimal
levels of expected
reward. Utilizing the ground truth transition with the learnt
reward, the QMDP RCNN performs marginally worse, with
a 63.476% accuracy and a −11.429% change in expected
reward.

The true efﬁcacy of the BP RCNN and the QMDP RCNN
lie in their ability to learn accurate transition models and
reward functions in partially observable settings, where they
outperforms existing model-based approaches of naive and
in terms of re-
weighted counting by signiﬁcant margins,
planning accuracy,
transition errors, and expected reward.
Finally, we note that while all 3 deﬁned RCNN architectures
are demonstrated for 2-D cases, these architectures can be
extended to any number of dimensions, and number of actions,
with suitable modiﬁcations to the convolution operation in
higher dimensions.

VI. CONCLUSION

In this paper, we deﬁned 3 RCNN like architectures, namely
the Value Iteration RCNN, the Belief Propagation RCNN, and
the QMDP RCNN, to facilitate a more natural representation of
solutions to model-based Reinforcement Learning. Together,
these contributions speed up the planning process in a partially
observable environment, reducing the cost of replanning for
model-based approaches. Given access to agent observations
and action choices over time, the BP RCNN learns the transi-
tion model, and the QMDP RCNN learns the reward function,
and subsequently replans with learnt models to make near-
optimal action choices. The proposed architectures were also
found to outperform existing model-based approaches in speed
and model accuracy. The natural symbiotic representation of
planning and learning algorithms allows these approaches to
be extended to more complex tasks, by integrating them with
sophisticated perception modules.

REFERENCES

[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” in NIPS Deep Learning Workshop, 2013.

[2] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” CoRR, vol. abs/1509.06461, 2015. [Online].
Available: http://arxiv.org/abs/1509.06461

[3] M. J. Hausknecht and P. Stone, “Deep recurrent q-learning for
partially observable mdps,” CoRR, vol. abs/1507.06527, 2015. [Online].
Available: http://arxiv.org/abs/1507.06527

[4] R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour, “Policy gradient
methods for reinforcement learning with function approximation,” in In
Advances in Neural Information Processing Systems 12. MIT Press,
2000, pp. 1057–1063.

learning,” in Proceedings of

IEEE Press, 1997, pp. 3557–3564.

[5] C. G. Atkeson and J. C. Santamar´ıa, “A comparison of direct
the
and model-based reinforcement
1997 IEEE International Conference on Robotics and Automation,
vol. 4.
[Online]. Available:
ftp://ftp.cc.gatech.edu/pub/people/cga/rl-compare.ps.gz
[6] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement

learning in
robotics: A survey,” I. J. Robotic Res., vol. 32, no. 11, pp. 1238–1274,
2013. [Online]. Available: http://dx.doi.org/10.1177/0278364913495721
[7] B. Bakker, V. Zhumatiy, G. Gruener, and J. Schmidhuber, “Quasi-online
reinforcement learning for robots,” in Proceedings of the 2006 IEEE
International Conference on Robotics and Automation ICRA, 2006.
[8] T. Hester, M. Quinlan, and P. Stone, “Generalized model learning for
reinforcement learning on a humanoid robot.” in ICRA.
IEEE, 2010,
pp. 2369–2374. [Online]. Available: http://dblp.uni-trier.de/db/conf/icra/
icra2010.html#HesterQS10

[9] J. Choi and K.-E. Kim, “Inverse reinforcement learning in partially
observable environments,” J. Mach. Learn. Res., vol. 12, pp. 691–
730, Jul. 2011. [Online]. Available: http://dl.acm.org/citation.cfm?id=
1953048.2021028

[10] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning,

1st ed. Cambridge, MA, USA: MIT Press, 1998.

[11] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-
forcement learning,” in In Proceedings of the Twenty-ﬁrst International
Conference on Machine Learning. ACM Press, 2004.

[12] A. Tamar, S. Levine, and P. Abbeel, “Value iteration networks,” CoRR,
vol. abs/1602.02867, 2016. [Online]. Available: http://arxiv.org/abs/
1602.02867

[13] A. C.

Ian Goodfellow, Yoshua Bengio, “Deep learning,” 2016,
[Online]. Available: http:

book in preparation for MIT Press.
//www.deeplearningbook.org

[14] L.-J. Lin, “Self-improving reactive agents based on reinforcement
learning, planning and teaching,” Machine Learning, vol. 8, no.
3–4, pp. 293–321, 1992. [Online]. Available: http://www.cs.ualberta.ca/
∼sutton/lin-92.pdf

[15] M. Kearns and S. Singh, “Near-optimal reinforcement

learning in
polynomial time,” Mach. Learn., vol. 49, no. 2-3, pp. 209–232, Nov.
2002. [Online]. Available: http://dx.doi.org/10.1023/A:1017984413808
[16] B. C. Stadie, S. Levine, and P. Abbeel, “Incentivizing exploration
in reinforcement learning with deep predictive models,” CoRR, vol.
abs/1507.00814, 2015. [Online]. Available: http://arxiv.org/abs/1507.
00814

[17] B. Bakker, “Reinforcement learning with long short-term memory,” in

In NIPS. MIT Press, 2002, pp. 1475–1482.

[18] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. P. Singh, “Action-conditional
video prediction using deep networks in atari games,” CoRR, vol.
abs/1507.08750, 2015. [Online]. Available: http://arxiv.org/abs/1507.
08750

[19] C. G. Atkeson,

“Nonparametric model-based

reinforcement
Information Processing Systems
[NIPS Conference, Denver, Colorado, USA, 1997], 1997,
http://papers.nips.cc/paper/
1008–1014.

learning,” in Advances in Neural
10,
pp.
1476-nonparametric-model-based-reinforcement-learning

[Online]. Available:

[20] S. Shalev-Shwartz, N. Ben-Zrihem, A. Cohen, and A. Shashua, “Long-
term planning by short-term prediction,” CoRR, vol. abs/1602.01580,
2016. [Online]. Available: http://arxiv.org/abs/1602.01580

Reinforcement Learning via Recurrent
Convolutional Neural Networks

Tanmay Shankar
Department of Mech. Engg.
IIT Guwahati
tanmay.shankar@gmail.com

Santosha K Dwivedy
Department of Mech. Engg.
IIT Guwahati
dwivedy@iitg.ac.in

Prithwijit Guha
Department of EEE
IIT Guwahati
pguha@iitg.ac.in

7
1
0
2
 
n
a
J
 
9
 
 
]

G
L
.
s
c
[
 
 
1
v
2
9
3
2
0
.
1
0
7
1
:
v
i
X
r
a

Abstract—Deep Reinforcement Learning has enabled the learn-
ing of policies for complex tasks in partially observable envi-
ronments, without explicitly learning the underlying model of
the tasks. While such model-free methods achieve considerable
performance, they often ignore the structure of task. We present a
natural representation of to Reinforcement Learning (RL) prob-
lems using Recurrent Convolutional Neural Networks (RCNNs),
to better exploit this inherent structure. We deﬁne 3 such RCNNs,
whose forward passes execute an efﬁcient Value Iteration, prop-
agate beliefs of state in partially observable environments, and
choose optimal actions respectively. Backpropagating gradients
through these RCNNs allows the system to explicitly learn the
Transition Model and Reward Function associated with the
underlying MDP, serving as an elegant alternative to classical
model-based RL. We evaluate the proposed algorithms in sim-
ulation, considering a robot planning problem. We demonstrate
the capability of our framework to reduce the cost of re-planning,
learn accurate MDP models, and ﬁnally re-plan with learnt
models to achieve near-optimal policies.

I. INTRODUCTION

techniques,

Deep Reinforcement Learning (DRL) algorithms exploit
model-free Reinforcement Learning (RL)
to
achieve high levels of performance on a variety of tasks, often
on par with human experts in the same domain [1]. These DRL
methods use deep networks to either approximate the action-
value functions as in Deep Q Networks [1]–[3], or directly
parametrizing the policy, as in policy gradient methods [4].
While DRL methods adopt model-free approaches in order
to generalize performance across various tasks, it is difﬁcult
to intuitively understand the reasoning of DRL approaches in
making a particular choice of action, since they often ignore
the underlying structure of the tasks.

In contrast, model-based methods [5]–[8] exploit this inher-
ent structure to make decisions, based on domain knowledge.
The estimates of the transition model and reward function
associated with the underlying Markov Decision Process
(MDP) are transferable across environments and agents [9],
and provide insight into the system’s choice of actions. A sig-
niﬁcant deterrent from model-based RL is the indirect nature
of learning with the estimated model; subsequent planning is
required to obtain the optimal policy [10].

To jointly exploit this inherent structure and overcome this
indirect nature, we present a novel fusion of Reinforcement
and Deep Learning, by representing classical solutions to RL

problems within the framework of Deep Learning architec-
tures; in particular Recurrent Convolutional Neural Networks
(RCNNs). By explicitly connecting the steps of solving MDPs
with architectural elements of RCNNs, our representation
inherits properties of such networks; allowing the use of
backpropagation on the deﬁned RCNNs as an elegant solution
to model-based RL problems. The representation also exploits
the inherent structure of the MDP to reduce the cost of
replanning, further incentivizing model-based approaches.

The contributions of this paper are hence three-fold. We
deﬁne a Value Iteration RCNN (VI RCNN), whose forward
passes carry out Value Iteration to efﬁciently obtain the
optimal policy. Second, we deﬁne a Belief Propagation RCNN
(BP RCNN), to update beliefs of state via the Bayes Filter.
Backpropagation through this network learns the underlying
transition model of the agent in partially observable envi-
ronments. Finally, we deﬁne a QMDP RCNN by combining
the VI RCNN and the BP RCNN. The QMDP RCNN com-
putes optimal choices of actions for a given belief of state.
Backpropagation through the QMDP RCNN learns the reward
function of an expert agent, in a Learning from Demonstration
via Inverse Reinforcement Learning (LfD-IRL) setting [11].
The learnt reward function and transition model may then be
used to re-plan via the VI RCNN, and the QMDP RCNN to
make optimal action choices for new beliefs and the learnt
Q-values. 1

We note that [12] follows an approach mathematically
similar to the VI RCNN; however it differs in being a model-
free approach. The model-based approach followed here learns
MDP models transferable across environments and agents.
Further, the QMDP RCNN approach proposed here follows
more naturally than the fully connected layer used in [12].
The gradient updates of the QMDP RCNN adopt an intuitive
form, that further contributes to an intuitive understanding of
the action choices of the QMDP RCNN, as compared to [12].
We evaluate each of the RCNNs proposed in simulation,
where the given task is a 2-D robot planning problem. We
demonstrate the capacity of the VI RCNN to reduce the cost
of re-planning by signiﬁcantly reducing the execution time
as compared to classical Value Iteration. We evaluate the BP

1Please ﬁnd our code and supplementary material at

https://

github.com/tanmayshankar/RCNN_MDP.

Fig. 1. Schematic representation of the Value Iteration Recurrent Convolution Network. Notice the 4 stages present- convolution, Fixed-bias,
Max-pooling and recurrence. The VI RCNN facilitates a natural representation of the Bellman update as a single RCNN layer.

RCNN based on the accuracy of the learnt transition models
against ground truth models, and show that it appreciably out-
performs naive model-based methods for partially observable
environments. Our experiments ﬁnally demonstrate that the
QMDP RCNN is able to generate policies via re-planning
with learnt MDP models, that accurately represent policies
generated from ground truth MDP models.

II. THE VALUE ITERATION RCNN

In this section, we formulate Value Iteration as a recurrent
convolution, and hence present the Value Iteration RCNN (VI
RCNN) to carry out Value Iteration. We consider a standard
Markov Decision Process, consisting of a 2-dimensional state
space S of size Nd × Nd, state s ∈ S, action space A
of size Na, actions a ∈ A, a transition model T (s, a, s(cid:48)),
reward function R(s, a), and discount factor γ. Value Iteration,
which is typically used in solving for optimal policies in an
MDP, invokes the Bellman Update equation as Vk+1(s) =
maxa R(s, a) + γ (cid:80)
s(cid:48) T (s, a, s(cid:48))Vk(s(cid:48)).
Value Iteration as a Convolution: In our 2-D state space, S,
states s and s(cid:48) are deﬁned by 2 pairs of indices (i, j) and (p, q).
The summation over all states s(cid:48) may hence be equivalently
represented as (cid:80)
(cid:80)Nd
q=1. The transition model is
hence of size N 2
d . We may express Vk+1(s) as
maxa R(s, a) + γ (cid:80)Nd

s(cid:48) = (cid:80)Nd
p=1
d × Na × N 2
(cid:80)Nd
p=1

q=1 T (s, a, s(cid:48))p,qVk(s(cid:48))p,q.

is impossible for the agent

Actions correspond to transitions in the agent’s state, dic-
tated by the internal dynamics of the agent, or in the case
of physical robots, restricted by the robot’s conﬁguration
to
and actuator capacities. It
immediately move from its current state to a far away state.
This allows us to deﬁne a w neighborhood centred around
the state s(cid:48), W (s(cid:48)), so the agent only has a ﬁnite probability
of transitioning to other states within W . Mathematically,
T (s, a, s(cid:48)) (cid:54)= 0 if |i − p| ≤ w and |j − q| ≤ w or if s ∈ W (s(cid:48)),
and T (s, a, s(cid:48)) = 0 for s (cid:54)∈ W (s(cid:48)).
the transition model

to the
location of the occurring transition. For example,
spatial
a differential drive robot’s dynamics are independent of
the robot’s position. We assume the transition model
is
stationary over
incorporating appropriate
state boundary conditions. We may now visualize this w
neighborhood restricted transition model
itself to be cen-
tred around s(cid:48). By deﬁning a ﬂipped transition model as:
T (s, a, s(cid:48))m,n = T (s, a, s(cid:48))Nt−m,Nt−n, and indexing it

is often invariant

the state space,

Further,

by (u, v), we may express Vk+1(s) as maxa R(s, a) +
γ (cid:80)w
v=−w T (s, a, s(cid:48))u,vVk(s(cid:48))i−u,j−v. We may rep-
resent this as a convolution:

(cid:80)w

u=−w

Vk+1(s) = max

R(s, a) + γ T (s, a, s(cid:48)) ∗ Vk(s(cid:48))

(1)

a

The Value Iteration RCNN: We deﬁne a Value Iteration
RCNN (VI RCNN) to represent classical Value Iteration, based
on the correlation between the Bellman update equation, and
the architectural elements of an RCNN. Equation (1) can be
thought of as a single layer of a recurrent convolutional neural
network consisting of the following 4 stages:
1) Convolutional Stage: The convolution of T (s, a, s(cid:48))∗Vk(s(cid:48))

represents the convolutional stage.

2) Max-Pooling Stage: The maximum at every state s taken
over all actions a, maxa is analogous to a max-pooling
stage along the action ‘channel’.

3) Fixed Bias Stage: The introduction of the reward term

R(s, a) is treated as addition of a ﬁxed bias.

4) Recurrence Stage: As k is incremented in successive iter-
ations, Vk+1(s) is fed back as an ‘input’ to the network,
introducing a recurrence relation into the network.
We may think of Vk(s(cid:48)) as a single-channel Nd ×Nd image.
T (s, a, s(cid:48)) then corresponds to a series of Na transition ﬁlters,
each of Nt × Nt size (Nt = 2w + 1), each to be convolved
with the image. The values of the transition ﬁlters correspond
directly to transition probabilities between states s and s(cid:48),
upon taking action a. Note that these convolutional transition
ﬁlters naturally capture the spatial invariance of transitions by
virtue of lateral parameter sharing inherent to convolutional
networks. Further, in this representation, each transition ﬁlter
corresponds directly to a particular action. This unique one-
to-one mapping between actions and ﬁlters proves to be very
useful in learning MDP or RL models, as we demonstrate in
section III. Finally, the VI RCNN is completely differentiable,
and can be trained by applying backpropagation through it.

III. THE BELIEF PROPAGATION RCNN
In this section, we present the Belief Propagation RCNN
(BP RCNN), to represent the Bayes ﬁlter belief update within
the architecture of an RCNN, and hence learn MDP transition
models. For an agent to make optimal choices of actions
in partially observable environments, an accurate belief of
state b(s) is required. Upon taking an action a and receiving
observation z, we may use the Bayes ﬁlter to propagate the

The Belief Propagation RCNN: We deﬁne a Belief Propaga-
tion RCNN (BP RCNN) to represent the Bayes Filter belief
update, analogous to the representation of Value Iteration as
the VI RCNN. A single layer of the BP RCNN represents
equation (3), consisting of the following stages:
1) Convolution Stage: The convolution T (s, a, s(cid:48)) ∗ b(s) rep-

resents the convolution stage of the BP RCNN.

2) Element-wise Product Stage: The element-wise multiplica-
tion O(sz) (cid:12) b(s(cid:48)), can be considered as an element-wise
product (or Hadamard product) layer in 2D.

3) Recurrence Stage: The network output, b(s(cid:48)), is fed as
the input b(s) at the next time step, forming an output
recurrence network.
Forward passes of the BP RCNN propagate beliefs of state,
given a choice of action at and a received observation zt.
The belief of state at any given instant, b(s), is treated as a
single channel Nd × Nd image, and is convolved with a single
transition ﬁlter Tat, corresponding to the action executed, at.
A key difference between the VI RCNN and the BP RCNN is
that only a single transition ﬁlter is activated during the belief
propagation, since the agent may only execute a single action
at any given time instant. The BP RCNN is also completely
differentiable.
Training and Loss: Learning the weights of the BP RCNN via
backpropagation learns the transition model of the agent. Our
objective is thus to learn a transition model T (cid:48)(s, a, s(cid:48)) such
that network output b(s(cid:48)) is as close to the target belief (cid:100)b(s(cid:48))
as possible, at every time step. Formally, we minimize a loss
function deﬁned as the least square error between both beliefs
over all states: Lt = (cid:80)Nd
i,j, with
conditions on the transition model, 0 ≤ T (cid:48)(s, a, s(cid:48))m,n ≤ 1,
and (cid:80)w
n=−w T (cid:48)(s, a, s(cid:48))m,n = 1 ∀ a ∈ A, being
incorporated using appropriate penalties.

t) − b(s(cid:48)

(cid:80)Nd
j=1

(cid:0)(cid:91)b(s(cid:48)

t)(cid:1)2

m=−w

(cid:80)w

i=1

The BP RCNN is trained in a typical RL setting, with
an agent interacting with an environment by taking random
action choices (at, at+1...), and receiving corresponding ob-
servations, (zt, zt+1...). The target beliefs, (cid:100)b(s(cid:48)), are gener-
ated as one-hot representations of the observed state. While
training the BP RCNN,
the randomly initialized transition
model magniﬁes the initial uncertainty in the belief as it is
propagated forward in time, leading to instability in learning
the transition model. Teacher forcing [13] is adopted to handle
this uncertainty; thus the target belief, rather than the network
output, is propagated as the input for the next time step. Since
the target belief is independent of the initial uncertainty of the
transition model, the network is able to learn a meaningful
set of ﬁlter values. Teacher forcing, or such target recurrence,
also decouples consecutive time steps, reducing the backprop-
agation through time to backpropagation over data points that
are only generated sequentially.

IV. THE QMDP RCNN

We ﬁnally present the QMDP RCNN as a combination of
the VI RCNN and the BP RCNN, to retrieve optimal choices
of actions, and learn the underlying reward function of the

Fig. 2. Schematic representation of the Belief Propagation Recurrent
Convolution Network. Forward passes represented by black arrows,
while backpropagation is symbolized by blue arrows.

belief b(s(cid:48)) in terms of an observation model O(s(cid:48), a, z),
associated with probability p(z|s(cid:48), a). The discrete Bayes ﬁlter
is depicted in (2). Note the denominator is equivalent
to
p(z|a, b), and can be implemented as a normalization factor
η.

b(s(cid:48)) =

O(s(cid:48), a, z) (cid:80)
s(cid:48)∈S O(s(cid:48), a, z) (cid:80)

(cid:80)

s∈S T (s, a, s(cid:48))b(s)

s∈S T (s, a, s(cid:48))b(s)

(2)

i=1

(cid:80)w

s as (cid:80)Nd

Bayes Filter as a Convolution and an Element-wise Product:
We may represent the motion update of the traditional Bayes
ﬁlter as a convolution, analogous to the representation of
(cid:80)
s(cid:48) T (s, a, s(cid:48))Vk(s(cid:48)) step of Value Iteration as T (s, a, s(cid:48)) ∗
Vk(s(cid:48)) in equation (1). Transitions in a Bayes ﬁlter occur
from s to s(cid:48), in contrast to the ‘backward’ passes executed
in Value Iteration, from s(cid:48) to s. This suggests our transition
model is centred around s rather than s(cid:48), and we may represent
(cid:80)
(cid:80)Nd
j=1. We write the intermediate belief b(s(cid:48))p,q
as (cid:80)w
v=−w T (s, a, s(cid:48))u,v b(s)p−u,q−v, which can be
expressed as a convolution b(s(cid:48)) = T (s, a, s(cid:48)) ∗ b(s). For
the Bayes Filter correction update, we consider a simple
observation model O(z, s(cid:48)) for observation z in state s(cid:48).
The agent observes its own state as sz
x,y, indexed by x and
y, given that the actual state of the robot is s(cid:48)
i,j, with a
probability p(sz
i,j). Analogous to the w neighborhood for
the transition model, we introduce a h neighborhood for the
observation model, centred around the observed state sz
x,y.
Thus O(sz
i,j within the h neighborhood
H(sz

x,y, s(cid:48)) (cid:54)= 0 for states s(cid:48)

x,y, s(cid:48)) = 0 for all s(cid:48)

x,y), and O(sz

i,j (cid:54)∈ H(sz

x,y|s(cid:48)

x,y).

u=−w

The correction update in the Bayes ﬁlter can be represented
as a masking operation on the intermediate belief b(s(cid:48)), or an
element-wise (Hadamard) product ((cid:12)). Since the observation
O(sz) is also centred around sz, we may express b(s(cid:48))i,j as
η O(sz)i−x,j−y b(s(cid:48))i,j ∀ i, j, which corresponds to a nor-
malized element-wise product b(s(cid:48)) = η O(sz) (cid:12) b(s(cid:48)). Upon
combining the motion update convolution and the correction
update element-wise product, we may represent the Bayes
Filter as a convolutional stage followed by an element-wise
product. We have:

b(s(cid:48)) = η O(sz) (cid:12) T (s, a, s(cid:48)) ∗ b(s)

(3)

Fig. 3. Schematic representation of the QMDP RCNN, as a combination of the Value Iteration RCNN and the Belief Propagation RCNN.
Notice the following stages: inner product, sum-pooling, and the softmax stage providing the output actions. The gradients propagated to
the reward function may alternately be propagated to the Q-value estimate. Forward passes of the QMDP RCNN are used for planning in
partially observable domains, and backpropagation learns the reward function of the MDP. The BP RCNN component can be trained in
parallel to the QMDP RCNN.

POMDP (in addition to the transition model). Treating the
outputs of the VI RCNN and the BP RCNN as the action-value
function, Q(s, a), and the belief of state, b(s) respectively
at every time step, we may fuse these outputs to compute
belief space Q-values Q(b(s), a) using the QMDP approxi-
mation. We have Q(b(s), a) = (cid:80)
s∈S Q(s, a)M DP b(s), which
may alternately be represented as a Frobenius Inner product
(cid:104)b(s), Q(s, a)(cid:105)F . Optimal action choices ya
t , corresponding to
the highest belief space Q-values, and can be retrieved from
the softmax of Q(b(s), a):

ya

t =

eQ(b(s),a)
a eQ(b(s),a)

(cid:80)

(4)

The QMDP RCNN: In addition to the stages of the VI RCNN
and BP RCNN, the QMDP RCNN is constructed from the
following stages:
1) Frobenius Inner Product Stage: The QMDP approximation
step Q(b(s), a) = (cid:104)b(s), Q(s, a)(cid:105)F . represents an Frobe-
nius inner product stage. This can be implemented as a
‘valid’ convolutional layer with zero stride.

2) Softmax Stage: ya

t = sof tmax Q(b(s), a) represents the

softmax stage of the network.
The resultant QMDP RCNN, as depicted in Figure 3,
combines planning and learning in a single network. Forward
passes of the network provide action choices ya
t as an output,
and backpropagating through the network learns the reward
function, and updates Q-values and optimal action choices via
planning through the VI RCNN component.
Training and Loss: The QMDP RCNN is trained in a Learning
from Demonstration - Inverse Reinforcement Learning setting.

An expert agent demonstrates a set of tasks, recorded as a
series of trajectories {(at, zt), (at+1, zt+1)...}, which serve
as inputs to the network. The actions executed in the expert
trajectories are represented in a one-hot encoding, serving as
the network targets (cid:98)ya
t . The objective is to hence learn a
reward function such that the action choices output by the
network match the target actions at every time step. The
loss function chosen is the cross-entropy loss between the
output actions and the target actions at a given time step,
deﬁned as Ct = − (cid:80)
t lnya
t . The QMDP RCNN learns
the reward function by backpropagating the gradients retrieved
from this loss function through the network, to update the
reward function. The updated reward estimate is then fed back
to the VI RCNN component to update the Q-values, and hence
the action choices of the network. Experience replay [14] is
used to randomize over the expert trajectories and transitions
while training the QMDP RCNN.

a (cid:98)ya

t − (cid:98)ya

t )b(s). The (ya

The closed form of the gradient for reward updates is of the
form of R(s, a) αt←− −(ya
t ) term thus
dictates the extent to which actions are positively or negatively
reinforced, while the belief term b(s) acts in a manner similar
to an attention mechanism, which directs the reward function
updates to speciﬁc regions of the state space where the agent
is believed to be.

t − (cid:98)ya

We emphasize that

the QMDP RCNN differs from tra-
ditional DRL approaches in that the QMDP RCNN is not
provided with samples of the reward function itself via its
interaction with the environment, rather, it is provided with
action choices made by the expert. While the LfD-IRL ap-
proach employed for the QMDP RCNN is on-policy, the in-

TABLE I
Transition Model Error and Replanning Policy Accuracy for BP RCNN.
Values in Bold represent best-in-class performance. Parameter values marked * are maintained during variation of alternate parameters, and are used during ﬁnal experiments.

Belief Propagation RCNN

Weighted Counting

Naive Counting

Parameter
Environment
Observability

Teacher Forcing

Learning Rate
Adaptation

Algorithm:
Type
Fully Observable
Partially Observable*
Output Recurrence
Target Recurrence*
RMSProp
Linear Decay
Filter-wise Decay*

Transition Error
0.0210
0.1133
3.5614
0.1133
2.8811
0.2418
0.1133

Replanning Accuracy
96.882 %
95.620 %
21.113 %
95.620 %
40.315 %
93.596 %
95.620 %

Transition Error
0.0249
1.0637
3.0831
1.0637
2.5703
1.3236
1.0637

Replanning Accuracy
96.564 %
75.571 %
27.239 %
75.571 %
44.092 %
52.451 %
75.571 %

Transition Error
0.0249
0.1840

Replanning Accuracy
96.564 %
83.590 %

No use of Recurrence.

No use of Learning Rate.

compared to the original policy. A comparison is provided
against the models learnt using a counting style algorithm
analogous to that used in [15], as well as a weighted-counting
style algorithm that updates model-estimates by counting over
belief values. We experiment with the performance of these
algorithms under both fully and partially observable settings,
and whether teacher forcing is used (via target recurrence) or
not (as in output recurrence). Different methods of adapting the
learning rate are explored, including RMSProp, linear decay,
and maintaining individual learning rates for each transition
ﬁlter, or ﬁlter-wise decay, as presented in Table I.

The BP RCNN’s loss function is deﬁned in terms of the
non-stationary beliefs at any given time instant. An online
training approach is hence rather than batch mode. We observe
the ﬁlter-wise decay outperforms linear decay when different
actions are chosen with varying frequencies. RMSProp suffers
from the oscillations in the loss function that arise due to this
dynamic nature, and hence performs poorly. The use of teacher
forcing mitigates this dynamic nature over time, increasing
replanning accuracy from 21.11% to 95.62%. The 95.62%
replanning accuracy attained under partial-observability ap-
proaches the 96.88% accuracy of the algorithm under fully
observable settings.
QMDP RCNN: For the QMDP RCNN, the objective is learn
a reward function that results in a similar policy and level
of performance as the original reward. Since learning such
a reward such that the demonstrated trajectories are optimal
does not have a unique solution, quantifying the accuracy of
the reward estimate is meaningless. Rather, we present the
replanning accuracy (as used in the BP RCNN) for both learnt
and known transition models and rewards. We also run policy
evaluation on the generated policy using the original rewards,
and present the increase in expected reward for the learnt
models; deﬁned as 1
. The results are
N 2
d
presented in Table II.

V (s)learnt−V (s)orig
V (s)orig

(cid:80)
s

As is the case in the BP RCNN, RMSProp (and adaptive
learning rates in general) counter the magnitude of reward
updates dictated by the QMDP RCNN, and hence adversely
affect performance. Experience Replay marginally increases
the replanning accuracy, but has a signiﬁcant effect on the
increase in expected reward. Similarly, using delayed feedback
(after passes through an entire trajectory) also boosts the
increase in expected reward. On learning both rewards and
transition models, the QMDP RCNN achieves an appreciable
65.120% policy error and a minimal −10.688% change in

Fig. 4. Plot of the time per iteration of Value Iteration versus size
of the State Space. The time taken by the VI RCNN is orders of
magnitude lesser than regular Value Iteration, for all transition space
sizes.

built planning (via the VI RCNN) causes reward updates to
permeate the entire state space. The dependence of the LfD-
IRL approach in the QMDP RCNN on expert-optimal actions
for training differs from the BP RCNN, which uses arbitrary
(and often suboptimal) choices of actions.

V. EXPERIMENTAL RESULTS AND DISCUSSIONS

In this section, experimental results are individually pre-

sented with respect to each of the 3 RCNNs deﬁned.
VI RCNN: Here, our contribution is a representation that
enables a more efﬁcient computation of Value Iteration. We
thus present the per-iteration run-time of Value Iteration via
the VI RCNN, versus a standard implementation of Value
Iteration. Both algorithms are implemented in Python, run on
an Intel Core i7 machine, 2.13 GHz, with 4-cores. The inherent
parallelization of the convolution facilitates the speed-up of
VI by several orders of magnitude, as depicted in Figure 4; at
best, the VI RCNN completes a single iteration 5106.42 times
faster, and at worst, it is 1704.43 times faster than the regular
implementation.
BP RCNN: The primary objective while training the BP
RCNN is to determine an accurate estimate of the transition
model. We evaluate performance using the least square error
between the transition model estimate and the ground truth
u,v)2. Since
model, deﬁned as C (cid:48)
a
the ﬁnal objective is to use these learnt models to generate
new policies by replanning via the VI RCNN, we also present
the replanning accuracy of each model; deﬁned as the per-
centage of actions chosen correctly by the network’s policy,

t = (cid:80)

u,a − T (cid:48)a

v (T a

(cid:80)
u

(cid:80)

TABLE II
Replanning Accuracy and Increase in Expected Reward for Inverse Reinforcement Learning from Demonstrations via the QMDP RCNN.
Values in Bold represent best-in-class performance. Parameter values marked * are maintained during variation of alternate parameters, and are used during ﬁnal experiments.

Algorithm:
Parameter:

Learning Rate

QMDP RCNN
Experience Replay

Environment:

Parameter Value:

Linear Decay*

None

Learnt Reward
Learnt Transition

Learnt Reward
Known Transition

Known Reward
Learnt Transition

Replanning Accuracy
Expected Reward Increase

Replanning Accuracy
Expected Reward Increase

Replanning Accuracy
Expected Reward Increase

RMSProp

45.113 %
-75.030 %

43.932 %
-79.601 %

65.120 %
-10.688 %

63.476 %
-11.429 %

62.211 %
-16.358 %

60.852 %
-16.106 %

Used*

65.120 %
-10.688 %

63.476 %
-11.429 %

Feedback Recurrence

Immediate

65.434 %
-16.063 %

63.900 %
-17.949 %

Delayed*

65.120 %
-10.688 %

63.476 %
-11.429 %

95.620 % (Known Reward used for Replanning)
+0.709 % (Known Reward used for Replanning)

Weighted
Counting

45.102 %
-74.030 %

49.192 %
-64.294 %

75.571 %
-0.617 %

Naive
Counting

50.192 %
-43.166 %

53.427 %
-31.812 %

83.590 %
-0.806 %

expected reward. We emphasize that given access solely to
observations and action choices of the agent, and without
assuming any feature engineering of the reward, the QMDP
RCNN is able to achieve near-optimal
levels of expected
reward. Utilizing the ground truth transition with the learnt
reward, the QMDP RCNN performs marginally worse, with
a 63.476% accuracy and a −11.429% change in expected
reward.

The true efﬁcacy of the BP RCNN and the QMDP RCNN
lie in their ability to learn accurate transition models and
reward functions in partially observable settings, where they
outperforms existing model-based approaches of naive and
in terms of re-
weighted counting by signiﬁcant margins,
planning accuracy,
transition errors, and expected reward.
Finally, we note that while all 3 deﬁned RCNN architectures
are demonstrated for 2-D cases, these architectures can be
extended to any number of dimensions, and number of actions,
with suitable modiﬁcations to the convolution operation in
higher dimensions.

VI. CONCLUSION

In this paper, we deﬁned 3 RCNN like architectures, namely
the Value Iteration RCNN, the Belief Propagation RCNN, and
the QMDP RCNN, to facilitate a more natural representation of
solutions to model-based Reinforcement Learning. Together,
these contributions speed up the planning process in a partially
observable environment, reducing the cost of replanning for
model-based approaches. Given access to agent observations
and action choices over time, the BP RCNN learns the transi-
tion model, and the QMDP RCNN learns the reward function,
and subsequently replans with learnt models to make near-
optimal action choices. The proposed architectures were also
found to outperform existing model-based approaches in speed
and model accuracy. The natural symbiotic representation of
planning and learning algorithms allows these approaches to
be extended to more complex tasks, by integrating them with
sophisticated perception modules.

REFERENCES

[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” in NIPS Deep Learning Workshop, 2013.

[2] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” CoRR, vol. abs/1509.06461, 2015. [Online].
Available: http://arxiv.org/abs/1509.06461

[3] M. J. Hausknecht and P. Stone, “Deep recurrent q-learning for
partially observable mdps,” CoRR, vol. abs/1507.06527, 2015. [Online].
Available: http://arxiv.org/abs/1507.06527

[4] R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour, “Policy gradient
methods for reinforcement learning with function approximation,” in In
Advances in Neural Information Processing Systems 12. MIT Press,
2000, pp. 1057–1063.

learning,” in Proceedings of

IEEE Press, 1997, pp. 3557–3564.

[5] C. G. Atkeson and J. C. Santamar´ıa, “A comparison of direct
the
and model-based reinforcement
1997 IEEE International Conference on Robotics and Automation,
vol. 4.
[Online]. Available:
ftp://ftp.cc.gatech.edu/pub/people/cga/rl-compare.ps.gz
[6] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement

learning in
robotics: A survey,” I. J. Robotic Res., vol. 32, no. 11, pp. 1238–1274,
2013. [Online]. Available: http://dx.doi.org/10.1177/0278364913495721
[7] B. Bakker, V. Zhumatiy, G. Gruener, and J. Schmidhuber, “Quasi-online
reinforcement learning for robots,” in Proceedings of the 2006 IEEE
International Conference on Robotics and Automation ICRA, 2006.
[8] T. Hester, M. Quinlan, and P. Stone, “Generalized model learning for
reinforcement learning on a humanoid robot.” in ICRA.
IEEE, 2010,
pp. 2369–2374. [Online]. Available: http://dblp.uni-trier.de/db/conf/icra/
icra2010.html#HesterQS10

[9] J. Choi and K.-E. Kim, “Inverse reinforcement learning in partially
observable environments,” J. Mach. Learn. Res., vol. 12, pp. 691–
730, Jul. 2011. [Online]. Available: http://dl.acm.org/citation.cfm?id=
1953048.2021028

[10] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning,

1st ed. Cambridge, MA, USA: MIT Press, 1998.

[11] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-
forcement learning,” in In Proceedings of the Twenty-ﬁrst International
Conference on Machine Learning. ACM Press, 2004.

[12] A. Tamar, S. Levine, and P. Abbeel, “Value iteration networks,” CoRR,
vol. abs/1602.02867, 2016. [Online]. Available: http://arxiv.org/abs/
1602.02867

[13] A. C.

Ian Goodfellow, Yoshua Bengio, “Deep learning,” 2016,
[Online]. Available: http:

book in preparation for MIT Press.
//www.deeplearningbook.org

[14] L.-J. Lin, “Self-improving reactive agents based on reinforcement
learning, planning and teaching,” Machine Learning, vol. 8, no.
3–4, pp. 293–321, 1992. [Online]. Available: http://www.cs.ualberta.ca/
∼sutton/lin-92.pdf

[15] M. Kearns and S. Singh, “Near-optimal reinforcement

learning in
polynomial time,” Mach. Learn., vol. 49, no. 2-3, pp. 209–232, Nov.
2002. [Online]. Available: http://dx.doi.org/10.1023/A:1017984413808
[16] B. C. Stadie, S. Levine, and P. Abbeel, “Incentivizing exploration
in reinforcement learning with deep predictive models,” CoRR, vol.
abs/1507.00814, 2015. [Online]. Available: http://arxiv.org/abs/1507.
00814

[17] B. Bakker, “Reinforcement learning with long short-term memory,” in

In NIPS. MIT Press, 2002, pp. 1475–1482.

[18] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. P. Singh, “Action-conditional
video prediction using deep networks in atari games,” CoRR, vol.
abs/1507.08750, 2015. [Online]. Available: http://arxiv.org/abs/1507.
08750

[19] C. G. Atkeson,

“Nonparametric model-based

reinforcement
Information Processing Systems
[NIPS Conference, Denver, Colorado, USA, 1997], 1997,
http://papers.nips.cc/paper/
1008–1014.

learning,” in Advances in Neural
10,
pp.
1476-nonparametric-model-based-reinforcement-learning

[Online]. Available:

[20] S. Shalev-Shwartz, N. Ben-Zrihem, A. Cohen, and A. Shashua, “Long-
term planning by short-term prediction,” CoRR, vol. abs/1602.01580,
2016. [Online]. Available: http://arxiv.org/abs/1602.01580

Reinforcement Learning via Recurrent
Convolutional Neural Networks

Tanmay Shankar
Department of Mech. Engg.
IIT Guwahati
tanmay.shankar@gmail.com

Santosha K Dwivedy
Department of Mech. Engg.
IIT Guwahati
dwivedy@iitg.ac.in

Prithwijit Guha
Department of EEE
IIT Guwahati
pguha@iitg.ac.in

7
1
0
2
 
n
a
J
 
9
 
 
]

G
L
.
s
c
[
 
 
1
v
2
9
3
2
0
.
1
0
7
1
:
v
i
X
r
a

Abstract—Deep Reinforcement Learning has enabled the learn-
ing of policies for complex tasks in partially observable envi-
ronments, without explicitly learning the underlying model of
the tasks. While such model-free methods achieve considerable
performance, they often ignore the structure of task. We present a
natural representation of to Reinforcement Learning (RL) prob-
lems using Recurrent Convolutional Neural Networks (RCNNs),
to better exploit this inherent structure. We deﬁne 3 such RCNNs,
whose forward passes execute an efﬁcient Value Iteration, prop-
agate beliefs of state in partially observable environments, and
choose optimal actions respectively. Backpropagating gradients
through these RCNNs allows the system to explicitly learn the
Transition Model and Reward Function associated with the
underlying MDP, serving as an elegant alternative to classical
model-based RL. We evaluate the proposed algorithms in sim-
ulation, considering a robot planning problem. We demonstrate
the capability of our framework to reduce the cost of re-planning,
learn accurate MDP models, and ﬁnally re-plan with learnt
models to achieve near-optimal policies.

I. INTRODUCTION

techniques,

Deep Reinforcement Learning (DRL) algorithms exploit
model-free Reinforcement Learning (RL)
to
achieve high levels of performance on a variety of tasks, often
on par with human experts in the same domain [1]. These DRL
methods use deep networks to either approximate the action-
value functions as in Deep Q Networks [1]–[3], or directly
parametrizing the policy, as in policy gradient methods [4].
While DRL methods adopt model-free approaches in order
to generalize performance across various tasks, it is difﬁcult
to intuitively understand the reasoning of DRL approaches in
making a particular choice of action, since they often ignore
the underlying structure of the tasks.

In contrast, model-based methods [5]–[8] exploit this inher-
ent structure to make decisions, based on domain knowledge.
The estimates of the transition model and reward function
associated with the underlying Markov Decision Process
(MDP) are transferable across environments and agents [9],
and provide insight into the system’s choice of actions. A sig-
niﬁcant deterrent from model-based RL is the indirect nature
of learning with the estimated model; subsequent planning is
required to obtain the optimal policy [10].

To jointly exploit this inherent structure and overcome this
indirect nature, we present a novel fusion of Reinforcement
and Deep Learning, by representing classical solutions to RL

problems within the framework of Deep Learning architec-
tures; in particular Recurrent Convolutional Neural Networks
(RCNNs). By explicitly connecting the steps of solving MDPs
with architectural elements of RCNNs, our representation
inherits properties of such networks; allowing the use of
backpropagation on the deﬁned RCNNs as an elegant solution
to model-based RL problems. The representation also exploits
the inherent structure of the MDP to reduce the cost of
replanning, further incentivizing model-based approaches.

The contributions of this paper are hence three-fold. We
deﬁne a Value Iteration RCNN (VI RCNN), whose forward
passes carry out Value Iteration to efﬁciently obtain the
optimal policy. Second, we deﬁne a Belief Propagation RCNN
(BP RCNN), to update beliefs of state via the Bayes Filter.
Backpropagation through this network learns the underlying
transition model of the agent in partially observable envi-
ronments. Finally, we deﬁne a QMDP RCNN by combining
the VI RCNN and the BP RCNN. The QMDP RCNN com-
putes optimal choices of actions for a given belief of state.
Backpropagation through the QMDP RCNN learns the reward
function of an expert agent, in a Learning from Demonstration
via Inverse Reinforcement Learning (LfD-IRL) setting [11].
The learnt reward function and transition model may then be
used to re-plan via the VI RCNN, and the QMDP RCNN to
make optimal action choices for new beliefs and the learnt
Q-values. 1

We note that [12] follows an approach mathematically
similar to the VI RCNN; however it differs in being a model-
free approach. The model-based approach followed here learns
MDP models transferable across environments and agents.
Further, the QMDP RCNN approach proposed here follows
more naturally than the fully connected layer used in [12].
The gradient updates of the QMDP RCNN adopt an intuitive
form, that further contributes to an intuitive understanding of
the action choices of the QMDP RCNN, as compared to [12].
We evaluate each of the RCNNs proposed in simulation,
where the given task is a 2-D robot planning problem. We
demonstrate the capacity of the VI RCNN to reduce the cost
of re-planning by signiﬁcantly reducing the execution time
as compared to classical Value Iteration. We evaluate the BP

1Please ﬁnd our code and supplementary material at

https://

github.com/tanmayshankar/RCNN_MDP.

Fig. 1. Schematic representation of the Value Iteration Recurrent Convolution Network. Notice the 4 stages present- convolution, Fixed-bias,
Max-pooling and recurrence. The VI RCNN facilitates a natural representation of the Bellman update as a single RCNN layer.

RCNN based on the accuracy of the learnt transition models
against ground truth models, and show that it appreciably out-
performs naive model-based methods for partially observable
environments. Our experiments ﬁnally demonstrate that the
QMDP RCNN is able to generate policies via re-planning
with learnt MDP models, that accurately represent policies
generated from ground truth MDP models.

II. THE VALUE ITERATION RCNN

In this section, we formulate Value Iteration as a recurrent
convolution, and hence present the Value Iteration RCNN (VI
RCNN) to carry out Value Iteration. We consider a standard
Markov Decision Process, consisting of a 2-dimensional state
space S of size Nd × Nd, state s ∈ S, action space A
of size Na, actions a ∈ A, a transition model T (s, a, s(cid:48)),
reward function R(s, a), and discount factor γ. Value Iteration,
which is typically used in solving for optimal policies in an
MDP, invokes the Bellman Update equation as Vk+1(s) =
maxa R(s, a) + γ (cid:80)
s(cid:48) T (s, a, s(cid:48))Vk(s(cid:48)).
Value Iteration as a Convolution: In our 2-D state space, S,
states s and s(cid:48) are deﬁned by 2 pairs of indices (i, j) and (p, q).
The summation over all states s(cid:48) may hence be equivalently
represented as (cid:80)
(cid:80)Nd
q=1. The transition model is
hence of size N 2
d . We may express Vk+1(s) as
maxa R(s, a) + γ (cid:80)Nd

s(cid:48) = (cid:80)Nd
p=1
d × Na × N 2
(cid:80)Nd
p=1

q=1 T (s, a, s(cid:48))p,qVk(s(cid:48))p,q.

is impossible for the agent

Actions correspond to transitions in the agent’s state, dic-
tated by the internal dynamics of the agent, or in the case
of physical robots, restricted by the robot’s conﬁguration
to
and actuator capacities. It
immediately move from its current state to a far away state.
This allows us to deﬁne a w neighborhood centred around
the state s(cid:48), W (s(cid:48)), so the agent only has a ﬁnite probability
of transitioning to other states within W . Mathematically,
T (s, a, s(cid:48)) (cid:54)= 0 if |i − p| ≤ w and |j − q| ≤ w or if s ∈ W (s(cid:48)),
and T (s, a, s(cid:48)) = 0 for s (cid:54)∈ W (s(cid:48)).
the transition model

to the
location of the occurring transition. For example,
spatial
a differential drive robot’s dynamics are independent of
the robot’s position. We assume the transition model
is
stationary over
incorporating appropriate
state boundary conditions. We may now visualize this w
neighborhood restricted transition model
itself to be cen-
tred around s(cid:48). By deﬁning a ﬂipped transition model as:
T (s, a, s(cid:48))m,n = T (s, a, s(cid:48))Nt−m,Nt−n, and indexing it

is often invariant

the state space,

Further,

by (u, v), we may express Vk+1(s) as maxa R(s, a) +
γ (cid:80)w
v=−w T (s, a, s(cid:48))u,vVk(s(cid:48))i−u,j−v. We may rep-
resent this as a convolution:

(cid:80)w

u=−w

Vk+1(s) = max

R(s, a) + γ T (s, a, s(cid:48)) ∗ Vk(s(cid:48))

(1)

a

The Value Iteration RCNN: We deﬁne a Value Iteration
RCNN (VI RCNN) to represent classical Value Iteration, based
on the correlation between the Bellman update equation, and
the architectural elements of an RCNN. Equation (1) can be
thought of as a single layer of a recurrent convolutional neural
network consisting of the following 4 stages:
1) Convolutional Stage: The convolution of T (s, a, s(cid:48))∗Vk(s(cid:48))

represents the convolutional stage.

2) Max-Pooling Stage: The maximum at every state s taken
over all actions a, maxa is analogous to a max-pooling
stage along the action ‘channel’.

3) Fixed Bias Stage: The introduction of the reward term

R(s, a) is treated as addition of a ﬁxed bias.

4) Recurrence Stage: As k is incremented in successive iter-
ations, Vk+1(s) is fed back as an ‘input’ to the network,
introducing a recurrence relation into the network.
We may think of Vk(s(cid:48)) as a single-channel Nd ×Nd image.
T (s, a, s(cid:48)) then corresponds to a series of Na transition ﬁlters,
each of Nt × Nt size (Nt = 2w + 1), each to be convolved
with the image. The values of the transition ﬁlters correspond
directly to transition probabilities between states s and s(cid:48),
upon taking action a. Note that these convolutional transition
ﬁlters naturally capture the spatial invariance of transitions by
virtue of lateral parameter sharing inherent to convolutional
networks. Further, in this representation, each transition ﬁlter
corresponds directly to a particular action. This unique one-
to-one mapping between actions and ﬁlters proves to be very
useful in learning MDP or RL models, as we demonstrate in
section III. Finally, the VI RCNN is completely differentiable,
and can be trained by applying backpropagation through it.

III. THE BELIEF PROPAGATION RCNN
In this section, we present the Belief Propagation RCNN
(BP RCNN), to represent the Bayes ﬁlter belief update within
the architecture of an RCNN, and hence learn MDP transition
models. For an agent to make optimal choices of actions
in partially observable environments, an accurate belief of
state b(s) is required. Upon taking an action a and receiving
observation z, we may use the Bayes ﬁlter to propagate the

The Belief Propagation RCNN: We deﬁne a Belief Propaga-
tion RCNN (BP RCNN) to represent the Bayes Filter belief
update, analogous to the representation of Value Iteration as
the VI RCNN. A single layer of the BP RCNN represents
equation (3), consisting of the following stages:
1) Convolution Stage: The convolution T (s, a, s(cid:48)) ∗ b(s) rep-

resents the convolution stage of the BP RCNN.

2) Element-wise Product Stage: The element-wise multiplica-
tion O(sz) (cid:12) b(s(cid:48)), can be considered as an element-wise
product (or Hadamard product) layer in 2D.

3) Recurrence Stage: The network output, b(s(cid:48)), is fed as
the input b(s) at the next time step, forming an output
recurrence network.
Forward passes of the BP RCNN propagate beliefs of state,
given a choice of action at and a received observation zt.
The belief of state at any given instant, b(s), is treated as a
single channel Nd × Nd image, and is convolved with a single
transition ﬁlter Tat, corresponding to the action executed, at.
A key difference between the VI RCNN and the BP RCNN is
that only a single transition ﬁlter is activated during the belief
propagation, since the agent may only execute a single action
at any given time instant. The BP RCNN is also completely
differentiable.
Training and Loss: Learning the weights of the BP RCNN via
backpropagation learns the transition model of the agent. Our
objective is thus to learn a transition model T (cid:48)(s, a, s(cid:48)) such
that network output b(s(cid:48)) is as close to the target belief (cid:100)b(s(cid:48))
as possible, at every time step. Formally, we minimize a loss
function deﬁned as the least square error between both beliefs
over all states: Lt = (cid:80)Nd
i,j, with
conditions on the transition model, 0 ≤ T (cid:48)(s, a, s(cid:48))m,n ≤ 1,
and (cid:80)w
n=−w T (cid:48)(s, a, s(cid:48))m,n = 1 ∀ a ∈ A, being
incorporated using appropriate penalties.

t) − b(s(cid:48)

(cid:80)Nd
j=1

(cid:0)(cid:91)b(s(cid:48)

t)(cid:1)2

m=−w

(cid:80)w

i=1

The BP RCNN is trained in a typical RL setting, with
an agent interacting with an environment by taking random
action choices (at, at+1...), and receiving corresponding ob-
servations, (zt, zt+1...). The target beliefs, (cid:100)b(s(cid:48)), are gener-
ated as one-hot representations of the observed state. While
training the BP RCNN,
the randomly initialized transition
model magniﬁes the initial uncertainty in the belief as it is
propagated forward in time, leading to instability in learning
the transition model. Teacher forcing [13] is adopted to handle
this uncertainty; thus the target belief, rather than the network
output, is propagated as the input for the next time step. Since
the target belief is independent of the initial uncertainty of the
transition model, the network is able to learn a meaningful
set of ﬁlter values. Teacher forcing, or such target recurrence,
also decouples consecutive time steps, reducing the backprop-
agation through time to backpropagation over data points that
are only generated sequentially.

IV. THE QMDP RCNN

We ﬁnally present the QMDP RCNN as a combination of
the VI RCNN and the BP RCNN, to retrieve optimal choices
of actions, and learn the underlying reward function of the

Fig. 2. Schematic representation of the Belief Propagation Recurrent
Convolution Network. Forward passes represented by black arrows,
while backpropagation is symbolized by blue arrows.

belief b(s(cid:48)) in terms of an observation model O(s(cid:48), a, z),
associated with probability p(z|s(cid:48), a). The discrete Bayes ﬁlter
is depicted in (2). Note the denominator is equivalent
to
p(z|a, b), and can be implemented as a normalization factor
η.

b(s(cid:48)) =

O(s(cid:48), a, z) (cid:80)
s(cid:48)∈S O(s(cid:48), a, z) (cid:80)

(cid:80)

s∈S T (s, a, s(cid:48))b(s)

s∈S T (s, a, s(cid:48))b(s)

(2)

i=1

(cid:80)w

s as (cid:80)Nd

Bayes Filter as a Convolution and an Element-wise Product:
We may represent the motion update of the traditional Bayes
ﬁlter as a convolution, analogous to the representation of
(cid:80)
s(cid:48) T (s, a, s(cid:48))Vk(s(cid:48)) step of Value Iteration as T (s, a, s(cid:48)) ∗
Vk(s(cid:48)) in equation (1). Transitions in a Bayes ﬁlter occur
from s to s(cid:48), in contrast to the ‘backward’ passes executed
in Value Iteration, from s(cid:48) to s. This suggests our transition
model is centred around s rather than s(cid:48), and we may represent
(cid:80)
(cid:80)Nd
j=1. We write the intermediate belief b(s(cid:48))p,q
as (cid:80)w
v=−w T (s, a, s(cid:48))u,v b(s)p−u,q−v, which can be
expressed as a convolution b(s(cid:48)) = T (s, a, s(cid:48)) ∗ b(s). For
the Bayes Filter correction update, we consider a simple
observation model O(z, s(cid:48)) for observation z in state s(cid:48).
The agent observes its own state as sz
x,y, indexed by x and
y, given that the actual state of the robot is s(cid:48)
i,j, with a
probability p(sz
i,j). Analogous to the w neighborhood for
the transition model, we introduce a h neighborhood for the
observation model, centred around the observed state sz
x,y.
Thus O(sz
i,j within the h neighborhood
H(sz

x,y, s(cid:48)) (cid:54)= 0 for states s(cid:48)

x,y, s(cid:48)) = 0 for all s(cid:48)

x,y), and O(sz

i,j (cid:54)∈ H(sz

x,y|s(cid:48)

x,y).

u=−w

The correction update in the Bayes ﬁlter can be represented
as a masking operation on the intermediate belief b(s(cid:48)), or an
element-wise (Hadamard) product ((cid:12)). Since the observation
O(sz) is also centred around sz, we may express b(s(cid:48))i,j as
η O(sz)i−x,j−y b(s(cid:48))i,j ∀ i, j, which corresponds to a nor-
malized element-wise product b(s(cid:48)) = η O(sz) (cid:12) b(s(cid:48)). Upon
combining the motion update convolution and the correction
update element-wise product, we may represent the Bayes
Filter as a convolutional stage followed by an element-wise
product. We have:

b(s(cid:48)) = η O(sz) (cid:12) T (s, a, s(cid:48)) ∗ b(s)

(3)

Fig. 3. Schematic representation of the QMDP RCNN, as a combination of the Value Iteration RCNN and the Belief Propagation RCNN.
Notice the following stages: inner product, sum-pooling, and the softmax stage providing the output actions. The gradients propagated to
the reward function may alternately be propagated to the Q-value estimate. Forward passes of the QMDP RCNN are used for planning in
partially observable domains, and backpropagation learns the reward function of the MDP. The BP RCNN component can be trained in
parallel to the QMDP RCNN.

POMDP (in addition to the transition model). Treating the
outputs of the VI RCNN and the BP RCNN as the action-value
function, Q(s, a), and the belief of state, b(s) respectively
at every time step, we may fuse these outputs to compute
belief space Q-values Q(b(s), a) using the QMDP approxi-
mation. We have Q(b(s), a) = (cid:80)
s∈S Q(s, a)M DP b(s), which
may alternately be represented as a Frobenius Inner product
(cid:104)b(s), Q(s, a)(cid:105)F . Optimal action choices ya
t , corresponding to
the highest belief space Q-values, and can be retrieved from
the softmax of Q(b(s), a):

ya

t =

eQ(b(s),a)
a eQ(b(s),a)

(cid:80)

(4)

The QMDP RCNN: In addition to the stages of the VI RCNN
and BP RCNN, the QMDP RCNN is constructed from the
following stages:
1) Frobenius Inner Product Stage: The QMDP approximation
step Q(b(s), a) = (cid:104)b(s), Q(s, a)(cid:105)F . represents an Frobe-
nius inner product stage. This can be implemented as a
‘valid’ convolutional layer with zero stride.

2) Softmax Stage: ya

t = sof tmax Q(b(s), a) represents the

softmax stage of the network.
The resultant QMDP RCNN, as depicted in Figure 3,
combines planning and learning in a single network. Forward
passes of the network provide action choices ya
t as an output,
and backpropagating through the network learns the reward
function, and updates Q-values and optimal action choices via
planning through the VI RCNN component.
Training and Loss: The QMDP RCNN is trained in a Learning
from Demonstration - Inverse Reinforcement Learning setting.

An expert agent demonstrates a set of tasks, recorded as a
series of trajectories {(at, zt), (at+1, zt+1)...}, which serve
as inputs to the network. The actions executed in the expert
trajectories are represented in a one-hot encoding, serving as
the network targets (cid:98)ya
t . The objective is to hence learn a
reward function such that the action choices output by the
network match the target actions at every time step. The
loss function chosen is the cross-entropy loss between the
output actions and the target actions at a given time step,
deﬁned as Ct = − (cid:80)
t lnya
t . The QMDP RCNN learns
the reward function by backpropagating the gradients retrieved
from this loss function through the network, to update the
reward function. The updated reward estimate is then fed back
to the VI RCNN component to update the Q-values, and hence
the action choices of the network. Experience replay [14] is
used to randomize over the expert trajectories and transitions
while training the QMDP RCNN.

a (cid:98)ya

t − (cid:98)ya

t )b(s). The (ya

The closed form of the gradient for reward updates is of the
form of R(s, a) αt←− −(ya
t ) term thus
dictates the extent to which actions are positively or negatively
reinforced, while the belief term b(s) acts in a manner similar
to an attention mechanism, which directs the reward function
updates to speciﬁc regions of the state space where the agent
is believed to be.

t − (cid:98)ya

We emphasize that

the QMDP RCNN differs from tra-
ditional DRL approaches in that the QMDP RCNN is not
provided with samples of the reward function itself via its
interaction with the environment, rather, it is provided with
action choices made by the expert. While the LfD-IRL ap-
proach employed for the QMDP RCNN is on-policy, the in-

TABLE I
Transition Model Error and Replanning Policy Accuracy for BP RCNN.
Values in Bold represent best-in-class performance. Parameter values marked * are maintained during variation of alternate parameters, and are used during ﬁnal experiments.

Belief Propagation RCNN

Weighted Counting

Naive Counting

Parameter
Environment
Observability

Teacher Forcing

Learning Rate
Adaptation

Algorithm:
Type
Fully Observable
Partially Observable*
Output Recurrence
Target Recurrence*
RMSProp
Linear Decay
Filter-wise Decay*

Transition Error
0.0210
0.1133
3.5614
0.1133
2.8811
0.2418
0.1133

Replanning Accuracy
96.882 %
95.620 %
21.113 %
95.620 %
40.315 %
93.596 %
95.620 %

Transition Error
0.0249
1.0637
3.0831
1.0637
2.5703
1.3236
1.0637

Replanning Accuracy
96.564 %
75.571 %
27.239 %
75.571 %
44.092 %
52.451 %
75.571 %

Transition Error
0.0249
0.1840

Replanning Accuracy
96.564 %
83.590 %

No use of Recurrence.

No use of Learning Rate.

compared to the original policy. A comparison is provided
against the models learnt using a counting style algorithm
analogous to that used in [15], as well as a weighted-counting
style algorithm that updates model-estimates by counting over
belief values. We experiment with the performance of these
algorithms under both fully and partially observable settings,
and whether teacher forcing is used (via target recurrence) or
not (as in output recurrence). Different methods of adapting the
learning rate are explored, including RMSProp, linear decay,
and maintaining individual learning rates for each transition
ﬁlter, or ﬁlter-wise decay, as presented in Table I.

The BP RCNN’s loss function is deﬁned in terms of the
non-stationary beliefs at any given time instant. An online
training approach is hence rather than batch mode. We observe
the ﬁlter-wise decay outperforms linear decay when different
actions are chosen with varying frequencies. RMSProp suffers
from the oscillations in the loss function that arise due to this
dynamic nature, and hence performs poorly. The use of teacher
forcing mitigates this dynamic nature over time, increasing
replanning accuracy from 21.11% to 95.62%. The 95.62%
replanning accuracy attained under partial-observability ap-
proaches the 96.88% accuracy of the algorithm under fully
observable settings.
QMDP RCNN: For the QMDP RCNN, the objective is learn
a reward function that results in a similar policy and level
of performance as the original reward. Since learning such
a reward such that the demonstrated trajectories are optimal
does not have a unique solution, quantifying the accuracy of
the reward estimate is meaningless. Rather, we present the
replanning accuracy (as used in the BP RCNN) for both learnt
and known transition models and rewards. We also run policy
evaluation on the generated policy using the original rewards,
and present the increase in expected reward for the learnt
models; deﬁned as 1
. The results are
N 2
d
presented in Table II.

V (s)learnt−V (s)orig
V (s)orig

(cid:80)
s

As is the case in the BP RCNN, RMSProp (and adaptive
learning rates in general) counter the magnitude of reward
updates dictated by the QMDP RCNN, and hence adversely
affect performance. Experience Replay marginally increases
the replanning accuracy, but has a signiﬁcant effect on the
increase in expected reward. Similarly, using delayed feedback
(after passes through an entire trajectory) also boosts the
increase in expected reward. On learning both rewards and
transition models, the QMDP RCNN achieves an appreciable
65.120% policy error and a minimal −10.688% change in

Fig. 4. Plot of the time per iteration of Value Iteration versus size
of the State Space. The time taken by the VI RCNN is orders of
magnitude lesser than regular Value Iteration, for all transition space
sizes.

built planning (via the VI RCNN) causes reward updates to
permeate the entire state space. The dependence of the LfD-
IRL approach in the QMDP RCNN on expert-optimal actions
for training differs from the BP RCNN, which uses arbitrary
(and often suboptimal) choices of actions.

V. EXPERIMENTAL RESULTS AND DISCUSSIONS

In this section, experimental results are individually pre-

sented with respect to each of the 3 RCNNs deﬁned.
VI RCNN: Here, our contribution is a representation that
enables a more efﬁcient computation of Value Iteration. We
thus present the per-iteration run-time of Value Iteration via
the VI RCNN, versus a standard implementation of Value
Iteration. Both algorithms are implemented in Python, run on
an Intel Core i7 machine, 2.13 GHz, with 4-cores. The inherent
parallelization of the convolution facilitates the speed-up of
VI by several orders of magnitude, as depicted in Figure 4; at
best, the VI RCNN completes a single iteration 5106.42 times
faster, and at worst, it is 1704.43 times faster than the regular
implementation.
BP RCNN: The primary objective while training the BP
RCNN is to determine an accurate estimate of the transition
model. We evaluate performance using the least square error
between the transition model estimate and the ground truth
u,v)2. Since
model, deﬁned as C (cid:48)
a
the ﬁnal objective is to use these learnt models to generate
new policies by replanning via the VI RCNN, we also present
the replanning accuracy of each model; deﬁned as the per-
centage of actions chosen correctly by the network’s policy,

t = (cid:80)

u,a − T (cid:48)a

v (T a

(cid:80)
u

(cid:80)

TABLE II
Replanning Accuracy and Increase in Expected Reward for Inverse Reinforcement Learning from Demonstrations via the QMDP RCNN.
Values in Bold represent best-in-class performance. Parameter values marked * are maintained during variation of alternate parameters, and are used during ﬁnal experiments.

Algorithm:
Parameter:

Learning Rate

QMDP RCNN
Experience Replay

Environment:

Parameter Value:

Linear Decay*

None

Learnt Reward
Learnt Transition

Learnt Reward
Known Transition

Known Reward
Learnt Transition

Replanning Accuracy
Expected Reward Increase

Replanning Accuracy
Expected Reward Increase

Replanning Accuracy
Expected Reward Increase

RMSProp

45.113 %
-75.030 %

43.932 %
-79.601 %

65.120 %
-10.688 %

63.476 %
-11.429 %

62.211 %
-16.358 %

60.852 %
-16.106 %

Used*

65.120 %
-10.688 %

63.476 %
-11.429 %

Feedback Recurrence

Immediate

65.434 %
-16.063 %

63.900 %
-17.949 %

Delayed*

65.120 %
-10.688 %

63.476 %
-11.429 %

95.620 % (Known Reward used for Replanning)
+0.709 % (Known Reward used for Replanning)

Weighted
Counting

45.102 %
-74.030 %

49.192 %
-64.294 %

75.571 %
-0.617 %

Naive
Counting

50.192 %
-43.166 %

53.427 %
-31.812 %

83.590 %
-0.806 %

expected reward. We emphasize that given access solely to
observations and action choices of the agent, and without
assuming any feature engineering of the reward, the QMDP
RCNN is able to achieve near-optimal
levels of expected
reward. Utilizing the ground truth transition with the learnt
reward, the QMDP RCNN performs marginally worse, with
a 63.476% accuracy and a −11.429% change in expected
reward.

The true efﬁcacy of the BP RCNN and the QMDP RCNN
lie in their ability to learn accurate transition models and
reward functions in partially observable settings, where they
outperforms existing model-based approaches of naive and
in terms of re-
weighted counting by signiﬁcant margins,
planning accuracy,
transition errors, and expected reward.
Finally, we note that while all 3 deﬁned RCNN architectures
are demonstrated for 2-D cases, these architectures can be
extended to any number of dimensions, and number of actions,
with suitable modiﬁcations to the convolution operation in
higher dimensions.

VI. CONCLUSION

In this paper, we deﬁned 3 RCNN like architectures, namely
the Value Iteration RCNN, the Belief Propagation RCNN, and
the QMDP RCNN, to facilitate a more natural representation of
solutions to model-based Reinforcement Learning. Together,
these contributions speed up the planning process in a partially
observable environment, reducing the cost of replanning for
model-based approaches. Given access to agent observations
and action choices over time, the BP RCNN learns the transi-
tion model, and the QMDP RCNN learns the reward function,
and subsequently replans with learnt models to make near-
optimal action choices. The proposed architectures were also
found to outperform existing model-based approaches in speed
and model accuracy. The natural symbiotic representation of
planning and learning algorithms allows these approaches to
be extended to more complex tasks, by integrating them with
sophisticated perception modules.

REFERENCES

[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” in NIPS Deep Learning Workshop, 2013.

[2] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” CoRR, vol. abs/1509.06461, 2015. [Online].
Available: http://arxiv.org/abs/1509.06461

[3] M. J. Hausknecht and P. Stone, “Deep recurrent q-learning for
partially observable mdps,” CoRR, vol. abs/1507.06527, 2015. [Online].
Available: http://arxiv.org/abs/1507.06527

[4] R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour, “Policy gradient
methods for reinforcement learning with function approximation,” in In
Advances in Neural Information Processing Systems 12. MIT Press,
2000, pp. 1057–1063.

learning,” in Proceedings of

IEEE Press, 1997, pp. 3557–3564.

[5] C. G. Atkeson and J. C. Santamar´ıa, “A comparison of direct
the
and model-based reinforcement
1997 IEEE International Conference on Robotics and Automation,
vol. 4.
[Online]. Available:
ftp://ftp.cc.gatech.edu/pub/people/cga/rl-compare.ps.gz
[6] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement

learning in
robotics: A survey,” I. J. Robotic Res., vol. 32, no. 11, pp. 1238–1274,
2013. [Online]. Available: http://dx.doi.org/10.1177/0278364913495721
[7] B. Bakker, V. Zhumatiy, G. Gruener, and J. Schmidhuber, “Quasi-online
reinforcement learning for robots,” in Proceedings of the 2006 IEEE
International Conference on Robotics and Automation ICRA, 2006.
[8] T. Hester, M. Quinlan, and P. Stone, “Generalized model learning for
reinforcement learning on a humanoid robot.” in ICRA.
IEEE, 2010,
pp. 2369–2374. [Online]. Available: http://dblp.uni-trier.de/db/conf/icra/
icra2010.html#HesterQS10

[9] J. Choi and K.-E. Kim, “Inverse reinforcement learning in partially
observable environments,” J. Mach. Learn. Res., vol. 12, pp. 691–
730, Jul. 2011. [Online]. Available: http://dl.acm.org/citation.cfm?id=
1953048.2021028

[10] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning,

1st ed. Cambridge, MA, USA: MIT Press, 1998.

[11] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-
forcement learning,” in In Proceedings of the Twenty-ﬁrst International
Conference on Machine Learning. ACM Press, 2004.

[12] A. Tamar, S. Levine, and P. Abbeel, “Value iteration networks,” CoRR,
vol. abs/1602.02867, 2016. [Online]. Available: http://arxiv.org/abs/
1602.02867

[13] A. C.

Ian Goodfellow, Yoshua Bengio, “Deep learning,” 2016,
[Online]. Available: http:

book in preparation for MIT Press.
//www.deeplearningbook.org

[14] L.-J. Lin, “Self-improving reactive agents based on reinforcement
learning, planning and teaching,” Machine Learning, vol. 8, no.
3–4, pp. 293–321, 1992. [Online]. Available: http://www.cs.ualberta.ca/
∼sutton/lin-92.pdf

[15] M. Kearns and S. Singh, “Near-optimal reinforcement

learning in
polynomial time,” Mach. Learn., vol. 49, no. 2-3, pp. 209–232, Nov.
2002. [Online]. Available: http://dx.doi.org/10.1023/A:1017984413808
[16] B. C. Stadie, S. Levine, and P. Abbeel, “Incentivizing exploration
in reinforcement learning with deep predictive models,” CoRR, vol.
abs/1507.00814, 2015. [Online]. Available: http://arxiv.org/abs/1507.
00814

[17] B. Bakker, “Reinforcement learning with long short-term memory,” in

In NIPS. MIT Press, 2002, pp. 1475–1482.

[18] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. P. Singh, “Action-conditional
video prediction using deep networks in atari games,” CoRR, vol.
abs/1507.08750, 2015. [Online]. Available: http://arxiv.org/abs/1507.
08750

[19] C. G. Atkeson,

“Nonparametric model-based

reinforcement
Information Processing Systems
[NIPS Conference, Denver, Colorado, USA, 1997], 1997,
http://papers.nips.cc/paper/
1008–1014.

learning,” in Advances in Neural
10,
pp.
1476-nonparametric-model-based-reinforcement-learning

[Online]. Available:

[20] S. Shalev-Shwartz, N. Ben-Zrihem, A. Cohen, and A. Shashua, “Long-
term planning by short-term prediction,” CoRR, vol. abs/1602.01580,
2016. [Online]. Available: http://arxiv.org/abs/1602.01580

