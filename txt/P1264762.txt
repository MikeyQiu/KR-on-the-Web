Unsupervised Neural Machine Translation with Weight Sharing

Zhen Yang1,2, Wei Chen1 , Feng Wang1,2∗, Bo Xu1
1Institute of Automation, Chinese Academy of Sciences
2University of Chinese Academy of Sciences
{yangzhen2014, wei.chen.media, feng.wang, xubo}@ia.ac.cn

8
1
0
2
 
r
p
A
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
7
5
0
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Unsupervised neural machine translation
(NMT) is a recently proposed approach
for machine translation which aims to train
the model without using any labeled data.
The models proposed for unsupervised
NMT often use only one shared encoder
to map the pairs of sentences from dif-
ferent languages to a shared-latent space,
which is weak in keeping the unique
and internal characteristics of each lan-
guage, such as the style, terminology, and
sentence structure. To address this is-
sue, we introduce an extension by utiliz-
ing two independent encoders but shar-
ing some partial weights which are re-
sponsible for extracting high-level repre-
sentations of the input sentences. Be-
sides, two different generative adversarial
networks (GANs), namely the local GAN
and global GAN, are proposed to enhance
the cross-language translation. With this
new approach, we achieve signiﬁcant im-
provements on English-German, English-
French and Chinese-to-English translation
tasks.

1

Introduction

Neural machine translation (Kalchbrenner and
Blunsom, 2013; Sutskever et al., 2014; Cho et al.,
2014; Bahdanau et al., 2014), directly applying a
single neural network to transform the source sen-
tence into the target sentence, has now reached im-
pressive performance (Shen et al., 2015; Wu et al.,
2016; Johnson et al., 2016; Gehring et al., 2017;
Vaswani et al., 2017). The NMT typically con-
sists of two sub neural networks. The encoder net-
work reads and encodes the source sentence into a

1Feng Wang is the corresponding author of this paper

context vector, and the decoder network generates
the target sentence iteratively based on the con-
text vector. NMT can be studied in supervised and
unsupervised learning settings. In the supervised
setting, bilingual corpora is available for training
the NMT model. In the unsupervised setting, we
only have two independent monolingual corpora
with one for each language and there is no bilin-
gual training example to provide alignment infor-
mation for the two languages. Due to lack of align-
ment information, the unsupervised NMT is con-
sidered more challenging. However, this task is
very promising, since the monolingual corpora is
usually easy to be collected.

Motivated by recent success in unsupervised
cross-lingual embeddings (Artetxe et al., 2016;
Zhang et al., 2017b; Conneau et al., 2017), the
models proposed for unsupervised NMT often as-
sume that a pair of sentences from two different
languages can be mapped to a same latent repre-
sentation in a shared-latent space (Lample et al.,
2017; Artetxe et al., 2017b). Following this as-
sumption, Lample et al. (2017) use a single en-
coder and a single decoder for both the source and
target languages. The encoder and decoder, act-
ing as a standard auto-encoder (AE), are trained to
reconstruct the inputs. And Artetxe et al. (2017b)
utilize a shared encoder but two independent de-
coders. With some good performance, they share
a glaring defect, i.e., only one encoder is shared
by the source and target languages. Although the
shared encoder is vital for mapping sentences from
different languages into the shared-latent space,
it is weak in keeping the uniqueness and inter-
nal characteristics of each language, such as the
style, terminology and sentence structure. Since
each language has its own characteristics,
the
source and target languages should be encoded
and learned independently. Therefore, we conjec-
ture that the shared encoder may be a factor limit-

ing the potential translation performance.

In order to address this issue, we extend the
encoder-shared model, i.e., the model with one
shared encoder, by leveraging two independent
encoders with each for one language. Similarly,
two independent decoders are utilized. For each
language, the encoder and its corresponding de-
coder perform an AE, where the encoder gener-
ates the latent representations from the perturbed
input sentences and the decoder reconstructs the
sentences from the latent representations. To map
the latent representations from different languages
to a shared-latent space, we propose the weight-
sharing constraint to the two AEs. Speciﬁcally,
we share the weights of the last few layers of two
encoders that are responsible for extracting high-
level representations of input sentences. Simi-
larly, we share the weights of the ﬁrst few lay-
ers of two decoders. To enforce the shared-latent
space, the word embeddings are used as a rein-
forced encoding component in our encoders. For
cross-language translation, we utilize the back-
translation following (Lample et al., 2017). Addi-
tionally, two different generative adversarial net-
works (GAN) (Yang et al., 2017), namely the lo-
cal and global GAN, are proposed to further im-
prove the cross-language translation. We utilize
the local GAN to constrain the source and tar-
get latent representations to have the same distri-
bution, whereby the encoder tries to fool a local
discriminator which is simultaneously trained to
distinguish the language of a given latent repre-
sentation. We apply the global GAN to ﬁnetune
the corresponding generator, i.e., the composition
of the encoder and decoder of the other language,
where a global discriminator is leveraged to guide
the training of the generator by assessing how far
the generated sentence is from the true data distri-
bution 1. In summary, we mainly make the follow-
ing contributions:

• We propose the weight-sharing constraint to
unsupervised NMT, enabling the model to
utilize an independent encoder for each lan-
guage. To enforce the shared-latent space,
we also propose the embedding-reinforced
encoders and two different GANs for our
model.

• We

conduct

extensive

experiments on

1The

code

evaluate

and
https://github.com/ZhenYangIACAS/unsupervised-NMT

that

we
our models

utilized
be
can

to
found

train
at

English-German,
English-French
and
Ex-
Chinese-to-English translation tasks.
perimental results show that the proposed
approach consistently achieves great success.

• Last but not least, we introduce the direc-
tional self-attention to model temporal order
information for the proposed model. Exper-
imental results reveal that it deserves more
efforts for researchers to investigate the tem-
poral order information within self-attention
layers of NMT.

2 Related Work

Several approaches have been proposed to train
NMT models without direct parallel corpora. The
scenario that has been widely investigated is one
where two languages have little parallel data be-
tween them but are well connected by one pivot
language. The most typical approach in this sce-
nario is to independently translate from the source
language to the pivot language and from the pivot
language to the target language (Saha et al., 2016;
Cheng et al., 2017). To improve the transla-
tion performance, Johnson et al. (2016) propose a
multilingual extension of a standard NMT model
and they achieve substantial improvement for lan-
guage pairs without direct parallel training data.

Recently, motivated by the success of cross-
lingual embeddings, researchers begin to show in-
terests in exploring the more ambitious scenario
where an NMT model is trained from monolingual
corpora only. Lample et al. (2017) and Artetxe
et al. (2017b) simultaneously propose an approach
for this scenario, which is based on pre-trained
cross lingual embeddings. Lample et al. (2017)
utilizes a single encoder and a single decoder for
both languages. The entire system is trained to
reconstruct its perturbed input. For cross-lingual
translation, they incorporate back-translation into
the training procedure. Different from (Lample
et al., 2017), Artetxe et al. (2017b) use two in-
dependent decoders with each for one language.
The two works mentioned above both use a sin-
gle shared encoder to guarantee the shared latent
space. However, a concomitant defect is that the
shared encoder is weak in keeping the uniqueness
of each language. Our work also belongs to this
more ambitious scenario, and to the best of our
knowledge, we are one among the ﬁrst endeav-
ors to investigate how to train an NMT model with
monolingual corpora only.

Figure 1: The architecture of the proposed model. We implement the shared-latent space assumption
using a weight sharing constraint where the connection of the last few layers in Encs and Enct are
tied (illustrated with dashed lines) and the connection of the ﬁrst few layers in Decs and Dect are
tied. ˜xEncs−Decs
is
s
the translated sentence from source to target and ˜xEnct−Decs
is the translation in reversed direction.
t
Dl is utilized to assess whether the hidden representation of the encoder is from the source or target
language. Dg1 and Dg2 are used to evaluate whether the translated sentences are realistic for each
language respectively. Z represents the shared-latent space.

are self-reconstructed sentences in each language. ˜xEncs−Dect

and ˜xEnct−Dect

s

t

3 The Approach

3.1 Model Architecture

The model architecture, as illustrated in ﬁgure 1,
is based on the AE and GAN. It consists of seven
sub networks: including two encoders Encs and
Enct, two decoders Decs and Dect, the local dis-
criminator Dl, and the global discriminators Dg1
and Dg2. For the encoder and decoder, we follow
the newly emerged Transformer (Vaswani et al.,
2017). Speciﬁcally, the encoder is composed of a
stack of four identical layers 2. Each layer con-
sists of a multi-head self-attention and a simple
position-wise fully connected feed-forward net-
work. The decoder is also composed of four iden-
tical layers. In addition to the two sub-layers in
each encoder layer, the decoder inserts a third sub-
layer, which performs multi-head attention over
the output of the encoder stack. For more details
about the multi-head self-attention layer, we refer
the reader to (Vaswani et al., 2017). We implement
the local discriminator as a multi-layer perceptron
and implement the global discriminator based on
the convolutional neural network (CNN). Several
ways exist to interpret the roles of the sub net-
works are summarised in table 1. The proposed
system has several striking components , which
are critical either for the system to be trained in an

2The layer number is selected according to our prelimi-

nary experiment, which is presented in appendix A.

unsupervised manner or for improving the transla-
tion performance.

Networks

Roles

{Encs, Decs}
{Enct, Dect}
{Encs, Dect}
{Enct, Decs}
{Encs, Dl}
{Enct, Dl}
{Enct, Decs, Dg1}
{Encs, Dect, Dg2}

AE for source language
AE for target language
translation source → target
translation target → source
1st local GAN (GANl1)
2nd local GAN (GANl2)
1st global GAN (GANg1)
2nd global GAN (GANg2)

Table 1: Interpretation of the roles for the subnet-
works in the proposed system.

Directional self-attention Compared to recur-
rent neural network, a disadvantage of the simple
self-attention mechanism is that the temporal or-
der information is lost. Although the Transformer
applies the positional encoding to the sequence be-
fore processed by the self-attention, how to model
temporal order information within an attention is
still an open question. Following (Shen et al.,
2017), we build the encoders in our model on the
directional self-attention which utilizes the posi-
tional masks to encode temporal order information
into attention output. More concretely, two posi-
tional masks, namely the forward mask M f and

backward mask M b, are calculated as:

(cid:26)

(cid:26)

M f

ij =

M b

ij =

−∞ otherwise

0

0

i < j

i > j

−∞ otherwise

(1)

(2)

With the forward mask M f , the later token only
makes attention connections to the early tokens
in the sequence, and vice versa with the back-
ward mask. Similar to (Zhou et al., 2016; Wang
et al., 2017), we utilize a self-attention network
to process the input sequence in forward direc-
tion. The output of this layer is taken by an upper
self-attention network as input, processed in the
reverse direction.

Weight sharing Based on the shared-latent
space assumption, we apply the weight sharing
constraint to relate the two AEs. Speciﬁcally, we
share the weights of the last few layers of the Encs
and Enct, which are responsible for extracting
high-level representations of the input sentences.
Similarly, we also share the ﬁrst few layers of
the Decs and Dect, which are expected to de-
code high-level representations that are vital for
reconstructing the input sentences. Compared to
(Cheng et al., 2016; Saha et al., 2016) which use
the fully shared encoder, we only share partial
weights for the encoders and decoders. In the pro-
posed model, the independent weights of the two
encoders are expected to learn and encode the hid-
den features about the internal characteristics of
each language, such as the terminology, style, and
sentence structure. The shared weights are utilized
to map the hidden features extracted by the inde-
pendent weights to the shared-latent space.

Embedding reinforced encoder We use pre-
trained cross-lingual embeddings in the encoders
that are kept ﬁxed during training. And the
ﬁxed embeddings are used as a reinforced en-
coding component
Formally,
given the input sequence embedding vectors E =
{e1, . . . , et} and the initial output sequence of the
encoder stack H = {h1, . . . , ht}, we compute Hr
as:

in our encoder.

Hr = g (cid:12) H + (1 − g) (cid:12) E

(3)

where Hr is the ﬁnal output sequence of the en-
coder which will be attended by the decoder (In
Transformer, H is the ﬁnal output of the encoder),
g is a gate unit and computed as:

g = σ(W1E + W2H + b)

(4)

where W1, W2 and b are trainable parameters
and they are shared by the two encoders. The
motivation behind is twofold. Firstly, taking the
ﬁxed cross-lingual embedding as the other encod-
ing component is helpful to reinforce the shared-
latent space. Additionally, from the point of multi-
channel encoders (Xiong et al., 2017), provid-
ing encoding components with different levels of
composition enables the decoder to take pieces of
source sentence at varying composition levels suit-
ing its own linguistic structure.

3.2 Unsupervised Training

Based on the architecture proposed above, we train
the NMT model with the monolingual corpora
only using the following four strategies:

Denoising auto-encoding Firstly, we train the
two AEs to reconstruct their inputs respectively.
In this form, each encoder should learn to com-
pose the embeddings of its corresponding lan-
guage and each decoder is expected to learn to de-
compose this representation into its corresponding
language. Nevertheless, without any constraint,
the AE quickly learns to merely copy every word
one by one, without capturing any internal struc-
ture of the language involved. To address this
problem, we utilize the same strategy of denois-
ing AE (Vincent et al., 2008) and add some noise
to the input sentences (Hill et al., 2016; Artetxe
et al., 2017b). To this end, we shufﬂe the input
sentences randomly. Speciﬁcally, we apply a ran-
dom permutation ε to the input sentence, verifying
the condition:

steps
s

|ε(i) − i| ≤ min(k([

] + 1), n), ∀i ∈ {1, n}
(5)
where n is the length of the input sentence, steps
is the global steps the model has been updated, k
and s are the tunable parameters which can be set
by users beforehand. This way, the system needs
to learn some useful structure of the involved lan-
guages to be able to recover the correct word order.
In practice, we set k = 2 and s = 100000.

Back-translation In spite of denoising auto-
encoding, the training procedure still involves a
single language at each time, without considering
our ﬁnal goal of mapping an input sentence from
the source/target language to the target/source lan-
guage. For the cross language training, we uti-
lize the back-translation approach for our unsu-
pervised training procedure. Back-translation has
shown its great effectiveness on improving NMT

model with monolingual data and has been widely
investigated by (Sennrich et al., 2015a; Zhang and
Zong, 2016).
In our approach, given an input
sentence in a given language, we apply the cor-
responding encoder and the decoder of the other
language to translate it to the other language 3.
By combining the translation with its original sen-
tence, we get a pseudo-parallel corpus which is
utilized to train the model to reconstruct the origi-
nal sentence from its translation.

Local GAN Although the weight sharing con-
straint is vital for the shared-latent space assump-
tion, it alone does not guarantee that the corre-
sponding sentences in two languages will have the
same or similar latent code. To further enforce
the shared-latent space, we train a discriminative
neural network, referred to as the local discrimi-
nator, to classify between the encoding of source
sentences and the encoding of target sentences.
The local discriminator, implemented as a multi-
layer perceptron with two hidden layers of size
256, takes the output of the encoder, i.e., Hr calcu-
lated as equation 3, as input, and produces a binary
prediction about the language of the input sen-
tence. The local discriminator is trained to predict
the language by minimizing the following cross-
entropy loss:

LDl(θDl) =
− Ex∈xs[log p(f = s|Encs(x))]
− Ex∈xt[log p(f = t|Enct(x))]

(6)

where θDl represents the parameters of the local
discriminator and f ∈ {s, t}. The encoders are
trained to fool the local discriminator:

LEncs(θEncs) =
− Ex∈xs[log p(f = t|Encs(x))]

(7)

LEnct(θEnct) =
− Ex∈xt[log p(f = s|Enct(x))]
where θEncs and θEnct are the parameters of the
two encoders.

(8)

Global GAN We apply the global GANs to
ﬁne tune the whole model so that the model is
able to generate sentences undistinguishable from
the true data, i.e., sentences in the training cor-
pus. Different from the local GANs which up-
dates the parameters of the encoders locally, the

global GANs are utilized to update the whole pa-
including the
rameters of the proposed model,
parameters of encoders and decoders. The pro-
posed model has two global GANs: GANg1 and
In GANg1, the Enct and Decs act as
GANg2.
4
the generator, which generates the sentence ˜xt
from xt. The Dg1, implemented based on CNN,
assesses whether the generated sentence ˜xt is the
true target-language sentence or the generated sen-
tence. The global discriminator aims to distin-
guish among the true sentences and generated sen-
tences, and it is trained to minimize its classiﬁ-
cation error rate. During training, the Dg1 feeds
back its assessment to ﬁnetune the encoder Enct
and decoder Decs. Since the machine transla-
tion is a sequence generation problem, following
(Yang et al., 2017), we leverage policy gradient re-
inforcement training to back-propagate the assess-
ment. We apply a similar processing to GANg2
(The details about the architecture of the global
discriminator and the training procedure of the
global GANs can be seen in appendix B and C).

There are two stages in the proposed unsuper-
vised training. In the ﬁrst stage, we train the pro-
posed model with denoising auto-encoding, back-
translation and the local GANs, until no improve-
ment is achieved on the development set. Specif-
ically, we perform one batch of denoising auto-
encoding for the source and target languages, one
batch of back-translation for the two languages,
and another batch of local GAN for the two lan-
guages. In the second stage, we ﬁne tune the pro-
posed model with the global GANs.

4 Experiments and Results

We evaluate the proposed approach on English-
German, English-French and Chinese-to-English
translation tasks 5. We ﬁrstly describe the datasets,
pre-processing and model hyper-parameters we
used, then we introduce the baseline systems, and
ﬁnally we present our experimental results.

4.1 Data Sets and Preprocessing

In English-German and English-French transla-
tion, we make our experiments comparable with
previous work by using the datasets from the

4The ˜xt is ˜xEnct−Decs
t

script for simplicity.

in ﬁgure 1. We omit the super-

3Since the quality of the translation shows little effect on
the performance of the model (Sennrich et al., 2015a), we
simply use greedy decoding for speed.

5The reason that we do not conduct experiments on
English-to-Chinese translation is that we do not get public
test sets for English-to-Chinese.

WMT 2014 and WMT 2016 shared tasks respec-
tively. For Chinese-to-English translation, we use
the datasets from LDC, which has been widely uti-
lized by previous works (Tu et al., 2017; Zhang
et al., 2017a).

WMT14 English-French Similar to (Lample
et al., 2017), we use the full training set of 36M
sentence pairs and we lower-case them and re-
move sentences longer than 50 words, resulting
in a parallel corpus of about 30M pairs of sen-
tences. To guarantee no exact correspondence be-
tween the source and target monolingual sets, we
build monolingual corpora by selecting English
sentences from 15M random pairs, and selecting
the French sentences from the complementary set.
Sentences are encoded with byte-pair encoding
(Sennrich et al., 2015b), which has an English vo-
cabulary of about 32000 tokens, and French vo-
cabulary of about 33000 tokens. We report results
on newstest2014.

WMT16 English-German We follow the same
procedure mentioned above to create monolingual
training corpora for English-German translation,
and we get two monolingual training data of 1.8M
sentences each. The two languages share a vocab-
ulary of about 32000 tokens. We report results on
newstest2016.

LDC Chinese-English For Chinese-to-English
translation, our training data consists of 1.6M sen-
tence pairs randomly extracted from LDC corpora
6. Since the data set is not big enough, we just
build the monolingual data set by randomly shuf-
ﬂing the Chinese and English sentences respec-
tively.
In spite of the fact that some correspon-
dence between examples in these two monolingual
sets may exist, we never utilize this alignment in-
formation in our training procedure (see Section
3.2). Both the Chinese and English sentences are
encoded with byte-pair encoding. We get an En-
glish vocabulary of about 34000 tokens, and Chi-
nese vocabulary of about 38000 tokens. The re-
sults are reported on N IST 02.

Since the proposed system relies on the pre-
trained cross-lingual embeddings, we utilize the
monolingual corpora described above to train the
embeddings for each language independently by
using word2vec (Mikolov et al., 2013). We then
apply the public implementation 7 of the method
proposed by (Artetxe et al., 2017a) to map these

6LDC2002L27,

LDC2002E18,
LDC2003E07, LDC2004T08, LDC2004E12, LDC2005T10

LDC2002T01,

7https://github.com/artetxem/vecmap

embeddings to a shared-latent space 8.

4.2 Model Hyper-parameters and Evaluation

Following the base model
in (Vaswani et al.,
2017), we set the dimension of word embedding
as 512, dropout rate as 0.1 and the head number
as 8. We use beam search with a beam size of 4
and length penalty α = 0.6. The model is im-
plemented in TensorFlow (Abadi et al., 2015) and
trained on up to four K80 GPUs synchronously in
a multi-GPU setup on a single machine.

For model selection, we stop training when the
model achieves no improvement for the tenth eval-
uation on the development set, which is com-
prised of 3000 source and target sentences ex-
tracted randomly from the monolingual training
corpora. Following (Lample et al., 2017), we
translate the source sentences to the target lan-
guage, and then translate the resulting sentences
back to the source language. The quality of the
model is then evaluated by computing the BLEU
score over the original inputs and their reconstruc-
tions via this two-step translation process. The
performance is ﬁnally averaged over two direc-
tions, i.e., from source to target and from target
to source. BLEU (Papineni et al., 2002) is utilized
as the evaluation metric. For Chinese-to-English,
we apply the script mteval-v11b.pl to evaluate the
translation performance. For English-German and
English-French, we evaluate the translation per-
formance with the script multi-belu.pl 9.

4.3 Baseline Systems

Word-by-word translation (WBW) The ﬁrst
is a system that per-
baseline we consider
forms word-by-word translations using the in-
ferred bilingual dictionary. Speciﬁcally, it trans-
lates a sentence word-by-word, replacing each
word with its nearest neighbor in the other lan-
guage.

Lample et al. (2017) The second baseline is
a previous work that uses the same training and
testing sets with this paper. Their model belongs
to the standard attention-based encoder-decoder
framework, which implements the encoder using
a bidirectional long short term memory network
(LSTM) and implements the decoder using a sim-

8The conﬁguration we used to run these open-source

toolkits can be found in appendix D
9https://github.com/moses-

smt/mosesdecoder/blob/617e8c8/scripts/generic/multi-
bleu.perl;mteval-v11b.pl

en-de

de-en

en-fr

fr-en

zh-en

Supervised
Word-by-word
Lample et al. (2017)

24.07
5.85
9.64

26.99
9.34
13.33

30.50
3.60
15.05

30.21
6.80
14.31

40.02
5.09
-

The proposed approach 10.86

14.62

16.97

15.58

14.52

Table 2: The translation performance on English-German, English-French and Chinese-to-English test
sets. The results of (Lample et al., 2017) are copied directly from their paper. We do not present the
results of (Artetxe et al., 2017b) since we use different training sets.

ple forward LSTM. They apply one single encoder
and decoder for the source and target languages.

Supervised training We ﬁnally consider ex-
actly the same model as ours, but trained using the
standard cross-entropy loss on the original parallel
sentences. This model can be viewed as an upper
bound for the proposed unsupervised model.

4.4 Results and Analysis

4.4.1 Number of weight-sharing layers

We ﬁrstly investigate how the number of weight-
sharing layers affects the translation performance.
In this experiment, we vary the number of weight-
sharing layers in the AEs from 0 to 4. Shar-
ing one layer in AEs means sharing one layer
for the encoders and in the meanwhile, shar-
ing one layer for the decoders.
The BLEU
scores of English-to-German, English-to-French
and Chinese-to-English translation tasks are re-
ported in ﬁgure 2. Each curve corresponds to a
different translation task and the x-axis denotes
the number of weight-sharing layers for the AEs.
We ﬁnd that the number of weight-sharing layers
shows much effect on the translation performance.
And the best translation performance is achieved
when only one layer is shared in our system. When
all of the four layers are shared, i.e., only one
shared encoder is utilized, we get poor translation
performance in all of the three translation tasks.
This veriﬁes our conjecture that the shared en-
coder is detrimental to the performance of unsu-
pervised NMT especially for the translation tasks
on distant language pairs. More concretely, for the
related language pair translation, i.e., English-to-
French, the encoder-shared model achieves -0.53
BLEU points decline than the best model where
only one layer is shared. For the more distant lan-
guage pair English-to-German, the encoder-shared
model achieves more signiﬁcant decline, i.e., -0.85
BLEU points decline. And for the most distant

language pair Chinese-to-English, the decline is
as large as -1.66 BLEU points. We explain this as
that the more distant the language pair is, the more
different characteristics they have. And the shared
encoder is weak in keeping the unique characteris-
tic of each language. Additionally, we also notice
that using two completely independent encoders,
i.e., setting the number of weight-sharing layers
as 0, results in poor translation performance too.
This conﬁrms our intuition that the shared layers
are vital to map the source and target latent rep-
resentations to a shared-latent space. In the rest
of our experiments, we set the number of weight-
sharing layer as 1.

Figure 2: The effects of the weight-sharing layer
number on English-to-German, English-to-French
and Chinese-to-English translation tasks.

4.4.2 Translation results

Table 2 shows the BLEU scores on English-
German, English-French and English-to-Chinese
test sets. As it can be seen, the proposed ap-
proach obtains signiﬁcant improvements than the
word-by-word baseline system, with at least +5.01
BLEU points in English-to-German translation
and up to +13.37 BLEU points in English-to-
French translation. This shows that the proposed

en-de

de-en

en-fr

fr-en

zh-en

Without weight sharing
Without embedding-reinforced encoder
Without directional self-attention
Without local GANs
Without Global GANs
Full model

10.23
10.45
10.60
10.51
10.34
10.86

13.84
14.17
14.21
14.35
14.05
14.62

16.02
16.55
16.82
16.40
16.19
16.97

14.82
15.27
15.30
15.07
15.21
15.58

13.75
14.10
14.29
14.12
14.09
14.52

Table 3: Ablation study on English-German, English-French and Chinese-to-English translation tasks.
Without weight sharing means no layers are shared in the two AEs.

model only trained with monolingual data effec-
tively learns to use the context information and
the internal structure of each language. Com-
pared to the work of (Lample et al., 2017), our
model also achieves up to +1.92 BLEU points im-
provement on English-to-French translation task.
We believe that the unsupervised NMT is very
promising. However, there is still a large room
for improvement compared to the supervised up-
per bound. The gap between the supervised and
unsupervised model is as large as 12.3-25.5 BLEU
points depending on the language pair and transla-
tion direction.

4.4.3 Ablation study
To understand the importance of different com-
ponents of the proposed system, we perform an
ablation study by training multiple versions of
the
our model with some missing components:
local GANs,
the directional
the global GANs,
self-attention, the weight-sharing, the embedding-
reinforced encoders, etc. Results are reported
in table 3. We do not test the the importance
of the auto-encoding, back-translation and the
pre-trained embeddings because they have been
widely tested in (Lample et al., 2017; Artetxe
et al., 2017b). Table 3 shows that the best per-
formance is obtained with the simultaneous use of
all the tested elements. The most critical compo-
nent is the weight-sharing constraint, which is vi-
tal to map sentences of different languages to the
shared-latent space. The embedding-reinforced
encoder also brings some improvement on all of
the translation tasks. When we remove the di-
rectional self-attention, we get up to -0.3 BLEU
points decline. This indicates that it deserves more
efforts to investigate the temporal order informa-
tion in self-attention mechanism. The GANs also
signiﬁcantly improve the translation performance
the global GANs
of our system. Speciﬁcally,

achieve improvement up to +0.78 BLEU points on
English-to-French translation and the local GANs
also obtain improvement up to +0.57 BLEU points
on English-to-French translation. This reveals that
the proposed model beneﬁts a lot from the cross-
domain loss deﬁned by GANs.

5 Conclusion and Future work

The models proposed recently for unsupervised
NMT use a single encoder to map sentences from
different languages to a shared-latent space. We
conjecture that the shared encoder is problem-
atic for keeping the unique and inherent char-
acteristic of each language.
In this paper, we
propose the weight-sharing constraint in unsuper-
vised NMT to address this issue. To enhance the
cross-language translation performance, we also
propose the embedding-reinforced encoders, local
GAN and global GAN into the proposed system.
Additionally, the directional self-attention is intro-
duced to model the temporal order information for
our system.
We test

the proposed model on English-
German, English-French and Chinese-to-English
translation tasks. The experimental results reveal
that our approach achieves signiﬁcant improve-
ment and verify our conjecture that the shared en-
coder is really a bottleneck for improving the un-
supervised NMT. The ablation study shows that
each component of our system achieves some im-
provement for the ﬁnal translation performance.

Unsupervised NMT opens exciting opportuni-
ties for the future research. However, there is
still a large room for improvement compared to
the supervised NMT. In the future, we would like
to investigate how to utilize the monolingual data
more effectively, such as incorporating the lan-
guage model and syntactic information into unsu-
pervised NMT. Besides, we decide to make more

efforts to explore how to reinforce the temporal or-
der information for the proposed model.

International Joint Conference on Artiﬁcial Intelli-
gence. pages 3974–3980.

Acknowledgements

This work is supported by the National Key Re-
search and Development Program of China un-
der Grant No. 2017YFB1002102, and Beijing
Engineering Research Center under Grant No.
Z171100002217015. We would like to thank Xu
Shuang for her preparing data used in this work.
Additionally, we also want to thank Jiaming Xu,
Suncong Zheng and Wenfu Wang for their invalu-
able discussions on this work.

References

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Man´e, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Vi´egas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems .

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Conference on Empirical Methods in Natural
Language Processing. pages 2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2017a. Learning bilingual word embeddings with
(almost) no bilingual data. In Meeting of the Asso-
ciation for Computational Linguistics. pages 451–
462.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2017b. Unsupervised neural ma-
chine translation .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473 .

Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, and
Wei Xu. 2016. Neural machine translation with
pivot languages. arXiv preprint arXiv:1611.04928
.

Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, Wei
Xu, Yong Cheng, Qian Yang, Yang Liu, Maosong
Joint training for pivot-
Sun, and Wei Xu. 2017.
In Twenty-Sixth
based neural machine translation.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Learning
Schwenk, and Yoshua Bengio. 2014.
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Herv Jgou. 2017.
Word translation without parallel data .

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning .

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. TACL .

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vi´egas, Martin Wattenberg, Greg Corrado,
et al. 2016. Google’s multilingual neural machine
translation system: Enabling zero-shot translation.
arXiv preprint arXiv:1611.04558 .

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. EMNLP pages
1700–1709.

Guillaume

Ludovic Denoyer,

and
Lample,
Marc’Aurelio Ranzato. 2017.
Unsupervised
machine translation using monolingual corpora only
.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems. pages 3111–3119.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. Association for Com-
putational Linguistics pages 311–318.

Amrita Saha, Mitesh M Khapra, Sarath Chandar, Ja-
narthanan Rajendran, and Kyunghyun Cho. 2016.
A correlational encoder decoder architecture for
arXiv preprint
pivot based sequence generation.
arXiv:1606.04754 .

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation
arXiv preprint

2015a.
models with monolingual data.
arXiv:1511.06709 .

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015b. Neural machine translation of rare words
with subword units. Computer Science .

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. arXiv
preprint arXiv:1512.02433 .

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding .

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. Advances in neural information processing
systems pages 3104–3112.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2017. Learning to remember translation his-
tory with a continuous cache .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need .

Pascal Vincent, Hugo Larochelle, Yoshua Bengio,
and Pierre-Antoine Manzagol. 2008. Extracting
and composing robust features with denoising au-
In Proceedings of the 25th interna-
toencoders.
tional conference on Machine learning. ACM, pages
1096–1103.

Mingxuan Wang, Zhengdong Lu,

Jie Zhou, and
Qun Liu. 2017. Deep neural machine transla-
arXiv preprint
tion with linear associative unit.
arXiv:1705.00861 .

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016.
Google’s neural ma-
chine translation system: Bridging the gap between
arXiv preprint
human and machine translation.
arXiv:1609.08144 .

bilingual lexicon induction. In Meeting of the Asso-
ciation for Computational Linguistics. pages 1959–
1970.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei
Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. arXiv
preprint arXiv:1606.04199 .

A Experiments on the layer number for

encoders and decoders

To determine the number of layers for encoders
and decoders in our system beforehand, we con-
duct experiments on English-German translation
tasks to test how the amount of layers in encoders
and decoders affects the translation performance.
We vary the number of layers from 2 to 6 and the
results are reported in table 4. We can ﬁnd that the
translation performance achieves substantial im-
provement with the layer number increasing from
2 to 4. However, with layer number set larger than
4, we get little improvement. To make a trade-off
between the translation performance and the com-
putation complexity, we set the layer number as 4
for our encoders and decoders.

layer num en-de

de-en

2
3
4
5
6

11.57
12.43
12.86
12.91
12.95

14.01
14.99
15.62
15.83
15.79

Table 4: The experiments on the number of layers
for encoders and decoders.

Hao Xiong, Zhongjun He, Xiaoguang Hu, and Hua Wu.
2017. Multi-channel encoder for neural machine
translation. arXiv preprint arXiv:1712.02109 .

B The architecture of the global

discriminator

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2017.
Improving neural machine translation with condi-
tional sequence generative adversarial nets .

Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu,
and Maosong Sun. 2017a. Prior knowledge integra-
tion for neural machine translation using posterior
In Meeting of the Association for
regularization.
Computational Linguistics. pages 1514–1523.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Conference on Empirical Methods in
Natural Language Processing. pages 1535–1545.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Adversarial training for unsupervised

The global discriminator is applied to classify the
generated sentences as source language, target lan-
guage or generated sentences. Following (Yang
et al., 2017), we implement the global discrimina-
tor based on CNN. Since sentences generated by
the generator (the composition of the encoder and
decoder) have variable lengths, the CNN padding
is used to transform the sentences to sequences
with ﬁxed length T , which is the maximum length
set for the output of the generator. Given the gen-
erated sequences x1, . . . , xT , we build the matrix
X1:T as:

X1:T = x1; x2; . . . ; xT

(9)

where xt ∈ Rk is the k-dimensional word embed-
ding and the semicolon is the concatenation oper-
ator. For the matrix X1:T , a kernel wj ∈ Rl×k
applies a convolutional operation to a window size
of l words to produce a series of feature maps:

cji = ρ(BN (wj ⊗ Xi:i+l−1 + b))

(10)

where ⊗ operator is the summation of element-
wise production and b is a bias term. ρ is a non-
linear activation function which is implemented as
ReLu in this paper. To get the ﬁnal feature with
respect to kernel wj, a max-over-time pooling op-
eration is leveraged over the feature maps:

(cid:101)cj = max{cj1, . . . , cjT −l+1}

(11)

We use various numbers of kernels with different
window sizes to extract different features, which
are then concatenated to form the ﬁnal sentence
representation xc. Finally, we pass xc through a
fully connected layer and a softmax layer to gen-
erate the probability p(fg|x1, . . . , xT ) as:

p(fg|x1, . . . , xT ) = sof tmax(V ∗ xc)

(12)

where V is the transformation matrix and fg ∈
{true, generated}.

C The training procedure of the global

GAN

We apply the global GANs to ﬁnetune the whole
model. Here, we provide detailed strategies for
training the global GANs. Firstly, we generate the
machine-generated source language sentences by
using Enct and Encs to decode the monolingual
data in target language. Similarly, we get the gen-
erated sentences in target language with Encs and
Dect by decoding source language monolingual
data. We simply use the greedy sampling method
instead of the beam search method for decoding.
Next, we pre-train Dg1 on the combination of true
monolingual data and the generated data in the
source language. Similarly, we also pre-train Dg2
on the combination of true monolingual data and
the generated data in the target language. Finally,
we jointly train the generators and discriminators.
The generators are trained with policy gradient
training methods. For the details about the pol-
icy gradient training, we refer the reader to (Yang
et al., 2017).

D The conﬁgurations for the open-source

toolkits

We train the word embedding use the following
script:

./word2vec -train text -output embedding.txt -
cbow 0 -size 512 -window 10 -negative 10 -hs 0
-sample 1e- -threads 50 -binary 0 -min-count 5 -
iter 10

After we get the embeddings for both the source
and target
languages, we use the open-source
VecMap 10 to map these embeddings to a shared-
latent space with the following scripts:

python3 normalize embeddings.py unit center -i

s embedding.txt -o s embedding.normalized.txt

python3 normalize embeddings.py unit center -i

t embedding.txt -o t embedding.normalized.txt

map embeddings.py

–
s embedding.normalized.txt

python3
orthogonal
t embedding.normalized.txt
s embedding.mapped.txt t embedding.mapped.txt
–numerals –self learning -v

10https://github.com/artetxem/vecmap

Unsupervised Neural Machine Translation with Weight Sharing

Zhen Yang1,2, Wei Chen1 , Feng Wang1,2∗, Bo Xu1
1Institute of Automation, Chinese Academy of Sciences
2University of Chinese Academy of Sciences
{yangzhen2014, wei.chen.media, feng.wang, xubo}@ia.ac.cn

8
1
0
2
 
r
p
A
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
7
5
0
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Unsupervised neural machine translation
(NMT) is a recently proposed approach
for machine translation which aims to train
the model without using any labeled data.
The models proposed for unsupervised
NMT often use only one shared encoder
to map the pairs of sentences from dif-
ferent languages to a shared-latent space,
which is weak in keeping the unique
and internal characteristics of each lan-
guage, such as the style, terminology, and
sentence structure. To address this is-
sue, we introduce an extension by utiliz-
ing two independent encoders but shar-
ing some partial weights which are re-
sponsible for extracting high-level repre-
sentations of the input sentences. Be-
sides, two different generative adversarial
networks (GANs), namely the local GAN
and global GAN, are proposed to enhance
the cross-language translation. With this
new approach, we achieve signiﬁcant im-
provements on English-German, English-
French and Chinese-to-English translation
tasks.

1

Introduction

Neural machine translation (Kalchbrenner and
Blunsom, 2013; Sutskever et al., 2014; Cho et al.,
2014; Bahdanau et al., 2014), directly applying a
single neural network to transform the source sen-
tence into the target sentence, has now reached im-
pressive performance (Shen et al., 2015; Wu et al.,
2016; Johnson et al., 2016; Gehring et al., 2017;
Vaswani et al., 2017). The NMT typically con-
sists of two sub neural networks. The encoder net-
work reads and encodes the source sentence into a

1Feng Wang is the corresponding author of this paper

context vector, and the decoder network generates
the target sentence iteratively based on the con-
text vector. NMT can be studied in supervised and
unsupervised learning settings. In the supervised
setting, bilingual corpora is available for training
the NMT model. In the unsupervised setting, we
only have two independent monolingual corpora
with one for each language and there is no bilin-
gual training example to provide alignment infor-
mation for the two languages. Due to lack of align-
ment information, the unsupervised NMT is con-
sidered more challenging. However, this task is
very promising, since the monolingual corpora is
usually easy to be collected.

Motivated by recent success in unsupervised
cross-lingual embeddings (Artetxe et al., 2016;
Zhang et al., 2017b; Conneau et al., 2017), the
models proposed for unsupervised NMT often as-
sume that a pair of sentences from two different
languages can be mapped to a same latent repre-
sentation in a shared-latent space (Lample et al.,
2017; Artetxe et al., 2017b). Following this as-
sumption, Lample et al. (2017) use a single en-
coder and a single decoder for both the source and
target languages. The encoder and decoder, act-
ing as a standard auto-encoder (AE), are trained to
reconstruct the inputs. And Artetxe et al. (2017b)
utilize a shared encoder but two independent de-
coders. With some good performance, they share
a glaring defect, i.e., only one encoder is shared
by the source and target languages. Although the
shared encoder is vital for mapping sentences from
different languages into the shared-latent space,
it is weak in keeping the uniqueness and inter-
nal characteristics of each language, such as the
style, terminology and sentence structure. Since
each language has its own characteristics,
the
source and target languages should be encoded
and learned independently. Therefore, we conjec-
ture that the shared encoder may be a factor limit-

ing the potential translation performance.

In order to address this issue, we extend the
encoder-shared model, i.e., the model with one
shared encoder, by leveraging two independent
encoders with each for one language. Similarly,
two independent decoders are utilized. For each
language, the encoder and its corresponding de-
coder perform an AE, where the encoder gener-
ates the latent representations from the perturbed
input sentences and the decoder reconstructs the
sentences from the latent representations. To map
the latent representations from different languages
to a shared-latent space, we propose the weight-
sharing constraint to the two AEs. Speciﬁcally,
we share the weights of the last few layers of two
encoders that are responsible for extracting high-
level representations of input sentences. Simi-
larly, we share the weights of the ﬁrst few lay-
ers of two decoders. To enforce the shared-latent
space, the word embeddings are used as a rein-
forced encoding component in our encoders. For
cross-language translation, we utilize the back-
translation following (Lample et al., 2017). Addi-
tionally, two different generative adversarial net-
works (GAN) (Yang et al., 2017), namely the lo-
cal and global GAN, are proposed to further im-
prove the cross-language translation. We utilize
the local GAN to constrain the source and tar-
get latent representations to have the same distri-
bution, whereby the encoder tries to fool a local
discriminator which is simultaneously trained to
distinguish the language of a given latent repre-
sentation. We apply the global GAN to ﬁnetune
the corresponding generator, i.e., the composition
of the encoder and decoder of the other language,
where a global discriminator is leveraged to guide
the training of the generator by assessing how far
the generated sentence is from the true data distri-
bution 1. In summary, we mainly make the follow-
ing contributions:

• We propose the weight-sharing constraint to
unsupervised NMT, enabling the model to
utilize an independent encoder for each lan-
guage. To enforce the shared-latent space,
we also propose the embedding-reinforced
encoders and two different GANs for our
model.

• We

conduct

extensive

experiments on

1The

code

evaluate

and
https://github.com/ZhenYangIACAS/unsupervised-NMT

that

we
our models

utilized
be
can

to
found

train
at

English-German,
English-French
and
Ex-
Chinese-to-English translation tasks.
perimental results show that the proposed
approach consistently achieves great success.

• Last but not least, we introduce the direc-
tional self-attention to model temporal order
information for the proposed model. Exper-
imental results reveal that it deserves more
efforts for researchers to investigate the tem-
poral order information within self-attention
layers of NMT.

2 Related Work

Several approaches have been proposed to train
NMT models without direct parallel corpora. The
scenario that has been widely investigated is one
where two languages have little parallel data be-
tween them but are well connected by one pivot
language. The most typical approach in this sce-
nario is to independently translate from the source
language to the pivot language and from the pivot
language to the target language (Saha et al., 2016;
Cheng et al., 2017). To improve the transla-
tion performance, Johnson et al. (2016) propose a
multilingual extension of a standard NMT model
and they achieve substantial improvement for lan-
guage pairs without direct parallel training data.

Recently, motivated by the success of cross-
lingual embeddings, researchers begin to show in-
terests in exploring the more ambitious scenario
where an NMT model is trained from monolingual
corpora only. Lample et al. (2017) and Artetxe
et al. (2017b) simultaneously propose an approach
for this scenario, which is based on pre-trained
cross lingual embeddings. Lample et al. (2017)
utilizes a single encoder and a single decoder for
both languages. The entire system is trained to
reconstruct its perturbed input. For cross-lingual
translation, they incorporate back-translation into
the training procedure. Different from (Lample
et al., 2017), Artetxe et al. (2017b) use two in-
dependent decoders with each for one language.
The two works mentioned above both use a sin-
gle shared encoder to guarantee the shared latent
space. However, a concomitant defect is that the
shared encoder is weak in keeping the uniqueness
of each language. Our work also belongs to this
more ambitious scenario, and to the best of our
knowledge, we are one among the ﬁrst endeav-
ors to investigate how to train an NMT model with
monolingual corpora only.

Figure 1: The architecture of the proposed model. We implement the shared-latent space assumption
using a weight sharing constraint where the connection of the last few layers in Encs and Enct are
tied (illustrated with dashed lines) and the connection of the ﬁrst few layers in Decs and Dect are
tied. ˜xEncs−Decs
is
s
the translated sentence from source to target and ˜xEnct−Decs
is the translation in reversed direction.
t
Dl is utilized to assess whether the hidden representation of the encoder is from the source or target
language. Dg1 and Dg2 are used to evaluate whether the translated sentences are realistic for each
language respectively. Z represents the shared-latent space.

are self-reconstructed sentences in each language. ˜xEncs−Dect

and ˜xEnct−Dect

s

t

3 The Approach

3.1 Model Architecture

The model architecture, as illustrated in ﬁgure 1,
is based on the AE and GAN. It consists of seven
sub networks: including two encoders Encs and
Enct, two decoders Decs and Dect, the local dis-
criminator Dl, and the global discriminators Dg1
and Dg2. For the encoder and decoder, we follow
the newly emerged Transformer (Vaswani et al.,
2017). Speciﬁcally, the encoder is composed of a
stack of four identical layers 2. Each layer con-
sists of a multi-head self-attention and a simple
position-wise fully connected feed-forward net-
work. The decoder is also composed of four iden-
tical layers. In addition to the two sub-layers in
each encoder layer, the decoder inserts a third sub-
layer, which performs multi-head attention over
the output of the encoder stack. For more details
about the multi-head self-attention layer, we refer
the reader to (Vaswani et al., 2017). We implement
the local discriminator as a multi-layer perceptron
and implement the global discriminator based on
the convolutional neural network (CNN). Several
ways exist to interpret the roles of the sub net-
works are summarised in table 1. The proposed
system has several striking components , which
are critical either for the system to be trained in an

2The layer number is selected according to our prelimi-

nary experiment, which is presented in appendix A.

unsupervised manner or for improving the transla-
tion performance.

Networks

Roles

{Encs, Decs}
{Enct, Dect}
{Encs, Dect}
{Enct, Decs}
{Encs, Dl}
{Enct, Dl}
{Enct, Decs, Dg1}
{Encs, Dect, Dg2}

AE for source language
AE for target language
translation source → target
translation target → source
1st local GAN (GANl1)
2nd local GAN (GANl2)
1st global GAN (GANg1)
2nd global GAN (GANg2)

Table 1: Interpretation of the roles for the subnet-
works in the proposed system.

Directional self-attention Compared to recur-
rent neural network, a disadvantage of the simple
self-attention mechanism is that the temporal or-
der information is lost. Although the Transformer
applies the positional encoding to the sequence be-
fore processed by the self-attention, how to model
temporal order information within an attention is
still an open question. Following (Shen et al.,
2017), we build the encoders in our model on the
directional self-attention which utilizes the posi-
tional masks to encode temporal order information
into attention output. More concretely, two posi-
tional masks, namely the forward mask M f and

backward mask M b, are calculated as:

(cid:26)

(cid:26)

M f

ij =

M b

ij =

−∞ otherwise

0

0

i < j

i > j

−∞ otherwise

(1)

(2)

With the forward mask M f , the later token only
makes attention connections to the early tokens
in the sequence, and vice versa with the back-
ward mask. Similar to (Zhou et al., 2016; Wang
et al., 2017), we utilize a self-attention network
to process the input sequence in forward direc-
tion. The output of this layer is taken by an upper
self-attention network as input, processed in the
reverse direction.

Weight sharing Based on the shared-latent
space assumption, we apply the weight sharing
constraint to relate the two AEs. Speciﬁcally, we
share the weights of the last few layers of the Encs
and Enct, which are responsible for extracting
high-level representations of the input sentences.
Similarly, we also share the ﬁrst few layers of
the Decs and Dect, which are expected to de-
code high-level representations that are vital for
reconstructing the input sentences. Compared to
(Cheng et al., 2016; Saha et al., 2016) which use
the fully shared encoder, we only share partial
weights for the encoders and decoders. In the pro-
posed model, the independent weights of the two
encoders are expected to learn and encode the hid-
den features about the internal characteristics of
each language, such as the terminology, style, and
sentence structure. The shared weights are utilized
to map the hidden features extracted by the inde-
pendent weights to the shared-latent space.

Embedding reinforced encoder We use pre-
trained cross-lingual embeddings in the encoders
that are kept ﬁxed during training. And the
ﬁxed embeddings are used as a reinforced en-
coding component
Formally,
given the input sequence embedding vectors E =
{e1, . . . , et} and the initial output sequence of the
encoder stack H = {h1, . . . , ht}, we compute Hr
as:

in our encoder.

Hr = g (cid:12) H + (1 − g) (cid:12) E

(3)

where Hr is the ﬁnal output sequence of the en-
coder which will be attended by the decoder (In
Transformer, H is the ﬁnal output of the encoder),
g is a gate unit and computed as:

g = σ(W1E + W2H + b)

(4)

where W1, W2 and b are trainable parameters
and they are shared by the two encoders. The
motivation behind is twofold. Firstly, taking the
ﬁxed cross-lingual embedding as the other encod-
ing component is helpful to reinforce the shared-
latent space. Additionally, from the point of multi-
channel encoders (Xiong et al., 2017), provid-
ing encoding components with different levels of
composition enables the decoder to take pieces of
source sentence at varying composition levels suit-
ing its own linguistic structure.

3.2 Unsupervised Training

Based on the architecture proposed above, we train
the NMT model with the monolingual corpora
only using the following four strategies:

Denoising auto-encoding Firstly, we train the
two AEs to reconstruct their inputs respectively.
In this form, each encoder should learn to com-
pose the embeddings of its corresponding lan-
guage and each decoder is expected to learn to de-
compose this representation into its corresponding
language. Nevertheless, without any constraint,
the AE quickly learns to merely copy every word
one by one, without capturing any internal struc-
ture of the language involved. To address this
problem, we utilize the same strategy of denois-
ing AE (Vincent et al., 2008) and add some noise
to the input sentences (Hill et al., 2016; Artetxe
et al., 2017b). To this end, we shufﬂe the input
sentences randomly. Speciﬁcally, we apply a ran-
dom permutation ε to the input sentence, verifying
the condition:

steps
s

|ε(i) − i| ≤ min(k([

] + 1), n), ∀i ∈ {1, n}
(5)
where n is the length of the input sentence, steps
is the global steps the model has been updated, k
and s are the tunable parameters which can be set
by users beforehand. This way, the system needs
to learn some useful structure of the involved lan-
guages to be able to recover the correct word order.
In practice, we set k = 2 and s = 100000.

Back-translation In spite of denoising auto-
encoding, the training procedure still involves a
single language at each time, without considering
our ﬁnal goal of mapping an input sentence from
the source/target language to the target/source lan-
guage. For the cross language training, we uti-
lize the back-translation approach for our unsu-
pervised training procedure. Back-translation has
shown its great effectiveness on improving NMT

model with monolingual data and has been widely
investigated by (Sennrich et al., 2015a; Zhang and
Zong, 2016).
In our approach, given an input
sentence in a given language, we apply the cor-
responding encoder and the decoder of the other
language to translate it to the other language 3.
By combining the translation with its original sen-
tence, we get a pseudo-parallel corpus which is
utilized to train the model to reconstruct the origi-
nal sentence from its translation.

Local GAN Although the weight sharing con-
straint is vital for the shared-latent space assump-
tion, it alone does not guarantee that the corre-
sponding sentences in two languages will have the
same or similar latent code. To further enforce
the shared-latent space, we train a discriminative
neural network, referred to as the local discrimi-
nator, to classify between the encoding of source
sentences and the encoding of target sentences.
The local discriminator, implemented as a multi-
layer perceptron with two hidden layers of size
256, takes the output of the encoder, i.e., Hr calcu-
lated as equation 3, as input, and produces a binary
prediction about the language of the input sen-
tence. The local discriminator is trained to predict
the language by minimizing the following cross-
entropy loss:

LDl(θDl) =
− Ex∈xs[log p(f = s|Encs(x))]
− Ex∈xt[log p(f = t|Enct(x))]

(6)

where θDl represents the parameters of the local
discriminator and f ∈ {s, t}. The encoders are
trained to fool the local discriminator:

LEncs(θEncs) =
− Ex∈xs[log p(f = t|Encs(x))]

(7)

LEnct(θEnct) =
− Ex∈xt[log p(f = s|Enct(x))]
where θEncs and θEnct are the parameters of the
two encoders.

(8)

Global GAN We apply the global GANs to
ﬁne tune the whole model so that the model is
able to generate sentences undistinguishable from
the true data, i.e., sentences in the training cor-
pus. Different from the local GANs which up-
dates the parameters of the encoders locally, the

global GANs are utilized to update the whole pa-
including the
rameters of the proposed model,
parameters of encoders and decoders. The pro-
posed model has two global GANs: GANg1 and
In GANg1, the Enct and Decs act as
GANg2.
4
the generator, which generates the sentence ˜xt
from xt. The Dg1, implemented based on CNN,
assesses whether the generated sentence ˜xt is the
true target-language sentence or the generated sen-
tence. The global discriminator aims to distin-
guish among the true sentences and generated sen-
tences, and it is trained to minimize its classiﬁ-
cation error rate. During training, the Dg1 feeds
back its assessment to ﬁnetune the encoder Enct
and decoder Decs. Since the machine transla-
tion is a sequence generation problem, following
(Yang et al., 2017), we leverage policy gradient re-
inforcement training to back-propagate the assess-
ment. We apply a similar processing to GANg2
(The details about the architecture of the global
discriminator and the training procedure of the
global GANs can be seen in appendix B and C).

There are two stages in the proposed unsuper-
vised training. In the ﬁrst stage, we train the pro-
posed model with denoising auto-encoding, back-
translation and the local GANs, until no improve-
ment is achieved on the development set. Specif-
ically, we perform one batch of denoising auto-
encoding for the source and target languages, one
batch of back-translation for the two languages,
and another batch of local GAN for the two lan-
guages. In the second stage, we ﬁne tune the pro-
posed model with the global GANs.

4 Experiments and Results

We evaluate the proposed approach on English-
German, English-French and Chinese-to-English
translation tasks 5. We ﬁrstly describe the datasets,
pre-processing and model hyper-parameters we
used, then we introduce the baseline systems, and
ﬁnally we present our experimental results.

4.1 Data Sets and Preprocessing

In English-German and English-French transla-
tion, we make our experiments comparable with
previous work by using the datasets from the

4The ˜xt is ˜xEnct−Decs
t

script for simplicity.

in ﬁgure 1. We omit the super-

3Since the quality of the translation shows little effect on
the performance of the model (Sennrich et al., 2015a), we
simply use greedy decoding for speed.

5The reason that we do not conduct experiments on
English-to-Chinese translation is that we do not get public
test sets for English-to-Chinese.

WMT 2014 and WMT 2016 shared tasks respec-
tively. For Chinese-to-English translation, we use
the datasets from LDC, which has been widely uti-
lized by previous works (Tu et al., 2017; Zhang
et al., 2017a).

WMT14 English-French Similar to (Lample
et al., 2017), we use the full training set of 36M
sentence pairs and we lower-case them and re-
move sentences longer than 50 words, resulting
in a parallel corpus of about 30M pairs of sen-
tences. To guarantee no exact correspondence be-
tween the source and target monolingual sets, we
build monolingual corpora by selecting English
sentences from 15M random pairs, and selecting
the French sentences from the complementary set.
Sentences are encoded with byte-pair encoding
(Sennrich et al., 2015b), which has an English vo-
cabulary of about 32000 tokens, and French vo-
cabulary of about 33000 tokens. We report results
on newstest2014.

WMT16 English-German We follow the same
procedure mentioned above to create monolingual
training corpora for English-German translation,
and we get two monolingual training data of 1.8M
sentences each. The two languages share a vocab-
ulary of about 32000 tokens. We report results on
newstest2016.

LDC Chinese-English For Chinese-to-English
translation, our training data consists of 1.6M sen-
tence pairs randomly extracted from LDC corpora
6. Since the data set is not big enough, we just
build the monolingual data set by randomly shuf-
ﬂing the Chinese and English sentences respec-
tively.
In spite of the fact that some correspon-
dence between examples in these two monolingual
sets may exist, we never utilize this alignment in-
formation in our training procedure (see Section
3.2). Both the Chinese and English sentences are
encoded with byte-pair encoding. We get an En-
glish vocabulary of about 34000 tokens, and Chi-
nese vocabulary of about 38000 tokens. The re-
sults are reported on N IST 02.

Since the proposed system relies on the pre-
trained cross-lingual embeddings, we utilize the
monolingual corpora described above to train the
embeddings for each language independently by
using word2vec (Mikolov et al., 2013). We then
apply the public implementation 7 of the method
proposed by (Artetxe et al., 2017a) to map these

6LDC2002L27,

LDC2002E18,
LDC2003E07, LDC2004T08, LDC2004E12, LDC2005T10

LDC2002T01,

7https://github.com/artetxem/vecmap

embeddings to a shared-latent space 8.

4.2 Model Hyper-parameters and Evaluation

Following the base model
in (Vaswani et al.,
2017), we set the dimension of word embedding
as 512, dropout rate as 0.1 and the head number
as 8. We use beam search with a beam size of 4
and length penalty α = 0.6. The model is im-
plemented in TensorFlow (Abadi et al., 2015) and
trained on up to four K80 GPUs synchronously in
a multi-GPU setup on a single machine.

For model selection, we stop training when the
model achieves no improvement for the tenth eval-
uation on the development set, which is com-
prised of 3000 source and target sentences ex-
tracted randomly from the monolingual training
corpora. Following (Lample et al., 2017), we
translate the source sentences to the target lan-
guage, and then translate the resulting sentences
back to the source language. The quality of the
model is then evaluated by computing the BLEU
score over the original inputs and their reconstruc-
tions via this two-step translation process. The
performance is ﬁnally averaged over two direc-
tions, i.e., from source to target and from target
to source. BLEU (Papineni et al., 2002) is utilized
as the evaluation metric. For Chinese-to-English,
we apply the script mteval-v11b.pl to evaluate the
translation performance. For English-German and
English-French, we evaluate the translation per-
formance with the script multi-belu.pl 9.

4.3 Baseline Systems

Word-by-word translation (WBW) The ﬁrst
is a system that per-
baseline we consider
forms word-by-word translations using the in-
ferred bilingual dictionary. Speciﬁcally, it trans-
lates a sentence word-by-word, replacing each
word with its nearest neighbor in the other lan-
guage.

Lample et al. (2017) The second baseline is
a previous work that uses the same training and
testing sets with this paper. Their model belongs
to the standard attention-based encoder-decoder
framework, which implements the encoder using
a bidirectional long short term memory network
(LSTM) and implements the decoder using a sim-

8The conﬁguration we used to run these open-source

toolkits can be found in appendix D
9https://github.com/moses-

smt/mosesdecoder/blob/617e8c8/scripts/generic/multi-
bleu.perl;mteval-v11b.pl

en-de

de-en

en-fr

fr-en

zh-en

Supervised
Word-by-word
Lample et al. (2017)

24.07
5.85
9.64

26.99
9.34
13.33

30.50
3.60
15.05

30.21
6.80
14.31

40.02
5.09
-

The proposed approach 10.86

14.62

16.97

15.58

14.52

Table 2: The translation performance on English-German, English-French and Chinese-to-English test
sets. The results of (Lample et al., 2017) are copied directly from their paper. We do not present the
results of (Artetxe et al., 2017b) since we use different training sets.

ple forward LSTM. They apply one single encoder
and decoder for the source and target languages.

Supervised training We ﬁnally consider ex-
actly the same model as ours, but trained using the
standard cross-entropy loss on the original parallel
sentences. This model can be viewed as an upper
bound for the proposed unsupervised model.

4.4 Results and Analysis

4.4.1 Number of weight-sharing layers

We ﬁrstly investigate how the number of weight-
sharing layers affects the translation performance.
In this experiment, we vary the number of weight-
sharing layers in the AEs from 0 to 4. Shar-
ing one layer in AEs means sharing one layer
for the encoders and in the meanwhile, shar-
ing one layer for the decoders.
The BLEU
scores of English-to-German, English-to-French
and Chinese-to-English translation tasks are re-
ported in ﬁgure 2. Each curve corresponds to a
different translation task and the x-axis denotes
the number of weight-sharing layers for the AEs.
We ﬁnd that the number of weight-sharing layers
shows much effect on the translation performance.
And the best translation performance is achieved
when only one layer is shared in our system. When
all of the four layers are shared, i.e., only one
shared encoder is utilized, we get poor translation
performance in all of the three translation tasks.
This veriﬁes our conjecture that the shared en-
coder is detrimental to the performance of unsu-
pervised NMT especially for the translation tasks
on distant language pairs. More concretely, for the
related language pair translation, i.e., English-to-
French, the encoder-shared model achieves -0.53
BLEU points decline than the best model where
only one layer is shared. For the more distant lan-
guage pair English-to-German, the encoder-shared
model achieves more signiﬁcant decline, i.e., -0.85
BLEU points decline. And for the most distant

language pair Chinese-to-English, the decline is
as large as -1.66 BLEU points. We explain this as
that the more distant the language pair is, the more
different characteristics they have. And the shared
encoder is weak in keeping the unique characteris-
tic of each language. Additionally, we also notice
that using two completely independent encoders,
i.e., setting the number of weight-sharing layers
as 0, results in poor translation performance too.
This conﬁrms our intuition that the shared layers
are vital to map the source and target latent rep-
resentations to a shared-latent space. In the rest
of our experiments, we set the number of weight-
sharing layer as 1.

Figure 2: The effects of the weight-sharing layer
number on English-to-German, English-to-French
and Chinese-to-English translation tasks.

4.4.2 Translation results

Table 2 shows the BLEU scores on English-
German, English-French and English-to-Chinese
test sets. As it can be seen, the proposed ap-
proach obtains signiﬁcant improvements than the
word-by-word baseline system, with at least +5.01
BLEU points in English-to-German translation
and up to +13.37 BLEU points in English-to-
French translation. This shows that the proposed

en-de

de-en

en-fr

fr-en

zh-en

Without weight sharing
Without embedding-reinforced encoder
Without directional self-attention
Without local GANs
Without Global GANs
Full model

10.23
10.45
10.60
10.51
10.34
10.86

13.84
14.17
14.21
14.35
14.05
14.62

16.02
16.55
16.82
16.40
16.19
16.97

14.82
15.27
15.30
15.07
15.21
15.58

13.75
14.10
14.29
14.12
14.09
14.52

Table 3: Ablation study on English-German, English-French and Chinese-to-English translation tasks.
Without weight sharing means no layers are shared in the two AEs.

model only trained with monolingual data effec-
tively learns to use the context information and
the internal structure of each language. Com-
pared to the work of (Lample et al., 2017), our
model also achieves up to +1.92 BLEU points im-
provement on English-to-French translation task.
We believe that the unsupervised NMT is very
promising. However, there is still a large room
for improvement compared to the supervised up-
per bound. The gap between the supervised and
unsupervised model is as large as 12.3-25.5 BLEU
points depending on the language pair and transla-
tion direction.

4.4.3 Ablation study
To understand the importance of different com-
ponents of the proposed system, we perform an
ablation study by training multiple versions of
the
our model with some missing components:
local GANs,
the directional
the global GANs,
self-attention, the weight-sharing, the embedding-
reinforced encoders, etc. Results are reported
in table 3. We do not test the the importance
of the auto-encoding, back-translation and the
pre-trained embeddings because they have been
widely tested in (Lample et al., 2017; Artetxe
et al., 2017b). Table 3 shows that the best per-
formance is obtained with the simultaneous use of
all the tested elements. The most critical compo-
nent is the weight-sharing constraint, which is vi-
tal to map sentences of different languages to the
shared-latent space. The embedding-reinforced
encoder also brings some improvement on all of
the translation tasks. When we remove the di-
rectional self-attention, we get up to -0.3 BLEU
points decline. This indicates that it deserves more
efforts to investigate the temporal order informa-
tion in self-attention mechanism. The GANs also
signiﬁcantly improve the translation performance
the global GANs
of our system. Speciﬁcally,

achieve improvement up to +0.78 BLEU points on
English-to-French translation and the local GANs
also obtain improvement up to +0.57 BLEU points
on English-to-French translation. This reveals that
the proposed model beneﬁts a lot from the cross-
domain loss deﬁned by GANs.

5 Conclusion and Future work

The models proposed recently for unsupervised
NMT use a single encoder to map sentences from
different languages to a shared-latent space. We
conjecture that the shared encoder is problem-
atic for keeping the unique and inherent char-
acteristic of each language.
In this paper, we
propose the weight-sharing constraint in unsuper-
vised NMT to address this issue. To enhance the
cross-language translation performance, we also
propose the embedding-reinforced encoders, local
GAN and global GAN into the proposed system.
Additionally, the directional self-attention is intro-
duced to model the temporal order information for
our system.
We test

the proposed model on English-
German, English-French and Chinese-to-English
translation tasks. The experimental results reveal
that our approach achieves signiﬁcant improve-
ment and verify our conjecture that the shared en-
coder is really a bottleneck for improving the un-
supervised NMT. The ablation study shows that
each component of our system achieves some im-
provement for the ﬁnal translation performance.

Unsupervised NMT opens exciting opportuni-
ties for the future research. However, there is
still a large room for improvement compared to
the supervised NMT. In the future, we would like
to investigate how to utilize the monolingual data
more effectively, such as incorporating the lan-
guage model and syntactic information into unsu-
pervised NMT. Besides, we decide to make more

efforts to explore how to reinforce the temporal or-
der information for the proposed model.

International Joint Conference on Artiﬁcial Intelli-
gence. pages 3974–3980.

Acknowledgements

This work is supported by the National Key Re-
search and Development Program of China un-
der Grant No. 2017YFB1002102, and Beijing
Engineering Research Center under Grant No.
Z171100002217015. We would like to thank Xu
Shuang for her preparing data used in this work.
Additionally, we also want to thank Jiaming Xu,
Suncong Zheng and Wenfu Wang for their invalu-
able discussions on this work.

References

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Man´e, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Vi´egas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems .

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Conference on Empirical Methods in Natural
Language Processing. pages 2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2017a. Learning bilingual word embeddings with
(almost) no bilingual data. In Meeting of the Asso-
ciation for Computational Linguistics. pages 451–
462.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2017b. Unsupervised neural ma-
chine translation .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473 .

Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, and
Wei Xu. 2016. Neural machine translation with
pivot languages. arXiv preprint arXiv:1611.04928
.

Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, Wei
Xu, Yong Cheng, Qian Yang, Yang Liu, Maosong
Joint training for pivot-
Sun, and Wei Xu. 2017.
In Twenty-Sixth
based neural machine translation.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Learning
Schwenk, and Yoshua Bengio. 2014.
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Herv Jgou. 2017.
Word translation without parallel data .

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning .

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. TACL .

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vi´egas, Martin Wattenberg, Greg Corrado,
et al. 2016. Google’s multilingual neural machine
translation system: Enabling zero-shot translation.
arXiv preprint arXiv:1611.04558 .

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. EMNLP pages
1700–1709.

Guillaume

Ludovic Denoyer,

and
Lample,
Marc’Aurelio Ranzato. 2017.
Unsupervised
machine translation using monolingual corpora only
.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems. pages 3111–3119.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. Association for Com-
putational Linguistics pages 311–318.

Amrita Saha, Mitesh M Khapra, Sarath Chandar, Ja-
narthanan Rajendran, and Kyunghyun Cho. 2016.
A correlational encoder decoder architecture for
arXiv preprint
pivot based sequence generation.
arXiv:1606.04754 .

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation
arXiv preprint

2015a.
models with monolingual data.
arXiv:1511.06709 .

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015b. Neural machine translation of rare words
with subword units. Computer Science .

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. arXiv
preprint arXiv:1512.02433 .

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding .

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. Advances in neural information processing
systems pages 3104–3112.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2017. Learning to remember translation his-
tory with a continuous cache .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need .

Pascal Vincent, Hugo Larochelle, Yoshua Bengio,
and Pierre-Antoine Manzagol. 2008. Extracting
and composing robust features with denoising au-
In Proceedings of the 25th interna-
toencoders.
tional conference on Machine learning. ACM, pages
1096–1103.

Mingxuan Wang, Zhengdong Lu,

Jie Zhou, and
Qun Liu. 2017. Deep neural machine transla-
arXiv preprint
tion with linear associative unit.
arXiv:1705.00861 .

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016.
Google’s neural ma-
chine translation system: Bridging the gap between
arXiv preprint
human and machine translation.
arXiv:1609.08144 .

bilingual lexicon induction. In Meeting of the Asso-
ciation for Computational Linguistics. pages 1959–
1970.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei
Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. arXiv
preprint arXiv:1606.04199 .

A Experiments on the layer number for

encoders and decoders

To determine the number of layers for encoders
and decoders in our system beforehand, we con-
duct experiments on English-German translation
tasks to test how the amount of layers in encoders
and decoders affects the translation performance.
We vary the number of layers from 2 to 6 and the
results are reported in table 4. We can ﬁnd that the
translation performance achieves substantial im-
provement with the layer number increasing from
2 to 4. However, with layer number set larger than
4, we get little improvement. To make a trade-off
between the translation performance and the com-
putation complexity, we set the layer number as 4
for our encoders and decoders.

layer num en-de

de-en

2
3
4
5
6

11.57
12.43
12.86
12.91
12.95

14.01
14.99
15.62
15.83
15.79

Table 4: The experiments on the number of layers
for encoders and decoders.

Hao Xiong, Zhongjun He, Xiaoguang Hu, and Hua Wu.
2017. Multi-channel encoder for neural machine
translation. arXiv preprint arXiv:1712.02109 .

B The architecture of the global

discriminator

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2017.
Improving neural machine translation with condi-
tional sequence generative adversarial nets .

Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu,
and Maosong Sun. 2017a. Prior knowledge integra-
tion for neural machine translation using posterior
In Meeting of the Association for
regularization.
Computational Linguistics. pages 1514–1523.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Conference on Empirical Methods in
Natural Language Processing. pages 1535–1545.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Adversarial training for unsupervised

The global discriminator is applied to classify the
generated sentences as source language, target lan-
guage or generated sentences. Following (Yang
et al., 2017), we implement the global discrimina-
tor based on CNN. Since sentences generated by
the generator (the composition of the encoder and
decoder) have variable lengths, the CNN padding
is used to transform the sentences to sequences
with ﬁxed length T , which is the maximum length
set for the output of the generator. Given the gen-
erated sequences x1, . . . , xT , we build the matrix
X1:T as:

X1:T = x1; x2; . . . ; xT

(9)

where xt ∈ Rk is the k-dimensional word embed-
ding and the semicolon is the concatenation oper-
ator. For the matrix X1:T , a kernel wj ∈ Rl×k
applies a convolutional operation to a window size
of l words to produce a series of feature maps:

cji = ρ(BN (wj ⊗ Xi:i+l−1 + b))

(10)

where ⊗ operator is the summation of element-
wise production and b is a bias term. ρ is a non-
linear activation function which is implemented as
ReLu in this paper. To get the ﬁnal feature with
respect to kernel wj, a max-over-time pooling op-
eration is leveraged over the feature maps:

(cid:101)cj = max{cj1, . . . , cjT −l+1}

(11)

We use various numbers of kernels with different
window sizes to extract different features, which
are then concatenated to form the ﬁnal sentence
representation xc. Finally, we pass xc through a
fully connected layer and a softmax layer to gen-
erate the probability p(fg|x1, . . . , xT ) as:

p(fg|x1, . . . , xT ) = sof tmax(V ∗ xc)

(12)

where V is the transformation matrix and fg ∈
{true, generated}.

C The training procedure of the global

GAN

We apply the global GANs to ﬁnetune the whole
model. Here, we provide detailed strategies for
training the global GANs. Firstly, we generate the
machine-generated source language sentences by
using Enct and Encs to decode the monolingual
data in target language. Similarly, we get the gen-
erated sentences in target language with Encs and
Dect by decoding source language monolingual
data. We simply use the greedy sampling method
instead of the beam search method for decoding.
Next, we pre-train Dg1 on the combination of true
monolingual data and the generated data in the
source language. Similarly, we also pre-train Dg2
on the combination of true monolingual data and
the generated data in the target language. Finally,
we jointly train the generators and discriminators.
The generators are trained with policy gradient
training methods. For the details about the pol-
icy gradient training, we refer the reader to (Yang
et al., 2017).

D The conﬁgurations for the open-source

toolkits

We train the word embedding use the following
script:

./word2vec -train text -output embedding.txt -
cbow 0 -size 512 -window 10 -negative 10 -hs 0
-sample 1e- -threads 50 -binary 0 -min-count 5 -
iter 10

After we get the embeddings for both the source
and target
languages, we use the open-source
VecMap 10 to map these embeddings to a shared-
latent space with the following scripts:

python3 normalize embeddings.py unit center -i

s embedding.txt -o s embedding.normalized.txt

python3 normalize embeddings.py unit center -i

t embedding.txt -o t embedding.normalized.txt

map embeddings.py

–
s embedding.normalized.txt

python3
orthogonal
t embedding.normalized.txt
s embedding.mapped.txt t embedding.mapped.txt
–numerals –self learning -v

10https://github.com/artetxem/vecmap

Unsupervised Neural Machine Translation with Weight Sharing

Zhen Yang1,2, Wei Chen1 , Feng Wang1,2∗, Bo Xu1
1Institute of Automation, Chinese Academy of Sciences
2University of Chinese Academy of Sciences
{yangzhen2014, wei.chen.media, feng.wang, xubo}@ia.ac.cn

8
1
0
2
 
r
p
A
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
7
5
0
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Unsupervised neural machine translation
(NMT) is a recently proposed approach
for machine translation which aims to train
the model without using any labeled data.
The models proposed for unsupervised
NMT often use only one shared encoder
to map the pairs of sentences from dif-
ferent languages to a shared-latent space,
which is weak in keeping the unique
and internal characteristics of each lan-
guage, such as the style, terminology, and
sentence structure. To address this is-
sue, we introduce an extension by utiliz-
ing two independent encoders but shar-
ing some partial weights which are re-
sponsible for extracting high-level repre-
sentations of the input sentences. Be-
sides, two different generative adversarial
networks (GANs), namely the local GAN
and global GAN, are proposed to enhance
the cross-language translation. With this
new approach, we achieve signiﬁcant im-
provements on English-German, English-
French and Chinese-to-English translation
tasks.

1

Introduction

Neural machine translation (Kalchbrenner and
Blunsom, 2013; Sutskever et al., 2014; Cho et al.,
2014; Bahdanau et al., 2014), directly applying a
single neural network to transform the source sen-
tence into the target sentence, has now reached im-
pressive performance (Shen et al., 2015; Wu et al.,
2016; Johnson et al., 2016; Gehring et al., 2017;
Vaswani et al., 2017). The NMT typically con-
sists of two sub neural networks. The encoder net-
work reads and encodes the source sentence into a

1Feng Wang is the corresponding author of this paper

context vector, and the decoder network generates
the target sentence iteratively based on the con-
text vector. NMT can be studied in supervised and
unsupervised learning settings. In the supervised
setting, bilingual corpora is available for training
the NMT model. In the unsupervised setting, we
only have two independent monolingual corpora
with one for each language and there is no bilin-
gual training example to provide alignment infor-
mation for the two languages. Due to lack of align-
ment information, the unsupervised NMT is con-
sidered more challenging. However, this task is
very promising, since the monolingual corpora is
usually easy to be collected.

Motivated by recent success in unsupervised
cross-lingual embeddings (Artetxe et al., 2016;
Zhang et al., 2017b; Conneau et al., 2017), the
models proposed for unsupervised NMT often as-
sume that a pair of sentences from two different
languages can be mapped to a same latent repre-
sentation in a shared-latent space (Lample et al.,
2017; Artetxe et al., 2017b). Following this as-
sumption, Lample et al. (2017) use a single en-
coder and a single decoder for both the source and
target languages. The encoder and decoder, act-
ing as a standard auto-encoder (AE), are trained to
reconstruct the inputs. And Artetxe et al. (2017b)
utilize a shared encoder but two independent de-
coders. With some good performance, they share
a glaring defect, i.e., only one encoder is shared
by the source and target languages. Although the
shared encoder is vital for mapping sentences from
different languages into the shared-latent space,
it is weak in keeping the uniqueness and inter-
nal characteristics of each language, such as the
style, terminology and sentence structure. Since
each language has its own characteristics,
the
source and target languages should be encoded
and learned independently. Therefore, we conjec-
ture that the shared encoder may be a factor limit-

ing the potential translation performance.

In order to address this issue, we extend the
encoder-shared model, i.e., the model with one
shared encoder, by leveraging two independent
encoders with each for one language. Similarly,
two independent decoders are utilized. For each
language, the encoder and its corresponding de-
coder perform an AE, where the encoder gener-
ates the latent representations from the perturbed
input sentences and the decoder reconstructs the
sentences from the latent representations. To map
the latent representations from different languages
to a shared-latent space, we propose the weight-
sharing constraint to the two AEs. Speciﬁcally,
we share the weights of the last few layers of two
encoders that are responsible for extracting high-
level representations of input sentences. Simi-
larly, we share the weights of the ﬁrst few lay-
ers of two decoders. To enforce the shared-latent
space, the word embeddings are used as a rein-
forced encoding component in our encoders. For
cross-language translation, we utilize the back-
translation following (Lample et al., 2017). Addi-
tionally, two different generative adversarial net-
works (GAN) (Yang et al., 2017), namely the lo-
cal and global GAN, are proposed to further im-
prove the cross-language translation. We utilize
the local GAN to constrain the source and tar-
get latent representations to have the same distri-
bution, whereby the encoder tries to fool a local
discriminator which is simultaneously trained to
distinguish the language of a given latent repre-
sentation. We apply the global GAN to ﬁnetune
the corresponding generator, i.e., the composition
of the encoder and decoder of the other language,
where a global discriminator is leveraged to guide
the training of the generator by assessing how far
the generated sentence is from the true data distri-
bution 1. In summary, we mainly make the follow-
ing contributions:

• We propose the weight-sharing constraint to
unsupervised NMT, enabling the model to
utilize an independent encoder for each lan-
guage. To enforce the shared-latent space,
we also propose the embedding-reinforced
encoders and two different GANs for our
model.

• We

conduct

extensive

experiments on

1The

code

evaluate

and
https://github.com/ZhenYangIACAS/unsupervised-NMT

that

we
our models

utilized
be
can

to
found

train
at

English-German,
English-French
and
Ex-
Chinese-to-English translation tasks.
perimental results show that the proposed
approach consistently achieves great success.

• Last but not least, we introduce the direc-
tional self-attention to model temporal order
information for the proposed model. Exper-
imental results reveal that it deserves more
efforts for researchers to investigate the tem-
poral order information within self-attention
layers of NMT.

2 Related Work

Several approaches have been proposed to train
NMT models without direct parallel corpora. The
scenario that has been widely investigated is one
where two languages have little parallel data be-
tween them but are well connected by one pivot
language. The most typical approach in this sce-
nario is to independently translate from the source
language to the pivot language and from the pivot
language to the target language (Saha et al., 2016;
Cheng et al., 2017). To improve the transla-
tion performance, Johnson et al. (2016) propose a
multilingual extension of a standard NMT model
and they achieve substantial improvement for lan-
guage pairs without direct parallel training data.

Recently, motivated by the success of cross-
lingual embeddings, researchers begin to show in-
terests in exploring the more ambitious scenario
where an NMT model is trained from monolingual
corpora only. Lample et al. (2017) and Artetxe
et al. (2017b) simultaneously propose an approach
for this scenario, which is based on pre-trained
cross lingual embeddings. Lample et al. (2017)
utilizes a single encoder and a single decoder for
both languages. The entire system is trained to
reconstruct its perturbed input. For cross-lingual
translation, they incorporate back-translation into
the training procedure. Different from (Lample
et al., 2017), Artetxe et al. (2017b) use two in-
dependent decoders with each for one language.
The two works mentioned above both use a sin-
gle shared encoder to guarantee the shared latent
space. However, a concomitant defect is that the
shared encoder is weak in keeping the uniqueness
of each language. Our work also belongs to this
more ambitious scenario, and to the best of our
knowledge, we are one among the ﬁrst endeav-
ors to investigate how to train an NMT model with
monolingual corpora only.

Figure 1: The architecture of the proposed model. We implement the shared-latent space assumption
using a weight sharing constraint where the connection of the last few layers in Encs and Enct are
tied (illustrated with dashed lines) and the connection of the ﬁrst few layers in Decs and Dect are
tied. ˜xEncs−Decs
is
s
the translated sentence from source to target and ˜xEnct−Decs
is the translation in reversed direction.
t
Dl is utilized to assess whether the hidden representation of the encoder is from the source or target
language. Dg1 and Dg2 are used to evaluate whether the translated sentences are realistic for each
language respectively. Z represents the shared-latent space.

are self-reconstructed sentences in each language. ˜xEncs−Dect

and ˜xEnct−Dect

s

t

3 The Approach

3.1 Model Architecture

The model architecture, as illustrated in ﬁgure 1,
is based on the AE and GAN. It consists of seven
sub networks: including two encoders Encs and
Enct, two decoders Decs and Dect, the local dis-
criminator Dl, and the global discriminators Dg1
and Dg2. For the encoder and decoder, we follow
the newly emerged Transformer (Vaswani et al.,
2017). Speciﬁcally, the encoder is composed of a
stack of four identical layers 2. Each layer con-
sists of a multi-head self-attention and a simple
position-wise fully connected feed-forward net-
work. The decoder is also composed of four iden-
tical layers. In addition to the two sub-layers in
each encoder layer, the decoder inserts a third sub-
layer, which performs multi-head attention over
the output of the encoder stack. For more details
about the multi-head self-attention layer, we refer
the reader to (Vaswani et al., 2017). We implement
the local discriminator as a multi-layer perceptron
and implement the global discriminator based on
the convolutional neural network (CNN). Several
ways exist to interpret the roles of the sub net-
works are summarised in table 1. The proposed
system has several striking components , which
are critical either for the system to be trained in an

2The layer number is selected according to our prelimi-

nary experiment, which is presented in appendix A.

unsupervised manner or for improving the transla-
tion performance.

Networks

Roles

{Encs, Decs}
{Enct, Dect}
{Encs, Dect}
{Enct, Decs}
{Encs, Dl}
{Enct, Dl}
{Enct, Decs, Dg1}
{Encs, Dect, Dg2}

AE for source language
AE for target language
translation source → target
translation target → source
1st local GAN (GANl1)
2nd local GAN (GANl2)
1st global GAN (GANg1)
2nd global GAN (GANg2)

Table 1: Interpretation of the roles for the subnet-
works in the proposed system.

Directional self-attention Compared to recur-
rent neural network, a disadvantage of the simple
self-attention mechanism is that the temporal or-
der information is lost. Although the Transformer
applies the positional encoding to the sequence be-
fore processed by the self-attention, how to model
temporal order information within an attention is
still an open question. Following (Shen et al.,
2017), we build the encoders in our model on the
directional self-attention which utilizes the posi-
tional masks to encode temporal order information
into attention output. More concretely, two posi-
tional masks, namely the forward mask M f and

backward mask M b, are calculated as:

(cid:26)

(cid:26)

M f

ij =

M b

ij =

−∞ otherwise

0

0

i < j

i > j

−∞ otherwise

(1)

(2)

With the forward mask M f , the later token only
makes attention connections to the early tokens
in the sequence, and vice versa with the back-
ward mask. Similar to (Zhou et al., 2016; Wang
et al., 2017), we utilize a self-attention network
to process the input sequence in forward direc-
tion. The output of this layer is taken by an upper
self-attention network as input, processed in the
reverse direction.

Weight sharing Based on the shared-latent
space assumption, we apply the weight sharing
constraint to relate the two AEs. Speciﬁcally, we
share the weights of the last few layers of the Encs
and Enct, which are responsible for extracting
high-level representations of the input sentences.
Similarly, we also share the ﬁrst few layers of
the Decs and Dect, which are expected to de-
code high-level representations that are vital for
reconstructing the input sentences. Compared to
(Cheng et al., 2016; Saha et al., 2016) which use
the fully shared encoder, we only share partial
weights for the encoders and decoders. In the pro-
posed model, the independent weights of the two
encoders are expected to learn and encode the hid-
den features about the internal characteristics of
each language, such as the terminology, style, and
sentence structure. The shared weights are utilized
to map the hidden features extracted by the inde-
pendent weights to the shared-latent space.

Embedding reinforced encoder We use pre-
trained cross-lingual embeddings in the encoders
that are kept ﬁxed during training. And the
ﬁxed embeddings are used as a reinforced en-
coding component
Formally,
given the input sequence embedding vectors E =
{e1, . . . , et} and the initial output sequence of the
encoder stack H = {h1, . . . , ht}, we compute Hr
as:

in our encoder.

Hr = g (cid:12) H + (1 − g) (cid:12) E

(3)

where Hr is the ﬁnal output sequence of the en-
coder which will be attended by the decoder (In
Transformer, H is the ﬁnal output of the encoder),
g is a gate unit and computed as:

g = σ(W1E + W2H + b)

(4)

where W1, W2 and b are trainable parameters
and they are shared by the two encoders. The
motivation behind is twofold. Firstly, taking the
ﬁxed cross-lingual embedding as the other encod-
ing component is helpful to reinforce the shared-
latent space. Additionally, from the point of multi-
channel encoders (Xiong et al., 2017), provid-
ing encoding components with different levels of
composition enables the decoder to take pieces of
source sentence at varying composition levels suit-
ing its own linguistic structure.

3.2 Unsupervised Training

Based on the architecture proposed above, we train
the NMT model with the monolingual corpora
only using the following four strategies:

Denoising auto-encoding Firstly, we train the
two AEs to reconstruct their inputs respectively.
In this form, each encoder should learn to com-
pose the embeddings of its corresponding lan-
guage and each decoder is expected to learn to de-
compose this representation into its corresponding
language. Nevertheless, without any constraint,
the AE quickly learns to merely copy every word
one by one, without capturing any internal struc-
ture of the language involved. To address this
problem, we utilize the same strategy of denois-
ing AE (Vincent et al., 2008) and add some noise
to the input sentences (Hill et al., 2016; Artetxe
et al., 2017b). To this end, we shufﬂe the input
sentences randomly. Speciﬁcally, we apply a ran-
dom permutation ε to the input sentence, verifying
the condition:

steps
s

|ε(i) − i| ≤ min(k([

] + 1), n), ∀i ∈ {1, n}
(5)
where n is the length of the input sentence, steps
is the global steps the model has been updated, k
and s are the tunable parameters which can be set
by users beforehand. This way, the system needs
to learn some useful structure of the involved lan-
guages to be able to recover the correct word order.
In practice, we set k = 2 and s = 100000.

Back-translation In spite of denoising auto-
encoding, the training procedure still involves a
single language at each time, without considering
our ﬁnal goal of mapping an input sentence from
the source/target language to the target/source lan-
guage. For the cross language training, we uti-
lize the back-translation approach for our unsu-
pervised training procedure. Back-translation has
shown its great effectiveness on improving NMT

model with monolingual data and has been widely
investigated by (Sennrich et al., 2015a; Zhang and
Zong, 2016).
In our approach, given an input
sentence in a given language, we apply the cor-
responding encoder and the decoder of the other
language to translate it to the other language 3.
By combining the translation with its original sen-
tence, we get a pseudo-parallel corpus which is
utilized to train the model to reconstruct the origi-
nal sentence from its translation.

Local GAN Although the weight sharing con-
straint is vital for the shared-latent space assump-
tion, it alone does not guarantee that the corre-
sponding sentences in two languages will have the
same or similar latent code. To further enforce
the shared-latent space, we train a discriminative
neural network, referred to as the local discrimi-
nator, to classify between the encoding of source
sentences and the encoding of target sentences.
The local discriminator, implemented as a multi-
layer perceptron with two hidden layers of size
256, takes the output of the encoder, i.e., Hr calcu-
lated as equation 3, as input, and produces a binary
prediction about the language of the input sen-
tence. The local discriminator is trained to predict
the language by minimizing the following cross-
entropy loss:

LDl(θDl) =
− Ex∈xs[log p(f = s|Encs(x))]
− Ex∈xt[log p(f = t|Enct(x))]

(6)

where θDl represents the parameters of the local
discriminator and f ∈ {s, t}. The encoders are
trained to fool the local discriminator:

LEncs(θEncs) =
− Ex∈xs[log p(f = t|Encs(x))]

(7)

LEnct(θEnct) =
− Ex∈xt[log p(f = s|Enct(x))]
where θEncs and θEnct are the parameters of the
two encoders.

(8)

Global GAN We apply the global GANs to
ﬁne tune the whole model so that the model is
able to generate sentences undistinguishable from
the true data, i.e., sentences in the training cor-
pus. Different from the local GANs which up-
dates the parameters of the encoders locally, the

global GANs are utilized to update the whole pa-
including the
rameters of the proposed model,
parameters of encoders and decoders. The pro-
posed model has two global GANs: GANg1 and
In GANg1, the Enct and Decs act as
GANg2.
4
the generator, which generates the sentence ˜xt
from xt. The Dg1, implemented based on CNN,
assesses whether the generated sentence ˜xt is the
true target-language sentence or the generated sen-
tence. The global discriminator aims to distin-
guish among the true sentences and generated sen-
tences, and it is trained to minimize its classiﬁ-
cation error rate. During training, the Dg1 feeds
back its assessment to ﬁnetune the encoder Enct
and decoder Decs. Since the machine transla-
tion is a sequence generation problem, following
(Yang et al., 2017), we leverage policy gradient re-
inforcement training to back-propagate the assess-
ment. We apply a similar processing to GANg2
(The details about the architecture of the global
discriminator and the training procedure of the
global GANs can be seen in appendix B and C).

There are two stages in the proposed unsuper-
vised training. In the ﬁrst stage, we train the pro-
posed model with denoising auto-encoding, back-
translation and the local GANs, until no improve-
ment is achieved on the development set. Specif-
ically, we perform one batch of denoising auto-
encoding for the source and target languages, one
batch of back-translation for the two languages,
and another batch of local GAN for the two lan-
guages. In the second stage, we ﬁne tune the pro-
posed model with the global GANs.

4 Experiments and Results

We evaluate the proposed approach on English-
German, English-French and Chinese-to-English
translation tasks 5. We ﬁrstly describe the datasets,
pre-processing and model hyper-parameters we
used, then we introduce the baseline systems, and
ﬁnally we present our experimental results.

4.1 Data Sets and Preprocessing

In English-German and English-French transla-
tion, we make our experiments comparable with
previous work by using the datasets from the

4The ˜xt is ˜xEnct−Decs
t

script for simplicity.

in ﬁgure 1. We omit the super-

3Since the quality of the translation shows little effect on
the performance of the model (Sennrich et al., 2015a), we
simply use greedy decoding for speed.

5The reason that we do not conduct experiments on
English-to-Chinese translation is that we do not get public
test sets for English-to-Chinese.

WMT 2014 and WMT 2016 shared tasks respec-
tively. For Chinese-to-English translation, we use
the datasets from LDC, which has been widely uti-
lized by previous works (Tu et al., 2017; Zhang
et al., 2017a).

WMT14 English-French Similar to (Lample
et al., 2017), we use the full training set of 36M
sentence pairs and we lower-case them and re-
move sentences longer than 50 words, resulting
in a parallel corpus of about 30M pairs of sen-
tences. To guarantee no exact correspondence be-
tween the source and target monolingual sets, we
build monolingual corpora by selecting English
sentences from 15M random pairs, and selecting
the French sentences from the complementary set.
Sentences are encoded with byte-pair encoding
(Sennrich et al., 2015b), which has an English vo-
cabulary of about 32000 tokens, and French vo-
cabulary of about 33000 tokens. We report results
on newstest2014.

WMT16 English-German We follow the same
procedure mentioned above to create monolingual
training corpora for English-German translation,
and we get two monolingual training data of 1.8M
sentences each. The two languages share a vocab-
ulary of about 32000 tokens. We report results on
newstest2016.

LDC Chinese-English For Chinese-to-English
translation, our training data consists of 1.6M sen-
tence pairs randomly extracted from LDC corpora
6. Since the data set is not big enough, we just
build the monolingual data set by randomly shuf-
ﬂing the Chinese and English sentences respec-
tively.
In spite of the fact that some correspon-
dence between examples in these two monolingual
sets may exist, we never utilize this alignment in-
formation in our training procedure (see Section
3.2). Both the Chinese and English sentences are
encoded with byte-pair encoding. We get an En-
glish vocabulary of about 34000 tokens, and Chi-
nese vocabulary of about 38000 tokens. The re-
sults are reported on N IST 02.

Since the proposed system relies on the pre-
trained cross-lingual embeddings, we utilize the
monolingual corpora described above to train the
embeddings for each language independently by
using word2vec (Mikolov et al., 2013). We then
apply the public implementation 7 of the method
proposed by (Artetxe et al., 2017a) to map these

6LDC2002L27,

LDC2002E18,
LDC2003E07, LDC2004T08, LDC2004E12, LDC2005T10

LDC2002T01,

7https://github.com/artetxem/vecmap

embeddings to a shared-latent space 8.

4.2 Model Hyper-parameters and Evaluation

Following the base model
in (Vaswani et al.,
2017), we set the dimension of word embedding
as 512, dropout rate as 0.1 and the head number
as 8. We use beam search with a beam size of 4
and length penalty α = 0.6. The model is im-
plemented in TensorFlow (Abadi et al., 2015) and
trained on up to four K80 GPUs synchronously in
a multi-GPU setup on a single machine.

For model selection, we stop training when the
model achieves no improvement for the tenth eval-
uation on the development set, which is com-
prised of 3000 source and target sentences ex-
tracted randomly from the monolingual training
corpora. Following (Lample et al., 2017), we
translate the source sentences to the target lan-
guage, and then translate the resulting sentences
back to the source language. The quality of the
model is then evaluated by computing the BLEU
score over the original inputs and their reconstruc-
tions via this two-step translation process. The
performance is ﬁnally averaged over two direc-
tions, i.e., from source to target and from target
to source. BLEU (Papineni et al., 2002) is utilized
as the evaluation metric. For Chinese-to-English,
we apply the script mteval-v11b.pl to evaluate the
translation performance. For English-German and
English-French, we evaluate the translation per-
formance with the script multi-belu.pl 9.

4.3 Baseline Systems

Word-by-word translation (WBW) The ﬁrst
is a system that per-
baseline we consider
forms word-by-word translations using the in-
ferred bilingual dictionary. Speciﬁcally, it trans-
lates a sentence word-by-word, replacing each
word with its nearest neighbor in the other lan-
guage.

Lample et al. (2017) The second baseline is
a previous work that uses the same training and
testing sets with this paper. Their model belongs
to the standard attention-based encoder-decoder
framework, which implements the encoder using
a bidirectional long short term memory network
(LSTM) and implements the decoder using a sim-

8The conﬁguration we used to run these open-source

toolkits can be found in appendix D
9https://github.com/moses-

smt/mosesdecoder/blob/617e8c8/scripts/generic/multi-
bleu.perl;mteval-v11b.pl

en-de

de-en

en-fr

fr-en

zh-en

Supervised
Word-by-word
Lample et al. (2017)

24.07
5.85
9.64

26.99
9.34
13.33

30.50
3.60
15.05

30.21
6.80
14.31

40.02
5.09
-

The proposed approach 10.86

14.62

16.97

15.58

14.52

Table 2: The translation performance on English-German, English-French and Chinese-to-English test
sets. The results of (Lample et al., 2017) are copied directly from their paper. We do not present the
results of (Artetxe et al., 2017b) since we use different training sets.

ple forward LSTM. They apply one single encoder
and decoder for the source and target languages.

Supervised training We ﬁnally consider ex-
actly the same model as ours, but trained using the
standard cross-entropy loss on the original parallel
sentences. This model can be viewed as an upper
bound for the proposed unsupervised model.

4.4 Results and Analysis

4.4.1 Number of weight-sharing layers

We ﬁrstly investigate how the number of weight-
sharing layers affects the translation performance.
In this experiment, we vary the number of weight-
sharing layers in the AEs from 0 to 4. Shar-
ing one layer in AEs means sharing one layer
for the encoders and in the meanwhile, shar-
ing one layer for the decoders.
The BLEU
scores of English-to-German, English-to-French
and Chinese-to-English translation tasks are re-
ported in ﬁgure 2. Each curve corresponds to a
different translation task and the x-axis denotes
the number of weight-sharing layers for the AEs.
We ﬁnd that the number of weight-sharing layers
shows much effect on the translation performance.
And the best translation performance is achieved
when only one layer is shared in our system. When
all of the four layers are shared, i.e., only one
shared encoder is utilized, we get poor translation
performance in all of the three translation tasks.
This veriﬁes our conjecture that the shared en-
coder is detrimental to the performance of unsu-
pervised NMT especially for the translation tasks
on distant language pairs. More concretely, for the
related language pair translation, i.e., English-to-
French, the encoder-shared model achieves -0.53
BLEU points decline than the best model where
only one layer is shared. For the more distant lan-
guage pair English-to-German, the encoder-shared
model achieves more signiﬁcant decline, i.e., -0.85
BLEU points decline. And for the most distant

language pair Chinese-to-English, the decline is
as large as -1.66 BLEU points. We explain this as
that the more distant the language pair is, the more
different characteristics they have. And the shared
encoder is weak in keeping the unique characteris-
tic of each language. Additionally, we also notice
that using two completely independent encoders,
i.e., setting the number of weight-sharing layers
as 0, results in poor translation performance too.
This conﬁrms our intuition that the shared layers
are vital to map the source and target latent rep-
resentations to a shared-latent space. In the rest
of our experiments, we set the number of weight-
sharing layer as 1.

Figure 2: The effects of the weight-sharing layer
number on English-to-German, English-to-French
and Chinese-to-English translation tasks.

4.4.2 Translation results

Table 2 shows the BLEU scores on English-
German, English-French and English-to-Chinese
test sets. As it can be seen, the proposed ap-
proach obtains signiﬁcant improvements than the
word-by-word baseline system, with at least +5.01
BLEU points in English-to-German translation
and up to +13.37 BLEU points in English-to-
French translation. This shows that the proposed

en-de

de-en

en-fr

fr-en

zh-en

Without weight sharing
Without embedding-reinforced encoder
Without directional self-attention
Without local GANs
Without Global GANs
Full model

10.23
10.45
10.60
10.51
10.34
10.86

13.84
14.17
14.21
14.35
14.05
14.62

16.02
16.55
16.82
16.40
16.19
16.97

14.82
15.27
15.30
15.07
15.21
15.58

13.75
14.10
14.29
14.12
14.09
14.52

Table 3: Ablation study on English-German, English-French and Chinese-to-English translation tasks.
Without weight sharing means no layers are shared in the two AEs.

model only trained with monolingual data effec-
tively learns to use the context information and
the internal structure of each language. Com-
pared to the work of (Lample et al., 2017), our
model also achieves up to +1.92 BLEU points im-
provement on English-to-French translation task.
We believe that the unsupervised NMT is very
promising. However, there is still a large room
for improvement compared to the supervised up-
per bound. The gap between the supervised and
unsupervised model is as large as 12.3-25.5 BLEU
points depending on the language pair and transla-
tion direction.

4.4.3 Ablation study
To understand the importance of different com-
ponents of the proposed system, we perform an
ablation study by training multiple versions of
the
our model with some missing components:
local GANs,
the directional
the global GANs,
self-attention, the weight-sharing, the embedding-
reinforced encoders, etc. Results are reported
in table 3. We do not test the the importance
of the auto-encoding, back-translation and the
pre-trained embeddings because they have been
widely tested in (Lample et al., 2017; Artetxe
et al., 2017b). Table 3 shows that the best per-
formance is obtained with the simultaneous use of
all the tested elements. The most critical compo-
nent is the weight-sharing constraint, which is vi-
tal to map sentences of different languages to the
shared-latent space. The embedding-reinforced
encoder also brings some improvement on all of
the translation tasks. When we remove the di-
rectional self-attention, we get up to -0.3 BLEU
points decline. This indicates that it deserves more
efforts to investigate the temporal order informa-
tion in self-attention mechanism. The GANs also
signiﬁcantly improve the translation performance
the global GANs
of our system. Speciﬁcally,

achieve improvement up to +0.78 BLEU points on
English-to-French translation and the local GANs
also obtain improvement up to +0.57 BLEU points
on English-to-French translation. This reveals that
the proposed model beneﬁts a lot from the cross-
domain loss deﬁned by GANs.

5 Conclusion and Future work

The models proposed recently for unsupervised
NMT use a single encoder to map sentences from
different languages to a shared-latent space. We
conjecture that the shared encoder is problem-
atic for keeping the unique and inherent char-
acteristic of each language.
In this paper, we
propose the weight-sharing constraint in unsuper-
vised NMT to address this issue. To enhance the
cross-language translation performance, we also
propose the embedding-reinforced encoders, local
GAN and global GAN into the proposed system.
Additionally, the directional self-attention is intro-
duced to model the temporal order information for
our system.
We test

the proposed model on English-
German, English-French and Chinese-to-English
translation tasks. The experimental results reveal
that our approach achieves signiﬁcant improve-
ment and verify our conjecture that the shared en-
coder is really a bottleneck for improving the un-
supervised NMT. The ablation study shows that
each component of our system achieves some im-
provement for the ﬁnal translation performance.

Unsupervised NMT opens exciting opportuni-
ties for the future research. However, there is
still a large room for improvement compared to
the supervised NMT. In the future, we would like
to investigate how to utilize the monolingual data
more effectively, such as incorporating the lan-
guage model and syntactic information into unsu-
pervised NMT. Besides, we decide to make more

efforts to explore how to reinforce the temporal or-
der information for the proposed model.

International Joint Conference on Artiﬁcial Intelli-
gence. pages 3974–3980.

Acknowledgements

This work is supported by the National Key Re-
search and Development Program of China un-
der Grant No. 2017YFB1002102, and Beijing
Engineering Research Center under Grant No.
Z171100002217015. We would like to thank Xu
Shuang for her preparing data used in this work.
Additionally, we also want to thank Jiaming Xu,
Suncong Zheng and Wenfu Wang for their invalu-
able discussions on this work.

References

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Man´e, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Vi´egas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems .

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Conference on Empirical Methods in Natural
Language Processing. pages 2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2017a. Learning bilingual word embeddings with
(almost) no bilingual data. In Meeting of the Asso-
ciation for Computational Linguistics. pages 451–
462.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2017b. Unsupervised neural ma-
chine translation .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473 .

Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, and
Wei Xu. 2016. Neural machine translation with
pivot languages. arXiv preprint arXiv:1611.04928
.

Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, Wei
Xu, Yong Cheng, Qian Yang, Yang Liu, Maosong
Joint training for pivot-
Sun, and Wei Xu. 2017.
In Twenty-Sixth
based neural machine translation.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Learning
Schwenk, and Yoshua Bengio. 2014.
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Herv Jgou. 2017.
Word translation without parallel data .

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning .

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. TACL .

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vi´egas, Martin Wattenberg, Greg Corrado,
et al. 2016. Google’s multilingual neural machine
translation system: Enabling zero-shot translation.
arXiv preprint arXiv:1611.04558 .

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. EMNLP pages
1700–1709.

Guillaume

Ludovic Denoyer,

and
Lample,
Marc’Aurelio Ranzato. 2017.
Unsupervised
machine translation using monolingual corpora only
.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems. pages 3111–3119.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. Association for Com-
putational Linguistics pages 311–318.

Amrita Saha, Mitesh M Khapra, Sarath Chandar, Ja-
narthanan Rajendran, and Kyunghyun Cho. 2016.
A correlational encoder decoder architecture for
arXiv preprint
pivot based sequence generation.
arXiv:1606.04754 .

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation
arXiv preprint

2015a.
models with monolingual data.
arXiv:1511.06709 .

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015b. Neural machine translation of rare words
with subword units. Computer Science .

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. arXiv
preprint arXiv:1512.02433 .

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding .

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. Advances in neural information processing
systems pages 3104–3112.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2017. Learning to remember translation his-
tory with a continuous cache .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need .

Pascal Vincent, Hugo Larochelle, Yoshua Bengio,
and Pierre-Antoine Manzagol. 2008. Extracting
and composing robust features with denoising au-
In Proceedings of the 25th interna-
toencoders.
tional conference on Machine learning. ACM, pages
1096–1103.

Mingxuan Wang, Zhengdong Lu,

Jie Zhou, and
Qun Liu. 2017. Deep neural machine transla-
arXiv preprint
tion with linear associative unit.
arXiv:1705.00861 .

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016.
Google’s neural ma-
chine translation system: Bridging the gap between
arXiv preprint
human and machine translation.
arXiv:1609.08144 .

bilingual lexicon induction. In Meeting of the Asso-
ciation for Computational Linguistics. pages 1959–
1970.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei
Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. arXiv
preprint arXiv:1606.04199 .

A Experiments on the layer number for

encoders and decoders

To determine the number of layers for encoders
and decoders in our system beforehand, we con-
duct experiments on English-German translation
tasks to test how the amount of layers in encoders
and decoders affects the translation performance.
We vary the number of layers from 2 to 6 and the
results are reported in table 4. We can ﬁnd that the
translation performance achieves substantial im-
provement with the layer number increasing from
2 to 4. However, with layer number set larger than
4, we get little improvement. To make a trade-off
between the translation performance and the com-
putation complexity, we set the layer number as 4
for our encoders and decoders.

layer num en-de

de-en

2
3
4
5
6

11.57
12.43
12.86
12.91
12.95

14.01
14.99
15.62
15.83
15.79

Table 4: The experiments on the number of layers
for encoders and decoders.

Hao Xiong, Zhongjun He, Xiaoguang Hu, and Hua Wu.
2017. Multi-channel encoder for neural machine
translation. arXiv preprint arXiv:1712.02109 .

B The architecture of the global

discriminator

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2017.
Improving neural machine translation with condi-
tional sequence generative adversarial nets .

Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu,
and Maosong Sun. 2017a. Prior knowledge integra-
tion for neural machine translation using posterior
In Meeting of the Association for
regularization.
Computational Linguistics. pages 1514–1523.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Conference on Empirical Methods in
Natural Language Processing. pages 1535–1545.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Adversarial training for unsupervised

The global discriminator is applied to classify the
generated sentences as source language, target lan-
guage or generated sentences. Following (Yang
et al., 2017), we implement the global discrimina-
tor based on CNN. Since sentences generated by
the generator (the composition of the encoder and
decoder) have variable lengths, the CNN padding
is used to transform the sentences to sequences
with ﬁxed length T , which is the maximum length
set for the output of the generator. Given the gen-
erated sequences x1, . . . , xT , we build the matrix
X1:T as:

X1:T = x1; x2; . . . ; xT

(9)

where xt ∈ Rk is the k-dimensional word embed-
ding and the semicolon is the concatenation oper-
ator. For the matrix X1:T , a kernel wj ∈ Rl×k
applies a convolutional operation to a window size
of l words to produce a series of feature maps:

cji = ρ(BN (wj ⊗ Xi:i+l−1 + b))

(10)

where ⊗ operator is the summation of element-
wise production and b is a bias term. ρ is a non-
linear activation function which is implemented as
ReLu in this paper. To get the ﬁnal feature with
respect to kernel wj, a max-over-time pooling op-
eration is leveraged over the feature maps:

(cid:101)cj = max{cj1, . . . , cjT −l+1}

(11)

We use various numbers of kernels with different
window sizes to extract different features, which
are then concatenated to form the ﬁnal sentence
representation xc. Finally, we pass xc through a
fully connected layer and a softmax layer to gen-
erate the probability p(fg|x1, . . . , xT ) as:

p(fg|x1, . . . , xT ) = sof tmax(V ∗ xc)

(12)

where V is the transformation matrix and fg ∈
{true, generated}.

C The training procedure of the global

GAN

We apply the global GANs to ﬁnetune the whole
model. Here, we provide detailed strategies for
training the global GANs. Firstly, we generate the
machine-generated source language sentences by
using Enct and Encs to decode the monolingual
data in target language. Similarly, we get the gen-
erated sentences in target language with Encs and
Dect by decoding source language monolingual
data. We simply use the greedy sampling method
instead of the beam search method for decoding.
Next, we pre-train Dg1 on the combination of true
monolingual data and the generated data in the
source language. Similarly, we also pre-train Dg2
on the combination of true monolingual data and
the generated data in the target language. Finally,
we jointly train the generators and discriminators.
The generators are trained with policy gradient
training methods. For the details about the pol-
icy gradient training, we refer the reader to (Yang
et al., 2017).

D The conﬁgurations for the open-source

toolkits

We train the word embedding use the following
script:

./word2vec -train text -output embedding.txt -
cbow 0 -size 512 -window 10 -negative 10 -hs 0
-sample 1e- -threads 50 -binary 0 -min-count 5 -
iter 10

After we get the embeddings for both the source
and target
languages, we use the open-source
VecMap 10 to map these embeddings to a shared-
latent space with the following scripts:

python3 normalize embeddings.py unit center -i

s embedding.txt -o s embedding.normalized.txt

python3 normalize embeddings.py unit center -i

t embedding.txt -o t embedding.normalized.txt

map embeddings.py

–
s embedding.normalized.txt

python3
orthogonal
t embedding.normalized.txt
s embedding.mapped.txt t embedding.mapped.txt
–numerals –self learning -v

10https://github.com/artetxem/vecmap

Unsupervised Neural Machine Translation with Weight Sharing

Zhen Yang1,2, Wei Chen1 , Feng Wang1,2∗, Bo Xu1
1Institute of Automation, Chinese Academy of Sciences
2University of Chinese Academy of Sciences
{yangzhen2014, wei.chen.media, feng.wang, xubo}@ia.ac.cn

8
1
0
2
 
r
p
A
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
7
5
0
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract

Unsupervised neural machine translation
(NMT) is a recently proposed approach
for machine translation which aims to train
the model without using any labeled data.
The models proposed for unsupervised
NMT often use only one shared encoder
to map the pairs of sentences from dif-
ferent languages to a shared-latent space,
which is weak in keeping the unique
and internal characteristics of each lan-
guage, such as the style, terminology, and
sentence structure. To address this is-
sue, we introduce an extension by utiliz-
ing two independent encoders but shar-
ing some partial weights which are re-
sponsible for extracting high-level repre-
sentations of the input sentences. Be-
sides, two different generative adversarial
networks (GANs), namely the local GAN
and global GAN, are proposed to enhance
the cross-language translation. With this
new approach, we achieve signiﬁcant im-
provements on English-German, English-
French and Chinese-to-English translation
tasks.

1

Introduction

Neural machine translation (Kalchbrenner and
Blunsom, 2013; Sutskever et al., 2014; Cho et al.,
2014; Bahdanau et al., 2014), directly applying a
single neural network to transform the source sen-
tence into the target sentence, has now reached im-
pressive performance (Shen et al., 2015; Wu et al.,
2016; Johnson et al., 2016; Gehring et al., 2017;
Vaswani et al., 2017). The NMT typically con-
sists of two sub neural networks. The encoder net-
work reads and encodes the source sentence into a

1Feng Wang is the corresponding author of this paper

context vector, and the decoder network generates
the target sentence iteratively based on the con-
text vector. NMT can be studied in supervised and
unsupervised learning settings. In the supervised
setting, bilingual corpora is available for training
the NMT model. In the unsupervised setting, we
only have two independent monolingual corpora
with one for each language and there is no bilin-
gual training example to provide alignment infor-
mation for the two languages. Due to lack of align-
ment information, the unsupervised NMT is con-
sidered more challenging. However, this task is
very promising, since the monolingual corpora is
usually easy to be collected.

Motivated by recent success in unsupervised
cross-lingual embeddings (Artetxe et al., 2016;
Zhang et al., 2017b; Conneau et al., 2017), the
models proposed for unsupervised NMT often as-
sume that a pair of sentences from two different
languages can be mapped to a same latent repre-
sentation in a shared-latent space (Lample et al.,
2017; Artetxe et al., 2017b). Following this as-
sumption, Lample et al. (2017) use a single en-
coder and a single decoder for both the source and
target languages. The encoder and decoder, act-
ing as a standard auto-encoder (AE), are trained to
reconstruct the inputs. And Artetxe et al. (2017b)
utilize a shared encoder but two independent de-
coders. With some good performance, they share
a glaring defect, i.e., only one encoder is shared
by the source and target languages. Although the
shared encoder is vital for mapping sentences from
different languages into the shared-latent space,
it is weak in keeping the uniqueness and inter-
nal characteristics of each language, such as the
style, terminology and sentence structure. Since
each language has its own characteristics,
the
source and target languages should be encoded
and learned independently. Therefore, we conjec-
ture that the shared encoder may be a factor limit-

ing the potential translation performance.

In order to address this issue, we extend the
encoder-shared model, i.e., the model with one
shared encoder, by leveraging two independent
encoders with each for one language. Similarly,
two independent decoders are utilized. For each
language, the encoder and its corresponding de-
coder perform an AE, where the encoder gener-
ates the latent representations from the perturbed
input sentences and the decoder reconstructs the
sentences from the latent representations. To map
the latent representations from different languages
to a shared-latent space, we propose the weight-
sharing constraint to the two AEs. Speciﬁcally,
we share the weights of the last few layers of two
encoders that are responsible for extracting high-
level representations of input sentences. Simi-
larly, we share the weights of the ﬁrst few lay-
ers of two decoders. To enforce the shared-latent
space, the word embeddings are used as a rein-
forced encoding component in our encoders. For
cross-language translation, we utilize the back-
translation following (Lample et al., 2017). Addi-
tionally, two different generative adversarial net-
works (GAN) (Yang et al., 2017), namely the lo-
cal and global GAN, are proposed to further im-
prove the cross-language translation. We utilize
the local GAN to constrain the source and tar-
get latent representations to have the same distri-
bution, whereby the encoder tries to fool a local
discriminator which is simultaneously trained to
distinguish the language of a given latent repre-
sentation. We apply the global GAN to ﬁnetune
the corresponding generator, i.e., the composition
of the encoder and decoder of the other language,
where a global discriminator is leveraged to guide
the training of the generator by assessing how far
the generated sentence is from the true data distri-
bution 1. In summary, we mainly make the follow-
ing contributions:

• We propose the weight-sharing constraint to
unsupervised NMT, enabling the model to
utilize an independent encoder for each lan-
guage. To enforce the shared-latent space,
we also propose the embedding-reinforced
encoders and two different GANs for our
model.

• We

conduct

extensive

experiments on

1The

code

evaluate

and
https://github.com/ZhenYangIACAS/unsupervised-NMT

that

we
our models

utilized
be
can

to
found

train
at

English-German,
English-French
and
Ex-
Chinese-to-English translation tasks.
perimental results show that the proposed
approach consistently achieves great success.

• Last but not least, we introduce the direc-
tional self-attention to model temporal order
information for the proposed model. Exper-
imental results reveal that it deserves more
efforts for researchers to investigate the tem-
poral order information within self-attention
layers of NMT.

2 Related Work

Several approaches have been proposed to train
NMT models without direct parallel corpora. The
scenario that has been widely investigated is one
where two languages have little parallel data be-
tween them but are well connected by one pivot
language. The most typical approach in this sce-
nario is to independently translate from the source
language to the pivot language and from the pivot
language to the target language (Saha et al., 2016;
Cheng et al., 2017). To improve the transla-
tion performance, Johnson et al. (2016) propose a
multilingual extension of a standard NMT model
and they achieve substantial improvement for lan-
guage pairs without direct parallel training data.

Recently, motivated by the success of cross-
lingual embeddings, researchers begin to show in-
terests in exploring the more ambitious scenario
where an NMT model is trained from monolingual
corpora only. Lample et al. (2017) and Artetxe
et al. (2017b) simultaneously propose an approach
for this scenario, which is based on pre-trained
cross lingual embeddings. Lample et al. (2017)
utilizes a single encoder and a single decoder for
both languages. The entire system is trained to
reconstruct its perturbed input. For cross-lingual
translation, they incorporate back-translation into
the training procedure. Different from (Lample
et al., 2017), Artetxe et al. (2017b) use two in-
dependent decoders with each for one language.
The two works mentioned above both use a sin-
gle shared encoder to guarantee the shared latent
space. However, a concomitant defect is that the
shared encoder is weak in keeping the uniqueness
of each language. Our work also belongs to this
more ambitious scenario, and to the best of our
knowledge, we are one among the ﬁrst endeav-
ors to investigate how to train an NMT model with
monolingual corpora only.

Figure 1: The architecture of the proposed model. We implement the shared-latent space assumption
using a weight sharing constraint where the connection of the last few layers in Encs and Enct are
tied (illustrated with dashed lines) and the connection of the ﬁrst few layers in Decs and Dect are
tied. ˜xEncs−Decs
is
s
the translated sentence from source to target and ˜xEnct−Decs
is the translation in reversed direction.
t
Dl is utilized to assess whether the hidden representation of the encoder is from the source or target
language. Dg1 and Dg2 are used to evaluate whether the translated sentences are realistic for each
language respectively. Z represents the shared-latent space.

are self-reconstructed sentences in each language. ˜xEncs−Dect

and ˜xEnct−Dect

s

t

3 The Approach

3.1 Model Architecture

The model architecture, as illustrated in ﬁgure 1,
is based on the AE and GAN. It consists of seven
sub networks: including two encoders Encs and
Enct, two decoders Decs and Dect, the local dis-
criminator Dl, and the global discriminators Dg1
and Dg2. For the encoder and decoder, we follow
the newly emerged Transformer (Vaswani et al.,
2017). Speciﬁcally, the encoder is composed of a
stack of four identical layers 2. Each layer con-
sists of a multi-head self-attention and a simple
position-wise fully connected feed-forward net-
work. The decoder is also composed of four iden-
tical layers. In addition to the two sub-layers in
each encoder layer, the decoder inserts a third sub-
layer, which performs multi-head attention over
the output of the encoder stack. For more details
about the multi-head self-attention layer, we refer
the reader to (Vaswani et al., 2017). We implement
the local discriminator as a multi-layer perceptron
and implement the global discriminator based on
the convolutional neural network (CNN). Several
ways exist to interpret the roles of the sub net-
works are summarised in table 1. The proposed
system has several striking components , which
are critical either for the system to be trained in an

2The layer number is selected according to our prelimi-

nary experiment, which is presented in appendix A.

unsupervised manner or for improving the transla-
tion performance.

Networks

Roles

{Encs, Decs}
{Enct, Dect}
{Encs, Dect}
{Enct, Decs}
{Encs, Dl}
{Enct, Dl}
{Enct, Decs, Dg1}
{Encs, Dect, Dg2}

AE for source language
AE for target language
translation source → target
translation target → source
1st local GAN (GANl1)
2nd local GAN (GANl2)
1st global GAN (GANg1)
2nd global GAN (GANg2)

Table 1: Interpretation of the roles for the subnet-
works in the proposed system.

Directional self-attention Compared to recur-
rent neural network, a disadvantage of the simple
self-attention mechanism is that the temporal or-
der information is lost. Although the Transformer
applies the positional encoding to the sequence be-
fore processed by the self-attention, how to model
temporal order information within an attention is
still an open question. Following (Shen et al.,
2017), we build the encoders in our model on the
directional self-attention which utilizes the posi-
tional masks to encode temporal order information
into attention output. More concretely, two posi-
tional masks, namely the forward mask M f and

backward mask M b, are calculated as:

(cid:26)

(cid:26)

M f

ij =

M b

ij =

−∞ otherwise

0

0

i < j

i > j

−∞ otherwise

(1)

(2)

With the forward mask M f , the later token only
makes attention connections to the early tokens
in the sequence, and vice versa with the back-
ward mask. Similar to (Zhou et al., 2016; Wang
et al., 2017), we utilize a self-attention network
to process the input sequence in forward direc-
tion. The output of this layer is taken by an upper
self-attention network as input, processed in the
reverse direction.

Weight sharing Based on the shared-latent
space assumption, we apply the weight sharing
constraint to relate the two AEs. Speciﬁcally, we
share the weights of the last few layers of the Encs
and Enct, which are responsible for extracting
high-level representations of the input sentences.
Similarly, we also share the ﬁrst few layers of
the Decs and Dect, which are expected to de-
code high-level representations that are vital for
reconstructing the input sentences. Compared to
(Cheng et al., 2016; Saha et al., 2016) which use
the fully shared encoder, we only share partial
weights for the encoders and decoders. In the pro-
posed model, the independent weights of the two
encoders are expected to learn and encode the hid-
den features about the internal characteristics of
each language, such as the terminology, style, and
sentence structure. The shared weights are utilized
to map the hidden features extracted by the inde-
pendent weights to the shared-latent space.

Embedding reinforced encoder We use pre-
trained cross-lingual embeddings in the encoders
that are kept ﬁxed during training. And the
ﬁxed embeddings are used as a reinforced en-
coding component
Formally,
given the input sequence embedding vectors E =
{e1, . . . , et} and the initial output sequence of the
encoder stack H = {h1, . . . , ht}, we compute Hr
as:

in our encoder.

Hr = g (cid:12) H + (1 − g) (cid:12) E

(3)

where Hr is the ﬁnal output sequence of the en-
coder which will be attended by the decoder (In
Transformer, H is the ﬁnal output of the encoder),
g is a gate unit and computed as:

g = σ(W1E + W2H + b)

(4)

where W1, W2 and b are trainable parameters
and they are shared by the two encoders. The
motivation behind is twofold. Firstly, taking the
ﬁxed cross-lingual embedding as the other encod-
ing component is helpful to reinforce the shared-
latent space. Additionally, from the point of multi-
channel encoders (Xiong et al., 2017), provid-
ing encoding components with different levels of
composition enables the decoder to take pieces of
source sentence at varying composition levels suit-
ing its own linguistic structure.

3.2 Unsupervised Training

Based on the architecture proposed above, we train
the NMT model with the monolingual corpora
only using the following four strategies:

Denoising auto-encoding Firstly, we train the
two AEs to reconstruct their inputs respectively.
In this form, each encoder should learn to com-
pose the embeddings of its corresponding lan-
guage and each decoder is expected to learn to de-
compose this representation into its corresponding
language. Nevertheless, without any constraint,
the AE quickly learns to merely copy every word
one by one, without capturing any internal struc-
ture of the language involved. To address this
problem, we utilize the same strategy of denois-
ing AE (Vincent et al., 2008) and add some noise
to the input sentences (Hill et al., 2016; Artetxe
et al., 2017b). To this end, we shufﬂe the input
sentences randomly. Speciﬁcally, we apply a ran-
dom permutation ε to the input sentence, verifying
the condition:

steps
s

|ε(i) − i| ≤ min(k([

] + 1), n), ∀i ∈ {1, n}
(5)
where n is the length of the input sentence, steps
is the global steps the model has been updated, k
and s are the tunable parameters which can be set
by users beforehand. This way, the system needs
to learn some useful structure of the involved lan-
guages to be able to recover the correct word order.
In practice, we set k = 2 and s = 100000.

Back-translation In spite of denoising auto-
encoding, the training procedure still involves a
single language at each time, without considering
our ﬁnal goal of mapping an input sentence from
the source/target language to the target/source lan-
guage. For the cross language training, we uti-
lize the back-translation approach for our unsu-
pervised training procedure. Back-translation has
shown its great effectiveness on improving NMT

model with monolingual data and has been widely
investigated by (Sennrich et al., 2015a; Zhang and
Zong, 2016).
In our approach, given an input
sentence in a given language, we apply the cor-
responding encoder and the decoder of the other
language to translate it to the other language 3.
By combining the translation with its original sen-
tence, we get a pseudo-parallel corpus which is
utilized to train the model to reconstruct the origi-
nal sentence from its translation.

Local GAN Although the weight sharing con-
straint is vital for the shared-latent space assump-
tion, it alone does not guarantee that the corre-
sponding sentences in two languages will have the
same or similar latent code. To further enforce
the shared-latent space, we train a discriminative
neural network, referred to as the local discrimi-
nator, to classify between the encoding of source
sentences and the encoding of target sentences.
The local discriminator, implemented as a multi-
layer perceptron with two hidden layers of size
256, takes the output of the encoder, i.e., Hr calcu-
lated as equation 3, as input, and produces a binary
prediction about the language of the input sen-
tence. The local discriminator is trained to predict
the language by minimizing the following cross-
entropy loss:

LDl(θDl) =
− Ex∈xs[log p(f = s|Encs(x))]
− Ex∈xt[log p(f = t|Enct(x))]

(6)

where θDl represents the parameters of the local
discriminator and f ∈ {s, t}. The encoders are
trained to fool the local discriminator:

LEncs(θEncs) =
− Ex∈xs[log p(f = t|Encs(x))]

(7)

LEnct(θEnct) =
− Ex∈xt[log p(f = s|Enct(x))]
where θEncs and θEnct are the parameters of the
two encoders.

(8)

Global GAN We apply the global GANs to
ﬁne tune the whole model so that the model is
able to generate sentences undistinguishable from
the true data, i.e., sentences in the training cor-
pus. Different from the local GANs which up-
dates the parameters of the encoders locally, the

global GANs are utilized to update the whole pa-
including the
rameters of the proposed model,
parameters of encoders and decoders. The pro-
posed model has two global GANs: GANg1 and
In GANg1, the Enct and Decs act as
GANg2.
4
the generator, which generates the sentence ˜xt
from xt. The Dg1, implemented based on CNN,
assesses whether the generated sentence ˜xt is the
true target-language sentence or the generated sen-
tence. The global discriminator aims to distin-
guish among the true sentences and generated sen-
tences, and it is trained to minimize its classiﬁ-
cation error rate. During training, the Dg1 feeds
back its assessment to ﬁnetune the encoder Enct
and decoder Decs. Since the machine transla-
tion is a sequence generation problem, following
(Yang et al., 2017), we leverage policy gradient re-
inforcement training to back-propagate the assess-
ment. We apply a similar processing to GANg2
(The details about the architecture of the global
discriminator and the training procedure of the
global GANs can be seen in appendix B and C).

There are two stages in the proposed unsuper-
vised training. In the ﬁrst stage, we train the pro-
posed model with denoising auto-encoding, back-
translation and the local GANs, until no improve-
ment is achieved on the development set. Specif-
ically, we perform one batch of denoising auto-
encoding for the source and target languages, one
batch of back-translation for the two languages,
and another batch of local GAN for the two lan-
guages. In the second stage, we ﬁne tune the pro-
posed model with the global GANs.

4 Experiments and Results

We evaluate the proposed approach on English-
German, English-French and Chinese-to-English
translation tasks 5. We ﬁrstly describe the datasets,
pre-processing and model hyper-parameters we
used, then we introduce the baseline systems, and
ﬁnally we present our experimental results.

4.1 Data Sets and Preprocessing

In English-German and English-French transla-
tion, we make our experiments comparable with
previous work by using the datasets from the

4The ˜xt is ˜xEnct−Decs
t

script for simplicity.

in ﬁgure 1. We omit the super-

3Since the quality of the translation shows little effect on
the performance of the model (Sennrich et al., 2015a), we
simply use greedy decoding for speed.

5The reason that we do not conduct experiments on
English-to-Chinese translation is that we do not get public
test sets for English-to-Chinese.

WMT 2014 and WMT 2016 shared tasks respec-
tively. For Chinese-to-English translation, we use
the datasets from LDC, which has been widely uti-
lized by previous works (Tu et al., 2017; Zhang
et al., 2017a).

WMT14 English-French Similar to (Lample
et al., 2017), we use the full training set of 36M
sentence pairs and we lower-case them and re-
move sentences longer than 50 words, resulting
in a parallel corpus of about 30M pairs of sen-
tences. To guarantee no exact correspondence be-
tween the source and target monolingual sets, we
build monolingual corpora by selecting English
sentences from 15M random pairs, and selecting
the French sentences from the complementary set.
Sentences are encoded with byte-pair encoding
(Sennrich et al., 2015b), which has an English vo-
cabulary of about 32000 tokens, and French vo-
cabulary of about 33000 tokens. We report results
on newstest2014.

WMT16 English-German We follow the same
procedure mentioned above to create monolingual
training corpora for English-German translation,
and we get two monolingual training data of 1.8M
sentences each. The two languages share a vocab-
ulary of about 32000 tokens. We report results on
newstest2016.

LDC Chinese-English For Chinese-to-English
translation, our training data consists of 1.6M sen-
tence pairs randomly extracted from LDC corpora
6. Since the data set is not big enough, we just
build the monolingual data set by randomly shuf-
ﬂing the Chinese and English sentences respec-
tively.
In spite of the fact that some correspon-
dence between examples in these two monolingual
sets may exist, we never utilize this alignment in-
formation in our training procedure (see Section
3.2). Both the Chinese and English sentences are
encoded with byte-pair encoding. We get an En-
glish vocabulary of about 34000 tokens, and Chi-
nese vocabulary of about 38000 tokens. The re-
sults are reported on N IST 02.

Since the proposed system relies on the pre-
trained cross-lingual embeddings, we utilize the
monolingual corpora described above to train the
embeddings for each language independently by
using word2vec (Mikolov et al., 2013). We then
apply the public implementation 7 of the method
proposed by (Artetxe et al., 2017a) to map these

6LDC2002L27,

LDC2002E18,
LDC2003E07, LDC2004T08, LDC2004E12, LDC2005T10

LDC2002T01,

7https://github.com/artetxem/vecmap

embeddings to a shared-latent space 8.

4.2 Model Hyper-parameters and Evaluation

Following the base model
in (Vaswani et al.,
2017), we set the dimension of word embedding
as 512, dropout rate as 0.1 and the head number
as 8. We use beam search with a beam size of 4
and length penalty α = 0.6. The model is im-
plemented in TensorFlow (Abadi et al., 2015) and
trained on up to four K80 GPUs synchronously in
a multi-GPU setup on a single machine.

For model selection, we stop training when the
model achieves no improvement for the tenth eval-
uation on the development set, which is com-
prised of 3000 source and target sentences ex-
tracted randomly from the monolingual training
corpora. Following (Lample et al., 2017), we
translate the source sentences to the target lan-
guage, and then translate the resulting sentences
back to the source language. The quality of the
model is then evaluated by computing the BLEU
score over the original inputs and their reconstruc-
tions via this two-step translation process. The
performance is ﬁnally averaged over two direc-
tions, i.e., from source to target and from target
to source. BLEU (Papineni et al., 2002) is utilized
as the evaluation metric. For Chinese-to-English,
we apply the script mteval-v11b.pl to evaluate the
translation performance. For English-German and
English-French, we evaluate the translation per-
formance with the script multi-belu.pl 9.

4.3 Baseline Systems

Word-by-word translation (WBW) The ﬁrst
is a system that per-
baseline we consider
forms word-by-word translations using the in-
ferred bilingual dictionary. Speciﬁcally, it trans-
lates a sentence word-by-word, replacing each
word with its nearest neighbor in the other lan-
guage.

Lample et al. (2017) The second baseline is
a previous work that uses the same training and
testing sets with this paper. Their model belongs
to the standard attention-based encoder-decoder
framework, which implements the encoder using
a bidirectional long short term memory network
(LSTM) and implements the decoder using a sim-

8The conﬁguration we used to run these open-source

toolkits can be found in appendix D
9https://github.com/moses-

smt/mosesdecoder/blob/617e8c8/scripts/generic/multi-
bleu.perl;mteval-v11b.pl

en-de

de-en

en-fr

fr-en

zh-en

Supervised
Word-by-word
Lample et al. (2017)

24.07
5.85
9.64

26.99
9.34
13.33

30.50
3.60
15.05

30.21
6.80
14.31

40.02
5.09
-

The proposed approach 10.86

14.62

16.97

15.58

14.52

Table 2: The translation performance on English-German, English-French and Chinese-to-English test
sets. The results of (Lample et al., 2017) are copied directly from their paper. We do not present the
results of (Artetxe et al., 2017b) since we use different training sets.

ple forward LSTM. They apply one single encoder
and decoder for the source and target languages.

Supervised training We ﬁnally consider ex-
actly the same model as ours, but trained using the
standard cross-entropy loss on the original parallel
sentences. This model can be viewed as an upper
bound for the proposed unsupervised model.

4.4 Results and Analysis

4.4.1 Number of weight-sharing layers

We ﬁrstly investigate how the number of weight-
sharing layers affects the translation performance.
In this experiment, we vary the number of weight-
sharing layers in the AEs from 0 to 4. Shar-
ing one layer in AEs means sharing one layer
for the encoders and in the meanwhile, shar-
ing one layer for the decoders.
The BLEU
scores of English-to-German, English-to-French
and Chinese-to-English translation tasks are re-
ported in ﬁgure 2. Each curve corresponds to a
different translation task and the x-axis denotes
the number of weight-sharing layers for the AEs.
We ﬁnd that the number of weight-sharing layers
shows much effect on the translation performance.
And the best translation performance is achieved
when only one layer is shared in our system. When
all of the four layers are shared, i.e., only one
shared encoder is utilized, we get poor translation
performance in all of the three translation tasks.
This veriﬁes our conjecture that the shared en-
coder is detrimental to the performance of unsu-
pervised NMT especially for the translation tasks
on distant language pairs. More concretely, for the
related language pair translation, i.e., English-to-
French, the encoder-shared model achieves -0.53
BLEU points decline than the best model where
only one layer is shared. For the more distant lan-
guage pair English-to-German, the encoder-shared
model achieves more signiﬁcant decline, i.e., -0.85
BLEU points decline. And for the most distant

language pair Chinese-to-English, the decline is
as large as -1.66 BLEU points. We explain this as
that the more distant the language pair is, the more
different characteristics they have. And the shared
encoder is weak in keeping the unique characteris-
tic of each language. Additionally, we also notice
that using two completely independent encoders,
i.e., setting the number of weight-sharing layers
as 0, results in poor translation performance too.
This conﬁrms our intuition that the shared layers
are vital to map the source and target latent rep-
resentations to a shared-latent space. In the rest
of our experiments, we set the number of weight-
sharing layer as 1.

Figure 2: The effects of the weight-sharing layer
number on English-to-German, English-to-French
and Chinese-to-English translation tasks.

4.4.2 Translation results

Table 2 shows the BLEU scores on English-
German, English-French and English-to-Chinese
test sets. As it can be seen, the proposed ap-
proach obtains signiﬁcant improvements than the
word-by-word baseline system, with at least +5.01
BLEU points in English-to-German translation
and up to +13.37 BLEU points in English-to-
French translation. This shows that the proposed

en-de

de-en

en-fr

fr-en

zh-en

Without weight sharing
Without embedding-reinforced encoder
Without directional self-attention
Without local GANs
Without Global GANs
Full model

10.23
10.45
10.60
10.51
10.34
10.86

13.84
14.17
14.21
14.35
14.05
14.62

16.02
16.55
16.82
16.40
16.19
16.97

14.82
15.27
15.30
15.07
15.21
15.58

13.75
14.10
14.29
14.12
14.09
14.52

Table 3: Ablation study on English-German, English-French and Chinese-to-English translation tasks.
Without weight sharing means no layers are shared in the two AEs.

model only trained with monolingual data effec-
tively learns to use the context information and
the internal structure of each language. Com-
pared to the work of (Lample et al., 2017), our
model also achieves up to +1.92 BLEU points im-
provement on English-to-French translation task.
We believe that the unsupervised NMT is very
promising. However, there is still a large room
for improvement compared to the supervised up-
per bound. The gap between the supervised and
unsupervised model is as large as 12.3-25.5 BLEU
points depending on the language pair and transla-
tion direction.

4.4.3 Ablation study
To understand the importance of different com-
ponents of the proposed system, we perform an
ablation study by training multiple versions of
the
our model with some missing components:
local GANs,
the directional
the global GANs,
self-attention, the weight-sharing, the embedding-
reinforced encoders, etc. Results are reported
in table 3. We do not test the the importance
of the auto-encoding, back-translation and the
pre-trained embeddings because they have been
widely tested in (Lample et al., 2017; Artetxe
et al., 2017b). Table 3 shows that the best per-
formance is obtained with the simultaneous use of
all the tested elements. The most critical compo-
nent is the weight-sharing constraint, which is vi-
tal to map sentences of different languages to the
shared-latent space. The embedding-reinforced
encoder also brings some improvement on all of
the translation tasks. When we remove the di-
rectional self-attention, we get up to -0.3 BLEU
points decline. This indicates that it deserves more
efforts to investigate the temporal order informa-
tion in self-attention mechanism. The GANs also
signiﬁcantly improve the translation performance
the global GANs
of our system. Speciﬁcally,

achieve improvement up to +0.78 BLEU points on
English-to-French translation and the local GANs
also obtain improvement up to +0.57 BLEU points
on English-to-French translation. This reveals that
the proposed model beneﬁts a lot from the cross-
domain loss deﬁned by GANs.

5 Conclusion and Future work

The models proposed recently for unsupervised
NMT use a single encoder to map sentences from
different languages to a shared-latent space. We
conjecture that the shared encoder is problem-
atic for keeping the unique and inherent char-
acteristic of each language.
In this paper, we
propose the weight-sharing constraint in unsuper-
vised NMT to address this issue. To enhance the
cross-language translation performance, we also
propose the embedding-reinforced encoders, local
GAN and global GAN into the proposed system.
Additionally, the directional self-attention is intro-
duced to model the temporal order information for
our system.
We test

the proposed model on English-
German, English-French and Chinese-to-English
translation tasks. The experimental results reveal
that our approach achieves signiﬁcant improve-
ment and verify our conjecture that the shared en-
coder is really a bottleneck for improving the un-
supervised NMT. The ablation study shows that
each component of our system achieves some im-
provement for the ﬁnal translation performance.

Unsupervised NMT opens exciting opportuni-
ties for the future research. However, there is
still a large room for improvement compared to
the supervised NMT. In the future, we would like
to investigate how to utilize the monolingual data
more effectively, such as incorporating the lan-
guage model and syntactic information into unsu-
pervised NMT. Besides, we decide to make more

efforts to explore how to reinforce the temporal or-
der information for the proposed model.

International Joint Conference on Artiﬁcial Intelli-
gence. pages 3974–3980.

Acknowledgements

This work is supported by the National Key Re-
search and Development Program of China un-
der Grant No. 2017YFB1002102, and Beijing
Engineering Research Center under Grant No.
Z171100002217015. We would like to thank Xu
Shuang for her preparing data used in this work.
Additionally, we also want to thank Jiaming Xu,
Suncong Zheng and Wenfu Wang for their invalu-
able discussions on this work.

References

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Man´e, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Vi´egas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems .

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Conference on Empirical Methods in Natural
Language Processing. pages 2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2017a. Learning bilingual word embeddings with
(almost) no bilingual data. In Meeting of the Asso-
ciation for Computational Linguistics. pages 451–
462.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2017b. Unsupervised neural ma-
chine translation .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473 .

Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, and
Wei Xu. 2016. Neural machine translation with
pivot languages. arXiv preprint arXiv:1611.04928
.

Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, Wei
Xu, Yong Cheng, Qian Yang, Yang Liu, Maosong
Joint training for pivot-
Sun, and Wei Xu. 2017.
In Twenty-Sixth
based neural machine translation.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Learning
Schwenk, and Yoshua Bengio. 2014.
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Herv Jgou. 2017.
Word translation without parallel data .

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning .

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. TACL .

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vi´egas, Martin Wattenberg, Greg Corrado,
et al. 2016. Google’s multilingual neural machine
translation system: Enabling zero-shot translation.
arXiv preprint arXiv:1611.04558 .

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. EMNLP pages
1700–1709.

Guillaume

Ludovic Denoyer,

and
Lample,
Marc’Aurelio Ranzato. 2017.
Unsupervised
machine translation using monolingual corpora only
.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems. pages 3111–3119.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. Association for Com-
putational Linguistics pages 311–318.

Amrita Saha, Mitesh M Khapra, Sarath Chandar, Ja-
narthanan Rajendran, and Kyunghyun Cho. 2016.
A correlational encoder decoder architecture for
arXiv preprint
pivot based sequence generation.
arXiv:1606.04754 .

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation
arXiv preprint

2015a.
models with monolingual data.
arXiv:1511.06709 .

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015b. Neural machine translation of rare words
with subword units. Computer Science .

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. arXiv
preprint arXiv:1512.02433 .

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding .

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. Advances in neural information processing
systems pages 3104–3112.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2017. Learning to remember translation his-
tory with a continuous cache .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need .

Pascal Vincent, Hugo Larochelle, Yoshua Bengio,
and Pierre-Antoine Manzagol. 2008. Extracting
and composing robust features with denoising au-
In Proceedings of the 25th interna-
toencoders.
tional conference on Machine learning. ACM, pages
1096–1103.

Mingxuan Wang, Zhengdong Lu,

Jie Zhou, and
Qun Liu. 2017. Deep neural machine transla-
arXiv preprint
tion with linear associative unit.
arXiv:1705.00861 .

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016.
Google’s neural ma-
chine translation system: Bridging the gap between
arXiv preprint
human and machine translation.
arXiv:1609.08144 .

bilingual lexicon induction. In Meeting of the Asso-
ciation for Computational Linguistics. pages 1959–
1970.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei
Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. arXiv
preprint arXiv:1606.04199 .

A Experiments on the layer number for

encoders and decoders

To determine the number of layers for encoders
and decoders in our system beforehand, we con-
duct experiments on English-German translation
tasks to test how the amount of layers in encoders
and decoders affects the translation performance.
We vary the number of layers from 2 to 6 and the
results are reported in table 4. We can ﬁnd that the
translation performance achieves substantial im-
provement with the layer number increasing from
2 to 4. However, with layer number set larger than
4, we get little improvement. To make a trade-off
between the translation performance and the com-
putation complexity, we set the layer number as 4
for our encoders and decoders.

layer num en-de

de-en

2
3
4
5
6

11.57
12.43
12.86
12.91
12.95

14.01
14.99
15.62
15.83
15.79

Table 4: The experiments on the number of layers
for encoders and decoders.

Hao Xiong, Zhongjun He, Xiaoguang Hu, and Hua Wu.
2017. Multi-channel encoder for neural machine
translation. arXiv preprint arXiv:1712.02109 .

B The architecture of the global

discriminator

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2017.
Improving neural machine translation with condi-
tional sequence generative adversarial nets .

Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu,
and Maosong Sun. 2017a. Prior knowledge integra-
tion for neural machine translation using posterior
In Meeting of the Association for
regularization.
Computational Linguistics. pages 1514–1523.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Conference on Empirical Methods in
Natural Language Processing. pages 1535–1545.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Adversarial training for unsupervised

The global discriminator is applied to classify the
generated sentences as source language, target lan-
guage or generated sentences. Following (Yang
et al., 2017), we implement the global discrimina-
tor based on CNN. Since sentences generated by
the generator (the composition of the encoder and
decoder) have variable lengths, the CNN padding
is used to transform the sentences to sequences
with ﬁxed length T , which is the maximum length
set for the output of the generator. Given the gen-
erated sequences x1, . . . , xT , we build the matrix
X1:T as:

X1:T = x1; x2; . . . ; xT

(9)

where xt ∈ Rk is the k-dimensional word embed-
ding and the semicolon is the concatenation oper-
ator. For the matrix X1:T , a kernel wj ∈ Rl×k
applies a convolutional operation to a window size
of l words to produce a series of feature maps:

cji = ρ(BN (wj ⊗ Xi:i+l−1 + b))

(10)

where ⊗ operator is the summation of element-
wise production and b is a bias term. ρ is a non-
linear activation function which is implemented as
ReLu in this paper. To get the ﬁnal feature with
respect to kernel wj, a max-over-time pooling op-
eration is leveraged over the feature maps:

(cid:101)cj = max{cj1, . . . , cjT −l+1}

(11)

We use various numbers of kernels with different
window sizes to extract different features, which
are then concatenated to form the ﬁnal sentence
representation xc. Finally, we pass xc through a
fully connected layer and a softmax layer to gen-
erate the probability p(fg|x1, . . . , xT ) as:

p(fg|x1, . . . , xT ) = sof tmax(V ∗ xc)

(12)

where V is the transformation matrix and fg ∈
{true, generated}.

C The training procedure of the global

GAN

We apply the global GANs to ﬁnetune the whole
model. Here, we provide detailed strategies for
training the global GANs. Firstly, we generate the
machine-generated source language sentences by
using Enct and Encs to decode the monolingual
data in target language. Similarly, we get the gen-
erated sentences in target language with Encs and
Dect by decoding source language monolingual
data. We simply use the greedy sampling method
instead of the beam search method for decoding.
Next, we pre-train Dg1 on the combination of true
monolingual data and the generated data in the
source language. Similarly, we also pre-train Dg2
on the combination of true monolingual data and
the generated data in the target language. Finally,
we jointly train the generators and discriminators.
The generators are trained with policy gradient
training methods. For the details about the pol-
icy gradient training, we refer the reader to (Yang
et al., 2017).

D The conﬁgurations for the open-source

toolkits

We train the word embedding use the following
script:

./word2vec -train text -output embedding.txt -
cbow 0 -size 512 -window 10 -negative 10 -hs 0
-sample 1e- -threads 50 -binary 0 -min-count 5 -
iter 10

After we get the embeddings for both the source
and target
languages, we use the open-source
VecMap 10 to map these embeddings to a shared-
latent space with the following scripts:

python3 normalize embeddings.py unit center -i

s embedding.txt -o s embedding.normalized.txt

python3 normalize embeddings.py unit center -i

t embedding.txt -o t embedding.normalized.txt

map embeddings.py

–
s embedding.normalized.txt

python3
orthogonal
t embedding.normalized.txt
s embedding.mapped.txt t embedding.mapped.txt
–numerals –self learning -v

10https://github.com/artetxem/vecmap

