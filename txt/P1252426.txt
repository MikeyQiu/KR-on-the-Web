Recovery of non-linear cause-effect relationships
from linearly mixed neuroimaging data

Sebastian Weichwald, Arthur Gretton, Bernhard Schölkopf, Moritz Grosse-Wentrup

1

6
1
0
2
 
p
e
S
 
0
3
 
 
]
E
M

.
t
a
t
s
[
 
 
2
v
1
9
3
0
0
.
5
0
6
1
:
v
i
X
r
a

Abstract—Causal

the identiﬁcation of
inference concerns
cause-effect relationships between variables. However, often only
linear combinations of variables constitute meaningful causal
variables. For example, recovering the signal of a cortical source
from electroencephalography requires a well-tuned combination
of signals recorded at multiple electrodes. We recently introduced
the MERLiN (Mixture Effect Recovery in Linear Networks)
algorithm that is able to recover, from an observed linear
mixture, a causal variable that is a linear effect of another
given variable. Here we relax the assumption of this cause-effect
relationship being linear and present an extended algorithm
that can pick up non-linear cause-effect relationships. Thus,
the main contribution is an algorithm (and ready to use code)
that has broader applicability and allows for a richer model
class. Furthermore, a comparative analysis indicates that the
assumption of linear cause-effect relationships is not restrictive
in analysing electroencephalographic data.

Index Terms—causal inference, causal variable construction,
instrumental variable, linear mixtures, regression-based condi-
tional independence criterion

I. INTRODUCTION

Causal inference requires causal variables. However, not
always do the variables in a dataset specify the candidate
causal relata. In electroencephalography (EEG) studies, for
example, what is measured at electrodes placed on the scalp
is instantaneously and linearly superimposed electromagnetic
activity of sources in the brain [1]. Standard causal inference
methods require to ﬁrst recover the cortical sources from the
observed electrode signals [2]. This is disadvantageous. First,
any source localisation procedure is prone to modelling errors,
which may distort the true cause-effect relationships between
cortical sources. Second, source localisation enlarges the data
dimensionality by roughly two orders of magnitude, which
leads to increased computational complexity.

We recently proposed a novel idea to construct causal vari-
ables, i. e., recover cortical sources, by directly optimising for
statistical in- and dependences that imply a certain cause-effect
relationship [3]. The linear MERLiN algorithm can – skipping
potentially error prone modelling steps – establish a linear
cause-effect relationship between brain state features that are
observed only as part of a linear mixture. This allows for
computationally efﬁcient insights into brain networks beyond
those readily obtained from encoding and decoding models

SW, BS, and MGW are with the Empirical Inference Department, Max
Planck Institute for Intelligent Systems, Tübingen, Germany, e-mail: [sweich-
wald, bs, moritzgw]@tue.mpg.de.

AG is with the Gatsby Computational Neuroscience Unit, Sainsbury Well-

come Centre, London, United Kingdom, e-mail: arthur.gretton@gmail.com.

trained on pre-deﬁned variables [4]. The linear MERLiN
algorithm, however, is unable to reconstruct cortical sources
with non-linear cause-effect relationships.

Here we present the non-linear MERLiN algorithm and
relax the assumption of linear cause-effect relationships. By
integrating kernel ridge regression and a non-linear indepen-
dence test, the extended algorithm can capture any higher order
dependence. We compare the results of our linear- and non-
linear MERLiN algorithms on EEG data for which cause-
effect relationships have previously only been computed by
an exhaustive search approach [5] and ﬁnd no qualitative
differences. The contribution of this work is thus two-fold.
First, we provide an algorithm to learn non-linear cause-effect
relationships from linear mixtures of causal variables, and,
second, we provide empirical evidence that linear methods
sufﬁce to identify cause-effect relationships within individual
EEG frequency bands. The Python implementation is available
at https://github.com/sweichwald/MERLiN.

A. Causal Bayesian Networks

II. METHODS

We brieﬂy introduce the main aspects of Causal Bayesian
Networks (CBNs). For an exhaustive treatment see [6], [7].
The important advantage of this framework over methods
based on information ﬂow is that it yields testable predictions
on the impact of interventions [8], [9].

Deﬁnition 1 (Structural Equation Model). We deﬁne a struc-
tural equation model (SEM) S as a set of equations Xi =
i ∈ N1:s where the so-called noise vari-
fi(PAi, Ni),
ables are independently distributed according to PN1,...,Ns =
PN1 · · · PNs . For i ∈ N1:s the set PAi ⊆ {X1, ..., Xs} \ Xi
contains the so-called parents of Xi and fi describes how Xi
relates to the random variables in PAi and Ni. The induced
joint distribution is denoted by PS (cid:44) PX1,...,Xs .

Replacing at least one of the functions fi, i ∈ N1:s by a
constant ♠ yields a new SEM. We say Xi has been intervened
on, which is denoted by do(Xi = ♠), leads to the SEM
S| do(Xi = ♠), and induces the interventional distribution
PS| do(Xi=♠) (cid:44) PX1,...,Xs| do(Xi=♠).

Deﬁnition 2 (Cause and Effect). Xi is a cause of Xj (i, j ∈
N1:s, i (cid:54)= j) wrt. a SEM S iff there exists ♥ ∈ R such that
PXj | do(Xi=♥) (cid:54)= PXj .1 Xj is an effect of Xi iff Xi is a cause
of Xj. Often the considered SEM S is omitted if it is clear
from the context.

1PXj | do(Xi=♥) and PXj denote the marginal distributions of Xj corre-

sponding to PS| do(Xi=♥) and PS respectively.

This is the author’s version of an article that is published in Pattern Recognition in Neuroimaging (PRNI), International Workshop on, 1–4, 2016, doi: 10.1109/PRNI.2016.7552331.
Copyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

2

For each SEM S there is a corresponding graph GS (V, E)
with V (cid:44) {X1, ..., Xs} and E (cid:44) {(Xi, Xj) : Xi ∈
PAj, Xj ∈ V } that has the random variables as nodes and
directed edges pointing from parents to children. We employ
the common assumption that this graph is acyclic, i.e., GS will
always be a directed acyclic graph (DAG).

So far a DAG GS simply depicts all parent-child relation-
ships deﬁned by the SEM S. Missing directed paths indicate
missing cause-effect relationships. In order to specify the link
between statistical independence (denoted by ⊥⊥) wrt. the joint
distribution PS and properties of the DAG GS (representing a
SEM S) we need the following deﬁnition.

Deﬁnition 3 (d-separation). For a ﬁxed graph G disjoint sets
of nodes A and B are d-separated by a third disjoint set C
(denoted by A ⊥d-sep B|C) iff all pairs of nodes a ∈ A and
b ∈ B are d-separated by C. A pair of nodes a (cid:54)= b is d-
separated by C iff every path between a and b is blocked by
C. A path between nodes a and b is blocked by C iff there is
an intermediate node z on the path such that (i) z ∈ C and z is
tail-to-tail (← z →) or head-to-tail (→ z →), or (ii) z is head-
to-head (→ z ←) and neither z nor any of its descendants is
in C.

assuming

Conveniently,
Markovianity3) we
correspondence
between
independence statements:

have

faithfulness2

(and

the

d-separation

following
and

exploiting
one-to-one
conditional

A ⊥d-sep B|C ⇐⇒ A ⊥⊥ B|C

Summing up, we have deﬁned interventional causation in
terms of SEMs and have seen how a SEM gives rise to
a DAG. This DAG has two convenient features. Firstly, the
DAG yields a visualisation that allows to easily grasp missing
cause-effect relationships that correspond to missing directed
paths. Secondly, since we assume faithfulness, d-separation
properties of this DAG are equivalent to conditional indepen-
dence properties of the joint distribution. Thus, conditional
independences translate into causal statements, e.g. ‘a variable
becomes independent of all its non-effects given its immedi-
ate causes’ or ‘cause and effect are marginally dependent’.
the causal graph GS can be identiﬁed from
Furthermore,
conditional independences observed in PS – at least up to
a so-called Markov equivalence class, the set of graphs that
entail the same conditional independences [10].

B. Formal problem description

In the following, the variables S, C1, ..., Cd, and F1, ..., Fd
may be thought of as a stimulus variable, the activity of
multiple cortical sources, and the EEG channel recordings
respectively. We aim at recovering an effect of a pre-deﬁned
target variable C1 = v(cid:62)F . The terminology introduced in
Section II-A allows to precisely state the problem as follows.

2Intuitively, this is saying that conditional independences are due to the
causal structure and not accidents of parameter values [6, p. 9]; more formally
the assumption reads A ⊥d-sep B|C ⇐= A ⊥⊥ B|C.

3The distribution PS generated by a SEM S is Markov wrt. GS (cf. [7,

Theorem 1.4.1] for a proof), i. e., A ⊥d-sep B|C =⇒ A ⊥⊥ B|C.

1) Assumptions: Let S and C1, ..., Cd denote (ﬁnitely
many) random variables. We assume existence of a SEM S,
potentially with additional unobserved variables h1, ..., hl, that
induces PS = PS,C1,...,Cd,h1,...,hl . We refer to the correspond-
ing graph GS as the true causal graph and call its nodes causal
variables. We further assume that

• S affects C2 indirectly via C1,4
• there are no edges in GS pointing into S.5

Importantly, we do not require that the structural equation that
relates C2 to its parents is linear in C1. Figure 1 depicts an
example of how GS might look like.

S

C1

C2

C5
...

Cd

C3

h1

C4

Fig. 1. Example graph where h1 is a hidden variable.

2) Given data:
• v ∈ Rd such that C1 = v(cid:62)F
• m iid6 samples S = [s1, ..., sm](cid:62) of S and F =
[fi,j]i=1:m,j=1:d of F where F (cid:44) [F1, ..., Fd](cid:62) = AC
is the observed linear mixture of the causal variables
C (cid:44) [C1, ..., Cd](cid:62) and A ∈ Rd×d the mixing matrix
3) Desired output: Find w ∈ Bd (cid:44) {x ∈ Rd : ||x|| = 1}
such that aCi = w(cid:62)F where Ci is an effect of C1 (i ∈
N2:d, a ∈ R \ {0}). For the graph shown in Figure 1 recovery
of the causal variable C2 is a valid solution.

C. Strategy

Our approach leverages the following causal inference rule
that – under the assumptions in Section II-B1 – applies to a
causal variable Ci (cf. [5]).

Causal Inference Rule: If Ci ⊥⊥ S|C1 and Y (cid:54)⊥⊥ C1, then
S indirectly affects Ci via C1. In particular, a causal path
C1 (cid:57)(cid:57)(cid:75) Ci exists.

The idea is to recover

the sought-after variable from
the mixture F by optimising for these statistical properties.
Thence, the general strategy relies on solving an optimisation
problem of the form7

max
w∈Bd−1

dep(C1, Yw) − dep(S, Yw|C1)

where Yw = w(cid:62)F and dep denotes a (conditional) de-
pendence criterion that estimates from empirical samples the
strength of association between the two variables.

4By saying a variable X causes Z indirectly via Y we imply (a) existence
of a path X (cid:57)(cid:57)(cid:75) Y (cid:57)(cid:57)(cid:75) Z, and (b) that there is no path X (cid:57)(cid:57)(cid:75) Z without Y
on it (this also excludes the edge X → Z).

5This condition can for example be ensured by randomising S.
6independent and identically distributed
7To attenuate the signal of C1 we restrict search onto the orthogonal
||x|| = 1, x ⊥ v} which is

complement v⊥ ∩ Bd = {x ∈ Rd :
diffeomorphic to Bd−1.

D. Non-Linear MERLiN algorithm

The linear MERLiN algorithm uses the partial correla-
tions ρC1,Yw|S and ρS,Yw|C1 for the terms dep(C1, Yw) and
dep(S, Yw|C1) in the objective function. As such only linear
dependence between C1 and Yw can be detected while re-
maining higher-order dependences between S and Yw given
C1 may go undetected. A general kernel-based indepen-
dence criterion, the Hilbert-Schmidt Independence Criterion
(HSIC) [11], and a regression-based conditional independence
criterion (cf. [5]) in conjunction with kernel ridge regres-
sion [12] allow extension to non-linear dependences.

Regression-Based Conditional Independence Criterion:
If there exists a (regression) function r such that Yw−r(C1) ⊥⊥
(S, C1) then S ⊥⊥ Yw|C1.

The non-linear MERLiN algorithm solves the following

optimisation problem

max
(w,σ,θ)∈Bd−1×R×R

HSIC(C1, Yw) − HSIC ((S, C1), Rw,σ,θ)

where HSIC(A, B) denotes the empirical HSIC estimate8 and
Rw,σ,θ corresponds to the residuals Yw − r(C1) using kernel
ridge regression with Gaussian kernel of width |σ| and ridge
regression parameter |θ|. To temper overﬁtting, the sample is
split into three partitions; the residuals of the ith partition are
obtained by using the kernel ridge regression function obtained
on the remaining partitions. The regression parameters σ and
θ are also being optimised over to allow an optimal regres-
sion ﬁt wrt. witnessing conditional independence and hence
minimising the second summand in the objective function.

Implementing the objective function in Theano [13], [14],
we use the Python toolbox Pymanopt [15] to run optimisation
on the product manifold Bd−1 × R × R using a steepest
descent algorithm with standard back-tracking line-search.
This approach is exact and efﬁcient, relying on automated
differentiation and respecting the manifold geometry.

E. Application to EEG data

We consider EEG trial-data of the form (cid:101)F ∈ Rd×m×n
where d denotes the number of electrodes, m the number of
trials, and n the length of the time series (cid:101)Fi,j,1:n for each
electrode i ∈ N1:d and each sample j ∈ N1:m; that is (cid:101)F holds
m iid samples of a Rd×n-valued random variable (cid:101)F . Analyses
of EEG data commonly focus on trial-averaged log-bandpower
in a particular frequency band. Accordingly, applying our al-
gorithms to EEG data we aim to identify a linear combination
w ∈ Bd such that the log-bandpower of the resulting one-
dimensional trial signals w(cid:62) (cid:101)F is a causal effect of the log-
bandpower of the one-dimensional trial signals v(cid:62) (cid:101)F .

However,

the two operations of computing the log-
bandpower and taking a linear combination do not commute.
The log-bandpower computation needs to be switched into the
objective function described above. This is accomplished by
letting Yw = logbp(w(cid:62) (cid:101)F ) and C1 = logbp(v(cid:62) (cid:101)F ) where
logbp denotes the operation of applying a Hanning window

8We compute the empirical HSIC estimate based on the Gaussian kernel
k(x, y) = exp(−δ−1||x − y||2) where the kernel size δ is determined by
the median distance between points in input space [11].

3

and computing the average log-bandpower in a speciﬁed
frequency range for the given time series.

III. COMPARATIVE EEG ANALYSIS

A. Experimental data

We applied the linear MERLiN algorithm and its non-
linear extension to EEG data recorded during a neurofeedback
experiment [16]. In this study the γ-log-bandpower (55–85 Hz)
in the right superior parietal cortex (SPC) was provided as
feedback signal and subjects were instructed to up- or down-
regulate the bandpower. 3 subjects were recorded in 2 sessions
each and each session had 60 trials á 60 seconds.

The data of one session consists of a stimulus vector S ∈
{−1, +1}60×1, a spatial ﬁlter v ∈ R121×1 that was used to
extract the feedback signal, and a tensor (cid:101)F ∈ R121×60×15000
that holds the time series (of length 15000) for each channel
and trial. The reader is referred to [16] for more details on the
experimental data.

B. How to compare to previous results

We compare our MERLiN algorithms against a causal
analysis of this neurofeedback experiment that is based on
source localisation in combination with an exhaustive search
procedure [5]. The hypothesis was, based on transcranial
magnetic stimulation studies [17], that γ-oscillations in the
SPC modulate γ-oscillations in the medial prefrontal cortex
(MPC). We brieﬂy describe this exhaustive search approach.
First, the signal of K (cid:44) 15028 dipoles across the cortical
surface was extracted using a LCMV beamformer and a three-
shell spherical head model [18]. Then, the authors applied their
newly introduced stimulus-based causal inference (SCI) algo-
rithm to assess for every dipole whether its γ-log-bandpower
is a linear causal effect of the γ-log-bandpower in the SPC.
Group results were summarised in a vector gSCI ∈ RK×1
where the ith entry denotes the percentage of dipoles within a
certain radius that were found to be modulated by the SPC.
The results of this exhaustive search analysis, visualising gSCI
on hemisphere plots, supported the hypothesis that the MPC
is a linear causal effect of the SPC. The reader is referred to
[5] for more details.

In contrast to exhaustive search, both our linear MERLiN
algorithm as well as its non-linear extension aim at imme-
diately recovering the causal effect by optimising a linear
combination w of electrode signals9. To allow for a qualitative
comparison of our results with the results summarised by
the vector gSCI we derive for each w a vector g ∈ RK×1.
This vector represents the involvement of each cortical dipole
in the recovered signal and is derived from w as follows.
First, a scalp topography is obtained via a ∝ Σw where
the ith entry of Σw is the covariance between the ith EEG
channel and the source that is recovered by w [19, Equation
(7)]. Here Σ denotes the session-speciﬁc covariance matrix
in the γ-frequency band. Second,
the dipole involvement

9Since there were only 60 samples per session we decided to select a
subset of 33 EEG channels distributed across the scalp (according to the
10–20 system). Hence, for each recording session we obtained a spatial ﬁlter
w ∈ R33×1.

4

vector g is obtained from a via dynamic statistical parametric
mapping (dSPM; with identity noise covariance matrix) [20].
Group results are obtained as average of the individual dipole
involvement vectors.

C. Experimental results

The group averaged results of our extended algorithm are
depicted in Figure 2.(a). Similar to the results in [5] and
the results we obtained with the linear MERLiN algorithm
(cf. Figure 2.(b)) the analysis indicates that the MPC is a
causal effect of the SPC. The non-linear method yields results
that are in high accordance with the ones obtained by our
linear method while exhaustive search additionally revealed
the anterior middle frontal gyrus as effect of the SPC.

Left hemisphere

Right hemisphere

(a) non-linear

Left hemisphere

Right hemisphere

w
e
i
v

l
a
r
e
t
a
L

w
e
i
v

l
a
i
d
e
M

w
e
i
v

l
a
r
e
t
a
L

w
e
i
v

l
a
i
d
e
M

(b) linear

Fig. 2. Group averaged dipole involvement corresponding to the spatial ﬁlters
identiﬁed by the (a) non-linear and (b) linear MERLiN algorithm; lateral and
medial views of the left and right hemisphere. (All colour scales from “blue”
to “red” range from 0 to the largest value to be plotted.)

IV. CONCLUSIONS
We have developed the non-linear MERLiN algorithm that
is able to recover a causal effect from an observed linear
mixture with no constraint on the functional form of this
cause-effect relationship. Iteratively projecting out directions
and applying the MERLiN algorithm may allow to identify
multiple distinct causal effects. For EEG data we found no

qualitative difference to the linear method, which indicates
that linear methods sufﬁce to identify within-frequency cause-
effect relationships in EEG data. Future research will focus on
theoretical analysis of the presented methods and assumptions
and investigate applicability to other real world data.

REFERENCES

[1] P. L. Nunez and R. Srinivasan, Electric Fields of

the Brain: The

Neurophysics of EEG. Oxford University Press, 2006.

[2] M. Grosse-Wentrup, “Understanding brain connectivity patterns during
motor imagery for brain-computer interfacing,” in Advances in Neural
Information Processing Systems, 2009, pp. 561–568.

[3] S. Weichwald, M. Grosse-Wentrup, and A. Gretton, “MERLiN: Mixture
Effect Recovery in Linear Networks,” arXiv preprint arXiv:1512.01255,
2015.

[4] S. Weichwald, T. Meyer, O. Özdenizci, B. Schölkopf, T. Ball, and
interpretation rules for encoding and
M. Grosse-Wentrup, “Causal
decoding models in neuroimaging,” NeuroImage, vol. 110, pp. 48–59,
2015.

[5] M. Grosse-Wentrup, D. Janzing, M. Siegel, and B. Schölkopf, “Identiﬁ-
cation of causal relations in neuroimaging data with latent confounders:
An instrumental variable approach,” NeuroImage, vol. 125, pp. 825–833,
2016.

[6] P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction, and

Search, 2nd ed. MIT press, 2000.

[7] J. Pearl, Causality: Models, Reasoning and Inference, 2nd ed. Cam-

bridge University Press, 2009.

[8] M. Eichler and V. Didelez, “On Granger causality and the effect of
interventions in time series,” Lifetime Data Analysis, vol. 16, no. 1, pp.
3–32, 2010.

[9] J. T. Lizier and M. Prokopenko, “Differentiating information transfer
and causal effect,” The European Physical Journal B, vol. 73, no. 4, pp.
605–615, 2010.

[10] T. Verma and J. Pearl, “Equivalence and synthesis of causal models,”
the 6th Conference on Uncertainty in Artiﬁcial

in Proceedings of
Intelligence. Elsevier, 1990, pp. 255–268.

[11] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Schölkopf, and A. J.
Smola, “A Kernel Statistical Test of Independence,” in Advances in
Neural Information Processing Systems 20, 2008, pp. 585–592.
[12] C. Saunders, A. Gammerman, and V. Vovk, “Ridge Regression Learning
Algorithm in Dual Variables,” in Proceedings of the 15th International
Conference on Machine Learning, 1998, pp. 515–521.

[13] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Des-
jardins, J. Turian, D. Warde-Farley, and Y. Bengio, “Theano: A CPU
and GPU math expression compiler,” in Proceedings of the Python for
Scientiﬁc Computing Conference (SciPy), Jun. 2010.

[14] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow,
A. Bergeron, N. Bouchard, and Y. Bengio, “Theano: New features
and speed improvements,” Deep Learning and Unsupervised Feature
Learning NIPS 2012 Workshop, 2012.

[15] J. Townsend, N. Koep, and S. Weichwald, “Pymanopt: A Python
Toolbox for Manifold Optimization using Automatic Differentiation,”
arXiv preprint arXiv:1603.03236, 2016.

[16] M. Grosse-Wentrup and B. Schölkopf, “A brain–computer interface
based on self-regulation of gamma-oscillations in the superior parietal
cortex,” Journal of Neural Engineering, vol. 11, no. 5, p. 056015, 2014.
[17] A. C. Chen, D. J. Oathes, C. Chang, T. Bradley, Z.-W. Zhou, L. M.
Williams, G. H. Glover, K. Deisseroth, and A. Etkin, “Causal
in-
teractions between fronto-parietal central executive and default-mode
networks in humans,” Proceedings of the National Academy of Sciences,
vol. 110, no. 49, pp. 19 944–19 949, 2013.

[18] J. C. Mosher, R. M. Leahy, and P. S. Lewis, “EEG and MEG: Forward
solutions for inverse methods,” Biomedical Engineering, IEEE Transac-
tions on, vol. 46, no. 3, pp. 245–259, 1999.

[19] S. Haufe, F. Meinecke, K. Görgen, S. Dähne, J.-D. Haynes, B. Blankertz,
and F. Bießmann, “On the interpretation of weight vectors of linear
models in multivariate neuroimaging,” NeuroImage, vol. 87, pp. 96–110,
2014.

[20] A. M. Dale, A. K. Liu, B. R. Fischl, R. L. Buckner, J. W. Belliveau,
J. D. Lewine, and E. Halgren, “Dynamic statistical parametric mapping:
Combining fMRI and MEG for high-resolution imaging of cortical
activity,” Neuron, vol. 26, no. 1, pp. 55–67, 2000.

Recovery of non-linear cause-effect relationships
from linearly mixed neuroimaging data

Sebastian Weichwald, Arthur Gretton, Bernhard Schölkopf, Moritz Grosse-Wentrup

1

6
1
0
2
 
p
e
S
 
0
3
 
 
]
E
M

.
t
a
t
s
[
 
 
2
v
1
9
3
0
0
.
5
0
6
1
:
v
i
X
r
a

Abstract—Causal

the identiﬁcation of
inference concerns
cause-effect relationships between variables. However, often only
linear combinations of variables constitute meaningful causal
variables. For example, recovering the signal of a cortical source
from electroencephalography requires a well-tuned combination
of signals recorded at multiple electrodes. We recently introduced
the MERLiN (Mixture Effect Recovery in Linear Networks)
algorithm that is able to recover, from an observed linear
mixture, a causal variable that is a linear effect of another
given variable. Here we relax the assumption of this cause-effect
relationship being linear and present an extended algorithm
that can pick up non-linear cause-effect relationships. Thus,
the main contribution is an algorithm (and ready to use code)
that has broader applicability and allows for a richer model
class. Furthermore, a comparative analysis indicates that the
assumption of linear cause-effect relationships is not restrictive
in analysing electroencephalographic data.

Index Terms—causal inference, causal variable construction,
instrumental variable, linear mixtures, regression-based condi-
tional independence criterion

I. INTRODUCTION

Causal inference requires causal variables. However, not
always do the variables in a dataset specify the candidate
causal relata. In electroencephalography (EEG) studies, for
example, what is measured at electrodes placed on the scalp
is instantaneously and linearly superimposed electromagnetic
activity of sources in the brain [1]. Standard causal inference
methods require to ﬁrst recover the cortical sources from the
observed electrode signals [2]. This is disadvantageous. First,
any source localisation procedure is prone to modelling errors,
which may distort the true cause-effect relationships between
cortical sources. Second, source localisation enlarges the data
dimensionality by roughly two orders of magnitude, which
leads to increased computational complexity.

We recently proposed a novel idea to construct causal vari-
ables, i. e., recover cortical sources, by directly optimising for
statistical in- and dependences that imply a certain cause-effect
relationship [3]. The linear MERLiN algorithm can – skipping
potentially error prone modelling steps – establish a linear
cause-effect relationship between brain state features that are
observed only as part of a linear mixture. This allows for
computationally efﬁcient insights into brain networks beyond
those readily obtained from encoding and decoding models

SW, BS, and MGW are with the Empirical Inference Department, Max
Planck Institute for Intelligent Systems, Tübingen, Germany, e-mail: [sweich-
wald, bs, moritzgw]@tue.mpg.de.

AG is with the Gatsby Computational Neuroscience Unit, Sainsbury Well-

come Centre, London, United Kingdom, e-mail: arthur.gretton@gmail.com.

trained on pre-deﬁned variables [4]. The linear MERLiN
algorithm, however, is unable to reconstruct cortical sources
with non-linear cause-effect relationships.

Here we present the non-linear MERLiN algorithm and
relax the assumption of linear cause-effect relationships. By
integrating kernel ridge regression and a non-linear indepen-
dence test, the extended algorithm can capture any higher order
dependence. We compare the results of our linear- and non-
linear MERLiN algorithms on EEG data for which cause-
effect relationships have previously only been computed by
an exhaustive search approach [5] and ﬁnd no qualitative
differences. The contribution of this work is thus two-fold.
First, we provide an algorithm to learn non-linear cause-effect
relationships from linear mixtures of causal variables, and,
second, we provide empirical evidence that linear methods
sufﬁce to identify cause-effect relationships within individual
EEG frequency bands. The Python implementation is available
at https://github.com/sweichwald/MERLiN.

A. Causal Bayesian Networks

II. METHODS

We brieﬂy introduce the main aspects of Causal Bayesian
Networks (CBNs). For an exhaustive treatment see [6], [7].
The important advantage of this framework over methods
based on information ﬂow is that it yields testable predictions
on the impact of interventions [8], [9].

Deﬁnition 1 (Structural Equation Model). We deﬁne a struc-
tural equation model (SEM) S as a set of equations Xi =
i ∈ N1:s where the so-called noise vari-
fi(PAi, Ni),
ables are independently distributed according to PN1,...,Ns =
PN1 · · · PNs . For i ∈ N1:s the set PAi ⊆ {X1, ..., Xs} \ Xi
contains the so-called parents of Xi and fi describes how Xi
relates to the random variables in PAi and Ni. The induced
joint distribution is denoted by PS (cid:44) PX1,...,Xs .

Replacing at least one of the functions fi, i ∈ N1:s by a
constant ♠ yields a new SEM. We say Xi has been intervened
on, which is denoted by do(Xi = ♠), leads to the SEM
S| do(Xi = ♠), and induces the interventional distribution
PS| do(Xi=♠) (cid:44) PX1,...,Xs| do(Xi=♠).

Deﬁnition 2 (Cause and Effect). Xi is a cause of Xj (i, j ∈
N1:s, i (cid:54)= j) wrt. a SEM S iff there exists ♥ ∈ R such that
PXj | do(Xi=♥) (cid:54)= PXj .1 Xj is an effect of Xi iff Xi is a cause
of Xj. Often the considered SEM S is omitted if it is clear
from the context.

1PXj | do(Xi=♥) and PXj denote the marginal distributions of Xj corre-

sponding to PS| do(Xi=♥) and PS respectively.

This is the author’s version of an article that is published in Pattern Recognition in Neuroimaging (PRNI), International Workshop on, 1–4, 2016, doi: 10.1109/PRNI.2016.7552331.
Copyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

2

For each SEM S there is a corresponding graph GS (V, E)
with V (cid:44) {X1, ..., Xs} and E (cid:44) {(Xi, Xj) : Xi ∈
PAj, Xj ∈ V } that has the random variables as nodes and
directed edges pointing from parents to children. We employ
the common assumption that this graph is acyclic, i.e., GS will
always be a directed acyclic graph (DAG).

So far a DAG GS simply depicts all parent-child relation-
ships deﬁned by the SEM S. Missing directed paths indicate
missing cause-effect relationships. In order to specify the link
between statistical independence (denoted by ⊥⊥) wrt. the joint
distribution PS and properties of the DAG GS (representing a
SEM S) we need the following deﬁnition.

Deﬁnition 3 (d-separation). For a ﬁxed graph G disjoint sets
of nodes A and B are d-separated by a third disjoint set C
(denoted by A ⊥d-sep B|C) iff all pairs of nodes a ∈ A and
b ∈ B are d-separated by C. A pair of nodes a (cid:54)= b is d-
separated by C iff every path between a and b is blocked by
C. A path between nodes a and b is blocked by C iff there is
an intermediate node z on the path such that (i) z ∈ C and z is
tail-to-tail (← z →) or head-to-tail (→ z →), or (ii) z is head-
to-head (→ z ←) and neither z nor any of its descendants is
in C.

assuming

Conveniently,
Markovianity3) we
correspondence
between
independence statements:

have

faithfulness2

(and

the

d-separation

following
and

exploiting
one-to-one
conditional

A ⊥d-sep B|C ⇐⇒ A ⊥⊥ B|C

Summing up, we have deﬁned interventional causation in
terms of SEMs and have seen how a SEM gives rise to
a DAG. This DAG has two convenient features. Firstly, the
DAG yields a visualisation that allows to easily grasp missing
cause-effect relationships that correspond to missing directed
paths. Secondly, since we assume faithfulness, d-separation
properties of this DAG are equivalent to conditional indepen-
dence properties of the joint distribution. Thus, conditional
independences translate into causal statements, e.g. ‘a variable
becomes independent of all its non-effects given its immedi-
ate causes’ or ‘cause and effect are marginally dependent’.
the causal graph GS can be identiﬁed from
Furthermore,
conditional independences observed in PS – at least up to
a so-called Markov equivalence class, the set of graphs that
entail the same conditional independences [10].

B. Formal problem description

In the following, the variables S, C1, ..., Cd, and F1, ..., Fd
may be thought of as a stimulus variable, the activity of
multiple cortical sources, and the EEG channel recordings
respectively. We aim at recovering an effect of a pre-deﬁned
target variable C1 = v(cid:62)F . The terminology introduced in
Section II-A allows to precisely state the problem as follows.

2Intuitively, this is saying that conditional independences are due to the
causal structure and not accidents of parameter values [6, p. 9]; more formally
the assumption reads A ⊥d-sep B|C ⇐= A ⊥⊥ B|C.

3The distribution PS generated by a SEM S is Markov wrt. GS (cf. [7,

Theorem 1.4.1] for a proof), i. e., A ⊥d-sep B|C =⇒ A ⊥⊥ B|C.

1) Assumptions: Let S and C1, ..., Cd denote (ﬁnitely
many) random variables. We assume existence of a SEM S,
potentially with additional unobserved variables h1, ..., hl, that
induces PS = PS,C1,...,Cd,h1,...,hl . We refer to the correspond-
ing graph GS as the true causal graph and call its nodes causal
variables. We further assume that

• S affects C2 indirectly via C1,4
• there are no edges in GS pointing into S.5

Importantly, we do not require that the structural equation that
relates C2 to its parents is linear in C1. Figure 1 depicts an
example of how GS might look like.

S

C1

C2

C5
...

Cd

C3

h1

C4

Fig. 1. Example graph where h1 is a hidden variable.

2) Given data:
• v ∈ Rd such that C1 = v(cid:62)F
• m iid6 samples S = [s1, ..., sm](cid:62) of S and F =
[fi,j]i=1:m,j=1:d of F where F (cid:44) [F1, ..., Fd](cid:62) = AC
is the observed linear mixture of the causal variables
C (cid:44) [C1, ..., Cd](cid:62) and A ∈ Rd×d the mixing matrix
3) Desired output: Find w ∈ Bd (cid:44) {x ∈ Rd : ||x|| = 1}
such that aCi = w(cid:62)F where Ci is an effect of C1 (i ∈
N2:d, a ∈ R \ {0}). For the graph shown in Figure 1 recovery
of the causal variable C2 is a valid solution.

C. Strategy

Our approach leverages the following causal inference rule
that – under the assumptions in Section II-B1 – applies to a
causal variable Ci (cf. [5]).

Causal Inference Rule: If Ci ⊥⊥ S|C1 and Y (cid:54)⊥⊥ C1, then
S indirectly affects Ci via C1. In particular, a causal path
C1 (cid:57)(cid:57)(cid:75) Ci exists.

The idea is to recover

the sought-after variable from
the mixture F by optimising for these statistical properties.
Thence, the general strategy relies on solving an optimisation
problem of the form7

max
w∈Bd−1

dep(C1, Yw) − dep(S, Yw|C1)

where Yw = w(cid:62)F and dep denotes a (conditional) de-
pendence criterion that estimates from empirical samples the
strength of association between the two variables.

4By saying a variable X causes Z indirectly via Y we imply (a) existence
of a path X (cid:57)(cid:57)(cid:75) Y (cid:57)(cid:57)(cid:75) Z, and (b) that there is no path X (cid:57)(cid:57)(cid:75) Z without Y
on it (this also excludes the edge X → Z).

5This condition can for example be ensured by randomising S.
6independent and identically distributed
7To attenuate the signal of C1 we restrict search onto the orthogonal
||x|| = 1, x ⊥ v} which is

complement v⊥ ∩ Bd = {x ∈ Rd :
diffeomorphic to Bd−1.

D. Non-Linear MERLiN algorithm

The linear MERLiN algorithm uses the partial correla-
tions ρC1,Yw|S and ρS,Yw|C1 for the terms dep(C1, Yw) and
dep(S, Yw|C1) in the objective function. As such only linear
dependence between C1 and Yw can be detected while re-
maining higher-order dependences between S and Yw given
C1 may go undetected. A general kernel-based indepen-
dence criterion, the Hilbert-Schmidt Independence Criterion
(HSIC) [11], and a regression-based conditional independence
criterion (cf. [5]) in conjunction with kernel ridge regres-
sion [12] allow extension to non-linear dependences.

Regression-Based Conditional Independence Criterion:
If there exists a (regression) function r such that Yw−r(C1) ⊥⊥
(S, C1) then S ⊥⊥ Yw|C1.

The non-linear MERLiN algorithm solves the following

optimisation problem

max
(w,σ,θ)∈Bd−1×R×R

HSIC(C1, Yw) − HSIC ((S, C1), Rw,σ,θ)

where HSIC(A, B) denotes the empirical HSIC estimate8 and
Rw,σ,θ corresponds to the residuals Yw − r(C1) using kernel
ridge regression with Gaussian kernel of width |σ| and ridge
regression parameter |θ|. To temper overﬁtting, the sample is
split into three partitions; the residuals of the ith partition are
obtained by using the kernel ridge regression function obtained
on the remaining partitions. The regression parameters σ and
θ are also being optimised over to allow an optimal regres-
sion ﬁt wrt. witnessing conditional independence and hence
minimising the second summand in the objective function.

Implementing the objective function in Theano [13], [14],
we use the Python toolbox Pymanopt [15] to run optimisation
on the product manifold Bd−1 × R × R using a steepest
descent algorithm with standard back-tracking line-search.
This approach is exact and efﬁcient, relying on automated
differentiation and respecting the manifold geometry.

E. Application to EEG data

We consider EEG trial-data of the form (cid:101)F ∈ Rd×m×n
where d denotes the number of electrodes, m the number of
trials, and n the length of the time series (cid:101)Fi,j,1:n for each
electrode i ∈ N1:d and each sample j ∈ N1:m; that is (cid:101)F holds
m iid samples of a Rd×n-valued random variable (cid:101)F . Analyses
of EEG data commonly focus on trial-averaged log-bandpower
in a particular frequency band. Accordingly, applying our al-
gorithms to EEG data we aim to identify a linear combination
w ∈ Bd such that the log-bandpower of the resulting one-
dimensional trial signals w(cid:62) (cid:101)F is a causal effect of the log-
bandpower of the one-dimensional trial signals v(cid:62) (cid:101)F .

However,

the two operations of computing the log-
bandpower and taking a linear combination do not commute.
The log-bandpower computation needs to be switched into the
objective function described above. This is accomplished by
letting Yw = logbp(w(cid:62) (cid:101)F ) and C1 = logbp(v(cid:62) (cid:101)F ) where
logbp denotes the operation of applying a Hanning window

8We compute the empirical HSIC estimate based on the Gaussian kernel
k(x, y) = exp(−δ−1||x − y||2) where the kernel size δ is determined by
the median distance between points in input space [11].

3

and computing the average log-bandpower in a speciﬁed
frequency range for the given time series.

III. COMPARATIVE EEG ANALYSIS

A. Experimental data

We applied the linear MERLiN algorithm and its non-
linear extension to EEG data recorded during a neurofeedback
experiment [16]. In this study the γ-log-bandpower (55–85 Hz)
in the right superior parietal cortex (SPC) was provided as
feedback signal and subjects were instructed to up- or down-
regulate the bandpower. 3 subjects were recorded in 2 sessions
each and each session had 60 trials á 60 seconds.

The data of one session consists of a stimulus vector S ∈
{−1, +1}60×1, a spatial ﬁlter v ∈ R121×1 that was used to
extract the feedback signal, and a tensor (cid:101)F ∈ R121×60×15000
that holds the time series (of length 15000) for each channel
and trial. The reader is referred to [16] for more details on the
experimental data.

B. How to compare to previous results

We compare our MERLiN algorithms against a causal
analysis of this neurofeedback experiment that is based on
source localisation in combination with an exhaustive search
procedure [5]. The hypothesis was, based on transcranial
magnetic stimulation studies [17], that γ-oscillations in the
SPC modulate γ-oscillations in the medial prefrontal cortex
(MPC). We brieﬂy describe this exhaustive search approach.
First, the signal of K (cid:44) 15028 dipoles across the cortical
surface was extracted using a LCMV beamformer and a three-
shell spherical head model [18]. Then, the authors applied their
newly introduced stimulus-based causal inference (SCI) algo-
rithm to assess for every dipole whether its γ-log-bandpower
is a linear causal effect of the γ-log-bandpower in the SPC.
Group results were summarised in a vector gSCI ∈ RK×1
where the ith entry denotes the percentage of dipoles within a
certain radius that were found to be modulated by the SPC.
The results of this exhaustive search analysis, visualising gSCI
on hemisphere plots, supported the hypothesis that the MPC
is a linear causal effect of the SPC. The reader is referred to
[5] for more details.

In contrast to exhaustive search, both our linear MERLiN
algorithm as well as its non-linear extension aim at imme-
diately recovering the causal effect by optimising a linear
combination w of electrode signals9. To allow for a qualitative
comparison of our results with the results summarised by
the vector gSCI we derive for each w a vector g ∈ RK×1.
This vector represents the involvement of each cortical dipole
in the recovered signal and is derived from w as follows.
First, a scalp topography is obtained via a ∝ Σw where
the ith entry of Σw is the covariance between the ith EEG
channel and the source that is recovered by w [19, Equation
(7)]. Here Σ denotes the session-speciﬁc covariance matrix
in the γ-frequency band. Second,
the dipole involvement

9Since there were only 60 samples per session we decided to select a
subset of 33 EEG channels distributed across the scalp (according to the
10–20 system). Hence, for each recording session we obtained a spatial ﬁlter
w ∈ R33×1.

4

vector g is obtained from a via dynamic statistical parametric
mapping (dSPM; with identity noise covariance matrix) [20].
Group results are obtained as average of the individual dipole
involvement vectors.

C. Experimental results

The group averaged results of our extended algorithm are
depicted in Figure 2.(a). Similar to the results in [5] and
the results we obtained with the linear MERLiN algorithm
(cf. Figure 2.(b)) the analysis indicates that the MPC is a
causal effect of the SPC. The non-linear method yields results
that are in high accordance with the ones obtained by our
linear method while exhaustive search additionally revealed
the anterior middle frontal gyrus as effect of the SPC.

Left hemisphere

Right hemisphere

(a) non-linear

Left hemisphere

Right hemisphere

w
e
i
v

l
a
r
e
t
a
L

w
e
i
v

l
a
i
d
e
M

w
e
i
v

l
a
r
e
t
a
L

w
e
i
v

l
a
i
d
e
M

(b) linear

Fig. 2. Group averaged dipole involvement corresponding to the spatial ﬁlters
identiﬁed by the (a) non-linear and (b) linear MERLiN algorithm; lateral and
medial views of the left and right hemisphere. (All colour scales from “blue”
to “red” range from 0 to the largest value to be plotted.)

IV. CONCLUSIONS
We have developed the non-linear MERLiN algorithm that
is able to recover a causal effect from an observed linear
mixture with no constraint on the functional form of this
cause-effect relationship. Iteratively projecting out directions
and applying the MERLiN algorithm may allow to identify
multiple distinct causal effects. For EEG data we found no

qualitative difference to the linear method, which indicates
that linear methods sufﬁce to identify within-frequency cause-
effect relationships in EEG data. Future research will focus on
theoretical analysis of the presented methods and assumptions
and investigate applicability to other real world data.

REFERENCES

[1] P. L. Nunez and R. Srinivasan, Electric Fields of

the Brain: The

Neurophysics of EEG. Oxford University Press, 2006.

[2] M. Grosse-Wentrup, “Understanding brain connectivity patterns during
motor imagery for brain-computer interfacing,” in Advances in Neural
Information Processing Systems, 2009, pp. 561–568.

[3] S. Weichwald, M. Grosse-Wentrup, and A. Gretton, “MERLiN: Mixture
Effect Recovery in Linear Networks,” arXiv preprint arXiv:1512.01255,
2015.

[4] S. Weichwald, T. Meyer, O. Özdenizci, B. Schölkopf, T. Ball, and
interpretation rules for encoding and
M. Grosse-Wentrup, “Causal
decoding models in neuroimaging,” NeuroImage, vol. 110, pp. 48–59,
2015.

[5] M. Grosse-Wentrup, D. Janzing, M. Siegel, and B. Schölkopf, “Identiﬁ-
cation of causal relations in neuroimaging data with latent confounders:
An instrumental variable approach,” NeuroImage, vol. 125, pp. 825–833,
2016.

[6] P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction, and

Search, 2nd ed. MIT press, 2000.

[7] J. Pearl, Causality: Models, Reasoning and Inference, 2nd ed. Cam-

bridge University Press, 2009.

[8] M. Eichler and V. Didelez, “On Granger causality and the effect of
interventions in time series,” Lifetime Data Analysis, vol. 16, no. 1, pp.
3–32, 2010.

[9] J. T. Lizier and M. Prokopenko, “Differentiating information transfer
and causal effect,” The European Physical Journal B, vol. 73, no. 4, pp.
605–615, 2010.

[10] T. Verma and J. Pearl, “Equivalence and synthesis of causal models,”
the 6th Conference on Uncertainty in Artiﬁcial

in Proceedings of
Intelligence. Elsevier, 1990, pp. 255–268.

[11] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Schölkopf, and A. J.
Smola, “A Kernel Statistical Test of Independence,” in Advances in
Neural Information Processing Systems 20, 2008, pp. 585–592.
[12] C. Saunders, A. Gammerman, and V. Vovk, “Ridge Regression Learning
Algorithm in Dual Variables,” in Proceedings of the 15th International
Conference on Machine Learning, 1998, pp. 515–521.

[13] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Des-
jardins, J. Turian, D. Warde-Farley, and Y. Bengio, “Theano: A CPU
and GPU math expression compiler,” in Proceedings of the Python for
Scientiﬁc Computing Conference (SciPy), Jun. 2010.

[14] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow,
A. Bergeron, N. Bouchard, and Y. Bengio, “Theano: New features
and speed improvements,” Deep Learning and Unsupervised Feature
Learning NIPS 2012 Workshop, 2012.

[15] J. Townsend, N. Koep, and S. Weichwald, “Pymanopt: A Python
Toolbox for Manifold Optimization using Automatic Differentiation,”
arXiv preprint arXiv:1603.03236, 2016.

[16] M. Grosse-Wentrup and B. Schölkopf, “A brain–computer interface
based on self-regulation of gamma-oscillations in the superior parietal
cortex,” Journal of Neural Engineering, vol. 11, no. 5, p. 056015, 2014.
[17] A. C. Chen, D. J. Oathes, C. Chang, T. Bradley, Z.-W. Zhou, L. M.
Williams, G. H. Glover, K. Deisseroth, and A. Etkin, “Causal
in-
teractions between fronto-parietal central executive and default-mode
networks in humans,” Proceedings of the National Academy of Sciences,
vol. 110, no. 49, pp. 19 944–19 949, 2013.

[18] J. C. Mosher, R. M. Leahy, and P. S. Lewis, “EEG and MEG: Forward
solutions for inverse methods,” Biomedical Engineering, IEEE Transac-
tions on, vol. 46, no. 3, pp. 245–259, 1999.

[19] S. Haufe, F. Meinecke, K. Görgen, S. Dähne, J.-D. Haynes, B. Blankertz,
and F. Bießmann, “On the interpretation of weight vectors of linear
models in multivariate neuroimaging,” NeuroImage, vol. 87, pp. 96–110,
2014.

[20] A. M. Dale, A. K. Liu, B. R. Fischl, R. L. Buckner, J. W. Belliveau,
J. D. Lewine, and E. Halgren, “Dynamic statistical parametric mapping:
Combining fMRI and MEG for high-resolution imaging of cortical
activity,” Neuron, vol. 26, no. 1, pp. 55–67, 2000.

Recovery of non-linear cause-effect relationships
from linearly mixed neuroimaging data

Sebastian Weichwald, Arthur Gretton, Bernhard Schölkopf, Moritz Grosse-Wentrup

1

6
1
0
2
 
p
e
S
 
0
3
 
 
]
E
M

.
t
a
t
s
[
 
 
2
v
1
9
3
0
0
.
5
0
6
1
:
v
i
X
r
a

Abstract—Causal

the identiﬁcation of
inference concerns
cause-effect relationships between variables. However, often only
linear combinations of variables constitute meaningful causal
variables. For example, recovering the signal of a cortical source
from electroencephalography requires a well-tuned combination
of signals recorded at multiple electrodes. We recently introduced
the MERLiN (Mixture Effect Recovery in Linear Networks)
algorithm that is able to recover, from an observed linear
mixture, a causal variable that is a linear effect of another
given variable. Here we relax the assumption of this cause-effect
relationship being linear and present an extended algorithm
that can pick up non-linear cause-effect relationships. Thus,
the main contribution is an algorithm (and ready to use code)
that has broader applicability and allows for a richer model
class. Furthermore, a comparative analysis indicates that the
assumption of linear cause-effect relationships is not restrictive
in analysing electroencephalographic data.

Index Terms—causal inference, causal variable construction,
instrumental variable, linear mixtures, regression-based condi-
tional independence criterion

I. INTRODUCTION

Causal inference requires causal variables. However, not
always do the variables in a dataset specify the candidate
causal relata. In electroencephalography (EEG) studies, for
example, what is measured at electrodes placed on the scalp
is instantaneously and linearly superimposed electromagnetic
activity of sources in the brain [1]. Standard causal inference
methods require to ﬁrst recover the cortical sources from the
observed electrode signals [2]. This is disadvantageous. First,
any source localisation procedure is prone to modelling errors,
which may distort the true cause-effect relationships between
cortical sources. Second, source localisation enlarges the data
dimensionality by roughly two orders of magnitude, which
leads to increased computational complexity.

We recently proposed a novel idea to construct causal vari-
ables, i. e., recover cortical sources, by directly optimising for
statistical in- and dependences that imply a certain cause-effect
relationship [3]. The linear MERLiN algorithm can – skipping
potentially error prone modelling steps – establish a linear
cause-effect relationship between brain state features that are
observed only as part of a linear mixture. This allows for
computationally efﬁcient insights into brain networks beyond
those readily obtained from encoding and decoding models

SW, BS, and MGW are with the Empirical Inference Department, Max
Planck Institute for Intelligent Systems, Tübingen, Germany, e-mail: [sweich-
wald, bs, moritzgw]@tue.mpg.de.

AG is with the Gatsby Computational Neuroscience Unit, Sainsbury Well-

come Centre, London, United Kingdom, e-mail: arthur.gretton@gmail.com.

trained on pre-deﬁned variables [4]. The linear MERLiN
algorithm, however, is unable to reconstruct cortical sources
with non-linear cause-effect relationships.

Here we present the non-linear MERLiN algorithm and
relax the assumption of linear cause-effect relationships. By
integrating kernel ridge regression and a non-linear indepen-
dence test, the extended algorithm can capture any higher order
dependence. We compare the results of our linear- and non-
linear MERLiN algorithms on EEG data for which cause-
effect relationships have previously only been computed by
an exhaustive search approach [5] and ﬁnd no qualitative
differences. The contribution of this work is thus two-fold.
First, we provide an algorithm to learn non-linear cause-effect
relationships from linear mixtures of causal variables, and,
second, we provide empirical evidence that linear methods
sufﬁce to identify cause-effect relationships within individual
EEG frequency bands. The Python implementation is available
at https://github.com/sweichwald/MERLiN.

A. Causal Bayesian Networks

II. METHODS

We brieﬂy introduce the main aspects of Causal Bayesian
Networks (CBNs). For an exhaustive treatment see [6], [7].
The important advantage of this framework over methods
based on information ﬂow is that it yields testable predictions
on the impact of interventions [8], [9].

Deﬁnition 1 (Structural Equation Model). We deﬁne a struc-
tural equation model (SEM) S as a set of equations Xi =
i ∈ N1:s where the so-called noise vari-
fi(PAi, Ni),
ables are independently distributed according to PN1,...,Ns =
PN1 · · · PNs . For i ∈ N1:s the set PAi ⊆ {X1, ..., Xs} \ Xi
contains the so-called parents of Xi and fi describes how Xi
relates to the random variables in PAi and Ni. The induced
joint distribution is denoted by PS (cid:44) PX1,...,Xs .

Replacing at least one of the functions fi, i ∈ N1:s by a
constant ♠ yields a new SEM. We say Xi has been intervened
on, which is denoted by do(Xi = ♠), leads to the SEM
S| do(Xi = ♠), and induces the interventional distribution
PS| do(Xi=♠) (cid:44) PX1,...,Xs| do(Xi=♠).

Deﬁnition 2 (Cause and Effect). Xi is a cause of Xj (i, j ∈
N1:s, i (cid:54)= j) wrt. a SEM S iff there exists ♥ ∈ R such that
PXj | do(Xi=♥) (cid:54)= PXj .1 Xj is an effect of Xi iff Xi is a cause
of Xj. Often the considered SEM S is omitted if it is clear
from the context.

1PXj | do(Xi=♥) and PXj denote the marginal distributions of Xj corre-

sponding to PS| do(Xi=♥) and PS respectively.

This is the author’s version of an article that is published in Pattern Recognition in Neuroimaging (PRNI), International Workshop on, 1–4, 2016, doi: 10.1109/PRNI.2016.7552331.
Copyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

2

For each SEM S there is a corresponding graph GS (V, E)
with V (cid:44) {X1, ..., Xs} and E (cid:44) {(Xi, Xj) : Xi ∈
PAj, Xj ∈ V } that has the random variables as nodes and
directed edges pointing from parents to children. We employ
the common assumption that this graph is acyclic, i.e., GS will
always be a directed acyclic graph (DAG).

So far a DAG GS simply depicts all parent-child relation-
ships deﬁned by the SEM S. Missing directed paths indicate
missing cause-effect relationships. In order to specify the link
between statistical independence (denoted by ⊥⊥) wrt. the joint
distribution PS and properties of the DAG GS (representing a
SEM S) we need the following deﬁnition.

Deﬁnition 3 (d-separation). For a ﬁxed graph G disjoint sets
of nodes A and B are d-separated by a third disjoint set C
(denoted by A ⊥d-sep B|C) iff all pairs of nodes a ∈ A and
b ∈ B are d-separated by C. A pair of nodes a (cid:54)= b is d-
separated by C iff every path between a and b is blocked by
C. A path between nodes a and b is blocked by C iff there is
an intermediate node z on the path such that (i) z ∈ C and z is
tail-to-tail (← z →) or head-to-tail (→ z →), or (ii) z is head-
to-head (→ z ←) and neither z nor any of its descendants is
in C.

assuming

Conveniently,
Markovianity3) we
correspondence
between
independence statements:

have

faithfulness2

(and

the

d-separation

following
and

exploiting
one-to-one
conditional

A ⊥d-sep B|C ⇐⇒ A ⊥⊥ B|C

Summing up, we have deﬁned interventional causation in
terms of SEMs and have seen how a SEM gives rise to
a DAG. This DAG has two convenient features. Firstly, the
DAG yields a visualisation that allows to easily grasp missing
cause-effect relationships that correspond to missing directed
paths. Secondly, since we assume faithfulness, d-separation
properties of this DAG are equivalent to conditional indepen-
dence properties of the joint distribution. Thus, conditional
independences translate into causal statements, e.g. ‘a variable
becomes independent of all its non-effects given its immedi-
ate causes’ or ‘cause and effect are marginally dependent’.
the causal graph GS can be identiﬁed from
Furthermore,
conditional independences observed in PS – at least up to
a so-called Markov equivalence class, the set of graphs that
entail the same conditional independences [10].

B. Formal problem description

In the following, the variables S, C1, ..., Cd, and F1, ..., Fd
may be thought of as a stimulus variable, the activity of
multiple cortical sources, and the EEG channel recordings
respectively. We aim at recovering an effect of a pre-deﬁned
target variable C1 = v(cid:62)F . The terminology introduced in
Section II-A allows to precisely state the problem as follows.

2Intuitively, this is saying that conditional independences are due to the
causal structure and not accidents of parameter values [6, p. 9]; more formally
the assumption reads A ⊥d-sep B|C ⇐= A ⊥⊥ B|C.

3The distribution PS generated by a SEM S is Markov wrt. GS (cf. [7,

Theorem 1.4.1] for a proof), i. e., A ⊥d-sep B|C =⇒ A ⊥⊥ B|C.

1) Assumptions: Let S and C1, ..., Cd denote (ﬁnitely
many) random variables. We assume existence of a SEM S,
potentially with additional unobserved variables h1, ..., hl, that
induces PS = PS,C1,...,Cd,h1,...,hl . We refer to the correspond-
ing graph GS as the true causal graph and call its nodes causal
variables. We further assume that

• S affects C2 indirectly via C1,4
• there are no edges in GS pointing into S.5

Importantly, we do not require that the structural equation that
relates C2 to its parents is linear in C1. Figure 1 depicts an
example of how GS might look like.

S

C1

C2

C5
...

Cd

C3

h1

C4

Fig. 1. Example graph where h1 is a hidden variable.

2) Given data:
• v ∈ Rd such that C1 = v(cid:62)F
• m iid6 samples S = [s1, ..., sm](cid:62) of S and F =
[fi,j]i=1:m,j=1:d of F where F (cid:44) [F1, ..., Fd](cid:62) = AC
is the observed linear mixture of the causal variables
C (cid:44) [C1, ..., Cd](cid:62) and A ∈ Rd×d the mixing matrix
3) Desired output: Find w ∈ Bd (cid:44) {x ∈ Rd : ||x|| = 1}
such that aCi = w(cid:62)F where Ci is an effect of C1 (i ∈
N2:d, a ∈ R \ {0}). For the graph shown in Figure 1 recovery
of the causal variable C2 is a valid solution.

C. Strategy

Our approach leverages the following causal inference rule
that – under the assumptions in Section II-B1 – applies to a
causal variable Ci (cf. [5]).

Causal Inference Rule: If Ci ⊥⊥ S|C1 and Y (cid:54)⊥⊥ C1, then
S indirectly affects Ci via C1. In particular, a causal path
C1 (cid:57)(cid:57)(cid:75) Ci exists.

The idea is to recover

the sought-after variable from
the mixture F by optimising for these statistical properties.
Thence, the general strategy relies on solving an optimisation
problem of the form7

max
w∈Bd−1

dep(C1, Yw) − dep(S, Yw|C1)

where Yw = w(cid:62)F and dep denotes a (conditional) de-
pendence criterion that estimates from empirical samples the
strength of association between the two variables.

4By saying a variable X causes Z indirectly via Y we imply (a) existence
of a path X (cid:57)(cid:57)(cid:75) Y (cid:57)(cid:57)(cid:75) Z, and (b) that there is no path X (cid:57)(cid:57)(cid:75) Z without Y
on it (this also excludes the edge X → Z).

5This condition can for example be ensured by randomising S.
6independent and identically distributed
7To attenuate the signal of C1 we restrict search onto the orthogonal
||x|| = 1, x ⊥ v} which is

complement v⊥ ∩ Bd = {x ∈ Rd :
diffeomorphic to Bd−1.

D. Non-Linear MERLiN algorithm

The linear MERLiN algorithm uses the partial correla-
tions ρC1,Yw|S and ρS,Yw|C1 for the terms dep(C1, Yw) and
dep(S, Yw|C1) in the objective function. As such only linear
dependence between C1 and Yw can be detected while re-
maining higher-order dependences between S and Yw given
C1 may go undetected. A general kernel-based indepen-
dence criterion, the Hilbert-Schmidt Independence Criterion
(HSIC) [11], and a regression-based conditional independence
criterion (cf. [5]) in conjunction with kernel ridge regres-
sion [12] allow extension to non-linear dependences.

Regression-Based Conditional Independence Criterion:
If there exists a (regression) function r such that Yw−r(C1) ⊥⊥
(S, C1) then S ⊥⊥ Yw|C1.

The non-linear MERLiN algorithm solves the following

optimisation problem

max
(w,σ,θ)∈Bd−1×R×R

HSIC(C1, Yw) − HSIC ((S, C1), Rw,σ,θ)

where HSIC(A, B) denotes the empirical HSIC estimate8 and
Rw,σ,θ corresponds to the residuals Yw − r(C1) using kernel
ridge regression with Gaussian kernel of width |σ| and ridge
regression parameter |θ|. To temper overﬁtting, the sample is
split into three partitions; the residuals of the ith partition are
obtained by using the kernel ridge regression function obtained
on the remaining partitions. The regression parameters σ and
θ are also being optimised over to allow an optimal regres-
sion ﬁt wrt. witnessing conditional independence and hence
minimising the second summand in the objective function.

Implementing the objective function in Theano [13], [14],
we use the Python toolbox Pymanopt [15] to run optimisation
on the product manifold Bd−1 × R × R using a steepest
descent algorithm with standard back-tracking line-search.
This approach is exact and efﬁcient, relying on automated
differentiation and respecting the manifold geometry.

E. Application to EEG data

We consider EEG trial-data of the form (cid:101)F ∈ Rd×m×n
where d denotes the number of electrodes, m the number of
trials, and n the length of the time series (cid:101)Fi,j,1:n for each
electrode i ∈ N1:d and each sample j ∈ N1:m; that is (cid:101)F holds
m iid samples of a Rd×n-valued random variable (cid:101)F . Analyses
of EEG data commonly focus on trial-averaged log-bandpower
in a particular frequency band. Accordingly, applying our al-
gorithms to EEG data we aim to identify a linear combination
w ∈ Bd such that the log-bandpower of the resulting one-
dimensional trial signals w(cid:62) (cid:101)F is a causal effect of the log-
bandpower of the one-dimensional trial signals v(cid:62) (cid:101)F .

However,

the two operations of computing the log-
bandpower and taking a linear combination do not commute.
The log-bandpower computation needs to be switched into the
objective function described above. This is accomplished by
letting Yw = logbp(w(cid:62) (cid:101)F ) and C1 = logbp(v(cid:62) (cid:101)F ) where
logbp denotes the operation of applying a Hanning window

8We compute the empirical HSIC estimate based on the Gaussian kernel
k(x, y) = exp(−δ−1||x − y||2) where the kernel size δ is determined by
the median distance between points in input space [11].

3

and computing the average log-bandpower in a speciﬁed
frequency range for the given time series.

III. COMPARATIVE EEG ANALYSIS

A. Experimental data

We applied the linear MERLiN algorithm and its non-
linear extension to EEG data recorded during a neurofeedback
experiment [16]. In this study the γ-log-bandpower (55–85 Hz)
in the right superior parietal cortex (SPC) was provided as
feedback signal and subjects were instructed to up- or down-
regulate the bandpower. 3 subjects were recorded in 2 sessions
each and each session had 60 trials á 60 seconds.

The data of one session consists of a stimulus vector S ∈
{−1, +1}60×1, a spatial ﬁlter v ∈ R121×1 that was used to
extract the feedback signal, and a tensor (cid:101)F ∈ R121×60×15000
that holds the time series (of length 15000) for each channel
and trial. The reader is referred to [16] for more details on the
experimental data.

B. How to compare to previous results

We compare our MERLiN algorithms against a causal
analysis of this neurofeedback experiment that is based on
source localisation in combination with an exhaustive search
procedure [5]. The hypothesis was, based on transcranial
magnetic stimulation studies [17], that γ-oscillations in the
SPC modulate γ-oscillations in the medial prefrontal cortex
(MPC). We brieﬂy describe this exhaustive search approach.
First, the signal of K (cid:44) 15028 dipoles across the cortical
surface was extracted using a LCMV beamformer and a three-
shell spherical head model [18]. Then, the authors applied their
newly introduced stimulus-based causal inference (SCI) algo-
rithm to assess for every dipole whether its γ-log-bandpower
is a linear causal effect of the γ-log-bandpower in the SPC.
Group results were summarised in a vector gSCI ∈ RK×1
where the ith entry denotes the percentage of dipoles within a
certain radius that were found to be modulated by the SPC.
The results of this exhaustive search analysis, visualising gSCI
on hemisphere plots, supported the hypothesis that the MPC
is a linear causal effect of the SPC. The reader is referred to
[5] for more details.

In contrast to exhaustive search, both our linear MERLiN
algorithm as well as its non-linear extension aim at imme-
diately recovering the causal effect by optimising a linear
combination w of electrode signals9. To allow for a qualitative
comparison of our results with the results summarised by
the vector gSCI we derive for each w a vector g ∈ RK×1.
This vector represents the involvement of each cortical dipole
in the recovered signal and is derived from w as follows.
First, a scalp topography is obtained via a ∝ Σw where
the ith entry of Σw is the covariance between the ith EEG
channel and the source that is recovered by w [19, Equation
(7)]. Here Σ denotes the session-speciﬁc covariance matrix
in the γ-frequency band. Second,
the dipole involvement

9Since there were only 60 samples per session we decided to select a
subset of 33 EEG channels distributed across the scalp (according to the
10–20 system). Hence, for each recording session we obtained a spatial ﬁlter
w ∈ R33×1.

4

vector g is obtained from a via dynamic statistical parametric
mapping (dSPM; with identity noise covariance matrix) [20].
Group results are obtained as average of the individual dipole
involvement vectors.

C. Experimental results

The group averaged results of our extended algorithm are
depicted in Figure 2.(a). Similar to the results in [5] and
the results we obtained with the linear MERLiN algorithm
(cf. Figure 2.(b)) the analysis indicates that the MPC is a
causal effect of the SPC. The non-linear method yields results
that are in high accordance with the ones obtained by our
linear method while exhaustive search additionally revealed
the anterior middle frontal gyrus as effect of the SPC.

Left hemisphere

Right hemisphere

(a) non-linear

Left hemisphere

Right hemisphere

w
e
i
v

l
a
r
e
t
a
L

w
e
i
v

l
a
i
d
e
M

w
e
i
v

l
a
r
e
t
a
L

w
e
i
v

l
a
i
d
e
M

(b) linear

Fig. 2. Group averaged dipole involvement corresponding to the spatial ﬁlters
identiﬁed by the (a) non-linear and (b) linear MERLiN algorithm; lateral and
medial views of the left and right hemisphere. (All colour scales from “blue”
to “red” range from 0 to the largest value to be plotted.)

IV. CONCLUSIONS
We have developed the non-linear MERLiN algorithm that
is able to recover a causal effect from an observed linear
mixture with no constraint on the functional form of this
cause-effect relationship. Iteratively projecting out directions
and applying the MERLiN algorithm may allow to identify
multiple distinct causal effects. For EEG data we found no

qualitative difference to the linear method, which indicates
that linear methods sufﬁce to identify within-frequency cause-
effect relationships in EEG data. Future research will focus on
theoretical analysis of the presented methods and assumptions
and investigate applicability to other real world data.

REFERENCES

[1] P. L. Nunez and R. Srinivasan, Electric Fields of

the Brain: The

Neurophysics of EEG. Oxford University Press, 2006.

[2] M. Grosse-Wentrup, “Understanding brain connectivity patterns during
motor imagery for brain-computer interfacing,” in Advances in Neural
Information Processing Systems, 2009, pp. 561–568.

[3] S. Weichwald, M. Grosse-Wentrup, and A. Gretton, “MERLiN: Mixture
Effect Recovery in Linear Networks,” arXiv preprint arXiv:1512.01255,
2015.

[4] S. Weichwald, T. Meyer, O. Özdenizci, B. Schölkopf, T. Ball, and
interpretation rules for encoding and
M. Grosse-Wentrup, “Causal
decoding models in neuroimaging,” NeuroImage, vol. 110, pp. 48–59,
2015.

[5] M. Grosse-Wentrup, D. Janzing, M. Siegel, and B. Schölkopf, “Identiﬁ-
cation of causal relations in neuroimaging data with latent confounders:
An instrumental variable approach,” NeuroImage, vol. 125, pp. 825–833,
2016.

[6] P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction, and

Search, 2nd ed. MIT press, 2000.

[7] J. Pearl, Causality: Models, Reasoning and Inference, 2nd ed. Cam-

bridge University Press, 2009.

[8] M. Eichler and V. Didelez, “On Granger causality and the effect of
interventions in time series,” Lifetime Data Analysis, vol. 16, no. 1, pp.
3–32, 2010.

[9] J. T. Lizier and M. Prokopenko, “Differentiating information transfer
and causal effect,” The European Physical Journal B, vol. 73, no. 4, pp.
605–615, 2010.

[10] T. Verma and J. Pearl, “Equivalence and synthesis of causal models,”
the 6th Conference on Uncertainty in Artiﬁcial

in Proceedings of
Intelligence. Elsevier, 1990, pp. 255–268.

[11] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Schölkopf, and A. J.
Smola, “A Kernel Statistical Test of Independence,” in Advances in
Neural Information Processing Systems 20, 2008, pp. 585–592.
[12] C. Saunders, A. Gammerman, and V. Vovk, “Ridge Regression Learning
Algorithm in Dual Variables,” in Proceedings of the 15th International
Conference on Machine Learning, 1998, pp. 515–521.

[13] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Des-
jardins, J. Turian, D. Warde-Farley, and Y. Bengio, “Theano: A CPU
and GPU math expression compiler,” in Proceedings of the Python for
Scientiﬁc Computing Conference (SciPy), Jun. 2010.

[14] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow,
A. Bergeron, N. Bouchard, and Y. Bengio, “Theano: New features
and speed improvements,” Deep Learning and Unsupervised Feature
Learning NIPS 2012 Workshop, 2012.

[15] J. Townsend, N. Koep, and S. Weichwald, “Pymanopt: A Python
Toolbox for Manifold Optimization using Automatic Differentiation,”
arXiv preprint arXiv:1603.03236, 2016.

[16] M. Grosse-Wentrup and B. Schölkopf, “A brain–computer interface
based on self-regulation of gamma-oscillations in the superior parietal
cortex,” Journal of Neural Engineering, vol. 11, no. 5, p. 056015, 2014.
[17] A. C. Chen, D. J. Oathes, C. Chang, T. Bradley, Z.-W. Zhou, L. M.
Williams, G. H. Glover, K. Deisseroth, and A. Etkin, “Causal
in-
teractions between fronto-parietal central executive and default-mode
networks in humans,” Proceedings of the National Academy of Sciences,
vol. 110, no. 49, pp. 19 944–19 949, 2013.

[18] J. C. Mosher, R. M. Leahy, and P. S. Lewis, “EEG and MEG: Forward
solutions for inverse methods,” Biomedical Engineering, IEEE Transac-
tions on, vol. 46, no. 3, pp. 245–259, 1999.

[19] S. Haufe, F. Meinecke, K. Görgen, S. Dähne, J.-D. Haynes, B. Blankertz,
and F. Bießmann, “On the interpretation of weight vectors of linear
models in multivariate neuroimaging,” NeuroImage, vol. 87, pp. 96–110,
2014.

[20] A. M. Dale, A. K. Liu, B. R. Fischl, R. L. Buckner, J. W. Belliveau,
J. D. Lewine, and E. Halgren, “Dynamic statistical parametric mapping:
Combining fMRI and MEG for high-resolution imaging of cortical
activity,” Neuron, vol. 26, no. 1, pp. 55–67, 2000.

Recovery of non-linear cause-effect relationships
from linearly mixed neuroimaging data

Sebastian Weichwald, Arthur Gretton, Bernhard Schölkopf, Moritz Grosse-Wentrup

1

6
1
0
2
 
p
e
S
 
0
3
 
 
]
E
M

.
t
a
t
s
[
 
 
2
v
1
9
3
0
0
.
5
0
6
1
:
v
i
X
r
a

Abstract—Causal

the identiﬁcation of
inference concerns
cause-effect relationships between variables. However, often only
linear combinations of variables constitute meaningful causal
variables. For example, recovering the signal of a cortical source
from electroencephalography requires a well-tuned combination
of signals recorded at multiple electrodes. We recently introduced
the MERLiN (Mixture Effect Recovery in Linear Networks)
algorithm that is able to recover, from an observed linear
mixture, a causal variable that is a linear effect of another
given variable. Here we relax the assumption of this cause-effect
relationship being linear and present an extended algorithm
that can pick up non-linear cause-effect relationships. Thus,
the main contribution is an algorithm (and ready to use code)
that has broader applicability and allows for a richer model
class. Furthermore, a comparative analysis indicates that the
assumption of linear cause-effect relationships is not restrictive
in analysing electroencephalographic data.

Index Terms—causal inference, causal variable construction,
instrumental variable, linear mixtures, regression-based condi-
tional independence criterion

I. INTRODUCTION

Causal inference requires causal variables. However, not
always do the variables in a dataset specify the candidate
causal relata. In electroencephalography (EEG) studies, for
example, what is measured at electrodes placed on the scalp
is instantaneously and linearly superimposed electromagnetic
activity of sources in the brain [1]. Standard causal inference
methods require to ﬁrst recover the cortical sources from the
observed electrode signals [2]. This is disadvantageous. First,
any source localisation procedure is prone to modelling errors,
which may distort the true cause-effect relationships between
cortical sources. Second, source localisation enlarges the data
dimensionality by roughly two orders of magnitude, which
leads to increased computational complexity.

We recently proposed a novel idea to construct causal vari-
ables, i. e., recover cortical sources, by directly optimising for
statistical in- and dependences that imply a certain cause-effect
relationship [3]. The linear MERLiN algorithm can – skipping
potentially error prone modelling steps – establish a linear
cause-effect relationship between brain state features that are
observed only as part of a linear mixture. This allows for
computationally efﬁcient insights into brain networks beyond
those readily obtained from encoding and decoding models

SW, BS, and MGW are with the Empirical Inference Department, Max
Planck Institute for Intelligent Systems, Tübingen, Germany, e-mail: [sweich-
wald, bs, moritzgw]@tue.mpg.de.

AG is with the Gatsby Computational Neuroscience Unit, Sainsbury Well-

come Centre, London, United Kingdom, e-mail: arthur.gretton@gmail.com.

trained on pre-deﬁned variables [4]. The linear MERLiN
algorithm, however, is unable to reconstruct cortical sources
with non-linear cause-effect relationships.

Here we present the non-linear MERLiN algorithm and
relax the assumption of linear cause-effect relationships. By
integrating kernel ridge regression and a non-linear indepen-
dence test, the extended algorithm can capture any higher order
dependence. We compare the results of our linear- and non-
linear MERLiN algorithms on EEG data for which cause-
effect relationships have previously only been computed by
an exhaustive search approach [5] and ﬁnd no qualitative
differences. The contribution of this work is thus two-fold.
First, we provide an algorithm to learn non-linear cause-effect
relationships from linear mixtures of causal variables, and,
second, we provide empirical evidence that linear methods
sufﬁce to identify cause-effect relationships within individual
EEG frequency bands. The Python implementation is available
at https://github.com/sweichwald/MERLiN.

A. Causal Bayesian Networks

II. METHODS

We brieﬂy introduce the main aspects of Causal Bayesian
Networks (CBNs). For an exhaustive treatment see [6], [7].
The important advantage of this framework over methods
based on information ﬂow is that it yields testable predictions
on the impact of interventions [8], [9].

Deﬁnition 1 (Structural Equation Model). We deﬁne a struc-
tural equation model (SEM) S as a set of equations Xi =
i ∈ N1:s where the so-called noise vari-
fi(PAi, Ni),
ables are independently distributed according to PN1,...,Ns =
PN1 · · · PNs . For i ∈ N1:s the set PAi ⊆ {X1, ..., Xs} \ Xi
contains the so-called parents of Xi and fi describes how Xi
relates to the random variables in PAi and Ni. The induced
joint distribution is denoted by PS (cid:44) PX1,...,Xs .

Replacing at least one of the functions fi, i ∈ N1:s by a
constant ♠ yields a new SEM. We say Xi has been intervened
on, which is denoted by do(Xi = ♠), leads to the SEM
S| do(Xi = ♠), and induces the interventional distribution
PS| do(Xi=♠) (cid:44) PX1,...,Xs| do(Xi=♠).

Deﬁnition 2 (Cause and Effect). Xi is a cause of Xj (i, j ∈
N1:s, i (cid:54)= j) wrt. a SEM S iff there exists ♥ ∈ R such that
PXj | do(Xi=♥) (cid:54)= PXj .1 Xj is an effect of Xi iff Xi is a cause
of Xj. Often the considered SEM S is omitted if it is clear
from the context.

1PXj | do(Xi=♥) and PXj denote the marginal distributions of Xj corre-

sponding to PS| do(Xi=♥) and PS respectively.

This is the author’s version of an article that is published in Pattern Recognition in Neuroimaging (PRNI), International Workshop on, 1–4, 2016, doi: 10.1109/PRNI.2016.7552331.
Copyright (c) 2016 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

2

For each SEM S there is a corresponding graph GS (V, E)
with V (cid:44) {X1, ..., Xs} and E (cid:44) {(Xi, Xj) : Xi ∈
PAj, Xj ∈ V } that has the random variables as nodes and
directed edges pointing from parents to children. We employ
the common assumption that this graph is acyclic, i.e., GS will
always be a directed acyclic graph (DAG).

So far a DAG GS simply depicts all parent-child relation-
ships deﬁned by the SEM S. Missing directed paths indicate
missing cause-effect relationships. In order to specify the link
between statistical independence (denoted by ⊥⊥) wrt. the joint
distribution PS and properties of the DAG GS (representing a
SEM S) we need the following deﬁnition.

Deﬁnition 3 (d-separation). For a ﬁxed graph G disjoint sets
of nodes A and B are d-separated by a third disjoint set C
(denoted by A ⊥d-sep B|C) iff all pairs of nodes a ∈ A and
b ∈ B are d-separated by C. A pair of nodes a (cid:54)= b is d-
separated by C iff every path between a and b is blocked by
C. A path between nodes a and b is blocked by C iff there is
an intermediate node z on the path such that (i) z ∈ C and z is
tail-to-tail (← z →) or head-to-tail (→ z →), or (ii) z is head-
to-head (→ z ←) and neither z nor any of its descendants is
in C.

assuming

Conveniently,
Markovianity3) we
correspondence
between
independence statements:

have

faithfulness2

(and

the

d-separation

following
and

exploiting
one-to-one
conditional

A ⊥d-sep B|C ⇐⇒ A ⊥⊥ B|C

Summing up, we have deﬁned interventional causation in
terms of SEMs and have seen how a SEM gives rise to
a DAG. This DAG has two convenient features. Firstly, the
DAG yields a visualisation that allows to easily grasp missing
cause-effect relationships that correspond to missing directed
paths. Secondly, since we assume faithfulness, d-separation
properties of this DAG are equivalent to conditional indepen-
dence properties of the joint distribution. Thus, conditional
independences translate into causal statements, e.g. ‘a variable
becomes independent of all its non-effects given its immedi-
ate causes’ or ‘cause and effect are marginally dependent’.
the causal graph GS can be identiﬁed from
Furthermore,
conditional independences observed in PS – at least up to
a so-called Markov equivalence class, the set of graphs that
entail the same conditional independences [10].

B. Formal problem description

In the following, the variables S, C1, ..., Cd, and F1, ..., Fd
may be thought of as a stimulus variable, the activity of
multiple cortical sources, and the EEG channel recordings
respectively. We aim at recovering an effect of a pre-deﬁned
target variable C1 = v(cid:62)F . The terminology introduced in
Section II-A allows to precisely state the problem as follows.

2Intuitively, this is saying that conditional independences are due to the
causal structure and not accidents of parameter values [6, p. 9]; more formally
the assumption reads A ⊥d-sep B|C ⇐= A ⊥⊥ B|C.

3The distribution PS generated by a SEM S is Markov wrt. GS (cf. [7,

Theorem 1.4.1] for a proof), i. e., A ⊥d-sep B|C =⇒ A ⊥⊥ B|C.

1) Assumptions: Let S and C1, ..., Cd denote (ﬁnitely
many) random variables. We assume existence of a SEM S,
potentially with additional unobserved variables h1, ..., hl, that
induces PS = PS,C1,...,Cd,h1,...,hl . We refer to the correspond-
ing graph GS as the true causal graph and call its nodes causal
variables. We further assume that

• S affects C2 indirectly via C1,4
• there are no edges in GS pointing into S.5

Importantly, we do not require that the structural equation that
relates C2 to its parents is linear in C1. Figure 1 depicts an
example of how GS might look like.

S

C1

C2

C5
...

Cd

C3

h1

C4

Fig. 1. Example graph where h1 is a hidden variable.

2) Given data:
• v ∈ Rd such that C1 = v(cid:62)F
• m iid6 samples S = [s1, ..., sm](cid:62) of S and F =
[fi,j]i=1:m,j=1:d of F where F (cid:44) [F1, ..., Fd](cid:62) = AC
is the observed linear mixture of the causal variables
C (cid:44) [C1, ..., Cd](cid:62) and A ∈ Rd×d the mixing matrix
3) Desired output: Find w ∈ Bd (cid:44) {x ∈ Rd : ||x|| = 1}
such that aCi = w(cid:62)F where Ci is an effect of C1 (i ∈
N2:d, a ∈ R \ {0}). For the graph shown in Figure 1 recovery
of the causal variable C2 is a valid solution.

C. Strategy

Our approach leverages the following causal inference rule
that – under the assumptions in Section II-B1 – applies to a
causal variable Ci (cf. [5]).

Causal Inference Rule: If Ci ⊥⊥ S|C1 and Y (cid:54)⊥⊥ C1, then
S indirectly affects Ci via C1. In particular, a causal path
C1 (cid:57)(cid:57)(cid:75) Ci exists.

The idea is to recover

the sought-after variable from
the mixture F by optimising for these statistical properties.
Thence, the general strategy relies on solving an optimisation
problem of the form7

max
w∈Bd−1

dep(C1, Yw) − dep(S, Yw|C1)

where Yw = w(cid:62)F and dep denotes a (conditional) de-
pendence criterion that estimates from empirical samples the
strength of association between the two variables.

4By saying a variable X causes Z indirectly via Y we imply (a) existence
of a path X (cid:57)(cid:57)(cid:75) Y (cid:57)(cid:57)(cid:75) Z, and (b) that there is no path X (cid:57)(cid:57)(cid:75) Z without Y
on it (this also excludes the edge X → Z).

5This condition can for example be ensured by randomising S.
6independent and identically distributed
7To attenuate the signal of C1 we restrict search onto the orthogonal
||x|| = 1, x ⊥ v} which is

complement v⊥ ∩ Bd = {x ∈ Rd :
diffeomorphic to Bd−1.

D. Non-Linear MERLiN algorithm

The linear MERLiN algorithm uses the partial correla-
tions ρC1,Yw|S and ρS,Yw|C1 for the terms dep(C1, Yw) and
dep(S, Yw|C1) in the objective function. As such only linear
dependence between C1 and Yw can be detected while re-
maining higher-order dependences between S and Yw given
C1 may go undetected. A general kernel-based indepen-
dence criterion, the Hilbert-Schmidt Independence Criterion
(HSIC) [11], and a regression-based conditional independence
criterion (cf. [5]) in conjunction with kernel ridge regres-
sion [12] allow extension to non-linear dependences.

Regression-Based Conditional Independence Criterion:
If there exists a (regression) function r such that Yw−r(C1) ⊥⊥
(S, C1) then S ⊥⊥ Yw|C1.

The non-linear MERLiN algorithm solves the following

optimisation problem

max
(w,σ,θ)∈Bd−1×R×R

HSIC(C1, Yw) − HSIC ((S, C1), Rw,σ,θ)

where HSIC(A, B) denotes the empirical HSIC estimate8 and
Rw,σ,θ corresponds to the residuals Yw − r(C1) using kernel
ridge regression with Gaussian kernel of width |σ| and ridge
regression parameter |θ|. To temper overﬁtting, the sample is
split into three partitions; the residuals of the ith partition are
obtained by using the kernel ridge regression function obtained
on the remaining partitions. The regression parameters σ and
θ are also being optimised over to allow an optimal regres-
sion ﬁt wrt. witnessing conditional independence and hence
minimising the second summand in the objective function.

Implementing the objective function in Theano [13], [14],
we use the Python toolbox Pymanopt [15] to run optimisation
on the product manifold Bd−1 × R × R using a steepest
descent algorithm with standard back-tracking line-search.
This approach is exact and efﬁcient, relying on automated
differentiation and respecting the manifold geometry.

E. Application to EEG data

We consider EEG trial-data of the form (cid:101)F ∈ Rd×m×n
where d denotes the number of electrodes, m the number of
trials, and n the length of the time series (cid:101)Fi,j,1:n for each
electrode i ∈ N1:d and each sample j ∈ N1:m; that is (cid:101)F holds
m iid samples of a Rd×n-valued random variable (cid:101)F . Analyses
of EEG data commonly focus on trial-averaged log-bandpower
in a particular frequency band. Accordingly, applying our al-
gorithms to EEG data we aim to identify a linear combination
w ∈ Bd such that the log-bandpower of the resulting one-
dimensional trial signals w(cid:62) (cid:101)F is a causal effect of the log-
bandpower of the one-dimensional trial signals v(cid:62) (cid:101)F .

However,

the two operations of computing the log-
bandpower and taking a linear combination do not commute.
The log-bandpower computation needs to be switched into the
objective function described above. This is accomplished by
letting Yw = logbp(w(cid:62) (cid:101)F ) and C1 = logbp(v(cid:62) (cid:101)F ) where
logbp denotes the operation of applying a Hanning window

8We compute the empirical HSIC estimate based on the Gaussian kernel
k(x, y) = exp(−δ−1||x − y||2) where the kernel size δ is determined by
the median distance between points in input space [11].

3

and computing the average log-bandpower in a speciﬁed
frequency range for the given time series.

III. COMPARATIVE EEG ANALYSIS

A. Experimental data

We applied the linear MERLiN algorithm and its non-
linear extension to EEG data recorded during a neurofeedback
experiment [16]. In this study the γ-log-bandpower (55–85 Hz)
in the right superior parietal cortex (SPC) was provided as
feedback signal and subjects were instructed to up- or down-
regulate the bandpower. 3 subjects were recorded in 2 sessions
each and each session had 60 trials á 60 seconds.

The data of one session consists of a stimulus vector S ∈
{−1, +1}60×1, a spatial ﬁlter v ∈ R121×1 that was used to
extract the feedback signal, and a tensor (cid:101)F ∈ R121×60×15000
that holds the time series (of length 15000) for each channel
and trial. The reader is referred to [16] for more details on the
experimental data.

B. How to compare to previous results

We compare our MERLiN algorithms against a causal
analysis of this neurofeedback experiment that is based on
source localisation in combination with an exhaustive search
procedure [5]. The hypothesis was, based on transcranial
magnetic stimulation studies [17], that γ-oscillations in the
SPC modulate γ-oscillations in the medial prefrontal cortex
(MPC). We brieﬂy describe this exhaustive search approach.
First, the signal of K (cid:44) 15028 dipoles across the cortical
surface was extracted using a LCMV beamformer and a three-
shell spherical head model [18]. Then, the authors applied their
newly introduced stimulus-based causal inference (SCI) algo-
rithm to assess for every dipole whether its γ-log-bandpower
is a linear causal effect of the γ-log-bandpower in the SPC.
Group results were summarised in a vector gSCI ∈ RK×1
where the ith entry denotes the percentage of dipoles within a
certain radius that were found to be modulated by the SPC.
The results of this exhaustive search analysis, visualising gSCI
on hemisphere plots, supported the hypothesis that the MPC
is a linear causal effect of the SPC. The reader is referred to
[5] for more details.

In contrast to exhaustive search, both our linear MERLiN
algorithm as well as its non-linear extension aim at imme-
diately recovering the causal effect by optimising a linear
combination w of electrode signals9. To allow for a qualitative
comparison of our results with the results summarised by
the vector gSCI we derive for each w a vector g ∈ RK×1.
This vector represents the involvement of each cortical dipole
in the recovered signal and is derived from w as follows.
First, a scalp topography is obtained via a ∝ Σw where
the ith entry of Σw is the covariance between the ith EEG
channel and the source that is recovered by w [19, Equation
(7)]. Here Σ denotes the session-speciﬁc covariance matrix
in the γ-frequency band. Second,
the dipole involvement

9Since there were only 60 samples per session we decided to select a
subset of 33 EEG channels distributed across the scalp (according to the
10–20 system). Hence, for each recording session we obtained a spatial ﬁlter
w ∈ R33×1.

4

vector g is obtained from a via dynamic statistical parametric
mapping (dSPM; with identity noise covariance matrix) [20].
Group results are obtained as average of the individual dipole
involvement vectors.

C. Experimental results

The group averaged results of our extended algorithm are
depicted in Figure 2.(a). Similar to the results in [5] and
the results we obtained with the linear MERLiN algorithm
(cf. Figure 2.(b)) the analysis indicates that the MPC is a
causal effect of the SPC. The non-linear method yields results
that are in high accordance with the ones obtained by our
linear method while exhaustive search additionally revealed
the anterior middle frontal gyrus as effect of the SPC.

Left hemisphere

Right hemisphere

(a) non-linear

Left hemisphere

Right hemisphere

w
e
i
v

l
a
r
e
t
a
L

w
e
i
v

l
a
i
d
e
M

w
e
i
v

l
a
r
e
t
a
L

w
e
i
v

l
a
i
d
e
M

(b) linear

Fig. 2. Group averaged dipole involvement corresponding to the spatial ﬁlters
identiﬁed by the (a) non-linear and (b) linear MERLiN algorithm; lateral and
medial views of the left and right hemisphere. (All colour scales from “blue”
to “red” range from 0 to the largest value to be plotted.)

IV. CONCLUSIONS
We have developed the non-linear MERLiN algorithm that
is able to recover a causal effect from an observed linear
mixture with no constraint on the functional form of this
cause-effect relationship. Iteratively projecting out directions
and applying the MERLiN algorithm may allow to identify
multiple distinct causal effects. For EEG data we found no

qualitative difference to the linear method, which indicates
that linear methods sufﬁce to identify within-frequency cause-
effect relationships in EEG data. Future research will focus on
theoretical analysis of the presented methods and assumptions
and investigate applicability to other real world data.

REFERENCES

[1] P. L. Nunez and R. Srinivasan, Electric Fields of

the Brain: The

Neurophysics of EEG. Oxford University Press, 2006.

[2] M. Grosse-Wentrup, “Understanding brain connectivity patterns during
motor imagery for brain-computer interfacing,” in Advances in Neural
Information Processing Systems, 2009, pp. 561–568.

[3] S. Weichwald, M. Grosse-Wentrup, and A. Gretton, “MERLiN: Mixture
Effect Recovery in Linear Networks,” arXiv preprint arXiv:1512.01255,
2015.

[4] S. Weichwald, T. Meyer, O. Özdenizci, B. Schölkopf, T. Ball, and
interpretation rules for encoding and
M. Grosse-Wentrup, “Causal
decoding models in neuroimaging,” NeuroImage, vol. 110, pp. 48–59,
2015.

[5] M. Grosse-Wentrup, D. Janzing, M. Siegel, and B. Schölkopf, “Identiﬁ-
cation of causal relations in neuroimaging data with latent confounders:
An instrumental variable approach,” NeuroImage, vol. 125, pp. 825–833,
2016.

[6] P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction, and

Search, 2nd ed. MIT press, 2000.

[7] J. Pearl, Causality: Models, Reasoning and Inference, 2nd ed. Cam-

bridge University Press, 2009.

[8] M. Eichler and V. Didelez, “On Granger causality and the effect of
interventions in time series,” Lifetime Data Analysis, vol. 16, no. 1, pp.
3–32, 2010.

[9] J. T. Lizier and M. Prokopenko, “Differentiating information transfer
and causal effect,” The European Physical Journal B, vol. 73, no. 4, pp.
605–615, 2010.

[10] T. Verma and J. Pearl, “Equivalence and synthesis of causal models,”
the 6th Conference on Uncertainty in Artiﬁcial

in Proceedings of
Intelligence. Elsevier, 1990, pp. 255–268.

[11] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Schölkopf, and A. J.
Smola, “A Kernel Statistical Test of Independence,” in Advances in
Neural Information Processing Systems 20, 2008, pp. 585–592.
[12] C. Saunders, A. Gammerman, and V. Vovk, “Ridge Regression Learning
Algorithm in Dual Variables,” in Proceedings of the 15th International
Conference on Machine Learning, 1998, pp. 515–521.

[13] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Des-
jardins, J. Turian, D. Warde-Farley, and Y. Bengio, “Theano: A CPU
and GPU math expression compiler,” in Proceedings of the Python for
Scientiﬁc Computing Conference (SciPy), Jun. 2010.

[14] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow,
A. Bergeron, N. Bouchard, and Y. Bengio, “Theano: New features
and speed improvements,” Deep Learning and Unsupervised Feature
Learning NIPS 2012 Workshop, 2012.

[15] J. Townsend, N. Koep, and S. Weichwald, “Pymanopt: A Python
Toolbox for Manifold Optimization using Automatic Differentiation,”
arXiv preprint arXiv:1603.03236, 2016.

[16] M. Grosse-Wentrup and B. Schölkopf, “A brain–computer interface
based on self-regulation of gamma-oscillations in the superior parietal
cortex,” Journal of Neural Engineering, vol. 11, no. 5, p. 056015, 2014.
[17] A. C. Chen, D. J. Oathes, C. Chang, T. Bradley, Z.-W. Zhou, L. M.
Williams, G. H. Glover, K. Deisseroth, and A. Etkin, “Causal
in-
teractions between fronto-parietal central executive and default-mode
networks in humans,” Proceedings of the National Academy of Sciences,
vol. 110, no. 49, pp. 19 944–19 949, 2013.

[18] J. C. Mosher, R. M. Leahy, and P. S. Lewis, “EEG and MEG: Forward
solutions for inverse methods,” Biomedical Engineering, IEEE Transac-
tions on, vol. 46, no. 3, pp. 245–259, 1999.

[19] S. Haufe, F. Meinecke, K. Görgen, S. Dähne, J.-D. Haynes, B. Blankertz,
and F. Bießmann, “On the interpretation of weight vectors of linear
models in multivariate neuroimaging,” NeuroImage, vol. 87, pp. 96–110,
2014.

[20] A. M. Dale, A. K. Liu, B. R. Fischl, R. L. Buckner, J. W. Belliveau,
J. D. Lewine, and E. Halgren, “Dynamic statistical parametric mapping:
Combining fMRI and MEG for high-resolution imaging of cortical
activity,” Neuron, vol. 26, no. 1, pp. 55–67, 2000.

