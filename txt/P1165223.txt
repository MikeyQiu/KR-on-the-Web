Xception: Deep Learning with Depthwise Separable Convolutions

Franc¸ois Chollet
Google, Inc.
fchollet@google.com

Abstract

We present an interpretation of Inception modules in con-
volutional neural networks as being an intermediate step
in-between regular convolution and the depthwise separable
convolution operation (a depthwise convolution followed by
a pointwise convolution). In this light, a depthwise separable
convolution can be understood as an Inception module with
a maximally large number of towers. This observation leads
us to propose a novel deep convolutional neural network
architecture inspired by Inception, where Inception modules
have been replaced with depthwise separable convolutions.
We show that this architecture, dubbed Xception, slightly
outperforms Inception V3 on the ImageNet dataset (which
Inception V3 was designed for), and signiﬁcantly outper-
forms Inception V3 on a larger image classiﬁcation dataset
comprising 350 million images and 17,000 classes. Since
the Xception architecture has the same number of param-
eters as Inception V3, the performance gains are not due
to increased capacity but rather to a more efﬁcient use of
model parameters.

1. Introduction

Convolutional neural networks have emerged as the mas-
ter algorithm in computer vision in recent years, and de-
veloping recipes for designing them has been a subject of
considerable attention. The history of convolutional neural
network design started with LeNet-style models [10], which
were simple stacks of convolutions for feature extraction
and max-pooling operations for spatial sub-sampling. In
2012, these ideas were reﬁned into the AlexNet architec-
ture [9], where convolution operations were being repeated
multiple times in-between max-pooling operations, allowing
the network to learn richer features at every spatial scale.
What followed was a trend to make this style of network
increasingly deeper, mostly driven by the yearly ILSVRC
competition; ﬁrst with Zeiler and Fergus in 2013 [25] and
then with the VGG architecture in 2014 [18].

At this point a new style of network emerged, the Incep-
tion architecture, introduced by Szegedy et al. in 2014 [20]

as GoogLeNet (Inception V1), later reﬁned as Inception V2
[7], Inception V3 [21], and most recently Inception-ResNet
[19]. Inception itself was inspired by the earlier Network-
In-Network architecture [11]. Since its ﬁrst introduction,
Inception has been one of the best performing family of
models on the ImageNet dataset [14], as well as internal
datasets in use at Google, in particular JFT [5].

The fundamental building block of Inception-style mod-
els is the Inception module, of which several different ver-
sions exist. In ﬁgure 1 we show the canonical form of an
Inception module, as found in the Inception V3 architec-
ture. An Inception model can be understood as a stack of
such modules. This is a departure from earlier VGG-style
networks which were stacks of simple convolution layers.

While Inception modules are conceptually similar to con-
volutions (they are convolutional feature extractors), they
empirically appear to be capable of learning richer repre-
sentations with less parameters. How do they work, and
how do they differ from regular convolutions? What design
strategies come after Inception?

1.1. The Inception hypothesis

A convolution layer attempts to learn ﬁlters in a 3D space,
with 2 spatial dimensions (width and height) and a chan-
nel dimension; thus a single convolution kernel is tasked
with simultaneously mapping cross-channel correlations and
spatial correlations.

This idea behind the Inception module is to make this
process easier and more efﬁcient by explicitly factoring it
into a series of operations that would independently look at
cross-channel correlations and at spatial correlations. More
precisely, the typical Inception module ﬁrst looks at cross-
channel correlations via a set of 1x1 convolutions, mapping
the input data into 3 or 4 separate spaces that are smaller than
the original input space, and then maps all correlations in
these smaller 3D spaces, via regular 3x3 or 5x5 convolutions.
This is illustrated in ﬁgure 1. In effect, the fundamental hy-
pothesis behind Inception is that cross-channel correlations
and spatial correlations are sufﬁciently decoupled that it is
preferable not to map them jointly 1.

1A variant of the process is to independently look at width-wise corre-

1251

Consider a simpliﬁed version of an Inception module that
only uses one size of convolution (e.g. 3x3) and does not
include an average pooling tower (ﬁgure 2). This Incep-
tion module can be reformulated as a large 1x1 convolution
followed by spatial convolutions that would operate on non-
overlapping segments of the output channels (ﬁgure 3). This
observation naturally raises the question: what is the ef-
fect of the number of segments in the partition (and their
size)? Would it be reasonable to make a much stronger
hypothesis than the Inception hypothesis, and assume that
cross-channel correlations and spatial correlations can be
mapped completely separately?

Figure 1. A canonical Inception module (Inception V3).

Figure 3. A strictly equivalent reformulation of the simpliﬁed In-
ception module.

Figure 4. An “extreme” version of our Inception module, with one
spatial convolution per output channel of the 1x1 convolution.

Figure 2. A simpliﬁed Inception module.

1.2. The continuum between convolutions and sep 

arable convolutions

An “extreme” version of an Inception module, based on
this stronger hypothesis, would ﬁrst use a 1x1 convolution to
map cross-channel correlations, and would then separately
map the spatial correlations of every output channel. This
is shown in ﬁgure 4. We remark that this extreme form of
an Inception module is almost identical to a depthwise sepa-
rable convolution, an operation that has been used in neural

lations and height-wise correlations. This is implemented by some of the
modules found in Inception V3, which alternate 7x1 and 1x7 convolutions.
The use of such spatially separable convolutions has a long history in im-
age processing and has been used in some convolutional neural network
implementations since at least 2012 (possibly earlier).

network design as early as 2014 [15] and has become more
popular since its inclusion in the TensorFlow framework [1]
in 2016.

A depthwise separable convolution, commonly called
“separable convolution” in deep learning frameworks such as
TensorFlow and Keras, consists in a depthwise convolution,
i.e. a spatial convolution performed independently over each
channel of an input, followed by a pointwise convolution,
i.e. a 1x1 convolution, projecting the channels output by the
depthwise convolution onto a new channel space. This is
not to be confused with a spatially separable convolution,
which is also commonly called “separable convolution” in
the image processing community.

Two minor differences between and “extreme” version of
an Inception module and a depthwise separable convolution
would be:

• The order of the operations: depthwise separable con-
volutions as usually implemented (e.g. in TensorFlow)
perform ﬁrst channel-wise spatial convolution and then
perform 1x1 convolution, whereas Inception performs
the 1x1 convolution ﬁrst.

• The presence or absence of a non-linearity after the
ﬁrst operation. In Inception, both operations are fol-
lowed by a ReLU non-linearity, however depthwise

1252

separable convolutions are usually implemented with-
out non-linearities.

We argue that the ﬁrst difference is unimportant, in par-
ticular because these operations are meant to be used in a
stacked setting. The second difference might matter, and we
investigate it in the experimental section (in particular see
ﬁgure 10).

We also note that other intermediate formulations of In-
ception modules that lie in between regular Inception mod-
ules and depthwise separable convolutions are also possible:
in effect, there is a discrete spectrum between regular convo-
lutions and depthwise separable convolutions, parametrized
by the number of independent channel-space segments used
for performing spatial convolutions. A regular convolution
(preceded by a 1x1 convolution), at one extreme of this
spectrum, corresponds to the single-segment case; a depth-
wise separable convolution corresponds to the other extreme
where there is one segment per channel; Inception modules
lie in between, dividing a few hundreds of channels into 3
or 4 segments. The properties of such intermediate modules
appear not to have been explored yet.

Having made these observations, we suggest that it may
be possible to improve upon the Inception family of archi-
tectures by replacing Inception modules with depthwise sep-
arable convolutions, i.e. by building models that would be
stacks of depthwise separable convolutions. This is made
practical by the efﬁcient depthwise convolution implementa-
tion available in TensorFlow. In what follows, we present a
convolutional neural network architecture based on this idea,
with a similar number of parameters as Inception V3, and
we evaluate its performance against Inception V3 on two
large-scale image classiﬁcation task.

2. Prior work

following areas:

The present work relies heavily on prior efforts in the

• Convolutional neural networks [10, 9, 25], in particular
the VGG-16 architecture [18], which is schematically
similar to our proposed architecture in a few respects.

• The Inception architecture family of convolutional neu-
ral networks [20, 7, 21, 19], which ﬁrst demonstrated
the advantages of factoring convolutions into multiple
branches operating successively on channels and then
on space.

• Depthwise separable convolutions, which our proposed
architecture is entirely based upon. While the use of spa-
tially separable convolutions in neural networks has a
long history, going back to at least 2012 [12] (but likely
even earlier), the depthwise version is more recent. Lau-
rent Sifre developed depthwise separable convolutions

during an internship at Google Brain in 2013, and used
them in AlexNet to obtain small gains in accuracy and
large gains in convergence speed, as well as a signiﬁcant
reduction in model size. An overview of his work was
ﬁrst made public in a presentation at ICLR 2014 [23].
Detailed experimental results are reported in Sifre’s the-
sis, section 6.2 [15]. This initial work on depthwise sep-
arable convolutions was inspired by prior research from
Sifre and Mallat on transformation-invariant scattering
[16, 15]. Later, a depthwise separable convolution was
used as the ﬁrst layer of Inception V1 and Inception
V2 [20, 7]. Within Google, Andrew Howard [6] has
introduced efﬁcient mobile models called MobileNets
using depthwise separable convolutions. Jin et al. in
2014 [8] and Wang et al. in 2016 [24] also did related
work aiming at reducing the size and computational
cost of convolutional neural networks using separable
convolutions. Additionally, our work is only possible
due to the inclusion of an efﬁcient implementation of
depthwise separable convolutions in the TensorFlow
framework [1].

• Residual connections, introduced by He et al. in [4],
which our proposed architecture uses extensively.

3. The Xception architecture

We propose a convolutional neural network architecture
based entirely on depthwise separable convolution layers.
In effect, we make the following hypothesis: that the map-
ping of cross-channels correlations and spatial correlations
in the feature maps of convolutional neural networks can be
entirely decoupled. Because this hypothesis is a stronger ver-
sion of the hypothesis underlying the Inception architecture,
we name our proposed architecture Xception, which stands
for “Extreme Inception”.

A complete description of the speciﬁcations of the net-
work is given in ﬁgure 5. The Xception architecture has
36 convolutional layers forming the feature extraction base
of the network. In our experimental evaluation we will ex-
clusively investigate image classiﬁcation and therefore our
convolutional base will be followed by a logistic regression
layer. Optionally one may insert fully-connected layers be-
fore the logistic regression layer, which is explored in the
experimental evaluation section (in particular, see ﬁgures
7 and 8). The 36 convolutional layers are structured into
14 modules, all of which have linear residual connections
around them, except for the ﬁrst and last modules.

In short, the Xception architecture is a linear stack of
depthwise separable convolution layers with residual con-
nections. This makes the architecture very easy to deﬁne
and modify; it takes only 30 to 40 lines of code using a high-
level library such as Keras [2] or TensorFlow-Slim [17], not
unlike an architecture such as VGG-16 [18], but rather un-

1253

like architectures such as Inception V2 or V3 which are far
more complex to deﬁne. An open-source implementation of
Xception using Keras and TensorFlow is provided as part of
the Keras Applications module2, under the MIT license.

4. Experimental evaluation

We choose to compare Xception to the Inception V3 ar-
chitecture, due to their similarity of scale: Xception and
Inception V3 have nearly the same number of parameters
(table 3), and thus any performance gap could not be at-
tributed to a difference in network capacity. We conduct
our comparison on two image classiﬁcation tasks: one is
the well-known 1000-class single-label classiﬁcation task on
the ImageNet dataset [14], and the other is a 17,000-class
multi-label classiﬁcation task on the large-scale JFT dataset.

– Learning rate decay: decay of rate 0.9 every

3,000,000 samples

For both datasets, the same exact same optimization con-
ﬁguration was used for both Xception and Inception V3.
Note that this conﬁguration was tuned for best performance
with Inception V3; we did not attempt to tune optimization
hyperparameters for Xception. Since the networks have dif-
ferent training proﬁles (ﬁgure 6), this may be suboptimal, es-
pecially on the ImageNet dataset, on which the optimization
conﬁguration used had been carefully tuned for Inception
V3.

Additionally, all models were evaluated using Polyak

averaging [13] at inference time.

4.1. The JFT dataset

4.3. Regularization conﬁguration

JFT is an internal Google dataset for large-scale image
classiﬁcation dataset, ﬁrst introduced by Hinton et al. in [5],
which comprises over 350 million high-resolution images
annotated with labels from a set of 17,000 classes. To eval-
uate the performance of a model trained on JFT, we use an
auxiliary dataset, FastEval14k.

FastEval14k is a dataset of 14,000 images with dense
annotations from about 6,000 classes (36.5 labels per im-
age on average). On this dataset we evaluate performance
using Mean Average Precision for top 100 predictions
(MAP@100), and we weight the contribution of each class
to MAP@100 with a score estimating how common (and
therefore important) the class is among social media images.
This evaluation procedure is meant to capture performance
on frequently occurring labels from social media, which is
crucial for production models at Google.

4.2. Optimization conﬁguration

A different optimization conﬁguration was used for Ima-

– Learning rate decay: decay of rate 0.94 every 2

geNet and JFT:

• On ImageNet:

– Optimizer: SGD

– Momentum: 0.9

– Initial learning rate: 0.045

epochs

• On JFT:

– Optimizer: RMSprop [22]

– Momentum: 0.9

– Initial learning rate: 0.001

2https://keras.io/applications/#xception

• Weight decay: The Inception V3 model uses a weight
decay (L2 regularization) rate of 4e − 5, which has
been carefully tuned for performance on ImageNet. We
found this rate to be quite suboptimal for Xception
and instead settled for 1e − 5. We did not perform
an extensive search for the optimal weight decay rate.
The same weight decay rates were used both for the
ImageNet experiments and the JFT experiments.

• Dropout: For the ImageNet experiments, both models
include a dropout layer of rate 0.5 before the logistic
regression layer. For the JFT experiments, no dropout
was included due to the large size of the dataset which
made overﬁtting unlikely in any reasonable amount of
time.

• Auxiliary loss tower: The Inception V3 architecture
may optionally include an auxiliary tower which back-
propagates the classiﬁcation loss earlier in the network,
serving as an additional regularization mechanism. For
simplicity, we choose not to include this auxiliary tower
in any of our models.

4.4. Training infrastructure

All networks were implemented using the TensorFlow
framework [1] and trained on 60 NVIDIA K80 GPUs each.
For the ImageNet experiments, we used data parallelism
with synchronous gradient descent to achieve the best classi-
ﬁcation performance, while for JFT we used asynchronous
gradient descent so as to speed up training. The ImageNet
experiments took approximately 3 days each, while the JFT
experiments took over one month each. The JFT models
were not trained to full convergence, which would have
taken over three month per experiment.

1254

Figure 5. The Xception architecture: the data ﬁrst goes through the entry ﬂow, then through the middle ﬂow which is repeated eight times,
and ﬁnally through the exit ﬂow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not
included in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).

4.5. Comparison with Inception V3

152 [4].

4.5.1 Classiﬁcation performance

All evaluations were run with a single crop of the inputs
images and a single model. ImageNet results are reported
on the validation set rather than the test set (i.e. on the
non-blacklisted images from the validation set of ILSVRC
2012). JFT results are reported after 30 million iterations
(one month of training) rather than after full convergence.
Results are provided in table 1 and table 2, as well as ﬁgure
6, ﬁgure 7, ﬁgure 8. On JFT, we tested both versions of our
networks that did not include any fully-connected layers, and
versions that included two fully-connected layers of 4096
units each before the logistic regression layer.

On ImageNet, Xception shows marginally better results
than Inception V3. On JFT, Xception shows a 4.3% rel-
ative improvement on the FastEval14k MAP@100 metric.
We also note that Xception outperforms ImageNet results
reported by He et al. for ResNet-50, ResNet-101 and ResNet-

Table 1. Classiﬁcation performance comparison on ImageNet (sin-
gle crop, single model). VGG-16 and ResNet-152 numbers are
only included as a reminder. The version of Inception V3 being
benchmarked does not include the auxiliary tower.

Top-1 accuracy Top-5 accuracy

VGG-16
ResNet-152
Inception V3
Xception

0.715
0.770
0.782
0.790

0.901
0.933
0.941
0.945

The Xception architecture shows a much larger perfor-
mance improvement on the JFT dataset compared to the
ImageNet dataset. We believe this may be due to the fact
that Inception V3 was developed with a focus on ImageNet
and may thus be by design over-ﬁt to this speciﬁc task. On
the other hand, neither architecture was tuned for JFT. It is
likely that a search for better hyperparameters for Xception
on ImageNet (in particular optimization parameters and reg-

1255

Figure 8. Training proﬁle on JFT, with fully-connected layers

Table 2. Classiﬁcation performance comparison on JFT (single
crop, single model).

Inception V3 - no FC layers
Xception - no FC layers
Inception V3 with FC layers
Xception with FC layers

FastEval14k MAP@100
6.36
6.70
6.50
6.78

Figure 6. Training proﬁle on ImageNet

Figure 7. Training proﬁle on JFT, without fully-connected layers

V3 and Xception. Parameter count is reported on ImageNet
(1000 classes, no fully-connected layers) and the number of
training steps (gradient updates) per second is reported on
ImageNet with 60 K80 GPUs running synchronous gradient
descent. Both architectures have approximately the same
size (within 3.5%), and Xception is marginally slower. We
expect that engineering optimizations at the level of the
depthwise convolution operations can make Xception faster
than Inception V3 in the near future. The fact that both
architectures have almost the same number of parameters
indicates that the improvement seen on ImageNet and JFT
does not come from added capacity but rather from a more
efﬁcient use of the model parameters.

4.6. Effect of the residual connections

Figure 9. Training proﬁle with and without residual connections.

ularization parameters) would yield signiﬁcant additional
improvement.

4.5.2 Size and speed

Table 3. Size and training speed comparison.

Inception V3
Xception

Parameter count
23,626,728
22,855,952

Steps/second
31
28

In table 3 we compare the size and speed of Inception

To quantify the beneﬁts of residual connections in the
Xception architecture, we benchmarked on ImageNet a mod-
iﬁed version of Xception that does not include any residual

1256

connections. Results are shown in ﬁgure 9. Residual con-
nections are clearly essential in helping with convergence,
both in terms of speed and ﬁnal classiﬁcation performance.
However we will note that benchmarking the non-residual
model with the same optimization conﬁguration as the resid-
ual model may be uncharitable and that better optimization
conﬁgurations might yield more competitive results.

Additionally, let us note that this result merely shows the
importance of residual connections for this speciﬁc architec-
ture, and that residual connections are in no way required
in order to build models that are stacks of depthwise sepa-
rable convolutions. We also obtained excellent results with
non-residual VGG-style models where all convolution layers
were replaced with depthwise separable convolutions (with
a depth multiplier of 1), superior to Inception V3 on JFT at
equal parameter count.

4.7. Effect of an intermediate activation after point 

wise convolutions

found in Inception modules) the non-linearity is helpful, but
for shallow ones (e.g.
the 1-channel deep feature spaces
of depthwise separable convolutions) it becomes harmful,
possibly due to a loss of information.

5. Future directions

We noted earlier the existence of a discrete spectrum be-
tween regular convolutions and depthwise separable convo-
lutions, parametrized by the number of independent channel-
space segments used for performing spatial convolutions. In-
ception modules are one point on this spectrum. We showed
in our empirical evaluation that the extreme formulation of
an Inception module, the depthwise separable convolution,
may have advantages over regular a regular Inception mod-
ule. However, there is no reason to believe that depthwise
separable convolutions are optimal. It may be that intermedi-
ate points on the spectrum, lying between regular Inception
modules and depthwise separable convolutions, hold further
advantages. This question is left for future investigation.

Figure 10. Training proﬁle with different activations between the
depthwise and pointwise operations of the separable convolution
layers.

6. Conclusions

We showed how convolutions and depthwise separable
convolutions lie at both extremes of a discrete spectrum,
with Inception modules being an intermediate point in be-
tween. This observation has led to us to propose replacing
Inception modules with depthwise separable convolutions in
neural computer vision architectures. We presented a novel
architecture based on this idea, named Xception, which has
a similar parameter count as Inception V3. Compared to
Inception V3, Xception shows small gains in classiﬁcation
performance on the ImageNet dataset and large gains on the
JFT dataset. We expect depthwise separable convolutions
to become a cornerstone of convolutional neural network
architecture design in the future, since they offer similar
properties as Inception modules, yet are as easy to use as
regular convolution layers.

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Van-
houcke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden,
M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org.
[2] F. Chollet. Keras. https://github.com/fchollet/keras, 2015.
[3] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and
accurate deep network learning by exponential linear units
(elus). arXiv preprint arXiv:1511.07289, 2015.

1257

We mentioned earlier that the analogy between depth-
wise separable convolutions and Inception modules suggests
that depthwise separable convolutions should potentially in-
clude a non-linearity between the depthwise and pointwise
operations. In the experiments reported so far, no such non-
linearity was included. However we also experimentally
tested the inclusion of either ReLU or ELU [3] as intermedi-
ate non-linearity. Results are reported on ImageNet in ﬁgure
10, and show that the absence of any non-linearity leads to
both faster convergence and better ﬁnal performance.

This is a remarkable observation, since Szegedy et al. re-
port the opposite result in [21] for Inception modules. It may
be that the depth of the intermediate feature spaces on which
spatial convolutions are applied is critical to the usefulness
those
of the non-linearity: for deep feature spaces (e.g.

[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. arXiv preprint arXiv:1512.03385,
2015.

Networks for Machine Learning, 4, 2012. Accessed: 2015-
11-05.

[23] V. Vanhoucke. Learning visual representations at scale. ICLR,

[5] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge

2014.

in a neural network, 2015.

[24] M. Wang, B. Liu, and H. Foroosh. Factorized convolutional

[6] A. Howard. Mobilenets: Efﬁcient convolutional neural net-

neural networks. arXiv preprint arXiv:1608.04337, 2016.

[25] M. D. Zeiler and R. Fergus. Visualizing and understanding
convolutional networks. In Computer Vision–ECCV 2014,
pages 818–833. Springer, 2014.

works for mobile vision applications. Forthcoming.

[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
In Proceedings of The 32nd International Conference on
Machine Learning, pages 448–456, 2015.

[8] J. Jin, A. Dundar, and E. Culurciello. Flattened convolutional
neural networks for feedforward acceleration. arXiv preprint
arXiv:1412.5474, 2014.

[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. In
Advances in neural information processing systems, pages
1097–1105, 2012.

[10] Y. LeCun, L. Jackel, L. Bottou, C. Cortes, J. S. Denker,
H. Drucker, I. Guyon, U. Muller, E. Sackinger, P. Simard,
et al. Learning algorithms for classiﬁcation: A comparison on
handwritten digit recognition. Neural networks: the statistical
mechanics perspective, 261:276, 1995.

[11] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv

preprint arXiv:1312.4400, 2013.

[12] F. Mamalet and C. Garcia. Simplifying ConvNets for Fast
Learning. In International Conference on Artiﬁcial Neural
Networks (ICANN 2012), pages 58–65. Springer, 2012.
[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochas-
tic approximation by averaging. SIAM J. Control Optim.,
30(4):838–855, July 1992.

[14] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Ima-
genet large scale visual recognition challenge. 2014.
[15] L. Sifre. Rigid-motion scattering for image classiﬁcation,

2014. Ph.D. thesis.

[16] L. Sifre and S. Mallat. Rotation, scaling and deformation
invariant scattering for texture discrimination. In 2013 IEEE
Conference on Computer Vision and Pattern Recognition,
Portland, OR, USA, June 23-28, 2013, pages 1233–1240,
2013.

[17] N. Silberman and S. Guadarrama. Tf-slim, 2016.
[18] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[19] C. Szegedy, S. Ioffe, and V. Vanhoucke.

Inception-v4,
inception-resnet and the impact of residual connections on
learning. arXiv preprint arXiv:1602.07261, 2016.

[20] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper
with convolutions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 1–9, 2015.
[21] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision.
arXiv preprint arXiv:1512.00567, 2015.

[22] T. Tieleman and G. Hinton. Divide the gradient by a run-
ning average of its recent magnitude. COURSERA: Neural

1258

