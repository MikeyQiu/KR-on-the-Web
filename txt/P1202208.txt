Convolutional Interaction Network for Natural Language Inference

Jingjing Gong‡, Xipeng Qiu∗‡, Xinchi Chen‡, Dong Liang§ †, Xuanjing Huang‡
‡ Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
‡ School of Computer Science, Fudan University
§ State Key Laboratory of Information Security, Institute of Information Engineering CAS
{jjgong15, xpqiu, xinchichen13, xjhuang}@fudan.edu.cn,liangdong@iie.ac.cn

Abstract

Attention-based neural models have achieved
great success in natural language inference
(NLI). In this paper, we propose the Con-
volutional Interaction Network (CIN), a gen-
eral model to capture the interaction between
two sentences, which can be an alternative
to the attention mechanism for NLI. Specif-
ically, CIN encodes one sentence with the
ﬁlters dynamically generated based on an-
other sentence. Since the ﬁlters may be de-
signed to have various numbers and sizes, CIN
can capture more complicated interaction pat-
terns. Experiments on three very large datasets
demonstrate CIN’s efﬁcacy.

1

Introduction

Natural language inference (NLI) is a pivotal and
challenging natural language processing (NLP)
task. The goal of NLI is to identify the logical
relationship (entailment, neutral, or contradiction)
between a premise and a corresponding hypothe-
sis. Generally, NLI is also related to many other
NLP tasks under the paradigm of semantic match-
ing of two texts, such as question answering Hu
et al. (2014); Wan et al. (2016) and information
retrieval Liu et al. (2015), and so on. An essential
challenge is to capture the semantic relevance of
two sentences. Due to the semantic gap (or lexical
chasm) problem, natural language inference is still
a challenging problem.

Recently, deep learning is raising a substan-
tial interest in natural language inference and has
achieved some great progresses Hu et al. (2014);
Parikh et al. (2016); Chen et al. (2017a). To
model the complicated semantic relationship be-
tween two sentences, previous models heavily uti-
lize various attention mechanism Bahdanau et al.

∗ Corresponding Author.
† Contribution during internship at Fudan University.

(2014); Vaswani et al. (2017) to build the interac-
tion at different granularity (word, phrase and sen-
tence level), such as ABCNN Yin et al. (2016),
Attention LSTM Rockt¨aschel et al. (2015), bi-
directional attention LSTM Chen et al. (2017a),
and so on. While attention is very successful in
natural language inference, its mechanism is quite
simple and can be regarded as a weighted sum
of the target vectors. This paradigm results in a
lack of ﬂexibility in more complicated interaction
model.

In this paper, we propose a new interaction
module, called Convolutional Interaction Network
(CIN), which can serve as an alternative module
of attention mechanism. Speciﬁcally, CIN utilizes
convolutional neural network to extract the valued
features (or representations) from sentences.
In
the case of NLI, whether a feature of one sentence
being important depends on another sentence. In-
spired by the idea of using one network to gen-
erate the parameters of another network Ha et al.
(2016a); De Brabandere et al. (2016), we intro-
duce a ﬁlter generation network to dynamically
generate convolutional ﬁlters. Each sentence is
convolved by a dynamically generated ﬁlter by
another sentence. Thus, the convolved features
of one sentence can be regarded as context-aware
representations under the inﬂuence of another sen-
tence.

The contributions of this paper can be summa-

rized as follows.

1. CIN is a new interaction model, invented as
an alternative module to the attention model.
CIN can also capture both the intra- or inter-
interactions of two sentences.

2. Compared to attention model, CIN is more
general and ﬂexible to capture the compli-
cated interaction. As discussed in Section

the attention model is approximately

3.3,
equivalent to a special case of CIN.

3. We perform extensive empirical studies on
three very large datasets. Experiment results
demonstrate that our proposed architecture is
effective for natural language inference.

While these two kinds of attentions have similar
performance, the multiplicative attention is more
popular in practice since it requires less computa-
tion power and have less memory demand with op-
timized matrix multiplication. With multiplicative
attention, we can compute the mimic representa-
tions for both X and Y .

2 Attentive Interaction for Natural

Language Inference

Currently, the dominative method for natural lan-
guage inference is to use attention mechanism to
model the interaction between two sentence.
input

=
[x1, x2, · · · , xm] and y = [y1, y2, · · · , yn]
with length m and n respectively, we ﬁrst encode
them into two vectorial sequences

sentences

Given

two

x

X = [x1, x2, · · · , xm] ∈ Rd×m,
Y = [y1, y2, · · · , yn] ∈ Rd×n.

(1)

(2)

The encoder usually consists of one or several
CNN/RNN layers to get the context-aware token
representations.

To capture the interaction between two sen-
tence, various neural attentions can be used, such
as sentence2word attention Rockt¨aschel et al.
(2015), word2word attention Parikh et al. (2016);
Chen et al. (2017a).

Attentive

Word2word
Interaction The
word2word attention captures the dependency
between two words xi and yj from the concerned
The word2word
two sentences respectively.
attention computes a similarity matrix M ,
in
which each element mi,j is the alignment score
between xi and yj .

mi,j = f (xi, yj), 1 ≤ i ≤ m, 1 ≤ j ≤ n,

(3)

where f is a score function.

There are two most prevalent attention func-
tions: multiplicative attention and additive atten-
tion. Multiplicative attention is:

f (xi, yj) = xT

i yj.

(4)

Additive attention computes a compatibility func-
tion by a feed-forward network with a single hid-
den layer.

f (xi, yj) = w(cid:62)σ(W1xi + W2yj + b),

(5)

where w, W1, W2 and b are learnable parame-
ters.

¯X = Y softmax(Y TX) ∈ Rd×m,
¯Y = Xsoftmax(X TY ) ∈ Rd×n,

(6)

(7)

where softmax(·) is column-wise normalization
function. Each vector ¯xi ∈ ¯X is called as
mimic vector, which is a weighted summation of
{yj}n
j=1. Intuitively, the mimic vector ¯xi provides
the related information of token xi extracted from
sentence Y .

Prediction After interaction, a prediction mod-
ule is used to aggregate the interaction informa-
tion and extract the ﬁx-length representation of
two sentences. Finally, the ﬁnal sentence repre-
sentations are fed into a feed-forward network to
predict the relationship between two sentences.

3 Convolutional Interaction Network

In this section, we propose a new interaction
method by utilizing dynamic convolutional ﬁlters,
called Convolutional Interaction Network (CIN).
CIN can serve as an alternative module of atten-
tion mechanism.

We ﬁrst brieﬂy introduce how the convolution
works over text sequence, then describe the pro-
posed model and its connection to attention model.

3.1 Convolution over Sequence

Convolution is an effective operation in deep neu-
ral networks, which convolves the input with a set
of ﬁlters to extract non-linear compositional fea-
tures. Although originally designed for computer
vision, convolutional models have subsequently
shown to be effective for NLP and have achieved
excellent performance in sentence modeling Kim
(2014); Kalchbrenner et al. (2014), and other tra-
ditional NLP tasks Hu et al. (2014); Zeng et al.
(2014); Gehring et al. (2017).

sentence

Given a

representation X =
[x1, x2, · · · , xm] ∈ Rd×m, a convolutional
ﬁlter W (f ) ∈ Rd×kd, the convolution process is
deﬁned as

x(cid:48)

i = f

W (f )[xi−[k/2], · · · , xi+[k/2]] + b(f )(cid:17)
(cid:16)

,

(8)

where τ is the width of ﬁlter, FGN(·) is the ﬁlter
generation network. A detailed implementation of
FGN is presented in Section 3.4.

Now we can convolve the two sentences with

the generated ﬁlters.

¯X = f (W (f )
¯Y = f (W (f )

y ⊗ X) ∈ Rd×m,
x ⊗ Y ) ∈ Rd×n,

(12)

(13)

where the attained matrix ¯X and ¯Y can be re-
garded as the context-aware representation of sen-
tences x and y, depending on each other.
Figure 1 gives an illustration of CIN.

3.3 Connection to Attentive Interaction

CIN is more general than attention model. Assum-
ing that we set k = 1 and FGN to be a function of
FGN(X) = XX T, Eq. (12) and (13) of CIN can
be written as

¯X = (Y Y T)X = Y (Y TX),
¯Y = (XX T)Y = X(X TY ).

(14)

(15)

Compared to Eq. (6) and (7), under the above
assumption, CIN is equivalent to the word2word
multiplicative attention model without softmax
normalization.

3.4 An Implementation of Filter Generation

Network (FGN)

To generate the dynamic ﬁlters, the key factor
is how to choose the ﬁlter generation network
FGN(·) in Eq. (10) and (11). Although many so-
phisticated networks can be employed, we give an
simple implementation in this paper.

For ease of presentation, we only describe how
we generate dynamical ﬁlter according to sentence
x. The same procedure is utilized for sentence y.
Firstly, we summarize the information of sen-

tence x with an over-time k-max pooling on X,

Ux = ReLU(Wu ⊗ X) ∈ Rd×m,
x = k-max(Ux) ∈ Rd×k,
z1:k

(16)

(17)

where Ux is a non-linear transformation of X by
convolution ﬁlter Wu ∈ Rd×d. The idea of k-max
pooling is to capture the most important features
(the k highest values) from sentence X.

Then we generate k ﬁlters W j

x for j = 1, · · · , k

Figure 1: Convolutional Interaction Network. ⊗
denotes the convolution operation.

where f (·) is a non-linear activation function, such
as ReLU, k indicates the size of convolution win-
dow, and b(f ) ∈ Rd is a bias vector.

The convolution can be abbreviated as

X (cid:48) = f (W (f ) ⊗ X) ∈ Rd×m,

(9)

where ⊗ denotes the convolutional operation. To
ensure the output of convolution has equal length
as to the input, we pad [ k
2 ] zero vectors on both
sides of the input.

3.2 Convolutional Interaction Network

Convolution is very effective when it comes to
extracting useful features from a sentence. But
for NLI, whether a word (or feature) being im-
portant in one sentence depends on another sen-
tence. Therefore, a better convolution operation
should have the ability to extract substantial fea-
tures from one sentence according to another sen-
tence. Thus, the convolutional ﬁlter should be
dynamically changeable.
Inspired by Jia et al.
(2016); Ha et al. (2016b), we propose a ﬁlter gen-
eration network (FGN) to generate a dynamical
ﬁlter, which is used to extract the context-aware
information.

Given two sentences x, y, and their representa-
tions X = [x1, x2, · · · , xm] ∈ Rd×m and Y =
[y1, y2, · · · , yn] ∈ Rd×n, the ﬁlter for each sen-
tence is generated according to the other sentence
by

W (f )
W (f )

x = FGN(X) ∈ Rd×τ d,
y = FGN(Y ) ∈ Rd×τ d,

by

(10)

(11)

W j

x = ReLU (cid:0)P diag(zj

x)Q + B(cid:1) ,

(18)

where P ∈ R d
are learnable parameters.

k ×d, Q ∈ Rd×τ d and B ∈ R d

k ×τ d

The ﬁnal ﬁlter is obtained by concatenating the

k generated ﬁlters,

W (f )

x = [W 1

x ; W 2

x ; · · · ; W k

x ] ∈ Rd×τ d.

(19)

Similar to x, we can also obtain the dynamic
according to the sentence y.

ﬁlters W (f )

y

4

Incorporating CIN into a Deep
Network Architecture for NLI

Our overall network architecture for NLI is based
on a successful model proposed by Chen et al.
(2017a). The major difference is that we use CIN
to capture the interaction, instead of bi-directional
attention.

The network architecture consists of three com-
ponents: (1) an encoding layer; (2) convolutional
interaction layers; (3) a prediction layer. Figure 2
gives an illustration.

4.1 Encoding Layer

The input of natural language inference task is a
pair of sentences x and y. Since each word in a
sentence is a symbol that can not be directly pro-
cessed by neural networks, we need ﬁrst map each
word to a d dimensional embedding vector.

Thus, the two sentences are mapped to two ma-
trix Ex ∈ Rde×m and Ey ∈ Rde×n respectively.
We also use syntactical and lexical information
such as part of speech tagging information, ex-
act match feature and character representation. In
this paper, exact match value of each word is set
to 1(default to be 0) if the word concerned share
the same stem or lemma with any word in coun-
terpart sentence. Character representation of the
word is obtained using a convolution neural net-
work followed by a max pooling along sequence
length dimension as same as Kim (2014). The ﬁ-
nal representation of word is a concatenation of
word embedding, character encoded vector, POS
tagging embedding and exact match feature. Both
character embedding and POS tagging embedding
are randomly initialized. All embeddings are up-
dated during training.
use

bi-directional LSTM (BiLSTM)
to in-
Hochreiter and Schmidhuber
corporate the forward and backward context
information of sequence. Thus, we can get the

(1997)

We

Figure 2: The overall network architecture for nat-
ural language inference. The Nx means the num-
ber of the stacking interaction layers.

phrase-level encoding of two input sentences,

X = [Bi-LSTM(Ex); Ex],
Y = [Bi-LSTM(Ey); Ey],

(20)

(21)

where X ∈ Rd×m and Y ∈ Rd×n are the phrase-
level encoding representation of sentence x and y
respectively.

4.2 Convolutional Interaction Layers

In the interaction layers, we use our proposed CIN
to model the interaction between two sentences.

x

and W (f )

We ﬁrst dynamically generate context-aware ﬁl-
ters W (f )
based on the sentence encod-
ings X and Y respectively, which are further used
for both intra-sentence and inter-sentence interac-
tion.

y

Intra-Sentence Interaction The intra-sentence
convolutional interaction is to convolve one sen-
tence by the ﬁlter generated by itself.

Xintra = f (W (f )
Yintra = f (W (f )

x ⊗ X),
y ⊗ Y ),

(22)

(23)

The role of the intra-sentence convolutional in-
teraction is the same as self-attention Shen et al.
(2017), which is also very useful in NLI.

Inter-Sentence Interaction The inter-sentence
interaction takes ﬁlters generated from the coun-

terpart sentence to convolve the inputs.

Train Dev Test Len(P) Len(H) Vocab

Xinter = f (W (f )
Yinter = f (W (f )

x ⊗ Y ),
y ⊗ X),

(24)

(25)

SNLI

549K 9.8K 9.8K
MultiNLI1 392K 9.8K 9.8K
MultiNLI2 392K 9.8K 9.8K
384K 10K 10K

Quora

14
22
22
12

8
11
11
12

36K
85K
85K
107K

The inter-sentence convolutional

interaction
plays a role similar to the cross-attention between
two sentences.

Fusion Layer After CIN, we can fuse two kinds
of context-aware representations of each sentence.
For sentence x, the Xintra and Xinter represent
the extracted features under consideration of in-
formation of itself and sentence y respectively.

To efﬁciently utilize Xintra and Xinter, a fusion
layer is used. We use the comparing operation pro-
posed in Chen et al. (2017a) to fuse the two kinds
of representation. Let ui and vi be intra and in-
ter attentive vector of the i-th word in sentence x,
a heuristic and effective composition operator is
used to combine two vectors.
¯x(c)
i = [ui; vi; ui − vi; ui (cid:12) vi; |ui − vi|], (26)
x(c)
i = ReLU(Wc¯xc
(27)
Thus, we can obtain two fused representations
X (c) and Y (c) for two sentences, which are fur-
ther fed into the prediction layer or another stacked
interaction layer. The interaction layers can be
stacked for Nx times to capture the complicated
matching information.

i + Wxxi + bc),

4.3 Prediction Layer

After interaction layers, an aggregation layer is
employed to aggregate the two sequences of fus-
tion vectors X (c) and Y (c) into a ﬁxed-length
matching vectors. The aggregation component
usually consists of another BiLSTM layer and a
following pooling layer. We then perform max
pooling over time for both X (c) and Y (c) to get
two ﬁx representation vector for two sentences, p
and h:

p = max(X (c)),
h = max(Y (c)),

(28)

(29)

Statistics of

three datasets:

Table 1:
SNLI,
MultiNLI, Quora. Len(P) and Len(H) refer to
the average length of two sentences. MultiNLI1
and MultiNLI2 indicate the in-domain and cross-
domain datasets.

and sof tmax output layer in our experiments.

m = [p; h; p − h; p ∗ h; |p − h|],

p(·|x, y) = FNN(m).

(30)

(31)

5 Training

Given a trainset {x(i), y(i), t(i)}N
is to minimize a cross entropy loss J (θ):

i=1, the objective

J (θ) = −

log p(t(i)|x(i), y(i))+λ||θ||2

2, (32)

(cid:88)

1
N

i

where θ represents all the connection weights.

We use the Adam optimizer Kingma and Ba
(2014) with an initial learning rate of 0.0004. De-
fault L2 regularization λ is set to 10−6. To avoid
overﬁtting, dropout is applied after each fully con-
nected, recurrent or convolutional layer.

Initialization We take advantage of pre-trained
word embeddings such as Glove Pennington et al.
(2014) to transfer more knowledge from vast un-
labeled data. For the words that don’t appear in
Glove, we randomly initialize their embeddings
from a normal distribution with mean 0.0 and stan-
dard deviation 0.1.

The network weights are initialized with Xavier
normalization Glorot and Bengio (2010) to main-
tain the variance of activations throughout the for-
ward and backward passes. Biases are uniformly
set to zero when the network is constructed.

where the functions max is the max pooling oper-
ations over time steps.

Finally, the pooled vectors are composed as one
relation vector and fed into a feed-forward net-
work to predict the relationship between two sen-
tences. Specially, the two-layer feed-forward net-
work has one hidden layers with tanh activation

5.1 Datasets

To make quantitative evaluation, our model was
evaluated on three well known datasets: Stan-
ford Natural Language Inference dataset (SNLI),
MultiNLI dataset and Quora Question pair dataset
(Quora). Detailed statistical information of these
datasets is shown in Table 1.

Models

Handcrafted features (Bowman et al., 2015)

LSTM with attention (Rockt¨aschel et al., 2015)
Match-LSTM (Wang and Jiang, 2016)
Decomposable attention model (Parikh et al., 2016)
BiMPM (Zhiguo Wang, 2017)
NTI-SLSTM-LSTM (Munkhdalai and Yu, 2017)
Re-read LSTM (Sha et al., 2016)
DIIN (Gong et al., 2017)
ESIM (Chen et al., 2017a)
CIN

ESIM (Chen et al., 2017a) (Ensemble)
BiMPM (Zhiguo Wang, 2017) (Ensemble)
DIIN (Gong et al., 2017) (Ensemble)
CIN (Ensemble)

Train

99.7

85.3
92.0
90.5
90.9
88.5
90.7
91.2
92.6
93.2

93.5
93.2
92.3
94.3

Test

78.2

83.5
86.1
86.8
87.5
87.3
87.5
88.0
88.0
88.0

88.6
88.8
88.9
89.1

Table 2: Performance on SNLI dataset.

SNLI The SNLI corpus Bowman et al. (2015)
consists of 570,152 sentence pairs. Each sentence
pair is labeled as one of entailment, contradiction
and neutral relation.

as

the

same

MultiNLI Orgnized
SNLI,
MultiNLI corpus Williams et al. (2017) is another
it contains 433,000 sentence
dataset for NLI,
pairs. Like SNLI, each pair is labeled with one
of entailment, contradiction and neutral
label.
Difference between MultiNLI and SNLI is that,
MultiNLI have in-domain test set and develop-
ment set as well as an out-of-domain test and
development set.

Quora The Quora Question pair dataset have
over 400k question pairs, each question pair is as-
signed with a binary label to indicate if the pair are
paraphrase to each other. We evaluate our model
on the data which was previously partitioned by
Zhiguo Wang (2017)

5.2 Overall Results

We use the accuracy to evaluate the performance
of our convolutional interaction network (CIN)
and other models on SNLI, MultiNLI and Quora.

SNLI Table 2 shows the results of different
models on the train set and test set of SNLI. The
ﬁrst row gives a baseline model with handcrafted
features presented by Bowman et al. (2015). All
the other models are attention-based neural net-
works. Wang and Jiang (2016) exploits the long

short-term memory (LSTM) for NLI. Parikh et al.
(2016) uses attention to decompose the problem
into subproblems that can be solved separately.
Chen et al. (2017a) incorporates the chain LSTM
and tree LSTM jointly. Zhiguo Wang (2017) pro-
poses a bilateral multi-perspective matching for
NLI.

In Table 2, the second block gives the single
models. As we can see, our proposed model
CIN achieves 88.0% in accuracy on SNLI test set.
Compared to the previous work, CIN obtains com-
petitive performance.

To further improve the performance of NLI sys-
tems, researchers have built ensemble models. En-
semble systems obtained the best performance on
SNLI. Our ensemble model obtains 89.1% in ac-
curacy and outperforms the current state-of-the-art
model.

Overall, single model of CIN performs compet-
itively well and outperforms the previous models
on ensemble scenarios for the natural language in-
ference task.

MultiNLI Table 3 shows the performance of
different models on MultiNLI. The original aim of
this dataset is to evaluate the quality of sentence
representations. Recently this dataset is also used
to evaluate the interaction model involving atten-
tion mechanism.

The ﬁrst line of Table 3 gives a baseline model
without interaction. The second block of Table 3
gives the attention-based models. The proposed

Models

Match Mismatch

Models

Dev Test

BiLSTM
(Williams et al., 2017)

67.0

67.6

InnerAtt
(Balazs et al., 2017)
ESIM
(Chen et al., 2017a)
Gated-Att BiLSTM
(Chen et al., 2017b)
ESIM
(Chen et al., 2017a)

CIN

72.1

72.1

72.3

72.1

73.2

73.6

76.3

77.0

75.8

77.6

Table 3: Performance on MultiNLI test set.

Models

Test

79.60
Siamese-CNN
81.38
Multi-Perspective CNN
82.58
Siamese-LSTM
83.21
Multi-Perspective-LSTM
85.55
L.D.C
BiMPM (Zhiguo Wang, 2017) 88.17

CIN

88.62

Table 4: Performance on Quora question pair
dataset.

model, CIN, achieves the accuracies of 77.0% and
77.6% on the match and mismatch test sets respec-
tively. The results show that our model outper-
forms the other models.

Quora Table 4 shows the performance of differ-
ent models on the Quora test set. The baselines
on Table 4 are all implemented in Zhiguo Wang
(2017). The Siamese-CNN model and Siamese-
LSTM model encode sentences with CNN and
the re-
LSTM respectively, and then predict
lationship between them based on the cosine
similarity. Multi-Perspective-CNN and Multi-
Perspective-LSTM are transformed from Siamese-
CNN and Siamese-LSTM respectively by replac-
ing the cosine similarity calculation layer with
their multi-perspective cosine matching function.
The L.D.C is a general compare-aggregate frame-
work that performs word-level matching followed
by a aggregation of convolution neural networks.
As we can see, our model outperforms the base-

88.6 88.0
CIN
Remove whole interaction 85.6 85.1
Remove intra-attention
88.1 87.7

Table 5: Ablation experiment on SNLI dataset.

Premise

(1) A girl playing a violin along with a group of people
(2) A girl playing a violin along with a group of people

Hypothesis

(1) A girl is playing an instrument .
(2) A girl is playing an instrument .

Table 6: Gradient visualization of premise and hy-
pothesis. (1) Gradient scale of X, Y on encoding
layer. (2) Gradient scale of X (c), Y (c) on ﬁrst CIN
layer. Darker color corresponds to a higher scale
of gradient, and implies a higher contribution to
the ﬁnal prediction.

lines and achieve 88.62% in the test sets of Quora
corpus.

5.3 Model Ablation

To better understand the performance of our
model, we analyze the effect of each key compo-
nent of the proposed model. As illustrated in Table
5, the ﬁrst row is the full CIN model. By dropping
convolutional interaction layers, the performance
decreases to 85.1% on the test set, which indi-
cate the interaction information is crucial for NLI.
By just dropping intra-attention layer, the perfor-
mance decreases to 87.7% on the test set. Accord-
ing to the results, all of the components positively
contribute to the ﬁnal performance.

5.4 Case Study

To give an intuitive understanding of how our
model works, we give an analysis on the follow-
ing case from the test set.

Premise: A girl playing a violin along with a
group of people.
Hypothesis: A girl is playing an instrument.
Label: Entailment.

The visualization results are produced from
model with two stacked CINs. X, Y is the hid-
den states at encoding layer, and X (c), Y (c) is the
hidden states at ﬁrst CIN layer. For a hidden state

eﬁting from the development of deep learning and
the availability of large-scale annotated datasets,
deep neural models have achieved great success.
Rockt¨aschel et al. (2015) ﬁrstly use LSTM with
attention for text matching task. Wang and Jiang
(2016) use word-by-word attention to exploit the
word-level match. Parikh et al. (2016) propose a
new framework to model the relationship between
two sentences using interact-compare-aggregate
architecture. Chen et al. (2017a) incorporates the
chain LSTM and tree LSTM jointly. Zhiguo Wang
(2017) use self-attention mechanism to capture
contextual information from the whole sentence.

Unlike the above models, we use an alternative
model to capture the complicate interaction infor-
mation of two sentences.

Another thread of work is the idea of using one
network to generate the parameters of another net-
work. De Brabandere et al. (2016) proposed the
dynamic ﬁlter network to implicitly learn a variety
of ﬁltering operations. Ha et al. (2016a) proposed
the model hypernetwork, which uses a small net-
work to generate the weights for a larger network.
Unlike these models, our dynamical ﬁlter is em-
ployed for interaction. Therefore, a ﬁlter genera-
tion function is proposed to capture the related in-
tra and inter dependent information of a sentence
pair.

7 Conclusion

In this paper, we propose an alternative interaction
model, Convolutional Interaction Network (CIN),
for natural language inference. CIN utilizes the
dynamic convolutional ﬁlters to model the inter-
action between two sentences. Speciﬁcally, each
sentence is convolved by dynamical ﬁlters gener-
ated based on another sentence. CIN is more gen-
eral and ﬂexible since the ﬁlters may have various
numbers and sizes, thereby capturing more com-
plicated interaction patterns. Experiments on three
very large datasets demonstrate the efﬁcacy of our
proposed model.

In future work, we hope to improve the extensi-
bility of CIN and apply it to other NLP tasks, such
as machine comprehension.

Acknowledgments

We would like to thank the anonymous review-
ers for their valuable comments. We would
like to thank the anonymous reviewers for their
The research work is
valuable comments.

Figure 3: A visualization of word to word correla-
tion. Darker color correspond to a higher correla-
tion. (a) Correlation of X TY at encoder layer. (b)
Correlation of X (c)TY (c) at ﬁrst CIN layer.

xi of word xi, we can calculate its gradient scale
|| ∂J
||2 to show its contribution to ﬁnal prediction.
∂xi
Table 6 shows the gradient scales of hidden
states of each word in the encoding layer and the
ﬁrst CIN layer. As we can see, some phrases (like
playing a violin and playing an instrument) in-
stead of isolated words (like violin and instrument)
become more focused after a CIN layer. It implies
CIN could capture some higher level patterns.

Figure 3 gives a visualization of correlations
of hidden states of two sentences. (a) shows the
correlations after the encoding layer,
the same
words are most correlated. This is because em-
bedding layer and encoding layer are shared be-
tween premise and hypothesis. (b) shows the cor-
relations after the ﬁrst CIN layer, the correlation
exists between phrases {playing a violin vs. play-
ing an instrument}, instead of the same words.
The interaction layer connects playing in Premise
to Hypothesis instrument, and connects playing in
Hypothesis to Premise violin. Thus, the correla-
tion between instrument in Hypothesis and violin
in Premise are boosted, as we know these are im-
portant to reasoning.

6 Related Work

There are mainly two threads of work related to
ours.

One thread of work is using attention-based
model for natural language inference (NLI). NLI
has been widely investigated for many years. Ben-

supported by Shanghai Municipal Science and
17JC1404100
Technology Commission (No.
and 16JC1420401), and National Natural Sci-
ence Foundation of China (No. 61672162 and
61751201), Fundamental Theory and Cutting-
Edge Technology Research Program of Institute
of Information Engineering, CAS (Grant No.
Y7Z0281102).

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Jorge A Balazs, Edison Marrese-Taylor, Pablo Loy-
ola, and Yutaka Matsuo. 2017. Reﬁning raw sen-
tence representations for textual entailment recogni-
tion via attention. arXiv preprint arXiv:1707.03103.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017a. Enhanced
LSTM for natural language inference. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 1657–1668.

David Ha, Andrew Dai, and Quoc V Le. 2016b. Hy-
pernetworks. arXiv preprint arXiv:1609.09106.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems.

Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and
Luc Van Gool. 2016. Dynamic ﬁlter networks.
In Advances in Neural Information Processing Sys-
tems 29: Annual Conference on Neural Information
Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain, pages 667–675.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye-Yi Wang. 2015. Representation
learning using multi-task deep neural networks for
semantic classiﬁcation and information retrieval. In
NAACL.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017b. Recurrent neural
network-based sentence encoder with gated atten-
tion for natural language inference. arXiv preprint
arXiv:1708.01353.

Tsendsuren Munkhdalai and Hong Yu. 2017. Neural
In Proceed-
tree indexers for text understanding.
ings of the conference. Association for Computa-
tional Linguistics. Meeting, volume 1, page 11. NIH
Public Access.

Bert De Brabandere, Xu Jia, Tinne Tuytelaars, and Luc
Van Gool. 2016. Dynamic ﬁlter networks. In Neural
Information Processing Systems (NIPS).

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convo-
In Pro-
lutional sequence to sequence learning.
ceedings of the 34th International Conference on
Machine Learning, ICML 2017, Sydney, NSW, Aus-
tralia, 6-11 August 2017, pages 1243–1252.

Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. arXiv preprint
arXiv:1606.01933.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neural
networks. In International conference on artiﬁcial
intelligence and statistics, pages 249–256.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´aˇs Koˇcisk`y, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
arXiv preprint arXiv:1509.06664.

Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natu-
ral language inference over interaction space. arXiv
preprint arXiv:1709.04348.

David Ha, Andrew Dai, and Quoc V Le. 2016a. Hy-
pernetworks. arXiv preprint arXiv:1609.09106.

Lei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.
2016. Reading and thinking: Re-read lstm unit
In Proceedings
for textual entailment recognition.
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 2870–2879.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan:
Directional
rnn/cnn-
arXiv preprint
free language understanding.
arXiv:1709.04696.

self-attention network for

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA, pages 6000–6010.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,
Liang Pang, and Xueqi Cheng. 2016. A deep ar-
chitecture for semantic matching with multiple po-
sitional sentence representations. In AAAI.

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with lstm. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics.

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
arXiv
sentence understanding through inference.
preprint arXiv:1704.05426.

Wenpeng Yin, Hinrich Sch¨utze, Bing Xiang, and
Bowen Zhou. 2016. ABCNN: attention-based con-
volutional neural network for modeling sentence
pairs. TACL, 4:259–272.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Radu Florian Zhiguo Wang, Wael Hamza. 2017. Bilat-
eral multi-perspective matching for natural language
sentences. In Proceedings of the Twenty-Sixth Inter-
national Joint Conference on Artiﬁcial Intelligence,
pages 4144–4150.

