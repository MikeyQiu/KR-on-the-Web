Not submitted to the 30th International Conference on Automated Planning and Scheduling

Neural-Symbolic Descriptive Action Model from Images:
The Search for STRIPS

Masataro Asai
MIT-IBM Watson AI Lab, Cambridge USA
IBM Research

9
1
0
2
 
c
e
D
 
1
1
 
 
]
I

A
.
s
c
[
 
 
1
v
2
9
4
5
0
.
2
1
9
1
:
v
i
X
r
a

Abstract

(:action a0 :parameters () :precondition [D0]

:effect (and (when [E00]

(z0))

Recent work on Neural-Symbolic systems that learn the dis-
crete planning model from images has opened a promising di-
rection for expanding the scope of Automated Planning and
Scheduling to the raw, noisy data. However, previous work
only partially addressed this problem, utilizing the black-box
neural model as the successor generator. In this work, we pro-
pose Double-Stage Action Model Acquisition (DSAMA), a
system that obtains a descriptive PDDL action model with
explicit preconditions and effects over the propositional vari-
ables unsupervised-learned from images. DSAMA trains a set
of Random Forest rule-based classiﬁers and compiles them
into logical formulae in PDDL. While we obtained a competi-
tively accurate PDDL model compared to a black-box model,
we observed that the resulting PDDL is too large and complex
for the state-of-the-art standard planners such as Fast Down-
ward primarily due to the PDDL-SAS+ translator bottleneck.
From this negative result, we show that this translator bottle-
neck cannot be addressed just by using a different, existing
rule-based learning method, and we point to the potential fu-
ture directions.

1

Introduction

Recently, Latplan system (Asai and Fukunaga 2018) suc-
cessfully connected a subsymbolic neural network (NN) sys-
tem and a symbolic Classical Planning system to solve var-
ious visually presented puzzle domains. The system con-
sists of four parts: 1) The State AutoEncoder (SAE) neu-
ral network learns a bidirectional mapping between images
and propositional states with unsupervised training. 2) Ac-
tion Model Acquisition module generates an action model
from the propositional state transitions encoded from the im-
ages. 3) Classical Planning module solves the problem in
the propositional state space with the learned action model.
4) The decoding module maps the propositional plan back
to an image sequence. The proposed framework opened a
promising direction for applying a variety of symbolic meth-
ods to the real world — For example, the search space
generated by Latplan was shown to be compatible with a
symbolic Goal Recognition system (Amado et al. 2018a;
2018b). Several variations replacing the state encoding mod-
ules have also been proposed: Causal InfoGAN (Kurutach
et al. 2018) uses a GAN-based framework, First-Order SAE
(Asai 2019) learns the First Order Logic symbols (instead
of the propositional ones), and Zero-Suppressed SAE (Asai

(when (not [E00]) (not (z0)))
(when [E01]
(when (not [E01]) (not (z1))) ...))

(z1))

Figure 1: An example DSAMA compilation result for the
ﬁrst action (i.e. a0) generated from the image planning do-
main. [...] consists of a negation normal form compiled
from a Random Forest (Ho 1998, RF).

and Kajino 2019, ZSAE) addresses the Symbol Stability is-
sue of the regular SAE with (cid:96)1 regularization.

Despite these efforts, Latplan is missing a critical fea-
ture of the traditional Classical Planning systems: The use of
State-of-the-Art heuristic functions. The main reason behind
this limitation is the lack of descriptive action model consist-
ing of logical formula for the preconditions and the effects,
which allows the heuristics to exploit its causal structures.
Obtaining the descriptive action models from the raw ob-
servations with minimal human interference is the next key
milestone for expanding the Automated Planning applica-
tions to the raw unstructured inputs, as it fully unleashes the
pruning power of state-of-the-art Classical Planning heuris-
tic functions which allow the planner to scale up to much
larger problems.

In this paper, we propose an approach called Dual-Stage
Action Model Acquisition (DSAMA), a dual-stage process
that ﬁrst learns the set of action symbols and action effects
via Action AutoEncoder neural network module in Latplan
AMA2 (Asai and Fukunaga 2018) and then trains a rule-
based machine learning system which are then converted
into propositional formula in a PDDL format. We tested
DSAMA with Random Forest (RF) framework (Ho 1998) as
the machine learning module due to its maturity and perfor-
mance. As a result, we successfully generated a descriptive
action model, as depicted in Fig. 1 for example, which is as
accurate as the black-box neural counterpart.

Despite the success in terms of the model accuracy, the
proposed approach turned out to be an impractical solution
for descriptive action model acquisition and gave us an in-
sight into the core problem of this approach. The generated
logical formula and the resulting PDDL was too large and
complex for the recipient classical planning system (Fast

Not submitted to the 30th International Conference on Automated Planning and Scheduling

Downward) to solve the given instance in a reasonable run-
time and memory, and if we trade the accuracy with the
model simplicity, the goal becomes unreachable. We provide
an analysis on the reason and discuss possible future direc-
tions. The code reproducing the experiments will be pub-
lished at github.com/guicho271828/latplan/.

2 Preliminaries
We denote a tensor (multi-dimensional array) in bold and
denote its elements with a subscript, e.g. when x ∈ RN ×M ,
the second row is x2 ∈ RM . We use dotted subscripts to de-
note a subarray, e.g. x2..5 = (x2, x3, x4). For a vector or a
set X, |X| denotes the number of elements. 1D and 0D de-
note the constant matrix of shape D with all elements being
1/0, respectively. a; b denotes a concatenation of tensors a
and b in the ﬁrst axis where the rest of the dimensions are
same between a and b. For a dataset, we generally denote its
i-th data point with a superscript i which we may sometimes
omit for clarity.

Let F(V ) be a propositional formula consisting of log-
ical operations {∧, ∨, ¬}, constants {(cid:62), ⊥}, and a set of
propositional variables V . We deﬁne a grounded (proposi-
tional) Classical Planning problem as a 4-tuple (cid:104)P, A, I, G(cid:105)
where P is a set of propositions, A is a set of actions,
I ⊂ P is the initial state, and G ⊂ P is a goal condition.
Each action a ∈ A is a 3-tuple (cid:104)PRE(a), EFF+(a), EFF−(a)(cid:105)
where PRE(a) ∈ F(P ) is a precondition and EFF+(a),
EFF−(a) are the sets of effects called add-effects and delete-
effects, respectively. Each effect is denoted as c (cid:46) e where
c ∈ F(P ) is an effect condition and e ∈ P . A state
s ⊆ P is a set of true propositions, an action a is
applicable when s (cid:15) PRE(a) (s satisﬁes PRE(a)), and
applying an action a to s yields a new successor state
a(s) which is a(s) = s ∪ {e | (c (cid:46) e) ∈ EFF+(a), c (cid:15) s} \
{e | (c (cid:46) e) ∈ EFF−(a), c (cid:15) s}.

Modern classical planners such as Fast Downward
(Helmert 2004) takes the PDDL (McDermott 2000) input
which speciﬁes the above planning problem, and returns an
action sequence that reaches the goal state from the initial
state. Recent planners typically convert a propositional plan-
ning model into SAS+ (B¨ackstr¨om and Nebel 1995) format,
upon which the disjunctions in the action model must be
eliminated by moving the disjunctions to the root of the for-
mula and splitting the actions (Helmert 2009).

Latplan Latplan (Asai and Fukunaga 2018) is a frame-
work for domain-independent image-based classical plan-
ning. It learns the state representation as well as the transi-
tion rules entirely from the image-based observation of the
environment with deep neural networks and solves the prob-
lem using a classical planner.

Latplan takes two inputs. The ﬁrst input is the transi-
tion input Tr, a set of pairs of raw data randomly sam-
pled from the environment. An i-th data pair in the dataset
tri = (xi,0, xi,1) ∈ Tr represents a randomly sampled tran-
sition from an environment observation xi,0 to another ob-
servation xi,1 where some unknown action took place. The
second input is the planning input (xI , xG), a pair of raw
data, which corresponds to the initial and the goal state of

the environment. The output of Latplan is a data sequence
representing the plan execution (xI , . . . xG) that reaches xG
from xI . While the original paper used an image-based im-
plementation (“data” = raw images), the type of data is arbi-
trary as long as it is compatible with neural networks.

Latplan works in 3 steps. In Step 1, a State AutoEn-
coder (SAE) (Fig. 2, left) neural network learns a bidi-
rectional mapping between raw data x (e.g., images) and
propositional states z ∈ {0, 1}F , where the propositional
states are represented by F -dimensional bit vector. The
network consists of two functions ENCODE and DECODE,
where ENCODE maps an image x to z = ENCODE(x),
and DECODE function maps z back to an image y =
DECODE(z). The training is performed by minimizing the
reconstruction loss ||y − x|| under some norm (e.g., Mean
Square Error for images). In order to guarantee that z
is a binary vector, the network must use a discrete la-
tent representation learning method such as Gumbel Soft-
max (Jang, Gu, and Poole 2017; Maddison, Mnih, and
Teh 2017) or Step Function with straight-through estima-
tor (Koul, Fern, and Greydanus 2019; Bengio, L´eonard, and
Courville 2013) — We used Gumbel Softmax annealing-
based continuous relaxation GUMBELSOFTMAX(x) =
SOFTMAX((x − log(− log(UNIFORM(0, 1))))/τ ), τ →
0 in this paper. After
learning the mapping from
(cid:8). . . xi,0, xi,1 . . .(cid:9), SAE obtains the propositional
tran-
sitions Tr = (cid:8)(ENCODE(xi,0), ENCODE(xi,1))(cid:9) =
(cid:8)(zi,0, zi,1)(cid:9). In Step 2, an Action Model Acquisition
(AMA) method learns an action model from Tr. In Step 3,
a planning problem instance is generated from the planning
input (xI , xG). These are converted to the discrete states
(zI , zG) and the classical planner ﬁnds the path connecting
them. For example, an 8-puzzle problem instance consists of
an image of the start (scrambled) conﬁguration of the puzzle
and an image of the solved state. In the ﬁnal step, Latplan
obtains a step-by-step, human-comprehensive visualization
of the plan execution by DECODE’ing the latent bit vectors
for each intermediate state and validates the visualized re-
sult using a custom domain-speciﬁc validator, for the evalu-
ation purpose. This is because the SAE-generated latent bit
vectors are learned unsupervised and not directly veriﬁable
through human knowledge.

Action Model Acquisition (AMA) The original Latplan
paper proposed two approaches for AMA. AMA1 is an orac-
ular model that directly generates a PDDL without learning,
and AMA2 is a neural model that approximates AMA1 by
learning from examples.

AMA1 is an oracular, idealistic AMA that does not in-
corporate machine learning, and instead generates the entire
propositional state transitions from the entire image transi-
tions in the search space. Each propositional transition is
turned into a single, grounded action schema. For exam-
ple, in a state space represented by 2 latent space propo-
sitions z = (z1, z2), a transition from zi,0 = (0, 1) to
zi,1 = (1, 0) is translated into an action with PRE(a) =
¬z1 ∧ z2, EFF+(a) = {(cid:62) (cid:46) z1}, EFF−(a) = {(cid:62) (cid:46) z2}. It is
impractical because it requires the entire image transitions,
but also because the size of the PDDL is proportional to the

Not submitted to the 30th International Conference on Automated Planning and Scheduling

Figure 2: The illustration of State AutoEncoder, Action AutoEncoder, and Double Stage AMA for effect prediction.

number of transitions in the state space, slowing down the
PDDL-SAS+ translation, preprocessing, and heuristic cal-
culation at each search node.

AMA2 consists of two neural networks: Action AutoEn-
coder (AAE) and Action Discriminator (AD). AAE is an
autoencoder that learns to cluster the state transitions into
a (preset) ﬁnite number of action labels. See Fig. 2 (middle)
for the illustration.

1≤j≤A ai

AAE’s encoder takes a propositional state pair (zi,0, zi,1)
as the input. The last layer of the encoder is activated by
a discrete activation function (such as Gumbel Softmax) to
become a one-hot vector of A categories, ai ∈ {0, 1}A
((cid:80)
j = 1), where A is a hyperparameter for the
maximum number of action labels and ai represents an ac-
tion label. For clarity, we use the one-hot vector ai and
the index ai = arg max ai interchangeably. AAE’s de-
coder takes the current state zi,0 and ai as the input and
output ˜zi,1, which is a reconstruction of zi,1. The encoder
ACTION(zi,0, zi,1) = ai acts as a function that tells “what
action has happened” and the decoder can be seen as a pro-
gression function APPLY(ai, zi,0) = ˜zi,1 (Fig. 2, middle).

AD is a binary classiﬁer that models the preconditions
of the actions. AD learns the condition from the observed
propositional state transitions Tr and a “fake” state transi-
tions. Let P = Tr and U be the fake transitions. U could be
generated by applying a random action 1 ≤ a ≤ A to the
states in Tr.

This learning task is a Positive-Unlabeled learning task
(Elkan and Noto 2008, PU-learning). While all examples in
P are guaranteed to be the positive (valid) examples ob-
tained from the observations, the examples in U are un-
labeled, i.e., we cannot guarantee that the examples in U
are always negative (invalid). Unlike the standard binary
classiﬁcation task, which takes the purely positive and the
purely negative dataset, PU-learning takes such a positive
and an unlabeled dataset and returns a positive-negative
classiﬁer. Under the assumption that the positive exam-
ples are i.i.d.-sampled from the entire distribution of posi-
tive examples, one can obtain a positive-negative classiﬁer
D(s) for the input s by correcting the conﬁdence value
of a labeled-unlabeled classiﬁer DP U (s) by an equation
D(s) = DP U (s)/c(VP ), where VP is a positive valida-
tion set and c(VP ) = Es∈VP [DP U (s)] is a constant com-
puted after the training of DP U (s) (Elkan and Noto 2008).
In AD, s is (zi,0; zi,1), i.e., the concatenation of the propo-
sitional current state and the successor state, unlike the stan-
dard STRIPS setting where the precondition only sees the
current state.

Combining AAE and AD yields a successor function that
can be used for graph search algorithms: It ﬁrst enumer-
ates the potential successor states from the current state z
by iterating za = apply(z, a) over 1 ≤ a ≤ A, then
prunes the generated successor states using AD, i.e., whether
D(z; za) > 0.5. The major drawback of this approach is
that both AAE and AD are black-box neural networks, and
thus are incompatible with the standard PDDL-based plan-
ners and heuristics, and requires a custom heuristic graph
search solver.

3 Double-Stage Learning
To overcome the lack of PDDL compatibility of the black-
box NNs in AMA2, we propose Double-Stage Action Model
Acquisition method (DSAMA) which consists of 3 steps: (1)
It trains the same AAE networks to identify actions and per-
form the clustering, (2) transfers the knowledge to a set of
Random Forest binary classiﬁers (Fig. 2, right), then ﬁnally
(3) converts the classiﬁers into logical preconditions / effects
in PDDL. Let M be a process that returns a Random For-
est binary classiﬁer and let TOPDDL(b) be a function that
converts a classiﬁer b into a logical formula b. The overall
DSAMA process is shown in Algorithm 1.

Algorithm 1 Double-Stage Action Model Acquisition for Tr
with a Random Forest classiﬁer M .

Perform AAE training on Tr and obtain ACTION function.
for all 1 ≤ a ≤ A do

a ← (cid:8)zi,0|(zi,0, zi,1) ∈ Tr, ACTION(zi,0, zi,1) = a(cid:9)
Z0
a ← (cid:8)zi,1|(zi,0, zi,1) ∈ Tr, ACTION(zi,0, zi,1) = a(cid:9)
Z1
Za ← (cid:8)zi,0; zi,1|(zi,0, zi,1) ∈ Tr, ACTION(zi,0, zi,1) = a(cid:9)
Ua ← (cid:8)zi,0; zi,1|(zi,0, zi,1) ∈ Tr, ACTION(zi,0, zi,1) (cid:54)= a(cid:9)
for all 1 ≤ f ≤ F do

(cid:110)

(cid:111)

af ←

Z1
Eaf ← M (Z0
Eaf ← TOPDDL(Eaf ) (Effect conditions for zf )

zf |z ∈ Z1
a
a, Z1
af )

Da ← M ((Za; Ua), (1|Za|; 0|Ua|)) ((X; Y ) = concat of X and Y )
Da ← TOPDDL(Da) (precondition)
(cid:8)Eaf (cid:46) zf
Da, (cid:83)
collect action

(cid:8)¬Eaf (cid:46) ¬zf

(cid:9), (cid:83)

(cid:9)(cid:69)

(cid:68)

.

f

f

In order to learn the action preconditions, DSAMA
performs a PU-learning following Action Discriminator
(Sec. 2). Similar to AD, it takes both the current and suc-
cessor states as the input – in the later experiments, we show
that the accuracy drops when we limit the input to the current
state. Unlike AD in AMA2, DSAMA trains a speciﬁc clas-
siﬁer for each action. For the action effects, DSAMA learns

Not submitted to the 30th International Conference on Automated Planning and Scheduling

the effect condition c of the conditional effect c (cid:46) e in PDDL
(Sec. 2). DSAMA iterates over every action a and every bit
f and trains a binary classiﬁer Eaf that translates to c.

(RF) While the binary classiﬁer

Random Forest
in
DSAMA could be any binary classiﬁer that could be con-
verted to a logical formula, we chose Random Forest (Ho
1998), a machine learning method based on decision trees.
It constructs an ensemble of decision trees t = (t1 . . . tT )
and averages the predictions returned by each tree ti. We do
not describe the details of its training, which is beyond the
scope of this paper. It is one of the most widely used rule-
based learning algorithms whose implementations are avail-
able in various machine learning packages 1. To address the
potential data imbalance, we used a Balanced Random For-
est algorithm (Chen, Liaw, and Breiman 2004).

A decision tree for classiﬁcation consists of decision
nodes and leaf nodes. A decision node is a 4-tuple
(i, θ, left, right), where each element is the feature index, a
threshold, and the left / right child nodes. A leaf node con-
tains a class probability vector p ∈ [0, 1]C, where C is a
number of classes to be classiﬁed, which is 2 in our binary
case. To classify a feature vector x ∈ RF , where F is the
number of features, it tests xi ≤ θ at each decision node
and recurses into the left/right children depending on suc-
cess. When a single tree is used for classiﬁcation, it takes
the arg max over the probability vector at the leaf node and
returns the result as the predicted classiﬁcation. For an en-
semble of decision trees, classiﬁcation is performed either
by taking the average of p returned by the trees and then
taking an arg max over the classes, or by taking the arg max
at each leaf node and returning the majority class voted by
each tree.

Since STRIPS/PDDL cannot directly represent the nu-
meric averaging operation, we simulate the voting-based in-
ference of Random Forest in the PDDL framework by com-
piling the RF into a majority gate boolean circuit.

First, converting a decision tree into a boolean logic for-
mula is straightforward. Since we assume a binary input
dataset, the decision nodes and the leaf nodes can be re-
cursively converted into a Negation Normal Form as in the
TOPDDL(tree t) method (Algorithm 2).

Next, recall that we now take the votes from all trees and
choose the class with the largest votes as the ﬁnal classiﬁ-
cation prediction. Since we are handling the binary classi-
ﬁcation, ﬁnding the most voted class is equivalent to com-
puting a majority function (Lee and Jen 1992), a boolean
function of T fan-ins and a single fan-out which returns 1
when more than (cid:98)T /2(cid:99) inputs are 1. One practical approach
for implementing such a function is based on bitonic sorting
network (Knuth 1997; Batcher 1968). First, we apply the
bitonic sorting algorithm proposed by Batcher (Algorithm
2) to ti, except that instead of taking the max and min in
the COMPAREANDSWAP function, we take ∨ and ∧ of the
elements being swapped because max(x, y) = x ∨ y and
min(x, y) = x ∧ y for 0/1 values seen as boolean values.
We then use the (cid:98)T /2(cid:99)-th element stored in the result as the
output. See examples in Fig. 3, Fig. 4, Fig. 1.

1We used cl-random-forest (Imai 2017)

Finally, since our preconditions Da takes the current and
the successor states as the input, we need to take care of the
decision nodes that points to the successor state. When the
binary latent space has F dimensions, the input vector to
the random forest has 2F dimensions where the ﬁrst and the
second half of 2F dimensions is for the current and the suc-
cessor state. In TOPDDL, when the index i of the decision
node satisﬁes F < i, we insert Ea(i−F ) instead of zi be-
cause our DSAMA formulation guarantees that zi is true in
the successor bit when the effect condition Ea(i−F ) is sat-
isﬁed. This can be seen as a trick to implement a one-step
lookahead in the action model.

Algorithm 2 TOPDDL method for Random Forest (Ho
1998) based on bitonic sorting network (Batcher 1968)

function TOPDDL(forest t = (t1 . . . tT ))

(cid:46) Bitonic sorting

SORT((cid:62), t)
return t(cid:98)T /2(cid:99)

function TOPDDL(tree t)

if t is a decision node (i, θ, left, right) then

if 0 < θ < 1 then

return (xi ∧ TOPDDL(left)) ∨ (¬xi ∧ TOPDDL(right))
if θ < 0 return TOPDDL(right), else return TOPDDL(left)

else if t is a leaf node (p0, p1) then

if p0 < p1 return (cid:62), else return ⊥

function SORT(bool u, vector x)
if |x| ≤ 1 then return x
else (cid:46) Note: (a; b) is a concatenation of vector a and b (Sec. 2).

d ← (cid:98)|x|/2(cid:99), a ← x0..d,
return MERGE(u, (SORT((cid:62), a); SORT(⊥, b))))

b ← xd..|x|

function MERGE(bool u, vector x)
if |x| ≤ 1 then return x
else

COMPAREANDSWAP(u, x)
d ← (cid:98)|x|/2(cid:99), a ← x0..d,
return (MERGE(u, a); MERGE(u, b))

b ← xd..|x|

function COMPAREANDSWAP(bool u, vector x)

d ← (cid:98)|x|/2(cid:99)
for all i ∈ {0..d} do

if u then

else

xi ← xi ∨ xi+d
xi+d ← xi ∧ xi+d

xi ← xi ∧ xi+d
xi+d ← xi ∨ xi+d

(cid:46) Compare in increasing order?
(cid:46) originally max(xi, xi+d)
(cid:46) min(xi, xi+d)

(cid:46) min(xi, xi+d)
(cid:46) max(xi, xi+d)

4 Evaluation
We evaluated our approach in the dataset used by Asai
and Fukunaga, which consists of 5 image-based domains.
MNIST 8-puzzle is an image-based version of the 8-
puzzle, where tiles contain hand-written digits (0-9) from
the MNIST database (LeCun et al. 1998). Valid moves in
this domain swap the “0” tile with a neighboring tile, i.e.,
the “0” serves as the “blank” tile in the classic 8-puzzle.
The Scrambled Photograph 8-puzzle (Mandrill, Spider)
cuts and scrambles real photographs, similar to the puzzles
sold in stores). These differ from the MNIST 8-puzzle in
that “tiles” are not cleanly separated by black regions (we
re-emphasize that Latplan has no built-in notion of square

Not submitted to the 30th International Conference on Automated Planning and Scheduling

equivalent to the tree depth because the same variable may
be selected several times.)

We ﬁrst compared the successor generation accuracy
between AAE (black-box NN model) and DSAMA. The
dataset Tr is divided into the training set and the test set by
9:1. AAE uses the same hyperparameters used in Latplan
(Asai and Fukunaga 2018). DSAMA uses a Random Forest
with the number of trees T = 80 and the maximum depth of
the tree D = 100, the largest number we used in this paper.
Note that, in general, Random Forest is claimed to achieve
the monotonically higher accuracy as it has more ensembles
and depth. Table 1 shows the average reconstruction accu-
racy for the successor states over F bits, over all transitions
in the test dataset. The results indicate that DSAMA based
on Random Forest is competitive against the black box neu-
ral model.

Next, we compared the F-measure based on the true pos-
itive rate (=recall) and the true negative rate (=speciﬁcity)
of the black-box precondition model (Action Discriminator)
and the DSAMA precondition model using Random Forest.
Note that this task is not only a PU-learning task, but also a
classiﬁcation task on a potentially highly imbalanced dataset
and therefore we cannot use the accuracy as the evaluation
metric as it could be skewed toward the majority dataset
(Wallace et al. 2011).

Similar results are obtained in Table 2: Rule-based
method (Random-Forest) is competitive against the black
box method when a sufﬁciently large capacity is provided
to the model (T = 80, D = 100). To address the concern
about using the successor states as part of the precondition,
we also tested the variants which learns only from the cur-
rent state. We observed a signiﬁcant drop in the accuracy
both in the black-box NN (AD) and the DSAMA Da.

Next, in order to see the effect of the random forest hy-
perparameters on the learned results, we performed an ex-
haustive experiment on (T, D) ∈ {1, 2, 5, 10, 20, 40, 80} ×
{4, 7, 12, 25, 50, 100} and compared the precondition accu-
racy, the effect accuracy and the size of the PDDL ﬁles. Note
that T = 1 is a degenerative case for a single decision tree
without ensembles. For the space constraint, we only show
the results for Mandrill 8-Puzzle with ZSAE (α = 0.5), but
the overall characteristics were the same across domains and
the choice of SAE / ZSAE.

We observed that the effect of larger T and D saturates
quickly, while small numbers negatively affect the perfor-
mance. The action applicability prediction (i.e., the precon-
dition accuracy, Table 3, left) tends to be more affected by
the depth D while the successor state reconstruction accu-
racy (i.e., the effect accuracy, Table 3, middle) tends to be
more affected by the number of trees T . Larger T and D also
implies larger ﬁle sizes. (Note: When generating the PDDL
ﬁle, we apply De-Morgan’s law to simplify obvious invari-
ants when one is encountered, e.g., v ∧ (cid:62) = v, v ∨ ⊥ = v,
v ∧ ¬v = ⊥ and v ∨ ¬v = (cid:62).)

4.2 Evaluation in the Latent Space
To measure the effectiveness of our approach for planning,
we ran fast downward on the domain PDDL ﬁles generated
by DSAMA system.

Figure 3: Majority-gate implementation for 4 inputs
p0 . . . p3, using 7 comparators d0 . . . d6 generated as part of
bitonic sorting network.

Figure 4: Compilation of a decision tree into a logical for-
mula in PDDL (as a precondition or an effect condition).

or movable region). LightsOut is a video game where a 4x4
grid of lights is in some on/off conﬁguration, and pressing
a light toggles its state as well as the states of its neigh-
bors. The goal is all lights Off. Twisted LightsOut distorts
the original LightsOut game image by a swirl effect, show-
ing that Latplan is not limited to handling rectangular “ob-
jects”/regions. In all domains, we used 9000 transitions for
training and 1000 transitions for testing. Note that 8-puzzle
contains 362880 states and 967680 transitions, and Light-
sOut contains 65536 states and 1048576 transitions.

We used the SAE with F = 100, i.e., it produces 100 la-
tent propositions. Following the work of (Asai and Kajino
2019), we used the standard version of SAE and a regular-
ized version of SAE (ZSAE) with a regularization constant
α = 0.5. For the AAE, we tuned the upper-bound A of the
number of actions in AAE by iteratively increasing A from
A = 8 to A = 128 by ∆A = 8 until the mean absolute er-
ror of AAE (|zi,1 − ˜zi,1|) goes below 0.01, i.e., below 1 bit
on average. This is because a large A reduces the number of
transitions that fall into a single action label and makes the
random forest training harder due to the lack of examples.

4.1 Accuracy
We compared the accuracy of DSAMA and AMA2.
DSAMA has two primary controlling hyperparameters for
Random Forest — the maximum depth of the tree and the
number of trees. Other hyperparameters of Random For-
est follows the standard parameters for classiﬁcation tasks:
Entropy-based variable selection, out-of-bag ratio 0.33 (each
decision tree is trained on the random 2
3 subset of the en-
tire dataset), and the number of variables considered by each
F for the F -dimensional dataset. (note: this is not
tree as

√

Not submitted to the 30th International Conference on Automated Planning and Scheduling

Domain
LOut
LOut
Twisted
Twisted
Mandrill
Mandrill
Mnist
Mnist
Spider
Spider

SAE
α
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5

MSE
2.25E-12
8.18E-11
1.03E-02
1.08E-02
7.14E-03
7.09E-03
3.34E-04
3.44E-03
9.51E-03
8.88E-03

AMA2 AAE
A
37
43
48
39
14
9
67
3
9
8

DSAMA Eaf
Successor bit accuracy
99.1%
99.4%
99.3%
98.8%
99.9%
99.4%
98.8%
99.8%
99.2%
99.5%

98.5%
99.2%
98.8%
98.4%
98.9%
98.9%
94.0%
99.5%
98.5%
98.4%

Table 1: Accuracy comparison between AAE and DSAMA
on the successor state prediction task, where the discrete
transitions are generated by the SAE or ZSAE (α = 0.5) for
5 domains. The accuracy is measured by the number of bits
correctly predicted by the model in the test dataset. DSAMA
uses a Random Forest with the number of trees T = 80
and the maximum depth of the tree D = 100. We observed
that the Random Forest is competitive for this task against
a black box NN and sometimes even outperformed it. As a
reference, we also showed: (1) A, the number of action la-
bels generated by the AAE, and (2) the image reconstruction
error of the SAE, where the values are MSE for the pixel val-
ues in [0, 1] ⊂ R.

In each of the 5 domains, we generated 20 problem in-
stances (xI , xG) by generating the initial state with a ran-
dom walk from the goal state using a problem-speciﬁc sim-
ulator. 10 instances are generated with 7 steps away from the
goal state while the others are generated with 14 steps.

We tested three scenarios: Blind search with A∗, FF
heuristics (Hoffmann and Nebel 2001) with Greedy Best
First Search, and max-heuristics (Haslum and Geffner 2000)
with A∗. We gave 1 hour time limit and a maximum of 256
GB memory to the planner.

We tested these conﬁgurations on a variety of PDDL do-
main ﬁles generated by different T and D. As stated in the
introduction, despite our RF models achieving high accu-
racy in terms of prediction, we did not manage to ﬁnd a plan
using Fast Downward. The failure modes are threefold: The
planner failed to ﬁnd the goal after exhaustively searching
the state space, the initial heuristic value being inﬁnity in
the reachability analysis (in hFF and hmax), or the problem
transformation to SAS+ does not ﬁnish within the resource
limit.

From the results in the previous tables, the reason of the
failure is obvious: There is a trade off between the accuracy
and the PDDL ﬁle size. When the PDDL model is inaccu-
rate, the search graph becomes disconnected and the search
fails. If we increase the accuracy of the PDDL model, the
ﬁle size increases and Fast Downward fails even to start
the search. Moreover, we observed the translation fails even
with a PDDL domain ﬁle with the moderate ﬁle size (e.g.
(T, D) = (10, 7), 11MB).

In order to narrow down the reason for failure, we tested
the domain ﬁles whose preconditions are removed, i.e., re-
placed with (and) and made always applicable. We ex-

AMA2 AD

Input dataset →
α =0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5

LOut
LOut
Twisted
Twisted
Mandrill
Mandrill
Mnist
Mnist
Spider
Spider

Tr
89.7%
89.1%
84.3%
84.5%
81.6%
66.0%
87.5%
63.8%
82.5%
60.1%

(cid:8)(zi,0; ai)(cid:9)
74.2%
68.7%
75.5%
72.6%
76.2%
29.1%
81.6%
57.0%
64.3%
15.9%

DSAMA Da
Z 0
Za
a

81.5% 75.0%
85.5% 71.7%
80.3% 71.5%
78.8% 69.6%
82.4% 76.5%
77.2% 59.6%
84.4% 78.2%
61.6% 57.5%
80.7% 69.2%
77.9% 68.3%

Table 2: F-measure of true positive rate (recall) and true neg-
ative rate (speciﬁcity), comparing AD and DSAMA on the
action applicability task. The discrete transitions are gen-
erated by the SAE or ZSAE (α = 0.5) for 5 domains.
Given the prediction X and the ground truth G, recall =
P (X = 1 | G = 1), speciﬁcity = P (X = 0 | G = 0),
and F = 2·recall·speciﬁcity
recall+speciﬁcity . The numbers are for a test dataset
where each transition is assigned a boolean ground-truth
value by a validator that works on the reconstructed image
pairs. DSAMA uses a Random Forest with the number of
trees T = 80 and the maximum depth of the tree D = 100.
We show the results with the current state as the input, and
those with the concatenation of the current and the succes-
sor states as the input. The accuracy drops when the input
dataset is limited to the current state only.

pected the planner to ﬁnd any sequence of actions which
may not be a valid solution. The results were the same: The
goal state is unreachable for the small PDDL ﬁles due to the
lack of accuracy and the translation does not ﬁnish for the
large PDDL ﬁles. Considering the fact that the effect of an
action is modeled by F random forests while the precondi-
tion is modeled by a single random forest, we conclude that
the effect modeling is the main bottleneck of the translator
failure. Note that, however, the maximum accuracy of the
effect modeling with DSAMA is comparable to the neural
model and quite high (typically > 98%). We analyze this
phenomenon in the next section.

experiments

5 Discussion
showed that Random-Forest based
Our
DSAMA approach does not work even if it achieves the
same or superior accuracy in the best hyperparameter. The
main bottleneck turned out to be the effect modeling, which
is accurate but is too complex for Fast Downward to han-
dle. Based on this observation, one question arises: Can the
translator bottleneck be addressed just by using a different
rule-based learning method, such as MAX-SAT based ap-
proaches (Yang, Wu, and Jiang 2007) or planning based ap-
proaches (Aineto, Jim´enez, and Onaindia 2018)? We argue
that this is not the case because (1) our Random Forest based
DSAMA approach can be considered as the upper bound of
existing Action Model Acquisition method in terms of accu-
racy and (2) should the same accuracy be achieved by other
approaches, the resulting PDDL must have the same com-
plexity. We explain the reasoning below.

Not submitted to the 30th International Conference on Automated Planning and Scheduling

Action applicability (precondition) F-measure
D

7

4

50

25

12

100
36.5% 56.1% 65.5% 49.2% 49.8% 49.5%
40.5% 59.1% 67.0% 63.8% 63.8% 64.0%
37.3% 56.8% 70.1% 69.0% 69.5% 68.8%
35.7% 56.2% 71.1% 72.7% 72.9% 72.5%
35.9% 56.8% 72.4% 75.0% 74.8% 74.6%
32.8% 55.6% 73.1% 76.6% 76.2% 76.4%
33.4% 55.6% 73.1% 77.1% 77.1% 77.2%

Successor state reconstruction (effect) accuracy
D

7

4

50

25

12

100
87.7% 90.6% 91.7% 91.8% 91.8% 91.8%
91.5% 93.0% 91.8% 91.9% 91.9% 91.9%
94.4% 95.8% 96.3% 96.3% 96.3% 96.3%
95.5% 97.3% 97.5% 97.5% 97.5% 97.5%
96.4% 98.0% 98.2% 98.2% 98.2% 98.2%
96.7% 98.2% 98.8% 98.8% 98.8% 98.8%
96.9% 98.5% 98.9% 98.8% 98.9% 98.9%

PDDL domain ﬁle size
D

7

25

12

50

100
4
1.5M 2.0M 3.0M 2.9M 2.9M
964K
2.2M 3.2M 4.1M 6.0M 6.0M 6.0M
7.8M 11M 13M 18M 18M 18M
24M 29M 34M 43M 44M 43M
71M 81M 91M 109M 109M 109M
204M 224M 244M 281M 281M 281M
557M 597M 636M 710M 710M 709M

T

1
2
5
10
20
40
80

Table 3: Accuracy for the precondition and the effects by DSAMA using various Random Forest hyperparameters, as well as the
PDDL domain ﬁle sizes (in bytes) resulted from their compilation. T is the number of trees in each Random Forest ensemble,
and D is the maximum depth of the tree. The table is showing the results for the test dataset of Mandrill 8-Puzzle, where the
discrete state vectors are generated by ZSAE (α = 0.5).

First, we note that the translation failure is due to the
heavy use of disjunctions in the PDDL ﬁle for the compila-
tion of Random Forest because, in Fast Downward, disjunc-
tions are “ﬂattened” (Helmert 2009), i.e., compiled away by
making the separate actions for each branch of the disjunc-
tion. This causes an exponential blowup when a huge num-
ber of disjunctions are presented to the translator, which is
exactly the case for our scenario. The use of effect conditions
are not an issue because Fast Downward uses them directly.
Next, in order to avoid this exponential blowup, the result-
ing rules learned by the binary classiﬁer must be disjunction-
free. In fact, existing approaches (Yang, Wu, and Jiang 2007;
Aineto, Jim´enez, and Onaindia 2018) learn the disjunction-
free action models. One trivial approach to achieve this in
DSAMA is to compile a decision tree into Decision Lists
(Cohen 1995), the degenerate case of decision trees where
the children (left, right) of every decision node can contain
at most one decision node. However this is trivially inef-
fective because compiling a decision tree into a decision
list is equivalent to how Fast Downward makes the actions
disjunction-free by splitting them. Both cases end up in an
exponentially large list of disjunction-free actions.

Finally, given that our Random Forest based DSAMA
achieved almost-perfect accuracy in the successor genera-
tion task (effect condition), we could argue that the rules
generated by our approach are quite close to the ground truth
rules and, therefore, the ground truth rules are at least as
complex as the rules found by DSAMA. Therefore, if the ex-
isting approaches achieved the same accuracy on the same
task, their resulting disjunction-free set of conditions would
be as large and complex as the exponentially large “ﬂat-
tened” form of our rules. This argument also applies to the
variants of DSAMA using Decision List based classiﬁers
(e.g., (Holte 1993, OneR),(Cohen 1995, RIPPER),(Maliotov
and Meel 2018, MLIC)).

6 Related Work
Traditionally, symbolic action learners tend to require a cer-
tain type of human domain knowledge and have been situat-
ing itself merely as an additional assistance tool for humans,
rather than a system that builds knowledge from the scratch,
e.g., from unstructured images. Many systems require a
structured input representation (i.e., First Order Logic) that
are partially hand-crafted and exploits the symmetry and
the structures provided by the structured representation, al-

though the requirements of the systems may vary. For exam-
ple, some systems require state sequences (Yang, Wu, and
Jiang 2007), while others require action sequences (Cress-
well and Gregory 2011; Cresswell, McCluskey, and West
2013). Some supports the noisy input (Mour˜ao et al. 2012;
Zhuo and Kambhampati 2013), partial observations in a
state and missing state/actions in a plan trace (Aineto,
Jim´enez, and Onaindia 2018), or a disordered plan trace
(Zhuo, Peng, and Kambhampati 2019). Approach-wise, they
can be grouped into 3 categories: MAX-SAT based ap-
proaches (Yang, Wu, and Jiang 2007; Zhuo and Kambham-
pati 2013; Zhuo, Peng, and Kambhampati 2019), Object-
centric approaches (Cresswell and Gregory 2011; Cresswell,
McCluskey, and West 2013) and learning-as-planning ap-
proaches (Aineto, Jim´enez, and Onaindia 2018). AMA2 and
DSAMA works on a factored but non-structured proposi-
tional representation. While we do not address the problem
of lifting the action description, combining these approaches
with the FOL symbols (relations/predicates) found by NN
(Asai 2019) is an interesting avenue for future work.

There are several lines of work that extracts a PDDL ac-
tion model from a natural language corpus. Framer (Lindsay
et al. 2017) uses a CoreNLP language model while EAS-
DRL (Feng, Zhuo, and Kambhampati 2018) uses Deep Re-
inforcement Learning (Mnih et al. 2015). The difference
from our approach is that they are reusing the symbols found
in the corpus while we generate the discrete propositional
symbols from the visual perception which completely lacks
such a predeﬁned set of discrete symbols.

While there are recent efforts in handling the complex
state space without having the action description (Frances
et al. 2017), action models could be used for other pur-
poses, including Goal Recognition (Ram´ırez and Geffner
2009), macro-action generation (Botea and Braghin 2015;
Chrpa, Vallati, and McCluskey 2015), or plan optimization
(Chrpa and Siddiqui 2015).

There are three lines of work that learn the binary rep-
resentation of the raw environment. Latplan SAE (Asai and
Fukunaga 2018) uses the Gumbel-Softmax VAE (Maddison,
Mnih, and Teh 2017; Jang, Gu, and Poole 2017) which was
modiﬁed from the original to maximize the KL divergence
term for the Bernoulli distribution (Asai and Kajino 2019).
Causal InfoGAN (Kurutach et al. 2018) uses GAN(Good-
fellow et al. 2014)-based approach combined with Gum-
bel Softmax prior and Mutual Information prior. Mutual In-

Not submitted to the 30th International Conference on Automated Planning and Scheduling

formation and the negated KL term are both the same en-
tropy term H(z|x), i.e., the randomness of the latent vec-
tor z given a particular input image x. Latplan ZSAE (Asai
and Kajino 2019) additionally penalizes the “true” cate-
gory in the binary categorical distribution to suppress the
chance of random ﬂips in the latent vector caused by the in-
put noise. It was shown that these random ﬂips negatively
affect the performance of the recipient symbolic systems
by violating the uniqueness assumption of the representa-
tion, dubbed as “symbol stability problem”. Quantized Bot-
tleneck Network (Koul, Fern, and Greydanus 2019) uses
quantized activations (i.e., step functions) in the latent space
to obtain the discrete representation. It trains the network
with Straight-Through gradient estimator (Bengio, L´eonard,
and Courville 2013), which enables the backpropagation
through the step function. There are more complex varia-
tions such as VQVAE (van den Oord, Vinyals, and others
2017), DVAE++(Vahdat et al. 2018), DVAE# (Vahdat, An-
driyash, and Macready 2018).

In the context of modern machine learning, Deep Rein-
forcement Learning (DRL) has solved complex problems,
including Atari video games (Mnih et al. 2015, DQN) or
Game of Go (Silver and others 2016, AlphaGo). However,
they both have a hard-coded list of action symbols (e.g.,
levers, Fire button, grids to put stones) and relies on the hard-
coded simulator for both learning and the correct execution.
In another line of work, Neural Networks model the ex-
ternal environment captured by video cameras by explic-
itly taking the temporal dependency into account (Lotter,
Kreiman, and Cox 2017), unlike Latplan SAE, which pro-
cesses each image frame one by one.

7 Conclusion

In this paper, we negatively answered a question of whether
simply replacing a neural, black-box Action Model Acquisi-
tion model with a rule-based machine learning model would
generate a useful descriptive action model from the raw,
unstructured input. Our approach hybrids a neural unsu-
pervised learning approach to the action label generation
and the precondition/effect-condition learning using State-
of-the-Art rule-based machine learning. While the proposed
method was able to generate accurate PDDL models, the
models are too complex for the standard planner to prepro-
cess in a reasonable runtime and memory.

The fact that the rather straightforward modeling of ef-
fects in DSAMA is causing such a huge problem is worth
noting. The planning domains written by humans, in gen-
eral, tend to have a speciﬁc human-originated property that
causes this type of phenomenon to happen less often, and
this might be reﬂected by the fact that STRIPS (without dis-
junctions) was the ﬁrst common language adapted by the
community. Unlike the planning models written by the hu-
man, we found that the set of propositions generated by the
State AutoEncoder network, as well as the set of action la-
bels generated by the Action AutoEncoder network, do not
have such a property. The state space and the clustering of
transitions are “less organized” compared to the typical hu-
man models, and the lack of human-like regularization be-

havior makes an otherwise trivial task of PDDL-SAS+ trans-
lation intractable in a modern planner.

The future directions are twofold. The ﬁrst one is to ﬁnd
the right regularization or the right architecture for the neu-
ral networks in order to further constrain the space of the
ground-truth transition model in the latent space. This is
similar to the approach pursued by (Asai and Kajino 2019)
which tries to suppress the instability of the propositional
values in the latent space. Machine Learning community
is increasingly focusing on the disentangled representation
learning (Higgins et al. 2017) that tries to separate the mean-
ing of the feature values in the latent space. Finding the right
structural bias for neural networks has a long history, no-
tably the convolutional neural networks (Fukushima 1980;
LeCun et al. 1989; Krizhevsky, Sutskever, and Hinton 2012)
for images, or LSTMs (Hochreiter and Schmidhuber 1997)
and transformers (Vaswani et al. 2017) for sequence model-
ing.

The second approach is to develop a planner that can di-
rectly handle the complex logical conditions in an efﬁcient
manner. Fast downward requires converting the input PDDL
into SAS+ with a rather slow translator, assuming that such a
task tends to be easy and tractable. While this may hold for
most hand-crafted domains (such as IPC domains), it may
not be a viable approach when the symbolic input is gener-
ated by neural networks.

References
Aineto, D.; Jim´enez, S.; and Onaindia, E. 2018. Learning
In Twenty-
STRIPS Action Models with Classical Planning.
Eighth International Conference on Automated Planning and
Scheduling.
Amado, L.; Pereira, R. F.; Aires, J.; Magnaguagno, M.;
Granada, R.; and Meneguzzi, F. 2018a. Goal Recognition in
In Proc. of International Joint Conference on
Latent Space.
Neural Networks (IJCNN).
Amado, L.; Pereira, R. F.; Aires, J.; Magnaguagno, M.;
Granada, R.; and Meneguzzi, F. 2018b. LSTM-based Goal
Recognition in Latent Space. arXiv preprint arXiv:1808.05249.
Asai, M., and Fukunaga, A. 2018. Classical Planning in Deep
Latent Space: Bridging the Subsymbolic-Symbolic Boundary.
In Proc. of AAAI Conference on Artiﬁcial Intelligence.
2019. Towards Stable Symbol
Asai, M., and Kajino, H.
Grounding with Zero-Suppressed State AutoEncoder. In Proc.
of the International Conference on Automated Planning and
Scheduling(ICAPS).
Asai, M. 2019. Unsupervised Grounding of Plannable First-
Order Logic Representation from Images. In Proc. of the In-
ternational Conference on Automated Planning and Schedul-
ing(ICAPS).
B¨ackstr¨om, C., and Nebel, B. 1995. Complexity Results for
SAS+ Planning. Computational Intelligence 11(4):625–655.
Batcher, K. E. 1968. Sorting Networks and Their Applications.
In Proceedings of the April 30–May 2, 1968, spring joint com-
puter conference, 307–314. ACM.
Bengio, Y.; L´eonard, N.; and Courville, A. 2013. Estimating
or Propagating Gradients through Stochastic Neurons for Con-
ditional Computation. arXiv preprint arXiv:1308.3432.

Not submitted to the 30th International Conference on Automated Planning and Scheduling

Botea, A., and Braghin, S. 2015. Contingent versus Determin-
In Proc. of the
istic Plans in Multi-Modal Journey Planning.
International Conference on Automated Planning and Schedul-
ing(ICAPS), 268–272.
Chen, C.; Liaw, A.; and Breiman, L. 2004. Using Random
Forest to Learn Imbalanced Data. Technical Report Technical
Report 666, Department of Statistics, UC Berkeley.
Chrpa, L., and Siddiqui, F. H. 2015. Exploiting Block Deorder-
ing for Improving Planners Efﬁciency. In Proc. of International
Joint Conference on Artiﬁcial Intelligence (IJCAI).
Chrpa, L.; Vallati, M.; and McCluskey, T. L. 2015. On the On-
line Generation of Effective Macro-Operators. In Proc. of In-
ternational Joint Conference on Artiﬁcial Intelligence (IJCAI).
Cohen, W. W. 1995. Fast Effective Rule Induction. In Proc. of
the International Conference on Machine Learning, 115–123.
Cresswell, S., and Gregory, P. 2011. Generalised Domain
In Proc. of the In-
Model Acquisition from Action Traces.
ternational Conference on Automated Planning and Schedul-
ing(ICAPS).
Cresswell, S.; McCluskey, T. L.; and West, M. M. 2013. Ac-
quiring planning domain models using LOCM. Knowledge Eng.
Review 28(2):195–213.
Elkan, C., and Noto, K. 2008. Learning Classiﬁers from Only
Positive and Unlabeled Data. In Proceedings of the 14th ACM
SIGKDD international conference on Knowledge discovery and
data mining, 213–220. ACM.
Feng, W.; Zhuo, H. H.; and Kambhampati, S. 2018. Extract-
ing Action Sequences from Texts Based on Deep Reinforce-
In Proceedings of the Twenty-Seventh Inter-
ment Learning.
national Joint Conference on Artiﬁcial Intelligence, IJCAI-18,
4064–4070. Proc. of International Joint Conference on Artiﬁ-
cial Intelligence (IJCAI).
Frances, G.; Ramırez, M.; Lipovetzky, N.; and Geffner, H.
2017. Purely Declarative Action Representations are Over-
In Proc. of Inter-
rated: Classical Planning with Simulators.
national Joint Conference on Artiﬁcial Intelligence (IJCAI),
4294–4301.
Fukushima, K. 1980. Neocognitron: A Self-Organizing Neural
Network Model for a Mechanism of Pattern Recognition Unaf-
fected by Shift in Position. Biological cybernetics 36(4):193–
202.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-
Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Gen-
In Advances in Neural Information
erative Adversarial Nets.
Processing Systems, 2672–2680.
Haslum, P., and Geffner, H. 2000. Admissible Heuristics for
Optimal Planning. In Proc. of the International Conference on
Artiﬁcial Intelligence Planning and Scheduling.
Helmert, M. 2004. A Planning Heuristic Based on Causal
In Proc. of the International Conference on
Graph Analysis.
Automated Planning and Scheduling(ICAPS), 161–170.
Helmert, M. 2009. Concise Finite-Domain Representations
for PDDL Planning Tasks. Artiﬁcial Intelligence 173(5-6):503–
535.
Higgins, I.; Matthey, L.; Pal, A.; Burgess, C.; Glorot, X.;
Botvinick, M.; Mohamed, S.; and Lerchner, A. 2017. beta-
VAE: Learning Basic Visual Concepts with a Constrained Vari-
ational Framework. volume 2, 6.

Ho, T. K. 1998. The Random Subspace Method for Construct-
ing Decision Forests. IEEE Transactions on Pattern Analysis
and Machine Intelligence 20(8):832–844.
Hochreiter, S., and Schmidhuber, J. 1997. Long Short-Term
Memory. Neural Computation 9(8):1735–1780.
Hoffmann, J., and Nebel, B. 2001. The FF Planning System:
Fast Plan Generation through Heuristic Search. J. Artif. Intell.
Res.(JAIR) 14:253–302.
Holte, R. C. 1993. Very Simple Classiﬁcation Rules Perform
Well on Most Commonly Used Datasets. Machine learning
11(1):63–90.
Imai, S. 2017. cl-random-forest. https://github.
com/masatoi/cl-random-forest.
Jang, E.; Gu, S.; and Poole, B. 2017. Categorical Reparame-
terization with Gumbel-Softmax. In Proc. of the International
Conference on Learning Representations.
Knuth, D. E. 1997. The Art of Computer Programming, vol-
ume 3. Pearson Education.
Koul, A.; Fern, A.; and Greydanus, S. 2019. Learning Finite
State Representations of Recurrent Policy Networks. In Proc.
of the International Conference on Learning Representations.
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012.
Ima-
genet Classiﬁcation with Deep Convolutional Neural Networks.
In Advances in Neural Information Processing Systems, 1097–
1105.
Kurutach, T.; Tamar, A.; Yang, G.; Russell, S.; and Abbeel, P.
2018. Learning Plannable Representations with Causal Info-
GAN. In In Proceedings of ICML / IJCAI / AAMAS 2018 Work-
shop on Planning and Learning (PAL-18).
LeCun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard,
R. E.; Hubbard, W.; and Jackel, L. D. 1989. Backpropagation
Applied to Handwritten Zip Code Recognition. Neural Com-
putation 1(4):541–551.
LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P.
1998.
Gradient-Based Learning Applied to Document Recognition.
Proc. of the IEEE 86(11):2278–2324.
Lee, C. L., and Jen, C.-W. 1992. Bit-Sliced Median Filter De-
IEE Proceedings G (Circuits,
sign based on Majority Gate.
Devices and Systems) 139(1):63–71.
Lindsay, A.; Read, J.; Ferreira, J. F.; Hayton, T.; Porteous, J.;
and Gregory, P. J. 2017. Framer: Planning Models from Natural
In Proc. of the International
Language Action Descriptions.
Conference on Automated Planning and Scheduling(ICAPS).
Lotter, W.; Kreiman, G.; and Cox, D. 2017. Deep Predic-
tive Coding Networks for Video Prediction and Unsupervised
Learning. In Proc. of the International Conference on Learning
Representations.
Maddison, C. J.; Mnih, A.; and Teh, Y. W. 2017. The Con-
crete Distribution: A Continuous Relaxation of Discrete Ran-
In Proc. of the International Conference on
dom Variables.
Learning Representations.
Maliotov, D., and Meel, K. S. 2018. Mlic: A maxsat-based
framework for learning interpretable classiﬁcation rules.
In
Proc. of the International Conference on Principles and Prac-
tice of Constraint Programming (CP), 312–327. Springer.
McDermott, D. V. 2000. The 1998 AI Planning Systems Com-
petition. AI Magazine 21(2):35–55.

Not submitted to the 30th International Conference on Automated Planning and Scheduling

Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.;
Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.;
Ostrovski, G.; et al. 2015. Human-Level Control through Deep
Reinforcement Learning. Nature 518(7540):529–533.
Mour˜ao, K.; Zettlemoyer, L. S.; Petrick, R. P. A.; and Steed-
man, M. 2012. Learning STRIPS Operators from Noisy and
Incomplete Observations. In Proc. of the International Confer-
ence on Uncertainty in Artiﬁcial Intelligence, 614–623.
Ram´ırez, M., and Geffner, H. 2009. Plan Recognition as Plan-
ning. In Proc. of AAAI Conference on Artiﬁcial Intelligence.
Silver, D., et al. 2016. Mastering the Game of Go with Deep
Neural Networks and Tree Search. Nature 529(7587):484–489.
Vahdat, A.; Andriyash, E.; and Macready, W. 2018. DVAE#:
Discrete variational autoencoders with relaxed Boltzmann pri-
In Advances in Neural Information Processing Systems,
ors.
1864–1874.
Vahdat, A.; Macready, W. G.; Bian, Z.; Khoshaman, A.; and
Andriyash, E.
2018. DVAE++: Discrete variational au-
toencoders with overlapping transformations. arXiv preprint
arXiv:1802.04920.
van den Oord, A.; Vinyals, O.; et al. 2017. Neural Discrete
Representation Learning. In Advances in Neural Information
Processing Systems, 6306–6315.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention
is all you need. In Advances in neural information processing
systems, 5998–6008.
Wallace, B. C.; Small, K.; Brodley, C. E.; and Trikalinos, T. A.
2011. Class Imbalance, Redux. In Proc. of IEEE International
Conference on Data Mining, 754–763. IEEE.
Yang, Q.; Wu, K.; and Jiang, Y. 2007. Learning Action Mod-
els from Plan Examples using Weighted MAX-SAT. Artiﬁcial
Intelligence 171(2-3):107–143.
Zhuo, H. H., and Kambhampati, S. 2013. Action-Model Acqui-
sition from Noisy Plan Traces. In Twenty-Third International
Joint Conference on Artiﬁcial Intelligence.
Zhuo, H. H.; Peng, J.; and Kambhampati, S. 2019. Learning
Action Models from Disordered and Noisy Plan Traces. arXiv
preprint arXiv:1908.09800.

