Stack-Pointer Networks for Dependency Parsing

Xuezhe Ma
Carnegie Mellon University
xuezhem@cs.cmu.edu

Zecong Hu∗
Tsinghua University
huzecong@gmail.com

Jingzhou Liu
Carnegie Mellon University
liujingzhou@cs.cmu.edu

Nanyun Peng
University of Southern California
npeng@isi.edu

Graham Neubig and Eduard Hovy
Carnegie Mellon University
{gneubig, ehovy}@cs.cmu.edu

Abstract

We introduce a novel architecture for de-
pendency parsing: stack-pointer networks
(STACKPTR). Combining pointer net-
works (Vinyals et al., 2015) with an in-
ternal stack,
the proposed model ﬁrst
reads and encodes the whole sentence,
then builds the dependency tree top-down
(from root-to-leaf) in a depth-ﬁrst fashion.
The stack tracks the status of the depth-
ﬁrst search and the pointer networks se-
lect one child for the word at the top of
the stack at each step. The STACKPTR
parser beneﬁts from the information of the
whole sentence and all previously derived
subtree structures, and removes the left-
to-right restriction in classical transition-
based parsers. Yet, the number of steps for
building any (including non-projective)
parse tree is linear in the length of the sen-
tence just as other transition-based parsers,
yielding an efﬁcient decoding algorithm
with O(n2) time complexity. We evalu-
ate our model on 29 treebanks spanning 20
languages and different dependency anno-
tation schemas, and achieve state-of-the-
art performance on 21 of them.

1

Introduction

Dependency parsing, which predicts the existence
and type of linguistic dependency relations be-
tween words, is a ﬁrst step towards deep language
Its importance is widely recog-
understanding.
nized in the natural language processing (NLP)
community, with it beneﬁting a wide range of
NLP applications, such as coreference resolu-
tion (Ng, 2010; Durrett and Klein, 2013; Ma et al.,

∗Work done while at Carnegie Mellon University.

2016), sentiment analysis (Tai et al., 2015), ma-
chine translation (Bastings et al., 2017), informa-
tion extraction (Nguyen et al., 2009; Angeli et al.,
2015; Peng et al., 2017), word sense disambigua-
tion (Fauceglia et al., 2015), and low-resource lan-
guages processing (McDonald et al., 2013; Ma and
Xia, 2014). There are two dominant approaches to
dependency parsing (Buchholz and Marsi, 2006;
local and greedy transition-
Nivre et al., 2007):
based algorithms (Yamada and Matsumoto, 2003;
Nivre and Scholz, 2004; Zhang and Nivre, 2011;
Chen and Manning, 2014), and the globally opti-
mized graph-based algorithms (Eisner, 1996; Mc-
Donald et al., 2005a,b; Koo and Collins, 2010).

parsers

dependency

Transition-based

read
words sequentially (commonly from left-to-right)
and build dependency trees incrementally by
making series of multiple choice decisions. The
advantage of this formalism is that the number of
operations required to build any projective parse
tree is linear with respect to the length of the sen-
tence. The challenge, however, is that the decision
made at each step is based on local information,
leading to error propagation and worse perfor-
mance compared to graph-based parsers on root
and long dependencies (McDonald and Nivre,
2011). Previous studies have explored solutions
to address this challenge. Stack LSTMs (Dyer
et al., 2015; Ballesteros et al., 2015, 2016) are
capable of learning representations of the parser
state that are sensitive to the complete contents of
the parser’s state. Andor et al. (2016) proposed a
globally normalized transition model to replace
the locally normalized classiﬁer. However, the
parsing accuracy is still behind state-of-the-art
graph-based parsers (Dozat and Manning, 2017).
Graph-based dependency parsers, on the other
hand, learn scoring functions for parse trees and
perform exhaustive search over all possible trees
for a sentence to ﬁnd the globally highest scoring

8
1
0
2
 
y
a
M
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
7
8
0
1
0
.
5
0
8
1
:
v
i
X
r
a

tree.
Incorporating this global search algorithm
with distributed representations learned from neu-
ral networks, neural graph-based parsers (Kiper-
wasser and Goldberg, 2016; Wang and Chang,
2016; Kuncoro et al., 2016; Dozat and Manning,
2017) have achieved the state-of-the-art accura-
cies on a number of treebanks in different lan-
guages. Nevertheless, these models, while accu-
rate, are usually slow (e.g. decoding is O(n3)
time complexity for ﬁrst-order models (McDonald
et al., 2005a,b) and higher polynomials for higher-
order models (McDonald and Pereira, 2006; Koo
and Collins, 2010; Ma and Zhao, 2012b,a)).

In this paper, we propose a novel neural net-
work architecture for dependency parsing, stack-
pointer networks (STACKPTR). STACKPTR is
a transition-based architecture, with the corre-
sponding asymptotic efﬁciency, but still main-
tains a global view of the sentence that proves es-
sential for achieving competitive accuracy. Our
STACKPTR parser has a pointer network (Vinyals
et al., 2015) as its backbone, and is equipped
with an internal stack to maintain the order of
head words in tree structures. The STACKPTR
parser performs parsing in an incremental, top-
down, depth-ﬁrst fashion; at each step, it gener-
ates an arc by assigning a child for the head word
at the top of the internal stack. This architecture
makes it possible to capture information from the
whole sentence and all the previously derived sub-
trees, while maintaining a number of parsing steps
linear in the sentence length.

We evaluate our parser on 29 treebanks across
20 languages and different dependency annotation
schemas, and achieve state-of-the-art performance
on 21 of them. The contributions of this work are
summarized as follows:

(i) We propose a neural network architecture for
dependency parsing that is simple, effective,
and efﬁcient.

(ii) Empirical evaluations on benchmark datasets
over 20 languages show that our method
achieves state-of-the-art performance on 21
different treebanks1.

(iii) Comprehensive error analysis is conducted
to compare the proposed method to a strong
graph-based baseline using biafﬁne atten-
tion (Dozat and Manning, 2017).

2 Background

We ﬁrst brieﬂy describe the task of dependency
parsing, setup the notation, and review Pointer
Networks (Vinyals et al., 2015).

2.1 Dependency Parsing and Notations

Dependency trees represent syntactic relationships
between words in the sentences through labeled
directed edges between head words and their de-
pendents. Figure 1 (a) shows a dependency tree
for the sentence, “But there were no buyers”.

In this paper, we will use the following notation:
Input: x = {w1, . . . , wn} represents a generic

sentence, where wi is the ith word.

Output: y = {p1, p2, · · · , pk} represents a
generic (possibly non-projective) dependency tree,
where each path pi = $, wi,1, wi,2, · · · , wi,li is a
sequence of words from the root to a leaf. “$” is
an universal virtual root that is added to each tree.
Stack: σ denotes a stack conﬁguration, which
is a sequence of words. We use σ|w to represent
a stack conﬁguration that pushes word w into the
stack σ.

Children: ch(wi) denotes the list of all the chil-

dren (modiﬁers) of word wi.

2.2 Pointer Networks

Pointer Networks (PTR-NET) (Vinyals et al.,
2015) are a variety of neural network capable of
learning the conditional probability of an output
sequence with elements that are discrete tokens
corresponding to positions in an input sequence.
This model cannot be trivially expressed by stan-
dard sequence-to-sequence networks (Sutskever
et al., 2014) due to the variable number of input
positions in each sentence. PTR-NET solves the
problem by using attention (Bahdanau et al., 2015;
Luong et al., 2015) as a pointer to select a member
of the input sequence as the output.

Formally, the words of the sentence x are fed
one-by-one into the encoder (a multiple-layer bi-
directional RNN), producing a sequence of en-
coder hidden states si. At each time step t, the
decoder (a uni-directional RNN) receives the input
from last step and outputs decoder hidden state ht.
The attention vector at is calculated as follows:

et
i = score(ht, si)
at = softmax (et)

(1)

1Source code is publicly available at https://

github.com/XuezheMax/NeuroNLP2

where score(·, ·) is the attention scoring function,
which has several variations such as dot-product,

(a)

(b)

Figure 1: Neural architecture for the STACKPTR network, together with the decoding procedure of an
example sentence. The BiRNN of the encoder is elided for brevity. For the inputs of decoder at each
time step, vectors in red and blue boxes indicate the sibling and grandparent.

concatenation, and biafﬁne (Luong et al., 2015).
PTR-NET regards the attention vector at as a prob-
it
ability distribution over the source words, i.e.
uses at
i as pointers to select the input elements.

3 Stack-Pointer Networks

3.1 Overview

Similarly to PTR-NET, STACKPTR ﬁrst reads the
whole sentence and encodes each word into the
encoder hidden state si. The internal stack σ is
always initialized with the root symbol $. At each
time step t, the decoder receives the input vector
corresponding to the top element of the stack σ
(the head word wp where p is the word index), gen-
erates the hidden state ht, and computes the atten-
tion vector at using Eq. (1). The parser chooses a
speciﬁc position c according to the attention scores
in at to generate a new dependency arc (wh, wc)
by selecting wc as a child of wh. Then the parser
pushes wc onto the stack, i.e. σ → σ|wc, and goes
to the next step. At one step if the parser points wh
to itself, i.e. c = h, it indicates that all children
of the head word wh have already been selected.
Then the parser goes to the next step by popping
wh out of σ.

At test time, in order to guarantee a valid de-
pendency tree containing all the words in the in-
put sentences exactly once, the decoder maintains
a list of “available” words. At each decoding step,
the parser selects a child for the current head word,

and removes the child from the list of available
words to make sure that it cannot be selected as a
child of other head words.

For head words with multiple children, it is pos-
sible that there is more than one valid selection
for each time step. In order to deﬁne a determin-
istic decoding process to make sure that there is
only one ground-truth choice at each step (which
is necessary for simple maximum likelihood esti-
mation), a predeﬁned order for each ch(wi) needs
to be introduced. The predeﬁned order of chil-
dren can have different alternatives, such as left-
to-right or inside-out2.
In this paper, we adopt
the inside-out order3 since it enables us to utilize
second-order sibling information, which has been
proven beneﬁcial for parsing performance (Mc-
Donald and Pereira, 2006; Koo and Collins, 2010)
(see § 3.4 for details). Figure 1 (b) depicts the ar-
chitecture of STACKPTR and the decoding proce-
dure for the example sentence in Figure 1 (a).

3.2 Encoder

The encoder of our parsing model is based on the
bi-directional LSTM-CNN architecture (BLSTM-
CNNs) (Chiu and Nichols, 2016; Ma and Hovy,
2016) where CNNs encode character-level infor-
mation of a word into its character-level repre-

2Order the children by the distances to the head word on

the left side, then the right side.

3We also tried left-to-right order which obtained worse

parsing accuracy than inside-out.

sentation and BLSTM models context informa-
tion of each word. Formally, for each word, the
CNN, with character embeddings as inputs, en-
codes the character-level representation. Then the
character-level representation vector is concate-
nated with the word embedding vector to feed into
the BLSTM network. To enrich word-level infor-
mation, we also use POS embeddings. Finally, the
encoder outputs a sequence of hidden states si.

3.3 Decoder

The decoder for our parser is a uni-directional
LSTM. Different from previous work (Bahdanau
et al., 2015; Vinyals et al., 2015) which uses word
embeddings of the previous word as the input to
the decoder, our decoder receives the encoder hid-
den state vector (si) of the top element in the stack
σ (see Figure 1 (b)). Compared to word embed-
dings, the encoder hidden states contain more con-
textual information, beneﬁting both the training
and decoding procedures. The decoder produces a
sequence of decoder hidden states hi, one for each
decoding step.

3.4 Higher-order Information

As mentioned before, our parser is capable of uti-
lizing higher-order information. In this paper, we
incorporate two kinds of higher-order structures
— grandparent and sibling. A sibling structure
is a head word with two successive modiﬁers, and
a grandparent structure is a pair of dependencies
connected head-to-tail:

To utilize higher-order information,

the de-
coder’s input at each step is the sum of the encoder
hidden states of three words:

βt = sh + sg + ss

where βt is the input vector of decoder at time
t and h, g, s are the indices of the head word
and its grandparent and sibling, respectively. Fig-
ure 1 (b) illustrates the details. Here we use the
element-wise sum operation instead of concatena-
tion because it does not increase the dimension of
the input vector βt, thus introducing no additional
model parameters.

3.5 Biafﬁne Attention Mechanism

For attention score function (Eq. (1)), we adopt the
biafﬁne attention mechanism (Luong et al., 2015;
Dozat and Manning, 2017):

i = hT
et

t Wsi + UT ht + VT si + b

where W, U, V, b are parameters, denoting the
weight matrix of the bi-linear term, the two weight
vectors of the linear terms, and the bias vector.

As discussed in Dozat and Manning (2017), ap-
plying a multilayer perceptron (MLP) to the out-
put vectors of the BLSTM before the score func-
tion can both reduce the dimensionality and over-
ﬁtting of the model. We follow this work by using
a one-layer perceptron to si and hi with elu (Clev-
ert et al., 2015) as its activation function.

Similarly, the dependency label classiﬁer also
uses a biafﬁne function to score each label, given
the head word vector ht and child vector si as in-
puts. Again, we use MLPs to transform ht and si
before feeding them into the classiﬁer.

3.6 Training Objectives

The STACKPTR parser is trained to optimize the
probability of the dependency trees given sen-
tences: Pθ(y|x), which can be factorized as:

Pθ(y|x) =

Pθ(pi|p<i, x)

=

Pθ(ci,j|ci,<j, p<i, x),

(2)

k
(cid:81)
i=1
k
(cid:81)
i=1

li(cid:81)
j=1

where θ represents model parameters. p<i denotes
the preceding paths that have already been gener-
ated. ci,j represents the jth word in pi and ci,<j
denotes all the proceeding words on the path pi.
Thus, the STACKPTR parser is an autoregressive
model, like sequence-to-sequence models, but it
factors the distribution according to a top-down
tree structure as opposed to a left-to-right chain.
We deﬁne Pθ(ci,j|ci,<j, p<i, x) = at, where atten-
tion vector at (of dimension n) is used as the dis-
tribution over the indices of words in a sentence.

Arc Prediction Our parser is trained by optimiz-
ing the conditional likelihood in Eq (2), which is
implemented as the cross-entropy loss.

Label Prediction We train a separated multi-
class classiﬁer in parallel to predict the depen-
dency labels.
Following Dozat and Manning
(2017), the classiﬁer takes the information of the

head word and its child as features. The label clas-
siﬁer is trained simultaneously with the parser by
optimizing the sum of their objectives.

3.7 Discussion

Time Complexity. The number of decoding
steps to build a parse tree for a sentence of length
n is 2n−1, linear in n. Together with the attention
mechanism (at each step, we need to compute the
attention vector at, whose runtime is O(n)), the
time complexity of decoding algorithm is O(n2),
which is more efﬁcient than graph-based parsers
that have O(n3) or worse complexity when using
dynamic programming or maximum spanning tree
(MST) decoding algorithms.

Top-down Parsing. When humans comprehend
a natural language sentence, they arguably do it
in an incremental,
left-to-right manner. How-
ever, when humans consciously annotate a sen-
tence with syntactic structure, they rarely ever pro-
cess in ﬁxed left-to-right order. Rather, they start
by reading the whole sentence, then seeking the
main predicates, jumping back-and-forth over the
sentence and recursively proceeding to the sub-
tree structures governed by certain head words.
Our parser follows a similar kind of annotation
process: starting from reading the whole sentence,
and processing in a top-down manner by ﬁnding
the main predicates ﬁrst and only then search for
sub-trees governed by them. When making latter
decisions, the parser has access to the entire struc-
ture built in earlier steps.

3.8

Implementation Details

Pre-trained Word Embeddings. For all
the
parsing models in different languages, we initial-
ize word vectors with pretrained word embed-
dings. For Chinese, Dutch, English, German and
Spanish, we use the structured-skipgram (Ling
et al., 2015) embeddings. For other languages we
use Polyglot embeddings (Al-Rfou et al., 2013).

Optimization. Parameter optimization is per-
formed with the Adam optimizer (Kingma and Ba,
2014) with β1 = β2 = 0.9. We choose an ini-
tial learning rate of η0 = 0.001. The learning
rate η is annealed by multiplying a ﬁxed decay
rate ρ = 0.75 when parsing performance stops in-
creasing on validation sets. To reduce the effects
of “gradient exploding”, we use gradient clipping
of 5.0 (Pascanu et al., 2013).

Dropout Training. To mitigate overﬁtting, we
apply dropout (Srivastava et al., 2014; Ma et al.,
2017). For BLSTM, we use recurrent dropout (Gal
and Ghahramani, 2016) with a drop rate of 0.33
between hidden states and 0.33 between layers.
Following Dozat and Manning (2017), we also use
embedding dropout with a rate of 0.33 on all word,
character, and POS embeddings.

Hyper-Parameters. Some parameters are cho-
sen from those reported in Dozat and Manning
(2017). We use the same hyper-parameters across
the models on different treebanks and languages,
due to time constraints. The details of the chosen
hyper-parameters for all experiments are summa-
rized in Appendix A.

4 Experiments

4.1 Setup

We evaluate our STACKPTR parser mainly on
the English Penn Treebank
three treebanks:
(PTB version 3.0) (Marcus et al., 1993),
the
Penn Chinese Treebank (CTB version 5.1) (Xue
et al., 2002), and the German CoNLL 2009 cor-
pus (Hajiˇc et al., 2009). We use the same experi-
mental settings as Kuncoro et al. (2016).

To make a thorough empirical comparison with
previous studies, we also evaluate our system on
treebanks from CoNLL shared task and the Uni-
versal Dependency (UD) Treebanks4. For the
CoNLL Treebanks, we use the English treebank
from CoNLL-2008 shared task (Surdeanu et al.,
2008) and all 13 treebanks from CoNLL-2006
shared task (Buchholz and Marsi, 2006). The ex-
perimental settings are the same as Ma and Hovy
(2015). For UD Treebanks, we select 12 lan-
guages. The details of the treebanks and experi-
mental settings are in § 4.5 and Appendix B.

Evaluation Metrics Parsing performance is
measured with ﬁve metrics: unlabeled attachment
score (UAS), labeled attachment score (LAS), un-
labeled complete match (UCM), labeled complete
match (LCM), and root accuracy (RA). Following
previous work (Kuncoro et al., 2016; Dozat and
Manning, 2017), we report results excluding punc-
tuations for Chinese and English. For each experi-
ment, we report the mean values with correspond-
ing standard deviations over 5 repetitions.

4http://universaldependencies.org/

Figure 2: Parsing performance of different variations of our model on the test sets for three languages,
together with baseline BIAF. For each of our STACKPTR models, we perform decoding with beam size
equal to 1 and 10. The improvements of decoding with beam size 10 over 1 are presented by stacked
bars with light colors.

Baseline For fair comparison of the parsing per-
formance, we re-implemented the graph-based
Deep Biafﬁne (BIAF) parser (Dozat and Manning,
2017), which achieved state-of-the-art results on a
wide range of languages. Our re-implementation
adds character-level information using the same
LSTM-CNN encoder as our model (§ 3.2) to the
original BIAF model, which boosts its perfor-
mance on all languages.

4.2 Main Results

We ﬁrst conduct experiments to demonstrate the
effectiveness of our neural architecture by compar-
ing with the strong baseline BIAF. We compare
the performance of four variations of our model
with different decoder inputs — Org, +gpar, +sib
and Full — where the Org model utilizes only the
encoder hidden states of head words, while the
+gpar and +sib models augments the original one
with grandparent and sibling information, respec-
tively. The Full model includes all the three infor-
mation as inputs.

Figure 2 illustrates the performance (ﬁve met-
rics) of different variations of our STACKPTR
parser together with the results of baseline BIAF
re-implemented by us, on the test sets of the three

languages. On UAS and LAS, the Full variation
of STACKPTR with decoding beam size 10 outper-
forms BIAF on Chinese, and obtains competitive
performance on English and German. An interest-
ing observation is that the Full model achieves the
best accuracy on English and Chinese, while per-
forms slightly worse than +sib on German. This
shows that the importance of higher-order infor-
mation varies in languages. On LCM and UCM,
STACKPTR signiﬁcantly outperforms BIAF on all
languages, showing the superiority of our parser
on complete sentence parsing. The results of our
parser on RA are slightly worse than BIAF. More
details of results are provided in Appendix C.

4.3 Comparison with Previous Work

Table 1 illustrates the UAS and LAS of the
four versions of our model (with decoding beam
size 10) on the three treebanks,
together with
previous top-performing systems for comparison.
Note that the results of STACKPTR and our re-
implementation of BIAF are the average of 5 rep-
etitions instead of a single run. Our Full model
signiﬁcantly outperforms all the transition-based
parsers on all three languages, and achieves bet-
ter results than most graph-based parsers. Our

English

Chinese

German

System
UAS
91.8
T
Chen and Manning (2014)
91.63
T
Ballesteros et al. (2015)
93.1
T
Dyer et al. (2015)
93.33
T
Bohnet and Nivre (2012)
93.56
T
Ballesteros et al. (2016)
93.9
T
Kiperwasser and Goldberg (2016)
94.26
T
Weiss et al. (2015)
T
94.61
Andor et al. (2016)
G 93.1
Kiperwasser and Goldberg (2016)
G 94.08
Wang and Chang (2016)
G 94.10
Cheng et al. (2016)
G 94.26
Kuncoro et al. (2016)
Ma and Hovy (2017)
G 94.88
BIAF: Dozat and Manning (2017) G 95.74
G 95.84
BIAF: re-impl
STACKPTR: Org
95.77
T
STACKPTR: +gpar
95.78
T
STACKPTR: +sib
95.85
T
95.87
STACKPTR: Full
T

LAS
89.6
89.44
90.9
91.22
91.42
91.9
92.41
92.79
91.0
91.82
91.49
92.06
92.98
94.08
94.21
94.12
94.12
94.18
94.19

UAS
83.9
85.30
87.2
87.3
87.65
87.6
–
–
86.6
87.55
88.1
88.87
89.05
89.30
90.43
90.48
90.49
90.43
90.59

LAS
82.4
83.72
85.7
85.9
86.21
86.1
–
–
85.1
86.23
85.7
87.30
87.74
88.23
89.14
89.19
89.19
89.15
89.29

UAS
–
88.83
–
91.4
–
–
–
90.91
–
–
–
91.60
92.58
93.46
93.85
93.59
93.65
93.76
93.65

LAS
–
86.10
–
89.4
–
–
–
89.15
–
–
–
89.24
90.54
91.44
92.32
92.06
92.12
92.21
92.11

Table 1: UAS and LAS of four versions of our model on test sets for three languages, together with top-
performing parsing systems. “T” and “G” indicate transition- and graph-based models, respectively. For
BIAF, we provide the original results reported in Dozat and Manning (2017) and our re-implementation.
For STACKPTR and our re-implementation of BiAF, we report the average over 5 runs.

(a)

(b)

(c)

Figure 3: Parsing performance of BIAF and STACKPTR parsers relative to length and graph factors.

LAS

UAS

POS
Gold 96.12±0.03 95.06±0.05 62.22±0.33 55.74±0.44
Pred 95.87±0.04 94.19±0.04 61.43±0.49 49.68±0.47
None 95.90±0.05 94.21±0.04 61.58±0.39 49.87±0.46

UCM

LCM

we follow McDonald and Nivre (2011) and report
labeled parsing metrics (either accuracy, precision,
or recall) for all experiments.

Table 2: Parsing performance on the test data of
PTB with different versions of POS tags.

re-implementation of BIAF obtains better perfor-
mance than the original one in Dozat and Man-
ning (2017), demonstrating the effectiveness of the
character-level information. Our model achieves
state-of-the-art performance on both UAS and
LAS on Chinese, and best UAS on English.
On German, the performance is competitive with
BIAF, and signiﬁcantly better than other models.

4.4 Error Analysis

In this section, we characterize the errors made by
BIAF and STACKPTR by presenting a number of
experiments that relate parsing errors to a set of
linguistic and structural properties. For simplicity,

4.4.1 Length and Graph Factors
Following McDonald and Nivre (2011), we ana-
lyze parsing errors related to structural factors.

Sentence Length. Figure 3 (a) shows the ac-
curacy of both parsing models relative to sen-
tence lengths. Consistent with the analysis in Mc-
Donald and Nivre (2011), STACKPTR tends to
perform better on shorter sentences, which make
fewer parsing decisions, signiﬁcantly reducing the
chance of error propagation.

Dependency Length. Figure 3 (b) measures
the precision and recall relative to dependency
lengths. While the graph-based BIAF parser
still performs better for longer dependency arcs
and transition-based STACKPTR parser does bet-
ter for shorter ones, the gap between the two sys-
tems is marginal, much smaller than that shown

Bi-Att
UAS [LAS]
80.34 [68.58]
93.96 [89.55]
–
91.16 [85.14]
91.56 [85.53]
87.15 [82.41]
–
92.71 [89.80]
93.44 [90.67]
92.77 [88.44]
86.01 [75.90]
88.74 [84.03]
90.50 [84.05]
78.43 [66.16]

NeuroMST
UAS [LAS]
80.80 [69.40]
94.28 [90.60]
93.40 [90.10]
91.18 [85.92]
91.86 [87.07]
87.85 [84.82]
94.66 [92.52]
93.62 [91.90]
94.02 [92.60]
92.71 [88.92]
86.73 [77.56]
89.20 [85.77]
91.22 [86.92]
77.71 [65.81]

BIAF
UAS [LAS]
82.15±0.34 [71.32±0.36]
94.62±0.14 [91.56±0.24]
94.05±0.27 [90.89±0.22]
92.24±0.22 [87.85±0.21]
92.80±0.26 [88.36±0.18]
90.07±0.18 [87.24±0.17]
95.19±0.05 [93.14±0.05]
94.52±0.11 [93.06±0.11]
93.95±0.06 [92.46±0.07]
93.41±0.08 [89.96±0.24]
87.55±0.17 [78.52±0.35]
90.43±0.13 [87.08±0.14]
92.22±0.15 [88.44±0.17]
79.84±0.23 [68.63±0.29]

STACKPTR
UAS [LAS]
83.04±0.29 [72.94±0.31]
94.66±0.10 [91.40±0.08]
93.88±0.24 [90.81±0.55]
92.83±0.13 [88.75±0.16]
92.08±0.15 [87.29±0.21]
90.10±0.27 [87.05±0.26]
93.25±0.05 [93.17±0.05]
94.77±0.05 [93.21±0.10]
93.38±0.08 [91.92±0.16]
93.57±0.12 [90.07±0.20]
87.59±0.36 [78.85±0.53]
90.87±0.26 [87.80±0.31]
92.49±0.21 [89.01±0.22]
79.56±0.22 [68.03±0.15]

ar
bg
zh
cs
da
nl
en
de
ja
pt
sl
es
sv
tr

Best Published
LAS
UAS
–
81.12
–
94.02
–
93.04
85.14
91.16
–
92.00
–
87.39
–
93.25
89.80
92.71
–
93.80
–
93.03
–
87.06
84.03
88.75
85.26
91.85
66.16
78.43

Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art
parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the
neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of
UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al.
(2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016).

in McDonald and Nivre (2011). One possible
reason is that, unlike traditional transition-based
parsers that scan the sentence from left to right,
STACKPTR processes in a top-down manner, thus
sometimes unnecessarily creating shorter depen-
dency arcs ﬁrst.

Root Distance. Figure 3 (c) plots the precision
and recall of each system for arcs of varying dis-
tance to the root. Different from the observation
in McDonald and Nivre (2011), STACKPTR does
not show an obvious advantage on the precision
for arcs further away from the root. Furthermore,
the STACKPTR parser does not have the tendency
to over-predict root modiﬁers reported in McDon-
ald and Nivre (2011). This behavior can be ex-
plained using the same reasoning as above:
the
fact that arcs further away from the root are usu-
ally constructed early in the parsing algorithm of
traditional transition-based parsers is not true for
the STACKPTR parser.

4.4.2 Effect of POS Embedding

The only prerequisite information that our pars-
ing model relies on is POS tags. With the goal of
achieving an end-to-end parser, we explore the ef-
fect of POS tags on parsing performance. We run
experiments on PTB using our STACKPTR parser
with gold-standard and predicted POS tags, and
without tags, respectively. STACKPTR in these ex-
periments is the Full model with beam=10.

Table 2 gives results of the parsers with differ-
ent versions of POS tags on the test data of PTB.

The parser with gold-standard POS tags signiﬁ-
cantly outperforms the other two parsers, show-
ing that dependency parsers can still beneﬁt from
accurate POS information. The parser with pre-
dicted (imperfect) POS tags, however, performs
even slightly worse than the parser without us-
ing POS tags.
It illustrates that an end-to-end
parser that doesn’t rely on POS information can
obtain competitive (or even better) performance
than parsers using imperfect predicted POS tags,
even if the POS tagger is relative high accuracy
(accuracy > 97% in this experiment on PTB).

4.5 Experiments on Other Treebanks

4.5.1 CoNLL Treebanks

Table 3 summarizes the parsing results of our
model on the test sets of 14 treebanks from the
CoNLL shared task, along with the state-of-the-
art baselines. Along with BIAF, we also list the
performance of the bi-directional attention based
Parser (Bi-Att) (Cheng et al., 2016) and the neural
MST parser (NeuroMST) (Ma and Hovy, 2017)
for comparison. Our parser achieves state-of-the-
art performance on both UAS and LAS on eight
languages — Arabic, Czech, English, German,
Portuguese, Slovene, Spanish, and Swedish. On
Bulgarian and Dutch, our parser obtains the best
UAS. On other languages, the performance of our
parser is competitive with BIAF, and signiﬁcantly
better than others. The only exception is Japanese,
on which NeuroMST obtains the best scores.

Dev

Test

BIAF

STACKPTR

BIAF

STACKPTR

UAS
93.92±0.13
94.21±0.05
94.14±0.03
91.89±0.11
92.51±0.08
93.46±0.05
95.05±0.04
94.89±0.12
93.39±0.08
95.44±0.05
91.97±0.13
93.81±0.05

LAS
89.05±0.11
91.97±0.06
90.89±0.04
88.39±0.17
90.50±0.07
91.13±0.07
92.76±0.07
92.58±0.12
90.90±0.07
93.73±0.05
85.38±0.03
91.85±0.06

UAS
94.09±0.16
94.47±0.02
94.33±0.04
92.26±0.11
92.47±0.03
93.54±0.06
94.97±0.04
94.93±0.09
93.94±0.11
95.52±0.08
92.06±0.08
94.11±0.07

LAS
89.17±0.14
92.51±0.05
91.24±0.05
88.79±0.15
90.46±0.02
91.34±0.05
92.57±0.06
92.90±0.10
91.67±0.08
93.80±0.08
85.58±0.12
92.29±0.10

bg
ca
cs
de
en
es
fr
it
nl
no
ro
ru

UAS
94.30±0.16
94.36±0.06
94.06±0.04
90.26±0.19
91.91±0.17
93.72±0.07
92.62±0.15
94.75±0.12
93.44±0.09
95.28±0.05
91.94±0.07
94.40±0.03

LAS
90.04±0.16
92.05±0.07
90.60±0.05
86.11±0.25
89.82±0.16
91.33±0.08
89.51±0.14
92.72±0.12
91.04±0.06
93.58±0.05
85.61±0.13
92.68±0.04

UAS
94.31±0.06
94.47±0.02
94.21±0.06
90.26±0.07
91.93±0.07
93.77±0.07
92.90±0.20
94.70±0.07
93.98±0.05
95.33±0.03
91.80±0.11
94.69±0.04

LAS
89.96±0.07
92.39±0.02
90.94±0.07
86.16±0.01
89.83±0.06
91.52±0.07
89.88±0.23
92.55±0.09
91.73±0.07
93.62±0.03
85.34±0.21
93.07±0.03

Table 4: UAS and LAS on both the development and test datasets of 12 treebanks from UD Treebanks,
together with BIAF for comparison.

the

results of

4.5.2 UD Treebanks
For UD Treebanks, we select 12 languages — Bul-
garian, Catalan, Czech, Dutch, English, French,
German, Italian, Norwegian, Romanian, Russian
and Spanish. For all the languages, we adopt the
standard training/dev/test splits, and use the uni-
versal POS tags (Petrov et al., 2012) provided in
each treebank. The statistics of these corpora are
provided in Appendix B.
Table 4 summarizes

the
STACKPTR parser, along with BIAF for compari-
son, on both the development and test datasets for
each language. First, both BIAF and STACKPTR
parsers achieve relatively high parsing accuracies
on all the 12 languages — all with UAS are higher
than 90%. On nine languages — Catalan, Czech,
Dutch, English, French, German, Norwegian,
Russian and Spanish — STACKPTR outperforms
BIAF for both UAS and LAS. On Bulgarian,
STACKPTR achieves slightly better UAS while
LAS is slightly worse than BIAF. On Italian
and Romanian, BIAF obtains marginally better
parsing performance than STACKPTR.

5 Conclusion

a
In this paper, we proposed STACKPTR,
transition-based neural network architecture, for
dependency parsing. Combining pointer networks
with an internal stack to track the status of the
top-down, depth-ﬁrst search in the decoding pro-
cedure, the STACKPTR parser is able to capture
information from the whole sentence and all the
previously derived subtrees, removing the left-
transition-based
to-right restriction in classical
parsers, while maintaining linear parsing steps,
w.r.t the length of the sentences. Experimental re-

sults on 29 treebanks show the effectiveness of our
parser across 20 languages, by achieving state-of-
the-art performance on 21 corpora.

There are several potential directions for future
work. First, we intend to consider how to conduct
experiments to improve the analysis of parsing er-
rors qualitatively and quantitatively. Another in-
teresting direction is to further improve our model
by exploring reinforcement learning approaches to
learn an optimal order for the children of head
words, instead of using a predeﬁned ﬁxed order.

Acknowledgements

The authors thank Chunting Zhou, Di Wang and
Zhengzhong Liu for their helpful discussions.
This research was supported in part by DARPA
grant FA8750-18-2-0018 funded under the AIDA
program. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect
the views of DARPA.

References

Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
In Proceedings of CoNLL-
for multilingual nlp.
2013. Soﬁa, Bulgaria, pages 183–192.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
In Pro-
malized transition-based neural networks.
ceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 2442–2452.

Gabor Angeli, Melvin Jose Johnson Premkumar, and
Christopher D. Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.

In Proceedings of ACL-2015 (Volume 1: Long Pa-
pers). Beijing, China, pages 344–354.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In Proceedings of
learning to align and translate.
ICLR-2015.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of EMNLP-2015. Lisbon, Portugal, pages
349–359.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and
Noah A. Smith. 2016. Training with exploration im-
proves a greedy stack lstm parser. In Proceedings of
EMNLP-2016. Austin, Texas, pages 2005–2010.

Joost Bastings,

Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Simaan. 2017. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. In Proceedings of EMNLP-2017.
Copenhagen, Denmark, pages 1957–1967.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In Pro-
ceedings of EMNLP-2012. Jeju Island, Korea, pages
1455–1465.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceeding of CoNLL-2006. New York, NY, pages
149–164.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
In Proceedings of EMNLP-2014. Doha,
works.
Qatar, pages 740–750.

Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao,
and Li Deng. 2016. Bi-directional attention with
agreement for dependency parsing. In Proceedings
of EMNLP-2016. Austin, Texas, pages 2204–2214.

Jason Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics 4:357–370.

Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp
Hochreiter. 2015. Fast and accurate deep network
learning by exponential linear units (elus). arXiv
preprint arXiv:1511.07289 .

Timothy Dozat and Christopher D. Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. In Proceedings of ICLR-2017 (Volume 1: Long
Papers). Toulon, France.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of ACL-2015 (Volume
1: Long Papers). Beijing, China, pages 334–343.

Jason M Eisner. 1996. Three new probabilistic models
In Pro-
for dependency parsing: An exploration.
ceedings of COLING-1996 (Volume 1). Association
for Computational Linguistics, pages 340–345.

Nicolas R Fauceglia, Yiu-Chang Lin, Xuezhe Ma, and
Eduard Hovy. 2015. Word sense disambiguation via
propstore and ontonotes for event mention detec-
In Proceedings of the The 3rd Workshop on
tion.
EVENTS: Deﬁnition, Detection, Coreference, and
Representation. Denver, Colorado, pages 11–15.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems.

Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
In Proceedings of CoNLL-
in multiple languages.
2009: Shared Task. pages 1–18.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
Transactions
tional lstm feature representations.
of the Association for Computational Linguistics
4:313–327.

Terry Koo and Michael Collins. 2010. Efﬁcient third-
order dependency parsers. In Proceedings of ACL-
2010. Uppsala, Sweden, pages 1–11.

Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
In Proceedings of EMNLP-2010. Cam-
automata.
bridge, MA, pages 1288–1298.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A. Smith. 2016. Dis-
tilling an ensemble of greedy dependency parsers
In Proceedings of EMNLP-
into one mst parser.
2016. Austin, Texas, pages 1744–1753.

Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of ACL-
2014 (Volume 1: Long Papers). Baltimore, Mary-
land, pages 1381–1391.

Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of EMNLP-2013. Seattle, Washington, USA,
pages 1971–1982.

Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
In Proceedings of
word2vec for syntax problems.
NAACL-2015. Denver, Colorado, pages 1299–1304.

Thang Luong, Hieu Pham, and Christopher D. Man-
Effective approaches to attention-
ning. 2015.
based neural machine translation. In Proceedings of
EMNLP-2015. Lisbon, Portugal, pages 1412–1421.

Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu,
Yuntian Deng, and Eduard Hovy. 2017. Dropout
with expectation-linear regularization. In Proceed-
ings of the 5th International Conference on Learn-
ing Representations (ICLR-2017). Toulon, France.

Xuezhe Ma and Eduard Hovy. 2015. Efﬁcient inner-to-
outer greedy algorithm for higher-order labeled de-
pendency parsing. In Proceedings of EMNLP-2015.
Lisbon, Portugal, pages 1322–1328.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 1064–1074.

Xuezhe Ma and Eduard Hovy. 2017. Neural proba-
bilistic model for non-projective mst parsing.
In
Proceedings of IJCNLP-2017 (Volume 1: Long Pa-
pers). Taipei, Taiwan, pages 59–69.

Xuezhe Ma, Zhengzhong Liu, and Eduard Hovy. 2016.
Unsupervised ranking model for entity coreference
In Proceedings of NAACL-2016. San
resolution.
Diego, California, USA.

Xuezhe Ma and Fei Xia. 2014. Unsupervised depen-
dency parsing with transferring distribution via par-
In Pro-
allel guidance and entropy regularization.
ceedings of ACL-2014. Baltimore, Maryland, pages
1337–1348.

Xuezhe Ma and Hai Zhao. 2012a. Fourth-order depen-
In Proceedings of COLING 2012:

dency parsing.
Posters. Mumbai, India, pages 785–796.

Xuezhe Ma and Hai Zhao. 2012b. Probabilistic models
for high-order projective dependency parsing. Tech-
nical Report, arXiv:1502.04174 .

Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics 19(2):313–330.

Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
In Proceedings of ACL-
projective turbo parsers.
2013 (Volume 2: Short Papers). Soﬁa, Bulgaria,
pages 617–622.

Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011. Dual decomposition with
In Proceedings
many overlapping components.
of EMNLP-2011. Edinburgh, Scotland, UK., pages
238–249.

Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics 37(1):197–230.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria
Bertomeu Castell´o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of ACL-2013. Soﬁa, Bulgaria,
pages 92–97.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceeding of EACL-2006.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP-2005. Vancouver, Canada, pages
523–530.

Vincent Ng. 2010. Supervised noun phrase coreference
research: The ﬁrst ﬁfteen years. In Proceedings of
ACL-2010. Association for Computational Linguis-
tics, Uppsala, Sweden, pages 1396–1411.

Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In Proceedings of EMNLP-
2009. Singapore, pages 1378–1387.

Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan
McDonald, Jens Nilsson, Sebastian Riedel, and
The CoNLL 2007 shared
Deniz Yuret. 2007.
In Proceedings of
task on dependency parsing.
the CoNLL Shared Task Session of EMNLP-CoNLL
2007. Prague, Czech Republic, pages 915–932.

Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING-2004. Geneva, Switzerland, pages 64–
70.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difﬁculty of training recurrent neu-
ral networks. In Proceedings of ICML-2013. pages
1310–1318.

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph lstms. Transac-
tions of the Association for Computational Linguis-
tics 5:101–115.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC-2012. Istanbul, Turkey, pages 2089–2096.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
In Proceedings of ACL-2005.
pendency parsers.
Ann Arbor, Michigan, USA, pages 91–98.

Emily Pitler and Ryan McDonald. 2015. A linear-time
transition system for crossing interval trees. In Pro-
ceedings of NAACL-2015. Denver, Colorado, pages
662–671.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research 15(1):1929–1958.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of CoNLL-
2008. pages 159–177.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings ACL-2015 (Volume 1: Long
Papers). Beijing, China, pages 1556–1566.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems. pages 2692–2700.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In Pro-
ceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 2306–2315.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural net-
In Proceedings of
work transition-based parsing.
ACL-2015 (Volume 1: Long Papers). Beijing, China,
pages 323–333.

Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of COLING-2002. pages 1–8.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
In Proceedings of IWPT. Nancy, France,
chines.
volume 3, pages 195–206.

Hao Zhang and Ryan McDonald. 2014. Enforcing
structural diversity in cube-pruned dependency pars-
ing. In Proceedings of ACL-2014 (Volume 2: Short
Papers). Baltimore, Maryland, pages 656–661.

Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2014. Greed is good if randomized: New
inference for dependency parsing. In Proceedings of
EMNLP-2014. Doha, Qatar, pages 1013–1024.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Portland, Oregon, USA, pages
188–193.

Appendix: Stack-Pointer Network for Dependency Parsing

Appendix A: Hyper-Parameters

Table 5 summarizes the chosen hyper-parameters used for all the experiments in this paper. Some pa-
rameters are chosen directly or similarly from those reported in Dozat and Manning (2017). We use the
same hyper-parameters across the models on different treebanks and languages, due to time constraints.

Layer

CNN

LSTM

MLP

Dropout

Learning

Hyper-parameter
window size
number of ﬁlters
encoder layers
encoder size
decoder layers
decoder size
arc MLP size
label MLP size
embeddings
LSTM hidden states
LSTM layers
optimizer
initial learning rate
(β1, β2)
decay rate
gradient clipping

Value
3
50
3
512
1
512
512
128
0.33
0.33
0.33
Adam
0.001
(0.9, 0.9)
0.75
5.0

Table 5: Hyper-parameters for all experiments.

Appendix B: UD Treebanks

Table 6 shows the corpora statistics of the treebanks for 12 languages. For evaluation, we report results
excluding punctuation, which is any tokens with POS tags “PUNCT” or “SYM”.

Corpora

Bulgarian

BTB

Catalan

AnCora

Czech

Dutch

PDT, CAC
CLTT
FicTree
Alpino
LassySmall

English

EWT

French

GSD

German

GSD

Italian

ISDT

Norwegian

Bokmaal
Nynorsk

Romanian

RRT

Russian

SynTagRus

Spanish

GSD
AnCora

Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test

#Sent
8,907
1,115
1,116
13,123
1,709
1,846
102,993
11,311
12,203
18,310
1,518
1,396
12,543
2,002
2,077
14,554
1,478
416
13,841
799
977
12,838
564
482
29,870
4,300
3,450
8,043
752
729
48,814
6,584
6,491
28,492
4,300
2,174

#Token (w.o punct)
124,336 (106,813)
16,089 (13,822)
15,724 (13,456)
417,587 (371,981)
56,482 (50,452)
57,738 (51,324)
1,806,230 (1,542,805)
191,679 (163,387)
205,597 (174,771)
267,289 (234,104)
22,091 (19,042)
21,126 (18,310)
204,585 (180,308)
25,148 (21,998)
25,096 (21,898)
356,638 (316,780)
35,768 (31,896)
10,020 (8,795)
263,536 (229,204)
12,348 (10,727)
16,268 (13,929)
270,703 (239,836)
11,908 (10,490)
10,417 (9,237)
48,9217 (43,2597)
67,619 (59,784)
54,739 (48,588)
185,113 (161,429)
17,074 (14,851)
16,324 (14,241)
870,034 (711,184)
118,426 (95,676)
117,276 (95,745)
827,053 (730,062)
89,487 (78,951)
64,617 (56,973)

Table 6: Corpora statistics of UD Treebanks for 12 languages. #Sent and #Token refer to the number of
sentences and the number of words (w./w.o punctuations) in each data set, respectively.

Appendix C: Main Results

Table 7 illustrates the details of the experimental results. For each STACKPRT parsing model, we ran
experiments with decoding beam size equals to 1, 5, and 10. For each experiment, we report the mean
values with corresponding standard deviations over 5 runs.

Model beam
BiAF

Model beam
BiAF

Basic

+gpar

+sib

Full

Basic

+gpar

+sib

Full

Basic

+gpar

+sib

Full

Dev

Test

English

UAS

LAS

UCM

LCM

UAS

LAS

UCM

LCM

95.73±0.04 93.97±0.06 60.58±0.77 47.47±0.63 95.84±0.06 94.21±0.04 59.49±0.23 49.07±0.34
95.71±0.02 93.88±0.03 62.33±0.33 47.75±0.32 95.71±0.06 94.07±0.06 60.91±0.35 49.54±0.48
95.71±0.04 93.88±0.05 62.40±0.45 47.80±0.44 95.76±0.11 94.12±0.11 61.09±0.43 49.67±0.41
95.72±0.03 93.89±0.04 62.40±0.45 47.80±0.44 95.77±0.11 94.12±0.11 61.09±0.43 49.67±0.41
95.68±0.04 93.82±0.02 61.82±0.36 47.32±0.14 95.73±0.04 94.07±0.05 60.99±0.46 49.83±0.59
95.67±0.01 93.83±0.02 61.93±0.32 47.44±0.20 95.76±0.06 94.11±0.06 61.23±0.47 50.07±0.59
95.69±0.02 93.83±0.02 61.95±0.32 47.44±0.20 95.78±0.05 94.12±0.06 61.24±0.46 50.07±0.59
95.75±0.03 93.93±0.04 61.93±0.49 47.66±0.48 95.77±0.15 94.11±0.06 61.32±0.37 49.75±0.29
95.74±0.02 93.93±0.05 62.16±0.22 47.68±0.54 95.84±0.09 94.17±0.09 61.52±0.57 49.91±0.76
95.75±0.02 93.94±0.06 62.17±0.20 47.68±0.54 95.85±0.10 94.18±0.09 61.52±0.57 49.91±0.76
95.63±0.08 93.78±0.08 61.56±0.63 47.12±0.36 95.79±0.06 94.11±0.06 61.02±0.31 49.45±0.23
95.75±0.06 93.90±0.08 62.06±0.42 47.43±0.36 95.87±0.04 94.20±0.03 61.43±0.49 49.68±0.47
95.75±0.06 93.90±0.08 62.08±0.39 47.43±0.36 95.87±0.04 94.19±0.04 61.43±0.49 49.68±0.47
Chinese

Dev

Test

UAS

LAS

UCM

LCM

UAS

LAS

UCM

LCM

90.20±0.17 88.94±0.13 43.41±0.83 38.42±0.79 90.43±0.08 89.14±0.09 42.92±0.29 38.68±0.25
89.76±0.32 88.44±0.28 45.18±0.80 40.13±0.63 90.04±0.32 88.74±0.40 45.00±0.47 40.12±0.42
89.97±0.13 88.67±0.14 45.33±0.58 40.25±0.65 90.46±0.15 89.17±0.18 45.41±0.48 40.53±0.48
89.97±0.14 88.68±0.14 45.33±0.58 40.25±0.65 90.48±0.11 89.19±0.15 45.44±0.44 40.56±0.43
90.05±0.14 88.71±0.16 45.63±0.52 40.45±0.61 90.28±0.10 88.96±0.10 45.26±0.59 40.38±0.43
90.17±0.14 88.85±0.13 46.03±0.53 40.69±0.55 90.45±0.15 89.14±0.14 45.71±0.46 40.80±0.26
90.18±0.16 88.87±0.14 46.05±0.58 40.69±0.55 90.46±0.16 89.16±0.15 45.71±0.46 40.80±0.26
89.91±0.07 88.59±0.10 45.50±0.50 40.40±0.48 90.25±0.10 88.94±0.12 45.42±0.52 40.54±0.69
89.99±0.05 88.70±0.09 45.55±0.36 40.37±0.14 90.41±0.07 89.12±0.07 45.76±0.46 40.69±0.52
90.00±0.04 88.72±0.09 45.58±0.32 40.37±0.14 90.43±0.09 89.15±0.10 45.75±0.44 40.68±0.50
90.21±0.15 88.85±0.15 45.83±0.52 40.54±0.60 90.36±0.16 89.05±0.15 45.60±0.33 40.73±0.23
90.23±0.13 88.89±0.14 46.00±0.54 40.75±0.64 90.58±0.12 89.27±0.11 46.20±0.26 41.25±0.22
90.29±0.13 88.95±0.13 46.03±0.54 40.75±0.64 90.59±0.12 89.29±0.11 46.20±0.26 41.25±0.22
German

Dev

Test

UAS

LAS

UCM

LCM

UAS

LAS

UCM

LCM

93.60±0.13 91.96±0.13 58.79±0.25 49.59±0.19 93.85±0.07 92.32±0.06 60.60±0.38 52.46±0.24
93.35±0.14 91.58±0.17 59.64±0.78 49.75±0.64 93.39±0.09 91.85±0.09 61.08±0.31 52.21±0.53
93.49±0.14 91.72±0.16 59.99±0.69 49.82±0.54 93.61±0.09 92.07±0.08 61.38±0.30 52.51±0.43
93.48±0.14 91.71±0.17 60.02±0.69 49.84±0.54 93.59±0.09 92.06±0.08 61.38±0.30 52.51±0.43
93.39±0.07 91.66±0.13 59.59±0.54 49.81±0.42 93.44±0.07 91.91±0.11 61.73±0.47 52.84±0.48
93.47±0.09 91.75±0.10 59.81±0.55 50.05±0.39 93.68±0.04 92.16±0.04 62.09±0.44 53.13±0.42
93.48±0.08 91.76±0.09 59.89±0.59 50.09±0.40 93.68±0.05 92.16±0.03 62.10±0.42 53.14±0.4
93.43±0.07 91.73±0.08 59.68±0.25 49.93±0.30 93.55±0.07 92.00±0.08 61.90±0.50 52.79±0.22
93.53±0.05 91.83±0.07 59.95±0.23 50.14±0.39 93.75±0.09 92.20±0.08 62.21±0.38 53.03±0.18
93.55±0.06 91.84±0.07 59.96±0.24 50.15±0.40 93.76±0.09 92.21±0.08 62.21±0.38 53.03±0.18
93.33±0.13 91.60±0.16 59.78±0.32 49.78±0.29 93.50±0.04 91.91±0.11 61.80±0.28 52.95±0.37
93.42±0.11 91.69±0.12 59.90±0.27 49.94±0.35 93.64±0.03 92.10±0.06 61.89±0.21 53.06±0.36
93.40±0.11 91.67±0.12 59.90±0.27 49.94±0.35 93.64±0.03 92.11±0.05 61.89±0.21 53.06±0.36

–
1
5
10
1
5
10
1
5
10
1
5
10

–
1
5
10
1
5
10
1
5
10
1
5
10

–
1
5
10
1
5
10
1
5
10
1
5
10

Model beam
BiAF

Table 7: Parsing performance of different variations of our model on both the development and test sets
for three languages, together with the BIAF parser as the baseline. Best results are highlighted with bold
print.

Stack-Pointer Networks for Dependency Parsing

Xuezhe Ma
Carnegie Mellon University
xuezhem@cs.cmu.edu

Zecong Hu∗
Tsinghua University
huzecong@gmail.com

Jingzhou Liu
Carnegie Mellon University
liujingzhou@cs.cmu.edu

Nanyun Peng
University of Southern California
npeng@isi.edu

Graham Neubig and Eduard Hovy
Carnegie Mellon University
{gneubig, ehovy}@cs.cmu.edu

Abstract

We introduce a novel architecture for de-
pendency parsing: stack-pointer networks
(STACKPTR). Combining pointer net-
works (Vinyals et al., 2015) with an in-
ternal stack,
the proposed model ﬁrst
reads and encodes the whole sentence,
then builds the dependency tree top-down
(from root-to-leaf) in a depth-ﬁrst fashion.
The stack tracks the status of the depth-
ﬁrst search and the pointer networks se-
lect one child for the word at the top of
the stack at each step. The STACKPTR
parser beneﬁts from the information of the
whole sentence and all previously derived
subtree structures, and removes the left-
to-right restriction in classical transition-
based parsers. Yet, the number of steps for
building any (including non-projective)
parse tree is linear in the length of the sen-
tence just as other transition-based parsers,
yielding an efﬁcient decoding algorithm
with O(n2) time complexity. We evalu-
ate our model on 29 treebanks spanning 20
languages and different dependency anno-
tation schemas, and achieve state-of-the-
art performance on 21 of them.

1

Introduction

Dependency parsing, which predicts the existence
and type of linguistic dependency relations be-
tween words, is a ﬁrst step towards deep language
Its importance is widely recog-
understanding.
nized in the natural language processing (NLP)
community, with it beneﬁting a wide range of
NLP applications, such as coreference resolu-
tion (Ng, 2010; Durrett and Klein, 2013; Ma et al.,

∗Work done while at Carnegie Mellon University.

2016), sentiment analysis (Tai et al., 2015), ma-
chine translation (Bastings et al., 2017), informa-
tion extraction (Nguyen et al., 2009; Angeli et al.,
2015; Peng et al., 2017), word sense disambigua-
tion (Fauceglia et al., 2015), and low-resource lan-
guages processing (McDonald et al., 2013; Ma and
Xia, 2014). There are two dominant approaches to
dependency parsing (Buchholz and Marsi, 2006;
local and greedy transition-
Nivre et al., 2007):
based algorithms (Yamada and Matsumoto, 2003;
Nivre and Scholz, 2004; Zhang and Nivre, 2011;
Chen and Manning, 2014), and the globally opti-
mized graph-based algorithms (Eisner, 1996; Mc-
Donald et al., 2005a,b; Koo and Collins, 2010).

parsers

dependency

Transition-based

read
words sequentially (commonly from left-to-right)
and build dependency trees incrementally by
making series of multiple choice decisions. The
advantage of this formalism is that the number of
operations required to build any projective parse
tree is linear with respect to the length of the sen-
tence. The challenge, however, is that the decision
made at each step is based on local information,
leading to error propagation and worse perfor-
mance compared to graph-based parsers on root
and long dependencies (McDonald and Nivre,
2011). Previous studies have explored solutions
to address this challenge. Stack LSTMs (Dyer
et al., 2015; Ballesteros et al., 2015, 2016) are
capable of learning representations of the parser
state that are sensitive to the complete contents of
the parser’s state. Andor et al. (2016) proposed a
globally normalized transition model to replace
the locally normalized classiﬁer. However, the
parsing accuracy is still behind state-of-the-art
graph-based parsers (Dozat and Manning, 2017).
Graph-based dependency parsers, on the other
hand, learn scoring functions for parse trees and
perform exhaustive search over all possible trees
for a sentence to ﬁnd the globally highest scoring

8
1
0
2
 
y
a
M
 
3
 
 
]
L
C
.
s
c
[
 
 
1
v
7
8
0
1
0
.
5
0
8
1
:
v
i
X
r
a

tree.
Incorporating this global search algorithm
with distributed representations learned from neu-
ral networks, neural graph-based parsers (Kiper-
wasser and Goldberg, 2016; Wang and Chang,
2016; Kuncoro et al., 2016; Dozat and Manning,
2017) have achieved the state-of-the-art accura-
cies on a number of treebanks in different lan-
guages. Nevertheless, these models, while accu-
rate, are usually slow (e.g. decoding is O(n3)
time complexity for ﬁrst-order models (McDonald
et al., 2005a,b) and higher polynomials for higher-
order models (McDonald and Pereira, 2006; Koo
and Collins, 2010; Ma and Zhao, 2012b,a)).

In this paper, we propose a novel neural net-
work architecture for dependency parsing, stack-
pointer networks (STACKPTR). STACKPTR is
a transition-based architecture, with the corre-
sponding asymptotic efﬁciency, but still main-
tains a global view of the sentence that proves es-
sential for achieving competitive accuracy. Our
STACKPTR parser has a pointer network (Vinyals
et al., 2015) as its backbone, and is equipped
with an internal stack to maintain the order of
head words in tree structures. The STACKPTR
parser performs parsing in an incremental, top-
down, depth-ﬁrst fashion; at each step, it gener-
ates an arc by assigning a child for the head word
at the top of the internal stack. This architecture
makes it possible to capture information from the
whole sentence and all the previously derived sub-
trees, while maintaining a number of parsing steps
linear in the sentence length.

We evaluate our parser on 29 treebanks across
20 languages and different dependency annotation
schemas, and achieve state-of-the-art performance
on 21 of them. The contributions of this work are
summarized as follows:

(i) We propose a neural network architecture for
dependency parsing that is simple, effective,
and efﬁcient.

(ii) Empirical evaluations on benchmark datasets
over 20 languages show that our method
achieves state-of-the-art performance on 21
different treebanks1.

(iii) Comprehensive error analysis is conducted
to compare the proposed method to a strong
graph-based baseline using biafﬁne atten-
tion (Dozat and Manning, 2017).

2 Background

We ﬁrst brieﬂy describe the task of dependency
parsing, setup the notation, and review Pointer
Networks (Vinyals et al., 2015).

2.1 Dependency Parsing and Notations

Dependency trees represent syntactic relationships
between words in the sentences through labeled
directed edges between head words and their de-
pendents. Figure 1 (a) shows a dependency tree
for the sentence, “But there were no buyers”.

In this paper, we will use the following notation:
Input: x = {w1, . . . , wn} represents a generic

sentence, where wi is the ith word.

Output: y = {p1, p2, · · · , pk} represents a
generic (possibly non-projective) dependency tree,
where each path pi = $, wi,1, wi,2, · · · , wi,li is a
sequence of words from the root to a leaf. “$” is
an universal virtual root that is added to each tree.
Stack: σ denotes a stack conﬁguration, which
is a sequence of words. We use σ|w to represent
a stack conﬁguration that pushes word w into the
stack σ.

Children: ch(wi) denotes the list of all the chil-

dren (modiﬁers) of word wi.

2.2 Pointer Networks

Pointer Networks (PTR-NET) (Vinyals et al.,
2015) are a variety of neural network capable of
learning the conditional probability of an output
sequence with elements that are discrete tokens
corresponding to positions in an input sequence.
This model cannot be trivially expressed by stan-
dard sequence-to-sequence networks (Sutskever
et al., 2014) due to the variable number of input
positions in each sentence. PTR-NET solves the
problem by using attention (Bahdanau et al., 2015;
Luong et al., 2015) as a pointer to select a member
of the input sequence as the output.

Formally, the words of the sentence x are fed
one-by-one into the encoder (a multiple-layer bi-
directional RNN), producing a sequence of en-
coder hidden states si. At each time step t, the
decoder (a uni-directional RNN) receives the input
from last step and outputs decoder hidden state ht.
The attention vector at is calculated as follows:

et
i = score(ht, si)
at = softmax (et)

(1)

1Source code is publicly available at https://

github.com/XuezheMax/NeuroNLP2

where score(·, ·) is the attention scoring function,
which has several variations such as dot-product,

(a)

(b)

Figure 1: Neural architecture for the STACKPTR network, together with the decoding procedure of an
example sentence. The BiRNN of the encoder is elided for brevity. For the inputs of decoder at each
time step, vectors in red and blue boxes indicate the sibling and grandparent.

concatenation, and biafﬁne (Luong et al., 2015).
PTR-NET regards the attention vector at as a prob-
it
ability distribution over the source words, i.e.
uses at
i as pointers to select the input elements.

3 Stack-Pointer Networks

3.1 Overview

Similarly to PTR-NET, STACKPTR ﬁrst reads the
whole sentence and encodes each word into the
encoder hidden state si. The internal stack σ is
always initialized with the root symbol $. At each
time step t, the decoder receives the input vector
corresponding to the top element of the stack σ
(the head word wp where p is the word index), gen-
erates the hidden state ht, and computes the atten-
tion vector at using Eq. (1). The parser chooses a
speciﬁc position c according to the attention scores
in at to generate a new dependency arc (wh, wc)
by selecting wc as a child of wh. Then the parser
pushes wc onto the stack, i.e. σ → σ|wc, and goes
to the next step. At one step if the parser points wh
to itself, i.e. c = h, it indicates that all children
of the head word wh have already been selected.
Then the parser goes to the next step by popping
wh out of σ.

At test time, in order to guarantee a valid de-
pendency tree containing all the words in the in-
put sentences exactly once, the decoder maintains
a list of “available” words. At each decoding step,
the parser selects a child for the current head word,

and removes the child from the list of available
words to make sure that it cannot be selected as a
child of other head words.

For head words with multiple children, it is pos-
sible that there is more than one valid selection
for each time step. In order to deﬁne a determin-
istic decoding process to make sure that there is
only one ground-truth choice at each step (which
is necessary for simple maximum likelihood esti-
mation), a predeﬁned order for each ch(wi) needs
to be introduced. The predeﬁned order of chil-
dren can have different alternatives, such as left-
to-right or inside-out2.
In this paper, we adopt
the inside-out order3 since it enables us to utilize
second-order sibling information, which has been
proven beneﬁcial for parsing performance (Mc-
Donald and Pereira, 2006; Koo and Collins, 2010)
(see § 3.4 for details). Figure 1 (b) depicts the ar-
chitecture of STACKPTR and the decoding proce-
dure for the example sentence in Figure 1 (a).

3.2 Encoder

The encoder of our parsing model is based on the
bi-directional LSTM-CNN architecture (BLSTM-
CNNs) (Chiu and Nichols, 2016; Ma and Hovy,
2016) where CNNs encode character-level infor-
mation of a word into its character-level repre-

2Order the children by the distances to the head word on

the left side, then the right side.

3We also tried left-to-right order which obtained worse

parsing accuracy than inside-out.

sentation and BLSTM models context informa-
tion of each word. Formally, for each word, the
CNN, with character embeddings as inputs, en-
codes the character-level representation. Then the
character-level representation vector is concate-
nated with the word embedding vector to feed into
the BLSTM network. To enrich word-level infor-
mation, we also use POS embeddings. Finally, the
encoder outputs a sequence of hidden states si.

3.3 Decoder

The decoder for our parser is a uni-directional
LSTM. Different from previous work (Bahdanau
et al., 2015; Vinyals et al., 2015) which uses word
embeddings of the previous word as the input to
the decoder, our decoder receives the encoder hid-
den state vector (si) of the top element in the stack
σ (see Figure 1 (b)). Compared to word embed-
dings, the encoder hidden states contain more con-
textual information, beneﬁting both the training
and decoding procedures. The decoder produces a
sequence of decoder hidden states hi, one for each
decoding step.

3.4 Higher-order Information

As mentioned before, our parser is capable of uti-
lizing higher-order information. In this paper, we
incorporate two kinds of higher-order structures
— grandparent and sibling. A sibling structure
is a head word with two successive modiﬁers, and
a grandparent structure is a pair of dependencies
connected head-to-tail:

To utilize higher-order information,

the de-
coder’s input at each step is the sum of the encoder
hidden states of three words:

βt = sh + sg + ss

where βt is the input vector of decoder at time
t and h, g, s are the indices of the head word
and its grandparent and sibling, respectively. Fig-
ure 1 (b) illustrates the details. Here we use the
element-wise sum operation instead of concatena-
tion because it does not increase the dimension of
the input vector βt, thus introducing no additional
model parameters.

3.5 Biafﬁne Attention Mechanism

For attention score function (Eq. (1)), we adopt the
biafﬁne attention mechanism (Luong et al., 2015;
Dozat and Manning, 2017):

i = hT
et

t Wsi + UT ht + VT si + b

where W, U, V, b are parameters, denoting the
weight matrix of the bi-linear term, the two weight
vectors of the linear terms, and the bias vector.

As discussed in Dozat and Manning (2017), ap-
plying a multilayer perceptron (MLP) to the out-
put vectors of the BLSTM before the score func-
tion can both reduce the dimensionality and over-
ﬁtting of the model. We follow this work by using
a one-layer perceptron to si and hi with elu (Clev-
ert et al., 2015) as its activation function.

Similarly, the dependency label classiﬁer also
uses a biafﬁne function to score each label, given
the head word vector ht and child vector si as in-
puts. Again, we use MLPs to transform ht and si
before feeding them into the classiﬁer.

3.6 Training Objectives

The STACKPTR parser is trained to optimize the
probability of the dependency trees given sen-
tences: Pθ(y|x), which can be factorized as:

Pθ(y|x) =

Pθ(pi|p<i, x)

=

Pθ(ci,j|ci,<j, p<i, x),

(2)

k
(cid:81)
i=1
k
(cid:81)
i=1

li(cid:81)
j=1

where θ represents model parameters. p<i denotes
the preceding paths that have already been gener-
ated. ci,j represents the jth word in pi and ci,<j
denotes all the proceeding words on the path pi.
Thus, the STACKPTR parser is an autoregressive
model, like sequence-to-sequence models, but it
factors the distribution according to a top-down
tree structure as opposed to a left-to-right chain.
We deﬁne Pθ(ci,j|ci,<j, p<i, x) = at, where atten-
tion vector at (of dimension n) is used as the dis-
tribution over the indices of words in a sentence.

Arc Prediction Our parser is trained by optimiz-
ing the conditional likelihood in Eq (2), which is
implemented as the cross-entropy loss.

Label Prediction We train a separated multi-
class classiﬁer in parallel to predict the depen-
dency labels.
Following Dozat and Manning
(2017), the classiﬁer takes the information of the

head word and its child as features. The label clas-
siﬁer is trained simultaneously with the parser by
optimizing the sum of their objectives.

3.7 Discussion

Time Complexity. The number of decoding
steps to build a parse tree for a sentence of length
n is 2n−1, linear in n. Together with the attention
mechanism (at each step, we need to compute the
attention vector at, whose runtime is O(n)), the
time complexity of decoding algorithm is O(n2),
which is more efﬁcient than graph-based parsers
that have O(n3) or worse complexity when using
dynamic programming or maximum spanning tree
(MST) decoding algorithms.

Top-down Parsing. When humans comprehend
a natural language sentence, they arguably do it
in an incremental,
left-to-right manner. How-
ever, when humans consciously annotate a sen-
tence with syntactic structure, they rarely ever pro-
cess in ﬁxed left-to-right order. Rather, they start
by reading the whole sentence, then seeking the
main predicates, jumping back-and-forth over the
sentence and recursively proceeding to the sub-
tree structures governed by certain head words.
Our parser follows a similar kind of annotation
process: starting from reading the whole sentence,
and processing in a top-down manner by ﬁnding
the main predicates ﬁrst and only then search for
sub-trees governed by them. When making latter
decisions, the parser has access to the entire struc-
ture built in earlier steps.

3.8

Implementation Details

Pre-trained Word Embeddings. For all
the
parsing models in different languages, we initial-
ize word vectors with pretrained word embed-
dings. For Chinese, Dutch, English, German and
Spanish, we use the structured-skipgram (Ling
et al., 2015) embeddings. For other languages we
use Polyglot embeddings (Al-Rfou et al., 2013).

Optimization. Parameter optimization is per-
formed with the Adam optimizer (Kingma and Ba,
2014) with β1 = β2 = 0.9. We choose an ini-
tial learning rate of η0 = 0.001. The learning
rate η is annealed by multiplying a ﬁxed decay
rate ρ = 0.75 when parsing performance stops in-
creasing on validation sets. To reduce the effects
of “gradient exploding”, we use gradient clipping
of 5.0 (Pascanu et al., 2013).

Dropout Training. To mitigate overﬁtting, we
apply dropout (Srivastava et al., 2014; Ma et al.,
2017). For BLSTM, we use recurrent dropout (Gal
and Ghahramani, 2016) with a drop rate of 0.33
between hidden states and 0.33 between layers.
Following Dozat and Manning (2017), we also use
embedding dropout with a rate of 0.33 on all word,
character, and POS embeddings.

Hyper-Parameters. Some parameters are cho-
sen from those reported in Dozat and Manning
(2017). We use the same hyper-parameters across
the models on different treebanks and languages,
due to time constraints. The details of the chosen
hyper-parameters for all experiments are summa-
rized in Appendix A.

4 Experiments

4.1 Setup

We evaluate our STACKPTR parser mainly on
the English Penn Treebank
three treebanks:
(PTB version 3.0) (Marcus et al., 1993),
the
Penn Chinese Treebank (CTB version 5.1) (Xue
et al., 2002), and the German CoNLL 2009 cor-
pus (Hajiˇc et al., 2009). We use the same experi-
mental settings as Kuncoro et al. (2016).

To make a thorough empirical comparison with
previous studies, we also evaluate our system on
treebanks from CoNLL shared task and the Uni-
versal Dependency (UD) Treebanks4. For the
CoNLL Treebanks, we use the English treebank
from CoNLL-2008 shared task (Surdeanu et al.,
2008) and all 13 treebanks from CoNLL-2006
shared task (Buchholz and Marsi, 2006). The ex-
perimental settings are the same as Ma and Hovy
(2015). For UD Treebanks, we select 12 lan-
guages. The details of the treebanks and experi-
mental settings are in § 4.5 and Appendix B.

Evaluation Metrics Parsing performance is
measured with ﬁve metrics: unlabeled attachment
score (UAS), labeled attachment score (LAS), un-
labeled complete match (UCM), labeled complete
match (LCM), and root accuracy (RA). Following
previous work (Kuncoro et al., 2016; Dozat and
Manning, 2017), we report results excluding punc-
tuations for Chinese and English. For each experi-
ment, we report the mean values with correspond-
ing standard deviations over 5 repetitions.

4http://universaldependencies.org/

Figure 2: Parsing performance of different variations of our model on the test sets for three languages,
together with baseline BIAF. For each of our STACKPTR models, we perform decoding with beam size
equal to 1 and 10. The improvements of decoding with beam size 10 over 1 are presented by stacked
bars with light colors.

Baseline For fair comparison of the parsing per-
formance, we re-implemented the graph-based
Deep Biafﬁne (BIAF) parser (Dozat and Manning,
2017), which achieved state-of-the-art results on a
wide range of languages. Our re-implementation
adds character-level information using the same
LSTM-CNN encoder as our model (§ 3.2) to the
original BIAF model, which boosts its perfor-
mance on all languages.

4.2 Main Results

We ﬁrst conduct experiments to demonstrate the
effectiveness of our neural architecture by compar-
ing with the strong baseline BIAF. We compare
the performance of four variations of our model
with different decoder inputs — Org, +gpar, +sib
and Full — where the Org model utilizes only the
encoder hidden states of head words, while the
+gpar and +sib models augments the original one
with grandparent and sibling information, respec-
tively. The Full model includes all the three infor-
mation as inputs.

Figure 2 illustrates the performance (ﬁve met-
rics) of different variations of our STACKPTR
parser together with the results of baseline BIAF
re-implemented by us, on the test sets of the three

languages. On UAS and LAS, the Full variation
of STACKPTR with decoding beam size 10 outper-
forms BIAF on Chinese, and obtains competitive
performance on English and German. An interest-
ing observation is that the Full model achieves the
best accuracy on English and Chinese, while per-
forms slightly worse than +sib on German. This
shows that the importance of higher-order infor-
mation varies in languages. On LCM and UCM,
STACKPTR signiﬁcantly outperforms BIAF on all
languages, showing the superiority of our parser
on complete sentence parsing. The results of our
parser on RA are slightly worse than BIAF. More
details of results are provided in Appendix C.

4.3 Comparison with Previous Work

Table 1 illustrates the UAS and LAS of the
four versions of our model (with decoding beam
size 10) on the three treebanks,
together with
previous top-performing systems for comparison.
Note that the results of STACKPTR and our re-
implementation of BIAF are the average of 5 rep-
etitions instead of a single run. Our Full model
signiﬁcantly outperforms all the transition-based
parsers on all three languages, and achieves bet-
ter results than most graph-based parsers. Our

English

Chinese

German

System
UAS
91.8
T
Chen and Manning (2014)
91.63
T
Ballesteros et al. (2015)
93.1
T
Dyer et al. (2015)
93.33
T
Bohnet and Nivre (2012)
93.56
T
Ballesteros et al. (2016)
93.9
T
Kiperwasser and Goldberg (2016)
94.26
T
Weiss et al. (2015)
T
94.61
Andor et al. (2016)
G 93.1
Kiperwasser and Goldberg (2016)
G 94.08
Wang and Chang (2016)
G 94.10
Cheng et al. (2016)
G 94.26
Kuncoro et al. (2016)
Ma and Hovy (2017)
G 94.88
BIAF: Dozat and Manning (2017) G 95.74
G 95.84
BIAF: re-impl
STACKPTR: Org
95.77
T
STACKPTR: +gpar
95.78
T
STACKPTR: +sib
95.85
T
95.87
STACKPTR: Full
T

LAS
89.6
89.44
90.9
91.22
91.42
91.9
92.41
92.79
91.0
91.82
91.49
92.06
92.98
94.08
94.21
94.12
94.12
94.18
94.19

UAS
83.9
85.30
87.2
87.3
87.65
87.6
–
–
86.6
87.55
88.1
88.87
89.05
89.30
90.43
90.48
90.49
90.43
90.59

LAS
82.4
83.72
85.7
85.9
86.21
86.1
–
–
85.1
86.23
85.7
87.30
87.74
88.23
89.14
89.19
89.19
89.15
89.29

UAS
–
88.83
–
91.4
–
–
–
90.91
–
–
–
91.60
92.58
93.46
93.85
93.59
93.65
93.76
93.65

LAS
–
86.10
–
89.4
–
–
–
89.15
–
–
–
89.24
90.54
91.44
92.32
92.06
92.12
92.21
92.11

Table 1: UAS and LAS of four versions of our model on test sets for three languages, together with top-
performing parsing systems. “T” and “G” indicate transition- and graph-based models, respectively. For
BIAF, we provide the original results reported in Dozat and Manning (2017) and our re-implementation.
For STACKPTR and our re-implementation of BiAF, we report the average over 5 runs.

(a)

(b)

(c)

Figure 3: Parsing performance of BIAF and STACKPTR parsers relative to length and graph factors.

LAS

UAS

POS
Gold 96.12±0.03 95.06±0.05 62.22±0.33 55.74±0.44
Pred 95.87±0.04 94.19±0.04 61.43±0.49 49.68±0.47
None 95.90±0.05 94.21±0.04 61.58±0.39 49.87±0.46

UCM

LCM

we follow McDonald and Nivre (2011) and report
labeled parsing metrics (either accuracy, precision,
or recall) for all experiments.

Table 2: Parsing performance on the test data of
PTB with different versions of POS tags.

re-implementation of BIAF obtains better perfor-
mance than the original one in Dozat and Man-
ning (2017), demonstrating the effectiveness of the
character-level information. Our model achieves
state-of-the-art performance on both UAS and
LAS on Chinese, and best UAS on English.
On German, the performance is competitive with
BIAF, and signiﬁcantly better than other models.

4.4 Error Analysis

In this section, we characterize the errors made by
BIAF and STACKPTR by presenting a number of
experiments that relate parsing errors to a set of
linguistic and structural properties. For simplicity,

4.4.1 Length and Graph Factors
Following McDonald and Nivre (2011), we ana-
lyze parsing errors related to structural factors.

Sentence Length. Figure 3 (a) shows the ac-
curacy of both parsing models relative to sen-
tence lengths. Consistent with the analysis in Mc-
Donald and Nivre (2011), STACKPTR tends to
perform better on shorter sentences, which make
fewer parsing decisions, signiﬁcantly reducing the
chance of error propagation.

Dependency Length. Figure 3 (b) measures
the precision and recall relative to dependency
lengths. While the graph-based BIAF parser
still performs better for longer dependency arcs
and transition-based STACKPTR parser does bet-
ter for shorter ones, the gap between the two sys-
tems is marginal, much smaller than that shown

Bi-Att
UAS [LAS]
80.34 [68.58]
93.96 [89.55]
–
91.16 [85.14]
91.56 [85.53]
87.15 [82.41]
–
92.71 [89.80]
93.44 [90.67]
92.77 [88.44]
86.01 [75.90]
88.74 [84.03]
90.50 [84.05]
78.43 [66.16]

NeuroMST
UAS [LAS]
80.80 [69.40]
94.28 [90.60]
93.40 [90.10]
91.18 [85.92]
91.86 [87.07]
87.85 [84.82]
94.66 [92.52]
93.62 [91.90]
94.02 [92.60]
92.71 [88.92]
86.73 [77.56]
89.20 [85.77]
91.22 [86.92]
77.71 [65.81]

BIAF
UAS [LAS]
82.15±0.34 [71.32±0.36]
94.62±0.14 [91.56±0.24]
94.05±0.27 [90.89±0.22]
92.24±0.22 [87.85±0.21]
92.80±0.26 [88.36±0.18]
90.07±0.18 [87.24±0.17]
95.19±0.05 [93.14±0.05]
94.52±0.11 [93.06±0.11]
93.95±0.06 [92.46±0.07]
93.41±0.08 [89.96±0.24]
87.55±0.17 [78.52±0.35]
90.43±0.13 [87.08±0.14]
92.22±0.15 [88.44±0.17]
79.84±0.23 [68.63±0.29]

STACKPTR
UAS [LAS]
83.04±0.29 [72.94±0.31]
94.66±0.10 [91.40±0.08]
93.88±0.24 [90.81±0.55]
92.83±0.13 [88.75±0.16]
92.08±0.15 [87.29±0.21]
90.10±0.27 [87.05±0.26]
93.25±0.05 [93.17±0.05]
94.77±0.05 [93.21±0.10]
93.38±0.08 [91.92±0.16]
93.57±0.12 [90.07±0.20]
87.59±0.36 [78.85±0.53]
90.87±0.26 [87.80±0.31]
92.49±0.21 [89.01±0.22]
79.56±0.22 [68.03±0.15]

ar
bg
zh
cs
da
nl
en
de
ja
pt
sl
es
sv
tr

Best Published
LAS
UAS
–
81.12
–
94.02
–
93.04
85.14
91.16
–
92.00
–
87.39
–
93.25
89.80
92.71
–
93.80
–
93.03
–
87.06
84.03
88.75
85.26
91.85
66.16
78.43

Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art
parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the
neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of
UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al.
(2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016).

in McDonald and Nivre (2011). One possible
reason is that, unlike traditional transition-based
parsers that scan the sentence from left to right,
STACKPTR processes in a top-down manner, thus
sometimes unnecessarily creating shorter depen-
dency arcs ﬁrst.

Root Distance. Figure 3 (c) plots the precision
and recall of each system for arcs of varying dis-
tance to the root. Different from the observation
in McDonald and Nivre (2011), STACKPTR does
not show an obvious advantage on the precision
for arcs further away from the root. Furthermore,
the STACKPTR parser does not have the tendency
to over-predict root modiﬁers reported in McDon-
ald and Nivre (2011). This behavior can be ex-
plained using the same reasoning as above:
the
fact that arcs further away from the root are usu-
ally constructed early in the parsing algorithm of
traditional transition-based parsers is not true for
the STACKPTR parser.

4.4.2 Effect of POS Embedding

The only prerequisite information that our pars-
ing model relies on is POS tags. With the goal of
achieving an end-to-end parser, we explore the ef-
fect of POS tags on parsing performance. We run
experiments on PTB using our STACKPTR parser
with gold-standard and predicted POS tags, and
without tags, respectively. STACKPTR in these ex-
periments is the Full model with beam=10.

Table 2 gives results of the parsers with differ-
ent versions of POS tags on the test data of PTB.

The parser with gold-standard POS tags signiﬁ-
cantly outperforms the other two parsers, show-
ing that dependency parsers can still beneﬁt from
accurate POS information. The parser with pre-
dicted (imperfect) POS tags, however, performs
even slightly worse than the parser without us-
ing POS tags.
It illustrates that an end-to-end
parser that doesn’t rely on POS information can
obtain competitive (or even better) performance
than parsers using imperfect predicted POS tags,
even if the POS tagger is relative high accuracy
(accuracy > 97% in this experiment on PTB).

4.5 Experiments on Other Treebanks

4.5.1 CoNLL Treebanks

Table 3 summarizes the parsing results of our
model on the test sets of 14 treebanks from the
CoNLL shared task, along with the state-of-the-
art baselines. Along with BIAF, we also list the
performance of the bi-directional attention based
Parser (Bi-Att) (Cheng et al., 2016) and the neural
MST parser (NeuroMST) (Ma and Hovy, 2017)
for comparison. Our parser achieves state-of-the-
art performance on both UAS and LAS on eight
languages — Arabic, Czech, English, German,
Portuguese, Slovene, Spanish, and Swedish. On
Bulgarian and Dutch, our parser obtains the best
UAS. On other languages, the performance of our
parser is competitive with BIAF, and signiﬁcantly
better than others. The only exception is Japanese,
on which NeuroMST obtains the best scores.

Dev

Test

BIAF

STACKPTR

BIAF

STACKPTR

UAS
93.92±0.13
94.21±0.05
94.14±0.03
91.89±0.11
92.51±0.08
93.46±0.05
95.05±0.04
94.89±0.12
93.39±0.08
95.44±0.05
91.97±0.13
93.81±0.05

LAS
89.05±0.11
91.97±0.06
90.89±0.04
88.39±0.17
90.50±0.07
91.13±0.07
92.76±0.07
92.58±0.12
90.90±0.07
93.73±0.05
85.38±0.03
91.85±0.06

UAS
94.09±0.16
94.47±0.02
94.33±0.04
92.26±0.11
92.47±0.03
93.54±0.06
94.97±0.04
94.93±0.09
93.94±0.11
95.52±0.08
92.06±0.08
94.11±0.07

LAS
89.17±0.14
92.51±0.05
91.24±0.05
88.79±0.15
90.46±0.02
91.34±0.05
92.57±0.06
92.90±0.10
91.67±0.08
93.80±0.08
85.58±0.12
92.29±0.10

bg
ca
cs
de
en
es
fr
it
nl
no
ro
ru

UAS
94.30±0.16
94.36±0.06
94.06±0.04
90.26±0.19
91.91±0.17
93.72±0.07
92.62±0.15
94.75±0.12
93.44±0.09
95.28±0.05
91.94±0.07
94.40±0.03

LAS
90.04±0.16
92.05±0.07
90.60±0.05
86.11±0.25
89.82±0.16
91.33±0.08
89.51±0.14
92.72±0.12
91.04±0.06
93.58±0.05
85.61±0.13
92.68±0.04

UAS
94.31±0.06
94.47±0.02
94.21±0.06
90.26±0.07
91.93±0.07
93.77±0.07
92.90±0.20
94.70±0.07
93.98±0.05
95.33±0.03
91.80±0.11
94.69±0.04

LAS
89.96±0.07
92.39±0.02
90.94±0.07
86.16±0.01
89.83±0.06
91.52±0.07
89.88±0.23
92.55±0.09
91.73±0.07
93.62±0.03
85.34±0.21
93.07±0.03

Table 4: UAS and LAS on both the development and test datasets of 12 treebanks from UD Treebanks,
together with BIAF for comparison.

the

results of

4.5.2 UD Treebanks
For UD Treebanks, we select 12 languages — Bul-
garian, Catalan, Czech, Dutch, English, French,
German, Italian, Norwegian, Romanian, Russian
and Spanish. For all the languages, we adopt the
standard training/dev/test splits, and use the uni-
versal POS tags (Petrov et al., 2012) provided in
each treebank. The statistics of these corpora are
provided in Appendix B.
Table 4 summarizes

the
STACKPTR parser, along with BIAF for compari-
son, on both the development and test datasets for
each language. First, both BIAF and STACKPTR
parsers achieve relatively high parsing accuracies
on all the 12 languages — all with UAS are higher
than 90%. On nine languages — Catalan, Czech,
Dutch, English, French, German, Norwegian,
Russian and Spanish — STACKPTR outperforms
BIAF for both UAS and LAS. On Bulgarian,
STACKPTR achieves slightly better UAS while
LAS is slightly worse than BIAF. On Italian
and Romanian, BIAF obtains marginally better
parsing performance than STACKPTR.

5 Conclusion

a
In this paper, we proposed STACKPTR,
transition-based neural network architecture, for
dependency parsing. Combining pointer networks
with an internal stack to track the status of the
top-down, depth-ﬁrst search in the decoding pro-
cedure, the STACKPTR parser is able to capture
information from the whole sentence and all the
previously derived subtrees, removing the left-
transition-based
to-right restriction in classical
parsers, while maintaining linear parsing steps,
w.r.t the length of the sentences. Experimental re-

sults on 29 treebanks show the effectiveness of our
parser across 20 languages, by achieving state-of-
the-art performance on 21 corpora.

There are several potential directions for future
work. First, we intend to consider how to conduct
experiments to improve the analysis of parsing er-
rors qualitatively and quantitatively. Another in-
teresting direction is to further improve our model
by exploring reinforcement learning approaches to
learn an optimal order for the children of head
words, instead of using a predeﬁned ﬁxed order.

Acknowledgements

The authors thank Chunting Zhou, Di Wang and
Zhengzhong Liu for their helpful discussions.
This research was supported in part by DARPA
grant FA8750-18-2-0018 funded under the AIDA
program. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect
the views of DARPA.

References

Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
In Proceedings of CoNLL-
for multilingual nlp.
2013. Soﬁa, Bulgaria, pages 183–192.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
In Pro-
malized transition-based neural networks.
ceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 2442–2452.

Gabor Angeli, Melvin Jose Johnson Premkumar, and
Christopher D. Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.

In Proceedings of ACL-2015 (Volume 1: Long Pa-
pers). Beijing, China, pages 344–354.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In Proceedings of
learning to align and translate.
ICLR-2015.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of EMNLP-2015. Lisbon, Portugal, pages
349–359.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and
Noah A. Smith. 2016. Training with exploration im-
proves a greedy stack lstm parser. In Proceedings of
EMNLP-2016. Austin, Texas, pages 2005–2010.

Joost Bastings,

Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Simaan. 2017. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. In Proceedings of EMNLP-2017.
Copenhagen, Denmark, pages 1957–1967.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In Pro-
ceedings of EMNLP-2012. Jeju Island, Korea, pages
1455–1465.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceeding of CoNLL-2006. New York, NY, pages
149–164.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
In Proceedings of EMNLP-2014. Doha,
works.
Qatar, pages 740–750.

Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao,
and Li Deng. 2016. Bi-directional attention with
agreement for dependency parsing. In Proceedings
of EMNLP-2016. Austin, Texas, pages 2204–2214.

Jason Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics 4:357–370.

Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp
Hochreiter. 2015. Fast and accurate deep network
learning by exponential linear units (elus). arXiv
preprint arXiv:1511.07289 .

Timothy Dozat and Christopher D. Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. In Proceedings of ICLR-2017 (Volume 1: Long
Papers). Toulon, France.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of ACL-2015 (Volume
1: Long Papers). Beijing, China, pages 334–343.

Jason M Eisner. 1996. Three new probabilistic models
In Pro-
for dependency parsing: An exploration.
ceedings of COLING-1996 (Volume 1). Association
for Computational Linguistics, pages 340–345.

Nicolas R Fauceglia, Yiu-Chang Lin, Xuezhe Ma, and
Eduard Hovy. 2015. Word sense disambiguation via
propstore and ontonotes for event mention detec-
In Proceedings of the The 3rd Workshop on
tion.
EVENTS: Deﬁnition, Detection, Coreference, and
Representation. Denver, Colorado, pages 11–15.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems.

Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
In Proceedings of CoNLL-
in multiple languages.
2009: Shared Task. pages 1–18.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
Transactions
tional lstm feature representations.
of the Association for Computational Linguistics
4:313–327.

Terry Koo and Michael Collins. 2010. Efﬁcient third-
order dependency parsers. In Proceedings of ACL-
2010. Uppsala, Sweden, pages 1–11.

Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
In Proceedings of EMNLP-2010. Cam-
automata.
bridge, MA, pages 1288–1298.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A. Smith. 2016. Dis-
tilling an ensemble of greedy dependency parsers
In Proceedings of EMNLP-
into one mst parser.
2016. Austin, Texas, pages 1744–1753.

Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of ACL-
2014 (Volume 1: Long Papers). Baltimore, Mary-
land, pages 1381–1391.

Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of EMNLP-2013. Seattle, Washington, USA,
pages 1971–1982.

Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
In Proceedings of
word2vec for syntax problems.
NAACL-2015. Denver, Colorado, pages 1299–1304.

Thang Luong, Hieu Pham, and Christopher D. Man-
Effective approaches to attention-
ning. 2015.
based neural machine translation. In Proceedings of
EMNLP-2015. Lisbon, Portugal, pages 1412–1421.

Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu,
Yuntian Deng, and Eduard Hovy. 2017. Dropout
with expectation-linear regularization. In Proceed-
ings of the 5th International Conference on Learn-
ing Representations (ICLR-2017). Toulon, France.

Xuezhe Ma and Eduard Hovy. 2015. Efﬁcient inner-to-
outer greedy algorithm for higher-order labeled de-
pendency parsing. In Proceedings of EMNLP-2015.
Lisbon, Portugal, pages 1322–1328.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 1064–1074.

Xuezhe Ma and Eduard Hovy. 2017. Neural proba-
bilistic model for non-projective mst parsing.
In
Proceedings of IJCNLP-2017 (Volume 1: Long Pa-
pers). Taipei, Taiwan, pages 59–69.

Xuezhe Ma, Zhengzhong Liu, and Eduard Hovy. 2016.
Unsupervised ranking model for entity coreference
In Proceedings of NAACL-2016. San
resolution.
Diego, California, USA.

Xuezhe Ma and Fei Xia. 2014. Unsupervised depen-
dency parsing with transferring distribution via par-
In Pro-
allel guidance and entropy regularization.
ceedings of ACL-2014. Baltimore, Maryland, pages
1337–1348.

Xuezhe Ma and Hai Zhao. 2012a. Fourth-order depen-
In Proceedings of COLING 2012:

dency parsing.
Posters. Mumbai, India, pages 785–796.

Xuezhe Ma and Hai Zhao. 2012b. Probabilistic models
for high-order projective dependency parsing. Tech-
nical Report, arXiv:1502.04174 .

Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics 19(2):313–330.

Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
In Proceedings of ACL-
projective turbo parsers.
2013 (Volume 2: Short Papers). Soﬁa, Bulgaria,
pages 617–622.

Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011. Dual decomposition with
In Proceedings
many overlapping components.
of EMNLP-2011. Edinburgh, Scotland, UK., pages
238–249.

Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics 37(1):197–230.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria
Bertomeu Castell´o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of ACL-2013. Soﬁa, Bulgaria,
pages 92–97.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceeding of EACL-2006.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP-2005. Vancouver, Canada, pages
523–530.

Vincent Ng. 2010. Supervised noun phrase coreference
research: The ﬁrst ﬁfteen years. In Proceedings of
ACL-2010. Association for Computational Linguis-
tics, Uppsala, Sweden, pages 1396–1411.

Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In Proceedings of EMNLP-
2009. Singapore, pages 1378–1387.

Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan
McDonald, Jens Nilsson, Sebastian Riedel, and
The CoNLL 2007 shared
Deniz Yuret. 2007.
In Proceedings of
task on dependency parsing.
the CoNLL Shared Task Session of EMNLP-CoNLL
2007. Prague, Czech Republic, pages 915–932.

Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING-2004. Geneva, Switzerland, pages 64–
70.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difﬁculty of training recurrent neu-
ral networks. In Proceedings of ICML-2013. pages
1310–1318.

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph lstms. Transac-
tions of the Association for Computational Linguis-
tics 5:101–115.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC-2012. Istanbul, Turkey, pages 2089–2096.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
In Proceedings of ACL-2005.
pendency parsers.
Ann Arbor, Michigan, USA, pages 91–98.

Emily Pitler and Ryan McDonald. 2015. A linear-time
transition system for crossing interval trees. In Pro-
ceedings of NAACL-2015. Denver, Colorado, pages
662–671.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research 15(1):1929–1958.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of CoNLL-
2008. pages 159–177.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings ACL-2015 (Volume 1: Long
Papers). Beijing, China, pages 1556–1566.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems. pages 2692–2700.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In Pro-
ceedings of ACL-2016 (Volume 1: Long Papers).
Berlin, Germany, pages 2306–2315.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural net-
In Proceedings of
work transition-based parsing.
ACL-2015 (Volume 1: Long Papers). Beijing, China,
pages 323–333.

Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of COLING-2002. pages 1–8.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
In Proceedings of IWPT. Nancy, France,
chines.
volume 3, pages 195–206.

Hao Zhang and Ryan McDonald. 2014. Enforcing
structural diversity in cube-pruned dependency pars-
ing. In Proceedings of ACL-2014 (Volume 2: Short
Papers). Baltimore, Maryland, pages 656–661.

Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2014. Greed is good if randomized: New
inference for dependency parsing. In Proceedings of
EMNLP-2014. Doha, Qatar, pages 1013–1024.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Portland, Oregon, USA, pages
188–193.

Appendix: Stack-Pointer Network for Dependency Parsing

Appendix A: Hyper-Parameters

Table 5 summarizes the chosen hyper-parameters used for all the experiments in this paper. Some pa-
rameters are chosen directly or similarly from those reported in Dozat and Manning (2017). We use the
same hyper-parameters across the models on different treebanks and languages, due to time constraints.

Layer

CNN

LSTM

MLP

Dropout

Learning

Hyper-parameter
window size
number of ﬁlters
encoder layers
encoder size
decoder layers
decoder size
arc MLP size
label MLP size
embeddings
LSTM hidden states
LSTM layers
optimizer
initial learning rate
(β1, β2)
decay rate
gradient clipping

Value
3
50
3
512
1
512
512
128
0.33
0.33
0.33
Adam
0.001
(0.9, 0.9)
0.75
5.0

Table 5: Hyper-parameters for all experiments.

Appendix B: UD Treebanks

Table 6 shows the corpora statistics of the treebanks for 12 languages. For evaluation, we report results
excluding punctuation, which is any tokens with POS tags “PUNCT” or “SYM”.

Corpora

Bulgarian

BTB

Catalan

AnCora

Czech

Dutch

PDT, CAC
CLTT
FicTree
Alpino
LassySmall

English

EWT

French

GSD

German

GSD

Italian

ISDT

Norwegian

Bokmaal
Nynorsk

Romanian

RRT

Russian

SynTagRus

Spanish

GSD
AnCora

Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test
Training
Dev
Test

#Sent
8,907
1,115
1,116
13,123
1,709
1,846
102,993
11,311
12,203
18,310
1,518
1,396
12,543
2,002
2,077
14,554
1,478
416
13,841
799
977
12,838
564
482
29,870
4,300
3,450
8,043
752
729
48,814
6,584
6,491
28,492
4,300
2,174

#Token (w.o punct)
124,336 (106,813)
16,089 (13,822)
15,724 (13,456)
417,587 (371,981)
56,482 (50,452)
57,738 (51,324)
1,806,230 (1,542,805)
191,679 (163,387)
205,597 (174,771)
267,289 (234,104)
22,091 (19,042)
21,126 (18,310)
204,585 (180,308)
25,148 (21,998)
25,096 (21,898)
356,638 (316,780)
35,768 (31,896)
10,020 (8,795)
263,536 (229,204)
12,348 (10,727)
16,268 (13,929)
270,703 (239,836)
11,908 (10,490)
10,417 (9,237)
48,9217 (43,2597)
67,619 (59,784)
54,739 (48,588)
185,113 (161,429)
17,074 (14,851)
16,324 (14,241)
870,034 (711,184)
118,426 (95,676)
117,276 (95,745)
827,053 (730,062)
89,487 (78,951)
64,617 (56,973)

Table 6: Corpora statistics of UD Treebanks for 12 languages. #Sent and #Token refer to the number of
sentences and the number of words (w./w.o punctuations) in each data set, respectively.

Appendix C: Main Results

Table 7 illustrates the details of the experimental results. For each STACKPRT parsing model, we ran
experiments with decoding beam size equals to 1, 5, and 10. For each experiment, we report the mean
values with corresponding standard deviations over 5 runs.

Model beam
BiAF

Model beam
BiAF

Basic

+gpar

+sib

Full

Basic

+gpar

+sib

Full

Basic

+gpar

+sib

Full

Dev

Test

English

UAS

LAS

UCM

LCM

UAS

LAS

UCM

LCM

95.73±0.04 93.97±0.06 60.58±0.77 47.47±0.63 95.84±0.06 94.21±0.04 59.49±0.23 49.07±0.34
95.71±0.02 93.88±0.03 62.33±0.33 47.75±0.32 95.71±0.06 94.07±0.06 60.91±0.35 49.54±0.48
95.71±0.04 93.88±0.05 62.40±0.45 47.80±0.44 95.76±0.11 94.12±0.11 61.09±0.43 49.67±0.41
95.72±0.03 93.89±0.04 62.40±0.45 47.80±0.44 95.77±0.11 94.12±0.11 61.09±0.43 49.67±0.41
95.68±0.04 93.82±0.02 61.82±0.36 47.32±0.14 95.73±0.04 94.07±0.05 60.99±0.46 49.83±0.59
95.67±0.01 93.83±0.02 61.93±0.32 47.44±0.20 95.76±0.06 94.11±0.06 61.23±0.47 50.07±0.59
95.69±0.02 93.83±0.02 61.95±0.32 47.44±0.20 95.78±0.05 94.12±0.06 61.24±0.46 50.07±0.59
95.75±0.03 93.93±0.04 61.93±0.49 47.66±0.48 95.77±0.15 94.11±0.06 61.32±0.37 49.75±0.29
95.74±0.02 93.93±0.05 62.16±0.22 47.68±0.54 95.84±0.09 94.17±0.09 61.52±0.57 49.91±0.76
95.75±0.02 93.94±0.06 62.17±0.20 47.68±0.54 95.85±0.10 94.18±0.09 61.52±0.57 49.91±0.76
95.63±0.08 93.78±0.08 61.56±0.63 47.12±0.36 95.79±0.06 94.11±0.06 61.02±0.31 49.45±0.23
95.75±0.06 93.90±0.08 62.06±0.42 47.43±0.36 95.87±0.04 94.20±0.03 61.43±0.49 49.68±0.47
95.75±0.06 93.90±0.08 62.08±0.39 47.43±0.36 95.87±0.04 94.19±0.04 61.43±0.49 49.68±0.47
Chinese

Dev

Test

UAS

LAS

UCM

LCM

UAS

LAS

UCM

LCM

90.20±0.17 88.94±0.13 43.41±0.83 38.42±0.79 90.43±0.08 89.14±0.09 42.92±0.29 38.68±0.25
89.76±0.32 88.44±0.28 45.18±0.80 40.13±0.63 90.04±0.32 88.74±0.40 45.00±0.47 40.12±0.42
89.97±0.13 88.67±0.14 45.33±0.58 40.25±0.65 90.46±0.15 89.17±0.18 45.41±0.48 40.53±0.48
89.97±0.14 88.68±0.14 45.33±0.58 40.25±0.65 90.48±0.11 89.19±0.15 45.44±0.44 40.56±0.43
90.05±0.14 88.71±0.16 45.63±0.52 40.45±0.61 90.28±0.10 88.96±0.10 45.26±0.59 40.38±0.43
90.17±0.14 88.85±0.13 46.03±0.53 40.69±0.55 90.45±0.15 89.14±0.14 45.71±0.46 40.80±0.26
90.18±0.16 88.87±0.14 46.05±0.58 40.69±0.55 90.46±0.16 89.16±0.15 45.71±0.46 40.80±0.26
89.91±0.07 88.59±0.10 45.50±0.50 40.40±0.48 90.25±0.10 88.94±0.12 45.42±0.52 40.54±0.69
89.99±0.05 88.70±0.09 45.55±0.36 40.37±0.14 90.41±0.07 89.12±0.07 45.76±0.46 40.69±0.52
90.00±0.04 88.72±0.09 45.58±0.32 40.37±0.14 90.43±0.09 89.15±0.10 45.75±0.44 40.68±0.50
90.21±0.15 88.85±0.15 45.83±0.52 40.54±0.60 90.36±0.16 89.05±0.15 45.60±0.33 40.73±0.23
90.23±0.13 88.89±0.14 46.00±0.54 40.75±0.64 90.58±0.12 89.27±0.11 46.20±0.26 41.25±0.22
90.29±0.13 88.95±0.13 46.03±0.54 40.75±0.64 90.59±0.12 89.29±0.11 46.20±0.26 41.25±0.22
German

Dev

Test

UAS

LAS

UCM

LCM

UAS

LAS

UCM

LCM

93.60±0.13 91.96±0.13 58.79±0.25 49.59±0.19 93.85±0.07 92.32±0.06 60.60±0.38 52.46±0.24
93.35±0.14 91.58±0.17 59.64±0.78 49.75±0.64 93.39±0.09 91.85±0.09 61.08±0.31 52.21±0.53
93.49±0.14 91.72±0.16 59.99±0.69 49.82±0.54 93.61±0.09 92.07±0.08 61.38±0.30 52.51±0.43
93.48±0.14 91.71±0.17 60.02±0.69 49.84±0.54 93.59±0.09 92.06±0.08 61.38±0.30 52.51±0.43
93.39±0.07 91.66±0.13 59.59±0.54 49.81±0.42 93.44±0.07 91.91±0.11 61.73±0.47 52.84±0.48
93.47±0.09 91.75±0.10 59.81±0.55 50.05±0.39 93.68±0.04 92.16±0.04 62.09±0.44 53.13±0.42
93.48±0.08 91.76±0.09 59.89±0.59 50.09±0.40 93.68±0.05 92.16±0.03 62.10±0.42 53.14±0.4
93.43±0.07 91.73±0.08 59.68±0.25 49.93±0.30 93.55±0.07 92.00±0.08 61.90±0.50 52.79±0.22
93.53±0.05 91.83±0.07 59.95±0.23 50.14±0.39 93.75±0.09 92.20±0.08 62.21±0.38 53.03±0.18
93.55±0.06 91.84±0.07 59.96±0.24 50.15±0.40 93.76±0.09 92.21±0.08 62.21±0.38 53.03±0.18
93.33±0.13 91.60±0.16 59.78±0.32 49.78±0.29 93.50±0.04 91.91±0.11 61.80±0.28 52.95±0.37
93.42±0.11 91.69±0.12 59.90±0.27 49.94±0.35 93.64±0.03 92.10±0.06 61.89±0.21 53.06±0.36
93.40±0.11 91.67±0.12 59.90±0.27 49.94±0.35 93.64±0.03 92.11±0.05 61.89±0.21 53.06±0.36

–
1
5
10
1
5
10
1
5
10
1
5
10

–
1
5
10
1
5
10
1
5
10
1
5
10

–
1
5
10
1
5
10
1
5
10
1
5
10

Model beam
BiAF

Table 7: Parsing performance of different variations of our model on both the development and test sets
for three languages, together with the BIAF parser as the baseline. Best results are highlighted with bold
print.

