Robust Subspace Clustering via Smoothed Rank
Approximation
Zhao Kang, Chong Peng, and Qiang Cheng∗

Abstract

Matrix rank minimizing subject to afﬁne constraints arises in many application areas, ranging from signal
processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank
exactly under some restricted and theoretically interesting conditions. However, for many real-world applications,
nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution
of higher accuracy than the nuclear norm, in this paper, we propose a rank approximation based on Logarithm-
Determinant. We consider using this rank approximation for subspace clustering application. Our framework can
model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee
to converge to a stationary point. The proposed method gives promising results on face clustering and motion
segmentation tasks compared to the state-of-the-art subspace clustering algorithms.

Subspace clustering, Matrix rank minimization, Nuclear norm, Nonconvex optimization.

Index Terms

I. INTRODUCTION

R ECENTLY there has been a surge of interest in ﬁnding minimum rank matrix within an afﬁne

constraint set [1], [2]. The problem is as follows,

min
Z

rank(Z)

s.t A(Z) = b,

where Z ∈ Rm×n is the unknown matrix, A : Rm×n → Rp is a linear mapping, and b ∈ Rp denotes
the observations. Unfortunately, however, minimizing the rank of a matrix is known to be NP-hard and a
very challenging problem.

Consequently, a widely-used convex relaxation approach is to replace the rank function with the nuclear
norm (cid:107)Z(cid:107)∗ = (cid:80)n
i=1 σi(Z), where σi(Z) is the i-th singular value of Z (suppose n < m). The nuclear
norm technique has been shown to be effective in encouraging a low-rank solution [3], [4]. Nevertheless,
there is no guarantee for the minimum nuclear norm solution to coincide with that of minimal rank in
many interesting circumstances, which is heavily dependent on the singular values of matrices in the
nullspace of A. Some variations of the nuclear norm have demonstrated promising results, e.g., singular
value thresholding [5], and truncated nuclear norm [6].
Another popular alternative approach is to compute

5
1
0
2
 
g
u
A
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
4
4
0
.
8
0
5
1
:
v
i
X
r
a

min
Z

n
(cid:88)

i=1

h(σi(Z))

s.t. A(Z) = b,

where h is usually a nonconvex and nonsmooth function. It has been observed that nonconvex approach
can succeed in a broader range of scenarios [7]. However, nonconvex optimization is often challenging.
To overcome the above-mentioned difﬁculties, in this paper, we propose to use a particular log-

determinant (LogDet) function to approximate the rank function. The formulation we consider is:

F (Z) = logdet(I + Z T Z) =

log(1 + σ2

i (Z)),

n
(cid:88)

i=1

∗The authors are with Computer Science Department, Southern Illinois University, Carbondale, IL, USA, 62901. Correspondence should

be sent to qcheng@cs.siu.edu.

1

(1)

(2)

(3)

where I ∈ Rn×n is an identity matrix. For large nonzero singular values, the LogDet function value will be
much smaller than the nuclear norm. It is easy to show that logdet(I + Z T Z) ≤ (cid:107)Z(cid:107)∗. Therefore, LogDet
is a tighter rank approximation function than the nuclear norm. Although a similar function logdet(Z +δI)
has been proposed and iterative linearization has been used to ﬁnd a local minimum [8], its δ is required to
be small (e.g., 10−6), which leads to signiﬁcantly biased approximation for small singular values and thus
limited applications. Smoothed Schatten-p function, T r(Z T Z + γI)p/2, has been well studied in matrix
completion [9]; nonetheless, the resulting algorithm is rather sensitive to parameter γ.

The main contributions of this work are as follows: 1) An efﬁcient algorithm is devised to optimize
LogDet associated nonconvex objective function; 2) Our method pushes the accuracy of subspace clus-
tering to a new level.

II. PROBLEM STATEMENT OF SUBSPACE CLUSTERING
An important application of the proposed LogDet function is the low-rank representation based subspace
clustering problem. There has been signiﬁcant research effort on this subject over the past several years due
to its promising applications in computer vision and machine learning [10]. Subspace clustering aims at
ﬁnding a low-dimensional subspace for each group of points, which is based on the widely-used assumption
that high-dimensional data actually reside in a union of multiple low-dimensional subspaces. Under such
an assumption the data could be separated in a projected subspace. Consequently, subspace clustering
mainly involves two tasks, ﬁrstly projecting the data into a latent subspace to describe the afﬁnities of
points, and subsequently, grouping the data in that subspace. Some spectral clustering methods such as
normalized cuts (NCuts) [11] are usually used in the second task to ﬁnd the cluster membership. Besides
this spectral clustering-based subspace clustering method, iterative, algebraic, and statistical methods are
also available in the literature [10], but they are usually sensitive to initialization, noise or outliers.

Typical spectral clustering-based subspace clustering methods are Local Subspace Afﬁnity (LSA) [12],
Sparse Subspace Clustering (SSC) [13], Low Rank Representation (LRR) [2], [14] and its more robust
variant LRSC [15], [16]. Among them, SSC and LRR give promising results even in the presence of
large outliers or corruption [17], [18]. They both suppose that each data point can be written as a linear
combination of other points in the dataset. SSC tries to ﬁnd the sparsest representation of data points
through l1-norm. Even when the subspaces overlap, SSC can successfully reveal subspace structure [19].
SSC’s solution is sometimes too sparse to form a fully connected afﬁnity graph for data in a single
subspace [20]. LRR uses the lowest-rank representation to depict the similarity among data points. It is
theoretically guaranteed to succeed when the subspaces are independent.

Let X = [x1, x2, · · · , xn] ∈ Rm×n store a set of n m-dimensional samples drawn from a union of k

subspaces. LRR considers the following regularized nuclear norm rank minimization problem:

min
Z,E

(cid:107)Z(cid:107)∗ + λ(cid:107)E(cid:107)l

s.t. X = XZ + E,

where λ > 0 is a parameter, E represents unknown corruption, and (cid:107) · (cid:107)l can be l2,1-norm, l1-norm, or
squared Frobenius norm. Speciﬁcally, if random corruption is assumed in the data, (cid:107)E(cid:107)1 := (cid:80)m
j=1 |Eij|
is usually adopted; (cid:107)E(cid:107)2,1 := (cid:80)n
i=1 E2
ij is more suitable to characterize sample-speciﬁc corrup-
tions and outliers; (cid:107)E(cid:107)2
ij often describes Gaussian noise. LRR is able to produce pretty
competitive performance on subspace clustering in the current literature. However, the solution to it might
not be unique due to the nuclear norm [21]; and furthermore, the rank surrogate can deviate far from the
true rank function.

F := (cid:80)m

(cid:113)(cid:80)m

j=1 E2

j=1
(cid:80)n

(cid:80)n

i=1

i=1

To better approximate the rank while possessing the desired robustness similar to LRR, in this paper,

we propose to use the above-mentioned LogDet function and solve the following problem:

logdet(I + Z T Z) + λ(cid:107)E(cid:107)l

s.t. X = XZ + E.

min
Z,E

The objective function of (5) is nonconvex. We design an effective optimization strategy based on an
augmented Lagrangian multiplier (ALM) method, which is potentially applicable to large-scale data

2

(4)

(5)

because of the decomposability of ALM and its admittance to parallel algorithms. For our optimization
method, we provide theoretical analysis for its convergence, which mathematically guarantees that our
algorithm can produce a convergent subsequence and the converged point is a stationary point of (5).

III. PROPOSED METHOD: CLAR

In this section, we present the proposed robust subspace clustering algorithm CLAR: Clustering with
Log-determinant Approximation to Rank. The basic theorems and optimization algorithm will be presented
below.

A. Smoothed rank minimization

by introducing an auxiliary variable J:

To make the objective function in (5) separable, we ﬁrst convert it to the following equivalent problem

min
E,J,Z

logdet(I + J T J) + λ||E||l s.t. X = XZ + E, Z = J.

We can solve problem (6) using a type of ALM method. The corresponding augmented Lagrangian
function is

where Y1 and Y2 are Lagrange multipliers, and µ > 0 is a penalty parameter. Then we can apply the
alternating minimization idea to update one of the variables with the others ﬁxed.
1 , Y t

Given the current point Et, J t, Z t, Y t

2 , the updating scheme is:

L(E, J, Y1, Y2, Z, µ) = logdet(I + J T J) + λ(cid:107)E(cid:107)l
+ T r(Y T
2 (X − XZ − E))+
µ
2

F + (cid:107)X − XZ − E(cid:107)2

1 (J − Z)) + T r(Y T

((cid:107)J − Z(cid:107)2

F ),

Z t+1 = arg min

T r[(Y t

1 )T (J t − Z)] +

(cid:107)J t − Z(cid:107)2
F

+ T r[(Y t

2 )T (X − XZ − Et)] +
logdet(I + J T J)+

J t+1 = arg min

(cid:107)X − XZ − Et(cid:107)2
F ,

µt
2
µt
2

µt
2

(cid:107)J − (Z t+1 −

Y t
1
µt )(cid:107)2
F ,
λ||E||l + T r[(Y t

Et+1 = arg min

+

µt
2

||X − XZ t+1 − E||2
F .

2 )T (X − XZ t+1 − E)]

Z

J

E

The ﬁrst equation above has a closed-form solution:

Z t+1 = (I + X T X)−1[X T (X − Et) + J t +

Y t
1 + X T Y t
2
µt

].

For J updating, it can be converted to scalar minimization problems due to the following theorem [22],
which is also proved in the supplementary material.

Theorem 1. If F (Z) is a unitarily invariant function and SVD of A is A = U ΣAV T , then the optimal
solution to the following problem

min
Z

F (Z) +

(cid:107)Z − A(cid:107)2
F

β
2

3

(6)

(7)

(8)

(9)

Algorithm 1 Smoothed Rank Minimization
Input: data matrix X ∈ Rm×n, parameters λ > 0, µ0 > 0, γ > 1.
Initialize: J = I ∈ Rn×n, E = 0, Y1 = Y2 = 0.
REPEAT
1: Obtain Z through (8).
2: Update J as (12).
3: Solve E by either (13), (14), or (15) according to l.
4: Update the multipliers:

5: Update the parameter µt by µt+1 = γµt.
UNTIL stopping criterion is met.

Y t+1
1
Y t+1
2

= Y t
= Y t

1 + µt(J t+1 − Zt+1),
2 + µt(X − XZt+1 − Et+1).

is Z ∗ with SVD U Σ∗
Z = diag (σ∗); moreover, F (Z) = f ◦ σ(Z), where σ(Z) is the vector of
nonincreasing singular values of Z, then σ∗ is obtained by using the Moreau-Yosida proximity operator
σ∗ = proxf,β(σA), where σA := diag(ΣA), and

ZV T , where Σ∗

proxf,β(σA) := arg min

f (σ) +

(cid:107)σ − σA(cid:107)2
2.

σ

β
2

According to the ﬁrst-order optimality condition, the gradient of the objective function of (10) with

respect to each singular value should vanish. Thus we have

2σi
1 + σ2
i

+ βk(σi − σt

i,A) = 0, s.t. σi ≥ 0, f or i = 1, ..., n.

The above equation is cubic and gives three roots. If σt
it can be shown that there is a unique minimizer σ∗
satisﬁed, we adopt µ0 = 0.4 in our experiments. Finally, we obtain the update of J variable with

i will be 0; otherwise,
4. To ensure this requirement is

i,A = 0, the minimizer σ∗

i,A) if β > 1

i ∈ [0, σt

Depending on different regularization strategies, we have different closed-form solutions for E. For squared
Forbenius norm,

J t+1 = U diag(σt+1

1

, ..., σt+1

n )V T .

Et+1 =

2 + µt(X − XZ t+1)
Y t
µt + 2λ

.

For l1-norm, according to [23], if we deﬁne Q = X − XZ t+1 + Y t
as:

1

µt , then E can be updated element-wisely

For l2,1-norm, by [24], we have

Et+1

ij =

(cid:26) Qij − λ
0,

µt sgn(Qij),

if |Qij| < λ
µt ;
otherwise.

[Et+1]:,i =

(cid:40) (cid:107)Q:,i(cid:107)2− λ
µt
(cid:107)Q:,i(cid:107)2

Q:,i,

0,

if (cid:107)Q:,i(cid:107)2 > λ
µt ;
otherwise.

The complete procedure for solving problem (5) is summarized in Algorithm 1. Since our objective
function is nonconvex, it is difﬁcult to give a rigorous mathematical proof for convergence to an (local)
optimum. As we show in the supplementary material, our algorithm converges to an accumulation point
and this accumulation point is a stationary point. Our experiments conﬁrm the convergence of the proposed
method. The experimental results are promising, despite that the solution obtained by the proposed
optimization method may be a local optimum.

4

(10)

(11)

(12)

(13)

(14)

(15)

B. Subspace segmentation

After we obtain the coefﬁcient matrix Z ∗, we consider constructing a similarity graph matrix W from
it, since postprocessing of the coefﬁcient matrix often improves the clustering performance [13]. Using
the angular information based technique in [14], we deﬁne (cid:101)U = U (Σ)1/2, where U and Σ are from the
skinny SVD of Z ∗ = U ΣV T . Inspired by [25], we deﬁne W as:

Wij = (

(cid:101)uT
i (cid:101)uj
(cid:107)(cid:101)ui(cid:107)2 (cid:107)(cid:101)uj(cid:107)2

)2φ,

where (cid:101)ui and (cid:101)uj stand for the i-th and j-th columns of (cid:101)U , and φ ∈ N ∗ tunes the sharpness of the afﬁnity
between two points. However, an excessively large φ would break afﬁnities between points of the same
group. φ = 2 is used in our experiments, and thus we have the same post-processing procedure as LRR1.
After obtaining W , we directly utilize NCuts to cluster the samples.

5

(16)

Fig. 1. Sample face images in Extended Yale B.

IV. EXPERIMENT
In this section, we apply CLAR to subspace clustering on two benchmark databases: the Extended
Yale B database (EYaleB) [26] and the Hopkins 155 motion database [27]. CLAR is compared with the
state-of-the-art subspace clustering algorithms: SSC, LSA, LRR, and LRSC. The segmentation error rate is
used to evaluate the subspace clustering performance, which is deﬁned to be the percentage of erroneously
clustered samples versus the total number of samples in the data set being considered. The parameters
are tuned to achieve the best performance. In general, when the corruptions or noise are slight, the value
of λ should be relatively large. For our two experiments, λ = 3 × 10−4 and 67 are used. γ inﬂuences
the convergence speed, and we adopt γ = 1.1 as often done in literature. For fair comparison, we follow
experimental settings in [13]. We stop the program when a maximum of 100 iterations or a relative
difference of 10−5 is reached. The experiments are implemented on Intel Core i5 2.3GHz MacBook Pro
2011 with 4G memory. The code is available at: https://github.com/sckangz/logdet.

A. Face clustering

EYaleB consists of 2,414 frontal face images of 38 individuals under 64 lighting conditions. The task
is to cluster these images into their individual subspaces by identity. EYaleB is challenging for subspace
clustering due to large noise or corruptions, which can be seen from sample images in Figure 1. As [13],
we model noise with (cid:107)E(cid:107)1. Each image is resized to a 2016-dimensional vector. We divide the 38 subjects
into four groups, i.e., 1 to 10, 11 to 20, 21 to 30, and 31 to 38, and consider all choices of {2, 3, 5, 8}
for each group and all choices of n = 10 in the ﬁrst three groups. There will be {163, 416, 812, 136, 3}
datasets for each n, respectively.

Mean and median error rates for the datasets corresponding to each n are reported in Table I. It can
be seen that CLAR outperforms the other methods signiﬁcantly. As more subjects are involved, the error
rate of CLAR remains at a low level, while those of other methods increase drastically. In particular,
in the most challenging case of 10 subjects, the mean clustering error rate of CLAR is 3.85%, which
improves by 7.09% compared to the best result provided by SSC. This implies that our method is robust
to in-sample outliers. In Table I, we also observe that the clustering error rates of LSA are much larger
than other methods. This is potentially because LSA is based on MSE, which is heavily inﬂuenced

1As we conﬁrmed with an author of [14], the power 2 of equation (12) in [14] is a typo, which should be 4.

6

Fig. 2. Examples of recovery results of face images. The three columns from left to right are the original image (X), the error matrix (E)
and the recovered image (XZ), respectively.

by outliers. In addition, the advantage of our method is much more signiﬁcant with respect to other
low-rank representation based algorithms such as LRR and LRSC; for example, there is 11% and 19%
improvement over LRR in the cases of 8 and 10 subjects, respectively. This veriﬁes the importance of
good rank approximation.

Figure 2 shows some recovery results from the 10-subject clustering scenario. As we can see, the error

term E is indeed sparse and it helps remove the shadows.

TABLE I
CLUSTERING ERROR RATE (%) ON THE EYALEB DATASET.

METHOD

LRR

SSC

LSA

LRSC CLAR

2 SUBJECTS
MEAN
MEDIAN
3 SUBJECTS
MEAN
MEDIAN
5 SUBJECTS
MEAN
MEDIAN
8 SUBJECTS
MEAN
MEDIAN
10 SUBJECTS
MEAN
MEDIAN

2.54
0.78

4.21
2.60

6.90
5.63

14.34
10.06

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

32.80
47.66

52.29
50.00

5.32
4.69

8.47
7.81

58.02
56.87

12.24
11.25

59.19
58.59

23.72
28.03

22.92
23.59

10.94
5.63

60.42
57.50

30.36
28.75

1.27
0.78

1.92
1.56

2.64
2.19

3.36
3.03

3.85
3.44

Fig. 3. Sample images in Hopkins 155 database. Trackers are denoted by different colors.

7

B. Motion segmentation

In this subsection, we evaluate the robustness of CLAR for motion segmentation problem, which is
an important step in video sequences analysis. Given multiple image frames of a dynamic scene, motion
segmentation is to cluster the points in those views into different motions undertaken by the moving
objects. Hopkins 155 motion database contains 155 video sequences along with features extracted and
tracked in all frames for each sequence. Since the trajectories associated with each motion reside in a
distinct afﬁne subspace of dimension d ≤ 3, every motion corresponds to a subspace. Figure 3 gives some
sample images. (cid:107)E(cid:107)2
F is applied to model the noise. Table II shows the clustering results on the Hopkins

TABLE II
SEGMENTATION ERROR RATE (%) AND MEAN COMPUTATIONAL TIME (S) ON THE HOPKINS 155 DATASET.

METHOD

LRR SSC

LSA

LRSC CLAR

2 MOTIONS
MEAN
MEDIAN
3 MOTIONS
MEAN
MEDIAN
ALL
MEAN
MEDIAN
AVERAGE TIME

2.13
0.00

4.03
1.43

2.56
0.00
6.44

1.52
0.00

4.40
0.56

2.18
0.00
5.09

4.23
0.56

7.02
1.45

4.86
0.89
17.17

3.69
0.29

7.69
3.80

4.59
0.60
0.70

1.32
0.00

2.60
0.51

1.61
0.00
3.80

Fig. 4. The inﬂuence of the parameter λ of CLAR on all 155 sequences of Hopkins 155.

155 dataset. CLAR achieves the best results in all cases. Speciﬁcally, the average clustering error rate is
1.32% for two motions and 2.60% for three motions. We also show the computational time in Table II.
As we can see, our computational time is less than LRR, SSC and LSA, though more than LRSC. Figure
4 demonstrates the sensitivity of our algorithm to λ. It shows that the performance of CLAR is quite
stable while λ varies in a pretty large range. We also test γ with values 1.05 and 1.2 which do not give
much difference in error rate. Since our problem is nonconvex, we repeat the experiments using different
random initializations and we can still get similar results after tuning the parameters. Thus, CLAR appears
quite insensitive to initilizations.

V. CONCLUSION
In this paper, we study the matrix rank minimization problem with log-determinant approximation.
This surrogate can better approximate the rank function. As an application, we study its use for the
robust subspace clustering problem. A minimization algorithm, based on a type of augmented Lagrangian
multipliers method, is developed to optimize the associated nonconvex objective function. Extensive
experiments on the face clustering and motion segmentation demonstrate the effectiveness and robustness
of the proposed method, which shows superior performance when compared to the state-of-the-art subspace
clustering methods.

This work is supported by US National Science Foundation grants IIS 1218712.

ACKNOWLEDGMENT

8

REFERENCES

[1] E. J. Cand`es and B. Recht, “Exact matrix completion via convex optimization,” Foundations of Computational mathematics, vol. 9,

no. 6, pp. 717–772, 2009.

[2] G. Liu, Z. Lin, and Y. Yu, “Robust subspace segmentation by low-rank representation,” in Proceedings of the 27th International

Conference on Machine Learning (ICML-10), 2010, pp. 663–670.

[3] M. Fazel, “Matrix rank minimization with applications,” Ph.D. dissertation, PhD thesis, Stanford University, 2002.
[4] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization,”

[5] J.-F. Cai, E. J. Cand`es, and Z. Shen, “A singular value thresholding algorithm for matrix completion,” SIAM Journal on Optimization,

SIAM review, vol. 52, no. 3, pp. 471–501, 2010.

vol. 20, no. 4, pp. 1956–1982, 2010.

[6] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He, “Fast and accurate matrix completion via truncated nuclear norm regularization,” Pattern

Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 9, pp. 2117–2130, 2013.

[7] C. Lu, J. Tang, S. Y. Yan, and Z. Lin, “Generalized nonconvex nonsmooth low-rank minimization,” in IEEE International Conference

on Computer Vision and Pattern Recognition.

IEEE, 2014.

[8] M. Fazel, H. Hindi, and S. P. Boyd, “Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance

matrices,” in American Control Conference, 2003. Proceedings of the 2003, vol. 3.

IEEE, 2003, pp. 2156–2162.

[9] K. Mohan and M. Fazel, “Iterative reweighted algorithms for matrix rank minimization,” The Journal of Machine Learning Research,

[10] R. Vidal, “A tutorial on subspace clustering,” IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 52–68, 2010.
[11] J. Shi and J. Malik, “Normalized cuts and image segmentation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on,

vol. 13, no. 1, pp. 3441–3473, 2012.

vol. 22, no. 8, pp. 888–905, 2000.

[12] J. Yan and M. Pollefeys, “A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and

non-degenerate,” in Computer Vision–ECCV 2006. Springer, 2006, pp. 94–106.

[13] E. Elhamifar and R. Vidal, “Sparse subspace clustering: Algorithm, theory, and applications,” Pattern Analysis and Machine Intelligence,

IEEE Transactions on, vol. 35, no. 11, pp. 2765–2781, 2013.

[14] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of subspace structures by low-rank representation,” Pattern Analysis

and Machine Intelligence, IEEE Transactions on, vol. 35, no. 1, pp. 171–184, 2013.

[15] P. Favaro, R. Vidal, and A. Ravichandran, “A closed form solution to robust subspace estimation and clustering,” in Computer Vision

and Pattern Recognition (CVPR), 2011 IEEE Conference on.

IEEE, 2011, pp. 1801–1807.

[16] R. Vidal and P. Favaro, “Low rank subspace clustering (lrsc),” Pattern Recognition Letters, vol. 43, pp. 47–61, 2014.
[17] G. Liu, H. Xu, and S. Yan, “Exact subspace segmentation and outlier detection by low-rank representation,” in International Conference

on Artiﬁcial Intelligence and Statistics, 2012, pp. 703–711.

[18] Y.-X. Wang and H. Xu, “Noisy sparse subspace clustering,” in Proceedings of The 30th International Conference on Machine Learning,

[19] M. Soltanolkotabi, E. J. Candes et al., “A geometric analysis of subspace clustering with outliers,” The Annals of Statistics, vol. 40,

2013, pp. 89–97.

no. 4, pp. 2195–2238, 2012.

[20] B. Nasihatkon and R. Hartley, “Graph connectivity in sparse subspace clustering,” in Computer Vision and Pattern Recognition (CVPR),

2011 IEEE Conference on.

IEEE, 2011, pp. 2137–2144.

[21] H. Zhang, Z. Lin, and C. Zhang, “A counterexample for the validity of using nuclear norm as a convex surrogate of rank,” in Machine

Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 226–241.

[22] Z. Kang, C. Peng, J. Cheng, and Q. Cheng, “Logdet rank minimization with application to subspace clustering,” Computational

[23] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algorithm for linear inverse problems,” SIAM Journal on Imaging

Intelligence and Neuroscience, vol. 2015, 2015.

Sciences, vol. 2, no. 1, pp. 183–202, 2009.

[24] J. Yang, W. Yin, Y. Zhang, and Y. Wang, “A fast algorithm for edge-preserving variational multichannel image restoration,” SIAM

[25] F. Lauer and C. Schnorr, “Spectral clustering of linear subspaces for motion segmentation,” in Computer Vision, 2009 IEEE 12th

Journal on Imaging Sciences, vol. 2, no. 2, pp. 569–592, 2009.

International Conference on.

IEEE, 2009, pp. 678–685.

[26] K.-C. Lee, J. Ho, and D. Kriegman, “Acquiring linear subspaces for face recognition under variable lighting,” Pattern Analysis and

Machine Intelligence, IEEE Transactions on, vol. 27, no. 5, pp. 684–698, 2005.

[27] R. Tron and R. Vidal, “A benchmark for the comparison of 3-d motion segmentation algorithms,” in Computer Vision and Pattern
IEEE, 2007, pp. 1–8.

Recognition, 2007. CVPR’07. IEEE Conference on.

Robust Subspace Clustering via Smoothed Rank
Approximation
Zhao Kang, Chong Peng, and Qiang Cheng∗

Abstract

Matrix rank minimizing subject to afﬁne constraints arises in many application areas, ranging from signal
processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank
exactly under some restricted and theoretically interesting conditions. However, for many real-world applications,
nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution
of higher accuracy than the nuclear norm, in this paper, we propose a rank approximation based on Logarithm-
Determinant. We consider using this rank approximation for subspace clustering application. Our framework can
model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee
to converge to a stationary point. The proposed method gives promising results on face clustering and motion
segmentation tasks compared to the state-of-the-art subspace clustering algorithms.

Subspace clustering, Matrix rank minimization, Nuclear norm, Nonconvex optimization.

Index Terms

I. INTRODUCTION

R ECENTLY there has been a surge of interest in ﬁnding minimum rank matrix within an afﬁne

constraint set [1], [2]. The problem is as follows,

min
Z

rank(Z)

s.t A(Z) = b,

where Z ∈ Rm×n is the unknown matrix, A : Rm×n → Rp is a linear mapping, and b ∈ Rp denotes
the observations. Unfortunately, however, minimizing the rank of a matrix is known to be NP-hard and a
very challenging problem.

Consequently, a widely-used convex relaxation approach is to replace the rank function with the nuclear
norm (cid:107)Z(cid:107)∗ = (cid:80)n
i=1 σi(Z), where σi(Z) is the i-th singular value of Z (suppose n < m). The nuclear
norm technique has been shown to be effective in encouraging a low-rank solution [3], [4]. Nevertheless,
there is no guarantee for the minimum nuclear norm solution to coincide with that of minimal rank in
many interesting circumstances, which is heavily dependent on the singular values of matrices in the
nullspace of A. Some variations of the nuclear norm have demonstrated promising results, e.g., singular
value thresholding [5], and truncated nuclear norm [6].
Another popular alternative approach is to compute

5
1
0
2
 
g
u
A
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
4
4
0
.
8
0
5
1
:
v
i
X
r
a

min
Z

n
(cid:88)

i=1

h(σi(Z))

s.t. A(Z) = b,

where h is usually a nonconvex and nonsmooth function. It has been observed that nonconvex approach
can succeed in a broader range of scenarios [7]. However, nonconvex optimization is often challenging.
To overcome the above-mentioned difﬁculties, in this paper, we propose to use a particular log-

determinant (LogDet) function to approximate the rank function. The formulation we consider is:

F (Z) = logdet(I + Z T Z) =

log(1 + σ2

i (Z)),

n
(cid:88)

i=1

∗The authors are with Computer Science Department, Southern Illinois University, Carbondale, IL, USA, 62901. Correspondence should

be sent to qcheng@cs.siu.edu.

1

(1)

(2)

(3)

where I ∈ Rn×n is an identity matrix. For large nonzero singular values, the LogDet function value will be
much smaller than the nuclear norm. It is easy to show that logdet(I + Z T Z) ≤ (cid:107)Z(cid:107)∗. Therefore, LogDet
is a tighter rank approximation function than the nuclear norm. Although a similar function logdet(Z +δI)
has been proposed and iterative linearization has been used to ﬁnd a local minimum [8], its δ is required to
be small (e.g., 10−6), which leads to signiﬁcantly biased approximation for small singular values and thus
limited applications. Smoothed Schatten-p function, T r(Z T Z + γI)p/2, has been well studied in matrix
completion [9]; nonetheless, the resulting algorithm is rather sensitive to parameter γ.

The main contributions of this work are as follows: 1) An efﬁcient algorithm is devised to optimize
LogDet associated nonconvex objective function; 2) Our method pushes the accuracy of subspace clus-
tering to a new level.

II. PROBLEM STATEMENT OF SUBSPACE CLUSTERING
An important application of the proposed LogDet function is the low-rank representation based subspace
clustering problem. There has been signiﬁcant research effort on this subject over the past several years due
to its promising applications in computer vision and machine learning [10]. Subspace clustering aims at
ﬁnding a low-dimensional subspace for each group of points, which is based on the widely-used assumption
that high-dimensional data actually reside in a union of multiple low-dimensional subspaces. Under such
an assumption the data could be separated in a projected subspace. Consequently, subspace clustering
mainly involves two tasks, ﬁrstly projecting the data into a latent subspace to describe the afﬁnities of
points, and subsequently, grouping the data in that subspace. Some spectral clustering methods such as
normalized cuts (NCuts) [11] are usually used in the second task to ﬁnd the cluster membership. Besides
this spectral clustering-based subspace clustering method, iterative, algebraic, and statistical methods are
also available in the literature [10], but they are usually sensitive to initialization, noise or outliers.

Typical spectral clustering-based subspace clustering methods are Local Subspace Afﬁnity (LSA) [12],
Sparse Subspace Clustering (SSC) [13], Low Rank Representation (LRR) [2], [14] and its more robust
variant LRSC [15], [16]. Among them, SSC and LRR give promising results even in the presence of
large outliers or corruption [17], [18]. They both suppose that each data point can be written as a linear
combination of other points in the dataset. SSC tries to ﬁnd the sparsest representation of data points
through l1-norm. Even when the subspaces overlap, SSC can successfully reveal subspace structure [19].
SSC’s solution is sometimes too sparse to form a fully connected afﬁnity graph for data in a single
subspace [20]. LRR uses the lowest-rank representation to depict the similarity among data points. It is
theoretically guaranteed to succeed when the subspaces are independent.

Let X = [x1, x2, · · · , xn] ∈ Rm×n store a set of n m-dimensional samples drawn from a union of k

subspaces. LRR considers the following regularized nuclear norm rank minimization problem:

min
Z,E

(cid:107)Z(cid:107)∗ + λ(cid:107)E(cid:107)l

s.t. X = XZ + E,

where λ > 0 is a parameter, E represents unknown corruption, and (cid:107) · (cid:107)l can be l2,1-norm, l1-norm, or
squared Frobenius norm. Speciﬁcally, if random corruption is assumed in the data, (cid:107)E(cid:107)1 := (cid:80)m
j=1 |Eij|
is usually adopted; (cid:107)E(cid:107)2,1 := (cid:80)n
i=1 E2
ij is more suitable to characterize sample-speciﬁc corrup-
tions and outliers; (cid:107)E(cid:107)2
ij often describes Gaussian noise. LRR is able to produce pretty
competitive performance on subspace clustering in the current literature. However, the solution to it might
not be unique due to the nuclear norm [21]; and furthermore, the rank surrogate can deviate far from the
true rank function.

F := (cid:80)m

(cid:113)(cid:80)m

j=1 E2

j=1
(cid:80)n

(cid:80)n

i=1

i=1

To better approximate the rank while possessing the desired robustness similar to LRR, in this paper,

we propose to use the above-mentioned LogDet function and solve the following problem:

logdet(I + Z T Z) + λ(cid:107)E(cid:107)l

s.t. X = XZ + E.

min
Z,E

The objective function of (5) is nonconvex. We design an effective optimization strategy based on an
augmented Lagrangian multiplier (ALM) method, which is potentially applicable to large-scale data

2

(4)

(5)

because of the decomposability of ALM and its admittance to parallel algorithms. For our optimization
method, we provide theoretical analysis for its convergence, which mathematically guarantees that our
algorithm can produce a convergent subsequence and the converged point is a stationary point of (5).

III. PROPOSED METHOD: CLAR

In this section, we present the proposed robust subspace clustering algorithm CLAR: Clustering with
Log-determinant Approximation to Rank. The basic theorems and optimization algorithm will be presented
below.

A. Smoothed rank minimization

by introducing an auxiliary variable J:

To make the objective function in (5) separable, we ﬁrst convert it to the following equivalent problem

min
E,J,Z

logdet(I + J T J) + λ||E||l s.t. X = XZ + E, Z = J.

We can solve problem (6) using a type of ALM method. The corresponding augmented Lagrangian
function is

where Y1 and Y2 are Lagrange multipliers, and µ > 0 is a penalty parameter. Then we can apply the
alternating minimization idea to update one of the variables with the others ﬁxed.
1 , Y t

Given the current point Et, J t, Z t, Y t

2 , the updating scheme is:

L(E, J, Y1, Y2, Z, µ) = logdet(I + J T J) + λ(cid:107)E(cid:107)l
+ T r(Y T
2 (X − XZ − E))+
µ
2

F + (cid:107)X − XZ − E(cid:107)2

1 (J − Z)) + T r(Y T

((cid:107)J − Z(cid:107)2

F ),

Z t+1 = arg min

T r[(Y t

1 )T (J t − Z)] +

(cid:107)J t − Z(cid:107)2
F

+ T r[(Y t

2 )T (X − XZ − Et)] +
logdet(I + J T J)+

J t+1 = arg min

(cid:107)X − XZ − Et(cid:107)2
F ,

µt
2
µt
2

µt
2

(cid:107)J − (Z t+1 −

Y t
1
µt )(cid:107)2
F ,
λ||E||l + T r[(Y t

Et+1 = arg min

+

µt
2

||X − XZ t+1 − E||2
F .

2 )T (X − XZ t+1 − E)]

Z

J

E

The ﬁrst equation above has a closed-form solution:

Z t+1 = (I + X T X)−1[X T (X − Et) + J t +

Y t
1 + X T Y t
2
µt

].

For J updating, it can be converted to scalar minimization problems due to the following theorem [22],
which is also proved in the supplementary material.

Theorem 1. If F (Z) is a unitarily invariant function and SVD of A is A = U ΣAV T , then the optimal
solution to the following problem

min
Z

F (Z) +

(cid:107)Z − A(cid:107)2
F

β
2

3

(6)

(7)

(8)

(9)

Algorithm 1 Smoothed Rank Minimization
Input: data matrix X ∈ Rm×n, parameters λ > 0, µ0 > 0, γ > 1.
Initialize: J = I ∈ Rn×n, E = 0, Y1 = Y2 = 0.
REPEAT
1: Obtain Z through (8).
2: Update J as (12).
3: Solve E by either (13), (14), or (15) according to l.
4: Update the multipliers:

5: Update the parameter µt by µt+1 = γµt.
UNTIL stopping criterion is met.

Y t+1
1
Y t+1
2

= Y t
= Y t

1 + µt(J t+1 − Zt+1),
2 + µt(X − XZt+1 − Et+1).

is Z ∗ with SVD U Σ∗
Z = diag (σ∗); moreover, F (Z) = f ◦ σ(Z), where σ(Z) is the vector of
nonincreasing singular values of Z, then σ∗ is obtained by using the Moreau-Yosida proximity operator
σ∗ = proxf,β(σA), where σA := diag(ΣA), and

ZV T , where Σ∗

proxf,β(σA) := arg min

f (σ) +

(cid:107)σ − σA(cid:107)2
2.

σ

β
2

According to the ﬁrst-order optimality condition, the gradient of the objective function of (10) with

respect to each singular value should vanish. Thus we have

2σi
1 + σ2
i

+ βk(σi − σt

i,A) = 0, s.t. σi ≥ 0, f or i = 1, ..., n.

The above equation is cubic and gives three roots. If σt
it can be shown that there is a unique minimizer σ∗
satisﬁed, we adopt µ0 = 0.4 in our experiments. Finally, we obtain the update of J variable with

i will be 0; otherwise,
4. To ensure this requirement is

i,A = 0, the minimizer σ∗

i,A) if β > 1

i ∈ [0, σt

Depending on different regularization strategies, we have different closed-form solutions for E. For squared
Forbenius norm,

J t+1 = U diag(σt+1

1

, ..., σt+1

n )V T .

Et+1 =

2 + µt(X − XZ t+1)
Y t
µt + 2λ

.

For l1-norm, according to [23], if we deﬁne Q = X − XZ t+1 + Y t
as:

1

µt , then E can be updated element-wisely

For l2,1-norm, by [24], we have

Et+1

ij =

(cid:26) Qij − λ
0,

µt sgn(Qij),

if |Qij| < λ
µt ;
otherwise.

[Et+1]:,i =

(cid:40) (cid:107)Q:,i(cid:107)2− λ
µt
(cid:107)Q:,i(cid:107)2

Q:,i,

0,

if (cid:107)Q:,i(cid:107)2 > λ
µt ;
otherwise.

The complete procedure for solving problem (5) is summarized in Algorithm 1. Since our objective
function is nonconvex, it is difﬁcult to give a rigorous mathematical proof for convergence to an (local)
optimum. As we show in the supplementary material, our algorithm converges to an accumulation point
and this accumulation point is a stationary point. Our experiments conﬁrm the convergence of the proposed
method. The experimental results are promising, despite that the solution obtained by the proposed
optimization method may be a local optimum.

4

(10)

(11)

(12)

(13)

(14)

(15)

B. Subspace segmentation

After we obtain the coefﬁcient matrix Z ∗, we consider constructing a similarity graph matrix W from
it, since postprocessing of the coefﬁcient matrix often improves the clustering performance [13]. Using
the angular information based technique in [14], we deﬁne (cid:101)U = U (Σ)1/2, where U and Σ are from the
skinny SVD of Z ∗ = U ΣV T . Inspired by [25], we deﬁne W as:

Wij = (

(cid:101)uT
i (cid:101)uj
(cid:107)(cid:101)ui(cid:107)2 (cid:107)(cid:101)uj(cid:107)2

)2φ,

where (cid:101)ui and (cid:101)uj stand for the i-th and j-th columns of (cid:101)U , and φ ∈ N ∗ tunes the sharpness of the afﬁnity
between two points. However, an excessively large φ would break afﬁnities between points of the same
group. φ = 2 is used in our experiments, and thus we have the same post-processing procedure as LRR1.
After obtaining W , we directly utilize NCuts to cluster the samples.

5

(16)

Fig. 1. Sample face images in Extended Yale B.

IV. EXPERIMENT
In this section, we apply CLAR to subspace clustering on two benchmark databases: the Extended
Yale B database (EYaleB) [26] and the Hopkins 155 motion database [27]. CLAR is compared with the
state-of-the-art subspace clustering algorithms: SSC, LSA, LRR, and LRSC. The segmentation error rate is
used to evaluate the subspace clustering performance, which is deﬁned to be the percentage of erroneously
clustered samples versus the total number of samples in the data set being considered. The parameters
are tuned to achieve the best performance. In general, when the corruptions or noise are slight, the value
of λ should be relatively large. For our two experiments, λ = 3 × 10−4 and 67 are used. γ inﬂuences
the convergence speed, and we adopt γ = 1.1 as often done in literature. For fair comparison, we follow
experimental settings in [13]. We stop the program when a maximum of 100 iterations or a relative
difference of 10−5 is reached. The experiments are implemented on Intel Core i5 2.3GHz MacBook Pro
2011 with 4G memory. The code is available at: https://github.com/sckangz/logdet.

A. Face clustering

EYaleB consists of 2,414 frontal face images of 38 individuals under 64 lighting conditions. The task
is to cluster these images into their individual subspaces by identity. EYaleB is challenging for subspace
clustering due to large noise or corruptions, which can be seen from sample images in Figure 1. As [13],
we model noise with (cid:107)E(cid:107)1. Each image is resized to a 2016-dimensional vector. We divide the 38 subjects
into four groups, i.e., 1 to 10, 11 to 20, 21 to 30, and 31 to 38, and consider all choices of {2, 3, 5, 8}
for each group and all choices of n = 10 in the ﬁrst three groups. There will be {163, 416, 812, 136, 3}
datasets for each n, respectively.

Mean and median error rates for the datasets corresponding to each n are reported in Table I. It can
be seen that CLAR outperforms the other methods signiﬁcantly. As more subjects are involved, the error
rate of CLAR remains at a low level, while those of other methods increase drastically. In particular,
in the most challenging case of 10 subjects, the mean clustering error rate of CLAR is 3.85%, which
improves by 7.09% compared to the best result provided by SSC. This implies that our method is robust
to in-sample outliers. In Table I, we also observe that the clustering error rates of LSA are much larger
than other methods. This is potentially because LSA is based on MSE, which is heavily inﬂuenced

1As we conﬁrmed with an author of [14], the power 2 of equation (12) in [14] is a typo, which should be 4.

6

Fig. 2. Examples of recovery results of face images. The three columns from left to right are the original image (X), the error matrix (E)
and the recovered image (XZ), respectively.

by outliers. In addition, the advantage of our method is much more signiﬁcant with respect to other
low-rank representation based algorithms such as LRR and LRSC; for example, there is 11% and 19%
improvement over LRR in the cases of 8 and 10 subjects, respectively. This veriﬁes the importance of
good rank approximation.

Figure 2 shows some recovery results from the 10-subject clustering scenario. As we can see, the error

term E is indeed sparse and it helps remove the shadows.

TABLE I
CLUSTERING ERROR RATE (%) ON THE EYALEB DATASET.

METHOD

LRR

SSC

LSA

LRSC CLAR

2 SUBJECTS
MEAN
MEDIAN
3 SUBJECTS
MEAN
MEDIAN
5 SUBJECTS
MEAN
MEDIAN
8 SUBJECTS
MEAN
MEDIAN
10 SUBJECTS
MEAN
MEDIAN

2.54
0.78

4.21
2.60

6.90
5.63

14.34
10.06

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

32.80
47.66

52.29
50.00

5.32
4.69

8.47
7.81

58.02
56.87

12.24
11.25

59.19
58.59

23.72
28.03

22.92
23.59

10.94
5.63

60.42
57.50

30.36
28.75

1.27
0.78

1.92
1.56

2.64
2.19

3.36
3.03

3.85
3.44

Fig. 3. Sample images in Hopkins 155 database. Trackers are denoted by different colors.

7

B. Motion segmentation

In this subsection, we evaluate the robustness of CLAR for motion segmentation problem, which is
an important step in video sequences analysis. Given multiple image frames of a dynamic scene, motion
segmentation is to cluster the points in those views into different motions undertaken by the moving
objects. Hopkins 155 motion database contains 155 video sequences along with features extracted and
tracked in all frames for each sequence. Since the trajectories associated with each motion reside in a
distinct afﬁne subspace of dimension d ≤ 3, every motion corresponds to a subspace. Figure 3 gives some
sample images. (cid:107)E(cid:107)2
F is applied to model the noise. Table II shows the clustering results on the Hopkins

TABLE II
SEGMENTATION ERROR RATE (%) AND MEAN COMPUTATIONAL TIME (S) ON THE HOPKINS 155 DATASET.

METHOD

LRR SSC

LSA

LRSC CLAR

2 MOTIONS
MEAN
MEDIAN
3 MOTIONS
MEAN
MEDIAN
ALL
MEAN
MEDIAN
AVERAGE TIME

2.13
0.00

4.03
1.43

2.56
0.00
6.44

1.52
0.00

4.40
0.56

2.18
0.00
5.09

4.23
0.56

7.02
1.45

4.86
0.89
17.17

3.69
0.29

7.69
3.80

4.59
0.60
0.70

1.32
0.00

2.60
0.51

1.61
0.00
3.80

Fig. 4. The inﬂuence of the parameter λ of CLAR on all 155 sequences of Hopkins 155.

155 dataset. CLAR achieves the best results in all cases. Speciﬁcally, the average clustering error rate is
1.32% for two motions and 2.60% for three motions. We also show the computational time in Table II.
As we can see, our computational time is less than LRR, SSC and LSA, though more than LRSC. Figure
4 demonstrates the sensitivity of our algorithm to λ. It shows that the performance of CLAR is quite
stable while λ varies in a pretty large range. We also test γ with values 1.05 and 1.2 which do not give
much difference in error rate. Since our problem is nonconvex, we repeat the experiments using different
random initializations and we can still get similar results after tuning the parameters. Thus, CLAR appears
quite insensitive to initilizations.

V. CONCLUSION
In this paper, we study the matrix rank minimization problem with log-determinant approximation.
This surrogate can better approximate the rank function. As an application, we study its use for the
robust subspace clustering problem. A minimization algorithm, based on a type of augmented Lagrangian
multipliers method, is developed to optimize the associated nonconvex objective function. Extensive
experiments on the face clustering and motion segmentation demonstrate the effectiveness and robustness
of the proposed method, which shows superior performance when compared to the state-of-the-art subspace
clustering methods.

This work is supported by US National Science Foundation grants IIS 1218712.

ACKNOWLEDGMENT

8

REFERENCES

[1] E. J. Cand`es and B. Recht, “Exact matrix completion via convex optimization,” Foundations of Computational mathematics, vol. 9,

no. 6, pp. 717–772, 2009.

[2] G. Liu, Z. Lin, and Y. Yu, “Robust subspace segmentation by low-rank representation,” in Proceedings of the 27th International

Conference on Machine Learning (ICML-10), 2010, pp. 663–670.

[3] M. Fazel, “Matrix rank minimization with applications,” Ph.D. dissertation, PhD thesis, Stanford University, 2002.
[4] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization,”

[5] J.-F. Cai, E. J. Cand`es, and Z. Shen, “A singular value thresholding algorithm for matrix completion,” SIAM Journal on Optimization,

SIAM review, vol. 52, no. 3, pp. 471–501, 2010.

vol. 20, no. 4, pp. 1956–1982, 2010.

[6] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He, “Fast and accurate matrix completion via truncated nuclear norm regularization,” Pattern

Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 9, pp. 2117–2130, 2013.

[7] C. Lu, J. Tang, S. Y. Yan, and Z. Lin, “Generalized nonconvex nonsmooth low-rank minimization,” in IEEE International Conference

on Computer Vision and Pattern Recognition.

IEEE, 2014.

[8] M. Fazel, H. Hindi, and S. P. Boyd, “Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance

matrices,” in American Control Conference, 2003. Proceedings of the 2003, vol. 3.

IEEE, 2003, pp. 2156–2162.

[9] K. Mohan and M. Fazel, “Iterative reweighted algorithms for matrix rank minimization,” The Journal of Machine Learning Research,

[10] R. Vidal, “A tutorial on subspace clustering,” IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 52–68, 2010.
[11] J. Shi and J. Malik, “Normalized cuts and image segmentation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on,

vol. 13, no. 1, pp. 3441–3473, 2012.

vol. 22, no. 8, pp. 888–905, 2000.

[12] J. Yan and M. Pollefeys, “A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and

non-degenerate,” in Computer Vision–ECCV 2006. Springer, 2006, pp. 94–106.

[13] E. Elhamifar and R. Vidal, “Sparse subspace clustering: Algorithm, theory, and applications,” Pattern Analysis and Machine Intelligence,

IEEE Transactions on, vol. 35, no. 11, pp. 2765–2781, 2013.

[14] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of subspace structures by low-rank representation,” Pattern Analysis

and Machine Intelligence, IEEE Transactions on, vol. 35, no. 1, pp. 171–184, 2013.

[15] P. Favaro, R. Vidal, and A. Ravichandran, “A closed form solution to robust subspace estimation and clustering,” in Computer Vision

and Pattern Recognition (CVPR), 2011 IEEE Conference on.

IEEE, 2011, pp. 1801–1807.

[16] R. Vidal and P. Favaro, “Low rank subspace clustering (lrsc),” Pattern Recognition Letters, vol. 43, pp. 47–61, 2014.
[17] G. Liu, H. Xu, and S. Yan, “Exact subspace segmentation and outlier detection by low-rank representation,” in International Conference

on Artiﬁcial Intelligence and Statistics, 2012, pp. 703–711.

[18] Y.-X. Wang and H. Xu, “Noisy sparse subspace clustering,” in Proceedings of The 30th International Conference on Machine Learning,

[19] M. Soltanolkotabi, E. J. Candes et al., “A geometric analysis of subspace clustering with outliers,” The Annals of Statistics, vol. 40,

2013, pp. 89–97.

no. 4, pp. 2195–2238, 2012.

[20] B. Nasihatkon and R. Hartley, “Graph connectivity in sparse subspace clustering,” in Computer Vision and Pattern Recognition (CVPR),

2011 IEEE Conference on.

IEEE, 2011, pp. 2137–2144.

[21] H. Zhang, Z. Lin, and C. Zhang, “A counterexample for the validity of using nuclear norm as a convex surrogate of rank,” in Machine

Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 226–241.

[22] Z. Kang, C. Peng, J. Cheng, and Q. Cheng, “Logdet rank minimization with application to subspace clustering,” Computational

[23] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algorithm for linear inverse problems,” SIAM Journal on Imaging

Intelligence and Neuroscience, vol. 2015, 2015.

Sciences, vol. 2, no. 1, pp. 183–202, 2009.

[24] J. Yang, W. Yin, Y. Zhang, and Y. Wang, “A fast algorithm for edge-preserving variational multichannel image restoration,” SIAM

[25] F. Lauer and C. Schnorr, “Spectral clustering of linear subspaces for motion segmentation,” in Computer Vision, 2009 IEEE 12th

Journal on Imaging Sciences, vol. 2, no. 2, pp. 569–592, 2009.

International Conference on.

IEEE, 2009, pp. 678–685.

[26] K.-C. Lee, J. Ho, and D. Kriegman, “Acquiring linear subspaces for face recognition under variable lighting,” Pattern Analysis and

Machine Intelligence, IEEE Transactions on, vol. 27, no. 5, pp. 684–698, 2005.

[27] R. Tron and R. Vidal, “A benchmark for the comparison of 3-d motion segmentation algorithms,” in Computer Vision and Pattern
IEEE, 2007, pp. 1–8.

Recognition, 2007. CVPR’07. IEEE Conference on.

Robust Subspace Clustering via Smoothed Rank
Approximation
Zhao Kang, Chong Peng, and Qiang Cheng∗

Abstract

Matrix rank minimizing subject to afﬁne constraints arises in many application areas, ranging from signal
processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank
exactly under some restricted and theoretically interesting conditions. However, for many real-world applications,
nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution
of higher accuracy than the nuclear norm, in this paper, we propose a rank approximation based on Logarithm-
Determinant. We consider using this rank approximation for subspace clustering application. Our framework can
model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee
to converge to a stationary point. The proposed method gives promising results on face clustering and motion
segmentation tasks compared to the state-of-the-art subspace clustering algorithms.

Subspace clustering, Matrix rank minimization, Nuclear norm, Nonconvex optimization.

Index Terms

I. INTRODUCTION

R ECENTLY there has been a surge of interest in ﬁnding minimum rank matrix within an afﬁne

constraint set [1], [2]. The problem is as follows,

min
Z

rank(Z)

s.t A(Z) = b,

where Z ∈ Rm×n is the unknown matrix, A : Rm×n → Rp is a linear mapping, and b ∈ Rp denotes
the observations. Unfortunately, however, minimizing the rank of a matrix is known to be NP-hard and a
very challenging problem.

Consequently, a widely-used convex relaxation approach is to replace the rank function with the nuclear
norm (cid:107)Z(cid:107)∗ = (cid:80)n
i=1 σi(Z), where σi(Z) is the i-th singular value of Z (suppose n < m). The nuclear
norm technique has been shown to be effective in encouraging a low-rank solution [3], [4]. Nevertheless,
there is no guarantee for the minimum nuclear norm solution to coincide with that of minimal rank in
many interesting circumstances, which is heavily dependent on the singular values of matrices in the
nullspace of A. Some variations of the nuclear norm have demonstrated promising results, e.g., singular
value thresholding [5], and truncated nuclear norm [6].
Another popular alternative approach is to compute

5
1
0
2
 
g
u
A
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
4
4
0
.
8
0
5
1
:
v
i
X
r
a

min
Z

n
(cid:88)

i=1

h(σi(Z))

s.t. A(Z) = b,

where h is usually a nonconvex and nonsmooth function. It has been observed that nonconvex approach
can succeed in a broader range of scenarios [7]. However, nonconvex optimization is often challenging.
To overcome the above-mentioned difﬁculties, in this paper, we propose to use a particular log-

determinant (LogDet) function to approximate the rank function. The formulation we consider is:

F (Z) = logdet(I + Z T Z) =

log(1 + σ2

i (Z)),

n
(cid:88)

i=1

∗The authors are with Computer Science Department, Southern Illinois University, Carbondale, IL, USA, 62901. Correspondence should

be sent to qcheng@cs.siu.edu.

1

(1)

(2)

(3)

where I ∈ Rn×n is an identity matrix. For large nonzero singular values, the LogDet function value will be
much smaller than the nuclear norm. It is easy to show that logdet(I + Z T Z) ≤ (cid:107)Z(cid:107)∗. Therefore, LogDet
is a tighter rank approximation function than the nuclear norm. Although a similar function logdet(Z +δI)
has been proposed and iterative linearization has been used to ﬁnd a local minimum [8], its δ is required to
be small (e.g., 10−6), which leads to signiﬁcantly biased approximation for small singular values and thus
limited applications. Smoothed Schatten-p function, T r(Z T Z + γI)p/2, has been well studied in matrix
completion [9]; nonetheless, the resulting algorithm is rather sensitive to parameter γ.

The main contributions of this work are as follows: 1) An efﬁcient algorithm is devised to optimize
LogDet associated nonconvex objective function; 2) Our method pushes the accuracy of subspace clus-
tering to a new level.

II. PROBLEM STATEMENT OF SUBSPACE CLUSTERING
An important application of the proposed LogDet function is the low-rank representation based subspace
clustering problem. There has been signiﬁcant research effort on this subject over the past several years due
to its promising applications in computer vision and machine learning [10]. Subspace clustering aims at
ﬁnding a low-dimensional subspace for each group of points, which is based on the widely-used assumption
that high-dimensional data actually reside in a union of multiple low-dimensional subspaces. Under such
an assumption the data could be separated in a projected subspace. Consequently, subspace clustering
mainly involves two tasks, ﬁrstly projecting the data into a latent subspace to describe the afﬁnities of
points, and subsequently, grouping the data in that subspace. Some spectral clustering methods such as
normalized cuts (NCuts) [11] are usually used in the second task to ﬁnd the cluster membership. Besides
this spectral clustering-based subspace clustering method, iterative, algebraic, and statistical methods are
also available in the literature [10], but they are usually sensitive to initialization, noise or outliers.

Typical spectral clustering-based subspace clustering methods are Local Subspace Afﬁnity (LSA) [12],
Sparse Subspace Clustering (SSC) [13], Low Rank Representation (LRR) [2], [14] and its more robust
variant LRSC [15], [16]. Among them, SSC and LRR give promising results even in the presence of
large outliers or corruption [17], [18]. They both suppose that each data point can be written as a linear
combination of other points in the dataset. SSC tries to ﬁnd the sparsest representation of data points
through l1-norm. Even when the subspaces overlap, SSC can successfully reveal subspace structure [19].
SSC’s solution is sometimes too sparse to form a fully connected afﬁnity graph for data in a single
subspace [20]. LRR uses the lowest-rank representation to depict the similarity among data points. It is
theoretically guaranteed to succeed when the subspaces are independent.

Let X = [x1, x2, · · · , xn] ∈ Rm×n store a set of n m-dimensional samples drawn from a union of k

subspaces. LRR considers the following regularized nuclear norm rank minimization problem:

min
Z,E

(cid:107)Z(cid:107)∗ + λ(cid:107)E(cid:107)l

s.t. X = XZ + E,

where λ > 0 is a parameter, E represents unknown corruption, and (cid:107) · (cid:107)l can be l2,1-norm, l1-norm, or
squared Frobenius norm. Speciﬁcally, if random corruption is assumed in the data, (cid:107)E(cid:107)1 := (cid:80)m
j=1 |Eij|
is usually adopted; (cid:107)E(cid:107)2,1 := (cid:80)n
i=1 E2
ij is more suitable to characterize sample-speciﬁc corrup-
tions and outliers; (cid:107)E(cid:107)2
ij often describes Gaussian noise. LRR is able to produce pretty
competitive performance on subspace clustering in the current literature. However, the solution to it might
not be unique due to the nuclear norm [21]; and furthermore, the rank surrogate can deviate far from the
true rank function.

F := (cid:80)m

(cid:113)(cid:80)m

j=1 E2

j=1
(cid:80)n

(cid:80)n

i=1

i=1

To better approximate the rank while possessing the desired robustness similar to LRR, in this paper,

we propose to use the above-mentioned LogDet function and solve the following problem:

logdet(I + Z T Z) + λ(cid:107)E(cid:107)l

s.t. X = XZ + E.

min
Z,E

The objective function of (5) is nonconvex. We design an effective optimization strategy based on an
augmented Lagrangian multiplier (ALM) method, which is potentially applicable to large-scale data

2

(4)

(5)

because of the decomposability of ALM and its admittance to parallel algorithms. For our optimization
method, we provide theoretical analysis for its convergence, which mathematically guarantees that our
algorithm can produce a convergent subsequence and the converged point is a stationary point of (5).

III. PROPOSED METHOD: CLAR

In this section, we present the proposed robust subspace clustering algorithm CLAR: Clustering with
Log-determinant Approximation to Rank. The basic theorems and optimization algorithm will be presented
below.

A. Smoothed rank minimization

by introducing an auxiliary variable J:

To make the objective function in (5) separable, we ﬁrst convert it to the following equivalent problem

min
E,J,Z

logdet(I + J T J) + λ||E||l s.t. X = XZ + E, Z = J.

We can solve problem (6) using a type of ALM method. The corresponding augmented Lagrangian
function is

where Y1 and Y2 are Lagrange multipliers, and µ > 0 is a penalty parameter. Then we can apply the
alternating minimization idea to update one of the variables with the others ﬁxed.
1 , Y t

Given the current point Et, J t, Z t, Y t

2 , the updating scheme is:

L(E, J, Y1, Y2, Z, µ) = logdet(I + J T J) + λ(cid:107)E(cid:107)l
+ T r(Y T
2 (X − XZ − E))+
µ
2

F + (cid:107)X − XZ − E(cid:107)2

1 (J − Z)) + T r(Y T

((cid:107)J − Z(cid:107)2

F ),

Z t+1 = arg min

T r[(Y t

1 )T (J t − Z)] +

(cid:107)J t − Z(cid:107)2
F

+ T r[(Y t

2 )T (X − XZ − Et)] +
logdet(I + J T J)+

J t+1 = arg min

(cid:107)X − XZ − Et(cid:107)2
F ,

µt
2
µt
2

µt
2

(cid:107)J − (Z t+1 −

Y t
1
µt )(cid:107)2
F ,
λ||E||l + T r[(Y t

Et+1 = arg min

+

µt
2

||X − XZ t+1 − E||2
F .

2 )T (X − XZ t+1 − E)]

Z

J

E

The ﬁrst equation above has a closed-form solution:

Z t+1 = (I + X T X)−1[X T (X − Et) + J t +

Y t
1 + X T Y t
2
µt

].

For J updating, it can be converted to scalar minimization problems due to the following theorem [22],
which is also proved in the supplementary material.

Theorem 1. If F (Z) is a unitarily invariant function and SVD of A is A = U ΣAV T , then the optimal
solution to the following problem

min
Z

F (Z) +

(cid:107)Z − A(cid:107)2
F

β
2

3

(6)

(7)

(8)

(9)

Algorithm 1 Smoothed Rank Minimization
Input: data matrix X ∈ Rm×n, parameters λ > 0, µ0 > 0, γ > 1.
Initialize: J = I ∈ Rn×n, E = 0, Y1 = Y2 = 0.
REPEAT
1: Obtain Z through (8).
2: Update J as (12).
3: Solve E by either (13), (14), or (15) according to l.
4: Update the multipliers:

5: Update the parameter µt by µt+1 = γµt.
UNTIL stopping criterion is met.

Y t+1
1
Y t+1
2

= Y t
= Y t

1 + µt(J t+1 − Zt+1),
2 + µt(X − XZt+1 − Et+1).

is Z ∗ with SVD U Σ∗
Z = diag (σ∗); moreover, F (Z) = f ◦ σ(Z), where σ(Z) is the vector of
nonincreasing singular values of Z, then σ∗ is obtained by using the Moreau-Yosida proximity operator
σ∗ = proxf,β(σA), where σA := diag(ΣA), and

ZV T , where Σ∗

proxf,β(σA) := arg min

f (σ) +

(cid:107)σ − σA(cid:107)2
2.

σ

β
2

According to the ﬁrst-order optimality condition, the gradient of the objective function of (10) with

respect to each singular value should vanish. Thus we have

2σi
1 + σ2
i

+ βk(σi − σt

i,A) = 0, s.t. σi ≥ 0, f or i = 1, ..., n.

The above equation is cubic and gives three roots. If σt
it can be shown that there is a unique minimizer σ∗
satisﬁed, we adopt µ0 = 0.4 in our experiments. Finally, we obtain the update of J variable with

i will be 0; otherwise,
4. To ensure this requirement is

i,A = 0, the minimizer σ∗

i,A) if β > 1

i ∈ [0, σt

Depending on different regularization strategies, we have different closed-form solutions for E. For squared
Forbenius norm,

J t+1 = U diag(σt+1

1

, ..., σt+1

n )V T .

Et+1 =

2 + µt(X − XZ t+1)
Y t
µt + 2λ

.

For l1-norm, according to [23], if we deﬁne Q = X − XZ t+1 + Y t
as:

1

µt , then E can be updated element-wisely

For l2,1-norm, by [24], we have

Et+1

ij =

(cid:26) Qij − λ
0,

µt sgn(Qij),

if |Qij| < λ
µt ;
otherwise.

[Et+1]:,i =

(cid:40) (cid:107)Q:,i(cid:107)2− λ
µt
(cid:107)Q:,i(cid:107)2

Q:,i,

0,

if (cid:107)Q:,i(cid:107)2 > λ
µt ;
otherwise.

The complete procedure for solving problem (5) is summarized in Algorithm 1. Since our objective
function is nonconvex, it is difﬁcult to give a rigorous mathematical proof for convergence to an (local)
optimum. As we show in the supplementary material, our algorithm converges to an accumulation point
and this accumulation point is a stationary point. Our experiments conﬁrm the convergence of the proposed
method. The experimental results are promising, despite that the solution obtained by the proposed
optimization method may be a local optimum.

4

(10)

(11)

(12)

(13)

(14)

(15)

B. Subspace segmentation

After we obtain the coefﬁcient matrix Z ∗, we consider constructing a similarity graph matrix W from
it, since postprocessing of the coefﬁcient matrix often improves the clustering performance [13]. Using
the angular information based technique in [14], we deﬁne (cid:101)U = U (Σ)1/2, where U and Σ are from the
skinny SVD of Z ∗ = U ΣV T . Inspired by [25], we deﬁne W as:

Wij = (

(cid:101)uT
i (cid:101)uj
(cid:107)(cid:101)ui(cid:107)2 (cid:107)(cid:101)uj(cid:107)2

)2φ,

where (cid:101)ui and (cid:101)uj stand for the i-th and j-th columns of (cid:101)U , and φ ∈ N ∗ tunes the sharpness of the afﬁnity
between two points. However, an excessively large φ would break afﬁnities between points of the same
group. φ = 2 is used in our experiments, and thus we have the same post-processing procedure as LRR1.
After obtaining W , we directly utilize NCuts to cluster the samples.

5

(16)

Fig. 1. Sample face images in Extended Yale B.

IV. EXPERIMENT
In this section, we apply CLAR to subspace clustering on two benchmark databases: the Extended
Yale B database (EYaleB) [26] and the Hopkins 155 motion database [27]. CLAR is compared with the
state-of-the-art subspace clustering algorithms: SSC, LSA, LRR, and LRSC. The segmentation error rate is
used to evaluate the subspace clustering performance, which is deﬁned to be the percentage of erroneously
clustered samples versus the total number of samples in the data set being considered. The parameters
are tuned to achieve the best performance. In general, when the corruptions or noise are slight, the value
of λ should be relatively large. For our two experiments, λ = 3 × 10−4 and 67 are used. γ inﬂuences
the convergence speed, and we adopt γ = 1.1 as often done in literature. For fair comparison, we follow
experimental settings in [13]. We stop the program when a maximum of 100 iterations or a relative
difference of 10−5 is reached. The experiments are implemented on Intel Core i5 2.3GHz MacBook Pro
2011 with 4G memory. The code is available at: https://github.com/sckangz/logdet.

A. Face clustering

EYaleB consists of 2,414 frontal face images of 38 individuals under 64 lighting conditions. The task
is to cluster these images into their individual subspaces by identity. EYaleB is challenging for subspace
clustering due to large noise or corruptions, which can be seen from sample images in Figure 1. As [13],
we model noise with (cid:107)E(cid:107)1. Each image is resized to a 2016-dimensional vector. We divide the 38 subjects
into four groups, i.e., 1 to 10, 11 to 20, 21 to 30, and 31 to 38, and consider all choices of {2, 3, 5, 8}
for each group and all choices of n = 10 in the ﬁrst three groups. There will be {163, 416, 812, 136, 3}
datasets for each n, respectively.

Mean and median error rates for the datasets corresponding to each n are reported in Table I. It can
be seen that CLAR outperforms the other methods signiﬁcantly. As more subjects are involved, the error
rate of CLAR remains at a low level, while those of other methods increase drastically. In particular,
in the most challenging case of 10 subjects, the mean clustering error rate of CLAR is 3.85%, which
improves by 7.09% compared to the best result provided by SSC. This implies that our method is robust
to in-sample outliers. In Table I, we also observe that the clustering error rates of LSA are much larger
than other methods. This is potentially because LSA is based on MSE, which is heavily inﬂuenced

1As we conﬁrmed with an author of [14], the power 2 of equation (12) in [14] is a typo, which should be 4.

6

Fig. 2. Examples of recovery results of face images. The three columns from left to right are the original image (X), the error matrix (E)
and the recovered image (XZ), respectively.

by outliers. In addition, the advantage of our method is much more signiﬁcant with respect to other
low-rank representation based algorithms such as LRR and LRSC; for example, there is 11% and 19%
improvement over LRR in the cases of 8 and 10 subjects, respectively. This veriﬁes the importance of
good rank approximation.

Figure 2 shows some recovery results from the 10-subject clustering scenario. As we can see, the error

term E is indeed sparse and it helps remove the shadows.

TABLE I
CLUSTERING ERROR RATE (%) ON THE EYALEB DATASET.

METHOD

LRR

SSC

LSA

LRSC CLAR

2 SUBJECTS
MEAN
MEDIAN
3 SUBJECTS
MEAN
MEDIAN
5 SUBJECTS
MEAN
MEDIAN
8 SUBJECTS
MEAN
MEDIAN
10 SUBJECTS
MEAN
MEDIAN

2.54
0.78

4.21
2.60

6.90
5.63

14.34
10.06

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

32.80
47.66

52.29
50.00

5.32
4.69

8.47
7.81

58.02
56.87

12.24
11.25

59.19
58.59

23.72
28.03

22.92
23.59

10.94
5.63

60.42
57.50

30.36
28.75

1.27
0.78

1.92
1.56

2.64
2.19

3.36
3.03

3.85
3.44

Fig. 3. Sample images in Hopkins 155 database. Trackers are denoted by different colors.

7

B. Motion segmentation

In this subsection, we evaluate the robustness of CLAR for motion segmentation problem, which is
an important step in video sequences analysis. Given multiple image frames of a dynamic scene, motion
segmentation is to cluster the points in those views into different motions undertaken by the moving
objects. Hopkins 155 motion database contains 155 video sequences along with features extracted and
tracked in all frames for each sequence. Since the trajectories associated with each motion reside in a
distinct afﬁne subspace of dimension d ≤ 3, every motion corresponds to a subspace. Figure 3 gives some
sample images. (cid:107)E(cid:107)2
F is applied to model the noise. Table II shows the clustering results on the Hopkins

TABLE II
SEGMENTATION ERROR RATE (%) AND MEAN COMPUTATIONAL TIME (S) ON THE HOPKINS 155 DATASET.

METHOD

LRR SSC

LSA

LRSC CLAR

2 MOTIONS
MEAN
MEDIAN
3 MOTIONS
MEAN
MEDIAN
ALL
MEAN
MEDIAN
AVERAGE TIME

2.13
0.00

4.03
1.43

2.56
0.00
6.44

1.52
0.00

4.40
0.56

2.18
0.00
5.09

4.23
0.56

7.02
1.45

4.86
0.89
17.17

3.69
0.29

7.69
3.80

4.59
0.60
0.70

1.32
0.00

2.60
0.51

1.61
0.00
3.80

Fig. 4. The inﬂuence of the parameter λ of CLAR on all 155 sequences of Hopkins 155.

155 dataset. CLAR achieves the best results in all cases. Speciﬁcally, the average clustering error rate is
1.32% for two motions and 2.60% for three motions. We also show the computational time in Table II.
As we can see, our computational time is less than LRR, SSC and LSA, though more than LRSC. Figure
4 demonstrates the sensitivity of our algorithm to λ. It shows that the performance of CLAR is quite
stable while λ varies in a pretty large range. We also test γ with values 1.05 and 1.2 which do not give
much difference in error rate. Since our problem is nonconvex, we repeat the experiments using different
random initializations and we can still get similar results after tuning the parameters. Thus, CLAR appears
quite insensitive to initilizations.

V. CONCLUSION
In this paper, we study the matrix rank minimization problem with log-determinant approximation.
This surrogate can better approximate the rank function. As an application, we study its use for the
robust subspace clustering problem. A minimization algorithm, based on a type of augmented Lagrangian
multipliers method, is developed to optimize the associated nonconvex objective function. Extensive
experiments on the face clustering and motion segmentation demonstrate the effectiveness and robustness
of the proposed method, which shows superior performance when compared to the state-of-the-art subspace
clustering methods.

This work is supported by US National Science Foundation grants IIS 1218712.

ACKNOWLEDGMENT

8

REFERENCES

[1] E. J. Cand`es and B. Recht, “Exact matrix completion via convex optimization,” Foundations of Computational mathematics, vol. 9,

no. 6, pp. 717–772, 2009.

[2] G. Liu, Z. Lin, and Y. Yu, “Robust subspace segmentation by low-rank representation,” in Proceedings of the 27th International

Conference on Machine Learning (ICML-10), 2010, pp. 663–670.

[3] M. Fazel, “Matrix rank minimization with applications,” Ph.D. dissertation, PhD thesis, Stanford University, 2002.
[4] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization,”

[5] J.-F. Cai, E. J. Cand`es, and Z. Shen, “A singular value thresholding algorithm for matrix completion,” SIAM Journal on Optimization,

SIAM review, vol. 52, no. 3, pp. 471–501, 2010.

vol. 20, no. 4, pp. 1956–1982, 2010.

[6] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He, “Fast and accurate matrix completion via truncated nuclear norm regularization,” Pattern

Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 9, pp. 2117–2130, 2013.

[7] C. Lu, J. Tang, S. Y. Yan, and Z. Lin, “Generalized nonconvex nonsmooth low-rank minimization,” in IEEE International Conference

on Computer Vision and Pattern Recognition.

IEEE, 2014.

[8] M. Fazel, H. Hindi, and S. P. Boyd, “Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance

matrices,” in American Control Conference, 2003. Proceedings of the 2003, vol. 3.

IEEE, 2003, pp. 2156–2162.

[9] K. Mohan and M. Fazel, “Iterative reweighted algorithms for matrix rank minimization,” The Journal of Machine Learning Research,

[10] R. Vidal, “A tutorial on subspace clustering,” IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 52–68, 2010.
[11] J. Shi and J. Malik, “Normalized cuts and image segmentation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on,

vol. 13, no. 1, pp. 3441–3473, 2012.

vol. 22, no. 8, pp. 888–905, 2000.

[12] J. Yan and M. Pollefeys, “A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and

non-degenerate,” in Computer Vision–ECCV 2006. Springer, 2006, pp. 94–106.

[13] E. Elhamifar and R. Vidal, “Sparse subspace clustering: Algorithm, theory, and applications,” Pattern Analysis and Machine Intelligence,

IEEE Transactions on, vol. 35, no. 11, pp. 2765–2781, 2013.

[14] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of subspace structures by low-rank representation,” Pattern Analysis

and Machine Intelligence, IEEE Transactions on, vol. 35, no. 1, pp. 171–184, 2013.

[15] P. Favaro, R. Vidal, and A. Ravichandran, “A closed form solution to robust subspace estimation and clustering,” in Computer Vision

and Pattern Recognition (CVPR), 2011 IEEE Conference on.

IEEE, 2011, pp. 1801–1807.

[16] R. Vidal and P. Favaro, “Low rank subspace clustering (lrsc),” Pattern Recognition Letters, vol. 43, pp. 47–61, 2014.
[17] G. Liu, H. Xu, and S. Yan, “Exact subspace segmentation and outlier detection by low-rank representation,” in International Conference

on Artiﬁcial Intelligence and Statistics, 2012, pp. 703–711.

[18] Y.-X. Wang and H. Xu, “Noisy sparse subspace clustering,” in Proceedings of The 30th International Conference on Machine Learning,

[19] M. Soltanolkotabi, E. J. Candes et al., “A geometric analysis of subspace clustering with outliers,” The Annals of Statistics, vol. 40,

2013, pp. 89–97.

no. 4, pp. 2195–2238, 2012.

[20] B. Nasihatkon and R. Hartley, “Graph connectivity in sparse subspace clustering,” in Computer Vision and Pattern Recognition (CVPR),

2011 IEEE Conference on.

IEEE, 2011, pp. 2137–2144.

[21] H. Zhang, Z. Lin, and C. Zhang, “A counterexample for the validity of using nuclear norm as a convex surrogate of rank,” in Machine

Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 226–241.

[22] Z. Kang, C. Peng, J. Cheng, and Q. Cheng, “Logdet rank minimization with application to subspace clustering,” Computational

[23] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algorithm for linear inverse problems,” SIAM Journal on Imaging

Intelligence and Neuroscience, vol. 2015, 2015.

Sciences, vol. 2, no. 1, pp. 183–202, 2009.

[24] J. Yang, W. Yin, Y. Zhang, and Y. Wang, “A fast algorithm for edge-preserving variational multichannel image restoration,” SIAM

[25] F. Lauer and C. Schnorr, “Spectral clustering of linear subspaces for motion segmentation,” in Computer Vision, 2009 IEEE 12th

Journal on Imaging Sciences, vol. 2, no. 2, pp. 569–592, 2009.

International Conference on.

IEEE, 2009, pp. 678–685.

[26] K.-C. Lee, J. Ho, and D. Kriegman, “Acquiring linear subspaces for face recognition under variable lighting,” Pattern Analysis and

Machine Intelligence, IEEE Transactions on, vol. 27, no. 5, pp. 684–698, 2005.

[27] R. Tron and R. Vidal, “A benchmark for the comparison of 3-d motion segmentation algorithms,” in Computer Vision and Pattern
IEEE, 2007, pp. 1–8.

Recognition, 2007. CVPR’07. IEEE Conference on.

Robust Subspace Clustering via Smoothed Rank
Approximation
Zhao Kang, Chong Peng, and Qiang Cheng∗

Abstract

Matrix rank minimizing subject to afﬁne constraints arises in many application areas, ranging from signal
processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank
exactly under some restricted and theoretically interesting conditions. However, for many real-world applications,
nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution
of higher accuracy than the nuclear norm, in this paper, we propose a rank approximation based on Logarithm-
Determinant. We consider using this rank approximation for subspace clustering application. Our framework can
model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee
to converge to a stationary point. The proposed method gives promising results on face clustering and motion
segmentation tasks compared to the state-of-the-art subspace clustering algorithms.

Subspace clustering, Matrix rank minimization, Nuclear norm, Nonconvex optimization.

Index Terms

I. INTRODUCTION

R ECENTLY there has been a surge of interest in ﬁnding minimum rank matrix within an afﬁne

constraint set [1], [2]. The problem is as follows,

min
Z

rank(Z)

s.t A(Z) = b,

where Z ∈ Rm×n is the unknown matrix, A : Rm×n → Rp is a linear mapping, and b ∈ Rp denotes
the observations. Unfortunately, however, minimizing the rank of a matrix is known to be NP-hard and a
very challenging problem.

Consequently, a widely-used convex relaxation approach is to replace the rank function with the nuclear
norm (cid:107)Z(cid:107)∗ = (cid:80)n
i=1 σi(Z), where σi(Z) is the i-th singular value of Z (suppose n < m). The nuclear
norm technique has been shown to be effective in encouraging a low-rank solution [3], [4]. Nevertheless,
there is no guarantee for the minimum nuclear norm solution to coincide with that of minimal rank in
many interesting circumstances, which is heavily dependent on the singular values of matrices in the
nullspace of A. Some variations of the nuclear norm have demonstrated promising results, e.g., singular
value thresholding [5], and truncated nuclear norm [6].
Another popular alternative approach is to compute

5
1
0
2
 
g
u
A
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
4
4
0
.
8
0
5
1
:
v
i
X
r
a

min
Z

n
(cid:88)

i=1

h(σi(Z))

s.t. A(Z) = b,

where h is usually a nonconvex and nonsmooth function. It has been observed that nonconvex approach
can succeed in a broader range of scenarios [7]. However, nonconvex optimization is often challenging.
To overcome the above-mentioned difﬁculties, in this paper, we propose to use a particular log-

determinant (LogDet) function to approximate the rank function. The formulation we consider is:

F (Z) = logdet(I + Z T Z) =

log(1 + σ2

i (Z)),

n
(cid:88)

i=1

∗The authors are with Computer Science Department, Southern Illinois University, Carbondale, IL, USA, 62901. Correspondence should

be sent to qcheng@cs.siu.edu.

1

(1)

(2)

(3)

where I ∈ Rn×n is an identity matrix. For large nonzero singular values, the LogDet function value will be
much smaller than the nuclear norm. It is easy to show that logdet(I + Z T Z) ≤ (cid:107)Z(cid:107)∗. Therefore, LogDet
is a tighter rank approximation function than the nuclear norm. Although a similar function logdet(Z +δI)
has been proposed and iterative linearization has been used to ﬁnd a local minimum [8], its δ is required to
be small (e.g., 10−6), which leads to signiﬁcantly biased approximation for small singular values and thus
limited applications. Smoothed Schatten-p function, T r(Z T Z + γI)p/2, has been well studied in matrix
completion [9]; nonetheless, the resulting algorithm is rather sensitive to parameter γ.

The main contributions of this work are as follows: 1) An efﬁcient algorithm is devised to optimize
LogDet associated nonconvex objective function; 2) Our method pushes the accuracy of subspace clus-
tering to a new level.

II. PROBLEM STATEMENT OF SUBSPACE CLUSTERING
An important application of the proposed LogDet function is the low-rank representation based subspace
clustering problem. There has been signiﬁcant research effort on this subject over the past several years due
to its promising applications in computer vision and machine learning [10]. Subspace clustering aims at
ﬁnding a low-dimensional subspace for each group of points, which is based on the widely-used assumption
that high-dimensional data actually reside in a union of multiple low-dimensional subspaces. Under such
an assumption the data could be separated in a projected subspace. Consequently, subspace clustering
mainly involves two tasks, ﬁrstly projecting the data into a latent subspace to describe the afﬁnities of
points, and subsequently, grouping the data in that subspace. Some spectral clustering methods such as
normalized cuts (NCuts) [11] are usually used in the second task to ﬁnd the cluster membership. Besides
this spectral clustering-based subspace clustering method, iterative, algebraic, and statistical methods are
also available in the literature [10], but they are usually sensitive to initialization, noise or outliers.

Typical spectral clustering-based subspace clustering methods are Local Subspace Afﬁnity (LSA) [12],
Sparse Subspace Clustering (SSC) [13], Low Rank Representation (LRR) [2], [14] and its more robust
variant LRSC [15], [16]. Among them, SSC and LRR give promising results even in the presence of
large outliers or corruption [17], [18]. They both suppose that each data point can be written as a linear
combination of other points in the dataset. SSC tries to ﬁnd the sparsest representation of data points
through l1-norm. Even when the subspaces overlap, SSC can successfully reveal subspace structure [19].
SSC’s solution is sometimes too sparse to form a fully connected afﬁnity graph for data in a single
subspace [20]. LRR uses the lowest-rank representation to depict the similarity among data points. It is
theoretically guaranteed to succeed when the subspaces are independent.

Let X = [x1, x2, · · · , xn] ∈ Rm×n store a set of n m-dimensional samples drawn from a union of k

subspaces. LRR considers the following regularized nuclear norm rank minimization problem:

min
Z,E

(cid:107)Z(cid:107)∗ + λ(cid:107)E(cid:107)l

s.t. X = XZ + E,

where λ > 0 is a parameter, E represents unknown corruption, and (cid:107) · (cid:107)l can be l2,1-norm, l1-norm, or
squared Frobenius norm. Speciﬁcally, if random corruption is assumed in the data, (cid:107)E(cid:107)1 := (cid:80)m
j=1 |Eij|
is usually adopted; (cid:107)E(cid:107)2,1 := (cid:80)n
i=1 E2
ij is more suitable to characterize sample-speciﬁc corrup-
tions and outliers; (cid:107)E(cid:107)2
ij often describes Gaussian noise. LRR is able to produce pretty
competitive performance on subspace clustering in the current literature. However, the solution to it might
not be unique due to the nuclear norm [21]; and furthermore, the rank surrogate can deviate far from the
true rank function.

F := (cid:80)m

(cid:113)(cid:80)m

j=1 E2

j=1
(cid:80)n

(cid:80)n

i=1

i=1

To better approximate the rank while possessing the desired robustness similar to LRR, in this paper,

we propose to use the above-mentioned LogDet function and solve the following problem:

logdet(I + Z T Z) + λ(cid:107)E(cid:107)l

s.t. X = XZ + E.

min
Z,E

The objective function of (5) is nonconvex. We design an effective optimization strategy based on an
augmented Lagrangian multiplier (ALM) method, which is potentially applicable to large-scale data

2

(4)

(5)

because of the decomposability of ALM and its admittance to parallel algorithms. For our optimization
method, we provide theoretical analysis for its convergence, which mathematically guarantees that our
algorithm can produce a convergent subsequence and the converged point is a stationary point of (5).

III. PROPOSED METHOD: CLAR

In this section, we present the proposed robust subspace clustering algorithm CLAR: Clustering with
Log-determinant Approximation to Rank. The basic theorems and optimization algorithm will be presented
below.

A. Smoothed rank minimization

by introducing an auxiliary variable J:

To make the objective function in (5) separable, we ﬁrst convert it to the following equivalent problem

min
E,J,Z

logdet(I + J T J) + λ||E||l s.t. X = XZ + E, Z = J.

We can solve problem (6) using a type of ALM method. The corresponding augmented Lagrangian
function is

where Y1 and Y2 are Lagrange multipliers, and µ > 0 is a penalty parameter. Then we can apply the
alternating minimization idea to update one of the variables with the others ﬁxed.
1 , Y t

Given the current point Et, J t, Z t, Y t

2 , the updating scheme is:

L(E, J, Y1, Y2, Z, µ) = logdet(I + J T J) + λ(cid:107)E(cid:107)l
+ T r(Y T
2 (X − XZ − E))+
µ
2

F + (cid:107)X − XZ − E(cid:107)2

1 (J − Z)) + T r(Y T

((cid:107)J − Z(cid:107)2

F ),

Z t+1 = arg min

T r[(Y t

1 )T (J t − Z)] +

(cid:107)J t − Z(cid:107)2
F

+ T r[(Y t

2 )T (X − XZ − Et)] +
logdet(I + J T J)+

J t+1 = arg min

(cid:107)X − XZ − Et(cid:107)2
F ,

µt
2
µt
2

µt
2

(cid:107)J − (Z t+1 −

Y t
1
µt )(cid:107)2
F ,
λ||E||l + T r[(Y t

Et+1 = arg min

+

µt
2

||X − XZ t+1 − E||2
F .

2 )T (X − XZ t+1 − E)]

Z

J

E

The ﬁrst equation above has a closed-form solution:

Z t+1 = (I + X T X)−1[X T (X − Et) + J t +

Y t
1 + X T Y t
2
µt

].

For J updating, it can be converted to scalar minimization problems due to the following theorem [22],
which is also proved in the supplementary material.

Theorem 1. If F (Z) is a unitarily invariant function and SVD of A is A = U ΣAV T , then the optimal
solution to the following problem

min
Z

F (Z) +

(cid:107)Z − A(cid:107)2
F

β
2

3

(6)

(7)

(8)

(9)

Algorithm 1 Smoothed Rank Minimization
Input: data matrix X ∈ Rm×n, parameters λ > 0, µ0 > 0, γ > 1.
Initialize: J = I ∈ Rn×n, E = 0, Y1 = Y2 = 0.
REPEAT
1: Obtain Z through (8).
2: Update J as (12).
3: Solve E by either (13), (14), or (15) according to l.
4: Update the multipliers:

5: Update the parameter µt by µt+1 = γµt.
UNTIL stopping criterion is met.

Y t+1
1
Y t+1
2

= Y t
= Y t

1 + µt(J t+1 − Zt+1),
2 + µt(X − XZt+1 − Et+1).

is Z ∗ with SVD U Σ∗
Z = diag (σ∗); moreover, F (Z) = f ◦ σ(Z), where σ(Z) is the vector of
nonincreasing singular values of Z, then σ∗ is obtained by using the Moreau-Yosida proximity operator
σ∗ = proxf,β(σA), where σA := diag(ΣA), and

ZV T , where Σ∗

proxf,β(σA) := arg min

f (σ) +

(cid:107)σ − σA(cid:107)2
2.

σ

β
2

According to the ﬁrst-order optimality condition, the gradient of the objective function of (10) with

respect to each singular value should vanish. Thus we have

2σi
1 + σ2
i

+ βk(σi − σt

i,A) = 0, s.t. σi ≥ 0, f or i = 1, ..., n.

The above equation is cubic and gives three roots. If σt
it can be shown that there is a unique minimizer σ∗
satisﬁed, we adopt µ0 = 0.4 in our experiments. Finally, we obtain the update of J variable with

i will be 0; otherwise,
4. To ensure this requirement is

i,A = 0, the minimizer σ∗

i,A) if β > 1

i ∈ [0, σt

Depending on different regularization strategies, we have different closed-form solutions for E. For squared
Forbenius norm,

J t+1 = U diag(σt+1

1

, ..., σt+1

n )V T .

Et+1 =

2 + µt(X − XZ t+1)
Y t
µt + 2λ

.

For l1-norm, according to [23], if we deﬁne Q = X − XZ t+1 + Y t
as:

1

µt , then E can be updated element-wisely

For l2,1-norm, by [24], we have

Et+1

ij =

(cid:26) Qij − λ
0,

µt sgn(Qij),

if |Qij| < λ
µt ;
otherwise.

[Et+1]:,i =

(cid:40) (cid:107)Q:,i(cid:107)2− λ
µt
(cid:107)Q:,i(cid:107)2

Q:,i,

0,

if (cid:107)Q:,i(cid:107)2 > λ
µt ;
otherwise.

The complete procedure for solving problem (5) is summarized in Algorithm 1. Since our objective
function is nonconvex, it is difﬁcult to give a rigorous mathematical proof for convergence to an (local)
optimum. As we show in the supplementary material, our algorithm converges to an accumulation point
and this accumulation point is a stationary point. Our experiments conﬁrm the convergence of the proposed
method. The experimental results are promising, despite that the solution obtained by the proposed
optimization method may be a local optimum.

4

(10)

(11)

(12)

(13)

(14)

(15)

B. Subspace segmentation

After we obtain the coefﬁcient matrix Z ∗, we consider constructing a similarity graph matrix W from
it, since postprocessing of the coefﬁcient matrix often improves the clustering performance [13]. Using
the angular information based technique in [14], we deﬁne (cid:101)U = U (Σ)1/2, where U and Σ are from the
skinny SVD of Z ∗ = U ΣV T . Inspired by [25], we deﬁne W as:

Wij = (

(cid:101)uT
i (cid:101)uj
(cid:107)(cid:101)ui(cid:107)2 (cid:107)(cid:101)uj(cid:107)2

)2φ,

where (cid:101)ui and (cid:101)uj stand for the i-th and j-th columns of (cid:101)U , and φ ∈ N ∗ tunes the sharpness of the afﬁnity
between two points. However, an excessively large φ would break afﬁnities between points of the same
group. φ = 2 is used in our experiments, and thus we have the same post-processing procedure as LRR1.
After obtaining W , we directly utilize NCuts to cluster the samples.

5

(16)

Fig. 1. Sample face images in Extended Yale B.

IV. EXPERIMENT
In this section, we apply CLAR to subspace clustering on two benchmark databases: the Extended
Yale B database (EYaleB) [26] and the Hopkins 155 motion database [27]. CLAR is compared with the
state-of-the-art subspace clustering algorithms: SSC, LSA, LRR, and LRSC. The segmentation error rate is
used to evaluate the subspace clustering performance, which is deﬁned to be the percentage of erroneously
clustered samples versus the total number of samples in the data set being considered. The parameters
are tuned to achieve the best performance. In general, when the corruptions or noise are slight, the value
of λ should be relatively large. For our two experiments, λ = 3 × 10−4 and 67 are used. γ inﬂuences
the convergence speed, and we adopt γ = 1.1 as often done in literature. For fair comparison, we follow
experimental settings in [13]. We stop the program when a maximum of 100 iterations or a relative
difference of 10−5 is reached. The experiments are implemented on Intel Core i5 2.3GHz MacBook Pro
2011 with 4G memory. The code is available at: https://github.com/sckangz/logdet.

A. Face clustering

EYaleB consists of 2,414 frontal face images of 38 individuals under 64 lighting conditions. The task
is to cluster these images into their individual subspaces by identity. EYaleB is challenging for subspace
clustering due to large noise or corruptions, which can be seen from sample images in Figure 1. As [13],
we model noise with (cid:107)E(cid:107)1. Each image is resized to a 2016-dimensional vector. We divide the 38 subjects
into four groups, i.e., 1 to 10, 11 to 20, 21 to 30, and 31 to 38, and consider all choices of {2, 3, 5, 8}
for each group and all choices of n = 10 in the ﬁrst three groups. There will be {163, 416, 812, 136, 3}
datasets for each n, respectively.

Mean and median error rates for the datasets corresponding to each n are reported in Table I. It can
be seen that CLAR outperforms the other methods signiﬁcantly. As more subjects are involved, the error
rate of CLAR remains at a low level, while those of other methods increase drastically. In particular,
in the most challenging case of 10 subjects, the mean clustering error rate of CLAR is 3.85%, which
improves by 7.09% compared to the best result provided by SSC. This implies that our method is robust
to in-sample outliers. In Table I, we also observe that the clustering error rates of LSA are much larger
than other methods. This is potentially because LSA is based on MSE, which is heavily inﬂuenced

1As we conﬁrmed with an author of [14], the power 2 of equation (12) in [14] is a typo, which should be 4.

6

Fig. 2. Examples of recovery results of face images. The three columns from left to right are the original image (X), the error matrix (E)
and the recovered image (XZ), respectively.

by outliers. In addition, the advantage of our method is much more signiﬁcant with respect to other
low-rank representation based algorithms such as LRR and LRSC; for example, there is 11% and 19%
improvement over LRR in the cases of 8 and 10 subjects, respectively. This veriﬁes the importance of
good rank approximation.

Figure 2 shows some recovery results from the 10-subject clustering scenario. As we can see, the error

term E is indeed sparse and it helps remove the shadows.

TABLE I
CLUSTERING ERROR RATE (%) ON THE EYALEB DATASET.

METHOD

LRR

SSC

LSA

LRSC CLAR

2 SUBJECTS
MEAN
MEDIAN
3 SUBJECTS
MEAN
MEDIAN
5 SUBJECTS
MEAN
MEDIAN
8 SUBJECTS
MEAN
MEDIAN
10 SUBJECTS
MEAN
MEDIAN

2.54
0.78

4.21
2.60

6.90
5.63

14.34
10.06

1.86
0.00

3.10
1.04

4.31
2.50

5.85
4.49

32.80
47.66

52.29
50.00

5.32
4.69

8.47
7.81

58.02
56.87

12.24
11.25

59.19
58.59

23.72
28.03

22.92
23.59

10.94
5.63

60.42
57.50

30.36
28.75

1.27
0.78

1.92
1.56

2.64
2.19

3.36
3.03

3.85
3.44

Fig. 3. Sample images in Hopkins 155 database. Trackers are denoted by different colors.

7

B. Motion segmentation

In this subsection, we evaluate the robustness of CLAR for motion segmentation problem, which is
an important step in video sequences analysis. Given multiple image frames of a dynamic scene, motion
segmentation is to cluster the points in those views into different motions undertaken by the moving
objects. Hopkins 155 motion database contains 155 video sequences along with features extracted and
tracked in all frames for each sequence. Since the trajectories associated with each motion reside in a
distinct afﬁne subspace of dimension d ≤ 3, every motion corresponds to a subspace. Figure 3 gives some
sample images. (cid:107)E(cid:107)2
F is applied to model the noise. Table II shows the clustering results on the Hopkins

TABLE II
SEGMENTATION ERROR RATE (%) AND MEAN COMPUTATIONAL TIME (S) ON THE HOPKINS 155 DATASET.

METHOD

LRR SSC

LSA

LRSC CLAR

2 MOTIONS
MEAN
MEDIAN
3 MOTIONS
MEAN
MEDIAN
ALL
MEAN
MEDIAN
AVERAGE TIME

2.13
0.00

4.03
1.43

2.56
0.00
6.44

1.52
0.00

4.40
0.56

2.18
0.00
5.09

4.23
0.56

7.02
1.45

4.86
0.89
17.17

3.69
0.29

7.69
3.80

4.59
0.60
0.70

1.32
0.00

2.60
0.51

1.61
0.00
3.80

Fig. 4. The inﬂuence of the parameter λ of CLAR on all 155 sequences of Hopkins 155.

155 dataset. CLAR achieves the best results in all cases. Speciﬁcally, the average clustering error rate is
1.32% for two motions and 2.60% for three motions. We also show the computational time in Table II.
As we can see, our computational time is less than LRR, SSC and LSA, though more than LRSC. Figure
4 demonstrates the sensitivity of our algorithm to λ. It shows that the performance of CLAR is quite
stable while λ varies in a pretty large range. We also test γ with values 1.05 and 1.2 which do not give
much difference in error rate. Since our problem is nonconvex, we repeat the experiments using different
random initializations and we can still get similar results after tuning the parameters. Thus, CLAR appears
quite insensitive to initilizations.

V. CONCLUSION
In this paper, we study the matrix rank minimization problem with log-determinant approximation.
This surrogate can better approximate the rank function. As an application, we study its use for the
robust subspace clustering problem. A minimization algorithm, based on a type of augmented Lagrangian
multipliers method, is developed to optimize the associated nonconvex objective function. Extensive
experiments on the face clustering and motion segmentation demonstrate the effectiveness and robustness
of the proposed method, which shows superior performance when compared to the state-of-the-art subspace
clustering methods.

This work is supported by US National Science Foundation grants IIS 1218712.

ACKNOWLEDGMENT

8

REFERENCES

[1] E. J. Cand`es and B. Recht, “Exact matrix completion via convex optimization,” Foundations of Computational mathematics, vol. 9,

no. 6, pp. 717–772, 2009.

[2] G. Liu, Z. Lin, and Y. Yu, “Robust subspace segmentation by low-rank representation,” in Proceedings of the 27th International

Conference on Machine Learning (ICML-10), 2010, pp. 663–670.

[3] M. Fazel, “Matrix rank minimization with applications,” Ph.D. dissertation, PhD thesis, Stanford University, 2002.
[4] B. Recht, M. Fazel, and P. A. Parrilo, “Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization,”

[5] J.-F. Cai, E. J. Cand`es, and Z. Shen, “A singular value thresholding algorithm for matrix completion,” SIAM Journal on Optimization,

SIAM review, vol. 52, no. 3, pp. 471–501, 2010.

vol. 20, no. 4, pp. 1956–1982, 2010.

[6] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He, “Fast and accurate matrix completion via truncated nuclear norm regularization,” Pattern

Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 9, pp. 2117–2130, 2013.

[7] C. Lu, J. Tang, S. Y. Yan, and Z. Lin, “Generalized nonconvex nonsmooth low-rank minimization,” in IEEE International Conference

on Computer Vision and Pattern Recognition.

IEEE, 2014.

[8] M. Fazel, H. Hindi, and S. P. Boyd, “Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance

matrices,” in American Control Conference, 2003. Proceedings of the 2003, vol. 3.

IEEE, 2003, pp. 2156–2162.

[9] K. Mohan and M. Fazel, “Iterative reweighted algorithms for matrix rank minimization,” The Journal of Machine Learning Research,

[10] R. Vidal, “A tutorial on subspace clustering,” IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 52–68, 2010.
[11] J. Shi and J. Malik, “Normalized cuts and image segmentation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on,

vol. 13, no. 1, pp. 3441–3473, 2012.

vol. 22, no. 8, pp. 888–905, 2000.

[12] J. Yan and M. Pollefeys, “A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and

non-degenerate,” in Computer Vision–ECCV 2006. Springer, 2006, pp. 94–106.

[13] E. Elhamifar and R. Vidal, “Sparse subspace clustering: Algorithm, theory, and applications,” Pattern Analysis and Machine Intelligence,

IEEE Transactions on, vol. 35, no. 11, pp. 2765–2781, 2013.

[14] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of subspace structures by low-rank representation,” Pattern Analysis

and Machine Intelligence, IEEE Transactions on, vol. 35, no. 1, pp. 171–184, 2013.

[15] P. Favaro, R. Vidal, and A. Ravichandran, “A closed form solution to robust subspace estimation and clustering,” in Computer Vision

and Pattern Recognition (CVPR), 2011 IEEE Conference on.

IEEE, 2011, pp. 1801–1807.

[16] R. Vidal and P. Favaro, “Low rank subspace clustering (lrsc),” Pattern Recognition Letters, vol. 43, pp. 47–61, 2014.
[17] G. Liu, H. Xu, and S. Yan, “Exact subspace segmentation and outlier detection by low-rank representation,” in International Conference

on Artiﬁcial Intelligence and Statistics, 2012, pp. 703–711.

[18] Y.-X. Wang and H. Xu, “Noisy sparse subspace clustering,” in Proceedings of The 30th International Conference on Machine Learning,

[19] M. Soltanolkotabi, E. J. Candes et al., “A geometric analysis of subspace clustering with outliers,” The Annals of Statistics, vol. 40,

2013, pp. 89–97.

no. 4, pp. 2195–2238, 2012.

[20] B. Nasihatkon and R. Hartley, “Graph connectivity in sparse subspace clustering,” in Computer Vision and Pattern Recognition (CVPR),

2011 IEEE Conference on.

IEEE, 2011, pp. 2137–2144.

[21] H. Zhang, Z. Lin, and C. Zhang, “A counterexample for the validity of using nuclear norm as a convex surrogate of rank,” in Machine

Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 226–241.

[22] Z. Kang, C. Peng, J. Cheng, and Q. Cheng, “Logdet rank minimization with application to subspace clustering,” Computational

[23] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algorithm for linear inverse problems,” SIAM Journal on Imaging

Intelligence and Neuroscience, vol. 2015, 2015.

Sciences, vol. 2, no. 1, pp. 183–202, 2009.

[24] J. Yang, W. Yin, Y. Zhang, and Y. Wang, “A fast algorithm for edge-preserving variational multichannel image restoration,” SIAM

[25] F. Lauer and C. Schnorr, “Spectral clustering of linear subspaces for motion segmentation,” in Computer Vision, 2009 IEEE 12th

Journal on Imaging Sciences, vol. 2, no. 2, pp. 569–592, 2009.

International Conference on.

IEEE, 2009, pp. 678–685.

[26] K.-C. Lee, J. Ho, and D. Kriegman, “Acquiring linear subspaces for face recognition under variable lighting,” Pattern Analysis and

Machine Intelligence, IEEE Transactions on, vol. 27, no. 5, pp. 684–698, 2005.

[27] R. Tron and R. Vidal, “A benchmark for the comparison of 3-d motion segmentation algorithms,” in Computer Vision and Pattern
IEEE, 2007, pp. 1–8.

Recognition, 2007. CVPR’07. IEEE Conference on.

