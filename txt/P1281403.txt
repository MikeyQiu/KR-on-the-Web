9
1
0
2
 
v
o
N
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
2
1
8
0
.
1
1
9
1
:
v
i
X
r
a

In Search of Credible News

Momchil Hardalov1, Ivan Koychev1, and Preslav Nakov2

1 FMI, Soﬁa University “St. Kliment Ohridski”, Soﬁa, Bulgaria,
momchil.hardalov@gmail.com,koychev@fmi.uni-sofia.bg
2 Qatar Computing Research Institute, HBKU, Doha, Qatar,
pnakov@qf.org.qa

Abstract. We study the problem of ﬁnding fake online news. This is
an important problem as news of questionable credibility have recently
been proliferating in social media at an alarming scale. As this is an
understudied problem, especially for languages other than English, we
ﬁrst collect and release to the research community three new balanced
credible vs. fake news datasets derived from four online sources. We then
propose a language-independent approach for automatically distinguish-
ing credible from fake news, based on a rich feature set. In particular, we
use linguistic (n-gram), credibility-related (capitalization, punctuation,
pronoun use, sentiment polarity), and semantic (embeddings and DB-
Pedia data) features. Our experiments on three diﬀerent testsets show
that our model can distinguish credible from fake news with very high
accuracy.

Keywords: Credibility, veracity, fact checking, humor detection.

1

Introduction

Internet and the proliferation of smart mobile devices have changed the way
information spreads, e.g., social media, blogs, and micro-blogging services such
as Twitter, Facebook and Google+ have become some of the main sources of
information for millions of users on a daily basis. On the positive side, this has
democratized and accelerated content creation and sharing. On the negative
side, it has made people vulnerable to manipulation, as the information in social
media is typically not monitored or moderated in any way. Thus, it has become
increasingly harder to distinguish real news from misinformation, rumors, unver-
iﬁed, manipulative, and even fake content. Not only are online blogs nowadays
ﬂooded by biased comments and fake content, but also online news media in
turn are ﬁlled with unreliable and unveriﬁed content, e.g., due to the willingness
of journalists to be the ﬁrst to write about a hot topic, often by-passing the
veriﬁcation of their information sources; there are also some online information
sources created with the sole purpose of spreading manipulative and biased in-
formation. Finally, the problem extends beyond the cyberspace, as in some cases,
fake news from online sources have crept into mainstream media.

Journalists, regular online users, and researchers are well aware of the issue,
and topics such as information credibility, veracity, and fact checking are becom-
ing increasingly important research directions [3,5,20]. For example, there was
a recent 2016 special issue of the ACM Transactions on Information Systems
journal on Trust and Veracity of Information in Social Media [15], and there is
an upcoming SemEval-2017 shared task on rumor detection.

As English is the primary language of the Web, most research on informa-
tion credibility and veracity has focused on English, while other languages have
been largely neglected. To bridge this gap, below we present experiments in dis-
tinguishing real from fake news in Bulgarian; yet, our approach is in principle
language-independent. In particular, we distinguish between real news vs. fake
news that in some cases are designed to sound funny (while still resembling real
ones); thus, our task can be also seen as humor detection [11,16].

As there was no publicly available dataset that we could use, we had to create
one ourselves. We collected two types of news: credible, coming from trusted
online sources, and fake news, written with the intention to amuse, or sometimes
confuse, the reader who is not knowledgeable enough about the subject. We then
built a model to distinguish between the two, which achieved very high accuracy.
The remainder of this paper is organized as follows: Section 2 presents some
related work. Section 3 introduces our method for distinguishing credible from
fake news. Section 4 presents our data, feature selection, the experiments, and
the results. Finally, Section 5 concludes and suggests directions for future work.

2 Related Work

Information credibility in social media is studied by Castillo & al. [3], who for-
mulate it as a problem of ﬁnding false information about a newsworthy event.
They focus on tweets using variety of features including user reputation, author
writing style, and various time-based features.

Zubiaga & al. [19] studied how people handle rumors in social media. They
found that users with higher reputation are more trusted, and thus can spread
rumors among other users without raising suspicions about the credibility of the
news or of its source.

Online personal blogs are another popular way to spread information by
presenting personal opinions, even though researchers disagree about how much
people trust such blogs. Johnson & al. [6] studied how blog users act in the time
of newsworthy event, e.g., such as the crisis in Iraq, and how biased users try to
inﬂuence other people.

It is not only social media that can spread information of questionable quality.
The credibility of the information published on online news portals has also been
questioned by a number of researchers [1,8,4]. As timing is a crucial factor when it
comes to publishing breaking news, it is simply not possible to double-check the
facts and the sources, as is usually standard in respectable printed newspapers
and magazines. This is one of the biggest concerns about online news media that
journalists have [2].

The interested reader can see [17] for a review of various methods for de-
tecting fake news, where diﬀerent approaches are compared based on linguistic
analysis, discourse, linked data, and social network features.

Finally, we should also mention work on humor detection. Yang & al. [16]
identify semantic structures behind humor, and then design sets of features for
each structure; they further develop anchors that enable humor in a sentence.
However, they mix diﬀerent genres such as news, community question answers,
and proverbs, as well as the One-Liner dataset [11]. In contrast, we focus on
news both for positive and for negative examples, and we do not assume that
the reason for a news being not credible is the humor it contains.

We propose a language-independent approach for automatically distinguishing
credible from fake news, based on a rich feature set. In particular, we use lin-
guistic (n-gram), credibility (capitalization, punctuation, pronoun use, sentiment
polarity), and semantic (embeddings and DBPedia data) features.

3 Method

3.1 Features

Linguistic (n-gram) Features Before generating these features, we ﬁrst per-
form initial pre-processing: tokenization and stop word removal. We deﬁne stop
words as the most common, functional words in a language (e.g., conjunctions,
prepositions, interjections, etc.); while they ﬁt well for problems such as author
proﬁling, they turn out not to be particularly useful for distinguishing credi-
ble from fake news. Eventually, we experimented with the following linguistic
features:

– n-grams: presence of individual uni-grams and bi-grams. The rationale is
that some n-grams are more typical of credible vs. fake news, and vice versa;

– tf-idf: the same n-grams, but weighted using tf-idf;
– vocabulary richness: the number of unique word types used in the article,

possibly normalized by the number of word tokens.

Credibility Features We also used the following credibility features, which
were previously proposed in the literature [3]:

1. Length of the article (number of tokens);
2. Fraction of words that only contain uppercase letters;
3. Fraction of words that start with an uppercase letter;
4. Fraction of words that contain at least one uppercase letter;
5. Fraction of words that only contain lowercase letters;
6. Fraction of plural pronouns;
7. Fraction of singular pronouns;
8. Fraction of ﬁrst person pronouns;

9. Fraction of second person pronouns;
10. Fraction of third person pronouns;
11. Number of URLs;
12. Number of occurrences of an exclamation mark;
13. Number of occurrences of a question mark;
14. Number of occurrences of a hashtag;
15. Number of occurrences of a single quote;
16. Number of occurrences of a double quote.

We further added some sentiment-polarity features from lexicons generated
from Bulgarian movie reviews [7] (5,016 positive, and 2,415 negative words),
which we further expanded with some more words. Based on these lexicons, we
calculated the following features:

17. Proportion of positive words;
18. Proportion of negative words;
19. Sum of the sentiment scores for the positive words;
20. Sum of the sentiment scores for the negative words.

Note that we eventually ended up using only a subset of the above features,

as we performed feature selection as described in Section 4.2 below.

Semantic (Embedding and DBPedia) Features Finally, we use embedding
vectors to model the semantics of the documents. We wanted to model implicitly
some general world knowledge, and thus we trained word2vec vectors on the text
of the long abstracts from the Bulgarian DBPedia.3 Then, we built vectors for
a document as the average of the word2vec vectors of the non-stop word tokens
it is composed of.

3.2 Classiﬁcation

As we have a rich set of partially overlapping features, we used logistic regres-
sion for classiﬁcation with L-BFGS [10] optimizer and elastic net regularization
[18], which combines L1 and L2 regularization. This classiﬁcation setup con-
verges very fast, ﬁts well in huge feature spaces, is robust to over-ﬁtting, and
handles overlapping features well. We ﬁne-tuned the hyper-parameters of our
classiﬁer (maximum number of iterations, elastic net parameters, and regular-
ization parameters) on the training dataset. We further applied feature selection
as described below.

4 Experiments and Evaluation

4.1 Data

As there was no pre-existing suitable dataset for Bulgarian, we had to create
one of our own. For this purpose, we collected a diverse dataset with enough

3 http://wiki.dbpedia.org/

samples in each category. We further wanted to make sure that our dataset will
be good for modeling credible vs. fake news, i.e., that will not degenerate into
related tasks such as topic detection (which might happen if the credible and
the fake news are about diﬀerent topics), authorship attribution (which could
be the case if the fake news are written by just 1-2 authors) or source prediction
(which can occur if all credible/fake news come from just one source). Thus, we
used four Bulgarian news sources (from which we generated one training and
three separate balanced testing datasets):

1. We retrieved most of our credible news from Dnevnik,4 a respected Bulgar-
ian newspaper; we focused mostly on politics. This dataset was previously
used in research on ﬁnding opinion manipulation trolls [12,13,14], but its
news content ﬁts well for our task too (5,896 credible news);

2. As our main online source of fake news, we used a website with funny news
called Ne!Novinite.5 We crawled topics such as politics, sports, culture,
world news, horoscopes, interviews, and user-written articles (6,382 fake
news);

3. As an additional source of fake news, we used articles from the Bazikileaks6
blog. These documents are written in the form of blog-posts and the content
may be classiﬁed as “ﬁctitious”, which is another subcategory of fake news.
The domain is politics (656 fake news);

4. And ﬁnally, we retrieved news from the bTV Lifestyle section,7 which
contains both credible (in the bTV subsection) and fake news (in the bTV
Duplex subsection). In both subsections, the articles are about popular peo-
ple and events (69 credible and 68 fake news);

We used the documents from Dnevnik and Ne!Novinite for training and test-
ing: 70% for training and 30% for testing. We further had two additional test
sets: one of bTV vs. bTV Duplex, and one on Dnevnik vs. Bazikileaks. All test
datasets are near-perfectly balanced.

Finally, as we have already mentioned above, we used the long abstracts in
the Bulgarian DbPedia to train word2vec vectors, which we then used to build
document vectors, which we used as features for classiﬁcation. (171,444 credible
samples).

4.2 Feature Selection

We performed feature selection on the credibility features. For this purpose,
we ﬁrst used Learning Vector Quantization (LVQ) [9] to obtain a ranking of the
features from Section 3.1 by their importance on the training dataset; the results
are shown in Table 1. See also Figure 1 for a comparison of the distribution of
some of the credibility features in credible. vs. funny news.

4 http://www.dnevnik.bg/
5 http://www.nenovinite.com/
6 https://neverojatno.wordpress.com/
7 http://www.btv.bg/lifestyle/all/

Features
doubleQuotes 16
upperCaseCount 4
lowerUpperCase 5
ﬁrstUpperCase 3
pluralPronouns 6
ﬁrstPersonPronouns 8
allUpperCaseCount 2
negativeWords 18
positiveWords 17
tokensCount 1
singularPronouns 7
thirdPersonPronouns 10
negativeWordsScore 20
hashtags 14
urls 11
positiveWordsScore 17
singleQuotes 15
secondPersonPronouns 9
questionMarks 13
exclMarks 12

Importance
0.7911
0.7748
0.7717
0.7708
0.6558
0.6346
0.6282
0.5944
0.5834
0.5779
0.5286
0.5273
0.5206
0.4998
0.4987
0.4910
0.4884
0.4408
0.4407
0.3160

Table 1. Features ranked by the LVQ importance metric.

Then, we experimented with various feature combinations of the top-ranked
features, and we selected the combination that worked best on cross-validation
on the training dataset (compare to Table 1):

– Fraction of negative words in the text (negativeWords);

– Fraction of words that contain uppercase letters only (allUpperCaseCount);

– Fraction of words that start with an uppercase letter (ﬁrstUpperCase);

– Fraction of words that only contain lowercase letters (lowerUpperCase);

– Fraction of plural pronouns in the text (pluralPronouns);

– Number of occurrences of exclamation marks (exclMarks);

– Number of occurrences of double quotes (doubleQuotes).

Fig. 1. Boxplots presenting the distributions of some credibility features in credible vs.
funny news.

Feature Groups

Dnevnik
Ne!Novinite

Credibility + Linguistic + Semantic 99.36
92.67
Credibility + Semantic
96.02
Linguistic + Credibility
98.95
Semantic
95.71
Linguistic
83.25
Credibility
52.60
Baseline (majority class)

Dnevnik vs.
bTV vs.
bTV Duplex Bazikileaks
62.04
75.91
59.12
61.31
56.93
62.04
50.36

85.53
82.99
61.94
71.01
73.25
79.85
50.86

Table 2. Accuracy for diﬀerent feature group combinations.

4.3 Results

Table 2 shows the results when using all feature groups and when turning oﬀ
some of them. We can see that the best results are achieved when experiment-
ing with “Credibility + Semantic” and “Credibility + Linguistic + Semantic”
feature combinations, and the results are worse when only using credibility and
linguistic features.

Analyzing the results on the Dnevnik vs. the Ne!Novinite testset (ﬁrst col-
umn), we can see that the linguistic features are more important than the
credibility ones. Yet, the semantic features are even more important. When we
combine all the feature groups, we achieve 99.36% accuracy, but this is only
marginally better than using the semantic features alone. Note, however, that
using semantic features only does not perform so well on the other two test
datasets, especially on the last one.

The linguistic features work relatively well on two of the test datasets, but
not on bTV, where the combination of “Credibility + Semantic” is the best-
performing one.

Naturally, the best results are on the Dnevnik vs. NE!Novinite, where the
classiﬁer achieves near perfect accuracy (note that this is despite the diﬀerent
class distribution on training vs. testing). The hardest testing dataset is bTV,
where both the positive and the negative class are from sources diﬀerent from
those used in the training dataset; yet, we achieve up to 75.91% accuracy, which
is well above the majority class baseline of 50.36. The Dnevnik vs. Bazikileaks
dataset falls somewhere in between, with up to 85.53% accuracy; this is to be
expected as the positive examples come from the same source as for the training
dataset (even though the negative class is diﬀerent).

Overall, on all three datasets, we achieved accuracy of 75-99%, which is well
above the majority class baseline. The strong relative performance on the three
diﬀerent test datasets that come from diﬀerent sources suggests that our model
really learns to distinguish credible vs. fake news rather than learning to classify
topics, sources, or author style.

5 Conclusion and Future Work

We have presented a feature-rich language-independent approach for distin-
guishing credible from fake news. In particular, we used linguistic (n-gram),
credibility-related (capitalization, punctuation, pronoun use, sentiment polar-
ity, etc., with feature selection), and semantic (embeddings and DBPedia data)
features. Our experiments on three diﬀerent testsets, derived from four diﬀerent
sources, have shown that our model can distinguish credible from fake news with
very high accuracy, well above a majority-class baseline.

In future work, we plan to experiment with more features, e.g., based on
linked data [17], or on discourse analysis [17]. Looking at features used for related
tasks such as humor- [16] and rumor-related [19] is another promising direction
for future work. We also want to apply deep learning, which can eliminate the
need for feature engineering altogether.

Last but not least, we would like to note that we have made our source code

and datasets publicly available for research purposes at the following URL:

https://github.com/mhardalov/news-credibility

Acknowledgments. This research was performed by Momchil Hardalov, a student
in Computer Science in the Soﬁa University “St Kliment Ohridski”, as part of
his MSc thesis. It is also part of the Interactive sYstems for Answer Search
(Iyas) project, which is developed by the Arabic Language Technologies (ALT)
group at the Qatar Computing Research Institute (QCRI), HBKU, part of Qatar
Foundation in collaboration with MIT-CSAIL.

References

1. Ann M Brill. Online journalists embrace new marketing function. Newspaper

Research Journal, 22(2):28, 2001.

2. William P Cassidy. Online news credibility: An examination of the perceptions of
newspaper journalists. Journal of Computer-Mediated Communication, 12(2):478–
498, 2007.

3. Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. Predicting information
credibility in time-sensitive social media. Internet Research, 23(5):560–588, 2013.
4. Howard Finberg, Martha L Stone, and Diane Lynch. Digital journalism credibility

study. Online News Association. Retrieved November, 3:2003, 2002.

5. Lucas Graves. Deciding What’s True: Fact-Checking Journalism and the New

Ecology of News. PhD thesis, Columbia University, 2013.

6. Thomas J Johnson, Barbara K Kaye, Shannon L Bichard, and W Joann Wong.
Every blog has its day: Politically-interested internet users perceptions of blog
credibility. Journal of Computer-Mediated Communication, 13(1):100–122, 2007.
7. Borislav Kapukaranov and Preslav Nakov. Fine-grained sentiment analysis for
movie reviews in Bulgarian. In Proceedings of Recent Advances in Natural Language
Processing, RANLP ’15, pages 266–274, Hissar, Bulgaria, 2015.

8. Stan Ketterer. Teaching students how to evaluate and use online resources. Jour-

nalism & Mass Communication Educator, 52(4):4, 1998.

9. Teuvo Kohonen.

Improved versions of learning vector quantization.

In IJCNN

International Joint Conference on Neural Networks, pages 545–550, 1990.

10. Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large

scale optimization. Mathematical programming, 45(1-3):503–528, 1989.

11. Rada Mihalcea and Carlo Strapparava. Making computers laugh: Investigations
in automatic humor recognition. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Natural Language Processing,
HLT-EMNLP ’05, pages 531–538, Vancouver, British Columbia, Canada, 2005.
12. Todor Mihaylov, Georgi Georgiev, and Preslav Nakov. Finding opinion manipula-
tion trolls in news community forums. In Proceedings of the Nineteenth Conference
on Computational Natural Language Learning, CoNLL ’15, pages 310–314, Beijing,
China, 2015.

13. Todor Mihaylov, Ivan Koychev, Georgi Georgiev, and Preslav Nakov. Exposing
paid opinion manipulation trolls. In Proceedings of the International Conference
Recent Advances in Natural Language Processing, RANLP ’15, pages 443–450,
Hissar, Bulgaria, 2015.

14. Todor Mihaylov and Preslav Nakov. Hunting for troll comments in news com-
munity forums. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL ’16, Berlin, Germany, 2016.

15. Symeon Papadopoulos, Kalina Bontcheva, Eva Jaho, Mihai Lupu, and Carlos
Castillo. Overview of the special issue on trust and veracity of information in
social media. ACM Trans. Inf. Syst., 34(3):14:1–14:5, April 2016.

16. Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. Humor recognition and
In Proceedings of the 2015 Conference on Empirical
humor anchor extraction.
Methods in Natural Language Processing, EMNLP ’15, pages 2367–2376, Lisbon,
Portugal, 2015.

17. Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion
Stoica. Spark: Cluster computing with working sets. In Proceedings of the 2nd
USENIX Conference on Hot Topics in Cloud Computing, HotCloud ’10, pages
10–10, Boston, MA, 2010.

18. Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic
net. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
67(2):301–320, 2005.

19. Arkaitz Zubiaga, Geraldine Wong Sak Hoi, Maria Liakata, Rob Procter, and Peter
Tolmie. Analysing how people orient to and spread rumours in social media by
looking at conversational threads. arXiv preprint arXiv:1511.07487, 2015.

20. Arkaitz Zubiaga and Heng Ji. Tweet, but verify: epistemic study of information
veriﬁcation on Twitter. Social Network Analysis and Mining, 4(1):1–12, 2014.

9
1
0
2
 
v
o
N
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
2
1
8
0
.
1
1
9
1
:
v
i
X
r
a

In Search of Credible News

Momchil Hardalov1, Ivan Koychev1, and Preslav Nakov2

1 FMI, Soﬁa University “St. Kliment Ohridski”, Soﬁa, Bulgaria,
momchil.hardalov@gmail.com,koychev@fmi.uni-sofia.bg
2 Qatar Computing Research Institute, HBKU, Doha, Qatar,
pnakov@qf.org.qa

Abstract. We study the problem of ﬁnding fake online news. This is
an important problem as news of questionable credibility have recently
been proliferating in social media at an alarming scale. As this is an
understudied problem, especially for languages other than English, we
ﬁrst collect and release to the research community three new balanced
credible vs. fake news datasets derived from four online sources. We then
propose a language-independent approach for automatically distinguish-
ing credible from fake news, based on a rich feature set. In particular, we
use linguistic (n-gram), credibility-related (capitalization, punctuation,
pronoun use, sentiment polarity), and semantic (embeddings and DB-
Pedia data) features. Our experiments on three diﬀerent testsets show
that our model can distinguish credible from fake news with very high
accuracy.

Keywords: Credibility, veracity, fact checking, humor detection.

1

Introduction

Internet and the proliferation of smart mobile devices have changed the way
information spreads, e.g., social media, blogs, and micro-blogging services such
as Twitter, Facebook and Google+ have become some of the main sources of
information for millions of users on a daily basis. On the positive side, this has
democratized and accelerated content creation and sharing. On the negative
side, it has made people vulnerable to manipulation, as the information in social
media is typically not monitored or moderated in any way. Thus, it has become
increasingly harder to distinguish real news from misinformation, rumors, unver-
iﬁed, manipulative, and even fake content. Not only are online blogs nowadays
ﬂooded by biased comments and fake content, but also online news media in
turn are ﬁlled with unreliable and unveriﬁed content, e.g., due to the willingness
of journalists to be the ﬁrst to write about a hot topic, often by-passing the
veriﬁcation of their information sources; there are also some online information
sources created with the sole purpose of spreading manipulative and biased in-
formation. Finally, the problem extends beyond the cyberspace, as in some cases,
fake news from online sources have crept into mainstream media.

Journalists, regular online users, and researchers are well aware of the issue,
and topics such as information credibility, veracity, and fact checking are becom-
ing increasingly important research directions [3,5,20]. For example, there was
a recent 2016 special issue of the ACM Transactions on Information Systems
journal on Trust and Veracity of Information in Social Media [15], and there is
an upcoming SemEval-2017 shared task on rumor detection.

As English is the primary language of the Web, most research on informa-
tion credibility and veracity has focused on English, while other languages have
been largely neglected. To bridge this gap, below we present experiments in dis-
tinguishing real from fake news in Bulgarian; yet, our approach is in principle
language-independent. In particular, we distinguish between real news vs. fake
news that in some cases are designed to sound funny (while still resembling real
ones); thus, our task can be also seen as humor detection [11,16].

As there was no publicly available dataset that we could use, we had to create
one ourselves. We collected two types of news: credible, coming from trusted
online sources, and fake news, written with the intention to amuse, or sometimes
confuse, the reader who is not knowledgeable enough about the subject. We then
built a model to distinguish between the two, which achieved very high accuracy.
The remainder of this paper is organized as follows: Section 2 presents some
related work. Section 3 introduces our method for distinguishing credible from
fake news. Section 4 presents our data, feature selection, the experiments, and
the results. Finally, Section 5 concludes and suggests directions for future work.

2 Related Work

Information credibility in social media is studied by Castillo & al. [3], who for-
mulate it as a problem of ﬁnding false information about a newsworthy event.
They focus on tweets using variety of features including user reputation, author
writing style, and various time-based features.

Zubiaga & al. [19] studied how people handle rumors in social media. They
found that users with higher reputation are more trusted, and thus can spread
rumors among other users without raising suspicions about the credibility of the
news or of its source.

Online personal blogs are another popular way to spread information by
presenting personal opinions, even though researchers disagree about how much
people trust such blogs. Johnson & al. [6] studied how blog users act in the time
of newsworthy event, e.g., such as the crisis in Iraq, and how biased users try to
inﬂuence other people.

It is not only social media that can spread information of questionable quality.
The credibility of the information published on online news portals has also been
questioned by a number of researchers [1,8,4]. As timing is a crucial factor when it
comes to publishing breaking news, it is simply not possible to double-check the
facts and the sources, as is usually standard in respectable printed newspapers
and magazines. This is one of the biggest concerns about online news media that
journalists have [2].

The interested reader can see [17] for a review of various methods for de-
tecting fake news, where diﬀerent approaches are compared based on linguistic
analysis, discourse, linked data, and social network features.

Finally, we should also mention work on humor detection. Yang & al. [16]
identify semantic structures behind humor, and then design sets of features for
each structure; they further develop anchors that enable humor in a sentence.
However, they mix diﬀerent genres such as news, community question answers,
and proverbs, as well as the One-Liner dataset [11]. In contrast, we focus on
news both for positive and for negative examples, and we do not assume that
the reason for a news being not credible is the humor it contains.

We propose a language-independent approach for automatically distinguishing
credible from fake news, based on a rich feature set. In particular, we use lin-
guistic (n-gram), credibility (capitalization, punctuation, pronoun use, sentiment
polarity), and semantic (embeddings and DBPedia data) features.

3 Method

3.1 Features

Linguistic (n-gram) Features Before generating these features, we ﬁrst per-
form initial pre-processing: tokenization and stop word removal. We deﬁne stop
words as the most common, functional words in a language (e.g., conjunctions,
prepositions, interjections, etc.); while they ﬁt well for problems such as author
proﬁling, they turn out not to be particularly useful for distinguishing credi-
ble from fake news. Eventually, we experimented with the following linguistic
features:

– n-grams: presence of individual uni-grams and bi-grams. The rationale is
that some n-grams are more typical of credible vs. fake news, and vice versa;

– tf-idf: the same n-grams, but weighted using tf-idf;
– vocabulary richness: the number of unique word types used in the article,

possibly normalized by the number of word tokens.

Credibility Features We also used the following credibility features, which
were previously proposed in the literature [3]:

1. Length of the article (number of tokens);
2. Fraction of words that only contain uppercase letters;
3. Fraction of words that start with an uppercase letter;
4. Fraction of words that contain at least one uppercase letter;
5. Fraction of words that only contain lowercase letters;
6. Fraction of plural pronouns;
7. Fraction of singular pronouns;
8. Fraction of ﬁrst person pronouns;

9. Fraction of second person pronouns;
10. Fraction of third person pronouns;
11. Number of URLs;
12. Number of occurrences of an exclamation mark;
13. Number of occurrences of a question mark;
14. Number of occurrences of a hashtag;
15. Number of occurrences of a single quote;
16. Number of occurrences of a double quote.

We further added some sentiment-polarity features from lexicons generated
from Bulgarian movie reviews [7] (5,016 positive, and 2,415 negative words),
which we further expanded with some more words. Based on these lexicons, we
calculated the following features:

17. Proportion of positive words;
18. Proportion of negative words;
19. Sum of the sentiment scores for the positive words;
20. Sum of the sentiment scores for the negative words.

Note that we eventually ended up using only a subset of the above features,

as we performed feature selection as described in Section 4.2 below.

Semantic (Embedding and DBPedia) Features Finally, we use embedding
vectors to model the semantics of the documents. We wanted to model implicitly
some general world knowledge, and thus we trained word2vec vectors on the text
of the long abstracts from the Bulgarian DBPedia.3 Then, we built vectors for
a document as the average of the word2vec vectors of the non-stop word tokens
it is composed of.

3.2 Classiﬁcation

As we have a rich set of partially overlapping features, we used logistic regres-
sion for classiﬁcation with L-BFGS [10] optimizer and elastic net regularization
[18], which combines L1 and L2 regularization. This classiﬁcation setup con-
verges very fast, ﬁts well in huge feature spaces, is robust to over-ﬁtting, and
handles overlapping features well. We ﬁne-tuned the hyper-parameters of our
classiﬁer (maximum number of iterations, elastic net parameters, and regular-
ization parameters) on the training dataset. We further applied feature selection
as described below.

4 Experiments and Evaluation

4.1 Data

As there was no pre-existing suitable dataset for Bulgarian, we had to create
one of our own. For this purpose, we collected a diverse dataset with enough

3 http://wiki.dbpedia.org/

samples in each category. We further wanted to make sure that our dataset will
be good for modeling credible vs. fake news, i.e., that will not degenerate into
related tasks such as topic detection (which might happen if the credible and
the fake news are about diﬀerent topics), authorship attribution (which could
be the case if the fake news are written by just 1-2 authors) or source prediction
(which can occur if all credible/fake news come from just one source). Thus, we
used four Bulgarian news sources (from which we generated one training and
three separate balanced testing datasets):

1. We retrieved most of our credible news from Dnevnik,4 a respected Bulgar-
ian newspaper; we focused mostly on politics. This dataset was previously
used in research on ﬁnding opinion manipulation trolls [12,13,14], but its
news content ﬁts well for our task too (5,896 credible news);

2. As our main online source of fake news, we used a website with funny news
called Ne!Novinite.5 We crawled topics such as politics, sports, culture,
world news, horoscopes, interviews, and user-written articles (6,382 fake
news);

3. As an additional source of fake news, we used articles from the Bazikileaks6
blog. These documents are written in the form of blog-posts and the content
may be classiﬁed as “ﬁctitious”, which is another subcategory of fake news.
The domain is politics (656 fake news);

4. And ﬁnally, we retrieved news from the bTV Lifestyle section,7 which
contains both credible (in the bTV subsection) and fake news (in the bTV
Duplex subsection). In both subsections, the articles are about popular peo-
ple and events (69 credible and 68 fake news);

We used the documents from Dnevnik and Ne!Novinite for training and test-
ing: 70% for training and 30% for testing. We further had two additional test
sets: one of bTV vs. bTV Duplex, and one on Dnevnik vs. Bazikileaks. All test
datasets are near-perfectly balanced.

Finally, as we have already mentioned above, we used the long abstracts in
the Bulgarian DbPedia to train word2vec vectors, which we then used to build
document vectors, which we used as features for classiﬁcation. (171,444 credible
samples).

4.2 Feature Selection

We performed feature selection on the credibility features. For this purpose,
we ﬁrst used Learning Vector Quantization (LVQ) [9] to obtain a ranking of the
features from Section 3.1 by their importance on the training dataset; the results
are shown in Table 1. See also Figure 1 for a comparison of the distribution of
some of the credibility features in credible. vs. funny news.

4 http://www.dnevnik.bg/
5 http://www.nenovinite.com/
6 https://neverojatno.wordpress.com/
7 http://www.btv.bg/lifestyle/all/

Features
doubleQuotes 16
upperCaseCount 4
lowerUpperCase 5
ﬁrstUpperCase 3
pluralPronouns 6
ﬁrstPersonPronouns 8
allUpperCaseCount 2
negativeWords 18
positiveWords 17
tokensCount 1
singularPronouns 7
thirdPersonPronouns 10
negativeWordsScore 20
hashtags 14
urls 11
positiveWordsScore 17
singleQuotes 15
secondPersonPronouns 9
questionMarks 13
exclMarks 12

Importance
0.7911
0.7748
0.7717
0.7708
0.6558
0.6346
0.6282
0.5944
0.5834
0.5779
0.5286
0.5273
0.5206
0.4998
0.4987
0.4910
0.4884
0.4408
0.4407
0.3160

Table 1. Features ranked by the LVQ importance metric.

Then, we experimented with various feature combinations of the top-ranked
features, and we selected the combination that worked best on cross-validation
on the training dataset (compare to Table 1):

– Fraction of negative words in the text (negativeWords);

– Fraction of words that contain uppercase letters only (allUpperCaseCount);

– Fraction of words that start with an uppercase letter (ﬁrstUpperCase);

– Fraction of words that only contain lowercase letters (lowerUpperCase);

– Fraction of plural pronouns in the text (pluralPronouns);

– Number of occurrences of exclamation marks (exclMarks);

– Number of occurrences of double quotes (doubleQuotes).

Fig. 1. Boxplots presenting the distributions of some credibility features in credible vs.
funny news.

Feature Groups

Dnevnik
Ne!Novinite

Credibility + Linguistic + Semantic 99.36
92.67
Credibility + Semantic
96.02
Linguistic + Credibility
98.95
Semantic
95.71
Linguistic
83.25
Credibility
52.60
Baseline (majority class)

Dnevnik vs.
bTV vs.
bTV Duplex Bazikileaks
62.04
75.91
59.12
61.31
56.93
62.04
50.36

85.53
82.99
61.94
71.01
73.25
79.85
50.86

Table 2. Accuracy for diﬀerent feature group combinations.

4.3 Results

Table 2 shows the results when using all feature groups and when turning oﬀ
some of them. We can see that the best results are achieved when experiment-
ing with “Credibility + Semantic” and “Credibility + Linguistic + Semantic”
feature combinations, and the results are worse when only using credibility and
linguistic features.

Analyzing the results on the Dnevnik vs. the Ne!Novinite testset (ﬁrst col-
umn), we can see that the linguistic features are more important than the
credibility ones. Yet, the semantic features are even more important. When we
combine all the feature groups, we achieve 99.36% accuracy, but this is only
marginally better than using the semantic features alone. Note, however, that
using semantic features only does not perform so well on the other two test
datasets, especially on the last one.

The linguistic features work relatively well on two of the test datasets, but
not on bTV, where the combination of “Credibility + Semantic” is the best-
performing one.

Naturally, the best results are on the Dnevnik vs. NE!Novinite, where the
classiﬁer achieves near perfect accuracy (note that this is despite the diﬀerent
class distribution on training vs. testing). The hardest testing dataset is bTV,
where both the positive and the negative class are from sources diﬀerent from
those used in the training dataset; yet, we achieve up to 75.91% accuracy, which
is well above the majority class baseline of 50.36. The Dnevnik vs. Bazikileaks
dataset falls somewhere in between, with up to 85.53% accuracy; this is to be
expected as the positive examples come from the same source as for the training
dataset (even though the negative class is diﬀerent).

Overall, on all three datasets, we achieved accuracy of 75-99%, which is well
above the majority class baseline. The strong relative performance on the three
diﬀerent test datasets that come from diﬀerent sources suggests that our model
really learns to distinguish credible vs. fake news rather than learning to classify
topics, sources, or author style.

5 Conclusion and Future Work

We have presented a feature-rich language-independent approach for distin-
guishing credible from fake news. In particular, we used linguistic (n-gram),
credibility-related (capitalization, punctuation, pronoun use, sentiment polar-
ity, etc., with feature selection), and semantic (embeddings and DBPedia data)
features. Our experiments on three diﬀerent testsets, derived from four diﬀerent
sources, have shown that our model can distinguish credible from fake news with
very high accuracy, well above a majority-class baseline.

In future work, we plan to experiment with more features, e.g., based on
linked data [17], or on discourse analysis [17]. Looking at features used for related
tasks such as humor- [16] and rumor-related [19] is another promising direction
for future work. We also want to apply deep learning, which can eliminate the
need for feature engineering altogether.

Last but not least, we would like to note that we have made our source code

and datasets publicly available for research purposes at the following URL:

https://github.com/mhardalov/news-credibility

Acknowledgments. This research was performed by Momchil Hardalov, a student
in Computer Science in the Soﬁa University “St Kliment Ohridski”, as part of
his MSc thesis. It is also part of the Interactive sYstems for Answer Search
(Iyas) project, which is developed by the Arabic Language Technologies (ALT)
group at the Qatar Computing Research Institute (QCRI), HBKU, part of Qatar
Foundation in collaboration with MIT-CSAIL.

References

1. Ann M Brill. Online journalists embrace new marketing function. Newspaper

Research Journal, 22(2):28, 2001.

2. William P Cassidy. Online news credibility: An examination of the perceptions of
newspaper journalists. Journal of Computer-Mediated Communication, 12(2):478–
498, 2007.

3. Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. Predicting information
credibility in time-sensitive social media. Internet Research, 23(5):560–588, 2013.
4. Howard Finberg, Martha L Stone, and Diane Lynch. Digital journalism credibility

study. Online News Association. Retrieved November, 3:2003, 2002.

5. Lucas Graves. Deciding What’s True: Fact-Checking Journalism and the New

Ecology of News. PhD thesis, Columbia University, 2013.

6. Thomas J Johnson, Barbara K Kaye, Shannon L Bichard, and W Joann Wong.
Every blog has its day: Politically-interested internet users perceptions of blog
credibility. Journal of Computer-Mediated Communication, 13(1):100–122, 2007.
7. Borislav Kapukaranov and Preslav Nakov. Fine-grained sentiment analysis for
movie reviews in Bulgarian. In Proceedings of Recent Advances in Natural Language
Processing, RANLP ’15, pages 266–274, Hissar, Bulgaria, 2015.

8. Stan Ketterer. Teaching students how to evaluate and use online resources. Jour-

nalism & Mass Communication Educator, 52(4):4, 1998.

9. Teuvo Kohonen.

Improved versions of learning vector quantization.

In IJCNN

International Joint Conference on Neural Networks, pages 545–550, 1990.

10. Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large

scale optimization. Mathematical programming, 45(1-3):503–528, 1989.

11. Rada Mihalcea and Carlo Strapparava. Making computers laugh: Investigations
in automatic humor recognition. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Natural Language Processing,
HLT-EMNLP ’05, pages 531–538, Vancouver, British Columbia, Canada, 2005.
12. Todor Mihaylov, Georgi Georgiev, and Preslav Nakov. Finding opinion manipula-
tion trolls in news community forums. In Proceedings of the Nineteenth Conference
on Computational Natural Language Learning, CoNLL ’15, pages 310–314, Beijing,
China, 2015.

13. Todor Mihaylov, Ivan Koychev, Georgi Georgiev, and Preslav Nakov. Exposing
paid opinion manipulation trolls. In Proceedings of the International Conference
Recent Advances in Natural Language Processing, RANLP ’15, pages 443–450,
Hissar, Bulgaria, 2015.

14. Todor Mihaylov and Preslav Nakov. Hunting for troll comments in news com-
munity forums. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL ’16, Berlin, Germany, 2016.

15. Symeon Papadopoulos, Kalina Bontcheva, Eva Jaho, Mihai Lupu, and Carlos
Castillo. Overview of the special issue on trust and veracity of information in
social media. ACM Trans. Inf. Syst., 34(3):14:1–14:5, April 2016.

16. Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. Humor recognition and
In Proceedings of the 2015 Conference on Empirical
humor anchor extraction.
Methods in Natural Language Processing, EMNLP ’15, pages 2367–2376, Lisbon,
Portugal, 2015.

17. Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion
Stoica. Spark: Cluster computing with working sets. In Proceedings of the 2nd
USENIX Conference on Hot Topics in Cloud Computing, HotCloud ’10, pages
10–10, Boston, MA, 2010.

18. Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic
net. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
67(2):301–320, 2005.

19. Arkaitz Zubiaga, Geraldine Wong Sak Hoi, Maria Liakata, Rob Procter, and Peter
Tolmie. Analysing how people orient to and spread rumours in social media by
looking at conversational threads. arXiv preprint arXiv:1511.07487, 2015.

20. Arkaitz Zubiaga and Heng Ji. Tweet, but verify: epistemic study of information
veriﬁcation on Twitter. Social Network Analysis and Mining, 4(1):1–12, 2014.

9
1
0
2
 
v
o
N
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
2
1
8
0
.
1
1
9
1
:
v
i
X
r
a

In Search of Credible News

Momchil Hardalov1, Ivan Koychev1, and Preslav Nakov2

1 FMI, Soﬁa University “St. Kliment Ohridski”, Soﬁa, Bulgaria,
momchil.hardalov@gmail.com,koychev@fmi.uni-sofia.bg
2 Qatar Computing Research Institute, HBKU, Doha, Qatar,
pnakov@qf.org.qa

Abstract. We study the problem of ﬁnding fake online news. This is
an important problem as news of questionable credibility have recently
been proliferating in social media at an alarming scale. As this is an
understudied problem, especially for languages other than English, we
ﬁrst collect and release to the research community three new balanced
credible vs. fake news datasets derived from four online sources. We then
propose a language-independent approach for automatically distinguish-
ing credible from fake news, based on a rich feature set. In particular, we
use linguistic (n-gram), credibility-related (capitalization, punctuation,
pronoun use, sentiment polarity), and semantic (embeddings and DB-
Pedia data) features. Our experiments on three diﬀerent testsets show
that our model can distinguish credible from fake news with very high
accuracy.

Keywords: Credibility, veracity, fact checking, humor detection.

1

Introduction

Internet and the proliferation of smart mobile devices have changed the way
information spreads, e.g., social media, blogs, and micro-blogging services such
as Twitter, Facebook and Google+ have become some of the main sources of
information for millions of users on a daily basis. On the positive side, this has
democratized and accelerated content creation and sharing. On the negative
side, it has made people vulnerable to manipulation, as the information in social
media is typically not monitored or moderated in any way. Thus, it has become
increasingly harder to distinguish real news from misinformation, rumors, unver-
iﬁed, manipulative, and even fake content. Not only are online blogs nowadays
ﬂooded by biased comments and fake content, but also online news media in
turn are ﬁlled with unreliable and unveriﬁed content, e.g., due to the willingness
of journalists to be the ﬁrst to write about a hot topic, often by-passing the
veriﬁcation of their information sources; there are also some online information
sources created with the sole purpose of spreading manipulative and biased in-
formation. Finally, the problem extends beyond the cyberspace, as in some cases,
fake news from online sources have crept into mainstream media.

Journalists, regular online users, and researchers are well aware of the issue,
and topics such as information credibility, veracity, and fact checking are becom-
ing increasingly important research directions [3,5,20]. For example, there was
a recent 2016 special issue of the ACM Transactions on Information Systems
journal on Trust and Veracity of Information in Social Media [15], and there is
an upcoming SemEval-2017 shared task on rumor detection.

As English is the primary language of the Web, most research on informa-
tion credibility and veracity has focused on English, while other languages have
been largely neglected. To bridge this gap, below we present experiments in dis-
tinguishing real from fake news in Bulgarian; yet, our approach is in principle
language-independent. In particular, we distinguish between real news vs. fake
news that in some cases are designed to sound funny (while still resembling real
ones); thus, our task can be also seen as humor detection [11,16].

As there was no publicly available dataset that we could use, we had to create
one ourselves. We collected two types of news: credible, coming from trusted
online sources, and fake news, written with the intention to amuse, or sometimes
confuse, the reader who is not knowledgeable enough about the subject. We then
built a model to distinguish between the two, which achieved very high accuracy.
The remainder of this paper is organized as follows: Section 2 presents some
related work. Section 3 introduces our method for distinguishing credible from
fake news. Section 4 presents our data, feature selection, the experiments, and
the results. Finally, Section 5 concludes and suggests directions for future work.

2 Related Work

Information credibility in social media is studied by Castillo & al. [3], who for-
mulate it as a problem of ﬁnding false information about a newsworthy event.
They focus on tweets using variety of features including user reputation, author
writing style, and various time-based features.

Zubiaga & al. [19] studied how people handle rumors in social media. They
found that users with higher reputation are more trusted, and thus can spread
rumors among other users without raising suspicions about the credibility of the
news or of its source.

Online personal blogs are another popular way to spread information by
presenting personal opinions, even though researchers disagree about how much
people trust such blogs. Johnson & al. [6] studied how blog users act in the time
of newsworthy event, e.g., such as the crisis in Iraq, and how biased users try to
inﬂuence other people.

It is not only social media that can spread information of questionable quality.
The credibility of the information published on online news portals has also been
questioned by a number of researchers [1,8,4]. As timing is a crucial factor when it
comes to publishing breaking news, it is simply not possible to double-check the
facts and the sources, as is usually standard in respectable printed newspapers
and magazines. This is one of the biggest concerns about online news media that
journalists have [2].

The interested reader can see [17] for a review of various methods for de-
tecting fake news, where diﬀerent approaches are compared based on linguistic
analysis, discourse, linked data, and social network features.

Finally, we should also mention work on humor detection. Yang & al. [16]
identify semantic structures behind humor, and then design sets of features for
each structure; they further develop anchors that enable humor in a sentence.
However, they mix diﬀerent genres such as news, community question answers,
and proverbs, as well as the One-Liner dataset [11]. In contrast, we focus on
news both for positive and for negative examples, and we do not assume that
the reason for a news being not credible is the humor it contains.

We propose a language-independent approach for automatically distinguishing
credible from fake news, based on a rich feature set. In particular, we use lin-
guistic (n-gram), credibility (capitalization, punctuation, pronoun use, sentiment
polarity), and semantic (embeddings and DBPedia data) features.

3 Method

3.1 Features

Linguistic (n-gram) Features Before generating these features, we ﬁrst per-
form initial pre-processing: tokenization and stop word removal. We deﬁne stop
words as the most common, functional words in a language (e.g., conjunctions,
prepositions, interjections, etc.); while they ﬁt well for problems such as author
proﬁling, they turn out not to be particularly useful for distinguishing credi-
ble from fake news. Eventually, we experimented with the following linguistic
features:

– n-grams: presence of individual uni-grams and bi-grams. The rationale is
that some n-grams are more typical of credible vs. fake news, and vice versa;

– tf-idf: the same n-grams, but weighted using tf-idf;
– vocabulary richness: the number of unique word types used in the article,

possibly normalized by the number of word tokens.

Credibility Features We also used the following credibility features, which
were previously proposed in the literature [3]:

1. Length of the article (number of tokens);
2. Fraction of words that only contain uppercase letters;
3. Fraction of words that start with an uppercase letter;
4. Fraction of words that contain at least one uppercase letter;
5. Fraction of words that only contain lowercase letters;
6. Fraction of plural pronouns;
7. Fraction of singular pronouns;
8. Fraction of ﬁrst person pronouns;

9. Fraction of second person pronouns;
10. Fraction of third person pronouns;
11. Number of URLs;
12. Number of occurrences of an exclamation mark;
13. Number of occurrences of a question mark;
14. Number of occurrences of a hashtag;
15. Number of occurrences of a single quote;
16. Number of occurrences of a double quote.

We further added some sentiment-polarity features from lexicons generated
from Bulgarian movie reviews [7] (5,016 positive, and 2,415 negative words),
which we further expanded with some more words. Based on these lexicons, we
calculated the following features:

17. Proportion of positive words;
18. Proportion of negative words;
19. Sum of the sentiment scores for the positive words;
20. Sum of the sentiment scores for the negative words.

Note that we eventually ended up using only a subset of the above features,

as we performed feature selection as described in Section 4.2 below.

Semantic (Embedding and DBPedia) Features Finally, we use embedding
vectors to model the semantics of the documents. We wanted to model implicitly
some general world knowledge, and thus we trained word2vec vectors on the text
of the long abstracts from the Bulgarian DBPedia.3 Then, we built vectors for
a document as the average of the word2vec vectors of the non-stop word tokens
it is composed of.

3.2 Classiﬁcation

As we have a rich set of partially overlapping features, we used logistic regres-
sion for classiﬁcation with L-BFGS [10] optimizer and elastic net regularization
[18], which combines L1 and L2 regularization. This classiﬁcation setup con-
verges very fast, ﬁts well in huge feature spaces, is robust to over-ﬁtting, and
handles overlapping features well. We ﬁne-tuned the hyper-parameters of our
classiﬁer (maximum number of iterations, elastic net parameters, and regular-
ization parameters) on the training dataset. We further applied feature selection
as described below.

4 Experiments and Evaluation

4.1 Data

As there was no pre-existing suitable dataset for Bulgarian, we had to create
one of our own. For this purpose, we collected a diverse dataset with enough

3 http://wiki.dbpedia.org/

samples in each category. We further wanted to make sure that our dataset will
be good for modeling credible vs. fake news, i.e., that will not degenerate into
related tasks such as topic detection (which might happen if the credible and
the fake news are about diﬀerent topics), authorship attribution (which could
be the case if the fake news are written by just 1-2 authors) or source prediction
(which can occur if all credible/fake news come from just one source). Thus, we
used four Bulgarian news sources (from which we generated one training and
three separate balanced testing datasets):

1. We retrieved most of our credible news from Dnevnik,4 a respected Bulgar-
ian newspaper; we focused mostly on politics. This dataset was previously
used in research on ﬁnding opinion manipulation trolls [12,13,14], but its
news content ﬁts well for our task too (5,896 credible news);

2. As our main online source of fake news, we used a website with funny news
called Ne!Novinite.5 We crawled topics such as politics, sports, culture,
world news, horoscopes, interviews, and user-written articles (6,382 fake
news);

3. As an additional source of fake news, we used articles from the Bazikileaks6
blog. These documents are written in the form of blog-posts and the content
may be classiﬁed as “ﬁctitious”, which is another subcategory of fake news.
The domain is politics (656 fake news);

4. And ﬁnally, we retrieved news from the bTV Lifestyle section,7 which
contains both credible (in the bTV subsection) and fake news (in the bTV
Duplex subsection). In both subsections, the articles are about popular peo-
ple and events (69 credible and 68 fake news);

We used the documents from Dnevnik and Ne!Novinite for training and test-
ing: 70% for training and 30% for testing. We further had two additional test
sets: one of bTV vs. bTV Duplex, and one on Dnevnik vs. Bazikileaks. All test
datasets are near-perfectly balanced.

Finally, as we have already mentioned above, we used the long abstracts in
the Bulgarian DbPedia to train word2vec vectors, which we then used to build
document vectors, which we used as features for classiﬁcation. (171,444 credible
samples).

4.2 Feature Selection

We performed feature selection on the credibility features. For this purpose,
we ﬁrst used Learning Vector Quantization (LVQ) [9] to obtain a ranking of the
features from Section 3.1 by their importance on the training dataset; the results
are shown in Table 1. See also Figure 1 for a comparison of the distribution of
some of the credibility features in credible. vs. funny news.

4 http://www.dnevnik.bg/
5 http://www.nenovinite.com/
6 https://neverojatno.wordpress.com/
7 http://www.btv.bg/lifestyle/all/

Features
doubleQuotes 16
upperCaseCount 4
lowerUpperCase 5
ﬁrstUpperCase 3
pluralPronouns 6
ﬁrstPersonPronouns 8
allUpperCaseCount 2
negativeWords 18
positiveWords 17
tokensCount 1
singularPronouns 7
thirdPersonPronouns 10
negativeWordsScore 20
hashtags 14
urls 11
positiveWordsScore 17
singleQuotes 15
secondPersonPronouns 9
questionMarks 13
exclMarks 12

Importance
0.7911
0.7748
0.7717
0.7708
0.6558
0.6346
0.6282
0.5944
0.5834
0.5779
0.5286
0.5273
0.5206
0.4998
0.4987
0.4910
0.4884
0.4408
0.4407
0.3160

Table 1. Features ranked by the LVQ importance metric.

Then, we experimented with various feature combinations of the top-ranked
features, and we selected the combination that worked best on cross-validation
on the training dataset (compare to Table 1):

– Fraction of negative words in the text (negativeWords);

– Fraction of words that contain uppercase letters only (allUpperCaseCount);

– Fraction of words that start with an uppercase letter (ﬁrstUpperCase);

– Fraction of words that only contain lowercase letters (lowerUpperCase);

– Fraction of plural pronouns in the text (pluralPronouns);

– Number of occurrences of exclamation marks (exclMarks);

– Number of occurrences of double quotes (doubleQuotes).

Fig. 1. Boxplots presenting the distributions of some credibility features in credible vs.
funny news.

Feature Groups

Dnevnik
Ne!Novinite

Credibility + Linguistic + Semantic 99.36
92.67
Credibility + Semantic
96.02
Linguistic + Credibility
98.95
Semantic
95.71
Linguistic
83.25
Credibility
52.60
Baseline (majority class)

Dnevnik vs.
bTV vs.
bTV Duplex Bazikileaks
62.04
75.91
59.12
61.31
56.93
62.04
50.36

85.53
82.99
61.94
71.01
73.25
79.85
50.86

Table 2. Accuracy for diﬀerent feature group combinations.

4.3 Results

Table 2 shows the results when using all feature groups and when turning oﬀ
some of them. We can see that the best results are achieved when experiment-
ing with “Credibility + Semantic” and “Credibility + Linguistic + Semantic”
feature combinations, and the results are worse when only using credibility and
linguistic features.

Analyzing the results on the Dnevnik vs. the Ne!Novinite testset (ﬁrst col-
umn), we can see that the linguistic features are more important than the
credibility ones. Yet, the semantic features are even more important. When we
combine all the feature groups, we achieve 99.36% accuracy, but this is only
marginally better than using the semantic features alone. Note, however, that
using semantic features only does not perform so well on the other two test
datasets, especially on the last one.

The linguistic features work relatively well on two of the test datasets, but
not on bTV, where the combination of “Credibility + Semantic” is the best-
performing one.

Naturally, the best results are on the Dnevnik vs. NE!Novinite, where the
classiﬁer achieves near perfect accuracy (note that this is despite the diﬀerent
class distribution on training vs. testing). The hardest testing dataset is bTV,
where both the positive and the negative class are from sources diﬀerent from
those used in the training dataset; yet, we achieve up to 75.91% accuracy, which
is well above the majority class baseline of 50.36. The Dnevnik vs. Bazikileaks
dataset falls somewhere in between, with up to 85.53% accuracy; this is to be
expected as the positive examples come from the same source as for the training
dataset (even though the negative class is diﬀerent).

Overall, on all three datasets, we achieved accuracy of 75-99%, which is well
above the majority class baseline. The strong relative performance on the three
diﬀerent test datasets that come from diﬀerent sources suggests that our model
really learns to distinguish credible vs. fake news rather than learning to classify
topics, sources, or author style.

5 Conclusion and Future Work

We have presented a feature-rich language-independent approach for distin-
guishing credible from fake news. In particular, we used linguistic (n-gram),
credibility-related (capitalization, punctuation, pronoun use, sentiment polar-
ity, etc., with feature selection), and semantic (embeddings and DBPedia data)
features. Our experiments on three diﬀerent testsets, derived from four diﬀerent
sources, have shown that our model can distinguish credible from fake news with
very high accuracy, well above a majority-class baseline.

In future work, we plan to experiment with more features, e.g., based on
linked data [17], or on discourse analysis [17]. Looking at features used for related
tasks such as humor- [16] and rumor-related [19] is another promising direction
for future work. We also want to apply deep learning, which can eliminate the
need for feature engineering altogether.

Last but not least, we would like to note that we have made our source code

and datasets publicly available for research purposes at the following URL:

https://github.com/mhardalov/news-credibility

Acknowledgments. This research was performed by Momchil Hardalov, a student
in Computer Science in the Soﬁa University “St Kliment Ohridski”, as part of
his MSc thesis. It is also part of the Interactive sYstems for Answer Search
(Iyas) project, which is developed by the Arabic Language Technologies (ALT)
group at the Qatar Computing Research Institute (QCRI), HBKU, part of Qatar
Foundation in collaboration with MIT-CSAIL.

References

1. Ann M Brill. Online journalists embrace new marketing function. Newspaper

Research Journal, 22(2):28, 2001.

2. William P Cassidy. Online news credibility: An examination of the perceptions of
newspaper journalists. Journal of Computer-Mediated Communication, 12(2):478–
498, 2007.

3. Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. Predicting information
credibility in time-sensitive social media. Internet Research, 23(5):560–588, 2013.
4. Howard Finberg, Martha L Stone, and Diane Lynch. Digital journalism credibility

study. Online News Association. Retrieved November, 3:2003, 2002.

5. Lucas Graves. Deciding What’s True: Fact-Checking Journalism and the New

Ecology of News. PhD thesis, Columbia University, 2013.

6. Thomas J Johnson, Barbara K Kaye, Shannon L Bichard, and W Joann Wong.
Every blog has its day: Politically-interested internet users perceptions of blog
credibility. Journal of Computer-Mediated Communication, 13(1):100–122, 2007.
7. Borislav Kapukaranov and Preslav Nakov. Fine-grained sentiment analysis for
movie reviews in Bulgarian. In Proceedings of Recent Advances in Natural Language
Processing, RANLP ’15, pages 266–274, Hissar, Bulgaria, 2015.

8. Stan Ketterer. Teaching students how to evaluate and use online resources. Jour-

nalism & Mass Communication Educator, 52(4):4, 1998.

9. Teuvo Kohonen.

Improved versions of learning vector quantization.

In IJCNN

International Joint Conference on Neural Networks, pages 545–550, 1990.

10. Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large

scale optimization. Mathematical programming, 45(1-3):503–528, 1989.

11. Rada Mihalcea and Carlo Strapparava. Making computers laugh: Investigations
in automatic humor recognition. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Natural Language Processing,
HLT-EMNLP ’05, pages 531–538, Vancouver, British Columbia, Canada, 2005.
12. Todor Mihaylov, Georgi Georgiev, and Preslav Nakov. Finding opinion manipula-
tion trolls in news community forums. In Proceedings of the Nineteenth Conference
on Computational Natural Language Learning, CoNLL ’15, pages 310–314, Beijing,
China, 2015.

13. Todor Mihaylov, Ivan Koychev, Georgi Georgiev, and Preslav Nakov. Exposing
paid opinion manipulation trolls. In Proceedings of the International Conference
Recent Advances in Natural Language Processing, RANLP ’15, pages 443–450,
Hissar, Bulgaria, 2015.

14. Todor Mihaylov and Preslav Nakov. Hunting for troll comments in news com-
munity forums. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL ’16, Berlin, Germany, 2016.

15. Symeon Papadopoulos, Kalina Bontcheva, Eva Jaho, Mihai Lupu, and Carlos
Castillo. Overview of the special issue on trust and veracity of information in
social media. ACM Trans. Inf. Syst., 34(3):14:1–14:5, April 2016.

16. Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. Humor recognition and
In Proceedings of the 2015 Conference on Empirical
humor anchor extraction.
Methods in Natural Language Processing, EMNLP ’15, pages 2367–2376, Lisbon,
Portugal, 2015.

17. Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion
Stoica. Spark: Cluster computing with working sets. In Proceedings of the 2nd
USENIX Conference on Hot Topics in Cloud Computing, HotCloud ’10, pages
10–10, Boston, MA, 2010.

18. Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic
net. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
67(2):301–320, 2005.

19. Arkaitz Zubiaga, Geraldine Wong Sak Hoi, Maria Liakata, Rob Procter, and Peter
Tolmie. Analysing how people orient to and spread rumours in social media by
looking at conversational threads. arXiv preprint arXiv:1511.07487, 2015.

20. Arkaitz Zubiaga and Heng Ji. Tweet, but verify: epistemic study of information
veriﬁcation on Twitter. Social Network Analysis and Mining, 4(1):1–12, 2014.

9
1
0
2
 
v
o
N
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
2
1
8
0
.
1
1
9
1
:
v
i
X
r
a

In Search of Credible News

Momchil Hardalov1, Ivan Koychev1, and Preslav Nakov2

1 FMI, Soﬁa University “St. Kliment Ohridski”, Soﬁa, Bulgaria,
momchil.hardalov@gmail.com,koychev@fmi.uni-sofia.bg
2 Qatar Computing Research Institute, HBKU, Doha, Qatar,
pnakov@qf.org.qa

Abstract. We study the problem of ﬁnding fake online news. This is
an important problem as news of questionable credibility have recently
been proliferating in social media at an alarming scale. As this is an
understudied problem, especially for languages other than English, we
ﬁrst collect and release to the research community three new balanced
credible vs. fake news datasets derived from four online sources. We then
propose a language-independent approach for automatically distinguish-
ing credible from fake news, based on a rich feature set. In particular, we
use linguistic (n-gram), credibility-related (capitalization, punctuation,
pronoun use, sentiment polarity), and semantic (embeddings and DB-
Pedia data) features. Our experiments on three diﬀerent testsets show
that our model can distinguish credible from fake news with very high
accuracy.

Keywords: Credibility, veracity, fact checking, humor detection.

1

Introduction

Internet and the proliferation of smart mobile devices have changed the way
information spreads, e.g., social media, blogs, and micro-blogging services such
as Twitter, Facebook and Google+ have become some of the main sources of
information for millions of users on a daily basis. On the positive side, this has
democratized and accelerated content creation and sharing. On the negative
side, it has made people vulnerable to manipulation, as the information in social
media is typically not monitored or moderated in any way. Thus, it has become
increasingly harder to distinguish real news from misinformation, rumors, unver-
iﬁed, manipulative, and even fake content. Not only are online blogs nowadays
ﬂooded by biased comments and fake content, but also online news media in
turn are ﬁlled with unreliable and unveriﬁed content, e.g., due to the willingness
of journalists to be the ﬁrst to write about a hot topic, often by-passing the
veriﬁcation of their information sources; there are also some online information
sources created with the sole purpose of spreading manipulative and biased in-
formation. Finally, the problem extends beyond the cyberspace, as in some cases,
fake news from online sources have crept into mainstream media.

Journalists, regular online users, and researchers are well aware of the issue,
and topics such as information credibility, veracity, and fact checking are becom-
ing increasingly important research directions [3,5,20]. For example, there was
a recent 2016 special issue of the ACM Transactions on Information Systems
journal on Trust and Veracity of Information in Social Media [15], and there is
an upcoming SemEval-2017 shared task on rumor detection.

As English is the primary language of the Web, most research on informa-
tion credibility and veracity has focused on English, while other languages have
been largely neglected. To bridge this gap, below we present experiments in dis-
tinguishing real from fake news in Bulgarian; yet, our approach is in principle
language-independent. In particular, we distinguish between real news vs. fake
news that in some cases are designed to sound funny (while still resembling real
ones); thus, our task can be also seen as humor detection [11,16].

As there was no publicly available dataset that we could use, we had to create
one ourselves. We collected two types of news: credible, coming from trusted
online sources, and fake news, written with the intention to amuse, or sometimes
confuse, the reader who is not knowledgeable enough about the subject. We then
built a model to distinguish between the two, which achieved very high accuracy.
The remainder of this paper is organized as follows: Section 2 presents some
related work. Section 3 introduces our method for distinguishing credible from
fake news. Section 4 presents our data, feature selection, the experiments, and
the results. Finally, Section 5 concludes and suggests directions for future work.

2 Related Work

Information credibility in social media is studied by Castillo & al. [3], who for-
mulate it as a problem of ﬁnding false information about a newsworthy event.
They focus on tweets using variety of features including user reputation, author
writing style, and various time-based features.

Zubiaga & al. [19] studied how people handle rumors in social media. They
found that users with higher reputation are more trusted, and thus can spread
rumors among other users without raising suspicions about the credibility of the
news or of its source.

Online personal blogs are another popular way to spread information by
presenting personal opinions, even though researchers disagree about how much
people trust such blogs. Johnson & al. [6] studied how blog users act in the time
of newsworthy event, e.g., such as the crisis in Iraq, and how biased users try to
inﬂuence other people.

It is not only social media that can spread information of questionable quality.
The credibility of the information published on online news portals has also been
questioned by a number of researchers [1,8,4]. As timing is a crucial factor when it
comes to publishing breaking news, it is simply not possible to double-check the
facts and the sources, as is usually standard in respectable printed newspapers
and magazines. This is one of the biggest concerns about online news media that
journalists have [2].

The interested reader can see [17] for a review of various methods for de-
tecting fake news, where diﬀerent approaches are compared based on linguistic
analysis, discourse, linked data, and social network features.

Finally, we should also mention work on humor detection. Yang & al. [16]
identify semantic structures behind humor, and then design sets of features for
each structure; they further develop anchors that enable humor in a sentence.
However, they mix diﬀerent genres such as news, community question answers,
and proverbs, as well as the One-Liner dataset [11]. In contrast, we focus on
news both for positive and for negative examples, and we do not assume that
the reason for a news being not credible is the humor it contains.

We propose a language-independent approach for automatically distinguishing
credible from fake news, based on a rich feature set. In particular, we use lin-
guistic (n-gram), credibility (capitalization, punctuation, pronoun use, sentiment
polarity), and semantic (embeddings and DBPedia data) features.

3 Method

3.1 Features

Linguistic (n-gram) Features Before generating these features, we ﬁrst per-
form initial pre-processing: tokenization and stop word removal. We deﬁne stop
words as the most common, functional words in a language (e.g., conjunctions,
prepositions, interjections, etc.); while they ﬁt well for problems such as author
proﬁling, they turn out not to be particularly useful for distinguishing credi-
ble from fake news. Eventually, we experimented with the following linguistic
features:

– n-grams: presence of individual uni-grams and bi-grams. The rationale is
that some n-grams are more typical of credible vs. fake news, and vice versa;

– tf-idf: the same n-grams, but weighted using tf-idf;
– vocabulary richness: the number of unique word types used in the article,

possibly normalized by the number of word tokens.

Credibility Features We also used the following credibility features, which
were previously proposed in the literature [3]:

1. Length of the article (number of tokens);
2. Fraction of words that only contain uppercase letters;
3. Fraction of words that start with an uppercase letter;
4. Fraction of words that contain at least one uppercase letter;
5. Fraction of words that only contain lowercase letters;
6. Fraction of plural pronouns;
7. Fraction of singular pronouns;
8. Fraction of ﬁrst person pronouns;

9. Fraction of second person pronouns;
10. Fraction of third person pronouns;
11. Number of URLs;
12. Number of occurrences of an exclamation mark;
13. Number of occurrences of a question mark;
14. Number of occurrences of a hashtag;
15. Number of occurrences of a single quote;
16. Number of occurrences of a double quote.

We further added some sentiment-polarity features from lexicons generated
from Bulgarian movie reviews [7] (5,016 positive, and 2,415 negative words),
which we further expanded with some more words. Based on these lexicons, we
calculated the following features:

17. Proportion of positive words;
18. Proportion of negative words;
19. Sum of the sentiment scores for the positive words;
20. Sum of the sentiment scores for the negative words.

Note that we eventually ended up using only a subset of the above features,

as we performed feature selection as described in Section 4.2 below.

Semantic (Embedding and DBPedia) Features Finally, we use embedding
vectors to model the semantics of the documents. We wanted to model implicitly
some general world knowledge, and thus we trained word2vec vectors on the text
of the long abstracts from the Bulgarian DBPedia.3 Then, we built vectors for
a document as the average of the word2vec vectors of the non-stop word tokens
it is composed of.

3.2 Classiﬁcation

As we have a rich set of partially overlapping features, we used logistic regres-
sion for classiﬁcation with L-BFGS [10] optimizer and elastic net regularization
[18], which combines L1 and L2 regularization. This classiﬁcation setup con-
verges very fast, ﬁts well in huge feature spaces, is robust to over-ﬁtting, and
handles overlapping features well. We ﬁne-tuned the hyper-parameters of our
classiﬁer (maximum number of iterations, elastic net parameters, and regular-
ization parameters) on the training dataset. We further applied feature selection
as described below.

4 Experiments and Evaluation

4.1 Data

As there was no pre-existing suitable dataset for Bulgarian, we had to create
one of our own. For this purpose, we collected a diverse dataset with enough

3 http://wiki.dbpedia.org/

samples in each category. We further wanted to make sure that our dataset will
be good for modeling credible vs. fake news, i.e., that will not degenerate into
related tasks such as topic detection (which might happen if the credible and
the fake news are about diﬀerent topics), authorship attribution (which could
be the case if the fake news are written by just 1-2 authors) or source prediction
(which can occur if all credible/fake news come from just one source). Thus, we
used four Bulgarian news sources (from which we generated one training and
three separate balanced testing datasets):

1. We retrieved most of our credible news from Dnevnik,4 a respected Bulgar-
ian newspaper; we focused mostly on politics. This dataset was previously
used in research on ﬁnding opinion manipulation trolls [12,13,14], but its
news content ﬁts well for our task too (5,896 credible news);

2. As our main online source of fake news, we used a website with funny news
called Ne!Novinite.5 We crawled topics such as politics, sports, culture,
world news, horoscopes, interviews, and user-written articles (6,382 fake
news);

3. As an additional source of fake news, we used articles from the Bazikileaks6
blog. These documents are written in the form of blog-posts and the content
may be classiﬁed as “ﬁctitious”, which is another subcategory of fake news.
The domain is politics (656 fake news);

4. And ﬁnally, we retrieved news from the bTV Lifestyle section,7 which
contains both credible (in the bTV subsection) and fake news (in the bTV
Duplex subsection). In both subsections, the articles are about popular peo-
ple and events (69 credible and 68 fake news);

We used the documents from Dnevnik and Ne!Novinite for training and test-
ing: 70% for training and 30% for testing. We further had two additional test
sets: one of bTV vs. bTV Duplex, and one on Dnevnik vs. Bazikileaks. All test
datasets are near-perfectly balanced.

Finally, as we have already mentioned above, we used the long abstracts in
the Bulgarian DbPedia to train word2vec vectors, which we then used to build
document vectors, which we used as features for classiﬁcation. (171,444 credible
samples).

4.2 Feature Selection

We performed feature selection on the credibility features. For this purpose,
we ﬁrst used Learning Vector Quantization (LVQ) [9] to obtain a ranking of the
features from Section 3.1 by their importance on the training dataset; the results
are shown in Table 1. See also Figure 1 for a comparison of the distribution of
some of the credibility features in credible. vs. funny news.

4 http://www.dnevnik.bg/
5 http://www.nenovinite.com/
6 https://neverojatno.wordpress.com/
7 http://www.btv.bg/lifestyle/all/

Features
doubleQuotes 16
upperCaseCount 4
lowerUpperCase 5
ﬁrstUpperCase 3
pluralPronouns 6
ﬁrstPersonPronouns 8
allUpperCaseCount 2
negativeWords 18
positiveWords 17
tokensCount 1
singularPronouns 7
thirdPersonPronouns 10
negativeWordsScore 20
hashtags 14
urls 11
positiveWordsScore 17
singleQuotes 15
secondPersonPronouns 9
questionMarks 13
exclMarks 12

Importance
0.7911
0.7748
0.7717
0.7708
0.6558
0.6346
0.6282
0.5944
0.5834
0.5779
0.5286
0.5273
0.5206
0.4998
0.4987
0.4910
0.4884
0.4408
0.4407
0.3160

Table 1. Features ranked by the LVQ importance metric.

Then, we experimented with various feature combinations of the top-ranked
features, and we selected the combination that worked best on cross-validation
on the training dataset (compare to Table 1):

– Fraction of negative words in the text (negativeWords);

– Fraction of words that contain uppercase letters only (allUpperCaseCount);

– Fraction of words that start with an uppercase letter (ﬁrstUpperCase);

– Fraction of words that only contain lowercase letters (lowerUpperCase);

– Fraction of plural pronouns in the text (pluralPronouns);

– Number of occurrences of exclamation marks (exclMarks);

– Number of occurrences of double quotes (doubleQuotes).

Fig. 1. Boxplots presenting the distributions of some credibility features in credible vs.
funny news.

Feature Groups

Dnevnik
Ne!Novinite

Credibility + Linguistic + Semantic 99.36
92.67
Credibility + Semantic
96.02
Linguistic + Credibility
98.95
Semantic
95.71
Linguistic
83.25
Credibility
52.60
Baseline (majority class)

Dnevnik vs.
bTV vs.
bTV Duplex Bazikileaks
62.04
75.91
59.12
61.31
56.93
62.04
50.36

85.53
82.99
61.94
71.01
73.25
79.85
50.86

Table 2. Accuracy for diﬀerent feature group combinations.

4.3 Results

Table 2 shows the results when using all feature groups and when turning oﬀ
some of them. We can see that the best results are achieved when experiment-
ing with “Credibility + Semantic” and “Credibility + Linguistic + Semantic”
feature combinations, and the results are worse when only using credibility and
linguistic features.

Analyzing the results on the Dnevnik vs. the Ne!Novinite testset (ﬁrst col-
umn), we can see that the linguistic features are more important than the
credibility ones. Yet, the semantic features are even more important. When we
combine all the feature groups, we achieve 99.36% accuracy, but this is only
marginally better than using the semantic features alone. Note, however, that
using semantic features only does not perform so well on the other two test
datasets, especially on the last one.

The linguistic features work relatively well on two of the test datasets, but
not on bTV, where the combination of “Credibility + Semantic” is the best-
performing one.

Naturally, the best results are on the Dnevnik vs. NE!Novinite, where the
classiﬁer achieves near perfect accuracy (note that this is despite the diﬀerent
class distribution on training vs. testing). The hardest testing dataset is bTV,
where both the positive and the negative class are from sources diﬀerent from
those used in the training dataset; yet, we achieve up to 75.91% accuracy, which
is well above the majority class baseline of 50.36. The Dnevnik vs. Bazikileaks
dataset falls somewhere in between, with up to 85.53% accuracy; this is to be
expected as the positive examples come from the same source as for the training
dataset (even though the negative class is diﬀerent).

Overall, on all three datasets, we achieved accuracy of 75-99%, which is well
above the majority class baseline. The strong relative performance on the three
diﬀerent test datasets that come from diﬀerent sources suggests that our model
really learns to distinguish credible vs. fake news rather than learning to classify
topics, sources, or author style.

5 Conclusion and Future Work

We have presented a feature-rich language-independent approach for distin-
guishing credible from fake news. In particular, we used linguistic (n-gram),
credibility-related (capitalization, punctuation, pronoun use, sentiment polar-
ity, etc., with feature selection), and semantic (embeddings and DBPedia data)
features. Our experiments on three diﬀerent testsets, derived from four diﬀerent
sources, have shown that our model can distinguish credible from fake news with
very high accuracy, well above a majority-class baseline.

In future work, we plan to experiment with more features, e.g., based on
linked data [17], or on discourse analysis [17]. Looking at features used for related
tasks such as humor- [16] and rumor-related [19] is another promising direction
for future work. We also want to apply deep learning, which can eliminate the
need for feature engineering altogether.

Last but not least, we would like to note that we have made our source code

and datasets publicly available for research purposes at the following URL:

https://github.com/mhardalov/news-credibility

Acknowledgments. This research was performed by Momchil Hardalov, a student
in Computer Science in the Soﬁa University “St Kliment Ohridski”, as part of
his MSc thesis. It is also part of the Interactive sYstems for Answer Search
(Iyas) project, which is developed by the Arabic Language Technologies (ALT)
group at the Qatar Computing Research Institute (QCRI), HBKU, part of Qatar
Foundation in collaboration with MIT-CSAIL.

References

1. Ann M Brill. Online journalists embrace new marketing function. Newspaper

Research Journal, 22(2):28, 2001.

2. William P Cassidy. Online news credibility: An examination of the perceptions of
newspaper journalists. Journal of Computer-Mediated Communication, 12(2):478–
498, 2007.

3. Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. Predicting information
credibility in time-sensitive social media. Internet Research, 23(5):560–588, 2013.
4. Howard Finberg, Martha L Stone, and Diane Lynch. Digital journalism credibility

study. Online News Association. Retrieved November, 3:2003, 2002.

5. Lucas Graves. Deciding What’s True: Fact-Checking Journalism and the New

Ecology of News. PhD thesis, Columbia University, 2013.

6. Thomas J Johnson, Barbara K Kaye, Shannon L Bichard, and W Joann Wong.
Every blog has its day: Politically-interested internet users perceptions of blog
credibility. Journal of Computer-Mediated Communication, 13(1):100–122, 2007.
7. Borislav Kapukaranov and Preslav Nakov. Fine-grained sentiment analysis for
movie reviews in Bulgarian. In Proceedings of Recent Advances in Natural Language
Processing, RANLP ’15, pages 266–274, Hissar, Bulgaria, 2015.

8. Stan Ketterer. Teaching students how to evaluate and use online resources. Jour-

nalism & Mass Communication Educator, 52(4):4, 1998.

9. Teuvo Kohonen.

Improved versions of learning vector quantization.

In IJCNN

International Joint Conference on Neural Networks, pages 545–550, 1990.

10. Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large

scale optimization. Mathematical programming, 45(1-3):503–528, 1989.

11. Rada Mihalcea and Carlo Strapparava. Making computers laugh: Investigations
in automatic humor recognition. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Natural Language Processing,
HLT-EMNLP ’05, pages 531–538, Vancouver, British Columbia, Canada, 2005.
12. Todor Mihaylov, Georgi Georgiev, and Preslav Nakov. Finding opinion manipula-
tion trolls in news community forums. In Proceedings of the Nineteenth Conference
on Computational Natural Language Learning, CoNLL ’15, pages 310–314, Beijing,
China, 2015.

13. Todor Mihaylov, Ivan Koychev, Georgi Georgiev, and Preslav Nakov. Exposing
paid opinion manipulation trolls. In Proceedings of the International Conference
Recent Advances in Natural Language Processing, RANLP ’15, pages 443–450,
Hissar, Bulgaria, 2015.

14. Todor Mihaylov and Preslav Nakov. Hunting for troll comments in news com-
munity forums. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL ’16, Berlin, Germany, 2016.

15. Symeon Papadopoulos, Kalina Bontcheva, Eva Jaho, Mihai Lupu, and Carlos
Castillo. Overview of the special issue on trust and veracity of information in
social media. ACM Trans. Inf. Syst., 34(3):14:1–14:5, April 2016.

16. Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. Humor recognition and
In Proceedings of the 2015 Conference on Empirical
humor anchor extraction.
Methods in Natural Language Processing, EMNLP ’15, pages 2367–2376, Lisbon,
Portugal, 2015.

17. Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion
Stoica. Spark: Cluster computing with working sets. In Proceedings of the 2nd
USENIX Conference on Hot Topics in Cloud Computing, HotCloud ’10, pages
10–10, Boston, MA, 2010.

18. Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic
net. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
67(2):301–320, 2005.

19. Arkaitz Zubiaga, Geraldine Wong Sak Hoi, Maria Liakata, Rob Procter, and Peter
Tolmie. Analysing how people orient to and spread rumours in social media by
looking at conversational threads. arXiv preprint arXiv:1511.07487, 2015.

20. Arkaitz Zubiaga and Heng Ji. Tweet, but verify: epistemic study of information
veriﬁcation on Twitter. Social Network Analysis and Mining, 4(1):1–12, 2014.

