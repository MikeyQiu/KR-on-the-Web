8
1
0
2
 
y
a
M
 
2
2
 
 
]
I

A
.
s
c
[
 
 
3
v
5
5
4
0
0
.
1
1
7
1
:
v
i
X
r
a

A Uniﬁed View of Piecewise Linear Neural Network Veriﬁcation

Rudy Bunel
University of Oxford
rudy@robots.ox.ac.uk

Ilker Turkaslan
University of Oxford
ilker.turkaslan@lmh.ox.ac.uk

Philip H.S. Torr
University of Oxford
philip.torr@eng.ox.ac.uk

Pushmeet Kohli
Deepmind
pushmeet@google.com

M. Pawan Kumar
University of Oxford
Alan Turing Institute
pawan@robots.ox.ac.uk

Abstract

The success of Deep Learning and its potential use in many safety-critical applica-
tions has motivated research on formal veriﬁcation of Neural Network (NN) mod-
els. Despite the reputation of learned NN models to behave as black boxes and the
theoretical hardness of proving their properties, researchers have been successful
in verifying some classes of models by exploiting their piecewise linear structure
and taking insights from formal methods such as Satisiﬁability Modulo Theory.
These methods are however still far from scaling to realistic neural networks. To
facilitate progress on this crucial area, we make two key contributions. First, we
present a uniﬁed framework that encompasses previous methods. This analysis
results in the identiﬁcation of new methods that combine the strengths of mul-
tiple existing approaches, accomplishing a speedup of two orders of magnitude
compared to the previous state of the art. Second, we propose a new data set of
benchmarks which includes a collection of previously released testcases. We use
the benchmark to provide the ﬁrst experimental comparison of existing algorithms
and identify the factors impacting the hardness of veriﬁcation problems.

1 Introduction
Despite their success in a wide variety of applications, Deep Neural Networks have seen limited
adoption in safety-critical settings. The main explanation for this lies in their reputation for being
black-boxes whose behaviour can not be predicted. Current approaches to evaluate trained models
mostly rely on testing using held-out data sets. However, as Edsger W. Dijkstra said [3], “testing
shows the presence, not the absence of bugs”. If deep learning models are to be deployed to applica-
tions such as autonomous driving cars, we need to be able to verify safety-critical behaviours.
To this end, some researchers have tried to use formal methods. To the best of our knowledge,
Zakrzewski [21] was the ﬁrst to propose a method to verify simple, one hidden layer neural networks.
However, only recently were researchers able to work with non-trivial models by taking advantage
of the structure of ReLU-based networks [5, 11]. Even then, these works are not scalable to the large
networks encountered in most real world problems.
This paper advances the ﬁeld of NN veriﬁcation by making the following key contributions:

1. We reframe state of the art veriﬁcation methods as special cases of Branch-and-Bound

optimization, which provides us with a uniﬁed framework to compare them.

2. We gather a data set of test cases based on the existing literature and extend it with new
benchmarks. We provide the ﬁrst experimental comparison of veriﬁcation methods.

3. Based on this framework, we identify algorithmic improvements in the veriﬁcation process,
speciﬁcally in the way bounds are computed, the type of branching that are considered, as

Preprint. Work in progress.

well as the strategies guiding the branching. Compared to the previous state of the art, these
improvements lead to speed-up of almost two orders of magnitudes.

Section 2 and 3 give the speciﬁcation of the problem and formalise the veriﬁcation process. Section
4 presents our uniﬁed framework, showing that previous methods are special cases and highlight-
ing potential improvements. Section 5 presents our experimental setup and Section 6 analyses the
results.
2 Problem speciﬁcation
We now specify the problem of formal veriﬁcation of neural networks. Given a network that imple-
ments a function ˆxn = f (x0), a bounded input domain C and a property P , we want to prove

(cid:8)

∀y

x0 ∈ C,

ˆxn = f (x0) =⇒ P (ˆxn).

ˆxn[ya] ≥ ˆxn[y](cid:9)
.

(1)
For example,
the property of robustness to adversarial examples in L∞ norm around a
training sample a with label ya would be encoded by using C , {x0| kx0 − ak∞ ≤ ǫ} and
P (ˆxn) =
In this paper, we are going to focus on Piecewise-Linear Neural Networks (PL-NN), that is, networks
for which we can decompose C into a set of polyhedra Ci such that C = ∪i Ci, and the restriction
of f to Ci is a linear function for each i. While this prevents us from including networks that
use activation functions such as sigmoid or tanh, PL-NNs allow the use of linear transformations
such as fully-connected or convolutional layers, pooling units such as MaxPooling and activation
functions such as ReLUs.
In other words, PL-NNs represent the majority of networks used in
practice. Operations such as Batch-Normalization or Dropout also preserve piecewise linearity at
test-time.
The properties that we are going to consider are Boolean formulas over linear inequalities. In our
robustness to adversarial example above, the property is a conjunction of linear inequalities, each of
which constrains the output of the original label to be greater than the output of another label.
The scope of this paper does not include approaches relying on additional assumptions such as twice
differentiability of the network [8, 21], limitation of the activation to binary values [5, 16] or restric-
tion to a single linear domain [2]. Since they do not provide formal guarantees, we also don’t include
approximate approaches relying on a limited set of perturbation [10] or on over-approximation meth-
ods that potentially lead to undecidable properties [17, 20].
3 Veriﬁcation Formalism
3.1 Veriﬁcation as a Satisﬁability problem
The methods we involve in our comparison all leverage the piecewise-linear structure of PL-NN
to make the problem more tractable. They all follow the same general principle: given a property
to prove, they attempt to discover a counterexample that would make the property false. This is
accomplished by deﬁning a set of variables corresponding to the inputs, hidden units and output of
the network, and the set of constraints that a counterexample would satisfy.
To help design a uniﬁed framework, we reduce all instances of veriﬁcation problems to a canoni-
cal representation. Speciﬁcally, the whole satisﬁability problem will be transformed into a global
optimization problem where the decision will be obtained by checking the sign of the minimum.
If the property to verify is a simple inequality P (ˆxn) , cT ˆxn ≥ b, it is sufﬁcient to add to the
network a ﬁnal fully connected layer with one output, with weight of c and a bias of −b. If the
global minimum of this network is positive, it indicates that for all ˆxn the original network can
output, we have cT ˆxn − b ≥ 0 =⇒ cT ˆxn ≥ b, and as a consequence the property is True. On the
other hand, if the global minimum is negative, then the minimizer provides a counter-example. The
supplementary material shows that OR and AND clauses in the property can similarly be expressed as
additional layers, using MaxPooling units.
We can formulate any Boolean formula over linear inequalities on the output of the network
as a sequence of additional linear and max-pooling layers. The veriﬁcation problem will be
reduced to the problem of ﬁnding whether the scalar output of the potentially modiﬁed net-
work can reach a negative value. Assuming the network only contains ReLU activations be-
the satisﬁability problem to ﬁnd a counterexample can be expressed as:
tween each layer,

l0 ≤ x0 ≤ u0
ˆxn ≤ 0

(2a)
(2b)

ˆxi+1 = Wi+1xi + bi+1
xi = max (ˆxi, 0)

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}.

(2c)
(2d)

2

Eq. (10a) represents the constraints on the input and Eq. (10h) on the neural network output.
Eq. (10b) encodes the linear layers of the network and Eq. (2d) the ReLU activation functions. If an
assignment to all the values can be found, this represents a counterexample. If this problem is unsat-
isﬁable, no counterexample can exist, implying that the property is True. We emphasise that we are
required to prove that no counter-examples can exist, and not simply that none could be found.
While for clarity of explanation, we have limited ourselves to the speciﬁc case where only ReLU
activation functions are used, this is not restrictive. The supplementary material contains a section
detailing how each method speciﬁcally handles MaxPooling units, as well as how to convert any
MaxPooling operation into a combination of linear layers and ReLU activation functions.
The problem described in (2) is still a hard problem. The addition of the ReLU non-linearities (2d)
transforms a problem that would have been solvable by simple Linear Programming into an NP-hard
problem [11]. Converting a veriﬁcation problem into this canonical representation does not make
its resolution simpler but it does provide a formalism advantage. Speciﬁcally, it allows us to prove
complex properties, containing several OR clauses, with a single procedure rather than having to
decompose the desired property into separate queries as was done in previous work [11].
Operationally, a valid strategy is to impose the constraints (10a) to (2d) and minimise the value of
ˆxn. Finding the exact global minimum is not necessary for veriﬁcation. However, it provides a
measure of satisﬁability or unsatisﬁability. If the value of the global minimum is positive, it will
correspond to the margin by which the property is satisﬁed.
3.2 Mixed Integer Programming formulation
A possible way to eliminate the non-linearities is to encode them with the help of binary variables,
transforming the PL-NN veriﬁcation problem (2) into a Mixed Integer Linear Program (MIP). This
can be done with the use of “big-M” encoding. The following encoding is from Tjeng & Tedrake
[19]. Assuming we have access to lower and upper bounds on the values that can be taken by the
coordinates of ˆxi, which we denote li and ui, we can replace the non-linearities:

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

xi ≤ ui · δi
xi ≤ ˆxi − li · (1 − δi)

(3a)
(3b)

is

in Eq.

easy to verify that

δi[j] = 0 ⇔ xi[j] = 0 (replacing δi[j]

It
δi[j] = 1 ⇔ xi[j] = ˆxi[j] (replacing δi[j] in Eq. (21b)).
By taking advantage of the feed-forward structure of the neural network, lower and upper bounds
li and ui can be obtained by applying interval arithmetic [9] to propagate the bounds on the inputs,
one layer at a time.
Thanks to this speciﬁc feed-forward structure of the problem, the generic, non-linear, non-convex
problem has been rewritten into an MIP. Optimization of MIP is well studied and highly efﬁcient
off-the-shelf solvers exist. As solving them is NP-hard, performance is going to be dependent on
the quality of both the solver used and the encoding. We now ask the following question: how much
efﬁciency can be gained by using a bespoke solver rather than a generic one? In order to answer this,
we present specialised solvers for the PLNN veriﬁcation task.

(21a))

and

4 Branch-and-Bound for Veriﬁcation
As described in Section 3.1, the veriﬁcation problem can be rephrased as a global optimization
problem. Algorithms such as Stochastic Gradient Descent are not appropriate as they have no way of
guaranteeing whether or not a minima is global. In this section, we present an approach to estimate
the global minimum, based on the Branch and Bound paradigm and show that several published
methods, introduced as examples of Satisﬁability Modulo Theories, ﬁt this framework.
Algorithm 1 describes its generic form. The input domain is repeatedly split into sub-domains (line
7), over which lower and upper bounds of the minimum are computed (lines 9-10). The best upper-
bound found so far serves as a candidate for the global minimum. Any domain whose lower bound
is greater than the current global upper bound can be pruned away as it cannot contain the global
minimum (line 13, lines 15-17). By iteratively splitting the domains, it is possible to compute tighter
lower bounds. We keep track of the global lower bound on the minimum by taking the minimum
over the lower bounds of all sub-domains (line 19). When the global upper bound and the global
lower bound differ by less than a small scalar ǫ (line 5), we consider that we have converged.
Algorithm 1 shows how to optimise and obtain the global minimum. If all that we are interested in
is the satisﬁability problem, the procedure can be simpliﬁed by initialising the global upper bound
with 0 (in line 2). Any subdomain with a lower bound greater than 0 (and therefore not eligible to

3

contain a counterexample) will be pruned out (by line 15). The computation of the lower bound
can therefore be replaced by the feasibility problem (or its relaxation) imposing the constraint that
the output is below zero without changing the algorithm. If it is feasible, there might still be a
counterexample and further branching is necessary. If it is infeasible, the subdomain can be pruned
out. In addition, if any upper bound improving on 0 is found on a subdomain (line 11), it is possible
to stop the algorithm as this already indicates the presence of a counterexample.

global_ub ← inf
global_lb ← − inf
doms ← [(global_lb, domain)]
while global_ub − global_lb > ǫ do

Algorithm 1 Branch and Bound
1: function BAB(net, domain, ǫ)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end function

end while
return global_ub

(_ , dom) ← pick_out(doms)
[subdom_1, . . . , subdom_s] ← split(dom)
for i = 1 . . . s do

dom_ub ← compute_UB(net, subdom_i)
dom_lb ← compute_LB(net, subdom_i)
if dom_ub < global_ub then
global_ub ← dom_ub
prune_domains(doms, global_ub)

end if
if dom_lb < global_ub then

domains.append((dom_lb, subdom_i))

end if
end for
global_lb ← min{lb | (lb, dom) ∈ doms}

The description of the veriﬁcation problem
as optimization and the pseudo-code of Algo-
rithm 1 are generic and would apply to veriﬁ-
cation problems beyond the speciﬁc case of PL-
NN. To obtain a practical algorithm, it is neces-
sary to specify several elements.
A search strategy, deﬁned by the pick_out
function, which chooses the next domain to
branch on. Several heuristics are possible, for
example those based on the results of previous
bound computations. For satisﬁable problems
or optimization problems, this allows to dis-
cover good upper bounds, enabling early prun-
ing.
A branching rule, deﬁned by the split
function, which
takes
dom
subdomain
partition
return
and
a
such
that
that
Si subdom_i = dom and
(subdom_i ∩ subdom_j) = ∅, ∀i 6= j.
This
will deﬁne the “shape” of the domains, which
impacts the hardness of computing bounds.
In addition, choosing the right partition can

domain

in

a

greatly impact the quality of the resulting bounds.
Bounding methods, deﬁned by the compute_{UB, LB} functions. These procedures estimate re-
spectively upper bounds and lower bounds over the minimum output that the network net can reach
over a given input domain. We want the lower bound to be as high as possible, so that this whole
domain can be pruned easily. This is usually done by introducing convex relaxations of the problem
and minimising them. On the other hand, the computed upper bound should be as small as possi-
ble, so as to allow pruning out other regions of the space or discovering counterexamples. As any
feasible point corresponds to an upper bound on the minimum, heuristic methods are sufﬁcient.
We now demonstrate how some published work in the literature can be understood as special case
of the branch-and-bound framework for veriﬁcation.
4.1 Reluplex
Katz et al. [11] present a procedure named Reluplex to verify properties of Neural Network contain-
ing linear functions and ReLU activation unit, functioning as an SMT solver using the splitting-on-
demand framework [1]. The principle of Reluplex is to always maintain an assignment to all of the
variables, even if some of the constraints are violated.
Starting from an initial assignment, it attempts to ﬁx some violated constraints at each step.
It
prioritises ﬁxing linear constraints ((10a), (10b), (10h) and some relaxation of (2d)) using a simplex
algorithm, even if it leads to violated ReLU constraints.
If no solution to this relaxed problem
containing only linear constraints exists, the counterexample search is unsatisﬁable. Otherwise,
either all ReLU are respected, which generates a counterexample, or Reluplex attempts to ﬁx one
of the violated ReLU; potentially leading to newly violated linear constraints. This process is not
guaranteed to converge, so to make progress, non-linearities that get ﬁxed too often are split into two
cases. Two new problems are generated, each corresponding to one of the phases of the ReLU. In
the worst setting, the problem will be split completely over all possible combinations of activation
patterns, at which point the sub-problems will all be simple LPs.
This algorithm can be mapped to the special case of branch-and-bound for satisﬁability. The search
strategy is handled by the SMT core and to the best of our knowledge does not prioritise any domain.
The branching rule is implemented by the ReLU-splitting procedure: when neither the upper bound
search, nor the detection of infeasibility are successful, one non-linear constraint over the j-th neu-

4

(cid:1)

ˆxi[j], 0
(cid:0)

is split out into two subdomains: {xi[j] = 0, ˆxi[j] ≤ 0}
ron of the i-th layer xi[j] = max
and {xi[j] = ˆxi[j], ˆxi[j] ≥ 0}. This deﬁnes the type of subdomains produced. The prioritisation of
ReLUs that have been frequently ﬁxed is a heuristic to decide between possible partitions.
As Reluplex only deal with satisﬁability, the analogue of the lower bound computation is an over-
approximation of the satisﬁability problem. The bounding method used is a convex relaxation,
obtained by dropping some of the constraints. The following relaxation is applied to ReLU units for
which the sign of the input is unknown (li[j] ≤ 0 and ui[j] ≥ 0).

xi = max (ˆxi, 0) ⇒ xi ≥ ˆxi

(4a)

xi ≥ 0

(4b)

xi ≤ ui.

(4c)

If this relaxation is unsatisﬁable, this indicates that the subdomain cannot contain any counterex-
ample and can be pruned out. The search for an assignment satisfying all the ReLU constraints by
iteratively attempting to correct the violated ReLUs is a heuristic that is equivalent to the search for
an upper bound lower than 0: success implies the end of the procedure but no guarantees can be
given.

4.2 Planet
Ehlers [6] also proposed an approach based on SMT. Unlike Reluplex, the proposed tool, named
Planet, operates by explicitly attempting to ﬁnd an assignment to the phase of the non-linearities.
Reusing the notation of Section 3.2, it assigns a value of 0 or 1 to each δi[j] variable, verifying at
each step the feasibility of the partial assignment so as to prune infeasible partial assignment early.
As in Reluplex, the search strategy is not explicitly encoded and simply enumerates all the domains
that have not yet been pruned. The branching rule is the same as for Reluplex, as ﬁxing the decision
variable δi[j] = 0 is equivalent to choosing {xi[j] = 0, ˆxi[j] ≤ 0} and ﬁxing δi[j] = 1 is equivalent
to {xi[j] = ˆxi[j], ˆxi[j] ≥ 0} . Note however that Planet does not include any heuristic to prioritise
which decision variables should be split over.
Planet does not include a mechanism for early termination based on a heuristic search of a feasible
point. For satisﬁable problems, only when a full complete assignment is identiﬁed is a solution
returned. In order to detect incoherent assignments, Ehlers [6] introduces a global linear approxi-
mation to a neural network, which is used as a bounding method to over-approximate the set of
values that each hidden unit can take. In addition to the existing linear constraints ((10a), (10b)
and (10h)), the non-linear constraints are approximated by sets of linear constraints representing the
non-linearities’ convex hull. Speciﬁcally, ReLUs with input of unknown sign are replaced by the set
of equations:

ˆxi[j] − li[j]
ui[j] − li[j]

(5a)

(5b)

xi[j] ≤ ui[j]

xi = max (ˆxi, 0) ⇒ xi ≥ ˆxi

xi ≥ 0
(5c)
where xi[j] corresponds to the value of the j-th coordinate of xi. An illustration of the feasible
domain is provided in the supplementary material.
Compared with the relaxation of Reluplex (4), the Planet relaxation is tighter. Speciﬁcally, Eq. (4a)
and (4b) are identical to Eq. (5a) and (5b) but Eq. (5c) implies Eq. (4c). Indeed, given that ˆxi[j]
is smaller than ui[j], the fraction multiplying ui[j] is necessarily smaller than 1, implying that this
provides a tighter bounds on xi[j].
To use this approximation to compute better bounds than the ones given by simple interval arithmetic,
it is possible to leverage the feed-forward structure of the neural networks and obtain bounds one
layer at a time. Having included all the constraints up until the i-th layer, it is possible to optimize
over the resulting linear program and obtain bounds for all the units of the i-th layer, which in turn
will allow us to create the constraints (5) for the next layer.
In addition to the pruning obtained by the convex relaxation, both Planet and Reluplex make use of
conﬂict analysis [15] to discover combinations of splits that cannot lead to satisﬁable assignments,
allowing them to perform further pruning of the domains.

4.3 Potential improvements
As can be seen, previous approaches to neural network veriﬁcation have relied on methodologies
developed in three communities: optimization, for the creation of upper and lower bounds; veriﬁca-
tion, especially SMT; and machine learning, especially the feed-forward nature of neural networks
for the creation of relaxations. A natural question that arises is “Can other existing literature from
these domains be exploited to further improve neural network veriﬁcation?” Our uniﬁed branch-and-
bound formulation makes it easy to answer this question. To illustrate its power, we now provide a
non-exhaustive list of suggestions to speed-up veriﬁcation algorithms.

5

Better bounding —While the relaxation proposed by Ehlers [6] is tighter than the one used by
Reluplex, it can be improved further still. Speciﬁcally, after a splitting operation, on a smaller
domain, we can reﬁne all the li, ui bounds, to obtain a tither relaxation. We show the importance
of this in the experiments section with the BaB-relusplit method that performs splitting on the
activation like Planet but updates its approximation completely at each step.
One other possible area of improvement lies in the tightness of the bounds used. Equation (5) is
very closely related to the Mixed Integer Formulation of Equation (20). Indeed, it corresponds to
level 0 of the Sherali-Adams hierarchy of relaxations [18]. The proof for this statement can be
found in the supplementary material. Stronger relaxations could be obtained by exploring higher
levels of the hierarchy. This would jointly constrain groups of ReLUs, rather than linearising them
independently.

]

]

l0[i⋆

l0[i⋆

and

]+u0[i⋆

]+u0[i⋆

Better branchingThe decision to split on the activation of the ReLU non-linearities made by Planet
and Reluplex is intuitive as it provides a clear set of decision variables to ﬁx. However, it ignores
another natural branching strategy, namely, splitting the input domain. Indeed, it could be argued
that since the function encoded by the neural networks are piecewise linear in their input, this could
result in the computation of highly useful upper and lower bounds. To demonstrate this, we propose
the novel BaB-input algorithm: a branch-and-bound method that branches over the input features
of the network. Based on a domain with input constrained by Eq. (10a), the split function would
return two subdomains where bounds would be identical in all dimension except for the dimension
with the largest length, denoted i⋆. The bounds for each subdomain for dimension i⋆ are given by
l0[i⋆] ≤ x0[i⋆] ≤
≤ x0[i⋆] ≤ u0[i⋆]. Based on these tighter input bounds,
2
tighter bounds at all layers can be re-evaluated.
One of the main advantage of branching over the variables is that all subdomains generated by the
BaB algorithm when splitting over the input variables end up only having simple bound constraints
over the value that input variable can take. In order to exploit this property to the fullest, we use
the highly efﬁcient lower bound computation approach of Kolter & Wong [13]. This approach was
initially proposed in the context of robust optimization. However, our uniﬁed framework opens
the door for its use in veriﬁcation. Speciﬁcally, Kolter & Wong [13] identiﬁed an efﬁcient way of
computing bounds for the type of problems we encounter, by generating a feasible solution to the
dual of the LP generated by the Planet relaxation. While this bound is quite loose compared to
the one obtained through actual optimization, they are very fast to evaluate. We propose a smart
branching method BaBSB to replace the longest edge heuristic of BaB-input. For all possible splits,
we compute fast bounds for each of the resulting subdomain, and execute the split resulting in the
highest lower bound. The intuition is that despite their looseness, the fast bounds will still be useful
in identifying the promising splits.

2

5 Experimental setup
The problem of PL-NN veriﬁcation has been shown to be NP-complete [11]. Meaningful compari-
son between approaches therefore needs to be experimental.
5.1 Methods
The simplest baseline we refer to is BlackBox, a direct encoding of Eq. (2) into the Gurobi solver,
which will perform its own relaxation, without taking advantage of the problem’s structure.
For the SMT based methods, Reluplex and Planet, we use the publicly available versions [7, 12].
Both tools are implemented in C++ and relies on the GLPK library to solve their relaxation. We
wrote some software to convert in both directions between the input format of both solvers.
We also evaluate the potential of using MIP solvers, based on the formulation of Eq. (20). Due to the
lack of availability of open-sourced methods at the time of our experiments, we reimplemented the
approach in Python, using the Gurobi MIP solver. We report results for a variant called MIPplanet,
which uses bounds derived from Planet’s convex relaxation rather than simple interval arithmetic.
The MIP is not treated as a simple feasibility problem but is encoded to minimize the output ˆxn of
Equation (10h), with a callback interrupting the optimization as soon as a negative value is found.
Additional discussions on encodings of the MIP problem can be found in supplementary materials.
In our benchmark, we include the methods derived from our Branch and Bound analysis. Our
implementation follows faithfully Algorithm 1, is implemented in Python and uses Gurobi to solve
LPs. The pick_out strategy consists in prioritising the domain that currently has the smallest lower
bound. Upper bounds are generated by randomly sampling points on the considered domain, and
we use the convex approximation of Ehlers [6] to obtain lower bounds. As opposed to the approach

6

taken by Ehlers [6] of building a single approximation of the network, we rebuild the approximation
and recompute all bounds for each sub-domain. This is motivated by the observation shown in
Figure 1 which demonstrate the signiﬁcant improvements it brings, especially for deeper networks.
For split, BaB-input performs branching by splitting the input domain in half along its longest
edge and BaBSB does it by splitting the input domain along the dimension improving the global
lower bound the most according to the fast bounds of Kolter & Wong [13]. We also include results
for the BaB-relusplit variant, where the split method is based on the phase of the non-linearities,
similarly to Planet.

d
n
u
o
b
 
r
e
w
o
L

36.0

35.5

35.0

34.5

34.0

33.5

33.0

32.5

32.0

ReApproximating
FixedApproximation

103

102

101

d
n
u
o
b
 
r
e
w
o
L

100
0
−100

−101

−102

−103

−104

−105

ReApproximating
FixedApproximation

10−12

10−9

10−6

10−3

100

10−6 10−5 10−4 10−3 10−2 10−1 100

Relative area

Relative area

Figure 1: Quality of the linear approximation,
depending on the size of the input domain. We
plot the value of the lower bound as a function of
the area on which it is computed (higher is bet-
ter). The domains are centered around the global
minimum and repeatedly shrunk. Rebuilding com-
pletely the linear approximation at each step al-
lows to create tighter lower-bounds thanks to bet-
ter li and ui, as opposed to using the same con-
straints and only changing the bounds on input
variables. This effect is even more signiﬁcant on
deeper networks.

(b) Approximation on a
deep net from ACAS

(a) Approximation on a
CollisionDetection net
5.2 Evaluation Criteria
For each of the data sets, we compare the different methods using the same protocol. We attempt to
verify each property with a timeout of two hours, and a maximum allowed memory usage of 20GB,
on a single core of a machine with an i7-5930K CPU. We measure the time taken by the solvers
to either prove or disprove the property. If the property is false and the search problem is therefore
satisﬁable, we expect from the solver to exhibit a counterexample. If the returned input is not a valid
counterexample, we don’t count the property as successfully proven, even if the property is indeed
satisﬁable. All code and data necessary to replicate our analysis are released.

6 Analysis
We attempt to perform veriﬁcation on three data sets of properties and report the comparison results.
The dimensions of all the problems can be found in the supplementary material.
The CollisionDetection data set [6] attempts to predict whether two vehicles with parameterized
trajectories are going to collide. 500 properties are extracted from problems arising from a binary
search to identify the size of the region around training examples in which the prediction of the
network does not change. The network used is relatively shallow but due to the process used to
generate the properties, some lie extremely close between the decision boundary between SAT and
UNSAT. Results presented in Figure 10 therefore highlight the accuracy of methods.
The Airborne Collision Avoidance System (ACAS) data set, as released by Katz et al. [11] is a
neural network based advisory system recommending horizontal manoeuvres for an aircraft in order
to avoid collisions, based on sensor measurements. Each of the ﬁve possible manoeuvres is assigned
a score by the neural network and the action with the minimum score is chosen. The 188 properties
to verify are based on some speciﬁcation describing various scenarios. Due to the deeper network
involved, this data set is useful in highlighting the scalability of the various algorithms.
PCAMNIST is a novel data set that we introduce to get a better understanding of what factors
inﬂuence the performance of various methods. It is generated in a way to give control over different
architecture parameters. Details of the dataset construction are given in supplementary materials. We
present plots in Figure 4 showing the evolution of runtimes depending on each of the architectural
parameters of the networks.
Comparative evaluation of veriﬁcation approaches — In Figure 10, on the shallow networks of
CollisionDetection, most solvers succeed against all properties in about 10s. In particular, the SMT
inspired solvers Planet, Reluplex and the MIP solver are extremely fast. The BlackBox solver
doesn’t reach 100% accuracy due to producing incorrect counterexamples. Additional analysis can
be found about the cause of those errors in the supplementary materials.
On the deeper networks of ACAS, in Figure 10b, no errors are observed but most methods timeout
on the most challenging testcases. The best baseline is Reluplex, who reaches 79.26% success rate
at the two hour timeout, while our best method, BaBSB, already achieves 98.40% with a budget of
one hour. To reach Reluplex’s success rate, the required runtime is two orders of magnitude smaller.

7

0

10−1

100
Computation time (in s)

102

101

103

0

0

2000
Computation time (in s)

4000

6000

CollisionDetection

(b) ACAS Dataset

(a)
Dataset

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

Method

BaBSB
BaB
reluBaB

reluplex

MIPplanet

planet

Average
time
per Node

1.81s
2.11s
1.69s

0.30s

0.017s

1.5e-3s

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

Table 1: Average time to
explore a node for each
method.

0

100

101

102

103
Number of Nodes visited

104

105

106

(a) Properties solved for a given
number of nodes
to explore
(log scale).

Figure 2: Proportion of properties veri-
ﬁable for varying time budgets depending
on the methods employed. A higher curve
means that for the same time budget, more
properties will be solvable. All methods
solve CollisionDetection quite quickly ex-
cept reluBaB, which is much slower and
BlackBox who produces several incorrect
counterexamples.

Figure 3: The trade-off taken by the various
methods are different. Figure 3a shows how
many subdomains needs to be explored be-
fore verifying properties while Table 1 shows
the average time cost of exploring each sub-
domain. Our methods have a higher cost
per node but they require signiﬁcantly less
branching, thanks to better bounding. Note
also that between BaBSB and BaB, the
smart branching reduces by an order of mag-
nitude the number of nodes to visit.

Impact of each improvement —To identify which changes allow our method to have good perfor-
mance, we perform an ablation study and study the impact of removing some components of our
methods. The only difference between BaBSB and BaB is the smart branching, which represents a
signiﬁcant part of the performance gap.
Branching over the inputs rather than over the activations does not contribute much, as shown by
the small difference between BaB and reluBaB. Note however that we are able to use the fast
methods of Kolter & Wong [13] for the smart branching because branching over the inputs makes
the bounding problem similar to the one solved in robust optimization. Even if it doesn’t improve
performance by itself, the new type of split enables the use of smart branching.
The rest of the performance gap can be attributed to using a better bounds: reluBaB signiﬁcantly
outperforms planet while using the same branching strategy and the same convex relaxations. The
improvement comes from the beneﬁts of rebuilding the approximation at each step shown in Fig-
ure 1.
Figure 3 presents some additional analysis on a 20-property subset of the ACAS dataset, showing
how the methods used impact the need for branching. Smart branching and the use of better lower
bounds reduce heavily the number of subdomains to explore.

Timeout

Timeout

Timeout

Timeout

)
.
s
 
n
i
(
 
g
n
m
T

i

i

104

103

102

101

100

10−1

10−2

104

103

102

101

100

)
.
s
 
n
i
(
 
g
n
m
T

i

i

10−1

10−2

101

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

104

103

102

101

100

10−1

)
.
s
 
n
i
(
 
g
n
m
T

i

i

SAT / False

UNSAT / True

101

102

103

Number of Inputs

Layer Width

102

−104−103−102−101−1000100 101 102 103 104
Satisfiability margin

(a) Number of inputs

(b) Layer width

(c) Margin

10−2

2 × 100

3 × 100 4 × 100
Layer Depth

6 × 100

(d) Network depth

Figure 4:
Impact of the various parameters over the runtimes of the different solvers. The base network has
10 inputs and 4 layers of 25 hidden units, and the property to prove is True with a margin of 1000. Each of the
plot correspond to a variation of one of this parameters.

In the graphs of Figure 4, the trend for all the methods are similar, which seems to indicate that
hard properties are intrinsically hard and not just hard for a speciﬁc solver. Figure 4a shows an
expected trend: the largest the number of inputs, the harder the problem is. Similarly, Figure 4b
shows unsurprisingly that wider networks require more time to solve, which can be explained by
the fact that they have more non-linearities. The impact of the margin, as shown in Figure 4c is

)
.
s
 
n
i
(
 
g
n
m
T

i

i

104

103

102

101

100

10−1

10−2

8

also clear. Properties that are True or False with large satisﬁability margin are easy to prove, while
properties that have small satisﬁability margins are signiﬁcantly harder.

7 Conclusion
The improvement of formal veriﬁcation of Neural Networks represents an important challenge to be
tackled before learned models can be used in safety critical applications. By providing both a uniﬁed
framework to reason about methods and a set of empirical benchmarks to measure performance with,
we hope to contribute to progress in this direction. Our analysis of published algorithms through the
lens of Branch and Bound optimization has already resulted in signiﬁcant improvements in runtime
on our benchmarks. Its continued analysis should reveal even more efﬁcient algorithms in the future.

9

References
[1] Barrett, Clark, Nieuwenhuis, Robert, Oliveras, Albert, and Tinelli, Cesare. Splitting on de-
mand in sat modulo theories. International Conference on Logic for Programming Artiﬁcial
Intelligence and Reasoning, 2006.

[2] Bastani, Osbert, Ioannou, Yani, Lampropoulos, Leonidas, Vytiniotis, Dimitrios, Nori, Aditya,

and Criminisi, Antonio. Measuring neural net robustness with constraints. NIPS, 2016.

[3] Buxton, John N and Randell, Brian. Software Engineering Techniques: Report on a Conference

Sponsored by the NATO Science Committee. NATO Science Committee, 1970.

[4] Cheng, Chih-Hong, Nührenberg, Georg, and Ruess, Harald. Maximum resilience of artiﬁcial

neural networks. Automated Technology for Veriﬁcation and Analysis, 2017.

[5] Cheng, Chih-Hong, Nührenberg, Georg, and Ruess, Harald. Veriﬁcation of binarized neural

networks. arXiv:1710.03107, 2017.

[6] Ehlers, Ruediger. Formal veriﬁcation of piece-wise linear feed-forward neural networks. Au-

tomated Technology for Veriﬁcation and Analysis, 2017.

[7] Ehlers, Ruediger. Planet. https://github.com/progirep/planet, 2017.

[8] Hein, Matthias and Andriushchenko, Maksym. Formal guarantees on the robustness of a clas-

siﬁer against adversarial manipulation. NIPS, 2017.

[9] Hickey, Timothy, Ju, Qun, and Van Emden, Maarten H. Interval arithmetic: From principles

to implementation. Journal of the ACM (JACM), 2001.

[10] Huang, Xiaowei, Kwiatkowska, Marta, Wang, Sen, and Wu, Min. Safety veriﬁcation of deep

neural networks. International Conference on Computer Aided Veriﬁcation, 2017.

[11] Katz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and Kochenderfer, Mykel. Reluplex: An

efﬁcient smt solver for verifying deep neural networks. CAV, 2017.

[12] Katz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and Kochenderfer, Mykel. Reluplex.

https://github.com/guykatzz/ReluplexCav2017, 2017.

[13] Kolter, Zico and Wong, Eric. Provable defenses against adversarial examples via the convex

outer adversarial polytope. arXiv:1711.00851, 2017.

[14] Lomuscio, Alessio and Maganti, Lalit. An approach to reachability analysis for feed-forward

relu neural networks. arXiv:1706.07351, 2017.

[15] Marques-Silva, João P and Sakallah, Karem A. Grasp: A search algorithm for propositional

satisﬁability. IEEE Transactions on Computers, 1999.

[16] Narodytska, Nina, Kasiviswanathan, Shiva Prasad, Ryzhyk, Leonid, Sagiv, Mooly, and Walsh,
Toby. Verifying properties of binarized deep neural networks. arXiv:1709.06662, 2017.

[17] Pulina, Luca and Tacchella, Armando. An abstraction-reﬁnement approach to veriﬁcation of

artiﬁcial neural networks. CAV, 2010.

[18] Sherali, Hanif D and Adams, Warren P. A hierarchy of relaxations and convex hull character-
izations for mixed-integer zero—one programming problems. Discrete Applied Mathematics,
1994.

[19] Tjeng, Vincent and Tedrake, Russ. Verifying neural networks with mixed integer programming.

arXiv:1711.07356, 2017.

[20] Xiang, Weiming, Tran, Hoang-Dung, and Johnson, Taylor T. Output reachable set estimation

and veriﬁcation for multi-layer neural networks. arXiv:1708.03322, 2017.

[21] Zakrzewski, Radosiaw R. Veriﬁcation of a trained neural network accuracy. IJCNN, 2001.

10

A Canonical Form of the veriﬁcation problem
If the property is a simple inequality P (ˆxn) , cT ˆxn ≥ b, it is sufﬁcient to add to the network a ﬁnal
fully connected layer with one output, with weight of c and a bias of −b. If the global minimum
of this network is positive, it indicates that for all ˆxn the original network can output, we have
cT ˆxn − b ≥ 0 =⇒ cT ˆxn ≥ b, and as a consequence the property is True. On the other hand, if the
global minimum is negative, then the minimizer provides a counter-example.
Clauses speciﬁed using OR (denoted by
property is P (ˆxn) ,
Clauses speciﬁed using AND (denoted by
is equivalent to mini (cid:0)cT

) can be encoded by using a MaxPooling unit. If the
ˆxn − bi

) can be encoded similarly:
i ˆxn − bi(cid:1) ≥ 0 ⇐⇒ − (cid:0)maxi (cid:0)−cT

Vi (cid:2)
(cid:3)
B Toy Problem example
We have speciﬁed the problem of formal veriﬁcation of neural networks as follows: given a network
that implements a function ˆxn = f (x0), a bounded input domain C and a property P , we want to
prove that

, this is equivalent to maxi
(cid:3)

(cid:1)
the property P (ˆxn) =

i ˆxn + bi(cid:1)(cid:1) ≥ 0

ˆxn ≥ bi

ˆxn ≥ bi

cT
i
(cid:0)

Wi (cid:2)

≥ 0.

cT
i

cT
i

W

V

x0 ∈ C,

ˆxn = f (x0) =⇒ P (ˆxn).

(6)

A toy-example of the Neural Network veriﬁcation problem is given in Figure 5. On the domain
C = [−2; 2] × [−2; 2], we want to prove that the output y of the one hidden-layer network always
satisfy the property P (y) , [y > −5]. We will use this as a running example to explain the methods
used for comparison in our experiments.

[-2, 2]

x1

[-2, 2]

x2

a

1

1

-1

-1

b

y

-1

-1

Prove that y > −5

Figure 5: Example Neural Network. We attempt to prove the property that the network output is
always greater than -5

B.1 Problem formulation
For the network of Figure 5, the variables would be {x1, x2, ain, aout, bin, bout, y} and the set of
constraints would be:

− 2 ≤ x1 ≤ 2

ˆa = x1 + x2
y = −a − b

a = max(ˆa, 0)
y ≤ −5

−2 ≤ x2 ≤ 2
ˆb = −x1 − x2

b = max(ˆb, 0)

Here, ˆa is the input value to hidden unit, while a is the value after the ReLU. Any point satisfying all
the above constraints would be a counterexample to the property, as it would imply that it is possible
to drive the output to -5 or less.
B.2 MIP formulation
In our example, the non-linearities of equation (7c) would be replaced by

(7a)

(7b)

(7c)
(7d)

(8)

a ≥ 0
a ≤ ˆa − la(1 − δa)

a ≥ ˆa
a ≤ uaδa

δa ∈ {0, 1}

11

Step

x1

x2

ˆa

a

0

0
0

0
0

-2

ˆb
0

b

0

4
4

4
4

4

0

0
0

0
0
Fix linear constraints
0
1
0
0
1
0
Fix a ReLU
4
1
0
0
4
1
Fix linear constraints
1
-2
. . .

0
0

-4

4

y

0

-5
-5

-5
-5

-5

1

2

3

Figure 6: Evolution of the Reluplex algorithm. Red cells corresponds to value violating Linear con-
straints, and orange cells corresponds to value violating ReLU constraints. Resolution of violation
of linear constraints are prioritised.

where la is a lower bound of the value that ˆa can take (such as -4) and ua is an upper bound (such
as 4). The binary variable δa indicates which phase the ReLU is in: if δa = 0, the ReLU is blocked
and aout = 0, else the ReLU is passing and aout = ain. The problem remains difﬁcult due to the
integrality constraint on δa.
B.3 Running Reluplex
Table 6 shows the initial steps of a run of the Reluplex algorithm on the example of Figure 5. Starting
from an initial assignment, it attempts to ﬁx some violated constraints at each step. It prioritises
ﬁxing linear constraints ((7a), (7b) and (7d) in our illustrative example) using a simplex algorithm,
even if it leads to violated ReLU constraints (7c). This can be seen in step 1 and 3 of the process.
If no solution to the problem containing only linear constraints exists, this shows that the counterex-
ample search is unsatisﬁable. Otherwise, all linear constraints are ﬁxed and Reluplex attempts to ﬁx
one violated ReLU at a time, such as in step 2 of Table 6 (ﬁxing the ReLU b), potentially leading
to newly violated linear constraints. In the case where no violated ReLU exists, this means that a
satisﬁable assignment has been found and that the search can be interrupted.
This process is not guaranteed to converge, so to guarantee progress, non-linearities getting ﬁxed too
often are split into two cases. This generates two new sub-problems, each involving an additional
linear constraint instead of the linear one. The ﬁrst one solves the problem where ˆa ≤ 0 and a = 0,
the second one where ˆa ≥ 0 and a = ˆa. In the worst setting, the problem will be split completely
over all possible combinations of activation patterns, at which point the sub-problems are simple
LPs.

C Planet approximation
The feasible set of the Mixed Integer Programming formulation is given by the following set of
equations. We assume that all li are negative and ui are positive. In case this isn’t true, it is possible
to just update the bounds such that they are.

l0 ≤ x0 ≤ u0
ˆxi+1 = Wi+ixi + bi+i
xi ≥ 0
xi ≥ ˆxi
xi ≤ ˆxi − li · (1 − δi)
xi ≤ ui · δi
δi ∈ {0, 1}hi
ˆxn ≤ 0

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}

∀i ∈ {1, n − 1}

(9a)
(9b)
(9c)
(9d)
(9e)
(9f)

(9g)
(9h)

The level 0 of the Sherali-Adams hierarchy of relaxation Sherali & Adams [18] doesn’t include any
additional constraints. Indeed, polynomials of degree 0 are simply constants and their multiplication
with existing constraints followed by linearization therefore doesn’t add any new constraints. As a

12

xi[j]

li[j]

ˆxi[j]

ui[j]

Figure 7: Feasible domain corresponding to the Planet relaxation for a single ReLU.

result, the feasible domain given by the level 0 of the relaxation corresponds simply to the removal
of the integrality constraints:

l0 ≤ x0 ≤ u0
ˆxi+1 = Wi+ixi + bi+i
xi ≥ 0
xi ≥ ˆxi
xi ≤ ˆxi − li · (1 − di)
xi ≤ ui · di
0 ≤ di ≤ 1
ˆxn ≤ 0

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}

Combining the equations (10e) and (10f), looking at a single unit j in layer i, we obtain:

(11)
The function mapping di[j] to an upperbound of xi[j] is a minimum of linear functions, which means
that it is a concave function. As one of them is increasing and the other is decreasing, the maximum
will be reached when they are both equals.

ˆxi[j] − li(1 − di[j]), ui[j]di[j](cid:1)
(cid:0)

xi[j] ≤ min

Plugging this equation for d⋆ into Equation(11) gives that:

ˆxi[j] − li[j](1 − d⋆

i[j]) = ui[j]d⋆
i[j]
ˆxi[j] − li[j]
ui[j] − li[j]

d⋆
i[j] =

⇔

xi[j] ≤ ui[j]

ˆxi[j] − li[j]
ui[j] − li[j]

which corresponds to the upper bound of xi[j] introduced for Planet [6].
D MaxPooling
For space reason, we only described the case of ReLU activation function in the main paper. We
now present how to handle MaxPooling activation, either by converting them to the already handled
case of ReLU activations or by introducing an explicit encoding of them when appropriate.
D.1 Mixed Integer Programming
Similarly to the encoding of ReLU constraints using binary variables and bounds on the inputs, it is
possible to similarly encode MaxPooling constraints. The constraint

can be replaced by

y = max (x1, . . . , xk)

y ≥ xi
y ≤ xi + (ux1:k − lxi)(1 − δi)

δi = 1

X
i∈{1...k}
δi ∈ {0, 1}

∀i ∈ {1 . . . k}
∀i ∈ {1 . . . k}

∀i ∈ {1 . . . k}

where ux1:k is an upper-bound on all xi for i ∈ {1 . . . k} and lxi is a lower bound on xi.

13

(10a)
(10b)
(10c)
(10d)
(10e)
(10f)
(10g)

(10h)

(12)

(13)

(14)

(15a)
(15b)

(15c)

(15d)

(16)

(17)

(18)

(19a)

(19b)

D.2 Reluplex
In the version introduced by [11], there is no support for MaxPooling units. As the canonical repre-
sentation we evaluate needs them, we provide a way of encoding a MaxPooling unit as a combination
of Linear function and ReLUs.
To do so, we decompose the element-wise maximum into a series of pairwise maximum

max (xj , x2, x3, x4) = max( max (x1, x2) ,
max (x3, x4))

and decompose the pairwise maximums as sum of ReLUs:

max (x1, x2) = max (x1 − x2, 0) + max (x2 − lx2, 0) + lx2,

where lx2 is a pre-computed lower bound of the value that x2 can take.
As a result, we have seen that an elementwise maximum such as a MaxPooling unit can be decom-
posed as a series of pairwise maximum, which can themselves be decomposed into a sum of ReLUs
units. The only requirement is to be able to compute a lower bound on the input to the ReLU, for
which the methods discussed in the paper can help.
D.3 Planet
As opposed to Reluplex, Planet Ehlers [6] directly supports MaxPooling units. The SMT solver
driving the search can split either on ReLUs, by considering separately the case of the ReLU be-
ing passing or blocking. It also has the possibility on splitting on MaxPooling units, by treating
separately each possible choice of units being the largest one.
For the computation of lower bounds, the constraint

is replaced by the set of constraints:

y = max (x1, x2, x3, x4)

y ≥ xi

∀i ∈ {1 . . . 4}

y ≤

(xi − lxi) + max

lxi,

i

X
i

where xi are the inputs to the MaxPooling unit and lxi their lower bounds.
E Mixed Integers Variants
E.1 Encoding
Several variants of encoding are available to use Mixed Integer Programming as a solver for Neural
Network Veriﬁcation. As a reminder, in the main paper we used the formulation of Tjeng & Tedrake
[19]:

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

xi ≤ ui · δi
xi ≤ ˆxi − li · (1 − δi)

(20a)
(20b)

An alternative formulation is the one of Lomuscio & Maganti [14] and Cheng et al. [4]:

xi ≤ Mi · δi
xi ≤ ˆxi − Mi · (1 − δi)

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

(21a)
(21b)
where Mi = max (−li, ui). This is fundamentally the same encoding but with a sligthly worse
bounds that is used, as one of the side of the bounds isn’t as tight as it could be.
E.2 Obtaining bounds
The formulation described in Equations (20) and (21) are dependant on obtaining lower and upper
bounds for the value of the activation of the network.
Interval AnalysisOne way to obtain them, mentionned in the paper, is the use of interval arith-
metic [9]. If we have bounds li, ui for a vector xi, we can derive the bounds ˆli+1, ˆui+1 for a vector
ˆxi+1 = Wi+1xi + bi+1

ˆli+1[j] =

(cid:16)W +

i+1[j,k]l+

i[k] + W −

i+1[j,k]u+

i[k](cid:17) + bi+1[j]

ˆui+1[j] =

(cid:16)W +

i+1[j,k]u+

i[k] + W −

i+1[j,k]l+

i[k](cid:17) + bi+1[j]

X
k

X
k

(22a)

(22b)

with the notation a+ = max(a, 0) and a− = min(a, 0). Propagating the bounds through a ReLU
activation is simply equivalent to applying the ReLU to the bounds.

14

Planet Linear approximationAn alternative way to obtain bounds is to use the relaxation of Planet.
This is the methods that was employed in the paper: we build incrementally the network approxi-
mation, layer by layer. To obtain the bounds over an activation, we optimize its value subject to the
constraints of the relaxation.
Given that this is a convex problem, we will achieve the optimum. Given that it is a relaxation, the
optimum will be a valid bound for the activation (given that the feasible domain of the relaxation
includes the feasible domains subject to the original constraints).
Once this value is obtained, we can use it to build the relaxation for the following layers. We can
build the linear approximation for the whole network and extract the bounds for each activation to
use in the encoding of the MIP. While obtaining the bounds in this manner is more expensive than
simply doing interval analysis, the obtained bounds are better.
E.3 Objective function
In the paper, we have formalised the veriﬁcation problem as a satisﬁability problem, equating the
existence of a counterexample with the feasibility of the output of a (potentially modiﬁed) network
being negative.
In practice, it is beneﬁcial to not simply formulate it as a feasibility problem but as an optimization
problem where the output of the network is explicitly minimized.
E.4 Comparison
We present here a comparison on CollisionDetection and ACAS of the different variants.

1. Planet-feasible uses the encoding of Equation (20), with bounds obtained based on the

planet relaxation, and solve the problem simply as a satisﬁability problem.

2. Interval is the same as Planet-feasible, except that the bounds used are obtained by interval

analysis rather than with the Planet relaxation.

3. Planet-symfeasible is the same as Planet-feasible, except that the encoding is the one of

Equation (21).

4. Planet-opt is the same as Planet-feasible, except that the problem is solved as an opti-
mization problem. The MIP solver attempt to ﬁnd the global minimum of the output of
the network. Using Gurobi’s callback, if a feasible solution is found with a negative value,
the optimization is interrupted and the current solution is returned. This corresponds to the
version that is reported in the main paper.

interval
planetsym-feasible
planet-feasible
planet-opt

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

interval
planetsym-feasible
planet-feasible
planet-opt

100

101

102

103

Computation time (in s)

(a) CollisionDetection Dataset

0
100

101
102
Computation time (in s)

103

(b) ACAS Dataset

Figure 8: Comparison between the different variants of MIP formulation for Neural Network veriﬁcation.

The ﬁrst observation that can be made is that when we look at the CollisionDetection dataset in
Figure 8a, only Planet-opt solves the dataset to 100% accuracy. The reason why the other methods
don’t reach it is not because of timeout but because they return spurious counterexamples. As
they encode only satisﬁability problem, they terminate as soon as they identify a solution with a
value of zero. Due to the large constants involved in the big-M, those solutions are sometimes not
actually valid counterexamples. This is the same reason why the BlackBox solver doesn’t reach
100% accuracy in the experiments reported in the main paper.

15

The other results that we can observe is the impact of the quality of the bounds when the networks get
deeper, and the problem becomes therefore more complex, such as in the ACAS dataset. Interval
has the worst bounds and is much slower than the other methods. Planetsym-feasible, with its
slightly worse bounds, performs worse than Planet-feasible and Planet-opt.

F Experimental setup details
We provide in Table 2 the characteristics of all of the datasets used for the experimental comparison
in the main paper.

Data set

Count

Collision
Detection

500

ACAS

188

PCAMNIST

27

Model Architecture
6 inputs
40 hidden unit layer, MaxPool
19 hidden unit layer
2 outputs
5 inputs
6 layers of 50 hidden units
5 outputs
10 or {5, 10, 25, 100, 500, 784} inputs
4 or {2, 3, 4, 5, 6, 7} layers
of 25 or {10, 15, 25, 50, 100} hidden units,
1 output, with a margin of +1000 or
{-1e4, -1000, -100, -50, -10, -1 ,1, 10, 50, 100, 1000, 1e4}

Table 2: Dimensions of all the data sets. For PCAMNIST, we use a base network with 10 inputs,
4 layers of 25 hidden units and a margin of 1000. We generate new problems by changing one
parameter at a time, using the values inside the brackets.

G Additional performance details
Given that there is a signiﬁcant difference in the way veriﬁcation works for SAT problems vs. UN-
SAT problems, we report also comparison results on the subset of data sorted by decision type.

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

0

10−1

101

102

100
Computation time (in s)
(a) On SAT properties

103

0

10−1

101

100
Computation time (in s)
(b) On UNSAT properties

102

103

Figure 9: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed on the CollisionDetection dataset. We can identify that all the errors that BlackBox makes
are on SAT properties, as it returns incorrect counterexamples.

H PCAMNIST details
PCAMNIST is a novel data set that we introduce to get a better understanding of what factors in-
ﬂuence the performance of various methods. It is generated in a way to give control over different

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

16

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

0

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

0

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

4000

2000
Computation time (in s)
(a) On SAT properties

6000

4000

2000
Computation time (in s)
(b) On UNSAT properties

6000

Figure 10: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed on the ACAS dataset. We observe that planet doesn’t succeed in solving any of the SAT
properties, while our proposed methods are extremely efﬁcient at it, even if there remains some
properties that they can’t solve.

architecture parameters. The networks takes k features as input, corresponding to the ﬁrst k eigen-
vectors of a Principal Component Analysis decomposition of the digits from the MNIST data set.
We also vary the depth (number of layers), width (number of hidden unit in each layer) of the net-
works. We train a different network for each combination of parameters on the task of predicting the
parity of the presented digit. This results in the accuracies reported in Table 3.
The properties that we attempt to verify are whether there exists an input for which the score assigned
to the odd class is greater than the score of the even class plus a large conﬁdence. By tweaking the
value of the conﬁdence in the properties, we can make the property either True or False, and we
can choose by how much is it true. This gives us the possibility of tweaking the “margin”, which
represent a good measure of difﬁculty of a network.
In addition to the impact of each factors separately as was shown in the main paper, we can also
look at it as a generic dataset and plot the cactus plots like for the other datasets. This can be found
in Figure 11

Network Parameter

Accuracy

Nb inputs Width Depth

Train

Test

5
10
25
100
500
784

10
10
10
10
10

25
25
25
25
25
25

10
15
25
50
100

88.18% 87.3%
97.42% 96.09%
99.87% 98.69%
100% 98.77%
100% 98.84%
100% 98.64%

96.34% 95.75%
96.31% 95.81%
97.42% 96.09%
97.35% 96.0%
97.72% 95.75%

2
3
4
5
6
7
Table 3: Accuracies of the network trained for the PCAMNIST dataset.

96.45% 95.71%
96.98% 96.05%
97.42% 96.09%
96.78% 95.9%
95.48% 95.2%
96.81% 96.07%

10
10
10
10
10
10

25
25
25
25
25
25

4
4
4
4
4
4

4
4
4
4
4

17

0

10−1

100
Computation time (in s)

102

101

103

Figure 11: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed. The PCAMNIST dataset is challenging as None of the methods reaches more than 50%
success rate.

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

18

8
1
0
2
 
y
a
M
 
2
2
 
 
]
I

A
.
s
c
[
 
 
3
v
5
5
4
0
0
.
1
1
7
1
:
v
i
X
r
a

A Uniﬁed View of Piecewise Linear Neural Network Veriﬁcation

Rudy Bunel
University of Oxford
rudy@robots.ox.ac.uk

Ilker Turkaslan
University of Oxford
ilker.turkaslan@lmh.ox.ac.uk

Philip H.S. Torr
University of Oxford
philip.torr@eng.ox.ac.uk

Pushmeet Kohli
Deepmind
pushmeet@google.com

M. Pawan Kumar
University of Oxford
Alan Turing Institute
pawan@robots.ox.ac.uk

Abstract

The success of Deep Learning and its potential use in many safety-critical applica-
tions has motivated research on formal veriﬁcation of Neural Network (NN) mod-
els. Despite the reputation of learned NN models to behave as black boxes and the
theoretical hardness of proving their properties, researchers have been successful
in verifying some classes of models by exploiting their piecewise linear structure
and taking insights from formal methods such as Satisiﬁability Modulo Theory.
These methods are however still far from scaling to realistic neural networks. To
facilitate progress on this crucial area, we make two key contributions. First, we
present a uniﬁed framework that encompasses previous methods. This analysis
results in the identiﬁcation of new methods that combine the strengths of mul-
tiple existing approaches, accomplishing a speedup of two orders of magnitude
compared to the previous state of the art. Second, we propose a new data set of
benchmarks which includes a collection of previously released testcases. We use
the benchmark to provide the ﬁrst experimental comparison of existing algorithms
and identify the factors impacting the hardness of veriﬁcation problems.

1 Introduction
Despite their success in a wide variety of applications, Deep Neural Networks have seen limited
adoption in safety-critical settings. The main explanation for this lies in their reputation for being
black-boxes whose behaviour can not be predicted. Current approaches to evaluate trained models
mostly rely on testing using held-out data sets. However, as Edsger W. Dijkstra said [3], “testing
shows the presence, not the absence of bugs”. If deep learning models are to be deployed to applica-
tions such as autonomous driving cars, we need to be able to verify safety-critical behaviours.
To this end, some researchers have tried to use formal methods. To the best of our knowledge,
Zakrzewski [21] was the ﬁrst to propose a method to verify simple, one hidden layer neural networks.
However, only recently were researchers able to work with non-trivial models by taking advantage
of the structure of ReLU-based networks [5, 11]. Even then, these works are not scalable to the large
networks encountered in most real world problems.
This paper advances the ﬁeld of NN veriﬁcation by making the following key contributions:

1. We reframe state of the art veriﬁcation methods as special cases of Branch-and-Bound

optimization, which provides us with a uniﬁed framework to compare them.

2. We gather a data set of test cases based on the existing literature and extend it with new
benchmarks. We provide the ﬁrst experimental comparison of veriﬁcation methods.

3. Based on this framework, we identify algorithmic improvements in the veriﬁcation process,
speciﬁcally in the way bounds are computed, the type of branching that are considered, as

Preprint. Work in progress.

well as the strategies guiding the branching. Compared to the previous state of the art, these
improvements lead to speed-up of almost two orders of magnitudes.

Section 2 and 3 give the speciﬁcation of the problem and formalise the veriﬁcation process. Section
4 presents our uniﬁed framework, showing that previous methods are special cases and highlight-
ing potential improvements. Section 5 presents our experimental setup and Section 6 analyses the
results.
2 Problem speciﬁcation
We now specify the problem of formal veriﬁcation of neural networks. Given a network that imple-
ments a function ˆxn = f (x0), a bounded input domain C and a property P , we want to prove

(cid:8)

∀y

x0 ∈ C,

ˆxn = f (x0) =⇒ P (ˆxn).

ˆxn[ya] ≥ ˆxn[y](cid:9)
.

(1)
For example,
the property of robustness to adversarial examples in L∞ norm around a
training sample a with label ya would be encoded by using C , {x0| kx0 − ak∞ ≤ ǫ} and
P (ˆxn) =
In this paper, we are going to focus on Piecewise-Linear Neural Networks (PL-NN), that is, networks
for which we can decompose C into a set of polyhedra Ci such that C = ∪i Ci, and the restriction
of f to Ci is a linear function for each i. While this prevents us from including networks that
use activation functions such as sigmoid or tanh, PL-NNs allow the use of linear transformations
such as fully-connected or convolutional layers, pooling units such as MaxPooling and activation
functions such as ReLUs.
In other words, PL-NNs represent the majority of networks used in
practice. Operations such as Batch-Normalization or Dropout also preserve piecewise linearity at
test-time.
The properties that we are going to consider are Boolean formulas over linear inequalities. In our
robustness to adversarial example above, the property is a conjunction of linear inequalities, each of
which constrains the output of the original label to be greater than the output of another label.
The scope of this paper does not include approaches relying on additional assumptions such as twice
differentiability of the network [8, 21], limitation of the activation to binary values [5, 16] or restric-
tion to a single linear domain [2]. Since they do not provide formal guarantees, we also don’t include
approximate approaches relying on a limited set of perturbation [10] or on over-approximation meth-
ods that potentially lead to undecidable properties [17, 20].
3 Veriﬁcation Formalism
3.1 Veriﬁcation as a Satisﬁability problem
The methods we involve in our comparison all leverage the piecewise-linear structure of PL-NN
to make the problem more tractable. They all follow the same general principle: given a property
to prove, they attempt to discover a counterexample that would make the property false. This is
accomplished by deﬁning a set of variables corresponding to the inputs, hidden units and output of
the network, and the set of constraints that a counterexample would satisfy.
To help design a uniﬁed framework, we reduce all instances of veriﬁcation problems to a canoni-
cal representation. Speciﬁcally, the whole satisﬁability problem will be transformed into a global
optimization problem where the decision will be obtained by checking the sign of the minimum.
If the property to verify is a simple inequality P (ˆxn) , cT ˆxn ≥ b, it is sufﬁcient to add to the
network a ﬁnal fully connected layer with one output, with weight of c and a bias of −b. If the
global minimum of this network is positive, it indicates that for all ˆxn the original network can
output, we have cT ˆxn − b ≥ 0 =⇒ cT ˆxn ≥ b, and as a consequence the property is True. On the
other hand, if the global minimum is negative, then the minimizer provides a counter-example. The
supplementary material shows that OR and AND clauses in the property can similarly be expressed as
additional layers, using MaxPooling units.
We can formulate any Boolean formula over linear inequalities on the output of the network
as a sequence of additional linear and max-pooling layers. The veriﬁcation problem will be
reduced to the problem of ﬁnding whether the scalar output of the potentially modiﬁed net-
work can reach a negative value. Assuming the network only contains ReLU activations be-
the satisﬁability problem to ﬁnd a counterexample can be expressed as:
tween each layer,

l0 ≤ x0 ≤ u0
ˆxn ≤ 0

(2a)
(2b)

ˆxi+1 = Wi+1xi + bi+1
xi = max (ˆxi, 0)

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}.

(2c)
(2d)

2

Eq. (10a) represents the constraints on the input and Eq. (10h) on the neural network output.
Eq. (10b) encodes the linear layers of the network and Eq. (2d) the ReLU activation functions. If an
assignment to all the values can be found, this represents a counterexample. If this problem is unsat-
isﬁable, no counterexample can exist, implying that the property is True. We emphasise that we are
required to prove that no counter-examples can exist, and not simply that none could be found.
While for clarity of explanation, we have limited ourselves to the speciﬁc case where only ReLU
activation functions are used, this is not restrictive. The supplementary material contains a section
detailing how each method speciﬁcally handles MaxPooling units, as well as how to convert any
MaxPooling operation into a combination of linear layers and ReLU activation functions.
The problem described in (2) is still a hard problem. The addition of the ReLU non-linearities (2d)
transforms a problem that would have been solvable by simple Linear Programming into an NP-hard
problem [11]. Converting a veriﬁcation problem into this canonical representation does not make
its resolution simpler but it does provide a formalism advantage. Speciﬁcally, it allows us to prove
complex properties, containing several OR clauses, with a single procedure rather than having to
decompose the desired property into separate queries as was done in previous work [11].
Operationally, a valid strategy is to impose the constraints (10a) to (2d) and minimise the value of
ˆxn. Finding the exact global minimum is not necessary for veriﬁcation. However, it provides a
measure of satisﬁability or unsatisﬁability. If the value of the global minimum is positive, it will
correspond to the margin by which the property is satisﬁed.
3.2 Mixed Integer Programming formulation
A possible way to eliminate the non-linearities is to encode them with the help of binary variables,
transforming the PL-NN veriﬁcation problem (2) into a Mixed Integer Linear Program (MIP). This
can be done with the use of “big-M” encoding. The following encoding is from Tjeng & Tedrake
[19]. Assuming we have access to lower and upper bounds on the values that can be taken by the
coordinates of ˆxi, which we denote li and ui, we can replace the non-linearities:

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

xi ≤ ui · δi
xi ≤ ˆxi − li · (1 − δi)

(3a)
(3b)

is

in Eq.

easy to verify that

δi[j] = 0 ⇔ xi[j] = 0 (replacing δi[j]

It
δi[j] = 1 ⇔ xi[j] = ˆxi[j] (replacing δi[j] in Eq. (21b)).
By taking advantage of the feed-forward structure of the neural network, lower and upper bounds
li and ui can be obtained by applying interval arithmetic [9] to propagate the bounds on the inputs,
one layer at a time.
Thanks to this speciﬁc feed-forward structure of the problem, the generic, non-linear, non-convex
problem has been rewritten into an MIP. Optimization of MIP is well studied and highly efﬁcient
off-the-shelf solvers exist. As solving them is NP-hard, performance is going to be dependent on
the quality of both the solver used and the encoding. We now ask the following question: how much
efﬁciency can be gained by using a bespoke solver rather than a generic one? In order to answer this,
we present specialised solvers for the PLNN veriﬁcation task.

(21a))

and

4 Branch-and-Bound for Veriﬁcation
As described in Section 3.1, the veriﬁcation problem can be rephrased as a global optimization
problem. Algorithms such as Stochastic Gradient Descent are not appropriate as they have no way of
guaranteeing whether or not a minima is global. In this section, we present an approach to estimate
the global minimum, based on the Branch and Bound paradigm and show that several published
methods, introduced as examples of Satisﬁability Modulo Theories, ﬁt this framework.
Algorithm 1 describes its generic form. The input domain is repeatedly split into sub-domains (line
7), over which lower and upper bounds of the minimum are computed (lines 9-10). The best upper-
bound found so far serves as a candidate for the global minimum. Any domain whose lower bound
is greater than the current global upper bound can be pruned away as it cannot contain the global
minimum (line 13, lines 15-17). By iteratively splitting the domains, it is possible to compute tighter
lower bounds. We keep track of the global lower bound on the minimum by taking the minimum
over the lower bounds of all sub-domains (line 19). When the global upper bound and the global
lower bound differ by less than a small scalar ǫ (line 5), we consider that we have converged.
Algorithm 1 shows how to optimise and obtain the global minimum. If all that we are interested in
is the satisﬁability problem, the procedure can be simpliﬁed by initialising the global upper bound
with 0 (in line 2). Any subdomain with a lower bound greater than 0 (and therefore not eligible to

3

contain a counterexample) will be pruned out (by line 15). The computation of the lower bound
can therefore be replaced by the feasibility problem (or its relaxation) imposing the constraint that
the output is below zero without changing the algorithm. If it is feasible, there might still be a
counterexample and further branching is necessary. If it is infeasible, the subdomain can be pruned
out. In addition, if any upper bound improving on 0 is found on a subdomain (line 11), it is possible
to stop the algorithm as this already indicates the presence of a counterexample.

global_ub ← inf
global_lb ← − inf
doms ← [(global_lb, domain)]
while global_ub − global_lb > ǫ do

Algorithm 1 Branch and Bound
1: function BAB(net, domain, ǫ)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end function

end while
return global_ub

(_ , dom) ← pick_out(doms)
[subdom_1, . . . , subdom_s] ← split(dom)
for i = 1 . . . s do

dom_ub ← compute_UB(net, subdom_i)
dom_lb ← compute_LB(net, subdom_i)
if dom_ub < global_ub then
global_ub ← dom_ub
prune_domains(doms, global_ub)

end if
if dom_lb < global_ub then

domains.append((dom_lb, subdom_i))

end if
end for
global_lb ← min{lb | (lb, dom) ∈ doms}

The description of the veriﬁcation problem
as optimization and the pseudo-code of Algo-
rithm 1 are generic and would apply to veriﬁ-
cation problems beyond the speciﬁc case of PL-
NN. To obtain a practical algorithm, it is neces-
sary to specify several elements.
A search strategy, deﬁned by the pick_out
function, which chooses the next domain to
branch on. Several heuristics are possible, for
example those based on the results of previous
bound computations. For satisﬁable problems
or optimization problems, this allows to dis-
cover good upper bounds, enabling early prun-
ing.
A branching rule, deﬁned by the split
function, which
takes
dom
subdomain
partition
return
and
a
such
that
that
Si subdom_i = dom and
(subdom_i ∩ subdom_j) = ∅, ∀i 6= j.
This
will deﬁne the “shape” of the domains, which
impacts the hardness of computing bounds.
In addition, choosing the right partition can

domain

in

a

greatly impact the quality of the resulting bounds.
Bounding methods, deﬁned by the compute_{UB, LB} functions. These procedures estimate re-
spectively upper bounds and lower bounds over the minimum output that the network net can reach
over a given input domain. We want the lower bound to be as high as possible, so that this whole
domain can be pruned easily. This is usually done by introducing convex relaxations of the problem
and minimising them. On the other hand, the computed upper bound should be as small as possi-
ble, so as to allow pruning out other regions of the space or discovering counterexamples. As any
feasible point corresponds to an upper bound on the minimum, heuristic methods are sufﬁcient.
We now demonstrate how some published work in the literature can be understood as special case
of the branch-and-bound framework for veriﬁcation.
4.1 Reluplex
Katz et al. [11] present a procedure named Reluplex to verify properties of Neural Network contain-
ing linear functions and ReLU activation unit, functioning as an SMT solver using the splitting-on-
demand framework [1]. The principle of Reluplex is to always maintain an assignment to all of the
variables, even if some of the constraints are violated.
Starting from an initial assignment, it attempts to ﬁx some violated constraints at each step.
It
prioritises ﬁxing linear constraints ((10a), (10b), (10h) and some relaxation of (2d)) using a simplex
algorithm, even if it leads to violated ReLU constraints.
If no solution to this relaxed problem
containing only linear constraints exists, the counterexample search is unsatisﬁable. Otherwise,
either all ReLU are respected, which generates a counterexample, or Reluplex attempts to ﬁx one
of the violated ReLU; potentially leading to newly violated linear constraints. This process is not
guaranteed to converge, so to make progress, non-linearities that get ﬁxed too often are split into two
cases. Two new problems are generated, each corresponding to one of the phases of the ReLU. In
the worst setting, the problem will be split completely over all possible combinations of activation
patterns, at which point the sub-problems will all be simple LPs.
This algorithm can be mapped to the special case of branch-and-bound for satisﬁability. The search
strategy is handled by the SMT core and to the best of our knowledge does not prioritise any domain.
The branching rule is implemented by the ReLU-splitting procedure: when neither the upper bound
search, nor the detection of infeasibility are successful, one non-linear constraint over the j-th neu-

4

(cid:1)

ˆxi[j], 0
(cid:0)

is split out into two subdomains: {xi[j] = 0, ˆxi[j] ≤ 0}
ron of the i-th layer xi[j] = max
and {xi[j] = ˆxi[j], ˆxi[j] ≥ 0}. This deﬁnes the type of subdomains produced. The prioritisation of
ReLUs that have been frequently ﬁxed is a heuristic to decide between possible partitions.
As Reluplex only deal with satisﬁability, the analogue of the lower bound computation is an over-
approximation of the satisﬁability problem. The bounding method used is a convex relaxation,
obtained by dropping some of the constraints. The following relaxation is applied to ReLU units for
which the sign of the input is unknown (li[j] ≤ 0 and ui[j] ≥ 0).

xi = max (ˆxi, 0) ⇒ xi ≥ ˆxi

(4a)

xi ≥ 0

(4b)

xi ≤ ui.

(4c)

If this relaxation is unsatisﬁable, this indicates that the subdomain cannot contain any counterex-
ample and can be pruned out. The search for an assignment satisfying all the ReLU constraints by
iteratively attempting to correct the violated ReLUs is a heuristic that is equivalent to the search for
an upper bound lower than 0: success implies the end of the procedure but no guarantees can be
given.

4.2 Planet
Ehlers [6] also proposed an approach based on SMT. Unlike Reluplex, the proposed tool, named
Planet, operates by explicitly attempting to ﬁnd an assignment to the phase of the non-linearities.
Reusing the notation of Section 3.2, it assigns a value of 0 or 1 to each δi[j] variable, verifying at
each step the feasibility of the partial assignment so as to prune infeasible partial assignment early.
As in Reluplex, the search strategy is not explicitly encoded and simply enumerates all the domains
that have not yet been pruned. The branching rule is the same as for Reluplex, as ﬁxing the decision
variable δi[j] = 0 is equivalent to choosing {xi[j] = 0, ˆxi[j] ≤ 0} and ﬁxing δi[j] = 1 is equivalent
to {xi[j] = ˆxi[j], ˆxi[j] ≥ 0} . Note however that Planet does not include any heuristic to prioritise
which decision variables should be split over.
Planet does not include a mechanism for early termination based on a heuristic search of a feasible
point. For satisﬁable problems, only when a full complete assignment is identiﬁed is a solution
returned. In order to detect incoherent assignments, Ehlers [6] introduces a global linear approxi-
mation to a neural network, which is used as a bounding method to over-approximate the set of
values that each hidden unit can take. In addition to the existing linear constraints ((10a), (10b)
and (10h)), the non-linear constraints are approximated by sets of linear constraints representing the
non-linearities’ convex hull. Speciﬁcally, ReLUs with input of unknown sign are replaced by the set
of equations:

ˆxi[j] − li[j]
ui[j] − li[j]

(5a)

(5b)

xi[j] ≤ ui[j]

xi = max (ˆxi, 0) ⇒ xi ≥ ˆxi

xi ≥ 0
(5c)
where xi[j] corresponds to the value of the j-th coordinate of xi. An illustration of the feasible
domain is provided in the supplementary material.
Compared with the relaxation of Reluplex (4), the Planet relaxation is tighter. Speciﬁcally, Eq. (4a)
and (4b) are identical to Eq. (5a) and (5b) but Eq. (5c) implies Eq. (4c). Indeed, given that ˆxi[j]
is smaller than ui[j], the fraction multiplying ui[j] is necessarily smaller than 1, implying that this
provides a tighter bounds on xi[j].
To use this approximation to compute better bounds than the ones given by simple interval arithmetic,
it is possible to leverage the feed-forward structure of the neural networks and obtain bounds one
layer at a time. Having included all the constraints up until the i-th layer, it is possible to optimize
over the resulting linear program and obtain bounds for all the units of the i-th layer, which in turn
will allow us to create the constraints (5) for the next layer.
In addition to the pruning obtained by the convex relaxation, both Planet and Reluplex make use of
conﬂict analysis [15] to discover combinations of splits that cannot lead to satisﬁable assignments,
allowing them to perform further pruning of the domains.

4.3 Potential improvements
As can be seen, previous approaches to neural network veriﬁcation have relied on methodologies
developed in three communities: optimization, for the creation of upper and lower bounds; veriﬁca-
tion, especially SMT; and machine learning, especially the feed-forward nature of neural networks
for the creation of relaxations. A natural question that arises is “Can other existing literature from
these domains be exploited to further improve neural network veriﬁcation?” Our uniﬁed branch-and-
bound formulation makes it easy to answer this question. To illustrate its power, we now provide a
non-exhaustive list of suggestions to speed-up veriﬁcation algorithms.

5

Better bounding —While the relaxation proposed by Ehlers [6] is tighter than the one used by
Reluplex, it can be improved further still. Speciﬁcally, after a splitting operation, on a smaller
domain, we can reﬁne all the li, ui bounds, to obtain a tither relaxation. We show the importance
of this in the experiments section with the BaB-relusplit method that performs splitting on the
activation like Planet but updates its approximation completely at each step.
One other possible area of improvement lies in the tightness of the bounds used. Equation (5) is
very closely related to the Mixed Integer Formulation of Equation (20). Indeed, it corresponds to
level 0 of the Sherali-Adams hierarchy of relaxations [18]. The proof for this statement can be
found in the supplementary material. Stronger relaxations could be obtained by exploring higher
levels of the hierarchy. This would jointly constrain groups of ReLUs, rather than linearising them
independently.

]

]

l0[i⋆

l0[i⋆

and

]+u0[i⋆

]+u0[i⋆

Better branchingThe decision to split on the activation of the ReLU non-linearities made by Planet
and Reluplex is intuitive as it provides a clear set of decision variables to ﬁx. However, it ignores
another natural branching strategy, namely, splitting the input domain. Indeed, it could be argued
that since the function encoded by the neural networks are piecewise linear in their input, this could
result in the computation of highly useful upper and lower bounds. To demonstrate this, we propose
the novel BaB-input algorithm: a branch-and-bound method that branches over the input features
of the network. Based on a domain with input constrained by Eq. (10a), the split function would
return two subdomains where bounds would be identical in all dimension except for the dimension
with the largest length, denoted i⋆. The bounds for each subdomain for dimension i⋆ are given by
l0[i⋆] ≤ x0[i⋆] ≤
≤ x0[i⋆] ≤ u0[i⋆]. Based on these tighter input bounds,
2
tighter bounds at all layers can be re-evaluated.
One of the main advantage of branching over the variables is that all subdomains generated by the
BaB algorithm when splitting over the input variables end up only having simple bound constraints
over the value that input variable can take. In order to exploit this property to the fullest, we use
the highly efﬁcient lower bound computation approach of Kolter & Wong [13]. This approach was
initially proposed in the context of robust optimization. However, our uniﬁed framework opens
the door for its use in veriﬁcation. Speciﬁcally, Kolter & Wong [13] identiﬁed an efﬁcient way of
computing bounds for the type of problems we encounter, by generating a feasible solution to the
dual of the LP generated by the Planet relaxation. While this bound is quite loose compared to
the one obtained through actual optimization, they are very fast to evaluate. We propose a smart
branching method BaBSB to replace the longest edge heuristic of BaB-input. For all possible splits,
we compute fast bounds for each of the resulting subdomain, and execute the split resulting in the
highest lower bound. The intuition is that despite their looseness, the fast bounds will still be useful
in identifying the promising splits.

2

5 Experimental setup
The problem of PL-NN veriﬁcation has been shown to be NP-complete [11]. Meaningful compari-
son between approaches therefore needs to be experimental.
5.1 Methods
The simplest baseline we refer to is BlackBox, a direct encoding of Eq. (2) into the Gurobi solver,
which will perform its own relaxation, without taking advantage of the problem’s structure.
For the SMT based methods, Reluplex and Planet, we use the publicly available versions [7, 12].
Both tools are implemented in C++ and relies on the GLPK library to solve their relaxation. We
wrote some software to convert in both directions between the input format of both solvers.
We also evaluate the potential of using MIP solvers, based on the formulation of Eq. (20). Due to the
lack of availability of open-sourced methods at the time of our experiments, we reimplemented the
approach in Python, using the Gurobi MIP solver. We report results for a variant called MIPplanet,
which uses bounds derived from Planet’s convex relaxation rather than simple interval arithmetic.
The MIP is not treated as a simple feasibility problem but is encoded to minimize the output ˆxn of
Equation (10h), with a callback interrupting the optimization as soon as a negative value is found.
Additional discussions on encodings of the MIP problem can be found in supplementary materials.
In our benchmark, we include the methods derived from our Branch and Bound analysis. Our
implementation follows faithfully Algorithm 1, is implemented in Python and uses Gurobi to solve
LPs. The pick_out strategy consists in prioritising the domain that currently has the smallest lower
bound. Upper bounds are generated by randomly sampling points on the considered domain, and
we use the convex approximation of Ehlers [6] to obtain lower bounds. As opposed to the approach

6

taken by Ehlers [6] of building a single approximation of the network, we rebuild the approximation
and recompute all bounds for each sub-domain. This is motivated by the observation shown in
Figure 1 which demonstrate the signiﬁcant improvements it brings, especially for deeper networks.
For split, BaB-input performs branching by splitting the input domain in half along its longest
edge and BaBSB does it by splitting the input domain along the dimension improving the global
lower bound the most according to the fast bounds of Kolter & Wong [13]. We also include results
for the BaB-relusplit variant, where the split method is based on the phase of the non-linearities,
similarly to Planet.

d
n
u
o
b
 
r
e
w
o
L

36.0

35.5

35.0

34.5

34.0

33.5

33.0

32.5

32.0

ReApproximating
FixedApproximation

103

102

101

d
n
u
o
b
 
r
e
w
o
L

100
0
−100

−101

−102

−103

−104

−105

ReApproximating
FixedApproximation

10−12

10−9

10−6

10−3

100

10−6 10−5 10−4 10−3 10−2 10−1 100

Relative area

Relative area

Figure 1: Quality of the linear approximation,
depending on the size of the input domain. We
plot the value of the lower bound as a function of
the area on which it is computed (higher is bet-
ter). The domains are centered around the global
minimum and repeatedly shrunk. Rebuilding com-
pletely the linear approximation at each step al-
lows to create tighter lower-bounds thanks to bet-
ter li and ui, as opposed to using the same con-
straints and only changing the bounds on input
variables. This effect is even more signiﬁcant on
deeper networks.

(b) Approximation on a
deep net from ACAS

(a) Approximation on a
CollisionDetection net
5.2 Evaluation Criteria
For each of the data sets, we compare the different methods using the same protocol. We attempt to
verify each property with a timeout of two hours, and a maximum allowed memory usage of 20GB,
on a single core of a machine with an i7-5930K CPU. We measure the time taken by the solvers
to either prove or disprove the property. If the property is false and the search problem is therefore
satisﬁable, we expect from the solver to exhibit a counterexample. If the returned input is not a valid
counterexample, we don’t count the property as successfully proven, even if the property is indeed
satisﬁable. All code and data necessary to replicate our analysis are released.

6 Analysis
We attempt to perform veriﬁcation on three data sets of properties and report the comparison results.
The dimensions of all the problems can be found in the supplementary material.
The CollisionDetection data set [6] attempts to predict whether two vehicles with parameterized
trajectories are going to collide. 500 properties are extracted from problems arising from a binary
search to identify the size of the region around training examples in which the prediction of the
network does not change. The network used is relatively shallow but due to the process used to
generate the properties, some lie extremely close between the decision boundary between SAT and
UNSAT. Results presented in Figure 10 therefore highlight the accuracy of methods.
The Airborne Collision Avoidance System (ACAS) data set, as released by Katz et al. [11] is a
neural network based advisory system recommending horizontal manoeuvres for an aircraft in order
to avoid collisions, based on sensor measurements. Each of the ﬁve possible manoeuvres is assigned
a score by the neural network and the action with the minimum score is chosen. The 188 properties
to verify are based on some speciﬁcation describing various scenarios. Due to the deeper network
involved, this data set is useful in highlighting the scalability of the various algorithms.
PCAMNIST is a novel data set that we introduce to get a better understanding of what factors
inﬂuence the performance of various methods. It is generated in a way to give control over different
architecture parameters. Details of the dataset construction are given in supplementary materials. We
present plots in Figure 4 showing the evolution of runtimes depending on each of the architectural
parameters of the networks.
Comparative evaluation of veriﬁcation approaches — In Figure 10, on the shallow networks of
CollisionDetection, most solvers succeed against all properties in about 10s. In particular, the SMT
inspired solvers Planet, Reluplex and the MIP solver are extremely fast. The BlackBox solver
doesn’t reach 100% accuracy due to producing incorrect counterexamples. Additional analysis can
be found about the cause of those errors in the supplementary materials.
On the deeper networks of ACAS, in Figure 10b, no errors are observed but most methods timeout
on the most challenging testcases. The best baseline is Reluplex, who reaches 79.26% success rate
at the two hour timeout, while our best method, BaBSB, already achieves 98.40% with a budget of
one hour. To reach Reluplex’s success rate, the required runtime is two orders of magnitude smaller.

7

0

10−1

100
Computation time (in s)

102

101

103

0

0

2000
Computation time (in s)

4000

6000

CollisionDetection

(b) ACAS Dataset

(a)
Dataset

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

Method

BaBSB
BaB
reluBaB

reluplex

MIPplanet

planet

Average
time
per Node

1.81s
2.11s
1.69s

0.30s

0.017s

1.5e-3s

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

Table 1: Average time to
explore a node for each
method.

0

100

101

102

103
Number of Nodes visited

104

105

106

(a) Properties solved for a given
number of nodes
to explore
(log scale).

Figure 2: Proportion of properties veri-
ﬁable for varying time budgets depending
on the methods employed. A higher curve
means that for the same time budget, more
properties will be solvable. All methods
solve CollisionDetection quite quickly ex-
cept reluBaB, which is much slower and
BlackBox who produces several incorrect
counterexamples.

Figure 3: The trade-off taken by the various
methods are different. Figure 3a shows how
many subdomains needs to be explored be-
fore verifying properties while Table 1 shows
the average time cost of exploring each sub-
domain. Our methods have a higher cost
per node but they require signiﬁcantly less
branching, thanks to better bounding. Note
also that between BaBSB and BaB, the
smart branching reduces by an order of mag-
nitude the number of nodes to visit.

Impact of each improvement —To identify which changes allow our method to have good perfor-
mance, we perform an ablation study and study the impact of removing some components of our
methods. The only difference between BaBSB and BaB is the smart branching, which represents a
signiﬁcant part of the performance gap.
Branching over the inputs rather than over the activations does not contribute much, as shown by
the small difference between BaB and reluBaB. Note however that we are able to use the fast
methods of Kolter & Wong [13] for the smart branching because branching over the inputs makes
the bounding problem similar to the one solved in robust optimization. Even if it doesn’t improve
performance by itself, the new type of split enables the use of smart branching.
The rest of the performance gap can be attributed to using a better bounds: reluBaB signiﬁcantly
outperforms planet while using the same branching strategy and the same convex relaxations. The
improvement comes from the beneﬁts of rebuilding the approximation at each step shown in Fig-
ure 1.
Figure 3 presents some additional analysis on a 20-property subset of the ACAS dataset, showing
how the methods used impact the need for branching. Smart branching and the use of better lower
bounds reduce heavily the number of subdomains to explore.

Timeout

Timeout

Timeout

Timeout

)
.
s
 
n
i
(
 
g
n
m
T

i

i

104

103

102

101

100

10−1

10−2

104

103

102

101

100

)
.
s
 
n
i
(
 
g
n
m
T

i

i

10−1

10−2

101

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

104

103

102

101

100

10−1

)
.
s
 
n
i
(
 
g
n
m
T

i

i

SAT / False

UNSAT / True

101

102

103

Number of Inputs

Layer Width

102

−104−103−102−101−1000100 101 102 103 104
Satisfiability margin

(a) Number of inputs

(b) Layer width

(c) Margin

10−2

2 × 100

3 × 100 4 × 100
Layer Depth

6 × 100

(d) Network depth

Figure 4:
Impact of the various parameters over the runtimes of the different solvers. The base network has
10 inputs and 4 layers of 25 hidden units, and the property to prove is True with a margin of 1000. Each of the
plot correspond to a variation of one of this parameters.

In the graphs of Figure 4, the trend for all the methods are similar, which seems to indicate that
hard properties are intrinsically hard and not just hard for a speciﬁc solver. Figure 4a shows an
expected trend: the largest the number of inputs, the harder the problem is. Similarly, Figure 4b
shows unsurprisingly that wider networks require more time to solve, which can be explained by
the fact that they have more non-linearities. The impact of the margin, as shown in Figure 4c is

)
.
s
 
n
i
(
 
g
n
m
T

i

i

104

103

102

101

100

10−1

10−2

8

also clear. Properties that are True or False with large satisﬁability margin are easy to prove, while
properties that have small satisﬁability margins are signiﬁcantly harder.

7 Conclusion
The improvement of formal veriﬁcation of Neural Networks represents an important challenge to be
tackled before learned models can be used in safety critical applications. By providing both a uniﬁed
framework to reason about methods and a set of empirical benchmarks to measure performance with,
we hope to contribute to progress in this direction. Our analysis of published algorithms through the
lens of Branch and Bound optimization has already resulted in signiﬁcant improvements in runtime
on our benchmarks. Its continued analysis should reveal even more efﬁcient algorithms in the future.

9

References
[1] Barrett, Clark, Nieuwenhuis, Robert, Oliveras, Albert, and Tinelli, Cesare. Splitting on de-
mand in sat modulo theories. International Conference on Logic for Programming Artiﬁcial
Intelligence and Reasoning, 2006.

[2] Bastani, Osbert, Ioannou, Yani, Lampropoulos, Leonidas, Vytiniotis, Dimitrios, Nori, Aditya,

and Criminisi, Antonio. Measuring neural net robustness with constraints. NIPS, 2016.

[3] Buxton, John N and Randell, Brian. Software Engineering Techniques: Report on a Conference

Sponsored by the NATO Science Committee. NATO Science Committee, 1970.

[4] Cheng, Chih-Hong, Nührenberg, Georg, and Ruess, Harald. Maximum resilience of artiﬁcial

neural networks. Automated Technology for Veriﬁcation and Analysis, 2017.

[5] Cheng, Chih-Hong, Nührenberg, Georg, and Ruess, Harald. Veriﬁcation of binarized neural

networks. arXiv:1710.03107, 2017.

[6] Ehlers, Ruediger. Formal veriﬁcation of piece-wise linear feed-forward neural networks. Au-

tomated Technology for Veriﬁcation and Analysis, 2017.

[7] Ehlers, Ruediger. Planet. https://github.com/progirep/planet, 2017.

[8] Hein, Matthias and Andriushchenko, Maksym. Formal guarantees on the robustness of a clas-

siﬁer against adversarial manipulation. NIPS, 2017.

[9] Hickey, Timothy, Ju, Qun, and Van Emden, Maarten H. Interval arithmetic: From principles

to implementation. Journal of the ACM (JACM), 2001.

[10] Huang, Xiaowei, Kwiatkowska, Marta, Wang, Sen, and Wu, Min. Safety veriﬁcation of deep

neural networks. International Conference on Computer Aided Veriﬁcation, 2017.

[11] Katz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and Kochenderfer, Mykel. Reluplex: An

efﬁcient smt solver for verifying deep neural networks. CAV, 2017.

[12] Katz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and Kochenderfer, Mykel. Reluplex.

https://github.com/guykatzz/ReluplexCav2017, 2017.

[13] Kolter, Zico and Wong, Eric. Provable defenses against adversarial examples via the convex

outer adversarial polytope. arXiv:1711.00851, 2017.

[14] Lomuscio, Alessio and Maganti, Lalit. An approach to reachability analysis for feed-forward

relu neural networks. arXiv:1706.07351, 2017.

[15] Marques-Silva, João P and Sakallah, Karem A. Grasp: A search algorithm for propositional

satisﬁability. IEEE Transactions on Computers, 1999.

[16] Narodytska, Nina, Kasiviswanathan, Shiva Prasad, Ryzhyk, Leonid, Sagiv, Mooly, and Walsh,
Toby. Verifying properties of binarized deep neural networks. arXiv:1709.06662, 2017.

[17] Pulina, Luca and Tacchella, Armando. An abstraction-reﬁnement approach to veriﬁcation of

artiﬁcial neural networks. CAV, 2010.

[18] Sherali, Hanif D and Adams, Warren P. A hierarchy of relaxations and convex hull character-
izations for mixed-integer zero—one programming problems. Discrete Applied Mathematics,
1994.

[19] Tjeng, Vincent and Tedrake, Russ. Verifying neural networks with mixed integer programming.

arXiv:1711.07356, 2017.

[20] Xiang, Weiming, Tran, Hoang-Dung, and Johnson, Taylor T. Output reachable set estimation

and veriﬁcation for multi-layer neural networks. arXiv:1708.03322, 2017.

[21] Zakrzewski, Radosiaw R. Veriﬁcation of a trained neural network accuracy. IJCNN, 2001.

10

A Canonical Form of the veriﬁcation problem
If the property is a simple inequality P (ˆxn) , cT ˆxn ≥ b, it is sufﬁcient to add to the network a ﬁnal
fully connected layer with one output, with weight of c and a bias of −b. If the global minimum
of this network is positive, it indicates that for all ˆxn the original network can output, we have
cT ˆxn − b ≥ 0 =⇒ cT ˆxn ≥ b, and as a consequence the property is True. On the other hand, if the
global minimum is negative, then the minimizer provides a counter-example.
Clauses speciﬁed using OR (denoted by
property is P (ˆxn) ,
Clauses speciﬁed using AND (denoted by
is equivalent to mini (cid:0)cT

) can be encoded by using a MaxPooling unit. If the
ˆxn − bi

) can be encoded similarly:
i ˆxn − bi(cid:1) ≥ 0 ⇐⇒ − (cid:0)maxi (cid:0)−cT

Vi (cid:2)
(cid:3)
B Toy Problem example
We have speciﬁed the problem of formal veriﬁcation of neural networks as follows: given a network
that implements a function ˆxn = f (x0), a bounded input domain C and a property P , we want to
prove that

, this is equivalent to maxi
(cid:3)

(cid:1)
the property P (ˆxn) =

i ˆxn + bi(cid:1)(cid:1) ≥ 0

ˆxn ≥ bi

ˆxn ≥ bi

cT
i
(cid:0)

Wi (cid:2)

≥ 0.

cT
i

cT
i

V

W

x0 ∈ C,

ˆxn = f (x0) =⇒ P (ˆxn).

(6)

A toy-example of the Neural Network veriﬁcation problem is given in Figure 5. On the domain
C = [−2; 2] × [−2; 2], we want to prove that the output y of the one hidden-layer network always
satisfy the property P (y) , [y > −5]. We will use this as a running example to explain the methods
used for comparison in our experiments.

[-2, 2]

x1

[-2, 2]

x2

a

1

1

-1

-1

b

y

-1

-1

Prove that y > −5

Figure 5: Example Neural Network. We attempt to prove the property that the network output is
always greater than -5

B.1 Problem formulation
For the network of Figure 5, the variables would be {x1, x2, ain, aout, bin, bout, y} and the set of
constraints would be:

− 2 ≤ x1 ≤ 2

ˆa = x1 + x2
y = −a − b

a = max(ˆa, 0)
y ≤ −5

−2 ≤ x2 ≤ 2
ˆb = −x1 − x2

b = max(ˆb, 0)

Here, ˆa is the input value to hidden unit, while a is the value after the ReLU. Any point satisfying all
the above constraints would be a counterexample to the property, as it would imply that it is possible
to drive the output to -5 or less.
B.2 MIP formulation
In our example, the non-linearities of equation (7c) would be replaced by

(7a)

(7b)

(7c)
(7d)

(8)

a ≥ 0
a ≤ ˆa − la(1 − δa)

a ≥ ˆa
a ≤ uaδa

δa ∈ {0, 1}

11

Step

x1

x2

ˆa

a

0

0
0

0
0

-2

ˆb
0

b

0

4
4

4
4

4

0

0
0

0
0
Fix linear constraints
0
1
0
0
1
0
Fix a ReLU
4
1
0
0
4
1
Fix linear constraints
1
-2
. . .

0
0

-4

4

y

0

-5
-5

-5
-5

-5

1

2

3

Figure 6: Evolution of the Reluplex algorithm. Red cells corresponds to value violating Linear con-
straints, and orange cells corresponds to value violating ReLU constraints. Resolution of violation
of linear constraints are prioritised.

where la is a lower bound of the value that ˆa can take (such as -4) and ua is an upper bound (such
as 4). The binary variable δa indicates which phase the ReLU is in: if δa = 0, the ReLU is blocked
and aout = 0, else the ReLU is passing and aout = ain. The problem remains difﬁcult due to the
integrality constraint on δa.
B.3 Running Reluplex
Table 6 shows the initial steps of a run of the Reluplex algorithm on the example of Figure 5. Starting
from an initial assignment, it attempts to ﬁx some violated constraints at each step. It prioritises
ﬁxing linear constraints ((7a), (7b) and (7d) in our illustrative example) using a simplex algorithm,
even if it leads to violated ReLU constraints (7c). This can be seen in step 1 and 3 of the process.
If no solution to the problem containing only linear constraints exists, this shows that the counterex-
ample search is unsatisﬁable. Otherwise, all linear constraints are ﬁxed and Reluplex attempts to ﬁx
one violated ReLU at a time, such as in step 2 of Table 6 (ﬁxing the ReLU b), potentially leading
to newly violated linear constraints. In the case where no violated ReLU exists, this means that a
satisﬁable assignment has been found and that the search can be interrupted.
This process is not guaranteed to converge, so to guarantee progress, non-linearities getting ﬁxed too
often are split into two cases. This generates two new sub-problems, each involving an additional
linear constraint instead of the linear one. The ﬁrst one solves the problem where ˆa ≤ 0 and a = 0,
the second one where ˆa ≥ 0 and a = ˆa. In the worst setting, the problem will be split completely
over all possible combinations of activation patterns, at which point the sub-problems are simple
LPs.

C Planet approximation
The feasible set of the Mixed Integer Programming formulation is given by the following set of
equations. We assume that all li are negative and ui are positive. In case this isn’t true, it is possible
to just update the bounds such that they are.

l0 ≤ x0 ≤ u0
ˆxi+1 = Wi+ixi + bi+i
xi ≥ 0
xi ≥ ˆxi
xi ≤ ˆxi − li · (1 − δi)
xi ≤ ui · δi
δi ∈ {0, 1}hi
ˆxn ≤ 0

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}

∀i ∈ {1, n − 1}

(9a)
(9b)
(9c)
(9d)
(9e)
(9f)

(9g)
(9h)

The level 0 of the Sherali-Adams hierarchy of relaxation Sherali & Adams [18] doesn’t include any
additional constraints. Indeed, polynomials of degree 0 are simply constants and their multiplication
with existing constraints followed by linearization therefore doesn’t add any new constraints. As a

12

xi[j]

li[j]

ˆxi[j]

ui[j]

Figure 7: Feasible domain corresponding to the Planet relaxation for a single ReLU.

result, the feasible domain given by the level 0 of the relaxation corresponds simply to the removal
of the integrality constraints:

l0 ≤ x0 ≤ u0
ˆxi+1 = Wi+ixi + bi+i
xi ≥ 0
xi ≥ ˆxi
xi ≤ ˆxi − li · (1 − di)
xi ≤ ui · di
0 ≤ di ≤ 1
ˆxn ≤ 0

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}

Combining the equations (10e) and (10f), looking at a single unit j in layer i, we obtain:

(11)
The function mapping di[j] to an upperbound of xi[j] is a minimum of linear functions, which means
that it is a concave function. As one of them is increasing and the other is decreasing, the maximum
will be reached when they are both equals.

ˆxi[j] − li(1 − di[j]), ui[j]di[j](cid:1)
(cid:0)

xi[j] ≤ min

Plugging this equation for d⋆ into Equation(11) gives that:

ˆxi[j] − li[j](1 − d⋆

i[j]) = ui[j]d⋆
i[j]
ˆxi[j] − li[j]
ui[j] − li[j]

d⋆
i[j] =

⇔

xi[j] ≤ ui[j]

ˆxi[j] − li[j]
ui[j] − li[j]

which corresponds to the upper bound of xi[j] introduced for Planet [6].
D MaxPooling
For space reason, we only described the case of ReLU activation function in the main paper. We
now present how to handle MaxPooling activation, either by converting them to the already handled
case of ReLU activations or by introducing an explicit encoding of them when appropriate.
D.1 Mixed Integer Programming
Similarly to the encoding of ReLU constraints using binary variables and bounds on the inputs, it is
possible to similarly encode MaxPooling constraints. The constraint

can be replaced by

y = max (x1, . . . , xk)

y ≥ xi
y ≤ xi + (ux1:k − lxi)(1 − δi)

δi = 1

X
i∈{1...k}
δi ∈ {0, 1}

∀i ∈ {1 . . . k}
∀i ∈ {1 . . . k}

∀i ∈ {1 . . . k}

where ux1:k is an upper-bound on all xi for i ∈ {1 . . . k} and lxi is a lower bound on xi.

13

(10a)
(10b)
(10c)
(10d)
(10e)
(10f)
(10g)

(10h)

(12)

(13)

(14)

(15a)
(15b)

(15c)

(15d)

(16)

(17)

(18)

(19a)

(19b)

D.2 Reluplex
In the version introduced by [11], there is no support for MaxPooling units. As the canonical repre-
sentation we evaluate needs them, we provide a way of encoding a MaxPooling unit as a combination
of Linear function and ReLUs.
To do so, we decompose the element-wise maximum into a series of pairwise maximum

max (xj , x2, x3, x4) = max( max (x1, x2) ,
max (x3, x4))

and decompose the pairwise maximums as sum of ReLUs:

max (x1, x2) = max (x1 − x2, 0) + max (x2 − lx2, 0) + lx2,

where lx2 is a pre-computed lower bound of the value that x2 can take.
As a result, we have seen that an elementwise maximum such as a MaxPooling unit can be decom-
posed as a series of pairwise maximum, which can themselves be decomposed into a sum of ReLUs
units. The only requirement is to be able to compute a lower bound on the input to the ReLU, for
which the methods discussed in the paper can help.
D.3 Planet
As opposed to Reluplex, Planet Ehlers [6] directly supports MaxPooling units. The SMT solver
driving the search can split either on ReLUs, by considering separately the case of the ReLU be-
ing passing or blocking. It also has the possibility on splitting on MaxPooling units, by treating
separately each possible choice of units being the largest one.
For the computation of lower bounds, the constraint

is replaced by the set of constraints:

y = max (x1, x2, x3, x4)

y ≥ xi

∀i ∈ {1 . . . 4}

y ≤

(xi − lxi) + max

lxi,

i

X
i

where xi are the inputs to the MaxPooling unit and lxi their lower bounds.
E Mixed Integers Variants
E.1 Encoding
Several variants of encoding are available to use Mixed Integer Programming as a solver for Neural
Network Veriﬁcation. As a reminder, in the main paper we used the formulation of Tjeng & Tedrake
[19]:

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

xi ≤ ui · δi
xi ≤ ˆxi − li · (1 − δi)

(20a)
(20b)

An alternative formulation is the one of Lomuscio & Maganti [14] and Cheng et al. [4]:

xi ≤ Mi · δi
xi ≤ ˆxi − Mi · (1 − δi)

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

(21a)
(21b)
where Mi = max (−li, ui). This is fundamentally the same encoding but with a sligthly worse
bounds that is used, as one of the side of the bounds isn’t as tight as it could be.
E.2 Obtaining bounds
The formulation described in Equations (20) and (21) are dependant on obtaining lower and upper
bounds for the value of the activation of the network.
Interval AnalysisOne way to obtain them, mentionned in the paper, is the use of interval arith-
metic [9]. If we have bounds li, ui for a vector xi, we can derive the bounds ˆli+1, ˆui+1 for a vector
ˆxi+1 = Wi+1xi + bi+1

ˆli+1[j] =

(cid:16)W +

i+1[j,k]l+

i[k] + W −

i+1[j,k]u+

i[k](cid:17) + bi+1[j]

ˆui+1[j] =

(cid:16)W +

i+1[j,k]u+

i[k] + W −

i+1[j,k]l+

i[k](cid:17) + bi+1[j]

X
k

X
k

(22a)

(22b)

with the notation a+ = max(a, 0) and a− = min(a, 0). Propagating the bounds through a ReLU
activation is simply equivalent to applying the ReLU to the bounds.

14

Planet Linear approximationAn alternative way to obtain bounds is to use the relaxation of Planet.
This is the methods that was employed in the paper: we build incrementally the network approxi-
mation, layer by layer. To obtain the bounds over an activation, we optimize its value subject to the
constraints of the relaxation.
Given that this is a convex problem, we will achieve the optimum. Given that it is a relaxation, the
optimum will be a valid bound for the activation (given that the feasible domain of the relaxation
includes the feasible domains subject to the original constraints).
Once this value is obtained, we can use it to build the relaxation for the following layers. We can
build the linear approximation for the whole network and extract the bounds for each activation to
use in the encoding of the MIP. While obtaining the bounds in this manner is more expensive than
simply doing interval analysis, the obtained bounds are better.
E.3 Objective function
In the paper, we have formalised the veriﬁcation problem as a satisﬁability problem, equating the
existence of a counterexample with the feasibility of the output of a (potentially modiﬁed) network
being negative.
In practice, it is beneﬁcial to not simply formulate it as a feasibility problem but as an optimization
problem where the output of the network is explicitly minimized.
E.4 Comparison
We present here a comparison on CollisionDetection and ACAS of the different variants.

1. Planet-feasible uses the encoding of Equation (20), with bounds obtained based on the

planet relaxation, and solve the problem simply as a satisﬁability problem.

2. Interval is the same as Planet-feasible, except that the bounds used are obtained by interval

analysis rather than with the Planet relaxation.

3. Planet-symfeasible is the same as Planet-feasible, except that the encoding is the one of

Equation (21).

4. Planet-opt is the same as Planet-feasible, except that the problem is solved as an opti-
mization problem. The MIP solver attempt to ﬁnd the global minimum of the output of
the network. Using Gurobi’s callback, if a feasible solution is found with a negative value,
the optimization is interrupted and the current solution is returned. This corresponds to the
version that is reported in the main paper.

interval
planetsym-feasible
planet-feasible
planet-opt

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

interval
planetsym-feasible
planet-feasible
planet-opt

100

101

102

103

Computation time (in s)

(a) CollisionDetection Dataset

0
100

101
102
Computation time (in s)

103

(b) ACAS Dataset

Figure 8: Comparison between the different variants of MIP formulation for Neural Network veriﬁcation.

The ﬁrst observation that can be made is that when we look at the CollisionDetection dataset in
Figure 8a, only Planet-opt solves the dataset to 100% accuracy. The reason why the other methods
don’t reach it is not because of timeout but because they return spurious counterexamples. As
they encode only satisﬁability problem, they terminate as soon as they identify a solution with a
value of zero. Due to the large constants involved in the big-M, those solutions are sometimes not
actually valid counterexamples. This is the same reason why the BlackBox solver doesn’t reach
100% accuracy in the experiments reported in the main paper.

15

The other results that we can observe is the impact of the quality of the bounds when the networks get
deeper, and the problem becomes therefore more complex, such as in the ACAS dataset. Interval
has the worst bounds and is much slower than the other methods. Planetsym-feasible, with its
slightly worse bounds, performs worse than Planet-feasible and Planet-opt.

F Experimental setup details
We provide in Table 2 the characteristics of all of the datasets used for the experimental comparison
in the main paper.

Data set

Count

Collision
Detection

500

ACAS

188

PCAMNIST

27

Model Architecture
6 inputs
40 hidden unit layer, MaxPool
19 hidden unit layer
2 outputs
5 inputs
6 layers of 50 hidden units
5 outputs
10 or {5, 10, 25, 100, 500, 784} inputs
4 or {2, 3, 4, 5, 6, 7} layers
of 25 or {10, 15, 25, 50, 100} hidden units,
1 output, with a margin of +1000 or
{-1e4, -1000, -100, -50, -10, -1 ,1, 10, 50, 100, 1000, 1e4}

Table 2: Dimensions of all the data sets. For PCAMNIST, we use a base network with 10 inputs,
4 layers of 25 hidden units and a margin of 1000. We generate new problems by changing one
parameter at a time, using the values inside the brackets.

G Additional performance details
Given that there is a signiﬁcant difference in the way veriﬁcation works for SAT problems vs. UN-
SAT problems, we report also comparison results on the subset of data sorted by decision type.

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

0

10−1

101

102

100
Computation time (in s)
(a) On SAT properties

103

0

10−1

101

100
Computation time (in s)
(b) On UNSAT properties

102

103

Figure 9: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed on the CollisionDetection dataset. We can identify that all the errors that BlackBox makes
are on SAT properties, as it returns incorrect counterexamples.

H PCAMNIST details
PCAMNIST is a novel data set that we introduce to get a better understanding of what factors in-
ﬂuence the performance of various methods. It is generated in a way to give control over different

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

16

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

0

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

0

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

4000

2000
Computation time (in s)
(a) On SAT properties

6000

4000

2000
Computation time (in s)
(b) On UNSAT properties

6000

Figure 10: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed on the ACAS dataset. We observe that planet doesn’t succeed in solving any of the SAT
properties, while our proposed methods are extremely efﬁcient at it, even if there remains some
properties that they can’t solve.

architecture parameters. The networks takes k features as input, corresponding to the ﬁrst k eigen-
vectors of a Principal Component Analysis decomposition of the digits from the MNIST data set.
We also vary the depth (number of layers), width (number of hidden unit in each layer) of the net-
works. We train a different network for each combination of parameters on the task of predicting the
parity of the presented digit. This results in the accuracies reported in Table 3.
The properties that we attempt to verify are whether there exists an input for which the score assigned
to the odd class is greater than the score of the even class plus a large conﬁdence. By tweaking the
value of the conﬁdence in the properties, we can make the property either True or False, and we
can choose by how much is it true. This gives us the possibility of tweaking the “margin”, which
represent a good measure of difﬁculty of a network.
In addition to the impact of each factors separately as was shown in the main paper, we can also
look at it as a generic dataset and plot the cactus plots like for the other datasets. This can be found
in Figure 11

Network Parameter

Accuracy

Nb inputs Width Depth

Train

Test

5
10
25
100
500
784

10
10
10
10
10

25
25
25
25
25
25

10
15
25
50
100

88.18% 87.3%
97.42% 96.09%
99.87% 98.69%
100% 98.77%
100% 98.84%
100% 98.64%

96.34% 95.75%
96.31% 95.81%
97.42% 96.09%
97.35% 96.0%
97.72% 95.75%

2
3
4
5
6
7
Table 3: Accuracies of the network trained for the PCAMNIST dataset.

96.45% 95.71%
96.98% 96.05%
97.42% 96.09%
96.78% 95.9%
95.48% 95.2%
96.81% 96.07%

10
10
10
10
10
10

25
25
25
25
25
25

4
4
4
4
4
4

4
4
4
4
4

17

0

10−1

100
Computation time (in s)

102

101

103

Figure 11: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed. The PCAMNIST dataset is challenging as None of the methods reaches more than 50%
success rate.

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

18

8
1
0
2
 
y
a
M
 
2
2
 
 
]
I

A
.
s
c
[
 
 
3
v
5
5
4
0
0
.
1
1
7
1
:
v
i
X
r
a

A Uniﬁed View of Piecewise Linear Neural Network Veriﬁcation

Rudy Bunel
University of Oxford
rudy@robots.ox.ac.uk

Ilker Turkaslan
University of Oxford
ilker.turkaslan@lmh.ox.ac.uk

Philip H.S. Torr
University of Oxford
philip.torr@eng.ox.ac.uk

Pushmeet Kohli
Deepmind
pushmeet@google.com

M. Pawan Kumar
University of Oxford
Alan Turing Institute
pawan@robots.ox.ac.uk

Abstract

The success of Deep Learning and its potential use in many safety-critical applica-
tions has motivated research on formal veriﬁcation of Neural Network (NN) mod-
els. Despite the reputation of learned NN models to behave as black boxes and the
theoretical hardness of proving their properties, researchers have been successful
in verifying some classes of models by exploiting their piecewise linear structure
and taking insights from formal methods such as Satisiﬁability Modulo Theory.
These methods are however still far from scaling to realistic neural networks. To
facilitate progress on this crucial area, we make two key contributions. First, we
present a uniﬁed framework that encompasses previous methods. This analysis
results in the identiﬁcation of new methods that combine the strengths of mul-
tiple existing approaches, accomplishing a speedup of two orders of magnitude
compared to the previous state of the art. Second, we propose a new data set of
benchmarks which includes a collection of previously released testcases. We use
the benchmark to provide the ﬁrst experimental comparison of existing algorithms
and identify the factors impacting the hardness of veriﬁcation problems.

1 Introduction
Despite their success in a wide variety of applications, Deep Neural Networks have seen limited
adoption in safety-critical settings. The main explanation for this lies in their reputation for being
black-boxes whose behaviour can not be predicted. Current approaches to evaluate trained models
mostly rely on testing using held-out data sets. However, as Edsger W. Dijkstra said [3], “testing
shows the presence, not the absence of bugs”. If deep learning models are to be deployed to applica-
tions such as autonomous driving cars, we need to be able to verify safety-critical behaviours.
To this end, some researchers have tried to use formal methods. To the best of our knowledge,
Zakrzewski [21] was the ﬁrst to propose a method to verify simple, one hidden layer neural networks.
However, only recently were researchers able to work with non-trivial models by taking advantage
of the structure of ReLU-based networks [5, 11]. Even then, these works are not scalable to the large
networks encountered in most real world problems.
This paper advances the ﬁeld of NN veriﬁcation by making the following key contributions:

1. We reframe state of the art veriﬁcation methods as special cases of Branch-and-Bound

optimization, which provides us with a uniﬁed framework to compare them.

2. We gather a data set of test cases based on the existing literature and extend it with new
benchmarks. We provide the ﬁrst experimental comparison of veriﬁcation methods.

3. Based on this framework, we identify algorithmic improvements in the veriﬁcation process,
speciﬁcally in the way bounds are computed, the type of branching that are considered, as

Preprint. Work in progress.

well as the strategies guiding the branching. Compared to the previous state of the art, these
improvements lead to speed-up of almost two orders of magnitudes.

Section 2 and 3 give the speciﬁcation of the problem and formalise the veriﬁcation process. Section
4 presents our uniﬁed framework, showing that previous methods are special cases and highlight-
ing potential improvements. Section 5 presents our experimental setup and Section 6 analyses the
results.
2 Problem speciﬁcation
We now specify the problem of formal veriﬁcation of neural networks. Given a network that imple-
ments a function ˆxn = f (x0), a bounded input domain C and a property P , we want to prove

(cid:8)

∀y

x0 ∈ C,

ˆxn = f (x0) =⇒ P (ˆxn).

ˆxn[ya] ≥ ˆxn[y](cid:9)
.

(1)
For example,
the property of robustness to adversarial examples in L∞ norm around a
training sample a with label ya would be encoded by using C , {x0| kx0 − ak∞ ≤ ǫ} and
P (ˆxn) =
In this paper, we are going to focus on Piecewise-Linear Neural Networks (PL-NN), that is, networks
for which we can decompose C into a set of polyhedra Ci such that C = ∪i Ci, and the restriction
of f to Ci is a linear function for each i. While this prevents us from including networks that
use activation functions such as sigmoid or tanh, PL-NNs allow the use of linear transformations
such as fully-connected or convolutional layers, pooling units such as MaxPooling and activation
functions such as ReLUs.
In other words, PL-NNs represent the majority of networks used in
practice. Operations such as Batch-Normalization or Dropout also preserve piecewise linearity at
test-time.
The properties that we are going to consider are Boolean formulas over linear inequalities. In our
robustness to adversarial example above, the property is a conjunction of linear inequalities, each of
which constrains the output of the original label to be greater than the output of another label.
The scope of this paper does not include approaches relying on additional assumptions such as twice
differentiability of the network [8, 21], limitation of the activation to binary values [5, 16] or restric-
tion to a single linear domain [2]. Since they do not provide formal guarantees, we also don’t include
approximate approaches relying on a limited set of perturbation [10] or on over-approximation meth-
ods that potentially lead to undecidable properties [17, 20].
3 Veriﬁcation Formalism
3.1 Veriﬁcation as a Satisﬁability problem
The methods we involve in our comparison all leverage the piecewise-linear structure of PL-NN
to make the problem more tractable. They all follow the same general principle: given a property
to prove, they attempt to discover a counterexample that would make the property false. This is
accomplished by deﬁning a set of variables corresponding to the inputs, hidden units and output of
the network, and the set of constraints that a counterexample would satisfy.
To help design a uniﬁed framework, we reduce all instances of veriﬁcation problems to a canoni-
cal representation. Speciﬁcally, the whole satisﬁability problem will be transformed into a global
optimization problem where the decision will be obtained by checking the sign of the minimum.
If the property to verify is a simple inequality P (ˆxn) , cT ˆxn ≥ b, it is sufﬁcient to add to the
network a ﬁnal fully connected layer with one output, with weight of c and a bias of −b. If the
global minimum of this network is positive, it indicates that for all ˆxn the original network can
output, we have cT ˆxn − b ≥ 0 =⇒ cT ˆxn ≥ b, and as a consequence the property is True. On the
other hand, if the global minimum is negative, then the minimizer provides a counter-example. The
supplementary material shows that OR and AND clauses in the property can similarly be expressed as
additional layers, using MaxPooling units.
We can formulate any Boolean formula over linear inequalities on the output of the network
as a sequence of additional linear and max-pooling layers. The veriﬁcation problem will be
reduced to the problem of ﬁnding whether the scalar output of the potentially modiﬁed net-
work can reach a negative value. Assuming the network only contains ReLU activations be-
the satisﬁability problem to ﬁnd a counterexample can be expressed as:
tween each layer,

l0 ≤ x0 ≤ u0
ˆxn ≤ 0

(2a)
(2b)

ˆxi+1 = Wi+1xi + bi+1
xi = max (ˆxi, 0)

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}.

(2c)
(2d)

2

Eq. (10a) represents the constraints on the input and Eq. (10h) on the neural network output.
Eq. (10b) encodes the linear layers of the network and Eq. (2d) the ReLU activation functions. If an
assignment to all the values can be found, this represents a counterexample. If this problem is unsat-
isﬁable, no counterexample can exist, implying that the property is True. We emphasise that we are
required to prove that no counter-examples can exist, and not simply that none could be found.
While for clarity of explanation, we have limited ourselves to the speciﬁc case where only ReLU
activation functions are used, this is not restrictive. The supplementary material contains a section
detailing how each method speciﬁcally handles MaxPooling units, as well as how to convert any
MaxPooling operation into a combination of linear layers and ReLU activation functions.
The problem described in (2) is still a hard problem. The addition of the ReLU non-linearities (2d)
transforms a problem that would have been solvable by simple Linear Programming into an NP-hard
problem [11]. Converting a veriﬁcation problem into this canonical representation does not make
its resolution simpler but it does provide a formalism advantage. Speciﬁcally, it allows us to prove
complex properties, containing several OR clauses, with a single procedure rather than having to
decompose the desired property into separate queries as was done in previous work [11].
Operationally, a valid strategy is to impose the constraints (10a) to (2d) and minimise the value of
ˆxn. Finding the exact global minimum is not necessary for veriﬁcation. However, it provides a
measure of satisﬁability or unsatisﬁability. If the value of the global minimum is positive, it will
correspond to the margin by which the property is satisﬁed.
3.2 Mixed Integer Programming formulation
A possible way to eliminate the non-linearities is to encode them with the help of binary variables,
transforming the PL-NN veriﬁcation problem (2) into a Mixed Integer Linear Program (MIP). This
can be done with the use of “big-M” encoding. The following encoding is from Tjeng & Tedrake
[19]. Assuming we have access to lower and upper bounds on the values that can be taken by the
coordinates of ˆxi, which we denote li and ui, we can replace the non-linearities:

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

xi ≤ ui · δi
xi ≤ ˆxi − li · (1 − δi)

(3a)
(3b)

is

in Eq.

easy to verify that

δi[j] = 0 ⇔ xi[j] = 0 (replacing δi[j]

It
δi[j] = 1 ⇔ xi[j] = ˆxi[j] (replacing δi[j] in Eq. (21b)).
By taking advantage of the feed-forward structure of the neural network, lower and upper bounds
li and ui can be obtained by applying interval arithmetic [9] to propagate the bounds on the inputs,
one layer at a time.
Thanks to this speciﬁc feed-forward structure of the problem, the generic, non-linear, non-convex
problem has been rewritten into an MIP. Optimization of MIP is well studied and highly efﬁcient
off-the-shelf solvers exist. As solving them is NP-hard, performance is going to be dependent on
the quality of both the solver used and the encoding. We now ask the following question: how much
efﬁciency can be gained by using a bespoke solver rather than a generic one? In order to answer this,
we present specialised solvers for the PLNN veriﬁcation task.

(21a))

and

4 Branch-and-Bound for Veriﬁcation
As described in Section 3.1, the veriﬁcation problem can be rephrased as a global optimization
problem. Algorithms such as Stochastic Gradient Descent are not appropriate as they have no way of
guaranteeing whether or not a minima is global. In this section, we present an approach to estimate
the global minimum, based on the Branch and Bound paradigm and show that several published
methods, introduced as examples of Satisﬁability Modulo Theories, ﬁt this framework.
Algorithm 1 describes its generic form. The input domain is repeatedly split into sub-domains (line
7), over which lower and upper bounds of the minimum are computed (lines 9-10). The best upper-
bound found so far serves as a candidate for the global minimum. Any domain whose lower bound
is greater than the current global upper bound can be pruned away as it cannot contain the global
minimum (line 13, lines 15-17). By iteratively splitting the domains, it is possible to compute tighter
lower bounds. We keep track of the global lower bound on the minimum by taking the minimum
over the lower bounds of all sub-domains (line 19). When the global upper bound and the global
lower bound differ by less than a small scalar ǫ (line 5), we consider that we have converged.
Algorithm 1 shows how to optimise and obtain the global minimum. If all that we are interested in
is the satisﬁability problem, the procedure can be simpliﬁed by initialising the global upper bound
with 0 (in line 2). Any subdomain with a lower bound greater than 0 (and therefore not eligible to

3

contain a counterexample) will be pruned out (by line 15). The computation of the lower bound
can therefore be replaced by the feasibility problem (or its relaxation) imposing the constraint that
the output is below zero without changing the algorithm. If it is feasible, there might still be a
counterexample and further branching is necessary. If it is infeasible, the subdomain can be pruned
out. In addition, if any upper bound improving on 0 is found on a subdomain (line 11), it is possible
to stop the algorithm as this already indicates the presence of a counterexample.

global_ub ← inf
global_lb ← − inf
doms ← [(global_lb, domain)]
while global_ub − global_lb > ǫ do

Algorithm 1 Branch and Bound
1: function BAB(net, domain, ǫ)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end function

end while
return global_ub

(_ , dom) ← pick_out(doms)
[subdom_1, . . . , subdom_s] ← split(dom)
for i = 1 . . . s do

dom_ub ← compute_UB(net, subdom_i)
dom_lb ← compute_LB(net, subdom_i)
if dom_ub < global_ub then
global_ub ← dom_ub
prune_domains(doms, global_ub)

end if
if dom_lb < global_ub then

domains.append((dom_lb, subdom_i))

end if
end for
global_lb ← min{lb | (lb, dom) ∈ doms}

The description of the veriﬁcation problem
as optimization and the pseudo-code of Algo-
rithm 1 are generic and would apply to veriﬁ-
cation problems beyond the speciﬁc case of PL-
NN. To obtain a practical algorithm, it is neces-
sary to specify several elements.
A search strategy, deﬁned by the pick_out
function, which chooses the next domain to
branch on. Several heuristics are possible, for
example those based on the results of previous
bound computations. For satisﬁable problems
or optimization problems, this allows to dis-
cover good upper bounds, enabling early prun-
ing.
A branching rule, deﬁned by the split
function, which
takes
dom
subdomain
partition
return
and
a
such
that
that
Si subdom_i = dom and
(subdom_i ∩ subdom_j) = ∅, ∀i 6= j.
This
will deﬁne the “shape” of the domains, which
impacts the hardness of computing bounds.
In addition, choosing the right partition can

domain

in

a

greatly impact the quality of the resulting bounds.
Bounding methods, deﬁned by the compute_{UB, LB} functions. These procedures estimate re-
spectively upper bounds and lower bounds over the minimum output that the network net can reach
over a given input domain. We want the lower bound to be as high as possible, so that this whole
domain can be pruned easily. This is usually done by introducing convex relaxations of the problem
and minimising them. On the other hand, the computed upper bound should be as small as possi-
ble, so as to allow pruning out other regions of the space or discovering counterexamples. As any
feasible point corresponds to an upper bound on the minimum, heuristic methods are sufﬁcient.
We now demonstrate how some published work in the literature can be understood as special case
of the branch-and-bound framework for veriﬁcation.
4.1 Reluplex
Katz et al. [11] present a procedure named Reluplex to verify properties of Neural Network contain-
ing linear functions and ReLU activation unit, functioning as an SMT solver using the splitting-on-
demand framework [1]. The principle of Reluplex is to always maintain an assignment to all of the
variables, even if some of the constraints are violated.
Starting from an initial assignment, it attempts to ﬁx some violated constraints at each step.
It
prioritises ﬁxing linear constraints ((10a), (10b), (10h) and some relaxation of (2d)) using a simplex
algorithm, even if it leads to violated ReLU constraints.
If no solution to this relaxed problem
containing only linear constraints exists, the counterexample search is unsatisﬁable. Otherwise,
either all ReLU are respected, which generates a counterexample, or Reluplex attempts to ﬁx one
of the violated ReLU; potentially leading to newly violated linear constraints. This process is not
guaranteed to converge, so to make progress, non-linearities that get ﬁxed too often are split into two
cases. Two new problems are generated, each corresponding to one of the phases of the ReLU. In
the worst setting, the problem will be split completely over all possible combinations of activation
patterns, at which point the sub-problems will all be simple LPs.
This algorithm can be mapped to the special case of branch-and-bound for satisﬁability. The search
strategy is handled by the SMT core and to the best of our knowledge does not prioritise any domain.
The branching rule is implemented by the ReLU-splitting procedure: when neither the upper bound
search, nor the detection of infeasibility are successful, one non-linear constraint over the j-th neu-

4

(cid:1)

ˆxi[j], 0
(cid:0)

is split out into two subdomains: {xi[j] = 0, ˆxi[j] ≤ 0}
ron of the i-th layer xi[j] = max
and {xi[j] = ˆxi[j], ˆxi[j] ≥ 0}. This deﬁnes the type of subdomains produced. The prioritisation of
ReLUs that have been frequently ﬁxed is a heuristic to decide between possible partitions.
As Reluplex only deal with satisﬁability, the analogue of the lower bound computation is an over-
approximation of the satisﬁability problem. The bounding method used is a convex relaxation,
obtained by dropping some of the constraints. The following relaxation is applied to ReLU units for
which the sign of the input is unknown (li[j] ≤ 0 and ui[j] ≥ 0).

xi = max (ˆxi, 0) ⇒ xi ≥ ˆxi

(4a)

xi ≥ 0

(4b)

xi ≤ ui.

(4c)

If this relaxation is unsatisﬁable, this indicates that the subdomain cannot contain any counterex-
ample and can be pruned out. The search for an assignment satisfying all the ReLU constraints by
iteratively attempting to correct the violated ReLUs is a heuristic that is equivalent to the search for
an upper bound lower than 0: success implies the end of the procedure but no guarantees can be
given.

4.2 Planet
Ehlers [6] also proposed an approach based on SMT. Unlike Reluplex, the proposed tool, named
Planet, operates by explicitly attempting to ﬁnd an assignment to the phase of the non-linearities.
Reusing the notation of Section 3.2, it assigns a value of 0 or 1 to each δi[j] variable, verifying at
each step the feasibility of the partial assignment so as to prune infeasible partial assignment early.
As in Reluplex, the search strategy is not explicitly encoded and simply enumerates all the domains
that have not yet been pruned. The branching rule is the same as for Reluplex, as ﬁxing the decision
variable δi[j] = 0 is equivalent to choosing {xi[j] = 0, ˆxi[j] ≤ 0} and ﬁxing δi[j] = 1 is equivalent
to {xi[j] = ˆxi[j], ˆxi[j] ≥ 0} . Note however that Planet does not include any heuristic to prioritise
which decision variables should be split over.
Planet does not include a mechanism for early termination based on a heuristic search of a feasible
point. For satisﬁable problems, only when a full complete assignment is identiﬁed is a solution
returned. In order to detect incoherent assignments, Ehlers [6] introduces a global linear approxi-
mation to a neural network, which is used as a bounding method to over-approximate the set of
values that each hidden unit can take. In addition to the existing linear constraints ((10a), (10b)
and (10h)), the non-linear constraints are approximated by sets of linear constraints representing the
non-linearities’ convex hull. Speciﬁcally, ReLUs with input of unknown sign are replaced by the set
of equations:

ˆxi[j] − li[j]
ui[j] − li[j]

(5a)

(5b)

xi[j] ≤ ui[j]

xi = max (ˆxi, 0) ⇒ xi ≥ ˆxi

xi ≥ 0
(5c)
where xi[j] corresponds to the value of the j-th coordinate of xi. An illustration of the feasible
domain is provided in the supplementary material.
Compared with the relaxation of Reluplex (4), the Planet relaxation is tighter. Speciﬁcally, Eq. (4a)
and (4b) are identical to Eq. (5a) and (5b) but Eq. (5c) implies Eq. (4c). Indeed, given that ˆxi[j]
is smaller than ui[j], the fraction multiplying ui[j] is necessarily smaller than 1, implying that this
provides a tighter bounds on xi[j].
To use this approximation to compute better bounds than the ones given by simple interval arithmetic,
it is possible to leverage the feed-forward structure of the neural networks and obtain bounds one
layer at a time. Having included all the constraints up until the i-th layer, it is possible to optimize
over the resulting linear program and obtain bounds for all the units of the i-th layer, which in turn
will allow us to create the constraints (5) for the next layer.
In addition to the pruning obtained by the convex relaxation, both Planet and Reluplex make use of
conﬂict analysis [15] to discover combinations of splits that cannot lead to satisﬁable assignments,
allowing them to perform further pruning of the domains.

4.3 Potential improvements
As can be seen, previous approaches to neural network veriﬁcation have relied on methodologies
developed in three communities: optimization, for the creation of upper and lower bounds; veriﬁca-
tion, especially SMT; and machine learning, especially the feed-forward nature of neural networks
for the creation of relaxations. A natural question that arises is “Can other existing literature from
these domains be exploited to further improve neural network veriﬁcation?” Our uniﬁed branch-and-
bound formulation makes it easy to answer this question. To illustrate its power, we now provide a
non-exhaustive list of suggestions to speed-up veriﬁcation algorithms.

5

Better bounding —While the relaxation proposed by Ehlers [6] is tighter than the one used by
Reluplex, it can be improved further still. Speciﬁcally, after a splitting operation, on a smaller
domain, we can reﬁne all the li, ui bounds, to obtain a tither relaxation. We show the importance
of this in the experiments section with the BaB-relusplit method that performs splitting on the
activation like Planet but updates its approximation completely at each step.
One other possible area of improvement lies in the tightness of the bounds used. Equation (5) is
very closely related to the Mixed Integer Formulation of Equation (20). Indeed, it corresponds to
level 0 of the Sherali-Adams hierarchy of relaxations [18]. The proof for this statement can be
found in the supplementary material. Stronger relaxations could be obtained by exploring higher
levels of the hierarchy. This would jointly constrain groups of ReLUs, rather than linearising them
independently.

]

]

l0[i⋆

l0[i⋆

and

]+u0[i⋆

]+u0[i⋆

Better branchingThe decision to split on the activation of the ReLU non-linearities made by Planet
and Reluplex is intuitive as it provides a clear set of decision variables to ﬁx. However, it ignores
another natural branching strategy, namely, splitting the input domain. Indeed, it could be argued
that since the function encoded by the neural networks are piecewise linear in their input, this could
result in the computation of highly useful upper and lower bounds. To demonstrate this, we propose
the novel BaB-input algorithm: a branch-and-bound method that branches over the input features
of the network. Based on a domain with input constrained by Eq. (10a), the split function would
return two subdomains where bounds would be identical in all dimension except for the dimension
with the largest length, denoted i⋆. The bounds for each subdomain for dimension i⋆ are given by
l0[i⋆] ≤ x0[i⋆] ≤
≤ x0[i⋆] ≤ u0[i⋆]. Based on these tighter input bounds,
2
tighter bounds at all layers can be re-evaluated.
One of the main advantage of branching over the variables is that all subdomains generated by the
BaB algorithm when splitting over the input variables end up only having simple bound constraints
over the value that input variable can take. In order to exploit this property to the fullest, we use
the highly efﬁcient lower bound computation approach of Kolter & Wong [13]. This approach was
initially proposed in the context of robust optimization. However, our uniﬁed framework opens
the door for its use in veriﬁcation. Speciﬁcally, Kolter & Wong [13] identiﬁed an efﬁcient way of
computing bounds for the type of problems we encounter, by generating a feasible solution to the
dual of the LP generated by the Planet relaxation. While this bound is quite loose compared to
the one obtained through actual optimization, they are very fast to evaluate. We propose a smart
branching method BaBSB to replace the longest edge heuristic of BaB-input. For all possible splits,
we compute fast bounds for each of the resulting subdomain, and execute the split resulting in the
highest lower bound. The intuition is that despite their looseness, the fast bounds will still be useful
in identifying the promising splits.

2

5 Experimental setup
The problem of PL-NN veriﬁcation has been shown to be NP-complete [11]. Meaningful compari-
son between approaches therefore needs to be experimental.
5.1 Methods
The simplest baseline we refer to is BlackBox, a direct encoding of Eq. (2) into the Gurobi solver,
which will perform its own relaxation, without taking advantage of the problem’s structure.
For the SMT based methods, Reluplex and Planet, we use the publicly available versions [7, 12].
Both tools are implemented in C++ and relies on the GLPK library to solve their relaxation. We
wrote some software to convert in both directions between the input format of both solvers.
We also evaluate the potential of using MIP solvers, based on the formulation of Eq. (20). Due to the
lack of availability of open-sourced methods at the time of our experiments, we reimplemented the
approach in Python, using the Gurobi MIP solver. We report results for a variant called MIPplanet,
which uses bounds derived from Planet’s convex relaxation rather than simple interval arithmetic.
The MIP is not treated as a simple feasibility problem but is encoded to minimize the output ˆxn of
Equation (10h), with a callback interrupting the optimization as soon as a negative value is found.
Additional discussions on encodings of the MIP problem can be found in supplementary materials.
In our benchmark, we include the methods derived from our Branch and Bound analysis. Our
implementation follows faithfully Algorithm 1, is implemented in Python and uses Gurobi to solve
LPs. The pick_out strategy consists in prioritising the domain that currently has the smallest lower
bound. Upper bounds are generated by randomly sampling points on the considered domain, and
we use the convex approximation of Ehlers [6] to obtain lower bounds. As opposed to the approach

6

taken by Ehlers [6] of building a single approximation of the network, we rebuild the approximation
and recompute all bounds for each sub-domain. This is motivated by the observation shown in
Figure 1 which demonstrate the signiﬁcant improvements it brings, especially for deeper networks.
For split, BaB-input performs branching by splitting the input domain in half along its longest
edge and BaBSB does it by splitting the input domain along the dimension improving the global
lower bound the most according to the fast bounds of Kolter & Wong [13]. We also include results
for the BaB-relusplit variant, where the split method is based on the phase of the non-linearities,
similarly to Planet.

d
n
u
o
b
 
r
e
w
o
L

36.0

35.5

35.0

34.5

34.0

33.5

33.0

32.5

32.0

ReApproximating
FixedApproximation

103

102

101

d
n
u
o
b
 
r
e
w
o
L

100
0
−100

−101

−102

−103

−104

−105

ReApproximating
FixedApproximation

10−12

10−9

10−6

10−3

100

10−6 10−5 10−4 10−3 10−2 10−1 100

Relative area

Relative area

Figure 1: Quality of the linear approximation,
depending on the size of the input domain. We
plot the value of the lower bound as a function of
the area on which it is computed (higher is bet-
ter). The domains are centered around the global
minimum and repeatedly shrunk. Rebuilding com-
pletely the linear approximation at each step al-
lows to create tighter lower-bounds thanks to bet-
ter li and ui, as opposed to using the same con-
straints and only changing the bounds on input
variables. This effect is even more signiﬁcant on
deeper networks.

(b) Approximation on a
deep net from ACAS

(a) Approximation on a
CollisionDetection net
5.2 Evaluation Criteria
For each of the data sets, we compare the different methods using the same protocol. We attempt to
verify each property with a timeout of two hours, and a maximum allowed memory usage of 20GB,
on a single core of a machine with an i7-5930K CPU. We measure the time taken by the solvers
to either prove or disprove the property. If the property is false and the search problem is therefore
satisﬁable, we expect from the solver to exhibit a counterexample. If the returned input is not a valid
counterexample, we don’t count the property as successfully proven, even if the property is indeed
satisﬁable. All code and data necessary to replicate our analysis are released.

6 Analysis
We attempt to perform veriﬁcation on three data sets of properties and report the comparison results.
The dimensions of all the problems can be found in the supplementary material.
The CollisionDetection data set [6] attempts to predict whether two vehicles with parameterized
trajectories are going to collide. 500 properties are extracted from problems arising from a binary
search to identify the size of the region around training examples in which the prediction of the
network does not change. The network used is relatively shallow but due to the process used to
generate the properties, some lie extremely close between the decision boundary between SAT and
UNSAT. Results presented in Figure 10 therefore highlight the accuracy of methods.
The Airborne Collision Avoidance System (ACAS) data set, as released by Katz et al. [11] is a
neural network based advisory system recommending horizontal manoeuvres for an aircraft in order
to avoid collisions, based on sensor measurements. Each of the ﬁve possible manoeuvres is assigned
a score by the neural network and the action with the minimum score is chosen. The 188 properties
to verify are based on some speciﬁcation describing various scenarios. Due to the deeper network
involved, this data set is useful in highlighting the scalability of the various algorithms.
PCAMNIST is a novel data set that we introduce to get a better understanding of what factors
inﬂuence the performance of various methods. It is generated in a way to give control over different
architecture parameters. Details of the dataset construction are given in supplementary materials. We
present plots in Figure 4 showing the evolution of runtimes depending on each of the architectural
parameters of the networks.
Comparative evaluation of veriﬁcation approaches — In Figure 10, on the shallow networks of
CollisionDetection, most solvers succeed against all properties in about 10s. In particular, the SMT
inspired solvers Planet, Reluplex and the MIP solver are extremely fast. The BlackBox solver
doesn’t reach 100% accuracy due to producing incorrect counterexamples. Additional analysis can
be found about the cause of those errors in the supplementary materials.
On the deeper networks of ACAS, in Figure 10b, no errors are observed but most methods timeout
on the most challenging testcases. The best baseline is Reluplex, who reaches 79.26% success rate
at the two hour timeout, while our best method, BaBSB, already achieves 98.40% with a budget of
one hour. To reach Reluplex’s success rate, the required runtime is two orders of magnitude smaller.

7

0

10−1

100
Computation time (in s)

102

101

103

0

0

2000
Computation time (in s)

4000

6000

CollisionDetection

(b) ACAS Dataset

(a)
Dataset

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

Method

BaBSB
BaB
reluBaB

reluplex

MIPplanet

planet

Average
time
per Node

1.81s
2.11s
1.69s

0.30s

0.017s

1.5e-3s

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

Table 1: Average time to
explore a node for each
method.

0

100

101

102

103
Number of Nodes visited

104

105

106

(a) Properties solved for a given
number of nodes
to explore
(log scale).

Figure 2: Proportion of properties veri-
ﬁable for varying time budgets depending
on the methods employed. A higher curve
means that for the same time budget, more
properties will be solvable. All methods
solve CollisionDetection quite quickly ex-
cept reluBaB, which is much slower and
BlackBox who produces several incorrect
counterexamples.

Figure 3: The trade-off taken by the various
methods are different. Figure 3a shows how
many subdomains needs to be explored be-
fore verifying properties while Table 1 shows
the average time cost of exploring each sub-
domain. Our methods have a higher cost
per node but they require signiﬁcantly less
branching, thanks to better bounding. Note
also that between BaBSB and BaB, the
smart branching reduces by an order of mag-
nitude the number of nodes to visit.

Impact of each improvement —To identify which changes allow our method to have good perfor-
mance, we perform an ablation study and study the impact of removing some components of our
methods. The only difference between BaBSB and BaB is the smart branching, which represents a
signiﬁcant part of the performance gap.
Branching over the inputs rather than over the activations does not contribute much, as shown by
the small difference between BaB and reluBaB. Note however that we are able to use the fast
methods of Kolter & Wong [13] for the smart branching because branching over the inputs makes
the bounding problem similar to the one solved in robust optimization. Even if it doesn’t improve
performance by itself, the new type of split enables the use of smart branching.
The rest of the performance gap can be attributed to using a better bounds: reluBaB signiﬁcantly
outperforms planet while using the same branching strategy and the same convex relaxations. The
improvement comes from the beneﬁts of rebuilding the approximation at each step shown in Fig-
ure 1.
Figure 3 presents some additional analysis on a 20-property subset of the ACAS dataset, showing
how the methods used impact the need for branching. Smart branching and the use of better lower
bounds reduce heavily the number of subdomains to explore.

Timeout

Timeout

Timeout

Timeout

)
.
s
 
n
i
(
 
g
n
m
T

i

i

104

103

102

101

100

10−1

10−2

104

103

102

101

100

)
.
s
 
n
i
(
 
g
n
m
T

i

i

10−1

10−2

101

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

104

103

102

101

100

10−1

)
.
s
 
n
i
(
 
g
n
m
T

i

i

SAT / False

UNSAT / True

101

102

103

Number of Inputs

Layer Width

102

−104−103−102−101−1000100 101 102 103 104
Satisfiability margin

(a) Number of inputs

(b) Layer width

(c) Margin

10−2

2 × 100

3 × 100 4 × 100
Layer Depth

6 × 100

(d) Network depth

Figure 4:
Impact of the various parameters over the runtimes of the different solvers. The base network has
10 inputs and 4 layers of 25 hidden units, and the property to prove is True with a margin of 1000. Each of the
plot correspond to a variation of one of this parameters.

In the graphs of Figure 4, the trend for all the methods are similar, which seems to indicate that
hard properties are intrinsically hard and not just hard for a speciﬁc solver. Figure 4a shows an
expected trend: the largest the number of inputs, the harder the problem is. Similarly, Figure 4b
shows unsurprisingly that wider networks require more time to solve, which can be explained by
the fact that they have more non-linearities. The impact of the margin, as shown in Figure 4c is

)
.
s
 
n
i
(
 
g
n
m
T

i

i

104

103

102

101

100

10−1

10−2

8

also clear. Properties that are True or False with large satisﬁability margin are easy to prove, while
properties that have small satisﬁability margins are signiﬁcantly harder.

7 Conclusion
The improvement of formal veriﬁcation of Neural Networks represents an important challenge to be
tackled before learned models can be used in safety critical applications. By providing both a uniﬁed
framework to reason about methods and a set of empirical benchmarks to measure performance with,
we hope to contribute to progress in this direction. Our analysis of published algorithms through the
lens of Branch and Bound optimization has already resulted in signiﬁcant improvements in runtime
on our benchmarks. Its continued analysis should reveal even more efﬁcient algorithms in the future.

9

References
[1] Barrett, Clark, Nieuwenhuis, Robert, Oliveras, Albert, and Tinelli, Cesare. Splitting on de-
mand in sat modulo theories. International Conference on Logic for Programming Artiﬁcial
Intelligence and Reasoning, 2006.

[2] Bastani, Osbert, Ioannou, Yani, Lampropoulos, Leonidas, Vytiniotis, Dimitrios, Nori, Aditya,

and Criminisi, Antonio. Measuring neural net robustness with constraints. NIPS, 2016.

[3] Buxton, John N and Randell, Brian. Software Engineering Techniques: Report on a Conference

Sponsored by the NATO Science Committee. NATO Science Committee, 1970.

[4] Cheng, Chih-Hong, Nührenberg, Georg, and Ruess, Harald. Maximum resilience of artiﬁcial

neural networks. Automated Technology for Veriﬁcation and Analysis, 2017.

[5] Cheng, Chih-Hong, Nührenberg, Georg, and Ruess, Harald. Veriﬁcation of binarized neural

networks. arXiv:1710.03107, 2017.

[6] Ehlers, Ruediger. Formal veriﬁcation of piece-wise linear feed-forward neural networks. Au-

tomated Technology for Veriﬁcation and Analysis, 2017.

[7] Ehlers, Ruediger. Planet. https://github.com/progirep/planet, 2017.

[8] Hein, Matthias and Andriushchenko, Maksym. Formal guarantees on the robustness of a clas-

siﬁer against adversarial manipulation. NIPS, 2017.

[9] Hickey, Timothy, Ju, Qun, and Van Emden, Maarten H. Interval arithmetic: From principles

to implementation. Journal of the ACM (JACM), 2001.

[10] Huang, Xiaowei, Kwiatkowska, Marta, Wang, Sen, and Wu, Min. Safety veriﬁcation of deep

neural networks. International Conference on Computer Aided Veriﬁcation, 2017.

[11] Katz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and Kochenderfer, Mykel. Reluplex: An

efﬁcient smt solver for verifying deep neural networks. CAV, 2017.

[12] Katz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and Kochenderfer, Mykel. Reluplex.

https://github.com/guykatzz/ReluplexCav2017, 2017.

[13] Kolter, Zico and Wong, Eric. Provable defenses against adversarial examples via the convex

outer adversarial polytope. arXiv:1711.00851, 2017.

[14] Lomuscio, Alessio and Maganti, Lalit. An approach to reachability analysis for feed-forward

relu neural networks. arXiv:1706.07351, 2017.

[15] Marques-Silva, João P and Sakallah, Karem A. Grasp: A search algorithm for propositional

satisﬁability. IEEE Transactions on Computers, 1999.

[16] Narodytska, Nina, Kasiviswanathan, Shiva Prasad, Ryzhyk, Leonid, Sagiv, Mooly, and Walsh,
Toby. Verifying properties of binarized deep neural networks. arXiv:1709.06662, 2017.

[17] Pulina, Luca and Tacchella, Armando. An abstraction-reﬁnement approach to veriﬁcation of

artiﬁcial neural networks. CAV, 2010.

[18] Sherali, Hanif D and Adams, Warren P. A hierarchy of relaxations and convex hull character-
izations for mixed-integer zero—one programming problems. Discrete Applied Mathematics,
1994.

[19] Tjeng, Vincent and Tedrake, Russ. Verifying neural networks with mixed integer programming.

arXiv:1711.07356, 2017.

[20] Xiang, Weiming, Tran, Hoang-Dung, and Johnson, Taylor T. Output reachable set estimation

and veriﬁcation for multi-layer neural networks. arXiv:1708.03322, 2017.

[21] Zakrzewski, Radosiaw R. Veriﬁcation of a trained neural network accuracy. IJCNN, 2001.

10

A Canonical Form of the veriﬁcation problem
If the property is a simple inequality P (ˆxn) , cT ˆxn ≥ b, it is sufﬁcient to add to the network a ﬁnal
fully connected layer with one output, with weight of c and a bias of −b. If the global minimum
of this network is positive, it indicates that for all ˆxn the original network can output, we have
cT ˆxn − b ≥ 0 =⇒ cT ˆxn ≥ b, and as a consequence the property is True. On the other hand, if the
global minimum is negative, then the minimizer provides a counter-example.
Clauses speciﬁed using OR (denoted by
property is P (ˆxn) ,
Clauses speciﬁed using AND (denoted by
is equivalent to mini (cid:0)cT

) can be encoded by using a MaxPooling unit. If the
ˆxn − bi

) can be encoded similarly:
i ˆxn − bi(cid:1) ≥ 0 ⇐⇒ − (cid:0)maxi (cid:0)−cT

Vi (cid:2)
(cid:3)
B Toy Problem example
We have speciﬁed the problem of formal veriﬁcation of neural networks as follows: given a network
that implements a function ˆxn = f (x0), a bounded input domain C and a property P , we want to
prove that

, this is equivalent to maxi
(cid:3)

(cid:1)
the property P (ˆxn) =

i ˆxn + bi(cid:1)(cid:1) ≥ 0

ˆxn ≥ bi

ˆxn ≥ bi

cT
i
(cid:0)

Wi (cid:2)

≥ 0.

cT
i

cT
i

W

V

x0 ∈ C,

ˆxn = f (x0) =⇒ P (ˆxn).

(6)

A toy-example of the Neural Network veriﬁcation problem is given in Figure 5. On the domain
C = [−2; 2] × [−2; 2], we want to prove that the output y of the one hidden-layer network always
satisfy the property P (y) , [y > −5]. We will use this as a running example to explain the methods
used for comparison in our experiments.

[-2, 2]

x1

[-2, 2]

x2

a

1

1

-1

-1

b

y

-1

-1

Prove that y > −5

Figure 5: Example Neural Network. We attempt to prove the property that the network output is
always greater than -5

B.1 Problem formulation
For the network of Figure 5, the variables would be {x1, x2, ain, aout, bin, bout, y} and the set of
constraints would be:

− 2 ≤ x1 ≤ 2

ˆa = x1 + x2
y = −a − b

a = max(ˆa, 0)
y ≤ −5

−2 ≤ x2 ≤ 2
ˆb = −x1 − x2

b = max(ˆb, 0)

Here, ˆa is the input value to hidden unit, while a is the value after the ReLU. Any point satisfying all
the above constraints would be a counterexample to the property, as it would imply that it is possible
to drive the output to -5 or less.
B.2 MIP formulation
In our example, the non-linearities of equation (7c) would be replaced by

(7a)

(7b)

(7c)
(7d)

(8)

a ≥ 0
a ≤ ˆa − la(1 − δa)

a ≥ ˆa
a ≤ uaδa

δa ∈ {0, 1}

11

Step

x1

x2

ˆa

a

0

0
0

0
0

-2

ˆb
0

b

0

4
4

4
4

4

0

0
0

0
0
Fix linear constraints
0
1
0
0
1
0
Fix a ReLU
4
1
0
0
4
1
Fix linear constraints
1
-2
. . .

0
0

-4

4

y

0

-5
-5

-5
-5

-5

1

2

3

Figure 6: Evolution of the Reluplex algorithm. Red cells corresponds to value violating Linear con-
straints, and orange cells corresponds to value violating ReLU constraints. Resolution of violation
of linear constraints are prioritised.

where la is a lower bound of the value that ˆa can take (such as -4) and ua is an upper bound (such
as 4). The binary variable δa indicates which phase the ReLU is in: if δa = 0, the ReLU is blocked
and aout = 0, else the ReLU is passing and aout = ain. The problem remains difﬁcult due to the
integrality constraint on δa.
B.3 Running Reluplex
Table 6 shows the initial steps of a run of the Reluplex algorithm on the example of Figure 5. Starting
from an initial assignment, it attempts to ﬁx some violated constraints at each step. It prioritises
ﬁxing linear constraints ((7a), (7b) and (7d) in our illustrative example) using a simplex algorithm,
even if it leads to violated ReLU constraints (7c). This can be seen in step 1 and 3 of the process.
If no solution to the problem containing only linear constraints exists, this shows that the counterex-
ample search is unsatisﬁable. Otherwise, all linear constraints are ﬁxed and Reluplex attempts to ﬁx
one violated ReLU at a time, such as in step 2 of Table 6 (ﬁxing the ReLU b), potentially leading
to newly violated linear constraints. In the case where no violated ReLU exists, this means that a
satisﬁable assignment has been found and that the search can be interrupted.
This process is not guaranteed to converge, so to guarantee progress, non-linearities getting ﬁxed too
often are split into two cases. This generates two new sub-problems, each involving an additional
linear constraint instead of the linear one. The ﬁrst one solves the problem where ˆa ≤ 0 and a = 0,
the second one where ˆa ≥ 0 and a = ˆa. In the worst setting, the problem will be split completely
over all possible combinations of activation patterns, at which point the sub-problems are simple
LPs.

C Planet approximation
The feasible set of the Mixed Integer Programming formulation is given by the following set of
equations. We assume that all li are negative and ui are positive. In case this isn’t true, it is possible
to just update the bounds such that they are.

l0 ≤ x0 ≤ u0
ˆxi+1 = Wi+ixi + bi+i
xi ≥ 0
xi ≥ ˆxi
xi ≤ ˆxi − li · (1 − δi)
xi ≤ ui · δi
δi ∈ {0, 1}hi
ˆxn ≤ 0

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}

∀i ∈ {1, n − 1}

(9a)
(9b)
(9c)
(9d)
(9e)
(9f)

(9g)
(9h)

The level 0 of the Sherali-Adams hierarchy of relaxation Sherali & Adams [18] doesn’t include any
additional constraints. Indeed, polynomials of degree 0 are simply constants and their multiplication
with existing constraints followed by linearization therefore doesn’t add any new constraints. As a

12

xi[j]

li[j]

ˆxi[j]

ui[j]

Figure 7: Feasible domain corresponding to the Planet relaxation for a single ReLU.

result, the feasible domain given by the level 0 of the relaxation corresponds simply to the removal
of the integrality constraints:

l0 ≤ x0 ≤ u0
ˆxi+1 = Wi+ixi + bi+i
xi ≥ 0
xi ≥ ˆxi
xi ≤ ˆxi − li · (1 − di)
xi ≤ ui · di
0 ≤ di ≤ 1
ˆxn ≤ 0

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}

Combining the equations (10e) and (10f), looking at a single unit j in layer i, we obtain:

(11)
The function mapping di[j] to an upperbound of xi[j] is a minimum of linear functions, which means
that it is a concave function. As one of them is increasing and the other is decreasing, the maximum
will be reached when they are both equals.

ˆxi[j] − li(1 − di[j]), ui[j]di[j](cid:1)
(cid:0)

xi[j] ≤ min

Plugging this equation for d⋆ into Equation(11) gives that:

ˆxi[j] − li[j](1 − d⋆

i[j]) = ui[j]d⋆
i[j]
ˆxi[j] − li[j]
ui[j] − li[j]

d⋆
i[j] =

⇔

xi[j] ≤ ui[j]

ˆxi[j] − li[j]
ui[j] − li[j]

which corresponds to the upper bound of xi[j] introduced for Planet [6].
D MaxPooling
For space reason, we only described the case of ReLU activation function in the main paper. We
now present how to handle MaxPooling activation, either by converting them to the already handled
case of ReLU activations or by introducing an explicit encoding of them when appropriate.
D.1 Mixed Integer Programming
Similarly to the encoding of ReLU constraints using binary variables and bounds on the inputs, it is
possible to similarly encode MaxPooling constraints. The constraint

can be replaced by

y = max (x1, . . . , xk)

y ≥ xi
y ≤ xi + (ux1:k − lxi)(1 − δi)

δi = 1

X
i∈{1...k}
δi ∈ {0, 1}

∀i ∈ {1 . . . k}
∀i ∈ {1 . . . k}

∀i ∈ {1 . . . k}

where ux1:k is an upper-bound on all xi for i ∈ {1 . . . k} and lxi is a lower bound on xi.

13

(10a)
(10b)
(10c)
(10d)
(10e)
(10f)
(10g)

(10h)

(12)

(13)

(14)

(15a)
(15b)

(15c)

(15d)

(16)

(17)

(18)

(19a)

(19b)

D.2 Reluplex
In the version introduced by [11], there is no support for MaxPooling units. As the canonical repre-
sentation we evaluate needs them, we provide a way of encoding a MaxPooling unit as a combination
of Linear function and ReLUs.
To do so, we decompose the element-wise maximum into a series of pairwise maximum

max (xj , x2, x3, x4) = max( max (x1, x2) ,
max (x3, x4))

and decompose the pairwise maximums as sum of ReLUs:

max (x1, x2) = max (x1 − x2, 0) + max (x2 − lx2, 0) + lx2,

where lx2 is a pre-computed lower bound of the value that x2 can take.
As a result, we have seen that an elementwise maximum such as a MaxPooling unit can be decom-
posed as a series of pairwise maximum, which can themselves be decomposed into a sum of ReLUs
units. The only requirement is to be able to compute a lower bound on the input to the ReLU, for
which the methods discussed in the paper can help.
D.3 Planet
As opposed to Reluplex, Planet Ehlers [6] directly supports MaxPooling units. The SMT solver
driving the search can split either on ReLUs, by considering separately the case of the ReLU be-
ing passing or blocking. It also has the possibility on splitting on MaxPooling units, by treating
separately each possible choice of units being the largest one.
For the computation of lower bounds, the constraint

is replaced by the set of constraints:

y = max (x1, x2, x3, x4)

y ≥ xi

∀i ∈ {1 . . . 4}

y ≤

(xi − lxi) + max

lxi,

i

X
i

where xi are the inputs to the MaxPooling unit and lxi their lower bounds.
E Mixed Integers Variants
E.1 Encoding
Several variants of encoding are available to use Mixed Integer Programming as a solver for Neural
Network Veriﬁcation. As a reminder, in the main paper we used the formulation of Tjeng & Tedrake
[19]:

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

xi ≤ ui · δi
xi ≤ ˆxi − li · (1 − δi)

(20a)
(20b)

An alternative formulation is the one of Lomuscio & Maganti [14] and Cheng et al. [4]:

xi ≤ Mi · δi
xi ≤ ˆxi − Mi · (1 − δi)

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

(21a)
(21b)
where Mi = max (−li, ui). This is fundamentally the same encoding but with a sligthly worse
bounds that is used, as one of the side of the bounds isn’t as tight as it could be.
E.2 Obtaining bounds
The formulation described in Equations (20) and (21) are dependant on obtaining lower and upper
bounds for the value of the activation of the network.
Interval AnalysisOne way to obtain them, mentionned in the paper, is the use of interval arith-
metic [9]. If we have bounds li, ui for a vector xi, we can derive the bounds ˆli+1, ˆui+1 for a vector
ˆxi+1 = Wi+1xi + bi+1

ˆli+1[j] =

(cid:16)W +

i+1[j,k]l+

i[k] + W −

i+1[j,k]u+

i[k](cid:17) + bi+1[j]

ˆui+1[j] =

(cid:16)W +

i+1[j,k]u+

i[k] + W −

i+1[j,k]l+

i[k](cid:17) + bi+1[j]

X
k

X
k

(22a)

(22b)

with the notation a+ = max(a, 0) and a− = min(a, 0). Propagating the bounds through a ReLU
activation is simply equivalent to applying the ReLU to the bounds.

14

Planet Linear approximationAn alternative way to obtain bounds is to use the relaxation of Planet.
This is the methods that was employed in the paper: we build incrementally the network approxi-
mation, layer by layer. To obtain the bounds over an activation, we optimize its value subject to the
constraints of the relaxation.
Given that this is a convex problem, we will achieve the optimum. Given that it is a relaxation, the
optimum will be a valid bound for the activation (given that the feasible domain of the relaxation
includes the feasible domains subject to the original constraints).
Once this value is obtained, we can use it to build the relaxation for the following layers. We can
build the linear approximation for the whole network and extract the bounds for each activation to
use in the encoding of the MIP. While obtaining the bounds in this manner is more expensive than
simply doing interval analysis, the obtained bounds are better.
E.3 Objective function
In the paper, we have formalised the veriﬁcation problem as a satisﬁability problem, equating the
existence of a counterexample with the feasibility of the output of a (potentially modiﬁed) network
being negative.
In practice, it is beneﬁcial to not simply formulate it as a feasibility problem but as an optimization
problem where the output of the network is explicitly minimized.
E.4 Comparison
We present here a comparison on CollisionDetection and ACAS of the different variants.

1. Planet-feasible uses the encoding of Equation (20), with bounds obtained based on the

planet relaxation, and solve the problem simply as a satisﬁability problem.

2. Interval is the same as Planet-feasible, except that the bounds used are obtained by interval

analysis rather than with the Planet relaxation.

3. Planet-symfeasible is the same as Planet-feasible, except that the encoding is the one of

Equation (21).

4. Planet-opt is the same as Planet-feasible, except that the problem is solved as an opti-
mization problem. The MIP solver attempt to ﬁnd the global minimum of the output of
the network. Using Gurobi’s callback, if a feasible solution is found with a negative value,
the optimization is interrupted and the current solution is returned. This corresponds to the
version that is reported in the main paper.

interval
planetsym-feasible
planet-feasible
planet-opt

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

interval
planetsym-feasible
planet-feasible
planet-opt

100

101

102

103

Computation time (in s)

(a) CollisionDetection Dataset

0
100

101
102
Computation time (in s)

103

(b) ACAS Dataset

Figure 8: Comparison between the different variants of MIP formulation for Neural Network veriﬁcation.

The ﬁrst observation that can be made is that when we look at the CollisionDetection dataset in
Figure 8a, only Planet-opt solves the dataset to 100% accuracy. The reason why the other methods
don’t reach it is not because of timeout but because they return spurious counterexamples. As
they encode only satisﬁability problem, they terminate as soon as they identify a solution with a
value of zero. Due to the large constants involved in the big-M, those solutions are sometimes not
actually valid counterexamples. This is the same reason why the BlackBox solver doesn’t reach
100% accuracy in the experiments reported in the main paper.

15

The other results that we can observe is the impact of the quality of the bounds when the networks get
deeper, and the problem becomes therefore more complex, such as in the ACAS dataset. Interval
has the worst bounds and is much slower than the other methods. Planetsym-feasible, with its
slightly worse bounds, performs worse than Planet-feasible and Planet-opt.

F Experimental setup details
We provide in Table 2 the characteristics of all of the datasets used for the experimental comparison
in the main paper.

Data set

Count

Collision
Detection

500

ACAS

188

PCAMNIST

27

Model Architecture
6 inputs
40 hidden unit layer, MaxPool
19 hidden unit layer
2 outputs
5 inputs
6 layers of 50 hidden units
5 outputs
10 or {5, 10, 25, 100, 500, 784} inputs
4 or {2, 3, 4, 5, 6, 7} layers
of 25 or {10, 15, 25, 50, 100} hidden units,
1 output, with a margin of +1000 or
{-1e4, -1000, -100, -50, -10, -1 ,1, 10, 50, 100, 1000, 1e4}

Table 2: Dimensions of all the data sets. For PCAMNIST, we use a base network with 10 inputs,
4 layers of 25 hidden units and a margin of 1000. We generate new problems by changing one
parameter at a time, using the values inside the brackets.

G Additional performance details
Given that there is a signiﬁcant difference in the way veriﬁcation works for SAT problems vs. UN-
SAT problems, we report also comparison results on the subset of data sorted by decision type.

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

0

10−1

101

102

100
Computation time (in s)
(a) On SAT properties

103

0

10−1

101

100
Computation time (in s)
(b) On UNSAT properties

102

103

Figure 9: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed on the CollisionDetection dataset. We can identify that all the errors that BlackBox makes
are on SAT properties, as it returns incorrect counterexamples.

H PCAMNIST details
PCAMNIST is a novel data set that we introduce to get a better understanding of what factors in-
ﬂuence the performance of various methods. It is generated in a way to give control over different

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

16

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

0

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

0

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

4000

2000
Computation time (in s)
(a) On SAT properties

6000

4000

2000
Computation time (in s)
(b) On UNSAT properties

6000

Figure 10: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed on the ACAS dataset. We observe that planet doesn’t succeed in solving any of the SAT
properties, while our proposed methods are extremely efﬁcient at it, even if there remains some
properties that they can’t solve.

architecture parameters. The networks takes k features as input, corresponding to the ﬁrst k eigen-
vectors of a Principal Component Analysis decomposition of the digits from the MNIST data set.
We also vary the depth (number of layers), width (number of hidden unit in each layer) of the net-
works. We train a different network for each combination of parameters on the task of predicting the
parity of the presented digit. This results in the accuracies reported in Table 3.
The properties that we attempt to verify are whether there exists an input for which the score assigned
to the odd class is greater than the score of the even class plus a large conﬁdence. By tweaking the
value of the conﬁdence in the properties, we can make the property either True or False, and we
can choose by how much is it true. This gives us the possibility of tweaking the “margin”, which
represent a good measure of difﬁculty of a network.
In addition to the impact of each factors separately as was shown in the main paper, we can also
look at it as a generic dataset and plot the cactus plots like for the other datasets. This can be found
in Figure 11

Network Parameter

Accuracy

Nb inputs Width Depth

Train

Test

5
10
25
100
500
784

10
10
10
10
10

25
25
25
25
25
25

10
15
25
50
100

88.18% 87.3%
97.42% 96.09%
99.87% 98.69%
100% 98.77%
100% 98.84%
100% 98.64%

96.34% 95.75%
96.31% 95.81%
97.42% 96.09%
97.35% 96.0%
97.72% 95.75%

2
3
4
5
6
7
Table 3: Accuracies of the network trained for the PCAMNIST dataset.

96.45% 95.71%
96.98% 96.05%
97.42% 96.09%
96.78% 95.9%
95.48% 95.2%
96.81% 96.07%

10
10
10
10
10
10

25
25
25
25
25
25

4
4
4
4
4
4

4
4
4
4
4

17

0

10−1

100
Computation time (in s)

102

101

103

Figure 11: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed. The PCAMNIST dataset is challenging as None of the methods reaches more than 50%
success rate.

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

18

8
1
0
2
 
y
a
M
 
2
2
 
 
]
I

A
.
s
c
[
 
 
3
v
5
5
4
0
0
.
1
1
7
1
:
v
i
X
r
a

A Uniﬁed View of Piecewise Linear Neural Network Veriﬁcation

Rudy Bunel
University of Oxford
rudy@robots.ox.ac.uk

Ilker Turkaslan
University of Oxford
ilker.turkaslan@lmh.ox.ac.uk

Philip H.S. Torr
University of Oxford
philip.torr@eng.ox.ac.uk

Pushmeet Kohli
Deepmind
pushmeet@google.com

M. Pawan Kumar
University of Oxford
Alan Turing Institute
pawan@robots.ox.ac.uk

Abstract

The success of Deep Learning and its potential use in many safety-critical applica-
tions has motivated research on formal veriﬁcation of Neural Network (NN) mod-
els. Despite the reputation of learned NN models to behave as black boxes and the
theoretical hardness of proving their properties, researchers have been successful
in verifying some classes of models by exploiting their piecewise linear structure
and taking insights from formal methods such as Satisiﬁability Modulo Theory.
These methods are however still far from scaling to realistic neural networks. To
facilitate progress on this crucial area, we make two key contributions. First, we
present a uniﬁed framework that encompasses previous methods. This analysis
results in the identiﬁcation of new methods that combine the strengths of mul-
tiple existing approaches, accomplishing a speedup of two orders of magnitude
compared to the previous state of the art. Second, we propose a new data set of
benchmarks which includes a collection of previously released testcases. We use
the benchmark to provide the ﬁrst experimental comparison of existing algorithms
and identify the factors impacting the hardness of veriﬁcation problems.

1 Introduction
Despite their success in a wide variety of applications, Deep Neural Networks have seen limited
adoption in safety-critical settings. The main explanation for this lies in their reputation for being
black-boxes whose behaviour can not be predicted. Current approaches to evaluate trained models
mostly rely on testing using held-out data sets. However, as Edsger W. Dijkstra said [3], “testing
shows the presence, not the absence of bugs”. If deep learning models are to be deployed to applica-
tions such as autonomous driving cars, we need to be able to verify safety-critical behaviours.
To this end, some researchers have tried to use formal methods. To the best of our knowledge,
Zakrzewski [21] was the ﬁrst to propose a method to verify simple, one hidden layer neural networks.
However, only recently were researchers able to work with non-trivial models by taking advantage
of the structure of ReLU-based networks [5, 11]. Even then, these works are not scalable to the large
networks encountered in most real world problems.
This paper advances the ﬁeld of NN veriﬁcation by making the following key contributions:

1. We reframe state of the art veriﬁcation methods as special cases of Branch-and-Bound

optimization, which provides us with a uniﬁed framework to compare them.

2. We gather a data set of test cases based on the existing literature and extend it with new
benchmarks. We provide the ﬁrst experimental comparison of veriﬁcation methods.

3. Based on this framework, we identify algorithmic improvements in the veriﬁcation process,
speciﬁcally in the way bounds are computed, the type of branching that are considered, as

Preprint. Work in progress.

well as the strategies guiding the branching. Compared to the previous state of the art, these
improvements lead to speed-up of almost two orders of magnitudes.

Section 2 and 3 give the speciﬁcation of the problem and formalise the veriﬁcation process. Section
4 presents our uniﬁed framework, showing that previous methods are special cases and highlight-
ing potential improvements. Section 5 presents our experimental setup and Section 6 analyses the
results.
2 Problem speciﬁcation
We now specify the problem of formal veriﬁcation of neural networks. Given a network that imple-
ments a function ˆxn = f (x0), a bounded input domain C and a property P , we want to prove

(cid:8)

∀y

x0 ∈ C,

ˆxn = f (x0) =⇒ P (ˆxn).

ˆxn[ya] ≥ ˆxn[y](cid:9)
.

(1)
For example,
the property of robustness to adversarial examples in L∞ norm around a
training sample a with label ya would be encoded by using C , {x0| kx0 − ak∞ ≤ ǫ} and
P (ˆxn) =
In this paper, we are going to focus on Piecewise-Linear Neural Networks (PL-NN), that is, networks
for which we can decompose C into a set of polyhedra Ci such that C = ∪i Ci, and the restriction
of f to Ci is a linear function for each i. While this prevents us from including networks that
use activation functions such as sigmoid or tanh, PL-NNs allow the use of linear transformations
such as fully-connected or convolutional layers, pooling units such as MaxPooling and activation
functions such as ReLUs.
In other words, PL-NNs represent the majority of networks used in
practice. Operations such as Batch-Normalization or Dropout also preserve piecewise linearity at
test-time.
The properties that we are going to consider are Boolean formulas over linear inequalities. In our
robustness to adversarial example above, the property is a conjunction of linear inequalities, each of
which constrains the output of the original label to be greater than the output of another label.
The scope of this paper does not include approaches relying on additional assumptions such as twice
differentiability of the network [8, 21], limitation of the activation to binary values [5, 16] or restric-
tion to a single linear domain [2]. Since they do not provide formal guarantees, we also don’t include
approximate approaches relying on a limited set of perturbation [10] or on over-approximation meth-
ods that potentially lead to undecidable properties [17, 20].
3 Veriﬁcation Formalism
3.1 Veriﬁcation as a Satisﬁability problem
The methods we involve in our comparison all leverage the piecewise-linear structure of PL-NN
to make the problem more tractable. They all follow the same general principle: given a property
to prove, they attempt to discover a counterexample that would make the property false. This is
accomplished by deﬁning a set of variables corresponding to the inputs, hidden units and output of
the network, and the set of constraints that a counterexample would satisfy.
To help design a uniﬁed framework, we reduce all instances of veriﬁcation problems to a canoni-
cal representation. Speciﬁcally, the whole satisﬁability problem will be transformed into a global
optimization problem where the decision will be obtained by checking the sign of the minimum.
If the property to verify is a simple inequality P (ˆxn) , cT ˆxn ≥ b, it is sufﬁcient to add to the
network a ﬁnal fully connected layer with one output, with weight of c and a bias of −b. If the
global minimum of this network is positive, it indicates that for all ˆxn the original network can
output, we have cT ˆxn − b ≥ 0 =⇒ cT ˆxn ≥ b, and as a consequence the property is True. On the
other hand, if the global minimum is negative, then the minimizer provides a counter-example. The
supplementary material shows that OR and AND clauses in the property can similarly be expressed as
additional layers, using MaxPooling units.
We can formulate any Boolean formula over linear inequalities on the output of the network
as a sequence of additional linear and max-pooling layers. The veriﬁcation problem will be
reduced to the problem of ﬁnding whether the scalar output of the potentially modiﬁed net-
work can reach a negative value. Assuming the network only contains ReLU activations be-
the satisﬁability problem to ﬁnd a counterexample can be expressed as:
tween each layer,

l0 ≤ x0 ≤ u0
ˆxn ≤ 0

(2a)
(2b)

ˆxi+1 = Wi+1xi + bi+1
xi = max (ˆxi, 0)

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}.

(2c)
(2d)

2

Eq. (10a) represents the constraints on the input and Eq. (10h) on the neural network output.
Eq. (10b) encodes the linear layers of the network and Eq. (2d) the ReLU activation functions. If an
assignment to all the values can be found, this represents a counterexample. If this problem is unsat-
isﬁable, no counterexample can exist, implying that the property is True. We emphasise that we are
required to prove that no counter-examples can exist, and not simply that none could be found.
While for clarity of explanation, we have limited ourselves to the speciﬁc case where only ReLU
activation functions are used, this is not restrictive. The supplementary material contains a section
detailing how each method speciﬁcally handles MaxPooling units, as well as how to convert any
MaxPooling operation into a combination of linear layers and ReLU activation functions.
The problem described in (2) is still a hard problem. The addition of the ReLU non-linearities (2d)
transforms a problem that would have been solvable by simple Linear Programming into an NP-hard
problem [11]. Converting a veriﬁcation problem into this canonical representation does not make
its resolution simpler but it does provide a formalism advantage. Speciﬁcally, it allows us to prove
complex properties, containing several OR clauses, with a single procedure rather than having to
decompose the desired property into separate queries as was done in previous work [11].
Operationally, a valid strategy is to impose the constraints (10a) to (2d) and minimise the value of
ˆxn. Finding the exact global minimum is not necessary for veriﬁcation. However, it provides a
measure of satisﬁability or unsatisﬁability. If the value of the global minimum is positive, it will
correspond to the margin by which the property is satisﬁed.
3.2 Mixed Integer Programming formulation
A possible way to eliminate the non-linearities is to encode them with the help of binary variables,
transforming the PL-NN veriﬁcation problem (2) into a Mixed Integer Linear Program (MIP). This
can be done with the use of “big-M” encoding. The following encoding is from Tjeng & Tedrake
[19]. Assuming we have access to lower and upper bounds on the values that can be taken by the
coordinates of ˆxi, which we denote li and ui, we can replace the non-linearities:

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

xi ≤ ui · δi
xi ≤ ˆxi − li · (1 − δi)

(3a)
(3b)

is

in Eq.

easy to verify that

δi[j] = 0 ⇔ xi[j] = 0 (replacing δi[j]

It
δi[j] = 1 ⇔ xi[j] = ˆxi[j] (replacing δi[j] in Eq. (21b)).
By taking advantage of the feed-forward structure of the neural network, lower and upper bounds
li and ui can be obtained by applying interval arithmetic [9] to propagate the bounds on the inputs,
one layer at a time.
Thanks to this speciﬁc feed-forward structure of the problem, the generic, non-linear, non-convex
problem has been rewritten into an MIP. Optimization of MIP is well studied and highly efﬁcient
off-the-shelf solvers exist. As solving them is NP-hard, performance is going to be dependent on
the quality of both the solver used and the encoding. We now ask the following question: how much
efﬁciency can be gained by using a bespoke solver rather than a generic one? In order to answer this,
we present specialised solvers for the PLNN veriﬁcation task.

(21a))

and

4 Branch-and-Bound for Veriﬁcation
As described in Section 3.1, the veriﬁcation problem can be rephrased as a global optimization
problem. Algorithms such as Stochastic Gradient Descent are not appropriate as they have no way of
guaranteeing whether or not a minima is global. In this section, we present an approach to estimate
the global minimum, based on the Branch and Bound paradigm and show that several published
methods, introduced as examples of Satisﬁability Modulo Theories, ﬁt this framework.
Algorithm 1 describes its generic form. The input domain is repeatedly split into sub-domains (line
7), over which lower and upper bounds of the minimum are computed (lines 9-10). The best upper-
bound found so far serves as a candidate for the global minimum. Any domain whose lower bound
is greater than the current global upper bound can be pruned away as it cannot contain the global
minimum (line 13, lines 15-17). By iteratively splitting the domains, it is possible to compute tighter
lower bounds. We keep track of the global lower bound on the minimum by taking the minimum
over the lower bounds of all sub-domains (line 19). When the global upper bound and the global
lower bound differ by less than a small scalar ǫ (line 5), we consider that we have converged.
Algorithm 1 shows how to optimise and obtain the global minimum. If all that we are interested in
is the satisﬁability problem, the procedure can be simpliﬁed by initialising the global upper bound
with 0 (in line 2). Any subdomain with a lower bound greater than 0 (and therefore not eligible to

3

contain a counterexample) will be pruned out (by line 15). The computation of the lower bound
can therefore be replaced by the feasibility problem (or its relaxation) imposing the constraint that
the output is below zero without changing the algorithm. If it is feasible, there might still be a
counterexample and further branching is necessary. If it is infeasible, the subdomain can be pruned
out. In addition, if any upper bound improving on 0 is found on a subdomain (line 11), it is possible
to stop the algorithm as this already indicates the presence of a counterexample.

global_ub ← inf
global_lb ← − inf
doms ← [(global_lb, domain)]
while global_ub − global_lb > ǫ do

Algorithm 1 Branch and Bound
1: function BAB(net, domain, ǫ)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end function

end while
return global_ub

(_ , dom) ← pick_out(doms)
[subdom_1, . . . , subdom_s] ← split(dom)
for i = 1 . . . s do

dom_ub ← compute_UB(net, subdom_i)
dom_lb ← compute_LB(net, subdom_i)
if dom_ub < global_ub then
global_ub ← dom_ub
prune_domains(doms, global_ub)

end if
if dom_lb < global_ub then

domains.append((dom_lb, subdom_i))

end if
end for
global_lb ← min{lb | (lb, dom) ∈ doms}

The description of the veriﬁcation problem
as optimization and the pseudo-code of Algo-
rithm 1 are generic and would apply to veriﬁ-
cation problems beyond the speciﬁc case of PL-
NN. To obtain a practical algorithm, it is neces-
sary to specify several elements.
A search strategy, deﬁned by the pick_out
function, which chooses the next domain to
branch on. Several heuristics are possible, for
example those based on the results of previous
bound computations. For satisﬁable problems
or optimization problems, this allows to dis-
cover good upper bounds, enabling early prun-
ing.
A branching rule, deﬁned by the split
function, which
takes
dom
subdomain
partition
return
and
a
such
that
that
Si subdom_i = dom and
(subdom_i ∩ subdom_j) = ∅, ∀i 6= j.
This
will deﬁne the “shape” of the domains, which
impacts the hardness of computing bounds.
In addition, choosing the right partition can

domain

in

a

greatly impact the quality of the resulting bounds.
Bounding methods, deﬁned by the compute_{UB, LB} functions. These procedures estimate re-
spectively upper bounds and lower bounds over the minimum output that the network net can reach
over a given input domain. We want the lower bound to be as high as possible, so that this whole
domain can be pruned easily. This is usually done by introducing convex relaxations of the problem
and minimising them. On the other hand, the computed upper bound should be as small as possi-
ble, so as to allow pruning out other regions of the space or discovering counterexamples. As any
feasible point corresponds to an upper bound on the minimum, heuristic methods are sufﬁcient.
We now demonstrate how some published work in the literature can be understood as special case
of the branch-and-bound framework for veriﬁcation.
4.1 Reluplex
Katz et al. [11] present a procedure named Reluplex to verify properties of Neural Network contain-
ing linear functions and ReLU activation unit, functioning as an SMT solver using the splitting-on-
demand framework [1]. The principle of Reluplex is to always maintain an assignment to all of the
variables, even if some of the constraints are violated.
Starting from an initial assignment, it attempts to ﬁx some violated constraints at each step.
It
prioritises ﬁxing linear constraints ((10a), (10b), (10h) and some relaxation of (2d)) using a simplex
algorithm, even if it leads to violated ReLU constraints.
If no solution to this relaxed problem
containing only linear constraints exists, the counterexample search is unsatisﬁable. Otherwise,
either all ReLU are respected, which generates a counterexample, or Reluplex attempts to ﬁx one
of the violated ReLU; potentially leading to newly violated linear constraints. This process is not
guaranteed to converge, so to make progress, non-linearities that get ﬁxed too often are split into two
cases. Two new problems are generated, each corresponding to one of the phases of the ReLU. In
the worst setting, the problem will be split completely over all possible combinations of activation
patterns, at which point the sub-problems will all be simple LPs.
This algorithm can be mapped to the special case of branch-and-bound for satisﬁability. The search
strategy is handled by the SMT core and to the best of our knowledge does not prioritise any domain.
The branching rule is implemented by the ReLU-splitting procedure: when neither the upper bound
search, nor the detection of infeasibility are successful, one non-linear constraint over the j-th neu-

4

(cid:1)

ˆxi[j], 0
(cid:0)

is split out into two subdomains: {xi[j] = 0, ˆxi[j] ≤ 0}
ron of the i-th layer xi[j] = max
and {xi[j] = ˆxi[j], ˆxi[j] ≥ 0}. This deﬁnes the type of subdomains produced. The prioritisation of
ReLUs that have been frequently ﬁxed is a heuristic to decide between possible partitions.
As Reluplex only deal with satisﬁability, the analogue of the lower bound computation is an over-
approximation of the satisﬁability problem. The bounding method used is a convex relaxation,
obtained by dropping some of the constraints. The following relaxation is applied to ReLU units for
which the sign of the input is unknown (li[j] ≤ 0 and ui[j] ≥ 0).

xi = max (ˆxi, 0) ⇒ xi ≥ ˆxi

(4a)

xi ≥ 0

(4b)

xi ≤ ui.

(4c)

If this relaxation is unsatisﬁable, this indicates that the subdomain cannot contain any counterex-
ample and can be pruned out. The search for an assignment satisfying all the ReLU constraints by
iteratively attempting to correct the violated ReLUs is a heuristic that is equivalent to the search for
an upper bound lower than 0: success implies the end of the procedure but no guarantees can be
given.

4.2 Planet
Ehlers [6] also proposed an approach based on SMT. Unlike Reluplex, the proposed tool, named
Planet, operates by explicitly attempting to ﬁnd an assignment to the phase of the non-linearities.
Reusing the notation of Section 3.2, it assigns a value of 0 or 1 to each δi[j] variable, verifying at
each step the feasibility of the partial assignment so as to prune infeasible partial assignment early.
As in Reluplex, the search strategy is not explicitly encoded and simply enumerates all the domains
that have not yet been pruned. The branching rule is the same as for Reluplex, as ﬁxing the decision
variable δi[j] = 0 is equivalent to choosing {xi[j] = 0, ˆxi[j] ≤ 0} and ﬁxing δi[j] = 1 is equivalent
to {xi[j] = ˆxi[j], ˆxi[j] ≥ 0} . Note however that Planet does not include any heuristic to prioritise
which decision variables should be split over.
Planet does not include a mechanism for early termination based on a heuristic search of a feasible
point. For satisﬁable problems, only when a full complete assignment is identiﬁed is a solution
returned. In order to detect incoherent assignments, Ehlers [6] introduces a global linear approxi-
mation to a neural network, which is used as a bounding method to over-approximate the set of
values that each hidden unit can take. In addition to the existing linear constraints ((10a), (10b)
and (10h)), the non-linear constraints are approximated by sets of linear constraints representing the
non-linearities’ convex hull. Speciﬁcally, ReLUs with input of unknown sign are replaced by the set
of equations:

ˆxi[j] − li[j]
ui[j] − li[j]

(5a)

(5b)

xi[j] ≤ ui[j]

xi = max (ˆxi, 0) ⇒ xi ≥ ˆxi

xi ≥ 0
(5c)
where xi[j] corresponds to the value of the j-th coordinate of xi. An illustration of the feasible
domain is provided in the supplementary material.
Compared with the relaxation of Reluplex (4), the Planet relaxation is tighter. Speciﬁcally, Eq. (4a)
and (4b) are identical to Eq. (5a) and (5b) but Eq. (5c) implies Eq. (4c). Indeed, given that ˆxi[j]
is smaller than ui[j], the fraction multiplying ui[j] is necessarily smaller than 1, implying that this
provides a tighter bounds on xi[j].
To use this approximation to compute better bounds than the ones given by simple interval arithmetic,
it is possible to leverage the feed-forward structure of the neural networks and obtain bounds one
layer at a time. Having included all the constraints up until the i-th layer, it is possible to optimize
over the resulting linear program and obtain bounds for all the units of the i-th layer, which in turn
will allow us to create the constraints (5) for the next layer.
In addition to the pruning obtained by the convex relaxation, both Planet and Reluplex make use of
conﬂict analysis [15] to discover combinations of splits that cannot lead to satisﬁable assignments,
allowing them to perform further pruning of the domains.

4.3 Potential improvements
As can be seen, previous approaches to neural network veriﬁcation have relied on methodologies
developed in three communities: optimization, for the creation of upper and lower bounds; veriﬁca-
tion, especially SMT; and machine learning, especially the feed-forward nature of neural networks
for the creation of relaxations. A natural question that arises is “Can other existing literature from
these domains be exploited to further improve neural network veriﬁcation?” Our uniﬁed branch-and-
bound formulation makes it easy to answer this question. To illustrate its power, we now provide a
non-exhaustive list of suggestions to speed-up veriﬁcation algorithms.

5

Better bounding —While the relaxation proposed by Ehlers [6] is tighter than the one used by
Reluplex, it can be improved further still. Speciﬁcally, after a splitting operation, on a smaller
domain, we can reﬁne all the li, ui bounds, to obtain a tither relaxation. We show the importance
of this in the experiments section with the BaB-relusplit method that performs splitting on the
activation like Planet but updates its approximation completely at each step.
One other possible area of improvement lies in the tightness of the bounds used. Equation (5) is
very closely related to the Mixed Integer Formulation of Equation (20). Indeed, it corresponds to
level 0 of the Sherali-Adams hierarchy of relaxations [18]. The proof for this statement can be
found in the supplementary material. Stronger relaxations could be obtained by exploring higher
levels of the hierarchy. This would jointly constrain groups of ReLUs, rather than linearising them
independently.

]

]

l0[i⋆

l0[i⋆

and

]+u0[i⋆

]+u0[i⋆

Better branchingThe decision to split on the activation of the ReLU non-linearities made by Planet
and Reluplex is intuitive as it provides a clear set of decision variables to ﬁx. However, it ignores
another natural branching strategy, namely, splitting the input domain. Indeed, it could be argued
that since the function encoded by the neural networks are piecewise linear in their input, this could
result in the computation of highly useful upper and lower bounds. To demonstrate this, we propose
the novel BaB-input algorithm: a branch-and-bound method that branches over the input features
of the network. Based on a domain with input constrained by Eq. (10a), the split function would
return two subdomains where bounds would be identical in all dimension except for the dimension
with the largest length, denoted i⋆. The bounds for each subdomain for dimension i⋆ are given by
l0[i⋆] ≤ x0[i⋆] ≤
≤ x0[i⋆] ≤ u0[i⋆]. Based on these tighter input bounds,
2
tighter bounds at all layers can be re-evaluated.
One of the main advantage of branching over the variables is that all subdomains generated by the
BaB algorithm when splitting over the input variables end up only having simple bound constraints
over the value that input variable can take. In order to exploit this property to the fullest, we use
the highly efﬁcient lower bound computation approach of Kolter & Wong [13]. This approach was
initially proposed in the context of robust optimization. However, our uniﬁed framework opens
the door for its use in veriﬁcation. Speciﬁcally, Kolter & Wong [13] identiﬁed an efﬁcient way of
computing bounds for the type of problems we encounter, by generating a feasible solution to the
dual of the LP generated by the Planet relaxation. While this bound is quite loose compared to
the one obtained through actual optimization, they are very fast to evaluate. We propose a smart
branching method BaBSB to replace the longest edge heuristic of BaB-input. For all possible splits,
we compute fast bounds for each of the resulting subdomain, and execute the split resulting in the
highest lower bound. The intuition is that despite their looseness, the fast bounds will still be useful
in identifying the promising splits.

2

5 Experimental setup
The problem of PL-NN veriﬁcation has been shown to be NP-complete [11]. Meaningful compari-
son between approaches therefore needs to be experimental.
5.1 Methods
The simplest baseline we refer to is BlackBox, a direct encoding of Eq. (2) into the Gurobi solver,
which will perform its own relaxation, without taking advantage of the problem’s structure.
For the SMT based methods, Reluplex and Planet, we use the publicly available versions [7, 12].
Both tools are implemented in C++ and relies on the GLPK library to solve their relaxation. We
wrote some software to convert in both directions between the input format of both solvers.
We also evaluate the potential of using MIP solvers, based on the formulation of Eq. (20). Due to the
lack of availability of open-sourced methods at the time of our experiments, we reimplemented the
approach in Python, using the Gurobi MIP solver. We report results for a variant called MIPplanet,
which uses bounds derived from Planet’s convex relaxation rather than simple interval arithmetic.
The MIP is not treated as a simple feasibility problem but is encoded to minimize the output ˆxn of
Equation (10h), with a callback interrupting the optimization as soon as a negative value is found.
Additional discussions on encodings of the MIP problem can be found in supplementary materials.
In our benchmark, we include the methods derived from our Branch and Bound analysis. Our
implementation follows faithfully Algorithm 1, is implemented in Python and uses Gurobi to solve
LPs. The pick_out strategy consists in prioritising the domain that currently has the smallest lower
bound. Upper bounds are generated by randomly sampling points on the considered domain, and
we use the convex approximation of Ehlers [6] to obtain lower bounds. As opposed to the approach

6

taken by Ehlers [6] of building a single approximation of the network, we rebuild the approximation
and recompute all bounds for each sub-domain. This is motivated by the observation shown in
Figure 1 which demonstrate the signiﬁcant improvements it brings, especially for deeper networks.
For split, BaB-input performs branching by splitting the input domain in half along its longest
edge and BaBSB does it by splitting the input domain along the dimension improving the global
lower bound the most according to the fast bounds of Kolter & Wong [13]. We also include results
for the BaB-relusplit variant, where the split method is based on the phase of the non-linearities,
similarly to Planet.

d
n
u
o
b
 
r
e
w
o
L

36.0

35.5

35.0

34.5

34.0

33.5

33.0

32.5

32.0

ReApproximating
FixedApproximation

103

102

101

d
n
u
o
b
 
r
e
w
o
L

100
0
−100

−101

−102

−103

−104

−105

ReApproximating
FixedApproximation

10−12

10−9

10−6

10−3

100

10−6 10−5 10−4 10−3 10−2 10−1 100

Relative area

Relative area

Figure 1: Quality of the linear approximation,
depending on the size of the input domain. We
plot the value of the lower bound as a function of
the area on which it is computed (higher is bet-
ter). The domains are centered around the global
minimum and repeatedly shrunk. Rebuilding com-
pletely the linear approximation at each step al-
lows to create tighter lower-bounds thanks to bet-
ter li and ui, as opposed to using the same con-
straints and only changing the bounds on input
variables. This effect is even more signiﬁcant on
deeper networks.

(b) Approximation on a
deep net from ACAS

(a) Approximation on a
CollisionDetection net
5.2 Evaluation Criteria
For each of the data sets, we compare the different methods using the same protocol. We attempt to
verify each property with a timeout of two hours, and a maximum allowed memory usage of 20GB,
on a single core of a machine with an i7-5930K CPU. We measure the time taken by the solvers
to either prove or disprove the property. If the property is false and the search problem is therefore
satisﬁable, we expect from the solver to exhibit a counterexample. If the returned input is not a valid
counterexample, we don’t count the property as successfully proven, even if the property is indeed
satisﬁable. All code and data necessary to replicate our analysis are released.

6 Analysis
We attempt to perform veriﬁcation on three data sets of properties and report the comparison results.
The dimensions of all the problems can be found in the supplementary material.
The CollisionDetection data set [6] attempts to predict whether two vehicles with parameterized
trajectories are going to collide. 500 properties are extracted from problems arising from a binary
search to identify the size of the region around training examples in which the prediction of the
network does not change. The network used is relatively shallow but due to the process used to
generate the properties, some lie extremely close between the decision boundary between SAT and
UNSAT. Results presented in Figure 10 therefore highlight the accuracy of methods.
The Airborne Collision Avoidance System (ACAS) data set, as released by Katz et al. [11] is a
neural network based advisory system recommending horizontal manoeuvres for an aircraft in order
to avoid collisions, based on sensor measurements. Each of the ﬁve possible manoeuvres is assigned
a score by the neural network and the action with the minimum score is chosen. The 188 properties
to verify are based on some speciﬁcation describing various scenarios. Due to the deeper network
involved, this data set is useful in highlighting the scalability of the various algorithms.
PCAMNIST is a novel data set that we introduce to get a better understanding of what factors
inﬂuence the performance of various methods. It is generated in a way to give control over different
architecture parameters. Details of the dataset construction are given in supplementary materials. We
present plots in Figure 4 showing the evolution of runtimes depending on each of the architectural
parameters of the networks.
Comparative evaluation of veriﬁcation approaches — In Figure 10, on the shallow networks of
CollisionDetection, most solvers succeed against all properties in about 10s. In particular, the SMT
inspired solvers Planet, Reluplex and the MIP solver are extremely fast. The BlackBox solver
doesn’t reach 100% accuracy due to producing incorrect counterexamples. Additional analysis can
be found about the cause of those errors in the supplementary materials.
On the deeper networks of ACAS, in Figure 10b, no errors are observed but most methods timeout
on the most challenging testcases. The best baseline is Reluplex, who reaches 79.26% success rate
at the two hour timeout, while our best method, BaBSB, already achieves 98.40% with a budget of
one hour. To reach Reluplex’s success rate, the required runtime is two orders of magnitude smaller.

7

0

10−1

100
Computation time (in s)

102

101

103

0

0

2000
Computation time (in s)

4000

6000

CollisionDetection

(b) ACAS Dataset

(a)
Dataset

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

Method

BaBSB
BaB
reluBaB

reluplex

MIPplanet

planet

Average
time
per Node

1.81s
2.11s
1.69s

0.30s

0.017s

1.5e-3s

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

Table 1: Average time to
explore a node for each
method.

0

100

101

102

103
Number of Nodes visited

104

105

106

(a) Properties solved for a given
number of nodes
to explore
(log scale).

Figure 2: Proportion of properties veri-
ﬁable for varying time budgets depending
on the methods employed. A higher curve
means that for the same time budget, more
properties will be solvable. All methods
solve CollisionDetection quite quickly ex-
cept reluBaB, which is much slower and
BlackBox who produces several incorrect
counterexamples.

Figure 3: The trade-off taken by the various
methods are different. Figure 3a shows how
many subdomains needs to be explored be-
fore verifying properties while Table 1 shows
the average time cost of exploring each sub-
domain. Our methods have a higher cost
per node but they require signiﬁcantly less
branching, thanks to better bounding. Note
also that between BaBSB and BaB, the
smart branching reduces by an order of mag-
nitude the number of nodes to visit.

Impact of each improvement —To identify which changes allow our method to have good perfor-
mance, we perform an ablation study and study the impact of removing some components of our
methods. The only difference between BaBSB and BaB is the smart branching, which represents a
signiﬁcant part of the performance gap.
Branching over the inputs rather than over the activations does not contribute much, as shown by
the small difference between BaB and reluBaB. Note however that we are able to use the fast
methods of Kolter & Wong [13] for the smart branching because branching over the inputs makes
the bounding problem similar to the one solved in robust optimization. Even if it doesn’t improve
performance by itself, the new type of split enables the use of smart branching.
The rest of the performance gap can be attributed to using a better bounds: reluBaB signiﬁcantly
outperforms planet while using the same branching strategy and the same convex relaxations. The
improvement comes from the beneﬁts of rebuilding the approximation at each step shown in Fig-
ure 1.
Figure 3 presents some additional analysis on a 20-property subset of the ACAS dataset, showing
how the methods used impact the need for branching. Smart branching and the use of better lower
bounds reduce heavily the number of subdomains to explore.

Timeout

Timeout

Timeout

Timeout

)
.
s
 
n
i
(
 
g
n
m
T

i

i

104

103

102

101

100

10−1

10−2

104

103

102

101

100

)
.
s
 
n
i
(
 
g
n
m
T

i

i

10−1

10−2

101

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

104

103

102

101

100

10−1

)
.
s
 
n
i
(
 
g
n
m
T

i

i

SAT / False

UNSAT / True

101

102

103

Number of Inputs

Layer Width

102

−104−103−102−101−1000100 101 102 103 104
Satisfiability margin

(a) Number of inputs

(b) Layer width

(c) Margin

10−2

2 × 100

3 × 100 4 × 100
Layer Depth

6 × 100

(d) Network depth

Figure 4:
Impact of the various parameters over the runtimes of the different solvers. The base network has
10 inputs and 4 layers of 25 hidden units, and the property to prove is True with a margin of 1000. Each of the
plot correspond to a variation of one of this parameters.

In the graphs of Figure 4, the trend for all the methods are similar, which seems to indicate that
hard properties are intrinsically hard and not just hard for a speciﬁc solver. Figure 4a shows an
expected trend: the largest the number of inputs, the harder the problem is. Similarly, Figure 4b
shows unsurprisingly that wider networks require more time to solve, which can be explained by
the fact that they have more non-linearities. The impact of the margin, as shown in Figure 4c is

)
.
s
 
n
i
(
 
g
n
m
T

i

i

104

103

102

101

100

10−1

10−2

8

also clear. Properties that are True or False with large satisﬁability margin are easy to prove, while
properties that have small satisﬁability margins are signiﬁcantly harder.

7 Conclusion
The improvement of formal veriﬁcation of Neural Networks represents an important challenge to be
tackled before learned models can be used in safety critical applications. By providing both a uniﬁed
framework to reason about methods and a set of empirical benchmarks to measure performance with,
we hope to contribute to progress in this direction. Our analysis of published algorithms through the
lens of Branch and Bound optimization has already resulted in signiﬁcant improvements in runtime
on our benchmarks. Its continued analysis should reveal even more efﬁcient algorithms in the future.

9

References
[1] Barrett, Clark, Nieuwenhuis, Robert, Oliveras, Albert, and Tinelli, Cesare. Splitting on de-
mand in sat modulo theories. International Conference on Logic for Programming Artiﬁcial
Intelligence and Reasoning, 2006.

[2] Bastani, Osbert, Ioannou, Yani, Lampropoulos, Leonidas, Vytiniotis, Dimitrios, Nori, Aditya,

and Criminisi, Antonio. Measuring neural net robustness with constraints. NIPS, 2016.

[3] Buxton, John N and Randell, Brian. Software Engineering Techniques: Report on a Conference

Sponsored by the NATO Science Committee. NATO Science Committee, 1970.

[4] Cheng, Chih-Hong, Nührenberg, Georg, and Ruess, Harald. Maximum resilience of artiﬁcial

neural networks. Automated Technology for Veriﬁcation and Analysis, 2017.

[5] Cheng, Chih-Hong, Nührenberg, Georg, and Ruess, Harald. Veriﬁcation of binarized neural

networks. arXiv:1710.03107, 2017.

[6] Ehlers, Ruediger. Formal veriﬁcation of piece-wise linear feed-forward neural networks. Au-

tomated Technology for Veriﬁcation and Analysis, 2017.

[7] Ehlers, Ruediger. Planet. https://github.com/progirep/planet, 2017.

[8] Hein, Matthias and Andriushchenko, Maksym. Formal guarantees on the robustness of a clas-

siﬁer against adversarial manipulation. NIPS, 2017.

[9] Hickey, Timothy, Ju, Qun, and Van Emden, Maarten H. Interval arithmetic: From principles

to implementation. Journal of the ACM (JACM), 2001.

[10] Huang, Xiaowei, Kwiatkowska, Marta, Wang, Sen, and Wu, Min. Safety veriﬁcation of deep

neural networks. International Conference on Computer Aided Veriﬁcation, 2017.

[11] Katz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and Kochenderfer, Mykel. Reluplex: An

efﬁcient smt solver for verifying deep neural networks. CAV, 2017.

[12] Katz, Guy, Barrett, Clark, Dill, David, Julian, Kyle, and Kochenderfer, Mykel. Reluplex.

https://github.com/guykatzz/ReluplexCav2017, 2017.

[13] Kolter, Zico and Wong, Eric. Provable defenses against adversarial examples via the convex

outer adversarial polytope. arXiv:1711.00851, 2017.

[14] Lomuscio, Alessio and Maganti, Lalit. An approach to reachability analysis for feed-forward

relu neural networks. arXiv:1706.07351, 2017.

[15] Marques-Silva, João P and Sakallah, Karem A. Grasp: A search algorithm for propositional

satisﬁability. IEEE Transactions on Computers, 1999.

[16] Narodytska, Nina, Kasiviswanathan, Shiva Prasad, Ryzhyk, Leonid, Sagiv, Mooly, and Walsh,
Toby. Verifying properties of binarized deep neural networks. arXiv:1709.06662, 2017.

[17] Pulina, Luca and Tacchella, Armando. An abstraction-reﬁnement approach to veriﬁcation of

artiﬁcial neural networks. CAV, 2010.

[18] Sherali, Hanif D and Adams, Warren P. A hierarchy of relaxations and convex hull character-
izations for mixed-integer zero—one programming problems. Discrete Applied Mathematics,
1994.

[19] Tjeng, Vincent and Tedrake, Russ. Verifying neural networks with mixed integer programming.

arXiv:1711.07356, 2017.

[20] Xiang, Weiming, Tran, Hoang-Dung, and Johnson, Taylor T. Output reachable set estimation

and veriﬁcation for multi-layer neural networks. arXiv:1708.03322, 2017.

[21] Zakrzewski, Radosiaw R. Veriﬁcation of a trained neural network accuracy. IJCNN, 2001.

10

A Canonical Form of the veriﬁcation problem
If the property is a simple inequality P (ˆxn) , cT ˆxn ≥ b, it is sufﬁcient to add to the network a ﬁnal
fully connected layer with one output, with weight of c and a bias of −b. If the global minimum
of this network is positive, it indicates that for all ˆxn the original network can output, we have
cT ˆxn − b ≥ 0 =⇒ cT ˆxn ≥ b, and as a consequence the property is True. On the other hand, if the
global minimum is negative, then the minimizer provides a counter-example.
Clauses speciﬁed using OR (denoted by
property is P (ˆxn) ,
Clauses speciﬁed using AND (denoted by
is equivalent to mini (cid:0)cT

) can be encoded by using a MaxPooling unit. If the
ˆxn − bi

) can be encoded similarly:
i ˆxn − bi(cid:1) ≥ 0 ⇐⇒ − (cid:0)maxi (cid:0)−cT

Vi (cid:2)
(cid:3)
B Toy Problem example
We have speciﬁed the problem of formal veriﬁcation of neural networks as follows: given a network
that implements a function ˆxn = f (x0), a bounded input domain C and a property P , we want to
prove that

, this is equivalent to maxi
(cid:3)

(cid:1)
the property P (ˆxn) =

i ˆxn + bi(cid:1)(cid:1) ≥ 0

ˆxn ≥ bi

ˆxn ≥ bi

cT
i
(cid:0)

Wi (cid:2)

≥ 0.

cT
i

cT
i

W

V

x0 ∈ C,

ˆxn = f (x0) =⇒ P (ˆxn).

(6)

A toy-example of the Neural Network veriﬁcation problem is given in Figure 5. On the domain
C = [−2; 2] × [−2; 2], we want to prove that the output y of the one hidden-layer network always
satisfy the property P (y) , [y > −5]. We will use this as a running example to explain the methods
used for comparison in our experiments.

[-2, 2]

x1

[-2, 2]

x2

a

1

1

-1

-1

b

y

-1

-1

Prove that y > −5

Figure 5: Example Neural Network. We attempt to prove the property that the network output is
always greater than -5

B.1 Problem formulation
For the network of Figure 5, the variables would be {x1, x2, ain, aout, bin, bout, y} and the set of
constraints would be:

− 2 ≤ x1 ≤ 2

ˆa = x1 + x2
y = −a − b

a = max(ˆa, 0)
y ≤ −5

−2 ≤ x2 ≤ 2
ˆb = −x1 − x2

b = max(ˆb, 0)

Here, ˆa is the input value to hidden unit, while a is the value after the ReLU. Any point satisfying all
the above constraints would be a counterexample to the property, as it would imply that it is possible
to drive the output to -5 or less.
B.2 MIP formulation
In our example, the non-linearities of equation (7c) would be replaced by

(7a)

(7b)

(7c)
(7d)

(8)

a ≥ 0
a ≤ ˆa − la(1 − δa)

a ≥ ˆa
a ≤ uaδa

δa ∈ {0, 1}

11

Step

x1

x2

ˆa

a

0

0
0

0
0

-2

ˆb
0

b

0

4
4

4
4

4

0

0
0

0
0
Fix linear constraints
0
1
0
0
1
0
Fix a ReLU
4
1
0
0
4
1
Fix linear constraints
1
-2
. . .

0
0

-4

4

y

0

-5
-5

-5
-5

-5

1

2

3

Figure 6: Evolution of the Reluplex algorithm. Red cells corresponds to value violating Linear con-
straints, and orange cells corresponds to value violating ReLU constraints. Resolution of violation
of linear constraints are prioritised.

where la is a lower bound of the value that ˆa can take (such as -4) and ua is an upper bound (such
as 4). The binary variable δa indicates which phase the ReLU is in: if δa = 0, the ReLU is blocked
and aout = 0, else the ReLU is passing and aout = ain. The problem remains difﬁcult due to the
integrality constraint on δa.
B.3 Running Reluplex
Table 6 shows the initial steps of a run of the Reluplex algorithm on the example of Figure 5. Starting
from an initial assignment, it attempts to ﬁx some violated constraints at each step. It prioritises
ﬁxing linear constraints ((7a), (7b) and (7d) in our illustrative example) using a simplex algorithm,
even if it leads to violated ReLU constraints (7c). This can be seen in step 1 and 3 of the process.
If no solution to the problem containing only linear constraints exists, this shows that the counterex-
ample search is unsatisﬁable. Otherwise, all linear constraints are ﬁxed and Reluplex attempts to ﬁx
one violated ReLU at a time, such as in step 2 of Table 6 (ﬁxing the ReLU b), potentially leading
to newly violated linear constraints. In the case where no violated ReLU exists, this means that a
satisﬁable assignment has been found and that the search can be interrupted.
This process is not guaranteed to converge, so to guarantee progress, non-linearities getting ﬁxed too
often are split into two cases. This generates two new sub-problems, each involving an additional
linear constraint instead of the linear one. The ﬁrst one solves the problem where ˆa ≤ 0 and a = 0,
the second one where ˆa ≥ 0 and a = ˆa. In the worst setting, the problem will be split completely
over all possible combinations of activation patterns, at which point the sub-problems are simple
LPs.

C Planet approximation
The feasible set of the Mixed Integer Programming formulation is given by the following set of
equations. We assume that all li are negative and ui are positive. In case this isn’t true, it is possible
to just update the bounds such that they are.

l0 ≤ x0 ≤ u0
ˆxi+1 = Wi+ixi + bi+i
xi ≥ 0
xi ≥ ˆxi
xi ≤ ˆxi − li · (1 − δi)
xi ≤ ui · δi
δi ∈ {0, 1}hi
ˆxn ≤ 0

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}

∀i ∈ {1, n − 1}

(9a)
(9b)
(9c)
(9d)
(9e)
(9f)

(9g)
(9h)

The level 0 of the Sherali-Adams hierarchy of relaxation Sherali & Adams [18] doesn’t include any
additional constraints. Indeed, polynomials of degree 0 are simply constants and their multiplication
with existing constraints followed by linearization therefore doesn’t add any new constraints. As a

12

xi[j]

li[j]

ˆxi[j]

ui[j]

Figure 7: Feasible domain corresponding to the Planet relaxation for a single ReLU.

result, the feasible domain given by the level 0 of the relaxation corresponds simply to the removal
of the integrality constraints:

l0 ≤ x0 ≤ u0
ˆxi+1 = Wi+ixi + bi+i
xi ≥ 0
xi ≥ ˆxi
xi ≤ ˆxi − li · (1 − di)
xi ≤ ui · di
0 ≤ di ≤ 1
ˆxn ≤ 0

∀i ∈ {0, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}
∀i ∈ {1, n − 1}

Combining the equations (10e) and (10f), looking at a single unit j in layer i, we obtain:

(11)
The function mapping di[j] to an upperbound of xi[j] is a minimum of linear functions, which means
that it is a concave function. As one of them is increasing and the other is decreasing, the maximum
will be reached when they are both equals.

ˆxi[j] − li(1 − di[j]), ui[j]di[j](cid:1)
(cid:0)

xi[j] ≤ min

Plugging this equation for d⋆ into Equation(11) gives that:

ˆxi[j] − li[j](1 − d⋆

i[j]) = ui[j]d⋆
i[j]
ˆxi[j] − li[j]
ui[j] − li[j]

d⋆
i[j] =

⇔

xi[j] ≤ ui[j]

ˆxi[j] − li[j]
ui[j] − li[j]

which corresponds to the upper bound of xi[j] introduced for Planet [6].
D MaxPooling
For space reason, we only described the case of ReLU activation function in the main paper. We
now present how to handle MaxPooling activation, either by converting them to the already handled
case of ReLU activations or by introducing an explicit encoding of them when appropriate.
D.1 Mixed Integer Programming
Similarly to the encoding of ReLU constraints using binary variables and bounds on the inputs, it is
possible to similarly encode MaxPooling constraints. The constraint

can be replaced by

y = max (x1, . . . , xk)

y ≥ xi
y ≤ xi + (ux1:k − lxi)(1 − δi)

δi = 1

X
i∈{1...k}
δi ∈ {0, 1}

∀i ∈ {1 . . . k}
∀i ∈ {1 . . . k}

∀i ∈ {1 . . . k}

where ux1:k is an upper-bound on all xi for i ∈ {1 . . . k} and lxi is a lower bound on xi.

13

(10a)
(10b)
(10c)
(10d)
(10e)
(10f)
(10g)

(10h)

(12)

(13)

(14)

(15a)
(15b)

(15c)

(15d)

(16)

(17)

(18)

(19a)

(19b)

D.2 Reluplex
In the version introduced by [11], there is no support for MaxPooling units. As the canonical repre-
sentation we evaluate needs them, we provide a way of encoding a MaxPooling unit as a combination
of Linear function and ReLUs.
To do so, we decompose the element-wise maximum into a series of pairwise maximum

max (xj , x2, x3, x4) = max( max (x1, x2) ,
max (x3, x4))

and decompose the pairwise maximums as sum of ReLUs:

max (x1, x2) = max (x1 − x2, 0) + max (x2 − lx2, 0) + lx2,

where lx2 is a pre-computed lower bound of the value that x2 can take.
As a result, we have seen that an elementwise maximum such as a MaxPooling unit can be decom-
posed as a series of pairwise maximum, which can themselves be decomposed into a sum of ReLUs
units. The only requirement is to be able to compute a lower bound on the input to the ReLU, for
which the methods discussed in the paper can help.
D.3 Planet
As opposed to Reluplex, Planet Ehlers [6] directly supports MaxPooling units. The SMT solver
driving the search can split either on ReLUs, by considering separately the case of the ReLU be-
ing passing or blocking. It also has the possibility on splitting on MaxPooling units, by treating
separately each possible choice of units being the largest one.
For the computation of lower bounds, the constraint

is replaced by the set of constraints:

y = max (x1, x2, x3, x4)

y ≥ xi

∀i ∈ {1 . . . 4}

y ≤

(xi − lxi) + max

lxi,

i

X
i

where xi are the inputs to the MaxPooling unit and lxi their lower bounds.
E Mixed Integers Variants
E.1 Encoding
Several variants of encoding are available to use Mixed Integer Programming as a solver for Neural
Network Veriﬁcation. As a reminder, in the main paper we used the formulation of Tjeng & Tedrake
[19]:

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

xi ≤ ui · δi
xi ≤ ˆxi − li · (1 − δi)

(20a)
(20b)

An alternative formulation is the one of Lomuscio & Maganti [14] and Cheng et al. [4]:

xi ≤ Mi · δi
xi ≤ ˆxi − Mi · (1 − δi)

xi = max (ˆxi, 0) ⇒ δi ∈ {0, 1}hi, xi ≥ 0,
xi ≥ ˆxi,

(21a)
(21b)
where Mi = max (−li, ui). This is fundamentally the same encoding but with a sligthly worse
bounds that is used, as one of the side of the bounds isn’t as tight as it could be.
E.2 Obtaining bounds
The formulation described in Equations (20) and (21) are dependant on obtaining lower and upper
bounds for the value of the activation of the network.
Interval AnalysisOne way to obtain them, mentionned in the paper, is the use of interval arith-
metic [9]. If we have bounds li, ui for a vector xi, we can derive the bounds ˆli+1, ˆui+1 for a vector
ˆxi+1 = Wi+1xi + bi+1

ˆli+1[j] =

(cid:16)W +

i+1[j,k]l+

i[k] + W −

i+1[j,k]u+

i[k](cid:17) + bi+1[j]

ˆui+1[j] =

(cid:16)W +

i+1[j,k]u+

i[k] + W −

i+1[j,k]l+

i[k](cid:17) + bi+1[j]

X
k

X
k

(22a)

(22b)

with the notation a+ = max(a, 0) and a− = min(a, 0). Propagating the bounds through a ReLU
activation is simply equivalent to applying the ReLU to the bounds.

14

Planet Linear approximationAn alternative way to obtain bounds is to use the relaxation of Planet.
This is the methods that was employed in the paper: we build incrementally the network approxi-
mation, layer by layer. To obtain the bounds over an activation, we optimize its value subject to the
constraints of the relaxation.
Given that this is a convex problem, we will achieve the optimum. Given that it is a relaxation, the
optimum will be a valid bound for the activation (given that the feasible domain of the relaxation
includes the feasible domains subject to the original constraints).
Once this value is obtained, we can use it to build the relaxation for the following layers. We can
build the linear approximation for the whole network and extract the bounds for each activation to
use in the encoding of the MIP. While obtaining the bounds in this manner is more expensive than
simply doing interval analysis, the obtained bounds are better.
E.3 Objective function
In the paper, we have formalised the veriﬁcation problem as a satisﬁability problem, equating the
existence of a counterexample with the feasibility of the output of a (potentially modiﬁed) network
being negative.
In practice, it is beneﬁcial to not simply formulate it as a feasibility problem but as an optimization
problem where the output of the network is explicitly minimized.
E.4 Comparison
We present here a comparison on CollisionDetection and ACAS of the different variants.

1. Planet-feasible uses the encoding of Equation (20), with bounds obtained based on the

planet relaxation, and solve the problem simply as a satisﬁability problem.

2. Interval is the same as Planet-feasible, except that the bounds used are obtained by interval

analysis rather than with the Planet relaxation.

3. Planet-symfeasible is the same as Planet-feasible, except that the encoding is the one of

Equation (21).

4. Planet-opt is the same as Planet-feasible, except that the problem is solved as an opti-
mization problem. The MIP solver attempt to ﬁnd the global minimum of the output of
the network. Using Gurobi’s callback, if a feasible solution is found with a negative value,
the optimization is interrupted and the current solution is returned. This corresponds to the
version that is reported in the main paper.

interval
planetsym-feasible
planet-feasible
planet-opt

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

interval
planetsym-feasible
planet-feasible
planet-opt

100

101

102

103

Computation time (in s)

(a) CollisionDetection Dataset

0
100

101
102
Computation time (in s)

103

(b) ACAS Dataset

Figure 8: Comparison between the different variants of MIP formulation for Neural Network veriﬁcation.

The ﬁrst observation that can be made is that when we look at the CollisionDetection dataset in
Figure 8a, only Planet-opt solves the dataset to 100% accuracy. The reason why the other methods
don’t reach it is not because of timeout but because they return spurious counterexamples. As
they encode only satisﬁability problem, they terminate as soon as they identify a solution with a
value of zero. Due to the large constants involved in the big-M, those solutions are sometimes not
actually valid counterexamples. This is the same reason why the BlackBox solver doesn’t reach
100% accuracy in the experiments reported in the main paper.

15

The other results that we can observe is the impact of the quality of the bounds when the networks get
deeper, and the problem becomes therefore more complex, such as in the ACAS dataset. Interval
has the worst bounds and is much slower than the other methods. Planetsym-feasible, with its
slightly worse bounds, performs worse than Planet-feasible and Planet-opt.

F Experimental setup details
We provide in Table 2 the characteristics of all of the datasets used for the experimental comparison
in the main paper.

Data set

Count

Collision
Detection

500

ACAS

188

PCAMNIST

27

Model Architecture
6 inputs
40 hidden unit layer, MaxPool
19 hidden unit layer
2 outputs
5 inputs
6 layers of 50 hidden units
5 outputs
10 or {5, 10, 25, 100, 500, 784} inputs
4 or {2, 3, 4, 5, 6, 7} layers
of 25 or {10, 15, 25, 50, 100} hidden units,
1 output, with a margin of +1000 or
{-1e4, -1000, -100, -50, -10, -1 ,1, 10, 50, 100, 1000, 1e4}

Table 2: Dimensions of all the data sets. For PCAMNIST, we use a base network with 10 inputs,
4 layers of 25 hidden units and a margin of 1000. We generate new problems by changing one
parameter at a time, using the values inside the brackets.

G Additional performance details
Given that there is a signiﬁcant difference in the way veriﬁcation works for SAT problems vs. UN-
SAT problems, we report also comparison results on the subset of data sorted by decision type.

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

0

10−1

101

102

100
Computation time (in s)
(a) On SAT properties

103

0

10−1

101

100
Computation time (in s)
(b) On UNSAT properties

102

103

Figure 9: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed on the CollisionDetection dataset. We can identify that all the errors that BlackBox makes
are on SAT properties, as it returns incorrect counterexamples.

H PCAMNIST details
PCAMNIST is a novel data set that we introduce to get a better understanding of what factors in-
ﬂuence the performance of various methods. It is generated in a way to give control over different

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

16

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

0

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

0

0

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

4000

2000
Computation time (in s)
(a) On SAT properties

6000

4000

2000
Computation time (in s)
(b) On UNSAT properties

6000

Figure 10: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed on the ACAS dataset. We observe that planet doesn’t succeed in solving any of the SAT
properties, while our proposed methods are extremely efﬁcient at it, even if there remains some
properties that they can’t solve.

architecture parameters. The networks takes k features as input, corresponding to the ﬁrst k eigen-
vectors of a Principal Component Analysis decomposition of the digits from the MNIST data set.
We also vary the depth (number of layers), width (number of hidden unit in each layer) of the net-
works. We train a different network for each combination of parameters on the task of predicting the
parity of the presented digit. This results in the accuracies reported in Table 3.
The properties that we attempt to verify are whether there exists an input for which the score assigned
to the odd class is greater than the score of the even class plus a large conﬁdence. By tweaking the
value of the conﬁdence in the properties, we can make the property either True or False, and we
can choose by how much is it true. This gives us the possibility of tweaking the “margin”, which
represent a good measure of difﬁculty of a network.
In addition to the impact of each factors separately as was shown in the main paper, we can also
look at it as a generic dataset and plot the cactus plots like for the other datasets. This can be found
in Figure 11

Network Parameter

Accuracy

Nb inputs Width Depth

Train

Test

5
10
25
100
500
784

10
10
10
10
10

25
25
25
25
25
25

10
15
25
50
100

88.18% 87.3%
97.42% 96.09%
99.87% 98.69%
100% 98.77%
100% 98.84%
100% 98.64%

96.34% 95.75%
96.31% 95.81%
97.42% 96.09%
97.35% 96.0%
97.72% 95.75%

2
3
4
5
6
7
Table 3: Accuracies of the network trained for the PCAMNIST dataset.

96.45% 95.71%
96.98% 96.05%
97.42% 96.09%
96.78% 95.9%
95.48% 95.2%
96.81% 96.07%

10
10
10
10
10
10

25
25
25
25
25
25

4
4
4
4
4
4

4
4
4
4
4

17

0

10−1

100
Computation time (in s)

102

101

103

Figure 11: Proportion of properties veriﬁable for varying time budgets depending on the methods
employed. The PCAMNIST dataset is challenging as None of the methods reaches more than 50%
success rate.

BaBSB
BaB
reluBaB
reluplex
MIPplanet
planet
BlackBox

100

d
e
i
f
i
r
e
v
 
s
e
i
t
r
e
p
o
r
p
 
f
o
 
%

80

60

40

20

18

