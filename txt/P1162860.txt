Tangent Convolutions for Dense Prediction in 3D

Maxim Tatarchenko∗
University of Freiburg

Jaesik Park∗
Intel Labs

Vladlen Koltun
Intel Labs

Qian-Yi Zhou
Intel Labs

8
1
0
2
 
l
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
3
4
4
2
0
.
7
0
8
1
:
v
i
X
r
a

Abstract

We present an approach to semantic scene analysis us-
ing deep convolutional networks. Our approach is based on
tangent convolutions – a new construction for convolutional
networks on 3D data. In contrast to volumetric approaches,
our method operates directly on surface geometry. Cru-
cially, the construction is applicable to unstructured point
clouds and other noisy real-world data. We show that tan-
gent convolutions can be evaluated efﬁciently on large-scale
point clouds with millions of points. Using tangent convo-
lutions, we design a deep fully-convolutional network for
semantic segmentation of 3D point clouds, and apply it to
challenging real-world datasets of indoor and outdoor 3D
environments. Experimental results show that the presented
approach outperforms other recent deep network construc-
tions in detailed analysis of large 3D scenes.

1. Introduction

Methods that utilize convolutional networks on 2D im-
ages dominate modern computer vision. A key contributing
factor to their success is efﬁcient local processing based on
the convolution operation. 2D convolution is deﬁned on a
regular grid, a domain that supports extremely efﬁcient im-
plementation. This in turn enables using powerful deep ar-
chitectures for processing large datasets at high resolution.
When it comes to analysis of large-scale 3D scenes, a
straightforward extension of this idea is volumetric convo-
lution on a voxel grid [35, 59, 10]. However, voxel-based
methods have limitations, including a cubic growth rate
of memory consumption and computation time. For this
reason, voxel-based ConvNets operate on low-resolution
voxel grids that limit their prediction accuracy. The prob-
lem can be alleviated by octree-based techniques that de-
ﬁne a ConvNet on an octree and enable processing some-
what higher-resolution volumes (e.g., up to 2563 vox-
els) [43, 57, 18, 42, 51]. Yet even this may be insufﬁcient
for detailed analysis of large-scale scenes.

On a deeper level, both efﬁcient and inefﬁcient voxel-
based methods treat 3D data as volumetric by exploiting 3D

∗Equal contribution.

Figure 1. Convolutional networks based on tangent convolutions
can be applied to semantic analysis of large-scale scenes, such
as urban environments. Top: point cloud from the Semantic3D
dataset. Bottom: semantic segmentation produced by the pre-
sented approach.

convolutions that integrate over volumes.
In reality, data
captured by 3D sensors such as RGB-D cameras and Li-
DAR typically represent surfaces: 2D structures embedded
in 3D space. (This is in contrast to truly volumetric 3D data,
as encountered for example in medical imaging.) Classic
features that are used for the analysis of such data are de-
ﬁned in terms that acknowledge the latent surface structure,
and do not treat the data as a volume [20, 12, 45].

The drawbacks of voxel-based methods are known in the
research community. A number of recent works argue that
volumetric data structures are not the natural substrate for
3D ConvNets, and propose alternative designs based on un-
ordered point sets [39], graphs [47], and sphere-type sur-
faces [32]. Unfortunately, these methods come with their
own drawbacks, such as limited sensitivity to local struc-
ture or restrictive topological assumptions.

We develop an alternative construction for convolutional
networks on surfaces, based on the notion of tangent con-

volution. This construction assumes that the data is sam-
pled from locally Euclidean surfaces. The latent surfaces
need not be known, and the data can be in any form that
supports approximate normal vector estimation, including
point clouds, meshes, and even polygon soup. (The same
assumption concerning normal vector estimation is made by
both classic and contemporary geometric feature descrip-
tors [20, 12, 45, 53, 46, 23].) The tangent convolution is
based on projecting local surface geometry on a tangent
plane around every point. This yields a set of tangent im-
ages. Every tangent image is treated as a regular 2D grid
that supports planar convolution. The content of all tan-
gent images can be precomputed from the surface geometry,
which enables efﬁcient implementation that scales to large
datasets, such as urban environments.

Using tangent convolution as the main building block,
we design a U-type network for dense semantic segmen-
tation of point clouds. Our proposed architecture is gen-
eral and can be applied to analysis of large-scale scenes.
We demonstrate its performance on three diverse real-world
datasets containing indoor and outdoor environments. A se-
mantic segmentation produced by a tangent convolutional
network is shown in Figure 1.

2. Related Work

Dense prediction in 3D, including semantic point cloud
segmentation, has a long history in computer vision. Pi-
oneering methods work on aerial LiDAR data and are
based on hand-crafted features with complex classiﬁers on
top [6, 7, 15]. Such approaches can also be combined
with high-level architectural rules [33]. A popular line of
work exploits graphical models, including conditional ran-
dom ﬁelds [38, 11, 2, 58, 19, 27, 56]. Related formula-
tions have also been proposed for interactive 3D segmenta-
tion [55, 37].

More recently, the deep learning revolution in computer
vision has spread to consume 3D data analysis. A variety of
methods that tackle 3D data using deep learning techniques
have been proposed. They can be considered in terms of the
underlying data representation.

A common representation of 3D data for deep learning
is a voxel grid. Deep networks that operate on voxelized
data have been applied to shape classiﬁcation [35, 59, 40],
semantic segmentation of indoor scenes [10], and biomed-
ical recordings [9, 8]. Due to the cubic complexity of
voxel grids, these methods can only operate at low res-
olution – typically not more than 643 – and have lim-
ited accuracy. Attempting to overcome this limitation,
researchers have proposed representations based on hier-
archical spatial data structures such as octrees and kd-
trees [43, 57, 18, 42, 13, 25, 51], which are more memory-
and computation-efﬁcient, and can therefore handle higher
resolutions. An alternative way of increasing the accu-

racy of voxel-based techniques is to add differentiable post-
processing, modeled upon the dense CRF [26, 52].

Other applications of deep networks consider RGB-D
images, which can be treated with fully-convolutional net-
works [16, 29, 36] and graph neural networks [41]. These
approaches support the use of powerful pretrained 2D net-
works, but are not generally applicable to unstructured point
clouds with unknown sensor poses. Attempting to address
this issue, Boulch et al. [5] train a ConvNet on images ren-
dered from point clouds using randomly placed virtual cam-
eras. In a more controlled setting with ﬁxed camera poses,
multi-view methods are successfully used for shape seg-
mentation [21], shape recognition [49, 40], and shape syn-
thesis [14, 30, 22, 50]. Our approach can be viewed as an
extreme multi-view approach in which a virtual camera is
associated with each point in the point cloud. A critical
problem that we address is the efﬁcient and scalable imple-
mentation of this approach, which enables its application to
dense point clouds of large-scale indoor and outdoor envi-
ronments.

Qi et al. [39] propose a network for analysing unordered
point sets, which is based on independent point process-
ing combined with global context aggregation through max-
pooling. Since the communication between the points is
quite weak, this approach experiences difﬁculties when ap-
plied to large-scale scenes with complex layouts.

There is a variety of more exotic deep learning formula-
tions for 3D analysis that do not address large-scale seman-
tic segmentation of whole scenes but provide interesting
ideas. Yi et al. [60] consider shape segmentation in the spec-
tral domain by synchronizing eigenvectors across models.
Masci et al. [34] and Boscaini et al. [4] design ConvNets
for Riemannian manifolds and use them to learn shape cor-
respondences. Sinha et al. [48] perform shape analysis on
geometry images. Simonovsky et al. [47] extend the con-
volution operator from regular grids to arbitrary graphs and
use it to design shape classiﬁcation networks. Li et al. [28]
introduce Field Probing Neural Networks which respect the
underlying sparsity of 3D data and are used for efﬁcient fea-
ture extraction. Tulsiani et al. [54] approximate 3D models
with volumetric primitives in an end-to-end differentiable
framework, and use this representation for solving several
tasks. Maron et al. [32] design ConvNets on surfaces for
sphere-type shapes.

Overall, most existing 3D deep learning systems either
rely on representations that do not support general scene
analysis, or have poor scalability. As we will show, deep
networks based on tangent convolutions scale to millions of
points and are suitable for detailed analysis of large scenes.

3. Tangent Convolution

In this section we formally introduce tangent convolu-
tions. All derivations are provided for point clouds, but they

np

p

i

j

Figure 2. Points q (blue) from the local neighborhood of a point p
(red) are projected onto the tangent image.

can easily be applied to any type of 3D data that supports
surface normal estimation, such as meshes.
Convolution with a continuous kernel. Let P = {p} be a
point cloud, and let F (p) be a discrete scalar function that
represents a signal deﬁned over P. F (p) can encode color,
geometry, or abstract features from intermediate network
layers. In order to convolve F , we need to extend it to a
continuous function. Conceptually, we introduce a virtual
orthogonal camera for p. It is conﬁgured to observe p along
the normal np. The image plane of this virtual camera is
the tangent plane πp of p. It parameterizes a virtual image
that can be represented as a continuous signal S(u), where
u ∈ R2 is a point in πp. We call S a tangent image.

The tangent convolution at p is deﬁned as

X(p) =

c(u)S(u) du,

(1)

(cid:90)

πp

where c(u) is the convolution kernel. We now describe how
S is computed from F .
Tangent plane estimation. For each point p we estimate
the orientation of its camera image using local covariance
analysis. This is a standard procedure [46] but we sum-
marize it here for completeness. Consider a set of points q
from a spherical neighborhood of p, such that (cid:107)p−q(cid:107) < R.
The orientation of the tangent plane is determined by the
eigenvectors of the covariance matrix C = (cid:80)
q rr(cid:62), where
r = q − p. The eigenvector of the smallest eigenvalue de-
ﬁnes the estimated surface normal np, and the other two
eigenvectors i and j deﬁne the 2D image axes that parame-
terize the tangent image.
Signal interpolation. Now our goal is to estimate image
signals S(u) from point signals F (q). We begin by pro-
jecting the neighbors q of p onto the tangent image, which
yields a set of projected points v = (r(cid:62)i, r(cid:62)j). This is
illustrated in Figure 2. We deﬁne

(a)

(b)

(c)

(d)

Figure 3. Signals from projected points (a) can be interpolated us-
ing one of the following schemes: nearest neighbor (b), full Gaus-
sian mixture (c), and Gaussian mixture with top-3 neighbors (d).

(4)

(5)

on the image plane. We thus need to interpolate their signals
in order to estimate the full function S(u) over the tangent
image:

S(u) =

(cid:88)

(cid:0)w(u, v) · S(v)(cid:1),

(3)

v
where w(u, v) is a kernel weight that satisﬁes (cid:80)
v w = 1.
We consider two schemes for signal interpolation: nearest
neighbor and Gaussian kernel mixture. These schemes are
illustrated in Figure 3. In the nearest neighbor (NN) case,

w(u, v) =

(cid:40)

1
0

if v is u’s NN,
otherwise.

In the Gaussian kernel mixture case,

w(u, v) =

exp

−

1
A

(cid:18)

(cid:107)u − v(cid:107)2
σ2

(cid:19)

,

where A normalizes the weights such that (cid:80)
v w = 1. More
sophisticated signal interpolation schemes can be consid-
ered, but we have not observed a signiﬁcant effect of the
interpolation scheme on empirical performance and will
mostly use simple nearest-neighbor estimation.

Finally, if we rewrite Equation (1) using the deﬁnitions
from Equations (2) and (3), we get the formula for the tan-
gent convolution:

X(p) =

c(u) ·

(cid:0)w(u, v) · F (q)(cid:1) du.

(6)

(cid:90)

πp

(cid:88)

v

Note that the role of the tangent image is increasingly
it provides the domain for u and ﬁgures in the
implicit:
evaluation of the weights w, but otherwise it need not be ex-
plicitly maintained. We will build on this observation in the
next section to show that tangent convolutions can be eval-
uated efﬁciently at scale, and can support the construction
of deep networks on point clouds with millions of points.

4. Efﬁciency

S(v) = F (q).

(2)

As shown in Figure 2 and Figure 3(a), points v are scattered

In this section we describe how the tangent convolution
deﬁned in Section 3 can be computed efﬁciently. In prac-
tice, the tangent image is treated as a discrete function on a

regular l ×l grid. Elements u are pixels in this virtual im-
age. The convolution kernel c is a discrete kernel applied
onto this image. Let us ﬁrst consider the nearest-neighbor
signal interpolation scheme introduced in Equation (4). We
can rewrite Equation (6) as

Cin

Fin

N

N

L

I

Cin

L

M

N

g(u)

F (g(u))

conv

Cout

Fout

X(p) =

(cid:88)

(cid:16)

c(u) · F (cid:0)g(u)(cid:1)(cid:17)

,

u

(7)

Figure 4. Efﬁcient evaluation of a convolutional layer built on tan-
gent convolutions.

where g(u) is a selection function that returns a point which
projects to the nearest neighbor of u on the image plane.
Note that g only depends on the point cloud geometry and
does not depend on the signal F . This allows us to precom-
pute g for all points.

From here on, we employ standard ConvNet terminol-
ogy and proceed to show how to implement a convolutional
layer using tangent convolutions. Our goal is to convolve an
input feature map Fin of size N ×Cin with a set of weights
W to produce an output feature map Fout of size N ×Cout,
where N is the number of points in the point cloud, while
Cin and Cout denote the number of input and output chan-
nels respectively. For implementation, we unroll 2D tangent
images and convolutional ﬁlters of size l×l into 1D vectors
of size 1×L, where L = l2. From then on, we compute 1D
convolutions. Note that such representation of a 2D tangent
convolution as a 1D convolution is not an approximation:
the results of the two operations are identical.

We start by precomputing the function g, which is repre-
sented as an N×L index matrix I. Elements of I are indices
of the corresponding tangent-plane nearest-neighbors in the
point cloud. Using I, we gather input signals (features) into
an intermediate tensor M of size N ×L×Cin. This tensor
is convolved with a ﬂattened set of kernels W of size 1×L,
which yields the output feature map Fout. This process is
illustrated in Figure 4.

Consider now the case of signal interpolation using
Gaussian kernel mixtures. For efﬁciency, we only consider
the set of top-k neighbors for each point, denoted N Nk. An
example image produced using the Gaussian kernel mixture
scheme with top-3 neighbors is shown in Figure 3(d). Equa-
tion (5) turns into

w(u, v) =

(cid:16)

− (cid:107)u−v(cid:107)2
σ2

(cid:17)

(cid:40) 1

A exp
0

if v ∈ N Nk

otherwise,

(8)

where A normalizes weights such that (cid:80)
v w = 1. With
this approximation, each pixel u has at most k non-zero
weights, denoted by w1..k(u). Their corresponding selec-
tion functions are denoted by g1..k(u). Both the weights
and the selection functions are independent of the signal F ,

and are thus precomputed. Equation (6) becomes

X(p) =

(cid:88)

(cid:16)

c(u) ·

(cid:0)wi(u) · F (gi(u))(cid:1)(cid:17)

(9)

k
(cid:88)

i=1

u

k
(cid:88)

(cid:88)

(cid:16)

i=1

u

=

wi(u) · c(u) · F (gi(u))

(10)

(cid:17)

.

signal

As with the nearest-neighbor

interpolation
scheme, we represent the precomputed selection functions
gi as k index matrices Ii of size N ×L. These index matri-
ces are used to assemble k intermediate signal tensors Mi
of size N ×L×Cin. Additionally, we collate the precom-
puted weights into k weight matrices Hi of size N×L. They
are used to compute the weighted sum M = (cid:80)
i Hi (cid:12) Mi,
which is ﬁnally convolved with the kernel W .

We implemented1 the presented construction in Tensor-
Flow [1]. It consists entirely of differentiable atomic oper-
ations, thus backpropagation is done seamlessly using the
automatic differentiation functionality of the framework.

5. Additional Ingredients

In this section we introduce additional ingredients that
are required to construct a convolutional network for point
cloud analysis.

5.1. Multi-scale analysis

Pooling. Convolutional networks commonly use pooling
to aggregate signals over larger spatial regions. We imple-
ment pooling in our framework via hashing onto a regular
3D grid. Points that are hashed onto the same grid point
pool their signals. The spacing of the grid determines the
pooling resolution. Consider points P = {p} and corre-
sponding signal values {F (p)}. Let g be a grid point and
let Vg be the set of points in P that hash to g. (The hash
function can be assumed to be simple quantization onto the
grid in each dimension.) Assume that Vg is not empty and
consider average pooling. All points in Vg and their signals
are pooled onto a single point:

p(cid:48)

g =

1
|Vg|

(cid:88)

p∈Vg

p

and

F (cid:48)(p(cid:48)

g) =

F (p).

(11)

1
|Vg|

(cid:88)

p∈Vg

1https://github.com/tatarchm/tangent_conv

In a convolutional network based on tangent convolutions,
we pool using progressively coarser grids. Starting with
some initial grid resolution (5cm in each dimension, say),
each successive pooling layer increases the step of the grid
by a factor of two (to 10cm, then 20cm, etc.). Such hashing
also alleviates the problem of non-uniform point density.
As a result, we can select the neighborhood radius for the
convolution operation globally for the entire dataset.

After each pooling layer, the radius r that is used to esti-
mate the tangent plane and the pixel size of the virtual tan-
gent image are doubled accordingly. Thus the resolution of
all tangent images decreases in step with the resolution of
the point cloud. Note that the downsampled point clouds
produced by pooling layers are independent of the signals
deﬁned over them. The downsampled point clouds, the as-
sociated tangent planes, and the corresponding index and
weight functions can thus all be precomputed for all layers
in the convolutional network: they need only be computed
once per pooling layer.

The implementation of a pooling layer is similar in spirit
to that of a convolutional layer described in Section 4. Con-
sider an input feature map Fin of size Nin ×C. Using grid
hashing, we assemble an index matrix I of size Nout × 8,
which contains indices of points that hash to the same grid
point. Assuming that we decrease the grid resolution by
a factor of 2 in each dimension in each pooling layer, the
number of points that hash to the same grid point will be
at most 8 in general.
(For initialization, we quantize the
points to some base resolution.) Using I, we assemble an
intermediate tensor of size Nout×8×C. We pool this tensor
along the second dimension according to the pooling oper-
ator (max, average, etc.), and thus obtain an output feature
map Fout of size Nout ×C.

Note that all stages in this process have linear complexity
in the number of points. Although points are hashed onto
regular grids, the grids themselves are never constructed or
represented. Hashing is performed via modular arithmetic
on individual point coordinates, and all data structures have
linear complexity in the number of points, independent of
the extent of the point set or the resolution of the grid.

Unpooling. The unpooling operation has an opposite ef-
fect to pooling:
it distributes signals from points in a
low-resolution feature map Fin onto points in a higher-
resolution feature map Fout. Unpooling reuses the index
matrix from the corresponding pooling operation. We copy
features from a single point in a low-resolution point cloud
to multiple points from which the information was aggre-
gated during pooling.

5.2. Local distance feature

So far, we have considered signals that could be ex-
pressed in terms of a scalar function F (q) with a well-
deﬁned value for each point q. This holds for color, in-

m 32 32

32

32

64 n

32 64 64

128

64 32

pool

unpool

skip

64

128

64

Figure 5. We use a fully-convolutional U-shaped network with
skip connections. The network receives m-dimensional features
as input and produces prediction scores for n classes.

tensity, and abstract ConvNet features. There is, however,
a signal that cannot be expressed in such terms and needs
special treatment. This signal is distance to the tangent
plane πp. This local signal is calculated by taking the
distance from each neighbor q to the tangent plane of p:
d = (q − p)(cid:62)np.

This signal is deﬁned in relation to the point p, there-
fore it cannot be directly plugged into the pipeline shown
in Figure 4. Instead, we precompute the distance images
for every point. Scattered signal interpolation is done in the
same way as for scalar signals (Equation (3)). After assem-
bling the intermediate tensor M for the ﬁrst convolutional
layer, we simply concatenate these distance images as an
additional channel in M. The ﬁrst convolutional layer gen-
erates a set of abstract features Fout that can be treated as
scalar signals from here on.
precomputations

implemented

using

All

are

Open3D [61].

6. Architecture

Using the ingredients introduced in the previous sec-
tions, we design an encoder-decoder network inspired by
the U-net [44]. The network architecture is illustrated in
Figure 5. It is a fully-convolutional network over a point
cloud, where the convolutions are tangent convolutions.
The encoder contains two pooling layers. The decoder con-
tains two corresponding unpooling layers. Encoder features
are propagated to corresponding decoder blocks via skip-
connections. All layers except the last one use 3 × 3 ﬁl-
ters and are followed by Leaky ReLU with negative slope
0.2 [31]. The last layer uses 1×1 convolutions to produce
ﬁnal class predictions. The network is trained by optimizing
the cross-entropy objective using the Adam optimizer with
initial learning rate 10−4 [24].
Receptive ﬁeld. The receptive ﬁeld size of one convolu-
tional layer is determined by the pixel size r of the tangent
image and the radius R that is used to collect the neighbors

of each point p. We set R = 2r, therefore the receptive
ﬁeld size of one layer is R. After each pooling layer, r is
doubled. The receptive ﬁeld of an element in the network
can be calculated by tracing the receptive ﬁelds of preced-
ing layers. With initial r = 5cm, the receptive ﬁeld size of
elements in the ﬁnal layer of the presented architecture is
4 · 10 + 4 · 20 + 2 · 40 = 200cm.

7. Experiments

We evaluate the performance of the presented approach
on the task of semantic 3D scene segmentation. Our ap-
proach is compared to recent deep networks for 3D data on
three different datasets.

7.1. Datasets and measures

We conduct experiments on three large-scale datasets
that contain real-world 3D scans of indoor and outdoor en-
vironments.

Semantic3D [17] is a dataset of scanned outdoor scenes
with over 3 billion points. It contains 15 training and 15 test
scenes annotated with 8 class labels. Being unable to evalu-
ate the baseline results on the ofﬁcial test server, we use our
own train/test split: Bildstein 1-3-5 are used for testing, the
rest for training.

Stanford Large-Scale 3D Indoor Spaces Dataset
(S3DIS) [3] contains 6 large-scale indoor areas from 3
different buildings, with 13 object classes. We use Area 5
for testing and the rest for training.

ScanNet [10] is a dataset with more than 1,500 scans of
indoor scenes with 20 object classes collected using an
RGB-D capture system. We follow the standard train/test
split provided by the authors.

Measures. We report three measures: mean accuracy over
classes (mA), mean intersection over union (mIoU), and
overall accuracy (oA). We build a full confusion matrix
based on the entire test set, and derive the ﬁnal scores from
it. Measures are evaluated over the original point clouds.
For approaches that produce labels over downsampled or
voxelized representations, we map these predictions to the
original point clouds via nearest-neighbor assignment.

Although we report oA for completeness, it is not a good
measure for semantic segmentation. If there are dominant
classes in the data (e.g., walls, ﬂoor, and ceiling in indoor
scenes), making correct predictions for these but poor pre-
dictions over the other classes will yield misleadingly high
oA scores.

7.2. Baselines

We compare our approach to three recent deep learn-
ing methods that operate on different underlying represen-
tations. We have chosen reasonably general methods that

have the potential to be applied to general scene analy-
sis and have open-source implementations. Our baselines
are PointNet [39], which operates on points, ScanNet [10],
which operates on low-resolution voxel grids, and Oct-
Net [43], which operates on higher-resolution octrees. We
used the source code provided by the authors. Due to the de-
sign of these methods, the data preparation routines and the
input signals are different for each dataset, and sometimes
deviate from the guidelines provided in the papers.

PointNet. For indoor datasets, we used the data sampling
strategy suggested in the original paper with global xyz, lo-
cally normalized xyz, and RGB as inputs. For Semantic3D,
we observed global xyz to be harmful, thus we only use lo-
cal xyz and color. Training data is generated by randomly
sampling (3m)3 cubes from the training scenes. Evaluation
is performed by applying a sliding window over the entire
scan.

ScanNet. The original network used 2 input channels: oc-
cupancy and a visibility mask computed using known cam-
era trajectories. Since scenes in general are not accompa-
nied by known camera trajectories, we only use occupancy
in the input signal. Following the original setup, we use
1.5×1.5×3m volumes voxelized into 31×31×62 grids and
augmented by 8 rotations. Each such cube yields a predic-
tion for one 1×1×62 column. (I.e., the ScanNet network
outputs a prediction for the central column only.) We use
random sampling for training, and exhaustive sliding win-
dow for testing.
OctNet. We use an architecture that operates on 2563 oc-
trees.
Inputs to the network are color, occupancy, and a
height-based feature that assigns each point to the top or
bottom part of the scan. Based on correspondence with the
authors regarding the best way to set up OctNet on differ-
ent datasets, we used (45m)3 volumes for Semantic3D and
(11m)3 volumes for the indoor datasets.

7.3. Setup of the presented approach

The architecture described in Section 6 is used in all ex-
periments. We evaluate four variants that use different in-
put signals: distance from tangent plane (D), height above
ground (H), normals (N), and color (RGB). All input signals
are normalized between 0 and 1. The initial resolution r of
the tangent image is 5cm for the indoor datasets and 10cm
for Semantic3D. It is doubled after each pooling layer. In
addition to providing the distance from tangent plane as in-
put to the ﬁrst convolutional layer, we concatenate the local
distance features from all scales of the point cloud to the
feature maps of the corresponding resolution produced by
pooling layers.

For ScanNet and S3DIS, we used whole rooms as indi-
vidual training batches. For Semantic3D, each batch was
a random sphere with a radius of 6m. For indoor scans,

we augment each scan by 8 rotations around the vertical
axis. To correct for imbalance between different classes,
we weigh the loss with the negative log of the training data
histogram.

7.4. Signal interpolation

We begin by comparing the effectiveness of two different
signal interpolation schemes: nearest neighbor and Gaus-
sian mixture. Both networks were trained on S3DIS with
D and H as the input signals. The resulting segmentation
scores are provided in the supplement. The two networks
produce similar results. We conclude that the nearest neigh-
bor signal estimation scheme is sufﬁcient, and use it in all
other experiments.

7.5. Main results

Quantitative results for all methods are summarized in
Table 2. Overall, our method produces high scores on all
datasets and consistently outperforms the baselines. Quali-
tative comparisons are shown in Figure 6.

Comparing the conﬁgurations of our networks that use
different input signals, we can see that geometry is much
more important than color on the indoor datasets. Adding
RGB information only slightly improves the scores on
S3DIS and is actually harmful for mean and overall accu-
racy on the ScanNet dataset. The situation is different for
the Semantic3D dataset: the network trained with color sig-
niﬁcantly outperforms all other conﬁgurations. Due to the
fact that H is normalized between 0 and 1 for every scan
separately, this information turns out to be harmful when
the global height of different scans is signiﬁcantly different.
Therefore, the network trained only with the distance signal
performs better than the other two geometric conﬁgurations.
In setting up and operating the baseline methods, we
found that all of them are quite hard to apply across datasets:
some non-trivial decisions had to be made for each new
dataset during the data preparation stage. None of the base-
lines showed consistent performance across the different
types of scenes.

PointNet reaches high oA scores on both indoor datasets.
However, the oA measure is strongly dominated by large
classes such as walls, ﬂoor, and ceiling. S3DIS has a fairly
regular layout because of the global room alignment pro-
cedure, which is very beneﬁcial for PointNet and allows it
to reach reasonable mA and mIoU scores on this dataset.
However, PointNet performs poorly on the ScanNet dataset,
which has more classes and noisy data. All but the most
prominent classes (i.e., walls and ﬂoor) are misclassiﬁed.
PointNet completely fails to produce meaningful predic-
tions on the even more challenging Semantic3D dataset.

Our conﬁguration of the ScanNet method produces rea-
sonable oA scores on both indoor datasets, but does much
worse in the other two measures. For reference, on the

ScanNet dataset we additionally report the number from the
original paper where a binary visibility-from-camera mask
was used as an additional input channel. This number is
much higher than our occupancy-only results, which do not
assume a known camera trajectory. Due to the fact that the
network only outputs predictions for the central column of
the voxel grid, evaluation is extremely time-consuming for
the large scenes in the Semantic3D dataset. Because of this
scalability issue, we did not succeed in evaluating ScanNet
on this dataset.

OctNet reaches good performance on the Semantic3D
dataset. However, the same network conﬁguration yields
bad results when applied to the indoor datasets. A possi-
ble explanation for this may be poor generalization due to
overﬁtting to the structure of training octrees.

7.6. Efﬁciency

We compared the efﬁciency of different methods on a
scan from S3DIS containing 125K points after grid hash-
ing. The results are reported in Table 1. Since ScanNet
and PointNet require multiple iterations for labeling a single
scan, we report both the time of a single forward pass and
the time for processing a full scan. OctNet and our method
process a full scan in one forward pass, which also ex-
plains their higher memory consumption compared to Scan-
Net and PointNet. ScanNet does not provide code for data
preprocessing, so we report the runtime of our Python im-
plementation needed for generating 38K sliding windows
during inference. Our method exhibits the best runtime for
both precomputation and inference.

Prep (s)

FP (s)

Full (s) Mem (GB)

PointNet
OctNet
ScanNet

Ours

16.5
15.5
867.8

1.59

0.01
0.61
0.002

0.52

0.65
0.61
6.34

0.52

0.39
3.33
0.97

2.35

Table 1. Efﬁciency of different methods. We report preprocessing
time (Prep), time for a single forward pass (FP), time for process-
ing a full scan (Full), and memory consumption (Mem).

8. Conclusion

We have presented tangent convolutions – a new con-
struction for convolutional networks on 3D data. The key
idea is to evaluate convolutions on virtual tangent planes at
every point. Crucially, tangent planes can be precomputed
and deep convolutional networks based on tangent convo-
lutions can be evaluated efﬁciently on large point clouds.
We have applied tangent convolutions to semantic segmen-
tation of large indoor and outdoor scenes. The presented
ideas may also be applicable to other problems in analysis,
processing, and synthesis of 3D data.

Semantic3D [17]

ScanNet [10]

S3DIS [3]

mIoU

3.76
50.7
n/a

58.1
58.0
52.5
66.4

mA

16.9
71.3
n/a

78.9
75.8
79.3
80.7

oA

16.3
80.7
n/a

84.8
83.3
79.5
89.3

mIoU

mA

oA

mIoU

12.2
18.1
13.5

40.9
40.3
40.7
40.9

17.9
26.4
19.2 (50.8)

68.1
76.6
69.4 (73.0)

52.5
52.2
55.3
55.1

80.9
80.6
80.3
80.1

41.3
26.3
24.6

49.8
50.0
51.7
52.8

mA

49.5
39.0
35.0

60.3
60.0
61.0
62.2

oA

78.8
68.9
64.2

80.2
81.2
82.2
82.5

PointNet [39]
OctNet [43]
ScanNet [10]

Ours (D)
Ours (DH)
Ours (DHN)
Ours (DHNRGB)

Table 2. Semantic segmentation accuracy for all methods across the three datasets. We report mean intersection over union (mIoU), mean
class accuracy (mA), and overall accuracy (oA). Note that oA is a bad measure and we recommend against using it in the future. We tested
different conﬁgurations of our method by combining four types of input signals: depth (D), height (H), normals (N), and color (RGB).

Color

PointNet [39]

ScanNet [10]

OctNet [43]

Ours (DHNRGB)

Ground truth

(cid:108) Ceiling (cid:108) Floor (cid:108) Walls (cid:108) Column (cid:108) Door (cid:108) Table (cid:108) Chair (cid:108) Sofa (cid:108) Bookcase (cid:108) Board (cid:108) Clutter

Color

OctNet [43]

Ours (DHNRGB)

Ground truth

(cid:108) Man made terrain (cid:108) Natural terrain (cid:108) High vegetation (cid:108) Low vegetation (cid:108) Building (cid:108) Hardscape (cid:108) Scanning artifacts (cid:108) Cars

Figure 6. Qualitative comparisons on S3DIS [3] (top) and Semantic3D [17] (bottom). Labels are coded by color.

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
et al. TensorFlow: A system for large-scale machine learn-
ing. In OSDI, 2016.

[2] A. Anand, H. S. Koppula, T. Joachims, and A. Saxena.
Contextually guided semantic labeling and search for three-
dimensional point clouds. International Journal of Robotics
Research, 32(1), 2013.

[3] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,
M. Fischer, and S. Savarese. 3D semantic parsing of large-
scale indoor spaces. In CVPR, 2016.

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. M. Bronstein.
Learning shape correspondence with anisotropic convolu-
tional neural networks. In NIPS, 2016.

[5] A. Boulch, B. L. Saux, and N. Audebert. Unstructured point
cloud semantic labeling using deep segmentation networks.
In Eurographics Workshop on 3D Object Retrieval, 2017.
[6] A. P. Charaniya, R. Manduchi, and S. K. Lodha. Supervised
In CVPR

parametric classiﬁcation of aerial LiDAR data.
Workshops, 2004.

[7] N. Chehata, L. Guo, and C. Mallet. Contribution of airborne
full-waveform lidar and image data for urban scene classiﬁ-
cation. In ICIP, 2009.

[9]

[8] H. Chen, Q. Dou, L. Yu, and P. Heng. VoxResNet: Deep vox-
elwise residual networks for volumetric brain segmentation.
arXiv:1608.05895, 2016.
¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and
O. Ronneberger. 3D U-Net: Learning dense volumetric seg-
mentation from sparse annotation. In MICCAI, 2016.
[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A.
Funkhouser, and M. Nießner. ScanNet: Richly-annotated
3D reconstructions of indoor scenes. In CVPR, 2017.
[11] G. Floros and B. Leibe. Joint 2D-3D temporally consistent
semantic segmentation of street scenes. In CVPR, 2012.
[12] A. Frome, D. Huber, R. Kolluri, T. B¨ulow, and J. Malik. Rec-
ognizing objects in range data using regional point descrip-
tors. In ECCV, 2004.

[13] M. Gadelha, S. Maji, and R. Wang. 3D shape generation
using spatially ordered point clouds. In BMVC, 2017.
[14] M. Gadelha, S. Maji, and R. Wang. 3D shape induction from

2D views of multiple objects. In 3DV, 2017.

[15] A. Golovinskiy, V. G. Kim, and T. A. Funkhouser. Shape-
based recognition of 3D point clouds in urban environments.
In ICCV, 2009.

[16] S. Gupta, P. A. Arbel´aez, R. B. Girshick, and J. Malik. In-
door scene understanding with RGB-D images: Bottom-up
segmentation, object detection and semantic segmentation.
IJCV, 112(2), 2015.

[17] T. Hackel, N. Savinov, L. Ladicky,

J. D. Wegner,
K. Schindler, and M. Pollefeys. Semantic3D.net: A new
large-scale point cloud classiﬁcation benchmark. In ISPRS
Annals of the Photogrammetry, Remote Sensing and Spatial
Information Sciences, 2017.

[18] C. H¨ane, S. Tulsiani, and J. Malik. Hierarchical surface pre-

diction for 3D object reconstruction. In 3DV, 2017.

[19] A. Hermans, G. Floros, and B. Leibe. Dense 3D semantic
In ICRA,

mapping of indoor scenes from RGB-D images.
2014.

[20] A. E. Johnson and M. Hebert. Using spin images for efﬁ-
cient object recognition in cluttered 3D scenes. PAMI, 21(5),
1999.

[21] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri. 3D
shape segmentation with projective convolutional networks.
In CVPR, 2017.

[22] A. Kar, C. H¨ane, and J. Malik. Learning a multi-view stereo

machine. In NIPS, 2017.

[23] M. Khoury, Q.-Y. Zhou, and V. Koltun. Learning compact

geometric features. In ICCV, 2017.

[24] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015.

[25] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-
networks for the recognition of 3D point cloud models. In
ICCV, 2017.

[26] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
In NIPS,

connected CRFs with Gaussian edge potentials.
2011.

[27] A. Kundu, Y. Li, F. Dellaert, F. Li, and J. M. Rehg. Joint se-
mantic segmentation and 3D reconstruction from monocular
video. In ECCV, 2014.

[28] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: Field
probing neural networks for 3D data. In NIPS, 2016.
[29] Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin. LSTM-
CF: Unifying context modeling and fusion with LSTMs for
RGB-D scene labeling. In ECCV, 2016.

[30] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang.
3D shape reconstruction from sketches via multi-view con-
volutional networks. In 3DV, 2017.

[31] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectiﬁer nonlin-
earities improve neural network acoustic models. In ICML
Workshops, 2013.

[32] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym,
E. Yumer, V. G. Kim, and Y. Lipman. Convolutional neu-
ral networks on surfaces via seamless toric covers. ACM
Transactions on Graphics, 36(4), 2017.

[33] A. Martinovic, J. Knopp, H. Riemenschneider, and L. J. Van
Gool. 3D all the way: Semantic segmentation of urban
scenes from start to end in 3D. In CVPR, 2015.

[34] J. Masci, D. Boscaini, M. M. Bronstein, and P. Van-
dergheynst. Geodesic convolutional neural networks on Rie-
mannian manifolds. In ICCV Workshops, 2015.

[35] D. Maturana and S. Scherer. VoxNet: A 3D convolutional
In IROS,

neural network for real-time object recognition.
2015.

[36] J. McCormac, A. Handa, A. J. Davison, and S. Leutenegger.
SemanticFusion: Dense 3D semantic mapping with convo-
lutional neural networks. In ICRA, 2017.

[37] O. Miksik, V. Vineet, M. Lidegaard, R. Prasaath, M. Nießner,
S. Golodetz, S. L. Hicks, P. P´erez, S. Izadi, and P. H. S. Torr.
The semantic paintbrush: Interactive 3D mapping and recog-
nition in large outdoor spaces. In CHI, 2015.

[38] D. Munoz, N. Vandapel, and M. Hebert. Onboard contextual
classiﬁcation of 3-D point clouds with learned high-order
Markov random ﬁelds. In ICRA, 2009.

[39] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In CVPR, 2017.

[40] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view CNNs for object clas-
siﬁcation on 3D data. In CVPR, 2016.

[41] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun. 3D graph
neural networks for RGBD semantic segmentation. In ICCV,
2017.

[42] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger. Oct-
NetFusion: Learning depth fusion from data. In 3DV, 2017.
[43] G. Riegler, A. O. Ulusoy, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In CVPR, 2017.
[44] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, 2015.

[45] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature
histograms (FPFH) for 3D registration. In ICRA, 2009.
[46] S. Salti, F. Tombari, and L. di Stefano. SHOT: Unique signa-
tures of histograms for surface and texture description. Com-
puter Vision and Image Understanding, 125, 2014.

[47] M. Simonovsky and N. Komodakis.

Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In CVPR, 2017.

[48] A. Sinha, J. Bai, and K. Ramani. Deep learning 3D shape

surfaces using geometry images. In ECCV, 2016.

[49] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3D shape
recognition. In ICCV, 2015.

[50] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3D
models from single images with a convolutional network. In
ECCV, 2016.

[51] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3D outputs. In ICCV, 2017.

[52] L. P. Tchapmi, C. B. Choy, I. Armeni, J. Gwak, and
S. Savarese. SEGCloud: Semantic segmentation of 3D point
clouds. In 3DV, 2017.

[53] F. Tombari, S. Salti, and L. Di Stefano. Unique shape context
In ACM Workshop on 3D Object

for 3D data description.
Retrieval, 2010.

[54] S. Tulsiani, H. Su, L. J. Guibas, A. A. Efros, and J. Malik.
Learning shape abstractions by assembling volumetric prim-
itives. In CVPR, 2017.

[55] J. P. C. Valentin, V. Vineet, M. Cheng, D. Kim, J. Shotton,
P. Kohli, M. Nießner, A. Criminisi, S. Izadi, and P. H. S. Torr.
SemanticPaint: Interactive 3D labeling and learning at your
ﬁngertips. ACM Transactions on Graphics, 34(5), 2015.
[56] V. Vineet, O. Miksik, M. Lidegaard, M. Nießner,
S. Golodetz, V. A. Prisacariu, O. K¨ahler, D. W. Murray,
S. Izadi, P. P´erez, and P. H. S. Torr. Incremental dense se-
mantic stereo fusion for large-scale semantic scene recon-
struction. In ICRA, 2015.

[57] P. Wang, Y. Liu, Y. Guo, C. Sun, and X. Tong. O-CNN:
Octree-based convolutional neural networks for 3D shape
analysis. ACM Transactions on Graphics, 36(4), 2017.
[58] C. Wu, I. Lenz, and A. Saxena. Hierarchical semantic label-
ing for task-relevant RGB-D perception. In RSS, 2014.
[59] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumetric
shapes. In CVPR, 2015.

[60] L. Yi, H. Su, X. Guo, and L. J. Guibas. SyncSpecCNN:
Synchronized spectral CNN for 3D shape segmentation. In
CVPR, 2017.

[61] Q.-Y. Zhou, J. Park, and V. Koltun. Open3D: A modern li-
brary for 3D data processing. arXiv:1801.09847, 2018.

D. Qualitative results

We provide more qualitative results of our method on

different datasets in Figures 7-9.

Appendix

A. Robustness to noise

We evaluated the robustness of our approach to noise.
Several instances of our network were trained on the S3DIS
dataset perturbed with different amounts of additive Gaus-
sian noise with standard deviation σ. The results are re-
ported in Table 3. We selected small subsets of the data for
training and testing (Area 1 for training and Area 5 for test-
ing), which is why the ﬁnal performance numbers are not
compatible with those reported in the main paper.

a
t
a
d

g
n
i
n
i
a
r
T

σ, m

OA

0.00

0.59

0.02

0.63

0.04

0.63

0.08

0.68

0.16

0.17

Table 3. Performance evaluation with different levels of noise.

Surprisingly, reasonable amounts of noise improve over-
all accuracy. The method only suffers if the noise severely
damages semantic structure in the point cloud. We did not
tune any parameters in the pipeline for these experiments.

B. Signal interpolation

In this experiment we compare the effectiveness of two
signal interpolation schemes: nearest neighbor and Gaus-
sian mixture. Quantitative results on S3DIS using D and H
as input signals are presented in Table 4. Both methods pro-
duce very similar results which is why the simpler nearest
neighbor interpolation is used throughout the paper.

Signal

NN
Gaussian

mIoU

50.0
50.7

mA

60.0
59.6

oA

81.2
81.3

Table 4. Signal interpolation using the nearest neighbor scheme
and the Gaussian mixture scheme produce similar results.

C. Comparison with SnapNet

We also compared our approach with the SnapNet by
Boulch et al. [5]. They project a 3D scene onto a set of
2D images. Those images are then segmented with a regu-
lar 2D ConvNet. The main strength of this approach is the
possibility to combine it with transfer learning and use the
weights of a network pre-trained on ImageNet for initializa-
tion. Applying this strategy yields the mIoU score of 67.7
on the Semantic3D datset, compared to 66.4 produced by
our approach. However, the non-trivial camera pose sam-
pling procedure required by SnapNet did not allow us to
apply it to indoor datasets.

Color

Prediction

Ground truth

(cid:108) Ceiling (cid:108) Floor (cid:108) Walls (cid:108) Column (cid:108) Window (cid:108) Door (cid:108) Table (cid:108) Chair (cid:108) Sofa (cid:108) Bookcase (cid:108) Board (cid:108) Clutter

Figure 7. Qualitative results on S3DIS [3].

Color

Prediction

Ground truth

(cid:108) Wall (cid:108) Floor (cid:108) Cabinet (cid:108) Bed (cid:108) Chair (cid:108) Sofa (cid:108) Table (cid:108) Door (cid:108) Window (cid:108) Bookshelf (cid:108) Desk (cid:108) Other furniture

Figure 8. Qualitative results on ScanNet [10].

Color

Prediction

Ground truth

(cid:108) Man made terrain (cid:108) Natural terrain (cid:108) High vegetation (cid:108) Low vegetation (cid:108) Building (cid:108) Hardscape (cid:108) Scanning artifacts (cid:108) Cars

Figure 9. Qualitative results on Semantic3D [17].

Tangent Convolutions for Dense Prediction in 3D

Maxim Tatarchenko∗
University of Freiburg

Jaesik Park∗
Intel Labs

Vladlen Koltun
Intel Labs

Qian-Yi Zhou
Intel Labs

8
1
0
2
 
l
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
3
4
4
2
0
.
7
0
8
1
:
v
i
X
r
a

Abstract

We present an approach to semantic scene analysis us-
ing deep convolutional networks. Our approach is based on
tangent convolutions – a new construction for convolutional
networks on 3D data. In contrast to volumetric approaches,
our method operates directly on surface geometry. Cru-
cially, the construction is applicable to unstructured point
clouds and other noisy real-world data. We show that tan-
gent convolutions can be evaluated efﬁciently on large-scale
point clouds with millions of points. Using tangent convo-
lutions, we design a deep fully-convolutional network for
semantic segmentation of 3D point clouds, and apply it to
challenging real-world datasets of indoor and outdoor 3D
environments. Experimental results show that the presented
approach outperforms other recent deep network construc-
tions in detailed analysis of large 3D scenes.

1. Introduction

Methods that utilize convolutional networks on 2D im-
ages dominate modern computer vision. A key contributing
factor to their success is efﬁcient local processing based on
the convolution operation. 2D convolution is deﬁned on a
regular grid, a domain that supports extremely efﬁcient im-
plementation. This in turn enables using powerful deep ar-
chitectures for processing large datasets at high resolution.
When it comes to analysis of large-scale 3D scenes, a
straightforward extension of this idea is volumetric convo-
lution on a voxel grid [35, 59, 10]. However, voxel-based
methods have limitations, including a cubic growth rate
of memory consumption and computation time. For this
reason, voxel-based ConvNets operate on low-resolution
voxel grids that limit their prediction accuracy. The prob-
lem can be alleviated by octree-based techniques that de-
ﬁne a ConvNet on an octree and enable processing some-
what higher-resolution volumes (e.g., up to 2563 vox-
els) [43, 57, 18, 42, 51]. Yet even this may be insufﬁcient
for detailed analysis of large-scale scenes.

On a deeper level, both efﬁcient and inefﬁcient voxel-
based methods treat 3D data as volumetric by exploiting 3D

∗Equal contribution.

Figure 1. Convolutional networks based on tangent convolutions
can be applied to semantic analysis of large-scale scenes, such
as urban environments. Top: point cloud from the Semantic3D
dataset. Bottom: semantic segmentation produced by the pre-
sented approach.

convolutions that integrate over volumes.
In reality, data
captured by 3D sensors such as RGB-D cameras and Li-
DAR typically represent surfaces: 2D structures embedded
in 3D space. (This is in contrast to truly volumetric 3D data,
as encountered for example in medical imaging.) Classic
features that are used for the analysis of such data are de-
ﬁned in terms that acknowledge the latent surface structure,
and do not treat the data as a volume [20, 12, 45].

The drawbacks of voxel-based methods are known in the
research community. A number of recent works argue that
volumetric data structures are not the natural substrate for
3D ConvNets, and propose alternative designs based on un-
ordered point sets [39], graphs [47], and sphere-type sur-
faces [32]. Unfortunately, these methods come with their
own drawbacks, such as limited sensitivity to local struc-
ture or restrictive topological assumptions.

We develop an alternative construction for convolutional
networks on surfaces, based on the notion of tangent con-

volution. This construction assumes that the data is sam-
pled from locally Euclidean surfaces. The latent surfaces
need not be known, and the data can be in any form that
supports approximate normal vector estimation, including
point clouds, meshes, and even polygon soup. (The same
assumption concerning normal vector estimation is made by
both classic and contemporary geometric feature descrip-
tors [20, 12, 45, 53, 46, 23].) The tangent convolution is
based on projecting local surface geometry on a tangent
plane around every point. This yields a set of tangent im-
ages. Every tangent image is treated as a regular 2D grid
that supports planar convolution. The content of all tan-
gent images can be precomputed from the surface geometry,
which enables efﬁcient implementation that scales to large
datasets, such as urban environments.

Using tangent convolution as the main building block,
we design a U-type network for dense semantic segmen-
tation of point clouds. Our proposed architecture is gen-
eral and can be applied to analysis of large-scale scenes.
We demonstrate its performance on three diverse real-world
datasets containing indoor and outdoor environments. A se-
mantic segmentation produced by a tangent convolutional
network is shown in Figure 1.

2. Related Work

Dense prediction in 3D, including semantic point cloud
segmentation, has a long history in computer vision. Pi-
oneering methods work on aerial LiDAR data and are
based on hand-crafted features with complex classiﬁers on
top [6, 7, 15]. Such approaches can also be combined
with high-level architectural rules [33]. A popular line of
work exploits graphical models, including conditional ran-
dom ﬁelds [38, 11, 2, 58, 19, 27, 56]. Related formula-
tions have also been proposed for interactive 3D segmenta-
tion [55, 37].

More recently, the deep learning revolution in computer
vision has spread to consume 3D data analysis. A variety of
methods that tackle 3D data using deep learning techniques
have been proposed. They can be considered in terms of the
underlying data representation.

A common representation of 3D data for deep learning
is a voxel grid. Deep networks that operate on voxelized
data have been applied to shape classiﬁcation [35, 59, 40],
semantic segmentation of indoor scenes [10], and biomed-
ical recordings [9, 8]. Due to the cubic complexity of
voxel grids, these methods can only operate at low res-
olution – typically not more than 643 – and have lim-
ited accuracy. Attempting to overcome this limitation,
researchers have proposed representations based on hier-
archical spatial data structures such as octrees and kd-
trees [43, 57, 18, 42, 13, 25, 51], which are more memory-
and computation-efﬁcient, and can therefore handle higher
resolutions. An alternative way of increasing the accu-

racy of voxel-based techniques is to add differentiable post-
processing, modeled upon the dense CRF [26, 52].

Other applications of deep networks consider RGB-D
images, which can be treated with fully-convolutional net-
works [16, 29, 36] and graph neural networks [41]. These
approaches support the use of powerful pretrained 2D net-
works, but are not generally applicable to unstructured point
clouds with unknown sensor poses. Attempting to address
this issue, Boulch et al. [5] train a ConvNet on images ren-
dered from point clouds using randomly placed virtual cam-
eras. In a more controlled setting with ﬁxed camera poses,
multi-view methods are successfully used for shape seg-
mentation [21], shape recognition [49, 40], and shape syn-
thesis [14, 30, 22, 50]. Our approach can be viewed as an
extreme multi-view approach in which a virtual camera is
associated with each point in the point cloud. A critical
problem that we address is the efﬁcient and scalable imple-
mentation of this approach, which enables its application to
dense point clouds of large-scale indoor and outdoor envi-
ronments.

Qi et al. [39] propose a network for analysing unordered
point sets, which is based on independent point process-
ing combined with global context aggregation through max-
pooling. Since the communication between the points is
quite weak, this approach experiences difﬁculties when ap-
plied to large-scale scenes with complex layouts.

There is a variety of more exotic deep learning formula-
tions for 3D analysis that do not address large-scale seman-
tic segmentation of whole scenes but provide interesting
ideas. Yi et al. [60] consider shape segmentation in the spec-
tral domain by synchronizing eigenvectors across models.
Masci et al. [34] and Boscaini et al. [4] design ConvNets
for Riemannian manifolds and use them to learn shape cor-
respondences. Sinha et al. [48] perform shape analysis on
geometry images. Simonovsky et al. [47] extend the con-
volution operator from regular grids to arbitrary graphs and
use it to design shape classiﬁcation networks. Li et al. [28]
introduce Field Probing Neural Networks which respect the
underlying sparsity of 3D data and are used for efﬁcient fea-
ture extraction. Tulsiani et al. [54] approximate 3D models
with volumetric primitives in an end-to-end differentiable
framework, and use this representation for solving several
tasks. Maron et al. [32] design ConvNets on surfaces for
sphere-type shapes.

Overall, most existing 3D deep learning systems either
rely on representations that do not support general scene
analysis, or have poor scalability. As we will show, deep
networks based on tangent convolutions scale to millions of
points and are suitable for detailed analysis of large scenes.

3. Tangent Convolution

In this section we formally introduce tangent convolu-
tions. All derivations are provided for point clouds, but they

np

p

i

j

Figure 2. Points q (blue) from the local neighborhood of a point p
(red) are projected onto the tangent image.

can easily be applied to any type of 3D data that supports
surface normal estimation, such as meshes.
Convolution with a continuous kernel. Let P = {p} be a
point cloud, and let F (p) be a discrete scalar function that
represents a signal deﬁned over P. F (p) can encode color,
geometry, or abstract features from intermediate network
layers. In order to convolve F , we need to extend it to a
continuous function. Conceptually, we introduce a virtual
orthogonal camera for p. It is conﬁgured to observe p along
the normal np. The image plane of this virtual camera is
the tangent plane πp of p. It parameterizes a virtual image
that can be represented as a continuous signal S(u), where
u ∈ R2 is a point in πp. We call S a tangent image.

The tangent convolution at p is deﬁned as

X(p) =

c(u)S(u) du,

(1)

(cid:90)

πp

where c(u) is the convolution kernel. We now describe how
S is computed from F .
Tangent plane estimation. For each point p we estimate
the orientation of its camera image using local covariance
analysis. This is a standard procedure [46] but we sum-
marize it here for completeness. Consider a set of points q
from a spherical neighborhood of p, such that (cid:107)p−q(cid:107) < R.
The orientation of the tangent plane is determined by the
eigenvectors of the covariance matrix C = (cid:80)
q rr(cid:62), where
r = q − p. The eigenvector of the smallest eigenvalue de-
ﬁnes the estimated surface normal np, and the other two
eigenvectors i and j deﬁne the 2D image axes that parame-
terize the tangent image.
Signal interpolation. Now our goal is to estimate image
signals S(u) from point signals F (q). We begin by pro-
jecting the neighbors q of p onto the tangent image, which
yields a set of projected points v = (r(cid:62)i, r(cid:62)j). This is
illustrated in Figure 2. We deﬁne

(a)

(b)

(c)

(d)

Figure 3. Signals from projected points (a) can be interpolated us-
ing one of the following schemes: nearest neighbor (b), full Gaus-
sian mixture (c), and Gaussian mixture with top-3 neighbors (d).

(4)

(5)

on the image plane. We thus need to interpolate their signals
in order to estimate the full function S(u) over the tangent
image:

S(u) =

(cid:88)

(cid:0)w(u, v) · S(v)(cid:1),

(3)

v
where w(u, v) is a kernel weight that satisﬁes (cid:80)
v w = 1.
We consider two schemes for signal interpolation: nearest
neighbor and Gaussian kernel mixture. These schemes are
illustrated in Figure 3. In the nearest neighbor (NN) case,

w(u, v) =

(cid:40)

1
0

if v is u’s NN,
otherwise.

In the Gaussian kernel mixture case,

w(u, v) =

exp

−

1
A

(cid:18)

(cid:107)u − v(cid:107)2
σ2

(cid:19)

,

where A normalizes the weights such that (cid:80)
v w = 1. More
sophisticated signal interpolation schemes can be consid-
ered, but we have not observed a signiﬁcant effect of the
interpolation scheme on empirical performance and will
mostly use simple nearest-neighbor estimation.

Finally, if we rewrite Equation (1) using the deﬁnitions
from Equations (2) and (3), we get the formula for the tan-
gent convolution:

X(p) =

c(u) ·

(cid:0)w(u, v) · F (q)(cid:1) du.

(6)

(cid:90)

πp

(cid:88)

v

Note that the role of the tangent image is increasingly
it provides the domain for u and ﬁgures in the
implicit:
evaluation of the weights w, but otherwise it need not be ex-
plicitly maintained. We will build on this observation in the
next section to show that tangent convolutions can be eval-
uated efﬁciently at scale, and can support the construction
of deep networks on point clouds with millions of points.

4. Efﬁciency

S(v) = F (q).

(2)

As shown in Figure 2 and Figure 3(a), points v are scattered

In this section we describe how the tangent convolution
deﬁned in Section 3 can be computed efﬁciently. In prac-
tice, the tangent image is treated as a discrete function on a

regular l ×l grid. Elements u are pixels in this virtual im-
age. The convolution kernel c is a discrete kernel applied
onto this image. Let us ﬁrst consider the nearest-neighbor
signal interpolation scheme introduced in Equation (4). We
can rewrite Equation (6) as

Cin

Fin

N

N

L

I

Cin

L

M

N

g(u)

F (g(u))

conv

Cout

Fout

X(p) =

(cid:88)

(cid:16)

c(u) · F (cid:0)g(u)(cid:1)(cid:17)

,

u

(7)

Figure 4. Efﬁcient evaluation of a convolutional layer built on tan-
gent convolutions.

where g(u) is a selection function that returns a point which
projects to the nearest neighbor of u on the image plane.
Note that g only depends on the point cloud geometry and
does not depend on the signal F . This allows us to precom-
pute g for all points.

From here on, we employ standard ConvNet terminol-
ogy and proceed to show how to implement a convolutional
layer using tangent convolutions. Our goal is to convolve an
input feature map Fin of size N ×Cin with a set of weights
W to produce an output feature map Fout of size N ×Cout,
where N is the number of points in the point cloud, while
Cin and Cout denote the number of input and output chan-
nels respectively. For implementation, we unroll 2D tangent
images and convolutional ﬁlters of size l×l into 1D vectors
of size 1×L, where L = l2. From then on, we compute 1D
convolutions. Note that such representation of a 2D tangent
convolution as a 1D convolution is not an approximation:
the results of the two operations are identical.

We start by precomputing the function g, which is repre-
sented as an N×L index matrix I. Elements of I are indices
of the corresponding tangent-plane nearest-neighbors in the
point cloud. Using I, we gather input signals (features) into
an intermediate tensor M of size N ×L×Cin. This tensor
is convolved with a ﬂattened set of kernels W of size 1×L,
which yields the output feature map Fout. This process is
illustrated in Figure 4.

Consider now the case of signal interpolation using
Gaussian kernel mixtures. For efﬁciency, we only consider
the set of top-k neighbors for each point, denoted N Nk. An
example image produced using the Gaussian kernel mixture
scheme with top-3 neighbors is shown in Figure 3(d). Equa-
tion (5) turns into

w(u, v) =

(cid:16)

− (cid:107)u−v(cid:107)2
σ2

(cid:17)

(cid:40) 1

A exp
0

if v ∈ N Nk

otherwise,

(8)

where A normalizes weights such that (cid:80)
v w = 1. With
this approximation, each pixel u has at most k non-zero
weights, denoted by w1..k(u). Their corresponding selec-
tion functions are denoted by g1..k(u). Both the weights
and the selection functions are independent of the signal F ,

and are thus precomputed. Equation (6) becomes

X(p) =

(cid:88)

(cid:16)

c(u) ·

(cid:0)wi(u) · F (gi(u))(cid:1)(cid:17)

(9)

k
(cid:88)

i=1

u

k
(cid:88)

(cid:88)

(cid:16)

i=1

u

=

wi(u) · c(u) · F (gi(u))

(10)

(cid:17)

.

signal

As with the nearest-neighbor

interpolation
scheme, we represent the precomputed selection functions
gi as k index matrices Ii of size N ×L. These index matri-
ces are used to assemble k intermediate signal tensors Mi
of size N ×L×Cin. Additionally, we collate the precom-
puted weights into k weight matrices Hi of size N×L. They
are used to compute the weighted sum M = (cid:80)
i Hi (cid:12) Mi,
which is ﬁnally convolved with the kernel W .

We implemented1 the presented construction in Tensor-
Flow [1]. It consists entirely of differentiable atomic oper-
ations, thus backpropagation is done seamlessly using the
automatic differentiation functionality of the framework.

5. Additional Ingredients

In this section we introduce additional ingredients that
are required to construct a convolutional network for point
cloud analysis.

5.1. Multi-scale analysis

Pooling. Convolutional networks commonly use pooling
to aggregate signals over larger spatial regions. We imple-
ment pooling in our framework via hashing onto a regular
3D grid. Points that are hashed onto the same grid point
pool their signals. The spacing of the grid determines the
pooling resolution. Consider points P = {p} and corre-
sponding signal values {F (p)}. Let g be a grid point and
let Vg be the set of points in P that hash to g. (The hash
function can be assumed to be simple quantization onto the
grid in each dimension.) Assume that Vg is not empty and
consider average pooling. All points in Vg and their signals
are pooled onto a single point:

p(cid:48)

g =

1
|Vg|

(cid:88)

p∈Vg

p

and

F (cid:48)(p(cid:48)

g) =

F (p).

(11)

1
|Vg|

(cid:88)

p∈Vg

1https://github.com/tatarchm/tangent_conv

In a convolutional network based on tangent convolutions,
we pool using progressively coarser grids. Starting with
some initial grid resolution (5cm in each dimension, say),
each successive pooling layer increases the step of the grid
by a factor of two (to 10cm, then 20cm, etc.). Such hashing
also alleviates the problem of non-uniform point density.
As a result, we can select the neighborhood radius for the
convolution operation globally for the entire dataset.

After each pooling layer, the radius r that is used to esti-
mate the tangent plane and the pixel size of the virtual tan-
gent image are doubled accordingly. Thus the resolution of
all tangent images decreases in step with the resolution of
the point cloud. Note that the downsampled point clouds
produced by pooling layers are independent of the signals
deﬁned over them. The downsampled point clouds, the as-
sociated tangent planes, and the corresponding index and
weight functions can thus all be precomputed for all layers
in the convolutional network: they need only be computed
once per pooling layer.

The implementation of a pooling layer is similar in spirit
to that of a convolutional layer described in Section 4. Con-
sider an input feature map Fin of size Nin ×C. Using grid
hashing, we assemble an index matrix I of size Nout × 8,
which contains indices of points that hash to the same grid
point. Assuming that we decrease the grid resolution by
a factor of 2 in each dimension in each pooling layer, the
number of points that hash to the same grid point will be
at most 8 in general.
(For initialization, we quantize the
points to some base resolution.) Using I, we assemble an
intermediate tensor of size Nout×8×C. We pool this tensor
along the second dimension according to the pooling oper-
ator (max, average, etc.), and thus obtain an output feature
map Fout of size Nout ×C.

Note that all stages in this process have linear complexity
in the number of points. Although points are hashed onto
regular grids, the grids themselves are never constructed or
represented. Hashing is performed via modular arithmetic
on individual point coordinates, and all data structures have
linear complexity in the number of points, independent of
the extent of the point set or the resolution of the grid.

Unpooling. The unpooling operation has an opposite ef-
fect to pooling:
it distributes signals from points in a
low-resolution feature map Fin onto points in a higher-
resolution feature map Fout. Unpooling reuses the index
matrix from the corresponding pooling operation. We copy
features from a single point in a low-resolution point cloud
to multiple points from which the information was aggre-
gated during pooling.

5.2. Local distance feature

So far, we have considered signals that could be ex-
pressed in terms of a scalar function F (q) with a well-
deﬁned value for each point q. This holds for color, in-

m 32 32

32

32

64 n

32 64 64

128

64 32

pool

unpool

skip

64

128

64

Figure 5. We use a fully-convolutional U-shaped network with
skip connections. The network receives m-dimensional features
as input and produces prediction scores for n classes.

tensity, and abstract ConvNet features. There is, however,
a signal that cannot be expressed in such terms and needs
special treatment. This signal is distance to the tangent
plane πp. This local signal is calculated by taking the
distance from each neighbor q to the tangent plane of p:
d = (q − p)(cid:62)np.

This signal is deﬁned in relation to the point p, there-
fore it cannot be directly plugged into the pipeline shown
in Figure 4. Instead, we precompute the distance images
for every point. Scattered signal interpolation is done in the
same way as for scalar signals (Equation (3)). After assem-
bling the intermediate tensor M for the ﬁrst convolutional
layer, we simply concatenate these distance images as an
additional channel in M. The ﬁrst convolutional layer gen-
erates a set of abstract features Fout that can be treated as
scalar signals from here on.
precomputations

implemented

using

All

are

Open3D [61].

6. Architecture

Using the ingredients introduced in the previous sec-
tions, we design an encoder-decoder network inspired by
the U-net [44]. The network architecture is illustrated in
Figure 5. It is a fully-convolutional network over a point
cloud, where the convolutions are tangent convolutions.
The encoder contains two pooling layers. The decoder con-
tains two corresponding unpooling layers. Encoder features
are propagated to corresponding decoder blocks via skip-
connections. All layers except the last one use 3 × 3 ﬁl-
ters and are followed by Leaky ReLU with negative slope
0.2 [31]. The last layer uses 1×1 convolutions to produce
ﬁnal class predictions. The network is trained by optimizing
the cross-entropy objective using the Adam optimizer with
initial learning rate 10−4 [24].
Receptive ﬁeld. The receptive ﬁeld size of one convolu-
tional layer is determined by the pixel size r of the tangent
image and the radius R that is used to collect the neighbors

of each point p. We set R = 2r, therefore the receptive
ﬁeld size of one layer is R. After each pooling layer, r is
doubled. The receptive ﬁeld of an element in the network
can be calculated by tracing the receptive ﬁelds of preced-
ing layers. With initial r = 5cm, the receptive ﬁeld size of
elements in the ﬁnal layer of the presented architecture is
4 · 10 + 4 · 20 + 2 · 40 = 200cm.

7. Experiments

We evaluate the performance of the presented approach
on the task of semantic 3D scene segmentation. Our ap-
proach is compared to recent deep networks for 3D data on
three different datasets.

7.1. Datasets and measures

We conduct experiments on three large-scale datasets
that contain real-world 3D scans of indoor and outdoor en-
vironments.

Semantic3D [17] is a dataset of scanned outdoor scenes
with over 3 billion points. It contains 15 training and 15 test
scenes annotated with 8 class labels. Being unable to evalu-
ate the baseline results on the ofﬁcial test server, we use our
own train/test split: Bildstein 1-3-5 are used for testing, the
rest for training.

Stanford Large-Scale 3D Indoor Spaces Dataset
(S3DIS) [3] contains 6 large-scale indoor areas from 3
different buildings, with 13 object classes. We use Area 5
for testing and the rest for training.

ScanNet [10] is a dataset with more than 1,500 scans of
indoor scenes with 20 object classes collected using an
RGB-D capture system. We follow the standard train/test
split provided by the authors.

Measures. We report three measures: mean accuracy over
classes (mA), mean intersection over union (mIoU), and
overall accuracy (oA). We build a full confusion matrix
based on the entire test set, and derive the ﬁnal scores from
it. Measures are evaluated over the original point clouds.
For approaches that produce labels over downsampled or
voxelized representations, we map these predictions to the
original point clouds via nearest-neighbor assignment.

Although we report oA for completeness, it is not a good
measure for semantic segmentation. If there are dominant
classes in the data (e.g., walls, ﬂoor, and ceiling in indoor
scenes), making correct predictions for these but poor pre-
dictions over the other classes will yield misleadingly high
oA scores.

7.2. Baselines

We compare our approach to three recent deep learn-
ing methods that operate on different underlying represen-
tations. We have chosen reasonably general methods that

have the potential to be applied to general scene analy-
sis and have open-source implementations. Our baselines
are PointNet [39], which operates on points, ScanNet [10],
which operates on low-resolution voxel grids, and Oct-
Net [43], which operates on higher-resolution octrees. We
used the source code provided by the authors. Due to the de-
sign of these methods, the data preparation routines and the
input signals are different for each dataset, and sometimes
deviate from the guidelines provided in the papers.

PointNet. For indoor datasets, we used the data sampling
strategy suggested in the original paper with global xyz, lo-
cally normalized xyz, and RGB as inputs. For Semantic3D,
we observed global xyz to be harmful, thus we only use lo-
cal xyz and color. Training data is generated by randomly
sampling (3m)3 cubes from the training scenes. Evaluation
is performed by applying a sliding window over the entire
scan.

ScanNet. The original network used 2 input channels: oc-
cupancy and a visibility mask computed using known cam-
era trajectories. Since scenes in general are not accompa-
nied by known camera trajectories, we only use occupancy
in the input signal. Following the original setup, we use
1.5×1.5×3m volumes voxelized into 31×31×62 grids and
augmented by 8 rotations. Each such cube yields a predic-
tion for one 1×1×62 column. (I.e., the ScanNet network
outputs a prediction for the central column only.) We use
random sampling for training, and exhaustive sliding win-
dow for testing.
OctNet. We use an architecture that operates on 2563 oc-
trees.
Inputs to the network are color, occupancy, and a
height-based feature that assigns each point to the top or
bottom part of the scan. Based on correspondence with the
authors regarding the best way to set up OctNet on differ-
ent datasets, we used (45m)3 volumes for Semantic3D and
(11m)3 volumes for the indoor datasets.

7.3. Setup of the presented approach

The architecture described in Section 6 is used in all ex-
periments. We evaluate four variants that use different in-
put signals: distance from tangent plane (D), height above
ground (H), normals (N), and color (RGB). All input signals
are normalized between 0 and 1. The initial resolution r of
the tangent image is 5cm for the indoor datasets and 10cm
for Semantic3D. It is doubled after each pooling layer. In
addition to providing the distance from tangent plane as in-
put to the ﬁrst convolutional layer, we concatenate the local
distance features from all scales of the point cloud to the
feature maps of the corresponding resolution produced by
pooling layers.

For ScanNet and S3DIS, we used whole rooms as indi-
vidual training batches. For Semantic3D, each batch was
a random sphere with a radius of 6m. For indoor scans,

we augment each scan by 8 rotations around the vertical
axis. To correct for imbalance between different classes,
we weigh the loss with the negative log of the training data
histogram.

7.4. Signal interpolation

We begin by comparing the effectiveness of two different
signal interpolation schemes: nearest neighbor and Gaus-
sian mixture. Both networks were trained on S3DIS with
D and H as the input signals. The resulting segmentation
scores are provided in the supplement. The two networks
produce similar results. We conclude that the nearest neigh-
bor signal estimation scheme is sufﬁcient, and use it in all
other experiments.

7.5. Main results

Quantitative results for all methods are summarized in
Table 2. Overall, our method produces high scores on all
datasets and consistently outperforms the baselines. Quali-
tative comparisons are shown in Figure 6.

Comparing the conﬁgurations of our networks that use
different input signals, we can see that geometry is much
more important than color on the indoor datasets. Adding
RGB information only slightly improves the scores on
S3DIS and is actually harmful for mean and overall accu-
racy on the ScanNet dataset. The situation is different for
the Semantic3D dataset: the network trained with color sig-
niﬁcantly outperforms all other conﬁgurations. Due to the
fact that H is normalized between 0 and 1 for every scan
separately, this information turns out to be harmful when
the global height of different scans is signiﬁcantly different.
Therefore, the network trained only with the distance signal
performs better than the other two geometric conﬁgurations.
In setting up and operating the baseline methods, we
found that all of them are quite hard to apply across datasets:
some non-trivial decisions had to be made for each new
dataset during the data preparation stage. None of the base-
lines showed consistent performance across the different
types of scenes.

PointNet reaches high oA scores on both indoor datasets.
However, the oA measure is strongly dominated by large
classes such as walls, ﬂoor, and ceiling. S3DIS has a fairly
regular layout because of the global room alignment pro-
cedure, which is very beneﬁcial for PointNet and allows it
to reach reasonable mA and mIoU scores on this dataset.
However, PointNet performs poorly on the ScanNet dataset,
which has more classes and noisy data. All but the most
prominent classes (i.e., walls and ﬂoor) are misclassiﬁed.
PointNet completely fails to produce meaningful predic-
tions on the even more challenging Semantic3D dataset.

Our conﬁguration of the ScanNet method produces rea-
sonable oA scores on both indoor datasets, but does much
worse in the other two measures. For reference, on the

ScanNet dataset we additionally report the number from the
original paper where a binary visibility-from-camera mask
was used as an additional input channel. This number is
much higher than our occupancy-only results, which do not
assume a known camera trajectory. Due to the fact that the
network only outputs predictions for the central column of
the voxel grid, evaluation is extremely time-consuming for
the large scenes in the Semantic3D dataset. Because of this
scalability issue, we did not succeed in evaluating ScanNet
on this dataset.

OctNet reaches good performance on the Semantic3D
dataset. However, the same network conﬁguration yields
bad results when applied to the indoor datasets. A possi-
ble explanation for this may be poor generalization due to
overﬁtting to the structure of training octrees.

7.6. Efﬁciency

We compared the efﬁciency of different methods on a
scan from S3DIS containing 125K points after grid hash-
ing. The results are reported in Table 1. Since ScanNet
and PointNet require multiple iterations for labeling a single
scan, we report both the time of a single forward pass and
the time for processing a full scan. OctNet and our method
process a full scan in one forward pass, which also ex-
plains their higher memory consumption compared to Scan-
Net and PointNet. ScanNet does not provide code for data
preprocessing, so we report the runtime of our Python im-
plementation needed for generating 38K sliding windows
during inference. Our method exhibits the best runtime for
both precomputation and inference.

Prep (s)

FP (s)

Full (s) Mem (GB)

PointNet
OctNet
ScanNet

Ours

16.5
15.5
867.8

1.59

0.01
0.61
0.002

0.52

0.65
0.61
6.34

0.52

0.39
3.33
0.97

2.35

Table 1. Efﬁciency of different methods. We report preprocessing
time (Prep), time for a single forward pass (FP), time for process-
ing a full scan (Full), and memory consumption (Mem).

8. Conclusion

We have presented tangent convolutions – a new con-
struction for convolutional networks on 3D data. The key
idea is to evaluate convolutions on virtual tangent planes at
every point. Crucially, tangent planes can be precomputed
and deep convolutional networks based on tangent convo-
lutions can be evaluated efﬁciently on large point clouds.
We have applied tangent convolutions to semantic segmen-
tation of large indoor and outdoor scenes. The presented
ideas may also be applicable to other problems in analysis,
processing, and synthesis of 3D data.

Semantic3D [17]

ScanNet [10]

S3DIS [3]

mIoU

3.76
50.7
n/a

58.1
58.0
52.5
66.4

mA

16.9
71.3
n/a

78.9
75.8
79.3
80.7

oA

16.3
80.7
n/a

84.8
83.3
79.5
89.3

mIoU

mA

oA

mIoU

12.2
18.1
13.5

40.9
40.3
40.7
40.9

17.9
26.4
19.2 (50.8)

68.1
76.6
69.4 (73.0)

52.5
52.2
55.3
55.1

80.9
80.6
80.3
80.1

41.3
26.3
24.6

49.8
50.0
51.7
52.8

mA

49.5
39.0
35.0

60.3
60.0
61.0
62.2

oA

78.8
68.9
64.2

80.2
81.2
82.2
82.5

PointNet [39]
OctNet [43]
ScanNet [10]

Ours (D)
Ours (DH)
Ours (DHN)
Ours (DHNRGB)

Table 2. Semantic segmentation accuracy for all methods across the three datasets. We report mean intersection over union (mIoU), mean
class accuracy (mA), and overall accuracy (oA). Note that oA is a bad measure and we recommend against using it in the future. We tested
different conﬁgurations of our method by combining four types of input signals: depth (D), height (H), normals (N), and color (RGB).

Color

PointNet [39]

ScanNet [10]

OctNet [43]

Ours (DHNRGB)

Ground truth

(cid:108) Ceiling (cid:108) Floor (cid:108) Walls (cid:108) Column (cid:108) Door (cid:108) Table (cid:108) Chair (cid:108) Sofa (cid:108) Bookcase (cid:108) Board (cid:108) Clutter

Color

OctNet [43]

Ours (DHNRGB)

Ground truth

(cid:108) Man made terrain (cid:108) Natural terrain (cid:108) High vegetation (cid:108) Low vegetation (cid:108) Building (cid:108) Hardscape (cid:108) Scanning artifacts (cid:108) Cars

Figure 6. Qualitative comparisons on S3DIS [3] (top) and Semantic3D [17] (bottom). Labels are coded by color.

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
et al. TensorFlow: A system for large-scale machine learn-
ing. In OSDI, 2016.

[2] A. Anand, H. S. Koppula, T. Joachims, and A. Saxena.
Contextually guided semantic labeling and search for three-
dimensional point clouds. International Journal of Robotics
Research, 32(1), 2013.

[3] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,
M. Fischer, and S. Savarese. 3D semantic parsing of large-
scale indoor spaces. In CVPR, 2016.

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. M. Bronstein.
Learning shape correspondence with anisotropic convolu-
tional neural networks. In NIPS, 2016.

[5] A. Boulch, B. L. Saux, and N. Audebert. Unstructured point
cloud semantic labeling using deep segmentation networks.
In Eurographics Workshop on 3D Object Retrieval, 2017.
[6] A. P. Charaniya, R. Manduchi, and S. K. Lodha. Supervised
In CVPR

parametric classiﬁcation of aerial LiDAR data.
Workshops, 2004.

[7] N. Chehata, L. Guo, and C. Mallet. Contribution of airborne
full-waveform lidar and image data for urban scene classiﬁ-
cation. In ICIP, 2009.

[9]

[8] H. Chen, Q. Dou, L. Yu, and P. Heng. VoxResNet: Deep vox-
elwise residual networks for volumetric brain segmentation.
arXiv:1608.05895, 2016.
¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and
O. Ronneberger. 3D U-Net: Learning dense volumetric seg-
mentation from sparse annotation. In MICCAI, 2016.
[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A.
Funkhouser, and M. Nießner. ScanNet: Richly-annotated
3D reconstructions of indoor scenes. In CVPR, 2017.
[11] G. Floros and B. Leibe. Joint 2D-3D temporally consistent
semantic segmentation of street scenes. In CVPR, 2012.
[12] A. Frome, D. Huber, R. Kolluri, T. B¨ulow, and J. Malik. Rec-
ognizing objects in range data using regional point descrip-
tors. In ECCV, 2004.

[13] M. Gadelha, S. Maji, and R. Wang. 3D shape generation
using spatially ordered point clouds. In BMVC, 2017.
[14] M. Gadelha, S. Maji, and R. Wang. 3D shape induction from

2D views of multiple objects. In 3DV, 2017.

[15] A. Golovinskiy, V. G. Kim, and T. A. Funkhouser. Shape-
based recognition of 3D point clouds in urban environments.
In ICCV, 2009.

[16] S. Gupta, P. A. Arbel´aez, R. B. Girshick, and J. Malik. In-
door scene understanding with RGB-D images: Bottom-up
segmentation, object detection and semantic segmentation.
IJCV, 112(2), 2015.

[17] T. Hackel, N. Savinov, L. Ladicky,

J. D. Wegner,
K. Schindler, and M. Pollefeys. Semantic3D.net: A new
large-scale point cloud classiﬁcation benchmark. In ISPRS
Annals of the Photogrammetry, Remote Sensing and Spatial
Information Sciences, 2017.

[18] C. H¨ane, S. Tulsiani, and J. Malik. Hierarchical surface pre-

diction for 3D object reconstruction. In 3DV, 2017.

[19] A. Hermans, G. Floros, and B. Leibe. Dense 3D semantic
In ICRA,

mapping of indoor scenes from RGB-D images.
2014.

[20] A. E. Johnson and M. Hebert. Using spin images for efﬁ-
cient object recognition in cluttered 3D scenes. PAMI, 21(5),
1999.

[21] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri. 3D
shape segmentation with projective convolutional networks.
In CVPR, 2017.

[22] A. Kar, C. H¨ane, and J. Malik. Learning a multi-view stereo

machine. In NIPS, 2017.

[23] M. Khoury, Q.-Y. Zhou, and V. Koltun. Learning compact

geometric features. In ICCV, 2017.

[24] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015.

[25] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-
networks for the recognition of 3D point cloud models. In
ICCV, 2017.

[26] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
In NIPS,

connected CRFs with Gaussian edge potentials.
2011.

[27] A. Kundu, Y. Li, F. Dellaert, F. Li, and J. M. Rehg. Joint se-
mantic segmentation and 3D reconstruction from monocular
video. In ECCV, 2014.

[28] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: Field
probing neural networks for 3D data. In NIPS, 2016.
[29] Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin. LSTM-
CF: Unifying context modeling and fusion with LSTMs for
RGB-D scene labeling. In ECCV, 2016.

[30] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang.
3D shape reconstruction from sketches via multi-view con-
volutional networks. In 3DV, 2017.

[31] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectiﬁer nonlin-
earities improve neural network acoustic models. In ICML
Workshops, 2013.

[32] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym,
E. Yumer, V. G. Kim, and Y. Lipman. Convolutional neu-
ral networks on surfaces via seamless toric covers. ACM
Transactions on Graphics, 36(4), 2017.

[33] A. Martinovic, J. Knopp, H. Riemenschneider, and L. J. Van
Gool. 3D all the way: Semantic segmentation of urban
scenes from start to end in 3D. In CVPR, 2015.

[34] J. Masci, D. Boscaini, M. M. Bronstein, and P. Van-
dergheynst. Geodesic convolutional neural networks on Rie-
mannian manifolds. In ICCV Workshops, 2015.

[35] D. Maturana and S. Scherer. VoxNet: A 3D convolutional
In IROS,

neural network for real-time object recognition.
2015.

[36] J. McCormac, A. Handa, A. J. Davison, and S. Leutenegger.
SemanticFusion: Dense 3D semantic mapping with convo-
lutional neural networks. In ICRA, 2017.

[37] O. Miksik, V. Vineet, M. Lidegaard, R. Prasaath, M. Nießner,
S. Golodetz, S. L. Hicks, P. P´erez, S. Izadi, and P. H. S. Torr.
The semantic paintbrush: Interactive 3D mapping and recog-
nition in large outdoor spaces. In CHI, 2015.

[38] D. Munoz, N. Vandapel, and M. Hebert. Onboard contextual
classiﬁcation of 3-D point clouds with learned high-order
Markov random ﬁelds. In ICRA, 2009.

[39] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In CVPR, 2017.

[40] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view CNNs for object clas-
siﬁcation on 3D data. In CVPR, 2016.

[41] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun. 3D graph
neural networks for RGBD semantic segmentation. In ICCV,
2017.

[42] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger. Oct-
NetFusion: Learning depth fusion from data. In 3DV, 2017.
[43] G. Riegler, A. O. Ulusoy, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In CVPR, 2017.
[44] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, 2015.

[45] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature
histograms (FPFH) for 3D registration. In ICRA, 2009.
[46] S. Salti, F. Tombari, and L. di Stefano. SHOT: Unique signa-
tures of histograms for surface and texture description. Com-
puter Vision and Image Understanding, 125, 2014.

[47] M. Simonovsky and N. Komodakis.

Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In CVPR, 2017.

[48] A. Sinha, J. Bai, and K. Ramani. Deep learning 3D shape

surfaces using geometry images. In ECCV, 2016.

[49] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3D shape
recognition. In ICCV, 2015.

[50] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3D
models from single images with a convolutional network. In
ECCV, 2016.

[51] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3D outputs. In ICCV, 2017.

[52] L. P. Tchapmi, C. B. Choy, I. Armeni, J. Gwak, and
S. Savarese. SEGCloud: Semantic segmentation of 3D point
clouds. In 3DV, 2017.

[53] F. Tombari, S. Salti, and L. Di Stefano. Unique shape context
In ACM Workshop on 3D Object

for 3D data description.
Retrieval, 2010.

[54] S. Tulsiani, H. Su, L. J. Guibas, A. A. Efros, and J. Malik.
Learning shape abstractions by assembling volumetric prim-
itives. In CVPR, 2017.

[55] J. P. C. Valentin, V. Vineet, M. Cheng, D. Kim, J. Shotton,
P. Kohli, M. Nießner, A. Criminisi, S. Izadi, and P. H. S. Torr.
SemanticPaint: Interactive 3D labeling and learning at your
ﬁngertips. ACM Transactions on Graphics, 34(5), 2015.
[56] V. Vineet, O. Miksik, M. Lidegaard, M. Nießner,
S. Golodetz, V. A. Prisacariu, O. K¨ahler, D. W. Murray,
S. Izadi, P. P´erez, and P. H. S. Torr. Incremental dense se-
mantic stereo fusion for large-scale semantic scene recon-
struction. In ICRA, 2015.

[57] P. Wang, Y. Liu, Y. Guo, C. Sun, and X. Tong. O-CNN:
Octree-based convolutional neural networks for 3D shape
analysis. ACM Transactions on Graphics, 36(4), 2017.
[58] C. Wu, I. Lenz, and A. Saxena. Hierarchical semantic label-
ing for task-relevant RGB-D perception. In RSS, 2014.
[59] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumetric
shapes. In CVPR, 2015.

[60] L. Yi, H. Su, X. Guo, and L. J. Guibas. SyncSpecCNN:
Synchronized spectral CNN for 3D shape segmentation. In
CVPR, 2017.

[61] Q.-Y. Zhou, J. Park, and V. Koltun. Open3D: A modern li-
brary for 3D data processing. arXiv:1801.09847, 2018.

D. Qualitative results

We provide more qualitative results of our method on

different datasets in Figures 7-9.

Appendix

A. Robustness to noise

We evaluated the robustness of our approach to noise.
Several instances of our network were trained on the S3DIS
dataset perturbed with different amounts of additive Gaus-
sian noise with standard deviation σ. The results are re-
ported in Table 3. We selected small subsets of the data for
training and testing (Area 1 for training and Area 5 for test-
ing), which is why the ﬁnal performance numbers are not
compatible with those reported in the main paper.

a
t
a
d

g
n
i
n
i
a
r
T

σ, m

OA

0.00

0.59

0.02

0.63

0.04

0.63

0.08

0.68

0.16

0.17

Table 3. Performance evaluation with different levels of noise.

Surprisingly, reasonable amounts of noise improve over-
all accuracy. The method only suffers if the noise severely
damages semantic structure in the point cloud. We did not
tune any parameters in the pipeline for these experiments.

B. Signal interpolation

In this experiment we compare the effectiveness of two
signal interpolation schemes: nearest neighbor and Gaus-
sian mixture. Quantitative results on S3DIS using D and H
as input signals are presented in Table 4. Both methods pro-
duce very similar results which is why the simpler nearest
neighbor interpolation is used throughout the paper.

Signal

NN
Gaussian

mIoU

50.0
50.7

mA

60.0
59.6

oA

81.2
81.3

Table 4. Signal interpolation using the nearest neighbor scheme
and the Gaussian mixture scheme produce similar results.

C. Comparison with SnapNet

We also compared our approach with the SnapNet by
Boulch et al. [5]. They project a 3D scene onto a set of
2D images. Those images are then segmented with a regu-
lar 2D ConvNet. The main strength of this approach is the
possibility to combine it with transfer learning and use the
weights of a network pre-trained on ImageNet for initializa-
tion. Applying this strategy yields the mIoU score of 67.7
on the Semantic3D datset, compared to 66.4 produced by
our approach. However, the non-trivial camera pose sam-
pling procedure required by SnapNet did not allow us to
apply it to indoor datasets.

Color

Prediction

Ground truth

(cid:108) Ceiling (cid:108) Floor (cid:108) Walls (cid:108) Column (cid:108) Window (cid:108) Door (cid:108) Table (cid:108) Chair (cid:108) Sofa (cid:108) Bookcase (cid:108) Board (cid:108) Clutter

Figure 7. Qualitative results on S3DIS [3].

Color

Prediction

Ground truth

(cid:108) Wall (cid:108) Floor (cid:108) Cabinet (cid:108) Bed (cid:108) Chair (cid:108) Sofa (cid:108) Table (cid:108) Door (cid:108) Window (cid:108) Bookshelf (cid:108) Desk (cid:108) Other furniture

Figure 8. Qualitative results on ScanNet [10].

Color

Prediction

Ground truth

(cid:108) Man made terrain (cid:108) Natural terrain (cid:108) High vegetation (cid:108) Low vegetation (cid:108) Building (cid:108) Hardscape (cid:108) Scanning artifacts (cid:108) Cars

Figure 9. Qualitative results on Semantic3D [17].

Tangent Convolutions for Dense Prediction in 3D

Maxim Tatarchenko∗
University of Freiburg

Jaesik Park∗
Intel Labs

Vladlen Koltun
Intel Labs

Qian-Yi Zhou
Intel Labs

8
1
0
2
 
l
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
3
4
4
2
0
.
7
0
8
1
:
v
i
X
r
a

Abstract

We present an approach to semantic scene analysis us-
ing deep convolutional networks. Our approach is based on
tangent convolutions – a new construction for convolutional
networks on 3D data. In contrast to volumetric approaches,
our method operates directly on surface geometry. Cru-
cially, the construction is applicable to unstructured point
clouds and other noisy real-world data. We show that tan-
gent convolutions can be evaluated efﬁciently on large-scale
point clouds with millions of points. Using tangent convo-
lutions, we design a deep fully-convolutional network for
semantic segmentation of 3D point clouds, and apply it to
challenging real-world datasets of indoor and outdoor 3D
environments. Experimental results show that the presented
approach outperforms other recent deep network construc-
tions in detailed analysis of large 3D scenes.

1. Introduction

Methods that utilize convolutional networks on 2D im-
ages dominate modern computer vision. A key contributing
factor to their success is efﬁcient local processing based on
the convolution operation. 2D convolution is deﬁned on a
regular grid, a domain that supports extremely efﬁcient im-
plementation. This in turn enables using powerful deep ar-
chitectures for processing large datasets at high resolution.
When it comes to analysis of large-scale 3D scenes, a
straightforward extension of this idea is volumetric convo-
lution on a voxel grid [35, 59, 10]. However, voxel-based
methods have limitations, including a cubic growth rate
of memory consumption and computation time. For this
reason, voxel-based ConvNets operate on low-resolution
voxel grids that limit their prediction accuracy. The prob-
lem can be alleviated by octree-based techniques that de-
ﬁne a ConvNet on an octree and enable processing some-
what higher-resolution volumes (e.g., up to 2563 vox-
els) [43, 57, 18, 42, 51]. Yet even this may be insufﬁcient
for detailed analysis of large-scale scenes.

On a deeper level, both efﬁcient and inefﬁcient voxel-
based methods treat 3D data as volumetric by exploiting 3D

∗Equal contribution.

Figure 1. Convolutional networks based on tangent convolutions
can be applied to semantic analysis of large-scale scenes, such
as urban environments. Top: point cloud from the Semantic3D
dataset. Bottom: semantic segmentation produced by the pre-
sented approach.

convolutions that integrate over volumes.
In reality, data
captured by 3D sensors such as RGB-D cameras and Li-
DAR typically represent surfaces: 2D structures embedded
in 3D space. (This is in contrast to truly volumetric 3D data,
as encountered for example in medical imaging.) Classic
features that are used for the analysis of such data are de-
ﬁned in terms that acknowledge the latent surface structure,
and do not treat the data as a volume [20, 12, 45].

The drawbacks of voxel-based methods are known in the
research community. A number of recent works argue that
volumetric data structures are not the natural substrate for
3D ConvNets, and propose alternative designs based on un-
ordered point sets [39], graphs [47], and sphere-type sur-
faces [32]. Unfortunately, these methods come with their
own drawbacks, such as limited sensitivity to local struc-
ture or restrictive topological assumptions.

We develop an alternative construction for convolutional
networks on surfaces, based on the notion of tangent con-

volution. This construction assumes that the data is sam-
pled from locally Euclidean surfaces. The latent surfaces
need not be known, and the data can be in any form that
supports approximate normal vector estimation, including
point clouds, meshes, and even polygon soup. (The same
assumption concerning normal vector estimation is made by
both classic and contemporary geometric feature descrip-
tors [20, 12, 45, 53, 46, 23].) The tangent convolution is
based on projecting local surface geometry on a tangent
plane around every point. This yields a set of tangent im-
ages. Every tangent image is treated as a regular 2D grid
that supports planar convolution. The content of all tan-
gent images can be precomputed from the surface geometry,
which enables efﬁcient implementation that scales to large
datasets, such as urban environments.

Using tangent convolution as the main building block,
we design a U-type network for dense semantic segmen-
tation of point clouds. Our proposed architecture is gen-
eral and can be applied to analysis of large-scale scenes.
We demonstrate its performance on three diverse real-world
datasets containing indoor and outdoor environments. A se-
mantic segmentation produced by a tangent convolutional
network is shown in Figure 1.

2. Related Work

Dense prediction in 3D, including semantic point cloud
segmentation, has a long history in computer vision. Pi-
oneering methods work on aerial LiDAR data and are
based on hand-crafted features with complex classiﬁers on
top [6, 7, 15]. Such approaches can also be combined
with high-level architectural rules [33]. A popular line of
work exploits graphical models, including conditional ran-
dom ﬁelds [38, 11, 2, 58, 19, 27, 56]. Related formula-
tions have also been proposed for interactive 3D segmenta-
tion [55, 37].

More recently, the deep learning revolution in computer
vision has spread to consume 3D data analysis. A variety of
methods that tackle 3D data using deep learning techniques
have been proposed. They can be considered in terms of the
underlying data representation.

A common representation of 3D data for deep learning
is a voxel grid. Deep networks that operate on voxelized
data have been applied to shape classiﬁcation [35, 59, 40],
semantic segmentation of indoor scenes [10], and biomed-
ical recordings [9, 8]. Due to the cubic complexity of
voxel grids, these methods can only operate at low res-
olution – typically not more than 643 – and have lim-
ited accuracy. Attempting to overcome this limitation,
researchers have proposed representations based on hier-
archical spatial data structures such as octrees and kd-
trees [43, 57, 18, 42, 13, 25, 51], which are more memory-
and computation-efﬁcient, and can therefore handle higher
resolutions. An alternative way of increasing the accu-

racy of voxel-based techniques is to add differentiable post-
processing, modeled upon the dense CRF [26, 52].

Other applications of deep networks consider RGB-D
images, which can be treated with fully-convolutional net-
works [16, 29, 36] and graph neural networks [41]. These
approaches support the use of powerful pretrained 2D net-
works, but are not generally applicable to unstructured point
clouds with unknown sensor poses. Attempting to address
this issue, Boulch et al. [5] train a ConvNet on images ren-
dered from point clouds using randomly placed virtual cam-
eras. In a more controlled setting with ﬁxed camera poses,
multi-view methods are successfully used for shape seg-
mentation [21], shape recognition [49, 40], and shape syn-
thesis [14, 30, 22, 50]. Our approach can be viewed as an
extreme multi-view approach in which a virtual camera is
associated with each point in the point cloud. A critical
problem that we address is the efﬁcient and scalable imple-
mentation of this approach, which enables its application to
dense point clouds of large-scale indoor and outdoor envi-
ronments.

Qi et al. [39] propose a network for analysing unordered
point sets, which is based on independent point process-
ing combined with global context aggregation through max-
pooling. Since the communication between the points is
quite weak, this approach experiences difﬁculties when ap-
plied to large-scale scenes with complex layouts.

There is a variety of more exotic deep learning formula-
tions for 3D analysis that do not address large-scale seman-
tic segmentation of whole scenes but provide interesting
ideas. Yi et al. [60] consider shape segmentation in the spec-
tral domain by synchronizing eigenvectors across models.
Masci et al. [34] and Boscaini et al. [4] design ConvNets
for Riemannian manifolds and use them to learn shape cor-
respondences. Sinha et al. [48] perform shape analysis on
geometry images. Simonovsky et al. [47] extend the con-
volution operator from regular grids to arbitrary graphs and
use it to design shape classiﬁcation networks. Li et al. [28]
introduce Field Probing Neural Networks which respect the
underlying sparsity of 3D data and are used for efﬁcient fea-
ture extraction. Tulsiani et al. [54] approximate 3D models
with volumetric primitives in an end-to-end differentiable
framework, and use this representation for solving several
tasks. Maron et al. [32] design ConvNets on surfaces for
sphere-type shapes.

Overall, most existing 3D deep learning systems either
rely on representations that do not support general scene
analysis, or have poor scalability. As we will show, deep
networks based on tangent convolutions scale to millions of
points and are suitable for detailed analysis of large scenes.

3. Tangent Convolution

In this section we formally introduce tangent convolu-
tions. All derivations are provided for point clouds, but they

np

p

i

j

Figure 2. Points q (blue) from the local neighborhood of a point p
(red) are projected onto the tangent image.

can easily be applied to any type of 3D data that supports
surface normal estimation, such as meshes.
Convolution with a continuous kernel. Let P = {p} be a
point cloud, and let F (p) be a discrete scalar function that
represents a signal deﬁned over P. F (p) can encode color,
geometry, or abstract features from intermediate network
layers. In order to convolve F , we need to extend it to a
continuous function. Conceptually, we introduce a virtual
orthogonal camera for p. It is conﬁgured to observe p along
the normal np. The image plane of this virtual camera is
the tangent plane πp of p. It parameterizes a virtual image
that can be represented as a continuous signal S(u), where
u ∈ R2 is a point in πp. We call S a tangent image.

The tangent convolution at p is deﬁned as

X(p) =

c(u)S(u) du,

(1)

(cid:90)

πp

where c(u) is the convolution kernel. We now describe how
S is computed from F .
Tangent plane estimation. For each point p we estimate
the orientation of its camera image using local covariance
analysis. This is a standard procedure [46] but we sum-
marize it here for completeness. Consider a set of points q
from a spherical neighborhood of p, such that (cid:107)p−q(cid:107) < R.
The orientation of the tangent plane is determined by the
eigenvectors of the covariance matrix C = (cid:80)
q rr(cid:62), where
r = q − p. The eigenvector of the smallest eigenvalue de-
ﬁnes the estimated surface normal np, and the other two
eigenvectors i and j deﬁne the 2D image axes that parame-
terize the tangent image.
Signal interpolation. Now our goal is to estimate image
signals S(u) from point signals F (q). We begin by pro-
jecting the neighbors q of p onto the tangent image, which
yields a set of projected points v = (r(cid:62)i, r(cid:62)j). This is
illustrated in Figure 2. We deﬁne

(a)

(b)

(c)

(d)

Figure 3. Signals from projected points (a) can be interpolated us-
ing one of the following schemes: nearest neighbor (b), full Gaus-
sian mixture (c), and Gaussian mixture with top-3 neighbors (d).

(4)

(5)

on the image plane. We thus need to interpolate their signals
in order to estimate the full function S(u) over the tangent
image:

S(u) =

(cid:88)

(cid:0)w(u, v) · S(v)(cid:1),

(3)

v
where w(u, v) is a kernel weight that satisﬁes (cid:80)
v w = 1.
We consider two schemes for signal interpolation: nearest
neighbor and Gaussian kernel mixture. These schemes are
illustrated in Figure 3. In the nearest neighbor (NN) case,

w(u, v) =

(cid:40)

1
0

if v is u’s NN,
otherwise.

In the Gaussian kernel mixture case,

w(u, v) =

exp

−

1
A

(cid:18)

(cid:107)u − v(cid:107)2
σ2

(cid:19)

,

where A normalizes the weights such that (cid:80)
v w = 1. More
sophisticated signal interpolation schemes can be consid-
ered, but we have not observed a signiﬁcant effect of the
interpolation scheme on empirical performance and will
mostly use simple nearest-neighbor estimation.

Finally, if we rewrite Equation (1) using the deﬁnitions
from Equations (2) and (3), we get the formula for the tan-
gent convolution:

X(p) =

c(u) ·

(cid:0)w(u, v) · F (q)(cid:1) du.

(6)

(cid:90)

πp

(cid:88)

v

Note that the role of the tangent image is increasingly
it provides the domain for u and ﬁgures in the
implicit:
evaluation of the weights w, but otherwise it need not be ex-
plicitly maintained. We will build on this observation in the
next section to show that tangent convolutions can be eval-
uated efﬁciently at scale, and can support the construction
of deep networks on point clouds with millions of points.

4. Efﬁciency

S(v) = F (q).

(2)

As shown in Figure 2 and Figure 3(a), points v are scattered

In this section we describe how the tangent convolution
deﬁned in Section 3 can be computed efﬁciently. In prac-
tice, the tangent image is treated as a discrete function on a

regular l ×l grid. Elements u are pixels in this virtual im-
age. The convolution kernel c is a discrete kernel applied
onto this image. Let us ﬁrst consider the nearest-neighbor
signal interpolation scheme introduced in Equation (4). We
can rewrite Equation (6) as

Cin

Fin

N

N

L

I

Cin

L

M

N

g(u)

F (g(u))

conv

Cout

Fout

X(p) =

(cid:88)

(cid:16)

c(u) · F (cid:0)g(u)(cid:1)(cid:17)

,

u

(7)

Figure 4. Efﬁcient evaluation of a convolutional layer built on tan-
gent convolutions.

where g(u) is a selection function that returns a point which
projects to the nearest neighbor of u on the image plane.
Note that g only depends on the point cloud geometry and
does not depend on the signal F . This allows us to precom-
pute g for all points.

From here on, we employ standard ConvNet terminol-
ogy and proceed to show how to implement a convolutional
layer using tangent convolutions. Our goal is to convolve an
input feature map Fin of size N ×Cin with a set of weights
W to produce an output feature map Fout of size N ×Cout,
where N is the number of points in the point cloud, while
Cin and Cout denote the number of input and output chan-
nels respectively. For implementation, we unroll 2D tangent
images and convolutional ﬁlters of size l×l into 1D vectors
of size 1×L, where L = l2. From then on, we compute 1D
convolutions. Note that such representation of a 2D tangent
convolution as a 1D convolution is not an approximation:
the results of the two operations are identical.

We start by precomputing the function g, which is repre-
sented as an N×L index matrix I. Elements of I are indices
of the corresponding tangent-plane nearest-neighbors in the
point cloud. Using I, we gather input signals (features) into
an intermediate tensor M of size N ×L×Cin. This tensor
is convolved with a ﬂattened set of kernels W of size 1×L,
which yields the output feature map Fout. This process is
illustrated in Figure 4.

Consider now the case of signal interpolation using
Gaussian kernel mixtures. For efﬁciency, we only consider
the set of top-k neighbors for each point, denoted N Nk. An
example image produced using the Gaussian kernel mixture
scheme with top-3 neighbors is shown in Figure 3(d). Equa-
tion (5) turns into

w(u, v) =

(cid:16)

− (cid:107)u−v(cid:107)2
σ2

(cid:17)

(cid:40) 1

A exp
0

if v ∈ N Nk

otherwise,

(8)

where A normalizes weights such that (cid:80)
v w = 1. With
this approximation, each pixel u has at most k non-zero
weights, denoted by w1..k(u). Their corresponding selec-
tion functions are denoted by g1..k(u). Both the weights
and the selection functions are independent of the signal F ,

and are thus precomputed. Equation (6) becomes

X(p) =

(cid:88)

(cid:16)

c(u) ·

(cid:0)wi(u) · F (gi(u))(cid:1)(cid:17)

(9)

k
(cid:88)

i=1

u

k
(cid:88)

(cid:88)

(cid:16)

i=1

u

=

wi(u) · c(u) · F (gi(u))

(10)

(cid:17)

.

signal

As with the nearest-neighbor

interpolation
scheme, we represent the precomputed selection functions
gi as k index matrices Ii of size N ×L. These index matri-
ces are used to assemble k intermediate signal tensors Mi
of size N ×L×Cin. Additionally, we collate the precom-
puted weights into k weight matrices Hi of size N×L. They
are used to compute the weighted sum M = (cid:80)
i Hi (cid:12) Mi,
which is ﬁnally convolved with the kernel W .

We implemented1 the presented construction in Tensor-
Flow [1]. It consists entirely of differentiable atomic oper-
ations, thus backpropagation is done seamlessly using the
automatic differentiation functionality of the framework.

5. Additional Ingredients

In this section we introduce additional ingredients that
are required to construct a convolutional network for point
cloud analysis.

5.1. Multi-scale analysis

Pooling. Convolutional networks commonly use pooling
to aggregate signals over larger spatial regions. We imple-
ment pooling in our framework via hashing onto a regular
3D grid. Points that are hashed onto the same grid point
pool their signals. The spacing of the grid determines the
pooling resolution. Consider points P = {p} and corre-
sponding signal values {F (p)}. Let g be a grid point and
let Vg be the set of points in P that hash to g. (The hash
function can be assumed to be simple quantization onto the
grid in each dimension.) Assume that Vg is not empty and
consider average pooling. All points in Vg and their signals
are pooled onto a single point:

p(cid:48)

g =

1
|Vg|

(cid:88)

p∈Vg

p

and

F (cid:48)(p(cid:48)

g) =

F (p).

(11)

1
|Vg|

(cid:88)

p∈Vg

1https://github.com/tatarchm/tangent_conv

In a convolutional network based on tangent convolutions,
we pool using progressively coarser grids. Starting with
some initial grid resolution (5cm in each dimension, say),
each successive pooling layer increases the step of the grid
by a factor of two (to 10cm, then 20cm, etc.). Such hashing
also alleviates the problem of non-uniform point density.
As a result, we can select the neighborhood radius for the
convolution operation globally for the entire dataset.

After each pooling layer, the radius r that is used to esti-
mate the tangent plane and the pixel size of the virtual tan-
gent image are doubled accordingly. Thus the resolution of
all tangent images decreases in step with the resolution of
the point cloud. Note that the downsampled point clouds
produced by pooling layers are independent of the signals
deﬁned over them. The downsampled point clouds, the as-
sociated tangent planes, and the corresponding index and
weight functions can thus all be precomputed for all layers
in the convolutional network: they need only be computed
once per pooling layer.

The implementation of a pooling layer is similar in spirit
to that of a convolutional layer described in Section 4. Con-
sider an input feature map Fin of size Nin ×C. Using grid
hashing, we assemble an index matrix I of size Nout × 8,
which contains indices of points that hash to the same grid
point. Assuming that we decrease the grid resolution by
a factor of 2 in each dimension in each pooling layer, the
number of points that hash to the same grid point will be
at most 8 in general.
(For initialization, we quantize the
points to some base resolution.) Using I, we assemble an
intermediate tensor of size Nout×8×C. We pool this tensor
along the second dimension according to the pooling oper-
ator (max, average, etc.), and thus obtain an output feature
map Fout of size Nout ×C.

Note that all stages in this process have linear complexity
in the number of points. Although points are hashed onto
regular grids, the grids themselves are never constructed or
represented. Hashing is performed via modular arithmetic
on individual point coordinates, and all data structures have
linear complexity in the number of points, independent of
the extent of the point set or the resolution of the grid.

Unpooling. The unpooling operation has an opposite ef-
fect to pooling:
it distributes signals from points in a
low-resolution feature map Fin onto points in a higher-
resolution feature map Fout. Unpooling reuses the index
matrix from the corresponding pooling operation. We copy
features from a single point in a low-resolution point cloud
to multiple points from which the information was aggre-
gated during pooling.

5.2. Local distance feature

So far, we have considered signals that could be ex-
pressed in terms of a scalar function F (q) with a well-
deﬁned value for each point q. This holds for color, in-

m 32 32

32

32

64 n

32 64 64

128

64 32

pool

unpool

skip

64

128

64

Figure 5. We use a fully-convolutional U-shaped network with
skip connections. The network receives m-dimensional features
as input and produces prediction scores for n classes.

tensity, and abstract ConvNet features. There is, however,
a signal that cannot be expressed in such terms and needs
special treatment. This signal is distance to the tangent
plane πp. This local signal is calculated by taking the
distance from each neighbor q to the tangent plane of p:
d = (q − p)(cid:62)np.

This signal is deﬁned in relation to the point p, there-
fore it cannot be directly plugged into the pipeline shown
in Figure 4. Instead, we precompute the distance images
for every point. Scattered signal interpolation is done in the
same way as for scalar signals (Equation (3)). After assem-
bling the intermediate tensor M for the ﬁrst convolutional
layer, we simply concatenate these distance images as an
additional channel in M. The ﬁrst convolutional layer gen-
erates a set of abstract features Fout that can be treated as
scalar signals from here on.
precomputations

implemented

using

All

are

Open3D [61].

6. Architecture

Using the ingredients introduced in the previous sec-
tions, we design an encoder-decoder network inspired by
the U-net [44]. The network architecture is illustrated in
Figure 5. It is a fully-convolutional network over a point
cloud, where the convolutions are tangent convolutions.
The encoder contains two pooling layers. The decoder con-
tains two corresponding unpooling layers. Encoder features
are propagated to corresponding decoder blocks via skip-
connections. All layers except the last one use 3 × 3 ﬁl-
ters and are followed by Leaky ReLU with negative slope
0.2 [31]. The last layer uses 1×1 convolutions to produce
ﬁnal class predictions. The network is trained by optimizing
the cross-entropy objective using the Adam optimizer with
initial learning rate 10−4 [24].
Receptive ﬁeld. The receptive ﬁeld size of one convolu-
tional layer is determined by the pixel size r of the tangent
image and the radius R that is used to collect the neighbors

of each point p. We set R = 2r, therefore the receptive
ﬁeld size of one layer is R. After each pooling layer, r is
doubled. The receptive ﬁeld of an element in the network
can be calculated by tracing the receptive ﬁelds of preced-
ing layers. With initial r = 5cm, the receptive ﬁeld size of
elements in the ﬁnal layer of the presented architecture is
4 · 10 + 4 · 20 + 2 · 40 = 200cm.

7. Experiments

We evaluate the performance of the presented approach
on the task of semantic 3D scene segmentation. Our ap-
proach is compared to recent deep networks for 3D data on
three different datasets.

7.1. Datasets and measures

We conduct experiments on three large-scale datasets
that contain real-world 3D scans of indoor and outdoor en-
vironments.

Semantic3D [17] is a dataset of scanned outdoor scenes
with over 3 billion points. It contains 15 training and 15 test
scenes annotated with 8 class labels. Being unable to evalu-
ate the baseline results on the ofﬁcial test server, we use our
own train/test split: Bildstein 1-3-5 are used for testing, the
rest for training.

Stanford Large-Scale 3D Indoor Spaces Dataset
(S3DIS) [3] contains 6 large-scale indoor areas from 3
different buildings, with 13 object classes. We use Area 5
for testing and the rest for training.

ScanNet [10] is a dataset with more than 1,500 scans of
indoor scenes with 20 object classes collected using an
RGB-D capture system. We follow the standard train/test
split provided by the authors.

Measures. We report three measures: mean accuracy over
classes (mA), mean intersection over union (mIoU), and
overall accuracy (oA). We build a full confusion matrix
based on the entire test set, and derive the ﬁnal scores from
it. Measures are evaluated over the original point clouds.
For approaches that produce labels over downsampled or
voxelized representations, we map these predictions to the
original point clouds via nearest-neighbor assignment.

Although we report oA for completeness, it is not a good
measure for semantic segmentation. If there are dominant
classes in the data (e.g., walls, ﬂoor, and ceiling in indoor
scenes), making correct predictions for these but poor pre-
dictions over the other classes will yield misleadingly high
oA scores.

7.2. Baselines

We compare our approach to three recent deep learn-
ing methods that operate on different underlying represen-
tations. We have chosen reasonably general methods that

have the potential to be applied to general scene analy-
sis and have open-source implementations. Our baselines
are PointNet [39], which operates on points, ScanNet [10],
which operates on low-resolution voxel grids, and Oct-
Net [43], which operates on higher-resolution octrees. We
used the source code provided by the authors. Due to the de-
sign of these methods, the data preparation routines and the
input signals are different for each dataset, and sometimes
deviate from the guidelines provided in the papers.

PointNet. For indoor datasets, we used the data sampling
strategy suggested in the original paper with global xyz, lo-
cally normalized xyz, and RGB as inputs. For Semantic3D,
we observed global xyz to be harmful, thus we only use lo-
cal xyz and color. Training data is generated by randomly
sampling (3m)3 cubes from the training scenes. Evaluation
is performed by applying a sliding window over the entire
scan.

ScanNet. The original network used 2 input channels: oc-
cupancy and a visibility mask computed using known cam-
era trajectories. Since scenes in general are not accompa-
nied by known camera trajectories, we only use occupancy
in the input signal. Following the original setup, we use
1.5×1.5×3m volumes voxelized into 31×31×62 grids and
augmented by 8 rotations. Each such cube yields a predic-
tion for one 1×1×62 column. (I.e., the ScanNet network
outputs a prediction for the central column only.) We use
random sampling for training, and exhaustive sliding win-
dow for testing.
OctNet. We use an architecture that operates on 2563 oc-
trees.
Inputs to the network are color, occupancy, and a
height-based feature that assigns each point to the top or
bottom part of the scan. Based on correspondence with the
authors regarding the best way to set up OctNet on differ-
ent datasets, we used (45m)3 volumes for Semantic3D and
(11m)3 volumes for the indoor datasets.

7.3. Setup of the presented approach

The architecture described in Section 6 is used in all ex-
periments. We evaluate four variants that use different in-
put signals: distance from tangent plane (D), height above
ground (H), normals (N), and color (RGB). All input signals
are normalized between 0 and 1. The initial resolution r of
the tangent image is 5cm for the indoor datasets and 10cm
for Semantic3D. It is doubled after each pooling layer. In
addition to providing the distance from tangent plane as in-
put to the ﬁrst convolutional layer, we concatenate the local
distance features from all scales of the point cloud to the
feature maps of the corresponding resolution produced by
pooling layers.

For ScanNet and S3DIS, we used whole rooms as indi-
vidual training batches. For Semantic3D, each batch was
a random sphere with a radius of 6m. For indoor scans,

we augment each scan by 8 rotations around the vertical
axis. To correct for imbalance between different classes,
we weigh the loss with the negative log of the training data
histogram.

7.4. Signal interpolation

We begin by comparing the effectiveness of two different
signal interpolation schemes: nearest neighbor and Gaus-
sian mixture. Both networks were trained on S3DIS with
D and H as the input signals. The resulting segmentation
scores are provided in the supplement. The two networks
produce similar results. We conclude that the nearest neigh-
bor signal estimation scheme is sufﬁcient, and use it in all
other experiments.

7.5. Main results

Quantitative results for all methods are summarized in
Table 2. Overall, our method produces high scores on all
datasets and consistently outperforms the baselines. Quali-
tative comparisons are shown in Figure 6.

Comparing the conﬁgurations of our networks that use
different input signals, we can see that geometry is much
more important than color on the indoor datasets. Adding
RGB information only slightly improves the scores on
S3DIS and is actually harmful for mean and overall accu-
racy on the ScanNet dataset. The situation is different for
the Semantic3D dataset: the network trained with color sig-
niﬁcantly outperforms all other conﬁgurations. Due to the
fact that H is normalized between 0 and 1 for every scan
separately, this information turns out to be harmful when
the global height of different scans is signiﬁcantly different.
Therefore, the network trained only with the distance signal
performs better than the other two geometric conﬁgurations.
In setting up and operating the baseline methods, we
found that all of them are quite hard to apply across datasets:
some non-trivial decisions had to be made for each new
dataset during the data preparation stage. None of the base-
lines showed consistent performance across the different
types of scenes.

PointNet reaches high oA scores on both indoor datasets.
However, the oA measure is strongly dominated by large
classes such as walls, ﬂoor, and ceiling. S3DIS has a fairly
regular layout because of the global room alignment pro-
cedure, which is very beneﬁcial for PointNet and allows it
to reach reasonable mA and mIoU scores on this dataset.
However, PointNet performs poorly on the ScanNet dataset,
which has more classes and noisy data. All but the most
prominent classes (i.e., walls and ﬂoor) are misclassiﬁed.
PointNet completely fails to produce meaningful predic-
tions on the even more challenging Semantic3D dataset.

Our conﬁguration of the ScanNet method produces rea-
sonable oA scores on both indoor datasets, but does much
worse in the other two measures. For reference, on the

ScanNet dataset we additionally report the number from the
original paper where a binary visibility-from-camera mask
was used as an additional input channel. This number is
much higher than our occupancy-only results, which do not
assume a known camera trajectory. Due to the fact that the
network only outputs predictions for the central column of
the voxel grid, evaluation is extremely time-consuming for
the large scenes in the Semantic3D dataset. Because of this
scalability issue, we did not succeed in evaluating ScanNet
on this dataset.

OctNet reaches good performance on the Semantic3D
dataset. However, the same network conﬁguration yields
bad results when applied to the indoor datasets. A possi-
ble explanation for this may be poor generalization due to
overﬁtting to the structure of training octrees.

7.6. Efﬁciency

We compared the efﬁciency of different methods on a
scan from S3DIS containing 125K points after grid hash-
ing. The results are reported in Table 1. Since ScanNet
and PointNet require multiple iterations for labeling a single
scan, we report both the time of a single forward pass and
the time for processing a full scan. OctNet and our method
process a full scan in one forward pass, which also ex-
plains their higher memory consumption compared to Scan-
Net and PointNet. ScanNet does not provide code for data
preprocessing, so we report the runtime of our Python im-
plementation needed for generating 38K sliding windows
during inference. Our method exhibits the best runtime for
both precomputation and inference.

Prep (s)

FP (s)

Full (s) Mem (GB)

PointNet
OctNet
ScanNet

Ours

16.5
15.5
867.8

1.59

0.01
0.61
0.002

0.52

0.65
0.61
6.34

0.52

0.39
3.33
0.97

2.35

Table 1. Efﬁciency of different methods. We report preprocessing
time (Prep), time for a single forward pass (FP), time for process-
ing a full scan (Full), and memory consumption (Mem).

8. Conclusion

We have presented tangent convolutions – a new con-
struction for convolutional networks on 3D data. The key
idea is to evaluate convolutions on virtual tangent planes at
every point. Crucially, tangent planes can be precomputed
and deep convolutional networks based on tangent convo-
lutions can be evaluated efﬁciently on large point clouds.
We have applied tangent convolutions to semantic segmen-
tation of large indoor and outdoor scenes. The presented
ideas may also be applicable to other problems in analysis,
processing, and synthesis of 3D data.

Semantic3D [17]

ScanNet [10]

S3DIS [3]

mIoU

3.76
50.7
n/a

58.1
58.0
52.5
66.4

mA

16.9
71.3
n/a

78.9
75.8
79.3
80.7

oA

16.3
80.7
n/a

84.8
83.3
79.5
89.3

mIoU

mA

oA

mIoU

12.2
18.1
13.5

40.9
40.3
40.7
40.9

17.9
26.4
19.2 (50.8)

68.1
76.6
69.4 (73.0)

52.5
52.2
55.3
55.1

80.9
80.6
80.3
80.1

41.3
26.3
24.6

49.8
50.0
51.7
52.8

mA

49.5
39.0
35.0

60.3
60.0
61.0
62.2

oA

78.8
68.9
64.2

80.2
81.2
82.2
82.5

PointNet [39]
OctNet [43]
ScanNet [10]

Ours (D)
Ours (DH)
Ours (DHN)
Ours (DHNRGB)

Table 2. Semantic segmentation accuracy for all methods across the three datasets. We report mean intersection over union (mIoU), mean
class accuracy (mA), and overall accuracy (oA). Note that oA is a bad measure and we recommend against using it in the future. We tested
different conﬁgurations of our method by combining four types of input signals: depth (D), height (H), normals (N), and color (RGB).

Color

PointNet [39]

ScanNet [10]

OctNet [43]

Ours (DHNRGB)

Ground truth

(cid:108) Ceiling (cid:108) Floor (cid:108) Walls (cid:108) Column (cid:108) Door (cid:108) Table (cid:108) Chair (cid:108) Sofa (cid:108) Bookcase (cid:108) Board (cid:108) Clutter

Color

OctNet [43]

Ours (DHNRGB)

Ground truth

(cid:108) Man made terrain (cid:108) Natural terrain (cid:108) High vegetation (cid:108) Low vegetation (cid:108) Building (cid:108) Hardscape (cid:108) Scanning artifacts (cid:108) Cars

Figure 6. Qualitative comparisons on S3DIS [3] (top) and Semantic3D [17] (bottom). Labels are coded by color.

References

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
et al. TensorFlow: A system for large-scale machine learn-
ing. In OSDI, 2016.

[2] A. Anand, H. S. Koppula, T. Joachims, and A. Saxena.
Contextually guided semantic labeling and search for three-
dimensional point clouds. International Journal of Robotics
Research, 32(1), 2013.

[3] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,
M. Fischer, and S. Savarese. 3D semantic parsing of large-
scale indoor spaces. In CVPR, 2016.

[4] D. Boscaini, J. Masci, E. Rodol`a, and M. M. Bronstein.
Learning shape correspondence with anisotropic convolu-
tional neural networks. In NIPS, 2016.

[5] A. Boulch, B. L. Saux, and N. Audebert. Unstructured point
cloud semantic labeling using deep segmentation networks.
In Eurographics Workshop on 3D Object Retrieval, 2017.
[6] A. P. Charaniya, R. Manduchi, and S. K. Lodha. Supervised
In CVPR

parametric classiﬁcation of aerial LiDAR data.
Workshops, 2004.

[7] N. Chehata, L. Guo, and C. Mallet. Contribution of airborne
full-waveform lidar and image data for urban scene classiﬁ-
cation. In ICIP, 2009.

[9]

[8] H. Chen, Q. Dou, L. Yu, and P. Heng. VoxResNet: Deep vox-
elwise residual networks for volumetric brain segmentation.
arXiv:1608.05895, 2016.
¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and
O. Ronneberger. 3D U-Net: Learning dense volumetric seg-
mentation from sparse annotation. In MICCAI, 2016.
[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A.
Funkhouser, and M. Nießner. ScanNet: Richly-annotated
3D reconstructions of indoor scenes. In CVPR, 2017.
[11] G. Floros and B. Leibe. Joint 2D-3D temporally consistent
semantic segmentation of street scenes. In CVPR, 2012.
[12] A. Frome, D. Huber, R. Kolluri, T. B¨ulow, and J. Malik. Rec-
ognizing objects in range data using regional point descrip-
tors. In ECCV, 2004.

[13] M. Gadelha, S. Maji, and R. Wang. 3D shape generation
using spatially ordered point clouds. In BMVC, 2017.
[14] M. Gadelha, S. Maji, and R. Wang. 3D shape induction from

2D views of multiple objects. In 3DV, 2017.

[15] A. Golovinskiy, V. G. Kim, and T. A. Funkhouser. Shape-
based recognition of 3D point clouds in urban environments.
In ICCV, 2009.

[16] S. Gupta, P. A. Arbel´aez, R. B. Girshick, and J. Malik. In-
door scene understanding with RGB-D images: Bottom-up
segmentation, object detection and semantic segmentation.
IJCV, 112(2), 2015.

[17] T. Hackel, N. Savinov, L. Ladicky,

J. D. Wegner,
K. Schindler, and M. Pollefeys. Semantic3D.net: A new
large-scale point cloud classiﬁcation benchmark. In ISPRS
Annals of the Photogrammetry, Remote Sensing and Spatial
Information Sciences, 2017.

[18] C. H¨ane, S. Tulsiani, and J. Malik. Hierarchical surface pre-

diction for 3D object reconstruction. In 3DV, 2017.

[19] A. Hermans, G. Floros, and B. Leibe. Dense 3D semantic
In ICRA,

mapping of indoor scenes from RGB-D images.
2014.

[20] A. E. Johnson and M. Hebert. Using spin images for efﬁ-
cient object recognition in cluttered 3D scenes. PAMI, 21(5),
1999.

[21] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri. 3D
shape segmentation with projective convolutional networks.
In CVPR, 2017.

[22] A. Kar, C. H¨ane, and J. Malik. Learning a multi-view stereo

machine. In NIPS, 2017.

[23] M. Khoury, Q.-Y. Zhou, and V. Koltun. Learning compact

geometric features. In ICCV, 2017.

[24] D. P. Kingma and J. Ba. Adam: A method for stochastic

optimization. In ICLR, 2015.

[25] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-
networks for the recognition of 3D point cloud models. In
ICCV, 2017.

[26] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully
In NIPS,

connected CRFs with Gaussian edge potentials.
2011.

[27] A. Kundu, Y. Li, F. Dellaert, F. Li, and J. M. Rehg. Joint se-
mantic segmentation and 3D reconstruction from monocular
video. In ECCV, 2014.

[28] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: Field
probing neural networks for 3D data. In NIPS, 2016.
[29] Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin. LSTM-
CF: Unifying context modeling and fusion with LSTMs for
RGB-D scene labeling. In ECCV, 2016.

[30] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang.
3D shape reconstruction from sketches via multi-view con-
volutional networks. In 3DV, 2017.

[31] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectiﬁer nonlin-
earities improve neural network acoustic models. In ICML
Workshops, 2013.

[32] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym,
E. Yumer, V. G. Kim, and Y. Lipman. Convolutional neu-
ral networks on surfaces via seamless toric covers. ACM
Transactions on Graphics, 36(4), 2017.

[33] A. Martinovic, J. Knopp, H. Riemenschneider, and L. J. Van
Gool. 3D all the way: Semantic segmentation of urban
scenes from start to end in 3D. In CVPR, 2015.

[34] J. Masci, D. Boscaini, M. M. Bronstein, and P. Van-
dergheynst. Geodesic convolutional neural networks on Rie-
mannian manifolds. In ICCV Workshops, 2015.

[35] D. Maturana and S. Scherer. VoxNet: A 3D convolutional
In IROS,

neural network for real-time object recognition.
2015.

[36] J. McCormac, A. Handa, A. J. Davison, and S. Leutenegger.
SemanticFusion: Dense 3D semantic mapping with convo-
lutional neural networks. In ICRA, 2017.

[37] O. Miksik, V. Vineet, M. Lidegaard, R. Prasaath, M. Nießner,
S. Golodetz, S. L. Hicks, P. P´erez, S. Izadi, and P. H. S. Torr.
The semantic paintbrush: Interactive 3D mapping and recog-
nition in large outdoor spaces. In CHI, 2015.

[38] D. Munoz, N. Vandapel, and M. Hebert. Onboard contextual
classiﬁcation of 3-D point clouds with learned high-order
Markov random ﬁelds. In ICRA, 2009.

[39] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In CVPR, 2017.

[40] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view CNNs for object clas-
siﬁcation on 3D data. In CVPR, 2016.

[41] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun. 3D graph
neural networks for RGBD semantic segmentation. In ICCV,
2017.

[42] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger. Oct-
NetFusion: Learning depth fusion from data. In 3DV, 2017.
[43] G. Riegler, A. O. Ulusoy, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In CVPR, 2017.
[44] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, 2015.

[45] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature
histograms (FPFH) for 3D registration. In ICRA, 2009.
[46] S. Salti, F. Tombari, and L. di Stefano. SHOT: Unique signa-
tures of histograms for surface and texture description. Com-
puter Vision and Image Understanding, 125, 2014.

[47] M. Simonovsky and N. Komodakis.

Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In CVPR, 2017.

[48] A. Sinha, J. Bai, and K. Ramani. Deep learning 3D shape

surfaces using geometry images. In ECCV, 2016.

[49] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3D shape
recognition. In ICCV, 2015.

[50] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3D
models from single images with a convolutional network. In
ECCV, 2016.

[51] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3D outputs. In ICCV, 2017.

[52] L. P. Tchapmi, C. B. Choy, I. Armeni, J. Gwak, and
S. Savarese. SEGCloud: Semantic segmentation of 3D point
clouds. In 3DV, 2017.

[53] F. Tombari, S. Salti, and L. Di Stefano. Unique shape context
In ACM Workshop on 3D Object

for 3D data description.
Retrieval, 2010.

[54] S. Tulsiani, H. Su, L. J. Guibas, A. A. Efros, and J. Malik.
Learning shape abstractions by assembling volumetric prim-
itives. In CVPR, 2017.

[55] J. P. C. Valentin, V. Vineet, M. Cheng, D. Kim, J. Shotton,
P. Kohli, M. Nießner, A. Criminisi, S. Izadi, and P. H. S. Torr.
SemanticPaint: Interactive 3D labeling and learning at your
ﬁngertips. ACM Transactions on Graphics, 34(5), 2015.
[56] V. Vineet, O. Miksik, M. Lidegaard, M. Nießner,
S. Golodetz, V. A. Prisacariu, O. K¨ahler, D. W. Murray,
S. Izadi, P. P´erez, and P. H. S. Torr. Incremental dense se-
mantic stereo fusion for large-scale semantic scene recon-
struction. In ICRA, 2015.

[57] P. Wang, Y. Liu, Y. Guo, C. Sun, and X. Tong. O-CNN:
Octree-based convolutional neural networks for 3D shape
analysis. ACM Transactions on Graphics, 36(4), 2017.
[58] C. Wu, I. Lenz, and A. Saxena. Hierarchical semantic label-
ing for task-relevant RGB-D perception. In RSS, 2014.
[59] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumetric
shapes. In CVPR, 2015.

[60] L. Yi, H. Su, X. Guo, and L. J. Guibas. SyncSpecCNN:
Synchronized spectral CNN for 3D shape segmentation. In
CVPR, 2017.

[61] Q.-Y. Zhou, J. Park, and V. Koltun. Open3D: A modern li-
brary for 3D data processing. arXiv:1801.09847, 2018.

D. Qualitative results

We provide more qualitative results of our method on

different datasets in Figures 7-9.

Appendix

A. Robustness to noise

We evaluated the robustness of our approach to noise.
Several instances of our network were trained on the S3DIS
dataset perturbed with different amounts of additive Gaus-
sian noise with standard deviation σ. The results are re-
ported in Table 3. We selected small subsets of the data for
training and testing (Area 1 for training and Area 5 for test-
ing), which is why the ﬁnal performance numbers are not
compatible with those reported in the main paper.

a
t
a
d

g
n
i
n
i
a
r
T

σ, m

OA

0.00

0.59

0.02

0.63

0.04

0.63

0.08

0.68

0.16

0.17

Table 3. Performance evaluation with different levels of noise.

Surprisingly, reasonable amounts of noise improve over-
all accuracy. The method only suffers if the noise severely
damages semantic structure in the point cloud. We did not
tune any parameters in the pipeline for these experiments.

B. Signal interpolation

In this experiment we compare the effectiveness of two
signal interpolation schemes: nearest neighbor and Gaus-
sian mixture. Quantitative results on S3DIS using D and H
as input signals are presented in Table 4. Both methods pro-
duce very similar results which is why the simpler nearest
neighbor interpolation is used throughout the paper.

Signal

NN
Gaussian

mIoU

50.0
50.7

mA

60.0
59.6

oA

81.2
81.3

Table 4. Signal interpolation using the nearest neighbor scheme
and the Gaussian mixture scheme produce similar results.

C. Comparison with SnapNet

We also compared our approach with the SnapNet by
Boulch et al. [5]. They project a 3D scene onto a set of
2D images. Those images are then segmented with a regu-
lar 2D ConvNet. The main strength of this approach is the
possibility to combine it with transfer learning and use the
weights of a network pre-trained on ImageNet for initializa-
tion. Applying this strategy yields the mIoU score of 67.7
on the Semantic3D datset, compared to 66.4 produced by
our approach. However, the non-trivial camera pose sam-
pling procedure required by SnapNet did not allow us to
apply it to indoor datasets.

Color

Prediction

Ground truth

(cid:108) Ceiling (cid:108) Floor (cid:108) Walls (cid:108) Column (cid:108) Window (cid:108) Door (cid:108) Table (cid:108) Chair (cid:108) Sofa (cid:108) Bookcase (cid:108) Board (cid:108) Clutter

Figure 7. Qualitative results on S3DIS [3].

Color

Prediction

Ground truth

(cid:108) Wall (cid:108) Floor (cid:108) Cabinet (cid:108) Bed (cid:108) Chair (cid:108) Sofa (cid:108) Table (cid:108) Door (cid:108) Window (cid:108) Bookshelf (cid:108) Desk (cid:108) Other furniture

Figure 8. Qualitative results on ScanNet [10].

Color

Prediction

Ground truth

(cid:108) Man made terrain (cid:108) Natural terrain (cid:108) High vegetation (cid:108) Low vegetation (cid:108) Building (cid:108) Hardscape (cid:108) Scanning artifacts (cid:108) Cars

Figure 9. Qualitative results on Semantic3D [17].

