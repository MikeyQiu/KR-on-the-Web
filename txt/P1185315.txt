7
1
0
2
 
v
o
N
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
8
0
2
4
0
.
6
0
7
1
:
v
i
X
r
a

Hybrid Reward Architecture for
Reinforcement Learning

Harm van Seijen1
harm.vanseijen@microsoft.com

Mehdi Fatemi1
mehdi.fatemi@microsoft.com

Joshua Romoff12
joshua.romoff@mail.mcgill.ca

Romain Laroche1
romain.laroche@microsoft.com

Tavian Barnes1
tavian.barnes@microsoft.com

Jeffrey Tsang1
tsang.jeffrey@microsoft.com

1Microsoft Maluuba, Montreal, Canada
2McGill University, Montreal, Canada

Abstract

One of the main challenges in reinforcement learning (RL) is generalisation. In
typical deep RL methods this is achieved by approximating the optimal value
function with a low-dimensional representation using a deep network. While
this approach works well in many domains, in domains where the optimal value
function cannot easily be reduced to a low-dimensional representation, learning can
be very slow and unstable. This paper contributes towards tackling such challenging
domains, by proposing a new method, called Hybrid Reward Architecture (HRA).
HRA takes as input a decomposed reward function and learns a separate value
function for each component reward function. Because each component typically
only depends on a subset of all features, the corresponding value function can be
approximated more easily by a low-dimensional representation, enabling more
effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms.
Pac-Man, where HRA achieves above-human performance.

1

Introduction

In reinforcement learning (RL) (Sutton & Barto, 1998; Szepesvári, 2009), the goal is to ﬁnd a
behaviour policy that maximises the return—the discounted sum of rewards received over time—in a
data-driven way. One of the main challenges of RL is to scale methods such that they can be applied
to large, real-world problems. Because the state-space of such problems is typically massive, strong
generalisation is required to learn a good policy efﬁciently.

Mnih et al. (2015) achieved a big breakthrough in this area: by combining standard RL techniques
with deep neural networks, they achieved above-human performance on a large number of Atari 2600
games, by learning a policy from pixels. The generalisation properties of their Deep Q-Networks
(DQN) method is achieved by approximating the optimal value function. A value function plays an
important role in RL, because it predicts the expected return, conditioned on a state or state-action
pair. Once the optimal value function is known, an optimal policy can be derived by acting greedily
with respect to it. By modelling the current estimate of the optimal value function with a deep neural
network, DQN carries out a strong generalisation on the value function, and hence on the policy.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

The generalisation behaviour of DQN is achieved by regularisation on the model for the optimal
value function. However, if the optimal value function is very complex, then learning an accurate
low-dimensional representation can be challenging or even impossible. Therefore, when the optimal
value function cannot easily be reduced to a low-dimensional representation, we argue to apply a
complementary form of regularisation on the target side. Speciﬁcally, we propose to replace the
optimal value function as target for training with an alternative value function that is easier to learn,
but still yields a reasonable—but generally not optimal—policy, when acting greedily with respect to
it.

The key observation behind regularisation on the target function is that two very different value
functions can result in the same policy when an agent acts greedily with respect to them. At the
same time, some value functions are much easier to learn than others. Intrinsic motivation (Stout
et al., 2005; Schmidhuber, 2010) uses this observation to improve learning in sparse-reward domains,
by adding a domain-speciﬁc intrinsic reward signal to the reward coming from the environment.
When the intrinsic reward function is potential-based, optimality of the resulting policy is maintained
(Ng et al., 1999). In our case, we aim for simpler value functions that are easier to represent with a
low-dimensional representation.

Our main strategy for constructing an easy-to-learn value function is to decompose the reward
function of the environment into n different reward functions. Each of them is assigned a separate
reinforcement-learning agent. Similar to the Horde architecture (Sutton et al., 2011), all these agents
can learn in parallel on the same sample sequence by using off-policy learning. Each agent gives its
action-values of the current state to an aggregator, which combines them into a single value for each
action. The current action is selected based on these aggregated values.

We test our approach on two domains: a toy-problem, where an agent has to eat 5 randomly located
fruits, and Ms. Pac-Man, one of the hard games from the ALE benchmark set (Bellemare et al.,
2013).

2 Related Work

Our HRA method builds upon the Horde architecture (Sutton et al., 2011). The Horde architecture
consists of a large number of ‘demons’ that learn in parallel via off-policy learning. Each demon
trains a separate general value function (GVF) based on its own policy and pseudo-reward function.
A pseudo-reward can be any feature-based signal that encodes useful information. The Horde
architecture is focused on building up general knowledge about the world, encoded via a large number
of GVFs. HRA focusses on training separate components of the environment-reward function, in
order to more efﬁciently learn a control policy. UVFA (Schaul et al., 2015) builds on Horde as well,
but extends it along a different direction. UVFA enables generalization across different tasks/goals. It
does not address how to solve a single, complex task, which is the focus of HRA.

Learning with respect to multiple reward functions is also a topic of multi-objective learning (Roijers
et al., 2013). So alternatively, HRA can be viewed as applying multi-objective learning in order to
more efﬁciently learn a policy for a single reward function.

Reward function decomposition has been studied among others by Russell & Zimdar (2003) and
Sprague & Ballard (2003). This earlier work focusses on strategies that achieve optimal behavior.
Our work is aimed at improving learning-efﬁciency by using simpler value functions and relaxing
optimality requirements.

There are also similarities between HRA and UNREAL (Jaderberg et al., 2017). Notably, both solve
multiple smaller problems in order to tackle one hard problem. However, the two architectures are
different in their workings, as well as the type of challenge they address. UNREAL is a technique that
boosts representation learning in difﬁcult scenarios. It does so by using auxiliary tasks to help train
the lower-level layers of a deep neural network. An example of such a challenging representation-
learning scenario is learning to navigate in the 3D Labyrinth domain. On Atari games, the reported
performance gain of UNREAL is minimal, suggesting that the standard deep RL architecture is
sufﬁciently powerful to extract the relevant representation. By contrast, the HRA architecture breaks
down a task into smaller pieces. HRA’s multiple smaller tasks are not unsupervised; they are tasks
that are directly relevant to the main task. Furthermore, whereas UNREAL is inherently a deep RL
technique, HRA is agnostic to the type of function approximation used. It can be combined with deep

2

neural networks, but it also works with exact, tabular representations. HRA is useful for domains
where having a high-quality representation is not sufﬁcient to solve the task efﬁciently.

Diuk’s object-oriented approach (Diuk et al., 2008) was one of the ﬁrst methods to show efﬁcient
learning in video games. This approach exploits domain knowledge related to the transition dynamic
to efﬁciently learn a compact transition model, which can then be used to ﬁnd a solution using
dynamic-programming techniques. This inherently model-based approach has the drawback that
while it efﬁciently learns a very compact model of the transition dynamics, it does not reduce the
state-space of the problem. Hence, it does not address the main challenge of Ms. Pac-Man: its huge
state-space, which is even for DP methods intractable (Diuk applied his method to an Atari game
with only 6 objects, whereas Ms. Pac-Man has over 150 objects).

Finally, HRA relates to options (Sutton et al., 1999; Bacon et al., 2017), and more generally hierar-
chical learning (Barto & Mahadevan, 2003; Kulkarni et al., 2016). Options are temporally-extended
actions that, like HRA’s heads, can be trained in parallel based on their own (intrinsic) reward
functions. However, once an option has been trained, the role of its intrinsic reward function is
over. A higher-level agent that uses an option sees it as just another action and evaluates it using its
own reward function. This can yield great speed-ups in learning and help substantially with better
exploration, but they do not directly make the value function of the higher-level agent less complex.
The heads of HRA represent values, trained with components of the environment reward. Even after
training, these values stay relevant, because the aggregator uses them to select its action.

3 Model

Consider a Markov Decision Process (cid:104)S, A, P, Renv, γ(cid:105) , which models an agent interacting with an
environment at discrete time steps t. It has a state set S, action set A, environment reward function
Renv : S ×A×S → R, and transition probability function P : S ×A×S → [0, 1]. At time step t, the
agent observes state st ∈ S and takes action at ∈ A. The agent observes the next state st+1, drawn
from the transition probability distribution P (st, at, ·), and a reward rt = Renv(st, at, st+1). The
behaviour is deﬁned by a policy π : S × A → [0, 1], which represents the selection probabilities over
actions. The goal of an agent is to ﬁnd a policy that maximises the expectation of the return, which is
the discounted sum of rewards: Gt := (cid:80)∞
i=0 γi rt+i, where the discount factor γ ∈ [0, 1] controls
the importance of immediate rewards versus future rewards. Each policy π has a corresponding
action-value function that gives the expected return conditioned on the state and action, when acting
according to that policy:

Qπ(s, a) = E[Gt|st = s, at = a, π]
(1)
The optimal policy π∗ can be found by iteratively improving an estimate of the optimal action-value
function Q∗(s, a) := maxπ Qπ(s, a), using sample-based updates. Once Q∗ is sufﬁciently accurate
approximated, acting greedy with respect to it yields the optimal policy.

3.1 Hybrid Reward Architecture

The Q-value function is commonly estimated using a function approximator with weight vector θ:
Q(s, a; θ). DQN uses a deep neural network as function approximator and iteratively improves an
estimate of Q∗ by minimising the sequence of loss functions:

Li(θi) = Es,a,r,s(cid:48)[(yDQN
yDQN
= r + γ max
i

i
Q(s(cid:48), a(cid:48); θi−1),

− Q(s, a; θi))2] ,

a(cid:48)

with

(2)

(3)

The weight vector from the previous iteration, θi−1, is encoded using a separate target network.

We refer to the Q-value function that minimises the loss function(s) as the training target. We will call
a training target consistent, if acting greedily with respect to it results in a policy that is optimal under
the reward function of the environment; we call a training target semi-consistent, if acting greedily
with respect to it results in a good policy—but not an optimal one—under the reward function of
the environment. For (2), the training target is Q∗
env, the optimal action-value function under Renv,
which is the default consistent training target.

That a training target is consistent says nothing about how easy it is to learn that target. For example,
if Renv is sparse, the default learning objective can be very hard to learn. In this case, adding a

3

n
(cid:88)

k=1

n
(cid:88)

k=1

potential-based additional reward signal to Renv can yield an alternative consistent learning objective
that is easier to learn. But a sparse environment reward is not the only reason a training target can be
hard to learn. We aim to ﬁnd an alternative training target for domains where the default training
target Q∗
env is hard to learn, due to the function being high-dimensional and hard to generalise for.
Our approach is based on a decomposition of the reward function.

We propose to decompose the reward function Renv into n reward functions:

Renv(s, a, s(cid:48)) =

Rk(s, a, s(cid:48)) ,

for all s, a, s(cid:48),

(4)

and to train a separate reinforcement-learning agent on each of these reward functions. There are
inﬁnitely many different decompositions of a reward function possible, but to achieve value functions
that are easy to learn, the decomposition should be such that each reward function is mainly affected
by only a small number of state variables.

Because each agent k has its own reward function, it has also its own Q-value function, Qk. In
general, different agents can share multiple lower-level layers of a deep Q-network. Hence, we will
use a single vector θ to describe the combined weights of the agents. We refer to the combined
network that represents all Q-value functions as the Hybrid Reward Architecture (HRA) (see Figure
1). Action selection for HRA is based on the sum of the agent’s Q-value functions, which we call
QHRA:

QHRA(s, a; θ) :=

Qk(s, a; θ) ,

for all s, a.

(5)

The collection of agents can be viewed alternatively as a single agent with multiple heads, with each
head producing the action-values of the current state under a different reward function.

The sequence of loss function associated with HRA is:

Li(θi) = Es,a,r,s(cid:48)

(yk,i − Qk(s, a; θi))2

,

(cid:35)

(cid:34) n
(cid:88)

k=1

with

yk,i = Rk(s, a, s(cid:48)) + γ max
a(cid:48)

Qk(s(cid:48), a(cid:48); θi−1) .

(6)

(7)

By minimising these loss functions, the different heads of HRA approximate the optimal action-value
functions under the different reward functions: Q∗
HRA,
deﬁned as:

n. Furthermore, QHRA approximates Q∗

1, . . . , Q∗

Q∗

HRA(s, a) :=

Q∗

k(s, a)

for all s, a .

n
(cid:88)

k=1

Note that Q∗

HRA is different from Q∗

env and generally not consistent.
An alternative training target is one that results from evaluating the uniformly random policy υ
HRA(s, a) := (cid:80)n
under each component reward function: Qυ
env, the

HRA is equal to Qυ

k(s, a). Qυ

k=1 Qυ

Figure 1: Illustration of Hybrid Reward Architecture.

4

Q-values of the random policy under Renv, as shown below:

Qυ

env(s, a) = E

γiRenv(st+i, at+i, st+1+i)|st = s, at = a, υ

(cid:34) ∞
(cid:88)

i=0
(cid:34) ∞
(cid:88)

= E

n
(cid:88)

γi

i=0

k=1

n
(cid:88)

E

(cid:34) ∞
(cid:88)

i=0

=

=

k=1
n
(cid:88)

k=1

Qυ

k(s, a) := Qυ

HRA(s, a) .

Rk(st+i, at+i, st+1+i)|st = s, at = a, υ

,

γiRk(st+i, at+i, st+1+i)|st = s, at = a, υ

,

(cid:35)

,

(cid:35)

(cid:35)

This training target can be learned using the expected Sarsa update rule (van Seijen et al., 2009), by
replacing (7), with

yk,i = Rk(s, a, s(cid:48)) + γ

Qk(s(cid:48), a(cid:48); θi−1) .

(8)

a(cid:48)∈A
Acting greedily with respect to the Q-values of a random policy might appear to yield a policy that is
just slightly better than random, but, surpringly, we found that for many navigation-based domains
Qυ

HRA acts as a semi-consistent training target.

(cid:88)

1
|A|

3.2 Improving Performance further by using high-level domain knowledge.

In its basic setting, the only domain knowledge applied to HRA is in the form of the decomposed
reward function. However, one of the strengths of HRA is that it can easily exploit more domain
knowledge, if available. Domain knowledge can be exploited in one of the following ways:

1. Removing irrelevant features. Features that do not affect the received reward in any way

(directly or indirectly) only add noise to the learning process and can be removed.

2. Identifying terminal states. Terminal states are states from which no further reward can
be received; they have by deﬁnition a value of 0. Using this knowledge, HRA can refrain
from approximating this value by the value network, such that the weights can be fully used
to represent the non-terminal states.

3. Using pseudo-reward functions. Instead of updating a head of HRA using a component
of the environment reward, it can be updated using a pseudo-reward. In this scenario, a set
of GVFs is trained in parallel using pseudo-rewards.

While these approaches are not speciﬁc to HRA, HRA can exploit domain knowledge to a much great
extend, because it can apply these approaches to each head individually. We show this empirically in
Section 4.1.

4 Experiments

4.1 Fruit Collection task

In our ﬁrst domain, we consider an agent that has to collect fruits as quickly as possible in a 10 × 10
grid.1 There are 10 possible fruit locations, spread out across the grid. For each episode, a fruit is
randomly placed on 5 of those 10 locations. The agent starts at a random position. The reward is +1
if a fruit gets eaten and 0 otherwise. An episode ends after all 5 fruits have been eaten or after 300
steps, whichever comes ﬁrst.

We compare the performance of DQN with HRA using the same network. For HRA, we decompose
the reward function into 10 different reward functions, one per possible fruit location. The network
consists of a binary input layer of length 110, encoding the agent’s position and whether there is

1The code for this experiment can be found at: https://github.com/Maluuba/hra

5

a fruit on each location. This is followed by a fully connected hidden layer of length 250. This
layer is connected to 10 heads consisting of 4 linear nodes each, representing the action-values of
the 4 actions under the different reward functions. Finally, the mean of all nodes across heads is
computed using a ﬁnal linear layer of length 4 that connects the output of corresponding nodes in
each head. This layer has ﬁxed weights with value 1 (i.e., it implements Equation 5). The difference
between HRA and DQN is that DQN updates the network from the fourth layer using loss function
(2), whereas HRA updates the network from the third layer using loss function (6).

Figure 2: The different network architectures used.

Besides the full network, we test using different levels of domain knowledge, as outlined in Section
3.2: 1) removing the irrelevant features for each head (providing only the position of the agent + the
corresponding fruit feature); 2) the above plus identifying terminal states; 3) the above plus using
pseudo rewards for learning GVFs to go to each of the 10 locations (instead of learning a value
function associated to the fruit at each location). The advantage is that these GVFs can be trained
even if there is no fruit at a location. The head for a particular location copies the Q-values of the
corresponding GVF if the location currently contains a fruit, or outputs 0s otherwise. We refer to
these as HRA+1, HRA+2 and HRA+3, respectively. For DQN, we also tested a version that was
applied to the same network as HRA+1; we refer to this version as DQN+1.

Training samples are generated by a random policy; the training process is tracked by evaluating the
greedy policy with respect to the learned value function after every episode. For HRA, we performed
HRA as training target (using Equation 7), as well as Qυ
experiments with Q∗
HRA (using Equation 8).
env, as well as Qυ
Similarly, for DQN we used the default training target, Q∗
env. We optimised the
step-size and the discount factor for each method separately.
The results are shown in Figure 3 for the best settings of each method. For DQN, using Q∗
env
as training target resulted in the best performance, while for HRA, using Qυ
HRA resulted in the
best performance. Overall, HRA shows a clear performance boost over DQN, even though the
network is identical. Furthermore, adding different forms of domain knowledge causes further
large improvements. Whereas using a network structure enhanced by domain knowledge improves
performance of HRA, using that same network for DQN results in a decrease in performance. The big

Figure 3: Results on the fruit collection domain, in which an agent has to eat 5 randomly placed fruits.
An episode ends after all 5 fruits are eaten or after 300 steps, whichever comes ﬁrst.

6

boost in performance that occurs when the the terminal states are identiﬁed is due to the representation
becoming a one-hot vector. Hence, we removed the hidden layer and directly fed this one-hot vector
into the different heads. Because the heads are linear, this representation reduces to an exact, tabular
representation. For the tabular representation, we used the same step-size as the optimal step-size for
the deep network version.

4.2 ATARI game: Ms. Pac-Man

Our second domain is the Atari 2600 game Ms. Pac-Man
(see Figure 4). Points are obtained by eating pellets, while
avoiding ghosts (contact with one causes Ms. Pac-Man to
lose a life). Eating one of the special power pellets turns
the ghosts blue for a small duration, allowing them to be
eaten for extra points. Bonus fruits can be eaten for further
points, twice per level. When all pellets have been eaten,
a new level is started. There are a total of 4 different maps
and 7 different fruit types, each with a different point value.
We provide full details on the domain in the supplementary
material.

Figure 4: The game Ms. Pac-Man.

Baselines. While our version of Ms. Pac-Man is the same as used in literature, we use different
preprocessing. Hence, to test the effect of our preprocessing, we implement the A3C method (Mnih
et al., 2016) and run it with our preprocessing. We refer to the version with our preprocessing as
‘A3C(channels)’, the version with the standard preprocessing ‘A3C(pixels)’, and A3C’s score reported
in literature ‘A3C(reported)’.

Preprocessing. Each frame from ALE is 210 × 160 pixels. We cut the bottom part and the top part
of the screen to end up with 160 × 160 pixels. From this, we extract the position of the different
objects and create for each object a separate input channel, encoding its location with an accuracy of
4 pixels. This results in 11 binary channels of size 40 × 40. Speciﬁcally, there is a channel for Ms.
Pac-Man, each of the four ghosts, each of the four blue ghosts (these are treated as different objects),
the fruit plus one channel with all the pellets (including power pellets). For A3C, we combine the 4
channels of the ghosts into a single channel, to allow it to generalise better across ghosts. We do the
same with the 4 channels of the blue ghosts. Instead of giving the history of the last 4 frames as done
in literature, we give the orientation of Ms. Pac-Man as a 1-hot vector of length 4 (representing the 4
compass directions).

HRA architecture. The environment reward signal corresponds with the points scored in the game.
Before decomposing the reward function, we perform reward shaping by adding a negative reward of
-1000 for contact with a ghost (which causes Ms. Pac-Man to lose a life). After this, the reward is
decomposed in a way that each object in the game (pellet/fruit/ghost/blue ghost) has its own reward
function. Hence, there is a separate RL agent associated with each object in the game that estimates a
Q-value function of its corresponding reward function.

To estimate each component reward function, we use the three forms of domain knowledge discussed
in Section 3.2. HRA uses GVFs that learn pseudo Q-values (with values in the range [0, 1]) for
getting to a particular location on the map (separate GVFs are learnt for each of the four maps). In
contrast to the fruit collection task (Section 4.1), HRA learns part of its representation during training:
it starts off with 0 GVFs and 0 heads for the pellets. By wandering around the maze, it discovers new
map locations it can reach, resulting in new GVFs being created. Whenever the agent ﬁnds a pellet at
a new location it creates a new head corresponding to the pellet.

The Q-values for an object (pellet/fruit/ghost/blue ghost) are set to the pseudo Q-values of the
GVF corresponding with the object’s location (i.e., moving objects use a different GVF each time),
multiplied with a weight that is set equal to the reward received when the object is eaten. If an object
is not on the screen, all its Q-values are 0.

We test two aggregator types. The ﬁrst one is a linear one that sums the Q-values of all heads (see
Equation 5). For the second one, we take the sum of all the heads that produce points, and normalise

7

the resulting Q-values; then, we add the sum of the Q-values of the heads of the regular ghosts,
multiplied with a weight vector.

For exploration, we test two complementary types of exploration. Each type adds an extra exploration
head to the architecture. The ﬁrst type, which we call diversiﬁcation, produces random Q-values,
drawn from a uniform distribution over [0, 20]. We ﬁnd that it is only necessary during the ﬁrst 50
steps, to ensure starting each episode randomly. The second type, which we call count-based, adds
a bonus for state-action pairs that have not been explored a lot. It is inspired by upper conﬁdence
bounds (Auer et al., 2002). Full details can be found in the supplementary material.

For our ﬁnal experiment, we implement a special head inspired by executive-memory literature (Fuster,
2003; Gluck et al., 2013). When a human game player reaches the maximum of his cognitive and
physical ability, he starts to look for favourable situations or even glitches and memorises them.
This cognitive process is indeed memorising a sequence of actions (also called habit), and is not
necessarily optimal. Our executive-memory head records every sequence of actions that led to pass
a level without any kill. Then, when facing the same level, the head gives a very high value to the
recorded action, in order to force the aggregator’s selection. Note that our simpliﬁed version of
executive memory does not generalise.

Evaluation metrics. There are two different evaluation methods used across literature which result
in very different scores. Because ALE is ultimately a deterministic environment (it implements
pseudo-randomness using a random number generator that always starts with the same seed), both
evaluation metrics aim to create randomness in the evaluation in order to rate methods with more
generalising behaviour higher. The ﬁrst metric introduces a mild form of randomness by taking a
random number of no-op actions before control is handed over to the learning algorithm. In the case
of Ms. Pac-Man, however, the game starts with a certain inactive period that exceeds the maximum
number of no-op steps, resulting in the game having a ﬁxed start after all. The second metric selects
random starting points along a human trajectory and results in much stronger randomness, and does
result in the intended random start evaluation. We refer to these metrics as ‘ﬁxed start’ and ‘random
start’.

Table 1: Final scores.

Results. Figure 5 shows the training curves; Table 1
shows the ﬁnal score after training. The best reported
ﬁxed start score comes from STRAW (Vezhnevets et al.,
2016); the best reported random start score comes from
the Dueling network architecture (Wang et al., 2016). The
human ﬁxed start score comes from Mnih et al. (2015); the
human random start score comes from Nair et al. (2015).
We train A3C for 800 million frames. Because HRA learns
fast, we train it only for 5,000 episodes, corresponding
with about 150 million frames (note that better policies
result in more frames per episode). We tried a few different settings for HRA: with/without normalisa-
tion and with/without each type of exploration. The score shown for HRA uses the best combination:
with normalisation and with both exploration types. All combinations achieved over 10,000 points in
training, except the combination with no exploration at all, which—not surprisingly—performed very
poorly. With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it
also signiﬁcantly outperforms the human score, convincingly demonstrating the strength of HRA.

ﬁxed
start
6,673
15,693
—
2,168
2,423
HRA 25,304

method
best reported
human
A3C (reported)
A3C (pixels)
A3C (channels)

random
start
2,251
15,375
654
626
589
23,770

Comparing A3C(pixels) and A3C(channels) in Table 1 reveals a surprising result: while we use
advanced preprocessing by separating the screen image into relevant object channels, this did not
signiﬁcantly change the performance of A3C.

In our ﬁnal experiment, we test how well HRA does if it exploits the weakness of the ﬁxed-start
evaluation metric by using a simpliﬁed version of executive memory. Using this version, we not only
surpass the human high-score of 266,330 points,2 we achieve the maximum possible score of 999,990
points in less than 3,000 episodes. The curve is slow in the ﬁrst stages because the model has to be
trained, but even though the further levels get more and more difﬁcult, the level passing speeds up by
taking advantage of already knowing the maps. Obtaining more points is impossible, not because the
game ends, but because the score overﬂows to 0 when reaching a million points.3

2See highscore.com: ‘Ms. Pac-Man (Atari 2600 Emulated)’.
3For a video of HRA’s ﬁnal trajectory reaching this point, see: https://youtu.be/VeXNw0Owf0Y

8

Figure 5: Training smoothed over 100 episodes. Figure 6: Training with trajectory memorisation.

5 Discussion

One of the strengths of HRA is that it can exploit domain knowledge to a much greater extent than
single-head methods. This is clearly shown by the fruit collection task: while removing irrelevant
features improves performance of HRA, the performance of DQN decreased when provided with the
same network architecture. Furthermore, separating the pixel image into multiple binary channels
only makes a small improvement in the performance of A3C over learning directly from pixels.
This demonstrates that the reason that modern deep RL struggle with Ms. Pac-Man is not related to
learning from pixels; the underlying issue is that the optimal value function for Ms. Pac-Man cannot
easily be mapped to a low-dimensional representation.

HRA solves Ms. Pac-Man by learning close to 1,800 general value functions. This results in an
exponential breakdown of the problem size: whereas the input state-space corresponding with the
binary channels is in the order of 1077, each GVF has a state-space in the order of 103 states, small
enough to be represented without any function approximation. While we could have used a deep
network for representing each GVF, using a deep network for such small problems hurts more than it
helps, as evidenced by the experiments on the fruit collection domain.

We argue that many real-world tasks allow for reward decomposition. Even if the reward function can
only be decomposed in two or three components, this can already help a lot, due to the exponential
decrease of the problem size that decomposition might cause.

References

Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem.

Machine learning, 47(2-3):235–256, 2002.

Bacon, P., Harb, J., and Precup, D. The option-critic architecture. In Proceedings of the Thirthy-ﬁrst

AAAI Conference On Artiﬁcial Intelligence (AAAI), 2017.

Barto, A. G. and Mahadevan, S. Recent advances in hierarchical reinforcement learning. Discrete

Event Dynamic Systems, 13(4):341–379, 2003.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
2013.

Diuk, C., Cohen, A., and Littman, M. L. An object-oriented representation for efﬁcient reinforcement
learning. In Proceedings of The 25th International Conference on Machine Learning, 2008.

Fuster, J. M. Cortex and mind: Unifying cognition. Oxford university press, 2003.

Gluck, M. A., Mercado, E., and Myers, C. E. Learning and memory: From brain to behavior.

Palgrave Macmillan, 2013.

9

He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level
performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on
computer vision, pp. 1026–1034, 2015.

Jaderberg, M., Mnih, V., Czarnecki, W.M., Schaul, T., Leibo, J.Z., Silver, D., and Kavukcuoglu,
K. Reinforcement learning with unsupervised auxiliary tasks. In International Conference on
Learning Representations, 2017.

Kulkarni, T. D., Narasimhan, K. R., Saeedi, A., and Tenenbaum, J. B. Hierarchical deep reinforce-
ment learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural
Information Processing Systems 29, 2016.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,
Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I.,
Kumaran, H. King D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep
reinforcement learning. Nature, 518:529–533, 2015.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., and Kavukcuoglu,
K. Asynchronous methods for deep reinforcement learning. In Proceedings of The 33rd Interna-
tional Conference on Machine Learning, pp. 1928–1937, 2016.

Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., Maria, A. De, Panneershelvam, V.,
Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver, D.
Massively parallel methods for deep reinforcement learning. In In Deep Learning Workshop,
ICML, 2015.

Ng, A. Y., Harada, D., and Russell, S. Policy invariance under reward transformations: theory and
application to reward shaping. In Proceedings of The 16th International Conference on Machine
Learning, 1999.

Roijers, D. M., Vamplew, P., Whiteson, S., and Dazeley, R. A survey of multi-objective sequential

decision-making. Journal of Artiﬁcial Intelligence Research, 2013.

Russell, S. and Zimdar, A. L. Q-decomposition for reinforcement learning agents. In Proceedings of

The 20th International Conference on Machine Learning, 2003.

Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal value function approximators. In

Proceedings of The 32rd International Conference on Machine Learning, 2015.

Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. In International

Conference on Learning Representations, 2016.

Schmidhuber, J. Formal theory of creativity, fun, and intrinsic motivation (1990–2010). In IEEE

Transactions on Autonomous Mental Development 2.3, pp. 230–247, 2010.

Sprague, N. and Ballard, D. Multiple-goal reinforcement learning with modular sarsa(0).

In

International Joint Conference on Artiﬁcial Intelligence, 2003.

Stout, A., Konidaris, G., and Barto, A. G. Intrinsically motivated reinforcement learning: A promising
framework for developmental robotics. In The AAAI Spring Symposium on Developmental Robotics,
2005.

Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. MIT Press, Cambridge,

1998.

Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, Doina.
Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor
interaction. In Proceedings of 10th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS), 2011.

Sutton, R.S., Precup, D., and Singh, S.P. Between mdps and semi-mdps: A framework for temporal

abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-2):181–211, 1999.

Szepesvári, C. Algorithms for reinforcement learning. Morgan and Claypool, 2009.

10

van Hasselt, H., Guez, A., Hessel, M., Mnih, V., and Silver, D. Learning values across many orders

of magnitude. In Advances in Neural Information Processing Systems 29, 2016a.

van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In

AAAI, pp. 2094–2100, 2016b.

van Seijen, H., van Hasselt, H., Whiteson, S., and Wiering, M. A theoretical and empirical analysis
of expected sarsa. In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement
Learning (ADPRL), pp. 177–184, 2009.

Vezhnevets, A., Mnih, V., Osindero, S., Graves, A., Vinyals, O., Agapiou, J., and Kavukcuoglu, K.
Strategic attentive writer for learning macro-actions. In Advances in Neural Information Processing
Systems 29, 2016.

Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and Freitas, N. Dueling network
architectures for deep reinforcement learning. In Proceedings of The 33rd International Conference
on Machine Learning, 2016.

11

A Ms. Pac-Man - experimental details

A.1 General information about Atari 2600 Ms. Pac-Man

The second domain is the Atari 2600 game Ms. Pac-Man. Points are obtained by eating pellets, while
avoiding ghosts (contact with one causes Ms. Pac-Man to lose a life). Eating one of the special power
pellets turns the ghost blue for a small duration, allowing them to be eaten for extra points. Bonus
fruits can be eaten for further increasing points, twice per level. When all pellets have been eaten, a
new level is started. There are a total of 4 different maps (see Figure 7 and Table 2) and 7 different
fruit types, each with a different point value (see Table 3).

Ms. Pac-Man is considered as one of hard games from the ALE benchmark set. When comparing
performance, it is important to realise that there are two different evaluation methods for ALE games
used across literature which result in hugely different scores (see Table 4). Because ALE is ultimately
a deterministic environment (it implements pseudo-randomness using a random number generator that
always starts with the same seed), both evaluation metrics aim to create randomness in the evaluation
in order to discourage methods from exploiting this deterministic property and rate methods with
more generalising behaviour higher. The ﬁrst metric introduces a mild form of randomness by taking
a random number of no-op actions before control is handed over to the learning algorithm. In the case
of Ms. Pac-Man, however, the game starts with a certain inactive period that exceeds the maximum
number of random no-op steps, resulting in the game having a ﬁxed start after all. The second metric
selects random starting points along a human trajectory and results in much stronger randomness,
and does result in the intended random start evaluation.

The best method with ﬁxed start evaluation is STRAW with 6.673 points (Vezhnevets et al., 2016);
the best with random start evaluation is the dueling network architecture with 2.251 points (Wang
et al., 2016). The human baseline, as reported by Mnih et al. (2015), is 15.693 points. The highest
reported score by a human is 266.330. For reference, A3C scored 654 points with random start
evaluation (Mnih et al., 2016); no score is reported for ﬁxed start evaluation.

Figure 7: The four different maps of Ms. Pac-Man.

12

Table 2: Map type and fruit type per level.

Table 3: Points breakdown of edible objects.

level map
red
red
blue
blue
white
white
green
green
white
green
white
green
...

1
2
3
4
5
6
7
8
9
10
11
12
...

fruit
cherry
strawberry
orange
pretzel
apple
pear
banana
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
...

object
pellet
power pellet
1st blue ghost
2nd blue ghost
3th blue ghost
4th blue ghost
cherry
strawberry
orange
pretzel
apple
pear
banana

points
10
50
200
400
800
1,600
100
200
500
700
1,000
2,000
5,000

Table 4: Reported scores on Ms. Pac-Man for ﬁxed start evaluation (called ‘random no-ops’ in
literature) and random start evaluation (‘human starts’ in literature).

algorithm
Human
Random
DQN
DDQN
Prio. Exp. Rep
Dueling
A3C
Gorila
Pop-Art
STRAW
HRA

ﬁxed start
15,693
308
2,311
3,210
6,519
6,284
—
3,234
4,964
6,673
25,304

source
Mnih et al. (2015)
Mnih et al. (2015)
Mnih et al. (2015)
van Hasselt et al. (2016b)
Schaul et al. (2016)
Wang et al. (2016)
—
Nair et al. (2015)
van Hasselt et al. (2016a)
Vezhnevets et al. (2016)
(this paper)

rand start
15,375
198
764
1,241
1,825
2,251
654
1,263
—
—
23,770

source
Nair et al. (2015)
Nair et al. (2015)
Nair et al. (2015)
van Hasselt et al. (2016b)
Schaul et al. (2016)
Wang et al. (2016)
Mnih et al. (2016)
Nair et al. (2015)
—
—
(this paper)

A.2 HRA architecture

GVF heads. Ms. Pac-Man state is deﬁned as its position on the map and her direction (heading
North, East, South or West). Depending on the map, there are about 400 positions and 950 states
(not all directions are possible for each position). A GVF is created online for each visited Ms.
Pac-Man position. Each GVF is then in charge of determining the value of the random policy of Ms.
Pac-Man state for getting the pseudo-reward placed on the GVF’s associated position. The GVFs are
trained online with off-policy 1-step bootstrapping with α = 1 and γ = 0.99. Thus, the full tabular
representation of the GVF grid contains nbmaps × nbpositions × nbstates × nbactions ≈ 14M entries.

Aggregator. For each object of the game: pellets, ghosts and fruits, the GVF corresponding to its
position is activated with a multiplier depending on the object type. Edible objects’ multipliers are
consistent with the number of points they grant: pellets’ multiplier is 10, power pellets’ 50, fruits’
200, and blue and edible ghosts’ 1,000. A ghosts’ multiplier of -1,000 has demonstrated to be a fair
balance between gaining points and not being killed. Finally, the aggregator sums up all the activated
and multiplied GVFs to compute a global score for each of the nine actions and chooses the action
that maximises it.

Diversiﬁcation head. The blue curve on Figure 8 reveals that this naïve setting performs bad
because it tends to deterministically repeat a bad trajectory like a robot hitting a wall continuously.
In order to avoid this pitfall, we need to add an exploratory mechanism. An (cid:15)-greedy exploration is
not suitable for this problem since it might unnecessarily put Ms. Pac-Man in danger. A Boltzmann

13

Figure 8: Training curves for incrementally head additions to the HRA architecture.

distributed exploration is more suitable because it favours exploring the safe actions. It would
be possible to apply it on top of the aggregator, but we chose here to add a diversiﬁcation head
Qdiv(st, a) that generates for each action a random value. This random value is drawn according to a
uniform distribution in [0,20]. We found that it is only necessary during the 50 ﬁrst steps, to ensure
starting each episode randomly.

Qdiv(st, a) ∼ U(0, 20)
Qdiv(st, a) = 0

if t < 50,
otherwise.

(9)
(10)

Score heads normalisation. The orange curve on Figure 8 shows that the diversiﬁcation head
solves the determinism issue. The so-built architecture progresses fast, up to 10,000 points, but then
starts regressing. The analysis of the generated trajectories reveals that the system has difﬁculty to
ﬁnish levels: indeed, when only a few pellets remain on the screen, the aggregator gets overwhelmed
by the ghost avoider values. The regression in score is explained by the fact that the more the system
learns the more is gets easily scared by the ghosts, and therefore the more difﬁcult it is for it to ﬁnish
the levels. We solve this issue by modifying the additive aggregator with a normalisation over the
score heads between 0 and 1. To ﬁt this new value scale, the ghost multiplier is modiﬁed to -10.

Targeted exploration head. The green curve on Figure 8 grows over time as expected. It might
be surprising at ﬁrst look that the orange curve grows faster, but it is because the episodes without
normalisation tend to last much longer, which allows more GVF updates per episode. In order to
speed up the learning, we decide to use a targeted exploration head Qteh(s, a), that is motivated by
trying out the less explored state-action couples. The value of this agent is computed as follows:

Qteh(s, a) = κ

√
(cid:115) 4

N
n(s, a)

,

(11)

where N is the number of actions taken until now and n(s, a) the number of times action a has been
performed in state s. This formula is inspired from upper conﬁdence bounds Auer et al. (2002), but
replacing the stochastically motivated logarithmic function by a less drastic one is more compliant

14

with our need for bootstrapping propagation. Note that this targeted exploration head is not a
replacement for the diversiﬁcation head. They are complementary: diversiﬁcation for making each
trajectory unique, and targeted exploration for prioritised exploration. The red curve on Figure 8
reveals that the new targeted exploration head helps exploration and makes the learning faster. This
setting constitutes the HRA architecture that is used in every experiment.

Executive memory head. When a human game player reaches the maximum of his cognitive and
physical ability, he starts to look for favourable situations or even glitches and memorises them. This
cognitive process is referred as executive memory in cognitive science literature (Fuster, 2003; Gluck
et al., 2013). The executive memory head records every sequence of actions that led to pass a level
without any kill. Then, when facing the same level, the head gives a high value to the recorded action,
in order to force the aggregator’s selection. Nota bene: since it does not allow generalisation, this
head is only employed for the level-passing experiment.

A.3 A3C baselines

Since we use low level features for the HRA architecture, we implement A3C and evaluate it both on
the pixel-based environment and on the low-level features. The implementation is performed in a
way to reproduce results of Mnih et al. (2015).
They are both trained similarly as in Mnih et al. (2016) on 8.108 frames, with γ = 0.99, entropy
regularisation of 0.01, n-step return of 5, 16 threads, gradient clipping of 40, and α is set to take the
maximum performance over the following values: [0.0001, 0.00025, 0.0005, 0.00075, 0.001]. The
pixel-based environment is a reproduction of the preprocessing and the network, except we only use
a history of 2, because our steps are twice as long.

With the low features, ﬁve channels of a 40 by 40 map are used embedding the positions of Ms.
Pac-Man, the pellets, the ghosts, the blue ghosts, and the special fruit. The input space is therefore
5 by 40 by 40 plus the direction appended after convolutions: 2 of them with 16 (respectfully 32)
ﬁlters of size 6 by 6 (respectfully 4 by 4) and subsampling of 2 by 2 and ReLU activation (for both).

Figure 9: Gridsearch on γ values without executive memory smoothed over 500 episodes.

15

Figure 10: Gridsearch on γ values with executive memory.

Then, the network uses a hidden layer of 256 fully connected units with ReLU activation. Finally, the
policy head has nbactions = 9 fully connected unit with softmax activation, and the value head has 1
unit with a linear activation. All weights are uniformly initialised He et al. (2015).

A.4 Results

Training curves. Most of the results are already presented in the main document. For more
completeness, we propose here the results of the gridsearch over γ values for both with and without
the executive memory. Values [0.95, 0.97, 0.99] have been tried independently for γscore and γghosts.

Figure 9 compares the training curves without executive memory. We can notice the following:

• all γ values turn out to yield very good results,
• those good results generalise over random human starts (not shown),
• high γ values for the ghosts tend to be better,
• the γ value for the score is less impactful.

Figure 10 compares the training curves with executive memory. We can notice the following:

• the comments on Figure 9 are still holding,
• it looks like that there is a bit more randomness in the level passing efﬁciency.

16

7
1
0
2
 
v
o
N
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
8
0
2
4
0
.
6
0
7
1
:
v
i
X
r
a

Hybrid Reward Architecture for
Reinforcement Learning

Harm van Seijen1
harm.vanseijen@microsoft.com

Mehdi Fatemi1
mehdi.fatemi@microsoft.com

Joshua Romoff12
joshua.romoff@mail.mcgill.ca

Romain Laroche1
romain.laroche@microsoft.com

Tavian Barnes1
tavian.barnes@microsoft.com

Jeffrey Tsang1
tsang.jeffrey@microsoft.com

1Microsoft Maluuba, Montreal, Canada
2McGill University, Montreal, Canada

Abstract

One of the main challenges in reinforcement learning (RL) is generalisation. In
typical deep RL methods this is achieved by approximating the optimal value
function with a low-dimensional representation using a deep network. While
this approach works well in many domains, in domains where the optimal value
function cannot easily be reduced to a low-dimensional representation, learning can
be very slow and unstable. This paper contributes towards tackling such challenging
domains, by proposing a new method, called Hybrid Reward Architecture (HRA).
HRA takes as input a decomposed reward function and learns a separate value
function for each component reward function. Because each component typically
only depends on a subset of all features, the corresponding value function can be
approximated more easily by a low-dimensional representation, enabling more
effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms.
Pac-Man, where HRA achieves above-human performance.

1

Introduction

In reinforcement learning (RL) (Sutton & Barto, 1998; Szepesvári, 2009), the goal is to ﬁnd a
behaviour policy that maximises the return—the discounted sum of rewards received over time—in a
data-driven way. One of the main challenges of RL is to scale methods such that they can be applied
to large, real-world problems. Because the state-space of such problems is typically massive, strong
generalisation is required to learn a good policy efﬁciently.

Mnih et al. (2015) achieved a big breakthrough in this area: by combining standard RL techniques
with deep neural networks, they achieved above-human performance on a large number of Atari 2600
games, by learning a policy from pixels. The generalisation properties of their Deep Q-Networks
(DQN) method is achieved by approximating the optimal value function. A value function plays an
important role in RL, because it predicts the expected return, conditioned on a state or state-action
pair. Once the optimal value function is known, an optimal policy can be derived by acting greedily
with respect to it. By modelling the current estimate of the optimal value function with a deep neural
network, DQN carries out a strong generalisation on the value function, and hence on the policy.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

The generalisation behaviour of DQN is achieved by regularisation on the model for the optimal
value function. However, if the optimal value function is very complex, then learning an accurate
low-dimensional representation can be challenging or even impossible. Therefore, when the optimal
value function cannot easily be reduced to a low-dimensional representation, we argue to apply a
complementary form of regularisation on the target side. Speciﬁcally, we propose to replace the
optimal value function as target for training with an alternative value function that is easier to learn,
but still yields a reasonable—but generally not optimal—policy, when acting greedily with respect to
it.

The key observation behind regularisation on the target function is that two very different value
functions can result in the same policy when an agent acts greedily with respect to them. At the
same time, some value functions are much easier to learn than others. Intrinsic motivation (Stout
et al., 2005; Schmidhuber, 2010) uses this observation to improve learning in sparse-reward domains,
by adding a domain-speciﬁc intrinsic reward signal to the reward coming from the environment.
When the intrinsic reward function is potential-based, optimality of the resulting policy is maintained
(Ng et al., 1999). In our case, we aim for simpler value functions that are easier to represent with a
low-dimensional representation.

Our main strategy for constructing an easy-to-learn value function is to decompose the reward
function of the environment into n different reward functions. Each of them is assigned a separate
reinforcement-learning agent. Similar to the Horde architecture (Sutton et al., 2011), all these agents
can learn in parallel on the same sample sequence by using off-policy learning. Each agent gives its
action-values of the current state to an aggregator, which combines them into a single value for each
action. The current action is selected based on these aggregated values.

We test our approach on two domains: a toy-problem, where an agent has to eat 5 randomly located
fruits, and Ms. Pac-Man, one of the hard games from the ALE benchmark set (Bellemare et al.,
2013).

2 Related Work

Our HRA method builds upon the Horde architecture (Sutton et al., 2011). The Horde architecture
consists of a large number of ‘demons’ that learn in parallel via off-policy learning. Each demon
trains a separate general value function (GVF) based on its own policy and pseudo-reward function.
A pseudo-reward can be any feature-based signal that encodes useful information. The Horde
architecture is focused on building up general knowledge about the world, encoded via a large number
of GVFs. HRA focusses on training separate components of the environment-reward function, in
order to more efﬁciently learn a control policy. UVFA (Schaul et al., 2015) builds on Horde as well,
but extends it along a different direction. UVFA enables generalization across different tasks/goals. It
does not address how to solve a single, complex task, which is the focus of HRA.

Learning with respect to multiple reward functions is also a topic of multi-objective learning (Roijers
et al., 2013). So alternatively, HRA can be viewed as applying multi-objective learning in order to
more efﬁciently learn a policy for a single reward function.

Reward function decomposition has been studied among others by Russell & Zimdar (2003) and
Sprague & Ballard (2003). This earlier work focusses on strategies that achieve optimal behavior.
Our work is aimed at improving learning-efﬁciency by using simpler value functions and relaxing
optimality requirements.

There are also similarities between HRA and UNREAL (Jaderberg et al., 2017). Notably, both solve
multiple smaller problems in order to tackle one hard problem. However, the two architectures are
different in their workings, as well as the type of challenge they address. UNREAL is a technique that
boosts representation learning in difﬁcult scenarios. It does so by using auxiliary tasks to help train
the lower-level layers of a deep neural network. An example of such a challenging representation-
learning scenario is learning to navigate in the 3D Labyrinth domain. On Atari games, the reported
performance gain of UNREAL is minimal, suggesting that the standard deep RL architecture is
sufﬁciently powerful to extract the relevant representation. By contrast, the HRA architecture breaks
down a task into smaller pieces. HRA’s multiple smaller tasks are not unsupervised; they are tasks
that are directly relevant to the main task. Furthermore, whereas UNREAL is inherently a deep RL
technique, HRA is agnostic to the type of function approximation used. It can be combined with deep

2

neural networks, but it also works with exact, tabular representations. HRA is useful for domains
where having a high-quality representation is not sufﬁcient to solve the task efﬁciently.

Diuk’s object-oriented approach (Diuk et al., 2008) was one of the ﬁrst methods to show efﬁcient
learning in video games. This approach exploits domain knowledge related to the transition dynamic
to efﬁciently learn a compact transition model, which can then be used to ﬁnd a solution using
dynamic-programming techniques. This inherently model-based approach has the drawback that
while it efﬁciently learns a very compact model of the transition dynamics, it does not reduce the
state-space of the problem. Hence, it does not address the main challenge of Ms. Pac-Man: its huge
state-space, which is even for DP methods intractable (Diuk applied his method to an Atari game
with only 6 objects, whereas Ms. Pac-Man has over 150 objects).

Finally, HRA relates to options (Sutton et al., 1999; Bacon et al., 2017), and more generally hierar-
chical learning (Barto & Mahadevan, 2003; Kulkarni et al., 2016). Options are temporally-extended
actions that, like HRA’s heads, can be trained in parallel based on their own (intrinsic) reward
functions. However, once an option has been trained, the role of its intrinsic reward function is
over. A higher-level agent that uses an option sees it as just another action and evaluates it using its
own reward function. This can yield great speed-ups in learning and help substantially with better
exploration, but they do not directly make the value function of the higher-level agent less complex.
The heads of HRA represent values, trained with components of the environment reward. Even after
training, these values stay relevant, because the aggregator uses them to select its action.

3 Model

Consider a Markov Decision Process (cid:104)S, A, P, Renv, γ(cid:105) , which models an agent interacting with an
environment at discrete time steps t. It has a state set S, action set A, environment reward function
Renv : S ×A×S → R, and transition probability function P : S ×A×S → [0, 1]. At time step t, the
agent observes state st ∈ S and takes action at ∈ A. The agent observes the next state st+1, drawn
from the transition probability distribution P (st, at, ·), and a reward rt = Renv(st, at, st+1). The
behaviour is deﬁned by a policy π : S × A → [0, 1], which represents the selection probabilities over
actions. The goal of an agent is to ﬁnd a policy that maximises the expectation of the return, which is
the discounted sum of rewards: Gt := (cid:80)∞
i=0 γi rt+i, where the discount factor γ ∈ [0, 1] controls
the importance of immediate rewards versus future rewards. Each policy π has a corresponding
action-value function that gives the expected return conditioned on the state and action, when acting
according to that policy:

Qπ(s, a) = E[Gt|st = s, at = a, π]
(1)
The optimal policy π∗ can be found by iteratively improving an estimate of the optimal action-value
function Q∗(s, a) := maxπ Qπ(s, a), using sample-based updates. Once Q∗ is sufﬁciently accurate
approximated, acting greedy with respect to it yields the optimal policy.

3.1 Hybrid Reward Architecture

The Q-value function is commonly estimated using a function approximator with weight vector θ:
Q(s, a; θ). DQN uses a deep neural network as function approximator and iteratively improves an
estimate of Q∗ by minimising the sequence of loss functions:

Li(θi) = Es,a,r,s(cid:48)[(yDQN
yDQN
= r + γ max
i

i
Q(s(cid:48), a(cid:48); θi−1),

− Q(s, a; θi))2] ,

a(cid:48)

with

(2)

(3)

The weight vector from the previous iteration, θi−1, is encoded using a separate target network.

We refer to the Q-value function that minimises the loss function(s) as the training target. We will call
a training target consistent, if acting greedily with respect to it results in a policy that is optimal under
the reward function of the environment; we call a training target semi-consistent, if acting greedily
with respect to it results in a good policy—but not an optimal one—under the reward function of
the environment. For (2), the training target is Q∗
env, the optimal action-value function under Renv,
which is the default consistent training target.

That a training target is consistent says nothing about how easy it is to learn that target. For example,
if Renv is sparse, the default learning objective can be very hard to learn. In this case, adding a

3

n
(cid:88)

k=1

n
(cid:88)

k=1

potential-based additional reward signal to Renv can yield an alternative consistent learning objective
that is easier to learn. But a sparse environment reward is not the only reason a training target can be
hard to learn. We aim to ﬁnd an alternative training target for domains where the default training
target Q∗
env is hard to learn, due to the function being high-dimensional and hard to generalise for.
Our approach is based on a decomposition of the reward function.

We propose to decompose the reward function Renv into n reward functions:

Renv(s, a, s(cid:48)) =

Rk(s, a, s(cid:48)) ,

for all s, a, s(cid:48),

(4)

and to train a separate reinforcement-learning agent on each of these reward functions. There are
inﬁnitely many different decompositions of a reward function possible, but to achieve value functions
that are easy to learn, the decomposition should be such that each reward function is mainly affected
by only a small number of state variables.

Because each agent k has its own reward function, it has also its own Q-value function, Qk. In
general, different agents can share multiple lower-level layers of a deep Q-network. Hence, we will
use a single vector θ to describe the combined weights of the agents. We refer to the combined
network that represents all Q-value functions as the Hybrid Reward Architecture (HRA) (see Figure
1). Action selection for HRA is based on the sum of the agent’s Q-value functions, which we call
QHRA:

QHRA(s, a; θ) :=

Qk(s, a; θ) ,

for all s, a.

(5)

The collection of agents can be viewed alternatively as a single agent with multiple heads, with each
head producing the action-values of the current state under a different reward function.

The sequence of loss function associated with HRA is:

Li(θi) = Es,a,r,s(cid:48)

(yk,i − Qk(s, a; θi))2

,

(cid:35)

(cid:34) n
(cid:88)

k=1

with

yk,i = Rk(s, a, s(cid:48)) + γ max
a(cid:48)

Qk(s(cid:48), a(cid:48); θi−1) .

(6)

(7)

By minimising these loss functions, the different heads of HRA approximate the optimal action-value
functions under the different reward functions: Q∗
HRA,
deﬁned as:

n. Furthermore, QHRA approximates Q∗

1, . . . , Q∗

Q∗

HRA(s, a) :=

Q∗

k(s, a)

for all s, a .

n
(cid:88)

k=1

Note that Q∗

HRA is different from Q∗

env and generally not consistent.
An alternative training target is one that results from evaluating the uniformly random policy υ
HRA(s, a) := (cid:80)n
under each component reward function: Qυ
env, the

HRA is equal to Qυ

k(s, a). Qυ

k=1 Qυ

Figure 1: Illustration of Hybrid Reward Architecture.

4

Q-values of the random policy under Renv, as shown below:

Qυ

env(s, a) = E

γiRenv(st+i, at+i, st+1+i)|st = s, at = a, υ

(cid:34) ∞
(cid:88)

i=0
(cid:34) ∞
(cid:88)

= E

n
(cid:88)

γi

i=0

k=1

n
(cid:88)

E

(cid:34) ∞
(cid:88)

i=0

=

=

k=1
n
(cid:88)

k=1

Qυ

k(s, a) := Qυ

HRA(s, a) .

Rk(st+i, at+i, st+1+i)|st = s, at = a, υ

,

γiRk(st+i, at+i, st+1+i)|st = s, at = a, υ

,

(cid:35)

,

(cid:35)

(cid:35)

This training target can be learned using the expected Sarsa update rule (van Seijen et al., 2009), by
replacing (7), with

yk,i = Rk(s, a, s(cid:48)) + γ

Qk(s(cid:48), a(cid:48); θi−1) .

(8)

a(cid:48)∈A
Acting greedily with respect to the Q-values of a random policy might appear to yield a policy that is
just slightly better than random, but, surpringly, we found that for many navigation-based domains
Qυ

HRA acts as a semi-consistent training target.

(cid:88)

1
|A|

3.2 Improving Performance further by using high-level domain knowledge.

In its basic setting, the only domain knowledge applied to HRA is in the form of the decomposed
reward function. However, one of the strengths of HRA is that it can easily exploit more domain
knowledge, if available. Domain knowledge can be exploited in one of the following ways:

1. Removing irrelevant features. Features that do not affect the received reward in any way

(directly or indirectly) only add noise to the learning process and can be removed.

2. Identifying terminal states. Terminal states are states from which no further reward can
be received; they have by deﬁnition a value of 0. Using this knowledge, HRA can refrain
from approximating this value by the value network, such that the weights can be fully used
to represent the non-terminal states.

3. Using pseudo-reward functions. Instead of updating a head of HRA using a component
of the environment reward, it can be updated using a pseudo-reward. In this scenario, a set
of GVFs is trained in parallel using pseudo-rewards.

While these approaches are not speciﬁc to HRA, HRA can exploit domain knowledge to a much great
extend, because it can apply these approaches to each head individually. We show this empirically in
Section 4.1.

4 Experiments

4.1 Fruit Collection task

In our ﬁrst domain, we consider an agent that has to collect fruits as quickly as possible in a 10 × 10
grid.1 There are 10 possible fruit locations, spread out across the grid. For each episode, a fruit is
randomly placed on 5 of those 10 locations. The agent starts at a random position. The reward is +1
if a fruit gets eaten and 0 otherwise. An episode ends after all 5 fruits have been eaten or after 300
steps, whichever comes ﬁrst.

We compare the performance of DQN with HRA using the same network. For HRA, we decompose
the reward function into 10 different reward functions, one per possible fruit location. The network
consists of a binary input layer of length 110, encoding the agent’s position and whether there is

1The code for this experiment can be found at: https://github.com/Maluuba/hra

5

a fruit on each location. This is followed by a fully connected hidden layer of length 250. This
layer is connected to 10 heads consisting of 4 linear nodes each, representing the action-values of
the 4 actions under the different reward functions. Finally, the mean of all nodes across heads is
computed using a ﬁnal linear layer of length 4 that connects the output of corresponding nodes in
each head. This layer has ﬁxed weights with value 1 (i.e., it implements Equation 5). The difference
between HRA and DQN is that DQN updates the network from the fourth layer using loss function
(2), whereas HRA updates the network from the third layer using loss function (6).

Figure 2: The different network architectures used.

Besides the full network, we test using different levels of domain knowledge, as outlined in Section
3.2: 1) removing the irrelevant features for each head (providing only the position of the agent + the
corresponding fruit feature); 2) the above plus identifying terminal states; 3) the above plus using
pseudo rewards for learning GVFs to go to each of the 10 locations (instead of learning a value
function associated to the fruit at each location). The advantage is that these GVFs can be trained
even if there is no fruit at a location. The head for a particular location copies the Q-values of the
corresponding GVF if the location currently contains a fruit, or outputs 0s otherwise. We refer to
these as HRA+1, HRA+2 and HRA+3, respectively. For DQN, we also tested a version that was
applied to the same network as HRA+1; we refer to this version as DQN+1.

Training samples are generated by a random policy; the training process is tracked by evaluating the
greedy policy with respect to the learned value function after every episode. For HRA, we performed
HRA as training target (using Equation 7), as well as Qυ
experiments with Q∗
HRA (using Equation 8).
env, as well as Qυ
Similarly, for DQN we used the default training target, Q∗
env. We optimised the
step-size and the discount factor for each method separately.
The results are shown in Figure 3 for the best settings of each method. For DQN, using Q∗
env
as training target resulted in the best performance, while for HRA, using Qυ
HRA resulted in the
best performance. Overall, HRA shows a clear performance boost over DQN, even though the
network is identical. Furthermore, adding different forms of domain knowledge causes further
large improvements. Whereas using a network structure enhanced by domain knowledge improves
performance of HRA, using that same network for DQN results in a decrease in performance. The big

Figure 3: Results on the fruit collection domain, in which an agent has to eat 5 randomly placed fruits.
An episode ends after all 5 fruits are eaten or after 300 steps, whichever comes ﬁrst.

6

boost in performance that occurs when the the terminal states are identiﬁed is due to the representation
becoming a one-hot vector. Hence, we removed the hidden layer and directly fed this one-hot vector
into the different heads. Because the heads are linear, this representation reduces to an exact, tabular
representation. For the tabular representation, we used the same step-size as the optimal step-size for
the deep network version.

4.2 ATARI game: Ms. Pac-Man

Our second domain is the Atari 2600 game Ms. Pac-Man
(see Figure 4). Points are obtained by eating pellets, while
avoiding ghosts (contact with one causes Ms. Pac-Man to
lose a life). Eating one of the special power pellets turns
the ghosts blue for a small duration, allowing them to be
eaten for extra points. Bonus fruits can be eaten for further
points, twice per level. When all pellets have been eaten,
a new level is started. There are a total of 4 different maps
and 7 different fruit types, each with a different point value.
We provide full details on the domain in the supplementary
material.

Figure 4: The game Ms. Pac-Man.

Baselines. While our version of Ms. Pac-Man is the same as used in literature, we use different
preprocessing. Hence, to test the effect of our preprocessing, we implement the A3C method (Mnih
et al., 2016) and run it with our preprocessing. We refer to the version with our preprocessing as
‘A3C(channels)’, the version with the standard preprocessing ‘A3C(pixels)’, and A3C’s score reported
in literature ‘A3C(reported)’.

Preprocessing. Each frame from ALE is 210 × 160 pixels. We cut the bottom part and the top part
of the screen to end up with 160 × 160 pixels. From this, we extract the position of the different
objects and create for each object a separate input channel, encoding its location with an accuracy of
4 pixels. This results in 11 binary channels of size 40 × 40. Speciﬁcally, there is a channel for Ms.
Pac-Man, each of the four ghosts, each of the four blue ghosts (these are treated as different objects),
the fruit plus one channel with all the pellets (including power pellets). For A3C, we combine the 4
channels of the ghosts into a single channel, to allow it to generalise better across ghosts. We do the
same with the 4 channels of the blue ghosts. Instead of giving the history of the last 4 frames as done
in literature, we give the orientation of Ms. Pac-Man as a 1-hot vector of length 4 (representing the 4
compass directions).

HRA architecture. The environment reward signal corresponds with the points scored in the game.
Before decomposing the reward function, we perform reward shaping by adding a negative reward of
-1000 for contact with a ghost (which causes Ms. Pac-Man to lose a life). After this, the reward is
decomposed in a way that each object in the game (pellet/fruit/ghost/blue ghost) has its own reward
function. Hence, there is a separate RL agent associated with each object in the game that estimates a
Q-value function of its corresponding reward function.

To estimate each component reward function, we use the three forms of domain knowledge discussed
in Section 3.2. HRA uses GVFs that learn pseudo Q-values (with values in the range [0, 1]) for
getting to a particular location on the map (separate GVFs are learnt for each of the four maps). In
contrast to the fruit collection task (Section 4.1), HRA learns part of its representation during training:
it starts off with 0 GVFs and 0 heads for the pellets. By wandering around the maze, it discovers new
map locations it can reach, resulting in new GVFs being created. Whenever the agent ﬁnds a pellet at
a new location it creates a new head corresponding to the pellet.

The Q-values for an object (pellet/fruit/ghost/blue ghost) are set to the pseudo Q-values of the
GVF corresponding with the object’s location (i.e., moving objects use a different GVF each time),
multiplied with a weight that is set equal to the reward received when the object is eaten. If an object
is not on the screen, all its Q-values are 0.

We test two aggregator types. The ﬁrst one is a linear one that sums the Q-values of all heads (see
Equation 5). For the second one, we take the sum of all the heads that produce points, and normalise

7

the resulting Q-values; then, we add the sum of the Q-values of the heads of the regular ghosts,
multiplied with a weight vector.

For exploration, we test two complementary types of exploration. Each type adds an extra exploration
head to the architecture. The ﬁrst type, which we call diversiﬁcation, produces random Q-values,
drawn from a uniform distribution over [0, 20]. We ﬁnd that it is only necessary during the ﬁrst 50
steps, to ensure starting each episode randomly. The second type, which we call count-based, adds
a bonus for state-action pairs that have not been explored a lot. It is inspired by upper conﬁdence
bounds (Auer et al., 2002). Full details can be found in the supplementary material.

For our ﬁnal experiment, we implement a special head inspired by executive-memory literature (Fuster,
2003; Gluck et al., 2013). When a human game player reaches the maximum of his cognitive and
physical ability, he starts to look for favourable situations or even glitches and memorises them.
This cognitive process is indeed memorising a sequence of actions (also called habit), and is not
necessarily optimal. Our executive-memory head records every sequence of actions that led to pass
a level without any kill. Then, when facing the same level, the head gives a very high value to the
recorded action, in order to force the aggregator’s selection. Note that our simpliﬁed version of
executive memory does not generalise.

Evaluation metrics. There are two different evaluation methods used across literature which result
in very different scores. Because ALE is ultimately a deterministic environment (it implements
pseudo-randomness using a random number generator that always starts with the same seed), both
evaluation metrics aim to create randomness in the evaluation in order to rate methods with more
generalising behaviour higher. The ﬁrst metric introduces a mild form of randomness by taking a
random number of no-op actions before control is handed over to the learning algorithm. In the case
of Ms. Pac-Man, however, the game starts with a certain inactive period that exceeds the maximum
number of no-op steps, resulting in the game having a ﬁxed start after all. The second metric selects
random starting points along a human trajectory and results in much stronger randomness, and does
result in the intended random start evaluation. We refer to these metrics as ‘ﬁxed start’ and ‘random
start’.

Table 1: Final scores.

Results. Figure 5 shows the training curves; Table 1
shows the ﬁnal score after training. The best reported
ﬁxed start score comes from STRAW (Vezhnevets et al.,
2016); the best reported random start score comes from
the Dueling network architecture (Wang et al., 2016). The
human ﬁxed start score comes from Mnih et al. (2015); the
human random start score comes from Nair et al. (2015).
We train A3C for 800 million frames. Because HRA learns
fast, we train it only for 5,000 episodes, corresponding
with about 150 million frames (note that better policies
result in more frames per episode). We tried a few different settings for HRA: with/without normalisa-
tion and with/without each type of exploration. The score shown for HRA uses the best combination:
with normalisation and with both exploration types. All combinations achieved over 10,000 points in
training, except the combination with no exploration at all, which—not surprisingly—performed very
poorly. With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it
also signiﬁcantly outperforms the human score, convincingly demonstrating the strength of HRA.

ﬁxed
start
6,673
15,693
—
2,168
2,423
HRA 25,304

method
best reported
human
A3C (reported)
A3C (pixels)
A3C (channels)

random
start
2,251
15,375
654
626
589
23,770

Comparing A3C(pixels) and A3C(channels) in Table 1 reveals a surprising result: while we use
advanced preprocessing by separating the screen image into relevant object channels, this did not
signiﬁcantly change the performance of A3C.

In our ﬁnal experiment, we test how well HRA does if it exploits the weakness of the ﬁxed-start
evaluation metric by using a simpliﬁed version of executive memory. Using this version, we not only
surpass the human high-score of 266,330 points,2 we achieve the maximum possible score of 999,990
points in less than 3,000 episodes. The curve is slow in the ﬁrst stages because the model has to be
trained, but even though the further levels get more and more difﬁcult, the level passing speeds up by
taking advantage of already knowing the maps. Obtaining more points is impossible, not because the
game ends, but because the score overﬂows to 0 when reaching a million points.3

2See highscore.com: ‘Ms. Pac-Man (Atari 2600 Emulated)’.
3For a video of HRA’s ﬁnal trajectory reaching this point, see: https://youtu.be/VeXNw0Owf0Y

8

Figure 5: Training smoothed over 100 episodes. Figure 6: Training with trajectory memorisation.

5 Discussion

One of the strengths of HRA is that it can exploit domain knowledge to a much greater extent than
single-head methods. This is clearly shown by the fruit collection task: while removing irrelevant
features improves performance of HRA, the performance of DQN decreased when provided with the
same network architecture. Furthermore, separating the pixel image into multiple binary channels
only makes a small improvement in the performance of A3C over learning directly from pixels.
This demonstrates that the reason that modern deep RL struggle with Ms. Pac-Man is not related to
learning from pixels; the underlying issue is that the optimal value function for Ms. Pac-Man cannot
easily be mapped to a low-dimensional representation.

HRA solves Ms. Pac-Man by learning close to 1,800 general value functions. This results in an
exponential breakdown of the problem size: whereas the input state-space corresponding with the
binary channels is in the order of 1077, each GVF has a state-space in the order of 103 states, small
enough to be represented without any function approximation. While we could have used a deep
network for representing each GVF, using a deep network for such small problems hurts more than it
helps, as evidenced by the experiments on the fruit collection domain.

We argue that many real-world tasks allow for reward decomposition. Even if the reward function can
only be decomposed in two or three components, this can already help a lot, due to the exponential
decrease of the problem size that decomposition might cause.

References

Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem.

Machine learning, 47(2-3):235–256, 2002.

Bacon, P., Harb, J., and Precup, D. The option-critic architecture. In Proceedings of the Thirthy-ﬁrst

AAAI Conference On Artiﬁcial Intelligence (AAAI), 2017.

Barto, A. G. and Mahadevan, S. Recent advances in hierarchical reinforcement learning. Discrete

Event Dynamic Systems, 13(4):341–379, 2003.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
2013.

Diuk, C., Cohen, A., and Littman, M. L. An object-oriented representation for efﬁcient reinforcement
learning. In Proceedings of The 25th International Conference on Machine Learning, 2008.

Fuster, J. M. Cortex and mind: Unifying cognition. Oxford university press, 2003.

Gluck, M. A., Mercado, E., and Myers, C. E. Learning and memory: From brain to behavior.

Palgrave Macmillan, 2013.

9

He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level
performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on
computer vision, pp. 1026–1034, 2015.

Jaderberg, M., Mnih, V., Czarnecki, W.M., Schaul, T., Leibo, J.Z., Silver, D., and Kavukcuoglu,
K. Reinforcement learning with unsupervised auxiliary tasks. In International Conference on
Learning Representations, 2017.

Kulkarni, T. D., Narasimhan, K. R., Saeedi, A., and Tenenbaum, J. B. Hierarchical deep reinforce-
ment learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural
Information Processing Systems 29, 2016.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,
Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I.,
Kumaran, H. King D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep
reinforcement learning. Nature, 518:529–533, 2015.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., and Kavukcuoglu,
K. Asynchronous methods for deep reinforcement learning. In Proceedings of The 33rd Interna-
tional Conference on Machine Learning, pp. 1928–1937, 2016.

Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., Maria, A. De, Panneershelvam, V.,
Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver, D.
Massively parallel methods for deep reinforcement learning. In In Deep Learning Workshop,
ICML, 2015.

Ng, A. Y., Harada, D., and Russell, S. Policy invariance under reward transformations: theory and
application to reward shaping. In Proceedings of The 16th International Conference on Machine
Learning, 1999.

Roijers, D. M., Vamplew, P., Whiteson, S., and Dazeley, R. A survey of multi-objective sequential

decision-making. Journal of Artiﬁcial Intelligence Research, 2013.

Russell, S. and Zimdar, A. L. Q-decomposition for reinforcement learning agents. In Proceedings of

The 20th International Conference on Machine Learning, 2003.

Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal value function approximators. In

Proceedings of The 32rd International Conference on Machine Learning, 2015.

Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. In International

Conference on Learning Representations, 2016.

Schmidhuber, J. Formal theory of creativity, fun, and intrinsic motivation (1990–2010). In IEEE

Transactions on Autonomous Mental Development 2.3, pp. 230–247, 2010.

Sprague, N. and Ballard, D. Multiple-goal reinforcement learning with modular sarsa(0).

In

International Joint Conference on Artiﬁcial Intelligence, 2003.

Stout, A., Konidaris, G., and Barto, A. G. Intrinsically motivated reinforcement learning: A promising
framework for developmental robotics. In The AAAI Spring Symposium on Developmental Robotics,
2005.

Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. MIT Press, Cambridge,

1998.

Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, Doina.
Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor
interaction. In Proceedings of 10th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS), 2011.

Sutton, R.S., Precup, D., and Singh, S.P. Between mdps and semi-mdps: A framework for temporal

abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-2):181–211, 1999.

Szepesvári, C. Algorithms for reinforcement learning. Morgan and Claypool, 2009.

10

van Hasselt, H., Guez, A., Hessel, M., Mnih, V., and Silver, D. Learning values across many orders

of magnitude. In Advances in Neural Information Processing Systems 29, 2016a.

van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In

AAAI, pp. 2094–2100, 2016b.

van Seijen, H., van Hasselt, H., Whiteson, S., and Wiering, M. A theoretical and empirical analysis
of expected sarsa. In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement
Learning (ADPRL), pp. 177–184, 2009.

Vezhnevets, A., Mnih, V., Osindero, S., Graves, A., Vinyals, O., Agapiou, J., and Kavukcuoglu, K.
Strategic attentive writer for learning macro-actions. In Advances in Neural Information Processing
Systems 29, 2016.

Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and Freitas, N. Dueling network
architectures for deep reinforcement learning. In Proceedings of The 33rd International Conference
on Machine Learning, 2016.

11

A Ms. Pac-Man - experimental details

A.1 General information about Atari 2600 Ms. Pac-Man

The second domain is the Atari 2600 game Ms. Pac-Man. Points are obtained by eating pellets, while
avoiding ghosts (contact with one causes Ms. Pac-Man to lose a life). Eating one of the special power
pellets turns the ghost blue for a small duration, allowing them to be eaten for extra points. Bonus
fruits can be eaten for further increasing points, twice per level. When all pellets have been eaten, a
new level is started. There are a total of 4 different maps (see Figure 7 and Table 2) and 7 different
fruit types, each with a different point value (see Table 3).

Ms. Pac-Man is considered as one of hard games from the ALE benchmark set. When comparing
performance, it is important to realise that there are two different evaluation methods for ALE games
used across literature which result in hugely different scores (see Table 4). Because ALE is ultimately
a deterministic environment (it implements pseudo-randomness using a random number generator that
always starts with the same seed), both evaluation metrics aim to create randomness in the evaluation
in order to discourage methods from exploiting this deterministic property and rate methods with
more generalising behaviour higher. The ﬁrst metric introduces a mild form of randomness by taking
a random number of no-op actions before control is handed over to the learning algorithm. In the case
of Ms. Pac-Man, however, the game starts with a certain inactive period that exceeds the maximum
number of random no-op steps, resulting in the game having a ﬁxed start after all. The second metric
selects random starting points along a human trajectory and results in much stronger randomness,
and does result in the intended random start evaluation.

The best method with ﬁxed start evaluation is STRAW with 6.673 points (Vezhnevets et al., 2016);
the best with random start evaluation is the dueling network architecture with 2.251 points (Wang
et al., 2016). The human baseline, as reported by Mnih et al. (2015), is 15.693 points. The highest
reported score by a human is 266.330. For reference, A3C scored 654 points with random start
evaluation (Mnih et al., 2016); no score is reported for ﬁxed start evaluation.

Figure 7: The four different maps of Ms. Pac-Man.

12

Table 2: Map type and fruit type per level.

Table 3: Points breakdown of edible objects.

level map
red
red
blue
blue
white
white
green
green
white
green
white
green
...

1
2
3
4
5
6
7
8
9
10
11
12
...

fruit
cherry
strawberry
orange
pretzel
apple
pear
banana
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
...

object
pellet
power pellet
1st blue ghost
2nd blue ghost
3th blue ghost
4th blue ghost
cherry
strawberry
orange
pretzel
apple
pear
banana

points
10
50
200
400
800
1,600
100
200
500
700
1,000
2,000
5,000

Table 4: Reported scores on Ms. Pac-Man for ﬁxed start evaluation (called ‘random no-ops’ in
literature) and random start evaluation (‘human starts’ in literature).

algorithm
Human
Random
DQN
DDQN
Prio. Exp. Rep
Dueling
A3C
Gorila
Pop-Art
STRAW
HRA

ﬁxed start
15,693
308
2,311
3,210
6,519
6,284
—
3,234
4,964
6,673
25,304

source
Mnih et al. (2015)
Mnih et al. (2015)
Mnih et al. (2015)
van Hasselt et al. (2016b)
Schaul et al. (2016)
Wang et al. (2016)
—
Nair et al. (2015)
van Hasselt et al. (2016a)
Vezhnevets et al. (2016)
(this paper)

rand start
15,375
198
764
1,241
1,825
2,251
654
1,263
—
—
23,770

source
Nair et al. (2015)
Nair et al. (2015)
Nair et al. (2015)
van Hasselt et al. (2016b)
Schaul et al. (2016)
Wang et al. (2016)
Mnih et al. (2016)
Nair et al. (2015)
—
—
(this paper)

A.2 HRA architecture

GVF heads. Ms. Pac-Man state is deﬁned as its position on the map and her direction (heading
North, East, South or West). Depending on the map, there are about 400 positions and 950 states
(not all directions are possible for each position). A GVF is created online for each visited Ms.
Pac-Man position. Each GVF is then in charge of determining the value of the random policy of Ms.
Pac-Man state for getting the pseudo-reward placed on the GVF’s associated position. The GVFs are
trained online with off-policy 1-step bootstrapping with α = 1 and γ = 0.99. Thus, the full tabular
representation of the GVF grid contains nbmaps × nbpositions × nbstates × nbactions ≈ 14M entries.

Aggregator. For each object of the game: pellets, ghosts and fruits, the GVF corresponding to its
position is activated with a multiplier depending on the object type. Edible objects’ multipliers are
consistent with the number of points they grant: pellets’ multiplier is 10, power pellets’ 50, fruits’
200, and blue and edible ghosts’ 1,000. A ghosts’ multiplier of -1,000 has demonstrated to be a fair
balance between gaining points and not being killed. Finally, the aggregator sums up all the activated
and multiplied GVFs to compute a global score for each of the nine actions and chooses the action
that maximises it.

Diversiﬁcation head. The blue curve on Figure 8 reveals that this naïve setting performs bad
because it tends to deterministically repeat a bad trajectory like a robot hitting a wall continuously.
In order to avoid this pitfall, we need to add an exploratory mechanism. An (cid:15)-greedy exploration is
not suitable for this problem since it might unnecessarily put Ms. Pac-Man in danger. A Boltzmann

13

Figure 8: Training curves for incrementally head additions to the HRA architecture.

distributed exploration is more suitable because it favours exploring the safe actions. It would
be possible to apply it on top of the aggregator, but we chose here to add a diversiﬁcation head
Qdiv(st, a) that generates for each action a random value. This random value is drawn according to a
uniform distribution in [0,20]. We found that it is only necessary during the 50 ﬁrst steps, to ensure
starting each episode randomly.

Qdiv(st, a) ∼ U(0, 20)
Qdiv(st, a) = 0

if t < 50,
otherwise.

(9)
(10)

Score heads normalisation. The orange curve on Figure 8 shows that the diversiﬁcation head
solves the determinism issue. The so-built architecture progresses fast, up to 10,000 points, but then
starts regressing. The analysis of the generated trajectories reveals that the system has difﬁculty to
ﬁnish levels: indeed, when only a few pellets remain on the screen, the aggregator gets overwhelmed
by the ghost avoider values. The regression in score is explained by the fact that the more the system
learns the more is gets easily scared by the ghosts, and therefore the more difﬁcult it is for it to ﬁnish
the levels. We solve this issue by modifying the additive aggregator with a normalisation over the
score heads between 0 and 1. To ﬁt this new value scale, the ghost multiplier is modiﬁed to -10.

Targeted exploration head. The green curve on Figure 8 grows over time as expected. It might
be surprising at ﬁrst look that the orange curve grows faster, but it is because the episodes without
normalisation tend to last much longer, which allows more GVF updates per episode. In order to
speed up the learning, we decide to use a targeted exploration head Qteh(s, a), that is motivated by
trying out the less explored state-action couples. The value of this agent is computed as follows:

Qteh(s, a) = κ

√
(cid:115) 4

N
n(s, a)

,

(11)

where N is the number of actions taken until now and n(s, a) the number of times action a has been
performed in state s. This formula is inspired from upper conﬁdence bounds Auer et al. (2002), but
replacing the stochastically motivated logarithmic function by a less drastic one is more compliant

14

with our need for bootstrapping propagation. Note that this targeted exploration head is not a
replacement for the diversiﬁcation head. They are complementary: diversiﬁcation for making each
trajectory unique, and targeted exploration for prioritised exploration. The red curve on Figure 8
reveals that the new targeted exploration head helps exploration and makes the learning faster. This
setting constitutes the HRA architecture that is used in every experiment.

Executive memory head. When a human game player reaches the maximum of his cognitive and
physical ability, he starts to look for favourable situations or even glitches and memorises them. This
cognitive process is referred as executive memory in cognitive science literature (Fuster, 2003; Gluck
et al., 2013). The executive memory head records every sequence of actions that led to pass a level
without any kill. Then, when facing the same level, the head gives a high value to the recorded action,
in order to force the aggregator’s selection. Nota bene: since it does not allow generalisation, this
head is only employed for the level-passing experiment.

A.3 A3C baselines

Since we use low level features for the HRA architecture, we implement A3C and evaluate it both on
the pixel-based environment and on the low-level features. The implementation is performed in a
way to reproduce results of Mnih et al. (2015).
They are both trained similarly as in Mnih et al. (2016) on 8.108 frames, with γ = 0.99, entropy
regularisation of 0.01, n-step return of 5, 16 threads, gradient clipping of 40, and α is set to take the
maximum performance over the following values: [0.0001, 0.00025, 0.0005, 0.00075, 0.001]. The
pixel-based environment is a reproduction of the preprocessing and the network, except we only use
a history of 2, because our steps are twice as long.

With the low features, ﬁve channels of a 40 by 40 map are used embedding the positions of Ms.
Pac-Man, the pellets, the ghosts, the blue ghosts, and the special fruit. The input space is therefore
5 by 40 by 40 plus the direction appended after convolutions: 2 of them with 16 (respectfully 32)
ﬁlters of size 6 by 6 (respectfully 4 by 4) and subsampling of 2 by 2 and ReLU activation (for both).

Figure 9: Gridsearch on γ values without executive memory smoothed over 500 episodes.

15

Figure 10: Gridsearch on γ values with executive memory.

Then, the network uses a hidden layer of 256 fully connected units with ReLU activation. Finally, the
policy head has nbactions = 9 fully connected unit with softmax activation, and the value head has 1
unit with a linear activation. All weights are uniformly initialised He et al. (2015).

A.4 Results

Training curves. Most of the results are already presented in the main document. For more
completeness, we propose here the results of the gridsearch over γ values for both with and without
the executive memory. Values [0.95, 0.97, 0.99] have been tried independently for γscore and γghosts.

Figure 9 compares the training curves without executive memory. We can notice the following:

• all γ values turn out to yield very good results,
• those good results generalise over random human starts (not shown),
• high γ values for the ghosts tend to be better,
• the γ value for the score is less impactful.

Figure 10 compares the training curves with executive memory. We can notice the following:

• the comments on Figure 9 are still holding,
• it looks like that there is a bit more randomness in the level passing efﬁciency.

16

7
1
0
2
 
v
o
N
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
8
0
2
4
0
.
6
0
7
1
:
v
i
X
r
a

Hybrid Reward Architecture for
Reinforcement Learning

Harm van Seijen1
harm.vanseijen@microsoft.com

Mehdi Fatemi1
mehdi.fatemi@microsoft.com

Joshua Romoff12
joshua.romoff@mail.mcgill.ca

Romain Laroche1
romain.laroche@microsoft.com

Tavian Barnes1
tavian.barnes@microsoft.com

Jeffrey Tsang1
tsang.jeffrey@microsoft.com

1Microsoft Maluuba, Montreal, Canada
2McGill University, Montreal, Canada

Abstract

One of the main challenges in reinforcement learning (RL) is generalisation. In
typical deep RL methods this is achieved by approximating the optimal value
function with a low-dimensional representation using a deep network. While
this approach works well in many domains, in domains where the optimal value
function cannot easily be reduced to a low-dimensional representation, learning can
be very slow and unstable. This paper contributes towards tackling such challenging
domains, by proposing a new method, called Hybrid Reward Architecture (HRA).
HRA takes as input a decomposed reward function and learns a separate value
function for each component reward function. Because each component typically
only depends on a subset of all features, the corresponding value function can be
approximated more easily by a low-dimensional representation, enabling more
effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms.
Pac-Man, where HRA achieves above-human performance.

1

Introduction

In reinforcement learning (RL) (Sutton & Barto, 1998; Szepesvári, 2009), the goal is to ﬁnd a
behaviour policy that maximises the return—the discounted sum of rewards received over time—in a
data-driven way. One of the main challenges of RL is to scale methods such that they can be applied
to large, real-world problems. Because the state-space of such problems is typically massive, strong
generalisation is required to learn a good policy efﬁciently.

Mnih et al. (2015) achieved a big breakthrough in this area: by combining standard RL techniques
with deep neural networks, they achieved above-human performance on a large number of Atari 2600
games, by learning a policy from pixels. The generalisation properties of their Deep Q-Networks
(DQN) method is achieved by approximating the optimal value function. A value function plays an
important role in RL, because it predicts the expected return, conditioned on a state or state-action
pair. Once the optimal value function is known, an optimal policy can be derived by acting greedily
with respect to it. By modelling the current estimate of the optimal value function with a deep neural
network, DQN carries out a strong generalisation on the value function, and hence on the policy.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

The generalisation behaviour of DQN is achieved by regularisation on the model for the optimal
value function. However, if the optimal value function is very complex, then learning an accurate
low-dimensional representation can be challenging or even impossible. Therefore, when the optimal
value function cannot easily be reduced to a low-dimensional representation, we argue to apply a
complementary form of regularisation on the target side. Speciﬁcally, we propose to replace the
optimal value function as target for training with an alternative value function that is easier to learn,
but still yields a reasonable—but generally not optimal—policy, when acting greedily with respect to
it.

The key observation behind regularisation on the target function is that two very different value
functions can result in the same policy when an agent acts greedily with respect to them. At the
same time, some value functions are much easier to learn than others. Intrinsic motivation (Stout
et al., 2005; Schmidhuber, 2010) uses this observation to improve learning in sparse-reward domains,
by adding a domain-speciﬁc intrinsic reward signal to the reward coming from the environment.
When the intrinsic reward function is potential-based, optimality of the resulting policy is maintained
(Ng et al., 1999). In our case, we aim for simpler value functions that are easier to represent with a
low-dimensional representation.

Our main strategy for constructing an easy-to-learn value function is to decompose the reward
function of the environment into n different reward functions. Each of them is assigned a separate
reinforcement-learning agent. Similar to the Horde architecture (Sutton et al., 2011), all these agents
can learn in parallel on the same sample sequence by using off-policy learning. Each agent gives its
action-values of the current state to an aggregator, which combines them into a single value for each
action. The current action is selected based on these aggregated values.

We test our approach on two domains: a toy-problem, where an agent has to eat 5 randomly located
fruits, and Ms. Pac-Man, one of the hard games from the ALE benchmark set (Bellemare et al.,
2013).

2 Related Work

Our HRA method builds upon the Horde architecture (Sutton et al., 2011). The Horde architecture
consists of a large number of ‘demons’ that learn in parallel via off-policy learning. Each demon
trains a separate general value function (GVF) based on its own policy and pseudo-reward function.
A pseudo-reward can be any feature-based signal that encodes useful information. The Horde
architecture is focused on building up general knowledge about the world, encoded via a large number
of GVFs. HRA focusses on training separate components of the environment-reward function, in
order to more efﬁciently learn a control policy. UVFA (Schaul et al., 2015) builds on Horde as well,
but extends it along a different direction. UVFA enables generalization across different tasks/goals. It
does not address how to solve a single, complex task, which is the focus of HRA.

Learning with respect to multiple reward functions is also a topic of multi-objective learning (Roijers
et al., 2013). So alternatively, HRA can be viewed as applying multi-objective learning in order to
more efﬁciently learn a policy for a single reward function.

Reward function decomposition has been studied among others by Russell & Zimdar (2003) and
Sprague & Ballard (2003). This earlier work focusses on strategies that achieve optimal behavior.
Our work is aimed at improving learning-efﬁciency by using simpler value functions and relaxing
optimality requirements.

There are also similarities between HRA and UNREAL (Jaderberg et al., 2017). Notably, both solve
multiple smaller problems in order to tackle one hard problem. However, the two architectures are
different in their workings, as well as the type of challenge they address. UNREAL is a technique that
boosts representation learning in difﬁcult scenarios. It does so by using auxiliary tasks to help train
the lower-level layers of a deep neural network. An example of such a challenging representation-
learning scenario is learning to navigate in the 3D Labyrinth domain. On Atari games, the reported
performance gain of UNREAL is minimal, suggesting that the standard deep RL architecture is
sufﬁciently powerful to extract the relevant representation. By contrast, the HRA architecture breaks
down a task into smaller pieces. HRA’s multiple smaller tasks are not unsupervised; they are tasks
that are directly relevant to the main task. Furthermore, whereas UNREAL is inherently a deep RL
technique, HRA is agnostic to the type of function approximation used. It can be combined with deep

2

neural networks, but it also works with exact, tabular representations. HRA is useful for domains
where having a high-quality representation is not sufﬁcient to solve the task efﬁciently.

Diuk’s object-oriented approach (Diuk et al., 2008) was one of the ﬁrst methods to show efﬁcient
learning in video games. This approach exploits domain knowledge related to the transition dynamic
to efﬁciently learn a compact transition model, which can then be used to ﬁnd a solution using
dynamic-programming techniques. This inherently model-based approach has the drawback that
while it efﬁciently learns a very compact model of the transition dynamics, it does not reduce the
state-space of the problem. Hence, it does not address the main challenge of Ms. Pac-Man: its huge
state-space, which is even for DP methods intractable (Diuk applied his method to an Atari game
with only 6 objects, whereas Ms. Pac-Man has over 150 objects).

Finally, HRA relates to options (Sutton et al., 1999; Bacon et al., 2017), and more generally hierar-
chical learning (Barto & Mahadevan, 2003; Kulkarni et al., 2016). Options are temporally-extended
actions that, like HRA’s heads, can be trained in parallel based on their own (intrinsic) reward
functions. However, once an option has been trained, the role of its intrinsic reward function is
over. A higher-level agent that uses an option sees it as just another action and evaluates it using its
own reward function. This can yield great speed-ups in learning and help substantially with better
exploration, but they do not directly make the value function of the higher-level agent less complex.
The heads of HRA represent values, trained with components of the environment reward. Even after
training, these values stay relevant, because the aggregator uses them to select its action.

3 Model

Consider a Markov Decision Process (cid:104)S, A, P, Renv, γ(cid:105) , which models an agent interacting with an
environment at discrete time steps t. It has a state set S, action set A, environment reward function
Renv : S ×A×S → R, and transition probability function P : S ×A×S → [0, 1]. At time step t, the
agent observes state st ∈ S and takes action at ∈ A. The agent observes the next state st+1, drawn
from the transition probability distribution P (st, at, ·), and a reward rt = Renv(st, at, st+1). The
behaviour is deﬁned by a policy π : S × A → [0, 1], which represents the selection probabilities over
actions. The goal of an agent is to ﬁnd a policy that maximises the expectation of the return, which is
the discounted sum of rewards: Gt := (cid:80)∞
i=0 γi rt+i, where the discount factor γ ∈ [0, 1] controls
the importance of immediate rewards versus future rewards. Each policy π has a corresponding
action-value function that gives the expected return conditioned on the state and action, when acting
according to that policy:

Qπ(s, a) = E[Gt|st = s, at = a, π]
(1)
The optimal policy π∗ can be found by iteratively improving an estimate of the optimal action-value
function Q∗(s, a) := maxπ Qπ(s, a), using sample-based updates. Once Q∗ is sufﬁciently accurate
approximated, acting greedy with respect to it yields the optimal policy.

3.1 Hybrid Reward Architecture

The Q-value function is commonly estimated using a function approximator with weight vector θ:
Q(s, a; θ). DQN uses a deep neural network as function approximator and iteratively improves an
estimate of Q∗ by minimising the sequence of loss functions:

Li(θi) = Es,a,r,s(cid:48)[(yDQN
yDQN
= r + γ max
i

i
Q(s(cid:48), a(cid:48); θi−1),

− Q(s, a; θi))2] ,

a(cid:48)

with

(2)

(3)

The weight vector from the previous iteration, θi−1, is encoded using a separate target network.

We refer to the Q-value function that minimises the loss function(s) as the training target. We will call
a training target consistent, if acting greedily with respect to it results in a policy that is optimal under
the reward function of the environment; we call a training target semi-consistent, if acting greedily
with respect to it results in a good policy—but not an optimal one—under the reward function of
the environment. For (2), the training target is Q∗
env, the optimal action-value function under Renv,
which is the default consistent training target.

That a training target is consistent says nothing about how easy it is to learn that target. For example,
if Renv is sparse, the default learning objective can be very hard to learn. In this case, adding a

3

n
(cid:88)

k=1

n
(cid:88)

k=1

potential-based additional reward signal to Renv can yield an alternative consistent learning objective
that is easier to learn. But a sparse environment reward is not the only reason a training target can be
hard to learn. We aim to ﬁnd an alternative training target for domains where the default training
target Q∗
env is hard to learn, due to the function being high-dimensional and hard to generalise for.
Our approach is based on a decomposition of the reward function.

We propose to decompose the reward function Renv into n reward functions:

Renv(s, a, s(cid:48)) =

Rk(s, a, s(cid:48)) ,

for all s, a, s(cid:48),

(4)

and to train a separate reinforcement-learning agent on each of these reward functions. There are
inﬁnitely many different decompositions of a reward function possible, but to achieve value functions
that are easy to learn, the decomposition should be such that each reward function is mainly affected
by only a small number of state variables.

Because each agent k has its own reward function, it has also its own Q-value function, Qk. In
general, different agents can share multiple lower-level layers of a deep Q-network. Hence, we will
use a single vector θ to describe the combined weights of the agents. We refer to the combined
network that represents all Q-value functions as the Hybrid Reward Architecture (HRA) (see Figure
1). Action selection for HRA is based on the sum of the agent’s Q-value functions, which we call
QHRA:

QHRA(s, a; θ) :=

Qk(s, a; θ) ,

for all s, a.

(5)

The collection of agents can be viewed alternatively as a single agent with multiple heads, with each
head producing the action-values of the current state under a different reward function.

The sequence of loss function associated with HRA is:

Li(θi) = Es,a,r,s(cid:48)

(yk,i − Qk(s, a; θi))2

,

(cid:35)

(cid:34) n
(cid:88)

k=1

with

yk,i = Rk(s, a, s(cid:48)) + γ max
a(cid:48)

Qk(s(cid:48), a(cid:48); θi−1) .

(6)

(7)

By minimising these loss functions, the different heads of HRA approximate the optimal action-value
functions under the different reward functions: Q∗
HRA,
deﬁned as:

n. Furthermore, QHRA approximates Q∗

1, . . . , Q∗

Q∗

HRA(s, a) :=

Q∗

k(s, a)

for all s, a .

n
(cid:88)

k=1

Note that Q∗

HRA is different from Q∗

env and generally not consistent.
An alternative training target is one that results from evaluating the uniformly random policy υ
HRA(s, a) := (cid:80)n
under each component reward function: Qυ
env, the

HRA is equal to Qυ

k(s, a). Qυ

k=1 Qυ

Figure 1: Illustration of Hybrid Reward Architecture.

4

Q-values of the random policy under Renv, as shown below:

Qυ

env(s, a) = E

γiRenv(st+i, at+i, st+1+i)|st = s, at = a, υ

(cid:34) ∞
(cid:88)

i=0
(cid:34) ∞
(cid:88)

= E

n
(cid:88)

γi

i=0

k=1

n
(cid:88)

E

(cid:34) ∞
(cid:88)

i=0

=

=

k=1
n
(cid:88)

k=1

Qυ

k(s, a) := Qυ

HRA(s, a) .

Rk(st+i, at+i, st+1+i)|st = s, at = a, υ

,

γiRk(st+i, at+i, st+1+i)|st = s, at = a, υ

,

(cid:35)

,

(cid:35)

(cid:35)

This training target can be learned using the expected Sarsa update rule (van Seijen et al., 2009), by
replacing (7), with

yk,i = Rk(s, a, s(cid:48)) + γ

Qk(s(cid:48), a(cid:48); θi−1) .

(8)

a(cid:48)∈A
Acting greedily with respect to the Q-values of a random policy might appear to yield a policy that is
just slightly better than random, but, surpringly, we found that for many navigation-based domains
Qυ

HRA acts as a semi-consistent training target.

(cid:88)

1
|A|

3.2 Improving Performance further by using high-level domain knowledge.

In its basic setting, the only domain knowledge applied to HRA is in the form of the decomposed
reward function. However, one of the strengths of HRA is that it can easily exploit more domain
knowledge, if available. Domain knowledge can be exploited in one of the following ways:

1. Removing irrelevant features. Features that do not affect the received reward in any way

(directly or indirectly) only add noise to the learning process and can be removed.

2. Identifying terminal states. Terminal states are states from which no further reward can
be received; they have by deﬁnition a value of 0. Using this knowledge, HRA can refrain
from approximating this value by the value network, such that the weights can be fully used
to represent the non-terminal states.

3. Using pseudo-reward functions. Instead of updating a head of HRA using a component
of the environment reward, it can be updated using a pseudo-reward. In this scenario, a set
of GVFs is trained in parallel using pseudo-rewards.

While these approaches are not speciﬁc to HRA, HRA can exploit domain knowledge to a much great
extend, because it can apply these approaches to each head individually. We show this empirically in
Section 4.1.

4 Experiments

4.1 Fruit Collection task

In our ﬁrst domain, we consider an agent that has to collect fruits as quickly as possible in a 10 × 10
grid.1 There are 10 possible fruit locations, spread out across the grid. For each episode, a fruit is
randomly placed on 5 of those 10 locations. The agent starts at a random position. The reward is +1
if a fruit gets eaten and 0 otherwise. An episode ends after all 5 fruits have been eaten or after 300
steps, whichever comes ﬁrst.

We compare the performance of DQN with HRA using the same network. For HRA, we decompose
the reward function into 10 different reward functions, one per possible fruit location. The network
consists of a binary input layer of length 110, encoding the agent’s position and whether there is

1The code for this experiment can be found at: https://github.com/Maluuba/hra

5

a fruit on each location. This is followed by a fully connected hidden layer of length 250. This
layer is connected to 10 heads consisting of 4 linear nodes each, representing the action-values of
the 4 actions under the different reward functions. Finally, the mean of all nodes across heads is
computed using a ﬁnal linear layer of length 4 that connects the output of corresponding nodes in
each head. This layer has ﬁxed weights with value 1 (i.e., it implements Equation 5). The difference
between HRA and DQN is that DQN updates the network from the fourth layer using loss function
(2), whereas HRA updates the network from the third layer using loss function (6).

Figure 2: The different network architectures used.

Besides the full network, we test using different levels of domain knowledge, as outlined in Section
3.2: 1) removing the irrelevant features for each head (providing only the position of the agent + the
corresponding fruit feature); 2) the above plus identifying terminal states; 3) the above plus using
pseudo rewards for learning GVFs to go to each of the 10 locations (instead of learning a value
function associated to the fruit at each location). The advantage is that these GVFs can be trained
even if there is no fruit at a location. The head for a particular location copies the Q-values of the
corresponding GVF if the location currently contains a fruit, or outputs 0s otherwise. We refer to
these as HRA+1, HRA+2 and HRA+3, respectively. For DQN, we also tested a version that was
applied to the same network as HRA+1; we refer to this version as DQN+1.

Training samples are generated by a random policy; the training process is tracked by evaluating the
greedy policy with respect to the learned value function after every episode. For HRA, we performed
HRA as training target (using Equation 7), as well as Qυ
experiments with Q∗
HRA (using Equation 8).
env, as well as Qυ
Similarly, for DQN we used the default training target, Q∗
env. We optimised the
step-size and the discount factor for each method separately.
The results are shown in Figure 3 for the best settings of each method. For DQN, using Q∗
env
as training target resulted in the best performance, while for HRA, using Qυ
HRA resulted in the
best performance. Overall, HRA shows a clear performance boost over DQN, even though the
network is identical. Furthermore, adding different forms of domain knowledge causes further
large improvements. Whereas using a network structure enhanced by domain knowledge improves
performance of HRA, using that same network for DQN results in a decrease in performance. The big

Figure 3: Results on the fruit collection domain, in which an agent has to eat 5 randomly placed fruits.
An episode ends after all 5 fruits are eaten or after 300 steps, whichever comes ﬁrst.

6

boost in performance that occurs when the the terminal states are identiﬁed is due to the representation
becoming a one-hot vector. Hence, we removed the hidden layer and directly fed this one-hot vector
into the different heads. Because the heads are linear, this representation reduces to an exact, tabular
representation. For the tabular representation, we used the same step-size as the optimal step-size for
the deep network version.

4.2 ATARI game: Ms. Pac-Man

Our second domain is the Atari 2600 game Ms. Pac-Man
(see Figure 4). Points are obtained by eating pellets, while
avoiding ghosts (contact with one causes Ms. Pac-Man to
lose a life). Eating one of the special power pellets turns
the ghosts blue for a small duration, allowing them to be
eaten for extra points. Bonus fruits can be eaten for further
points, twice per level. When all pellets have been eaten,
a new level is started. There are a total of 4 different maps
and 7 different fruit types, each with a different point value.
We provide full details on the domain in the supplementary
material.

Figure 4: The game Ms. Pac-Man.

Baselines. While our version of Ms. Pac-Man is the same as used in literature, we use different
preprocessing. Hence, to test the effect of our preprocessing, we implement the A3C method (Mnih
et al., 2016) and run it with our preprocessing. We refer to the version with our preprocessing as
‘A3C(channels)’, the version with the standard preprocessing ‘A3C(pixels)’, and A3C’s score reported
in literature ‘A3C(reported)’.

Preprocessing. Each frame from ALE is 210 × 160 pixels. We cut the bottom part and the top part
of the screen to end up with 160 × 160 pixels. From this, we extract the position of the different
objects and create for each object a separate input channel, encoding its location with an accuracy of
4 pixels. This results in 11 binary channels of size 40 × 40. Speciﬁcally, there is a channel for Ms.
Pac-Man, each of the four ghosts, each of the four blue ghosts (these are treated as different objects),
the fruit plus one channel with all the pellets (including power pellets). For A3C, we combine the 4
channels of the ghosts into a single channel, to allow it to generalise better across ghosts. We do the
same with the 4 channels of the blue ghosts. Instead of giving the history of the last 4 frames as done
in literature, we give the orientation of Ms. Pac-Man as a 1-hot vector of length 4 (representing the 4
compass directions).

HRA architecture. The environment reward signal corresponds with the points scored in the game.
Before decomposing the reward function, we perform reward shaping by adding a negative reward of
-1000 for contact with a ghost (which causes Ms. Pac-Man to lose a life). After this, the reward is
decomposed in a way that each object in the game (pellet/fruit/ghost/blue ghost) has its own reward
function. Hence, there is a separate RL agent associated with each object in the game that estimates a
Q-value function of its corresponding reward function.

To estimate each component reward function, we use the three forms of domain knowledge discussed
in Section 3.2. HRA uses GVFs that learn pseudo Q-values (with values in the range [0, 1]) for
getting to a particular location on the map (separate GVFs are learnt for each of the four maps). In
contrast to the fruit collection task (Section 4.1), HRA learns part of its representation during training:
it starts off with 0 GVFs and 0 heads for the pellets. By wandering around the maze, it discovers new
map locations it can reach, resulting in new GVFs being created. Whenever the agent ﬁnds a pellet at
a new location it creates a new head corresponding to the pellet.

The Q-values for an object (pellet/fruit/ghost/blue ghost) are set to the pseudo Q-values of the
GVF corresponding with the object’s location (i.e., moving objects use a different GVF each time),
multiplied with a weight that is set equal to the reward received when the object is eaten. If an object
is not on the screen, all its Q-values are 0.

We test two aggregator types. The ﬁrst one is a linear one that sums the Q-values of all heads (see
Equation 5). For the second one, we take the sum of all the heads that produce points, and normalise

7

the resulting Q-values; then, we add the sum of the Q-values of the heads of the regular ghosts,
multiplied with a weight vector.

For exploration, we test two complementary types of exploration. Each type adds an extra exploration
head to the architecture. The ﬁrst type, which we call diversiﬁcation, produces random Q-values,
drawn from a uniform distribution over [0, 20]. We ﬁnd that it is only necessary during the ﬁrst 50
steps, to ensure starting each episode randomly. The second type, which we call count-based, adds
a bonus for state-action pairs that have not been explored a lot. It is inspired by upper conﬁdence
bounds (Auer et al., 2002). Full details can be found in the supplementary material.

For our ﬁnal experiment, we implement a special head inspired by executive-memory literature (Fuster,
2003; Gluck et al., 2013). When a human game player reaches the maximum of his cognitive and
physical ability, he starts to look for favourable situations or even glitches and memorises them.
This cognitive process is indeed memorising a sequence of actions (also called habit), and is not
necessarily optimal. Our executive-memory head records every sequence of actions that led to pass
a level without any kill. Then, when facing the same level, the head gives a very high value to the
recorded action, in order to force the aggregator’s selection. Note that our simpliﬁed version of
executive memory does not generalise.

Evaluation metrics. There are two different evaluation methods used across literature which result
in very different scores. Because ALE is ultimately a deterministic environment (it implements
pseudo-randomness using a random number generator that always starts with the same seed), both
evaluation metrics aim to create randomness in the evaluation in order to rate methods with more
generalising behaviour higher. The ﬁrst metric introduces a mild form of randomness by taking a
random number of no-op actions before control is handed over to the learning algorithm. In the case
of Ms. Pac-Man, however, the game starts with a certain inactive period that exceeds the maximum
number of no-op steps, resulting in the game having a ﬁxed start after all. The second metric selects
random starting points along a human trajectory and results in much stronger randomness, and does
result in the intended random start evaluation. We refer to these metrics as ‘ﬁxed start’ and ‘random
start’.

Table 1: Final scores.

Results. Figure 5 shows the training curves; Table 1
shows the ﬁnal score after training. The best reported
ﬁxed start score comes from STRAW (Vezhnevets et al.,
2016); the best reported random start score comes from
the Dueling network architecture (Wang et al., 2016). The
human ﬁxed start score comes from Mnih et al. (2015); the
human random start score comes from Nair et al. (2015).
We train A3C for 800 million frames. Because HRA learns
fast, we train it only for 5,000 episodes, corresponding
with about 150 million frames (note that better policies
result in more frames per episode). We tried a few different settings for HRA: with/without normalisa-
tion and with/without each type of exploration. The score shown for HRA uses the best combination:
with normalisation and with both exploration types. All combinations achieved over 10,000 points in
training, except the combination with no exploration at all, which—not surprisingly—performed very
poorly. With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it
also signiﬁcantly outperforms the human score, convincingly demonstrating the strength of HRA.

ﬁxed
start
6,673
15,693
—
2,168
2,423
HRA 25,304

method
best reported
human
A3C (reported)
A3C (pixels)
A3C (channels)

random
start
2,251
15,375
654
626
589
23,770

Comparing A3C(pixels) and A3C(channels) in Table 1 reveals a surprising result: while we use
advanced preprocessing by separating the screen image into relevant object channels, this did not
signiﬁcantly change the performance of A3C.

In our ﬁnal experiment, we test how well HRA does if it exploits the weakness of the ﬁxed-start
evaluation metric by using a simpliﬁed version of executive memory. Using this version, we not only
surpass the human high-score of 266,330 points,2 we achieve the maximum possible score of 999,990
points in less than 3,000 episodes. The curve is slow in the ﬁrst stages because the model has to be
trained, but even though the further levels get more and more difﬁcult, the level passing speeds up by
taking advantage of already knowing the maps. Obtaining more points is impossible, not because the
game ends, but because the score overﬂows to 0 when reaching a million points.3

2See highscore.com: ‘Ms. Pac-Man (Atari 2600 Emulated)’.
3For a video of HRA’s ﬁnal trajectory reaching this point, see: https://youtu.be/VeXNw0Owf0Y

8

Figure 5: Training smoothed over 100 episodes. Figure 6: Training with trajectory memorisation.

5 Discussion

One of the strengths of HRA is that it can exploit domain knowledge to a much greater extent than
single-head methods. This is clearly shown by the fruit collection task: while removing irrelevant
features improves performance of HRA, the performance of DQN decreased when provided with the
same network architecture. Furthermore, separating the pixel image into multiple binary channels
only makes a small improvement in the performance of A3C over learning directly from pixels.
This demonstrates that the reason that modern deep RL struggle with Ms. Pac-Man is not related to
learning from pixels; the underlying issue is that the optimal value function for Ms. Pac-Man cannot
easily be mapped to a low-dimensional representation.

HRA solves Ms. Pac-Man by learning close to 1,800 general value functions. This results in an
exponential breakdown of the problem size: whereas the input state-space corresponding with the
binary channels is in the order of 1077, each GVF has a state-space in the order of 103 states, small
enough to be represented without any function approximation. While we could have used a deep
network for representing each GVF, using a deep network for such small problems hurts more than it
helps, as evidenced by the experiments on the fruit collection domain.

We argue that many real-world tasks allow for reward decomposition. Even if the reward function can
only be decomposed in two or three components, this can already help a lot, due to the exponential
decrease of the problem size that decomposition might cause.

References

Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem.

Machine learning, 47(2-3):235–256, 2002.

Bacon, P., Harb, J., and Precup, D. The option-critic architecture. In Proceedings of the Thirthy-ﬁrst

AAAI Conference On Artiﬁcial Intelligence (AAAI), 2017.

Barto, A. G. and Mahadevan, S. Recent advances in hierarchical reinforcement learning. Discrete

Event Dynamic Systems, 13(4):341–379, 2003.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
2013.

Diuk, C., Cohen, A., and Littman, M. L. An object-oriented representation for efﬁcient reinforcement
learning. In Proceedings of The 25th International Conference on Machine Learning, 2008.

Fuster, J. M. Cortex and mind: Unifying cognition. Oxford university press, 2003.

Gluck, M. A., Mercado, E., and Myers, C. E. Learning and memory: From brain to behavior.

Palgrave Macmillan, 2013.

9

He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level
performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on
computer vision, pp. 1026–1034, 2015.

Jaderberg, M., Mnih, V., Czarnecki, W.M., Schaul, T., Leibo, J.Z., Silver, D., and Kavukcuoglu,
K. Reinforcement learning with unsupervised auxiliary tasks. In International Conference on
Learning Representations, 2017.

Kulkarni, T. D., Narasimhan, K. R., Saeedi, A., and Tenenbaum, J. B. Hierarchical deep reinforce-
ment learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural
Information Processing Systems 29, 2016.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,
Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I.,
Kumaran, H. King D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep
reinforcement learning. Nature, 518:529–533, 2015.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., and Kavukcuoglu,
K. Asynchronous methods for deep reinforcement learning. In Proceedings of The 33rd Interna-
tional Conference on Machine Learning, pp. 1928–1937, 2016.

Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., Maria, A. De, Panneershelvam, V.,
Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver, D.
Massively parallel methods for deep reinforcement learning. In In Deep Learning Workshop,
ICML, 2015.

Ng, A. Y., Harada, D., and Russell, S. Policy invariance under reward transformations: theory and
application to reward shaping. In Proceedings of The 16th International Conference on Machine
Learning, 1999.

Roijers, D. M., Vamplew, P., Whiteson, S., and Dazeley, R. A survey of multi-objective sequential

decision-making. Journal of Artiﬁcial Intelligence Research, 2013.

Russell, S. and Zimdar, A. L. Q-decomposition for reinforcement learning agents. In Proceedings of

The 20th International Conference on Machine Learning, 2003.

Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal value function approximators. In

Proceedings of The 32rd International Conference on Machine Learning, 2015.

Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. In International

Conference on Learning Representations, 2016.

Schmidhuber, J. Formal theory of creativity, fun, and intrinsic motivation (1990–2010). In IEEE

Transactions on Autonomous Mental Development 2.3, pp. 230–247, 2010.

Sprague, N. and Ballard, D. Multiple-goal reinforcement learning with modular sarsa(0).

In

International Joint Conference on Artiﬁcial Intelligence, 2003.

Stout, A., Konidaris, G., and Barto, A. G. Intrinsically motivated reinforcement learning: A promising
framework for developmental robotics. In The AAAI Spring Symposium on Developmental Robotics,
2005.

Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. MIT Press, Cambridge,

1998.

Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, Doina.
Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor
interaction. In Proceedings of 10th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS), 2011.

Sutton, R.S., Precup, D., and Singh, S.P. Between mdps and semi-mdps: A framework for temporal

abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-2):181–211, 1999.

Szepesvári, C. Algorithms for reinforcement learning. Morgan and Claypool, 2009.

10

van Hasselt, H., Guez, A., Hessel, M., Mnih, V., and Silver, D. Learning values across many orders

of magnitude. In Advances in Neural Information Processing Systems 29, 2016a.

van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In

AAAI, pp. 2094–2100, 2016b.

van Seijen, H., van Hasselt, H., Whiteson, S., and Wiering, M. A theoretical and empirical analysis
of expected sarsa. In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement
Learning (ADPRL), pp. 177–184, 2009.

Vezhnevets, A., Mnih, V., Osindero, S., Graves, A., Vinyals, O., Agapiou, J., and Kavukcuoglu, K.
Strategic attentive writer for learning macro-actions. In Advances in Neural Information Processing
Systems 29, 2016.

Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and Freitas, N. Dueling network
architectures for deep reinforcement learning. In Proceedings of The 33rd International Conference
on Machine Learning, 2016.

11

A Ms. Pac-Man - experimental details

A.1 General information about Atari 2600 Ms. Pac-Man

The second domain is the Atari 2600 game Ms. Pac-Man. Points are obtained by eating pellets, while
avoiding ghosts (contact with one causes Ms. Pac-Man to lose a life). Eating one of the special power
pellets turns the ghost blue for a small duration, allowing them to be eaten for extra points. Bonus
fruits can be eaten for further increasing points, twice per level. When all pellets have been eaten, a
new level is started. There are a total of 4 different maps (see Figure 7 and Table 2) and 7 different
fruit types, each with a different point value (see Table 3).

Ms. Pac-Man is considered as one of hard games from the ALE benchmark set. When comparing
performance, it is important to realise that there are two different evaluation methods for ALE games
used across literature which result in hugely different scores (see Table 4). Because ALE is ultimately
a deterministic environment (it implements pseudo-randomness using a random number generator that
always starts with the same seed), both evaluation metrics aim to create randomness in the evaluation
in order to discourage methods from exploiting this deterministic property and rate methods with
more generalising behaviour higher. The ﬁrst metric introduces a mild form of randomness by taking
a random number of no-op actions before control is handed over to the learning algorithm. In the case
of Ms. Pac-Man, however, the game starts with a certain inactive period that exceeds the maximum
number of random no-op steps, resulting in the game having a ﬁxed start after all. The second metric
selects random starting points along a human trajectory and results in much stronger randomness,
and does result in the intended random start evaluation.

The best method with ﬁxed start evaluation is STRAW with 6.673 points (Vezhnevets et al., 2016);
the best with random start evaluation is the dueling network architecture with 2.251 points (Wang
et al., 2016). The human baseline, as reported by Mnih et al. (2015), is 15.693 points. The highest
reported score by a human is 266.330. For reference, A3C scored 654 points with random start
evaluation (Mnih et al., 2016); no score is reported for ﬁxed start evaluation.

Figure 7: The four different maps of Ms. Pac-Man.

12

Table 2: Map type and fruit type per level.

Table 3: Points breakdown of edible objects.

level map
red
red
blue
blue
white
white
green
green
white
green
white
green
...

1
2
3
4
5
6
7
8
9
10
11
12
...

fruit
cherry
strawberry
orange
pretzel
apple
pear
banana
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
(cid:104)random(cid:105)
...

object
pellet
power pellet
1st blue ghost
2nd blue ghost
3th blue ghost
4th blue ghost
cherry
strawberry
orange
pretzel
apple
pear
banana

points
10
50
200
400
800
1,600
100
200
500
700
1,000
2,000
5,000

Table 4: Reported scores on Ms. Pac-Man for ﬁxed start evaluation (called ‘random no-ops’ in
literature) and random start evaluation (‘human starts’ in literature).

algorithm
Human
Random
DQN
DDQN
Prio. Exp. Rep
Dueling
A3C
Gorila
Pop-Art
STRAW
HRA

ﬁxed start
15,693
308
2,311
3,210
6,519
6,284
—
3,234
4,964
6,673
25,304

source
Mnih et al. (2015)
Mnih et al. (2015)
Mnih et al. (2015)
van Hasselt et al. (2016b)
Schaul et al. (2016)
Wang et al. (2016)
—
Nair et al. (2015)
van Hasselt et al. (2016a)
Vezhnevets et al. (2016)
(this paper)

rand start
15,375
198
764
1,241
1,825
2,251
654
1,263
—
—
23,770

source
Nair et al. (2015)
Nair et al. (2015)
Nair et al. (2015)
van Hasselt et al. (2016b)
Schaul et al. (2016)
Wang et al. (2016)
Mnih et al. (2016)
Nair et al. (2015)
—
—
(this paper)

A.2 HRA architecture

GVF heads. Ms. Pac-Man state is deﬁned as its position on the map and her direction (heading
North, East, South or West). Depending on the map, there are about 400 positions and 950 states
(not all directions are possible for each position). A GVF is created online for each visited Ms.
Pac-Man position. Each GVF is then in charge of determining the value of the random policy of Ms.
Pac-Man state for getting the pseudo-reward placed on the GVF’s associated position. The GVFs are
trained online with off-policy 1-step bootstrapping with α = 1 and γ = 0.99. Thus, the full tabular
representation of the GVF grid contains nbmaps × nbpositions × nbstates × nbactions ≈ 14M entries.

Aggregator. For each object of the game: pellets, ghosts and fruits, the GVF corresponding to its
position is activated with a multiplier depending on the object type. Edible objects’ multipliers are
consistent with the number of points they grant: pellets’ multiplier is 10, power pellets’ 50, fruits’
200, and blue and edible ghosts’ 1,000. A ghosts’ multiplier of -1,000 has demonstrated to be a fair
balance between gaining points and not being killed. Finally, the aggregator sums up all the activated
and multiplied GVFs to compute a global score for each of the nine actions and chooses the action
that maximises it.

Diversiﬁcation head. The blue curve on Figure 8 reveals that this naïve setting performs bad
because it tends to deterministically repeat a bad trajectory like a robot hitting a wall continuously.
In order to avoid this pitfall, we need to add an exploratory mechanism. An (cid:15)-greedy exploration is
not suitable for this problem since it might unnecessarily put Ms. Pac-Man in danger. A Boltzmann

13

Figure 8: Training curves for incrementally head additions to the HRA architecture.

distributed exploration is more suitable because it favours exploring the safe actions. It would
be possible to apply it on top of the aggregator, but we chose here to add a diversiﬁcation head
Qdiv(st, a) that generates for each action a random value. This random value is drawn according to a
uniform distribution in [0,20]. We found that it is only necessary during the 50 ﬁrst steps, to ensure
starting each episode randomly.

Qdiv(st, a) ∼ U(0, 20)
Qdiv(st, a) = 0

if t < 50,
otherwise.

(9)
(10)

Score heads normalisation. The orange curve on Figure 8 shows that the diversiﬁcation head
solves the determinism issue. The so-built architecture progresses fast, up to 10,000 points, but then
starts regressing. The analysis of the generated trajectories reveals that the system has difﬁculty to
ﬁnish levels: indeed, when only a few pellets remain on the screen, the aggregator gets overwhelmed
by the ghost avoider values. The regression in score is explained by the fact that the more the system
learns the more is gets easily scared by the ghosts, and therefore the more difﬁcult it is for it to ﬁnish
the levels. We solve this issue by modifying the additive aggregator with a normalisation over the
score heads between 0 and 1. To ﬁt this new value scale, the ghost multiplier is modiﬁed to -10.

Targeted exploration head. The green curve on Figure 8 grows over time as expected. It might
be surprising at ﬁrst look that the orange curve grows faster, but it is because the episodes without
normalisation tend to last much longer, which allows more GVF updates per episode. In order to
speed up the learning, we decide to use a targeted exploration head Qteh(s, a), that is motivated by
trying out the less explored state-action couples. The value of this agent is computed as follows:

Qteh(s, a) = κ

√
(cid:115) 4

N
n(s, a)

,

(11)

where N is the number of actions taken until now and n(s, a) the number of times action a has been
performed in state s. This formula is inspired from upper conﬁdence bounds Auer et al. (2002), but
replacing the stochastically motivated logarithmic function by a less drastic one is more compliant

14

with our need for bootstrapping propagation. Note that this targeted exploration head is not a
replacement for the diversiﬁcation head. They are complementary: diversiﬁcation for making each
trajectory unique, and targeted exploration for prioritised exploration. The red curve on Figure 8
reveals that the new targeted exploration head helps exploration and makes the learning faster. This
setting constitutes the HRA architecture that is used in every experiment.

Executive memory head. When a human game player reaches the maximum of his cognitive and
physical ability, he starts to look for favourable situations or even glitches and memorises them. This
cognitive process is referred as executive memory in cognitive science literature (Fuster, 2003; Gluck
et al., 2013). The executive memory head records every sequence of actions that led to pass a level
without any kill. Then, when facing the same level, the head gives a high value to the recorded action,
in order to force the aggregator’s selection. Nota bene: since it does not allow generalisation, this
head is only employed for the level-passing experiment.

A.3 A3C baselines

Since we use low level features for the HRA architecture, we implement A3C and evaluate it both on
the pixel-based environment and on the low-level features. The implementation is performed in a
way to reproduce results of Mnih et al. (2015).
They are both trained similarly as in Mnih et al. (2016) on 8.108 frames, with γ = 0.99, entropy
regularisation of 0.01, n-step return of 5, 16 threads, gradient clipping of 40, and α is set to take the
maximum performance over the following values: [0.0001, 0.00025, 0.0005, 0.00075, 0.001]. The
pixel-based environment is a reproduction of the preprocessing and the network, except we only use
a history of 2, because our steps are twice as long.

With the low features, ﬁve channels of a 40 by 40 map are used embedding the positions of Ms.
Pac-Man, the pellets, the ghosts, the blue ghosts, and the special fruit. The input space is therefore
5 by 40 by 40 plus the direction appended after convolutions: 2 of them with 16 (respectfully 32)
ﬁlters of size 6 by 6 (respectfully 4 by 4) and subsampling of 2 by 2 and ReLU activation (for both).

Figure 9: Gridsearch on γ values without executive memory smoothed over 500 episodes.

15

Figure 10: Gridsearch on γ values with executive memory.

Then, the network uses a hidden layer of 256 fully connected units with ReLU activation. Finally, the
policy head has nbactions = 9 fully connected unit with softmax activation, and the value head has 1
unit with a linear activation. All weights are uniformly initialised He et al. (2015).

A.4 Results

Training curves. Most of the results are already presented in the main document. For more
completeness, we propose here the results of the gridsearch over γ values for both with and without
the executive memory. Values [0.95, 0.97, 0.99] have been tried independently for γscore and γghosts.

Figure 9 compares the training curves without executive memory. We can notice the following:

• all γ values turn out to yield very good results,
• those good results generalise over random human starts (not shown),
• high γ values for the ghosts tend to be better,
• the γ value for the score is less impactful.

Figure 10 compares the training curves with executive memory. We can notice the following:

• the comments on Figure 9 are still holding,
• it looks like that there is a bit more randomness in the level passing efﬁciency.

16

