7
1
0
2
 
b
e
F
 
9
 
 
]

G
L
.
s
c
[
 
 
2
v
6
3
8
4
0
.
9
0
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

ON LARGE-BATCH TRAINING FOR DEEP LEARNING:
GENERALIZATION GAP AND SHARP MINIMA

Nitish Shirish Keskar∗
Northwestern University
Evanston, IL 60208
keskar.nitish@u.northwestern.edu

Dheevatsa Mudigere
Intel Corporation
Bangalore, India
dheevatsa.mudigere@intel.com

Jorge Nocedal
Northwestern University
Evanston, IL 60208
j-nocedal@northwestern.edu

Mikhail Smelyanskiy
Intel Corporation
Santa Clara, CA 95054
mikhail.smelyanskiy@intel.com

Ping Tak Peter Tang
Intel Corporation
Santa Clara, CA 95054
peter.tang@intel.com

ABSTRACT

The stochastic gradient descent (SGD) method and its variants are algorithms of
choice for many Deep Learning tasks. These methods operate in a small-batch
regime wherein a fraction of the training data, say 32–512 data points, is sampled
to compute an approximation to the gradient. It has been observed in practice that
when using a larger batch there is a degradation in the quality of the model, as
measured by its ability to generalize. We investigate the cause for this generaliza-
tion drop in the large-batch regime and present numerical evidence that supports
the view that large-batch methods tend to converge to sharp minimizers of the
training and testing functions—and as is well known, sharp minima lead to poorer
generalization. In contrast, small-batch methods consistently converge to ﬂat min-
imizers, and our experiments support a commonly held view that this is due to the
inherent noise in the gradient estimation. We discuss several strategies to attempt
to help large-batch methods eliminate this generalization gap.

1

INTRODUCTION

Deep Learning has emerged as one of the cornerstones of large-scale machine learning. Deep Learn-
ing models are used for achieving state-of-the-art results on a wide variety of tasks including com-
puter vision, natural language processing and reinforcement learning; see (Bengio et al., 2016) and
the references therein. The problem of training these networks is one of non-convex optimization.
Mathematically, this can be represented as:

min
x∈Rn

f (x) :=

1
M

M
(cid:88)

i=1

fi(x),

(1)

where fi is a loss function for data point i ∈ {1, 2, · · · , M } which captures the deviation of the
model prediction from the data, and x is the vector of weights being optimized. The process of
optimizing this function is also called training of the network. Stochastic Gradient Descent (SGD)
(Bottou, 1998; Sutskever et al., 2013) and its variants are often used for training deep networks.

∗Work was performed when author was an intern at Intel Corporation

1

Published as a conference paper at ICLR 2017

These methods minimize the objective function f by iteratively taking steps of the form:

xk+1 = xk − αk

(cid:32)

1
|Bk|

(cid:88)

i∈Bk

(cid:33)

∇fi(xk)

,

(2)

where Bk ⊂ {1, 2, · · · , M } is the batch sampled from the data set and αk is the step size at iteration
k. These methods can be interpreted as gradient descent using noisy gradients, which and are often
referred to as mini-batch gradients with batch size |Bk|. SGD and its variants are employed in a
small-batch regime, where |Bk| (cid:28) M and typically |Bk| ∈ {32, 64, · · · , 512}. These conﬁgura-
tions have been successfully used in practice for a large number of applications; see e.g. (Simonyan
& Zisserman, 2014; Graves et al., 2013; Mnih et al., 2013). Many theoretical properties of these
methods are known. These include guarantees of: (a) convergence to minimizers of strongly-convex
functions and to stationary points for non-convex functions (Bottou et al., 2016), (b) saddle-point
avoidance (Ge et al., 2015; Lee et al., 2016), and (c) robustness to input data (Hardt et al., 2015).

Stochastic gradient methods have, however, a major drawback: owing to the sequential nature of the
iteration and small batch sizes, there is limited avenue for parallelization. While some efforts have
been made to parallelize SGD for Deep Learning (Dean et al., 2012; Das et al., 2016; Zhang et al.,
2015), the speed-ups and scalability obtained are often limited by the small batch sizes. One natu-
ral avenue for improving parallelism is to increase the batch size |Bk|. This increases the amount
of computation per iteration, which can be effectively distributed. However, practitioners have ob-
served that this leads to a loss in generalization performance; see e.g. (LeCun et al., 2012). In other
words, the performance of the model on testing data sets is often worse when trained with large-
batch methods as compared to small-batch methods. In our experiments, we have found the drop in
generalization (also called generalization gap) to be as high as 5% even for smaller networks.

In this paper, we present numerical results that shed light into this drawback of large-batch methods.
We observe that the generalization gap is correlated with a marked sharpness of the minimizers
obtained by large-batch methods. This motivates efforts at remedying the generalization problem,
as a training algorithm that employs large batches without sacriﬁcing generalization performance
would have the ability to scale to a much larger number of nodes than is possible today. This could
potentially reduce the training time by orders-of-magnitude; we present an idealized performance
model in the Appendix C to support this claim.

The paper is organized as follows. In the remainder of this section, we deﬁne the notation used in
this paper, and in Section 2 we present our main ﬁndings and their supporting numerical evidence.
In Section 3 we explore the performance of small-batch methods, and in Section 4 we brieﬂy discuss
the relationship between our results and recent theoretical work. We conclude with open questions
concerning the generalization gap, sharp minima, and possible modiﬁcations to make large-batch
training viable. In Appendix E, we present some attempts to overcome the problems of large-batch
training.

1.1 NOTATION

We use the notation fi to denote the composition of loss function and a prediction function corre-
sponding to the ith data point. The vector of weights is denoted by x and is subscripted by k to
denote an iteration. We use the term small-batch (SB) method to denote SGD, or one of its variants
like ADAM (Kingma & Ba, 2015) and ADAGRAD (Duchi et al., 2011), with the proviso that the
gradient approximation is based on a small mini-batch. In our setup, the batch Bk is randomly sam-
pled and its size is kept ﬁxed for every iteration. We use the term large-batch (LB) method to denote
any training algorithm that uses a large mini-batch. In our experiments, ADAM is used to explore
the behavior of both a small or a large batch method.

2 DRAWBACKS OF LARGE-BATCH METHODS

2.1 OUR MAIN OBSERVATION

As mentioned in Section 1, practitioners have observed a generalization gap when using large-batch
methods for training deep learning models. Interestingly, this is despite the fact that large-batch
methods usually yield a similar value of the training function as small-batch methods. One may put

2

Published as a conference paper at ICLR 2017

forth the following as possible causes for this phenomenon: (i) LB methods over-ﬁt the model; (ii)
LB methods are attracted to saddle points; (iii) LB methods lack the explorative properties of SB
methods and tend to zoom-in on the minimizer closest to the initial point; (iv) SB and LB methods
converge to qualitatively different minimizers with differing generalization properties. The data
presented in this paper supports the last two conjectures.

The main observation of this paper is as follows:

The lack of generalization ability is due to the fact that large-batch methods tend to converge
to sharp minimizers of the training function. These minimizers are characterized by a signif-
icant number of large positive eigenvalues in ∇2f (x), and tend to generalize less well.
In
contrast, small-batch methods converge to ﬂat minimizers characterized by having numerous
small eigenvalues of ∇2f (x). We have observed that the loss function landscape of deep neural
networks is such that large-batch methods are attracted to regions with sharp minimizers and
that, unlike small-batch methods, are unable to escape basins of attraction of these minimizers.

The concept of sharp and ﬂat minimizers have been discussed in the statistics and machine learning
literature. (Hochreiter & Schmidhuber, 1997) (informally) deﬁne a ﬂat minimizer ¯x as one for which
the function varies slowly in a relatively large neighborhood of ¯x. In contrast, a sharp minimizer ˆx
is such that the function increases rapidly in a small neighborhood of ˆx. A ﬂat minimum can be de-
scribed with low precision, whereas a sharp minimum requires high precision. The large sensitivity
of the training function at a sharp minimizer negatively impacts the ability of the trained model to
generalize on new data; see Figure 1 for a hypothetical illustration. This can be explained through
the lens of the minimum description length (MDL) theory, which states that statistical models that
require fewer bits to describe (i.e., are of low complexity) generalize better (Rissanen, 1983). Since
ﬂat minimizers can be speciﬁed with lower precision than to sharp minimizers, they tend to have bet-
ter generalization performance. Alternative explanations are proffered through the Bayesian view
of learning (MacKay, 1992), and through the lens of free Gibbs energy; see e.g. Chaudhari et al.
(2016).

Training Function

Testing Function

f (x)

Flat Minimum

Sharp Minimum

Figure 1: A Conceptual Sketch of Flat and Sharp Minima. The Y-axis indicates value of the loss
function and the X-axis the variables (parameters)

2.2 NUMERICAL EXPERIMENTS

In this section, we present numerical results to support the observations made above. To this end,
we make use of the visualization technique employed by (Goodfellow et al., 2014b) and a proposed
heuristic metric of sharpness (Equation (4)). We consider 6 multi-class classiﬁcation network con-
ﬁgurations for our experiments; they are described in Table 1. The details about the data sets and
network conﬁgurations are presented in Appendices A and B respectively. As is common for such
problems, we use the mean cross entropy loss as the objective function f .

The networks were chosen to exemplify popular conﬁgurations used in practice like AlexNet
(Krizhevsky et al., 2012) and VGGNet (Simonyan & Zisserman, 2014). Results on other networks

3

Published as a conference paper at ICLR 2017

Table 1: Network Conﬁgurations

Name Network Type
F1
F2
C1
C2
C3
C4

Fully Connected
Fully Connected
(Shallow) Convolutional
(Deep) Convolutional
(Shallow) Convolutional
(Deep) Convolutional

Architecture Data set
Section B.1 MNIST (LeCun et al., 1998a)
TIMIT (Garofolo et al., 1993)
Section B.2
CIFAR-10 (Krizhevsky & Hinton, 2009)
Section B.3
CIFAR-10
Section B.4
CIFAR-100 (Krizhevsky & Hinton, 2009)
Section B.3
CIFAR-100
Section B.4

and using other initialization strategies, activation functions, and data sets showed similar behavior.
Since the goal of our work is not to achieve state-of-the-art accuracy or time-to-solution on these
tasks but rather to characterize the nature of the minima for LB and SB methods, we only describe
the ﬁnal testing accuracy in the main paper and ignore convergence trends.

For all experiments, we used 10% of the training data as batch size for the large-batch experiments
and 256 data points for small-batch experiments. We used the ADAM optimizer for both regimes.
Experiments with other optimizers for the large-batch experiments, including ADAGRAD (Duchi
et al., 2011), SGD (Sutskever et al., 2013) and adaQN (Keskar & Berahas, 2016), led to similar
results. All experiments were conducted 5 times from different (uniformly distributed random)
starting points and we report both mean and standard-deviation of measured quantities. The baseline
performance for our setup is presented Table 2. From this, we can observe that on all networks, both
approaches led to high training accuracy but there is a signiﬁcant difference in the generalization
performance. The networks were trained, without any budget or limits, until the loss function ceased
to improve.

Table 2: Performance of small-batch (SB) and large-batch (LB) variants of ADAM on the 6 networks
listed in Table 1

Name
F1
F2
C1
C2
C3
C4

Training Accuracy

Testing Accuracy

SB

LB

SB
99.66% ± 0.05% 99.92% ± 0.01% 98.03% ± 0.07% 97.81% ± 0.07%
99.99% ± 0.03% 98.35% ± 2.08% 64.02% ± 0.2%
59.45% ± 1.05%
99.89% ± 0.02% 99.66% ± 0.2%
80.04% ± 0.12% 77.26% ± 0.42%
99.99% ± 0.04% 99.99% ± 0.01% 89.24% ± 0.12% 87.26% ± 0.07%
99.56% ± 0.44% 99.88% ± 0.30% 49.58% ± 0.39% 46.45% ± 0.43%
57.81% ± 0.17%
99.10% ± 1.23% 99.57% ± 1.84% 63.08% ± 0.5%

LB

We emphasize that the generalization gap is not due to over-ﬁtting or over-training as commonly
observed in statistics. This phenomenon manifest themselves in the form of a testing accuracy curve
that, at a certain iterate peaks, and then decays due to the model learning idiosyncrasies of the
training data. This is not what we observe in our experiments; see Figure 2 for the training–testing
curve of the F2 and C1 networks, which are representative of the rest. As such, early-stopping
heuristics aimed at preventing models from over-ﬁtting would not help reduce the generalization
gap. The difference between the training and testing accuracies for the networks is due to the
speciﬁc choice of the network (e.g. AlexNet, VGGNet etc.) and is not the focus of this study.
Rather, our goal is to study the source of the testing performance disparity of the two regimes, SB
and LB, on a given network model.

2.2.1 PARAMETRIC PLOTS

s and x(cid:63)

We ﬁrst present parametric 1-D plots of the function as described in (Goodfellow et al., 2014b).
Let x(cid:63)
(cid:96) indicate the solutions obtained by running ADAM using small and large batch sizes
respectively. We plot the loss function, on both training and testing data sets, along a line-segment
(cid:96) + (1 − α)x(cid:63)
containing the two points. Speciﬁcally, for α ∈ [−1, 2], we plot the function f (αx(cid:63)
s)
and also superimpose the classiﬁcation accuracy at the intermediate points; see Figure 31. For this

1The code to reproduce the parametric plot on exemplary networks can be found in our GitHub repository:

https://github.com/keskarnitish/large-batch-training.

4

Published as a conference paper at ICLR 2017

(a) Network F2

(b) Network C1

Figure 2: Training and testing accuracy for SB and LB methods as a function of epochs.

experiment, we randomly chose a pair of SB and LB minimizers from the 5 trials used to generate
the data in Table 2. The plots show that the LB minima are strikingly sharper than the SB minima
in this one-dimensional manifold. The plots in Figure 3 only explore a linear slice of the function,
but in Figure 7 in Appendix D, we plot f (sin( απ
2 )x(cid:63)
s) to monitor the function along a
curved path between the two minimizers . There too, the relative sharpness of the minima is evident.

(cid:96) + cos( απ

2 )x(cid:63)

2.2.2 SHARPNESS OF MINIMA

So far, we have used the term sharp minimizer loosely, but we noted that this concept has received
attention in the literature (Hochreiter & Schmidhuber, 1997). Sharpness of a minimizer can be
characterized by the magnitude of the eigenvalues of ∇2f (x), but given the prohibitive cost of this
computation in deep learning applications, we employ a sensitivity measure that, although imperfect,
is computationally feasible, even for large networks. It is based on exploring a small neighborhood
of a solution and computing the largest value that the function f can attain in that neighborhood. We
use that value to measure the sensitivity of the training function at the given local minimizer. Now,
since the maximization process is not accurate, and to avoid being mislead by the case when a large
value of f is attained only in a tiny subspace of Rn, we perform the maximization both in the entire
space Rn as well as in random manifolds. For that purpose, we introduce an n × p matrix A, whose
columns are randomly generated. Here p determines the dimension of the manifold, which in our
experiments is chosen as p = 100.

Speciﬁcally, let C(cid:15) denote a box around the solution over which the maximization of f is performed,
and let A ∈ Rn×p be the matrix deﬁned above. In order to ensure invariance of sharpness to problem
dimension and sparsity, we deﬁne the constraint set C(cid:15) as:

C(cid:15) = {z ∈ Rp : −(cid:15)(|(A+x)i| + 1) ≤ zi ≤ (cid:15)(|(A+x)i| + 1) ∀i ∈ {1, 2, · · · , p}},
where A+ denotes the pseudo-inverse of A. Thus (cid:15) controls the size of the box. We can now deﬁne
our measure of sharpness (or sensitivity).
Metric 2.1. Given x ∈ Rn, (cid:15) > 0 and A ∈ Rn×p, we deﬁne the (C(cid:15), A)-sharpness of f at x as:

(3)

φx,f ((cid:15), A) :=

(maxy∈C(cid:15) f (x + Ay)) − f (x)
1 + f (x)

× 100.

(4)

Unless speciﬁed otherwise, we use this metric for sharpness for the rest of the paper; if A is not spec-
iﬁed, it is assumed to be the identity matrix, In. (We note in passing that, in the convex optimization
literature, the term sharp minimum has a different deﬁnition (Ferris, 1988), but that concept is not
useful for our purposes.)

In Tables 3 and 4, we present the values of the sharpness metric (4) for the minimizers of the various
problems. Table 3 explores the full-space (i.e., A = In) whereas Table 4 uses a randomly sampled
n × 100 dimensional matrix A. We report results with two values of (cid:15), (10−3, 5 · 10−4). In all
experiments, we solve the maximization problem in Equation (4) inexactly by applying 10 iterations
of L-BFGS-B (Byrd et al., 1995). This limit on the number of iterations was necessitated by the

5

Published as a conference paper at ICLR 2017

(a) F1

(b) F2

(c) C1

(d) C2

(e) C3

(f) C4

Figure 3: Parametric Plots – Linear (Left vertical axis corresponds to cross-entropy loss, f , and
right vertical axis corresponds to classiﬁcation accuracy; solid line indicates training data set and
dashed line indicated testing data set); α = 0 corresponds to the SB minimizer and α = 1 to the LB
minimizer.

large cost of evaluating the true objective f . Both tables show a 1–2 order-of-magnitude difference
between the values of our metric for the SB and LB regimes. These results reinforce the view that
the solutions obtained by a large-batch method deﬁnes points of larger sensitivity of the training
function. In Appedix E, we describe approaches to attempt to remedy this generalization problem
of LB methods. These approaches include data augmentation, conservative training and adversarial
training. Our preliminary ﬁndings show that these approaches help reduce the generalization gap
but still lead to relatively sharp minimizers and as such, do not completely remedy the problem.
Note that Metric 2.1 is closely related to the spectrum of ∇2f (x). Assuming (cid:15) to be small enough,
when A = In, the value (4) relates to the largest eigenvalue of ∇2f (x) and when A is randomly
sampled it approximates the Ritz value of ∇2f (x) projected onto the column-space of A.

6

Published as a conference paper at ICLR 2017

Table 3: Sharpness of Minima in Full Space; (cid:15) is deﬁned in (3).

(cid:15) = 10−3

(cid:15) = 5 · 10−4

SB
1.23 ± 0.83
1.39 ± 0.02
28.58 ± 3.13
8.68 ± 1.32
29.85 ± 5.98
12.83 ± 3.84

LB
205.14 ± 69.52
310.64 ± 38.46
707.23 ± 43.04
925.32 ± 38.29
258.75 ± 8.96
421.84 ± 36.97

SB
0.61 ± 0.27
0.90 ± 0.05
7.08 ± 0.88
2.07 ± 0.86
8.56 ± 0.99
4.07 ± 0.87

LB
42.90 ± 17.14
93.15 ± 6.81
227.31 ± 23.23
175.31 ± 18.28
105.11 ± 13.22
109.35 ± 16.57

Table 4: Sharpness of Minima in Random Subspaces of Dimension 100

(cid:15) = 10−3

(cid:15) = 5 · 10−4

SB
0.11 ± 0.00
0.29 ± 0.02
2.18 ± 0.23
0.95 ± 0.34
17.02 ± 2.20
6.05 ± 1.13

LB
9.22 ± 0.56
23.63 ± 0.54
137.25 ± 21.60
25.09 ± 2.61
236.03 ± 31.26
72.99 ± 10.96

SB
0.05 ± 0.00
0.05 ± 0.00
0.71 ± 0.15
0.31 ± 0.08
4.03 ± 1.45
1.89 ± 0.33

LB
9.17 ± 0.14
6.28 ± 0.19
29.50 ± 7.48
5.82 ± 0.52
86.96 ± 27.39
19.85 ± 4.12

F1
F2
C1
C2
C3
C4

F1
F2
C1
C2
C3
C4

We conclude this section by noting that the sharp minimizers identiﬁed in our experiments do not
resemble a cone, i.e., the function does not increase rapidly along all (or even most) directions. By
sampling the loss function in a neighborhood of LB solutions, we observe that it rises steeply only
along a small dimensional subspace (e.g. 5% of the whole space); on most other directions, the
function is relatively ﬂat.

3 SUCCESS OF SMALL-BATCH METHODS

It is often reported that when increasing the batch size for a problem, there exists a threshold after
which there is a deterioration in the quality of the model. This behavior can be observed for the F2
and C1 networks in Figure 4. In both of these experiments, there is a batch size (≈ 15000 for F2 and
≈ 500 for C1) after which there is a large drop in testing accuracy. Notice also that the upward drift
in value of the sharpness is considerably reduced around this threshold. Similar thresholds exist for
the other networks in Table 1.

Let us now consider the behavior of SB methods, which use noisy gradients in the step computation.
From the results reported in the previous section, it appears that noise in the gradient pushes the
iterates out of the basin of attraction of sharp minimizers and encourages movement towards a ﬂatter
minimizer where noise will not cause exit from that basin. When the batch size is greater than the
threshold mentioned above, the noise in the stochastic gradient is not sufﬁcient to cause ejection
from the initial basin leading to convergence to sharper a minimizer.

To explore that in more detail, consider the following experiment. We train the network for 100
epochs using ADAM with a batch size of 256, and retain the iterate after each epoch in memory.
Using these 100 iterates as starting points we train the network using a LB method for 100 epochs
and receive a 100 piggybacked (or warm-started) large-batch solutions. We plot in Figure 5 the
testing accuracy and sharpness of these large-batch solutions, along with the testing accuracy of the
small-batch iterates. Note that when warm-started with only a few initial epochs, the LB method
does not yield a generalization improvement. The concomitant sharpness of the iterates also stays
high. On the other hand, after certain number of epochs of warm-starting, the accuracy improves
and sharpness of the large-batch iterates drop. This happens, apparently, when the SB method has
ended its exploration phase and discovered a ﬂat minimizer; the LB method is then able to converge
towards it, leading to good testing accuracy.

It has been speculated that LB methods tend to be attracted to minimizers close to the starting point
x0, whereas SB methods move away and locate minimizers that are farther away. Our numerical

7

Published as a conference paper at ICLR 2017

(a) F2

(b) C1

Figure 4: Testing Accuracy and Sharpness v/s Batch Size. The X-axis corresponds to the batch size
used for training the network for 100 epochs, left Y-axis corresponds to the testing accuracy at the
ﬁnal iterate and right Y-axis corresponds to the sharpness of that iterate. We report sharpness for
two values of (cid:15): 10−3 and 5 · 10−4.

(a) F2

(b) C1

Figure 5: Warm-starting experiments. The upper ﬁgures report the testing accuracy of the SB
method (blue line) and the testing accuracy of the warm started (piggybacked) LB method (red line),
as a function of the number of epochs of the SB method. The lower ﬁgures plot the sharpness mea-
sure (4) for the solutions obtained by the piggybacked LB method v/s the number of warm-starting
epochs of the SB method.

8

Published as a conference paper at ICLR 2017

(a) F2

(b) C1

Figure 6: Sharpness v/s Cross Entropy Loss for SB and LB methods.

experiments support this view: we observed that the ratio of (cid:107)x(cid:63)
range of 3–10.

s − x0(cid:107)2 and (cid:107)x(cid:63)

(cid:96) − x0(cid:107)2 was in the

In order to further illustrate the qualitative difference between the solutions obtained by SB and LB
methods, we plot in Figure 6 our sharpness measure (4) against the loss function (cross entropy)
for one random trial of the F2 and C1 networks. For larger values of the loss function, i.e., near
the initial point, SB and LB method yield similar values of sharpness. As the loss function reduces,
the sharpness of the iterates corresponding to the LB method rapidly increases, whereas for the SB
method the sharpness stays relatively constant initially and then reduces, suggesting an exploration
phase followed by convergence to a ﬂat minimizer.

4 DISCUSSION AND CONCLUSION

In this paper, we present numerical experiments that support the view that convergence to sharp
minimizers gives rise to the poor generalization of large-batch methods for deep learning. To this
end, we provide one-dimensional parametric plots and perturbation (sharpness) measures for a vari-
ety of deep learning architectures. In Appendix E, we describe our attempts to remedy the problem,
including data augmentation, conservative training and robust optimization. Our preliminary inves-
tigation suggests that these strategies do not correct the problem; they improve the generalization of
large-batch methods but still lead to relatively sharp minima. Another prospective remedy includes
the use of dynamic sampling where the batch size is increased gradually as the iteration progresses
(Byrd et al., 2012; Friedlander & Schmidt, 2012). The potential viability of this approach is sug-
gested by our warm-starting experiments (see Figure 5) wherein high testing accuracy is achieved
using a large-batch method that is warm-start with a small-batch method.

Recently, a number of researchers have described interesting theoretical properties of the loss sur-
face of deep neural networks; see e.g. (Choromanska et al., 2015; Soudry & Carmon, 2016; Lee
et al., 2016). Their work shows that, under certain regularity assumptions, the loss function of deep
learning models is fraught with many local minimizers and that many of these minimizers corre-
spond to a similar loss function value. Our results are in alignment these observations since, in our
experiments, both sharp and ﬂat minimizers have very similar loss function values. We do not know,
however, if the theoretical models mentioned above provide information about the existence and
density of sharp minimizers of the loss surface.

Our results suggest some questions: (a) can one prove that large-batch (LB) methods typically con-
verge to sharp minimizers of deep learning training functions? (In this paper, we only provided some
numerical evidence.); (b) what is the relative density of the two kinds of minima?; (c) can one design
neural network architectures for various tasks that are suitable to the properties of LB methods?; (d)
can the networks be initialized in a way that enables LB methods to succeed?; (e) is it possible,
through algorithmic or regulatory means to steer LB methods away from sharp minimizers?

9

Published as a conference paper at ICLR 2017

REFERENCES

Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning. Book in preparation for MIT

Press, 2016. URL http://www.deeplearningbook.org.

Dimitris Bertsimas, Omid Nohadani, and Kwong Meng Teo. Robust optimization for unconstrained

simulation-based problems. Operations Research, 58(1):161–178, 2010.

L´eon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,

17(9):142, 1998.

L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine

learning. arXiv preprint arXiv:1606.04838, 2016.

Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for
bound constrained optimization. SIAM Journal on Scientiﬁc Computing, 16(5):1190–1208, 1995.

Richard H Byrd, Gillian M Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in opti-
mization methods for machine learning. Mathematical programming, 134(1):127–155, 2012.

Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing

gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.

Anna Choromanska, Mikael Henaff, Michael Mathieu, G´erard Ben Arous, and Yann LeCun. The

loss surfaces of multilayer networks. In AISTATS, 2015.

Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan Vaidynathan, Srinivas Srid-
haran, Dhiraj Kalamkar, Bharat Kaul, and Pradeep Dubey. Distributed deep learning using syn-
chronous stochastic gradient descent. arXiv preprint arXiv:1602.06709, 2016.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in
neural information processing systems, pp. 1223–1231, 2012.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

Michael Charles Ferris. Weak sharp minima and penalty functions in mathematical programming.

PhD thesis, University of Cambridge, 1988.

Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting.

SIAM Journal on Scientiﬁc Computing, 34(3):A1380–A1405, 2012.

John S Garofolo, Lori F Lamel, William M Fisher, Jonathan G Fiscus, David S Pallett, Nancy L
Dahlgren, and Victor Zue. Timit acoustic-phonetic continuous speech corpus. Linguistic data
consortium, Philadelphia, 33, 1993.

Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pp. 797–842, 2015.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014a.

Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network

optimization problems. arXiv preprint arXiv:1412.6544, 2014b.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
In 2013 IEEE international conference on acoustics, speech and signal

rent neural networks.
processing, pp. 6645–6649. IEEE, 2013.

M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient

descent. arXiv preprint arXiv:1509.01240, 2015.

Sepp Hochreiter and J¨urgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

10

Published as a conference paper at ICLR 2017

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by

reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

Nitish Shirish Keskar and Albert S. Berahas. adaQN: An Adaptive Quasi-Newton Algorithm for

Training RNNs, pp. 1–16. Springer International Publishing, Cham, 2016.

D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on

Learning Representations (ICLR 2015), 2015.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998a.

Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits,

1998b.

Yann A LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller. Efﬁcient backprop. In

Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012.

Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges

to minimizers. University of California, Berkeley, 1050:16, 2016.

Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efﬁcient mini-batch training for
stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 661–670. ACM, 2014.

David JC MacKay. A practical bayesian framework for backpropagation networks. Neural compu-

tation, 4(3):448–472, 1992.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

Hossein Mobahi. Training recurrent neural networks by diffusion. arXiv preprint arXiv:1601.04114,

2016.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel,
Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. The kaldi speech recognition
In IEEE 2011 workshop on automatic speech recognition and understanding, number
toolkit.
EPFL-CONF-192584. IEEE Signal Processing Society, 2011.

Jorma Rissanen. A universal prior for integers and estimation by minimum description length. The

Annals of statistics, pp. 416–431, 1983.

Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432, 2015.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556, 2014.

Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees

for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
In Proceedings of the 30th International Conference on Machine Learning

in deep learning.
(ICML 2013), pp. 1139–1147, 2013.

11

Published as a conference paper at ICLR 2017

Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In

Advances in Neural Information Processing Systems, pp. 685–693, 2015.

Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep

neural networks via stability training. arXiv preprint arXiv:1604.04326, 2016.

A DETAILS ABOUT DATA SETS

We summarize the data sets used in our experiments in Table 5. TIMIT is a speech recognition
data set which is pre-processed using Kaldi (Povey et al., 2011) and trained using a fully-connected
network. The rest of the data sets are used without any pre-processing.

Table 5: Data Sets

Data Set
MNIST
TIMIT
CIFAR-10
CIFAR-100

# Data Points
Test
Train
10000
60000
310621
721329
10000
50000
10000
50000

# Features
28 × 28
360
32 × 32
32 × 32

# Classes Reference

10
1973
10
100

(LeCun et al., 1998a;b)
(Garofolo et al., 1993)
(Krizhevsky & Hinton, 2009)
(Krizhevsky & Hinton, 2009)

B ARCHITECTURE OF NETWORKS

B.1 NETWORK F1

For this network, we use a 784-dimensional input layer followed by 5 batch-normalized (Ioffe &
Szegedy, 2015) layers of 512 neurons each with ReLU activations. The output layer consists of 10
neurons with the softmax activation.

B.2 NETWORK F2

The network architecture for F2 is similar to F1. We use a 360-dimensional input layer followed by
7 batch-normalized layers of 512 neurons with ReLU activation. The output layer consists of 1973
neurons with the softmax activation.

B.3 NETWORKS C1 AND C3

The C1 network is a modiﬁed version of the popular AlexNet conﬁguration (Krizhevsky et al.,
2012). For simplicity, denote a stack of n convolution layers of a ﬁlters and a Kernel size of b × c
with stride length of d as n×[a, b, c, d]. The C1 conﬁguration uses 2 sets of [64, 5, 5, 2]–MaxPool(3)
followed by 2 dense layers of sizes (384, 192) and ﬁnally, an output layer of size 10. We use batch-
normalization for all layers and ReLU activations. We also use Dropout (Srivastava et al., 2014) of
0.5 retention probability for the two dense layers. The conﬁguration C3 is identical to C1 except it
uses 100 softmax outputs instead of 10.

B.4 NETWORKS C2 AND C4

The C2 network is a modiﬁed version of the popular VGG conﬁguration (Simonyan & Zisserman,
2014). The C3 network uses the conﬁguration: 2×[64, 3, 3, 1], 2×[128, 3, 3, 1], 3×[256, 3, 3, 1], 3×
[512, 3, 3, 1], 3 × [512, 3, 3, 1] which a MaxPool(2) after each stack. This stack is followed by a 512-
dimensional dense layer and ﬁnally, a 10-dimensional output layer. The activation and properties of
each layer is as in B.3. As is the case with C3 and C1, the conﬁguration C4 is identical to C2 except
that it uses 100 softmax outputs instead of 10.

12

Published as a conference paper at ICLR 2017

C PERFORMANCE MODEL

As mentioned in Section 1, a training algorithm that operates in the large-batch regime without
suffering from a generalization gap would have the ability to scale to much larger number of nodes
than is currently possible. Such and algorithm might also improve training time through faster
convergence. We present an idealized performance model that demonstrates our goal.

For LB method to be competitive with SB method, the LB method must (i) converge to minimizers
that generalize well, and (ii) do it in a reasonably number of iterations, which we analyze here. Let Is
and I(cid:96) be number of iterations required by SB and LB methods to reach the point of comparable test
accuracy, respectively. Let Bs and B(cid:96) be corresponding batch sizes and P be number of processors
being used for training. Assume that P < B(cid:96), and let fs(P ) be the parallel efﬁciency of the SB
method. For simplicity, we assume that f(cid:96)(P ), the parallel efﬁciency of the LB method, is 1.0. In
other words, we assume that the LB method is perfectly scalable due to use of a large batch size.

For LB to be faster than SB, we must have

In other words, the ratio of iterations of LB to the iterations of SB should be

I(cid:96)

B(cid:96)
P

< Is

Bs
P fs(P )

.

I(cid:96)
Is

<

Bs
fs(P )B(cid:96)

.

For example, if fs(P ) = 0.2 and Bs/B(cid:96) = 0.1, the LB method must converge in at most half as
many iterations as the SB method to see performance beneﬁts. We refer the reader to (Das et al.,
2016) for a more detailed model and a commentary on the effect of batch-size on the performance.

D CURVILINEAR PARAMETRIC PLOTS

The parametric plots for the curvilinear path from x(cid:63)
found in Figure 7.

s to x(cid:63)

(cid:96) , i.e., f (sin( απ

2 )x(cid:63)

(cid:96) + cos( απ

2 )x(cid:63)

s) can be

E ATTEMPTS TO IMPROVE LB METHODS

In this section, we discuss a few strategies that aim to remedy the problem of poor generalization
for large-batch methods. As in Section 2, we use 10% as the percentage batch-size for large-batch
experiments and 256 for small-batch methods. For all experiments, we use ADAM as the optimizer
irrespective of batch-size.

E.1 DATA AUGMENTATION

Given that large-batch methods appear to be attracted to sharp minimizers, one can ask whether it is
possible to modify the geometry of the loss function so that it is more benign to large-batch meth-
ods. The loss function depends both on the geometry of the objective function and to the size and
properties of the training set. One approach we consider is data augmentation; see e.g. (Krizhevsky
et al., 2012; Simonyan & Zisserman, 2014). The application of this technique is domain speciﬁc but
generally involves augmenting the data set through controlled modiﬁcations on the training data. For
instance, in the case of image recognition, the training set can be augmented through translations,
rotations, shearing and ﬂipping of the training data. This technique leads to regularization of the
network and has been employed for improving testing accuracy on several data sets.

In our experiments, we train the 4 image-based (convolutional) networks using aggressive data aug-
mentation and present the results in Table 6. For the augmentation, we use horizontal reﬂections,
random rotations up to 10◦ and random translation of up to 0.2 times the size of the image. It is
evident from the table that, while the LB method achieves accuracy comparable to the SB method
(also with training data augmented), the sharpness of the minima still exists, suggesting sensitivity
to images contained in neither training or testing set. In this section, we exclude parametric plots and
sharpness values for the SB method owing to space constraints and the similarity to those presented
in Section 2.2.

13

Published as a conference paper at ICLR 2017

(a) F1

(b) F2

(c) C1

(d) C2

(e) C3

(f) C4

Figure 7: Parametric Plots – Curvilinear (Left vertical axis corresponds to cross-entropy loss, f ,
and right vertical axis corresponds to classiﬁcation accuracy; solid line indicates training data set
and dashed line indicated testing data set); α = 0 corresponds to the SB minimizer while α = 1
corresponds to the LB minimizer

Table 6: Effect of Data Augmentation

Testing Accuracy

Sharpness (LB method)

Baseline (SB)

Augmented LB
83.63% ± 0.14% 82.50% ± 0.67% 231.77 ± 30.50
89.82% ± 0.12% 90.26% ± 1.15% 468.65 ± 47.86
54.55% ± 0.44% 53.03% ± 0.33% 103.68 ± 11.93
65.88 ± 0.13% 271.06 ± 29.69
63.05% ± 0.5%

(cid:15) = 10−3

(cid:15) = 5 · 10−4
45.89 ± 3.83
105.22 ± 19.57
37.67 ± 3.46
45.31 ± 5.93

C1
C2
C3
C4

14

Published as a conference paper at ICLR 2017

Table 7: Effect of Conservative Training

Testing Accuracy

Sharpness (LB method)

Baseline (SB)

(cid:15) = 10−3
Conservative LB
232.25 ± 63.81
98.03% ± 0.07% 98.12% ± 0.01%
928.40 ± 51.63
64.02% ± 0.2%
61.94% ± 1.10%
80.04% ± 0.12% 78.41% ± 0.22%
520.34 ± 34.91
89.24% ± 0.05% 88.495% ± 0.63% 632.01 ± 208.01
337.92 ± 33.09
49.58% ± 0.39% 45.98% ± 0.54%
354.94 ± 20.23
63.08% ± 0.10%

62.51 ± 0.67

(cid:15) = 5 · 10−4
46.02 ± 12.58
190.77 ± 25.33
171.19 ± 15.13
108.88 ± 47.36
110.69 ± 3.88
68.76 ± 16.29

F1
F2
C1
C2
C3
C4

E.2 CONSERVATIVE TRAINING

In (Li et al., 2014), the authors argue that the convergence rate of SGD for the large-batch setting
can be improved by obtaining iterates through the following proximal sub-problem.

xk+1 = arg min

x

1
|Bk|

(cid:88)

i∈Bk

fi(x) +

(cid:107)x − xk(cid:107)2
2

λ
2

(5)

The motivation for this strategy is, in the context of large-batch methods, to better utilize a batch
before moving onto the next one. The minimization problem is solved inexactly using 3–5 itera-
tions of gradient descent, co-ordinate descent or L-BFGS. (Li et al., 2014) report that this not only
improves the convergence rate of SGD but also leads to improved empirical performance on con-
vex machine learning problems. The underlying idea of utilizing a batch is not speciﬁc to convex
problems and we can apply the same framework for deep learning, however, without theoretical
guarantees. Indeed, similar algorithms were proposed in (Zhang et al., 2015) and (Mobahi, 2016)
for Deep Learning. The former placed emphasis on parallelization of small-batch SGD and asyn-
chrony while the latter on a diffusion-continuation mechanism for training. The results using the
conservative training approach are presented in Figure 7. In all experiments, we solve the problem
(5) using 3 iterations of ADAM and set the regularization parameter λ to be 10−3. Again, there is
a statistically signiﬁcant improvement in the testing accuracy of the large-batch method but it does
not solve the problem of sensitivity.

E.3 ROBUST TRAINING

A natural way of avoiding sharp minima is through robust optimization techniques. These methods
attempt to optimize a worst-case cost as opposed to the nominal (or true) cost. Mathematically,
given an (cid:15) > 0, these techniques solve the problem

min
x

φ(x) := max

f (x + ∆x)

(cid:107)∆x(cid:107)≤(cid:15)

(6)

Geometrically, classical (nominal) optimization attempts to locate the lowest point of a valley, while
robust optimization attempts to lower an (cid:15)–disc down the loss surface. We refer an interested reader
to (Bertsimas et al., 2010), and the references therein, for a review of non-convex robust optimiza-
tion. A direct application of this technique is, however, not feasible in our context since each itera-
tion is prohibitively expensive because it involves solving a large-scale second-order conic program
(SOCP).

15

Published as a conference paper at ICLR 2017

Figure 8: Illustration of Robust Optimization

In the context of Deep Learning, there are two inter-dependent forms of robustness: robustness to
the data and robustness to the solution. The former exploits the fact that the function f is inherently
a statistical model, while the latter treats f as a black-box function. In (Shaham et al., 2015), the
authors prove the equivalence between robustness of the solution (with respect to the data) and
adversarial training (Goodfellow et al., 2014a).

Given the partial success of the data augmentation strategy, it is natural to question the efﬁcacy
of adversarial training. As described in (Goodfellow et al., 2014a), adversarial training also aims
to artiﬁcially increase the training set but, unlike randomized data augmentation, uses the model’s
sensitivity to construct new examples. Despite its intuitive appeal, in our experiments, we found
that this strategy did not improve generalization. Similarly, we observed no generalization beneﬁt
from the stability training proposed by (Zheng et al., 2016). In both cases, the testing accuracy,
sharpness values and the parametric plots were similar to the unmodiﬁed (baseline) case discussed
in Section 2. It remains to be seen whether adversarial training (or any other form of robust training)
can increase the viability of large-batch training.

16

7
1
0
2
 
b
e
F
 
9
 
 
]

G
L
.
s
c
[
 
 
2
v
6
3
8
4
0
.
9
0
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

ON LARGE-BATCH TRAINING FOR DEEP LEARNING:
GENERALIZATION GAP AND SHARP MINIMA

Nitish Shirish Keskar∗
Northwestern University
Evanston, IL 60208
keskar.nitish@u.northwestern.edu

Dheevatsa Mudigere
Intel Corporation
Bangalore, India
dheevatsa.mudigere@intel.com

Jorge Nocedal
Northwestern University
Evanston, IL 60208
j-nocedal@northwestern.edu

Mikhail Smelyanskiy
Intel Corporation
Santa Clara, CA 95054
mikhail.smelyanskiy@intel.com

Ping Tak Peter Tang
Intel Corporation
Santa Clara, CA 95054
peter.tang@intel.com

ABSTRACT

The stochastic gradient descent (SGD) method and its variants are algorithms of
choice for many Deep Learning tasks. These methods operate in a small-batch
regime wherein a fraction of the training data, say 32–512 data points, is sampled
to compute an approximation to the gradient. It has been observed in practice that
when using a larger batch there is a degradation in the quality of the model, as
measured by its ability to generalize. We investigate the cause for this generaliza-
tion drop in the large-batch regime and present numerical evidence that supports
the view that large-batch methods tend to converge to sharp minimizers of the
training and testing functions—and as is well known, sharp minima lead to poorer
generalization. In contrast, small-batch methods consistently converge to ﬂat min-
imizers, and our experiments support a commonly held view that this is due to the
inherent noise in the gradient estimation. We discuss several strategies to attempt
to help large-batch methods eliminate this generalization gap.

1

INTRODUCTION

Deep Learning has emerged as one of the cornerstones of large-scale machine learning. Deep Learn-
ing models are used for achieving state-of-the-art results on a wide variety of tasks including com-
puter vision, natural language processing and reinforcement learning; see (Bengio et al., 2016) and
the references therein. The problem of training these networks is one of non-convex optimization.
Mathematically, this can be represented as:

min
x∈Rn

f (x) :=

1
M

M
(cid:88)

i=1

fi(x),

(1)

where fi is a loss function for data point i ∈ {1, 2, · · · , M } which captures the deviation of the
model prediction from the data, and x is the vector of weights being optimized. The process of
optimizing this function is also called training of the network. Stochastic Gradient Descent (SGD)
(Bottou, 1998; Sutskever et al., 2013) and its variants are often used for training deep networks.

∗Work was performed when author was an intern at Intel Corporation

1

Published as a conference paper at ICLR 2017

These methods minimize the objective function f by iteratively taking steps of the form:

xk+1 = xk − αk

(cid:32)

1
|Bk|

(cid:88)

i∈Bk

(cid:33)

∇fi(xk)

,

(2)

where Bk ⊂ {1, 2, · · · , M } is the batch sampled from the data set and αk is the step size at iteration
k. These methods can be interpreted as gradient descent using noisy gradients, which and are often
referred to as mini-batch gradients with batch size |Bk|. SGD and its variants are employed in a
small-batch regime, where |Bk| (cid:28) M and typically |Bk| ∈ {32, 64, · · · , 512}. These conﬁgura-
tions have been successfully used in practice for a large number of applications; see e.g. (Simonyan
& Zisserman, 2014; Graves et al., 2013; Mnih et al., 2013). Many theoretical properties of these
methods are known. These include guarantees of: (a) convergence to minimizers of strongly-convex
functions and to stationary points for non-convex functions (Bottou et al., 2016), (b) saddle-point
avoidance (Ge et al., 2015; Lee et al., 2016), and (c) robustness to input data (Hardt et al., 2015).

Stochastic gradient methods have, however, a major drawback: owing to the sequential nature of the
iteration and small batch sizes, there is limited avenue for parallelization. While some efforts have
been made to parallelize SGD for Deep Learning (Dean et al., 2012; Das et al., 2016; Zhang et al.,
2015), the speed-ups and scalability obtained are often limited by the small batch sizes. One natu-
ral avenue for improving parallelism is to increase the batch size |Bk|. This increases the amount
of computation per iteration, which can be effectively distributed. However, practitioners have ob-
served that this leads to a loss in generalization performance; see e.g. (LeCun et al., 2012). In other
words, the performance of the model on testing data sets is often worse when trained with large-
batch methods as compared to small-batch methods. In our experiments, we have found the drop in
generalization (also called generalization gap) to be as high as 5% even for smaller networks.

In this paper, we present numerical results that shed light into this drawback of large-batch methods.
We observe that the generalization gap is correlated with a marked sharpness of the minimizers
obtained by large-batch methods. This motivates efforts at remedying the generalization problem,
as a training algorithm that employs large batches without sacriﬁcing generalization performance
would have the ability to scale to a much larger number of nodes than is possible today. This could
potentially reduce the training time by orders-of-magnitude; we present an idealized performance
model in the Appendix C to support this claim.

The paper is organized as follows. In the remainder of this section, we deﬁne the notation used in
this paper, and in Section 2 we present our main ﬁndings and their supporting numerical evidence.
In Section 3 we explore the performance of small-batch methods, and in Section 4 we brieﬂy discuss
the relationship between our results and recent theoretical work. We conclude with open questions
concerning the generalization gap, sharp minima, and possible modiﬁcations to make large-batch
training viable. In Appendix E, we present some attempts to overcome the problems of large-batch
training.

1.1 NOTATION

We use the notation fi to denote the composition of loss function and a prediction function corre-
sponding to the ith data point. The vector of weights is denoted by x and is subscripted by k to
denote an iteration. We use the term small-batch (SB) method to denote SGD, or one of its variants
like ADAM (Kingma & Ba, 2015) and ADAGRAD (Duchi et al., 2011), with the proviso that the
gradient approximation is based on a small mini-batch. In our setup, the batch Bk is randomly sam-
pled and its size is kept ﬁxed for every iteration. We use the term large-batch (LB) method to denote
any training algorithm that uses a large mini-batch. In our experiments, ADAM is used to explore
the behavior of both a small or a large batch method.

2 DRAWBACKS OF LARGE-BATCH METHODS

2.1 OUR MAIN OBSERVATION

As mentioned in Section 1, practitioners have observed a generalization gap when using large-batch
methods for training deep learning models. Interestingly, this is despite the fact that large-batch
methods usually yield a similar value of the training function as small-batch methods. One may put

2

Published as a conference paper at ICLR 2017

forth the following as possible causes for this phenomenon: (i) LB methods over-ﬁt the model; (ii)
LB methods are attracted to saddle points; (iii) LB methods lack the explorative properties of SB
methods and tend to zoom-in on the minimizer closest to the initial point; (iv) SB and LB methods
converge to qualitatively different minimizers with differing generalization properties. The data
presented in this paper supports the last two conjectures.

The main observation of this paper is as follows:

The lack of generalization ability is due to the fact that large-batch methods tend to converge
to sharp minimizers of the training function. These minimizers are characterized by a signif-
icant number of large positive eigenvalues in ∇2f (x), and tend to generalize less well.
In
contrast, small-batch methods converge to ﬂat minimizers characterized by having numerous
small eigenvalues of ∇2f (x). We have observed that the loss function landscape of deep neural
networks is such that large-batch methods are attracted to regions with sharp minimizers and
that, unlike small-batch methods, are unable to escape basins of attraction of these minimizers.

The concept of sharp and ﬂat minimizers have been discussed in the statistics and machine learning
literature. (Hochreiter & Schmidhuber, 1997) (informally) deﬁne a ﬂat minimizer ¯x as one for which
the function varies slowly in a relatively large neighborhood of ¯x. In contrast, a sharp minimizer ˆx
is such that the function increases rapidly in a small neighborhood of ˆx. A ﬂat minimum can be de-
scribed with low precision, whereas a sharp minimum requires high precision. The large sensitivity
of the training function at a sharp minimizer negatively impacts the ability of the trained model to
generalize on new data; see Figure 1 for a hypothetical illustration. This can be explained through
the lens of the minimum description length (MDL) theory, which states that statistical models that
require fewer bits to describe (i.e., are of low complexity) generalize better (Rissanen, 1983). Since
ﬂat minimizers can be speciﬁed with lower precision than to sharp minimizers, they tend to have bet-
ter generalization performance. Alternative explanations are proffered through the Bayesian view
of learning (MacKay, 1992), and through the lens of free Gibbs energy; see e.g. Chaudhari et al.
(2016).

Training Function

Testing Function

f (x)

Flat Minimum

Sharp Minimum

Figure 1: A Conceptual Sketch of Flat and Sharp Minima. The Y-axis indicates value of the loss
function and the X-axis the variables (parameters)

2.2 NUMERICAL EXPERIMENTS

In this section, we present numerical results to support the observations made above. To this end,
we make use of the visualization technique employed by (Goodfellow et al., 2014b) and a proposed
heuristic metric of sharpness (Equation (4)). We consider 6 multi-class classiﬁcation network con-
ﬁgurations for our experiments; they are described in Table 1. The details about the data sets and
network conﬁgurations are presented in Appendices A and B respectively. As is common for such
problems, we use the mean cross entropy loss as the objective function f .

The networks were chosen to exemplify popular conﬁgurations used in practice like AlexNet
(Krizhevsky et al., 2012) and VGGNet (Simonyan & Zisserman, 2014). Results on other networks

3

Published as a conference paper at ICLR 2017

Table 1: Network Conﬁgurations

Name Network Type
F1
F2
C1
C2
C3
C4

Fully Connected
Fully Connected
(Shallow) Convolutional
(Deep) Convolutional
(Shallow) Convolutional
(Deep) Convolutional

Architecture Data set
Section B.1 MNIST (LeCun et al., 1998a)
TIMIT (Garofolo et al., 1993)
Section B.2
CIFAR-10 (Krizhevsky & Hinton, 2009)
Section B.3
CIFAR-10
Section B.4
CIFAR-100 (Krizhevsky & Hinton, 2009)
Section B.3
CIFAR-100
Section B.4

and using other initialization strategies, activation functions, and data sets showed similar behavior.
Since the goal of our work is not to achieve state-of-the-art accuracy or time-to-solution on these
tasks but rather to characterize the nature of the minima for LB and SB methods, we only describe
the ﬁnal testing accuracy in the main paper and ignore convergence trends.

For all experiments, we used 10% of the training data as batch size for the large-batch experiments
and 256 data points for small-batch experiments. We used the ADAM optimizer for both regimes.
Experiments with other optimizers for the large-batch experiments, including ADAGRAD (Duchi
et al., 2011), SGD (Sutskever et al., 2013) and adaQN (Keskar & Berahas, 2016), led to similar
results. All experiments were conducted 5 times from different (uniformly distributed random)
starting points and we report both mean and standard-deviation of measured quantities. The baseline
performance for our setup is presented Table 2. From this, we can observe that on all networks, both
approaches led to high training accuracy but there is a signiﬁcant difference in the generalization
performance. The networks were trained, without any budget or limits, until the loss function ceased
to improve.

Table 2: Performance of small-batch (SB) and large-batch (LB) variants of ADAM on the 6 networks
listed in Table 1

Name
F1
F2
C1
C2
C3
C4

Training Accuracy

Testing Accuracy

SB

LB

SB
99.66% ± 0.05% 99.92% ± 0.01% 98.03% ± 0.07% 97.81% ± 0.07%
99.99% ± 0.03% 98.35% ± 2.08% 64.02% ± 0.2%
59.45% ± 1.05%
99.89% ± 0.02% 99.66% ± 0.2%
80.04% ± 0.12% 77.26% ± 0.42%
99.99% ± 0.04% 99.99% ± 0.01% 89.24% ± 0.12% 87.26% ± 0.07%
99.56% ± 0.44% 99.88% ± 0.30% 49.58% ± 0.39% 46.45% ± 0.43%
57.81% ± 0.17%
99.10% ± 1.23% 99.57% ± 1.84% 63.08% ± 0.5%

LB

We emphasize that the generalization gap is not due to over-ﬁtting or over-training as commonly
observed in statistics. This phenomenon manifest themselves in the form of a testing accuracy curve
that, at a certain iterate peaks, and then decays due to the model learning idiosyncrasies of the
training data. This is not what we observe in our experiments; see Figure 2 for the training–testing
curve of the F2 and C1 networks, which are representative of the rest. As such, early-stopping
heuristics aimed at preventing models from over-ﬁtting would not help reduce the generalization
gap. The difference between the training and testing accuracies for the networks is due to the
speciﬁc choice of the network (e.g. AlexNet, VGGNet etc.) and is not the focus of this study.
Rather, our goal is to study the source of the testing performance disparity of the two regimes, SB
and LB, on a given network model.

2.2.1 PARAMETRIC PLOTS

s and x(cid:63)

We ﬁrst present parametric 1-D plots of the function as described in (Goodfellow et al., 2014b).
Let x(cid:63)
(cid:96) indicate the solutions obtained by running ADAM using small and large batch sizes
respectively. We plot the loss function, on both training and testing data sets, along a line-segment
(cid:96) + (1 − α)x(cid:63)
containing the two points. Speciﬁcally, for α ∈ [−1, 2], we plot the function f (αx(cid:63)
s)
and also superimpose the classiﬁcation accuracy at the intermediate points; see Figure 31. For this

1The code to reproduce the parametric plot on exemplary networks can be found in our GitHub repository:

https://github.com/keskarnitish/large-batch-training.

4

Published as a conference paper at ICLR 2017

(a) Network F2

(b) Network C1

Figure 2: Training and testing accuracy for SB and LB methods as a function of epochs.

experiment, we randomly chose a pair of SB and LB minimizers from the 5 trials used to generate
the data in Table 2. The plots show that the LB minima are strikingly sharper than the SB minima
in this one-dimensional manifold. The plots in Figure 3 only explore a linear slice of the function,
but in Figure 7 in Appendix D, we plot f (sin( απ
2 )x(cid:63)
s) to monitor the function along a
curved path between the two minimizers . There too, the relative sharpness of the minima is evident.

(cid:96) + cos( απ

2 )x(cid:63)

2.2.2 SHARPNESS OF MINIMA

So far, we have used the term sharp minimizer loosely, but we noted that this concept has received
attention in the literature (Hochreiter & Schmidhuber, 1997). Sharpness of a minimizer can be
characterized by the magnitude of the eigenvalues of ∇2f (x), but given the prohibitive cost of this
computation in deep learning applications, we employ a sensitivity measure that, although imperfect,
is computationally feasible, even for large networks. It is based on exploring a small neighborhood
of a solution and computing the largest value that the function f can attain in that neighborhood. We
use that value to measure the sensitivity of the training function at the given local minimizer. Now,
since the maximization process is not accurate, and to avoid being mislead by the case when a large
value of f is attained only in a tiny subspace of Rn, we perform the maximization both in the entire
space Rn as well as in random manifolds. For that purpose, we introduce an n × p matrix A, whose
columns are randomly generated. Here p determines the dimension of the manifold, which in our
experiments is chosen as p = 100.

Speciﬁcally, let C(cid:15) denote a box around the solution over which the maximization of f is performed,
and let A ∈ Rn×p be the matrix deﬁned above. In order to ensure invariance of sharpness to problem
dimension and sparsity, we deﬁne the constraint set C(cid:15) as:

C(cid:15) = {z ∈ Rp : −(cid:15)(|(A+x)i| + 1) ≤ zi ≤ (cid:15)(|(A+x)i| + 1) ∀i ∈ {1, 2, · · · , p}},
where A+ denotes the pseudo-inverse of A. Thus (cid:15) controls the size of the box. We can now deﬁne
our measure of sharpness (or sensitivity).
Metric 2.1. Given x ∈ Rn, (cid:15) > 0 and A ∈ Rn×p, we deﬁne the (C(cid:15), A)-sharpness of f at x as:

(3)

φx,f ((cid:15), A) :=

(maxy∈C(cid:15) f (x + Ay)) − f (x)
1 + f (x)

× 100.

(4)

Unless speciﬁed otherwise, we use this metric for sharpness for the rest of the paper; if A is not spec-
iﬁed, it is assumed to be the identity matrix, In. (We note in passing that, in the convex optimization
literature, the term sharp minimum has a different deﬁnition (Ferris, 1988), but that concept is not
useful for our purposes.)

In Tables 3 and 4, we present the values of the sharpness metric (4) for the minimizers of the various
problems. Table 3 explores the full-space (i.e., A = In) whereas Table 4 uses a randomly sampled
n × 100 dimensional matrix A. We report results with two values of (cid:15), (10−3, 5 · 10−4). In all
experiments, we solve the maximization problem in Equation (4) inexactly by applying 10 iterations
of L-BFGS-B (Byrd et al., 1995). This limit on the number of iterations was necessitated by the

5

Published as a conference paper at ICLR 2017

(a) F1

(b) F2

(c) C1

(d) C2

(e) C3

(f) C4

Figure 3: Parametric Plots – Linear (Left vertical axis corresponds to cross-entropy loss, f , and
right vertical axis corresponds to classiﬁcation accuracy; solid line indicates training data set and
dashed line indicated testing data set); α = 0 corresponds to the SB minimizer and α = 1 to the LB
minimizer.

large cost of evaluating the true objective f . Both tables show a 1–2 order-of-magnitude difference
between the values of our metric for the SB and LB regimes. These results reinforce the view that
the solutions obtained by a large-batch method deﬁnes points of larger sensitivity of the training
function. In Appedix E, we describe approaches to attempt to remedy this generalization problem
of LB methods. These approaches include data augmentation, conservative training and adversarial
training. Our preliminary ﬁndings show that these approaches help reduce the generalization gap
but still lead to relatively sharp minimizers and as such, do not completely remedy the problem.
Note that Metric 2.1 is closely related to the spectrum of ∇2f (x). Assuming (cid:15) to be small enough,
when A = In, the value (4) relates to the largest eigenvalue of ∇2f (x) and when A is randomly
sampled it approximates the Ritz value of ∇2f (x) projected onto the column-space of A.

6

Published as a conference paper at ICLR 2017

Table 3: Sharpness of Minima in Full Space; (cid:15) is deﬁned in (3).

(cid:15) = 10−3

(cid:15) = 5 · 10−4

SB
1.23 ± 0.83
1.39 ± 0.02
28.58 ± 3.13
8.68 ± 1.32
29.85 ± 5.98
12.83 ± 3.84

LB
205.14 ± 69.52
310.64 ± 38.46
707.23 ± 43.04
925.32 ± 38.29
258.75 ± 8.96
421.84 ± 36.97

SB
0.61 ± 0.27
0.90 ± 0.05
7.08 ± 0.88
2.07 ± 0.86
8.56 ± 0.99
4.07 ± 0.87

LB
42.90 ± 17.14
93.15 ± 6.81
227.31 ± 23.23
175.31 ± 18.28
105.11 ± 13.22
109.35 ± 16.57

Table 4: Sharpness of Minima in Random Subspaces of Dimension 100

(cid:15) = 10−3

(cid:15) = 5 · 10−4

SB
0.11 ± 0.00
0.29 ± 0.02
2.18 ± 0.23
0.95 ± 0.34
17.02 ± 2.20
6.05 ± 1.13

LB
9.22 ± 0.56
23.63 ± 0.54
137.25 ± 21.60
25.09 ± 2.61
236.03 ± 31.26
72.99 ± 10.96

SB
0.05 ± 0.00
0.05 ± 0.00
0.71 ± 0.15
0.31 ± 0.08
4.03 ± 1.45
1.89 ± 0.33

LB
9.17 ± 0.14
6.28 ± 0.19
29.50 ± 7.48
5.82 ± 0.52
86.96 ± 27.39
19.85 ± 4.12

F1
F2
C1
C2
C3
C4

F1
F2
C1
C2
C3
C4

We conclude this section by noting that the sharp minimizers identiﬁed in our experiments do not
resemble a cone, i.e., the function does not increase rapidly along all (or even most) directions. By
sampling the loss function in a neighborhood of LB solutions, we observe that it rises steeply only
along a small dimensional subspace (e.g. 5% of the whole space); on most other directions, the
function is relatively ﬂat.

3 SUCCESS OF SMALL-BATCH METHODS

It is often reported that when increasing the batch size for a problem, there exists a threshold after
which there is a deterioration in the quality of the model. This behavior can be observed for the F2
and C1 networks in Figure 4. In both of these experiments, there is a batch size (≈ 15000 for F2 and
≈ 500 for C1) after which there is a large drop in testing accuracy. Notice also that the upward drift
in value of the sharpness is considerably reduced around this threshold. Similar thresholds exist for
the other networks in Table 1.

Let us now consider the behavior of SB methods, which use noisy gradients in the step computation.
From the results reported in the previous section, it appears that noise in the gradient pushes the
iterates out of the basin of attraction of sharp minimizers and encourages movement towards a ﬂatter
minimizer where noise will not cause exit from that basin. When the batch size is greater than the
threshold mentioned above, the noise in the stochastic gradient is not sufﬁcient to cause ejection
from the initial basin leading to convergence to sharper a minimizer.

To explore that in more detail, consider the following experiment. We train the network for 100
epochs using ADAM with a batch size of 256, and retain the iterate after each epoch in memory.
Using these 100 iterates as starting points we train the network using a LB method for 100 epochs
and receive a 100 piggybacked (or warm-started) large-batch solutions. We plot in Figure 5 the
testing accuracy and sharpness of these large-batch solutions, along with the testing accuracy of the
small-batch iterates. Note that when warm-started with only a few initial epochs, the LB method
does not yield a generalization improvement. The concomitant sharpness of the iterates also stays
high. On the other hand, after certain number of epochs of warm-starting, the accuracy improves
and sharpness of the large-batch iterates drop. This happens, apparently, when the SB method has
ended its exploration phase and discovered a ﬂat minimizer; the LB method is then able to converge
towards it, leading to good testing accuracy.

It has been speculated that LB methods tend to be attracted to minimizers close to the starting point
x0, whereas SB methods move away and locate minimizers that are farther away. Our numerical

7

Published as a conference paper at ICLR 2017

(a) F2

(b) C1

Figure 4: Testing Accuracy and Sharpness v/s Batch Size. The X-axis corresponds to the batch size
used for training the network for 100 epochs, left Y-axis corresponds to the testing accuracy at the
ﬁnal iterate and right Y-axis corresponds to the sharpness of that iterate. We report sharpness for
two values of (cid:15): 10−3 and 5 · 10−4.

(a) F2

(b) C1

Figure 5: Warm-starting experiments. The upper ﬁgures report the testing accuracy of the SB
method (blue line) and the testing accuracy of the warm started (piggybacked) LB method (red line),
as a function of the number of epochs of the SB method. The lower ﬁgures plot the sharpness mea-
sure (4) for the solutions obtained by the piggybacked LB method v/s the number of warm-starting
epochs of the SB method.

8

Published as a conference paper at ICLR 2017

(a) F2

(b) C1

Figure 6: Sharpness v/s Cross Entropy Loss for SB and LB methods.

experiments support this view: we observed that the ratio of (cid:107)x(cid:63)
range of 3–10.

s − x0(cid:107)2 and (cid:107)x(cid:63)

(cid:96) − x0(cid:107)2 was in the

In order to further illustrate the qualitative difference between the solutions obtained by SB and LB
methods, we plot in Figure 6 our sharpness measure (4) against the loss function (cross entropy)
for one random trial of the F2 and C1 networks. For larger values of the loss function, i.e., near
the initial point, SB and LB method yield similar values of sharpness. As the loss function reduces,
the sharpness of the iterates corresponding to the LB method rapidly increases, whereas for the SB
method the sharpness stays relatively constant initially and then reduces, suggesting an exploration
phase followed by convergence to a ﬂat minimizer.

4 DISCUSSION AND CONCLUSION

In this paper, we present numerical experiments that support the view that convergence to sharp
minimizers gives rise to the poor generalization of large-batch methods for deep learning. To this
end, we provide one-dimensional parametric plots and perturbation (sharpness) measures for a vari-
ety of deep learning architectures. In Appendix E, we describe our attempts to remedy the problem,
including data augmentation, conservative training and robust optimization. Our preliminary inves-
tigation suggests that these strategies do not correct the problem; they improve the generalization of
large-batch methods but still lead to relatively sharp minima. Another prospective remedy includes
the use of dynamic sampling where the batch size is increased gradually as the iteration progresses
(Byrd et al., 2012; Friedlander & Schmidt, 2012). The potential viability of this approach is sug-
gested by our warm-starting experiments (see Figure 5) wherein high testing accuracy is achieved
using a large-batch method that is warm-start with a small-batch method.

Recently, a number of researchers have described interesting theoretical properties of the loss sur-
face of deep neural networks; see e.g. (Choromanska et al., 2015; Soudry & Carmon, 2016; Lee
et al., 2016). Their work shows that, under certain regularity assumptions, the loss function of deep
learning models is fraught with many local minimizers and that many of these minimizers corre-
spond to a similar loss function value. Our results are in alignment these observations since, in our
experiments, both sharp and ﬂat minimizers have very similar loss function values. We do not know,
however, if the theoretical models mentioned above provide information about the existence and
density of sharp minimizers of the loss surface.

Our results suggest some questions: (a) can one prove that large-batch (LB) methods typically con-
verge to sharp minimizers of deep learning training functions? (In this paper, we only provided some
numerical evidence.); (b) what is the relative density of the two kinds of minima?; (c) can one design
neural network architectures for various tasks that are suitable to the properties of LB methods?; (d)
can the networks be initialized in a way that enables LB methods to succeed?; (e) is it possible,
through algorithmic or regulatory means to steer LB methods away from sharp minimizers?

9

Published as a conference paper at ICLR 2017

REFERENCES

Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning. Book in preparation for MIT

Press, 2016. URL http://www.deeplearningbook.org.

Dimitris Bertsimas, Omid Nohadani, and Kwong Meng Teo. Robust optimization for unconstrained

simulation-based problems. Operations Research, 58(1):161–178, 2010.

L´eon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,

17(9):142, 1998.

L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine

learning. arXiv preprint arXiv:1606.04838, 2016.

Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for
bound constrained optimization. SIAM Journal on Scientiﬁc Computing, 16(5):1190–1208, 1995.

Richard H Byrd, Gillian M Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in opti-
mization methods for machine learning. Mathematical programming, 134(1):127–155, 2012.

Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing

gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.

Anna Choromanska, Mikael Henaff, Michael Mathieu, G´erard Ben Arous, and Yann LeCun. The

loss surfaces of multilayer networks. In AISTATS, 2015.

Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan Vaidynathan, Srinivas Srid-
haran, Dhiraj Kalamkar, Bharat Kaul, and Pradeep Dubey. Distributed deep learning using syn-
chronous stochastic gradient descent. arXiv preprint arXiv:1602.06709, 2016.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in
neural information processing systems, pp. 1223–1231, 2012.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

Michael Charles Ferris. Weak sharp minima and penalty functions in mathematical programming.

PhD thesis, University of Cambridge, 1988.

Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting.

SIAM Journal on Scientiﬁc Computing, 34(3):A1380–A1405, 2012.

John S Garofolo, Lori F Lamel, William M Fisher, Jonathan G Fiscus, David S Pallett, Nancy L
Dahlgren, and Victor Zue. Timit acoustic-phonetic continuous speech corpus. Linguistic data
consortium, Philadelphia, 33, 1993.

Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pp. 797–842, 2015.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014a.

Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network

optimization problems. arXiv preprint arXiv:1412.6544, 2014b.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
In 2013 IEEE international conference on acoustics, speech and signal

rent neural networks.
processing, pp. 6645–6649. IEEE, 2013.

M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient

descent. arXiv preprint arXiv:1509.01240, 2015.

Sepp Hochreiter and J¨urgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

10

Published as a conference paper at ICLR 2017

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by

reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

Nitish Shirish Keskar and Albert S. Berahas. adaQN: An Adaptive Quasi-Newton Algorithm for

Training RNNs, pp. 1–16. Springer International Publishing, Cham, 2016.

D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on

Learning Representations (ICLR 2015), 2015.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998a.

Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits,

1998b.

Yann A LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller. Efﬁcient backprop. In

Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012.

Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges

to minimizers. University of California, Berkeley, 1050:16, 2016.

Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efﬁcient mini-batch training for
stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 661–670. ACM, 2014.

David JC MacKay. A practical bayesian framework for backpropagation networks. Neural compu-

tation, 4(3):448–472, 1992.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

Hossein Mobahi. Training recurrent neural networks by diffusion. arXiv preprint arXiv:1601.04114,

2016.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel,
Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. The kaldi speech recognition
In IEEE 2011 workshop on automatic speech recognition and understanding, number
toolkit.
EPFL-CONF-192584. IEEE Signal Processing Society, 2011.

Jorma Rissanen. A universal prior for integers and estimation by minimum description length. The

Annals of statistics, pp. 416–431, 1983.

Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432, 2015.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556, 2014.

Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees

for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
In Proceedings of the 30th International Conference on Machine Learning

in deep learning.
(ICML 2013), pp. 1139–1147, 2013.

11

Published as a conference paper at ICLR 2017

Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In

Advances in Neural Information Processing Systems, pp. 685–693, 2015.

Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep

neural networks via stability training. arXiv preprint arXiv:1604.04326, 2016.

A DETAILS ABOUT DATA SETS

We summarize the data sets used in our experiments in Table 5. TIMIT is a speech recognition
data set which is pre-processed using Kaldi (Povey et al., 2011) and trained using a fully-connected
network. The rest of the data sets are used without any pre-processing.

Table 5: Data Sets

Data Set
MNIST
TIMIT
CIFAR-10
CIFAR-100

# Data Points
Test
Train
10000
60000
310621
721329
10000
50000
10000
50000

# Features
28 × 28
360
32 × 32
32 × 32

# Classes Reference

10
1973
10
100

(LeCun et al., 1998a;b)
(Garofolo et al., 1993)
(Krizhevsky & Hinton, 2009)
(Krizhevsky & Hinton, 2009)

B ARCHITECTURE OF NETWORKS

B.1 NETWORK F1

For this network, we use a 784-dimensional input layer followed by 5 batch-normalized (Ioffe &
Szegedy, 2015) layers of 512 neurons each with ReLU activations. The output layer consists of 10
neurons with the softmax activation.

B.2 NETWORK F2

The network architecture for F2 is similar to F1. We use a 360-dimensional input layer followed by
7 batch-normalized layers of 512 neurons with ReLU activation. The output layer consists of 1973
neurons with the softmax activation.

B.3 NETWORKS C1 AND C3

The C1 network is a modiﬁed version of the popular AlexNet conﬁguration (Krizhevsky et al.,
2012). For simplicity, denote a stack of n convolution layers of a ﬁlters and a Kernel size of b × c
with stride length of d as n×[a, b, c, d]. The C1 conﬁguration uses 2 sets of [64, 5, 5, 2]–MaxPool(3)
followed by 2 dense layers of sizes (384, 192) and ﬁnally, an output layer of size 10. We use batch-
normalization for all layers and ReLU activations. We also use Dropout (Srivastava et al., 2014) of
0.5 retention probability for the two dense layers. The conﬁguration C3 is identical to C1 except it
uses 100 softmax outputs instead of 10.

B.4 NETWORKS C2 AND C4

The C2 network is a modiﬁed version of the popular VGG conﬁguration (Simonyan & Zisserman,
2014). The C3 network uses the conﬁguration: 2×[64, 3, 3, 1], 2×[128, 3, 3, 1], 3×[256, 3, 3, 1], 3×
[512, 3, 3, 1], 3 × [512, 3, 3, 1] which a MaxPool(2) after each stack. This stack is followed by a 512-
dimensional dense layer and ﬁnally, a 10-dimensional output layer. The activation and properties of
each layer is as in B.3. As is the case with C3 and C1, the conﬁguration C4 is identical to C2 except
that it uses 100 softmax outputs instead of 10.

12

Published as a conference paper at ICLR 2017

C PERFORMANCE MODEL

As mentioned in Section 1, a training algorithm that operates in the large-batch regime without
suffering from a generalization gap would have the ability to scale to much larger number of nodes
than is currently possible. Such and algorithm might also improve training time through faster
convergence. We present an idealized performance model that demonstrates our goal.

For LB method to be competitive with SB method, the LB method must (i) converge to minimizers
that generalize well, and (ii) do it in a reasonably number of iterations, which we analyze here. Let Is
and I(cid:96) be number of iterations required by SB and LB methods to reach the point of comparable test
accuracy, respectively. Let Bs and B(cid:96) be corresponding batch sizes and P be number of processors
being used for training. Assume that P < B(cid:96), and let fs(P ) be the parallel efﬁciency of the SB
method. For simplicity, we assume that f(cid:96)(P ), the parallel efﬁciency of the LB method, is 1.0. In
other words, we assume that the LB method is perfectly scalable due to use of a large batch size.

For LB to be faster than SB, we must have

In other words, the ratio of iterations of LB to the iterations of SB should be

I(cid:96)

B(cid:96)
P

< Is

Bs
P fs(P )

.

I(cid:96)
Is

<

Bs
fs(P )B(cid:96)

.

For example, if fs(P ) = 0.2 and Bs/B(cid:96) = 0.1, the LB method must converge in at most half as
many iterations as the SB method to see performance beneﬁts. We refer the reader to (Das et al.,
2016) for a more detailed model and a commentary on the effect of batch-size on the performance.

D CURVILINEAR PARAMETRIC PLOTS

The parametric plots for the curvilinear path from x(cid:63)
found in Figure 7.

s to x(cid:63)

(cid:96) , i.e., f (sin( απ

2 )x(cid:63)

(cid:96) + cos( απ

2 )x(cid:63)

s) can be

E ATTEMPTS TO IMPROVE LB METHODS

In this section, we discuss a few strategies that aim to remedy the problem of poor generalization
for large-batch methods. As in Section 2, we use 10% as the percentage batch-size for large-batch
experiments and 256 for small-batch methods. For all experiments, we use ADAM as the optimizer
irrespective of batch-size.

E.1 DATA AUGMENTATION

Given that large-batch methods appear to be attracted to sharp minimizers, one can ask whether it is
possible to modify the geometry of the loss function so that it is more benign to large-batch meth-
ods. The loss function depends both on the geometry of the objective function and to the size and
properties of the training set. One approach we consider is data augmentation; see e.g. (Krizhevsky
et al., 2012; Simonyan & Zisserman, 2014). The application of this technique is domain speciﬁc but
generally involves augmenting the data set through controlled modiﬁcations on the training data. For
instance, in the case of image recognition, the training set can be augmented through translations,
rotations, shearing and ﬂipping of the training data. This technique leads to regularization of the
network and has been employed for improving testing accuracy on several data sets.

In our experiments, we train the 4 image-based (convolutional) networks using aggressive data aug-
mentation and present the results in Table 6. For the augmentation, we use horizontal reﬂections,
random rotations up to 10◦ and random translation of up to 0.2 times the size of the image. It is
evident from the table that, while the LB method achieves accuracy comparable to the SB method
(also with training data augmented), the sharpness of the minima still exists, suggesting sensitivity
to images contained in neither training or testing set. In this section, we exclude parametric plots and
sharpness values for the SB method owing to space constraints and the similarity to those presented
in Section 2.2.

13

Published as a conference paper at ICLR 2017

(a) F1

(b) F2

(c) C1

(d) C2

(e) C3

(f) C4

Figure 7: Parametric Plots – Curvilinear (Left vertical axis corresponds to cross-entropy loss, f ,
and right vertical axis corresponds to classiﬁcation accuracy; solid line indicates training data set
and dashed line indicated testing data set); α = 0 corresponds to the SB minimizer while α = 1
corresponds to the LB minimizer

Table 6: Effect of Data Augmentation

Testing Accuracy

Sharpness (LB method)

Baseline (SB)

Augmented LB
83.63% ± 0.14% 82.50% ± 0.67% 231.77 ± 30.50
89.82% ± 0.12% 90.26% ± 1.15% 468.65 ± 47.86
54.55% ± 0.44% 53.03% ± 0.33% 103.68 ± 11.93
65.88 ± 0.13% 271.06 ± 29.69
63.05% ± 0.5%

(cid:15) = 10−3

(cid:15) = 5 · 10−4
45.89 ± 3.83
105.22 ± 19.57
37.67 ± 3.46
45.31 ± 5.93

C1
C2
C3
C4

14

Published as a conference paper at ICLR 2017

Table 7: Effect of Conservative Training

Testing Accuracy

Sharpness (LB method)

Baseline (SB)

(cid:15) = 10−3
Conservative LB
232.25 ± 63.81
98.03% ± 0.07% 98.12% ± 0.01%
928.40 ± 51.63
64.02% ± 0.2%
61.94% ± 1.10%
80.04% ± 0.12% 78.41% ± 0.22%
520.34 ± 34.91
89.24% ± 0.05% 88.495% ± 0.63% 632.01 ± 208.01
337.92 ± 33.09
49.58% ± 0.39% 45.98% ± 0.54%
354.94 ± 20.23
63.08% ± 0.10%

62.51 ± 0.67

(cid:15) = 5 · 10−4
46.02 ± 12.58
190.77 ± 25.33
171.19 ± 15.13
108.88 ± 47.36
110.69 ± 3.88
68.76 ± 16.29

F1
F2
C1
C2
C3
C4

E.2 CONSERVATIVE TRAINING

In (Li et al., 2014), the authors argue that the convergence rate of SGD for the large-batch setting
can be improved by obtaining iterates through the following proximal sub-problem.

xk+1 = arg min

x

1
|Bk|

(cid:88)

i∈Bk

fi(x) +

(cid:107)x − xk(cid:107)2
2

λ
2

(5)

The motivation for this strategy is, in the context of large-batch methods, to better utilize a batch
before moving onto the next one. The minimization problem is solved inexactly using 3–5 itera-
tions of gradient descent, co-ordinate descent or L-BFGS. (Li et al., 2014) report that this not only
improves the convergence rate of SGD but also leads to improved empirical performance on con-
vex machine learning problems. The underlying idea of utilizing a batch is not speciﬁc to convex
problems and we can apply the same framework for deep learning, however, without theoretical
guarantees. Indeed, similar algorithms were proposed in (Zhang et al., 2015) and (Mobahi, 2016)
for Deep Learning. The former placed emphasis on parallelization of small-batch SGD and asyn-
chrony while the latter on a diffusion-continuation mechanism for training. The results using the
conservative training approach are presented in Figure 7. In all experiments, we solve the problem
(5) using 3 iterations of ADAM and set the regularization parameter λ to be 10−3. Again, there is
a statistically signiﬁcant improvement in the testing accuracy of the large-batch method but it does
not solve the problem of sensitivity.

E.3 ROBUST TRAINING

A natural way of avoiding sharp minima is through robust optimization techniques. These methods
attempt to optimize a worst-case cost as opposed to the nominal (or true) cost. Mathematically,
given an (cid:15) > 0, these techniques solve the problem

min
x

φ(x) := max

f (x + ∆x)

(cid:107)∆x(cid:107)≤(cid:15)

(6)

Geometrically, classical (nominal) optimization attempts to locate the lowest point of a valley, while
robust optimization attempts to lower an (cid:15)–disc down the loss surface. We refer an interested reader
to (Bertsimas et al., 2010), and the references therein, for a review of non-convex robust optimiza-
tion. A direct application of this technique is, however, not feasible in our context since each itera-
tion is prohibitively expensive because it involves solving a large-scale second-order conic program
(SOCP).

15

Published as a conference paper at ICLR 2017

Figure 8: Illustration of Robust Optimization

In the context of Deep Learning, there are two inter-dependent forms of robustness: robustness to
the data and robustness to the solution. The former exploits the fact that the function f is inherently
a statistical model, while the latter treats f as a black-box function. In (Shaham et al., 2015), the
authors prove the equivalence between robustness of the solution (with respect to the data) and
adversarial training (Goodfellow et al., 2014a).

Given the partial success of the data augmentation strategy, it is natural to question the efﬁcacy
of adversarial training. As described in (Goodfellow et al., 2014a), adversarial training also aims
to artiﬁcially increase the training set but, unlike randomized data augmentation, uses the model’s
sensitivity to construct new examples. Despite its intuitive appeal, in our experiments, we found
that this strategy did not improve generalization. Similarly, we observed no generalization beneﬁt
from the stability training proposed by (Zheng et al., 2016). In both cases, the testing accuracy,
sharpness values and the parametric plots were similar to the unmodiﬁed (baseline) case discussed
in Section 2. It remains to be seen whether adversarial training (or any other form of robust training)
can increase the viability of large-batch training.

16

7
1
0
2
 
b
e
F
 
9
 
 
]

G
L
.
s
c
[
 
 
2
v
6
3
8
4
0
.
9
0
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

ON LARGE-BATCH TRAINING FOR DEEP LEARNING:
GENERALIZATION GAP AND SHARP MINIMA

Nitish Shirish Keskar∗
Northwestern University
Evanston, IL 60208
keskar.nitish@u.northwestern.edu

Dheevatsa Mudigere
Intel Corporation
Bangalore, India
dheevatsa.mudigere@intel.com

Jorge Nocedal
Northwestern University
Evanston, IL 60208
j-nocedal@northwestern.edu

Mikhail Smelyanskiy
Intel Corporation
Santa Clara, CA 95054
mikhail.smelyanskiy@intel.com

Ping Tak Peter Tang
Intel Corporation
Santa Clara, CA 95054
peter.tang@intel.com

ABSTRACT

The stochastic gradient descent (SGD) method and its variants are algorithms of
choice for many Deep Learning tasks. These methods operate in a small-batch
regime wherein a fraction of the training data, say 32–512 data points, is sampled
to compute an approximation to the gradient. It has been observed in practice that
when using a larger batch there is a degradation in the quality of the model, as
measured by its ability to generalize. We investigate the cause for this generaliza-
tion drop in the large-batch regime and present numerical evidence that supports
the view that large-batch methods tend to converge to sharp minimizers of the
training and testing functions—and as is well known, sharp minima lead to poorer
generalization. In contrast, small-batch methods consistently converge to ﬂat min-
imizers, and our experiments support a commonly held view that this is due to the
inherent noise in the gradient estimation. We discuss several strategies to attempt
to help large-batch methods eliminate this generalization gap.

1

INTRODUCTION

Deep Learning has emerged as one of the cornerstones of large-scale machine learning. Deep Learn-
ing models are used for achieving state-of-the-art results on a wide variety of tasks including com-
puter vision, natural language processing and reinforcement learning; see (Bengio et al., 2016) and
the references therein. The problem of training these networks is one of non-convex optimization.
Mathematically, this can be represented as:

min
x∈Rn

f (x) :=

1
M

M
(cid:88)

i=1

fi(x),

(1)

where fi is a loss function for data point i ∈ {1, 2, · · · , M } which captures the deviation of the
model prediction from the data, and x is the vector of weights being optimized. The process of
optimizing this function is also called training of the network. Stochastic Gradient Descent (SGD)
(Bottou, 1998; Sutskever et al., 2013) and its variants are often used for training deep networks.

∗Work was performed when author was an intern at Intel Corporation

1

Published as a conference paper at ICLR 2017

These methods minimize the objective function f by iteratively taking steps of the form:

xk+1 = xk − αk

(cid:32)

1
|Bk|

(cid:88)

i∈Bk

(cid:33)

∇fi(xk)

,

(2)

where Bk ⊂ {1, 2, · · · , M } is the batch sampled from the data set and αk is the step size at iteration
k. These methods can be interpreted as gradient descent using noisy gradients, which and are often
referred to as mini-batch gradients with batch size |Bk|. SGD and its variants are employed in a
small-batch regime, where |Bk| (cid:28) M and typically |Bk| ∈ {32, 64, · · · , 512}. These conﬁgura-
tions have been successfully used in practice for a large number of applications; see e.g. (Simonyan
& Zisserman, 2014; Graves et al., 2013; Mnih et al., 2013). Many theoretical properties of these
methods are known. These include guarantees of: (a) convergence to minimizers of strongly-convex
functions and to stationary points for non-convex functions (Bottou et al., 2016), (b) saddle-point
avoidance (Ge et al., 2015; Lee et al., 2016), and (c) robustness to input data (Hardt et al., 2015).

Stochastic gradient methods have, however, a major drawback: owing to the sequential nature of the
iteration and small batch sizes, there is limited avenue for parallelization. While some efforts have
been made to parallelize SGD for Deep Learning (Dean et al., 2012; Das et al., 2016; Zhang et al.,
2015), the speed-ups and scalability obtained are often limited by the small batch sizes. One natu-
ral avenue for improving parallelism is to increase the batch size |Bk|. This increases the amount
of computation per iteration, which can be effectively distributed. However, practitioners have ob-
served that this leads to a loss in generalization performance; see e.g. (LeCun et al., 2012). In other
words, the performance of the model on testing data sets is often worse when trained with large-
batch methods as compared to small-batch methods. In our experiments, we have found the drop in
generalization (also called generalization gap) to be as high as 5% even for smaller networks.

In this paper, we present numerical results that shed light into this drawback of large-batch methods.
We observe that the generalization gap is correlated with a marked sharpness of the minimizers
obtained by large-batch methods. This motivates efforts at remedying the generalization problem,
as a training algorithm that employs large batches without sacriﬁcing generalization performance
would have the ability to scale to a much larger number of nodes than is possible today. This could
potentially reduce the training time by orders-of-magnitude; we present an idealized performance
model in the Appendix C to support this claim.

The paper is organized as follows. In the remainder of this section, we deﬁne the notation used in
this paper, and in Section 2 we present our main ﬁndings and their supporting numerical evidence.
In Section 3 we explore the performance of small-batch methods, and in Section 4 we brieﬂy discuss
the relationship between our results and recent theoretical work. We conclude with open questions
concerning the generalization gap, sharp minima, and possible modiﬁcations to make large-batch
training viable. In Appendix E, we present some attempts to overcome the problems of large-batch
training.

1.1 NOTATION

We use the notation fi to denote the composition of loss function and a prediction function corre-
sponding to the ith data point. The vector of weights is denoted by x and is subscripted by k to
denote an iteration. We use the term small-batch (SB) method to denote SGD, or one of its variants
like ADAM (Kingma & Ba, 2015) and ADAGRAD (Duchi et al., 2011), with the proviso that the
gradient approximation is based on a small mini-batch. In our setup, the batch Bk is randomly sam-
pled and its size is kept ﬁxed for every iteration. We use the term large-batch (LB) method to denote
any training algorithm that uses a large mini-batch. In our experiments, ADAM is used to explore
the behavior of both a small or a large batch method.

2 DRAWBACKS OF LARGE-BATCH METHODS

2.1 OUR MAIN OBSERVATION

As mentioned in Section 1, practitioners have observed a generalization gap when using large-batch
methods for training deep learning models. Interestingly, this is despite the fact that large-batch
methods usually yield a similar value of the training function as small-batch methods. One may put

2

Published as a conference paper at ICLR 2017

forth the following as possible causes for this phenomenon: (i) LB methods over-ﬁt the model; (ii)
LB methods are attracted to saddle points; (iii) LB methods lack the explorative properties of SB
methods and tend to zoom-in on the minimizer closest to the initial point; (iv) SB and LB methods
converge to qualitatively different minimizers with differing generalization properties. The data
presented in this paper supports the last two conjectures.

The main observation of this paper is as follows:

The lack of generalization ability is due to the fact that large-batch methods tend to converge
to sharp minimizers of the training function. These minimizers are characterized by a signif-
icant number of large positive eigenvalues in ∇2f (x), and tend to generalize less well.
In
contrast, small-batch methods converge to ﬂat minimizers characterized by having numerous
small eigenvalues of ∇2f (x). We have observed that the loss function landscape of deep neural
networks is such that large-batch methods are attracted to regions with sharp minimizers and
that, unlike small-batch methods, are unable to escape basins of attraction of these minimizers.

The concept of sharp and ﬂat minimizers have been discussed in the statistics and machine learning
literature. (Hochreiter & Schmidhuber, 1997) (informally) deﬁne a ﬂat minimizer ¯x as one for which
the function varies slowly in a relatively large neighborhood of ¯x. In contrast, a sharp minimizer ˆx
is such that the function increases rapidly in a small neighborhood of ˆx. A ﬂat minimum can be de-
scribed with low precision, whereas a sharp minimum requires high precision. The large sensitivity
of the training function at a sharp minimizer negatively impacts the ability of the trained model to
generalize on new data; see Figure 1 for a hypothetical illustration. This can be explained through
the lens of the minimum description length (MDL) theory, which states that statistical models that
require fewer bits to describe (i.e., are of low complexity) generalize better (Rissanen, 1983). Since
ﬂat minimizers can be speciﬁed with lower precision than to sharp minimizers, they tend to have bet-
ter generalization performance. Alternative explanations are proffered through the Bayesian view
of learning (MacKay, 1992), and through the lens of free Gibbs energy; see e.g. Chaudhari et al.
(2016).

Training Function

Testing Function

f (x)

Flat Minimum

Sharp Minimum

Figure 1: A Conceptual Sketch of Flat and Sharp Minima. The Y-axis indicates value of the loss
function and the X-axis the variables (parameters)

2.2 NUMERICAL EXPERIMENTS

In this section, we present numerical results to support the observations made above. To this end,
we make use of the visualization technique employed by (Goodfellow et al., 2014b) and a proposed
heuristic metric of sharpness (Equation (4)). We consider 6 multi-class classiﬁcation network con-
ﬁgurations for our experiments; they are described in Table 1. The details about the data sets and
network conﬁgurations are presented in Appendices A and B respectively. As is common for such
problems, we use the mean cross entropy loss as the objective function f .

The networks were chosen to exemplify popular conﬁgurations used in practice like AlexNet
(Krizhevsky et al., 2012) and VGGNet (Simonyan & Zisserman, 2014). Results on other networks

3

Published as a conference paper at ICLR 2017

Table 1: Network Conﬁgurations

Name Network Type
F1
F2
C1
C2
C3
C4

Fully Connected
Fully Connected
(Shallow) Convolutional
(Deep) Convolutional
(Shallow) Convolutional
(Deep) Convolutional

Architecture Data set
Section B.1 MNIST (LeCun et al., 1998a)
TIMIT (Garofolo et al., 1993)
Section B.2
CIFAR-10 (Krizhevsky & Hinton, 2009)
Section B.3
CIFAR-10
Section B.4
CIFAR-100 (Krizhevsky & Hinton, 2009)
Section B.3
CIFAR-100
Section B.4

and using other initialization strategies, activation functions, and data sets showed similar behavior.
Since the goal of our work is not to achieve state-of-the-art accuracy or time-to-solution on these
tasks but rather to characterize the nature of the minima for LB and SB methods, we only describe
the ﬁnal testing accuracy in the main paper and ignore convergence trends.

For all experiments, we used 10% of the training data as batch size for the large-batch experiments
and 256 data points for small-batch experiments. We used the ADAM optimizer for both regimes.
Experiments with other optimizers for the large-batch experiments, including ADAGRAD (Duchi
et al., 2011), SGD (Sutskever et al., 2013) and adaQN (Keskar & Berahas, 2016), led to similar
results. All experiments were conducted 5 times from different (uniformly distributed random)
starting points and we report both mean and standard-deviation of measured quantities. The baseline
performance for our setup is presented Table 2. From this, we can observe that on all networks, both
approaches led to high training accuracy but there is a signiﬁcant difference in the generalization
performance. The networks were trained, without any budget or limits, until the loss function ceased
to improve.

Table 2: Performance of small-batch (SB) and large-batch (LB) variants of ADAM on the 6 networks
listed in Table 1

Name
F1
F2
C1
C2
C3
C4

Training Accuracy

Testing Accuracy

SB

LB

SB
99.66% ± 0.05% 99.92% ± 0.01% 98.03% ± 0.07% 97.81% ± 0.07%
99.99% ± 0.03% 98.35% ± 2.08% 64.02% ± 0.2%
59.45% ± 1.05%
99.89% ± 0.02% 99.66% ± 0.2%
80.04% ± 0.12% 77.26% ± 0.42%
99.99% ± 0.04% 99.99% ± 0.01% 89.24% ± 0.12% 87.26% ± 0.07%
99.56% ± 0.44% 99.88% ± 0.30% 49.58% ± 0.39% 46.45% ± 0.43%
57.81% ± 0.17%
99.10% ± 1.23% 99.57% ± 1.84% 63.08% ± 0.5%

LB

We emphasize that the generalization gap is not due to over-ﬁtting or over-training as commonly
observed in statistics. This phenomenon manifest themselves in the form of a testing accuracy curve
that, at a certain iterate peaks, and then decays due to the model learning idiosyncrasies of the
training data. This is not what we observe in our experiments; see Figure 2 for the training–testing
curve of the F2 and C1 networks, which are representative of the rest. As such, early-stopping
heuristics aimed at preventing models from over-ﬁtting would not help reduce the generalization
gap. The difference between the training and testing accuracies for the networks is due to the
speciﬁc choice of the network (e.g. AlexNet, VGGNet etc.) and is not the focus of this study.
Rather, our goal is to study the source of the testing performance disparity of the two regimes, SB
and LB, on a given network model.

2.2.1 PARAMETRIC PLOTS

s and x(cid:63)

We ﬁrst present parametric 1-D plots of the function as described in (Goodfellow et al., 2014b).
Let x(cid:63)
(cid:96) indicate the solutions obtained by running ADAM using small and large batch sizes
respectively. We plot the loss function, on both training and testing data sets, along a line-segment
(cid:96) + (1 − α)x(cid:63)
containing the two points. Speciﬁcally, for α ∈ [−1, 2], we plot the function f (αx(cid:63)
s)
and also superimpose the classiﬁcation accuracy at the intermediate points; see Figure 31. For this

1The code to reproduce the parametric plot on exemplary networks can be found in our GitHub repository:

https://github.com/keskarnitish/large-batch-training.

4

Published as a conference paper at ICLR 2017

(a) Network F2

(b) Network C1

Figure 2: Training and testing accuracy for SB and LB methods as a function of epochs.

experiment, we randomly chose a pair of SB and LB minimizers from the 5 trials used to generate
the data in Table 2. The plots show that the LB minima are strikingly sharper than the SB minima
in this one-dimensional manifold. The plots in Figure 3 only explore a linear slice of the function,
but in Figure 7 in Appendix D, we plot f (sin( απ
2 )x(cid:63)
s) to monitor the function along a
curved path between the two minimizers . There too, the relative sharpness of the minima is evident.

(cid:96) + cos( απ

2 )x(cid:63)

2.2.2 SHARPNESS OF MINIMA

So far, we have used the term sharp minimizer loosely, but we noted that this concept has received
attention in the literature (Hochreiter & Schmidhuber, 1997). Sharpness of a minimizer can be
characterized by the magnitude of the eigenvalues of ∇2f (x), but given the prohibitive cost of this
computation in deep learning applications, we employ a sensitivity measure that, although imperfect,
is computationally feasible, even for large networks. It is based on exploring a small neighborhood
of a solution and computing the largest value that the function f can attain in that neighborhood. We
use that value to measure the sensitivity of the training function at the given local minimizer. Now,
since the maximization process is not accurate, and to avoid being mislead by the case when a large
value of f is attained only in a tiny subspace of Rn, we perform the maximization both in the entire
space Rn as well as in random manifolds. For that purpose, we introduce an n × p matrix A, whose
columns are randomly generated. Here p determines the dimension of the manifold, which in our
experiments is chosen as p = 100.

Speciﬁcally, let C(cid:15) denote a box around the solution over which the maximization of f is performed,
and let A ∈ Rn×p be the matrix deﬁned above. In order to ensure invariance of sharpness to problem
dimension and sparsity, we deﬁne the constraint set C(cid:15) as:

C(cid:15) = {z ∈ Rp : −(cid:15)(|(A+x)i| + 1) ≤ zi ≤ (cid:15)(|(A+x)i| + 1) ∀i ∈ {1, 2, · · · , p}},
where A+ denotes the pseudo-inverse of A. Thus (cid:15) controls the size of the box. We can now deﬁne
our measure of sharpness (or sensitivity).
Metric 2.1. Given x ∈ Rn, (cid:15) > 0 and A ∈ Rn×p, we deﬁne the (C(cid:15), A)-sharpness of f at x as:

(3)

φx,f ((cid:15), A) :=

(maxy∈C(cid:15) f (x + Ay)) − f (x)
1 + f (x)

× 100.

(4)

Unless speciﬁed otherwise, we use this metric for sharpness for the rest of the paper; if A is not spec-
iﬁed, it is assumed to be the identity matrix, In. (We note in passing that, in the convex optimization
literature, the term sharp minimum has a different deﬁnition (Ferris, 1988), but that concept is not
useful for our purposes.)

In Tables 3 and 4, we present the values of the sharpness metric (4) for the minimizers of the various
problems. Table 3 explores the full-space (i.e., A = In) whereas Table 4 uses a randomly sampled
n × 100 dimensional matrix A. We report results with two values of (cid:15), (10−3, 5 · 10−4). In all
experiments, we solve the maximization problem in Equation (4) inexactly by applying 10 iterations
of L-BFGS-B (Byrd et al., 1995). This limit on the number of iterations was necessitated by the

5

Published as a conference paper at ICLR 2017

(a) F1

(b) F2

(c) C1

(d) C2

(e) C3

(f) C4

Figure 3: Parametric Plots – Linear (Left vertical axis corresponds to cross-entropy loss, f , and
right vertical axis corresponds to classiﬁcation accuracy; solid line indicates training data set and
dashed line indicated testing data set); α = 0 corresponds to the SB minimizer and α = 1 to the LB
minimizer.

large cost of evaluating the true objective f . Both tables show a 1–2 order-of-magnitude difference
between the values of our metric for the SB and LB regimes. These results reinforce the view that
the solutions obtained by a large-batch method deﬁnes points of larger sensitivity of the training
function. In Appedix E, we describe approaches to attempt to remedy this generalization problem
of LB methods. These approaches include data augmentation, conservative training and adversarial
training. Our preliminary ﬁndings show that these approaches help reduce the generalization gap
but still lead to relatively sharp minimizers and as such, do not completely remedy the problem.
Note that Metric 2.1 is closely related to the spectrum of ∇2f (x). Assuming (cid:15) to be small enough,
when A = In, the value (4) relates to the largest eigenvalue of ∇2f (x) and when A is randomly
sampled it approximates the Ritz value of ∇2f (x) projected onto the column-space of A.

6

Published as a conference paper at ICLR 2017

Table 3: Sharpness of Minima in Full Space; (cid:15) is deﬁned in (3).

(cid:15) = 10−3

(cid:15) = 5 · 10−4

SB
1.23 ± 0.83
1.39 ± 0.02
28.58 ± 3.13
8.68 ± 1.32
29.85 ± 5.98
12.83 ± 3.84

LB
205.14 ± 69.52
310.64 ± 38.46
707.23 ± 43.04
925.32 ± 38.29
258.75 ± 8.96
421.84 ± 36.97

SB
0.61 ± 0.27
0.90 ± 0.05
7.08 ± 0.88
2.07 ± 0.86
8.56 ± 0.99
4.07 ± 0.87

LB
42.90 ± 17.14
93.15 ± 6.81
227.31 ± 23.23
175.31 ± 18.28
105.11 ± 13.22
109.35 ± 16.57

Table 4: Sharpness of Minima in Random Subspaces of Dimension 100

(cid:15) = 10−3

(cid:15) = 5 · 10−4

SB
0.11 ± 0.00
0.29 ± 0.02
2.18 ± 0.23
0.95 ± 0.34
17.02 ± 2.20
6.05 ± 1.13

LB
9.22 ± 0.56
23.63 ± 0.54
137.25 ± 21.60
25.09 ± 2.61
236.03 ± 31.26
72.99 ± 10.96

SB
0.05 ± 0.00
0.05 ± 0.00
0.71 ± 0.15
0.31 ± 0.08
4.03 ± 1.45
1.89 ± 0.33

LB
9.17 ± 0.14
6.28 ± 0.19
29.50 ± 7.48
5.82 ± 0.52
86.96 ± 27.39
19.85 ± 4.12

F1
F2
C1
C2
C3
C4

F1
F2
C1
C2
C3
C4

We conclude this section by noting that the sharp minimizers identiﬁed in our experiments do not
resemble a cone, i.e., the function does not increase rapidly along all (or even most) directions. By
sampling the loss function in a neighborhood of LB solutions, we observe that it rises steeply only
along a small dimensional subspace (e.g. 5% of the whole space); on most other directions, the
function is relatively ﬂat.

3 SUCCESS OF SMALL-BATCH METHODS

It is often reported that when increasing the batch size for a problem, there exists a threshold after
which there is a deterioration in the quality of the model. This behavior can be observed for the F2
and C1 networks in Figure 4. In both of these experiments, there is a batch size (≈ 15000 for F2 and
≈ 500 for C1) after which there is a large drop in testing accuracy. Notice also that the upward drift
in value of the sharpness is considerably reduced around this threshold. Similar thresholds exist for
the other networks in Table 1.

Let us now consider the behavior of SB methods, which use noisy gradients in the step computation.
From the results reported in the previous section, it appears that noise in the gradient pushes the
iterates out of the basin of attraction of sharp minimizers and encourages movement towards a ﬂatter
minimizer where noise will not cause exit from that basin. When the batch size is greater than the
threshold mentioned above, the noise in the stochastic gradient is not sufﬁcient to cause ejection
from the initial basin leading to convergence to sharper a minimizer.

To explore that in more detail, consider the following experiment. We train the network for 100
epochs using ADAM with a batch size of 256, and retain the iterate after each epoch in memory.
Using these 100 iterates as starting points we train the network using a LB method for 100 epochs
and receive a 100 piggybacked (or warm-started) large-batch solutions. We plot in Figure 5 the
testing accuracy and sharpness of these large-batch solutions, along with the testing accuracy of the
small-batch iterates. Note that when warm-started with only a few initial epochs, the LB method
does not yield a generalization improvement. The concomitant sharpness of the iterates also stays
high. On the other hand, after certain number of epochs of warm-starting, the accuracy improves
and sharpness of the large-batch iterates drop. This happens, apparently, when the SB method has
ended its exploration phase and discovered a ﬂat minimizer; the LB method is then able to converge
towards it, leading to good testing accuracy.

It has been speculated that LB methods tend to be attracted to minimizers close to the starting point
x0, whereas SB methods move away and locate minimizers that are farther away. Our numerical

7

Published as a conference paper at ICLR 2017

(a) F2

(b) C1

Figure 4: Testing Accuracy and Sharpness v/s Batch Size. The X-axis corresponds to the batch size
used for training the network for 100 epochs, left Y-axis corresponds to the testing accuracy at the
ﬁnal iterate and right Y-axis corresponds to the sharpness of that iterate. We report sharpness for
two values of (cid:15): 10−3 and 5 · 10−4.

(a) F2

(b) C1

Figure 5: Warm-starting experiments. The upper ﬁgures report the testing accuracy of the SB
method (blue line) and the testing accuracy of the warm started (piggybacked) LB method (red line),
as a function of the number of epochs of the SB method. The lower ﬁgures plot the sharpness mea-
sure (4) for the solutions obtained by the piggybacked LB method v/s the number of warm-starting
epochs of the SB method.

8

Published as a conference paper at ICLR 2017

(a) F2

(b) C1

Figure 6: Sharpness v/s Cross Entropy Loss for SB and LB methods.

experiments support this view: we observed that the ratio of (cid:107)x(cid:63)
range of 3–10.

s − x0(cid:107)2 and (cid:107)x(cid:63)

(cid:96) − x0(cid:107)2 was in the

In order to further illustrate the qualitative difference between the solutions obtained by SB and LB
methods, we plot in Figure 6 our sharpness measure (4) against the loss function (cross entropy)
for one random trial of the F2 and C1 networks. For larger values of the loss function, i.e., near
the initial point, SB and LB method yield similar values of sharpness. As the loss function reduces,
the sharpness of the iterates corresponding to the LB method rapidly increases, whereas for the SB
method the sharpness stays relatively constant initially and then reduces, suggesting an exploration
phase followed by convergence to a ﬂat minimizer.

4 DISCUSSION AND CONCLUSION

In this paper, we present numerical experiments that support the view that convergence to sharp
minimizers gives rise to the poor generalization of large-batch methods for deep learning. To this
end, we provide one-dimensional parametric plots and perturbation (sharpness) measures for a vari-
ety of deep learning architectures. In Appendix E, we describe our attempts to remedy the problem,
including data augmentation, conservative training and robust optimization. Our preliminary inves-
tigation suggests that these strategies do not correct the problem; they improve the generalization of
large-batch methods but still lead to relatively sharp minima. Another prospective remedy includes
the use of dynamic sampling where the batch size is increased gradually as the iteration progresses
(Byrd et al., 2012; Friedlander & Schmidt, 2012). The potential viability of this approach is sug-
gested by our warm-starting experiments (see Figure 5) wherein high testing accuracy is achieved
using a large-batch method that is warm-start with a small-batch method.

Recently, a number of researchers have described interesting theoretical properties of the loss sur-
face of deep neural networks; see e.g. (Choromanska et al., 2015; Soudry & Carmon, 2016; Lee
et al., 2016). Their work shows that, under certain regularity assumptions, the loss function of deep
learning models is fraught with many local minimizers and that many of these minimizers corre-
spond to a similar loss function value. Our results are in alignment these observations since, in our
experiments, both sharp and ﬂat minimizers have very similar loss function values. We do not know,
however, if the theoretical models mentioned above provide information about the existence and
density of sharp minimizers of the loss surface.

Our results suggest some questions: (a) can one prove that large-batch (LB) methods typically con-
verge to sharp minimizers of deep learning training functions? (In this paper, we only provided some
numerical evidence.); (b) what is the relative density of the two kinds of minima?; (c) can one design
neural network architectures for various tasks that are suitable to the properties of LB methods?; (d)
can the networks be initialized in a way that enables LB methods to succeed?; (e) is it possible,
through algorithmic or regulatory means to steer LB methods away from sharp minimizers?

9

Published as a conference paper at ICLR 2017

REFERENCES

Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning. Book in preparation for MIT

Press, 2016. URL http://www.deeplearningbook.org.

Dimitris Bertsimas, Omid Nohadani, and Kwong Meng Teo. Robust optimization for unconstrained

simulation-based problems. Operations Research, 58(1):161–178, 2010.

L´eon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,

17(9):142, 1998.

L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine

learning. arXiv preprint arXiv:1606.04838, 2016.

Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for
bound constrained optimization. SIAM Journal on Scientiﬁc Computing, 16(5):1190–1208, 1995.

Richard H Byrd, Gillian M Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in opti-
mization methods for machine learning. Mathematical programming, 134(1):127–155, 2012.

Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing

gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.

Anna Choromanska, Mikael Henaff, Michael Mathieu, G´erard Ben Arous, and Yann LeCun. The

loss surfaces of multilayer networks. In AISTATS, 2015.

Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan Vaidynathan, Srinivas Srid-
haran, Dhiraj Kalamkar, Bharat Kaul, and Pradeep Dubey. Distributed deep learning using syn-
chronous stochastic gradient descent. arXiv preprint arXiv:1602.06709, 2016.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in
neural information processing systems, pp. 1223–1231, 2012.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

Michael Charles Ferris. Weak sharp minima and penalty functions in mathematical programming.

PhD thesis, University of Cambridge, 1988.

Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting.

SIAM Journal on Scientiﬁc Computing, 34(3):A1380–A1405, 2012.

John S Garofolo, Lori F Lamel, William M Fisher, Jonathan G Fiscus, David S Pallett, Nancy L
Dahlgren, and Victor Zue. Timit acoustic-phonetic continuous speech corpus. Linguistic data
consortium, Philadelphia, 33, 1993.

Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pp. 797–842, 2015.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014a.

Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network

optimization problems. arXiv preprint arXiv:1412.6544, 2014b.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
In 2013 IEEE international conference on acoustics, speech and signal

rent neural networks.
processing, pp. 6645–6649. IEEE, 2013.

M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient

descent. arXiv preprint arXiv:1509.01240, 2015.

Sepp Hochreiter and J¨urgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

10

Published as a conference paper at ICLR 2017

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by

reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

Nitish Shirish Keskar and Albert S. Berahas. adaQN: An Adaptive Quasi-Newton Algorithm for

Training RNNs, pp. 1–16. Springer International Publishing, Cham, 2016.

D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on

Learning Representations (ICLR 2015), 2015.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998a.

Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits,

1998b.

Yann A LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller. Efﬁcient backprop. In

Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012.

Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges

to minimizers. University of California, Berkeley, 1050:16, 2016.

Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efﬁcient mini-batch training for
stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 661–670. ACM, 2014.

David JC MacKay. A practical bayesian framework for backpropagation networks. Neural compu-

tation, 4(3):448–472, 1992.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

Hossein Mobahi. Training recurrent neural networks by diffusion. arXiv preprint arXiv:1601.04114,

2016.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel,
Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. The kaldi speech recognition
In IEEE 2011 workshop on automatic speech recognition and understanding, number
toolkit.
EPFL-CONF-192584. IEEE Signal Processing Society, 2011.

Jorma Rissanen. A universal prior for integers and estimation by minimum description length. The

Annals of statistics, pp. 416–431, 1983.

Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432, 2015.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556, 2014.

Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees

for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
In Proceedings of the 30th International Conference on Machine Learning

in deep learning.
(ICML 2013), pp. 1139–1147, 2013.

11

Published as a conference paper at ICLR 2017

Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In

Advances in Neural Information Processing Systems, pp. 685–693, 2015.

Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep

neural networks via stability training. arXiv preprint arXiv:1604.04326, 2016.

A DETAILS ABOUT DATA SETS

We summarize the data sets used in our experiments in Table 5. TIMIT is a speech recognition
data set which is pre-processed using Kaldi (Povey et al., 2011) and trained using a fully-connected
network. The rest of the data sets are used without any pre-processing.

Table 5: Data Sets

Data Set
MNIST
TIMIT
CIFAR-10
CIFAR-100

# Data Points
Test
Train
10000
60000
310621
721329
10000
50000
10000
50000

# Features
28 × 28
360
32 × 32
32 × 32

# Classes Reference

10
1973
10
100

(LeCun et al., 1998a;b)
(Garofolo et al., 1993)
(Krizhevsky & Hinton, 2009)
(Krizhevsky & Hinton, 2009)

B ARCHITECTURE OF NETWORKS

B.1 NETWORK F1

For this network, we use a 784-dimensional input layer followed by 5 batch-normalized (Ioffe &
Szegedy, 2015) layers of 512 neurons each with ReLU activations. The output layer consists of 10
neurons with the softmax activation.

B.2 NETWORK F2

The network architecture for F2 is similar to F1. We use a 360-dimensional input layer followed by
7 batch-normalized layers of 512 neurons with ReLU activation. The output layer consists of 1973
neurons with the softmax activation.

B.3 NETWORKS C1 AND C3

The C1 network is a modiﬁed version of the popular AlexNet conﬁguration (Krizhevsky et al.,
2012). For simplicity, denote a stack of n convolution layers of a ﬁlters and a Kernel size of b × c
with stride length of d as n×[a, b, c, d]. The C1 conﬁguration uses 2 sets of [64, 5, 5, 2]–MaxPool(3)
followed by 2 dense layers of sizes (384, 192) and ﬁnally, an output layer of size 10. We use batch-
normalization for all layers and ReLU activations. We also use Dropout (Srivastava et al., 2014) of
0.5 retention probability for the two dense layers. The conﬁguration C3 is identical to C1 except it
uses 100 softmax outputs instead of 10.

B.4 NETWORKS C2 AND C4

The C2 network is a modiﬁed version of the popular VGG conﬁguration (Simonyan & Zisserman,
2014). The C3 network uses the conﬁguration: 2×[64, 3, 3, 1], 2×[128, 3, 3, 1], 3×[256, 3, 3, 1], 3×
[512, 3, 3, 1], 3 × [512, 3, 3, 1] which a MaxPool(2) after each stack. This stack is followed by a 512-
dimensional dense layer and ﬁnally, a 10-dimensional output layer. The activation and properties of
each layer is as in B.3. As is the case with C3 and C1, the conﬁguration C4 is identical to C2 except
that it uses 100 softmax outputs instead of 10.

12

Published as a conference paper at ICLR 2017

C PERFORMANCE MODEL

As mentioned in Section 1, a training algorithm that operates in the large-batch regime without
suffering from a generalization gap would have the ability to scale to much larger number of nodes
than is currently possible. Such and algorithm might also improve training time through faster
convergence. We present an idealized performance model that demonstrates our goal.

For LB method to be competitive with SB method, the LB method must (i) converge to minimizers
that generalize well, and (ii) do it in a reasonably number of iterations, which we analyze here. Let Is
and I(cid:96) be number of iterations required by SB and LB methods to reach the point of comparable test
accuracy, respectively. Let Bs and B(cid:96) be corresponding batch sizes and P be number of processors
being used for training. Assume that P < B(cid:96), and let fs(P ) be the parallel efﬁciency of the SB
method. For simplicity, we assume that f(cid:96)(P ), the parallel efﬁciency of the LB method, is 1.0. In
other words, we assume that the LB method is perfectly scalable due to use of a large batch size.

For LB to be faster than SB, we must have

In other words, the ratio of iterations of LB to the iterations of SB should be

I(cid:96)

B(cid:96)
P

< Is

Bs
P fs(P )

.

I(cid:96)
Is

<

Bs
fs(P )B(cid:96)

.

For example, if fs(P ) = 0.2 and Bs/B(cid:96) = 0.1, the LB method must converge in at most half as
many iterations as the SB method to see performance beneﬁts. We refer the reader to (Das et al.,
2016) for a more detailed model and a commentary on the effect of batch-size on the performance.

D CURVILINEAR PARAMETRIC PLOTS

The parametric plots for the curvilinear path from x(cid:63)
found in Figure 7.

s to x(cid:63)

(cid:96) , i.e., f (sin( απ

2 )x(cid:63)

(cid:96) + cos( απ

2 )x(cid:63)

s) can be

E ATTEMPTS TO IMPROVE LB METHODS

In this section, we discuss a few strategies that aim to remedy the problem of poor generalization
for large-batch methods. As in Section 2, we use 10% as the percentage batch-size for large-batch
experiments and 256 for small-batch methods. For all experiments, we use ADAM as the optimizer
irrespective of batch-size.

E.1 DATA AUGMENTATION

Given that large-batch methods appear to be attracted to sharp minimizers, one can ask whether it is
possible to modify the geometry of the loss function so that it is more benign to large-batch meth-
ods. The loss function depends both on the geometry of the objective function and to the size and
properties of the training set. One approach we consider is data augmentation; see e.g. (Krizhevsky
et al., 2012; Simonyan & Zisserman, 2014). The application of this technique is domain speciﬁc but
generally involves augmenting the data set through controlled modiﬁcations on the training data. For
instance, in the case of image recognition, the training set can be augmented through translations,
rotations, shearing and ﬂipping of the training data. This technique leads to regularization of the
network and has been employed for improving testing accuracy on several data sets.

In our experiments, we train the 4 image-based (convolutional) networks using aggressive data aug-
mentation and present the results in Table 6. For the augmentation, we use horizontal reﬂections,
random rotations up to 10◦ and random translation of up to 0.2 times the size of the image. It is
evident from the table that, while the LB method achieves accuracy comparable to the SB method
(also with training data augmented), the sharpness of the minima still exists, suggesting sensitivity
to images contained in neither training or testing set. In this section, we exclude parametric plots and
sharpness values for the SB method owing to space constraints and the similarity to those presented
in Section 2.2.

13

Published as a conference paper at ICLR 2017

(a) F1

(b) F2

(c) C1

(d) C2

(e) C3

(f) C4

Figure 7: Parametric Plots – Curvilinear (Left vertical axis corresponds to cross-entropy loss, f ,
and right vertical axis corresponds to classiﬁcation accuracy; solid line indicates training data set
and dashed line indicated testing data set); α = 0 corresponds to the SB minimizer while α = 1
corresponds to the LB minimizer

Table 6: Effect of Data Augmentation

Testing Accuracy

Sharpness (LB method)

Baseline (SB)

Augmented LB
83.63% ± 0.14% 82.50% ± 0.67% 231.77 ± 30.50
89.82% ± 0.12% 90.26% ± 1.15% 468.65 ± 47.86
54.55% ± 0.44% 53.03% ± 0.33% 103.68 ± 11.93
65.88 ± 0.13% 271.06 ± 29.69
63.05% ± 0.5%

(cid:15) = 10−3

(cid:15) = 5 · 10−4
45.89 ± 3.83
105.22 ± 19.57
37.67 ± 3.46
45.31 ± 5.93

C1
C2
C3
C4

14

Published as a conference paper at ICLR 2017

Table 7: Effect of Conservative Training

Testing Accuracy

Sharpness (LB method)

Baseline (SB)

(cid:15) = 10−3
Conservative LB
232.25 ± 63.81
98.03% ± 0.07% 98.12% ± 0.01%
928.40 ± 51.63
64.02% ± 0.2%
61.94% ± 1.10%
80.04% ± 0.12% 78.41% ± 0.22%
520.34 ± 34.91
89.24% ± 0.05% 88.495% ± 0.63% 632.01 ± 208.01
337.92 ± 33.09
49.58% ± 0.39% 45.98% ± 0.54%
354.94 ± 20.23
63.08% ± 0.10%

62.51 ± 0.67

(cid:15) = 5 · 10−4
46.02 ± 12.58
190.77 ± 25.33
171.19 ± 15.13
108.88 ± 47.36
110.69 ± 3.88
68.76 ± 16.29

F1
F2
C1
C2
C3
C4

E.2 CONSERVATIVE TRAINING

In (Li et al., 2014), the authors argue that the convergence rate of SGD for the large-batch setting
can be improved by obtaining iterates through the following proximal sub-problem.

xk+1 = arg min

x

1
|Bk|

(cid:88)

i∈Bk

fi(x) +

(cid:107)x − xk(cid:107)2
2

λ
2

(5)

The motivation for this strategy is, in the context of large-batch methods, to better utilize a batch
before moving onto the next one. The minimization problem is solved inexactly using 3–5 itera-
tions of gradient descent, co-ordinate descent or L-BFGS. (Li et al., 2014) report that this not only
improves the convergence rate of SGD but also leads to improved empirical performance on con-
vex machine learning problems. The underlying idea of utilizing a batch is not speciﬁc to convex
problems and we can apply the same framework for deep learning, however, without theoretical
guarantees. Indeed, similar algorithms were proposed in (Zhang et al., 2015) and (Mobahi, 2016)
for Deep Learning. The former placed emphasis on parallelization of small-batch SGD and asyn-
chrony while the latter on a diffusion-continuation mechanism for training. The results using the
conservative training approach are presented in Figure 7. In all experiments, we solve the problem
(5) using 3 iterations of ADAM and set the regularization parameter λ to be 10−3. Again, there is
a statistically signiﬁcant improvement in the testing accuracy of the large-batch method but it does
not solve the problem of sensitivity.

E.3 ROBUST TRAINING

A natural way of avoiding sharp minima is through robust optimization techniques. These methods
attempt to optimize a worst-case cost as opposed to the nominal (or true) cost. Mathematically,
given an (cid:15) > 0, these techniques solve the problem

min
x

φ(x) := max

f (x + ∆x)

(cid:107)∆x(cid:107)≤(cid:15)

(6)

Geometrically, classical (nominal) optimization attempts to locate the lowest point of a valley, while
robust optimization attempts to lower an (cid:15)–disc down the loss surface. We refer an interested reader
to (Bertsimas et al., 2010), and the references therein, for a review of non-convex robust optimiza-
tion. A direct application of this technique is, however, not feasible in our context since each itera-
tion is prohibitively expensive because it involves solving a large-scale second-order conic program
(SOCP).

15

Published as a conference paper at ICLR 2017

Figure 8: Illustration of Robust Optimization

In the context of Deep Learning, there are two inter-dependent forms of robustness: robustness to
the data and robustness to the solution. The former exploits the fact that the function f is inherently
a statistical model, while the latter treats f as a black-box function. In (Shaham et al., 2015), the
authors prove the equivalence between robustness of the solution (with respect to the data) and
adversarial training (Goodfellow et al., 2014a).

Given the partial success of the data augmentation strategy, it is natural to question the efﬁcacy
of adversarial training. As described in (Goodfellow et al., 2014a), adversarial training also aims
to artiﬁcially increase the training set but, unlike randomized data augmentation, uses the model’s
sensitivity to construct new examples. Despite its intuitive appeal, in our experiments, we found
that this strategy did not improve generalization. Similarly, we observed no generalization beneﬁt
from the stability training proposed by (Zheng et al., 2016). In both cases, the testing accuracy,
sharpness values and the parametric plots were similar to the unmodiﬁed (baseline) case discussed
in Section 2. It remains to be seen whether adversarial training (or any other form of robust training)
can increase the viability of large-batch training.

16

